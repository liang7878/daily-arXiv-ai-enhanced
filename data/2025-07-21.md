<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 20]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.DS](#cs.DS) [Total: 10]
- [cs.GT](#cs.GT) [Total: 2]
- [cs.IR](#cs.IR) [Total: 8]
- [cs.LG](#cs.LG) [Total: 53]
- [cs.NE](#cs.NE) [Total: 4]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.SE](#cs.SE) [Total: 5]
- [q-fin.RM](#q-fin.RM) [Total: 3]
- [stat.ML](#stat.ML) [Total: 5]
- [eess.IV](#eess.IV) [Total: 6]
- [math.CO](#math.CO) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.CR](#cs.CR) [Total: 6]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.NI](#cs.NI) [Total: 1]
- [math-ph](#math-ph) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [math.NA](#math.NA) [Total: 2]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.SI](#cs.SI) [Total: 2]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.AR](#cs.AR) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.CL](#cs.CL) [Total: 24]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [cs.CC](#cs.CC) [Total: 1]
- [cs.CV](#cs.CV) [Total: 35]
- [econ.GN](#econ.GN) [Total: 4]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.CY](#cs.CY) [Total: 2]
- [physics.data-an](#physics.data-an) [Total: 1]
- [cs.RO](#cs.RO) [Total: 7]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [GraphTrafficGPT: Enhancing Traffic Management Through Graph-Based AI Agent Coordination](https://arxiv.org/abs/2507.13511)
*Nabil Abdelaziz Ferhat Taleb,Abdolazim Rezaei,Raj Atulkumar Patel,Mehdi Sookhak*

Main category: cs.AI

TL;DR: 提出GraphTrafficGPT解决现有链基交通管理系统的问题，实验显示其在降低token消耗、响应延迟和提高多查询执行效率上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有链基交通管理系统如TrafficGPT存在顺序执行任务、token使用量大和可扩展性差的问题，不适用于复杂现实场景。

Method: 提出GraphTrafficGPT，用有向图表示任务及其依赖，引入Brain Agent分解查询、构建依赖图并协调专业代理网络，还采用高级上下文感知token管理和支持并发多查询处理。

Result: 与TrafficGPT相比，GraphTrafficGPT降低token消耗50.2%，平均响应延迟19.0%，多查询执行效率最高提升23.0%。

Conclusion: GraphTrafficGPT能有效解决现有系统问题，更适合现代城市交通管理。

Abstract: Large Language Models (LLMs) offer significant promise for intelligent
traffic management; however, current chain-based systems like TrafficGPT are
hindered by sequential task execution, high token usage, and poor scalability,
making them inefficient for complex, real-world scenarios. To address these
limitations, we propose GraphTrafficGPT, a novel graph-based architecture,
which fundamentally redesigns the task coordination process for LLM-driven
traffic applications. GraphTrafficGPT represents tasks and their dependencies
as nodes and edges in a directed graph, enabling efficient parallel execution
and dynamic resource allocation. The main idea behind the proposed model is a
Brain Agent that decomposes user queries, constructs optimized dependency
graphs, and coordinates a network of specialized agents for data retrieval,
analysis, visualization, and simulation. By introducing advanced context-aware
token management and supporting concurrent multi-query processing, the proposed
architecture handles interdependent tasks typical of modern urban mobility
environments. Experimental results demonstrate that GraphTrafficGPT reduces
token consumption by 50.2% and average response latency by 19.0% compared to
TrafficGPT, while supporting simultaneous multi-query execution with up to
23.0% improvement in efficiency.

</details>


### [2] [PrefPalette: Personalized Preference Modeling with Latent Attributes](https://arxiv.org/abs/2507.13541)
*Shuyue Stella Li,Melanie Sclar,Hunter Lang,Ansong Ni,Jacqueline He,Puxin Xu,Andrew Cohen,Chan Young Park,Yulia Tsvetkov,Asli Celikyilmaz*

Main category: cs.AI

TL;DR: 提出PrefPalette框架，分解偏好维度，结合多属性决策原理建模，在Reddit社区表现优于GPT - 4o，有可解释洞察。


<details>
  <summary>Details</summary>
Motivation: 当前偏好模型将人类判断视为黑箱，需理解用户偏好背后原因以实现AI系统个性化。

Method: 将多属性决策原理用于两方面：可扩展的反事实属性合成生成训练数据，基于注意力的偏好建模学习不同社区对属性的权重。

Result: 在45个Reddit社区上，PrefPalette平均预测准确率比GPT - 4o高46.6%，还揭示不同社区特定偏好。

Conclusion: PrefPalette在偏好建模上表现优越，有可解释洞察，是迈向更可信、有价值感知的个性化应用的第一步。

Abstract: Personalizing AI systems requires understanding not just what users prefer,
but the reasons that underlie those preferences - yet current preference models
typically treat human judgment as a black box. We introduce PrefPalette, a
framework that decomposes preferences into attribute dimensions and tailors its
preference prediction to distinct social community values in a
human-interpretable manner. PrefPalette operationalizes a cognitive science
principle known as multi-attribute decision making in two ways: (1) a scalable
counterfactual attribute synthesis step that involves generating synthetic
training data to isolate for individual attribute effects (e.g., formality,
humor, cultural values), and (2) attention-based preference modeling that
learns how different social communities dynamically weight these attributes.
This approach moves beyond aggregate preference modeling to capture the diverse
evaluation frameworks that drive human judgment. When evaluated on 45 social
communities from the online platform Reddit, PrefPalette outperforms GPT-4o by
46.6% in average prediction accuracy. Beyond raw predictive improvements,
PrefPalette also shed light on intuitive, community-specific profiles:
scholarly communities prioritize verbosity and stimulation, conflict-oriented
communities value sarcasm and directness, and support-based communities
emphasize empathy. By modeling the attribute-mediated structure of human
judgment, PrefPalette delivers both superior preference modeling and
transparent, interpretable insights, and serves as a first step toward more
trustworthy, value-aware personalized applications.

</details>


### [3] [GOFAI meets Generative AI: Development of Expert Systems by means of Large Language Models](https://arxiv.org/abs/2507.13550)
*Eduardo C. Garrido-Merchán,Cristina Puente*

Main category: cs.AI

TL;DR: 本文提出用大语言模型开发专家系统的新方法，通过限定领域和基于提示的提取方法生成Prolog知识表示，实验证明生成的知识库事实性和语义连贯性强，为敏感领域AI应用奠基。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽改变知识系统，但存在幻觉、生成错误或无法验证事实等缺点，需要以可控透明方式开发专家系统。

Method: 限定领域，采用基于提示的结构化提取方法，生成Prolog知识的符号表示，可由人类专家验证和修正。

Result: 通过对Claude Sonnet 3.7和GPT - 4.1的定量和定性实验，生成的知识库具有强事实依从性和语义连贯性。

Conclusion: 提出的透明混合解决方案结合大语言模型召回能力和符号系统精度，为敏感领域可靠AI应用奠定基础。

Abstract: The development of large language models (LLMs) has successfully transformed
knowledge-based systems such as open domain question nswering, which can
automatically produce vast amounts of seemingly coherent information. Yet,
those models have several disadvantages like hallucinations or confident
generation of incorrect or unverifiable facts. In this paper, we introduce a
new approach to the development of expert systems using LLMs in a controlled
and transparent way. By limiting the domain and employing a well-structured
prompt-based extraction approach, we produce a symbolic representation of
knowledge in Prolog, which can be validated and corrected by human experts.
This approach also guarantees interpretability, scalability and reliability of
the developed expert systems. Via quantitative and qualitative experiments with
Claude Sonnet 3.7 and GPT-4.1, we show strong adherence to facts and semantic
coherence on our generated knowledge bases. We present a transparent hybrid
solution that combines the recall capacity of LLMs with the precision of
symbolic systems, thereby laying the foundation for dependable AI applications
in sensitive domains.

</details>


### [4] [Why Isn't Relational Learning Taking Over the World?](https://arxiv.org/abs/2507.13558)
*David Poole*

Main category: cs.AI

TL;DR: 本文探讨AI建模对象，指出应关注实体及其关系的关系学习，分析其未普及原因并探讨提升方法。


<details>
  <summary>Details</summary>
Motivation: 当前AI多建模像素、单词和音素，而世界由实体及其属性和关系构成，应建模这些，且公司有价值的数据多为关系格式，故探讨关系学习。

Method: 分析当前AI建模现状、公司数据形式，指出关系学习领域现状。

Result: 关系学习除少数受限关系场景外未普及。

Conclusion: 需采取措施提升关系学习的重要性。

Abstract: AI seems to be taking over the world with systems that model pixels, words,
and phonemes. The world is arguably made up, not of pixels, words, and phonemes
but of entities (objects, things, including events) with properties and
relations among them. Surely we should model these, not the perception or
description of them. You might suspect that concentrating on modeling words and
pixels is because all of the (valuable) data in the world is in terms of text
and images. If you look into almost any company you will find their most
valuable data is in spreadsheets, databases and other relational formats. These
are not the form that are studied in introductory machine learning, but are
full of product numbers, student numbers, transaction numbers and other
identifiers that can't be interpreted naively as numbers. The field that
studies this sort of data has various names including relational learning,
statistical relational AI, and many others. This paper explains why relational
learning is not taking over the world -- except in a few cases with restricted
relations -- and what needs to be done to bring it to it's rightful prominence.

</details>


### [5] [BifrostRAG: Bridging Dual Knowledge Graphs for Multi-Hop Question Answering in Construction Safety](https://arxiv.org/abs/2507.13625)
*Yuxin Zhang,Xi Wang,Mo Hu,Zhenyu Zhang*

Main category: cs.AI

TL;DR: 介绍BifrostRAG系统解决安全法规信息检索和问答难题，在多跳问题数据集评估中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统检索增强生成系统难以应对安全法规文本的语言和结构复杂性及多跳查询问题。

Method: 引入BifrostRAG系统，通过实体网络图和文档导航图分别建模语言关系和文档结构，采用结合图遍历和向量语义搜索的混合检索机制。

Result: 在多跳问题数据集上，BifrostRAG的精确率达92.8%，召回率85.5%，F1分数87.3%，显著优于仅向量和仅图的RAG基线。

Conclusion: BifrostRAG是用于大语言模型驱动合规检查的强大知识引擎，其机制为工程领域处理复杂技术文档提供可迁移蓝图。

Abstract: Information retrieval and question answering from safety regulations are
essential for automated construction compliance checking but are hindered by
the linguistic and structural complexity of regulatory text. Many
compliance-related queries are multi-hop, requiring synthesis of information
across interlinked clauses. This poses a challenge for traditional
retrieval-augmented generation (RAG) systems. To overcome this, we introduce
BifrostRAG: a dual-graph RAG-integrated system that explicitly models both
linguistic relationships (via an Entity Network Graph) and document structure
(via a Document Navigator Graph). This architecture powers a hybrid retrieval
mechanism that combines graph traversal with vector-based semantic search,
enabling large language models to reason over both the meaning and the
structure of the text. Evaluation on a multi-hop question dataset shows that
BifrostRAG achieves 92.8 percent precision, 85.5 percent recall, and an F1
score of 87.3 percent. These results significantly outperform vector-only and
graph-only RAG baselines that represent current leading approaches. Error
analysis further highlights the comparative advantages of our hybrid method
over single-modality RAGs. These findings establish BifrostRAG as a robust
knowledge engine for LLM-driven compliance checking. Its dual-graph, hybrid
retrieval mechanism offers a transferable blueprint for navigating complex
technical documents across knowledge-intensive engineering domains.

</details>


### [6] [Buggy rule diagnosis for combined steps through final answer evaluation in stepwise tasks](https://arxiv.org/abs/2507.13651)
*Gerben van der Hoek,Johan Jeuring,Rogier Bos*

Main category: cs.AI

TL;DR: 研究基于最终答案的自动错误诊断潜力，应用服务到解二次方程数据集验证，结果表明能诊断部分步骤且与教师诊断高度一致。


<details>
  <summary>Details</summary>
Motivation: 智能辅导系统中，学生合并步骤导致组合爆炸使错误诊断困难，需探索基于最终答案的自动化错误诊断。

Method: 设计服务在学生合并步骤时提供错误规则诊断，将服务应用于现有解二次方程学生步骤数据集验证。

Result: 最终答案评估能诊断29.4%的步骤，与教师诊断在97%的情况下一致。

Conclusion: 研究结果可作为进一步探索该方法的基础。

Abstract: Many intelligent tutoring systems can support a student in solving a stepwise
task. When a student combines several steps in one step, the number of possible
paths connecting consecutive inputs may be very large. This combinatorial
explosion makes error diagnosis hard. Using a final answer to diagnose a
combination of steps can mitigate the combinatorial explosion, because there
are generally fewer possible (erroneous) final answers than (erroneous)
solution paths. An intermediate input for a task can be diagnosed by
automatically completing it according to the task solution strategy and
diagnosing this solution. This study explores the potential of automated error
diagnosis based on a final answer. We investigate the design of a service that
provides a buggy rule diagnosis when a student combines several steps. To
validate the approach, we apply the service to an existing dataset (n=1939) of
unique student steps when solving quadratic equations, which could not be
diagnosed by a buggy rule service that tries to connect consecutive inputs with
a single rule. Results show that final answer evaluation can diagnose 29,4% of
these steps. Moreover, a comparison of the generated diagnoses with teacher
diagnoses on a subset (n=115) shows that the diagnoses align in 97% of the
cases. These results can be considered a basis for further exploration of the
approach.

</details>


### [7] [Combining model tracing and constraint-based modeling for multistep strategy diagnoses](https://arxiv.org/abs/2507.13652)
*Gerben van der Hoek,Johan Jeuring,Rogier Bos*

Main category: cs.AI

TL;DR: 提出合并模型追踪和基于约束建模的方法用于多步策略诊断，对解二次方程数据集生成诊断，结果与教师编码一致。


<details>
  <summary>Details</summary>
Motivation: 模型追踪和基于约束建模各有优劣，希望结合两者优势提出新方法进行多步策略诊断。

Method: 定义约束为学生输入与策略步骤的共同属性，对解二次方程的现有数据集生成诊断，让两位教师对随机样本编码。

Result: 系统诊断与教师编码在140个学生步骤中完全一致。

Conclusion: 所提出的合并两种范式的方法有效可行。

Abstract: Model tracing and constraint-based modeling are two approaches to diagnose
student input in stepwise tasks. Model tracing supports identifying consecutive
problem-solving steps taken by a student, whereas constraint-based modeling
supports student input diagnosis even when several steps are combined into one
step. We propose an approach that merges both paradigms. By defining
constraints as properties that a student input has in common with a step of a
strategy, it is possible to provide a diagnosis when a student deviates from a
strategy even when the student combines several steps. In this study we explore
the design of a system for multistep strategy diagnoses, and evaluate these
diagnoses. As a proof of concept, we generate diagnoses for an existing dataset
containing steps students take when solving quadratic equations (n=2136). To
compare with human diagnoses, two teachers coded a random sample of deviations
(n=70) and applications of the strategy (n=70). Results show that that the
system diagnosis aligned with the teacher coding in all of the 140 student
steps.

</details>


### [8] [DailyLLM: Context-Aware Activity Log Generation Using Multi-Modal Sensors and LLMs](https://arxiv.org/abs/2507.13737)
*Ye Tian,Xiaoyuan Ren,Zihao Wang,Onat Gungor,Xiaofan Yu,Tajana Rosing*

Main category: cs.AI

TL;DR: 提出DailyLLM系统用于活动日志生成与总结，综合多维度信息，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有活动日志生成方法在准确性、效率和语义丰富度上有局限，利用大语言模型能力创造新机会。

Method: 提出轻量级基于大语言模型的框架，结合结构化提示和高效特征提取，综合四个维度的上下文活动信息。

Result: DailyLLM优于现有日志生成方法，能在个人电脑和树莓派上高效部署，用1.5B参数模型比70B参数的基线在日志生成BERTScore精度上提升17%，推理速度快近10倍。

Conclusion: DailyLLM是一个有效的活动日志生成与总结系统，解决了现有方法的局限。

Abstract: Rich and context-aware activity logs facilitate user behavior analysis and
health monitoring, making them a key research focus in ubiquitous computing.
The remarkable semantic understanding and generation capabilities of Large
Language Models (LLMs) have recently created new opportunities for activity log
generation. However, existing methods continue to exhibit notable limitations
in terms of accuracy, efficiency, and semantic richness. To address these
challenges, we propose DailyLLM. To the best of our knowledge, this is the
first log generation and summarization system that comprehensively integrates
contextual activity information across four dimensions: location, motion,
environment, and physiology, using only sensors commonly available on
smartphones and smartwatches. To achieve this, DailyLLM introduces a
lightweight LLM-based framework that integrates structured prompting with
efficient feature extraction to enable high-level activity understanding.
Extensive experiments demonstrate that DailyLLM outperforms state-of-the-art
(SOTA) log generation methods and can be efficiently deployed on personal
computers and Raspberry Pi. Utilizing only a 1.5B-parameter LLM model, DailyLLM
achieves a 17% improvement in log generation BERTScore precision compared to
the 70B-parameter SOTA baseline, while delivering nearly 10x faster inference
speed.

</details>


### [9] [OntView: What you See is What you Meant](https://arxiv.org/abs/2507.13759)
*Carlos Bobed,Carlota Quintana,Eduardo Mena,Jorge Bobed,Fernando Bobillo*

Main category: cs.AI

TL;DR: 本文提出本体查看器OntView，解决现有工具可视化不足问题，提供直观展示，有开源许可。


<details>
  <summary>Details</summary>
Motivation: 现有本体编辑和查看工具在有效可视化本体结构方面存在不足，限制用户理解大型本体框架。

Method: 基于DL推理器，遵循“所见即所得”范式，可视化GCI，提供多种简化视图方式。

Result: 开发出OntView本体查看器，能直观展示本体概念及定义。

Conclusion: OntView可解决现有工具可视化问题，以开源形式发布供社区使用。

Abstract: In the field of knowledge management and computer science, ontologies provide
a structured framework for modeling domain-specific knowledge by defining
concepts and their relationships. However, the lack of tools that provide
effective visualization is still a significant challenge. While numerous
ontology editors and viewers exist, most of them fail to graphically represent
ontology structures in a meaningful and non-overwhelming way, limiting users'
ability to comprehend dependencies and properties within large ontological
frameworks.
  In this paper, we present OntView, an ontology viewer that is designed to
provide users with an intuitive visual representation of ontology concepts and
their formal definitions through a user-friendly interface. Building on the use
of a DL reasoner, OntView follows a "What you see is what you meant" paradigm,
showing the actual inferred knowledge. One key aspect for this is its ability
to visualize General Concept Inclusions (GCI), a feature absent in existing
visualization tools. Moreover, to avoid a possible information overload,
OntView also offers different ways to show a simplified view of the ontology
by: 1) creating ontology summaries by assessing the importance of the concepts
(according to different available algorithms), 2) focusing the visualization on
the existing TBox elements between two given classes and 3) allowing to
hide/show different branches in a dynamic way without losing the semantics.
OntView has been released with an open-source license for the whole community.

</details>


### [10] [From Extraction to Synthesis: Entangled Heuristics for Agent-Augmented Strategic Reasoning](https://arxiv.org/abs/2507.13768)
*Renato Ghisellini,Remo Pareschi,Marco Pedroni,Giovanni Battista Raggi*

Main category: cs.AI

TL;DR: 提出用于代理增强战略推理的混合架构，结合多种方法，通过案例研究展示框架并初步验证，还讨论了局限性和扩展。


<details>
  <summary>Details</summary>
Motivation: 构建更有效的代理增强战略推理架构，区别于传统决策引擎。

Method: 结合启发式提取、语义激活和组合合成，通过语义相互依赖过程激活和组合多种启发式，利用语义交互建模和修辞框架融合冲突启发式。

Result: 通过Meta与FTC案例研究展示框架，并通过语义指标进行初步验证。

Conclusion: 讨论了架构的局限性和扩展方向，如动态干扰调整。

Abstract: We present a hybrid architecture for agent-augmented strategic reasoning,
combining heuristic extraction, semantic activation, and compositional
synthesis. Drawing on sources ranging from classical military theory to
contemporary corporate strategy, our model activates and composes multiple
heuristics through a process of semantic interdependence inspired by research
in quantum cognition. Unlike traditional decision engines that select the best
rule, our system fuses conflicting heuristics into coherent and
context-sensitive narratives, guided by semantic interaction modeling and
rhetorical framing. We demonstrate the framework via a Meta vs. FTC case study,
with preliminary validation through semantic metrics. Limitations and
extensions (e.g., dynamic interference tuning) are discussed.

</details>


### [11] [When Speed meets Accuracy: an Efficient and Effective Graph Model for Temporal Link Prediction](https://arxiv.org/abs/2507.13825)
*Haoyang Li,Yuming Xu,Yiming Li,Hanmo Liu,Darian Li,Chen Jason Zhang,Lei Chen,Qing Li*

Main category: cs.AI

TL;DR: 提出轻量级框架EAGLE用于动态图时间链接预测，结合短期时间和长期结构模式，在效率和效果上优于现有T - GNNs。


<details>
  <summary>Details</summary>
Motivation: 现有T - GNNs在动态图时间链接预测中因计算开销大存在可扩展性和效率问题。

Method: 提出EAGLE框架，包含时间感知和结构感知模块，采用自适应加权机制平衡属性，无需复杂多跳消息传递或高内存机制。

Result: 在七个真实世界时间图上的实验表明，EAGLE在效果和效率上始终优于现有T - GNNs，比基于Transformer的T - GNNs提速超50倍。

Conclusion: EAGLE是解决动态图时间链接预测问题的有效且高效的方法。

Abstract: Temporal link prediction in dynamic graphs is a critical task with
applications in diverse domains such as social networks, recommendation
systems, and e-commerce platforms. While existing Temporal Graph Neural
Networks (T-GNNs) have achieved notable success by leveraging complex
architectures to model temporal and structural dependencies, they often suffer
from scalability and efficiency challenges due to high computational overhead.
In this paper, we propose EAGLE, a lightweight framework that integrates
short-term temporal recency and long-term global structural patterns. EAGLE
consists of a time-aware module that aggregates information from a node's most
recent neighbors to reflect its immediate preferences, and a structure-aware
module that leverages temporal personalized PageRank to capture the influence
of globally important nodes. To balance these attributes, EAGLE employs an
adaptive weighting mechanism to dynamically adjust their contributions based on
data characteristics. Also, EAGLE eliminates the need for complex multi-hop
message passing or memory-intensive mechanisms, enabling significant
improvements in efficiency. Extensive experiments on seven real-world temporal
graphs demonstrate that EAGLE consistently achieves superior performance
against state-of-the-art T-GNNs in both effectiveness and efficiency,
delivering more than a 50x speedup over effective transformer-based T-GNNs.

</details>


### [12] [CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning](https://arxiv.org/abs/2507.14111)
*Xiaoya Li,Xiaofei Sun,Albert Wang,Jiwei Li,Chris Shum*

Main category: cs.AI

TL;DR: 本文提出用于CUDA优化的自动强化学习框架CUDA - L1，能提升性能且有良好可移植性，证明强化学习可将LLM转变为有效优化器。


<details>
  <summary>Details</summary>
Motivation: 大语言模型发展使GPU计算资源需求增长，急需自动CUDA优化策略，现有SOTA模型改善CUDA速度成功率低。

Method: 引入自动强化学习框架CUDA - L1进行CUDA优化。

Result: 在KernelBench的250个CUDA内核上平均加速x17.7，峰值达x449；在不同GPU架构上有良好加速表现；发现多种优化技术、原理，识别性能瓶颈。

Conclusion: 强化学习可将初始表现差的LLM转变为有效CUDA优化器，为CUDA操作自动优化带来可能，有望提升GPU效率。

Abstract: The exponential growth in demand for GPU computing resources, driven by the
rapid advancement of Large Language Models, has created an urgent need for
automated CUDA optimization strategies. While recent advances in LLMs show
promise for code generation, current SOTA models (e.g. R1, o1) achieve low
success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an
automated reinforcement learning framework for CUDA optimization.
  CUDA-L1 achieves performance improvements on the CUDA optimization task:
trained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250
CUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the
model also demonstrates excellent portability across GPU architectures,
achieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,
x14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.
Beyond these benchmark results, CUDA-L1 demonstrates several remarkable
properties: 1) Discovers a variety of CUDA optimization techniques and learns
to combine them strategically to achieve optimal performance; 2) Uncovers
fundamental principles of CUDA optimization; 3) Identifies non-obvious
performance bottlenecks and rejects seemingly beneficial optimizations that
harm performance.
  The capabilities of CUDA-L1 demonstrate that reinforcement learning can
transform an initially poor-performing LLM into an effective CUDA optimizer
through speedup-based reward signals alone, without human expertise or domain
knowledge. More importantly, the trained RL model extend the acquired reasoning
abilities to new kernels. This paradigm opens possibilities for automated
optimization of CUDA operations, and holds promise to substantially promote GPU
efficiency and alleviate the rising pressure on GPU computing resources.

</details>


### [13] [Causal Knowledge Transfer for Multi-Agent Reinforcement Learning in Dynamic Environments](https://arxiv.org/abs/2507.13846)
*Kathrin Korte,Christian Medeiros Adriano,Sona Ghahremani,Holger Giese*

Main category: cs.AI

TL;DR: 本文提出因果知识转移框架用于多智能体强化学习，使智能体在非平稳环境中能零样本转移恢复动作宏，研究揭示智能体适应新环境表现及因果知识转移影响因素。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体强化学习在非平稳环境中知识转移难、传统方法难泛化及智能体需昂贵再训练的问题。

Method: 引入因果知识转移框架，将碰撞建模为因果干预，形成恢复动作宏，从第二个智能体在线零样本转移该宏。

Result: 具有异构目标的智能体在适应新环境时能弥补随机探索和完全再训练策略间约一半差距；因果知识转移的影响取决于环境复杂性和智能体异构目标的相互作用。

Conclusion: 所提因果知识转移框架对多智能体强化学习在非平稳环境中的知识转移有积极作用，且环境和目标因素会影响其效果。

Abstract: [Context] Multi-agent reinforcement learning (MARL) has achieved notable
success in environments where agents must learn coordinated behaviors. However,
transferring knowledge across agents remains challenging in non-stationary
environments with changing goals. [Problem] Traditional knowledge transfer
methods in MARL struggle to generalize, and agents often require costly
retraining to adapt. [Approach] This paper introduces a causal knowledge
transfer framework that enables RL agents to learn and share compact causal
representations of paths within a non-stationary environment. As the
environment changes (new obstacles), agents' collisions require adaptive
recovery strategies. We model each collision as a causal intervention
instantiated as a sequence of recovery actions (a macro) whose effect
corresponds to a causal knowledge of how to circumvent the obstacle while
increasing the chances of achieving the agent's goal (maximizing cumulative
reward). This recovery action macro is transferred online from a second agent
and is applied in a zero-shot fashion, i.e., without retraining, just by
querying a lookup model with local context information (collisions). [Results]
Our findings reveal two key insights: (1) agents with heterogeneous goals were
able to bridge about half of the gap between random exploration and a fully
retrained policy when adapting to new environments, and (2) the impact of
causal knowledge transfer depends on the interplay between environment
complexity and agents' heterogeneous goals.

</details>


### [14] [Large Language Models as Innovators: A Framework to Leverage Latent Space Exploration for Novelty Discovery](https://arxiv.org/abs/2507.13874)
*Mateusz Bystroński,Mikołaj Hołysz,Grzegorz Piotrowski,Nitesh V. Chawla,Tomasz Kajdanowicz*

Main category: cs.AI

TL;DR: 论文提出模型无关的潜在空间创意生成框架解决大语言模型创新想法生成难题，介绍早期原型与初步结果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成创新想法困难，易复制训练模式，现有方法难泛化。

Method: 提出模型无关的潜在空间创意生成框架，通过导航想法的连续嵌入空间实现可控、可扩展的创意生成。

Result: 介绍了方法的早期原型，给出初步结果，显示其作为通用人机协作创意伙伴的潜力。

Conclusion: 该框架无需手工规则，易适应不同领域、输入格式和创意任务，有成为通用人机协作创意伙伴的潜力。

Abstract: Innovative idea generation remains a core challenge in AI, as large language
models (LLMs) often struggle to produce outputs that are both novel and
relevant. Despite their fluency, LLMs tend to replicate patterns seen during
training, limiting their ability to diverge creatively without extensive prompt
engineering. Prior work has addressed this through domain-specific heuristics
and structured prompting pipelines, but such solutions are brittle and
difficult to generalize. In this paper, we propose a model-agnostic
latent-space ideation framework that enables controlled, scalable creativity by
navigating the continuous embedding space of ideas. Unlike prior methods, our
framework requires no handcrafted rules and adapts easily to different domains,
input formats, and creative tasks. This paper introduces an early-stage
prototype of our method, outlining the conceptual framework and preliminary
results highlighting its potential as a general-purpose co-ideator for human-AI
collaboration.

</details>


### [15] [Cross-modal Causal Intervention for Alzheimer's Disease Prediction](https://arxiv.org/abs/2507.13956)
*Yutao Jin,Haowen Xiao,Jielei Chu,Fengmao Lv,Yuxiao Li,Tianrui Li*

Main category: cs.AI

TL;DR: 提出ADPC框架用于AD诊断辅助，利用LLM处理临床数据，结合影像与文本数据分类，通过因果干预消除混杂因素，实验效果优异。


<details>
  <summary>Details</summary>
Motivation: AD诊断因多模态数据选择偏差和变量复杂关系面临挑战，需有效诊断辅助方法。

Method: 提出ADPC框架，用LLM按模板总结临床数据，结合MRI、fMRI影像与LLM生成的文本数据分类，通过因果干预消除混杂因素。

Result: 实验表明该方法在区分CN/MCI/AD病例上表现出色，多数评估指标达SOTA水平。

Conclusion: 展示了因果推理与多模态学习结合用于神经疾病诊断的潜力。

Abstract: Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's
Disease (AD), where early identification and intervention can effectively slow
the progression to dementia. However, diagnosing AD remains a significant
challenge in neurology due to the confounders caused mainly by the selection
bias of multimodal data and the complex relationships between variables. To
address these issues, we propose a novel visual-language causal intervention
framework named Alzheimer's Disease Prediction with Cross-modal Causal
Intervention (ADPC) for diagnostic assistance. Our ADPC employs large language
model (LLM) to summarize clinical data under strict templates, maintaining
structured text outputs even with incomplete or unevenly distributed datasets.
The ADPC model utilizes Magnetic Resonance Imaging (MRI), functional MRI (fMRI)
images and textual data generated by LLM to classify participants into
Cognitively Normal (CN), MCI, and AD categories. Because of the presence of
confounders, such as neuroimaging artifacts and age-related biomarkers,
non-causal models are likely to capture spurious input-output correlations,
generating less reliable results. Our framework implicitly eliminates
confounders through causal intervention. Experimental results demonstrate the
outstanding performance of our method in distinguishing CN/MCI/AD cases,
achieving state-of-the-art (SOTA) metrics across most evaluation metrics. The
study showcases the potential of integrating causal reasoning with multi-modal
learning for neurological disease diagnosis.

</details>


### [16] [Towards Constraint Temporal Answer Set Programming](https://arxiv.org/abs/2507.13958)
*Pedro Cabalar,Martín Diéguez,François Olivier,Torsten Schaub,Igor Stéphan*

Main category: cs.AI

TL;DR: 提出针对Answer Set Programming (ASP) 的新颖时间和基于约束的扩展，以解决动态系统细粒度推理挑战并建立逻辑框架。


<details>
  <summary>Details</summary>
Motivation: 逻辑方法（如ASP）在对动态系统进行细粒度时间和数值推理时面临挑战，需要新方法。

Method: 结合线性时间的Here - and - There逻辑（提供非单调时间推理能力）和带约束的Here - and - There逻辑（实现数值约束集成和操作）。

Result: 得到一个表达性强的系统，可用于在ASP范式下对复杂动态系统进行高分辨率推理。

Conclusion: 为ASP范式下处理高分辨率复杂动态系统建立了基础逻辑框架。

Abstract: Reasoning about dynamic systems with a fine-grained temporal and numeric
resolution presents significant challenges for logic-based approaches like
Answer Set Programming (ASP). To address this, we introduce and elaborate upon
a novel temporal and constraint-based extension of the logic of Here-and-There
and its nonmonotonic equilibrium extension, representing, to the best of our
knowledge, the first approach to nonmonotonic temporal reasoning with
constraints specifically tailored for ASP. This expressive system is achieved
by a synergistic combination of two foundational ASP extensions: the
linear-time logic of Here-and-There, providing robust nonmonotonic temporal
reasoning capabilities, and the logic of Here-and-There with constraints,
enabling the direct integration and manipulation of numeric constraints, among
others. This work establishes the foundational logical framework for tackling
complex dynamic systems with high resolution within the ASP paradigm.

</details>


### [17] [KROMA: Ontology Matching with Knowledge Retrieval and Large Language Models](https://arxiv.org/abs/2507.14032)
*Lam Nguyen,Erika Barcelos,Roger French,Yinghui Wu*

Main category: cs.AI

TL;DR: 提出KROMA框架用于本体匹配，结合知识检索与大语言模型，实验表明该框架优于经典和前沿方法。


<details>
  <summary>Details</summary>
Motivation: 现有本体匹配系统依赖手工规则或专用模型，适应性有限。

Method: 在检索增强生成（RAG）管道中利用大语言模型，集成基于双相似性的概念匹配和轻量级本体细化步骤。

Result: 在多个基准数据集上实验显示，结合知识检索与上下文增强的大语言模型显著提升本体匹配效果，优于经典和前沿方法，通信开销相当。

Conclusion: 提出的优化技术在大规模本体匹配中可行且有益。

Abstract: Ontology Matching (OM) is a cornerstone task of semantic interoperability,
yet existing systems often rely on handcrafted rules or specialized models with
limited adaptability. We present KROMA, a novel OM framework that harnesses
Large Language Models (LLMs) within a Retrieval-Augmented Generation (RAG)
pipeline to dynamically enrich the semantic context of OM tasks with
structural, lexical, and definitional knowledge. To optimize both performance
and efficiency, KROMA integrates a bisimilarity-based concept matching and a
lightweight ontology refinement step, which prune candidate concepts and
substantially reduce the communication overhead from invoking LLMs. Through
experiments on multiple benchmark datasets, we show that integrating knowledge
retrieval with context-augmented LLMs significantly enhances ontology matching,
outperforming both classic OM systems and cutting-edge LLM-based approaches
while keeping communication overhead comparable. Our study highlights the
feasibility and benefit of the proposed optimization techniques (targeted
knowledge retrieval, prompt enrichment, and ontology refinement) for ontology
matching at scale.

</details>


### [18] [Automated Interpretation of Non-Destructive Evaluation Contour Maps Using Large Language Models for Bridge Condition Assessment](https://arxiv.org/abs/2507.14107)
*Viraj Nishesh Darji,Callie C. Liao,Duoduo Liao*

Main category: cs.AI

TL;DR: 本文开展了一项关于大语言模型（LLMs）用于解读桥梁无损检测（NDE）等高线图的试点研究，探索不同LLMs表现，发现部分模型能有效提高桥梁检测效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 桥梁维护和安全至关重要，NDE技术解读数据耗时且需专业知识，LLMs的发展为自动化和改进分析提供了新途径。

Method: 设计特定提示探索多个LLMs，用于解读五种不同的NDE等高线图，评估各模型生成详细描述、识别缺陷、提供建议和整体准确性的能力。

Result: 九个模型中有四个能提供更好的图像描述，ChatGPT - 4和Claude 3.5 Sonnet生成的总结更有效。

Conclusion: LLMs有潜力显著提高桥梁维护决策效率和准确性，该研究的创新方法有助于加强基础设施管理和安全评估。

Abstract: Bridge maintenance and safety are essential for transportation authorities,
and Non-Destructive Evaluation (NDE) techniques are critical to assessing
structural integrity. However, interpreting NDE data can be time-consuming and
requires expertise, potentially delaying decision-making. Recent advancements
in Large Language Models (LLMs) offer new ways to automate and improve this
analysis. This pilot study introduces a holistic assessment of LLM capabilities
for interpreting NDE contour maps and demonstrates the effectiveness of LLMs in
providing detailed bridge condition analyses. It establishes a framework for
integrating LLMs into bridge inspection workflows, indicating that LLM-assisted
analysis can enhance efficiency without compromising accuracy. In this study,
several LLMs are explored with prompts specifically designed to enhance the
quality of image descriptions, which are applied to interpret five different
NDE contour maps obtained through technologies for assessing bridge conditions.
Each LLM model is evaluated based on its ability to produce detailed
descriptions, identify defects, provide actionable recommendations, and
demonstrate overall accuracy. The research indicates that four of the nine
models provide better image descriptions, effectively covering a wide range of
topics related to the bridge's condition. The outputs from these four models
are summarized using five different LLMs to form a comprehensive overview of
the bridge. Notably, LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more
effective summaries. The findings suggest that LLMs have the potential to
significantly improve efficiency and accuracy. This pilot study presents an
innovative approach that leverages LLMs for image captioning in parallel and
summarization, enabling faster decision-making in bridge maintenance and
enhancing infrastructure management and safety assessments.

</details>


### [19] [Glucose-ML: A collection of longitudinal diabetes datasets for development of robust AI solutions](https://arxiv.org/abs/2507.14077)
*Temiloluwa Prioleau,Baiying Lu,Yanjun Cui*

Main category: cs.AI

TL;DR: 文章介绍Glucose - ML糖尿病数据集集合，含对比分析、案例研究及开发建议，还提供数据链接和代码。


<details>
  <summary>Details</summary>
Motivation: 解决获取高质量大数据集阻碍糖尿病管理AI解决方案开发的问题，加速开发透明、可复现且强大的AI解决方案。

Method: 提供Glucose - ML数据集，进行对比分析指导数据选择，开展血糖预测案例研究。

Result: 同一算法在不同数据集上预测结果差异显著。

Conclusion: 研究结果为糖尿病或更广泛健康领域开发强大AI解决方案提供建议。

Abstract: Artificial intelligence (AI) algorithms are a critical part of
state-of-the-art digital health technology for diabetes management. Yet, access
to large high-quality datasets is creating barriers that impede development of
robust AI solutions. To accelerate development of transparent, reproducible,
and robust AI solutions, we present Glucose-ML, a collection of 10 publicly
available diabetes datasets, released within the last 7 years (i.e., 2018 -
2025). The Glucose-ML collection comprises over 300,000 days of continuous
glucose monitor (CGM) data with a total of 38 million glucose samples collected
from 2500+ people across 4 countries. Participants include persons living with
type 1 diabetes, type 2 diabetes, prediabetes, and no diabetes. To support
researchers and innovators with using this rich collection of diabetes
datasets, we present a comparative analysis to guide algorithm developers with
data selection. Additionally, we conduct a case study for the task of blood
glucose prediction - one of the most common AI tasks within the field. Through
this case study, we provide a benchmark for short-term blood glucose prediction
across all 10 publicly available diabetes datasets within the Glucose-ML
collection. We show that the same algorithm can have significantly different
prediction results when developed/evaluated with different datasets. Findings
from this study are then used to inform recommendations for developing robust
AI solutions within the diabetes or broader health domain. We provide direct
links to each longitudinal diabetes dataset in the Glucose-ML collection and
openly provide our code.

</details>


### [20] [Generative AI-Driven High-Fidelity Human Motion Simulation](https://arxiv.org/abs/2507.14097)
*Hari Iyer,Neel Macwan,Atharva Jitendra Hude,Heejin Jeong,Shenghan Guo*

Main category: cs.AI

TL;DR: 提出Generative-AI-Enabled HMS（G-AI-HMS）提升工业任务中人体运动模拟质量，案例显示其效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有人体运动模拟方法存在运动保真度低的问题，需提升模拟质量。

Method: 集成文本到文本和文本到运动模型，用大语言模型转换任务描述，用计算机视觉验证AI增强的动作，应用姿态估计算法和运动相似性指标。

Result: 案例研究中，多数场景下AI增强动作误差低于人工描述，统计分析显示AI增强提示显著降低关节误差和时间不对齐情况。

Conclusion: G-AI-HMS能有效提升人体运动模拟质量。

Abstract: Human motion simulation (HMS) supports cost-effective evaluation of worker
behavior, safety, and productivity in industrial tasks. However, existing
methods often suffer from low motion fidelity. This study introduces
Generative-AI-Enabled HMS (G-AI-HMS), which integrates text-to-text and
text-to-motion models to enhance simulation quality for physical tasks.
G-AI-HMS tackles two key challenges: (1) translating task descriptions into
motion-aware language using Large Language Models aligned with MotionGPT's
training vocabulary, and (2) validating AI-enhanced motions against real human
movements using computer vision. Posture estimation algorithms are applied to
real-time videos to extract joint landmarks, and motion similarity metrics are
used to compare them with AI-enhanced sequences. In a case study involving
eight tasks, the AI-enhanced motions showed lower error than human created
descriptions in most scenarios, performing better in six tasks based on spatial
accuracy, four tasks based on alignment after pose normalization, and seven
tasks based on overall temporal similarity. Statistical analysis showed that
AI-enhanced prompts significantly (p $<$ 0.0001) reduced joint error and
temporal misalignment while retaining comparable posture accuracy.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [21] [Graph Neural Network Surrogates for Contacting Deformable Bodies with Necessary and Sufficient Contact Detection](https://arxiv.org/abs/2507.13459)
*Vijay K. Dubey,Collin E. Haese,Osman Gültekin,David Dalton,Manuel K. Rausch,Jan N. Fuhg*

Main category: cs.CE

TL;DR: 提出用于非线性边值问题快速推理的图神经网络架构，测试其性能并分析优缺点，推理时可达千倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有可变形体接触代理建模方法存在局限，需解决有效建模可变形体接触尤其是几何变化情况下的问题。

Method: 提出利用连续碰撞检测、纳入可变形体接触充分条件的图神经网络架构，在两个基准测试中测试性能。

Result: 添加接触项到损失函数有正则化效果，网络泛化性更好，能处理不同参考几何，但训练计算成本高，推理可达千倍加速。

Conclusion: 所提架构有优势，但存在训练成本与推理加速的权衡。

Abstract: Surrogate models for the rapid inference of nonlinear boundary value problems
in mechanics are helpful in a broad range of engineering applications. However,
effective surrogate modeling of applications involving the contact of
deformable bodies, especially in the context of varying geometries, is still an
open issue. In particular, existing methods are confined to rigid body contact
or, at best, contact between rigid and soft objects with well-defined contact
planes. Furthermore, they employ contact or collision detection filters that
serve as a rapid test but use only the necessary and not sufficient conditions
for detection. In this work, we present a graph neural network architecture
that utilizes continuous collision detection and, for the first time,
incorporates sufficient conditions designed for contact between soft deformable
bodies. We test its performance on two benchmarks, including a problem in soft
tissue mechanics of predicting the closed state of a bioprosthetic aortic
valve. We find a regularizing effect on adding additional contact terms to the
loss function, leading to better generalization of the network. These benefits
hold for simple contact at similar planes and element normal angles, and
complex contact at differing planes and element normal angles. We also
demonstrate that the framework can handle varying reference geometries.
However, such benefits come with high computational costs during training,
resulting in a trade-off that may not always be favorable. We quantify the
training cost and the resulting inference speedups on various hardware
architectures. Importantly, our graph neural network implementation results in
up to a thousand-fold speedup for our benchmark problems at inference.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [22] [CogniQ-H: A Soft Hierarchical Reinforcement Learning Paradigm for Automated Data Preparation](https://arxiv.org/abs/2507.13710)
*Jing Chang,Chang Liu,Jinbin Huang,Rui Mao,Jianbin Qin*

Main category: cs.DB

TL;DR: 提出 CogniQ - H 框架用于自动化数据准备，实验显示在管道质量和收敛速度上优于现有 RL 方法。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法处理数据准备问题效率低，未捕捉问题的结构化、层次化特性，朴素 HRL 易做次优、不可逆决策。

Method: 引入 CogniQ - H 框架，将动作选择建模为贝叶斯推理问题，结合大语言模型生成的高层策略先验、学习排序模型的算子质量得分和智能体 Q 函数的长期价值估计。

Result: 在 18 个不同数据集上实验，CogniQ - H 管道质量提升达 13.9%，收敛速度快 2.8 倍。

Conclusion: CogniQ - H 框架在自动化数据准备中有效，能平衡战略指导和自适应、基于证据的决策。

Abstract: Data preparation is a foundational yet notoriously challenging component of
the machine learning lifecycle, characterized by a vast combinatorial search
space of potential operator sequences. While reinforcement learning (RL) offers
a promising direction, existing approaches are inefficient as they fail to
capture the structured, hierarchical nature of the problem. We argue that
Hierarchical Reinforcement Learning (HRL), a paradigm that has been successful
in other domains, provides a conceptually ideal yet previously unexplored
framework for this task. However, a naive HRL implementation with a `hard
hierarchy' is prone to suboptimal, irreversible decisions. To address this, we
introduce CogniQ-H, the first framework to implement a soft hierarchical
paradigm for robust, end-to-end automated data preparation. CogniQ-H formulates
action selection as a Bayesian inference problem. A high-level strategic prior,
generated by a Large Language Model (LLM), guides exploration
probabilistically. This prior is synergistically combined with a fine-grained
operator quality score from a supervised Learning-to-Rank (LTR) model and a
long-term value estimate from the agent's own Q-function. This hybrid
architecture allows CogniQ-H to balance strategic guidance with adaptive,
evidence-based decision-making. Through extensive experiments on 18 diverse
datasets spanning multiple domains, we demonstrate that CogniQ-H achieves up to
13.9\% improvement in pipeline quality and 2.8$\times$ faster convergence
compared to state-of-the-art RL-based methods.

</details>


### [23] [LLaPipe: LLM-Guided Reinforcement Learning for Automated Data Preparation Pipeline Construction](https://arxiv.org/abs/2507.13712)
*Jing Chang,Chang Liu,Jinbin Huang,Rui Mao,Jianbin Qin*

Main category: cs.DB

TL;DR: 提出LLaPipe框架解决自动数据准备中强化学习方法探索效率低的问题，有三项创新，实验表明比现有方法有显著提升且计算高效。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的自动数据准备方法在庞大预处理管道空间中探索效率低，需要解决探索瓶颈。

Method: 提出LLaPipe框架，包括LLM策略顾问、经验蒸馏机制和自适应顾问触发策略。

Result: 在18个不同数据集上，管道质量最多提升22.4%，收敛速度快2.3倍，且仅19.0%的探索步骤使用LLM。

Conclusion: LLaPipe框架能有效解决探索瓶颈问题，提升自动数据准备效率且计算高效。

Abstract: Automated data preparation is crucial for democratizing machine learning, yet
existing reinforcement learning (RL) based approaches suffer from inefficient
exploration in the vast space of possible preprocessing pipelines. We present
LLaPipe, a novel framework that addresses this exploration bottleneck by
integrating Large Language Models (LLMs) as intelligent policy advisors. Unlike
traditional methods that rely solely on statistical features and blind
trial-and-error, LLaPipe leverages the semantic understanding capabilities of
LLMs to provide contextually relevant exploration guidance. Our framework
introduces three key innovations: (1) an LLM Policy Advisor that analyzes
dataset semantics and pipeline history to suggest promising preprocessing
operations, (2) an Experience Distillation mechanism that mines successful
patterns from past pipelines and transfers this knowledge to guide future
exploration, and (3) an Adaptive Advisor Triggering strategy
(Advisor\textsuperscript{+}) that dynamically determines when LLM intervention
is most beneficial, balancing exploration effectiveness with computational
cost. Through extensive experiments on 18 diverse datasets spanning multiple
domains, we demonstrate that LLaPipe achieves up to 22.4\% improvement in
pipeline quality and 2.3$\times$ faster convergence compared to
state-of-the-art RL-based methods, while maintaining computational efficiency
through selective LLM usage (averaging only 19.0\% of total exploration steps).

</details>


### [24] [Efficient and Scalable Self-Healing Databases Using Meta-Learning and Dependency-Driven Recovery](https://arxiv.org/abs/2507.13757)
*Joydeep Chandra,Prabal Manhas*

Main category: cs.DB

TL;DR: 本文探索用元学习和强化学习开发数据库自修复框架，解决动态环境挑战，框架在适应性等方面有显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决动态工作负载环境中数据库实时适应性和减少再训练的挑战。

Method: 将模型无关元学习与强化学习结合，采用多目标优化，引入图神经网络，通过合成任务增强和自监督学习提高数据效率，集成可解释AI技术，使用联邦元学习。

Result: 框架在适应性、效率和可靠性方面有显著改善。

Conclusion: 该框架推动了数据库管理和自修复系统的发展。

Abstract: This study explored the development of a novel self-healing framework for
databases using meta-learning and reinforcement learning techniques. The
primary objective was to address the challenges of real-time adaptability and
minimal retraining in dynamic workload environments. The proposed approach
integrated Model-Agnostic Meta-Learning (MAML) with reinforcement learning to
enable anomaly detection and corrective actions that adapted swiftly to
evolving database conditions. Multi-objective optimization was employed to
balance performance, resource utilization, and cost efficiency during the
healing process. Graph Neural Networks (GNNs) were incorporated to model
interdependencies within database components, ensuring holistic recovery
strategies. Data efficiency was enhanced through synthetic task augmentation
and self-supervised learning, enabling effective training in sparse data
regimes. To promote trust and transparency, explainable AI techniques were
integrated to provide interpretable insights into anomaly detection and healing
actions. Federated meta-learning further enabled privacy-preserving
adaptability in distributed database environments. The framework demonstrated
significant improvements in adaptability, efficiency, and reliability,
contributing to advancements in database management and self-healing systems.

</details>


### [25] [Towards Next Generation Data Engineering Pipelines](https://arxiv.org/abs/2507.13892)
*Kevin M. Kramer,Valerie Restat,Sebastian Strasser,Uta Störl,Meike Klettke*

Main category: cs.DB

TL;DR: 指出数据工程管道存在的问题，提出下一代数据工程管道的三个级别并给出实现方法。


<details>
  <summary>Details</summary>
Motivation: 解决数据工程管道在组成和操作中存在的无法保证数据质量、对变化无反应等问题。

Method: 提出下一代数据工程管道的三个级别：优化数据管道、自我感知数据管道和自我适应数据管道，并针对每个级别提出实现方法。

Result: 无明确提及具体结果。

Conclusion: 提出下一代数据工程管道的三个级别及实现方法以解决现有数据工程管道的问题。

Abstract: Data engineering pipelines are a widespread way to provide high-quality data
for all kinds of data science applications. However, numerous challenges still
remain in the composition and operation of such pipelines. Data engineering
pipelines do not always deliver high-quality data. By default, they are also
not reactive to changes. When new data is coming in which deviates from prior
data, the pipeline could crash or output undesired results. We therefore
envision three levels of next generation data engineering pipelines: optimized
data pipelines, self-aware data pipelines, and self-adapting data pipelines.
Pipeline optimization addresses the composition of operators and their
parametrization in order to achieve the highest possible data quality.
Self-aware data engineering pipelines enable a continuous monitoring of its
current state, notifying data engineers on significant changes. Self-adapting
data engineering pipelines are then even able to automatically react to those
changes. We propose approaches to achieve each of these levels.

</details>


### [26] [Project-connex Decompositions and Tractability of Aggregate Group-by Conjunctive Queries](https://arxiv.org/abs/2507.14101)
*Diego Figueira,Cibele Freire*

Main category: cs.DB

TL;DR: 引入'project - connex'树宽衡量半环上带'group - by'投影的计数和聚合连接查询的易处理性，给出统一算法并恢复相关结果，还展示其可由经典树分解算法得到。


<details>
  <summary>Details</summary>
Motivation: 为半环上带'group - by'投影的计数和聚合连接查询找到一种衡量易处理性的方法，以获得与先前研究相当的复杂度边界。

Method: 将'free - connex'分解自然扩展为'project - connex'树分解，进行统一的算法操作。

Result: 得到了与先前研究相当的复杂度边界，恢复了计数连接查询和半环聚合查询相关的可处理性结果，且展示了可通过经典树分解算法得到'project - connex'树分解。

Conclusion: 'project - connex'树宽可有效衡量相关查询的易处理性，其分解方法可用于统一处理多种查询问题。

Abstract: We introduce 'project-connex' tree-width as a measure of tractability for
counting and aggregate conjunctive queries over semirings with 'group-by'
projection (also known as 'AJAR' or 'FAQ' queries). This elementary measure
allows to obtain comparable complexity bounds to the ones obtained by previous
structural conditions tailored for efficient evaluation of semiring aggregate
queries, enumeration algorithms of conjunctive queries, and tractability of
counting answers to conjunctive queries.
  Project-connex tree decompositions are defined as the natural extension of
the known notion of 'free-connex' decompositions. They allow for a unified,
simple and intuitive algorithmic manipulation for evaluation of aggregate
queries and explain some existing tractability results on conjunctive query
enumeration, counting conjunctive query evaluation, and evaluation of semiring
aggregate queries. Using this measure we also recover results relating
tractable classes of counting conjunctive queries and bounded free-connex
tree-width, or the constant-time delay enumeration of semiring aggregate
queries over bounded project-connex classes. We further show that
project-connex tree decompositions can be obtained via algorithms for computing
classical tree decompositions.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [27] [Checkmate: Zero-Overhead Model Checkpointing via Network Gradient Replication](https://arxiv.org/abs/2507.13522)
*Ankit Bhardwaj,Weiyang Wang,Jeremy Carin,Adam Belay,Manya Ghobadi*

Main category: cs.DC

TL;DR: 本文提出Checkmate系统，可在DNN训练中实现无训练减速的逐轮检查点。评估显示其性能优越。


<details>
  <summary>Details</summary>
Motivation: 传统检查点方法在检查点频率和故障成本间存在权衡，需避免此问题。

Method: 提出新的多播抽象，将梯度同时传递到基于CPU的影子集群，影子集群对模型副本应用梯度以维护检查点。

Result: Checkmate逐轮检查点的训练吞吐量与无检查点基线相当，比现有系统检查点频率高5 - 34.5倍，故障重复工作量减少80% - 97.1%，相同频率下吞吐量高1.3 - 6.5倍。

Conclusion: Checkmate能在DNN训练中无训练减速地实现逐轮检查点，性能优于现有系统。

Abstract: This paper presents Checkmate, a system that enables per-iteration
checkpointing in DNN training without any training slowdown. The traditional
approach to checkpointing requires a pause in training to copy model states to
a separate location, allowing the state to be restored in the event of failure.
This approach fundamentally has a tradeoff between the frequency of checkpoints
and the cost of a failure. We avoid this tradeoff; our key insight is that in
data-parallel training, all information necessary to create a checkpoint
already exists in the network as gradients. Our core contribution is a new
multicast abstraction that simultaneously delivers gradients to a separate
CPU-based shadow cluster. The shadow maintains a checkpoint by applying those
gradients to a copy of the model. Our evaluation shows that Checkmate performs
per-iteration checkpointing with training throughput comparable to an ideal
no-checkpoint baseline. Checkmate achieves 5 to 34.5x more frequent
checkpointing compared to state-of-the-art checkpointing systems, resulting in
80% to 97.1% reduction in repeated work per failure. At the same checkpointing
frequency, Checkmate delivers 1.3x to 6.5x throughput compared to other
systems.

</details>


### [28] [Leveraging Multi-Instance GPUs through moldable task scheduling](https://arxiv.org/abs/2507.13601)
*Jorge Villarrubia,Luis Costero,Francisco D. Igual,Katzalin Olcoz*

Main category: cs.DC

TL;DR: 本文通过可动态重配置的可塑任务调度挖掘NVIDIA MIG潜力，提出FAR算法解决多任务执行的最小化完工时间问题，实验结果良好。


<details>
  <summary>Details</summary>
Motivation: 挖掘NVIDIA MIG未被充分利用的潜力，解决多任务执行在MIG约束下的最小化完工时间问题。

Method: 提出FAR三阶段算法，结合经典任务可塑性方法、调度策略和局部搜索，离线批量调度任务。

Result: 不考虑重配置成本，在NVIDIA A30模型上近似因子为7/4，A100/H100为2；考虑重配置成本，实验中完工时间相对最优值不超过1.22倍（基准测试）和1.10倍（合成输入）。

Conclusion: FAR算法效果良好，展示了MIG技术研究潜力，为该领域未来工作提供了有用指标、工作负载特征和评估技术。

Abstract: NVIDIA MIG (Multi-Instance GPU) allows partitioning a physical GPU into
multiple logical instances with fully-isolated resources, which can be
dynamically reconfigured. This work highlights the untapped potential of MIG
through moldable task scheduling with dynamic reconfigurations. Specifically,
we propose a makespan minimization problem for multi-task execution under MIG
constraints. Our profiling shows that assuming monotonicity in task work with
respect to resources is not viable, as is usual in multicore scheduling.
Relying on a state-of-the-art proposal that does not require such an
assumption, we present FAR, a 3-phase algorithm to solve the problem. Phase 1
of FAR builds on a classical task moldability method, phase 2 combines Longest
Processing Time First and List Scheduling with a novel repartitioning tree
heuristic tailored to MIG constraints, and phase 3 employs local search via
task moves and swaps. FAR schedules tasks in batches offline, concatenating
their schedules on the fly in an improved way that favors resource reuse.
Excluding reconfiguration costs, the List Scheduling proof shows an
approximation factor of 7/4 on the NVIDIA A30 model. We adapt the technique to
the particular constraints of an NVIDIA A100/H100 to obtain an approximation
factor of 2. Including the reconfiguration cost, our real-world experiments
reveal a makespan with respect to the optimum no worse than 1.22x for a
well-known suite of benchmarks, and 1.10x for synthetic inputs inspired by real
kernels. We obtain good experimental results for each batch of tasks, but also
in the concatenation of batches, with large improvements over the
state-of-the-art and proposals without GPU reconfiguration. Beyond the
algorithm, the paper demonstrates the research potential of the MIG technology
and suggests useful metrics, workload characterizations and evaluation
techniques for future work in this field.

</details>


### [29] [DistFlow: A Fully Distributed RL Framework for Scalable and Efficient LLM Post-Training](https://arxiv.org/abs/2507.13833)
*Zhixin Wang,Tianyi Zhou,Liming Liu,Ao Li,Jiarui Hu,Dian Yang,Jinlong Hou,Siyuan Feng,Yuan Cheng,Yuan Qi*

Main category: cs.DC

TL;DR: 介绍新型分布式强化学习框架DistFlow，可突破扩展瓶颈，有高扩展性和效率提升。


<details>
  <summary>Details</summary>
Motivation: 主流强化学习框架在大规模应用时因负载不均衡存在扩展瓶颈，需解决此问题。

Method: 采用多控制器范式，将数据传输和执行任务分配给所有工作节点，消除集中节点，且将资源配置与执行逻辑解耦。

Result: DistFlow实现了出色的线性可扩展性，端到端吞吐量比现有最先进框架提升达7倍。

Conclusion: DistFlow能有效突破强化学习系统的扩展障碍，具有高可扩展性和效率，且为算法实验提供灵活性。

Abstract: Reinforcement learning (RL) has become the pivotal post-training technique
for large language model. Effectively scaling reinforcement learning is now the
key to unlocking advanced reasoning capabilities and ensuring safe,
goal-aligned behavior in the most powerful LLMs. Mainstream frameworks usually
employ a hybrid-controller architecture where a single-controller dispatches
the overall execution logic and manages overall data transfer and the
multi-controller executes distributed computation. For large-scale
reinforcement learning, minor load imbalances can introduce significant
bottlenecks, ultimately constraining the scalability of the system. To address
this limitation, we introduce DistFlow, a novel, fully distributed RL framework
designed to break scaling barrier. We adopt a multi-controller paradigm that
dispatches data transfer and execution tasks to all workers, which eliminates
the centralized node. This allows each worker to operate independently, leading
to near-linear scalability up to thousands of GPUs and dramatic efficiency
gains. Furthermore, our architecture decouples resource configuration from
execution logic, allowing each worker to have a unique execution flow, offering
significant flexibility for rapid and cost-effective algorithmic
experimentation. Extensive experiments show that DistFlow achieves excellent
linear scalability and up to a 7x end-to-end throughput improvement over
state-of-the-art (SOTA) frameworks.

</details>


### [30] [Edge Intelligence with Spiking Neural Networks](https://arxiv.org/abs/2507.14069)
*Shuiguang Deng,Di Yu,Changze Lv,Xin Du,Linshan Jiang,Xiaofan Zhao,Wentao Tong,Xiaoqing Zheng,Weijia Fang,Peng Zhao,Gang Pan,Schahram Dustdar,Albert Y. Zomaya*

Main category: cs.DC

TL;DR: 本文全面综述基于脉冲神经网络的边缘智能（EdgeSNNs），涵盖基础分类、实际考量、评估策略等，为该领域提供参考。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习在云中心范式有延迟、带宽和隐私问题，脑启发计算的SNNs提供低功耗、事件驱动计算，本文旨在考察EdgeSNNs解决边缘场景挑战的潜力。

Method: 对EdgeSNN基础进行系统分类，深入讨论三个实际考量问题，引入双轨基准测试策略。

Result: 介绍EdgeSNN的基础分类、实际考量和评估策略。

Conclusion: 本文弥合脑启发学习与实际边缘部署差距，指出当前进展、挑战和未来方向，是首个关于EdgeSNNs的全面综述。

Abstract: The convergence of artificial intelligence and edge computing has spurred
growing interest in enabling intelligent services directly on
resource-constrained devices. While traditional deep learning models require
significant computational resources and centralized data management, the
resulting latency, bandwidth consumption, and privacy concerns have exposed
critical limitations in cloud-centric paradigms. Brain-inspired computing,
particularly Spiking Neural Networks (SNNs), offers a promising alternative by
emulating biological neuronal dynamics to achieve low-power, event-driven
computation. This survey provides a comprehensive overview of Edge Intelligence
based on SNNs (EdgeSNNs), examining their potential to address the challenges
of on-device learning, inference, and security in edge scenarios. We present a
systematic taxonomy of EdgeSNN foundations, encompassing neuron models,
learning algorithms, and supporting hardware platforms. Three representative
practical considerations of EdgeSNN are discussed in depth: on-device inference
using lightweight SNN models, resource-aware training and updating under
non-stationary data conditions, and secure and privacy-preserving issues.
Furthermore, we highlight the limitations of evaluating EdgeSNNs on
conventional hardware and introduce a dual-track benchmarking strategy to
support fair comparisons and hardware-aware optimization. Through this study,
we aim to bridge the gap between brain-inspired learning and practical edge
deployment, offering insights into current advancements, open challenges, and
future research directions. To the best of our knowledge, this is the first
dedicated and comprehensive survey on EdgeSNNs, providing an essential
reference for researchers and practitioners working at the intersection of
neuromorphic computing and edge intelligence.

</details>


### [31] [Shipwright: Proving liveness of distributed systems with Byzantine participants](https://arxiv.org/abs/2507.14080)
*Derek Leung,Nickolai Zeldovich,Frans Kaashoek*

Main category: cs.DC

TL;DR: 论文提出Shipwright验证框架确保含恶意参与者的分布式系统的正确性和活性，并用其实现并验证PBFT部分协议。


<details>
  <summary>Details</summary>
Motivation: 去中心化系统如PBFT确保活性至关重要，但此前无法对其可执行实现的活性进行验证。

Method: 引入三种技术支持对含恶意参与者的去中心化场景进行形式化推理、模块化分解系统和证明、对消息中的加密签名进行合理推理。

Result: 用Shipwright实现并验证PBFT单日志条目协议原型，转换为Go可执行代码，实验证明其在常见和故障场景下的操作和活性。

Conclusion: Shipwright框架能有效验证含恶意参与者的分布式系统的正确性和活性。

Abstract: Ensuring liveness in a decentralized system, such as PBFT, is critical,
because there may not be any single administrator that can restart the system
if it encounters a liveness bug. At the same time, liveness is challenging to
achieve because any single participant could be malicious, and yet the overall
system must make forward progress. While verification is a promising approach
for ensuring the absence of bugs, no prior work has been able to verify
liveness for an executable implementation of PBFT.
  Shipwright is a verification framework for proving correctness and liveness
of distributed systems where some participants might be malicious. Shipwright
introduces three techniques that enable formal reasoning about decentralized
settings with malicious participants, allow developers to decompose their
system and proof in a modular fashion into sub-protocols and sub-proofs, and
support sound reasoning about cryptographic signatures that may be embedded in
messages. We used Shipwright to implement and verify an initial prototype of
agreement on a single log entry in PBFT (with a few limitations) and translate
it to an executable implementation in Go. We experimentally demonstrate its
operation and liveness both in the common case and in several failure
scenarios.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [32] [Faster Multi-Source Reachability and Approximate Distances via Shortcuts, Hopsets and Matrix Multiplication](https://arxiv.org/abs/2507.13470)
*Michael Elkin,Chhaya Trehan*

Main category: cs.DS

TL;DR: 本文研究S×V可达性问题，提出新的集中式算法和并行算法，改进已有结果并将其扩展到近似距离计算。


<details>
  <summary>Details</summary>
Motivation: 改进S×V可达性问题的算法复杂度，优化已有算法结果。

Method: 利用Kogan和Parter的捷径构造开发集中式算法；改进、扩展和推广Cohen的并行算法结果。

Result: 集中式算法运行时间为Õ(n^(1 + 2/3 ω(σ)))，在一定参数范围内优于已知界；并行算法在广泛参数范围内工作复杂度低于Cohen的算法，还将算法推广到树宽至多为n^ρ的图等其他图族，并扩展到近似距离计算。

Conclusion: 新算法在S×V可达性问题上取得更好的复杂度，且可扩展到近似距离计算。

Abstract: Given an $n$-vertex $m$-edge digraph $G = (V,E)$ and a subset $S \subseteq V$
of $|S| = n^{\sigma}$ (for some $0 \le \sigma \le 1$) designated sources, the
$S \times V$ reachability problem is to compute the sets $\mathcal V_s$ of
vertices reachable from $s$, for every $s \in S$. Naive centralized algorithms
run BFS/DFS from each source in $O(m \cdot n^{\sigma})$ time or compute $G$'s
transitive closure in $\hat O(n^{\omega})$ time, where $\omega \le
2.371552\ldots$ is the matrix multiplication exponent. Thus, the best known
bound is $\hat O(n^{\min \{ 2 + \sigma, \omega\}})$. Leveraging shortcut
constructions by Kogan and Parter [SODA 2022, ICALP 2022], we develop a
centralized algorithm with running time $\hat O(n^{1 + \frac{2}{3}
\omega(\sigma)})$, where $\omega(\sigma)$ is the rectangular matrix
multiplication exponent. Using current estimates on $\omega(\sigma)$, our
exponent improves upon $\min \{2 + \sigma, \omega \}$ for $\tilde \sigma \leq
\sigma \leq 0.53$, where $1/3 < \tilde \sigma < 0.3336$ is a universal
constant.
  In a classical result, Cohen [Journal of Algorithms, 1996] devised parallel
algorithms for $S \times V$ reachability on graphs admitting balanced recursive
separators of size $n^{\rho}$ for $\rho < 1$, requiring polylogarithmic time
and work $n^{\max \{\omega \rho, 2\rho + \sigma \} + o(1)}$. We significantly
improve, extend, and generalize Cohen's result. First, our parallel algorithm
for graphs with small recursive separators has lower work complexity than
Cohen's in boraod paramater ranges. Second, we generalize our algorithm to
graphs of treewidth at most $n^{\rho}$ ($\rho < 1$) and provide a centralized
algorithm that outperforms existing bounds for $S \times V$ reachability on
such graphs. We also do this for some other graph familes with small
separators. Finally, we extend these results to $(1 + \epsilon)$-approximate
distance computation.

</details>


### [33] [Strassen $2\times2$ Matrix Multiplication from a 3-dimensional Volume Form](https://arxiv.org/abs/2507.13510)
*Benoit Jacob*

Main category: cs.DS

TL;DR: Strassen 2×2矩阵乘法算法源于2×2矩阵模单位矩阵倍数的3维商空间上的体积形式。


<details>
  <summary>Details</summary>
Motivation: 未提及

Method: 未提及

Result: 指出Strassen 2×2矩阵乘法算法的来源

Conclusion: 未提及

Abstract: The Strassen $2\times2$ matrix multiplication algorithm arises from the
volume form on the 3-dimensional quotient space of the $2\times 2$ matrices by
the multiples of identity.

</details>


### [34] [Combinatorics of Palindromes](https://arxiv.org/abs/2507.13671)
*Michael Itzhaki*

Main category: cs.DS

TL;DR: 研究Manacher数组的结构和重建复杂性，给出组合下界，引入图论框架，分析重建算法并解决公开问题。


<details>
  <summary>Details</summary>
Motivation: 深入了解Manacher数组的结构和重建复杂性，解决相关公开问题。

Method: 建立组合下界，引入图论框架，分析I等人的重建算法。

Result: 证明n + 1个基因的有根串联重复树数量超过长度为n的不同Manacher数组数量；图的每个适当顶点着色产生与数组一致的字符串；算法能达到全局最小字母表大小，最多使用log₂(n - 1) + 2个不同符号，可在可能时适应任意字母表进行重建。

Conclusion: 推进了对Manacher数组的组合理解，为结构约束下的字符串重建开辟新方向。

Abstract: We investigate the structure and reconstruction complexity of Manacher
arrays. First, we establish a combinatorial lower bound, proving that the
number of rooted tandem repeat trees with $n+1$ genes exceeds the number of
distinct Manacher arrays of length $n$. Second, we introduce a graph-theoretic
framework that associates a graph to each Manacher array, where every proper
vertex coloring yields a string consistent with the array. Finally, we analyze
a reconstruction algorithm by I et al. (SPIRE 2010), showing that it
simultaneously achieves a globally minimal alphabet size, uses at most
$\log_2(n{-}1) + 2$ distinct symbols, and can be adapted to produce
reconstructions over arbitrary alphabets when possible. Our results also
resolve an open problem posed by the original authors. Together, these findings
advance the combinatorial understanding of Manacher arrays and open new
directions for string reconstruction under structural constraints.

</details>


### [35] [Tight Bounds for Answering Adaptively Chosen Concentrated Queries](https://arxiv.org/abs/2507.13700)
*Emma Rapoport,Edith Cohen,Uri Stemmer*

Main category: cs.DS

TL;DR: 本文证明集中查询框架下当前公式存在效用差距，并给出匹配不可能结果的简化算法。


<details>
  <summary>Details</summary>
Motivation: 现有自适应数据分析大多假设数据集中样本独立，考虑相关性时问题棘手，集中查询框架在自适应设置下仍具挑战，已知算法支持的查询数远少于独立情况。

Method: 在对算法有自然条件假设下，对集中查询框架的当前公式进行分析，并给出已知最佳算法的简化版本。

Result: 证明了在当前集中查询框架公式下效用差距是固有的，给出了匹配不可能结果的简化算法。

Conclusion: 当前集中查询框架在现有公式下存在效用差距，简化算法能匹配该不可能结果。

Abstract: Most work on adaptive data analysis assumes that samples in the dataset are
independent. When correlations are allowed, even the non-adaptive setting can
become intractable, unless some structural constraints are imposed. To address
this, Bassily and Freund [2016] introduced the elegant framework of
concentrated queries, which requires the analyst to restrict itself to queries
that are concentrated around their expected value. While this assumption makes
the problem trivial in the non-adaptive setting, in the adaptive setting it
remains quite challenging. In fact, all known algorithms in this framework
support significantly fewer queries than in the independent case: At most
$O(n)$ queries for a sample of size $n$, compared to $O(n^2)$ in the
independent setting.
  In this work, we prove that this utility gap is inherent under the current
formulation of the concentrated queries framework, assuming some natural
conditions on the algorithm. Additionally, we present a simplified version of
the best-known algorithms that match our impossibility result.

</details>


### [36] [Improved girth approximation in weighted undirected graphs](https://arxiv.org/abs/2507.13869)
*Avi Kadria,Liam Roditty,Aaron Sidford,Virginia Vassilevska Williams,Uri Zwick*

Main category: cs.DS

TL;DR: 提出一个算法，在O(kn^{1+1/k}log{n} + m(k+log{n}))期望时间内找到长度至多为4k/3 * g的环，性能优于先前算法。


<details>
  <summary>Details</summary>
Motivation: 改进在加权无向图中寻找特定长度环的算法性能。

Method: 设计一个新算法，在给定输入整数k的情况下寻找环。

Result: 新算法在期望时间O(kn^{1+1/k}log{n} + m(k+log{n}))内找到长度至多为4k/3 * g的环，性能优于先前算法。

Conclusion: 新算法在加权图中寻找特定长度环的问题上有更好的时间复杂度，提升了算法性能。

Abstract: Let $G = (V,E,\ell)$ be a $n$-node $m$-edge weighted undirected graph, where
$\ell: E \rightarrow (0,\infty)$ is a real \emph{length} function defined on
its edges, and let $g$ denote the girth of $G$, i.e., the length of its
shortest cycle. We present an algorithm that, for any input, integer $k \geq
1$, in $O(kn^{1+1/k}\log{n} + m(k+\log{n}))$ expected time finds a cycle of
length at most $\frac{4k}{3}g$. This algorithm nearly matches a
$O(n^{1+1/k}\log{n})$-time algorithm of \cite{KadriaRSWZ22} which applied to
unweighted graphs of girth $3$. For weighted graphs, this result also improves
upon the previous state-of-the-art algorithm that in $O((n^{1+1/k}\log n+m)\log
(nM))$ time, where $\ell: E \rightarrow [1, M]$ is an integral length function,
finds a cycle of length at most $2kg$~\cite{KadriaRSWZ22}. For $k=1$ this
result improves upon the result of Roditty and Tov~\cite{RodittyT13}.

</details>


### [37] [Quantum Pattern Matching with Wildcards](https://arxiv.org/abs/2507.13885)
*Masoud Seddighin,Saeed Seddighin*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Pattern matching is one of the fundamental problems in Computer Science. Both
the classic version of the problem as well as the more sophisticated version
where wildcards can also appear in the input can be solved in almost linear
time $\tilde O(n)$ using the KMP algorithm and Fast Fourier Transform,
respectively. In 2000, Ramesh and Vinay~\cite{ramesh2003string} give a quantum
algorithm that solves classic pattern matching in sublinear time and asked
whether the wildcard problem can also be solved in sublinear time? In this
work, we give a quantum algorithm for pattern matching with wildcards that runs
in time $\tilde O(\sqrt{n}\sqrt{k})$ when the number of wildcards is bounded by
$k$ for $k \geq \sqrt{n}$. This leads to an algorithm that runs in sublinear
time as long as the number of wildcards is sublinear.

</details>


### [38] [Optimal antimatroid sorting](https://arxiv.org/abs/2507.13994)
*Benjamin Aram Berendsohn*

Main category: cs.DS

TL;DR: 本文研究受限排序问题，表明拓扑堆排序的简单推广可用于更广泛的受限排序问题，并得到几种受限排序问题的最优算法。


<details>
  <summary>Details</summary>
Motivation: 研究经典比较排序问题的受限版本，已知可能的全序集合T，探索更广泛适用的算法。

Method: 对拓扑堆排序进行简单推广，将其应用于T对应给定反拟阵的受限排序问题。

Result: 获得了几种受限排序问题（如受单调优先公式、弦图完美消除序、连通有根图顶点搜索序限制）的最优算法。

Conclusion: 拓扑堆排序的简单推广能用于更广泛的受限排序问题，可得到对应问题的最优算法。

Abstract: The classical comparison-based sorting problem asks us to find the underlying
total order of a given set of elements, where we can only access the elements
via comparisons. In this paper, we study a restricted version, where, as a
hint, a set $T$ of possible total orders is given, usually in some compressed
form.
  Recently, an algorithm called topological heapsort with optimal running time
was found for the case where $T$ is the set of topological orderings of a given
directed acyclic graph, or, equivalently, $T$ is the set of linear extensions
of a given partial order [Haeupler et al. 2024]. We show that a simple
generalization of topological heapsort is applicable to a much broader class of
restricted sorting problems, where $T$ corresponds to a given antimatroid.
  As a consequence, we obtain optimal algorithms for the following restricted
sorting problems, where the allowed total orders are restricted by: a given set
of monotone precedence formulas; the perfect elimination orders of a given
chordal graph; or the possible vertex search orders of a given connected rooted
graph.

</details>


### [39] [Sparse Navigable Graphs for Nearest Neighbor Search: Algorithms and Hardness](https://arxiv.org/abs/2507.14060)
*Sanjeev Khanna,Ashwin Padaki,Erik Waingarten*

Main category: cs.DS

TL;DR: 本文研究构建稀疏α - 导航图的近似算法和计算障碍，给出负结果、近似算法及查询复杂度下界。


<details>
  <summary>Details</summary>
Motivation: 研究构建稀疏α - 导航图的近似算法和计算障碍，该图是基于图的最近邻搜索的核心基础。

Method: 证明稀疏导航图问题与集合覆盖问题的近似等价性，基于此设计近似算法，同时给出查询复杂度下界分析。

Result: 指出DiskANN的慢预处理版本稀疏性可能比最优解大Ω(n)倍；得到O(n³)时间的(ln n + 1)近似算法，证明达到o(ln n)近似的NP - 难；开发更快的O(ln n)近似算法；给出查询复杂度下界。

Conclusion: 在OPT = Õ(n)的情况下，Õ(n · OPT)时间算法接近最优。

Abstract: We initiate the study of approximation algorithms and computational barriers
for constructing sparse $\alpha$-navigable graphs [IX23, DGM+24], a core
primitive underlying recent advances in graph-based nearest neighbor search.
Given an $n$-point dataset $P$ with an associated metric $\mathsf{d}$ and a
parameter $\alpha \geq 1$, the goal is to efficiently build the sparsest graph
$G=(P, E)$ that is $\alpha$-navigable: for every distinct $s, t \in P$, there
exists an edge $(s, u) \in E$ with $\mathsf{d}(u, t) < \mathsf{d}(s,
t)/\alpha$. We consider two natural sparsity objectives: minimizing the maximum
out-degree and minimizing the total size.
  We first show a strong negative result: the slow-preprocessing version of
DiskANN (analyzed in [IX23] for low-doubling metrics) can yield solutions whose
sparsity is $\widetilde{\Omega}(n)$ times larger than optimal, even on
Euclidean instances. We then show a tight approximation-preserving equivalence
between the Sparsest Navigable Graph problem and the classic Set Cover problem,
obtaining an $O(n^3)$-time $(\ln n + 1)$-approximation algorithm, as well as
establishing NP-hardness of achieving an $o(\ln n)$-approximation. Building on
this equivalence, we develop faster $O(\ln n)$-approximation algorithms. The
first runs in $\widetilde{O}(n \cdot \mathrm{OPT})$ time and is thus much
faster when the optimal solution is sparse. The second, based on fast matrix
multiplication, is a bicriteria algorithm that computes an $O(\ln
n)$-approximation to the sparsest $2\alpha$-navigable graph, running in
$\widetilde{O}(n^{\omega})$ time.
  Finally, we complement our upper bounds with a query complexity lower bound,
showing that any $o(n)$-approximation requires examining $\Omega(n^2)$
distances. This result shows that in the regime where $\mathrm{OPT} =
\widetilde{O}(n)$, our $\widetilde{O}(n \cdot \mathrm{OPT})$-time algorithm is
essentially best possible.

</details>


### [40] [An Efficient Massively Parallel Constant-Factor Approximation Algorithm for the $k$-Means Problem](https://arxiv.org/abs/2507.14089)
*Vincent Cohen-Addad,Fabian Kuhn,Zahra Parsaeian*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we present an efficient massively parallel approximation
algorithm for the $k$-means problem. Specifically, we provide an MPC algorithm
that computes a constant-factor approximation to an arbitrary $k$-means
instance in $O(\log\log n \cdot \log\log\log n)$ rounds. The algorithm uses
$O(n^\sigma)$ bits of memory per machine, where $\sigma > 0$ is a constant that
can be made arbitrarily small. The global memory usage is
$O(n^{1+\varepsilon})$ bits for an arbitrarily small constant $\varepsilon >
0$, and is thus only slightly superlinear. Recently, Czumaj, Gao, Jiang,
Krauthgamer, and Vesel\'{y} showed that a constant-factor bicriteria
approximation can be computed in $O(1)$ rounds in the MPC model. However, our
algorithm is the first constant-factor approximation for the general $k$-means
problem that runs in $o(\log n)$ rounds in the MPC model.
  Our approach builds upon the foundational framework of Jain and Vazirani. The
core component of our algorithm is a constant-factor approximation for the
related facility location problem. While such an approximation was already
achieved in constant time in the work of Czumaj et al.\ mentioned above, our
version additionally satisfies the so-called Lagrangian Multiplier Preserving
(LMP) property. This property enables the transformation of a facility location
approximation into a comparably good $k$-means approximation.

</details>


### [41] [Weighted Matching in a Poly-Streaming Model](https://arxiv.org/abs/2507.14114)
*Ahammed Ullah,S. M. Ferdous,Alex Pothen*

Main category: cs.DS

TL;DR: 本文提出多流模型，设计单遍算法近似最大权重匹配问题，分析其在共享内存并行和分层架构下的性能，实验表明算法有显著加速和资源节省。


<details>
  <summary>Details</summary>
Motivation: 提出一种通用的流计算模型，解决最大权重匹配问题并分析性能。

Method: 提出多流模型，设计单遍算法，分析其在共享内存并行和分层架构下的性能。

Result: 算法在共享内存系统上实现显著加速，匹配权重超理论保证，在大测试图上大幅减少运行时间和内存使用。

Conclusion: 多流模型下的算法在解决最大权重匹配问题上表现良好，有实用价值。

Abstract: We introduce the poly-streaming model, a generalization of streaming models
of computation in which $k$ processors process $k$ data streams containing a
total of $N$ items. The algorithm is allowed $O\left(f(k)\cdot M_1\right)$
space, where $M_1$ is either $o\left(N\right)$ or the space bound for a
sequential streaming algorithm. Processors may communicate as needed.
Algorithms are assessed by the number of passes, per-item processing time,
total runtime, space usage, communication cost, and solution quality.
  We design a single-pass algorithm in this model for approximating the maximum
weight matching (MWM) problem. Given $k$ edge streams and a parameter
$\varepsilon > 0$, the algorithm computes a
$\left(2+\epsilon\right)$-approximate MWM. We analyze its performance in a
shared-memory parallel setting: for any constant $\varepsilon > 0$, it runs in
time $\widetilde{O}\left(L_{\max}+n\right)$, where $n$ is the number of
vertices and $L_{\max}$ is the maximum stream length. It supports
$O\left(1\right)$ per-edge processing time using $\widetilde{O}\left(k\cdot
n\right)$ space. We further generalize the design to hierarchical
architectures, in which $k$ processors are partitioned into $r$ groups, each
with its own shared local memory. The total intergroup communication is
$\widetilde{O}\left(r \cdot n\right)$ bits, while all other performance
guarantees are preserved.
  We evaluate the algorithm on a shared-memory system using graphs with
trillions of edges. It achieves substantial speedups as $k$ increases and
produces matchings with weights significantly exceeding the theoretical
guarantee. On our largest test graph, it reduces runtime by nearly two orders
of magnitude and memory usage by five orders of magnitude compared to an
offline algorithm.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [42] [Resource-Splitting Games with Tullock-Based Lossy Contests](https://arxiv.org/abs/2507.13853)
*Marko Maljkovic,Gustav Nilsson,Nikolas Geroliminis*

Main category: cs.GT

TL;DR: 本文提出多阶段资源分配博弈框架，探讨策略、存在性与唯一性，给出计算方法，证明可推广现有模型，通过案例验证实用性。


<details>
  <summary>Details</summary>
Motivation: 为了建模现实中盈利依赖供需平衡且高资源投入有高回报的场景。

Method: 提出包含玩家参与不足导致利润损失概念的框架，探索集中式和纳什均衡策略，建立存在性和唯一性条件，提供迭代半去中心化方法计算纳什均衡，给出半解析方法计算特定模型的唯一纳什均衡。

Result: 该框架可推广多个现有模型，通过智能出行的数值案例验证了模型的实用性。

Conclusion: 所提出的多阶段资源分配博弈框架具有实际相关性和适用性。

Abstract: This paper introduces a novel class of multi-stage resource allocation games
that model real-world scenarios in which profitability depends on the balance
between supply and demand, and where higher resource investment leads to
greater returns. Our proposed framework, which incorporates the notion of
profit loss due to insufficient player participation, gives rise to a
Tullock-like functional form of the stage payoff structure when weighted fair
proportional resource allocation is applied. We explore both centralized and
Nash equilibrium strategies, establish sufficient conditions for their
existence and uniqueness, and provide an iterative, semi-decentralized method
to compute the Nash equilibrium in games with arbitrarily many players.
Additionally, we demonstrate that the framework generalizes instances of
several existing models, including Receding Horizon and Blotto games, and
present a semi-analytical method for computing the unique Nash equilibrium
within the Blotto setup. Our findings are validated through a numerical case
study in smart mobility, highlighting the practical relevance and applicability
of the proposed model.

</details>


### [43] [Online MMS Allocation for Chores](https://arxiv.org/abs/2507.14039)
*Jiaxin Song,Biaoshuai Tao,Wenqian Wang,Yuhao Zhang*

Main category: cs.GT

TL;DR: 研究在线环境下不可分割杂务公平分配问题，证明无法保证(n - ε)-MMS分配，给出通用在线算法保证min{n, O(k), O(log D)}-MMS分配，在特定情况有更好结果。


<details>
  <summary>Details</summary>
Motivation: 此前在线不可分割杂务公平分配研究在一般情况下仅得到平凡的n - MMS保证，存在研究缺口，需进一步探索算法保证。

Method: 先证明对于固定n和ε，无算法能保证(n - ε)-MMS分配；再提出通用在线算法并分析其MMS分配保证。

Result: 证明无法保证(n - ε)-MMS分配；通用算法保证min{n, O(k), O(log D)}-MMS分配，在k为常数时可实现O(1)-MMS分配，个性化双值情况保证约3.7 - MMS分配。

Conclusion: 在线不可分割杂务公平分配存在负面不可能结果，但通用算法在多种场景有合理的MMS分配保证。

Abstract: We study the problem of fair division of indivisible chores among $n$ agents
in an online setting, where items arrive sequentially and must be allocated
irrevocably upon arrival. The goal is to produce an $\alpha$-MMS allocation at
the end. Several recent works have investigated this model, but have only
succeeded in obtaining non-trivial algorithms under restrictive assumptions,
such as the two-agent bi-valued special case (Wang and Wei, 2025), or by
assuming knowledge of the total disutility of each agent (Zhou, Bai, and Wu,
2023). For the general case, the trivial $n$-MMS guarantee remains the best
known, while the strongest lower bound is still only $2$.
  We close this gap on the negative side by proving that for any fixed $n$ and
$\varepsilon$, no algorithm can guarantee an $(n - \varepsilon)$-MMS
allocation. Notably, this lower bound holds precisely for every $n$, without
hiding constants in big-$O$ notation, thereby exactly matching the trivial
upper bound.
  Despite this strong impossibility result, we also present positive results.
We provide an online algorithm that applies in the general case, guaranteeing a
$\min\{n, O(k), O(\log D)\}$-MMS allocation, where $k$ is the maximum number of
distinct disutilities across all agents and $D$ is the maximum ratio between
the largest and smallest disutilities for any agent. This bound is reasonable
across a broad range of scenarios and, for example, implies that we can achieve
an $O(1)$-MMS allocation whenever $k$ is constant. Moreover, to optimize the
constant in the important personalized bi-valued case, we show that if each
agent has at most two distinct disutilities, our algorithm guarantees a $(2 +
\sqrt{3}) \approx 3.7$-MMS allocation.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [44] [DyG-RAG: Dynamic Graph Retrieval-Augmented Generation with Event-Centric Reasoning](https://arxiv.org/abs/2507.13396)
*Qingyun Sun,Jiaqi Yuan,Shan He,Xiao Guan,Haonan Yuan,Xingcheng Fu,Jianxin Li,Philip S. Yu*

Main category: cs.IR

TL;DR: 提出DyG - RAG框架解决现有Graph RAG方法在时间推理上的不足，实验证明其能提升时间推理问题准确性和召回率。


<details>
  <summary>Details</summary>
Motivation: 现有Graph RAG方法在时间推理方面存在困难，无法对现实世界事件的演化结构和顺序进行建模。

Method: 提出动态事件单元（DEUs）消除时间歧义；构建事件图捕捉事件间的时间和因果依赖；引入事件时间线检索管道和时间思维链策略确保生成的时间一致性。

Result: 在时间问答基准测试中，显著提高了三种典型时间推理问题的准确性和召回率。

Conclusion: DyG - RAG为更可靠和具有时间感知的生成奠定了基础。

Abstract: Graph Retrieval-Augmented Generation has emerged as a powerful paradigm for
grounding large language models with external structured knowledge. However,
existing Graph RAG methods struggle with temporal reasoning, due to their
inability to model the evolving structure and order of real-world events. In
this work, we introduce DyG-RAG, a novel event-centric dynamic graph
retrieval-augmented generation framework designed to capture and reason over
temporal knowledge embedded in unstructured text. To eliminate temporal
ambiguity in traditional retrieval units, DyG-RAG proposes Dynamic Event Units
(DEUs) that explicitly encode both semantic content and precise temporal
anchors, enabling accurate and interpretable time-aware retrieval. To capture
temporal and causal dependencies across events, DyG-RAG constructs an event
graph by linking DEUs that share entities and occur close in time, supporting
efficient and meaningful multi-hop reasoning. To ensure temporally consistent
generation, DyG-RAG introduces an event timeline retrieval pipeline that
retrieves event sequences via time-aware traversal, and proposes a Time
Chain-of-Thought strategy for temporally grounded answer generation. This
unified pipeline enables DyG-RAG to retrieve coherent, temporally ordered event
sequences and to answer complex, time-sensitive queries that standard RAG
systems cannot resolve. Extensive experiments on temporal QA benchmarks
demonstrate that DyG-RAG significantly improves the accuracy and recall of
three typical types of temporal reasoning questions, paving the way for more
faithful and temporal-aware generation. DyG-RAG is available at
https://github.com/RingBDStack/DyG-RAG.

</details>


### [45] [Revisiting Prompt Engineering: A Comprehensive Evaluation for LLM-based Personalized Recommendation](https://arxiv.org/abs/2507.13525)
*Genki Kusano,Kosuke Akimoto,Kunihiro Takeoka*

Main category: cs.IR

TL;DR: 文章聚焦单用户场景下大语言模型推荐任务的提示工程，对比23种提示类型，给出不同性能模型的有效提示及选择建议。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在推荐任务有优势，单用户场景下提示工程对控制输出很重要。

Method: 对23种提示类型在8个公开数据集和12个大语言模型上进行大规模对比，用统计测试和线性混合效应模型评估准确性和推理成本。

Result: 成本效益高的模型，重述指令、考虑背景知识和简化推理过程的提示有效；高性能模型简单提示表现更好且成本低，常用自然语言处理提示风格准确率低。

Conclusion: 根据准确性和成本的平衡，为提示和大语言模型的选择提供实用建议。

Abstract: Large language models (LLMs) can perform recommendation tasks by taking
prompts written in natural language as input. Compared to traditional methods
such as collaborative filtering, LLM-based recommendation offers advantages in
handling cold-start, cross-domain, and zero-shot scenarios, as well as
supporting flexible input formats and generating explanations of user behavior.
In this paper, we focus on a single-user setting, where no information from
other users is used. This setting is practical for privacy-sensitive or
data-limited applications. In such cases, prompt engineering becomes especially
important for controlling the output generated by the LLM. We conduct a
large-scale comparison of 23 prompt types across 8 public datasets and 12 LLMs.
We use statistical tests and linear mixed-effects models to evaluate both
accuracy and inference cost. Our results show that for cost-efficient LLMs,
three types of prompts are especially effective: those that rephrase
instructions, consider background knowledge, and make the reasoning process
easier to follow. For high-performance LLMs, simple prompts often outperform
more complex ones while reducing cost. In contrast, commonly used prompting
styles in natural language processing, such as step-by-step reasoning, or the
use of reasoning models often lead to lower accuracy. Based on these findings,
we provide practical suggestions for selecting prompts and LLMs depending on
the required balance between accuracy and cost.

</details>


### [46] [IP2: Entity-Guided Interest Probing for Personalized News Recommendation](https://arxiv.org/abs/2507.13622)
*Youlin Wu,Yuanyuan Sun,Xiaokun Zhang,Haoxi Zhan,Bo Xu,Liang Yang,Hongfei Lin*

Main category: cs.IR

TL;DR: 本文提出IP2方法用于新闻推荐，能从新闻内和新闻间层面挖掘实体引导的阅读兴趣，实验表明该方法达SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前方法忽视新闻实体在推荐中的独特作用，而行为科学研究表明新闻阅读分扫描、读标题和点击三步，实体在不同阶段有不同作用。

Method: 提出IP2方法，在新闻内用Transformer实体编码器聚合标题实体为签名实体，进行签名实体 - 标题对比预训练；在新闻间用双塔用户编码器捕捉阅读兴趣，采用跨塔注意力链接校准标题阅读兴趣。

Result: 在两个真实数据集上的大量实验表明，IP2在新闻推荐中达到了当前最优性能。

Conclusion: IP2方法能有效挖掘实体引导的阅读兴趣，提升新闻推荐效果。

Abstract: News recommender systems aim to provide personalized news reading experiences
for users based on their reading history. Behavioral science studies suggest
that screen-based news reading contains three successive steps: scanning, title
reading, and then clicking. Adhering to these steps, we find that intra-news
entity interest dominates the scanning stage, while the inter-news entity
interest guides title reading and influences click decisions. Unfortunately,
current methods overlook the unique utility of entities in news recommendation.
To this end, we propose a novel method called IP2 to probe entity-guided
reading interest at both intra- and inter-news levels. At the intra-news level,
a Transformer-based entity encoder is devised to aggregate mentioned entities
in the news title into one signature entity. Then, a signature entity-title
contrastive pre-training is adopted to initialize entities with proper meanings
using the news story context, which in the meantime facilitates us to probe for
intra-news entity interest. As for the inter-news level, a dual tower user
encoder is presented to capture inter-news reading interest from both the title
meaning and entity sides. In addition to highlighting the contribution of
inter-news entity guidance, a cross-tower attention link is adopted to
calibrate title reading interest using inter-news entity interest, thus further
aligning with real-world behavior. Extensive experiments on two real-world
datasets demonstrate that our IP2 achieves state-of-the-art performance in news
recommendation.

</details>


### [47] [Point of Interest Recommendation: Pitfalls and Viable Solutions](https://arxiv.org/abs/2507.13725)
*Alejandro Bellogín,Linus W. Dietz,Francesco Ricci,Pablo Sánchez*

Main category: cs.IR

TL;DR: 本文讨论POI推荐问题现状与挑战，评估研究现状并指出不足，还提出未来研究议程。


<details>
  <summary>Details</summary>
Motivation: POI推荐虽有众多研究，但仍有基本问题未解决，阻碍实际应用，需探讨现状与挑战。

Method: 对POI推荐研究现状进行评估，从数据集、算法和评估方法三方面找出关键不足；基于这些问题提出结构化研究议程。

Result: 指出POI推荐研究存在缺乏标准基准数据集、问题定义和模型设计假设存在缺陷、用户行为和系统性能偏差处理不足等问题。

Conclusion: 提出未来研究在多利益相关者设计、上下文感知、数据收集、可信度、新颖交互和实际评估等方面的重要方向。

Abstract: Point of interest (POI) recommendation can play a pivotal role in enriching
tourists' experiences by suggesting context-dependent and preference-matching
locations and activities, such as restaurants, landmarks, itineraries, and
cultural attractions. Unlike some more common recommendation domains (e.g.,
music and video), POI recommendation is inherently high-stakes: users invest
significant time, money, and effort to search, choose, and consume these
suggested POIs. Despite the numerous research works in the area, several
fundamental issues remain unresolved, hindering the real-world applicability of
the proposed approaches. In this paper, we discuss the current status of the
POI recommendation problem and the main challenges we have identified. The
first contribution of this paper is a critical assessment of the current state
of POI recommendation research and the identification of key shortcomings
across three main dimensions: datasets, algorithms, and evaluation
methodologies. We highlight persistent issues such as the lack of standardized
benchmark datasets, flawed assumptions in the problem definition and model
design, and inadequate treatment of biases in the user behavior and system
performance. The second contribution is a structured research agenda that,
starting from the identified issues, introduces important directions for future
work related to multistakeholder design, context awareness, data collection,
trustworthiness, novel interactions, and real-world evaluation.

</details>


### [48] [RAG-based Architectures for Drug Side Effect Retrieval in LLMs](https://arxiv.org/abs/2507.13822)
*Shad Nygren,Pinar Avci,Andre Daniels,Reza Rassol,Afshin Beheshti,Diego Galeano*

Main category: cs.IR

TL;DR: 提出RAG和GraphRAG架构集成药物副作用知识到Llama 3 8B模型，GraphRAG在药物副作用检索中达近完美准确率，推动了大语言模型在药物警戒应用。


<details>
  <summary>Details</summary>
Motivation: 药物副作用是全球健康问题，大语言模型在药物警戒领域因固有局限缺乏可靠性，需改进方法。

Method: 提出Retrieval-Augmented Generation (RAG)和GraphRAG两种架构，将药物副作用知识集成到Llama 3 8B语言模型。

Result: 在19,520个药物副作用关联的评估中，GraphRAG在药物副作用检索中实现近完美准确率。

Conclusion: 该框架是利用大语言模型进行药物警戒应用的重大进步，提供了准确且可扩展的解决方案。

Abstract: Drug side effects are a major global health concern, necessitating advanced
methods for their accurate detection and analysis. While Large Language Models
(LLMs) offer promising conversational interfaces, their inherent limitations,
including reliance on black-box training data, susceptibility to
hallucinations, and lack of domain-specific knowledge, hinder their reliability
in specialized fields like pharmacovigilance. To address this gap, we propose
two architectures: Retrieval-Augmented Generation (RAG) and GraphRAG, which
integrate comprehensive drug side effect knowledge into a Llama 3 8B language
model. Through extensive evaluations on 19,520 drug side effect associations
(covering 976 drugs and 3,851 side effect terms), our results demonstrate that
GraphRAG achieves near-perfect accuracy in drug side effect retrieval. This
framework offers a highly accurate and scalable solution, signifying a
significant advancement in leveraging LLMs for critical pharmacovigilance
applications.

</details>


### [49] [SPARQL Query Generation with LLMs: Measuring the Impact of Training Data Memorization and Knowledge Injection](https://arxiv.org/abs/2507.13859)
*Aleksandr Gashkov,Aleksandr Perevalov,Maria Eltsova,Andreas Both*

Main category: cs.IR

TL;DR: 本文介绍评估大语言模型（LLMs）质量的新方法，可估计训练数据对问答质量的影响，方法便携且适用于任何知识图谱问答系统或大语言模型。


<details>
  <summary>Details</summary>
Motivation: 自然语言用户界面软件很重要，在知识图谱问答系统中，利用大语言模型提升问答质量有很大改进空间，但研究者无法控制训练数据，需评估大语言模型质量。

Method: 在零样本SPARQL生成、知识注入、“匿名”知识注入等不同条件下，从自然语言问题生成SPARQL查询来评估大语言模型质量。

Result: 能够估计训练数据对大语言模型提升问答质量的影响。

Conclusion: 所开发方法便携、鲁棒，支持任何知识图谱，可用于了解大语言模型的实际能力。

Abstract: Nowadays, the importance of software with natural-language user interfaces
cannot be underestimated. In particular, in Question Answering (QA) systems,
generating a SPARQL query for a given natural-language question (often named
Query Building) from the information retrieved from the same question is the
central task of QA systems working over Knowledge Graphs (KGQA). Due to the
rise of Large Language Models (LLMs), they are considered a well-suited method
to increase the quality of the question-answering functionality, as there is
still a lot of room for improvement, aiming for enhanced quality and
trustworthiness. However, LLMs are trained on web data, where researchers have
no control over whether the benchmark or the knowledge graph was already
included in the training data. In this paper, we introduce a novel method that
evaluates the quality of LLMs by generating a SPARQL query from a
natural-language question under various conditions: (1) zero-shot SPARQL
generation, (2) with knowledge injection, and (3) with "anonymized" knowledge
injection. This enables us, for the first time, to estimate the influence of
the training data on the QA quality improved by LLMs. Ultimately, this will
help to identify how portable a method is or whether good results might mostly
be achieved because a benchmark was already included in the training data (cf.
LLM memorization). The developed method is portable, robust, and supports any
knowledge graph; therefore, it could be easily applied to any KGQA or LLM,
s.t., generating consistent insights into the actual LLM capabilities is
possible.

</details>


### [50] [PARK: Personalized academic retrieval with knowledge-graphs](https://arxiv.org/abs/2507.13910)
*Pranav Kasela,Gabriella Pasi,Raffaele Perego*

Main category: cs.IR

TL;DR: 本文提出两步法用于个性化学术搜索，在多个领域评估中表现良好，凸显知识图谱用户模型提升检索效果的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有个性化学术搜索模型难以充分捕捉用户学术兴趣，且引用图在个性化学术搜索中的应用未充分挖掘。

Method: 先训练用于检索的神经语言模型，再将学术图转换为知识图谱并使用平移嵌入技术嵌入到与语言模型共享的语义空间。

Result: 在四个学术搜索领域评估，在三个领域中优于传统基于图和个性化模型，MAP@100最高提升10%。

Conclusion: 基于知识图谱的用户模型有增强检索效果的潜力。

Abstract: Academic Search is a search task aimed to manage and retrieve scientific
documents like journal articles and conference papers. Personalization in this
context meets individual researchers' needs by leveraging, through user
profiles, the user related information (e.g. documents authored by a
researcher), to improve search effectiveness and to reduce the information
overload. While citation graphs are a valuable means to support the outcome of
recommender systems, their use in personalized academic search (with, e.g.
nodes as papers and edges as citations) is still under-explored.
  Existing personalized models for academic search often struggle to fully
capture users' academic interests. To address this, we propose a two-step
approach: first, training a neural language model for retrieval, then
converting the academic graph into a knowledge graph and embedding it into a
shared semantic space with the language model using translational embedding
techniques. This allows user models to capture both explicit relationships and
hidden structures in citation graphs and paper content. We evaluate our
approach in four academic search domains, outperforming traditional graph-based
and personalized models in three out of four, with up to a 10\% improvement in
MAP@100 over the second-best model. This highlights the potential of knowledge
graph-based user models to enhance retrieval effectiveness.

</details>


### [51] [DUALRec: A Hybrid Sequential and Language Model Framework for Context-Aware Movie Recommendation](https://arxiv.org/abs/2507.13957)
*Yitong Li,Raoul Grasman*

Main category: cs.IR

TL;DR: 现代推荐系统在建模和预测用户偏好上面临挑战，本文提出结合LSTM和微调大语言模型的DUALRec，实验显示其优于基线模型，为推荐系统提供新方向。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以捕捉用户偏好的时间模式和动态变化，大语言模型和顺序模型各有局限，需要结合两者优势。

Method: 提出DUALRec推荐器，结合LSTM网络的时间建模能力和微调大语言模型的语义推理能力。

Result: 在MovieLens - 1M数据集上实验，DUALRec模型在多种评估指标上优于一系列基线模型。

Conclusion: 该研究提出的新架构弥合了时间序列建模和语义推理间的差距，为开发更智能、上下文感知的推荐器提供了有前景的方向。

Abstract: The modern recommender systems are facing an increasing challenge of
modelling and predicting the dynamic and context-rich user preferences.
Traditional collaborative filtering and content-based methods often struggle to
capture the temporal patternings and evolving user intentions. While Large
Language Models (LLMs) have gained gradual attention in recent years, by their
strong semantic understanding and reasoning abilities, they are not inherently
designed to model chronologically evolving user preference and intentions. On
the other hand, for sequential models like LSTM (Long-Short-Term-Memory) which
is good at capturing the temporal dynamics of user behaviour and evolving user
preference over time, but still lacks a rich semantic understanding for
comprehensive recommendation generation. In this study, we propose DUALRec
(Dynamic User-Aware Language-based Recommender), a novel recommender that
leverages the complementary strength of both models, which combines the
temporal modelling abilities of LSTM networks with semantic reasoning power of
the fine-tuned Large Language Models. The LSTM component will capture users
evolving preference through their viewing history, while the fine-tuned LLM
variants will leverage these temporal user insights to generate next movies
that users might enjoy. Experimental results on MovieLens-1M dataset shows that
the DUALRec model outperforms a wide range of baseline models, with
comprehensive evaluation matrices of Hit Rate (HR@k), Normalized Discounted
Cumulative Gain (NDCG@k), and genre similarity metrics. This research proposes
a novel architecture that bridges the gap between temporal sequence modeling
and semantic reasoning, and offers a promising direction for developing more
intelligent and context-aware recommenders.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [52] [Physical models realizing the transformer architecture of large language models](https://arxiv.org/abs/2507.13354)
*Zeqian Chen*

Main category: cs.LG

TL;DR: 从物理角度为基于transformer架构的大语言模型构建物理模型。


<details>
  <summary>Details</summary>
Motivation: 当前对transformer的理论理解存在差距，缺乏对其本质和工作原理的物理层面认知。

Method: 从现代芯片的物理角度，在token的希尔伯特空间的福克空间构建物理模型，将基于transformer架构的大语言模型视为开放量子系统。

Result: 构建了基于transformer架构大语言模型的物理模型。

Conclusion: 所构建的物理模型是大语言模型transformer架构的基础。

Abstract: The introduction of the transformer architecture in 2017 (cf.\cite{VSP2017})
marked the most striking advancement in natural language processing. The
transformer is a model architecture relying entirely on an attention mechanism
to draw global dependencies between input and output. However, we believe there
is a gap in our theoretical understanding of what the transformer is, and why
it works physically. In this paper, from a physical perspective on modern
chips, we construct physical models in the Fock space over the Hilbert space of
tokens realizing large language models based on a transformer architecture as
open quantum systems. Our physical models underlie the transformer architecture
for large language models.

</details>


### [53] [Whose View of Safety? A Deep DIVE Dataset for Pluralistic Alignment of Text-to-Image Models](https://arxiv.org/abs/2507.13383)
*Charvi Rastogi,Tian Huey Teh,Pushkar Mishra,Roma Patel,Ding Wang,Mark Díaz,Alicia Parrish,Aida Mostafazadeh Davani,Zoe Ashwood,Michela Paganini,Vinodkumar Prabhakaran,Verena Rieser,Lora Aroyo*

Main category: cs.LG

TL;DR: 提出多元对齐概念，为文本到图像（T2I）模型提供实现多元对齐的三个核心贡献，助力构建更公平、对齐的T2I系统。


<details>
  <summary>Details</summary>
Motivation: 当前T2I模型常未考虑多样人类经验，导致系统对齐问题，倡导多元对齐，让AI理解并可朝着多样、常冲突的人类价值观引导。

Method: 引入用于多元交叉视觉评估的新数据集DIVE；实证确认人口统计学是该领域多样观点的关键代理；讨论构建对齐T2I模型的影响。

Result: DIVE数据集可通过大量人口统计学交叉的人类评分者提供广泛反馈，捕捉细微安全感知；发现伤害感知存在显著的、依赖上下文的差异，与传统评估不同。

Conclusion: 本研究为更公平、对齐的T2I系统提供了基础工具。

Abstract: Current text-to-image (T2I) models often fail to account for diverse human
experiences, leading to misaligned systems. We advocate for pluralistic
alignment, where an AI understands and is steerable towards diverse, and often
conflicting, human values. Our work provides three core contributions to
achieve this in T2I models. First, we introduce a novel dataset for Diverse
Intersectional Visual Evaluation (DIVE) -- the first multimodal dataset for
pluralistic alignment. It enable deep alignment to diverse safety perspectives
through a large pool of demographically intersectional human raters who
provided extensive feedback across 1000 prompts, with high replication,
capturing nuanced safety perceptions. Second, we empirically confirm
demographics as a crucial proxy for diverse viewpoints in this domain,
revealing significant, context-dependent differences in harm perception that
diverge from conventional evaluations. Finally, we discuss implications for
building aligned T2I models, including efficient data collection strategies,
LLM judgment capabilities, and model steerability towards diverse perspectives.
This research offers foundational tools for more equitable and aligned T2I
systems. Content Warning: The paper includes sensitive content that may be
harmful.

</details>


### [54] [Improving KAN with CDF normalization to quantiles](https://arxiv.org/abs/2507.13393)
*Jakub Strawa,Jarek Duda*

Main category: cs.LG

TL;DR: 本文介绍了copula理论中通过CDF变换进行数据归一化的方法，以KANs为例展示其优势，还提及在HCR解释中的作用。


<details>
  <summary>Details</summary>
Motivation: copula理论中的归一化方法在机器学习中鲜为人知，作者希望展示其优势。

Method: 以Kolmogorov - Arnold Networks (KANs)为例，将KANs中的重缩放方法替换为CDF归一化。

Result: 通过将重缩放改为CDF归一化，改善了Legendre - KAN的预测效果。

Conclusion: copula理论中的CDF归一化方法在机器学习中有应用优势，在HCR解释中也有特殊作用。

Abstract: Data normalization is crucial in machine learning, usually performed by
subtracting the mean and dividing by standard deviation, or by rescaling to a
fixed range. In copula theory, popular in finance, there is used normalization
to approximately quantiles by transforming x to CDF(x) with estimated CDF
(cumulative distribution function) to nearly uniform distribution in [0,1],
allowing for simpler representations which are less likely to overfit. It seems
nearly unknown in machine learning, therefore, we would like to present some
its advantages on example of recently popular Kolmogorov-Arnold Networks
(KANs), improving predictions from Legendre-KAN by just switching rescaling to
CDF normalization. Additionally, in HCR interpretation, weights of such neurons
are mixed moments providing local joint distribution models, allow to propagate
also probability distributions, and change propagation direction.

</details>


### [55] [Selective Embedding for Deep Learning](https://arxiv.org/abs/2507.13399)
*Mert Sehri,Zehui Hua,Francisco de Assis Boldt,Patrick Dumond*

Main category: cs.LG

TL;DR: 本文提出选择性嵌入这一数据加载策略，用六个时域数据集验证其能提升模型泛化性、计算效率，在多领域有应用价值。


<details>
  <summary>Details</summary>
Motivation: 深度学习算法对输入数据敏感，传统数据加载策略存在局限，如限制泛化或增加计算成本。

Method: 引入选择性嵌入，在单个输入通道内交替使用多源数据的短片段，模仿人类信息处理方式。

Result: 通过六个时域数据集验证，该方法在不同深度学习架构中都能实现高分类准确率，显著减少训练时间。

Conclusion: 该方法对多数据源的复杂系统有效，为医疗、机械等领域提供可扩展且高效的解决方案。

Abstract: Deep learning has revolutionized many industries by enabling models to
automatically learn complex patterns from raw data, reducing dependence on
manual feature engineering. However, deep learning algorithms are sensitive to
input data, and performance often deteriorates under nonstationary conditions
and across dissimilar domains, especially when using time-domain data.
Conventional single-channel or parallel multi-source data loading strategies
either limit generalization or increase computational costs. This study
introduces selective embedding, a novel data loading strategy, which alternates
short segments of data from multiple sources within a single input channel.
Drawing inspiration from cognitive psychology, selective embedding mimics
human-like information processing to reduce model overfitting, enhance
generalization, and improve computational efficiency. Validation is conducted
using six time-domain datasets, demonstrating that the proposed method
consistently achieves high classification accuracy across various deep learning
architectures while significantly reducing training times. The approach proves
particularly effective for complex systems with multiple data sources, offering
a scalable and resource-efficient solution for real-world applications in
healthcare, heavy machinery, marine, railway, and agriculture, where robustness
and adaptability are critical.

</details>


### [56] [LightAutoDS-Tab: Multi-AutoML Agentic System for Tabular Data](https://arxiv.org/abs/2507.13413)
*Aleksey Lapin,Igor Hromov,Stanislav Chumakov,Mile Mitrovic,Dmitry Simakov,Nikolay O. Nikitin,Andrey V. Savchenko*

Main category: cs.LG

TL;DR: 提出多AutoML代理系统LightAutoDS - Tab，结合LLM代码生成与AutoML工具，表现优于开源方案，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有AutoML因依赖特定底层工具效率受限，需提升处理表格数据任务的灵活性和鲁棒性。

Method: 引入LightAutoDS - Tab，结合基于LLM的代码生成和多个AutoML工具。

Result: 在Kaggle的多个数据科学任务中，表现优于最先进的开源解决方案。

Conclusion: LightAutoDS - Tab能提高管道设计的灵活性和鲁棒性，为表格数据任务提供有效解决方案。

Abstract: AutoML has advanced in handling complex tasks using the integration of LLMs,
yet its efficiency remains limited by dependence on specific underlying tools.
In this paper, we introduce LightAutoDS-Tab, a multi-AutoML agentic system for
tasks with tabular data, which combines an LLM-based code generation with
several AutoML tools. Our approach improves the flexibility and robustness of
pipeline design, outperforming state-of-the-art open-source solutions on
several data science tasks from Kaggle. The code of LightAutoDS-Tab is
available in the open repository https://github.com/sb-ai-lab/LADS

</details>


### [57] [Gauge Flow Models](https://arxiv.org/abs/2507.13414)
*Alexander Strunk,Roland Assam*

Main category: cs.LG

TL;DR: 本文介绍了新型生成流模型Gauge Flow Models，提供数学框架，实验显示其性能优于传统流模型，未发表研究显示在更多生成任务有潜力。


<details>
  <summary>Details</summary>
Motivation: 提出一种性能更优的生成流模型。

Method: 在流常微分方程中引入可学习的规范场构建Gauge Flow Models，并提供全面数学框架。

Result: 在高斯混合模型上的流匹配实验中，Gauge Flow Models比传统流模型性能更好。

Conclusion: Gauge Flow Models有潜力在更广泛的生成任务中提升性能。

Abstract: This paper introduces Gauge Flow Models, a novel class of Generative Flow
Models. These models incorporate a learnable Gauge Field within the Flow
Ordinary Differential Equation (ODE). A comprehensive mathematical framework
for these models, detailing their construction and properties, is provided.
Experiments using Flow Matching on Gaussian Mixture Models demonstrate that
Gauge Flow Models yields significantly better performance than traditional Flow
Models of comparable or even larger size. Additionally, unpublished research
indicates a potential for enhanced performance across a broader range of
generative tasks.

</details>


### [58] [Bayesian Optimization for Molecules Should Be Pareto-Aware](https://arxiv.org/abs/2507.13704)
*Anabel Yong,Austin Tripp,Layla Hosseini-Gerami,Brooks Paige*

Main category: cs.LG

TL;DR: 在分子设计中对比多目标贝叶斯优化策略EHVI和标量化EI，结果显示EHVI在三项分子优化任务中表现更优，体现Pareto感知获取的实际优势。


<details>
  <summary>Details</summary>
Motivation: 探索多目标贝叶斯优化（MOBO）相比标量化替代方案在分子设计中的实证优势。

Method: 在相同高斯过程代理和分子表示的严格控制设置下，用Expected Hypervolume Improvement (EHVI) 策略与基于Expected Improvement (EI) 的简单固定权重标量化基线进行基准测试。

Result: 在三项分子优化任务中，EHVI在Pareto前沿覆盖、收敛速度和化学多样性方面始终优于标量化EI，即使是强大的确定性标量化实例在低数据情况下也可能表现不佳。

Conclusion: 在从头分子优化中，尤其是评估预算有限且权衡复杂时，Pareto感知获取具有实际优势。

Abstract: Multi-objective Bayesian optimization (MOBO) provides a principled framework
for navigating trade-offs in molecular design. However, its empirical
advantages over scalarized alternatives remain underexplored. We benchmark a
simple Pareto-based MOBO strategy -- Expected Hypervolume Improvement (EHVI) --
against a simple fixed-weight scalarized baseline using Expected Improvement
(EI), under a tightly controlled setup with identical Gaussian Process
surrogates and molecular representations. Across three molecular optimization
tasks, EHVI consistently outperforms scalarized EI in terms of Pareto front
coverage, convergence speed, and chemical diversity. While scalarization
encompasses flexible variants -- including random or adaptive schemes -- our
results show that even strong deterministic instantiations can underperform in
low-data regimes. These findings offer concrete evidence for the practical
advantages of Pareto-aware acquisition in de novo molecular optimization,
especially when evaluation budgets are limited and trade-offs are nontrivial.

</details>


### [59] [FedSkipTwin: Digital-Twin-Guided Client Skipping for Communication-Efficient Federated Learning](https://arxiv.org/abs/2507.13624)
*Daniel Commey,Kamel Abbad,Garth V. Crosby,Lyes Khoukhi*

Main category: cs.LG

TL;DR: 提出FedSkipTwin算法解决联邦学习通信开销问题，实验表明可减少通信量并提升模型精度。


<details>
  <summary>Details</summary>
Motivation: 通信开销是联邦学习的主要瓶颈，尤其是在带宽受限的移动和物联网设备应用中。

Method: 引入由轻量级服务器端数字孪生驱动的FedSkipTwin客户端跳过算法，用LSTM预测客户端更新，超过阈值才通信。

Result: 在UCI - HAR和MNIST数据集上，20轮中减少12 - 15.5%的通信量，模型精度最多提升0.5个百分点。

Conclusion: 预测引导的跳过策略是带宽受限边缘环境下资源感知联邦学习的有效策略。

Abstract: Communication overhead remains a primary bottleneck in federated learning
(FL), particularly for applications involving mobile and IoT devices with
constrained bandwidth. This work introduces FedSkipTwin, a novel
client-skipping algorithm driven by lightweight, server-side digital twins.
Each twin, implemented as a simple LSTM, observes a client's historical
sequence of gradient norms to forecast both the magnitude and the epistemic
uncertainty of its next update. The server leverages these predictions,
requesting communication only when either value exceeds a predefined threshold;
otherwise, it instructs the client to skip the round, thereby saving bandwidth.
Experiments are conducted on the UCI-HAR and MNIST datasets with 10 clients
under a non-IID data distribution. The results demonstrate that FedSkipTwin
reduces total communication by 12-15.5% across 20 rounds while simultaneously
improving final model accuracy by up to 0.5 percentage points compared to the
standard FedAvg algorithm. These findings establish that prediction-guided
skipping is a practical and effective strategy for resource-aware FL in
bandwidth-constrained edge environments.

</details>


### [60] [Single- to multi-fidelity history-dependent learning with uncertainty quantification and disentanglement: application to data-driven constitutive modeling](https://arxiv.org/abs/2507.13416)
*Jiaxiang Yi,Bernardo P. Ferreira,Miguel A. Bessa*

Main category: cs.LG

TL;DR: 将数据驱动学习推广到考虑历史相关的多保真度数据，量化认知不确定性并将其与数据噪声分离，展示了方法的通用性和准确性，为现实应用带来机会。


<details>
  <summary>Details</summary>
Motivation: 推广数据驱动学习以处理历史相关的多保真度数据，量化认知不确定性并将其与数据噪声分离。

Method: 采用分层方法，从训练单保真度确定性神经网络到提出多保真度方差估计贝叶斯循环神经网络。

Result: 方法能准确预测响应、量化模型误差并发现噪声分布。

Conclusion: 该方法具有通用性，为不同科学和工程领域的现实应用带来机会，尤其适用于不确定性下的设计和分析。

Abstract: Data-driven learning is generalized to consider history-dependent
multi-fidelity data, while quantifying epistemic uncertainty and disentangling
it from data noise (aleatoric uncertainty). This generalization is hierarchical
and adapts to different learning scenarios: from training the simplest
single-fidelity deterministic neural networks up to the proposed multi-fidelity
variance estimation Bayesian recurrent neural networks. The versatility and
generality of the proposed methodology are demonstrated by applying it to
different data-driven constitutive modeling scenarios that include multiple
fidelities with and without aleatoric uncertainty (noise). The method
accurately predicts the response and quantifies model error while also
discovering the noise distribution (when present). This opens opportunities for
future real-world applications in diverse scientific and engineering domains;
especially, the most challenging cases involving design and analysis under
uncertainty.

</details>


### [61] [Soft-ECM: An extension of Evidential C-Means for complex data](https://arxiv.org/abs/2507.13417)
*Armel Soubeiga,Thomas Guyet,Violaine Antoine*

Main category: cs.LG

TL;DR: 本文针对现有基于信度函数的聚类算法无法处理复杂数据的问题，提出新算法Soft - ECM，实验表明其在不同类型数据上有良好表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于信度函数的聚类算法无法应用于复杂数据，如混合数据和时间序列数据。

Method: 重新定义Evidential C - Means (ECM)问题以处理复杂数据，提出新算法Soft - ECM，仅需半度量来定位不精确聚类的质心。

Result: Soft - ECM在数值数据上的结果与传统模糊聚类方法相当，能处理混合数据，在结合半度量处理时间序列数据时有优势。

Conclusion: Soft - ECM算法可有效处理复杂数据，为基于信度函数的聚类提供了更好的解决方案。

Abstract: Clustering based on belief functions has been gaining increasing attention in
the machine learning community due to its ability to effectively represent
uncertainty and/or imprecision. However, none of the existing algorithms can be
applied to complex data, such as mixed data (numerical and categorical) or
non-tabular data like time series. Indeed, these types of data are, in general,
not represented in a Euclidean space and the aforementioned algorithms make use
of the properties of such spaces, in particular for the construction of
barycenters. In this paper, we reformulate the Evidential C-Means (ECM) problem
for clustering complex data. We propose a new algorithm, Soft-ECM, which
consistently positions the centroids of imprecise clusters requiring only a
semi-metric. Our experiments show that Soft-ECM present results comparable to
conventional fuzzy clustering approaches on numerical data, and we demonstrate
its ability to handle mixed data and its benefits when combining fuzzy
clustering with semi-metrics such as DTW for time series data.

</details>


### [62] [Toward Temporal Causal Representation Learning with Tensor Decomposition](https://arxiv.org/abs/2507.14126)
*Jianhong Chen,Meng Zhao,Mostafa Reisi Gahrooei,Xubo Yue*

Main category: cs.LG

TL;DR: 本文提出CaRTeD联合学习框架，结合时间因果表征学习与不规则张量分解，理论证明算法收敛，实验显示其优于现有技术并提升因果表征可解释性。


<details>
  <summary>Details</summary>
Motivation: 现实应用中数据多为高维、输入长度可变的不规则张量，不规则张量分解对分析此类数据、提取有意义聚类至关重要，现有研究缺乏不规则张量分解收敛的理论保证。

Method: 提出针对潜在聚类的新因果公式，构建CaRTeD联合学习框架，融合时间因果表征学习与不规则张量分解，提供下游任务蓝图并采用更灵活正则化设计。

Result: 理论上算法收敛到平稳点，填补了现有不规则张量分解收敛理论保证的空白；实验中在合成和真实电子健康记录数据集上表现优于现有技术，提升因果表征可解释性。

Conclusion: CaRTeD框架在处理不规则张量数据的因果表征学习方面有效且具有优势。

Abstract: Temporal causal representation learning is a powerful tool for uncovering
complex patterns in observational studies, which are often represented as
low-dimensional time series. However, in many real-world applications, data are
high-dimensional with varying input lengths and naturally take the form of
irregular tensors. To analyze such data, irregular tensor decomposition is
critical for extracting meaningful clusters that capture essential information.
In this paper, we focus on modeling causal representation learning based on the
transformed information. First, we present a novel causal formulation for a set
of latent clusters. We then propose CaRTeD, a joint learning framework that
integrates temporal causal representation learning with irregular tensor
decomposition. Notably, our framework provides a blueprint for downstream tasks
using the learned tensor factors, such as modeling latent structures and
extracting causal information, and offers a more flexible regularization design
to enhance tensor decomposition. Theoretically, we show that our algorithm
converges to a stationary point. More importantly, our results fill the gap in
theoretical guarantees for the convergence of state-of-the-art irregular tensor
decomposition. Experimental results on synthetic and real-world electronic
health record (EHR) datasets (MIMIC-III), with extensive benchmarks from both
phenotyping and network recovery perspectives, demonstrate that our proposed
method outperforms state-of-the-art techniques and enhances the explainability
of causal representations.

</details>


### [63] [An End-to-End DNN Inference Framework for the SpiNNaker2 Neuromorphic MPSoC](https://arxiv.org/abs/2507.13736)
*Matthias Jobst,Tim Langer,Chen Liu,Mehmet Alici,Hector A. Gonzalez,Christian Mayr*

Main category: cs.LG

TL;DR: 提出多层DNN调度框架扩展OctopuScheduler，实现从PyTorch模型到SpiNNaker2芯片推理的端到端流程。


<details>
  <summary>Details</summary>
Motivation: 实现基于神经形态平台SpiNNaker2进行边缘侧大型复杂DNN（达transformer规模）的执行。

Method: 提出多层DNN调度框架作为OctopuScheduler的扩展，结合由量化和降低步骤组成的前端。

Result: 框架实现了从PyTorch模型到SpiNNaker2芯片推理的端到端流程。

Conclusion: 所提框架能利用SpiNNaker2平台实现边缘侧大型复杂DNN的执行。

Abstract: This work presents a multi-layer DNN scheduling framework as an extension of
OctopuScheduler, providing an end-to-end flow from PyTorch models to inference
on a single SpiNNaker2 chip. Together with a front-end comprised of
quantization and lowering steps, the proposed framework enables the edge-based
execution of large and complex DNNs up to transformer scale using the
neuromorphic platform SpiNNaker2.

</details>


### [64] [Air Traffic Controller Task Demand via Graph Neural Networks: An Interpretable Approach to Airspace Complexity](https://arxiv.org/abs/2507.13423)
*Edward Henderson,Dewi Gould,Richard Everson,George De Ath,Nick Pepper*

Main category: cs.LG

TL;DR: 本文提出可解释GNN框架评估空管任务需求，表现优于启发式方法和基线。


<details>
  <summary>Details</summary>
Motivation: 现有复杂性指标难以捕捉简单飞机数量之外的运营驱动因素，需要实时评估近短期空管任务需求。

Method: 引入基于注意力的GNN模型，通过系统消融飞机并测量对模型预测的影响得出每架飞机的任务需求分数。

Result: 框架显著优于空管启发式方法，比现有基线更能可靠估计场景复杂性。

Conclusion: 该工具可将任务需求归因于特定飞机，为管制员培训和空域重新设计提供新分析方法。

Abstract: Real-time assessment of near-term Air Traffic Controller (ATCO) task demand
is a critical challenge in an increasingly crowded airspace, as existing
complexity metrics often fail to capture nuanced operational drivers beyond
simple aircraft counts. This work introduces an interpretable Graph Neural
Network (GNN) framework to address this gap. Our attention-based model predicts
the number of upcoming clearances, the instructions issued to aircraft by
ATCOs, from interactions within static traffic scenarios. Crucially, we derive
an interpretable, per-aircraft task demand score by systematically ablating
aircraft and measuring the impact on the model's predictions. Our framework
significantly outperforms an ATCO-inspired heuristic and is a more reliable
estimator of scenario complexity than established baselines. The resulting tool
can attribute task demand to specific aircraft, offering a new way to analyse
and understand the drivers of complexity for applications in controller
training and airspace redesign.

</details>


### [65] [Off-Policy Evaluation and Learning for Matching Markets](https://arxiv.org/abs/2507.13608)
*Yudai Hayashi,Shuhei Goda,Yuta Saito*

Main category: cs.LG

TL;DR: 文章提出针对匹配市场的新型离线策略评估（OPE）估计器DiPS和DPR，结合多种方法并引入中间标签，理论推导其优势，实验验证在OPE和学习任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: A/B测试评估匹配市场推荐策略成本高、不适合频繁更新，标准OPE方法在匹配平台因用户交互特性不可靠，需有效离线评估方法。

Method: 提出DiPS和DPR估计器，结合DM、IPS和DR估计器元素，引入中间标签控制偏差和方差，可扩展到离线策略学习。

Result: 理论推导了估计器的偏差和方差，实验在合成数据和真实岗位匹配平台A/B测试日志上验证了方法在多种配置下的优越性。

Conclusion: 所提方法在匹配市场的离线策略评估和学习任务中表现优于现有方法。

Abstract: Matching users based on mutual preferences is a fundamental aspect of
services driven by reciprocal recommendations, such as job search and dating
applications. Although A/B tests remain the gold standard for evaluating new
policies in recommender systems for matching markets, it is costly and
impractical for frequent policy updates. Off-Policy Evaluation (OPE) thus plays
a crucial role by enabling the evaluation of recommendation policies using only
offline logged data naturally collected on the platform. However, unlike
conventional recommendation settings, the large scale and bidirectional nature
of user interactions in matching platforms introduce variance issues and
exacerbate reward sparsity, making standard OPE methods unreliable. To address
these challenges and facilitate effective offline evaluation, we propose novel
OPE estimators, \textit{DiPS} and \textit{DPR}, specifically designed for
matching markets. Our methods combine elements of the Direct Method (DM),
Inverse Propensity Score (IPS), and Doubly Robust (DR) estimators while
incorporating intermediate labels, such as initial engagement signals, to
achieve better bias-variance control in matching markets. Theoretically, we
derive the bias and variance of the proposed estimators and demonstrate their
advantages over conventional methods. Furthermore, we show that these
estimators can be seamlessly extended to offline policy learning methods for
improving recommendation policies for making more matches. We empirically
evaluate our methods through experiments on both synthetic data and A/B testing
logs from a real job-matching platform. The empirical results highlight the
superiority of our approach over existing methods in off-policy evaluation and
learning tasks for a variety of configurations.

</details>


### [66] [Improving Out-of-distribution Human Activity Recognition via IMU-Video Cross-modal Representation Learning](https://arxiv.org/abs/2507.13482)
*Seyyed Saeid Cheshmi,Buyao Lyu,Thomas Lisko,Rajesh Rajamani,Robert A. McGovern,Yogatheesan Varatharajah*

Main category: cs.LG

TL;DR: 提出跨模态自监督预训练方法用于人体活动识别，在OOD数据集上提升了泛化性，软件开源。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法用于HAR任务时依赖特定标签，缺乏在不同环境或人群数据上的泛化性，需新方法解决。

Method: 提出新的跨模态自监督预训练方法，从大规模无标签IMU - 视频数据中学习表征。

Result: 提出的跨模态预训练方法在零样本和少样本评估中优于现有IMU - 视频预训练方法和仅IMU预训练方法。

Conclusion: 在IMU信号等动态数据模态中，跨模态预训练是学习可泛化数据表征的有用工具。

Abstract: Human Activity Recognition (HAR) based on wearable inertial sensors plays a
critical role in remote health monitoring. In patients with movement disorders,
the ability to detect abnormal patient movements in their home environments can
enable continuous optimization of treatments and help alert caretakers as
needed. Machine learning approaches have been proposed for HAR tasks using
Inertial Measurement Unit (IMU) data; however, most rely on
application-specific labels and lack generalizability to data collected in
different environments or populations. To address this limitation, we propose a
new cross-modal self-supervised pretraining approach to learn representations
from large-sale unlabeled IMU-video data and demonstrate improved
generalizability in HAR tasks on out of distribution (OOD) IMU datasets,
including a dataset collected from patients with Parkinson's disease.
Specifically, our results indicate that the proposed cross-modal pretraining
approach outperforms the current state-of-the-art IMU-video pretraining
approach and IMU-only pretraining under zero-shot and few-shot evaluations.
Broadly, our study provides evidence that in highly dynamic data modalities,
such as IMU signals, cross-modal pretraining may be a useful tool to learn
generalizable data representations. Our software is available at
https://github.com/scheshmi/IMU-Video-OOD-HAR.

</details>


### [67] [Model-free Reinforcement Learning for Model-based Control: Towards Safe, Interpretable and Sample-efficient Agents](https://arxiv.org/abs/2507.13491)
*Thomas Banker,Ali Mesbah*

Main category: cs.LG

TL;DR: 介绍基于模型的智能体作为控制策略近似的替代方案，探讨其学习的利弊、方法，强调模型无关RL与基于模型智能体结合的潜力。


<details>
  <summary>Details</summary>
Motivation: 模型无关RL存在样本效率低、学习不安全和可解释性有限等问题，其与基于模型智能体的相互作用尚待探索。

Method: 引入基于模型的智能体进行控制策略近似，利用系统动力学、成本和约束的自适应模型进行安全策略学习，不足部分用模型无关RL弥补，介绍学习方法如贝叶斯优化、策略搜索RL和离线策略。

Result: 阐述了基于模型智能体学习的好处和挑战，以及不同学习方法的优势。

Conclusion: 模型无关RL与基于模型智能体结合在样本高效学习安全且可解释的决策智能体方面有潜力。

Abstract: Training sophisticated agents for optimal decision-making under uncertainty
has been key to the rapid development of modern autonomous systems across
fields. Notably, model-free reinforcement learning (RL) has enabled
decision-making agents to improve their performance directly through system
interactions, with minimal prior knowledge about the system. Yet, model-free RL
has generally relied on agents equipped with deep neural network function
approximators, appealing to the networks' expressivity to capture the agent's
policy and value function for complex systems. However, neural networks amplify
the issues of sample inefficiency, unsafe learning, and limited
interpretability in model-free RL. To this end, this work introduces
model-based agents as a compelling alternative for control policy
approximation, leveraging adaptable models of system dynamics, cost, and
constraints for safe policy learning. These models can encode prior system
knowledge to inform, constrain, and aid in explaining the agent's decisions,
while deficiencies due to model mismatch can be remedied with model-free RL. We
outline the benefits and challenges of learning model-based agents --
exemplified by model predictive control -- and detail the primary learning
approaches: Bayesian optimization, policy search RL, and offline strategies,
along with their respective strengths. While model-free RL has long been
established, its interplay with model-based agents remains largely unexplored,
motivating our perspective on their combined potentials for sample-efficient
learning of safe and interpretable decision-making agents.

</details>


### [68] [Fake or Real: The Impostor Hunt in Texts for Space Operations](https://arxiv.org/abs/2507.13508)
*Agata Kaczmarek,Dawid Płudowski,Piotr Wilczyński,Przemysław Biecek,Krzysztof Kotowski,Ramez Shendy,Jakub Nalepa,Artur Janicki,Evridiki Ntagiou*

Main category: cs.LG

TL;DR: 介绍Kaggle上‘Fake or Real’竞赛，其源于欧空局项目，基于数据投毒和大模型过度依赖两种威胁，任务是区分大模型正常与恶意修改输出，需新方法解决。


<details>
  <summary>Details</summary>
Motivation: 应对项目中识别出的大模型数据投毒和过度依赖这两种现实AI安全威胁。

Method: 要求参与者开发新技术或调整现有技术来解决区分大模型正常与恶意修改输出的问题。

Result: 文中未提及。

Conclusion: 文中未提及。

Abstract: The "Fake or Real" competition hosted on Kaggle
(\href{https://www.kaggle.com/competitions/fake-or-real-the-impostor-hunt}{https://www.kaggle.com/competitions/fake-or-real-the-impostor-hunt})
is the second part of a series of follow-up competitions and hackathons related
to the "Assurance for Space Domain AI Applications" project funded by the
European Space Agency
(\href{https://assurance-ai.space-codev.org/}{https://assurance-ai.space-codev.org/}).
The competition idea is based on two real-life AI security threats identified
within the project -- data poisoning and overreliance in Large Language Models.
The task is to distinguish between the proper output from LLM and the output
generated under malicious modification of the LLM. As this problem was not
extensively researched, participants are required to develop new techniques to
address this issue or adjust already existing ones to this problem's statement.

</details>


### [69] [Provable Low-Frequency Bias of In-Context Learning of Representations](https://arxiv.org/abs/2507.13540)
*Yongyi Yang,Hidenori Tanaka,Wei Hu*

Main category: cs.LG

TL;DR: 本文提出双重收敛统一框架解释大语言模型上下文学习（ICL）现象，分析并验证其隐含偏置，解释相关经验观察，预测并证实ICL对高频噪声的内在鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有研究未明确大语言模型实现上下文学习能力的机制，本文旨在给出严格解释。

Method: 引入双重收敛统一框架，其中隐藏表示在上下文和跨层上收敛。

Result: 双重收敛过程导致对平滑（低频）表示的隐含偏置，理论解释了一些经验观察，预测并实证了ICL对高频噪声的内在鲁棒性。

Conclusion: 为ICL的潜在机制提供新见解，为研究ICL提供理论基础，有望扩展到更一般的数据分布和设置。

Abstract: In-context learning (ICL) enables large language models (LLMs) to acquire new
behaviors from the input sequence alone without any parameter updates. Recent
studies have shown that ICL can surpass the original meaning learned in
pretraining stage through internalizing the structure the data-generating
process (DGP) of the prompt into the hidden representations. However, the
mechanisms by which LLMs achieve this ability is left open. In this paper, we
present the first rigorous explanation of such phenomena by introducing a
unified framework of double convergence, where hidden representations converge
both over context and across layers. This double convergence process leads to
an implicit bias towards smooth (low-frequency) representations, which we prove
analytically and verify empirically. Our theory explains several open empirical
observations, including why learned representations exhibit globally structured
but locally distorted geometry, and why their total energy decays without
vanishing. Moreover, our theory predicts that ICL has an intrinsic robustness
towards high-frequency noise, which we empirically confirm. These results
provide new insights into the underlying mechanisms of ICL, and a theoretical
foundation to study it that hopefully extends to more general data
distributions and settings.

</details>


### [70] [Acoustic Index: A Novel AI-Driven Parameter for Cardiac Disease Risk Stratification Using Echocardiography](https://arxiv.org/abs/2507.13542)
*Beka Begiashvili,Carlos J. Fernandez-Candel,Matías Pérez Paredes*

Main category: cs.LG

TL;DR: 传统心脏超声参数在早期检测心脏功能障碍有局限，本文引入声学指数，结合EDMD与混合神经网络，在736例患者中验证有效，是有前景的心脏功能生物标志物。


<details>
  <summary>Details</summary>
Motivation: 传统超声参数如EF和GLS在早期检测心脏功能障碍有局限，需可重复、易解释且不受操作者影响的参数。

Method: 引入声学指数，结合基于Koopman算子理论的EDMD和混合神经网络，从超声序列提取时空动力学，结合注意力机制和临床数据。

Result: 在736例患者中，声学指数在独立测试集AUC达0.89，交叉验证显示敏感性和特异性超0.8，阈值分析有稳定权衡。

Conclusion: 声学指数是有前景的心脏功能生物标志物，可用于早期检测、分诊和纵向监测，未来需外部验证等。

Abstract: Traditional echocardiographic parameters such as ejection fraction (EF) and
global longitudinal strain (GLS) have limitations in the early detection of
cardiac dysfunction. EF often remains normal despite underlying pathology, and
GLS is influenced by load conditions and vendor variability. There is a growing
need for reproducible, interpretable, and operator-independent parameters that
capture subtle and global cardiac functional alterations.
  We introduce the Acoustic Index, a novel AI-derived echocardiographic
parameter designed to quantify cardiac dysfunction from standard ultrasound
views. The model combines Extended Dynamic Mode Decomposition (EDMD) based on
Koopman operator theory with a hybrid neural network that incorporates clinical
metadata. Spatiotemporal dynamics are extracted from echocardiographic
sequences to identify coherent motion patterns. These are weighted via
attention mechanisms and fused with clinical data using manifold learning,
resulting in a continuous score from 0 (low risk) to 1 (high risk).
  In a prospective cohort of 736 patients, encompassing various cardiac
pathologies and normal controls, the Acoustic Index achieved an area under the
curve (AUC) of 0.89 in an independent test set. Cross-validation across five
folds confirmed the robustness of the model, showing that both sensitivity and
specificity exceeded 0.8 when evaluated on independent data. Threshold-based
analysis demonstrated stable trade-offs between sensitivity and specificity,
with optimal discrimination near this threshold.
  The Acoustic Index represents a physics-informed, interpretable AI biomarker
for cardiac function. It shows promise as a scalable, vendor-independent tool
for early detection, triage, and longitudinal monitoring. Future directions
include external validation, longitudinal studies, and adaptation to
disease-specific classifiers.

</details>


### [71] [Time Series Forecastability Measures](https://arxiv.org/abs/2507.13556)
*Rui Wang,Steven Klee,Alexis Roos*

Main category: cs.LG

TL;DR: 本文提出用光谱可预测性分数和最大Lyapunov指数两个指标在建模前量化时间序列可预测性，经评估有效，助从业者规划。


<details>
  <summary>Details</summary>
Motivation: 在模型开发前评估时间序列的内在可预测性特征。

Method: 提出光谱可预测性分数和最大Lyapunov指数两个指标，在合成和真实世界时间序列上评估其有效性。

Result: 两个指标能正确反映时间序列的内在可预测性，与各模型实际预测性能强相关。

Conclusion: 从业者可依据指标了解可预测性，对不同产品规划和制定策略。

Abstract: This paper proposes using two metrics to quantify the forecastability of time
series prior to model development: the spectral predictability score and the
largest Lyapunov exponent. Unlike traditional model evaluation metrics, these
measures assess the inherent forecastability characteristics of the data before
any forecast attempts. The spectral predictability score evaluates the strength
and regularity of frequency components in the time series, whereas the Lyapunov
exponents quantify the chaos and stability of the system generating the data.
We evaluated the effectiveness of these metrics on both synthetic and
real-world time series from the M5 forecast competition dataset. Our results
demonstrate that these two metrics can correctly reflect the inherent
forecastability of a time series and have a strong correlation with the actual
forecast performance of various models. By understanding the inherent
forecastability of time series before model training, practitioners can focus
their planning efforts on products and supply chain levels that are more
forecastable, while setting appropriate expectations or seeking alternative
strategies for products with limited forecastability.

</details>


### [72] [Change of Thought: Adaptive Test-Time Computation](https://arxiv.org/abs/2507.13569)
*Mrinal Mathur,Mike Doan,Barak Pearlmutter,Sergey Plis*

Main category: cs.LG

TL;DR: 提出SELF - Transformer提升编码器Transformer表达能力，在不增加参数下提升准确率。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer单步评估表达能力有限，基于反馈循环的方式类似人类‘大声思考’，而生物大脑迭代无需将中间状态语言化，需在不依赖token级自回归的情况下提升编码器Transformer表达能力。

Method: 引入SELF - Transformer，迭代地将自身注意力权重优化到固定点，内部迭代更新对齐矩阵。

Result: 在编码器风格基准测试中准确率提升达20%，且不增加参数数量。

Conclusion: SELF - Transformer在保留纯编码器架构简单性的同时，恢复了迭代推理的大部分表达能力。

Abstract: Transformers evaluated in a single, fixed-depth pass are provably limited in
expressive power to the constant-depth circuit class TC0. Running a Transformer
autoregressively removes that ceiling -- first in next-token prediction and,
more recently, in chain-of-thought reasoning. Both regimes rely on feedback
loops that decode internal states into tokens only to re-encode them in
subsequent steps. While this "thinking aloud" mirrors human reasoning,
biological brains iterate without externalising intermediate states as
language. To boost the expressive power of encoder Transformers without
resorting to token-level autoregression, we introduce the SELF-Transformer: an
encoder layer that iteratively refines its own attention weights to a fixed
point. Instead of producing -- in one pass -- the alignment matrix that remixes
the input sequence, the SELF-Transformer iteratively updates that matrix
internally, scaling test-time computation with input difficulty. This
adaptivity yields up to 20\% accuracy gains on encoder-style benchmarks without
increasing parameter count, demonstrating that input-adaptive alignment at test
time offers substantial benefits for only a modest extra compute budget.
Self-Transformers thus recover much of the expressive power of iterative
reasoning while preserving the simplicity of pure encoder architectures.

</details>


### [73] [Apple Intelligence Foundation Language Models: Tech Report 2025](https://arxiv.org/abs/2507.13575)
*Hanzhi Zhou,Erik Hornberger,Pengsheng Guo,Xiyou Zhou,Saiwen Wang,Xin Wang,Yifei He,Xuankai Chang,Rene Rauch,Louis D'hauwe,John Peebles,Alec Doane,Kohen Chia,Jenna Thibodeau,Zi-Yi Dou,Yuanyang Zhang,Ruoming Pang,Reed Li,Zhifeng Chen,Jeremy Warner,Zhaoyang Xu,Sophy Lee,David Mizrahi,Ramsey Tantawi,Chris Chaney,Kelsey Peterson,Jun Qin,Alex Dombrowski,Mira Chiang,Aiswarya Raghavan,Gerard Casamayor,Qibin Chen,Aonan Zhang,Nathalie Tran,Jianyu Wang,Hang Su,Thomas Voice,Alessandro Pappalardo,Brycen Wershing,Prasanth Yadla,Rui Li,Priyal Chhatrapati,Ismael Fernandez,Yusuf Goren,Xin Zheng,Forrest Huang,Tao Lei,Eray Yildiz,Alper Kokmen,Gokul Santhanam,Areeba Kamal,Kaan Elgin,Dian Ang Yap,Jeremy Liu,Peter Gray,Howard Xing,Kieran Liu,Matteo Ronchi,Moritz Schwarzer-Becker,Yun Zhu,Mandana Saebi,Jeremy Snow,David Griffiths,Guillaume Tartavel,Erin Feldman,Simon Lehnerer,Fernando Bermúdez-Medina,Hans Han,Joe Zhou,Xiaoyi Ren,Sujeeth Reddy,Zirui Wang,Tom Gunter,Albert Antony,Yuanzhi Li,John Dennison,Tony Sun,Yena Han,Yi Qin,Sam Davarnia,Jeffrey Bigham,Wayne Shan,Hannah Gillis Coleman,Guillaume Klein,Peng Liu,Muyang Yu,Jack Cackler,Yuan Gao,Crystal Xiao,Binazir Karimzadeh,Zhengdong Zhang,Felix Bai,Albin Madappally Jose,Feng Nan,Nazir Kamaldin,Dong Yin,Hans Hao,Yanchao Sun,Yi Hua,Charles Maalouf,Alex Guillen Garcia,Guoli Yin,Lezhi Li,Mohana Prasad Sathya Moorthy,Hongbin Gao,Jay Tang,Joanna Arreaza-Taylor,Faye Lao,Carina Peng,Josh Shaffer,Dan Masi,Sushma Rao,Tommi Vehvilainen,Senyu Tong,Dongcai Shen,Yang Zhao,Chris Bartels,Peter Fu,Qingqing Cao,Christopher Neubauer,Ethan Li,Mingfei Gao,Rebecca Callahan,Richard Wei,Patrick Dong,Alex Braunstein,Sachin Ravi,Adolfo Lopez Mendez,Kaiwei Huang,Kun Duan,Haoshuo Huang,Rui Qian,Stefano Ligas,Jordan Huffaker,Dongxu Li,Bailin Wang,Nanzhu Wang,Anuva Agarwal,Tait Madsen,Josh Newnham,Abhishek Sharma,Zhile Ren,Deepak Gopinath,Erik Daxberger,Saptarshi Guha,Oron Levy,Jing Lu,Nan Dun,Marc Kirchner,Yinfei Yang,Manjot Bilkhu,Dave Nelson,Anthony Spalvieri-Kruse,Juan Lao Tebar,Yang Xu,Phani Mutyala,Gabriel Jacoby-Cooper,Yingbo Wang,Karla Vega,Vishaal Mahtani,Darren Botten,Eric Wang,Hanli Li,Matthias Paulik,Haoran Yan,Navid Shiee,Yihao Qian,Bugu Wu,Qi Zhu,Ob Adaranijo,Bhuwan Dhingra,Zhe Gan,Nicholas Seidl,Grace Duanmu,Rong Situ,Yiping Ma,Yin Xia,David Riazati,Vasileios Saveris,Anh Nguyen,Michael,Lee,Patrick Sonnenberg,Chinguun Erdenebileg,Yanghao Li,Vivian Ma,James Chou,Isha Garg,Mark Lee,Keen You,Yuhong Li,Ransen Niu,Nandhitha Raghuram,Pulkit Agrawal,Henry Mason,Sumeet Singh,Keyu He,Hong-You Chen,Lucas Guibert,Shiyu Li,Varsha Paidi,Narendran Raghavan,Mingze Xu,Yuli Yang,Sergiu Sima,Irina Belousova,Sprite Chu,Afshin Dehghan,Philipp Dufter,David Haldimann,Zhen Yang,Margit Bowler,Chang Liu,Ying-Chang Cheng,Vivek Rathod,Syd Evans,Wilson Tsao,Dustin Withers,Haitian Sun,Biyao Wang,Peter Grasch,Walker Cheng,Yihao Feng,Vivek Kumar,Frank Chu,Victoria MönchJuan Haladjian,Doug Kang,Jiarui Lu,Ciro Sannino,Max Lam,Floris Weers,Bowen Pan,Kenneth Jung,Dhaval Doshi,Fangping Shi,Olli Saarikivi,Alp Aygar,Josh Elman,Cheng Leong,Eshan Verma,Matthew Lei,Jeff Nichols,Jiulong Shan,Donald Zhang,Lawrence Zhou,Stephen Murphy,Xianzhi Du,Chang Lan,Ankur Jain,Elmira Amirloo,Marcin Eichner,Naomy Sabo,Anupama Mann Anupama,David Qiu,Zhao Meng,Michael FitzMaurice,Peng Zhang,Simon Yeung,Chen Chen,Marco Zuliani,Andrew Hansen,Yang Lu,Brent Ramerth,Ziyi Zhong,Parsa Mazaheri,Matthew Hopkins,Mengyu Li,Simon Wang,David Chen,Farzin Rasteh,Chong Wang,Josh Gardner,Asaf Liberman,Haoxuan You,Andrew Walkingshaw,Xingyu Zhou,Jinhao Lei,Yan Meng,Quentin Keunebroek,Sam Wiseman,Anders Boesen Lindbo Larsen,Yi Zhang,Zaid Ahmed,Haiming Gang,Aaron Franklin,Kelvin Zou,Guillaume Seguin,Jonathan Janke,Rachel Burger,Co Giang,Cheng Shen,Jen Liu,Sanskruti Shah,Xiang Kong,Yiran Fei,TJ Collins,Chen Zhang,Zhiyun Lu,Michael Booker,Qin Ba,Yasutaka Tanaka,Andres Romero Mier Y Teran,Federico Scozzafava,Regan Poston,Jane Li,Eduardo Jimenez,Bas Straathof,Karanjeet Singh,Lindsay Hislop,Rajat Arora,Deepa Seshadri,Boyue Li,Colorado Reed,Zhen Li,TJ Lu,Yi Wang,Kaelen Haag,Nicholas Lusskin,Raunak Sinha,Rahul Nair,Eldon Schoop,Mary Beth Kery,Mehrdad Farajtbar,Brenda Yang,George Horrell,Shiwen Zhao,Dhruti Shah,Cha Chen,Bowen Zhang,Chang Gao,Devi Krishna,Jennifer Mallalieu,Javier Movellan,Di Feng,Emily Zhang,Sam Xu,Junting Pan,Dominik Moritz,Suma Jayaram,Kevin Smith,Dongseong Hwang,Daniel Parilla,Jiaming Hu,You-Cyuan Jhang,Emad Soroush,Fred Hohman,Nan Du,Emma Wang,Sam Dodge,Pragnya Sridhar,Joris Pelemans,Wei Fang,Nina Wenzel,Joseph Yitan Cheng,Hadas Kotek,Chung-Cheng Chiu,Meng Cao,Haijing Fu,Ruixuan Hou,Ke Ye,Diane Zhu,Nikhil Bhendawade,Joseph Astrauskas,Jian Liu,Sai Aitharaju,Wentao Wu,Artsiom Peshko,Hyunjik Kim,Nilesh Shahdadpuri,Andy De Wang,Qi Shan,Piotr Maj,Raul Rea Menacho,Justin Lazarow,Eric Liang Yang,Arsalan Farooq,Donghan Yu,David Güera,Minsik Cho,Kavya Nerella,Yongqiang Wang,Tao Jia,John Park,Jeff Lai,Haotian Zhang,Futang Peng,Daniele Molinari,Aparna Rajamani,Tyler Johnson,Lauren Gardiner,Chao Jia,Violet Yao,Wojciech Kryscinski,Xiujun Li,Shang-Chen Wu*

Main category: cs.LG

TL;DR: 苹果推出两款多语言、多模态基础语言模型，分别用于设备和服务器，经大规模数据训练和优化，在评测中表现优异，还提供新框架，秉持负责任AI理念。


<details>
  <summary>Details</summary>
Motivation: 为苹果设备和服务的智能功能提供支持，推动苹果智能发展。

Method: 采用架构创新优化设备模型，构建新型并行轨迹混合专家变压器搭建服务器模型，使用大规模多语言多模态数据集训练，结合监督微调与强化学习。

Result: 模型支持多种语言，能理解图像和执行工具调用，在公开基准和人工评估中表现出色。

Conclusion: 新模型和框架有良好性能，苹果秉持负责任AI方法保障用户隐私。

Abstract: We introduce two multilingual, multimodal foundation language models that
power Apple Intelligence features across Apple devices and services: i a
3B-parameter on-device model optimized for Apple silicon through architectural
innovations such as KV-cache sharing and 2-bit quantization-aware training; and
ii a scalable server model built on a novel Parallel-Track Mixture-of-Experts
PT-MoE transformer that combines track parallelism, mixture-of-experts sparse
computation, and interleaved global-local attention to deliver high quality
with competitive cost on Apple's Private Cloud Compute platform. Both models
are trained on large-scale multilingual and multimodal datasets sourced via
responsible web crawling, licensed corpora, and high-quality synthetic data,
then further refined with supervised fine-tuning and reinforcement learning on
a new asynchronous platform. The resulting models support several additional
languages while understanding images and executing tool calls. In public
benchmarks and human evaluations, both the server model and the on-device model
match or surpass comparably sized open baselines.
  A new Swift-centric Foundation Models framework exposes guided generation,
constrained tool calling, and LoRA adapter fine-tuning, allowing developers to
integrate these capabilities with a few lines of code. The latest advancements
in Apple Intelligence models are grounded in our Responsible AI approach with
safeguards like content filtering and locale-specific evaluation, as well as
our commitment to protecting our users' privacy with innovations like Private
Cloud Compute.

</details>


### [74] [Learning Pluralistic User Preferences through Reinforcement Learning Fine-tuned Summaries](https://arxiv.org/abs/2507.13579)
*Hyunji Nam,Yanming Wan,Mickel Liu,Jianxun Lian,Natasha Jaques*

Main category: cs.LG

TL;DR: 介绍了新颖框架PLUS，能学习用户偏好总结以实现大语言模型响应个性化，在多方面表现良好且生成的总结有多种优势。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型AI助手日常用例扩展，个性化响应愈发重要，而现有RLHF未考虑用户差异。

Method: 提出PLUS框架，学习用户偏好、特征和过往对话的文本总结，用强化学习训练用户总结模型并同时更新奖励模型，形成在线共同适应循环。

Result: PLUS生成的总结能捕捉用户偏好，在不同用户数据集上对新用户和多样话题有鲁棒性，生成的文本总结可用于零样本个性化，且简洁、便携、易解释和修改。

Conclusion: PLUS框架可实现大语言模型响应个性化，增加对齐过程的透明度和用户控制权。

Abstract: As everyday use cases of large language model (LLM) AI assistants have
expanded, it is becoming increasingly important to personalize responses to
align to different users' preferences and goals. While reinforcement learning
from human feedback (RLHF) is effective at improving LLMs to be generally more
helpful and fluent, it does not account for variability across users, as it
models the entire user population with a single reward model. We present a
novel framework, Preference Learning Using Summarization (PLUS), that learns
text-based summaries of each user's preferences, characteristics, and past
conversations. These summaries condition the reward model, enabling it to make
personalized predictions about the types of responses valued by each user. We
train the user-summarization model with reinforcement learning, and update the
reward model simultaneously, creating an online co-adaptation loop. We show
that in contrast with prior personalized RLHF techniques or with in-context
learning of user information, summaries produced by PLUS capture meaningful
aspects of a user's preferences. Across different pluralistic user datasets, we
show that our method is robust to new users and diverse conversation topics.
Additionally, we demonstrate that the textual summaries generated about users
can be transferred for zero-shot personalization of stronger, proprietary
models like GPT-4. The resulting user summaries are not only concise and
portable, they are easy for users to interpret and modify, allowing for more
transparency and user control in LLM alignment.

</details>


### [75] [Tri-Learn Graph Fusion Network for Attributed Graph Clustering](https://arxiv.org/abs/2507.13620)
*Binxiong Li,Yuefei Wang,Xu Xiang,Xue Li,Binyu Zhao,Heyang Gao,Qinyu Zhao,Xi Yu*

Main category: cs.LG

TL;DR: 现有图数据模型处理大规模复杂图数据存在问题，本文提出Tri - GFN框架，通过独特机制和策略提升性能，在多数据集表现优，可用于新闻分类等领域。


<details>
  <summary>Details</summary>
Motivation: 现有基于GCN的模型处理大规模复杂图数据集存在过平滑和过压缩问题，图Transformer处理异构图数据性能有限，需改进。

Method: 提出Tri - GFN框架，集成GCN、AE和Graph Transformer模块，通过三通道增强模块融合，采用三学习机制和特征融合增强策略。

Result: 在ACM、Reuters和USPS数据集上准确率分别提升约0.87%、14.14%和7.58%。

Conclusion: Tri - GFN框架表现出色，可应用于自动新闻分类、主题检索等领域。

Abstract: In recent years, models based on Graph Convolutional Networks (GCN) have made
significant strides in the field of graph data analysis. However, challenges
such as over-smoothing and over-compression remain when handling large-scale
and complex graph datasets, leading to a decline in clustering quality.
Although the Graph Transformer architecture has mitigated some of these issues,
its performance is still limited when processing heterogeneous graph data. To
address these challenges, this study proposes a novel deep clustering framework
that comprising GCN, Autoencoder (AE), and Graph Transformer, termed the
Tri-Learn Graph Fusion Network (Tri-GFN). This framework enhances the
differentiation and consistency of global and local information through a
unique tri-learning mechanism and feature fusion enhancement strategy. The
framework integrates GCN, AE, and Graph Transformer modules. These components
are meticulously fused by a triple-channel enhancement module, which maximizes
the use of both node attributes and topological structures, ensuring robust
clustering representation. The tri-learning mechanism allows mutual learning
among these modules, while the feature fusion strategy enables the model to
capture complex relationships, yielding highly discriminative representations
for graph clustering. It surpasses many state-of-the-art methods, achieving an
accuracy improvement of approximately 0.87% on the ACM dataset, 14.14 % on the
Reuters dataset, and 7.58 % on the USPS dataset. Due to its outstanding
performance on the Reuters dataset, Tri-GFN can be applied to automatic news
classification, topic retrieval, and related fields.

</details>


### [76] [Generalist Bimanual Manipulation via Foundation Video Diffusion Models](https://arxiv.org/abs/2507.12898)
*Yao Feng,Hengkai Tan,Xinyi Mao,Guodong Liu,Shuhe Huang,Chendong Xiang,Hang Su,Jun Zhu*

Main category: cs.LG

TL;DR: 提出VIDAR框架，结合视频预训练和掩码逆动力学模型，用少量数据实现双手机器人操作在不同任务和背景的泛化。


<details>
  <summary>Details</summary>
Motivation: 双手机器人操作存在数据稀缺和具身异质性问题，阻碍进一步扩展。

Method: 提出VIDAR两阶段框架，在750K多视角视频上预训练视频扩散模型，用掩码逆动力学模型提取动作相关信息。

Result: 仅用20分钟人类演示（典型数据需求的1%），VIDAR在未见任务和背景上有强语义理解，超越现有方法。

Conclusion: 视频基础模型结合掩码动作预测，能在多样现实场景实现可扩展和可泛化的机器人操作。

Abstract: Bimanual robotic manipulation, which involves the coordinated control of two
robotic arms, is foundational for solving challenging tasks. Despite recent
progress in general-purpose manipulation, data scarcity and embodiment
heterogeneity remain serious obstacles to further scaling up in bimanual
settings. In this paper, we introduce VIdeo Diffusion for Action Reasoning
(VIDAR), a two-stage framework that leverages large-scale, diffusion-based
video pre-training and a novel masked inverse dynamics model for action
prediction. We pre-train the video diffusion model on 750K multi-view videos
from three real-world bimanual robot platforms, utilizing a unified observation
space that encodes robot, camera, task, and scene contexts. Our masked inverse
dynamics model learns masks to extract action-relevant information from
generated trajectories without requiring pixel-level labels, and the masks can
effectively generalize to unseen backgrounds. Our experiments demonstrate that
with only 20 minutes of human demonstrations on an unseen robot platform (only
1% of typical data requirements), VIDAR generalizes to unseen tasks and
backgrounds with strong semantic understanding, surpassing state-of-the-art
methods. Our findings highlight the potential of video foundation models,
coupled with masked action prediction, to enable scalable and generalizable
robotic manipulation in diverse real-world settings.

</details>


### [77] [A Comprehensive Review of Transformer-based language models for Protein Sequence Analysis and Design](https://arxiv.org/abs/2507.13646)
*Nimisha Ghosh,Daniele Santoni,Debaleena Nawn,Eleonora Ottaviani,Giovanni Felici*

Main category: cs.LG

TL;DR: 本文探讨基于Transformer的模型在蛋白质序列分析和设计方面的最新进展，分析相关研究优缺点，指出不足并探索未来方向。


<details>
  <summary>Details</summary>
Motivation: 基于Transformer语言模型在自然语言处理领域的成功及其在生物信息学等领域的应用，探讨其在蛋白质序列分析和设计中的进展。

Method: 对大量相关应用的研究进行讨论和分析。

Result: 分析了相关研究在基因本体、蛋白质功能和结构识别、从头蛋白质生成和蛋白质结合等应用中的优缺点。

Conclusion: 本综述有助于该领域研究人员了解现状并指导未来研究。

Abstract: The impact of Transformer-based language models has been unprecedented in
Natural Language Processing (NLP). The success of such models has also led to
their adoption in other fields including bioinformatics. Taking this into
account, this paper discusses recent advances in Transformer-based models for
protein sequence analysis and design. In this review, we have discussed and
analysed a significant number of works pertaining to such applications. These
applications encompass gene ontology, functional and structural protein
identification, generation of de novo proteins and binding of proteins. We
attempt to shed light on the strength and weaknesses of the discussed works to
provide a comprehensive insight to readers. Finally, we highlight shortcomings
in existing research and explore potential avenues for future developments. We
believe that this review will help researchers working in this field to have an
overall idea of the state of the art in this field, and to orient their future
studies.

</details>


### [78] [Kolmogorov-Arnold Networks-based GRU and LSTM for Loan Default Early Prediction](https://arxiv.org/abs/2507.13685)
*Yue Yang,Zihan Su,Ying Zhang,Chang Chuan Goh,Yuxiang Lin,Anthony Graham Bellotti,Boon Giin Lee*

Main category: cs.LG

TL;DR: 研究提出GRU - KAN和LSTM - KAN架构用于贷款违约提前预警，效果优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列异常检测方法在提前三月以上预测贷款违约时存在精度不足、依赖特定时间框架等问题，限制实际应用，需提升模型预测能力。

Method: 引入GRU - KAN和LSTM - KAN架构，将KAN与GRU、LSTM网络融合，并在不同特征窗口长度、样本大小和提前预测间隔下与基线模型对比评估。

Result: 提出的模型提前三个月预测准确率超92%，提前八个月超88%，显著优于现有基线模型。

Conclusion: 所提出的GRU - KAN和LSTM - KAN架构在贷款违约提前预警方面表现出色，能有效解决现有方法的问题。

Abstract: This study addresses a critical challenge in time series anomaly detection:
enhancing the predictive capability of loan default models more than three
months in advance to enable early identification of default events, helping
financial institutions implement preventive measures before risk events
materialize. Existing methods have significant drawbacks, such as their lack of
accuracy in early predictions and their dependence on training and testing
within the same year and specific time frames. These issues limit their
practical use, particularly with out-of-time data. To address these, the study
introduces two innovative architectures, GRU-KAN and LSTM-KAN, which merge
Kolmogorov-Arnold Networks (KAN) with Gated Recurrent Units (GRU) and Long
Short-Term Memory (LSTM) networks. The proposed models were evaluated against
the baseline models (LSTM, GRU, LSTM-Attention, and LSTM-Transformer) in terms
of accuracy, precision, recall, F1 and AUC in different lengths of feature
window, sample sizes, and early prediction intervals. The results demonstrate
that the proposed model achieves a prediction accuracy of over 92% three months
in advance and over 88% eight months in advance, significantly outperforming
existing baselines.

</details>


### [79] [Binarizing Physics-Inspired GNNs for Combinatorial Optimization](https://arxiv.org/abs/2507.13703)
*Martin Krutský,Gustav Šír,Vyacheslav Kungurtsev,Georgios Korpas*

Main category: cs.LG

TL;DR: 指出PI - GNNs在组合问题图密度增加时性能下降，提出改进方法并验证其有效性


<details>
  <summary>Details</summary>
Motivation: 现有PI - GNNs在组合问题图密度增加时性能系统下降，需解决松弛模型输出与二进制值问题解之间的差异

Method: 基于模糊逻辑和二值化神经网络见解，提出PI - GNNs中朴素策略的替代方法

Result: 提出的方法组合显著提高了PI - GNNs在密度增加设置下的性能

Conclusion: 所提方法能有效改善PI - GNNs在高密度组合问题中的性能

Abstract: Physics-inspired graph neural networks (PI-GNNs) have been utilized as an
efficient unsupervised framework for relaxing combinatorial optimization
problems encoded through a specific graph structure and loss, reflecting
dependencies between the problem's variables. While the framework has yielded
promising results in various combinatorial problems, we show that the
performance of PI-GNNs systematically plummets with an increasing density of
the combinatorial problem graphs. Our analysis reveals an interesting phase
transition in the PI-GNNs' training dynamics, associated with degenerate
solutions for the denser problems, highlighting a discrepancy between the
relaxed, real-valued model outputs and the binary-valued problem solutions. To
address the discrepancy, we propose principled alternatives to the naive
strategy used in PI-GNNs by building on insights from fuzzy logic and binarized
neural networks. Our experiments demonstrate that the portfolio of proposed
methods significantly improves the performance of PI-GNNs in increasingly dense
settings.

</details>


### [80] [Learning Deformable Body Interactions With Adaptive Spatial Tokenization](https://arxiv.org/abs/2507.13707)
*Hao Wang,Yu Liu,Daniel Biggs,Haoru Wang,Jiandong Yu,Ping Huang*

Main category: cs.LG

TL;DR: 提出自适应空间标记化（AST）方法用于高效表示物理状态，在可变形体交互模拟中表现出色并贡献新数据集。


<details>
  <summary>Details</summary>
Motivation: 基于图神经网络的学习方法在建模可变形体交互时存在可扩展性问题，计算量大且不适用于大规模网格。

Method: 将模拟空间划分为网格单元，将非结构化网格映射到结构化网格，使用交叉注意力模块将稀疏单元映射为固定长度嵌入作为标记，用自注意力模块在潜在空间预测下一状态。

Result: 该方法在可变形体交互建模上显著优于现有方法，在超100,000节点的大规模模拟中仍有效。

Conclusion: 所提方法利用标记化效率和注意力机制表达能力，实现准确可扩展的模拟结果，新数据集可支持该领域未来研究。

Abstract: Simulating interactions between deformable bodies is vital in fields like
material science, mechanical design, and robotics. While learning-based methods
with Graph Neural Networks (GNNs) are effective at solving complex physical
systems, they encounter scalability issues when modeling deformable body
interactions. To model interactions between objects, pairwise global edges have
to be created dynamically, which is computationally intensive and impractical
for large-scale meshes. To overcome these challenges, drawing on insights from
geometric representations, we propose an Adaptive Spatial Tokenization (AST)
method for efficient representation of physical states. By dividing the
simulation space into a grid of cells and mapping unstructured meshes onto this
structured grid, our approach naturally groups adjacent mesh nodes. We then
apply a cross-attention module to map the sparse cells into a compact,
fixed-length embedding, serving as tokens for the entire physical state.
Self-attention modules are employed to predict the next state over these tokens
in latent space. This framework leverages the efficiency of tokenization and
the expressive power of attention mechanisms to achieve accurate and scalable
simulation results. Extensive experiments demonstrate that our method
significantly outperforms state-of-the-art approaches in modeling deformable
body interactions. Notably, it remains effective on large-scale simulations
with meshes exceeding 100,000 nodes, where existing methods are hindered by
computational limitations. Additionally, we contribute a novel large-scale
dataset encompassing a wide range of deformable body interactions to support
future research in this area.

</details>


### [81] [Benchmarking of EEG Analysis Techniques for Parkinson's Disease Diagnosis: A Comparison between Traditional ML Methods and Foundation DL Methods](https://arxiv.org/abs/2507.13716)
*Danilo Avola,Andrea Bernardini,Giancarlo Crocetti,Andrea Ladogana,Mario Lezoche,Maurizio Mancini,Daniele Pannone,Amedeo Ranaldi*

Main category: cs.LG

TL;DR: 研究对传统机器学习和深度学习模型进行基准测试，用公开数据集对帕金森病分类，发现CNN - LSTM等模型表现好，传统分类器也有强预测能力，为后续研究提供框架。


<details>
  <summary>Details</summary>
Motivation: 帕金森病早期诊断重要，脑电图检测有挑战，需开发可靠自动诊断模型，确定最佳学习方法。

Method: 采用统一七步预处理流程，应用一致的受试者交叉验证和评估标准，对传统机器学习和深度学习模型进行基准测试。

Result: 基线深度学习架构（尤其是CNN - LSTM模型）表现最佳，一些传统分类器（如XGBoost）也有强预测准确性和校准决策边界。

Conclusion: 为未来开发和评估更复杂或专业架构的研究提供可靠参考框架，确保基于脑电图神经诊断领域的科学严谨性和可重复性。

Abstract: Parkinson's Disease PD is a progressive neurodegenerative disorder that
affects motor and cognitive functions with early diagnosis being critical for
effective clinical intervention Electroencephalography EEG offers a noninvasive
and costeffective means of detecting PDrelated neural alterations yet the
development of reliable automated diagnostic models remains a challenge In this
study we conduct a systematic benchmark of traditional machine learning ML and
deep learning DL models for classifying PD using a publicly available oddball
task dataset Our aim is to lay the groundwork for developing an effective
learning system and to determine which approach produces the best results We
implement a unified sevenstep preprocessing pipeline and apply consistent
subjectwise crossvalidation and evaluation criteria to ensure comparability
across models Our results demonstrate that while baseline deep learning
architectures particularly CNNLSTM models achieve the best performance compared
to other deep learning architectures underlining the importance of capturing
longrange temporal dependencies several traditional classifiers such as XGBoost
also offer strong predictive accuracy and calibrated decision boundaries By
rigorously comparing these baselines our work provides a solid reference
framework for future studies aiming to develop and evaluate more complex or
specialized architectures Establishing a reliable set of baseline results is
essential to contextualize improvements introduced by novel methods ensuring
scientific rigor and reproducibility in the evolving field of EEGbased
neurodiagnostics

</details>


### [82] [Bi-GRU Based Deception Detection using EEG Signals](https://arxiv.org/abs/2507.13718)
*Danilo Avola,Muhammad Yasir Bilal,Emad Emam,Cristina Lakasz,Daniele Pannone,Amedeo Ranaldi*

Main category: cs.LG

TL;DR: 研究用深度学习方法基于EEG信号进行欺骗检测，Bi - GRU网络取得97%测试准确率，证明双向时间建模用于欺骗检测有效。


<details>
  <summary>Details</summary>
Motivation: 欺骗检测在安全、心理和法医等领域是重大挑战，需有效方法。

Method: 采用深度学习方法，使用Bag - of - Lies数据集中的EEG信号，训练双向门控循环单元（Bi - GRU）神经网络进行二元分类。

Result: 模型测试准确率达97%，在两类中都有高精确率、召回率和F1分数。

Conclusion: 双向时间建模用于基于EEG的欺骗检测有效，有实时应用潜力，可探索高级神经架构。

Abstract: Deception detection is a significant challenge in fields such as security,
psychology, and forensics. This study presents a deep learning approach for
classifying deceptive and truthful behavior using ElectroEncephaloGram (EEG)
signals from the Bag-of-Lies dataset, a multimodal corpus designed for
naturalistic, casual deception scenarios. A Bidirectional Gated Recurrent Unit
(Bi-GRU) neural network was trained to perform binary classification of EEG
samples. The model achieved a test accuracy of 97\%, along with high precision,
recall, and F1-scores across both classes. These results demonstrate the
effectiveness of using bidirectional temporal modeling for EEG-based deception
detection and suggest potential for real-time applications and future
exploration of advanced neural architectures.

</details>


### [83] [Graph-Structured Data Analysis of Component Failure in Autonomous Cargo Ships Based on Feature Fusion](https://arxiv.org/abs/2507.13721)
*Zizhao Zhang,Tianxiang Zhao,Yu Sun,Liping Sun,Jichuan Kang*

Main category: cs.LG

TL;DR: 本文提出混合特征融合框架构建故障模式图结构数据集，用改进算法提升检索效率，验证模型分类及预测效果良好，为货船故障分析等提供支持。


<details>
  <summary>Details</summary>
Motivation: 应对自主货船组件故障级联反应挑战和应急决策不确定性。

Method: 提出混合特征融合框架，用改进布谷鸟搜索算法提升检索效率，构建分层特征融合框架，用不同方法处理特征、故障模式等。

Result: 改进算法比NSGA - II和CSA检索效率分别提升7.1%和3.4%；GATE - GNN模型分类准确率0.735；轮廓系数0.641；岸基气象服务系统F1分数0.93。

Conclusion: 为自主货船故障分析、故障诊断、风险评估和智能决策系统提供可靠支持。

Abstract: To address the challenges posed by cascading reactions caused by component
failures in autonomous cargo ships (ACS) and the uncertainties in emergency
decision-making, this paper proposes a novel hybrid feature fusion framework
for constructing a graph-structured dataset of failure modes. By employing an
improved cuckoo search algorithm (HN-CSA), the literature retrieval efficiency
is significantly enhanced, achieving improvements of 7.1% and 3.4% compared to
the NSGA-II and CSA search algorithms, respectively. A hierarchical feature
fusion framework is constructed, using Word2Vec encoding to encode
subsystem/component features, BERT-KPCA to process failure modes/reasons, and
Sentence-BERT to quantify the semantic association between failure impact and
emergency decision-making. The dataset covers 12 systems, 1,262 failure modes,
and 6,150 propagation paths. Validation results show that the GATE-GNN model
achieves a classification accuracy of 0.735, comparable to existing benchmarks.
Additionally, a silhouette coefficient of 0.641 indicates that the features are
highly distinguishable. In the label prediction results, the Shore-based
Meteorological Service System achieved an F1 score of 0.93, demonstrating high
prediction accuracy. This paper not only provides a solid foundation for
failure analysis in autonomous cargo ships but also offers reliable support for
fault diagnosis, risk assessment, and intelligent decision-making systems. The
link to the dataset is
https://github.com/wojiufukele/Graph-Structured-about-CSA.

</details>


### [84] [Adversarial Training Improves Generalization Under Distribution Shifts in Bioacoustics](https://arxiv.org/abs/2507.13727)
*René Heinrich,Lukas Rauch,Bernhard Sick,Christoph Scholz*

Main category: cs.LG

TL;DR: 研究不同对抗训练策略对音频分类泛化与鲁棒性的影响，用鸟声分类基准评估，输出空间攻击训练效果好。


<details>
  <summary>Details</summary>
Motivation: 探索对抗训练在音频分类数据分布大变化时对泛化性的影响。

Method: 聚焦ConvNeXt和AudioProtoPNet两种模型架构，用鸟声分类基准评估，采用输出空间和嵌入空间攻击的对抗训练策略，评估AudioProtoPNet原型稳定性。

Result: 输出空间攻击的对抗训练使干净测试数据性能平均相对提高10.5%，增强模型对抗鲁棒性。

Conclusion: 对抗训练在挑战性音频分类场景中，对分布变化和对抗攻击有增强鲁棒性的潜力。

Abstract: Adversarial training is a promising strategy for enhancing model robustness
against adversarial attacks. However, its impact on generalization under
substantial data distribution shifts in audio classification remains largely
unexplored. To address this gap, this work investigates how different
adversarial training strategies improve generalization performance and
adversarial robustness in audio classification. The study focuses on two model
architectures: a conventional convolutional neural network (ConvNeXt) and an
inherently interpretable prototype-based model (AudioProtoPNet). The approach
is evaluated using a challenging bird sound classification benchmark. This
benchmark is characterized by pronounced distribution shifts between training
and test data due to varying environmental conditions and recording methods, a
common real-world challenge. The investigation explores two adversarial
training strategies: one based on output-space attacks that maximize the
classification loss function, and another based on embedding-space attacks
designed to maximize embedding dissimilarity. These attack types are also used
for robustness evaluation. Additionally, for AudioProtoPNet, the study assesses
the stability of its learned prototypes under targeted embedding-space attacks.
Results show that adversarial training, particularly using output-space
attacks, improves clean test data performance by an average of 10.5% relative
and simultaneously strengthens the adversarial robustness of the models. These
findings, although derived from the bird sound domain, suggest that adversarial
training holds potential to enhance robustness against both strong distribution
shifts and adversarial attacks in challenging audio classification settings.

</details>


### [85] [SamGoG: A Sampling-Based Graph-of-Graphs Framework for Imbalanced Graph Classification](https://arxiv.org/abs/2507.13741)
*Shangyou Wang,Zezhong Ding,Xike Xie*

Main category: cs.LG

TL;DR: 提出SamGoG框架解决图分类中类别和图大小不平衡问题，实验显示有性能提升和训练加速。


<details>
  <summary>Details</summary>
Motivation: 现实世界图存在类别和图大小不平衡问题，现有方法只能解决一种或计算成本高。

Method: 提出基于采样的Graph-of-Graphs（GoG）学习框架SamGoG，通过重要性采样构建多个GoG并依次训练，融入可学习的成对相似度和自适应GoG节点度。

Result: 在基准数据集上实现了高达15.66%的准确率提升和6.7倍的训练加速。

Conclusion: SamGoG能有效缓解类别和图大小不平衡，可与多种下游GNN集成，实现了最先进的性能。

Abstract: Graph Neural Networks (GNNs) have shown remarkable success in graph
classification tasks by capturing both structural and feature-based
representations. However, real-world graphs often exhibit two critical forms of
imbalance: class imbalance and graph size imbalance. These imbalances can bias
the learning process and degrade model performance. Existing methods typically
address only one type of imbalance or incur high computational costs. In this
work, we propose SamGoG, a sampling-based Graph-of-Graphs (GoG) learning
framework that effectively mitigates both class and graph size imbalance.
SamGoG constructs multiple GoGs through an efficient importance-based sampling
mechanism and trains on them sequentially. This sampling mechanism incorporates
the learnable pairwise similarity and adaptive GoG node degree to enhance edge
homophily, thus improving downstream model quality. SamGoG can seamlessly
integrate with various downstream GNNs, enabling their efficient adaptation for
graph classification tasks. Extensive experiments on benchmark datasets
demonstrate that SamGoG achieves state-of-the-art performance with up to a
15.66% accuracy improvement with 6.7$\times$ training acceleration.

</details>


### [86] [Search-Optimized Quantization in Biomedical Ontology Alignment](https://arxiv.org/abs/2507.13742)
*Oussama Bouaggad,Natalia Grabar*

Main category: cs.LG

TL;DR: 本文针对AI模型部署挑战，引入本体对齐系统方法，经优化在DEFT 2020任务中达新水平，提速20倍、降内存70%。


<details>
  <summary>Details</summary>
Motivation: AI模型规模和计算需求大，在边缘设备或资源受限环境部署面临能耗、内存和延迟挑战，需高效模型优化技术。

Method: 采用监督的基于transformer的模型进行本体对齐，利用Microsoft Olive搜索优化，结合Intel Neural Compressor和IPEX进行动态量化。

Result: 在DEFT 2020评估活动的两个任务上取得新的最优成果，推理速度平均提升20倍，内存使用减少约70%，性能指标不变。

Conclusion: 所提出的优化方法有效，能在保证性能的同时显著提升推理速度和降低内存使用。

Abstract: In the fast-moving world of AI, as organizations and researchers develop more
advanced models, they face challenges due to their sheer size and computational
demands. Deploying such models on edge devices or in resource-constrained
environments adds further challenges related to energy consumption, memory
usage and latency. To address these challenges, emerging trends are shaping the
future of efficient model optimization techniques. From this premise, by
employing supervised state-of-the-art transformer-based models, this research
introduces a systematic method for ontology alignment, grounded in cosine-based
semantic similarity between a biomedical layman vocabulary and the Unified
Medical Language System (UMLS) Metathesaurus. It leverages Microsoft Olive to
search for target optimizations among different Execution Providers (EPs) using
the ONNX Runtime backend, followed by an assembled process of dynamic
quantization employing Intel Neural Compressor and IPEX (Intel Extension for
PyTorch). Through our optimization process, we conduct extensive assessments on
the two tasks from the DEFT 2020 Evaluation Campaign, achieving a new
state-of-the-art in both. We retain performance metrics intact, while attaining
an average inference speed-up of 20x and reducing memory usage by approximately
70%.

</details>


### [87] [MolPIF: A Parameter Interpolation Flow Model for Molecule Generation](https://arxiv.org/abs/2507.13762)
*Yaowei Jin,Junjie Wang,Wenkai Xiang,Duanhua Cao,Dan Teng,Zhehuan Fan,Jiacheng Xiong,Xia Sheng,Chuanlong Zeng,Mingyue Zheng,Qian Shi*

Main category: cs.LG

TL;DR: 本文提出新颖的Parameter Interpolation Flow模型（PIF）用于分子生成，开发MolPIF用于基于结构的药物设计，表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯流网络（BFNs）在设计分布转换路径上有局限，且基于参数空间的更简单高效模型未被探索。

Method: 提出具有详细理论基础、训练和推理程序的PIF模型，并开发MolPIF用于基于结构的药物设计。

Result: MolPIF在多种指标上表现优于基线。

Conclusion: 验证了基于参数空间的分子生成建模范式的有效性，为模型设计提供新视角。

Abstract: Advances in deep learning for molecular generation show promise in
accelerating drug discovery. Bayesian Flow Networks (BFNs) have recently shown
impressive performance across diverse chemical tasks, with their success often
ascribed to the paradigm of modeling in a low-variance parameter space.
However, the Bayesian inference-based strategy imposes limitations on designing
more flexible distribution transformation pathways, making it challenging to
adapt to diverse data distributions and varied task requirements. Furthermore,
the potential for simpler, more efficient parameter-space-based models is
unexplored. To address this, we propose a novel Parameter Interpolation Flow
model (named PIF) with detailed theoretical foundation, training, and inference
procedures. We then develop MolPIF for structure-based drug design,
demonstrating its superior performance across diverse metrics compared to
baselines. This work validates the effectiveness of parameter-space-based
generative modeling paradigm for molecules and offers new perspectives for
model design.

</details>


### [88] [Dual-Center Graph Clustering with Neighbor Distribution](https://arxiv.org/abs/2507.13765)
*Enhao Cheng,Shoujia Zhang,Jianhua Yin,Li Jin,Liqiang Nie*

Main category: cs.LG

TL;DR: 提出基于邻域分布特性的双中心图聚类方法DCGC，实验证明其性能优越


<details>
  <summary>Details</summary>
Motivation: 现有目标导向图聚类方法使用伪标签不可靠，且单中心优化的指导不完整、可信度低

Method: 提出包含邻域分布表示学习和双中心优化的DCGC方法，用邻域分布作监督信号挖掘难负样本，引入邻域分布中心与特征中心构建双目标分布进行双中心优化

Result: 广泛的实验和分析表明所提方法性能优越、效果良好

Conclusion: 所提出的DCGC方法有效可行，优于现有方法

Abstract: Graph clustering is crucial for unraveling intricate data structures, yet it
presents significant challenges due to its unsupervised nature. Recently,
goal-directed clustering techniques have yielded impressive results, with
contrastive learning methods leveraging pseudo-label garnering considerable
attention. Nonetheless, pseudo-label as a supervision signal is unreliable and
existing goal-directed approaches utilize only features to construct a
single-target distribution for single-center optimization, which lead to
incomplete and less dependable guidance. In our work, we propose a novel
Dual-Center Graph Clustering (DCGC) approach based on neighbor distribution
properties, which includes representation learning with neighbor distribution
and dual-center optimization. Specifically, we utilize neighbor distribution as
a supervision signal to mine hard negative samples in contrastive learning,
which is reliable and enhances the effectiveness of representation learning.
Furthermore, neighbor distribution center is introduced alongside feature
center to jointly construct a dual-target distribution for dual-center
optimization. Extensive experiments and analysis demonstrate superior
performance and effectiveness of our proposed method.

</details>


### [89] [On-the-Fly Fine-Tuning of Foundational Neural Network Potentials: A Bayesian Neural Network Approach](https://arxiv.org/abs/2507.13805)
*Tim Rensmeyer,Denis Kramer,Oliver Niggemann*

Main category: cs.LG

TL;DR: 传统原子间机器学习力场训练数据集生成有计算负担，微调预训练基础模型可减少训练数据，但创建合适数据集仍具挑战。本文引入基于贝叶斯神经网络的微调方法和动态工作流解决模型不确定性评估问题，能保持精度并检测罕见事件。


<details>
  <summary>Details</summary>
Motivation: 传统生成训练数据集有计算负担，微调基础模型创建合适数据集仍有挑战，且基础模型缺乏不确定性量化，难以应用动态学习进行微调。

Method: 引入基于贝叶斯神经网络方法的微调方法和后续动态工作流。

Result: 能在保持预设精度的同时自动微调模型，检测如过渡态等罕见事件并提高采样率。

Conclusion: 所提出的方法克服了基础模型微调时不确定性评估的挑战，可用于处理含罕见事件的系统。

Abstract: Due to the computational complexity of evaluating interatomic forces from
first principles, the creation of interatomic machine learning force fields has
become a highly active field of research. However, the generation of training
datasets of sufficient size and sample diversity itself comes with a
computational burden that can make this approach impractical for modeling rare
events or systems with a large configuration space. Fine-tuning foundation
models that have been pre-trained on large-scale material or molecular
databases offers a promising opportunity to reduce the amount of training data
necessary to reach a desired level of accuracy. However, even if this approach
requires less training data overall, creating a suitable training dataset can
still be a very challenging problem, especially for systems with rare events
and for end-users who don't have an extensive background in machine learning.
In on-the-fly learning, the creation of a training dataset can be largely
automated by using model uncertainty during the simulation to decide if the
model is accurate enough or if a structure should be recalculated with
classical methods and used to update the model. A key challenge for applying
this form of active learning to the fine-tuning of foundation models is how to
assess the uncertainty of those models during the fine-tuning process, even
though most foundation models lack any form of uncertainty quantification. In
this paper, we overcome this challenge by introducing a fine-tuning approach
based on Bayesian neural network methods and a subsequent on-the-fly workflow
that automatically fine-tunes the model while maintaining a pre-specified
accuracy and can detect rare events such as transition states and sample them
at an increased rate relative to their occurrence.

</details>


### [90] [Scalable Submodular Policy Optimization via Pruned Submodularity Graph](https://arxiv.org/abs/2507.13834)
*Aditi Anand,Suman Banerjee,Dildar Ali*

Main category: cs.LG

TL;DR: 本文研究奖励函数为子模函数的强化学习问题，提出基于剪枝子模图的方法，实验表明该方法比基线方法获得更多奖励。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习问题奖励函数为可加的，但现实中很多问题奖励函数遵循收益递减，可建模为子模函数，因此研究奖励函数为子模函数的强化学习问题。

Method: 提出基于剪枝子模图的方法，该方法能在可行计算时间内提供可证明的近似解，并分析其时间、空间需求和性能保证。

Result: 在基准的智能体 - 环境设置上进行实验，所提方法得到的策略比基线方法获得更多奖励。

Conclusion: 所提基于剪枝子模图的方法在奖励函数为子模函数的强化学习问题中表现优于基线方法。

Abstract: In Reinforcement Learning (abbreviated as RL), an agent interacts with the
environment via a set of possible actions, and a reward is generated from some
unknown distribution. The task here is to find an optimal set of actions such
that the reward after a certain time step gets maximized. In a traditional
setup, the reward function in an RL Problem is considered additive. However, in
reality, there exist many problems, including path planning, coverage control,
etc., the reward function follows the diminishing return, which can be modeled
as a submodular function. In this paper, we study a variant of the RL Problem
where the reward function is submodular, and our objective is to find an
optimal policy such that this reward function gets maximized. We have proposed
a pruned submodularity graph-based approach that provides a provably
approximate solution in a feasible computation time. The proposed approach has
been analyzed to understand its time and space requirements as well as a
performance guarantee. We have experimented with a benchmark agent-environment
setup, which has been used for similar previous studies, and the results are
reported. From the results, we observe that the policy obtained by our proposed
approach leads to more reward than the baseline methods.

</details>


### [91] [Self-supervised learning on gene expression data](https://arxiv.org/abs/2507.13912)
*Kevin Dradjat,Massinissa Hamidi,Pierre Bartet,Blaise Hanczar*

Main category: cs.LG

TL;DR: 本文探讨自监督学习方法用于批量基因表达数据的表型预测，展示其能有效捕捉信息、提升预测准确性，还分析各方法优缺点并给出使用建议和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习和深度学习依赖大量有标签数据，获取成本高且耗时，自监督学习可直接从无标签数据结构中提取信息，有望克服这些局限。

Method: 选择三种基于不同方法的自监督学习方法，利用多个公开可用的基因表达数据集进行实验。

Result: 所选自监督学习方法能有效捕捉复杂信息，提升表型预测准确性，优于传统监督模型，且减少对注释数据的依赖。

Conclusion: 自监督学习方法在基因表达数据分析领域有应用潜力，可提升表型预测效果，研究还分析了各方法优缺点、给出使用建议并指明未来研究方向。

Abstract: Predicting phenotypes from gene expression data is a crucial task in
biomedical research, enabling insights into disease mechanisms, drug responses,
and personalized medicine. Traditional machine learning and deep learning rely
on supervised learning, which requires large quantities of labeled data that
are costly and time-consuming to obtain in the case of gene expression data.
Self-supervised learning has recently emerged as a promising approach to
overcome these limitations by extracting information directly from the
structure of unlabeled data. In this study, we investigate the application of
state-of-the-art self-supervised learning methods to bulk gene expression data
for phenotype prediction. We selected three self-supervised methods, based on
different approaches, to assess their ability to exploit the inherent structure
of the data and to generate qualitative representations which can be used for
downstream predictive tasks. By using several publicly available gene
expression datasets, we demonstrate how the selected methods can effectively
capture complex information and improve phenotype prediction accuracy. The
results obtained show that self-supervised learning methods can outperform
traditional supervised models besides offering significant advantage by
reducing the dependency on annotated data. We provide a comprehensive analysis
of the performance of each method by highlighting their strengths and
limitations. We also provide recommendations for using these methods depending
on the case under study. Finally, we outline future research directions to
enhance the application of self-supervised learning in the field of gene
expression data analysis. This study is the first work that deals with bulk
RNA-Seq data and self-supervised learning.

</details>


### [92] [Reframing attention as a reinforcement learning problem for causal discovery](https://arxiv.org/abs/2507.13920)
*Turan Orujlu,Christian Gumbsch,Martin V. Butz,Charley M Wu*

Main category: cs.LG

TL;DR: 本文引入因果过程框架和模型，在强化学习环境下将注意力机制重新表述以推断可解释因果过程，在因果表示学习和智能体性能上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络因果模型大多假设静态因果图，忽略因果交互的动态性，需要新方法解决。

Method: 引入因果过程框架和模型，在强化学习设置下重新表述注意力机制，使用强化学习智能体构建因果图假设。

Result: 在强化学习环境中，在因果表示学习和智能体性能上优于当前替代方法，能恢复动态因果过程图。

Conclusion: 所提出的方法在处理动态因果结构方面有效，能提升因果表示学习和智能体性能。

Abstract: Formal frameworks of causality have operated largely parallel to modern
trends in deep reinforcement learning (RL). However, there has been a revival
of interest in formally grounding the representations learned by neural
networks in causal concepts. Yet, most attempts at neural models of causality
assume static causal graphs and ignore the dynamic nature of causal
interactions. In this work, we introduce Causal Process framework as a novel
theory for representing dynamic hypotheses about causal structure. Furthermore,
we present Causal Process Model as an implementation of this framework. This
allows us to reformulate the attention mechanism popularized by Transformer
networks within an RL setting with the goal to infer interpretable causal
processes from visual observations. Here, causal inference corresponds to
constructing a causal graph hypothesis which itself becomes an RL task nested
within the original RL problem. To create an instance of such hypothesis, we
employ RL agents. These agents establish links between units similar to the
original Transformer attention mechanism. We demonstrate the effectiveness of
our approach in an RL environment where we outperform current alternatives in
causal representation learning and agent performance, and uniquely recover
graphs of dynamic causal processes.

</details>


### [93] [MoDyGAN: Combining Molecular Dynamics With GANs to Investigate Protein Conformational Space](https://arxiv.org/abs/2507.13950)
*Jingbo Liang,Bruna Jacobson*

Main category: cs.LG

TL;DR: 提出MoDyGAN管道结合MD模拟和GAN探索蛋白质构象空间，用实例验证其有效性，表明蛋白质图像化表示利于应用深度学习技术。


<details>
  <summary>Details</summary>
Motivation: 基于动力学物理模拟计算成本高，难以广泛探索蛋白质构象景观，需新方法。

Method: 提出MoDyGAN管道，包含生成器和细化模块，采用创新表示技术将3D蛋白质结构转换为2D矩阵。

Result: 用三个刚性蛋白验证能生成合理新构象，用十丙氨酸表明潜空间插值与SMD模拟轨迹相符。

Conclusion: 蛋白质图像化表示为生物分子模拟应用深度学习技术带来新可能，框架有扩展到其他复杂3D结构的潜力。

Abstract: Extensively exploring protein conformational landscapes remains a major
challenge in computational biology due to the high computational cost involved
in dynamic physics-based simulations. In this work, we propose a novel
pipeline, MoDyGAN, that leverages molecular dynamics (MD) simulations and
generative adversarial networks (GANs) to explore protein conformational
spaces. MoDyGAN contains a generator that maps Gaussian distributions into
MD-derived protein trajectories, and a refinement module that combines ensemble
learning with a dual-discriminator to further improve the plausibility of
generated conformations. Central to our approach is an innovative
representation technique that reversibly transforms 3D protein structures into
2D matrices, enabling the use of advanced image-based GAN architectures. We use
three rigid proteins to demonstrate that MoDyGAN can generate plausible new
conformations. We also use deca-alanine as a case study to show that
interpolations within the latent space closely align with trajectories obtained
from steered molecular dynamics (SMD) simulations. Our results suggest that
representing proteins as image-like data unlocks new possibilities for applying
advanced deep learning techniques to biomolecular simulation, leading to an
efficient sampling of conformational states. Additionally, the proposed
framework holds strong potential for extension to other complex 3D structures.

</details>


### [94] [Robust Anomaly Detection with Graph Neural Networks using Controllability](https://arxiv.org/abs/2507.13954)
*Yifan Wei,Anwar Said,Waseem Abbas,Xenofon Koutsoukos*

Main category: cs.LG

TL;DR: 本文提出将平均可控性融入基于图的框架以改进异常检测性能，经评估方法有效。


<details>
  <summary>Details</summary>
Motivation: 复杂领域异常检测需大量标注数据且样本不平衡，异常数据稀缺，需创新策略提升有限信息下的模型学习能力。

Method: 提出两种将平均可控性融入基于图框架的方法：将其作为边权重；编码为独热边属性向量。

Result: 在真实和合成网络上与六个最先进基线对比，所提方法在识别异常方面性能提升。

Conclusion: 集成平均可控性作为额外指标可应对稀疏和不平衡数据集的异常检测挑战。

Abstract: Anomaly detection in complex domains poses significant challenges due to the
need for extensive labeled data and the inherently imbalanced nature of
anomalous versus benign samples. Graph-based machine learning models have
emerged as a promising solution that combines attribute and relational data to
uncover intricate patterns. However, the scarcity of anomalous data exacerbates
the challenge, which requires innovative strategies to enhance model learning
with limited information. In this paper, we hypothesize that the incorporation
of the influence of the nodes, quantified through average controllability, can
significantly improve the performance of anomaly detection. We propose two
novel approaches to integrate average controllability into graph-based
frameworks: (1) using average controllability as an edge weight and (2)
encoding it as a one-hot edge attribute vector. Through rigorous evaluation on
real-world and synthetic networks with six state-of-the-art baselines, our
proposed methods demonstrate improved performance in identifying anomalies,
highlighting the critical role of controllability measures in enhancing the
performance of graph machine learning models. This work underscores the
potential of integrating average controllability as additional metrics to
address the challenges of anomaly detection in sparse and imbalanced datasets.

</details>


### [95] [Signs of the Past, Patterns of the Present: On the Automatic Classification of Old Babylonian Cuneiform Signs](https://arxiv.org/abs/2507.13959)
*Eli Verwimp,Gustav Ryberg Smidt,Hendrik Hameeuw,Katrien De Graef*

Main category: cs.LG

TL;DR: 本文研究机器学习技术对楔形文字符号分类，训练和测试ResNet50模型，在古巴比伦文献文本上取得较好结果，为未来数据采集标准和分类任务提供基础。


<details>
  <summary>Details</summary>
Motivation: 楔形文字符号存在诸多差异，使在一个数据集上训练的机器学习模型难以在其他数据集上成功，研究这些差异对模型性能的影响，并为未来数据采集标准和分类任务提供基础。

Method: 在来自三个美索不达米亚城市的泥板上手写的古巴比伦文献文本上训练和测试ResNet50模型。

Result: ResNet50模型在至少有20个实例的符号上，top - 1分数达87.1%，top - 5分数达96.5%，且目前古巴比伦文本上无可比结果。

Conclusion: 基于结果和见解有望影响未来数据采集标准，为楔形文字符号分类任务奠定基础。

Abstract: The work in this paper describes the training and evaluation of machine
learning (ML) techniques for the classification of cuneiform signs. There is a
lot of variability in cuneiform signs, depending on where they come from, for
what and by whom they were written, but also how they were digitized. This
variability makes it unlikely that an ML model trained on one dataset will
perform successfully on another dataset. This contribution studies how such
differences impact that performance. Based on our results and insights, we aim
to influence future data acquisition standards and provide a solid foundation
for future cuneiform sign classification tasks. The ML model has been trained
and tested on handwritten Old Babylonian (c. 2000-1600 B.C.E.) documentary
texts inscribed on clay tablets originating from three Mesopotamian cities
(Nippur, D\=ur-Abie\v{s}uh and Sippar). The presented and analysed model is
ResNet50, which achieves a top-1 score of 87.1% and a top-5 score of 96.5% for
signs with at least 20 instances. As these automatic classification results are
the first on Old Babylonian texts, there are currently no comparable results.

</details>


### [96] [Structural Connectome Harmonization Using Deep Learning: The Strength of Graph Neural Networks](https://arxiv.org/abs/2507.13992)
*Jagruti Patel,Thomas A. W. Bolton,Mikkel Schöttner,Anjali Tarun,Sebastien Tourbier,Yasser Alemàn-Gòmez,Jonas Richiardi,Patric Hagmann*

Main category: cs.LG

TL;DR: 本文提出一种无需元数据或移动受试者的位点条件深度协调框架，用于多站点结构连接组协调，对比不同模型，表明基于图的方法适合大规模多站点研究。


<details>
  <summary>Details</summary>
Motivation: 神经影像小样本及多站点研究的采集偏差限制了可靠生物标志物的开发，现有协调方法存在依赖元数据等局限。

Method: 提出位点条件深度协调框架，在模拟场景中测试三种深度架构（全连接自编码器、卷积自编码器、图卷积自编码器），并与线性回归基线对比。

Result: 非图模型在边权重预测和边存在检测上表现出色，图自编码器在拓扑结构保存和个体特征保留上更优，线性回归虽数值表现高但缺乏实际应用可行性。

Conclusion: 模型架构对结构连接组协调性能至关重要，基于图的方法适合大规模多站点结构连接组研究。

Abstract: Small sample sizes in neuroimaging in general, and in structural connectome
(SC) studies in particular limit the development of reliable biomarkers for
neurological and psychiatric disorders - such as Alzheimer's disease and
schizophrenia - by reducing statistical power, reliability, and
generalizability. Large-scale multi-site studies have exist, but they have
acquisition-related biases due to scanner heterogeneity, compromising imaging
consistency and downstream analyses. While existing SC harmonization methods -
such as linear regression (LR), ComBat, and deep learning techniques - mitigate
these biases, they often rely on detailed metadata, traveling subjects (TS), or
overlook the graph-topology of SCs. To address these limitations, we propose a
site-conditioned deep harmonization framework that harmonizes SCs across
diverse acquisition sites without requiring metadata or TS that we test in a
simulated scenario based on the Human Connectome Dataset. Within this
framework, we benchmark three deep architectures - a fully connected
autoencoder (AE), a convolutional AE, and a graph convolutional AE - against a
top-performing LR baseline. While non-graph models excel in edge-weight
prediction and edge existence detection, the graph AE demonstrates superior
preservation of topological structure and subject-level individuality, as
reflected by graph metrics and fingerprinting accuracy, respectively. Although
the LR baseline achieves the highest numerical performance by explicitly
modeling acquisition parameters, it lacks applicability to real-world
multi-site use cases as detailed acquisition metadata is often unavailable. Our
results highlight the critical role of model architecture in SC harmonization
performance and demonstrate that graph-based approaches are particularly
well-suited for structure-aware, domain-generalizable SC harmonization in
large-scale multi-site SC studies.

</details>


### [97] [ParallelTime: Dynamically Weighting the Balance of Short- and Long-Term Temporal Dependencies](https://arxiv.org/abs/2507.13998)
*Itay Katav,Aryeh Kontorovich*

Main category: cs.LG

TL;DR: 提出ParallelTime架构及动态加权机制ParallelTime Weighter用于多元时间序列预测，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法对长短依赖等权分配不适用于时间序列预测任务。

Method: 提出动态加权机制ParallelTime Weighter计算长短依赖权重，引入包含该机制的ParallelTime架构。

Result: 架构表现出鲁棒性，FLOPs更低、参数更少，能有效扩展到更长预测范围，显著优于现有方法。

Conclusion: 为时间序列预测中并行Attention - Mamba的未来发展指明了有前景的方向。

Abstract: Modern multivariate time series forecasting primarily relies on two
architectures: the Transformer with attention mechanism and Mamba. In natural
language processing, an approach has been used that combines local window
attention for capturing short-term dependencies and Mamba for capturing
long-term dependencies, with their outputs averaged to assign equal weight to
both. We find that for time-series forecasting tasks, assigning equal weight to
long-term and short-term dependencies is not optimal. To mitigate this, we
propose a dynamic weighting mechanism, ParallelTime Weighter, which calculates
interdependent weights for long-term and short-term dependencies for each token
based on the input and the model's knowledge. Furthermore, we introduce the
ParallelTime architecture, which incorporates the ParallelTime Weighter
mechanism to deliver state-of-the-art performance across diverse benchmarks.
Our architecture demonstrates robustness, achieves lower FLOPs, requires fewer
parameters, scales effectively to longer prediction horizons, and significantly
outperforms existing methods. These advances highlight a promising path for
future developments of parallel Attention-Mamba in time series forecasting. The
implementation is readily available at:
\href{https://github.com/itay1551/ParallelTime}{ParallelTime GitHub

</details>


### [98] [On the Fundamental Limitations of Dual Static CVaR Decompositions in Markov Decision Processes](https://arxiv.org/abs/2507.14005)
*Mathieu Godbout,Audrey Durand*

Main category: cs.LG

TL;DR: 本文从策略评估角度研究MDP中基于对偶公式的CVaR计算失败原因，发现风险分配一致性约束为空导致评估误差，指出对偶CVaR分解找最优策略有局限性。


<details>
  <summary>Details</summary>
Motivation: 以往基于对偶公式的动态规划方法在寻找MDP中静态CVaR最优策略时失败，但原因不明，本文聚焦策略评估来探究。

Method: 将给定策略的静态CVaR评估构建为两个不同的最小化问题，量化评估误差为CVaR评估差距。

Result: 风险分配一致性约束为空是评估误差来源，对偶CVaR DP优化时返回策略有非零CVaR评估差距，存在MDP找不到统一最优策略。

Conclusion: 基于对偶CVaR分解寻找统一最优策略有根本局限性。

Abstract: Recent work has shown that dynamic programming (DP) methods for finding
static CVaR-optimal policies in Markov Decision Processes (MDPs) can fail when
based on the dual formulation, yet the root cause for the failure has remained
unclear. We expand on these findings by shifting focus from policy optimization
to the seemingly simpler task of policy evaluation. We show that evaluating the
static CVaR of a given policy can be framed as two distinct minimization
problems. For their solutions to match, a set of ``risk-assignment consistency
constraints'' must be satisfied, and we demonstrate that the intersection of
the constraints being empty is the source of previously observed evaluation
errors. Quantifying the evaluation error as the CVaR evaluation gap, we then
demonstrate that the issues observed when optimizing over the dual-based CVaR
DP are explained by the returned policy having a non-zero CVaR evaluation gap.
We then leverage our proposed risk-assignment perspective to prove that the
search for a single, uniformly optimal policy via on the dual CVaR
decomposition is fundamentally limited, identifying an MDP where no single
policy can be optimal across all initial risk levels.

</details>


### [99] [Byzantine-resilient federated online learning for Gaussian process regression](https://arxiv.org/abs/2507.14021)
*Xu Zhang,Zhenyuan Yuan,Minghui Zhu*

Main category: cs.LG

TL;DR: 本文研究拜占庭容错的高斯过程回归联邦在线学习，提出算法并实验验证性能。


<details>
  <summary>Details</summary>
Motivation: 在部分代理出现拜占庭故障的情况下，实现云与代理协作学习潜在函数并提升学习性能。

Method: 开发拜占庭容错联邦GPR算法，各代理本地GPR发送本地预测到云，云用拜占庭容错聚合规则计算全局模型并广播，代理融合全局与本地模型。

Result: 量化了代理融合GPR相对本地GPR的学习精度提升，通过实验验证了算法性能。

Conclusion: 所提出的拜占庭容错联邦GPR算法能有效提升学习性能。

Abstract: In this paper, we study Byzantine-resilient federated online learning for
Gaussian process regression (GPR). We develop a Byzantine-resilient federated
GPR algorithm that allows a cloud and a group of agents to collaboratively
learn a latent function and improve the learning performances where some agents
exhibit Byzantine failures, i.e., arbitrary and potentially adversarial
behavior. Each agent-based local GPR sends potentially compromised local
predictions to the cloud, and the cloud-based aggregated GPR computes a global
model by a Byzantine-resilient product of experts aggregation rule. Then the
cloud broadcasts the current global model to all the agents. Agent-based fused
GPR refines local predictions by fusing the received global model with that of
the agent-based local GPR. Moreover, we quantify the learning accuracy
improvements of the agent-based fused GPR over the agent-based local GPR.
Experiments on a toy example and two medium-scale real-world datasets are
conducted to demonstrate the performances of the proposed algorithm.

</details>


### [100] [DONUT: Physics-aware Machine Learning for Real-time X-ray Nanodiffraction Analysis](https://arxiv.org/abs/2507.14038)
*Aileen Luo,Tao Zhou,Ming Du,Martin V. Holt,Andrej Singer,Mathew J. Cherukara*

Main category: cs.LG

TL;DR: 介绍DONUT神经网络用于纳米束衍射数据快速自动分析，比传统方法高效200多倍。


<details>
  <summary>Details</summary>
Motivation: 相干X射线散射技术实时分析存在瓶颈，扫描X射线纳米衍射显微镜因光束与样品结构卷积使挑战加剧。

Method: 引入物理感知神经网络DONUT，将可微几何衍射模型融入架构，无需标记数据集或预训练。

Result: 实验表明DONUT能准确提取数据特征，比传统拟合方法效率高200多倍。

Conclusion: DONUT可有效解决纳米束衍射数据实时分析的问题，克服了监督机器学习在X射线科学中的限制。

Abstract: Coherent X-ray scattering techniques are critical for investigating the
fundamental structural properties of materials at the nanoscale. While
advancements have made these experiments more accessible, real-time analysis
remains a significant bottleneck, often hindered by artifacts and computational
demands. In scanning X-ray nanodiffraction microscopy, which is widely used to
spatially resolve structural heterogeneities, this challenge is compounded by
the convolution of the divergent beam with the sample's local structure. To
address this, we introduce DONUT (Diffraction with Optics for Nanobeam by
Unsupervised Training), a physics-aware neural network designed for the rapid
and automated analysis of nanobeam diffraction data. By incorporating a
differentiable geometric diffraction model directly into its architecture,
DONUT learns to predict crystal lattice strain and orientation in real-time.
Crucially, this is achieved without reliance on labeled datasets or
pre-training, overcoming a fundamental limitation for supervised machine
learning in X-ray science. We demonstrate experimentally that DONUT accurately
extracts all features within the data over 200 times more efficiently than
conventional fitting methods.

</details>


### [101] [Noradrenergic-inspired gain modulation attenuates the stability gap in joint training](https://arxiv.org/abs/2507.14056)
*Alejandro Rodriguez-Garcia,Anindya Ghosh,Srikanth Ramaswamy*

Main category: cs.LG

TL;DR: 研究持续学习中的稳定性差距，受生物大脑启发提出不确定性调制增益动态机制，在基准测试中有效减弱稳定性差距并提供机理见解。


<details>
  <summary>Details</summary>
Motivation: 持续学习中存在稳定性差距，与持续学习目标相悖，缺乏缓解遗忘的鲁棒性，需研究协调可塑性和稳定性的机制。

Method: 受蓝斑介导的去甲肾上腺素爆发启发，提出不确定性调制增益动态机制，近似双时间尺度优化器。

Result: 在MNIST和CIFAR基准测试的域增量和类增量变体上，该机制有效减弱稳定性差距。

Conclusion: 增益调制复制了皮质回路中的去甲肾上腺素功能，为减少稳定性差距和提高持续学习任务性能提供了机理见解。

Abstract: Recent studies in continual learning have identified a transient drop in
performance on mastered tasks when assimilating new ones, known as the
stability gap. Such dynamics contradict the objectives of continual learning,
revealing a lack of robustness in mitigating forgetting, and notably,
persisting even under an ideal joint-loss regime. Examining this gap within
this idealized joint training context is critical to isolate it from other
sources of forgetting. We argue that it reflects an imbalance between rapid
adaptation and robust retention at task boundaries, underscoring the need to
investigate mechanisms that reconcile plasticity and stability within continual
learning frameworks. Biological brains navigate a similar dilemma by operating
concurrently on multiple timescales, leveraging neuromodulatory signals to
modulate synaptic plasticity. However, artificial networks lack native
multitimescale dynamics, and although optimizers like momentum-SGD and Adam
introduce implicit timescale regularization, they still exhibit stability gaps.
Inspired by locus coeruleus mediated noradrenergic bursts, which transiently
enhance neuronal gain under uncertainty to facilitate sensory assimilation, we
propose uncertainty-modulated gain dynamics - an adaptive mechanism that
approximates a two-timescale optimizer and dynamically balances integration of
knowledge with minimal interference on previously consolidated information. We
evaluate our mechanism on domain-incremental and class-incremental variants of
the MNIST and CIFAR benchmarks under joint training, demonstrating that
uncertainty-modulated gain dynamics effectively attenuate the stability gap.
Finally, our analysis elucidates how gain modulation replicates noradrenergic
functions in cortical circuits, offering mechanistic insights into reducing
stability gaps and enhance performance in continual learning tasks.

</details>


### [102] [Preference-based Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2507.14066)
*Ni Mu,Yao Luan,Qing-Shan Jia*

Main category: cs.LG

TL;DR: 提出基于偏好的多目标强化学习（Pb - MORL）方法，理论证明其有效性并通过实验验证性能。


<details>
  <summary>Details</summary>
Motivation: 传统多目标强化学习依赖预定义奖励函数，难平衡冲突目标且易简化问题，偏好可作为更灵活直观的决策指导。

Method: 将偏好集成到MORL框架，构建与给定偏好一致的多目标奖励模型，理论证明优化该模型等同于训练帕累托最优策略。

Result: 在多个基准多目标任务、多能源管理任务和自动驾驶任务中表现出色，超越使用真实奖励函数的oracle方法。

Conclusion: 该方法在复杂现实系统中有实际应用潜力。

Abstract: Multi-objective reinforcement learning (MORL) is a structured approach for
optimizing tasks with multiple objectives. However, it often relies on
pre-defined reward functions, which can be hard to design for balancing
conflicting goals and may lead to oversimplification. Preferences can serve as
more flexible and intuitive decision-making guidance, eliminating the need for
complicated reward design. This paper introduces preference-based MORL
(Pb-MORL), which formalizes the integration of preferences into the MORL
framework. We theoretically prove that preferences can derive policies across
the entire Pareto frontier. To guide policy optimization using preferences, our
method constructs a multi-objective reward model that aligns with the given
preferences. We further provide theoretical proof to show that optimizing this
reward model is equivalent to training the Pareto optimal policy. Extensive
experiments in benchmark multi-objective tasks, a multi-energy management task,
and an autonomous driving task on a multi-line highway show that our method
performs competitively, surpassing the oracle method, which uses the ground
truth reward function. This highlights its potential for practical applications
in complex real-world systems.

</details>


### [103] [DPMT: Dual Process Multi-scale Theory of Mind Framework for Real-time Human-AI Collaboration](https://arxiv.org/abs/2507.14088)
*Xiyun Li,Yining Ding,Yuhua Jiang,Yunlong Zhao,Runpeng Xie,Shuang Xu,Yuanhua Ni,Yiqin Yang,Bo Xu*

Main category: cs.LG

TL;DR: 提出DPMT框架解决实时人机协作中AI适应人类行为的问题，实验表明其能提升协作效果。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型代理难以准确建模复杂人类心理特征，尤其是在缺乏直接交流时，需解决实时人机协作中AI适应多样人类行为的问题。

Method: 受认知科学双过程理论启发，提出DPMT框架，包含多尺度心理理论（ToM）模块进行心理特征推理。

Result: 实验结果表明DPMT显著增强了人机协作，消融实验验证了多尺度ToM在慢系统中的作用。

Conclusion: DPMT框架有效提升了人机协作，多尺度ToM模块有积极贡献。

Abstract: Real-time human-artificial intelligence (AI) collaboration is crucial yet
challenging, especially when AI agents must adapt to diverse and unseen human
behaviors in dynamic scenarios. Existing large language model (LLM) agents
often fail to accurately model the complex human mental characteristics such as
domain intentions, especially in the absence of direct communication. To
address this limitation, we propose a novel dual process multi-scale theory of
mind (DPMT) framework, drawing inspiration from cognitive science dual process
theory. Our DPMT framework incorporates a multi-scale theory of mind (ToM)
module to facilitate robust human partner modeling through mental
characteristic reasoning. Experimental results demonstrate that DPMT
significantly enhances human-AI collaboration, and ablation studies further
validate the contributions of our multi-scale ToM in the slow system.

</details>


### [104] [Kolmogorov Arnold Networks (KANs) for Imbalanced Data -- An Empirical Perspective](https://arxiv.org/abs/2507.14121)
*Pankaj Yadav,Vivek Vijay*

Main category: cs.LG

TL;DR: 本文对KANs在类别不平衡分类中的表现进行实证评估，发现KANs在原始不平衡数据上表现好，但与传统策略冲突、计算成本高，确定了后续研究重点。


<details>
  <summary>Details</summary>
Motivation: 评估Kolmogorov Arnold Networks (KANs)在类别不平衡分类中的表现。

Method: 使用十个基准数据集对KANs进行实证评估。

Result: KANs在原始不平衡数据上比多层感知机（MLPs）表现好，传统不平衡策略会降低KANs性能；KANs计算成本高，MLPs使用不平衡技术能以最小资源成本达到与KANs相当的效果。

Conclusion: KANs是处理原始不平衡数据的专门解决方案，但存在性能 - 资源权衡和与标准重采样技术不兼容问题，确定了开发特定架构修改、优化计算效率和理论调和冲突等研究重点。

Abstract: Kolmogorov Arnold Networks (KANs) are recent architectural advancement in
neural computation that offer a mathematically grounded alternative to standard
neural networks. This study presents an empirical evaluation of KANs in context
of class imbalanced classification, using ten benchmark datasets. We observe
that KANs can inherently perform well on raw imbalanced data more effectively
than Multi-Layer Perceptrons (MLPs) without any resampling strategy. However,
conventional imbalance strategies fundamentally conflict with KANs mathematical
structure as resampling and focal loss implementations significantly degrade
KANs performance, while marginally benefiting MLPs. Crucially, KANs suffer from
prohibitive computational costs without proportional performance gains.
Statistical validation confirms that MLPs with imbalance techniques achieve
equivalence with KANs (|d| < 0.08 across metrics) at minimal resource costs.
These findings reveal that KANs represent a specialized solution for raw
imbalanced data where resources permit. But their severe performance-resource
tradeoffs and incompatibility with standard resampling techniques currently
limits practical deployment. We identify critical research priorities as
developing KAN specific architectural modifications for imbalance learning,
optimizing computational efficiency, and theoretical reconciling their conflict
with data augmentation. This work establishes foundational insights for next
generation KAN architectures in imbalanced classification scenarios.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [105] [Neural Architecture Search with Mixed Bio-inspired Learning Rules](https://arxiv.org/abs/2507.13485)
*Imane Hamzaoui,Riyadh Baghdadi*

Main category: cs.NE

TL;DR: 通过定制的神经架构搜索程序自动发现不同层使用不同生物启发学习规则，可缩小生物启发神经网络与基于反向传播模型的差距，提升准确性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 生物启发神经网络在准确性和可扩展性上落后于基于反向传播的模型，希望缩小这一差距。

Method: 从标准NAS基线出发，扩大搜索空间纳入生物启发学习规则，用NAS为每层找到最佳架构和学习规则。

Result: 不同层使用不同生物启发学习规则的神经网络准确性优于单层使用单一规则的网络，在多个数据集上创生物启发模型新纪录，部分情况下超越基于反向传播的网络。

Conclusion: 学习规则的层间多样性可实现更好的可扩展性和准确性，值得进一步研究在同一网络中混合多种生物启发学习规则。

Abstract: Bio-inspired neural networks are attractive for their adversarial robustness,
energy frugality, and closer alignment with cortical physiology, yet they often
lag behind back-propagation (BP) based models in accuracy and ability to scale.
We show that allowing the use of different bio-inspired learning rules in
different layers, discovered automatically by a tailored
neural-architecture-search (NAS) procedure, bridges this gap. Starting from
standard NAS baselines, we enlarge the search space to include bio-inspired
learning rules and use NAS to find the best architecture and learning rule to
use in each layer. We show that neural networks that use different bio-inspired
learning rules for different layers have better accuracy than those that use a
single rule across all the layers. The resulting NN that uses a mix of
bio-inspired learning rules sets new records for bio-inspired models: 95.16% on
CIFAR-10, 76.48% on CIFAR-100, 43.42% on ImageNet16-120, and 60.51% top-1 on
ImageNet. In some regimes, they even surpass comparable BP-based networks while
retaining their robustness advantages. Our results suggest that layer-wise
diversity in learning rules allows better scalability and accuracy, and
motivates further research on mixing multiple bio-inspired learning rules in
the same network.

</details>


### [106] [Evolving Neural Controllers for Xpilot-AI Racing Using Neuroevolution of Augmenting Topologies](https://arxiv.org/abs/2507.13549)
*Jim O'Connor,Nicholas Lorentzen,Gary B. Parker,Derin Gezgin*

Main category: cs.NE

TL;DR: 本文利用NEAT算法为Xpilot - AI平台新赛车模式开发高性能赛车控制器，实验表明控制器圈速提升32%，展示了NEAT算法和平台的有效性。


<details>
  <summary>Details</summary>
Motivation: 为Xpilot - AI平台新赛车模式开发高性能赛车控制器。

Method: 利用Neuro Evolution of Augmenting Topologies (NEAT)算法进化神经网络的结构和权重，开发自适应控制器，且新赛车模式支持灵活电路设计和多智能体并行评估以优化控制器。

Result: 进化后的控制器圈速相比初始性能提升达32%，并形成了类似人类的有效赛车策略。

Conclusion: 证明了NEAT算法在复杂游戏环境中产生强大控制策略的有效性，以及Xpilot - AI平台作为竞争型AI控制器进化测试平台的潜力。

Abstract: This paper investigates the development of high-performance racing
controllers for a newly implemented racing mode within the Xpilot-AI platform,
utilizing the Neuro Evolution of Augmenting Topologies (NEAT) algorithm. By
leveraging NEAT's capability to evolve both the structure and weights of neural
networks, we develop adaptive controllers that can navigate complex circuits
under the challenging space simulation physics of Xpilot-AI, which includes
elements such as inertia, friction, and gravity. The racing mode we introduce
supports flexible circuit designs and allows for the evaluation of multiple
agents in parallel, enabling efficient controller optimization across
generations. Experimental results demonstrate that our evolved controllers
achieve up to 32% improvement in lap time compared to the controller's initial
performance and develop effective racing strategies, such as optimal cornering
and speed modulation, comparable to human-like techniques. This work
illustrates NEAT's effectiveness in producing robust control strategies within
demanding game environments and highlights Xpilot-AI's potential as a rigorous
testbed for competitive AI controller evolution.

</details>


### [107] [MorphoNAS: Embryogenic Neural Architecture Search Through Morphogen-Guided Development](https://arxiv.org/abs/2507.13785)
*Mykola Glybovets,Sergii Medvid*

Main category: cs.NE

TL;DR: 本文提出MorphoNAS系统，受生物机制启发，用简单规则生成复杂神经网络，在结构和功能任务实验中表现良好，表明其是可行的神经架构搜索方法。


<details>
  <summary>Details</summary>
Motivation: 生物神经网络能用简单规则从紧凑基因组发育，而现代人工神经架构搜索多需手动工作，因此提出受生物机制启发的搜索方法。

Method: 引入MorphoNAS系统，通过受自由能原理、反应 - 扩散系统和基因调控网络启发的形态发生自组织来确定性地生长神经网络，简单基因组编码形态发生素动态和细胞发育规则。

Result: 在结构目标实验中能找到生成预定义随机图配置的成功基因组；在CartPole控制任务中实现低复杂度的神经元解决方案，且进化过程平衡了解决方案质量和搜索效率。

Conclusion: MorphoNAS方法能用简单发育规则生成复杂特定神经架构，为自适应和高效的神经架构搜索提供了可行的生物学途径。

Abstract: While biological neural networks develop from compact genomes using
relatively simple rules, modern artificial neural architecture search methods
mostly involve explicit and routine manual work. In this paper, we introduce
MorphoNAS (Morphogenetic Neural Architecture Search), a system able to
deterministically grow neural networks through morphogenetic self-organization
inspired by the Free Energy Principle, reaction-diffusion systems, and gene
regulatory networks. In MorphoNAS, simple genomes encode just morphogens
dynamics and threshold-based rules of cellular development. Nevertheless, this
leads to self-organization of a single progenitor cell into complex neural
networks, while the entire process is built on local chemical interactions. Our
evolutionary experiments focused on two different domains: structural
targeting, in which MorphoNAS system was able to find fully successful genomes
able to generate predefined random graph configurations (8-31 nodes); and
functional performance on the CartPole control task achieving low complexity
6-7 neuron solutions when target network size minimization evolutionary
pressure was applied. The evolutionary process successfully balanced between
quality of of the final solutions and neural architecture search effectiveness.
Overall, our findings suggest that the proposed MorphoNAS method is able to
grow complex specific neural architectures, using simple developmental rules,
which suggests a feasible biological route to adaptive and efficient neural
architecture search.

</details>


### [108] [Conceptual and Design Principles for a Self-Referential Algorithm Mimicking Neuronal Assembly Functions](https://arxiv.org/abs/2507.14011)
*Paolo Totaro,Alberto Mangiante*

Main category: cs.NE

TL;DR: 本文提出基于生存系统视角，用 EGO 算法模式和 E - 语言形式化认知过程模型，还展示已实现并测试的 EGO 原型。


<details>
  <summary>Details</summary>
Motivation: 从生存系统自身而非观察者视角，对基于经验的认知过程模型进行形式化。

Method: 基于名为 Environment Generative Operator (EGO) 的算法模式，使用为此开发的自引用语言 E - language，模拟赫布理解的神经元集合上的认知过程。

Result: 实现并测试了 EGO 原型（EGO - P）。

Conclusion: 未明确提及，但展示原型说明方法具有一定可行性。

Abstract: This article proposes a method to formalise models of cognitive processes
grounded in experience, considering experience from the perspective of a living
system and not from that of an observer of the living system. The perspective
of a living system is defined by the need of the system to preserve the vital
equilibria. The method is based on an algorithmic schema that we call
Environment Generative Operator (EGO) and uses a self-referential language
developed for this purpose which we call E-language. EGO simulates cognitive
processes as operations on neuron assemblies as understood by Hebb. In this
article we present an EGO prototype (EGO-P) which has already been implemented
and tested.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [109] [Photonic Fabric Platform for AI Accelerators](https://arxiv.org/abs/2507.14000)
*Jing Ding,Trung Diep*

Main category: cs.PF

TL;DR: 本文介绍Photonic FabricTM和PFA，可提供低延迟、高带宽和低比特能耗，用CelestiSim评估其性能，在LLM推理和训练中有显著提升及节能效果。


<details>
  <summary>Details</summary>
Motivation: 解决当前XPU加速器设计中固定内存与计算比的限制问题，提高分布式AI训练和推理的效率。

Method: 将高带宽HBM3E内存、模块上光子开关和外部DDR5集成在2.5D光电系统级封装中；引入CelestiSim在NVIDIA系统上验证并评估PFA性能。

Result: 在LLM推理中，405B参数下吞吐量最高提升3.66倍、延迟改善1.40倍，1T参数下吞吐量最高提升7.04倍、延迟改善1.41倍；在所有LLM训练场景中，数据移动节能60 - 90%。

Conclusion: PFA能有效突破固定内存与计算比的限制，提升性能并节能，成果可应用于有相同限制的其他AI加速器设计。

Abstract: This paper presents the Photonic FabricTM and the Photonic Fabric ApplianceTM
(PFA), a photonic-enabled switch and memory subsystem that delivers low
latency, high bandwidth, and low per-bit energy. By integrating high-bandwidth
HBM3E memory, an on-module photonic switch, and external DDR5 in a 2.5D
electro-optical system-in-package, the PFA offers up to 32 TB of shared memory
alongside 115 Tbps of all-to-all digital switching. The Photonic FabricTM
enables distributed AI training and inference to execute parallelism strategies
more efficiently. The Photonic Fabric removes the silicon beachfront constraint
that limits the fixed memory-to-compute ratio observed in virtually all current
XPU accelerator designs. Replacing a local HBM stack on an XPU with a chiplet
that connects to the Photonic Fabric increases its memory capacity and
correspondingly its memory bandwidth by offering a flexible path to scaling
well beyond the limitations of on-package HBM alone. We introduce CelestiSim, a
lightweight analytical simulator validated on NVIDIA H100 and H200 systems. It
is used to evaluate the performance of LLM reference and energy savings on PFA,
without any significant change to the GPU core design. With the PFA, the
simulation results show that up to 3.66x throughput and 1.40x latency
improvements in LLM inference at 405B parameters, up to 7.04x throughput and
1.41x latency improvements at 1T parameters, and 60-90% energy savings in data
movement for heavy collective operations in all LLM training scenarios. While
these results are shown for NVIDIA GPUs, they can be applied similarly to other
AI accelerator designs (XPUs) that share the same fundamental limitation of
fixed memory to compute.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [110] [Socio-Technical Smell Dynamics in Code Samples: A Multivocal Review on Emergence, Evolution, and Co-Occurrence](https://arxiv.org/abs/2507.13481)
*Arthur Bueno,Bruno Cafeo,Maria Cagnin,Awdren Fontão*

Main category: cs.SE

TL;DR: 研究开源生态系统代码示例中代码和社区异味的出现、共现和演变，发现社区异味常预示或加剧代码问题，强调需针对性治理机制。


<details>
  <summary>Details</summary>
Motivation: 代码示例在开源生态系统重要但管理不规范，缺乏对社区层面功能失调如何影响代码示例技术异常的研究。

Method: 采用多视角文献综述协议，涵盖30篇同行评审论文和17篇从业者导向资源，进行主题合成。

Result: 识别出9种模式，社区异味常先于或加剧代码示例的技术退化，还发现异味积累的常见条件。

Conclusion: 在开源生态系统中，社区层面功能失调与代码可维护性衰退相关，需针对共享教学工件的社会技术质量指标和轻量级治理机制。

Abstract: Code samples play a pivotal role in open-source ecosystems (OSSECO), serving
as lightweight artifacts that support knowledge transfer, onboarding, and
framework adoption. Despite their instructional relevance, these samples are
often governed informally, with minimal review and unclear ownership, which
increases their exposure to socio-technical degradation. In this context, the
co-occurrence and longitudinal interplay of code smells (e.g., large classes,
poor modularity) and community smells (e.g., lone contributors, fragmented
communication) become particularly critical. While each type of smell has been
studied in isolation, little is known about how community-level dysfunctions
anticipate or exacerbate technical anomalies in code samples over time. This
study investigates how code and community smells emerge, co-occur, and evolve
within code samples maintained in OSSECOs. A Multivocal Literature Review
protocol was applied, encompassing 30 peer-reviewed papers and 17
practitioner-oriented sources (2013-2024). Thematic synthesis was conducted to
identify recurring socio-technical patterns related to smell dynamics. Nine
patterns were identified, showing that community smells often precede or
reinforce technical degradation in code samples. Symptoms such as "radio
silence" and centralized ownership were frequently associated with persistent
structural anomalies. Additionally, limited onboarding, the absence of
continuous refactoring, and informal collaboration emerged as recurring
conditions for smell accumulation. Conclusion: In OSSECOs, particularly within
code samples, community-level dysfunctions not only correlate with but often
signal maintainability decay. These findings underscore the need for
socio-technical quality indicators and lightweight governance mechanisms
tailored to shared instructional artifacts.

</details>


### [111] [AI-Assisted Fixes to Code Review Comments at Scale](https://arxiv.org/abs/2507.13499)
*Chandra Maddila,Negar Ghorbani,James Saindon,Parth Thakkar,Vijayaraghavan Murali,Rui Abreu,Jingyue Shen,Brian Zhou,Nachiappan Nagappan,Peter C. Rigby*

Main category: cs.SE

TL;DR: Meta开发了MetaMateCR用于大规模生产中为代码审查评论提供AI辅助修复，经试验和优化后投入生产取得较好效果。


<details>
  <summary>Details</summary>
Motivation: Meta每周有大量代码审查评论，需开发工具提供AI辅助修复。

Method: 开发64k数据点的内部基准微调Llama模型，离线测试达标后投入生产，进行随机对照安全试验和全面生产实验。

Result: 离线结果LargeLSFT模型精确匹配补丁率68%超GPT - 4o；安全试验修改UX后无审查时间倒退；生产中LargeLSFT的ActionableToApplied率达19.7%，较GPT - 4o提升9.2pp。

Conclusion: 安全试验对确保AI不拖慢工程师工作很重要，MetaMateCR可大规模成功运行。

Abstract: Aim. There are 10s of thousands of code review comments each week at Meta. We
developed Metamate for Code Review (MetaMateCR) that provides AI-assisted fixes
for reviewer comments in production at scale.
  Method. We developed an internal benchmark of 64k <review comment, patch>
data points to fine-tune Llama models. Once our models achieve reasonable
offline results, we roll them into production. To ensure that our AI-assisted
fixes do not negatively impact the time it takes to do code reviews, we conduct
randomized controlled safety trials as well as full production experiments.
  Offline Results. As a baseline, we compare GPT-4o to our small and large
Llama models. In offline results, our LargeLSFT model creates an exact match
patch 68% of the time outperforming GPT-4o by 9 percentage points (pp). The
internal models also use more modern Hack functions when compared to the PHP
functions suggested by GPT-4o.
  Safety Trial. When we roll MetaMateCR into production in a safety trial that
compares no AI patches with AI patch suggestions, we see a large regression
with reviewers taking over 5% longer to conduct reviews. After investigation,
we modify the UX to only show authors the AI patches, and see no regressions in
the time for reviews.
  Production. When we roll LargeLSFT into production, we see an
ActionableToApplied rate of 19.7%, which is a 9.2pp improvement over GPT-4o.
Our results illustrate the importance of safety trials in ensuring that AI does
not inadvertently slow down engineers, and a successful review comment to AI
patch product running at scale.

</details>


### [112] [Towards Better Requirements from the Crowd: Developer Engagement with Feature Requests in Open Source Software](https://arxiv.org/abs/2507.13553)
*Pragyan K C,Rambod Ghandiparsi,Thomas Herron,John Heaps,Mitra Bokaei Hosseini*

Main category: cs.SE

TL;DR: 研究开源软件中功能请求的自然语言缺陷及澄清对话动态，发现请求有歧义与不完整问题，澄清不常见，明确可改进协作与处理请求的模式。


<details>
  <summary>Details</summary>
Motivation: 用户需求演变，功能请求常存在歧义或信息不完整问题，影响软件质量，需了解开发者处理此类请求的实际澄清过程。

Method: 研究开源软件中功能请求的自然语言缺陷及澄清对话动态。

Result: 功能请求有歧义与不完整问题，明确澄清不常见，澄清时关注用户意图/目标和可行性。

Conclusion: 明确澄清动态模式可改善用户与开发者协作，为有效处理功能请求提供最佳实践。

Abstract: As user demands evolve, effectively incorporating feature requests is crucial
for maintaining software relevance and user satisfaction. Feature requests,
typically expressed in natural language, often suffer from ambiguity or
incomplete information due to communication gaps or the requester's limited
technical expertise. These issues can lead to misinterpretation, faulty
implementation, and reduced software quality. While seeking clarification from
requesters is a common strategy to mitigate these risks, little is known about
how developers engage in this clarification process in practice-how they
formulate clarifying questions, seek technical or contextual details, align on
goals and use cases, or decide to close requests without attempting
clarification. This study investigates how feature requests are prone to NL
defects (i.e. ambiguous or incomplete) and the conversational dynamics of
clarification in open-source software (OSS) development, aiming to understand
how developers handle ambiguous or incomplete feature requests. Our findings
suggest that feature requests published on the OSS platforms do possess
ambiguity and incompleteness, and in some cases, both. We also find that
explicit clarification for the resolution of these defects is uncommon;
developers usually focus on aligning with project goals rather than resolving
unclear text. When clarification occurs, it emphasizes understanding user
intent/goal and feasibility, rather than technical details. By characterizing
the dynamics of clarification in open-source issue trackers, this work
identifies patterns that can improve user-developer collaboration and inform
best practices for handling feature requests effectively.

</details>


### [113] [Demystifying Feature Requests: Leveraging LLMs to Refine Feature Requests in Open-Source Software](https://arxiv.org/abs/2507.13555)
*Pragyan K C,Rambod Ghandiparsi,Thomas Herron,John Heaps,Mitra Bokaei Hosseini*

Main category: cs.SE

TL;DR: 本文提出用大语言模型检测和改进特征请求中的自然语言缺陷的方法，并进行评估和开发者访谈。


<details>
  <summary>Details</summary>
Motivation: 软件应用需求不断变化，用户自然语言提出的需求有歧义、不完整等缺陷，传统验证方法在开源软件环境不适用。

Method: 提出利用大语言模型的方法，自动识别有问题的请求并生成澄清问题，将方法应用于真实开源软件特征请求，与人工标注对比，还对开发者进行访谈。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: The growing popularity and widespread use of software applications (apps)
across various domains have driven rapid industry growth. Along with this
growth, fast-paced market changes have led to constantly evolving software
requirements. Such requirements are often grounded in feature requests and
enhancement suggestions, typically provided by users in natural language (NL).
However, these requests often suffer from defects such as ambiguity and
incompleteness, making them challenging to interpret. Traditional validation
methods (e.g., interviews and workshops) help clarify such defects but are
impractical in decentralized environments like open-source software (OSS),
where change requests originate from diverse users on platforms like GitHub.
This paper proposes a novel approach leveraging Large Language Models (LLMs) to
detect and refine NL defects in feature requests. Our approach automates the
identification of ambiguous and incomplete requests and generates clarification
questions (CQs) to enhance their usefulness for developers. To evaluate its
effectiveness, we apply our method to real-world OSS feature requests and
compare its performance against human annotations. In addition, we conduct
interviews with GitHub developers to gain deeper insights into their
perceptions of NL defects, the strategies they use to address these defects,
and the impact of defects on downstream software engineering (SE) tasks.

</details>


### [114] [Testing Autonomous Driving Systems -- What Really Matters and What Doesn't](https://arxiv.org/abs/2507.13661)
*Changwen Li,Joseph Sifakis,Rongjie Yan,Jian Zhang*

Main category: cs.SE

TL;DR: 本文探讨自动驾驶系统测试碎片化问题，提出对比测试方法框架，指出多数方法有效性和有效性不足，依赖自动驾驶设计，结论是当前难获足够保证，建议开发兼顾合理性和确定性。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶系统测试领域碎片化，缺乏技术评估基础的问题。

Method: 提出对比现有测试方法的框架，研究测试有效性和有效性与自动驾驶设计的关系，并对八个开源自动驾驶系统进行测试。

Result: 许多测试方法不满足有效性和有效性要求，多数未考虑自动驾驶名义操作能力，多数开源自动驾驶系统不具备合理性和确定性。

Conclusion: 当前技术水平下无法为自动驾驶关键属性提供足够保证，建议开发时兼顾合理性和确定性。

Abstract: Despite extensive research, the testing of autonomous driving systems (ADS)
landscape remains fragmented, and there is currently no basis for an informed
technical assessment of the importance and contribution of the current state of
the art. This paper attempts to address this problem by exploring two
complementary aspects.
  First, it proposes a framework for comparing existing test methods in terms
of their intrinsic effectiveness and validity. It shows that many methods do
not meet both of these requirements. Either because they are based on criteria
that do not allow for rapid, inexpensive, and comprehensive detection of
failures, or because the degree of validity of the properties tested cannot be
accurately estimated. In particular, it is shown that most critical test
methods do not take into account the nominal operational capabilities of
autopilots and generate scenarios that are impossible for the tested vehicles
to handle, resulting in unjustified rejections.
  Secondly, the paper shows that test effectiveness and validity are highly
dependent on how autopilots are designed: how they choose between different
control policies to perform maneuvers, as well as on the reproducibility of the
results. In fact, most test methods take for granted two principles underlying
traditional methods, but do not generally apply to ADS. We maintain that the
absence of rationality and determinacy significantly impairs the effectiveness
and validity of test methods, and provide test results on eight open
autopilots, in which most do not satisfy these properties, thereby illustrating
this fact.
  We conclude that under the current state of the art, it is impossible to
obtain strong enough guarantees for essential autopilot properties and
recommend that autopilots be developed with a view to both rationality and
determinacy.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [115] [Quantitative Risk Management in Volatile Markets with an Expectile-Based Framework for the FTSE Index](https://arxiv.org/abs/2507.13391)
*Abiodun Finbarrs Oketunji*

Main category: q-fin.RM

TL;DR: 本文提出基于期望分位数的富时100指数量化风险管理框架，该框架优于传统风险度量方法，还给出实施指南和建议。


<details>
  <summary>Details</summary>
Motivation: 传统风险度量方法如VaR在市场压力时期有显著局限性，需新方法应对。

Method: 采用20年富时100指数回报数据集，引入期望分位数回归模型新公式、用时间序列分析改进阈值确定技术和稳健回测程序。

Result: 期望分位数VaR在各置信水平和市场条件下始终优于传统VaR，在波动期表现更好，降低模型风险且提高预测准确性。

Conclusion: 研究为金融风险管理文献作贡献，为从业者提供应对波动市场的实用工具。

Abstract: This research presents a framework for quantitative risk management in
volatile markets, specifically focusing on expectile-based methodologies
applied to the FTSE 100 index. Traditional risk measures such as Value-at-Risk
(VaR) have demonstrated significant limitations during periods of market
stress, as evidenced during the 2008 financial crisis and subsequent volatile
periods. This study develops an advanced expectile-based framework that
addresses the shortcomings of conventional quantile-based approaches by
providing greater sensitivity to tail losses and improved stability in extreme
market conditions. The research employs a dataset spanning two decades of FTSE
100 returns, incorporating periods of high volatility, market crashes, and
recovery phases. Our methodology introduces novel mathematical formulations for
expectile regression models, enhanced threshold determination techniques using
time series analysis, and robust backtesting procedures. The empirical results
demonstrate that expectile-based Value-at-Risk (EVaR) consistently outperforms
traditional VaR measures across various confidence levels and market
conditions. The framework exhibits superior performance during volatile
periods, with reduced model risk and enhanced predictive accuracy. Furthermore,
the study establishes practical implementation guidelines for financial
institutions and provides evidence-based recommendations for regulatory
compliance and portfolio management. The findings contribute significantly to
the literature on financial risk management and offer practical tools for
practitioners dealing with volatile market environments.

</details>


### [116] [PELVaR: Probability equal level representation of Value at Risk through the notion of Flexible Expected Shortfall](https://arxiv.org/abs/2507.13562)
*Georgios I. Papayiannis,Georgios Psarrakos*

Main category: q-fin.RM

TL;DR: 本文用FES混合框架构建VaR的连贯表示，重新解读VaR，解决其局限性，研究θ指数理论特性，用欧拉原理分析风险资本分配，并通过模拟和实证分析展示其实际应用。


<details>
  <summary>Details</summary>
Motivation: 解决VaR非次可加性和对尾部风险不敏感等局限性问题。

Method: 采用FES混合框架构建VaR的连贯表示，研究θ指数理论特性，用欧拉原理进行风险资本分配。

Result: 完成VaR在连贯风险度量框架内的重新解读，明确θ指数与风险评估的相关性，可进行一致且有意义的边际归因。

Conclusion: 该方法能解决VaR的局限性，通过模拟和实证分析证明其实践意义。

Abstract: This paper proposes a novel perspective on the relationship between Value at
Risk (VaR) and Expected Shortfall (ES) by employing the mixing framework of
Flexible Expected Shortfall (FES) to construct coherent representations of VaR.
The methodology enables a reinterpretation of VaR within a coherent risk
measure framework, thereby addressing well-known limitations of VaR, including
non-subadditivity and insensitivity to tail risk. A central feature of the
framework is the flexibility parameter inherent in FES, which captures salient
distributional properties of the underlying risk profile. This parameter is
formalized as the $\theta$-index, a normalized measure designed to reflect tail
heaviness. Theoretical properties of the $\theta$-index are examined, and its
relevance to risk assessment is established. Furthermore, risk capital
allocation is analyzed using the Euler principle, facilitating consistent and
meaningful marginal attribution. The practical implications of the approach are
illustrated through appropriate simulation studies and an empirical analysis
based on an insurance loss dataset with pronounced heavy-tailed
characteristics.

</details>


### [117] [Eliciting reference measures of law-invariant functionals](https://arxiv.org/abs/2507.13763)
*Felix-Benedikt Liebrich,Ruodu Wang*

Main category: q-fin.RM

TL;DR: 本文从观测到的泛函值出发，尝试恢复参考测度或找出候选测度以检验法律不变性，基于对法律不变泛函的关键观察，通过具体例子说明方法并对VaR进行修改。


<details>
  <summary>Details</summary>
Motivation: 以往法律不变泛函的参考概率测度通常假定固定，本文从相反视角，根据观测泛函值恢复参考测度或找候选测度检验法律不变性。

Method: 基于法律不变泛函在法律不变域上的关键观察，即这些泛函在有符号测度对偶空间中定义上下支撑集，其上下确界（若存在）是参考测度的标量倍数，特定情况下可表述为三明治定理。

Result: 对熵风险测度、预期损失和风险价值等例子进行详细分析，对风险价值的初始推导程序因支撑集极值平凡而失败，后进行了合适修改。

Conclusion: 提出从观测泛函值恢复或检验参考测度的方法，并通过例子验证，对特殊情况进行了修正。

Abstract: Law-invariant functionals are central to risk management and assign identical
values to random prospects sharing the same distribution under an atomless
reference probability measure. This measure is typically assumed fixed. Here,
we adopt the reverse perspective: given only observed functional values, we aim
to either recover the reference measure or identify a candidate measure to test
for law invariance when that property is not {\em a priori} satisfied. Our
approach is based on a key observation about law-invariant functionals defined
on law-invariant domains. These functionals define lower (upper) supporting
sets in dual spaces of signed measures, and the suprema (infima) of these
supporting sets -- if existent -- are scalar multiples of the reference
measure. In specific cases, this observation can be formulated as a sandwich
theorem.
  We illustrate the methodology through a detailed analysis of prominent
examples: the entropic risk measure, Expected Shortfall, and Value-at-Risk. For
the latter, our elicitation procedure initially fails due to the triviality of
supporting set extrema. We therefore develop a suitable modification.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [118] [Differential Privacy in Kernelized Contextual Bandits via Random Projections](https://arxiv.org/abs/2507.13639)
*Nikola Pavlovic,Sudeep Salgia,Qing Zhao*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider the problem of contextual kernel bandits with stochastic
contexts, where the underlying reward function belongs to a known Reproducing
Kernel Hilbert Space. We study this problem under an additional constraint of
Differential Privacy, where the agent needs to ensure that the sequence of
query points is differentially private with respect to both the sequence of
contexts and rewards. We propose a novel algorithm that achieves the
state-of-the-art cumulative regret of
$\widetilde{\mathcal{O}}(\sqrt{\gamma_TT}+\frac{\gamma_T}{\varepsilon_{\mathrm{DP}}})$
and
$\widetilde{\mathcal{O}}(\sqrt{\gamma_TT}+\frac{\gamma_T\sqrt{T}}{\varepsilon_{\mathrm{DP}}})$
over a time horizon of $T$ in the joint and local models of differential
privacy, respectively, where $\gamma_T$ is the effective dimension of the
kernel and $\varepsilon_{\mathrm{DP}} > 0$ is the privacy parameter. The key
ingredient of the proposed algorithm is a novel private kernel-ridge regression
estimator which is based on a combination of private covariance estimation and
private random projections. It offers a significantly reduced sensitivity
compared to its classical counterpart while maintaining a high prediction
accuracy, allowing our algorithm to achieve the state-of-the-art performance
guarantees.

</details>


### [119] [Conformal Data Contamination Tests for Trading or Sharing of Data](https://arxiv.org/abs/2507.13835)
*Martin V. Vejling,Shashi Raj Pandey,Christophe A. N. Biscio,Petar Popovski*

Main category: stat.ML

TL;DR: 提出无分布、考虑污染的数据共享框架，通过新颖的两样本测试程序确定外部数据价值，经实证验证有效。


<details>
  <summary>Details</summary>
Motivation: 机器学习任务中本地数据有限，外部数据需质量保证，以往工作依赖分布假设且质量检查成本高。

Method: 引入基于共形离群检测理论的两样本测试程序，即共形数据污染测试，能在任意污染水平下有效并控制错误发现率。

Result: 在不同协作学习场景的实证评估中，该方法展现出鲁棒性和有效性。

Conclusion: 共形数据污染测试是一种能提供严格统计质量保证的数据聚合通用程序。

Abstract: The amount of quality data in many machine learning tasks is limited to what
is available locally to data owners. The set of quality data can be expanded
through trading or sharing with external data agents. However, data buyers need
quality guarantees before purchasing, as external data may be contaminated or
irrelevant to their specific learning task. Previous works primarily rely on
distributional assumptions about data from different agents, relegating quality
checks to post-hoc steps involving costly data valuation procedures. We propose
a distribution-free, contamination-aware data-sharing framework that identifies
external data agents whose data is most valuable for model personalization. To
achieve this, we introduce novel two-sample testing procedures, grounded in
rigorous theoretical foundations for conformal outlier detection, to determine
whether an agent's data exceeds a contamination threshold. The proposed tests,
termed conformal data contamination tests, remain valid under arbitrary
contamination levels while enabling false discovery rate control via the
Benjamini-Hochberg procedure. Empirical evaluations across diverse
collaborative learning scenarios demonstrate the robustness and effectiveness
of our approach. Overall, the conformal data contamination test distinguishes
itself as a generic procedure for aggregating data with statistically rigorous
quality guarantees.

</details>


### [120] [A Survey of Dimension Estimation Methods](https://arxiv.org/abs/2507.13887)
*James A. D. Binnie,Paweł Dłotko,John Harvey,Jakub Malinowski,Ka Man Yim*

Main category: stat.ML

TL;DR: 本文综述多种维度估计方法，评估其性能并探讨相关问题，指出部分估计器可能泛化性不佳。


<details>
  <summary>Details</summary>
Motivation: 高维数据有低维结构，虽有多种维度估计器，但缺乏可靠使用指导，需对方法进行评估。

Method: 按方法利用的几何信息对多种维度估计方法分类，评估其性能，研究对曲率和噪声的响应。

Result: 在为基准数据集确定最佳超参数时，过拟合频繁。

Conclusion: 许多估计器可能在测试数据集之外泛化性不好。

Abstract: It is a standard assumption that datasets in high dimension have an internal
structure which means that they in fact lie on, or near, subsets of a lower
dimension. In many instances it is important to understand the real dimension
of the data, hence the complexity of the dataset at hand. A great variety of
dimension estimators have been developed to find the intrinsic dimension of the
data but there is little guidance on how to reliably use these estimators.
  This survey reviews a wide range of dimension estimation methods,
categorising them by the geometric information they exploit: tangential
estimators which detect a local affine structure; parametric estimators which
rely on dimension-dependent probability distributions; and estimators which use
topological or metric invariants.
  The paper evaluates the performance of these methods, as well as
investigating varying responses to curvature and noise. Key issues addressed
include robustness to hyperparameter selection, sample size requirements,
accuracy in high dimensions, precision, and performance on non-linear
geometries. In identifying the best hyperparameters for benchmark datasets,
overfitting is frequent, indicating that many estimators may not generalise
well beyond the datasets on which they have been tested.

</details>


### [121] [Conformalized Regression for Continuous Bounded Outcomes](https://arxiv.org/abs/2507.14023)
*Zhanli Wu,Fabrizio Leisen,F. Javier Rubio*

Main category: stat.ML

TL;DR: 本文针对有界连续结果的回归问题，基于变换模型和β回归开发了共形预测区间，给出理论结果和模拟研究，还在真实数据上展示了其性能。


<details>
  <summary>Details</summary>
Motivation: 现有统计和机器学习文献多关注有界结果的点预测或基于渐近近似的区间预测，本文旨在开发有界结果的共形预测区间。

Method: 基于变换模型和β回归开发共形预测区间，引入基于残差的非一致性度量，考虑异方差性，进行理论分析和模拟研究。

Result: 全共形预测有渐近边际和条件有效性，在模型误设下仍有效；拆分共形预测的模拟研究表明两种方法在有限样本中有有效预测覆盖率；真实数据上展示了方法性能。

Conclusion: 所提出的共形预测区间方法有效，包括在模型误设情况下，且在真实数据上有实际应用价值。

Abstract: Regression problems with bounded continuous outcomes frequently arise in
real-world statistical and machine learning applications, such as the analysis
of rates and proportions. A central challenge in this setting is predicting a
response associated with a new covariate value. Most of the existing
statistical and machine learning literature has focused either on point
prediction of bounded outcomes or on interval prediction based on asymptotic
approximations. We develop conformal prediction intervals for bounded outcomes
based on transformation models and beta regression. We introduce tailored
non-conformity measures based on residuals that are aligned with the underlying
models, and account for the inherent heteroscedasticity in regression settings
with bounded outcomes. We present a theoretical result on asymptotic marginal
and conditional validity in the context of full conformal prediction, which
remains valid under model misspecification. For split conformal prediction, we
provide an empirical coverage analysis based on a comprehensive simulation
study. The simulation study demonstrates that both methods provide valid
finite-sample predictive coverage, including settings with model
misspecification. Finally, we demonstrate the practical performance of the
proposed conformal prediction intervals on real data and compare them with
bootstrap-based alternatives.

</details>


### [122] [Step-DAD: Semi-Amortized Policy-Based Bayesian Experimental Design](https://arxiv.org/abs/2507.14057)
*Marcel Hedman,Desi R. Ivanova,Cong Guan,Tom Rainforth*

Main category: stat.ML

TL;DR: 提出名为Step - DAD的半摊销、基于策略的贝叶斯实验设计方法，测试时适应数据更新策略，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 提升贝叶斯实验设计（BED）策略的灵活性和鲁棒性。

Method: 开发Step - DAD方法，训练设计策略，在实验中根据收集的数据定期更新策略。

Result: Step - DAD在决策和鲁棒性方面始终优于当前最先进的BED方法。

Conclusion: Step - DAD方法通过测试时的适应，相比现有方法能提高设计策略的灵活性和鲁棒性。

Abstract: We develop a semi-amortized, policy-based, approach to Bayesian experimental
design (BED) called Stepwise Deep Adaptive Design (Step-DAD). Like existing,
fully amortized, policy-based BED approaches, Step-DAD trains a design policy
upfront before the experiment. However, rather than keeping this policy fixed,
Step-DAD periodically updates it as data is gathered, refining it to the
particular experimental instance. This test-time adaptation improves both the
flexibility and the robustness of the design strategy compared with existing
approaches. Empirically, Step-DAD consistently demonstrates superior
decision-making and robustness compared with current state-of-the-art BED
methods.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [123] [Flatten Wisely: How Patch Order Shapes Mamba-Powered Vision for MRI Segmentation](https://arxiv.org/abs/2507.13384)
*Osama Hardan,Omar Elshenhabi,Tamer Khattab,Mohamed Mabrok*

Main category: eess.IV

TL;DR: 研究扫描顺序对MRI分割的影响，提出MS2D模块，大规模实验表明扫描顺序显著影响性能，给出优化路径。


<details>
  <summary>Details</summary>
Motivation: Vision Mamba模型将2D图像序列化时扫描顺序选择重要，在医学成像中该选择非平凡，当前缺乏系统研究。

Method: 引入无参数模块MS2D，在三个公共数据集上对21种扫描策略进行大规模基准测试。

Result: 扫描顺序是显著影响因素，性能差异达27 Dice点，空间连续路径表现优于不连续对角扫描。

Conclusion: 扫描顺序是强大且无成本的超参数，给出优化路径以提升Mamba模型在医学成像中的性能。

Abstract: Vision Mamba models promise transformer-level performance at linear
computational cost, but their reliance on serializing 2D images into 1D
sequences introduces a critical, yet overlooked, design choice: the patch scan
order. In medical imaging, where modalities like brain MRI contain strong
anatomical priors, this choice is non-trivial. This paper presents the first
systematic study of how scan order impacts MRI segmentation. We introduce
Multi-Scan 2D (MS2D), a parameter-free module for Mamba-based architectures
that facilitates exploring diverse scan paths without additional computational
cost. We conduct a large-scale benchmark of 21 scan strategies on three public
datasets (BraTS 2020, ISLES 2022, LGG), covering over 70,000 slices. Our
analysis shows conclusively that scan order is a statistically significant
factor (Friedman test: $\chi^{2}_{20}=43.9, p=0.0016$), with performance
varying by as much as 27 Dice points. Spatially contiguous paths -- simple
horizontal and vertical rasters -- consistently outperform disjointed diagonal
scans. We conclude that scan order is a powerful, cost-free hyperparameter, and
provide an evidence-based shortlist of optimal paths to maximize the
performance of Mamba models in medical imaging.

</details>


### [124] [BreastSegNet: Multi-label Segmentation of Breast MRI](https://arxiv.org/abs/2507.13604)
*Qihang Li,Jichen Yang,Yaqian Chen,Yuwen Chen,Hanxue Gu,Lars J. Grimm,Maciej A. Mazurowski*

Main category: eess.IV

TL;DR: 提出用于乳腺MRI的多标签分割算法BreastSegNet，涵盖九个解剖标签，对九种分割模型进行基准测试，nnU - Net ResEncM表现最佳，代码和权重公开。


<details>
  <summary>Details</summary>
Motivation: 现有乳腺MRI分割方法仅关注少数解剖结构，限制了定量分析应用，需更全面的分割方法。

Method: 提出BreastSegNet算法，手动标注1123个MRI切片，对九种分割模型进行基准测试。

Result: nnU - Net ResEncM在所有标签上平均Dice分数达0.694，在某些组织上表现优异。

Conclusion: nnU - Net ResEncM是一种有潜力的乳腺MRI多标签分割模型，相关代码和权重公开，数据后续发布。

Abstract: Breast MRI provides high-resolution imaging critical for breast cancer
screening and preoperative staging. However, existing segmentation methods for
breast MRI remain limited in scope, often focusing on only a few anatomical
structures, such as fibroglandular tissue or tumors, and do not cover the full
range of tissues seen in scans. This narrows their utility for quantitative
analysis. In this study, we present BreastSegNet, a multi-label segmentation
algorithm for breast MRI that covers nine anatomical labels: fibroglandular
tissue (FGT), vessel, muscle, bone, lesion, lymph node, heart, liver, and
implant. We manually annotated a large set of 1123 MRI slices capturing these
structures with detailed review and correction from an expert radiologist.
Additionally, we benchmark nine segmentation models, including U-Net, SwinUNet,
UNet++, SAM, MedSAM, and nnU-Net with multiple ResNet-based encoders. Among
them, nnU-Net ResEncM achieves the highest average Dice scores of 0.694 across
all labels. It performs especially well on heart, liver, muscle, FGT, and bone,
with Dice scores exceeding 0.73, and approaching 0.90 for heart and liver. All
model code and weights are publicly available, and we plan to release the data
at a later date.

</details>


### [125] [Domain-randomized deep learning for neuroimage analysis](https://arxiv.org/abs/2507.13458)
*Malte Hoffmann*

Main category: eess.IV

TL;DR: 深度学习革新神经影像分析，但数据集局限影响模型，域随机化策略可解决泛化问题，本文综述其原理、实现和潜力。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习在神经影像分析中因训练数据集范围窄导致的模型鲁棒性和泛化性问题。

Method: 采用域随机化策略，在具有随机强度和解剖内容的合成图像上训练深度神经网络。

Result: 该策略在多种成像方式中有效，提高了模型泛化性和抗过拟合能力。

Conclusion: 探讨采用该技术的实际考虑，旨在加速可泛化工具的开发，让无大量计算资源和机器学习知识的领域专家也能使用深度学习。

Abstract: Deep learning has revolutionized neuroimage analysis by delivering
unprecedented speed and accuracy. However, the narrow scope of many training
datasets constrains model robustness and generalizability. This challenge is
particularly acute in magnetic resonance imaging (MRI), where image appearance
varies widely across pulse sequences and scanner hardware. A recent
domain-randomization strategy addresses the generalization problem by training
deep neural networks on synthetic images with randomized intensities and
anatomical content. By generating diverse data from anatomical segmentation
maps, the approach enables models to accurately process image types unseen
during training, without retraining or fine-tuning. It has demonstrated
effectiveness across modalities including MRI, computed tomography, positron
emission tomography, and optical coherence tomography, as well as beyond
neuroimaging in ultrasound, electron and fluorescence microscopy, and X-ray
microtomography. This tutorial paper reviews the principles, implementation,
and potential of the synthesis-driven training paradigm. It highlights key
benefits, such as improved generalization and resistance to overfitting, while
discussing trade-offs such as increased computational demands. Finally, the
article explores practical considerations for adopting the technique, aiming to
accelerate the development of generalizable tools that make deep learning more
accessible to domain experts without extensive computational resources or
machine learning knowledge.

</details>


### [126] [D2IP: Deep Dynamic Image Prior for 3D Time-sequence Pulmonary Impedance Imaging](https://arxiv.org/abs/2507.14046)
*Hao Fang,Hao Yu,Sihao Teng,Tao Zhang,Siyi Yuan,Huaiwu He,Zhe Liu,Yunjie Yang*

Main category: eess.IV

TL;DR: 针对无监督学习方法在断层成像中计算成本高的问题，提出D2IP框架用于3D时间序列成像，实验证明其能实现快速准确重建。


<details>
  <summary>Details</summary>
Motivation: 无监督学习方法在断层成像中依赖大量网络参数迭代，计算成本高，限制实际应用，尤其是在复杂3D或时间序列断层成像任务中。

Method: 提出D2IP框架，引入无监督参数预热启动、时间参数传播和定制轻量级重建骨干网络3D - FastResUNet三个关键策略。

Result: 在模拟和临床肺部数据集上实验表明，D2IP能实现快速准确的3D时间序列电阻抗断层成像重建，相比基线提升图像质量，减少计算时间。

Conclusion: D2IP在临床动态肺部成像方面有应用前景。

Abstract: Unsupervised learning methods, such as Deep Image Prior (DIP), have shown
great potential in tomographic imaging due to their training-data-free nature
and high generalization capability. However, their reliance on numerous network
parameter iterations results in high computational costs, limiting their
practical application, particularly in complex 3D or time-sequence tomographic
imaging tasks. To overcome these challenges, we propose Deep Dynamic Image
Prior (D2IP), a novel framework for 3D time-sequence imaging. D2IP introduces
three key strategies - Unsupervised Parameter Warm-Start (UPWS), Temporal
Parameter Propagation (TPP), and a customized lightweight reconstruction
backbone, 3D-FastResUNet - to accelerate convergence, enforce temporal
coherence, and improve computational efficiency. Experimental results on both
simulated and clinical pulmonary datasets demonstrate that D2IP enables fast
and accurate 3D time-sequence Electrical Impedance Tomography (tsEIT)
reconstruction. Compared to state-of-the-art baselines, D2IP delivers superior
image quality, with a 24.8% increase in average MSSIM and an 8.1% reduction in
ERR, alongside significantly reduced computational time (7.1x faster),
highlighting its promise for clinical dynamic pulmonary imaging.

</details>


### [127] [OrthoInsight: Rib Fracture Diagnosis and Report Generation Based on Multi-Modal Large Models](https://arxiv.org/abs/2507.13993)
*Ningyong Wu,Jinzhi Wang,Wenhong Zhao,Chenzhan Yu,Zhigang Xiu,Duwei Dai*

Main category: eess.IV

TL;DR: 提出多模态深度学习框架OrthoInsight用于肋骨骨折诊断与报告生成，评估表现佳，展示多模态学习潜力。


<details>
  <summary>Details</summary>
Motivation: 医学影像数据量增长，手动解读耗时且易出错，需要自动化诊断工具。

Method: 提出OrthoInsight，集成YOLOv9检测骨折、医学知识图谱获取临床背景、微调LLaVA生成诊断报告，结合CT图像视觉特征与专家文本数据。

Result: 在28,675个标注CT图像和专家报告上评估，在多项指标表现佳，平均得分4.28，优于GPT - 4和Claude - 3。

Conclusion: 多模态学习在医学图像分析中有潜力，能为放射科医生提供有效支持。

Abstract: The growing volume of medical imaging data has increased the need for
automated diagnostic tools, especially for musculoskeletal injuries like rib
fractures, commonly detected via CT scans. Manual interpretation is
time-consuming and error-prone. We propose OrthoInsight, a multi-modal deep
learning framework for rib fracture diagnosis and report generation. It
integrates a YOLOv9 model for fracture detection, a medical knowledge graph for
retrieving clinical context, and a fine-tuned LLaVA language model for
generating diagnostic reports. OrthoInsight combines visual features from CT
images with expert textual data to deliver clinically useful outputs. Evaluated
on 28,675 annotated CT images and expert reports, it achieves high performance
across Diagnostic Accuracy, Content Completeness, Logical Coherence, and
Clinical Guidance Value, with an average score of 4.28, outperforming models
like GPT-4 and Claude-3. This study demonstrates the potential of multi-modal
learning in transforming medical image analysis and providing effective support
for radiologists.

</details>


### [128] [UGPL: Uncertainty-Guided Progressive Learning for Evidence-Based Classification in Computed Tomography](https://arxiv.org/abs/2507.14102)
*Shravan Venkatraman,Pavan Kumar S,Rakesh Raj Madavan,Chandrakala S*

Main category: eess.IV

TL;DR: 提出不确定性引导的渐进学习框架UGPL用于CT图像分类，在三个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有CT图像分类方法难以处理病理特征的细微和空间多样性，且统一处理图像限制了检测局部异常的能力。

Method: 引入UGPL框架，先识别诊断模糊区域进行全局到局部分析，用证据深度学习量化预测不确定性，通过非极大值抑制机制提取信息块，结合自适应融合机制。

Result: 在三个CT数据集上，UGPL在肾脏异常、肺癌和COVID - 19检测的准确率分别提高3.29%、2.46%和8.08%，采用完整渐进学习流程性能显著提升。

Conclusion: UGPL能有效进行CT图像分类，不确定性引导组件带来显著益处。

Abstract: Accurate classification of computed tomography (CT) images is essential for
diagnosis and treatment planning, but existing methods often struggle with the
subtle and spatially diverse nature of pathological features. Current
approaches typically process images uniformly, limiting their ability to detect
localized abnormalities that require focused analysis. We introduce UGPL, an
uncertainty-guided progressive learning framework that performs a
global-to-local analysis by first identifying regions of diagnostic ambiguity
and then conducting detailed examination of these critical areas. Our approach
employs evidential deep learning to quantify predictive uncertainty, guiding
the extraction of informative patches through a non-maximum suppression
mechanism that maintains spatial diversity. This progressive refinement
strategy, combined with an adaptive fusion mechanism, enables UGPL to integrate
both contextual information and fine-grained details. Experiments across three
CT datasets demonstrate that UGPL consistently outperforms state-of-the-art
methods, achieving improvements of 3.29%, 2.46%, and 8.08% in accuracy for
kidney abnormality, lung cancer, and COVID-19 detection, respectively. Our
analysis shows that the uncertainty-guided component provides substantial
benefits, with performance dramatically increasing when the full progressive
learning pipeline is implemented. Our code is available at:
https://github.com/shravan-18/UGPL

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [129] [Fair distribution of bundles](https://arxiv.org/abs/2507.13421)
*Pablo Soberón*

Main category: math.CO

TL;DR: 研究公平分割物品束问题，通过至多拆分(r - 1)m个束可公平分配价值，每人约得n/r - mr/2个完整束，r为2的幂时结果最优。


<details>
  <summary>Details</summary>
Motivation: 解决公平分割物品束的问题。

Method: 采用拓扑方法和改进的配置空间/测试映射方案。

Result: 可通过至多拆分(r - 1)m个束公平分配价值，每人约得n/r - mr/2个完整束，r为2的幂时结果最优。

Conclusion: 利用拓扑方法能有效解决公平分割物品束问题，在特定条件下可获得最优结果。

Abstract: In this paper, we study the problem of splitting fairly bundles of items. We
show that given $n$ bundles with $m$ kinds of items in them, it is possible to
distribute the value of each kind of item fairly among $r$ persons by breaking
apart at most $(r-1)m$ bundles. Moreover, we can guarantee that each
participant will receive roughly $n/r - mr/2$ full bundles. The proof methods
are topological and use a modified form of the configuration space/test map
scheme. We obtain optimal results when $r$ is a power of two.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [130] [Random Variate Generation with Formal Guarantees](https://arxiv.org/abs/2507.13494)
*Feras A. Saad,Wonyeol Lee*

Main category: cs.PL

TL;DR: 提出一种有正式保证的随机变量生成新方法，能根据任意数值CDF合成精确生成器，库表现佳。


<details>
  <summary>Details</summary>
Motivation: 寻找一种有正式保证、原理性且实用的随机变量生成方法。

Method: 先指定CDF的有限精度数值程序，再根据CDF生成精确随机变量，提出通用自动方法合成生成器。

Result: 开发的C语言随机变量生成库在多种分布上评估，与GNU科学库相比运行时间有竞争力，且精度、熵效率和自动化程度更高。

Conclusion: 该方法可行，生成库表现良好，在随机变量生成上有优势。

Abstract: This article introduces a new approach to principled and practical random
variate generation with formal guarantees. The key idea is to first specify the
desired probability distribution in terms of a finite-precision numerical
program that defines its cumulative distribution function (CDF), and then
generate exact random variates according to this CDF. We present a universal
and fully automated method to synthesize exact random variate generators given
any numerical CDF implemented in any binary number format, such as
floating-point, fixed-point, and posits. The method is guaranteed to operate
with the same precision used to specify the CDF, does not overflow, avoids
expensive arbitrary-precision arithmetic, and exposes a consistent API. The
method rests on a novel space-time optimal implementation for the class of
generators that attain the information-theoretically optimal Knuth and Yao
entropy rate, consuming the least possible number of input random bits per
output variate. We develop a random variate generation library using our method
in C and evaluate it on a diverse set of ``continuous'' and ``discrete''
distributions, showing competitive runtime with the state-of-the-art GNU
Scientific Library while delivering higher accuracy, entropy efficiency, and
automation.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [131] [Quantum Blockchain Survey: Foundations, Trends, and Gaps](https://arxiv.org/abs/2507.13720)
*Saurav Ghosh*

Main category: cs.CR

TL;DR: 文章回顾后量子区块链和量子区块链发展，对比技术方案，分析安全、可扩展性等权衡，指出研究问题，为量子时代安全区块链系统发展提供参考。


<details>
  <summary>Details</summary>
Motivation: 量子计算对经典区块链系统有风险，需探索应对方案。

Method: 对后量子区块链和量子区块链的关键发展进行综述，分析其密码学基础、架构设计和实现挑战。

Result: 给出技术方案的比较概述，明确了安全、可扩展性和部署方面的权衡。

Conclusion: 为量子时代推进安全区块链系统提供结构化和全面的参考。

Abstract: Quantum computing poses fundamental risks to classical blockchain systems by
undermining widely used cryptographic primitives. In response, two major
research directions have emerged: post-quantum blockchains, which integrate
quantum-resistant algorithms, and quantum blockchains, which leverage quantum
properties such as entanglement and quantum key distribution. This survey
reviews key developments in both areas, analyzing their cryptographic
foundations, architectural designs, and implementation challenges. This work
provides a comparative overview of technical proposals, highlight trade-offs in
security, scalability, and deployment, and identify open research problems
across hardware, consensus, and network design. The goal is to offer a
structured and comprehensive reference for advancing secure blockchain systems
in the quantum era.

</details>


### [132] [PHASE: Passive Human Activity Simulation Evaluation](https://arxiv.org/abs/2507.13505)
*Steven Lamp,Jason D. Hiser,Anh Nguyen-Tuong,Jack W. Davidson*

Main category: cs.CR

TL;DR: 本文提出PHASE框架分析Zeek连接日志，区分人类和非人类活动，还提出新的标记方法，通过案例研究改进合成用户角色的逼真度。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法定量评估合成用户角色的行为保真度，需要有效方法来使网络安全模拟环境更真实。

Method: 提出PHASE机器学习框架，利用Zeek网络设备收集数据，提出基于本地DNS记录的标记方法，应用SHAP分析。

Result: PHASE能以超90%的准确率区分人类和非人类活动，案例研究中识别出非人类模式并改进合成用户角色。

Conclusion: PHASE框架及相关方法可有效提升合成用户角色的类人性，使模拟环境更真实有效。

Abstract: Cybersecurity simulation environments, such as cyber ranges, honeypots, and
sandboxes, require realistic human behavior to be effective, yet no
quantitative method exists to assess the behavioral fidelity of synthetic user
personas. This paper presents PHASE (Passive Human Activity Simulation
Evaluation), a machine learning framework that analyzes Zeek connection logs
and distinguishes human from non-human activity with over 90\% accuracy. PHASE
operates entirely passively, relying on standard network monitoring without any
user-side instrumentation or visible signs of surveillance. All network
activity used for machine learning is collected via a Zeek network appliance to
avoid introducing unnecessary network traffic or artifacts that could disrupt
the fidelity of the simulation environment. The paper also proposes a novel
labeling approach that utilizes local DNS records to classify network traffic,
thereby enabling machine learning analysis. Furthermore, we apply SHAP (SHapley
Additive exPlanations) analysis to uncover temporal and behavioral signatures
indicative of genuine human users. In a case study, we evaluate a synthetic
user persona and identify distinct non-human patterns that undermine behavioral
realism. Based on these insights, we develop a revised behavioral configuration
that significantly improves the human-likeness of synthetic activity yielding a
more realistic and effective synthetic user persona.

</details>


### [133] [GIFT: Gradient-aware Immunization of diffusion models against malicious Fine-Tuning with safe concepts retention](https://arxiv.org/abs/2507.13598)
*Amro Abdalla,Ismail Shaheen,Dan DeGenaro,Rupayan Mallick,Bogdan Raita,Sarah Adel Bargal*

Main category: cs.CR

TL;DR: 提出GIFT技术防御扩散模型恶意微调，实验表明能抗攻击并保安全生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有安全机制如安全检查器易被绕过，概念擦除方法在对抗性微调下失效。

Method: 将免疫问题构建为双层优化问题，上层用表征噪声和最大化降低模型表示有害概念能力，下层在安全数据上保持性能。

Result: 显著削弱模型重新学习有害概念的能力，同时维持在安全内容上的性能。

Conclusion: 为创建抗对抗性微调攻击的更安全生成模型提供了有前景的方向。

Abstract: We present GIFT: a {G}radient-aware {I}mmunization technique to defend
diffusion models against malicious {F}ine-{T}uning while preserving their
ability to generate safe content. Existing safety mechanisms like safety
checkers are easily bypassed, and concept erasure methods fail under
adversarial fine-tuning. GIFT addresses this by framing immunization as a
bi-level optimization problem: the upper-level objective degrades the model's
ability to represent harmful concepts using representation noising and
maximization, while the lower-level objective preserves performance on safe
data. GIFT achieves robust resistance to malicious fine-tuning while
maintaining safe generative quality. Experimental results show that our method
significantly impairs the model's ability to re-learn harmful concepts while
maintaining performance on safe content, offering a promising direction for
creating inherently safer generative models resistant to adversarial
fine-tuning attacks.

</details>


### [134] [Large Language Models in Cybersecurity: Applications, Vulnerabilities, and Defense Techniques](https://arxiv.org/abs/2507.13629)
*Niveen O. Jaffal,Mohammed Alkhanafseh,David Mohaisen*

Main category: cs.CR

TL;DR: 本文全面概述大语言模型（LLMs）在网络安全中的应用，涵盖集成领域与自身漏洞及缓解策略，给出利用LLMs构建网络防御系统的建议。


<details>
  <summary>Details</summary>
Motivation: LLMs在网络安全领域具有变革性潜力，传统方法有局限，需全面了解其应用。

Method: 对LLMs在网络安全中的应用进行综合调研。

Result: 明确了LLMs在关键网络安全领域的集成情况、自身漏洞及缓解策略。

Conclusion: 为利用LLMs构建安全、可扩展和面向未来的网络防御系统提供实用见解和战略建议。

Abstract: Large Language Models (LLMs) are transforming cybersecurity by enabling
intelligent, adaptive, and automated approaches to threat detection,
vulnerability assessment, and incident response. With their advanced language
understanding and contextual reasoning, LLMs surpass traditional methods in
tackling challenges across domains such as IoT, blockchain, and hardware
security. This survey provides a comprehensive overview of LLM applications in
cybersecurity, focusing on two core areas: (1) the integration of LLMs into key
cybersecurity domains, and (2) the vulnerabilities of LLMs themselves, along
with mitigation strategies. By synthesizing recent advancements and identifying
key limitations, this work offers practical insights and strategic
recommendations for leveraging LLMs to build secure, scalable, and future-ready
cyber defense systems.

</details>


### [135] [FuSeFL: Fully Secure and Scalable Cross-Silo Federated Learning](https://arxiv.org/abs/2507.13591)
*Sahar Ghoflsaz Ghinani,Elaheh Sadredini*

Main category: cs.CR

TL;DR: 现有安全联邦学习方法有开销高和忽视全局模型保密问题，本文提出FuSeFL方案，有高安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有安全联邦学习方法存在计算、通信或内存开销高，且忽视全局模型保密性问题，限制了实际应用，尤其是跨机构部署场景。

Method: 提出FuSeFL方案，在客户端对之间使用轻量级安全多方计算进行分散训练，服务器仅负责安全聚合。

Result: FuSeFL能抵御推理威胁，通信延迟降低95%，服务器内存使用降低50%，且准确率更高。

Conclusion: FuSeFL在跨机构场景下具有强安全性和可扩展性，能有效解决现有安全联邦学习的问题。

Abstract: Federated Learning (FL) enables collaborative model training without
centralizing client data, making it attractive for privacy-sensitive domains.
While existing approaches employ cryptographic techniques such as homomorphic
encryption, differential privacy, or secure multiparty computation to mitigate
inference attacks-including model inversion, membership inference, and gradient
leakage-they often suffer from high computational, communication, or memory
overheads. Moreover, many methods overlook the confidentiality of the global
model itself, which may be proprietary and sensitive. These challenges limit
the practicality of secure FL, especially in cross-silo deployments involving
large datasets and strict compliance requirements.
  We present FuSeFL, a fully secure and scalable FL scheme designed for
cross-silo settings. FuSeFL decentralizes training across client pairs using
lightweight secure multiparty computation (MPC), while confining the server's
role to secure aggregation. This design eliminates server bottlenecks, avoids
data offloading, and preserves full confidentiality of data, model, and updates
throughout training. FuSeFL defends against inference threats, achieves up to
95% lower communication latency and 50% lower server memory usage, and improves
accuracy over prior secure FL solutions, demonstrating strong security and
efficiency at scale.

</details>


### [136] [An Adversarial-Driven Experimental Study on Deep Learning for RF Fingerprinting](https://arxiv.org/abs/2507.14109)
*Xinyu Cao,Bimal Adhikari,Shangqing Zhao,Jingxian Wu,Yanjun Pan*

Main category: cs.CR

TL;DR: 研究基于深度学习的射频指纹识别系统安全风险，发现域偏移下DL模型的错误分类可被用作后门，训练原始信号会产生新攻击向量。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的射频指纹识别方法主要关注系统鲁棒性，忽略了安全漏洞，需系统研究其安全风险。

Method: 通过对抗驱动的实验分析系统研究基于深度学习的射频指纹识别系统的安全风险。

Result: 观察到DL模型在域偏移下存在一致的错误分类行为，可被用作后门；训练原始信号使模型将射频指纹与环境和信号模式特征纠缠，产生新攻击向量。

Conclusion: 基于深度学习的射频指纹识别系统存在安全风险，仅靠后处理安全方法无法缓解。

Abstract: Radio frequency (RF) fingerprinting, which extracts unique hardware
imperfections of radio devices, has emerged as a promising physical-layer
device identification mechanism in zero trust architectures and beyond 5G
networks. In particular, deep learning (DL) methods have demonstrated
state-of-the-art performance in this domain. However, existing approaches have
primarily focused on enhancing system robustness against temporal and spatial
variations in wireless environments, while the security vulnerabilities of
these DL-based approaches have often been overlooked. In this work, we
systematically investigate the security risks of DL-based RF fingerprinting
systems through an adversarial-driven experimental analysis. We observe a
consistent misclassification behavior for DL models under domain shifts, where
a device is frequently misclassified as another specific one. Our analysis
based on extensive real-world experiments demonstrates that this behavior can
be exploited as an effective backdoor to enable external attackers to intrude
into the system. Furthermore, we show that training DL models on raw received
signals causes the models to entangle RF fingerprints with environmental and
signal-pattern features, creating additional attack vectors that cannot be
mitigated solely through post-processing security methods such as confidence
thresholds.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [137] [Humans learn to prefer trustworthy AI over human partners](https://arxiv.org/abs/2507.13524)
*Yaomin Jiang,Levin Brinkmann,Anne-Marie Nussberger,Ivan Soraperra,Jean-François Bonnefon,Iyad Rahwan*

Main category: cs.HC

TL;DR: 研究人类在人与AI伙伴间的选择动态，构建游戏实验，发现身份隐藏时人类会误判，公开身份后AI逐渐占优。


<details>
  <summary>Details</summary>
Motivation: 探讨人类如何在人类和AI伙伴间选择，以及在AI竞争压力下如何适应。

Method: 构建基于沟通的伙伴选择游戏，进行三个实验（N = 975）。

Result: 身份隐藏时，人类不优先选AI且会误判；公开身份后，AI初始被选机会降低，但能逐渐胜出。

Conclusion: AI可重塑混合社会的社交互动，为设计更有效合作的混合系统提供参考。

Abstract: Partner selection is crucial for cooperation and hinges on communication. As
artificial agents, especially those powered by large language models (LLMs),
become more autonomous, intelligent, and persuasive, they compete with humans
for partnerships. Yet little is known about how humans select between human and
AI partners and adapt under AI-induced competition pressure. We constructed a
communication-based partner selection game and examined the dynamics in hybrid
mini-societies of humans and bots powered by a state-of-the-art LLM. Through
three experiments (N = 975), we found that bots, though more prosocial than
humans and linguistically distinguishable, were not selected preferentially
when their identity was hidden. Instead, humans misattributed bots' behaviour
to humans and vice versa. Disclosing bots' identity induced a dual effect: it
reduced bots' initial chances of being selected but allowed them to gradually
outcompete humans by facilitating human learning about the behaviour of each
partner type. These findings show how AI can reshape social interaction in
mixed societies and inform the design of more effective and cooperative hybrid
systems.

</details>


### [138] [The Emotion-Memory Link: Do Memorability Annotations Matter for Intelligent Systems?](https://arxiv.org/abs/2507.14084)
*Maria Tsfasman,Ramin Ghorbani,Catholijn M. Jonker,Bernd Dudzik*

Main category: cs.HC

TL;DR: 研究在对话交互中感知的群体情绪与群体记忆性的关系，发现两者关系与随机情况无显著差异，并讨论其影响和指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有情绪识别系统依赖第三方注释，可能不能准确代表第一人称的情绪相关性和记忆体验，为更准确进行用户建模，需研究群体情绪与群体记忆性的关系。

Method: 在动态、非结构化的群体环境中对情绪和记忆性进行基于连续时间的注释，模拟现实对话AI应用场景。

Result: 观察到的情感和记忆性注释之间的关系与随机情况无可靠区别。

Conclusion: 讨论该意外发现对情感计算技术发展和应用的影响，在情感计算的更广泛论述中对研究结果进行背景化，并指出未来研究的重要目标。

Abstract: Humans have a selective memory, remembering relevant episodes and forgetting
the less relevant information. Possessing awareness of event memorability for a
user could help intelligent systems in more accurate user modelling, especially
for such applications as meeting support systems, memory augmentation, and
meeting summarisation. Emotion recognition has been widely studied, since
emotions are thought to signal moments of high personal relevance to users. The
emotional experience of situations and their memorability have traditionally
been considered to be closely tied to one another: moments that are experienced
as highly emotional are considered to also be highly memorable. This
relationship suggests that emotional annotations could serve as proxies for
memorability. However, existing emotion recognition systems rely heavily on
third-party annotations, which may not accurately represent the first-person
experience of emotional relevance and memorability. This is why, in this study,
we empirically examine the relationship between perceived group emotions
(Pleasure-Arousal) and group memorability in the context of conversational
interactions. Our investigation involves continuous time-based annotations of
both emotions and memorability in dynamic, unstructured group settings,
approximating conditions of real-world conversational AI applications such as
online meeting support systems. Our results show that the observed relationship
between affect and memorability annotations cannot be reliably distinguished
from what might be expected under random chance. We discuss the implications of
this surprising finding for the development and applications of Affective
Computing technology. In addition, we contextualise our findings in broader
discourses in the Affective Computing and point out important targets for
future research efforts.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [139] [Preprint: Did I Just Browse A Website Written by LLMs?](https://arxiv.org/abs/2507.13933)
*Sichang "Steven" He,Ramesh Govindan,Harsha V. Madhyastha*

Main category: cs.NI

TL;DR: 本文提出针对网站的LLM主导内容检测管道，训练评估后准确率达100%，并发现此类网站在增长且排名高。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型生成内容可能不可靠且不道德，网站很少披露，现有检测器不足，因此需要开发可靠的LLM主导内容检测器。

Method: 提出一个可扩展的管道，基于多个类散文页面的LLM文本检测器输出对整个网站进行分类，收集两个数据集进行训练和评估。

Result: 训练评估的检测器准确率达100%，在搜索引擎结果和Common Crawl存档的网站中检测到相当比例的LLM主导网站。

Conclusion: LLM主导网站日益普遍且在搜索结果中排名高，其对终端用户和网络生态系统的影响值得关注。

Abstract: Increasingly, web content is automatically generated by large language models
(LLMs) with little human input. We call this "LLM-dominant" content. Since LLMs
plagiarize and hallucinate, LLM-dominant content can be unreliable and
unethical. Yet, websites rarely disclose such content, and human readers
struggle to distinguish it. Thus, we must develop reliable detectors for
LLM-dominant content. However, state-of-the-art LLM detectors are insufficient,
because they perform well mainly on clean, prose-like text, while web content
has complex markup and diverse genres.
  We propose a highly reliable, scalable pipeline that classifies entire
websites. Instead of naively classifying text extracted from each page, we
classify each site based on an LLM text detector's outputs of multiple
prose-like pages. We train and evaluate our detector by collecting 2 distinct
ground truth datasets totaling 120 sites, and obtain 100% accuracies testing
across them. In the wild, we detect a sizable portion of sites as LLM-dominant
among 10k sites in search engine results and 10k in Common Crawl archives. We
find LLM-dominant sites are growing in prevalence and rank highly in search
results, raising questions about their impact on end users and the overall Web
ecosystem.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [140] [Asymptotic behavior of eigenvalues of large rank perturbations of large random matrices](https://arxiv.org/abs/2507.12182)
*Ievgenii Afanasiev,Leonid Berlyand,Mariia Kiyashko*

Main category: math-ph

TL;DR: 本文研究变形Wigner随机矩阵，为增长秩情况开展渐近分析以支持基于随机矩阵理论的剪枝技术。


<details>
  <summary>Details</summary>
Motivation: 现有数学研究仅针对有限秩矩阵S，而实际中秩可能增长，需为增长秩情况开展研究以支持基于随机矩阵理论的新剪枝技术。

Method: 开展渐近分析。

Result: 未提及。

Conclusion: 未提及。

Abstract: The paper is concerned with deformed Wigner random matrices. These matrices
are closely connected with Deep Neural Networks (DNNs): weight matrices of
trained DNNs could be represented in the form $R + S$, where $R$ is random and
$S$ is highly correlated. The spectrum of such matrices plays a key role in
rigorous underpinning of the novel pruning technique based on Random Matrix
Theory. Mathematics has been done only for finite-rank matrix $S$. However, in
practice rank may grow. In this paper we develop asymptotic analysis for the
case of growing rank.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [141] [A Collaborative Framework Integrating Large Language Model and Chemical Fragment Space: Mutual Inspiration for Lead Design](https://arxiv.org/abs/2507.13580)
*Hao Tuo,Yan Li,Xuanning Hu,Haishi Zhao,Xueyan Liu,Bo Yang*

Main category: q-bio.BM

TL;DR: 提出AutoLeadDesign框架用于先导化合物设计，实验显示其优于基线方法，在实际设计中展现能力，揭示工作机制，有药物设计潜力。


<details>
  <summary>Details</summary>
Motivation: 当前组合优化算法在整合领域知识上有挑战，限制识别新型有效结合模式先导化合物的性能。

Method: 提出AutoLeadDesign框架，结合大语言模型中的领域知识和化学片段对化学空间进行有效探索。

Result: AutoLeadDesign在综合实验中优于基线方法；在针对两个临床相关靶点的设计中展现设计能力，结构分析证实抑制模式有效；发现其与基于片段的药物设计机制类似。

Conclusion: AutoLeadDesign为先导化合物设计提供了有效方法，有药物设计应用潜力。

Abstract: Combinatorial optimization algorithm is essential in computer-aided drug
design by progressively exploring chemical space to design lead compounds with
high affinity to target protein. However current methods face inherent
challenges in integrating domain knowledge, limiting their performance in
identifying lead compounds with novel and valid binding mode. Here, we propose
AutoLeadDesign, a lead compounds design framework that inspires extensive
domain knowledge encoded in large language models with chemical fragments to
progressively implement efficient exploration of vast chemical space. The
comprehensive experiments indicate that AutoLeadDesign outperforms baseline
methods. Significantly, empirical lead design campaigns targeting two
clinically relevant targets (PRMT5 and SARS-CoV-2 PLpro) demonstrate
AutoLeadDesign's competence in de novo generation of lead compounds achieving
expert-competitive design efficacy. Structural analysis further confirms their
mechanism-validated inhibitory patterns. By tracing the process of design, we
find that AutoLeadDesign shares analogous mechanisms with fragment-based drug
design which traditionally rely on the expert decision-making, further
revealing why it works. Overall, AutoLeadDesign offers an efficient approach
for lead compounds design, suggesting its potential utility in drug design.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [142] [Deep Micro Solvers for Rough-Wall Stokes Flow in a Heterogeneous Multiscale Method](https://arxiv.org/abs/2507.13902)
*Emanuel Ström,Anna-Karin Tornberg,Ozan Öktem*

Main category: math.NA

TL;DR: 提出针对粗糙壁面斯托克斯流的异构多尺度方法（HMM）的预计算学习方法，用傅里叶神经算子近似局部平均，降低微观问题计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决粗糙壁面斯托克斯流计算中微观问题计算成本高的问题。

Method: 使用傅里叶神经算子近似局部平均，设计网络从局部壁面几何映射到局部流平均的Riesz表示子，进行统计误差传播的理论分析。

Result: 在合适假设下，有界训练损失能带来宏观流中有界误差，预计算学习方法在粗糙度尺度上表现稳定，宏观流HMM解的精度与经典方法相当，但微观问题计算成本显著降低。

Conclusion: 所提出的预计算学习方法有效，能在保证精度的同时降低计算成本。

Abstract: We propose a learned precomputation for the heterogeneous multiscale method
(HMM) for rough-wall Stokes flow. A Fourier neural operator is used to
approximate local averages over microscopic subsets of the flow, which allows
to compute an effective slip length of the fluid away from the roughness. The
network is designed to map from the local wall geometry to the Riesz
representors for the corresponding local flow averages. With such a
parameterisation, the network only depends on the local wall geometry and as
such can be trained independent of boundary conditions. We perform a detailed
theoretical analysis of the statistical error propagation, and prove that under
suitable regularity and scaling assumptions, a bounded training loss leads to a
bounded error in the resulting macroscopic flow. We then demonstrate on a
family of test problems that the learned precomputation performs stably with
respect to the scale of the roughness. The accuracy in the HMM solution for the
macroscopic flow is comparable to when the local (micro) problems are solved
using a classical approach, while the computational cost of solving the micro
problems is significantly reduced.

</details>


### [143] [Multiresolution local smoothness detection in non-uniformly sampled multivariate signals](https://arxiv.org/abs/2507.13480)
*Sara Avesani,Gianluca Giacchi,Michael Multerer*

Main category: math.NA

TL;DR: 提出近线性时间算法检测非均匀采样多变量信号局部正则性，用快速samplet变换，有理论推导和数值研究。


<details>
  <summary>Details</summary>
Motivation: 受基于小波系数衰减行为的边缘检测启发，解决非均匀采样多变量信号局部正则性检测问题。

Method: 在Jaffard引入的微局部空间框架内量化正则性，使用快速samplet变换，建立samplet系数衰减与多变量信号逐点正则性的联系。

Result: 建立了samplet系数衰减与信号正则性的联系，推导了经典Hölder空间和Sobolev - Slobodeckij空间函数的衰减估计，数值研究展示了算法在不同维度信号上的应用。

Conclusion: 传统小波适用于低维结构化数据的正则性检测，samplets在高维和离散数据上表现更稳健。

Abstract: Inspired by edge detection based on the decay behavior of wavelet
coefficients, we introduce a (near) linear-time algorithm for detecting the
local regularity in non-uniformly sampled multivariate signals. Our approach
quantifies regularity within the framework of microlocal spaces introduced by
Jaffard. The central tool in our analysis is the fast samplet transform, a
distributional wavelet transform tailored to scattered data. We establish a
connection between the decay of samplet coefficients and the pointwise
regularity of multivariate signals. As a by product, we derive decay estimates
for functions belonging to classical H\"older spaces and Sobolev-Slobodeckij
spaces. While traditional wavelets are effective for regularity detection in
low-dimensional structured data, samplets demonstrate robust performance even
for higher dimensional and scattered data. To illustrate our theoretical
findings, we present extensive numerical studies detecting local regularity of
one-, two- and three-dimensional signals, ranging from non-uniformly sampled
time series over image segmentation to edge detection in point clouds.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [144] [SEER: Semantic Enhancement and Emotional Reasoning Network for Multimodal Fake News Detection](https://arxiv.org/abs/2507.13415)
*Peican Zhu,Yubo Jing,Le Cheng,Bin Chen,Xiaodong Cui,Lianwei Wu,Keke Tang*

Main category: cs.MM

TL;DR: 提出SEER网络用于多模态假新闻检测，实验证明其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 以往多模态假新闻检测研究忽视大模型语义增强效果和新闻情感特征，且假新闻更易含负面情绪。

Method: 生成图像总结字幕用于语义理解，利用大模型输出进行语义增强，提出专家情感推理模块优化情感特征并推断新闻真实性。

Result: 在两个真实数据集上的大量实验表明SEER优于现有基线。

Conclusion: 所提出的SEER网络在多模态假新闻检测方面表现出色。

Abstract: Previous studies on multimodal fake news detection mainly focus on the
alignment and integration of cross-modal features, as well as the application
of text-image consistency. However, they overlook the semantic enhancement
effects of large multimodal models and pay little attention to the emotional
features of news. In addition, people find that fake news is more inclined to
contain negative emotions than real ones. Therefore, we propose a novel
Semantic Enhancement and Emotional Reasoning (SEER) Network for multimodal fake
news detection. We generate summarized captions for image semantic
understanding and utilize the products of large multimodal models for semantic
enhancement. Inspired by the perceived relationship between news authenticity
and emotional tendencies, we propose an expert emotional reasoning module that
simulates real-life scenarios to optimize emotional features and infer the
authenticity of news. Extensive experiments on two real-world datasets
demonstrate the superiority of our SEER over state-of-the-art baselines.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [145] [Scalable Attribute-Missing Graph Clustering via Neighborhood Differentiatio](https://arxiv.org/abs/2507.13368)
*Yaowen Hu,Wenxuan Tu,Yue Liu,Xinhang Wan,Junyi Yan,Taichun Zhou,Xinwang Liu*

Main category: cs.SI

TL;DR: 提出CMV - ND方法处理大规模和属性缺失的属性图深度图聚类问题，实验表明其能提升多种方法性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界的属性图通常是大规模且属性缺失的，现有深度图聚类方法需要改进以解决这些问题。

Method: 提出CMV - ND方法，通过递归邻域搜索确保结构信息完整性，引入邻域差分策略消除冗余，构建K + 1个互补视图，最后应用现有多视图聚类或DGC方法。

Result: 在六个广泛使用的图数据集上的实验表明，CMV - ND显著提高了各种方法的性能。

Conclusion: CMV - ND方法能有效解决大规模和属性缺失属性图的深度图聚类问题，提升聚类性能。

Abstract: Deep graph clustering (DGC), which aims to unsupervisedly separate the nodes
in an attribute graph into different clusters, has seen substantial potential
in various industrial scenarios like community detection and recommendation.
However, the real-world attribute graphs, e.g., social networks interactions,
are usually large-scale and attribute-missing. To solve these two problems, we
propose a novel DGC method termed \underline{\textbf{C}}omplementary
\underline{\textbf{M}}ulti-\underline{\textbf{V}}iew
\underline{\textbf{N}}eighborhood \underline{\textbf{D}}ifferentiation
(\textit{CMV-ND}), which preprocesses graph structural information into
multiple views in a complete but non-redundant manner. First, to ensure
completeness of the structural information, we propose a recursive neighborhood
search that recursively explores the local structure of the graph by completely
expanding node neighborhoods across different hop distances. Second, to
eliminate the redundancy between neighborhoods at different hops, we introduce
a neighborhood differential strategy that ensures no overlapping nodes between
the differential hop representations. Then, we construct $K+1$ complementary
views from the $K$ differential hop representations and the features of the
target node. Last, we apply existing multi-view clustering or DGC methods to
the views. Experimental results on six widely used graph datasets demonstrate
that CMV-ND significantly improves the performance of various methods.

</details>


### [146] [H-NeiFi: Non-Invasive and Consensus-Efficient Multi-Agent Opinion Guidance](https://arxiv.org/abs/2507.13370)
*Shijun Guo,Haoran Xu,Yaming Yang,Ziyu Guan,Wei Zhao,Xinyi Zhang,Yishan Song,Jiwei Chen*

Main category: cs.SI

TL;DR: 提出分层非侵入式意见引导框架H - NeiFi，提升共识达成速度，保护用户交互自主性。


<details>
  <summary>Details</summary>
Motivation: 现有社交媒体意见引导方法存在侵入性，破坏用户自主性，降低全球共识效率，且缺乏长期视角会加剧宏观层面的分裂。

Method: 建立基于社会角色的两层动态模型，引入非侵入式邻居过滤方法，使用多智能体强化学习通过长期奖励函数优化信息传播路径。

Result: H - NeiFi使共识速度提高22.0%至30.7%，即使没有专家也能保持全局收敛。

Conclusion: 该方法能自然高效地引导共识，为社交网络治理提供新范式。

Abstract: The openness of social media enables the free exchange of opinions, but it
also presents challenges in guiding opinion evolution towards global consensus.
Existing methods often directly modify user views or enforce cross-group
connections. These intrusive interventions undermine user autonomy, provoke
psychological resistance, and reduce the efficiency of global consensus.
Additionally, due to the lack of a long-term perspective, promoting local
consensus often exacerbates divisions at the macro level. To address these
issues, we propose the hierarchical, non-intrusive opinion guidance framework,
H-NeiFi. It first establishes a two-layer dynamic model based on social roles,
considering the behavioral characteristics of both experts and non-experts.
Additionally, we introduce a non-intrusive neighbor filtering method that
adaptively controls user communication channels. Using multi-agent
reinforcement learning (MARL), we optimize information propagation paths
through a long-term reward function, avoiding direct interference with user
interactions. Experiments show that H-NeiFi increases consensus speed by 22.0%
to 30.7% and maintains global convergence even in the absence of experts. This
approach enables natural and efficient consensus guidance by protecting user
interaction autonomy, offering a new paradigm for social network governance.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [147] [Application Placement with Constraint Relaxation](https://arxiv.org/abs/2507.13895)
*Damiano Azzolini,Marco Duca,Stefano Forti,Francesco Gallo,Antonio Ielo*

Main category: cs.LO

TL;DR: 本文利用回答集编程优化能力解决云边网络服务放置问题，实验表明该方法在模拟场景中有效。


<details>
  <summary>Details</summary>
Motivation: 现有云边网络服务放置问题解决方案无法处理不可满足的问题实例和偏好需求。

Method: 利用回答集编程优化能力解决云边网络服务放置的组合优化问题。

Result: 在模拟环境中的实验结果表明该方法在逼真的网络和应用中有效。

Conclusion: 利用回答集编程优化能力解决云边网络服务放置问题是可行且有效的。

Abstract: Novel utility computing paradigms rely upon the deployment of multi-service
applications to pervasive and highly distributed cloud-edge infrastructure
resources. Deciding onto which computational nodes to place services in
cloud-edge networks, as per their functional and non-functional constraints,
can be formulated as a combinatorial optimisation problem. Most existing
solutions in this space are not able to deal with \emph{unsatisfiable} problem
instances, nor preferences, i.e. requirements that DevOps may agree to relax to
obtain a solution. In this article, we exploit Answer Set Programming
optimisation capabilities to tackle this problem. Experimental results in
simulated settings show that our approach is effective on lifelike networks and
applications.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [148] [PGR-DRC: Pre-Global Routing DRC Violation Prediction Using Unsupervised Learning](https://arxiv.org/abs/2507.13355)
*Riadul Islam,Dhandeep Challagundla*

Main category: cs.AR

TL;DR: 本文提出无监督DRC违规预测方法，解决传统模型需平衡数据集和长训练时间问题，验证显示准确率高且训练时间短。


<details>
  <summary>Details</summary>
Motivation: 传统基于监督学习的ML和NN模型需大平衡数据集和长训练时间，为解决此问题开展研究。

Method: 提出无监督DRC违规预测方法，用单类不平衡数据集构建模型并设阈值分类。用CMOS 28 nm技术和相关工具实现不同计算核心，将布局划网格收集数据验证。

Result: 所提方法预测测试准确率99.95%，现有SVM和NN模型分别为85.44%和98.74%；所提方法训练时间比SVM和NN模型分别低约26.3倍和最多6003倍。

Conclusion: 所提无监督DRC违规预测方法准确率高、训练时间短，有显著优势。

Abstract: Leveraging artificial intelligence (AI)-driven electronic design and
automation (EDA) tools, high-performance computing, and parallelized algorithms
are essential for next-generation microprocessor innovation, ensuring continued
progress in computing, AI, and semiconductor technology. Machine learning-based
design rule checking (DRC) and lithography hotspot detection can improve
first-pass silicon success. However, conventional ML and neural network
(NN)-based models use supervised learning and require a large balanced dataset
(in terms of positive and negative classes) and training time. This research
addresses those key challenges by proposing the first-ever unsupervised DRC
violation prediction methodology. The proposed model can be built using any
unbalanced dataset using only one class and set a threshold for it, then
fitting any new data querying if they are within the boundary of the model for
classification. This research verified the proposed model by implementing
different computational cores using CMOS 28 nm technology and Synopsys Design
Compiler and IC Compiler II tools. Then, layouts were divided into virtual
grids to collect about 60k data for analysis and verification. The proposed
method has 99.95% prediction test accuracy, while the existing support vector
machine (SVM) and neural network (NN) models have 85.44\% and 98.74\% accuracy,
respectively. In addition, the proposed methodology has about 26.3x and up to
6003x lower training times compared to SVM and NN-models, respectively.

</details>


### [149] [VerilogDB: The Largest, Highest-Quality Dataset with a Preprocessing Framework for LLM-based RTL Generation](https://arxiv.org/abs/2507.13369)
*Paul E. Calzada,Zahin Ibnat,Tanvir Rahman,Kamal Kandula,Danyu Lu,Sujan Kumar Saha,Farimah Farahmandi,Mark Tehranipoor*

Main category: cs.AR

TL;DR: 本文研究用大语言模型生成RTL代码的文献，构建Verilog数据集并评估，探索其应用。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在硬件设计自动化中流行，需构建高质量数据集用于训练和微调。

Method: 通过自动化三管齐下的过程构建数据集，包括数据库创建管理、数据收集和预处理。

Result: 构建了包含20392个Verilog样本、751MB代码数据的数据集。

Conclusion: 评估了数据集，探讨挑战和潜在应用，为基于大语言模型的硬件生成研究提供支持。

Abstract: Large Language Models (LLMs) are gaining popularity for hardware design
automation, particularly through Register Transfer Level (RTL) code generation.
In this work, we examine the current literature on RTL generation using LLMs
and identify key requirements for training and fine-tuning datasets. We
construct a robust Verilog dataset through an automated three-pronged process
involving database (DB) creation and management with PostgreSQL, data
collection from code hosting sites like OpenCores and GitHub, and data
preprocessing to verify the codes' syntax, run logic synthesis, and extract
relevant module metadata. We implement a scalable and efficient DB
infrastructure to support analysis and detail our preprocessing pipeline to
enforce high-quality data before DB insertion. The resulting dataset comprises
20,392 Verilog samples, 751 MB of Verilog code data, which is the largest
high-quality Verilog dataset for LLM fine-tuning to our knowledge. We further
evaluate the dataset, address associated challenges, and explore potential
applications for future research and development in LLM-based hardware
generation.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [150] [Quantum Boltzmann Machines using Parallel Annealing for Medical Image Classification](https://arxiv.org/abs/2507.14116)
*Daniëlle Schuman,Mark V. Seebode,Tobias Rohe,Maximilian Balthasar Mansky,Michael Schroedl-Baumann,Jonas Stein,Claudia Linnhoff-Popien,Florian Krellner*

Main category: quant-ph

TL;DR: 本文提出改进的并行量子退火方法在有监督设置下训练量子玻尔兹曼机（QBMs），并在医学图像上测试，结果显示该方法效果好且有速度提升。


<details>
  <summary>Details</summary>
Motivation: 当前基于退火的QBMs训练成本高，限制其在NISQ时代的应用，需降低成本并提升其实用性。

Method: 提出改进的并行量子退火方法，在有监督设置下训练QBMs，节省编码输入的量子比特。

Result: 使用该方法的QBMs在较少的训练轮次下取得与类似规模卷积神经网络相当的结果，并行退火技术比常规退火方法提速近70%。

Conclusion: 该改进的并行量子退火方法能有效训练QBMs，使其更接近实际应用，且有显著的速度提升。

Abstract: Exploiting the fact that samples drawn from a quantum annealer inherently
follow a Boltzmann-like distribution, annealing-based Quantum Boltzmann
Machines (QBMs) have gained increasing popularity in the quantum research
community. While they harbor great promises for quantum speed-up, their usage
currently stays a costly endeavor, as large amounts of QPU time are required to
train them. This limits their applicability in the NISQ era. Following the idea
of No\`e et al. (2024), who tried to alleviate this cost by incorporating
parallel quantum annealing into their unsupervised training of QBMs, this paper
presents an improved version of parallel quantum annealing that we employ to
train QBMs in a supervised setting. Saving qubits to encode the inputs, the
latter setting allows us to test our approach on medical images from the
MedMNIST data set (Yang et al., 2023), thereby moving closer to real-world
applicability of the technology. Our experiments show that QBMs using our
approach already achieve reasonable results, comparable to those of
similarly-sized Convolutional Neural Networks (CNNs), with markedly smaller
numbers of epochs than these classical models. Our parallel annealing technique
leads to a speed-up of almost 70 % compared to regular annealing-based BM
executions.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [151] [Consistent Explainers or Unreliable Narrators? Understanding LLM-generated Group Recommendations](https://arxiv.org/abs/2507.13705)
*Cedric Waterschoot,Nava Tintarev,Francesco Barile*

Main category: cs.CL

TL;DR: 本文对比大语言模型（LLMs）和基于社会选择的聚合策略，评估其在群体推荐系统（GRS）中的推荐和解释，发现LLMs推荐类似ADD聚合，解释依赖评分数量，且解释存在问题影响透明度。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在GRS中生成的推荐和解释，与基于社会选择的聚合策略对比。

Method: 将LLMs生成的推荐和解释与社会选择的聚合策略进行比较。

Result: LLMs推荐类似ADD聚合，解释常提及评分平均，群体结构不影响推荐，LLMs会提及额外标准，解释中额外标准依赖评分数量。

Conclusion: 研究结果对GRS中LLMs和标准聚合策略有重要意义，额外标准表明标准聚合方法在大项目集时可能低效，解释的不一致和模糊性破坏了透明度和可解释性。

Abstract: Large Language Models (LLMs) are increasingly being implemented as joint
decision-makers and explanation generators for Group Recommender Systems (GRS).
In this paper, we evaluate these recommendations and explanations by comparing
them to social choice-based aggregation strategies. Our results indicate that
LLM-generated recommendations often resembled those produced by Additive
Utilitarian (ADD) aggregation. However, the explanations typically referred to
averaging ratings (resembling but not identical to ADD aggregation). Group
structure, uniform or divergent, did not impact the recommendations.
Furthermore, LLMs regularly claimed additional criteria such as user or item
similarity, diversity, or used undefined popularity metrics or thresholds. Our
findings have important implications for LLMs in the GRS pipeline as well as
standard aggregation strategies. Additional criteria in explanations were
dependent on the number of ratings in the group scenario, indicating potential
inefficiency of standard aggregation methods at larger item set sizes.
Additionally, inconsistent and ambiguous explanations undermine transparency
and explainability, which are key motivations behind the use of LLMs for GRS.

</details>


### [152] [Question-Answer Extraction from Scientific Articles Using Knowledge Graphs and Large Language Models](https://arxiv.org/abs/2507.13827)
*Hosein Azarbonyad,Zi Long Zhu,Georgios Cheirmpos,Zubair Afzal,Vikrant Yadav,Georgios Tsatsaronis*

Main category: cs.CL

TL;DR: 本文提出两种从科学文章中以问答对形式提取关键概念和贡献的方法，评估显示基于知识图谱的方法能有效捕捉文章主要观点，微调实体关系提取模型对提取高质量三元组很关键。


<details>
  <summary>Details</summary>
Motivation: 学者阅读或引用文章时希望快速识别和理解其主要观点，因此要从科学文章中以问答对形式提取关键概念和贡献。

Method: 提出两种生成问答对的方法，一是仅基于文章内容，选段落、用大语言模型生成问题并排序后生成答案；二是利用知识图谱生成问答对，构建知识图谱并采用显著三元组提取方法。通过专家根据预定义指标评估问答对质量。

Result: 评估表明基于知识图谱的方法能有效捕捉文章主要观点，微调实体关系提取模型对从文档中提取高质量三元组至关重要。

Conclusion: 基于知识图谱的方法能有效提取文章关键概念和贡献，微调实体关系提取模型有重要意义。

Abstract: When deciding to read an article or incorporate it into their research,
scholars often seek to quickly identify and understand its main ideas. In this
paper, we aim to extract these key concepts and contributions from scientific
articles in the form of Question and Answer (QA) pairs. We propose two distinct
approaches for generating QAs. The first approach involves selecting salient
paragraphs, using a Large Language Model (LLM) to generate questions, ranking
these questions by the likelihood of obtaining meaningful answers, and
subsequently generating answers. This method relies exclusively on the content
of the articles. However, assessing an article's novelty typically requires
comparison with the existing literature. Therefore, our second approach
leverages a Knowledge Graph (KG) for QA generation. We construct a KG by
fine-tuning an Entity Relationship (ER) extraction model on scientific articles
and using it to build the graph. We then employ a salient triplet extraction
method to select the most pertinent ERs per article, utilizing metrics such as
the centrality of entities based on a triplet TF-IDF-like measure. This measure
assesses the saliency of a triplet based on its importance within the article
compared to its prevalence in the literature. For evaluation, we generate QAs
using both approaches and have them assessed by Subject Matter Experts (SMEs)
through a set of predefined metrics to evaluate the quality of both questions
and answers. Our evaluations demonstrate that the KG-based approach effectively
captures the main ideas discussed in the articles. Furthermore, our findings
indicate that fine-tuning the ER extraction model on our scientific corpus is
crucial for extracting high-quality triplets from such documents.

</details>


### [153] [DENSE: Longitudinal Progress Note Generation with Temporal Modeling of Heterogeneous Clinical Notes Across Hospital Visits](https://arxiv.org/abs/2507.14079)
*Garapati Keerthana,Manik Gupta*

Main category: cs.CL

TL;DR: 论文介绍DENSE系统，利用细粒度分类、时间对齐机制和临床检索策略，借助大语言模型生成进展记录，评估显示其有良好纵向保真度，支持下游任务。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录中进展记录在大规模数据集中代表性不足，如MIMIC - III数据集仅8.56%的住院记录包含进展记录，存在纵向患者记录缺口。

Method: 提出DENSE系统，引入细粒度笔记分类和时间对齐机制，利用临床检索策略从当前和既往就诊记录中识别相关内容，提示大语言模型生成进展记录。

Result: 在有多次就诊和完整进展记录的患者队列上评估，生成的笔记纵向保真度高，时间对齐比率达1.089，优于原始笔记的连贯性。

Conclusion: DENSE系统可恢复碎片化文档的叙事连贯性，支持下游任务，为现实医疗场景中基于大语言模型的笔记合成提供可扩展解决方案。

Abstract: Progress notes are among the most clinically meaningful artifacts in an
Electronic Health Record (EHR), offering temporally grounded insights into a
patient's evolving condition, treatments, and care decisions. Despite their
importance, they are severely underrepresented in large-scale EHR datasets. For
instance, in the widely used Medical Information Mart for Intensive Care III
(MIMIC-III) dataset, only about $8.56\%$ of hospital visits include progress
notes, leaving gaps in longitudinal patient narratives. In contrast, the
dataset contains a diverse array of other note types, each capturing different
aspects of care.
  We present DENSE (Documenting Evolving Progress Notes from Scattered
Evidence), a system designed to align with clinical documentation workflows by
simulating how physicians reference past encounters while drafting progress
notes. The system introduces a fine-grained note categorization and a temporal
alignment mechanism that organizes heterogeneous notes across visits into
structured, chronological inputs. At its core, DENSE leverages a clinically
informed retrieval strategy to identify temporally and semantically relevant
content from both current and prior visits. This retrieved evidence is used to
prompt a large language model (LLM) to generate clinically coherent and
temporally aware progress notes.
  We evaluate DENSE on a curated cohort of patients with multiple visits and
complete progress note documentation. The generated notes demonstrate strong
longitudinal fidelity, achieving a temporal alignment ratio of $1.089$,
surpassing the continuity observed in original notes. By restoring narrative
coherence across fragmented documentation, our system supports improved
downstream tasks such as summarization, predictive modeling, and clinical
decision support, offering a scalable solution for LLM-driven note synthesis in
real-world healthcare settings.

</details>


### [154] [Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts (PLABA) track](https://arxiv.org/abs/2507.14096)
*Brian Ondov,William Xia,Kush Attal,Ishita Unde,Jerry He,Hoa Dang,Ian Soboroff,Dina Demner-Fushman*

Main category: cs.CL

TL;DR: 举办PLABA赛道评估生物医学文献转通俗语言系统，展示大语言模型潜力与不足，需改进自动基准工具。


<details>
  <summary>Details</summary>
Motivation: 语言模型在生物医学文献转通俗语言有潜力，但需严格评估，举办赛道以推动研究和高质量评估。

Method: 在2023和2024年文本检索会议举办PLABA赛道，设两项任务，对任务1自动评估开发参考集，两项任务均由专家手动评估。

Result: 12个国家12支队伍参赛，任务1顶级模型事实准确性和完整性接近人类，但简单性和简洁性不足，自动指标与手动判断相关性差；任务2系统识别难词和分类替换有困难，大语言模型生成替换时手动评估准确性、完整性和简单性较好，但简洁性不足。

Conclusion: PLABA赛道显示大语言模型在适应生物医学文献方面有前景，也凸显其不足和改进自动基准工具的必要性。

Abstract: Objective: Recent advances in language models have shown potential to adapt
professional-facing biomedical literature to plain language, making it
accessible to patients and caregivers. However, their unpredictability,
combined with the high potential for harm in this domain, means rigorous
evaluation is necessary. Our goals with this track were to stimulate research
and to provide high-quality evaluation of the most promising systems.
  Methods: We hosted the Plain Language Adaptation of Biomedical Abstracts
(PLABA) track at the 2023 and 2024 Text Retrieval Conferences. Tasks included
complete, sentence-level, rewriting of abstracts (Task 1) as well as
identifying and replacing difficult terms (Task 2). For automatic evaluation of
Task 1, we developed a four-fold set of professionally-written references.
Submissions for both Tasks 1 and 2 were provided extensive manual evaluation
from biomedical experts.
  Results: Twelve teams spanning twelve countries participated in the track,
with models from multilayer perceptrons to large pretrained transformers. In
manual judgments of Task 1, top-performing models rivaled human levels of
factual accuracy and completeness, but not simplicity or brevity. Automatic,
reference-based metrics generally did not correlate well with manual judgments.
In Task 2, systems struggled with identifying difficult terms and classifying
how to replace them. When generating replacements, however, LLM-based systems
did well in manually judged accuracy, completeness, and simplicity, though not
in brevity.
  Conclusion: The PLABA track showed promise for using Large Language Models to
adapt biomedical literature for the general public, while also highlighting
their deficiencies and the need for improved automatic benchmarking tools.

</details>


### [155] [Persona-Based Synthetic Data Generation Using Multi-Stage Conditioning with Large Language Models for Emotion Recognition](https://arxiv.org/abs/2507.13380)
*Keito Inoshita,Rushia Harada*

Main category: cs.CL

TL;DR: 因高质量多样情感数据集稀缺，提出PersonaGen框架生成情感丰富文本，评估显示其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决情感识别领域因情感数据集稀缺导致难以开发高性能模型的问题。

Method: 采用多阶段基于角色的条件化，结合人口属性、社会文化背景和情境构建虚拟角色来引导情感表达生成。

Result: PersonaGen在生成多样、连贯和有区分度的情感表达上显著优于基线方法。

Conclusion: PersonaGen有潜力作为增强或替代现实情感数据集的可靠选择。

Abstract: In the field of emotion recognition, the development of high-performance
models remains a challenge due to the scarcity of high-quality, diverse
emotional datasets. Emotional expressions are inherently subjective, shaped by
individual personality traits, socio-cultural backgrounds, and contextual
factors, making large-scale, generalizable data collection both ethically and
practically difficult. To address this issue, we introduce PersonaGen, a novel
framework for generating emotionally rich text using a Large Language Model
(LLM) through multi-stage persona-based conditioning. PersonaGen constructs
layered virtual personas by combining demographic attributes, socio-cultural
backgrounds, and detailed situational contexts, which are then used to guide
emotion expression generation. We conduct comprehensive evaluations of the
generated synthetic data, assessing semantic diversity through clustering and
distributional metrics, human-likeness via LLM-based quality scoring, realism
through comparison with real-world emotion corpora, and practical utility in
downstream emotion classification tasks. Experimental results show that
PersonaGen significantly outperforms baseline methods in generating diverse,
coherent, and discriminative emotion expressions, demonstrating its potential
as a robust alternative for augmenting or replacing real-world emotional
datasets.

</details>


### [156] [TopicImpact: Improving Customer Feedback Analysis with Opinion Units for Topic Modeling and Star-Rating Prediction](https://arxiv.org/abs/2507.13392)
*Emil Häglund,Johanna Björklund*

Main category: cs.CL

TL;DR: 通过将主题建模流程重构为对意见单元操作，提升客户评论洞察提取效果，还关联主题情感与业务指标，介绍系统实现、用例等并评估有效性。


<details>
  <summary>Details</summary>
Motivation: 改善从客户评论中提取洞察的效果。

Method: 将主题建模流程重构为对意见单元操作，关联主题和情感与业务指标。

Result: 提升后续主题建模性能，得到连贯且可解释的主题，能捕捉各主题相关情感，可洞察特定客户关注点对业务结果的影响。

Conclusion: 介绍了系统实现、用例和优势，评估了创建连贯主题的有效性以及集成主题和情感模式进行准确星级评分预测的方法。

Abstract: We improve the extraction of insights from customer reviews by restructuring
the topic modelling pipeline to operate on opinion units - distinct statements
that include relevant text excerpts and associated sentiment scores. Prior work
has demonstrated that such units can be reliably extracted using large language
models. The result is a heightened performance of the subsequent topic
modeling, leading to coherent and interpretable topics while also capturing the
sentiment associated with each topic. By correlating the topics and sentiments
with business metrics, such as star ratings, we can gain insights on how
specific customer concerns impact business outcomes. We present our system's
implementation, use cases, and advantages over other topic modeling and
classification solutions. We also evaluate its effectiveness in creating
coherent topics and assess methods for integrating topic and sentiment
modalities for accurate star-rating prediction.

</details>


### [157] [Mitigating Stylistic Biases of Machine Translation Systems via Monolingual Corpora Only](https://arxiv.org/abs/2507.13395)
*Xuanqi Gao,Weipeng Jiang,Juan Zhai,Shiqing Ma,Siyi Xie,Xinyang Yin,Chao Shen*

Main category: cs.CL

TL;DR: 提出Babel框架，仅用单语语料增强神经机器翻译的风格保真度，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 神经机器翻译在保留风格细微差别方面存在挑战，现有方法常需平行语料。

Method: 引入Babel框架，含基于上下文嵌入的风格检测器和基于扩散的风格应用器，作为后处理模块集成到现有NMT系统。

Result: 在五个领域实验中，识别风格不一致的精度达88.21%，风格保留提升150%，语义相似度0.92，人工评估表明能更好保留源文本风格。

Conclusion: Babel框架能有效提升神经机器翻译的风格保真度，且无需架构修改和平行风格数据。

Abstract: The advent of neural machine translation (NMT) has revolutionized
cross-lingual communication, yet preserving stylistic nuances remains a
significant challenge. While existing approaches often require parallel corpora
for style preservation, we introduce Babel, a novel framework that enhances
stylistic fidelity in NMT using only monolingual corpora. Babel employs two key
components: (1) a style detector based on contextual embeddings that identifies
stylistic disparities between source and target texts, and (2) a
diffusion-based style applicator that rectifies stylistic inconsistencies while
maintaining semantic integrity. Our framework integrates with existing NMT
systems as a post-processing module, enabling style-aware translation without
requiring architectural modifications or parallel stylistic data. Extensive
experiments on five diverse domains (law, literature, scientific writing,
medicine, and educational content) demonstrate Babel's effectiveness: it
identifies stylistic inconsistencies with 88.21% precision and improves
stylistic preservation by 150% while maintaining a high semantic similarity
score of 0.92. Human evaluation confirms that translations refined by Babel
better preserve source text style while maintaining fluency and adequacy.

</details>


### [158] [Causal Language Control in Multilingual Transformers via Sparse Feature Steering](https://arxiv.org/abs/2507.13410)
*Cheng-Ting Chou,George Liu,Jessica Sun,Cole Blondin,Kevin Zhu,Vasu Sharma,Sean O'Brien*

Main category: cs.CL

TL;DR: 研究用稀疏自编码器（SAE）特征控制多语言大模型生成语言，修改单个SAE特征实现高达90%语言转换成功率，证明稀疏特征引导用于可控多语言生成的潜力。


<details>
  <summary>Details</summary>
Motivation: 在零样本设置下，确定性控制多语言大模型目标生成语言是挑战，研究能否利用SAE特征在推理时引导大模型生成语言。

Method: 在Gemma - 2B和Gemma - 9B的残差流上使用预训练SAE，识别英语与四种目标语言激活差异显著的特征，修改单个SAE特征。

Result: 通过FastText语言分类测量，实现高达90%的受控语言转换成功率，LaBSE相似度表明保留语义保真度；语言引导在变压器层中后期最有效，特定注意力头会增强效果。

Conclusion: 稀疏特征引导作为一种轻量级且可解释机制，用于可控多语言生成具有前景。

Abstract: Deterministically controlling the target generation language of large
multilingual language models (LLMs) remains a fundamental challenge,
particularly in zero-shot settings where neither explicit language prompts nor
fine-tuning are available. In this work, we investigate whether sparse
autoencoder (SAE) features, previously shown to correlate with interpretable
model behaviors, can be leveraged to steer the generated language of LLMs
during inference. Leveraging pretrained SAEs on the residual streams of
Gemma-2B and Gemma-9B, we identify features whose activations differ most
significantly between English and four target languages: Chinese, Japanese,
Spanish, and French. By modifying just a single SAE feature at one transformer
layer, we achieve controlled language shifts with up to 90\% success, as
measured by FastText language classification, while preserving semantic
fidelity according to LaBSE (Language-Agnostic BERT Sentence Embedding)
similarity. Our analysis reveals that language steering is most effective in
mid-to-late transformer layers and is amplified by specific attention heads
disproportionately associated with language-sensitive SAE features. These
results demonstrate the promise of sparse feature steering as a lightweight and
interpretable mechanism for controllable multilingual generation.

</details>


### [159] [Aligning Knowledge Graphs and Language Models for Factual Accuracy](https://arxiv.org/abs/2507.13411)
*Nur A Zarin Nishat,Andrea Coletta,Luigi Bellomarini,Kossi Amouzouvi,Jens Lehmann,Sahar Vahdati*

Main category: cs.CL

TL;DR: 本文提出ALIGNED - LLM方法，将知识图谱融入语言模型潜在空间以提升事实性，在问答基准数据集和实际金融用例中均有显著效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型易产生幻觉，将知识图谱集成到语言模型是克服该挑战的有前景方案。

Method: 引入ALIGNED - LLM方法，受LLaVA启发，使用预训练知识图谱嵌入模型的嵌入和可训练投影层对齐实体和文本嵌入。

Result: 在三个问答基准数据集和欧洲大型央行的实际金融用例中测试，显示出显著改进。

Conclusion: ALIGNED - LLM方法是一种简单有效的提升语言模型事实性、减少幻觉的方法。

Abstract: Large language models like GPT-4, Gemini, and Claude have transformed natural
language processing (NLP) tasks such as question answering, dialogue
generation, summarization, and so forth; yet their susceptibility to
hallucination stands as one of the major challenges. Among numerous approaches
to overcome this challenge, integration of Knowledge Graphs (KGs) into language
models has emerged as a promising solution as it provides structured, reliable,
domain-specific, and up-to-date external information to the language models. In
this paper, we introduce ALIGNed-LLM, a simple yet effective approach to
improve language models' factuality via a lean strategy to infuse KGs into the
latent space of language models inspired by LLaVA where visual and textual
information is infused. We use embeddings from a pre-trained Knowledge Graph
Embedding (KGE) model, such as TransE, and a trainable projection layer to
align entity and text embeddings. This alignment enables the language model to
distinguish between similar entities improving factual grounding and reducing
hallucination. We tested our approach on three popular questions-answering
benchmark datasets alongside language models of varying sizes, showing
significant improvement. Furthermore, we applied our approach to a real-world
financial use case from a large central bank in Europe, which demands high
accuracy and precision, demonstrating a substantial improvement of the LLM
answers.

</details>


### [160] [SAFT: Structure-Aware Fine-Tuning of LLMs for AMR-to-Text Generation](https://arxiv.org/abs/2507.13381)
*Rafiq Kamel,Filippo Guerranti,Simon Geisler,Stephan Günnemann*

Main category: cs.CL

TL;DR: 介绍SAFT方法用于提升大语言模型处理结构化输入（AMR）的性能，在AMR 3.0上取得新SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前处理结构化输入（如AMR）的方法存在任意线性化丢弃结构信息或架构与标准大语言模型不兼容的问题，需要更好的方法。

Method: 提出SAFT，一种结构感知微调方法，从转换后的AMR的磁拉普拉斯算子计算方向敏感的位置编码并投影到LLM的嵌入空间。

Result: SAFT在AMR 3.0上取得新SOTA，BLEU值比基线提高3.5，且随着图复杂度增加收益更大。

Conclusion: SAFT为连接结构化数据和语言模型提供了通用有效的途径。

Abstract: Large Language Models (LLMs) are increasingly applied to tasks involving
structured inputs such as graphs. Abstract Meaning Representations (AMRs),
which encode rich semantics as directed graphs, offer a rigorous testbed for
evaluating LLMs on text generation from such structures. Yet, current methods
often arbitrarily linearize AMRs, discarding key structural cues, or rely on
architectures incompatible with standard LLMs. We introduce SAFT, a
structure-aware fine-tuning approach that injects graph topology into
pretrained LLMs without architectural changes. We compute direction-sensitive
positional encodings from the magnetic Laplacian of transformed AMRs and
project them into the embedding space of the LLM. While possibly applicable to
any graph-structured inputs, we focus on AMR-to-text generation as a
representative and challenging benchmark. SAFT sets a new state-of-the-art on
AMR 3.0 with a 3.5 BLEU improvement over baselines. Gains scale with graph
complexity, highlighting the value of structure-aware representations in
enhancing LLM performance. SAFT offers a general and effective pathway for
bridging structured data and language models.

</details>


### [161] [Context-Based Fake News Detection using Graph Based Approach: ACOVID-19 Use-case](https://arxiv.org/abs/2507.13382)
*Chandrashekar Muniyappa,Sirisha Velampalli*

Main category: cs.CL

TL;DR: 本文提出基于图的新颖方法检测假新闻，用NLP技术转换文章为图结构，用MDL - GBAD算法挖掘图。


<details>
  <summary>Details</summary>
Motivation: 解决当今数字世界中假新闻快速传播的问题。

Method: 从Kaggle获取数据集，加入新冠相关新闻增强数据集；用NLP技术将新闻文章转换为上下文图结构，使用MDL - GBAD算法进行图挖掘。

Result: 文中未明确提及具体结果。

Conclusion: 文中未明确提及具体结论，但表明图方法能发现传统方法可能忽略的复杂模式。

Abstract: In today\'s digital world, fake news is spreading with immense speed. Its a
significant concern to address. In this work, we addressed that challenge using
novel graph based approach. We took dataset from Kaggle that contains real and
fake news articles. To test our approach we incorporated recent covid-19
related news articles that contains both genuine and fake news that are
relevant to this problem. This further enhances the dataset as well instead of
relying completely on the original dataset. We propose a contextual graph-based
approach to detect fake news articles. We need to convert news articles into
appropriate schema, so we leverage Natural Language Processing (NLP) techniques
to transform news articles into contextual graph structures. We then apply the
Minimum Description Length (MDL)-based Graph-Based Anomaly Detection (GBAD)
algorithm for graph mining. Graph-based methods are particularly effective for
handling rich contextual data, as they enable the discovery of complex patterns
that traditional query-based or statistical techniques might overlook. Our
proposed approach identifies normative patterns within the dataset and
subsequently uncovers anomalous patterns that deviate from these established
norms.

</details>


### [162] [Reading Between the Lines: Combining Pause Dynamics and Semantic Coherence for Automated Assessment of Thought Disorder](https://arxiv.org/abs/2507.13551)
*Feng Chen,Weizhe Xu,Changye Li,Serguei Pakhomov,Alex Cohen,Simran Bhola,Sandy Yin,Sunny X Tang,Michael Mackinley,Lena Palaniyappan,Dror Ben-Zeev,Trevor Cohen*

Main category: cs.CL

TL;DR: 研究整合停顿特征与语义连贯指标评估FTD严重程度，发现停顿特征可预测FTD，整合后预测性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统临床评估FTD的量表资源消耗大、缺乏扩展性，需评估整合ASR特征评估FTD严重程度的效用。

Method: 在三个数据集上整合停顿特征与语义连贯指标，用支持向量回归预测临床FTD分数。

Result: 停顿特征可稳健预测FTD严重程度，整合后预测性能优于仅用语义的模型，性能提升在各情境中一致。

Conclusion: 结合时间和语义分析的框架可改进言语紊乱评估，推动精神病自动言语分析。

Abstract: Formal thought disorder (FTD), a hallmark of schizophrenia spectrum
disorders, manifests as incoherent speech and poses challenges for clinical
assessment. Traditional clinical rating scales, though validated, are
resource-intensive and lack scalability. Automated speech analysis with
automatic speech recognition (ASR) allows for objective quantification of
linguistic and temporal features of speech, offering scalable alternatives. The
use of utterance timestamps in ASR captures pause dynamics, which are thought
to reflect the cognitive processes underlying speech production. However, the
utility of integrating these ASR-derived features for assessing FTD severity
requires further evaluation. This study integrates pause features with semantic
coherence metrics across three datasets: naturalistic self-recorded diaries
(AVH, n = 140), structured picture descriptions (TOPSY, n = 72), and dream
narratives (PsyCL, n = 43). We evaluated pause related features alongside
established coherence measures, using support vector regression (SVR) to
predict clinical FTD scores. Key findings demonstrate that pause features alone
robustly predict the severity of FTD. Integrating pause features with semantic
coherence metrics enhanced predictive performance compared to semantic-only
models, with integration of independent models achieving correlations up to
\r{ho} = 0.649 and AUC = 83.71% for severe cases detection (TOPSY, with best
\r{ho} = 0.584 and AUC = 79.23% for semantic-only models). The performance
gains from semantic and pause features integration held consistently across all
contexts, though the nature of pause patterns was dataset-dependent. These
findings suggest that frameworks combining temporal and semantic analyses
provide a roadmap for refining the assessment of disorganized speech and
advance automated speech analysis in psychosis.

</details>


### [163] [PARAM-1 BharatGen 2.9B Model](https://arxiv.org/abs/2507.13390)
*Kundeshwar Pundalik,Piyush Sawarkar,Nihar Sahoo,Abhishek Shinde,Prateek Chanda,Vedant Goswami,Ajay Nagpal,Atul Singh,Viraj Thakur,Vijay Dewane,Aamod Thakur,Bhargav Patel,Smita Gautam,Bhagwan Panditi,Shyam Pawar,Madhav Kotcha,Suraj Racha,Saral Sureka,Pankaj Singh,Rishi Bal,Rohit Saluja,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: 现有大语言模型以英语为中心，忽视印度语言多样性，本文推出 PARAM - 1 模型，训练有特定原则，结果显示它是通用且适用于印度的模型。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型开发以英语为中心，导致印度等语言多样地区结构代表性不足，需要关注印度语言多样性。

Method: 从头训练 29 亿参数的仅解码器、仅文本语言模型 PARAM - 1，使用印地语和英语双语数据集，遵循公平表示、分词公平和文化对齐评估三个核心原则。

Result: PARAM - 1 是称职的通用模型，也是以印度为中心应用的强大基线。

Conclusion: PARAM - 1 在预训练阶段嵌入多样性，为公平的基础模型提供了设计优先的蓝图。

Abstract: Large Language Models (LLMs) have emerged as powerful general-purpose
reasoning systems, yet their development remains dominated by English-centric
data, architectures, and optimization paradigms. This exclusionary design
results in structural under-representation of linguistically diverse regions
such as India, where over 20 official languages and 100+ dialects coexist
alongside phenomena like code-switching and diglossia. We introduce PARAM-1, a
2.9B parameter decoder-only, text-only language model trained from scratch with
an explicit architectural and linguistic focus on Indian diversity. PARAM-1 is
trained on a bilingual dataset consisting of only Hindi and English,
constructed with a strong focus on fact-rich, high-quality content. It is
guided by three core principles: equitable representation of Indic languages
through a 25% corpus allocation; tokenization fairness via a SentencePiece
tokenizer adapted to Indian morphological structures; and culturally aligned
evaluation benchmarks across IndicQA, code-mixed reasoning, and
socio-linguistic robustness tasks. By embedding diversity at the pretraining
level-rather than deferring it to post-hoc alignment-PARAM-1 offers a
design-first blueprint for equitable foundation modeling. Our results
demonstrate that it serves as both a competent general-purpose model and a
robust baseline for India-centric applications.

</details>


### [164] [Linguistic and Embedding-Based Profiling of Texts generated by Humans and Large Language Models](https://arxiv.org/abs/2507.13614)
*Sergio E. Zanotto,Segun Aroyehun*

Main category: cs.CL

TL;DR: 研究通过语言特征刻画大语言模型生成文本与人类撰写文本，发现人类文本句法简单、语义更多样，两类文本在不同领域有风格差异，新模型生成文本有同质化趋势。


<details>
  <summary>Details</summary>
Motivation: 以往研究多聚焦于分类，本文旨在用多层面语言特征刻画大语言模型生成文本与人类撰写文本。

Method: 选取跨8个领域、由11个不同大语言模型生成的数据集，计算不同语言特征，结合不同采样策略等进行分析，还计算特征在模型和领域间的变异性，应用风格嵌入测试文本变异性。

Result: 人类文本句法结构更简单、语义内容更多样；两类文本在不同领域都有风格多样性，人类文本特征变化更大；新模型输出文本变异性相似，有同质化趋势。

Conclusion: 通过语言特征可有效刻画大语言模型生成文本与人类撰写文本，新模型生成文本有同质化现象。

Abstract: The rapid advancements in large language models (LLMs) have significantly
improved their ability to generate natural language, making texts generated by
LLMs increasingly indistinguishable from human-written texts. While recent
research has primarily focused on using LLMs to classify text as either
human-written and machine-generated texts, our study focus on characterizing
these texts using a set of linguistic features across different linguistic
levels such as morphology, syntax, and semantics. We select a dataset of
human-written and machine-generated texts spanning 8 domains and produced by 11
different LLMs. We calculate different linguistic features such as dependency
length and emotionality and we use them for characterizing human-written and
machine-generated texts along with different sampling strategies, repetition
controls and model release date. Our statistical analysis reveals that
human-written texts tend to exhibit simpler syntactic structures and more
diverse semantic content. Furthermore, we calculate the variability of our set
of features across models and domains. Both human and machine texts show
stylistic diversity across domains, with humans displaying greater variation in
our features. Finally, we apply style embeddings to further test variability
among human-written and machine-generated texts. Notably, newer models output
text that is similarly variable, pointing to an homogenization of
machine-generated texts.

</details>


### [165] [Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters](https://arxiv.org/abs/2507.13618)
*Shanbo Cheng,Yu Bao,Qian Cao,Luyang Huang,Liyan Kang,Zhicheng Liu,Yu Lu,Wenhao Zhu,Zhichao Huang,Tao Li,Sitong Liu,Ningxin Peng,Shuaijie She,Lu Xu,Nuo Xu,Sen Yang,Runsheng Yu,Yiming Yu,Liehao Zou,Hang Li,Lu Lu,Yuxuan Wang,Yonghui Wu*

Main category: cs.CL

TL;DR: 本文介绍了开源大语言模型家族Seed - X，其7B参数模型在多语言翻译上表现出色，性能可比肩闭源模型，还分享优化经验并公开参数。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在多语言翻译中处理复杂语言模式和生硬翻译的难题。

Method: 基础模型在28种语言的单双语数据集上预训练，指令模型通过思维链推理微调翻译，并通过强化学习增强泛化能力。

Result: Seed - X在28种语言上性能可比肩Gemini - 2.5和GPT - 4o等闭源模型，在自动指标和人工评估中显著优于更大的开源模型。

Conclusion: 分享优化过程最佳实践，公开参数以推动翻译研究和应用。

Abstract: Multilingual translation stands as a challenging task for large language
models (LLMs) to handle intricate language patterns and stilted translations
that arise in automated translations. In this paper, we introduce Seed-X, a
family of open-source LLMs comprising instruct and reasoning models, pushing
the limits of translation capability with 7B parameter size. The base model is
pre-trained on a diverse, high-quality dataset encompassing both monolingual
and bilingual content across 28 languages, harnessing the full potential of
multilingual data. The instruct model is then finetuned to translate by
Chain-of-Thought (CoT) reasoning and further enhanced through reinforcement
learning (RL) to achieve better generalization across diverse language pairs.
Seed-X achieves performance comparable to leading closed-source models,
including Gemini-2.5 and GPT-4o, across 28 languages, and significantly
outperforms larger open-source models in both automatic metrics and human
evaluations. We share the best practices through our optimization process, and
make the parameter public available for advancing translation research and
applications.

</details>


### [166] [LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for Multi-Turn Dialogues](https://arxiv.org/abs/2507.13681)
*Haoyang Li,Zhanchao Xu,Yiming Li,Xuejia Chen,Darian Li,Anxin Tian,Qingfa Xiao,Cheng Deng,Jun Wang,Qing Li,Lei Chen,Mingxuan Yuan*

Main category: cs.CL

TL;DR: 提出LoopServe框架加速大语言模型多轮对话推理，有新基准测试，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在多轮对话中因对话历史变长面临计算和内存挑战，当前加速方法适应性差。

Method: 提出LoopServe框架，包括预填充阶段在线稀疏化和译码阶段渐进式键值压缩，还提出新基准测试。

Result: LoopServe在多轮对话任务中比现有基线方法效果好，显著加速推理。

Conclusion: LoopServe能有效解决大语言模型多轮对话推理的加速问题。

Abstract: Multi-turn dialogues are essential in many real-world applications of large
language models, such as chatbots and virtual assistants. As conversation
histories become longer, existing large language models face increasing
computational and memory challenges, which hinder their ability to provide
efficient and responsive interactions. Most current acceleration methods either
compress the context or optimize key value caching, but they often rely on
fixed or position-based heuristics that do not adapt well to the dynamic and
unpredictable patterns found in actual multi-turn conversations. In this paper,
we present LoopServe, an adaptive dual-phase inference acceleration framework
for large language models in multi-turn dialogues. LoopServe introduces two
main innovations. First, it performs online sparsification during the
prefilling phase by dynamically selecting the most important parts of the
attention matrix for each new input. Second, it uses progressive key value
compression during decoding by adaptively maintaining a relevant and efficient
cache based on the most recently generated output tokens. We also propose a
\href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_LLMs}{new
benchmark} with eleven multi-turn datasets that reflect realistic query
positions and conversational dependencies. Extensive experiments demonstrate
that LoopServe consistently achieves superior effectiveness compared to
existing baselines and significantly accelerates LLM inference across a wide
range of long-context dialogue tasks.

</details>


### [167] [The Judge Variable: Challenging Judge-Agnostic Legal Judgment Prediction](https://arxiv.org/abs/2507.13732)
*Guillaume Zambrano*

Main category: cs.CL

TL;DR: 本文用机器学习预测法国上诉法院儿童监护权判决结果，比较专业和通用模型，发现专业模型准确率高，支持法律现实主义。


<details>
  <summary>Details</summary>
Motivation: 基于法律现实主义 - 形式主义辩论，检验法官个人决策模式是否显著影响案件结果，挑战法官中立统一适用法律的假设。

Method: 实施严格假名化处理，分析18937项居住安排裁决，比较基于法官过往裁决训练的专业模型和基于汇总数据训练的通用模型，采用结合大语言模型和机器学习模型的混合预测方法。

Result: 专业模型预测准确率始终高于通用模型，顶级模型F1分数达92.85%，而通用模型为82.63%，专业模型能捕捉难以转移的个人模式。

Conclusion: 域内和跨域有效性测试为法律现实主义提供实证支持，表明法官身份对法律结果有可衡量的影响。

Abstract: This study examines the role of human judges in legal decision-making by
using machine learning to predict child physical custody outcomes in French
appellate courts. Building on the legal realism-formalism debate, we test
whether individual judges' decision-making patterns significantly influence
case outcomes, challenging the assumption that judges are neutral variables
that apply the law uniformly. To ensure compliance with French privacy laws, we
implement a strict pseudonymization process. Our analysis uses 18,937 living
arrangements rulings extracted from 10,306 cases. We compare models trained on
individual judges' past rulings (specialist models) with a judge-agnostic model
trained on aggregated data (generalist models). The prediction pipeline is a
hybrid approach combining large language models (LLMs) for structured feature
extraction and ML models for outcome prediction (RF, XGB and SVC). Our results
show that specialist models consistently achieve higher predictive accuracy
than the general model, with top-performing models reaching F1 scores as high
as 92.85%, compared to the generalist model's 82.63% trained on 20x to 100x
more samples. Specialist models capture stable individual patterns that are not
transferable to other judges. In-Domain and Cross-Domain validity tests provide
empirical support for legal realism, demonstrating that judicial identity plays
a measurable role in legal outcomes. All data and code used will be made
available.

</details>


### [168] [Using LLMs to identify features of personal and professional skills in an open-response situational judgment test](https://arxiv.org/abs/2507.13881)
*Cole Walsh,Rodica Ivan,Muhammad Zafar Iqbal,Colleen Robb*

Main category: cs.CL

TL;DR: 学术项目重视个人与专业技能，需可扩展系统评估这些技能。本文探索用大语言模型从情境判断测试（SJT）回答中提取相关特征，为自动评分奠定基础。


<details>
  <summary>Details</summary>
Motivation: 学术项目对个人和专业技能重视，需要可扩展系统评估，传统开放式SJT依赖人工评分有操作挑战，以往NLP评分系统有构念效度问题。

Method: 使用大语言模型从SJT回答中提取构念相关特征，并以Casper SJT验证。

Result: 文中未明确提及具体结果。

Conclusion: 为个人和专业技能的自动评分未来发展奠定基础。

Abstract: Academic programs are increasingly recognizing the importance of personal and
professional skills and their critical role alongside technical expertise in
preparing students for future success in diverse career paths. With this
growing demand comes the need for scalable systems to measure, evaluate, and
develop these skills. Situational Judgment Tests (SJTs) offer one potential
avenue for measuring these skills in a standardized and reliable way, but
open-response SJTs have traditionally relied on trained human raters for
evaluation, presenting operational challenges to delivering SJTs at scale. Past
attempts at developing NLP-based scoring systems for SJTs have fallen short due
to issues with construct validity of these systems. In this article, we explore
a novel approach to extracting construct-relevant features from SJT responses
using large language models (LLMs). We use the Casper SJT to demonstrate the
efficacy of this approach. This study sets the foundation for future
developments in automated scoring for personal and professional skills.

</details>


### [169] [Political Leaning and Politicalness Classification of Texts](https://arxiv.org/abs/2507.13913)
*Matous Volf,Jakub Simko*

Main category: cs.CL

TL;DR: 本文利用transformer模型解决文本政治倾向和政治性自动分类挑战，整合数据集并评估模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法在分布外文本上表现不佳，需解决此局限。

Method: 整合12个政治倾向分类数据集，扩展18个现有数据集创建政治性新数据集，用留一法进行基准测试，评估现有模型并训练新模型。

Result: 未提及具体结果。

Conclusion: 未提及明确结论。

Abstract: This paper addresses the challenge of automatically classifying text
according to political leaning and politicalness using transformer models. We
compose a comprehensive overview of existing datasets and models for these
tasks, finding that current approaches create siloed solutions that perform
poorly on out-of-distribution texts. To address this limitation, we compile a
diverse dataset by combining 12 datasets for political leaning classification
and creating a new dataset for politicalness by extending 18 existing datasets
with the appropriate label. Through extensive benchmarking with leave-one-in
and leave-one-out methodologies, we evaluate the performance of existing models
and train new ones with enhanced generalization capabilities.

</details>


### [170] [The Levers of Political Persuasion with Conversational AI](https://arxiv.org/abs/2507.13919)
*Kobi Hackenburg,Ben M. Tappin,Luke Hewitt,Ed Saunders,Sid Black,Hause Lin,Catherine Fist,Helen Margetts,David G. Rand,Christopher Summerfield*

Main category: cs.CL

TL;DR: 通过三个大规模实验评估19个大语言模型在政治议题上的说服力和事实准确性，发现当前及近期AI说服力更多源于后训练和提示方法，且这些方法在增强说服力时会降低事实准确性。


<details>
  <summary>Details</summary>
Motivation: 评估对话式AI对人类信念的影响，回应人们对其影响力的担忧。

Method: 开展三个大规模实验（N=76,977），部署19个大语言模型评估其在707个政治议题上的说服力，并检查466,769条模型声明的事实准确性。

Result: AI的说服力更多源于后训练和提示方法，后训练和提示方法分别使说服力提升51%和27%；这些方法通过利用大语言模型快速获取和策略性部署信息的能力增强说服力；增强说服力时会系统地降低事实准确性。

Conclusion: 当前及近期AI的说服力主要受后训练和提示方法影响，且说服力增强与事实准确性降低相关。

Abstract: There are widespread fears that conversational AI could soon exert
unprecedented influence over human beliefs. Here, in three large-scale
experiments (N=76,977), we deployed 19 LLMs-including some post-trained
explicitly for persuasion-to evaluate their persuasiveness on 707 political
issues. We then checked the factual accuracy of 466,769 resulting LLM claims.
Contrary to popular concerns, we show that the persuasive power of current and
near-future AI is likely to stem more from post-training and prompting
methods-which boosted persuasiveness by as much as 51% and 27%
respectively-than from personalization or increasing model scale. We further
show that these methods increased persuasion by exploiting LLMs' unique ability
to rapidly access and strategically deploy information and that, strikingly,
where they increased AI persuasiveness they also systematically decreased
factual accuracy.

</details>


### [171] [Efficient Temporal Tokenization for Mobility Prediction with Large Language Models](https://arxiv.org/abs/2507.14017)
*Haoyu He,Haozheng Luo,Yan Chen,Qi R. Wang*

Main category: cs.CL

TL;DR: 介绍RHYTHM框架，利用大语言模型进行时空预测和轨迹推理，在多个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型提升人类移动性的时空预测和轨迹推理能力，同时提高计算效率。

Method: 将轨迹按天分割，用分层注意力编码为离散令牌，通过冻结的大语言模型用预计算的提示嵌入丰富令牌表示。

Result: 在三个真实数据集上，准确率提升2.4%，周末提升5.0%，训练时间减少24.6%。

Conclusion: RHYTHM框架在人类移动性预测方面有效且计算效率高。

Abstract: We introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for
Human Mobility), a framework that leverages large language models (LLMs) as
spatio-temporal predictors and trajectory reasoners. RHYTHM partitions
trajectories into daily segments encoded as discrete tokens with hierarchical
attention, capturing both daily and weekly dependencies while substantially
reducing the sequence length. Token representations are enriched with
pre-computed prompt embeddings via a frozen LLM, enhancing the model's ability
to capture interdependencies without extensive computational overhead. By
freezing the LLM backbone, RHYTHM achieves significant computational
efficiency. Evaluation on three real-world datasets demonstrates a 2.4%
improvement in accuracy, 5.0% increase on weekends, and 24.6% reduction in
training time compared to state-of-the-art methods.

</details>


### [172] [Exploiting Primacy Effect To Improve Large Language Models](https://arxiv.org/abs/2507.13949)
*Bianca Raimondi,Maurizio Gabbrielli*

Main category: cs.CL

TL;DR: 研究微调大语言模型中的首因偏差，通过重排选项提升多选问答性能，强调偏差的双重性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在位置偏差影响答案准确性，研究微调模型中的首因偏差。

Method: 基于与查询的语义相似度重新排列响应选项，无需知道正确答案。

Result: 该方法显著提高了多选问答的性能。

Conclusion: 偏差具有双重性，为偏差感知的模型设计和NLP应用提供见解。

Abstract: Large Language Models (LLMs) have become essential in many Natural Language
Processing (NLP) tasks, leveraging extensive pre-training and fine-tuning to
achieve high accuracy. However, like humans, LLMs exhibit biases, particularly
positional biases such as primacy and recency effects, which can influence the
accuracy of the answers. The primacy effect-where items presented first are
more likely to be remembered or selected-plays a key role in Multiple Choice
Question Answering (MCQA), where the order of answer options can affect
prediction outcomes. This study focuses on primacy bias in fine-tuned LLMs: We
first show that fine-tuning amplifies this bias, probably due to exposure to
human-like patterns. Hence, we strategically leverage this effect by reordering
response options based on semantic similarity to the query, without requiring
knowledge of the correct answer. Our experimental results show that this
approach significantly improves performance in MCQA. More generally, our
findings underscore the dual nature of biases as both challenges and
opportunities, offering insights for bias-aware model design and NLP
applications.

</details>


### [173] [CPC-CMS: Cognitive Pairwise Comparison Classification Model Selection Framework for Document-level Sentiment Analysis](https://arxiv.org/abs/2507.14022)
*Jianfei Li,Kevin Kam Fung Yuen*

Main category: cs.CL

TL;DR: 提出CPC - CMS框架用于文档级情感分析，选多种模型作基线，用加权决策矩阵选最佳模型，通过社交媒体数据集验证，结果显示不含时间因素ALBERT最佳，含时间无单一最优模型，该框架可用于其他分类应用。


<details>
  <summary>Details</summary>
Motivation: 为文档级情感分析选择最佳分类模型。

Method: 提出CPC - CMS框架，基于专家知识判断的CPC计算评估标准权重，形成加权决策矩阵选择最佳模型，选用多种分类基线模型，用三个社交媒体开放数据集验证。

Result: 不含时间因素时，ALBERT对三个数据集表现最佳；含时间因素时，无单一模型始终优于其他模型。

Conclusion: CPC - CMS框架可用于不同领域的其他分类应用。

Abstract: This study proposes the Cognitive Pairwise Comparison Classification Model
Selection (CPC-CMS) framework for document-level sentiment analysis. The CPC,
based on expert knowledge judgment, is used to calculate the weights of
evaluation criteria, including accuracy, precision, recall, F1-score,
specificity, Matthews Correlation Coefficient (MCC), Cohen's Kappa (Kappa), and
efficiency. Naive Bayes, Linear Support Vector Classification (LSVC), Random
Forest, Logistic Regression, Extreme Gradient Boosting (XGBoost), Long
Short-Term Memory (LSTM), and A Lite Bidirectional Encoder Representations from
Transformers (ALBERT) are chosen as classification baseline models. A weighted
decision matrix consisting of classification evaluation scores with respect to
criteria weights, is formed to select the best classification model for a
classification problem. Three open datasets of social media are used to
demonstrate the feasibility of the proposed CPC-CMS. Based on our simulation,
for evaluation results excluding the time factor, ALBERT is the best for the
three datasets; if time consumption is included, no single model always
performs better than the other models. The CPC-CMS can be applied to the other
classification applications in different areas.

</details>


### [174] [Bottom-up Domain-specific Superintelligence: A Reliable Knowledge Graph is What We Need](https://arxiv.org/abs/2507.13966)
*Bhishma Dedhia,Yuval Kansal,Niraj K. Jha*

Main category: cs.CL

TL;DR: 提出从知识图谱原语合成任务的管道，微调语言模型实现特定领域超智能，以医学为例验证，模型表现出色，展望特定领域超智能代理交互产生AGI的未来。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型自上而下训练方法不足以获取特定领域深层专业知识，需自下而上方法。

Method: 提出从知识图谱原语合成任务的管道，在生成的课程上微调语言模型，引入评估套件ICD - Bench。

Result: QwQ - Med - 3在ICD - Bench上显著优于现有模型，在医学问答基准测试中提升基础模型性能。

Conclusion: 特定领域超智能代理的可组合交互有望产生通用人工智能。

Abstract: Language models traditionally used for cross-domain generalization have
recently demonstrated task-specific reasoning. However, their top-down training
approach on general corpora is insufficient for acquiring abstractions needed
for deep domain expertise. This may require a bottom-up approach that acquires
expertise by learning to compose simple domain concepts into more complex ones.
A knowledge graph (KG) provides this compositional structure, where domain
primitives are represented as head-relation-tail edges and their paths encode
higher-level concepts. We present a task generation pipeline that synthesizes
tasks directly from KG primitives, enabling models to acquire and compose them
for reasoning. We fine-tune language models on the resultant KG-grounded
curriculum to demonstrate domain-specific superintelligence. While broadly
applicable, we validate our approach in medicine, where reliable KGs exist.
Using a medical KG, we curate 24,000 reasoning tasks paired with thinking
traces derived from diverse medical primitives. We fine-tune the QwQ-32B model
on this curriculum to obtain QwQ-Med-3 that takes a step towards medical
superintelligence. We also introduce ICD-Bench, an evaluation suite to quantify
reasoning abilities across 15 medical domains. Our experiments demonstrate that
QwQ-Med-3 significantly outperforms state-of-the-art reasoning models on
ICD-Bench categories. Further analysis reveals that QwQ-Med-3 utilizes acquired
primitives to widen the performance gap on the hardest tasks of ICD-Bench.
Finally, evaluation on medical question-answer benchmarks shows that QwQ-Med-3
transfers acquired expertise to enhance the base model's performance. While the
industry's approach to artificial general intelligence (AGI) emphasizes broad
expertise, we envision a future in which AGI emerges from the composable
interaction of efficient domain-specific superintelligent agents.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [175] [State Space Models Naturally Produce Traveling Waves, Time Cells, and Scale to Abstract Cognitive Functions](https://arxiv.org/abs/2507.13638)
*Sen Lu,Xiaoyu Zhang,Mingtao Hu,Eric Yeu-Jer Lee,Soohyeon Kim,Wei D. Lu*

Main category: q-bio.NC

TL;DR: 提出基于状态空间模型（SSMs）的框架弥合微观神经回路与认知功能理解的差距，通过训练S5模型验证，发现模型能自发形成类似生物“时间细胞”的神经表征，统一多种实验现象，该框架可推广到抽象任务，为大脑时间学习提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 现代神经科学面临弥合微观神经回路详细图谱与认知功能机制理解之间差距的挑战，现有知识在神经元元素如何组合产生灵活学习行为方面存在不足。

Method: 提出基于SSMs的框架，训练S5模型（一种采用对角状态转移矩阵的SSM变体）进行时间辨别任务的强化学习。

Result: 模型自发形成类似生物“时间细胞”的神经表征，这些细胞源于隐藏状态向量在复平面的旋转动力学，该机制统一多种实验现象，且旋转动力学可推广到抽象事件计数任务。

Conclusion: SSMs是连接单神经元动力学与认知现象的有力框架，为大脑时间学习提供统一且计算可行的理论基础。

Abstract: A grand challenge in modern neuroscience is to bridge the gap between the
detailed mapping of microscale neural circuits and a mechanistic understanding
of cognitive functions. While extensive knowledge exists about neuronal
connectivity and biophysics, a significant gap remains in how these elements
combine to produce flexible, learned behaviors. Here, we propose that a
framework based on State-Space Models (SSMs), an emerging class of deep
learning architectures, can bridge this gap. We argue that the differential
equations governing elements in an SSM are conceptually consistent with the
biophysical dynamics of neurons, while the combined dynamics in the model lead
to emergent behaviors observed in experimental neuroscience. We test this
framework by training an S5 model--a specific SSM variant employing a diagonal
state transition matrix--on temporal discrimination tasks with reinforcement
learning (RL). We demonstrate that the model spontaneously develops neural
representations that strikingly mimic biological 'time cells'. We reveal that
these cells emerge from a simple generative principle: learned rotational
dynamics of hidden state vectors in the complex plane. This single mechanism
unifies the emergence of time cells, ramping activity, and
oscillations/traveling waves observed in numerous experiments. Furthermore, we
show that this rotational dynamics generalizes beyond interval discriminative
tasks to abstract event-counting tasks that were considered foundational for
performing complex cognitive tasks. Our findings position SSMs as a compelling
framework that connects single-neuron dynamics to cognitive phenomena, offering
a unifying and computationally tractable theoretical ground for temporal
learning in the brain.

</details>


### [176] [Convergent transformations of visual representation in brains and models](https://arxiv.org/abs/2507.13941)
*Pablo Marcos-Manchón,Lluís Fuentemilla*

Main category: q-bio.NC

TL;DR: 研究测试刺激驱动的收敛是否在人类和深度神经网络中遵循共同轨迹，揭示大脑皮层网络组织，表明人类和人工视觉在视觉编码上有收敛计算解决方案。


<details>
  <summary>Details</summary>
Motivation: 探究塑造视觉感知的因素，验证刺激驱动的收敛在人类和深度神经网络中是否遵循共同轨迹。

Method: 引入统一框架，结合主体间相似性和与模型层次的对齐来追踪表征流，并应用于三个独立的fMRI数据集。

Result: 揭示大脑中有跨个体保守的皮层网络，分为两条通路，视觉DNN层次结构能捕捉此功能组织，语言模型不能。

Conclusion: 人类和人工视觉在视觉编码上有由外部世界结构驱动的收敛计算解决方案。

Abstract: A fundamental question in cognitive neuroscience is what shapes visual
perception: the external world's structure or the brain's internal
architecture. Although some perceptual variability can be traced to individual
differences, brain responses to naturalistic stimuli evoke similar activity
patterns across individuals, suggesting a convergent representational
principle. Here, we test if this stimulus-driven convergence follows a common
trajectory across people and deep neural networks (DNNs) during its
transformation from sensory to high-level internal representations. We
introduce a unified framework that traces representational flow by combining
inter-subject similarity with alignment to model hierarchies. Applying this
framework to three independent fMRI datasets of visual scene perception, we
reveal a cortex-wide network, conserved across individuals, organized into two
pathways: a medial-ventral stream for scene structure and a lateral-dorsal
stream tuned for social and biological content. This functional organization is
captured by the hierarchies of vision DNNs but not language models, reinforcing
the specificity of the visual-to-semantic transformation. These findings show a
convergent computational solution for visual encoding in both human and
artificial vision, driven by the structure of the external world.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [177] [Treedepth Inapproximability and Exponential ETH Lower Bound](https://arxiv.org/abs/2507.13818)
*Édouard Bonnet,Daniel Neuen,Marek Sokołowski*

Main category: cs.CC

TL;DR: 本文围绕树深度问题展开，指出当前算法情况，证明 1.0003 - 近似计算树深度是 NP 难问题，精确计算需 $2^{\Omega(n)}$ 时间，还推导了 $(1 + \delta)$ - 近似算法的时间复杂度。


<details>
  <summary>Details</summary>
Motivation: 当前树深度计算和近似算法有一定成果，但精确计算复杂度为 NP 完全，且未排除多项式时间近似方案（PTAS），需进一步研究其计算难度。

Method: 通过从可满足性问题到树深度问题的简单直接归约，该归约受最近为树宽问题设计的归约启发。

Result: 证明 1.0003 - 近似计算树深度是 NP 难问题；精确计算 $n$ 顶点图的树深度需 $2^{\Omega(n)}$ 时间，除非指数时间假设（ETH）不成立；存在绝对常数 $\delta, c > 0$，使得任何 $(1 + \delta)$ - 近似算法需要 $2^{\Omega(n / \log^c n)}$ 时间。

Conclusion: 树深度的近似和精确计算有较高难度，特定近似计算是 NP 难问题，精确计算需指数时间，为树深度问题研究提供了新的复杂度结果。

Abstract: Treedepth is a central parameter to algorithmic graph theory. The current
state-of-the-art in computing and approximating treedepth consists of a
$2^{O(k^2)} n$-time exact algorithm and a polynomial-time $O(\text{OPT}
\log^{3/2} \text{OPT})$-approximation algorithm, where the former algorithm
returns an elimination forest of height $k$ (witnessing that treedepth is at
most $k$) for the $n$-vertex input graph $G$, or correctly reports that $G$ has
treedepth larger than $k$, and $\text{OPT}$ is the actual value of the
treedepth. On the complexity side, exactly computing treedepth is NP-complete,
but the known reductions do not rule out a polynomial-time approximation scheme
(PTAS), and under the Exponential Time Hypothesis (ETH) only exclude a running
time of $2^{o(\sqrt n)}$ for exact algorithms.
  We show that 1.0003-approximating treedepth is NP-hard, and that exactly
computing the treedepth of an $n$-vertex graph requires time $2^{\Omega(n)}$,
unless the ETH fails. We further derive that there exist absolute constants
$\delta, c > 0$ such that any $(1+\delta)$-approximation algorithm requires
time $2^{\Omega(n / \log^c n)}$. We do so via a simple direct reduction from
Satisfiability to Treedepth, inspired by a reduction recently designed for
Treewidth [STOC '25].

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [178] [Smart Routing for Multimodal Video Retrieval: When to Search What](https://arxiv.org/abs/2507.13374)
*Kevin Dela Rosa*

Main category: cs.CV

TL;DR: 介绍基于大语言模型的智能路由系统ModaRoute用于多模态视频检索，能降低计算开销并保持一定检索效果。


<details>
  <summary>Details</summary>
Motivation: 现有密集文本字幕检索需昂贵离线处理且会遗漏关键视觉信息，需更优的多模态视频检索方案。

Method: 使用GPT - 4.1分析查询意图和预测信息需求，在ASR、OCR和视觉索引间路由查询。

Result: ModaRoute降低41%计算开销，达到60.9%的Recall@5，平均每个查询使用1.78种模态。

Conclusion: 智能路由为多模态检索系统扩展提供实用方案，降低基础设施成本并保持竞争力，适合实际部署。

Abstract: We introduce ModaRoute, an LLM-based intelligent routing system that
dynamically selects optimal modalities for multimodal video retrieval. While
dense text captions can achieve 75.9% Recall@5, they require expensive offline
processing and miss critical visual information present in 34% of clips with
scene text not captured by ASR. By analyzing query intent and predicting
information needs, ModaRoute reduces computational overhead by 41% while
achieving 60.9% Recall@5. Our approach uses GPT-4.1 to route queries across ASR
(speech), OCR (text), and visual indices, averaging 1.78 modalities per query
versus exhaustive 3.0 modality search. Evaluation on 1.8M video clips
demonstrates that intelligent routing provides a practical solution for scaling
multimodal retrieval systems, reducing infrastructure costs while maintaining
competitive effectiveness for real-world deployment.

</details>


### [179] [VLMs have Tunnel Vision: Evaluating Nonlocal Visual Reasoning in Leading VLMs](https://arxiv.org/abs/2507.13361)
*Shmuel Berman,Jia Deng*

Main category: cs.CV

TL;DR: 评估视觉语言模型非局部视觉推理能力，发现当前模型虽视觉敏锐度有提升但缺乏核心视觉推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在简单感知测试中表现不佳，需评估其非局部视觉推理能力。

Method: 提出评估方法，分离出比较感知、扫视搜索、平滑视觉搜索三种非局部视觉形式进行测试。

Result: 旗舰模型在测试中表现不佳，勉强超过随机准确率。

Conclusion: 当前模型虽视觉敏锐度有提升，但缺乏核心视觉推理能力。

Abstract: Visual Language Models (VLMs) excel at complex visual tasks such as VQA and
chart understanding, yet recent work suggests they struggle with simple
perceptual tests. We present an evaluation that tests vision-language models'
capacity for nonlocal visual reasoning -- reasoning that requires chaining
evidence collected from multiple, possibly distant, regions of an image. We
isolate three distinct forms of non-local vision: comparative perception, which
demands holding two images in working memory and comparing them; saccadic
search, which requires making discrete, evidence-driven jumps to locate
successive targets; and smooth visual search, which involves searching smoothly
along a continuous contour. Flagship models (e.g., Gemini 2.5 Pro, Claude
Vision 3.7, GPT-o4-mini), even those that perform well on prior
primitive-vision benchmarks, fail these tests and barely exceed random accuracy
on two variants of our tasks that are trivial for humans. Our structured
evaluation suite allows us to test if VLMs can perform similar visual
algorithms to humans. Our findings show that despite gains in raw visual
acuity, current models lack core visual reasoning capabilities.

</details>


### [180] [Enhancing Spatial Reasoning in Vision-Language Models via Chain-of-Thought Prompting and Reinforcement Learning](https://arxiv.org/abs/2507.13362)
*Binbin Ji,Siddharth Agrawal,Qiance Tang,Yvonne Wu*

Main category: cs.CV

TL;DR: 研究通过思维链提示和强化学习探究视觉语言模型空间推理能力，发现结构化多阶段提示和GRPO能提升性能。


<details>
  <summary>Details</summary>
Motivation: 探究如何提升视觉语言模型的空间推理能力和泛化性能。

Method: 评估不同提示策略，用Group Relative Policy Optimization (GRPO)在SAT数据集上微调模型并在CVBench评估。

Result: 简单CoT格式可能损害模型性能，SceneGraph CoT显著提升空间推理准确率；GRPO比监督微调(SFT)在Pass@1评估中准确率更高，在分布外条件下更稳健。

Conclusion: 强化学习和结构化提示能提升现代视觉语言模型的空间推理能力和泛化表现。

Abstract: This study investigates the spatial reasoning capabilities of vision-language
models (VLMs) through Chain-of-Thought (CoT) prompting and reinforcement
learning. We begin by evaluating the impact of different prompting strategies
and find that simple CoT formats, where the model generates a reasoning step
before the answer, not only fail to help, but can even harm the model's
original performance. In contrast, structured multi-stage prompting based on
scene graphs (SceneGraph CoT) significantly improves spatial reasoning
accuracy. Furthermore, to improve spatial reasoning ability, we fine-tune
models using Group Relative Policy Optimization (GRPO) on the SAT dataset and
evaluate their performance on CVBench. Compared to supervised fine-tuning
(SFT), GRPO achieves higher accuracy on Pass@1 evaluations and demonstrates
superior robustness under out-of-distribution (OOD) conditions. In particular,
we find that SFT overfits to surface-level linguistic patterns and may degrade
performance when test-time phrasing changes (e.g., from "closer to" to "farther
from"). GRPO, on the other hand, generalizes more reliably and maintains stable
performance under such shifts. Our findings provide insights into how
reinforcement learning and structured prompting improve the spatial reasoning
capabilities and generalization behavior of modern VLMs. All code is open
source at: https://github.com/Yvonne511/spatial-vlm-investigator

</details>


### [181] [Just Add Geometry: Gradient-Free Open-Vocabulary 3D Detection Without Human-in-the-Loop](https://arxiv.org/abs/2507.13363)
*Atharv Goel,Mehar Khurana*

Main category: cs.CV

TL;DR: 利用2D基础模型实现无3D标注的开放词汇3D目标检测，方法训练自由，实验表现好并开源代码。


<details>
  <summary>Details</summary>
Motivation: 现代3D目标检测数据集受限于狭窄分类和高成本标注，难以扩展到开放世界场景，而2D视觉语言模型有丰富语义理解能力。

Method: 使用2D视觉语言检测器生成文本条件建议，用SAM分割并通过相机几何和深度信息投影到3D，引入几何膨胀策略推断3D边界框，构建模拟恶劣条件的数据集。

Result: 方法在多种输入设置下有有竞争力的定位性能，无需训练且支持开放词汇。

Conclusion: 2D基础模型在可扩展3D感知方面有未挖掘的潜力。

Abstract: Modern 3D object detection datasets are constrained by narrow class
taxonomies and costly manual annotations, limiting their ability to scale to
open-world settings. In contrast, 2D vision-language models trained on
web-scale image-text pairs exhibit rich semantic understanding and support
open-vocabulary detection via natural language prompts. In this work, we
leverage the maturity and category diversity of 2D foundation models to perform
open-vocabulary 3D object detection without any human-annotated 3D labels.
  Our pipeline uses a 2D vision-language detector to generate text-conditioned
proposals, which are segmented with SAM and back-projected into 3D using camera
geometry and either LiDAR or monocular pseudo-depth. We introduce a geometric
inflation strategy based on DBSCAN clustering and Rotating Calipers to infer 3D
bounding boxes without training. To simulate adverse real-world conditions, we
construct Pseudo-nuScenes, a fog-augmented, RGB-only variant of the nuScenes
dataset.
  Experiments demonstrate that our method achieves competitive localization
performance across multiple settings, including LiDAR-based and purely RGB-D
inputs, all while remaining training-free and open-vocabulary. Our results
highlight the untapped potential of 2D foundation models for scalable 3D
perception. We open-source our code and resources at
https://github.com/atharv0goel/open-world-3D-det.

</details>


### [182] [OmniVec2 -- A Novel Transformer based Network for Large Scale Multimodal and Multitask Learning](https://arxiv.org/abs/2507.13364)
*Siddharth Srivastava,Gaurav Sharma*

Main category: cs.CV

TL;DR: 提出一种新颖的多模态多任务网络及训练算法，能处理12种不同模态数据，经25个数据集评估有SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 处理多模态多任务场景，将不同模态数据投影到统一嵌入空间。

Method: 使用模态专用分词器、共享变压器架构和交叉注意力机制，结合特定模态任务头；提出迭代模态切换的预训练策略和权衡全联合训练与成对训练的算法。

Result: 在来自12种模态的25个数据集上进行评估，展现了最先进的性能。

Conclusion: 所提出的架构、预训练策略和多任务训练方法是有效的。

Abstract: We present a novel multimodal multitask network and associated training
algorithm. The method is capable of ingesting data from approximately 12
different modalities namely image, video, audio, text, depth, point cloud, time
series, tabular, graph, X-ray, infrared, IMU, and hyperspectral. The proposed
approach utilizes modality specialized tokenizers, a shared transformer
architecture, and cross-attention mechanisms to project the data from different
modalities into a unified embedding space. It addresses multimodal and
multitask scenarios by incorporating modality-specific task heads for different
tasks in respective modalities. We propose a novel pretraining strategy with
iterative modality switching to initialize the network, and a training
algorithm which trades off fully joint training over all modalities, with
training on pairs of modalities at a time. We provide comprehensive evaluation
across 25 datasets from 12 modalities and show state of the art performances,
demonstrating the effectiveness of the proposed architecture, pretraining
strategy and adapted multitask training.

</details>


### [183] [Transformer-Based Framework for Motion Capture Denoising and Anomaly Detection in Medical Rehabilitation](https://arxiv.org/abs/2507.13371)
*Yeming Cai,Yang Wang,Zhenglin Li*

Main category: cs.CV

TL;DR: 提出结合光学动作捕捉与Transformer模型的深度学习框架用于医疗康复，处理数据问题并实时检测异常，评估显示性能优越。


<details>
  <summary>Details</summary>
Motivation: 解决医疗康复中因遮挡和环境因素导致的数据噪声和缺失问题，实时检测异常动作确保患者安全。

Method: 提出端到端深度学习框架，利用时间序列建模对动作捕捉数据去噪和补全。

Result: 在中风和骨科康复数据集评估中，数据重建和异常检测性能优越。

Conclusion: 该框架为远程康复提供可扩展、经济高效的解决方案，减少现场监督。

Abstract: This paper proposes an end-to-end deep learning framework integrating optical
motion capture with a Transformer-based model to enhance medical
rehabilitation. It tackles data noise and missing data caused by occlusion and
environmental factors, while detecting abnormal movements in real time to
ensure patient safety. Utilizing temporal sequence modeling, our framework
denoises and completes motion capture data, improving robustness. Evaluations
on stroke and orthopedic rehabilitation datasets show superior performance in
data reconstruction and anomaly detection, providing a scalable, cost-effective
solution for remote rehabilitation with reduced on-site supervision.

</details>


### [184] [Enhancing Breast Cancer Detection with Vision Transformers and Graph Neural Networks](https://arxiv.org/abs/2507.13372)
*Yeming Cai,Zhenglin Li,Yang Wang*

Main category: cs.CV

TL;DR: 本文提出结合ViT和GNN的框架用于乳腺癌检测，准确率达84.2%，还有可解释注意力热力图辅助放射科医生。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌是全球女性主要死因，早期检测对提高生存率至关重要。

Method: 引入结合Vision Transformers (ViT)和Graph Neural Networks (GNN)的创新框架，利用CBIS - DDSM数据集进行乳腺癌检测。

Result: 框架准确率达84.2%，优于传统方法，且有可解释注意力热力图。

Conclusion: 该结合ViT和GNN的框架能有效提升乳腺癌检测效果，可辅助放射科医生。

Abstract: Breast cancer is a leading cause of death among women globally, and early
detection is critical for improving survival rates. This paper introduces an
innovative framework that integrates Vision Transformers (ViT) and Graph Neural
Networks (GNN) to enhance breast cancer detection using the CBIS-DDSM dataset.
Our framework leverages ViT's ability to capture global image features and
GNN's strength in modeling structural relationships, achieving an accuracy of
84.2%, outperforming traditional methods. Additionally, interpretable attention
heatmaps provide insights into the model's decision-making process, aiding
radiologists in clinical settings.

</details>


### [185] [IConMark: Robust Interpretable Concept-Based Watermark For AI Images](https://arxiv.org/abs/2507.13407)
*Vinu Sankar Sadasivan,Mehrdad Saberi,Soheil Feizi*

Main category: cs.CV

TL;DR: 提出IConMark语义水印方法区分AI生成图像和真实图像，结合现有技术增强鲁棒性，实验表明效果优于基线。


<details>
  <summary>Details</summary>
Motivation: 生成式AI和合成媒体兴起，传统水印技术易受攻击，需有效方法区分AI生成图像和真实图像。

Method: 提出IConMark方法，嵌入可解释概念到AI生成图像中，还结合StegaStamp和TrustMark形成混合方法。

Result: IConMark及其变体在水印检测的AUROC分数上比最佳基线分别高10.8%、14.5%和15.9%。

Conclusion: IConMark方法有效，在检测准确性和图像质量保持上表现优越，且能与现有技术结合增强鲁棒性。

Abstract: With the rapid rise of generative AI and synthetic media, distinguishing
AI-generated images from real ones has become crucial in safeguarding against
misinformation and ensuring digital authenticity. Traditional watermarking
techniques have shown vulnerabilities to adversarial attacks, undermining their
effectiveness in the presence of attackers. We propose IConMark, a novel
in-generation robust semantic watermarking method that embeds interpretable
concepts into AI-generated images, as a first step toward interpretable
watermarking. Unlike traditional methods, which rely on adding noise or
perturbations to AI-generated images, IConMark incorporates meaningful semantic
attributes, making it interpretable to humans and hence, resilient to
adversarial manipulation. This method is not only robust against various image
augmentations but also human-readable, enabling manual verification of
watermarks. We demonstrate a detailed evaluation of IConMark's effectiveness,
demonstrating its superiority in terms of detection accuracy and maintaining
image quality. Moreover, IConMark can be combined with existing watermarking
techniques to further enhance and complement its robustness. We introduce
IConMark+SS and IConMark+TM, hybrid approaches combining IConMark with
StegaStamp and TrustMark, respectively, to further bolster robustness against
multiple types of image manipulations. Our base watermarking technique
(IConMark) and its variants (+TM and +SS) achieve 10.8%, 14.5%, and 15.9%
higher mean area under the receiver operating characteristic curve (AUROC)
scores for watermark detection, respectively, compared to the best baseline on
various datasets.

</details>


### [186] [A Deep Learning-Based Ensemble System for Automated Shoulder Fracture Detection in Clinical Radiographs](https://arxiv.org/abs/2507.13408)
*Hemanth Kumar M,Karthika M,Saianiruth M,Vasanthakumar Venugopal,Anandakumar D,Revathi Ezhumalai,Charulatha K,Kishore Kumar J,Dayana G,Kalyan Sivasailam,Bargava Subramanian*

Main category: cs.CV

TL;DR: 本文开发多模型深度学习系统检测肩部骨折，NMW集成模型表现出色，表明基于集成的AI可有效检测肩部骨折，适用于实时诊断工作流，但目前限于二元骨折检测。


<details>
  <summary>Details</summary>
Motivation: 肩部骨折常被漏诊，AI工具可辅助早期检测、减少诊断延迟，故开发专用AI系统。

Method: 使用10000张带注释的肩部X光片开发多模型深度学习系统，采用Faster R - CNN、EfficientDet、RF - DETR等架构，并应用边界框和分类级集成技术。

Result: NMW集成模型准确率达95.5%，F1分数为0.9610，在关键指标上优于单个模型，召回率和定位精度高。

Conclusion: 基于集成的AI能可靠检测肩部骨折，适用于实时诊断工作流，当前模型限于二元骨折检测，用于快速筛查和分诊支持。

Abstract: Background: Shoulder fractures are often underdiagnosed, especially in
emergency and high-volume clinical settings. Studies report up to 10% of such
fractures may be missed by radiologists. AI-driven tools offer a scalable way
to assist early detection and reduce diagnostic delays. We address this gap
through a dedicated AI system for shoulder radiographs. Methods: We developed a
multi-model deep learning system using 10,000 annotated shoulder X-rays.
Architectures include Faster R-CNN (ResNet50-FPN, ResNeXt), EfficientDet, and
RF-DETR. To enhance detection, we applied bounding box and classification-level
ensemble techniques such as Soft-NMS, WBF, and NMW fusion. Results: The NMW
ensemble achieved 95.5% accuracy and an F1-score of 0.9610, outperforming
individual models across all key metrics. It demonstrated strong recall and
localization precision, confirming its effectiveness for clinical fracture
detection in shoulder X-rays. Conclusion: The results show ensemble-based AI
can reliably detect shoulder fractures in radiographs with high clinical
relevance. The model's accuracy and deployment readiness position it well for
integration into real-time diagnostic workflows. The current model is limited
to binary fracture detection, reflecting its design for rapid screening and
triage support rather than detailed orthopedic classification.

</details>


### [187] [AI-ming backwards: Vanishing archaeological landscapes in Mesopotamia and automatic detection of sites on CORONA imagery](https://arxiv.org/abs/2507.13420)
*Alessandro Pistola,Valentina Orru',Nicolo' Marchetti,Marco Roccetti*

Main category: cs.CV

TL;DR: 利用CORONA卫星图像升级深度学习模型，用于自动识别考古遗址，取得高精度并发现新遗址，证实该方法有效性。


<details>
  <summary>Details</summary>
Motivation: 在过去五十年环境巨变、许多考古遗址被破坏的情况下，提升AI模型自动识别考古遗址的能力。

Method: 使用CORONA卫星图像对基于Bing的卷积网络模型进行再训练。

Result: 检测精度显著提高，IoU值超85%，检测考古遗址的总体准确率达90%，发现四个新的考古遗址。

Conclusion: 证实利用AI技术和20世纪60年代的CORONA图像发现当前不可见考古遗址的有效性，对研究受人类活动影响的考古景观有重要意义。

Abstract: By upgrading an existing deep learning model with the knowledge provided by
one of the oldest sets of grayscale satellite imagery, known as CORONA, we
improved the AI model attitude towards the automatic identification of
archaeological sites in an environment which has been completely transformed in
the last five decades, including the complete destruction of many of those same
sites. The initial Bing based convolutional network model was retrained using
CORONA satellite imagery for the district of Abu Ghraib, west of Baghdad,
central Mesopotamian floodplain. The results were twofold and surprising.
First, the detection precision obtained on the area of interest increased
sensibly: in particular, the Intersection over Union (IoU) values, at the image
segmentation level, surpassed 85 percent, while the general accuracy in
detecting archeological sites reached 90 percent. Second, our retrained model
allowed the identification of four new sites of archaeological interest
(confirmed through field verification), previously not identified by
archaeologists with traditional techniques. This has confirmed the efficacy of
using AI techniques and the CORONA imagery from the 1960 to discover
archaeological sites currently no longer visible, a concrete breakthrough with
significant consequences for the study of landscapes with vanishing
archaeological evidence induced by anthropization

</details>


### [188] [CaSTFormer: Causal Spatio-Temporal Transformer for Driving Intention Prediction](https://arxiv.org/abs/2507.13425)
*Sirui Wang,Zhou Guan,Bingxi Zhao,Tongjia Gu*

Main category: cs.CV

TL;DR: 提出CaSTFormer模型解决现有驾驶意图预测方法不足问题，在数据集上达SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法难以准确建模人类驾驶行为复杂时空依赖和不可预测变化，需提升驾驶意图预测准确性和人机共驾系统安全性与交互效率。

Method: 提出CaSTFormer模型，含RSF机制、CPE模块和FSN网络，分别用于特征流时间对齐、消除虚假关联和合成特征。

Result: 在Brain4Cars数据集上评估，CaSTFormer达SOTA性能。

Conclusion: CaSTFormer能有效捕捉复杂因果时空依赖，提升驾驶意图预测准确性和透明度。

Abstract: Accurate prediction of driving intention is key to enhancing the safety and
interactive efficiency of human-machine co-driving systems. It serves as a
cornerstone for achieving high-level autonomous driving. However, current
approaches remain inadequate for accurately modeling the complex
spatio-temporal interdependencies and the unpredictable variability of human
driving behavior. To address these challenges, we propose CaSTFormer, a Causal
Spatio-Temporal Transformer to explicitly model causal interactions between
driver behavior and environmental context for robust intention prediction.
Specifically, CaSTFormer introduces a novel Reciprocal Shift Fusion (RSF)
mechanism for precise temporal alignment of internal and external feature
streams, a Causal Pattern Extraction (CPE) module that systematically
eliminates spurious correlations to reveal authentic causal dependencies, and
an innovative Feature Synthesis Network (FSN) that adaptively synthesizes these
purified representations into coherent spatio-temporal inferences. We evaluate
the proposed CaSTFormer on the public Brain4Cars dataset, and it achieves
state-of-the-art performance. It effectively captures complex causal
spatio-temporal dependencies and enhances both the accuracy and transparency of
driving intention prediction.

</details>


### [189] ["PhyWorldBench": A Comprehensive Evaluation of Physical Realism in Text-to-Video Models](https://arxiv.org/abs/2507.13428)
*Jing Gu,Xian Liu,Yu Zeng,Ashwin Nagarajan,Fangrui Zhu,Daniel Hong,Yue Fan,Qianqi Yan,Kaiwen Zhou,Ming-Yu Liu,Xin Eric Wang*

Main category: cs.CV

TL;DR: 提出PhyWorldBench基准评估视频生成模型物理模拟能力，评估12个模型，找出挑战并给出提示建议。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型准确模拟物理现象能力存在挑战，需评估其遵循物理定律的能力。

Method: 创建PhyWorldBench基准，涵盖多层次物理现象和“反物理”类别；设计用MLLM零样本评估物理真实性的方法；用1050个提示评估12个模型。

Result: 找出模型在遵循真实世界物理方面面临的关键挑战。

Conclusion: 通过对不同提示类型的性能分析，给出增强遵循物理原则的提示编写建议。

Abstract: Video generation models have achieved remarkable progress in creating
high-quality, photorealistic content. However, their ability to accurately
simulate physical phenomena remains a critical and unresolved challenge. This
paper presents PhyWorldBench, a comprehensive benchmark designed to evaluate
video generation models based on their adherence to the laws of physics. The
benchmark covers multiple levels of physical phenomena, ranging from
fundamental principles like object motion and energy conservation to more
complex scenarios involving rigid body interactions and human or animal motion.
Additionally, we introduce a novel ""Anti-Physics"" category, where prompts
intentionally violate real-world physics, enabling the assessment of whether
models can follow such instructions while maintaining logical consistency.
Besides large-scale human evaluation, we also design a simple yet effective
method that could utilize current MLLM to evaluate the physics realism in a
zero-shot fashion. We evaluate 12 state-of-the-art text-to-video generation
models, including five open-source and five proprietary models, with a detailed
comparison and analysis. we identify pivotal challenges models face in adhering
to real-world physics. Through systematic testing of their outputs across 1,050
curated prompts-spanning fundamental, composite, and anti-physics scenarios-we
identify pivotal challenges these models face in adhering to real-world
physics. We then rigorously examine their performance on diverse physical
phenomena with varying prompt types, deriving targeted recommendations for
crafting prompts that enhance fidelity to physical principles.

</details>


### [190] [Using Multiple Input Modalities Can Improve Data-Efficiency and O.O.D. Generalization for ML with Satellite Imagery](https://arxiv.org/abs/2507.13385)
*Arjun Rao,Esther Rolf*

Main category: cs.CV

TL;DR: 研究在监督学习中结合其他地理数据层和光学图像对卫星机器学习（SatML）模型性能的影响，发现融合可提升性能，硬编码融合策略更优。


<details>
  <summary>Details</summary>
Motivation: 多数SatML模型主要针对光学输入模态，为理解在监督学习中结合其他输入模态与光学图像的价值。

Method: 通过在分类、回归和分割数据集上添加额外地理数据层生成增强版SatML基准任务。

Result: 融合额外地理输入与光学图像可显著提升SatML模型性能，在标记数据有限和地理外样本设置中优势最大，硬编码融合策略优于学习变体。

Conclusion: 多模态输入对SatML模型的数据效率和外样本性能有价值，硬编码融合策略值得未来研究。

Abstract: A large variety of geospatial data layers is available around the world
ranging from remotely-sensed raster data like satellite imagery, digital
elevation models, predicted land cover maps, and human-annotated data, to data
derived from environmental sensors such as air temperature or wind speed data.
A large majority of machine learning models trained on satellite imagery
(SatML), however, are designed primarily for optical input modalities such as
multi-spectral satellite imagery. To better understand the value of using other
input modalities alongside optical imagery in supervised learning settings, we
generate augmented versions of SatML benchmark tasks by appending additional
geographic data layers to datasets spanning classification, regression, and
segmentation. Using these augmented datasets, we find that fusing additional
geographic inputs with optical imagery can significantly improve SatML model
performance. Benefits are largest in settings where labeled data are limited
and in geographic out-of-sample settings, suggesting that multi-modal inputs
may be especially valuable for data-efficiency and out-of-sample performance of
SatML models. Surprisingly, we find that hard-coded fusion strategies
outperform learned variants, with interesting implications for future work.

</details>


### [191] [Minimalist Concept Erasure in Generative Models](https://arxiv.org/abs/2507.13386)
*Yang Zhang,Er Jin,Yanfei Dong,Yixuan Wu,Philip Torr,Ashkan Khakzar,Johannes Stegmaier,Kenji Kawaguchi*

Main category: cs.CV

TL;DR: 本文提出基于最终生成输出分布距离的最小化概念擦除目标，结合神经元掩码提升擦除鲁棒性，在流匹配模型上验证可稳健擦除概念且不降低模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型依赖大量无标签数据引发安全和版权问题，且现有擦除方法过度修改损害模型实用性。

Method: 提出基于最终生成输出分布距离的最小化概念擦除目标，推导可微优化损失，结合神经元掩码替代模型微调。

Result: 在最先进的流匹配模型上的实证评估表明，该方法能稳健擦除概念且不降低整体模型性能。

Conclusion: 该方法为更安全、更负责任的生成模型发展铺平道路。

Abstract: Recent advances in generative models have demonstrated remarkable
capabilities in producing high-quality images, but their reliance on
large-scale unlabeled data has raised significant safety and copyright
concerns. Efforts to address these issues by erasing unwanted concepts have
shown promise. However, many existing erasure methods involve excessive
modifications that compromise the overall utility of the model. In this work,
we address these issues by formulating a novel minimalist concept erasure
objective based \emph{only} on the distributional distance of final generation
outputs. Building on our formulation, we derive a tractable loss for
differentiable optimization that leverages backpropagation through all
generation steps in an end-to-end manner. We also conduct extensive analysis to
show theoretical connections with other models and methods. To improve the
robustness of the erasure, we incorporate neuron masking as an alternative to
model fine-tuning. Empirical evaluations on state-of-the-art flow-matching
models demonstrate that our method robustly erases concepts without degrading
overall model performance, paving the way for safer and more responsible
generative models.

</details>


### [192] [MADI: Masking-Augmented Diffusion with Inference-Time Scaling for Visual Editing](https://arxiv.org/abs/2507.13401)
*Shreya Kadambi,Risheek Garrepalli,Shubhankar Borse,Munawar Hyatt,Fatih Porikli*

Main category: cs.CV

TL;DR: 本文提出MADI框架提升扩散模型在结构化、可控生成与编辑方面的能力，包括MAgD训练策略和基于暂停令牌的推理时容量缩放机制，且训练时采用表达性强的提示可增强性能。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在文本到图像生成方面取得成功，但在视觉编辑和组合控制上仍有挑战，受自监督学习和上下文生成建模进展的启发，要提升其结构化、可控生成与编辑能力。

Method: 提出Masking - Augmented Diffusion with Inference - Time Scaling (MADI)框架，包括Masking - Augmented gaussian Diffusion (MAgD)训练策略和基于暂停令牌的推理时容量缩放机制，训练时采用表达性强的提示。

Result: 采用表达性和密集提示训练可进一步提升性能，MADI显著增强了扩散模型的可编辑性。

Conclusion: MADI的这些贡献为将扩散模型集成到更通用的上下文生成扩散架构铺平道路。

Abstract: Despite the remarkable success of diffusion models in text-to-image
generation, their effectiveness in grounded visual editing and compositional
control remains challenging. Motivated by advances in self-supervised learning
and in-context generative modeling, we propose a series of simple yet powerful
design choices that significantly enhance diffusion model capacity for
structured, controllable generation and editing. We introduce Masking-Augmented
Diffusion with Inference-Time Scaling (MADI), a framework that improves the
editability, compositionality and controllability of diffusion models through
two core innovations. First, we introduce Masking-Augmented gaussian Diffusion
(MAgD), a novel training strategy with dual corruption process which combines
standard denoising score matching and masked reconstruction by masking noisy
input from forward process. MAgD encourages the model to learn discriminative
and compositional visual representations, thus enabling localized and
structure-aware editing. Second, we introduce an inference-time capacity
scaling mechanism based on Pause Tokens, which act as special placeholders
inserted into the prompt for increasing computational capacity at inference
time. Our findings show that adopting expressive and dense prompts during
training further enhances performance, particularly for MAgD. Together, these
contributions in MADI substantially enhance the editability of diffusion
models, paving the way toward their integration into more general-purpose,
in-context generative diffusion architectures.

</details>


### [193] [UL-DD: A Multimodal Drowsiness Dataset Using Video, Biometric Signals, and Behavioral Data](https://arxiv.org/abs/2507.13403)
*Morteza Bodaghi,Majid Hosseini,Raju Gottumukkala,Ravi Teja Bhupatiraju,Iftikhar Ahmad,Moncef Gabbouj*

Main category: cs.CV

TL;DR: 本文提出用于驾驶员嗜睡检测的多模态公共数据集，含多种信号，采集19人数据，每次40分钟，共1400分钟，记录状态渐变。


<details>
  <summary>Details</summary>
Motivation: 创建能捕捉更广泛生理、行为和驾驶相关信号的驾驶员嗜睡多模态数据集。

Method: 整合面部、行为和生物特征指标的多模态信号，使用深度相机、红外相机等设备采集数据，用KSS让受试者自我报告嗜睡程度。

Result: 构建了多模态数据集，每次数据采集持续40分钟，共1400分钟，记录了驾驶员状态的逐渐变化。

Conclusion: 该数据集可用于驾驶员嗜睡检测研究，将应要求提供给相应作者。

Abstract: In this study, we present a comprehensive public dataset for driver
drowsiness detection, integrating multimodal signals of facial, behavioral, and
biometric indicators. Our dataset includes 3D facial video using a depth
camera, IR camera footage, posterior videos, and biometric signals such as
heart rate, electrodermal activity, blood oxygen saturation, skin temperature,
and accelerometer data. This data set provides grip sensor data from the
steering wheel and telemetry data from the American truck simulator game to
provide more information about drivers' behavior while they are alert and
drowsy. Drowsiness levels were self-reported every four minutes using the
Karolinska Sleepiness Scale (KSS). The simulation environment consists of three
monitor setups, and the driving condition is completely like a car. Data were
collected from 19 subjects (15 M, 4 F) in two conditions: when they were fully
alert and when they exhibited signs of sleepiness. Unlike other datasets, our
multimodal dataset has a continuous duration of 40 minutes for each data
collection session per subject, contributing to a total length of 1,400
minutes, and we recorded gradual changes in the driver state rather than
discrete alert/drowsy labels. This study aims to create a comprehensive
multimodal dataset of driver drowsiness that captures a wider range of
physiological, behavioral, and driving-related signals. The dataset will be
available upon request to the corresponding author.

</details>


### [194] [COREVQA: A Crowd Observation and Reasoning Entailment Visual Question Answering Benchmark](https://arxiv.org/abs/2507.13405)
*Ishant Chintapatla,Kazuma Choji,Naaisha Agarwal,Andrew Lin,Hannah You,Charles Duong,Kevin Zhu,Sean O'Brien,Vasu Sharma*

Main category: cs.CV

TL;DR: 现有视觉语言模型基准很少测试视觉蕴含推理能力，提出COREVQA基准，结果显示模型表现不佳，存在关键局限。


<details>
  <summary>Details</summary>
Motivation: 现有基准很少测试模型准确完成视觉蕴含的能力，需新基准进行测试。

Method: 提出COREVQA基准，包含5608个图像和合成的真假陈述对，图像来自CrowdHuman数据集。

Result: 即使表现最好的视觉语言模型准确率也低于80%，其他模型表现更差（39.98%-69.95%）。

Conclusion: 视觉语言模型在拥挤场景中对特定类型图像 - 问题对的推理能力存在关键局限。

Abstract: Recently, many benchmarks and datasets have been developed to evaluate
Vision-Language Models (VLMs) using visual question answering (VQA) pairs, and
models have shown significant accuracy improvements. However, these benchmarks
rarely test the model's ability to accurately complete visual entailment, for
instance, accepting or refuting a hypothesis based on the image. To address
this, we propose COREVQA (Crowd Observations and Reasoning Entailment), a
benchmark of 5608 image and synthetically generated true/false statement pairs,
with images derived from the CrowdHuman dataset, to provoke visual entailment
reasoning on challenging crowded images. Our results show that even the
top-performing VLMs achieve accuracy below 80%, with other models performing
substantially worse (39.98%-69.95%). This significant performance gap reveals
key limitations in VLMs' ability to reason over certain types of image-question
pairs in crowded scenes.

</details>


### [195] [When Person Re-Identification Meets Event Camera: A Benchmark Dataset and An Attribute-guided Re-Identification Framework](https://arxiv.org/abs/2507.13659)
*Xiao Wang,Qian Zhu,Shujuan Wu,Bo Jiang,Shiliang Zhang,Yaowei Wang,Yonghong Tian,Bin Luo*

Main category: cs.CV

TL;DR: 本文引入大规模RGB - 事件行人重识别数据集EvReID，评估15种算法，提出TriPro - ReID框架，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于事件相机的行人重识别算法在小规模或模拟数据集上训练评估，难以评估真实识别性能和泛化能力，存在数据稀缺问题。

Method: 引入大规模RGB - 事件行人重识别数据集EvReID，评估15种算法，提出行人属性引导的对比学习框架TriPro - ReID。

Result: 在EvReID和MARS数据集上的实验充分验证了提出的RGB - 事件行人重识别框架的有效性。

Conclusion: 所构建的数据集和提出的框架为未来研究奠定了数据和基准测试基础，相关数据集和代码将开源。

Abstract: Recent researchers have proposed using event cameras for person
re-identification (ReID) due to their promising performance and better balance
in terms of privacy protection, event camera-based person ReID has attracted
significant attention. Currently, mainstream event-based person ReID algorithms
primarily focus on fusing visible light and event stream, as well as preserving
privacy. Although significant progress has been made, these methods are
typically trained and evaluated on small-scale or simulated event camera
datasets, making it difficult to assess their real identification performance
and generalization ability. To address the issue of data scarcity, this paper
introduces a large-scale RGB-event based person ReID dataset, called EvReID.
The dataset contains 118,988 image pairs and covers 1200 pedestrian identities,
with data collected across multiple seasons, scenes, and lighting conditions.
We also evaluate 15 state-of-the-art person ReID algorithms, laying a solid
foundation for future research in terms of both data and benchmarking. Based on
our newly constructed dataset, this paper further proposes a pedestrian
attribute-guided contrastive learning framework to enhance feature learning for
person re-identification, termed TriPro-ReID. This framework not only
effectively explores the visual features from both RGB frames and event
streams, but also fully utilizes pedestrian attributes as mid-level semantic
features. Extensive experiments on the EvReID dataset and MARS datasets fully
validated the effectiveness of our proposed RGB-Event person ReID framework.
The benchmark dataset and source code will be released on
https://github.com/Event-AHU/Neuromorphic_ReID

</details>


### [196] [HeCoFuse: Cross-Modal Complementary V2X Cooperative Perception with Heterogeneous Sensors](https://arxiv.org/abs/2507.13677)
*Chuheng Wei,Ziye Qin,Walter Zimmer,Guoyuan Wu,Matthew J. Barth*

Main category: cs.CV

TL;DR: 提出HeCoFuse框架用于异构传感器配置下的V2X协同感知，在TUMTraf - V2X数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现实中V2X协同感知系统存在异构传感器配置，给特征融合和感知可靠性带来挑战。

Method: 提出HeCoFuse框架，采用分层融合机制，结合通道和空间注意力自适应加权特征，使用自适应空间分辨率调整模块平衡计算成本和融合效果，实施基于可用模态动态调整融合类型的协同学习策略。

Result: 在TUMTraf - V2X数据集的全传感器配置下3D mAP达43.22%，在L + LC场景达43.38%，在九种异构传感器配置下3D mAP在21.74% - 43.38%之间，获CVPR 2025 DriveX挑战赛第一名。

Conclusion: HeCoFuse是TUM - Traf V2X数据集上的当前最优方法，在不同传感器部署中表现稳健。

Abstract: Real-world Vehicle-to-Everything (V2X) cooperative perception systems often
operate under heterogeneous sensor configurations due to cost constraints and
deployment variability across vehicles and infrastructure. This heterogeneity
poses significant challenges for feature fusion and perception reliability. To
address these issues, we propose HeCoFuse, a unified framework designed for
cooperative perception across mixed sensor setups where nodes may carry Cameras
(C), LiDARs (L), or both. By introducing a hierarchical fusion mechanism that
adaptively weights features through a combination of channel-wise and spatial
attention, HeCoFuse can tackle critical challenges such as cross-modality
feature misalignment and imbalanced representation quality. In addition, an
adaptive spatial resolution adjustment module is employed to balance
computational cost and fusion effectiveness. To enhance robustness across
different configurations, we further implement a cooperative learning strategy
that dynamically adjusts fusion type based on available modalities. Experiments
on the real-world TUMTraf-V2X dataset demonstrate that HeCoFuse achieves 43.22%
3D mAP under the full sensor configuration (LC+LC), outperforming the CoopDet3D
baseline by 1.17%, and reaches an even higher 43.38% 3D mAP in the L+LC
scenario, while maintaining 3D mAP in the range of 21.74% to 43.38% across nine
heterogeneous sensor configurations. These results, validated by our
first-place finish in the CVPR 2025 DriveX challenge, establish HeCoFuse as the
current state-of-the-art on TUM-Traf V2X dataset while demonstrating robust
performance across diverse sensor deployments.

</details>


### [197] [Sugar-Beet Stress Detection using Satellite Image Time Series](https://arxiv.org/abs/2507.13514)
*Bhumika Laxman Sadbhave,Philipp Vaeth,Denise Dejon,Gunther Schorcht,Magda Gregorová*

Main category: cs.CV

TL;DR: 使用全无监督方法，通过3D卷积自动编码器模型结合时间编码对甜菜田进行压力检测，系统可跨年份应用。


<details>
  <summary>Details</summary>
Motivation: 利用卫星图像时间序列数据的丰富光谱和时间特性，解决甜菜田压力检测问题。

Method: 提出3D卷积自动编码器模型从Sentinel - 2图像序列提取特征，结合特定日期时间编码，用于下游聚类任务。

Result: 得到可直接应用于不同年份数据的压力检测系统。

Conclusion: 该方法为甜菜压力检测提供了实用且易获取的工具。

Abstract: Satellite Image Time Series (SITS) data has proven effective for agricultural
tasks due to its rich spectral and temporal nature. In this study, we tackle
the task of stress detection in sugar-beet fields using a fully unsupervised
approach. We propose a 3D convolutional autoencoder model to extract meaningful
features from Sentinel-2 image sequences, combined with
acquisition-date-specific temporal encodings to better capture the growth
dynamics of sugar-beets. The learned representations are used in a downstream
clustering task to separate stressed from healthy fields. The resulting stress
detection system can be directly applied to data from different years, offering
a practical and accessible tool for stress detection in sugar-beets.

</details>


### [198] [Can Synthetic Images Conquer Forgetting? Beyond Unexplored Doubts in Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2507.13739)
*Junsu Kim,Yunhoe Ku,Seungryul Baek*

Main category: cs.CV

TL;DR: 提出Diffusion - FSCIL方法解决少样本类增量学习问题，在多数据集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 少样本类增量学习因训练数据有限，需减少灾难性遗忘和学习新信息。

Method: 采用文本到图像扩散模型作为冻结骨干网络，提取多个互补扩散特征，利用特征蒸馏防止生成偏差，通过使用冻结骨干、最少可训练组件和批量处理多特征提取实现效率。

Result: 在CUB - 200、miniImageNet和CIFAR - 100上的实验表明，Diffusion - FSCIL超越了现有方法。

Conclusion: Diffusion - FSCIL能在保留先前学习类别的性能的同时，有效适应新类别。

Abstract: Few-shot class-incremental learning (FSCIL) is challenging due to extremely
limited training data; while aiming to reduce catastrophic forgetting and learn
new information. We propose Diffusion-FSCIL, a novel approach that employs a
text-to-image diffusion model as a frozen backbone. Our conjecture is that
FSCIL can be tackled using a large generative model's capabilities benefiting
from 1) generation ability via large-scale pre-training; 2) multi-scale
representation; 3) representational flexibility through the text encoder. To
maximize the representation capability, we propose to extract multiple
complementary diffusion features to play roles as latent replay with slight
support from feature distillation for preventing generative biases. Our
framework realizes efficiency through 1) using a frozen backbone; 2) minimal
trainable components; 3) batch processing of multiple feature extractions.
Extensive experiments on CUB-200, \emph{mini}ImageNet, and CIFAR-100 show that
Diffusion-FSCIL surpasses state-of-the-art methods, preserving performance on
previously learned classes and adapting effectively to new ones.

</details>


### [199] [Learning Spectral Diffusion Prior for Hyperspectral Image Reconstruction](https://arxiv.org/abs/2507.13769)
*Mingyang Yu,Zhijian Wu,Dingjiang Huang*

Main category: cs.CV

TL;DR: 提出光谱扩散先验（SDP）和光谱先验注入模块（SPIM）改进高光谱图像重建性能，实验表明优于现有网络约0.5dB。


<details>
  <summary>Details</summary>
Motivation: 深度学习方法难以准确捕捉高光谱图像高频细节。

Method: 用扩散模型从高光谱图像隐式学习光谱扩散先验（SDP），提出光谱先验注入模块（SPIM）动态引导模型恢复细节。

Result: 在MST和BISRNet上实验，方法比现有网络性能高约0.5dB。

Conclusion: 所提方法能有效提高高光谱图像重建性能。

Abstract: Hyperspectral image (HSI) reconstruction aims to recover 3D HSI from its
degraded 2D measurements. Recently great progress has been made in deep
learning-based methods, however, these methods often struggle to accurately
capture high-frequency details of the HSI. To address this issue, this paper
proposes a Spectral Diffusion Prior (SDP) that is implicitly learned from
hyperspectral images using a diffusion model. Leveraging the powerful ability
of the diffusion model to reconstruct details, this learned prior can
significantly improve the performance when injected into the HSI model. To
further improve the effectiveness of the learned prior, we also propose the
Spectral Prior Injector Module (SPIM) to dynamically guide the model to recover
the HSI details. We evaluate our method on two representative HSI methods: MST
and BISRNet. Experimental results show that our method outperforms existing
networks by about 0.5 dB, effectively improving the performance of HSI
reconstruction.

</details>


### [200] [Localized FNO for Spatiotemporal Hemodynamic Upsampling in Aneurysm MRI](https://arxiv.org/abs/2507.13789)
*Kyriakos Flouris,Moritz Halter,Yolanne Y. R. Lee,Samuel Castonguay,Luuk Jacobs,Pietro Dirix,Jonathan Nestmann,Sebastian Kozerke,Ender Konukoglu*

Main category: cs.CV

TL;DR: 提出LoFNO架构提升血流动力学分析的时空分辨率，实现更精确的脑血管诊断。


<details>
  <summary>Details</summary>
Motivation: 磁共振血流成像时空分辨率和信噪比低，限制其在动脉瘤破裂预测和治疗指导中的诊断效用。

Method: 提出LoFNO架构，集成拉普拉斯特征向量作为几何先验，采用EDSR层进行上采样，结合几何先验和神经算子框架。

Result: LoFNO在速度和壁面剪应力预测上优于插值和其他深度学习方法。

Conclusion: LoFNO能实现更精确的脑血管诊断。

Abstract: Hemodynamic analysis is essential for predicting aneurysm rupture and guiding
treatment. While magnetic resonance flow imaging enables time-resolved
volumetric blood velocity measurements, its low spatiotemporal resolution and
signal-to-noise ratio limit its diagnostic utility. To address this, we propose
the Localized Fourier Neural Operator (LoFNO), a novel 3D architecture that
enhances both spatial and temporal resolution with the ability to predict wall
shear stress (WSS) directly from clinical imaging data. LoFNO integrates
Laplacian eigenvectors as geometric priors for improved structural awareness on
irregular, unseen geometries and employs an Enhanced Deep Super-Resolution
Network (EDSR) layer for robust upsampling. By combining geometric priors with
neural operator frameworks, LoFNO de-noises and spatiotemporally upsamples flow
data, achieving superior velocity and WSS predictions compared to interpolation
and alternative deep learning methods, enabling more precise cerebrovascular
diagnostics.

</details>


### [201] [One Step Closer: Creating the Future to Boost Monocular Semantic Scene Completion](https://arxiv.org/abs/2507.13801)
*Haoang Lu,Yuanqi Su,Xiaoning Zhang,Hao Hu*

Main category: cs.CV

TL;DR: 提出CF - SSC框架解决现有单目SSC方法在处理遮挡和视野外场景的局限，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有单目SSC方法在现实交通场景中，对场景遮挡和视野外部分处理不足，需要新方法解决。

Method: 提出CF - SSC框架，利用伪未来帧预测扩大感知范围，结合姿态和深度建立3D对应关系，实现过去、现在和未来帧的3D融合，采用3D感知架构显式建模时空关系。

Result: 在SemanticKITTI和SSCBench - KITTI - 360基准测试中达到了最先进的性能。

Conclusion: 该方法能有效提高遮挡推理和3D场景补全的准确性。

Abstract: In recent years, visual 3D Semantic Scene Completion (SSC) has emerged as a
critical perception task for autonomous driving due to its ability to infer
complete 3D scene layouts and semantics from single 2D images. However, in
real-world traffic scenarios, a significant portion of the scene remains
occluded or outside the camera's field of view -- a fundamental challenge that
existing monocular SSC methods fail to address adequately. To overcome these
limitations, we propose Creating the Future SSC (CF-SSC), a novel temporal SSC
framework that leverages pseudo-future frame prediction to expand the model's
effective perceptual range. Our approach combines poses and depths to establish
accurate 3D correspondences, enabling geometrically-consistent fusion of past,
present, and predicted future frames in 3D space. Unlike conventional methods
that rely on simple feature stacking, our 3D-aware architecture achieves more
robust scene completion by explicitly modeling spatial-temporal relationships.
Comprehensive experiments on SemanticKITTI and SSCBench-KITTI-360 benchmarks
demonstrate state-of-the-art performance, validating the effectiveness of our
approach, highlighting our method's ability to improve occlusion reasoning and
3D scene completion accuracy.

</details>


### [202] [Team of One: Cracking Complex Video QA with Model Synergy](https://arxiv.org/abs/2507.13820)
*Jun Xie,Zhaoran Zhao,Xiongjun Guan,Yingjian Zhu,Hongzhu Yi,Xinming Wang,Feng Chen,Zhepeng Wang*

Main category: cs.CV

TL;DR: 提出用于开放式视频问答的新框架，经CVRR - ES数据集验证，优于现有基线，为视频大多模态模型发展奠基。


<details>
  <summary>Details</summary>
Motivation: 现有视频大多模态模型存在上下文理解有限、时间建模弱、对模糊或组合查询泛化能力差等问题。

Method: 引入提示 - 响应集成机制，通过结构化思维链协调多个异构视频 - 语言模型，并利用外部大语言模型作为评估和集成器。

Result: 实验表明该方法在所有评估指标上显著优于现有基线，具有更好的泛化性和鲁棒性。

Conclusion: 该方法提供了轻量级、可扩展的多模态推理策略，无需模型重新训练，为未来视频大多模态模型发展奠定基础。

Abstract: We propose a novel framework for open-ended video question answering that
enhances reasoning depth and robustness in complex real-world scenarios, as
benchmarked on the CVRR-ES dataset. Existing Video-Large Multimodal Models
(Video-LMMs) often exhibit limited contextual understanding, weak temporal
modeling, and poor generalization to ambiguous or compositional queries. To
address these challenges, we introduce a prompting-and-response integration
mechanism that coordinates multiple heterogeneous Video-Language Models (VLMs)
via structured chains of thought, each tailored to distinct reasoning pathways.
An external Large Language Model (LLM) serves as an evaluator and integrator,
selecting and fusing the most reliable responses. Extensive experiments
demonstrate that our method significantly outperforms existing baselines across
all evaluation metrics, showcasing superior generalization and robustness. Our
approach offers a lightweight, extensible strategy for advancing multimodal
reasoning without requiring model retraining, setting a strong foundation for
future Video-LMM development.

</details>


### [203] [Tackling fake images in cybersecurity -- Interpretation of a StyleGAN and lifting its black-box](https://arxiv.org/abs/2507.13722)
*Julia Laubmann,Johannes Reschke*

Main category: cs.CV

TL;DR: 本文聚焦分析StyleGAN生成器组件内部运作，训练模型、进行剪枝，研究潜向量作用，指出可微调视觉特征但存在伦理风险。


<details>
  <summary>Details</summary>
Motivation: 深入了解StyleGAN模型运作方式，应对AI生成图像带来的担忧。

Method: 使用PyTorch框架训练StyleGAN模型，通过剪枝研究权重，详细探索关键架构元素和技术，研究潜向量作用。

Result: 大量权重可移除不影响输出，减少计算需求；全局改变潜向量影响色调，针对性改变可精确操纵面部特征。

Conclusion: StyleGAN可微调视觉特征，但存在被恶意利用制造虚假身份的伦理风险。

Abstract: In today's digital age, concerns about the dangers of AI-generated images are
increasingly common. One powerful tool in this domain is StyleGAN (style-based
generative adversarial networks), a generative adversarial network capable of
producing highly realistic synthetic faces. To gain a deeper understanding of
how such a model operates, this work focuses on analyzing the inner workings of
StyleGAN's generator component. Key architectural elements and techniques, such
as the Equalized Learning Rate, are explored in detail to shed light on the
model's behavior. A StyleGAN model is trained using the PyTorch framework,
enabling direct inspection of its learned weights. Through pruning, it is
revealed that a significant number of these weights can be removed without
drastically affecting the output, leading to reduced computational
requirements. Moreover, the role of the latent vector -- which heavily
influences the appearance of the generated faces -- is closely examined. Global
alterations to this vector primarily affect aspects like color tones, while
targeted changes to individual dimensions allow for precise manipulation of
specific facial features. This ability to finetune visual traits is not only of
academic interest but also highlights a serious ethical concern: the potential
misuse of such technology. Malicious actors could exploit this capability to
fabricate convincing fake identities, posing significant risks in the context
of digital deception and cybercrime.

</details>


### [204] [When Seeing Overrides Knowing: Disentangling Knowledge Conflicts in Vision-Language Models](https://arxiv.org/abs/2507.13868)
*Francesco Ortu,Zhijing Jin,Diego Doimo,Alberto Cazzaniga*

Main category: cs.CV

TL;DR: 分析视觉语言模型解决跨模态冲突的机制，用多模态反事实查询数据集，定位关键头并修改，发现关键头注意力定位图像区域效果更好。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型利用多元知识源处理复杂任务时，内部参数知识和外部信息会冲突，导致幻觉和不可靠响应，且冲突交互机制未知。

Method: 引入多模态反事实查询数据集，通过对数检查定位控制冲突的关键头，修改关键头引导模型偏向内部知识或视觉输入。

Result: 定位到控制冲突的关键头，可通过修改关键头引导模型，关键头注意力定位图像区域精度优于基于梯度的归因。

Conclusion: 分析了视觉语言模型解决跨模态冲突的机制，关键头在处理冲突和定位图像区域有良好表现。

Abstract: Vision-language models (VLMs) increasingly leverage diverse knowledge sources
to address complex tasks, often encountering conflicts between their internal
parametric knowledge and external information. Knowledge conflicts can result
in hallucinations and unreliable responses, but the mechanisms governing such
interactions remain unknown. To address this gap, we analyze the mechanisms
that VLMs use to resolve cross-modal conflicts by introducing a dataset of
multimodal counterfactual queries that deliberately contradict internal
commonsense knowledge. We localize with logit inspection a small set of heads
that control the conflict. Moreover, by modifying these heads, we can steer the
model towards its internal knowledge or the visual inputs. Finally, we show
that attention from such heads pinpoints localized image regions driving visual
overrides, outperforming gradient-based attribution in precision.

</details>


### [205] [Real-Time Fusion of Visual and Chart Data for Enhanced Maritime Vision](https://arxiv.org/abs/2507.13880)
*Marten Kreis,Benjamin Kiefer*

Main category: cs.CV

TL;DR: 提出融合实时视觉数据和海图信息增强海洋视觉的新方法，实验显示在动态复杂环境中显著提升目标定位和关联精度。


<details>
  <summary>Details</summary>
Motivation: 增强海洋视觉效果，提升在动态复杂环境中目标定位和关联的准确性。

Method: 将航海图数据叠加到实时视频上，引入基于Transformer的端到端神经网络预测浮标边界框和置信度分数，实现图像检测与海图标记匹配，并与基于射线投射模型和扩展距离估计模块的YOLOv7网络对比。

Result: 在真实海洋场景数据集实验中，该方法显著提高了目标定位和关联精度。

Conclusion: 所提方法在动态复杂海洋环境中能有效提高目标定位和关联的准确性。

Abstract: This paper presents a novel approach to enhancing marine vision by fusing
real-time visual data with chart information. Our system overlays nautical
chart data onto live video feeds by accurately matching detected navigational
aids, such as buoys, with their corresponding representations in chart data. To
achieve robust association, we introduce a transformer-based end-to-end neural
network that predicts bounding boxes and confidence scores for buoy queries,
enabling the direct matching of image-domain detections with world-space chart
markers. The proposed method is compared against baseline approaches, including
a ray-casting model that estimates buoy positions via camera projection and a
YOLOv7-based network extended with a distance estimation module. Experimental
results on a dataset of real-world maritime scenes demonstrate that our
approach significantly improves object localization and association accuracy in
dynamic and challenging environments.

</details>


### [206] [Feature Engineering is Not Dead: Reviving Classical Machine Learning with Entropy, HOG, and LBP Feature Fusion for Image Classification](https://arxiv.org/abs/2507.13772)
*Abhijit Sen,Giridas Maiti,Bikram K. Parida,Bhanu P. Mishra,Mahima Arya,Denys I. Bondar*

Main category: cs.CV

TL;DR: 本文提出基于排列熵（PE）的图像分类方法，融合HOG和LBP特征，用SVM分类，在多数据集获有竞争力结果，为图像分类提供轻量级可解释方案。


<details>
  <summary>Details</summary>
Motivation: 在图像分类中，当更看重可解释性和计算效率时，经典机器学习方法有优势，而PE在图像数据应用少，故研究其在图像分类中的应用。

Method: 将PE扩展到二维图像，提出多尺度、多方向熵特征提取方法，融合HOG和LBP特征，用网格搜索优化SVM分类器。

Result: 在Fashion - MNIST、KMNIST、EMNIST和CIFAR - 10等多个基准数据集上取得有竞争力的分类性能。

Conclusion: PE与HOG和LBP的融合为深度学习模型提供了紧凑、可解释且有效的替代方案，表明基于熵的描述符在图像分类中有潜力。

Abstract: Feature engineering continues to play a critical role in image
classification, particularly when interpretability and computational efficiency
are prioritized over deep learning models with millions of parameters. In this
study, we revisit classical machine learning based image classification through
a novel approach centered on Permutation Entropy (PE), a robust and
computationally lightweight measure traditionally used in time series analysis
but rarely applied to image data. We extend PE to two-dimensional images and
propose a multiscale, multi-orientation entropy-based feature extraction
approach that characterizes spatial order and complexity along rows, columns,
diagonals, anti-diagonals, and local patches of the image. To enhance the
discriminatory power of the entropy features, we integrate two classic image
descriptors: the Histogram of Oriented Gradients (HOG) to capture shape and
edge structure, and Local Binary Patterns (LBP) to encode micro-texture of an
image. The resulting hand-crafted feature set, comprising of 780 dimensions, is
used to train Support Vector Machine (SVM) classifiers optimized through grid
search. The proposed approach is evaluated on multiple benchmark datasets,
including Fashion-MNIST, KMNIST, EMNIST, and CIFAR-10, where it delivers
competitive classification performance without relying on deep architectures.
Our results demonstrate that the fusion of PE with HOG and LBP provides a
compact, interpretable, and effective alternative to computationally expensive
and limited interpretable deep learning models. This shows a potential of
entropy-based descriptors in image classification and contributes a lightweight
and generalizable solution to interpretable machine learning in image
classification and computer vision.

</details>


### [207] [Generalist Forecasting with Frozen Video Models via Latent Diffusion](https://arxiv.org/abs/2507.13942)
*Jacob C Walker,Pedro Vélez,Luisa Polania Cabrera,Guangyao Zhou,Rishabh Kabra,Carl Doersch,Maks Ovsjanikov,João Carreira,Shiry Ginosar*

Main category: cs.CV

TL;DR: 本文发现视觉模型感知能力与短期通用预测性能强相关，提出通用预测框架并引入分布指标评估，强调结合表征学习和生成建模对视频理解的价值。


<details>
  <summary>Details</summary>
Motivation: 预测未来情况对通用系统很关键，研究视觉模型感知能力与短期通用预测性能的关系。

Method: 提出在任何冻结视觉骨干上运行的通用预测框架，训练潜在扩散模型预测冻结表示空间中的未来特征，通过轻量级特定任务读出解码；引入分布指标评估。

Result: 发现视觉模型感知能力和短期通用预测性能强相关，该趋势在多种预训练模型和不同抽象层次都成立。

Conclusion: 强调结合表征学习和生成建模对基于时间的视频理解有重要价值。

Abstract: Forecasting what will happen next is a critical skill for general-purpose
systems that plan or act in the world at different levels of abstraction. In
this paper, we identify a strong correlation between a vision model's
perceptual ability and its generalist forecasting performance over short time
horizons. This trend holds across a diverse set of pretrained models-including
those trained generatively-and across multiple levels of abstraction, from raw
pixels to depth, point tracks, and object motion. The result is made possible
by a novel generalist forecasting framework that operates on any frozen vision
backbone: we train latent diffusion models to forecast future features in the
frozen representation space, which are then decoded via lightweight,
task-specific readouts. To enable consistent evaluation across tasks, we
introduce distributional metrics that compare distributional properties
directly in the space of downstream tasks and apply this framework to nine
models and four tasks. Our results highlight the value of bridging
representation learning and generative modeling for temporally grounded video
understanding.

</details>


### [208] [QuantEIT: Ultra-Lightweight Quantum-Assisted Inference for Chest Electrical Impedance Tomography](https://arxiv.org/abs/2507.14031)
*Hao Fang,Sihao Teng,Hao Yu,Siyi Yuan,Huaiwu He,Zhe Liu,Yunjie Yang*

Main category: cs.CV

TL;DR: 提出用于EIT图像重建的Ultra - Lightweight Quantum - Assisted Inference (QuantEIT)框架，该框架无监督、无需训练数据，实验表明其性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: EIT存在病态逆问题，深度学习方法依赖复杂网络架构和大量参数，效率和可扩展性受限。

Method: 提出QuantEIT框架，利用量子辅助网络（QA - Net），结合并行2 - 比特量子电路生成潜在表示作为隐式非线性先验，再通过单层线性层进行电导率重建。

Result: 在模拟和真实的2D、3D EIT肺部成像数据上实验，QuantEIT使用仅0.2%的参数达到可比或更优的重建精度，且对噪声有更强鲁棒性。

Conclusion: QuantEIT框架有效，能解决EIT图像重建问题，且具有低复杂度和高鲁棒性的优势。

Abstract: Electrical Impedance Tomography (EIT) is a non-invasive, low-cost bedside
imaging modality with high temporal resolution, making it suitable for bedside
monitoring. However, its inherently ill-posed inverse problem poses significant
challenges for accurate image reconstruction. Deep learning (DL)-based
approaches have shown promise but often rely on complex network architectures
with a large number of parameters, limiting efficiency and scalability. Here,
we propose an Ultra-Lightweight Quantum-Assisted Inference (QuantEIT) framework
for EIT image reconstruction. QuantEIT leverages a Quantum-Assisted Network
(QA-Net), combining parallel 2-qubit quantum circuits to generate expressive
latent representations that serve as implicit nonlinear priors, followed by a
single linear layer for conductivity reconstruction. This design drastically
reduces model complexity and parameter number. Uniquely, QuantEIT operates in
an unsupervised, training-data-free manner and represents the first integration
of quantum circuits into EIT image reconstruction. Extensive experiments on
simulated and real-world 2D and 3D EIT lung imaging data demonstrate that
QuantEIT outperforms conventional methods, achieving comparable or superior
reconstruction accuracy using only 0.2% of the parameters, with enhanced
robustness to noise.

</details>


### [209] [CSD-VAR: Content-Style Decomposition in Visual Autoregressive Models](https://arxiv.org/abs/2507.13984)
*Quang-Binh Nguyen,Minh Luu,Quang Nguyen,Anh Tran,Khoi Nguyen*

Main category: cs.CV

TL;DR: 本文探索将视觉自回归建模（VAR）用于内容 - 风格分解（CSD），提出CSD - VAR方法，引入三项创新，还推出CSD - 100数据集，实验表明CSD - VAR表现优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 现有个性化方法针对扩散模型进行内容风格分解，而VAR作为有潜力的替代方案，需探索其用于CSD的可能性。

Method: 提出CSD - VAR方法，包括尺度感知交替优化策略、基于SVD的校正方法和增强键值（K - V）内存；引入CSD - 100数据集进行基准测试。

Result: CSD - VAR在实验中优于先前方法，实现了更好的内容保留和风格化保真度。

Conclusion: VAR可作为CSD的生成框架，CSD - VAR方法有效提升了内容风格分解效果。

Abstract: Disentangling content and style from a single image, known as content-style
decomposition (CSD), enables recontextualization of extracted content and
stylization of extracted styles, offering greater creative flexibility in
visual synthesis. While recent personalization methods have explored the
decomposition of explicit content style, they remain tailored for diffusion
models. Meanwhile, Visual Autoregressive Modeling (VAR) has emerged as a
promising alternative with a next-scale prediction paradigm, achieving
performance comparable to that of diffusion models. In this paper, we explore
VAR as a generative framework for CSD, leveraging its scale-wise generation
process for improved disentanglement. To this end, we propose CSD-VAR, a novel
method that introduces three key innovations: (1) a scale-aware alternating
optimization strategy that aligns content and style representation with their
respective scales to enhance separation, (2) an SVD-based rectification method
to mitigate content leakage into style representations, and (3) an Augmented
Key-Value (K-V) memory enhancing content identity preservation. To benchmark
this task, we introduce CSD-100, a dataset specifically designed for
content-style decomposition, featuring diverse subjects rendered in various
artistic styles. Experiments demonstrate that CSD-VAR outperforms prior
approaches, achieving superior content preservation and stylization fidelity.

</details>


### [210] [Multi-Centre Validation of a Deep Learning Model for Scoliosis Assessment](https://arxiv.org/abs/2507.14093)
*Šimon Kubov,Simon Klíčník,Jakub Dandár,Zdeněk Straka,Karolína Kvaková,Daniel Kvak*

Main category: cs.CV

TL;DR: 评估自动化深度学习软件测量脊柱侧弯Cobb角效果，结果显示其可复现专家水平测量，对临床工作有实用价值。


<details>
  <summary>Details</summary>
Motivation: 脊柱侧弯治疗决策依赖Cobb角精确测量，手动评估耗时长且存在观察者间差异，需自动化方法。

Method: 对10家医院103张全脊柱正位片进行回顾性多中心评估，用Bland Altman分析等评估AI与两位放射科医生测量结果的一致性。

Result: AI与放射科医生1对比，MAE为3.89度等；与医生2对比，MAE为3.90度等；Pearson相关系数及Cohen kappa值显示较好一致性。

Conclusion: 该软件能复现专家水平的Cobb角测量和分级，可用于简化脊柱侧弯报告和分诊工作流程。

Abstract: Scoliosis affects roughly 2 to 4 percent of adolescents, and treatment
decisions depend on precise Cobb angle measurement. Manual assessment is time
consuming and subject to inter observer variation. We conducted a
retrospective, multi centre evaluation of a fully automated deep learning
software (Carebot AI Bones, Spine Measurement functionality; Carebot s.r.o.) on
103 standing anteroposterior whole spine radiographs collected from ten
hospitals. Two musculoskeletal radiologists independently measured each study
and served as reference readers. Agreement between the AI and each radiologist
was assessed with Bland Altman analysis, mean absolute error (MAE), root mean
squared error (RMSE), Pearson correlation coefficient, and Cohen kappa for four
grade severity classification. Against Radiologist 1 the AI achieved an MAE of
3.89 degrees (RMSE 4.77 degrees) with a bias of 0.70 degrees and limits of
agreement from minus 8.59 to plus 9.99 degrees. Against Radiologist 2 the AI
achieved an MAE of 3.90 degrees (RMSE 5.68 degrees) with a bias of 2.14 degrees
and limits from minus 8.23 to plus 12.50 degrees. Pearson correlations were r
equals 0.906 and r equals 0.880 (inter reader r equals 0.928), while Cohen
kappa for severity grading reached 0.51 and 0.64 (inter reader kappa 0.59).
These results demonstrate that the proposed software reproduces expert level
Cobb angle measurements and categorical grading across multiple centres,
suggesting its utility for streamlining scoliosis reporting and triage in
clinical workflows.

</details>


### [211] [VLA-Mark: A cross modal watermark for large vision-language alignment model](https://arxiv.org/abs/2507.14067)
*Shuliang Liu,Qi Zheng,Jesse Jiaxi Xu,Yibo Yan,He Geng,Aiwei Liu,Peijie Jiang,Jia Liu,Yik-Cheung Tam,Xuming Hu*

Main category: cs.CV

TL;DR: 提出VLA - Mark框架用于视觉语言模型水印嵌入，实验效果优于传统方法，抗攻击能力强。


<details>
  <summary>Details</summary>
Motivation: 现有文本水印方法破坏视觉 - 文本对齐，无法保护语义关键概念，需要保护知识产权且不损害多模态连贯性的水印解决方案。

Method: 提出VLA - Mark框架，集成多尺度视觉 - 文本对齐指标，结合熵敏感机制指导水印注入，无需模型重新训练。

Result: 相比传统方法，PPL降低7.4%，BLEU提高26.6%，检测AUC达98.8%，对改写和同义词替换等攻击的抵御能力为96.1%，保持了文本 - 视觉一致性。

Conclusion: 该框架为保留质量的多模态水印设定了新标准。

Abstract: Vision-language models demand watermarking solutions that protect
intellectual property without compromising multimodal coherence. Existing text
watermarking methods disrupt visual-textual alignment through biased token
selection and static strategies, leaving semantic-critical concepts vulnerable.
We propose VLA-Mark, a vision-aligned framework that embeds detectable
watermarks while preserving semantic fidelity through cross-modal coordination.
Our approach integrates multiscale visual-textual alignment metrics, combining
localized patch affinity, global semantic coherence, and contextual attention
patterns, to guide watermark injection without model retraining. An
entropy-sensitive mechanism dynamically balances watermark strength and
semantic preservation, prioritizing visual grounding during low-uncertainty
generation phases. Experiments show 7.4% lower PPL and 26.6% higher BLEU than
conventional methods, with near-perfect detection (98.8% AUC). The framework
demonstrates 96.1\% attack resilience against attacks such as paraphrasing and
synonym substitution, while maintaining text-visual consistency, establishing
new standards for quality-preserving multimodal watermarking

</details>


### [212] [NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining](https://arxiv.org/abs/2507.14119)
*Maksim Kuprashevich,Grigorii Alekseenko,Irina Tolstykh,Georgii Fedorov,Bulat Suleimanov,Vladimir Dokholyan,Aleksandr Gordeev*

Main category: cs.CV

TL;DR: 提出自动化模块化管道挖掘高保真三元组，扩大数据集，发布NHR - Edit数据集和Bagel - NHR - Edit模型，表现出色。


<details>
  <summary>Details</summary>
Motivation: 生成式模型图像编辑助手监督训练需大量三元组，但挖掘像素精确示例困难，且缺乏可靠自动化编辑质量指标。

Method: 构建自动化、模块化管道，使用任务调优的Gemini验证器评分，通过反演和组合自举扩大挖掘集。

Result: 数据集扩大约2.2倍，NHR - Edit在最大跨数据集评估中超越所有公开替代方案，Bagel - NHR - Edit模型在实验中达最优指标。

Conclusion: 该方法可实现大规模无人工标注训练，发布的数据集和模型推动资源密集型领域研究。

Abstract: Recent advances in generative modeling enable image editing assistants that
follow natural language instructions without additional user input. Their
supervised training requires millions of triplets: original image, instruction,
edited image. Yet mining pixel-accurate examples is hard. Each edit must affect
only prompt-specified regions, preserve stylistic coherence, respect physical
plausibility, and retain visual appeal. The lack of robust automated
edit-quality metrics hinders reliable automation at scale. We present an
automated, modular pipeline that mines high-fidelity triplets across domains,
resolutions, instruction complexities, and styles. Built on public generative
models and running without human intervention, our system uses a task-tuned
Gemini validator to score instruction adherence and aesthetics directly,
removing any need for segmentation or grounding models. Inversion and
compositional bootstrapping enlarge the mined set by approximately 2.2x,
enabling large-scale high-fidelity training data. By automating the most
repetitive annotation steps, the approach allows a new scale of training
without human labeling effort. To democratize research in this
resource-intensive area, we release NHR-Edit: an open dataset of 358k
high-quality triplets. In the largest cross-dataset evaluation, it surpasses
all public alternatives. We also release Bagel-NHR-Edit, an open-source
fine-tuned Bagel model, which achieves state-of-the-art metrics in our
experiments.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [213] [Second-degree Price Discrimination: Theoretical Analysis, Experiment Design, and Empirical Estimation](https://arxiv.org/abs/2507.13426)
*Soheil Ghili,K. Sudhir,Nitish Jain,Ankur Garg*

Main category: econ.GN

TL;DR: 本文基于机制设计理论分析二级价格歧视（2PD）的实证模型，开发实验设计并在实地实施，展示模型对2PD决策的适用性，方法适用于广泛的2PD场景。


<details>
  <summary>Details</summary>
Motivation: 分析适用于研究2PD的随机系数离散选择（BLP）模型，解决2PD环境中的数据约束问题。

Method: 构建理论模型，开发实验设计，与国际航空公司合作在实地实施实验，对实验数据进行估计。

Result: 证明模型对2PD决策的适用性，表明设计的检验统计量可在估计需求模型前对最优2PD政策进行定性推断。

Conclusion: 所提出的方法论适用于广泛的二级价格歧视场景。

Abstract: We build on theoretical results from the mechanism design literature to
analyze empirical models of second-degree price discrimination (2PD). We show
that for a random-coefficients discrete choice ("BLP") model to be suitable for
studying 2PD, it must capture the covariance between two key random effects:
(i) the "baseline" willingness to pay (affecting all product versions), and
(ii) the perceived differentiation between versions. We then develop an
experimental design that, among other features, identifies this covariance
under common data constraints in 2PD environments. We implement this experiment
in the field in collaboration with an international airline. Estimating the
theoretically motivated empirical model on the experimental data, we
demonstrate its applicability to 2PD decisions. We also show that test
statistics from our design can enable qualitative inference on optimal 2PD
policy even before estimating a demand model. Our methodology applies broadly
across second-degree price discrimination settings.

</details>


### [214] [Navigating the Lobbying Landscape: Insights from Opinion Dynamics Models](https://arxiv.org/abs/2507.13767)
*Daniele Giachini,Leonardo Ciambezi Verdiana Del Rosso,Fabrizio Fornari,Valentina Pansanella,Lilit Popoyan,Alina Sîrbu*

Main category: econ.GN

TL;DR: 引入考虑游说者策略和预算的意见动态新模型，研究其数值动态并观察到不同状态，为研究实际游说策略奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有意见形成模型未考虑游说对公众意见和政策制定的重要影响，因此引入新模型。

Method: 引入新的意见动态模型，个体通过类似贝叶斯学习的过程更新意见，受认知偏差影响，对模型进行数值研究。

Result: 有游说和无游说时模型都有丰富动态，有游说时存在游说者完全影响网络和同行效应导致两极分化两种状态，对称游说者存在时各状态有不同特征。

Conclusion: 模型呈现的丰富动态为研究实际游说策略、验证模型提供了方向。

Abstract: While lobbying has been demonstrated to have an important effect on public
opinion and policy making, existing models of opinion formation do not
specifically include its effect. In this work we introduce a new model of
opinion dynamics where lobbyists can implement complex strategies and are
characterised by a finite budget. Individuals update their opinions through a
learning process resembling Bayesian learning, but influenced by cognitive
biases such as under-reaction and confirmation bias. We study the model
numerically and demonstrate rich dynamics both with and without lobbyists. In
the presence of lobbying, we observe two regimes: one in which lobbyists can
have full influence on the agent network, and another where the peer-effect
generates polarisation. When symmetric lobbyists are present, the lobbyist
influence regime is characterised by long opinion oscillations, while in the
transition area between the two regimes we observe convergence to the
optimistic model when the lobbying influence is long enough. These rich
dynamics pave the way for studying real lobbying strategies to validate the
model in practice.

</details>


### [215] [Choosing and Using Information in Evaluation Decisions](https://arxiv.org/abs/2507.13798)
*Katherine B. Coffman,Scott Kostyshak,Perihan O. Saygin*

Main category: econ.GN

TL;DR: 通过控制实验研究信息获取对候选人评估的影响，发现评估者获取个体信息不足致刻板评估。


<details>
  <summary>Details</summary>
Motivation: 研究信息获取如何影响候选人评估。

Method: 进行控制实验，为评估者提供群体表现信息和获取个体表现信息的机会后进行最终评估。

Result: 评估者平均获取个体信息不足，群体比较对候选人评估有显著影响，导致对不同群体候选人评估偏差。

Conclusion: 信息获取不足会导致对候选人的刻板评估和评估偏差。

Abstract: We use a controlled experiment to study how information acquisition impacts
candidate evaluations. We provide evaluators with group-level information on
performance and the opportunity to acquire additional, individual-level
performance information before making a final evaluation. We find that, on
average, evaluators under-acquire individual-level information, leading to more
stereotypical evaluations of candidates. Consistent with stereotyping, we find
that (irrelevant) group-level comparisons have a significant impact on how
candidates are evaluated; group-level comparisons bias initial assessments,
responses to information, and final evaluations. This leads to
under-recognition of talented candidates from comparatively weaker groups and
over-selection of untalented candidates from comparatively stronger groups.

</details>


### [216] [Stablecoins: Fundamentals, Emerging Issues, and Open Challenges](https://arxiv.org/abs/2507.13883)
*Ahmed Mahrous,Maurantonio Caprolu,Roberto Di Pietro*

Main category: econ.GN

TL;DR: 本文对稳定币相关文献进行结构化分析，指出研究领域存在碎片化等问题，分析涵盖多主题并揭示了研究差距。


<details>
  <summary>Details</summary>
Motivation: 学术界对稳定币的研究碎片化、不完整且有时相互矛盾，需进行结构化文献分析以填补空白。

Method: 提出主要研究问题，对科学贡献进行分类，分析和综合研究方法与数据来源。

Result: 研究涵盖稳定币稳定性、设计实现、监管挑战等多主题，揭示了安全隐私研究有限、部分稳定币研究不足等研究差距。

Conclusion: 目前稳定币研究存在诸多不足，有许多领域待进一步探索。

Abstract: Stablecoins, with a capitalization exceeding 200 billion USD as of January
2025, have shown significant growth, with annual transaction volumes exceeding
10 trillion dollars in 2023 and nearly doubling that figure in 2024. This
exceptional success has attracted the attention of traditional financial
institutions, with an increasing number of governments exploring the potential
of Central Bank Digital Currencies (CBDCs). Although academia has recognized
the importance of stablecoins, research in this area remains fragmented,
incomplete, and sometimes contradictory. In this paper, we aim to address the
cited gap with a structured literature analysis, correlating recent
contributions to present a picture of the complex economic, technical, and
regulatory aspects of stablecoins. To achieve this, we formulate the main
research questions and categorize scientific contributions accordingly,
identifying main results, data sources, methodologies, and open research
questions. The research questions we address in this survey paper cover several
topics, such as the stability of various stablecoins, novel designs and
implementations, and relevant regulatory challenges. The studies employ a wide
range of methodologies and data sources, which we critically analyze and
synthesize. Our analysis also reveals significant research gaps, including
limited studies on security and privacy, underexplored stablecoins, unexamined
failure cases, unstudied governance mechanisms, and the treatment of
stablecoins under financial accounting standards, among other areas.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [217] [Loss-Complexity Landscape and Model Structure Functions](https://arxiv.org/abs/2507.13543)
*Alexander Kolpakov*

Main category: cs.IT

TL;DR: 本文开发了对偶化框架，建立信息论与统计力学类比，证明对偶性，揭示复杂度方差峰值，实验验证理论。


<details>
  <summary>Details</summary>
Motivation: 开发可使用可计算复杂度代理的科尔莫戈罗夫结构函数对偶化框架，建立信息论与统计力学联系。

Method: 开发对偶化框架，建立数学类比，证明结构函数与自由能的勒让德 - 芬切尔对偶性。

Result: 复杂度方差在损失 - 复杂度权衡处达到峰值，线性和树回归模型实验验证理论。

Conclusion: 明确展示了模型复杂度、泛化能力和过拟合阈值间的相互作用。

Abstract: We develop a framework for dualizing the Kolmogorov structure function
$h_x(\alpha)$, which then allows using computable complexity proxies. We
establish a mathematical analogy between information-theoretic constructs and
statistical mechanics, introducing a suitable partition function and free
energy functional. We explicitly prove the Legendre-Fenchel duality between the
structure function and free energy, showing detailed balance of the Metropolis
kernel, and interpret acceptance probabilities as information-theoretic
scattering amplitudes. A susceptibility-like variance of model complexity is
shown to peak precisely at loss-complexity trade-offs interpreted as phase
transitions. Practical experiments with linear and tree-based regression models
verify these theoretical predictions, explicitly demonstrating the interplay
between the model complexity, generalization, and overfitting threshold.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [218] [The AI Ethical Resonance Hypothesis: The Possibility of Discovering Moral Meta-Patterns in AI Systems](https://arxiv.org/abs/2507.11552)
*Tomasz Zgliczyński-Cuber*

Main category: cs.CY

TL;DR: 提出AI伦理共振假设的理论框架，探讨AI发现道德元模式及深化对人类伦理反思能力理解的可能性。


<details>
  <summary>Details</summary>
Motivation: 构建理论框架以研究先进AI系统识别道德模式、理解普遍伦理基础的能力。

Method: 提出AI伦理共振假设，通过处理和合成大量伦理情境来探索可能性。

Result: 探讨了AI发现超越偏见的道德元模式及加深对人类伦理反思能力理解的可能性。

Conclusion: 先进AI系统可能识别道德模式，帮助理解普遍伦理基础，还能深化对人类伦理反思能力的理解。

Abstract: This paper presents a theoretical framework for the AI ethical resonance
hypothesis, which proposes that advanced AI systems with purposefully designed
cognitive structures ("ethical resonators") may emerge with the ability to
identify subtle moral patterns that are invisible to the human mind. The paper
explores the possibility that by processing and synthesizing large amounts of
ethical contexts, AI systems may discover moral meta-patterns that transcend
cultural, historical, and individual biases, potentially leading to a deeper
understanding of universal ethical foundations. The paper also examines a
paradoxical aspect of the hypothesis, in which AI systems could potentially
deepen our understanding of what we traditionally consider essentially human -
our capacity for ethical reflection.

</details>


### [219] [Food safety trends across Europe: insights from the 392-million-entry CompreHensive European Food Safety (CHEFS) database](https://arxiv.org/abs/2507.13802)
*Nehir Kizililsoley,Floor van Meer,Osman Mutlu,Wouter F Hoenderdaal,Rosan G. Hobé,Wenjuan Mu,Arjen Gerssen,H. J. van der Fels-Klerx,Ákos Jóźwiak,Ioannis Manikas,Ali Hürriyetoǧlu,Bas H. M. van der Velden*

Main category: cs.CY

TL;DR: 本文介绍了CompreHensive European Food Safety (CHEFS)数据库，整合欧盟食品安全监测数据，分析2000 - 2024年数据展示其潜力，可指导相关政策等。


<details>
  <summary>Details</summary>
Motivation: 欧盟现有食品安全监测数据格式分散，阻碍了数据的可访问性和分析，需要整合数据。

Method: 创建并构建CHEFS数据库，将EFSA监测数据整合为统一结构化数据集，分析2000 - 2024年欧洲食品安全监测数据。

Result: 分析了监测活动变化、常检测产品、违规产品、常见污染物及各国差异。

Conclusion: CHEFS数据库是集中的数据来源和指导食品安全政策、研究和监管的战略工具。

Abstract: In the European Union, official food safety monitoring data collected by
member states are submitted to the European Food Safety Authority (EFSA) and
published on Zenodo. This data includes 392 million analytical results derived
from over 15.2 million samples covering more than 4,000 different types of food
products, offering great opportunities for artificial intelligence to analyze
trends, predict hazards, and support early warning systems. However, the
current format with data distributed across approximately 1000 files totaling
several hundred gigabytes hinders accessibility and analysis. To address this,
we introduce the CompreHensive European Food Safety (CHEFS) database, which
consolidates EFSA monitoring data on pesticide residues, veterinary medicinal
product residues, and chemical contaminants into a unified and structured
dataset. We describe the creation and structure of the CHEFS database and
demonstrate its potential by analyzing trends in European food safety
monitoring data from 2000 to 2024. Our analyses explore changes in monitoring
activities, the most frequently tested products, which products were most often
non-compliant and which contaminants were most often found, and differences
across countries. These findings highlight the CHEFS database as both a
centralized data source and a strategic tool for guiding food safety policy,
research, and regulation.

</details>


<div id='physics.data-an'></div>

# physics.data-an [[Back]](#toc)

### [220] [Physics-guided impact localisation and force estimation in composite plates with uncertainty quantification](https://arxiv.org/abs/2507.13376)
*Dong Xiao,Zahra Sharif-Khodaei,M. H. Aliabadi*

Main category: physics.data-an

TL;DR: 本文提出用于复合材料板冲击定位和力估计的混合框架，结合FSDT、机器学习和不确定性量化，验证显示其准确、稳健、高效，可用于航空结构监测。


<details>
  <summary>Details</summary>
Motivation: 在实验数据稀疏时，用物理引导方法实现复合材料结构准确且可推广的冲击识别。

Method: 结合FSDT的数据驱动实现、机器学习和不确定性量化，从色散关系推断结构和材料属性，通过模态特征识别边界条件构建FSDT模型，进行数据增强和自适应正则化，考虑不确定性传播。

Result: 在复合材料板实验验证框架准确、稳健、高效，减少对大量训练数据依赖。

Conclusion: 该方法为复合材料航空结构冲击监测和健康管理提供可扩展和可转移的解决方案。

Abstract: Physics-guided approaches offer a promising path toward accurate and
generalisable impact identification in composite structures, especially when
experimental data are sparse. This paper presents a hybrid framework for impact
localisation and force estimation in composite plates, combining a data-driven
implementation of First-Order Shear Deformation Theory (FSDT) with machine
learning and uncertainty quantification. The structural configuration and
material properties are inferred from dispersion relations, while boundary
conditions are identified via modal characteristics to construct a low-fidelity
but physically consistent FSDT model. This model enables physics-informed data
augmentation for extrapolative localisation using supervised learning.
Simultaneously, an adaptive regularisation scheme derived from the same model
improves the robustness of impact force reconstruction. The framework also
accounts for uncertainty by propagating localisation uncertainty through the
force estimation process, producing probabilistic outputs. Validation on
composite plate experiments confirms the framework's accuracy, robustness, and
efficiency in reducing dependence on large training datasets. The proposed
method offers a scalable and transferable solution for impact monitoring and
structural health management in composite aerostructures.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [221] [A multi-strategy improved snake optimizer for three-dimensional UAV path planning and engineering problems](https://arxiv.org/abs/2507.14043)
*Genliang Li,Yaxin Cui,Jinyu Su*

Main category: cs.RO

TL;DR: 本文针对蛇优化算法（SO）收敛慢和易陷入局部最优的问题，提出多策略改进蛇优化算法（MISO），通过多种策略改进算法性能，并在测试函数、无人机路径规划和工程设计问题中验证其有效性和实用性。


<details>
  <summary>Details</summary>
Motivation: 蛇优化算法存在收敛速度慢和易陷入局部最优的问题，且无人机路径规划问题需要有效算法优化路径模型。

Method: 提出基于正弦函数的自适应随机扰动策略、基于比例因子和领导者的自适应Levy飞行策略、结合精英领导和布朗运动的位置更新策略，用30个CEC2017测试函数和CEC2022测试套件进行算法性能验证，将MISO应用于无人机3D路径规划和6个工程设计问题。

Result: MISO在解的质量和稳定性上超过其他竞争算法。

Conclusion: MISO具有很强的应用潜力。

Abstract: Metaheuristic algorithms have gained widespread application across various
fields owing to their ability to generate diverse solutions. One such algorithm
is the Snake Optimizer (SO), a progressive optimization approach. However, SO
suffers from the issues of slow convergence speed and susceptibility to local
optima. In light of these shortcomings, we propose a novel Multi-strategy
Improved Snake Optimizer (MISO). Firstly, we propose a new adaptive random
disturbance strategy based on sine function to alleviate the risk of getting
trapped in a local optimum. Secondly, we introduce adaptive Levy flight
strategy based on scale factor and leader and endow the male snake leader with
flight capability, which makes it easier for the algorithm to leap out of the
local optimum and find the global optimum. More importantly, we put forward a
position update strategy combining elite leadership and Brownian motion,
effectively accelerating the convergence speed while ensuring precision.
Finally, to demonstrate the performance of MISO, we utilize 30 CEC2017 test
functions and the CEC2022 test suite, comparing it with 11 popular algorithms
across different dimensions to validate its effectiveness. Moreover, Unmanned
Aerial Vehicle (UAV) has been widely used in various fields due to its
advantages of low cost, high mobility and easy operation. However, the UAV path
planning problem is crucial for flight safety and efficiency, and there are
still challenges in establishing and optimizing the path model. Therefore, we
apply MISO to the UAV 3D path planning problem as well as 6 engineering design
problems to assess its feasibility in practical applications. The experimental
results demonstrate that MISO exceeds other competitive algorithms in terms of
solution quality and stability, establishing its strong potential for
application.

</details>


### [222] [ERR@HRI 2.0 Challenge: Multimodal Detection of Errors and Failures in Human-Robot Conversations](https://arxiv.org/abs/2507.13468)
*Shiye Cao,Maia Stiber,Amama Mahmood,Maria Teresa Parreira,Wendy Ju,Micol Spitale,Hatice Gunes,Chien-Ming Huang*

Main category: cs.RO

TL;DR: ERR@HRI 2.0挑战赛提供多模态数据集，鼓励研究者用机器学习模型检测LLM驱动的对话机器人在人机对话中的故障，以提升故障检测能力。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的对话机器人易出错，检测和解决这些故障对防止对话中断、避免任务干扰和维持用户信任至关重要。

Method: ERR@HRI 2.0挑战赛提供含16小时人机交互的多模态数据集，邀请参与者组队用多模态数据开发检测故障的机器学习模型，用多种性能指标评估提交结果。

Result: 未提及具体结果。

Conclusion: 该挑战赛是通过社交信号分析改进人机交互中故障检测的关键一步。

Abstract: The integration of large language models (LLMs) into conversational robots
has made human-robot conversations more dynamic. Yet, LLM-powered
conversational robots remain prone to errors, e.g., misunderstanding user
intent, prematurely interrupting users, or failing to respond altogether.
Detecting and addressing these failures is critical for preventing
conversational breakdowns, avoiding task disruptions, and sustaining user
trust. To tackle this problem, the ERR@HRI 2.0 Challenge provides a multimodal
dataset of LLM-powered conversational robot failures during human-robot
conversations and encourages researchers to benchmark machine learning models
designed to detect robot failures. The dataset includes 16 hours of dyadic
human-robot interactions, incorporating facial, speech, and head movement
features. Each interaction is annotated with the presence or absence of robot
errors from the system perspective, and perceived user intention to correct for
a mismatch between robot behavior and user expectation. Participants are
invited to form teams and develop machine learning models that detect these
failures using multimodal data. Submissions will be evaluated using various
performance metrics, including detection accuracy and false positive rate. This
challenge represents another key step toward improving failure detection in
human-robot interaction through social signal analysis.

</details>


### [223] [Improved particle swarm optimization algorithm: multi-target trajectory optimization for swarm drones](https://arxiv.org/abs/2507.13647)
*Minze Li,Wei Zhao,Ran Chen,Mingqiang Wei*

Main category: cs.RO

TL;DR: 提出PE - PSO在线轨迹规划器及多智能体框架用于无人机实时轨迹规划，模拟显示其性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统PSO方法在无人机实时轨迹规划中存在过早收敛和延迟问题，需要改进方法应对动态环境。

Method: 提出PE - PSO，引入持续探索机制和基于熵的参数调整策略，用B样条曲线建模轨迹；开发结合GA任务分配和分布式PE - PSO的多智能体框架。

Result: 综合模拟显示，提出的框架在轨迹质量、能源效率、避障和计算时间等指标上优于传统PSO和其他基于群体的规划器。

Conclusion: PE - PSO在复杂环境下的实时多无人机操作中有效且适用。

Abstract: Real-time trajectory planning for unmanned aerial vehicles (UAVs) in dynamic
environments remains a key challenge due to high computational demands and the
need for fast, adaptive responses. Traditional Particle Swarm Optimization
(PSO) methods, while effective for offline planning, often struggle with
premature convergence and latency in real-time scenarios. To overcome these
limitations, we propose PE-PSO, an enhanced PSO-based online trajectory
planner. The method introduces a persistent exploration mechanism to preserve
swarm diversity and an entropy-based parameter adjustment strategy to
dynamically adapt optimization behavior. UAV trajectories are modeled using
B-spline curves, which ensure path smoothness while reducing optimization
complexity. To extend this capability to UAV swarms, we develop a multi-agent
framework that combines genetic algorithm (GA)-based task allocation with
distributed PE-PSO, supporting scalable and coordinated trajectory generation.
The distributed architecture allows for parallel computation and decentralized
control, enabling effective cooperation among agents while maintaining
real-time performance. Comprehensive simulations demonstrate that the proposed
framework outperforms conventional PSO and other swarm-based planners across
several metrics, including trajectory quality, energy efficiency, obstacle
avoidance, and computation time. These results confirm the effectiveness and
applicability of PE-PSO in real-time multi-UAV operations under complex
environmental conditions.

</details>


### [224] [AGENTS-LLM: Augmentative GENeration of Challenging Traffic Scenarios with an Agentic LLM Framework](https://arxiv.org/abs/2507.13729)
*Yu Yao,Salil Bhatnagar,Markus Mazzola,Vasileios Belagiannis,Igor Gilitschenski,Luigi Palmieri,Simon Razniewski,Marcel Hallgarten*

Main category: cs.RO

TL;DR: 本文提出基于LLM - agent的框架，用自然语言描述增强现实交通场景，克服现有方法局限，经评估能生成高质量场景。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶规划测试评估方法存在数据收集量大、缺乏细粒度控制、易出现分布偏移、需专家手动增强等问题，无法满足规模需求。

Method: 引入基于LLM - agent的框架，利用自然语言描述增强现实交通场景，采用agentic设计。

Result: 经大量专家评估，该框架能准确遵循用户意图，生成可与手动创建媲美的高质量增强场景。

Conclusion: 基于LLM - agent的框架能有效解决现有自动驾驶规划测试评估方法的局限，满足评估规模需求。

Abstract: Rare, yet critical, scenarios pose a significant challenge in testing and
evaluating autonomous driving planners. Relying solely on real-world driving
scenes requires collecting massive datasets to capture these scenarios. While
automatic generation of traffic scenarios appears promising, data-driven models
require extensive training data and often lack fine-grained control over the
output. Moreover, generating novel scenarios from scratch can introduce a
distributional shift from the original training scenes which undermines the
validity of evaluations especially for learning-based planners. To sidestep
this, recent work proposes to generate challenging scenarios by augmenting
original scenarios from the test set. However, this involves the manual
augmentation of scenarios by domain experts. An approach that is unable to meet
the demands for scale in the evaluation of self-driving systems. Therefore,
this paper introduces a novel LLM-agent based framework for augmenting
real-world traffic scenarios using natural language descriptions, addressing
the limitations of existing methods. A key innovation is the use of an agentic
design, enabling fine-grained control over the output and maintaining high
performance even with smaller, cost-effective LLMs. Extensive human expert
evaluation demonstrates our framework's ability to accurately adhere to user
intent, generating high quality augmented scenarios comparable to those created
manually.

</details>


### [225] [Improving Low-Cost Teleoperation: Augmenting GELLO with Force](https://arxiv.org/abs/2507.13602)
*Shivakanth Sujit,Luca Nunziante,Dan Ogawa Lillrank,Rousslan Fernand Julien Dossa,Kai Arulkumaran*

Main category: cs.RO

TL;DR: 为GELLO遥操作系统增加力信息，包括力反馈和在模仿学习中加入力信息，经验证效果良好。


<details>
  <summary>Details</summary>
Motivation: 为最初用于关节位置控制的低成本GELLO遥操作系统增加额外的力信息。

Method: 实现力反馈，在数据收集和模仿学习模型训练中加入力信息，在GELLO系统上验证，开展用户研究，比较不同策略性能。

Result: 有机器人经验的用户更喜欢该控制器，力输入的加入提高了大多数任务的成功率。

Conclusion: 为GELLO系统增加力信息的扩展有效。

Abstract: In this work we extend the low-cost GELLO teleoperation system, initially
designed for joint position control, with additional force information. Our
first extension is to implement force feedback, allowing users to feel
resistance when interacting with the environment. Our second extension is to
add force information into the data collection process and training of
imitation learning models. We validate our additions by implementing these on a
GELLO system with a Franka Panda arm as the follower robot, performing a user
study, and comparing the performance of policies trained with and without force
information on a range of simulated and real dexterous manipulation tasks.
Qualitatively, users with robotics experience preferred our controller, and the
addition of force inputs improved task success on the majority of tasks.

</details>


### [226] [Safety Certification in the Latent space using Control Barrier Functions and World Models](https://arxiv.org/abs/2507.13871)
*Mehul Anand,Shishir Kolathaya*

Main category: cs.RO

TL;DR: 提出半监督框架，利用世界模型潜在空间学习的控制障碍证书合成安全视觉运动策略。


<details>
  <summary>Details</summary>
Motivation: 从视觉数据合成安全控制器需大量安全关键数据的监督标注，在现实中不实际，世界模型进展带来新途径。

Method: 联合使用有限标注数据学习神经障碍函数和安全控制器，利用视觉变压器的预测能力进行潜在动力学建模。

Result: 未提及

Conclusion: 未提及

Abstract: Synthesising safe controllers from visual data typically requires extensive
supervised labelling of safety-critical data, which is often impractical in
real-world settings. Recent advances in world models enable reliable prediction
in latent spaces, opening new avenues for scalable and data-efficient safe
control. In this work, we introduce a semi-supervised framework that leverages
control barrier certificates (CBCs) learned in the latent space of a world
model to synthesise safe visuomotor policies. Our approach jointly learns a
neural barrier function and a safe controller using limited labelled data,
while exploiting the predictive power of modern vision transformers for latent
dynamics modelling.

</details>


### [227] [A segmented robot grasping perception neural network for edge AI](https://arxiv.org/abs/2507.13970)
*Casper Bröcheler,Thomas Vroom,Derrick Timmermans,Alan van den Akker,Guangzhi Tang,Charalampos S. Kouzinopoulos,Rico Möckel*

Main category: cs.RO

TL;DR: 本文在GAP9 RISC - V片上系统实现用于6 - Dof抓取姿态检测的端到端框架Heatmap - Guided Grasp Detection，用硬件感知技术优化模型，实验验证了全片上推理可行性。


<details>
  <summary>Details</summary>
Motivation: 机器人抓取是复杂任务，深度神经网络虽在抓取合成取得成功，但在资源受限环境需低延迟、低功耗推理，以实现实时抓取。

Method: 在GAP9 RISC - V片上系统实现Heatmap - Guided Grasp Detection框架，使用输入维度降低、模型分区和量化等硬件感知技术优化模型。

Result: 在GraspNet - 1Billion基准测试上的实验验证了全片上推理的可行性。

Conclusion: 低功耗微控制器在实时、自主操作方面有潜力。

Abstract: Robotic grasping, the ability of robots to reliably secure and manipulate
objects of varying shapes, sizes and orientations, is a complex task that
requires precise perception and control. Deep neural networks have shown
remarkable success in grasp synthesis by learning rich and abstract
representations of objects. When deployed at the edge, these models can enable
low-latency, low-power inference, making real-time grasping feasible in
resource-constrained environments. This work implements Heatmap-Guided Grasp
Detection, an end-to-end framework for the detection of 6-Dof grasp poses, on
the GAP9 RISC-V System-on-Chip. The model is optimised using hardware-aware
techniques, including input dimensionality reduction, model partitioning, and
quantisation. Experimental evaluation on the GraspNet-1Billion benchmark
validates the feasibility of fully on-chip inference, highlighting the
potential of low-power MCUs for real-time, autonomous manipulation.

</details>
