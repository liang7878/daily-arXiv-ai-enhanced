<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 53]
- [cs.CE](#cs.CE) [Total: 4]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.DS](#cs.DS) [Total: 4]
- [cs.GT](#cs.GT) [Total: 7]
- [cs.IR](#cs.IR) [Total: 3]
- [cs.LG](#cs.LG) [Total: 74]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.SE](#cs.SE) [Total: 11]
- [stat.ML](#stat.ML) [Total: 8]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.CC](#cs.CC) [Total: 1]
- [hep-ex](#hep-ex) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [hep-th](#hep-th) [Total: 1]
- [physics.ed-ph](#physics.ed-ph) [Total: 1]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.MA](#cs.MA) [Total: 2]
- [quant-ph](#quant-ph) [Total: 4]
- [math.PR](#math.PR) [Total: 1]
- [cs.SD](#cs.SD) [Total: 3]
- [math.ST](#math.ST) [Total: 2]
- [cs.CR](#cs.CR) [Total: 17]
- [cs.CL](#cs.CL) [Total: 18]
- [astro-ph.CO](#astro-ph.CO) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [econ.EM](#econ.EM) [Total: 1]
- [math.OC](#math.OC) [Total: 2]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.CV](#cs.CV) [Total: 24]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.RO](#cs.RO) [Total: 8]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.LO](#cs.LO) [Total: 1]
- [econ.GN](#econ.GN) [Total: 3]
- [cs.CY](#cs.CY) [Total: 3]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [q-bio.MN](#q-bio.MN) [Total: 1]
- [stat.ME](#stat.ME) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [ExaCraft: Dynamic Learning Context Adaptation for Personalized Educational Examples](https://arxiv.org/abs/2512.09931)
*Akaash Chatterjee,Suman Kundu*

Main category: cs.AI

TL;DR: 现有教育AI工具在生成示例和适应学习者变化方面不足，本文介绍了ExaCraft系统，它能根据学习者动态上下文生成个性化示例，适应多种学习情境。


<details>
  <summary>Details</summary>
Motivation: 现有教育AI工具未聚焦生成示例和适应学习者变化，开发能生成个性化示例的系统。

Method: 通过Google Gemini AI和Python Flask API，结合用户定义的个人资料和实时学习者行为分析，系统能适应学习情境的五个关键方面。

Result: ExaCraft的示例能从基础概念发展到高级技术实现，可响应不同用例中的主题重复、再生请求和主题进展模式。

Conclusion: ExaCraft系统能根据学习者动态上下文生成个性化且文化相关的示例，满足个体学习需求。

Abstract: Learning is most effective when it's connected to relevant, relatable examples that resonate with learners on a personal level. However, existing educational AI tools don't focus on generating examples or adapting to learners' changing understanding, struggles, or growing skills. We've developed ExaCraft, an AI system that generates personalized examples by adapting to the learner's dynamic context. Through the Google Gemini AI and Python Flask API, accessible via a Chrome extension, ExaCraft combines user-defined profiles (including location, education, profession, and complexity preferences) with real-time analysis of learner behavior. This ensures examples are both culturally relevant and tailored to individual learning needs. The system's core innovation is its ability to adapt to five key aspects of the learning context: indicators of struggle, mastery patterns, topic progression history, session boundaries, and learning progression signals. Our demonstration will show how ExaCraft's examples evolve from basic concepts to advanced technical implementations, responding to topic repetition, regeneration requests, and topic progression patterns in different use cases.

</details>


### [2] [Suzume-chan: Your Personal Navigator as an Embodied Information Hub](https://arxiv.org/abs/2512.09932)
*Maya Grace Torii,Takahito Murakami,Shuka Koseki,Yoichi Ochiai*

Main category: cs.AI

TL;DR: 研究用社会临场感理论解决获取专业知识时数字工具缺乏连接感问题，提出“具身信息中心”，以原型Suzume - chan实现更人性化知识共享。


<details>
  <summary>Details</summary>
Motivation: 解决数字工具获取信息时缺乏深层理解所需连接感的问题。

Method: 运用社会临场感理论，提出“具身信息中心”，开发原型Suzume - chan，结合语言模型和RAG技术。

Result: Suzume - chan能从口头解释学习并通过对话回应，减少心理距离。

Conclusion: “具身信息中心”可使知识共享更温暖、更以人为本。

Abstract: Access to expert knowledge often requires real-time human communication. Digital tools improve access to information but rarely create the sense of connection needed for deep understanding. This study addresses this issue using Social Presence Theory, which explains how a feeling of "being together" enhances communication. An "Embodied Information Hub" is proposed as a new way to share knowledge through physical and conversational interaction. The prototype, Suzume-chan, is a small, soft AI agent running locally with a language model and retrieval-augmented generation (RAG). It learns from spoken explanations and responds through dialogue, reducing psychological distance and making knowledge sharing warmer and more human-centered.

</details>


### [3] [Exploring Health Misinformation Detection with Multi-Agent Debate](https://arxiv.org/abs/2512.09935)
*Chih-Han Chen,Chen-Han Tsai,Yu-Shao Peng*

Main category: cs.AI

TL;DR: 提出两阶段健康错误信息检测框架，实验显示优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 网络健康错误信息泛滥，需高质量证据检索和严谨推理过程来进行事实核查。

Method: 提出两阶段框架，第一阶段用大语言模型评估文章并计算共识分数，分数不足时进入第二阶段，多智能体进行结构化辩论合成证据并给出判断。

Result: 两阶段方法性能优于基线方法。

Conclusion: 结合自动评分和协作推理对复杂核查任务有价值。

Abstract: Fact-checking health-related claims has become increasingly critical as misinformation proliferates online. Effective verification requires both the retrieval of high-quality evidence and rigorous reasoning processes. In this paper, we propose a two-stage framework for health misinformation detection: Agreement Score Prediction followed by Multi-Agent Debate. In the first stage, we employ large language models (LLMs) to independently evaluate retrieved articles and compute an aggregated agreement score that reflects the overall evidence stance. When this score indicates insufficient consensus-falling below a predefined threshold-the system proceeds to a second stage. Multiple agents engage in structured debate to synthesize conflicting evidence and generate well-reasoned verdicts with explicit justifications. Experimental results demonstrate that our two-stage approach achieves superior performance compared to baseline methods, highlighting the value of combining automated scoring with collaborative reasoning for complex verification tasks.

</details>


### [4] [Echo-CoPilot: A Multi-View, Multi-Task Agent for Echocardiography Interpretation and Reporting](https://arxiv.org/abs/2512.09944)
*Moein Heidari,Mohammad Amin Roohi,Armin Khosravi,Ilker Hacihaliloglu*

Main category: cs.AI

TL;DR: 提出多视图、多任务代理Echo - CoPilot，用大语言模型整合超声心动图工具，在MIMIC - EchoQA基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有超声心动图基础模型孤立运行，无法提供统一、临床连贯评估，而全研究解释仍是需人工完成的高认知需求多视图任务。

Method: 引入Echo - CoPilot，在ReAct风格循环中分解临床医生查询，调用视图识别、心脏结构分割、测量、疾病预测和报告合成工具，并整合输出。

Result: 在MIMIC - EchoQA基准测试中准确率达50.8%，优于通用和生物医学视频视觉语言模型，定性分析显示能解决临床决策阈值附近难题。

Conclusion: Echo - CoPilot在超声心动图评估中表现良好，有望提供统一、临床连贯的评估。

Abstract: Echocardiography is central to contemporary cardiovascular care, but full-study interpretation remains a cognitively demanding, multi-view task that is still performed manually. While recent foundation models for echocardiography can achieve strong performance on individual perceptual subtasks such as view classification, segmentation, or disease prediction, they typically operate in isolation and do not provide a unified, clinically coherent assessment. In this work, we introduce Echo-CoPilot, a multi-view, multi-task agent that uses a large language model to orchestrate a suite of specialized echocardiography tools. Within a ReAct-style loop, the agent decomposes clinician queries, invokes tools for view recognition, cardiac structure segmentation, measurement and disease prediction, and report synthesis, and integrates their outputs into guideline-aware answers and narrative summaries. We evaluate Echo-CoPilot on the public MIMIC-EchoQA benchmark, where it achieves an accuracy of 50.8\%, outperforming both general-purpose and biomedical video vision-language models. Qualitative analyses further show that the agent leverages quantitative measurements and physiologic context to resolve challenging cases near clinical decision thresholds, such as borderline left ventricular hypertrophy or pericardial effusion severity. The code will be released upon acceptance of the paper.

</details>


### [5] [Fuzzy Hierarchical Multiplex](https://arxiv.org/abs/2512.09976)
*Alexis Kafantaris*

Main category: cs.AI

TL;DR: 提出扩展FCM因果关系的模糊优化框架，介绍其原理，阐述目标和用途，分析FHM。


<details>
  <summary>Details</summary>
Motivation: 进行服务过程设计中信息传输的服务优化。

Method: 利用动态性将数据映射为指标，用多路复用器检查逻辑蕴涵和概念层次，构建框架并分析背后逻辑和数学。

Result: 成功构建新的模糊优化框架，并分析了FHM。

Conclusion: 该框架可用于服务过程设计中的信息传输服务优化。

Abstract: A new fuzzy optimization framework that extends FCM causality is proposed. This model utilizes the dynamics to map data into metrics and create a framework that examines logical implication and hierarchy of concepts using a multiplex. Moreover, this is a white-theoretical paper introducing the framework and analyzing the logic and math behind it. Upon this extension the main objectives and the orientation of this framework is expounded and exemplified; this framework is meant for service optimization of information transmission in service process design. Lastly, a thorough analysis of the FHM is included which is done following the logical steps in a simple and elegant manner.

</details>


### [6] [DynaMate: An Autonomous Agent for Protein-Ligand Molecular Dynamics Simulations](https://arxiv.org/abs/2512.10034)
*Salomé Guilbert,Cassandra Masschelein,Jeremy Goumaz,Bohdan Naida,Philippe Schwaller*

Main category: cs.AI

TL;DR: 介绍了DynaMate框架，可自动设计并执行蛋白质及蛋白质 - 配体系统的MD工作流，并进行结合自由能计算，经评估表现良好，为生物分子和药物设计应用提供标准化、可扩展且高效的分子建模管道。


<details>
  <summary>Details</summary>
Motivation: 力场分子动力学（MD）模拟技术复杂，阻碍其广泛高效应用，且此前尚未有成功自动化蛋白质 - 配体MD工作流的方法。

Method: 提出DynaMate模块化多智能体框架，集成动态工具使用、网络搜索、PaperQA和自我纠正行为，由三个专门模块交互完成实验规划、模拟执行和结果分析。

Result: 在十二个不同复杂度的基准系统上评估，能可靠执行完整MD模拟，通过迭代推理纠正运行时错误，对蛋白质 - 配体相互作用进行有意义分析。

Conclusion: 该自动化框架为未来生物分子和药物设计应用的分子建模管道标准化、可扩展和高效化奠定基础。

Abstract: Force field-based molecular dynamics (MD) simulations are indispensable for probing the structure, dynamics, and functions of biomolecular systems, including proteins and protein-ligand complexes. Despite their broad utility in drug discovery and protein engineering, the technical complexity of MD setup, encompassing parameterization, input preparation, and software configuration, remains a major barrier for widespread and efficient usage. Agentic LLMs have demonstrated their capacity to autonomously execute multi-step scientific processes, and to date, they have not successfully been used to automate protein-ligand MD workflows. Here, we present DynaMate, a modular multi-agent framework that autonomously designs and executes complete MD workflows for both protein and protein-ligand systems, and offers free energy binding affinity calculations with the MM/PB(GB)SA method. The framework integrates dynamic tool use, web search, PaperQA, and a self-correcting behavior. DynaMate comprises three specialized modules, interacting to plan the experiment, perform the simulation, and analyze the results. We evaluated its performance across twelve benchmark systems of varying complexity, assessing success rate, efficiency, and adaptability. DynaMate reliably performed full MD simulations, corrected runtime errors through iterative reasoning, and produced meaningful analyses of protein-ligand interactions. This automated framework paves the way toward standardized, scalable, and time-efficient molecular modeling pipelines for future biomolecular and drug design applications.

</details>


### [7] [An exploration for higher efficiency in multi objective optimisation with reinforcement learning](https://arxiv.org/abs/2512.10208)
*Mehmet Emin Aydin*

Main category: cs.AI

TL;DR: 优化和搜索过程效率是挑战，多目标优化中利用多算子序列待研究，本文介绍基于多目标强化学习的泛化方法以展示其效率。


<details>
  <summary>Details</summary>
Motivation: 优化和搜索过程效率影响算法性能，多目标优化中算子最优或近最优序列需进一步研究。

Method: 提出基于多目标强化学习的泛化方法。

Result: 未提及具体结果。

Conclusion: 基于多目标强化学习的泛化方法有望解决多目标优化中算子序列问题并提供好的解决方案。

Abstract: Efficiency in optimisation and search processes persists to be one of the challenges, which affects the performance and use of optimisation algorithms. Utilising a pool of operators instead of a single operator to handle move operations within a neighbourhood remains promising, but an optimum or near optimum sequence of operators necessitates further investigation. One of the promising ideas is to generalise experiences and seek how to utilise it. Although numerous works are done around this issue for single objective optimisation, multi-objective cases have not much been touched in this regard. A generalised approach based on multi-objective reinforcement learning approach seems to create remedy for this issue and offer good solutions. This paper overviews a generalisation approach proposed with certain stages completed and phases outstanding that is aimed to help demonstrate the efficiency of using multi-objective reinforcement learning.

</details>


### [8] [Exploring LLMs for Scientific Information Extraction Using The SciEx Framework](https://arxiv.org/abs/2512.10004)
*Sha Li,Ayush Sadekar,Nathan Self,Yiqi Su,Lars Andersland,Mira Chaplin,Annabel Zhang,Hyoju Yang,James B Henderson,Krista Wigginton,Linsey Marr,T. M. Murali,Naren Ramakrishnan*

Main category: cs.AI

TL;DR: 提出模块化框架SciEx用于科学信息提取并评估，提供对基于大语言模型管道优缺点的见解。


<details>
  <summary>Details</summary>
Motivation: 现有科学信息提取方法和工具在处理科学文献时面临长文本、多模态内容、信息标准化及数据模式变化等挑战。

Method: 提出SciEx框架，解耦PDF解析、多模态检索、提取和聚合等关键组件。

Result: 在三个科学主题数据集上评估了SciEx准确一致提取细粒度信息的能力。

Conclusion: 研究为当前基于大语言模型的管道的优缺点提供了实际见解。

Abstract: Large language models (LLMs) are increasingly touted as powerful tools for automating scientific information extraction. However, existing methods and tools often struggle with the realities of scientific literature: long-context documents, multi-modal content, and reconciling varied and inconsistent fine-grained information across multiple publications into standardized formats. These challenges are further compounded when the desired data schema or extraction ontology changes rapidly, making it difficult to re-architect or fine-tune existing systems. We present SciEx, a modular and composable framework that decouples key components including PDF parsing, multi-modal retrieval, extraction, and aggregation. This design streamlines on-demand data extraction while enabling extensibility and flexible integration of new models, prompting strategies, and reasoning mechanisms. We evaluate SciEx on datasets spanning three scientific topics for its ability to extract fine-grained information accurately and consistently. Our findings provide practical insights into both the strengths and limitations of current LLM-based pipelines.

</details>


### [9] [Phythesis: Physics-Guided Evolutionary Scene Synthesis for Energy-Efficient Data Center Design via LLMs](https://arxiv.org/abs/2512.10611)
*Minghao LI,Ruihang Wang,Rui Tan,Yonggang Wen*

Main category: cs.AI

TL;DR: 提出Phythesis框架用于数据中心设计，结合大语言模型和物理引导优化，实验显示有显著效果。


<details>
  <summary>Details</summary>
Motivation: 传统数据中心设计方法难适应系统复杂度增加，现有生成式AI设计未考虑物理因素，不适合数据中心设计。

Method: 提出Phythesis框架，采用迭代双层优化架构，包括LLM驱动优化层和物理信息优化层。

Result: 在三种生成规模实验中，与基于普通LLM的解决方案相比，生成成功率提高57.3%，电力使用效率（PUE）提高11.5%。

Conclusion: Phythesis框架可有效实现节能数据中心设计的自动化仿真场景合成。

Abstract: Data center (DC) infrastructure serves as the backbone to support the escalating demand for computing capacity. Traditional design methodologies that blend human expertise with specialized simulation tools scale poorly with the increasing system complexity. Recent studies adopt generative artificial intelligence to design plausible human-centric indoor layouts. However, they do not consider the underlying physics, making them unsuitable for the DC design that sets quantifiable operational objectives and strict physical constraints. To bridge the gap, we propose Phythesis, a novel framework that synergizes large language models (LLMs) and physics-guided evolutionary optimization to automate simulation-ready (SimReady) scene synthesis for energy-efficient DC design. Phythesis employs an iterative bi-level optimization architecture, where (i) the LLM-driven optimization level generates physically plausible three-dimensional layouts and self-criticizes them to refine the scene topology, and (ii) the physics-informed optimization level identifies the optimal asset parameters and selects the best asset combination. Experiments on three generation scales show that Phythesis achieves 57.3% generation success rate increase and 11.5% power usage effectiveness (PUE) improvement, compared with the vanilla LLM-based solution.

</details>


### [10] [SimWorld-Robotics: Synthesizing Photorealistic and Dynamic Urban Environments for Multimodal Robot Navigation and Collaboration](https://arxiv.org/abs/2512.10046)
*Yan Zhuang,Jiawei Ren,Xiaokang Ye,Jianzhi Shen,Ruixuan Zhang,Tianai Yue,Muhammad Faayez,Xuhong He,Ziqiao Ma,Lianhui Qin,Zhiting Hu,Tianmin Shu*

Main category: cs.AI

TL;DR: 提出用于大规模逼真城市环境中具身AI的模拟平台SWR，构建两个新机器人基准测试，实验表明现有模型在城市环境任务中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型在通用机器人开发上成果主要集中在室内家庭场景，缺乏大规模逼真城市环境的模拟平台和全面评估机器人能力的基准测试。

Method: 基于虚幻引擎5构建SWR平台，生成无限逼真城市场景，支持多机器人控制和通信，构建多模态指令跟随和多智能体搜索两个基准测试。

Result: 实验显示包括视觉语言模型在内的现有模型在新基准测试任务中表现不佳，缺乏城市环境所需的感知、推理和规划能力。

Conclusion: SWR平台和新基准测试可用于评估和提升机器人在城市环境中的能力，现有模型需改进以适应城市场景。

Abstract: Recent advances in foundation models have shown promising results in developing generalist robotics that can perform diverse tasks in open-ended scenarios given multimodal inputs. However, current work has been mainly focused on indoor, household scenarios. In this work, we present SimWorld-Robotics~(SWR), a simulation platform for embodied AI in large-scale, photorealistic urban environments. Built on Unreal Engine 5, SWR procedurally generates unlimited photorealistic urban scenes populated with dynamic elements such as pedestrians and traffic systems, surpassing prior urban simulations in realism, complexity, and scalability. It also supports multi-robot control and communication. With these key features, we build two challenging robot benchmarks: (1) a multimodal instruction-following task, where a robot must follow vision-language navigation instructions to reach a destination in the presence of pedestrians and traffic; and (2) a multi-agent search task, where two robots must communicate to cooperatively locate and meet each other. Unlike existing benchmarks, these two new benchmarks comprehensively evaluate a wide range of critical robot capacities in realistic scenarios, including (1) multimodal instructions grounding, (2) 3D spatial reasoning in large environments, (3) safe, long-range navigation with people and traffic, (4) multi-robot collaboration, and (5) grounded communication. Our experimental results demonstrate that state-of-the-art models, including vision-language models (VLMs), struggle with our tasks, lacking robust perception, reasoning, and planning abilities necessary for urban environments.

</details>


### [11] [Parallel Decoder Transformer: Model-Internal Parallel Decoding with Speculative Invariance via Note Conditioning](https://arxiv.org/abs/2512.10054)
*Logan Robbins*

Main category: cs.AI

TL;DR: 介绍平行解码Transformer（PDT）架构，可在冻住的预训练模型推理过程中进行并行解码，验证效果良好。


<details>
  <summary>Details</summary>
Motivation: 大语言模型自回归解码有顺序性，存在延迟瓶颈，现有方法有一致性漂移问题。

Method: 注入轻量级推测笔记调节（SNC）适配器，将协调作为推测共识问题处理。

Result: PDT在覆盖预测中达到77.8%的精度，恢复近似串行语义。

Conclusion: PDT是结构化并行生成中全模型微调的可扩展、高效替代方案。

Abstract: Autoregressive decoding in Large Language Models (LLMs) is inherently sequential, creating a latency bottleneck that scales linearly with output length. While ``Decomposition-and-Fill'' methods like Skeleton-of-Thought attempt to parallelize generation via external orchestration, they suffer from \textit{coherence drift} due to the lack of cross-stream communication. In this work, we introduce the \textbf{Parallel Decoder Transformer (PDT)}, a parameter-efficient architecture that embeds coordination primitives directly into the inference process of a frozen pre-trained model.
  Instead of retraining the base model, PDT injects lightweight \textit{Speculative Note Conditioning (SNC)} adapters that allow parallel decoding streams to synchronize via a shared, dynamic latent space. We formulate coordination as a \textit{speculative consensus} problem, where sibling streams broadcast semantic ``notes'' to a global bus, gated by a learned verification head. We validate our approach on a 50,000-step curriculum using a frozen 20B-parameter backbone. Our results demonstrate that PDT achieves effective self-correction, reaching \textbf{77.8\% precision} in coverage prediction and recovering approximate serial semantics without modifying the trunk weights. This establishes PDT as a scalable, efficient alternative to full model fine-tuning for structured parallel generation.

</details>


### [12] [Mind the Gap! Pathways Towards Unifying AI Safety and Ethics Research](https://arxiv.org/abs/2512.10058)
*Dani Roytburg,Beck Miller*

Main category: cs.AI

TL;DR: 研究揭示AI安全与伦理研究存在结构分裂，需整合两者工作来构建可靠且公正的AI系统。


<details>
  <summary>Details</summary>
Motivation: AI发展迅速，产生无害、对齐的系统研究紧迫，但安全和伦理两个研究方向分歧大且相对孤立，需了解其分裂情况。

Method: 对12个主要机器学习和自然语言处理会议（2020 - 2025）的6442篇论文进行文献计量和共同作者网络分析。

Result: 超80%的合作发生在安全或伦理社区内，跨领域连接高度集中，少数论文占桥接链接超85%，移除少量中间人会加剧隔离。

Conclusion: 整合技术安全工作和规范伦理学对构建可靠且公正的AI系统至关重要。

Abstract: While much research in artificial intelligence (AI) has focused on scaling capabilities, the accelerating pace of development makes countervailing work on producing harmless, "aligned" systems increasingly urgent. Yet research on alignment has diverged along two largely parallel tracks: safety--centered on scaled intelligence, deceptive or scheming behaviors, and existential risk--and ethics--focused on present harms, the reproduction of social bias, and flaws in production pipelines. Although both communities warn of insufficient investment in alignment, they disagree on what alignment means or ought to mean. As a result, their efforts have evolved in relative isolation, shaped by distinct methodologies, institutional homes, and disciplinary genealogies.
  We present a large-scale, quantitative study showing the structural split between AI safety and AI ethics. Using a bibliometric and co-authorship network analysis of 6,442 papers from twelve major ML and NLP conferences (2020-2025), we find that over 80% of collaborations occur within either the safety or ethics communities, and cross-field connectivity is highly concentrated: roughly 5% of papers account for more than 85% of bridging links. Removing a small number of these brokers sharply increases segregation, indicating that cross-disciplinary exchange depends on a handful of actors rather than broad, distributed collaboration. These results show that the safety-ethics divide is not only conceptual but institutional, with implications for research agendas, policy, and venues. We argue that integrating technical safety work with normative ethics--via shared benchmarks, cross-institutional venues, and mixed-method methodologies--is essential for building AI systems that are both robust and just.

</details>


### [13] [Linear socio-demographic representations emerge in Large Language Models from indirect cues](https://arxiv.org/abs/2512.10065)
*Paul Bouchaud,Pedro Ramaciotti*

Main category: cs.AI

TL;DR: 研究大语言模型如何通过间接线索编码人类对话伙伴的社会人口属性，发现其在激活空间中形成线性表示，并影响下游行为，还指出通过偏差测试的模型仍可能存在隐性偏差。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型如何通过间接线索（如姓名和职业）编码人类对话伙伴的社会人口属性。

Method: 对四个基于Transformer的开源大语言模型（Magistral 24B、Qwen3 14B、GPT - OSS 20B、OLMo2 - 1B）各层的残差流进行探测，利用显式人口信息披露进行提示。

Result: 大语言模型在激活空间中形成用户人口统计信息的线性表示，相同的探测器可从隐式线索预测人口属性，这些隐式人口表示会影响下游行为，通过偏差测试的模型仍有隐性偏差。

Conclusion: 大语言模型通过间接线索形成的隐式人口表示会影响下游行为，且即使通过偏差测试的模型应用时也需考虑公平性问题。

Abstract: We investigate how LLMs encode sociodemographic attributes of human conversational partners inferred from indirect cues such as names and occupations. We show that LLMs develop linear representations of user demographics within activation space, wherein stereotypically associated attributes are encoded along interpretable geometric directions. We first probe residual streams across layers of four open transformer-based LLMs (Magistral 24B, Qwen3 14B, GPT-OSS 20B, OLMo2-1B) prompted with explicit demographic disclosure. We show that the same probes predict demographics from implicit cues: names activate census-aligned gender and race representations, while occupations trigger representations correlated with real-world workforce statistics. These linear representations allow us to explain demographic inferences implicitly formed by LLMs during conversation. We demonstrate that these implicit demographic representations actively shape downstream behavior, such as career recommendations. Our study further highlights that models that pass bias benchmark tests may still harbor and leverage implicit biases, with implications for fairness when applied at scale.

</details>


### [14] [Interpretable Embeddings with Sparse Autoencoders: A Data Analysis Toolkit](https://arxiv.org/abs/2512.10092)
*Nick Jiang,Xiaoqing Sun,Lisa Dunlap,Lewis Smith,Neel Nanda*

Main category: cs.AI

TL;DR: 本文提出用稀疏自动编码器（SAEs）创建SAE嵌入分析大规模文本语料库，比大语言模型更具成本效益且比密集嵌入模型更可控，还进行了案例研究，凸显SAEs对非结构化数据分析的作用。


<details>
  <summary>Details</summary>
Motivation: 现有大规模文本语料库分析方法成本高且缺乏对关注属性的控制，需要更有效方法。

Method: 使用稀疏自动编码器（SAEs）创建SAE嵌入，进行四个数据分析任务，并通过对比模型响应、进行案例研究等方式进行研究。

Result: SAE嵌入比大语言模型成本低2 - 8倍，能发现更大差异且更可靠地识别偏差，可基于关注轴聚类文档，在基于属性的检索中优于密集嵌入。如Grok - 4比其他模型更常澄清歧义。

Conclusion: SAEs是用于非结构化数据分析的通用工具，强调了通过数据解释模型的重要性。

Abstract: Analyzing large-scale text corpora is a core challenge in machine learning, crucial for tasks like identifying undesirable model behaviors or biases in training data. Current methods often rely on costly LLM-based techniques (e.g. annotating dataset differences) or dense embedding models (e.g. for clustering), which lack control over the properties of interest. We propose using sparse autoencoders (SAEs) to create SAE embeddings: representations whose dimensions map to interpretable concepts. Through four data analysis tasks, we show that SAE embeddings are more cost-effective and reliable than LLMs and more controllable than dense embeddings. Using the large hypothesis space of SAEs, we can uncover insights such as (1) semantic differences between datasets and (2) unexpected concept correlations in documents. For instance, by comparing model responses, we find that Grok-4 clarifies ambiguities more often than nine other frontier models. Relative to LLMs, SAE embeddings uncover bigger differences at 2-8x lower cost and identify biases more reliably. Additionally, SAE embeddings are controllable: by filtering concepts, we can (3) cluster documents along axes of interest and (4) outperform dense embeddings on property-based retrieval. Using SAE embeddings, we study model behavior with two case studies: investigating how OpenAI model behavior has changed over time and finding "trigger" phrases learned by Tulu-3 (Lambert et al., 2024) from its training data. These results position SAEs as a versatile tool for unstructured data analysis and highlight the neglected importance of interpreting models through their data.

</details>


### [15] [Robust AI Security and Alignment: A Sisyphean Endeavor?](https://arxiv.org/abs/2512.10100)
*Apostol Vassilev*

Main category: cs.AI

TL;DR: 将哥德尔不完备性定理扩展到AI，建立了AI安全和对齐鲁棒性的信息论限制，还给出应对挑战方法及证明认知推理局限性影响。


<details>
  <summary>Details</summary>
Motivation: 了解AI技术鲁棒性信息论限制及其带来的挑战，以实现负责任的AI技术应用。

Method: 将哥德尔不完备性定理扩展到AI。

Result: 建立了AI安全和对齐鲁棒性的信息论限制，证明了对AI系统认知推理局限性的更广泛影响。

Conclusion: 了解限制并应对挑战对AI技术负责任采用至关重要，同时给出应对挑战的实用方法。

Abstract: This manuscript establishes information-theoretic limitations for robustness of AI security and alignment by extending Gödel's incompleteness theorem to AI. Knowing these limitations and preparing for the challenges they bring is critically important for the responsible adoption of the AI technology. Practical approaches to dealing with these challenges are provided as well. Broader implications for cognitive reasoning limitations of AI systems are also proven.

</details>


### [16] [Modeling Narrative Archetypes in Conspiratorial Narratives: Insights from Singapore-Based Telegram Groups](https://arxiv.org/abs/2512.10105)
*Soorya Ram Shimgekar,Abhay Goyal,Lam Yin Cheung,Roy Ka-Wei Lee,Koustuv Saha,Pi Zonooz,Navin Kumar*

Main category: cs.AI

TL;DR: 本文分析新加坡Telegram群组中的阴谋论叙事，提出两阶段计算框架，识别七种叙事原型，结果表明阴谋论话语存在于日常交流中，挑战常见假设，框架有应用价值。


<details>
  <summary>Details</summary>
Motivation: 阴谋论话语嵌入数字通信生态系统，但结构和传播难以研究，需对其进行分析。

Method: 提出两阶段计算框架，先微调RoBERTa - large分类消息，再构建有符号信念图，引入SiBeGNN学习嵌入，用层次聚类识别叙事原型。

Result: 分类F1分数达0.866，SiBeGNN聚类质量优于基线方法，专家评估评分者间一致性达88%，发现阴谋论消息存在于日常讨论中。

Conclusion: 阴谋论话语在日常社交互动中存在，挑战了在线激进主义的常见假设，框架推动了信念驱动话语分析的计算方法发展，有多种应用。

Abstract: Conspiratorial discourse is increasingly embedded within digital communication ecosystems, yet its structure and spread remain difficult to study. This work analyzes conspiratorial narratives in Singapore-based Telegram groups, showing that such content is woven into everyday discussions rather than confined to isolated echo chambers. We propose a two-stage computational framework. First, we fine-tune RoBERTa-large to classify messages as conspiratorial or not, achieving an F1-score of 0.866 on 2,000 expert-labeled messages. Second, we build a signed belief graph in which nodes represent messages and edge signs reflect alignment in belief labels, weighted by textual similarity. We introduce a Signed Belief Graph Neural Network (SiBeGNN) that uses a Sign Disentanglement Loss to learn embeddings that separate ideological alignment from stylistic features.
  Using hierarchical clustering on these embeddings, we identify seven narrative archetypes across 553,648 messages: legal topics, medical concerns, media discussions, finance, contradictions in authority, group moderation, and general chat. SiBeGNN yields stronger clustering quality (cDBI = 8.38) than baseline methods (13.60 to 67.27), supported by 88 percent inter-rater agreement in expert evaluations. Our analysis shows that conspiratorial messages appear not only in clusters focused on skepticism or distrust, but also within routine discussions of finance, law, and everyday matters. These findings challenge common assumptions about online radicalization by demonstrating that conspiratorial discourse operates within ordinary social interaction. The proposed framework advances computational methods for belief-driven discourse analysis and offers applications for stance detection, political communication studies, and content moderation policy.

</details>


### [17] [AgriRegion: Region-Aware Retrieval for High-Fidelity Agricultural Advice](https://arxiv.org/abs/2512.10114)
*Mesafint Fanuel,Mahmoud Nabil Mahmoud,Crystal Cook Marshal,Vishal Lakhotia,Biswanath Dari,Kaushik Roy,Shaohu Zhang*

Main category: cs.AI

TL;DR: 论文介绍专门用于农业咨询的RAG框架AgriRegion，它能减少幻觉并提高信任度。


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型在农业领域存在上下文幻觉问题，需高保真、区域感知的农业咨询框架。

Method: 引入AgriRegion框架，结合地理空间元数据注入层和区域优先重排序机制，限制知识库并实施地理空间约束，还创建基准数据集AgriRegion - Eval。

Result: AgriRegion比现有大语言模型系统减少10 - 20%的幻觉，综合评估中信任得分显著提高。

Conclusion: AgriRegion框架能有效解决农业领域大语言模型的幻觉问题，提供更准确的本地农业建议。

Abstract: Large Language Models (LLMs) have demonstrated significant potential in democratizing access to information. However, in the domain of agriculture, general-purpose models frequently suffer from contextual hallucination, which provides non-factual advice or answers are scientifically sound in one region but disastrous in another due to variations in soil, climate, and local regulations. We introduce AgriRegion, a Retrieval-Augmented Generation (RAG) framework designed specifically for high-fidelity, region-aware agricultural advisory. Unlike standard RAG approaches that rely solely on semantic similarity, AgriRegion incorporates a geospatial metadata injection layer and a region-prioritized re-ranking mechanism. By restricting the knowledge base to verified local agricultural extension services and enforcing geo-spatial constraints during retrieval, AgriRegion ensures that the advice regarding planting schedules, pest control, and fertilization is locally accurate. We create a novel benchmark dataset, AgriRegion-Eval, which comprises 160 domain-specific questions across 12 agricultural subfields. Experiments demonstrate that AgriRegion reduces hallucinations by 10-20% compared to state-of-the-art LLMs systems and significantly improves trust scores according to a comprehensive evaluation.

</details>


### [18] [The 2025 Foundation Model Transparency Index](https://arxiv.org/abs/2512.10169)
*Alexander Wan,Kevin Klyman,Sayash Kapoor,Nestor Maslej,Shayne Longpre,Betty Xiong,Percy Liang,Rishi Bommasani*

Main category: cs.AI

TL;DR: 2025年基础模型透明度指数显示，基础模型开发者透明度较2024年下降，IBM表现突出，部分公司处于中游，研究揭示现状及政策干预方向。


<details>
  <summary>Details</summary>
Motivation: 探究随着基础模型开发公司影响力增大，其透明度实践如何演变。

Method: 通过2025年基础模型透明度指数，引入新指标评估公司，对比2024年数据。

Result: 2025年透明度平均得分从2024年的58降至40，公司在训练数据等方面最不透明，IBM得分95，xAI和Midjourney仅14，前沿模型论坛成员处于中游。

Conclusion: 揭示基础模型开发者透明度现状，指出政策影响及需更激进政策干预的方向。

Abstract: Foundation model developers are among the world's most important companies. As these companies become increasingly consequential, how do their transparency practices evolve? The 2025 Foundation Model Transparency Index is the third edition of an annual effort to characterize and quantify the transparency of foundation model developers. The 2025 FMTI introduces new indicators related to data acquisition, usage data, and monitoring and evaluates companies like Alibaba, DeepSeek, and xAI for the first time. The 2024 FMTI reported that transparency was improving, but the 2025 FMTI finds this progress has deteriorated: the average score out of 100 fell from 58 in 2024 to 40 in 2025. Companies are most opaque about their training data and training compute as well as the post-deployment usage and impact of their flagship models. In spite of this general trend, IBM stands out as a positive outlier, scoring 95, in contrast to the lowest scorers, xAI and Midjourney, at just 14. The five members of the Frontier Model Forum we score end up in the middle of the Index: we posit that these companies avoid reputational harms from low scores but lack incentives to be transparency leaders. As policymakers around the world increasingly mandate certain types of transparency, this work reveals the current state of transparency for foundation model developers, how it may change given newly enacted policy, and where more aggressive policy interventions are necessary to address critical information deficits.

</details>


### [19] [CP-Env: Evaluating Large Language Models on Clinical Pathways in a Controllable Hospital Environment](https://arxiv.org/abs/2512.10206)
*Yakun Zhu,Zhongzhen Huang,Qianhan Feng,Linjie Mu,Yannian Gu,Shaoting Zhang,Qi Dou,Xiaofan Zhang*

Main category: cs.AI

TL;DR: 本文引入CP - Env评估大语言模型在端到端临床路径中的表现，提出三层评估框架，结果显示多数模型有问题，CP - Env推动医疗AI代理发展并提供工具。


<details>
  <summary>Details</summary>
Motivation: 现有基准不足以评估大语言模型在动态临床场景中的表现。

Method: 引入CP - Env可控代理医院环境模拟医院生态供代理交互，提出三层评估框架。

Result: 多数模型难以应对路径复杂性，存在幻觉和丢失关键诊断细节问题，过多推理步骤可能适得其反，顶级模型减少工具依赖。

Conclusion: CP - Env通过全面的端到端临床评估推动医疗AI代理发展，并提供基准和评估工具。

Abstract: Medical care follows complex clinical pathways that extend beyond isolated physician-patient encounters, emphasizing decision-making and transitions between different stages. Current benchmarks focusing on static exams or isolated dialogues inadequately evaluate large language models (LLMs) in dynamic clinical scenarios. We introduce CP-Env, a controllable agentic hospital environment designed to evaluate LLMs across end-to-end clinical pathways. CP-Env simulates a hospital ecosystem with patient and physician agents, constructing scenarios ranging from triage and specialist consultation to diagnostic testing and multidisciplinary team meetings for agent interaction. Following real hospital adaptive flow of healthcare, it enables branching, long-horizon task execution. We propose a three-tiered evaluation framework encompassing Clinical Efficacy, Process Competency, and Professional Ethics. Results reveal that most models struggle with pathway complexity, exhibiting hallucinations and losing critical diagnostic details. Interestingly, excessive reasoning steps can sometimes prove counterproductive, while top models tend to exhibit reduced tool dependency through internalized knowledge. CP-Env advances medical AI agents development through comprehensive end-to-end clinical evaluation. We provide the benchmark and evaluation tools for further research and development at https://github.com/SPIRAL-MED/CP-Env.

</details>


### [20] [ID-PaS : Identity-Aware Predict-and-Search for General Mixed-Integer Linear Programs](https://arxiv.org/abs/2512.10211)
*Junyang Cai,El Mehdi Er Raqabi,Pascal Van Hentenryck,Bistra Dilkina*

Main category: cs.AI

TL;DR: 本文将Predict - and - Search框架扩展到参数化混合整数线性规划（MIP），引入ID - PaS框架，实验表明其性能优于现有求解器和原框架。


<details>
  <summary>Details</summary>
Motivation: 现有将机器学习融入Predict - and - Search框架的方法局限于二进制问题，且忽略实际中常见的固定变量。

Method: 将Predict - and - Search (PaS)框架扩展到参数化MIPs，引入身份感知学习框架ID - PaS以让机器学习模型更有效处理异构变量。

Result: 在多个现实大规模问题的实验中，ID - PaS相比最先进求解器Gurobi和PaS始终有更优表现。

Conclusion: ID - PaS框架在解决参数化MIPs问题上具有显著优势。

Abstract: Mixed-Integer Linear Programs (MIPs) are powerful and flexible tools for modeling a wide range of real-world combinatorial optimization problems. Predict-and-Search methods operate by using a predictive model to estimate promising variable assignments and then guiding a search procedure toward high-quality solutions. Recent research has demonstrated that incorporating machine learning (ML) into the Predict-and-Search framework significantly enhances its performance. Still, it is restricted to binary problems and overlooks the presence of fixed variables that commonly arise in practical settings. This work extends the Predict-and-Search (PaS) framework to parametric MIPs and introduces ID-PaS, an identity-aware learning framework that enables the ML model to handle heterogeneous variables more effectively. Experiments on several real-world large-scale problems demonstrate that ID-PaS consistently achieves superior performance compared to the state-of-the-art solver Gurobi and PaS.

</details>


### [21] [Reverse Thinking Enhances Missing Information Detection in Large Language Models](https://arxiv.org/abs/2512.10273)
*Yuxin Liu,Chaojie Gu,Yihang Zhang,Bin Qian,Shibo He*

Main category: cs.AI

TL;DR: 文章指出大语言模型处理缺失信息问题能力不足，提出基于反向思维的框架，实验表明该方法优于传统正向推理方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型处理含缺失信息问题时存在不足，正向推理方法难系统识别和恢复缺失信息。

Method: 受反向推理启发，提出用反向思维引导大语言模型识别必要条件和缺失元素的框架。

Result: 反向思维方法比传统正向推理方法有显著性能提升。

Conclusion: 反向思维方法为提升大语言模型逻辑完整性和推理鲁棒性提供了有前景的方向。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in various reasoning tasks, yet they often struggle with problems involving missing information, exhibiting issues such as incomplete responses, factual errors, and hallucinations. While forward reasoning approaches like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) have shown success in structured problem-solving, they frequently fail to systematically identify and recover omitted information. In this paper, we explore the potential of reverse thinking methodologies to enhance LLMs' performance on missing information detection tasks. Drawing inspiration from recent work on backward reasoning, we propose a novel framework that guides LLMs through reverse thinking to identify necessary conditions and pinpoint missing elements. Our approach transforms the challenging task of missing information identification into a more manageable backward reasoning problem, significantly improving model accuracy. Experimental results demonstrate that our reverse thinking approach achieves substantial performance gains compared to traditional forward reasoning methods, providing a promising direction for enhancing LLMs' logical completeness and reasoning robustness.

</details>


### [22] [Neuronal Attention Circuit (NAC) for Representation Learning](https://arxiv.org/abs/2512.10282)
*Waleed Razzaq,Izis Kankaraway,Yun-Bo Zhao*

Main category: cs.AI

TL;DR: 提出Neuronal Attention Circuit (NAC)机制，有三种计算模式和稀疏方案，有理论保证，在多领域实验中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有注意力机制离散性限制连续时间建模，需新机制。

Method: 将注意力对数计算重新表述为一阶线性常微分方程的解，用稀疏感官门替换密集投影，实现三种计算模式和稀疏Top-K方案。

Result: NAC在准确性上与或超过竞争基线，在运行时间和内存效率上处于中间位置。

Conclusion: NAC是一种有效可行的连续时间注意力机制。

Abstract: Attention improves representation learning over RNNs, but its discrete nature limits continuous-time (CT) modeling. We introduce Neuronal Attention Circuit (NAC), a novel, biologically plausible CT-Attention mechanism that reformulates attention logits computation as the solution to a linear first-order ODE with nonlinear interlinked gates derived from repurposing \textit{C. elegans} Neuronal Circuit Policies (NCPs) wiring mechanism. NAC replaces dense projections with sparse sensory gates for key-query projections and a sparse backbone network with two heads for computing \textit{content-target} and \textit{learnable time-constant} gates, enabling efficient adaptive dynamics. NAC supports three attention logit computation modes: (i) explicit Euler integration, (ii) exact closed-form solution, and (iii) steady-state approximation. To improve memory intensity, we implemented a sparse Top-\emph{K} pairwise concatenation scheme that selectively curates key-query interactions. We provide rigorous theoretical guarantees, including state stability, bounded approximation errors, and universal approximation. Empirically, we implemented NAC in diverse domains, including irregular time-series classification, lane-keeping for autonomous vehicles, and industrial prognostics. We observed that NAC matches or outperforms competing baselines in accuracy and occupies an intermediate position in runtime and memory efficiency compared with several CT baselines.

</details>


### [23] [Investigating The Functional Roles of Attention Heads in Vision Language Models: Evidence for Reasoning Modules](https://arxiv.org/abs/2512.10300)
*Yanbei Jiang,Xueqi Ma,Shu Liu,Sarah Monazam Erfani,Tongliang Liu,James Bailey,Jey Han Lau,Krista A. Ehinger*

Main category: cs.AI

TL;DR: 提出新颖的可解释性框架分析VLM内部机制，用CogVision数据集，发现功能注意力头特征及作用，为设计模型提供方向。


<details>
  <summary>Details</summary>
Motivation: 当前VLM大多是黑盒，需要系统性分析其内部机制，聚焦注意力头在多模态推理中的功能角色。

Method: 提出可解释性框架，引入CogVision数据集，用基于探测的方法识别功能注意力头。

Result: 功能注意力头普遍稀疏，数量和分布因功能而异，介导交互和层次组织；移除会导致性能下降，强调能提高准确率。

Conclusion: 研究为VLM的认知组织提供新见解，为设计更具人类感知和推理能力的模型指明方向。

Abstract: Despite excelling on multimodal benchmarks, vision-language models (VLMs) largely remain a black box. In this paper, we propose a novel interpretability framework to systematically analyze the internal mechanisms of VLMs, focusing on the functional roles of attention heads in multimodal reasoning. To this end, we introduce CogVision, a dataset that decomposes complex multimodal questions into step-by-step subquestions designed to simulate human reasoning through a chain-of-thought paradigm, with each subquestion associated with specific receptive or cognitive functions such as high-level visual reception and inference. Using a probing-based methodology, we identify attention heads that specialize in these functions and characterize them as functional heads. Our analysis across diverse VLM families reveals that these functional heads are universally sparse, vary in number and distribution across functions, and mediate interactions and hierarchical organization. Furthermore, intervention experiments demonstrate their critical role in multimodal reasoning: removing functional heads leads to performance degradation, while emphasizing them enhances accuracy. These findings provide new insights into the cognitive organization of VLMs and suggest promising directions for designing models with more human-aligned perceptual and reasoning abilities.

</details>


### [24] [Trustworthy Orchestration Artificial Intelligence by the Ten Criteria with Control-Plane Governance](https://arxiv.org/abs/2512.10304)
*Byeong Ho Kang,Wenli Yang,Muhammad Bilal Amin*

Main category: cs.AI

TL;DR: 本文提出可信编排AI的十条标准，建立综合保障框架，将治理融入AI生态执行架构，确保系统可信且受人为控制。


<details>
  <summary>Details</summary>
Motivation: AI系统决策角色增加，技术能力与机构问责间差距扩大，仅靠伦理指导不足以应对此挑战，需将治理嵌入生态执行架构。

Method: 提出Ten Criteria for Trustworthy Orchestration AI，将人类输入、语义一致性、审计和起源完整性集成到统一的控制面板架构中。

Result: 展示了可通过工程方式将可信性系统地融入AI系统。

Conclusion: 该框架确保AI系统的执行架构可验证、透明、可重现，并受有意义的人类控制。

Abstract: As Artificial Intelligence (AI) systems increasingly assume consequential decision-making roles, a widening gap has emerged between technical capabilities and institutional accountability. Ethical guidance alone is insufficient to counter this challenge; it demands architectures that embed governance into the execution fabric of the ecosystem. This paper presents the Ten Criteria for Trustworthy Orchestration AI, a comprehensive assurance framework that integrates human input, semantic coherence, audit and provenance integrity into a unified Control-Panel architecture. Unlike conventional agentic AI initiatives that primarily focus on AI-to-AI coordination, the proposed framework provides an umbrella of governance to the entire AI components, their consumers and human participants. By taking aspiration from international standards and Australia's National Framework for AI Assurance initiative, this work demonstrates that trustworthiness can be systematically incorporated (by engineering) into AI systems, ensuring the execution fabric remains verifiable, transparent, reproducible and under meaningful human control.

</details>


### [25] [InfoCom: Kilobyte-Scale Communication-Efficient Collaborative Perception with Information Bottleneck](https://arxiv.org/abs/2512.10305)
*Quanmin Wei,Penglin Dai,Wei Li,Bingyi Liu,Xiao Wu*

Main category: cs.AI

TL;DR: 提出InfoCom框架解决协作感知通信性能权衡问题，减少通信开销且实现近无损感知。


<details>
  <summary>Details</summary>
Motivation: 协作感知存在通信 - 性能权衡，现有通信高效方法因网络约束可能失效。

Method: 基于扩展信息瓶颈原则，引入信息净化范式，有信息感知编码、稀疏掩码生成和多尺度解码等创新。

Result: 在多个数据集实验中，InfoCom将通信开销从MB级降至KB级，比Where2comm和ERMVP分别减少440倍和90倍。

Conclusion: InfoCom能实现近无损感知并大幅降低通信开销。

Abstract: Precise environmental perception is critical for the reliability of autonomous driving systems. While collaborative perception mitigates the limitations of single-agent perception through information sharing, it encounters a fundamental communication-performance trade-off. Existing communication-efficient approaches typically assume MB-level data transmission per collaboration, which may fail due to practical network constraints. To address these issues, we propose InfoCom, an information-aware framework establishing the pioneering theoretical foundation for communication-efficient collaborative perception via extended Information Bottleneck principles. Departing from mainstream feature manipulation, InfoCom introduces a novel information purification paradigm that theoretically optimizes the extraction of minimal sufficient task-critical information under Information Bottleneck constraints. Its core innovations include: i) An Information-Aware Encoding condensing features into minimal messages while preserving perception-relevant information; ii) A Sparse Mask Generation identifying spatial cues with negligible communication cost; and iii) A Multi-Scale Decoding that progressively recovers perceptual information through mask-guided mechanisms rather than simple feature reconstruction. Comprehensive experiments across multiple datasets demonstrate that InfoCom achieves near-lossless perception while reducing communication overhead from megabyte to kilobyte-scale, representing 440-fold and 90-fold reductions per agent compared to Where2comm and ERMVP, respectively.

</details>


### [26] [EpiPlanAgent: Agentic Automated Epidemic Response Planning](https://arxiv.org/abs/2512.10313)
*Kangkun Mao,Fang Xu,Jinru Ding,Yidong Jiang,Yujun Yao,Yirong Chen,Junming Liu,Xiaoqin Wu,Qian Wu,Xiaoyan Huang,Jie Xu*

Main category: cs.AI

TL;DR: 本文介绍了用大语言模型开发的EpiPlanAgent系统，能自动生成和验证应急响应计划，结果显示其效果优于手动流程，为疫情响应规划提供有效解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统疫情响应规划依赖人工，劳动密集，本研究旨在设计自动生成和验证应急计划的系统。

Method: 设计基于多智能体框架的EpiPlanAgent系统，集成任务分解、知识接地和模拟模块，由公共卫生专家在受控评估中用真实疫情场景测试该系统。

Result: EpiPlanAgent显著提高了计划的完整性和与指南的一致性，大幅缩短了开发时间，AI生成内容与人工撰写内容高度一致，用户反馈实用性强。

Conclusion: EpiPlanAgent为智能疫情响应规划提供了有效且可扩展的解决方案，展示了智能AI改变公共卫生准备工作的潜力。

Abstract: Epidemic response planning is essential yet traditionally reliant on labor-intensive manual methods. This study aimed to design and evaluate EpiPlanAgent, an agent-based system using large language models (LLMs) to automate the generation and validation of digital emergency response plans. The multi-agent framework integrated task decomposition, knowledge grounding, and simulation modules. Public health professionals tested the system using real-world outbreak scenarios in a controlled evaluation. Results demonstrated that EpiPlanAgent significantly improved the completeness and guideline alignment of plans while drastically reducing development time compared to manual workflows. Expert evaluation confirmed high consistency between AI-generated and human-authored content. User feedback indicated strong perceived utility. In conclusion, EpiPlanAgent provides an effective, scalable solution for intelligent epidemic response planning, demonstrating the potential of agentic AI to transform public health preparedness.

</details>


### [27] [User-Feedback-Driven Continual Adaptation for Vision-and-Language Navigation](https://arxiv.org/abs/2512.10322)
*Yongqiang Yu,Xuhui Li,Hazza Mahmood,Jinxing Zhou,Haodong Hong,Longtao Jiang,Zhiqiang Xu,Qi Wu,Xiaojun Chang*

Main category: cs.AI

TL;DR: 本文提出用户反馈驱动的自适应框架拓展GSA - VLN，结合人类交互与持续学习，实验显示优于基线。


<details>
  <summary>Details</summary>
Motivation: 当前GSA - VLN框架排除用户反馈，而用户反馈可提升适应质量。

Method: 引入用户反馈驱动的自适应框架，将用户反馈转化为训练数据，使用记忆库暖启动机制。

Result: 在GSA - R2R基准上超GR - DUET等基线，记忆库暖启动稳定导航并减少更新后性能下降。

Conclusion: 该框架在不同部署条件下有持续改进，具鲁棒性和通用性。

Abstract: Vision-and-Language Navigation (VLN) requires agents to navigate complex environments by following natural-language instructions. General Scene Adaptation for VLN (GSA-VLN) shifts the focus from zero-shot generalization to continual, environment-specific adaptation, narrowing the gap between static benchmarks and real-world deployment. However, current GSA-VLN frameworks exclude user feedback, relying solely on unsupervised adaptation from repeated environmental exposure. In practice, user feedback offers natural and valuable supervision that can significantly enhance adaptation quality. We introduce a user-feedback-driven adaptation framework that extends GSA-VLN by systematically integrating human interactions into continual learning. Our approach converts user feedback-navigation instructions and corrective signals-into high-quality, environment-aligned training data, enabling efficient and realistic adaptation. A memory-bank warm-start mechanism further reuses previously acquired environmental knowledge, mitigating cold-start degradation and ensuring stable redeployment. Experiments on the GSA-R2R benchmark show that our method consistently surpasses strong baselines such as GR-DUET, improving navigation success and path efficiency. The memory-bank warm start stabilizes early navigation and reduces performance drops after updates. Results under both continual and hybrid adaptation settings confirm the robustness and generality of our framework, demonstrating sustained improvement across diverse deployment conditions.

</details>


### [28] [On the Collapse of Generative Paths: A Criterion and Correction for Diffusion Steering](https://arxiv.org/abs/2512.10339)
*Ziseok Lee,Minyeong Hwang,Sanghyun Jo,Wooyeol Lee,Jihyung Ko,Young Bin Park,Jae-Mun Choi,Eunho Yang,Kyungsu Kim*

Main category: cs.AI

TL;DR: 本文指出推理时转向中密度比方法存在边际路径崩溃问题，提出ACE方法解决该问题，使异构专家的密度比转向成为可控生成的可靠工具。


<details>
  <summary>Details</summary>
Motivation: 解决推理时转向中密度比方法存在的边际路径崩溃问题，让异构专家的密度比转向更可靠。

Method: 推导简单路径存在准则预测崩溃发生条件；引入Adaptive path Correction with Exponents (ACE) 方法，扩展Feynman - Kac转向至时变指数。

Result: 在合成2D基准和灵活姿势支架装饰任务中，ACE消除了崩溃，实现高引导组合生成，在分布和对接指标上优于恒定指数基线和特定任务的支架装饰模型。

Conclusion: 工作将异构专家的密度比转向从不稳定的启发式方法转变为可控生成的可靠工具。

Abstract: Inference-time steering enables pretrained diffusion/flow models to be adapted to new tasks without retraining. A widely used approach is the ratio-of-densities method, which defines a time-indexed target path by reweighting probability-density trajectories from multiple models with positive, or in some cases, negative exponents. This construction, however, harbors a critical and previously unformalized failure mode: Marginal Path Collapse, where intermediate densities become non-normalizable even though endpoints remain valid. Collapse arises systematically when composing heterogeneous models trained on different noise schedules or datasets, including a common setting in molecular design where de-novo, conformer, and pocket-conditioned models must be combined for tasks such as flexible-pose scaffold decoration. We provide a novel and complete solution for the problem. First, we derive a simple path existence criterion that predicts exactly when collapse occurs from noise schedules and exponents alone. Second, we introduce Adaptive path Correction with Exponents (ACE), which extends Feynman-Kac steering to time-varying exponents and guarantees a valid probability path. On a synthetic 2D benchmark and on flexible-pose scaffold decoration, ACE eliminates collapse and enables high-guidance compositional generation, improving distributional and docking metrics over constant-exponent baselines and even specialized task-specific scaffold decoration models. Our work turns ratio-of-densities steering with heterogeneous experts from an unstable heuristic into a reliable tool for controllable generation.

</details>


### [29] [REMISVFU: Vertical Federated Unlearning via Representation Misdirection for Intermediate Output Feature](https://arxiv.org/abs/2512.10348)
*Wenhan Wu,Zhili He,Huanghuang Liang,Yili Gong,Jiawei Jiang,Chuang Hu,Dazhao Cheng*

Main category: cs.AI

TL;DR: 本文提出REMISVFU框架用于splitVFL系统的客户端快速遗忘学习，在性能上优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 数据保护法规要求联邦系统满足遗忘权，现有遗忘学习技术多针对HFL，在VFL中无效，因此需要适用于VFL的遗忘学习方法。

Method: 提出REMISVFU框架，删除请求到来时，遗忘方将编码器输出坍缩到单位球上随机采样的锚点，服务器联合优化保留损失和遗忘损失，通过正交投影对齐梯度。

Result: 在公共基准测试中，REMISVFU将后门攻击成功率抑制到自然类先验水平，仅牺牲约2.5%的干净准确率，优于现有基线。

Conclusion: REMISVFU是一种有效的splitVFL系统客户端遗忘学习方法。

Abstract: Data-protection regulations such as the GDPR grant every participant in a federated system a right to be forgotten. Federated unlearning has therefore emerged as a research frontier, aiming to remove a specific party's contribution from the learned model while preserving the utility of the remaining parties. However, most unlearning techniques focus on Horizontal Federated Learning (HFL), where data are partitioned by samples. In contrast, Vertical Federated Learning (VFL) allows organizations that possess complementary feature spaces to train a joint model without sharing raw data. The resulting feature-partitioned architecture renders HFL-oriented unlearning methods ineffective. In this paper, we propose REMISVFU, a plug-and-play representation misdirection framework that enables fast, client-level unlearning in splitVFL systems. When a deletion request arrives, the forgetting party collapses its encoder output to a randomly sampled anchor on the unit sphere, severing the statistical link between its features and the global model. To maintain utility for the remaining parties, the server jointly optimizes a retention loss and a forgetting loss, aligning their gradients via orthogonal projection to eliminate destructive interference. Evaluations on public benchmarks show that REMISVFU suppresses back-door attack success to the natural class-prior level and sacrifices only about 2.5% points of clean accuracy, outperforming state-of-the-art baselines.

</details>


### [30] [LLM-Empowered Representation Learning for Emerging Item Recommendation](https://arxiv.org/abs/2512.10370)
*Ziying Zhang,Quanming Yao,Yaqing Wang*

Main category: cs.AI

TL;DR: 提出EmerFlow框架解决新兴物品推荐挑战，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有新兴物品推荐方法忽略动态过程，假设过于简化，需在保留独特性的同时利用与成熟物品的共享模式。

Method: 提出EmerFlow框架，先通过LLM推理丰富新兴物品原始特征，再将表示与现有推荐模型嵌入空间对齐，最后通过元学习纳入新交互来细化嵌入。

Result: 在电影和制药等多个领域的广泛实验表明，EmerFlow始终优于现有方法。

Conclusion: EmerFlow能从有限交互中学习新兴物品的有效嵌入，可用于解决新兴物品推荐问题。

Abstract: In this work, we tackle the challenge of recommending emerging items, whose interactions gradually accumulate over time. Existing methods often overlook this dynamic process, typically assuming that emerging items have few or even no historical interactions. Such an assumption oversimplifies the problem, as a good model must preserve the uniqueness of emerging items while leveraging their shared patterns with established ones. To address this challenge, we propose EmerFlow, a novel LLM-empowered representation learning framework that generates distinctive embeddings for emerging items. It first enriches the raw features of emerging items through LLM reasoning, then aligns these representations with the embedding space of the existing recommendation model. Finally, new interactions are incorporated through meta-learning to refine the embeddings. This enables EmerFlow to learn expressive embeddings for emerging items from only limited interactions. Extensive experiments across diverse domains, including movies and pharmaceuticals, show that EmerFlow consistently outperforms existing methods.

</details>


### [31] [AgentProg: Empowering Long-Horizon GUI Agents with Program-Guided Context Management](https://arxiv.org/abs/2512.10371)
*Shizuo Tian,Hao Wen,Yuxuan Chen,Jiacheng Liu,Shanhui Zhao,Guohong Liu,Ju Ren,Yunxin Liu,Yuanchun Li*

Main category: cs.AI

TL;DR: 提出AgentProg解决移动GUI代理长程任务自动化中上下文管理问题，实验达SOTA且性能稳健，系统开源。


<details>
  <summary>Details</summary>
Motivation: 移动GUI代理长程任务自动化中，现有上下文管理和压缩技术无法保留关键语义信息，导致任务性能下降。

Method: 提出AgentProg，将交互历史重构为带变量和控制流的程序，还集成受Belief MDP框架启发的全局信念状态机制。

Result: 在AndroidWorld和扩展的长程任务套件实验中，AgentProg达到了SOTA成功率，在长程任务上保持稳健性能。

Conclusion: AgentProg能有效解决长程任务自动化中上下文管理瓶颈问题，具有良好性能。

Abstract: The rapid development of mobile GUI agents has stimulated growing research interest in long-horizon task automation. However, building agents for these tasks faces a critical bottleneck: the reliance on ever-expanding interaction history incurs substantial context overhead. Existing context management and compression techniques often fail to preserve vital semantic information, leading to degraded task performance. We propose AgentProg, a program-guided approach for agent context management that reframes the interaction history as a program with variables and control flow. By organizing information according to the structure of program, this structure provides a principled mechanism to determine which information should be retained and which can be discarded. We further integrate a global belief state mechanism inspired by Belief MDP framework to handle partial observability and adapt to unexpected environmental changes. Experiments on AndroidWorld and our extended long-horizon task suite demonstrate that AgentProg has achieved the state-of-the-art success rates on these benchmarks. More importantly, it maintains robust performance on long-horizon tasks while baseline methods experience catastrophic degradation. Our system is open-sourced at https://github.com/MobileLLM/AgentProg.

</details>


### [32] [Boosting RL-Based Visual Reasoning with Selective Adversarial Entropy Intervention](https://arxiv.org/abs/2512.10414)
*Yang Yu,Zhuangzhuang Chen,Siqi Wang,Lanqing Li,Xiaomeng Li*

Main category: cs.AI

TL;DR: 本文提出Selective - adversarial Entropy Intervention (SaEI)方法，通过对视觉输入进行对抗性扰动提升策略熵，实验表明该方法能提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的视觉语言模型微调方法多在策略优化时进行熵干预，忽略了在强化学习采样时的熵干预，而这能提升GRPO性能。

Method: 提出熵引导的对抗性采样（EgAS）将采样响应的熵作为对抗目标，用对抗梯度攻击视觉输入产生对抗样本；提出令牌选择熵计算（TsEC）在不扭曲模型事实知识的情况下最大化对抗攻击效果。

Result: 在域内和域外数据集上的大量实验表明，所提方法可通过熵干预极大提升策略探索能力。

Conclusion: 所提SaEI方法能通过熵干预提升策略探索，进而增强视觉语言模型的推理能力。

Abstract: Recently, reinforcement learning (RL) has become a common choice in enhancing the reasoning capabilities of vision-language models (VLMs). Considering existing RL-based finetuning methods, entropy intervention turns out to be an effective way to benefit exploratory ability, thereby improving policy performance. Notably, most existing studies intervene in entropy by simply controlling the update of specific tokens during policy optimization of RL. They ignore the entropy intervention during the RL sampling that can boost the performance of GRPO by improving the diversity of responses. In this paper, we propose Selective-adversarial Entropy Intervention, namely SaEI, which enhances policy entropy by distorting the visual input with the token-selective adversarial objective coming from the entropy of sampled responses. Specifically, we first propose entropy-guided adversarial sampling (EgAS) that formulates the entropy of sampled responses as an adversarial objective. Then, the corresponding adversarial gradient can be used to attack the visual input for producing adversarial samples, allowing the policy model to explore a larger answer space during RL sampling. Then, we propose token-selective entropy computation (TsEC) to maximize the effectiveness of adversarial attack in EgAS without distorting factual knowledge within VLMs. Extensive experiments on both in-domain and out-of-domain datasets show that our proposed method can greatly improve policy exploration via entropy intervention, to boost reasoning capabilities. Code will be released once the paper is accepted.

</details>


### [33] [Representation of the structure of graphs by sequences of instructions](https://arxiv.org/abs/2512.10429)
*Ezequiel Lopez-Rubio*

Main category: cs.AI

TL;DR: 提出一种新的图表示方法，将图的邻接矩阵用简单指令字符串表示，可逆且紧凑，实验结果良好。


<details>
  <summary>Details</summary>
Motivation: 现有图表示方法不适合深度学习语言模型处理，希望找到一种新方法助力深度学习模型处理图。

Method: 将图的邻接矩阵用一串简单指令表示，逐步构建邻接矩阵，且转换可逆。

Result: 进行了初步计算实验，结果积极。

Conclusion: 提出的图表示方法紧凑且保留图局部结构模式，有望促进深度学习模型处理图。

Abstract: The representation of graphs is commonly based on the adjacency matrix concept. This formulation is the foundation of most algebraic and computational approaches to graph processing. The advent of deep learning language models offers a wide range of powerful computational models that are specialized in the processing of text. However, current procedures to represent graphs are not amenable to processing by these models. In this work, a new method to represent graphs is proposed. It represents the adjacency matrix of a graph by a string of simple instructions. The instructions build the adjacency matrix step by step. The transformation is reversible, i.e. given a graph the string can be produced and vice versa. The proposed representation is compact and it maintains the local structural patterns of the graph. Therefore, it is envisaged that it could be useful to boost the processing of graphs by deep learning models. A tentative computational experiment is reported, with favorable results.

</details>


### [34] [Targeted Data Protection for Diffusion Model by Matching Training Trajectory](https://arxiv.org/abs/2512.10433)
*Hojun Lee,Mijin Koo,Yeji Song,Nojun Kwak*

Main category: cs.AI

TL;DR: 文章介绍了TAFAP方法，能在扩散模型微调中通过控制训练轨迹实现有效目标数据保护，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型微调的个性化需求增加，但存在数据未经授权使用和隐私侵犯问题，现有保护方法有局限性。

Method: 引入TAFAP方法，通过对抗扰动微调实现轨迹对齐，控制整个训练轨迹。

Result: 实验验证TAFAP在扩散模型中首次成功实现目标转换，同时控制身份和视觉模式，图像质量高。

Conclusion: TAFAP显著优于现有目标数据保护尝试，提供可验证的保障和控制、追踪扩散模型输出变化的新框架。

Abstract: Recent advancements in diffusion models have made fine-tuning text-to-image models for personalization increasingly accessible, but have also raised significant concerns regarding unauthorized data usage and privacy infringement. Current protection methods are limited to passively degrading image quality, failing to achieve stable control. While Targeted Data Protection (TDP) offers a promising paradigm for active redirection toward user-specified target concepts, existing TDP attempts suffer from poor controllability due to snapshot-matching approaches that fail to account for complete learning dynamics. We introduce TAFAP (Trajectory Alignment via Fine-tuning with Adversarial Perturbations), the first method to successfully achieve effective TDP by controlling the entire training trajectory. Unlike snapshot-based methods whose protective influence is easily diluted as training progresses, TAFAP employs trajectory-matching inspired by dataset distillation to enforce persistent, verifiable transformations throughout fine-tuning. We validate our method through extensive experiments, demonstrating the first successful targeted transformation in diffusion models with simultaneous control over both identity and visual patterns. TAFAP significantly outperforms existing TDP attempts, achieving robust redirection toward target concepts while maintaining high image quality. This work enables verifiable safeguards and provides a new framework for controlling and tracing alterations in diffusion model outputs.

</details>


### [35] [When Reject Turns into Accept: Quantifying the Vulnerability of LLM-Based Scientific Reviewers to Indirect Prompt Injection](https://arxiv.org/abs/2512.10449)
*Devanshu Sahoo,Manish Prasad,Vasudev Majhi,Jahnvi Singh,Vinay Chamola,Yash Sinha,Murari Mandal,Dhruv Kumar*

Main category: cs.AI

TL;DR: 研究LLM在科学同行评审系统中受对抗性PDF操纵的鲁棒性，开发WAVS指标，实验表明某些策略能操纵评分。


<details>
  <summary>Details</summary>
Motivation: 科学同行评审融入LLM的趋势下，研究‘LLM-as-a-Judge’系统对对抗性PDF操纵的鲁棒性。

Method: 开发WAVS评估指标，整理200篇科学论文数据集，采用15种特定领域攻击策略对13种语言模型进行评估。

Result: 如‘Maximum Mark Magyk’等混淆策略能成功操纵评分，在大规模模型中实现惊人的决策翻转率。

Conclusion: 将发布完整数据集和注入框架，以推动该领域更多研究。

Abstract: The landscape of scientific peer review is rapidly evolving with the integration of Large Language Models (LLMs). This shift is driven by two parallel trends: the widespread individual adoption of LLMs by reviewers to manage workload (the "Lazy Reviewer" hypothesis) and the formal institutional deployment of AI-powered assessment systems by conferences like AAAI and Stanford's Agents4Science. This study investigates the robustness of these "LLM-as-a-Judge" systems (both illicit and sanctioned) to adversarial PDF manipulation. Unlike general jailbreaks, we focus on a distinct incentive: flipping "Reject" decisions to "Accept," for which we develop a novel evaluation metric which we term as WAVS (Weighted Adversarial Vulnerability Score). We curated a dataset of 200 scientific papers and adapted 15 domain-specific attack strategies to this task, evaluating them across 13 Language Models, including GPT-5, Claude Haiku, and DeepSeek. Our results demonstrate that obfuscation strategies like "Maximum Mark Magyk" successfully manipulate scores, achieving alarming decision flip rates even in large-scale models. We will release our complete dataset and injection framework to facilitate more research on this topic.

</details>


### [36] [Zero-shot 3D Map Generation with LLM Agents: A Dual-Agent Architecture for Procedural Content Generation](https://arxiv.org/abs/2512.10501)
*Lim Chien Her,Ming Yan,Yunshu Bai,Ruihao Li,Hao Zhang*

Main category: cs.AI

TL;DR: 提出无训练架构，用大语言模型代理进行零样本PCG参数配置，在3D地图生成上验证，性能优于单代理基线。


<details>
  <summary>Details</summary>
Motivation: 现有PCG管道控制需精确配置不透明技术参数，大语言模型虽有自然语言接口但难弥合抽象指令与参数规范语义差距。

Method: 采用Actor代理和Critic代理配对，实现迭代工作流，自主推理工具参数并优化配置。

Result: 在3D地图生成上建立新基准，优于单代理基线，能从自然语言描述生成多样且结构有效的环境。

Conclusion: 现成大语言模型可有效用作通用代理，方法提供无需特定任务微调的可扩展框架。

Abstract: Procedural Content Generation (PCG) offers scalable methods for algorithmically creating complex, customizable worlds. However, controlling these pipelines requires the precise configuration of opaque technical parameters. We propose a training-free architecture that utilizes LLM agents for zero-shot PCG parameter configuration. While Large Language Models (LLMs) promise a natural language interface for PCG tools, off-the-shelf models often fail to bridge the semantic gap between abstract user instructions and strict parameter specifications. Our system pairs an Actor agent with a Critic agent, enabling an iterative workflow where the system autonomously reasons over tool parameters and refines configurations to progressively align with human design preferences. We validate this approach on the generation of various 3D maps, establishing a new benchmark for instruction-following in PCG. Experiments demonstrate that our approach outperforms single-agent baselines, producing diverse and structurally valid environments from natural language descriptions. These results demonstrate that off-the-shelf LLMs can be effectively repurposed as generalized agents for arbitrary PCG tools. By shifting the burden from model training to architectural reasoning, our method offers a scalable framework for mastering complex software without task-specific fine-tuning.

</details>


### [37] [Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning](https://arxiv.org/abs/2512.10534)
*Haiteng Zhao,Junhao Shen,Yiming Zhang,Songyang Gao,Kuikun Liu,Tianyou Ma,Fan Zheng,Dahua Lin,Wenwei Zhang,Kai Chen*

Main category: cs.AI

TL;DR: 本文提出几何大语言模型代理InternGeometry，克服启发式限制，结合动态内存机制与CBRL，用少量数据解决大量IMO几何问题，还能提出新辅助构造并将开源。


<details>
  <summary>Details</summary>
Motivation: 现有几何问题求解AI受限于启发式辅助构造，依赖大规模数据，本文旨在构建奖牌级几何大语言模型代理。

Method: 迭代提出命题和辅助构造，用符号引擎验证并根据反馈指导后续提议；采用动态内存机制；引入CBRL逐步增加合成问题复杂度。

Result: InternGeometry仅用13K训练样本解决50道IMO几何问题中的44道，超过金牌平均分，还能提出新辅助构造。

Conclusion: 证明了大语言模型代理在专家级几何任务上的潜力。

Abstract: Large language model (LLM) agents exhibit strong mathematical problem-solving abilities and can even solve International Mathematical Olympiad (IMO) level problems with the assistance of formal proof systems. However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. In this work, we make the first attempt to build a medalist-level LLM agent for geometry and present InternGeometry. InternGeometry overcomes the heuristic limitations in geometry by iteratively proposing propositions and auxiliary constructions, verifying them with a symbolic engine, and reflecting on the engine's feedback to guide subsequent proposals. A dynamic memory mechanism enables InternGeometry to conduct more than two hundred interactions with the symbolic engine per problem. To further accelerate learning, we introduce Complexity-Boosting Reinforcement Learning (CBRL), which gradually increases the complexity of synthesized problems across training stages. Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-level geometry tasks. InternGeometry can also propose novel auxiliary constructions for IMO problems that do not appear in human solutions. We will release the model, data, and symbolic engine to support future research.

</details>


### [38] [NormCode: A Semi-Formal Language for Context-Isolated AI Planning](https://arxiv.org/abs/2512.10563)
*Xin Guan*

Main category: cs.AI

TL;DR: 提出NormCode语言解决大语言模型多步工作流的上下文污染问题，通过验证且使AI工作流可审计。


<details>
  <summary>Details</summary>
Motivation: 大语言模型多步工作流存在上下文污染问题，如模型产生幻觉、混淆输出和丢失任务约束。

Method: 提出NormCode语言，构建推理计划，严格分离语义和句法操作，有三种同构格式。

Result: 通过两个演示验证，加法算法达100%准确率，实现自身编译器管道自托管执行。

Conclusion: 工作编排器使AI工作流可审计，满足高风险领域透明度需求。

Abstract: Multistep workflows that chain large language model (LLM) calls suffer from context pollution: as information accumulates across steps, models hallucinate, confuse intermediate outputs, and lose track of task constraints. We present NormCode, a semiformal language for constructing plans of inferences, structured decompositions where each step operates in data isolation and receives only explicitly passed inputs, which eliminates crossstep contamination by design. NormCode enforces a strict separation between semantic operations (LLMdriven reasoning, nondeterministic) and syntactic operations (deterministic data restructuring), enabling precise cost and reliability tracing. The language exists in three isomorphic formats: .ncds for human authoring, .ncd for machine execution, and .ncn for human verification, supporting progressive formalization from sketch to production. We validate NormCode through two demonstrations: (1) a base X addition algorithm achieving 100 percent accuracy on arbitrary length inputs, and (2) self hosted execution of NormCode's own five phase compiler pipeline. The working orchestrator provides dependency driven scheduling, SQLite backed checkpointing, and loop management, making AI workflows auditable by design and addressing a critical need for transparency in high stakes domains such as legal reasoning, medical decision making, and financial analysis.

</details>


### [39] [Refinement Contrastive Learning of Cell-Gene Associations for Unsupervised Cell Type Identification](https://arxiv.org/abs/2512.10640)
*Liang Peng,Haopeng Liu,Yixuan Ye,Cheng Liu,Wenjun Shen,Si Wu,Hau-San Wong*

Main category: cs.AI

TL;DR: 提出scRCL框架用于无监督细胞类型识别，实验表明其在准确率上优于现有方法，且结果具生物学相关性。


<details>
  <summary>Details</summary>
Motivation: 现有聚类方法大多只关注细胞内在结构，忽略细胞 - 基因关联，限制区分相似细胞类型的能力。

Method: 提出scRCL框架，引入两个对比分布对齐组件揭示细胞内在结构，开发细化模块整合基因相关性结构学习以增强细胞嵌入。

Result: 在多个单细胞RNA测序和空间转录组基准数据集上，方法在细胞类型识别准确率上优于现有方法，下游生物学分析验证细胞群体具有一致基因表达特征。

Conclusion: scRCL框架有效提升无监督细胞类型识别效果，结果具有生物学相关性。

Abstract: Unsupervised cell type identification is crucial for uncovering and characterizing heterogeneous populations in single cell omics studies. Although a range of clustering methods have been developed, most focus exclusively on intrinsic cellular structure and ignore the pivotal role of cell-gene associations, which limits their ability to distinguish closely related cell types. To this end, we propose a Refinement Contrastive Learning framework (scRCL) that explicitly incorporates cell-gene interactions to derive more informative representations. Specifically, we introduce two contrastive distribution alignment components that reveal reliable intrinsic cellular structures by effectively exploiting cell-cell structural relationships. Additionally, we develop a refinement module that integrates gene-correlation structure learning to enhance cell embeddings by capturing underlying cell-gene associations. This module strengthens connections between cells and their associated genes, refining the representation learning to exploiting biologically meaningful relationships. Extensive experiments on several single-cell RNA-seq and spatial transcriptomics benchmark datasets demonstrate that our method consistently outperforms state-of-the-art baselines in cell-type identification accuracy. Moreover, downstream biological analyses confirm that the recovered cell populations exhibit coherent gene-expression signatures, further validating the biological relevance of our approach. The code is available at https://github.com/THPengL/scRCL.

</details>


### [40] [CAPTAIN: Semantic Feature Injection for Memorization Mitigation in Text-to-Image Diffusion Models](https://arxiv.org/abs/2512.10655)
*Tong Zhang,Carlos Hinojosa,Bernard Ghanem*

Main category: cs.AI

TL;DR: 提出训练无关框架CAPTAIN，通过在去噪时修改潜在特征缓解扩散模型记忆问题，实验显示其效果优于基于CFG的基线方法且保持与提示对齐。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在大规模部署时会无意复制训练样本，引发隐私和版权问题，现有推理时缓解方法难以在不影响与提示对齐的情况下减少记忆。

Method: CAPTAIN先应用基于频率的噪声初始化减少去噪早期复制记忆模式的倾向，再确定特征注入的最佳去噪时间步和定位记忆区域，最后将非记忆参考图像的语义对齐特征注入局部潜在区域。

Result: 实验表明，与基于CFG的基线相比，CAPTAIN在减少记忆方面有显著效果。

Conclusion: CAPTAIN能在减少记忆的同时，保持与提示的强对齐性并维持视觉质量。

Abstract: Diffusion models can unintentionally reproduce training examples, raising privacy and copyright concerns as these systems are increasingly deployed at scale. Existing inference-time mitigation methods typically manipulate classifier-free guidance (CFG) or perturb prompt embeddings; however, they often struggle to reduce memorization without compromising alignment with the conditioning prompt. We introduce CAPTAIN, a training-free framework that mitigates memorization by directly modifying latent features during denoising. CAPTAIN first applies frequency-based noise initialization to reduce the tendency to replicate memorized patterns early in the denoising process. It then identifies the optimal denoising timesteps for feature injection and localizes memorized regions. Finally, CAPTAIN injects semantically aligned features from non-memorized reference images into localized latent regions, suppressing memorization while preserving prompt fidelity and visual quality. Our experiments show that CAPTAIN achieves substantial reductions in memorization compared to CFG-based baselines while maintaining strong alignment with the intended prompt.

</details>


### [41] [On the Dynamics of Multi-Agent LLM Communities Driven by Value Diversity](https://arxiv.org/abs/2512.10665)
*Muhua Huang,Qinlin Zhao,Xiaoyuan Yi,Xing Xie*

Main category: cs.AI

TL;DR: 研究价值多样性如何塑造AI社区集体行为，发现价值多样性有积极影响但极端异质性会致不稳定。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的多智能体系统流行，探讨价值多样性如何塑造AI社区集体行为这一基础问题。

Method: 基于施瓦茨的基本人类价值观理论进行自然主义价值引出，构建多智能体模拟，让不同数量智能体社区进行开放式交互和规则形成。

Result: 价值多样性增强价值稳定性、促进涌现行为、使智能体自主制定更有创造性原则，但极端异质性导致不稳定。

Conclusion: 将价值多样性定位为未来AI能力新维度，连接AI能力和制度涌现的社会学研究。

Abstract: As Large Language Models (LLM) based multi-agent systems become increasingly prevalent, the collective behaviors, e.g., collective intelligence, of such artificial communities have drawn growing attention. This work aims to answer a fundamental question: How does diversity of values shape the collective behavior of AI communities? Using naturalistic value elicitation grounded in the prevalent Schwartz's Theory of Basic Human Values, we constructed multi-agent simulations where communities with varying numbers of agents engaged in open-ended interactions and constitution formation. The results show that value diversity enhances value stability, fosters emergent behaviors, and brings more creative principles developed by the agents themselves without external guidance. However, these effects also show diminishing returns: extreme heterogeneity induces instability. This work positions value diversity as a new axis of future AI capability, bridging AI ability and sociological studies of institutional emergence.

</details>


### [42] [AEBNAS: Strengthening Exit Branches in Early-Exit Networks through Hardware-Aware Neural Architecture Search](https://arxiv.org/abs/2512.10671)
*Oscar Robben,Saeed Khalilian,Nirvana Meratnia*

Main category: cs.AI

TL;DR: 本文提出用硬件感知的NAS加强早退网络的退出分支，在多数据集验证其能以相同或更低的平均MAC数达到更高准确率。


<details>
  <summary>Details</summary>
Motivation: 早退网络设计需平衡效率和性能，现有NAS方法未充分考虑退出分支的深度和类型，本文旨在设计更高效准确的早退网络。

Method: 使用硬件感知的NAS加强退出分支，考虑退出分支不同深度和层类型，并进行自适应阈值调整。

Result: 在CIFAR - 10、CIFAR - 100和SVHN数据集上，提出的框架能以相同或更低的平均MAC数达到比现有方法更高的准确率。

Conclusion: 所提出的框架在设计早退网络时，能在保证效率的同时提高模型准确性。

Abstract: Early-exit networks are effective solutions for reducing the overall energy consumption and latency of deep learning models by adjusting computation based on the complexity of input data. By incorporating intermediate exit branches into the architecture, they provide less computation for simpler samples, which is particularly beneficial for resource-constrained devices where energy consumption is crucial. However, designing early-exit networks is a challenging and time-consuming process due to the need to balance efficiency and performance. Recent works have utilized Neural Architecture Search (NAS) to design more efficient early-exit networks, aiming to reduce average latency while improving model accuracy by determining the best positions and number of exit branches in the architecture. Another important factor affecting the efficiency and accuracy of early-exit networks is the depth and types of layers in the exit branches. In this paper, we use hardware-aware NAS to strengthen exit branches, considering both accuracy and efficiency during optimization. Our performance evaluation on the CIFAR-10, CIFAR-100, and SVHN datasets demonstrates that our proposed framework, which considers varying depths and layers for exit branches along with adaptive threshold tuning, designs early-exit networks that achieve higher accuracy with the same or lower average number of MACs compared to the state-of-the-art approaches.

</details>


### [43] [Challenges of Evaluating LLM Safety for User Welfare](https://arxiv.org/abs/2512.10687)
*Manon Kempermann,Sai Suresh Macharla Vasu,Mahalakshmi Raveenthiran,Theo Farrell,Ingmar Weber*

Main category: cs.AI

TL;DR: 研究评估大语言模型在金融和健康领域给不同脆弱程度用户的建议安全性，发现有效评估需考虑用户多样情况，现实用户上下文披露不足。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型安全评估多关注普遍风险，而个人高风险话题建议的用户福利安全评估发展不足，且设计中考虑用户上下文存在根本问题。

Method: 评估GPT - 5、Claude Sonnet 4和Gemini 2.5 Pro在金融和健康方面给不同脆弱用户的建议，进行两轮评估，一轮有无上下文对比，一轮含用户披露上下文。

Result: 无上下文评估者比有上下文评估者认为回复更安全，高脆弱用户安全评分降低；含用户披露上下文的提示评估无显著改善。

Conclusion: 有效用户福利安全评估需评估者针对多样用户配置文件评估回复，现实用户上下文披露不足，研究为上下文感知评估提供起点和基础证据，并公开代码和数据集。

Abstract: Safety evaluations of large language models (LLMs) typically focus on universal risks like dangerous capabilities or undesirable propensities. However, millions use LLMs for personal advice on high-stakes topics like finance and health, where harms are context-dependent rather than universal. While frameworks like the OECD's AI classification recognize the need to assess individual risks, user-welfare safety evaluations remain underdeveloped. We argue that developing such evaluations is non-trivial due to fundamental questions about accounting for user context in evaluation design. In this exploratory study, we evaluated advice on finance and health from GPT-5, Claude Sonnet 4, and Gemini 2.5 Pro across user profiles of varying vulnerability. First, we demonstrate that evaluators must have access to rich user context: identical LLM responses were rated significantly safer by context-blind evaluators than by those aware of user circumstances, with safety scores for high-vulnerability users dropping from safe (5/7) to somewhat unsafe (3/7). One might assume this gap could be addressed by creating realistic user prompts containing key contextual information. However, our second study challenges this: we rerun the evaluation on prompts containing context users report they would disclose, finding no significant improvement. Our work establishes that effective user-welfare safety evaluation requires evaluators to assess responses against diverse user profiles, as realistic user context disclosure alone proves insufficient, particularly for vulnerable populations. By demonstrating a methodology for context-aware evaluation, this study provides both a starting point for such assessments and foundational evidence that evaluating individual welfare demands approaches distinct from existing universal-risk frameworks. We publish our code and dataset to aid future developments.

</details>


### [44] [Enhancing Radiology Report Generation and Visual Grounding using Reinforcement Learning](https://arxiv.org/abs/2512.10691)
*Benjamin Gundersen,Nicolas Deperrois,Samuel Ruiperez-Campillo,Thomas M. Sutter,Julia E. Vogt,Michael Moor,Farhad Nooralahzadeh,Michael Krauthammer*

Main category: cs.AI

TL;DR: 研究在CXR VLM中结合强化学习（RL）和显式中间推理（思考）的效果，发现RL有额外增益，思考未进一步提升结果，RL优化模型达SOTA。


<details>
  <summary>Details</summary>
Motivation: 许多医学视觉语言模型仅依赖监督微调，未评估答案质量，而强化学习可结合特定任务反馈，探究其与思考在CXR VLM中的效果。

Method: 在CXR数据上进行大规模监督微调构建基于Qwen3 - VL的RadVLM，冷启动SFT赋予模型基本思考能力，应用GRPO结合特定任务奖励，对特定领域和通用领域Qwen3 - VL变体进行有/无思考的RL实验。

Result: 强SFT对高基础性能很关键，RL在两项任务上有额外增益，显式思考未进一步改善结果，RL优化的RadVLM模型表现优于基线并达SOTA。

Conclusion: 临床对齐的RL是医学视觉语言模型监督微调的有力补充。

Abstract: Recent advances in vision-language models (VLMs) have improved Chest X-ray (CXR) interpretation in multiple aspects. However, many medical VLMs rely solely on supervised fine-tuning (SFT), which optimizes next-token prediction without evaluating answer quality. In contrast, reinforcement learning (RL) can incorporate task-specific feedback, and its combination with explicit intermediate reasoning ("thinking") has demonstrated substantial gains on verifiable math and coding tasks. To investigate the effects of RL and thinking in a CXR VLM, we perform large-scale SFT on CXR data to build an updated RadVLM based on Qwen3-VL, followed by a cold-start SFT stage that equips the model with basic thinking ability. We then apply Group Relative Policy Optimization (GRPO) with clinically grounded, task-specific rewards for report generation and visual grounding, and run matched RL experiments on both domain-specific and general-domain Qwen3-VL variants, with and without thinking. Across these settings, we find that while strong SFT remains crucial for high base performance, RL provides additional gains on both tasks, whereas explicit thinking does not appear to further improve results. Under a unified evaluation pipeline, the RL-optimized RadVLM models outperform their baseline counterparts and reach state-of-the-art performance on both report generation and grounding, highlighting clinically aligned RL as a powerful complement to SFT for medical VLMs.

</details>


### [45] [Remember Me, Refine Me: A Dynamic Procedural Memory Framework for Experience-Driven Agent Evolution](https://arxiv.org/abs/2512.10696)
*Zouying Cao,Jiaji Deng,Li Yu,Weikang Zhou,Zhaoyang Liu,Bolin Ding,Hai Zhao*

Main category: cs.AI

TL;DR: 提出ReMe框架以解决大语言模型代理程序记忆的静态存储与动态推理差距问题，经实验验证效果佳并发布代码和数据集。


<details>
  <summary>Details</summary>
Motivation: 现有框架多为“被动积累”范式，将记忆视为静态仅追加的存档，需弥合静态存储和动态推理间的差距。

Method: 提出ReMe框架，通过多方面提炼、上下文自适应复用和基于效用的优化三种机制创新记忆生命周期。

Result: 在BFCL - V3和AppWorld上实验表明ReMe在代理记忆系统中达新的最优水平，有显著记忆扩展效应。

Conclusion: 自进化记忆为终身学习提供了计算高效的途径。

Abstract: Procedural memory enables large language model (LLM) agents to internalize "how-to" knowledge, theoretically reducing redundant trial-and-error. However, existing frameworks predominantly suffer from a "passive accumulation" paradigm, treating memory as a static append-only archive. To bridge the gap between static storage and dynamic reasoning, we propose $\textbf{ReMe}$ ($\textit{Remember Me, Refine Me}$), a comprehensive framework for experience-driven agent evolution. ReMe innovates across the memory lifecycle via three mechanisms: 1) $\textit{multi-faceted distillation}$, which extracts fine-grained experiences by recognizing success patterns, analyzing failure triggers and generating comparative insights; 2) $\textit{context-adaptive reuse}$, which tailors historical insights to new contexts via scenario-aware indexing; and 3) $\textit{utility-based refinement}$, which autonomously adds valid memories and prunes outdated ones to maintain a compact, high-quality experience pool. Extensive experiments on BFCL-V3 and AppWorld demonstrate that ReMe establishes a new state-of-the-art in agent memory system. Crucially, we observe a significant memory-scaling effect: Qwen3-8B equipped with ReMe outperforms larger, memoryless Qwen3-14B, suggesting that self-evolving memory provides a computation-efficient pathway for lifelong learning. We release our code and the $\texttt{reme.library}$ dataset to facilitate further research.

</details>


### [46] [COMPARE: Clinical Optimization with Modular Planning and Assessment via RAG-Enhanced AI-OCT: Superior Decision Support for Percutaneous Coronary Intervention Compared to ChatGPT-5 and Junior Operators](https://arxiv.org/abs/2512.10702)
*Wei Fang,Chiyao Wang,Wenshuai Ma,Hui Liu,Jianqiang Hu,Xiaona Niu,Yi Chu,Mingming Zhang,Jingxiao Yang,Dongwei Zhang,Zelin Li,Pengyun Liu,Jiawei Zheng,Pengke Zhang,Chaoshi Qin,Wangang Guo,Bin Wang,Yugang Xue,Wei Zhang,Zikuan Wang,Rui Zhu,Yihui Cao,Quanmao Lu,Rui Meng,Yan Li*

Main category: cs.AI

TL;DR: 评估CA - GPT在AI - OCT系统中用于OCT引导的PCI规划和评估的表现，发现其优于ChatGPT - 5和初级医生。


<details>
  <summary>Details</summary>
Motivation: 血管内成像解读依赖操作者，通用AI缺乏特定领域可靠性，因此评估CA - GPT在OCT引导的PCI规划和评估中的性能。

Method: 单中心分析96例接受OCT引导PCI患者，将CA - GPT、ChatGPT - 5和初级医生的决策与专家记录对比，用10个指标评估一致性。

Result: 在PCI术前规划和术后评估中，CA - GPT的一致性得分显著高于ChatGPT - 5和初级医生，在复杂场景中优势明显。

Conclusion: 基于CA - GPT的AI - OCT系统在PCI规划和评估中决策一致性更优，为血管内成像解读提供可靠方法，有潜力增强操作者专业能力和优化OCT引导的PCI。

Abstract: Background: While intravascular imaging, particularly optical coherence tomography (OCT), improves percutaneous coronary intervention (PCI) outcomes, its interpretation is operator-dependent. General-purpose artificial intelligence (AI) shows promise but lacks domain-specific reliability. We evaluated the performance of CA-GPT, a novel large model deployed on an AI-OCT system, against that of the general-purpose ChatGPT-5 and junior physicians for OCT-guided PCI planning and assessment.
  Methods: In this single-center analysis of 96 patients who underwent OCT-guided PCI, the procedural decisions generated by the CA-GPT, ChatGPT-5, and junior physicians were compared with an expert-derived procedural record. Agreement was assessed using ten pre-specified metrics across pre-PCI and post-PCI phases.
  Results: For pre-PCI planning, CA-GPT demonstrated significantly higher median agreement scores (5[IQR 3.75-5]) compared to both ChatGPT-5 (3[2-4], P<0.001) and junior physicians (4[3-4], P<0.001). CA-GPT significantly outperformed ChatGPT-5 across all individual pre-PCI metrics and showed superior performance to junior physicians in stent diameter (90.3% vs. 72.2%, P<0.05) and length selection (80.6% vs. 52.8%, P<0.01). In post-PCI assessment, CA-GPT maintained excellent overall agreement (5[4.75-5]), significantly higher than both ChatGPT-5 (4[4-5], P<0.001) and junior physicians (5[4-5], P<0.05). Subgroup analysis confirmed CA-GPT's robust performance advantage in complex scenarios.
  Conclusion: The CA-GPT-based AI-OCT system achieved superior decision-making agreement versus a general-purpose large language model and junior physicians across both PCI planning and assessment phases. This approach provides a standardized and reliable method for intravascular imaging interpretation, demonstrating significant potential to augment operator expertise and optimize OCT-guided PCI.

</details>


### [47] [Replace, Don't Expand: Mitigating Context Dilution in Multi-Hop RAG via Fixed-Budget Evidence Assembly](https://arxiv.org/abs/2512.10787)
*Moshe Lahmy,Roi Yozevitch*

Main category: cs.AI

TL;DR: 本文提出无训练控制器SEAL - RAG，采用“替换而非扩展”策略解决上下文稀释问题，在多跳查询上效果优于多个基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有纠正方法在多跳查询时，简单扩展上下文窗口会导致上下文稀释问题，影响查询效果。

Method: 提出无训练控制器SEAL - RAG，执行搜索、提取、评估、循环的周期，基于实体进行提取、查询和排序，替换干扰项。

Result: 在HotpotQA和2WikiMultiHopQA数据集上，SEAL - RAG在答案正确性、证据精度和准确率上优于多个基线模型，且效果显著。

Conclusion: SEAL - RAG采用固定k替换策略，能保证成本可预测，优化前k个槽的精度。

Abstract: Retrieval-Augmented Generation (RAG) systems often fail on multi-hop queries when the initial retrieval misses a bridge fact. Prior corrective approaches, such as Self-RAG, CRAG, and Adaptive-$k$, typically address this by \textit{adding} more context or pruning existing lists. However, simply expanding the context window often leads to \textbf{context dilution}, where distractors crowd out relevant information. We propose \textbf{SEAL-RAG}, a training-free controller that adopts a \textbf{``replace, don't expand''} strategy to fight context dilution under a fixed retrieval depth $k$. SEAL executes a (\textbf{S}earch $\rightarrow$ \textbf{E}xtract $\rightarrow$ \textbf{A}ssess $\rightarrow$ \textbf{L}oop) cycle: it performs on-the-fly, entity-anchored extraction to build a live \textit{gap specification} (missing entities/relations), triggers targeted micro-queries, and uses \textit{entity-first ranking} to actively swap out distractors for gap-closing evidence. We evaluate SEAL-RAG against faithful re-implementations of Basic RAG, CRAG, Self-RAG, and Adaptive-$k$ in a shared environment on \textbf{HotpotQA} and \textbf{2WikiMultiHopQA}. On HotpotQA ($k=3$), SEAL improves answer correctness by \textbf{+3--13 pp} and evidence precision by \textbf{+12--18 pp} over Self-RAG. On 2WikiMultiHopQA ($k=5$), it outperforms Adaptive-$k$ by \textbf{+8.0 pp} in accuracy and maintains \textbf{96\%} evidence precision compared to 22\% for CRAG. These gains are statistically significant ($p<0.001$). By enforcing fixed-$k$ replacement, SEAL yields a predictable cost profile while ensuring the top-$k$ slots are optimized for precision rather than mere breadth. We release our code and data at https://github.com/mosherino/SEAL-RAG.

</details>


### [48] [HAROOD: A Benchmark for Out-of-distribution Generalization in Sensor-based Human Activity Recognition](https://arxiv.org/abs/2512.10807)
*Wang Lu,Yao Zhu,Jindong Wang*

Main category: cs.AI

TL;DR: 本文提出用于传感器人类活动识别HAR在分布外（OOD）设置的综合基准HAROOD，进行实验并给出研究发现，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现实场景中相同活动存在显著分布偏移问题，现有工作仅针对特定分布偏移场景，缺乏对算法有效性的全面洞察。

Method: 定义4种OOD场景，构建涵盖6个数据集、16种比较方法和两种模型选择协议的测试平台。

Result: 实验发现没有单一方法能始终优于其他方法，有很大改进空间。

Conclusion: 代码模块化且易扩展，希望推动基于OOD的HAR研究。

Abstract: Sensor-based human activity recognition (HAR) mines activity patterns from the time-series sensory data. In realistic scenarios, variations across individuals, devices, environments, and time introduce significant distributional shifts for the same activities. Recent efforts attempt to solve this challenge by applying or adapting existing out-of-distribution (OOD) algorithms, but only in certain distribution shift scenarios (e.g., cross-device or cross-position), lacking comprehensive insights on the effectiveness of these algorithms. For instance, is OOD necessary to HAR? Which OOD algorithm performs the best? In this paper, we fill this gap by proposing HAROOD, a comprehensive benchmark for HAR in OOD settings. We define 4 OOD scenarios: cross-person, cross-position, cross-dataset, and cross-time, and build a testbed covering 6 datasets, 16 comparative methods (implemented with CNN-based and Transformer-based architectures), and two model selection protocols. Then, we conduct extensive experiments and present several findings for future research, e.g., no single method consistently outperforms others, highlighting substantial opportunity for advancement. Our codebase is highly modular and easy to extend for new datasets, algorithms, comparisons, and analysis, with the hope to facilitate the research in OOD-based HAR. Our implementation is released and can be found at https://github.com/AIFrontierLab/HAROOD.

</details>


### [49] [Agile Deliberation: Concept Deliberation for Subjective Visual Classification](https://arxiv.org/abs/2512.10821)
*Leijie Wang,Otilia Stretcu,Wei Qiao,Thomas Denby,Krishnamurthy Viswanathan,Enming Luo,Chun-Ta Lu,Tushar Dogra,Ranjay Krishna,Ariel Fuxman*

Main category: cs.AI

TL;DR: 本文提出用于视觉概念分类的人机协作框架Agile Deliberation，支持不断演变和主观的概念，经用户测试效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有基于人机协作的视觉概念分类方法假设用户对概念有清晰稳定的理解，而实际用户常从模糊概念开始，需迭代细化。

Method: 通过对内容审核专家的结构化访谈，将实际内容审核员的常见策略转化为Agile Deliberation框架，有概念范围界定和概念迭代两个阶段。

Result: 经过18次、每次1.5小时的用户测试，Agile Deliberation比自动分解基线的F1分数高7.5%，比手动审议高3%以上，参与者概念理解更清晰、认知负担更低。

Conclusion: Agile Deliberation框架能有效支持视觉概念分类，处理不断演变和主观的概念。

Abstract: From content moderation to content curation, applications requiring vision classifiers for visual concepts are rapidly expanding. Existing human-in-the-loop approaches typically assume users begin with a clear, stable concept understanding to be able to provide high-quality supervision. In reality, users often start with a vague idea and must iteratively refine it through "concept deliberation", a practice we uncovered through structured interviews with content moderation experts. We operationalize the common strategies in deliberation used by real content moderators into a human-in-the-loop framework called "Agile Deliberation" that explicitly supports evolving and subjective concepts. The system supports users in defining the concept for themselves by exposing them to borderline cases. The system does this with two deliberation stages: (1) concept scoping, which decomposes the initial concept into a structured hierarchy of sub-concepts, and (2) concept iteration, which surfaces semantically borderline examples for user reflection and feedback to iteratively align an image classifier with the user's evolving intent. Since concept deliberation is inherently subjective and interactive, we painstakingly evaluate the framework through 18 user sessions, each 1.5h long, rather than standard benchmarking datasets. We find that Agile Deliberation achieves 7.5% higher F1 scores than automated decomposition baselines and more than 3% higher than manual deliberation, while participants reported clearer conceptual understanding and lower cognitive effort.

</details>


### [50] [V-OCBF: Learning Safety Filters from Offline Data via Value-Guided Offline Control Barrier Functions](https://arxiv.org/abs/2512.10822)
*Mumuksh Tayal,Manan Tayal,Aditya Singh,Shishir Kolathaya,Ravi Prakash*

Main category: cs.AI

TL;DR: 提出V - OCBF框架，从离线演示中学习神经CBF，在多案例研究中减少安全违规且保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有安全离线强化学习方法不能保证前向不变性，控制障碍函数依赖专家设计或系统动力学知识，需要新方法确保自主系统安全。

Method: 引入V - OCBF框架，推导递归有限差分障碍更新实现无模型学习，结合基于期望分位数的目标，用二次规划合成实时安全控制。

Result: 在多个案例研究中，V - OCBF比基线方法的安全违规显著减少，同时保持了较强的任务性能。

Conclusion: V - OCBF可扩展性强，能在无在线交互和手工设计障碍的情况下进行安全关键控制器的离线合成。

Abstract: Ensuring safety in autonomous systems requires controllers that satisfy hard, state-wise constraints without relying on online interaction. While existing Safe Offline RL methods typically enforce soft expected-cost constraints, they do not guarantee forward invariance. Conversely, Control Barrier Functions (CBFs) provide rigorous safety guarantees but usually depend on expert-designed barrier functions or full knowledge of the system dynamics. We introduce Value-Guided Offline Control Barrier Functions (V-OCBF), a framework that learns a neural CBF entirely from offline demonstrations. Unlike prior approaches, V-OCBF does not assume access to the dynamics model; instead, it derives a recursive finite-difference barrier update, enabling model-free learning of a barrier that propagates safety information over time. Moreover, V-OCBF incorporates an expectile-based objective that avoids querying the barrier on out-of-distribution actions and restricts updates to the dataset-supported action set. The learned barrier is then used with a Quadratic Program (QP) formulation to synthesize real-time safe control. Across multiple case studies, V-OCBF yields substantially fewer safety violations than baseline methods while maintaining strong task performance, highlighting its scalability for offline synthesis of safety-critical controllers without online interaction or hand-engineered barriers.

</details>


### [51] [LLMs Can Assist with Proposal Selection at Large User Facilities](https://arxiv.org/abs/2512.10895)
*Lijie Ding,Janell Thomson,Jon Taylor,Changwoo Do*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We explore how large language models (LLMs) can enhance the proposal selection process at large user facilities, offering a scalable, consistent, and cost-effective alternative to traditional human review. Proposal selection depends on assessing the relative strength among submitted proposals; however, traditional human scoring often suffers from weak inter-proposal correlations and is subject to reviewer bias and inconsistency. A pairwise preference-based approach is logically superior, providing a more rigorous and internally consistent basis for ranking, but its quadratic workload makes it impractical for human reviewers. We address this limitation using LLMs. Leveraging the uniquely well-curated proposals and publication records from three beamlines at the Spallation Neutron Source (SNS), Oak Ridge National Laboratory (ORNL), we show that the LLM rankings correlate strongly with the human rankings (Spearman $ρ\simeq 0.2-0.8$, improving to $\geq 0.5$ after 10\% outlier removal). Moreover, LLM performance is no worse than that of human reviewers in identifying proposals with high publication potential, while costing over two orders of magnitude less. Beyond ranking, LLMs enable advanced analyses that are challenging for humans, such as quantitative assessment of proposal similarity via embedding models, which provides information crucial for review committees.

</details>


### [52] [Multi-Granular Node Pruning for Circuit Discovery](https://arxiv.org/abs/2512.10903)
*Muhammad Umair Haider,Hammad Rizwan,Hassan Sajjad,A. B. Siddique*

Main category: cs.AI

TL;DR: 提出用于电路发现的节点级剪枝框架，解决可扩展性和粒度限制问题，发现更小节点电路，内存占用显著降低。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型电路发现方法依赖迭代边剪枝，计算成本高，且粒度粗，忽略单个神经元等精细结构。

Method: 提出在统一优化目标下，跨多个粒度级别引入可学习掩码，利用特定粒度的稀疏惩罚项指导剪枝过程。

Result: 发现的电路节点更小；证明粗粒度方法认为重要的许多神经元实际无关；仍能维持任务性能；内存占用降低5 - 10倍。

Conclusion: 所提节点级剪枝框架能有效解决现有方法的可扩展性和粒度限制问题，且更具内存效率。

Abstract: Circuit discovery aims to identify minimal subnetworks that are responsible for specific behaviors in large language models (LLMs). Existing approaches primarily rely on iterative edge pruning, which is computationally expensive and limited to coarse-grained units such as attention heads or MLP blocks, overlooking finer structures like individual neurons. We propose a node-level pruning framework for circuit discovery that addresses both scalability and granularity limitations. Our method introduces learnable masks across multiple levels of granularity, from entire blocks to individual neurons, within a unified optimization objective. Granularity-specific sparsity penalties guide the pruning process, allowing a comprehensive compression in a single fine-tuning run. Empirically, our approach identifies circuits that are smaller in nodes than those discovered by prior methods; moreover, we demonstrate that many neurons deemed important by coarse methods are actually irrelevant, while still maintaining task performance. Furthermore, our method has a significantly lower memory footprint, 5-10x, as it does not require keeping intermediate activations in the memory to work.

</details>


### [53] [On Decision-Making Agents and Higher-Order Causal Processes](https://arxiv.org/abs/2512.10937)
*Matt Wilson*

Main category: cs.AI

TL;DR: 建立部分可观测马尔可夫决策过程（POMDPs）中决策代理与单输入过程函数的精确对应，拓展到多智能体系统。


<details>
  <summary>Details</summary>
Motivation: 探索POMDPs中决策代理与过程函数的关系，以及拓展到多智能体系统。

Method: 通过建立对应关系，将代理的策略和内存更新组合成过程函数，通过链接积与环境交互。

Result: 建立对应关系，提出双重解释，将视角拓展到多智能体系统。

Conclusion: 可将观察独立的分散式POMDPs视为多输入过程函数的自然领域。

Abstract: We establish a precise correspondence between decision-making agents in partially observable Markov decision processes (POMDPs) and one-input process functions, the classical limit of higher-order quantum operations. In this identification an agent's policy and memory update combine into a process function w that interacts with a POMDP environment via the link product. This suggests a dual interpretation: in the physics view, the process function acts as the environment into which local operations (agent interventions) are inserted, whereas in the AI view it encodes the agent and the inserted functions represent environments. We extend this perspective to multi-agent systems by identifying observation-independent decentralized POMDPs as natural domains for multi-input process functions.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [54] [An Efficient Graph-Transformer Operator for Learning Physical Dynamics with Manifolds Embedding](https://arxiv.org/abs/2512.10227)
*Pengwei Liu,Xingyu Ren,Pengkai Wang,Hangjie Yuan,Zhongkai Hao,Guanyu Chen,Chao Xu,Dong Ni,Shengze Cai*

Main category: cs.CE

TL;DR: 传统数值求解器处理复杂物理模拟计算成本高，现有深度学习方法灵活性和泛化性不足。提出PhysGTO，在物理和潜在空间学习物理动态，在多个数据集上实现高精度并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统数值求解器处理复杂动态场景模拟计算成本高，现有深度学习方法在非结构化网格上灵活性和泛化性差，限制实际应用。

Method: 提出PhysGTO，在物理空间用统一图嵌入模块处理异构输入；在潜在空间结合轻量级通量消息传递和投影注意力捕捉依赖关系，确保线性复杂度。

Result: 在十一个数据集组成的综合基准上，PhysGTO始终达到最先进精度，显著降低计算成本。

Conclusion: PhysGTO在广泛模拟任务中展现出卓越的灵活性、可扩展性和泛化性。

Abstract: Accurate and efficient physical simulations are essential in science and engineering, yet traditional numerical solvers face significant challenges in computational cost when handling simulations across dynamic scenarios involving complex geometries, varying boundary/initial conditions, and diverse physical parameters. While deep learning offers promising alternatives, existing methods often struggle with flexibility and generalization, particularly on unstructured meshes, which significantly limits their practical applicability. To address these challenges, we propose PhysGTO, an efficient Graph-Transformer Operator for learning physical dynamics through explicit manifold embeddings in both physical and latent spaces. In the physical space, the proposed Unified Graph Embedding module aligns node-level conditions and constructs sparse yet structure-preserving graph connectivity to process heterogeneous inputs. In the latent space, PhysGTO integrates a lightweight flux-oriented message-passing scheme with projection-inspired attention to capture local and global dependencies, facilitating multilevel interactions among complex physical correlations. This design ensures linear complexity relative to the number of mesh points, reducing both the number of trainable parameters and computational costs in terms of floating-point operations (FLOPs), and thereby allowing efficient inference in real-time applications. We introduce a comprehensive benchmark spanning eleven datasets, covering problems with unstructured meshes, transient flow dynamics, and large-scale 3D geometries. PhysGTO consistently achieves state-of-the-art accuracy while significantly reducing computational costs, demonstrating superior flexibility, scalability, and generalization in a wide range of simulation tasks.

</details>


### [55] [Integrated Planning and Machine-Level Scheduling for High-Mix Discrete Manufacturing: A Profit-Driven Heuristic Framework](https://arxiv.org/abs/2512.10358)
*Runhao Liu,Ziming Chen,You Li,Zequn Xie,Peng Zhang*

Main category: cs.CE

TL;DR: 论文提出利润驱动集成框架解决高混合离散制造系统计划与调度问题，评估显示稳定性执行策略效果佳。


<details>
  <summary>Details</summary>
Motivation: 现代制造企业在多品种、小批量和急单条件下难制定高效可靠生产计划，高混合离散制造系统需联合优化中期生产计划和机器层调度。

Method: 采用利润驱动集成框架，规划层在约束下分配生产等，调度层细化分配。

Result: 灵活机器层执行方案准时完成率73.3%且外包需求大，稳定执行策略准时完成率100%、无外包且机器利用率平衡。

Conclusion: 规划决策与稳定性执行规则结合能在复杂制造环境中实现实用且可解释的利润最大化决策。

Abstract: Modern manufacturing enterprises struggle to create efficient and reliable production schedules under multi-variety, small-batch, and rush-order conditions. High-mix discrete manufacturing systems require jointly optimizing mid-term production planning and machine-level scheduling under heterogeneous resources and stringent delivery commitments. We address this problem with a profit-driven integrated framework that couples a mixed-integer planning model with a machine-level scheduling heuristic. The planning layer allocates production, accessory co-production, and outsourcing under aggregate economic and capacity constraints, while the scheduling layer refines these allocations using a structure-aware procedure that enforces execution feasibility and stabilizes daily machine behavior. This hierarchical design preserves the tractability of aggregated optimization while capturing detailed operational restrictions. Evaluations are conducted on a real industrial scenario. A flexible machine-level execution scheme yields 73.3% on-time completion and significant outsourcing demand, revealing bottleneck congestion. In contrast, a stability-enforcing execution policy achieves 100% on-time completion, eliminates all outsourcing, and maintains balanced machine utilization with only 1.9 to 4.6% capacity loss from changeovers. These results show that aligning planning decisions with stability-oriented execution rules enables practical and interpretable profit-maximizing decisions in complex manufacturing environments.

</details>


### [56] [Robust Crop Planning under Uncertainty: Aligning Economic Optimality with Agronomic Sustainability](https://arxiv.org/abs/2512.10396)
*Runhao Liu,Ziming Chen,You Li,Peng Zhang*

Main category: cs.CE

TL;DR: 提出多层鲁棒作物规划框架应对复杂农业规划挑战，经华北农场数据验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有方法不能保证面对市场和气候波动的弹性，需解决复杂条件下的作物分配优化问题。

Method: 提出MLRCPF，通过结构化交互矩阵处理作物关系，采用分布鲁棒优化层应对风险。

Result: 自动生成可持续轮作模式，提高豆类种植比例，解决了最优性与稳定性的权衡。

Conclusion: 在复杂农业系统中，将特定领域结构先验明确编码到优化模型对弹性决策很重要。

Abstract: Long-horizon agricultural planning requires optimizing crop allocation under complex spatial heterogeneity, temporal agronomic dependencies, and multi-source environmental uncertainty. Existing approaches often treat crop interactions, such as legume-cereal complementarity, which implicitly or rely on static deterministic formulations that fail to guarantee resilience against market and climate volatility. To address these challenges, we propose a Multi-Layer Robust Crop Planning Framework (MLRCPF) that integrates spatial reasoning, temporal dynamics, and robust optimization. Specifically, we formalize crop-to-crop relationships through a structured interaction matrix embedded within the state-transition logic, and employ a distributionally robust optimization layer to mitigate worst-case risks defined by a data-driven ambiguity set. Evaluations on a real-world high-mix farming dataset from North China demonstrate the effectiveness of the proposed approach. The framework autonomously generates sustainable checkerboard rotation patterns that restore soil fertility, significantly increasing the legume planting ratio compared to deterministic baselines. Economically, it successfully resolves the trade-off between optimality and stability. These results highlight the importance of explicitly encoding domain-specific structural priors into optimization models for resilient decision-making in complex agricultural systems.

</details>


### [57] [HypeR Adaptivity: Joint $hr$-Adaptive Meshing via Hypergraph Multi-Agent Deep Reinforcement Learning](https://arxiv.org/abs/2512.10439)
*Niccolò Grillo,James Rowbottom,Pietro Liò,Carola Bibiane Schönlieb,Stefania Fresca*

Main category: cs.CE

TL;DR: 提出HypeR框架联合优化网格重定位和细化，在基准PDEs上降低近似误差，提升网格生成能力。


<details>
  <summary>Details</summary>
Motivation: 传统自适应网格细化方法有计算瓶颈和成本 - 精度权衡限制，现有机器学习方法单独处理h或r自适应，需要联合优化的方法。

Method: 使用超图神经网络和多智能体强化学习工具，将细化问题建模为异构多智能体马尔可夫决策过程，重定位采用基于各向异性扩散的策略。

Result: 在基准PDEs上，与最先进的h自适应基线相比，在相近元素数量下将近似误差降低6 - 10倍，生成的网格形状指标和与解的各向异性对齐度更好。

Conclusion: 联合学习的hr自适应策略可显著增强自动网格生成能力。

Abstract: Adaptive mesh refinement is central to the efficient solution of partial differential equations (PDEs) via the finite element method (FEM). Classical $r$-adaptivity optimizes vertex positions but requires solving expensive auxiliary PDEs such as the Monge-Ampère equation, while classical $h$-adaptivity modifies topology through element subdivision but suffers from expensive error indicator computation and is constrained by isotropic refinement patterns that impose accuracy ceilings. Combined $hr$-adaptive techniques naturally outperform single-modality approaches, yet inherit both computational bottlenecks and the restricted cost-accuracy trade-off. Emerging machine learning methods for adaptive mesh refinement seek to overcome these limitations, but existing approaches address $h$-adaptivity or $r$-adaptivity in isolation. We present HypeR, a deep reinforcement learning framework that jointly optimizes mesh relocation and refinement. HypeR casts the joint adaptation problem using tools from hypergraph neural networks and multi-agent reinforcement learning. Refinement is formulated as a heterogeneous multi-agent Markov decision process (MDP) where element agents decide discrete refinement actions, while relocation follows an anisotropic diffusion-based policy on vertex agents with provable prevention of mesh tangling. The reward function combines local and global error reduction to promote general accuracy. Across benchmark PDEs, HypeR reduces approximation error by up to 6--10$\times$ versus state-of-art $h$-adaptive baselines at comparable element counts, breaking through the uniform refinement accuracy ceiling that constrains subdivision-only methods. The framework produces meshes with improved shape metrics and alignment to solution anisotropy, demonstrating that jointly learned $hr$-adaptivity strategies can substantially enhance the capabilities of automated mesh generation.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [58] [PANDAExpress: a Simpler and Faster PANDA Algorithm](https://arxiv.org/abs/2512.10217)
*Mahmoud Abo Khamis,Hung Q. Ngo,Dan Suciu*

Main category: cs.DB

TL;DR: 介绍了PANDA算法处理查询的能力及弱点，提出PANDAExpress算法解决其弱点，去除运行时间中的polylog(N)因子。


<details>
  <summary>Details</summary>
Motivation: PANDA算法运行时间存在大的polylog(N)因子，不实用，需解决此弱点。

Method: 证明新的概率不等式上界DDRs输出大小；基于此不等式提出PANDAExpress算法，采用基于数据偏斜统计动态构建的任意超平面切割的分区方案。

Result: PANDAExpress算法去除了PANDA运行时间中的polylog(N)因子，运行时间与复杂的专用算法相当。

Conclusion: PANDAExpress算法在保留PANDA通用性和强大功能的同时，解决了其不实用问题。

Abstract: PANDA is a powerful generic algorithm for answering conjunctive queries (CQs) and disjunctive datalog rules (DDRs) given input degree constraints. In the special case where degree constraints are cardinality constraints and the query is Boolean, PANDA runs in $\tilde O (N^{subw})$-time, where $N$ is the input size, and $subw$ is the submodular width of the query, a notion introduced by Daniel Marx (JACM 2013). When specialized to certain classes of sub-graph pattern finding problems, the $\tilde O(N^{subw})$ runtime matches the optimal runtime possible, modulo some conjectures in fine-grained complexity (Bringmann and Gorbachev (STOC 25)). The PANDA framework is much more general, as it handles arbitrary input degree constraints, which capture common statistics and integrity constraints used in relational database management systems, it works for queries with free variables, and for both CQs and DDRs.
  The key weakness of PANDA is the large $polylog(N)$-factor hidden in the $\tilde O(\cdot)$ notation. This makes PANDA completely impractical, and fall short of what is achievable with specialized algorithms. This paper resolves this weakness with two novel ideas. First, we prove a new probabilistic inequality that upper-bounds the output size of DDRs under arbitrary degree constraints. Second, the proof of this inequality directly leads to a new algorithm named PANDAExpress that is both simpler and faster than PANDA. The novel feature of PANDAExpress is a new partitioning scheme that uses arbitrary hyperplane cuts instead of axis-parallel hyperplanes used in PANDA. These hyperplanes are dynamically constructed based on data-skewness statistics carefully tracked throughout the algorithm's execution. As a result, PANDAExpress removes the $polylog(N)$-factor from the runtime of PANDA, matching the runtimes of intricate specialized algorithms, while retaining all its generality and power.

</details>


### [59] [Efficient Hypergraph Pattern Matching via Match-and-Filter and Intersection Constraint](https://arxiv.org/abs/2512.10621)
*Siwoo Song,Wonseok Shin,Kunsoo Park,Giuseppe F. Italiano,Zhengyi Yang,Wenjie Zhang*

Main category: cs.DB

TL;DR: 提出一种新的超图模式匹配算法，引入交集约束、候选超边空间和匹配过滤框架，实验显示该算法性能大幅优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 超图能建模多顶点复杂关系，超图模式匹配是基础问题，需高效算法。

Method: 引入交集约束加速验证；采用候选超边空间存储可能映射；使用匹配过滤框架，在回溯过程中维护候选超边空间。

Result: 在真实数据集实验中，新算法在查询处理时间上比现有算法快几个数量级。

Conclusion: 所提出的新算法在超图模式匹配上性能显著，优于当前最先进算法。

Abstract: A hypergraph is a generalization of a graph, in which a hyperedge can connect multiple vertices, modeling complex relationships involving multiple vertices simultaneously. Hypergraph pattern matching, which is to find all isomorphic embeddings of a query hypergraph in a data hypergraph, is one of the fundamental problems. In this paper, we present a novel algorithm for hypergraph pattern matching by introducing (1) the intersection constraint, a necessary and sufficient condition for valid embeddings, which significantly speeds up the verification process, (2) the candidate hyperedge space, a data structure that stores potential mappings between hyperedges in the query hypergraph and the data hypergraph, and (3) the Match-and-Filter framework, which interleaves matching and filtering operations to maintain only compatible candidates in the candidate hyperedge space during backtracking. Experimental results on real-world datasets demonstrate that our algorithm significantly outperforms the state-of-the-art algorithms, by up to orders of magnitude in terms of query processing time.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [60] [A study of the spectrum resource leasing method based on ERC4907 extension](https://arxiv.org/abs/2512.09942)
*Zhiming Liang,Bin Chen,Litao Ye,Chen Sun,Shuo Wang,Zhe Peng*

Main category: cs.DC

TL;DR: 本文针对ERC4907标准在多时段调度场景的局限，提出M - ERC4907扩展方法，实验表明该方法提升了可扩展性和资源分配效率。


<details>
  <summary>Details</summary>
Motivation: ERC4907标准仅支持单用户、单时段授权，在去中心化多时段调度场景中适用性和效率受限。

Method: 提出M - ERC4907扩展方法，引入新功能支持多时段批量配置和多用户同时授权。

Result: 在Remix开发平台实验显示，M - ERC4907方法显著减少链上交易和总Gas消耗。

Conclusion: M - ERC4907方法提升了可扩展性和资源分配效率。

Abstract: The ERC4907 standard enables rentable Non-Fungible Tokens (NFTs) but is limited to single-user, single-time-slot authorization, which severely limits its applicability and efficiency in decentralized multi-slot scheduling scenarios. To address this limitation, this paper proposes Multi-slot ERC4907 (M-ERC4907) extension method. The M-ERC4907 method introduces novel functionalities to support the batch configuration of multiple time slots and simultaneous authorization of multiple users, thereby effectively eliminating the rigid sequential authorization constraint of ERC4907. The experiment was conducted on the Remix development platform. Experimental results show that the M-ERC4907 method significantly reduces on-chain transactions and overall Gas consumption, leading to enhanced scalability and resource allocation efficiency.

</details>


### [61] [ELANA: A Simple Energy and Latency Analyzer for LLMs](https://arxiv.org/abs/2512.09946)
*Hung-Yueh Chiang,Bokun Wang,Diana Marculescu*

Main category: cs.DC

TL;DR: 为解决大语言模型在不同硬件平台服务时的延迟和功耗问题，开源了分析工具ELANA，介绍其特点并给出开源地址。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在不同硬件平台服务时，延迟和功耗是主要限制，基准测试对优化效率很关键，因此需要开发评估工具。

Method: 开源一个轻量级、学术友好的分析工具ELANA，可分析模型多项指标，支持Hugging Face上的公开模型，有简单命令行界面和能耗日志功能，兼容Hugging Face API，可定制。

Result: 成功开发并开源ELANA工具。

Conclusion: ELANA适合高效大语言模型研究或小规模概念验证研究。

Abstract: The latency and power consumption of large language models (LLMs) are major constraints when serving them across a wide spectrum of hardware platforms, from mobile edge devices to cloud GPU clusters. Benchmarking is crucial for optimizing efficiency in both model deployment and next-generation model development. To address this need, we open-source a simple profiling tool, \textbf{ELANA}, for evaluating LLMs. ELANA is designed as a lightweight, academic-friendly profiler for analyzing model size, key-value (KV) cache size, prefilling latency (Time-to-first-token, TTFT), generation latency (Time-per-output-token, TPOT), and end-to-end latency (Time-to-last-token, TTLT) of LLMs on both multi-GPU and edge GPU platforms. It supports all publicly available models on Hugging Face and offers a simple command-line interface, along with optional energy consumption logging. Moreover, ELANA is fully compatible with popular Hugging Face APIs and can be easily customized or adapted to compressed or low bit-width models, making it ideal for research on efficient LLMs or for small-scale proof-of-concept studies. We release the ELANA profiling tool at: https://github.com/enyac-group/Elana.

</details>


### [62] [CloudFix: Automated Policy Repair for Cloud Access Control Policies Using Large Language Models](https://arxiv.org/abs/2512.09957)
*Bethel Hall,Owen Ungaro,William Eiers*

Main category: cs.DC

TL;DR: 介绍首个结合形式化方法与大语言模型的云访问控制自动策略修复框架CloudFix，评估显示其能提高修复准确率，并公开工具和数据集。


<details>
  <summary>Details</summary>
Motivation: 云管理员手动编写和更新访问控制策略易出错、耗时且可能导致安全漏洞，现有符号分析方法通用性有限，大语言模型在修复云访问控制策略方面未被探索。

Method: CloudFix采用基于形式化方法的故障定位识别策略中的错误语句，利用大语言模型生成潜在修复方案，再用SMT求解器验证。

Result: 通过对282个真实AWS访问控制策略的实验，CloudFix在不同请求大小下比基线实现提高了修复准确率。

Conclusion: 首次利用大语言模型进行策略修复，证明了大语言模型在访问控制中的有效性，实现了云访问控制策略的高效自动修复。

Abstract: Access control policies are vital for securing modern cloud computing, where organizations must manage access to sensitive data across thousands of users in distributed system settings. Cloud administrators typically write and update policies manually, which can be an error-prone and time-consuming process and can potentially lead to security vulnerabilities. Existing approaches based on symbolic analysis have demonstrated success in automated debugging and repairing access control policies; however, their generalizability is limited in the context of cloud-based access control. Conversely, Large Language Models (LLMs) have been utilized for automated program repair; however, their applicability to repairing cloud access control policies remains unexplored. In this work, we introduce CloudFix, the first automated policy repair framework for cloud access control that combines formal methods with LLMs. Given an access control policy and a specification of allowed and denied access requests, CloudFix employs Formal Methods-based Fault Localization to identify faulty statements in the policy and leverages LLMs to generate potential repairs, which are then verified using SMT solvers. To evaluate CloudFix, we curated a dataset of 282 real-world AWS access control policies extracted from forum posts and augmented them with synthetically generated request sets based on real scenarios. Our experimental results show that CloudFix improves repair accuracy over a Baseline implementation across varying request sizes. Our work is the first to leverage LLMs for policy repair, showcasing the effectiveness of LLMs for access control and enabling efficient and automated repair of cloud access control policies. We make our tool Cloudfix and AWS dataset publicly available.

</details>


### [63] [TDC-Cache: A Trustworthy Decentralized Cooperative Caching Framework for Web3.0](https://arxiv.org/abs/2512.09961)
*Jinyu Chen,Long Shi,Taotao Wang,Jiaheng Wang,Wei Zhang*

Main category: cs.DC

TL;DR: 提出 Web3.0 的 TDC - Cache 框架及相关策略，实验显示性能提升，是该领域首次探索。


<details>
  <summary>Details</summary>
Motivation: 应对 Web3.0 去中心化数据访问中冗余数据复制的效率问题和数据不一致的安全漏洞。

Method: 开发 TDC - Cache 框架，采用两层架构；提出 DRL - DC 动态优化缓存策略；开发 PoCL 共识维护缓存决策一致性。

Result: 与现有方法相比，平均访问延迟降低 20%，缓存命中率最多提高 18%，平均成功共识率提高 10%。

Conclusion: 本文是对 Web3.0 去中心化缓存框架和策略的首次探索。

Abstract: The rapid growth of Web3.0 is transforming the Internet from a centralized structure to decentralized, which empowers users with unprecedented self-sovereignty over their own data. However, in the context of decentralized data access within Web3.0, it is imperative to cope with efficiency concerns caused by the replication of redundant data, as well as security vulnerabilities caused by data inconsistency. To address these challenges, we develop a Trustworthy Decentralized Cooperative Caching (TDC-Cache) framework for Web3.0 to ensure efficient caching and enhance system resilience against adversarial threats. This framework features a two-layer architecture, wherein the Decentralized Oracle Network (DON) layer serves as a trusted intermediary platform for decentralized caching, bridging the contents from decentralized storage and the content requests from users. In light of the complexity of Web3.0 network topologies and data flows, we propose a Deep Reinforcement Learning-Based Decentralized Caching (DRL-DC) for TDC-Cache to dynamically optimize caching strategies of distributed oracles. Furthermore, we develop a Proof of Cooperative Learning (PoCL) consensus to maintain the consistency of decentralized caching decisions within DON. Experimental results show that, compared with existing approaches, the proposed framework reduces average access latency by 20%, increases the cache hit rate by at most 18%, and improves the average success consensus rate by 10%. Overall, this paper serves as a first foray into the investigation of decentralized caching framework and strategy for Web3.0.

</details>


### [64] [GoodSpeed: Optimizing Fair Goodput with Adaptive Speculative Decoding in Distributed Edge Inference](https://arxiv.org/abs/2512.09963)
*Phuong Tran,Tzu-Hao Liu,Long Tan Le,Tung-Anh Nguyen,Van Quan La,Eason Yu,Han Shu,Choong Seon Hong,Nguyen H. Tran*

Main category: cs.DC

TL;DR: 本文提出GOODSPEED框架优化大语言模型分布式推理的有效吞吐量和公平性，经分析在稳态和动态负载下均有良好表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型实时推理计算需求高，现有投机解码技术在多用户环境中有效吞吐量和各草稿服务器公平性难以保证。

Method: 引入GOODSPEED分布式推理框架，采用中央验证服务器协调异构草稿服务器，结合梯度调度算法分配验证任务。

Result: 经流体样本路径分析，GOODSPEED在稳态下能收敛到最优有效吞吐量分配，动态负载下性能接近最优且误差有界。

Conclusion: GOODSPEED为分布式大语言模型推理系统的多服务器投机解码提供了可扩展、公平且高效的解决方案。

Abstract: Large language models (LLMs) have revolutionized natural language processing, yet their high computational demands pose significant challenges for real-time inference, especially in multi-user server speculative decoding and resource-constrained environments. Speculative decoding has emerged as a promising technique to accelerate LLM inference by using lightweight draft models to generate candidate tokens, which are subsequently verified by a larger, more accurate model. However, ensuring both high goodput (the effective rate of accepted tokens) and fairness across multiple draft servers cooperating with a central verification server remains an open challenge. This paper introduces GOODSPEED, a novel distributed inference framework that optimizes goodput through adaptive speculative decoding. GOODSPEED employs a central verification server that coordinates a set of heterogeneous draft servers, each running a small language model to generate speculative tokens. To manage resource allocation effectively, GOODSPEED incorporates a gradient scheduling algorithm that dynamically assigns token verification tasks, maximizing a logarithmic utility function to ensure proportional fairness across servers. By processing speculative outputs from all draft servers in parallel, the framework enables efficient collaboration between the verification server and distributed draft generators, streamlining both latency and throughput. Through rigorous fluid sample path analysis, we show that GOODSPEED converges to the optimal goodput allocation in steady-state conditions and maintains near-optimal performance with provably bounded error under dynamic workloads. These results demonstrate that GOODSPEED provides a scalable, fair and efficient solution for multi-server speculative decoding in distributed LLM inference systems.

</details>


### [65] [Design Space Exploration of DMA based Finer-Grain Compute Communication Overlap](https://arxiv.org/abs/2512.10236)
*Shagnik Pal,Shaizeen Aga,Suchita Pati,Mahzabeen Islam,Lizy K. John*

Main category: cs.DC

TL;DR: 本文提出细粒度计算 - 通信重叠技术FiCCO，分析操作低效损失，设计调度方案和启发式方法，通过GPU DMA引擎减少争用低效，实验显示有速度提升和准确指导。


<details>
  <summary>Details</summary>
Motivation: 现有分布式ML并行化技术存在数据依赖的通信和计算操作，导致性能损失，粗粒度重叠有局限。

Method: 提出FiCCO实现细粒度计算 - 通信重叠，分析操作低效损失，设计FiCCO调度方案和启发式方法，将通信卸载到GPU DMA引擎。

Result: 提出的定制调度方案实现了高达1.6倍的加速，启发式方法在81%的未见场景中提供了准确的指导。

Conclusion: FiCCO技术能有效提升分布式ML并行化的性能，通过合理设计调度和利用GPU DMA引擎可减少低效损失。

Abstract: As both ML training and inference are increasingly distributed, parallelization techniques that shard (divide) ML model across GPUs of a distributed system, are often deployed. With such techniques, there is a high prevalence of data-dependent communication and computation operations where communication is exposed, leaving as high as 1.7x ideal performance on the table. Prior works harness the fact that ML model state and inputs are already sharded, and employ careful overlap of individual computation/communication shards. While such coarse-grain overlap is promising, in this work, we instead make a case for finer-grain compute-communication overlap which we term FiCCO, where we argue for finer-granularity, one-level deeper overlap than at shard-level, to unlock compute/communication overlap for a wider set of network topologies, finer-grain dataflow and more. We show that FiCCO opens up a wider design space of execution schedules than possible at shard-level alone. At the same time, decomposition of ML operations into smaller operations (done in both shard-based and finer-grain techniques) causes operation-level inefficiency losses. To balance the two, we first present a detailed characterization of these inefficiency losses, then present a design space of FiCCO schedules, and finally overlay the schedules with concomitant inefficiency signatures. Doing so helps us design heuristics that frameworks and runtimes can harness to select bespoke FiCCO schedules based on the nature of underlying ML operations. Finally, to further minimize contention inefficiencies inherent with operation overlap, we offload communication to GPU DMA engines. We evaluate several scenarios from realistic ML deployments and demonstrate that our proposed bespoke schedules deliver up to 1.6x speedup and our heuristics provide accurate guidance in 81% of unseen scenarios.

</details>


### [66] [Hybrid Learning and Optimization-Based Dynamic Scheduling for DL Workloads on Heterogeneous GPU Clusters](https://arxiv.org/abs/2512.10271)
*Shruti Dongare,Redwan Ibne Seraj Khan,Hadeel Albahar,Nannan Zhao,Diego Melendez Maita,Ali R. Butt*

Main category: cs.DC

TL;DR: 提出应用无关的基于强化学习的调度框架RLTune，可优化异构GPU集群上的DL作业调度，提升利用率、减少延迟和完成时间。


<details>
  <summary>Details</summary>
Motivation: 现代云平台承载大规模深度学习工作负载，现有调度器在GPU集群异构性和应用特征可见性方面面临挑战。

Method: 将强化学习驱动的优先级排序与基于MILP的作业到节点映射相结合。

Result: 在大规模生产轨迹上训练，提升GPU利用率达20%，减少排队延迟达81%，缩短作业完成时间达70%。

Conclusion: RLTune能跨不同工作负载泛化，无需逐作业分析，适合云提供商大规模部署。

Abstract: Modern cloud platforms increasingly host large-scale deep learning (DL) workloads, demanding high-throughput, low-latency GPU scheduling. However, the growing heterogeneity of GPU clusters and limited visibility into application characteristics pose major challenges for existing schedulers, which often rely on offline profiling or application-specific assumptions. We present RLTune, an application-agnostic reinforcement learning (RL)-based scheduling framework that dynamically prioritizes and allocates DL jobs on heterogeneous GPU clusters. RLTune integrates RL-driven prioritization with MILP-based job-to-node mapping to optimize system-wide objectives such as job completion time (JCT), queueing delay, and resource utilization. Trained on large-scale production traces from Microsoft Philly, Helios, and Alibaba, RLTune improves GPU utilization by up to 20%, reduces queueing delay by up to 81%, and shortens JCT by as much as 70 percent. Unlike prior approaches, RLTune generalizes across diverse workloads without requiring per-job profiling, making it practical for cloud providers to deploy at scale for more efficient, fair, and sustainable DL workload management.

</details>


### [67] [High-Dimensional Data Processing: Benchmarking Machine Learning and Deep Learning Architectures in Local and Distributed Environments](https://arxiv.org/abs/2512.10312)
*Julian Rodriguez,Piotr Lopez,Emiliano Lerma,Rafael Medrano,Jacobo Hernandez*

Main category: cs.DC

TL;DR: 文档汇报大数据课程实践序列与方法，涵盖数据集处理、文本和电影特征分析及分布式计算集群技术实现


<details>
  <summary>Details</summary>
Motivation: 记录大数据课程中的实践经验

Method: 通过分组和个人策略处理Epsilon数据集，用RestMex进行文本分析和分类，用IMDb分析电影特征，在Linux上用Scala和Apache Spark实现分布式计算集群

Result: 未提及

Conclusion: 未提及

Abstract: This document reports the sequence of practices and methodologies implemented during the Big Data course. It details the workflow beginning with the processing of the Epsilon dataset through group and individual strategies, followed by text analysis and classification with RestMex and movie feature analysis with IMDb. Finally, it describes the technical implementation of a distributed computing cluster with Apache Spark on Linux using Scala.

</details>


### [68] [Making Wide Stripes Practical: Cascaded Parity LRCs for Efficient Repair and High Reliability](https://arxiv.org/abs/2512.10425)
*Fan Yu,Guodong Li,Si Wu,Weijun Fang,Sihuang Hu*

Main category: cs.DC

TL;DR: 本文提出级联奇偶校验局部可修复码（CP - LRCs），可降低宽条带擦除编码修复成本，在阿里云评估中减少了单节点和双节点故障修复时间。


<details>
  <summary>Details</summary>
Motivation: 现有局部可修复码（LRCs）在宽条带设置下存在结构局限性，如单节点修复成本高、多节点故障全局修复昂贵和可靠性急剧下降等问题。

Method: 提出CP - LRCs，通过将全局奇偶校验块分解到所有局部奇偶校验块中嵌入奇偶校验块之间的结构化依赖关系，提供通用系数生成框架，开发利用级联的修复算法，并实例化CP - Azure和CP - Uniform。

Result: 在阿里云上的评估显示，单节点故障修复时间最多减少41%，两节点故障修复时间最多减少26%。

Conclusion: CP - LRCs能在保持MDS级容错的同时，实现低带宽的单节点和多节点修复。

Abstract: Erasure coding with wide stripes is increasingly adopted to reduce storage overhead in large-scale storage systems. However, existing Locally Repairable Codes (LRCs) exhibit structural limitations in this setting: inflated local groups increase single-node repair cost, multi-node failures frequently trigger expensive global repair, and reliability degrades sharply. We identify a key root cause: local and global parity blocks are designed independently, preventing them from cooperating during repair. We present Cascaded Parity LRCs (CP-LRCs), a new family of wide stripe LRCs that embed structured dependency between parity blocks by decomposing a global parity block across all local parity blocks. This creates a cascaded parity group that preserves MDS-level fault tolerance while enabling low-bandwidth single-node and multi-node repairs. We provide a general coefficient-generation framework, develop repair algorithms exploiting cascading, and instantiate the design with CP-Azure and CP-Uniform. Evaluations on Alibaba Cloud show reductions in repair time of up to 41% for single-node failures and 26% for two-node failures.

</details>


### [69] [Clustered Federated Learning with Hierarchical Knowledge Distillation](https://arxiv.org/abs/2512.10443)
*Sabtain Ahmad,Meerzhan Kanatbekova,Ivona Brandic,Atakan Aral*

Main category: cs.DC

TL;DR: 本文提出CFLHKD方案用于聚类联邦学习，采用双级聚合，实验表明其在模型准确性上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统CFL方法存在学习碎片化问题且未利用集群见解，新型分层CFL虽提升效率但有通信挑战，需新的个性化方案。

Method: 提出CFLHKD方案，基于多教师知识蒸馏，采用双级聚合实现集群间知识共享和个性化。

Result: 在标准基准数据集评估中，CFLHKD在特定集群和全局模型准确性上优于代表性基线，性能提升3.32 - 7.57%。

Conclusion: CFLHKD能有效解决现有CFL问题，提升聚类联邦学习的性能。

Abstract: Clustered Federated Learning (CFL) has emerged as a powerful approach for addressing data heterogeneity and ensuring privacy in large distributed IoT environments. By clustering clients and training cluster-specific models, CFL enables personalized models tailored to groups of heterogeneous clients. However, conventional CFL approaches suffer from fragmented learning for training independent global models for each cluster and fail to take advantage of collective cluster insights. This paper advocates a shift to hierarchical CFL, allowing bi-level aggregation to train cluster-specific models at the edge and a unified global model at the cloud. This shift improves training efficiency yet might introduce communication challenges. To this end, we propose CFLHKD, a novel personalization scheme for integrating hierarchical cluster knowledge into CFL. Built upon multi-teacher knowledge distillation, CFLHKD enables inter-cluster knowledge sharing while preserving cluster-specific personalization. CFLHKD adopts a bi-level aggregation to bridge the gap between local and global learning. Extensive evaluations of standard benchmark datasets demonstrate that CFLHKD outperforms representative baselines in cluster-specific and global model accuracy and achieves a performance improvement of 3.32-7.57\%.

</details>


### [70] [ESS: An Offload-Centric Latent-Cache Management Architecture for DeepSeek-V3.2-Exp](https://arxiv.org/abs/2512.10576)
*Xinhang Chen,Chao Zhang,Jiahuan He,Wei Liu,Jianming Zhang,Wenlong Zhou,Xiao Li,Pai Zeng,Shiyong Li,Yuanpan Qian,Dong Li,Zhaogeng Li*

Main category: cs.DC

TL;DR: DeepSeek-V3.2-Exp的Decode阶段有瓶颈，提出ESS系统，模拟显示ESS能提升吞吐量，是长上下文LLM服务实用可扩展方案。


<details>
  <summary>Details</summary>
Motivation: 解决DeepSeek-V3.2-Exp在长上下文场景中Decode阶段因Latent - Cache与GPU内存冲突导致的吞吐量瓶颈问题。

Method: 提出ESS系统，将Latent - Cache选择性卸载到CPU内存，关键组件保留在GPU，解除批量大小与GPU内存的关联。

Result: 高保真模拟显示，在32K上下文长度时吞吐量提升69.4%，128K时提升达123%。

Conclusion: ESS是长上下文大语言模型服务实用且可扩展的解决方案。

Abstract: DeepSeek-V3.2-Exp introduces a sparse attention mechanism that significantly reduces inference latency in long-context scenarios. Although the overall throughput has improved greatly, the Decode-stage of PD disaggregation remains to be a major bottleneck. This bottleneck primarily stems from the conflict between linear growth of Latent-Cache with sequence length and the limited GPU memory capacity, which constrains the feasible batch-size and thereby suppresses Decode-stage throughput.
  To address this challenge, we propose ESS (Extended Sparse Server), an offload-centric system design tailored for DeepSeek-V3.2-Exp. ESS selectively offloads Latent-Cache to CPU memory while preserving latency-critical components on GPU. By freeing up GPU memory, ESS effectively decoupling batch-size scaling from GPU memory constraints. This design significantly improves Decode-stage throughput, thereby reducing deployment costs in real-world settings.
  Our high-fidelity simulations show that ESS delivers 69.4\% throughput improvement at 32K context length and up to 123\% throughput improvement at 128K, demonstrating its effectiveness for large-context inference workloads. These results highlight ESS as a practical and scalable solution for long-context LLM serving.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [71] [Universal Hirschberg for Width Bounded Dynamic Programs](https://arxiv.org/abs/2512.10132)
*Logan Nye*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Hirschberg's algorithm (1975) reduces the space complexity for the longest common subsequence problem from $O(N^2)$ to $O(N)$ via recursive midpoint bisection on a grid dynamic program (DP). We show that the underlying idea generalizes to a broad class of dynamic programs with local dependencies on directed acyclic graphs (DP DAGs). Modeling a DP as deterministic time evolution over a topologically ordered DAG with frontier width $ω$ and bounded in-degree, and assuming a max-type semiring with deterministic tie breaking, we prove that in a standard offline random-access model any such DP admits deterministic traceback in space $O(ω\log T + (\log T)^{O(1)})$ cells over a fixed finite alphabet, where $T$ is the number of states. Our construction replaces backward dynamic programs by forward-only recomputation and organizes the time order into a height-compressed recursion tree whose nodes expose small "middle frontiers'' across which every optimal path must pass. The framework yields near-optimal traceback bounds for asymmetric and banded sequence alignment, one-dimensional recurrences, and dynamic-programming formulations on graphs of bounded pathwidth. We also show that an $Ω(ω)$ space term (in bits) is unavoidable in forward single-pass models and discuss conjectured $\sqrt{T}$-type barriers in streaming settings, supporting the view that space-efficient traceback is a structural property of width-bounded DP DAGs rather than a peculiarity of grid-based algorithms.

</details>


### [72] [Efficient Defective Clique Enumeration and Search with Worst-Case Optimal Search Space](https://arxiv.org/abs/2512.10354)
*Jihoon Jang,Yehyun Nam,Kunsoo Park,Hyunjoon Kim*

Main category: cs.DS

TL;DR: 提出新的分支限界框架和枢轴技术，用于枚举最大k-缺陷团和搜索最大k-缺陷团，实验表明在处理时间上远超现有算法。


<details>
  <summary>Details</summary>
Motivation: 现有枚举最大k-缺陷团和搜索最大k-缺陷团的算法存在小部分解组合爆炸和搜索空间次优的问题。

Method: 提出先生成团再添加缺失边的分支限界框架，引入新的枢轴技术，利用缺陷团的直径二属性缩小搜索空间，还提出基于分支限界的高效搜索框架。

Result: 算法搜索空间达到最坏情况最优，在真实数据集上实验，处理时间比现有算法快达四个数量级。

Conclusion: 提出的算法在枚举最大k-缺陷团和搜索最大k-缺陷团方面表现优于现有算法。

Abstract: A $k$-defective clique is a relaxation of the traditional clique definition, allowing up to $k$ missing edges. This relaxation is crucial in various real-world applications such as link prediction, community detection, and social network analysis. Although the problems of enumerating maximal $k$-defective cliques and searching a maximum $k$-defective clique have been extensively studied, existing algorithms suffer from limitations such as the combinatorial explosion of small partial solutions and sub-optimal search spaces. To address these limitations, we propose a novel clique-first branch-and-bound framework that first generates cliques and then adds missing edges. Furthermore, we introduce a new pivoting technique that achieves a search space size of $\mathcal{O}(3^{\frac{n}{3}} \cdot n^k)$, where $n$ is the number of vertices in the input graph. We prove that the worst-case number of maximal $k$-defective cliques is $Ω(3^{\frac{n}{3}} \cdot n^k)$ when $k$ is a constant, establishing that our algorithm's search space is worst-case optimal. Leveraging the diameter-two property of defective cliques, we further reduce the search space size to $\mathcal{O}(n \cdot 3^{\fracδ{3}} \cdot (δΔ)^k)$, where $δ$ is the degeneracy and $Δ$ is the maximum degree of the input graph. We also propose an efficient framework for maximum $k$-defective clique search based on our branch-and-bound, together with practical techniques to reduce the search space. Experiments on real-world benchmark datasets with more than 1 million edges demonstrate that each of our proposed algorithms for maximal $k$-defective clique enumeration and maximum $k$-defective clique search outperforms the respective state-of-the-art algorithms by up to four orders of magnitude in terms of processing time.

</details>


### [73] [Approximate Counting in Local Lemma Regimes](https://arxiv.org/abs/2512.10134)
*Ryan L. Mann,Gabriel Waite*

Main category: cs.DS

TL;DR: 本文为局部引理机制下的几个自然问题建立高效近似计数算法，涉及事件交集概率和子空间交集维度，给出不同情况下的算法及推论。


<details>
  <summary>Details</summary>
Motivation: 为局部引理机制下的自然问题（事件交集概率和子空间交集维度）建立高效近似计数算法。

Method: 基于簇展开方法。

Result: 对交换投影算子的交集概率和维度得到完全多项式时间近似方案；对一般投影算子，在全局容斥稳定性条件下给出完全多项式时间近似方案，在谱间隙假设下给出高效仿射近似；得到近似合取范式公式的满足赋值数量和量子可满足性公式的满足子空间维度的高效算法。

Conclusion: 通过所提方法和算法能有效解决相关近似计数问题。

Abstract: We establish efficient approximate counting algorithms for several natural problems in local lemma regimes. In particular, we consider the probability of intersection of events and the dimension of intersection of subspaces. Our approach is based on the cluster expansion method. We obtain fully polynomial-time approximation schemes for both the probability of intersection and the dimension of intersection for commuting projectors. For general projectors, we provide two algorithms: a fully polynomial-time approximation scheme under a global inclusion-exclusion stability condition, and an efficient affine approximation under a spectral gap assumption. As corollaries of our results, we obtain efficient algorithms for approximating the number of satisfying assignments of conjunctive normal form formulae and the dimension of satisfying subspaces of quantum satisfiability formulae.

</details>


### [74] [Semi-Robust Communication Complexity of Maximum Matching](https://arxiv.org/abs/2512.10532)
*Gabriel Cipriani Huete,Adithya Diddapur,Pavel Dvořák,Christian Konrad*

Main category: cs.DS

TL;DR: 研究半鲁棒设置下最大匹配的单向两方通信复杂度，提出简单协议有3/4近似率，在全鲁棒设置也有此近似率但无法超越现有最优结果。


<details>
  <summary>Details</summary>
Motivation: 研究半鲁棒设置下最大匹配的单向两方通信复杂度，探索简单有效协议。

Method: 提出Alice将字典序首个最大匹配信息传达给Bob的简单协议。

Result: 协议在半鲁棒设置下期望有3/4近似率且分析是紧的，在全鲁棒设置也有3/4近似率，存在实例使协议近似率为0.832。

Conclusion: 简单协议表现良好，但无法在全鲁棒设置下超越现有最优结果。

Abstract: We study the one-way two-party communication complexity of Maximum Matching in the semi-robust setting where the edges of a maximum matching are randomly partitioned between Alice and Bob, but all remaining edges of the input graph are adversarially partitioned between the two parties.
  We show that the simple protocol where Alice solely communicates a lexicographically-first maximum matching of their edges to Bob is surprisingly powerful: We prove that it yields a $3/4$-approximation in expectation and that our analysis is tight.
  The semi-robust setting is at least as hard as the fully robust setting. In this setting, all edges of the input graph are randomly partitioned between Alice and Bob, and the state-of-the-art result is a fairly involved $5/6$-approximation protocol that is based on the computation of edge-degree constrained subgraphs [Azarmehr, Behnezhad, ICALP'23]. Our protocol also immediately yields a $3/4$-approximation in the fully robust setting. One may wonder whether an improved analysis of our protocol in the fully robust setting is possible: While we cannot rule this out, we give an instance where our protocol only achieves a $0.832 < 5/6 = 0.83$-approximation. Hence, while our simple protocol performs surprisingly well, it cannot be used to improve over the state-of-the-art in the fully robust setting.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [75] [Does Timeboost Reduce MEV-Related Spam? Theory and Evidence from Layer-2 Transactions](https://arxiv.org/abs/2512.10094)
*Brian Zhu*

Main category: cs.GT

TL;DR: 研究Arbitrum上的Timeboost机制，从模型和实证两方面证明其可减少Layer - 2区块链的MEV相关垃圾交易并增加收益


<details>
  <summary>Details</summary>
Motivation: Layer - 2区块链中MEV机会常引发垃圾交易，浪费区块空间，需研究有效机制解决

Method: 建立博弈论模型研究用户交易提交行为，整合多个Layer - 2网络的内存池数据进行实证研究和事件分析

Result: 模型显示Timeboost能减少垃圾交易并增加收益，实证发现Arbitrum采用Timeboost后MEV相关垃圾交易减少、收益增加

Conclusion: Timeboost机制有效，能将用户成本从回滚成本转移到拍卖出价，减少垃圾交易并增加收益

Abstract: Maximal extractable value opportunities often induce spam in Layer-2 blockchains: many identical transactions are submitted near simultaneously, most of which revert, wasting blockspace. We study Timeboost, a mechanism on Arbitrum that auctions a timestamp advantage, crucial under first-come first-served sequencing rules. We develop a game-theoretic model in which users choose the number of transaction copies to submit, and extend upon the baseline setting by modeling the Timeboost auction and subsequent transaction submission behavior. We show that Timeboost reduces spam and increases sequencer/DAO revenue in equilibrium relative to the baseline, transferring user payments from revert costs to auction bids. Empirically, we assemble mempool data from multiple Layer-2 networks, measuring spam via identical transactions submitted in narrow time intervals, and conduct an event study around Timeboost adoption on Arbitrum using other L2s as contemporaneous benchmarks. We find a decline in MEV-related spam and an increase in revenue on Arbitrum post-adoption, consistent with model predictions.

</details>


### [76] [Computing Evolutionarily Stable Strategies in Imperfect-Information Games](https://arxiv.org/abs/2512.10279)
*Sam Ganzfried*

Main category: cs.GT

TL;DR: 提出计算对称不完全信息完美回忆扩展式博弈中演化稳定策略（ESS）的算法，能处理两人与多人游戏，具健全性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 找到在对称不完全信息完美回忆扩展式博弈中计算ESS的有效方法。

Method: 设计算法处理两人游戏并扩展到多人游戏，有健全性，可提前停止。

Result: 在非退化游戏中计算所有ESS，在退化游戏中计算部分ESS，通过癌症信号博弈和随机游戏实验验证可扩展性。

Conclusion: 所提算法能有效计算相关博弈的ESS，具有一定灵活性和可扩展性。

Abstract: We present an algorithm for computing evolutionarily stable strategies (ESSs) in symmetric perfect-recall extensive-form games of imperfect information. Our main algorithm is for two-player games, and we describe how it can be extended to multiplayer games. The algorithm is sound and computes all ESSs in nondegenerate games and a subset of them in degenerate games which contain an infinite continuum of symmetric Nash equilibria. The algorithm is anytime and can be stopped early to find one or more ESSs. We experiment on an imperfect-information cancer signaling game as well as random games to demonstrate scalability.

</details>


### [77] [Certifying Concavity and Monotonicity in Games via Sum-of-Squares Hierarchies](https://arxiv.org/abs/2512.10292)
*Vincent Leon,Iosif Sakos,Ryann Sim,Antonios Varvitsiotis*

Main category: cs.GT

TL;DR: 研究多人游戏中凹性和单调性认证问题，指出认证NP难，提出和证明平方和程序层次可解决，定义SOS - 凹/单调游戏并应用于示例。


<details>
  <summary>Details</summary>
Motivation: 凹性及其细化特性对多人游戏可处理性很重要，但不清楚认证凹性或单调性的复杂度及方法。

Method: 开发两个平方和程序层次来认证游戏的凹性和单调性；引入SOS - 凹/单调游戏全局近似凹/单调游戏。

Result: 认证凹性或单调性是NP难；平方和程序层次可在多项式时间求解，能认证几乎所有凹/单调游戏；可在多项式时间计算给定游戏最接近的SOS - 凹/单调游戏。

Conclusion: 所提方法能有效处理多人游戏凹性和单调性认证问题，并可应用于不完美回忆扩展式游戏。

Abstract: Concavity and its refinements underpin tractability in multiplayer games, where players independently choose actions to maximize their own payoffs which depend on other players' actions. In concave games, where players' strategy sets are compact and convex, and their payoffs are concave in their own actions, strong guarantees follow: Nash equilibria always exist and decentralized algorithms converge to equilibria. If the game is furthermore monotone, an even stronger guarantee holds: Nash equilibria are unique under strictness assumptions. Unfortunately, we show that certifying concavity or monotonicity is NP-hard, already for games where utilities are multivariate polynomials and compact, convex basic semialgebraic strategy sets -- an expressive class that captures extensive-form games with imperfect recall. On the positive side, we develop two hierarchies of sum-of-squares programs that certify concavity and monotonicity of a given game, and each level of the hierarchies can be solved in polynomial time. We show that almost all concave/monotone games are certified at some finite level of the hierarchies. Subsequently, we introduce SOS-concave/monotone games, which globally approximate concave/monotone games, and show that for any given game we can compute the closest SOS-concave/monotone game in polynomial time. Finally, we apply our techniques to canonical examples of imperfect recall extensive-form games.

</details>


### [78] [The $k$-flip Ising game](https://arxiv.org/abs/2512.10389)
*Kovalenko Aleksandr,Andrey Leonidov*

Main category: cs.GT

TL;DR: 研究完整图上N个玩家的k - flip Ising游戏，给出转移矩阵及相关分布矩的解析计算，探讨首达时间分布矩与k的关系。


<details>
  <summary>Details</summary>
Motivation: 研究离散时间下部分并行动态嘈杂二元选择游戏，分析不同参数对游戏状态转移的影响。

Method: 进行游戏转移矩阵的解析计算，考虑相关分布的前两阶矩。

Result: 得出游戏转移矩阵及相关分布的前两阶矩，发现亚稳态衰减时相关矩与k存在非平凡依赖关系，特定k*处有最小值。

Conclusion: 特定k*处的最小值归因于k依赖的扩散力和恢复力的竞争。

Abstract: A partially parallel dynamical noisy binary choice (Ising) game in discrete time of $N$ players on complete graphs with $k$ players having a possibility of changing their strategies at each time moment called $k$-flip Ising game is considered. Analytical calculation of the transition matrix of game as well as the first two moments of the distribution of $\varphi=N^+/N$, where $N^+$ is a number of players adhering to one of the two strategies, is presented. First two moments of the first hitting time distribution for sample trajectories corresponding to transition from a metastable and unstable states to a stable one are considered. A nontrivial dependence of these moments on $k$ for the decay of a metastable state is discussed. A presence of the minima at certain $k^*$ is attributed to a competition between $k$-dependent diffusion and restoring forces.

</details>


### [79] [LLM-Auction: Generative Auction towards LLM-Native Advertising](https://arxiv.org/abs/2512.10551)
*Chujie Zhao,Qun Hu,Shiping Song,Dagui Chen,Han Zhu,Jian Xu,Bo Zheng*

Main category: cs.GT

TL;DR: 提出用于大语言模型原生广告的学习型生成式拍卖机制LLM - Auction，性能优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 大语言模型原生广告拍卖机制将拍卖对象转变，现有机制存在忽略外部性或需多次推理等问题，不适合工业场景。

Method: 提出LLM - Auction，将分配优化建模为偏好对齐问题，引入IRPO算法交替优化奖励模型和大语言模型，确定其分配单调性和连续性以证明支付规则激励特性，设计模拟环境评估性能。

Result: 大量定量和定性实验表明LLM - Auction在分配效率上显著优于现有基线。

Conclusion: LLM - Auction能有效解决大语言模型原生广告拍卖机制设计难题，具备理想机制特性。

Abstract: The rapid advancement of large language models (LLMs) necessitates novel monetization strategies, among which LLM-native advertising has emerged as a promising paradigm by naturally integrating advertisement within LLM-generated responses. However, this paradigm fundamentally shifts the auction object from discrete ad slots to the distribution over LLM outputs, posing new challenges for designing auction mechanisms. Existing mechanisms for LLM-native advertising adopt frameworks that decouple auction and generation, which either ignore externalities or require multiple LLM inferences for ad allocation, rendering them impractical for industrial scenarios. To address these challenges, we propose LLM-Auction, which to the best of our knowledge is the first learning-based generative auction mechanism that integrates auction and LLM generation for LLM-native advertising. By formulating the allocation optimization as a preference alignment problem between LLM outputs and the mechanism's objective which reflects both advertisers' expected value and user experience, we introduce Iterative Reward-Preference Optimization (IRPO) algorithm that alternately optimizes the reward model and the LLM. This approach enables the LLM to inherently model allocation externalities without any extra inference cost. We further identify the allocation monotonicity and continuity of LLM-Auction, which allows us to prove that a simple first-price payment rule exhibits favorable incentive properties. Additionally, we design an LLM-as-a-judge simulation environment to facilitate large-scale data construction and enable comprehensive quantitative evaluation of the mechanism's performance. Extensive quantitative and qualitative experiments demonstrate that LLM-Auction significantly outperforms existing baselines in allocation efficiency, while achieving the desired mechanism properties.

</details>


### [80] [Dynamics of multidimensional Simple Clock Auctions](https://arxiv.org/abs/2512.10614)
*Jad Zeroual,Marianne Akian,Aurélien Bechler,Matthieu Chardy,Stéphane Gaubert*

Main category: cs.GT

TL;DR: 研究单信息完备玩家参与的简单时钟拍卖，分析连续时间版本，证明解的唯一性、价值函数特性并以澳洲频谱拍卖为例说明。


<details>
  <summary>Details</summary>
Motivation: 研究单信息完备玩家在简单时钟拍卖中面对直接投标者的情况。

Method: 考虑连续时间版本的SCA拍卖，价格遵循分段常数动态的微分包含，分析对手估值满足普通替代条件下的投标策略。

Result: 存在Filippov意义下的唯一解，连续时间模型与离散时间拍卖价格增量趋于零时的极限一致，极限拍卖的价值函数分段线性（可能不连续）。

Conclusion: 通过理论分析和实例验证了相关结果在频谱拍卖中的应用。

Abstract: Simple Clock Auctions (SCA) are a mechanism commonly used in spectrum auctions to sell lots of frequency bandwidths. We study such an auction with one player having access to perfect information against straightforward bidders. When the opponents' valuations satisfy the ordinary substitutes condition, we show that it is optimal to bid on a fixed lot overtime. In this setting, we consider a continuous-time version of the SCA auction in which the prices follow a differential inclusion with a piecewise-constant dynamics. We show that there exists a unique solution in the sense of Filippov. This guarantees that the continuous-time model coincides with the limit of the discrete-time auction when price increments tend to zero. Moreover, we show that the value function of this limit auction is piecewise linear (though possibly discontinuous). Finally, we illustrate these results by analyzing a simplified version of the multiband Australian spectrum auction of 2017.

</details>


### [81] [Designing Truthful Mechanisms for Asymptotic Fair Division](https://arxiv.org/abs/2512.10892)
*Jugal Garg,Vishnu V. Narayan,Yuang Eric Shen*

Main category: cs.GT

TL;DR: 研究渐近情况下商品公平分配问题，扩展联合价值分布理论，给出新随机机制解决公开问题并拓展到多种场景。


<details>
  <summary>Details</summary>
Motivation: 先前研究依赖非策略证明机制，限制在策略型代理场景的应用，需扩展理论和提出新机制。

Method: 将理论扩展到更广泛、更现实的联合价值分布类，提出新的随机机制。

Result: 当m = Ω(n log n)时，无嫉妒分配仍大概率存在；新机制期望诚实、可多项式时间高效实现且大概率输出无嫉妒分配。

Conclusion: 扩展了商品公平分配理论，提出有效新机制并拓展到多种场景，解决了公开问题。

Abstract: We study the problem of fairly allocating a set of $m$ goods among $n$ agents in the asymptotic setting, where each item's value for each agent is drawn from an underlying joint distribution. Prior works have shown that if this distribution is well-behaved, then an envy-free allocation exists with high probability when $m=Ω(n\log{n})$ [Dickerson et al., 2014]. Under the stronger assumption that item values are independently and identically distributed (i.i.d.) across agents, this requirement improves to $m=Ω(n\log{n}/\log{\log{n}})$, which is tight [Manurangsi and Suksompong, 2021]. However, these results rely on non-strategyproof mechanisms, such as maximum-welfare allocation or the round-robin algorithm, limiting their applicability in settings with strategic agents.
  In this work, we extend the theory to a broader, more realistic class of joint value distributions, allowing for correlations among agents, atomicity, and unequal probabilities of having the highest value for an item. We show that envy-free allocations continue to exist with a high probability when $m=Ω(n\log{n})$. More importantly, we give a new randomized mechanism that is truthful in expectation, efficiently implementable in polynomial time, and outputs envy-free allocations with high probability, answering an open question posed by [Manurangsi and Suksompong, 2017]. We further extend our mechanism to settings with asymptotic weighted fair division and multiple agent types and good types, proving new results in each case.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [82] [STARS: Semantic Tokens with Augmented Representations for Recommendation at Scale](https://arxiv.org/abs/2512.10149)
*Han Chen,Steven Zhu,Yingrui Li*

Main category: cs.IR

TL;DR: 介绍适用于大规模、低延迟电商场景的基于Transformer的顺序推荐框架STARS，在离线评估和A/B测试中表现良好，证明结合语义增强等可在不牺牲效率下提升推荐质量。


<details>
  <summary>Details</summary>
Motivation: 现实电商推荐系统需在低延迟下应对冷启动、用户意图快速变化和动态上下文等挑战，提供相关商品推荐。

Method: 构建STARS框架，包括双内存用户嵌入、语义商品令牌、上下文感知评分和低延迟两阶段检索管道。

Result: 离线评估中，STARS比现有LambdaMART系统的Hit@5提高超75%；大规模A/B测试显示多项指标有显著提升。

Conclusion: 结合语义增强、多意图建模和面向部署的设计可在现实环境实现先进推荐质量且不牺牲服务效率。

Abstract: Real-world ecommerce recommender systems must deliver relevant items under strict tens-of-milliseconds latency constraints despite challenges such as cold-start products, rapidly shifting user intent, and dynamic context including seasonality, holidays, and promotions. We introduce STARS, a transformer-based sequential recommendation framework built for large-scale, low-latency ecommerce settings. STARS combines several innovations: dual-memory user embeddings that separate long-term preferences from short-term session intent; semantic item tokens that fuse pretrained text embeddings, learnable deltas, and LLM-derived attribute tags, strengthening content-based matching, long-tail coverage, and cold-start performance; context-aware scoring with learned calendar and event offsets; and a latency-conscious two-stage retrieval pipeline that performs offline embedding generation and online maximum inner-product search with filtering, enabling tens-of-milliseconds response times. In offline evaluations on production-scale data, STARS improves Hit@5 by more than 75 percent relative to our existing LambdaMART system. A large-scale A/B test on 6 million visits shows statistically significant lifts, including Total Orders +0.8%, Add-to-Cart on Home +2.0%, and Visits per User +0.5%. These results demonstrate that combining semantic enrichment, multi-intent modeling, and deployment-oriented design can yield state-of-the-art recommendation quality in real-world environments without sacrificing serving efficiency.

</details>


### [83] [The Best of the Two Worlds: Harmonizing Semantic and Hash IDs for Sequential Recommendation](https://arxiv.org/abs/2512.10388)
*Ziwei Liu,Yejing Wang,Qidong Liu,Zijian Zhang,Chong Chen,Wei Huang,Xiangyu Zhao*

Main category: cs.IR

TL;DR: 提出一种融合语义ID和哈希ID的框架H2Rec，平衡热门和长尾物品推荐质量，超越现有基线。


<details>
  <summary>Details</summary>
Motivation: 传统顺序推荐系统的哈希ID易受长尾问题影响，现有结合辅助信息的方法有噪音或语义同质化问题，基于语义ID的方法有协作压倒性现象和量化机制问题。

Method: 提出H2Rec框架，设计双分支建模架构，引入双级对齐策略。

Result: 在三个真实数据集上的实验表明，H2Rec能有效平衡热门和长尾物品的推荐质量，超越现有基线。

Conclusion: H2Rec框架有效解决了现有推荐系统的问题，可实现更优的推荐效果。

Abstract: Conventional Sequential Recommender Systems (SRS) typically assign unique Hash IDs (HID) to construct item embeddings. These HID embeddings effectively learn collaborative information from historical user-item interactions, making them vulnerable to situations where most items are rarely consumed (the long-tail problem). Recent methods that incorporate auxiliary information often suffer from noisy collaborative sharing caused by co-occurrence signals or semantic homogeneity caused by flat dense embeddings. Semantic IDs (SIDs), with their capability of code sharing and multi-granular semantic modeling, provide a promising alternative. However, the collaborative overwhelming phenomenon hinders the further development of SID-based methods. The quantization mechanisms commonly compromise the uniqueness of identifiers required for modeling head items, creating a performance seesaw between head and tail items. To address this dilemma, we propose \textbf{\name}, a novel framework that harmonizes the SID and HID. Specifically, we devise a dual-branch modeling architecture that enables the model to capture both the multi-granular semantics within SID while preserving the unique collaborative identity of HID. Furthermore, we introduce a dual-level alignment strategy that bridges the two representations, facilitating knowledge transfer and supporting robust preference modeling. Extensive experiments on three real-world datasets show that \name~ effectively balances recommendation quality for both head and tail items while surpassing the existing baselines. The implementation code can be found online\footnote{https://github.com/ziwliu8/H2Rec}.

</details>


### [84] [Rethinking Popularity Bias in Collaborative Filtering via Analytical Vector Decomposition](https://arxiv.org/abs/2512.10688)
*Lingfeng Liu,Yixin Song,Dazhong Shen,Bing Yin,Hao Li,Yanyong Zhang,Chao Wang*

Main category: cs.IR

TL;DR: 本文揭示协同过滤模型中流行度偏差的本质原因，提出DDC框架纠正嵌入几何，实验表明其优于现有去偏方法。


<details>
  <summary>Details</summary>
Motivation: 流行度偏差破坏协同过滤模型个性化能力，现有方法将其视为外部因素，本文旨在揭示其内在原因并解决该问题。

Method: 通过数学分析证明BPR优化中流行度偏差是几何产物，提出DDC框架，通过不对称方向更新纠正嵌入几何。

Result: 在多个基于BPR的架构上进行大量实验，DDC显著优于现有去偏方法，训练损失降至微调基线的不到5%，推荐质量和公平性更优。

Conclusion: DDC能有效解决协同过滤模型中的流行度偏差问题，提高推荐效果和公平性。

Abstract: Popularity bias fundamentally undermines the personalization capabilities of collaborative filtering (CF) models, causing them to disproportionately recommend popular items while neglecting users' genuine preferences for niche content. While existing approaches treat this as an external confounding factor, we reveal that popularity bias is an intrinsic geometric artifact of Bayesian Pairwise Ranking (BPR) optimization in CF models. Through rigorous mathematical analysis, we prove that BPR systematically organizes item embeddings along a dominant "popularity direction" where embedding magnitudes directly correlate with interaction frequency. This geometric distortion forces user embeddings to simultaneously handle two conflicting tasks-expressing genuine preference and calibrating against global popularity-trapping them in suboptimal configurations that favor popular items regardless of individual tastes. We propose Directional Decomposition and Correction (DDC), a universally applicable framework that surgically corrects this embedding geometry through asymmetric directional updates. DDC guides positive interactions along personalized preference directions while steering negative interactions away from the global popularity direction, disentangling preference from popularity at the geometric source. Extensive experiments across multiple BPR-based architectures demonstrate that DDC significantly outperforms state-of-the-art debiasing methods, reducing training loss to less than 5% of heavily-tuned baselines while achieving superior recommendation quality and fairness. Code is available in https://github.com/LingFeng-Liu-AI/DDC.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [85] [HGC-Herd: Efficient Heterogeneous Graph Condensation via Representative Node Herding](https://arxiv.org/abs/2512.09947)
*Fuyan Ou,Siqi Ai,Yulin Hu*

Main category: cs.LG

TL;DR: 提出训练无关的异构图凝聚框架HGC - Herd，实验验证其高效可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有HGNN在大规模图上因结构冗余和高维特征可扩展性差，现有图凝聚方法主要针对同构图且有较大开销。

Method: 提出HGC - Herd，集成轻量级特征传播编码多跳关系上下文，用类内聚集机制识别每类代表性节点。

Result: 在ACM、DBLP和Freebase上实验表明，HGC - Herd与全图训练精度相当或更优，显著减少运行时间和内存消耗。

Conclusion: HGC - Herd对高效可扩展的异构图表示学习有实用价值。

Abstract: Heterogeneous graph neural networks (HGNNs) have demonstrated strong capability in modeling complex semantics across multi-type nodes and relations. However, their scalability to large-scale graphs remains challenging due to structural redundancy and high-dimensional node features. Existing graph condensation approaches, such as GCond, are primarily developed for homogeneous graphs and rely on gradient matching, resulting in considerable computational, memory, and optimization overhead. We propose HGC-Herd, a training-free condensation framework that generates compact yet informative heterogeneous graphs while maintaining both semantic and structural fidelity. HGC-Herd integrates lightweight feature propagation to encode multi-hop relational context and employs a class-wise herding mechanism to identify representative nodes per class, producing balanced and discriminative subsets for downstream learning tasks. Extensive experiments on ACM, DBLP, and Freebase validate that HGC-Herd attains comparable or superior accuracy to full-graph training while markedly reducing both runtime and memory consumption. These results underscore its practical value for efficient and scalable heterogeneous graph representation learning.

</details>


### [86] [BAMBO: Construct Ability and Efficiency LLM Pareto Set via Bayesian Adaptive Multi-objective Block-wise Optimization](https://arxiv.org/abs/2512.09972)
*Kesheng Chen,Wenjian Luo,Zhenqian Zhu,Yamin Hu,Yiya Xi*

Main category: cs.LG

TL;DR: 现有大语言模型合并技术在构建帕累托集方面不足，提出BAMBO框架自动构建，实验显示优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型合并技术在构建帕累托集时存在不足，粗粒度方法解稀疏且次优，细粒度方法有维度灾难。

Method: 提出BAMBO框架，引入混合最优块划分策略，将其表述为一维聚类问题，用动态规划平衡块内同质性和块间信息分布，在qEHVI采集函数驱动的进化循环中自动化处理。

Result: BAMBO发现的帕累托前沿优于基线，能根据不同操作约束进行灵活模型选择。

Conclusion: BAMBO框架有效解决了大语言模型构建帕累托集的问题，可推动模型选择。

Abstract: Constructing a Pareto set is pivotal for navigating the capability-efficiency trade-offs in Large Language Models (LLMs); however, existing merging techniques remain inadequate for this task. Coarse-grained, model-level methods yield only a sparse set of suboptimal solutions, while fine-grained, layer-wise approaches suffer from the "curse of dimensionality," rendering the search space computationally intractable. To resolve this dichotomy, we propose BAMBO (Bayesian Adaptive Multi-objective Block-wise Optimization), a novel framework that automatically constructs the LLM Pareto set. BAMBO renders the search tractable by introducing a Hybrid Optimal Block Partitioning strategy. Formulated as a 1D clustering problem, this strategy leverages a dynamic programming approach to optimally balance intra-block homogeneity and inter-block information distribution, thereby dramatically reducing dimensionality without sacrificing critical granularity. The entire process is automated within an evolutionary loop driven by the q-Expected Hypervolume Improvement (qEHVI) acquisition function. Experiments demonstrate that BAMBO discovers a superior and more comprehensive Pareto frontier than baselines, enabling agile model selection tailored to diverse operational constraints. Code is available at: https://github.com/xin8coder/BAMBO.

</details>


### [87] [Latent Action World Models for Control with Unlabeled Trajectories](https://arxiv.org/abs/2512.10016)
*Marvin Alles,Xingyuan Zhang,Patrick van der Smagt,Philip Becker-Ehmck*

Main category: cs.LG

TL;DR: 本文提出潜动作世界模型，结合有动作和无动作数据学习，减少有动作标签样本使用且提升效率。


<details>
  <summary>Details</summary>
Motivation: 标准世界模型依赖动作条件轨迹，动作标签稀缺时效果受限，受人类结合直接交互和无动作经验启发，研究可从异质数据学习的世界模型。

Method: 引入潜动作世界模型，学习共享潜动作表示，联合使用有动作和无动作数据；通过离线强化学习学习潜动作策略。

Result: 在DeepMind控制套件上，比纯动作条件基线模型少用约一个数量级有动作标签样本，仍取得良好性能。

Conclusion: 潜动作能让世界模型在被动和交互数据上训练，使其学习更高效。

Abstract: Inspired by how humans combine direct interaction with action-free experience (e.g., videos), we study world models that learn from heterogeneous data. Standard world models typically rely on action-conditioned trajectories, which limits effectiveness when action labels are scarce. We introduce a family of latent-action world models that jointly use action-conditioned and action-free data by learning a shared latent action representation. This latent space aligns observed control signals with actions inferred from passive observations, enabling a single dynamics model to train on large-scale unlabeled trajectories while requiring only a small set of action-labeled ones. We use the latent-action world model to learn a latent-action policy through offline reinforcement learning (RL), thereby bridging two traditionally separate domains: offline RL, which typically relies on action-conditioned data, and action-free training, which is rarely used with subsequent RL. On the DeepMind Control Suite, our approach achieves strong performance while using about an order of magnitude fewer action-labeled samples than purely action-conditioned baselines. These results show that latent actions enable training on both passive and interactive data, which makes world models learn more efficiently.

</details>


### [88] [Cluster-Dags as Powerful Background Knowledge For Causal Discovery](https://arxiv.org/abs/2512.10032)
*Jan Marco Ruiz de Vargas,Kirtan Padh,Niki Kilbertus*

Main category: cs.LG

TL;DR: 本文利用Cluster - DAGs作为先验知识框架进行因果发现，提出Cluster - PC和Cluster - FCI算法，实验表明其优于无先验知识的基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前因果发现方法在处理高维数据和复杂依赖关系时面临挑战，利用系统先验知识可辅助因果发现。

Method: 利用Cluster - DAGs作为先验知识框架，提出Cluster - PC和Cluster - FCI两种改进的基于约束的算法，分别用于全观测和部分观测场景。

Result: 在模拟数据上的实验表明，Cluster - PC和Cluster - FCI优于各自无先验知识的基线方法。

Conclusion: Cluster - DAGs作为先验知识框架可有效改进因果发现算法性能。

Abstract: Finding cause-effect relationships is of key importance in science. Causal discovery aims to recover a graph from data that succinctly describes these cause-effect relationships. However, current methods face several challenges, especially when dealing with high-dimensional data and complex dependencies. Incorporating prior knowledge about the system can aid causal discovery. In this work, we leverage Cluster-DAGs as a prior knowledge framework to warm-start causal discovery. We show that Cluster-DAGs offer greater flexibility than existing approaches based on tiered background knowledge and introduce two modified constraint-based algorithms, Cluster-PC and Cluster-FCI, for causal discovery in the fully and partially observed setting, respectively. Empirical evaluation on simulated data demonstrates that Cluster-PC and Cluster-FCI outperform their respective baselines without prior knowledge.

</details>


### [89] [Robust Gradient Descent via Heavy-Ball Momentum with Predictive Extrapolation](https://arxiv.org/abs/2512.10033)
*Sarwan Ali*

Main category: cs.LG

TL;DR: 提出了HB - SGE方法，结合重球动量和预测梯度外推，在多种问题上表现稳定，虽在良态问题上稍慢于NAG，但在病态和非凸问题上更优。


<details>
  <summary>Details</summary>
Motivation: 加速梯度方法如NAG在病态或非凸问题上常因激进动量积累而发散，需要一种更稳定的一阶方法。

Method: 提出Heavy - Ball Synthetic Gradient Extrapolation (HB - SGE)方法，结合重球动量和预测梯度外推，用局部泰勒近似估计未来梯度方向。

Result: 理论证明了对强凸函数的收敛性；在病态二次函数和非凸Rosenbrock函数上表现优于NAG和标准动量方法，且只需要$O(d)$内存开销，与标准动量方法有相同超参数。

Conclusion: NAG在良态问题上更快，但HB - SGE是一个稳健的替代方法，在不同问题上比SGD有加速效果。

Abstract: Accelerated gradient methods like Nesterov's Accelerated Gradient (NAG) achieve faster convergence on well-conditioned problems but often diverge on ill-conditioned or non-convex landscapes due to aggressive momentum accumulation. We propose Heavy-Ball Synthetic Gradient Extrapolation (HB-SGE), a robust first-order method that combines heavy-ball momentum with predictive gradient extrapolation. Unlike classical momentum methods that accumulate historical gradients, HB-SGE estimates future gradient directions using local Taylor approximations, providing adaptive acceleration while maintaining stability. We prove convergence guarantees for strongly convex functions and demonstrate empirically that HB-SGE prevents divergence on problems where NAG and standard momentum fail. On ill-conditioned quadratics (condition number $κ=50$), HB-SGE converges in 119 iterations while both SGD and NAG diverge. On the non-convex Rosenbrock function, HB-SGE achieves convergence in 2,718 iterations where classical momentum methods diverge within 10 steps. While NAG remains faster on well-conditioned problems, HB-SGE provides a robust alternative with speedup over SGD across diverse landscapes, requiring only $O(d)$ memory overhead and the same hyperparameters as standard momentum.

</details>


### [90] [Intelligently Weighting Multiple Reference Models for Direct Preference Optimization of LLMs](https://arxiv.org/abs/2512.10040)
*Skyler Wu,Aymen Echarghaoui*

Main category: cs.LG

TL;DR: 本文针对MRPO参考权重设置问题提出四种新策略，实验显示新策略优于现有MRPO方法，但单参考DPO表现更优。


<details>
  <summary>Details</summary>
Motivation: 当前MRPO设置参考权重的方法随意且统计上不合理，导致性能不可靠。

Method: 提出四种新的参考权重策略，包括两种离线方法、两种在线方法。

Result: 四种新策略在偏好准确性上优于当前MRPO加权方法；单参考DPO使用7个参考模型中的6个时，始终优于多参考方法。

Conclusion: 多参考方法的实际吸引力存疑。

Abstract: Fine-tuning is integral for aligning large language models (LLMs) with human preferences. Multiple-Reference Preference Optimization (MRPO) builds on Direct Preference Optimization (DPO) by fine-tuning LLMs on preference datasets while regularizing the policy towards a mixture of reference models to leverage their collective desirable properties. However, current methods for setting the reference weights are ad-hoc and statistically unsound, leading to unreliable performance. To address this, we introduce four new weighting strategies: two offline methods that leverage held-out validation signal; one online method that uses a sliding-window estimator to reduce overfitting; and an online method that treats reference weighting as a $K$-armed bandit via Thompson Sampling. Experiments using Qwen2.5-0.5B as the policy model and seven reference models from the Llama, Mistral, Qwen, Yi, and Phi families (0.5B-14B each) show that all 4 of our strategies outperform the current MRPO weighting methods on UltraFeedback and SafeRLHF in preference accuracy. More thought-provokingly, however, we find that single-reference DPO, using any of 6 out of 7 references, consistently outperforms all tested multiple-reference approaches -- calling into question the practical appeal of multiple-reference approaches.

</details>


### [91] [Physics-Informed Learning of Flow Distribution and Receiver Heat Losses in Parabolic Trough Solar Fields](https://arxiv.org/abs/2512.10886)
*Stefan Matthes,Markus Schramm*

Main category: cs.LG

TL;DR: 提出物理信息学习框架，从常规运行数据推断抛物槽式CSP厂回路质量流比和接收器传热系数，模型准确且结果与无人机热成像对应。


<details>
  <summary>Details</summary>
Motivation: 抛物槽式CSP厂的回路质量流量和接收器热损失参数无法观测，难以用标准工具诊断水力不平衡或接收器退化。

Method: 利用夜间热油循环的均质期分离水力和热损失影响，将可微共轭传热模型离散化嵌入端到端学习管道并使用历史数据优化。

Result: 模型准确重构回路温度（RMSE <2°C），能给出有物理意义的回路不平衡和接收器热损失估计，与无人机热成像结果对应。

Conclusion: 结合适当建模和可微优化，CSP运行数据含足够信息恢复潜在物理参数。

Abstract: Parabolic trough Concentrating Solar Power (CSP) plants operate large hydraulic networks of collector loops that must deliver a uniform outlet temperature despite spatially heterogeneous optical performance, heat losses, and pressure drops. While loop temperatures are measured, loop-level mass flows and receiver heat-loss parameters are unobserved, making it impossible to diagnose hydraulic imbalances or receiver degradation using standard monitoring tools.
  We present a physics-informed learning framework that infers (i) loop-level mass-flow ratios and (ii) time-varying receiver heat-transfer coefficients directly from routine operational data. The method exploits nocturnal homogenization periods -- when hot oil is circulated through a non-irradiated field -- to isolate hydraulic and thermal-loss effects. A differentiable conjugate heat-transfer model is discretized and embedded into an end-to-end learning pipeline optimized using historical plant data from the 50 MW Andasol 3 solar field.
  The model accurately reconstructs loop temperatures (RMSE $<2^\circ$C) and produces physically meaningful estimates of loop imbalances and receiver heat losses. Comparison against drone-based infrared thermography (QScan) shows strong correspondence, correctly identifying all areas with high-loss receivers. This demonstrates that noisy real-world CSP operational data contain enough information to recover latent physical parameters when combined with appropriate modeling and differentiable optimization.

</details>


### [92] [SEMDICE: Off-policy State Entropy Maximization via Stationary Distribution Correction Estimation](https://arxiv.org/abs/2512.10042)
*Jongmin Lee,Meiqi Sun,Pieter Abbeel*

Main category: cs.LG

TL;DR: 本文提出SEMDICE算法用于强化学习无监督预训练，实验表明其在最大化状态熵和下游任务适应效率上表现出色。


<details>
  <summary>Details</summary>
Motivation: 在强化学习无监督预训练中，探索不依赖特定任务奖励函数学习先验策略的方法，聚焦于状态熵最大化。

Method: 引入SEMDICE算法，从任意离线策略数据集中计算状态熵最大化策略，在平稳分布空间内直接优化策略。

Result: SEMDICE在最大化状态熵方面优于基线算法，在基于状态熵最大化的无监督强化学习预训练方法中，下游任务适应效率最佳。

Conclusion: SEMDICE算法在强化学习无监督预训练中有较好表现。

Abstract: In the unsupervised pre-training for reinforcement learning, the agent aims to learn a prior policy for downstream tasks without relying on task-specific reward functions. We focus on state entropy maximization (SEM), where the goal is to learn a policy that maximizes the entropy of the state stationary distribution. In this paper, we introduce SEMDICE, a principled off-policy algorithm that computes an SEM policy from an arbitrary off-policy dataset, which optimizes the policy directly within the space of stationary distributions. SEMDICE computes a single, stationary Markov state-entropy-maximizing policy from an arbitrary off-policy dataset. Experimental results demonstrate that SEMDICE outperforms baseline algorithms in maximizing state entropy while achieving the best adaptation efficiency for downstream tasks among SEM-based unsupervised RL pre-training methods.

</details>


### [93] [Local LLM Ensembles for Zero-shot Portuguese Named Entity Recognition](https://arxiv.org/abs/2512.10043)
*João Lucas Luz Lima Sarcinelli,Diego Furtado Silva*

Main category: cs.LG

TL;DR: 本文提出用于零样本命名实体识别（NER）的三步集成管道，结合多个小型大语言模型（LLM），在葡萄牙语NER数据集上表现良好，推动了可扩展、低资源和零样本NER发展。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在命名实体识别任务，尤其是低资源语言如葡萄牙语上表现不佳，现有LLM集成方法主要关注文本生成或分类，NER研究不足，因此提出新的集成方法。

Method: 提出三步集成管道，使用启发式方法选择最优模型组合，结合多个能力相近的本地运行LLM。

Result: 在四个葡萄牙语NER数据集中优于单个LLM，不同源数据集上的集成在跨数据集配置中通常也优于单个LLM，可能无需当前任务的标注数据。

Conclusion: 有效结合多个小型LLM，无需微调，推动了可扩展、低资源和零样本NER发展。

Abstract: Large Language Models (LLMs) excel in many Natural Language Processing (NLP) tasks through in-context learning but often under-perform in Named Entity Recognition (NER), especially for lower-resource languages like Portuguese. While open-weight LLMs enable local deployment, no single model dominates all tasks, motivating ensemble approaches. However, existing LLM ensembles focus on text generation or classification, leaving NER under-explored. In this context, this work proposes a novel three-step ensemble pipeline for zero-shot NER using similarly capable, locally run LLMs. Our method outperforms individual LLMs in four out of five Portuguese NER datasets by leveraging a heuristic to select optimal model combinations with minimal annotated data. Moreover, we show that ensembles obtained on different source datasets generally outperform individual LLMs in cross-dataset configurations, potentially eliminating the need for annotated data for the current task. Our work advances scalable, low-resource, and zero-shot NER by effectively combining multiple small LLMs without fine-tuning. Code is available at https://github.com/Joao-Luz/local-llm-ner-ensemble.

</details>


### [94] [Detailed balance in large language model-driven agents](https://arxiv.org/abs/2512.10047)
*Zhuo-Yang Song,Qing-Hong Cao,Ming-xing Luo,Hua Xing Zhu*

Main category: cs.LG

TL;DR: 本文提出基于最小作用量原理的方法研究大语言模型驱动智能体，发现生成转移中的细致平衡，尝试建立复杂AI系统宏观动力学理论。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏理解和统一大语言模型驱动智能体宏观动力学的理论框架。

Method: 基于最小作用量原理估计智能体中嵌入的大语言模型潜在生成方向性，实验测量状态间转移概率。

Result: 发现大语言模型生成转移中的细致平衡，表明其可能隐式学习一类潜在势函数。

Conclusion: 这是首次在大语言模型生成动力学中发现不依赖特定模型细节的宏观物理定律，旨在将AI智能体研究提升到可预测、可量化的科学层面。

Abstract: Large language model (LLM)-driven agents are emerging as a powerful new paradigm for solving complex problems. Despite the empirical success of these practices, a theoretical framework to understand and unify their macroscopic dynamics remains lacking. This Letter proposes a method based on the least action principle to estimate the underlying generative directionality of LLMs embedded within agents. By experimentally measuring the transition probabilities between LLM-generated states, we statistically discover a detailed balance in LLM-generated transitions, indicating that LLM generation may not be achieved by generally learning rule sets and strategies, but rather by implicitly learning a class of underlying potential functions that may transcend different LLM architectures and prompt templates. To our knowledge, this is the first discovery of a macroscopic physical law in LLM generative dynamics that does not depend on specific model details. This work is an attempt to establish a macroscopic dynamics theory of complex AI systems, aiming to elevate the study of AI agents from a collection of engineering practices to a science built on effective measurements that are predictable and quantifiable.

</details>


### [95] [DB2-TransF: All You Need Is Learnable Daubechies Wavelets for Time Series Forecasting](https://arxiv.org/abs/2512.10051)
*Moulik Gupta,Achyut Mani Tripathi*

Main category: cs.LG

TL;DR: 提出DB2 - TransF用于时间序列预测，减少内存使用且精度相当或更优。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer架构在处理大规模高维时间序列时计算复杂度高，限制可扩展性和适应性。

Method: 引入DB2 - TransF，用可学习的Daubechies小波系数层替代自注意力机制。

Result: 在13个标准预测基准测试中，DB2 - TransF预测精度与传统Transformer相当或更优，大幅减少内存使用。

Conclusion: DB2 - TransF是用于高级时间序列预测的可扩展且资源高效的框架。

Abstract: Time series forecasting requires models that can efficiently capture complex temporal dependencies, especially in large-scale and high-dimensional settings. While Transformer-based architectures excel at modeling long-range dependencies, their quadratic computational complexity poses limitations on scalability and adaptability. To overcome these challenges, we introduce DB2-TransF, a novel Transformer-inspired architecture that replaces the self-attention mechanism with a learnable Daubechies wavelet coefficient layer. This wavelet-based module efficiently captures multi-scale local and global patterns and enhances the modeling of correlations across multiple time series for the time series forecasting task. Extensive experiments on 13 standard forecasting benchmarks demonstrate that DB2-TransF achieves comparable or superior predictive accuracy to conventional Transformers, while substantially reducing memory usage for the time series forecasting task. The obtained experimental results position DB2-TransF as a scalable and resource-efficient framework for advanced time series forecasting. Our code is available at https://github.com/SteadySurfdom/DB2-TransF

</details>


### [96] [Mitigating Exposure Bias in Risk-Aware Time Series Forecasting with Soft Tokens](https://arxiv.org/abs/2512.10056)
*Alireza Namazi,Amirreza Dolatpour Fathkouhi,Heman Shakeri*

Main category: cs.LG

TL;DR: 提出SoTra方法用于预测控制，在血糖和血压预测中降低风险，支持安全关键的预测控制


<details>
  <summary>Details</summary>
Motivation: 标准自回归预测模型存在曝光偏差，多步预测不稳定，在糖尿病和血液动力学管理中不同操作区有不同临床风险

Method: 引入Soft - Token Trajectory Forecasting (SoTra)，传播连续概率分布减轻曝光偏差，学习带校准且有不确定性感知的轨迹，使用风险感知解码模块最小化预期临床危害

Result: 在血糖预测中，SoTra将基于区域的平均风险降低18%；在血压预测中，有效临床风险降低约15%

Conclusion: 这些改进支持其在安全关键的预测控制中使用

Abstract: Autoregressive forecasting is central to predictive control in diabetes and hemodynamic management, where different operating zones carry different clinical risks. Standard models trained with teacher forcing suffer from exposure bias, yielding unstable multi-step forecasts for closed-loop use. We introduce Soft-Token Trajectory Forecasting (SoTra), which propagates continuous probability distributions (``soft tokens'') to mitigate exposure bias and learn calibrated, uncertainty-aware trajectories. A risk-aware decoding module then minimizes expected clinical harm. In glucose forecasting, SoTra reduces average zone-based risk by 18\%; in blood-pressure forecasting, it lowers effective clinical risk by approximately 15\%. These improvements support its use in safety-critical predictive control.

</details>


### [97] [\textsc{Text2Graph}: Combining Lightweight LLMs and GNNs for Efficient Text Classification in Label-Scarce Scenarios](https://arxiv.org/abs/2512.10061)
*João Lucas Luz Lima Sarcinelli,Ricardo Marcondes Marcacini*

Main category: cs.LG

TL;DR: 提出开源包Text2Graph实现文本到图分类，可灵活结合LLM注释和GNN标签传播，在多数据集上测试并对比能耗和碳排放，图传播方法节能且效果有竞争力。


<details>
  <summary>Details</summary>
Motivation: LLMs作为零样本分类器在大规模注释中有高计算需求和环境成本，限制其在HPC环境中实用性，需更可持续工作流。

Method: 开发开源Python包Text2Graph，灵活结合LLM部分注释和GNN标签传播，用不同组件；在零样本设置下用五个数据集进行基准测试，对比多种变体。

Result: 图传播方法达到有竞争力的结果，能耗和环境成本大幅降低。

Conclusion: Text2Graph能实现更可持续的文本分类工作流，图传播方法是节能高效的选择。

Abstract: Large Language Models (LLMs) have become effective zero-shot classifiers, but their high computational requirements and environmental costs limit their practicality for large-scale annotation in high-performance computing (HPC) environments. To support more sustainable workflows, we present \textsc{Text2Graph}, an open-source Python package that provides a modular implementation of existing text-to-graph classification approaches. The framework enables users to combine LLM-based partial annotation with Graph Neural Network (GNN) label propagation in a flexible manner, making it straightforward to swap components such as feature extractors, edge construction methods, and sampling strategies. We benchmark \textsc{Text2Graph} on a zero-shot setting using five datasets spanning topic classification and sentiment analysis tasks, comparing multiple variants against other zero-shot approaches for text classification. In addition to reporting performance, we provide detailed estimates of energy consumption and carbon emissions, showing that graph-based propagation achieves competitive results at a fraction of the energy and environmental cost.

</details>


### [98] [MedXAI: A Retrieval-Augmented and Self-Verifying Framework for Knowledge-Guided Medical Image Analysis](https://arxiv.org/abs/2512.10098)
*Midhat Urooj,Ayan Banerjee,Farhat Shaikh,Kuntal Thakur,Sandeep Gupta*

Main category: cs.LG

TL;DR: 提出MedXAI框架解决医学AI图像诊断挑战，实验显示性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决医学AI图像诊断在领域偏移和罕见类别条件下的准确性和可解释性问题，以及深度学习模型的泛化、偏差和透明度问题。

Method: 引入基于专家知识的MedXAI框架，将深度视觉模型与临床专家知识集成，定位相关诊断特征。

Result: 在十个多中心数据集实验中，跨域泛化提高3%，罕见类别F1分数提高10%，大幅超越深度学习基线。

Conclusion: MedXAI在多模态医学AI中实现临床对齐解释，在域内和跨域表现出色，尤其对罕见病。

Abstract: Accurate and interpretable image-based diagnosis remains a fundamental challenge in medical AI, particularly under domain shifts and rare-class conditions. Deep learning models often struggle with real-world distribution changes, exhibit bias against infrequent pathologies, and lack the transparency required for deployment in safety-critical clinical environments. We introduce MedXAI (An Explainable Framework for Medical Imaging Classification), a unified expert knowledge based framework that integrates deep vision models with clinician-derived expert knowledge to improve generalization, reduce rare-class bias, and provide human-understandable explanations by localizing the relevant diagnostic features rather than relying on technical post-hoc methods (e.g., Saliency Maps, LIME). We evaluate MedXAI across heterogeneous modalities on two challenging tasks: (i) Seizure Onset Zone localization from resting-state fMRI, and (ii) Diabetic Retinopathy grading. Ex periments on ten multicenter datasets show consistent gains, including a 3% improvement in cross-domain generalization and a 10% improvmnet in F1 score of rare class, substantially outperforming strong deep learning baselines. Ablations confirm that the symbolic components act as effective clinical priors and regularizers, improving robustness under distribution shift. MedXAI delivers clinically aligned explanations while achieving superior in-domain and cross-domain performance, particularly for rare diseases in multimodal medical AI.

</details>


### [99] [CHyLL: Learning Continuous Neural Representations of Hybrid Systems](https://arxiv.org/abs/2512.10117)
*Sangli Teng,Hang Liu,Jingyu Song,Koushil Sreenath*

Main category: cs.LG

TL;DR: 提出CHyLL方法学习混合系统流，可准确预测并识别拓扑不变量，还应用于随机最优控制问题。


<details>
  <summary>Details</summary>
Motivation: 现有学习混合系统流的方法受模式切换和流中不连续性组合的困扰，学习有挑战。

Method: 提出CHyLL方法，不进行轨迹分割、使用事件函数或模式切换，通过重置映射重塑状态空间为分段光滑商流形，结合微分拓扑嵌入定理，同时学习高维空间中无奇异的神经嵌入和连续流。

Result: CHyLL能以较高精度准确预测混合系统的流，识别混合系统的拓扑不变量。

Conclusion: CHyLL方法在学习混合系统流方面有良好效果，可应用于随机最优控制问题。

Abstract: Learning the flows of hybrid systems that have both continuous and discrete time dynamics is challenging. The existing method learns the dynamics in each discrete mode, which suffers from the combination of mode switching and discontinuities in the flows. In this work, we propose CHyLL (Continuous Hybrid System Learning in Latent Space), which learns a continuous neural representation of a hybrid system without trajectory segmentation, event functions, or mode switching. The key insight of CHyLL is that the reset map glues the state space at the guard surface, reformulating the state space as a piecewise smooth quotient manifold where the flow becomes spatially continuous. Building upon these insights and the embedding theorems grounded in differential topology, CHyLL concurrently learns a singularity-free neural embedding in a higher-dimensional space and the continuous flow in it. We showcase that CHyLL can accurately predict the flow of hybrid systems with superior accuracy and identify the topological invariants of the hybrid systems. Finally, we apply CHyLL to the stochastic optimal control problem.

</details>


### [100] [Hybrid Physics-ML Model for Forward Osmosis Flux with Complete Uncertainty Quantification](https://arxiv.org/abs/2512.10457)
*Shiv Ratn,Shivang Rampriyan,Bahni Ray*

Main category: cs.LG

TL;DR: 本文提出一种鲁棒混合物理 - 机器学习框架预测正向渗透水通量，在少量数据上取得优异结果，可用于过程优化和数字孪生开发。


<details>
  <summary>Details</summary>
Motivation: 正向渗透水通量准确建模存在挑战，传统机械模型和纯数据驱动模型有各自缺陷，需要新方法。

Method: 引入基于高斯过程回归的鲁棒混合物理 - 机器学习框架，在物理模型预测值与实际值的残差上训练GPR，并实施全不确定性量化方法。

Result: 模型在仅120个数据点上训练，独立测试数据的平均绝对百分比误差为0.26%，R2为0.999。

Conclusion: 该模型是正向渗透过程优化和数字孪生开发的强大可靠替代模型。

Abstract: Forward Osmosis (FO) is a promising low-energy membrane separation technology, but challenges in accurately modelling its water flux (Jw) persist due to complex internal mass transfer phenomena. Traditional mechanistic models struggle with empirical parameter variability, while purely data-driven models lack physical consistency and rigorous uncertainty quantification (UQ). This study introduces a novel Robust Hybrid Physics-ML framework employing Gaussian Process Regression (GPR) for highly accurate, uncertainty-aware Jw prediction. The core innovation lies in training the GPR on the residual error between the detailed, non-linear FO physical model prediction (Jw_physical) and the experimental water flux (Jw_actual). Crucially, we implement a full UQ methodology by decomposing the total predictive variance (sigma2_total) into model uncertainty (epistemic, from GPR's posterior variance) and input uncertainty (aleatoric, analytically propagated via the Delta method for multi-variate correlated inputs). Leveraging the inherent strength of GPR in low-data regimes, the model, trained on a meagre 120 data points, achieved a state-of-the-art Mean Absolute Percentage Error (MAPE) of 0.26% and an R2 of 0.999 on the independent test data, validating a truly robust and reliable surrogate model for advanced FO process optimization and digital twin development.

</details>


### [101] [Partitioning the Sample Space for a More Precise Shannon Entropy Estimation](https://arxiv.org/abs/2512.10133)
*Gabriel F. A. Bastos,Jugurta Montalvão*

Main category: cs.LG

TL;DR: 本文介绍一种离散熵估计器，实验表明其在欠采样情况下表现良好。


<details>
  <summary>Details</summary>
Motivation: 从可能样本数量小于可能结果数量的小数据集中可靠地进行香农熵的数据驱动估计在多个应用中至关重要。

Method: 利用可分解性，结合对缺失质量和未出现结果数量的估计，补偿由此产生的负偏差。

Result: 所提方法在欠采样情况下优于一些经典估计器，与一些先进估计器表现相当。

Conclusion: 所提出的离散熵估计器在小数据集的熵估计方面具有较好的性能。

Abstract: Reliable data-driven estimation of Shannon entropy from small data sets, where the number of examples is potentially smaller than the number of possible outcomes, is a critical matter in several applications. In this paper, we introduce a discrete entropy estimator, where we use the decomposability property in combination with estimations of the missing mass and the number of unseen outcomes to compensate for the negative bias induced by them. Experimental results show that the proposed method outperforms some classical estimators in undersampled regimes, and performs comparably with some well-established state-of-the-art estimators.

</details>


### [102] [Sequence-to-Image Transformation for Sequence Classification Using Rips Complex Construction and Chaos Game Representation](https://arxiv.org/abs/2512.10141)
*Sarwan Ali,Taslim Murad,Imdadullah Khan*

Main category: cs.LG

TL;DR: 本文提出一种拓扑方法将分子序列转化为图像用于分类，实验表现优于多种方法。


<details>
  <summary>Details</summary>
Motivation: 传统特征工程有稀疏性和计算复杂度问题，深度学习模型在表格生物数据上表现不佳。

Method: 结合混沌游戏表示（CGR）和代数拓扑中的Rips复形构建，将序列元素映射到二维坐标，计算成对距离并构建Rips复形。

Result: 在抗癌肽数据集上实验，在乳腺癌和肺癌数据集上分别达到86.8%和94.5%的准确率，优于基于向量、序列语言模型和现有基于图像的方法。

Conclusion: 拓扑表示保留关键序列信息，能有效利用基于视觉的深度学习架构进行分子序列分析。

Abstract: Traditional feature engineering approaches for molecular sequence classification suffer from sparsity issues and computational complexity, while deep learning models often underperform on tabular biological data. This paper introduces a novel topological approach that transforms molecular sequences into images by combining Chaos Game Representation (CGR) with Rips complex construction from algebraic topology. Our method maps sequence elements to 2D coordinates via CGR, computes pairwise distances, and constructs Rips complexes to capture both local structural and global topological features. We provide formal guarantees on representation uniqueness, topological stability, and information preservation. Extensive experiments on anticancer peptide datasets demonstrate superior performance over vector-based, sequence language models, and existing image-based methods, achieving 86.8\% and 94.5\% accuracy on breast and lung cancer datasets, respectively. The topological representation preserves critical sequence information while enabling effective utilization of vision-based deep learning architectures for molecular sequence analysis.

</details>


### [103] [Extrapolation of Periodic Functions Using Binary Encoding of Continuous Numerical Values](https://arxiv.org/abs/2512.10817)
*Brian P. Powell,Jordan A. Caraballo-Vega,Mark L. Carroll,Thomas Maxwell,Andrew Ptak,Greg Olmschenk,Jorge Martinez-Palomera*

Main category: cs.LG

TL;DR: 二进制编码使神经网络能在训练范围外外推周期函数，NB2E编码让MLP成功外推多样周期信号。


<details>
  <summary>Details</summary>
Motivation: 探索让神经网络在训练范围外外推周期函数的方法。

Method: 引入Normalized Base - 2 Encoding (NB2E)对连续数值进行编码。

Result: 使用NB2E编码，普通多层感知机（MLP）能成功外推多样周期信号，且内部激活分析显示NB2E能诱导位相表示。

Conclusion: 二进制编码可使神经网络在训练范围外外推周期函数，NB2E编码让MLP能独立于位置学习和外推信号结构。

Abstract: We report the discovery that binary encoding allows neural networks to extrapolate periodic functions beyond their training bounds. We introduce Normalized Base-2 Encoding (NB2E) as a method for encoding continuous numerical values and demonstrate that, using this input encoding, vanilla multi-layer perceptrons (MLP) successfully extrapolate diverse periodic signals without prior knowledge of their functional form. Internal activation analysis reveals that NB2E induces bit-phase representations, enabling MLPs to learn and extrapolate signal structure independently of position.

</details>


### [104] [Murmur2Vec: A Hashing Based Solution For Embedding Generation Of COVID-19 Spike Sequences](https://arxiv.org/abs/2512.10147)
*Sarwan Ali,Taslim Murad*

Main category: cs.LG

TL;DR: 本文提出一种可扩展的嵌入方法用于SARS - CoV - 2谱系分类，经评估该方法效率高、准确率可观。


<details>
  <summary>Details</summary>
Motivation: 现有新冠病毒序列计算分析方法存在计算量大、效率低等局限，需要新方法进行大规模病毒序列分析。

Method: 聚焦刺突蛋白区域相关的SARS - CoV - 2谱系，利用哈希生成刺突序列的紧凑低维表示，训练机器学习模型进行谱系分类。

Result: 所提嵌入方法效率大幅提升，分类准确率达86.4%，嵌入生成时间最多减少99.81%。

Conclusion: 该方法是大规模病毒序列分析快速、有效且可扩展的解决方案。

Abstract: Early detection and characterization of coronavirus disease (COVID-19), caused by SARS-CoV-2, remain critical for effective clinical response and public-health planning. The global availability of large-scale viral sequence data presents significant opportunities for computational analysis; however, existing approaches face notable limitations. Phylogenetic tree-based methods are computationally intensive and do not scale efficiently to today's multi-million-sequence datasets. Similarly, current embedding-based techniques often rely on aligned sequences or exhibit suboptimal predictive performance and high runtime costs, creating barriers to practical large-scale analysis. In this study, we focus on the most prevalent SARS-CoV-2 lineages associated with the spike protein region and introduce a scalable embedding method that leverages hashing to generate compact, low-dimensional representations of spike sequences. These embeddings are subsequently used to train a variety of machine learning models for supervised lineage classification. We conduct an extensive evaluation comparing our approach with multiple baseline and state-of-the-art biological sequence embedding methods across diverse metrics. Our results demonstrate that the proposed embeddings offer substantial improvements in efficiency, achieving up to 86.4\% classification accuracy while reducing embedding generation time by as much as 99.81\%. This highlights the method's potential as a fast, effective, and scalable solution for large-scale viral sequence analysis.

</details>


### [105] [Generative Modeling from Black-box Corruptions via Self-Consistent Stochastic Interpolants](https://arxiv.org/abs/2512.10857)
*Chirag Modi,Jiequn Han,Eric Vanden-Eijnden,Joan Bruna*

Main category: cs.LG

TL;DR: 提出基于随机插值的自洽随机插值法（SCSI）解决从含噪数据构建生成模型的逆问题，有计算高效、灵活和理论保证等优点，在图像处理和科学重建中表现出色。


<details>
  <summary>Details</summary>
Motivation: 许多科学和工程领域只有被噪声、病态通道污染的测量数据，需要在分布层面解决逆问题来构建原始数据的生成模型。

Method: 基于随机插值，仅利用含噪数据集和对污染通道的黑盒访问，迭代更新含噪和干净数据样本之间的传输映射。

Result: 在自然图像处理和科学重建的逆问题中表现优越，在适当假设下建立了方案的收敛性保证。

Conclusion: 所提出的自洽随机插值法（SCSI）是解决从含噪数据构建生成模型逆问题的有效方法，具有计算效率、灵活性和理论保障。

Abstract: Transport-based methods have emerged as a leading paradigm for building generative models from large, clean datasets. However, in many scientific and engineering domains, clean data are often unavailable: instead, we only observe measurements corrupted through a noisy, ill-conditioned channel. A generative model for the original data thus requires solving an inverse problem at the level of distributions. In this work, we introduce a novel approach to this task based on Stochastic Interpolants: we iteratively update a transport map between corrupted and clean data samples using only access to the corrupted dataset as well as black box access to the corruption channel. Under appropriate conditions, this iterative procedure converges towards a self-consistent transport map that effectively inverts the corruption channel, thus enabling a generative model for the clean data. We refer to the resulting method as the self-consistent stochastic interpolant (SCSI). It (i) is computationally efficient compared to variational alternatives, (ii) highly flexible, handling arbitrary nonlinear forward models with only black-box access, and (iii) enjoys theoretical guarantees. We demonstrate superior performance on inverse problems in natural image processing and scientific reconstruction, and establish convergence guarantees of the scheme under appropriate assumptions.

</details>


### [106] [Rethinking Causal Discovery Through the Lens of Exchangeability](https://arxiv.org/abs/2512.10152)
*Tiago Brogueira,Mário Figueiredo*

Main category: cs.LG

TL;DR: 本文提出将独立同分布（i.i.d.）设置重新构建为可交换性框架，并引入仅基于可交换性假设的合成数据集，证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统因果发现方法在i.i.d.和时间序列数据两种不同机制下发展，本文认为i.i.d.设置应重新构建为更通用的可交换性框架。

Method: 提出概念和实证两方面论据，引入仅基于可交换性假设的合成数据集，提出基于神经网络的因果发现算法。

Result: 新的可交换合成数据集比其他i.i.d.合成数据集更接近真实世界“i.i.d.”数据集的统计结构，基于该数据集训练的算法在真实世界基准上表现与其他先进方法相似。

Conclusion: i.i.d.设置可且应重新构建为可交换性框架，新合成数据集有效。

Abstract: Causal discovery methods have traditionally been developed under two distinct regimes: independent and identically distributed (i.i.d.) and timeseries data, each governed by separate modelling assumptions. In this paper, we argue that the i.i.d. setting can and should be reframed in terms of exchangeability, a strictly more general symmetry principle. We present the implications of this reframing, alongside two core arguments: (1) a conceptual argument, based on extending the dependency of experimental causal inference on exchangeability to causal discovery; and (2) an empirical argument, showing that many existing i.i.d. causal-discovery methods are predicated on exchangeability assumptions, and that the sole extensive widely-used real-world "i.i.d." benchmark (the Tübingen dataset) consists mainly of exchangeable (and not i.i.d.) examples. Building on this insight, we introduce a novel synthetic dataset that enforces only the exchangeability assumption, without imposing the stronger i.i.d. assumption. We show that our exchangeable synthetic dataset mirrors the statistical structure of the real-world "i.i.d." dataset more closely than all other i.i.d. synthetic datasets. Furthermore, we demonstrate the predictive capability of this dataset by proposing a neural-network-based causal-discovery algorithm trained exclusively on our synthetic dataset, and which performs similarly to other state-of-the-art i.i.d. methods on the real-world benchmark.

</details>


### [107] [Decoupled Q-Chunking](https://arxiv.org/abs/2512.10926)
*Qiyang Li,Seohong Park,Sergey Levine*

Main category: cs.LG

TL;DR: 本文指出TD方法有自举偏差问题，现有分块评论家方法提取策略有挑战。提出解耦评论家与策略的分块长度，设计新算法，在长程离线目标条件任务上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决TD方法的自举偏差问题，以及分块评论家方法提取策略时的开环次优和长分块策略学习困难的问题。

Method: 解耦评论家与策略的分块长度，针对部分动作分块的蒸馏评论家优化策略，通过乐观回溯原分块评论家来构建蒸馏评论家。

Result: 在具有挑战性的长程离线目标条件任务上，该方法可靠地优于先前方法。

Conclusion: 所提出的算法保留了多步价值传播的好处，同时避免了开环次优和长动作分块策略学习的困难。

Abstract: Temporal-difference (TD) methods learn state and action values efficiently by bootstrapping from their own future value predictions, but such a self-bootstrapping mechanism is prone to bootstrapping bias, where the errors in the value targets accumulate across steps and result in biased value estimates. Recent work has proposed to use chunked critics, which estimate the value of short action sequences ("chunks") rather than individual actions, speeding up value backup. However, extracting policies from chunked critics is challenging: policies must output the entire action chunk open-loop, which can be sub-optimal for environments that require policy reactivity and also challenging to model especially when the chunk length grows. Our key insight is to decouple the chunk length of the critic from that of the policy, allowing the policy to operate over shorter action chunks. We propose a novel algorithm that achieves this by optimizing the policy against a distilled critic for partial action chunks, constructed by optimistically backing up from the original chunked critic to approximate the maximum value achievable when a partial action chunk is extended to a complete one. This design retains the benefits of multi-step value propagation while sidestepping both the open-loop sub-optimality and the difficulty of learning action chunking policies for long action chunks. We evaluate our method on challenging, long-horizon offline goal-conditioned tasks and show that it reliably outperforms prior methods. Code: github.com/ColinQiyangLi/dqc.

</details>


### [108] [CIEGAD: Cluster-Conditioned Interpolative and Extrapolative Framework for Geometry-Aware and Domain-Aligned Data Augmentation](https://arxiv.org/abs/2512.10178)
*Keito Inoshita,Xiaokang Zhou,Akira Kawai,Katsutoshi Yada*

Main category: cs.LG

TL;DR: 针对深度学习数据稀缺和标签分布不平衡问题，提出CIEGAD数据增强框架，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 实际深度学习部署中数据稀缺和标签分布不平衡，阻碍模型训练，现有大语言模型数据增强缺乏集成框架。

Method: 提出CIEGAD框架，通过集群条件构建域配置文件，用分层频率 - 几何分配进行生成，通过插值和外推合成控制生成方向，结合几何约束过滤和LLM - as - a - Judge机制进行质量控制。

Result: 在多分类任务实验中，CIEGAD有效扩展数据分布外围，保持生成数据与真实数据高对齐和语义多样性，在长尾和多类分类任务中提高F1和召回率。

Conclusion: CIEGAD是实用的数据增强框架，能补充数据不足区域并保持与真实数据对齐。

Abstract: In practical deep learning deployment, the scarcity of data and the imbalance of label distributions often lead to semantically uncovered regions within the real-world data distribution, hindering model training and causing misclassification near class boundaries as well as unstable behaviors in peripheral areas. Although recent large language models (LLMs) show promise for data augmentation, an integrated framework that simultaneously achieves directional control of generation, domain alignment, and quality control has not yet been fully established. To address these challenges, we propose a Cluster-conditioned Interpolative and Extrapolative framework for Geometry-Aware and Domain-aligned data augmentation (CIEGAD), which systematically complements both in-distribution and out-of-distribution semantically uncovered regions. CIEGAD constructs domain profiles through cluster conditioning, allocates generation with a hierarchical frequency-geometric allocation integrating class frequency and geometric indicators, and finely controls generation directions via the coexistence of interpolative and extrapolative synthesis. It further performs quality control through geometry-constrained filtering combined with an LLM-as-a-Judge mechanism. Experiments on multiple classification tasks demonstrate that CIEGAD effectively extends the periphery of real-world data distributions while maintaining high alignment between generated and real-world data as well as semantic diversity. In particular, for long-tailed and multi-class classification tasks, CIEGAD consistently improves F1 and recall, validating the triple harmony of distributional consistency, diversity, and quality. These results indicate that CIEGAD serves as a practically oriented data augmentation framework that complements underrepresented regions while preserving alignment with real-world data.

</details>


### [109] [Assessing Neuromorphic Computing for Fingertip Force Decoding from Electromyography](https://arxiv.org/abs/2512.10179)
*Abolfazl Shahrooei,Luke Arthur,Om Patel,Derek Kamper*

Main category: cs.LG

TL;DR: 评估尖峰神经网络（SNN）和时间卷积网络（TCN）从高密度表面肌电图（HD - sEMG）解码指尖力的性能，TCN更准确，但SNN有改进潜力。


<details>
  <summary>Details</summary>
Motivation: HD - sEMG为辅助和康复控制提供非侵入式神经接口，但将神经活动映射到用户运动意图有挑战，需评估不同模型解码指尖力的能力。

Method: 收集单参与者10次试验数据，用FastICA分解获得运动单元活动，采用端到端因果卷积在重叠窗口上训练模型。

Result: 在保留试验中，TCN的均方根误差为4.44% MVC（皮尔逊r = 0.974），SNN为8.25% MVC（r = 0.922）。

Conclusion: TCN更准确，但SNN是现实的神经形态基线，通过适度架构和超参数改进可缩小差距。

Abstract: High-density surface electromyography (HD-sEMG) provides a noninvasive neural interface for assistive and rehabilitation control, but mapping neural activity to user motor intent remains challenging. We assess a spiking neural network (SNN) as a neuromorphic architecture against a temporal convolutional network (TCN) for decoding fingertip force from motor-unit (MU) firing derived from HD-sEMG. Data were collected from a single participant (10 trials) with two forearm electrode arrays; MU activity was obtained via FastICA-based decomposition, and models were trained on overlapping windows with end-to-end causal convolutions. On held-out trials, the TCN achieved 4.44% MVC RMSE (Pearson r = 0.974) while the SNN achieved 8.25% MVC (r = 0.922). While the TCN was more accurate, we view the SNN as a realistic neuromorphic baseline that could close much of this gap with modest architectural and hyperparameter refinements.

</details>


### [110] [MiniF2F-Dafny: LLM-Guided Mathematical Theorem Proving via Auto-Active Verification](https://arxiv.org/abs/2512.10187)
*Mantas Baksys,Stefan Zetzsche,Olivier Bouissou*

Main category: cs.LG

TL;DR: 提出miniF2F - Dafny，将miniF2F基准转换至Dafny，测试Dafny自动化证明能力及LLMs提供证明提示效果，展示了分工模式。


<details>
  <summary>Details</summary>
Motivation: 此前数学推理基准miniF2F仅存在于交互式定理证明器中，研究将其转换到自动化定理证明器Dafny上的效果。

Method: 将miniF2F转换到Dafny，测试Dafny自动化证明；对空证明失败的问题，用12个现成LLMs提供证明提示并采用迭代纠错。

Result: Dafny空证明可验证测试集40.6%、验证集44.7%；最佳LLM模型迭代纠错后pass@4成功率达55.7%。

Conclusion: 初步结果显示了有效分工：LLMs提供高层指导，自动化处理底层细节。

Abstract: We present miniF2F-Dafny, the first translation of the mathematical reasoning benchmark miniF2F to an automated theorem prover: Dafny. Previously, the benchmark existed only in interactive theorem provers (Lean, Isabelle, HOL Light, Metamath). We find that Dafny's automation verifies 99/244 (40.6%) of the test set and 109/244 (44.7%) of the validation set with empty proofs--requiring no manual proof steps. For problems where empty proofs fail, we evaluate 12 off-the-shelf LLMs on providing proof hints. The best model we test achieves 55.7% pass@4 success rate employing iterative error correction. These preliminary results highlight an effective division of labor: LLMs provide high-level guidance while automation handles low-level details. Our benchmark can be found on GitHub at http://github.com/dafny-lang/miniF2F .

</details>


### [111] [Exact Recovery of Non-Random Missing Multidimensional Time Series via Temporal Isometric Delay-Embedding Transform](https://arxiv.org/abs/2512.10191)
*Hao Shu,Jicheng Li,Yu Jin,Ling Zhou*

Main category: cs.LG

TL;DR: 提出LRTC - TIDT模型解决多维时间序列非随机缺失数据恢复问题，在模拟和真实任务中效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理多维时间序列非随机缺失数据时，存在难以解释低秩性和缺乏精确恢复理论的问题，威胁数据驱动分析和决策可靠性。

Method: 提出时间等距延迟嵌入变换构建Hankel张量，在此基础上开发LRTC - TIDT模型表征低秩结构。

Result: 模拟实验证实满足条件时模型可精确恢复，在真实任务中表现优于现有张量方法。

Conclusion: LRTC - TIDT模型是处理多维时间序列非随机缺失数据的有效方法，代码公开。

Abstract: Non-random missing data is a ubiquitous yet undertreated flaw in multidimensional time series, fundamentally threatening the reliability of data-driven analysis and decision-making. Pure low-rank tensor completion, as a classical data recovery method, falls short in handling non-random missingness, both methodologically and theoretically. Hankel-structured tensor completion models provide a feasible approach for recovering multidimensional time series with non-random missing patterns. However, most Hankel-based multidimensional data recovery methods both suffer from unclear sources of Hankel tensor low-rankness and lack an exact recovery theory for non-random missing data. To address these issues, we propose the temporal isometric delay-embedding transform, which constructs a Hankel tensor whose low-rankness is naturally induced by the smoothness and periodicity of the underlying time series. Leveraging this property, we develop the \textit{Low-Rank Tensor Completion with Temporal Isometric Delay-embedding Transform} (LRTC-TIDT) model, which characterizes the low-rank structure under the \textit{Tensor Singular Value Decomposition} (t-SVD) framework. Once the prescribed non-random sampling conditions and mild incoherence assumptions are satisfied, the proposed LRTC-TIDT model achieves exact recovery, as confirmed by simulation experiments under various non-random missing patterns. Furthermore, LRTC-TIDT consistently outperforms existing tensor-based methods across multiple real-world tasks, including network flow reconstruction, urban traffic estimation, and temperature field prediction. Our implementation is publicly available at https://github.com/HaoShu2000/LRTC-TIDT.

</details>


### [112] [Federated Domain Generalization with Latent Space Inversion](https://arxiv.org/abs/2512.10224)
*Ragja Palakkadavath,Hung Le,Thanh Nguyen-Tang,Svetha Venkatesh,Sunil Gupta*

Main category: cs.LG

TL;DR: 提出新技术和聚合策略解决FedDG中隐私和本地适应性问题，实验效果优且通信开销小。


<details>
  <summary>Details</summary>
Motivation: 现有FedDG方法在提高全局模型泛化能力时会损害数据隐私，且非i.i.d客户端模型聚合会丢弃本地适应性。

Method: 提出潜空间反转技术改进本地客户端训练以保护隐私；提出重要权重聚合策略处理非i.i.d客户端模型聚合。

Result: 所提方法比现有技术取得更优结果，且通信开销更小。

Conclusion: 提出的新技术和聚合策略能有效解决FedDG中的隐私和适应问题。

Abstract: Federated domain generalization (FedDG) addresses distribution shifts among clients in a federated learning framework. FedDG methods aggregate the parameters of locally trained client models to form a global model that generalizes to unseen clients while preserving data privacy. While improving the generalization capability of the global model, many existing approaches in FedDG jeopardize privacy by sharing statistics of client data between themselves. Our solution addresses this problem by contributing new ways to perform local client training and model aggregation. To improve local client training, we enforce (domain) invariance across local models with the help of a novel technique, \textbf{latent space inversion}, which enables better client privacy. When clients are not \emph{i.i.d}, aggregating their local models may discard certain local adaptations. To overcome this, we propose an \textbf{important weight} aggregation strategy to prioritize parameters that significantly influence predictions of local models during aggregation. Our extensive experiments show that our approach achieves superior results over state-of-the-art methods with less communication overhead.

</details>


### [113] [Adaptive Information Routing for Multimodal Time Series Forecasting](https://arxiv.org/abs/2512.10229)
*Jun Seo,Hyeokjun Choe,Seohui Bae,Soyeon Park,Wonbin Ahn,Taeyoon Lim,Junhyuk Kang,Sangjun Han,Jaehoon Lee,Dongwan Kang,Minjae Kim,Sungdong Yoo,Soonyoung Lee*

Main category: cs.LG

TL;DR: 文章提出用于多模态时间序列预测的自适应信息路由（AIR）框架，结合文本精炼管道和基准，实验显示其能增强预测准确性。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测仅依靠历史数据，信息有限，难以准确预测，需探索多模态预测方法。

Method: 引入 AIR 框架，利用文本信息动态引导时间序列模型；提出文本精炼管道，将原始文本数据转换为适合多模态预测的形式；引入用于多模态预测实验的基准。

Result: 用原油价格和汇率等现实市场数据实验表明，AIR 可利用文本输入有效调节时间序列模型行为，显著提高预测准确性。

Conclusion: AIR 框架在多模态时间序列预测中有效，能提升各类时间序列预测任务的准确性。

Abstract: Time series forecasting is a critical task for artificial intelligence with numerous real-world applications. Traditional approaches primarily rely on historical time series data to predict the future values. However, in practical scenarios, this is often insufficient for accurate predictions due to the limited information available. To address this challenge, multimodal time series forecasting methods which incorporate additional data modalities, mainly text data, alongside time series data have been explored. In this work, we introduce the Adaptive Information Routing (AIR) framework, a novel approach for multimodal time series forecasting. Unlike existing methods that treat text data on par with time series data as interchangeable auxiliary features for forecasting, AIR leverages text information to dynamically guide the time series model by controlling how and to what extent multivariate time series information should be combined. We also present a text-refinement pipeline that employs a large language model to convert raw text data into a form suitable for multimodal forecasting, and we introduce a benchmark that facilitates multimodal forecasting experiments based on this pipeline. Experiment results with the real world market data such as crude oil price and exchange rates demonstrate that AIR effectively modulates the behavior of the time series model using textual inputs, significantly enhancing forecasting accuracy in various time series forecasting tasks.

</details>


### [114] [R^2-HGP: A Double-Regularized Gaussian Process for Heterogeneous Transfer Learning](https://arxiv.org/abs/2512.10258)
*Duo Wang,Xinming Wang,Chao Wang,Xiaowei Yue,Jianguo Wu*

Main category: cs.LG

TL;DR: 传统多输出高斯过程模型在迁移学习有挑战，本文提出双正则化异构高斯过程框架R^2 - HGP，经实验验证有效且优于基准模型。


<details>
  <summary>Details</summary>
Motivation: 传统多输出高斯过程模型在迁移学习中面临输入空间异构、忽略先验知识和物理信息、易负迁移等问题，传统模型无法统一解决。

Method: 提出可训练先验概率映射模型对齐异构输入域，构建多源迁移GP模型并集成到基于CVAE的框架，引入物理信息作为正则化项，对迁移系数施加稀疏惩罚。

Result: 大量模拟和实际工程案例研究验证了R^2 - HGP的有效性，在多种评估指标上始终优于现有基准模型。

Conclusion: R^2 - HGP框架能有效解决多输出高斯过程模型在迁移学习中的问题，具有良好性能和应用价值。

Abstract: Multi-output Gaussian process (MGP) models have attracted significant attention for their flexibility and uncertainty-quantification capabilities, and have been widely adopted in multi-source transfer learning scenarios due to their ability to capture inter-task correlations. However, they still face several challenges in transfer learning. First, the input spaces of the source and target domains are often heterogeneous, which makes direct knowledge transfer difficult. Second, potential prior knowledge and physical information are typically ignored during heterogeneous transfer, hampering the utilization of domain-specific insights and leading to unstable mappings. Third, inappropriate information sharing among target and sources can easily lead to negative transfer. Traditional models fail to address these issues in a unified way. To overcome these limitations, this paper proposes a Double-Regularized Heterogeneous Gaussian Process framework (R^2-HGP). Specifically, a trainable prior probability mapping model is first proposed to align the heterogeneous input domains. The resulting aligned inputs are treated as latent variables, upon which a multi-source transfer GP model is constructed and the entire structure is integrated into a novel conditional variational autoencoder (CVAE) based framework. Physical insights is further incorporated as a regularization term to ensure that the alignment results adhere to known physical knowledge. Next, within the multi-source transfer GP model, a sparsity penalty is imposed on the transfer coefficients, enabling the model to adaptively select the most informative source outputs and suppress negative transfer. Extensive simulations and real-world engineering case studies validate the effectiveness of our R^2-HGP, demonstrating consistent superiority over state-of-the-art benchmarks across diverse evaluation metrics.

</details>


### [115] [A Kernel-based Resource-efficient Neural Surrogate for Multi-fidelity Prediction of Aerodynamic Field](https://arxiv.org/abs/2512.10287)
*Apurba Sarker,Reza T. Batley,Darshan Sarojini,Sourav Saha*

Main category: cs.LG

TL;DR: 本文提出使用基于核的神经替代模型KHRONOS预测空气动力场，与三种模型对比，结果表明KHRONOS在资源受限条件下表现出色。


<details>
  <summary>Details</summary>
Motivation: 替代模型可替代高成本空气动力学模拟，研究旨在用KHRONOS在计算资源受限下预测空气动力场。

Method: 将稀疏高保真数据与低保真信息融合，基于变分原理、插值理论和张量分解构建KHRONOS，用AirfRANS数据集和NeuralFoil生成数据，与MLP、GNN、PINN对比。

Result: 所有模型最终预测精度相当，但KHRONOS在资源受限条件下所需可训练参数少，训练和推理速度快。

Conclusion: KHRONOS等架构在多保真空气动力场预测中可平衡精度和效率。

Abstract: Surrogate models provide fast alternatives to costly aerodynamic simulations and are extremely useful in design and optimization applications. This study proposes the use of a recent kernel-based neural surrogate, KHRONOS. In this work, we blend sparse high-fidelity (HF) data with low-fidelity (LF) information to predict aerodynamic fields under varying constraints in computational resources. Unlike traditional approaches, KHRONOS is built upon variational principles, interpolation theory, and tensor decomposition. These elements provide a mathematical basis for heavy pruning compared to dense neural networks. Using the AirfRANS dataset as a high-fidelity benchmark and NeuralFoil to generate low-fidelity counterparts, this work compares the performance of KHRONOS with three contemporary model architectures: a multilayer perceptron (MLP), a graph neural network (GNN), and a physics-informed neural network (PINN). We consider varying levels of high-fidelity data availability (0%, 10%, and 30%) and increasingly complex geometry parameterizations. These are used to predict the surface pressure coefficient distribution over the airfoil. Results indicate that, whilst all models eventually achieve comparable predictive accuracy, KHRONOS excels in resource-constrained conditions. In this domain, KHRONOS consistently requires orders of magnitude fewer trainable parameters and delivers much faster training and inference than contemporary dense neural networks at comparable accuracy. These findings highlight the potential of KHRONOS and similar architectures to balance accuracy and efficiency in multi-fidelity aerodynamic field prediction.

</details>


### [116] [An Interpretable AI Tool for SAVR vs TAVR in Low to Intermediate Risk Patients with Severe Aortic Stenosis](https://arxiv.org/abs/2512.10308)
*Vasiliki Stoumpou,Maciej Tysarowski,Talhat Azemi,Jawad Haider,Howard L. Haronian,Robert C. Hagberg,Dimitris Bertsimas*

Main category: cs.LG

TL;DR: 针对中低风险严重主动脉瓣狭窄患者，提出可解释的处方框架推荐手术，能降低5年死亡率。


<details>
  <summary>Details</summary>
Motivation: 临床中对中低风险严重主动脉瓣狭窄患者手术方式选择存在差异，且缺乏可解释、个性化治疗推荐来优化长期结果。

Method: 引入可解释的处方框架，整合预后匹配、反事实结果建模和最优决策树，依据两地医院数据进行分析以推荐降低5年预期死亡率的治疗方法。

Result: 应用最优决策树处方，可使两地5年死亡率分别降低20.3%和13.8%，决策边界与实际结果和临床观察一致。

Conclusion: 该可解释的处方框架首次为手术方式选择提供透明、数据驱动的推荐，改善长期结果，推动结构性心脏病精准医疗。

Abstract: Background. Treatment selection for low to intermediate risk patients with severe aortic stenosis between surgical (SAVR) and transcatheter (TAVR) aortic valve replacement remains variable in clinical practice, driven by patient heterogeneity and institutional preferences. While existing models predict postprocedural risk, there is a lack of interpretable, individualized treatment recommendations that directly optimize long-term outcomes.
  Methods. We introduce an interpretable prescriptive framework that integrates prognostic matching, counterfactual outcome modeling, and an Optimal Policy Tree (OPT) to recommend the treatment minimizing expected 5-year mortality. Using data from Hartford Hospital and St. Vincent's Hospital, we emulate randomization via prognostic matching and sample weighting and estimate counterfactual mortality under both SAVR and TAVR. The policy model, trained on these counterfactual predictions, partitions patients into clinically coherent subgroups and prescribes the treatment associated with lower estimated risk.
  Findings. If the OPT prescriptions are applied, counterfactual evaluation showed an estimated reduction in 5-year mortality of 20.3\% in Hartford and 13.8\% in St. Vincent's relative to real-life prescriptions, showing promising generalizability to unseen data from a different institution. The learned decision boundaries aligned with real-world outcomes and clinical observations.
  Interpretation. Our interpretable prescriptive framework is, to the best of our knowledge, the first to provide transparent, data-driven recommendations for TAVR versus SAVR that improve estimated long-term outcomes both in an internal and external cohort, while remaining clinically grounded and contributing toward a more systematic and evidence-based approach to precision medicine in structural heart disease.

</details>


### [117] [A Privacy-Preserving Cloud Architecture for Distributed Machine Learning at Scale](https://arxiv.org/abs/2512.10341)
*Vinoth Punniyamoorthy,Ashok Gadi Parthi,Mayilsamy Palanigounder,Ravi Kiran Kodali,Bikesh Kumar,Kabilan Kannan*

Main category: cs.LG

TL;DR: 本文介绍云原生隐私保护架构，支持安全训练与推理，原型展示其优势，实验证明实用且可扩展部署。


<details>
  <summary>Details</summary>
Motivation: 分布式机器学习系统需强隐私保障、可验证合规性及跨异构多云环境可扩展部署。

Method: 引入集成联邦学习、差分隐私、零知识合规证明和强化学习驱动的自适应治理的云原生隐私保护架构。

Result: 全功能原型降低成员推理风险、执行隐私预算、保证模型性能；多机构工作负载实验显示架构实用且开销小。

Conclusion: 该框架为大规模部署可信合规的分布式机器学习系统奠定了实践基础。

Abstract: Distributed machine learning systems require strong privacy guarantees, verifiable compliance, and scalable deployment across heterogeneous and multi-cloud environments. This work introduces a cloud-native privacy-preserving architecture that integrates federated learning, differential privacy, zero-knowledge compliance proofs, and adaptive governance powered by reinforcement learning. The framework supports secure model training and inference without centralizing sensitive data, while enabling cryptographically verifiable policy enforcement across institutions and cloud platforms. A full prototype deployed across hybrid Kubernetes clusters demonstrates reduced membership-inference risk, consistent enforcement of formal privacy budgets, and stable model performance under differential privacy. Experimental evaluation across multi-institution workloads shows that the architecture maintains utility with minimal overhead while providing continuous, risk-aware governance. The proposed framework establishes a practical foundation for deploying trustworthy and compliant distributed machine learning systems at scale.

</details>


### [118] [Dynamics of Agentic Loops in Large Language Models: A Geometric Theory of Trajectories](https://arxiv.org/abs/2512.10350)
*Nicolas Tacheny*

Main category: cs.LG

TL;DR: 本文引入几何框架分析大语言模型代理循环在语义嵌入空间的轨迹，通过实验识别两种基本模式，表明提示设计可控制代理循环的动态模式。


<details>
  <summary>Details</summary>
Motivation: 目前对大语言模型构建的代理循环的几何行为理解不足，需要进行研究。

Method: 引入几何框架，区分工件空间和嵌入空间，采用等渗校准消除偏差；通过对单一代理循环的实验进行分析。

Result: 识别出收缩重写循环和探索性总结否定循环两种基本模式，有不同的几何特征。

Conclusion: 提示设计可直接控制代理循环的动态模式，实现对迭代LLM转换中收敛、发散和轨迹结构的系统控制。

Abstract: Agentic systems built on large language models operate through recursive feedback loops, where each output becomes the next input. Yet the geometric behavior of these agentic loops (whether they converge, diverge, or exhibit more complex dynamics) remains poorly understood. This paper introduces a geometric framework for analyzing agentic trajectories in semantic embedding space, treating iterative transformations as discrete dynamical systems. We distinguish the artifact space, where linguistic transformations occur, from the embedding space, where geometric measurements are performed. Because cosine similarity is biased by embedding anisotropy, we introduce an isotonic calibration that eliminates systematic bias and aligns similarities with human semantic judgments while preserving high local stability. This enables rigorous measurement of trajectories, clusters and attractors. Through controlled experiments on singular agentic loops, we identify two fundamental regimes. A contractive rewriting loop converges toward a stable attractor with decreasing dispersion, while an exploratory summarize and negate loop produces unbounded divergence with no cluster formation. These regimes display qualitatively distinct geometric signatures of contraction and expansion. Our results show that prompt design directly governs the dynamical regime of an agentic loop, enabling systematic control of convergence, divergence and trajectory structure in iterative LLM transformations.

</details>


### [119] [Better Prevent than Tackle: Valuing Defense in Soccer Based on Graph Neural Networks](https://arxiv.org/abs/2512.10355)
*Hyunsung Kim,Sangwoo Seo,Hoyoung Choi,Tom Boomstra,Jinsung Yoon,Chanyoung Park*

Main category: cs.LG

TL;DR: 提出DEFCON框架量化球员防守贡献，与市场估值正相关并有实用应用。


<details>
  <summary>Details</summary>
Motivation: 现有足球防守评估方法聚焦有球动作，未充分衡量防守球员的真实影响。

Method: 提出DEFCON框架，利用图注意力网络估计进攻选项的成功概率和预期值，根据防守球员对对手预期控球值的影响分配正负积分。

Result: DEFCON的球员积分与市场估值呈强正相关。

Conclusion: DEFCON可为评估足球防守贡献提供有效方法，展示了多种应用场景。

Abstract: Evaluating defensive performance in soccer remains challenging, as effective defending is often expressed not through visible on-ball actions such as interceptions and tackles, but through preventing dangerous opportunities before they arise. Existing approaches have largely focused on valuing on-ball actions, leaving much of defenders' true impact unmeasured. To address this gap, we propose DEFCON (DEFensive CONtribution evaluator), a comprehensive framework that quantifies player-level defensive contributions for every attacking situation in soccer. Leveraging Graph Attention Networks, DEFCON estimates the success probability and expected value of each attacking option, along with each defender's responsibility for stopping it. These components yield an Expected Possession Value (EPV) for the attacking team before and after each action, and DEFCON assigns positive or negative credits to defenders according to whether they reduced or increased the opponent's EPV. Trained on 2023-24 and evaluated on 2024-25 Eredivisie event and tracking data, DEFCON's aggregated player credits exhibit strong positive correlations with market valuations. Finally, we showcase several practical applications, including in-game timelines of defensive contributions, spatial analyses across pitch zones, and pairwise summaries of attacker-defender interactions.

</details>


### [120] [GPG: Generalized Policy Gradient Theorem for Transformer-based Policies](https://arxiv.org/abs/2512.10365)
*Hangyu Mao,Guangting Dong,Zhicheng Dou*

Main category: cs.LG

TL;DR: 提出适用于Transformer策略的广义策略梯度（GPG）定理，标准策略梯度定理和GRPO是其特例，并探讨其在训练大语言模型中的应用。


<details>
  <summary>Details</summary>
Motivation: 为Transformer基策略设计合适的策略梯度定理，并探索有效策略优化方法。

Method: 提出广义策略梯度（GPG）定理。

Result: 证明标准策略梯度定理和GRPO是GPG框架的特例。

Conclusion: GPG定理可用于训练大语言模型，为策略优化提供新见解。

Abstract: We present the Generalized Policy Gradient (GPG) Theorem, specifically designed for Transformer-based policies. Notably, we demonstrate that both standard Policy Gradient Theorem and GRPO emerge as special cases within our GPG framework. Furthermore, we explore its practical applications in training Large Language Models (LLMs), offering new insights into efficient policy optimization.

</details>


### [121] [Fitting magnetization data using continued fraction of straight lines](https://arxiv.org/abs/2512.10390)
*Vijay Prakash S*

Main category: cs.LG

TL;DR: 本文用直线连分数组合近似铁磁物质磁化的非线性函数，以解释磁畴变化的非线性行为。


<details>
  <summary>Details</summary>
Motivation: 解释铁磁物质磁化随外磁场增强的非线性行为的物理基础。

Method: 将非线性函数近似为直线连分数组合，用非线性回归估计参数。

Result: 得到拟合结果用于解释磁畴增长和缩小的非线性行为。

Conclusion: 直线连分数组合可用于解释铁磁物质磁化的非线性行为。

Abstract: Magnetization of a ferromagnetic substance in response to an externally applied magnetic field increases with the strength of the field. This is because at the microscopic level, magnetic moments in certain regions or domains of the substance increasingly align with the applied field, while the amount of misaligned domains decreases. The alignment of such magnetic domains with an applied magnetic field forms the physical basis for the nonlinearity of magnetization. In this paper, the nonlinear function is approximated as a combination of continued fraction of straight lines. The resulting fit is used to interpret the nonlinear behavior in both growing and shrinking magnetic domains. The continued fraction of straight lines used here is an algebraic expression which can be used to estimate parameters using nonlinear regression.

</details>


### [122] [The Eminence in Shadow: Exploiting Feature Boundary Ambiguity for Robust Backdoor Attacks](https://arxiv.org/abs/2512.10402)
*Zhou Feng,Jiahao Chen,Chunyi Zhou,Yuwen Pu,Tianyu Du,Jinbao Li,Jianhai Chen,Shouling Ji*

Main category: cs.LG

TL;DR: 本文对深度神经网络后门攻击进行理论分析，提出Eminence框架，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击研究缺乏严格理论分析，限制对攻击机制的理解和攻击的可预测性与适应性。

Method: 对后门攻击进行理论分析，推导模糊边界区域，用影响函数分析参数变化，提出Eminence框架优化通用、视觉隐蔽的触发器。

Result: Eminence攻击成功率超90%，干净准确率损失可忽略，跨模型、数据集和场景有高可迁移性，验证了边缘中毒与对抗边界操纵的指数关系。

Conclusion: 理论分析为后门攻击提供依据，Eminence框架有效且具有鲁棒性和隐蔽性。

Abstract: Deep neural networks (DNNs) underpin critical applications yet remain vulnerable to backdoor attacks, typically reliant on heuristic brute-force methods. Despite significant empirical advancements in backdoor research, the lack of rigorous theoretical analysis limits understanding of underlying mechanisms, constraining attack predictability and adaptability. Therefore, we provide a theoretical analysis targeting backdoor attacks, focusing on how sparse decision boundaries enable disproportionate model manipulation. Based on this finding, we derive a closed-form, ambiguous boundary region, wherein negligible relabeled samples induce substantial misclassification. Influence function analysis further quantifies significant parameter shifts caused by these margin samples, with minimal impact on clean accuracy, formally grounding why such low poison rates suffice for efficacious attacks. Leveraging these insights, we propose Eminence, an explainable and robust black-box backdoor framework with provable theoretical guarantees and inherent stealth properties. Eminence optimizes a universal, visually subtle trigger that strategically exploits vulnerable decision boundaries and effectively achieves robust misclassification with exceptionally low poison rates (< 0.1%, compared to SOTA methods typically requiring > 1%). Comprehensive experiments validate our theoretical discussions and demonstrate the effectiveness of Eminence, confirming an exponential relationship between margin poisoning and adversarial boundary manipulation. Eminence maintains > 90% attack success rate, exhibits negligible clean-accuracy loss, and demonstrates high transferability across diverse models, datasets and scenarios.

</details>


### [123] [The Operator Origins of Neural Scaling Laws: A Generalized Spectral Transport Dynamics of Deep Learning](https://arxiv.org/abs/2512.10427)
*Yizhou Zhang*

Main category: cs.LG

TL;DR: 本文从梯度下降直接推导神经网络训练动力学的统一算子理论描述，得到谱输运 - 耗散PDE，证明训练保持功能正则性，给出弱耦合下自相似解及相关性质，连接不同训练情况，提供统一谱框架。


<details>
  <summary>Details</summary>
Motivation: 现代深度网络在有限正则性粗糙区域运行，Jacobian诱导算子有重尾谱和强基漂移，需对神经网络训练动力学进行统一描述。

Method: 从函数空间的精确演化出发，应用Kato扰动理论得到耦合模式常微分方程系统，粗粒化后得到谱输运 - 耗散PDE；通过理论证明得到训练性质和自相似解。

Result: 得到谱输运 - 耗散PDE；证明训练保持功能正则性，弱耦合下有自相似解；明确缩放律指数，解释双下降几何；表明NTK训练和特征学习是同一PDE的两个极限。

Conclusion: 提供了连接算子几何、优化动力学和现代深度网络通用缩放行为的统一谱框架。

Abstract: Modern deep networks operate in a rough, finite-regularity regime where Jacobian-induced operators exhibit heavy-tailed spectra and strong basis drift. In this work, we derive a unified operator-theoretoretic description of neural training dynamics directly from gradient descent. Starting from the exact evolution $\dot e_t = -M(t)e_t$ in function space, we apply Kato perturbation theory to obtain a rigorous system of coupled mode ODEs and show that, after coarse-graining, these dynamics converge to a spectral transport-dissipation PDE \[ \partial_t g + \partial_λ(v g) = -λg + S, \] where $v$ captures eigenbasis drift and $S$ encodes nonlocal spectral coupling.
  We prove that neural training preserves functional regularity, forcing the drift to take an asymptotic power-law form $v(λ,t)\sim -c(t)λ^b$. In the weak-coupling regime -- naturally induced by spectral locality and SGD noise -- the PDE admits self-similar solutions with a resolution frontier, polynomial amplitude growth, and power-law dissipation. This structure yields explicit scaling-law exponents, explains the geometry of double descent, and shows that the effective training time satisfies $τ(t)=t^αL(t)$ for slowly varying $L$.
  Finally, we show that NTK training and feature learning arise as two limits of the same PDE: $v\equiv 0$ recovers lazy dynamics, while $v\neq 0$ produces representation drift. Our results provide a unified spectral framework connecting operator geometry, optimization dynamics, and the universal scaling behavior of modern deep networks.

</details>


### [124] [Metacognitive Sensitivity for Test-Time Dynamic Model Selection](https://arxiv.org/abs/2512.10451)
*Le Tuan Minh Trinh,Le Minh Vu Pham,Thi Minh Anh Pham,An Duc Nguyen*

Main category: cs.LG

TL;DR: 本文借鉴人类认知科学，提出评估和利用AI元认知的框架，实验证明该元认知方法能提高联合推理准确率，为AI模型提供新的行为解释。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在预测时置信度校准不佳，需评估和利用AI元认知，探究模型是否真的知道自己所知道的。

Method: 引入心理层面的元认知敏感度指标meta - d'，用其动态敏感度分数作为基于多臂老虎机的仲裁器的上下文进行测试时的模型选择。

Result: 在多个数据集和多种深度学习模型组合上的实验表明，该元认知方法能提高联合推理准确率。

Conclusion: 为AI模型提供了新的行为解释，将集成选择问题转化为评估短期信号和中期特征的问题。

Abstract: A key aspect of human cognition is metacognition - the ability to assess one's own knowledge and judgment reliability. While deep learning models can express confidence in their predictions, they often suffer from poor calibration, a cognitive bias where expressed confidence does not reflect true competence. Do models truly know what they know? Drawing from human cognitive science, we propose a new framework for evaluating and leveraging AI metacognition. We introduce meta-d', a psychologically-grounded measure of metacognitive sensitivity, to characterise how reliably a model's confidence predicts its own accuracy. We then use this dynamic sensitivity score as context for a bandit-based arbiter that performs test-time model selection, learning which of several expert models to trust for a given task. Our experiments across multiple datasets and deep learning model combinations (including CNNs and VLMs) demonstrate that this metacognitive approach improves joint-inference accuracy over constituent models. This work provides a novel behavioural account of AI models, recasting ensemble selection as a problem of evaluating both short-term signals (confidence prediction scores) and medium-term traits (metacognitive sensitivity).

</details>


### [125] [T-SKM-Net: Trainable Neural Network Framework for Linear Constraint Satisfaction via Sampling Kaczmarz-Motzkin Method](https://arxiv.org/abs/2512.10461)
*Haoyu Zhu,Yao Zhang,Jiashen Ren,Qingchun Hou*

Main category: cs.LG

TL;DR: 提出T - SKM - Net框架将SKM方法集成到神经网络约束满足中，高效处理混合约束，在DCOPF case118基准上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有约束满足方法存在效率 - 适用性权衡，SKM方法因非可微操作在神经网络应用中面临挑战。

Method: 提出T - SKM - Net框架，通过零空间变换将混合约束问题转化为纯不等式问题，用SKM迭代求解并映射回原约束空间。

Result: 在DCOPF case118基准上，后处理模式下4.27ms/item GPU串行前向推理，最大最优性差距0.0025%；联合训练模式下5.25ms/item，最大最优性差距0.0008%，比pandapower求解器快超25倍且零约束违反。

Conclusion: 该框架虽有非可微操作，但支持标准反向传播，能高效解决神经网络约束满足问题。

Abstract: Neural network constraint satisfaction is crucial for safety-critical applications such as power system optimization, robotic path planning, and autonomous driving. However, existing constraint satisfaction methods face efficiency-applicability trade-offs, with hard constraint methods suffering from either high computational complexity or restrictive assumptions on constraint structures. The Sampling Kaczmarz-Motzkin (SKM) method is a randomized iterative algorithm for solving large-scale linear inequality systems with favorable convergence properties, but its argmax operations introduce non-differentiability, posing challenges for neural network applications. This work proposes the Trainable Sampling Kaczmarz-Motzkin Network (T-SKM-Net) framework and, for the first time, systematically integrates SKM-type methods into neural network constraint satisfaction. The framework transforms mixed constraint problems into pure inequality problems through null space transformation, employs SKM for iterative solving, and maps solutions back to the original constraint space, efficiently handling both equality and inequality constraints. We provide theoretical proof of post-processing effectiveness in expectation and end-to-end trainability guarantees based on unbiased gradient estimators, demonstrating that despite non-differentiable operations, the framework supports standard backpropagation. On the DCOPF case118 benchmark, our method achieves 4.27ms/item GPU serial forward inference with 0.0025% max optimality gap with post-processing mode and 5.25ms/item with 0.0008% max optimality gap with joint training mode, delivering over 25$\times$ speedup compared to the pandapower solver while maintaining zero constraint violations under given tolerance.

</details>


### [126] [UACER: An Uncertainty-Aware Critic Ensemble Framework for Robust Adversarial Reinforcement Learning](https://arxiv.org/abs/2512.10492)
*Jiaxi Wu,Tiantian Zhang,Yuxing Wang,Yongzhe Chang,Xueqian Wang*

Main category: cs.LG

TL;DR: 提出UACER方法解决鲁棒对抗强化学习训练不稳定问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒对抗强化学习中对手可训练导致学习动态非平稳，在高维复杂环境训练不稳定、收敛困难。

Method: 提出UACER方法，包含多样化评论家集成和时变衰减不确定性机制。

Result: 在多个MuJoCo控制问题实验中，UACER在整体性能、稳定性和效率上优于现有方法。

Conclusion: UACER方法有效，能解决训练不稳定和收敛困难问题。

Abstract: Robust adversarial reinforcement learning has emerged as an effective paradigm for training agents to handle uncertain disturbance in real environments, with critical applications in sequential decision-making domains such as autonomous driving and robotic control. Within this paradigm, agent training is typically formulated as a zero-sum Markov game between a protagonist and an adversary to enhance policy robustness. However, the trainable nature of the adversary inevitably induces non-stationarity in the learning dynamics, leading to exacerbated training instability and convergence difficulties, particularly in high-dimensional complex environments. In this paper, we propose a novel approach, Uncertainty-Aware Critic Ensemble for robust adversarial Reinforcement learning (UACER), which consists of two strategies: 1) Diversified critic ensemble: a diverse set of K critic networks is exploited in parallel to stabilize Q-value estimation rather than conventional single-critic architectures for both variance reduction and robustness enhancement. 2) Time-varying Decay Uncertainty (TDU) mechanism: advancing beyond simple linear combinations, we develop a variance-derived Q-value aggregation strategy that explicitly incorporates epistemic uncertainty to dynamically regulate the exploration-exploitation trade-off while simultaneously stabilizing the training process. Comprehensive experiments across several MuJoCo control problems validate the superior effectiveness of UACER, outperforming state-of-the-art methods in terms of overall performance, stability, and efficiency.

</details>


### [127] [Adaptive Replay Buffer for Offline-to-Online Reinforcement Learning](https://arxiv.org/abs/2512.10510)
*Chihyeon Song,Jaewoo Lee,Jinkyoo Park*

Main category: cs.LG

TL;DR: 提出自适应回放缓冲区（ARB）解决离线到在线强化学习（O2O RL）数据平衡难题，实验证明其能改善性能。


<details>
  <summary>Details</summary>
Motivation: 标准O2O RL方法在平衡固定离线数据集和新在线经验时存在困难，难以兼顾早期学习稳定性和渐近性能。

Method: 引入基于“on - policyness”轻量级指标动态优先采样数据的自适应回放缓冲区（ARB），它学习自由且易实现，能评估轨迹与当前策略的匹配度并分配采样权重。

Result: 在D4RL基准测试中，ARB持续减轻早期性能下降，显著提升各种O2O RL算法的最终性能。

Conclusion: 自适应、行为感知的回放缓冲区设计对O2O RL很重要。

Abstract: Offline-to-Online Reinforcement Learning (O2O RL) faces a critical dilemma in balancing the use of a fixed offline dataset with newly collected online experiences. Standard methods, often relying on a fixed data-mixing ratio, struggle to manage the trade-off between early learning stability and asymptotic performance. To overcome this, we introduce the Adaptive Replay Buffer (ARB), a novel approach that dynamically prioritizes data sampling based on a lightweight metric we call 'on-policyness'. Unlike prior methods that rely on complex learning procedures or fixed ratios, ARB is designed to be learning-free and simple to implement, seamlessly integrating into existing O2O RL algorithms. It assesses how closely collected trajectories align with the current policy's behavior and assigns a proportional sampling weight to each transition within that trajectory. This strategy effectively leverages offline data for initial stability while progressively focusing learning on the most relevant, high-rewarding online experiences. Our extensive experiments on D4RL benchmarks demonstrate that ARB consistently mitigates early performance degradation and significantly improves the final performance of various O2O RL algorithms, highlighting the importance of an adaptive, behavior-aware replay buffer design.

</details>


### [128] [Disentangled and Distilled Encoder for Out-of-Distribution Reasoning with Rademacher Guarantees](https://arxiv.org/abs/2512.10522)
*Zahra Rahiminasab,Michael Yuhas,Arvind Easwaran*

Main category: cs.LG

TL;DR: 本文提出解纠缠蒸馏编码器（DDE）框架，在保留解纠缠的同时减小OOD推理器大小，以部署在资源受限设备上，并进行了理论分析和实证评估。


<details>
  <summary>Details</summary>
Motivation: 利用变分自编码器（VAE）的解纠缠潜在空间对多标签分布外（OOD）测试样本进行推理，需减小OOD推理器大小以部署在资源受限设备上。

Method: 提出DDE框架，将学生 - 教师蒸馏用于模型压缩，形式化为带解纠缠约束的优化问题，并基于拉德马赫复杂度建立蒸馏过程解纠缠的理论保证。

Result: 通过在NVIDIA上部署压缩模型进行了实证评估。

Conclusion: 文中未明确提及，但可推测该框架能在保留解纠缠的情况下减小模型大小，适用于资源受限设备。

Abstract: Recently, the disentangled latent space of a variational autoencoder (VAE) has been used to reason about multi-label out-of-distribution (OOD) test samples that are derived from different distributions than training samples. Disentangled latent space means having one-to-many maps between latent dimensions and generative factors or important characteristics of an image. This paper proposes a disentangled distilled encoder (DDE) framework to decrease the OOD reasoner size for deployment on resource-constrained devices while preserving disentanglement. DDE formalizes student-teacher distillation for model compression as a constrained optimization problem while preserving disentanglement with disentanglement constraints. Theoretical guarantees for disentanglement during distillation based on Rademacher complexity are established. The approach is evaluated empirically by deploying the compressed model on an NVIDIA

</details>


### [129] [Mode-Seeking for Inverse Problems with Diffusion Models](https://arxiv.org/abs/2512.10524)
*Sai Bharath Chandra Gutha,Ricardo Vinuesa,Hossein Azizpour*

Main category: cs.LG

TL;DR: 提出变分模式寻找损失（VML）及VML - MAP算法解决逆问题，在性能和计算时间上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有后验采样和MAP估计方法依赖建模近似且计算量大，需改进。

Method: 提出VML，在反向扩散步骤中最小化该损失以引导样本向MAP估计；基于理论见解提出VML - MAP算法。

Result: 通过多数据集上的图像恢复任务实验，验证了VML - MAP算法在性能和计算时间上优于现有方法。

Conclusion: VML和VML - MAP算法是解决逆问题的有效方案。

Abstract: A pre-trained unconditional diffusion model, combined with posterior sampling or maximum a posteriori (MAP) estimation techniques, can solve arbitrary inverse problems without task-specific training or fine-tuning. However, existing posterior sampling and MAP estimation methods often rely on modeling approximations and can be computationally demanding. In this work, we propose the variational mode-seeking loss (VML), which, when minimized during each reverse diffusion step, guides the generated sample towards the MAP estimate. VML arises from a novel perspective of minimizing the Kullback-Leibler (KL) divergence between the diffusion posterior $p(\mathbf{x}_0|\mathbf{x}_t)$ and the measurement posterior $p(\mathbf{x}_0|\mathbf{y})$, where $\mathbf{y}$ denotes the measurement. Importantly, for linear inverse problems, VML can be analytically derived and need not be approximated. Based on further theoretical insights, we propose VML-MAP, an empirically effective algorithm for solving inverse problems, and validate its efficacy over existing methods in both performance and computational time, through extensive experiments on diverse image-restoration tasks across multiple datasets.

</details>


### [130] [Unlocking the Address Book: Dissecting the Sparse Semantic Structure of LLM Key-Value Caches via Sparse Autoencoders](https://arxiv.org/abs/2512.10547)
*Qingsen Ma,Dianyun Wang,Jiaming Lyu,Yaoye Wang,Lechen Ning,Sujie Zhu,Zhenbo Xu,Liuyu Xiang,Huining Li,Huijia Wu,Zhaofeng He*

Main category: cs.LG

TL;DR: 提出STA - Attention框架，用Top - K稀疏自编码器分解KV缓存为语义原子，引入双预算策略，实验表明语义重建能保持模型性能。


<details>
  <summary>Details</summary>
Motivation: KV缓存是长上下文大语言模型的主要内存瓶颈，且通常被视为不透明数值张量，需进行可解释分解。

Method: 提出STA - Attention框架，利用Top - K稀疏自编码器分解KV缓存，引入双预算策略过滤噪声。

Result: 在Yi - 6B、Mistral - 7B、Qwen2.5 - 32B等模型上实验，语义重建保持了与原模型相当的困惑度和零样本性能。

Conclusion: 能有效弥合机械可解释性和忠实注意力建模之间的差距。

Abstract: The Key-Value (KV) cache is the primary memory bottleneck in long-context Large Language Models, yet it is typically treated as an opaque numerical tensor. In this work, we propose \textbf{STA-Attention}, a framework that utilizes Top-K Sparse Autoencoders (SAEs) to decompose the KV cache into interpretable ``semantic atoms.'' Unlike standard $L_1$-regularized SAEs, our Top-K approach eliminates shrinkage bias, preserving the precise dot-product geometry required for attention. Our analysis uncovers a fundamental \textbf{Key-Value Asymmetry}: while Key vectors serve as highly sparse routers dominated by a ``Semantic Elbow,'' deep Value vectors carry dense content payloads requiring a larger budget. Based on this structure, we introduce a Dual-Budget Strategy that selectively preserves the most informative semantic components while filtering representational noise. Experiments on Yi-6B, Mistral-7B, Qwen2.5-32B, and others show that our semantic reconstructions maintain perplexity and zero-shot performance comparable to the original models, effectively bridging the gap between mechanistic interpretability and faithful attention modeling.

</details>


### [131] [Is the Information Bottleneck Robust Enough? Towards Label-Noise Resistant Information Bottleneck Learning](https://arxiv.org/abs/2512.10573)
*Yi Huang,Qingyun Sun,Yisen Gao,Haonan Yuan,Xingcheng Fu,Jianxin Li*

Main category: cs.LG

TL;DR: 提出LaT - IB方法解决信息瓶颈（IB）原理在标签噪声下的性能问题，通过引入MSC准则、噪声感知潜在解缠和三阶段训练框架，实验证明其在标签噪声下有更好的鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统信息瓶颈原理依赖准确标签，在现实标签噪声场景下性能下降和过拟合，需要解决该问题。

Method: 提出LaT - IB方法，引入“Minimal - Sufficient - Clean”（MSC）准则作为互信息正则化器，采用噪声感知潜在解缠将潜在表示分解，推导目标各部分互信息边界，设计三阶段训练框架。

Result: 实验表明LaT - IB在标签噪声下实现了卓越的鲁棒性和效率。

Conclusion: LaT - IB显著增强了在有标签噪声的现实场景中的鲁棒性和适用性。

Abstract: The Information Bottleneck (IB) principle facilitates effective representation learning by preserving label-relevant information while compressing irrelevant information. However, its strong reliance on accurate labels makes it inherently vulnerable to label noise, prevalent in real-world scenarios, resulting in significant performance degradation and overfitting. To address this issue, we propose LaT-IB, a novel Label-Noise ResistanT Information Bottleneck method which introduces a "Minimal-Sufficient-Clean" (MSC) criterion. Instantiated as a mutual information regularizer to retain task-relevant information while discarding noise, MSC addresses standard IB's vulnerability to noisy label supervision. To achieve this, LaT-IB employs a noise-aware latent disentanglement that decomposes the latent representation into components aligned with to the clean label space and the noise space. Theoretically, we first derive mutual information bounds for each component of our objective including prediction, compression, and disentanglement, and moreover prove that optimizing it encourages representations invariant to input noise and separates clean and noisy label information. Furthermore, we design a three-phase training framework: Warmup, Knowledge Injection and Robust Training, to progressively guide the model toward noise-resistant representations. Extensive experiments demonstrate that LaT-IB achieves superior robustness and efficiency under label noise, significantly enhancing robustness and applicability in real-world scenarios with label noise.

</details>


### [132] [THeGAU: Type-Aware Heterogeneous Graph Autoencoder and Augmentation](https://arxiv.org/abs/2512.10589)
*Ming-Yi Hong,Miao-Chen Chiang,Youchen Teng,Yu-Hsiang Wang,Chih-Yu Wang,Che Lin*

Main category: cs.LG

TL;DR: 提出THeGAU框架结合类型感知图自编码器与引导图增强改进异构图节点分类，在多数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有HGNNs存在类型信息丢失和结构噪声问题，限制了表示保真度和泛化能力。

Method: 提出THeGAU框架，将重建模式有效边作为辅助任务保留节点类型语义，引入解码器驱动的增强机制选择性优化噪声结构。

Result: 在IMDB、ACM和DBLP三个基准HIN数据集上的大量实验表明，THeGAU始终优于现有HGNN方法，在多个骨干网络上达到了最先进的性能。

Conclusion: THeGAU框架可增强鲁棒性、准确性和效率，同时显著降低计算开销。

Abstract: Heterogeneous Graph Neural Networks (HGNNs) are effective for modeling Heterogeneous Information Networks (HINs), which encode complex multi-typed entities and relations. However, HGNNs often suffer from type information loss and structural noise, limiting their representational fidelity and generalization. We propose THeGAU, a model-agnostic framework that combines a type-aware graph autoencoder with guided graph augmentation to improve node classification. THeGAU reconstructs schema-valid edges as an auxiliary task to preserve node-type semantics and introduces a decoder-driven augmentation mechanism to selectively refine noisy structures. This joint design enhances robustness, accuracy, and efficiency while significantly reducing computational overhead. Extensive experiments on three benchmark HIN datasets (IMDB, ACM, and DBLP) demonstrate that THeGAU consistently outperforms existing HGNN methods, achieving state-of-the-art performance across multiple backbones.

</details>


### [133] [Multi-Objective Reward and Preference Optimization: Theory and Algorithms](https://arxiv.org/abs/2512.10601)
*Akhil Agnihotri*

Main category: cs.LG

TL;DR: 论文提出理论框架和算法推进约束强化学习在控制、偏好学习和大语言模型对齐方面的发展，统一不同范式的约束强化学习。


<details>
  <summary>Details</summary>
Motivation: 推动约束强化学习在控制、偏好学习和大语言模型对齐上的研究，实现安全和对齐的决策。

Method: 提出ACPO算法处理平均成本准则下的约束马尔可夫决策过程；用e - COP将约束强化学习扩展到有限时域；提出warmPref - PS和PSPL用于基于偏好的强化学习；提出MOPO用于大规模模型对齐。

Result: ACPO有理论保证且实现了最优经验性能；e - COP在安全关键环境有可证明性能、简单性和可扩展性；warmPref - PS减少遗憾并提高数据收集效率；PSPL有贝叶斯简单遗憾保证；MOPO可扩展到数十亿参数语言模型。

Conclusion: 论文统一了不同范式的约束强化学习，提供了安全和对齐决策的理论进展和实用工具。

Abstract: This thesis develops theoretical frameworks and algorithms that advance constrained reinforcement learning (RL) across control, preference learning, and alignment of large language models. The first contribution addresses constrained Markov Decision Processes (CMDPs) under the average-cost criterion through the Average-Constrained Policy Optimization (ACPO) algorithm. ACPO integrates sensitivity analysis with trust-region updates to ensure stable constraint handling, achieving state-of-the-art empirical performance with theoretical guarantees. Constrained RL is then extended to finite-horizon settings via e-COP, the first policy optimization method for episodic CMDPs. Built on an episodic policy difference lemma, e-COP offers provable performance, simplicity, and scalability in safety-critical environments. The thesis then investigates reinforcement learning from human preferences. warmPref-PS introduces a posterior sampling strategy for linear bandits that integrates offline preference data from heterogeneous raters into online learning. Explicit modeling of rater competence yields substantial regret reduction and more efficient data collection for RLHF. The PSPL algorithm further advances preference-based RL by jointly sampling reward models and transition dynamics from pairwise trajectory comparisons, providing Bayesian simple-regret guarantees and robust empirical identification of optimal policies. The final contribution applies these methods to large-scale model alignment. A multi-objective constrained optimization view yields MOPO, an iterative algorithm with closed-form updates that scales to multi-billion-parameter language models and remains robust across alignment settings. Collectively, the thesis unifies constrained RL across average-cost, episodic, and preference-driven paradigms, delivering theoretical advances and practical tools for safe and aligned decision-making.

</details>


### [134] [Uncertainty-Preserving QBNNs: Multi-Level Quantization of SVI-Based Bayesian Neural Networks for Image Classification](https://arxiv.org/abs/2512.10602)
*Hendrik Borras,Yong Wu,Bernhard Klein,Holger Fröning*

Main category: cs.LG

TL;DR: 提出基于随机变分推理的贝叶斯神经网络多级量化框架，可将BNNs量化至4位精度，实现内存减少且能保持性能，让其能在资源受限设备上部署。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯神经网络存在计算和内存开销大问题，量化技术在概率模型应用待探索，需解决资源需求问题。

Method: 引入多级别量化框架，区分VPQ、SPQ、JQ三种策略，用对数量化方差参数和特殊激活函数。

Result: 在Dirty - MNIST上实验表明，可将BNNs量化到4位精度，Joint Quantization在4位时相比浮点实现内存最多减少8倍，精度和不确定性估计降级小。

Conclusion: 能在资源受限边缘设备部署BNNs，并为未来低精度模拟“贝叶斯机器”提供设计准则。

Abstract: Bayesian Neural Networks (BNNs) provide principled uncertainty quantification but suffer from substantial computational and memory overhead compared to deterministic networks. While quantization techniques have successfully reduced resource requirements in standard deep learning models, their application to probabilistic models remains largely unexplored. We introduce a systematic multi-level quantization framework for Stochastic Variational Inference based BNNs that distinguishes between three quantization strategies: Variational Parameter Quantization (VPQ), Sampled Parameter Quantization (SPQ), and Joint Quantization (JQ). Our logarithmic quantization for variance parameters, and specialized activation functions to preserve the distributional structure are essential for calibrated uncertainty estimation. Through comprehensive experiments on Dirty-MNIST, we demonstrate that BNNs can be quantized down to 4-bit precision while maintaining both classification accuracy and uncertainty disentanglement. At 4 bits, Joint Quantization achieves up to 8x memory reduction compared to floating-point implementations with minimal degradation in epistemic and aleatoric uncertainty estimation. These results enable deployment of BNNs on resource-constrained edge devices and provide design guidelines for future analog "Bayesian Machines" operating at inherently low precision.

</details>


### [135] [Supporting Migration Policies with Forecasts: Illegal Border Crossings in Europe through a Mixed Approach](https://arxiv.org/abs/2512.10633)
*C. Bosco,U. Minora,D. de Rigo,J. Pingsdorf,R. Cortinovis*

Main category: cs.LG

TL;DR: 本文提出混合方法预测欧洲五条关键移民路线非法越境情况，结合机器学习与专家见解，经测试验证其在移民政策中的适用性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 应对欧盟移民和庇护协定中的预测需求，解决移民模式突然变化及传统数据集的局限性，为战略决策等提供信息。

Method: 将机器学习技术与移民专家的定性见解相结合，纳入人为评估协变量。

Result: 该方法经过已知数据的测试和验证。

Conclusion: 该方法具有创新性，可作为适用于欧盟移民治理的运营工具，能满足政策相关预测需求。

Abstract: This paper presents a mixed-methodology to forecast illegal border crossings in Europe across five key migratory routes, with a one-year time horizon. The methodology integrates machine learning techniques with qualitative insights from migration experts. This approach aims at improving the predictive capacity of data-driven models through the inclusion of a human-assessed covariate, an innovation that addresses challenges posed by sudden shifts in migration patterns and limitations in traditional datasets. The proposed methodology responds directly to the forecasting needs outlined in the EU Pact on Migration and Asylum, supporting the Asylum and Migration Management Regulation (AMMR). It is designed to provide policy-relevant forecasts that inform strategic decisions, early warning systems, and solidarity mechanisms among EU Member States. By joining data-driven modeling with expert judgment, this work aligns with existing academic recommendations and introduces a novel operational tool tailored for EU migration governance. The methodology is tested and validated with known data to demonstrate its applicability and reliability in migration-related policy context.

</details>


### [136] [Token Sample Complexity of Attention](https://arxiv.org/abs/2512.10656)
*Léa Bohbot,Cyril Letrouit,Gabriel Peyré,François-Xavier Vialard*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: As context windows in large language models continue to expand, it is essential to characterize how attention behaves at extreme sequence lengths. We introduce token-sample complexity: the rate at which attention computed on $n$ tokens converges to its infinite-token limit. We estimate finite-$n$ convergence bounds at two levels: pointwise uniform convergence of the attention map, and convergence of moments for the transformed token distribution. For compactly supported (and more generally sub-Gaussian) distributions, our first result shows that the attention map converges uniformly on a ball of radius $R$ at rate $C(R)/\sqrt{n}$, where $C(R)$ grows exponentially with $R$. For large $R$, this estimate loses practical value, and our second result addresses this issue by establishing convergence rates for the moments of the transformed distribution (the token output of the attention layer). In this case, the rate is $C'(R)/n^β$ with $β<\tfrac{1}{2}$, and $C'(R)$ depends polynomially on the size of the support of the distribution. The exponent $β$ depends on the attention geometry and the spectral properties of the tokens distribution. We also examine the regime in which the attention parameter tends to infinity and the softmax approaches a hardmax, and in this setting, we establish a logarithmic rate of convergence. Experiments on synthetic Gaussian data and real BERT models on Wikipedia text confirm our predictions.

</details>


### [137] [DCFO Additional Material](https://arxiv.org/abs/2512.10659)
*Tommaso Amico,Pernille Matthews,Lena Krieger,Arthur Zimek,Ira Assent*

Main category: cs.LG

TL;DR: 提出DCFO方法为LOF生成反事实解释，在50个OpenML数据集实验中表现优于竞品。


<details>
  <summary>Details</summary>
Motivation: 现有反事实解释方法忽视离群点检测挑战且未针对经典算法，LOF缺乏可解释性。

Method: 引入DCFO，将数据空间划分为LOF平滑区域，进行基于梯度的优化。

Result: 在50个OpenML数据集上实验，DCFO生成的反事实解释在接近性和有效性上优于竞品。

Conclusion: DCFO能有效为LOF提供反事实解释，有较好效果。

Abstract: Outlier detection identifies data points that significantly deviate from the majority of the data distribution. Explaining outliers is crucial for understanding the underlying factors that contribute to their detection, validating their significance, and identifying potential biases or errors. Effective explanations provide actionable insights, facilitating preventive measures to avoid similar outliers in the future. Counterfactual explanations clarify why specific data points are classified as outliers by identifying minimal changes required to alter their prediction. Although valuable, most existing counterfactual explanation methods overlook the unique challenges posed by outlier detection, and fail to target classical, widely adopted outlier detection algorithms. Local Outlier Factor (LOF) is one the most popular unsupervised outlier detection methods, quantifying outlierness through relative local density. Despite LOF's widespread use across diverse applications, it lacks interpretability. To address this limitation, we introduce Density-based Counterfactuals for Outliers (DCFO), a novel method specifically designed to generate counterfactual explanations for LOF. DCFO partitions the data space into regions where LOF behaves smoothly, enabling efficient gradient-based optimisation. Extensive experimental validation on 50 OpenML datasets demonstrates that DCFO consistently outperforms benchmarked competitors, offering superior proximity and validity of generated counterfactuals.

</details>


### [138] [Learning by Analogy: A Causal Framework for Composition Generalization](https://arxiv.org/abs/2512.10669)
*Lingjing Kong,Shaoan Xie,Yang Jiao,Yetian Chen,Yanhui Guo,Simone Shao,Yan Gao,Guangyi Chen,Kun Zhang*

Main category: cs.LG

TL;DR: 本文探讨组合泛化能力，提出用因果模块化和最小变化原则形式化相关过程，引入分层数据生成过程，理论证明其优势和可恢复性，实验在基准数据集取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 目前对实现组合泛化能力的数据结构和原理理解不足，需深入研究。

Method: 用因果模块化和最小变化原则形式化过程，引入分层数据生成过程。

Result: 理论上证明方法能支持复杂关系的组合泛化，且潜在分层结构可从可观测数据恢复；实验在基准数据集取得显著改进。

Conclusion: 提出的方法能有效实现组合泛化，推动该领域研究。

Abstract: Compositional generalization -- the ability to understand and generate novel combinations of learned concepts -- enables models to extend their capabilities beyond limited experiences. While effective, the data structures and principles that enable this crucial capability remain poorly understood. We propose that compositional generalization fundamentally requires decomposing high-level concepts into basic, low-level concepts that can be recombined across similar contexts, similar to how humans draw analogies between concepts. For example, someone who has never seen a peacock eating rice can envision this scene by relating it to their previous observations of a chicken eating rice.
  In this work, we formalize these intuitive processes using principles of causal modularity and minimal changes. We introduce a hierarchical data-generating process that naturally encodes different levels of concepts and their interaction mechanisms. Theoretically, we demonstrate that this approach enables compositional generalization supporting complex relations between composed concepts, advancing beyond prior work that assumes simpler interactions like additive effects. Critically, we also prove that this latent hierarchical structure is provably recoverable (identifiable) from observable data like text-image pairs, a necessary step for learning such a generative process. To validate our theory, we apply insights from our theoretical framework and achieve significant improvements on benchmark datasets.

</details>


### [139] [HybridVFL: Disentangled Feature Learning for Edge-Enabled Vertical Federated Multimodal Classification](https://arxiv.org/abs/2512.10701)
*Mostafa Anoosha,Zeinab Dehghani,Kuniko Paxton,Koorosh Aslansefat,Dhavalkumar Thakker*

Main category: cs.LG

TL;DR: 本文提出HybridVFL框架，通过客户端特征解缠和服务器端跨模态变压器进行上下文感知融合，在HAM10000数据集上表现优于标准联邦基线。


<details>
  <summary>Details</summary>
Motivation: 标准垂直联邦学习（VFL）系统因简单特征融合存在性能限制，需要更好的解决方案用于边缘AI场景。

Method: 引入HybridVFL框架，采用客户端特征解缠和服务器端跨模态变压器进行上下文感知融合。

Result: 在多模态HAM10000皮肤病变数据集上，HybridVFL显著优于标准联邦基线。

Conclusion: 先进的融合机制对构建强大的隐私保护系统至关重要。

Abstract: Vertical Federated Learning (VFL) offers a privacy-preserving paradigm for Edge AI scenarios like mobile health diagnostics, where sensitive multimodal data reside on distributed, resource-constrained devices. Yet, standard VFL systems often suffer performance limitations due to simplistic feature fusion. This paper introduces HybridVFL, a novel framework designed to overcome this bottleneck by employing client-side feature disentanglement paired with a server-side cross-modal transformer for context-aware fusion. Through systematic evaluation on the multimodal HAM10000 skin lesion dataset, we demonstrate that HybridVFL significantly outperforms standard federated baselines, validating the criticality of advanced fusion mechanisms in robust, privacy-preserving systems.

</details>


### [140] [Beyond the Black Box: Identifiable Interpretation and Control in Generative Models via Causal Minimality](https://arxiv.org/abs/2512.10720)
*Lingjing Kong,Shaoan Xie,Guangyi Chen,Yuewen Sun,Xiangchen Song,Eric P. Xing,Kun Zhang*

Main category: cs.LG

TL;DR: 本文旨在为可解释生成模型建立原则性基础，利用因果最小性原则和新理论框架，使生成模型的潜在表征有因果解释和可识别控制，还能提取概念图并用于模型微调。


<details>
  <summary>Details</summary>
Motivation: 现有深度生成模型多为黑盒，缺乏理论保证，阻碍人类理解、控制和校准，因此要建立可解释生成模型的原则性基础。

Method: 运用因果最小性原则，引入分层选择模型的理论框架，在理论推导的最小性条件下进行研究。

Result: 在领先的生成模型上应用约束条件，可提取其内在的分层概念图，获得对其内部知识组织的新见解，且因果概念可用于模型的细粒度控制。

Conclusion: 基于因果最小性原则和分层选择模型理论框架，能让生成模型具有清晰因果解释、可识别控制，实现模型的透明和可靠。

Abstract: Deep generative models, while revolutionizing fields like image and text generation, largely operate as opaque black boxes, hindering human understanding, control, and alignment. While methods like sparse autoencoders (SAEs) show remarkable empirical success, they often lack theoretical guarantees, risking subjective insights. Our primary objective is to establish a principled foundation for interpretable generative models. We demonstrate that the principle of causal minimality -- favoring the simplest causal explanation -- can endow the latent representations of diffusion vision and autoregressive language models with clear causal interpretation and robust, component-wise identifiable control. We introduce a novel theoretical framework for hierarchical selection models, where higher-level concepts emerge from the constrained composition of lower-level variables, better capturing the complex dependencies in data generation. Under theoretically derived minimality conditions (manifesting as sparsity or compression constraints), we show that learned representations can be equivalent to the true latent variables of the data-generating process. Empirically, applying these constraints to leading generative models allows us to extract their innate hierarchical concept graphs, offering fresh insights into their internal knowledge organization. Furthermore, these causally grounded concepts serve as levers for fine-grained model steering, paving the way for transparent, reliable systems.

</details>


### [141] [Generalized Spherical Neural Operators: Green's Function Formulation](https://arxiv.org/abs/2512.10723)
*Hao Tang,Hao Chen,Chao Li*

Main category: cs.LG

TL;DR: 提出基于可设计球面格林函数及其谐波展开的算子设计框架，构建GSNO算子和GSHNet架构，在多任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有球面算子在解决参数偏微分方程时难以兼顾几何特性与旋转一致性，且缺乏应对现实复杂性的灵活性。

Method: 提出基于可设计球面格林函数及其谐波展开的算子设计框架，设计绝对和相对位置相关的格林函数，构建GSNO算子和GSHNet架构。

Result: 在扩散MRI、浅水动力学和全球天气预报任务中，GSNO和GSHNet始终优于现有方法。

Conclusion: GSNO是球面算子学习的原则性通用框架，弥合了严格理论与现实复杂性之间的差距。

Abstract: Neural operators offer powerful approaches for solving parametric partial differential equations, but extending them to spherical domains remains challenging due to the need to preserve intrinsic geometry while avoiding distortions that break rotational consistency. Existing spherical operators rely on rotational equivariance but often lack the flexibility for real-world complexity. We propose a general operator-design framework based on the designable spherical Green's function and its harmonic expansion, establishing a solid operator-theoretic foundation for spherical learning. Based on this, we propose an absolute and relative position-dependent Green's function that enables flexible balance of equivariance and invariance for real-world modeling. The resulting operator, Green's-function Spherical Neural Operator (GSNO) with a novel spectral learning method, can adapt to anisotropic, constraint-rich systems while retaining spectral efficiency. To exploit GSNO, we develop GSHNet, a hierarchical architecture that combines multi-scale spectral modeling with spherical up-down sampling, enhancing global feature representation. Evaluations on diffusion MRI, shallow water dynamics, and global weather forecasting, GSNO and GSHNet consistently outperform state-of-the-art methods. Our results position GSNO as a principled and general framework for spherical operator learning, bridging rigorous theory with real-world complexity.

</details>


### [142] [LGAN: An Efficient High-Order Graph Neural Network via the Line Graph Aggregation](https://arxiv.org/abs/2512.10735)
*Lin Du,Lu Bai,Jincheng Li,Lixin Cui,Hangyuan Du,Lichi Zhang,Yuting Chen,Zhao Li*

Main category: cs.LG

TL;DR: 提出Line Graph Aggregation Network (LGAN)用于图分类，有更高表达能力、更低复杂度和更好可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有GNN表达能力受1 - WL测试限制，k - WL计算成本高且可解释性差。

Method: 提出LGAN，从以每个节点为中心的诱导子图构建线图进行高阶聚合。

Result: 理论证明LGAN在单射聚合假设下表达能力强于2 - WL，且时间复杂度低；实验表明LGAN性能优于现有k - WL基GNN。

Conclusion: LGAN能克服现有GNN缺点，有更好性能和可解释性。

Abstract: Graph Neural Networks (GNNs) have emerged as a dominant paradigm for graph classification. Specifically, most existing GNNs mainly rely on the message passing strategy between neighbor nodes, where the expressivity is limited by the 1-dimensional Weisfeiler-Lehman (1-WL) test. Although a number of k-WL-based GNNs have been proposed to overcome this limitation, their computational cost increases rapidly with k, significantly restricting the practical applicability. Moreover, since the k-WL models mainly operate on node tuples, these k-WL-based GNNs cannot retain fine-grained node- or edge-level semantics required by attribution methods (e.g., Integrated Gradients), leading to the less interpretable problem. To overcome the above shortcomings, in this paper, we propose a novel Line Graph Aggregation Network (LGAN), that constructs a line graph from the induced subgraph centered at each node to perform the higher-order aggregation. We theoretically prove that the LGAN not only possesses the greater expressive power than the 2-WL under injective aggregation assumptions, but also has lower time complexity. Empirical evaluations on benchmarks demonstrate that the LGAN outperforms state-of-the-art k-WL-based GNNs, while offering better interpretability.

</details>


### [143] [UniExtreme: A Universal Foundation Model for Extreme Weather Forecasting](https://arxiv.org/abs/2508.01426)
*Hang Ni,Weijia Zhang,Hao Liu*

Main category: cs.LG

TL;DR: 现有深度学习基础模型预测极端天气能力有限，本文提出通用极端天气预测基础模型UniExtreme，实验表明其性能优于现有基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有天气预测模型预测极端天气能力有限，且未考虑真实世界多样化极端事件的大气模式。

Method: 提出UniExtreme，包含捕捉正常和极端天气区域光谱差异的自适应频率调制（AFM）模块和结合特定区域极端事件先验的事件先验增强（EPA）模块。

Result: 大量实验表明UniExtreme在极端和一般天气预测方面均优于现有基线模型。

Conclusion: UniExtreme具有跨多种极端场景的卓越适应性。

Abstract: Recent advancements in deep learning have led to the development of Foundation Models (FMs) for weather forecasting, yet their ability to predict extreme weather events remains limited. Existing approaches either focus on general weather conditions or specialize in specific-type extremes, neglecting the real-world atmospheric patterns of diversified extreme events. In this work, we identify two key characteristics of extreme events: (1) the spectral disparity against normal weather regimes, and (2) the hierarchical drivers and geographic blending of diverse extremes. Along this line, we propose UniExtreme, a universal extreme weather forecasting foundation model that integrates (1) an Adaptive Frequency Modulation (AFM) module that captures region-wise spectral differences between normal and extreme weather, through learnable Beta-distribution filters and multi-granularity spectral aggregation, and (2) an Event Prior Augmentation (EPA) module which incorporates region-specific extreme event priors to resolve hierarchical extreme diversity and composite extreme schema, via a dual-level memory fusion network. Extensive experiments demonstrate that UniExtreme outperforms state-of-the-art baselines in both extreme and general weather forecasting, showcasing superior adaptability across diverse extreme scenarios.

</details>


### [144] [Template-Free Retrosynthesis with Graph-Prior Augmented Transformers](https://arxiv.org/abs/2512.10770)
*Youjun Zhao*

Main category: cs.LG

TL;DR: 本文提出无模板、基于Transformer的逆合成反应预测框架，在USPTO - 50K基准上取得无模板方法的最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有逆合成反应预测模型在准确性和鲁棒性上不能满足实际部署要求。

Method: 采用无模板、基于Transformer的框架，将分子图信息注入注意力机制，结合SMILES序列和结构线索，并使用配对数据增强策略。

Result: 在USPTO - 50K基准上，该方法达到无模板方法的最优性能，大幅超越普通Transformer基线。

Conclusion: 所提出的无模板、基于Transformer的框架在逆合成反应预测中有效。

Abstract: Retrosynthesis reaction prediction seeks to infer plausible reactant molecules for a given product and is a central problem in computer-aided organic synthesis. Despite recent progress, many existing models still fall short of the accuracy and robustness required for practical deployment. This work studies a template-free, Transformer-based framework that eliminates reliance on handcrafted reaction templates or additional chemical rule engines. The model injects molecular graph information into the attention mechanism to jointly exploit \SMILES\ sequences and structural cues, and further applies a paired data augmentation strategy to enhance training diversity and scale. On the USPTO-50K benchmark, our proposed approach achieves state-of-the-art performance among template-free methods and substantially outperforming a vanilla Transformer baseline.

</details>


### [145] [Interpretable and Steerable Concept Bottleneck Sparse Autoencoders](https://arxiv.org/abs/2512.10805)
*Akshay Kulkarni,Tsui-Wei Weng,Vivek Narayanaswamy,Shusen Liu,Wesam A. Sakla,Kowshik Thopalli*

Main category: cs.LG

TL;DR: 本文针对稀疏自动编码器在大语言和视觉语言模型中的问题，提出Concept Bottleneck Sparse Autoencoders，提升了解释性和可操控性。


<details>
  <summary>Details</summary>
Motivation: 稀疏自动编码器的潜在功能需所学特征具备可解释性和可操控性，但现状存在不足，限制其实用性。

Method: 引入新的可解释性和可操控性度量指标进行系统分析，提出Concept Bottleneck Sparse Autoencoders这一事后框架对结果进行改进。

Result: CB - SAE在LVLMs和图像生成任务中使可解释性提升32.1%，可操控性提升14.5%。

Conclusion: CB - SAE能有效改善稀疏自动编码器的可解释性和可操控性，且代码和模型权重将公开。

Abstract: Sparse autoencoders (SAEs) promise a unified approach for mechanistic interpretability, concept discovery, and model steering in LLMs and LVLMs. However, realizing this potential requires that the learned features be both interpretable and steerable. To that end, we introduce two new computationally inexpensive interpretability and steerability metrics and conduct a systematic analysis on LVLMs. Our analysis uncovers two observations; (i) a majority of SAE neurons exhibit either low interpretability or low steerability or both, rendering them ineffective for downstream use; and (ii) due to the unsupervised nature of SAEs, user-desired concepts are often absent in the learned dictionary, thus limiting their practical utility. To address these limitations, we propose Concept Bottleneck Sparse Autoencoders (CB-SAE) - a novel post-hoc framework that prunes low-utility neurons and augments the latent space with a lightweight concept bottleneck aligned to a user-defined concept set. The resulting CB-SAE improves interpretability by +32.1% and steerability by +14.5% across LVLMs and image generation tasks. We will make our code and model weights available.

</details>


### [146] [Learning Controllable and Diverse Player Behaviors in Multi-Agent Environments](https://arxiv.org/abs/2512.10835)
*Atahan Cilan,Atay Özgövde*

Main category: cs.LG

TL;DR: 提出一种免人类游戏数据的强化学习框架，实现可控多样玩家行为，单策略可复现新风格，实验效果好，可用于多场景。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大量玩家轨迹、分类型训练模型或缺乏参数与策略映射，限制了可扩展性与可控性。

Method: 在N维连续空间定义玩家行为，均匀采样目标行为向量，训练时让智能体接收当前和目标向量，以两者距离归一化减少量作为奖励。

Result: 在自定义多人游戏实验中，该框架比仅以获胜为目标的基线产生更多样的行为，能可靠匹配不同目标行为向量。

Conclusion: 该方法为游戏自动化测试、平衡、类人行为模拟和替换断线玩家等提供可扩展解决方案。

Abstract: This paper introduces a reinforcement learning framework that enables controllable and diverse player behaviors without relying on human gameplay data. Existing approaches often require large-scale player trajectories, train separate models for different player types, or provide no direct mapping between interpretable behavioral parameters and the learned policy, limiting their scalability and controllability. We define player behavior in an N-dimensional continuous space and uniformly sample target behavior vectors from a region that encompasses the subset representing real human styles. During training, each agent receives both its current and target behavior vectors as input, and the reward is based on the normalized reduction in distance between them. This allows the policy to learn how actions influence behavioral statistics, enabling smooth control over attributes such as aggressiveness, mobility, and cooperativeness. A single PPO-based multi-agent policy can reproduce new or unseen play styles without retraining. Experiments conducted in a custom multi-player Unity game show that the proposed framework produces significantly greater behavioral diversity than a win-only baseline and reliably matches specified behavior vectors across diverse targets. The method offers a scalable solution for automated playtesting, game balancing, human-like behavior simulation, and replacing disconnected players in online games.

</details>


### [147] [Bayesian Symbolic Regression via Posterior Sampling](https://arxiv.org/abs/2512.10849)
*Geoffrey F. Bomarito,Patrick E. Leser*

Main category: cs.LG

TL;DR: 本文提出用于贝叶斯符号回归的顺序蒙特卡罗（SMC）框架，提升噪声环境下符号回归的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 符号回归对噪声敏感阻碍其广泛应用，需提升其在噪声环境下的鲁棒性。

Method: 引入SMC框架，结合概率选择、自适应回火和归一化边际似然来探索符号表达式搜索空间。

Result: 与传统遗传编程基线相比，该方法能更好处理含噪声的基准数据集，减少过拟合。

Conclusion: 该方法为科学发现和工程设计应用中的符号回归提供更稳健方案。

Abstract: Symbolic regression is a powerful tool for discovering governing equations directly from data, but its sensitivity to noise hinders its broader application. This paper introduces a Sequential Monte Carlo (SMC) framework for Bayesian symbolic regression that approximates the posterior distribution over symbolic expressions, enhancing robustness and enabling uncertainty quantification for symbolic regression in the presence of noise. Differing from traditional genetic programming approaches, the SMC-based algorithm combines probabilistic selection, adaptive tempering, and the use of normalized marginal likelihood to efficiently explore the search space of symbolic expressions, yielding parsimonious expressions with improved generalization. When compared to standard genetic programming baselines, the proposed method better deals with challenging, noisy benchmark datasets. The reduced tendency to overfit and enhanced ability to discover accurate and interpretable equations paves the way for more robust symbolic regression in scientific discovery and engineering design applications.

</details>


### [148] [Scaling Behavior of Discrete Diffusion Language Models](https://arxiv.org/abs/2512.10858)
*Dimitri von Rütte,Janis Fluri,Omead Pooladzandi,Bernhard Schölkopf,Thomas Hofmann,Antonio Orvieto*

Main category: cs.LG

TL;DR: 研究离散扩散语言模型（DLMs）不同噪声类型下的缩放行为，发现其缩放行为依赖噪声类型且与自回归语言模型（ALMs）不同，均匀扩散在数据受限场景有潜力，还训练出已知最大公开均匀扩散模型。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型预训练消耗大，DLMs缩放行为未充分探索，此前认为其需更多数据和计算资源才能达ALMs性能，故研究其缩放行为。

Method: 通过在掩码和均匀扩散间平滑插值研究不同噪声类型下DLMs缩放行为，关注批大小和学习率等超参数。

Result: DLMs缩放行为强烈依赖噪声类型，与ALMs显著不同；所有噪声类型在计算受限缩放时收敛到相似损失值；均匀扩散在计算高效训练时比掩码扩散需更多参数和更少数据；将均匀扩散模型扩展到100亿参数，证实预测的缩放行为。

Conclusion: DLMs缩放行为与噪声类型相关，均匀扩散在数据受限场景是有前景的选择，训练出的100亿参数均匀扩散模型验证了缩放行为预测。

Abstract: Modern LLM pre-training consumes vast amounts of compute and training data, making the scaling behavior, or scaling laws, of different models a key distinguishing factor. Discrete diffusion language models (DLMs) have been proposed as an alternative to autoregressive language models (ALMs). However, their scaling behavior has not yet been fully explored, with prior work suggesting that they require more data and compute to match the performance of ALMs.
  We study the scaling behavior of DLMs on different noise types by smoothly interpolating between masked and uniform diffusion while paying close attention to crucial hyperparameters such as batch size and learning rate. Our experiments reveal that the scaling behavior of DLMs strongly depends on the noise type and is considerably different from ALMs. While all noise types converge to similar loss values in compute-bound scaling, we find that uniform diffusion requires more parameters and less data for compute-efficient training compared to masked diffusion, making them a promising candidate in data-bound settings. We scale our uniform diffusion model up to 10B parameters trained for $10^{22}$ FLOPs, confirming the predicted scaling behavior and making it the largest publicly known uniform diffusion model to date.

</details>


### [149] [UrbanAI 2025 Challenge: Linear vs Transformer Models for Long-Horizon Exogenous Temperature Forecasting](https://arxiv.org/abs/2512.10866)
*Ruslan Gokhman*

Main category: cs.LG

TL;DR: 研究仅使用室内温度历史值的长时程温度预测，对比线性和Transformer模型，线性模型表现更好。


<details>
  <summary>Details</summary>
Motivation: 解决仅使用室内温度历史值进行长时程温度预测这一具有挑战性的单变量预测问题。

Method: 使用线性和Transformer系列模型，在标准化的训练、验证和测试集上评估Linear、NLinear、DLinear、Transformer、Informer和Autoformer。

Result: 线性基线模型（Linear、NLinear、DLinear）始终优于更复杂的Transformer系列架构，DLinear在所有分割中总体准确率最高。

Conclusion: 精心设计的线性模型在仅含外生变量的挑战性时间序列预测中仍是强大的基线模型。

Abstract: We study long-horizon exogenous-only temperature forecasting - a challenging univariate setting where only the past values of the indoor temperature are used for prediction - using linear and Transformer-family models. We evaluate Linear, NLinear, DLinear, Transformer, Informer, and Autoformer under standardized train, validation, and test splits. Results show that linear baselines (Linear, NLinear, DLinear) consistently outperform more complex Transformer-family architectures, with DLinear achieving the best overall accuracy across all splits. These findings highlight that carefully designed linear models remain strong baselines for time series forecasting in challenging exogenous-only settings.

</details>


### [150] [Guided Transfer Learning for Discrete Diffusion Models](https://arxiv.org/abs/2512.10877)
*Julian Kleutgens,Claudio Battiloro,Lingkai Kong,Benjamin Grewe,Francesca Dominici,Mauricio Tec*

Main category: cs.LG

TL;DR: 提出离散扩散模型的引导式迁移学习方法GTL及高效引导采样器，降低计算成本，使大规模引导语言建模可行。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型性能依赖大训练集，现有迁移学习方法计算成本高，引导离散扩散在处理大词汇表和长序列时效率低。

Method: 基于连续扩散的比率迁移学习，提出GTL，不修改预训练去噪器，给出统一处理方式；提出高效引导采样器，集中评估位置和候选词。

Result: 在顺序数据上评估GTL，进行了实证分析。

Conclusion: GTL及高效引导采样器使大规模引导语言建模成为可能。

Abstract: Discrete diffusion models achieve strong performance across language and other discrete domains, providing a powerful alternative to autoregressive models. However, their strong performance relies on large training datasets, which are costly or risky to obtain, especially when adapting to new domains. Transfer learning is the natural way to adapt pretrained discrete diffusion models, but current methods require fine-tuning large diffusion models, which is computationally expensive and often impractical. Building on ratio-based transfer learning for continuous diffusion, we provide Guided Transfer Learning for discrete diffusion models (GTL). This enables sampling from a target distribution without modifying the pretrained denoiser. The same guidance formulation applies to both discrete-time diffusion and continuous-time score-based discrete diffusion, yielding a unified treatment. Guided discrete diffusion often requires many forward passes of the guidance network, which becomes impractical for large vocabularies and long sequences. To address this, we further present an efficient guided sampler that concentrates evaluations on planner-selected positions and top candidate tokens, thus lowering sampling time and computation. This makes guided language modeling practical at scale for large vocabularies and long sequences. We evaluate GTL on sequential data, including synthetic Markov chains and language modeling, and provide empirical analyses of its behavior.

</details>


### [151] [Classifier Reconstruction Through Counterfactual-Aware Wasserstein Prototypes](https://arxiv.org/abs/2512.10878)
*Xuan Zhao,Zhuo Cao,Arya Bangun,Hanno Scharr,Ira Assent*

Main category: cs.LG

TL;DR: 本文提出结合原始样本和反事实样本的方法改进模型重构，实证验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有模型重构方法可改进，尤其在标注数据有限场景，且朴素使用反事实样本会有决策边界偏移问题。

Method: 将原始数据样本与反事实样本结合，用Wasserstein重心近似类原型，保留各类分布结构。

Result: 多个数据集的实证结果表明，该方法提高了替代模型与目标模型间的保真度。

Conclusion: 提出的方法有效，能加强替代模型质量，缓解决策边界偏移问题。

Abstract: Counterfactual explanations provide actionable insights by identifying minimal input changes required to achieve a desired model prediction. Beyond their interpretability benefits, counterfactuals can also be leveraged for model reconstruction, where a surrogate model is trained to replicate the behavior of a target model. In this work, we demonstrate that model reconstruction can be significantly improved by recognizing that counterfactuals, which typically lie close to the decision boundary, can serve as informative though less representative samples for both classes. This is particularly beneficial in settings with limited access to labeled data. We propose a method that integrates original data samples with counterfactuals to approximate class prototypes using the Wasserstein barycenter, thereby preserving the underlying distributional structure of each class. This approach enhances the quality of the surrogate model and mitigates the issue of decision boundary shift, which commonly arises when counterfactuals are naively treated as ordinary training instances. Empirical results across multiple datasets show that our method improves fidelity between the surrogate and target models, validating its effectiveness.

</details>


### [152] [SparseSwaps: Tractable LLM Pruning Mask Refinement at Scale](https://arxiv.org/abs/2512.10922)
*Max Zimmer,Christophe Roux,Moritz Wagner,Deborah Hendrych,Sebastian Pokutta*

Main category: cs.LG

TL;DR: 本文提出一种可处理的简单1 - swap算法，能在大语言模型规模下有效减少每层剪枝误差，提升困惑度和零样本准确率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型剪枝方法在Transformer架构上效果欠佳，精确求解掩码选择问题计算不可行，需更优方法。

Method: 通过强制每行具有相同稀疏度来解耦行，利用校准数据的Gram矩阵高效计算最优1 - swap，提出1 - swap算法。

Result: 该方法相比Wanda最多可减少60%的每层剪枝误差，在GPT架构上持续改善困惑度和零样本准确率。

Conclusion: 所提算法在大语言模型规模下使掩码选择问题更易处理，且效果良好。

Abstract: The resource requirements of Neural Networks can be significantly reduced through pruning -- the removal of seemingly less important parameters. However, with the rise of Large Language Models (LLMs), full retraining to recover pruning-induced performance degradation is often prohibitive and classical approaches such as global magnitude pruning are suboptimal on Transformer architectures. State-of-the-art methods hence solve a layer-wise mask selection problem, the problem of finding a pruning mask which minimizes the per-layer pruning error on a small set of calibration data. Exactly solving this problem to optimality using Integer Programming (IP) solvers is computationally infeasible due to its combinatorial nature and the size of the search space, and existing approaches therefore rely on approximations or heuristics. In this work, we demonstrate that the mask selection problem can be made drastically more tractable at LLM scale. To that end, we decouple the rows by enforcing equal sparsity levels per row. This allows us to derive optimal 1-swaps (exchanging one kept and one pruned weight) that can be computed efficiently using the Gram matrix of the calibration data. Using these observations, we propose a tractable and simple 1-swap algorithm that warm starts from any pruning mask, runs efficiently on GPUs at LLM scale, and is essentially hyperparameter-free. We demonstrate that our approach reduces per-layer pruning error by up to 60% over Wanda (Sun et al., 2023) and consistently improves perplexity and zero-shot accuracy across state-of-the-art GPT architectures.

</details>


### [153] [Digital Twin Supervised Reinforcement Learning Framework for Autonomous Underwater Navigation](https://arxiv.org/abs/2512.10925)
*Zamirddine Mari,Mohamad Motasem Nawaf,Pierre Drap*

Main category: cs.LG

TL;DR: 本文围绕BlueROV2研究水下自主导航问题，提出基于PPO算法的深度强化学习方法，与DWA对比，结果表明PPO在复杂环境更优，且学习行为可从仿真迁移到现实。


<details>
  <summary>Details</summary>
Motivation: 解决水下因无GPS、能见度低和有障碍物导致的自主导航难题。

Method: 提出基于PPO算法的深度强化学习方法，观察空间结合目标导航信息、虚拟占用网格和射线投射；与DWA对比，在仿真环境评估并在物理BlueROV2上用数字孪生验证。

Result: PPO策略在高度复杂环境中始终优于DWA，能更好地局部适应并减少碰撞。

Conclusion: 学习行为可从仿真迁移到现实，证实深度强化学习对水下机器人自主导航有重要意义。

Abstract: Autonomous navigation in underwater environments remains a major challenge due to the absence of GPS, degraded visibility, and the presence of submerged obstacles. This article investigates these issues through the case of the BlueROV2, an open platform widely used for scientific experimentation. We propose a deep reinforcement learning approach based on the Proximal Policy Optimization (PPO) algorithm, using an observation space that combines target-oriented navigation information, a virtual occupancy grid, and ray-casting along the boundaries of the operational area. The learned policy is compared against a reference deterministic kinematic planner, the Dynamic Window Approach (DWA), commonly employed as a robust baseline for obstacle avoidance. The evaluation is conducted in a realistic simulation environment and complemented by validation on a physical BlueROV2 supervised by a 3D digital twin of the test site, helping to reduce risks associated with real-world experimentation. The results show that the PPO policy consistently outperforms DWA in highly cluttered environments, notably thanks to better local adaptation and reduced collisions. Finally, the experiments demonstrate the transferability of the learned behavior from simulation to the real world, confirming the relevance of deep RL for autonomous navigation in underwater robotics.

</details>


### [154] [Asynchronous Reasoning: Training-Free Interactive Thinking LLMs](https://arxiv.org/abs/2512.10931)
*George Yakushev,Nataliia Babina,Masoud Vahid Dastgerdi,Vyacheslav Zhdanovskiy,Alina Shutova,Denis Kuznedelev*

Main category: cs.LG

TL;DR: 本文提出方法使大语言模型能实时思考、倾听和生成输出，减少响应时间。


<details>
  <summary>Details</summary>
Motivation: 现有具备推理能力的大语言模型交互性差，无法实时响应和适应新信息，而人类能异步思考行动，因此要改进大语言模型。

Method: 利用旋转嵌入的特性，让为顺序交互构建的大语言模型能同时思考、倾听和生成输出。

Result: 在数学、常识和安全推理任务上评估，能实时生成准确的思考增强答案，将首次非思考标记的时间从数分钟减少到不超过5秒，整体实时延迟降低6 - 11倍。

Conclusion: 所提方法无需额外训练，能有效提升大语言模型实时交互能力。

Abstract: Many state-of-the-art LLMs are trained to think before giving their answer. Reasoning can greatly improve language model capabilities and safety, but it also makes them less interactive: given a new input, a model must stop thinking before it can respond. Real-world use cases such as voice-based or embedded assistants require an LLM agent to respond and adapt to additional information in real time, which is incompatible with sequential interactions. In contrast, humans can listen, think, and act asynchronously: we begin thinking about the problem while reading it and continue thinking while formulating the answer. In this work, we augment LLMs capable of reasoning to operate in a similar way without additional training. Our method uses the properties of rotary embeddings to enable LLMs built for sequential interactions to simultaneously think, listen, and generate outputs. We evaluate our approach on math, commonsense, and safety reasoning and find that it can generate accurate thinking-augmented answers in real time, reducing time to first non-thinking token from minutes to <= 5s. and the overall real-time delays by 6-11x.

</details>


### [155] [Empirical evaluation of the Frank-Wolfe methods for constructing white-box adversarial attacks](https://arxiv.org/abs/2512.10936)
*Kristina Korotkova,Aleksandr Katrutsa*

Main category: cs.LG

TL;DR: 从数值优化角度用改进Frank - Wolfe方法构建白盒对抗攻击，并在数据集上实验。


<details>
  <summary>Details</summary>
Motivation: 构建神经网络对抗攻击是其应用关键挑战，需要快速有效方法评估对抗鲁棒性。

Method: 从数值优化角度，用改进Frank - Wolfe方法构建白盒对抗攻击，对方法进行理论和数值评估，并与标准方法对比。

Result: 在MNIST和CIFAR - 10数据集上，用多类逻辑回归模型、CNN和ViT进行数值实验。

Conclusion: 未在摘要体现明确结论。

Abstract: The construction of adversarial attacks for neural networks appears to be a crucial challenge for their deployment in various services. To estimate the adversarial robustness of a neural network, a fast and efficient approach is needed to construct adversarial attacks. Since the formalization of adversarial attack construction involves solving a specific optimization problem, we consider the problem of constructing an efficient and effective adversarial attack from a numerical optimization perspective. Specifically, we suggest utilizing advanced projection-free methods, known as modified Frank-Wolfe methods, to construct white-box adversarial attacks on the given input data. We perform a theoretical and numerical evaluation of these methods and compare them with standard approaches based on projection operations or geometrical intuition. Numerical experiments are performed on the MNIST and CIFAR-10 datasets, utilizing a multiclass logistic regression model, the convolutional neural networks (CNNs), and the Vision Transformer (ViT).

</details>


### [156] [Stronger Normalization-Free Transformers](https://arxiv.org/abs/2512.10938)
*Mingzhi Chen,Taiming Lu,Jiachen Zhu,Mingjie Sun,Zhuang Liu*

Main category: cs.LG

TL;DR: 文章探讨替代归一化层的逐点函数，找到Derf函数，它在多领域表现优于其他方法，适合免归一化Transformer架构。


<details>
  <summary>Details</summary>
Motivation: 寻找能超越Dynamic Tanh（DyT）的函数设计，为深度学习架构提供更好的替代归一化层的方案。

Method: 先研究逐点函数固有属性对训练和性能的影响，再进行大规模搜索。

Result: 引入Derf函数，其在多种领域表现优于LayerNorm、RMSNorm和DyT。

Conclusion: Derf性能提升源于泛化能力，简单且性能强，适合免归一化Transformer架构。

Abstract: Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) has demonstrated that alternatives are possible. The point-wise function DyT constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how the intrinsic properties of point-wise functions influence training and performance. Building on these findings, we conduct a large-scale search for a more effective function design. Through this exploration, we introduce $\mathrm{Derf}(x) = \mathrm{erf}(αx + s)$, where $\mathrm{erf}(x)$ is the rescaled Gaussian cumulative distribution function, and identify it as the most performant design. Derf outperforms LayerNorm, RMSNorm, and DyT across a wide range of domains, including vision (image recognition and generation), speech representation, and DNA sequence modeling. Our findings suggest that the performance gains of Derf largely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for normalization-free Transformer architectures.

</details>


### [157] [Hierarchical Dataset Selection for High-Quality Data Sharing](https://arxiv.org/abs/2512.10952)
*Xiaona Zhou,Yingyan Zeng,Ran Jin,Ismini Lourentzou*

Main category: cs.LG

TL;DR: 提出DaSH数据集选择方法，在两基准上表现优于基线，适用于多源学习


<details>
  <summary>Details</summary>
Motivation: 现有方法在数据选择时忽略数据集及来源差异，需解决从大量异构数据集中选择数据集以提升下游性能的问题

Method: 提出Dataset Selection via Hierarchies (DaSH)方法，在数据集和组级别对效用建模

Result: 在Digit - Five和DomainNet基准上，DaSH准确率最高超现有基线26.2%，且探索步骤少

Conclusion: DaSH在低资源和缺乏相关数据集场景下稳健，适用于多源学习中可扩展和自适应的数据集选择

Abstract: The success of modern machine learning hinges on access to high-quality training data. In many real-world scenarios, such as acquiring data from public repositories or sharing across institutions, data is naturally organized into discrete datasets that vary in relevance, quality, and utility. Selecting which repositories or institutions to search for useful datasets, and which datasets to incorporate into model training are therefore critical decisions, yet most existing methods select individual samples and treat all data as equally relevant, ignoring differences between datasets and their sources. In this work, we formalize the task of dataset selection: selecting entire datasets from a large, heterogeneous pool to improve downstream performance under resource constraints. We propose Dataset Selection via Hierarchies (DaSH), a dataset selection method that models utility at both dataset and group (e.g., collections, institutions) levels, enabling efficient generalization from limited observations. Across two public benchmarks (Digit-Five and DomainNet), DaSH outperforms state-of-the-art data selection baselines by up to 26.2% in accuracy, while requiring significantly fewer exploration steps. Ablations show DaSH is robust to low-resource settings and lack of relevant datasets, making it suitable for scalable and adaptive dataset selection in practical multi-source learning workflows.

</details>


### [158] [Bidirectional Normalizing Flow: From Data to Noise and Back](https://arxiv.org/abs/2512.10953)
*Yiyang Lu,Qiao Sun,Xianbang Wang,Zhicheng Jiang,Hanhong Zhao,Kaiming He*

Main category: cs.LG

TL;DR: 提出双向归一化流BiFlow框架，无需精确解析逆，在ImageNet实验中提升生成质量且加速采样。


<details>
  <summary>Details</summary>
Motivation: 标准NF存在精确逆的限制，近期TARFlow及其变体暴露因果解码瓶颈，需更灵活的框架。

Method: 引入BiFlow框架，学习近似噪声到数据逆映射的反向模型。

Result: 在ImageNet上相比因果解码方法，提升生成质量，加速采样达两个数量级，在NF方法中达SOTA，在单评估方法中表现有竞争力。

Conclusion: 希望该工作能吸引更多关注经典的NF范式。

Abstract: Normalizing Flows (NFs) have been established as a principled framework for generative modeling. Standard NFs consist of a forward process and a reverse process: the forward process maps data to noise, while the reverse process generates samples by inverting it. Typical NF forward transformations are constrained by explicit invertibility, ensuring that the reverse process can serve as their exact analytic inverse. Recent developments in TARFlow and its variants have revitalized NF methods by combining Transformers and autoregressive flows, but have also exposed causal decoding as a major bottleneck. In this work, we introduce Bidirectional Normalizing Flow ($\textbf{BiFlow}$), a framework that removes the need for an exact analytic inverse. BiFlow learns a reverse model that approximates the underlying noise-to-data inverse mapping, enabling more flexible loss functions and architectures. Experiments on ImageNet demonstrate that BiFlow, compared to its causal decoding counterpart, improves generation quality while accelerating sampling by up to two orders of magnitude. BiFlow yields state-of-the-art results among NF-based methods and competitive performance among single-evaluation ("1-NFE") methods. Following recent encouraging progress on NFs, we hope our work will draw further attention to this classical paradigm.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [159] [Spatial Spiking Neural Networks Enable Efficient and Robust Temporal Computation](https://arxiv.org/abs/2512.10011)
*Lennart P. L. Landsmeer,Amirreza Movahedin,Mario Negrello,Said Hamdioui,Christos Strydis*

Main category: cs.NE

TL;DR: 提出空间脉冲神经网络（SpSNNs），减少参数、保留时间表达能力，在基准测试中表现优异，适合硬件实现。


<details>
  <summary>Details</summary>
Motivation: 现有脉冲神经网络（SNNs）将突触延迟作为可训练无约束参数，导致内存占用大、计算需求高且缺乏生物合理性，需改进。

Method: 引入SpSNNs框架，让神经元学习有限维欧几里得空间坐标，延迟由神经元间距离产生，用位置学习替代突触延迟学习，通过自动微分计算精确延迟梯度。

Result: 在基准测试中SpSNNs参数少但性能优于SNNs，2D和3D网络性能最佳，动态稀疏化后参数大幅减少仍保持精度，适合硬件实现。

Conclusion: SpSNNs为探索时间计算中的空间结构提供平台，是可扩展、节能神经形态智能的硬件友好基础。

Abstract: The efficiency of modern machine intelligence depends on high accuracy with minimal computational cost. In spiking neural networks (SNNs), synaptic delays are crucial for encoding temporal structure, yet existing models treat them as fully trainable, unconstrained parameters, leading to large memory footprints, higher computational demand, and a departure from biological plausibility. In the brain, however, delays arise from physical distances between neurons embedded in space. Building on this principle, we introduce Spatial Spiking Neural Networks (SpSNNs), a framework in which neurons learn coordinates in a finite-dimensional Euclidean space and delays emerge from inter-neuron distances. This replaces per-synapse delay learning with position learning, substantially reducing parameter count while retaining temporal expressiveness. Across the Yin-Yang and Spiking Heidelberg Digits benchmarks, SpSNNs outperform SNNs with unconstrained delays despite using far fewer parameters. Performance consistently peaks in 2D and 3D networks rather than infinite-dimensional delay spaces, revealing a geometric regularization effect. Moreover, dynamically sparsified SpSNNs maintain full accuracy even at 90% sparsity, matching standard delay-trained SNNs while using up to 18x fewer parameters. Because learned spatial layouts map naturally onto hardware geometries, SpSNNs lend themselves to efficient neuromorphic implementation. Methodologically, SpSNNs compute exact delay gradients via automatic differentiation with custom-derived rules, supporting arbitrary neuron models and architectures. Altogether, SpSNNs provide a principled platform for exploring spatial structure in temporal computation and offer a hardware-friendly substrate for scalable, energy-efficient neuromorphic intelligence.

</details>


### [160] [A Spiking Neural Network Implementation of Gaussian Belief Propagation](https://arxiv.org/abs/2512.10638)
*Sepideh Adamiat,Wouter M. Kouw,Bert de Vries*

Main category: cs.NE

TL;DR: 研究通过模拟的漏电积分-放电神经元网络执行分布式贝叶斯推理，实现核心线性操作，验证准确且展示应用潜力，为概率推理的神经形态实现迈进了一步。


<details>
  <summary>Details</summary>
Motivation: 探究神经机制如何执行贝叶斯推理的抽象操作。

Method: 在漏电积分 - 放电神经元模拟网络中执行因子图上的消息传递进行高斯信念传播，实现三个核心线性操作。

Result: 与标准和积算法对比显示消息更新准确，在卡尔曼滤波和贝叶斯线性回归应用中展示了框架在静态和动态推理任务中的潜力。

Conclusion: 研究为概率推理的生物基础神经形态实现提供了进展。

Abstract: Bayesian inference offers a principled account of information processing in natural agents. However, it remains an open question how neural mechanisms perform their abstract operations. We investigate a hypothesis where a distributed form of Bayesian inference, namely message passing on factor graphs, is performed by a simulated network of leaky-integrate-and-fire neurons. Specifically, we perform Gaussian belief propagation by encoding messages that come into factor nodes as spike-based signals, propagating these signals through a spiking neural network (SNN) and decoding the spike-based signal back to an outgoing message. Three core linear operations, equality (branching), addition, and multiplication, are realized in networks of leaky integrate-and-fire models. Validation against the standard sum-product algorithm shows accurate message updates, while applications to Kalman filtering and Bayesian linear regression demonstrate the framework's potential for both static and dynamic inference tasks. Our results provide a step toward biologically grounded, neuromorphic implementations of probabilistic reasoning.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [161] [Search-based Software Testing Driven by Domain Knowledge: Reflections and New Perspectives](https://arxiv.org/abs/2512.10079)
*Federico Formica,Mark Lawford,Claudio Menghi*

Main category: cs.SE

TL;DR: 本文回顾基于搜索的软件测试（SBST）结合领域知识的实验结果，为未来研究提供新方向。


<details>
  <summary>Details</summary>
Motivation: SBST缺乏工程师的领域知识，虽有技术将领域知识融入现有SBST框架，但需从新视角审视。

Method: 回顾近期实验结果，强调其中大胆且意外的结果。

Result: 无明确提及具体结果

Conclusion: 有助于从新视角重新审视基于领域知识的SBST技术，并为未来研究指明新方向。

Abstract: Search-based Software Testing (SBST) can automatically generate test cases to search for requirements violations. Unlike manual test case development, it can generate a substantial number of test cases in a limited time. However, SBST does not possess the domain knowledge of engineers. Several techniques have been proposed to integrate engineers' domain knowledge within existing SBST frameworks. This paper will reflect on recent experimental results by highlighting bold and unexpected results. It will help re-examine SBST techniques driven by domain knowledge from a new perspective, suggesting new directions for future research.

</details>


### [162] [ATLAS: Automated Toolkit for Large-Scale Verified Code Synthesis](https://arxiv.org/abs/2512.10173)
*Mantas Baksys,Stefan Zetzsche,Olivier Bouissou,Remi Delmas,Soonho Kong*

Main category: cs.SE

TL;DR: 提出ATLAS自动化管道合成大量验证程序，微调模型后在相关测试集表现提升，证明合成代码可增强大模型形式验证能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在程序验证方面进展受限于训练用验证代码稀缺。

Method: 构建ATLAS自动化管道，将合成过程分解为多个专业任务生成Dafny程序并提取训练示例，用这些数据微调Qwen 2.5 7B Coder。

Result: 生成2.7K个验证程序，提取超19K训练示例，微调后模型在DafnyBench提升23个百分点，在DafnySynthesis提升50个百分点。

Conclusion: 合成的验证代码能有效增强大语言模型的形式验证能力。

Abstract: Large language models have shown potential for program verification, but progress is hindered by the scarcity of verified code for training. We present ATLAS, an automated pipeline that synthesizes verified programs at scale to address this data bottleneck. ATLAS generates complete Dafny programs with specifications, implementations, and proofs, producing 2.7K verified programs from which we extract over 19K training examples--more than 7 per verified program--by decomposing the synthesis process into multiple specialized tasks. Fine-tuning Qwen 2.5 7B Coder on this dataset produces substantial gains: +23 percentage points on DafnyBench and +50 percentage points on DafnySynthesis. These results demonstrate that synthetic verified code can effectively enhance LLM capabilities for formal verification.

</details>


### [163] [Does SWE-Bench-Verified Test Agent Ability or Model Memory?](https://arxiv.org/abs/2512.10218)
*Thanosan Prathifkumar,Noble Saji Mathews,Meiyappan Nagappan*

Main category: cs.SE

TL;DR: 研究SWE - Bench - Verified基准，发现模型在该基准表现好可能因训练数据重叠，分数不能反映解决实际问题能力，建议转向新数据集。


<details>
  <summary>Details</summary>
Motivation: 研究SWE - Bench - Verified基准是否与模型训练数据重叠，其分数能否反映解决问题能力。

Method: 测试两个Claude模型，分别用问题文本和问题文本加文件路径找相关文件，在不同基准测试。

Result: 模型在SWE - Bench - Verified基准表现远好于其他基准，提示训练数据可能重叠。

Conclusion: 依赖旧的流行基准有风险，应转向考虑污染问题构建的新数据集。

Abstract: SWE-Bench-Verified, a dataset comprising 500 issues, serves as a de facto benchmark for evaluating various large language models (LLMs) on their ability to resolve GitHub issues. But this benchmark may overlap with model training data. If that is true, scores may reflect training recall, not issue-solving skill. To study this, we test two Claude models that frequently appear in top-performing agents submitted to the benchmark. We ask them to find relevant files using only issue text, and then issue text plus file paths. We then run the same setup on BeetleBox and SWE-rebench. Despite both benchmarks involving popular open-source Python projects, models performed 3 times better on SWE-Bench-Verified. They were also 6 times better at finding edited files, without any additional context about the projects themselves. This gap suggests the models may have seen many SWE-Bench-Verified tasks during training. As a result, scores on this benchmark may not reflect an agent's ability to handle real software issues, yet it continues to be used in ways that can misrepresent progress and lead to choices that favour agents that use certain models over strong agent design. Our setup tests the localization step with minimal context to the extent that the task should be logically impossible to solve. Our results show the risk of relying on older popular benchmarks and support the shift toward newer datasets built with contamination in mind.

</details>


### [164] [Studying and Automating Issue Resolution for Software Quality](https://arxiv.org/abs/2512.10238)
*Antu Saha*

Main category: cs.SE

TL;DR: 研究通过三个方向解决软件问题解决中的挑战，推动AI驱动的问题解决，支持高质量软件系统。


<details>
  <summary>Details</summary>
Motivation: 开发者在软件问题解决中面临低质量问题报告、对实际工作流理解有限和缺乏自动化支持等挑战。

Method: 一是利用大语言模型推理和特定应用信息提高问题报告质量；二是实证分析传统和AI增强系统中的开发者工作流；三是通过机器学习、深度学习和大语言模型方法自动化认知要求高的解决任务。

Result: 获得了实证见解、实用工具和自动化方法。

Conclusion: 研究成果能推动AI驱动的问题解决，支持更易维护和高质量的软件系统。

Abstract: Effective issue resolution is crucial for maintaining software quality. Yet developers frequently encounter challenges such as low-quality issue reports, limited understanding of real-world workflows, and a lack of automated support. This research aims to address these challenges through three complementary directions. First, we enhance issue report quality by proposing techniques that leverage LLM reasoning and application-specific information. Second, we empirically characterize developer workflows in both traditional and AI-augmented systems. Third, we automate cognitively demanding resolution tasks, including buggy UI localization and solution identification, through ML, DL, and LLM-based approaches. Together, our work delivers empirical insights, practical tools, and automated methods to advance AI-driven issue resolution, supporting more maintainable and high-quality software systems.

</details>


### [165] [Cross-modal Retrieval Models for Stripped Binary Analysis](https://arxiv.org/abs/2512.10393)
*Guoqiang Chen,Lingyun Ying,Ziyang Song,Daguang Liu,Qiang Wang,Zhiqi Wang,Li Hu,Shaoyin Cheng,Weiming Zhang,Nenghai Yu*

Main category: cs.SE

TL;DR: 本文提出名为BinSeek的两阶段跨模态检索框架用于剥离符号的二进制代码分析，评估显示其性能达最优。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型智能体的二进制代码分析有场景应用潜力，但基于用户查询从大量剥离符号的二进制函数中检索所需内容的研究不足且有挑战，与源代码检索不同。

Method: 提出BinSeek框架，包含BinSeek Embedding和BinSeek - Reranker两个模型，构建基于大语言模型的数据合成管道自动构建训练，推导领域基准。

Result: BinSeek取得最优性能，Rec@3相比同规模模型提升31.42%，MRR@3提升27.17%，领先参数大16倍的通用模型。

Conclusion: BinSeek在剥离符号的二进制代码分析中表现出色，提供了有效的检索框架。

Abstract: LLM-agent based binary code analysis has demonstrated significant potential across a wide range of software security scenarios, including vulnerability detection, malware analysis, etc. In agent workflow, however, retrieving the positive from thousands of stripped binary functions based on user query remains under-studied and challenging, as the absence of symbolic information distinguishes it from source code retrieval. In this paper, we introduce, BinSeek, the first two-stage cross-modal retrieval framework for stripped binary code analysis. It consists of two models: BinSeekEmbedding is trained on large-scale dataset to learn the semantic relevance of the binary code and the natural language description, furthermore, BinSeek-Reranker learns to carefully judge the relevance of the candidate code to the description with context augmentation. To this end, we built an LLM-based data synthesis pipeline to automate training construction, also deriving a domain benchmark for future research. Our evaluation results show that BinSeek achieved the state-of-the-art performance, surpassing the the same scale models by 31.42% in Rec@3 and 27.17% in MRR@3, as well as leading the advanced general-purpose models that have 16 times larger parameters.

</details>


### [166] [How to Trick Your AI TA: A Systematic Study of Academic Jailbreaking in LLM Code Evaluation](https://arxiv.org/abs/2512.10415)
*Devanshu Sahoo,Vasudev Majhi,Arjun Neekhra,Yash Sinha,Murari Mandal,Dhruv Kumar*

Main category: cs.SE

TL;DR: 本文对学术环境中基于大语言模型的自动代码评估器越狱攻击进行大规模研究，提出学术越狱攻击，发布数据集，定义评估指标并评估，发现模型易受攻击，为后续评估器研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 学生可能使用对抗性提示策略使大语言模型自动代码评估器误判，影响其可靠性，因此研究学术环境中的越狱攻击。

Method: 系统调整20多种越狱策略用于学术场景；发布2.5万个对抗性学生提交的中毒数据集；定义三个越狱评估指标；用六个大语言模型评估学术越狱攻击。

Result: 大语言模型对说服和角色扮演类攻击表现出显著脆弱性，越狱成功率最高达97%。

Conclusion: 研究的对抗性数据集和基准套件为下一代学术代码评估中基于大语言模型的鲁棒评估器奠定基础。

Abstract: The use of Large Language Models (LLMs) as automatic judges for code evaluation is becoming increasingly prevalent in academic environments. But their reliability can be compromised by students who may employ adversarial prompting strategies in order to induce misgrading and secure undeserved academic advantages. In this paper, we present the first large-scale study of jailbreaking LLM-based automated code evaluators in academic context. Our contributions are: (i) We systematically adapt 20+ jailbreaking strategies for jailbreaking AI code evaluators in the academic context, defining a new class of attacks termed academic jailbreaking. (ii) We release a poisoned dataset of 25K adversarial student submissions, specifically designed for the academic code-evaluation setting, sourced from diverse real-world coursework and paired with rubrics and human-graded references, and (iii) In order to capture the multidimensional impact of academic jailbreaking, we systematically adapt and define three jailbreaking metrics (Jailbreak Success Rate, Score Inflation, and Harmfulness). (iv) We comprehensively evalulate the academic jailbreaking attacks using six LLMs. We find that these models exhibit significant vulnerability, particularly to persuasive and role-play-based attacks (up to 97% JSR). Our adversarial dataset and benchmark suite lay the groundwork for next-generation robust LLM-based evaluators in academic code assessment.

</details>


### [167] [UniCoR: Modality Collaboration for Robust Cross-Language Hybrid Code Retrieval](https://arxiv.org/abs/2512.10452)
*Yang Yang,Li Kuang,Jiakun Liu,Zhongxin Liu,Yingjie Xia,David Lo*

Main category: cs.SE

TL;DR: 本文指出混合代码检索现有方法存在的问题，提出 UniCoR 框架解决问题，实验表明其性能优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法在混合代码检索，尤其是跨语言场景下能否有效利用混合查询尚不明确，需研究代表性代码模型并解决存在的挑战。

Method: 提出 UniCoR 框架，设计多视角监督对比学习模块增强语义理解和模态融合，引入表示分布一致性学习模块提升跨语言泛化能力。

Result: 在经验基准和大规模基准实验中，UniCoR 优于所有基线模型，MRR 平均提升 8.64%，MAP 平均提升 11.54%，且在混合代码检索中表现稳定，在跨语言场景有泛化能力。

Conclusion: UniCoR 框架能有效解决现有混合代码检索方法存在的语义理解不足、融合效率低和跨语言泛化弱的问题。

Abstract: Effective code retrieval is indispensable and it has become an important paradigm to search code in hybrid mode using both natural language and code snippets. Nevertheless, it remains unclear whether existing approaches can effectively leverage such hybrid queries, particularly in cross-language contexts. We conduct a comprehensive empirical study of representative code models and reveal three challenges: (1) insufficient semantic understanding; (2) inefficient fusion in hybrid code retrieval; and (3) weak generalization in cross-language scenarios. To address these challenges, we propose UniCoR, a novel self-supervised framework that learns Unified Code Representations framework designed to learn unified and robust code representations. Firstly, we design a multi-perspective supervised contrastive learning module to enhance semantic understanding and modality fusion. It aligns representations from multiple perspectives, including code-to-code, natural language-to-code, and natural language-to-natural language, enforcing the model to capture a semantic essence among modalities. Secondly, we introduce a representation distribution consistency learning module to improve cross-language generalization, which explicitly aligns the feature distributions of different programming languages, enabling language-agnostic representation learning. Extensive experiments on both empirical benchmark and large-scale benchmark show that UniCoR outperforms all baseline models, achieving an average improvement of 8.64% in MRR and 11.54% in MAP over the best-performing baseline. Furthermore, UniCoR exhibits stability in hybrid code retrieval and generalization capability in cross-language scenarios.

</details>


### [168] [Decoding Human-LLM Collaboration in Coding: An Empirical Study of Multi-Turn Conversations in the Wild](https://arxiv.org/abs/2512.10493)
*Binquan Zhang,Li Zhang,Haoyuan Zhang,Fang Liu,Song Wang,Bo Shen,An Fu,Lin Shi*

Main category: cs.SE

TL;DR: 本文利用LMSYS - Chat - 1M和WildChat数据集对人机编码协作进行实证分析，揭示了任务类型影响交互模式、不同任务对大语言模型指令遵循能力的挑战及用户满意度差异，为改进LLM接口和未来研究提供方向。


<details>
  <summary>Details</summary>
Motivation: 现有研究很少系统地探索编码场景中人类与大语言模型（LLM）的协作机制，本文旨在深入了解人机协作机制、LLMs指令遵循能力和用户满意度。

Method: 使用LMSYS - Chat - 1M和WildChat数据集进行实证分析。

Result: 1) 任务类型塑造交互模式；2) 错误修复和代码重构对LLMs指令遵循挑战更大；3) 不同任务下用户满意度不同。

Conclusion: 研究结果为改进LLM接口和提高用户满意度提出建议，拓宽了对人机协同的理解，支持更有效的AI辅助开发，并为未来自适应对话系统研究指明方向。

Abstract: Large language models (LLMs) are increasingly acting as dynamic conversational interfaces, supporting multi-turn interactions that mimic human-like conversation and facilitate complex tasks like coding. While datasets such as LMSYS-Chat-1M and WildChat capture real-world user-LLM conversations, few studies systematically explore the mechanisms of human-LLM collaboration in coding scenarios. What tortuous paths do users experience during the interaction process? How well do the LLMs follow instructions? Are users satisfied? In this paper, we conduct an empirical analysis on human-LLM coding collaboration using LMSYS-Chat-1M and WildChat datasets to explore the human-LLM collaboration mechanism, LLMs' instruction following ability, and human satisfaction. This study yields interesting findings: 1) Task types shape interaction patterns(linear, star and tree), with code quality optimization favoring linear patterns, design-driven tasks leaning toward tree structures, and queries preferring star patterns; 2) Bug fixing and code refactoring pose greater challenges to LLMs' instruction following, with non-compliance rates notably higher than in information querying; 3) Code quality optimization and requirements-driven development tasks show lower user satisfaction, whereas structured knowledge queries and algorithm designs yield higher levels. These insights offer recommendations for improving LLM interfaces and user satisfaction in coding collaborations, while highlighting avenues for future research on adaptive dialogue systems. We believe this work broadens understanding of human-LLM synergies and supports more effective AI-assisted development.

</details>


### [169] [Analyzing developer discussions on EU and US privacy legislation compliance in GitHub repositories](https://arxiv.org/abs/2512.10618)
*Georgia M. Kapitsaki,Maria Papoutsoglou,Christoph Treude,Ioanna Theophilou*

Main category: cs.SE

TL;DR: 通过挖掘分析GitHub仓库32,820个问题，对开源软件开发者讨论隐私立法合规问题进行研究，得出24个讨论类别并分6个集群，创建的分类法有多种应用价值。


<details>
  <summary>Details</summary>
Motivation: 隐私立法影响软件系统开发，缺乏开源软件开发者讨论隐私立法合规问题的实证证据。

Method: 挖掘分析GitHub仓库32,820个问题，部分自动分析识别法律用户权利和原则，手动分析1186个问题样本。

Result: 得出24个讨论类别，分6个集群，开发者主要关注特定用户权利，多数讨论集中在用户同意、权利功能、漏洞和cookie管理。

Conclusion: 创建的分类法有助于从业者、教育界和研究界应对隐私立法合规问题。

Abstract: Context: Privacy legislation has impacted the way software systems are developed, prompting practitioners to update their implementations. Specifically, the EU General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) have forced the community to focus on users' data privacy. Despite the vast amount of data on developer issues available in GitHub repositories, there is a lack of empirical evidence on the issues developers of Open Source Software discuss to comply with privacy legislation. Method: In this work, we examine such discussions by mining and analyzing 32,820 issues from GitHub repositories. We partially analyzed the dataset automatically to identify law user rights and principles indicated, and manually analyzed a sample of 1,186 issues based on the type of concern addressed. Results: We devised 24 discussion categories placed in six clusters: features/bugs, consent-related, documentation, data storing/sharing, adaptability, and general compliance. Our results show that developers mainly focus on specific user rights from the legislation (right to erasure, right to opt-out, right to access), addressing other rights less frequently, while most discussions concern user consent, user rights functionality, bugs and cookies management. Conclusion: The created taxonomy can help practitioners understand which issues are discussed for law compliance, so that they ensure they address them first in their systems. In addition, the educational community can reshape curricula to better educate future engineers on the privacy law concerns raised, and the research community can identify gaps and areas for improvement to support and accelerate data privacy law compliance.

</details>


### [170] [PACIFIC: a framework for generating benchmarks to check Precise Automatically Checked Instruction Following In Code](https://arxiv.org/abs/2512.10713)
*Itay Dreyfuss,Antonio Abu Nassar,Samuel Ackerman,Axel Ben David,Rami Katan,Orna Raz,Marcel Zalmanovici*

Main category: cs.SE

TL;DR: 提出PACIFIC框架自动生成基准测试评估大语言模型代码指令遵循和干运行能力，经实验验证该框架可有效区分模型能力且抗数据污染。


<details>
  <summary>Details</summary>
Motivation: 大语言模型代码助手需准确遵循用户指令，现有方法存在不足，需要有效评估模型代码指令遵循和干运行能力的方法。

Method: 提出PACIFIC框架自动生成基准测试，可控制难度，产生有明确预期输出的基准变体，通过简单输出比较评估。

Result: 生成不同难度基准测试评估多个先进大语言模型，PACIFIC能产生更具挑战性基准，有效区分模型指令遵循和干运行能力。

Conclusion: PACIFIC框架为评估大语言模型代码相关核心能力提供可扩展、抗污染的方法。

Abstract: Large Language Model (LLM)-based code assistants have emerged as a powerful application of generative AI, demonstrating impressive capabilities in code generation and comprehension. A key requirement for these systems is their ability to accurately follow user instructions. We present Precise Automatically Checked Instruction Following In Code (PACIFIC), a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs, while allowing control over benchmark difficulty. PACIFIC produces benchmark variants with clearly defined expected outputs, enabling straightforward and reliable evaluation through simple output comparisons. In contrast to existing approaches that often rely on tool usage or agentic behavior, our work isolates and evaluates the LLM's intrinsic ability to reason through code behavior step-by-step without execution (dry running) and to follow instructions. Furthermore, our framework mitigates training data contamination by facilitating effortless generation of novel benchmark variations. We validate our framework by generating a suite of benchmarks spanning a range of difficulty levels and evaluating multiple state-of-the-art LLMs. Our results demonstrate that PACIFIC can produce increasingly challenging benchmarks that effectively differentiate instruction-following and dry running capabilities, even among advanced models. Overall, our framework offers a scalable, contamination-resilient methodology for assessing core competencies of LLMs in code-related tasks.

</details>


### [171] [Zorya: Automated Concolic Execution of Single-Threaded Go Binaries](https://arxiv.org/abs/2512.10799)
*Karolina Gorna,Nicolas Iooss,Yannick Seurin,Rida Khatoun*

Main category: cs.SE

TL;DR: 现有符号执行工具难以检测Go二进制文件漏洞，本文基于Zorya改进，实现实用漏洞检测。


<details>
  <summary>Details</summary>
Motivation: Go在关键基础设施中应用增加，需要系统地检测Go二进制文件中的漏洞，但现有工具因运行时复杂性和可扩展性挑战难以应对。

Method: 基于Zorya，添加对具体未执行路径的漏洞检测和多层过滤机制，聚焦于与panic相关的路径。

Result: 对5个Go漏洞的评估显示，panic可达性门控在过滤33 - 70%的分支时加速1.8 - 3.9倍，Zorya能检测到所有panic，而现有工具最多检测到2个；在复杂程序中，函数模式分析比从main开始运行快约两个数量级。

Conclusion: 专门的混合执行可以在具有运行时安全检查的语言生态系统中实现实用的漏洞检测。

Abstract: Go's adoption in critical infrastructure intensifies the need for systematic vulnerability detection, yet existing symbolic execution tools struggle with Go binaries due to runtime complexity and scalability challenges. In this work, we build upon Zorya, a concolic execution framework that translates Go binaries to Ghidra's P-Code intermediate representation to address these challenges. We added the detection of bugs in concretely not taken paths and a multi-layer filtering mechanism to concentrate symbolic reasoning on panic-relevant paths. Evaluation on five Go vulnerabilities demonstrates that panic-reachability gating achieves 1.8-3.9x speedups when filtering 33-70% of branches, and that Zorya detects all panics while existing tools detect at most two. Function-mode analysis proved essential for complex programs, running roughly two orders of magnitude faster than starting from main. This work establishes that specialized concolic execution can achieve practical vulnerability detection in language ecosystems with runtime safety checks.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [172] [The Interplay of Statistics and Noisy Optimization: Learning Linear Predictors with Random Data Weights](https://arxiv.org/abs/2512.10188)
*Gabriel Clara,Yazan Mash'al*

Main category: stat.ML

TL;DR: 本文分析线性回归模型中随机加权数据点的梯度下降法，给出统一框架研究噪声影响，刻画隐式正则化，推导收敛界并研究平稳分布，讨论权重分布选择的影响。


<details>
  <summary>Details</summary>
Motivation: 为分析各种噪声对训练轨迹的影响提供统一框架，研究随机加权数据点梯度下降法在不同权重分布下的特性。

Method: 通过对线性回归模型中随机加权数据的梯度下降法分析，利用几何矩收缩等方法进行研究。

Result: 刻画了随机加权产生的隐式正则化，推导出一、二阶矩收敛的非渐近界，研究了噪声产生的平稳分布。

Conclusion: 特定权重分布选择会影响优化问题和估计器的统计性质，快速收敛的权重可能带来较差统计性能。

Abstract: We analyze gradient descent with randomly weighted data points in a linear regression model, under a generic weighting distribution. This includes various forms of stochastic gradient descent, importance sampling, but also extends to weighting distributions with arbitrary continuous values, thereby providing a unified framework to analyze the impact of various kinds of noise on the training trajectory. We characterize the implicit regularization induced through the random weighting, connect it with weighted linear regression, and derive non-asymptotic bounds for convergence in first and second moments. Leveraging geometric moment contraction, we also investigate the stationary distribution induced by the added noise. Based on these results, we discuss how specific choices of weighting distribution influence both the underlying optimization problem and statistical properties of the resulting estimator, as well as some examples for which weightings that lead to fast convergence cause bad statistical performance.

</details>


### [173] [LxCIM: a new rank-based binary classifier performance metric invariant to local exchange of classes](https://arxiv.org/abs/2512.10053)
*Tiago Brogueira,Mário A. T. Figueiredo*

Main category: stat.ML

TL;DR: 论文指出传统分类评估指标AUROC有局限，提出LxCIM指标，其有多种优势，可用于双变量因果发现问题。


<details>
  <summary>Details</summary>
Motivation: 现有二分类评估指标（如AUROC）对局部类别交换不变性问题不理想，需新评估指标。

Method: 提出LxCIM指标，并阐述其与AUROC、准确率、AUDRC的理论联系。

Result: 展示LxCIM在双变量因果发现问题上的适用性，能解决该领域现有指标的局限。

Conclusion: LxCIM是一种有潜力的分类评估指标，适用于局部类别交换不变性问题。

Abstract: Binary classification is one of the oldest, most prevalent, and studied problems in machine learning. However, the metrics used to evaluate model performance have received comparatively little attention. The area under the receiver operating characteristic curve (AUROC) has long been a standard choice for model comparison. Despite its advantages, AUROC is not always ideal, particularly for problems that are invariant to local exchange of classes (LxC), a new form of metric invariance introduced in this work. To address this limitation, we propose LxCIM (LxC-invariant metric), which is not only rank-based and invariant under local exchange of classes, but also intuitive, logically consistent, and always computable, while enabling more detailed analysis through the cumulative accuracy-decision rate curve. Moreover, LxCIM exhibits clear theoretical connections to AUROC, accuracy, and the area under the accuracy-decision rate curve (AUDRC). These relationships allow for multiple complementary interpretations: as a symmetric form of AUROC, a rank-based analogue of accuracy, or a more representative and more interpretable variant of AUDRC. Finally, we demonstrate the direct applicability of LxCIM to the bivariate causal discovery problem (which exhibits invariance to local exchange of classes) and show how it addresses the acknowledged limitations of existing metrics used in this field. All code and implementation details are publicly available at github.com/tiagobrogueira/Causal-Discovery-In-Exchangeable-Data.

</details>


### [174] [Error Analysis of Generalized Langevin Equations with Approximated Memory Kernels](https://arxiv.org/abs/2512.10256)
*Quanjun Lang,Jianfeng Lu*

Main category: stat.ML

TL;DR: 分析含记忆的随机动力系统预测误差，给出轨迹差异衰减率及相关界限，有理论推导和数值验证。


<details>
  <summary>Details</summary>
Motivation: 研究含记忆的随机动力系统（以广义朗之万方程为例）的预测误差。

Method: 结合同步噪声耦合与Volterra比较定理，在加权空间用预解式估计推导一阶模型矩和扰动界限，用次强制Lyapunov型距离证明二阶模型的收缩与稳定性。

Result: 在强凸势下，轨迹差异衰减率由记忆核的衰减决定，并受核估计误差定量约束；该框架适用于非平移不变核和白噪声强迫。

Conclusion: 理论上建立了更好的核估计与更优的轨迹预测之间的联系，数值例子验证了理论发现。

Abstract: We analyze prediction error in stochastic dynamical systems with memory, focusing on generalized Langevin equations (GLEs) formulated as stochastic Volterra equations. We establish that, under a strongly convex potential, trajectory discrepancies decay at a rate determined by the decay of the memory kernel and are quantitatively bounded by the estimation error of the kernel in a weighted norm. Our analysis integrates synchronized noise coupling with a Volterra comparison theorem, encompassing both subexponential and exponential kernel classes. For first-order models, we derive moment and perturbation bounds using resolvent estimates in weighted spaces. For second-order models with confining potentials, we prove contraction and stability under kernel perturbations using a hypocoercive Lyapunov-type distance. This framework accommodates non-translation-invariant kernels and white-noise forcing, explicitly linking improved kernel estimation to enhanced trajectory prediction. Numerical examples validate these theoretical findings.

</details>


### [175] [Diffusion differentiable resampling](https://arxiv.org/abs/2512.10401)
*Jennifer Rosina Andersson,Zheng Zhao*

Main category: stat.ML

TL;DR: 提出基于集成分数扩散模型的可微重采样方法，实验显示其在随机滤波和参数估计中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 研究序贯蒙特卡罗中的可微重采样问题。

Method: 基于集成分数扩散模型，提出新的信息重采样方法，该方法可立即进行逐路径微分。

Result: 证明扩散重采样方法能对重采样分布提供一致估计，实验表明其在随机滤波和参数估计中优于现有可微重采样方法。

Conclusion: 所提出的扩散重采样方法在相关应用中表现良好，是一种有效的可微重采样方法。

Abstract: This paper is concerned with differentiable resampling in the context of sequential Monte Carlo (e.g., particle filtering). We propose a new informative resampling method that is instantly pathwise differentiable, based on an ensemble score diffusion model. We prove that our diffusion resampling method provides a consistent estimate to the resampling distribution, and we show by experiments that it outperforms the state-of-the-art differentiable resampling methods when used for stochastic filtering and parameter estimation.

</details>


### [176] [Supervised Learning of Random Neural Architectures Structured by Latent Random Fields on Compact Boundaryless Multiply-Connected Manifolds](https://arxiv.org/abs/2512.10407)
*Christian Soize*

Main category: stat.ML

TL;DR: 本文提出用于神经系统监督学习的概率框架，介绍架构生成、学习方法并进行数学分析。


<details>
  <summary>Details</summary>
Motivation: 为建模复杂、不确定且输出非高斯的系统，建立几何感知、场驱动的生成过程的概念和数学框架。

Method: 通过潜在各向异性高斯随机场随机生成架构，用降阶参数化确定神经元位置，依据潜在场评估确定输入输出神经元，通过测地距离和局部场亲和力建立连接，用百分位扩散掩码稀疏化架构，用负对数似然损失和蒙特卡罗采样进行监督学习。

Result: 开展模型数学分析，确立适定性、可测性等基础属性及随机映射表达变异性的初步分析。

Conclusion: 该框架具有内部一致性，为几何驱动的随机学习理论奠定基础。

Abstract: This paper introduces a new probabilistic framework for supervised learning in neural systems. It is designed to model complex, uncertain systems whose random outputs are strongly non-Gaussian given deterministic inputs. The architecture itself is a random object stochastically generated by a latent anisotropic Gaussian random field defined on a compact, boundaryless, multiply-connected manifold. The goal is to establish a novel conceptual and mathematical framework in which neural architectures are realizations of a geometry-aware, field-driven generative process. Both the neural topology and synaptic weights emerge jointly from a latent random field. A reduced-order parameterization governs the spatial intensity of an inhomogeneous Poisson process on the manifold, from which neuron locations are sampled. Input and output neurons are identified via extremal evaluations of the latent field, while connectivity is established through geodesic proximity and local field affinity. Synaptic weights are conditionally sampled from the field realization, inducing stochastic output responses even for deterministic inputs. To ensure scalability, the architecture is sparsified via percentile-based diffusion masking, yielding geometry-aware sparse connectivity without ad hoc structural assumptions. Supervised learning is formulated as inference on the generative hyperparameters of the latent field, using a negative log-likelihood loss estimated through Monte Carlo sampling from single-observation-per-input datasets. The paper initiates a mathematical analysis of the model, establishing foundational properties such as well-posedness, measurability, and a preliminary analysis of the expressive variability of the induced stochastic mappings, which support its internal coherence and lay the groundwork for a broader theory of geometry-driven stochastic learning.

</details>


### [177] [Maximum Risk Minimization with Random Forests](https://arxiv.org/abs/2512.10445)
*Francesco Freni,Anya Fries,Linus Kühne,Markus Reichstein,Jonas Peters*

Main category: stat.ML

TL;DR: 本文在不同数据分布环境的回归设置下，基于MaxRM原则引入随机森林变体，提供高效算法、证明统计一致性和新的样本外保证，并在模拟和真实数据上评估。


<details>
  <summary>Details</summary>
Motivation: 在不同数据分布环境下，提升OOD泛化能力，设计更好泛化到测试环境的方法。

Method: 基于MaxRM原则引入随机森林变体，提供计算高效算法，使用三种风险评估。

Result: 证明主要方法的统计一致性，对以后悔为风险的MaxRM证明新的样本外保证，在模拟和真实数据上评估方法。

Conclusion: 提出的基于MaxRM原则的随机森林变体方法在不同环境回归问题中具有一定有效性和理论保证。

Abstract: We consider a regression setting where observations are collected in different environments modeled by different data distributions. The field of out-of-distribution (OOD) generalization aims to design methods that generalize better to test environments whose distributions differ from those observed during training. One line of such works has proposed to minimize the maximum risk across environments, a principle that we refer to as MaxRM (Maximum Risk Minimization). In this work, we introduce variants of random forests based on the principle of MaxRM. We provide computationally efficient algorithms and prove statistical consistency for our primary method. Our proposed method can be used with each of the following three risks: the mean squared error, the negative reward (which relates to the explained variance), and the regret (which quantifies the excess risk relative to the best predictor). For MaxRM with regret as the risk, we prove a novel out-of-sample guarantee over unseen test distributions. Finally, we evaluate the proposed methods on both simulated and real-world data.

</details>


### [178] [Flexible Deep Neural Networks for Partially Linear Survival Data](https://arxiv.org/abs/2512.10570)
*Asaf Ben Arie,Malka Gorfine*

Main category: stat.ML

TL;DR: 提出FLEXI - Haz框架用于生存数据分析，有理论保证，模拟和实际数据分析显示其效果好，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有DNN方法用于部分线性Cox模型依赖比例风险假设，需更灵活方法。

Method: 提出FLEXI - Haz框架，含参数线性和非参数DNN组件。

Result: 神经网络组件达最小最大最优收敛率，线性估计量有良好性质，模拟和实际数据分析准确估计线性效应。

Conclusion: FLEXI - Haz是基于比例风险现代方法的原则性和可解释替代方案。

Abstract: We propose a flexible deep neural network (DNN) framework for modeling survival data within a partially linear regression structure. The approach preserves interpretability through a parametric linear component for covariates of primary interest, while a nonparametric DNN component captures complex time-covariate interactions among nuisance variables. We refer to the method as FLEXI-Haz, a flexible hazard model with a partially linear structure. In contrast to existing DNN approaches for partially linear Cox models, FLEXI-Haz does not rely on the proportional hazards assumption. We establish theoretical guarantees: the neural network component attains minimax-optimal convergence rates based on composite Holder classes, and the linear estimator is root-n consistent, asymptotically normal, and semiparametrically efficient. Extensive simulations and real-data analyses demonstrate that FLEXI-Haz provides accurate estimation of the linear effect, offering a principled and interpretable alternative to modern methods based on proportional hazards. Code for implementing FLEXI-Haz, as well as scripts for reproducing data analyses and simulations, is available at: https://github.com/AsafBanana/FLEXI-Haz

</details>


### [179] [Physics-informed Polynomial Chaos Expansion with Enhanced Constrained Optimization Solver and D-optimal Sampling](https://arxiv.org/abs/2512.10873)
*Qitian Lu,Himanshu Sharma,Michael D. Shields,Lukáš Novák*

Main category: stat.ML

TL;DR: 本文提出对物理信息多项式混沌展开（PC²）框架的两项改进，通过采用SULM求解器和D - 最优采样策略，增强了PC²在高维不确定性量化任务中的综合能力。


<details>
  <summary>Details</summary>
Motivation: PC²在高维参数空间、数据有限或训练数据无代表性时性能和效率会下降，需对其进行改进。

Method: 采用数值高效的约束优化求解器SULM替代传统KKT求解器，利用D - 最优采样策略选择信息丰富的虚拟点，并将这些方法集成到PC²框架中。

Result: 通过数值示例验证，增强后的PC²比标准PC²有更好的综合能力。

Conclusion: 增强后的PC²适用于高维不确定性量化任务。

Abstract: Physics-informed polynomial chaos expansions (PC$^2$) provide an efficient physically constrained surrogate modeling framework by embedding governing equations and other physical constraints into the standard data-driven polynomial chaos expansions (PCE) and solving via the Karush-Kuhn-Tucker (KKT) conditions. This approach improves the physical interpretability of surrogate models while achieving high computational efficiency and accuracy. However, the performance and efficiency of PC$^2$ can still be degraded with high-dimensional parameter spaces, limited data availability, or unrepresentative training data. To address this problem, this study explores two complementary enhancements to the PC$^2$ framework. First, a numerically efficient constrained optimization solver, straightforward updating of Lagrange multipliers (SULM), is adopted as an alternative to the conventional KKT solver. The SULM method significantly reduces computational cost when solving physically constrained problems with high-dimensionality and derivative boundary conditions that require a large number of virtual points. Second, a D-optimal sampling strategy is utilized to select informative virtual points to improve the stability and achieve the balance of accuracy and efficiency of the PC$^2$. The proposed methods are integrated into the PC$^2$ framework and evaluated through numerical examples of representative physical systems governed by ordinary or partial differential equations. The results demonstrate that the enhanced PC$^2$ has better comprehensive capability than standard PC$^2$, and is well-suited for high-dimensional uncertainty quantification tasks.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [180] [A Vertically Integrated Framework for Templatized Chip Design](https://arxiv.org/abs/2512.10155)
*Jeongeun Kim,Christopher Torng*

Main category: cs.AR

TL;DR: 论文探讨从高层面向对象软件规范生成芯片，为入门学习者提供方法，减少开发者参与芯片创建所需专业知识。


<details>
  <summary>Details</summary>
Motivation: 开发者难以将定制硬件融入应用，而专用芯片有诸多益处，要为入门级学习者提供从软件规范生成芯片的方法，保持软件与芯片设计思路连贯。

Method: 将软件对象对应到芯片管芯区域，采用模块化构建策略，用基于序列的形式化类型系统检查硬件模块交互，研究硬件互连策略和布局技术。

Result: 能为新学习者保持从软件到芯片设计的思路连贯，实现实用的布局生成。

Conclusion: 这些方法可减少软件开发人员参与芯片创建所需的专业知识。

Abstract: Developers who primarily engage with software often struggle to incorporate custom hardware into their applications, even though specialized silicon can provide substantial benefits to machine learning and AI, as well as to the application domains that they enable. This work investigates how a chip can be generated from a high-level object-oriented software specification, targeting introductory-level chip design learners with only very light performance requirements, while maintaining mental continuity between the chip layout and the software source program. In our approach, each software object is represented as a corresponding region on the die, producing a one-to-one structural mapping that preserves these familiar abstractions throughout the design flow. To support this mapping, we employ a modular construction strategy in which vertically composed IP blocks implement the behavioral protocols expressed in software. A direct syntactic translation, however, cannot meet hardware-level efficiency or communication constraints. For this reason, we leverage formal type systems based on sequences that check whether interactions between hardware modules adhere to the communication patterns described in the software model. We further examine hardware interconnect strategies for composing many such modules and develop layout techniques suited to this object-aligned design style. Together, these contributions preserve mental continuity from software to chip design for new learners and enables practical layout generation, ultimately reducing the expertise required for software developers to participate in chip creation.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [181] [Equivalent Instances for Scheduling and Packing Problems](https://arxiv.org/abs/2512.10635)
*Klaus Jansen,Kai Kahler,Corinna Wambsganz*

Main category: cs.CC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Two instances $(I,k)$ and $(I',k')$ of a parameterized problem $P$ are equivalent if they have the same set of solutions (static equivalent) or if the set of solutions of $(I,k)$ can be constructed by the set of solutions for $(I',k')$ and some computable pre-solutions. If the algorithm constructing such a (static) equivalent instance whose size is polynomial bounded runs in fixed-parameter tractable (FPT) time, we say that there exists a (static) equivalent instance for this problem. In this paper we present (static) equivalent instances for Scheduling and Knapsack problems. We improve the bound for the $\ell_1$-norm of an equivalent vector given by Eisenbrand, Hunkenschröder, Klein, Koutecký, Levin, and Onn and show how this yields equivalent instances for integer linear programs (ILPs) and related problems. We obtain an $O(MN^2\log(NU))$ static equivalent instance for feasibility ILPs where $M$ is the number of constraints, $N$ is the number of variables and $U$ is an upper bound for the $\ell_\infty$-norm of the smallest feasible solution. With this, we get an $O(n^2\log(n))$ static equivalent instance for Knapsack where $n$ is the number of items. Moreover, we give an $O(M^2N\log(NMΔ))$ kernel for feasibility ILPs where $Δ$ is an upper bound for the $\ell_\infty$-norm of the given constraint matrix.
  Using balancing results by Knop et al., the ConfILP and a proximity result by Eisenbrand and Weismantel we give an $O(d^2\log(p_{\max}))$ equivalent instance for LoadBalancing, a generalization of scheduling problems. Here $d$ is the number of different processing times and $p_{\max}$ is the largest processing time.

</details>


<div id='hep-ex'></div>

# hep-ex [[Back]](#toc)

### [182] [Deep sets and event-level maximum-likelihood estimation for fast pile-up jet rejection in ATLAS](https://arxiv.org/abs/2512.10819)
*Mohammed Aboelela*

Main category: hep-ex

TL;DR: 为应对LHC高亮度下多喷注事件率增加问题，引入基于Deep Sets架构的DIPz模型回归喷注沿束线的起源位置，构建MLPL判别器进行事件选择，实现多喷注末态堆积排除。


<details>
  <summary>Details</summary>
Motivation: LHC亮度增加，多喷注事件率升高，需有效按束线起源对喷注分组，尤其在触发层面。

Method: 引入基于Deep Sets架构的DIPz模型，以与喷注关联的带电粒子轨迹为输入回归喷注起源位置，构建MLPL判别器并进行切割优化。

Result: 提出了一种鲁棒且计算高效的多喷注末态堆积排除方法。

Conclusion: 该方法适用于ATLAS高级触发的实时事件选择。

Abstract: Multiple proton-proton collisions (pile-up) occur at every bunch crossing at the LHC, with the mean number of interactions expected to reach 80 during Run 3 and up to 200 at the High-Luminosity LHC. As a direct consequence, events with multijet signatures will occur at increasingly high rates. To cope with the increased luminosity, being able to efficiently group jets according to their origin along the beamline is crucial, particularly at the trigger level. In this work, a novel uncertainty-aware jet regression model based on a Deep Sets architecture is introduced, DIPz, to regress on a jet origin position along the beamline. The inputs to the DIPz algorithm are the charged particle tracks associated to each jet. An event-level discriminant, the Maximum Log Product of Likelihoods (MLPL), is constructed by combining the DIPz per-jet predictions. MLPL is cut-optimized to select events compatible with targeted multi-jet signature selection. This combined approach provides a robust and computationally efficient method for pile-up rejection in multi-jet final states, applicable to real-time event selections at the ATLAS High Level Trigger.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [183] [Topology Identification and Inference over Graphs](https://arxiv.org/abs/2512.10183)
*Gonzalo Mateos,Yanning Shen,Georgios B. Giannakis,Ananthram Swami*

Main category: eess.SP

TL;DR: 本文概述图拓扑识别和多维关系数据统计推断方法，涵盖无向和有向关系处理，该方法支持多种学习算法，适合多维网络数据。


<details>
  <summary>Details</summary>
Motivation: 图上过程的拓扑识别和推断在多领域应用广泛，需有效方法处理。

Method: 介绍无向链接采用从相关度量到协方差选择等方法，有向关系通过核方法，还利用结构方程和向量自回归进行泛化。

Result: 该方法支持批量和在线学习算法，适用于张量公式和分解，可利用高阶统计信息。

Conclusion: 该方法对图拓扑识别和多维关系数据统计推断有效，能处理多种复杂情况。

Abstract: Topology identification and inference of processes evolving over graphs arise in timely applications involving brain, transportation, financial, power, as well as social and information networks. This chapter provides an overview of graph topology identification and statistical inference methods for multidimensional relational data. Approaches for undirected links connecting graph nodes are outlined, going all the way from correlation metrics to covariance selection, and revealing ties with smooth signal priors. To account for directional (possibly causal) relations among nodal variables and address the limitations of linear time-invariant models in handling dynamic as well as nonlinear dependencies, a principled framework is surveyed to capture these complexities through judiciously selected kernels from a prescribed dictionary. Generalizations are also described via structural equations and vector autoregressions that can exploit attributes such as low rank, sparsity, acyclicity, and smoothness to model dynamic processes over possibly time-evolving topologies. It is argued that this approach supports both batch and online learning algorithms with convergence rate guarantees, is amenable to tensor (that is, multi-way array) formulations as well as decompositions that are well-suited for multidimensional network data, and can seamlessly leverage high-order statistical information.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [184] [Offscript: Automated Auditing of Instruction Adherence in LLMs](https://arxiv.org/abs/2512.10172)
*Nicholas Clark,Ryan Bai,Tanu Mitra*

Main category: cs.HC

TL;DR: 提出自动化审计工具Offscript，分析Reddit上自定义指令，能检测LLM潜在指令遵循失败情况，研究表明自动化审计可用于评估信息搜索相关行为指令的合规性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏评估大语言模型是否有效遵循自定义指令及行为提示的机制。

Method: 开发自动化审计工具Offscript来识别大语言模型中指令遵循失败情况，并通过分析Reddit上的自定义指令进行试点研究。

Result: 在分析中，Offscript检测到86.4%的对话存在潜在偏离指令行为，其中22.2%经人工审查确认为重大违规。

Conclusion: 自动化审计是评估信息搜索行为指令合规性的可行方法。

Abstract: Large Language Models (LLMs) and generative search systems are increasingly used for information seeking by diverse populations with varying preferences for knowledge sourcing and presentation. While users can customize LLM behavior through custom instructions and behavioral prompts, no mechanism exists to evaluate whether these instructions are being followed effectively. We present Offscript, an automated auditing tool that efficiently identifies potential instruction following failures in LLMs. In a pilot study analyzing custom instructions sourced from Reddit, Offscript detected potential deviations from instructed behavior in 86.4% of conversations, 22.2% of which were confirmed as material violations through human review. Our findings suggest that automated auditing serves as a viable approach for evaluating compliance to behavioral instructions related to information seeking.

</details>


### [185] [InFerActive: Towards Scalable Human Evaluation of Large Language Models through Interactive Inference](https://arxiv.org/abs/2512.10234)
*Junhyeong Hwangbo,Soohyun Lee,Minsoo Cheong,Hyeon Jeon,Jinwook Seo*

Main category: cs.HC

TL;DR: 提出交互式推理系统InFerActive用于大语言模型可扩展的人工评估，提升评估效率。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型人工评估范式扩展性差，传统树可视化无法处理现代采样方法生成的指数级大树。

Method: 提出InFerActive系统，通过基于概率的过滤和评估功能实现按需探索，用自适应可视化技术缩小计算标记与可读文本的语义差距。

Result: 技术评估和用户研究表明InFerActive显著提高评估效率，能更全面评估模型行为；专家案例研究证明其实用性。

Conclusion: InFerActive有潜力改变大语言模型评估工作流程。

Abstract: Human evaluation remains the gold standard for evaluating outputs of Large Language Models (LLMs). The current evaluation paradigm reviews numerous individual responses, leading to significant scalability challenges. LLM outputs can be more efficiently represented as a tree structure, reflecting their autoregressive generation process and stochastic token selection. However, conventional tree visualization cannot scale to the exponentially large trees generated by modern sampling methods of LLMs. To address this problem, we present InFerActive, an interactive inference system for scalable human evaluation. InFerActive enables on-demand exploration through probability-based filtering and evaluation features, while bridging the semantic gap between computational tokens and human-readable text through adaptive visualization techniques. Through a technical evaluation and user study (N=12), we demonstrate that InFerActive significantly improves evaluation efficiency and enables more comprehensive assessment of model behavior. We further conduct expert case studies that demonstrate InFerActive's practical applicability and potential for transforming LLM evaluation workflows.

</details>


<div id='hep-th'></div>

# hep-th [[Back]](#toc)

### [186] [Hermitian Yang--Mills connections on general vector bundles: geometry and physical Yukawa couplings](https://arxiv.org/abs/2512.10907)
*Challenger Mishra,Justin Tan*

Main category: hep-th

TL;DR: 通过基于几何机器学习的交替优化程序计算全纯向量丛上的厄米杨 - 米尔斯方程解，并用于计算一类杂化弦紧致化中的汤川耦合。


<details>
  <summary>Details</summary>
Motivation: 计算全纯向量丛上的厄米杨 - 米尔斯方程解，以及杂化弦紧致化中的物理归一化汤川耦合。

Method: 采用基于几何机器学习的交替优化程序，仅需能列举给定丛的整体截面基。

Result: 能够计算一类杂化弦紧致化中的物理归一化汤川耦合，并对含非阿贝尔结构群规范丛的杂化紧致化进行了完整计算。

Conclusion: 所提方法具有一般性，可用于相关计算。

Abstract: We compute solutions to the Hermitian Yang-Mills equations on holomorphic vector bundles $V$ via an alternating optimisation procedure founded on geometric machine learning. The proposed method is fully general with respect to the rank and structure group of $V$, requiring only the ability to enumerate a basis of global sections for a given bundle. This enables us to compute the physically normalised Yukawa couplings in a broad class of heterotic string compactifications. Using this method, we carry out this computation in full for a heterotic compactification incorporating a gauge bundle with non-Abelian structure group.

</details>


<div id='physics.ed-ph'></div>

# physics.ed-ph [[Back]](#toc)

### [187] [Developing and Evaluating a Large Language Model-Based Automated Feedback System Grounded in Evidence-Centered Design for Supporting Physics Problem Solving](https://arxiv.org/abs/2512.10785)
*Holger Maus,Paul Tschisgale,Fabian Kieser,Stefan Petersen,Peter Wulff*

Main category: physics.ed-ph

TL;DR: 研究基于证据中心设计（ECD）的大语言模型（LLM）物理问题反馈系统，在德国物理奥林匹克竞赛中评估其性能，反馈有用但有20%存在事实错误，探讨相关风险与改进方向。


<details>
  <summary>Details</summary>
Motivation: 生成式AI为个性化和自适应学习带来新机遇，但LLM在需要高级领域专业知识的任务（如物理问题解决）中提供高质量反馈仍有挑战。

Method: 设计基于ECD的LLM物理问题反馈系统，并在德国物理奥林匹克竞赛中评估，让参与者评价反馈的有用性和准确性。

Result: 反馈普遍被认为有用且高度准确，但20%的反馈存在事实错误且常未被学生察觉。

Conclusion: 讨论了盲目依赖LLM反馈系统的风险，并指出未来生成更自适应和可靠反馈的潜在方向。

Abstract: Generative AI offers new opportunities for individualized and adaptive learning, particularly through large language model (LLM)-based feedback systems. While LLMs can produce effective feedback for relatively straightforward conceptual tasks, delivering high-quality feedback for tasks that require advanced domain expertise, such as physics problem solving, remains a substantial challenge. This study presents the design of an LLM-based feedback system for physics problem solving grounded in evidence-centered design (ECD) and evaluates its performance within the German Physics Olympiad. Participants assessed the usefulness and accuracy of the generated feedback, which was generally perceived as useful and highly accurate. However, an in-depth analysis revealed that the feedback contained factual errors in 20% of cases; errors that often went unnoticed by the students. We discuss the risks associated with uncritical reliance on LLM-based feedback systems and outline potential directions for generating more adaptive and reliable LLM-based feedback in the future.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [188] [Classifying Metamorphic versus Single-Fold Proteins with Statistical Learning and AlphaFold2](https://arxiv.org/abs/2512.10066)
*Yongkai Chen,Samuel WK Wong,SC Kou*

Main category: stat.AP

TL;DR: 本文利用AlphaFold2开发新分类框架，识别潜在变构蛋白，结合AI与统计学习加深对变构蛋白功能理解。


<details>
  <summary>Details</summary>
Motivation: AlphaFold2难以对变构蛋白的构象多样性建模，对给定蛋白质进行变构或单折叠分类存在挑战。

Method: 通过多序列比对采样方法重新利用AlphaFold2生成构象集合，提取特征，用随机森林分类器在基准数据集上训练。

Result: 随机森林分类器交叉验证平均AUC达0.869，对PDB中600个蛋白分析，识别出潜在变构蛋白候选。

Conclusion: 结合AI蛋白结构预测与统计学习，为发现变构蛋白提供新方法，加深对其分子功能作用的理解。

Abstract: The remarkable success of AlphaFold2 in providing accurate atomic-level prediction of protein structures from their amino acid sequence has transformed approaches to the protein folding problem. However, its core paradigm of mapping one sequence to one structure may only be appropriate for single-fold proteins with one stable conformation. Metamorphic proteins, which can adopt multiple distinct conformations, have conformational diversity that cannot be adequately modeled by AlphaFold2. Hence, classifying whether a given protein is metamorphic or single-fold remains a critical challenge for both laboratory experiments and computational methods. To address this challenge, we developed a novel classification framework by re-purposing AlphaFold2 to generate conformational ensembles via a multiple sequence alignment sampling method. From these ensembles, we extract a comprehensive set of features characterizing the conformational ensemble's modality and structural dispersion. A random forest classifier trained on a carefully curated benchmark dataset of known metamorphic and single-fold proteins achieves a mean AUC of 0.869 with cross-validation, demonstrating the effectiveness of our integrated approach. Furthermore, by applying our classifier to 600 randomly sampled proteins from the Protein Data Bank, we identified several potential metamorphic protein candidates -- including the 40S ribosomal protein S30, whose conformational change is crucial for its secondary function in antimicrobial defense. By combining AI-driven protein structure prediction with statistical learning, our work provides a powerful new approach for discovering metamorphic proteins and deepens our understanding of their role in their molecular function.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [189] [Distributionally Robust Markov Games with Average Reward](https://arxiv.org/abs/2508.03136)
*Zachary Roch,Yue Wang*

Main category: cs.MA

TL;DR: 研究平均奖励准则下的分布鲁棒马尔可夫博弈，建立理论基础并提出算法。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体在环境不确定和对手行动未知情况下的长期决策问题，确保满意性能。

Method: 建立最佳响应策略与单智能体问题最优策略的联系；推导最优策略与鲁棒贝尔曼方程解的对应关系；构造集值映射；提出鲁棒纳什迭代算法和基于时间差分的算法。

Result: 证明平稳纳什均衡的存在性；给出算法收敛性保证；表明平均奖励鲁棒纳什均衡可由折扣均衡近似。

Conclusion: 为复杂、不确定和长期运行的多玩家环境决策提供了全面的理论和算法基础。

Abstract: We study distributionally robust Markov games (DR-MGs) with the average-reward criterion, a framework for multi-agent decision-making under uncertainty over extended horizons. In average reward DR-MGs, agents aim to maximize their worst-case infinite-horizon average reward, to ensure satisfactory performance under environment uncertainties and opponent actions. We first establish a connection between the best-response policies and the optimal policies for the induced single-agent problems. Under a standard irreducible assumption, we derive a correspondence between the optimal policies and the solutions of the robust Bellman equation, and derive the existence of stationary Nash Equilibrium (NE) based on these results. We further study DR-MGs under the weakly communicating setting, where we construct a set-valued map and show its value is a subset of the best-response policies, convex and upper hemi-continuous, and derive the existence of NE. We then explore algorithmic solutions, by first proposing a Robust Nash-Iteration algorithm and providing convergence guarantees under some additional assumptions and a NE computing oracle. We further develop a temporal-difference based algorithm for DR-MGs, and provide convergence guarantees without any additional oracle or assumptions. Finally, we connect average-reward robust NE to discounted ones, showing that the average reward robust NE can be approximated by the discounted ones under a large discount factor. Our studies provide a comprehensive theoretical and algorithmic foundation for decision-making in complex, uncertain, and long-running multi-player environments.

</details>


### [190] [Norm-Governed Multi-Agent Decision-Making in Simulator-Coupled Environments:The Reinsurance Constrained Multi-Agent Simulation Process (R-CMASP)](https://arxiv.org/abs/2512.09939)
*Stella C. Dong*

Main category: cs.MA

TL;DR: 文章指出再保险决策需多智能体模型，提出R - CMASP模型，实验表明其优于传统方法，说明规范治理、模拟器耦合的多智能体系统适合模拟决策环境。


<details>
  <summary>Details</summary>
Motivation: 再保险决策具有多智能体模型所需核心结构特性，而确定性工作流自动化无法满足其要求。

Method: 提出R - CMASP模型，在合成环境中使用基于大语言模型的智能体进行实验。

Result: 多智能体协调比确定性自动化或整体大语言模型基线表现更优，能减少定价差异、提高资本效率和条款解释准确性，嵌入规范约束和结构化通信可增强均衡稳定性。

Conclusion: 受监管、模拟器驱动的决策环境最适合用规范治理、模拟器耦合的多智能体系统建模。

Abstract: Reinsurance decision-making exhibits the core structural properties that motivate multi-agent models: distributed and asymmetric information, partial observability, heterogeneous epistemic responsibilities, simulator-driven environment dynamics, and binding prudential and regulatory constraints. Deterministic workflow automation cannot meet these requirements, as it lacks the epistemic flexibility, cooperative coordination mechanisms, and norm-sensitive behaviour required for institutional risk-transfer.
  We propose the Reinsurance Constrained Multi-Agent Simulation Process (R-CMASP), a formal model that extends stochastic games and Dec-POMDPs by adding three missing elements: (i) simulator-coupled transition dynamics grounded in catastrophe, capital, and portfolio engines; (ii) role-specialized agents with structured observability, belief updates, and typed communication; and (iii) a normative feasibility layer encoding solvency, regulatory, and organizational rules as admissibility constraints on joint actions.
  Using LLM-based agents with tool access and typed message protocols, we show in a domain-calibrated synthetic environment that governed multi-agent coordination yields more stable, coherent, and norm-adherent behaviour than deterministic automation or monolithic LLM baselines--reducing pricing variance, improving capital efficiency, and increasing clause-interpretation accuracy. Embedding prudential norms as admissibility constraints and structuring communication into typed acts measurably enhances equilibrium stability.
  Overall, the results suggest that regulated, simulator-driven decision environments are most naturally modelled as norm-governed, simulator-coupled multi-agent systems.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [191] [Optimal learning of quantum channels in diamond distance](https://arxiv.org/abs/2512.10214)
*Antonio Anna Mele,Lennart Bittel*

Main category: quant-ph

TL;DR: 研究量子过程层析中估计未知量子通道所需通道使用次数，给出O(d^4/ε^2)的使用次数，证明其基本最优，还推广到特定输入输出维度和Kraus秩的通道，得到相关最优策略。


<details>
  <summary>Details</summary>
Motivation: 确定在钻石距离下学习未知量子通道所需的最优通道使用次数，这是量子信息理论中的一个长期未决问题。

Method: 非自适应地使用通道制备Choi态副本，并行纯化，对纯化态进行样本最优纯态层析，并通过半定规划表征直接在钻石距离下分析估计器。

Result: 得出在钻石距离下估计作用于d维系统的量子通道至精度ε所需O(d^4/ε^2)次通道使用，该缩放基本最优；推广到特定输入输出维度和Kraus秩的通道，O(d_in d_out k/ε^2)次通道使用足够；获得二元POVM和等距算子范数学习的首次基本最优策略，恢复固定秩态的最优迹距离层析。

Conclusion: 解决了量子通道在钻石距离下的样本复杂度问题。

Abstract: Quantum process tomography, the task of estimating an unknown quantum channel, is a central problem in quantum information theory and a key primitive for characterising noisy quantum devices. A long-standing open question is to determine the optimal number of uses of an unknown channel required to learn it in diamond distance, the standard measure of worst-case distinguishability between quantum processes. Here we show that a quantum channel acting on a $d$-dimensional system can be estimated to accuracy $\varepsilon$ in diamond distance using $O(d^4/\varepsilon^2)$ channel uses. This scaling is essentially optimal, as it matches lower bounds up to logarithmic factors. Our analysis extends to channels with input and output dimensions $d_{\mathrm{in}}$ and $d_{\mathrm{out}}$ and Kraus rank at most $k$, for which $O(d_{\mathrm{in}} d_{\mathrm{out}} k/\varepsilon^2)$ channel uses suffice, interpolating between unitary and fully generic channels. As by-products, we obtain, to the best of our knowledge, the first essentially optimal strategies for operator-norm learning of binary POVMs and isometries, and we recover optimal trace-distance tomography for fixed-rank states. Our approach consists of using the channel only non-adaptively to prepare copies of the Choi state, purify them in parallel, perform sample-optimal pure-state tomography on the purifications, and analyse the resulting estimator directly in diamond distance via its semidefinite-program characterisation. While the sample complexity of state tomography in trace distance is by now well understood, our results finally settle the corresponding problem for quantum channels in diamond distance.

</details>


### [192] [Topology-Guided Quantum GANs for Constrained Graph Generation](https://arxiv.org/abs/2512.10582)
*Tobias Rohe,Markus Baumann,Michael Poppel,Gerhard Stenzel,Maximilian Zorn,Claudia Linnhoff-Popien*

Main category: quant-ph

TL;DR: 研究表明将特定任务归纳偏置融入量子电路设计，能提升混合量子生成对抗网络在生成几何约束图任务的表现，凸显结构化、任务感知的量子电路拓扑的价值。


<details>
  <summary>Details</summary>
Motivation: 当前量子计算文献多依赖通用架构，未充分探索特定领域的量子电路拓扑，作者希望研究特定任务的归纳偏置对量子电路设计的影响。

Method: 在量子电路设计中纳入任务特定的归纳偏置（几何先验），评估多种纠缠拓扑和损失函数设计对统计保真度和几何约束合规性的影响。

Result: 使电路拓扑与问题结构匹配可带来显著益处，三角拓扑的量子生成对抗网络在量子模型中几何有效性最高，与经典生成对抗网络性能相当；特定架构选择影响几何一致性和分布准确性的权衡。

Conclusion: 强调了结构化、任务感知的量子电路拓扑的重要性和价值。

Abstract: Quantum computing (QC) promises theoretical advantages, benefiting computational problems that would not be efficiently classically simulatable. However, much of this theoretical speedup depends on the quantum circuit design solving the problem. We argue that QC literature has yet to explore more domain specific ansatz-topologies, instead of relying on generic, one-size-fits-all architectures. In this work, we show that incorporating task-specific inductive biases -- specifically geometric priors -- into quantum circuit design can enhance the performance of hybrid Quantum Generative Adversarial Networks (QuGANs) on the task of generating geometrically constrained K4 graphs. We evaluate a portfolio of entanglement topologies and loss-function designs to assess their impact on both statistical fidelity and compliance with geometric constraints, including the Triangle and Ptolemaic inequalities. Our results show that aligning circuit topology with the underlying problem structure yields substantial benefits: the Triangle-topology QuGAN achieves the highest geometric validity among quantum models and matches the performance of classical Generative Adversarial Networks (GAN). Additionally, we showcase how specific architectural choices, such as entangling gate types, variance regularization and output-scaling govern the trade-off between geometric consistency and distributional accuracy, thus emphasizing the value of structured, task-aware quantum ansatz-topologies.

</details>


### [193] [Quantum Approaches to Urban Logistics: From Core QAOA to Clustered Scalability](https://arxiv.org/abs/2512.10813)
*F. Picariello,G. Turati,R. Antonelli,I. Bailo,S. Bonura,G. Ciarfaglia,S. Cipolla,P. Cremonesi,M. Ferrari Dacrema,M. Gabusi,I. Gentile,V. Morreale,A. Noto*

Main category: quant-ph

TL;DR: 研究用QAOA解决带现实约束的TSP问题，提出Cl - QAOA提高可扩展性并评估QAOA性能。


<details>
  <summary>Details</summary>
Motivation: 传统算法在大规模TSP实例中难在合理时间内产生高质量解，探索QAOA解决带现实约束TSP的潜力。

Method: 采用基于QUBO的TSP公式，整合现实物流约束；在模拟环境用HPC评估QAOA；提出结合经典机器学习与QAOA的Cl - QAOA分解大问题。

Result: 全面评估了QAOA在解决带约束TSP场景中的优缺点。

Conclusion: 推动了量子优化，为未来大规模应用奠定基础。

Abstract: The Traveling Salesman Problem (TSP) is a fundamental challenge in combinatorial optimization, widely applied in logistics and transportation. As the size of TSP instances grows, traditional algorithms often struggle to produce high-quality solutions within reasonable timeframes. This study investigates the potential of the Quantum Approximate Optimization Algorithm (QAOA), a hybrid quantum-classical method, to solve TSP under realistic constraints. We adopt a QUBO-based formulation of TSP that integrates real-world logistical constraints reflecting operational conditions, such as vehicle capacity, road accessibility, and time windows, while ensuring compatibility with the limitations of current quantum hardware. Our experiments are conducted in a simulated environment using high-performance computing (HPC) resources to assess QAOA's performance across different problem sizes and quantum circuit depths. In order to improve scalability, we propose clustering QAOA (Cl-QAOA), a hybrid approach combining classical machine learning with QAOA. This method decomposes large TSP instances into smaller sub-problems, making quantum optimization feasible even on devices with a limited number of qubits. The results offer a comprehensive evaluation of QAOA's strengths and limitations in solving constrained TSP scenarios. This study advances quantum optimization and lays groundwork for future large-scale applications.

</details>


### [194] [Noisy Quantum Learning Theory](https://arxiv.org/abs/2512.10929)
*Jordan Cotler,Weiyuan Gong,Ishaan Kannan*

Main category: quant-ph

TL;DR: 提出从有噪声量子实验中学习的框架，研究噪声对量子学习优势的影响，指出多数指数量子学习优势对噪声脆弱，未来需结合抗噪物理性质与算法技术。


<details>
  <summary>Details</summary>
Motivation: 研究有噪声容错设备访问未表征系统时，从有噪声量子实验中学习的情况，探索噪声对量子学习优势的影响。

Method: 以复杂度类NBQP为起点，研究自然预言问题、具体有噪声学习任务，分析有噪声Pauli阴影断层扫描。

Result: 噪声可消除理想无噪声学习者的指数量子学习优势，保留NISQ与容错设备间超多项式差距；纯度测试中指数优势在局部去极化噪声下崩溃；抗噪结构可恢复量子学习优势；得出有噪声Pauli阴影断层扫描样本复杂度的下界。

Conclusion: 多数指数量子学习优势对噪声脆弱，未来实验实现量子优势需理解抗噪物理性质与算法技术的结合。

Abstract: We develop a framework for learning from noisy quantum experiments, focusing on fault-tolerant devices accessing uncharacterized systems through noisy couplings. Our starting point is the complexity class $\textsf{NBQP}$ ("noisy BQP"), modeling noisy fault-tolerant quantum computers that cannot, in general, error-correct the oracle systems they query. Using this class, we show that for natural oracle problems, noise can eliminate exponential quantum learning advantages of ideal noiseless learners while preserving a superpolynomial gap between NISQ and fault-tolerant devices. Beyond oracle separations, we study concrete noisy learning tasks. For purity testing, the exponential two-copy advantage collapses under a single application of local depolarizing noise. Nevertheless, we identify a setting motivated by AdS/CFT in which noise-resilient structure restores a quantum learning advantage in a noisy regime. We then analyze noisy Pauli shadow tomography, deriving lower bounds that characterize how instance size, quantum memory, and noise control sample complexity, and design algorithms with parametrically similar scalings. Together, our results show that the Bell-basis and SWAP-test primitives underlying most exponential quantum learning advantages are fundamentally fragile to noise unless the experimental system has latent noise-robust structure. Thus, realizing meaningful quantum advantages in future experiments will require understanding how noise-robust physical properties interface with available algorithmic techniques.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [195] [The Localization Method for High-Dimensional Inequalities](https://arxiv.org/abs/2512.10848)
*Yunbum Kook,Santosh S. Vempala*

Main category: math.PR

TL;DR: 本文介绍高维不等式证明的定位方法及其随机扩展，该方法应用广泛，核心是将高维不等式转化为结构化实例。


<details>
  <summary>Details</summary>
Motivation: 介绍高维不等式证明方法及其应用。

Method: 回顾Lovász和Simonovits（1993）开创的定位方法及Eldan（2012）的随机扩展。

Result: 该方法在等周不等式、优化、测度集中和马尔可夫链混合率界定等多种场景有应用。

Conclusion: 该方法可将高维不等式实例转化为高度结构化实例，常为一维。

Abstract: We survey the localization method for proving inequalities in high dimension, pioneered by Lovász and Simonovits (1993), and its stochastic extension developed by Eldan (2012). The method has found applications in a surprising wide variety of settings, ranging from its original motivation in isoperimetric inequalities to optimization, concentration of measure, and bounding the mixing rate of Markov chains. At heart, the method converts a given instance of an inequality (for a set or distribution in high dimension) into a highly structured instance, often just one-dimensional.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [196] [VocSim: A Training-free Benchmark for Zero-shot Content Identity in Single-source Audio](https://arxiv.org/abs/2512.10120)
*Maris Basha,Anja Zai,Sabine Stoll,Richard Hahnloser*

Main category: cs.SD

TL;DR: 提出无训练基准VocSim评估通用音频表示，发现简单管道有零样本性能但存在泛化差距，顶级嵌入有外部验证效果并公开相关资源。


<details>
  <summary>Details</summary>
Motivation: 现有监督分类基准通过参数更新衡量适应性，需无训练基准探测冻结嵌入的内在几何对齐。

Method: 引入VocSim基准，聚合19个语料库的单源音频片段，用Precision@k和GSR评估嵌入，报告GSR相对经验排列基线的提升。

Result: 简单管道有强零样本性能，但在低资源语音上局部检索下降，绝对几何结构崩溃；顶级嵌入有外部验证效果。

Conclusion: 所测内在几何质量可代表未列出下游应用的效用，发布资源标准化音频几何评估。

Abstract: General-purpose audio representations aim to map acoustically variable instances of the same event to nearby points, resolving content identity in a zero-shot setting. Unlike supervised classification benchmarks that measure adaptability via parameter updates, we introduce VocSim, a training-free benchmark probing the intrinsic geometric alignment of frozen embeddings. VocSim aggregates 125k single-source clips from 19 corpora spanning human speech, animal vocalizations, and environmental sounds. By restricting to single-source audio, we isolate content representation from the confound of source separation. We evaluate embeddings using Precision@k for local purity and the Global Separation Rate (GSR) for point-wise class separation. To calibrate GSR, we report lift over an empirical permutation baseline. Across diverse foundation models, a simple pipeline, frozen Whisper encoder features, time-frequency pooling, and label-free PCA, yields strong zero-shot performance. However, VocSim also uncovers a consistent generalization gap. On blind, low-resource speech, local retrieval drops sharply. While performance remains statistically distinguishable from chance, the absolute geometric structure collapses, indicating a failure to generalize to unseen phonotactics. As external validation, our top embeddings predict avian perceptual similarity, improve bioacoustic classification, and achieve state-of-the-art results on the HEAR benchmark. We posit that the intrinsic geometric quality measured here proxies utility in unlisted downstream applications. We release data, code, and a public leaderboard to standardize the evaluation of intrinsic audio geometry.

</details>


### [197] [Semantic-Aware Confidence Calibration for Automated Audio Captioning](https://arxiv.org/abs/2512.10170)
*Lucas Dunker,Sai Akshay Menta,Snigdha Mohana Addepalli,Venkata Krishna Rayalu Garapati*

Main category: cs.SD

TL;DR: 自动化音频字幕模型常产生过度自信预测，本文提出框架，将置信度预测融入音频字幕并以语义相似度定义正确性，实验表明该方法校准效果和字幕质量均有提升。


<details>
  <summary>Details</summary>
Motivation: 自动化音频字幕模型常产生过度自信预测，可靠性不足，原因是评估指标无法捕捉语义正确性和缺乏校准的置信度估计。

Method: 将置信度预测集成到音频字幕中，用学习的置信度预测头增强基于Whisper的模型，用CLAP音频 - 文本嵌入和句子转换器相似度定义语义正确性，计算预期校准误差。

Result: 在Clotho v2上实验表明，基于语义评估的置信度引导束搜索比贪心解码基线校准效果大幅提升，同时字幕质量也提高。

Conclusion: 语义相似度比传统n - gram指标为音频字幕的置信度校准提供了更有意义的基础。

Abstract: Automated audio captioning models frequently produce overconfident predictions regardless of semantic accuracy, limiting their reliability in deployment. This deficiency stems from two factors: evaluation metrics based on n-gram overlap that fail to capture semantic correctness, and the absence of calibrated confidence estimation. We present a framework that addresses both limitations by integrating confidence prediction into audio captioning and redefining correctness through semantic similarity. Our approach augments a Whisper-based audio captioning model with a learned confidence prediction head that estimates uncertainty from decoder hidden states. We employ CLAP audio-text embeddings and sentence transformer similarities (FENSE) to define semantic correctness, enabling Expected Calibration Error (ECE) computation that reflects true caption quality rather than surface-level text overlap. Experiments on Clotho v2 demonstrate that confidence-guided beam search with semantic evaluation achieves dramatically improved calibration (CLAP-based ECE of 0.071) compared to greedy decoding baselines (ECE of 0.488), while simultaneously improving caption quality across standard metrics. Our results establish that semantic similarity provides a more meaningful foundation for confidence calibration in audio captioning than traditional n-gram metrics.

</details>


### [198] [Neural personal sound zones with flexible bright zone control](https://arxiv.org/abs/2512.10375)
*Wenye Zhu,Jun Tang,Xiaofei Li*

Main category: cs.SD

TL;DR: 本文提出用于个人声区（PSZ）再现的3D卷积神经网络，能处理灵活控制点网格上的不同再现目标，还可从稀疏采样点学习全局空间信息。


<details>
  <summary>Details</summary>
Motivation: 传统PSZ再现系统在实际应用中需在同一固定接收器阵列上测量重建目标，不便且成本高。

Method: 提出一个3D卷积神经网络，以虚拟目标场景为输入，PSZ预滤波器为输出。

Result: 与传统方法对比实验表明，该方法仅需一次训练就能处理灵活控制点网格上的不同再现目标，还能从PSZ中的稀疏采样点学习全局空间信息。

Conclusion: 所提方法在PSZ再现中具有处理灵活目标和学习空间信息的优势。

Abstract: Personal sound zone (PSZ) reproduction system, which attempts to create distinct virtual acoustic scenes for different listeners at their respective positions within the same spatial area using one loudspeaker array, is a fundamental technology in the application of virtual reality. For practical applications, the reconstruction targets must be measured on the same fixed receiver array used to record the local room impulse responses (RIRs) from the loudspeaker array to the control points in each PSZ, which makes the system inconvenient and costly for real-world use. In this paper, a 3D convolutional neural network (CNN) designed for PSZ reproduction with flexible control microphone grid and alternative reproduction target is presented, utilizing the virtual target scene as inputs and the PSZ pre-filters as output. Experimental results of the proposed method are compared with the traditional method, demonstrating that the proposed method is able to handle varied reproduction targets on flexible control point grid using only one training session. Furthermore, the proposed method also demonstrates the capability to learn global spatial information from sparse sampling points distributed in PSZs.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [199] [On Learning-Curve Monotonicity for Maximum Likelihood Estimators](https://arxiv.org/abs/2512.10220)
*Mark Sellke,Steven Yin*

Main category: math.ST

TL;DR: 本文为多种参数设置下的最大似然估计建立了非平凡的单调性保证，给出高斯向量和伽马变量等在对数损失下的单调性结果，还指出反向KL散度在一般指数族的单调性，且结果由GPT - 5.2 Pro推导。


<details>
  <summary>Details</summary>
Motivation: 为最大似然估计建立非平凡的单调性保证，解决之前研究中提出的开放性问题。

Method: 利用GPT - 5.2 Pro推导结果，人类仅进行提示、验证和转录。

Result: 为多种参数设置下的最大似然估计建立非平凡单调性保证；证明高斯向量和伽马变量在对数损失下的单调性；发现反向KL散度在一般指数族的单调性。

Conclusion: 通过GPT - 5.2 Pro成功为最大似然估计建立了单调性保证，解决了部分开放性问题。

Abstract: The property of learning-curve monotonicity, highlighted in a recent series of work by Loog, Mey and Viering, describes algorithms which only improve in average performance given more data, for any underlying data distribution within a given family. We establish the first nontrivial monotonicity guarantees for the maximum likelihood estimator in a variety of well-specified parametric settings. For sequential prediction with log loss, we show monotonicity (in fact complete monotonicity) of the forward KL divergence for Gaussian vectors with unknown covariance and either known or unknown mean, as well as for Gamma variables with unknown scale parameter. The Gaussian setting was explicitly highlighted as open in the aforementioned works, even in dimension 1. Finally we observe that for reverse KL divergence, a folklore trick yields monotonicity for very general exponential families.
  All results in this paper were derived by variants of GPT-5.2 Pro. Humans did not provide any proof strategies or intermediate arguments, but only prompted the model to continue developing additional results, and verified and transcribed its proofs.

</details>


### [200] [An Elementary Proof of the Near Optimality of LogSumExp Smoothing](https://arxiv.org/abs/2512.10825)
*Thabo Samakhoana,Benjamin Grimmer*

Main category: math.ST

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider the design of smoothings of the (coordinate-wise) max function in $\mathbb{R}^d$ in the infinity norm. The LogSumExp function $f(x)=\ln(\sum^d_i\exp(x_i))$ provides a classical smoothing, differing from the max function in value by at most $\ln(d)$. We provide an elementary construction of a lower bound, establishing that every overestimating smoothing of the max function must differ by at least $\sim 0.8145\ln(d)$. Hence, LogSumExp is optimal up to constant factors. However, in small dimensions, we provide stronger, exactly optimal smoothings attaining our lower bound, showing that the entropy-based LogSumExp approach to smoothing is not exactly optimal.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [201] [LLM-PEA: Leveraging Large Language Models Against Phishing Email Attacks](https://arxiv.org/abs/2512.10104)
*Najmul Hassan,Prashanth BusiReddyGari,Haitao Zhao,Yihao Ren,Jinsheng Xu,Shaohu Zhang*

Main category: cs.CR

TL;DR: 本文提出LLMPEA框架检测多向量钓鱼邮件攻击，评估三种前沿大模型，发现大模型检测准确率超90%，但检测系统可能受对抗攻击等利用。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型应用增多，系统面临利用其架构的钓鱼邮件威胁，当前大模型在邮件安全系统部署前需强化，尤其是应对多向量攻击。

Method: 提出LLMPEA框架检测多向量钓鱼邮件攻击，评估GPT - 4o、Claude Sonnet 4和Grok - 3三种前沿大模型及综合提示设计。

Result: 大模型检测钓鱼邮件准确率超90%，但基于大模型的钓鱼邮件检测系统可能被对抗攻击、提示注入和多语言攻击利用。

Conclusion: 研究结果为现实场景中基于大模型的钓鱼检测提供关键见解，攻击者会组合利用多种漏洞。

Abstract: Email phishing is one of the most prevalent and globally consequential vectors of cyber intrusion. As systems increasingly deploy Large Language Models (LLMs) applications, these systems face evolving phishing email threats that exploit their fundamental architectures. Current LLMs require substantial hardening before deployment in email security systems, particularly against coordinated multi-vector attacks that exploit architectural vulnerabilities. This paper proposes LLMPEA, an LLM-based framework to detect phishing email attacks across multiple attack vectors, including prompt injection, text refinement, and multilingual attacks. We evaluate three frontier LLMs (e.g., GPT-4o, Claude Sonnet 4, and Grok-3) and comprehensive prompting design to assess their feasibility, robustness, and limitations against phishing email attacks. Our empirical analysis reveals that LLMs can detect the phishing email over 90% accuracy while we also highlight that LLM-based phishing email detection systems could be exploited by adversarial attack, prompt injection, and multilingual attacks. Our findings provide critical insights for LLM-based phishing detection in real-world settings where attackers exploit multiple vulnerabilities in combination.

</details>


### [202] [When Quantum Federated Learning Meets Blockchain in 6G Networks](https://arxiv.org/abs/2512.09958)
*Dinh C. Nguyen,Md Bokhtiar Al Zami,Ratun Rahman,Shaba Shaon,Tuy Tan Nguyen,Fatemeh Afghah*

Main category: cs.CR

TL;DR: 本文提出QFLchain框架，将量子联邦学习与区块链结合，支持可扩展和安全的6G智能，还研究了其四个关键问题并通过案例展示优势。


<details>
  <summary>Details</summary>
Motivation: 未来6G环境动态、分散且数据密集，传统集中式联邦学习框架无法满足需求，需要新框架支持可扩展和安全的6G智能。

Method: 提出QFLchain框架，研究其在6G环境下通信与共识开销、可扩展性与存储开销、能源低效和安全漏洞四个关键问题，并进行案例研究。

Result: 通过模拟案例研究，表明QFLchain在训练性能上比现有方法有潜在优势。

Conclusion: QFLchain作为集成QFL和区块链的框架，能有效支持6G环境下的智能发展，在训练性能上表现更佳。

Abstract: Quantum federated learning (QFL) is emerging as a key enabler for intelligent, secure, and privacy-preserving model training in next-generation 6G networks. By leveraging the computational advantages of quantum devices, QFL offers significant improvements in learning efficiency and resilience against quantum-era threats. However, future 6G environments are expected to be highly dynamic, decentralized, and data-intensive, which necessitates moving beyond traditional centralized federated learning frameworks. To meet this demand, blockchain technology provides a decentralized, tamper-resistant infrastructure capable of enabling trustless collaboration among distributed quantum edge devices. This paper presents QFLchain, a novel framework that integrates QFL with blockchain to support scalable and secure 6G intelligence. In this work, we investigate four key pillars of \textit{QFLchain} in the 6G context: (i) communication and consensus overhead, (ii) scalability and storage overhead, (iii) energy inefficiency, and (iv) security vulnerability. A case study is also presented, demonstrating potential advantages of QFLchain, based on simulation, over state-of-the-art approaches in terms of training performance.

</details>


### [203] [IoTEdu: Access Control, Detection, and Automatic Incident Response in Academic IoT Networks](https://arxiv.org/abs/2512.09934)
*Joner Assolin,Diego Kreutz,Leandro Bertholdo*

Main category: cs.CR

TL;DR: 论文提出IoTEdu平台应对学术环境中IoT设备带来的安全问题，经模拟攻击评估，有良好效果。


<details>
  <summary>Details</summary>
Motivation: 学术环境中IoT设备增多导致操作复杂且暴露安全弱点，缺乏统一策略。

Method: 提出IoTEdu集成平台，结合访问控制、事件检测和自动阻止功能。

Result: 在模拟攻击的受控环境中评估，检测到阻止平均用时28.6秒，减少了手动干预。

Conclusion: 该平台能减少手动干预、规范响应、统一注册、监控和事件响应流程。

Abstract: The growing presence of IoT devices in academic environments has increased operational complexity and exposed security weaknesses, especially in academic institutions without unified policies for registration, monitoring, and incident response involving IoT. This work presents IoTEdu, an integrated platform that combines access control, incident detection, and automatic blocking of IoT devices. The solution was evaluated in a controlled environment with simulated attacks, achieving an average time of 28.6 seconds between detection and blocking. The results show a reduction in manual intervention, standardization of responses, and unification of the processes of registration, monitoring, and incident response.

</details>


### [204] [A Comparative Analysis of zk-SNARKs and zk-STARKs: Theory and Practice](https://arxiv.org/abs/2512.10020)
*Ayush Nainwal,Atharva Kamble,Nitin Awathare*

Main category: cs.CR

TL;DR: 本文在消费级ARM平台上对zk - SNARKs和zk - STARKs进行实现层面的系统比较，分析性能差异并为相关人员提供选择和优化建议。


<details>
  <summary>Details</summary>
Motivation: 虽然零知识证明的理论基础研究充分，但其实在现实条件下的性能尚不清楚，需要进行对比研究。

Method: 使用公开可用的参考实现，在消费级ARM平台上对zk - SNARKs (Groth16)和zk - STARKs进行系统比较，进行包括证明生成时间、验证延迟、证明大小和CPU分析等实证评估。

Result: zk - SNARKs证明生成快68倍、证明大小小123倍，但验证慢且需可信设置；zk - STARKs证明大、生成慢，但验证快，且透明、后量子安全；两种系统有不同的计算瓶颈。

Conclusion: 研究结果为开发者、协议设计者和研究人员在选择和优化证明系统方面提供了可行的见解。

Abstract: Zero-knowledge proofs (ZKPs) are central to secure and privacy-preserving computation, with zk-SNARKs and zk-STARKs emerging as leading frameworks offering distinct trade-offs in efficiency, scalability, and trust assumptions. While their theoretical foundations are well studied, practical performance under real-world conditions remains less understood.
  In this work, we present a systematic, implementation-level comparison of zk-SNARKs (Groth16) and zk-STARKs using publicly available reference implementations on a consumer-grade ARM platform. Our empirical evaluation covers proof generation time, verification latency, proof size, and CPU profiling. Results show that zk-SNARKs generate proofs 68x faster with 123x smaller proof size, but verify slower and require trusted setup, whereas zk-STARKs, despite larger proofs and slower generation, verify faster and remain transparent and post-quantum secure. Profiling further identifies distinct computational bottlenecks across the two systems, underscoring how execution models and implementation details significantly affect real-world performance. These findings provide actionable insights for developers, protocol designers, and researchers in selecting and optimizing proof systems for applications such as privacy-preserving transactions, verifiable computation, and scalable rollups.

</details>


### [205] [Bit of a Close Talker: A Practical Guide to Serverless Cloud Co-Location Attacks](https://arxiv.org/abs/2512.10361)
*Wei Shao,Najmeh Nazari,Behnam Omidi,Setareh Rafatirad,Houman Homayoun,Khaled N. Khasawneh,Chongzhou Fang*

Main category: cs.CR

TL;DR: 本文研究无服务器云调度程序漏洞与共定位攻击，揭示漏洞、实现共定位并提出缓解策略。


<details>
  <summary>Details</summary>
Motivation: 无服务器云用户易受攻击，尤其微架构旁道攻击，需研究调度程序并评估不同调度算法安全性。

Method: 提出全面方法，通过正常用户界面揭示调度算法可利用特征并构建共定位攻击。

Result: 在开源基础设施和微软Azure Functions上成功揭示漏洞并实现实例共定位。

Conclusion: 指出当前云调度程序需加强安全的关键领域，为抵御共定位攻击提供启示。

Abstract: Serverless computing has revolutionized cloud computing by offering an efficient and cost-effective way for users to develop and deploy applications without managing infrastructure details. However, serverless cloud users remain vulnerable to various types of attacks, including micro-architectural side-channel attacks. These attacks typically rely on the physical co-location of victim and attacker instances, and attackers will need to exploit cloud schedulers to achieve co-location with victims. Therefore, it is crucial to study vulnerabilities in serverless cloud schedulers and assess the security of different serverless scheduling algorithms. This study addresses the gap in understanding and constructing co-location attacks in serverless clouds. We present a comprehensive methodology to uncover exploitable features in serverless scheduling algorithms and devise strategies for constructing co-location attacks through normal user interfaces. In our experiments, we successfully reveal exploitable vulnerabilities and achieve instance co-location on prevalent open-source infrastructures and Microsoft Azure Functions. We also present a mitigation strategy to defend against co-location attacks in serverless clouds. Our work highlights critical areas for security enhancements in current cloud schedulers, offering insights to fortify serverless computing environments against potential co-location attacks.

</details>


### [206] [D2M: A Decentralized, Privacy-Preserving, Incentive-Compatible Data Marketplace for Collaborative Learning](https://arxiv.org/abs/2512.10372)
*Yash Srivastava,Shalin Jain,Sneha Awathare,Nitin Awathare*

Main category: cs.CR

TL;DR: 本文提出去中心化数据市场Prot，结合多方技术实现隐私数据共享，在多数据集上评估，结果显示其实用且高效。


<details>
  <summary>Details</summary>
Motivation: 现有协作机器学习和数据分析的数据共享框架存在不足，如联邦学习缺乏拜占庭鲁棒性、区块链框架计算及激励集成困难，因此需要新框架。

Method: 提出Prot框架，结合联邦学习、区块链仲裁和经济激励；通过智能合约管理请求，将计算密集型训练委托给Cone执行层；集成改良YODA协议实现共识，引入Corrected OSMD减轻恶意贡献。

Result: 在MNIST和Fashion - MNIST上达到较高准确率，在30%拜占庭节点下精度下降小于3%，CIFAR - 10达到56%准确率。

Conclusion: Prot能确保隐私，在对抗条件下保持鲁棒性，可随参与者数量有效扩展，是现实世界去中心化数据共享的实用基础。

Abstract: The rising demand for collaborative machine learning and data analytics calls for secure and decentralized data sharing frameworks that balance privacy, trust, and incentives. Existing approaches, including federated learning (FL) and blockchain-based data markets, fall short: FL often depends on trusted aggregators and lacks Byzantine robustness, while blockchain frameworks struggle with computation-intensive training and incentive integration.
  We present \prot, a decentralized data marketplace that unifies federated learning, blockchain arbitration, and economic incentives into a single framework for privacy-preserving data sharing. \prot\ enables data buyers to submit bid-based requests via blockchain smart contracts, which manage auctions, escrow, and dispute resolution. Computationally intensive training is delegated to \cone\ (\uline{Co}mpute \uline{N}etwork for \uline{E}xecution), an off-chain distributed execution layer. To safeguard against adversarial behavior, \prot\ integrates a modified YODA protocol with exponentially growing execution sets for resilient consensus, and introduces Corrected OSMD to mitigate malicious or low-quality contributions from sellers. All protocols are incentive-compatible, and our game-theoretic analysis establishes honesty as the dominant strategy.
  We implement \prot\ on Ethereum and evaluate it over benchmark datasets -- MNIST, Fashion-MNIST, and CIFAR-10 -- under varying adversarial settings. \prot\ achieves up to 99\% accuracy on MNIST and 90\% on Fashion-MNIST, with less than 3\% degradation up to 30\% Byzantine nodes, and 56\% accuracy on CIFAR-10 despite its complexity. Our results show that \prot\ ensures privacy, maintains robustness under adversarial conditions, and scales efficiently with the number of participants, making it a practical foundation for real-world decentralized data sharing.

</details>


### [207] [Differential Privacy for Secure Machine Learning in Healthcare IoT-Cloud Systems](https://arxiv.org/abs/2512.10426)
*N Mangala,Murtaza Rangwala,S Aishwarya,B Eswara Reddy,Rajkumar Buyya,KR Venugopal,SS Iyengar,LM Patnaik*

Main category: cs.CR

TL;DR: 本文提出多层架构提升应急医疗响应速度，用差分隐私框架保障患者数据隐私，评估多种噪声机制，结合区块链安全，边缘计算降低延迟。


<details>
  <summary>Details</summary>
Motivation: 实时响应对于缓解患者紧急情况至关重要，同时保护患者隐私在数据驱动的医疗保健中极为重要。

Method: 提出多层物联网、边缘和云架构，基于响应关键性和存储持久性分配任务；提出跨多个机器学习模型的差分隐私框架；建立综合威胁模型，评估多种噪声机制；结合区块链安全。

Result: 监督算法最高达86%准确率；混合拉普拉斯 - 高斯噪声机制有更好的隐私 - 效用权衡；在阈值ε = 5.0时，监督算法准确率82 - 84%，减少属性推理攻击和数据重建相关性；边缘计算在紧急情况下延迟降低8倍。

Conclusion: 所提架构和方法能有效提升应急医疗响应速度，保障患者数据隐私，适用于时间关键操作。

Abstract: Healthcare has become exceptionally sophisticated, as wearables and connected medical devices are revolutionising remote patient monitoring, emergency response, medication management, diagnosis, and predictive and prescriptive analytics. Internet of Things and Cloud computing integrated systems (IoT-Cloud) facilitate sensing, automation, and processing for these healthcare applications. While real-time response is crucial for alleviating patient emergencies, protecting patient privacy is extremely important in data-driven healthcare. In this paper, we propose a multi-layer IoT, Edge and Cloud architecture to enhance the speed of response for emergency healthcare by distributing tasks based on response criticality and permanence of storage. Privacy of patient data is assured by proposing a Differential Privacy framework across several machine learning models such as K-means, Logistic Regression, Random Forest and Naive Bayes. We establish a comprehensive threat model identifying three adversary classes and evaluate Laplace, Gaussian, and hybrid noise mechanisms across varying privacy budgets, with supervised algorithms achieving up to 86% accuracy. The proposed hybrid Laplace-Gaussian noise mechanism with adaptive budget allocation provides a balanced approach, offering moderate tails and better privacy-utility trade-offs for both low and high dimension datasets. At the practical threshold of $\varepsilon = 5.0$, supervised algorithms achieve 82-84% accuracy while reducing attribute inference attacks by up to 18% and data reconstruction correlation by 70%. Blockchain security further ensures trusted communication through time-stamping, traceability, and immutability for analytics applications. Edge computing demonstrates 8$\times$ latency reduction for emergency scenarios, validating the hierarchical architecture for time-critical operations.

</details>


### [208] [From Lab to Reality: A Practical Evaluation of Deep Learning Models and LLMs for Vulnerability Detection](https://arxiv.org/abs/2512.10485)
*Chaomeng Lu,Bert Lagaisse*

Main category: cs.CR

TL;DR: 研究系统评估两种DL模型和四个预训练LLMs在多个数据集上的表现，发现模型在区分漏洞代码和跨数据集泛化方面表现不佳，凸显学术基准与实际部署间的差距。


<details>
  <summary>Details</summary>
Motivation: 深度学习漏洞检测方法在基准数据集上表现良好，但实际有效性未充分研究，需评估模型在现实场景的适用性。

Method: 对两种代表DL模型（ReVeal和LineVul）在四个数据集上独立训练，用t - SNE分析代码表示，将模型和四个预训练LLMs部署在新数据集VentiVul上进行评估。

Result: 当前模型在表示空间难以区分漏洞和非漏洞代码，跨不同分布数据集泛化差，在新构建的VentiVul数据集上表现大幅下降。

Conclusion: 学术基准和实际部署存在持久差距，强调部署导向评估框架的价值，以及需要更强大的代码表示和更高质量的数据集。

Abstract: Vulnerability detection methods based on deep learning (DL) have shown strong performance on benchmark datasets, yet their real-world effectiveness remains underexplored. Recent work suggests that both graph neural network (GNN)-based and transformer-based models, including large language models (LLMs), yield promising results when evaluated on curated benchmark datasets. These datasets are typically characterized by consistent data distributions and heuristic or partially noisy labels. In this study, we systematically evaluate two representative DL models-ReVeal and LineVul-across four representative datasets: Juliet, Devign, BigVul, and ICVul. Each model is trained independently on each respective dataset, and their code representations are analyzed using t-SNE to uncover vulnerability related patterns. To assess realistic applicability, we deploy these models along with four pretrained LLMs, Claude 3.5 Sonnet, GPT-o3-mini, GPT-4o, and GPT-5 on a curated dataset, VentiVul, comprising 20 recently (May 2025) fixed vulnerabilities from the Linux kernel. Our experiments reveal that current models struggle to distinguish vulnerable from non-vulnerable code in representation space and generalize poorly across datasets with differing distributions. When evaluated on VentiVul, our newly constructed time-wise out-of-distribution dataset, performance drops sharply, with most models failing to detect vulnerabilities reliably. These results expose a persistent gap between academic benchmarks and real-world deployment, emphasizing the value of our deployment-oriented evaluation framework and the need for more robust code representations and higher-quality datasets.

</details>


### [209] [A Proof of Success and Reward Distribution Protocol for Multi-bridge Architecture in Cross-chain Communication](https://arxiv.org/abs/2512.10667)
*Damilare Peter Oyinloye,Mohd Sameen Chishti,Jingyue Li*

Main category: cs.CR

TL;DR: 本文提出PSCRD协议解决单桥区块链方案的中心化和单点故障问题，经数学分析和模拟验证其有效性，能提供更具弹性和安全性的跨链桥系统且不显著增加用户成本。


<details>
  <summary>Details</summary>
Motivation: 单桥区块链方案存在中心化和单点故障风险，需要新协议解决这些问题。

Method: 提出Proof of Success and Reward Distribution (PSCRD) 协议，引入公平奖励分配系统。

Result: 通过基尼指数和中本聪系数两个关键指标，验证了PSCRD协议在奖励分配公平性和去中心化方面的有效性。

Conclusion: PSCRD能提供更具弹性和安全性的跨链桥系统，且不会大幅增加用户成本。

Abstract: Single-bridge blockchain solutions enable cross-chain communication. However, they are associated with centralization and single-point-of-failure risks. This paper proposes Proof of Success and Reward Distribution (PSCRD), a novel multi-bridge response coordination and incentive distribution protocol designed to address the challenges. PSCRD introduces a fair reward distribution system that equitably distributes the transfer fee among participating bridges, incentivizing honest behavior and sustained commitment. The purpose is to encourage bridge participation for higher decentralization and lower single-point-of-failure risks. The mathematical analysis and simulation results validate the effectiveness of PSCRD using two key metrics: the Gini index, which demonstrates a progressive improvement in the fairness of the reward distribution as new bridge groups joined the network; and the Nakamoto coefficient, which shows a significant improvement in decentralization over time. These findings highlight that PSCRD provides a more resilient and secure cross-chain bridge system without substantially increasing user costs.

</details>


### [210] [TriHaRd: Higher Resilience for TEE Trusted Time](https://arxiv.org/abs/2512.10732)
*Matthieu Bettinger,Sonia Ben Mokhtar,Pascal Felber,Etienne Rivière,Valerio Schiavoni,Anthony Simonet-Boulogne*

Main category: cs.CR

TL;DR: 本文指出现有TEE时间协议漏洞，提出TriHaRd协议，经实验证明可缓解已知攻击。


<details>
  <summary>Details</summary>
Motivation: 现有TEE时间源易受恶意主机操纵，Triad协议也存在攻击者控制操作系统操纵时钟速度等问题，需更具弹性的协议。

Method: 提出TriHaRd协议，通过拜占庭弹性时钟更新和一致性检查实现对时钟速度和偏移操纵的高弹性。

Result: 实验表明TriHaRd能缓解针对Triad的已知攻击。

Conclusion: TriHaRd协议在应对时钟速度和偏移操纵方面表现良好，具有高弹性。

Abstract: Accurately measuring time passing is critical for many applications. However, in Trusted Execution Environments (TEEs) such as Intel SGX, the time source is outside the Trusted Computing Base: a malicious host can manipulate the TEE's notion of time, jumping in time or affecting perceived time speed. Previous work (Triad) proposes protocols for TEEs to maintain a trustworthy time source by building a cluster of TEEs that collaborate with each other and with a remote Time Authority to maintain a continuous notion of passing time. However, such approaches still allow an attacker to control the operating system and arbitrarily manipulate their own TEE's perceived clock speed. An attacker can even propagate faster passage of time to honest machines participating in Triad's trusted time protocol, causing them to skip to timestamps arbitrarily far in the future. We propose TriHaRd, a TEE trusted time protocol achieving high resilience against clock speed and offset manipulations, notably through Byzantine-resilient clock updates and consistency checks. We empirically show that TriHaRd mitigates known attacks against Triad.

</details>


### [211] [ZK-APEX: Zero-Knowledge Approximate Personalized Unlearning with Executable Proofs](https://arxiv.org/abs/2512.09953)
*Mohammad M Maheri,Sunil Cotterill,Alex Davidson,Hamed Haddadi*

Main category: cs.CR

TL;DR: 介绍ZK APEX零样本个性化无学习方法，可直接在个性化模型上操作，无需重新训练，在任务中验证效果好且速度快，是边缘设备可验证个性化无学习的实用框架。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，边缘设备个性化模型进行数据删除时验证困难，需满足模型遗忘目标样本同时保留本地效用，且验证要轻量级。

Method: 引入ZK APEX方法，结合提供商端稀疏掩码和客户端小Group OBS补偿步骤，使用块经验Fisher矩阵创建低开销的曲率感知更新，搭配Halo2零知识证明。

Result: 在Vision Transformer分类任务和OPT125M生成模型上，能有效移除目标信息并恢复一定准确率；证明生成速度比基于重新训练的检查快超千万倍，内存使用少，证明大小合适。

Conclusion: ZK APEX是首个适用于边缘设备可验证个性化无学习的实用框架。

Abstract: Machine unlearning aims to remove the influence of specific data points from a trained model to satisfy privacy, copyright, and safety requirements. In real deployments, providers distribute a global model to many edge devices, where each client personalizes the model using private data. When a deletion request is issued, clients may ignore it or falsely claim compliance, and providers cannot check their parameters or data. This makes verification difficult, especially because personalized models must forget the targeted samples while preserving local utility, and verification must remain lightweight on edge devices.
  We introduce ZK APEX, a zero-shot personalized unlearning method that operates directly on the personalized model without retraining. ZK APEX combines sparse masking on the provider side with a small Group OBS compensation step on the client side, using a blockwise empirical Fisher matrix to create a curvature-aware update designed for low overhead. Paired with Halo2 zero-knowledge proofs, it enables the provider to verify that the correct unlearning transformation was applied without revealing any private data or personalized parameters.
  On Vision Transformer classification tasks, ZK APEX recovers nearly all personalization accuracy while effectively removing the targeted information. Applied to the OPT125M generative model trained on code data, it recovers around seventy percent of the original accuracy. Proof generation for the ViT case completes in about two hours, more than ten million times faster than retraining-based checks, with less than one gigabyte of memory use and proof sizes around four hundred megabytes. These results show the first practical framework for verifiable personalized unlearning on edge devices.

</details>


### [212] [Graph Neural Network Based Adaptive Threat Detection for Cloud Identity and Access Management Logs](https://arxiv.org/abs/2512.10280)
*Venkata Tanuja Madireddy*

Main category: cs.CR

TL;DR: 本文提出基于图神经网络的自适应威胁检测框架，用于实时学习IAM审计跟踪中的潜在用户资源交互模式，实验表明该方法优于基线分类器，能助力零信任访问分析。


<details>
  <summary>Details</summary>
Motivation: 云基础设施和分布式身份系统的扩展增加企业复杂性和攻击面，传统检测系统难以识别IAM日志中的新威胁。

Method: 将IAM日志建模为异构动态图，结合基于注意力的聚合和图嵌入更新，实时学习潜在用户资源交互模式。

Result: 在合成和真实世界的IAM数据集上，该方法比基线LSTM和GCN分类器有更高的检测精度和召回率，且在多租户云环境中可扩展。

Conclusion: 该框架的适应性有助于主动缓解内部威胁等攻击，为AI驱动的零信任访问分析奠定基础，弥合了基于图的机器学习和云安全情报之间的差距。

Abstract: The rapid expansion of cloud infrastructures and distributed identity systems has significantly increased the complexity and attack surface of modern enterprises. Traditional rule based or signature driven detection systems are often inadequate in identifying novel or evolving threats within Identity and Access Management logs, where anomalous behavior may appear statistically benign but contextually malicious. This paper presents a Graph Neural Network Based Adaptive Threat Detection framework designed to learn latent user resource interaction patterns from IAM audit trails in real time. By modeling IAM logs as heterogeneous dynamic graphs, the proposed system captures temporal, relational, and contextual dependencies across entities such as users, roles, sessions, and access actions. The model incorporates attention based aggregation and graph embedding updates to enable continual adaptation to changing cloud environments. Experimental evaluation on synthesized and real world IAM datasets demonstrates that the proposed method achieves higher detection precision and recall than baseline LSTM and GCN classifiers, while maintaining scalability across multi tenant cloud environments. The frameworks adaptability enables proactive mitigation of insider threats, privilege escalation, and lateral movement attacks, contributing to the foundation of AI driven zero trust access analytics. This work bridges the gap between graph based machine learning and operational cloud security intelligence.

</details>


### [213] [FLARE: A Wireless Side-Channel Fingerprinting Attack on Federated Learning](https://arxiv.org/abs/2512.10296)
*Md Nahid Hasan Shuvo,Moinul Hossain,Anik Mallik,Jeffrey Twigg,Fikadu Dagefu*

Main category: cs.CR

TL;DR: 本文提出指纹框架FLARE，通过加密无线流量推断联邦学习客户端的深度学习模型架构，评估显示其有高准确率，揭示当前联邦学习系统存在侧信道漏洞。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽能保护数据和隐私，但模型架构被间接泄露可能导致更高级攻击，该方面研究尚不充分。

Method: 提出新颖的侧信道指纹攻击，利用联邦学习客户端加密无线流量的流级和包级统计信息推断模型架构。

Result: 在不同CNN和RNN变体上评估，FLARE在封闭世界场景F1分数超98%，开放世界场景达91%，表明CNN和RNN模型会泄露可区分的流量模式。

Conclusion: 这是首个通过嗅探加密无线流量对联邦学习模型架构进行指纹识别的工作，揭示了当前联邦学习系统存在关键的侧信道漏洞。

Abstract: Federated Learning (FL) enables collaborative model training across distributed devices while safeguarding data and user privacy. However, FL remains susceptible to privacy threats that can compromise data via direct means. That said, indirectly compromising the confidentiality of the FL model architecture (e.g., a convolutional neural network (CNN) or a recurrent neural network (RNN)) on a client device by an outsider remains unexplored. If leaked, this information can enable next-level attacks tailored to the architecture. This paper proposes a novel side-channel fingerprinting attack, leveraging flow-level and packet-level statistics of encrypted wireless traffic from an FL client to infer its deep learning model architecture. We name it FLARE, a fingerprinting framework based on FL Architecture REconnaissance. Evaluation across various CNN and RNN variants-including pre-trained and custom models trained over IEEE 802.11 Wi-Fi-shows that FLARE achieves over 98% F1-score in closed-world and up to 91% in open-world scenarios. These results reveal that CNN and RNN models leak distinguishable traffic patterns, enabling architecture fingerprinting even under realistic FL settings with hardware, software, and data heterogeneity. To our knowledge, this is the first work to fingerprint FL model architectures by sniffing encrypted wireless traffic, exposing a critical side-channel vulnerability in current FL systems.

</details>


### [214] [Authority Backdoor: A Certifiable Backdoor Mechanism for Authoring DNNs](https://arxiv.org/abs/2512.10600)
*Han Yang,Shaofeng Li,Tian Dong,Xiangyu Xu,Guangchi Liu,Zhen Ling*

Main category: cs.CR

TL;DR: 本文提出‘Authority Backdoor’主动保护方案，嵌入访问约束保护DNN，结合可认证鲁棒性，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有DNN保护方法多为被动，无法主动防止模型被盗用，需主动保护方案。

Method: 提出‘Authority Backdoor’方案，利用后门学习框架锁定模型效用，结合可认证鲁棒性防止攻击者移除后门。

Result: 在不同架构和数据集上的大量实验验证了框架的有效性和可认证鲁棒性。

Conclusion: 所提框架为DNN建立了安全的授权机制，结合访问控制和可认证鲁棒性对抗攻击。

Abstract: Deep Neural Networks (DNNs), as valuable intellectual property, face unauthorized use. Existing protections, such as digital watermarking, are largely passive; they provide only post-hoc ownership verification and cannot actively prevent the illicit use of a stolen model. This work proposes a proactive protection scheme, dubbed ``Authority Backdoor," which embeds access constraints directly into the model. In particular, the scheme utilizes a backdoor learning framework to intrinsically lock a model's utility, such that it performs normally only in the presence of a specific trigger (e.g., a hardware fingerprint). But in its absence, the DNN's performance degrades to be useless. To further enhance the security of the proposed authority scheme, the certifiable robustness is integrated to prevent an adaptive attacker from removing the implanted backdoor. The resulting framework establishes a secure authority mechanism for DNNs, combining access control with certifiable robustness against adversarial attacks. Extensive experiments on diverse architectures and datasets validate the effectiveness and certifiable robustness of the proposed framework.

</details>


### [215] [Adaptive Intrusion Detection System Leveraging Dynamic Neural Models with Adversarial Learning for 5G/6G Networks](https://arxiv.org/abs/2512.10637)
*Neha,Tarunpreet Bhatia*

Main category: cs.CR

TL;DR: 提出一种基于对抗训练和动态神经网络的入侵检测系统（IDS）框架，用增量学习减少重训需求，评估显示能提供准确网络攻击分类且抗数据毒化。


<details>
  <summary>Details</summary>
Motivation: 传统IDS基于签名的方法难以检测新型和不断演变的攻击，需提升5G/6G网络安全防护能力。

Method: 提出整合对抗训练、动态神经网络、增量学习算法的IDS框架，利用较少特征和统计属性，用对抗训练增强对毒化数据抗性。

Result: 使用NSL - KDD数据集评估，对各类网络攻击多类分类准确率达82.33%，能抵御数据集毒化。

Conclusion: 对抗训练的动态神经网络在构建有弹性的IDS解决方案方面具有潜力。

Abstract: Intrusion Detection Systems (IDS) are critical components in safeguarding 5G/6G networks from both internal and external cyber threats. While traditional IDS approaches rely heavily on signature-based methods, they struggle to detect novel and evolving attacks. This paper presents an advanced IDS framework that leverages adversarial training and dynamic neural networks in 5G/6G networks to enhance network security by providing robust, real-time threat detection and response capabilities. Unlike conventional models, which require costly retraining to update knowledge, the proposed framework integrates incremental learning algorithms, reducing the need for frequent retraining. Adversarial training is used to fortify the IDS against poisoned data. By using fewer features and incorporating statistical properties, the system can efficiently detect potential threats. Extensive evaluations using the NSL- KDD dataset demonstrate that the proposed approach provides better accuracy of 82.33% for multiclass classification of various network attacks while resisting dataset poisoning. This research highlights the potential of adversarial-trained, dynamic neural networks for building resilient IDS solutions.

</details>


### [216] [Virtual camera detection: Catching video injection attacks in remote biometric systems](https://arxiv.org/abs/2512.10653)
*Daniyar Kurmankhojayev,Andrei Shadrikov,Dmitrii Gordin,Mikhail Shkorin,Danijar Gabdullin,Aigerim Kambetbayeva,Kanat Kuatov*

Main category: cs.CR

TL;DR: 本文介绍基于机器学习的虚拟相机检测（VCD）方法用于人脸反欺骗，经实证验证有效。


<details>
  <summary>Details</summary>
Motivation: 视频注入攻击对基于人脸识别的远程生物认证系统构成挑战，现有文献对VCD实际应用和评估研究有限。

Method: 采用机器学习方法，基于真实用户会话收集的元数据训练模型。

Result: 该模型能有效识别视频注入企图，降低恶意用户绕过人脸反欺骗系统的风险。

Conclusion: 基于机器学习的VCD方法在人脸反欺骗中有实际效果。

Abstract: Face anti-spoofing (FAS) is a vital component of remote biometric authentication systems based on facial recognition, increasingly used across web-based applications. Among emerging threats, video injection attacks -- facilitated by technologies such as deepfakes and virtual camera software -- pose significant challenges to system integrity. While virtual camera detection (VCD) has shown potential as a countermeasure, existing literature offers limited insight into its practical implementation and evaluation. This study introduces a machine learning-based approach to VCD, with a focus on its design and validation. The model is trained on metadata collected during sessions with authentic users. Empirical results demonstrate its effectiveness in identifying video injection attempts and reducing the risk of malicious users bypassing FAS systems.

</details>


### [217] [Metaphor-based Jailbreaking Attacks on Text-to-Image Models](https://arxiv.org/abs/2512.10766)
*Chenyu Zhang,Yiwen Ma,Lanjun Wang,Wenhui Li,Yi Tu,An-An Liu*

Main category: cs.CR

TL;DR: 提出基于隐喻的越狱攻击方法MJA，在无防御类型先验知识下攻击文本到图像模型的防御机制，实验表明其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有攻击方法假设攻击者知晓防御类型，限制了对未知或多样防御机制的攻击效果，需新方法应对。

Method: MJA包含基于大语言模型的多智能体生成模块（MLAG）和对抗性提示优化模块（APO），MLAG分解生成任务并协调智能体生成提示，APO训练替代模型预测结果并设计策略识别最优提示。

Result: 在多种防御机制的文本到图像模型上实验，MJA优于六个基线方法，攻击性能更强且查询次数更少。

Conclusion: MJA是一种有效且高效的攻击方法，能在无防御类型先验知识下攻击多样防御机制。

Abstract: Text-to-image~(T2I) models commonly incorporate defense mechanisms to prevent the generation of sensitive images. Unfortunately, recent jailbreaking attacks have shown that adversarial prompts can effectively bypass these mechanisms and induce T2I models to produce sensitive content, revealing critical safety vulnerabilities. However, existing attack methods implicitly assume that the attacker knows the type of deployed defenses, which limits their effectiveness against unknown or diverse defense mechanisms. In this work, we introduce \textbf{MJA}, a \textbf{m}etaphor-based \textbf{j}ailbreaking \textbf{a}ttack method inspired by the Taboo game, aiming to effectively and efficiently attack diverse defense mechanisms without prior knowledge of their type by generating metaphor-based adversarial prompts. Specifically, MJA consists of two modules: an LLM-based multi-agent generation module~(MLAG) and an adversarial prompt optimization module~(APO). MLAG decomposes the generation of metaphor-based adversarial prompts into three subtasks: metaphor retrieval, context matching, and adversarial prompt generation. Subsequently, MLAG coordinates three LLM-based agents to generate diverse adversarial prompts by exploring various metaphors and contexts. To enhance attack efficiency, APO first trains a surrogate model to predict the attack results of adversarial prompts and then designs an acquisition strategy to adaptively identify optimal adversarial prompts. Extensive experiments on T2I models with various external and internal defense mechanisms demonstrate that MJA outperforms six baseline methods, achieving stronger attack performance while using fewer queries. Code is available in https://github.com/datar001/metaphor-based-jailbreaking-attack.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [218] [Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale](https://arxiv.org/abs/2512.10398)
*Zhaodong Wang,Zhenting Qi,Sherman Wong,Nathan Hu,Samuel Lin,Jun Ge,Erwin Gao,Yining Yang,Ben Maurer,Wenlin Chen,David Recordon,Yilun Du,Minlan Yu,Ying Zhang*

Main category: cs.CL

TL;DR: 提出开源的Confucius Code Agent (CCA)，基于Confucius SDK，在软件工程任务上表现出色，为AI智能体提供基础。


<details>
  <summary>Details</summary>
Motivation: 现有开源编码智能体在工业级工作负载表现不足，专有编码智能体扩展性、可解释性和可控性有限，需工业级开源AI软件工程师。

Method: 构建Confucius SDK，引入统一编排器、持久笔记系统和模块化扩展模块，通过元智能体的构建 - 测试 - 改进循环自动化配置智能体。

Result: CCA在SWE - Bench - Pro上达到54.3%的Resolve@1性能，大幅超越先前编码智能体。

Conclusion: Confucius SDK和CCA为AI智能体提供透明、可扩展和可重现基础，弥合研究原型与生产级系统差距，支持工业级开发部署。

Abstract: Real-world AI software engineering demands coding agents that can reason over massive repositories, maintain durable memory across and within long sessions, and robustly coordinate complex toolchains at test time. Existing open-source coding agents provide transparency but frequently fall short when pushed to these industrial-scale workloads, while proprietary coding agents offer strong practical performance but limited extensibility, interpretability, and controllability. We present the Confucius Code Agent (CCA), an open-sourced AI software engineer that can operate at an industrial scale. CCA is built atop the Confucius SDK, an open-sourced agent development platform designed around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK introduces a unified orchestrator with hierarchical working memory for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension module for robust tool use. Moreover, a meta-agent automates the synthesis, evaluation, and refinement of agent configurations through a build-test-improve loop, enabling rapid agent development on new tasks, environments, and tool stacks. Instantiated on Confucius SDK with these mechanisms, CCA delivers strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%, substantially improving over prior coding agents. Together, the Confucius SDK and CCA provide a transparent, extensible, and reproducible foundation for AI agents, bridge gaps between research prototypes and production-grade systems, and support agent development and deployment at industrial scale.

</details>


### [219] [Unsupervised Acquisition of Discrete Grammatical Categories](https://arxiv.org/abs/2503.18702)
*David Ph. Shakouri,Crit Cremers,Niels O. Schiller*

Main category: cs.CL

TL;DR: 文章利用语言习得实验计算实验室环境做实验，实现含母女语言模型的多智能体系统，展示获取抽象语法知识的过程并验证可获取非平凡语法知识。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用计算实验室环境让语言模型获取抽象语法知识。

Method: 实现含母女语言模型的多智能体系统，女儿模型只能接触母亲模型生成的语言样本；对母亲模型生成的话语进行层次聚类分析获取语法规则；用训练数据确定环境参数配置并在测试集验证。

Result: 通过统计分析输入数据模式得到离散语法规则，添加到女儿模型语法知识中；在测试集也实现非平凡语法知识的获取。

Conclusion: 该系统和方法可用于获取类似自然语言语法类别的结构，能实现非平凡语法知识的获取。

Abstract: This article presents experiments performed using a computational laboratory environment for language acquisition experiments. It implements a multi-agent system consisting of two agents: an adult language model and a daughter language model that aims to learn the mother language. Crucially, the daughter agent does not have access to the internal knowledge of the mother language model but only to the language exemplars the mother agent generates. These experiments illustrate how this system can be used to acquire abstract grammatical knowledge. We demonstrate how statistical analyses of patterns in the input data corresponding to grammatical categories yield discrete grammatical rules. These rules are subsequently added to the grammatical knowledge of the daughter language model. To this end, hierarchical agglomerative cluster analysis was applied to the utterances consecutively generated by the mother language model. It is argued that this procedure can be used to acquire structures resembling grammatical categories proposed by linguists for natural languages. Thus, it is established that non-trivial grammatical knowledge has been acquired. Moreover, the parameter configuration of this computational laboratory environment determined using training data generated by the mother language model is validated in a second experiment with a test set similarly resulting in the acquisition of non-trivial categories.

</details>


### [220] [What Kind of Reasoning (if any) is an LLM actually doing? On the Stochastic Nature and Abductive Appearance of Large Language Models](https://arxiv.org/abs/2512.10080)
*Luciano Floridi,Jessica Morley,Claudio Novelli,David Watson*

Main category: cs.CL

TL;DR: 文章探讨当前基于token - completion方法的大语言模型推理机制，指出其非真正溯因推理，分析其影响并回应异议。


<details>
  <summary>Details</summary>
Motivation: 研究当前基于token - completion方法的大语言模型的推理工作方式。

Method: 通过举例说明大语言模型在无真实依据、语义理解等情况下生成看似合理的输出，最后回应五点异议。

Result: 大语言模型基于学习模式生成文本，非真正溯因推理，其随机本质和看似溯因的表现对评估和应用有重要影响。

Conclusion: 大语言模型可辅助生成想法和支持人类思维，但输出需批判性评估，同时指出分析的局限性并给出总体评价。

Abstract: This article looks at how reasoning works in current Large Language Models (LLMs) that function using the token-completion method. It examines their stochastic nature and their similarity to human abductive reasoning. The argument is that these LLMs create text based on learned patterns rather than performing actual abductive reasoning. When their output seems abductive, this is largely because they are trained on human-generated texts that include reasoning structures. Examples are used to show how LLMs can produce plausible ideas, mimic commonsense reasoning, and give explanatory answers without being grounded in truth, semantics, verification, or understanding, and without performing any real abductive reasoning. This dual nature, where the models have a stochastic base but appear abductive in use, has important consequences for how LLMs are evaluated and applied. They can assist with generating ideas and supporting human thinking, but their outputs must be critically assessed because they cannot identify truth or verify their explanations. The article concludes by addressing five objections to these points, noting some limitations in the analysis, and offering an overall evaluation.

</details>


### [221] [Workflow is All You Need: Escaping the "Statistical Smoothing Trap" via High-Entropy Information Foraging and Adversarial Pacing](https://arxiv.org/abs/2512.10121)
*Zhongjie Jiang*

Main category: cs.CL

TL;DR: 当前大语言模型在垂直领域长文生成面临“不可能三角”问题，本文提出DeepNews框架解决这一问题，实验显示有良好效果。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在垂直领域长文生成中低幻觉、深度逻辑连贯和个性化表达无法同时实现的瓶颈，原因是现有范式陷入统计平滑陷阱。

Method: 提出DeepNews框架，包含基于信息觅食理论的双粒度检索机制、模式引导的战略规划、对抗约束提示三个核心模块。

Result: 实验发现金融深度报道存在知识悬崖，DeepNews系统在盲测中投稿接受率达25%，远高于SOTA模型的0%。

Conclusion: DeepNews框架能有效提升大语言模型在垂直领域长文生成的质量，解决原有的瓶颈问题。

Abstract: Central to long-form text generation in vertical domains is the "impossible trinity" confronting current large language models (LLMs): the simultaneous achievement of low hallucination, deep logical coherence, and personalized expression. This study establishes that this bottleneck arises from existing generative paradigms succumbing to the Statistical Smoothing Trap, a phenomenon that overlooks the high-entropy information acquisition and structured cognitive processes integral to expert-level writing. To address this limitation, we propose the DeepNews Framework, an agentic workflow that explicitly models the implicit cognitive processes of seasoned financial journalists. The framework integrates three core modules: first, a dual-granularity retrieval mechanism grounded in information foraging theory, which enforces a 10:1 saturated information input ratio to mitigate hallucinatory outputs; second, schema-guided strategic planning, a process leveraging domain expert knowledge bases (narrative schemas) and Atomic Blocks to forge a robust logical skeleton; third, adversarial constraint prompting, a technique deploying tactics including Rhythm Break and Logic Fog to disrupt the probabilistic smoothness inherent in model-generated text. Experiments delineate a salient Knowledge Cliff in deep financial reporting: content truthfulness collapses when retrieved context falls below 15,000 characters, while a high-redundancy input exceeding 30,000 characters stabilizes the Hallucination-Free Rate (HFR) above 85%. In an ecological validity blind test conducted with a top-tier Chinese technology media outlet, the DeepNews system--built on a previous-generation model (DeepSeek-V3-0324)-achieved a 25% submission acceptance rate, significantly outperforming the 0% acceptance rate of zero-shot generation by a state-of-the-art (SOTA) model (GPT-5).

</details>


### [222] [PARAN: Persona-Augmented Review ANswering system on Food Delivery Review Dataset](https://arxiv.org/abs/2512.10148)
*Moonsoo Park,Jeongseok Yun,Bohyung Kim*

Main category: cs.CL

TL;DR: 提出两阶段提示框架，从短评中推断显式和隐式用户角色，融入响应生成提示，调整解码温度，用韩国外卖应用数据集评估，证明无需微调模型即可提升自动回复相关性和个性化。


<details>
  <summary>Details</summary>
Motivation: 在用户信息有限的领域，大语言模型缺乏上下文用户数据时生成通用回复，降低参与度和效果，需解决个性化评论回复生成问题。

Method: 提出两阶段提示框架，从短评中推断显式和隐式用户角色，将角色属性融入响应生成提示，推理时调整解码温度。

Result: 使用韩国外卖应用的真实数据集评估，在精度、多样性和语义一致性方面有效果。

Conclusion: 基于角色增强的提示方法无需模型微调，能有效提升自动回复的相关性和个性化。

Abstract: Personalized review response generation presents a significant challenge in domains where user information is limited, such as food delivery platforms. While large language models (LLMs) offer powerful text generation capabilities, they often produce generic responses when lacking contextual user data, reducing engagement and effectiveness. In this work, we propose a two-stage prompting framework that infers both explicit (e.g., user-stated preferences) and implicit (e.g., demographic or stylistic cues) personas directly from short review texts. These inferred persona attributes are then incorporated into the response generation prompt to produce user-tailored replies. To encourage diverse yet faithful generations, we adjust decoding temperature during inference. We evaluate our method using a real-world dataset collected from a Korean food delivery app, and assess its impact on precision, diversity, and semantic consistency. Our findings highlight the effectiveness of persona-augmented prompting in enhancing the relevance and personalization of automated responses without requiring model fine-tuning.

</details>


### [223] [Unforgotten Safety: Preserving Safety Alignment of Large Language Models with Continual Learning](https://arxiv.org/abs/2512.10150)
*Lama Alssum,Hani Itani,Hasan Abed Al Kader Hammoud,Philip Torr,Adel Bibi,Bernard Ghanem*

Main category: cs.CL

TL;DR: 研究大语言模型适应新任务时的安全退化问题，采用持续学习（CL）方法，评估不同CL方法缓解安全退化能力，结果显示CL优于标准微调，DER表现最佳。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型普及，其安全对齐愈发重要，有必要研究适应新任务时的安全退化问题。

Method: 将调优时的安全问题框定为持续学习问题，考虑微调即服务设置，采用正则化、基于内存和模型合并等CL方法，在良性和恶意用户数据两种场景下评估。

Result: CL方法攻击成功率低于标准微调，DER表现优于其他方法且保持任务效用，结果在三个下游任务和三个模型族中通用。

Conclusion: CL是一种有效的保持大语言模型安全的实用解决方案。

Abstract: The safety alignment of large language models (LLMs) is becoming increasingly important with their democratization. In this paper, we study the safety degradation that comes with adapting LLMs to new tasks. We attribute this safety compromise to catastrophic forgetting and frame the problem of preserving safety when fine-tuning as a continual learning (CL) problem. We consider the fine-tuning-as-a-service setup where the user uploads their data to a service provider to get a customized model that excels on the user's selected task. We adapt several CL approaches from the literature and systematically evaluate their ability to mitigate safety degradation. These include regularization-based, memory-based, and model merging approaches. We consider two scenarios, (1) benign user data and (2) poisoned user data. Our results demonstrate that CL approaches consistently achieve lower attack success rates than standard fine-tuning. Among these, DER outperforms both other CL methods and existing safety-preserving baselines while maintaining task utility. These findings generalize across three downstream tasks (GSM8K, SST2, Code) and three model families (LLaMA2-7B, Mistral-7B, Gemma-2B), establishing CL as a practical solution to preserve safety.

</details>


### [224] [Multilingual VLM Training: Adapting an English-Trained VLM to French](https://arxiv.org/abs/2512.10336)
*Jules Lahmi,Alexis Roger*

Main category: cs.CL

TL;DR: 现有视觉语言模型多限于英语，本文探讨适配不同语言挑战，比较多种方法，发现数据集翻译是主要瓶颈，建议聚焦母语数据集收集和改进翻译策略。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型发展多限于英语，降低非英语使用者可用性，需将其能力拓展到更多语言。

Method: 探索并比较基于翻译的管道、LoRA微调、将视觉和语言适配分离的两阶段微调策略，用目标语言翻译的标准多模态基准和母语专家人工评估来评价方法。

Result: 数据集翻译是多语言视觉语言模型性能的主要瓶颈，数据质量限制训练和评估效果。

Conclusion: 未来应聚焦于母语数据集收集和改进翻译策略。

Abstract: Artificial intelligence has made great progress in recent years, particularly in the development of Vision--Language Models (VLMs) that understand both visual and textual data. However, these advancements remain largely limited to English, reducing their accessibility for non--English speakers. It is essential to extend these capabilities to a broader range of languages. This paper explores the challenges of adapting an English-trained VLM to different languages. To this end, we will explore and compare different methods for their performance and computational cost. We consider a translation-based pipeline, LoRA finetuning, and a two-stage finetuning strategy that separates vision adaptation from language adaptation. To evaluate these methods, we use a combination of standard multimodal benchmarks translated into the target language and manual assessments by native experts. The results reveal that dataset translation remains a major bottleneck in multilingual VLM performance, with data quality limiting the effectiveness of training and evaluation. These findings suggest that future efforts should focus on native-language dataset collection and improved translation strategies.

</details>


### [225] [AutoMedic: An Automated Evaluation Framework for Clinical Conversational Agents with Medical Dataset Grounding](https://arxiv.org/abs/2512.10195)
*Gyutaek Oh,Sangjoon Park,Byung-Hoon Kim*

Main category: cs.CL

TL;DR: 提出AutoMedic多智能体仿真框架用于自动评估大语言模型作为临床对话代理的表现，经专家验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有静态医学问答基准在评估大语言模型于动态临床多轮对话场景的效果及多维度评估策略方面存在不足，且动态场景评估难标准化和量化。

Method: 引入AutoMedic框架，将现成静态问答数据集转化为虚拟患者档案，实现大语言模型代理间真实临床多轮对话，并基于CARE指标评估性能。

Result: 经人类专家验证，证明AutoMedic作为临床对话代理自动评估框架的有效性。

Conclusion: AutoMedic为对话式医疗应用中大语言模型的有效开发提供实用指南。

Abstract: Evaluating large language models (LLMs) has recently emerged as a critical issue for safe and trustworthy application of LLMs in the medical domain. Although a variety of static medical question-answering (QA) benchmarks have been proposed, many aspects remain underexplored, such as the effectiveness of LLMs in generating responses in dynamic, interactive clinical multi-turn conversation situations and the identification of multi-faceted evaluation strategies beyond simple accuracy. However, formally evaluating a dynamic, interactive clinical situation is hindered by its vast combinatorial space of possible patient states and interaction trajectories, making it difficult to standardize and quantitatively measure such scenarios. Here, we introduce AutoMedic, a multi-agent simulation framework that enables automated evaluation of LLMs as clinical conversational agents. AutoMedic transforms off-the-shelf static QA datasets into virtual patient profiles, enabling realistic and clinically grounded multi-turn clinical dialogues between LLM agents. The performance of various clinical conversational agents is then assessed based on our CARE metric, which provides a multi-faceted evaluation standard of clinical conversational accuracy, efficiency/strategy, empathy, and robustness. Our findings, validated by human experts, demonstrate the validity of AutoMedic as an automated evaluation framework for clinical conversational agents, offering practical guidelines for the effective development of LLMs in conversational medical applications.

</details>


### [226] [Sliding Window Attention Adaptation](https://arxiv.org/abs/2512.10411)
*Yijiong Yu,Jiale Liu,Qingyun Wu,Huazheng Wang,Ji Pei*

Main category: cs.CL

TL;DR: 本文聚焦Transformer大语言模型自注意力机制处理长文本成本高的问题，提出SWA适配方法SWAA。


<details>
  <summary>Details</summary>
Motivation: Transformer大语言模型自注意力机制计算成本高，滑动窗口注意力虽能降低成本，但直接使用会因训练和推理不匹配导致长文本性能下降，故探索不预训练情况下让FA预训练模型适配SWA的方法。

Method: 提出Sliding Window Attention Adaptation (SWAA)，结合五种方法进行适配：仅在预填充时应用SWA；保留“sink”令牌；交错使用FA/SWA层；使用思维链；进行微调。

Result: 实验表明SWA适配可行但非易事，单一方法不足，特定组合能有效恢复长文本性能。

Conclusion: 分析不同SWAA配置的性能-效率权衡，为不同场景提供推荐方法，代码开源。

Abstract: The self-attention mechanism in Transformer-based Large Language Models (LLMs) scales quadratically with input length, making long-context inference expensive. Sliding window attention (SWA) reduces this cost to linear complexity, but naively enabling complete SWA at inference-time for models pretrained with full attention (FA) causes severe long-context performance degradation due to training-inference mismatch. This makes us wonder: Can FA-pretrained LLMs be well adapted to SWA without pretraining? We investigate this by proposing Sliding Window Attention Adaptation (SWAA), a set of practical recipes that combine five methods for better adaptation: (1) applying SWA only during prefilling; (2) preserving "sink" tokens; (3) interleaving FA/SWA layers; (4) chain-of-thought (CoT); and (5) fine-tuning. Our experiments show that SWA adaptation is feasible while non-trivial: no single method suffices, yet specific synergistic combinations effectively recover the original long-context performance. We further analyze the performance-efficiency trade-offs of different SWAA configurations and provide recommended recipes for diverse scenarios. Our code is available at https://github.com/yuyijiong/sliding-window-attention-adaptation

</details>


### [227] [Cooperative Retrieval-Augmented Generation for Question Answering: Mutual Information Exchange and Ranking by Contrasting Layers](https://arxiv.org/abs/2512.10422)
*Youmin Ko,Sungjong Seo,Hyunjoon Kim*

Main category: cs.CL

TL;DR: 现有RAG方法用于问答易出错，提出CoopRAG框架，实验表明其在多数据集上表现优于当前最佳QA方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型生成事实不准确，现有RAG方法用于问答仍易出现错误检索和幻觉。

Method: 提出CoopRAG框架，包括将问题展开为子问题和推理链、检索相关文档、通过对比检索器层对文档重新排序、通过大语言模型填充推理链中被掩盖位置。

Result: CoopRAG在三个多跳QA数据集和一个简单QA数据集的检索和问答性能上均优于现有QA方法。

Conclusion: CoopRAG是一种有效的问答任务RAG框架。

Abstract: Since large language models (LLMs) have a tendency to generate factually inaccurate output, retrieval-augmented generation (RAG) has gained significant attention as a key means to mitigate this downside of harnessing only LLMs. However, existing RAG methods for simple and multi-hop question answering (QA) are still prone to incorrect retrievals and hallucinations. To address these limitations, we propose CoopRAG, a novel RAG framework for the question answering task in which a retriever and an LLM work cooperatively with each other by exchanging informative knowledge, and the earlier and later layers of the retriever model work cooperatively with each other to accurately rank the retrieved documents relevant to a given query. In this framework, we (i) unroll a question into sub-questions and a reasoning chain in which uncertain positions are masked, (ii) retrieve the documents relevant to the question augmented with the sub-questions and the reasoning chain, (iii) rerank the documents by contrasting layers of the retriever, and (iv) reconstruct the reasoning chain by filling the masked positions via the LLM. Our experiments demonstrate that CoopRAG consistently outperforms state-of-the-art QA methods on three multi-hop QA datasets as well as a simple QA dataset in terms of both the retrieval and QA performances. Our code is available.\footnote{https://github.com/meaningful96/CoopRAG}

</details>


### [228] [Causal Reasoning Favors Encoders: On The Limits of Decoder-Only Models](https://arxiv.org/abs/2512.10561)
*Amartya Roy,Elamparithy M,Kripabandhu Ghosh,Ponnurangam Kumaraguru,Adrian de Wynter*

Main category: cs.CL

TL;DR: 研究上下文学习（ICL）在因果推理中的表现，对比不同架构模型，发现ICL单独用于因果推理不足，有针对性微调的编码器或编码器 - 解码器架构更适合成本效益高、短期的稳健因果推理。


<details>
  <summary>Details</summary>
Motivation: ICL在因果推理中的作用和性能不明确，因果推理有特殊要求且依赖输入虚假词汇关系会导致误导结果，因此研究不同架构在因果推理中的表现。

Method: 比较微调后的编码器、编码器 - 解码器和解码器架构模型在零样本和少样本ICL的自然语言和非自然语言场景中的表现。

Result: ICL单独不足以进行可靠因果推理，常过度关注无关输入特征；解码器模型对分布变化脆弱，微调的编码器和编码器 - 解码器模型能更稳健泛化，大尺度下解码器模型才更有优势。

Conclusion: 对于成本效益高、短期的稳健因果推理，有针对性微调的编码器或编码器 - 解码器架构更可取。

Abstract: In context learning (ICL) underpins recent advances in large language models (LLMs), although its role and performance in causal reasoning remains unclear. Causal reasoning demands multihop composition and strict conjunctive control, and reliance on spurious lexical relations of the input could provide misleading results. We hypothesize that, due to their ability to project the input into a latent space, encoder and encoder decoder architectures are better suited for said multihop conjunctive reasoning versus decoder only models. To do this, we compare fine-tuned versions of all the aforementioned architectures with zero and few shot ICL in both natural language and non natural language scenarios. We find that ICL alone is insufficient for reliable causal reasoning, often overfocusing on irrelevant input features. In particular, decoder only models are noticeably brittle to distributional shifts, while finetuned encoder and encoder decoder models can generalize more robustly across our tests, including the non natural language split. Both architectures are only matched or surpassed by decoder only architectures at large scales. We conclude by noting that for cost effective, short horizon robust causal reasoning, encoder or encoder decoder architectures with targeted finetuning are preferable.

</details>


### [229] [Textual Data Bias Detection and Mitigation - An Extensible Pipeline with Experimental Evaluation](https://arxiv.org/abs/2512.10734)
*Rebekka Görge,Sujan Sai Gannamaneni,Tabea Naeven,Hammam Abdelwahab,Héctor Allende-Cid,Armin B. Cremers,Lennard Helmer,Michael Mock,Anna Schmitz,Songkai Xue,Elif Yildirir,Maximilian Poretschkin,Stefan Wrobel*

Main category: cs.CL

TL;DR: 本文提出数据偏差检测与缓解管道，评估其对数据和模型去偏效果，发现当前评估方法有差距。


<details>
  <summary>Details</summary>
Motivation: 训练大语言模型的文本数据存在多种偏差，相关法规需识别和缓解偏差，但缺乏实用指导和操作方法。

Method: 提出包含四组件的管道，检测和缓解表征偏差与刻板印象，包括利用大模型生成词表、量化表征偏差、过滤刻板印象、数据增强。

Result: 成功减少文本数据偏差，但微调后的大模型在偏差基准测试中未稳定提升性能。

Conclusion: 当前评估方法存在关键差距，需针对性数据操作解决模型偏差。

Abstract: Textual data used to train large language models (LLMs) exhibits multifaceted bias manifestations encompassing harmful language and skewed demographic distributions. Regulations such as the European AI Act require identifying and mitigating biases against protected groups in data, with the ultimate goal of preventing unfair model outputs. However, practical guidance and operationalization are lacking. We propose a comprehensive data bias detection and mitigation pipeline comprising four components that address two data bias types, namely representation bias and (explicit) stereotypes for a configurable sensitive attribute. First, we leverage LLM-generated word lists created based on quality criteria to detect relevant group labels. Second, representation bias is quantified using the Demographic Representation Score. Third, we detect and mitigate stereotypes using sociolinguistically informed filtering. Finally, we compensate representation bias through Grammar- and Context-Aware Counterfactual Data Augmentation. We conduct a two-fold evaluation using the examples of gender, religion and age. First, the effectiveness of each individual component on data debiasing is evaluated through human validation and baseline comparison. The findings demonstrate that we successfully reduce representation bias and (explicit) stereotypes in a text dataset. Second, the effect of data debiasing on model bias reduction is evaluated by bias benchmarking of several models (0.6B-8B parameters), fine-tuned on the debiased text dataset. This evaluation reveals that LLMs fine-tuned on debiased data do not consistently show improved performance on bias benchmarks, exposing critical gaps in current evaluation methodologies and highlighting the need for targeted data manipulation to address manifested model bias.

</details>


### [230] [Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving](https://arxiv.org/abs/2512.10739)
*Songyang Gao,Yuzhe Gu,Zijian Wu,Lingkai Kong,Wenwei Zhang,Zhongrui Cai,Fan Zheng,Tianyou Ma,Junhao Shen,Haiteng Zhao,Duanyang Zhang,Huilun Zhang,Kuikun Liu,Chengqi Lyu,Yanhui Duan,Chiyu Chen,Ningsheng Ma,Jianfei Gao,Han Lyu,Dahua Lin,Kai Chen*

Main category: cs.CL

TL;DR: 本文提出基于结果的过程验证器OPV解决长推理链验证问题，通过迭代主动学习框架提升性能，实验显示其性能优越、适用性广。


<details>
  <summary>Details</summary>
Motivation: 当前基于结果的验证器无法检查长推理链中不可靠的中间步骤，基于过程的验证器因高质量注释稀缺难以可靠检测复杂长推理链的错误。

Method: 提出OPV验证长推理链总结结果的基本原理过程，采用迭代主动学习框架和专家注释，通过拒绝微调（RFT）和强化学习与可验证奖励（RLVR）改进OPV。

Result: OPV在保留测试集上取得新的最优结果，有效检测合成数据集中的误报，与策略模型协作时持续提升性能。

Conclusion: OPV具有优越性能和广泛适用性。

Abstract: Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the \textbf{O}utcome-based \textbf{P}rocess \textbf{V}erifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out \textsc{\thisbench}, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2\% to 73.3\% on AIME2025 as the compute budget scales.

</details>


### [231] [Grow Up and Merge: Scaling Strategies for Efficient Language Adaptation](https://arxiv.org/abs/2512.10772)
*Kevin Glocker,Kätriin Kukk,Romina Oji,Marcel Bollmann,Marco Kuhlmann,Jenny Kunz*

Main category: cs.CL

TL;DR: 研究了扩大模型规模作为预训练模型适配新目标语言的策略，发现扩大规模能提升数据效率、减少灾难性遗忘，且扩大规模的合并模型效果更好。


<details>
  <summary>Details</summary>
Motivation: 解决中低资源语言的高性能语言模型构建难题，探究扩大模型规模适配新目标语言的有效性。

Method: 通过对近似FLOP匹配的模型进行全面的规模消融实验，对比扩大英语基础模型规模与标准持续预训练的效果。

Result: 扩大规模的大模型在目标语言数据充足时性能可匹配或超越小模型；扩大规模有助于保留基础模型的英语能力；扩大规模的合并模型表现优于小模型，但合并效果仍不如联合多语言训练。

Conclusion: 扩大模型规模是适配新目标语言的有效策略，在数据效率和保留基础语言能力方面有优势，合并方法有改进潜力。

Abstract: Achieving high-performing language models which include medium- and lower-resource languages remains a challenge. Massively multilingual models still underperform compared to language-specific adaptations, especially at smaller model scales. In this work, we investigate scaling as an efficient strategy for adapting pretrained models to new target languages. Through comprehensive scaling ablations with approximately FLOP-matched models, we test whether upscaling an English base model enables more effective and resource-efficient adaptation than standard continued pretraining. We find that, once exposed to sufficient target-language data, larger upscaled models can match or surpass the performance of smaller models continually pretrained on much more data, demonstrating the benefits of scaling for data efficiency. Scaling also helps preserve the base model's capabilities in English, thus reducing catastrophic forgetting. Finally, we explore whether such scaled, language-specific models can be merged to construct modular and flexible multilingual systems. We find that while merging remains less effective than joint multilingual training, upscaled merges perform better than smaller ones. We observe large performance differences across merging methods, suggesting potential for improvement through merging approaches specialized for language-level integration.

</details>


### [232] [The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality](https://arxiv.org/abs/2512.10791)
*Aileen Cheng,Alon Jacovi,Amir Globerson,Ben Golan,Charles Kwong,Chris Alberti,Connie Tao,Eyal Ben-David,Gaurav Singh Tomar,Lukas Haas,Yonatan Bitton,Adam Bloniarz,Aijun Bai,Andrew Wang,Anfal Siddiqui,Arturo Bajuelos Castillo,Aviel Atias,Chang Liu,Corey Fry,Daniel Balle,Deepanway Ghosal,Doron Kukliansky,Dror Marcus,Elena Gribovskaya,Eran Ofek,Honglei Zhuang,Itay Laish,Jan Ackermann,Lily Wang,Meg Risdal,Megan Barnes,Michael Fink,Mohamed Amin,Moran Ambar,Natan Potikha,Nikita Gupta,Nitzan Katz,Noam Velan,Ofir Roval,Ori Ram,Polina Zablotskaia,Prathamesh Bang,Priyanka Agrawal,Rakesh Ghiya,Sanjay Ganapathy,Simon Baumgartner,Sofia Erell,Sushant Prakash,Thibault Sellam,Vikram Rao,Xuanhui Wang,Yaroslav Akulov,Yulong Yang,Zhen Yang,Zhixin Lai,Zhongru Wu,Anca Dragan,Avinatan Hassidim,Fernando Pereira,Slav Petrov,Srinivasan Venkatachary,Tulsee Doshi,Yossi Matias,Sasha Goldshtein,Dipanjan Das*

Main category: cs.CL

TL;DR: 介绍FACTS Leaderboard，一套能全面评估语言模型生成准确事实文本能力的在线基准测试套件。


<details>
  <summary>Details</summary>
Motivation: 全面评估语言模型在不同场景下生成事实准确文本的能力。

Method: 通过四个子排行榜（FACTS Multimodal、FACTS Parametric、FACTS Search、FACTS Grounding (v2)）聚合模型表现，各子榜用自动化评判模型打分，最终分数为四部分平均分。

Result: 构建了FACTS Leaderboard Suite，可对模型事实性进行稳健、均衡评估。

Conclusion: 该套件将被积极维护，有公开和私有部分，可在https://www.kaggle.com/benchmarks/google/facts 找到。

Abstract: We introduce The FACTS Leaderboard, an online leaderboard suite and associated set of benchmarks that comprehensively evaluates the ability of language models to generate factually accurate text across diverse scenarios. The suite provides a holistic measure of factuality by aggregating the performance of models on four distinct sub-leaderboards: (1) FACTS Multimodal, which measures the factuality of responses to image-based questions; (2) FACTS Parametric, which assesses models' world knowledge by answering closed-book factoid questions from internal parameters; (3) FACTS Search, which evaluates factuality in information-seeking scenarios, where the model must use a search API; and (4) FACTS Grounding (v2), which evaluates whether long-form responses are grounded in provided documents, featuring significantly improved judge models. Each sub-leaderboard employs automated judge models to score model responses, and the final suite score is an average of the four components, designed to provide a robust and balanced assessment of a model's overall factuality. The FACTS Leaderboard Suite will be actively maintained, containing both public and private splits to allow for external participation while guarding its integrity. It can be found at https://www.kaggle.com/benchmarks/google/facts .

</details>


### [233] [OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification](https://arxiv.org/abs/2512.10756)
*Zijian Wu,Lingkai Kong,Wenwei Zhang,Songyang Gao,Yuzhe Gu,Zhongrui Cai,Tianyou Ma,Yuhong Liu,Zhi Wang,Runyuan Ma,Guangyu Wang,Wei Li,Conghui He,Dahua Lin,Kai Chen*

Main category: cs.CL

TL;DR: 本文提出基于结果的过程验证器OPV，采用迭代主动学习框架提升其验证能力，实验表明OPV性能优越、适用性广。


<details>
  <summary>Details</summary>
Motivation: 当前基于结果的验证器（OVs）无法检查长推理链中不可靠的中间步骤，基于过程的验证器（PVs）因高质量标注稀缺难以可靠检测复杂长推理链中的错误。

Method: 提出Outcome-based Process Verifier (OPV)，采用迭代主动学习框架，用专家注释通过拒绝微调（RFT）和强化学习与可验证奖励（RLVR）逐步提升OPV验证能力。

Result: OPV在OPV - Bench上取得新的最优结果，F1分数达83.1；有效检测合成数据集中的误报；与策略模型协作提升性能，如在AIME2025上将DeepSeek - R1 - Distill - Qwen - 32B的准确率从55.2%提升到73.3%。

Conclusion: OPV具有优越性能和广泛适用性。

Abstract: Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out OPV-Bench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2% to 73.3% on AIME2025 as the compute budget scales.

</details>


### [234] [LabelFusion: Learning to Fuse LLMs and Transformer Classifiers for Robust Text Classification](https://arxiv.org/abs/2512.10793)
*Michael Schlee,Christoph Weisser,Timo Kivimäki,Melchizedek Mashiku,Benjamin Saefken*

Main category: cs.CL

TL;DR: LabelFusion是用于文本分类的融合集成方法，结合传统基于transformer的分类器和大语言模型，有简单接口和灵活API，性能好。


<details>
  <summary>Details</summary>
Motivation: 为文本分类任务提供准确且考虑成本的预测，结合传统分类器和大语言模型的优势。

Method: 通过结构化提示工程策略获取大语言模型的每类分数，与ML骨干的嵌入向量拼接，输入到FusionMLP产生最终预测。

Result: 在AG News上准确率达92.4%，在10类Reuters 21578主题分类上达92.3%。

Conclusion: 该融合方法能捕捉大语言模型推理和传统分类器的互补优势，在不同领域表现稳健，可在准确率、延迟和成本间进行权衡。

Abstract: LabelFusion is a fusion ensemble for text classification that learns to combine a traditional transformer-based classifier (e.g., RoBERTa) with one or more Large Language Models (LLMs such as OpenAI GPT, Google Gemini, or DeepSeek) to deliver accurate and cost-aware predictions across multi-class and multi-label tasks. The package provides a simple high-level interface (AutoFusionClassifier) that trains the full pipeline end-to-end with minimal configuration, and a flexible API for advanced users. Under the hood, LabelFusion integrates vector signals from both sources by concatenating the ML backbone's embeddings with the LLM-derived per-class scores -- obtained through structured prompt-engineering strategies -- and feeds this joint representation into a compact multi-layer perceptron (FusionMLP) that produces the final prediction. This learned fusion approach captures complementary strengths of LLM reasoning and traditional transformer-based classifiers, yielding robust performance across domains -- achieving 92.4% accuracy on AG News and 92.3% on 10-class Reuters 21578 topic classification -- while enabling practical trade-offs between accuracy, latency, and cost.

</details>


### [235] [Script Gap: Evaluating LLM Triage on Indian Languages in Native vs Roman Scripts in a Real World Setting](https://arxiv.org/abs/2512.10780)
*Manurag Khullar,Utkarsh Desai,Poorva Malviya,Aman Dalmia,Zheyuan Ryan Shi*

Main category: cs.CL

TL;DR: 研究印度语境下罗马化对大语言模型在母婴健康分诊领域可靠性的影响，发现罗马化文本会使表现下降并存在安全隐患。


<details>
  <summary>Details</summary>
Motivation: 印度临床应用中人们常用罗马化文本交流，但现有研究很少用真实世界数据评估这种正字法变体，需研究其对大语言模型可靠性的影响。

Method: 在包含五种印度语言和尼泊尔语的真实用户生成查询数据集上对领先的大语言模型进行基准测试。

Result: 罗马化消息的性能出现持续下降，F1分数比原生文字脚本低5 - 12分，可能导致近200万次额外分诊错误。模型能推断语义意图，但面对罗马化输入的正字法噪音时，最终分类输出不稳定。

Conclusion: 基于大语言模型的健康系统存在关键安全盲点，看似理解罗马化输入的模型仍可能无法可靠地据此行动。

Abstract: Large Language Models (LLMs) are increasingly deployed in high-stakes clinical applications in India. In many such settings, speakers of Indian languages frequently communicate using romanized text rather than native scripts, yet existing research rarely evaluates this orthographic variation using real-world data. We investigate how romanization impacts the reliability of LLMs in a critical domain: maternal and newborn healthcare triage. We benchmark leading LLMs on a real-world dataset of user-generated queries spanning five Indian languages and Nepali. Our results reveal consistent degradation in performance for romanized messages, with F1 scores trailing those of native scripts by 5-12 points. At our partner maternal health organization in India, this gap could cause nearly 2 million excess errors in triage. Crucially, this performance gap by scripts is not due to a failure in clinical reasoning. We demonstrate that LLMs often correctly infer the semantic intent of romanized queries. Nevertheless, their final classification outputs remain brittle in the presence of orthographic noise in romanized inputs. Our findings highlight a critical safety blind spot in LLM-based health systems: models that appear to understand romanized input may still fail to act on it reliably.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [236] [Galaxy Phase-Space and Field-Level Cosmology: The Strength of Semi-Analytic Models](https://arxiv.org/abs/2512.10222)
*Natalí S. M. de Santi,Francisco Villaescusa-Navarro,Pablo Araya-Araya,Gabriella De Lucia,Fabio Fontanot,Lucia A. Perez,Manuel Arnés-Curto,Violeta Gonzalez-Perez,Ángel Chandro-Gómez,Rachel S. Somerville,Tiago Castro*

Main category: astro-ph.CO

TL;DR: 利用星系3D位置和径向速度训练图神经网络与矩神经网络，可精确估算物质密度参数，且网络对多种变化有鲁棒性，强化半解析模型生成模拟目录的潜力。


<details>
  <summary>Details</summary>
Motivation: 半解析模型是模拟星系属性常用方法且能生成精确星系目录，本文旨在基于此构建机器学习模型估算物质密度参数。

Method: 使用星系3D位置和径向速度训练图神经网络与矩神经网络。

Result: 网络能以约10%的精度估算物质密度参数$Ω_{m m}$，可成功外推到其他半解析模型和水动力学模拟，且对多种变化有鲁棒性。

Conclusion: 半解析模型相空间中的物理关系很大程度独立于具体物理处方，强化了其作为生成用于宇宙学参数推断的真实模拟目录工具的潜力。

Abstract: Semi-analytic models are a widely used approach to simulate galaxy properties within a cosmological framework, relying on simplified yet physically motivated prescriptions. They have also proven to be an efficient alternative for generating accurate galaxy catalogs, offering a faster and less computationally expensive option compared to full hydrodynamical simulations. In this paper, we demonstrate that using only galaxy $3$D positions and radial velocities, we can train a graph neural network coupled to a moment neural network to obtain a robust machine learning based model capable of estimating the matter density parameters, $Ω_{\rm m}$, with a precision of approximately 10%. The network is trained on ($25 h^{-1}$Mpc)$^3$ volumes of galaxy catalogs from L-Galaxies and can successfully extrapolate its predictions to other semi-analytic models (GAEA, SC-SAM, and Shark) and, more remarkably, to hydrodynamical simulations (Astrid, SIMBA, IllustrisTNG, and SWIFT-EAGLE). Our results show that the network is robust to variations in astrophysical and subgrid physics, cosmological and astrophysical parameters, and the different halo-profile treatments used across simulations. This suggests that the physical relationships encoded in the phase-space of semi-analytic models are largely independent of their specific physical prescriptions, reinforcing their potential as tools for the generation of realistic mock catalogs for cosmological parameter inference.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [237] [Enhancing Fake-News Detection with Node-Level Topological Features](https://arxiv.org/abs/2512.09974)
*Kaiyuan Xu*

Main category: cs.SI

TL;DR: 针对假新闻检测，在原嵌入基础上加图论指标提升效果，还给出融合模板。


<details>
  <summary>Details</summary>
Motivation: 以往假新闻自动化检测方法将图级表示学习全交给GNN，隐藏明确拓扑线索，需改进。

Method: 给每个节点的原BERT和配置嵌入添加度中心性和局部聚类系数这两个经典图论指标。

Result: 在UPFD Politifact子集中，改进后宏观F1值从0.7753提升到0.8344。

Conclusion: 明确拓扑特征在假新闻检测中有实用价值，还为其他信息传播任务融合图指标提供可解释、易复现的模板。

Abstract: In recent years, the proliferation of misinformation and fake news has posed serious threats to individuals and society, spurring intense research into automated detection methods. Previous work showed that integrating content, user preferences, and propagation structure achieves strong performance, but leaves all graph-level representation learning entirely to the GNN, hiding any explicit topological cues. To close this gap, we introduce a lightweight enhancement: for each node, we append two classical graph-theoretic metrics, degree centrality and local clustering coefficient, to its original BERT and profile embeddings, thus explicitly flagging the roles of hub and community. In the UPFD Politifact subset, this simple modification boosts macro F1 from 0.7753 to 0.8344 over the original baseline. Our study not only demonstrates the practical value of explicit topology features in fake-news detection but also provides an interpretable, easily reproducible template for fusing graph metrics in other information-diffusion tasks.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [238] [Hyperspectral Image Data Reduction for Endmember Extraction](https://arxiv.org/abs/2512.10506)
*Tomohiko Mizutani*

Main category: eess.IV

TL;DR: 本文提出数据降维自字典方法，在不牺牲端元提取精度的前提下大幅减少原自字典方法计算时间。


<details>
  <summary>Details</summary>
Motivation: 现有自字典方法计算成本高，限制其在大规模高光谱图像中的应用，虽有缓解方法但挑战仍在。

Method: 假设高光谱图像遵循线性混合模型和纯像素假设，开发数据降维技术移除不包含端元的像素，分析降维步骤理论性质并保留靠近端元的像素，将数据降维与基于线性规划的自字典方法结合。

Result: 数值实验表明，所提方法能大幅减少原自字典方法的计算时间，且不牺牲端元提取精度。

Conclusion: 所提数据降维自字典方法有效解决了自字典方法计算成本高的问题。

Abstract: Endmember extraction from hyperspectral images aims to identify the spectral signatures of materials present in a scene. Recent studies have shown that self-dictionary methods can achieve high extraction accuracy; however, their high computational cost limits their applicability to large-scale hyperspectral images. Although several approaches have been proposed to mitigate this issue, it remains a major challenge. Motivated by this situation, this paper pursues a data reduction approach. Assuming that the hyperspectral image follows the linear mixing model with the pure-pixel assumption, we develop a data reduction technique that removes pixels that do not contain endmembers. We analyze the theoretical properties of this reduction step and show that it preserves pixels that lie close to the endmembers. Building on this result, we propose a data-reduced self-dictionary method that integrates the data reduction with a self-dictionary method based on a linear programming formulation. Numerical experiments demonstrate that the proposed method can substantially reduce the computational time of the original self-dictionary method without sacrificing endmember extraction accuracy.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [239] [Inference for Batched Adaptive Experiments](https://arxiv.org/abs/2512.10156)
*Jan Kemper,Davud Rostam-Afschar*

Main category: econ.EM

TL;DR: 本文提出用于自适应实验因果推断的BOLS检验统计量，并给出模拟结果


<details>
  <summary>Details</summary>
Motivation: 自适应实验在应用中虽广泛但给因果推断带来挑战，需要合适的方法进行处理效应推断

Method: 提出BOLS检验统计量，该统计量能在异方差下对各期处理 - 对照差异进行精度均衡聚合，结合的检验统计量是异方差各期z统计量的标准化平均值

Result: 提供了在处理期少且每批观测少（或多）的典型情况下的拒绝率模拟结果

Conclusion: 所提出的BOLS检验统计量可用于构建渐近有效的置信区间

Abstract: The advantages of adaptive experiments have led to their rapid adoption in economics, other fields, as well as among practitioners. However, adaptive experiments pose challenges for causal inference. This note suggests a BOLS (batched ordinary least squares) test statistic for inference of treatment effects in adaptive experiments. The statistic provides a precision-equalizing aggregation of per-period treatment-control differences under heteroskedasticity. The combined test statistic is a normalized average of heteroskedastic per-period z-statistics and can be used to construct asymptotically valid confidence intervals. We provide simulation results comparing rejection rates in the typical case with few treatment periods and few (or many) observations per batch.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [240] [Residual subspace evolution strategies for nonlinear inverse problems](https://arxiv.org/abs/2512.10325)
*Francesco Alemanno*

Main category: math.OC

TL;DR: 本文提出无导数求解器RSES，在多问题上表现良好，尤其在平滑性或协方差假设不成立时优势明显。


<details>
  <summary>Details</summary>
Motivation: 现有非线性逆问题的基于雅可比矩阵的求解器不可靠，无导数优化器有平滑性假设或需大量评估，集合卡尔曼反演有协方差相关问题。

Method: 引入残差子空间进化策略（RSES），在当前迭代点周围采样高斯样本，构建仅含残差的代理模型，通过最小二乘法求解更新。

Result: 在多个问题的基准测试中，RSES能稳定降低误差，在相同评估预算下，性能匹配或超过xNES和NEWUOA，与EKI具有竞争力。

Conclusion: RSES是一种有效的无导数求解器，尤其在平滑性或协方差假设不成立的场景中表现出色。

Abstract: Nonlinear inverse problems often feature noisy, non-differentiable, or expensive residual evaluations that make Jacobian-based solvers unreliable. Popular derivative-free optimizers such as natural evolution strategies (NES) or Powell's NEWUOA still assume smoothness or expend many evaluations to maintain stability. Ensemble Kalman inversion (EKI) relies on empirical covariances that require preconditioning and scale poorly with residual dimension.
  We introduce residual subspace evolution strategies (RSES), a derivative-free solver that samples Gaussian probes around the current iterate, builds a residual-only surrogate from their differences, and recombines the probes through a least-squares solve yielding an optimal update without forming Jacobians or covariances. Each iteration costs $k+1$ residual evaluations, where $k \ll n$ for $n$-dimensional problems, with $O(k^3)$ linear algebra overhead.
  Benchmarks on calibration, regression, and deconvolution problems demonstrate consistent misfit reduction in both deterministic and stochastic settings. RSES matches or surpasses xNES and NEWUOA while staying competitive with EKI under matched evaluation budgets, particularly when smoothness or covariance assumptions fail.

</details>


### [241] [Distributionally Robust Regret Optimal Control Under Moment-Based Ambiguity Sets](https://arxiv.org/abs/2512.10906)
*Feras Al Taha,Eilyan Bitar*

Main category: math.OC

TL;DR: 本文研究有限时域线性二次随机控制问题，考虑分布不确定性，提出可扩展对偶投影次梯度法求解最优控制器并进行数值实验。


<details>
  <summary>Details</summary>
Motivation: 解决噪声过程概率分布未知且存在分布不确定性的有限时域线性二次随机控制问题，同时克服现有半定规划求解方法对问题规模扩展性差的局限。

Method: 设计因果仿射控制策略，将极小极大最优控制问题转化为可处理的凸规划问题，提出可扩展对偶投影次梯度法计算最优控制器。

Result: 得到与标称线性二次随机控制问题正则化版本对应的凸规划，提出可求解最优控制器到任意精度的方法。

Conclusion: 通过数值实验对比所提方法与最先进的数据驱动和分布鲁棒控制设计方法。

Abstract: In this paper, we consider a class of finite-horizon, linear-quadratic stochastic control problems, where the probability distribution governing the noise process is unknown but assumed to belong to an ambiguity set consisting of all distributions whose mean and covariance lie within norm balls centered at given nominal values. To address the distributional ambiguity, we explore the design of causal affine control policies to minimize the worst-case expected regret over all distributions in the given ambiguity set. The resulting minimax optimal control problem is shown to admit an equivalent reformulation as a tractable convex program that corresponds to a regularized version of the nominal linear-quadratic stochastic control problem. While this convex program can be recast as a semidefinite program, semidefinite programs are typically solved using primal-dual interior point methods that scale poorly with the problem size in practice. To address this limitation, we propose a scalable dual projected subgradient method to compute optimal controllers to an arbitrary accuracy. Numerical experiments are presented to benchmark the proposed method against state-of-the-art data-driven and distributionally robust control design approaches.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [242] [QSTAformer: A Quantum-Enhanced Transformer for Robust Short-Term Voltage Stability Assessment against Adversarial Attacks](https://arxiv.org/abs/2512.09936)
*Yang Li,Chong Ma,Yuanzheng Li,Sen Li,Yanbo Chen,Zhaoyang Dong*

Main category: eess.SY

TL;DR: 提出QSTAformer用于鲁棒高效的短期电压稳定性评估，并采用对抗训练策略，案例表明其有竞争力。


<details>
  <summary>Details</summary>
Motivation: 经典机器学习方法在对抗条件下的短期电压稳定性评估（STVSA）中鲁棒性存在挑战。

Method: 提出将参数化量子电路嵌入注意力机制的QSTAformer架构，开发对抗训练策略，对不同PQC架构进行基准测试。

Result: 在IEEE 39 - 母线系统案例中，QSTAformer实现了有竞争力的准确性、更低的复杂度和更强的鲁棒性。

Conclusion: QSTAformer在对抗条件下的STVSA有实现安全和可扩展的潜力。

Abstract: Short-term voltage stability assessment (STVSA) is critical for secure power system operation. While classical machine learning-based methods have demonstrated strong performance, they still face challenges in robustness under adversarial conditions. This paper proposes QSTAformer-a tailored quantum-enhanced Transformer architecture that embeds parameterized quantum circuits (PQCs) into attention mechanisms-for robust and efficient STVSA. A dedicated adversarial training strategy is developed to defend against both white-box and gray-box attacks. Furthermore, diverse PQC architectures are benchmarked to explore trade-offs between expressiveness, convergence, and efficiency. To the best of our knowledge, this is the first work to systematically investigate the adversarial vulnerability of quantum machine learning-based STVSA. Case studies on the IEEE 39-bus system demonstrate that QSTAformer achieves competitive accuracy, reduced complexity, and stronger robustness, underscoring its potential for secure and scalable STVSA under adversarial conditions.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [243] [Neuromorphic Eye Tracking for Low-Latency Pupil Detection](https://arxiv.org/abs/2512.09969)
*Paul Hueber,Luca Peres,Florian Pitters,Alejandro Gloriani,Oliver Rhodes*

Main category: cs.CV

TL;DR: 本文提出基于神经形态的事件驱动眼动追踪模型，在保证精度的同时大幅提升效率，适用于实时可穿戴设备。


<details>
  <summary>Details</summary>
Motivation: 传统帧处理管道在可穿戴系统眼动追踪中存在问题，现有SNN方法有局限性，为新兴技术如AR、VR提供低延迟、低功耗且准确的眼动追踪方案。

Method: 将表现优异的事件驱动眼动追踪模型神经形态化，用轻量级LIF层替代循环和注意力模块，利用深度可分离卷积降低模型复杂度。

Result: 模型平均误差3.7 - 4.1px，接近Retina系统，模型大小缩小20倍，理论计算量减少850倍，预计功耗3.9 - 4.9 mW，延迟3 ms，频率1 kHz。

Conclusion: 高性能事件驱动眼动追踪架构可重新设计为SNN，在保证精度的同时显著提高效率，适用于实时可穿戴部署。

Abstract: Eye tracking for wearable systems demands low latency and milliwatt-level power, but conventional frame-based pipelines struggle with motion blur, high compute cost, and limited temporal resolution. Such capabilities are vital for enabling seamless and responsive interaction in emerging technologies like augmented reality (AR) and virtual reality (VR), where understanding user gaze is key to immersion and interface design. Neuromorphic sensors and spiking neural networks (SNNs) offer a promising alternative, yet existing SNN approaches are either too specialized or fall short of the performance of modern ANN architectures. This paper presents a neuromorphic version of top-performing event-based eye-tracking models, replacing their recurrent and attention modules with lightweight LIF layers and exploiting depth-wise separable convolutions to reduce model complexity. Our models obtain 3.7-4.1px mean error, approaching the accuracy of the application-specific neuromorphic system, Retina (3.24px), while reducing model size by 20x and theoretical compute by 850x, compared to the closest ANN variant of the proposed model. These efficient variants are projected to operate at an estimated 3.9-4.9 mW with 3 ms latency at 1 kHz. The present results indicate that high-performing event-based eye-tracking architectures can be redesigned as SNNs with substantial efficiency gains, while retaining accuracy suitable for real-time wearable deployment.

</details>


### [244] [Benchmarking Document Parsers on Mathematical Formula Extraction from PDFs](https://arxiv.org/abs/2512.09874)
*Pius Horn,Janis Keuper*

Main category: cs.CV

TL;DR: 提出新的PDF数学公式解析基准框架，用LLM评估公式语义，评估20+解析器，证明LLM评估与人类判断相关性高，为从业者选解析器提供见解。


<details>
  <summary>Details</summary>
Motivation: 现有PDF数学公式解析基准存在要么排除公式，要么缺乏语义感知评估指标的问题。

Method: 引入基于合成PDF的基准框架，用LLM作为评估公式语义的裁判，结合两阶段匹配流程处理解析器输出不一致问题。

Result: 通过人类验证，LLM评估与人类判断相关性高达0.78，高于CDM和文本相似性；评估20+解析器发现性能差异大。

Conclusion: 研究为从业者选解析器提供见解，建立了可重复评估PDF公式提取质量的方法。

Abstract: Correctly parsing mathematical formulas from PDFs is critical for training large language models and building scientific knowledge bases from academic literature, yet existing benchmarks either exclude formulas entirely or lack semantically-aware evaluation metrics. We introduce a novel benchmarking framework centered on synthetically generated PDFs with precise LaTeX ground truth, enabling systematic control over layout, formulas, and content characteristics. A key methodological contribution is pioneering LLM-as-a-judge for semantic formula assessment, combined with a robust two-stage matching pipeline that handles parser output inconsistencies. Through human validation on 250 formula pairs (750 ratings from 30 evaluators), we demonstrate that LLM-based evaluation achieves substantially higher correlation with human judgment (Pearson r=0.78) compared to CDM (r=0.34) and text similarity (r~0). Evaluating 20+ contemporary PDF parsers (including specialized OCR models, vision-language models, and rule-based approaches) across 100 synthetic documents with 2,000+ formulas reveals significant performance disparities. Our findings provide crucial insights for practitioners selecting parsers for downstream applications and establish a robust, scalable methodology that enables reproducible evaluation of PDF formula extraction quality. Code and benchmark data: https://github.com/phorn1/pdf-parse-bench

</details>


### [245] [What matters for Representation Alignment: Global Information or Spatial Structure?](https://arxiv.org/abs/2512.10794)
*Jaskirat Singh,Xingjian Leng,Zongze Wu,Liang Zheng,Richard Zhang,Eli Shechtman,Saining Xie*

Main category: cs.CV

TL;DR: 本文研究目标表征的全局语义信息和空间结构对生成的影响，发现空间结构驱动生成性能，提出iREPA方法提升收敛速度。


<details>
  <summary>Details</summary>
Motivation: 探究目标表征中对生成起关键作用的是全局语义信息还是空间结构。

Method: 对27种不同视觉编码器进行大规模实证分析，用简单卷积层替换REPA的标准MLP投影层并引入空间归一化层以突出空间信息传递。

Result: 空间结构而非全局性能驱动目标表征的生成性能，iREPA能在多种视觉编码器、模型大小和训练变体中提升REPA的收敛速度。

Conclusion: 需重新审视表征对齐的基本工作机制及如何用于改进生成模型的训练。

Abstract: Representation alignment (REPA) guides generative training by distilling representations from a strong, pretrained vision encoder to intermediate diffusion features. We investigate a fundamental question: what aspect of the target representation matters for generation, its \textit{global} \revision{semantic} information (e.g., measured by ImageNet-1K accuracy) or its spatial structure (i.e. pairwise cosine similarity between patch tokens)? Prevalent wisdom holds that stronger global semantic performance leads to better generation as a target representation. To study this, we first perform a large-scale empirical analysis across 27 different vision encoders and different model scales. The results are surprising; spatial structure, rather than global performance, drives the generation performance of a target representation. To further study this, we introduce two straightforward modifications, which specifically accentuate the transfer of \emph{spatial} information. We replace the standard MLP projection layer in REPA with a simple convolution layer and introduce a spatial normalization layer for the external representation. Surprisingly, our simple method (implemented in $<$4 lines of code), termed iREPA, consistently improves convergence speed of REPA, across a diverse set of vision encoders, model sizes, and training variants (such as REPA, REPA-E, Meanflow, JiT etc). %, etc. Our work motivates revisiting the fundamental working mechanism of representational alignment and how it can be leveraged for improved training of generative models. The code and project page are available at https://end2end-diffusion.github.io/irepa

</details>


### [246] [ABBSPO: Adaptive Bounding Box Scaling and Symmetric Prior based Orientation Prediction for Detecting Aerial Image Objects](https://arxiv.org/abs/2512.10031)
*Woojin Lee,Hyugjae Chang,Jaeho Moon,Jaehyup Lee,Munchurl Kim*

Main category: cs.CV

TL;DR: 本文提出用于弱监督定向目标检测的ABBSPO框架，解决此前方法局限，实验显示其性能达到了当前最优。


<details>
  <summary>Details</summary>
Motivation: 现有水平边界框监督的弱监督定向目标检测方法存在尺度估计不准确和学习崩溃问题，需要改进。

Method: 提出自适应边界框缩放（ABBS）优化尺度预测，以及对称先验角（SPA）损失用于自监督学习。

Result: ABBSPO在实验中取得了当前最优性能，超越了现有方法。

Conclusion: ABBSPO框架有效解决了现有HBox - 监督OOD方法的局限，性能优越。

Abstract: Weakly supervised oriented object detection (WS-OOD) has gained attention as a cost-effective alternative to fully supervised methods, providing both efficiency and high accuracy. Among weakly supervised approaches, horizontal bounding box (HBox)-supervised OOD stands out for its ability to directly leverage existing HBox annotations while achieving the highest accuracy under weak supervision settings. This paper introduces adaptive bounding box scaling and symmetry-prior-based orientation prediction, called ABBSPO, a framework for WS-OOD. Our ABBSPO addresses limitations of previous HBox-supervised OOD methods, which compare ground truth (GT) HBoxes directly with the minimum circumscribed rectangles of predicted RBoxes, often leading to inaccurate scale estimation. To overcome this, we propose: (i) Adaptive Bounding Box Scaling (ABBS), which appropriately scales GT HBoxes to optimize for the size of each predicted RBox, ensuring more accurate scale prediction; and (ii) a Symmetric Prior Angle (SPA) loss that exploits inherent symmetry of aerial objects for self-supervised learning, resolving issues in previous methods where learning collapses when predictions for all three augmented views (original, rotated, and flipped) are consistently incorrect. Extensive experimental results demonstrate that ABBSPO achieves state-of-the-art performance, outperforming existing methods.

</details>


### [247] [MetaVoxel: Joint Diffusion Modeling of Imaging and Clinical Metadata](https://arxiv.org/abs/2512.10041)
*Yihao Liu,Chenyu Gao,Lianrui Zuo,Michael E. Kim,Brian D. Boyd,Lisa L. Barnes,Walter A. Kukull,Lori L. Beason-Held,Susan M. Resnick,Timothy J. Hohman,Warren D. Taylor,Bennett A. Landman*

Main category: cs.CV

TL;DR: 介绍MetaVoxel生成联合扩散建模框架，可统一医疗AI任务，实验证明其性能和灵活推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法多针对特定预测方向和输入变量训练，需统一模型。

Method: 引入MetaVoxel框架，学习单一扩散过程来建模成像数据和临床元数据的联合分布。

Result: 用超10000个T1加权MRI扫描和临床元数据实验，单MetaVoxel模型能完成多任务，性能与特定基线相当。

Conclusion: 联合多模态扩散为统一医疗AI模型和扩大临床应用提供了有前景的方向。

Abstract: Modern deep learning methods have achieved impressive results across tasks from disease classification, estimating continuous biomarkers, to generating realistic medical images. Most of these approaches are trained to model conditional distributions defined by a specific predictive direction with a specific set of input variables. We introduce MetaVoxel, a generative joint diffusion modeling framework that models the joint distribution over imaging data and clinical metadata by learning a single diffusion process spanning all variables. By capturing the joint distribution, MetaVoxel unifies tasks that traditionally require separate conditional models and supports flexible zero-shot inference using arbitrary subsets of inputs without task-specific retraining. Using more than 10,000 T1-weighted MRI scans paired with clinical metadata from nine datasets, we show that a single MetaVoxel model can perform image generation, age estimation, and sex prediction, achieving performance comparable to established task-specific baselines. Additional experiments highlight its capabilities for flexible inference.Together, these findings demonstrate that joint multimodal diffusion offers a promising direction for unifying medical AI models and enabling broader clinical applicability.

</details>


### [248] [RobustSora: De-Watermarked Benchmark for Robust AI-Generated Video Detection](https://arxiv.org/abs/2512.10248)
*Zhuo Wang,Xiliang Liu,Ligang Sun*

Main category: cs.CV

TL;DR: 介绍RobustSora基准评估AIGC视频检测水印鲁棒性，构建数据集评估，实验显示模型对水印有依赖，强调需水印感知训练策略。


<details>
  <summary>Details</summary>
Motivation: 现有AIGC视频检测基准忽略了生成模型输出嵌入水印，探测器可能部分依赖水印模式的影响，需评估该影响。

Method: 构建包含6500个视频的数据集，设计两项评估任务（Task-I测试去除水印的AI视频性能，Task-II评估含假水印真实视频的误报率），用十种模型进行实验。

Result: 实验显示水印操作下模型性能有2 - 8pp的变化，Transformer模型有中度依赖，MLLM模式多样。

Conclusion: 模型对水印存在部分依赖，需要水印感知训练策略，RobustSora为推进AIGC检测研究提供工具。

Abstract: The proliferation of AI-generated video technologies poses challenges to information integrity. While recent benchmarks advance AIGC video detection, they overlook a critical factor: many state-of-the-art generative models embed digital watermarks in outputs, and detectors may partially rely on these patterns. To evaluate this influence, we present RobustSora, the benchmark designed to assess watermark robustness in AIGC video detection. We systematically construct a dataset of 6,500 videos comprising four types: Authentic-Clean (A-C), Authentic-Spoofed with fake watermarks (A-S), Generated-Watermarked (G-W), and Generated-DeWatermarked (G-DeW). Our benchmark introduces two evaluation tasks: Task-I tests performance on watermark-removed AI videos, while Task-II assesses false alarm rates on authentic videos with fake watermarks. Experiments with ten models spanning specialized AIGC detectors, transformer architectures, and MLLM approaches reveal performance variations of 2-8pp under watermark manipulation. Transformer-based models show consistent moderate dependency (6-8pp), while MLLMs exhibit diverse patterns (2-8pp). These findings indicate partial watermark dependency and highlight the need for watermark-aware training strategies. RobustSora provides essential tools to advance robust AIGC detection research.

</details>


### [249] [Independent Density Estimation](https://arxiv.org/abs/2512.10067)
*Jiahao Liu*

Main category: cs.CV

TL;DR: 提出IDE方法解决大模型组合泛化难题，构建两个模型并结合推理方法，在多数据集上表现更优。


<details>
  <summary>Details</summary>
Motivation: 大尺度视觉语言模型在组合泛化方面存在困难，需新方法解决。

Method: 提出独立密度估计（IDE）方法，构建两个基于IDE理念的模型，提出基于熵的组合推理方法。

Result: 模型在多个数据集上对未见组合的泛化能力优于当前模型。

Conclusion: IDE方法能有效提升视觉语言模型的组合泛化能力。

Abstract: Large-scale Vision-Language models have achieved remarkable results in various domains, such as image captioning and conditioned image generation. Nevertheless, these models still encounter difficulties in achieving human-like compositional generalization. In this study, we propose a new method called Independent Density Estimation (IDE) to tackle this challenge. IDE aims to learn the connection between individual words in a sentence and the corresponding features in an image, enabling compositional generalization. We build two models based on the philosophy of IDE. The first one utilizes fully disentangled visual representations as input, and the second leverages a Variational Auto-Encoder to obtain partially disentangled features from raw images. Additionally, we propose an entropy-based compositional inference method to combine predictions of each word in the sentence. Our models exhibit superior generalization to unseen compositions compared to current models when evaluated on various datasets.

</details>


### [250] [MotionEdit: Benchmarking and Learning Motion-Centric Image Editing](https://arxiv.org/abs/2512.10284)
*Yixin Wan,Lei Ke,Wenhao Yu,Kai-Wei Chang,Dong Yu*

Main category: cs.CV

TL;DR: 介绍MotionEdit数据集和MotionEdit - Bench基准测试，指出现有模型在运动编辑上的挑战，提出MotionNFT框架并证明其有效性


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑数据集缺乏高质量运动编辑数据，新的运动中心图像编辑任务有科学挑战和实际意义，需评估模型并提升性能

Method: 引入MotionEdit数据集和MotionEdit - Bench基准测试，提出MotionNFT后训练框架计算运动对齐奖励引导模型

Result: 基准测试显示现有扩散编辑模型在运动编辑上有挑战，MotionNFT在多个模型上提升了运动编辑质量和保真度且不牺牲通用编辑能力

Conclusion: MotionNFT框架在运动编辑任务中有效

Abstract: We introduce MotionEdit, a novel dataset for motion-centric image editing-the task of modifying subject actions and interactions while preserving identity, structure, and physical plausibility. Unlike existing image editing datasets that focus on static appearance changes or contain only sparse, low-quality motion edits, MotionEdit provides high-fidelity image pairs depicting realistic motion transformations extracted and verified from continuous videos. This new task is not only scientifically challenging but also practically significant, powering downstream applications such as frame-controlled video synthesis and animation.
  To evaluate model performance on the novel task, we introduce MotionEdit-Bench, a benchmark that challenges models on motion-centric edits and measures model performance with generative, discriminative, and preference-based metrics. Benchmark results reveal that motion editing remains highly challenging for existing state-of-the-art diffusion-based editing models. To address this gap, we propose MotionNFT (Motion-guided Negative-aware Fine Tuning), a post-training framework that computes motion alignment rewards based on how well the motion flow between input and model-edited images matches the ground-truth motion, guiding models toward accurate motion transformations. Extensive experiments on FLUX.1 Kontext and Qwen-Image-Edit show that MotionNFT consistently improves editing quality and motion fidelity of both base models on the motion editing task without sacrificing general editing ability, demonstrating its effectiveness.

</details>


### [251] [Visual Funnel: Resolving Contextual Blindness in Multimodal Large Language Models](https://arxiv.org/abs/2512.10362)
*Woojun Jung,Jaehoon Go,Mingyu Jeon,Sunjae Yoon,Junyeong Kim*

Main category: cs.CV

TL;DR: 多模态大语言模型存细节感知问题及‘上下文盲目’局限，提出免训练的Visual Funnel方法解决，实验表明其表现更优且强调分层结构重要性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型难以感知图像细粒度视觉细节，现有裁剪显著区域方法存在‘上下文盲目’问题。

Method: 提出Visual Funnel方法，先通过上下文锚定确定感兴趣区域，再构建基于注意力熵动态确定裁剪大小和细化裁剪中心的熵尺度组合。

Result: Visual Funnel显著优于单裁剪和无结构多裁剪基线，增加无结构裁剪益处有限甚至有害。

Conclusion: 模型输入的‘结构多样性’不足导致‘上下文盲目’，Visual Funnel的分层结构是解决该问题的关键。

Abstract: Multimodal Large Language Models (MLLMs) demonstrate impressive reasoning capabilities, but often fail to perceive fine-grained visual details, limiting their applicability in precision-demanding tasks. While methods that crop salient regions of an image offer a partial solution, we identify a critical limitation they introduce: "Contextual Blindness". This failure occurs due to structural disconnect between high-fidelity details (from the crop) and the broader global context (from the original image), even when all necessary visual information is present. We argue that this limitation stems not from a lack of information 'Quantity', but from a lack of 'Structural Diversity' in the model's input. To resolve this, we propose Visual Funnel, a training-free, two-step approach. Visual Funnel first performs Contextual Anchoring to identify the region of interest in a single forward pass. It then constructs an Entropy-Scaled Portfolio that preserves the hierarchical context - ranging from focal detail to broader surroundings - by dynamically determining crop sizes based on attention entropy and refining crop centers. Through extensive experiments, we demonstrate that Visual Funnel significantly outperforms naive single-crop and unstructured multi-crop baselines. Our results further validate that simply adding more unstructured crops provides limited or even detrimental benefits, confirming that the hierarchical structure of our portfolio is key to resolving Contextual Blindness.

</details>


### [252] [Towards Fine-Grained Recognition with Large Visual Language Models: Benchmark and Optimization Strategies](https://arxiv.org/abs/2512.10384)
*Cong Pang,Hongtao Yu,Zixuan Chen,Lewei Lu,Xin Lou*

Main category: cs.CV

TL;DR: 本文引入FROW基准评估大视觉语言模型细粒度识别能力，提出优化策略，实验表明数据有效提升模型性能，基准将开源。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注推理任务，忽视细粒度识别，而其对实际应用场景至关重要。

Method: 引入FROW基准，从数据构建和训练过程两方面提出优化策略，数据集包含拼接数据和开放世界数据。

Result: 拼接数据使类别识别准确率提高1%，开放世界数据使FROW基准准确率提高10%-20%、内容准确率提高6%-12%，预训练阶段加入细粒度数据使类别识别准确率最高提高10%。

Conclusion: 所提出的基准和优化策略能有效提升大视觉语言模型的细粒度识别性能。

Abstract: Large Vision Language Models (LVLMs) have made remarkable progress, enabling sophisticated vision-language interaction and dialogue applications. However, existing benchmarks primarily focus on reasoning tasks, often neglecting fine-grained recognition, which is crucial for practical application scenarios. To address this gap, we introduce the Fine-grained Recognition Open World (FROW) benchmark, designed for detailed evaluation of LVLMs with GPT-4o. On the basis of that, we propose a novel optimization strategy from two perspectives: \textit{data construction} and \textit{training process}, to improve the performance of LVLMs. Our dataset includes mosaic data, which combines multiple short-answer responses, and open-world data, generated from real-world questions and answers using GPT-4o, creating a comprehensive framework for evaluating fine-grained recognition in LVLMs. Experiments show that mosaic data improves category recognition accuracy by 1\% and open-world data boosts FROW benchmark accuracy by 10\%-20\% and content accuracy by 6\%-12\%. Meanwhile, incorporating fine-grained data into the pre-training phase can improve the model's category recognition accuracy by up to 10\%. The benchmark will be available at https://github.com/pc-inno/FROW.

</details>


### [253] [Solving Semi-Supervised Few-Shot Learning from an Auto-Annotation Perspective](https://arxiv.org/abs/2512.10244)
*Tian Liu,Anwesha Basu,James Caverlee,Shu Kong*

Main category: cs.CV

TL;DR: 本文提出SWIFT方法解决半监督少样本学习中未有效利用开源资源问题，在五个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 半监督少样本学习（SSFSL）文献大多忽视开源视觉语言模型（VLMs）及其预训练数据，为实现现实的自动标注，SSFSL应利用这些资源。

Method: 先应用既定SSL方法微调VLM，经分析发现问题后采用分类器初始化和温度调整技术，提出Stage - Wise Finetuning with Temperature Tuning (SWIFT)方法。

Result: 在五个SSFSL基准测试中，SWIFT比近期FSL和SSL方法准确率高约5个百分点，甚至可与有真实标签监督学习相媲美。

Conclusion: SWIFT能使现有SSL方法在有限标签数据、大量无标签数据和任务相关的噪声数据上有效微调VLM。

Abstract: Semi-supervised few-shot learning (SSFSL) formulates real-world applications like ''auto-annotation'', as it aims to learn a model over a few labeled and abundant unlabeled examples to annotate the unlabeled ones. Despite the availability of powerful open-source Vision-Language Models (VLMs) and their pretraining data, the SSFSL literature largely neglects these open-source resources. In contrast, the related area few-shot learning (FSL) has already exploited them to boost performance. Arguably, to achieve auto-annotation in the real world, SSFSL should leverage such open-source resources. To this end, we start by applying established SSL methods to finetune a VLM. Counterintuitively, they significantly underperform FSL baselines. Our in-depth analysis reveals the root cause: VLMs produce rather ''flat'' distributions of softmax probabilities. This results in zero utilization of unlabeled data and weak supervision signals. We address this issue with embarrassingly simple techniques: classifier initialization and temperature tuning. They jointly increase the confidence scores of pseudo-labels, improving the utilization rate of unlabeled data, and strengthening supervision signals. Building on this, we propose: Stage-Wise Finetuning with Temperature Tuning (SWIFT), which enables existing SSL methods to effectively finetune a VLM on limited labeled data, abundant unlabeled data, and task-relevant but noisy data retrieved from the VLM's pretraining set. Extensive experiments on five SSFSL benchmarks show that SWIFT outperforms recent FSL and SSL methods by $\sim$5 accuracy points. SWIFT even rivals supervised learning, which finetunes VLMs with the unlabeled data being labeled with ground truth!

</details>


### [254] [Beyond Endpoints: Path-Centric Reasoning for Vectorized Off-Road Network Extraction](https://arxiv.org/abs/2512.10416)
*Wenfei Guan,Jilin Mei,Tong Shen,Xumin Wu,Shuo Wang,Cheng Min,Yu Hu*

Main category: cs.CV

TL;DR: 本文针对越野环境道路提取难题，发布WildRoad数据集并提出MaGRoad框架，取得SOTA效果且推理更快。


<details>
  <summary>Details</summary>
Motivation: 深度学习在越野环境道路提取研究不足，现有模型因缺乏大规模数据集和方法结构弱点，在越野场景易失败。

Method: 一是发布用专用交互注释工具构建的WildRoad全球越野道路网络数据集；二是引入以路径为中心的MaGRoad框架，沿候选路径聚合多尺度视觉证据来推断连通性。

Result: MaGRoad在WildRoad基准测试中达到SOTA性能，对城市数据集泛化性好，推理速度约快2.5倍。

Conclusion: 数据集和以路径为中心的范式为野外道路映射提供了更坚实基础。

Abstract: Deep learning has advanced vectorized road extraction in urban settings, yet off-road environments remain underexplored and challenging. A significant domain gap causes advanced models to fail in wild terrains due to two key issues: lack of large-scale vectorized datasets and structural weakness in prevailing methods. Models such as SAM-Road employ a node-centric paradigm that reasons at sparse endpoints, making them fragile to occlusions and ambiguous junctions in off-road scenes, leading to topological errors.This work addresses these limitations in two complementary ways. First, we release WildRoad, a gloabal off-road road network dataset constructed efficiently with a dedicated interactive annotation tool tailored for road-network labeling. Second, we introduce MaGRoad (Mask-aware Geodesic Road network extractor), a path-centric framework that aggregates multi-scale visual evidence along candidate paths to infer connectivity robustly.Extensive experiments show that MaGRoad achieves state-of-the-art performance on our challenging WildRoad benchmark while generalizing well to urban datasets. A streamlined pipeline also yields roughly 2.5x faster inference, improving practical applicability. Together, the dataset and path-centric paradigm provide a stronger foundation for mapping roads in the wild.

</details>


### [255] [An M-Health Algorithmic Approach to Identify and Assess Physiotherapy Exercises in Real Time](https://arxiv.org/abs/2512.10437)
*Stylianos Kandylakis,Christos Orfanopoulos,Georgios Siolas,Panayiotis Tsanakas*

Main category: cs.CV

TL;DR: 提出用移动设备对人体理疗运动进行实时识别、分类和评估的算法框架，系统在客户端运行，实验验证其有效性和适用性。


<details>
  <summary>Details</summary>
Motivation: 实现用移动设备对人体理疗运动的实时识别、分类和评估，用于远程理疗监督和移动健康应用。

Method: 将运动解释为静态姿势序列，用姿态估计神经网络从相机输入估计姿势；提取身体关键点转化为三角角度特征，用轻量级监督模型分类；用基于改进的Levenshtein距离算法的动态规划方案识别完整运动并检测偏差。

Result: 系统能在客户端运行，保证可扩展性和实时性能。

Conclusion: 该方法有效，适用于远程理疗监督和移动健康应用。

Abstract: This work presents an efficient algorithmic framework for real-time identification, classification, and evaluation of human physiotherapy exercises using mobile devices. The proposed method interprets a kinetic movement as a sequence of static poses, which are estimated from camera input using a pose-estimation neural network. Extracted body keypoints are transformed into trigonometric angle-based features and classified with lightweight supervised models to generate frame-level pose predictions and accuracy scores. To recognize full exercise movements and detect deviations from prescribed patterns, we employ a dynamic-programming scheme based on a modified Levenshtein distance algorithm, enabling robust sequence matching and localization of inaccuracies. The system operates entirely on the client side, ensuring scalability and real-time performance. Experimental evaluation demonstrates the effectiveness of the methodology and highlights its applicability to remote physiotherapy supervision and m-health applications.

</details>


### [256] [Beyond Pixels: A Training-Free, Text-to-Text Framework for Remote Sensing Image Retrieval](https://arxiv.org/abs/2512.10596)
*J. Xiao,Y. Guo,X. Zi,K. Thiyagarajan,C. Moreira,M. Prasad*

Main category: cs.CV

TL;DR: 引入RSRT数据集，提出训练免费的TRSLLaVA方法用于遥感图像语义检索，实验效果佳。


<details>
  <summary>Details</summary>
Motivation: 解决语义检索的语义鸿沟问题，弥补现有方法依赖特定训练和缺乏零样本检索基准的不足。

Method: 引入RSRT数据集，提出TRSLLaVA方法，将跨模态检索转化为文本匹配问题，在统一文本嵌入空间里计算。

Result: 在RSITMD和RSICD基准测试中训练免费方法与先进监督模型竞争力高，如在RSITMD上平均召回率42.62%，近两倍于标准零样本CLIP基线。

Conclusion: 通过结构化文本的高质量语义表示为遥感图像检索提供了强大且经济高效的范式。

Abstract: Semantic retrieval of remote sensing (RS) images is a critical task fundamentally challenged by the \textquote{semantic gap}, the discrepancy between a model's low-level visual features and high-level human concepts. While large Vision-Language Models (VLMs) offer a promising path to bridge this gap, existing methods often rely on costly, domain-specific training, and there is a lack of benchmarks to evaluate the practical utility of VLM-generated text in a zero-shot retrieval context. To address this research gap, we introduce the Remote Sensing Rich Text (RSRT) dataset, a new benchmark featuring multiple structured captions per image. Based on this dataset, we propose a fully training-free, text-only retrieval reference called TRSLLaVA. Our methodology reformulates cross-modal retrieval as a text-to-text (T2T) matching problem, leveraging rich text descriptions as queries against a database of VLM-generated captions within a unified textual embedding space. This approach completely bypasses model training or fine-tuning. Experiments on the RSITMD and RSICD benchmarks show our training-free method is highly competitive with state-of-the-art supervised models. For instance, on RSITMD, our method achieves a mean Recall of 42.62\%, nearly doubling the 23.86\% of the standard zero-shot CLIP baseline and surpassing several top supervised models. This validates that high-quality semantic representation through structured text provides a powerful and cost-effective paradigm for remote sensing image retrieval.

</details>


### [257] [Optimal transport unlocks end-to-end learning for single-molecule localization](https://arxiv.org/abs/2512.10683)
*Romain Seailles,Jean-Baptiste Masson,Jean Ponce,Julien Mairal*

Main category: cs.CV

TL;DR: 本文提出新的单分子定位显微镜（SMLM）训练目标和架构，提升表现，代码开源。


<details>
  <summary>Details</summary>
Motivation: 当前高效SMLM采集时间长，深度学习方法依赖非可微NMS层。

Method: 将SMLM训练目标重写成集合匹配问题，推导最优传输损失函数；提出迭代神经网络融入显微镜光学系统知识。

Result: 在合成基准和真实生物数据实验中，新损失函数和架构在中高发射体密度时超越现有技术。

Conclusion: 所提新方法在SMLM中有更好性能。

Abstract: Single-molecule localization microscopy (SMLM) allows reconstructing biology-relevant structures beyond the diffraction limit by detecting and localizing individual fluorophores -- fluorescent molecules stained onto the observed specimen -- over time to reconstruct super-resolved images. Currently, efficient SMLM requires non-overlapping emitting fluorophores, leading to long acquisition times that hinders live-cell imaging. Recent deep-learning approaches can handle denser emissions, but they rely on variants of non-maximum suppression (NMS) layers, which are unfortunately non-differentiable and may discard true positives with their local fusion strategy. In this presentation, we reformulate the SMLM training objective as a set-matching problem, deriving an optimal-transport loss that eliminates the need for NMS during inference and enables end-to-end training. Additionally, we propose an iterative neural network that integrates knowledge of the microscope's optical system inside our model. Experiments on synthetic benchmarks and real biological data show that both our new loss function and architecture surpass the state of the art at moderate and high emitter densities. Code is available at https://github.com/RSLLES/SHOT.

</details>


### [258] [Sharp Monocular View Synthesis in Less Than a Second](https://arxiv.org/abs/2512.10685)
*Lars Mescheder,Wei Dong,Shiwei Li,Xuyang Bai,Marcel Santos,Peiyun Hu,Bruno Lecouat,Mingmin Zhen,Amaël Delaunoy,Tian Fang,Yanghai Tsin,Stephan R. Richter,Vladlen Koltun*

Main category: cs.CV

TL;DR: 提出SHARP方法，可从单张图像进行逼真视图合成，速度快且效果优，代码开源。


<details>
  <summary>Details</summary>
Motivation: 实现从单张图像进行高质量、快速的逼真视图合成。

Method: 通过神经网络单次前馈，在标准GPU上不到一秒内回归出3D高斯表示的参数，再实时渲染。

Result: 在多个数据集上实现零样本泛化，降低LPIPS和DISTS指标，合成时间降低三个数量级。

Conclusion: SHARP方法有效且性能优越，在单张图像视图合成上达到新的最先进水平。

Abstract: We present SHARP, an approach to photorealistic view synthesis from a single image. Given a single photograph, SHARP regresses the parameters of a 3D Gaussian representation of the depicted scene. This is done in less than a second on a standard GPU via a single feedforward pass through a neural network. The 3D Gaussian representation produced by SHARP can then be rendered in real time, yielding high-resolution photorealistic images for nearby views. The representation is metric, with absolute scale, supporting metric camera movements. Experimental results demonstrate that SHARP delivers robust zero-shot generalization across datasets. It sets a new state of the art on multiple datasets, reducing LPIPS by 25-34% and DISTS by 21-43% versus the best prior model, while lowering the synthesis time by three orders of magnitude. Code and weights are provided at https://github.com/apple/ml-sharp

</details>


### [259] [MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence](https://arxiv.org/abs/2512.10863)
*Jingli Lin,Runsen Xu,Shaohao Zhu,Sihan Yang,Peizhou Cao,Yunlong Ran,Miao Hu,Chenming Zhu,Yiman Xie,Yilin Long,Wenbo Hu,Dahua Lin,Tai Wang,Jiangmiao Pang*

Main category: cs.CV

TL;DR: 本文提出MMSI-Video-Bench基准评估多模态大语言模型视频空间智能，评估25个模型发现人与AI差距大，还分析了模型问题。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏全面评估多模态大语言模型在连续视觉输入上空间理解进展的基准。

Method: 提出MMSI-Video-Bench基准，包含四个级别框架，基于多数据集和自制视频设计1106个问题，支持三个子基准。

Result: 评估25个模型，发现人与AI差距大，空间微调模型泛化性差，分析出模型在多方面存在系统性失败，典型策略效果不佳。

Conclusion: 该基准为推进基于视频的空间智能研究提供了坚实的测试平台。

Abstract: Spatial understanding over continuous visual input is crucial for MLLMs to evolve into general-purpose assistants in physical environments. Yet there is still no comprehensive benchmark that holistically assesses the progress toward this goal. In this work, we introduce MMSI-Video-Bench, a fully human-annotated benchmark for video-based spatial intelligence in MLLMs. It operationalizes a four-level framework, Perception, Planning, Prediction, and Cross-Video Reasoning, through 1,106 questions grounded in 1,278 clips from 25 datasets and in-house videos. Each item is carefully designed and reviewed by 3DV experts with explanatory rationales to ensure precise, unambiguous grounding. Leveraging its diverse data sources and holistic task coverage, MMSI-Video-Bench also supports three domain-oriented sub-benchmarks (Indoor Scene Perception Bench, Robot Bench and Grounding Bench) for targeted capability assessment. We evaluate 25 strong open-source and proprietary MLLMs, revealing a striking human--AI gap: many models perform near chance, and the best reasoning model lags humans by nearly 60%. We further find that spatially fine-tuned models still fail to generalize effectively on our benchmark. Fine-grained error analysis exposes systematic failures in geometric reasoning, motion grounding, long-horizon prediction, and cross-video correspondence. We also show that typical frame-sampling strategies transfer poorly to our reasoning-intensive benchmark, and that neither 3D spatial cues nor chain-of-thought prompting yields meaningful gains. We expect our benchmark to establish a solid testbed for advancing video-based spatial intelligence.

</details>


### [260] [BabyVLM-V2: Toward Developmentally Grounded Pretraining and Benchmarking of Vision Foundation Models](https://arxiv.org/abs/2512.10932)
*Shengao Wang,Wenqi Wang,Zecheng Wang,Max Whitton,Michael Wakeham,Arjun Chandra,Joey Huang,Pengyue Zhu,Helen Chen,David Li,Jeffrey Li,Shawn Li,Andrew Zagula,Amy Zhao,Andrew Zhu,Sayaka Nakamura,Yuki Yamamoto,Jerry Jun Yokono,Aaron Mueller,Bryan A. Plummer,Kate Saenko,Venkatesh Saligrama,Boqing Gong*

Main category: cs.CV

TL;DR: 文中介绍BabyVLM - V2框架用于视觉基础模型预训练，实验表明从头预训练的紧凑模型表现佳，希望该框架加速相关研究。


<details>
  <summary>Details</summary>
Motivation: 早期儿童发展轨迹为视觉基础模型的样本高效预训练设立自然目标，要改进BabyVLM - V1。

Method: 引入BabyVLM - V2框架，构建纵向多方面预训练集，采用DevCV Toolbox进行认知评估。

Result: 从头预训练的紧凑模型在DevCV Toolbox上有竞争力，在部分任务上超越GPT - 4o。

Conclusion: 有原则、统一的BabyVLM - V2框架有望加速视觉基础模型发育可行预训练的研究。

Abstract: Early children's developmental trajectories set up a natural goal for sample-efficient pretraining of vision foundation models. We introduce BabyVLM-V2, a developmentally grounded framework for infant-inspired vision-language modeling that extensively improves upon BabyVLM-V1 through a longitudinal, multifaceted pretraining set, a versatile model, and, most importantly, DevCV Toolbox for cognitive evaluation. The pretraining set maximizes coverage while minimizing curation of a longitudinal, infant-centric audiovisual corpus, yielding video-utterance, image-utterance, and multi-turn conversational data that mirror infant experiences. DevCV Toolbox adapts all vision-related measures of the recently released NIH Baby Toolbox into a benchmark suite of ten multimodal tasks, covering spatial reasoning, memory, and vocabulary understanding aligned with early children's capabilities. Experimental results show that a compact model pretrained from scratch can achieve competitive performance on DevCV Toolbox, outperforming GPT-4o on some tasks. We hope the principled, unified BabyVLM-V2 framework will accelerate research in developmentally plausible pretraining of vision foundation models.

</details>


### [261] [Any4D: Unified Feed-Forward Metric 4D Reconstruction](https://arxiv.org/abs/2512.10935)
*Jay Karhade,Nikhil Keetha,Yuchen Zhang,Tanisha Gupta,Akash Sharma,Sebastian Scherer,Deva Ramanan*

Main category: cs.CV

TL;DR: 提出Any4D，用于4D重建，性能优越，支持多模态。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限，如聚焦2 - view场景流或稀疏3D点跟踪，且对多模态处理不足，需更好方法。

Method: 采用模块化4D场景表示，用不同坐标表示不同因素。

Result: 在多种设置下，精度提升2 - 3倍，计算效率提升15倍。

Conclusion: Any4D性能优越，为下游应用开辟新途径。

Abstract: We present Any4D, a scalable multi-view transformer for metric-scale, dense feed-forward 4D reconstruction. Any4D directly generates per-pixel motion and geometry predictions for N frames, in contrast to prior work that typically focuses on either 2-view dense scene flow or sparse 3D point tracking. Moreover, unlike other recent methods for 4D reconstruction from monocular RGB videos, Any4D can process additional modalities and sensors such as RGB-D frames, IMU-based egomotion, and Radar Doppler measurements, when available. One of the key innovations that allows for such a flexible framework is a modular representation of a 4D scene; specifically, per-view 4D predictions are encoded using a variety of egocentric factors (depthmaps and camera intrinsics) represented in local camera coordinates, and allocentric factors (camera extrinsics and scene flow) represented in global world coordinates. We achieve superior performance across diverse setups - both in terms of accuracy (2-3X lower error) and compute efficiency (15X faster), opening avenues for multiple downstream applications.

</details>


### [262] [OmniView: An All-Seeing Diffusion Model for 3D and 4D View Synthesis](https://arxiv.org/abs/2512.10940)
*Xiang Fan,Sharath Girish,Vivek Ramanujan,Chaoyang Wang,Ashkan Mirzaei,Petr Sushko,Aliaksandr Siarohin,Sergey Tulyakov,Ranjay Krishna*

Main category: cs.CV

TL;DR: 提出OmniView统一框架，能跨多种4D一致性任务，表现有竞争力，证明通用4D视频模型可行性。


<details>
  <summary>Details</summary>
Motivation: 现有将相机控制注入扩散模型的方法专注特定4D一致性任务子集，数据训练分散。

Method: 分别表示空间、时间和视图条件，实现输入灵活组合。

Result: 在多基准和指标上与特定任务模型竞争，提升图像质量分数，减少相机轨迹误差。

Conclusion: OmniView证明了通用4D视频模型的可行性。

Abstract: Prior approaches injecting camera control into diffusion models have focused on specific subsets of 4D consistency tasks: novel view synthesis, text-to-video with camera control, image-to-video, amongst others. Therefore, these fragmented approaches are trained on disjoint slices of available 3D/4D data. We introduce OmniView, a unified framework that generalizes across a wide range of 4D consistency tasks. Our method separately represents space, time, and view conditions, enabling flexible combinations of these inputs. For example, OmniView can synthesize novel views from static, dynamic, and multiview inputs, extrapolate trajectories forward and backward in time, and create videos from text or image prompts with full camera control. OmniView is competitive with task-specific models across diverse benchmarks and metrics, improving image quality scores among camera-conditioned diffusion models by up to 33\% in multiview NVS LLFF dataset, 60\% in dynamic NVS Neural 3D Video benchmark, 20\% in static camera control on RE-10K, and reducing camera trajectory errors by 4x in text-conditioned video generation. With strong generalizability in one model, OmniView demonstrates the feasibility of a generalist 4D video model. Project page is available at https://snap-research.github.io/OmniView/

</details>


### [263] [Mull-Tokens: Modality-Agnostic Latent Thinking](https://arxiv.org/abs/2512.10941)
*Arijit Ray,Ahmed Abdelkader,Chengzhi Mao,Bryan A. Plummer,Kate Saenko,Ranjay Krishna,Leonidas Guibas,Wen-Sheng Chu*

Main category: cs.CV

TL;DR: 提出Mull - Tokens以解决现有多模态推理模型的问题，在四个空间推理基准测试中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有探索图像推理的多模态模型脆弱且难以扩展，依赖专业工具、高成本图像生成或手工推理数据。

Method: 先使用交错文本 - 图像轨迹的监督训练Mull - Tokens，再无监督微调。

Result: 在四个空间推理基准测试中，Mull - Tokens比多个基线有平均3%的提升，在解谜推理重的部分提升达16%。

Conclusion: Mull - Tokens为多模态抽象思考提供简单解决方案。

Abstract: Reasoning goes beyond language; the real world requires reasoning about space, time, affordances, and much more that words alone cannot convey. Existing multimodal models exploring the potential of reasoning with images are brittle and do not scale. They rely on calling specialist tools, costly generation of images, or handcrafted reasoning data to switch between text and image thoughts. Instead, we offer a simpler alternative -- Mull-Tokens -- modality-agnostic latent tokens pre-trained to hold intermediate information in either image or text modalities to let the model think free-form towards the correct answer. We investigate best practices to train Mull-Tokens inspired by latent reasoning frameworks. We first train Mull-Tokens using supervision from interleaved text-image traces, and then fine-tune without any supervision by only using the final answers. Across four challenging spatial reasoning benchmarks involving tasks such as solving puzzles and taking different perspectives, we demonstrate that Mull-Tokens improve upon several baselines utilizing text-only reasoning or interleaved image-text reasoning, achieving a +3% average improvement and up to +16% on a puzzle solving reasoning-heavy split compared to our strongest baseline. Adding to conversations around challenges in grounding textual and visual reasoning, Mull-Tokens offers a simple solution to abstractly think in multiple modalities.

</details>


### [264] [AlcheMinT: Fine-grained Temporal Control for Multi-Reference Consistent Video Generation](https://arxiv.org/abs/2512.10943)
*Sharath Girish,Viacheslav Ivanov,Tsai-Shien Chen,Hao Chen,Aliaksandr Siarohin,Sergey Tulyakov*

Main category: cs.CV

TL;DR: 提出AlcheMinT框架用于主题驱动视频生成，实现多主题生成的精确时间控制，视觉质量达先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有主题驱动视频生成方法缺乏对主题出现和消失的细粒度时间控制，无法满足合成视频、故事板和可控动画等应用需求。

Method: 提出AlcheMinT统一框架，引入显式时间戳条件；采用新的位置编码机制；结合主题描述文本标记；通过标记级联避免额外的交叉注意力模块。

Result: 建立评估多主题身份保留、视频保真度和时间依从性的基准，实验表明AlcheMinT视觉质量与现有先进视频个性化方法相当，首次实现视频中多主题生成的精确时间控制。

Conclusion: AlcheMinT在实现精确时间控制的同时保证了视觉质量，为主题驱动视频生成提供了有效解决方案。

Abstract: Recent advances in subject-driven video generation with large diffusion models have enabled personalized content synthesis conditioned on user-provided subjects. However, existing methods lack fine-grained temporal control over subject appearance and disappearance, which are essential for applications such as compositional video synthesis, storyboarding, and controllable animation. We propose AlcheMinT, a unified framework that introduces explicit timestamps conditioning for subject-driven video generation. Our approach introduces a novel positional encoding mechanism that unlocks the encoding of temporal intervals, associated in our case with subject identities, while seamlessly integrating with the pretrained video generation model positional embeddings. Additionally, we incorporate subject-descriptive text tokens to strengthen binding between visual identity and video captions, mitigating ambiguity during generation. Through token-wise concatenation, AlcheMinT avoids any additional cross-attention modules and incurs negligible parameter overhead. We establish a benchmark evaluating multiple subject identity preservation, video fidelity, and temporal adherence. Experimental results demonstrate that AlcheMinT achieves visual quality matching state-of-the-art video personalization methods, while, for the first time, enabling precise temporal control over multi-subject generation within videos. Project page is at https://snap-research.github.io/Video-AlcheMinT

</details>


### [265] [Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation](https://arxiv.org/abs/2512.10949)
*Yiwen Tang,Zoey Guo,Kaixin Zhu,Ray Zhang,Qizhi Chen,Dongzhi Jiang,Junli Liu,Bohan Zeng,Haoming Song,Delin Qu,Tianyi Bai,Dan Xu,Wentao Zhang,Bin Zhao*

Main category: cs.CV

TL;DR: 本文首次系统研究强化学习用于文本到3D生成，在奖励设计、算法、基准测试和高级范式方面开展工作，开发了首个强化学习增强的文本到3D模型AR3D - R1。


<details>
  <summary>Details</summary>
Motivation: 由于3D对象空间复杂度高，强化学习在3D生成中的应用研究较少，且3D生成对奖励设计和算法敏感，需要开展系统研究。

Method: 从奖励设计、强化学习算法、文本到3D基准测试、高级强化学习范式四个维度进行系统研究，提出Hi - GRPO，开发AR3D - R1。

Result: 评估了奖励维度和模型选择，研究GRPO变体，引入MME - 3DR基准测试，提出Hi - GRPO并开发AR3D - R1。

Conclusion: 该研究为3D生成的强化学习驱动推理提供了见解。

Abstract: Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.

</details>


### [266] [SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model](https://arxiv.org/abs/2512.10957)
*Yukai Shi,Weiyu Li,Zihao Wang,Hongyang Li,Xingyu Chen,Ping Tan,Lei Zhang*

Main category: cs.CV

TL;DR: 提出名为 SceneMaker 的解耦合 3D 场景生成框架，通过解耦模型、改进姿态估计模型和构建数据集等方法，实验证明该框架在室内和开放集场景上的优越性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在严重遮挡和开放集设置下，因缺乏足够的开放集去遮挡和姿态估计先验，难以同时生成高质量几何形状和准确姿态。

Method: 1. 解耦去遮挡模型与 3D 对象生成，并利用图像数据集和去遮挡数据集增强去遮挡模型；2. 提出统一的姿态估计模型，融合全局和局部机制；3. 构建开放集 3D 场景数据集。

Result: 综合实验表明解耦合框架在室内和开放集场景上具有优越性。

Conclusion: 所提出的 SceneMaker 解耦合 3D 场景生成框架有效，代码和数据集已公开。

Abstract: We propose a decoupled 3D scene generation framework called SceneMaker in this work. Due to the lack of sufficient open-set de-occlusion and pose estimation priors, existing methods struggle to simultaneously produce high-quality geometry and accurate poses under severe occlusion and open-set settings. To address these issues, we first decouple the de-occlusion model from 3D object generation, and enhance it by leveraging image datasets and collected de-occlusion datasets for much more diverse open-set occlusion patterns. Then, we propose a unified pose estimation model that integrates global and local mechanisms for both self-attention and cross-attention to improve accuracy. Besides, we construct an open-set 3D scene dataset to further extend the generalization of the pose estimation model. Comprehensive experiments demonstrate the superiority of our decoupled framework on both indoor and open-set scenes. Our codes and datasets is released at https://idea-research.github.io/SceneMaker/.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [267] [PMB-NN: Physiology-Centred Hybrid AI for Personalized Hemodynamic Monitoring from Photoplethysmography](https://arxiv.org/abs/2512.10745)
*Yaowen Zhang,Libera Fresiello,Peter H. Veltink,Dirk W. Donker,Ying Wang*

Main category: physics.med-ph

TL;DR: 提出改进的Physiological Model - Based Neural Network (PMB - NN) 方法用于血压估计，在准确性、可解释性和合理性等方面表现良好，是日常血流动力学监测的可行替代方案。


<details>
  <summary>Details</summary>
Motivation: 连续监测血压和血流动力学参数对早期血管功能障碍检测至关重要，但现有数据驱动的血压估计方法缺乏可解释性。

Method: 改进先前提出的生理学中心混合AI方法PMB - NN，将深度学习与基于2 - 元素Windkessel模型结合，以R和C作为物理约束；使用PPG衍生的时序特征进行特定受试者训练，用人口统计信息推断心输出量。

Result: PMB - NN的收缩压和舒张压准确性与深度学习基准相当，在生理合理性上优于深度学习基线和单独的生理模型；能准确识别R和C。

Conclusion: PMB - NN兼具生理原理和数据驱动技术优点，是纯数据驱动方法的均衡、基于生理的替代方案。

Abstract: Continuous monitoring of blood pressure (BP) and hemodynamic parameters such as peripheral resistance (R) and arterial compliance (C) are critical for early vascular dysfunction detection. While photoplethysmography (PPG) wearables has gained popularity, existing data-driven methods for BP estimation lack interpretability. We advanced our previously proposed physiology-centered hybrid AI method-Physiological Model-Based Neural Network (PMB-NN)-in blood pressure estimation, that unifies deep learning with a 2-element Windkessel based model parameterized by R and C acting as physics constraints. The PMB-NN model was trained in a subject-specific manner using PPG-derived timing features, while demographic information was used to infer an intermediate variable: cardiac output. We validated the model on 10 healthy adults performing static and cycling activities across two days for model's day-to-day robustness, benchmarked against deep learning (DL) models (FCNN, CNN-LSTM, Transformer) and standalone Windkessel based physiological model (PM). Validation was conducted on three perspectives: accuracy, interpretability and plausibility. PMB-NN achieved systolic BP accuracy (MAE: 7.2 mmHg) comparable to DL benchmarks, diastolic performance (MAE: 3.9 mmHg) lower than DL models. However, PMB-NN exhibited higher physiological plausibility than both DL baselines and PM, suggesting that the hybrid architecture unifies and enhances the respective merits of physiological principles and data-driven techniques. Beyond BP, PMB-NN identified R (ME: 0.15 mmHg$\cdot$s/ml) and C (ME: -0.35 ml/mmHg) during training with accuracy similar to PM, demonstrating that the embedded physiological constraints confer interpretability to the hybrid AI framework. These results position PMB-NN as a balanced, physiologically grounded alternative to purely data-driven approaches for daily hemodynamic monitoring.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [268] [Symphony: A Heuristic Normalized Calibrated Advantage Actor and Critic Algorithm in application for Humanoid Robots](https://arxiv.org/abs/2512.10477)
*Timur Ishuov,Michele Folgheraiter,Madi Nurmanov,Goncalo Gordo,Richárd Farkas,József Dombi*

Main category: cs.RO

TL;DR: 指出人类学习并非快速，机器人从头学习无大量训练步骤的条件，提出“Swaddling”正则化、Symphony算法等，采用Fading Replay Buffer，使训练更安全高效。


<details>
  <summary>Details</summary>
Motivation: 解决机器人从头学习时无法进行大量训练步骤的问题，让机器人训练更安全高效。

Method: 采用“Swaddling”正则化约束智能体，提出Symphony算法，设置有限参数噪声，使用Fading Replay Buffer调整采样概率。

Result: 使机器人训练在环境和自身机制上都更安全，可利用时间优势单次更新Actor和Critic。

Conclusion: 所提出的方法能有效解决机器人从无到有训练的问题，提升训练安全性与效率。

Abstract: In our work we not explicitly hint that it is a misconception to think that humans learn fast. Learning process takes time. Babies start learning to move in the restricted liquid area called placenta. Children often are limited by underdeveloped body. Even adults are not allowed to participate in complex competitions right away. However, with robots, when learning from scratch, we often don't have the privilege of waiting for dozen millions of steps. "Swaddling" regularization is responsible for restraining an agent in rapid but unstable development penalizing action strength in a specific way not affecting actions directly. The Symphony, Transitional-policy Deterministic Actor and Critic algorithm, is a concise combination of different ideas for possibility of training humanoid robots from scratch with Sample Efficiency, Sample Proximity and Safety of Actions in mind. It is no secret that continuous increase in Gaussian noise without appropriate smoothing is harmful for motors and gearboxes. Compared to Stochastic algorithms, we set a limited parametric noise and promote a reduced strength of actions, safely increasing entropy, since the actions are kind of immersed in weaker noise. When actions require more extreme values, actions rise above the weak noise. Training becomes empirically much safer for both the environment around and the robot's mechanisms. We use Fading Replay Buffer: using a fixed formula containing the hyperbolic tangent, we adjust the batch sampling probability: the memory contains a recent memory and a long-term memory trail. Fading Replay Buffer allows us to use Temporal Advantage when we improve the current Critic Network prediction compared to the exponential moving average. Temporal Advantage allows us to update Actor and Critic in one pass, as well as combine Actor and Critic in one Object and implement their Losses in one line.

</details>


### [269] [Push Smarter, Not Harder: Hierarchical RL-Diffusion Policy for Efficient Nonprehensile Manipulation](https://arxiv.org/abs/2512.10099)
*Steven Caro,Stephen L. Smith*

Main category: cs.RO

TL;DR: 提出分层强化学习 - 扩散策略HeRD解决非抓持操作推物问题，在2D模拟环境中表现优于基线，证明分层控制方法前景好。


<details>
  <summary>Details</summary>
Motivation: 非抓持操作因复杂接触动力学和长视野规划要求，是具有挑战性的控制问题。

Method: 提出HeRD策略，将推物任务分解为高级目标选择和低级轨迹生成，高级用强化学习选目标，低级用目标条件扩散模型生成轨迹。

Result: 在2D模拟环境评估中，HeRD在成功率、路径效率和跨环境泛化上优于现有基线。

Conclusion: 带生成式低级规划的分层控制是可扩展、目标导向非抓持操作的有前景方向。

Abstract: Nonprehensile manipulation, such as pushing objects across cluttered environments, presents a challenging control problem due to complex contact dynamics and long-horizon planning requirements. In this work, we propose HeRD, a hierarchical reinforcement learning-diffusion policy that decomposes pushing tasks into two levels: high-level goal selection and low-level trajectory generation. We employ a high-level reinforcement learning (RL) agent to select intermediate spatial goals, and a low-level goal-conditioned diffusion model to generate feasible, efficient trajectories to reach them.
  This architecture combines the long-term reward maximizing behaviour of RL with the generative capabilities of diffusion models. We evaluate our method in a 2D simulation environment and show that it outperforms the state-of-the-art baseline in success rate, path efficiency, and generalization across multiple environment configurations. Our results suggest that hierarchical control with generative low-level planning is a promising direction for scalable, goal-directed nonprehensile manipulation. Code, documentation, and trained models are available: https://github.com/carosteven/HeRD.

</details>


### [270] [RoboNeuron: A Modular Framework Linking Foundation Models and ROS for Embodied AI](https://arxiv.org/abs/2512.10394)
*Weifan Guan,Huasen Xi,Chenxiao Zhang,Aosheng Li,Qinghao Hu,Jian Cheng*

Main category: cs.RO

TL;DR: 提出RoboNeuron通用部署框架解决具身AI系统工程障碍，提升性能并奠定应用基础。


<details>
  <summary>Details</summary>
Motivation: 当前具身AI系统存在跨场景适应性差、模块耦合僵硬和推理加速碎片化等工程障碍，需解决这些问题。

Method: 提出RoboNeuron框架，深度集成LLM和VLA模型认知能力与ROS实时执行骨干，用MCP作语义桥，利用ROS接口建立高模块化架构，引入自动工具转换ROS消息。

Result: RoboNeuron增强了跨场景适应性和组件灵活性，建立了水平性能基准测试的系统平台。

Conclusion: RoboNeuron为可扩展的现实世界具身应用奠定了坚实基础。

Abstract: Current embodied AI systems face severe engineering impediments, primarily characterized by poor cross-scenario adaptability, rigid inter-module coupling, and fragmented inference acceleration. To overcome these limitations, we propose RoboNeuron, a universal deployment framework for embodied intelligence. RoboNeuron is the first framework to deeply integrate the cognitive capabilities of Large Language Models (LLMs) and Vision-Language-Action (VLA) models with the real-time execution backbone of the Robot Operating System (ROS). We utilize the Model Context Protocol (MCP) as a semantic bridge, enabling the LLM to dynamically orchestrate underlying robotic tools. The framework establishes a highly modular architecture that strictly decouples sensing, reasoning, and control by leveraging ROS's unified communication interfaces. Crucially, we introduce an automated tool to translate ROS messages into callable MCP functions, significantly streamlining development. RoboNeuron significantly enhances cross-scenario adaptability and component flexibility, while establishing a systematic platform for horizontal performance benchmarking, laying a robust foundation for scalable real-world embodied applications.

</details>


### [271] [Evaluating Gemini Robotics Policies in a Veo World Simulator](https://arxiv.org/abs/2512.10675)
*Gemini Robotics Team,Coline Devin,Yilun Du,Debidatta Dwibedi,Ruiqi Gao,Abhishek Jindal,Thomas Kipf,Sean Kirmani,Fangchen Liu,Anirudha Majumdar,Andrew Marmon,Carolina Parada,Yulia Rubanova,Dhruv Shah,Vikas Sindhwani,Jie Tan,Fei Xia,Ted Xiao,Sherry Yang,Wenhao Yu,Allan Zhou*

Main category: cs.RO

TL;DR: 本文展示视频模型可用于机器人策略评估全场景，介绍基于前沿视频基础模型构建的生成式评估系统，通过大量实验验证其能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频模型在机器人领域的应用局限于分布内评估，希望拓展其在全场景策略评估的应用。

Method: 引入基于前沿视频基础模型构建的生成式评估系统，优化其支持机器人动作条件和多视图一致性，集成生成式图像编辑和多视图补全。

Result: 系统能准确模拟编辑后的场景，可精确预测不同条件下策略相对表现，确定泛化轴对策略性能的影响，进行策略红队测试。

Conclusion: 通过对多个策略检查点和任务的大量实际评估，验证了系统在机器人策略评估全场景的能力。

Abstract: Generative world models hold significant potential for simulating interactions with visuomotor policies in varied environments. Frontier video models can enable generation of realistic observations and environment interactions in a scalable and general manner. However, the use of video models in robotics has been limited primarily to in-distribution evaluations, i.e., scenarios that are similar to ones used to train the policy or fine-tune the base video model. In this report, we demonstrate that video models can be used for the entire spectrum of policy evaluation use cases in robotics: from assessing nominal performance to out-of-distribution (OOD) generalization, and probing physical and semantic safety. We introduce a generative evaluation system built upon a frontier video foundation model (Veo). The system is optimized to support robot action conditioning and multi-view consistency, while integrating generative image-editing and multi-view completion to synthesize realistic variations of real-world scenes along multiple axes of generalization. We demonstrate that the system preserves the base capabilities of the video model to enable accurate simulation of scenes that have been edited to include novel interaction objects, novel visual backgrounds, and novel distractor objects. This fidelity enables accurately predicting the relative performance of different policies in both nominal and OOD conditions, determining the relative impact of different axes of generalization on policy performance, and performing red teaming of policies to expose behaviors that violate physical or semantic safety constraints. We validate these capabilities through 1600+ real-world evaluations of eight Gemini Robotics policy checkpoints and five tasks for a bimanual manipulator.

</details>


### [272] [How to Brake? Ethical Emergency Braking with Deep Reinforcement Learning](https://arxiv.org/abs/2512.10698)
*Jianbo Wang,Galina Sidorenko,Johan Thunberg*

Main category: cs.RO

TL;DR: 本文研究利用深度强化学习（DRL）提高涉及紧急制动的多车跟车场景安全性，提出结合DRL与解析表达式选最优恒定减速度方法的混合策略，提升可靠性并改善性能。


<details>
  <summary>Details</summary>
Motivation: 互联自动驾驶车辆（CAVs）虽有提升驾驶安全的潜力，但基于最坏情况的保守控制策略会降低灵活性和整体性能，需新方法提升安全性。

Method: 提出结合深度强化学习和基于解析表达式选择最优恒定减速度的混合方法。

Result: 相比单独的深度强化学习，混合方法提高了可靠性，在整体减少伤害和避免碰撞方面有更优表现。

Conclusion: 结合DRL和先前方法的混合方法可改善多车跟车场景中的安全性和整体性能。

Abstract: Connected and automated vehicles (CAVs) have the potential to enhance driving safety, for example by enabling safe vehicle following and more efficient traffic scheduling. For such future deployments, safety requirements should be addressed, where the primary such are avoidance of vehicle collisions and substantial mitigating of harm when collisions are unavoidable. However, conservative worst-case-based control strategies come at the price of reduced flexibility and may compromise overall performance. In light of this, we investigate how Deep Reinforcement Learning (DRL) can be leveraged to improve safety in multi-vehicle-following scenarios involving emergency braking. Specifically, we investigate how DRL with vehicle-to-vehicle communication can be used to ethically select an emergency breaking profile in scenarios where overall, or collective, three-vehicle harm reduction or collision avoidance shall be obtained instead of single-vehicle such. As an algorithm, we provide a hybrid approach that combines DRL with a previously published method based on analytical expressions for selecting optimal constant deceleration. By combining DRL with the previous method, the proposed hybrid approach increases the reliability compared to standalone DRL, while achieving superior performance in terms of overall harm reduction and collision avoidance.

</details>


### [273] [Iterative Compositional Data Generation for Robot Control](https://arxiv.org/abs/2512.10891)
*Anh-Quan Pham,Marcel Hussing,Shubhankar P. Patankar,Dani S. Bassett,Jorge Mendez-Mendez,Eric Eaton*

Main category: cs.RO

TL;DR: 提出语义组合扩散变换器，可零样本生成高质量转换数据学习控制策略，还引入迭代自我改进程序，提升零样本表现。


<details>
  <summary>Details</summary>
Motivation: 收集机器人操作数据成本高，现有生成模型难以应对多对象、多机器人和多环境中任务组合的情况，缺乏对机器人领域组合结构的利用。

Method: 提出语义组合扩散变换器，将转换分解为特定组件并通过注意力学习交互；引入迭代自我改进程序，用离线强化学习验证合成数据并融入后续训练。

Result: 模型能零样本生成高质量转换数据，最终能解决几乎所有保留任务。

Conclusion: 该方法优于整体和硬编码的组合基线，展示了学习表示中出现有意义的组合结构。

Abstract: Collecting robotic manipulation data is expensive, making it impractical to acquire demonstrations for the combinatorially large space of tasks that arise in multi-object, multi-robot, and multi-environment settings. While recent generative models can synthesize useful data for individual tasks, they do not exploit the compositional structure of robotic domains and struggle to generalize to unseen task combinations. We propose a semantic compositional diffusion transformer that factorizes transitions into robot-, object-, obstacle-, and objective-specific components and learns their interactions through attention. Once trained on a limited subset of tasks, we show that our model can zero-shot generate high-quality transitions from which we can learn control policies for unseen task combinations. Then, we introduce an iterative self-improvement procedure in which synthetic data is validated via offline reinforcement learning and incorporated into subsequent training rounds. Our approach substantially improves zero-shot performance over monolithic and hard-coded compositional baselines, ultimately solving nearly all held-out tasks and demonstrating the emergence of meaningful compositional structure in the learned representations.

</details>


### [274] [Curriculum-Based Reinforcement Learning for Autonomous UAV Navigation in Unknown Curved Tubular Conduit](https://arxiv.org/abs/2512.10934)
*Zamirddine Mari,Jérôme Pasquet,Julien Seinturier*

Main category: cs.RO

TL;DR: 提出强化学习方法使无人机在未知三维管道中导航，经实验验证效果良好，为未知管道自主导航提供框架。


<details>
  <summary>Details</summary>
Motivation: 受限管状环境中无人机自主导航存在挑战，需新方法实现无先验几何知识导航。

Method: 采用强化学习方法，结合纯追踪算法作基线，用课程学习策略训练智能体，设计转弯协商机制。

Result: PPO策略获得鲁棒且可泛化行为，表现优于确定性控制器，验证了学习行为可迁移到连续物理动力学。

Conclusion: 该方法为未知管状环境自主导航提供完整框架，对工业、地下或医疗等应用有前景。

Abstract: Autonomous drone navigation in confined tubular environments remains a major challenge due to the constraining geometry of the conduits, the proximity of the walls, and the perceptual limitations inherent to such scenarios. We propose a reinforcement learning approach enabling a drone to navigate unknown three-dimensional tubes without any prior knowledge of their geometry, relying solely on local observations from LiDAR and a conditional visual detection of the tube center. In contrast, the Pure Pursuit algorithm, used as a deterministic baseline, benefits from explicit access to the centerline, creating an information asymmetry designed to assess the ability of RL to compensate for the absence of a geometric model. The agent is trained through a progressive Curriculum Learning strategy that gradually exposes it to increasingly curved geometries, where the tube center frequently disappears from the visual field. A turning-negotiation mechanism, based on the combination of direct visibility, directional memory, and LiDAR symmetry cues, proves essential for ensuring stable navigation under such partial observability conditions. Experiments show that the PPO policy acquires robust and generalizable behavior, consistently outperforming the deterministic controller despite its limited access to geometric information. Validation in a high-fidelity 3D environment further confirms the transferability of the learned behavior to a continuous physical dynamics.
  The proposed approach thus provides a complete framework for autonomous navigation in unknown tubular environments and opens perspectives for industrial, underground, or medical applications where progressing through narrow and weakly perceptive conduits represents a central challenge.

</details>


### [275] [ImplicitRDP: An End-to-End Visual-Force Diffusion Policy with Structural Slow-Fast Learning](https://arxiv.org/abs/2512.10946)
*Wendi Chen,Han Xue,Yi Wang,Fangyuan Zhou,Jun Lv,Yang Jin,Shirun Tang,Chuan Wen,Cewu Lu*

Main category: cs.RO

TL;DR: 文章提出ImplicitRDP视觉 - 力扩散策略，结合结构快慢学习和虚拟目标表示正则化，在富接触任务实验中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 视觉和力觉信号在频率和信息上存在差异，整合二者具有挑战性，旨在实现人类水平的富接触操作。

Method: 提出ImplicitRDP统一端到端视觉 - 力扩散策略，采用结构快慢学习机制处理异步信号，以及虚拟目标表示正则化缓解模态崩溃。

Result: 在富接触任务的大量实验中，ImplicitRDP显著优于仅视觉和分层基线方法，获得更好的反应性和成功率。

Conclusion: 所提出的ImplicitRDP能通过简化训练流程有效整合视觉和力觉信号，提升富接触操作性能。

Abstract: Human-level contact-rich manipulation relies on the distinct roles of two key modalities: vision provides spatially rich but temporally slow global context, while force sensing captures rapid, high-frequency local contact dynamics. Integrating these signals is challenging due to their fundamental frequency and informational disparities. In this work, we propose ImplicitRDP, a unified end-to-end visual-force diffusion policy that integrates visual planning and reactive force control within a single network. We introduce Structural Slow-Fast Learning, a mechanism utilizing causal attention to simultaneously process asynchronous visual and force tokens, allowing the policy to perform closed-loop adjustments at the force frequency while maintaining the temporal coherence of action chunks. Furthermore, to mitigate modality collapse where end-to-end models fail to adjust the weights across different modalities, we propose Virtual-target-based Representation Regularization. This auxiliary objective maps force feedback into the same space as the action, providing a stronger, physics-grounded learning signal than raw force prediction. Extensive experiments on contact-rich tasks demonstrate that ImplicitRDP significantly outperforms both vision-only and hierarchical baselines, achieving superior reactivity and success rates with a streamlined training pipeline. Code and videos will be publicly available at https://implicit-rdp.github.io.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [276] [BookReconciler: An Open-Source Tool for Metadata Enrichment and Work-Level Clustering](https://arxiv.org/abs/2512.10165)
*Matt Miller,Dan Sinykin,Melanie Walsh*

Main category: cs.DL

TL;DR: 介绍开源工具BookReconciler，可增强和聚类书籍数据，评估显示对美国作品准确性高，对全球文本性能低，支持跨领域和应用重用书目数据。


<details>
  <summary>Details</summary>
Motivation: 为方便组合相关藏书和大规模分析书籍，解决书目数据增强和聚类问题。

Method: 将其设计为OpenRefine扩展，连接主要书目服务，通过交互式界面让用户人工评估匹配。

Result: 在获奖书籍和当代世界小说数据集上评估，对美国作品接近完美准确，对全球文本性能较低。

Conclusion: BookReconciler支持跨领域和应用重用书目数据，助力数字图书馆和数字人文工作。

Abstract: We present BookReconciler, an open-source tool for enhancing and clustering book data. BookReconciler allows users to take spreadsheets with minimal metadata, such as book title and author, and automatically 1) add authoritative, persistent identifiers like ISBNs 2) and cluster related Expressions and Manifestations of the same Work, e.g., different translations or editions. This enhancement makes it easier to combine related collections and analyze books at scale. The tool is currently designed as an extension for OpenRefine -- a popular software application -- and connects to major bibliographic services including the Library of Congress, VIAF, OCLC, HathiTrust, Google Books, and Wikidata. Our approach prioritizes human judgment. Through an interactive interface, users can manually evaluate matches and define the contours of a Work (e.g., to include translations or not). We evaluate reconciliation performance on datasets of U.S. prize-winning books and contemporary world fiction. BookReconciler achieves near-perfect accuracy for U.S. works but lower performance for global texts, reflecting structural weaknesses in bibliographic infrastructures for non-English and global literature. Overall, BookReconciler supports the reuse of bibliographic data across domains and applications, contributing to ongoing work in digital libraries and digital humanities.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [277] [Natural Language Interface for Firewall Configuration](https://arxiv.org/abs/2512.10789)
*F. Taghiyev,A. Aslanbayli*

Main category: cs.NI

TL;DR: 本文设计并实现企业防火墙自然语言接口原型，可将自然语言策略转为特定配置，集成三层验证，探讨其用于防火墙策略管理的可扩展性等。


<details>
  <summary>Details</summary>
Motivation: 为企业防火墙配置提供更便捷方式，让管理员能用自然语言表达访问控制策略。

Method: 采用紧凑模式绑定的中间表示分离人类意图和设备语法，使用大语言模型作为辅助解析器，集成静态检查器、安全门和基于Batfish的模拟器三层验证。

Result: 实现了自然语言接口原型，可将自然语言策略编译为Palo Alto PAN OS命令行配置，且可扩展到其他平台。

Conclusion: 该方法可发展为可扩展、可审计且以人为中心的防火墙策略管理工作流。

Abstract: This paper presents the design and prototype implementation of a natural language interface for configuring enterprise firewalls. The framework allows administrators to express access control policies in plain language, which are then translated into vendor specific configurations. A compact schema bound intermediate representation separates human intent from device syntax and in the current prototype compiles to Palo Alto PAN OS command line configuration while remaining extensible to other platforms. Large language models are used only as assistive parsers that generate typed intermediate representation objects, while compilation and enforcement remain deterministic. The prototype integrates three validation layers, namely a static linter that checks structural and vendor specific constraints, a safety gate that blocks overly permissive rules such as any to any allows, and a Batfish based simulator that validates configuration syntax and referential integrity against a synthetic device model. The paper describes the architecture, implementation, and test methodology on synthetic network context datasets and discusses how this approach can evolve into a scalable auditable and human centered workflow for firewall policy management.

</details>


### [278] [A Differentiable Digital Twin of Distributed Link Scheduling for Contention-Aware Networking](https://arxiv.org/abs/2512.10874)
*Zhongyuan Zhao,Yujun Ming,Kevin Chan,Ananthram Swami,Santiago Segarra*

Main category: cs.NI

TL;DR: 本文提出分析网络数字孪生模型预测无线链路占空比，比包级模拟快5000倍，可优化链路调度。


<details>
  <summary>Details</summary>
Motivation: 有线网络的最小成本流方法不适用于无线多跳网络，因共享频谱资源竞争，链路容量和成本结构假设不成立，需新方法。

Method: 开发分析网络数字孪生模型，将随机竞争泛化为在冲突图上用加权Luby算法找最大独立集，推导链路占空比分析模型，引入迭代程序解决占空比、链路容量和竞争概率间的循环依赖。

Result: 所提NDT能准确预测链路占空比和拥塞模式，比包级模拟快达5000倍，可通过梯度下降优化链路调度。

Conclusion: 该分析网络数字孪生模型有效，提高预测速度并可用于优化链路调度以减少拥塞和无线电覆盖范围。

Abstract: Many routing and flow optimization problems in wired networks can be solved efficiently using minimum cost flow formulations. However, this approach does not extend to wireless multi-hop networks, where the assumptions of fixed link capacity and linear cost structure collapse due to contention for shared spectrum resources. The key challenge is that the long-term capacity of a wireless link becomes a non-linear function of its network context, including network topology, link quality, and the traffic assigned to neighboring links. In this work, we pursue a new direction of modeling wireless network under randomized medium access control by developing an analytical network digital twin (NDT) that predicts link duty cycles from network context. We generalize randomized contention as finding a Maximal Independent Set (MIS) on the conflict graph using weighted Luby's algorithm, derive an analytical model of link duty cycles, and introduce an iterative procedure that resolves the circular dependency among duty cycle, link capacity, and contention probability. Our numerical experiments show that the proposed NDT accurately predicts link duty cycles and congestion patterns with up to a 5000x speedup over packet-level simulation, and enables us to optimize link scheduling using gradient descent for reduced congestion and radio footprint.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [279] [Translating Informal Proofs into Formal Proofs Using a Chain of States](https://arxiv.org/abs/2512.10317)
*Ziyu Wang,Bowen Yang,Shihao Zhou,Chenyi Li,Yuan Zhang,Bin Dong,Zaiwen Wen*

Main category: cs.LO

TL;DR: 旨在受限预算下将自然语言数学证明转化为Lean4形式证明，提出两阶段框架，效果超基线。


<details>
  <summary>Details</summary>
Motivation: 解决在受限计算预算下将自然语言表达的非正式数学证明转换为Lean4形式证明的问题。

Method: 提出两阶段框架，先提取中间形式证明状态链CoS，再生成状态转换策略。构建数据集和基准，引入交互框架。

Result: 所提方法显著优于现有基线，实现了更高的证明成功率。

Conclusion: 中间表示降低策略生成复杂性，更好匹配非正式推理模式，提出方法有效。

Abstract: We address the problem of translating informal mathematical proofs expressed in natural language into formal proofs in Lean4 under a constrained computational budget. Our approach is grounded in two key insights. First, informal proofs tend to proceed via a sequence of logical transitions - often implications or equivalences - without explicitly specifying intermediate results or auxiliary lemmas. In contrast, formal systems like Lean require an explicit representation of each proof state and the tactics that connect them. Second, each informal reasoning step can be viewed as an abstract transformation between proof states, but identifying the corresponding formal tactics often requires nontrivial domain knowledge and precise control over proof context. To bridge this gap, we propose a two stage framework. Rather than generating formal tactics directly, we first extract a Chain of States (CoS), a sequence of intermediate formal proof states aligned with the logical structure of the informal argument. We then generate tactics to transition between adjacent states in the CoS, thereby constructing the full formal proof. This intermediate representation significantly reduces the complexity of tactic generation and improves alignment with informal reasoning patterns. We build dedicated datasets and benchmarks for training and evaluation, and introduce an interactive framework to support tactic generation from formal states. Empirical results show that our method substantially outperforms existing baselines, achieving higher proof success rates.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [280] [The Distributional Consequences of Paid-Priority Queues](https://arxiv.org/abs/2512.10594)
*Alejandro Corvalan*

Main category: econ.GN

TL;DR: 研究引入快速通道队列对服务分配的影响，表明采用优先级系统的意愿仅由收入决定。


<details>
  <summary>Details</summary>
Motivation: 探究在个体收入和服务估值都存在差异时，引入快速通道队列对服务分配的影响。

Method: 对比单免费队列和快速通道优先级系统。

Result: 采用优先级系统的意愿仅由收入决定，高收入者受益，低收入者更差，中等收入者在优先级制度下会付费使用快速通道。

Conclusion: 优先队列的使用不能反映对优先级系统的偏好。

Abstract: This note examines the distributional implications of introducing a fast-track queue for accessing a service when agents are heterogeneous in both income and service valuation. Relative to a single free queue, I show that willingness to adopt the priority system is determined solely by income, regardless of service valuation. High-income individuals benefit from the fast-track access, while low-income individuals are worse off and remain in the free line. Middle-income individuals weakly prefer the single free queue; yet, under the priority regime, they pay for fast-track access. Thus, the use of the priority queue does not reveal preferences for the priority system.

</details>


### [281] [Capability Accumulation and Conditional Convergence: Towards a Dynamic Theory of Economic Complexity](https://arxiv.org/abs/2512.10672)
*Cesar A. Hidalgo,Viktor Stojkoski*

Main category: econ.GN

TL;DR: 开发动态经济复杂性模型，解释收敛转变与路径依赖多样化。


<details>
  <summary>Details</summary>
Motivation: 解释经济发展中无条件收敛和条件收敛之间的转变。

Method: 构建动态模型并进行解析求解，推导区分两种收敛的边界。

Result: 模型能解释收敛转变和路径依赖多样化过程。

Conclusion: 该模型可解释收敛转变和路径依赖多样化现象。

Abstract: We develop a dynamic model of economic complexity that endogenously generates a transition between unconditional and conditional convergence. In this model, convergence turns conditional as the capability intensity of activities rises. We solve the model analytically, deriving closed-form solutions for the boundary separating unconditional from conditional convergence and show that this model also explains the path-dependent diversification process known as the principle of relatedness. This model provides an explanation for transitions between conditional and unconditional convergence and path-dependent diversification.

</details>


### [282] [Multidimensional Sorting: Comparative Statics](https://arxiv.org/abs/2512.10853)
*Job Boerma,Andrea Ottolini,Aleh Tsyvinski*

Main category: econ.GN

TL;DR: 本文为一般多维分配模型中的技术变革提供比较静态分析的完整理论，并对美国数据进行相关量化。


<details>
  <summary>Details</summary>
Motivation: 多维分配模型中具有一般产出函数和输入分布的比较静态分析是重要开放性问题，需进行研究。

Method: 将任何技术变革唯一分解为梯度和无散度两个不同组件，通过泊松方程刻画边际收益变化，刻画劳动力再分配。

Result: 成功将技术变革分解为两个组件，可用于刻画相关特征；对美国数据给出了排序和收入有关认知技能偏向型技术变革的均衡响应量化结果。

Conclusion: 建立了一般多维分配模型技术变革比较静态分析的完整理论，并可应用于实际数据量化。

Abstract: In sorting literature, comparative statics for multidimensional assignment models with general output functions and input distributions is an important open question. We provide a complete theory of comparative statics for technological change in general multidimensional assignment models. Our main result is that any technological change is uniquely decomposed into two distinct components. The first component (gradient) gives a characterization of changes in marginal earnings through a Poisson equation. The second component (divergence-free) gives a characterization of labor reallocation. For U.S. data, we quantify equilibrium responses in sorting and earnings with respect to cognitive skill-biased technological change.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [283] [Defining the Scope of Learning Analytics: An Axiomatic Approach for Analytic Practice and Measurable Learning Phenomena](https://arxiv.org/abs/2512.10081)
*Kensuke Takii,Changhao Liang,Hiroaki Ogata*

Main category: cs.CY

TL;DR: 本文提出首个公理化理论定义学习分析（LA）的结构、范围和限制，推导定理和命题，阐明LA认识论立场，统一解释多样LA方法，为领域成熟奠定理论基础。


<details>
  <summary>Details</summary>
Motivation: 学习分析虽发展迅速，但基础身份在理论上未明确界定，本文旨在填补此空白。

Method: 从学习的心理学定义和学习分析的方法学要求出发，提出包含五个公理的框架，推导相关定理和命题。

Result: 明确了LA的认识论立场，定义了LA结构和实践为形式对象，能统一解释多样的LA方法。

Conclusion: 该理论为设计分析方法和解释学习数据提供指导原则，将LA定位为基于可观测性的严谨状态转换系统科学，为领域成熟提供理论基础。

Abstract: Learning Analytics (LA) has rapidly expanded through practical and technological innovation, yet its foundational identity has remained theoretically under-specified. This paper addresses this gap by proposing the first axiomatic theory that formally defines the essential structure, scope, and limitations of LA. Derived from the psychological definition of learning and the methodological requirements of LA, the framework consists of five axioms specifying discrete observation, experience construction, state transition, and inference. From these axioms, we derive a set of theorems and propositions that clarify the epistemological stance of LA, including the inherent unobservability of learner states, the irreducibility of temporal order, constraints on reachable states, and the impossibility of deterministically predicting future learning. We further define LA structure and LA practice as formal objects, demonstrating the sufficiency and necessity of the axioms and showing that diverse LA approaches -- such as Bayesian Knowledge Tracing and dashboards -- can be uniformly explained within this framework. The theory provides guiding principles for designing analytic methods and interpreting learning data while avoiding naive behaviorism and category errors by establishing an explicit theoretical inference layer between observations and states. This work positions LA as a rigorous science of state transition systems based on observability, establishing the theoretical foundation necessary for the field's maturation as a scholarly discipline.

</details>


### [284] [Enhancing Large Language Models for End-to-End Circuit Analysis Problem Solving](https://arxiv.org/abs/2512.10159)
*Liangliang Chen,Weiyu Sun,Ying Zhang*

Main category: cs.CY

TL;DR: 论文针对Gemini 2.5 Pro在电路分析中表现不足，构建了增强型端到端电路问题求解器，显著提升求解准确率，拓展了大语言模型在多模态工程问题解决中的能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在工程任务中可靠性有限，Gemini 2.5 Pro在电路分析任务中不能持续给出正确解决方案，同时工程教育需要可扩展的AI工具。

Method: 先在本科电路问题集上对Gemini进行基准测试，找出两种失败模式；针对识别错误，集成微调的YOLO探测器和OpenCV处理；针对推理错误，引入基于ngspice的验证循环。

Result: 在83个问题中，提出的管道成功率达到97.59%（81个正确解决方案），远超Gemini 2.5 Pro原有的79.52%准确率。

Conclusion: 该系统拓展了大语言模型在多模态工程问题解决中的能力，可支持创建高质量教育数据集和AI教学工具。

Abstract: Large language models (LLMs) have shown strong performance in data-rich domains such as programming, but their reliability in engineering tasks remains limited. Circuit analysis -- requiring multimodal understanding and precise mathematical reasoning -- highlights these challenges. Although Gemini 2.5 Pro improves diagram interpretation and analog-circuit reasoning, it still struggles to consistently produce correct solutions when given both text and circuit diagrams. At the same time, engineering education needs scalable AI tools capable of generating accurate solutions for tasks such as automated homework feedback and question-answering. This paper presents an enhanced, end-to-end circuit problem solver built on Gemini 2.5 Pro. We first benchmark Gemini on a representative set of undergraduate circuit problems and identify two major failure modes: 1) circuit-recognition hallucinations, particularly incorrect source polarity detection, and 2) reasoning-process hallucinations, such as incorrect current directions. To address recognition errors, we integrate a fine-tuned YOLO detector and OpenCV processing to isolate voltage and current sources, enabling Gemini to re-identify source polarities from cropped images with near-perfect accuracy. To reduce reasoning errors, we introduce an ngspice-based verification loop in which Gemini generates a .cir file, ngspice simulates the circuit, and discrepancies trigger iterative regeneration with optional human-in-the-loop review. Across 83 problems, the proposed pipeline achieves a 97.59% success rate (81 correct solutions), substantially outperforming Gemini 2.5 Pro's original 79.52% accuracy. This system extends LLM capabilities for multimodal engineering problem-solving and supports the creation of high-quality educational datasets and AI-powered instructional tools.

</details>


### [285] [Designing AI-Resilient Assessments Using Interconnected Problems: A Theoretically Grounded and Empirically Validated Framework](https://arxiv.org/abs/2512.10758)
*Kaihua Ding*

Main category: cs.CY

TL;DR: 本文提出AI弹性评估框架，挑战开放式评估为学术诚信主要保障的观点，经实证验证并转化为实用设计框架。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的快速应用破坏了计算机教育中传统模块化评估，使学术评估与行业实践脱节。

Method: 建立理论结果，用四所大学数据科学课程数据验证，最后将结果转化为评估设计框架。

Result: 互联问题评估比模块化评估更具AI弹性；半结构化问题衡量学生能力更可靠；AI辅助下模块化作业成绩有虚高，互联项目能抗AI滥用。

Conclusion: 提出的评估设计框架能促进综合思维，反映现实AI增强工作流程，抵制将任务轻易委托给生成式AI，恢复学术诚信。

Abstract: The rapid adoption of generative AI has undermined traditional modular assessments in computing education, creating a disconnect between academic evaluation and industry practice. This paper presents a theoretically grounded framework for designing AI-resilient assessments, supported by formal analysis and multi-year empirical validation.
  We make three contributions. First, we establish two theoretical results: (1) assessments composed of interconnected problems, where outputs feed into subsequent stages, are more AI-resilient than modular assessments because current language models struggle with sustained multi-step reasoning and context; and (2) semi-structured problems with deterministic success criteria provide more reliable measures of student competency than fully open-ended projects, which allow AI systems to default to familiar solution patterns. These results challenge common policy and institutional guidance that promotes open-ended assessments as the primary safeguard for academic integrity.
  Second, we validate these results using data from four university data science courses (N = 138). While students achieve near-perfect scores on AI-assisted modular homework, performance drops by roughly 30 percentage points on proctored exams, indicating substantial AI score inflation. Interconnected projects remain strongly correlated with modular assessments, suggesting they measure the same underlying skills while resisting AI misuse. Proctored exams show weaker alignment, implying they may assess test-taking ability rather than intended learning outcomes.
  Third, we translate these findings into a practical assessment design framework. The proposed approach enables educators to create assessments that promote integrative thinking, reflect real-world AI-augmented workflows, and naturally resist trivial delegation to generative AI, thereby helping restore academic integrity.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [286] [MULE -- A Co-Generation Fission Power Plant Concept to Support Lunar In-Situ Resource Utilisation](https://arxiv.org/abs/2512.10705)
*Julius Mercz,Philipp Reiss,Christian Reiter*

Main category: physics.comp-ph

TL;DR: 提出月球热电联产裂变电厂概念，用Serpent 2建模微反应堆，计算表明其有至少10年热功率100kW的可行运行时间。


<details>
  <summary>Details</summary>
Motivation: 为月球持续人类存在提供可靠原位资源利用供应链，利用裂变反应堆直接加热熔盐电解过程。

Method: 使用中子输运代码Serpent 2对陶瓷芯、气冷超高温微反应堆进行建模，通过燃耗模拟和临界搜索估算其寿命。

Result: 计算得出反应堆在100kW热功率下至少有10年的中子学可行运行时间，获得了功率分布。

Conclusion: 所得功率分布为反应堆设计和电厂的热工水力技术可行性研究奠定基础。

Abstract: For a sustained human presence on the Moon, robust in-situ resource utilisation supply chains to provide consumables and propellant are necessary. A promising process is molten salt electrolysis, which typically requires temperatures in excess of 900°C. Fission reactors do not depend on solar irradiance and are thus well suited for power generation on the Moon, especially during the 14-day lunar night. As of now, fission reactors have only been considered for electric power generation, but the reactor coolant could also be used directly to heat those processes to their required temperatures. In this work, a concept for a co-generation fission power plant on the Moon that can directly heat a MSE plant to the required temperatures and provide a surplus of electrical energy for the lunar base is presented. The neutron transport code Serpent 2 is used to model a ceramic core, gas-cooled very-high-temperature microreactor design and estimate its lifetime with a burnup simulation in hot conditions with an integrated step-wise criticality search. Calculations show a neutronically feasible operation time of at least 10 years at 100kW thermal power. The obtained power distributions lay a basis for further thermal-hydraulic studies on the technical feasibility of the reactor design and the power plant.

</details>


### [287] [A Model-Guided Neural Network Method for the Inverse Scattering Problem](https://arxiv.org/abs/2512.10123)
*Olivia Tsang,Owen Melia,Vasileios Charisopoulos,Jeremy Hoskins,Yuehaw Khoo,Rebecca Willett*

Main category: physics.comp-ph

TL;DR: 本文提出一种赋予机器学习框架问题物理显式知识的方法，以较低计算和采样成本实现高质量反介质散射重建。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法在散射行为高度非线性时表现不佳，且难以融入散射过程物理知识。

Method: 以可微求解器形式赋予机器学习框架问题物理显式知识，按经典策略用递增波频率测量逐步细化散射势重建。

Result: 该方法以较低计算或采样成本实现高质量重建。

Conclusion: 所提方法能有效解决反介质散射问题，在成本和重建质量上有优势。

Abstract: Inverse medium scattering is an ill-posed, nonlinear wave-based imaging problem arising in medical imaging, remote sensing, and non-destructive testing. Machine learning (ML) methods offer increased inference speed and flexibility in capturing prior knowledge of imaging targets relative to classical optimization-based approaches; however, they perform poorly in regimes where the scattering behavior is highly nonlinear. A key limitation is that ML methods struggle to incorporate the physics governing the scattering process, which are typically inferred implicitly from the training data or loosely enforced via architectural design. In this paper, we present a method that endows a machine learning framework with explicit knowledge of problem physics, in the form of a differentiable solver representing the forward model. The proposed method progressively refines reconstructions of the scattering potential using measurements at increasing wave frequencies, following a classical strategy to stabilize recovery. Empirically, we find that our method provides high-quality reconstructions at a fraction of the computational or sampling costs of competing approaches.

</details>


<div id='q-bio.MN'></div>

# q-bio.MN [[Back]](#toc)

### [288] [Tracking large chemical reaction networks and rare events by neural networks](https://arxiv.org/abs/2512.10309)
*Jiayu Weng,Xinyi Zhu,Jing Liu,Linyuan Lü,Pan Zhang,Ying Tang*

Main category: q-bio.MN

TL;DR: 提出利用更快优化和增强采样策略的神经网络方法，实现化学反应网络高效建模，取得比以往方法更好效果并拓展应用场景。


<details>
  <summary>Details</summary>
Motivation: 解决因状态空间随系统规模指数增长导致求解化学反应主方程的难题，且提升现有自回归神经网络在高维系统和处理罕见事件时的效率。

Method: 利用自然梯度下降等更快优化和时间相关变分原理提高计算速度，采用增强采样策略捕捉罕见事件。

Result: 实现5 - 22倍的加速，在复杂反应网络中降低计算成本、提高准确性，还能应用于空间扩展的反应扩散系统。

Conclusion: 该方法能实现化学反应网络的高效建模。

Abstract: Chemical reaction networks are widely used to model stochastic dynamics in chemical kinetics, systems biology and epidemiology. Solving the chemical master equation that governs these systems poses a significant challenge due to the large state space exponentially growing with system sizes. The development of autoregressive neural networks offers a flexible framework for this problem; however, its efficiency is limited especially for high-dimensional systems and in scenarios with rare events. Here, we push the frontier of neural-network approach by exploiting faster optimizations such as natural gradient descent and time-dependent variational principle, achieving a 5- to 22-fold speedup, and by leveraging enhanced-sampling strategies to capture rare events. We demonstrate reduced computational cost and higher accuracy over the previous neural-network method in challenging reaction networks, including the mitogen-activated protein kinase (MAPK) cascade network, the hitherto largest biological network handled by the previous approaches of solving the chemical master equation. We further apply the approach to spatially extended reaction-diffusion systems, the Schlögl model with rare events, on two-dimensional lattices, beyond the recent tensor-network approach that handles one-dimensional lattices. The present approach thus enables efficient modeling of chemical reaction networks in general.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [289] [Time-Averaged Drift Approximations are Inconsistent for Inference in Drift Diffusion Models](https://arxiv.org/abs/2512.10250)
*Sicheng Liu,Alexander Fengler,Michael J. Frank,Matthew T. Harrison*

Main category: stat.ME

TL;DR: 指出漂移扩散模型（DDMs）常用的时间平均漂移近似（TADA）估计量不一致，可能使科学结论有偏差，并给出证明和数值示例。


<details>
  <summary>Details</summary>
Motivation: DDMs中时变漂移率模型准确似然评估计算成本高，常用TADA进行参数推断，但需研究其是否可靠。

Method: 先在布朗运动中证明TADA估计量的不一致性，再用注意力DDM（aDDM）进行数值示例。

Result: 证明TADA估计量不一致，数值示例显示TADA会系统性错误估计决策中注意力的影响。

Conclusion: TADA估计量不一致，使用TADA和类似替代方法得出的参数估计可能使科学结论产生偏差。

Abstract: Drift diffusion models (DDMs) have found widespread use in computational neuroscience and other fields. They model evidence accumulation in simple decision tasks as a stochastic process drifting towards a decision barrier. In models where the drift rate is both time-varying within a trial and variable across trials, the high computational cost for accurate likelihood evaluation has led to the common use of a computationally convenient surrogate for parameter inference, the time-averaged drift approximation (TADA). In each trial, the TADA assumes that the time-varying drift rate can be replaced by its temporal average throughout the trial. This approach enables fast parameter inference using analytical likelihood formulas for DDMs with constant drift. In this work, we show that such an estimator is inconsistent: it does not converge to the true drift, posing a risk of biasing scientific conclusions drawn from parameter estimates produced by TADA and similar surrogates. We provide an elementary proof of this inconsistency in what is perhaps the simplest possible setting: a Brownian motion with piecewise constant drift hitting a one-sided upper boundary. Furthermore, we conduct numerical examples with an attentional DDM (aDDM) to show that the use of TADA systematically misestimates the effect of attention in decision making.

</details>


### [290] [A Bayesian Two-Sample Mean Test for High-Dimensional Data](https://arxiv.org/abs/2512.10537)
*Daojiang He,Suren Xu,Jing Zhou*

Main category: stat.ME

TL;DR: 提出基于非信息先验贝叶斯因子的两样本贝叶斯均值检验，适用于p随n线性增长场景，模拟显示表现好且对分布误设稳健，还用数据集验证。


<details>
  <summary>Details</summary>
Motivation: 在p随n线性增长的场景下开展有效的两样本均值检验，处理方差异质性和小样本情况。

Method: 提出基于非信息先验贝叶斯因子的两样本贝叶斯均值检验，建立检验统计量的渐近正态性和渐近功效。

Result: 模拟表明该检验竞争力强，对分布误设稳健，能检测均值向量差异且控制一类错误率；在SRBCTs数据集上验证了性能。

Conclusion: 所提检验方法在多种情况下表现良好，能有效检测均值差异并控制一类错误率。

Abstract: We propose a two-sample Bayesian mean test based on the Bayes factor with non-informative priors, specifically designed for scenarios where $p$ grows with $n$ with a linear rate $p/n \to c_1 \in (0, \infty)$. We establish the asymptotic normality of the test statistic and the asymptotic power. Through extensive simulations, we demonstrate that the proposed test performs competitively, particularly when the diagonal elements have heterogeneous variances and for small sample sizes. Furthermore, our test remains robust under distribution misspecification. The proposed method not only effectively detects both sparse and non-sparse differences in mean vectors but also maintains a well-controlled type I error rate, even in small-sample scenarios. We also demonstrate the performance of our proposed test using the \texttt{SRBCTs} dataset.

</details>
