<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 13]
- [cs.CE](#cs.CE) [Total: 3]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.DS](#cs.DS) [Total: 10]
- [cs.GT](#cs.GT) [Total: 2]
- [cs.IR](#cs.IR) [Total: 7]
- [cs.LG](#cs.LG) [Total: 77]
- [cs.NE](#cs.NE) [Total: 4]
- [cs.SE](#cs.SE) [Total: 11]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [stat.ML](#stat.ML) [Total: 9]
- [stat.CO](#stat.CO) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 3]
- [math.OC](#math.OC) [Total: 2]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [math.NA](#math.NA) [Total: 2]
- [cs.CR](#cs.CR) [Total: 4]
- [cs.CG](#cs.CG) [Total: 1]
- [math.DS](#math.DS) [Total: 1]
- [cs.RO](#cs.RO) [Total: 3]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [stat.ME](#stat.ME) [Total: 2]
- [q-bio.CB](#q-bio.CB) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [econ.GN](#econ.GN) [Total: 3]
- [cs.CV](#cs.CV) [Total: 28]
- [cs.CL](#cs.CL) [Total: 23]
- [math.ST](#math.ST) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.SD](#cs.SD) [Total: 7]
- [econ.EM](#econ.EM) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [eess.IV](#eess.IV) [Total: 4]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [The Singapore Consensus on Global AI Safety Research Priorities](https://arxiv.org/abs/2506.20702)
*Yoshua Bengio,Tegan Maharaj,Luke Ong,Stuart Russell,Dawn Song,Max Tegmark,Lan Xue,Ya-Qin Zhang,Stephen Casper,Wan Sie Lee,Sören Mindermann,Vanessa Wilfred,Vidhisha Balachandran,Fazl Barez,Michael Belinsky,Imane Bello,Malo Bourgon,Mark Brakel,Siméon Campos,Duncan Cass-Beggs,Jiahao Chen,Rumman Chowdhury,Kuan Chua Seah,Jeff Clune,Juntao Dai,Agnes Delaborde,Nouha Dziri,Francisco Eiras,Joshua Engels,Jinyu Fan,Adam Gleave,Noah Goodman,Fynn Heide,Dan Hendrycks,Cyrus Hodes,Bryan Low Kian Hsiang,Minlie Huang,Sami Jawhar,Wang Jingyu,Adam Tauman Kalai,Meindert Kamphuis,Mohan Kankanhalli,Subhash Kantamneni,Mathias Bonde Kirk,Thomas Kwa,Jeffrey Ladish,Kwok-Yan Lam,Wan Lee Sie,Taewhi Lee,Xiaojian Li,Jiajun Liu,Chaochao Lu,Yifan Mai,Richard Mallah,Julian Michael,Nick Moës,Simon Möller,Kihyuk Nam,Kwan Yee Ng,Mark Nitzberg,Besmira Nushi,Seán O hÉigeartaigh,Alejandro Ortega,Pierre Peigné,James Petrie,Benjamin Prud'Homme,Reihaneh Rabbany,Nayat Sanchez-Pi,Sarah Schwettmann,Buck Shlegeris,Saad Siddiqui,Aradhana Sinha,Martín Soto,Cheston Tan,Dong Ting,Robert Trager,Brian Tse,Anthony Tung K. H.,Vanessa Wilfred,John Willes,Denise Wong,Wei Xu,Rongwu Xu,Yi Zeng,HongJiang Zhang,Djordje Žikelić*

Main category: cs.AI

TL;DR: 随着AI能力提升，确保其安全引发讨论。2025新加坡AI安全会议汇集科学家确定研究优先级，报告按纵深防御模型将AI安全研究领域分为三类。


<details>
  <summary>Details</summary>
Motivation: 快速发展的AI能力引发安全讨论，需构建可信生态，会议旨在支持该领域研究。

Method: 汇集不同地区AI科学家确定并综合AI安全研究优先级，采用纵深防御模型组织研究领域。

Result: 生成了一份基于国际AI安全报告的报告，将AI安全研究领域分为发展、评估、控制三类。

Conclusion: 强调构建可信AI生态的重要性，以及对AI安全研究领域进行分类的意义。

Abstract: Rapidly improving AI capabilities and autonomy hold significant promise of
transformation, but are also driving vigorous debate on how to ensure that AI
is safe, i.e., trustworthy, reliable, and secure. Building a trusted ecosystem
is therefore essential -- it helps people embrace AI with confidence and gives
maximal space for innovation while avoiding backlash.
  The "2025 Singapore Conference on AI (SCAI): International Scientific
Exchange on AI Safety" aimed to support research in this space by bringing
together AI scientists across geographies to identify and synthesise research
priorities in AI safety. This resulting report builds on the International AI
Safety Report chaired by Yoshua Bengio and backed by 33 governments. By
adopting a defence-in-depth model, this report organises AI safety research
domains into three types: challenges with creating trustworthy AI systems
(Development), challenges with evaluating their risks (Assessment), and
challenges with monitoring and intervening after deployment (Control).

</details>


### [2] [MAGPIE: A dataset for Multi-AGent contextual PrIvacy Evaluation](https://arxiv.org/abs/2506.20737)
*Gurusha Juneja,Alon Albalak,Wenyue Hua,William Yang Wang*

Main category: cs.AI

TL;DR: 本文提出MAGPIE基准评估大语言模型代理的上下文隐私保护能力，发现当前模型在隐私理解和任务协作方面存在不足。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的代理协作系统的发展，隐私保护至关重要，现有基准无法有效评估上下文隐私，因此研究模型对上下文隐私的理解和保护能力。

Method: 提出包含15个领域158个高风险场景的MAGPIE基准，评估现有模型对上下文隐私数据的理解和协作时保护用户隐私的能力。

Result: GPT - 4o和Claude - 2.7 - Sonnet等模型对上下文隐私理解不足，误判隐私数据可共享比例分别为25.2%和43.6%，多轮对话中即使有明确隐私指令仍会泄露隐私，多智能体系统71%的场景无法完成任务。

Conclusion: 当前模型在上下文隐私保护和协作任务解决方面未达到要求。

Abstract: The proliferation of LLM-based agents has led to increasing deployment of
inter-agent collaboration for tasks like scheduling, negotiation, resource
allocation etc. In such systems, privacy is critical, as agents often access
proprietary tools and domain-specific databases requiring strict
confidentiality. This paper examines whether LLM-based agents demonstrate an
understanding of contextual privacy. And, if instructed, do these systems
preserve inference time user privacy in non-adversarial multi-turn
conversation. Existing benchmarks to evaluate contextual privacy in LLM-agents
primarily assess single-turn, low-complexity tasks where private information
can be easily excluded. We first present a benchmark - MAGPIE comprising 158
real-life high-stakes scenarios across 15 domains. These scenarios are designed
such that complete exclusion of private data impedes task completion yet
unrestricted information sharing could lead to substantial losses. We then
evaluate the current state-of-the-art LLMs on (a) their understanding of
contextually private data and (b) their ability to collaborate without
violating user privacy. Empirical experiments demonstrate that current models,
including GPT-4o and Claude-2.7-Sonnet, lack robust understanding of contextual
privacy, misclassifying private data as shareable 25.2\% and 43.6\% of the
time. In multi-turn conversations, these models disclose private information in
59.9\% and 50.5\% of cases even under explicit privacy instructions.
Furthermore, multi-agent systems fail to complete tasks in 71\% of scenarios.
These results underscore that current models are not aligned towards both
contextual privacy preservation and collaborative task-solving.

</details>


### [3] [Dynamic Context-Aware Prompt Recommendation for Domain-Specific AI Applications](https://arxiv.org/abs/2506.20815)
*Xinye Tang,Haijun Zhai,Chaitanya Belwal,Vineeth Thayanithi,Philip Baumann,Yogesh K Roy*

Main category: cs.AI

TL;DR: 提出用于特定领域AI应用的动态上下文感知提示推荐系统，结合多种技术生成提示建议，实验证明其有用且相关。


<details>
  <summary>Details</summary>
Motivation: 大语言模型驱动的应用易受用户提示质量影响，编写高质量提示有挑战，尤其针对特定领域应用。

Method: 结合上下文查询分析、检索增强知识基础、分层技能组织和自适应技能排名，利用行为遥测和两阶段分层推理过程动态选择和排名相关技能，使用预定义和自适应模板结合少样本学习合成提示。

Result: 在真实数据集上的实验表明，该方法通过自动化和专家评估验证，达到了较高的有用性和相关性。

Conclusion: 所提出的动态上下文感知提示推荐系统在特定领域AI应用中有效可行。

Abstract: LLM-powered applications are highly susceptible to the quality of user
prompts, and crafting high-quality prompts can often be challenging especially
for domain-specific applications. This paper presents a novel dynamic
context-aware prompt recommendation system for domain-specific AI applications.
Our solution combines contextual query analysis, retrieval-augmented knowledge
grounding, hierarchical skill organization, and adaptive skill ranking to
generate relevant and actionable prompt suggestions.
  The system leverages behavioral telemetry and a two-stage hierarchical
reasoning process to dynamically select and rank relevant skills, and
synthesizes prompts using both predefined and adaptive templates enhanced with
few-shot learning. Experiments on real-world datasets demonstrate that our
approach achieves high usefulness and relevance, as validated by both automated
and expert evaluations.

</details>


### [4] [Beyond Reactive Safety: Risk-Aware LLM Alignment via Long-Horizon Simulation](https://arxiv.org/abs/2506.20949)
*Chenkai Sun,Denghui Zhang,ChengXiang Zhai,Heng Ji*

Main category: cs.AI

TL;DR: 提出框架预测模型建议传播影响，引入数据集评估语言模型长期安全意识，取得较好效果。


<details>
  <summary>Details</summary>
Motivation: 语言模型代理影响重大社会决策，需理解其建议影响以确保有益影响。

Method: 提出概念验证框架，引入含100个间接伤害场景的数据集。

Result: 在新数据集上提升超20%，在现有安全基准测试中平均胜率超70%。

Conclusion: 为构建更安全的代理指明了有前景的方向。

Abstract: Given the growing influence of language model-based agents on high-stakes
societal decisions, from public policy to healthcare, ensuring their beneficial
impact requires understanding the far-reaching implications of their
suggestions. We propose a proof-of-concept framework that projects how
model-generated advice could propagate through societal systems on a
macroscopic scale over time, enabling more robust alignment. To assess the
long-term safety awareness of language models, we also introduce a dataset of
100 indirect harm scenarios, testing models' ability to foresee adverse,
non-obvious outcomes from seemingly harmless user prompts. Our approach
achieves not only over 20% improvement on the new dataset but also an average
win rate exceeding 70% against strong baselines on existing safety benchmarks
(AdvBench, SafeRLHF, WildGuardMix), suggesting a promising direction for safer
agents.

</details>


### [5] [Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?](https://arxiv.org/abs/2506.21215)
*Haoang Chi,He Li,Wenjing Yang,Feng Liu,Long Lan,Xiaoguang Ren,Tongliang Liu,Bo Han*

Main category: cs.AI

TL;DR: 研究探讨大语言模型因果推理能力，指其仅能进行浅层因果推理，提出G^2 - Reasoner方法提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 明确大语言模型是否具备类似人类的真正因果推理能力，推动其向强人工智能发展。

Method: 深入研究基于Transformer的大语言模型自回归机制，引入新的因果问答基准CausalProbe - 2024，提出G^2 - Reasoner方法。

Result: 大语言模型在CausalProbe - 2024上表现显著下降，G^2 - Reasoner显著提升大语言模型因果推理能力。

Conclusion: 为大语言模型超越一级因果推理，迈向二级因果推理指明新方向。

Abstract: Causal reasoning capability is critical in advancing large language models
(LLMs) toward strong artificial intelligence. While versatile LLMs appear to
have demonstrated capabilities in understanding contextual causality and
providing responses that obey the laws of causality, it remains unclear whether
they perform genuine causal reasoning akin to humans. However, current evidence
indicates the contrary. Specifically, LLMs are only capable of performing
shallow (level-1) causal reasoning, primarily attributed to the causal
knowledge embedded in their parameters, but they lack the capacity for genuine
human-like (level-2) causal reasoning. To support this hypothesis,
methodologically, we delve into the autoregression mechanism of
transformer-based LLMs, revealing that it is not inherently causal.
Empirically, we introduce a new causal Q&A benchmark called CausalProbe-2024,
whose corpora are fresh and nearly unseen for the studied LLMs. The LLMs
exhibit a significant performance drop on CausalProbe-2024 compared to earlier
benchmarks, indicating the fact that they primarily engage in level-1 causal
reasoning. To bridge the gap towards level-2 causal reasoning, we draw
inspiration from the fact that human reasoning is usually facilitated by
general knowledge and intended goals. We propose G^2-Reasoner, a method that
incorporates general knowledge and goal-oriented prompts into LLMs' causal
reasoning processes. Experiments demonstrate that G^2-Reasoner significantly
enhances LLMs' causal reasoning capability, particularly in fresh and
counterfactual contexts. This work sheds light on a new path for LLMs to
advance towards genuine causal reasoning, going beyond level-1 and making
strides towards level-2.

</details>


### [6] [World-aware Planning Narratives Enhance Large Vision-Language Model Planner](https://arxiv.org/abs/2506.21230)
*Junhao Shi,Zhaoye Fei,Siyin Wang,Qipeng Guo,Jingjing Gong,Xipeng QIu*

Main category: cs.AI

TL;DR: 提出WAP框架提升大视觉语言模型在具身规划任务表现，在EB - ALFRED基准测试有显著改进，开源模型超专有系统。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型在复杂场景具身规划任务表现不佳，现有方法将指令与环境上下文分离。

Method: 提出WAP框架，通过四种认知能力让模型理解环境，用课程学习仅基于原始视觉观察开发和评估模型。

Result: 在EB - ALFRED基准测试中，Qwen2.5 - VL任务成功率绝对提升60.7，常识推理和长视野规划分别提升60.0和70.0，开源模型大幅超越专有系统。

Conclusion: WAP框架能有效提升大视觉语言模型在具身规划任务中的表现。

Abstract: Large Vision-Language Models (LVLMs) show promise for embodied planning tasks
but struggle with complex scenarios involving unfamiliar environments and
multi-step goals. Current approaches rely on environment-agnostic imitation
learning that disconnects instructions from environmental contexts, causing
models to struggle with context-sensitive instructions and rely on
supplementary cues rather than visual reasoning during long-horizon
interactions. In this work, we propose World-Aware Planning Narrative
Enhancement (WAP), a framework that infuses LVLMs with comprehensive
environmental understanding through four cognitive capabilities (visual
appearance modeling, spatial reasoning, functional abstraction, and syntactic
grounding) while developing and evaluating models using only raw visual
observations through curriculum learning. Evaluations on the EB-ALFRED
benchmark demonstrate substantial improvements, with Qwen2.5-VL achieving a
60.7 absolute improvement in task success rates, particularly in commonsense
reasoning (+60.0) and long-horizon planning (+70.0). Notably, our enhanced
open-source models outperform proprietary systems like GPT-4o and
Claude-3.5-Sonnet by a large margin.

</details>


### [7] [IXAII: An Interactive Explainable Artificial Intelligence Interface for Decision Support Systems](https://arxiv.org/abs/2506.21310)
*Pauline Speckmann,Mario Nadj,Christian Janiesch*

Main category: cs.AI

TL;DR: 开发交互式可解释智能系统IXAII，提供多方法解释和多用户定制视图，评估显示其有助于提升透明度。


<details>
  <summary>Details</summary>
Motivation: 现有事后可解释AI方法多为静态且忽略用户视角，有效性受限。

Method: 开发IXAII系统，提供四种可解释AI方法的解释，为五类用户提供定制视图，让用户控制解释内容和格式，通过访谈专家和普通用户评估。

Result: IXAII提供不同解释和多种可视化选项，被认为有助于提升透明度。

Conclusion: 弥合可解释AI方法、交互性和实际应用间的差距，为AI解释实践和人机交互提供新视角。

Abstract: Although several post-hoc methods for explainable AI have been developed,
most are static and neglect the user perspective, limiting their effectiveness
for the target audience. In response, we developed the interactive explainable
intelligent system called IXAII that offers explanations from four explainable
AI methods: LIME, SHAP, Anchors, and DiCE. Our prototype provides tailored
views for five user groups and gives users agency over the explanations'
content and their format. We evaluated IXAII through interviews with experts
and lay users. Our results indicate that IXAII, which provides different
explanations with multiple visualization options, is perceived as helpful to
increase transparency. By bridging the gaps between explainable AI methods,
interactivity, and practical implementation, we provide a novel perspective on
AI explanation practices and human-AI interaction.

</details>


### [8] [Active Inference AI Systems for Scientific Discovery](https://arxiv.org/abs/2506.21329)
*Karthik Duraisamy*

Main category: cs.AI

TL;DR: 当前AI驱动科学发现系统存在局限，需弥合三个差距，提出主动推理AI系统架构，强调人类判断不可或缺。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能系统在科学发现中受架构、推理机制和脱离实验现实等因素限制，期望推动AI驱动科学进步。

Method: 定义主动推理AI系统，包含维持研究记忆、配备规划器、构建知识图、通过闭环交互完善内部表征。

Result: 提出一个发现源于内部模型和外部验证相互作用的架构。

Conclusion: 由于模拟和实验反馈的模糊性及不确定性，人类判断应作为永久架构组件。

Abstract: The rapid evolution of artificial intelligence has led to expectations of
transformative scientific discovery, yet current systems remain fundamentally
limited by their operational architectures, brittle reasoning mechanisms, and
their separation from experimental reality. Building on earlier work, we
contend that progress in AI-driven science now depends on closing three
fundamental gaps -- the abstraction gap, the reasoning gap, and the reality gap
-- rather than on model size/data/test time compute. Scientific reasoning
demands internal representations that support simulation of actions and
response, causal structures that distinguish correlation from mechanism, and
continuous calibration. We define active inference AI systems for scientific
discovery as those that (i) maintain long-lived research memories grounded in
causal self-supervised foundation models, (ii) symbolic or neuro-symbolic
planners equipped with Bayesian guardrails, (iii) grow persistent knowledge
graphs where thinking generates novel conceptual nodes, reasoning establishes
causal edges, and real-world interaction prunes false connections while
strengthening verified pathways, and (iv) refine their internal representations
through closed-loop interaction with both high-fidelity simulators and
automated laboratories - an operational loop where mental simulation guides
action and empirical surprise reshapes understanding. In essence, we outline an
architecture where discovery arises from the interplay between internal models
that enable counterfactual reasoning and external validation that grounds
hypotheses in reality. It is also argued that the inherent ambiguity in
feedback from simulations and experiments, and underlying uncertainties makes
human judgment indispensable, not as a temporary scaffold but as a permanent
architectural component.

</details>


### [9] [TableMoE: Neuro-Symbolic Routing for Structured Expert Reasoning in Multimodal Table Understanding](https://arxiv.org/abs/2506.21393)
*Junwen Zhang,Pu Chen,Yin Zhang*

Main category: cs.AI

TL;DR: 提出用于多模态表格数据结构化推理的TableMoE架构，有创新路由机制，引入大规模数据集预训练，发布四个基准测试，实验显示其超越现有模型，证明神经符号推理有效性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在处理真实场景表格的WildStruct条件时性能有限、泛化能力差。

Method: 提出TableMoE架构，含神经符号路由机制，引入TableMoE - Align数据集预训练，发布四个WildStruct基准测试。

Result: TableMoE显著超越现有最先进模型，消融实验验证核心组件，定性分析展示其可解释性和鲁棒性。

Conclusion: 集成神经符号推理对多模态表格理解有效。

Abstract: Multimodal understanding of tables in real-world contexts is challenging due
to the complexity of structure, symbolic density, and visual degradation (blur,
skew, watermarking, incomplete structures or fonts, multi-span or
hierarchically nested layouts). Existing multimodal large language models
(MLLMs) struggle with such WildStruct conditions, resulting in limited
performance and poor generalization. To address these challenges, we propose
TableMoE, a neuro-symbolic Mixture-of-Connector-Experts (MoCE) architecture
specifically designed for robust, structured reasoning over multimodal table
data. TableMoE features an innovative Neuro-Symbolic Routing mechanism, which
predicts latent semantic token roles (e.g., header, data cell, axis, formula)
and dynamically routes table elements to specialized experts (Table-to-HTML,
Table-to-JSON, Table-to-Code) using a confidence-aware gating strategy informed
by symbolic reasoning graphs. To facilitate effective alignment-driven
pretraining, we introduce the large-scale TableMoE-Align dataset, consisting of
1.2M table-HTML-JSON-code quadruples across finance, science, biomedicine and
industry, utilized exclusively for model pretraining. For evaluation, we curate
and release four challenging WildStruct benchmarks: WMMFinQA, WMMTatQA,
WMMTabDialog, and WMMFinanceMath, designed specifically to stress-test models
under real-world multimodal degradation and structural complexity. Experimental
results demonstrate that TableMoE significantly surpasses existing
state-of-the-art models. Extensive ablation studies validate each core
component, emphasizing the critical role of Neuro-Symbolic Routing and
structured expert alignment. Through qualitative analyses, we further showcase
TableMoE's interpretability and enhanced robustness, underscoring the
effectiveness of integrating neuro-symbolic reasoning for multimodal table
understanding.

</details>


### [10] [Spatial Mental Modeling from Limited Views](https://arxiv.org/abs/2506.21458)
*Baiqiao Yin,Qineng Wang,Pingyue Zhang,Jianshu Zhang,Kangrui Wang,Zihan Wang,Jieyu Zhang,Keshigeyan Chandrasegaran,Han Liu,Ranjay Krishna,Saining Xie,Manling Li,Jiajun Wu,Li Fei-Fei*

Main category: cs.AI

TL;DR: 本文提出MindCube基准测试暴露现有视觉语言模型在构建空间心理模型上的不足，探索三种方法提升性能，‘map - then - reason’方法及强化学习显著提高准确率。


<details>
  <summary>Details</summary>
Motivation: 探究视觉语言模型能否像人类一样从少量视图想象完整场景，构建空间心理模型。

Method: 提出MindCube基准测试评估模型构建空间心理模型的能力，探索包括使用未见中间视图、自然语言推理链和认知地图三种方法，采用‘map - then - reason’协同方法及强化学习。

Result: ‘map - then - reason’方法使准确率从37.8%提升到60.8%，加入强化学习后提升到70.7%。

Conclusion: 搭建空间心理模型，积极构建和利用内部结构化空间表征并灵活推理，能显著提升对不可见空间的理解。

Abstract: Can Vision Language Models (VLMs) imagine the full scene from just a few
views, like humans do? Humans form spatial mental models, internal
representations of unseen space, to reason about layout, perspective, and
motion. Our new MindCube benchmark with 21,154 questions across 3,268 images
exposes this critical gap, where existing VLMs exhibit near-random performance.
Using MindCube, we systematically evaluate how well VLMs build robust spatial
mental models through representing positions (cognitive mapping), orientations
(perspective-taking), and dynamics (mental simulation for "what-if" movements).
We then explore three approaches to help VLMs approximate spatial mental
models, including unseen intermediate views, natural language reasoning chains,
and cognitive maps. The significant improvement comes from a synergistic
approach, "map-then-reason", that jointly trains the model to first generate a
cognitive map and then reason upon it. By training models to reason over these
internal maps, we boosted accuracy from 37.8% to 60.8% (+23.0%). Adding
reinforcement learning pushed performance even further to 70.7% (+32.9%). Our
key insight is that such scaffolding of spatial mental models, actively
constructing and utilizing internal structured spatial representations with
flexible reasoning processes, significantly improves understanding of
unobservable space.

</details>


### [11] [Ad-Hoc Human-AI Coordination Challenge](https://arxiv.org/abs/2506.21490)
*Tin Dizdarević,Ravi Hammond,Tobias Gessler,Anisoara Calinescu,Jonathan Cook,Matteo Gallici,Andrei Lupu,Jakob Nicolaus Foerster*

Main category: cs.AI

TL;DR: 提出Ad - Hoc Human - AI Coordination Challenge (AH2AC2) 解决人类评估难题，开发人类代理，开源数据集并给出基线结果，代码开源。


<details>
  <summary>Details</summary>
Motivation: 实现AI代理与人类无缝协调是现实应用的关键挑战，Hanabi虽适合测试但人类评估存在限制。

Method: 引入AH2AC2，在大规模人类数据集上开发人类代理，开源3079个游戏的数据集，通过受控评估系统托管代理。

Result: 给出两人和三人Hanabi场景的基线结果。

Conclusion: 通过一系列方法和措施，有望促进人类与AI协调的研究发展，代码开源利于进一步探索。

Abstract: Achieving seamless coordination between AI agents and humans is crucial for
real-world applications, yet it remains a significant open challenge. Hanabi is
a cooperative card game featuring imperfect information, constrained
communication, theory of mind requirements, and coordinated action -- making it
an ideal testbed for human-AI coordination. However, its use for human-AI
interaction has been limited by the challenges of human evaluation. In this
work, we introduce the Ad-Hoc Human-AI Coordination Challenge (AH2AC2) to
overcome the constraints of costly and difficult-to-reproduce human
evaluations. We develop \textit{human proxy agents} on a large-scale human
dataset that serve as robust, cheap, and reproducible human-like evaluation
partners in AH2AC2. To encourage the development of data-efficient methods, we
open-source a dataset of 3,079 games, deliberately limiting the amount of
available human gameplay data. We present baseline results for both two- and
three- player Hanabi scenarios. To ensure fair evaluation, we host the proxy
agents through a controlled evaluation system rather than releasing them
publicly. The code is available at
\href{https://github.com/FLAIROx/ah2ac2}{https://github.com/FLAIROx/ah2ac2}.

</details>


### [12] [Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge](https://arxiv.org/abs/2506.21506)
*Boyu Gou,Zanming Huang,Yuting Ning,Yu Gu,Michael Lin,Weijian Qi,Andrei Kopanev,Botao Yu,Bernal Jiménez Gutiérrez,Yiheng Shu,Chan Hee Song,Jiaman Wu,Shijie Chen,Hanane Nour Moussa,Tianshu Zhang,Jian Xie,Yifei Li,Tianci Xue,Zeyi Liao,Kai Zhang,Boyuan Zheng,Zhaowei Cai,Viktor Rozgic,Morteza Ziyadi,Huan Sun,Yu Su*

Main category: cs.AI

TL;DR: 本文引入Mind2Web 2基准和Agent - as - Judge框架，评估九种前沿代理搜索系统，表现最佳的系统已能达到人类性能的50 - 70%，为下一代代理搜索系统发展提供基础。


<details>
  <summary>Details</summary>
Motivation: 现有评估基准和方法无法适应代理搜索日益增长的复杂性和开放性，需要新的评估工具。

Method: 引入Mind2Web 2基准，提出Agent - as - Judge框架，基于树状评分设计构建特定任务的评判代理评估答案正确性和来源归因。

Result: 对九种前沿代理搜索系统和人类表现进行全面评估及详细错误分析，最佳系统OpenAI Deep Research能达到人类性能的50 - 70%且耗时减半。

Conclusion: Mind2Web 2为下一代代理搜索系统的开发和基准测试提供了严格基础。

Abstract: Agentic search such as Deep Research systems, where large language models
autonomously browse the web, synthesize information, and return comprehensive
citation-backed answers, represents a major shift in how users interact with
web-scale information. While promising greater efficiency and cognitive
offloading, the growing complexity and open-endedness of agentic search have
outpaced existing evaluation benchmarks and methodologies, which largely assume
short search horizons and static answers. In this paper, we introduce Mind2Web
2, a benchmark of 130 realistic, high-quality, and long-horizon tasks that
require real-time web browsing and extensive information synthesis, constructed
with over 1,000 hours of human labor. To address the challenge of evaluating
time-varying and complex answers, we propose a novel Agent-as-a-Judge
framework. Our method constructs task-specific judge agents based on a
tree-structured rubric design to automatically assess both answer correctness
and source attribution. We conduct a comprehensive evaluation of nine frontier
agentic search systems and human performance, along with a detailed error
analysis to draw insights for future development. The best-performing system,
OpenAI Deep Research, can already achieve 50-70% of human performance while
spending half the time, showing a great potential. Altogether, Mind2Web 2
provides a rigorous foundation for developing and benchmarking the next
generation of agentic search systems.

</details>


### [13] [PsyLite Technical Report](https://arxiv.org/abs/2506.21536)
*Fangjun Ding,Renyu Zhang,Xinyu Feng,Chengye Xie,Zheng Zhang,Yanting Zhang*

Main category: cs.AI

TL;DR: 本文提出轻量级心理咨询大语言模型代理PsyLite，经两阶段训练策略增强能力，设计创新条件RAG，评估显示其在多方面表现出色，还可用量化技术低硬件部署。


<details>
  <summary>Details</summary>
Motivation: 现有AI驱动的心理咨询模型在对话安全、详细场景处理和轻量级部署方面存在不足。

Method: 基于InternLM2.5 - 7B - chat开发PsyLite，采用两阶段训练策略，用Ollama和Open WebUI部署，用Pipelines创建自定义工作流，设计创新条件RAG。

Result: PsyLite在中文通用评估、心理咨询专业评估和对话安全评估中优于基线模型，心理咨询专业性得分提高47.6%，对话安全得分提高2.4%，能用5GB内存运行。

Conclusion: PsyLite为资源受限环境下的心理咨询应用提供了可行解决方案。

Abstract: With the rapid development of digital technology, AI-driven psychological
counseling has gradually become an important research direction in the field of
mental health. However, existing models still have deficiencies in dialogue
safety, detailed scenario handling, and lightweight deployment. To address
these issues, this study proposes PsyLite, a lightweight psychological
counseling large language model agent developed based on the base model
InternLM2.5-7B-chat. Through a two-stage training strategy (hybrid distillation
data fine-tuning and ORPO preference optimization), PsyLite enhances the
model's deep-reasoning ability, psychological counseling ability, and safe
dialogue ability. After deployment using Ollama and Open WebUI, a custom
workflow is created with Pipelines. An innovative conditional RAG is designed
to introduce crosstalk humor elements at appropriate times during psychological
counseling to enhance user experience and decline dangerous requests to
strengthen dialogue safety. Evaluations show that PsyLite outperforms the
baseline models in the Chinese general evaluation (CEval), psychological
counseling professional evaluation (CPsyCounE), and dialogue safety evaluation
(SafeDialBench), particularly in psychological counseling professionalism
(CPsyCounE score improvement of 47.6\%) and dialogue safety (\safe{} score
improvement of 2.4\%). Additionally, the model uses quantization technology
(GGUF q4\_k\_m) to achieve low hardware deployment (5GB memory is sufficient
for operation), providing a feasible solution for psychological counseling
applications in resource-constrained environments.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [14] [A generalised framework for phase field-based modelling of coupled problems: application to thermo-mechanical fracture, hydraulic fracture, hydrogen embrittlement and corrosion](https://arxiv.org/abs/2506.20763)
*Y. Navidtehrani,C. Betegón,E. Martínez-Pañeda*

Main category: cs.CE

TL;DR: 提出结合相场和多物理场建模处理耦合结构完整性问题的通用公式，在Abaqus实现，应用于四个问题，结果与实验等数据吻合，代码开源。


<details>
  <summary>Details</summary>
Motivation: 提出新的通用方法处理耦合结构完整性问题。

Method: 结合相场和多物理场建模，利用传热方程的通用性，通过简单子程序在Abaqus中实现耦合多变量现象。

Result: 结果与实验数据、现有数值和解析解非常吻合。

Conclusion: 所提出的通用理论和计算框架有效，开发的用户子程序可在指定网址免费获取。

Abstract: We present a novel, generalised formulation to treat coupled structural
integrity problems by combining phase field and multi-physics modelling. The
approach exploits the versatility of the heat transfer equation and is
therefore well suited to be adopted in commercial finite element packages,
requiring only integration point-level implementation. This aspect is
demonstrated here by implementing coupled, multi-variable phenomena through
simple \texttt{UMAT} and \texttt{UMATHT} subroutines in the finite element
package \texttt{Abaqus}. The generalised theoretical and computational
framework presented is particularised to four problems of engineering and
scientific relevance: thermo-mechanical fracture, hydraulic fracture,
hydrogen-assisted cracking and metallic corrosion. 2D and 3D problems are
considered. The results reveal a very good agreement with experimental data,
and existing numerical and analytical solutions.The user subroutines developed
are made freely available at https://mechmat.web.ox.ac.uk/codes.

</details>


### [15] [A Hereditary Integral, Transient Network Approach to Modeling Permanent Set and Viscoelastic Response in Polymers](https://arxiv.org/abs/2506.20773)
*Stephen T. Castonguay,Joshua B. Fernandes,Michael A. Puso,Sylvie Aubry*

Main category: cs.CE

TL;DR: 提出用于模拟聚合物粘弹性和永久变形的高效数值框架，用多种材料模型验证，多示例展示其处理复杂加载的能力。


<details>
  <summary>Details</summary>
Motivation: 为聚合物的粘弹性和永久变形建模提供有效方法。

Method: 基于瞬态网络理论的遗传积分形式，对不同自由能的核进行分解建立递推关系，使用多种材料模型。

Result: 建立了适用于高可压缩和近不可压缩材料的技术，多个示例展示能处理复杂加载下的率相关响应和残余应变。

Conclusion: 该数值框架能有效模拟聚合物的粘弹性和永久变形，处理复杂加载情况。

Abstract: An efficient numerical framework is presented for modeling viscoelasticity
and permanent set of polymers. It is based on the hereditary integral form of
transient network theory, in which polymer chains belong to distinct networks
each with different natural equilibrium states. Chains continually detach from
previously formed networks and reattach to new networks in a state of zero
stress. The free energy of these networks is given in terms of the deformation
gradient relative to the configuration at which the network was born. A
decomposition of the kernel for various free energies allows for a recurrence
relationship to be established, bypassing the need to integrate over all time
history. The technique is established for both highly compressible and nearly
incompressible materials through the use of neo-Hookean, Blatz-Ko, Yeoh, and
Ogden-Hill material models. Multiple examples are presented showing the ability
to handle rate-dependent response and residual strains under complex loading
histories.

</details>


### [16] [Counterfactual Voting Adjustment for Quality Assessment and Fairer Voting in Online Platforms with Helpfulness Evaluation](https://arxiv.org/abs/2506.21362)
*Chang Liu,Yixin Wang,Moontae Lee*

Main category: cs.CE

TL;DR: 为公平评估信息质量，提出反事实投票调整（CVA）框架，实验证明其有效且优于基于聚合投票的系统排名。


<details>
  <summary>Details</summary>
Motivation: 现有聚合投票评估信息质量存在位置和先验投票的级联影响导致的偏差，需要更公平的评估方法。

Method: 提出反事实投票调整（CVA）因果框架，考虑个体投票的背景情况。

Result: 通过初步和半合成实验，CVA有效建模位置和羊群效应偏差；真实实验中，基于CVA学习的质量对内容重新排名与用户情绪和GPT - 4o的质量评估更一致，优于其他排名方法。

Conclusion: CVA框架能更公平地评估信息质量，还能提供专家用户群体行为动态的比较见解。

Abstract: Efficient access to high-quality information is vital for online platforms.
To promote more useful information, users not only create new content but also
evaluate existing content, often through helpfulness voting. Although
aggregated votes help service providers rank their user content, these votes
are often biased by disparate accessibility per position and the cascaded
influence of prior votes. For a fairer assessment of information quality, we
propose the Counterfactual Voting Adjustment (CVA), a causal framework that
accounts for the context in which individual votes are cast. Through
preliminary and semi-synthetic experiments, we show that CVA effectively models
the position and herding biases, accurately recovering the predefined content
quality. In a real experiment, we demonstrate that reranking content based on
the learned quality by CVA exhibits stronger alignment with both user sentiment
and quality evaluation assessed by GPT-4o, outperforming system rankings based
on aggregated votes and model-based rerankings without causal inference. Beyond
the individual quality inference, our embeddings offer comparative insights
into the behavioral dynamics of expert user groups across 120 major
StackExchange communities.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [17] [Condensed Representation of RDF and its Application on Graph Versioning](https://arxiv.org/abs/2506.21203)
*Jey Puget Gil,Emmanuel Coquery,John Samuel,Gilles Gesquiere*

Main category: cs.DB

TL;DR: 本文旨在介绍并形式化不断演变的知识图的浓缩表示，以应对知识图数据不断更新的情况。


<details>
  <summary>Details</summary>
Motivation: 研究领域中的演变现象有助于理解不同时间点实体间关系和预测未来趋势，知识图可表示复杂现象，但需组织不断演变的数据以便分析。

Method: 文章未提及具体方法，将介绍并形式化不断演变的知识图的浓缩表示。

Result: 文章未提及具体结果。

Conclusion: 文章未提及具体结论。

Abstract: The study of the evolving phenomena in a domain helps to understand the
relationships between entities at different points in time and predict future
trends. These phenomena, often complex, can be represented using knowledge
graphs, which have the capability to model heterogeneous data from multiple
sources. Nowadays, a considerable amount of sources delivering periodic updates
to knowledge graphs in various domains is openly available. The evolution of
data is of interest to knowledge graph management systems, and therefore it is
crucial to organize these constantly evolving data to make them easily
accessible and exploitable for analyzes. In this article, we will present and
formalize the condensed representation of these evolving graphs.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [18] [Scalable GPU Performance Variability Analysis framework](https://arxiv.org/abs/2506.20674)
*Ankur Lahiry,Ayush Pokharel,Seth Ockerman,Amal Gueroudji,Line Pouchard,Tanzima Z. Islam*

Main category: cs.DC

TL;DR: 提出分布式数据分析框架以处理GPU性能日志，可扩展且能减少内存压力、避免瓶颈，还应用于实际工作负载分析。


<details>
  <summary>Details</summary>
Motivation: 现有GPU性能日志分析工具需大量内存和时间，顺序处理方式不适用于复杂和大规模的HPC工作流，阻碍性能分析集成到自动化工作流。

Method: 引入分布式数据分析框架，将数据集划分为可独立分析的分片，并通过MPI级别并发处理。

Result: 将框架应用于实际HPC和AI工作负载的Nsight Compute跟踪数据，展示其诊断性能可变性的能力，揭示内存传输延迟对GPU内核行为的影响。

Conclusion: 分布式数据分析框架能有效应对大规模GPU性能日志分析，减少内存压力和避免瓶颈，有助于及时获取见解并应用于自动化工作流。

Abstract: Analyzing large-scale performance logs from GPU profilers often requires
terabytes of memory and hours of runtime, even for basic summaries. These
constraints prevent timely insight and hinder the integration of performance
analytics into automated workflows. Existing analysis tools typically process
data sequentially, making them ill-suited for HPC workflows with growing trace
complexity and volume. We introduce a distributed data analysis framework that
scales with dataset size and compute availability. Rather than treating the
dataset as a single entity, our system partitions it into independently
analyzable shards and processes them concurrently across MPI ranks. This design
reduces per-node memory pressure, avoids central bottlenecks, and enables
low-latency exploration of high-dimensional trace data. We apply the framework
to end-to-end Nsight Compute traces from real HPC and AI workloads, demonstrate
its ability to diagnose performance variability, and uncover the impact of
memory transfer latency on GPU kernel behavior.

</details>


### [19] [ClusterRCA: Network Failure Diagnosis in HPC Systems Using Multimodal Data](https://arxiv.org/abs/2506.20673)
*Yongqian Sun,Xijie Pan,Xiao Xiong,Lei Tao,Jiaju Wang,Shenglin Zhang,Yuan Yuan,Yuqi Li,Kunlin Jian*

Main category: cs.DC

TL;DR: 本文提出用于HPC系统网络故障诊断的ClusterRCA框架，结合多模态数据，实验证明其准确性高且性能稳健。


<details>
  <summary>Details</summary>
Motivation: 现有网络故障诊断方法因数据异质性和准确性不足，无法直接应用于HPC场景。

Method: ClusterRCA框架从拓扑连接的NIC对提取特征，结合基于分类器和基于图的方法，构建故障图并执行随机游走定位根因。

Result: 在顶级全球HPC设备供应商收集的数据集上实验，ClusterRCA诊断HPC系统网络故障准确性高，在不同应用场景性能稳健。

Conclusion: ClusterRCA是一种有效且稳健的HPC系统网络故障诊断框架。

Abstract: Network failure diagnosis is challenging yet critical for high-performance
computing (HPC) systems. Existing methods cannot be directly applied to HPC
scenarios due to data heterogeneity and lack of accuracy. This paper proposes a
novel framework, called ClusterRCA, to localize culprit nodes and determine
failure types by leveraging multimodal data. ClusterRCA extracts features from
topologically connected network interface controller (NIC) pairs to analyze the
diverse, multimodal data in HPC systems. To accurately localize culprit nodes
and determine failure types, ClusterRCA combines classifier-based and
graph-based approaches. A failure graph is constructed based on the output of
the state classifier, and then it performs a customized random walk on the
graph to localize the root cause. Experiments on datasets collected by a
top-tier global HPC device vendor show ClusterRCA achieves high accuracy in
diagnosing network failure for HPC systems. ClusterRCA also maintains robust
performance across different application scenarios.

</details>


### [20] [Utility-Driven Speculative Decoding for Mixture-of-Experts](https://arxiv.org/abs/2506.20675)
*Anish Saxena,Po-An Tsai,Hritvik Taneja,Aamer Jaleel,Moinuddin Qureshi*

Main category: cs.DC

TL;DR: 传统推测解码不适用于MoE模型，提出Cascade框架，通过选择性启用推测和解调K值，使推测解码在MoE模型中实用化。


<details>
  <summary>Details</summary>
Motivation: 解决推测解码在新兴MoE模型中无效甚至导致减速的问题，让推测解码在MoE模型中发挥作用。

Method: 提出Cascade框架，利用推测效用这一轻量级指标，通过短测试和长设置阶段进行周期性决策，为每个请求选择效用最大化的K值。

Result: 在五个流行的MoE模型上评估，将减速限制在5%（对比1.5倍减速），吞吐量比静态K值提高7 - 14%。

Conclusion: Cascade框架使推测解码在MoE模型中具有实用性。

Abstract: GPU memory bandwidth is the main bottleneck for low-latency Large Language
Model (LLM) inference. Speculative decoding leverages idle GPU compute by using
a lightweight drafter to propose K tokens, which the LLM verifies in parallel,
boosting token throughput. In conventional dense LLMs, all model weights are
fetched each iteration, so speculation adds no latency overhead. Emerging
Mixture of Experts (MoE) models activate only a subset of weights per token,
greatly reducing data movement. However, we show that speculation is
ineffective for MoEs: draft tokens collectively activate more weights,
increasing data movement and verification time by 2-3x. When token throughput
gains fail to offset this overhead, speculation causes slowdowns up to 1.5x,
making it infeasible. Even when useful, the optimal K varies by task, model,
and even between requests and iterations. Thus, despite widespread use in dense
LLMs, speculation remains impractical in leading MoEs.
  We present Cascade, a utility-driven framework that selectively enables
speculation to avoid slowdowns and dynamically tunes K to accelerate MoE
serving. Cascade uses a lightweight metric, speculation utility, the ratio of
token gains to verification cost, which shows iteration-level locality,
enabling periodic decisions via short test and longer set phases. For each
request, Cascade disables speculation if utility drops below one during
testing, and when utility exceeds one, tests multiple K-values to choose the
utility-maximizing K for the set phase. We implement Cascade in vLLM and
evaluate it on five popular MoEs with workloads spanning code, math,
extraction, and mixed tasks. Cascade limits slowdown to 5% (vs. 1.5x) and
improves throughput by 7-14% over static K, making speculative decoding
practical for MoEs.

</details>


### [21] [ParEval-Repo: A Benchmark Suite for Evaluating LLMs with Repository-level HPC Translation Tasks](https://arxiv.org/abs/2506.20938)
*Joshua H. Davis,Daniel Nichols,Ishan Khillan,Abhinav Bhatele*

Main category: cs.DC

TL;DR: 提出ParEval - Repo框架评估大语言模型跨GPGPU执行模型自动翻译代码库的效果，结果显示小项目可行但大项目有挑战。


<details>
  <summary>Details</summary>
Motivation: GPGPU架构多样化，现有可移植执行模型需开发者大量精力移植和优化，大语言模型可减轻程序员负担。

Method: 提出ParEval - Repo基准和测试框架，用该框架评估多种开源和商业大语言模型，采用非代理和自上而下的代理方法。

Result: 大语言模型对小的科学应用程序代码翻译可行，但在生成功能性构建系统和处理跨文件依赖方面难以扩展到更大代码库。

Conclusion: 大语言模型在跨GPGPU执行模型自动翻译代码库上有一定效果，但在处理大型代码库时面临挑战。

Abstract: GPGPU architectures have become significantly diverse in recent years, which
has led to an emergence of a variety of specialized programming models and
software stacks to support them. While portable execution models exist, they
still require significant developer effort to port to and optimize for
different hardware architectures. Recent advances in large language models
(LLMs) can help us reduce some of this programmer burden. In this paper, we
present a novel benchmark and testing framework, ParEval-Repo, which can be
used to evaluate the efficacy of LLM-based approaches in automatically
translating entire codebases across GPGPU execution models. ParEval-Repo
includes several scientific computing and AI mini-applications in a range of
programming models, and levels of repository complexity. We use ParEval-Repo to
evaluate a range of state-of-the-art open-source and commercial LLMs, with both
a non-agentic and a top-down agentic approach. We assess code generated by the
LLMs and approaches in terms of compilability, functional correctness,
categories of build errors, and the cost of translation in terms of the number
of inference tokens. Our results demonstrate that LLM translation of scientific
applications is feasible for small programs but difficulty with generating
functional build systems and cross-file dependencies pose challenges in scaling
to larger codebases.

</details>


### [22] [Portable High-Performance Kernel Generation for a Computational Fluid Dynamics Code with DaCe](https://arxiv.org/abs/2506.20994)
*Måns I. Andersson,Martin Karp,Niclas Jansson,Stefano Markidis*

Main category: cs.DC

TL;DR: 本文利用DaCe框架自动生成高性能内核，以计算流体动力学（CFD）关键计算内核为例，展示其跨平台的可移植性和性能，强调自动代码生成对大规模科学应用可持续性的可行性。


<details>
  <summary>Details</summary>
Motivation: 新的高性能计算（HPC）加速器出现，HPC系统硬件多样性增加，开发特定架构代码阻碍大规模科学应用的可持续性，需有效针对不同硬件架构。

Method: 利用数据中心并行编程框架DaCe自动生成高性能内核，将其应用于CFD关键计算内核，用DaCe的SDFG表示法阐述内核公式，将生成代码集成到Neko求解器。

Result: 生成的代码在Nvidia GH200、Nvidia A100和AMD MI250X等多个GPU平台上具有可移植性和竞争力的性能。

Conclusion: 自动代码生成有潜力，使用可移植解决方案确保大规模科学应用的长期可持续性是可行的。

Abstract: With the emergence of new high-performance computing (HPC) accelerators, such
as Nvidia and AMD GPUs, efficiently targeting diverse hardware architectures
has become a major challenge for HPC application developers. The increasing
hardware diversity in HPC systems often necessitates the development of
architecture-specific code, hindering the sustainability of large-scale
scientific applications. In this work, we leverage DaCe, a data-centric
parallel programming framework, to automate the generation of high-performance
kernels. DaCe enables automatic code generation for multicore processors and
various accelerators, reducing the burden on developers who would otherwise
need to rewrite code for each new architecture. Our study demonstrates DaCe's
capabilities by applying its automatic code generation to a critical
computational kernel used in Computational Fluid Dynamics (CFD). Specifically,
we focus on Neko, a Fortran-based solver that employs the spectral-element
method, which relies on small tensor operations. We detail the formulation of
this computational kernel using DaCe's Stateful Dataflow Multigraph (SDFG)
representation and discuss how this approach facilitates high-performance code
generation. Additionally, we outline the workflow for seamlessly integrating
DaCe's generated code into the Neko solver. Our results highlight the
portability and performance of the generated code across multiple platforms,
including Nvidia GH200, Nvidia A100, and AMD MI250X GPUs, with competitive
performance results. By demonstrating the potential of automatic code
generation, we emphasise the feasibility of using portable solutions to ensure
the long-term sustainability of large-scale scientific applications.

</details>


### [23] [Bridding OT and PaaS in Edge-to-Cloud Continuum](https://arxiv.org/abs/2506.21072)
*Carlos J Barrios,Yves Denneulin*

Main category: cs.DC

TL;DR: 本文介绍OTPaaS可高效管理和存储数据，展示其成功部署、应用管理及集成组件，还提及应对的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 为工业转型和数据主权提供高效的数据管理和存储框架，提升安全性、可靠性等多方面性能。

Method: 利用平台即服务（PaaS）模型。

Result: 实现了OTPaaS的成功部署，具备可适应的应用管理能力，有适用于边缘和云环境的集成组件。

Conclusion: OTPaaS能解决特定用例的关键挑战，对工业转型和数据主权具有重要意义。

Abstract: The Operational Technology Platform as a Service (OTPaaS) initiative provides
a structured framework for the efficient management and storage of data. It
ensures excellent response times while improving security, reliability, data
and technology sovereignty, robustness, and energy efficiency, which are
crucial for industrial transformation and data sovereignty. This paper
illustrates successful deployment, adaptable application management, and
various integration components catering to Edge and Cloud environments. It
leverages the advantages of the Platform as a Service model and highlights key
challenges that have been addressed for specific use cases.

</details>


### [24] [BLOCKS: Blockchain-supported Cross-Silo Knowledge Sharing for Efficient LLM Services](https://arxiv.org/abs/2506.21033)
*Zhaojiacheng Zhou,Hongze Liu,Shijing Yuan,Hanning Zhang,Jiong Lou,Chentao Wu,Jie Li*

Main category: cs.DC

TL;DR: 提出基于区块链的外部知识框架解决大语言模型幻觉问题，经实验验证能在区块链环境实现高效知识共享。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在幻觉问题，下游任务相关知识分散难获取，需解决知识鸿沟并保证数据安全。

Method: 将本地数据知识提炼为提示，在区块链执行交易和记录，引入声誉机制和交叉验证，设计查询生成框架。

Result: 在各种知识源上的实验表明，该框架能在区块链环境实现高效的大语言模型服务知识共享。

Conclusion: 所提出的基于区块链的外部知识框架是有效的，可解决大语言模型幻觉问题及知识获取难题。

Abstract: The hallucination problem of Large Language Models (LLMs) has increasingly
drawn attention. Augmenting LLMs with external knowledge is a promising
solution to address this issue. However, due to privacy and security concerns,
a vast amount of downstream task-related knowledge remains dispersed and
isolated across various "silos," making it difficult to access. To bridge this
knowledge gap, we propose a blockchain-based external knowledge framework that
coordinates multiple knowledge silos to provide reliable foundational knowledge
for large model retrieval while ensuring data security. Technically, we distill
knowledge from local data into prompts and execute transactions and records on
the blockchain. Additionally, we introduce a reputation mechanism and
cross-validation to ensure knowledge quality and provide incentives for
participation. Furthermore, we design a query generation framework that
provides a direct API interface for large model retrieval. To evaluate the
performance of our proposed framework, we conducted extensive experiments on
various knowledge sources. The results demonstrate that the proposed framework
achieves efficient LLM service knowledge sharing in blockchain environments.

</details>


### [25] [Enabling Bitcoin Smart Contracts on the Internet Computer](https://arxiv.org/abs/2506.21327)
*Ryan Croote,Islam El-Ashi,Thomas Locher,Yvonne-Anne Pignolet*

Main category: cs.DC

TL;DR: 提出在互联网计算机（IC）上执行图灵完备的比特币智能合约的架构，直接让IC与比特币节点交互，评估显示能实现复杂比特币去中心化应用。


<details>
  <summary>Details</summary>
Motivation: 人们对以编程方式获取比特币锁定价值兴趣渐长，现有机制多存在问题，需新方法。

Method: 提出一种架构，让IC与比特币节点直接交互，执行图灵完备的比特币智能合约，解决两者特性差异问题。

Result: 基于主网运行的比特币集成测量得出评估结果，能在几秒内完成最终确定且执行成本低。

Conclusion: 该集成可实现此前不可行或不经济的复杂比特币去中心化应用。

Abstract: There is growing interest in providing programmatic access to the value
locked in Bitcoin, which famously offers limited programmability itself.
Various approaches have been put forth in recent years, with the vast majority
of proposed mechanisms either building new functionality on top of Bitcoin or
leveraging a bridging mechanism to enable smart contracts that make use of
``wrapped'' bitcoins on entirely different platforms.
  In this work, an architecture is presented that follows a different approach.
The architecture enables the execution of Turing-complete Bitcoin smart
contracts on the Internet Computer (IC), a blockchain platform for hosting and
executing decentralized applications. Instead of using a bridge, IC and Bitcoin
nodes interact directly, eliminating potential security risks that the use of a
bridge entails. This integration requires novel concepts, in particular to
reconcile the probabilistic nature of Bitcoin with the irreversibility of
finalized state changes on the IC, which may be of independent interest.
  In addition to the presentation of the architecture, we provide evaluation
results based on measurements of the Bitcoin integration running on mainnet.
The evaluation results demonstrate that, with finalization in a few seconds and
low execution costs, this integration enables complex Bitcoin-based
decentralized applications that were not practically feasible or economically
viable before.

</details>


### [26] [Carbon-Aware Microservice Deployment for Optimal User Experience on a Budget](https://arxiv.org/abs/2506.21422)
*Kevin Kreutz,Philipp Wiesner,Monica Vitali*

Main category: cs.DC

TL;DR: 提出一种在小时级碳预算下运行微服务的碳感知方法，实验证明该方法能适应工作负载和碳强度变化。


<details>
  <summary>Details</summary>
Motivation: 现有碳感知策略不适用于面向服务的云应用，需新方法处理微服务碳预算问题。

Method: 为每个微服务选择最合适的版本和水平扩展，在预算约束内最大化用户体验和收益。

Result: 在各种应用配置和碳预算下的实验表明，该方法能适应工作负载和碳强度的变化。

Conclusion: 提出的碳感知方法可有效应对微服务在碳预算下的运营问题。

Abstract: The carbon footprint of data centers has recently become a critical concern.
So far, most carbon-aware strategies have focused on leveraging the flexibility
of scheduling decisions for batch processing by shifting the time and location
of workload executions. However, such approaches cannot be applied to
service-oriented cloud applications, since they have to be reachable at every
point in time and often at low latencies. We propose a carbon-aware approach
for operating microservices under hourly carbon budgets. By choosing the most
appropriate version and horizontal scaleout for each microservice, our strategy
maximizes user experience and revenue while staying within budget constraints.
Experiments across various application configurations and carbon budgets
demonstrate that the approach adapts properly to changing workloads and carbon
intensities.

</details>


### [27] [exa-AMD: A Scalable Workflow for Accelerating AI-Assisted Materials Discovery and Design](https://arxiv.org/abs/2506.21449)
*Maxim Moraru,Weiyi Xia,Zhuo Ye,Feng Zhang,Yongxin Yao,Ying Wai Li,Cai-Zhuang Wang*

Main category: cs.DC

TL;DR: exa - AMD是基于Python的应用程序，集成AI/ML工具等加速功能材料发现设计，依赖Parsl实现灵活任务执行与工作流扩展。


<details>
  <summary>Details</summary>
Motivation: 加速功能材料的发现和设计。

Method: 将AI/ML工具、材料数据库和量子力学计算集成到可扩展的高性能工作流中，利用Parsl库执行任务。

Result: exa - AMD能将工作流逻辑与执行配置解耦。

Conclusion: 研究人员无需为每个系统重新实现工作流即可扩展工作流。

Abstract: exa-AMD is a Python-based application designed to accelerate the discovery
and design of functional materials by integrating AI/ML tools, materials
databases, and quantum mechanical calculations into scalable, high-performance
workflows. The execution model of exa-AMD relies on Parsl, a task-parallel
programming library that enables a flexible execution of tasks on any computing
resource from laptops to supercomputers. By using Parsl, exa-AMD is able to
decouple the workflow logic from execution configuration, thereby empowering
researchers to scale their workflows without having to reimplement them for
each system.

</details>


### [28] [Efficient and Reuseable Cloud Configuration Search Using Discovery Spaces](https://arxiv.org/abs/2506.21467)
*Michael Johnston,Burkhard Ringlein,Christoph Hagleitner,Alessandro Pomponio,Vassilis Vassiliadis,Christian Pinto,Srikumar Venugopal*

Main category: cs.DC

TL;DR: 提出Discovery Space抽象概念，解决云资源配置问题，可跨工作负载应用，提升搜索效率和知识复用。


<details>
  <summary>Details</summary>
Motivation: 在满足服务水平协议的前提下，以最小成本为给定工作负载找到最优云资源集，配置空间的大量选项使搜索困难。

Method: 提出Discovery Space抽象概念，形式化描述工作负载配置问题，并给出具体实现。

Result: 实现安全透明的数据共享，提高大搜索空间中最优配置检测效率，使配置搜索速度提升超90%。

Conclusion: Discovery Space能跨不同工作负载应用，可提高搜索效率和实现知识复用。

Abstract: Finding the optimal set of cloud resources to deploy a given workload at
minimal cost while meeting a defined service level agreement is an active area
of research. Combining tens of parameters applicable across a large selection
of compute, storage, and services offered by cloud providers with similar
numbers of application-specific parameters leads to configuration spaces with
millions of deployment options.
  In this paper, we propose Discovery Space, an abstraction that formalizes the
description of workload configuration problems, and exhibits a set of
characteristics required for structured, robust and distributed investigations
of large search spaces. We describe a concrete implementation of the Discovery
Space abstraction and show that it is generalizable across a diverse set of
workloads such as Large Language Model inference and Big Data Analytics.
  We demonstrate that our approach enables safe, transparent sharing of data
between executions of best-of-breed optimizers increasing the efficiency of
optimal configuration detection in large search spaces. We also demonstrate how
Discovery Spaces enable transfer and reuse of knowledge across similar search
spaces, enabling configuration search speed-ups of over 90%.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [29] [Adaptive Hybrid Sort: Dynamic Strategy Selection for Optimal Sorting Across Diverse Data Distributions](https://arxiv.org/abs/2506.20677)
*Shrinivass Arunachalam Balasubramanian*

Main category: cs.DS

TL;DR: 本文提出自适应混合排序范式，根据输入数据模式选最优排序算法，实验证明其优于传统静态排序算法，适用于多种数据处理场景。


<details>
  <summary>Details</summary>
Motivation: 因无排序算法在所有数据分布下最优，需新范式以提升排序性能。

Method: 通过特征提取模块计算数据参数，用有限状态机和XGBoost分类器决策，根据不同情况选择计数排序、基数排序或快速排序。

Result: 合成和真实数据集实验表明，该方案在执行时间、灵活性和效率上显著优于传统静态排序算法。

Conclusion: 该框架可扩展、高效，适用于大数据分析、边缘计算等多种数据处理操作。

Abstract: Sorting is an essential operation in computer science with direct
consequences on the performance of large scale data systems, real-time systems,
and embedded computation. However, no sorting algorithm is optimal under all
distributions of data. The new adaptive hybrid sorting paradigm proposed in
this paper is the paradigm that automatically selects the most effective
sorting algorithm Counting Sort, Radix Sort, or QuickSort based on real-time
monitoring of patterns in input data. The architecture begins by having a
feature extraction module to compute significant parameters such as data
volume, value range and entropy. These parameters are sent to a decision engine
involving Finite State Machine and XGBoost classifier to aid smart and
effective in choosing the optimal sorting strategy. It implements Counting Sort
on small key ranges, Radix Sort on large range structured input with
low-entropy keys and QuickSort on general purpose sorting. The experimental
findings of both synthetic and real life dataset confirm that the proposed
solution is actually inclined to excel significantly by comparison in execution
time, flexibility and the efficiency of conventional static sorting algorithms.
The proposed framework provides a scalable, high perhaps and applicable to a
wide range of data processing operations like big data analytics, edge
computing, and systems with hardware limitations.

</details>


### [30] [Practical and Accurate Local Edge Differentially Private Graph Algorithms](https://arxiv.org/abs/2506.20828)
*Pranay Mundra,Charalampos Papamanthou,Julian Shun,Quanquan C. Liu*

Main category: cs.DS

TL;DR: 论文提出用于图k - 核分解和三角形计数的新型LDP算法，理论上提升效用，分布式模拟实验显示有显著准确性提升。


<details>
  <summary>Details</summary>
Motivation: 大规模网络图分析涉及敏感数据，存在隐私问题，现有集中式模型假设可信第三方，而LDP可在无信任第三方时确保个体隐私。

Method: 引入基于输入依赖的私有图属性（图的退化度和最大度）的新型LDP算法，利用私有出度定向和新分析方法。

Result: 理论上误差界由最大度决定，更严格；实验中k - 核分解误差大幅降低，三角形计数乘法近似误差最多降低六个数量级，且运行时间有竞争力。

Conclusion: 提出的新型LDP算法在理论和实际实验中都优于现有方法。

Abstract: The rise of massive networks across diverse domains necessitates
sophisticated graph analytics, often involving sensitive data and raising
privacy concerns. This paper addresses these challenges using local
differential privacy (LDP), which enforces privacy at the individual level,
where no third-party entity is trusted, unlike centralized models that assume a
trusted curator. We introduce novel LDP algorithms for two fundamental graph
statistics: k-core decomposition and triangle counting. Our approach leverages
input-dependent private graph properties, specifically the degeneracy and
maximum degree of the graph, to improve theoretical utility. Unlike prior
methods, our error bounds are determined by the maximum degree rather than the
total number of edges, resulting in significantly tighter guarantees. For
triangle counting, we improve upon the work of Imola, Murakami, and
Chaudhury~\cite{IMC21locally, IMC21communication}, which bounds error in terms
of edge count. Instead, our algorithm achieves bounds based on graph degeneracy
by leveraging a private out-degree orientation, a refined variant of Eden et
al.'s randomized response technique~\cite{ELRS23, and a novel analysis,
yielding stronger guarantees than prior work. Beyond theoretical gains, we are
the first to evaluate local DP algorithms in a distributed simulation, unlike
prior work tested on a single processor. Experiments on real-world graphs show
substantial accuracy gains: our k-core decomposition achieves errors within 3x
of exact values, far outperforming the 131x error in the baseline of Dhulipala
et al.~\cite{DLRSSY22}. Our triangle counting algorithm reduces multiplicative
approximation errors by up to six orders of magnitude, while maintaining
competitive runtime.

</details>


### [31] [Review of Three Variants of the k-d Tree](https://arxiv.org/abs/2506.20687)
*Russell A. Brown*

Main category: cs.DS

TL;DR: 文章对比三种不同划分方式的k - d树变体性能，并对其中一种提出并分析双线程执行。


<details>
  <summary>Details</summary>
Motivation: 传统平衡技术不适用于k - d树，构建平衡k - d树时，划分数据的方法影响计算复杂度，因此要对比不同变体性能。

Method: 描述并对比三种不同划分方式的k - d树变体，对其中一种提出双线程执行并分析。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: The original description of the k-d tree recognized that rebalancing
techniques, such as used to build an AVL tree or a red-black tree, are not
applicable to a k-d tree. Hence, in order to build a balanced k-d tree, it is
necessary to find the median of a set of data for each recursive subdivision of
that set. The sort or selection used to find the median, and the technique used
to partition the set about that median, strongly influence the computational
complexity of building a k-d tree. This article describes and contrasts three
variants of the k-d tree that differ in their technique used to partition the
set, and compares the performance of those variants. In addition, dual-threaded
execution is proposed and analyzed for one of the three variants.

</details>


### [32] [A Framework for Building Data Structures from Communication Protocols](https://arxiv.org/abs/2506.20761)
*Alexandr Andoni,Shunhua Jiang,Omri Weinstein*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present a general framework for designing efficient data structures for
high-dimensional pattern-matching problems ($\exists \;? i\in[n], f(x_i,y)=1$)
through communication models in which $f(x,y)$ admits sublinear communication
protocols with exponentially-small error. Specifically, we reduce the data
structure problem to the Unambiguous Arthur-Merlin (UAM) communication
complexity of $f(x,y)$ under product distributions.
  We apply our framework to the Partial Match problem (a.k.a, matching with
wildcards), whose underlying communication problem is sparse set-disjointness.
When the database consists of $n$ points in dimension $d$, and the number of
$\star$'s in the query is at most $w = c\log n \;(\ll d)$, the fastest known
linear-space data structure (Cole, Gottlieb and Lewenstein, STOC'04) had query
time $t \approx 2^w = n^c$, which is nontrivial only when $c<1$. By contrast,
our framework produces a data structure with query time $n^{1-1/(c \log^2 c)}$
and space close to linear.
  To achieve this, we develop a one-sided $\epsilon$-error communication
protocol for Set-Disjointness under product distributions with
$\tilde{\Theta}(\sqrt{d\log(1/\epsilon)})$ complexity, improving on the
classical result of Babai, Frankl and Simon (FOCS'86). Building on this
protocol, we show that the Unambiguous AM communication complexity of
$w$-Sparse Set-Disjointness with $\epsilon$-error under product distributions
is $\tilde{O}(\sqrt{w \log(1/\epsilon)})$, independent of the ambient dimension
$d$, which is crucial for the Partial Match result. Our framework sheds further
light on the power of data-dependent data structures, which is instrumental for
reducing to the (much easier) case of product distributions.

</details>


### [33] [Almost Tight Additive Guarantees for \boldmath $k$-Edge-Connectivity](https://arxiv.org/abs/2506.20906)
*Nikhil Kumar,Chaitanya Swamy*

Main category: cs.DS

TL;DR: 本文研究k - 边连通生成子图问题，针对不同k值给出算法，结果接近最优，还拓展到相关问题及度约束版本。


<details>
  <summary>Details</summary>
Motivation: 解决k - 边连通生成子图（kECSS）问题，在该问题APX - 难的情况下寻求接近最优的解。

Method: 针对偶数k和奇数k分别设计多项式时间算法。

Result: 偶数k得到(k - 2) - 边连通子图，奇数k得到(k - 3) - 边连通子图，成本至多为LP*；还有其他替代保证；对k - 边连通生成多重图（kECSM）问题得到近似算法；拓展到度约束版本有相应结果。

Conclusion: 结果接近最优，显著改进前人工作，是度约束kECSS和kECSM问题首次成本至多为最优且连通性约束有常数违反的结果。

Abstract: We consider the \emph{$k$-edge connected spanning subgraph} (kECSS) problem,
where we are given an undirected graph $G = (V, E)$ with nonnegative edge costs
$\{c_e\}_{e\in E}$, and we seek a minimum-cost \emph{$k$-edge connected}
subgraph $H$ of $G$. For even $k$, we present a polytime algorithm that
computes a $(k-2)$-edge connected subgraph of cost at most the optimal value
$LP^*$ of the natural LP-relaxation for kECSS; for odd $k$, we obtain a
$(k-3)$-edge connected subgraph of cost at most $LP^*$. Since kECSS is APX-hard
for all $k\geq 2$, our results are nearly optimal. They also significantly
improve upon the recent work of Hershkowitz et al., both in terms of solution
quality and the simplicity of algorithm and its analysis. Our techniques also
yield an alternate guarantee, where we obtain a $(k-1)$-edge connected subgraph
of cost at most $1.5\cdot LP^*$; with unit edge costs, the cost guarantee
improves to $(1+\frac{4}{3k})\cdot LP^*$, which improves upon the
state-of-the-art approximation for unit edge costs, but with a unit loss in
edge connectivity.
  Our kECSS-result also yields results for the \emph{$k$-edge connected
spanning multigraph} (kECSM) problem, where multiple copies of an edge can be
selected: we obtain a $(1+2/k)$-approximation algorithm for even $k$, and a
$(1+3/k)$-approximation algorithm for odd $k$.
  Our techniques extend to the degree-bounded versions of kECSS and kECSM,
wherein we also impose degree lower- and upper- bounds on the nodes. We obtain
the same cost and connectivity guarantees for these degree-bounded versions
with an additive violation of (roughly) $2$ for the degree bounds. These are
the first results for degree-bounded \{kECSS,kECSM\} of the form where the cost
of the solution obtained is at most the optimum, and the connectivity
constraints are violated by an additive constant.

</details>


### [34] [Courcelle's Theorem for Lipschitz Continuity](https://arxiv.org/abs/2506.21118)
*Tatsuya Gima,Soh Kumabe,Yuichi Yoshida*

Main category: cs.DS

TL;DR: 本文提出Lipschitz连续算法领域的首个算法元定理，为有界树宽图和有界团宽图上特定问题提供近似算法，结果在近似性和Lipschitz连续性上表现佳，还构造了Lipschitz连续版Baker分解。


<details>
  <summary>Details</summary>
Motivation: 现有Lipschitz连续算法针对特定组合优化问题设计，缺乏通用性，需要统一方法。

Method: 提出类似Courcelle定理的Lipschitz连续算法元定理，研究有界树宽图上受MSO_2约束的顶点集问题和有界团宽图上受MSO_1约束的问题。

Result: 对于有界树宽图上问题，存在具有多对数Lipschitz常数的(1±ε)近似算法；在有界团宽图上有类似结果；构造了Lipschitz连续版Baker分解。

Conclusion: 该元定理能为有界树宽图和有界团宽图上问题提供有效Lipschitz连续近似算法，优于多数现有算法。

Abstract: Lipschitz continuity of algorithms, introduced by Kumabe and Yoshida
(FOCS'23), measures the stability of an algorithm against small input
perturbations. Algorithms with small Lipschitz continuity are desirable, as
they ensure reliable decision-making and reproducible scientific research.
Several studies have proposed Lipschitz continuous algorithms for various
combinatorial optimization problems, but these algorithms are problem-specific,
requiring a separate design for each problem.
  To address this issue, we provide the first algorithmic meta-theorem in the
field of Lipschitz continuous algorithms. Our result can be seen as a Lipschitz
continuous analogue of Courcelle's theorem, which offers Lipschitz continuous
algorithms for problems on bounded-treewidth graphs. Specifically, we consider
the problem of finding a vertex set in a graph that maximizes or minimizes the
total weight, subject to constraints expressed in monadic second-order logic
(MSO_2). We show that for any $\varepsilon>0$, there exists a $(1\pm
\varepsilon)$-approximation algorithm for the problem with a polylogarithmic
Lipschitz constant on bounded treewidth graphs. On such graphs, our result
outperforms most existing Lipschitz continuous algorithms in terms of
approximability and/or Lipschitz continuity. Further, we provide similar
results for problems on bounded-clique-width graphs subject to constraints
expressed in MSO_1. Additionally, we construct a Lipschitz continuous version
of Baker's decomposition using our meta-theorem as a subroutine.

</details>


### [35] [On Minimizing Wiggle in Stacked Area Charts](https://arxiv.org/abs/2506.21175)
*Alexander Dobler,Martin Nöllenburg*

Main category: cs.DS

TL;DR: 本文分析堆叠面积图优化可读性的摆动最小化问题的复杂度，提出精确混合整数线性规划公式并与启发式算法对比，还研究了摆动最小化的特殊情况。


<details>
  <summary>Details</summary>
Motivation: 许多启发式算法用于最小化堆叠面积图的摆动，但摆动最小化的计算复杂度未被正式分析。

Method: 证明摆动最小化不同变体是NP难且难以近似，提出精确混合整数线性规划公式并实验评估，研究摆动最小化的特殊情况。

Result: 得出摆动最小化不同变体的复杂度结果，对比了精确公式和启发式算法性能，给出特殊情况的复杂度结果。

Conclusion: 摆动最小化问题具有较高复杂度，特殊情况的复杂度结果可推导摆动最小化的一些困难结果。

Abstract: Stacked area charts are a widely used visualization technique for numerical
time series. The x-axis represents time, and the time series are displayed as
horizontal, variable-height layers stacked on top of each other. The height of
each layer corresponds to the time series values at each time point. The main
aesthetic criterion for optimizing the readability of stacked area charts is
the amount of vertical change of the borders between the time series in the
visualization, called wiggle. While many heuristic algorithms have been
developed to minimize wiggle, the computational complexity of minimizing wiggle
has not been formally analyzed. In this paper, we show that different variants
of wiggle minimization are NP-hard and even hard to approximate. We also
present an exact mixed-integer linear programming formulation and compare its
performance with a state-of-the-art heuristic in an experimental evaluation.
Lastly, we consider a special case of wiggle minimization that corresponds to
the fundamentally interesting and natural problem of ordering a set of numbers
as to minimize their sum of absolute prefix sums. We show several complexity
results for this problem that imply some of the mentioned hardness results for
wiggle minimization.

</details>


### [36] [Edge Clique Partition and Cover Beyond Independence](https://arxiv.org/abs/2506.21216)
*Fedor V. Fomin,Petr A. Golovach,Danil Sagunov,Kirill Simonov*

Main category: cs.DS

TL;DR: 研究图的边团覆盖和划分问题的基于最大独立集的参数化复杂性，揭示两种变体的不同复杂度情况。


<details>
  <summary>Details</summary>
Motivation: 经典图边团覆盖和划分问题以团总数为参数的参数化对稀疏图无意义，实际中边覆盖或划分的最小团数接近最大独立集大小，因此研究基于最大独立集的参数化。

Method: 引入并研究Edge Clique Cover Above Independent Set (ECC/α) 和 Edge Clique Partition Above Independent Set (ECP/α) 问题，分析其复杂度。

Result: ECP/α是固定参数可解的；ECC/α对所有k ≥ 2是NP完全的，但k ∈ {0,1}时可多项式时间求解；ECC/α以k + ω(G)为参数时是固定参数可解的；对H - 子式自由图设计了亚指数算法。

Conclusion: 从基于自然下界的参数化视角，边团覆盖和划分两个问题存在有趣差异。

Abstract: Covering and partitioning the edges of a graph into cliques are classical
problems at the intersection of combinatorial optimization and graph theory,
having been studied through a range of algorithmic and complexity-theoretic
lenses. Despite the well-known fixed-parameter tractability of these problems
when parameterized by the total number of cliques, such a parameterization
often fails to be meaningful for sparse graphs. In many real-world instances,
on the other hand, the minimum number of cliques in an edge cover or partition
can be very close to the size of a maximum independent set \alpha(G).
  Motivated by this observation, we investigate above \alpha parameterizations
of the edge clique cover and partition problems. Concretely, we introduce and
study Edge Clique Cover Above Independent Set (ECC/\alpha) and Edge Clique
Partition Above Independent Set (ECP/\alpha), where the goal is to cover or
partition all edges of a graph using at most \alpha(G) + k cliques, and k is
the parameter. Our main results reveal a distinct complexity landscape for the
two variants. We show that ECP/\alpha is fixed-parameter tractable, whereas
ECC/\alpha is NP-complete for all k \geq 2, yet can be solved in polynomial
time for k \in {0,1}. These findings highlight intriguing differences between
the two problems when viewed through the lens of parameterization above a
natural lower bound.
  Finally, we demonstrate that ECC/\alpha becomes fixed-parameter tractable
when parameterized by k + \omega(G), where \omega(G) is the size of a maximum
clique of the graph G. This result is particularly relevant for sparse graphs,
in which \omega is typically small. For H-minor free graphs, we design a
subexponential algorithm of running time f(H)^{\sqrt{k}}n^{O(1)}.

</details>


### [37] [Vantage Point Selection Algorithms for Bottleneck Capacity Estimation](https://arxiv.org/abs/2506.21418)
*Vikrant Ashvinkumar,Rezaul Chowdhury,Jie Gao,Mayank Goswami,Joseph S. B. Mitchell,Valentin Polishchuk*

Main category: cs.DS

TL;DR: 本文研究互联网中瓶颈容量估计的有利点选择问题，给出非自适应和自适应设置下的算法和边界。


<details>
  <summary>Details</summary>
Motivation: 解决互联网中瓶颈容量估计问题，提出有利点选择问题。

Method: 非自适应设置考虑随机排列的松弛模型，给出 1 - 1/e 近似算法；自适应设置考虑最严格模型，与实例最优解比较并给出边界。

Result: 非自适应设置得到 1 - 1/e 近似算法，自适应设置给出实例最优近似算法的上下界。

Conclusion: 提出的算法和边界为解决有利点选择问题提供有效方法。

Abstract: Motivated by the problem of estimating bottleneck capacities on the Internet,
we formulate and study the problem of vantage point selection. We are given a
graph $G=(V, E)$ whose edges $E$ have unknown capacity values that are to be
discovered. Probes from a vantage point, i.e, a vertex $v \in V$, along
shortest paths from $v$ to all other vertices, reveal bottleneck edge
capacities along each path. Our goal is to select $k$ vantage points from $V$
that reveal the maximum number of bottleneck edge capacities.
  We consider both a non-adaptive setting where all $k$ vantage points are
selected before any bottleneck capacity is revealed, and an adaptive setting
where each vantage point selection instantly reveals bottleneck capacities
along all shortest paths starting from that point. In the non-adaptive setting,
by considering a relaxed model where edge capacities are drawn from a random
permutation (which still leaves the problem of maximizing the expected number
of revealed edges NP-hard), we are able to give a $1-1/e$ approximate
algorithm. In the adaptive setting we work with the least permissive model
where edge capacities are arbitrarily fixed but unknown. We compare with the
best solution for the particular input instance (i.e. by enumerating all
choices of $k$ tuples), and provide both lower bounds on instance optimal
approximation algorithms and upper bounds for trees and planar graphs.

</details>


### [38] [Succinct Preferential Attachment Graphs](https://arxiv.org/abs/2506.21436)
*Ziad Ismaili Alaoui,Namrata,Sebastian Wild*

Main category: cs.DS

TL;DR: 本文设计了一种空间使用随图的可压缩性自动改善的数据结构，能高效支持导航操作，还分析了实例最优空间使用。


<details>
  <summary>Details</summary>
Motivation: 现有图的压缩数据计算支持不完善，简洁数据结构结果局限于特定图类且空间使用情况不佳，需改进。

Method: 设计一种数据结构，使其空间使用随图的可压缩性自动改善，支持导航操作，并对实例最优空间使用进行分析。

Result: 当图根据经典Barabási - Albert模型生成时，空间使用接近实例最优空间；对任意图，保证大小渐近不大于熵压缩边列表。

Conclusion: 设计的数据结构能根据图的可压缩性自动优化空间使用，在支持导航操作方面表现良好，关键技术在于对实例最优空间使用的分析。

Abstract: Computing over compressed data combines the space saving of data compression
with efficient support for queries directly on the compressed representation.
Such data structures are widely applied in text indexing and have been
successfully generalised to trees. For graphs, support for computing over
compressed data remains patchy; typical results in the area of succinct data
structures are restricted to a specific class of graphs and use the same,
worst-case amount of space for any graph from this class.
  In this work, we design a data structure whose space usage automatically
improves with the compressibility of the graph at hand, while efficiently
supporting navigational operations (simulating adjacency-list access).
Specifically, we show that the space usage approaches the instance-optimal
space when the graph is drawn according to the classic Barab\'asi-Albert model
of preferential-attachment graphs. Our data-structure techniques also work for
arbitrary graphs, guaranteeing a size asymptotically no larger than an
entropy-compressed edge list. A key technical contribution is the careful
analysis of the instance-optimal space usage.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [39] [Smoothness Meets Autobidding: Tight Price of Anarchy Bounds for Simultaneous First-Price Auctions](https://arxiv.org/abs/2506.20908)
*Riccardo Colini-Baldeschi,Sophie Klumper,Twan Kroll,Stefano Leonardi,Guido Schäfer,Artem Tsikiridis*

Main category: cs.GT

TL;DR: 本文将Syrgkanis和Tardos的平滑框架扩展到自动出价，提出平衡异质代理类型平滑参数的技术，具有简单、可扩展和通用的优点，得到多种模型的POA界限。


<details>
  <summary>Details</summary>
Motivation: 在线广告系统转向自动出价，现有文献研究核心拍卖格式的无政府价格（POA），但需要高级证明来推导严格的POA界限，本文旨在提供更有效的方法。

Method: 将平滑框架扩展到自动出价，提出平衡异质代理类型平滑参数的技术，通过求解POA揭示数学规划来找到最佳POA界限。

Result: 证明了单项一级价格拍卖（FPA）的平滑性，得到混合自动出价的严格POA为2.18；扩展定理适用于多种复杂情况，建立了多个模型的（大多）严格POA界限；框架能界定粗相关均衡（CCE）的POA。

Conclusion: 所提出的方法简单、可扩展且通用，能有效解决自动出价中多种模型的POA界限问题。

Abstract: Online advertising systems have recently transitioned to autobidding,
enabling advertisers to delegate bidding decisions to automated agents. Each
advertiser directs their agent to optimize a valuation-dependent objective
subject to return-on-investment (ROI) or budget constraints. Given their
relevance, there has been a surge in literature studying the liquid welfare
price of anarchy (POA) of core auction formats in autobidding, among which
simultaneous first-price auctions (FPA). These models capture a large range of
heterogeneous agent behaviors, requiring advanced proofs to derive tight POA
bounds. Recently, Deng et al. (NeurIPS 2024) showed that the POA of FPA for
mixed autobidders (i.e., value and utility maximizers) under ROI is 2.18 for
additive valuations.
  We extend the smoothness framework of Syrgkanis and Tardos (STOC 2013) to
autobidding. A key contribution is a technique to balance smoothness parameters
across heterogeneous agent types. Finding the best POA bound reduces to solving
a POA-revealing mathematical program. Our approach has three strengths: (1)
Simplicity: We prove smoothness for single-item FPA. Results for simultaneous
FPA follow via our theorem. For example, by showing smoothness for value and
utility maximizers, we obtain the tight POA of 2.18 for mixed autobidding. (2)
Extendibility: Our Extension Theorem adapts to simultaneous FPA with reserve
prices and agents with fractionally subadditive valuations and heterogeneous
payment sensitivities and target ROI parameters. We establish the first
(mostly) tight POA bounds for several models beyond the autobidding state of
the art. (3) Generality: Our framework bounds the POA of coarse correlated
equilibria (CCE), which arise when hybrid agents employ regret-minimizing
algorithms. Building on Kolumbus and Nisan (WWW 2022), we show that CCE from
such agents have properties that keep their POA low.

</details>


### [40] [From multi-allocations to allocations, with subadditive valuations](https://arxiv.org/abs/2506.21493)
*Uriel Feige*

Main category: cs.GT

TL;DR: 研究不可分割物品在单调次可加估值下的公平分配，证明d - 多分配可转化为普通分配，给出相关MMS分配存在性结果。


<details>
  <summary>Details</summary>
Motivation: 解决不可分割物品在单调次可加估值下的公平分配问题。

Method: 证明d - 多分配能转化为普通分配，且价值损失不超d倍。

Result: 若存在ρ - MMS d - 多分配，则存在ρ/(4d) - MMS分配；结合其他结果，得出Ω(1/(log log n)) - MMS分配存在。

Conclusion: 对于不可分割物品在单调次可加估值下的公平分配问题，得到了相关MMS分配的存在性结论。

Abstract: We consider the problem of fair allocation of $m$ indivisible items to $n$
agents with monotone subadditive valuations. For integer $d \ge 2$, a
$d$-multi-allocation is an allocation in which each item is allocated to at
most $d$ different agents. We show that $d$-multi-allocations can be
transformed into allocations, while not losing much more than a factor of $d$
in the value that each agent receives. One consequence of this result is that
for allocation instances with equal entitlements and subadditive valuations, if
$\rho$-MMS $d$-multi-allocations exist, then so do $\frac{\rho}{4d}$-MMS
allocations. Combined with recent results of Seddighin and Seddighin [EC 2025],
this implies the existence of $\Omega(\frac{1}{\log\log n})$-MMS allocations.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [41] [RAG-VisualRec: An Open Resource for Vision- and Text-Enhanced Retrieval-Augmented Generation in Recommendation](https://arxiv.org/abs/2506.20817)
*Ali Tourani,Fatemeh Nazary,Yashar Deldjoo*

Main category: cs.IR

TL;DR: 论文提出结合LLM生成剧情描述与预告片视觉嵌入的资源用于电影多模态推荐系统，实验表明CCA融合提升召回率，LLM重排提升NDCG。


<details>
  <summary>Details</summary>
Motivation: 解决电影领域多模态推荐系统因元数据有限难以生成可靠推荐的问题。

Method: 引入结合LLM生成剧情描述与预告片视觉嵌入的资源，有数据增强步骤将稀疏元数据转为丰富文本信号，采用PCA、CCA等融合策略集成视觉线索。

Result: CCA-based融合显著提升召回率，LLM驱动的重排步骤提升NDCG，尤其在文本数据有限的场景。

Conclusion: 发布框架以推动多模态推荐技术在冷启动、注重新颖性和特定领域场景的探索。

Abstract: This paper addresses the challenge of developing multimodal recommender
systems for the movie domain, where limited metadata (e.g., title, genre) often
hinders the generation of robust recommendations. We introduce a resource that
combines LLM-generated plot descriptions with trailer-derived visual embeddings
in a unified pipeline supporting both Retrieval-Augmented Generation (RAG) and
collaborative filtering. Central to our approach is a data augmentation step
that transforms sparse metadata into richer textual signals, alongside fusion
strategies (e.g., PCA, CCA) that integrate visual cues. Experimental
evaluations demonstrate that CCA-based fusion significantly boosts recall
compared to unimodal baselines, while an LLM-driven re-ranking step further
improves NDCG, particularly in scenarios with limited textual data. By
releasing this framework, we invite further exploration of multi-modal
recommendation techniques tailored to cold-start, novelty-focused, and
domain-specific settings. All code, data, and detailed documentation are
publicly available at: https://github.com/RecSys-lab/RAG-VisualRec

</details>


### [42] [The Next Phase of Scientific Fact-Checking: Advanced Evidence Retrieval from Complex Structured Academic Papers](https://arxiv.org/abs/2506.20844)
*Xingyu Deng,Xi Wang,Mark Stevenson*

Main category: cs.IR

TL;DR: 本文探讨当前科学事实核查系统局限，识别证据检索关键挑战并开展初步实验，旨在用专业IR系统推动科学事实核查发展。


<details>
  <summary>Details</summary>
Motivation: 现有科学事实核查方法基于小规模摘要数据集，未处理完整文档的独特挑战，需改进系统性能。

Method: 分析当前系统局限性，识别证据检索的关键研究挑战，进行初步实验。

Result: 确定证据检索的关键挑战，包括解决语义局限和主题失衡、处理过时信息、利用长距离上下文、处理复杂科学表达、评估文献可信度等。

Conclusion: 有望通过专门的IR系统推进科学事实核查在实际应用中的发展。

Abstract: Scientific fact-checking aims to determine the veracity of scientific claims
by retrieving and analysing evidence from research literature. The problem is
inherently more complex than general fact-checking since it must accommodate
the evolving nature of scientific knowledge, the structural complexity of
academic literature and the challenges posed by long-form, multimodal
scientific expression. However, existing approaches focus on simplified
versions of the problem based on small-scale datasets consisting of abstracts
rather than full papers, thereby avoiding the distinct challenges associated
with processing complete documents. This paper examines the limitations of
current scientific fact-checking systems and reveals the many potential
features and resources that could be exploited to advance their performance. It
identifies key research challenges within evidence retrieval, including (1)
evidence-driven retrieval that addresses semantic limitations and topic
imbalance (2) time-aware evidence retrieval with citation tracking to mitigate
outdated information, (3) structured document parsing to leverage long-range
context, (4) handling complex scientific expressions, including tables,
figures, and domain-specific terminology and (5) assessing the credibility of
scientific literature. Preliminary experiments were conducted to substantiate
these challenges and identify potential solutions. This perspective paper aims
to advance scientific fact-checking with a specialised IR system tailored for
real-world applications.

</details>


### [43] [Towards Two-Stage Counterfactual Learning to Rank](https://arxiv.org/abs/2506.20854)
*Shashank Gupta,Yiming Liao,Maarten de Rijke*

Main category: cs.IR

TL;DR: 本文提出用于两阶段排序的反事实学习排序（CLTR）估计器和联合优化方法，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有CLTR方法在处理大量候选文档时不实用，且缺乏同时训练候选生成器和排序器的方法。

Method: 提出考虑两阶段交互的两阶段CLTR估计器，及分别训练候选和排序策略的联合优化方法。

Result: 在半合成基准测试中，所提联合CLTR方法比基线方法更有效。

Conclusion: 所提CLTR估计器和学习方法可用于两阶段排序。

Abstract: Counterfactual learning to rank (CLTR) aims to learn a ranking policy from
user interactions while correcting for the inherent biases in interaction data,
such as position bias. Existing CLTR methods assume a single ranking policy
that selects top-K ranking from the entire document candidate set. In
real-world applications, the candidate document set is on the order of
millions, making a single-stage ranking policy impractical. In order to scale
to millions of documents, real-world ranking systems are designed in a
two-stage fashion, with a candidate generator followed by a ranker. The
existing CLTR method for a two-stage offline ranking system only considers the
top-1 ranking set-up and only focuses on training the candidate generator, with
the ranker fixed. A CLTR method for training both the ranker and candidate
generator jointly is missing from the existing literature. In this paper, we
propose a two-stage CLTR estimator that considers the interaction between the
two stages and estimates the joint value of the two policies offline. In
addition, we propose a novel joint optimization method to train the candidate
and ranker policies, respectively. To the best of our knowledge, we are the
first to propose a CLTR estimator and learning method for two-stage ranking.
Experimental results on a semi-synthetic benchmark demonstrate the
effectiveness of the proposed joint CLTR method over baselines.

</details>


### [44] [EraRAG: Efficient and Incremental Retrieval Augmented Generation for Growing Corpora](https://arxiv.org/abs/2506.20963)
*Fangyuan Zhang,Zhengjun Huang,Yingli Zhou,Qintian Guo,Zhixun Li,Wensheng Luo,Di Jiang,Yixiang Fang,Xiaofang Zhou*

Main category: cs.IR

TL;DR: 提出EraRAG框架解决现有Graph - RAG在动态环境可扩展性问题，实验显示其有更好性能。


<details>
  <summary>Details</summary>
Motivation: 现有Graph - RAG方法假设语料库静态，新文档到来需昂贵的全图重建，限制其在动态环境的可扩展性。

Method: 引入EraRAG框架，利用基于超平面的局部敏感哈希（LSH）将原始语料库划分为分层图结构，实现新数据的高效局部插入。

Result: 在大规模基准测试中，EraRAG相比现有Graph - RAG系统，更新时间和令牌消耗最多降低一个数量级，且精度性能更优。

Conclusion: 为在不断增长语料库上运行的RAG系统提供了实用途径，弥合了检索效率和适应性之间的差距。

Abstract: Graph-based Retrieval-Augmented Generation (Graph-RAG) enhances large
language models (LLMs) by structuring retrieval over an external corpus.
However, existing approaches typically assume a static corpus, requiring
expensive full-graph reconstruction whenever new documents arrive, limiting
their scalability in dynamic, evolving environments. To address these
limitations, we introduce EraRAG, a novel multi-layered Graph-RAG framework
that supports efficient and scalable dynamic updates. Our method leverages
hyperplane-based Locality-Sensitive Hashing (LSH) to partition and organize the
original corpus into hierarchical graph structures, enabling efficient and
localized insertions of new data without disrupting the existing topology. The
design eliminates the need for retraining or costly recomputation while
preserving high retrieval accuracy and low latency. Experiments on large-scale
benchmarks demonstrate that EraRag achieves up to an order of magnitude
reduction in update time and token consumption compared to existing Graph-RAG
systems, while providing superior accuracy performance. This work offers a
practical path forward for RAG systems that must operate over continually
growing corpora, bridging the gap between retrieval efficiency and
adaptability. Our code and data are available at
https://github.com/EverM0re/EraRAG-Official.

</details>


### [45] [Response Quality Assessment for Retrieval-Augmented Generation via Conditional Conformal Factuality](https://arxiv.org/abs/2506.20978)
*Naihe Feng,Yi Sui,Shiyi Hou,Jesse C. Cresswell,Ga Wu*

Main category: cs.IR

TL;DR: 提出Conformal - RAG框架，解决现有RAG研究对生成响应中子声明质量关注不足及评估方法缺陷，实验表明其优势。


<details>
  <summary>Details</summary>
Motivation: 现有RAG研究多关注整体问答准确率，忽视子声明质量，且改进RAG可信度的方法有局限性。

Method: 提出Conformal - RAG框架，利用CP和RAG机制内部信息，无需手动标记共形集。

Result: Conformal - RAG能为细化子声明质量提供统计保证，保留更多高质量子声明。

Conclusion: Conformal - RAG在不依赖真实答案的情况下确保响应可靠性，适用于复杂RAG应用。

Abstract: Existing research on Retrieval-Augmented Generation (RAG) primarily focuses
on improving overall question-answering accuracy, often overlooking the quality
of sub-claims within generated responses. Recent methods that attempt to
improve RAG trustworthiness, such as through auto-evaluation metrics, lack
probabilistic guarantees or require ground truth answers. To address these
limitations, we propose Conformal-RAG, a novel framework inspired by recent
applications of conformal prediction (CP) on large language models (LLMs).
Conformal-RAG leverages CP and internal information from the RAG mechanism to
offer statistical guarantees on response quality. It ensures group-conditional
coverage spanning multiple sub-domains without requiring manual labelling of
conformal sets, making it suitable for complex RAG applications. Compared to
existing RAG auto-evaluation methods, Conformal-RAG offers statistical
guarantees on the quality of refined sub-claims, ensuring response reliability
without the need for ground truth answers. Additionally, our experiments
demonstrate that by leveraging information from the RAG system, Conformal-RAG
retains up to 60\% more high-quality sub-claims from the response compared to
direct applications of CP to LLMs, while maintaining the same reliability
guarantee.

</details>


### [46] [RecCoT: Enhancing Recommendation via Chain-of-Thought](https://arxiv.org/abs/2506.21032)
*Shuo Yang,Jiangxia Cao,Haipeng Li,Yuqi Mao,Shuchao Pang*

Main category: cs.IR

TL;DR: 现有推荐系统在二元学习范式下难理解用户偏好原因，部分利用评论内容的方法又缺乏可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决现代推荐系统难以理解用户喜欢或不喜欢某些物品的原因，以及现有利用评论内容方法缺乏可解释性的问题。

Method: 文中未明确提及具体方法

Result: 未提及相关结果

Conclusion: 现有推荐系统在二元学习范式下存在理解用户偏好原因的困难，部分改进方法缺乏可解释性。

Abstract: In real-world applications, users always interact with items in multiple
aspects, such as through implicit binary feedback (e.g., clicks, dislikes, long
views) and explicit feedback (e.g., comments, reviews). Modern recommendation
systems (RecSys) learn user-item collaborative signals from these implicit
feedback signals as a large-scale binary data-streaming, subsequently
recommending other highly similar items based on users' personalized historical
interactions. However, from this collaborative-connection perspective, the
RecSys does not focus on the actual content of the items themselves but instead
prioritizes higher-probability signals of behavioral co-occurrence among items.
Consequently, under this binary learning paradigm, the RecSys struggles to
understand why a user likes or dislikes certain items. To alleviate it, some
works attempt to utilize the content-based reviews to capture the semantic
knowledge to enhance recommender models. However, most of these methods focus
on predicting the ratings of reviews, but do not provide a human-understandable
explanation.

</details>


### [47] [Real-time and personalized product recommendations for large e-commerce platforms](https://arxiv.org/abs/2506.21368)
*Matteo Tolloso,Davide Bacciu,Shahab Mokarizadeh,Marco Varesi*

Main category: cs.IR

TL;DR: 提出一种为大型电商平台提供实时个性化商品推荐的方法，在实际数据集验证有效。


<details>
  <summary>Details</summary>
Motivation: 为大型电商平台（尤其是时尚零售）提供实时、个性化的商品推荐，确保用户满意度。

Method: 利用图神经网络和简约学习方法。

Result: 在大型电商平台数据集上实验表明，该方法能有效预测购买序列和处理多交互场景，实现高效个性化推荐。

Conclusion: 所提出的方法能在现实约束下实现准确、可扩展且响应时间短的个性化推荐。

Abstract: We present a methodology to provide real-time and personalized product
recommendations for large e-commerce platforms, specifically focusing on
fashion retail. Our approach aims to achieve accurate and scalable
recommendations with minimal response times, ensuring user satisfaction,
leveraging Graph Neural Networks and parsimonious learning methodologies.
Extensive experimentation with datasets from one of the largest e-commerce
platforms demonstrates the effectiveness of our approach in forecasting
purchase sequences and handling multi-interaction scenarios, achieving
efficient personalized recommendations under real-world constraints.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [48] [Progressive Size-Adaptive Federated Learning: A Comprehensive Framework for Heterogeneous Multi-Modal Data Systems](https://arxiv.org/abs/2506.20685)
*Sajid Hussain,Muhammad Sohail,Nauman Ali Khan,Naima Iltaf,Ihtesham ul Islam*

Main category: cs.LG

TL;DR: 本文提出基于数据集大小特征的自适应联邦学习框架SAFL，通过多模态数据集实验得到关键见解，SAFL准确率高、通信效率优，为联邦学习策略提供理论和实践指导。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法多关注模型异质性和聚合技术，忽略了数据集大小特征对联邦训练动态的基本影响。

Method: 引入Size-Based Adaptive Federated Learning (SAFL) 框架，基于数据集大小特征对异构多模态数据进行联邦学习的渐进式训练。

Result: 1. 发现联邦学习有效数据集大小的最优范围是1000 - 1500样本；2. 结构化数据性能显著优于非结构化数据；3. 超过2000样本的大数据集性能会系统下降。SAFL平均准确率达87.68%，结构化数据准确率超99%，通信效率高，实时监测框架能洞察系统资源利用等情况。

Conclusion: 该工作填补了理解数据特征如何驱动联邦学习策略的关键空白，为神经网络和学习系统的实际FL部署提供理论和实践指导。

Abstract: Federated Learning (FL) has emerged as a transformative paradigm for
distributed machine learning while preserving data privacy. However, existing
approaches predominantly focus on model heterogeneity and aggregation
techniques, largely overlooking the fundamental impact of dataset size
characteristics on federated training dynamics. This paper introduces
Size-Based Adaptive Federated Learning (SAFL), a novel progressive training
framework that systematically organizes federated learning based on dataset
size characteristics across heterogeneous multi-modal data. Our comprehensive
experimental evaluation across 13 diverse datasets spanning 7 modalities
(vision, text, time series, audio, sensor, medical vision, and multimodal)
reveals critical insights: 1) an optimal dataset size range of 1000-1500
samples for federated learning effectiveness; 2) a clear modality performance
hierarchy with structured data (time series, sensor) significantly
outperforming unstructured data (text, multimodal); and 3) systematic
performance degradation for large datasets exceeding 2000 samples. SAFL
achieves an average accuracy of 87.68% across all datasets, with structured
data modalities reaching 99%+ accuracy. The framework demonstrates superior
communication efficiency, reducing total data transfer to 7.38 GB across 558
communications while maintaining high performance. Our real-time monitoring
framework provides unprecedented insights into system resource utilization,
network efficiency, and training dynamics. This work fills critical gaps in
understanding how data characteristics should drive federated learning
strategies, providing both theoretical insights and practical guidance for
real-world FL deployments in neural network and learning systems.

</details>


### [49] [E-ABIN: an Explainable module for Anomaly detection in BIological Networks](https://arxiv.org/abs/2506.20693)
*Ugo Lomoio,Tommaso Mazza,Pierangelo Veltri,Pietro Hiram Guzzi*

Main category: cs.LG

TL;DR: 文章介绍通用可解释生物网络异常检测框架E - ABIN，结合多种技术，通过案例证明其能有效发现生物相关异常并洞察疾病机制。


<details>
  <summary>Details</summary>
Motivation: 大规模组学数据需强大分析框架处理复杂基因表达数据集，当前基因异常检测方法有局限，如限于单数据集且缺乏易用图形界面。

Method: E - ABIN结合经典机器学习和基于图的深度学习技术，集成支持向量机、随机森林、图自编码器和图对抗属性网络等算法。

Result: 通过膀胱癌和乳糜泻案例研究，E - ABIN有效发现生物学相关异常。

Conclusion: E - ABIN能检测和解释基因表达或甲基化衍生网络中的异常，保证高预测准确性和可解释性，为疾病机制提供见解。

Abstract: The increasing availability of large-scale omics data calls for robust
analytical frameworks capable of handling complex gene expression datasets
while offering interpretable results. Recent advances in artificial
intelligence have enabled the identification of aberrant molecular patterns
distinguishing disease states from healthy controls. Coupled with improvements
in model interpretability, these tools now support the identification of genes
potentially driving disease phenotypes. However, current approaches to gene
anomaly detection often remain limited to single datasets and lack accessible
graphical interfaces. Here, we introduce E-ABIN, a general-purpose, explainable
framework for Anomaly detection in Biological Networks. E-ABIN combines
classical machine learning and graph-based deep learning techniques within a
unified, user-friendly platform, enabling the detection and interpretation of
anomalies from gene expression or methylation-derived networks. By integrating
algorithms such as Support Vector Machines, Random Forests, Graph Autoencoders
(GAEs), and Graph Adversarial Attributed Networks (GAANs), E-ABIN ensures a
high predictive accuracy while maintaining interpretability. We demonstrate the
utility of E-ABIN through case studies of bladder cancer and coeliac disease,
where it effectively uncovers biologically relevant anomalies and offers
insights into disease mechanisms.

</details>


### [50] [On Context-Content Uncertainty Principle](https://arxiv.org/abs/2506.20699)
*Xin Li*

Main category: cs.LG

TL;DR: 本文基于上下文 - 内容不确定性原理（CCUP）开发分层计算框架，提出四层操作原则，证明形式等价定理，经模拟验证效率提升，为理解脑与机器减少不确定性提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 基于CCUP的熵不对称性，开发计算框架以理解脑和机器如何在不确定性下进行推理。

Method: 构建分层计算框架，确定四层操作原则，证明形式等价定理和依赖关系，进行计算模拟。

Result: 提出的CCUP对齐推理在计算模拟中显示出效率提升。

Conclusion: 为脑和机器通过递归结构 - 特异性对齐减少不确定性提供统一理论基础，脑是循环一致的熵梯度解析器。

Abstract: The Context-Content Uncertainty Principle (CCUP) proposes that inference
under uncertainty is governed by an entropy asymmetry between context and
content: high-entropy contexts must be interpreted through alignment with
low-entropy, structured content. In this paper, we develop a layered
computational framework that derives operational principles from this
foundational asymmetry. At the base level, CCUP formalizes inference as
directional entropy minimization, establishing a variational gradient that
favors content-first structuring. Building upon this, we identify four
hierarchical layers of operational principles: (\textbf{L1}) \emph{Core
Inference Constraints}, including structure-before-specificity, asymmetric
inference flow, cycle-consistent bootstrapping, and conditional compression,
all shown to be mutually reducible; (\textbf{L2}) \emph{Resource Allocation
Principles}, such as precision-weighted attention, asymmetric learning rates,
and attractor-based memory encoding; (\textbf{L3}) \emph{Temporal Bootstrapping
Dynamics}, which organize learning over time via structure-guided curricula;
and (\textbf{L4}) \emph{Spatial Hierarchical Composition}, which integrates
these mechanisms into self-organizing cycles of memory, inference, and
planning. We present formal equivalence theorems, a dependency lattice among
principles, and computational simulations demonstrating the efficiency gains of
CCUP-aligned inference. This work provides a unified theoretical foundation for
understanding how brains and machines minimize uncertainty through recursive
structure-specificity alignment. The brain is not just an inference machine. It
is a cycle-consistent entropy gradient resolver, aligning structure and
specificity via path-dependent, content-seeded simulation.

</details>


### [51] [A Survey of AI for Materials Science: Foundation Models, LLM Agents, Datasets, and Tools](https://arxiv.org/abs/2506.20743)
*Minh-Hao Van,Prateek Verma,Chen Zhao,Xintao Wu*

Main category: cs.LG

TL;DR: 本文全面概述支持材料科学基础模型发展的相关要素，介绍应用分类，讨论进展、工具等，评估成果与局限并指明未来方向。


<details>
  <summary>Details</summary>
Motivation: 基础模型在材料科学领域带来变革，能实现可扩展、通用和多模态的科学发现，本文旨在全面介绍其相关情况。

Method: 引入任务驱动的分类法，涵盖六个应用领域；讨论单模态和多模态基础模型及大语言模型智能体进展；回顾数据集、工具和实验平台。

Result: 评估了基础模型的早期成功，也识别出泛化性、可解释性等方面的持续局限。

Conclusion: 未来研究方向集中在可扩展预训练、持续学习、数据治理和可信性等方面。

Abstract: Foundation models (FMs) are catalyzing a transformative shift in materials
science (MatSci) by enabling scalable, general-purpose, and multimodal AI
systems for scientific discovery. Unlike traditional machine learning models,
which are typically narrow in scope and require task-specific engineering, FMs
offer cross-domain generalization and exhibit emergent capabilities. Their
versatility is especially well-suited to materials science, where research
challenges span diverse data types and scales. This survey provides a
comprehensive overview of foundation models, agentic systems, datasets, and
computational tools supporting this growing field. We introduce a task-driven
taxonomy encompassing six broad application areas: data extraction,
interpretation and Q\&A; atomistic simulation; property prediction; materials
structure, design and discovery; process planning, discovery, and optimization;
and multiscale modeling. We discuss recent advances in both unimodal and
multimodal FMs, as well as emerging large language model (LLM) agents.
Furthermore, we review standardized datasets, open-source tools, and autonomous
experimental platforms that collectively fuel the development and integration
of FMs into research workflows. We assess the early successes of foundation
models and identify persistent limitations, including challenges in
generalizability, interpretability, data imbalance, safety concerns, and
limited multimodal fusion. Finally, we articulate future research directions
centered on scalable pretraining, continual learning, data governance, and
trustworthiness.

</details>


### [52] [GPU Kernel Scientist: An LLM-Driven Framework for Iterative Kernel Optimization](https://arxiv.org/abs/2506.20807)
*Martin Andrews,Sam Witteveen*

Main category: cs.LG

TL;DR: 本文介绍了由大语言模型驱动的“GPU内核科学家”自动化方法，用于迭代优化加速器内核，虽无定量结果，但展示了架构设计、工作流程和定性见解。


<details>
  <summary>Details</summary>
Motivation: 优化GPU内核性能复杂，尤其在新的或文档较少的GPU架构上传统开发辅助手段稀缺，需要新方法。

Method: 采用大语言模型进行多阶段、进化式过程，包括选择有前景的代码版本、生成优化假设、自主实现实验。

Result: 因定量结果处于保密期未公布，展示了架构设计、工作流程和定性见解。

Conclusion: 大语言模型驱动的代理有潜力使GPU内核优化更普及、更高效，尤其在资源受限或硬件快速发展的环境中。

Abstract: Optimizing GPU kernels for high performance is a complex task, often
demanding deep architectural knowledge, extensive profiling, and iterative
experimentation. This challenge is amplified when targeting newer or
less-documented GPU architectures where traditional development aids are
scarce. This paper introduces an LLM-powered "GPU Kernel Scientist," an
automated methodology for iteratively refining accelerator kernels.
  Our methodology employs LLMs in a multi-stage, evolutionary process: (a)
strategically selecting promising prior code versions as a basis for new
iterations; (b) generating hypotheses for optimization experiments, based on
existing code and assimilated knowledge from general GPU literature; and (c)
autonomously implementing these experiments through code modification and
subsequent submission to an external evaluation system, using only observed
timing data as performance feedback. We detail how this approach navigates the
challenges of the AMD MI300 target architecture and leverages LLMs to
compensate for limited domain-specific human expertise.
  Since quantitative results from an ongoing performance competition were
embargoed on paper submission date, we present the architectural design,
operational workflow, and qualitative insights, highlighting the potential of
LLM-driven agents to democratise and accelerate GPU kernel optimization,
especially in resource-constrained or rapidly evolving hardware environments.

</details>


### [53] [Diffusion Tree Sampling: Scalable inference-time alignment of diffusion models](https://arxiv.org/abs/2506.20701)
*Vineet Jain,Kusha Sareen,Mohammad Pedramfar,Siamak Ravanbakhsh*

Main category: cs.LG

TL;DR: 本文提出Diffusion Tree Sampling (DTS)及其贪心变体DTS*，解决推理时预训练扩散模型适配新目标问题，减少计算量。


<details>
  <summary>Details</summary>
Motivation: 现有推理时引导方法存在价值估计不准确、不重用过往信息导致计算低效的问题。

Method: 将推理时对齐问题转化为搜索问题，提出基于树的方法，通过传播终端奖励和迭代细化价值估计来采样。

Result: 在MNIST和CIFAR - 10上，DTS用至多10倍少的计算量达到最佳基线FID；在文本到图像生成和语言完成任务中，DTS*用至多5倍少的计算量搜索高奖励样本。

Conclusion: 通过重用过往信息，得到可扩展的算法，能将更多计算转化为更好样本，用于扩散模型推理时对齐。

Abstract: Adapting a pretrained diffusion model to new objectives at inference time
remains an open problem in generative modeling. Existing steering methods
suffer from inaccurate value estimation, especially at high noise levels, which
biases guidance. Moreover, information from past runs is not reused to improve
sample quality, resulting in inefficient use of compute. Inspired by the
success of Monte Carlo Tree Search, we address these limitations by casting
inference-time alignment as a search problem that reuses past computations. We
introduce a tree-based approach that samples from the reward-aligned target
density by propagating terminal rewards back through the diffusion chain and
iteratively refining value estimates with each additional generation. Our
proposed method, Diffusion Tree Sampling (DTS), produces asymptotically exact
samples from the target distribution in the limit of infinite rollouts, and its
greedy variant, Diffusion Tree Search (DTS$^\star$), performs a global search
for high reward samples. On MNIST and CIFAR-10 class-conditional generation,
DTS matches the FID of the best-performing baseline with up to $10\times$ less
compute. In text-to-image generation and language completion tasks, DTS$^\star$
effectively searches for high reward samples that match best-of-N with up to
$5\times$ less compute. By reusing information from previous generations, we
get an anytime algorithm that turns additional compute into steadily better
samples, providing a scalable approach for inference-time alignment of
diffusion models.

</details>


### [54] [On Convolutions, Intrinsic Dimension, and Diffusion Models](https://arxiv.org/abs/2506.20705)
*Kin Kwan Leung,Rasa Hosseinzadeh,Gabriel Loaiza-Ganem*

Main category: cs.LG

TL;DR: 本文在现实假设下证明了FLIPD局部固有维度（LID）估计器的正确性，并探讨了用均匀卷积替代高斯卷积的类似结果。


<details>
  <summary>Details</summary>
Motivation: 现有LID估计器FLIPD在理论基础上存在不足，原证明基于不现实的仿射子流形假设。

Method: 在现实假设下对FLIPD的正确性进行形式化证明，并研究用均匀卷积替代高斯卷积的情况。

Result: 成功在现实假设下证明了FLIPD的正确性，且发现用均匀卷积替代高斯卷积也有类似结果。

Conclusion: 弥补了FLIPD理论基础的缺失，为其实际应用提供更坚实的理论支持。

Abstract: The manifold hypothesis asserts that data of interest in high-dimensional
ambient spaces, such as image data, lies on unknown low-dimensional
submanifolds. Diffusion models (DMs) -- which operate by convolving data with
progressively larger amounts of Gaussian noise and then learning to revert this
process -- have risen to prominence as the most performant generative models,
and are known to be able to learn distributions with low-dimensional support.
For a given datum in one of these submanifolds, we should thus intuitively
expect DMs to have implicitly learned its corresponding local intrinsic
dimension (LID), i.e. the dimension of the submanifold it belongs to. Kamkari
et al. (2024b) recently showed that this is indeed the case by linking this LID
to the rate of change of the log marginal densities of the DM with respect to
the amount of added noise, resulting in an LID estimator known as FLIPD. LID
estimators such as FLIPD have a plethora of uses, among others they quantify
the complexity of a given datum, and can be used to detect outliers,
adversarial examples and AI-generated text. FLIPD achieves state-of-the-art
performance at LID estimation, yet its theoretical underpinnings are incomplete
since Kamkari et al. (2024b) only proved its correctness under the highly
unrealistic assumption of affine submanifolds. In this work we bridge this gap
by formally proving the correctness of FLIPD under realistic assumptions.
Additionally, we show that an analogous result holds when Gaussian convolutions
are replaced with uniform ones, and discuss the relevance of this result.

</details>


### [55] [Test-time Scaling Techniques in Theoretical Physics -- A Comparison of Methods on the TPBench Dataset](https://arxiv.org/abs/2506.20729)
*Zhiqi Gao,Tianyi Li,Yurii Kvasiuk,Sai Chaitanya Tadepalli,Maja Rudolph,Daniel J. H. Chung,Frederic Sala,Moritz Münchmeyer*

Main category: cs.LG

TL;DR: 本文研究数学推理基准测试的经验能否推广到高级理论物理领域，评估常见测试时缩放方法，提出新的符号弱验证框架，该方法在物理数据集和数学问题上均有效。


<details>
  <summary>Details</summary>
Motivation: 探究数学推理基准测试的经验能否推广到高级理论物理领域。

Method: 在TPBench物理数据集上评估常见测试时缩放方法，与AIME结果对比；开发新颖的符号弱验证框架以利用物理问题结构。

Result: 新方法在TPBench上显著优于现有测试时缩放方法，在AIME上也证实有效。

Conclusion: 逐步符号验证对解决复杂科学问题有强大作用。

Abstract: Large language models (LLMs) have shown strong capabilities in complex
reasoning, and test-time scaling techniques can enhance their performance with
comparably low cost. Many of these methods have been developed and evaluated on
mathematical reasoning benchmarks such as AIME. This paper investigates whether
the lessons learned from these benchmarks generalize to the domain of advanced
theoretical physics. We evaluate a range of common test-time scaling methods on
the TPBench physics dataset and compare their effectiveness with results on
AIME. To better leverage the structure of physics problems, we develop a novel,
symbolic weak-verifier framework to improve parallel scaling results. Our
empirical results demonstrate that this method significantly outperforms
existing test-time scaling approaches on TPBench. We also evaluate our method
on AIME, confirming its effectiveness in solving advanced mathematical
problems. Our findings highlight the power of step-wise symbolic verification
for tackling complex scientific problems.

</details>


### [56] [LLM-guided Chemical Process Optimization with a Multi-Agent Approach](https://arxiv.org/abs/2506.20921)
*Tong Zeng,Srivathsan Badrinarayanan,Janghoon Ock,Cheng-Kai Lai,Amir Barati Farimani*

Main category: cs.LG

TL;DR: 提出基于大语言模型的多智能体框架用于化学过程优化，在加氢脱烷基化过程验证，有计算效率且能理解过程，适用于约束不明场景。


<details>
  <summary>Details</summary>
Motivation: 传统化学过程优化方法在操作约束不明确或不可用时不实用，需解决约束定义瓶颈。

Method: 提出基于AutoGen的多智能体框架，用OpenAI的o3模型，有约束生成、参数验证等专门智能体，分自主约束生成和迭代多智能体优化两阶段。

Result: 在加氢脱烷基化过程中，与传统优化方法性能相当，计算效率更高，收敛速度比网格搜索快31倍，能正确识别效用权衡并应用领域启发式。

Conclusion: 该方法在操作约束不明或不可用的优化场景有显著潜力，尤其适用于新兴过程和改造应用。

Abstract: Chemical process optimization is crucial to maximize production efficiency
and economic performance. Traditional methods, including gradient-based
solvers, evolutionary algorithms, and parameter grid searches, become
impractical when operating constraints are ill-defined or unavailable,
requiring engineers to rely on subjective heuristics to estimate feasible
parameter ranges. To address this constraint definition bottleneck, we present
a multi-agent framework of large language model (LLM) agents that autonomously
infer operating constraints from minimal process descriptions, then
collaboratively guide optimization using the inferred constraints. Our
AutoGen-based agentic framework employs OpenAI's o3 model, with specialized
agents for constraint generation, parameter validation, simulation execution,
and optimization guidance. Through two phases - autonomous constraint
generation using embedded domain knowledge, followed by iterative multi-agent
optimization - the framework eliminates the need for predefined operational
bounds. Validated on the hydrodealkylation process across cost, yield, and
yield-to-cost ratio metrics, the framework demonstrated competitive performance
with conventional optimization methods while achieving better computational
efficiency, requiring fewer iterations to converge. Our approach converged in
under 20 minutes, achieving a 31-fold speedup over grid search. Beyond
computational efficiency, the framework's reasoning-guided search demonstrates
sophisticated process understanding, correctly identifying utility trade-offs,
and applying domain-informed heuristics. This approach shows significant
potential for optimization scenarios where operational constraints are poorly
characterized or unavailable, particularly for emerging processes and retrofit
applications.

</details>


### [57] [Multiple Streams of Relation Extraction: Enriching and Recalling in Transformers](https://arxiv.org/abs/2506.20746)
*Todd Nief,David Reber,Sean Richardson,Ari Holtzman*

Main category: cs.LG

TL;DR: 提出动态权重嫁接方法分析微调大语言模型中关系信息路径，发现信息提取和召回路径特点。


<details>
  <summary>Details</summary>
Motivation: 现有定位方法不适用于分析微调大语言模型中关系信息去向，需新方法填补空白。

Method: 在微调与预训练语言模型间进行动态权重嫁接。

Result: 微调模型在处理实体时提取关系信息，在生成预测时召回信息，不同情况所需路径不同，召回路径通过特定机制和网络输出。

Conclusion: 明确了微调大语言模型中关系信息的提取和召回路径及相关特点。

Abstract: When an LLM learns a relation during finetuning (e.g., new movie releases,
corporate mergers, etc.), where does this information go? Is it extracted when
the model processes an entity, recalled just-in-time before a prediction, or
are there multiple separate heuristics? Existing localization approaches (e.g.
activation patching) are ill-suited for this analysis because they tend to
replace parts of the residual stream, potentially deleting information. To fill
this gap, we propose dynamic weight-grafting between fine-tuned and pre-trained
language models to show that fine-tuned language models both (1) extract
relation information learned during finetuning while processing entities and
(2) ``recall" this information in later layers while generating predictions. In
some cases, models need both of these pathways to correctly generate finetuned
information while, in other cases, a single ``enrichment" or ``recall" pathway
alone is sufficient. We examine the necessity and sufficiency of these
information pathways, examining what layers they occur at, how much redundancy
they exhibit, and which model components are involved -- finding that the
``recall" pathway occurs via both task-specific attention mechanisms and a
relation extraction step in the output of the attention and the feedforward
networks at the final layers before next token prediction.

</details>


### [58] [Characterization and Mitigation of Training Instabilities in Microscaling Formats](https://arxiv.org/abs/2506.20752)
*Huangyuan Su,Mujin Kwun,Stephanie Gil,Sham Kakade,Nikhil Anand*

Main category: cs.LG

TL;DR: 研究MX格式在大语言模型训练中的挑战与可行性，发现训练存在损失不稳定问题，提出简单模型解释现象，展示调整精度方案可避免不稳定，评估稳定策略。


<details>
  <summary>Details</summary>
Motivation: 大语言模型训练成本高，下一代硬件加速器支持低精度算术格式，研究块缩放精度格式在模型训练中的挑战与可行性。

Method: 训练近千个从零开始的语言模型，在较小代理模型上进行控制实验和消融实验，进行原位干预实验，评估大语言模型稳定策略。

Result: MX格式训练损失存在随机不稳定现象，提出简单模型解释，通过调整精度方案可避免或延迟不稳定，特定混合配置可恢复与全精度训练相当的性能。

Conclusion: MX格式在大语言模型训练中有损失不稳定问题，但可通过调整精度方案和采用特定混合配置来稳定训练，恢复性能。

Abstract: Training large language models is an expensive, compute-bound process that
must be repeated as models scale, algorithms improve, and new data is
collected. To address this, next-generation hardware accelerators increasingly
support lower-precision arithmetic formats, such as the Microscaling (MX)
formats introduced in NVIDIA's Blackwell architecture. These formats use a
shared scale within blocks of parameters to extend representable range and
perform forward/backward GEMM operations in reduced precision for efficiency
gains. In this work, we investigate the challenges and viability of
block-scaled precision formats during model training. Across nearly one
thousand language models trained from scratch -- spanning compute budgets from
$2 \times 10^{17}$ to $4.8 \times 10^{19}$ FLOPs and sweeping over a broad
range of weight-activation precision combinations -- we consistently observe
that training in MX formats exhibits sharp, stochastic instabilities in the
loss, particularly at larger compute scales. To explain this phenomenon, we
conduct controlled experiments and ablations on a smaller proxy model that
exhibits similar behavior as the language model, sweeping across architectural
settings, hyperparameters, and precision formats. These experiments motivate a
simple model in which multiplicative gradient bias introduced by the
quantization of layer-norm affine parameters and a small fraction of
activations can trigger runaway divergence. Through \emph{in situ} intervention
experiments on our proxy model, we demonstrate that instabilities can be
averted or delayed by modifying precision schemes mid-training. Guided by these
findings, we evaluate stabilization strategies in the LLM setting and show that
certain hybrid configurations recover performance competitive with
full-precision training. We release our code at
https://github.com/Hither1/systems-scaling.

</details>


### [59] [Stochastic and Non-local Closure Modeling for Nonlinear Dynamical Systems via Latent Score-based Generative Models](https://arxiv.org/abs/2506.20771)
*Xinghao Dong,Huchen Yang,Jin-Long Wu*

Main category: cs.LG

TL;DR: 提出基于潜在分数的生成式AI框架学习计算力学中非线性动力系统的随机、非局部封闭模型和本构定律，联合训练卷积自动编码器和条件扩散模型降低计算成本，模拟时可加速且保证精度。


<details>
  <summary>Details</summary>
Motivation: 解决无清晰尺度分离的复杂多尺度动力系统建模难题，经典封闭建模方法假设受限，现有扩散随机模型计算推理成本高。

Method: 在潜在空间联合训练卷积自动编码器和条件扩散模型，降低采样过程维度同时保留物理特征。

Result: 联合训练有助于找到合适潜在空间，保证小重构误差和扩散模型性能，集成到数值模拟时可加速且精度与标准扩散模型相当。

Conclusion: 提出的潜在条件扩散模型随机建模框架能显著加速计算，同时保持较好预测精度。

Abstract: We propose a latent score-based generative AI framework for learning
stochastic, non-local closure models and constitutive laws in nonlinear
dynamical systems of computational mechanics. This work addresses a key
challenge of modeling complex multiscale dynamical systems without a clear
scale separation, for which numerically resolving all scales is prohibitively
expensive, e.g., for engineering turbulent flows. While classical closure
modeling methods leverage domain knowledge to approximate subgrid-scale
phenomena, their deterministic and local assumptions can be too restrictive in
regimes lacking a clear scale separation. Recent developments of
diffusion-based stochastic models have shown promise in the context of closure
modeling, but their prohibitive computational inference cost limits practical
applications for many real-world applications. This work addresses this
limitation by jointly training convolutional autoencoders with conditional
diffusion models in the latent spaces, significantly reducing the
dimensionality of the sampling process while preserving essential physical
characteristics. Numerical results demonstrate that the joint training approach
helps discover a proper latent space that not only guarantees small
reconstruction errors but also ensures good performance of the diffusion model
in the latent space. When integrated into numerical simulations, the proposed
stochastic modeling framework via latent conditional diffusion models achieves
significant computational acceleration while maintaining comparable predictive
accuracy to standard diffusion models in physical spaces.

</details>


### [60] [Stochastic Parameter Decomposition](https://arxiv.org/abs/2506.20790)
*Lucius Bushnaq,Dan Braun,Lee Sharkey*

Main category: cs.LG

TL;DR: 本文提出随机参数分解（SPD）方法，比基于归因的参数分解（APD）更具扩展性和鲁棒性，还避免了一些问题，为机械可解释性研究开辟新可能。


<details>
  <summary>Details</summary>
Motivation: 当前线性参数分解框架中的主要方法APD存在计算成本高和对超参数敏感的问题，需要更实用的方法。

Method: 提出随机参数分解（SPD）方法。

Result: 通过分解比APD能处理的更大更复杂的模型，证明SPD更具扩展性和对超参数的鲁棒性，还避免了学习参数收缩等问题，能更好识别玩具模型中的真实机制。

Conclusion: SPD移除了线性参数分解方法扩展到更大模型的障碍，为机械可解释性研究开辟了新的研究可能性。

Abstract: A key step in reverse engineering neural networks is to decompose them into
simpler parts that can be studied in relative isolation. Linear parameter
decomposition -- a framework that has been proposed to resolve several issues
with current decomposition methods -- decomposes neural network parameters into
a sum of sparsely used vectors in parameter space. However, the current main
method in this framework, Attribution-based Parameter Decomposition (APD), is
impractical on account of its computational cost and sensitivity to
hyperparameters. In this work, we introduce \textit{Stochastic Parameter
Decomposition} (SPD), a method that is more scalable and robust to
hyperparameters than APD, which we demonstrate by decomposing models that are
slightly larger and more complex than was possible to decompose with APD. We
also show that SPD avoids other issues, such as shrinkage of the learned
parameters, and better identifies ground truth mechanisms in toy models. By
bridging causal mediation analysis and network decomposition methods, this
demonstration opens up new research possibilities in mechanistic
interpretability by removing barriers to scaling linear parameter decomposition
methods to larger models. We release a library for running SPD and reproducing
our experiments at https://github.com/goodfire-ai/spd.

</details>


### [61] [FINN-GL: Generalized Mixed-Precision Extensions for FPGA-Accelerated LSTMs](https://arxiv.org/abs/2506.20810)
*Shashwat Khandelwal,Jakoba Petri-Koenig,Thomas B. Preußer,Michaela Blott,Shreejith Shanker*

Main category: cs.LG

TL;DR: 利用FINN框架实现LSTM在FPGA上的通用部署，验证工具流在股票预测任务上的效果，生成的加速器能平衡性能与资源消耗。


<details>
  <summary>Details</summary>
Motivation: RNN尤其是LSTM计算复杂，实时部署有挑战，现有FPGA工具主要针对前馈网络，LSTM加速需定制实现，需填补这一空白。

Method: 利用ONNX的Scan算子建模LSTM计算，在FINN编译器中引入自定义转换，将量化ONNX计算图映射到硬件块。

Result: 在股票预测任务上验证工具流，生成的量化ConvLSTM加速器能平衡性能与资源消耗，推理精度不低于现有模型。

Conclusion: 提出的工具流具有通用性，为FPGA上资源高效的RNN加速器设计奠定基础。

Abstract: Recurrent neural networks (RNNs), particularly LSTMs, are effective for
time-series tasks like sentiment analysis and short-term stock prediction.
However, their computational complexity poses challenges for real-time
deployment in resource constrained environments. While FPGAs offer a promising
platform for energy-efficient AI acceleration, existing tools mainly target
feed-forward networks, and LSTM acceleration typically requires full custom
implementation. In this paper, we address this gap by leveraging the
open-source and extensible FINN framework to enable the generalized deployment
of LSTMs on FPGAs. Specifically, we leverage the Scan operator from the Open
Neural Network Exchange (ONNX) specification to model the recurrent nature of
LSTM computations, enabling support for mixed quantisation within them and
functional verification of LSTM-based models. Furthermore, we introduce custom
transformations within the FINN compiler to map the quantised ONNX computation
graph to hardware blocks from the HLS kernel library of the FINN compiler and
Vitis HLS. We validate the proposed tool-flow by training a quantised ConvLSTM
model for a mid-price stock prediction task using the widely used dataset and
generating a corresponding hardware IP of the model using our flow, targeting
the XCZU7EV device. We show that the generated quantised ConvLSTM accelerator
through our flow achieves a balance between performance (latency) and resource
consumption, while matching (or bettering) inference accuracy of
state-of-the-art models with reduced precision. We believe that the
generalisable nature of the proposed flow will pave the way for
resource-efficient RNN accelerator designs on FPGAs.

</details>


### [62] [Optimal Single-Policy Sample Complexity and Transient Coverage for Average-Reward Offline RL](https://arxiv.org/abs/2506.20904)
*Matthew Zurek,Guy Zamir,Yudong Chen*

Main category: cs.LG

TL;DR: 研究平均奖励MDPs中的离线强化学习，提出仅依赖目标策略的保证和首个单策略样本复杂度界限，处理一般弱连通MDPs，引入新算法并给出下界。


<details>
  <summary>Details</summary>
Motivation: 平均奖励MDPs中的离线强化学习在分布偏移和非均匀覆盖方面有挑战且理论研究不足，以往工作在单策略数据覆盖假设下的性能保证使用了对所有策略统一的复杂度度量。

Method: 引入基于悲观折扣值迭代并由新的分位数裁剪技术增强的算法，使用更尖锐的基于经验跨度的惩罚函数，且算法实现无需先验参数知识。

Result: 得到仅依赖目标策略的尖锐保证和首个单策略样本复杂度界限，处理一般弱连通MDPs，通过困难例子表明学习需超越目标策略平稳分布的覆盖假设，给出接近主结果的下界。

Conclusion: 提出了新的理论结果和算法，推动了平均奖励MDPs中离线强化学习的研究，区分了单策略复杂度度量与以往情况。

Abstract: We study offline reinforcement learning in average-reward MDPs, which
presents increased challenges from the perspectives of distribution shift and
non-uniform coverage, and has been relatively underexamined from a theoretical
perspective. While previous work obtains performance guarantees under
single-policy data coverage assumptions, such guarantees utilize additional
complexity measures which are uniform over all policies, such as the uniform
mixing time. We develop sharp guarantees depending only on the target policy,
specifically the bias span and a novel policy hitting radius, yielding the
first fully single-policy sample complexity bound for average-reward offline
RL. We are also the first to handle general weakly communicating MDPs,
contrasting restrictive structural assumptions made in prior work. To achieve
this, we introduce an algorithm based on pessimistic discounted value iteration
enhanced by a novel quantile clipping technique, which enables the use of a
sharper empirical-span-based penalty function. Our algorithm also does not
require any prior parameter knowledge for its implementation. Remarkably, we
show via hard examples that learning under our conditions requires coverage
assumptions beyond the stationary distribution of the target policy,
distinguishing single-policy complexity measures from previously examined
cases. We also develop lower bounds nearly matching our main result.

</details>


### [63] [An Information-Theoretic Analysis for Federated Learning under Concept Drift](https://arxiv.org/abs/2506.21036)
*Fu Peng,Meng Zhang,Ming Tang*

Main category: cs.LG

TL;DR: 本文用信息论分析概念漂移下联邦学习性能，提出算法缓解性能下降，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习多在静态数据集上训练，而现实数据是流数据且分布会变，存在概念漂移导致性能下降问题。

Method: 将概念漂移建模为马尔可夫链，引入平稳泛化误差评估模型，用KL散度和互信息推导其上界；研究三种漂移模式；提出用KL散度和互信息正则化经验风险最小化的算法；确定帕累托前沿探索性能 - 成本权衡；搭建测试平台验证。

Result: 实验结果与理论相符，漂移模式显著影响性能，所提方法在三种模式下均优于现有方法。

Conclusion: 所提方法能有效适应联邦学习中的概念漂移。

Abstract: Recent studies in federated learning (FL) commonly train models on static
datasets. However, real-world data often arrives as streams with shifting
distributions, causing performance degradation known as concept drift. This
paper analyzes FL performance under concept drift using information theory and
proposes an algorithm to mitigate the performance degradation. We model concept
drift as a Markov chain and introduce the \emph{Stationary Generalization
Error} to assess a model's capability to capture characteristics of future
unseen data. Its upper bound is derived using KL divergence and mutual
information. We study three drift patterns (periodic, gradual, and random) and
their impact on FL performance. Inspired by this, we propose an algorithm that
regularizes the empirical risk minimization approach with KL divergence and
mutual information, thereby enhancing long-term performance. We also explore
the performance-cost tradeoff by identifying a Pareto front. To validate our
approach, we build an FL testbed using Raspberry Pi4 devices. Experimental
results corroborate with theoretical findings, confirming that drift patterns
significantly affect performance. Our method consistently outperforms existing
approaches for these three patterns, demonstrating its effectiveness in
adapting concept drift in FL.

</details>


### [64] [Divide, Specialize, and Route: A New Approach to Efficient Ensemble Learning](https://arxiv.org/abs/2506.20814)
*Jakub Piwko,Jędrzej Ruciński,Dawid Płudowski,Antoni Zajko,Patryzja Żak,Mateusz Zacharecki,Anna Kozak,Katarzyna Woźnica*

Main category: cs.LG

TL;DR: 提出Hellsemble集成框架解决传统集成学习问题，实验显示其优于经典方法，按实例难度构建集成系统有前景。


<details>
  <summary>Details</summary>
Motivation: 传统集成学习方法计算成本高、对异构数据适应性有限。

Method: 提出Hellsemble框架，迭代划分数据集为难度圈，训练专门的基学习器，用路由模型分配新实例。

Result: 在OpenML - CC18和Tabzilla基准测试中，Hellsemble常优于经典集成方法。

Conclusion: 考虑实例级难度为构建高效稳健集成系统提供了有前景的方向。

Abstract: Ensemble learning has proven effective in boosting predictive performance,
but traditional methods such as bagging, boosting, and dynamic ensemble
selection (DES) suffer from high computational cost and limited adaptability to
heterogeneous data distributions. To address these limitations, we propose
Hellsemble, a novel and interpretable ensemble framework for binary
classification that leverages dataset complexity during both training and
inference. Hellsemble incrementally partitions the dataset into circles of
difficulty by iteratively passing misclassified instances from simpler models
to subsequent ones, forming a committee of specialised base learners. Each
model is trained on increasingly challenging subsets, while a separate router
model learns to assign new instances to the most suitable base model based on
inferred difficulty. Hellsemble achieves strong classification accuracy while
maintaining computational efficiency and interpretability. Experimental results
on OpenML-CC18 and Tabzilla benchmarks demonstrate that Hellsemble often
outperforms classical ensemble methods. Our findings suggest that embracing
instance-level difficulty offers a promising direction for constructing
efficient and robust ensemble systems.

</details>


### [65] [Universal and Efficient Detection of Adversarial Data through Nonuniform Impact on Network Layers](https://arxiv.org/abs/2506.20816)
*Furkan Mumcu,Yasin Yilmaz*

Main category: cs.LG

TL;DR: 本文指出DNN对抗攻击检测研究不足，现有方法存在缺陷，提出通过分析攻击对不同DNN层影响检测对抗样本的方法，该方法有效、高效且通用。


<details>
  <summary>Details</summary>
Motivation: DNN易受对抗攻击，防御技术研究不足，现有检测方法存在无效或计算效率低的问题，需更实用的检测方法。

Method: 训练轻量级回归模型，根据早期层特征预测深层特征，用预测误差检测对抗样本。

Result: 通过理论论证和大量实验表明，该检测方法高效、适用于实时处理、与任何DNN架构兼容且跨领域适用。

Conclusion: 提出的检测对抗样本方法有效、高效且通用，能解决现有检测方法的问题。

Abstract: Deep Neural Networks (DNNs) are notoriously vulnerable to adversarial input
designs with limited noise budgets. While numerous successful attacks with
subtle modifications to original input have been proposed, defense techniques
against these attacks are relatively understudied. Existing defense approaches
either focus on improving DNN robustness by negating the effects of
perturbations or use a secondary model to detect adversarial data. Although
equally important, the attack detection approach, which is studied in this
work, provides a more practical defense compared to the robustness approach. We
show that the existing detection methods are either ineffective against the
state-of-the-art attack techniques or computationally inefficient for real-time
processing. We propose a novel universal and efficient method to detect
adversarial examples by analyzing the varying degrees of impact of attacks on
different DNN layers. {Our method trains a lightweight regression model that
predicts deeper-layer features from early-layer features, and uses the
prediction error to detect adversarial samples.} Through theoretical arguments
and extensive experiments, we demonstrate that our detection method is highly
effective, computationally efficient for real-time processing, compatible with
any DNN architecture, and applicable across different domains, such as image,
video, and audio.

</details>


### [66] [Chain-of-Thought Enhanced Shallow Transformers for Wireless Symbol Detection](https://arxiv.org/abs/2506.21093)
*Li Fan,Peng Wang,Jing Yang,Cong Shen*

Main category: cs.LG

TL;DR: 提出CHOOSE框架用于无线符号检测，使浅层Transformer性能媲美深层模型，适用于资源受限设备。


<details>
  <summary>Details</summary>
Motivation: 以往基于ICL的Transformer模型依赖深层架构，存储和计算成本高，需提升浅层模型性能。

Method: 提出CHOOSE框架，在隐藏空间引入自回归潜在推理步骤，提升浅层模型推理能力。

Result: CHOOSE方法优于传统浅层Transformer，性能与深层Transformer相当，且保持存储和计算效率。

Conclusion: 该方法为在计算资源有限的无线接收器中实现基于Transformer的算法提供了有前景的方向。

Abstract: Transformers have shown potential in solving wireless communication problems,
particularly via in-context learning (ICL), where models adapt to new tasks
through prompts without requiring model updates. However, prior ICL-based
Transformer models rely on deep architectures with many layers to achieve
satisfactory performance, resulting in substantial storage and computational
costs. In this work, we propose CHain Of thOught Symbol dEtection (CHOOSE), a
CoT-enhanced shallow Transformer framework for wireless symbol detection. By
introducing autoregressive latent reasoning steps within the hidden space,
CHOOSE significantly improves the reasoning capacity of shallow models (1-2
layers) without increasing model depth. This design enables lightweight
Transformers to achieve detection performance comparable to much deeper models,
making them well-suited for deployment on resource-constrained mobile devices.
Experimental results demonstrate that our approach outperforms conventional
shallow Transformers and achieves performance comparable to that of deep
Transformers, while maintaining storage and computational efficiency. This
represents a promising direction for implementing Transformer-based algorithms
in wireless receivers with limited computational resources.

</details>


### [67] [Demystifying Distributed Training of Graph Neural Networks for Link Prediction](https://arxiv.org/abs/2506.20818)
*Xin Huang,Chul-Ho Lee*

Main category: cs.LG

TL;DR: 本文研究分布式GNN链路预测训练中性能下降问题，提出SpLPG方法，降低通信开销同时保证预测准确率。


<details>
  <summary>Details</summary>
Motivation: 多数分布式GNN框架针对节点分类优化，链路预测性能研究不足，需解决分布式训练中性能下降问题。

Method: 提出SpLPG方法，利用图稀疏化减少性能下降问题和通信开销。

Result: 在多个公开真实数据集上实验，SpLPG最多降低约80%通信开销，基本保留链路预测准确率。

Conclusion: SpLPG能有效降低分布式GNN链路预测训练的通信开销，同时保证预测准确率。

Abstract: Graph neural networks (GNNs) are powerful tools for solving graph-related
problems. Distributed GNN frameworks and systems enhance the scalability of
GNNs and accelerate model training, yet most are optimized for node
classification. Their performance on link prediction remains underexplored.
This paper demystifies distributed training of GNNs for link prediction by
investigating the issue of performance degradation when each worker trains a
GNN on its assigned partitioned subgraph without having access to the entire
graph. We discover that the main sources of the issue come from not only the
information loss caused by graph partitioning but also the ways of drawing
negative samples during model training. While sharing the complete graph
information with each worker resolves the issue and preserves link prediction
accuracy, it incurs a high communication cost. We propose SpLPG, which
effectively leverages graph sparsification to mitigate the issue of performance
degradation at a reduced communication cost. Experiment results on several
public real-world datasets demonstrate the effectiveness of SpLPG, which
reduces the communication overhead by up to about 80% while mostly preserving
link prediction accuracy.

</details>


### [68] [Linearity-based neural network compression](https://arxiv.org/abs/2506.21146)
*Silas Dobler,Florian Lemmerich*

Main category: cs.LG

TL;DR: 提出基于线性的神经网络压缩方法，可无损压缩至原模型1/4大小，不同压缩技术可结合。


<details>
  <summary>Details</summary>
Motivation: 增强现有的高度优化的神经网络压缩解决方案。

Method: 基于ReLU类激活函数下神经元线性行为的直觉，提出线性压缩方法，将后续层合并。

Result: 在多数测试模型中实现无损压缩至原模型1/4大小，与基于重要性的剪枝模型干扰小。

Conclusion: 为新型压缩方法奠定基础，使神经网络模型更小、更高效。

Abstract: In neural network compression, most current methods reduce unnecessary
parameters by measuring importance and redundancy. To augment already highly
optimized existing solutions, we propose linearity-based compression as a novel
way to reduce weights in a neural network. It is based on the intuition that
with ReLU-like activation functions, neurons that are almost always activated
behave linearly, allowing for merging of subsequent layers. We introduce the
theory underlying this compression and evaluate our approach experimentally.
Our novel method achieves a lossless compression down to 1/4 of the original
model size in over the majority of tested models. Applying our method on
already importance-based pruned models shows very little interference between
different types of compression, demonstrating the option of successful
combination of techniques. Overall, our work lays the foundation for a new type
of compression method that enables smaller and ultimately more efficient neural
network models.

</details>


### [69] [Learning-Based Resource Management in Integrated Sensing and Communication Systems](https://arxiv.org/abs/2506.20849)
*Ziyang Lu,M. Cenk Gursoy,Chilukuri K. Mohan,Pramod K. Varshney*

Main category: cs.LG

TL;DR: 提出CDRL方法解决雷达通信一体化系统自适应时间分配问题，数值结果证明其能在时间约束下提高通信质量。


<details>
  <summary>Details</summary>
Motivation: 解决雷达通信一体化系统中跟踪与通信的资源分配问题，提高目标通信质量。

Method: 引入约束深度强化学习（CDRL）方法来优化跟踪和通信之间的资源分配。

Result: 数值结果表明提出的CDRL框架能在高度动态环境中，在遵守时间约束的同时最大化通信质量。

Conclusion: 提出的CDRL方法在解决雷达通信一体化系统自适应时间分配问题上是有效的。

Abstract: In this paper, we tackle the task of adaptive time allocation in integrated
sensing and communication systems equipped with radar and communication units.
The dual-functional radar-communication system's task involves allocating dwell
times for tracking multiple targets and utilizing the remaining time for data
transmission towards estimated target locations. We introduce a novel
constrained deep reinforcement learning (CDRL) approach, designed to optimize
resource allocation between tracking and communication under time budget
constraints, thereby enhancing target communication quality. Our numerical
results demonstrate the efficiency of our proposed CDRL framework, confirming
its ability to maximize communication quality in highly dynamic environments
while adhering to time constraints.

</details>


### [70] [Multi-Objective Reinforcement Learning for Cognitive Radar Resource Management](https://arxiv.org/abs/2506.20853)
*Ziyang Lu,Subodh Kalia,M. Cenk Gursoy,Chilukuri K. Mohan,Pramod K. Varshney*

Main category: cs.LG

TL;DR: 针对多功能认知雷达系统时间分配问题，用多目标优化和深度强化学习求解，对比DDPG和SAC算法，还用NSGA - II估计Pareto前沿上限，助力高效自适应雷达系统发展。


<details>
  <summary>Details</summary>
Motivation: 解决多功能认知雷达系统中扫描新目标和跟踪已检测目标的时间分配权衡问题。

Method: 将问题建模为多目标优化问题，使用深度强化学习，对比DDPG和SAC算法，用NSGA - II算法估计Pareto前沿上限。

Result: DDPG和SAC算法在不同场景均有效，SAC比DDPG稳定性和样本效率更高。

Conclusion: 该研究有助于开发能在动态环境平衡多目标的高效自适应认知雷达系统。

Abstract: The time allocation problem in multi-function cognitive radar systems focuses
on the trade-off between scanning for newly emerging targets and tracking the
previously detected targets. We formulate this as a multi-objective
optimization problem and employ deep reinforcement learning to find
Pareto-optimal solutions and compare deep deterministic policy gradient (DDPG)
and soft actor-critic (SAC) algorithms. Our results demonstrate the
effectiveness of both algorithms in adapting to various scenarios, with SAC
showing improved stability and sample efficiency compared to DDPG. We further
employ the NSGA-II algorithm to estimate an upper bound on the Pareto front of
the considered problem. This work contributes to the development of more
efficient and adaptive cognitive radar systems capable of balancing multiple
competing objectives in dynamic environments.

</details>


### [71] [Leaner Training, Lower Leakage: Revisiting Memorization in LLM Fine-Tuning with LoRA](https://arxiv.org/abs/2506.20856)
*Fei Wang,Baochun Li*

Main category: cs.LG

TL;DR: 研究大语言模型微调中的记忆问题，发现LoRA微调与预训练和全量微调在记忆表现上不同，且LoRA可降低记忆风险并保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究对大语言模型微调尤其是LoRA微调中记忆问题的影响探索较少，而记忆问题使模型易受数据提取攻击。

Method: 使用更宽松的基于相似度的记忆度量方法。

Result: 不同微调策略下记忆表现与之前发现有差异，模型规模和数据重复等因素在LoRA微调中影响趋势不同，LoRA相比全量微调显著降低记忆风险且保持强任务性能。

Conclusion: LoRA微调在降低记忆风险和保持性能上有优势。

Abstract: Memorization in large language models (LLMs) makes them vulnerable to data
extraction attacks. While pre-training memorization has been extensively
studied, fewer works have explored its impact in fine-tuning, particularly for
LoRA fine-tuning, a widely adopted parameter-efficient method.
  In this work, we re-examine memorization in fine-tuning and uncover a
surprising divergence from prior findings across different fine-tuning
strategies. Factors such as model scale and data duplication, which strongly
influence memorization in pre-training and full fine-tuning, do not follow the
same trend in LoRA fine-tuning. Using a more relaxed similarity-based
memorization metric, we demonstrate that LoRA significantly reduces
memorization risks compared to full fine-tuning, while still maintaining strong
task performance.

</details>


### [72] [Omniwise: Predicting GPU Kernels Performance with LLMs](https://arxiv.org/abs/2506.20886)
*Zixian Wang,Cole Ramos,Muhammad A. Awad,Keith Lowery*

Main category: cs.LG

TL;DR: 介绍Omniwise，首个将大语言模型应用于GPU内核性能预测的端到端自监督微调管道，表现良好并开发相关工具集成到开发者工作流。


<details>
  <summary>Details</summary>
Motivation: 随着深度神经网络发展，将大语言模型应用于GPU内核性能预测这一新型性能分析用例。

Method: 提出Omniwise管道，该管道模型无关且轻量级，可直接从内核代码预测关键性能指标。

Result: 在AMD MI250和MI300X架构上执行的GPU内核，超90%的预测相对误差在10%以内，还开发了在线推理服务器和VS Code插件。

Conclusion: Omniwise在GPU内核性能预测方面表现良好，相关工具能将基于大语言模型的性能预测无缝集成到开发者工作流。

Abstract: In recent years, the rapid advancement of deep neural networks (DNNs) has
revolutionized artificial intelligence, enabling models with unprecedented
capabilities in understanding, generating, and processing complex data. These
powerful architectures have transformed a wide range of downstream
applications, tackling tasks beyond human reach. In this paper, we introduce
Omniwise, the first end-to-end, self-supervised fine-tuning pipeline that
applies large language models (LLMs) to GPU kernel performance prediction--a
novel use case in performance profiling. Omniwise is model-agnostic and
lightweight, achieving strong results even with a small 3B-parameter model. It
can predict key performance metrics, including memory bandwidth, cache hit
rates, GFLOPs, and arithmetic intensity, directly from kernel code without the
need for code execution or profiling tools. Our approach achieves over 90% of
predictions within 10% relative error on GPU kernels executed on AMD MI250 and
MI300X architectures. In addition to the pipeline, we develop an online
inference server and a Visual Studio Code plugin that seamlessly integrate
LLM-based performance prediction into developers' workflows.

</details>


### [73] [On the Necessity of Output Distribution Reweighting for Effective Class Unlearning](https://arxiv.org/abs/2506.20893)
*Yian Wang,Ali Ebrahimpour-Boroojeny,Hari Sundaram*

Main category: cs.LG

TL;DR: 提出轻量级输出重加权遗忘方法RWFT，可在不重新训练的情况下从训练好的分类器中删除整个类，实验表明该方法优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 忘记训练模型中的特定类对保障用户删除权和减少有害或有偏差的预测至关重要，但全量重新训练成本高，现有遗忘方法存在缺陷。

Method: 设计成员推理攻击变体MIA - NN证明现有方法的失败，提出对遗忘类样本预测概率质量的简单重新分配，引入基于总变差距离的新指标量化残余泄漏。

Result: 实验显示该方法在先前评估指标和新指标上都能达到全量重新训练的效果，在先前指标上比现有最佳方法提高2.79%，在新指标上提高111.45%。

Conclusion: 提出的RWFT方法是一种有效的轻量级遗忘方法，能解决现有方法的问题。

Abstract: In this work, we introduce an output-reweighting unlearning method, RWFT, a
lightweight technique that erases an entire class from a trained classifier
without full retraining. Forgetting specific classes from trained models is
essential for enforcing user deletion rights and mitigating harmful or biased
predictions. The full retraining is costly and existing unlearning methods fail
to replicate the behavior of the retrained models when predicting samples from
the unlearned class. We prove this failure by designing a variant of membership
inference attacks, MIA-NN that successfully reveals the unlearned class for any
of these methods. We propose a simple redistribution of the probability mass
for the prediction on the samples in the forgotten class which is robust to
MIA-NN. We also introduce a new metric based on the total variation (TV)
distance of the prediction probabilities to quantify residual leakage to
prevent future methods from susceptibility to the new attack. Through extensive
experiments with state of the art baselines in machine unlearning, we show that
our approach matches the results of full retraining in both metrics used for
evaluation by prior work and the new metric we propose in this work. Compare to
state-of-the-art methods, we gain 2.79% in previously used metrics and 111.45%
in our new TV-based metric over the best existing method.

</details>


### [74] [Graph-Structured Feedback Multimodel Ensemble Online Conformal Prediction](https://arxiv.org/abs/2506.20898)
*Erfan Hajihashemi,Yanning Shen*

Main category: cs.LG

TL;DR: 提出新颖多模型在线共形预测算法，通过二分图反馈选有效模型子集，降低计算复杂度和预测集大小，经实验验证优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模型在线共形预测中预选模型集带来计算复杂度增加和无关模型影响性能等问题。

Method: 提出新算法，在每个时间步通过二分图反馈识别有效模型子集，从中选模型构建预测集，还结合预测集大小和模型损失作为反馈。

Result: 所提算法保证有效覆盖和次线性遗憾，实验表明构建的预测集更小，优于现有方法。

Conclusion: 所提新颖多模型在线共形预测算法有效，能降低计算复杂度和预测集大小，提升效率。

Abstract: Online conformal prediction has demonstrated its capability to construct a
prediction set for each incoming data point that covers the true label with a
predetermined probability. To cope with potential distribution shift,
multi-model online conformal prediction has been introduced to select and
leverage different models from a preselected candidate set. Along with the
improved flexibility, the choice of the preselected set also brings challenges.
A candidate set that includes a large number of models may increase the
computational complexity. In addition, the inclusion of irrelevant models with
poor performance may negatively impact the performance and lead to
unnecessarily large prediction sets. To address these challenges, we propose a
novel multi-model online conformal prediction algorithm that identifies a
subset of effective models at each time step by collecting feedback from a
bipartite graph, which is refined upon receiving new data. A model is then
selected from this subset to construct the prediction set, resulting in reduced
computational complexity and smaller prediction sets. Additionally, we
demonstrate that using prediction set size as feedback, alongside model loss,
can significantly improve efficiency by constructing smaller prediction sets
while still satisfying the required coverage guarantee. The proposed algorithms
are proven to ensure valid coverage and achieve sublinear regret. Experiments
on real and synthetic datasets validate that the proposed methods construct
smaller prediction sets and outperform existing multi-model online conformal
prediction approaches.

</details>


### [75] [Explainable AI for Radar Resource Management: Modified LIME in Deep Reinforcement Learning](https://arxiv.org/abs/2506.20916)
*Ziyang Lu,M. Cenk Gursoy,Chilukuri K. Mohan,Pramod K. Varshney*

Main category: cs.LG

TL;DR: 本文提出DL - LIME改进方法用于雷达资源管理的深度强化学习，结果显示其性能优于传统LIME。


<details>
  <summary>Details</summary>
Motivation: 神经网络“黑盒”特性限制其应用，传统LIME采样过程忽略特征相关性，需要改进方法。

Method: 提出将深度学习融入采样过程的改进LIME方法DL - LIME，并用于雷达资源管理的深度强化学习。

Result: DL - LIME在保真度和任务性能上优于传统LIME，还能指出雷达资源管理决策中更重要的因素。

Conclusion: DL - LIME是一种有效的可解释AI方法，能提升雷达资源管理决策的可解释性和性能。

Abstract: Deep reinforcement learning has been extensively studied in decision-making
processes and has demonstrated superior performance over conventional
approaches in various fields, including radar resource management (RRM).
However, a notable limitation of neural networks is their ``black box" nature
and recent research work has increasingly focused on explainable AI (XAI)
techniques to describe the rationale behind neural network decisions. One
promising XAI method is local interpretable model-agnostic explanations (LIME).
However, the sampling process in LIME ignores the correlations between
features. In this paper, we propose a modified LIME approach that integrates
deep learning (DL) into the sampling process, which we refer to as DL-LIME. We
employ DL-LIME within deep reinforcement learning for radar resource
management. Numerical results show that DL-LIME outperforms conventional LIME
in terms of both fidelity and task performance, demonstrating superior
performance with both metrics. DL-LIME also provides insights on which factors
are more important in decision making for radar resource management.

</details>


### [76] [Interpretable Representation Learning for Additive Rule Ensembles](https://arxiv.org/abs/2506.20927)
*Shahrzad Behzadimanesh,Pierre Le Bodic,Geoffrey I. Webb,Mario Boley*

Main category: cs.LG

TL;DR: 本文提出一种扩展经典规则集成的方法，通过引入含可学习稀疏线性变换的逻辑命题，利用序贯贪婪优化学习，实验表明该方法能有效构建规则集成，降低模型复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统规则集成依赖精心策划的输入特征，缺乏时需增加规则数量和复杂度，会降低模型可解释性，因此要进行改进。

Method: 引入含可学习稀疏线性变换的逻辑命题，提出基于逻辑回归迭代重加权公式的序贯贪婪优化学习方法。

Result: 在十个基准数据集上，该方法能有效构建规则集成，与现有最优方法有相同测试风险，同时显著降低模型复杂度。

Conclusion: 提出的扩展规则集成方法在保证测试风险的同时，能有效降低模型复杂度。

Abstract: Small additive ensembles of symbolic rules offer interpretable prediction
models. Traditionally, these ensembles use rule conditions based on
conjunctions of simple threshold propositions $x \geq t$ on a single input
variable $x$ and threshold $t$, resulting geometrically in axis-parallel
polytopes as decision regions. While this form ensures a high degree of
interpretability for individual rules and can be learned efficiently using the
gradient boosting approach, it relies on having access to a curated set of
expressive and ideally independent input features so that a small ensemble of
axis-parallel regions can describe the target variable well. Absent such
features, reaching sufficient accuracy requires increasing the number and
complexity of individual rules, which diminishes the interpretability of the
model. Here, we extend classical rule ensembles by introducing logical
propositions with learnable sparse linear transformations of input variables,
i.e., propositions of the form $\mathbf{x}^\mathrm{T}\mathbf{w} \geq t$, where
$\mathbf{w}$ is a learnable sparse weight vector, enabling decision regions as
general polytopes with oblique faces. We propose a learning method using
sequential greedy optimization based on an iteratively reweighted formulation
of logistic regression. Experimental results demonstrate that the proposed
method efficiently constructs rule ensembles with the same test risk as
state-of-the-art methods while significantly reducing model complexity across
ten benchmark datasets.

</details>


### [77] [Model State Arithmetic for Machine Unlearning](https://arxiv.org/abs/2506.20941)
*Keivan Rezaei,Mehrdad Saberi,Abhilasha Ravichander,Soheil Feizi*

Main category: cs.LG

TL;DR: 提出新算法MSA用于估计和消除数据点影响，实验表明其优于现有算法，或能让大语言模型更灵活地进行数据擦除。


<details>
  <summary>Details</summary>
Motivation: 大语言模型训练数据存在问题，完全重新训练计算成本高，现有消除特定数据点影响的算法有挑战。

Method: 提出新算法MSA，利用模型检查点估计和消除数据点影响。

Result: MSA在多个基准测试、模型和评估指标上始终优于现有机器遗忘算法。

Conclusion: MSA可能是实现更灵活、可进行数据擦除的大语言模型的有效方法。

Abstract: Large language models are trained on massive corpora of web data, which may
include private data, copyrighted material, factually inaccurate data, or data
that degrades model performance. Eliminating the influence of such problematic
datapoints through complete retraining -- by repeatedly pretraining the model
on datasets that exclude these specific instances -- is computationally
prohibitive. For this reason, unlearning algorithms have emerged that aim to
eliminate the influence of particular datapoints, while otherwise preserving
the model -- at a low computational cost. However, precisely estimating and
undoing the influence of individual datapoints has proved to be challenging. In
this work, we propose a new algorithm, MSA, for estimating and undoing the
influence of datapoints -- by leveraging model checkpoints i.e. artifacts
capturing model states at different stages of pretraining. Our experimental
results demonstrate that MSA consistently outperforms existing machine
unlearning algorithms across multiple benchmarks, models, and evaluation
metrics, suggesting that MSA could be an effective approach towards more
flexible large language models that are capable of data erasure.

</details>


### [78] [Antibody Design and Optimization with Multi-scale Equivariant Graph Diffusion Models for Accurate Complex Antigen Binding](https://arxiv.org/abs/2506.20957)
*Jiameng Chen,Xiantao Cai,Jia Wu,Wenbin Hu*

Main category: cs.LG

TL;DR: 本文提出抗体设计框架AbMEGD，结合多尺度等变图扩散，实验显示其在氨基酸恢复、偏差减少等方面优于DiffAb。


<details>
  <summary>Details</summary>
Motivation: 当前计算方法在捕捉几何特征、保持对称性和泛化新抗原界面上存在局限，无法准确捕捉分子相互作用和保持结构完整性，需要新的抗体设计方法。

Method: 提出AbMEGD框架，结合原子级几何特征与残基级嵌入，使用E(3)等变扩散方法。

Result: 使用SAbDab数据库实验，相比DiffAb，AbMEGD在关键CDR - H3区域氨基酸恢复率提高10.13%，改进百分比提高3.32%，均方根偏差降低0.062 Å。

Conclusion: AbMEGD能平衡结构完整性和功能改进，为序列 - 结构协同设计和亲和力优化树立新标杆。

Abstract: Antibody design remains a critical challenge in therapeutic and diagnostic
development, particularly for complex antigens with diverse binding interfaces.
Current computational methods face two main limitations: (1) capturing
geometric features while preserving symmetries, and (2) generalizing novel
antigen interfaces. Despite recent advancements, these methods often fail to
accurately capture molecular interactions and maintain structural integrity. To
address these challenges, we propose \textbf{AbMEGD}, an end-to-end framework
integrating \textbf{M}ulti-scale \textbf{E}quivariant \textbf{G}raph
\textbf{D}iffusion for antibody sequence and structure co-design. Leveraging
advanced geometric deep learning, AbMEGD combines atomic-level geometric
features with residue-level embeddings, capturing local atomic details and
global sequence-structure interactions. Its E(3)-equivariant diffusion method
ensures geometric precision, computational efficiency, and robust
generalizability for complex antigens. Furthermore, experiments using the
SAbDab database demonstrate a 10.13\% increase in amino acid recovery, 3.32\%
rise in improvement percentage, and a 0.062~\AA\ reduction in root mean square
deviation within the critical CDR-H3 region compared to DiffAb, a leading
antibody design model. These results highlight AbMEGD's ability to balance
structural integrity with improved functionality, establishing a new benchmark
for sequence-structure co-design and affinity optimization. The code is
available at: https://github.com/Patrick221215/AbMEGD.

</details>


### [79] [SharpZO: Hybrid Sharpness-Aware Vision Language Model Prompt Tuning via Forward-Only Passes](https://arxiv.org/abs/2506.20990)
*Yifan Yang,Zhen Zhang,Rupak Vignesh Swaminathan,Jing Liu,Nathan Susanj,Zheng Zhang*

Main category: cs.LG

TL;DR: 本文提出混合SharpZO方法用于VLM微调，通过两阶段优化提升ZO VLM微调性能，实验表明其能显著提高准确率和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 传统微调需反向传播，不适用于内存受限的推理边缘设备，现有无BP微调方法依赖高方差策略且性能不佳。

Method: 提出SharpZO方法，包含两阶段优化：先进行锐度感知ES阶段全局探索和平滑损失景观构建初始化，再通过稀疏ZO优化进行细粒度局部搜索，仅依赖前向传播。

Result: 在CLIP模型上的理论分析和实验表明，SharpZO显著提高准确率和收敛速度，比现有仅前向传播方法平均提升达7%。

Conclusion: SharpZO方法有效提升了ZO VLM微调性能，为内存受限边缘设备上的VLM微调提供了更好的解决方案。

Abstract: Fine-tuning vision language models (VLMs) has achieved remarkable performance
across various downstream tasks; yet, it requires access to model gradients
through backpropagation (BP), making them unsuitable for memory-constrained,
inference-only edge devices. To address this limitation, previous work has
explored various BP-free fine-tuning methods. However, these approaches often
rely on high-variance evolutionary strategies (ES) or zeroth-order (ZO)
optimization, and often fail to achieve satisfactory performance. In this
paper, we propose a hybrid Sharpness-aware Zeroth-order optimization (SharpZO)
approach, specifically designed to enhance the performance of ZO VLM
fine-tuning via a sharpness-aware warm-up training. SharpZO features a
two-stage optimization process: a sharpness-aware ES stage that globally
explores and smooths the loss landscape to construct a strong initialization,
followed by a fine-grained local search via sparse ZO optimization. The entire
optimization relies solely on forward passes. Detailed theoretical analysis and
extensive experiments on CLIP models demonstrate that SharpZO significantly
improves accuracy and convergence speed, achieving up to 7% average gain over
state-of-the-art forward-only methods.

</details>


### [80] [Distilling Normalizing Flows](https://arxiv.org/abs/2506.21003)
*Steven Walton,Valeriy Klyukin,Maksim Artemev,Denis Derkach,Nikita Orlov,Humphrey Shi*

Main category: cs.LG

TL;DR: 本文介绍显式密度学习器优势与不足，提出用知识蒸馏技术提升小型学生归一化流的采样质量和密度估计能力，发现蒸馏可使模型变小且性能提升。


<details>
  <summary>Details</summary>
Motivation: 显式密度学习器难训练且采样质量低，为提升小型学生归一化流的采样质量和密度估计能力，研究知识蒸馏在组合归一化流中的作用。

Method: 提出新颖的知识蒸馏技术，利用归一化流中间层进行非传统知识转移。

Result: 通过蒸馏，学生模型可显著变小，且性能比未蒸馏的学生模型有大幅提升，小模型吞吐量成比例增加。

Conclusion: 知识蒸馏技术对提升归一化流性能有效，能在减小模型规模的同时提高性能。

Abstract: Explicit density learners are becoming an increasingly popular technique for
generative models because of their ability to better model probability
distributions. They have advantages over Generative Adversarial Networks due to
their ability to perform density estimation and having exact latent-variable
inference. This has many advantages, including: being able to simply
interpolate, calculate sample likelihood, and analyze the probability
distribution. The downside of these models is that they are often more
difficult to train and have lower sampling quality.
  Normalizing flows are explicit density models, that use composable bijective
functions to turn an intractable probability function into a tractable one. In
this work, we present novel knowledge distillation techniques to increase
sampling quality and density estimation of smaller student normalizing flows.
We seek to study the capacity of knowledge distillation in Compositional
Normalizing Flows to understand the benefits and weaknesses provided by these
architectures. Normalizing flows have unique properties that allow for a
non-traditional forms of knowledge transfer, where we can transfer that
knowledge within intermediate layers. We find that through this distillation,
we can make students significantly smaller while making substantial performance
gains over a non-distilled student. With smaller models there is a
proportionally increased throughput as this is dependent upon the number of
bijectors, and thus parameters, in the network.

</details>


### [81] [TRIDENT: Tri-Modal Molecular Representation Learning with Taxonomic Annotations and Local Correspondence](https://arxiv.org/abs/2506.21028)
*Feng Jiang,Mangal Prakash,Hehuan Ma,Jianyuan Deng,Yuzhi Guo,Amina Mollaysa,Tommaso Mansi,Rui Liao,Junzhou Huang*

Main category: cs.LG

TL;DR: 提出TRIDENT框架，结合分子SMILES、文本描述和分类功能注释学习分子表示，在11个下游任务达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有多模态学习在分子表示学习中忽视文本和分类信息。

Method: 构建分子 - 文本对数据集；采用基于体积的对齐目标进行全局对齐；引入局部对齐目标；用基于动量的机制平衡全局和局部对齐。

Result: TRIDENT在11个下游任务上达到了当前最优性能。

Conclusion: 结合SMILES、文本和分类功能注释对分子属性预测有价值。

Abstract: Molecular property prediction aims to learn representations that map chemical
structures to functional properties. While multimodal learning has emerged as a
powerful paradigm to learn molecular representations, prior works have largely
overlooked textual and taxonomic information of molecules for representation
learning. We introduce TRIDENT, a novel framework that integrates molecular
SMILES, textual descriptions, and taxonomic functional annotations to learn
rich molecular representations. To achieve this, we curate a comprehensive
dataset of molecule-text pairs with structured, multi-level functional
annotations. Instead of relying on conventional contrastive loss, TRIDENT
employs a volume-based alignment objective to jointly align tri-modal features
at the global level, enabling soft, geometry-aware alignment across modalities.
Additionally, TRIDENT introduces a novel local alignment objective that
captures detailed relationships between molecular substructures and their
corresponding sub-textual descriptions. A momentum-based mechanism dynamically
balances global and local alignment, enabling the model to learn both broad
functional semantics and fine-grained structure-function mappings. TRIDENT
achieves state-of-the-art performance on 11 downstream tasks, demonstrating the
value of combining SMILES, textual, and taxonomic functional annotations for
molecular property prediction.

</details>


### [82] [Little By Little: Continual Learning via Self-Activated Sparse Mixture-of-Rank Adaptive Learning](https://arxiv.org/abs/2506.21035)
*Haodong Lu,Chongyang Zhao,Jason Xue,Lina Yao,Kristen Moore,Dong Gong*

Main category: cs.LG

TL;DR: 现有基于LoRA的Mixture - of - Experts方法在持续学习中存在干扰、冗余和路由模糊问题，本文提出MoRA方法，在CLIP和大语言模型的持续学习任务中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决持续学习中基于LoRA的Mixture - of - Experts方法存在的干扰、冗余和路由模糊问题，提升大预训练模型的持续学习能力。

Method: 提出MoRA方法，将每个秩 - r更新分解为r个秩 - 1组件，每个组件作为独立专家，通过中间激活推断相关性，结合秩剪枝和激活预算自适应选择稀疏秩混合。

Result: 在CLIP和大语言模型的持续学习任务中验证了MoRA的有效性，增强了大预训练模型的持续学习能力，减少遗忘并提高泛化性。

Conclusion: MoRA在提升大预训练模型持续学习能力、减少遗忘和提高泛化性方面效果显著。

Abstract: Continual learning (CL) with large pre-trained models is challenged by
catastrophic forgetting and task interference. Existing LoRA-based
Mixture-of-Experts (MoE) approaches mitigate forgetting by assigning and
freezing task-specific adapters, but suffer from interference, redundancy, and
ambiguous routing due to coarse adapter-level selection. However, this design
introduces three key challenges: 1) Interference: Activating full LoRA experts
per input leads to subspace interference and prevents selective reuse of useful
components across tasks. 2) Redundancy: Newly added experts often duplicate or
contradict existing knowledge due to unnecessary activation of unrelated ranks
and insufficient reuse of relevant ones. 3) Ambiguity: Overlapping features
across tasks confuse the router, resulting in unstable expert assignments. As
more experts accumulate, earlier task routing degrades, accelerating
forgetting. We propose MoRA, a Mixture-of-Rank Adaptive learning approach with
self-activated and sparse rank activation for CL. Unlike mixing multiple
low-rank matrices, MoRA decomposes each rank-r update into r rank-1 components,
each treated as an independent expert, enabling fine-grained mixture of rank-1
expert utilization while mitigating interference and redundancy. To avoid
ambiguous routing, we propose that each rank-1 expert can infer its own
relevance via intermediate activations. Coupled with our proposed rank pruning
and activation budgets, MoRA adaptively selects a sparse mixture of ranks per
input. We validate MoRA on continual learning tasks with CLIP and large
language models (LLMs), analyzing both in-domain learning and out-of-domain
forgetting/generalization during fine-tuning. MoRA shows significant
effectiveness on enhancing CL with PTMs, and improving generalization while
mitigating forgetting.

</details>


### [83] [RL-Selector: Reinforcement Learning-Guided Data Selection via Redundancy Assessment](https://arxiv.org/abs/2506.21037)
*Suorong Yang,Peijia Li,Furao Shen,Jian Zhao*

Main category: cs.LG

TL;DR: 现有数据选择方法有局限，提出RL - Selector方法，实验证明其优于现有方法，提升训练效率和泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现代深度架构训练大数据集开销大，现有数据选择方法有缺陷，需更有效的数据选择范式。

Method: 引入epsilon - sample cover概念，将数据选择问题重新表述为强化学习过程，提出RL - Selector，利用epsilon - sample cover作为奖励信号优化选择策略。

Result: 在多个基准数据集和不同架构上实验，该方法始终优于现有最先进的基线方法。

Conclusion: 所提出的方法能提高训练效率，让模型有更好的泛化性能。

Abstract: Modern deep architectures often rely on large-scale datasets, but training on
these datasets incurs high computational and storage overhead. Real-world
datasets often contain substantial redundancies, prompting the need for more
data-efficient training paradigms. Data selection has shown promise to mitigate
redundancy by identifying the most representative samples, thereby reducing
training costs without compromising performance. Existing methods typically
rely on static scoring metrics or pretrained models, overlooking the combined
effect of selected samples and their evolving dynamics during training. We
introduce the concept of epsilon-sample cover, which quantifies sample
redundancy based on inter-sample relationships, capturing the intrinsic
structure of the dataset. Based on this, we reformulate data selection as a
reinforcement learning (RL) process and propose RL-Selector, where a
lightweight RL agent optimizes the selection policy by leveraging
epsilon-sample cover derived from evolving dataset distribution as a reward
signal. Extensive experiments across benchmark datasets and diverse
architectures demonstrate that our method consistently outperforms existing
state-of-the-art baselines. Models trained with our selected datasets show
enhanced generalization performance with improved training efficiency.

</details>


### [84] [Strict Subgoal Execution: Reliable Long-Horizon Planning in Hierarchical Reinforcement Learning](https://arxiv.org/abs/2506.21039)
*Jaebak Hwang,Sanghyeon Lee,Jeongmo Kim,Seungyul Han*

Main category: cs.LG

TL;DR: 提出Strict Subgoal Execution (SSE) 图基分层强化学习框架解决长程目标条件任务，实验显示其效率和成功率优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 长程目标条件任务对强化学习构成挑战，现有分层和图基方法存在子目标不可行和规划低效问题。

Method: 引入SSE框架，通过结构约束高层决策确保单步子目标可达性，采用解耦探索策略增强探索，使用失败感知路径细化改进图基规划。

Result: 在不同长程基准测试中，SSE在效率和成功率上始终优于现有目标条件强化学习和分层强化学习方法。

Conclusion: SSE是解决长程目标条件任务的有效框架。

Abstract: Long-horizon goal-conditioned tasks pose fundamental challenges for
reinforcement learning (RL), particularly when goals are distant and rewards
are sparse. While hierarchical and graph-based methods offer partial solutions,
they often suffer from subgoal infeasibility and inefficient planning. We
introduce Strict Subgoal Execution (SSE), a graph-based hierarchical RL
framework that enforces single-step subgoal reachability by structurally
constraining high-level decision-making. To enhance exploration, SSE employs a
decoupled exploration policy that systematically traverses underexplored
regions of the goal space. Furthermore, a failure-aware path refinement, which
refines graph-based planning by dynamically adjusting edge costs according to
observed low-level success rates, thereby improving subgoal reliability.
Experimental results across diverse long-horizon benchmarks demonstrate that
SSE consistently outperforms existing goal-conditioned RL and hierarchical RL
approaches in both efficiency and success rate.

</details>


### [85] [Efficient Skill Discovery via Regret-Aware Optimization](https://arxiv.org/abs/2506.21044)
*He Zhang,Ming Zhou,Shaopeng Zhai,Ying Sun,Hui Xiong*

Main category: cs.LG

TL;DR: 本文提出基于后悔感知的技能发现方法，将其构建为技能生成与策略学习的极小极大博弈，实验表明该方法在效率和多样性上优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有无监督技能发现方法在效率上存在局限，尤其是高维情况。

Method: 将技能发现构建为技能生成与策略学习的极小极大博弈，基于时间表征学习提出后悔感知方法，用后悔值衡量强度收敛程度，用可学习的技能生成器引导技能发现，且技能生成来自可升级的生成器群体。

Result: 在不同复杂度和维度的环境中实验，方法在效率和多样性上优于基线，在高维环境零样本性能提升15%。

Conclusion: 提出的方法在无监督技能发现中，相比现有方法在效率、多样性和高维环境零样本性能上表现更优。

Abstract: Unsupervised skill discovery aims to learn diverse and distinguishable
behaviors in open-ended reinforcement learning. For existing methods, they
focus on improving diversity through pure exploration, mutual information
optimization, and learning temporal representation. Despite that they perform
well on exploration, they remain limited in terms of efficiency, especially for
the high-dimensional situations. In this work, we frame skill discovery as a
min-max game of skill generation and policy learning, proposing a regret-aware
method on top of temporal representation learning that expands the discovered
skill space along the direction of upgradable policy strength. The key insight
behind the proposed method is that the skill discovery is adversarial to the
policy learning, i.e., skills with weak strength should be further explored
while less exploration for the skills with converged strength. As an
implementation, we score the degree of strength convergence with regret, and
guide the skill discovery with a learnable skill generator. To avoid
degeneration, skill generation comes from an up-gradable population of skill
generators. We conduct experiments on environments with varying complexities
and dimension sizes. Empirical results show that our method outperforms
baselines in both efficiency and diversity. Moreover, our method achieves a 15%
zero shot improvement in high-dimensional environments, compared to existing
methods.

</details>


### [86] [FedDAA: Dynamic Client Clustering for Concept Drift Adaptation in Federated Learning](https://arxiv.org/abs/2506.21054)
*Fu Peng,Ming Tang*

Main category: cs.LG

TL;DR: 提出FedDAA框架应对联邦学习中的多源概念漂移，实验显示有精度提升。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法处理概念漂移多关注真实漂移，对虚拟或标签漂移处理不佳，无法区分不同漂移源导致策略欠佳。

Method: 提出FedDAA框架，包含集群数量确定、真实漂移检测和概念漂移适应三个模块。

Result: FedDAA在Fashion - MNIST、CIFAR - 10和CIFAR - 100上比现有方法精度提升7.84%到8.52%。

Conclusion: FedDAA能适应多源概念漂移并保留有价值的历史知识。

Abstract: In federated learning (FL), the data distribution of each client may change
over time, introducing both temporal and spatial data heterogeneity, known as
concept drift. Data heterogeneity arises from three drift sources: real drift
(a shift in the conditional distribution P(y|x)), virtual drift (a shift in the
input distribution P(x)), and label drift (a shift in the label distribution
P(y)). However, most existing FL methods addressing concept drift primarily
focus on real drift. When clients experience virtual or label drift, these
methods often fail to selectively retain useful historical knowledge, leading
to catastrophic forgetting. A key challenge lies in distinguishing different
sources of drift, as they require distinct adaptation strategies: real drift
calls for discarding outdated data, while virtual or label drift benefits from
retaining historical data. Without explicitly identifying the drift sources, a
general adaptation strategy is suboptimal and may harm generalization. To
address this challenge, we propose FedDAA, a dynamic clustered FL framework
designed to adapt to multi-source concept drift while preserving valuable
historical knowledge. Specifically, FedDAA integrates three modules: a cluster
number determination module to find the optimal number of clusters; a real
drift detection module to distinguish real drift from virtual/label drift; and
a concept drift adaptation module to adapt to new data while retaining useful
historical information. We provide theoretical convergence guarantees, and
experiments show that FedDAA achieves 7.84% to 8.52% accuracy improvements over
state-of-the-art methods on Fashion-MNIST, CIFAR-10, and CIFAR-100.

</details>


### [87] [Enhancing LLM Tool Use with High-quality Instruction Data from Knowledge Graph](https://arxiv.org/abs/2506.21071)
*Jingwei Wang,Zai Zhang,Hao Qian,Chunjing Gan,Binbin Hu,Ziqi Liu,Zhiqiang Zhang,Jun Zhou,Bin Shi,Bo Dong*

Main category: cs.LG

TL;DR: 本文提出用知识图谱为大语言模型生成高质量指令数据的方法，少量合成数据微调可提升模型工具使用和整体能力。


<details>
  <summary>Details</summary>
Motivation: 教大语言模型使用工具很重要，但此前方法生成的数据质量不足，需新方法生成高质量指令数据。

Method: 从给定知识图谱中提取查询路径转化为用户查询，将实体关系转化为可操作工具，把查询路径解析为详细解决步骤，生成高质量指令数据。

Result: 在少量合成数据上微调能显著提升大语言模型的工具使用和整体能力。

Conclusion: 使用知识图谱生成高质量指令数据的方法能有效提升大语言模型的工具利用和整体能力。

Abstract: Teaching large language models (LLMs) to use tools is crucial for improving
their problem-solving abilities and expanding their applications. However,
effectively using tools is challenging because it requires a deep understanding
of tool functionalities and user intentions. Previous methods relied mainly on
LLMs to generate instruction data, but the quality of these data was often
insufficient. In this paper, we propose a new method that uses knowledge graphs
to generate high-quality instruction data for LLMs. Knowledge graphs are
manually curated datasets rich in semantic information. We begin by extracting
various query pathways from a given knowledge graph, which are transformed into
a broad spectrum of user queries. We then translate the relationships between
entities into actionable tools and parse the pathways of each query into
detailed solution steps, thereby creating high-quality instruction data. Our
experiments show that fine-tuning on just a small sample of this synthetic data
can significantly improve the tool utilization and overall capabilities of
LLMs.

</details>


### [88] [FeDa4Fair: Client-Level Federated Datasets for Fairness Evaluation](https://arxiv.org/abs/2506.21095)
*Xenia Heilmann,Luca Corbucci,Mattia Cerrato,Anna Monreale*

Main category: cs.LG

TL;DR: 论文指出联邦学习公平性问题，提出为公平性研究进行基准测试，贡献包括引入FeDa4Fair库、发布数据集和基准、提供评估函数。


<details>
  <summary>Details</summary>
Motivation: 现有公平性解决方案多关注单一敏感属性，无法满足不同客户端多样且可能冲突的公平性需求，为支持更稳健和可复现的联邦学习公平性研究。

Method: 引入FeDa4Fair库生成表格数据集；发布四个有偏差异质性的数据集及对应基准；提供评估公平性结果的可用函数。

Result: 得到了FeDa4Fair库、四个数据集及基准、评估函数。

Conclusion: 通过这些贡献可对公平感知的联邦学习方法在全局和客户端层面进行一致的基准测试。

Abstract: Federated Learning (FL) enables collaborative model training across multiple
clients without sharing clients' private data. However, fairness remains a key
concern, as biases in local clients' datasets can impact the entire federated
system. Heterogeneous data distributions across clients may lead to models that
are fairer for some clients than others. Although several fairness-enhancing
solutions are present in the literature, most focus on mitigating bias for a
single sensitive attribute, typically binary, overlooking the diverse and
sometimes conflicting fairness needs of different clients. This limited
perspective can limit the effectiveness of fairness interventions for the
different clients. To support more robust and reproducible fairness research in
FL, we aim to enable a consistent benchmarking of fairness-aware FL methods at
both the global and client levels. In this paper, we contribute in three ways:
(1) We introduce FeDa4Fair, a library to generate tabular datasets tailored to
evaluating fair FL methods under heterogeneous client bias; (2) we release four
bias-heterogeneous datasets and corresponding benchmarks to compare fairness
mitigation methods in a controlled environment; (3) we provide ready-to-use
functions for evaluating fairness outcomes for these datasets.

</details>


### [89] [Interpretable Hierarchical Concept Reasoning through Attention-Guided Graph Learning](https://arxiv.org/abs/2506.21102)
*David Debot,Pietro Barbiero,Gabriele Dominici,Giuseppe Marra*

Main category: cs.LG

TL;DR: 提出Hierarchical Concept Memory Reasoner (H - CMR)以解决当前概念基模型概念预测不可解释问题，实验表明其性能好且支持强人类交互。


<details>
  <summary>Details</summary>
Motivation: 当前概念基模型仅对最终任务预测有可解释性，概念预测通常由黑盒神经网络完成，缺乏可解释性。

Method: 提出H - CMR，用有向无环图建模概念关系，推理时用神经注意力机制选择规则并分层应用来预测概念和任务。

Result: H - CMR达到了最先进的性能，能通过概念和模型干预实现强人类交互，前者可在推理时提高准确率，后者在有背景知识时可提高训练数据效率。

Conclusion: H - CMR是一种有效的新模型，可解决现有概念基模型概念预测不可解释的问题。

Abstract: Concept-Based Models (CBMs) are a class of deep learning models that provide
interpretability by explaining predictions through high-level concepts. These
models first predict concepts and then use them to perform a downstream task.
However, current CBMs offer interpretability only for the final task
prediction, while the concept predictions themselves are typically made via
black-box neural networks. To address this limitation, we propose Hierarchical
Concept Memory Reasoner (H-CMR), a new CBM that provides interpretability for
both concept and task predictions. H-CMR models relationships between concepts
using a learned directed acyclic graph, where edges represent logic rules that
define concepts in terms of other concepts. During inference, H-CMR employs a
neural attention mechanism to select a subset of these rules, which are then
applied hierarchically to predict all concepts and the final task. Experimental
results demonstrate that H-CMR matches state-of-the-art performance while
enabling strong human interaction through concept and model interventions. The
former can significantly improve accuracy at inference time, while the latter
can enhance data efficiency during training when background knowledge is
available.

</details>


### [90] [Learning to Skip the Middle Layers of Transformers](https://arxiv.org/abs/2506.21103)
*Tim Lawson,Laurence Aitchison*

Main category: cs.LG

TL;DR: 提出动态跳过中间层的Transformer架构，但未在验证交叉熵和FLOPs权衡上优于少层密集基线。


<details>
  <summary>Details</summary>
Motivation: 现有条件计算方法常针对单个模块或独立跳过层，而Transformer中间层冗余大，希望减少简单token计算量并形成多级表征层次。

Method: 提出新架构，用学习的门控机制决定是否跳过中间对称块，用门控注意力机制防止后续token关注跳过位置，用特定方案控制残差范数和门稀疏性。

Result: 在研究规模下，该方法未在验证交叉熵和估计FLOPs的权衡上比少层密集基线有改进。

Conclusion: 提出的跳过中间层架构未达预期效果，代码已开源。

Abstract: Conditional computation is a popular strategy to make Transformers more
efficient. Existing methods often target individual modules (e.g.,
mixture-of-experts layers) or skip layers independently of one another.
However, interpretability research has demonstrated that the middle layers of
Transformers exhibit greater redundancy, and that early layers aggregate
information into token positions. Guided by these insights, we propose a novel
architecture that dynamically skips a variable number of layers from the middle
outward. In particular, a learned gating mechanism determines whether to bypass
a symmetric span of central blocks based on the input, and a gated attention
mechanism prevents subsequent tokens from attending to skipped token positions.
Residual norms are controlled with a 'sandwich' or 'perilayernorm' scheme and
gate sparsity with an adaptive regularization loss. We had aimed to reduce
compute requirements for 'simpler' tokens and potentially foster an emergent
multi-level representational hierarchy but, at the scales investigated, our
approach does not achieve improvements in the trade-off between validation
cross-entropy and estimated FLOPs compared to dense baselines with fewer
layers. We release our code at https://github.com/tim-lawson/skip-middle.

</details>


### [91] [Unlasting: Unpaired Single-Cell Multi-Perturbation Estimation by Dual Conditional Diffusion Implicit Bridges](https://arxiv.org/abs/2506.21107)
*Changxi Chi,Jun Xia,Yufei Huang,Jingbo Zhou,Siyuan Li,Yunfan Liu,Chang Yu,Stan Z. Li*

Main category: cs.LG

TL;DR: 本文提出Unlasting框架解决单细胞扰动数据非配对问题，结合GRN信息和掩码机制，还引入新评估指标。


<details>
  <summary>Details</summary>
Motivation: 单细胞测序是破坏性过程，导致扰动前后数据非配对，现有方法处理非配对数据有缺陷。

Method: 提出基于Dual Diffusion Implicit Bridges (DDIB)的框架，整合基因调控网络(GRN)信息，引入掩码机制预测沉默基因，提出新评估指标。

Result: 提出的Unlasting模型克服非配对单细胞扰动数据问题，在GRN指导下增强对扰动的洞察，掩码模型提升生成质量。

Conclusion: 所提方法能有效处理非配对单细胞扰动数据，新评估指标能更好反映单细胞响应的内在异质性。

Abstract: Estimating single-cell responses across various perturbations facilitates the
identification of key genes and enhances drug screening, significantly boosting
experimental efficiency. However, single-cell sequencing is a destructive
process, making it impossible to capture the same cell's phenotype before and
after perturbation. Consequently, data collected under perturbed and
unperturbed conditions are inherently unpaired. Existing methods either attempt
to forcibly pair unpaired data using random sampling, or neglect the inherent
relationship between unperturbed and perturbed cells during the modeling. In
this work, we propose a framework based on Dual Diffusion Implicit Bridges
(DDIB) to learn the mapping between different data distributions, effectively
addressing the challenge of unpaired data. We further interpret this framework
as a form of data augmentation. We integrate gene regulatory network (GRN)
information to propagate perturbation signals in a biologically meaningful way,
and further incorporate a masking mechanism to predict silent genes, improving
the quality of generated profiles. Moreover, gene expression under the same
perturbation often varies significantly across cells, frequently exhibiting a
bimodal distribution that reflects intrinsic heterogeneity. To capture this, we
introduce a more suitable evaluation metric. We propose Unlasting, dual
conditional diffusion models that overcome the problem of unpaired single-cell
perturbation data and strengthen the model's insight into perturbations under
the guidance of the GRN, with a dedicated mask model designed to improve
generation quality by predicting silent genes. In addition, we introduce a
biologically grounded evaluation metric that better reflects the inherent
heterogeneity in single-cell responses.

</details>


### [92] [Robust Policy Switching for Antifragile Reinforcement Learning for UAV Deconfliction in Adversarial Environments](https://arxiv.org/abs/2506.21127)
*Deepak Kumar Panda,Weisi Guo*

Main category: cs.LG

TL;DR: 本文针对无人机导航中强化学习易受对抗攻击的问题，提出基于折扣汤普森采样切换机制的抗脆弱强化学习框架，经模拟验证其性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒强化学习方法在应对分布偏移时泛化能力有限，主要针对固定扰动设计，难以适应最优值分布的分布外偏移。

Method: 引入基于折扣汤普森采样（DTS）的切换机制，推导动作鲁棒策略集并建模为多臂老虎机问题，通过DTS选择策略；优化DTS以最小化分布偏移带来的遗憾。

Result: 广泛的数值模拟验证了该框架在复杂导航环境中，面对多种攻击时的有效性，相比传统方法，导航路径更短，无冲突导航轨迹率更高。

Conclusion: 所提出的抗脆弱强化学习框架能有效适应未见的对抗攻击，在复杂环境中性能优于传统鲁棒、非自适应强化学习方法。

Abstract: The increasing automation of navigation for unmanned aerial vehicles (UAVs)
has exposed them to adversarial attacks that exploit vulnerabilities in
reinforcement learning (RL) through sensor manipulation. Although existing
robust RL methods aim to mitigate such threats, their effectiveness has limited
generalization to out-of-distribution shifts from the optimal value
distribution, as they are primarily designed to handle fixed perturbation. To
address this limitation, this paper introduces an antifragile RL framework that
enhances adaptability to broader distributional shifts by incorporating a
switching mechanism based on discounted Thompson sampling (DTS). This mechanism
dynamically selects among multiple robust policies to minimize adversarially
induced state-action-value distribution shifts. The proposed approach first
derives a diverse ensemble of action robust policies by accounting for a range
of perturbations in the policy space. These policies are then modeled as a
multiarmed bandit (MAB) problem, where DTS optimally selects policies in
response to nonstationary Bernoulli rewards, effectively adapting to evolving
adversarial strategies. Theoretical framework has also been provided where by
optimizing the DTS to minimize the overall regrets due to distributional shift,
results in effective adaptation against unseen adversarial attacks thus
inducing antifragility. Extensive numerical simulations validate the
effectiveness of the proposed framework in complex navigation environments with
multiple dynamic three-dimensional obstacles and with stronger projected
gradient descent (PGD) and spoofing attacks. Compared to conventional robust,
non-adaptive RL methods, the antifragile approach achieves superior
performance, demonstrating shorter navigation path lengths and a higher rate of
conflict-free navigation trajectories compared to existing robust RL techniques

</details>


### [93] [Curriculum-Guided Antifragile Reinforcement Learning for Secure UAV Deconfliction under Observation-Space Attacks](https://arxiv.org/abs/2506.21129)
*Deepak Kumar Panda,Adolfo Perrusquia,Weisi Guo*

Main category: cs.LG

TL;DR: 针对强化学习策略在关键系统中易受分布外对抗攻击的问题，提出反脆弱强化学习框架，在无人机解冲突场景验证效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习策略在安全关键系统中易受分布外对抗攻击，导致价值估计下降、决策不安全或次优。

Method: 提出反脆弱强化学习框架，引入模拟攻击者增加扰动强度，用Wasserstein距离最小化进行迭代专家引导的评论家对齐。

Result: 在无人机解冲突场景中，反脆弱策略在投影梯度下降和GPS欺骗攻击下优于标准和鲁棒强化学习基线，累计奖励高15%，冲突事件少30%以上。

Conclusion: 反脆弱强化学习在不断演变威胁场景的环境中进行安全和弹性决策具有实践和理论可行性。

Abstract: Reinforcement learning (RL) policies deployed in safety-critical systems,
such as unmanned aerial vehicle (UAV) navigation in dynamic airspace, are
vulnerable to out-ofdistribution (OOD) adversarial attacks in the observation
space. These attacks induce distributional shifts that significantly degrade
value estimation, leading to unsafe or suboptimal decision making rendering the
existing policy fragile. To address this vulnerability, we propose an
antifragile RL framework designed to adapt against curriculum of incremental
adversarial perturbations. The framework introduces a simulated attacker which
incrementally increases the strength of observation-space perturbations which
enables the RL agent to adapt and generalize across a wider range of OOD
observations and anticipate previously unseen attacks. We begin with a
theoretical characterization of fragility, formally defining catastrophic
forgetting as a monotonic divergence in value function distributions with
increasing perturbation strength. Building on this, we define antifragility as
the boundedness of such value shifts and derive adaptation conditions under
which forgetting is stabilized. Our method enforces these bounds through
iterative expert-guided critic alignment using Wasserstein distance
minimization across incrementally perturbed observations. We empirically
evaluate the approach in a UAV deconfliction scenario involving dynamic 3D
obstacles. Results show that the antifragile policy consistently outperforms
standard and robust RL baselines when subjected to both projected gradient
descent (PGD) and GPS spoofing attacks, achieving up to 15% higher cumulative
reward and over 30% fewer conflict events. These findings demonstrate the
practical and theoretical viability of antifragile reinforcement learning for
secure and resilient decision-making in environments with evolving threat
scenarios.

</details>


### [94] [NaLaFormer: Norm-Aware Linear Attention for Transformer Models](https://arxiv.org/abs/2506.21137)
*Weikang Meng,Yadan Luo,Liangyu Huo,Yaowei Wang,Xin Li,Zheng Zhang*

Main category: cs.LG

TL;DR: 本文提出Norm - Aware Linear Attention机制（NaLaFormer）解决线性注意力机制存在的问题，实验表明其在视觉和语言任务上有性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有线性注意力机制忽略查询范数导致熵差，且抑制查询和键向量负值导致映射后内积交互缺失。

Method: 将查询和键矩阵解耦为范数和方向两个分量；提出查询范数感知核函数动态控制熵减；采用保范映射将角度矩阵元素投影为正值，利用余弦相似度抑制相反方向维度。

Result: NaLaFormer在视觉和语言任务上提升了性能，表现力和效率最多提高4.2%。

Conclusion: 提出的Norm - Aware Linear Attention机制有效，能提升模型在视觉和语言任务的表现。

Abstract: Linear attention has emerged as a viable alternative to softmax attention by
reducing complexity from quadratic to linear in sequence length. To preserve
two fundamental properties of softmax, non-negativity and entropy reduction,
current works employ various linearly separatable kernel functions with $L1$
normalization instead of softmax operator. However, query norms are neglected
by the normalization operation in linear attention, such degradation heavily
leads to an entropy gap. Meanwhile, existing works inhibit negative values of
query and key vectors resulting in a missing inner-product interactions after
being mapped. To address these dual challenges, we propose a novel Norm-Aware
Linear Attention mechanism serving to restore norm-guided dynamic spikiness and
recover kernel-perturbed norm distributions. Specifically, we first decouple
query and key matrices into two components: norm and direction, to achieve
norm-aware spikiness control and norm consistency, respectively. We
mathematically reveal that the extent of entropy reduction varies with the
query norm in softmax normalization, motivating a query-norm aware kernel
function for dynamic control over entropy reduction. Furthermore, to ensure
norm consistency and enforce non-negativity constraints, we employ a
norm-preserving mapping to project all elements of the angular matrix into
positive values, leveraging cosine similarity to inhibit dimensions with
opposite directions. We conduct extensive experiments demonstrating that the
NaLaFormer improves performance on vision and language tasks, enhancing both
expressiveness and efficiency by up to 4.2\%.

</details>


### [95] [DBConformer: Dual-Branch Convolutional Transformer for EEG Decoding](https://arxiv.org/abs/2506.21140)
*Ziwei Wang,Hongbin Wang,Tianwang Jia,Xingyi He,Siyang Li,Dongrui Wu*

Main category: cs.LG

TL;DR: 提出用于EEG解码的双分支卷积Transformer网络DBConformer，实验显示其性能优于基线模型，参数少且特征可解释。


<details>
  <summary>Details</summary>
Motivation: 现有CNN用于EEG解码难捕捉长时依赖和全局通道关系，CNN - Transformer混合模型集成特征不佳且忽视通道建模。

Method: 提出DBConformer，集成时间和空间Conformer分别建模长时依赖和通道交互，用轻量级通道注意力模块细化空间表征。

Result: 在七个数据集三种评估设置下，DBConformer性能优于10个基线模型，参数比高容量EEG Conformer少超八倍，特征可解释。

Conclusion: DBConformer性能和可解释性好，适合可靠且可解释的EEG解码。

Abstract: Electroencephalography (EEG)-based brain-computer interfaces (BCIs) transform
spontaneous/evoked neural activity into control commands for external
communication. While convolutional neural networks (CNNs) remain the mainstream
backbone for EEG decoding, their inherently short receptive field makes it
difficult to capture long-range temporal dependencies and global inter-channel
relationships. Recent CNN-Transformer (Conformers) hybrids partially address
this issue, but most adopt a serial design, resulting in suboptimal integration
of local and global features, and often overlook explicit channel-wise
modeling. To address these limitations, we propose DBConformer, a dual-branch
convolutional Transformer network tailored for EEG decoding. It integrates a
temporal Conformer to model long-range temporal dependencies and a spatial
Conformer to extract inter-channel interactions, capturing both temporal
dynamics and spatial patterns in EEG signals. A lightweight channel attention
module further refines spatial representations by assigning data-driven
importance to EEG channels. Extensive experiments on five motor imagery (MI)
datasets and two seizure detection datasets under three evaluation settings
demonstrate that DBConformer consistently outperforms 10 competitive baseline
models, with over eight times fewer parameters than the high-capacity EEG
Conformer baseline. Further, the visualization results confirm that the
features extracted by DBConformer are physiologically interpretable and aligned
with sensorimotor priors in MI. The superior performance and interpretability
of DBConformer make it reliable for robust and explainable EEG decoding. Code
is publicized at https://github.com/wzwvv/DBConformer.

</details>


### [96] [Generative Adversarial Evasion and Out-of-Distribution Detection for UAV Cyber-Attacks](https://arxiv.org/abs/2506.21142)
*Deepak Kumar Panda,Weisi Guo*

Main category: cs.LG

TL;DR: 文章引入基于cGAN的框架生成规避入侵检测系统（IDS）的对抗样本，并用CVAE检测扰动，结果显示CVAE优于传统检测器，强调先进概率建模对强化IDS的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着无人机融入民用空域，传统异常检测方法难以识别新威胁，现有OOD检测器难以区分隐秘对抗攻击和真实OOD事件，需要提升IDS能力。

Method: 设计多类IDS分类器，用cGAN基于此分类器生成对抗样本并迭代优化，用CVAE检测扰动。

Result: CVAE基于遗憾分数在识别隐秘对抗威胁方面显著优于传统基于马氏距离的检测器。

Conclusion: 强调先进概率建模对加强IDS抵御自适应、基于生成模型的网络入侵能力的重要性。

Abstract: The growing integration of UAVs into civilian airspace underscores the need
for resilient and intelligent intrusion detection systems (IDS), as traditional
anomaly detection methods often fail to identify novel threats. A common
approach treats unfamiliar attacks as out-of-distribution (OOD) samples;
however, this leaves systems vulnerable when mitigation is inadequate.
Moreover, conventional OOD detectors struggle to distinguish stealthy
adversarial attacks from genuine OOD events. This paper introduces a
conditional generative adversarial network (cGAN)-based framework for crafting
stealthy adversarial attacks that evade IDS mechanisms. We first design a
robust multi-class IDS classifier trained on benign UAV telemetry and known
cyber-attacks, including Denial of Service (DoS), false data injection (FDI),
man-in-the-middle (MiTM), and replay attacks. Using this classifier, our cGAN
perturbs known attacks to generate adversarial samples that misclassify as
benign while retaining statistical resemblance to OOD distributions. These
adversarial samples are iteratively refined to achieve high stealth and success
rates. To detect such perturbations, we implement a conditional variational
autoencoder (CVAE), leveraging negative log-likelihood to separate adversarial
inputs from authentic OOD samples. Comparative evaluation shows that CVAE-based
regret scores significantly outperform traditional Mahalanobis distance-based
detectors in identifying stealthy adversarial threats. Our findings emphasize
the importance of advanced probabilistic modeling to strengthen IDS
capabilities against adaptive, generative-model-based cyber intrusions.

</details>


### [97] [Personalized Federated Learning via Dual-Prompt Optimization and Cross Fusion](https://arxiv.org/abs/2506.21144)
*Yuguang Zhang,Kuangpu Guo,Zhihe Lu,Yunbo Wang,Jian Liang*

Main category: cs.LG

TL;DR: 提出基于双提示学习和交叉融合的个性化联邦学习框架pFedDC，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临数据、计算和通信异质性挑战，现有联邦提示学习方法仅依赖文本提示且忽略联合标签-领域分布偏移。

Method: 提出pFedDC框架，每个客户端维护视觉和语言模态的全局和局部提示，设计交叉融合模块自适应集成不同级别的提示。

Result: 在九个具有不同异质性类型的数据集上的广泛实验表明，pFedDC始终优于现有方法。

Conclusion: pFedDC是解决联邦学习异质性问题的有效方法。

Abstract: Federated learning (FL) enables collaborative model training across
decentralized clients without sharing local data, but is challenged by
heterogeneity in data, computation, and communication. Pretrained
vision-language models (VLMs), with their strong generalization and lightweight
tuning via prompts, offer a promising solution. However, existing federated
prompt-learning methods rely only on text prompts and overlook joint
label-domain distribution shifts. In this paper, we propose a personalized FL
framework based on dual-prompt learning and cross fusion, termed pFedDC.
Specifically, each client maintains both global and local prompts across vision
and language modalities: global prompts capture common knowledge shared across
the federation, while local prompts encode client-specific semantics and domain
characteristics. Meanwhile, a cross-fusion module is designed to adaptively
integrate prompts from different levels, enabling the model to generate
personalized representations aligned with each client's unique data
distribution. Extensive experiments across nine datasets with various types of
heterogeneity show that pFedDC consistently outperforms state-of-the-art
methods.

</details>


### [98] [Diverse Mini-Batch Selection in Reinforcement Learning for Efficient Chemical Exploration in de novo Drug Design](https://arxiv.org/abs/2506.21158)
*Hampus Gummesson Svensson,Ola Engkvist,Jon Paul Janet,Christian Tyrchan,Morteza Haghir Chehreghani*

Main category: cs.LG

TL;DR: 本文针对强化学习引入多样化小批量选择，用行列式点过程完成该任务，在药物发现场景实验表明能提高解的多样性和质量。


<details>
  <summary>Details</summary>
Motivation: 在现实应用中评估实例优劣成本高、耗时长，强化学习中需评估新交互实例，充分探索很关键，从多样化小批量学习可缓解模式崩溃。

Method: 引入多样化小批量选择用于强化学习，使用行列式点过程完成该任务，并在药物发现场景进行实验。

Result: 通过对三个成熟分子生成预言机进行综合评估，该多样化小批量选择框架能大幅提高解的多样性，同时获得高质量解。

Conclusion: 该框架能有效提升解的多样性和质量，在药物发现中可能更快满足未满足的药物需求。

Abstract: In many real-world applications, evaluating the goodness of instances is
often costly and time-consuming, e.g., human feedback and physics simulations,
in contrast to proposing new instances. In particular, this is even more
critical in reinforcement learning, as new interactions with the environment
(i.e., new instances) need to be evaluated to provide a reward signal to learn
from. As sufficient exploration is crucial, learning from a diverse mini-batch
can have a large impact and help mitigate mode collapse. In this paper, we
introduce diverse mini-batch selection for reinforcement learning and propose
to use determinantal point processes for this task. We study this framework in
the context of a real-world problem, namely drug discovery. We experimentally
study how our proposed framework can improve the effectiveness of chemical
exploration in de novo drug design, where finding diverse and high-quality
solutions is essential. We conduct a comprehensive evaluation with three
well-established molecular generation oracles over numerous generative steps.
Our experiments conclude that our diverse mini-batch selection framework can
substantially improve the diversity of the solutions, while still obtaining
solutions of high quality. In drug discovery, such outcome can potentially lead
to fulfilling unmet medication needs faster.

</details>


### [99] [Artificial Delegates Resolve Fairness Issues in Perpetual Voting with Partial Turnout](https://arxiv.org/abs/2506.21186)
*Apurva Shah,Axel Abels,Ann Nowé,Tom Lenaerts*

Main category: cs.LG

TL;DR: 研究将人工代表集成到永续投票系统，发现其可减轻缺席投票对公平性的影响。


<details>
  <summary>Details</summary>
Motivation: 现有永续投票规则依赖全员参与和完整批准信息，与实际部分投票情况不符，需解决该问题。

Method: 研究将偏好学习的人工代表集成到永续投票系统，考察不同投票方法下缺席投票对公平性和代表性的影响。

Result: 缺席投票显著影响公平性，人工代表能可靠减轻这些影响。

Conclusion: 人工代表可增强永续投票系统在不同场景下的稳健性。

Abstract: Perpetual voting addresses fairness in sequential collective decision-making
by evaluating representational equity over time. However, existing perpetual
voting rules rely on full participation and complete approval information,
assumptions that rarely hold in practice, where partial turnout is the norm. In
this work, we study the integration of Artificial Delegates,
preference-learning agents trained to represent absent voters, into perpetual
voting systems. We examine how absenteeism affects fairness and
representativeness under various voting methods and evaluate the extent to
which Artificial Delegates can compensate for missing participation. Our
findings indicate that while absenteeism significantly affects fairness,
Artificial Delegates reliably mitigate these effects and enhance robustness
across diverse scenarios.

</details>


### [100] [Complexity-aware fine-tuning](https://arxiv.org/abs/2506.21220)
*Andrey Goncharov,Daniil Vyazhev,Petr Sychev,Edvard Khalafyan,Alexey Zaytsev*

Main category: cs.LG

TL;DR: 提出基于熵识别复杂数据进行高效微调的蓝图，在小模型验证其优于标准SFT方法且省数据，还公布代码数据。


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型常通过SFT微调提升特定领域性能，蒸馏大模型思维链成本高，需更高效微调方法。

Method: 用单令牌答案熵将训练数据分类，通过SFT和蒸馏微调大语言模型。

Result: 所提方法显著优于标准SFT方法（平均准确率0.55 vs 0.43），用少62%数据达到与蒸馏相当性能（平均准确率均为0.55）。

Conclusion: 提出的高效微调蓝图有效，代码和数据公布利于后续研究。

Abstract: General-purpose Large Language Models (LLMs) are frequently fine-tuned
through supervised fine-tuning (SFT) to enhance performance in specific
domains. Better results can be achieved by distilling the chain-of-thought of a
larger model at the cost of numerous expensive calls and a much greater amount
of data. We propose a novel blueprint for efficient fine-tuning that uses
reasoning only for complex data identified by entropy. Specifically, across two
small open models ($\approx 3B$) we split the training data into complexity
categories by a single token answer entropy (ROC AUC $0.73$), fine-tune large
language models (LLMs) via SFT and distillation, and show that our pipeline
significantly outperforms the standard SFT approach ($0.55$ vs $0.43$ average
accuracy) and provides comparable with distillation performance while using
$62\%$ less data ($0.55$ average accuracy for both). We publish our code and
data to facilitate further research in this direction.

</details>


### [101] [Zero-Shot Learning for Obsolescence Risk Forecasting](https://arxiv.org/abs/2506.21240)
*Elie Saad,Aya Mrabah,Mariem Besbes,Marc Zolghadri,Victor Czmil,Claude Baron,Vincent Bourgeois*

Main category: cs.LG

TL;DR: 本文提出用零样本学习和大语言模型预测组件过时风险，解决数据不足问题，经实际数据集验证有效，还强调选对模型重要性。


<details>
  <summary>Details</summary>
Motivation: 电子组件过时给相关行业带来成本增加和系统安全可用性受扰等挑战，且因缺乏可靠数据，准确预测过时风险困难。

Method: 采用零样本学习（ZSL）结合大语言模型（LLMs），利用表格数据集的领域特定知识进行过时风险预测。

Result: 应用于两个真实世界数据集，该方法能有效进行风险预测；对四个LLMs的比较评估强调了为特定预测任务选择合适模型的重要性。

Conclusion: 所提方法能解决数据限制问题，有效预测过时风险，且选择合适模型对预测任务很关键。

Abstract: Component obsolescence poses significant challenges in industries reliant on
electronic components, causing increased costs and disruptions in the security
and availability of systems. Accurate obsolescence risk prediction is essential
but hindered by a lack of reliable data. This paper proposes a novel approach
to forecasting obsolescence risk using zero-shot learning (ZSL) with large
language models (LLMs) to address data limitations by leveraging
domain-specific knowledge from tabular datasets. Applied to two real-world
datasets, the method demonstrates effective risk prediction. A comparative
evaluation of four LLMs underscores the importance of selecting the right model
for specific forecasting tasks.

</details>


### [102] [DiLoCoX: A Low-Communication Large-Scale Training Framework for Decentralized Cluster](https://arxiv.org/abs/2506.21263)
*Ji Qi,WenPeng Zhu,Li Li,Ming Wu,YingJun Wu,Wu He,Xun Gao,Jason Zeng,Michael Heinrich*

Main category: cs.LG

TL;DR: 提出DiLoCoX框架用于低通信大规模去中心化集群训练，提升参数规模和预训练速度，能在1Gbps网络预训练107B模型，比AllReduce有357倍加速。


<details>
  <summary>Details</summary>
Motivation: 解决在慢速网络和去中心化集群上训练超1000亿参数模型的问题，分布式训练基础模型对通信要求高，依赖中心化集群。

Method: 结合Pipeline Parallelism、Dual Optimizer Policy、One - Step - Delay Overlap of Communication and Local Training和Adaptive Gradient Compression Scheme。

Result: 能在1Gbps网络预训练107B基础模型，比vanilla AllReduce分布式训练有357倍加速，模型收敛下降可忽略。

Conclusion: DiLoCoX是首个成功应用于超1000亿参数模型的去中心化训练框架。

Abstract: The distributed training of foundation models, particularly large language
models (LLMs), demands a high level of communication. Consequently, it is
highly dependent on a centralized cluster with fast and reliable interconnects.
Can we conduct training on slow networks and thereby unleash the power of
decentralized clusters when dealing with models exceeding 100 billion
parameters? In this paper, we propose DiLoCoX, a low-communication large-scale
decentralized cluster training framework. It combines Pipeline Parallelism with
Dual Optimizer Policy, One-Step-Delay Overlap of Communication and Local
Training, and an Adaptive Gradient Compression Scheme. This combination
significantly improves the scale of parameters and the speed of model
pre-training. We justify the benefits of one-step-delay overlap of
communication and local training, as well as the adaptive gradient compression
scheme, through a theoretical analysis of convergence. Empirically, we
demonstrate that DiLoCoX is capable of pre-training a 107B foundation model
over a 1Gbps network. Compared to vanilla AllReduce, DiLoCoX can achieve a 357x
speedup in distributed training while maintaining negligible degradation in
model convergence. To the best of our knowledge, this is the first
decentralized training framework successfully applied to models with over 100
billion parameters.

</details>


### [103] [Improved seeding strategies for k-means and k-GMM](https://arxiv.org/abs/2506.21291)
*Guillaume Carrière,Frédéric Cazals*

Main category: cs.LG

TL;DR: 重新审视k - means聚类和k - GMM的随机种子技术，提出新初始化方法，实验显示有改进，对实际应用和理论分析有价值。


<details>
  <summary>Details</summary>
Motivation: 改进k - means聚类和k - GMM的随机种子技术，以获得更好的聚类效果。

Method: 形式化随机种子技术的三个关键要素，利用前瞻原则和多遍策略提出新初始化方法。

Result: 实验表明新方法在最终指标上比经典方法有恒定因子改进，尤其在k - means上优于多交换策略。

Conclusion: 新的种子方法有望成为标准技术，其形式化打开了新的分析途径。

Abstract: We revisit the randomized seeding techniques for k-means clustering and k-GMM
(Gaussian Mixture model fitting with Expectation-Maximization), formalizing
their three key ingredients: the metric used for seed sampling, the number of
candidate seeds, and the metric used for seed selection. This analysis yields
novel families of initialization methods exploiting a lookahead
principle--conditioning the seed selection to an enhanced coherence with the
final metric used to assess the algorithm, and a multipass strategy to tame
down the effect of randomization.
  Experiments show a consistent constant factor improvement over classical
contenders in terms of the final metric (SSE for k-means, log-likelihood for
k-GMM), at a modest overhead. In particular, for k-means, our methods improve
on the recently designed multi-swap strategy, which was the first one to
outperform the greedy k-means++ seeding.
  Our experimental analysis also shed light on subtle properties of k-means
often overlooked, including the (lack of) correlations between the SSE upon
seeding and the final SSE, the variance reduction phenomena observed in
iterative seeding methods, and the sensitivity of the final SSE to the pool
size for greedy methods.
  Practically, our most effective seeding methods are strong candidates to
become one of the--if not the--standard techniques. From a theoretical
perspective, our formalization of seeding opens the door to a new line of
analytical approaches.

</details>


### [104] [Latent Prototype Routing: Achieving Near-Perfect Load Balancing in Mixture-of-Experts](https://arxiv.org/abs/2506.21328)
*Jiajie Yang*

Main category: cs.LG

TL;DR: 本文提出潜在原型路由（LPR）框架解决混合专家（MoE）架构负载不均衡问题，实验显示其能实现近乎完美的负载均衡。


<details>
  <summary>Details</summary>
Motivation: 当前MoE系统存在严重的负载不均衡问题，导致模型容量和计算资源利用率低。

Method: 从聚类角度重新审视专家路由，提出LPR路由框架。

Result: 在多个开源MoE模型上实验，LPR平均将专家负载的基尼系数从0.70降至0.035，将最小 - 最大专家负载比从1e - 6提高到0.70。

Conclusion: LPR能在不影响下游性能的前提下，促进专家均衡利用，实现近乎完美的负载均衡。

Abstract: Mixture-of-Experts (MoE) architectures have emerged as a key strategy for
scaling large language models (LLMs) efficiently. However, current MoE systems
suffer from severe load imbalance, where only a small subset of experts is
consistently activated during training and inference, leading to significant
underutilization of model capacity and computational resources. In this work,
we revisit expert routing through a clustering perspective and propose Latent
Prototype Routing (LPR), a novel routing framework that generalizes existing
approaches while promoting balanced expert utilization without compromising
downstream performance. Extensive experiments across multiple open-source MoE
models -- including DeepSeek-V3, Qwen3-MoE, and Mixtral -- demonstrate that LPR
reduces the Gini coefficient of expert load from 0.70 to 0.035 on average,
improves the min-max expert load ratio from 1e-6 to 0.70, achieving
near-perfect load balancing.

</details>


### [105] [AGTCNet: A Graph-Temporal Approach for Principled Motor Imagery EEG Classification](https://arxiv.org/abs/2506.21338)
*Galvin Brice S. Lim,Brian Godwin S. Lim,Argel A. Bandala,John Anthony C. Jose,Timothy Scott C. Chu,Edwin Sybingco*

Main category: cs.LG

TL;DR: 本文提出用于运动想象脑电（MI - EEG）分类的AGTCNet模型，相比现有模型表现更优，尺寸小、推理快，在多个数据集上准确率高。


<details>
  <summary>Details</summary>
Motivation: 开发跨个体和跨会话不变的脑机接口（BCI）系统存在挑战，现有方法难以捕捉多通道脑电信号的复杂时空依赖关系。

Method: 引入AGTCNet模型，利用脑电电极的地形配置作为归纳偏置，并集成图卷积注意力网络（GCAT）联合学习脑电信号的时空表征。

Result: 在BCI Competition IV Dataset 2a和EEG Motor Movement/Imagery Dataset上，AGTCNet模型尺寸减小49.87%，推理时间加快64.65%，在多个分类任务上取得较高准确率。

Conclusion: AGTCNet模型有效且实用，适用于BCI部署。

Abstract: Brain-computer interface (BCI) technology utilizing electroencephalography
(EEG) marks a transformative innovation, empowering motor-impaired individuals
to engage with their environment on equal footing. Despite its promising
potential, developing subject-invariant and session-invariant BCI systems
remains a significant challenge due to the inherent complexity and variability
of neural activity across individuals and over time, compounded by EEG hardware
constraints. While prior studies have sought to develop robust BCI systems,
existing approaches remain ineffective in capturing the intricate
spatiotemporal dependencies within multichannel EEG signals. This study
addresses this gap by introducing the attentive graph-temporal convolutional
network (AGTCNet), a novel graph-temporal model for motor imagery EEG (MI-EEG)
classification. Specifically, AGTCNet leverages the topographic configuration
of EEG electrodes as an inductive bias and integrates graph convolutional
attention network (GCAT) to jointly learn expressive spatiotemporal EEG
representations. The proposed model significantly outperformed existing MI-EEG
classifiers, achieving state-of-the-art performance while utilizing a compact
architecture, underscoring its effectiveness and practicality for BCI
deployment. With a 49.87% reduction in model size, 64.65% faster inference
time, and shorter input EEG signal, AGTCNet achieved a moving average accuracy
of 66.82% for subject-independent classification on the BCI Competition IV
Dataset 2a, which further improved to 82.88% when fine-tuned for
subject-specific classification. On the EEG Motor Movement/Imagery Dataset,
AGTCNet achieved moving average accuracies of 64.14% and 85.22% for 4-class and
2-class subject-independent classifications, respectively, with further
improvements to 72.13% and 90.54% for subject-specific classifications.

</details>


### [106] [DynamicBench: Evaluating Real-Time Report Generation in Large Language Models](https://arxiv.org/abs/2506.21343)
*Jingyao Li,Hao Sun,Zile Qiao,Yong Jiang,Pengjun Xie,Fei Huang,Hong Xu,Jiaya Jia*

Main category: cs.LG

TL;DR: 提出DynamicBench基准评估大语言模型处理实时数据能力，引入先进报告生成系统，实验效果佳，代码数据将公开。


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型基准无法满足当代应用实时信息处理的动态需求，需新基准。

Method: 设计DynamicBench，采用双路径检索管道，结合网络搜索与本地报告数据库，评估不同场景下模型能力，引入先进报告生成系统。

Result: 方法达到了最先进水平，在无文档和有文档辅助场景下分别比GPT4o高7.0%和5.8%。

Conclusion: 提出的方法有效，代码和数据将公开。

Abstract: Traditional benchmarks for large language models (LLMs) typically rely on
static evaluations through storytelling or opinion expression, which fail to
capture the dynamic requirements of real-time information processing in
contemporary applications. To address this limitation, we present DynamicBench,
a benchmark designed to evaluate the proficiency of LLMs in storing and
processing up-to-the-minute data. DynamicBench utilizes a dual-path retrieval
pipeline, integrating web searches with local report databases. It necessitates
domain-specific knowledge, ensuring accurate responses report generation within
specialized fields. By evaluating models in scenarios that either provide or
withhold external documents, DynamicBench effectively measures their capability
to independently process recent information or leverage contextual
enhancements. Additionally, we introduce an advanced report generation system
adept at managing dynamic information synthesis. Our experimental results
confirm the efficacy of our approach, with our method achieving
state-of-the-art performance, surpassing GPT4o in document-free and
document-assisted scenarios by 7.0% and 5.8%, respectively. The code and data
will be made publicly available.

</details>


### [107] [Lipschitz Bounds for Persistent Laplacian Eigenvalues under One-Simplex Insertions](https://arxiv.org/abs/2506.21352)
*Le Vu Anh,Mehmet Dik,Nguyen Viet Anh*

Main category: cs.LG

TL;DR: 本文证明了单单纯形插入时上持久拉普拉斯算子特征值的一致Lipschitz界，为谱拓扑数据分析提供特征值级鲁棒性保证。


<details>
  <summary>Details</summary>
Motivation: 此前虽有持久拉普拉斯算子全局代数稳定性结果，但单单纯形插入时单个特征值的精确变化未知，而下游工具依赖这些特征值。

Method: 证明一致Lipschitz界。

Result: 插入一个单纯形后，每个上持久拉普拉斯特征值的变化至多为该单纯形边界欧几里得范数的两倍，与过滤尺度和复形大小无关。

Conclusion: 为谱拓扑数据分析提供首个特征值级鲁棒性保证，确保谱特征在局部更新下稳定，能在动态数据中进行可靠误差控制。

Abstract: Persistent Laplacians are matrix operators that track how the shape and
structure of data transform across scales and are popularly adopted in biology,
physics, and machine learning. Their eigenvalues are concise descriptors of
geometric and topological features in a filtration. Although earlier work
established global algebraic stability for these operators, the precise change
in a single eigenvalue when one simplex, such as a vertex, edge, or triangle,
is added has remained unknown. This is important because downstream tools,
including heat-kernel signatures and spectral neural networks, depend directly
on these eigenvalues. We close this gap by proving a uniform Lipschitz bound:
after inserting one simplex, every up-persistent Laplacian eigenvalue can vary
by at most twice the Euclidean norm of that simplex's boundary, independent of
filtration scale and complex size. This result delivers the first
eigenvalue-level robustness guarantee for spectral topological data analysis.
It guarantees that spectral features remain stable under local updates and
enables reliable error control in dynamic data settings.

</details>


### [108] [SMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context Learning](https://arxiv.org/abs/2506.21355)
*Melanie Rieff,Maya Varma,Ossian Rabow,Subathra Adithan,Julie Kim,Ken Chang,Hannah Lee,Nidhi Rohatgi,Christian Bluethgen,Mohamed S. Muneer,Jean-Benoit Delbrouck,Michael Moor*

Main category: cs.LG

TL;DR: 本文提出医学多模态上下文学习基准SMMILE及增强版SMMILE++，评估15个多模态大语言模型，发现其在医学任务多模态上下文学习能力存在局限和偏差。


<details>
  <summary>Details</summary>
Motivation: 多模态上下文学习在医学领域潜力大但研究不足，多模态大语言模型从上下文学习多模态任务的能力未知。

Method: 引入专家驱动的医学多模态上下文学习基准SMMILE及SMMILE++，对15个多模态大语言模型进行全面评估。

Result: 多数模型在医学任务多模态上下文学习能力中等到差，开放评估中上下文学习提升有限，易受无关示例影响，示例排序有近期偏差。

Conclusion: 当前多模态大语言模型在从上下文学习多模态医学任务时存在关键局限和偏差。

Abstract: Multimodal in-context learning (ICL) remains underexplored despite
significant potential for domains such as medicine. Clinicians routinely
encounter diverse, specialized tasks requiring adaptation from limited
examples, such as drawing insights from a few relevant prior cases or
considering a constrained set of differential diagnoses. While multimodal large
language models (MLLMs) have shown advances in medical visual question
answering (VQA), their ability to learn multimodal tasks from context is
largely unknown. We introduce SMMILE, the first expert-driven multimodal ICL
benchmark for medical tasks. Eleven medical experts curated problems, each
including a multimodal query and multimodal in-context examples as task
demonstrations. SMMILE encompasses 111 problems (517 question-image-answer
triplets) covering 6 medical specialties and 13 imaging modalities. We further
introduce SMMILE++, an augmented variant with 1038 permuted problems. A
comprehensive evaluation of 15 MLLMs demonstrates that most models exhibit
moderate to poor multimodal ICL ability in medical tasks. In open-ended
evaluations, ICL contributes only 8% average improvement over zero-shot on
SMMILE and 9.4% on SMMILE++. We observe a susceptibility for irrelevant
in-context examples: even a single noisy or irrelevant example can degrade
performance by up to 9.5%. Moreover, example ordering exhibits a recency bias,
i.e., placing the most relevant example last can lead to substantial
performance improvements by up to 71%. Our findings highlight critical
limitations and biases in current MLLMs when learning multimodal medical tasks
from context.

</details>


### [109] [rQdia: Regularizing Q-Value Distributions With Image Augmentation](https://arxiv.org/abs/2506.21367)
*Sam Lerman,Jing Bi*

Main category: cs.LG

TL;DR: rQdia通过增强图像正则化Q值分布，提升多种算法在不同任务中的表现，推动无模型连续控制超越状态编码基线。


<details>
  <summary>Details</summary>
Motivation: 提升像素级深度强化学习的性能，提高样本效率和长期训练效果。

Method: 使用简单辅助损失，通过均方误差（MSE）使Q值分布相等。

Result: 在MuJoCo连续控制套件和Atari游戏环境中提升了DrQ、SAC和Data - Efficient Rainbow等算法的表现。

Conclusion: rQdia的加入推动了无模型连续控制从像素层面超越状态编码基线。

Abstract: rQdia regularizes Q-value distributions with augmented images in pixel-based
deep reinforcement learning. With a simple auxiliary loss, that equalizes these
distributions via MSE, rQdia boosts DrQ and SAC on 9/12 and 10/12 tasks
respectively in the MuJoCo Continuous Control Suite from pixels, and
Data-Efficient Rainbow on 18/26 Atari Arcade environments. Gains are measured
in both sample efficiency and longer-term training. Moreover, the addition of
rQdia finally propels model-free continuous control from pixels over the state
encoding baseline.

</details>


### [110] [MAx-DNN: Multi-Level Arithmetic Approximation for Energy-Efficient DNN Hardware Accelerators](https://arxiv.org/abs/2506.21371)
*Vasileios Leon,Georgios Makris,Sotirios Xydis,Kiamal Pekmestzi,Dimitrios Soudris*

Main category: cs.LG

TL;DR: 本文针对低功耗DNN计算，结合硬件近似技术探索DNN工作负载细粒度误差弹性，实现更高能效，在CIFAR - 10数据集上用ResNet - 8模型评估，有良好效果。


<details>
  <summary>Details</summary>
Motivation: 当前DNN发展迅速，目标是实现低功耗DNN计算，提高能源效率。

Method: 利用先进的ROUP近似乘法器，按层、滤波器和内核级方法在网络中系统探索其细粒度分布，并研究其对准确性和能量的影响。

Result: 与基线量化模型相比，所提方案最多可实现54%的能量增益，代价是最多4%的精度损失；与现有DNN近似方法相比，可实现2倍能量增益且精度更高。

Conclusion: 结合细粒度误差弹性和硬件近似技术可有效提高DNN计算的能源效率。

Abstract: Nowadays, the rapid growth of Deep Neural Network (DNN) architectures has
established them as the defacto approach for providing advanced Machine
Learning tasks with excellent accuracy. Targeting low-power DNN computing, this
paper examines the interplay of fine-grained error resilience of DNN workloads
in collaboration with hardware approximation techniques, to achieve higher
levels of energy efficiency. Utilizing the state-of-the-art ROUP approximate
multipliers, we systematically explore their fine-grained distribution across
the network according to our layer-, filter-, and kernel-level approaches, and
examine their impact on accuracy and energy. We use the ResNet-8 model on the
CIFAR-10 dataset to evaluate our approximations. The proposed solution delivers
up to 54% energy gains in exchange for up to 4% accuracy loss, compared to the
baseline quantized model, while it provides 2x energy gains with better
accuracy versus the state-of-the-art DNN approximations.

</details>


### [111] [Pay Attention to Small Weights](https://arxiv.org/abs/2506.21374)
*Chao Zhou,Tom Jacobs,Advait Gadhikar,Rebekka Burkholz*

Main category: cs.LG

TL;DR: 微调预训练大模型资源消耗大，提出NANOADAM仅更新小幅度权重，有多项优势并在多任务验证


<details>
  <summary>Details</summary>
Motivation: 微调预训练大模型资源消耗大，需限制训练参数子集以缓解该问题

Method: 分析微调时梯度和权重关系，提出NANOADAM动态更新小幅度权重

Result: 在NLP和视觉任务实验中，NANOADAM允许使用更大学习率，有更好泛化性能

Conclusion: NANOADAM是一种有效微调方法，能减少资源消耗、降低灾难性遗忘风险且有良好泛化性

Abstract: Finetuning large pretrained neural networks is known to be
resource-intensive, both in terms of memory and computational cost. To mitigate
this, a common approach is to restrict training to a subset of the model
parameters. By analyzing the relationship between gradients and weights during
finetuning, we observe a notable pattern: large gradients are often associated
with small-magnitude weights. This correlation is more pronounced in finetuning
settings than in training from scratch. Motivated by this observation, we
propose NANOADAM, which dynamically updates only the small-magnitude weights
during finetuning and offers several practical advantages: first, this
criterion is gradient-free -- the parameter subset can be determined without
gradient computation; second, it preserves large-magnitude weights, which are
likely to encode critical features learned during pretraining, thereby reducing
the risk of catastrophic forgetting; thirdly, it permits the use of larger
learning rates and consistently leads to better generalization performance in
experiments. We demonstrate this for both NLP and vision tasks.

</details>


### [112] [Temporal-Aware Graph Attention Network for Cryptocurrency Transaction Fraud Detection](https://arxiv.org/abs/2506.21382)
*Zhi Zheng,Bochuan Zhou,Yuping Song*

Main category: cs.LG

TL;DR: 针对加密货币交易欺诈检测问题，提出ATGAT模型，实验显示性能提升，设计原则可推广到其他任务。


<details>
  <summary>Details</summary>
Motivation: 解决加密货币交易欺诈检测中交易模式复杂和类别不平衡的问题，弥补传统方法在捕捉交易网络中时间和结构依赖方面的不足。

Method: 提出ATGAT模型，包括先进的时间嵌入模块、时间感知三重注意力机制和加权BCE损失函数。

Result: 在Elliptic++数据集上，ATGAT的AUC达到0.9130，比XGBoost、GCN和标准GAT分别提高9.2%、12.0%和10.0%。

Conclusion: 验证了时间感知和三重注意力机制对图神经网络的增强效果，为金融机构提供可靠工具，设计原则可推广到其他时间图异常检测任务。

Abstract: Cryptocurrency transaction fraud detection faces the dual challenges of
increasingly complex transaction patterns and severe class imbalance.
Traditional methods rely on manual feature engineering and struggle to capture
temporal and structural dependencies in transaction networks. This paper
proposes an Augmented Temporal-aware Graph Attention Network (ATGAT) that
enhances detection performance through three modules: (1) designing an advanced
temporal embedding module that fuses multi-scale time difference features with
periodic position encoding; (2) constructing a temporal-aware triple attention
mechanism that jointly optimizes structural, temporal, and global context
attention; (3) employing weighted BCE loss to address class imbalance.
Experiments on the Elliptic++ cryptocurrency dataset demonstrate that ATGAT
achieves an AUC of 0.9130, representing a 9.2% improvement over the best
traditional method XGBoost, 12.0% over GCN, and 10.0% over standard GAT. This
method not only validates the enhancement effect of temporal awareness and
triple attention mechanisms on graph neural networks, but also provides
financial institutions with more reliable fraud detection tools, with its
design principles generalizable to other temporal graph anomaly detection
tasks.

</details>


### [113] [Early Stopping Tabular In-Context Learning](https://arxiv.org/abs/2506.21387)
*Jaris Küken,Lennart Purucker,Frank Hutter*

Main category: cs.LG

TL;DR: 提出提前停止上下文学习过程以解决表格基础模型推理成本高的问题，实验证明该方法可加速推理且性能损失小。


<details>
  <summary>Details</summary>
Motivation: 表格基础模型推理成本高，尤其是处理大数据集时，需降低推理成本。

Method: 在每个Transformer编码器层后动态评估是否停止上下文学习，停止后用预训练的逐层解码器解码嵌入。

Result: 在34个小分类任务中加速推理达1.3倍，性能损失可忽略；在5个大分类任务中加速达2.2倍。

Conclusion: 提前退出是提高表格上下文学习效率的有效实用策略。

Abstract: Tabular foundation models have shown strong performance across various
tabular learning tasks via in-context learning, offering robust generalization
without any downstream finetuning. However, their inference-time costs remain
high, particularly for larger datasets. To address this, we propose
early-stopping the in-context learning process. We achieve this by dynamically
evaluating whether to stop in-context learning after each Transformer encoder
layer. Once stopped, we decode the embedding using a pre-trained layer-wise
decoder. Experiments across 34 small classification tasks size show that early
stopping in-context learning accelerates inference by up to x1.3 with
negligible degradation in predictive performance. To assess scalability, we
further evaluate our method on five larger classification tasks, achieving
speedups of up to x2.2. Our results demonstrate the potential of early exiting
as an effective and practical strategy for improving the efficiency of tabular
in-context learning.

</details>


### [114] [Scalable Bayesian Low-Rank Adaptation of Large Language Models via Stochastic Variational Subspace Inference](https://arxiv.org/abs/2506.21408)
*Colin Samplawski,Adam D. Cobb,Manoj Acharya,Ramneet Kaur,Susmit Jha*

Main category: cs.LG

TL;DR: 提出ScalaBL方法对大语言模型进行不确定性量化，以低额外参数实现有竞争力性能并可扩展到更大模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在幻觉和校准不佳问题，不确定性量化很重要，现有贝叶斯深度学习方法在扩展到更大模型时存在问题。

Method: 在r维子空间进行贝叶斯推理，将LoRA参数用作投影矩阵，用随机变分推理学习参数。

Result: 仅需约1000个额外参数就可实现与现有方法有竞争力的性能，能扩展到目前最大的贝叶斯大语言模型。

Conclusion: ScalaBL方法能有效进行大语言模型的不确定性量化，且在扩展性和参数效率上表现良好。

Abstract: Despite their widespread use, large language models (LLMs) are known to
hallucinate incorrect information and be poorly calibrated. This makes the
uncertainty quantification of these models of critical importance, especially
in high-stakes domains, such as autonomy and healthcare. Prior work has made
Bayesian deep learning-based approaches to this problem more tractable by
performing inference over the low-rank adaptation (LoRA) parameters of a
fine-tuned model. While effective, these approaches struggle to scale to larger
LLMs due to requiring further additional parameters compared to LoRA. In this
work we present $\textbf{Scala}$ble $\textbf{B}$ayesian $\textbf{L}$ow-Rank
Adaptation via Stochastic Variational Subspace Inference (ScalaBL). We perform
Bayesian inference in an $r$-dimensional subspace, for LoRA rank $r$. By
repurposing the LoRA parameters as projection matrices, we are able to map
samples from this subspace into the full weight space of the LLM. This allows
us to learn all the parameters of our approach using stochastic variational
inference. Despite the low dimensionality of our subspace, we are able to
achieve competitive performance with state-of-the-art approaches while only
requiring ${\sim}1000$ additional parameters. Furthermore, it allows us to
scale up to the largest Bayesian LLM to date, with four times as a many base
parameters as prior work.

</details>


### [115] [Distributed Cross-Channel Hierarchical Aggregation for Foundation Models](https://arxiv.org/abs/2506.21411)
*Aristeidis Tsaris,Isaac Lyngaas,John Lagregren,Mohamed Wahib,Larry York,Prasanna Balaprakash,Dan Lu,Feiyi Wang,Xiao Wang*

Main category: cs.LG

TL;DR: 本文提出D - CHAG方法用于多通道图像数据集，兼容多种模型并行策略和视觉Transformer架构，提升计算效率，在超光谱成像和天气预报任务上评估效果良好。


<details>
  <summary>Details</summary>
Motivation: 当前分布式方法未完全解决图像标记和聚合计算密集的问题，需提高计算效率。

Method: 引入分布式跨通道分层聚合（D - CHAG）方法，该方法兼容任何模型并行策略和视觉Transformer架构。

Result: 在超光谱成像和天气预报任务上评估，结合张量并行和模型分片时，在Frontier超级计算机的1024个AMD GPU上内存使用最多减少75%，持续吞吐量翻倍以上。

Conclusion: D - CHAG方法能显著提高计算效率。

Abstract: Vision-based scientific foundation models hold significant promise for
advancing scientific discovery and innovation. This potential stems from their
ability to aggregate images from diverse sources such as varying physical
groundings or data acquisition systems and to learn spatio-temporal
correlations using transformer architectures. However, tokenizing and
aggregating images can be compute-intensive, a challenge not fully addressed by
current distributed methods. In this work, we introduce the Distributed
Cross-Channel Hierarchical Aggregation (D-CHAG) approach designed for datasets
with a large number of channels across image modalities. Our method is
compatible with any model-parallel strategy and any type of vision transformer
architecture, significantly improving computational efficiency. We evaluated
D-CHAG on hyperspectral imaging and weather forecasting tasks. When integrated
with tensor parallelism and model sharding, our approach achieved up to a 75%
reduction in memory usage and more than doubled sustained throughput on up to
1,024 AMD GPUs on the Frontier Supercomputer.

</details>


### [116] [Flow-Based Single-Step Completion for Efficient and Expressive Policy Learning](https://arxiv.org/abs/2506.21427)
*Prajwal Koirala,Cody Fleming*

Main category: cs.LG

TL;DR: 提出单步完成策略（SSCP）用于强化学习，结合生成模型表达力与单峰策略效率，在多场景有优势。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型用于离线强化学习时存在推理成本高和训练不稳定问题。

Method: 提出SSCP，用增强的流匹配目标训练生成策略，直接从中间流样本预测完成向量；在离策略演员 - 评论家框架中结合生成模型与单峰策略优势；扩展到目标条件强化学习。

Result: 在标准离线强化学习和行为克隆基准测试中取得了良好结果。

Conclusion: SSCP是一个通用、富有表现力且高效的深度强化学习和序列决策框架。

Abstract: Generative models such as diffusion and flow-matching offer expressive
policies for offline reinforcement learning (RL) by capturing rich, multimodal
action distributions, but their iterative sampling introduces high inference
costs and training instability due to gradient propagation across sampling
steps. We propose the \textit{Single-Step Completion Policy} (SSCP), a
generative policy trained with an augmented flow-matching objective to predict
direct completion vectors from intermediate flow samples, enabling accurate,
one-shot action generation. In an off-policy actor-critic framework, SSCP
combines the expressiveness of generative models with the training and
inference efficiency of unimodal policies, without requiring long
backpropagation chains. Our method scales effectively to offline,
offline-to-online, and online RL settings, offering substantial gains in speed
and adaptability over diffusion-based baselines. We further extend SSCP to
goal-conditioned RL, enabling flat policies to exploit subgoal structures
without explicit hierarchical inference. SSCP achieves strong results across
standard offline RL and behavior cloning benchmarks, positioning it as a
versatile, expressive, and efficient framework for deep RL and sequential
decision-making.

</details>


### [117] [Deception Detection in Dyadic Exchanges Using Multimodal Machine Learning: A Study on a Swedish Cohort](https://arxiv.org/abs/2506.21429)
*Franco Rugolon,Thomas Jack Samuels,Stephan Hau,Lennart Högman*

Main category: cs.LG

TL;DR: 研究用多模态机器学习技术在二元互动中检测欺骗，对比融合策略，发现结合双方多模态信息效果佳。


<details>
  <summary>Details</summary>
Motivation: 探究多模态机器学习技术在二元互动中检测欺骗的有效性，结合双方数据进行研究。

Method: 比较早期和晚期融合方法，利用音频和视频数据（动作单元和注视信息），对所有可能的模态和参与者组合进行分析，使用新收集的瑞典母语者数据集。

Result: 结合语音和面部信息比单模态方法效果好，纳入双方数据显著提高检测准确率，晚期融合策略对双方两种模态效果最佳，准确率达71%。

Conclusion: 研究结果符合心理学理论，为斯堪的纳维亚人群二元互动研究奠基，尤其适用于心理治疗场景。

Abstract: This study investigates the efficacy of using multimodal machine learning
techniques to detect deception in dyadic interactions, focusing on the
integration of data from both the deceiver and the deceived. We compare early
and late fusion approaches, utilizing audio and video data - specifically,
Action Units and gaze information - across all possible combinations of
modalities and participants. Our dataset, newly collected from Swedish native
speakers engaged in truth or lie scenarios on emotionally relevant topics,
serves as the basis for our analysis. The results demonstrate that
incorporating both speech and facial information yields superior performance
compared to single-modality approaches. Moreover, including data from both
participants significantly enhances deception detection accuracy, with the best
performance (71%) achieved using a late fusion strategy applied to both
modalities and participants. These findings align with psychological theories
suggesting differential control of facial and vocal expressions during initial
interactions. As the first study of its kind on a Scandinavian cohort, this
research lays the groundwork for future investigations into dyadic
interactions, particularly within psychotherapy settings.

</details>


### [118] [Towards an Optimal Control Perspective of ResNet Training](https://arxiv.org/abs/2506.21453)
*Jens Püttschneider,Simon Heilig,Asja Fischer,Timm Faulwasser*

Main category: cs.LG

TL;DR: 提出适用于ResNets的训练公式，展示其使不必要深层残差层权重消失，有理论依据的层剪枝潜力。


<details>
  <summary>Details</summary>
Motivation: 提出一种适用于标准架构和通用损失函数的ResNets训练公式。

Method: 通过惩罚隐藏状态的中间输出，对应最优控制中的阶段成本项；对标准ResNets，通过后续跳跃连接和输出层传播状态获取中间输出。

Result: 训练动态使不必要的更深层残差层权重趋于消失。

Conclusion: 该方法具有发展理论依据的层剪枝策略的潜力。

Abstract: We propose a training formulation for ResNets reflecting an optimal control
problem that is applicable for standard architectures and general loss
functions. We suggest bridging both worlds via penalizing intermediate outputs
of hidden states corresponding to stage cost terms in optimal control. For
standard ResNets, we obtain intermediate outputs by propagating the state
through the subsequent skip connections and the output layer. We demonstrate
that our training dynamic biases the weights of the unnecessary deeper residual
layers to vanish. This indicates the potential for a theory-grounded layer
pruning strategy.

</details>


### [119] [A Keyword-Based Technique to Evaluate Broad Question Answer Script](https://arxiv.org/abs/2506.21461)
*Tamim Al Mahmud,Md Gulzar Hussain,Sumaiya Kabir,Hasnain Ahmad,Mahmudus Sobhan*

Main category: cs.LG

TL;DR: 文章提出电子评估主观答卷的有效方案，介绍系统方法并给出测试结果。


<details>
  <summary>Details</summary>
Motivation: 寻找电子评估主观答卷的有效方案。

Method: 提出并实现综合系统，从答卷找关键词与不同领域关键词对比，检查语法和拼写错误。

Result: 用100名学生答卷测试，精确率达0.91。

Conclusion: 所提出的系统能有效评估主观答卷。

Abstract: Evaluation is the method of assessing and determining the educational system
through various techniques such as verbal or viva-voice test, subjective or
objective written test. This paper presents an efficient solution to evaluate
the subjective answer script electronically. In this paper, we proposed and
implemented an integrated system that examines and evaluates the written answer
script. This article focuses on finding the keywords from the answer script and
then compares them with the keywords that have been parsed from both open and
closed domain. The system also checks the grammatical and spelling errors in
the answer script. Our proposed system tested with answer scripts of 100
students and gives precision score 0.91.

</details>


### [120] [Optimising 4th-Order Runge-Kutta Methods: A Dynamic Heuristic Approach for Efficiency and Low Storage](https://arxiv.org/abs/2506.21465)
*Gavin Lee Goodship,Luis Miralles-Pechuan,Stephen O'Sullivan*

Main category: cs.LG

TL;DR: 研究引入混合遗传算法和强化学习方法优化低存储ESRK方法，经测试验证其效果，展示了自适应启发式发现的潜力。


<details>
  <summary>Details</summary>
Motivation: 解决大规模科学工程计算中ESRK方法在平衡精度、稳定性和计算效率上的难题，特别是高阶低存储方案的挑战。

Method: 采用混合遗传算法和强化学习的方法，利用GA驱动的变异进行搜索空间探索，用RL启发的状态转移机制动态优化启发式选择。

Result: 最佳启发式在基准问题测试中使IPOPT运行时间相比传统ESRK优化过程减少25%，同时保持数值稳定性和准确性。

Conclusion: 自适应启发式发现可提高高保真模拟资源效率，拓宽低存储Runge - Kutta方法应用范围，为数值方法启发式优化建立了新范式。

Abstract: Extended Stability Runge-Kutta (ESRK) methods are crucial for solving
large-scale computational problems in science and engineering, including
weather forecasting, aerodynamic analysis, and complex biological modelling.
However, balancing accuracy, stability, and computational efficiency remains
challenging, particularly for high-order, low-storage schemes. This study
introduces a hybrid Genetic Algorithm (GA) and Reinforcement Learning (RL)
approach for automated heuristic discovery, optimising low-storage ESRK
methods. Unlike traditional approaches that rely on manually designed
heuristics or exhaustive numerical searches, our method leverages GA-driven
mutations for search-space exploration and an RL-inspired state transition
mechanism to refine heuristic selection dynamically. This enables systematic
parameter reduction, preserving fourth-order accuracy while significantly
improving computational efficiency.The proposed GA-RL heuristic optimisation
framework is validated through rigorous testing on benchmark problems,
including the 1D and 2D Brusselator systems and the steady-state Navier-Stokes
equations. The best-performing heuristic achieves a 25\% reduction in IPOPT
runtime compared to traditional ESRK optimisation processes while maintaining
numerical stability and accuracy. These findings demonstrate the potential of
adaptive heuristic discovery to improve resource efficiency in high-fidelity
simulations and broaden the applicability of low-storage Runge-Kutta methods in
real-world computational fluid dynamics, physics simulations, and other
demanding fields. This work establishes a new paradigm in heuristic
optimisation for numerical methods, opening pathways for further exploration
using Deep RL and AutoML-based heuristic search

</details>


### [121] [Devising a solution to the problems of Cancer awareness in Telangana](https://arxiv.org/abs/2506.21500)
*Priyanka Avhad,Vedanti Kshirsagar,Urvi Ranjan,Mahek Nakhua*

Main category: cs.LG

TL;DR: 2020年特伦甘纳邦女性癌症筛查比例低，开发ML模型预测癌症易感性，设计系统提供就医建议等，旨在提高癌症意识和降低死亡率。


<details>
  <summary>Details</summary>
Motivation: 特伦甘纳邦民众对宫颈癌和乳腺癌的症状及筛查意识低，早期检测是降低发病率和死亡率的唯一途径。

Method: 开发ML分类模型，用决策树和支持向量分类算法分别预测宫颈癌和乳腺癌易感性；设计系统根据用户位置提供就医建议；整合健康卡记录医疗信息并开展宣传活动。

Result: 构建了相应的模型和系统。

Conclusion: 该解决方案有助于提高特伦甘纳邦民众的癌症意识，降低癌症死亡率，提高癌症知识水平。

Abstract: According to the data, the percent of women who underwent screening for
cervical cancer, breast and oral cancer in Telangana in the year 2020 was 3.3
percent, 0.3 percent and 2.3 percent respectively. Although early detection is
the only way to reduce morbidity and mortality, people have very low awareness
about cervical and breast cancer signs and symptoms and screening practices. We
developed an ML classification model to predict if a person is susceptible to
breast or cervical cancer based on demographic factors. We devised a system to
provide suggestions for the nearest hospital or Cancer treatment centres based
on the users location or address. In addition to this, we can integrate the
health card to maintain medical records of all individuals and conduct
awareness drives and campaigns. For ML classification models, we used decision
tree classification and support vector classification algorithms for cervical
cancer susceptibility and breast cancer susceptibility respectively. Thus, by
devising this solution we come one step closer to our goal which is spreading
cancer awareness, thereby, decreasing the cancer mortality and increasing
cancer literacy among the people of Telangana.

</details>


### [122] [Process mining-driven modeling and simulation to enhance fault diagnosis in cyber-physical systems](https://arxiv.org/abs/2506.21502)
*Francesco Vitale,Nicola Dall'Ora,Sebastiano Gaiardelli,Enrico Fraccaroli,Nicola Mazzocca,Franco Fummi*

Main category: cs.LG

TL;DR: 提出一种无监督故障诊断方法，结合多元时间序列异常检测、流程挖掘和随机模拟，用RoAD数据集验证，可用于预测性维护和数字孪生开发。


<details>
  <summary>Details</summary>
Motivation: 解决网络物理系统（CPS）中手动建模故障行为需大量领域知识、模型复杂易错且难解释的问题。

Method: 先通过多元时间序列分析从低级传感器数据中检测集体异常，将异常转化为结构化事件日志，用流程挖掘发现可解释的流程模型，将时间分布融入提取的Petri网以支持随机模拟。

Result: 使用RoAD数据集验证，实验结果表明该方法在CPS故障行为建模、模拟和分类方面有效。

Conclusion: 该方法可创建全面的故障字典，支持预测性维护和工业环境数字孪生的开发。

Abstract: Fault diagnosis in Cyber-Physical Systems (CPSs) is essential for ensuring
system dependability and operational efficiency by accurately detecting
anomalies and identifying their root causes. However, the manual modeling of
faulty behaviors often demands extensive domain expertise and produces models
that are complex, error-prone, and difficult to interpret. To address this
challenge, we present a novel unsupervised fault diagnosis methodology that
integrates collective anomaly detection in multivariate time series, process
mining, and stochastic simulation. Initially, collective anomalies are detected
from low-level sensor data using multivariate time-series analysis. These
anomalies are then transformed into structured event logs, enabling the
discovery of interpretable process models through process mining. By
incorporating timing distributions into the extracted Petri nets, the approach
supports stochastic simulation of faulty behaviors, thereby enhancing root
cause analysis and behavioral understanding. The methodology is validated using
the Robotic Arm Dataset (RoAD), a widely recognized benchmark in smart
manufacturing. Experimental results demonstrate its effectiveness in modeling,
simulating, and classifying faulty behaviors in CPSs. This enables the creation
of comprehensive fault dictionaries that support predictive maintenance and the
development of digital twins for industrial environments.

</details>


### [123] [mTSBench: Benchmarking Multivariate Time Series Anomaly Detection and Model Selection at Scale](https://arxiv.org/abs/2506.21550)
*Xiaona Zhou,Constantin Brif,Ismini Lourentzou*

Main category: cs.LG

TL;DR: 本文介绍了用于多变量时间序列异常检测（MTS - AD）和无监督模型选择的基准mTSBench，评估多种方法，结果表明无单一检测器最优且现有选择方法有差距，mTSBench可助力未来研究。


<details>
  <summary>Details</summary>
Motivation: MTS - AD因变量依赖、时间动态和稀疏异常标签等问题具有挑战性，需要统一评估方法。

Method: 引入mTSBench，涵盖344个标记时间序列、19个数据集和12个应用领域，评估24种异常检测方法和无监督模型选择技术。

Result: 没有单一检测器在所有数据集上表现出色，现有最先进的选择方法远非最优。

Conclusion: mTSBench提供统一评估套件，可促进自适应异常检测和鲁棒模型选择的未来发展。

Abstract: Multivariate time series anomaly detection (MTS-AD) is critical in domains
like healthcare, cybersecurity, and industrial monitoring, yet remains
challenging due to complex inter-variable dependencies, temporal dynamics, and
sparse anomaly labels. We introduce mTSBench, the largest benchmark to date for
MTS-AD and unsupervised model selection, spanning 344 labeled time series
across 19 datasets and 12 diverse application domains. mTSBench evaluates 24
anomaly detection methods, including large language model (LLM)-based detectors
for multivariate time series, and systematically benchmarks unsupervised model
selection techniques under standardized conditions. Consistent with prior
findings, our results confirm that no single detector excels across datasets,
underscoring the importance of model selection. However, even state-of-the-art
selection methods remain far from optimal, revealing critical gaps. mTSBench
provides a unified evaluation suite to enable rigorous, reproducible
comparisons and catalyze future advances in adaptive anomaly detection and
robust model selection.

</details>


### [124] [Where to find Grokking in LLM Pretraining? Monitor Memorization-to-Generalization without Test](https://arxiv.org/abs/2506.21551)
*Ziyue Li,Chenrui Fan,Tianyi Zhou*

Main category: cs.LG

TL;DR: 本文首次研究7B大语言模型OLMoE单遍预训练检查点中的grokking现象，验证其在大规模基础模型预训练中存在，通过研究内部动态揭示机制，开发新指标预测泛化性能并给出理论解释。


<details>
  <summary>Details</summary>
Motivation: Grokking使神经网络泛化和推理等能力的机制变得神秘，此前研究多针对小模型在少量玩具或特定任务上进行，本文旨在研究大语言模型预训练中的grokking现象。

Method: 在7B大语言模型OLMoE的单遍预训练检查点上研究grokking，计算训练损失并在多种基准任务上评估泛化性能，研究LLM内部动态，开发两个新指标量化路径距离和单路径复杂度。

Result: 验证了grokking在大规模基础模型预训练中仍会发生，不同数据进入grokking阶段可能异步；发现训练样本路径从随机、特定实例向更结构化和可共享转变，路径复杂度降低；新指标能预测下游任务泛化性能提升。

Conclusion: 揭示了grokking中从记忆到泛化的转换，为延迟泛化提供机制解释；新指标对预训练有实用价值，理论上更结构化的路径可降低模型复杂度并改善泛化界。

Abstract: Grokking, i.e., test performance keeps improving long after training loss
converged, has been recently witnessed in neural network training, making the
mechanism of generalization and other emerging capabilities such as reasoning
mysterious. While prior studies usually train small models on a few toy or
highly-specific tasks for thousands of epochs, we conduct the first study of
grokking on checkpoints during one-pass pretraining of a 7B large language
model (LLM), i.e., OLMoE. We compute the training loss and evaluate
generalization on diverse benchmark tasks, including math reasoning, code
generation, and commonsense/domain-specific knowledge retrieval tasks.
  Our study, for the first time, verifies that grokking still happens in the
pretraining of large-scale foundation models, though different data may enter
grokking stages asynchronously. We further demystify grokking's "emergence of
generalization" by investigating LLM internal dynamics. Specifically, we find
that training samples' pathways (i.e., expert choices across layers) evolve
from random, instance-specific to more structured and shareable between samples
during grokking. Also, the complexity of a sample's pathway reduces despite the
converged loss. These indicate a memorization-to-generalization conversion,
providing a mechanistic explanation of delayed generalization. In the study, we
develop two novel metrics to quantify pathway distance and the complexity of a
single pathway. We show their ability to predict the generalization improvement
on diverse downstream tasks. They are efficient, simple to compute and solely
dependent on training data. Hence, they have practical value for pretraining,
enabling us to monitor the generalization performance without finetuning and
test. Theoretically, we show that more structured pathways reduce model
complexity and improve the generalization bound.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [125] [Spiking Neural Networks for SAR Interferometric Phase Unwrapping: A Theoretical Framework for Energy-Efficient Processing](https://arxiv.org/abs/2506.20782)
*Marc Bara*

Main category: cs.NE

TL;DR: 提出首个将脉冲神经网络用于合成孔径雷达干涉相位解缠的理论框架，开发编码方案与架构，开启新研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有方法未将SNN用于相位解缠，且随着地球观测数据量增长，需要节能处理，SNN有节能潜力。

Method: 开发适用于包裹相位数据的脉冲编码方案，提出利用相位解缠空间传播特性的SNN架构，并进行计算复杂度和收敛性理论分析。

Result: 框架展示了SNN固有时间动态可自然建模相位解缠的空间连续性约束。

Conclusion: 本工作开启神经形态计算和SAR干涉测量交叉领域新研究方向，为现有算法提供补充，有望实现更可持续的大规模InSAR处理。

Abstract: We present the first theoretical framework for applying spiking neural
networks (SNNs) to synthetic aperture radar (SAR) interferometric phase
unwrapping. Despite extensive research in both domains, our comprehensive
literature review confirms that SNNs have never been applied to phase
unwrapping, representing a significant gap in current methodologies. As Earth
observation data volumes continue to grow exponentially (with missions like
NISAR expected to generate 100PB in two years) energy-efficient processing
becomes critical for sustainable data center operations. SNNs, with their
event-driven computation model, offer potential energy savings of 30-100x
compared to conventional approaches while maintaining comparable accuracy. We
develop spike encoding schemes specifically designed for wrapped phase data,
propose SNN architectures that leverage the spatial propagation nature of phase
unwrapping, and provide theoretical analysis of computational complexity and
convergence properties. Our framework demonstrates how the temporal dynamics
inherent in SNNs can naturally model the spatial continuity constraints
fundamental to phase unwrapping. This work opens a new research direction at
the intersection of neuromorphic computing and SAR interferometry, offering a
complementary approach to existing algorithms that could enable more
sustainable large-scale InSAR processing.

</details>


### [126] [Brain2Model Transfer: Training sensory and decision models with human neural activity as a teacher](https://arxiv.org/abs/2506.20834)
*Tomas Gallo Aquino,Victoria Liu,Habiba Azab,Raissa Mathura,Andrew J Watrous,Eleonora Bartoli,Benjamin Y Hayden,Paul Sajda,Sameer A Sheth,Nuttida Rungratsameetaweemana*

Main category: cs.NE

TL;DR: 本文提出Brain2Model Transfer Learning (B2M)框架，利用人脑神经活动作为教师模型训练人工神经网络，验证表明该方法使学生网络收敛更快、准确率更高。


<details>
  <summary>Details</summary>
Motivation: 人类大脑能以更少数据和算力创建低维抽象表征，受此启发，希望将人脑神经活动用于人工神经网络训练以提升训练效果。

Method: 提出两种B2M策略，即Brain Contrastive Transfer（通过对比目标对齐大脑活动和网络激活）和Brain Latent Transfer（通过监督回归将类似认知任务的潜在动态投影到学生网络）。

Result: 在基于记忆的决策和自动驾驶场景重建任务中验证，受益于基于大脑的迁移学习的学生网络收敛更快，预测准确率更高。

Conclusion: 大脑的表征对人工学习者有价值，为复杂决策表征的高效学习铺平道路。

Abstract: Transfer learning enhances the training of novel sensory and decision models
by employing rich feature representations from large, pre-trained teacher
models. Cognitive neuroscience shows that the human brain creates
low-dimensional, abstract representations for efficient sensorimotor coding.
Importantly, the brain can learn these representations with significantly fewer
data points and less computational power than artificial models require. We
introduce Brain2Model Transfer Learning (B2M), a framework where neural
activity from human sensory and decision-making tasks acts as the teacher model
for training artificial neural networks. We propose two B2M strategies: (1)
Brain Contrastive Transfer, which aligns brain activity and network activations
through a contrastive objective; and (2) Brain Latent Transfer, which projects
latent dynamics from similar cognitive tasks onto student networks via
supervised regression of brain-derived features. We validate B2M in
memory-based decision-making with a recurrent neural network and scene
reconstruction for autonomous driving with a variational autoencoder. The
results show that student networks benefiting from brain-based transfer
converge faster and achieve higher predictive accuracy than networks trained in
isolation. Our findings indicate that the brain's representations are valuable
for artificial learners, paving the way for more efficient learning of complex
decision-making representations, which would be costly or slow through purely
artificial training.

</details>


### [127] [Stochastic Quantum Spiking Neural Networks with Quantum Memory and Local Learning](https://arxiv.org/abs/2506.21324)
*Jiechen Chen,Bipin Rajendran,Osvaldo Simeone*

Main category: cs.NE

TL;DR: 本文提出随机量子脉冲（SQS）神经元模型及SQS神经网络（SQSNNs），结合神经形态与量子计算优势，解决现有量子脉冲模型局限。


<details>
  <summary>Details</summary>
Motivation: 现有量子脉冲模型存在依赖经典内存机制、需重复测量估计概率和用传统反向传播训练等局限，需提出新模型。

Method: 提出SQS神经元模型，用多量子比特电路实现带内部量子内存的脉冲单元，通过硬件友好的局部学习规则训练SQSNNs。

Result: 提出了SQS神经元模型和SQSNNs，融合了神经形态计算处理时间序列的效率和量子计算的大状态空间。

Conclusion: SQSNN模型为可在量子硬件上训练、模块化和可扩展的量子脉冲神经网络铺平道路。

Abstract: Neuromorphic and quantum computing have recently emerged as promising
paradigms for advancing artificial intelligence, each offering complementary
strengths. Neuromorphic systems built on spiking neurons excel at processing
time-series data efficiently through sparse, event-driven computation,
consuming energy only upon input events. Quantum computing, on the other hand,
leverages superposition and entanglement to explore feature spaces that are
exponentially large in the number of qubits. Hybrid approaches combining these
paradigms have begun to show potential, but existing quantum spiking models
have important limitations. Notably, prior quantum spiking neuron
implementations rely on classical memory mechanisms on single qubits, requiring
repeated measurements to estimate firing probabilities, and they use
conventional backpropagation on classical simulators for training. Here we
propose a stochastic quantum spiking (SQS) neuron model that addresses these
challenges. The SQS neuron uses multi-qubit quantum circuits to realize a
spiking unit with internal quantum memory, enabling event-driven probabilistic
spike generation in a single shot. Furthermore, we outline how networks of SQS
neurons -- dubbed SQS neural networks (SQSNNs) -- can be trained via a
hardware-friendly local learning rule, eliminating the need for global
classical backpropagation. The proposed SQSNN model fuses the time-series
efficiency of neuromorphic computing with the exponentially large inner state
space of quantum computing, paving the way for quantum spiking neural networks
that are modular, scalable, and trainable on quantum hardware.

</details>


### [128] [Assessing an evolutionary search engine for small language models, prompts, and evaluation metrics](https://arxiv.org/abs/2506.21512)
*Cláudio Lúcio do Val Lopes,Lucca Machado*

Main category: cs.NE

TL;DR: 本文针对小语言模型，采用双目标进化搜索引擎，用NSGA - II算法和提示语法优化任务准确率和令牌效率，找到高绩效模型 - 提示组合，为决策提供优化方案和人机交互模式框架。


<details>
  <summary>Details</summary>
Motivation: 在部署高效AI系统时，语言模型和指令提示的并发优化面临挑战，需平衡性能和计算成本。

Method: 引入双目标进化搜索引擎，使用NSGA - II算法和提示语法，对小语言模型在推理任务中同时优化任务准确率和令牌效率。

Result: 成功找到多样的高绩效模型 - 提示组合，揭示两个目标间的权衡关系，发现特定小语言模型和提示结构的任务特定亲和力，生成实用的帕累托前沿。

Conclusion: 该自动化方法超越传统手动调整，为发现有效的人机交互模式提供基础框架。

Abstract: The concurrent optimization of language models and instructional prompts
presents a significant challenge for deploying efficient and effective AI
systems, particularly when balancing performance against computational costs
like token usage. This paper introduces and assesses a bi-objective
evolutionary search engine designed to navigate this complex space, focusing
specifically on Small Language Models (SLMs). We employ the NSGA-II algorithm
and prompt grammar to simultaneously optimize for task accuracy and token
efficiency across some reasoning tasks. Our results successfully identify
diverse, high-performing model-prompt combinations, quantitatively revealing
the critical trade-off between the two objectives. This research highlights
task-specific affinities between particular SLMs and prompt structures (e.g.,
instructions, context, chain of thought). The generated practical Pareto fronts
offer decision-makers a portfolio of optimized solutions adaptable to their
specific constraints. This automated approach moves beyond traditional manual
tuning, providing a foundational framework for discovering effective human-AI
interaction patterns.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [129] [Domain Knowledge in Requirements Engineering: A Systematic Mapping Study](https://arxiv.org/abs/2506.20754)
*Marina Araújo,Júlia Araújo,Romeu Oliveira,Lucas Romao,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 本文通过系统映射研究，对将领域知识融入需求工程的现有成果进行全面概述，为知识驱动的需求工程奠定基础。


<details>
  <summary>Details</summary>
Motivation: 科学文献缺乏关于如何在需求工程中有效使用和实施领域知识的系统整合，本文旨在填补这一空白。

Method: 采用结合数据库搜索与迭代前后滚雪球的混合搜索策略进行系统映射研究。

Result: 找到75篇符合标准的论文，分析出主要需求类型、常考虑的质量属性和领域知识在形式化、获取及长期维护方面的常见挑战，为研究和实践提供支持并指出未来研究方向。

Conclusion: 本研究提供全面概述，有助于为知识驱动的需求工程构建概念和方法论基础。

Abstract: [Context] Domain knowledge is recognized as a key component for the success
of Requirements Engineering (RE), as it provides the conceptual support needed
to understand the system context, ensure alignment with stakeholder needs, and
reduce ambiguity in requirements specification. Despite its relevance, the
scientific literature still lacks a systematic consolidation of how domain
knowledge can be effectively used and operationalized in RE. [Goal] This paper
addresses this gap by offering a comprehensive overview of existing
contributions, including methods, techniques, and tools to incorporate domain
knowledge into RE practices. [Method] We conducted a systematic mapping study
using a hybrid search strategy that combines database searches with iterative
backward and forward snowballing. [Results] In total, we found 75 papers that
met our inclusion criteria. The analysis highlights the main types of
requirements addressed, the most frequently considered quality attributes, and
recurring challenges in the formalization, acquisition, and long-term
maintenance of domain knowledge. The results provide support for researchers
and practitioners in identifying established approaches and unresolved issues.
The study also outlines promising directions for future research, emphasizing
the development of scalable, automated, and sustainable solutions to integrate
domain knowledge into RE processes. [Conclusion] The study contributes by
providing a comprehensive overview that helps to build a conceptual and
methodological foundation for knowledge-driven requirements engineering.

</details>


### [130] [Agile Management for Machine Learning: A Systematic Mapping Study](https://arxiv.org/abs/2506.20759)
*Lucas Romao,Hugo Villamizar,Romeu Oliveira,Silvio Alonso,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 本文探讨机器学习系统的敏捷管理，通过系统映射研究确定相关论文，总结框架、主题和挑战，指出需更多实证评估。


<details>
  <summary>Details</summary>
Motivation: 机器学习开发的动态性给传统项目管理带来挑战，需明确如何在机器学习系统中有效应用敏捷方法，故概述该领域的最新进展。

Method: 采用结合数据库搜索与前后滚雪球迭代的混合搜索策略进行系统映射研究。

Result: 确定27篇2008 - 2024年发表的论文，找出八个框架，将建议和实践分为八个关键主题，主要挑战是准确估算机器学习相关任务的工作量。

Conclusion: 本研究描绘了该领域的现状并指出差距，现有工作需更多实证评估来验证。

Abstract: [Context] Machine learning (ML)-enabled systems are present in our society,
driving significant digital transformations. The dynamic nature of ML
development, characterized by experimental cycles and rapid changes in data,
poses challenges to traditional project management. Agile methods, with their
flexibility and incremental delivery, seem well-suited to address this
dynamism. However, it is unclear how to effectively apply these methods in the
context of ML-enabled systems, where challenges require tailored approaches.
[Goal] Our goal is to outline the state of the art in agile management for
ML-enabled systems. [Method] We conducted a systematic mapping study using a
hybrid search strategy that combines database searches with backward and
forward snowballing iterations. [Results] Our study identified 27 papers
published between 2008 and 2024. From these, we identified eight frameworks and
categorized recommendations and practices into eight key themes, such as
Iteration Flexibility, Innovative ML-specific Artifacts, and the Minimal Viable
Model. The main challenge identified across studies was accurate effort
estimation for ML-related tasks. [Conclusion] This study contributes by mapping
the state of the art and identifying open gaps in the field. While relevant
work exists, more robust empirical evaluation is still needed to validate these
contributions.

</details>


### [131] [Generating Reliable Adverse event Profiles for Health through Automated Integrated Data (GRAPH-AID): A Semi-Automated Ontology Building Approach](https://arxiv.org/abs/2506.20851)
*Srikar Reddy Gadusu,Larry Callahan,Samir Lababidi,Arunasri Nishtala,Sophia Healey,Hande McGinty*

Main category: cs.SE

TL;DR: 数据和知识快速增长，传统KNARM方法整合数据库与OWL有挑战，本文用Python和rdflib库提供用户友好方法，以FDA数据为例展示，支持药物安全监测和决策。


<details>
  <summary>Details</summary>
Motivation: 数据和知识快速增长，传统KNARM方法整合Neo4j数据库与OWL存在挑战，需更易上手的方法。

Method: 利用Python和其rdflib库，以FDA的FAERS数据库数据创建Neo4j数据库，开发Python脚本自动生成类和公理。

Result: 实现了更平滑的整合过程。

Conclusion: 该方法为快速增长的药物不良事件数据集的本体生成挑战提供实用解决方案，支持药物安全监测和公共卫生决策。

Abstract: As data and knowledge expand rapidly, adopting systematic methodologies for
ontology generation has become crucial. With the daily increases in data
volumes and frequent content changes, the demand for databases to store and
retrieve information for the creation of knowledge graphs has become
increasingly urgent. The previously established Knowledge Acquisition and
Representation Methodology (KNARM) outlines a systematic approach to address
these challenges and create knowledge graphs. However, following this
methodology highlights the existing challenge of seamlessly integrating Neo4j
databases with the Web Ontology Language (OWL). Previous attempts to integrate
data from Neo4j into an ontology have been discussed, but these approaches
often require an understanding of description logics (DL) syntax, which may not
be familiar to many users. Thus, a more accessible method is necessary to
bridge this gap. This paper presents a user-friendly approach that utilizes
Python and its rdflib library to support ontology development. We showcase our
novel approach through a Neo4j database we created by integrating data from the
Food and Drug Administration (FDA) Adverse Event Reporting System (FAERS)
database. Using this dataset, we developed a Python script that automatically
generates the required classes and their axioms, facilitating a smoother
integration process. This approach offers a practical solution to the
challenges of ontology generation in the context of rapidly growing adverse
drug event datasets, supporting improved drug safety monitoring and public
health decision-making.

</details>


### [132] [Engineering RAG Systems for Real-World Applications: Design, Development, and Evaluation](https://arxiv.org/abs/2506.20869)
*Md Toufique Hasan,Muhammad Waseem,Kai-Kristian Kemell,Ayman Asad Khan,Mika Saari,Pekka Abrahamsson*

Main category: cs.SE

TL;DR: 本文介绍五个特定领域的RAG应用，通过用户评估并总结关键经验教训。


<details>
  <summary>Details</summary>
Motivation: 缺乏基于真实用例、经用户评估且有经验总结的RAG实现的实证研究。

Method: 开发五个领域特定RAG应用，结合多语言OCR、语义检索和领域适配大模型，通过本地服务器或云API部署，进行100人网络评估。

Result: 通过六维度评估系统，并基于用户反馈和开发经验总结12条关键经验。

Conclusion: 指出影响RAG系统可靠性和可用性的技术、操作和伦理挑战。

Abstract: Retrieval-Augmented Generation (RAG) systems are emerging as a key approach
for grounding Large Language Models (LLMs) in external knowledge, addressing
limitations in factual accuracy and contextual relevance. However, there is a
lack of empirical studies that report on the development of RAG-based
implementations grounded in real-world use cases, evaluated through general
user involvement, and accompanied by systematic documentation of lessons
learned. This paper presents five domain-specific RAG applications developed
for real-world scenarios across governance, cybersecurity, agriculture,
industrial research, and medical diagnostics. Each system incorporates
multilingual OCR, semantic retrieval via vector embeddings, and domain-adapted
LLMs, deployed through local servers or cloud APIs to meet distinct user needs.
A web-based evaluation involving a total of 100 participants assessed the
systems across six dimensions: (i) Ease of Use, (ii) Relevance, (iii)
Transparency, (iv) Responsiveness, (v) Accuracy, and (vi) Likelihood of
Recommendation. Based on user feedback and our development experience, we
documented twelve key lessons learned, highlighting technical, operational, and
ethical challenges affecting the reliability and usability of RAG systems in
practice.

</details>


### [133] [Complex Model Transformations by Reinforcement Learning with Uncertain Human Guidance](https://arxiv.org/abs/2506.20883)
*Kyanna Dagenais,Istvan David*

Main category: cs.SE

TL;DR: 本文提出通过强化学习开发复杂模型转换序列的方法，由人类建议引导，评估显示人类引导能提升性能，迈向强化学习驱动的人在环工程方法。


<details>
  <summary>Details</summary>
Motivation: 手动开发复杂模型转换易出错且不可行，强化学习在复杂问题中有性能问题，需要人类引导。

Method: 提出一种方法和技术框架，将用户定义的模型转换映射到强化学习原语，作为强化学习程序执行以找到最优模型转换序列。

Result: 人类引导（即使不确定）能显著提升强化学习性能，更高效地开发复杂模型转换。

Conclusion: 通过权衡人类建议的确定性和及时性，向强化学习驱动的人在环工程方法迈进了一步。

Abstract: Model-driven engineering problems often require complex model transformations
(MTs), i.e., MTs that are chained in extensive sequences. Pertinent examples of
such problems include model synchronization, automated model repair, and design
space exploration. Manually developing complex MTs is an error-prone and often
infeasible process. Reinforcement learning (RL) is an apt way to alleviate
these issues. In RL, an autonomous agent explores the state space through trial
and error to identify beneficial sequences of actions, such as MTs. However, RL
methods exhibit performance issues in complex problems. In these situations,
human guidance can be of high utility. In this paper, we present an approach
and technical framework for developing complex MT sequences through RL, guided
by potentially uncertain human advice. Our framework allows user-defined MTs to
be mapped onto RL primitives, and executes them as RL programs to find optimal
MT sequences. Our evaluation shows that human guidance, even if uncertain,
substantially improves RL performance, and results in more efficient
development of complex MTs. Through a trade-off between the certainty and
timeliness of human advice, our method takes a step towards RL-driven
human-in-the-loop engineering methods.

</details>


### [134] [Boosting Vulnerability Detection with Inter-function Multilateral Association Insights](https://arxiv.org/abs/2506.21014)
*Shaojian Qiu,Mengyang Huang,Jiahao Cheng*

Main category: cs.SE

TL;DR: 提出IFMA - VD框架用于漏洞检测，构建代码行为超图并利用超边卷积提取特征，在三个数据集上评估显示效果提升。


<details>
  <summary>Details</summary>
Motivation: 当前多数基于深度学习的漏洞检测方法忽略函数间复杂多边关联，无法检测关联中的漏洞。

Method: 构建代码行为超图，用超边卷积提取多边关联特征，先解析函数生成函数内特征，再分割程序依赖图构建超图，最后用超图网络增强漏洞检测。

Result: 在三个常用漏洞数据集上评估，F - measure和Recall相比基线方法有提升，多边关联特征可增强代码特征表示。

Conclusion: IFMA - VD框架有效，多边关联特征有助于漏洞检测。

Abstract: Vulnerability detection is a crucial yet challenging technique for ensuring
the security of software systems. Currently, most deep learning-based
vulnerability detection methods focus on stand-alone functions, neglecting the
complex inter-function interrelations, particularly the multilateral
associations. This oversight can fail to detect vulnerabilities in these
interrelations. To address this gap, we present an Inter-Function Multilateral
Association analysis framework for Vulnerability Detection (IFMA-VD). The
cornerstone of the IFMA-VD lies in constructing a code behavior hypergraph and
utilizing hyperedge convolution to extract multilateral association features.
Specifically, we first parse functions into a code property graph to generate
intra-function features. Following this, we construct a code behavior
hypergraph by segmenting the program dependency graph to isolate and encode
behavioral features into hyperedges. Finally, we utilize a hypergraph network
to capture the multilateral association knowledge for augmenting vulnerability
detection. We evaluate IFMA-VD on three widely used vulnerability datasets and
demonstrate improvements in F-measure and Recall compared to baseline methods.
Additionally, we illustrate that multilateral association features can boost
code feature representation and validate the effectiveness of IFMA-VD on
real-world datasets.

</details>


### [135] [How Good Are Synthetic Requirements ? Evaluating LLM-Generated Datasets for AI4RE](https://arxiv.org/abs/2506.21138)
*Abdelkarim El-Hajjami,Camille Salinesi*

Main category: cs.SE

TL;DR: 本文提出Synthline v1用于生成合成需求数据，研究不同策略对数据质量的影响，发现合成数据在特定任务中可超越人工编写数据，为缓解数据集稀缺提供思路。


<details>
  <summary>Details</summary>
Motivation: 公开可用的带标签需求数据集短缺阻碍AI4RE发展，且缺乏控制和优化生成需求质量的系统方法。

Method: 提出Synthline v1，通过先进生成策略和策展技术扩展早期版本；研究四种提示策略、自动提示优化和生成后策展对四个分类任务数据质量的影响。

Result: 多样本提示显著提高数据效用和多样性；自动提示优化结果因任务而异；基于相似度的策展提高多样性但常损害分类性能；合成需求在特定任务中可匹配或超越人工编写数据。

Conclusion: 研究结果为AI4RE提供实用见解，通过系统的合成生成可缓解数据集稀缺问题。

Abstract: The shortage of publicly available, labeled requirements datasets remains a
major barrier to advancing Artificial Intelligence for Requirements Engineering
(AI4RE). While Large Language Models offer promising capabilities for synthetic
data generation, systematic approaches to control and optimize the quality of
generated requirements remain underexplored. This paper presents Synthline v1,
an enhanced Product Line approach for generating synthetic requirements data
that extends our earlier v0 version with advanced generation strategies and
curation techniques. We investigate four research questions assessing how
prompting strategies, automated prompt optimization, and post-generation
curation affect data quality across four classification tasks: defect
detection, functional vs. non-functional, quality vs. non-quality, and security
vs. non-security. Our evaluation shows that multi-sample prompting
significantly boosts both utility and diversity over single-sample generation,
with F1-score gains from 6 to 44 points. The use of PACE (Prompt Actor-Critic
Editing) for automated prompt optimization yields task-dependent results,
greatly improving functional classification (+32.5 points) but reducing
performance on others. Interestingly, similarity-based curation improves
diversity but often harms classification performance, indicating that some
redundancy may help ML models. Most importantly, our results show that
synthetic requirements can match or outperform human-authored ones for specific
tasks, with synthetic data surpassing human data for security (+7.8 points) and
defect classification (+15.4 points). These findings offer practical insights
for AI4RE and chart a viable path to mitigating dataset scarcity through
systematic synthetic generation.

</details>


### [136] [$T^3$: Multi-level Tree-based Automatic Program Repair with Large Language Models](https://arxiv.org/abs/2506.21211)
*Quanming Liu,Xupeng Bu,Zhichao Yan,Ru Li*

Main category: cs.SE

TL;DR: 文章评估常见CoT技术在APR任务中的表现，提出创新框架$T^3$提高生成候选修复方案的精度，为APR任务优化提供指导。


<details>
  <summary>Details</summary>
Motivation: APR是软件开发和维护的核心技术，虽LLMs和CoT技术发展提升推理能力，但CoT技术在APR领域应用不足。

Method: 系统评估常见CoT技术在APR任务中的表现，提出集成LLMs推理能力和树搜索的创新框架$T^3$。

Result: $T^3$有效提高了生成候选修复方案的精度。

Conclusion: $T^3$为APR任务中样本选择和修复策略优化提供指导，建立了高效自动调试的框架。

Abstract: Automatic Program Repair (APR) is a core technology in software development
and maintenance, with aims to enable automated defect repair with minimal human
intervention. In recent years, the substantial advancements in Large Language
Models (LLMs) and the Chain-of-Thought (CoT) techniques have significantly
enhanced the reasoning capabilities of these models. However, due to the
complex logic and multi-step reasoning ability needed, the application of CoT
techniques in the APR domain remains insufficient. This study systematically
evaluates the performance of several common CoT techniques in APR tasks and
proposes an innovative framework $T^3$, which integrates the powerful reasoning
capabilities of LLMs with tree search, effectively improving the precision of
generating candidate repair solutions. Furthermore, $T^3$ provides valuable
guidance for optimizing sample selection and repair strategies in APR tasks,
establishing a robust framework for achieving efficient automated debugging.

</details>


### [137] [KOALA: a Configurable Tool for Collecting IDE Data When Solving Programming Tasks](https://arxiv.org/abs/2506.21266)
*Daniil Karol,Elizaveta Artser,Ilya Vlasov,Yaroslav Golubev,Hieke Keuning,Anastasiia Birillo*

Main category: cs.SE

TL;DR: 本文提出KOALA工具，用于收集学生在JetBrains IDEs中解决编程任务的数据，克服了现有工具的局限，并展示了其应用。


<details>
  <summary>Details</summary>
Motivation: 现有数据收集工具存在收集代码粒度不可控、不收集编程环境特定事件、难以配置等局限，需要新工具。

Method: 开发KOALA插件，可安装在IDEs中进行配置，收集代码快照、IDE操作等数据并发送到服务器存储，转换为ProgSnap2格式。

Result: 收集了28名学生在两门课程中解决任务的数据，并从中得到一些见解。

Conclusion: KOALA是一个方便且高度可配置的工具，能有效收集学生解决编程任务的数据。

Abstract: Collecting data of students solving programming tasks is incredibly valuable
for researchers and educators. It allows verifying that the students correctly
apply the features and concepts they are taught, or finding students'
misconceptions. However, existing data collection tools have limitations, e.g.,
no control over the granularity of the collected code, not collecting the
specific events of the programming environment used, and overall being hard to
configure.
  To overcome these limitations, we propose KOALA, a convenient and highly
configurable tool for collecting code snapshots and feature usage from students
solving programming tasks in JetBrains IDEs. The plugin can be installed in
IDEs and configured to provide the students with the necessary tasks, enable or
disable certain IDE features like code completion, and run surveys. During
problem solving, the plugin collects code snapshots at the configured
granularity, all IDE actions like running and debugging, as well as some data
not collected in prior works, like employed hotkeys and switching focus between
files. The collected data is sent to the server that comes with the tool, where
it is stored and can be converted to the standardized ProgSnap2 format. To
showcase the tool, we collected data from 28 students solving tasks in two
courses within the IDE, highlighting some insights from this data.

</details>


### [138] [Exploring Micro Frontends: A Case Study Application in E-Commerce](https://arxiv.org/abs/2506.21297)
*Ricardo Hideki Hangai Kojo,Luiz Fernando Corte Real,Renato Cordeiro Ferreira,Thatiane de Oliveira Rosa,Alfredo Goldman*

Main category: cs.SE

TL;DR: 本文探讨微前端架构的应用时机，通过研究文献、实际应用和开发者调查发现，在某手工产品市场，虽微前端架构采用成功但非必要，因团队基础设施复用和知识共享使其成为便利选择。


<details>
  <summary>Details</summary>
Motivation: 理解在行业背景下何时值得采用微前端架构。

Method: 研究学术和灰色文献了解微前端现状，在手工产品市场实施该架构，用半开放式问卷评估。

Result: 微前端架构采用成功，但非严格必要满足公司需求，其他方案如单体前端也可获类似结果。

Conclusion: 公司背景下因单体拆解和微服务采用，通过基础设施复用和团队知识共享使微前端成为便利选择。

Abstract: In the micro frontends architectural style, the frontend is divided into
smaller components, which can range from a simple button to an entire page. The
goal is to improve scalability, resilience, and team independence, albeit at
the cost of increased complexity and infrastructure demands. This paper seeks
to understand when it is worth adopting micro frontends, particularly in the
context of industry. To achieve this, we conducted an investigation into the
state of the art of micro frontends, based on both academic and gray
literature. We then implemented this architectural style in a marketplace for
handcrafted products, which already used microservices. Finally, we evaluated
the implementation through a semi-open questionnaire with the developers. At
the studied marketplace company, the need for architectural change arose due to
the tight coupling between their main system (a Java monolith) and a dedicated
frontend system. Additionally, there were deprecated technologies and poor
developer experience. To address these issues, the micro frontends architecture
was adopted, along with the API Gateway and Backend for Frontend patterns, and
technologies such as Svelte and Fastify. Although the adoption of Micro
Frontends was successful, it was not strictly necessary to meet the company's
needs. According to the analysis of the mixed questionnaire responses, other
alternatives, such as a monolithic frontend, could have achieved comparable
results. What made adopting micro frontends the most convenient choice in the
company's context was the monolith strangulation and microservices adoption,
which facilitated implementation through infrastructure reuse and knowledge
sharing between teams.

</details>


### [139] [An object-centric core metamodel for IoT-enhanced event logs](https://arxiv.org/abs/2506.21300)
*Yannis Bertrand,Christian Imenkamp,Lukas Malburg,Matthias Ehrendorfer,Marco Franceschetti,Joscha Grüger,Francesco Leotta,Jürgen Mangler,Ronny Seiger,Agnes Koschmider,Stefanie Rinderle-Ma,Barbara Weber,Estefania Serral*

Main category: cs.SE

TL;DR: 本文提出核心模型整合物联网数据与流程数据，方便数据共享和协作，并通过Python原型实现验证。


<details>
  <summary>Details</summary>
Motivation: 物联网设备产生大量数据，结合流程数据可挖掘业务流程新知识，但两类数据特性差异大，现有数据模型分散阻碍数据交换和协作。

Method: 提出核心模型综合现有数据模型重要特征，用Python原型实现评估模型。

Result: 模型能满足常见需求。

Conclusion: 核心模型基于常见需求，极大促进该领域数据共享和协作。

Abstract: Advances in Internet-of-Things (IoT) technologies have prompted the
integration of IoT devices with business processes (BPs) in many organizations
across various sectors, such as manufacturing, healthcare and smart spaces. The
proliferation of IoT devices leads to the generation of large amounts of IoT
data providing a window on the physical context of BPs, which facilitates the
discovery of new insights about BPs using process mining (PM) techniques.
However, to achieve these benefits, IoT data need to be combined with
traditional process (event) data, which is challenging due to the very
different characteristics of IoT and process data, for instance in terms of
granularity levels. Recently, several data models were proposed to integrate
IoT data with process data, each focusing on different aspects of data
integration based on different assumptions and requirements. This fragmentation
hampers data exchange and collaboration in the field of PM, e.g., making it
tedious for researchers to share data. In this paper, we present a core model
synthesizing the most important features of existing data models. As the core
model is based on common requirements, it greatly facilitates data sharing and
collaboration in the field. A prototypical Python implementation is used to
evaluate the model against various use cases and demonstrate that it satisfies
these common requirements.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [140] [From On-chain to Macro: Assessing the Importance of Data Source Diversity in Cryptocurrency Market Forecasting](https://arxiv.org/abs/2506.21246)
*Giorgos Demosthenous,Chryssis Georgiou,Eliada Polydorou*

Main category: q-fin.PM

TL;DR: 研究通过整合多类数据，引入Crypto100指数和特征约简算法，发现数据源多样性提升加密货币预测模型性能。


<details>
  <summary>Details</summary>
Motivation: 探究数据源多样性对加密货币预测模型性能的影响。

Method: 整合技术指标、链上指标等多种数据类别，引入Crypto100指数，提出特征约简算法。

Result: 数据源多样性显著提升不同时间跨度预测模型的性能，链上指标对长短预测都重要，传统市场指数和宏观经济指标对长期预测相关性增加。

Conclusion: 研究结果有助于揭示加密货币市场的长短驱动因素，为开发更准确、有弹性的预测模型奠定基础。

Abstract: This study investigates the impact of data source diversity on the
performance of cryptocurrency forecasting models by integrating various data
categories, including technical indicators, on-chain metrics, sentiment and
interest metrics, traditional market indices, and macroeconomic indicators. We
introduce the Crypto100 index, representing the top 100 cryptocurrencies by
market capitalization, and propose a novel feature reduction algorithm to
identify the most impactful and resilient features from diverse data sources.
Our comprehensive experiments demonstrate that data source diversity
significantly enhances the predictive performance of forecasting models across
different time horizons. Key findings include the paramount importance of
on-chain metrics for both short-term and long-term predictions, the growing
relevance of traditional market indices and macroeconomic indicators for
longer-term forecasts, and substantial improvements in model accuracy when
diverse data sources are utilized. These insights help demystify the short-term
and long-term driving factors of the cryptocurrency market and lay the
groundwork for developing more accurate and resilient forecasting models.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [141] [The final solution of the Hitchhiker's problem #5](https://arxiv.org/abs/2506.20672)
*Matjaž Omladič,Martin Vuk,Aljaž Zalar*

Main category: stat.ML

TL;DR: 此前用线性规划解决部分准关联问题，本文用解析方法完整解答原问题


<details>
  <summary>Details</summary>
Motivation: 之前工作解决了部分准关联问题，希望用新方法完整解答原问题

Method: 采用解析方法

Result: 用解析方法为原问题提供了完整答案

Conclusion: 解析方法可完整解决多元准关联相关原问题

Abstract: A recent survey, nicknamed "Hitchhiker's Guide", J.J. Arias-Garc{\i}a, R.
Mesiar, and B. De Baets, A hitchhiker's guide to quasi-copulas, Fuzzy Sets and
Systems 393 (2020) 1-28, has raised the rating of quasi-copula problems in the
dependence modeling community in spite of the lack of statistical
interpretation of quasi-copulas. In our previous work (arXiv:2410.19339,
accepted in Fuzzy Sets and Systems), we addressed the question of extreme
values of the mass distribution associated with multivariate quasi-copulas.
Using a linear programming approach, we were able to solve Open Problem 5 of
the "Guide" up to dimension d = 17 and disprove a recent conjecture on the
solution to that problem. In this paper, we use an analytical approach to
provide a complete answer to the original question.

</details>


### [142] [Forecasting Geopolitical Events with a Sparse Temporal Fusion Transformer and Gaussian Process Hybrid: A Case Study in Middle Eastern and U.S. Conflict Dynamics](https://arxiv.org/abs/2506.20935)
*Hsin-Hsiung Huang,Hayden Hampton*

Main category: stat.ML

TL;DR: 本文介绍了STFT - VNNGP混合架构，它克服数据问题赢得竞赛，在预测中东和美国冲突动态中优于TFT，提供可靠情报框架且代码公开。


<details>
  <summary>Details</summary>
Motivation: 从GDELT等数据源预测地缘政治冲突是国家安全的关键挑战，现有标准深度学习模型因数据特性无法进行可靠的长周期预测。

Method: 采用两阶段过程，先使用TFT捕捉复杂时间动态生成多分位数预测，再将分位数作为输入给VNNGP进行时空平滑和不确定性量化。

Result: 在预测中东和美国冲突动态的案例研究中，STFT - VNNGP始终优于独立的TFT，在长周期预测中能更好地预测突发事件的时间和规模。

Conclusion: 该工作为从具有挑战性的事件数据中生成更可靠、可操作的情报提供了强大框架，且代码和工作流程公开以确保可重复性。

Abstract: Forecasting geopolitical conflict from data sources like the Global Database
of Events, Language, and Tone (GDELT) is a critical challenge for national
security. The inherent sparsity, burstiness, and overdispersion of such data
cause standard deep learning models, including the Temporal Fusion Transformer
(TFT), to produce unreliable long-horizon predictions. We introduce STFT-VNNGP,
a hybrid architecture that won the 2023 Algorithms for Threat Detection (ATD)
competition by overcoming these limitations. Designed to bridge this gap, our
model employs a two-stage process: first, a TFT captures complex temporal
dynamics to generate multi-quantile forecasts. These quantiles then serve as
informed inputs for a Variational Nearest Neighbor Gaussian Process (VNNGP),
which performs principled spatiotemporal smoothing and uncertainty
quantification. In a case study forecasting conflict dynamics in the Middle
East and the U.S., STFT-VNNGP consistently outperforms a standalone TFT,
showing a superior ability to predict the timing and magnitude of bursty event
periods, particularly at long-range horizons. This work offers a robust
framework for generating more reliable and actionable intelligence from
challenging event data, with all code and workflows made publicly available to
ensure reproducibility.

</details>


### [143] [Stable Minima of ReLU Neural Networks Suffer from the Curse of Dimensionality: The Neural Shattering Phenomenon](https://arxiv.org/abs/2506.20779)
*Tongtong Liang,Dan Qiao,Yu-Xiang Wang,Rahul Parhi*

Main category: stat.ML

TL;DR: 研究两层过参数化ReLU网络中平坦性的隐式偏差及对泛化的影响，证明上下界，发现高维时平坦解泛化性能差，有数值模拟佐证。


<details>
  <summary>Details</summary>
Motivation: 由梯度下降训练中的最小值稳定性和稳定性边缘现象所驱动，现有工作存在局限，如要求插值或仅关注单变量输入。

Method: 对两种自然设置证明上下界，基于新颖的打包论证构建极小极大下界。

Result: 平坦性虽意味着泛化，但收敛速度随输入维度增加指数恶化，平坦解与低范数解有指数分离，平坦解在高维表现差。

Conclusion: 首次系统解释了高维情况下平坦最小值可能无法泛化的原因。

Abstract: We study the implicit bias of flatness / low (loss) curvature and its effects
on generalization in two-layer overparameterized ReLU networks with
multivariate inputs -- a problem well motivated by the minima stability and
edge-of-stability phenomena in gradient-descent training. Existing work either
requires interpolation or focuses only on univariate inputs. This paper
presents new and somewhat surprising theoretical results for multivariate
inputs. On two natural settings (1) generalization gap for flat solutions, and
(2) mean-squared error (MSE) in nonparametric function estimation by stable
minima, we prove upper and lower bounds, which establish that while flatness
does imply generalization, the resulting rates of convergence necessarily
deteriorate exponentially as the input dimension grows. This gives an
exponential separation between the flat solutions vis-\`a-vis low-norm
solutions (i.e., weight decay), which knowingly do not suffer from the curse of
dimensionality. In particular, our minimax lower bound construction, based on a
novel packing argument with boundary-localized ReLU neurons, reveals how flat
solutions can exploit a kind of ''neural shattering'' where neurons rarely
activate, but with high weight magnitudes. This leads to poor performance in
high dimensions. We corroborate these theoretical findings with extensive
numerical simulations. To the best of our knowledge, our analysis provides the
first systematic explanation for why flat minima may fail to generalize in high
dimensions.

</details>


### [144] [Active Learning for Manifold Gaussian Process Regression](https://arxiv.org/abs/2506.20928)
*Yuanxing Cheng,Lulu Kang,Yiwei Wang,Chun Liu*

Main category: stat.ML

TL;DR: 本文提出用于流形高斯过程回归的主动学习框架，结合流形学习与数据选择，实验表现优于随机顺序学习，有实用价值，未来关注可扩展性等。


<details>
  <summary>Details</summary>
Motivation: 提升高维空间中高斯过程回归的准确性。

Method: 联合优化用于降维的神经网络和潜在空间中的高斯过程回归器，由最小化全局预测误差的主动学习准则监督。

Result: 在合成数据实验中表现优于随机顺序学习，能有效处理复杂、不连续函数且保持计算可行性。

Conclusion: 该框架有实用价值，未来工作聚焦可扩展性和不确定性感知的流形学习。

Abstract: This paper introduces an active learning framework for manifold Gaussian
Process (GP) regression, combining manifold learning with strategic data
selection to improve accuracy in high-dimensional spaces. Our method jointly
optimizes a neural network for dimensionality reduction and a Gaussian process
regressor in the latent space, supervised by an active learning criterion that
minimizes global prediction error. Experiments on synthetic data demonstrate
superior performance over randomly sequential learning. The framework
efficiently handles complex, discontinuous functions while preserving
computational tractability, offering practical value for scientific and
engineering applications. Future work will focus on scalability and
uncertainty-aware manifold learning.

</details>


### [145] [Lower Bounds on the Size of Markov Equivalence Classes](https://arxiv.org/abs/2506.20933)
*Erik Jahn,Frederick Eberhardt,Leonard J. Schulman*

Main category: stat.ML

TL;DR: 论文指出因果发现算法在放松部分假设时，马尔可夫等价类平均大小不再小，并给出三种图设置下等价类期望大小的指数级下界。


<details>
  <summary>Details</summary>
Motivation: 探究因果发现算法在放松无环性、因果充分性和统一模型先验假设时，马尔可夫等价类大小的变化。

Method: 对稀疏随机有向无环图、均匀随机无环有向混合图和均匀随机有向循环图三种设置进行分析，证明马尔可夫等价类期望大小的指数级下界。

Result: 得到三种图设置下马尔可夫等价类期望大小的指数级下界。

Conclusion: 当无环性、因果充分性和统一模型先验假设中的任何一个被放松时，马尔可夫等价类平均大小不再小。

Abstract: Causal discovery algorithms typically recover causal graphs only up to their
Markov equivalence classes unless additional parametric assumptions are made.
The sizes of these equivalence classes reflect the limits of what can be
learned about the underlying causal graph from purely observational data. Under
the assumptions of acyclicity, causal sufficiency, and a uniform model prior,
Markov equivalence classes are known to be small on average. In this paper, we
show that this is no longer the case when any of these assumptions is relaxed.
Specifically, we prove exponentially large lower bounds for the expected size
of Markov equivalence classes in three settings: sparse random directed acyclic
graphs, uniformly random acyclic directed mixed graphs, and uniformly random
directed cyclic graphs.

</details>


### [146] [Homogenization of Multi-agent Learning Dynamics in Finite-state Markov Games](https://arxiv.org/abs/2506.21079)
*Yann Kerzreho*

Main category: stat.ML

TL;DR: 提出新方法近似有限状态马尔可夫博弈中多强化学习智能体学习动态，证明该方法收敛到常微分方程。


<details>
  <summary>Details</summary>
Motivation: 寻找有限状态马尔可夫博弈中多强化学习智能体学习动态的近似方法。

Method: 通过同时降低学习率和增加更新频率来重新调整学习过程，将智能体参数视为受快速混合游戏状态影响的慢变量。

Result: 在温和假设下，证明重新调整后的过程收敛到常微分方程。

Conclusion: 该常微分方程可对智能体学习动态进行易处理的确定性近似。

Abstract: This paper introduces a new approach for approximating the learning dynamics
of multiple reinforcement learning (RL) agents interacting in a finite-state
Markov game. The idea is to rescale the learning process by simultaneously
reducing the learning rate and increasing the update frequency, effectively
treating the agent's parameters as a slow-evolving variable influenced by the
fast-mixing game state. Under mild assumptions-ergodicity of the state process
and continuity of the updates-we prove the convergence of this rescaled process
to an ordinary differential equation (ODE). This ODE provides a tractable,
deterministic approximation of the agent's learning dynamics. An implementation
of the framework is available at\,:
https://github.com/yannKerzreho/MarkovGameApproximation

</details>


### [147] [Hyperspherical Variational Autoencoders Using Efficient Spherical Cauchy Distribution](https://arxiv.org/abs/2506.21278)
*Lukas Sablica,Kurt Hornik*

Main category: stat.ML

TL;DR: 提出采用球面柯西（spCauchy）潜在分布的新型变分自编码器（VAE）架构，具有理论和实际效率优势。


<details>
  <summary>Details</summary>
Motivation: 传统高斯潜在空间和常用的von Mises - Fisher（vMF）分布有不足，如vMF有数值不稳定问题，需更好的潜在分布用于VAE。

Method: 提出采用spCauchy潜在分布的VAE架构，通过莫比乌斯变换实现可微高效的重参数化技巧。

Result: spCauchy能自然表示潜在变量，避免过正则化，消除数值不稳定问题，KL散度可通过快速收敛的幂级数计算。

Conclusion: spCauchy是VAE的有力替代方案，在高维生成建模中有理论和实际效率优势。

Abstract: We propose a novel variational autoencoder (VAE) architecture that employs a
spherical Cauchy (spCauchy) latent distribution. Unlike traditional Gaussian
latent spaces or the widely used von Mises-Fisher (vMF) distribution, spCauchy
provides a more natural hyperspherical representation of latent variables,
better capturing directional data while maintaining flexibility. Its
heavy-tailed nature prevents over-regularization, ensuring efficient latent
space utilization while offering a more expressive representation.
Additionally, spCauchy circumvents the numerical instabilities inherent to vMF,
which arise from computing normalization constants involving Bessel functions.
Instead, it enables a fully differentiable and efficient reparameterization
trick via M\"obius transformations, allowing for stable and scalable training.
The KL divergence can be computed through a rapidly converging power series,
eliminating concerns of underflow or overflow associated with evaluation of
ratios of hypergeometric functions. These properties make spCauchy a compelling
alternative for VAEs, offering both theoretical advantages and practical
efficiency in high-dimensional generative modeling.

</details>


### [148] [Wild refitting for black box prediction](https://arxiv.org/abs/2506.21460)
*Martin J. Wainwright*

Main category: stat.ML

TL;DR: 提出一种计算高效的重拟合程序来计算惩罚非参数估计的实例均方预测误差的高概率上界，介绍步骤、理论保证并举例应用。


<details>
  <summary>Details</summary>
Motivation: 为计算惩罚非参数估计的实例均方预测误差的高概率上界提供计算高效的方法。

Method: 提出包含计算残差、对称缩放和解决修正预测问题三个步骤的重拟合程序，使用Rademacher残差对称化。

Result: 在相对温和条件下，为程序性能建立高概率保证，表明合适噪声尺度的重拟合能给出预测误差上界。

Conclusion: 理论分析为程序设计提供指导，且该程序适用于多种问题。

Abstract: We describe and analyze a computionally efficient refitting procedure for
computing high-probability upper bounds on the instance-wise mean-squared
prediction error of penalized nonparametric estimates based on least-squares
minimization. Requiring only a single dataset and black box access to the
prediction method, it consists of three steps: computing suitable residuals,
symmetrizing and scaling them with a pre-factor $\rho$, and using them to
define and solve a modified prediction problem recentered at the current
estimate. We refer to it as wild refitting, since it uses Rademacher residual
symmetrization as in a wild bootstrap variant. Under relatively mild conditions
allowing for noise heterogeneity, we establish a high probability guarantee on
its performance, showing that the wild refit with a suitably chosen wild noise
scale $\rho$ gives an upper bound on prediction error. This theoretical
analysis provides guidance into the design of such procedures, including how
the residuals should be formed, the amount of noise rescaling in the wild
sub-problem needed for upper bounds, and the local stability properties of the
block-box procedure. We illustrate the applicability of this procedure to
various problems, including non-rigid structure-from-motion recovery with
structured matrix penalties; plug-and-play image restoration with deep neural
network priors; and randomized sketching with kernel methods.

</details>


### [149] [Gaussian Invariant Markov Chain Monte Carlo](https://arxiv.org/abs/2506.21511)
*Michalis K. Titsias,Angelos Alexopoulos,Siran Liu,Petros Dellaportas*

Main category: stat.ML

TL;DR: 开发高斯不变版本的采样方法，展示其能得到更高效遍历估计量，通过实例验证新采样器和估计量并给出理论结果。


<details>
  <summary>Details</summary>
Motivation: 提升采样方法的统计效率，解决难以处理的目标分布估计问题。

Method: 开发高斯不变版本的随机游走Metropolis、Metropolis调整Langevin算法和二阶Hessian或流形MALA。

Result: 新采样方法能得到更高效的遍历估计量，在多个实例中取得了最先进的结果。

Conclusion: 高斯不变采样方法在统计效率上表现出色，且给出了几何遍历性和最优缩放的理论结果。

Abstract: We develop sampling methods, which consist of Gaussian invariant versions of
random walk Metropolis (RWM), Metropolis adjusted Langevin algorithm (MALA) and
second order Hessian or Manifold MALA. Unlike standard RWM and MALA we show
that Gaussian invariant sampling can lead to ergodic estimators with improved
statistical efficiency. This is due to a remarkable property of Gaussian
invariance that allows us to obtain exact analytical solutions to the Poisson
equation for Gaussian targets. These solutions can be used to construct
efficient and easy to use control variates for variance reduction of estimators
under any intractable target. We demonstrate the new samplers and estimators in
several examples, including high dimensional targets in latent Gaussian models
where we compare against several advanced methods and obtain state-of-the-art
results. We also provide theoretical results regarding geometric ergodicity,
and an optimal scaling analysis that shows the dependence of the optimal
acceptance rate on the Gaussianity of the target.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [150] [AutoWMM and JAGStree -- R packages for Population Size Estimation on Relational Tree-Structured Data](https://arxiv.org/abs/2506.21023)
*Mallory J Flynn,Paul Gustafson*

Main category: stat.CO

TL;DR: 开发两个R包，分别简化加权乘数法（WMM）和分层贝叶斯模型对树结构的总体规模估计。


<details>
  <summary>Details</summary>
Motivation: WMM理论易理解但执行需大量计算，分层贝叶斯模型实现需专业知识，需工具简化总体规模估计。

Method: 开发AutoWMM包简化WMM估计，开发JAGStree包自动生成合适的JAGS MCMC建模代码。

Result: 成功开发两个R包AutoWMM和JAGStree。

Conclusion: 这两个包有助于在树结构上进行总体规模估计。

Abstract: The weighted multiplier method (WMM) is an extension of the traditional
method of back-calculation method to estimate the size of a target population,
which synthesizes available evidence from multiple subgroups of the target
population with known counts and estimated proportions by leveraging the
tree-structure inherent to the data. Hierarchical Bayesian models offer an
alternative to modeling population size estimation on such a structure, but
require non-trivial theoretical and practical knowledge to implement. While the
theory underlying the WMM methodology may be more accessible to researchers in
diverse fields, a barrier still exists in execution of this method, which
requires significant computation. We develop two \texttt{R} packages to help
facilitate population size estimation on trees using both the WMM and
hierarchical Bayesian modeling; \textit{AutoWMM} simplifies WMM estimation for
any general tree topology, and \textit{JAGStree} automates the creation of
suitable JAGS MCMC modeling code for these same networks.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [151] [Pull-off strength of mushroom-shaped fibrils adhered to rigid substrates](https://arxiv.org/abs/2506.20745)
*C. Betegón,C. Rodríguez,E. Martínez-Pañeda,R. M. McMeeking*

Main category: cond-mat.soft

TL;DR: 本文用计算方法分析蘑菇形纤维粘附体的脱粘行为，发现宽薄帽纤维可增强粘附，中心缺陷会降低脱粘强度，为仿生粘合剂优化提供见解。


<details>
  <summary>Details</summary>
Motivation: 生物纤维结构的粘附特性启发了合成粘附表面的发展，蘑菇形纤维表现出更好的脱粘强度，需对其脱粘行为深入研究以优化仿生粘合剂。

Method: 采用基于Dugdale内聚区模型的计算方法分析蘑菇形纤维在刚性基底上的脱粘行为。

Result: 得到完整的脱粘曲线，脱粘过程在载荷控制下不稳定；宽薄帽纤维可减少应力集中并促进中心脱粘；并非所有几何形状都有中心脱粘，边缘脱粘在一定条件下都可能发生；中心粘附缺陷会显著降低脱粘强度。

Conclusion: 研究结果有助于优化仿生粘合剂和微结构表面，用于各种工程应用。

Abstract: The exceptional adhesion properties of biological fibrillar structures --
such as those found in geckos -- have inspired the development of synthetic
adhesive surfaces. Among these, mushroom-shaped fibrils have demonstrated
superior pull-off strength compared to other geometries. In this study, we
employ a computational approach based on a Dugdale cohesive zone model to
analyze the detachment behavior of these fibrils when adhered to a rigid
substrate. The results provide complete pull-off curves, revealing that the
separation process is inherently unstable under load control, regardless of
whether detachment initiates at the fibril edge or center. Our findings show
that fibrils with a wide, thin mushroom cap effectively reduce stress
concentrations and promote central detachment, leading to enhanced adhesion.
However, detachment from the center is not observed in all geometries, whereas
edge detachment can occur under certain conditions in all cases. Additionally,
we investigate the impact of adhesion defects at the fibril center, showing
that they can significantly reduce pull-off strength, particularly at high
values of the dimensionless parameter \c{hi}. These insights contribute to the
optimization of bio-inspired adhesives and microstructured surfaces for various
engineering applications.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [152] [Evolution and determinants of firm-level systemic risk in local production networks](https://arxiv.org/abs/2506.21426)
*Anna Mancini,Balázs Lengyel,Riccardo Di Clemente,Giulio Cimini*

Main category: physics.soc-ph

TL;DR: 本文研究匈牙利生产网络中企业层面系统性风险的动态和决定因素，发现新冠期间高风险企业结构变化，企业适应性行为使经济更具韧性，国际贸易量影响系统性风险。


<details>
  <summary>Details</summary>
Motivation: 近期危机暴露供应链脆弱性，此前研究忽略企业应对危机时重新连接供应链接的能力，限制对生产网络弹性的理解。

Method: 使用启发式最大熵零模型生成均衡状态下的生产网络集合作为基准。

Result: 新冠期间高系统性风险企业集合发生结构变化，企业适应性行为使经济更具韧性，国际贸易量是系统性风险的重要预测指标，但进出口对本地系统性风险影响相反。

Conclusion: 企业在危机中的适应性行为有助于增强经济韧性，国际贸易量与系统性风险有关，但进出口影响复杂。

Abstract: Recent crises like the COVID-19 pandemic and geopolitical tensions have
exposed vulnerabilities and caused disruptions of supply chains, leading to
product shortages, increased costs, and economic instability. This has prompted
increasing efforts to assess systemic risk, namely the effects of firm
disruptions on entire economies. However, the ability of firms to react to
crises by rewiring their supply links has been largely overlooked, limiting our
understanding of production networks resilience. Here we study dynamics and
determinants of firm-level systemic risk in the Hungarian production network
from 2015 to 2022. We use as benchmark a heuristic maximum entropy null model
that generates an ensemble of production networks at equilibrium, by preserving
the total input (demand) and output (supply) of each firm at the sector level.
We show that the fairly stable set of firms with highest systemic risk
undergoes a structural change during COVID-19, as those enabling economic
exchanges become key players in the economy -- a result which is not reproduced
by the null model. Although the empirical systemic risk aligns well with the
null value until the onset of the pandemic, it becomes significantly smaller
afterwards as the adaptive behavior of firms leads to a more resilient economy.
Furthermore, firms' international trade volume (being a subject of disruption)
becomes a significant predictor of their systemic risk. However, international
links cannot provide an unequivocal explanation for the observed trends, as
imports and exports have opposing effects on local systemic risk through the
supply and demand channels.

</details>


### [153] [The Devil's Dung? Money as a mechanism of generalized reciprocity in human societies](https://arxiv.org/abs/2506.20691)
*Eduardo C. Ferraciolli,Francesco Renzini,Tanya V. Araujo,Flaminio Squazzoni*

Main category: physics.soc-ph

TL;DR: 本文探讨金钱对社会演化的影响，指出其能促进合作，但过度流动性有弊端，货币供应制度对维持互惠很关键。


<details>
  <summary>Details</summary>
Motivation: 虽然金钱常与负面事物关联，但作者想基于Nowak规则探讨金钱在社会演化中促进合作的作用。

Method: 通过进化锦标赛，验证金钱交换是一种进化稳定策略。

Result: 金钱能促进合作，且不依赖直接互惠或声誉机制；但过度流动性会使背叛更有利可图。

Conclusion: 除促进信任和惩罚的制度外，规范货币供应的制度对维持人类群体内和群体间的广义互惠至关重要。

Abstract: St. Francis of Assisi (1181/82-1226) famously called money the devil's dung,
and indeed money is often associated with greed, inequality, and corruption.
Drawing on Nowak's five rules for the evolution of cooperation, we argue here
that money promotes the formation of circuits of generalized reciprocity across
human groups that are fundamental to social evolution. In an evolutionary
tournament, we show that money exchange is an evolutionary stable strategy that
promotes cooperation without relying on the cognitive demands of direct
reciprocity or reputation mechanisms. However, we also find that excessive
liquidity can be detrimental because it can distort the informational value of
money as a signal of past cooperation, making defection more profitable. Our
results suggest that, in addition to institutions that promoted trust and
punishment, the emergence of institutions that regulated the money supply was
key to maintaining generalized reciprocity within and across human groups.

</details>


### [154] [Modeling Income Distribution with the Gause-Witt Population Ecology System](https://arxiv.org/abs/2506.20881)
*Marcelo B. Ribeiro*

Main category: physics.soc-ph

TL;DR: 本文将种群生态与生态系统的Gause - Witt模型应用于经济系统中社会阶层收入分配竞争动态研究，对巴西收入数据应用该模型，结果显示99%和1%收入阶层大多处于稳定共存动态状态。


<details>
  <summary>Details</summary>
Motivation: 将种群生态的Gause - Witt模型应用于经济系统中社会阶层收入分配竞争动态的研究。

Method: 运用Gause - Witt耦合非线性一阶常微分方程数学系统，对巴西收入数据进行分析。

Result: 对巴西收入数据应用Gause - Witt系统，结果表明99%和1%收入阶层大多处于稳定共存的动态状态。

Conclusion: Gause - Witt模型可用于描述经济系统中不同收入阶层的分配竞争动态，且巴西99%和1%收入阶层处于稳定共存状态。

Abstract: This paper presents an empirical application of the Gause-Witt model of
population ecology and ecosystems to the income distribution competitive
dynamics of social classes in economic systems. The Gause-Witt mathematical
system of coupled nonlinear first-order ordinary differential equations
employed to model population of species sharing the same ecological niche and
competing for the same resources was applied to the income data of Brazil.
Previous studies using Brazilian income data from 1981 to 2009 showed that the
complementary cumulative distribution functions built from yearly datasets have
two distinct segments: the lower income region comprising of about 99% of the
population can be represented by the Gompertz curve, whereas the richest 1% is
described by the Pareto power-law. The results of applying the Gause-Witt
system to Brazilian income data in order to describe the distributive
competition dynamics of these two population shares indicate that the 99% and
1% income classes are mostly in the dynamic state of stable coexistence.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [155] [Faster Fixed-Point Methods for Multichain MDPs](https://arxiv.org/abs/2506.20910)
*Matthew Zurek,Yudong Chen*

Main category: math.OC

TL;DR: 研究平均奖励准则下多链马尔可夫决策过程（MDPs）的值迭代（VI）算法，开发算法解决导航子问题以实现更快收敛，得到更好结果并拓展理论基础。


<details>
  <summary>Details</summary>
Motivation: 平均奖励准则下的多链MDPs是基础但理论上有挑战的设定，缺乏收缩性和贝尔曼算子解的非唯一性，且最优策略需解决导航子问题。

Method: 开发更好解决导航子问题的算法。

Result: 获得了比先前工作更好的收敛速度和复杂度衡量，包括平均奖励与折扣问题的新联系、折扣VI的最优不动点方法等。

Conclusion: 为折扣和平均奖励问题带来更快收敛速度，拓展了VI方法的理论基础。

Abstract: We study value-iteration (VI) algorithms for solving general (a.k.a.
multichain) Markov decision processes (MDPs) under the average-reward
criterion, a fundamental but theoretically challenging setting. Beyond the
difficulties inherent to all average-reward problems posed by the lack of
contractivity and non-uniqueness of solutions to the Bellman operator, in the
multichain setting an optimal policy must solve the navigation subproblem of
steering towards the best connected component, in addition to optimizing
long-run performance within each component. We develop algorithms which better
solve this navigational subproblem in order to achieve faster convergence for
multichain MDPs, obtaining improved rates of convergence and sharper measures
of complexity relative to prior work. Many key components of our results are of
potential independent interest, including novel connections between
average-reward and discounted problems, optimal fixed-point methods for
discounted VI which extend to general Banach spaces, new sublinear convergence
rates for the discounted value error, and refined suboptimality decompositions
for multichain MDPs. Overall our results yield faster convergence rates for
discounted and average-reward problems and expand the theoretical foundations
of VI approaches.

</details>


### [156] [Control and optimization for Neural Partial Differential Equations in Supervised Learning](https://arxiv.org/abs/2506.20764)
*Alain Bensoussan,Minh-Binh Tran,Bangjie Wang*

Main category: math.OC

TL;DR: 本文聚焦抛物和双曲系统中算子系数的控制与优化问题，将神经网络视为PDEs，提出对偶系统公式并给出理论证明。


<details>
  <summary>Details</summary>
Motivation: 现有文献未充分探索抛物和双曲系统中算子系数的控制与优化问题，且该问题在神经网络和监督学习中自然出现。

Method: 将神经网络解释为PDEs，把传统ODEs控制问题转化为PDEs控制问题；提出抛物PDEs控制与优化问题的对偶系统公式。

Result: 证明抛物PDEs控制与优化问题存在极小值；研究双曲PDEs控制问题并证明相应近似控制问题解的存在性。

Conclusion: 为抛物和双曲系统中算子系数的控制与优化问题研究奠定基础，有望开发高效数值方案。

Abstract: Although there is a substantial body of literature on control and
optimization problems for parabolic and hyperbolic systems, the specific
problem of controlling and optimizing the coefficients of the associated
operators within such systems has not yet been thoroughly explored. In this
work, we aim to initiate a line of research in control theory focused on
optimizing and controlling the coefficients of these operators-a problem that
naturally arises in the context of neural networks and supervised learning.
  In supervised learning, the primary objective is to transport initial data
toward target data through the layers of a neural network. We propose a novel
perspective: neural networks can be interpreted as partial differential
equations (PDEs). From this viewpoint, the control problem traditionally
studied in the context of ordinary differential equations (ODEs) is
reformulated as a control problem for PDEs, specifically targeting the
optimization and control of coefficients in parabolic and hyperbolic operators.
To the best of our knowledge, this specific problem has not yet been
systematically addressed in the control theory of PDEs.
  To this end, we propose a dual system formulation for the control and
optimization problem associated with parabolic PDEs, laying the groundwork for
the development of efficient numerical schemes in future research. We also
provide a theoretical proof showing that the control and optimization problem
for parabolic PDEs admits minimizers. Finally, we investigate the control
problem associated with hyperbolic PDEs and prove the existence of solutions
for a corresponding approximated control problem.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [157] [IMC-PINN-FE: A Physics-Informed Neural Network for Patient-Specific Left Ventricular Finite Element Modeling with Image Motion Consistency and Biomechanical Parameter Estimation](https://arxiv.org/abs/2506.20696)
*Siyu Mu,Wei Xuan Chan,Choon Hwai Yap*

Main category: physics.med-ph

TL;DR: 提出IMC - PINN - FE框架用于患者特异性左心室生物力学建模，加速计算且提高运动匹配精度。


<details>
  <summary>Details</summary>
Motivation: 传统有限元方法计算昂贵且难以重现心脏运动，需新方法阐明心肌生物力学行为。

Method: 先从影像估计心脏运动并提取运动模式，再通过拟合临床压力测量快速估计心肌刚度和主动张力，最后进行有限元建模。

Result: 计算从数小时加速到数秒，速度提升75倍，平均Dice从0.849提高到0.927，保留现实压力 - 容积行为。

Conclusion: IMC - PINN - FE是一种强大高效的方法，可实现快速、个性化且与影像一致的心脏生物力学建模。

Abstract: Elucidating the biomechanical behavior of the myocardium is crucial for
understanding cardiac physiology, but cannot be directly inferred from clinical
imaging and typically requires finite element (FE) simulations. However,
conventional FE methods are computationally expensive and often fail to
reproduce observed cardiac motions. We propose IMC-PINN-FE, a physics-informed
neural network (PINN) framework that integrates imaged motion consistency (IMC)
with FE modeling for patient-specific left ventricular (LV) biomechanics.
Cardiac motion is first estimated from MRI or echocardiography using either a
pre-trained attention-based network or an unsupervised cyclic-regularized
network, followed by extraction of motion modes. IMC-PINN-FE then rapidly
estimates myocardial stiffness and active tension by fitting clinical pressure
measurements, accelerating computation from hours to seconds compared to
traditional inverse FE. Based on these parameters, it performs FE modeling
across the cardiac cycle at 75x speedup. Through motion constraints, it matches
imaged displacements more accurately, improving average Dice from 0.849 to
0.927, while preserving realistic pressure-volume behavior. IMC-PINN-FE
advances previous PINN-FE models by introducing back-computation of material
properties and better motion fidelity. Using motion from a single subject to
reconstruct shape modes also avoids the need for large datasets and improves
patient specificity. IMC-PINN-FE offers a robust and efficient approach for
rapid, personalized, and image-consistent cardiac biomechanical modeling.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [158] [MegaFold: System-Level Optimizations for Accelerating Protein Structure Prediction Models](https://arxiv.org/abs/2506.20686)
*Hoa La,Ahan Gupta,Alex Morehead,Jianlin Cheng,Minjia Zhang*

Main category: q-bio.BM

TL;DR: 提出跨平台系统MegaFold加速AlphaFold3训练，减少内存使用、提高训练速度并提升可扩展性，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质结构预测模型如AlphaFold3虽有进步但系统成本高，训练可扩展性受限。

Method: 采用提前缓存消除数据管道GPU空闲时间，基于Triton的内核实现内存高效的EvoAttention，深度融合AF3中常见关键小算子。

Result: 在NVIDIA H200和AMD MI250 GPU上评估，MegaFold最多减少1.23倍内存使用，分别最多提高1.73倍和1.62倍迭代训练时间，能处理比PyTorch基线长1.35倍的序列长度。

Conclusion: MegaFold显著提高现代蛋白质折叠模型的可扩展性。

Abstract: Protein structure prediction models such as AlphaFold3 (AF3) push the
frontier of biomolecular modeling by incorporating science-informed
architectural changes to the transformer architecture. However, these advances
come at a steep system cost, introducing: compute- and memory-intensive
operators, 2D attention mechanisms, and retrieval-augmented data pipelines,
which collectively hinder the scalability of AF3 training. In this work, we
present MegaFold, a cross-platform system to accelerate AF3 training. MegaFold
tackles key bottlenecks through ahead-of-time caching to eliminate GPU idle
time from the retrieval-augmented data pipeline, Triton-based kernels for
memory-efficient EvoAttention on heterogeneous devices, and deep fusion for
common and critical small operators in AF3. Evaluation on both NVIDIA H200 and
AMD MI250 GPUs shows that MegaFold reduces peak memory usage of AF3 training by
up to 1.23$\times$ and improves per-iteration training time by up-to
1.73$\times$ and 1.62$\times$ respectively. More importantly, MegaFold enables
training on 1.35$\times$ longer sequence lengths compared to PyTorch baselines
without running out-of-memory, significantly improving the scalability of
modern protein folding models. We open source our code at
https://github.com/Supercomputing-System-AI-Lab/MegaFold/.

</details>


### [159] [CovDocker: Benchmarking Covalent Drug Design with Tasks, Datasets, and Solutions](https://arxiv.org/abs/2506.21085)
*Yangzhe Peng,Kaiyuan Gao,Liang He,Yuheng Cong,Haiguang Liu,Kun He,Lijun Wu*

Main category: q-bio.BM

TL;DR: 介绍了共价对接基准CovDocker，将对接过程分解为三个任务，用先进模型验证其有效性，可推动共价药物设计研究。


<details>
  <summary>Details</summary>
Motivation: 现有对接方法和深度学习方法难以考虑共价键形成及结构变化，需解决这一问题。

Method: 引入CovDocker基准，将共价对接过程分解为三个任务，采用Uni - Mol和Chemformer等先进模型建立基线性能。

Result: 基准能准确预测相互作用位点，对共价结合的分子转变进行建模。

Conclusion: 该基准是推进共价药物设计研究的严格框架，数据驱动方法可加速选择性共价抑制剂发现，解决治疗开发中的关键挑战。

Abstract: Molecular docking plays a crucial role in predicting the binding mode of
ligands to target proteins, and covalent interactions, which involve the
formation of a covalent bond between the ligand and the target, are
particularly valuable due to their strong, enduring binding nature. However,
most existing docking methods and deep learning approaches hardly account for
the formation of covalent bonds and the associated structural changes. To
address this gap, we introduce a comprehensive benchmark for covalent docking,
CovDocker, which is designed to better capture the complexities of covalent
binding. We decompose the covalent docking process into three main tasks:
reactive location prediction, covalent reaction prediction, and covalent
docking. By adapting state-of-the-art models, such as Uni-Mol and Chemformer,
we establish baseline performances and demonstrate the effectiveness of the
benchmark in accurately predicting interaction sites and modeling the molecular
transformations involved in covalent binding. These results confirm the role of
the benchmark as a rigorous framework for advancing research in covalent drug
design. It underscores the potential of data-driven approaches to accelerate
the discovery of selective covalent inhibitors and addresses critical
challenges in therapeutic development.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [160] [Multicontinuum Homogenization for Poroelasticity Model](https://arxiv.org/abs/2506.20890)
*Dmitry Ammosov,Mohammed Al-Kobaisi,Yalchin Efendiev*

Main category: math.NA

TL;DR: 本文用多连续体均匀化方法推导多连续体孔隙弹性模型，给出通用及简化方程，数值结果显示模型精度高。


<details>
  <summary>Details</summary>
Motivation: 孔隙弹性介质特性对比度高，标准均匀化方法因缺乏宏观参数难以给出准确解，需新方法。

Method: 应用多连续体均匀化方法，在过采样区域制定耦合约束胞元问题，推导多连续体方程。

Result: 给出最通用版本和简化方程，不同非均质介质的数值结果显示模型精度高。

Conclusion: 所提出的多连续体孔隙弹性模型具有较高准确性。

Abstract: In this paper, we derive multicontinuum poroelasticity models using the
multicontinuum homogenization method. Poroelasticity models are widely used in
many areas of science and engineering to describe coupled flow and mechanics
processes in porous media. However, in many applications, the properties of
poroelastic media possess high contrast, presenting serious computational
challenges. It is well known that standard homogenization approaches often fail
to give an accurate solution due to the lack of macroscopic parameters.
Multicontinuum approaches allow us to consider such cases by defining several
average states known as continua. In the field of poroelasticity,
multiple-network models arising from the multiple porous media theory are
representatives of these approaches. In this work, we extend previous findings
by deriving the generalized multicontinuum poroelasticity model. We apply the
recently developed multicontinuum homogenization method and provide a rigorous
derivation of multicontinuum equations. For this purpose, we formulate coupled
constraint cell problems in oversampled regions to consider different
homogenized effects. Then, we obtain a multicontinuum expansion of the
fine-scale fields and derive the multicontinuum model supposing the smoothness
of macroscopic variables. We present the most general version of equations and
the simplified ones based on our numerical experiments. Numerical results are
presented for different heterogeneous media cases and demonstrate the high
accuracy of our proposed multicontinuum models.

</details>


### [161] [On Uniform Weighted Deep Polynomial approximation](https://arxiv.org/abs/2506.21306)
*Kingsley Yeon,Steven B. Damelin*

Main category: math.NA

TL;DR: 本文引入并分析一类加权深度多项式逼近器，用于处理具有不对称行为的函数，数值结果显示该框架表现更优，并提出稳定的基于图的参数化策略。


<details>
  <summary>Details</summary>
Motivation: 已有研究表明某些非光滑或奇异函数用有理函数逼近有根指数收敛，多项式逼近只有代数收敛，近期复合多项式架构可恢复指数逼近率，本文要处理具有不对称行为的函数。

Method: 将可学习的深度多项式与单边权重相乘，提出基于图的参数化策略。

Result: 数值显示该框架优于泰勒、切比雪夫和标准深度多项式逼近器，即便使用相同数量的参数。

Conclusion: 所提出的加权深度多项式逼近器能有效处理不对称行为函数，基于图的参数化策略可用于优化逼近器。

Abstract: It is a classical result in rational approximation theory that certain
non-smooth or singular functions, such as $|x|$ and $x^{1/p}$, can be
efficiently approximated using rational functions with root-exponential
convergence in terms of degrees of freedom \cite{Sta, GN}. In contrast,
polynomial approximations admit only algebraic convergence by Jackson's theorem
\cite{Lub2}. Recent work shows that composite polynomial architectures can
recover exponential approximation rates even without smoothness \cite{KY}. In
this work, we introduce and analyze a class of weighted deep polynomial
approximants tailored for functions with asymmetric behavior-growing unbounded
on one side and decaying on the other. By multiplying a learnable deep
polynomial with a one-sided weight, we capture both local non-smoothness and
global growth. We show numerically that this framework outperforms Taylor,
Chebyshev, and standard deep polynomial approximants, even when all use the
same number of parameters. To optimize these approximants in practice, we
propose a stable graph-based parameterization strategy building on \cite{Jar}.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [162] [Poster: Enhancing GNN Robustness for Network Intrusion Detection via Agent-based Analysis](https://arxiv.org/abs/2506.20806)
*Zhonghao Zhan,Huichi Zhou,Hamed Haddadi*

Main category: cs.CR

TL;DR: 论文提出用大语言模型（LLMs）增强图神经网络（GNNs）在网络入侵检测系统（NIDS）中的鲁棒性和泛化能力，实验表明LLM分析可提升NIDS弹性。


<details>
  <summary>Details</summary>
Motivation: GNNs在NIDS中因分布漂移性能下降，且当前鲁棒性评估不现实，缺乏对不同类型对抗攻击的系统分析。

Method: 在代理管道中使用LLMs作为模拟网络安全专家代理，审查网络流数据的图结构，在GNN处理前识别并减轻可疑或受对抗干扰的元素。

Result: 通过设计的框架进行实验，使用多种对抗攻击及物理测试平台收集的数据集，证明集成LLM分析能显著提高基于GNN的NIDS的弹性。

Conclusion: LLM代理有潜力作为入侵检测架构的补充层。

Abstract: Graph Neural Networks (GNNs) show great promise for Network Intrusion
Detection Systems (NIDS), particularly in IoT environments, but suffer
performance degradation due to distribution drift and lack robustness against
realistic adversarial attacks. Current robustness evaluations often rely on
unrealistic synthetic perturbations and lack demonstrations on systematic
analysis of different kinds of adversarial attack, which encompass both
black-box and white-box scenarios. This work proposes a novel approach to
enhance GNN robustness and generalization by employing Large Language Models
(LLMs) in an agentic pipeline as simulated cybersecurity expert agents. These
agents scrutinize graph structures derived from network flow data, identifying
and potentially mitigating suspicious or adversarially perturbed elements
before GNN processing. Our experiments, using a framework designed for
realistic evaluation and testing with a variety of adversarial attacks
including a dataset collected from physical testbed experiments, demonstrate
that integrating LLM analysis can significantly improve the resilience of
GNN-based NIDS against challenges, showcasing the potential of LLM agent as a
complementary layer in intrusion detection architectures.

</details>


### [163] [ZKPROV: A Zero-Knowledge Approach to Dataset Provenance for Large Language Models](https://arxiv.org/abs/2506.20915)
*Mina Namazi,Alexander Nemecek,Erman Ayday*

Main category: cs.CR

TL;DR: 介绍了零知识证明框架ZKPROV，用于验证大语言模型计算来源，实验证明其高效可扩展，有安全保障。


<details>
  <summary>Details</summary>
Motivation: 在敏感领域部署大语言模型时，确保其计算来源完整性面临挑战，尤其在医疗等严格监管领域。

Method: 通过零知识证明将训练模型与授权训练数据集绑定，利用数据集签名元数据和紧凑模型参数承诺。

Result: 实验证明ZKPROV在生成和验证证明时高效可扩展，可用于实际部署。

Conclusion: ZKPROV能在保证数据集机密性的同时，确保可信的数据集来源。

Abstract: As the deployment of large language models (LLMs) grows in sensitive domains,
ensuring the integrity of their computational provenance becomes a critical
challenge, particularly in regulated sectors such as healthcare, where strict
requirements are applied in dataset usage. We introduce ZKPROV, a novel
cryptographic framework that enables zero-knowledge proofs of LLM provenance.
It allows users to verify that a model is trained on a reliable dataset without
revealing sensitive information about it or its parameters. Unlike prior
approaches that focus on complete verification of the training process
(incurring significant computational cost) or depend on trusted execution
environments, ZKPROV offers a distinct balance. Our method cryptographically
binds a trained model to its authorized training dataset(s) through
zero-knowledge proofs while avoiding proof of every training step. By
leveraging dataset-signed metadata and compact model parameter commitments,
ZKPROV provides sound and privacy-preserving assurances that the result of the
LLM is derived from a model trained on the claimed authorized and relevant
dataset. Experimental results demonstrate the efficiency and scalability of the
ZKPROV in generating this proof and verifying it, achieving a practical
solution for real-world deployments. We also provide formal security
guarantees, proving that our approach preserves dataset confidentiality while
ensuring trustworthy dataset provenance.

</details>


### [164] [PhishKey: A Novel Centroid-Based Approach for Enhanced Phishing Detection Using Adaptive HTML Component Extraction](https://arxiv.org/abs/2506.21106)
*Felipe Castaño,Eduardo Fidalgo,Enrique Alegre,Rocio Alaiz-Rodríguez,Raul Orduna,Francesco Zola*

Main category: cs.CR

TL;DR: 本文提出新的钓鱼检测方法PhishKey，结合字符和单词级处理，实验证明其有效，F1分数高且抗攻击能力强。


<details>
  <summary>Details</summary>
Motivation: 钓鱼攻击不断演变，现有检测机制面临适应性、鲁棒性和效率挑战，需新方法应对。

Method: PhishKey采用混合源自动特征提取，结合CNN进行URL分类，用CAPE处理HTML内容，通过软投票集成预测结果。

Result: 在四个数据集上实验，F1分数达98.70%，对注入攻击等对抗性操作有强抵抗力，性能下降极小。

Conclusion: PhishKey是一种有效、准确且可靠的钓鱼检测方法。

Abstract: Phishing attacks pose a significant cybersecurity threat, evolving rapidly to
bypass detection mechanisms and exploit human vulnerabilities. This paper
introduces PhishKey to address the challenges of adaptability, robustness, and
efficiency. PhishKey is a novel phishing detection method using automatic
feature extraction from hybrid sources. PhishKey combines character-level
processing with Convolutional Neural Networks (CNN) for URL classification, and
a Centroid-Based Key Component Phishing Extractor (CAPE) for HTML content at
the word level. CAPE reduces noise and ensures complete sample processing
avoiding crop operations on the input data. The predictions from both modules
are integrated using a soft-voting ensemble to achieve more accurate and
reliable classifications. Experimental evaluations on four state-of-the-art
datasets demonstrate the effectiveness of PhishKey. It achieves up to 98.70% F1
Score and shows strong resistance to adversarial manipulations such as
injection attacks with minimal performance degradation.

</details>


### [165] [Empowering Digital Agriculture: A Privacy-Preserving Framework for Data Sharing and Collaborative Research](https://arxiv.org/abs/2506.20872)
*Osama Zafar,Rosemarie Santa González,Mina Namazi,Alfonso Morales,Erman Ayday*

Main category: cs.CR

TL;DR: 提出隐私保护框架解决数据驱动农业中农民数据共享的隐私问题，经真实数据集验证有效。


<details>
  <summary>Details</summary>
Motivation: 数据驱动农业有潜力但隐私问题阻碍农民数据共享，需解决该障碍。

Method: 结合降维技术（如PCA）和引入拉普拉斯噪声的差分隐私构建框架，支持通过联邦学习训练模型和农民寻找潜在合作者。

Result: 在真实数据集上验证框架能抵御对抗攻击，实用性能与集中式系统相当。

Conclusion: 框架可促进农民合作和研究，支持负责任地利用农业数据，推动数据驱动农业发展。

Abstract: Data-driven agriculture, which integrates technology and data into
agricultural practices, has the potential to improve crop yield, disease
resilience, and long-term soil health. However, privacy concerns, such as
adverse pricing, discrimination, and resource manipulation, deter farmers from
sharing data, as it can be used against them. To address this barrier, we
propose a privacy-preserving framework that enables secure data sharing and
collaboration for research and development while mitigating privacy risks. The
framework combines dimensionality reduction techniques (like Principal
Component Analysis (PCA)) and differential privacy by introducing Laplacian
noise to protect sensitive information. The proposed framework allows
researchers to identify potential collaborators for a target farmer and train
personalized machine learning models either on the data of identified
collaborators via federated learning or directly on the aggregated
privacy-protected data. It also allows farmers to identify potential
collaborators based on similarities. We have validated this on real-life
datasets, demonstrating robust privacy protection against adversarial attacks
and utility performance comparable to a centralized system. We demonstrate how
this framework can facilitate collaboration among farmers and help researchers
pursue broader research objectives. The adoption of the framework can empower
researchers and policymakers to leverage agricultural data responsibly, paving
the way for transformative advances in data-driven agriculture. By addressing
critical privacy challenges, this work supports secure data integration,
fostering innovation and sustainability in agricultural systems.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [166] [Guarding Offices with Maximum Dispersion](https://arxiv.org/abs/2506.21307)
*Sándor P. Fekete,Kai Kobbe,Dominik Krupke,Joseph S. B. Mitchell,Christian Rieck,Christian Scheffer*

Main category: cs.CG

TL;DR: 研究办公室类正交多边形顶点警卫的分散艺术画廊问题，证明特定分散距离判定的NP完备性，给出不同算法。


<details>
  <summary>Details</summary>
Motivation: 研究现实世界平面图特性的办公室类正交多边形的分散艺术画廊问题，最大化警卫间最小距离。

Method: 理论证明NP完备性和NP难，设计多项式时间算法、动态规划算法，比较SAT、CP、MIP求解器。

Result: 证明特定分散距离判定的NP完备性和NP难，给出保证不同分散距离的算法，SAT求解器对随机实例计算高效。

Conclusion: 解决了相关开放问题，问题对任意正交多边形实际可解，SAT求解器效率高。

Abstract: We investigate the Dispersive Art Gallery Problem with vertex guards and
rectangular visibility ($r$-visibility) for a class of orthogonal polygons that
reflect the properties of real-world floor plans: these office-like polygons
consist of rectangular rooms and corridors. In the dispersive variant of the
Art Gallery Problem, the objective is not to minimize the number of guards but
to maximize the minimum geodesic $L_1$-distance between any two guards, called
the dispersion distance.
  Our main contributions are as follows. We prove that determining whether a
vertex guard set can achieve a dispersion distance of $4$ in office-like
polygons is NP-complete, where vertices of the polygon are restricted to
integer coordinates. Additionally, we present a simple worst-case optimal
algorithm that guarantees a dispersion distance of $3$ in polynomial time. Our
complexity result extends to polyominoes, resolving an open question posed by
Rieck and Scheffer (CGTA 2024). When vertex coordinates are allowed to be
rational, we establish analogous results, proving that achieving a dispersion
distance of $2+\varepsilon$ is NP-hard for any $\varepsilon > 0$, while the
classic Art Gallery Problem remains solvable in polynomial time for this class
of polygons. Furthermore, we give a straightforward polynomial-time algorithm
that computes worst-case optimal solutions with a dispersion distance of $2$.
  On the other hand, for the more restricted class of hole-free independent
office-like polygons, we propose a dynamic programming approach that computes
optimal solutions. Moreover, we demonstrate that the problem is practically
tractable for arbitrary orthogonal polygons. To this end, we compare solvers
based on SAT, CP, and MIP formulations. Notably, SAT solvers efficiently
compute optimal solutions for randomly generated instances with up to $1600$
vertices in under $15$s.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [167] [Structural System Identification via Validation and Adaptation](https://arxiv.org/abs/2506.20799)
*Cristian López,Keegan J. Moore*

Main category: math.DS

TL;DR: 提出从数据直接进行结构系统识别、不确定性量化和验证的新方法，经实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 估计控制方程参数值对结合实验数据与科学理论理解、验证和预测复杂系统动态至关重要，需新方法。

Method: 受生成建模框架启发，用神经网络将随机噪声映射到有物理意义的参数，结合已知运动方程获得虚假加速度与真实数据对比，用判别器网络验证学习的参数。

Result: 分析和实际实验显示该方法对不同非线性结构系统的参数估计准确性和模型验证效果。

Conclusion: 所提方法能有效进行结构系统识别、不确定性量化和验证。

Abstract: Estimating the governing equation parameter values is essential for
integrating experimental data with scientific theory to understand, validate,
and predict the dynamics of complex systems. In this work, we propose a new
method for structural system identification (SI), uncertainty quantification,
and validation directly from data. Inspired by generative modeling frameworks,
a neural network maps random noise to physically meaningful parameters. These
parameters are then used in the known equation of motion to obtain fake
accelerations, which are compared to real training data via a mean square error
loss. To simultaneously validate the learned parameters, we use independent
validation datasets. The generated accelerations from these datasets are
evaluated by a discriminator network, which determines whether the output is
real or fake, and guides the parameter-generator network. Analytical and real
experiments show the parameter estimation accuracy and model validation for
different nonlinear structural systems.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [168] [Parallels Between VLA Model Post-Training and Human Motor Learning: Progress, Challenges, and Trends](https://arxiv.org/abs/2506.20966)
*Tian-Yu Xiang,Ao-Qun Jin,Xiao-Hu Zhou,Mei-Jiang Gui,Xiao-Liang Xie,Shi-Qi Liu,Shuang-Yi Wang,Sheng-Bin Duan,Fu-Chao Xie,Wen-Kai Wang,Si-Cheng Wang,Ling-Yun Li,Tian Tu,Zeng-Guang Hou*

Main category: cs.RO

TL;DR: 本文从人类运动学习角度综述视觉 - 语言 - 动作（VLA）模型的后训练策略，介绍分类法，识别关键挑战和趋势。


<details>
  <summary>Details</summary>
Motivation: VLA模型在高精度应用中有性能差距，后训练对使基础模型适应下游应用至关重要，因此研究VLA模型的后训练策略。

Method: 从人类运动学习角度，围绕环境、具身和任务三个维度，引入与人类学习机制对齐的结构化分类法。

Result: 识别出VLA模型后训练的关键挑战和趋势。

Conclusion: 从人类运动学习视角全面概述了当前VLA模型后训练方法，为VLA模型开发提供实用见解。

Abstract: Vision-language-action (VLA) models extend vision-language models (VLM) by
integrating action generation modules for robotic manipulation. Leveraging
strengths of VLM in vision perception and instruction understanding, VLA models
exhibit promising generalization across diverse manipulation tasks. However,
applications demanding high precision and accuracy reveal performance gaps
without further adaptation. Evidence from multiple domains highlights the
critical role of post-training to align foundational models with downstream
applications, spurring extensive research on post-training VLA models. VLA
model post-training aims to address the challenge of improving an embodiment's
ability to interact with the environment for the given tasks, analogous to the
process of humans motor skills acquisition. Accordingly, this paper reviews
post-training strategies for VLA models through the lens of human motor
learning, focusing on three dimensions: environments, embodiments, and tasks. A
structured taxonomy is introduced aligned with human learning mechanisms: (1)
enhancing environmental perception, (2) improving embodiment awareness, (3)
deepening task comprehension, and (4) multi-component integration. Finally, key
challenges and trends in post-training VLA models are identified, establishing
a conceptual framework to guide future research. This work delivers both a
comprehensive overview of current VLA model post-training methods from a human
motor learning perspective and practical insights for VLA model development.
(Project website: https://github.com/AoqunJin/Awesome-VLA-Post-Training)

</details>


### [169] [V2X-REALM: Vision-Language Model-Based Robust End-to-End Cooperative Autonomous Driving with Adaptive Long-Tail Modeling](https://arxiv.org/abs/2506.21041)
*Junwei You,Pei Li,Zhuoyu Jiang,Zilin Huang,Rui Gan,Haotian Shi,Bin Ran*

Main category: cs.RO

TL;DR: 提出V2X - REALM框架用于长尾场景下的协作自动驾驶，有三项核心创新，实验表明其性能显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 解决城市自动驾驶在长尾场景下以及协作环境中的规划和决策鲁棒性挑战。

Method: 提出基于视觉语言模型的V2X - REALM框架，包含提示驱动的长尾场景生成评估管道、门控多场景自适应注意力模块和多任务场景感知对比学习目标。

Result: 在复杂驾驶条件下，V2X - REALM在鲁棒性、语义推理、安全性和规划准确性上显著优于现有基线。

Conclusion: V2X - REALM提升了端到端协作自动驾驶的可扩展性。

Abstract: Ensuring robust planning and decision-making under rare, diverse, and
visually degraded long-tail scenarios remains a fundamental challenge for
autonomous driving in urban environments. This issue becomes more critical in
cooperative settings, where vehicles and infrastructure jointly perceive and
reason across complex environments. To address this challenge, we propose
V2X-REALM, a vision-language model (VLM)-based framework with adaptive
multimodal learning for robust cooperative autonomous driving under long-tail
scenarios. V2X-REALM introduces three core innovations: (i) a prompt-driven
long-tail scenario generation and evaluation pipeline that leverages foundation
models to synthesize realistic long-tail conditions such as snow and fog across
vehicle- and infrastructure-side views, enriching training diversity
efficiently; (ii) a gated multi-scenario adaptive attention module that
modulates the visual stream using scenario priors to recalibrate ambiguous or
corrupted features; and (iii) a multi-task scenario-aware contrastive learning
objective that improves multimodal alignment and promotes cross-scenario
feature separability. Extensive experiments demonstrate that V2X-REALM
significantly outperforms existing baselines in robustness, semantic reasoning,
safety, and planning accuracy under complex, challenging driving conditions,
advancing the scalability of end-to-end cooperative autonomous driving.

</details>


### [170] [WorldVLA: Towards Autoregressive Action World Model](https://arxiv.org/abs/2506.21539)
*Jun Cen,Chaohui Yu,Hangjie Yuan,Yuming Jiang,Siteng Huang,Jiayan Guo,Xin Li,Yibing Song,Hao Luo,Fan Wang,Deli Zhao,Hao Chen*

Main category: cs.RO

TL;DR: 提出WorldVLA统一动作与图像理解和生成，展示其优势，分析动作模型问题并提出解决策略。


<details>
  <summary>Details</summary>
Motivation: 统一动作和图像理解与生成，学习环境物理规律以改进动作生成。

Method: 将Vision - Language - Action (VLA) 模型和世界模型集成到WorldVLA框架，提出注意力掩码策略。

Result: WorldVLA优于独立的动作和世界模型，注意力掩码策略提升动作块生成任务性能。

Conclusion: WorldVLA实现世界模型和动作模型相互增强，注意力掩码策略可解决动作模型自回归生成问题。

Abstract: We present WorldVLA, an autoregressive action world model that unifies action
and image understanding and generation. Our WorldVLA intergrates
Vision-Language-Action (VLA) model and world model in one single framework. The
world model predicts future images by leveraging both action and image
understanding, with the purpose of learning the underlying physics of the
environment to improve action generation. Meanwhile, the action model generates
the subsequent actions based on image observations, aiding in visual
understanding and in turn helps visual generation of the world model. We
demonstrate that WorldVLA outperforms standalone action and world models,
highlighting the mutual enhancement between the world model and the action
model. In addition, we find that the performance of the action model
deteriorates when generating sequences of actions in an autoregressive manner.
This phenomenon can be attributed to the model's limited generalization
capability for action prediction, leading to the propagation of errors from
earlier actions to subsequent ones. To address this issue, we propose an
attention mask strategy that selectively masks prior actions during the
generation of the current action, which shows significant performance
improvement in the action chunk generation task.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [171] [Evaluating PDE discovery methods for multiscale modeling of biological signals](https://arxiv.org/abs/2506.20694)
*Andréa Ducos,Audrey Denizot,Thomas Guyet,Hugues Berry*

Main category: q-bio.QM

TL;DR: 本文结合基于粒子的模拟和偏微分方程（PDE）发现方法，评估五种PDE发现方法在星形胶质细胞钙扩散模拟中的表现，结果显示部分方法能准确恢复扩散项。


<details>
  <summary>Details</summary>
Motivation: 生物系统具有非线性、存在未观测变量且物理原理部分未知，其活动在多时空尺度上相互依赖，表征其行为具有挑战，需弥合尺度间的差距。

Method: 提出结合基于粒子的模拟和PDE发现的框架，在受控环境中评估五种最先进的PDE发现方法。

Result: 几种方法能准确恢复扩散项。

Conclusion: PDE发现方法在从微观数据捕获生物系统宏观动力学方面有潜力。

Abstract: Biological systems are non-linear, include unobserved variables and the
physical principles that govern their dynamics are partly unknown. This makes
the characterization of their behavior very challenging. Notably, their
activity occurs on multiple interdependent spatial and temporal scales that
require linking mechanisms across scales. To address the challenge of bridging
gaps between scales, we leverage partial differential equations (PDE)
discovery. PDE discovery suggests meso-scale dynamics characteristics from
micro-scale data. In this article, we present our framework combining
particle-based simulations and PDE discovery and conduct preliminary
experiments to assess equation discovery in controlled settings. We evaluate
five state-of-the-art PDE discovery methods on particle-based simulations of
calcium diffusion in astrocytes. The performances of the methods are evaluated
on both the form of the discovered equation and the forecasted temporal
variations of calcium concentration. Our results show that several methods
accurately recover the diffusion term, highlighting the potential of PDE
discovery for capturing macroscopic dynamics in biological systems from
microscopic data.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [172] [Performance improvement of spatial semantic segmentation with enriched audio features and agent-based error correction for DCASE 2025 Challenge Task 4](https://arxiv.org/abs/2506.21174)
*Jongyeon Park,Joonhee Lee,Do-Hyeon Lim,Hong Kook Kim,Hyeongcheol Geum,Jeong Eun Lim*

Main category: eess.AS

TL;DR: 介绍DCASE 2025挑战任务4的提交系统，通过加入额外音频特征、应用标签校正系统和优化训练数据集提升性能，实验显示CA - SDRi相对基线最多提升14.7%。


<details>
  <summary>Details</summary>
Motivation: 混合音频含细微线索，仅靠梅尔频谱图难捕捉；少量异类数据会降低模型性能。

Method: 在梅尔频谱特征提取的嵌入特征中加入额外音频特征；应用基于代理的标签校正系统；去除无关样本并引入外部数据优化训练数据集。

Result: 采用这些方法的提交系统相对DCASE 2025挑战任务4的基线，CA - SDRi最多提升14.7%。

Conclusion: 通过加入额外特征、校正标签和优化数据集的方法能有效提升音频分类性能。

Abstract: This technical report presents submission systems for Task 4 of the DCASE
2025 Challenge. This model incorporates additional audio features (spectral
roll-off and chroma features) into the embedding feature extracted from the
mel-spectral feature to im-prove the classification capabilities of an
audio-tagging model in the spatial semantic segmentation of sound scenes (S5)
system. This approach is motivated by the fact that mixed audio often contains
subtle cues that are difficult to capture with mel-spectrograms alone. Thus,
these additional features offer alterna-tive perspectives for the model.
Second, an agent-based label correction system is applied to the outputs
processed by the S5 system. This system reduces false positives, improving the
final class-aware signal-to-distortion ratio improvement (CA-SDRi) metric.
Finally, we refine the training dataset to enhance the classi-fication accuracy
of low-performing classes by removing irrele-vant samples and incorporating
external data. That is, audio mix-tures are generated from a limited number of
data points; thus, even a small number of out-of-class data points could
degrade model performance. The experiments demonstrate that the submit-ted
systems employing these approaches relatively improve CA-SDRi by up to 14.7%
compared to the baseline of DCASE 2025 Challenge Task 4.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [173] [When Servers Meet Species: A Fab-to-Grave Lens on Computing's Biodiversity Impact](https://arxiv.org/abs/2506.20442)
*Tianyao Shi,Ritbik Kumar,Inez Hua,Yi Ding*

Main category: cs.CY

TL;DR: 本文首次对计算系统的生物多样性影响进行端到端分析，引入新指标和建模框架，强调可持续计算设计需考虑生物多样性。


<details>
  <summary>Details</summary>
Motivation: 以往计算领域的可持续性工作忽视了生物多样性，缺乏合适指标和建模框架，本文旨在研究计算系统与生物多样性的联系。

Method: 引入体现生物多样性指数（EBI）和运营生物多样性指数（OBI）两个新指标，提出将计算工作负载与生物多样性影响联系起来的建模框架FABRIC。

Result: 评估强调了在可持续计算设计和优化中，除碳和水外还需考虑生物多样性。

Conclusion: 在可持续计算设计和优化中应将生物多样性与碳和水一同考虑。

Abstract: Biodiversity loss is a critical planetary boundary, yet its connection to
computing remains largely unexamined. Prior sustainability efforts in computing
have focused on carbon and water, overlooking biodiversity due to the lack of
appropriate metrics and modeling frameworks. This paper presents the first
end-to-end analysis of biodiversity impact from computing systems. We introduce
two new metrics--Embodied Biodiversity Index (EBI) and Operational Biodiversity
Index (OBI)--to quantify biodiversity impact across the lifecycle, and present
FABRIC, a modeling framework that links computing workloads to biodiversity
impacts. Our evaluation highlights the need to consider biodiversity alongside
carbon and water in sustainable computing design and optimization. The code is
available at https://github.com/TianyaoShi/FABRIC.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [174] [Benchmarking and Parallelization of Electrostatic Particle-In-Cell for low-temperature Plasma Simulation by particle-thread Binding](https://arxiv.org/abs/2506.21524)
*Libn Varghese,Bhaskar Chaudhury,Miral Shah,Mainak Bandyopadhyay*

Main category: physics.comp-ph

TL;DR: 提出基于粒子线程绑定策略的新方法，提高PIC模拟中电荷沉积的可扩展性和性能，评估显示有良好扩展性且硬件依赖小。


<details>
  <summary>Details</summary>
Motivation: 二维和三维设备级PIC模拟计算成本高，电荷沉积子程序常成瓶颈，传统方法有可扩展性问题。

Method: 提出基于粒子线程绑定策略的新方法，仅需少量私有网格，用额外函数和标志避免并发问题。

Result: 使用PIC基准测试，在共享和分布式内存系统上证明方法可扩展性，且硬件依赖小。

Conclusion: 该方法在保持传统数据结构和最小化代码改动下，提升了电荷沉积可扩展性和性能。

Abstract: The Particle-In-Cell (PIC) method for plasma simulation tracks particle phase
space information using particle and grid data structures. High computational
costs in 2D and 3D device-scale PIC simulations necessitate parallelization,
with the Charge Deposition (CD) subroutine often becoming a bottleneck due to
frequent particle-grid interactions. Conventional methods mitigate dependencies
by generating private grids for each core, but this approach faces scalability
issues. We propose a novel approach based on a particle-thread binding strategy
that requires only four private grids per node in distributed memory systems or
four private grids in shared memory systems, enhancing CD scalability and
performance while maintaining conventional data structures and requiring
minimal changes to existing PIC codes. This method ensures complete
accessibility of grid data structure for concurrent threads and avoids
simultaneous access to particles within the same cell using additional
functions and flags. Performance evaluations using a PIC benchmark for
low-temperature partially magnetized E x B discharge simulation on a shared
memory as well as a distributed memory system (1000 cores) demonstrate the
method's scalability, and additionally, we show the method has little hardware
dependency.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [175] [Enhancing Homophily-Heterophily Separation: Relation-Aware Learning in Heterogeneous Graphs](https://arxiv.org/abs/2506.20980)
*Ziyu Zheng,Yaming Yang,Ziyu Guan,Wei Zhao,Weigang Lu*

Main category: cs.SI

TL;DR: 提出RASH框架处理异质图中的节点异质性问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 异质性问题在异质图中研究不足，现有方法转换图类型会丢失潜在异质性信息。

Method: 提出RASH对比学习框架，引入双异质超图，动态构建同质性和异质性图，设计多关系对比损失函数。

Result: 在基准数据集的各种下游任务上证明了RASH的有效性。

Conclusion: RASH能同时解决异质图中异质性和异质性问题。

Abstract: Real-world networks usually have a property of node heterophily, that is, the
connected nodes usually have different features or different labels. This
heterophily issue has been extensively studied in homogeneous graphs but
remains under-explored in heterogeneous graphs, where there are multiple types
of nodes and edges. Capturing node heterophily in heterogeneous graphs is very
challenging since both node/edge heterogeneity and node heterophily should be
carefully taken into consideration. Existing methods typically convert
heterogeneous graphs into homogeneous ones to learn node heterophily, which
will inevitably lose the potential heterophily conveyed by heterogeneous
relations. To bridge this gap, we propose Relation-Aware Separation of
Homophily and Heterophily (RASH), a novel contrastive learning framework that
explicitly models high-order semantics of heterogeneous interactions and
adaptively separates homophilic and heterophilic patterns. Particularly, RASH
introduces dual heterogeneous hypergraphs to encode multi-relational bipartite
subgraphs and dynamically constructs homophilic graphs and heterophilic graphs
based on relation importance. A multi-relation contrastive loss is designed to
align heterogeneous and homophilic/heterophilic views by maximizing mutual
information. In this way, RASH simultaneously resolves the challenges of
heterogeneity and heterophily in heterogeneous graphs. Extensive experiments on
benchmark datasets demonstrate the effectiveness of RASH across various
downstream tasks. The code is available at:
https://github.com/zhengziyu77/RASH.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [176] [Efficacy of Temporal Fusion Transformers for Runoff Simulation](https://arxiv.org/abs/2506.20831)
*Sinan Rasiya Koya,Tirthankar Roy*

Main category: physics.geo-ph

TL;DR: 研究对比TFT和LSTM在降雨 - 径流建模中的表现，发现TFT略优且能处理长序列，可识别关键变量，但两者在Caravan数据集上性能下降。


<details>
  <summary>Details</summary>
Motivation: 探索Temporal Fusion Transformers (TFTs)相对Long Short - Term Memory (LSTM)网络在降雨 - 径流建模中的优势。

Method: 训练10个随机初始化的TFT和LSTM模型用于美国531个CAMELS集水区，并在Caravan数据集的五个子集上重复实验，评估模型性能、集水区属性变异性和数据集差异。

Result: TFT略优于LSTM，尤其在模拟水文图中部和峰值时；TFT能处理更长序列，适用于更高或更大集水区；TFT可识别关键动态和静态变量；两者在Caravan数据集上性能显著下降。

Conclusion: 强调TFT在改进水文建模和理解方面的潜力。

Abstract: Combining attention with recurrence has shown to be valuable in sequence
modeling, including hydrological predictions. Here, we explore the strength of
Temporal Fusion Transformers (TFTs) over Long Short-Term Memory (LSTM) networks
in rainfall-runoff modeling. We train ten randomly initialized models, TFT and
LSTM, for 531 CAMELS catchments in the US. We repeat the experiment with five
subsets of the Caravan dataset, each representing catchments in the US,
Australia, Brazil, Great Britain, and Chile. Then, the performance of the
models, their variability regarding the catchment attributes, and the
difference according to the datasets are assessed. Our findings show that TFT
slightly outperforms LSTM, especially in simulating the midsection and peak of
hydrographs. Furthermore, we show the ability of TFT to handle longer sequences
and why it can be a better candidate for higher or larger catchments. Being an
explainable AI technique, TFT identifies the key dynamic and static variables,
providing valuable scientific insights. However, both TFT and LSTM exhibit a
considerable drop in performance with the Caravan dataset, indicating possible
data quality issues. Overall, the study highlights the potential of TFT in
improving hydrological modeling and understanding.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [177] [Quantum Reinforcement Learning Trading Agent for Sector Rotation in the Taiwan Stock Market](https://arxiv.org/abs/2506.20930)
*Chi-Sheng Chen,Xinyu Zhang,Ya-Chuan Chen*

Main category: quant-ph

TL;DR: 提出混合量子 - 经典强化学习框架用于台湾股市板块轮动，实证发现量子增强模型训练奖励高但实际投资指标不如经典模型，指出奖励与投资目标不匹配问题并提出改进方向。


<details>
  <summary>Details</summary>
Motivation: 探索量子强化学习在台湾股市板块轮动中的应用，解决传统方法在金融领域的局限。

Method: 采用近端策略优化（PPO）为骨干算法，集成经典架构（LSTM、Transformer）和量子增强模型（QNN、QRWKV、QASA）作为策略和价值网络，用自动化特征工程管道提取财务指标。

Result: 量子增强模型训练奖励高，但在实际投资指标如累积回报和夏普比率上不如经典模型。

Conclusion: 当前奖励设计可能导致过拟合短期波动，而非优化风险调整回报，提出奖励塑造、模型正则化和基于验证的早停等改进方向，为量子强化学习在金融领域应用提供基准和见解。

Abstract: We propose a hybrid quantum-classical reinforcement learning framework for
sector rotation in the Taiwan stock market. Our system employs Proximal Policy
Optimization (PPO) as the backbone algorithm and integrates both classical
architectures (LSTM, Transformer) and quantum-enhanced models (QNN, QRWKV,
QASA) as policy and value networks. An automated feature engineering pipeline
extracts financial indicators from capital share data to ensure consistent
model input across all configurations. Empirical backtesting reveals a key
finding: although quantum-enhanced models consistently achieve higher training
rewards, they underperform classical models in real-world investment metrics
such as cumulative return and Sharpe ratio. This discrepancy highlights a core
challenge in applying reinforcement learning to financial domains -- namely,
the mismatch between proxy reward signals and true investment objectives. Our
analysis suggests that current reward designs may incentivize overfitting to
short-term volatility rather than optimizing risk-adjusted returns. This issue
is compounded by the inherent expressiveness and optimization instability of
quantum circuits under Noisy Intermediate-Scale Quantum (NISQ) constraints. We
discuss the implications of this reward-performance gap and propose directions
for future improvement, including reward shaping, model regularization, and
validation-based early stopping. Our work offers a reproducible benchmark and
critical insights into the practical challenges of deploying quantum
reinforcement learning in real-world finance.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [178] [Nonparametric Bayesian analysis for the Galton-Watson process](https://arxiv.org/abs/2506.21304)
*Massimo Cannas,Michele Guindani,Nicola Piras*

Main category: stat.ME

TL;DR: 提出用狄利克雷过程先验对高尔顿 - 沃森模型进行贝叶斯推断的非参数方法，通过模拟比较性能，用新冠数据示例。


<details>
  <summary>Details</summary>
Motivation: 现有高尔顿 - 沃森过程推断多关注后代均值，需新方法进行贝叶斯推断并考虑数据过分散性。

Method: 采用狄利克雷过程先验进行贝叶斯推断，通过模拟与频率论和贝叶斯程序比较。

Result: 使用狄利克雷过程先验在完整和不完整数据下都有良好分类性能。

Conclusion: 所提非参数方法在高尔顿 - 沃森模型贝叶斯推断中有效，可用于实际数据分析。

Abstract: The Galton-Watson process is a model for population growth which assumes that
individuals reproduce independently according to the same offspring
distribution. Inference usually focuses on the offspring average as it allows
to classify the process with respect to extinction. We propose a fully
non-parametric approach for Bayesian inference on the GW model using a
Dirichlet Process prior. The prior naturally generalizes the Dirichlet
conjugate prior distribution, and it allows learning the support of the
offspring distribution from the data as well as taking into account possible
overdispersion of the data. The performance of the proposed approach is
compared with both frequentist and Bayesian procedures via simulation. In
particular, we show that the use of a DP prior yields good classification
performance with both complete and incomplete data. A real-world data example
concerning COVID-19 data from Sardinia illustrates the use of the approach in
practice.

</details>


### [179] [Transformer-Based Spatial-Temporal Counterfactual Outcomes Estimation](https://arxiv.org/abs/2506.21154)
*He Li,Haoang Chi,Mingyu Liu,Wanrong Huang,Liyang Xu,Wenjing Yang*

Main category: stat.ME

TL;DR: 本文提出用Transformer估计含时空属性反事实结果的新框架，经实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 现实有时间和空间维度，以往基于经典统计模型的方法在性能和泛化性上有局限，需更好方法估计含时空属性的反事实结果。

Method: 提出基于Transformer估计含时空属性反事实结果的新框架。

Result: 模拟实验表明估计器比基线方法有更强估计能力；真实数据实验得出冲突对哥伦比亚森林损失因果效应的有价值结论。

Conclusion: 所提框架有效，其估计器在温和假设下具有一致性和渐近正态性。

Abstract: The real world naturally has dimensions of time and space. Therefore,
estimating the counterfactual outcomes with spatial-temporal attributes is a
crucial problem. However, previous methods are based on classical statistical
models, which still have limitations in performance and generalization. This
paper proposes a novel framework for estimating counterfactual outcomes with
spatial-temporal attributes using the Transformer, exhibiting stronger
estimation ability. Under mild assumptions, the proposed estimator within this
framework is consistent and asymptotically normal. To validate the
effectiveness of our approach, we conduct simulation experiments and real data
experiments. Simulation experiments show that our estimator has a stronger
estimation capability than baseline methods. Real data experiments provide a
valuable conclusion to the causal effect of conflicts on forest loss in
Colombia. The source code is available at
https://github.com/lihe-maxsize/DeppSTCI_Release_Version-master.

</details>


<div id='q-bio.CB'></div>

# q-bio.CB [[Back]](#toc)

### [180] [scMamba: A Scalable Foundation Model for Single-Cell Multi-Omics Integration Beyond Highly Variable Feature Selection](https://arxiv.org/abs/2506.20697)
*Zhen Yuan,Shaoqing Jiao,Yihang Xiao,Jiajie Peng*

Main category: q-bio.CB

TL;DR: 提出scMamba基础模型用于单细胞多组学数据整合，无需特征选择且保留基因组位置信息，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有单细胞多组学数据整合方法在预处理时选择高变基因或峰可能丢弃关键生物信息，存在挑战。

Method: 引入基于补丁的细胞标记化策略，基于状态空间对偶概念，采用增强余弦相似度正则化的对比学习方法。

Result: 在多个数据集的系统基准测试中，scMamba在保留生物变异、对齐组学层和增强下游任务方面显著优于现有方法。

Conclusion: scMamba是大规模单细胞多组学整合的强大工具，可处理大规模图谱并推动生物学发现。

Abstract: The advent of single-cell multi-omics technologies has enabled the
simultaneous profiling of diverse omics layers within individual cells.
Integrating such multimodal data provides unprecedented insights into cellular
identity, regulatory processes, and disease mechanisms. However, it remains
challenging, as current methods often rely on selecting highly variable genes
or peaks during preprocessing, which may inadvertently discard crucial
biological information. Here, we present scMamba, a foundation model designed
to integrate single-cell multi-omics data without the need for prior feature
selection while preserving genomic positional information. scMamba introduces a
patch-based cell tokenization strategy that treats genomics regions as words
(tokens) and cells as sentences. Building upon the concept of state space
duality, scMamba distills rich biological insights from high-dimensional,
sparse single-cell multi-omics data. Additionally, our novel contrastive
learning approach, enhanced with cosine similarity regularization, enables
superior alignment across omics layers compared to traditional methods.
Systematic benchmarking across multiple datasets demonstrates that scMamba
significantly outperforms state-of-the-art methods in preserving biological
variation, aligning omics layers, and enhancing key downstream tasks such as
clustering, cell type annotation, and trajectory inference. Our findings
position scMamba as a powerful tool for large-scale single-cell multi-omics
integration, capable of handling large-scale atlases and advancing biological
discovery.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [181] [Metadata Enrichment of Long Text Documents using Large Language Models](https://arxiv.org/abs/2506.20918)
*Manika Lamba,You Peng,Sophie Nikolov,Glen Layne-Worthey,J. Stephen Downie*

Main category: cs.DL

TL;DR: 通过手动和大语言模型结合的方式，对1920 - 2020年哈蒂信托数字图书馆英文长文本文件、论文和学位论文的元数据进行语义丰富和增强，该数据集对多领域研究有价值，此方法对数字存储库有益。


<details>
  <summary>Details</summary>
Motivation: 为推进计算社会科学、数字人文和信息科学等领域的研究，解决数字存储库元数据缺失问题。

Method: 结合手动努力和大语言模型对哈蒂信托数字图书馆相关英文文档元数据进行语义丰富和增强。

Result: 得到了有价值的数据集，引入了额外元数据访问点。

Conclusion: 使用大语言模型丰富元数据对数字存储库特别有益，能增强搜索结果和提高数字存储库的可访问性。

Abstract: In this project, we semantically enriched and enhanced the metadata of long
text documents, theses and dissertations, retrieved from the HathiTrust Digital
Library in English published from 1920 to 2020 through a combination of manual
efforts and large language models. This dataset provides a valuable resource
for advancing research in areas such as computational social science, digital
humanities, and information science. Our paper shows that enriching metadata
using LLMs is particularly beneficial for digital repositories by introducing
additional metadata access points that may not have originally been foreseen to
accommodate various content types. This approach is particularly effective for
repositories that have significant missing data in their existing metadata
fields, enhancing search results and improving the accessibility of the digital
repository.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [182] [Uncertainty-Aware Machine-Learning Framework for Predicting Dislocation Plasticity and Stress-Strain Response in FCC Alloys](https://arxiv.org/abs/2506.20839)
*Jing Luo,Yejun Gu,Yanfei Wang,Xiaolong Ma,Jaafar. A El-Awady*

Main category: cond-mat.mtrl-sci

TL;DR: 本文用MDN模型基于文献实验数据预测位错密度和应力分布，结合位错介导塑性模型进行应力 - 应变预测并量化不确定性，助力合金设计和新材料开发。


<details>
  <summary>Details</summary>
Motivation: 整合现有数据，量化结构材料预测建模中的不确定性，提高机械性能预测的准确性和可靠性，优化合金设计，推动新材料开发。

Method: 利用混合密度网络（MDN）模型，基于文献中的大量实验数据进行训练，预测位错密度和应力分布，并将预测分布的统计参数纳入位错介导塑性模型。

Result: 能够准确进行应力 - 应变预测，并明确量化不确定性。

Conclusion: 该策略提高了机械性能预测的准确性和可靠性，对优化合金设计和新材料开发有重要作用。

Abstract: Machine learning has significantly advanced the understanding and application
of structural materials, with an increasing emphasis on integrating existing
data and quantifying uncertainties in predictive modeling. This study presents
a comprehensive methodology utilizing a mixed density network (MDN) model,
trained on extensive experimental data from literature. This approach uniquely
predicts the distribution of dislocation density, inferred as a latent
variable, and the resulting stress distribution at the grain level. The
incorporation of statistical parameters of those predicted distributions into a
dislocation-mediated plasticity model allows for accurate stress-strain
predictions with explicit uncertainty quantification. This strategy not only
improves the accuracy and reliability of mechanical property predictions but
also plays a vital role in optimizing alloy design, thereby facilitating the
development of new materials in a rapidly evolving industry.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [183] [Exploring the Effects of Chatbot Anthropomorphism and Human Empathy on Human Prosocial Behavior Toward Chatbots](https://arxiv.org/abs/2506.20748)
*Jingshu Li,Zicheng Zhu,Renwen Zhang,Yi-Chieh Lee*

Main category: cs.HC

TL;DR: 研究探讨聊天机器人拟人化对人类亲社会行为的影响，发现人类身份和情感表达可增加亲社会行为，同理心起中介作用，还识别出两种行为动机。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对人类帮助聊天机器人动机因素的探索，填补此研究空白。

Method: 基于CASA框架，进行在线实验（N = 244），让聊天机器人在协作图像标注任务中犯错并解释原因，测量参与者亲社会行为和意图，进行定性分析。

Result: 聊天机器人的人类身份和情感表达增加参与者亲社会行为和意图，同理心起中介作用，定性分析识别出两种行为动机。

Conclusion: 讨论研究结果对理解和促进人类对聊天机器人亲社会行为的意义。

Abstract: Chatbots are increasingly integrated into people's lives and are widely used
to help people. Recently, there has also been growing interest in the reverse
direction-humans help chatbots-due to a wide range of benefits including better
chatbot performance, human well-being, and collaborative outcomes. However,
little research has explored the factors that motivate people to help chatbots.
To address this gap, we draw on the Computers Are Social Actors (CASA)
framework to examine how chatbot anthropomorphism-including human-like
identity, emotional expression, and non-verbal expression-influences human
empathy toward chatbots and their subsequent prosocial behaviors and
intentions. We also explore people's own interpretations of their prosocial
behaviors toward chatbots. We conducted an online experiment (N = 244) in which
chatbots made mistakes in a collaborative image labeling task and explained the
reasons to participants. We then measured participants' prosocial behaviors and
intentions toward the chatbots. Our findings revealed that human identity and
emotional expression of chatbots increased participants' prosocial behavior and
intention toward chatbots, with empathy mediating these effects. Qualitative
analysis further identified two motivations for participants' prosocial
behaviors: empathy for the chatbot and perceiving the chatbot as human-like. We
discuss the implications of these results for understanding and promoting human
prosocial behaviors toward chatbots.

</details>


### [184] [A Systematic Review of Human-AI Co-Creativity](https://arxiv.org/abs/2506.21333)
*Saloni Singh,Koen Hndriks,Drik Heylen,Kim Baraka*

Main category: cs.HC

TL;DR: 对62篇共创系统论文进行系统文献综述，确定系统设计关键维度，得出高用户控制和主动系统优势，提取设计考虑要点，指出当前存在的差距。


<details>
  <summary>Details</summary>
Motivation: 利用先前工作的设计考虑为未来共创系统提供有价值且高效的基础。

Method: 对62篇涵盖多种应用的共创系统论文进行系统文献综述。

Result: 确定系统设计关键维度；高用户控制带来更高满意度、信任和更强的创作成果所有权感；自适应且上下文敏感的主动系统可增强协作；提取24条设计考虑要点。

Conclusion: 尽管有进展，但在早期创意阶段支持和用户适应AI系统方面仍存在重要差距。

Abstract: The co creativity community is making significant progress in developing more
sophisticated and tailored systems to support and enhance human creativity.
Design considerations from prior work can serve as a valuable and efficient
foundation for future systems. To support this effort, we conducted a
systematic literature review of 62 papers on co-creative systems. These papers
cover a diverse range of applications, including visual arts, design, and
writing, where the AI acts not just as a tool but as an active collaborator in
the creative process. From this review, we identified several key dimensions
relevant to system design: phase of the creative process, creative task,
proactive behavior of the system, user control, system embodiment, and AI model
type. Our findings suggest that systems offering high user control lead to
greater satisfaction, trust, and a stronger sense of ownership over creative
outcomes. Furthermore, proactive systems, when adaptive and context sensitive,
can enhance collaboration. We also extracted 24 design considerations,
highlighting the value of encouraging users to externalize their thoughts and
of increasing the system's social presence and transparency to foster trust.
Despite recent advancements, important gaps remain, such as limited support for
early creative phases like problem clarification, and challenges related to
user adaptation to AI systems.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [185] [Rational Miner Behaviour, Protocol Stability, and Time Preference: An Austrian and Game-Theoretic Analysis of Bitcoin's Incentive Environment](https://arxiv.org/abs/2506.20965)
*Craig Steven Wright*

Main category: econ.GN

TL;DR: 本文结合奥地利资本理论与重复博弈理论，研究区块链系统中不同制度条件下矿工的战略行为，指出协议规则可变会带来问题，强调协议不可变的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究区块链系统中不同制度条件下矿工的战略行为。

Method: 运用正式的博弈论分析和奥地利经济学原理。

Result: 协议规则可变会提高有效时间偏好，破坏理性长期规划与合作均衡，使矿工激励从生产性投资转向政治寻租和影响博弈；原始比特币协议是制度锚。

Conclusion: 协议不可变性对恢复战略一致性、企业家信心和可持续网络均衡至关重要。

Abstract: This paper integrates Austrian capital theory with repeated game theory to
examine strategic miner behaviour under different institutional conditions in
blockchain systems. It shows that when protocol rules are mutable, effective
time preference rises, undermining rational long-term planning and cooperative
equilibria. Using formal game-theoretic analysis and Austrian economic
principles, the paper demonstrates how mutable protocols shift miner incentives
from productive investment to political rent-seeking and influence games. The
original Bitcoin protocol is interpreted as an institutional anchor: a fixed
rule-set enabling calculability and low time preference. Drawing on the work of
Bohm-Bawerk, Mises, and Hayek, the argument is made that protocol immutability
is essential for restoring strategic coherence, entrepreneurial confidence, and
sustainable network equilibrium.

</details>


### [186] [Institutional Noise, Strategic Deviation, and Intertemporal Collapse: A Formal Model of Miner Behaviour under Protocol Uncertainty](https://arxiv.org/abs/2506.20992)
*Craig Steven Wright*

Main category: econ.GN

TL;DR: 本文构建博弈论模型研究协议可变性对区块链合作挖矿行为的影响，发现可变协议导致短期主义等问题，认为协议设计应作为制度约束。


<details>
  <summary>Details</summary>
Motivation: 研究协议可变性如何破坏区块链系统中的合作挖矿行为。

Method: 构建具有随机规则冲击的重复博弈框架的形式化博弈论模型，并进行模拟。

Result: 制度规则的微小不确定性会增加时间偏好和战略偏差，可变协议导致短期主义、高贴现率和合作崩溃，模拟识别出参数空间的不稳定区域。

Conclusion: 若要在去中心化系统中实现可持续合作，协议设计必须作为制度经济约束，而非可自由裁量的变量。

Abstract: This paper develops a formal game-theoretic model to examine how protocol
mutability disrupts cooperative mining behaviour in blockchain systems. Using a
repeated game framework with stochastic rule shocks, we show that even minor
uncertainty in institutional rules increases time preference and induces
strategic deviation. Fixed-rule environments support long-term investment and
stable equilibrium strategies; in contrast, mutable protocols lead to
short-termism, higher discounting, and collapse of coordinated engagement.
Simulation results identify instability zones in the parameter space where
rational mining gives way to extractive or arbitrage conduct. These findings
support an Austrian economic interpretation: calculability requires rule
stability. Institutional noise undermines the informational basis for
productive action. We conclude that protocol design must be treated as a
constitutional economic constraint, not a discretionary variable, if
sustainable cooperation is to emerge in decentralised systems.

</details>


### [187] [Suspense and Surprise in European Football](https://arxiv.org/abs/2506.21253)
*Raphael Flepp,Tim Pawlowski,Travis Richardson*

Main category: econ.GN

TL;DR: 本文提议将比赛悬念和惊喜度作为联赛组织者的政策目标，通过模拟和分析大量足球比赛发现平均比赛悬念低于基准，惊喜度与基准一致，还呈现出时间趋势和联赛俱乐部差异。


<details>
  <summary>Details</summary>
Motivation: 提出将比赛悬念和惊喜度作为联赛组织者和管理者的替代政策目标。

Method: 通过模拟得出完美平衡比赛的悬念和惊喜度基准范围，分析欧洲顶级足球联赛超25000场男子比赛和725场女子比赛。

Result: 平均比赛悬念低于基准，尤其是顶级球队；惊喜度与基准一致；男子足球有随时间的细微趋势，男女比赛的联赛和俱乐部有显著差异。

Conclusion: 研究增强了对比赛吸引力源于竞争平衡的理解，有重要政策意义。

Abstract: We propose utilizing match-level suspense and surprise - which capture the
entertainment utility created by competitive balance and outcome uncertainty
for sports spectators - as alternative policy targets for league organizers and
managers. Through simulations, we derive a benchmark range for suspense and
surprise based on a perfectly balanced match before analyzing over 25,000 men's
matches (2010/11-2023/24) and 725 women's matches (2023/24) from Europe's top
football leagues. Our findings reveal that an average match generates lower
suspense compared to the benchmark range, particularly for top teams, while
surprise values consistently align with the benchmark. Moreover, we observe
nuanced trends over time in men's football and highlight notable differences
across leagues and clubs in both men's and women's competitions. These insights
enhance our understanding of how the attractiveness of matches arises from
competitive balance and carry important policy implications.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [188] [Maximal Matching Matters: Preventing Representation Collapse for Robust Cross-Modal Retrieval](https://arxiv.org/abs/2506.21538)
*Hani Alomari,Anushka Sivakumar,Andrew Zhang,Chris Thomas*

Main category: cs.CV

TL;DR: 传统跨模态图文检索方法有局限，基于集合的方法虽有潜力但面临问题，本文提出新优化方法和损失函数，在数据集上取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以捕捉跨模态的细微多样关系，基于集合的方法存在稀疏监督和集合坍缩问题。

Method: 提出最大对分配相似度优化嵌入集的一对一匹配，引入全局判别损失和集内散度损失。

Result: 在MS - COCO和Flickr30k数据集上取得了当前最优性能，且不依赖外部数据。

Conclusion: 所提方法有效解决了基于集合的跨模态图文检索方法的问题，提升了性能。

Abstract: Cross-modal image-text retrieval is challenging because of the diverse
possible associations between content from different modalities. Traditional
methods learn a single-vector embedding to represent semantics of each sample,
but struggle to capture nuanced and diverse relationships that can exist across
modalities. Set-based approaches, which represent each sample with multiple
embeddings, offer a promising alternative, as they can capture richer and more
diverse relationships. In this paper, we show that, despite their promise,
these set-based representations continue to face issues including sparse
supervision and set collapse, which limits their effectiveness. To address
these challenges, we propose Maximal Pair Assignment Similarity to optimize
one-to-one matching between embedding sets which preserve semantic diversity
within the set. We also introduce two loss functions to further enhance the
representations: Global Discriminative Loss to enhance distinction among
embeddings, and Intra-Set Divergence Loss to prevent collapse within each set.
Our method achieves state-of-the-art performance on MS-COCO and Flickr30k
without relying on external data.

</details>


### [189] [Leveraging Vision-Language Models to Select Trustworthy Super-Resolution Samples Generated by Diffusion Models](https://arxiv.org/abs/2506.20832)
*Cansu Korkmaz,Ahmet Murat Tekalp,Zafer Dogan*

Main category: cs.CV

TL;DR: 本文提出利用视觉语言模型（VLM）从扩散生成的超分辨率（SR）图像集中识别最可信样本的框架，提出信任度分数（TWS）评估，相比传统指标更优。


<details>
  <summary>Details</summary>
Motivation: 回归式SR模型会引入伪影，扩散模型难选最可信解，需解决扩散SR空间的不确定性问题。

Method: 用结构化查询提示VLM评估语义正确性、视觉质量和伪影情况，对排名靠前的SR候选进行集成；提出TWS量化SR可靠性。

Result: TWS与人类偏好强相关，VLM引导的选择TWS值高，比PSNR、LPIPS等传统指标更能反映信息保真度。

Conclusion: 该方法为生成式SR的可信度设定了新基准，具有原则性、可扩展性和通用性。

Abstract: Super-resolution (SR) is an ill-posed inverse problem with many feasible
solutions consistent with a given low-resolution image. On one hand, regressive
SR models aim to balance fidelity and perceptual quality to yield a single
solution, but this trade-off often introduces artifacts that create ambiguity
in information-critical applications such as recognizing digits or letters. On
the other hand, diffusion models generate a diverse set of SR images, but
selecting the most trustworthy solution from this set remains a challenge. This
paper introduces a robust, automated framework for identifying the most
trustworthy SR sample from a diffusion-generated set by leveraging the semantic
reasoning capabilities of vision-language models (VLMs). Specifically, VLMs
such as BLIP-2, GPT-4o, and their variants are prompted with structured queries
to assess semantic correctness, visual quality, and artifact presence. The
top-ranked SR candidates are then ensembled to yield a single trustworthy
output in a cost-effective manner. To rigorously assess the validity of
VLM-selected samples, we propose a novel Trustworthiness Score (TWS) a hybrid
metric that quantifies SR reliability based on three complementary components:
semantic similarity via CLIP embeddings, structural integrity using SSIM on
edge maps, and artifact sensitivity through multi-level wavelet decomposition.
We empirically show that TWS correlates strongly with human preference in both
ambiguous and natural images, and that VLM-guided selections consistently yield
high TWS values. Compared to conventional metrics like PSNR, LPIPS, which fail
to reflect information fidelity, our approach offers a principled, scalable,
and generalizable solution for navigating the uncertainty of the diffusion SR
space. By aligning outputs with human expectations and semantic correctness,
this work sets a new benchmark for trustworthiness in generative SR.

</details>


### [190] [FixCLR: Negative-Class Contrastive Learning for Semi-Supervised Domain Generalization](https://arxiv.org/abs/2506.20841)
*Ha Min Son,Shahbaz Rezaei,Xin Liu*

Main category: cs.CV

TL;DR: 介绍FixCLR用于半监督领域泛化，能结合现有方法提升性能，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有半监督领域泛化方法未显式正则化学习跨所有领域的不变表示，需改进。

Method: 引入FixCLR，改变对比学习的两个关键组件，可添加到现有半监督方法上。

Result: 进行大量未探索的实验，如对比半监督方法改进、预训练与非预训练模型性能、多领域数据集测试。

Conclusion: FixCLR是有效的半监督领域泛化方法，与其他半监督方法结合效果更佳。

Abstract: Semi-supervised domain generalization (SSDG) aims to solve the problem of
generalizing to out-of-distribution data when only a few labels are available.
Due to label scarcity, applying domain generalization methods often
underperform. Consequently, existing SSDG methods combine semi-supervised
learning methods with various regularization terms. However, these methods do
not explicitly regularize to learn domains invariant representations across all
domains, which is a key goal for domain generalization. To address this, we
introduce FixCLR. Inspired by success in self-supervised learning, we change
two crucial components to adapt contrastive learning for explicit domain
invariance regularization: utilization of class information from pseudo-labels
and using only a repelling term. FixCLR can also be added on top of most
existing SSDG and semi-supervised methods for complementary performance
improvements. Our research includes extensive experiments that have not been
previously explored in SSDG studies. These experiments include benchmarking
different improvements to semi-supervised methods, evaluating the performance
of pretrained versus non-pretrained models, and testing on datasets with many
domains. Overall, FixCLR proves to be an effective SSDG method, especially when
combined with other semi-supervised methods.

</details>


### [191] [THIRDEYE: Cue-Aware Monocular Depth Estimation via Brain-Inspired Multi-Stage Fusion](https://arxiv.org/abs/2506.20877)
*Calin Teodor Ioan*

Main category: cs.CV

TL;DR: 提出名为ThirdEye的单目深度估计提示感知管道，利用预训练冻结网络提供单目提示并融合，只需少量微调。


<details>
  <summary>Details</summary>
Motivation: 传统单目深度估计方法隐式学习易忽略人类视觉系统依赖的显式单目提示，需利用这些提示提升性能。

Method: 通过专门、预训练且冻结的网络提供单目提示，在三阶段皮质层次结构中融合提示，使用自适应仓变换器头生成高分辨率视差图。

Result: 文中未给出定量结果，称将在未来版本呈现。

Conclusion: 因提示专家网络冻结，ThirdEye继承大量外部监督，仅需适度微调。

Abstract: Monocular depth estimation methods traditionally train deep models to infer
depth directly from RGB pixels. This implicit learning often overlooks explicit
monocular cues that the human visual system relies on, such as occlusion
boundaries, shading, and perspective. Rather than expecting a network to
discover these cues unaided, we present ThirdEye, a cue-aware pipeline that
deliberately supplies each cue through specialised, pre-trained, and frozen
networks. These cues are fused in a three-stage cortical hierarchy (V1->V2->V3)
equipped with a key-value working-memory module that weights them by
reliability. An adaptive-bins transformer head then produces a high-resolution
disparity map. Because the cue experts are frozen, ThirdEye inherits large
amounts of external supervision while requiring only modest fine-tuning. This
extended version provides additional architectural detail, neuroscientific
motivation, and an expanded experimental protocol; quantitative results will
appear in a future revision.

</details>


### [192] [OmniEval: A Benchmark for Evaluating Omni-modal Models with Visual, Auditory, and Textual Inputs](https://arxiv.org/abs/2506.20960)
*Yiman Zhang,Ziheng Luo,Qiangyu Yan,Wei He,Borui Jiang,Xinghao Chen,Kai Han*

Main category: cs.CV

TL;DR: 本文介绍多模态评估基准OmniEval，具有全模态协作、视频和任务多样性等特点，并对多个模型进行实验，希望为评估多模态理解能力提供平台。


<details>
  <summary>Details</summary>
Motivation: 为评估像MiniCPM - O 2.6这样的多模态模型，设计更有效的评估基准。

Method: 设计突出音视频强耦合的评估任务，采用多种类型视频和丰富多样的问答对，包括不同任务类型和子类型，还引入Grounding任务。

Result: 在OmniEval上对多个多模态模型进行了实验。

Conclusion: OmniEval能为评估模型从各模态上下文构建和理解连贯性的能力提供平台。

Abstract: In this paper, we introduce OmniEval, a benchmark for evaluating
omni-modality models like MiniCPM-O 2.6, which encompasses visual, auditory,
and textual inputs. Compared with existing benchmarks, our OmniEval has several
distinctive features: (i) Full-modal collaboration: We design evaluation tasks
that highlight the strong coupling between audio and video, requiring models to
effectively leverage the collaborative perception of all modalities; (ii)
Diversity of videos: OmniEval includes 810 audio-visual synchronized videos,
285 Chinese videos and 525 English videos; (iii) Diversity and granularity of
tasks: OmniEval contains 2617 question-answer pairs, comprising 1412 open-ended
questions and 1205 multiple-choice questions. These questions are divided into
3 major task types and 12 sub-task types to achieve comprehensive evaluation.
Among them, we introduce a more granular video localization task named
Grounding. Then we conduct experiments on OmniEval with several omni-modality
models. We hope that our OmniEval can provide a platform for evaluating the
ability to construct and understand coherence from the context of all
modalities. Codes and data could be found at https://omnieval.github.io/.

</details>


### [193] [Evidence-based diagnostic reasoning with multi-agent copilot for human pathology](https://arxiv.org/abs/2506.20964)
*Chengkuan Chen,Luca L. Weishaupt,Drew F. K. Williamson,Richard J. Chen,Tong Ding,Bowen Chen,Anurag Vaidya,Long Phi Le,Guillaume Jaume,Ming Y. Lu,Faisal Mahmood*

Main category: cs.CV

TL;DR: 文章介绍了针对计算病理学现有问题提出的PathChat+和SlideSeek，前者表现优于其他模型，后者能对全切片图像自主评估并生成报告。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习计算病理学传统模型未整合自然语言指令和文本上下文，多模态大语言模型存在训练数据不足、多图像理解支持和评估不足、缺乏自主诊断推理能力等局限。

Method: 引入专门为人类病理学设计的MLLM模型PathChat+，在超100万个病理学特定指令样本和近550万个问答回合上训练；推出借助PathChat+的推理多智能体AI系统SlideSeek进行迭代、分层诊断推理。

Result: PathChat+在多个病理学基准测试中大幅超越之前的PathChat及其他模型；SlideSeek在DDxBench基准测试中达到高准确率，还能生成可视化、可解释的总结报告。

Conclusion: PathChat+和SlideSeek在计算病理学领域有显著优势和应用价值。

Abstract: Pathology is experiencing rapid digital transformation driven by whole-slide
imaging and artificial intelligence (AI). While deep learning-based
computational pathology has achieved notable success, traditional models
primarily focus on image analysis without integrating natural language
instruction or rich, text-based context. Current multimodal large language
models (MLLMs) in computational pathology face limitations, including
insufficient training data, inadequate support and evaluation for multi-image
understanding, and a lack of autonomous, diagnostic reasoning capabilities. To
address these limitations, we introduce PathChat+, a new MLLM specifically
designed for human pathology, trained on over 1 million diverse,
pathology-specific instruction samples and nearly 5.5 million question answer
turns. Extensive evaluations across diverse pathology benchmarks demonstrated
that PathChat+ substantially outperforms the prior PathChat copilot, as well as
both state-of-the-art (SOTA) general-purpose and other pathology-specific
models. Furthermore, we present SlideSeek, a reasoning-enabled multi-agent AI
system leveraging PathChat+ to autonomously evaluate gigapixel whole-slide
images (WSIs) through iterative, hierarchical diagnostic reasoning, reaching
high accuracy on DDxBench, a challenging open-ended differential diagnosis
benchmark, while also capable of generating visually grounded,
humanly-interpretable summary reports.

</details>


### [194] [DFVEdit: Conditional Delta Flow Vector for Zero-shot Video Editing](https://arxiv.org/abs/2506.20967)
*Lingling Cai,Kang Zhao,Hangjie Yuan,Xiang Wang,Yingya Zhang,Kejie Huang*

Main category: cs.CV

TL;DR: 提出针对Video DiTs的高效零样本视频编辑方法DFVEdit，提升效率和质量。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑方法用于Video DiTs时计算开销大，需解决该问题。

Method: 通过流变换直接操作干净潜变量，提出CDFV，集成ICA和ER。

Result: 相比基于注意力工程的编辑方法，推理速度至少提升20倍，内存减少85%，在流行Video DiTs上达到SOTA。

Conclusion: DFVEdit是适用于Video DiTs的高效零样本视频编辑方法。

Abstract: The advent of Video Diffusion Transformers (Video DiTs) marks a milestone in
video generation. However, directly applying existing video editing methods to
Video DiTs often incurs substantial computational overhead, due to
resource-intensive attention modification or finetuning. To alleviate this
problem, we present DFVEdit, an efficient zero-shot video editing method
tailored for Video DiTs. DFVEdit eliminates the need for both attention
modification and fine-tuning by directly operating on clean latents via flow
transformation. To be more specific, we observe that editing and sampling can
be unified under the continuous flow perspective. Building upon this
foundation, we propose the Conditional Delta Flow Vector (CDFV) -- a
theoretically unbiased estimation of DFV -- and integrate Implicit Cross
Attention (ICA) guidance as well as Embedding Reinforcement (ER) to further
enhance editing quality. DFVEdit excels in practical efficiency, offering at
least 20x inference speed-up and 85\% memory reduction on Video DiTs compared
to attention-engineering-based editing methods. Extensive quantitative and
qualitative experiments demonstrate that DFVEdit can be seamlessly applied to
popular Video DiTs (e.g., CogVideoX and Wan2.1), attaining state-of-the-art
performance on structural fidelity, spatial-temporal consistency, and editing
quality.

</details>


### [195] [From Cradle to Cane: A Two-Pass Framework for High-Fidelity Lifespan Face Aging](https://arxiv.org/abs/2506.20977)
*Tao Liu,Dafeng Zhang,Gengchen Li,Shizhuo Liu,Yongqi Song,Senmao Li,Shiqi Yang,Boqian Li,Kai Wang,Yaxing Wang*

Main category: cs.CV

TL;DR: 提出基于T2I扩散模型的两阶段人脸老化框架Cradle2Cane，解决现有方法难以平衡年龄准确性和身份一致性的问题，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在整个人生跨度实现逼真无缝转换有困难，难以平衡年龄准确性和身份一致性。

Method: 提出两阶段框架Cradle2Cane，第一阶段引入AdaNI机制解决年龄准确性，第二阶段用两个身份感知嵌入增强身份保留，两阶段端到端联合训练。

Result: 在CelebA - HQ测试数据集上，通过Face++和Qwen - VL协议评估，Cradle2Cane在年龄准确性和身份一致性上优于现有方法。

Conclusion: Cradle2Cane框架有效解决了人脸老化中年龄准确性和身份一致性的平衡问题。

Abstract: Face aging has become a crucial task in computer vision, with applications
ranging from entertainment to healthcare. However, existing methods struggle
with achieving a realistic and seamless transformation across the entire
lifespan, especially when handling large age gaps or extreme head poses. The
core challenge lies in balancing age accuracy and identity preservation--what
we refer to as the Age-ID trade-off. Most prior methods either prioritize age
transformation at the expense of identity consistency or vice versa. In this
work, we address this issue by proposing a two-pass face aging framework, named
Cradle2Cane, based on few-step text-to-image (T2I) diffusion models. The first
pass focuses on solving age accuracy by introducing an adaptive noise injection
(AdaNI) mechanism. This mechanism is guided by including prompt descriptions of
age and gender for the given person as the textual condition. Also, by
adjusting the noise level, we can control the strength of aging while allowing
more flexibility in transforming the face. However, identity preservation is
weakly ensured here to facilitate stronger age transformations. In the second
pass, we enhance identity preservation while maintaining age-specific features
by conditioning the model on two identity-aware embeddings (IDEmb): SVR-ArcFace
and Rotate-CLIP. This pass allows for denoising the transformed image from the
first pass, ensuring stronger identity preservation without compromising the
aging accuracy. Both passes are jointly trained in an end-to-end way. Extensive
experiments on the CelebA-HQ test dataset, evaluated through Face++ and Qwen-VL
protocols, show that our Cradle2Cane outperforms existing face aging methods in
age accuracy and identity consistency.

</details>


### [196] [Segment Anything in Pathology Images with Natural Language](https://arxiv.org/abs/2506.20988)
*Zhixuan Chen,Junlin Hou,Liqi Lin,Yihui Wang,Yequan Bie,Xi Wang,Yanning Zhou,Ronald Cheong Kin Chan,Hao Chen*

Main category: cs.CV

TL;DR: 提出针对病理图像的文本提示分割基础模型PathSegmentor及最大病理分割数据集PathSeg，实验表明其性能优，可助力临床决策，推动精准肿瘤学可解释AI发展。


<details>
  <summary>Details</summary>
Motivation: 解决当前病理图像分割方法因标注数据有限和类别定义受限在临床应用中面临的挑战。

Method: 提出PathSegmentor模型，构建PathSeg数据集，用自然语言提示进行语义分割。

Result: PathSegmentor在准确性、适用性上超越专业模型，整体Dice分数大幅领先，对复杂结构分割有强鲁棒性，能泛化到外部数据集。

Conclusion: PathSegmentor可增强诊断模型可解释性，为临床决策提供支持，推动精准肿瘤学可解释AI发展。

Abstract: Pathology image segmentation is crucial in computational pathology for
analyzing histological features relevant to cancer diagnosis and prognosis.
However, current methods face major challenges in clinical applications due to
limited annotated data and restricted category definitions. To address these
limitations, we propose PathSegmentor, the first text-prompted segmentation
foundation model designed specifically for pathology images. We also introduce
PathSeg , the largest and most comprehensive dataset for pathology
segmentation, built from 17 public sources and containing 275k image-mask-label
triples across 160 diverse categories. With PathSegmentor, users can perform
semantic segmentation using natural language prompts, eliminating the need for
laborious spatial inputs such as points or boxes. Extensive experiments
demonstrate that PathSegmentor outperforms specialized models with higher
accuracy and broader applicability, while maintaining a compact architecture.
It significantly surpasses existing spatial- and text-prompted models by 0.145
and 0.429 in overall Dice scores, respectively, showing strong robustness in
segmenting complex structures and generalizing to external datasets. Moreover,
PathSegmentor's outputs enhance the interpretability of diagnostic models
through feature importance estimation and imaging biomarker discovery, offering
pathologists evidence-based support for clinical decision-making. This work
advances the development of explainable AI in precision oncology.

</details>


### [197] [Multimodal Prompt Alignment for Facial Expression Recognition](https://arxiv.org/abs/2506.21017)
*Fuyan Ma,Yiran He,Bin Sun,Shutao Li*

Main category: cs.CV

TL;DR: 提出MPA - FER框架用于FER，实验显示其优于现有方法，保留预训练模型优势并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有基于VLM的FER方法难以捕捉细粒度的文本 - 视觉关系，无法区分面部表情的细微差异。

Method: 提出多模态提示对齐框架MPA - FER，包括多粒度硬提示生成策略、注入外部知识、原型引导的视觉特征对齐和跨模态全局 - 局部对齐模块。

Result: 在三个FER基准数据集上，该框架优于现有方法，保留预训练模型优势并降低计算成本。

Conclusion: MPA - FER框架能有效解决现有FER方法的问题，提高FER性能。

Abstract: Prompt learning has been widely adopted to efficiently adapt vision-language
models (VLMs) like CLIP for various downstream tasks. Despite their success,
current VLM-based facial expression recognition (FER) methods struggle to
capture fine-grained textual-visual relationships, which are essential for
distinguishing subtle differences between facial expressions. To address this
challenge, we propose a multimodal prompt alignment framework for FER, called
MPA-FER, that provides fine-grained semantic guidance to the learning process
of prompted visual features, resulting in more precise and interpretable
representations. Specifically, we introduce a multi-granularity hard prompt
generation strategy that utilizes a large language model (LLM) like ChatGPT to
generate detailed descriptions for each facial expression. The LLM-based
external knowledge is injected into the soft prompts by minimizing the feature
discrepancy between the soft prompts and the hard prompts. To preserve the
generalization abilities of the pretrained CLIP model, our approach
incorporates prototype-guided visual feature alignment, ensuring that the
prompted visual features from the frozen image encoder align closely with
class-specific prototypes. Additionally, we propose a cross-modal global-local
alignment module that focuses on expression-relevant facial features, further
improving the alignment between textual and visual features. Extensive
experiments demonstrate our framework outperforms state-of-the-art methods on
three FER benchmark datasets, while retaining the benefits of the pretrained
model and minimizing computational costs.

</details>


### [198] [Improving Diffusion-Based Image Editing Faithfulness via Guidance and Scheduling](https://arxiv.org/abs/2506.21045)
*Hansam Cho,Seoung Bum Kim*

Main category: cs.CV

TL;DR: 本文提出FGS方法解决图像编辑中可编辑性和忠实性的权衡问题，实验表明其在保持可编辑性的同时提升了忠实性，且与多种编辑方法兼容。


<details>
  <summary>Details</summary>
Motivation: 文本引导的扩散模型用于图像编辑时，可编辑性和忠实性之间存在权衡，难以取得最优结果。

Method: 提出Faithfulness Guidance and Scheduling (FGS)方法，通过融入忠实性引导来加强输入图像信息的保留，并引入调度策略解决可编辑性和忠实性的不一致问题。

Result: 实验结果显示FGS在保持可编辑性的同时实现了更优的忠实性。

Conclusion: FGS能实现精确、高质量的图像编辑，且与多种编辑方法兼容，适用于不同任务。

Abstract: Text-guided diffusion models have become essential for high-quality image
synthesis, enabling dynamic image editing. In image editing, two crucial
aspects are editability, which determines the extent of modification, and
faithfulness, which reflects how well unaltered elements are preserved.
However, achieving optimal results is challenging because of the inherent
trade-off between editability and faithfulness. To address this, we propose
Faithfulness Guidance and Scheduling (FGS), which enhances faithfulness with
minimal impact on editability. FGS incorporates faithfulness guidance to
strengthen the preservation of input image information and introduces a
scheduling strategy to resolve misalignment between editability and
faithfulness. Experimental results demonstrate that FGS achieves superior
faithfulness while maintaining editability. Moreover, its compatibility with
various editing methods enables precise, high-quality image edits across
diverse tasks.

</details>


### [199] [EgoAdapt: Adaptive Multisensory Distillation and Policy Learning for Efficient Egocentric Perception](https://arxiv.org/abs/2506.21080)
*Sanjoy Chowdhury,Subrata Biswas,Sayan Nag,Tushar Nagarajan,Calvin Murdock,Ishwarya Ananthabhotla,Yijun Qian,Vamsi Krishna Ithapu,Dinesh Manocha,Ruohan Gao*

Main category: cs.CV

TL;DR: 本文提出EgoAdapt框架，可跨不同以自我为中心的感知任务进行高效推理，实验表明该方法能显著提升效率且性能良好。


<details>
  <summary>Details</summary>
Motivation: 现代以自我为中心的多感官感知模型计算成本高，在现实场景尤其是资源受限环境中部署困难。

Method: 引入EgoAdapt框架，进行自适应跨模态蒸馏和策略学习，策略模块可适应特定任务的动作空间。

Result: 在三个以自我为中心的数据集上实验，该方法显著提升效率，减少GMACs达89.09%、参数达82.02%、能量达9.6倍，性能与对应最先进模型相当甚至更优。

Conclusion: EgoAdapt框架能有效解决以自我为中心的感知模型高计算成本问题，在效率和性能上表现出色。

Abstract: Modern perception models, particularly those designed for multisensory
egocentric tasks, have achieved remarkable performance but often come with
substantial computational costs. These high demands pose challenges for
real-world deployment, especially in resource-constrained environments. In this
paper, we introduce EgoAdapt, a framework that adaptively performs cross-modal
distillation and policy learning to enable efficient inference across different
egocentric perception tasks, including egocentric action recognition, active
speaker localization, and behavior anticipation. Our proposed policy module is
adaptable to task-specific action spaces, making it broadly applicable.
Experimental results on three challenging egocentric datasets EPIC-Kitchens,
EasyCom, and Aria Everyday Activities demonstrate that our method significantly
enhances efficiency, reducing GMACs by up to 89.09%, parameters up to 82.02%,
and energy up to 9.6x, while still on-par and in many cases outperforming, the
performance of corresponding state-of-the-art models.

</details>


### [200] [IPFormer-VideoLLM: Enhancing Multi-modal Video Understanding for Multi-shot Scenes](https://arxiv.org/abs/2506.21116)
*Yujia Liang,Jile Jiao,Zhicheng Wang,Xuetao Feng,Zixuan Ye,Yuan Wang,Hao Lu*

Main category: cs.CV

TL;DR: 提出MultiClip - Bench数据集和IPFormer - VideoLLM模型，提升多场景视频理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有VideoLLMs在多镜头场景表现不佳，原因是缺乏多镜头标注数据集且当前模型编码实例特征方式有问题。

Method: 引入MultiClip - Bench数据集，提出IPFormer - VideoLLM模型，通过高效注意力连接器注入实例级特征。

Result: 数据集提升多镜头性能，测试基准可靠，新数据集和模型显著增强多场景视频理解。

Conclusion: 提出的数据集和模型在多场景视频理解上有显著提升，在各视频基准上有优势。

Abstract: Video Large Language Models (VideoLLMs) have demonstrated remarkable
understanding capabilities, but are found struggling to tackle multi-shot
scenarios,e.g., video clips with varying camera angles or scene changes. This
challenge can render failures such as instance identity forgetting and key
frame negligence. In this work, we first attribute the challenge to the lack of
multi-shot annotations among existing datasets and therefore we introduce a new
dataset termed MultiClip-Bench, featuring dense descriptions and
instruction-based question-answering pairs tailored for multi-shot scenarios.
We empirically find that the training set significantly boosts the multi-shot
performance, while the testing benchmark provides a reliable measure of the
model capability in multi-shot scenarios. By further analyzing and discovering
that current models only encode instance features in a discrete or lossy
manner, at the risk of missing identity information, we then contribute a new
model IPFormer-VideoLLM. Its key idea is the injection of instance-level
features as instance prompts through an efficient attention-based connector.
This allows for the aggregation of instance-specific information across scenes.
Experiments demonstrate that our proposed dataset and model not only enhance
the multi-scene video understanding significantly, but also offer distinct
advantages across various video benchmarks.

</details>


### [201] [Robust Deep Learning for Myocardial Scar Segmentation in Cardiac MRI with Noisy Labels](https://arxiv.org/abs/2506.21151)
*Aida Moafi,Danial Moafi,Evgeny M. Mirkes,Gerry P. McCann,Abbas S. Alatrany,Jayanth R. Arnold,Mostafa Mehdipour Ghazi*

Main category: cs.CV

TL;DR: 提出基于微调模型的深度学习流程用于心肌疤痕自动检测和分割，解决标签噪声等问题，性能优于现有模型，为临床应用奠定基础。


<details>
  <summary>Details</summary>
Motivation: 准确分割心脏MRI中的心肌疤痕对临床评估和治疗规划至关重要。

Method: 通过微调先进模型构建深度学习流程，利用Kullback - Leibler损失和大量数据增强解决标签噪声、数据异质性和类别不平衡问题。

Result: 模型在急慢性病例中表现良好，能产生准确平滑的分割，优于nnU - Net，在分布外测试集有强泛化性。

Conclusion: 为心肌疤痕自动量化建立可靠基础，支持深度学习在心脏成像中的广泛临床应用。

Abstract: The accurate segmentation of myocardial scars from cardiac MRI is essential
for clinical assessment and treatment planning. In this study, we propose a
robust deep-learning pipeline for fully automated myocardial scar detection and
segmentation by fine-tuning state-of-the-art models. The method explicitly
addresses challenges of label noise from semi-automatic annotations, data
heterogeneity, and class imbalance through the use of Kullback-Leibler loss and
extensive data augmentation. We evaluate the model's performance on both acute
and chronic cases and demonstrate its ability to produce accurate and smooth
segmentations despite noisy labels. In particular, our approach outperforms
state-of-the-art models like nnU-Net and shows strong generalizability in an
out-of-distribution test set, highlighting its robustness across various
imaging conditions and clinical tasks. These results establish a reliable
foundation for automated myocardial scar quantification and support the broader
clinical adoption of deep learning in cardiac imaging.

</details>


### [202] [Task-Aware KV Compression For Cost-Effective Long Video Understanding](https://arxiv.org/abs/2506.21184)
*Minghao Qin,Yan Shu,Peitian Zhang,Kun Lun,Huaying Yuan,Juenjie Zhou,Shitao Xiao,Bo Zhao,Zheng Liu*

Main category: cs.CV

TL;DR: 提出Video - X^2L解决长视频理解计算成本高问题，经实验表现优于现有方法且节省成本。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在长视频理解中计算成本高，现有KV压缩方法高压缩比下信息损失大。

Method: 引入Video - X^2L，包括双级KV压缩（生成低压缩和高压缩KV）和选择性KV重新加载（解码阶段关键视频块加载低压缩KV，其他用高压缩KV），且无需额外训练，兼容现有KV可压缩MLLMs。

Result: 在多种长视频理解基准测试中，Video - X^2L大幅优于现有KV压缩方法，同时显著节省计算成本。

Conclusion: Video - X^2L能有效解决长视频理解的计算成本问题，在长视频理解任务中有良好表现。

Abstract: Long-video understanding (LVU) remains a severe challenge for existing
multimodal large language models (MLLMs), primarily due to the prohibitive
computational cost. Recent approaches have explored KV compression to mitigate
this issue, but they often suffer from significant information loss at high
compression ratios. In this paper, we introduce Video-X^2L, which flexibly
preserves critical video information for each LVU task. Video-X^2L involves two
key operations. The first one is called bi-level KV compression. During the
MLLM's pre-filling stage, Video-X^2L generates two types of compressed KVs:
low-compression KVs (L-KVs) to capture fine-grained video details and
high-compression KVs (H-KVs) to offer compact video representations. The second
one is called selective KV re-loading. During the MLLM's decoding stage,
Video-X^2L selectively re-loads L-KVs for the most critical video chunks while
using H-KVs for other less important ones. This allows the MLLM to fully
utilize task-specific information while maintaining the overall compactness.
Video-X^2L is simple yet effective: it is free from additional training and
directly compatible with existing KV-compressible MLLMs. We evaluate Video-X^2L
with a variety of popular LVU benchmarks, including VideoMME, MLVU,
LongVideoBench, and VNBench. Our experiment result shows that Video-X^2L
outperforms existing KV-compression methods by a huge advantage while
substantially saving the computation cost.

</details>


### [203] [Transferring disentangled representations: bridging the gap between synthetic and real images](https://arxiv.org/abs/2409.18017)
*Jacopo Dapueto,Nicoletta Noceti,Francesca Odone*

Main category: cs.CV

TL;DR: 研究利用合成数据学习适用于真实数据的解纠缠表示，提出新度量，结果表明表示从合成数据迁移到真实数据有一定解纠缠效果。


<details>
  <summary>Details</summary>
Motivation: 解纠缠表示学习在真实图像上因生成因素相关、分辨率和缺乏真实标签等问题未充分发挥潜力，研究利用合成数据学习通用解纠缠表示的可能性。

Method: 进行广泛实证研究，提出基于干预的可解释新度量来衡量表示中因素编码质量。

Result: 将表示从合成数据迁移到真实数据，一定程度的解纠缠是可能且有效的。

Conclusion: 利用合成数据学习适用于真实数据的解纠缠表示是可行的，新度量可衡量因素编码质量。

Abstract: Developing meaningful and efficient representations that separate the
fundamental structure of the data generation mechanism is crucial in
representation learning. However, Disentangled Representation Learning has not
fully shown its potential on real images, because of correlated generative
factors, their resolution and limited access to ground truth labels.
Specifically on the latter, we investigate the possibility of leveraging
synthetic data to learn general-purpose disentangled representations applicable
to real data, discussing the effect of fine-tuning and what properties of
disentanglement are preserved after the transfer. We provide an extensive
empirical study to address these issues. In addition, we propose a new
interpretable intervention-based metric, to measure the quality of factors
encoding in the representation. Our results indicate that some level of
disentanglement, transferring a representation from synthetic to real data, is
possible and effective.

</details>


### [204] [BitMark for Infinity: Watermarking Bitwise Autoregressive Image Generative Models](https://arxiv.org/abs/2506.21209)
*Louis Kerner,Michel Meintz,Bihe Zhao,Franziska Boenisch,Adam Dziedzic*

Main category: cs.CV

TL;DR: 提出用于Infinity的BitMark位级水印框架，可防止图像生成模型崩溃。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型输出易被复用为训练数据导致模型崩溃，需有效方法检测生成内容。

Method: 在Infinity图像生成过程中，在多个尺度的令牌流位级嵌入水印，影响位以保持视觉保真度和生成速度。

Result: 水印对多种去除技术具有鲁棒性，有高放射性，二次训练模型输出也会携带水印。

Conclusion: 该方法为防止图像生成模型崩溃提供了可靠检测生成输出的有效步骤。

Abstract: State-of-the-art text-to-image models like Infinity generate photorealistic
images at an unprecedented speed. These models operate in a bitwise
autoregressive manner over a discrete set of tokens that is practically
infinite in size. However, their impressive generative power comes with a
growing risk: as their outputs increasingly populate the Internet, they are
likely to be scraped and reused as training data-potentially by the very same
models. This phenomenon has been shown to lead to model collapse, where
repeated training on generated content, especially from the models' own
previous versions, causes a gradual degradation in performance. A promising
mitigation strategy is watermarking, which embeds human-imperceptible yet
detectable signals into generated images-enabling the identification of
generated content. In this work, we introduce BitMark, a robust bitwise
watermarking framework for Infinity. Our method embeds a watermark directly at
the bit level of the token stream across multiple scales (also referred to as
resolutions) during Infinity's image generation process. Our bitwise watermark
subtly influences the bits to preserve visual fidelity and generation speed
while remaining robust against a spectrum of removal techniques. Furthermore,
it exhibits high radioactivity, i.e., when watermarked generated images are
used to train another image generative model, this second model's outputs will
also carry the watermark. The radioactive traces remain detectable even when
only fine-tuning diffusion or image autoregressive models on images watermarked
with our BitMark. Overall, our approach provides a principled step toward
preventing model collapse in image generative models by enabling reliable
detection of generated outputs.

</details>


### [205] [Holistic Surgical Phase Recognition with Hierarchical Input Dependent State Space Models](https://arxiv.org/abs/2506.21330)
*Haoyang Wu,Tsun-Hsuan Wang,Mathias Lechner,Ramin Hasani,Jennifer A. Eckhoff,Paul Pak,Ozanan R. Meireles,Guy Rosman,Yutong Ban,Daniela Rus*

Main category: cs.CV

TL;DR: 提出分层输入依赖状态空间模型用于机器人辅助手术视频分析，性能大幅超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有手术视频分析方法处理长视频效率低，Transformer模型的二次注意力机制受限。

Method: 提出分层输入依赖状态空间模型，包含局部聚合和全局关系模块，用混合离散 - 连续监督策略训练。

Result: 在Cholec80、MICCAI2016和Heichole数据集上分别比现有最优方法提升2.8%、4.3%和12.9%。

Conclusion: 所提方法能有效处理长手术视频，在手术流程分析上表现优异。

Abstract: Surgical workflow analysis is essential in robot-assisted surgeries, yet the
long duration of such procedures poses significant challenges for comprehensive
video analysis. Recent approaches have predominantly relied on transformer
models; however, their quadratic attention mechanism restricts efficient
processing of lengthy surgical videos. In this paper, we propose a novel
hierarchical input-dependent state space model that leverages the linear
scaling property of state space models to enable decision making on full-length
videos while capturing both local and global dynamics. Our framework
incorporates a temporally consistent visual feature extractor, which appends a
state space model head to a visual feature extractor to propagate temporal
information. The proposed model consists of two key modules: a
local-aggregation state space model block that effectively captures intricate
local dynamics, and a global-relation state space model block that models
temporal dependencies across the entire video. The model is trained using a
hybrid discrete-continuous supervision strategy, where both signals of discrete
phase labels and continuous phase progresses are propagated through the
network. Experiments have shown that our method outperforms the current
state-of-the-art methods by a large margin (+2.8% on Cholec80, +4.3% on
MICCAI2016, and +12.9% on Heichole datasets). Code will be publicly available
after paper acceptance.

</details>


### [206] [CA-I2P: Channel-Adaptive Registration Network with Global Optimal Selection](https://arxiv.org/abs/2506.21364)
*Zhixin Cheng,Jiacheng Deng,Xinjun Li,Xiaotian Yin,Bohao Liao,Baoqun Yin,Wenfei Yang,Tianzhu Zhang*

Main category: cs.CV

TL;DR: 针对检测无关的图像与点云配准方法存在的特征通道注意力差异和冗余对应问题，提出CAA和GOS模块，实验证明方法达到了当前最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有检测无关方法中图像和点云特征通道注意力差异会导致匹配结果下降、场景中相似结构会产生冗余对应，影响配准精度。

Method: 提出Channel Adaptive Adjustment Module (CAA)和Global Optimal Selection Module (GOS)，CAA增强模态内特征、抑制跨模态敏感性，GOS用全局优化替代局部选择。

Result: 在RGB - D Scenes V2和7 - Scenes上的实验表明该方法具有优越性。

Conclusion: 所提方法在图像到点云配准中达到了当前最优性能。

Abstract: Detection-free methods typically follow a coarse-to-fine pipeline, extracting
image and point cloud features for patch-level matching and refining dense
pixel-to-point correspondences. However, differences in feature channel
attention between images and point clouds may lead to degraded matching
results, ultimately impairing registration accuracy. Furthermore, similar
structures in the scene could lead to redundant correspondences in cross-modal
matching. To address these issues, we propose Channel Adaptive Adjustment
Module (CAA) and Global Optimal Selection Module (GOS). CAA enhances
intra-modal features and suppresses cross-modal sensitivity, while GOS replaces
local selection with global optimization. Experiments on RGB-D Scenes V2 and
7-Scenes demonstrate the superiority of our method, achieving state-of-the-art
performance in image-to-point cloud registration.

</details>


### [207] [Step-by-Step Video-to-Audio Synthesis via Negative Audio Guidance](https://arxiv.org/abs/2506.20995)
*Akio Hayakawa,Masato Ishii,Takashi Shibuya,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: 提出新颖逐步视频转音频生成方法，可生成多音频轨道，实验显示优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 全面捕捉视频中的所有声音事件，改进现有视频转音频方法。

Method: 采用逐步生成单个音频轨道，每个步骤为有引导的视频到音频合成任务，引入利用预训练模型的训练框架，无需专业配对数据集。

Result: 为单个输入视频生成多个语义不同的音频轨道，合成的复合音频质量高于现有基线。

Conclusion: 所提出的方法在视频转音频任务中表现良好，能生成高质量的复合音频。

Abstract: We propose a novel step-by-step video-to-audio generation method that
sequentially produces individual audio tracks, each corresponding to a specific
sound event in the video. Our approach mirrors traditional Foley workflows,
aiming to capture all sound events induced by a given video comprehensively.
Each generation step is formulated as a guided video-to-audio synthesis task,
conditioned on a target text prompt and previously generated audio tracks. This
design is inspired by the idea of concept negation from prior compositional
generation frameworks. To enable this guided generation, we introduce a
training framework that leverages pre-trained video-to-audio models and
eliminates the need for specialized paired datasets, allowing training on more
accessible data. Experimental results demonstrate that our method generates
multiple semantically distinct audio tracks for a single input video, leading
to higher-quality composite audio synthesis than existing baselines.

</details>


### [208] [TITAN: Query-Token based Domain Adaptive Adversarial Learning](https://arxiv.org/abs/2506.21484)
*Tajamul Ashraf,Janibul Bashir*

Main category: cs.CV

TL;DR: 提出TITAN网络解决无源域自适应目标检测问题，在多个数据集上表现优于现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于师生框架的方法中，教师模型崩溃导致学生模型性能大幅下降，主要源于伪标签噪声大。

Method: 提出TITAN网络，将目标图像分为易、难两个子集，用估计方差策略划分目标域，在师生基线框架中加入基于查询令牌的对抗模块。

Result: 在四个自然图像数据集和两个医学数据集上实验，TITAN性能优于现有SOTA方法，在多个基准上mAP有显著提升。

Conclusion: TITAN网络能有效解决无源域自适应目标检测问题，性能出色。

Abstract: We focus on the source-free domain adaptive object detection (SF-DAOD)
problem when source data is unavailable during adaptation and the model must
adapt to an unlabeled target domain. The majority of approaches for the problem
employ a self-supervised approach using a student-teacher (ST) framework where
pseudo-labels are generated via a source-pretrained model for further
fine-tuning. We observe that the performance of a student model often degrades
drastically, due to the collapse of the teacher model, primarily caused by high
noise in pseudo-labels, resulting from domain bias, discrepancies, and a
significant domain shift across domains. To obtain reliable pseudo-labels, we
propose a Target-based Iterative Query-Token Adversarial Network (TITAN), which
separates the target images into two subsets: those similar to the source
(easy) and those dissimilar (hard). We propose a strategy to estimate variance
to partition the target domain. This approach leverages the insight that higher
detection variances correspond to higher recall and greater similarity to the
source domain. Also, we incorporate query-token-based adversarial modules into
a student-teacher baseline framework to reduce the domain gaps between two
feature representations. Experiments conducted on four natural imaging datasets
and two challenging medical datasets have substantiated the superior
performance of TITAN compared to existing state-of-the-art (SOTA)
methodologies. We report an mAP improvement of +22.7, +22.2, +21.1, and +3.7
percent over the current SOTA on C2F, C2B, S2C, and K2C benchmarks,
respectively.

</details>


### [209] [HybridQ: Hybrid Classical-Quantum Generative Adversarial Network for Skin Disease Image Generation](https://arxiv.org/abs/2506.21015)
*Qingyue Jiao,Kangyu Zheng,Yiyu Shi,Zhiding Liang*

Main category: cs.CV

TL;DR: 本文提出新颖的经典 - 量子潜空间融合技术，构建首个能生成彩色医学图像的经典 - 量子生成对抗网络，在图像生成质量和分类性能提升上表现出色，参数和训练轮数少，还在含硬件噪声的真实量子计算机上验证了模型性能。


<details>
  <summary>Details</summary>
Motivation: 机器学习辅助皮肤病诊断需大量高质量数据，但皮肤病数据集存在类别不平衡等问题，经典生成模型计算资源需求大、训练时间长，现有量子图像生成方法只能生成低质量灰度图像。

Method: 采用新颖的经典 - 量子潜空间融合技术，构建经典 - 量子生成对抗网络。

Result: 模型在图像生成质量和分类性能提升上优于经典深度卷积GAN和现有混合经典 - 量子GAN，与最先进经典生成模型性能提升相当，但参数少超25倍、训练轮数少10倍，在含硬件噪声的真实IBM量子机上表现稳健。

Conclusion: 随着量子硬件发展，量子图像生成前景光明。

Abstract: Machine learning-assisted diagnosis is gaining traction in skin disease
detection, but training effective models requires large amounts of high-quality
data. Skin disease datasets often suffer from class imbalance, privacy
concerns, and object bias, making data augmentation essential. While classical
generative models are widely used, they demand extensive computational
resources and lengthy training time. Quantum computing offers a promising
alternative, but existing quantum-based image generation methods can only yield
grayscale low-quality images. Through a novel classical-quantum latent space
fusion technique, our work overcomes this limitation and introduces the first
classical-quantum generative adversarial network (GAN) capable of generating
color medical images. Our model outperforms classical deep convolutional GANs
and existing hybrid classical-quantum GANs in both image generation quality and
classification performance boost when used as data augmentation. Moreover, the
performance boost is comparable with that achieved using state-of-the-art
classical generative models, yet with over 25 times fewer parameters and 10
times fewer training epochs. Such results suggest a promising future for
quantum image generation as quantum hardware advances. Finally, we demonstrate
the robust performance of our model on real IBM quantum machine with hardware
noise.

</details>


### [210] [Pushing Trade-Off Boundaries: Compact yet Effective Remote Sensing Change Detection](https://arxiv.org/abs/2506.21109)
*Luosheng Xu,Dalin Zhang,Zhaohui Song*

Main category: cs.CV

TL;DR: 本文提出轻量级遥感变化检测模型FlickCD，在减少计算和存储开销的同时保持高精度，代码公开。


<details>
  <summary>Details</summary>
Motivation: 深度学习遥感变化检测模型复杂度和计算需求增加，但精度提升不显著，需要高效轻量级模型以满足星上处理需求。

Method: 提出FlickCD，引入增强差异模块EDM，解码器采用局部 - 全局融合块，利用SWSA和EGSA捕获多尺度语义信息。

Result: 在四个基准数据集上，FlickCD减少一个数量级以上的计算和存储开销，达到SOTA性能或仅有<1% F1的精度损失。

Conclusion: FlickCD能在性能和资源消耗间取得良好平衡，适合遥感变化检测。

Abstract: Remote sensing change detection is essential for monitoring urban expansion,
disaster assessment, and resource management, offering timely, accurate, and
large-scale insights into dynamic landscape transformations. While deep
learning has revolutionized change detection, the increasing complexity and
computational demands of modern models have not necessarily translated into
significant accuracy gains. Instead of following this trend, this study
explores a more efficient approach, focusing on lightweight models that
maintain high accuracy while minimizing resource consumption, which is an
essential requirement for on-satellite processing. To this end, we propose
FlickCD, which means quick flick then get great results, pushing the boundaries
of the performance-resource trade-off. FlickCD introduces an Enhanced
Difference Module (EDM) to amplify critical feature differences between
temporal phases while suppressing irrelevant variations such as lighting and
weather changes, thereby reducing computational costs in the subsequent change
decoder. Additionally, the FlickCD decoder incorporates Local-Global Fusion
Blocks, leveraging Shifted Window Self-Attention (SWSA) and Enhanced Global
Self-Attention (EGSA) to efficiently capture semantic information at multiple
scales, preserving both coarse- and fine-grained changes. Extensive experiments
on four benchmark datasets demonstrate that FlickCD reduces computational and
storage overheads by more than an order of magnitude while achieving
state-of-the-art (SOTA) performance or incurring only a minor (<1\% F1)
accuracy trade-off. The implementation code is publicly available at
https://github.com/xulsh8/FlickCD.

</details>


### [211] [HalluSegBench: Counterfactual Visual Reasoning for Segmentation Hallucination Evaluation](https://arxiv.org/abs/2506.21546)
*Xinzhuo Li,Adheesh Juvekar,Xingyou Liu,Muntasir Wahed,Kiet A. Nguyen,Ismini Lourentzou*

Main category: cs.CV

TL;DR: 提出HalluSegBench基准评估视觉定位中的幻觉，实验显示视觉驱动幻觉更普遍。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言分割模型存在幻觉问题，且现有评估协议有局限，无法诊断关键故障。

Method: 引入HalluSegBench基准，包含1340个反事实实例对和新指标，用于评估视觉定位中的幻觉。

Result: 实验表明视觉驱动幻觉比标签驱动幻觉更普遍，模型常出现错误分割。

Conclusion: 需要使用反事实推理来诊断视觉定位的保真度。

Abstract: Recent progress in vision-language segmentation has significantly advanced
grounded visual understanding. However, these models often exhibit
hallucinations by producing segmentation masks for objects not grounded in the
image content or by incorrectly labeling irrelevant regions. Existing
evaluation protocols for segmentation hallucination primarily focus on label or
textual hallucinations without manipulating the visual context, limiting their
capacity to diagnose critical failures. In response, we introduce
HalluSegBench, the first benchmark specifically designed to evaluate
hallucinations in visual grounding through the lens of counterfactual visual
reasoning. Our benchmark consists of a novel dataset of 1340 counterfactual
instance pairs spanning 281 unique object classes, and a set of newly
introduced metrics that quantify hallucination sensitivity under visually
coherent scene edits. Experiments on HalluSegBench with state-of-the-art
vision-language segmentation models reveal that vision-driven hallucinations
are significantly more prevalent than label-driven ones, with models often
persisting in false segmentation, highlighting the need for counterfactual
reasoning to diagnose grounding fidelity.

</details>


### [212] [Whole-Body Conditioned Egocentric Video Prediction](https://arxiv.org/abs/2506.21552)
*Yutong Bai,Danny Tran,Amir Bar,Yann LeCun,Trevor Darrell,Jitendra Malik*

Main category: cs.CV

TL;DR: 本文训练模型PEVA，基于过去视频和相对3D身体姿势预测以自我为中心的视频，在大规模数据集上训练并设计分层评估协议。


<details>
  <summary>Details</summary>
Motivation: 从人类视角通过视频预测解决建模复杂现实环境和具身智能体行为的挑战。

Method: 基于身体关节层次的运动学姿势轨迹训练自回归条件扩散变压器，在Nymeria数据集上训练，设计分层评估协议。

Result: 未明确提及具体结果

Conclusion: 这是从人类视角用视频预测解决相关挑战的初步尝试。

Abstract: We train models to Predict Ego-centric Video from human Actions (PEVA), given
the past video and an action represented by the relative 3D body pose. By
conditioning on kinematic pose trajectories, structured by the joint hierarchy
of the body, our model learns to simulate how physical human actions shape the
environment from a first-person point of view. We train an auto-regressive
conditional diffusion transformer on Nymeria, a large-scale dataset of
real-world egocentric video and body pose capture. We further design a
hierarchical evaluation protocol with increasingly challenging tasks, enabling
a comprehensive analysis of the model's embodied prediction and control
abilities. Our work represents an initial attempt to tackle the challenges of
modeling complex real-world environments and embodied agent behaviors with
video prediction from the perspective of a human.

</details>


### [213] [A Comprehensive Dataset for Underground Miner Detection in Diverse Scenario](https://arxiv.org/abs/2506.21451)
*Cyrus Addy,Ajay Kumar Gurumadaiah,Yixiang Gao,Kwame Awuah-Offei*

Main category: cs.CV

TL;DR: 本文提出用于矿工检测系统开发和验证的热成像数据集，评估多种目标检测算法，展示热成像用于矿工检测的可行性。


<details>
  <summary>Details</summary>
Motivation: 地下采矿面临安全挑战，应急响应需可靠矿工检测能力，深度学习算法缺适用训练数据集。

Method: 系统采集多种采矿活动和场景的热成像，创建数据集，评估YOLOv8、YOLOv10、YOLO11和RT - DETR等目标检测算法。

Result: 创建了用于矿工检测系统的热成像数据集。

Conclusion: 证明热成像用于矿工检测可行，为关键安全应用的后续研究奠定基础。

Abstract: Underground mining operations face significant safety challenges that make
emergency response capabilities crucial. While robots have shown promise in
assisting with search and rescue operations, their effectiveness depends on
reliable miner detection capabilities. Deep learning algorithms offer potential
solutions for automated miner detection, but require comprehensive training
datasets, which are currently lacking for underground mining environments. This
paper presents a novel thermal imaging dataset specifically designed to enable
the development and validation of miner detection systems for potential
emergency applications. We systematically captured thermal imagery of various
mining activities and scenarios to create a robust foundation for detection
algorithms. To establish baseline performance metrics, we evaluated several
state-of-the-art object detection algorithms including YOLOv8, YOLOv10, YOLO11,
and RT-DETR on our dataset. While not exhaustive of all possible emergency
situations, this dataset serves as a crucial first step toward developing
reliable thermal-based miner detection systems that could eventually be
deployed in real emergency scenarios. This work demonstrates the feasibility of
using thermal imaging for miner detection and establishes a foundation for
future research in this critical safety application.

</details>


### [214] [Evaluation of Traffic Signals for Daily Traffic Pattern](https://arxiv.org/abs/2506.21469)
*Mohammad Shokrolah Shirazi,Hung-Fu Chang*

Main category: cs.CV

TL;DR: 本文提出三种基于转弯运动计数（TMC）的交通信号配置方法，用视觉跟踪系统估计TMC，经模拟实验得出不同信号配置适用情况及区域交通模式分布对信号设计选择的影响。


<details>
  <summary>Details</summary>
Motivation: 转弯运动计数数据对交通信号设计等至关重要，为改进交通流管理提出合适的交通信号配置方法。

Method: 提出动态、静态和混合三种TMC交通信号配置方法，用视觉跟踪系统估计TMC，将相关文件导入仿真软件评估，内置交通生成模块和信号设计模块进行实验。

Result: 初始实验表明90和120秒周期时间对所有交叉口效果最佳，4个交叉口动态信号定时配置表现更好；扩展实验表明区域交通模式分布影响信号设计选择。

Conclusion: 静态方法适用于区域交通均匀分布，混合方法适用于东西和南北交叉口对交通权重高的情况。

Abstract: The turning movement count data is crucial for traffic signal design,
intersection geometry planning, traffic flow, and congestion analysis. This
work proposes three methods called dynamic, static, and hybrid configuration
for TMC-based traffic signals. A vision-based tracking system is developed to
estimate the TMC of six intersections in Las Vegas using traffic cameras. The
intersection design, route (e.g. vehicle movement directions), and signal
configuration files with compatible formats are synthesized and imported into
Simulation of Urban MObility for signal evaluation with realistic data. The
initial experimental results based on estimated waiting times indicate that the
cycle time of 90 and 120 seconds works best for all intersections. In addition,
four intersections show better performance for dynamic signal timing
configuration, and the other two with lower performance have a lower ratio of
total vehicle count to total lanes of the intersection leg. Since daily traffic
flow often exhibits a bimodal pattern, we propose a hybrid signal method that
switches between dynamic and static methods, adapting to peak and off-peak
traffic conditions for improved flow management. So, a built-in traffic
generator module creates vehicle routes for 4 hours, including peak hours, and
a signal design module produces signal schedule cycles according to static,
dynamic, and hybrid methods. Vehicle count distributions are weighted
differently for each zone (i.e., West, North, East, South) to generate diverse
traffic patterns. The extended experimental results for 6 intersections with 4
hours of simulation time imply that zone-based traffic pattern distributions
affect signal design selection. Although the static method works great for
evenly zone-based traffic distribution, the hybrid method works well for highly
weighted traffic at intersection pairs of the West-East and North-South zones.

</details>


### [215] [Towards Reliable Detection of Empty Space: Conditional Marked Point Processes for Object Detection](https://arxiv.org/abs/2506.21486)
*Tobias J. Riedlinger,Kira Maag,Hanno Gottschalk*

Main category: cs.CV

TL;DR: 提出基于空间统计的目标检测模型，可提供区域是否可行驶的置信估计并验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 现有目标检测和分割模型的置信估计常校准不佳，且无法量化检测框外不确定性，在自动驾驶等应用有安全风险。

Method: 提出基于空间统计的目标检测模型，利用标记点过程描述边界框数据，进行基于似然的训练。

Result: 通过校准评估和性能评估证明了方法的有效性。

Conclusion: 所提基于空间统计的目标检测模型能提供区域是否可行驶的明确置信估计。

Abstract: Deep neural networks have set the state-of-the-art in computer vision tasks
such as bounding box detection and semantic segmentation. Object detectors and
segmentation models assign confidence scores to predictions, reflecting the
model's uncertainty in object detection or pixel-wise classification. However,
these confidence estimates are often miscalibrated, as their architectures and
loss functions are tailored to task performance rather than probabilistic
foundation. Even with well calibrated predictions, object detectors fail to
quantify uncertainty outside detected bounding boxes, i.e., the model does not
make a probability assessment of whether an area without detected objects is
truly free of obstacles. This poses a safety risk in applications such as
automated driving, where uncertainty in empty areas remains unexplored. In this
work, we propose an object detection model grounded in spatial statistics.
Bounding box data matches realizations of a marked point process, commonly used
to describe the probabilistic occurrence of spatial point events identified as
bounding box centers, where marks are used to describe the spatial extension of
bounding boxes and classes. Our statistical framework enables a
likelihood-based training and provides well-defined confidence estimates for
whether a region is drivable, i.e., free of objects. We demonstrate the
effectiveness of our method through calibration assessments and evaluation of
performance.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [216] [MultiFinRAG: An Optimized Multimodal Retrieval-Augmented Generation (RAG) Framework for Financial Question Answering](https://arxiv.org/abs/2506.20821)
*Chinmay Gondhalekar,Urjitkumar Patel,Fang-Chun Yeh*

Main category: cs.CL

TL;DR: 提出用于金融问答的MultiFinRAG框架，能跨模态推理，在复杂金融问答任务上比ChatGPT - 4o（免费版）准确率高19个百分点。


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型和检索增强生成管道在处理金融多模态文档问答时，因令牌限制、布局丢失和跨模态上下文碎片化而受限。

Method: MultiFinRAG先进行多模态提取，将表格和图像分组处理得到结构化JSON输出和文本摘要，对输出和文本进行嵌入和索引以便精确检索，采用分层回退策略实现跨模态推理并减少无关上下文。

Result: 在涉及文本、表格、图像和多模态推理的复杂金融问答任务上，MultiFinRAG比ChatGPT - 4o（免费版）准确率高19个百分点。

Conclusion: MultiFinRAG能有效处理金融多模态文档的问答，即使在普通硬件上也有出色表现。

Abstract: Financial documents--such as 10-Ks, 10-Qs, and investor presentations--span
hundreds of pages and combine diverse modalities, including dense narrative
text, structured tables, and complex figures. Answering questions over such
content often requires joint reasoning across modalities, which strains
traditional large language models (LLMs) and retrieval-augmented generation
(RAG) pipelines due to token limitations, layout loss, and fragmented
cross-modal context. We introduce MultiFinRAG, a retrieval-augmented generation
framework purpose-built for financial QA. MultiFinRAG first performs multimodal
extraction by grouping table and figure images into batches and sending them to
a lightweight, quantized open-source multimodal LLM, which produces both
structured JSON outputs and concise textual summaries. These outputs, along
with narrative text, are embedded and indexed with modality-aware similarity
thresholds for precise retrieval. A tiered fallback strategy then dynamically
escalates from text-only to text+table+image contexts when necessary, enabling
cross-modal reasoning while reducing irrelevant context. Despite running on
commodity hardware, MultiFinRAG achieves 19 percentage points higher accuracy
than ChatGPT-4o (free-tier) on complex financial QA tasks involving text,
tables, images, and combined multimodal reasoning.

</details>


### [217] [A Semi-supervised Scalable Unified Framework for E-commerce Query Classification](https://arxiv.org/abs/2506.21049)
*Chunyuan Yuan,Chong Zhang,Zheng Fang,Ming Pang,Xue Jiang,Changping Peng,Zhangang Lin,Ching Law*

Main category: cs.CL

TL;DR: 本文提出半监督可扩展统一框架SSUF解决电商查询分类问题，实验表明其优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 电商查询分类先验信息不足，现有方法存在马太效应，且缺乏统一框架，算法优化效率低。

Method: 提出SSUF框架，包含知识增强、标签增强和结构增强模块，各模块可插拔。

Result: 离线和在线A/B实验显示SSUF显著优于现有模型。

Conclusion: SSUF能有效解决电商查询分类问题，表现出色。

Abstract: Query classification, including multiple subtasks such as intent and category
prediction, is vital to e-commerce applications. E-commerce queries are usually
short and lack context, and the information between labels cannot be used,
resulting in insufficient prior information for modeling. Most existing
industrial query classification methods rely on users' posterior click behavior
to construct training samples, resulting in a Matthew vicious cycle.
Furthermore, the subtasks of query classification lack a unified framework,
leading to low efficiency for algorithm optimization.
  In this paper, we propose a novel Semi-supervised Scalable Unified Framework
(SSUF), containing multiple enhanced modules to unify the query classification
tasks. The knowledge-enhanced module uses world knowledge to enhance query
representations and solve the problem of insufficient query information. The
label-enhanced module uses label semantics and semi-supervised signals to
reduce the dependence on posterior labels. The structure-enhanced module
enhances the label representation based on the complex label relations. Each
module is highly pluggable, and input features can be added or removed as
needed according to each subtask. We conduct extensive offline and online A/B
experiments, and the results show that SSUF significantly outperforms the
state-of-the-art models.

</details>


### [218] [CBF-AFA: Chunk-Based Multi-SSL Fusion for Automatic Fluency Assessment](https://arxiv.org/abs/2506.20243)
*Papa Séga Wade,Mihai Andries,Ioannis Kanellos,Thierry Moudenc*

Main category: cs.CL

TL;DR: 本文提出基于块的方法，融合自监督学习模型与CNN - BiLSTM框架进行自动流利度评估，在两个数据集上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 自动流利度评估具有挑战性，尤其是捕捉非母语者的语音节奏、停顿和不流畅性。

Method: 使用Silero - VAD将语音分割成呼吸组块，融合自监督学习模型（Wav2Vec2、HuBERT、WavLM）的嵌入，通过可学习加权机制平衡特征，并加入块级流利度标记，用CNN - BiLSTM捕捉依赖关系。

Result: 在Avalinguo和Speechocean762数据集上，相比单SSL基线，F1分数和皮尔逊相关性有显著提升，也超越基于Pyannote.audio的分割基线。

Conclusion: 基于块的多SSL融合方法可用于稳健的流利度评估，但未来需探索对韵律不规则方言的泛化能力。

Abstract: Automatic fluency assessment (AFA) remains challenging, particularly in
capturing speech rhythm, pauses, and disfluencies in non-native speakers. We
introduce a chunk-based approach integrating self-supervised learning (SSL)
models (Wav2Vec2, HuBERT, and WavLM) selected for their complementary strengths
in phonetic, prosodic, and noisy speech modeling, with a hierarchical
CNN-BiLSTM framework. Speech is segmented into breath-group chunks using Silero
voice activity detection (Silero-VAD), enabling fine-grained temporal analysis
while mitigating over-segmentation artifacts. SSL embeddings are fused via a
learnable weighted mechanism, balancing acoustic and linguistic features, and
enriched with chunk-level fluency markers (e.g., speech rate, pause durations,
n-gram repetitions). The CNN-BiLSTM captures local and long-term dependencies
across chunks. Evaluated on Avalinguo and Speechocean762, our approach improves
F1-score by 2.8 and Pearson correlation by 6.2 points over single SSL baselines
on Speechocean762, with gains of 4.2 F1-score and 4.0 Pearson points on
Avalinguo, surpassing Pyannote.audio-based segmentation baselines. These
findings highlight chunk-based multi-SSL fusion for robust fluency evaluation,
though future work should explore generalization to dialects with irregular
prosody.

</details>


### [219] [Enhancing Automatic Term Extraction with Large Language Models via Syntactic Retrieval](https://arxiv.org/abs/2506.21222)
*Yongchan Chun,Minhyuk Kim,Dongjun Kim,Chanjun Park,Heuiseok Lim*

Main category: cs.CL

TL;DR: 本文提出基于检索的提示策略用于自动术语提取，实验表明句法检索提升F1分数，强调句法线索重要性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自动术语提取方面的潜力未被充分研究，需要探索其在该任务中的应用。

Method: 提出基于检索的提示策略，在少样本设置下根据句法而非语义相似度选择示例，且该句法检索方法与领域无关。

Result: 在三个专业自动术语提取基准上的实验表明，句法检索提高了F1分数。

Conclusion: 在将大语言模型应用于术语提取任务时，句法线索非常重要。

Abstract: Automatic Term Extraction (ATE) identifies domain-specific expressions that
are crucial for downstream tasks such as machine translation and information
retrieval. Although large language models (LLMs) have significantly advanced
various NLP tasks, their potential for ATE has scarcely been examined. We
propose a retrieval-based prompting strategy that, in the few-shot setting,
selects demonstrations according to \emph{syntactic} rather than semantic
similarity. This syntactic retrieval method is domain-agnostic and provides
more reliable guidance for capturing term boundaries. We evaluate the approach
in both in-domain and cross-domain settings, analyzing how lexical overlap
between the query sentence and its retrieved examples affects performance.
Experiments on three specialized ATE benchmarks show that syntactic retrieval
improves F1-score. These findings highlight the importance of syntactic cues
when adapting LLMs to terminology-extraction tasks.

</details>


### [220] [Small Encoders Can Rival Large Decoders in Detecting Groundedness](https://arxiv.org/abs/2506.21288)
*Istabrak Abbes,Gabriele Prato,Quentin Fournier,Fernando Rodriguez,Alaa Boukhary,Adam Elwood,Sarath Chandar*

Main category: cs.CL

TL;DR: 研究聚焦于在大语言模型生成答案前检测查询是否基于给定文档，轻量级编码器模型在有监督数据集微调后可在接地性检测上媲美大语言模型，还能大幅降低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在上下文信息不足时回答不可靠，接地性对确保事实一致性和可信度至关重要，需在答案生成前检测查询是否基于文档以减少推理时间和资源消耗。

Method: 使用如RoBERTa和NomicBERT等轻量级、特定任务编码器模型，在有监督数据集上进行微调。

Result: 轻量级模型在接地性检测上能达到与Llama3 8B和GPT4o等先进大语言模型相当的准确率，且推理延迟大幅降低。

Conclusion: 轻量级任务特定编码器模型在接地性检测方面具有有效性和优势，可显著提升效率。

Abstract: Augmenting large language models (LLMs) with external context significantly
improves their performance in natural language processing (NLP) tasks. However,
LLMs struggle to answer queries reliably when the provided context lacks
information, often resorting to ungrounded speculation or internal knowledge.
Groundedness - generating responses strictly supported by the context - is
essential for ensuring factual consistency and trustworthiness. This study
focuses on detecting whether a given query is grounded in a document provided
in context before the costly answer generation by LLMs. Such a detection
mechanism can significantly reduce both inference time and resource
consumption. We show that lightweight, task specific encoder models such as
RoBERTa and NomicBERT, fine-tuned on curated datasets, can achieve accuracy
comparable to state-of-the-art LLMs, such as Llama3 8B and GPT4o, in
groundedness detection while reducing inference latency by orders of magnitude.
The code is available at : https://github.com/chandarlab/Hallucinate-less

</details>


### [221] [Maintaining MTEB: Towards Long Term Usability and Reproducibility of Embedding Benchmarks](https://arxiv.org/abs/2506.21182)
*Isaac Chung,Imene Kerboua,Marton Kardos,Roman Solomatin,Kenneth Enevoldsen*

Main category: cs.CL

TL;DR: 本文聚焦MTEB工程方面，以确保其可复现性和可扩展性，介绍相关工程实践及应对策略，经验为基准维护者提供参考。


<details>
  <summary>Details</summary>
Motivation: 确保MTEB持续的可复现性和可扩展性。

Method: 维护强大的持续集成管道，验证数据集完整性、自动化测试执行、评估基准结果泛化性；详细设计以增强可复现性和可用性；讨论处理社区贡献及扩展新任务和数据集的策略。

Result: 工程实践使MTEB更全面，同时保持质量和领域相关性。

Conclusion: 经验为面临类似挑战的机器学习评估框架基准维护者提供有价值见解。

Abstract: The Massive Text Embedding Benchmark (MTEB) has become a standard evaluation
platform for text embedding models. While previous work has established the
core benchmark methodology, this paper focuses on the engineering aspects that
ensure MTEB's continued reproducibility and extensibility. We present our
approach to maintaining robust continuous integration pipelines that validate
dataset integrity, automate test execution, and assess benchmark results'
generalizability. We detail the design choices that collectively enhance
reproducibility and usability. Furthermore, we discuss our strategies for
handling community contributions and extending the benchmark with new tasks and
datasets. These engineering practices have been instrumental in scaling MTEB to
become more comprehensive while maintaining quality and, ultimately, relevance
to the field. Our experiences offer valuable insights for benchmark maintainers
facing similar challenges in ensuring reproducibility and usability in machine
learning evaluation frameworks. The MTEB repository is available at:
https://github.com/embeddings-benchmark/mteb

</details>


### [222] [Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented Generation](https://arxiv.org/abs/2506.21384)
*Guanting Dong,Xiaoxi Li,Yuyao Zhang,Mengjie Deng*

Main category: cs.CL

TL;DR: 论文介绍了用于实时开放领域的Omni - RAG框架，能增强RAG系统处理复杂噪声查询的能力。


<details>
  <summary>Details</summary>
Motivation: 现实世界中实时RAG系统处理复杂、噪声大、多意图用户查询时面临挑战，当前系统在处理此类复杂输入时存在困难。

Method: Omni - RAG通过三个关键模块对用户输入进行预处理，包括深度查询理解与分解、意图感知知识检索、重排序与生成。

Result: 未提及

Conclusion: Omni - RAG旨在弥合当前RAG能力与现实应用需求之间的差距，能稳健处理复杂噪声查询。

Abstract: Real-world live retrieval-augmented generation (RAG) systems face significant
challenges when processing user queries that are often noisy, ambiguous, and
contain multiple intents. While RAG enhances large language models (LLMs) with
external knowledge, current systems typically struggle with such complex
inputs, as they are often trained or evaluated on cleaner data. This paper
introduces Omni-RAG, a novel framework designed to improve the robustness and
effectiveness of RAG systems in live, open-domain settings. Omni-RAG employs
LLM-assisted query understanding to preprocess user inputs through three key
modules: (1) Deep Query Understanding and Decomposition, which utilizes LLMs
with tailored prompts to denoise queries (e.g., correcting spelling errors) and
decompose multi-intent queries into structured sub-queries; (2) Intent-Aware
Knowledge Retrieval, which performs retrieval for each sub-query from a corpus
(i.e., FineWeb using OpenSearch) and aggregates the results; and (3) Reranking
and Generation, where a reranker (i.e., BGE) refines document selection before
a final response is generated by an LLM (i.e., Falcon-10B) using a
chain-of-thought prompt. Omni-RAG aims to bridge the gap between current RAG
capabilities and the demands of real-world applications, such as those
highlighted by the SIGIR 2025 LiveRAG Challenge, by robustly handling complex
and noisy queries.

</details>


### [223] [Text2Cypher Across Languages: Evaluating Foundational Models Beyond English](https://arxiv.org/abs/2506.21445)
*Makbule Gulcin Ozsoy,William Tai*

Main category: cs.CL

TL;DR: 本文研究多语言Text2Cypher任务中基础大模型的性能，创建多语言测试集评估，发现性能英语最高、西班牙语次之、土耳其语最低，提示翻译影响小，强调需更具包容性的评估和开发。


<details>
  <summary>Details</summary>
Motivation: 当前多数自然语言转数据库查询接口的研究聚焦英语，其他语言评估有限，因此研究基础大模型在多语言Text2Cypher任务中的性能。

Method: 创建并发布多语言测试集，将英语问题翻译成西班牙语和土耳其语，用标准化提示和指标评估多个基础模型，还探索任务提示翻译的影响。

Result: 模型性能呈现英语最高、西班牙语次之、土耳其语最低的模式，提示翻译对评估指标影响小。

Conclusion: 强调在多语言查询生成中需要更具包容性的评估和开发，未来工作包括模式本地化和跨多种语言微调。

Abstract: Recent advances in large language models have enabled natural language
interfaces that translate user questions into database queries, such as
Text2SQL, Text2SPARQL, and Text2Cypher. While these interfaces enhance database
accessibility, most research today focuses solely on English, with limited
evaluation in other languages. This paper investigates the performance of
foundational LLMs on the Text2Cypher task across multiple languages. We create
and release a multilingual test set by translating English questions into
Spanish and Turkish while preserving the original Cypher queries, enabling fair
cross-lingual comparison. We evaluate multiple foundational models using
standardized prompts and metrics. Our results show a consistent performance
pattern: highest on English, then Spanish, and lowest on Turkish. We attribute
this to differences in training data availability and linguistic
characteristics. Additionally, we explore the impact of translating task
prompts into Spanish and Turkish. Results show little to no change in
evaluation metrics, suggesting prompt translation has minor impact. Our
findings highlight the need for more inclusive evaluation and development in
multilingual query generation. Future work includes schema localization and
fine-tuning across diverse languages.

</details>


### [224] [skLEP: A Slovak General Language Understanding Benchmark](https://arxiv.org/abs/2506.21508)
*Marek Šuppa,Andrej Ridzik,Daniel Hládek,Tomáš Javůrek,Viktória Ondrejová,Kristína Sásiková,Martin Tamajka,Marián Šimko*

Main category: cs.CL

TL;DR: 本文介绍了首个用于评估斯洛伐克语自然语言理解（NLU）模型的综合基准skLEP，进行了多种模型评估，并发布了相关数据、工具包和排行榜。


<details>
  <summary>Details</summary>
Motivation: 缺乏专门评估斯洛伐克语NLU模型的基准，需要一个全面评估模型能力的工具。

Method: 构建涵盖九种不同任务的skLEP基准，包括整理新数据集和翻译英语资源，使用skLEP任务对多种预训练语言模型进行评估。

Result: 完成了对多种斯洛伐克语、多语言和英语预训练语言模型的评估。

Conclusion: 发布完整基准数据、开源工具包和公共排行榜，以促进斯洛伐克语NLU的可重复性和未来研究。

Abstract: In this work, we introduce skLEP, the first comprehensive benchmark
specifically designed for evaluating Slovak natural language understanding
(NLU) models. We have compiled skLEP to encompass nine diverse tasks that span
token-level, sentence-pair, and document-level challenges, thereby offering a
thorough assessment of model capabilities. To create this benchmark, we curated
new, original datasets tailored for Slovak and meticulously translated
established English NLU resources. Within this paper, we also present the first
systematic and extensive evaluation of a wide array of Slovak-specific,
multilingual, and English pre-trained language models using the skLEP tasks.
Finally, we also release the complete benchmark data, an open-source toolkit
facilitating both fine-tuning and evaluation of models, and a public
leaderboard at https://github.com/slovak-nlp/sklep in the hopes of fostering
reproducibility and drive future research in Slovak NLU.

</details>


### [225] [The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas](https://arxiv.org/abs/2506.20803)
*Chenglei Si,Tatsunori Hashimoto,Diyi Yang*

Main category: cs.CL

TL;DR: 本文研究大语言模型（LLMs）生成的研究想法执行效果，发现执行后其与人类专家想法差距缩小甚至排名反转，凸显当前LLMs生成有效想法的局限。


<details>
  <summary>Details</summary>
Motivation: 测试AI生成的研究想法是否能带来更好的研究成果，因好想法不仅要新颖，还需执行后效果好。

Method: 招募43位专家执行随机分配的人类专家或LLM生成的想法，专家花费超100小时实现想法并撰写4页短文记录实验，由NLP专家盲审所有项目。

Result: 执行后，LLM生成想法在各评估指标上得分降幅显著大于人类专家想法，缩小了构思阶段的差距，部分指标上人类想法排名反超。

Conclusion: 当前LLMs在生成真正有效的研究想法方面存在局限，且缺乏执行结果时评估研究想法具有挑战性。

Abstract: Large Language Models (LLMs) have shown promise in accelerating the
scientific research pipeline. A key capability for this process is the ability
to generate novel research ideas, and prior studies have found settings in
which LLM-generated research ideas were judged as more novel than human-expert
ideas. However, a good idea should not simply appear to be novel, it should
also result in better research after being executed. To test whether
AI-generated ideas lead to better research outcomes, we conduct an execution
study by recruiting 43 expert researchers to execute randomly-assigned ideas,
either written by experts or generated by an LLM. Each expert spent over 100
hours implementing the idea and wrote a 4-page short paper to document the
experiments. All the executed projects are then reviewed blindly by expert NLP
researchers. Comparing the review scores of the same ideas before and after
execution, the scores of the LLM-generated ideas decrease significantly more
than expert-written ideas on all evaluation metrics (novelty, excitement,
effectiveness, and overall; p < 0.05), closing the gap between LLM and human
ideas observed at the ideation stage. When comparing the aggregated review
scores from the execution study, we even observe that for many metrics there is
a flip in rankings where human ideas score higher than LLM ideas. This
ideation-execution gap highlights the limitations of current LLMs in generating
truly effective research ideas and the challenge of evaluating research ideas
in the absence of execution outcomes.

</details>


### [226] [Uncovering Hidden Violent Tendencies in LLMs: A Demographic Analysis via Behavioral Vignettes](https://arxiv.org/abs/2506.20822)
*Quintin Myers,Yanjun Gao*

Main category: cs.CL

TL;DR: 用VBVQ评估大语言模型对暴力内容的推理能力，发现表面文本生成与内部暴力倾向有差异且暴力倾向因人口统计学特征而异。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于检测和应对网络暴力内容，但对其在道德模糊、现实场景中的推理能力研究不足。

Method: 用VBVQ评估大语言模型，引入基于人物角色的提示以评估潜在偏差，在统一零样本设置下评估六个不同背景的大语言模型。

Result: 大语言模型表面文本生成常与内部对暴力回应的偏好不同；其暴力倾向因人口统计学特征而异，常与犯罪学、社会科学和心理学的既定研究结果相矛盾。

Conclusion: 大语言模型在处理道德模糊的现实场景中的暴力内容推理方面存在问题，需进一步改进。

Abstract: Large language models (LLMs) are increasingly proposed for detecting and
responding to violent content online, yet their ability to reason about morally
ambiguous, real-world scenarios remains underexamined. We present the first
study to evaluate LLMs using a validated social science instrument designed to
measure human response to everyday conflict, namely the Violent Behavior
Vignette Questionnaire (VBVQ). To assess potential bias, we introduce
persona-based prompting that varies race, age, and geographic identity within
the United States. Six LLMs developed across different geopolitical and
organizational contexts are evaluated under a unified zero-shot setting. Our
study reveals two key findings: (1) LLMs surface-level text generation often
diverges from their internal preference for violent responses; (2) their
violent tendencies vary across demographics, frequently contradicting
established findings in criminology, social science, and psychology.

</details>


### [227] [Optimising Language Models for Downstream Tasks: A Post-Training Perspective](https://arxiv.org/abs/2506.20917)
*Zhengyan Shi*

Main category: cs.CL

TL;DR: 本文提出一系列方法使语言模型更好适应下游应用，经实证研究证明这些方法能提升模型性能与适应性。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在特定任务的高效、稳健适配方面存在不足，如未充分利用无标签数据、易过拟合和计算成本高。

Method: 提出从无标签数据提取知识的继续预训练技术、参数高效微调方法、改进的监督微调方法，还开发新的评估方法和基准。

Result: 这些方法显著提升了语言模型的鲁棒性、效率和泛化能力，使其更适应广泛应用。

Conclusion: 这些进展朝着更强大高效的语言模型迈进，接近通用人工智能目标。

Abstract: Language models (LMs) have demonstrated remarkable capabilities in NLP, yet
adapting them efficiently and robustly to specific tasks remains challenging.
As their scale and complexity grow, fine-tuning LMs on labelled data often
underutilizes available unlabelled data, leads to overfitting on small
task-specific sets, and imposes significant computational costs. These
limitations hamper their application to the open-ended landscape of real-world
language tasks.
  This thesis proposes a series of methods to better adapt LMs to downstream
applications. First, we explore strategies for extracting task-relevant
knowledge from unlabelled data, introducing a novel continued pre-training
technique that outperforms state-of-the-art semi-supervised approaches. Next,
we present a parameter-efficient fine-tuning method that substantially reduces
memory and compute costs while maintaining competitive performance. We also
introduce improved supervised fine-tuning methods that enable LMs to better
follow instructions, especially when labelled data is scarce, enhancing their
performance across a range of NLP tasks, including open-ended generation.
Finally, we develop new evaluation methods and benchmarks, such as multi-hop
spatial reasoning tasks, to assess LM capabilities and adaptation more
comprehensively.
  Through extensive empirical studies across diverse NLP tasks, our results
demonstrate that these approaches substantially improve LM robustness,
efficiency, and generalization, making them more adaptable to a broad range of
applications. These advances mark a significant step towards more robust and
efficient LMs, bringing us closer to the goal of artificial general
intelligence.

</details>


### [228] [SAC: A Framework for Measuring and Inducing Personality Traits in LLMs with Dynamic Intensity Control](https://arxiv.org/abs/2506.20993)
*Adithya Chittem,Aishna Shrivastava,Sai Tarun Pendela,Jagat Sesh Challa,Dhruv Kumar*

Main category: cs.CL

TL;DR: 本文通过扩展MPI模型并开发SAC框架来解决现有大语言模型人格建模的局限性，实验表明可实现更一致可控的人格表达，推动人机交互发展。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型人格建模依赖大五框架，缺乏特质强度控制机制，为解决此局限开展研究。

Method: 扩展MPI模型纳入16PF模型，开发SAC框架，引入基于形容词的语义锚定，利用五个强度因素的行为问题。

Result: 将强度建模为连续谱比二元特质切换能产生更一致可控的人格表达，目标特质强度变化会影响相关特质。

Conclusion: 本研究为医疗、教育等领域的人机交互开辟新途径，向类人社交机器迈进。

Abstract: Large language models (LLMs) have gained significant traction across a wide
range of fields in recent years. There is also a growing expectation for them
to display human-like personalities during interactions. To meet this
expectation, numerous studies have proposed methods for modelling LLM
personalities through psychometric evaluations. However, most existing models
face two major limitations: they rely on the Big Five (OCEAN) framework, which
only provides coarse personality dimensions, and they lack mechanisms for
controlling trait intensity. In this paper, we address this gap by extending
the Machine Personality Inventory (MPI), which originally used the Big Five
model, to incorporate the 16 Personality Factor (16PF) model, allowing
expressive control over sixteen distinct traits. We also developed a structured
framework known as Specific Attribute Control (SAC) for evaluating and
dynamically inducing trait intensity in LLMs. Our method introduces
adjective-based semantic anchoring to guide trait intensity expression and
leverages behavioural questions across five intensity factors:
\textit{Frequency}, \textit{Depth}, \textit{Threshold}, \textit{Effort}, and
\textit{Willingness}. Through experimentation, we find that modelling intensity
as a continuous spectrum yields substantially more consistent and controllable
personality expression compared to binary trait toggling. Moreover, we observe
that changes in target trait intensity systematically influence closely related
traits in psychologically coherent directions, suggesting that LLMs internalize
multi-dimensional personality structures rather than treating traits in
isolation. Our work opens new pathways for controlled and nuanced human-machine
interactions in domains such as healthcare, education, and interviewing
processes, bringing us one step closer to truly human-like social machines.

</details>


### [229] [Large Language Models Acing Chartered Accountancy](https://arxiv.org/abs/2506.21031)
*Jatin Gupta,Akhil Sharma,Saransh Singhania,Mohammad Adnan,Sakshi Deo,Ali Imam Abidi,Keshav Gupta*

Main category: cs.CL

TL;DR: 本文针对印度金融领域，引入CA - Ben基准评估大语言模型金融、法律和定量推理能力，评估六个模型，指出性能差异与挑战，建议后续改进方法。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在特定金融领域知识捕获和应用效果不明确的问题，填补印度金融领域评估空白。

Method: 引入基于印度特许会计师考试的CA - Ben基准，对六个大语言模型进行标准化评估。

Result: Claude 3.5 Sonnet和GPT - 4o表现更优，在概念和法律推理方面突出，模型在数值计算和法律解释上存在挑战。

Conclusion: 强调当前大语言模型的优缺点，建议通过混合推理和检索增强生成方法改进定量分析和法律解释。

Abstract: Advanced intelligent systems, particularly Large Language Models (LLMs), are
significantly reshaping financial practices through advancements in Natural
Language Processing (NLP). However, the extent to which these models
effectively capture and apply domain-specific financial knowledge remains
uncertain. Addressing a critical gap in the expansive Indian financial context,
this paper introduces CA-Ben, a Chartered Accountancy benchmark specifically
designed to evaluate the financial, legal, and quantitative reasoning
capabilities of LLMs. CA-Ben comprises structured question-answer datasets
derived from the rigorous examinations conducted by the Institute of Chartered
Accountants of India (ICAI), spanning foundational, intermediate, and advanced
CA curriculum stages. Six prominent LLMs i.e. GPT 4o, LLAMA 3.3 70B, LLAMA 3.1
405B, MISTRAL Large, Claude 3.5 Sonnet, and Microsoft Phi 4 were evaluated
using standardized protocols. Results indicate variations in performance, with
Claude 3.5 Sonnet and GPT-4o outperforming others, especially in conceptual and
legal reasoning. Notable challenges emerged in numerical computations and legal
interpretations. The findings emphasize the strengths and limitations of
current LLMs, suggesting future improvements through hybrid reasoning and
retrieval-augmented generation methods, particularly for quantitative analysis
and accurate legal interpretation.

</details>


### [230] [ComRAG: Retrieval-Augmented Generation with Dynamic Vector Stores for Real-time Community Question Answering in Industry](https://arxiv.org/abs/2506.21098)
*Qinwen Chen,Wenbiao Tao,Zhiwei Zhu,Mingfan Xi,Liangzhong Guo,Yuan Wang,Wei Wang,Yunshi Lan*

Main category: cs.CL

TL;DR: 提出ComRAG框架用于实时工业CQA，在多个数据集上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法在利用外部知识、整合动态历史QA上下文和适配工业部署的内存机制方面存在不足，难以有效利用历史交互和领域知识。

Method: 提出ComRAG，通过基于质心的内存机制将静态知识与动态历史QA对集成，用于检索、生成和高效存储。

Result: 在三个工业CQA数据集上，ComRAG始终优于所有基线，向量相似度最高提升25.9%，延迟降低8.7% - 23.3%，块增长从20.23%降至2.06%。

Conclusion: ComRAG在实时工业CQA场景中具有较好的性能和效果。

Abstract: Community Question Answering (CQA) platforms can be deemed as important
knowledge bases in community, but effectively leveraging historical
interactions and domain knowledge in real-time remains a challenge. Existing
methods often underutilize external knowledge, fail to incorporate dynamic
historical QA context, or lack memory mechanisms suited for industrial
deployment. We propose ComRAG, a retrieval-augmented generation framework for
real-time industrial CQA that integrates static knowledge with dynamic
historical QA pairs via a centroid-based memory mechanism designed for
retrieval, generation, and efficient storage. Evaluated on three industrial CQA
datasets, ComRAG consistently outperforms all baselines--achieving up to 25.9%
improvement in vector similarity, reducing latency by 8.7% to 23.3%, and
lowering chunk growth from 20.23% to 2.06% over iterations.

</details>


### [231] [Progtuning: Progressive Fine-tuning Framework for Transformer-based Language Models](https://arxiv.org/abs/2506.21119)
*Xiaoshuang Ji,Zhendong Zhao,Xiaojun Chen,Xin Zhao,Zeyao Liu*

Main category: cs.CL

TL;DR: 提出Progtuning框架，结合渐进学习，减少更新参数并优化资源分配，性能有竞争力且适应性强。


<details>
  <summary>Details</summary>
Motivation: 现有微调方法更新参数成本高且资源分配低效，忽略了Transformer块贡献不均问题。

Method: 提出Progtuning框架，根据贡献逐步减少更新的Transformer块。

Result: 优化资源分配，减少约25%更新参数，保持有竞争力性能，与参数高效微调方法适应性高。

Conclusion: Progtuning是有效的Transformer语言模型微调框架，能优化资源并在多种场景表现出色。

Abstract: Fine-tuning is a promising technique for leveraging Transformer-based
language models in downstream tasks. As model sizes continue to grow, updating
all model parameters becomes increasingly costly. Parameter-efficient
fine-tuning methods effectively address this issue by selectively updating a
small subset of parameters. However, fine-tuning and most existing
parameter-efficient fine-tuning methods require updating the same number of
parameters as the initial size, ignoring the unequal contribution across
Transformer blocks and leading to extremely inefficient allocation of computing
resources. In this paper, we propose Progtuning, the novel fine-tuning
framework combined with progressive learning for Transformer-based language
models. Specifically, Progtuning progressively reduces the number of updated
transformer blocks based on the contribution. Remarkably, Progtuning optimizes
resource allocation and reduces the number of updated parameters by
approximately 25\%, while still maintaining competitive performance. And it
also exhibits high adaptability with parameter-efficient fine-tuning methods,
demonstrating excellent performance across various adaptation scenarios.

</details>


### [232] [Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling across Perception, Planning, and Safety in Real-World Multimodal Agents](https://arxiv.org/abs/2506.21252)
*Tianyi Men,Zhuoran Jin,Pengfei Cao,Yubo Chen,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: 随着多模态大语言模型发展，多模态智能体面临缺乏外部反馈等问题，论文提出 Agent - RewardBench 基准评估奖励建模能力，实验表明现有模型表现有限。


<details>
  <summary>Details</summary>
Motivation: 多模态智能体因缺乏外部反馈难以自我修正和泛化，且缺乏为智能体选择奖励模型的方法，需要构建针对智能体的奖励基准。

Method: 提出 Agent - RewardBench 基准，具备多维度和真实场景评估、步骤级奖励评估、难度适宜且高质量三个关键特征。

Result: 实验显示即使是最先进的多模态模型表现也有限。

Conclusion: 有必要对智能体奖励建模进行专门训练。

Abstract: As Multimodal Large Language Models (MLLMs) advance, multimodal agents show
promise in real-world tasks like web navigation and embodied intelligence.
However, due to limitations in a lack of external feedback, these agents
struggle with self-correction and generalization. A promising approach is to
use reward models as external feedback, but there is no clear on how to select
reward models for agents. Thus, there is an urgent need to build a reward bench
targeted at agents. To address these challenges, we propose Agent-RewardBench,
a benchmark designed to evaluate reward modeling ability in MLLMs. The
benchmark is characterized by three key features: (1) Multiple dimensions and
real-world agent scenarios evaluation. It covers perception, planning, and
safety with 7 scenarios; (2) Step-level reward evaluation. It allows for the
assessment of agent capabilities at the individual steps of a task, providing a
more granular view of performance during the planning process; and (3)
Appropriately difficulty and high-quality. We carefully sample from 10 diverse
models, difficulty control to maintain task challenges, and manual verification
to ensure the integrity of the data. Experiments demonstrate that even
state-of-the-art multimodal models show limited performance, highlighting the
need for specialized training in agent reward modeling. Code is available at
github.

</details>


### [233] [Detecting Referring Expressions in Visually Grounded Dialogue with Autoregressive Language Models](https://arxiv.org/abs/2506.21294)
*Bram Willemsen,Gabriel Skantze*

Main category: cs.CL

TL;DR: 本文探索用纯文本自回归语言建模方法从视觉对话中提取指代表达，发现纯文本方法有效，但该任务本质是多模态问题。


<details>
  <summary>Details</summary>
Motivation: 研究仅依靠语言上下文能在多大程度上帮助检测对话视觉上下文中有（视觉可感知）指称对象的提及。

Method: 调整预训练大语言模型，通过下一个词预测来划分文本中提及跨度的边界，对展开的对话进行相对粗粒度的标注。

Result: 即使使用中等规模的大语言模型、相对小的数据集和参数高效微调，纯文本方法仍有效。

Conclusion: 该任务本质是多模态问题，单模态方法存在局限性。

Abstract: In this paper, we explore the use of a text-only, autoregressive language
modeling approach for the extraction of referring expressions from visually
grounded dialogue. More specifically, the aim is to investigate the extent to
which the linguistic context alone can inform the detection of mentions that
have a (visually perceivable) referent in the visual context of the
conversation. To this end, we adapt a pretrained large language model (LLM) to
perform a relatively course-grained annotation of mention spans in unfolding
conversations by demarcating mention span boundaries in text via next-token
prediction. Our findings indicate that even when using a moderately sized LLM,
relatively small datasets, and parameter-efficient fine-tuning, a text-only
approach can be effective, highlighting the relative importance of the
linguistic context for this task. Nevertheless, we argue that the task
represents an inherently multimodal problem and discuss limitations fundamental
to unimodal approaches.

</details>


### [234] [Domain Knowledge-Enhanced LLMs for Fraud and Concept Drift Detection](https://arxiv.org/abs/2506.21443)
*Ali Şenol,Garima Agrawal,Huan Liu*

Main category: cs.CL

TL;DR: 论文提出DK增强LLM框架用于欺诈和概念漂移检测，在数据集上验证效果良好，提升了高风险NLP应用性能。


<details>
  <summary>Details</summary>
Motivation: 动态平台上检测欺诈对话因语言模式演变和概念漂移而困难，LLM在风险敏感场景有不足。

Method: 提出DK增强LLM框架，包含DK - LLM模块、漂移检测单元和第二个DK - LLM模块。

Result: 系统能高精度检测虚假对话、有效分类漂移性质，基于LLaMA实现达98%分类准确率。

Conclusion: 融入领域知识和漂移感知显著提升高风险NLP应用的性能、可解释性和鲁棒性。

Abstract: Detecting deceptive conversations on dynamic platforms is increasingly
difficult due to evolving language patterns and Concept Drift (CD)-i.e.,
semantic or topical shifts that alter the context or intent of interactions
over time. These shifts can obscure malicious intent or mimic normal dialogue,
making accurate classification challenging. While Large Language Models (LLMs)
show strong performance in natural language tasks, they often struggle with
contextual ambiguity and hallucinations in risk-sensitive scenarios. To address
these challenges, we present a Domain Knowledge (DK)-Enhanced LLM framework
that integrates pretrained LLMs with structured, task-specific insights to
perform fraud and concept drift detection. The proposed architecture consists
of three main components: (1) a DK-LLM module to detect fake or deceptive
conversations; (2) a drift detection unit (OCDD) to determine whether a
semantic shift has occurred; and (3) a second DK-LLM module to classify the
drift as either benign or fraudulent. We first validate the value of domain
knowledge using a fake review dataset and then apply our full framework to
SEConvo, a multiturn dialogue dataset that includes various types of fraud and
spam attacks. Results show that our system detects fake conversations with high
accuracy and effectively classifies the nature of drift. Guided by structured
prompts, the LLaMA-based implementation achieves 98% classification accuracy.
Comparative studies against zero-shot baselines demonstrate that incorporating
domain knowledge and drift awareness significantly improves performance,
interpretability, and robustness in high-stakes NLP applications.

</details>


### [235] [Can Gradient Descent Simulate Prompting?](https://arxiv.org/abs/2506.20989)
*Eric Zhang,Leshem Choshen,Jacob Andreas*

Main category: cs.CL

TL;DR: 本文探讨让微调模拟提示的方法，通过元训练使梯度更新模拟新信息条件作用，在部分任务有效果，为长上下文建模提供新思路。


<details>
  <summary>Details</summary>
Motivation: 当前参数更新虽无长期存储成本，但提示在很多模型更新中更有效，想让微调模拟提示效果。

Method: 使用基于梯度的元学习工具，以语言模型自身提示预测为目标，无需真实标签进行元训练。

Result: 后续梯度下降训练恢复部分或全部提示模型性能，在‘反转诅咒’任务有改进，单次梯度更新后能回答文本段落问题。

Conclusion: 适当初始化时，梯度下降表达能力惊人，为长上下文建模提供新途径，有助于理解基于梯度学习的泛化能力。

Abstract: There are two primary ways of incorporating new information into a language
model (LM): changing its prompt or changing its parameters, e.g. via
fine-tuning. Parameter updates incur no long-term storage cost for model
changes. However, for many model updates, prompting is significantly more
effective: prompted models can generalize robustly from single examples and
draw logical inferences that do not occur under standard fine-tuning. Can
models be modified so that fine-tuning does emulate prompting? This paper
describes a method for meta-training LMs such that gradient updates emulate the
effects of conditioning on new information. Our approach uses tools from
gradient-based meta-learning but uses an LM's own prompted predictions as
targets, eliminating the need for ground-truth labels. Subsequent gradient
descent training recovers some (and occasionally all) of prompted model
performance -- showing improvement on the ``reversal curse'' tasks, and
answering questions about text passages after a single gradient update. These
results suggest that, with appropriate initialization, gradient descent can be
surprisingly expressive. Our results suggest new avenues for long-context
modeling and offer insight into the generalization capabilities of
gradient-based learning.

</details>


### [236] [Potemkin Understanding in Large Language Models](https://arxiv.org/abs/2506.21521)
*Marina Mancoridis,Bec Weeks,Keyon Vafa,Sendhil Mullainathan*

Main category: cs.CL

TL;DR: 本文探讨基于基准数据集评估大语言模型能力的合理性，提出量化虚假理解的方法，发现虚假理解普遍存在且反映模型概念表征的内部不一致。


<details>
  <summary>Details</summary>
Motivation: 探究基于精心挑选的问题对大语言模型能力进行推断的合理性。

Method: 引入正式框架，提出两种量化虚假理解存在性的程序，一是在三个领域使用专门设计的基准测试，二是用通用程序提供其普遍程度的下限。

Result: 发现虚假理解在各模型、任务和领域中普遍存在，且这些失败不仅反映理解错误，还体现概念表征的深层内部不一致。

Conclusion: 基于现有基准数据集评估大语言模型能力可能存在问题，因为可能存在虚假理解。

Abstract: Large language models (LLMs) are regularly evaluated using benchmark
datasets. But what justifies making inferences about an LLM's capabilities
based on its answers to a curated set of questions? This paper first introduces
a formal framework to address this question. The key is to note that the
benchmarks used to test LLMs -- such as AP exams -- are also those used to test
people. However, this raises an implication: these benchmarks are only valid
tests if LLMs misunderstand concepts in ways that mirror human
misunderstandings. Otherwise, success on benchmarks only demonstrates potemkin
understanding: the illusion of understanding driven by answers irreconcilable
with how any human would interpret a concept. We present two procedures for
quantifying the existence of potemkins: one using a specially designed
benchmark in three domains, the other using a general procedure that provides a
lower-bound on their prevalence. We find that potemkins are ubiquitous across
models, tasks, and domains. We also find that these failures reflect not just
incorrect understanding, but deeper internal incoherence in concept
representations.

</details>


### [237] ["What's Up, Doc?": Analyzing How Users Seek Health Information in Large-Scale Conversational AI Datasets](https://arxiv.org/abs/2506.21532)
*Akshay Paruchuri,Maryam Aziz,Rohit Vartak,Ayman Ali,Best Uchehara,Xin Liu,Ishan Chatterjee,Monica Agrawal*

Main category: cs.CL

TL;DR: 本文筛选数据集得到HealthChat - 11K，研究用户与大语言模型在医疗信息查询时的交互，揭示交互本质并指出大语言模型医疗支持能力需提升。


<details>
  <summary>Details</summary>
Motivation: 人们越来越通过交互式聊天机器人向大语言模型寻求医疗信息，但此类对话的本质和风险尚未被充分探索。

Method: 筛选大规模对话式AI数据集得到HealthChat - 11K，结合临床驱动的分类法，研究21个不同健康专业的用户交互情况。

Result: 分析揭示了用户寻求健康信息的方式和原因，如常见交互、上下文不完整情况、情感行为以及可能导致逢迎的交互。

Conclusion: 部署为对话式AI的大语言模型的医疗支持能力需要改进。

Abstract: People are increasingly seeking healthcare information from large language
models (LLMs) via interactive chatbots, yet the nature and inherent risks of
these conversations remain largely unexplored. In this paper, we filter
large-scale conversational AI datasets to achieve HealthChat-11K, a curated
dataset of 11K real-world conversations composed of 25K user messages. We use
HealthChat-11K and a clinician-driven taxonomy for how users interact with LLMs
when seeking healthcare information in order to systematically study user
interactions across 21 distinct health specialties. Our analysis reveals
insights into the nature of how and why users seek health information, such as
common interactions, instances of incomplete context, affective behaviors, and
interactions (e.g., leading questions) that can induce sycophancy, underscoring
the need for improvements in the healthcare support capabilities of LLMs
deployed as conversational AI. Code and artifacts to retrieve our analyses and
combine them into a curated dataset can be found here:
https://github.com/yahskapar/HealthChat

</details>


### [238] [Aligning Spoken Dialogue Models from User Interactions](https://arxiv.org/abs/2506.21463)
*Anne Wu,Laurent Mazaré,Neil Zeghidour,Alexandre Défossez*

Main category: cs.CL

TL;DR: 提出偏好对齐框架改进实时对话语音模型，创建数据集微调模型，实验证明有效，强调平衡动态因素重要性。


<details>
  <summary>Details</summary>
Motivation: 现有偏好学习方法不适用于实时语音交互的复杂性，需改进语音对话模型。

Method: 从原始多轮语音对话创建含AI反馈的大规模偏好对数据集，用离线对齐方法微调全双工自回归语音到语音模型。

Result: 通用对话反馈能有效改进语音对话模型，使其交互更具事实性、安全性和上下文一致性。

Conclusion: 自然实时语音对话系统需校准各种动态因素间的平衡。

Abstract: We propose a novel preference alignment framework for improving spoken
dialogue models on real-time conversations from user interactions. Current
preference learning methods primarily focus on text-based language models, and
are not directly suited to the complexities of real-time speech interactions,
with richer dynamics (e.g. interruption, interjection) and no explicit
segmentation between speaker turns.We create a large-scale dataset of more than
150,000 preference pairs from raw multi-turn speech conversations, annotated
with AI feedback, to cover preferences over both linguistic content and
temporal context variations. We leverage offline alignment methods to finetune
a full-duplex autoregressive speech-to-speech model. Extensive experiments
demonstrate that feedback on generic conversations can be consistently
effective in improving spoken dialogue models to produce more factual, safer
and more contextually aligned interactions. We deploy the finetuned model and
conduct holistic human evaluations to assess the impact beyond single-turn
conversations. Our findings shed light on the importance of a well-calibrated
balance among various dynamics, crucial for natural real-time speech dialogue
systems.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [239] [Robust Alignment via Partial Gromov-Wasserstein Distances](https://arxiv.org/abs/2506.21507)
*Xiaoyun Gong,Sloan Nietert,Ziv Goldfeld*

Main category: math.ST

TL;DR: 提出基于部分Gromov - Wasserstein (GW)距离的估计器解决GW对齐对异常值敏感问题，该估计器在总体和有限样本情况下有良好表现，并赋予部分GW距离操作意义。


<details>
  <summary>Details</summary>
Motivation: 传统GW对齐对异常值敏感，易使匹配方案失真，需要解决此问题以准确估计原始数据的GW对齐成本。

Method: 提出基于部分GW距离的估计器，在优化对齐前从每个分布中去除部分质量。

Result: 估计器在总体设置中是极小极大最优的，在有限样本情况下接近最优，差距源于经验估计中插件估计器的次优性。

Conclusion: 结果赋予部分GW距离在数据可能被污染时作为经典距离的鲁棒替代的操作意义。

Abstract: The Gromov-Wasserstein (GW) problem provides a powerful framework for
aligning heterogeneous datasets by matching their internal structures in a way
that minimizes distortion. However, GW alignment is sensitive to data
contamination by outliers, which can greatly distort the resulting matching
scheme. To address this issue, we study robust GW alignment, where upon
observing contaminated versions of the clean data distributions, our goal is to
accurately estimate the GW alignment cost between the original (uncontaminated)
measures. We propose an estimator based on the partial GW distance, which trims
out a fraction of the mass from each distribution before optimally aligning the
rest. The estimator is shown to be minimax optimal in the population setting
and is near-optimal in the finite-sample regime, where the optimality gap
originates only from the suboptimality of the plug-in estimator in the
empirical estimation setting (i.e., without contamination). Towards the
analysis, we derive new structural results pertaining to the approximate
pseudo-metric structure of the partial GW distance. Overall, our results endow
the partial GW distance with an operational meaning by posing it as a robust
surrogate of the classical distance when the observed data may be contaminated.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [240] [Latent-space Field Tension for Astrophysical Component Detection An application to X-ray imaging](https://arxiv.org/abs/2506.20758)
*Matteo Guardiani,Vincent Eberle,Margret Westerkamp,Julian Rüstig,Philipp Frank,Torsten Enßlin*

Main category: astro-ph.IM

TL;DR: 本文介绍一种新型多频贝叶斯天空发射场模型，可自动分离天体物理发射成分，在合成数据和观测数据上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现代天文台需有原则性数据分析方法，以从受噪声和仪器效应影响的数据中分离和重建天体物理成分。

Method: 引入利用潜空间张力指示模型误设的多频贝叶斯天空发射场模型，用潜空间先验期望偏差诊断模型误设，指导引入新天空成分。

Result: 该方法能高精度重建天体物理成分，实现点源亚像素定位、扩展发射稳健分离和详细不确定性量化。

Conclusion: 该方法提供通用且有依据的框架，适用于多种天文数据集，适合支持下一代多波长和多信使调查的分析需求。

Abstract: Modern observatories are designed to deliver increasingly detailed views of
astrophysical signals. To fully realize the potential of these observations,
principled data-analysis methods are required to effectively separate and
reconstruct the underlying astrophysical components from data corrupted by
noise and instrumental effects. In this work, we introduce a novel
multi-frequency Bayesian model of the sky emission field that leverages
latent-space tension as an indicator of model misspecification, enabling
automated separation of diffuse, point-like, and extended astrophysical
emission components across wavelength bands. Deviations from latent-space prior
expectations are used as diagnostics for model misspecification, thus
systematically guiding the introduction of new sky components, such as
point-like and extended sources. We demonstrate the effectiveness of this
method on synthetic multi-frequency imaging data and apply it to observational
X-ray data from the eROSITA Early Data Release (EDR) of the SN1987A region in
the Large Magellanic Cloud (LMC). Our results highlight the method's capability
to reconstruct astrophysical components with high accuracy, achieving sub-pixel
localization of point sources, robust separation of extended emission, and
detailed uncertainty quantification. The developed methodology offers a general
and well-founded framework applicable to a wide variety of astronomical
datasets, and is therefore well suited to support the analysis needs of
next-generation multi-wavelength and multi-messenger surveys.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [241] [DRAGON: Distributional Rewards Optimize Diffusion Generative Models](https://arxiv.org/abs/2504.15217)
*Yatong Bai,Jonah Casebeer,Somayeh Sojoudi,Nicholas J. Bryan*

Main category: cs.SD

TL;DR: 提出DRAGON框架用于微调媒体生成模型，比传统方法更灵活，在20种奖励函数评估中表现出色，展示了设计和优化奖励函数新方法。


<details>
  <summary>Details</summary>
Motivation: 寻找更灵活的微调媒体生成模型以达到期望结果的方法，改进传统RLHF和DPO等方法。

Method: 构建DRAGON框架，可优化多种奖励函数；选择编码器和参考示例创建范例分布；收集生成结果，构建正负示范集并利用对比最大化奖励。

Result: 在20种目标奖励中平均胜率81.45%；基于范例集的奖励函数提升生成效果；合适范例集下音乐质量人类投票胜率60.95%。

Conclusion: DRAGON展示了设计和优化奖励函数以提高人类感知质量的新方法。

Abstract: We present Distributional RewArds for Generative OptimizatioN (DRAGON), a
versatile framework for fine-tuning media generation models towards a desired
outcome. Compared with traditional reinforcement learning with human feedback
(RLHF) or pairwise preference approaches such as direct preference optimization
(DPO), DRAGON is more flexible. It can optimize reward functions that evaluate
either individual examples or distributions of them, making it compatible with
a broad spectrum of instance-wise, instance-to-distribution, and
distribution-to-distribution rewards. Leveraging this versatility, we construct
novel reward functions by selecting an encoder and a set of reference examples
to create an exemplar distribution. When cross-modality encoders such as CLAP
are used, the reference examples may be of a different modality (e.g., text
versus audio). Then, DRAGON gathers online and on-policy generations, scores
them to construct a positive demonstration set and a negative set, and
leverages the contrast between the two sets to maximize the reward. For
evaluation, we fine-tune an audio-domain text-to-music diffusion model with 20
different reward functions, including a custom music aesthetics model, CLAP
score, Vendi diversity, and Frechet audio distance (FAD). We further compare
instance-wise (per-song) and full-dataset FAD settings while ablating multiple
FAD encoders and reference sets. Over all 20 target rewards, DRAGON achieves an
81.45% average win rate. Moreover, reward functions based on exemplar sets
indeed enhance generations and are comparable to model-based rewards. With an
appropriate exemplar set, DRAGON achieves a 60.95% human-voted music quality
win rate without training on human preference annotations. As such, DRAGON
exhibits a new approach to designing and optimizing reward functions for
improving human-perceived quality. Sound examples at
https://ml-dragon.github.io/web.

</details>


### [242] [PeakNetFP: Peak-based Neural Audio Fingerprinting Robust to Extreme Time Stretching](https://arxiv.org/abs/2506.21086)
*Guillem Cortès-Sebastià,Benjamin Martin,Emilio Molina,Xavier Serra,Romain Hennequin*

Main category: cs.SD

TL;DR: 本文介绍了首个基于频谱峰值的神经音频指纹系统PeakNetFP，它结合传统方法与深度学习，性能好且高效，为音频指纹技术提供新方向。


<details>
  <summary>Details</summary>
Motivation: 设计一个结合传统峰值音频指纹方法与深度学习优势的音频指纹系统，解决时间拉伸音频数据处理问题并提高效率。

Method: 采用类似计算机视觉模型PointNet++的分层点特征提取技术，使用对比学习进行训练。

Result: PeakNetFP性能优于传统音频指纹系统，处理时间拉伸音频数据时与NeuralFP相当，在50% - 200%拉伸因子下Top - 1命中率超90%，参数比NeuralFP少100倍，输入数据小11倍。

Conclusion: PeakNetFP是涉及时间拉伸的音频指纹任务的轻量级高效解决方案，为未来音频指纹技术指明了有前景的方向。

Abstract: This work introduces PeakNetFP, the first neural audio fingerprinting (AFP)
system designed specifically around spectral peaks. This novel system is
designed to leverage the sparse spectral coordinates typically computed by
traditional peak-based AFP methods. PeakNetFP performs hierarchical point
feature extraction techniques similar to the computer vision model PointNet++,
and is trained using contrastive learning like in the state-of-the-art deep
learning AFP, NeuralFP. This combination allows PeakNetFP to outperform
conventional AFP systems and achieves comparable performance to NeuralFP when
handling challenging time-stretched audio data. In extensive evaluation,
PeakNetFP maintains a Top-1 hit rate of over 90% for stretching factors ranging
from 50% to 200%. Moreover, PeakNetFP offers significant efficiency advantages:
compared to NeuralFP, it has 100 times fewer parameters and uses 11 times
smaller input data. These features make PeakNetFP a lightweight and efficient
solution for AFP tasks where time stretching is involved. Overall, this system
represents a promising direction for future AFP technologies, as it
successfully merges the lightweight nature of peak-based AFP with the
adaptability and pattern recognition capabilities of neural network-based
approaches, paving the way for more scalable and efficient solutions in the
field.

</details>


### [243] [A Hierarchical Deep Learning Approach for Minority Instrument Detection](https://arxiv.org/abs/2506.21167)
*Dylan Sechet,Francesca Bugiotti,Matthieu Kowalski,Edouard d'Hérouville,Filip Langiewicz*

Main category: cs.SD

TL;DR: 本文聚焦音乐音频中乐器活动识别，用MedleyDB数据集评估分层分类系统，提出集成策略并测试新模型，实现更可靠的粗粒度乐器检测。


<details>
  <summary>Details</summary>
Motivation: 在音乐信息检索中，识别音频片段内的乐器活动对音乐编目和发现有重要意义，以往深度学习方法多关注数据丰富的乐器类别，因此需新方法解决问题。

Method: 基于Hornbostel - Sachs分类，使用MedleyDB数据集评估分层分类系统，提出将分层结构集成到模型的策略，测试新的分层音乐预测模型。

Result: 实现了更可靠的粗粒度乐器检测。

Conclusion: 该研究缩小了详细乐器识别和组级识别之间的差距，为该领域进一步发展奠定基础。

Abstract: Identifying instrument activities within audio excerpts is vital in music
information retrieval, with significant implications for music cataloging and
discovery. Prior deep learning endeavors in musical instrument recognition have
predominantly emphasized instrument classes with ample data availability.
Recent studies have demonstrated the applicability of hierarchical
classification in detecting instrument activities in orchestral music, even
with limited fine-grained annotations at the instrument level. Based on the
Hornbostel-Sachs classification, such a hierarchical classification system is
evaluated using the MedleyDB dataset, renowned for its diversity and richness
concerning various instruments and music genres. This work presents various
strategies to integrate hierarchical structures into models and tests a new
class of models for hierarchical music prediction. This study showcases more
reliable coarse-level instrument detection by bridging the gap between detailed
instrument identification and group-level recognition, paving the way for
further advancements in this domain.

</details>


### [244] [Integrating Vehicle Acoustic Data for Enhanced Urban Traffic Management: A Study on Speed Classification in Suzhou](https://arxiv.org/abs/2506.21269)
*Pengfei Fan,Yuli Zhang,Xinheng Wang,Ruiyuan Jiang,Hankang Gu,Dongyao Jia,Shangbo Wang*

Main category: cs.SD

TL;DR: 研究发布苏州城市道路声学数据集，提出双模态特征融合深度卷积神经网络BMCNN进行车速分类，实验效果良好，方法可用于智慧城市交通管理。


<details>
  <summary>Details</summary>
Motivation: 为了对车辆噪声和行驶速度之间的耦合关系进行建模，解决城市交通噪声监测和车速估计问题，优化交通流控制。

Method: 提出BMCNN，预处理采用自适应去噪和归一化策略，网络中并行分支提取MFCC和小波包能量特征，通过跨模态注意力机制融合特征。

Result: BMCNN在SZUR - Acoustic数据集上分类准确率达87.56%，在公共IDMT - Traffic数据集上达96.28%，消融研究和鲁棒性测试验证了各模块作用。

Conclusion: 提出的基于声学的车速分类方法可集成到智慧城市交通管理系统，实现实时噪声监测和车速估计，优化交通控制、减少噪声污染和支持城市规划。

Abstract: This study presents and publicly releases the Suzhou Urban Road Acoustic
Dataset (SZUR-Acoustic Dataset), which is accompanied by comprehensive
data-acquisition protocols and annotation guidelines to ensure transparency and
reproducibility of the experimental workflow. To model the coupling between
vehicular noise and driving speed, we propose a bimodal-feature-fusion deep
convolutional neural network (BMCNN). During preprocessing, an adaptive
denoising and normalization strategy is applied to suppress environmental
background interference; in the network architecture, parallel branches extract
Mel-frequency cepstral coefficients (MFCCs) and wavelet-packet energy features,
which are subsequently fused via a cross-modal attention mechanism in the
intermediate feature space to fully exploit time-frequency information.
Experimental results demonstrate that BMCNN achieves a classification accuracy
of 87.56% on the SZUR-Acoustic Dataset and 96.28% on the public IDMT-Traffic
dataset. Ablation studies and robustness tests on the Suzhou dataset further
validate the contributions of each module to performance improvement and
overfitting mitigation. The proposed acoustics-based speed classification
method can be integrated into smart-city traffic management systems for
real-time noise monitoring and speed estimation, thereby optimizing traffic
flow control, reducing roadside noise pollution, and supporting sustainable
urban planning.

</details>


### [245] [Exploring Adapter Design Tradeoffs for Low Resource Music Generation](https://arxiv.org/abs/2506.21298)
*Atharva Mehta,Shivam Chauhan,Monojit Choudhury*

Main category: cs.SD

TL;DR: 研究音乐生成模型适配器配置，发现不同类型适配器特点及最佳规模，对比不同模型优劣势。


<details>
  <summary>Details</summary>
Motivation: 微调大规模音乐生成模型计算成本高，参数高效微调技术中适配器设计选择多，需明确低资源音乐流派下的最优组合。

Method: 研究MusicGen和Mustango两个AI音乐模型在印度古典音乐和土耳其玛卡姆音乐两种流派下的各种适配器配置。

Result: 卷积适配器擅捕捉局部细节，Transformer适配器保留长距离依赖；中型适配器达表现力和质量平衡；Mustango输出多样但不稳定、计算量大，MusicGen训练快、输出质量好但有冗余。

Conclusion: 不同适配器和模型各有优劣，需根据需求选择合适配置。

Abstract: Fine-tuning large-scale music generation models, such as MusicGen and
Mustango, is a computationally expensive process, often requiring updates to
billions of parameters and, therefore, significant hardware resources.
Parameter-Efficient Fine-Tuning (PEFT) techniques, particularly adapter-based
methods, have emerged as a promising alternative, enabling adaptation with
minimal trainable parameters while preserving model performance. However, the
design choices for adapters, including their architecture, placement, and size,
are numerous, and it is unclear which of these combinations would produce
optimal adapters and why, for a given case of low-resource music genre. In this
paper, we attempt to answer this question by studying various adapter
configurations for two AI music models, MusicGen and Mustango, on two genres:
Hindustani Classical and Turkish Makam music.
  Our findings reveal distinct trade-offs: convolution-based adapters excel in
capturing fine-grained local musical details such as ornamentations and short
melodic phrases, while transformer-based adapters better preserve long-range
dependencies crucial for structured improvisation. Additionally, we analyze
computational resource requirements across different adapter scales,
demonstrating how mid-sized adapters (40M parameters) achieve an optimal
balance between expressivity and quality. Furthermore, we find that Mustango, a
diffusion-based model, generates more diverse outputs with better adherence to
the description in the input prompt while lacking in providing stability in
notes, rhythm alignment, and aesthetics. Also, it is computationally intensive
and requires significantly more time to train. In contrast, autoregressive
models like MusicGen offer faster training and are more efficient, and can
produce better quality output in comparison, but have slightly higher
redundancy in their generations.

</details>


### [246] [SmoothSinger: A Conditional Diffusion Model for Singing Voice Synthesis with Multi-Resolution Architecture](https://arxiv.org/abs/2506.21478)
*Kehan Sui,Jinxu Xiang,Fang Jin*

Main category: cs.SD

TL;DR: 提出SmoothSinger条件扩散模型用于合成高质量自然歌声，在Opencpop数据集上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 扩散模型应用于SVS有挑战，先前方法依赖声码器常引入失真，需更好的合成方法。

Method: 采用参考引导双分支架构，用低质量音频引导去噪；增强U - Net，添加并行低频上采样路径；训练时用退化的真实音频替换参考音频。

Result: 在Opencpop数据集的客观和主观评估中取得SOTA结果，消融实验证实可减少伪影、提高合成语音自然度。

Conclusion: SmoothSinger能有效合成高质量、自然的歌声。

Abstract: Singing voice synthesis (SVS) aims to generate expressive and high-quality
vocals from musical scores, requiring precise modeling of pitch, duration, and
articulation. While diffusion-based models have achieved remarkable success in
image and video generation, their application to SVS remains challenging due to
the complex acoustic and musical characteristics of singing, often resulting in
artifacts that degrade naturalness. In this work, we propose SmoothSinger, a
conditional diffusion model designed to synthesize high quality and natural
singing voices. Unlike prior methods that depend on vocoders as a final stage
and often introduce distortion, SmoothSinger refines low-quality synthesized
audio directly in a unified framework, mitigating the degradation associated
with two-stage pipelines. The model adopts a reference-guided dual-branch
architecture, using low-quality audio from any baseline system as a reference
to guide the denoising process, enabling more expressive and context-aware
synthesis. Furthermore, it enhances the conventional U-Net with a parallel
low-frequency upsampling path, allowing the model to better capture pitch
contours and long term spectral dependencies. To improve alignment during
training, we replace reference audio with degraded ground truth audio,
addressing temporal mismatch between reference and target signals. Experiments
on the Opencpop dataset, a large-scale Chinese singing corpus, demonstrate that
SmoothSinger achieves state-of-the-art results in both objective and subjective
evaluations. Extensive ablation studies confirm its effectiveness in reducing
artifacts and improving the naturalness of synthesized voices.

</details>


### [247] [Learnable Adaptive Time-Frequency Representation via Differentiable Short-Time Fourier Transform](https://arxiv.org/abs/2506.21440)
*Maxime Leiber,Yosra Marnissi,Axel Barrau,Sylvain Meignen,Laurent Massoulié*

Main category: cs.SD

TL;DR: 提出可微分的短时傅里叶变换（STFT）公式，可基于梯度优化参数，与神经网络集成，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统 STFT 参数调整对参数敏感，手动或启发式调整结果欠佳，且传统方法依赖计算密集的离散搜索。

Method: 提出统一的可微分 STFT 公式，实现基于梯度的参数优化，并与神经网络集成进行联合优化。

Result: 通过模拟和真实世界数据实验，证明可微分 STFT 能增强时频表示（TFR），提升下游任务性能。

Conclusion: 可微分 STFT 方法能克服传统 STFT 参数调整的局限，有效提升 TFR 和下游任务表现。

Abstract: The short-time Fourier transform (STFT) is widely used for analyzing
non-stationary signals. However, its performance is highly sensitive to its
parameters, and manual or heuristic tuning often yields suboptimal results. To
overcome this limitation, we propose a unified differentiable formulation of
the STFT that enables gradient-based optimization of its parameters. This
approach addresses the limitations of traditional STFT parameter tuning
methods, which often rely on computationally intensive discrete searches. It
enables fine-tuning of the time-frequency representation (TFR) based on any
desired criterion. Moreover, our approach integrates seamlessly with neural
networks, allowing joint optimization of the STFT parameters and network
weights. The efficacy of the proposed differentiable STFT in enhancing TFRs and
improving performance in downstream tasks is demonstrated through experiments
on both simulated and real-world data.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [248] [Heterogeneous Exposures to Systematic and Idiosyncratic Risk across Crypto Assets: A Divide-and-Conquer Approach](https://arxiv.org/abs/2506.21100)
*Nektarios Aslanidis,Aurelio Bariviera,George Kapetanios,Vasilis Sarafidis*

Main category: econ.EM

TL;DR: 本文通过估计对特质和系统性风险的异质敞口，分析广泛加密资产的实现回报行为，提出两阶段方法，揭示不同类别资产风险敞口特征并构建风险分析框架。


<details>
  <summary>Details</summary>
Motivation: 由于宏观金融代理高频数据不可用且低频数据对实证相关性指导有限，需分析加密资产回报行为并分离不同层次风险。

Method: 采用两阶段“分而治之”方法，第一阶段用资产级IV回归估计高频特质和市场风险敞口，第二阶段通过主成分分析和高维变量选择识别潜在的全经济因素，用均值组估计器揭示敞口异质性。

Result: 发现短期均值回归，不同类别资产对风险因素敞口不同，如绿色和DeFi资产风险敞口更大，稳定币风险敞口更小。

Conclusion: 研究构建了隔离加密市场不同层次风险的连贯框架，为投资组合设计和监管监督提供重要见解。

Abstract: This paper analyzes realized return behavior across a broad set of crypto
assets by estimating heterogeneous exposures to idiosyncratic and systematic
risk. A key challenge arises from the latent nature of broader economy-wide
risk sources: macro-financial proxies are unavailable at high-frequencies,
while the abundance of low-frequency candidates offers limited guidance on
empirical relevance. To address this, we develop a two-stage
``divide-and-conquer'' approach. The first stage estimates exposures to
high-frequency idiosyncratic and market risk only, using asset-level IV
regressions. The second stage identifies latent economy-wide factors by
extracting the leading principal component from the model residuals and mapping
it to lower-frequency macro-financial uncertainty and sentiment-based
indicators via high-dimensional variable selection. Structured patterns of
heterogeneity in exposures are uncovered using Mean Group estimators across
asset categories. The method is applied to a broad sample of crypto assets,
covering more than 80% of total market capitalization. We document short-term
mean reversion and significant average exposures to idiosyncratic volatility
and illiquidity. Green and DeFi assets are, on average, more exposed to
market-level and economy-wide risk than their non-Green and non-DeFi
counterparts. By contrast, stablecoins are less exposed to idiosyncratic,
market-level, and economy-wide risk factors relative to non-stablecoins. At a
conceptual level, our study develops a coherent framework for isolating
distinct layers of risk in crypto markets. Empirically, it sheds light on how
return sensitivities vary across digital asset categories -- insights that are
important for both portfolio design and regulatory oversight.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [249] [Thinning to improve two-sample discrepancy](https://arxiv.org/abs/2506.20932)
*Gleb Smirnov,Roman Vershynin*

Main category: math.PR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The discrepancy between two independent samples \(X_1,\dots,X_n\) and
\(Y_1,\dots,Y_n\) drawn from the same distribution on $\mathbb{R}^d$ typically
has order \(O(\sqrt{n})\) even in one dimension. We give a simple online
algorithm that reduces the discrepancy to \(O(\log^{2d} n)\) by discarding a
small fraction of the points.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [250] [Consistent Zero-shot 3D Texture Synthesis Using Geometry-aware Diffusion and Temporal Video Models](https://arxiv.org/abs/2506.20946)
*Donggoo Kang,Jangyeong Kim,Dasol Jeong,Junyoung Choi,Jeonga Wi,Hyunmin Lee,Joonho Gwon,Joonki Paik*

Main category: cs.GR

TL;DR: 提出VideoTex框架用于无缝纹理合成，结合视频生成模型解决3D纹理时空不一致问题，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有纹理合成方法因缺乏全局上下文和几何理解存在不一致问题，而视频生成模型在时间一致性上表现出色，故提出新方法解决3D纹理时空不一致。

Method: 引入VideoTex框架，结合几何感知条件利用3D网格结构，提出结构-wise UV扩散策略保留语义信息。

Result: VideoTex实现了UV边界平滑过渡，保证视频帧间高质量、时间稳定的纹理，在纹理保真度、接缝融合和稳定性上优于现有方法。

Conclusion: VideoTex为要求视觉质量和时间连贯性的动态实时应用铺平道路。

Abstract: Current texture synthesis methods, which generate textures from fixed
viewpoints, suffer from inconsistencies due to the lack of global context and
geometric understanding. Meanwhile, recent advancements in video generation
models have demonstrated remarkable success in achieving temporally consistent
videos. In this paper, we introduce VideoTex, a novel framework for seamless
texture synthesis that leverages video generation models to address both
spatial and temporal inconsistencies in 3D textures. Our approach incorporates
geometry-aware conditions, enabling precise utilization of 3D mesh structures.
Additionally, we propose a structure-wise UV diffusion strategy, which enhances
the generation of occluded areas by preserving semantic information, resulting
in smoother and more coherent textures. VideoTex not only achieves smoother
transitions across UV boundaries but also ensures high-quality, temporally
stable textures across video frames. Extensive experiments demonstrate that
VideoTex outperforms existing methods in texture fidelity, seam blending, and
stability, paving the way for dynamic real-time applications that demand both
visual quality and temporal coherence.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [251] [Global and Local Contrastive Learning for Joint Representations from Cardiac MRI and ECG](https://arxiv.org/abs/2506.20683)
*Alexander Selivanov,Philip Müller,Özgün Turgut,Nil Stolt-Ansó,Daniel Rückert*

Main category: eess.IV

TL;DR: 提出PTACL多模态对比学习框架，融合CMR时空信息增强ECG表征，在两项临床任务上优于基线方法，有潜力提升无创心脏诊断。


<details>
  <summary>Details</summary>
Motivation: ECG无法直接测量关键心脏功能参数，CMR虽准确但昂贵且难获取，需方法弥补差距。

Method: 提出PTACL框架，使用全局患者级和局部时间级对比损失，在不引入新可学习权重下丰富ECG表征。

Result: 在27,951名受试者的配对ECG - CMR数据上评估，在两项临床相关任务上比基线方法表现更好。

Conclusion: PTACL有潜力利用ECG增强无创心脏诊断。

Abstract: An electrocardiogram (ECG) is a widely used, cost-effective tool for
detecting electrical abnormalities in the heart. However, it cannot directly
measure functional parameters, such as ventricular volumes and ejection
fraction, which are crucial for assessing cardiac function. Cardiac magnetic
resonance (CMR) is the gold standard for these measurements, providing detailed
structural and functional insights, but is expensive and less accessible. To
bridge this gap, we propose PTACL (Patient and Temporal Alignment Contrastive
Learning), a multimodal contrastive learning framework that enhances ECG
representations by integrating spatio-temporal information from CMR. PTACL uses
global patient-level contrastive loss and local temporal-level contrastive
loss. The global loss aligns patient-level representations by pulling ECG and
CMR embeddings from the same patient closer together, while pushing apart
embeddings from different patients. Local loss enforces fine-grained temporal
alignment within each patient by contrasting encoded ECG segments with
corresponding encoded CMR frames. This approach enriches ECG representations
with diagnostic information beyond electrical activity and transfers more
insights between modalities than global alignment alone, all without
introducing new learnable weights. We evaluate PTACL on paired ECG-CMR data
from 27,951 subjects in the UK Biobank. Compared to baseline approaches, PTACL
achieves better performance in two clinically relevant tasks: (1) retrieving
patients with similar cardiac phenotypes and (2) predicting CMR-derived cardiac
function parameters, such as ventricular volumes and ejection fraction. Our
results highlight the potential of PTACL to enhance non-invasive cardiac
diagnostics using ECG. The code is available at:
https://github.com/alsalivan/ecgcmr

</details>


### [252] [U-R-VEDA: Integrating UNET, Residual Links, Edge and Dual Attention, and Vision Transformer for Accurate Semantic Segmentation of CMRs](https://arxiv.org/abs/2506.20689)
*Racheal Mukisa,Arvind K. Bansal*

Main category: eess.IV

TL;DR: 提出基于深度学习的增强UNet模型U - R - Veda用于心脏磁共振图像语义分割，性能表现佳。


<details>
  <summary>Details</summary>
Motivation: 人工智能在心脏疾病医学图像分析有重要作用，自动准确勾勒心脏图像是疾病量化和诊断的必要初始步骤。

Method: 提出U - R - Veda模型，集成卷积变换、视觉变换器、残差链接、通道和空间注意力以及基于边缘检测的跳跃连接，还给出双注意力模块算法。

Result: U - R - Veda基于DSC指标平均准确率达95.2%，在DSC和HD指标上优于其他模型，尤其在右心室和左心室心肌勾勒方面。

Conclusion: U - R - Veda模型显著改善心脏磁共振图像语义分割，利于医学图像分析。

Abstract: Artificial intelligence, including deep learning models, will play a
transformative role in automated medical image analysis for the diagnosis of
cardiac disorders and their management. Automated accurate delineation of
cardiac images is the first necessary initial step for the quantification and
automated diagnosis of cardiac disorders. In this paper, we propose a deep
learning based enhanced UNet model, U-R-Veda, which integrates convolution
transformations, vision transformer, residual links, channel-attention, and
spatial attention, together with edge-detection based skip-connections for an
accurate fully-automated semantic segmentation of cardiac magnetic resonance
(CMR) images. The model extracts local-features and their interrelationships
using a stack of combination convolution blocks, with embedded channel and
spatial attention in the convolution block, and vision transformers. Deep
embedding of channel and spatial attention in the convolution block identifies
important features and their spatial localization. The combined edge
information with channel and spatial attention as skip connection reduces
information-loss during convolution transformations. The overall model
significantly improves the semantic segmentation of CMR images necessary for
improved medical image analysis. An algorithm for the dual attention module
(channel and spatial attention) has been presented. Performance results show
that U-R-Veda achieves an average accuracy of 95.2%, based on DSC metrics. The
model outperforms the accuracy attained by other models, based on DSC and HD
metrics, especially for the delineation of right-ventricle and
left-ventricle-myocardium.

</details>


### [253] [A Novel Framework for Integrating 3D Ultrasound into Percutaneous Liver Tumour Ablation](https://arxiv.org/abs/2506.21162)
*Shuwei Xing,Derek W. Cool,David Tessier,Elvis C. S. Chen,Terry M. Peters,Aaron Fenster*

Main category: eess.IV

TL;DR: 提出将3D超声集成到标准消融工作流程的框架，含2D US - CT/MRI配准方法和可视化技术，验证了配准工作流程有效性，推动3D超声在经皮肿瘤消融应用。


<details>
  <summary>Details</summary>
Motivation: 3D超声成像对经皮肝肿瘤消融有好处，但超声图像中肿瘤识别挑战阻碍其广泛应用，需将3D超声集成到治疗领域。

Method: 提出将3D US集成到标准消融工作流程的框架，给出临床可行的2D US - CT/MRI配准方法，利用3D US作中介降低配准复杂度，还提出多模态图像可视化技术。

Result: 2D US - CT/MRI配准地标距离误差约2 - 4 mm，每对图像运行时间0.22s，非刚性配准比刚性配准平均对齐误差降低约40%。

Conclusion: 所提集成框架提升了3D超声成像在经皮肿瘤消融中的能力，展示了3D超声在临床干预中扩展治疗作用的潜力。

Abstract: 3D ultrasound (US) imaging has shown significant benefits in enhancing the
outcomes of percutaneous liver tumour ablation. Its clinical integration is
crucial for transitioning 3D US into the therapeutic domain. However,
challenges of tumour identification in US images continue to hinder its broader
adoption. In this work, we propose a novel framework for integrating 3D US into
the standard ablation workflow. We present a key component, a clinically viable
2D US-CT/MRI registration approach, leveraging 3D US as an intermediary to
reduce registration complexity. To facilitate efficient verification of the
registration workflow, we also propose an intuitive multimodal image
visualization technique. In our study, 2D US-CT/MRI registration achieved a
landmark distance error of approximately 2-4 mm with a runtime of 0.22s per
image pair. Additionally, non-rigid registration reduced the mean alignment
error by approximately 40% compared to rigid registration. Results demonstrated
the efficacy of the proposed 2D US-CT/MRI registration workflow. Our
integration framework advanced the capabilities of 3D US imaging in improving
percutaneous tumour ablation, demonstrating the potential to expand the
therapeutic role of 3D US in clinical interventions.

</details>


### [254] [Exploring the Design Space of 3D MLLMs for CT Report Generation](https://arxiv.org/abs/2506.21535)
*Mohammed Baharoon,Jun Ma,Congyu Fang,Augustin Toma,Bo Wang*

Main category: eess.IV

TL;DR: 本文系统研究3D多模态大语言模型用于CT报告生成的设计空间，引入知识增强方法，在AMOS - MM挑战中获第2名，并得出关于LLM大小、体积大小和使用分割掩码的相关结论，代码开源。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型为放射学报告生成提供了有前景的方法，作者旨在系统研究3D多模态大语言模型在CT报告生成中的设计空间。

Method: 系统研究3D MLLMs的设计空间，包括视觉输入表示、投影器、大语言模型和微调技术，引入两种基于知识的报告增强方法。

Result: 知识增强方法使GREEN分数提高达10%，在MICCAI 2024 AMOS - MM挑战中获第2名；RRG在相同训练协议下与LLM大小基本无关；原ViT在较小体积上预训练时，更大体积不一定提升性能；使用分割掩码可提升性能。

Conclusion: 研究为3D多模态大语言模型在放射学报告生成中的应用提供了有价值的见解和方法。

Abstract: Multimodal Large Language Models (MLLMs) have emerged as a promising way to
automate Radiology Report Generation (RRG). In this work, we systematically
investigate the design space of 3D MLLMs, including visual input
representation, projectors, Large Language Models (LLMs), and fine-tuning
techniques for 3D CT report generation. We also introduce two knowledge-based
report augmentation methods that improve performance on the GREEN score by up
to 10\%, achieving the 2nd place on the MICCAI 2024 AMOS-MM challenge. Our
results on the 1,687 cases from the AMOS-MM dataset show that RRG is largely
independent of the size of LLM under the same training protocol. We also show
that larger volume size does not always improve performance if the original ViT
was pre-trained on a smaller volume size. Lastly, we show that using a
segmentation mask along with the CT volume improves performance. The code is
publicly available at https://github.com/bowang-lab/AMOS-MM-Solution

</details>
