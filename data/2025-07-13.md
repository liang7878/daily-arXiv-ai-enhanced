<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 26]
- [cs.CE](#cs.CE) [Total: 2]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.DS](#cs.DS) [Total: 5]
- [cs.GT](#cs.GT) [Total: 4]
- [cs.IR](#cs.IR) [Total: 5]
- [cs.LG](#cs.LG) [Total: 87]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.SE](#cs.SE) [Total: 6]
- [q-fin.PM](#q-fin.PM) [Total: 2]
- [stat.ML](#stat.ML) [Total: 6]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 3]
- [stat.ME](#stat.ME) [Total: 3]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [cs.GR](#cs.GR) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [math.OC](#math.OC) [Total: 1]
- [cs.NI](#cs.NI) [Total: 2]
- [math.NA](#math.NA) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [cs.SD](#cs.SD) [Total: 3]
- [econ.GN](#econ.GN) [Total: 2]
- [eess.SY](#eess.SY) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [hep-ph](#hep-ph) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.CL](#cs.CL) [Total: 30]
- [math.MG](#math.MG) [Total: 1]
- [cs.CV](#cs.CV) [Total: 28]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [q-fin.GN](#q-fin.GN) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [cs.CR](#cs.CR) [Total: 6]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.RO](#cs.RO) [Total: 5]
- [stat.AP](#stat.AP) [Total: 1]
- [eess.IV](#eess.IV) [Total: 3]
- [cs.CY](#cs.CY) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Autonomous Control Leveraging LLMs: An Agentic Framework for Next-Generation Industrial Automation](https://arxiv.org/abs/2507.07115)
*Javal Vyas,Mehmet Mercangoz*

Main category: cs.AI

TL;DR: 本文引入统一的智能框架，利用大语言模型进行离散故障恢复规划和连续过程控制，通过案例验证其有效性，展示了大语言模型在化工自动化中的潜力。


<details>
  <summary>Details</summary>
Motivation: 现代化学过程复杂度增加、劳动力短缺和故障场景复杂，需要结合符号推理和自适应控制的新自动化范式。

Method: 引入统一智能框架，采用有限状态机作为可解释操作范围，利用大语言模型规划，模拟代理执行和检查，验证 - 重新提示循环优化计划。

Result: 案例 1 中，GPT - 4o 和 GPT - 4o - mini 在五次重新提示内实现 100% 有效路径成功率；案例 2 中，基于大语言模型的控制器与经典 PID 控制性能相当。

Conclusion: 通过结构化反馈和模块化代理，大语言模型可统一高级符号规划和低级连续控制，为化工工程中基于语言的弹性自动化铺平道路。

Abstract: The increasing complexity of modern chemical processes, coupled with
workforce shortages and intricate fault scenarios, demands novel automation
paradigms that blend symbolic reasoning with adaptive control. In this work, we
introduce a unified agentic framework that leverages large language models
(LLMs) for both discrete fault-recovery planning and continuous process control
within a single architecture. We adopt Finite State Machines (FSMs) as
interpretable operating envelopes: an LLM-driven planning agent proposes
recovery sequences through the FSM, a Simulation Agent executes and checks each
transition, and a Validator-Reprompting loop iteratively refines invalid plans.
In Case Study 1, across 180 randomly generated FSMs of varying sizes (4-25
states, 4-300 transitions), GPT-4o and GPT-4o-mini achieve 100% valid-path
success within five reprompts-outperforming open-source LLMs in both accuracy
and latency. In Case Study 2, the same framework modulates dual-heater inputs
on a laboratory TCLab platform (and its digital twin) to maintain a target
average temperature under persistent asymmetric disturbances. Compared to
classical PID control, our LLM-based controller attains similar performance,
while ablation of the prompting loop reveals its critical role in handling
nonlinear dynamics. We analyze key failure modes-such as instruction following
lapses and coarse ODE approximations. Our results demonstrate that, with
structured feedback and modular agents, LLMs can unify high-level symbolic
planningand low-level continuous control, paving the way towards resilient,
language-driven automation in chemical engineering.

</details>


### [2] [BOOST: Out-of-Distribution-Informed Adaptive Sampling for Bias Mitigation in Stylistic Convolutional Neural Networks](https://arxiv.org/abs/2507.07134)
*Mridula Vijendran,Shuang Chen,Jingjing Deng,Hubert P. H. Shum*

Main category: cs.AI

TL;DR: 文章指出AI偏差影响绘画分类，提出BOOST方法解决偏差问题，并设计SODC指标评估，方法能兼顾性能与公平。


<details>
  <summary>Details</summary>
Motivation: AI偏差在绘画分类中日益严重，现有研究忽视解决潜在偏差，需要更强大的偏差缓解方法。

Method: 提出BOOST方法，通过动态调整温度缩放和采样概率；设计SODC指标评估类间分离和类偏差减少情况。

Result: 在KaoKore和PACS数据集上评估，方法能减少类偏差。

Conclusion: 该方法是艺术领域消除AI模型偏差的可靠解决方案，能兼顾高性能与公平性。

Abstract: The pervasive issue of bias in AI presents a significant challenge to
painting classification, and is getting more serious as these systems become
increasingly integrated into tasks like art curation and restoration. Biases,
often arising from imbalanced datasets where certain artistic styles dominate,
compromise the fairness and accuracy of model predictions, i.e., classifiers
are less accurate on rarely seen paintings. While prior research has made
strides in improving classification performance, it has largely overlooked the
critical need to address these underlying biases, that is, when dealing with
out-of-distribution (OOD) data. Our insight highlights the necessity of a more
robust approach to bias mitigation in AI models for art classification on
biased training data. We propose a novel OOD-informed model bias adaptive
sampling method called BOOST (Bias-Oriented OOD Sampling and Tuning). It
addresses these challenges by dynamically adjusting temperature scaling and
sampling probabilities, thereby promoting a more equitable representation of
all classes. We evaluate our proposed approach to the KaoKore and PACS
datasets, focusing on the model's ability to reduce class-wise bias. We further
propose a new metric, Same-Dataset OOD Detection Score (SODC), designed to
assess class-wise separation and per-class bias reduction. Our method
demonstrates the ability to balance high performance with fairness, making it a
robust solution for unbiasing AI models in the art domain.

</details>


### [3] [State-Inference-Based Prompting for Natural Language Trading with Game NPCs](https://arxiv.org/abs/2507.07203)
*Minkyung Kim,Junsik Kim,Hwidong Bae,Woongcheol Yang,Sangdon Park,Sohee Bae*

Main category: cs.AI

TL;DR: 大语言模型在规则交易系统有问题，SIBP方法解决交易问题，评估效果好，为游戏NPC交互奠定基础。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在规则交易系统存在规则违反问题，如物品幻觉和计算错误，影响玩家信任。

Method: 提出基于状态推理的提示（SIBP）方法，将交易分解为六个状态，实现上下文感知的物品引用和基于占位符的价格计算。

Result: 在100个交易对话评估中，状态合规率>97%，引用准确率>95%，计算精度99.7%，计算效率高且优于基线方法。

Conclusion: SIBP为商业游戏中可信的NPC交互建立了实用基础。

Abstract: Large Language Models enable dynamic game interactions but struggle with
rule-governed trading systems. Current implementations suffer from rule
violations, such as item hallucinations and calculation errors, that erode
player trust. Here, State-Inference-Based Prompting (SIBP) enables reliable
trading through autonomous dialogue state inference and context-specific rule
adherence. The approach decomposes trading into six states within a unified
prompt framework, implementing context-aware item referencing and
placeholder-based price calculations. Evaluation across 100 trading dialogues
demonstrates >97% state compliance, >95% referencing accuracy, and 99.7%
calculation precision. SIBP maintains computational efficiency while
outperforming baseline approaches, establishing a practical foundation for
trustworthy NPC interactions in commercial games.

</details>


### [4] [Neurosymbolic Feature Extraction for Identifying Forced Labor in Supply Chains](https://arxiv.org/abs/2507.07217)
*Zili Wang,Frank Montabon,Kristin Yvonne Rozier*

Main category: cs.AI

TL;DR: 本文探讨利用神经符号方法识别供应链中的非法活动，对比新闻文章特征提取方式，提出用问题树查询大语言模型评估人、机对文章分类差异。


<details>
  <summary>Details</summary>
Motivation: 供应链网络复杂，尤其是存在非法活动时更难分析，传统机器学习需大量训练数据，而非法供应链数据稀疏、不可靠，需无需大量训练数据的方法自动检测非法活动新模式。

Method: 探索神经符号方法识别非法活动，对比手动和自动特征提取效果，提出用问题树查询大语言模型识别和量化文章相关性。

Result: 文中未提及明确结果。

Conclusion: 文中未提及明确结论。

Abstract: Supply chain networks are complex systems that are challenging to analyze;
this problem is exacerbated when there are illicit activities involved in the
supply chain, such as counterfeit parts, forced labor, or human trafficking.
While machine learning (ML) can find patterns in complex systems like supply
chains, traditional ML techniques require large training data sets. However,
illicit supply chains are characterized by very sparse data, and the data that
is available is often (purposely) corrupted or unreliable in order to hide the
nature of the activities. We need to be able to automatically detect new
patterns that correlate with such illegal activity over complex, even temporal
data, without requiring large training data sets. We explore neurosymbolic
methods for identifying instances of illicit activity in supply chains and
compare the effectiveness of manual and automated feature extraction from news
articles accurately describing illicit activities uncovered by authorities. We
propose a question tree approach for querying a large language model (LLM) to
identify and quantify the relevance of articles. This enables a systematic
evaluation of the differences between human and machine classification of news
articles related to forced labor in supply chains.

</details>


### [5] [Working with AI: Measuring the Occupational Implications of Generative AI](https://arxiv.org/abs/2507.07935)
*Kiran Tomlinson,Sonia Jaffe,Will Wang,Scott Counts,Siddharth Suri*

Main category: cs.AI

TL;DR: 分析用户与微软Bing Copilot对话，计算各职业AI适用性得分，发现知识工作职业组适用性高，还研究工作活动类型、工资教育与AI适用性关联等。


<details>
  <summary>Details</summary>
Motivation: 理解AI对经济的影响，这是社会重要问题。

Method: 分析20万条用户与微软Bing Copilot匿名隐私处理对话数据集，结合活动分类、任务成功率和影响范围计算各职业AI适用性得分。

Result: 人们最常寻求AI协助的工作活动是信息收集和写作，AI主要进行信息提供、协助、写作、教学和建议；知识工作职业组如计算机和数学、办公行政支持以及销售等职业AI适用性得分高。

Conclusion: 完成对人们使用AI工作活动的分析，计算出职业AI适用性得分，并对工作活动类型、工资教育与AI适用性关系等进行了表征。

Abstract: Given the rapid adoption of generative AI and its potential to impact a wide
range of tasks, understanding the effects of AI on the economy is one of
society's most important questions. In this work, we take a step toward that
goal by analyzing the work activities people do with AI, how successfully and
broadly those activities are done, and combine that with data on what
occupations do those activities. We analyze a dataset of 200k anonymized and
privacy-scrubbed conversations between users and Microsoft Bing Copilot, a
publicly available generative AI system. We find the most common work
activities people seek AI assistance for involve gathering information and
writing, while the most common activities that AI itself is performing are
providing information and assistance, writing, teaching, and advising.
Combining these activity classifications with measurements of task success and
scope of impact, we compute an AI applicability score for each occupation. We
find the highest AI applicability scores for knowledge work occupation groups
such as computer and mathematical, and office and administrative support, as
well as occupations such as sales whose work activities involve providing and
communicating information. Additionally, we characterize the types of work
activities performed most successfully, how wage and education correlate with
AI applicability, and how real-world usage compares to predictions of
occupational AI impact.

</details>


### [6] [Open Source Planning & Control System with Language Agents for Autonomous Scientific Discovery](https://arxiv.org/abs/2507.07257)
*Licong Xu,Milind Sarkar,Anto I. Lonappan,Íñigo Zubeldia,Pablo Villanueva-Domingo,Santiago Casas,Christian Fidler,Chetana Amancharla,Ujjwal Tiwari,Adrian Bayer,Chadi Ait Ekiou,Miles Cranmer,Adrian Dimitrov,James Fergusson,Kahaan Gandhi,Sven Krippendorf,Andrew Laverick,Julien Lesgourgues,Antony Lewis,Thomas Meier,Blake Sherwin,Kristen Surrao,Francisco Villaescusa-Navarro,Chi Wang,Xueqing Xu,Boris Bolliet*

Main category: cs.AI

TL;DR: 介绍多智能体系统cmbagent，由约30个大语言模型智能体组成，能自动化科研任务，在宇宙学任务和基准测试集上表现优异，代码开源，系统可云部署。


<details>
  <summary>Details</summary>
Motivation: 实现科学研究任务的自动化，减少人工干预。

Method: 构建约30个大语言模型智能体组成的系统，采用规划与控制策略编排工作流，各智能体分工合作并可本地执行代码。

Result: 成功应用于博士水平的宇宙学任务，在两个基准测试集上表现优于现有大语言模型。

Conclusion: cmbagent系统在科研任务自动化方面表现出色，具有较高的应用价值，代码开源和云部署方便使用。

Abstract: We present a multi-agent system for automation of scientific research tasks,
cmbagent. The system is formed by about 30 Large Language Model (LLM) agents
and implements a Planning & Control strategy to orchestrate the agentic
workflow, with no human-in-the-loop at any point. Each agent specializes in a
different task (performing retrieval on scientific papers and codebases,
writing code, interpreting results, critiquing the output of other agents) and
the system is able to execute code locally. We successfully apply cmbagent to
carry out a PhD level cosmology task (the measurement of cosmological
parameters using supernova data) and evaluate its performance on two benchmark
sets, finding superior performance over state-of-the-art LLMs. The source code
is available on GitHub, demonstration videos are also available, and the system
is deployed on HuggingFace and will be available on the cloud.

</details>


### [7] [Application of LLMs to Multi-Robot Path Planning and Task Allocation](https://arxiv.org/abs/2507.07302)
*Ashish Kumar*

Main category: cs.AI

TL;DR: 研究用大语言模型作为专家规划器，解决多智能体强化学习高效探索问题


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习中高效探索问题因算法复杂性而加剧，需有效探索方法

Method: 将大语言模型作为专家规划器用于多智能体基于规划的任务中进行高效探索

Result: 未提及

Conclusion: 未提及

Abstract: Efficient exploration is a well known problem in deep reinforcement learning
and this problem is exacerbated in multi-agent reinforcement learning due the
intrinsic complexities of such algorithms. There are several approaches to
efficiently explore an environment to learn to solve tasks by multi-agent
operating in that environment, of which, the idea of expert exploration is
investigated in this work. More specifically, this work investigates the
application of large-language models as expert planners for efficient
exploration in planning based tasks for multiple agents.

</details>


### [8] [ViDove: A Translation Agent System with Multimodal Context and Memory-Augmented Reasoning](https://arxiv.org/abs/2507.07306)
*Yichen Lu,Wei Dai,Jiaen Liu,Ching Wing Kwok,Zongheng Wu,Xudong Xiao,Ao Sun,Sheng Fu,Jianyuan Zhan,Yian Wang,Takatomo Saito,Sicheng Lai*

Main category: cs.AI

TL;DR: 本文介绍多模态翻译代理系统ViDove，其提升了翻译质量，还引入新基准DoveBench。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大语言模型的翻译代理通常限于文本输入，需要设计多模态输入的翻译代理系统。

Method: 受人类翻译工作流程启发，利用视觉和上下文背景信息，集成多模态记忆系统和富含特定领域知识的长短时记忆模块。

Result: ViDove在字幕生成和一般翻译任务中显著提高翻译质量，BLEU分数提高28%，SubER提高15%；引入新基准DoveBench。

Conclusion: ViDove能在多模态输入场景下更准确、自适应地进行翻译，在相关翻译任务中有出色表现。

Abstract: LLM-based translation agents have achieved highly human-like translation
results and are capable of handling longer and more complex contexts with
greater efficiency. However, they are typically limited to text-only inputs. In
this paper, we introduce ViDove, a translation agent system designed for
multimodal input. Inspired by the workflow of human translators, ViDove
leverages visual and contextual background information to enhance the
translation process. Additionally, we integrate a multimodal memory system and
long-short term memory modules enriched with domain-specific knowledge,
enabling the agent to perform more accurately and adaptively in real-world
scenarios. As a result, ViDove achieves significantly higher translation
quality in both subtitle generation and general translation tasks, with a 28%
improvement in BLEU scores and a 15% improvement in SubER compared to previous
state-of-the-art baselines. Moreover, we introduce DoveBench, a new benchmark
for long-form automatic video subtitling and translation, featuring 17 hours of
high-quality, human-annotated data. Our code is available here:
https://github.com/pigeonai-org/ViDove

</details>


### [9] [On the Impossibility of Separating Intelligence from Judgment: The Computational Intractability of Filtering for AI Alignment](https://arxiv.org/abs/2507.07341)
*Sarah Ball,Greg Gluch,Shafi Goldwasser,Frauke Kreuter,Omer Reingold,Guy N. Rothblum*

Main category: cs.AI

TL;DR: 研究大语言模型安全过滤问题，指出输入输出过滤计算有挑战，认为仅外部过滤无法实现安全。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型部署增加，关注其生成有害内容问题，研究防止生成不安全信息的过滤方法。

Method: 研究输入提示和输出过滤两个干预点，在密码学硬度假设下进行分析，还研究了缓解方法。

Result: 发现存在大语言模型没有高效提示过滤器，输出过滤在自然场景下计算不可行，研究缓解方法也有计算障碍。

Conclusion: 仅设计大语言模型外部过滤器无法实现安全，对齐的人工智能系统的智能和判断不能分离。

Abstract: With the increased deployment of large language models (LLMs), one concern is
their potential misuse for generating harmful content. Our work studies the
alignment challenge, with a focus on filters to prevent the generation of
unsafe information. Two natural points of intervention are the filtering of the
input prompt before it reaches the model, and filtering the output after
generation. Our main results demonstrate computational challenges in filtering
both prompts and outputs. First, we show that there exist LLMs for which there
are no efficient prompt filters: adversarial prompts that elicit harmful
behavior can be easily constructed, which are computationally indistinguishable
from benign prompts for any efficient filter. Our second main result identifies
a natural setting in which output filtering is computationally intractable. All
of our separation results are under cryptographic hardness assumptions. In
addition to these core findings, we also formalize and study relaxed mitigation
approaches, demonstrating further computational barriers. We conclude that
safety cannot be achieved by designing filters external to the LLM internals
(architecture and weights); in particular, black-box access to the LLM will not
suffice. Based on our technical results, we argue that an aligned AI system's
intelligence cannot be separated from its judgment.

</details>


### [10] [Supply Chain Optimization via Generative Simulation and Iterative Decision Policies](https://arxiv.org/abs/2507.07355)
*Haoyue Bai,Haoyu Wang,Nanxu Gong,Xinyuan Wang,Wangyang Ying,Haifeng Chen,Yanjie Fu*

Main category: cs.AI

TL;DR: 提出Sim - to - Dec框架用于供应链运输策略设计，实验表明其能提升及时交付率和利润。


<details>
  <summary>Details</summary>
Motivation: 高响应性和经济效率是供应链运输关键目标，受运输方式战略决策影响，需要一个理想的模拟 - 决策框架。

Method: 提出Sim - to - Dec框架，包含利用自回归建模的生成模拟模块和通过端到端优化迭代改进的历史 - 未来双感知决策模型。

Result: 在三个真实世界数据集上的大量实验显示，Sim - to - Dec显著提高了及时交付率和利润。

Conclusion: Sim - to - Dec框架满足理想模拟 - 决策框架的要求，能有效提升供应链运输的性能。

Abstract: High responsiveness and economic efficiency are critical objectives in supply
chain transportation, both of which are influenced by strategic decisions on
shipping mode. An integrated framework combining an efficient simulator with an
intelligent decision-making algorithm can provide an observable, low-risk
environment for transportation strategy design. An ideal simulation-decision
framework must (1) generalize effectively across various settings, (2) reflect
fine-grained transportation dynamics, (3) integrate historical experience with
predictive insights, and (4) maintain tight integration between simulation
feedback and policy refinement. We propose Sim-to-Dec framework to satisfy
these requirements. Specifically, Sim-to-Dec consists of a generative
simulation module, which leverages autoregressive modeling to simulate
continuous state changes, reducing dependence on handcrafted domain-specific
rules and enhancing robustness against data fluctuations; and a history-future
dual-aware decision model, refined iteratively through end-to-end optimization
with simulator interactions. Extensive experiments conducted on three
real-world datasets demonstrate that Sim-to-Dec significantly improves timely
delivery rates and profit.

</details>


### [11] [DrugMCTS: a drug repurposing framework combining multi-agent, RAG and Monte Carlo Tree Search](https://arxiv.org/abs/2507.07426)
*Zerui Yang,Yuwei Wan,Yinqiao Li,Yudai Matsuda,Tong Xie,Linqi Song*

Main category: cs.AI

TL;DR: 提出DrugMCTS框架用于药物再利用，无需特定微调，表现优于对比模型，凸显结构化推理等的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在科学领域推理受限，传统方法有计算开销大或无法充分利用科学数据的问题。

Method: 提出DrugMCTS框架，整合RAG、多智能体协作和蒙特卡罗树搜索，使用五个专业智能体检索和分析信息。

Result: DrugMCTS使Qwen2.5 - 7B - Instruct比Deepseek - R1性能高超20%，在数据集上召回率和鲁棒性更优。

Conclusion: 结构化推理、基于智能体的协作和反馈驱动的搜索机制对推进大语言模型在药物发现中的应用很重要。

Abstract: Recent advances in large language models have demonstrated considerable
potential in scientific domains such as drug discovery. However, their
effectiveness remains constrained when reasoning extends beyond the knowledge
acquired during pretraining. Conventional approaches, such as fine-tuning or
retrieval-augmented generation, face limitations in either imposing high
computational overhead or failing to fully exploit structured scientific data.
To overcome these challenges, we propose DrugMCTS, a novel framework that
synergistically integrates RAG, multi-agent collaboration, and Monte Carlo Tree
Search for drug repurposing. The framework employs five specialized agents
tasked with retrieving and analyzing molecular and protein information, thereby
enabling structured and iterative reasoning. Without requiring domain-specific
fine-tuning, DrugMCTS empowers Qwen2.5-7B-Instruct to outperform Deepseek-R1 by
over 20\%. Extensive experiments on the DrugBank and KIBA datasets demonstrate
that DrugMCTS achieves substantially higher recall and robustness compared to
both general-purpose LLMs and deep learning baselines. Our results highlight
the importance of structured reasoning, agent-based collaboration, and
feedback-driven search mechanisms in advancing LLM applications for drug
discovery.

</details>


### [12] [StarDojo: Benchmarking Open-Ended Behaviors of Agentic Multimodal LLMs in Production-Living Simulations with Stardew Valley](https://arxiv.org/abs/2507.07445)
*Weihao Tan,Changjiu Jiang,Yu Duan,Mingcong Lei,Jiageng Li,Yitian Hong,Xinrun Wang,Bo An*

Main category: cs.AI

TL;DR: 引入基于《星露谷物语》的基准测试StarDojo评估AI代理，评估显示现有模型有局限，旨在推动复杂生产生活环境中智能体研究。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试很少同时评估AI代理在生产活动和社交互动方面的技能，需要新基准来填补空白。

Method: 基于《星露谷物语》创建StarDojo，有1000个精心策划任务，含5个关键领域，还有100个代表性任务子集，提供统一友好界面。

Result: 对最先进的多模态大语言模型代理的广泛评估显示有很大局限性，表现最好的GPT - 4.1成功率仅12.7%。

Conclusion: StarDojo作为用户友好的环境和基准，有助于推动复杂生产生活环境中强大、开放式智能体的进一步研究。

Abstract: Autonomous agents navigating human society must master both production
activities and social interactions, yet existing benchmarks rarely evaluate
these skills simultaneously. To bridge this gap, we introduce StarDojo, a novel
benchmark based on Stardew Valley, designed to assess AI agents in open-ended
production-living simulations. In StarDojo, agents are tasked to perform
essential livelihood activities such as farming and crafting, while
simultaneously engaging in social interactions to establish relationships
within a vibrant community. StarDojo features 1,000 meticulously curated tasks
across five key domains: farming, crafting, exploration, combat, and social
interactions. Additionally, we provide a compact subset of 100 representative
tasks for efficient model evaluation. The benchmark offers a unified,
user-friendly interface that eliminates the need for keyboard and mouse
control, supports all major operating systems, and enables the parallel
execution of multiple environment instances, making it particularly well-suited
for evaluating the most capable foundation agents, powered by multimodal large
language models (MLLMs). Extensive evaluations of state-of-the-art MLLMs agents
demonstrate substantial limitations, with the best-performing model, GPT-4.1,
achieving only a 12.7% success rate, primarily due to challenges in visual
understanding, multimodal reasoning and low-level manipulation. As a
user-friendly environment and benchmark, StarDojo aims to facilitate further
research towards robust, open-ended agents in complex production-living
environments.

</details>


### [13] [Position: We Need An Algorithmic Understanding of Generative AI](https://arxiv.org/abs/2507.07544)
*Oliver Eberle,Thomas McGee,Hamza Giaffar,Taylor Webb,Ida Momennejad*

Main category: cs.AI

TL;DR: 提出AlgEval框架研究大语言模型学习和使用的算法，以案例说明方法，为模型计算提供可解释性及优化方向。


<details>
  <summary>Details</summary>
Motivation: 当前研究多聚焦模型规模提升性能，缺少对大语言模型解决问题所用算法的研究，存在理论和实证差距。

Method: 提出AlgEval框架，通过案例研究聚焦涌现搜索算法，采用自上而下提出假设、自下而上通过电路级分析验证假设。

Result: 以案例展示了如何研究大语言模型解决任务的算法。

Conclusion: 对大语言模型解决任务的严格系统评估可替代资源密集型的模型规模扩展，带来模型可解释性，有助于训练和提升性能及开发新架构。

Abstract: What algorithms do LLMs actually learn and use to solve problems? Studies
addressing this question are sparse, as research priorities are focused on
improving performance through scale, leaving a theoretical and empirical gap in
understanding emergent algorithms. This position paper proposes AlgEval: a
framework for systematic research into the algorithms that LLMs learn and use.
AlgEval aims to uncover algorithmic primitives, reflected in latent
representations, attention, and inference-time compute, and their algorithmic
composition to solve task-specific problems. We highlight potential
methodological paths and a case study toward this goal, focusing on emergent
search algorithms. Our case study illustrates both the formation of top-down
hypotheses about candidate algorithms, and bottom-up tests of these hypotheses
via circuit-level analysis of attention patterns and hidden states. The
rigorous, systematic evaluation of how LLMs actually solve tasks provides an
alternative to resource-intensive scaling, reorienting the field toward a
principled understanding of underlying computations. Such algorithmic
explanations offer a pathway to human-understandable interpretability, enabling
comprehension of the model's internal reasoning performance measures. This can
in turn lead to more sample-efficient methods for training and improving
performance, as well as novel architectures for end-to-end and multi-agent
systems.

</details>


### [14] [On Trustworthy Rule-Based Models and Explanations](https://arxiv.org/abs/2507.07576)
*Mohamed Siala,Jordi Planes,Joao Marques-Silva*

Main category: cs.AI

TL;DR: 论文探讨基于规则的机器学习模型解释与不良特征关系，开发分析算法，指出常用学习工具会使规则集有负面特征。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域，机器学习模型解释的严谨性至关重要，需要研究基于规则的机器学习模型解释与不良特征的关系。

Method: 开发用于分析基于规则系统不良特征的算法。

Result: 常用的学习基于规则的机器学习模型的工具会使规则集呈现一种或多种负面特征。

Conclusion: 常用学习基于规则的机器学习模型的工具会导致规则集有负面特征。

Abstract: A task of interest in machine learning (ML) is that of ascribing explanations
to the predictions made by ML models. Furthermore, in domains deemed high risk,
the rigor of explanations is paramount. Indeed, incorrect explanations can and
will mislead human decision makers. As a result, and even if interpretability
is acknowledged as an elusive concept, so-called interpretable models are
employed ubiquitously in high-risk uses of ML and data mining (DM). This is the
case for rule-based ML models, which encompass decision trees, diagrams, sets
and lists. This paper relates explanations with well-known undesired facets of
rule-based ML models, which include negative overlap and several forms of
redundancy. The paper develops algorithms for the analysis of these undesired
facets of rule-based systems, and concludes that well-known and widely used
tools for learning rule-based ML models will induce rule sets that exhibit one
or more negative facets.

</details>


### [15] [Context Pooling: Query-specific Graph Pooling for Generic Inductive Link Prediction in Knowledge Graphs](https://arxiv.org/abs/2507.07595)
*Zhixiang Su,Di Wang,Chunyan Miao*

Main category: cs.AI

TL;DR: 本文提出Context Pooling方法提升知识图谱链接预测中基于GNN模型的效果，在多个数据集上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 近期研究表明普通聚合对知识图谱链接预测中基于GNN模型的性能影响不大，需要新方法提升其效果。

Method: 提出Context Pooling方法，它是首个在知识图谱中应用图池化的方法，能为归纳设置生成特定查询图；设计邻域精度和邻域召回两个指标评估邻居逻辑相关性以识别相关邻居用于链接预测。

Result: 将该方法应用于两个SOTA模型和三个公共数据集，在48种设置中的42种取得SOTA性能。

Conclusion: Context Pooling方法能有效提升知识图谱链接预测中基于GNN模型的性能。

Abstract: Recent investigations on the effectiveness of Graph Neural Network
(GNN)-based models for link prediction in Knowledge Graphs (KGs) show that
vanilla aggregation does not significantly impact the model performance. In
this paper, we introduce a novel method, named Context Pooling, to enhance
GNN-based models' efficacy for link predictions in KGs. To our best of
knowledge, Context Pooling is the first methodology that applies graph pooling
in KGs. Additionally, Context Pooling is first-of-its-kind to enable the
generation of query-specific graphs for inductive settings, where testing
entities are unseen during training. Specifically, we devise two metrics,
namely neighborhood precision and neighborhood recall, to assess the neighbors'
logical relevance regarding the given queries, thereby enabling the subsequent
comprehensive identification of only the logically relevant neighbors for link
prediction. Our method is generic and assessed by being applied to two
state-of-the-art (SOTA) models on three public transductive and inductive
datasets, achieving SOTA performance in 42 out of 48 settings.

</details>


### [16] [Enhancing Vaccine Safety Surveillance: Extracting Vaccine Mentions from Emergency Department Triage Notes Using Fine-Tuned Large Language Models](https://arxiv.org/abs/2507.07599)
*Sedigh Khademi,Jim Black,Christopher Palmer,Muhammad Javed,Hazel Clothier,Jim Buttery,Gerardo Luis Dimaguila*

Main category: cs.AI

TL;DR: 研究评估微调Llama 3.2模型从急诊分诊记录中提取疫苗信息用于疫苗安全监测，微调的30亿参数模型表现佳，量化可实现资源受限部署。


<details>
  <summary>Details</summary>
Motivation: 评估微调Llama 3.2模型从急诊分诊记录中提取疫苗相关信息，以支持近实时疫苗安全监测。

Method: 使用提示工程创建标注数据集并由人工确认，比较提示工程模型、微调模型和基于规则方法的性能。

Result: 微调的Llama 30亿参数模型在提取疫苗名称的准确性上优于其他模型，模型量化可在资源受限环境高效部署。

Conclusion: 大语言模型在自动提取急诊记录数据、支持疫苗安全监测和早期检测免疫后不良事件方面有潜力。

Abstract: This study evaluates fine-tuned Llama 3.2 models for extracting
vaccine-related information from emergency department triage notes to support
near real-time vaccine safety surveillance. Prompt engineering was used to
initially create a labeled dataset, which was then confirmed by human
annotators. The performance of prompt-engineered models, fine-tuned models, and
a rule-based approach was compared. The fine-tuned Llama 3 billion parameter
model outperformed other models in its accuracy of extracting vaccine names.
Model quantization enabled efficient deployment in resource-constrained
environments. Findings demonstrate the potential of large language models in
automating data extraction from emergency department notes, supporting
efficient vaccine safety surveillance and early detection of emerging adverse
events following immunization issues.

</details>


### [17] [Towards conservative inference in credal networks using belief functions: the case of credal chains](https://arxiv.org/abs/2507.07619)
*Marco Sangalli,Thomas Krak,Cassio de Campos*

Main category: cs.AI

TL;DR: 本文基于Dempster - Shafer理论探索信任网络中的信念推理，提出新框架并分析其优缺点。


<details>
  <summary>Details</summary>
Motivation: 探索信任网络中基于Dempster - Shafer理论的信念推理方法。

Method: 在先前工作基础上，提出用于一类信任网络（链）的不确定性传播新框架，通过信念和似然函数产生保守区间。

Result: 数值结果显示了在该框架下应用信念推理的优点和局限性。

Conclusion: 为链和一般信任网络的信念推理实际应用提供了见解。

Abstract: This paper explores belief inference in credal networks using Dempster-Shafer
theory. By building on previous work, we propose a novel framework for
propagating uncertainty through a subclass of credal networks, namely chains.
The proposed approach efficiently yields conservative intervals through belief
and plausibility functions, combining computational speed with robust
uncertainty representation. Key contributions include formalizing belief-based
inference methods and comparing belief-based inference against classical
sensitivity analysis. Numerical results highlight the advantages and
limitations of applying belief inference within this framework, providing
insights into its practical utility for chains and for credal networks in
general.

</details>


### [18] [PlanQA: A Benchmark for Spatial Reasoning in LLMs using Structured Representations](https://arxiv.org/abs/2507.07644)
*Fedor Rodionov,Abdelrahman Eldesokey,Michael Birsak,John Femiani,Bernard Ghanem,Peter Wonka*

Main category: cs.AI

TL;DR: 提出诊断基准PlanQA评估大语言模型几何与空间推理能力，测试发现模型存在推理盲点。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在几何和空间推理方面的能力，发现其在处理现实世界布局时的问题。

Method: 引入基于室内场景结构化表示的PlanQA基准，包含多种问题类型测试推理能力。

Result: 各类前沿开源和商业大语言模型在浅层查询可能成功，但在模拟物理约束、保持空间连贯性和布局扰动泛化方面常失败。

Conclusion: 当前大语言模型在处理现实世界布局推理上存在明显盲点，希望该基准能启发相关新研究。

Abstract: We introduce PlanQA, a diagnostic benchmark for evaluating geometric and
spatial reasoning in large-language models (LLMs). PlanQA is grounded in
structured representations of indoor scenes, such as kitchens, living rooms,
and bedrooms, encoded in a symbolic format (e.g., JSON, XML layouts). The
benchmark includes diverse question types that test not only metric and
topological reasoning (e.g., distance, visibility, shortest paths) but also
interior design constraints such as affordance, clearance, balance, and
usability. Our results across a variety of frontier open-source and commercial
LLMs show that while models may succeed in shallow queries, they often fail to
simulate physical constraints, preserve spatial coherence, or generalize under
layout perturbation. PlanQA uncovers a clear blind spot in today's LLMs: they
do not consistently reason about real-world layouts. We hope that this
benchmark inspires new work on language models that can accurately infer and
manipulate spatial and geometric properties in practical settings.

</details>


### [19] [Stable Preference Optimization for LLMs: A Bilevel Approach Beyond Direct Preference Optimization](https://arxiv.org/abs/2507.07723)
*Chengtao Jian,Kai Yang,Ye Ouyang,Xiaozhou Ye*

Main category: cs.AI

TL;DR: 本文分析DPO局限性，提出稳定偏好优化框架，实验表明该方法优于标准DPO，为偏好对齐目标设计提供新见解。


<details>
  <summary>Details</summary>
Motivation: DPO虽经验上成功，但理论特性和内在局限未充分探索，需深入分析其问题并改进。

Method: 从概率演化角度分析DPO动态，提出结合监督微调与增强DPO目标的双层优化框架，引入正则化方案。

Result: 在推理和摘要基准测试中，方法提高推理准确性，使输出分布更符合预期偏好，优于标准DPO。

Conclusion: 稳定偏好优化为基于偏好的对齐目标设计提供新见解，为语言模型对齐开辟新途径。

Abstract: Direct Preference Optimization (DPO) has emerged as a popular and efficient
alternative to reward modeling and reinforcement learning for aligning language
models with human preferences. Despite its empirical success, the theoretical
properties and intrinsic limitations of DPO remain underexplored. In this work,
we first present a comprehensive analysis of DPO's dynamics from a probability
evolution perspective. Our analysis reveals that DPO is highly sensitive to
initialization. It also tends to misallocate probability mass, which can
inadvertently shift probability toward irrelevant or undesired responses. This
misallocation may unintentionally reinforce model bias, thereby compromising
both the stability of model alignment and the consistency with intended
preferences. Motivated by these theoretical findings, we propose a
theoretically grounded bilevel optimization framework that tightly integrate
supervised fine-tuning with an enhanced DPO objective a.k.a. stable preference
optimization. Our approach introduces a principled regularization scheme to
explicitly encourage absolute probability improvement for preferred outputs,
while maintaining stable optimization dynamics. Experiments on challenging
reasoning and summarization benchmarks elucidate that our method consistently
improves reasoning accuracy and better aligns output distributions with
intended preferences, outperforming standard DPO. Stable preference
optimization provides new insights into the design of preference-based
alignment objectives and opens up new avenues towards more reliable and
interpretable language model alignment.

</details>


### [20] [Identification of Violin Reduction via Contour Lines Classification](https://arxiv.org/abs/2507.07743)
*Philémon Beghin,Anne-Emmanuelle Ceulemans,François Glineur*

Main category: cs.AI

TL;DR: 本文提出基于轮廓线对小提琴是否经过尺寸缩减分类的方法，研究25件乐器，发现一定程度上可区分，开口参数β最具预测性。


<details>
  <summary>Details</summary>
Motivation: 小提琴尺寸缩减影响轮廓线，但差异未被定量研究，需方法对缩减和未缩减小提琴分类。

Method: 研究25件乐器3D网格，提取轮廓线并拟合抛物线，计算额外特征，处理异常值和不等数量层级，获得数值轮廓，应用分类方法。

Result: 一定程度上能区分缩减和未缩减小提琴，开口参数β最具预测性。

Conclusion: 基于几何特征对小提琴尺寸缩减分类有一定可行性，但对不同变形程度的小提琴缩减量化较难。

Abstract: The first violins appeared in late 16th-century Italy. Over the next 200
years, they spread across Europe and luthiers of various royal courts, eager to
experiment with new techniques, created a highly diverse family of instruments.
Around 1750, size standards were introduced to unify violin making for
orchestras and conservatories. Instruments that fell between two standards were
then reduced to a smaller size by luthiers. These reductions have an impact on
several characteristics of violins, in particular on the contour lines, i.e.
lines of constant altitude, which look more like a U for non reduced
instruments and a V for reduced ones. While such differences are observed by
experts, they have not been studied quantitatively.
  This paper presents a method for classifying violins as reduced or
non-reduced based on their contour lines. We study a corpus of 25 instruments
whose 3D geometric meshes were acquired via photogrammetry. For each
instrument, we extract 10-20 contour lines regularly spaced every millimetre.
Each line is fitted with a parabola-like curve (with an equation of the type y
= alpha*abs(x)**beta) depending on two parameters, describing how open (beta)
and how vertically stretched (alpha) the curve is. We compute additional
features from those parameters, using regressions and counting how many values
fall under some threshold. We also deal with outliers and non equal numbers of
levels, and eventually obtain a numerical profile for each instrument.
  We then apply classification methods to assess whether geometry alone can
predict size reduction. We find that distinguishing between reduced and non
reduced instruments is feasible to some degree, taking into account that a
whole spectrum of more or less transformed violins exists, for which it is more
difficult to quantify the reduction. We also find the opening parameter beta to
be the most predictive.

</details>


### [21] [Measuring AI Alignment with Human Flourishing](https://arxiv.org/abs/2507.07787)
*Elizabeth Hilliard,Akshaya Jagadeesh,Alex Cook,Steele Billings,Nicholas Skytland,Alicia Llewellyn,Jackson Paull,Nathan Paull,Nolan Kurylo,Keatra Nesbitt,Robert Gruenewald,Anthony Jantzi,Omar Chavez*

Main category: cs.AI

TL;DR: 本文介绍了FAI基准，它从七个维度评估AI与人类繁荣的一致性，对28个语言模型测试发现无模型能在各维度达标，为AI发展等提供框架。


<details>
  <summary>Details</summary>
Motivation: 现有的传统基准多关注技术能力或防止危害，缺乏对AI促进人类全面繁荣的评估，因此需要新的评估框架。

Method: 采用包含1229个主客观问题的综合方法，利用专业判断大语言模型和跨维度评估，通过几何平均评分确保各维度平衡。

Result: 对28个领先语言模型的初步测试显示，虽有模型接近整体对齐，但没有一个模型在所有维度上达到可接受的对齐程度，特别是在信仰与灵性、品格与美德、意义与目的方面。

Conclusion: 建立了一个开发积极支持人类繁荣的AI系统的框架，对AI发展、伦理和评估具有重要意义。

Abstract: This paper introduces the Flourishing AI Benchmark (FAI Benchmark), a novel
evaluation framework that assesses AI alignment with human flourishing across
seven dimensions: Character and Virtue, Close Social Relationships, Happiness
and Life Satisfaction, Meaning and Purpose, Mental and Physical Health,
Financial and Material Stability, and Faith and Spirituality. Unlike
traditional benchmarks that focus on technical capabilities or harm prevention,
the FAI Benchmark measures AI performance on how effectively models contribute
to the flourishing of a person across these dimensions. The benchmark evaluates
how effectively LLM AI systems align with current research models of holistic
human well-being through a comprehensive methodology that incorporates 1,229
objective and subjective questions. Using specialized judge Large Language
Models (LLMs) and cross-dimensional evaluation, the FAI Benchmark employs
geometric mean scoring to ensure balanced performance across all flourishing
dimensions. Initial testing of 28 leading language models reveals that while
some models approach holistic alignment (with the highest-scoring models
achieving 72/100), none are acceptably aligned across all dimensions,
particularly in Faith and Spirituality, Character and Virtue, and Meaning and
Purpose. This research establishes a framework for developing AI systems that
actively support human flourishing rather than merely avoiding harm, offering
significant implications for AI development, ethics, and evaluation.

</details>


### [22] [MoSE: Skill-by-Skill Mixture-of-Expert Learning for Autonomous Driving](https://arxiv.org/abs/2507.07818)
*Lu Xu,Jiaqian Yu,Xiongfeng Peng,Yiwei Chen,Weiming Li,Jaewook Yoo,Sunghyun Chunag,Dongwook Lee,Daehyun Ji,Chao Zhang*

Main category: cs.AI

TL;DR: 提出技能导向的MoE（MoSE）模型用于自动驾驶，在推理任务上性能优且激活模型尺寸小。


<details>
  <summary>Details</summary>
Motivation: 通用MoE模型训练需大量数据和复杂优化，受人类驾驶员学习过程启发。

Method: 提出技能导向路由机制，构建分层技能数据集，预训练路由器，将辅助任务集成到单次前向过程。

Result: 不到3B稀疏激活参数的模型在CODA AD边缘案例推理任务上优于多个8B+参数模型，与现有方法比，单轮对话时激活模型尺寸至少减小62.5%。

Conclusion: 所提MoSE方法在自动驾驶中以更小激活模型尺寸达到了先进性能。

Abstract: Recent studies show large language models (LLMs) and vision language models
(VLMs) trained using web-scale data can empower end-to-end autonomous driving
systems for a better generalization and interpretation. Specifically, by
dynamically routing inputs to specialized subsets of parameters, the
Mixture-of-Experts (MoE) technique enables general LLMs or VLMs to achieve
substantial performance improvements while maintaining computational
efficiency. However, general MoE models usually demands extensive training data
and complex optimization. In this work, inspired by the learning process of
human drivers, we propose a skill-oriented MoE, called MoSE, which mimics human
drivers' learning process and reasoning process, skill-by-skill and
step-by-step. We propose a skill-oriented routing mechanism that begins with
defining and annotating specific skills, enabling experts to identify the
necessary driving competencies for various scenarios and reasoning tasks,
thereby facilitating skill-by-skill learning. Further align the driving process
to multi-step planning in human reasoning and end-to-end driving models, we
build a hierarchical skill dataset and pretrain the router to encourage the
model to think step-by-step. Unlike multi-round dialogs, MoSE integrates
valuable auxiliary tasks (e.g.\ description, reasoning, planning) in one single
forward process without introducing any extra computational cost. With less
than 3B sparsely activated parameters, our model outperforms several 8B+
parameters on CODA AD corner case reasoning task. Compared to existing methods
based on open-source models and data, our approach achieves state-of-the-art
performance with significantly reduced activated model size (at least by
$62.5\%$) with a single-turn conversation.

</details>


### [23] [AI Should Sense Better, Not Just Scale Bigger: Adaptive Sensing as a Paradigm Shift](https://arxiv.org/abs/2507.07820)
*Eunsu Baek,Keondo Park,Jeonggil Ko,Min-hwan Oh,Taesik Gong,Hyung-Sin Kim*

Main category: cs.AI

TL;DR: 当前AI发展依赖扩大模型和数据集，但有诸多成本，提出自适应感知可缓解协变量偏移、提高效率，还给出应用路线图、评估挑战并提出研究方向，推动AI可持续发展。


<details>
  <summary>Details</summary>
Motivation: 当前AI发展范式带来显著环境、经济和伦理成本，限制可持续性和公平获取，需新方法。

Method: 倡导自适应感知，在输入层主动调节传感器参数，还给出应用路线图、评估挑战和提出研究方向。

Result: 自适应感知能让小模型超越大模型，后者使用更多数据和计算资源。

Conclusion: 这些努力旨在推动AI社区向可持续、稳健和公平的人工智能系统转变。

Abstract: Current AI advances largely rely on scaling neural models and expanding
training datasets to achieve generalization and robustness. Despite notable
successes, this paradigm incurs significant environmental, economic, and
ethical costs, limiting sustainability and equitable access. Inspired by
biological sensory systems, where adaptation occurs dynamically at the input
(e.g., adjusting pupil size, refocusing vision)--we advocate for adaptive
sensing as a necessary and foundational shift. Adaptive sensing proactively
modulates sensor parameters (e.g., exposure, sensitivity, multimodal
configurations) at the input level, significantly mitigating covariate shifts
and improving efficiency. Empirical evidence from recent studies demonstrates
that adaptive sensing enables small models (e.g., EfficientNet-B0) to surpass
substantially larger models (e.g., OpenCLIP-H) trained with significantly more
data and compute. We (i) outline a roadmap for broadly integrating adaptive
sensing into real-world applications spanning humanoid, healthcare, autonomous
systems, agriculture, and environmental monitoring, (ii) critically assess
technical and ethical integration challenges, and (iii) propose targeted
research directions, such as standardized benchmarks, real-time adaptive
algorithms, multimodal integration, and privacy-preserving methods.
Collectively, these efforts aim to transition the AI community toward
sustainable, robust, and equitable artificial intelligence systems.

</details>


### [24] [Searching for actual causes: Approximate algorithms with adjustable precision](https://arxiv.org/abs/2507.07857)
*Samuel Reyd,Ada Diaconescu,Jean-Louis Dessalles*

Main category: cs.AI

TL;DR: 本文提出一套算法识别实际原因，具有多项式复杂度，可调整精度和详尽度，实验表明算法能处理不同系统且可调整精度。


<details>
  <summary>Details</summary>
Motivation: 现有可解释人工智能文献受批评，经典文献研究不符合非专家用户期望，形式化实际原因概念是开放问题，识别实际原因是NP完全问题且实用解决方案少。

Method: 提出一套具有多项式复杂度且精度和详尽度可调整的算法来识别实际原因。

Result: 算法能识别现有方法无法处理的不同类别系统（非布尔、黑盒和随机系统）的原因，且可通过增加计算时间提高精度和详尽度。

Conclusion: 所提出的算法在识别实际原因方面有良好表现，能处理不同系统并可调整精度和详尽度。

Abstract: Causality has gained popularity in recent years. It has helped improve the
performance, reliability, and interpretability of machine learning models.
However, recent literature on explainable artificial intelligence (XAI) has
faced criticism. The classical XAI and causality literature focuses on
understanding which factors contribute to which consequences. While such
knowledge is valuable for researchers and engineers, it is not what non-expert
users expect as explanations. Instead, these users often await facts that cause
the target consequences, i.e., actual causes. Formalizing this notion is still
an open problem. Additionally, identifying actual causes is reportedly an
NP-complete problem, and there are too few practical solutions to approximate
formal definitions. We propose a set of algorithms to identify actual causes
with a polynomial complexity and an adjustable level of precision and
exhaustiveness. Our experiments indicate that the algorithms (1) identify
causes for different categories of systems that are not handled by existing
approaches (i.e., non-boolean, black-box, and stochastic systems), (2) can be
adjusted to gain more precision and exhaustiveness with more computation time.

</details>


### [25] [An Integrated Framework of Prompt Engineering and Multidimensional Knowledge Graphs for Legal Dispute Analysis](https://arxiv.org/abs/2507.07893)
*Mingda Zhang,Na Zhao,Jianglong Qing,Qing xu,Kaiwen Pan,Ting luo*

Main category: cs.AI

TL;DR: 本文提出集成提示工程与多维知识图谱的增强框架用于法律纠纷分析，实验显示该框架在法律纠纷分析上性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在法律纠纷分析中存在法律知识表示不足、概念理解有限和推理缺陷等问题。

Method: 提出包含任务定义、知识背景和推理指导的三阶段分层提示结构，构建三层知识图谱架构，采用四种互补方法精确检索法律概念，并与网络搜索技术集成。

Result: 在法律纠纷分析中性能显著提升，能对复杂案件进行准确法律适用分析，展现对司法决策逻辑的细致理解。

Conclusion: 为实现智能法律辅助系统提供了新颖的技术途径。

Abstract: The rapid development of artificial intelligence has positioned large
language models as fundamental components of intelligent legal systems.
However, these models face significant limitations in legal dispute analysis,
including insufficient legal knowledge representation, limited concept
understanding, and reasoning deficiencies. This research proposes an enhanced
framework integrating prompt engineering with multidimensional knowledge
graphs. The framework introduces a three-stage hierarchical prompt structure
comprising task definition, knowledge background, and reasoning guidance,
supplemented by legal-specific reasoning templates and dynamic optimization
mechanisms. A three-layer knowledge graph architecture is constructed with
legal classification ontology, representation, and instance layers. Four
complementary methods enable precise legal concept retrieval: direct legal norm
code matching, domain-specific semantic vector similarity, ontology-based path
reasoning, and specialized lexical segmentation. These components integrate
with web search technology to establish a knowledge-enhanced framework for
legal decision-making. Experimental results demonstrate significant performance
improvements in legal dispute analysis, enabling accurate legal application
analysis for complex cases while exhibiting nuanced understanding of judicial
decision-making logic, providing a novel technical approach for implementing
intelligent legal assistance systems.

</details>


### [26] [Meek Models Shall Inherit the Earth](https://arxiv.org/abs/2507.07931)
*Hans Gundlach,Jayson Lynch,Neil Thompson*

Main category: cs.AI

TL;DR: 论文指出计算规模收益递减将使AI模型能力趋同，实力较弱的模型性能将接近最佳模型，还建议重新审视AI策略和政策。


<details>
  <summary>Details</summary>
Motivation: 针对过去十年少数公司扩大AI系统规模导致模型性能不平等的现象，探讨模型能力发展趋势。

Method: 建立模型说明在固定分布的下一个标记目标下，原始计算的边际能力回报大幅缩减；用基准数据和理论性能模型说明代理指标能捕捉重要能力指标；分析AI模型能力差异的实证数据。

Result: 计算规模收益递减显著，即使能快速扩展模型的公司最终在能力上优势也不大。

Conclusion: 鉴于实力较弱模型能力提升，需重新审视AI策略和政策，并指出受影响的领域。

Abstract: The past decade has seen incredible scaling of AI systems by a few companies,
leading to inequality in AI model performance. This paper argues that, contrary
to prevailing intuition, the diminishing returns to compute scaling will lead
to a convergence of AI model capabilities. In other words, meek models (those
with limited computation budget) shall inherit the earth, approaching the
performance level of the best models overall. We develop a model illustrating
that under a fixed-distribution next-token objective, the marginal capability
returns to raw compute shrink substantially. Given current scaling practices,
we argue that these diminishing returns are strong enough that even companies
that can scale their models exponentially faster than other organizations will
eventually have little advantage in capabilities. As part of our argument, we
give several reasons that proxies like training loss differences capture
important capability measures using evidence from benchmark data and
theoretical performance models. In addition, we analyze empirical data on the
capability difference of AI models over time. Finally, in light of the
increasing ability of meek models, we argue that AI strategy and policy require
reexamination, and we outline the areas this shift will affect.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [27] [The Pandora's Box Problem with Sequential Inspections](https://arxiv.org/abs/2507.07508)
*Ali Aouad,Jingwei Ji,Yaron Shaposhnik*

Main category: cs.CE

TL;DR: 研究潘多拉盒子问题的重要推广，分析新权衡，用多种方法研究模型并给出相关结果，表明基于阈值的策略可指导搜索决策。


<details>
  <summary>Details</summary>
Motivation: 研究潘多拉盒子问题的推广，其中代理人可选择全开或部分打开盒子，引入信息获取与成本效率的新权衡。

Method: 建立难度结果，运用随机优化技术，包括识别最优策略结构特性、推导问题松弛和近似最优解、刻画特殊情况的最优策略、进行数值研究。

Result: 对模型进行了全面分析，识别了最优策略结构，得到近似最优解，刻画特殊情况最优策略，通过数值研究比较策略性能。

Conclusion: 直观的基于阈值的策略能有效指导搜索决策。

Abstract: The Pandora's box problem (Weitzman 1979) is a core model in economic theory
that captures an agent's (Pandora's) search for the best alternative (box). We
study an important generalization of the problem where the agent can either
fully open boxes for a certain fee to reveal their exact values or partially
open them at a reduced cost. This introduces a new tradeoff between information
acquisition and cost efficiency. We establish a hardness result and employ an
array of techniques in stochastic optimization to provide a comprehensive
analysis of this model. This includes (1) the identification of structural
properties of the optimal policy that provide insights about optimal decisions;
(2) the derivation of problem relaxations and provably near-optimal solutions;
(3) the characterization of the optimal policy in special yet non-trivial
cases; and (4) an extensive numerical study that compares the performance of
various policies, and which provides additional insights about the optimal
policy. Throughout, we show that intuitive threshold-based policies that extend
the Pandora's box optimal solution can effectively guide search decisions.

</details>


### [28] [Meshless projection model-order reduction via reference spaces for smoothed-particle hydrodynamics](https://arxiv.org/abs/2507.07830)
*Steven N. Rodriguez,Steven L. Brunton,Liam K. Magargal,Parisa Khodabakshi,Justin W. Jaworski,Nicoleta A. Apetre,John C. Steuben,John G. Michopoulos,Athanasios Iliopoulos*

Main category: cs.CE

TL;DR: 本文提出无网格弱可压缩光滑粒子流体动力学（SPH）方法的降阶模型框架，经实验验证有效，可降低SPH模拟成本。


<details>
  <summary>Details</summary>
Motivation: 克服SPH模拟中从非结构化、动态和混合数值拓扑中发现低维子空间的挑战。

Method: 引入模态参考空间概念，将SPH快照数据投影到参考空间构建模态参考空间，使用传统模态分解技术发现场量低维性，在线预测阶段通过散点数据插值将模态量映射回无网格SPH空间，将框架应用于无网格Galerkin POD和Adjoint Petrov - Galerkin投影降阶模型。

Result: 在三个数值实验中，重建和预测的速度场吻合良好；压力场对投影误差敏感，可通过非线性近似缓解。

Conclusion: 所提出的无网格降阶模型框架有助于大幅节省SPH模拟成本。

Abstract: This work proposes a model-order reduction framework for the meshless weakly
compressible smoothed particle hydrodynamics (SPH) method. The proposed
framework introduces the concept of modal reference spaces to overcome the
challenges of discovering low-dimensional subspaces from unstructured, dynamic,
and mixing numerical topology that is often seen in SPH simulations. The
proposed modal reference spaces enable a low-dimensional representation of the
SPH field equations while maintaining their inherent meshless qualities. Modal
reference spaces are constructed by projecting SPH snapshot data onto a
reference space where low-dimensionality of field quantities can be discovered
via traditional modal decomposition techniques (e.g., the proper orthogonal
decomposition (POD)). Modal quantities are mapped back to the meshless SPH
space via scattered data interpolation during the online predictive stage. The
proposed model-order reduction framework is cast into the \emph{meshless}
Galerkin POD (GPOD) and the Adjoint Petrov--Galerkin (APG) projection
model-order reduction (PMOR) formulation. The PMORs are tested on three
numerical experiments: 1) the Taylor--Green vortex; 2) lid-driven cavity; and
3) flow past an open cavity. Results show good agreement in reconstructed and
predictive velocity fields, which showcase the ability of the proposed
framework to evolve the unstructured, dynamic, and mixing SPH field equations
in a low-dimensional subspace. Results also show that the pressure field is
sensitive to the projection error due to the stiff weakly-compressible
assumption made in the current SPH framework, but can be alleviated through
nonlinear approximations, such as the APG approach. Ultimately, the presented
meshless model-order reduction framework marks a step toward enabling drastic
cost savings of SPH simulations.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [29] [Algorithmic Complexity Attacks on All Learned Cardinality Estimators: A Data-centric Approach](https://arxiv.org/abs/2507.07438)
*Yingze Li,Xianglong Liu,Dong Wang,Zixuan Wang,Hongzhi Wang,Kaixing Zhang,Yiming Guan*

Main category: cs.DB

TL;DR: 本文研究学习型基数估计器在数据漂移下的脆弱性，提出黑盒攻击算法并设计近似算法，实验证明攻击有效性，还给出应对措施。


<details>
  <summary>Details</summary>
Motivation: 学习型基数估计器对训练数据漂移脆弱，有现实部署风险，需研究最小数据漂移对其精度的最大影响。

Method: 提出数据中心算法复杂度攻击，证明找最优攻击策略是NP难问题，设计多项式时间近似算法。

Result: 在STATS - CEB和IMDB - JOB基准上，修改0.8%训练元组使90%分位数Qerror提高三个数量级，端到端处理时间最多提高20倍。

Conclusion: 揭示已部署学习型估计器的关键漏洞，提供数据更新下脆弱性的统一最坏情况理论分析，给出两种缓解黑盒攻击的对策。

Abstract: Learned cardinality estimators show promise in query cardinality prediction,
yet they universally exhibit fragility to training data drifts, posing risks
for real-world deployment. This work is the first to theoretical investigate
how minimal data-level drifts can maximally degrade the accuracy of learned
estimators. We propose data-centric algorithmic complexity attacks against
learned estimators in a black-box setting, proving that finding the optimal
attack strategy is NP-Hard. To address this, we design a polynomial-time
approximation algorithm with a $(1-\kappa)$ approximation ratio. Extensive
experiments demonstrate our attack's effectiveness: on STATS-CEB and IMDB-JOB
benchmarks, modifying just 0.8\% of training tuples increases the 90th
percentile Qerror by three orders of magnitude and raises end-to-end processing
time by up to 20$\times$. Our work not only reveals critical vulnerabilities in
deployed learned estimators but also provides the first unified worst-case
theoretical analysis of their fragility under data updates. Additionally, we
identify two countermeasures to mitigate such black-box attacks, offering
insights for developing robust learned database optimizers.

</details>


### [30] [JOB-Complex: A Challenging Benchmark for Traditional & Learned Query Optimization](https://arxiv.org/abs/2507.07471)
*Johannes Wehrstein,Timo Eckmann,Roman Heinrich,Carsten Binnig*

Main category: cs.DB

TL;DR: 现有查询优化基准有局限，本文提出新基准JOB - Complex，评估显示传统和学习型成本模型在其上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有基准不能反映查询优化的很多现实属性，高估了传统和学习型优化器性能，需新基准评估。

Method: 引入新基准JOB - Complex，包含30个SQL查询和近6000个执行计划的计划选择基准。

Result: 传统和学习型成本模型在JOB - Complex上难以实现高性能，运行时间比最优计划慢达11倍。

Conclusion: JOB - Complex是评估查询优化器和成本模型在现实场景中性能的有价值资源。

Abstract: Query optimization is a fundamental task in database systems that is crucial
to providing high performance. To evaluate learned and traditional optimizer's
performance, several benchmarks, such as the widely used JOB benchmark, are
used. However, in this paper, we argue that existing benchmarks are inherently
limited, as they do not reflect many real-world properties of query
optimization, thus overstating the performance of both traditional and learned
optimizers. In fact, simple but realistic properties, such as joins over string
columns or complex filter predicates, can drastically reduce the performance of
existing query optimizers. Thus, we introduce JOB-Complex, a new benchmark
designed to challenge traditional and learned query optimizers by reflecting
real-world complexity. Overall, JOB-Complex contains 30 SQL queries and comes
together with a plan-selection benchmark containing nearly 6000 execution
plans, making it a valuable resource to evaluate the performance of query
optimizers and cost models in real-world scenarios. In our evaluation, we show
that traditional and learned cost models struggle to achieve high performance
on JOB-Complex, providing a runtime of up to 11x slower compared to the optimal
plans.

</details>


### [31] [A Service Architecture for Dataspaces](https://arxiv.org/abs/2507.07979)
*Benedikt T. Arnold,Christoph Lange,Christina Gillmann,Stefan Decker*

Main category: cs.DB

TL;DR: 本文提出在数据空间中提供通用服务的抽象层，并给出EDC连接器服务架构的初始实现及其实用性演示。


<details>
  <summary>Details</summary>
Motivation: 现有数据空间专注数据资产交换，不支持服务，但实际有对基于数据空间服务的需求，因此需要提出解决方案。

Method: 提出一个用于在数据空间中提供通用服务的抽象层，让采用者能开发自有服务并与现有技术无缝集成。

Result: 给出了EDC连接器服务架构的初始实现。

Conclusion: 该抽象层和服务架构具有实际应用价值。

Abstract: Dataspaces are designed to support sovereign, trusted and decentralized data
exchange between participants forming an ecosystem. They are standardized by
initiatives such as the International Data Spaces Association or Gaia-X and
have gained adoption in several domains such as mobility, manufacturing,
tourism or culture. In dataspaces, participants use connectors to communicate
peer-to-peer. The Eclipse Dataspace Components (EDC) Connector is a broadly
adopted, open-source implementation that adheres to the standards and is
supported by a large community. As dataspaces in general, it focuses on the
exchange of data assets with associated usage policies and does not support
services. In practice, however, there is demand for dataspace-based services
and conceptual arguments support their inclusion in dataspaces. In this paper,
we propose an abstraction layer for providing generic services within
dataspaces. Adopters can use this layer to easily develop own services,
seamlessly integrated with the existing dataspace technology. Besides, we
present an initial implementation of this service architecture for the EDC
Connector and demonstrate its practical applicability.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [32] [Distributed Training under Packet Loss](https://arxiv.org/abs/2507.07114)
*Erez Weintraub,Ron Banner,Ariel Orda*

Main category: cs.DC

TL;DR: 提出新分布式训练框架，可在不可靠连接上运行，有实验验证效果


<details>
  <summary>Details</summary>
Motivation: 现有分布式框架假设可靠连接，利用不可靠连接会牺牲准确性和收敛性，缺乏端到端解决方案

Method: 采用两阶段防御机制：无偏梯度聚合和有界漂移参数广播

Result: 在64个GPU的LLAMA2 7B模型实验中，容忍10%随机丢包时困惑度变化最多0.8%

Conclusion: 该框架弥合通信效率和模型训练要求差距，可在通用或广域网进行鲁棒、高吞吐量学习

Abstract: State-of-the-art language and vision models are routinely trained across
thousands of GPUs, often spanning multiple data-centers, yet today's
distributed frameworks still assume reliable connections (e.g., InfiniBand or
RoCE). The resulting acknowledgment traffic and retransmissions inflate tail
latencies and limit scalability. Leveraging unreliable connections will reduce
latency but may sacrifice model accuracy and convergence once packets are
dropped. A principled, end-to-end solution that preserves accuracy and
convergence guarantees under genuine packet loss has previously been missing.
We address this critical gap by introducing a novel distributed training
framework capable of operating over unreliable connections, offering unbiased
gradient aggregation and bounded parameter drift without modifying model code
or optimizers. The key insight is a two-stage defense against missing messages:
(i) Unbiased gradient aggregation: each worker reconstructs a consistent
gradient estimate from whatever packets arrive, guaranteeing expectation-level
correctness; and (ii) Bounded-drift parameter broadcasts: we prove the
inter-worker model discrepancy remains O(1) even after arbitrarily many
iterations, preventing the unbounded divergence typical of asynchronous setups.
Analytical bounds are matched by experiments on the LLAMA2 7B model with 64
GPUs: tolerating 10% random packet loss yields at most 0.8% perplexity change.
This work bridges the gap between communication-efficient datacenter protocols
and the accuracy and generalization guarantees demanded by modern large-model
training, enabling robust, high-throughput learning on commodity or wide-area
networks.

</details>


### [33] [Analysing semantic data storage in Distributed Ledger Technologies for Data Spaces](https://arxiv.org/abs/2507.07116)
*Juan Cano-Benito,Andrea Cimmino,Sven Hertling,Heiko Paulheim,Raúl García-Castro*

Main category: cs.DC

TL;DR: 本文对不同类型分布式账本技术（DLT）存储语义数据进行系统评估，得出私有 DLT 最有效，混合 DLT 有平衡优势的结论。


<details>
  <summary>Details</summary>
Motivation: 数据空间虽需 DLT 作为底层基础设施，但在语义数据高效存储上存在差距，需研究不同 DLT 存储语义数据的情况。

Method: 以真实世界知识图谱为实验基础，对不同类型（公共、私有、混合）DLT 存储语义数据进行系统评估，比较性能、存储效率等指标。

Result: 私有 DLT 在存储和管理语义内容上最有效，混合 DLT 在公共可审计性和运营效率间有平衡优势。

Conclusion: 可根据去中心化数据生态系统的数据主权要求选择最合适的 DLT 基础设施。

Abstract: Data spaces are emerging as decentralised infrastructures that enable
sovereign, secure, and trustworthy data exchange among multiple participants.
To achieve semantic interoperability within these environments, the use of
semantic web technologies and knowledge graphs has been proposed. Although
distributed ledger technologies (DLT) fit as the underlying infrastructure for
data spaces, there remains a significant gap in terms of the efficient storage
of semantic data on these platforms. This paper presents a systematic
evaluation of semantic data storage across different types of DLT (public,
private, and hybrid), using a real-world knowledge graph as an experimental
basis. The study compares performance, storage efficiency, resource
consumption, and the capabilities to update and query semantic data. The
results show that private DLTs are the most efficient for storing and managing
semantic content, while hybrid DLTs offer a balanced trade-off between public
auditability and operational efficiency. This research leads to a discussion on
the selection of the most appropriate DLT infrastructure based on the data
sovereignty requirements of decentralised data ecosystems.

</details>


### [34] [Collective Communication Profiling of Modern-day Machine Learning Workloads](https://arxiv.org/abs/2507.07117)
*Jit Gupta,Andrew Li,Tarun Banka,Ariel Cohen,T. Sridhar,Raj Yavatkar*

Main category: cs.DC

TL;DR: 文章分析机器学习作业集体通信行为，用NCCL日志功能并调整参数，以DeepSeek V3为例，建议重新思考通信框架和网络拓扑。


<details>
  <summary>Details</summary>
Motivation: 机器学习作业的集体通信操作会导致网络拥塞和丢包，影响作业性能，需分析通信模式以合理分配网络资源。

Method: 利用Nvidia Collective Communication Library日志功能获取信息，调整影响集体通信行为的配置参数。

Result: 给出开源DeepSeek V3推理模型集体通信行为的结果，包括操作类型和数量、每次操作的传输大小和请求大小分布。

Conclusion: 有必要重新思考当前集体通信框架和网络拓扑，以适应网络异常对工作负载的影响。

Abstract: Machine Learning jobs, carried out on large number of distributed high
performance systems, involve periodic communication using operations like
AllReduce, AllGather, and Broadcast. These operations may create high bandwidth
and bursty traffic patterns, leading to network congestion and packet loss,
thus impacting the performance of these jobs. Hence it is imperative to analyze
these patterns, which can be helpful in provisioning network resources
depending on the type of machine learning workloads. In this poster we carry
out extensive analysis of the collective communication behavior seen in a wide
variety of models (ex. DeepSeek, GPT, Llama, etc.) To achieve this we
instrument Nvidia Collective Communication Library logging functionality for
richer context about the collectives and workloads. We adjust configuration
parameters that influence collective communication behavior, such as
parallelism, number of nodes, and model type. This overview presents and
discusses some of the results on the collective communication behavior for the
open source DeepSeek V3 inferencing model, which includes operation type and
count, transfer sizes per operation, and request size distribution. Our
analysis shows that it makes sense to rethink current collective communication
frameworks and network topologies so as to accommodate the effect of network
anomalies on the mentioned workloads.

</details>


### [35] [Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding](https://arxiv.org/abs/2507.07120)
*Nidhi Bhatia,Ankit More,Ritika Borkar,Tiyasa Mitra,Ramon Matas,Ritchie Zhao,Maximilian Golub,Dheevatsa Mudigere,Brian Pharris,Bita Darvish Rouhani*

Main category: cs.DC

TL;DR: 提出Helix Parallelism混合执行策略，减少Token-to-Token延迟，提高GPU效率和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型在长KV历史下实时自回归解码面临压力，传统张量并行在注意力计算中扩展性不佳，DRAM读取效率受限。

Method: 引入Helix Parallelism，在注意力阶段进行KV并行分片，FFN计算时复用GPU进行张量并行或专家并行，包含轻量级通信步骤，使用Helix HOP - B减少通信开销。

Result: 与传统并行方法相比，在固定批量大小下将TTL降低达1.5倍，在相同延迟预算下支持达32倍更大批量。

Conclusion: Helix Parallelism推动了吞吐量 - 延迟的帕累托改进，使超长序列实时推理成为可能。

Abstract: As LLMs scale to multi-million-token KV histories, real-time autoregressive
decoding under tight Token-to-Token Latency (TTL) constraints faces growing
pressure. Two core bottlenecks dominate: accessing Feed-Forward Network (FFN)
weights and reading long KV caches. While Tensor Parallelism (TP) helps
mitigate the cost of FFN weight reads, it does not scale well for attention.
When TP width exceeds the number of KV heads, it leads to inefficient KV
duplication, limits parallelism, and constrains batch size. Simultaneously,
DRAM reads for long KV histories scale linearly with batch size, further
capping efficiency.
  We introduce Helix Parallelism, a hybrid execution strategy that applies KV
parallelism during attention to shard KV caches across GPUs, then reuses the
same GPUs for TP in dense LLMs or TPxExpert Parallel (EP) in MoEs during FFN
computation. To preserve exact attention behavior, Helix includes a lightweight
communication step. To minimize the exposed communication cost, we introduce
Helix HOP-B. Helix HOP-B effectively minimizes communication overhead through
batchwise overlap, preserving low TTL while improving GPU efficiency. Compared
to conventional parallelism approaches, Helix reduces TTL by up to 1.5x at
fixed batch sizes and supports up to 32x larger batches under the same latency
budget for DeepSeek-R1, pushing forward the throughput-latency Pareto on
Blackwell and making real-time inference with ultra-long-sequence practical.

</details>


### [36] [Ampere: Communication-Efficient and High-Accuracy Split Federated Learning](https://arxiv.org/abs/2507.07130)
*Zihan Zhang,Leon Wong,Blesson Varghese*

Main category: cs.DC

TL;DR: 提出Ampere协作训练系统，可减少设备计算和通信开销，提升模型准确率，实验表明比SFL性能更优。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习系统有设备计算成本高问题，拆分联邦学习系统有通信开销大、非IID数据下模型准确率低的问题，需要改进。

Method: 开发单向块间训练以本地损失顺序训练设备和服务器块，消除梯度传输；使用轻量级辅助网络生成方法解耦设备和服务器训练；整合设备块激活来训练服务器块以减轻数据异质性影响。

Result: 相比SFL，Ampere提升模型准确率最高达13.26%，减少训练时间最高达94.6%，减少设备 - 服务器通信开销最高达99.1%，减少设备计算最高达93.13%，减少不同非IID程度下准确率标准差53.39%。

Conclusion: Ampere在减少设备计算和通信开销、提升模型准确率方面表现出色，面对异构数据时性能优越。

Abstract: A Federated Learning (FL) system collaboratively trains neural networks
across devices and a server but is limited by significant on-device computation
costs. Split Federated Learning (SFL) systems mitigate this by offloading a
block of layers of the network from the device to a server. However, in doing
so, it introduces large communication overheads due to frequent exchanges of
intermediate activations and gradients between devices and the server and
reduces model accuracy for non-IID data. We propose Ampere, a novel
collaborative training system that simultaneously minimizes on-device
computation and device-server communication while improving model accuracy.
Unlike SFL, which uses a global loss by iterative end-to-end training, Ampere
develops unidirectional inter-block training to sequentially train the device
and server block with a local loss, eliminating the transfer of gradients. A
lightweight auxiliary network generation method decouples training between the
device and server, reducing frequent intermediate exchanges to a single
transfer, which significantly reduces the communication overhead. Ampere
mitigates the impact of data heterogeneity by consolidating activations
generated by the trained device block to train the server block, in contrast to
SFL, which trains on device-specific, non-IID activations. Extensive
experiments on multiple CNNs and transformers show that, compared to
state-of-the-art SFL baseline systems, Ampere (i) improves model accuracy by up
to 13.26% while reducing training time by up to 94.6%, (ii) reduces
device-server communication overhead by up to 99.1% and on-device computation
by up to 93.13%, and (iii) reduces standard deviation of accuracy by 53.39% for
various non-IID degrees highlighting superior performance when faced with
heterogeneous data.

</details>


### [37] [M$^2$-MFP: A Multi-Scale and Multi-Level Memory Failure Prediction Framework for Reliable Cloud Infrastructure](https://arxiv.org/abs/2507.07144)
*Hongyi Xie,Min Zhou,Qiao Yu,Jialiang Yu,Zhenli Sheng,Hong Xie,Defu Lian*

Main category: cs.DC

TL;DR: 云服务中内存故障威胁系统稳定，现有预测方法有局限，提出M² - MFP框架，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 云服务中硬件可靠性重要，现有内存故障预测方法存在泛化性差、召回率低和性能不佳等问题。

Method: 提出M² - MFP框架，将可纠正错误转换为多级二进制矩阵表示，引入二进制空间特征提取器自动提取特征，构建双路径时间建模架构。

Result: 在基准数据集和实际部署实验中，M² - MFP显著优于现有方法。

Conclusion: M² - MFP框架能有效提升云基础设施的可靠性和可用性。

Abstract: As cloud services become increasingly integral to modern IT infrastructure,
ensuring hardware reliability is essential to sustain high-quality service.
Memory failures pose a significant threat to overall system stability, making
accurate failure prediction through the analysis of memory error logs (i.e.,
Correctable Errors) imperative. Existing memory failure prediction approaches
have notable limitations: rule-based expert models suffer from limited
generalizability and low recall rates, while automated feature extraction
methods exhibit suboptimal performance. To address these limitations, we
propose M$^2$-MFP: a Multi-scale and hierarchical memory failure prediction
framework designed to enhance the reliability and availability of cloud
infrastructure. M$^2$-MFP converts Correctable Errors (CEs) into multi-level
binary matrix representations and introduces a Binary Spatial Feature Extractor
(BSFE) to automatically extract high-order features at both DIMM-level and
bit-level. Building upon the BSFE outputs, we develop a dual-path temporal
modeling architecture: 1) a time-patch module that aggregates multi-level
features within observation windows, and 2) a time-point module that employs
interpretable rule-generation trees trained on bit-level patterns. Experiments
on both benchmark datasets and real-world deployment show the superiority of
M$^2$-MFP as it outperforms existing state-of-the-art methods by significant
margins. Code and data are available at this repository:
https://github.com/hwcloud-RAS/M2-MFP.

</details>


### [38] [Compute Can't Handle the Truth: Why Communication Tax Prioritizes Memory and Interconnects in Modern AI Infrastructure](https://arxiv.org/abs/2507.07223)
*Myoungsoo Jung*

Main category: cs.DC

TL;DR: 现代AI工作负载对资源要求高，传统GPU架构扩展性差，本文提出模块化数据中心架构、混合设计和分层内存模型，评估显示提升了AI基础设施的可扩展性、吞吐量和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现代AI工作负载对内存、通信带宽和资源灵活性要求高，传统GPU架构因通信开销难以扩展。

Method: 提出基于CXL的模块化数据中心架构，探索XLink互连，引入混合CXL - over - XLink设计，提出分层内存模型，评估轻量级CXL实现、HBM和硅光子学。

Result: 评估显示AI基础设施的可扩展性、吞吐量和灵活性得到改善。

Conclusion: 所提出的架构、设计和模型有助于解决AI硬件和数据中心扩展性问题，提升AI基础设施性能。

Abstract: Modern AI workloads such as large language models (LLMs) and
retrieval-augmented generation (RAG) impose severe demands on memory,
communication bandwidth, and resource flexibility. Traditional GPU-centric
architectures struggle to scale due to growing inter-GPU communication
overheads. This report introduces key AI concepts and explains how Transformers
revolutionized data representation in LLMs. We analyze large-scale AI hardware
and data center designs, identifying scalability bottlenecks in hierarchical
systems. To address these, we propose a modular data center architecture based
on Compute Express Link (CXL) that enables disaggregated scaling of memory,
compute, and accelerators. We further explore accelerator-optimized
interconnects-collectively termed XLink (e.g., UALink, NVLink, NVLink
Fusion)-and introduce a hybrid CXL-over-XLink design to reduce long-distance
data transfers while preserving memory coherence. We also propose a
hierarchical memory model that combines local and pooled memory, and evaluate
lightweight CXL implementations, HBM, and silicon photonics for efficient
scaling. Our evaluations demonstrate improved scalability, throughput, and
flexibility in AI infrastructure.

</details>


### [39] [Machine Learning-driven Multiscale MD Workflows: The Mini-MuMMI Experience](https://arxiv.org/abs/2507.07352)
*Loïc Pottier,Konstantia Georgouli,Timothy S. Carpenter,Fikret Aydin,Jeremy O. B. Tempkin,Dwight V. Nissley,Frederick H. Streitz,Thomas R. W. Scogland,Peer-Timo Bremer,Felice C. Lightstone,Helgi I. Ingólfsson*

Main category: cs.DC

TL;DR: 本文介绍多尺度工作流管理基础设施MuMMI及精简版mini - MuMMI，展示mini - MuMMI在研究RAS - RAF膜相互作用中的应用并探讨多尺度工作流挑战。


<details>
  <summary>Details</summary>
Motivation: 多尺度模型建模复杂相互作用有需求，但在并行系统上编排多尺度研究有挑战，需有效工作流管理系统。

Method: 提出并介绍大规模并行的多尺度机器学习建模基础设施MuMMI及其精简版mini - MuMMI。

Result: 展示mini - MuMMI在探索RAS - RAF膜相互作用中的实用性。

Conclusion: 讨论多尺度工作流推广挑战及mini - MuMMI在MD和RAS - RAF相互作用之外更广泛应用的可能性。

Abstract: Computational models have become one of the prevalent methods to model
complex phenomena. To accurately model complex interactions, such as detailed
biomolecular interactions, scientists often rely on multiscale models comprised
of several internal models operating at difference scales, ranging from
microscopic to macroscopic length and time scales. Bridging the gap between
different time and length scales has historically been challenging but the
advent of newer machine learning (ML) approaches has shown promise for tackling
that task. Multiscale models require massive amounts of computational power and
a powerful workflow management system. Orchestrating ML-driven multiscale
studies on parallel systems with thousands of nodes is challenging, the
workflow must schedule, allocate and control thousands of simulations operating
at different scales. Here, we discuss the massively parallel Multiscale
Machine-Learned Modeling Infrastructure (MuMMI), a multiscale workflow
management infrastructure, that can orchestrate thousands of molecular dynamics
(MD) simulations operating at different timescales, spanning from millisecond
to nanosecond. More specifically, we introduce a novel version of MuMMI called
"mini-MuMMI". Mini-MuMMI is a curated version of MuMMI designed to run on
modest HPC systems or even laptops whereas MuMMI requires larger HPC systems.
We demonstrate mini-MuMMI utility by exploring RAS-RAF membrane interactions
and discuss the different challenges behind the generalization of multiscale
workflows and how mini-MuMMI can be leveraged to target a broader range of
applications outside of MD and RAS-RAF interactions.

</details>


### [40] [KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent Workflows](https://arxiv.org/abs/2507.07400)
*Zaifeng Pan,Ajjkumar Patel,Zhengding Hu,Yipeng Shen,Yue Guan,Wan-Lu Li,Lianhui Qin,Yida Wang,Yufei Ding*

Main category: cs.DC

TL;DR: 提出针对代理工作负载的KVFlow框架，通过工作流感知管理和预取机制提升KV缓存管理效率，相比SGLang有显著加速。


<details>
  <summary>Details</summary>
Motivation: 现有LLM系统使用LRU策略管理KV缓存，无法预见未来代理使用情况，导致频繁缓存未命中和大量重计算或交换开销。

Method: 将代理执行计划抽象为代理步骤图，为每个代理分配执行步骤值以指导细粒度驱逐策略，引入完全重叠的KV预取机制。

Result: 相比SGLang，单工作流大提示下加速达1.83倍，多并发工作流场景加速达2.19倍。

Conclusion: KVFlow能有效提升代理工作负载的服务效率。

Abstract: Large language model (LLM) based agentic workflows have become a popular
paradigm for coordinating multiple specialized agents to solve complex tasks.
To improve serving efficiency, existing LLM systems employ prefix caching to
reuse key-value (KV) tensors corresponding to agents' fixed prompts, thereby
avoiding redundant computation across repeated invocations. However, current
systems typically evict KV caches using a Least Recently Used (LRU) policy,
which fails to anticipate future agent usage and often discards KV caches
shortly before their reuse. This leads to frequent cache misses and substantial
recomputation or swapping overhead. We present KVFlow, a workflow-aware KV
cache management framework tailored for agentic workloads. KVFlow abstracts the
agent execution schedule as an Agent Step Graph and assigns each agent a
steps-to-execution value that estimates its temporal proximity to future
activation. These values guide a fine-grained eviction policy at the KV node
level, allowing KVFlow to preserve entries likely to be reused and efficiently
manage shared prefixes in tree-structured caches. Moreover, KVFlow introduces a
fully overlapped KV prefetching mechanism, which proactively loads required
tensors from CPU to GPU in background threads for agents scheduled in the next
step, thereby avoiding cache miss stalls during generation. Compared to SGLang
with hierarchical radix cache, KVFlow achieves up to 1.83$\times$ speedup for
single workflows with large prompts, and up to 2.19$\times$ speedup for
scenarios with many concurrent workflows.

</details>


### [41] [Multi-agent Reinforcement Learning-based In-place Scaling Engine for Edge-cloud Systems](https://arxiv.org/abs/2507.07671)
*Jovan Prodanov,Blaž Bertalanič,Carolina Fortuna,Shih-Kai Chou,Matjaž Branko Jurič,Ramon Sanchez-Iborra,Jernej Hribar*

Main category: cs.DC

TL;DR: 提出基于多智能体强化学习的就地扩展引擎MARLISE解决边缘云系统资源扩展问题，表现优于启发式方法。


<details>
  <summary>Details</summary>
Motivation: 现代边缘云系统在处理动态和不可预测工作负载时，传统扩展方法难以优化资源利用率和维持性能，需要新的就地扩展方法。

Method: 使用深度Q网络（DQN）和近端策略优化（PPO）两种深度强化学习算法开发MARLISE。

Result: MARLISE在管理资源弹性、维持微服务响应时间和提高资源效率方面优于启发式方法。

Conclusion: MARLISE能实现无缝、动态、反应式的就地资源扩展控制，有效解决边缘云系统资源扩展问题。

Abstract: Modern edge-cloud systems face challenges in efficiently scaling resources to
handle dynamic and unpredictable workloads. Traditional scaling approaches
typically rely on static thresholds and predefined rules, which are often
inadequate for optimizing resource utilization and maintaining performance in
distributed and dynamic environments. This inefficiency hinders the
adaptability and performance required in edge-cloud infrastructures, which can
only be achieved through the newly proposed in-place scaling. To address this
problem, we propose the Multi-Agent Reinforcement Learning-based In-place
Scaling Engine (MARLISE) that enables seamless, dynamic, reactive control with
in-place resource scaling. We develop our solution using two Deep Reinforcement
Learning algorithms: Deep Q-Network (DQN), and Proximal Policy Optimization
(PPO). We analyze each version of the proposed MARLISE solution using dynamic
workloads, demonstrating their ability to ensure low response times of
microservices and scalability. Our results show that MARLISE-based approaches
outperform heuristic method in managing resource elasticity while maintaining
microservice response times and achieving higher resource efficiency.

</details>


### [42] [KIS-S: A GPU-Aware Kubernetes Inference Simulator with RL-Based Auto-Scaling](https://arxiv.org/abs/2507.07932)
*Guilin Zhang,Wulan Guo,Ziqi Tan,Qiang Guan,Hailong Jiang*

Main category: cs.DC

TL;DR: 提出KIS - S框架，含KISim模拟器和基于PPO的KIScaler自动伸缩器，实验表明其效果好，能弥合反应式自动伸缩与智能编排差距。


<details>
  <summary>Details</summary>
Motivation: Kubernetes中默认自动伸缩机制在动态和突发流量模式下有挑战，且缺乏与GPU级指标集成。

Method: 构建KIS - S框架，KIScaler在模拟中学习策略后直接部署。

Result: KIScaler使平均奖励提高75.2%，P95延迟最多降低6.7倍，无需重新训练即可泛化。

Conclusion: 工作弥合了可扩展GPU加速环境中反应式自动伸缩与智能编排的差距。

Abstract: Autoscaling GPU inference workloads in Kubernetes remains challenging due to
the reactive and threshold-based nature of default mechanisms such as the
Horizontal Pod Autoscaler (HPA), which struggle under dynamic and bursty
traffic patterns and lack integration with GPU-level metrics. We present KIS-S,
a unified framework that combines KISim, a GPU-aware Kubernetes Inference
Simulator, with KIScaler, a Proximal Policy Optimization (PPO)-based
autoscaler. KIScaler learns latency-aware and resource-efficient scaling
policies entirely in simulation, and is directly deployed without retraining.
Experiments across four traffic patterns show that KIScaler improves average
reward by 75.2%, reduces P95 latency up to 6.7x over CPU baselines, and
generalizes without retraining. Our work bridges the gap between reactive
autoscaling and intelligent orchestration for scalable GPU-accelerated
environments.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [43] [Finding One Local Optimum Is Easy -- But What about Two?](https://arxiv.org/abs/2507.07524)
*Yasuaki Kobayashi,Kazuhiro Kurita,Yutaro Yamaguchi*

Main category: cs.DS

TL;DR: 本文从不同角度研究局部搜索问题复杂度，证明多个自然无权局部搜索问题计算两个局部最优解是NP难的，并讨论了一些可解情况。


<details>
  <summary>Details</summary>
Motivation: 以往研究表明局部搜索问题计算复杂度在有权和无权情况下不同，本文希望从新角度研究局部搜索问题复杂度。

Method: 对多个自然无权局部搜索问题进行分析，证明计算两个局部最优解的复杂度。

Result: 计算多个自然无权局部搜索问题（如最大独立集、最小支配集等）的两个局部最优解是NP难的，并讨论了一些可解情况。

Conclusion: 从新角度揭示了局部搜索问题在计算多个局部最优解时的复杂度情况。

Abstract: The class PLS (Polynomial Local Search) captures the complexity of finding a
solution that is locally optimal and has proven to be an important concept in
the theory of local search. It has been shown that local search versions of
various combinatorial optimization problems, such as Maximum Independent Set
and Max Cut, are complete for this class. Such computational intractability
typically arises in local search problems allowing arbitrary weights; in
contrast, for unweighted problems, locally optimal solutions can be found in
polynomial time under standard settings. In this paper, we pursue the
complexity of local search problems from a different angle: We show that
computing two locally optimal solutions is NP-hard for various natural
unweighted local search problems, including Maximum Independent Set, Minimum
Dominating Set, Max SAT, and Max Cut. We also discuss several tractable cases
for finding two (or more) local optimal solutions.

</details>


### [44] [On the Complexity of Hyperpath and Minimal Separator Enumeration in Directed Hypergraphs](https://arxiv.org/abs/2507.07528)
*Kazuhiro Kurita,Kevin Mann*

Main category: cs.DS

TL;DR: 研究有向超图中(s,t)超路径和最小(s,t)分隔符的枚举问题，指出从图扩展到超图复杂度改变，有负、正两方面结果。


<details>
  <summary>Details</summary>
Motivation: 将经典的图中(s,t)路径和最小(s,t)分隔符枚举问题推广到有向超图。

Method: 分析复杂度，对比经典问题，用回溯法处理B - 超图中(s,t)超路径枚举。

Result: 诱导(s,t)超路径和最小(s,t)分隔符枚举无输出多项式时间算法（除非P = NP）；若(s,t)超路径枚举有输出多项式时间算法，最小横截枚举在BF - 超图可解决；B - 超图中(s,t)超路径枚举可多项式延迟解决。

Conclusion: 有向超图中这些枚举问题复杂度与经典图中不同，BF - 超图中(s,t)超路径枚举不易，B - 超图中该问题可较好解决。

Abstract: In this paper, we address the enumeration of (induced) $s$-$t$ paths and
minimal $s$-$t$ separators. These problems are some of the most famous
classical enumeration problems that can be solved in polynomial delay by simple
backtracking for a (un)directed graph. As a generalization of these problems,
we consider the (induced) $s$-$t$ hyperpath and minimal $s$-$t$ separator
enumeration in a \emph{directed hypergraph}. We show that extending these
classical enumeration problems to directed hypergraphs drastically changes
their complexity. More precisely, there are no output-polynomial time
algorithms for the enumeration of induced $s$-$t$ hyperpaths and minimal
$s$-$t$ separators unless $P = NP$, and if there is an output-polynomial time
algorithm for the $s$-$t$ hyperpath enumeration, then the minimal transversal
enumeration can be solved in output polynomial time even if a directed
hypergraph is $BF$-hypergraph. Since the existence of an output-polynomial time
algorithm for the minimal transversal enumeration has remained an open problem
for over 45 years, it indicates that the $s$-$t$ hyperpath enumeration for a
$BF$-hypergraph is not an easy problem. As a positive result, the $s$-$t$
hyperpath enumeration for a $B$-hypergraph can be solved in polynomial delay by
backtracking.

</details>


### [45] [Efficient and Adaptive Estimation of Local Triadic Coefficients](https://arxiv.org/abs/2507.07536)
*Ilie Sarpe,Aristides Gionis*

Main category: cs.DS

TL;DR: 本文研究高效计算输入图节点分区的局部三元系数平均值问题，提出基于采样的Triad算法，可高效准确估计，还通过案例展示其能捕捉协作网络高阶模式。


<details>
  <summary>Details</summary>
Motivation: 计算输入图节点分区的局部三元系数平均值，能更好洞察图结构与节点属性的相互作用，但精确计算对大型网络不可行，因此需高精度概率估计。

Method: 开发基于采样的自适应算法Triad，使用新的无偏估计器和非平凡的样本复杂度界限。

Result: Triad算法能在大型网络上高效使用，案例显示平均局部三元系数可捕捉协作网络的高阶模式。

Conclusion: Triad算法可高效准确估计平均局部三元系数，在实际大型网络中有应用价值。

Abstract: Characterizing graph properties is fundamental to the analysis and to our
understanding of real-world networked systems. The local clustering
coefficient, and the more recently introduced, local closure coefficient,
capture powerful properties that are essential in a large number of
applications, ranging from graph embeddings to graph partitioning. Such
coefficients capture the local density of the neighborhood of each node,
considering incident triadic structures and paths of length two. For this
reason, we refer to these coefficients collectively as local triadic
coefficients.
  In this work, we consider the novel problem of computing efficiently the
average of local triadic coefficients, over a given partition of the nodes of
the input graph into a set of disjoint buckets. The average local triadic
coefficients of the nodes in each bucket provide a better insight into the
interplay of graph structure and the properties of the nodes associated to each
bucket. Unfortunately, exact computation, which requires listing all triangles
in a graph, is infeasible for large networks. Hence, we focus on obtaining
highly-accurate probabilistic estimates.
  We develop Triad, an adaptive algorithm based on sampling, which can be used
to estimate the average local triadic coefficients for a partition of the nodes
into buckets. Triad is based on a new class of unbiased estimators, and
non-trivial bounds on its sample complexity, enabling the efficient computation
of highly accurate estimates. Finally, we show how Triad can be efficiently
used in practice on large networks, and we present a case study showing that
average local triadic coefficients can capture high-order patterns over
collaboration networks.

</details>


### [46] [A Randomized Rounding Approach for DAG Edge Deletion](https://arxiv.org/abs/2507.07943)
*Sina Kalantarzadeh,Nathan Klein,Victor Reis*

Main category: cs.DS

TL;DR: 本文研究DAG Edge Deletion问题，引入基于顶点标签分布的随机舍入框架，改进了近似比，并给出不同图的近似结果。


<details>
  <summary>Details</summary>
Motivation: 改善DAG Edge Deletion问题的近似比，该问题在调度中有应用。

Method: 引入基于[0,1]上顶点标签分布的随机舍入框架，使用自然均匀分布和修改后的独立标签分布。

Result: 得到(2 - √2)(k + 1)、0.549(k + 1)近似比，证明独立分布不能使分析低于0.542(k + 1)，给出二分图和特定LP解实例的0.5(k + 1)近似比。

Conclusion: 改进了DAG Edge Deletion问题的近似比，但一般情况下能否达到0.5(k + 1)近似比仍未知。

Abstract: In the DAG Edge Deletion problem, we are given an edge-weighted directed
acyclic graph and a parameter $k$, and the goal is to delete the minimum weight
set of edges so that the resulting graph has no paths of length $k$. This
problem, which has applications to scheduling, was introduced in 2015 by
Kenkre, Pandit, Purohit, and Saket. They gave a $k$-approximation and showed
that it is UGC-Hard to approximate better than $\lfloor 0.5k \rfloor$ for any
constant $k \ge 4$ using a work of Svensson from 2012. The approximation ratio
was improved to $\frac{2}{3}(k+1)$ by Klein and Wexler in 2016.
  In this work, we introduce a randomized rounding framework based on
distributions over vertex labels in $[0,1]$. The most natural distribution is
to sample labels independently from the uniform distribution over $[0,1]$. We
show this leads to a $(2-\sqrt{2})(k+1) \approx 0.585(k+1)$-approximation. By
using a modified (but still independent) label distribution, we obtain a
$0.549(k+1)$-approximation for the problem, as well as show that no independent
distribution over labels can improve our analysis to below $0.542(k+1)$.
Finally, we show a $0.5(k+1)$-approximation for bipartite graphs and for
instances with structured LP solutions. Whether this ratio can be obtained in
general is open.

</details>


### [47] [Finding sparse induced subgraphs on graphs of bounded induced matching treewidth](https://arxiv.org/abs/2507.07975)
*Hans L. Bodlaender,Fedor V. Fomin,Tuukka Korhonen*

Main category: cs.DS

TL;DR: 本文证明了Lima等人关于最大权重有界树宽导出子图问题的猜想，表明在tree - μ(G)、w和|Φ|有界时该问题可多项式时间求解。


<details>
  <summary>Details</summary>
Motivation: Lima等人猜想最大权重独立集的多项式时间算法可推广到最大权重有界树宽导出子图问题，作者旨在证明该猜想的一般情况。

Method: 未提及具体方法。

Result: 证明了猜想的一般情况，给出算法在n顶点图上运行时间为f(k, w, |Φ|) · n^{O(k w^2)} ，其中tree - μ(G) ≤ k，f为可计算函数。

Conclusion: 最大权重有界树宽导出子图问题在tree - μ(G)、w和|Φ|有界时可多项式时间求解。

Abstract: The induced matching width of a tree decomposition of a graph $G$ is the
cardinality of a largest induced matching $M$ of $G$, such that there exists a
bag that intersects every edge in $M$. The induced matching treewidth of a
graph $G$, denoted by $\mathsf{tree-}\mu(G)$, is the minimum induced matching
width of a tree decomposition of $G$. The parameter $\mathsf{tree-}\mu$ was
introduced by Yolov [SODA '18], who showed that, for example, Maximum-Weight
Independent Set can be solved in polynomial-time on graphs of bounded
$\mathsf{tree-}\mu$. Lima, Milani\v{c}, Mur\v{s}i\v{c}, Okrasa,
Rz\k{a}\.zewski, and \v{S}torgel [ESA '24] conjectured that this algorithm can
be generalized to a meta-problem called Maximum-Weight Induced Subgraph of
Bounded Treewidth, where we are given a vertex-weighted graph $G$, an integer
$w$, and a $\mathsf{CMSO}_2$-sentence $\Phi$, and are asked to find a
maximum-weight set $X \subseteq V(G)$ so that $G[X]$ has treewidth at most $w$
and satisfies $\Phi$. They proved the conjecture for some special cases, such
as for the problem Maximum-Weight Induced Forest.
  In this paper, we prove the general case of the conjecture. In particular, we
show that Maximum-Weight Induced Subgraph of Bounded Treewidth is
polynomial-time solvable when $\mathsf{tree-}\mu(G)$, $w$, and $|\Phi|$ are
bounded. The running time of our algorithm for $n$-vertex graphs $G$ with
$\mathsf{tree} - \mu(G) \le k$ is $f(k, w, |\Phi|) \cdot n^{O(k w^2)}$ for a
computable function $f$.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [48] [Optimal Auction Design in the Joint Advertising](https://arxiv.org/abs/2507.07418)
*Yang Li,Yuchao Ma,Qi Qi*

Main category: cs.GT

TL;DR: 本文针对联合广告，找出单广告位最优机制，提出多广告位的BundleNet方法，实验表明其表现优异能增加平台收入。


<details>
  <summary>Details</summary>
Motivation: 现有联合广告机制未实现最优，倾向关注单个广告商而忽略捆绑结构。

Method: 单广告位找出最优机制，多广告位提出BundleNet这种基于捆绑的神经网络方法。

Result: BundleNet生成的机制在单广告位近似理论分析结果，多广告位达到最优表现。

Conclusion: BundleNet显著增加平台收入，确保近似占优策略激励相容性和个体理性。

Abstract: Online advertising is a vital revenue source for major internet platforms.
Recently, joint advertising, which assigns a bundle of two advertisers in an ad
slot instead of allocating a single advertiser, has emerged as an effective
method for enhancing allocation efficiency and revenue. However, existing
mechanisms for joint advertising fail to realize the optimality, as they tend
to focus on individual advertisers and overlook bundle structures. This paper
identifies an optimal mechanism for joint advertising in a single-slot setting.
For multi-slot joint advertising, we propose \textbf{BundleNet}, a novel
bundle-based neural network approach specifically designed for joint
advertising. Our extensive experiments demonstrate that the mechanisms
generated by \textbf{BundleNet} approximate the theoretical analysis results in
the single-slot setting and achieve state-of-the-art performance in the
multi-slot setting. This significantly increases platform revenue while
ensuring approximate dominant strategy incentive compatibility and individual
rationality.

</details>


### [49] [Incentive Mechanism for Mobile Crowd Sensing with Assumed Bid Cost Reverse Auction](https://arxiv.org/abs/2507.07688)
*Jowa Yangchin,Ningrinla Marchang*

Main category: cs.GT

TL;DR: 本文提出RA - ABC和RA - ABCDR方法优化MCS系统，模拟显示其性能优于Tullock函数，动态招募提升多方面表现。


<details>
  <summary>Details</summary>
Motivation: 优化MCS系统，提高用户、任务提供者和平台效用，减少资源消耗。

Method: 提出RA - ABC方法让用户先投标后采集数据，计算用户ROI决定是否继续参与；提出RA - ABCDR扩展方法允许新用户在投标轮次随时加入。

Result: RA - ABC和RA - ABCDR优于Tullock函数，RA - ABCDR用户保留率提高54.6%，拍卖成本降低22.2%。

Conclusion: 动态用户招募能显著提升系统稳定性、公平性和成本效率等性能。

Abstract: Mobile Crowd Sensing (MCS) is the mechanism wherein people can contribute in
data collection process using their own mobile devices which have sensing
capabilities. Incentives are rewards that individuals get in exchange for data
they submit. Reverse Auction Bidding (RAB) is a framework that allows users to
place bids for selling the data they collected. Task providers can select users
to buy data from by looking at bids. Using the RAB framework, MCS system can be
optimized for better user utility, task provider utility and platform utility.
In this paper, we propose a novel approach called Reverse Auction with Assumed
Bid Cost (RA-ABC) which allows users to place a bid in the system before
collecting data. We opine that performing the tasks only after winning helps in
reducing resource consumption instead of performing the tasks before bidding.
User Return on Investment (ROI) is calculated with which they decide to further
participate or not by either increasing or decreasing their bids. We also
propose an extension of RA-ABC with dynamic recruitment (RA-ABCDR) in which we
allow new users to join the system at any time during bidding rounds.
Simulation results demonstrate that RA-ABC and RA-ABCDR outperform the widely
used Tullock Optimal Prize Function, with RA-ABCDR achieving up to 54.6\%
higher user retention and reducing auction cost by 22.2\%, thereby ensuring
more efficient and sustainable system performance. Extensive simulations
confirm that dynamic user recruitment significantly enhances performance across
stability, fairness, and cost-efficiency metrics.

</details>


### [50] [Hybrid Advertising in the Sponsored Search](https://arxiv.org/abs/2507.07711)
*Zhen Zhang,Weian Li,Yuhan Wang,Qi Qi,Kun Huang*

Main category: cs.GT

TL;DR: 本文提出混合广告模型及HRegNet架构，实验表明该机制可显著提升平台收入。


<details>
  <summary>Details</summary>
Motivation: 传统广告和联合广告吸引不同用户群体，导致点击率低，为解决此局限并增强通用性。

Method: 提出混合广告模型，每个广告位可分配给独立店铺或捆绑组合；引入用于该模型的神经网络架构HRegNet以找到最优拍卖机制。

Result: 在合成和真实数据上的大量实验表明，HRegNet生成的机制相比现有基线方法显著提高了平台收入。

Conclusion: 所提出的混合广告模型及HRegNet架构能有效提升电商平台广告收入。

Abstract: Online advertisements are a primary revenue source for e-commerce platforms.
Traditional advertising models are store-centric, selecting winning stores
through auction mechanisms. Recently, a new approach known as joint advertising
has emerged, which presents sponsored bundles combining one store and one brand
in ad slots. Unlike traditional models, joint advertising allows platforms to
collect payments from both brands and stores. However, each of these two
advertising models appeals to distinct user groups, leading to low
click-through rates when users encounter an undesirable advertising model. To
address this limitation and enhance generality, we propose a novel advertising
model called ''Hybrid Advertising''. In this model, each ad slot can be
allocated to either an independent store or a bundle. To find the optimal
auction mechanisms in hybrid advertising, while ensuring nearly dominant
strategy incentive compatibility and individual rationality, we introduce the
Hybrid Regret Network (HRegNet), a neural network architecture designed for
this purpose. Extensive experiments on both synthetic and real-world data
demonstrate that the mechanisms generated by HRegNet significantly improve
platform revenue compared to established baseline methods.

</details>


### [51] [Improving the Price of Anarchy via Predictions in Parallel-Link Networks](https://arxiv.org/abs/2507.07915)
*George Christodoulou,Vasilis Christoforidis,Alkmini Sgouritsa,Ioannis Vlachos*

Main category: cs.GT

TL;DR: 研究并行链路网络上带仿射成本函数的非原子拥塞博弈，探讨机器学习预测在协调机制设计中的作用，证明输入速率建议可优化社会成本，有一致性和有界鲁棒性，给出一致机制刻画，证明机制最优，还探索平滑性并扩展机制实现容错。


<details>
  <summary>Details</summary>
Motivation: 研究机器学习预测在设计协调机制中的作用，以最小化自私行为的影响。

Method: 在并行链路网络的非原子拥塞博弈中，为协调机制提供输入速率建议，并进行相关理论分析。

Result: 准确建议能优化社会成本，不准确时损失最小；给出所有单调成本函数的一致机制刻画；机制在鲁棒性方面最优；扩展机制实现容错。

Conclusion: 基于输入速率建议的协调机制在一致性、鲁棒性和平滑性方面表现良好，可有效应对预测误差。

Abstract: We study non-atomic congestion games on parallel-link networks with affine
cost functions. We investigate the power of machine-learned predictions in the
design of coordination mechanisms aimed at minimizing the impact of
selfishness. Our main results demonstrate that enhancing coordination
mechanisms with a simple advice on the input rate can optimize the social cost
whenever the advice is accurate (consistency), while only incurring minimal
losses even when the predictions are arbitrarily inaccurate (bounded
robustness). Moreover, we provide a full characterization of the consistent
mechanisms that holds for all monotone cost functions, and show that our
suggested mechanism is optimal with respect to the robustness. We further
explore the notion of smoothness within this context: we extend our mechanism
to achieve error-tolerance, i.e. we provide an approximation guarantee that
degrades smoothly as a function of the prediction error, up to a predetermined
threshold, while achieving a bounded robustness.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [52] [A Language-Driven Framework for Improving Personalized Recommendations: Merging LLMs with Traditional Algorithms](https://arxiv.org/abs/2507.07251)
*Aaron Goldstein,Ayan Dutta*

Main category: cs.IR

TL;DR: 本文提出利用大语言模型（LLMs）改进传统电影推荐算法，能自动生成偏好配置文件，在多个评估指标上远超传统算法。


<details>
  <summary>Details</summary>
Motivation: 传统推荐算法无法基于文本形式的用户偏好提供个性化推荐，而LLMs在自然语言处理方面表现出色。

Method: 提出模仿好友推荐的框架，用SVD或SVD++算法生成初始推荐，结合LLMs优化结果，使用留一法验证命中率等指标评估性能。

Result: 该框架在各项评估指标上大幅超越SVD和SVD++算法，但计算开销略有增加。

Conclusion: 利用LLMs可有效提升电影推荐系统的性能，实现更个性化的推荐。

Abstract: Traditional recommendation algorithms are not designed to provide
personalized recommendations based on user preferences provided through text,
e.g., "I enjoy light-hearted comedies with a lot of humor". Large Language
Models (LLMs) have emerged as one of the most promising tools for natural
language processing in recent years. This research proposes a novel framework
that mimics how a close friend would recommend items based on their knowledge
of an individual's tastes. We leverage LLMs to enhance movie recommendation
systems by refining traditional algorithm outputs and integrating them with
language-based user preference inputs. We employ Singular Value Decomposition
(SVD) or SVD++ algorithms to generate initial movie recommendations,
implemented using the Surprise Python library and trained on the
MovieLens-Latest-Small dataset. We compare the performance of the base
algorithms with our LLM-enhanced versions using leave-one-out validation hit
rates and cumulative hit rates. Additionally, to compare the performance of our
framework against the current state-of-the-art recommendation systems, we use
rating and ranking metrics with an item-based stratified 0.75 train, 0.25 test
split. Our framework can generate preference profiles automatically based on
users' favorite movies or allow manual preference specification for more
personalized results. Using an automated approach, our framework overwhelmingly
surpassed SVD and SVD++ on every evaluation metric used (e.g., improvements of
up to ~6x in cumulative hit rate, ~3.7x in NDCG, etc.), albeit at the cost of a
slight increase in computational overhead.

</details>


### [53] [When Graph Contrastive Learning Backfires: Spectral Vulnerability and Defense in Recommendation](https://arxiv.org/abs/2507.07436)
*Zongwei Wang,Min Gao,Junliang Yu,Shazia Sadiq,Hongzhi Yin,Ling Liu*

Main category: cs.IR

TL;DR: 研究揭示图对比学习使推荐系统易受攻击，提出攻击方法CLeaR和缓解框架SIM。


<details>
  <summary>Details</summary>
Motivation: 发现图对比学习在增强推荐系统时带来易受针对性推广攻击的问题。

Method: 理论研究与实证验证确定问题根源，提出CLeaR攻击方法和SIM缓解框架。

Result: 实验表明GCL推荐模型用CLeaR评估更易受攻击，SIM能有效缓解。

Conclusion: 急需鲁棒对策，SIM可在不影响性能下检测和抑制目标物品。

Abstract: Graph Contrastive Learning (GCL) has demonstrated substantial promise in
enhancing the robustness and generalization of recommender systems,
particularly by enabling models to leverage large-scale unlabeled data for
improved representation learning. However, in this paper, we reveal an
unexpected vulnerability: the integration of GCL inadvertently increases the
susceptibility of a recommender to targeted promotion attacks. Through both
theoretical investigation and empirical validation, we identify the root cause
as the spectral smoothing effect induced by contrastive optimization, which
disperses item embeddings across the representation space and unintentionally
enhances the exposure of target items. Building on this insight, we introduce
CLeaR, a bi-level optimization attack method that deliberately amplifies
spectral smoothness, enabling a systematic investigation of the susceptibility
of GCL-based recommendation models to targeted promotion attacks. Our findings
highlight the urgent need for robust countermeasures; in response, we further
propose SIM, a spectral irregularity mitigation framework designed to
accurately detect and suppress targeted items without compromising model
performance. Extensive experiments on multiple benchmark datasets demonstrate
that, compared to existing targeted promotion attacks, GCL-based recommendation
models exhibit greater susceptibility when evaluated with CLeaR, while SIM
effectively mitigates these vulnerabilities.

</details>


### [54] [NLGCL: Naturally Existing Neighbor Layers Graph Contrastive Learning for Recommendation](https://arxiv.org/abs/2507.07522)
*Jinfeng Xu,Zheyu Chen,Shuo Yang,Jinze Li,Hewei Wang,Wei Wang,Xiping Hu,Edith Ngai*

Main category: cs.IR

TL;DR: 本文提出NLGCL框架解决现有GCL方法问题，实验证明其在有效性和效率上优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有GCL方法依赖增强技术，引入语义无关噪声，有高计算和存储成本，影响有效性和效率。

Method: 提出NLGCL框架，利用GNN中相邻层间自然对比视图，将节点及其下一层邻居作为正样本对，其他节点作为负样本对。

Result: 在四个公开数据集上的实验表明，NLGCL在有效性和效率上优于现有基线。

Conclusion: NLGCL能避免基于增强的噪声，保留语义相关性，消除昂贵的视图构建和存储，适用于现实场景。

Abstract: Graph Neural Networks (GNNs) are widely used in collaborative filtering to
capture high-order user-item relationships. To address the data sparsity
problem in recommendation systems, Graph Contrastive Learning (GCL) has emerged
as a promising paradigm that maximizes mutual information between contrastive
views. However, existing GCL methods rely on augmentation techniques that
introduce semantically irrelevant noise and incur significant computational and
storage costs, limiting effectiveness and efficiency.
  To overcome these challenges, we propose NLGCL, a novel contrastive learning
framework that leverages naturally contrastive views between neighbor layers
within GNNs. By treating each node and its neighbors in the next layer as
positive pairs, and other nodes as negatives, NLGCL avoids augmentation-based
noise while preserving semantic relevance. This paradigm eliminates costly view
construction and storage, making it computationally efficient and practical for
real-world scenarios. Extensive experiments on four public datasets demonstrate
that NLGCL outperforms state-of-the-art baselines in effectiveness and
efficiency.

</details>


### [55] [Document Similarity Enhanced IPS Estimation for Unbiased Learning to Rank](https://arxiv.org/abs/2507.07909)
*Zeyan Liang,Graham McDonald,Iadh Ounis*

Main category: cs.IR

TL;DR: 文章指出传统学习排序（LTR）模型训练有位置偏差，提出考虑文档相似度的IPSsim扩展方法，实验表明其在学习无偏LTR模型上比现有IPS估计器更有效。


<details>
  <summary>Details</summary>
Motivation: 传统LTR模型训练时用户点击数据存在位置偏差，现有逆倾向评分（IPS）方法不能很好解决。

Method: 提出IPS的扩展方法IPSsim，在估计IPS时考虑文档相似度，并在两个公开LTR数据集上用不同模拟用户点击设置和训练点击数进行评估。

Result: 实验显示IPSsim估计器在学习无偏LTR模型上比现有IPS估计器更有效，如n = 50时NDCG有显著约3%提升。

Conclusion: IPSsim估计器在学习无偏LTR模型上效果更好，尤其在top - n（n >= 30）设置中。

Abstract: Learning to Rank (LTR) models learn from historical user interactions, such
as user clicks. However, there is an inherent bias in the clicks of users due
to position bias, i.e., users are more likely to click highly-ranked documents
than low-ranked documents. To address this bias when training LTR models, many
approaches from the literature re-weight the users' click data using Inverse
Propensity Scoring (IPS). IPS re-weights the user's clicks proportionately to
the position in the historical ranking that a document was placed when it was
clicked since low-ranked documents are less likely to be seen by a user. In
this paper, we argue that low-ranked documents that are similar to
highly-ranked relevant documents are also likely to be relevant. Moreover,
accounting for the similarity of low-ranked documents to highly ranked relevant
documents when calculating IPS can more effectively mitigate the effects of
position bias. Therefore, we propose an extension to IPS, called IPSsim, that
takes into consideration the similarity of documents when estimating IPS. We
evaluate our IPSsim estimator using two large publicly available LTR datasets
under a number of simulated user click settings, and with different numbers of
training clicks. Our experiments show that our IPSsim estimator is more
effective than the existing IPS estimators for learning an unbiased LTR model,
particularly in top-n settings when n >= 30. For example, when n = 50, our
IPSsim estimator achieves a statistically significant ~3% improvement (p <
0.05) in terms of NDCG compared to the Doubly Robust estimator from the
literature.

</details>


### [56] [Measuring Hypothesis Testing Errors in the Evaluation of Retrieval Systems](https://arxiv.org/abs/2507.07924)
*Jack McKechnie,Graham McDonald,Craig Macdonald*

Main category: cs.IR

TL;DR: 本文聚焦信息检索系统评估中qrels的判别力，指出量化II类错误及使用平衡分类指标衡量qrels判别力的重要性。


<details>
  <summary>Details</summary>
Motivation: 获取大量人工相关性评估成本高，需高效评估方法并比较qrels效果，以往研究侧重I类错误，而II类错误也会误导科研方向。

Method: 量化II类错误，提出用平衡分类指标衡量qrels判别力，用不同相关性评估方法生成的qrels进行实验。

Result: 量化II类错误可深入了解qrels判别力，平衡分类指标能以单一数字全面概括判别力。

Conclusion: 量化II类错误及使用平衡分类指标对衡量qrels判别力有重要意义。

Abstract: The evaluation of Information Retrieval (IR) systems typically uses
query-document pairs with corresponding human-labelled relevance assessments
(qrels). These qrels are used to determine if one system is better than another
based on average retrieval performance. Acquiring large volumes of human
relevance assessments is expensive. Therefore, more efficient relevance
assessment approaches have been proposed, necessitating comparisons between
qrels to ascertain their efficacy. Discriminative power, i.e. the ability to
correctly identify significant differences between systems, is important for
drawing accurate conclusions on the robustness of qrels. Previous work has
measured the proportion of pairs of systems that are identified as
significantly different and has quantified Type I statistical errors. Type I
errors lead to incorrect conclusions due to false positive significance tests.
We argue that also identifying Type II errors (false negatives) is important as
they lead science in the wrong direction. We quantify Type II errors and
propose that balanced classification metrics, such as balanced accuracy, can be
used to portray the discriminative power of qrels. We perform experiments using
qrels generated using alternative relevance assessment methods to investigate
measuring hypothesis testing errors in IR evaluation. We find that additional
insights into the discriminative power of qrels can be gained by quantifying
Type II errors, and that balanced classification metrics can be used to give an
overall summary of discriminative power in one, easily comparable, number.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [57] [Growing Transformers: Modular Composition and Layer-wise Expansion on a Frozen Substrate](https://arxiv.org/abs/2507.07129)
*A. Bochkov*

Main category: cs.LG

TL;DR: 本文探索大语言模型可替代的构建式开发方法，提出模块化组合和逐层增长两种扩展范式，推动AI开发范式转变。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型整体端到端训练范式资源密集且缺乏灵活性，需探索新方法。

Method: 基于不可训练的确定性输入嵌入，提出模块化组合（将不同专家模型输出对数平均合并）和逐层构建训练（逐层堆叠训练Transformer层）两种方法。

Result: 模块化组合后的模型在推理基准上性能提升；逐层构建训练方法收敛稳定，模型深度与复杂推理能力相关。

Conclusion: 建议从整体优化转向更具生物性或构建性的AI开发范式，为资源高效扩展等开辟新途径。

Abstract: The prevailing paradigm for scaling large language models (LLMs) involves
monolithic, end-to-end training, a resource-intensive process that lacks
flexibility. This paper explores an alternative, constructive approach to model
development, built upon the foundation of non-trainable, deterministic input
embeddings. In prior [1], we established that high-level semantic reasoning can
emerge in Transformers using frozen embeddings derived from the visual
structure of Unicode glyphs. Here, we demonstrate that this fixed
representational substrate acts as a universal "docking port," enabling two
powerful and efficient scaling paradigms: seamless modular composition and
progressive layer-wise growth.
  First, we show that specialist models trained on disparate datasets (e.g.,
Russian and Chinese text) can be merged into a single, more capable
Mixture-of-Experts (MoE) model, post-training, with zero architectural
modification. This is achieved by simply averaging their output logits. The
resulting MoE model exhibits immediate performance improvements on reasoning
benchmarks like MMLU, surpassing its constituent experts without catastrophic
forgetting. Second, we introduce a layer-wise constructive training
methodology, where a deep Transformer is "grown" by progressively stacking and
training one layer at a time. This method demonstrates stable convergence and a
clear correlation between model depth and the emergence of complex reasoning
abilities, such as those required for SQuAD.
  Our findings suggest a paradigm shift from monolithic optimization towards a
more biological or constructive model of AI development, where complexity is
built incrementally and modules can be composed freely. This opens new avenues
for resource-efficient scaling, continual learning, and a more democratized
ecosystem for building powerful AI systems. We release all code and models to
facilitate further research.

</details>


### [58] [Bridging the Plausibility-Validity Gap by Fine-Tuning a Reasoning-Enhanced LLM for Chemical Synthesis and Discovery](https://arxiv.org/abs/2507.07328)
*Malikussaid,Hilal Hudan Nuha*

Main category: cs.LG

TL;DR: 本文针对大语言模型在化学领域的合理性 - 有效性差距问题，提出开发专业科学助手的方法，微调模型有显著提升但也有局限。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在化学等专业领域生成信息存在合理性 - 有效性差距的问题。

Method: 利用Magistral Small模型，使用LoRA进行微调，创建并标准化双领域数据集。

Result: 微调模型在格式遵循、生成分子化学有效性和合成路线可行性上有显著提升，呈现分层学习模式，与人类专家对比有竞争表现也有局限。

Conclusion: 建立了将通用大语言模型转化为化学研究可靠专业工具的可行框架，指出未来改进关键领域。

Abstract: Large Language Models (LLMs) often generate scientifically plausible but
factually invalid information, a challenge we term the "plausibility-validity
gap," particularly in specialized domains like chemistry. This paper presents a
systematic methodology to bridge this gap by developing a specialized
scientific assistant. We utilized the Magistral Small model, noted for its
integrated reasoning capabilities, and fine-tuned it using Low-Rank Adaptation
(LoRA). A key component of our approach was the creation of a "dual-domain
dataset," a comprehensive corpus curated from various sources encompassing both
molecular properties and chemical reactions, which was standardized to ensure
quality. Our evaluation demonstrates that the fine-tuned model achieves
significant improvements over the baseline model in format adherence, chemical
validity of generated molecules, and the feasibility of proposed synthesis
routes. The results indicate a hierarchical learning pattern, where syntactic
correctness is learned more readily than chemical possibility and synthesis
feasibility. While a comparative analysis with human experts revealed
competitive performance in areas like chemical creativity and reasoning, it
also highlighted key limitations, including persistent errors in
stereochemistry, a static knowledge cutoff, and occasional reference
hallucination. This work establishes a viable framework for adapting generalist
LLMs into reliable, specialized tools for chemical research, while also
delineating critical areas for future improvement.

</details>


### [59] [FACap: A Large-scale Fashion Dataset for Fine-grained Composed Image Retrieval](https://arxiv.org/abs/2507.07135)
*François Gardères,Shizhe Chen,Camille-Sovanneary Gauthier,Jean Ponce*

Main category: cs.LG

TL;DR: 提出大规模时尚领域CIR数据集FACap和模型FashionBLIP - 2，提升时尚CIR性能。


<details>
  <summary>Details</summary>
Motivation: 现有CIR方法在时尚领域表现不佳，缺乏大规模时尚数据集及详细标注。

Method: 构建FACap数据集，采用两阶段标注流程；提出FashionBLIP - 2模型，在FACap上微调BLIP - 2，使用轻量级适配器和多头查询 - 候选匹配。

Result: FashionBLIP - 2结合FACap预训练在时尚CIR中显著提升性能，尤其对细粒度修改文本检索。

Conclusion: 数据集和方法在电商等要求高的环境有价值。

Abstract: The composed image retrieval (CIR) task is to retrieve target images given a
reference image and a modification text. Recent methods for CIR leverage large
pretrained vision-language models (VLMs) and achieve good performance on
general-domain concepts like color and texture. However, they still struggle
with application domains like fashion, because the rich and diverse vocabulary
used in fashion requires specific fine-grained vision and language
understanding. An additional difficulty is the lack of large-scale fashion
datasets with detailed and relevant annotations, due to the expensive cost of
manual annotation by specialists. To address these challenges, we introduce
FACap, a large-scale, automatically constructed fashion-domain CIR dataset. It
leverages web-sourced fashion images and a two-stage annotation pipeline
powered by a VLM and a large language model (LLM) to generate accurate and
detailed modification texts. Then, we propose a new CIR model FashionBLIP-2,
which fine-tunes the general-domain BLIP-2 model on FACap with lightweight
adapters and multi-head query-candidate matching to better account for
fine-grained fashion-specific information. FashionBLIP-2 is evaluated with and
without additional fine-tuning on the Fashion IQ benchmark and the enhanced
evaluation dataset enhFashionIQ, leveraging our pipeline to obtain
higher-quality annotations. Experimental results show that the combination of
FashionBLIP-2 and pretraining with FACap significantly improves the model's
performance in fashion CIR especially for retrieval with fine-grained
modification texts, demonstrating the value of our dataset and approach in a
highly demanding environment such as e-commerce websites. Code is available at
https://fgxaos.github.io/facap-paper-website/.

</details>


### [60] [Scale leads to compositional generalization](https://arxiv.org/abs/2507.07207)
*Florian Redhardt,Yassir Akram,Simon Schug*

Main category: cs.LG

TL;DR: 研究标准神经网络在组合结构任务上的泛化能力，发现扩展数据和模型大小可实现组合泛化，还证明MLP的近似能力及隐藏层激活与任务成分的线性解码关系。


<details>
  <summary>Details</summary>
Motivation: 探究标准神经网络能否系统捕捉离散、组合任务结构，理解其在共享组合结构任务上的泛化条件。

Method: 通过实验观察不同任务编码下，扩展数据和模型大小对组合泛化的影响，证明标准多层感知机的近似能力，研究隐藏层激活与任务成分的关系。

Result: 简单扩展数据和模型大小可实现组合泛化，标准多层感知机可近似组合任务族，若网络成功组合泛化，任务成分可从隐藏层激活线性解码。

Conclusion: 扩展数据和模型大小有助于神经网络在组合结构任务上的泛化，隐藏层激活与任务成分的线性解码关系可作为衡量文本到图像生成模型组合概念能力的指标。

Abstract: Can neural networks systematically capture discrete, compositional task
structure despite their continuous, distributed nature? The impressive
capabilities of large-scale neural networks suggest that the answer to this
question is yes. However, even for the most capable models, there are still
frequent failure cases that raise doubts about their compositionality. Here, we
seek to understand what it takes for a standard neural network to generalize
over tasks that share compositional structure. We find that simply scaling data
and model size leads to compositional generalization. We show that this holds
across different task encodings as long as the training distribution
sufficiently covers the task space. In line with this finding, we prove that
standard multilayer perceptrons can approximate a general class of
compositional task families to arbitrary precision using only a linear number
of neurons with respect to the number of task modules. Finally, we uncover that
if networks successfully compositionally generalize, the constituents of a task
can be linearly decoded from their hidden activations. We show that this metric
correlates with failures of text-to-image generation models to compose known
concepts.

</details>


### [61] [Automating Evaluation of Diffusion Model Unlearning with (Vision-) Language Model World Knowledge](https://arxiv.org/abs/2507.07137)
*Eric Yeats,Darryl Hannan,Henry Kvinge,Timothy Doster,Scott Mahan*

Main category: cs.LG

TL;DR: 介绍自动工具autoeval - dmun评估扩散模型的遗忘学习，评估流行扩散模型遗忘学习方法并揭示语言模型作用。


<details>
  <summary>Details</summary>
Motivation: 机器遗忘学习（MU）虽成本低，但证明信息完全移除难，且会损害模型对周边概念的性能，需工具评估。

Method: 引入autoeval - dmun工具，利用（视觉 - ）语言模型，提取相关世界知识识别可能受损的周边概念，用对抗性提示规避遗忘学习。

Result: 评估流行扩散模型遗忘学习方法，发现语言模型能施加与遗忘学习损害相关性好的语义排序，且能有效用合成对抗性提示规避遗忘学习。

Conclusion: autoeval - dmun工具可有效评估扩散模型的遗忘学习。

Abstract: Machine unlearning (MU) is a promising cost-effective method to cleanse
undesired information (generated concepts, biases, or patterns) from
foundational diffusion models. While MU is orders of magnitude less costly than
retraining a diffusion model without the undesired information, it can be
challenging and labor-intensive to prove that the information has been fully
removed from the model. Moreover, MU can damage diffusion model performance on
surrounding concepts that one would like to retain, making it unclear if the
diffusion model is still fit for deployment. We introduce autoeval-dmun, an
automated tool which leverages (vision-) language models to thoroughly assess
unlearning in diffusion models. Given a target concept, autoeval-dmun extracts
structured, relevant world knowledge from the language model to identify nearby
concepts which are likely damaged by unlearning and to circumvent unlearning
with adversarial prompts. We use our automated tool to evaluate popular
diffusion model unlearning methods, revealing that language models (1) impose
semantic orderings of nearby concepts which correlate well with unlearning
damage and (2) effectively circumvent unlearning with synthetic adversarial
prompts.

</details>


### [62] [Attentions Under the Microscope: A Comparative Study of Resource Utilization for Variants of Self-Attention](https://arxiv.org/abs/2507.07247)
*Zhengyu Tian,Anantha Padmanaban Krishna Kumar,Hemant Krishnakumar,Reza Rawassizadeh*

Main category: cs.LG

TL;DR: 本文对GPT - 2架构训练中的八种注意力机制进行基准测试，发现优化内核实现的机制能效最佳，强调了注意力设计中能源感知基准测试的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和视觉语言模型发展，注意力机制的高复杂度成瓶颈，且缺乏对其训练时实际能源使用和硬件资源需求的严格评估。

Method: 对GPT - 2架构训练中的八种注意力机制进行基准测试，测量训练时间、GPU内存使用、FLOPS、CPU使用和功耗等关键指标。

Result: 优化内核实现的注意力机制，如Flash Attention、LSH Attention和MLA能效最佳；仅降低GPU功率不能保证减少能源使用，训练时间同样重要。

Conclusion: 强调了注意力设计中能源感知基准测试的重要性，为选择资源高效机制提供实用见解。

Abstract: As large language models (LLMs) and visual language models (VLMs) grow in
scale and application, attention mechanisms have become a central computational
bottleneck due to their high memory and time complexity. While many efficient
attention variants have been proposed, there remains a lack of rigorous
evaluation on their actual energy usage and hardware resource demands during
training. In this work, we benchmark eight attention mechanisms in training
GPT-2 architecture, measuring key metrics including training time, GPU memory
usage, FLOPS, CPU usage, and power consumption. Our results reveal that
attention mechanisms with optimized kernel implementations, including Flash
Attention, Locality-Sensitive Hashing (LSH) Attention, and Multi-Head Latent
Attention (MLA), achieve the best energy efficiency. We further show that lower
GPU power alone does not guarantee reduced energy use, as training time plays
an equally important role. Our study highlights the importance of energy-aware
benchmarking in attention design and provides a practical insight for selecting
resource-efficient mechanisms. All our codes are available at GitHub.

</details>


### [63] [Bias-Aware Mislabeling Detection via Decoupled Confident Learning](https://arxiv.org/abs/2507.07216)
*Yunyi Li,Maria De-Arteaga,Maytal Saar-Tsechansky*

Main category: cs.LG

TL;DR: 本文提出DeCoLe框架检测标签偏差数据集中的错误标签，理论证明其有效性并在仇恨言论检测中评估，结果显示优于其他方法，可提升数据可靠性。


<details>
  <summary>Details</summary>
Motivation: 标签偏差影响数据完整性，而有效解决该问题的方法稀缺。

Method: 提出基于机器学习的解耦置信学习（DeCoLe）框架检测标签偏差数据集中的错误标签。

Result: 在仇恨言论检测中，DeCoLe在偏差感知的错误标签检测方面表现出色，始终优于其他标签错误检测方法。

Conclusion: 识别并解决了偏差感知的错误标签检测挑战，可将DeCoLe集成到组织数据管理实践中以增强数据可靠性。

Abstract: Reliable data is a cornerstone of modern organizational systems. A notable
data integrity challenge stems from label bias, which refers to systematic
errors in a label, a covariate that is central to a quantitative analysis, such
that its quality differs across social groups. This type of bias has been
conceptually and empirically explored and is widely recognized as a pressing
issue across critical domains. However, effective methodologies for addressing
it remain scarce. In this work, we propose Decoupled Confident Learning
(DeCoLe), a principled machine learning based framework specifically designed
to detect mislabeled instances in datasets affected by label bias, enabling
bias aware mislabelling detection and facilitating data quality improvement. We
theoretically justify the effectiveness of DeCoLe and evaluate its performance
in the impactful context of hate speech detection, a domain where label bias is
a well documented challenge. Empirical results demonstrate that DeCoLe excels
at bias aware mislabeling detection, consistently outperforming alternative
approaches for label error detection. Our work identifies and addresses the
challenge of bias aware mislabeling detection and offers guidance on how DeCoLe
can be integrated into organizational data management practices as a powerful
tool to enhance data reliability.

</details>


### [64] [GNNs Meet Sequence Models Along the Shortest-Path: an Expressive Method for Link Prediction](https://arxiv.org/abs/2507.07138)
*Francesco Ferrini,Veronica Lachi,Antonio Longa,Bruno Lepri,Andrea Passerini*

Main category: cs.LG

TL;DR: 提出SP4LP框架结合GNN节点编码与最短路径序列建模用于链路预测，有计算效率且表现优。


<details>
  <summary>Details</summary>
Motivation: 现有GNN在链路预测中难以捕捉链路特定结构模式，现有注入结构上下文方法有计算成本高或无法建模多跳依赖的问题。

Method: 先使用GNN计算所有节点表示，再提取候选节点对间最短路径，用序列模型处理节点嵌入序列。

Result: SP4LP在链路预测基准测试中达到了最先进的性能。

Conclusion: 理论证明SP4LP比标准消息传递GNN和一些先进结构特征方法更具表达能力，是图中链路预测的通用原则性方法。

Abstract: Graph Neural Networks (GNNs) often struggle to capture the link-specific
structural patterns crucial for accurate link prediction, as their node-centric
message-passing schemes overlook the subgraph structures connecting a pair of
nodes. Existing methods to inject such structural context either incur high
computational cost or rely on simplistic heuristics (e.g., common neighbor
counts) that fail to model multi-hop dependencies. We introduce SP4LP (Shortest
Path for Link Prediction), a novel framework that combines GNN-based node
encodings with sequence modeling over shortest paths. Specifically, SP4LP first
applies a GNN to compute representations for all nodes, then extracts the
shortest path between each candidate node pair and processes the resulting
sequence of node embeddings using a sequence model. This design enables SP4LP
to capture expressive multi-hop relational patterns with computational
efficiency. Empirically, SP4LP achieves state-of-the-art performance across
link prediction benchmarks. Theoretically, we prove that SP4LP is strictly more
expressive than standard message-passing GNNs and several state-of-the-art
structural features methods, establishing it as a general and principled
approach for link prediction in graphs.

</details>


### [65] [Exploring Sparse Adapters for Scalable Merging of Parameter Efficient Experts](https://arxiv.org/abs/2507.07140)
*Samin Yeasar Arnob,Zhan Su,Minseon Kim,Oleksiy Ostapenko,Riyasat Ohib,Esra'a Saleh,Doina Precup,Lucas Caccia,Alessandro Sordoni*

Main category: cs.LG

TL;DR: 研究稀疏适配器作为模块化架构构建块的特性，提出训练方法，对比LoRA和全微调，发现合并后分布内性能更优，但所有方法实现强保留集性能仍有挑战。


<details>
  <summary>Details</summary>
Motivation: 研究稀疏适配器作为模块化架构潜在构建块的特性，因合并参数高效任务专家构建模块化架构受关注，而现有多以LoRA为基础。

Method: 提出简单方法训练高效稀疏适配器，研究其对多达20个自然语言处理任务适配器的合并特性。

Result: 提出的方法比LoRA和全微调效果好，稀疏适配器合并后分布内性能优于LoRA或全模型合并。

Conclusion: 稀疏适配器可作为模块化架构的有效构建块，但所有方法在实现强保留集性能上仍面临挑战。

Abstract: Merging parameter-efficient task experts has recently gained growing
attention as a way to build modular architectures that can be rapidly adapted
on the fly for specific downstream tasks, without requiring additional
fine-tuning. Typically, LoRA serves as the foundational building block of such
parameter-efficient modular architectures, leveraging low-rank weight
structures to reduce the number of trainable parameters. In this paper, we
study the properties of sparse adapters, which train only a subset of weights
in the base neural network, as potential building blocks of modular
architectures. First, we propose a simple method for training highly effective
sparse adapters, which is conceptually simpler than existing methods in the
literature and surprisingly outperforms both LoRA and full fine-tuning in our
setting. Next, we investigate the merging properties of these sparse adapters
by merging adapters for up to 20 natural language processing tasks, thus
scaling beyond what is usually studied in the literature. Our findings
demonstrate that sparse adapters yield superior in-distribution performance
post-merging compared to LoRA or full model merging. Achieving strong held-out
performance remains a challenge for all methods considered.

</details>


### [66] [Str-GCL: Structural Commonsense Driven Graph Contrastive Learning](https://arxiv.org/abs/2507.07141)
*Dongxiao He,Yongqi Huang,Jitao Zhao,Xiaobao Wang,Zhen Wang*

Main category: cs.LG

TL;DR: 提出Str - GCL框架将结构常识融入图对比学习，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图对比学习方法忽略图结构和属性中的结构常识，且识别和整合这些常识有挑战。

Method: 提出Str - GCL框架，用一阶逻辑规则表示结构常识并集成到GCL框架，引入拓扑和基于属性的规则，采用表示对齐机制。

Result: Str - GCL在实验中优于现有GCL方法。

Conclusion: Str - GCL为图表示学习中利用结构常识提供了新视角。

Abstract: Graph Contrastive Learning (GCL) is a widely adopted approach in
self-supervised graph representation learning, applying contrastive objectives
to produce effective representations. However, current GCL methods primarily
focus on capturing implicit semantic relationships, often overlooking the
structural commonsense embedded within the graph's structure and attributes,
which contains underlying knowledge crucial for effective representation
learning. Due to the lack of explicit information and clear guidance in general
graph, identifying and integrating such structural commonsense in GCL poses a
significant challenge. To address this gap, we propose a novel framework called
Structural Commonsense Unveiling in Graph Contrastive Learning (Str-GCL).
Str-GCL leverages first-order logic rules to represent structural commonsense
and explicitly integrates them into the GCL framework. It introduces
topological and attribute-based rules without altering the original graph and
employs a representation alignment mechanism to guide the encoder in
effectively capturing this commonsense. To the best of our knowledge, this is
the first attempt to directly incorporate structural commonsense into GCL.
Extensive experiments demonstrate that Str-GCL outperforms existing GCL
methods, providing a new perspective on leveraging structural commonsense in
graph representation learning.

</details>


### [67] [TRIP: A Nonparametric Test to Diagnose Biased Feature Importance Scores](https://arxiv.org/abs/2507.07276)
*Aaron Foote,Danny Krizanc*

Main category: cs.LG

TL;DR: 本文提出TRIP测试以检测因模型外推导致的不可靠排列特征重要性分数，还展示其在高维环境的应用，测试表明该测试可可靠检测分数可靠性。


<details>
  <summary>Details</summary>
Motivation: 排列特征重要性在处理相关特征时可能产生误导，需要方法检测其不可靠分数。

Method: 开发TRIP测试，只需最少假设，能检测不可靠排列特征重要性分数，并展示其在高维环境的补充使用方法。

Result: 通过模拟数据测试和应用，表明该测试可可靠检测排列特征重要性分数的不可靠性。

Conclusion: TRIP测试能有效检测排列特征重要性分数的可靠性。

Abstract: Along with accurate prediction, understanding the contribution of each
feature to the making of the prediction, i.e., the importance of the feature,
is a desirable and arguably necessary component of a machine learning model.
For a complex model such as a random forest, such importances are not innate --
as they are, e.g., with linear regression. Efficient methods have been created
to provide such capabilities, with one of the most popular among them being
permutation feature importance due to its efficiency, model-agnostic nature,
and perceived intuitiveness. However, permutation feature importance has been
shown to be misleading in the presence of dependent features as a result of the
creation of unrealistic observations when permuting the dependent features. In
this work, we develop TRIP (Test for Reliable Interpretation via Permutation),
a test requiring minimal assumptions that is able to detect unreliable
permutation feature importance scores that are the result of model
extrapolation. To build on this, we demonstrate how the test can be
complemented in order to allow its use in high dimensional settings. Through
testing on simulated data and applications, our results show that the test can
be used to reliably detect when permutation feature importance scores are
unreliable.

</details>


### [68] [Understanding Malware Propagation Dynamics through Scientific Machine Learning](https://arxiv.org/abs/2507.07143)
*Karthik Pappu,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.LG

TL;DR: 文章将科学机器学习应用于恶意软件建模，评估三种方法，发现UDE方法可大幅降低预测误差，还引入符号恢复方法揭示抑制机制，表明混合物理信息模型更优。


<details>
  <summary>Details</summary>
Motivation: 准确建模恶意软件传播对设计有效网络安全防御至关重要，传统和现有神经方法无法充分捕捉现实网络中的非线性反馈机制。

Method: 应用科学机器学习，评估经典常微分方程（ODEs）、通用微分方程（UDEs）和神经ODE三种方法，使用Code Red蠕虫爆发数据，引入符号恢复方法。

Result: UDE方法比传统和神经基线将预测误差降低44%，能保留可解释性，揭示了网络饱和、安全响应和恶意软件变种进化等抑制机制。

Conclusion: 混合物理信息模型优于纯分析和纯神经方法，能提高预测准确性，深入了解恶意软件传播动态，有助于开发预警系统、高效应对策略和针对性网络防御干预措施。

Abstract: Accurately modeling malware propagation is essential for designing effective
cybersecurity defenses, particularly against adaptive threats that evolve in
real time. While traditional epidemiological models and recent neural
approaches offer useful foundations, they often fail to fully capture the
nonlinear feedback mechanisms present in real-world networks. In this work, we
apply scientific machine learning to malware modeling by evaluating three
approaches: classical Ordinary Differential Equations (ODEs), Universal
Differential Equations (UDEs), and Neural ODEs. Using data from the Code Red
worm outbreak, we show that the UDE approach substantially reduces prediction
error compared to both traditional and neural baselines by 44%, while
preserving interpretability. We introduce a symbolic recovery method that
transforms the learned neural feedback into explicit mathematical expressions,
revealing suppression mechanisms such as network saturation, security response,
and malware variant evolution. Our results demonstrate that hybrid
physics-informed models can outperform both purely analytical and purely neural
approaches, offering improved predictive accuracy and deeper insight into the
dynamics of malware spread. These findings support the development of early
warning systems, efficient outbreak response strategies, and targeted cyber
defense interventions.

</details>


### [69] [Goal-Oriented Sequential Bayesian Experimental Design for Causal Learning](https://arxiv.org/abs/2507.07359)
*Zheyu Zhang,Jiayuan Dong,Jie Liu,Xun Huan*

Main category: cs.LG

TL;DR: 提出GO - CBED框架用于顺序因果实验设计，通过变分下界估计器优化，在多任务中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 传统方法旨在推断完整因果模型，不够有针对性和高效，需要一种能直接最大化用户指定因果量预期信息增益的方法。

Method: 引入变分下界估计器，通过基于transformer的策略网络和基于归一化流的变分后验联合优化，用摊销网络实现实时决策。

Result: GO - CBED在多种因果推理和发现任务中始终优于现有基线，尤其在实验预算有限和因果机制复杂的情况下。

Conclusion: 实验设计目标与特定研究目标对齐以及前瞻性顺序规划有益。

Abstract: We present GO-CBED, a goal-oriented Bayesian framework for sequential causal
experimental design. Unlike conventional approaches that select interventions
aimed at inferring the full causal model, GO-CBED directly maximizes the
expected information gain (EIG) on user-specified causal quantities of
interest, enabling more targeted and efficient experimentation. The framework
is both non-myopic, optimizing over entire intervention sequences, and
goal-oriented, targeting only model aspects relevant to the causal query. To
address the intractability of exact EIG computation, we introduce a variational
lower bound estimator, optimized jointly through a transformer-based policy
network and normalizing flow-based variational posteriors. The resulting policy
enables real-time decision-making via an amortized network. We demonstrate that
GO-CBED consistently outperforms existing baselines across various causal
reasoning and discovery tasks-including synthetic structural causal models and
semi-synthetic gene regulatory networks-particularly in settings with limited
experimental budgets and complex causal mechanisms. Our results highlight the
benefits of aligning experimental design objectives with specific research
goals and of forward-looking sequential planning.

</details>


### [70] [CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs](https://arxiv.org/abs/2507.07145)
*Zhaojing Zhou,Xunchao Li,Minghao Li,Handi Zhang,Haoshuang Wang,Wenbin Chang,Yiqun Liu,Qingqing Dang,Dianhai Yu,Yanjun Ma,Haifeng Wang*

Main category: cs.LG

TL;DR: 提出卷积码量化（CCQ）方法将大语言模型压缩到2.0 - 2.75位，减少推理成本，实验表现出色且开源部分模型和推理引擎。


<details>
  <summary>Details</summary>
Motivation: 大语言模型扩展带来推理成本高和部署障碍，现有低比特量化方法存在精度、可扩展性和效率问题。

Method: CCQ集成硬件感知的位移编码解码方案与卷积码、混合编码和码簇，构建无查找编码空间，借鉴向量量化的数据映射概念。

Result: CCQ在多个基准测试中表现出色，压缩DeepSeek - V3和ERNIE - 4.5 - 300B - A47B，实现ERNIE 4.5单GPU部署，开源2位ERNIE - 4.5 - 300B - A47B模型和推理引擎。

Conclusion: CCQ是一种有效的大语言模型量化方法，能在低比特下实现高性能推理。

Abstract: The rapid scaling of Large Language Models (LLMs) elevates inference costs
and compounds substantial deployment barriers. While quantization to 8 or 4
bits mitigates this, sub-3-bit methods face severe accuracy, scalability, and
efficiency degradation. We propose Convolutional Code Quantization (CCQ), an
inference-optimized quantization approach compressing LLMs to 2.0-2.75 bits
with minimal accuracy loss. Departing from error-prone scalar quantization or
slow vector quantization, CCQ integrates a hardware-aware bit-shift encoding
and decoding solution with Convolutional Code, Hybrid Encoding, and Code
Cluster, jointly overcoming accuracy-speed bottlenecks. We construct a
lookup-free encoding space, enabling a linear mapping between the codebook and
weight vectors, thereby optimizing inference performance. Meanwhile, by drawing
on the concept of data mapping from vector quantization, we minimize the
performance degradation of the model under extremely low-bit conditions.
Experiments demonstrate that CCQ achieves outstanding performance on LLMs
across various benchmarks. We compress DeepSeek-V3 (671B total parameters) to
184GB and ERNIE-4.5-300B-A47B to 89GB, enabling single-GPU deployment of ERNIE
4.5 and eliminating inter-card communication. The 2-bit ERNIE-4.5-300B-A47B
model and inference engine have been open-sourced.

</details>


### [71] [Plausible Counterfactual Explanations of Recommendations](https://arxiv.org/abs/2507.07919)
*Jakub Černý,Jiří Němeček,Ivan Dovica,Jakub Mareček*

Main category: cs.LG

TL;DR: 提出在推荐系统中生成高度可信反事实解释（CE）的方法并进行评估


<details>
  <summary>Details</summary>
Motivation: 解释在推荐系统中有多种作用，反事实解释是自然有用的形式

Method: 提出在推荐系统中生成高度可信CE的方法

Result: 对提出的方法进行了数值评估和用户研究

Conclusion: 未明确提及具体结论，但提出的方法经评估验证

Abstract: Explanations play a variety of roles in various recommender systems, from a
legally mandated afterthought, through an integral element of user experience,
to a key to persuasiveness. A natural and useful form of an explanation is the
Counterfactual Explanation (CE). We present a method for generating highly
plausible CEs in recommender systems and evaluate it both numerically and with
a user study.

</details>


### [72] [An attention-aware GNN-based input defender against multi-turn jailbreak on LLMs](https://arxiv.org/abs/2507.07146)
*Zixuan Huang,Kecheng Huang,Lihao Yin,Bowei He,Huiling Zhen,Mingxuan Yuan,Zili Shao*

Main category: cs.LG

TL;DR: 论文提出G - Guard防御大语言模型多轮越狱攻击，评估显示其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽经安全训练仍易受多轮越狱攻击，多轮攻击难检测和缓解。

Method: 提出基于注意力感知GNN的输入分类器G - Guard，构建实体图，引入注意力感知增强机制。

Result: G - Guard在所有数据集和评估指标上均优于所有基线。

Conclusion: G - Guard能有效防御大语言模型的多轮越狱攻击。

Abstract: Large Language Models (LLMs) have gained widespread popularity and are
increasingly integrated into various applications. However, their capabilities
can be exploited for both benign and harmful purposes. Despite rigorous
training and fine-tuning for safety, LLMs remain vulnerable to jailbreak
attacks. Recently, multi-turn attacks have emerged, exacerbating the issue.
Unlike single-turn attacks, multi-turn attacks gradually escalate the dialogue,
making them more difficult to detect and mitigate, even after they are
identified.
  In this study, we propose G-Guard, an innovative attention-aware GNN-based
input classifier designed to defend against multi-turn jailbreak attacks on
LLMs. G-Guard constructs an entity graph for multi-turn queries, explicitly
capturing relationships between harmful keywords and queries even when those
keywords appear only in previous queries. Additionally, we introduce an
attention-aware augmentation mechanism that retrieves the most similar
single-turn query based on the multi-turn conversation. This retrieved query is
treated as a labeled node in the graph, enhancing the ability of GNN to
classify whether the current query is harmful. Evaluation results demonstrate
that G-Guard outperforms all baselines across all datasets and evaluation
metrics.

</details>


### [73] [An Empirical Bernstein Inequality for Dependent Data in Hilbert Spaces and Applications](https://arxiv.org/abs/2507.07826)
*Erfan Mirzaei,Andreas Maurer,Vladimir R. Kostic,Massimiliano Pontil*

Main category: cs.LG

TL;DR: 本文引入适用于希尔伯特空间向量值过程的数据相关伯恩斯坦不等式，应用于协方差算子估计和算子学习，得到新风险界并做数值实验。


<details>
  <summary>Details</summary>
Motivation: 解决统计学习中从非独立同分布数据学习的难题。

Method: 引入适用于希尔伯特空间向量值过程的数据相关伯恩斯坦不等式，利用时间分离变量间相关性的快速衰减来改进估计。

Result: 将不等式应用于协方差算子估计和动力系统中的算子学习，获得了新的风险界，并进行了数值实验。

Conclusion: 提出的数据相关伯恩斯坦不等式在协方差算子估计和算子学习中具有实用性。

Abstract: Learning from non-independent and non-identically distributed data poses a
persistent challenge in statistical learning. In this study, we introduce
data-dependent Bernstein inequalities tailored for vector-valued processes in
Hilbert space. Our inequalities apply to both stationary and non-stationary
processes and exploit the potential rapid decay of correlations between
temporally separated variables to improve estimation. We demonstrate the
utility of these bounds by applying them to covariance operator estimation in
the Hilbert-Schmidt norm and to operator learning in dynamical systems,
achieving novel risk bounds. Finally, we perform numerical experiments to
illustrate the practical implications of these bounds in both contexts.

</details>


### [74] [Weighted Multi-Prompt Learning with Description-free Large Language Model Distillation](https://arxiv.org/abs/2507.07147)
*Sua Lee,Kyubum Shin,Jung Ho Park*

Main category: cs.LG

TL;DR: 提出无描述多提示学习方法DeMul，直接从大语言模型提炼知识到提示中，在多提示设置中证明提示加权潜力，实验在11个识别数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有结合大语言模型到提示学习的方法存在高变异性和低可靠性问题，需改进。

Method: 提出DeMul方法，省去提取描述过程，直接从大语言模型提炼知识到提示中，采用无描述方式让提示包含更丰富语义，在多提示设置中进行提示加权。

Result: 所提方法在11个识别数据集上取得优异性能。

Conclusion: DeMul方法有效，可解决现有方法问题并提升性能。

Abstract: Recent advances in pre-trained Vision Language Models (VLM) have shown
promising potential for effectively adapting to downstream tasks through prompt
learning, without the need for additional annotated paired datasets. To
supplement the text information in VLM trained on correlations with vision
data, new approaches leveraging Large Language Models (LLM) in prompts have
been proposed, enhancing robustness to unseen and diverse data. Existing
methods typically extract text-based responses (i.e., descriptions) from LLM to
incorporate into prompts; however, this approach suffers from high variability
and low reliability. In this work, we propose Description-free Multi-prompt
Learning(DeMul), a novel method that eliminates the process of extracting
descriptions and instead directly distills knowledge from LLM into prompts. By
adopting a description-free approach, prompts can encapsulate richer semantics
while still being represented as continuous vectors for optimization, thereby
eliminating the need for discrete pre-defined templates. Additionally, in a
multi-prompt setting, we empirically demonstrate the potential of prompt
weighting in reflecting the importance of different prompts during training.
Experimental results show that our approach achieves superior performance
across 11 recognition datasets.

</details>


### [75] [Pre-Trained AI Model Assisted Online Decision-Making under Missing Covariates: A Theoretical Perspective](https://arxiv.org/abs/2507.07852)
*Haichen Hu,David Simchi-Levi*

Main category: cs.LG

TL;DR: 研究有缺失协变量的序贯上下文决策问题，引入“模型弹性”概念，在MAR设定下校准预训练模型提升决策效果。


<details>
  <summary>Details</summary>
Motivation: 解决序贯上下文决策中协变量缺失问题，分析预训练模型对决策过程遗憾值的影响。

Method: 引入“模型弹性”概念，在MAR设定下用正交统计学习和双重稳健回归工具校准预训练模型。

Result: 校准预训练模型显著提高协变量插补质量，带来更好的遗憾保证。

Conclusion: 准确的预训练模型在序贯决策任务中有实用价值，模型弹性可作为理解和改进预训练模型集成的基本指标。

Abstract: We study a sequential contextual decision-making problem in which certain
covariates are missing but can be imputed using a pre-trained AI model. From a
theoretical perspective, we analyze how the presence of such a model influences
the regret of the decision-making process. We introduce a novel notion called
"model elasticity", which quantifies the sensitivity of the reward function to
the discrepancy between the true covariate and its imputed counterpart. This
concept provides a unified way to characterize the regret incurred due to model
imputation, regardless of the underlying missingness mechanism. More
surprisingly, we show that under the missing at random (MAR) setting, it is
possible to sequentially calibrate the pre-trained model using tools from
orthogonal statistical learning and doubly robust regression. This calibration
significantly improves the quality of the imputed covariates, leading to much
better regret guarantees. Our analysis highlights the practical value of having
an accurate pre-trained model in sequential decision-making tasks and suggests
that model elasticity may serve as a fundamental metric for understanding and
improving the integration of pre-trained models in a wide range of data-driven
decision-making problems.

</details>


### [76] [Bridging the Last Mile of Prediction: Enhancing Time Series Forecasting with Conditional Guided Flow Matching](https://arxiv.org/abs/2507.07192)
*Huibo Xu,Runlong Yu,Likang Wu,Xianquan Wang,Qi Liu*

Main category: cs.LG

TL;DR: 本文提出条件引导流匹配（CGFM）方法用于时间序列预测，通过学习辅助模型的误差提升性能，实验表明其优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在时间序列预测中有局限，流匹配虽有优势但潜力未充分挖掘，需提出新方法解决问题。

Method: 提出CGFM，结合辅助模型输出，在时间序列预测中融入历史数据作为条件和引导，构建双边条件概率路径，用一般仿射路径扩展概率路径空间。

Result: 大量实验显示CGFM持续提升并超越了现有最优模型。

Conclusion: CGFM在改进时间序列预测方法上非常有效。

Abstract: Diffusion models, a type of generative model, have shown promise in time
series forecasting. But they face limitations like rigid source distributions
and limited sampling paths, which hinder their performance. Flow matching
offers faster generation, higher-quality outputs, and greater flexibility,
while also possessing the ability to utilize valuable information from the
prediction errors of prior models, which were previously inaccessible yet
critically important. To address these challenges and fully unlock the untapped
potential of flow matching, we propose Conditional Guided Flow Matching (CGFM).
CGFM extends flow matching by incorporating the outputs of an auxiliary model,
enabling a previously unattainable capability in the field: learning from the
errors of the auxiliary model. For time series forecasting tasks, it integrates
historical data as conditions and guidance, constructs two-sided conditional
probability paths, and uses a general affine path to expand the space of
probability paths, ultimately leading to improved predictions. Extensive
experiments show that CGFM consistently enhances and outperforms
state-of-the-art models, highlighting its effectiveness in advancing
forecasting methods.

</details>


### [77] [Combining Pre-Trained Models for Enhanced Feature Representation in Reinforcement Learning](https://arxiv.org/abs/2507.07197)
*Elia Piccoli,Malio Li,Giacomo Carfì,Vincenzo Lomonaco,Davide Bacciu*

Main category: cs.LG

TL;DR: 提出Weight Sharing Attention (WSA)架构结合多预训练模型嵌入以丰富状态表示，在Atari游戏上有可比性能，还研究泛化能力与模型数量影响。


<details>
  <summary>Details</summary>
Motivation: 预训练模型在多领域有进展，强化学习存在无先验知识等问题，如何有效结合预训练模型在强化学习中是待研究问题。

Method: 提出WSA架构来结合多个预训练模型的嵌入以形成丰富的状态表示。

Result: WSA在多个Atari游戏上与端到端模型相比获得了可比的性能。

Conclusion: WSA架构能平衡效率和性能，还研究了其泛化能力及模型数量对代理性能的影响。

Abstract: The recent focus and release of pre-trained models have been a key components
to several advancements in many fields (e.g. Natural Language Processing and
Computer Vision), as a matter of fact, pre-trained models learn disparate
latent embeddings sharing insightful representations. On the other hand,
Reinforcement Learning (RL) focuses on maximizing the cumulative reward
obtained via agent's interaction with the environment. RL agents do not have
any prior knowledge about the world, and they either learn from scratch an
end-to-end mapping between the observation and action spaces or, in more recent
works, are paired with monolithic and computationally expensive Foundational
Models. How to effectively combine and leverage the hidden information of
different pre-trained models simultaneously in RL is still an open and
understudied question. In this work, we propose Weight Sharing Attention (WSA),
a new architecture to combine embeddings of multiple pre-trained models to
shape an enriched state representation, balancing the tradeoff between
efficiency and performance. We run an extensive comparison between several
combination modes showing that WSA obtains comparable performance on multiple
Atari games compared to end-to-end models. Furthermore, we study the
generalization capabilities of this approach and analyze how scaling the number
of models influences agents' performance during and after training.

</details>


### [78] [Stress Monitoring in Healthcare: An Ensemble Machine Learning Framework Using Wearable Sensor Data](https://arxiv.org/abs/2507.07589)
*Arpana Sinhal,Anay Sinhal,Amit Sinhal*

Main category: cs.LG

TL;DR: 本文引入多模态数据集，经预处理后用多种机器学习模型评估并组合成堆叠分类器，推动可部署压力监测系统发展，为保障医护人员心理健康提供实际意义，还指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 医护人员职业压力大，可穿戴传感器监测压力的现有研究缺乏综合数据集和强大分析框架，且现有压力检测方法存在处理类别不平衡和优化模型泛化性的问题。

Method: 引入包含生理信号等的多模态数据集，用SMOTE技术对数据集进行预处理以平衡压力状态表示，评估随机森林、XGBoost和多层感知器等先进机器学习模型并组合成堆叠分类器。

Result: 利用公开数据集和可重复的分析流程，推进了可部署压力监测系统的发展。

Conclusion: 该研究为保障医护人员心理健康有实际意义，未来可扩大人口多样性并探索边缘计算实现低延迟压力警报。

Abstract: Healthcare professionals, particularly nurses, face elevated occupational
stress, a concern amplified during the COVID-19 pandemic. While wearable
sensors offer promising avenues for real-time stress monitoring, existing
studies often lack comprehensive datasets and robust analytical frameworks.
This study addresses these gaps by introducing a multimodal dataset comprising
physiological signals, electrodermal activity, heart rate and skin temperature.
A systematic literature review identified limitations in prior stress-detection
methodologies, particularly in handling class imbalance and optimizing model
generalizability. To overcome these challenges, the dataset underwent
preprocessing with the Synthetic Minority Over sampling Technique (SMOTE),
ensuring balanced representation of stress states. Advanced machine learning
models including Random Forest, XGBoost and a Multi-Layer Perceptron (MLP) were
evaluated and combined into a Stacking Classifier to leverage their collective
predictive strengths. By using a publicly accessible dataset and a reproducible
analytical pipeline, this work advances the development of deployable
stress-monitoring systems, offering practical implications for safeguarding
healthcare workers' mental health. Future research directions include expanding
demographic diversity and exploring edge-computing implementations for low
latency stress alerts.

</details>


### [79] [Prospective Learning in Retrospect](https://arxiv.org/abs/2507.07965)
*Yuxin Bai,Cecelia Shuai,Ashwin De Silva,Siyu Yu,Pratik Chaudhari,Joshua T. Vogelstein*

Main category: cs.LG

TL;DR: 论文基于前瞻性学习框架改进算法和数值结果，并将其拓展到顺序决策场景。


<details>
  <summary>Details</summary>
Motivation: 大多数机器学习算法依托的PAC学习框架无法处理动态数据分布和不断变化的目标，导致性能不佳。

Method: 基于前瞻性学习框架进行改进和拓展。

Result: 得到改进算法和数值结果，并将其拓展到顺序决策场景（觅食）。

Conclusion: 展示了基于前瞻性学习框架在改进算法及拓展应用场景方面的初步成果，代码可公开获取。

Abstract: In most real-world applications of artificial intelligence, the distributions
of the data and the goals of the learners tend to change over time. The
Probably Approximately Correct (PAC) learning framework, which underpins most
machine learning algorithms, fails to account for dynamic data distributions
and evolving objectives, often resulting in suboptimal performance. Prospective
learning is a recently introduced mathematical framework that overcomes some of
these limitations. We build on this framework to present preliminary results
that improve the algorithm and numerical results, and extend prospective
learning to sequential decision-making scenarios, specifically foraging. Code
is available at: https://github.com/neurodata/prolearn2.

</details>


### [80] [Reinforcement Learning with Action Chunking](https://arxiv.org/abs/2507.07969)
*Qiyang Li,Zhiyuan Zhou,Sergey Levine*

Main category: cs.LG

TL;DR: 提出Q-chunking方法改进长视野、稀疏奖励任务的强化学习算法，在离线到在线设置中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决离线到在线强化学习设置中有效探索和样本高效学习的挑战，不清楚如何利用离线数据获得好的探索策略。

Method: 将动作分块技术应用于基于时间差分的强化学习方法，在分块动作空间中直接运行强化学习。

Result: Q-chunking在离线性能和在线样本效率上表现出色，在一系列长视野、稀疏奖励操作任务中优于先前最佳的离线到在线方法。

Conclusion: Q-chunking是一种简单有效的改进长视野、稀疏奖励任务强化学习算法的方法。

Abstract: We present Q-chunking, a simple yet effective recipe for improving
reinforcement learning (RL) algorithms for long-horizon, sparse-reward tasks.
Our recipe is designed for the offline-to-online RL setting, where the goal is
to leverage an offline prior dataset to maximize the sample-efficiency of
online learning. Effective exploration and sample-efficient learning remain
central challenges in this setting, as it is not obvious how the offline data
should be utilized to acquire a good exploratory policy. Our key insight is
that action chunking, a technique popularized in imitation learning where
sequences of future actions are predicted rather than a single action at each
timestep, can be applied to temporal difference (TD)-based RL methods to
mitigate the exploration challenge. Q-chunking adopts action chunking by
directly running RL in a 'chunked' action space, enabling the agent to (1)
leverage temporally consistent behaviors from offline data for more effective
online exploration and (2) use unbiased $n$-step backups for more stable and
efficient TD learning. Our experimental results demonstrate that Q-chunking
exhibits strong offline performance and online sample efficiency, outperforming
prior best offline-to-online methods on a range of long-horizon, sparse-reward
manipulation tasks.

</details>


### [81] [Efficient Parametric SVD of Koopman Operator for Stochastic Dynamical Systems](https://arxiv.org/abs/2507.07222)
*Minchan Jeong,J. Jon Ryu,Se-Young Yun,Gregory W. Wornell*

Main category: cs.LG

TL;DR: 提出基于低秩近似学习Koopman算子前k个奇异函数的方法，消除不稳定线性代数运算，结果可靠有效。


<details>
  <summary>Details</summary>
Motivation: 现有学习Koopman算子领先奇异子空间的方法需在目标计算中进行可能数值不稳定的操作，会引入有偏梯度估计并阻碍大规模系统的可扩展性。

Method: 基于低秩近似的思想，提出一种可扩展且概念简单的方法，用于学习随机动力系统的Koopman算子的前k个奇异函数，避免不稳定线性代数运算并易于集成到现代深度学习流程中。

Result: 学习到的奇异子空间对特征分析和多步预测等下游任务既可靠又有效。

Conclusion: 所提方法可行，能有效学习Koopman算子的奇异子空间，适用于下游任务。

Abstract: The Koopman operator provides a principled framework for analyzing nonlinear
dynamical systems through linear operator theory. Recent advances in dynamic
mode decomposition (DMD) have shown that trajectory data can be used to
identify dominant modes of a system in a data-driven manner. Building on this
idea, deep learning methods such as VAMPnet and DPNet have been proposed to
learn the leading singular subspaces of the Koopman operator. However, these
methods require backpropagation through potentially numerically unstable
operations on empirical second moment matrices, such as singular value
decomposition and matrix inversion, during objective computation, which can
introduce biased gradient estimates and hinder scalability to large systems. In
this work, we propose a scalable and conceptually simple method for learning
the top-k singular functions of the Koopman operator for stochastic dynamical
systems based on the idea of low-rank approximation. Our approach eliminates
the need for unstable linear algebraic operations and integrates easily into
modern deep learning pipelines. Empirical results demonstrate that the learned
singular subspaces are both reliable and effective for downstream tasks such as
eigen-analysis and multi-step prediction.

</details>


### [82] [An Information-Theoretic Perspective on Multi-LLM Uncertainty Estimation](https://arxiv.org/abs/2507.07236)
*Maya Kruse,Majid Afshar,Saksham Khatwani,Anoop Mayampurath,Guanhua Chen,Yanjun Gao*

Main category: cs.LG

TL;DR: 针对大语言模型输入输出不一致问题，提出MUSE方法，实验显示其在二元预测任务上表现优于单模型和简单集成基线。


<details>
  <summary>Details</summary>
Motivation: 大语言模型输入输出不一致，现有校准和不确定性量化工作常忽略模型多样性，需要利用模型多样性来更好地量化不确定性。

Method: 提出MUSE方法，使用Jensen - Shannon散度识别并聚合大语言模型中校准良好的子集。

Result: 在二元预测任务实验中，相比单模型和简单集成基线，校准和预测性能得到提升。

Conclusion: 利用模型多样性的MUSE方法能实现更可靠的不确定性估计，提升性能。

Abstract: Large language models (LLMs) often behave inconsistently across inputs,
indicating uncertainty and motivating the need for its quantification in
high-stakes settings. Prior work on calibration and uncertainty quantification
often focuses on individual models, overlooking the potential of model
diversity. We hypothesize that LLMs make complementary predictions due to
differences in training and the Zipfian nature of language, and that
aggregating their outputs leads to more reliable uncertainty estimates. To
leverage this, we propose MUSE (Multi-LLM Uncertainty via Subset Ensembles), a
simple information-theoretic method that uses Jensen-Shannon Divergence to
identify and aggregate well-calibrated subsets of LLMs. Experiments on binary
prediction tasks demonstrate improved calibration and predictive performance
compared to single-model and naive ensemble baselines.

</details>


### [83] [Towards Robust Surrogate Models: Benchmarking Machine Learning Approaches to Expediting Phase Field Simulations of Brittle Fracture](https://arxiv.org/abs/2507.07237)
*Erfan Hamdi,Emma Lejeune*

Main category: cs.LG

TL;DR: 文章引入基于相场模型模拟的数据集来评估机器学习方法在断裂建模中的应用，并实现评估了几种模型作为基线，结果展示了现有模型的优缺点和数据集的作用。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习技术在近似相场模型模拟中多依赖简单基准，无法反映断裂过程的真实复杂性，需引入有挑战性的数据集来评估和推动机器学习方法在断裂建模中的应用。

Method: 引入基于相场模型模拟的数据集，该数据集包含三种能量分解方法、两种边界条件和1000个随机初始裂纹配置，共6000次模拟；实现并评估物理信息神经网络、傅里叶神经算子和UNet模型作为基线，探索集成策略对预测精度的影响。

Result: 结果凸显了当前流行模型的潜力和局限性，证明了该数据集作为推进断裂力学研究中机器学习的试验台的实用性。

Conclusion: 结合数据集和文献中的基线模型，为评估固体力学中的机器学习方法提供了标准化且有挑战性的基准。

Abstract: Data driven approaches have the potential to make modeling complex, nonlinear
physical phenomena significantly more computationally tractable. For example,
computational modeling of fracture is a core challenge where machine learning
techniques have the potential to provide a much needed speedup that would
enable progress in areas such as mutli-scale modeling and uncertainty
quantification. Currently, phase field modeling (PFM) of fracture is one such
approach that offers a convenient variational formulation to model crack
nucleation, branching and propagation. To date, machine learning techniques
have shown promise in approximating PFM simulations. However, most studies rely
on overly simple benchmarks that do not reflect the true complexity of the
fracture processes where PFM excels as a method. To address this gap, we
introduce a challenging dataset based on PFM simulations designed to benchmark
and advance ML methods for fracture modeling. This dataset includes three
energy decomposition methods, two boundary conditions, and 1,000 random initial
crack configurations for a total of 6,000 simulations. Each sample contains 100
time steps capturing the temporal evolution of the crack field. Alongside this
dataset, we also implement and evaluate Physics Informed Neural Networks
(PINN), Fourier Neural Operators (FNO) and UNet models as baselines, and
explore the impact of ensembling strategies on prediction accuracy. With this
combination of our dataset and baseline models drawn from the literature we aim
to provide a standardized and challenging benchmark for evaluating machine
learning approaches to solid mechanics. Our results highlight both the promise
and limitations of popular current models, and demonstrate the utility of this
dataset as a testbed for advancing machine learning in fracture mechanics
research.

</details>


### [84] [Exploiting Edge Features for Transferable Adversarial Attacks in Distributed Machine Learning](https://arxiv.org/abs/2507.07259)
*Giulio Rossolini,Fabio Brau,Alessandro Biondi,Battista Biggio,Giorgio Buttazzo*

Main category: cs.LG

TL;DR: 在物联网边缘环境中，分区深度学习范式带来新安全风险，研究发现即使模型组件为黑盒，拦截中间特征的攻击者也能构成威胁，提出策略提升攻击可迁移性，强调需关注中间特征泄漏。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型在物联网边缘环境部署，分区深度学习范式带来新安全风险，传统推理设置不同，分布式管道攻击面更广，需探索潜在漏洞。

Method: 提出适用于分布式设置的利用策略，通过简单统计分析从矢量化传输特征重建原始张量形状，并相应调整替代架构以实现有效特征提取。

Result: 综合实验表明，利用所提策略训练的替代模型极大提高了对抗攻击的可迁移性。

Conclusion: 在安全分布式深度学习系统设计中，迫切需要考虑中间特征泄漏问题。

Abstract: As machine learning models become increasingly deployed across the edge of
internet of things environments, a partitioned deep learning paradigm in which
models are split across multiple computational nodes introduces a new dimension
of security risk. Unlike traditional inference setups, these distributed
pipelines span the model computation across heterogeneous nodes and
communication layers, thereby exposing a broader attack surface to potential
adversaries. Building on these motivations, this work explores a previously
overlooked vulnerability: even when both the edge and cloud components of the
model are inaccessible (i.e., black-box), an adversary who intercepts the
intermediate features transmitted between them can still pose a serious threat.
We demonstrate that, under these mild and realistic assumptions, an attacker
can craft highly transferable proxy models, making the entire deep learning
system significantly more vulnerable to evasion attacks. In particular, the
intercepted features can be effectively analyzed and leveraged to distill
surrogate models capable of crafting highly transferable adversarial examples
against the target model. To this end, we propose an exploitation strategy
specifically designed for distributed settings, which involves reconstructing
the original tensor shape from vectorized transmitted features using simple
statistical analysis, and adapting surrogate architectures accordingly to
enable effective feature distillation. A comprehensive and systematic
experimental evaluation has been conducted to demonstrate that surrogate models
trained with the proposed strategy, i.e., leveraging intermediate features,
tremendously improve the transferability of adversarial attacks. These findings
underscore the urgent need to account for intermediate feature leakage in the
design of secure distributed deep learning systems.

</details>


### [85] [Robust Multimodal Learning Framework For Intake Gesture Detection Using Contactless Radar and Wearable IMU Sensors](https://arxiv.org/abs/2507.07261)
*Chunzhuo Wang,Hans Hallez,Bart Vanrumste*

Main category: cs.LG

TL;DR: 研究提出MM - TCN - CMA框架融合IMU和雷达数据用于食物摄入手势检测，实验表明该框架性能优于单模态模型，且在缺失模态情况下仍有提升。


<details>
  <summary>Details</summary>
Motivation: 探索多模态学习结合可穿戴和非接触式传感方式能否提升食物摄入手势检测性能，解决多模态学习中模态缺失时鲁棒性降低的问题。

Method: 提出鲁棒的多模态时间卷积网络与跨模态注意力机制（MM - TCN - CMA），开发包含52个用餐会话的新数据集。

Result: 该框架相比单模态雷达和IMU模型，分段F1分数分别提高4.3%和5.2%；在缺失雷达和IMU输入时，仍分别有1.3%和2.4%的提升。

Conclusion: 这是首个有效融合IMU和雷达数据用于食物摄入手势检测的鲁棒多模态学习框架。

Abstract: Automated food intake gesture detection plays a vital role in dietary
monitoring, enabling objective and continuous tracking of eating behaviors to
support better health outcomes. Wrist-worn inertial measurement units (IMUs)
have been widely used for this task with promising results. More recently,
contactless radar sensors have also shown potential. This study explores
whether combining wearable and contactless sensing modalities through
multimodal learning can further improve detection performance. We also address
a major challenge in multimodal learning: reduced robustness when one modality
is missing. To this end, we propose a robust multimodal temporal convolutional
network with cross-modal attention (MM-TCN-CMA), designed to integrate IMU and
radar data, enhance gesture detection, and maintain performance under missing
modality conditions. A new dataset comprising 52 meal sessions (3,050 eating
gestures and 797 drinking gestures) from 52 participants is developed and made
publicly available. Experimental results show that the proposed framework
improves the segmental F1-score by 4.3% and 5.2% over unimodal Radar and IMU
models, respectively. Under missing modality scenarios, the framework still
achieves gains of 1.3% and 2.4% for missing radar and missing IMU inputs. This
is the first study to demonstrate a robust multimodal learning framework that
effectively fuses IMU and radar data for food intake gesture detection.

</details>


### [86] [Beyond the ATE: Interpretable Modelling of Treatment Effects over Dose and Time](https://arxiv.org/abs/2507.07271)
*Julianna Piskorz,Krzysztof Kacprzyk,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 提出在因果推断中建模治疗效果轨迹的框架，该框架可提取临床见解，方法准确、可解释且可编辑。


<details>
  <summary>Details</summary>
Motivation: 传统平均治疗效果（ATE）指标在医疗等应用中无法捕捉治疗效果随剂量和时间变化的细微动态。

Method: 将SemanticODE框架应用于因果推断场景，解耦轨迹形状估计与临床相关属性指定，支持领域先验、事后编辑和透明分析。

Result: 该方法能得到准确、可解释和可编辑的治疗动态模型。

Conclusion: 此方法有助于严格的因果分析和实际决策。

Abstract: The Average Treatment Effect (ATE) is a foundational metric in causal
inference, widely used to assess intervention efficacy in randomized controlled
trials (RCTs). However, in many applications -- particularly in healthcare --
this static summary fails to capture the nuanced dynamics of treatment effects
that vary with both dose and time. We propose a framework for modelling
treatment effect trajectories as smooth surfaces over dose and time, enabling
the extraction of clinically actionable insights such as onset time, peak
effect, and duration of benefit. To ensure interpretability, robustness, and
verifiability -- key requirements in high-stakes domains -- we adapt
SemanticODE, a recent framework for interpretable trajectory modelling, to the
causal setting where treatment effects are never directly observed. Our
approach decouples the estimation of trajectory shape from the specification of
clinically relevant properties (e.g., maxima, inflection points), supporting
domain-informed priors, post-hoc editing, and transparent analysis. We show
that our method yields accurate, interpretable, and editable models of
treatment dynamics, facilitating both rigorous causal analysis and practical
decision-making.

</details>


### [87] [Natural Evolutionary Search meets Probabilistic Numerics](https://arxiv.org/abs/2507.07288)
*Pierre Osselin,Masaki Adachi,Xiaowen Dong,Michael A. Osborne*

Main category: cs.LG

TL;DR: 本文介绍了ProbNES算法，它结合贝叶斯求积改进NES框架，在多任务中表现优于非概率对应算法和全局样本高效方法。


<details>
  <summary>Details</summary>
Motivation: 现有NES算法依赖随机采样和蒙特卡罗估计，样本效率有限，需要改进。

Method: 引入ProbNES算法，将贝叶斯求积融入NES框架。

Result: ProbNES算法在基准测试函数、数据驱动优化任务、用户告知超参数调整任务和运动任务等多种任务中，始终优于非概率对应算法和贝叶斯优化等全局样本高效方法。

Conclusion: ProbNES算法有效提升了NES框架的性能，在多任务中有更好表现。

Abstract: Zeroth-order local optimisation algorithms are essential for solving
real-valued black-box optimisation problems. Among these, Natural Evolution
Strategies (NES) represent a prominent class, particularly well-suited for
scenarios where prior distributions are available. By optimising the objective
function in the space of search distributions, NES algorithms naturally
integrate prior knowledge during initialisation, making them effective in
settings such as semi-supervised learning and user-prior belief frameworks.
However, due to their reliance on random sampling and Monte Carlo estimates,
NES algorithms can suffer from limited sample efficiency. In this paper, we
introduce a novel class of algorithms, termed Probabilistic Natural
Evolutionary Strategy Algorithms (ProbNES), which enhance the NES framework
with Bayesian quadrature. We show that ProbNES algorithms consistently
outperforms their non-probabilistic counterparts as well as global sample
efficient methods such as Bayesian Optimisation (BO) or $\pi$BO across a wide
range of tasks, including benchmark test functions, data-driven optimisation
tasks, user-informed hyperparameter tuning tasks and locomotion tasks.

</details>


### [88] [Estimating Dataset Dimension via Singular Metrics under the Manifold Hypothesis: Application to Inverse Problems](https://arxiv.org/abs/2507.07291)
*Paola Causin,Alessio Marta*

Main category: cs.LG

TL;DR: 提出用混合变分自编码器和黎曼几何工具处理高维数据低维流形结构的框架，估计数据集本征维数，助力流形参数化和逆问题求解，还探究网络剪枝影响。


<details>
  <summary>Details</summary>
Motivation: 高维数据集存在低维几何结构，要充分利用此特性需解决估计流形本征维数、构建局部坐标和学习空间映射三个关键任务。

Method: 提出使用混合变分自编码器和黎曼几何工具的框架，通过分析VAE解码器拉回度量的数值秩估计本征维数，用可逆VAE混合构建局部图表集。

Result: 该方法能增强不适定逆问题的解决方案，尤其在生物医学成像中，确保重建结果位于学习到的流形上，且本征维数可有效监测模型容量。

Conclusion: 所提框架能有效处理高维数据的低维流形结构相关任务，在逆问题求解和模型容量监测方面有积极作用。

Abstract: High-dimensional datasets often exhibit low-dimensional geometric structures,
as suggested by the manifold hypothesis, which implies that data lie on a
smooth manifold embedded in a higher-dimensional ambient space. While this
insight underpins many advances in machine learning and inverse problems, fully
leveraging it requires to deal with three key tasks: estimating the intrinsic
dimension (ID) of the manifold, constructing appropriate local coordinates, and
learning mappings between ambient and manifold spaces. In this work, we propose
a framework that addresses all these challenges using a Mixture of Variational
Autoencoders (VAEs) and tools from Riemannian geometry. We specifically focus
on estimating the ID of datasets by analyzing the numerical rank of the VAE
decoder pullback metric. The estimated ID guides the construction of an atlas
of local charts using a mixture of invertible VAEs, enabling accurate manifold
parameterization and efficient inference. We how this approach enhances
solutions to ill-posed inverse problems, particularly in biomedical imaging, by
enforcing that reconstructions lie on the learned manifold. Lastly, we explore
the impact of network pruning on manifold geometry and reconstruction quality,
showing that the intrinsic dimension serves as an effective proxy for
monitoring model capacity.

</details>


### [89] [Discretization-independent multifidelity operator learning for partial differential equations](https://arxiv.org/abs/2507.07292)
*Jacob Hauck,Yanzhi Zhang*

Main category: cs.LG

TL;DR: 本文开发了新的编码 - 近似 - 重建算子学习模型，引入数值算子学习和离散化独立性概念，有理论保证，经实验验证多保真训练有效。


<details>
  <summary>Details</summary>
Motivation: 研究离散化独立性如何实现鲁棒且高效的多保真算子学习。

Method: 开发利用输入输出函数分布基的神经表示的编码 - 近似 - 重建算子学习模型，引入数值算子学习和离散化独立性概念。

Result: 多保真训练显著提高了准确性和计算效率，进一步增强了经验离散化独立性。

Conclusion: 该模型在多保真算子学习方面有效且具有优势。

Abstract: We develop a new and general encode-approximate-reconstruct operator learning
model that leverages learned neural representations of bases for input and
output function distributions. We introduce the concepts of \textit{numerical
operator learning} and \textit{discretization independence}, which clarify the
relationship between theoretical formulations and practical realizations of
operator learning models. Our model is discretization-independent, making it
particularly effective for multifidelity learning. We establish theoretical
approximation guarantees, demonstrating uniform universal approximation under
strong assumptions on the input functions and statistical approximation under
weaker conditions. To our knowledge, this is the first comprehensive study that
investigates how discretization independence enables robust and efficient
multifidelity operator learning. We validate our method through extensive
numerical experiments involving both local and nonlocal PDEs, including
time-independent and time-dependent problems. The results show that
multifidelity training significantly improves accuracy and computational
efficiency. Moreover, multifidelity training further enhances empirical
discretization independence.

</details>


### [90] [AdeptHEQ-FL: Adaptive Homomorphic Encryption for Federated Learning of Hybrid Classical-Quantum Models with Dynamic Layer Sparing](https://arxiv.org/abs/2507.07316)
*Md Abrar Jahin,Taufikur Rahman Fuad,M. F. Mridha,Nafiz Fahad,Md. Jakir Hossen*

Main category: cs.LG

TL;DR: 提出AdeptHEQ - FL混合经典 - 量子联邦学习框架，经实验验证在准确率和通信开销上有优势。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在非IID分散环境下平衡模型性能、隐私保护和通信效率的挑战，且现有方法存在牺牲隐私保证、高开销或忽略量子增强表达性等问题。

Method: 引入AdeptHEQ - FL框架，包括混合CNN - PQC架构、自适应精度加权聚合方案、选择性同态加密和动态层自适应冻结。

Result: 在CIFAR - 10数据集上，比Standard - FedQNN和FHE - FedQNN分别提高约25.43%和14.17%的准确率，还可通过冻结不重要层减少通信开销。

Conclusion: AdeptHEQ - FL框架在联邦学习中隐私保护和资源利用方面具有效率和实用性。

Abstract: Federated Learning (FL) faces inherent challenges in balancing model
performance, privacy preservation, and communication efficiency, especially in
non-IID decentralized environments. Recent approaches either sacrifice formal
privacy guarantees, incur high overheads, or overlook quantum-enhanced
expressivity. We introduce AdeptHEQ-FL, a unified hybrid classical-quantum FL
framework that integrates (i) a hybrid CNN-PQC architecture for expressive
decentralized learning, (ii) an adaptive accuracy-weighted aggregation scheme
leveraging differentially private validation accuracies, (iii) selective
homomorphic encryption (HE) for secure aggregation of sensitive model layers,
and (iv) dynamic layer-wise adaptive freezing to minimize communication
overhead while preserving quantum adaptability. We establish formal privacy
guarantees, provide convergence analysis, and conduct extensive experiments on
the CIFAR-10, SVHN, and Fashion-MNIST datasets. AdeptHEQ-FL achieves a $\approx
25.43\%$ and $\approx 14.17\%$ accuracy improvement over Standard-FedQNN and
FHE-FedQNN, respectively, on the CIFAR-10 dataset. Additionally, it reduces
communication overhead by freezing less important layers, demonstrating the
efficiency and practicality of our privacy-preserving, resource-aware design
for FL.

</details>


### [91] [Optimizing Communication and Device Clustering for Clustered Federated Learning with Differential Privacy](https://arxiv.org/abs/2507.07320)
*Dongyu Wei,Xiaoren Xu,Shiwen Mao,Mingzhe Chen*

Main category: cs.LG

TL;DR: 提出安全高效的CFL设计，用DPVD - MARL算法优化问题，仿真显示比独立Q学习有更好表现。


<details>
  <summary>Details</summary>
Motivation: 基站处理能力异构、用户数据非IID，需优化RB分配和用户调度，且CFL方法有额外通信开销，要最小化训练损失。

Method: 提出DPVD - MARL算法，采用新的惩罚分配方案，让分布式基站独立决策并联合最小化训练损失。

Result: DPVD - MARL相比独立Q学习，收敛率最多提高20%，最终累积奖励提高15%。

Conclusion: DPVD - MARL算法能有效解决CFL中的优化问题，提高收敛速度和累积奖励。

Abstract: In this paper, a secure and communication-efficient clustered federated
learning (CFL) design is proposed. In our model, several base stations (BSs)
with heterogeneous task-handling capabilities and multiple users with
non-independent and identically distributed (non-IID) data jointly perform CFL
training incorporating differential privacy (DP) techniques. Since each BS can
process only a subset of the learning tasks and has limited wireless resource
blocks (RBs) to allocate to users for federated learning (FL) model parameter
transmission, it is necessary to jointly optimize RB allocation and user
scheduling for CFL performance optimization. Meanwhile, our considered CFL
method requires devices to use their limited data and FL model information to
determine their task identities, which may introduce additional communication
overhead. We formulate an optimization problem whose goal is to minimize the
training loss of all learning tasks while considering device clustering, RB
allocation, DP noise, and FL model transmission delay. To solve the problem, we
propose a novel dynamic penalty function assisted value decomposed multi-agent
reinforcement learning (DPVD-MARL) algorithm that enables distributed BSs to
independently determine their connected users, RBs, and DP noise of the
connected users but jointly minimize the training loss of all learning tasks
across all BSs. Different from the existing MARL methods that assign a large
penalty for invalid actions, we propose a novel penalty assignment scheme that
assigns penalty depending on the number of devices that cannot meet
communication constraints (e.g., delay), which can guide the MARL scheme to
quickly find valid actions, thus improving the convergence speed. Simulation
results show that the DPVD-MARL can improve the convergence rate by up to 20%
and the ultimate accumulated rewards by 15% compared to independent Q-learning.

</details>


### [92] [Generative Adversarial Evasion and Out-of-Distribution Detection for UAV Cyber-Attacks](https://arxiv.org/abs/2506.21142)
*Deepak Kumar Panda,Weisi Guo*

Main category: cs.LG

TL;DR: 本文针对无人机空域入侵检测系统面临的新威胁，提出基于cGAN生成对抗样本，用CVAE检测，结果显示CVAE在识别威胁上优于传统方法，强调先进概率建模对增强入侵检测系统能力的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着无人机融入民用空域，传统异常检测方法难以识别新威胁，常规OOD检测器难区分隐蔽对抗攻击与真实OOD事件，需提升入侵检测系统能力。

Method: 设计多类IDS分类器，用cGAN生成对抗样本并迭代优化，用CVAE检测扰动，通过负对数似然区分对抗输入和真实OOD样本。

Result: CVAE的遗憾分数在识别隐蔽对抗威胁方面显著优于传统基于马氏距离的检测器。

Conclusion: 先进的概率建模对增强入侵检测系统抵御自适应、基于生成模型的网络入侵能力至关重要。

Abstract: The growing integration of UAVs into civilian airspace underscores the need
for resilient and intelligent intrusion detection systems (IDS), as traditional
anomaly detection methods often fail to identify novel threats. A common
approach treats unfamiliar attacks as out-of-distribution (OOD) samples;
however, this leaves systems vulnerable when mitigation is inadequate.
Moreover, conventional OOD detectors struggle to distinguish stealthy
adversarial attacks from genuine OOD events. This paper introduces a
conditional generative adversarial network (cGAN)-based framework for crafting
stealthy adversarial attacks that evade IDS mechanisms. We first design a
robust multi-class IDS classifier trained on benign UAV telemetry and known
cyber-attacks, including Denial of Service (DoS), false data injection (FDI),
man-in-the-middle (MiTM), and replay attacks. Using this classifier, our cGAN
perturbs known attacks to generate adversarial samples that misclassify as
benign while retaining statistical resemblance to OOD distributions. These
adversarial samples are iteratively refined to achieve high stealth and success
rates. To detect such perturbations, we implement a conditional variational
autoencoder (CVAE), leveraging negative log-likelihood to separate adversarial
inputs from authentic OOD samples. Comparative evaluation shows that CVAE-based
regret scores significantly outperform traditional Mahalanobis distance-based
detectors in identifying stealthy adversarial threats. Our findings emphasize
the importance of advanced probabilistic modeling to strengthen IDS
capabilities against adaptive, generative-model-based cyber intrusions.

</details>


### [93] [Optimizing Model Splitting and Device Task Assignment for Deceptive Signal Assisted Private Multi-hop Split Learning](https://arxiv.org/abs/2507.07323)
*Dongyu Wei,Xiaoren Xu,Yuchen Liu,H. Vincent Poor,Mingzhe Chen*

Main category: cs.LG

TL;DR: 研究欺骗信号辅助的私有分裂学习，提出ICM - CA软演员 - 评论家深度强化学习框架解决设备分配问题，仿真显示有较好效果。


<details>
  <summary>Details</summary>
Motivation: 防止窃听者收集模型和数据信息，需确定欺骗信号传输设备子集、模型训练设备子集及分配给各训练设备的模型。

Method: 提出带有内在好奇心模块和交叉注意力的软演员 - 评论家深度强化学习框架（ICM - CA），利用ICM模块鼓励探索，CA模块确定历史状态 - 动作对重要性。

Result: 与传统SAC算法相比，所提方法将收敛速度提高达3倍，减少泄露给窃听者的信息达13%。

Conclusion: 所提ICM - CA框架能有效解决欺骗信号辅助的私有分裂学习中的设备分配问题，提高训练效率并减少信息泄露。

Abstract: In this paper, deceptive signal-assisted private split learning is
investigated. In our model, several edge devices jointly perform collaborative
training, and some eavesdroppers aim to collect the model and data information
from devices. To prevent the eavesdroppers from collecting model and data
information, a subset of devices can transmit deceptive signals. Therefore, it
is necessary to determine the subset of devices used for deceptive signal
transmission, the subset of model training devices, and the models assigned to
each model training device. This problem is formulated as an optimization
problem whose goal is to minimize the information leaked to eavesdroppers while
meeting the model training energy consumption and delay constraints. To solve
this problem, we propose a soft actor-critic deep reinforcement learning
framework with intrinsic curiosity module and cross-attention (ICM-CA) that
enables a centralized agent to determine the model training devices, the
deceptive signal transmission devices, the transmit power, and sub-models
assigned to each model training device without knowing the position and
monitoring probability of eavesdroppers. The proposed method uses an ICM module
to encourage the server to explore novel actions and states and a CA module to
determine the importance of each historical state-action pair thus improving
training efficiency. Simulation results demonstrate that the proposed method
improves the convergence rate by up to 3x and reduces the information leaked to
eavesdroppers by up to 13% compared to the traditional SAC algorithm.

</details>


### [94] [Leveraging Manifold Embeddings for Enhanced Graph Transformer Representations and Learning](https://arxiv.org/abs/2507.07335)
*Ankit Jyothish,Ali Jannesari*

Main category: cs.LG

TL;DR: 提出轻量级黎曼混合专家层，插入集成图变换器提升节点分类精度，使表示更易解释。


<details>
  <summary>Details</summary>
Motivation: 解决图变换器将节点嵌入单一欧几里得空间模糊异质拓扑的问题。

Method: 在最先进的集成图变换器前添加轻量级黎曼混合专家层，将节点路由到不同流形。

Result: 在四个节点分类基准上，该投影器使准确率最高提升3%。

Conclusion: 显式、几何感知的投影可增强预测能力，使图表示更易解释。

Abstract: Graph transformers typically embed every node in a single Euclidean space,
blurring heterogeneous topologies. We prepend a lightweight Riemannian
mixture-of-experts layer that routes each node to various kinds of manifold,
mixture of spherical, flat, hyperbolic - best matching its local structure.
These projections provide intrinsic geometric explanations to the latent space.
Inserted into a state-of-the-art ensemble graph transformer, this projector
lifts accuracy by up to 3% on four node-classification benchmarks. The ensemble
makes sure that both euclidean and non-euclidean features are captured.
Explicit, geometry-aware projection thus sharpens predictive power while making
graph representations more interpretable.

</details>


### [95] [Zero-Shot Context Generalization in Reinforcement Learning from Few Training Contexts](https://arxiv.org/abs/2507.07348)
*James Chapman,Kedar Karhadkar,Guido Montufar*

Main category: cs.LG

TL;DR: 本文针对深度强化学习（DRL）策略泛化性问题，引入上下文增强贝尔曼方程（CEBE）并推导上下文样本增强（CSE）方法，验证其提升DRL泛化性的潜力。


<details>
  <summary>Details</summary>
Motivation: DRL训练的策略在不同参数评估环境中泛化能力差，且获取多上下文训练数据在实际应用中不现实。

Method: 考虑具有上下文参数规律的上下文马尔可夫决策过程（CMDPs），引入CEBE，推导CSE作为数据增强方法。

Result: 理论和实验证明CEBE能一阶近似多上下文训练的Q函数，在模拟环境中验证CSE性能。

Conclusion: CSE有提升DRL泛化性的潜力。

Abstract: Deep reinforcement learning (DRL) has achieved remarkable success across
multiple domains, including competitive games, natural language processing, and
robotics. Despite these advancements, policies trained via DRL often struggle
to generalize to evaluation environments with different parameters. This
challenge is typically addressed by training with multiple contexts and/or by
leveraging additional structure in the problem. However, obtaining sufficient
training data across diverse contexts can be impractical in real-world
applications. In this work, we consider contextual Markov decision processes
(CMDPs) with transition and reward functions that exhibit regularity in context
parameters. We introduce the context-enhanced Bellman equation (CEBE) to
improve generalization when training on a single context. We prove both
analytically and empirically that the CEBE yields a first-order approximation
to the Q-function trained across multiple contexts. We then derive context
sample enhancement (CSE) as an efficient data augmentation method for
approximating the CEBE in deterministic control environments. We numerically
validate the performance of CSE in simulation environments, showcasing its
potential to improve generalization in DRL.

</details>


### [96] [Learning from positive and unlabeled examples -Finite size sample bounds](https://arxiv.org/abs/2507.07354)
*Farnam Mansouri,Shai Ben-David*

Main category: cs.LG

TL;DR: 本文对PU学习在更广泛设置下的统计复杂性进行理论分析，不假设类先验已知，并证明所需样本量的上下界。


<details>
  <summary>Details</summary>
Motivation: 现有PU学习工作大多依赖简化假设，本文要在更广泛设置下研究PU学习的统计复杂性。

Method: 进行理论分析，不假设类先验已知，证明所需样本量的上下界。

Result: 得出了正样本和未标记样本所需样本量的上下界。

Conclusion: 在不假设类先验已知的更广泛设置下，对PU学习的统计复杂性有了理论上的界定。

Abstract: PU (Positive Unlabeled) learning is a variant of supervised classification
learning in which the only labels revealed to the learner are of positively
labeled instances. PU learning arises in many real-world applications. Most
existing work relies on the simplifying assumptions that the positively labeled
training data is drawn from the restriction of the data generating distribution
to positively labeled instances and/or that the proportion of positively
labeled points (a.k.a. the class prior) is known apriori to the learner. This
paper provides a theoretical analysis of the statistical complexity of PU
learning under a wider range of setups. Unlike most prior work, our study does
not assume that the class prior is known to the learner. We prove upper and
lower bounds on the required sample sizes (of both the positively labeled and
the unlabeled samples).

</details>


### [97] [Atherosclerosis through Hierarchical Explainable Neural Network Analysis](https://arxiv.org/abs/2507.07373)
*Irsyad Adam,Steven Swee,Erika Yilin,Ethan Ji,William Speier,Dean Wang,Alex Bui,Wei Wang,Karol Watson,Peipei Ping*

Main category: cs.LG

TL;DR: 本文提出ATHENA框架用于亚临床动脉粥样硬化个性化分类，结合临床特征与分子数据，提升了分类性能，有助于患者亚型发现和个性化干预。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的疾病分类方法缺乏对队列特征的一致性和理解，且理解患者亚型时未整合患者间的致病相互依赖关系。

Method: 引入ATHENA框架，通过集成模态学习构建分层网络表示，优化患者特定分子指纹。

Result: 在391名患者的临床数据集上，相比各种基线，ROC曲线下面积（AUC）最多提高13%，F1分数最多提高20%。

Conclusion: ATHENA通过可解释AI驱动的子网络聚类实现机制驱动的患者亚型发现，加强个性化干预策略，改善动脉粥样硬化疾病进展预测和临床可操作结果管理。

Abstract: In this work, we study the problem pertaining to personalized classification
of subclinical atherosclerosis by developing a hierarchical graph neural
network framework to leverage two characteristic modalities of a patient:
clinical features within the context of the cohort, and molecular data unique
to individual patients. Current graph-based methods for disease classification
detect patient-specific molecular fingerprints, but lack consistency and
comprehension regarding cohort-wide features, which are an essential
requirement for understanding pathogenic phenotypes across diverse
atherosclerotic trajectories. Furthermore, understanding patient subtypes often
considers clinical feature similarity in isolation, without integration of
shared pathogenic interdependencies among patients. To address these
challenges, we introduce ATHENA: Atherosclerosis Through Hierarchical
Explainable Neural Network Analysis, which constructs a novel hierarchical
network representation through integrated modality learning; subsequently, it
optimizes learned patient-specific molecular fingerprints that reflect
individual omics data, enforcing consistency with cohort-wide patterns. With a
primary clinical dataset of 391 patients, we demonstrate that this
heterogeneous alignment of clinical features with molecular interaction
patterns has significantly boosted subclinical atherosclerosis classification
performance across various baselines by up to 13% in area under the receiver
operating curve (AUC) and 20% in F1 score. Taken together, ATHENA enables
mechanistically-informed patient subtype discovery through explainable AI
(XAI)-driven subnetwork clustering; this novel integration framework
strengthens personalized intervention strategies, thereby improving the
prediction of atherosclerotic disease progression and management of their
clinical actionable outcomes.

</details>


### [98] [Bradley-Terry and Multi-Objective Reward Modeling Are Complementary](https://arxiv.org/abs/2507.07375)
*Zhiwei Zhang,Hui Liu,Xiaomin Li,Zhenwei Dai,Jingying Zeng,Fali Wang,Minhua Lin,Ramraj Chandradevan,Zhen Li,Chen Luo,Xianfeng Tang,Qi He,Suhang Wang*

Main category: cs.LG

TL;DR: 本文指出当前RLHF易受奖励破解影响，现有方法在OOD场景表现不佳，提出统一奖励建模框架，实验证明可提升奖励模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有缓解奖励破解的方法多关注分布内场景，在OOD场景效果不佳，且多目标奖励函数因数据质量问题表现弱。

Method: 提出统一奖励建模框架，联合训练BT单目标和多目标回归奖励函数，使用共享嵌入空间，建立BT损失与回归目标的联系。

Result: 所提框架显著提升奖励模型的鲁棒性和评分性能，使7B模型优于70B基线。

Conclusion: 统一奖励建模框架有效，能应对OOD场景奖励破解问题，提升奖励模型性能。

Abstract: Reward models trained on human preference data have demonstrated strong
effectiveness in aligning Large Language Models (LLMs) with human intent under
the framework of Reinforcement Learning from Human Feedback (RLHF). However,
RLHF remains vulnerable to reward hacking, where the policy exploits
imperfections in the reward function rather than genuinely learning the
intended behavior. Although significant efforts have been made to mitigate
reward hacking, they predominantly focus on and evaluate in-distribution
scenarios, where the training and testing data for the reward model share the
same distribution. In this paper, we empirically show that state-of-the-art
methods struggle in more challenging out-of-distribution (OOD) settings. We
further demonstrate that incorporating fine-grained multi-attribute scores
helps address this challenge. However, the limited availability of high-quality
data often leads to weak performance of multi-objective reward functions, which
can negatively impact overall performance and become the bottleneck. To address
this issue, we propose a unified reward modeling framework that jointly trains
Bradley--Terry (BT) single-objective and multi-objective regression-based
reward functions using a shared embedding space. We theoretically establish a
connection between the BT loss and the regression objective and highlight their
complementary benefits. Specifically, the regression task enhances the
single-objective reward function's ability to mitigate reward hacking in
challenging OOD settings, while BT-based training improves the scoring
capability of the multi-objective reward function, enabling a 7B model to
outperform a 70B baseline. Extensive experimental results demonstrate that our
framework significantly improves both the robustness and the scoring
performance of reward models.

</details>


### [99] [GRIT: Graph Transformer For Internal Ice Layer Thickness Prediction](https://arxiv.org/abs/2507.07388)
*Zesheng Liu,Maryam Rahnemoonfar*

Main category: cs.LG

TL;DR: 本文介绍用于冰层厚度的图变换器GRIT，其性能优于基线图神经网络，能有效建模复杂时空动态。


<details>
  <summary>Details</summary>
Motivation: 深入理解雷达图像中内部冰层的厚度和变异性，对监测积雪、评估冰动力学过程和减少气候模型不确定性至关重要。

Method: 引入GRIT，将归纳几何图学习框架与注意力机制相结合，映射浅层和深层冰层之间的关系。

Result: 与基线图神经网络相比，GRIT的预测误差始终较低。

Conclusion: 注意力机制能有效捕捉冰层的时间变化，图变换器结合了变压器学习长距离依赖和图神经网络捕捉空间模式的优势，可对复杂时空动态进行稳健建模。

Abstract: Gaining a deeper understanding of the thickness and variability of internal
ice layers in Radar imagery is essential in monitoring the snow accumulation,
better evaluating ice dynamics processes, and minimizing uncertainties in
climate models. Radar sensors, capable of penetrating ice, capture detailed
radargram images of internal ice layers. In this work, we introduce GRIT, graph
transformer for ice layer thickness. GRIT integrates an inductive geometric
graph learning framework with an attention mechanism, designed to map the
relationships between shallow and deeper ice layers. Compared to baseline graph
neural networks, GRIT demonstrates consistently lower prediction errors. These
results highlight the attention mechanism's effectiveness in capturing temporal
changes across ice layers, while the graph transformer combines the strengths
of transformers for learning long-range dependencies with graph neural networks
for capturing spatial patterns, enabling robust modeling of complex
spatiotemporal dynamics.

</details>


### [100] [ST-GRIT: Spatio-Temporal Graph Transformer For Internal Ice Layer Thickness Prediction](https://arxiv.org/abs/2507.07389)
*Zesheng Liu,Maryam Rahnemoonfar*

Main category: cs.LG

TL;DR: 提出ST - GRIT处理雷达图像，实验表明其优于现有方法，凸显自注意力机制优势。


<details>
  <summary>Details</summary>
Motivation: 理解雷达图像中冰内层厚度和变异性对监测积雪、评估冰动力学及减少气候模型不确定性至关重要，需处理雷达图像。

Method: 提出ST - GRIT，利用归纳几何图学习框架提取局部空间特征，用时空注意力块分别建模长距离依赖。

Result: 在格陵兰冰盖雷达图数据实验中，ST - GRIT均方根误差更低，优于现有方法和其他基线图神经网络。

Conclusion: 图上自注意力机制有处理噪声、避免过平滑和捕捉长距离依赖的优势，时空注意力块能实现更全面有效的学习。

Abstract: Understanding the thickness and variability of internal ice layers in radar
imagery is crucial for monitoring snow accumulation, assessing ice dynamics,
and reducing uncertainties in climate models. Radar sensors, capable of
penetrating ice, provide detailed radargram images of these internal layers. In
this work, we present ST-GRIT, a spatio-temporal graph transformer for ice
layer thickness, designed to process these radargrams and capture the
spatiotemporal relationships between shallow and deep ice layers. ST-GRIT
leverages an inductive geometric graph learning framework to extract local
spatial features as feature embeddings and employs a series of temporal and
spatial attention blocks separately to model long-range dependencies
effectively in both dimensions. Experimental evaluation on radargram data from
the Greenland ice sheet demonstrates that ST-GRIT consistently outperforms
current state-of-the-art methods and other baseline graph neural networks by
achieving lower root mean-squared error. These results highlight the advantages
of self-attention mechanisms on graphs over pure graph neural networks,
including the ability to handle noise, avoid oversmoothing, and capture
long-range dependencies. Moreover, the use of separate spatial and temporal
attention blocks allows for distinct and robust learning of spatial
relationships and temporal patterns, providing a more comprehensive and
effective approach.

</details>


### [101] [Learning Collective Variables from Time-lagged Generation](https://arxiv.org/abs/2507.07390)
*Seonghyun Park,Kiyoung Seong,Soojung Yang,Rafael Gómez-Bombarelli,Sungsoo Ahn*

Main category: cs.LG

TL;DR: 提出TLC框架从生成模型的时间滞后条件学习集体变量（CVs），在丙氨酸二肽系统上验证其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法在发现CVs时未充分编码准确采样所需的详细动力学，难以直接观察分子动力学模拟中的罕见事件。

Method: 提出TLC框架，通过对时间滞后条件分布建模来学习CVs。

Result: 在丙氨酸二肽系统的两个基于CV的增强采样任务中，TLC在过渡路径采样和状态区分方面表现与现有MLCV方法相当或更优。

Conclusion: TLC框架是一种有效的学习CVs的方法，可用于增强采样任务。

Abstract: Rare events such as state transitions are difficult to observe directly with
molecular dynamics simulations due to long timescales. Enhanced sampling
techniques overcome this by introducing biases along carefully chosen
low-dimensional features, known as collective variables (CVs), which capture
the slow degrees of freedom. Machine learning approaches (MLCVs) have automated
CV discovery, but existing methods typically focus on discriminating
meta-stable states without fully encoding the detailed dynamics essential for
accurate sampling. We propose TLC, a framework that learns CVs directly from
time-lagged conditions of a generative model. Instead of modeling the static
Boltzmann distribution, TLC models a time-lagged conditional distribution
yielding CVs to capture the slow dynamic behavior. We validate TLC on the
Alanine Dipeptide system using two CV-based enhanced sampling tasks: (i)
steered molecular dynamics (SMD) and (ii) on-the-fly probability enhanced
sampling (OPES), demonstrating equal or superior performance compared to
existing MLCV methods in both transition path sampling and state
discrimination.

</details>


### [102] [Generalized Tree Edit Distance (GTED): A Faithful Evaluation Metric for Statement Autoformalization](https://arxiv.org/abs/2507.07399)
*Yuntian Liu,Tao Zhu,Xiaoyang Liu,Yu Chen,Zhaoxuan Liu,Qingfeng Guo,Jiashuo Zhang,Kangjie Bao,Tao Luo*

Main category: cs.LG

TL;DR: 现有语句自动形式化评估指标存在不足，本文提出 GTED 框架，在基准测试中表现优于基线指标。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法缺乏语义理解、计算成本高且受自动定理证明进展限制。

Method: 提出 GTED 框架，先将形式语句标准化并转换为运算符树，再用 GTED 指标确定语义相似度。

Result: 在 miniF2F 和 ProofNet 基准测试中，GTED 取得最高准确率和 Kappa 分数，优于所有基线指标。

Conclusion: GTED 为社区提供了更可靠的自动评估指标。

Abstract: Statement autoformalization, the automated translation of statement from
natural language into formal languages, has become a subject of extensive
research, yet the development of robust automated evaluation metrics remains
limited. Existing evaluation methods often lack semantic understanding, face
challenges with high computational costs, and are constrained by the current
progress of automated theorem proving. To address these issues, we propose GTED
(Generalized Tree Edit Distance), a novel evaluation framework that first
standardizes formal statements and converts them into operator trees, then
determines the semantic similarity using the eponymous GTED metric. On the
miniF2F and ProofNet benchmarks, GTED outperforms all baseline metrics by
achieving the highest accuracy and Kappa scores, thus providing the community
with a more faithful metric for automated evaluation. The code and experimental
results are available at https://github.com/XiaoyangLiu-sjtu/GTED.

</details>


### [103] [HGMP:Heterogeneous Graph Multi-Task Prompt Learning](https://arxiv.org/abs/2507.07405)
*Pengfei Jiao,Jialong Ni,Di Jin,Xuan Guo,Huan Liu,Hongjiang Chen,Yanxian Bi*

Main category: cs.LG

TL;DR: 本文提出异构图领域多任务提示框架HGMP，通过统一任务格式、设计对比预训练策略和引入异构特征提示，在公共数据集实验中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 预训练和微调方法存在预训练模型与下游任务不匹配问题，现有图提示学习方法难以整合对比预训练策略，需解决这些问题以提升异构图任务性能。

Method: 将下游任务统一为图级任务格式，设计图级对比预训练策略，引入异构特征提示。

Result: 在公共数据集上实验，所提方法能适应多种任务，显著优于基线方法。

Conclusion: 提出的HGMP框架能有效解决异构图任务中的问题，提升模型性能。

Abstract: The pre-training and fine-tuning methods have gained widespread attention in
the field of heterogeneous graph neural networks due to their ability to
leverage large amounts of unlabeled data during the pre-training phase,
allowing the model to learn rich structural features. However, these methods
face the issue of a mismatch between the pre-trained model and downstream
tasks, leading to suboptimal performance in certain application scenarios.
Prompt learning methods have emerged as a new direction in heterogeneous graph
tasks, as they allow flexible adaptation of task representations to address
target inconsistency. Building on this idea, this paper proposes a novel
multi-task prompt framework for the heterogeneous graph domain, named HGMP.
First, to bridge the gap between the pre-trained model and downstream tasks, we
reformulate all downstream tasks into a unified graph-level task format. Next,
we address the limitations of existing graph prompt learning methods, which
struggle to integrate contrastive pre-training strategies in the heterogeneous
graph domain. We design a graph-level contrastive pre-training strategy to
better leverage heterogeneous information and enhance performance in multi-task
scenarios. Finally, we introduce heterogeneous feature prompts, which enhance
model performance by refining the representation of input graph features.
Experimental results on public datasets show that our proposed method adapts
well to various tasks and significantly outperforms baseline methods.

</details>


### [104] [Neural networks leverage nominally quantum and post-quantum representations](https://arxiv.org/abs/2507.07432)
*Paul M. Riechers,Thomas J. Elliott,Adam S. Shai*

Main category: cs.LG

TL;DR: 预训练的深度神经网络（如transformers和RNNs）能发现并表示训练数据的低维生成模型，相关几何关系与网络架构无关。


<details>
  <summary>Details</summary>
Motivation: 研究预训练深度神经网络对训练数据生成模型的发现和表示能力。

Method: 对预训练的深度神经网络（transformers和RNNs）进行研究。

Result: 神经网络能发现相关表示，对应几何关系与架构无关，点反映未来概率密度及过去对未来的影响。

Conclusion: 预训练的深度神经网络在处理训练数据时具有独特能力，能类比贝叶斯更新。

Abstract: We show that deep neural networks, including transformers and RNNs,
pretrained as usual on next-token prediction, intrinsically discover and
represent beliefs over 'quantum' and 'post-quantum' low-dimensional generative
models of their training data -- as if performing iterative Bayesian updates
over the latent state of this world model during inference as they observe more
context. Notably, neural nets easily find these representation whereas there is
no finite classical circuit that would do the job. The corresponding geometric
relationships among neural activations induced by different input sequences are
found to be largely independent of neural-network architecture. Each point in
this geometry corresponds to a history-induced probability density over all
possible futures, and the relative displacement of these points reflects the
difference in mechanism and magnitude for how these distinct pasts affect the
future.

</details>


### [105] [General purpose models for the chemical sciences](https://arxiv.org/abs/2507.07456)
*Nawaf Alampara,Anagha Aneesh,Martiño Ríos-García,Adrian Mirza,Mara Schilling-Wilhelmi,Ali Asghar Aghajani,Meiling Sun,Gordan Prastalo,Kevin Maik Jablonka*

Main category: cs.LG

TL;DR: 探讨通用模型（GPMs）在化学科学应用，虽多处于原型阶段，但未来有望成熟。


<details>
  <summary>Details</summary>
Motivation: 数据驱动技术虽有潜力，但化学科学数据集特点致传统机器学习难完全利用，GPMs有解决不同任务及处理少量不同格式数据的能力。

Method: 讨论GPMs基本构建原则并回顾其在化学科学全流程的近期应用。

Result: 许多应用仍处于原型阶段。

Conclusion: 对GPMs的兴趣增加将使其在未来几年成熟。

Abstract: Data-driven techniques have a large potential to transform and accelerate the
chemical sciences. However, chemical sciences also pose the unique challenge of
very diverse, small, fuzzy datasets that are difficult to leverage in
conventional machine learning approaches completely. A new class of models,
general-purpose models (GPMs) such as large language models, have shown the
ability to solve tasks they have not been directly trained on, and to flexibly
operate with low amounts of data in different formats. In this review, we
discuss fundamental building principles of GPMs and review recent applications
of those models in the chemical sciences across the entire scientific process.
While many of these applications are still in the prototype phase, we expect
that the increasing interest in GPMs will make many of them mature in the
coming years.

</details>


### [106] [Resolving Token-Space Gradient Conflicts: Token Space Manipulation for Transformer-Based Multi-Task Learning](https://arxiv.org/abs/2507.07485)
*Wooseong Jeong,Kuk-Jin Yoon*

Main category: cs.LG

TL;DR: 提出DTME - MTL框架解决MTL负迁移和预训练模型适应性问题，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: MTL存在负迁移问题，预训练transformers适应性有限，以往动态网络架构效率低。

Method: 提出DTME - MTL框架，在token空间识别梯度冲突并根据类型应用自适应解决方案。

Result: DTME - MTL能持续提升多任务性能，计算开销小。

Conclusion: DTME - MTL为增强基于transformer的MTL模型提供了可扩展且有效的解决方案。

Abstract: Multi-Task Learning (MTL) enables multiple tasks to be learned within a
shared network, but differences in objectives across tasks can cause negative
transfer, where the learning of one task degrades another task's performance.
While pre-trained transformers significantly improve MTL performance, their
fixed network capacity and rigid structure limit adaptability. Previous dynamic
network architectures attempt to address this but are inefficient as they
directly convert shared parameters into task-specific ones. We propose Dynamic
Token Modulation and Expansion (DTME-MTL), a framework applicable to any
transformer-based MTL architecture. DTME-MTL enhances adaptability and reduces
overfitting by identifying gradient conflicts in token space and applying
adaptive solutions based on conflict type. Unlike prior methods that mitigate
negative transfer by duplicating network parameters, DTME-MTL operates entirely
in token space, enabling efficient adaptation without excessive parameter
growth. Extensive experiments demonstrate that DTME-MTL consistently improves
multi-task performance with minimal computational overhead, offering a scalable
and effective solution for enhancing transformer-based MTL models.

</details>


### [107] [Uncertainty Quantification for Motor Imagery BCI -- Machine Learning vs. Deep Learning](https://arxiv.org/abs/2507.07511)
*Joris Suurmeijer,Ivo Pascal de Jong,Matias Valdenegro-Toro,Andreea Ioana Sburlea*

Main category: cs.LG

TL;DR: 对比脑机接口（BCI）不同分类器的不确定性量化能力，发现CSP - LDA和MDRM - T不确定性估计最佳，Deep Ensembles和标准CNN分类效果最佳，且可通过拒绝模糊样本提高BCI准确性。


<details>
  <summary>Details</summary>
Motivation: 脑机接口分类器并非总是准确，标准运动想象BCI分类器虽能给出分类概率，但不确定性量化研究局限于深度学习，需对比不同分类器的不确定性量化能力。

Method: 对比基于Common Spatial Patterns（CSP - LDA）和Riemannian Geometry（MDRM）的成熟BCI分类器与深度学习中专门方法（Deep Ensembles和Direct Uncertainty Quantification）以及标准卷积神经网络（CNNs）的不确定性量化能力。

Result: 深度学习常见的过度自信问题在CSP - LDA和MDRM中不存在；MDRM存在信心不足问题，通过添加Temperature Scaling（MDRM - T）解决；CSP - LDA和MDRM - T的不确定性估计最佳，Deep Ensembles和标准CNN的分类效果最佳；所有模型都能区分难易估计。

Conclusion: 可以通过拒绝模糊样本提高运动想象BCI的准确性。

Abstract: Brain-computer interfaces (BCIs) turn brain signals into functionally useful
output, but they are not always accurate. A good Machine Learning classifier
should be able to indicate how confident it is about a given classification, by
giving a probability for its classification. Standard classifiers for Motor
Imagery BCIs do give such probabilities, but research on uncertainty
quantification has been limited to Deep Learning. We compare the uncertainty
quantification ability of established BCI classifiers using Common Spatial
Patterns (CSP-LDA) and Riemannian Geometry (MDRM) to specialized methods in
Deep Learning (Deep Ensembles and Direct Uncertainty Quantification) as well as
standard Convolutional Neural Networks (CNNs).
  We found that the overconfidence typically seen in Deep Learning is not a
problem in CSP-LDA and MDRM. We found that MDRM is underconfident, which we
solved by adding Temperature Scaling (MDRM-T). CSP-LDA and MDRM-T give the best
uncertainty estimates, but Deep Ensembles and standard CNNs give the best
classifications. We show that all models are able to separate between easy and
difficult estimates, so that we can increase the accuracy of a Motor Imagery
BCI by rejecting samples that are ambiguous.

</details>


### [108] [Neural Concept Verifier: Scaling Prover-Verifier Games via Concept Encodings](https://arxiv.org/abs/2507.07532)
*Berkant Turan,Suhrab Asadulla,David Steinmann,Wolfgang Stammer,Sebastian Pokutta*

Main category: cs.LG

TL;DR: 本文提出Neural Concept Verifier (NCV)框架，结合Prover - Verifier Games和概念编码用于高维环境下可解释的非线性分类，表现优于基线并能缓解捷径行为。


<details>
  <summary>Details</summary>
Motivation: Prover - Verifier Games未应用于高维图像等复杂输入，Concept Bottleneck Models依赖低容量线性预测器有局限，需要新方法。

Method: 利用最小监督概念发现模型从原始输入提取结构化概念编码，证明器选择子集，验证器（非线性预测器）用于决策。

Result: NCV在高维、逻辑复杂数据集上优于CBM和基于像素的PVG分类器基线，能缓解捷径行为。

Conclusion: NCV是迈向高性能、可验证AI的有前景一步。

Abstract: While Prover-Verifier Games (PVGs) offer a promising path toward
verifiability in nonlinear classification models, they have not yet been
applied to complex inputs such as high-dimensional images. Conversely, Concept
Bottleneck Models (CBMs) effectively translate such data into interpretable
concepts but are limited by their reliance on low-capacity linear predictors.
In this work, we introduce the Neural Concept Verifier (NCV), a unified
framework combining PVGs with concept encodings for interpretable, nonlinear
classification in high-dimensional settings. NCV achieves this by utilizing
recent minimally supervised concept discovery models to extract structured
concept encodings from raw inputs. A prover then selects a subset of these
encodings, which a verifier -- implemented as a nonlinear predictor -- uses
exclusively for decision-making. Our evaluations show that NCV outperforms CBM
and pixel-based PVG classifier baselines on high-dimensional, logically complex
datasets and also helps mitigate shortcut behavior. Overall, we demonstrate NCV
as a promising step toward performative, verifiable AI.

</details>


### [109] [Real-Time Decorrelation-Based Anomaly Detection for Multivariate Time Series](https://arxiv.org/abs/2507.07559)
*Amirhossein Sadough,Mahyar Shahsavari,Mark Wijtvliet,Marcel van Gerven*

Main category: cs.LG

TL;DR: 提出用于多元时间序列的实时异常检测方法DAD，有调优策略，实验显示性能优，适用于高维数据流。


<details>
  <summary>Details</summary>
Motivation: 实时异常检测需求增长，需处理高维流数据、单遍操作且内存使用少的方法。

Method: 提出基于在线去相关学习的DAD方法，动态单遍学习和监测数据相关性，还有调优策略。

Result: 在基准数据集上实验表明，DAD在不同异常类型上表现一致且优于现有方法，对高维数据鲁棒。

Conclusion: DAD在检测效果和计算效率间达最佳平衡，为实时、内存受限的异常检测设新标准。

Abstract: Anomaly detection (AD) plays a vital role across a wide range of real-world
domains by identifying data instances that deviate from expected patterns,
potentially signaling critical events such as system failures, fraudulent
activities, or rare medical conditions. The demand for real-time AD has surged
with the rise of the (Industrial) Internet of Things, where massive volumes of
multivariate sensor data must be processed instantaneously. Real-time AD
requires methods that not only handle high-dimensional streaming data but also
operate in a single-pass manner, without the burden of storing historical
instances, thereby ensuring minimal memory usage and fast decision-making. We
propose DAD, a novel real-time decorrelation-based anomaly detection method for
multivariate time series, based on an online decorrelation learning approach.
Unlike traditional proximity-based or reconstruction-based detectors that
process entire data or windowed instances, DAD dynamically learns and monitors
the correlation structure of data sample by sample in a single pass, enabling
efficient and effective detection. To support more realistic benchmarking
practices, we also introduce a practical hyperparameter tuning strategy
tailored for real-time anomaly detection scenarios. Extensive experiments on
widely used benchmark datasets demonstrate that DAD achieves the most
consistent and superior performance across diverse anomaly types compared to
state-of-the-art methods. Crucially, its robustness to increasing
dimensionality makes it particularly well-suited for real-time,
high-dimensional data streams. Ultimately, DAD not only strikes an optimal
balance between detection efficacy and computational efficiency but also sets a
new standard for real-time, memory-constrained anomaly detection.

</details>


### [110] [COALA: Numerically Stable and Efficient Framework for Context-Aware Low-Rank Approximation](https://arxiv.org/abs/2507.07580)
*Uliana Parkina,Maxim Rakhuba*

Main category: cs.LG

TL;DR: 现有神经网络低秩近似方法有数值不稳定问题，本文提出无逆正则化框架解决该问题。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络上下文感知低秩近似方法依赖经典公式，存在数值不稳定问题，会降低近似质量或导致矩阵奇异。

Method: 提出一种基于稳定分解的无逆正则化框架，可处理多种复杂场景，如校准矩阵超显存、输入激活矩阵近似奇异、数据不足等。

Result: 针对数据不足情况，证明解决方案收敛到期望近似并推导明确误差界。

Conclusion: 所提无逆正则化框架克服了现有方法的数值缺陷，能应对多种挑战场景。

Abstract: Recent studies suggest that context-aware low-rank approximation is a useful
tool for compression and fine-tuning of modern large-scale neural networks. In
this type of approximation, a norm is weighted by a matrix of input
activations, significantly improving metrics over the unweighted case.
Nevertheless, existing methods for neural networks suffer from numerical
instabilities due to their reliance on classical formulas involving explicit
Gram matrix computation and their subsequent inversion. We demonstrate that
this can degrade the approximation quality or cause numerically singular
matrices.
  To address these limitations, we propose a novel inversion-free regularized
framework that is based entirely on stable decompositions and overcomes the
numerical pitfalls of prior art. Our method can handle possible challenging
scenarios: (1) when calibration matrices exceed GPU memory capacity, (2) when
input activation matrices are nearly singular, and even (3) when insufficient
data prevents unique approximation. For the latter, we prove that our solution
converges to a desired approximation and derive explicit error bounds.

</details>


### [111] [CHOMET: Conditional Handovers via Meta-Learning](https://arxiv.org/abs/2507.07581)
*Michail Kalntis,Fernando A. Kuipers,George Iosifidis*

Main category: cs.LG

TL;DR: 本文提出一种基于O - RAN范式、利用元学习优化条件切换（CHO）的框架，在不稳定信号条件下性能比3GPP基准至少高180%。


<details>
  <summary>Details</summary>
Motivation: 传统切换面临延迟长和失败率高的问题，3GPP引入的CHO虽有优势但也带来新挑战，如资源分配和信令开销问题。

Method: 提出与O - RAN范式对齐、利用元学习进行CHO优化的框架。

Result: 该框架能提供强大的动态遗憾保证，在不稳定信号条件下性能比其他3GPP基准至少高180%。

Conclusion: 所提出的框架能有效优化CHO，解决其面临的新挑战。

Abstract: Handovers (HOs) are the cornerstone of modern cellular networks for enabling
seamless connectivity to a vast and diverse number of mobile users. However, as
mobile networks become more complex with more diverse users and smaller cells,
traditional HOs face significant challenges, such as prolonged delays and
increased failures. To mitigate these issues, 3GPP introduced conditional
handovers (CHOs), a new type of HO that enables the preparation (i.e., resource
allocation) of multiple cells for a single user to increase the chance of HO
success and decrease the delays in the procedure. Despite its advantages, CHO
introduces new challenges that must be addressed, including efficient resource
allocation and managing signaling/communication overhead from frequent cell
preparations and releases. This paper presents a novel framework aligned with
the O-RAN paradigm that leverages meta-learning for CHO optimization, providing
robust dynamic regret guarantees and demonstrating at least 180% superior
performance than other 3GPP benchmarks in volatile signal conditions.

</details>


### [112] [Improving Clustering on Occupational Text Data through Dimensionality Reduction](https://arxiv.org/abs/2507.07582)
*Iago Xabier Vázquez García,Damla Partanaz,Emrullah Fatih Yetkin*

Main category: cs.LG

TL;DR: 研究提出基于O*NET数据库的职业最优聚类机制，用BERT技术和聚类方法获映射图，考察降维影响，用轮廓法改进结果，助力职业自动区分。


<details>
  <summary>Details</summary>
Motivation: O*NET中职业定义因企业和国家而异，若要扩展不同任务定义职业的数据，需定义间的映射。

Method: 提出使用多种基于BERT技术和不同聚类方法的管道，考察降维方法对聚类算法性能指标的影响，用专门的轮廓法改进结果。

Result: 得到职业定义间的映射图，改进了聚类结果。

Conclusion: 基于聚类和降维的新映射方法可自动区分职业，为职业转换者开辟新路径。

Abstract: In this study, we focused on proposing an optimal clustering mechanism for
the occupations defined in the well-known US-based occupational database,
O*NET. Even though all occupations are defined according to well-conducted
surveys in the US, their definitions can vary for different firms and
countries. Hence, if one wants to expand the data that is already collected in
O*NET for the occupations defined with different tasks, a map between the
definitions will be a vital requirement. We proposed a pipeline using several
BERT-based techniques with various clustering approaches to obtain such a map.
We also examined the effect of dimensionality reduction approaches on several
metrics used in measuring performance of clustering algorithms. Finally, we
improved our results by using a specialized silhouette approach. This new
clustering-based mapping approach with dimensionality reduction may help
distinguish the occupations automatically, creating new paths for people
wanting to change their careers.

</details>


### [113] [Synthetic MC via Biological Transmitters: Therapeutic Modulation of the Gut-Brain Axis](https://arxiv.org/abs/2507.07604)
*Sebastian Lotter,Elisabeth Mohr,Andrina Rutsch,Lukas Brand,Francesca Ronchi,Laura Díaz-Marugán*

Main category: cs.LG

TL;DR: 本文提出利用个人健康数据设计基于肠道 - 脑轴（GBA）调制的治疗方法，定义理论要求，用机器学习模型验证，评估确认其准确性并识别关键调节途径。


<details>
  <summary>Details</summary>
Motivation: 合成分子通信（SMC）中在体内生成信号存在挑战，现有基于GBA调制的治疗方法因信号通路不明，只能帮助部分患者，需要更通用和稳健的治疗方法。

Method: 定义治疗性GBA调制的理论要求，提出机器学习模型在有限数据下验证要求。

Result: 评估显示模型在识别GBA不同调节剂方面准确性高。

Conclusion: 利用个人健康数据结合机器学习模型设计GBA调制治疗方法是可行的，能识别重要调节途径。

Abstract: Synthetic molecular communication (SMC) is a key enabler for future
healthcare systems in which Internet of Bio-Nano-Things (IoBNT) devices
facilitate the continuous monitoring of a patient's biochemical signals. To
close the loop between sensing and actuation, both the detection and the
generation of in-body molecular communication (MC) signals is key. However,
generating signals inside the human body, e.g., via synthetic nanodevices,
poses a challenge in SMC, due to technological obstacles as well as legal,
safety, and ethical issues. Hence, this paper considers an SMC system in which
signals are generated indirectly via the modulation of a natural in-body MC
system, namely the gut-brain axis (GBA). Therapeutic GBA modulation is already
established as treatment for neurological diseases, e.g., drug refractory
epilepsy (DRE), and performed via the administration of nutritional supplements
or specific diets. However, the molecular signaling pathways that mediate the
effect of such treatments are mostly unknown. Consequently, existing treatments
are standardized or designed heuristically and able to help only some patients
while failing to help others. In this paper, we propose to leverage personal
health data, e.g., gathered by in-body IoBNT devices, to design more versatile
and robust GBA modulation-based treatments as compared to the existing ones. To
show the feasibility of our approach, we define a catalog of theoretical
requirements for therapeutic GBA modulation. Then, we propose a machine
learning model to verify these requirements for practical scenarios when only
limited data on the GBA modulation exists. By evaluating the proposed model on
several datasets, we confirm its excellent accuracy in identifying different
modulators of the GBA. Finally, we utilize the proposed model to identify
specific modulatory pathways that play an important role for therapeutic GBA
modulation.

</details>


### [114] [Sparse Self-Federated Learning for Energy Efficient Cooperative Intelligence in Society 5.0](https://arxiv.org/abs/2507.07613)
*Davide Domini,Laura Erhan,Gianluca Aguzzi,Lucia Cavallaro,Amirhossein Douzandeh Zenoozi,Antonio Liotta,Mirko Viroli*

Main category: cs.LG

TL;DR: 提出资源感知的SParSeFuL方法，结合聚合计算和神经网络稀疏化减少能耗和带宽消耗，解决传统联邦学习在新兴物联网生态中可持续性不足问题。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习方法难以满足新兴物联网生态的可持续性需求，与绿色人工智能原则冲突。

Method: 引入Sparse Proximity-based Self-Federated Learning (SParSeFuL)，结合聚合计算进行自组织和神经网络稀疏化。

Result: 未提及

Conclusion: 未提及

Abstract: Federated Learning offers privacy-preserving collaborative intelligence but
struggles to meet the sustainability demands of emerging IoT ecosystems
necessary for Society 5.0-a human-centered technological future balancing
social advancement with environmental responsibility. The excessive
communication bandwidth and computational resources required by traditional FL
approaches make them environmentally unsustainable at scale, creating a
fundamental conflict with green AI principles as billions of
resource-constrained devices attempt to participate. To this end, we introduce
Sparse Proximity-based Self-Federated Learning (SParSeFuL), a resource-aware
approach that bridges this gap by combining aggregate computing for
self-organization with neural network sparsification to reduce energy and
bandwidth consumption.

</details>


### [115] [Sparse Causal Discovery with Generative Intervention for Unsupervised Graph Domain Adaptation](https://arxiv.org/abs/2507.07621)
*Junyu Luo,Yuhao Tang,Yiwei Fu,Xiao Luo,Zhizhuo Kou,Zhiping Xiao,Wei Ju,Wentao Zhang,Ming Zhang*

Main category: cs.LG

TL;DR: 提出SLOGAN方法解决无监督图域适应问题，实验表明其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有无监督图域适应方法因因果 - 虚假特征纠缠和全局对齐策略失败，结果不理想。

Method: 构建稀疏因果图结构，利用互信息瓶颈约束和解缠因果特征，设计生成干预机制打破局部虚假耦合，引入类别自适应动态校准策略。

Result: 在多个真实世界数据集上，SLOGAN显著优于现有基线。

Conclusion: SLOGAN能通过稀疏因果建模和动态干预机制实现稳定的图表示迁移。

Abstract: Unsupervised Graph Domain Adaptation (UGDA) leverages labeled source domain
graphs to achieve effective performance in unlabeled target domains despite
distribution shifts. However, existing methods often yield suboptimal results
due to the entanglement of causal-spurious features and the failure of global
alignment strategies. We propose SLOGAN (Sparse Causal Discovery with
Generative Intervention), a novel approach that achieves stable graph
representation transfer through sparse causal modeling and dynamic intervention
mechanisms. Specifically, SLOGAN first constructs a sparse causal graph
structure, leveraging mutual information bottleneck constraints to disentangle
sparse, stable causal features while compressing domain-dependent spurious
correlations through variational inference. To address residual spurious
correlations, we innovatively design a generative intervention mechanism that
breaks local spurious couplings through cross-domain feature recombination
while maintaining causal feature semantic consistency via covariance
constraints. Furthermore, to mitigate error accumulation in target domain
pseudo-labels, we introduce a category-adaptive dynamic calibration strategy,
ensuring stable discriminative learning. Extensive experiments on multiple
real-world datasets demonstrate that SLOGAN significantly outperforms existing
baselines.

</details>


### [116] [TransformEEG: Towards Improving Model Generalizability in Deep Learning-based EEG Parkinson's Disease Detection](https://arxiv.org/abs/2507.07622)
*Federico Del Pup,Riccardo Brun,Filippo Iotti,Edoardo Paccagnella,Mattia Pezzato,Sabrina Bertozzo,Andrea Zanola,Louis Fabrice Tshimanga,Henning Müller,Manfredo Atzori*

Main category: cs.LG

TL;DR: 本文提出用于帕金森病检测的混合卷积 - 变换器模型TransformEEG，在多个数据集上验证，相比其他模型结果更稳定准确。


<details>
  <summary>Details</summary>
Motivation: 当前基于脑电图（EEG）的深度学习模型在帕金森病检测中因高受试者间变异性而泛化性差，需开发更适合EEG数据的新架构。

Method: 提出TransformEEG，采用深度卷积分词器，使用四个公共数据集，进行10 - 外层、10 - 内层嵌套留N个受试者交叉验证，并与其他七个模型对比，还结合数据增强和阈值校正。

Result: TransformEEG平衡准确率中位数最高（78.45%）、四分位距最低（6.37%），结合数据增强和阈值校正后中位数准确率达80.10%，四分位距为5.74%。

Conclusion: TransformEEG结果更一致、偏差更小，相比其他模型减少了变异性，在帕金森病检测中更可靠。

Abstract: Electroencephalography (EEG) is establishing itself as an important,
low-cost, noninvasive diagnostic tool for the early detection of Parkinson's
Disease (PD). In this context, EEG-based Deep Learning (DL) models have shown
promising results due to their ability to discover highly nonlinear patterns
within the signal. However, current state-of-the-art DL models suffer from poor
generalizability caused by high inter-subject variability. This high
variability underscores the need for enhancing model generalizability by
developing new architectures better tailored to EEG data. This paper introduces
TransformEEG, a hybrid Convolutional-Transformer designed for Parkinson's
disease detection using EEG data. Unlike transformer models based on the EEGNet
structure, TransformEEG incorporates a depthwise convolutional tokenizer. This
tokenizer is specialized in generating tokens composed by channel-specific
features, which enables more effective feature mixing within the self-attention
layers of the transformer encoder. To evaluate the proposed model, four public
datasets comprising 290 subjects (140 PD patients, 150 healthy controls) were
harmonized and aggregated. A 10-outer, 10-inner Nested-Leave-N-Subjects-Out
(N-LNSO) cross-validation was performed to provide an unbiased comparison
against seven other consolidated EEG deep learning models. TransformEEG
achieved the highest balanced accuracy's median (78.45%) as well as the lowest
interquartile range (6.37%) across all the N-LNSO partitions. When combined
with data augmentation and threshold correction, median accuracy increased to
80.10%, with an interquartile range of 5.74%. In conclusion, TransformEEG
produces more consistent and less skewed results. It demonstrates a substantial
reduction in variability and more reliable PD detection using EEG data compared
to the other investigated models.

</details>


### [117] [HLF-FSL. A Decentralized Federated Split Learning Solution for IoT on Hyperledger Fabric](https://arxiv.org/abs/2507.07637)
*Carlos Beis Penedo,Rebeca P. Díaz Redondo,Ana Fernández Vilas,Manuel Fernández Veiga,Francisco Troncoso Pastoriza*

Main category: cs.LG

TL;DR: 提出结合联邦拆分学习（FSL）和许可区块链Hyperledger Fabric（HLF）的去中心化架构，在基准测试中表现良好，证明企业级可行性。


<details>
  <summary>Details</summary>
Motivation: 敏感领域的协作机器学习需要可扩展、保护隐私的企业级解决方案，传统联邦学习和拆分学习存在不足。

Method: 提出去中心化架构，用链码编排FSL的拆分模型执行和点对点聚合，利用HLF的瞬态字段和私有数据集合保护数据隐私。

Result: 在CIFAR - 10和MNIST基准测试中，HLF - FSL达到集中式FSL的准确率，减少每轮训练时间，性能和可扩展性测试显示区块链开销小且准确率得以保留。

Conclusion: 该架构具有企业级可行性。

Abstract: Collaborative machine learning in sensitive domains demands scalable, privacy
preserving solutions for enterprise deployment. Conventional Federated Learning
(FL) relies on a central server, introducing single points of failure and
privacy risks, while Split Learning (SL) partitions models for privacy but
scales poorly due to sequential training. We present a decentralized
architecture that combines Federated Split Learning (FSL) with the permissioned
blockchain Hyperledger Fabric (HLF). Our chaincode orchestrates FSL's split
model execution and peer-to-peer aggregation without any central coordinator,
leveraging HLF's transient fields and Private Data Collections (PDCs) to keep
raw data and model activations private. On CIFAR-10 and MNIST benchmarks,
HLF-FSL matches centralized FSL accuracy while reducing per epoch training time
compared to Ethereum-based works. Performance and scalability tests show
minimal blockchain overhead and preserved accuracy, demonstrating enterprise
grade viability.

</details>


### [118] [Some Theoretical Results on Layerwise Effective Dimension Oscillations in Finite Width ReLU Networks](https://arxiv.org/abs/2507.07675)
*Darshan Makwana*

Main category: cs.LG

TL;DR: 分析有限宽度全连接ReLU网络的逐层有效维度，推导隐藏激活矩阵期望秩的闭式表达式，揭示秩振荡行为及相关性质。


<details>
  <summary>Details</summary>
Motivation: 精确刻画随机ReLU层如何交替压缩和部分恢复输入变化子空间，为深度网络表达能力的先前研究增添细节。

Method: 针对固定批次输入和随机高斯权重，推导隐藏激活矩阵期望秩的闭式表达式，证明次高斯集中界。

Result: 得出期望有效维度公式，发现秩亏损几何衰减，确定期望秩局部最大值的“复兴”深度及高度，指出振荡秩行为是有限宽度现象。

Conclusion: 研究为随机ReLU层对输入子空间的影响提供了精确刻画，补充了深度网络表达能力研究。

Abstract: We analyze the layerwise effective dimension (rank of the feature matrix) in
fully-connected ReLU networks of finite width. Specifically, for a fixed batch
of $m$ inputs and random Gaussian weights, we derive closed-form expressions
for the expected rank of the \$m\times n\$ hidden activation matrices. Our main
result shows that $\mathbb{E}[EDim(\ell)]=m[1-(1-2/\pi)^\ell]+O(e^{-c m})$ so
that the rank deficit decays geometrically with ratio $1-2 / \pi \approx
0.3634$. We also prove a sub-Gaussian concentration bound, and identify the
"revival" depths at which the expected rank attains local maxima. In
particular, these peaks occur at depths
$\ell_k^*\approx(k+1/2)\pi/\log(1/\rho)$ with height $\approx (1-e^{-\pi/2}) m
\approx 0.79m$. We further show that this oscillatory rank behavior is a
finite-width phenomenon: under orthogonal weight initialization or strong
negative-slope leaky-ReLU, the rank remains (nearly) full. These results
provide a precise characterization of how random ReLU layers alternately
collapse and partially revive the subspace of input variations, adding nuance
to prior work on expressivity of deep networks.

</details>


### [119] [Balancing the Past and Present: A Coordinated Replay Framework for Federated Class-Incremental Learning](https://arxiv.org/abs/2507.07712)
*Zhuang Qi,Lei Meng,Han Yu*

Main category: cs.LG

TL;DR: 提出FedCBDR方法解决FCIL中的类别不平衡问题，实验表明有精度提升。


<details>
  <summary>Details</summary>
Motivation: 现有FCIL的数据重放方法受类别不平衡问题限制，性能不佳。

Method: 提出FedCBDR方法，含全局视角数据重放模块和任务感知温度缩放模块。

Result: FedCBDR在异构数据分布下实现类别平衡采样，在任务不平衡时提高泛化能力，Top - 1精度比六种先进方法提升2% - 15%。

Conclusion: FedCBDR方法能有效解决FCIL中的类别不平衡问题，提升模型性能。

Abstract: Federated Class Incremental Learning (FCIL) aims to collaboratively process
continuously increasing incoming tasks across multiple clients. Among various
approaches, data replay has become a promising solution, which can alleviate
forgetting by reintroducing representative samples from previous tasks.
However, their performance is typically limited by class imbalance, both within
the replay buffer due to limited global awareness and between replayed and
newly arrived classes. To address this issue, we propose a class wise balancing
data replay method for FCIL (FedCBDR), which employs a global coordination
mechanism for class-level memory construction and reweights the learning
objective to alleviate the aforementioned imbalances. Specifically, FedCBDR has
two key components: 1) the global-perspective data replay module reconstructs
global representations of prior task in a privacy-preserving manner, which then
guides a class-aware and importance-sensitive sampling strategy to achieve
balanced replay; 2) Subsequently, to handle class imbalance across tasks, the
task aware temperature scaling module adaptively adjusts the temperature of
logits at both class and instance levels based on task dynamics, which reduces
the model's overconfidence in majority classes while enhancing its sensitivity
to minority classes. Experimental results verified that FedCBDR achieves
balanced class-wise sampling under heterogeneous data distributions and
improves generalization under task imbalance between earlier and recent tasks,
yielding a 2%-15% Top-1 accuracy improvement over six state-of-the-art methods.

</details>


### [120] [GuardVal: Dynamic Large Language Model Jailbreak Evaluation for Comprehensive Safety Testing](https://arxiv.org/abs/2507.07735)
*Peiyan Zhang,Haibo Jin,Liying Kang,Haohan Wang*

Main category: cs.LG

TL;DR: 本文回顾现有越狱评估方法，提出新评估协议GuardVal及优化方法，对不同模型在多个安全领域评估，揭示模型行为模式，助于开发更安全模型。


<details>
  <summary>Details</summary>
Motivation: 越狱攻击暴露大语言模型漏洞，现有基准和评估方法难以全面应对评估挑战，存在漏洞评估缺口。

Method: 回顾现有越狱评估实践，确定有效评估协议的三个假定要求；引入GuardVal评估协议，动态生成和优化越狱提示；提出防止提示优化停滞的新方法。

Result: 对Mistral - 7b到GPT - 4等多种模型在10个安全领域评估，发现模型不同行为模式。

Conclusion: 评估过程加深对大语言模型行为的理解，为未来研究提供见解，推动更安全模型的开发。

Abstract: Jailbreak attacks reveal critical vulnerabilities in Large Language Models
(LLMs) by causing them to generate harmful or unethical content. Evaluating
these threats is particularly challenging due to the evolving nature of LLMs
and the sophistication required in effectively probing their vulnerabilities.
Current benchmarks and evaluation methods struggle to fully address these
challenges, leaving gaps in the assessment of LLM vulnerabilities. In this
paper, we review existing jailbreak evaluation practices and identify three
assumed desiderata for an effective jailbreak evaluation protocol. To address
these challenges, we introduce GuardVal, a new evaluation protocol that
dynamically generates and refines jailbreak prompts based on the defender LLM's
state, providing a more accurate assessment of defender LLMs' capacity to
handle safety-critical situations. Moreover, we propose a new optimization
method that prevents stagnation during prompt refinement, ensuring the
generation of increasingly effective jailbreak prompts that expose deeper
weaknesses in the defender LLMs. We apply this protocol to a diverse set of
models, from Mistral-7b to GPT-4, across 10 safety domains. Our findings
highlight distinct behavioral patterns among the models, offering a
comprehensive view of their robustness. Furthermore, our evaluation process
deepens the understanding of LLM behavior, leading to insights that can inform
future research and drive the development of more secure models.

</details>


### [121] [Efficient and Scalable Estimation of Distributional Treatment Effects with Multi-Task Neural Networks](https://arxiv.org/abs/2507.07738)
*Tomu Hirata,Undral Byambadalai,Tatsushi Oka,Shota Yasui,Shingo Uto*

Main category: cs.LG

TL;DR: 提出一种用于估计随机实验中分布处理效应（DTE）的多任务神经网络方法，在模拟和真实数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统回归调整方法估计DTE存在精度和计算效率问题，如分布尾部精度受数据不平衡影响，大规模数据集需解决大量回归问题。

Method: 利用多任务神经网络估计条件结果分布，结合单调形状约束和多阈值标签学习提高准确性。

Result: 在模拟和真实数据集上实验结果表明该方法性能优越。

Conclusion: 该方法是现代因果推断应用中理解处理效应异质性的可靠实用解决方案。

Abstract: We propose a novel multi-task neural network approach for estimating
distributional treatment effects (DTE) in randomized experiments. While DTE
provides more granular insights into the experiment outcomes over conventional
methods focusing on the Average Treatment Effect (ATE), estimating it with
regression adjustment methods presents significant challenges. Specifically,
precision in the distribution tails suffers due to data imbalance, and
computational inefficiencies arise from the need to solve numerous regression
problems, particularly in large-scale datasets commonly encountered in
industry. To address these limitations, our method leverages multi-task neural
networks to estimate conditional outcome distributions while incorporating
monotonic shape constraints and multi-threshold label learning to enhance
accuracy. To demonstrate the practical effectiveness of our proposed method, we
apply our method to both simulated and real-world datasets, including a
randomized field experiment aimed at reducing water consumption in the US and a
large-scale A/B test from a leading streaming platform in Japan. The
experimental results consistently demonstrate superior performance across
various datasets, establishing our method as a robust and practical solution
for modern causal inference applications requiring a detailed understanding of
treatment effect heterogeneity.

</details>


### [122] [OPC: One-Point-Contraction Unlearning Toward Deep Feature Forgetting](https://arxiv.org/abs/2507.07754)
*Jaeheun Jung,Bosung Jung,Suhyun Bae,Donghun Lee*

Main category: cs.LG

TL;DR: 现有机器学习遗忘方法存在浅层遗忘问题，本文提出OPC算法解决该问题并展现良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习遗忘方法存在浅层遗忘现象，内部表征仍保留足够信息，无法满足隐私、法律或道德要求。

Method: 通过无训练性能恢复攻击和基于梯度反演的数据重建攻击证实浅层遗忘的普遍性；基于待遗忘数据特征表示的单点收缩定义“深度遗忘”理论标准；提出有效近似算法构建OPC算法。

Result: 在图像分类遗忘基准测试中，OPC不仅实现有效遗忘，还对性能恢复攻击和梯度反演攻击具有出色的抵御能力。

Conclusion: OPC独特的遗忘性能源于其理论基础所强制的深度特征遗忘，强调提高机器学习遗忘方法鲁棒性的必要性。

Abstract: Machine unlearning seeks to remove the influence of particular data or class
from trained models to meet privacy, legal, or ethical requirements. Existing
unlearning methods tend to forget shallowly: phenomenon of an unlearned model
pretend to forget by adjusting only the model response, while its internal
representations retain information sufficiently to restore the forgotten data
or behavior. We empirically confirm the widespread shallowness by reverting the
forgetting effect of various unlearning methods via training-free performance
recovery attack and gradient-inversion-based data reconstruction attack. To
address this vulnerability fundamentally, we define a theoretical criterion of
``deep forgetting'' based on one-point-contraction of feature representations
of data to forget. We also propose an efficient approximation algorithm, and
use it to construct a novel general-purpose unlearning algorithm:
One-Point-Contraction (OPC). Empirical evaluations on image classification
unlearning benchmarks show that OPC achieves not only effective unlearning
performance but also superior resilience against both performance recovery
attack and gradient-inversion attack. The distinctive unlearning performance of
OPC arises from the deep feature forgetting enforced by its theoretical
foundation, and recaps the need for improved robustness of machine unlearning
methods.

</details>


### [123] [TRIX- Trading Adversarial Fairness via Mixed Adversarial Training](https://arxiv.org/abs/2507.07768)
*Tejaswini Medi,Steffen Jung,Margret Keuper*

Main category: cs.LG

TL;DR: 现有对抗训练存在对抗不公平问题，本文提出TRIX框架，经实验证明能提升最差类准确率、减少类间鲁棒性差异并保持整体准确率。


<details>
  <summary>Details</summary>
Motivation: 现有对抗训练方法对所有类采用统一训练目标，忽略类间脆弱性差异，导致对抗不公平，强类更鲁棒，弱类易受攻击。

Method: 引入TRIX框架，为强类分配较弱目标对抗样本，通过均匀采样目标促进特征多样性；为弱类分配较强无目标对抗样本，增强其针对性鲁棒性，还结合每类损失加权和扰动强度调整。

Result: 在标准图像分类基准上的综合实验表明，TRIX显著提高了干净和对抗数据上的最差类准确率，减少了类间鲁棒性差异，保留了整体准确率。

Conclusion: TRIX是迈向公平有效对抗防御的实用步骤。

Abstract: Adversarial Training (AT) is a widely adopted defense against adversarial
examples. However, existing approaches typically apply a uniform training
objective across all classes, overlooking disparities in class-wise
vulnerability. This results in adversarial unfairness: classes with well
distinguishable features (strong classes) tend to become more robust, while
classes with overlapping or shared features(weak classes) remain
disproportionately susceptible to adversarial attacks. We observe that strong
classes do not require strong adversaries during training, as their non-robust
features are quickly suppressed. In contrast, weak classes benefit from
stronger adversaries to effectively reduce their vulnerabilities. Motivated by
this, we introduce TRIX, a feature-aware adversarial training framework that
adaptively assigns weaker targeted adversaries to strong classes, promoting
feature diversity via uniformly sampled targets, and stronger untargeted
adversaries to weak classes, enhancing their focused robustness. TRIX further
incorporates per-class loss weighting and perturbation strength adjustments,
building on prior work, to emphasize weak classes during the optimization.
Comprehensive experiments on standard image classification benchmarks,
including evaluations under strong attacks such as PGD and AutoAttack,
demonstrate that TRIX significantly improves worst-case class accuracy on both
clean and adversarial data, reducing inter-class robustness disparities, and
preserves overall accuracy. Our results highlight TRIX as a practical step
toward fair and effective adversarial defense.

</details>


### [124] [BEAVER: Building Environments with Assessable Variation for Evaluating Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2507.07769)
*Ruohong Liu,Jack Umenberger,Yize Chen*

Main category: cs.LG

TL;DR: 研究针对跨环境、多目标建筑能源管理任务的强化学习方法，构建新基准评估算法，发现现有方法有一定权衡能力但特定环境下性能下降。


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习的建筑能源管理方法在可扩展性和泛化性方面存在问题，需要进一步研究。

Method: 形式化定义跨环境、多目标建筑能源管理任务的泛化空间，提出多目标上下文强化学习问题，对上下文信息参数化并构建新基准。

Result: 现有多目标强化学习方法能在冲突目标间取得合理权衡，但特定环境变化下性能下降。

Conclusion: 在策略学习过程中纳入依赖动态的上下文信息很重要。

Abstract: Recent years have seen significant advancements in designing reinforcement
learning (RL)-based agents for building energy management. While individual
success is observed in simulated or controlled environments, the scalability of
RL approaches in terms of efficiency and generalization across building
dynamics and operational scenarios remains an open question. In this work, we
formally characterize the generalization space for the cross-environment,
multi-objective building energy management task, and formulate the
multi-objective contextual RL problem. Such a formulation helps understand the
challenges of transferring learned policies across varied operational contexts
such as climate and heat convection dynamics under multiple control objectives
such as comfort level and energy consumption. We provide a principled way to
parameterize such contextual information in realistic building RL environments,
and construct a novel benchmark to facilitate the evaluation of generalizable
RL algorithms in practical building control tasks. Our results show that
existing multi-objective RL methods are capable of achieving reasonable
trade-offs between conflicting objectives. However, their performance degrades
under certain environment variations, underscoring the importance of
incorporating dynamics-dependent contextual information into the policy
learning process.

</details>


### [125] [Synchronizing Task Behavior: Aligning Multiple Tasks during Test-Time Training](https://arxiv.org/abs/2507.07778)
*Wooseong Jeong,Jegyeong Cho,Youngho Yoon,Kuk-Jin Yoon*

Main category: cs.LG

TL;DR: 提出S4T方法解决多任务测试时训练中任务不同步问题，在多基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统测试时训练（TTT）方法在处理多任务时存在任务行为不同步问题，需改进。

Method: 提出Synchronizing Tasks for Test - time Training (S4T)方法，通过预测跨域任务关系同步任务，并与传统TTT协议结合应用于多任务基准测试。

Result: S4T在各种基准测试中优于最先进的TTT方法。

Conclusion: S4T是解决多任务测试时训练中任务不同步问题的有效方法。

Abstract: Generalizing neural networks to unseen target domains is a significant
challenge in real-world deployments. Test-time training (TTT) addresses this by
using an auxiliary self-supervised task to reduce the domain gap caused by
distribution shifts between the source and target. However, we find that when
models are required to perform multiple tasks under domain shifts, conventional
TTT methods suffer from unsynchronized task behavior, where the adaptation
steps needed for optimal performance in one task may not align with the
requirements of other tasks. To address this, we propose a novel TTT approach
called Synchronizing Tasks for Test-time Training (S4T), which enables the
concurrent handling of multiple tasks. The core idea behind S4T is that
predicting task relations across domain shifts is key to synchronizing tasks
during test time. To validate our approach, we apply S4T to conventional
multi-task benchmarks, integrating it with traditional TTT protocols. Our
empirical results show that S4T outperforms state-of-the-art TTT methods across
various benchmarks.

</details>


### [126] [Space-Filling Regularization for Robust and Interpretable Nonlinear State Space Models](https://arxiv.org/abs/2507.07792)
*Hermann Klein,Max Heinz Herkersdorf,Oliver Nelles*

Main category: cs.LG

TL;DR: 本文提出基于数据分布惩罚的空间填充正则化方法，解决状态轨迹变形问题，并在局部模型网络架构中验证，还在系统识别基准上展示结果。


<details>
  <summary>Details</summary>
Motivation: 状态轨迹在训练中显著变形，导致状态空间数据覆盖差，阻碍训练，降低可解释性和鲁棒性。

Method: 提出一种新的空间填充正则化方法，引入基于数据分布的惩罚，确保状态空间中有良好的数据分布，并针对局部仿射状态空间模型的数据点分布提出两种正则化技术。

Result: 在局部模型网络架构中进行了方法验证，并在系统识别基准上展示结果。

Conclusion: 所提出的方法可解决状态轨迹变形带来的问题，确保状态空间数据分布良好。

Abstract: The state space dynamics representation is the most general approach for
nonlinear systems and often chosen for system identification. During training,
the state trajectory can deform significantly leading to poor data coverage of
the state space. This can cause significant issues for space-oriented training
algorithms which e.g. rely on grid structures, tree partitioning, or similar.
Besides hindering training, significant state trajectory deformations also
deteriorate interpretability and robustness properties. This paper proposes a
new type of space-filling regularization that ensures a favorable data
distribution in state space via introducing a data-distribution-based penalty.
This method is demonstrated in local model network architectures where good
interpretability is a major concern. The proposed approach integrates ideas
from modeling and design of experiments for state space structures. This is why
we present two regularization techniques for the data point distributions of
the state trajectories for local affine state space models. Beyond that, we
demonstrate the results on a widely known system identification benchmark.

</details>


### [127] [Deep Survival Analysis in Multimodal Medical Data: A Parametric and Probabilistic Approach with Competing Risks](https://arxiv.org/abs/2507.07804)
*Alba Garrido,Alejandro Almodóvar,Patricia A. Apellániz,Juan Parras,Santiago Zazo*

Main category: cs.LG

TL;DR: 提出SAMVAE多模态深度学习框架用于肿瘤生存分析，整合六种数据模态，在两个癌症队列评估，性能有竞争力。


<details>
  <summary>Details</summary>
Motivation: 传统单模态肿瘤生存预测方法难以捕捉肿瘤生物学复杂性，需多模态方法。

Method: 提出SAMVAE架构，利用特定模态编码器将输入投影到共享潜在空间，在两个癌症队列进行预处理、降维和超参数优化。

Result: 成功整合多模态数据用于标准生存分析和竞争风险场景，模型性能有竞争力。

Conclusion: SAMVAE是首个结合竞争风险、模拟特定事件连续时间的参数化多模态深度学习架构，为肿瘤可解释、数据驱动的生存分析奠定基础。

Abstract: Accurate survival prediction is critical in oncology for prognosis and
treatment planning. Traditional approaches often rely on a single data
modality, limiting their ability to capture the complexity of tumor biology. To
address this challenge, we introduce a multimodal deep learning framework for
survival analysis capable of modeling both single and competing risks
scenarios, evaluating the impact of integrating multiple medical data sources
on survival predictions. We propose SAMVAE (Survival Analysis Multimodal
Variational Autoencoder), a novel deep learning architecture designed for
survival prediction that integrates six data modalities: clinical variables,
four molecular profiles, and histopathological images. SAMVAE leverages
modality specific encoders to project inputs into a shared latent space,
enabling robust survival prediction while preserving modality specific
information. Its parametric formulation enables the derivation of clinically
meaningful statistics from the output distributions, providing patient-specific
insights through interactive multimedia that contribute to more informed
clinical decision-making and establish a foundation for interpretable,
data-driven survival analysis in oncology. We evaluate SAMVAE on two cancer
cohorts breast cancer and lower grade glioma applying tailored preprocessing,
dimensionality reduction, and hyperparameter optimization. The results
demonstrate the successful integration of multimodal data for both standard
survival analysis and competing risks scenarios across different datasets. Our
model achieves competitive performance compared to state-of-the-art multimodal
survival models. Notably, this is the first parametric multimodal deep learning
architecture to incorporate competing risks while modeling continuous time to a
specific event, using both tabular and image data.

</details>


### [128] [Pay Attention to Attention Distribution: A New Local Lipschitz Bound for Transformers](https://arxiv.org/abs/2507.07814)
*Nikolay Yudin,Alexander Gaponov,Sergei Kudriashov,Maxim Rakhuba*

Main category: cs.LG

TL;DR: 提出变压器自注意力块的局部Lipschitz界，引入正则化项提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 获取更准确的局部Lipschitz界，从Lipschitz常数角度解释注意力图分布对鲁棒性的影响，提升变压器鲁棒性。

Method: 基于softmax函数谱范数的精确闭式表达式得出Lipschitz界，引入JaSMin正则化项。

Result: 得到的界比现有技术更准确，揭示Lipschitz常数与注意力得分图的依赖关系，JaSMin提升了变压器鲁棒性并降低网络局部Lipschitz常数。

Conclusion: 新的Lipschitz界和JaSMin正则化项有助于提升变压器的鲁棒性。

Abstract: We present a novel local Lipschitz bound for self-attention blocks of
transformers. This bound is based on a refined closed-form expression for the
spectral norm of the softmax function. The resulting bound is not only more
accurate than in the prior art, but also unveils the dependence of the
Lipschitz constant on attention score maps. Based on the new findings, we
suggest an explanation of the way distributions inside the attention map affect
the robustness from the Lipschitz constant perspective. We also introduce a new
lightweight regularization term called JaSMin (Jacobian Softmax norm
Minimization), which boosts the transformer's robustness and decreases local
Lipschitz constants of the whole network.

</details>


### [129] [Towards Benchmarking Foundation Models for Tabular Data With Text](https://arxiv.org/abs/2507.07829)
*Martin Mráz,Breenda Das,Anshul Gupta,Lennart Purucker,Frank Hutter*

Main category: cs.LG

TL;DR: 提出融合文本到表格管道的策略，手动整理含文本特征的表格数据集对表格基础模型处理文本数据进行基准测试，推动含文本表格数据基础模型基准测试改进。


<details>
  <summary>Details</summary>
Motivation: 表格数据基础模型发展中想拓展支持文本特征，但现有表格数据基准很少含文本列，识别含语义丰富文本特征的真实表格数据集有难度。

Method: 提出一系列简单有效的消融式策略将文本融入传统表格管道，手动整理含有意义文本特征的真实表格数据集进行基准测试。

Result: 对现有表格基础模型处理文本数据的能力进行了基准测试。

Conclusion: 该研究是改进含文本表格数据基础模型基准测试的重要一步。

Abstract: Foundation models for tabular data are rapidly evolving, with increasing
interest in extending them to support additional modalities such as free-text
features. However, existing benchmarks for tabular data rarely include textual
columns, and identifying real-world tabular datasets with semantically rich
text features is non-trivial. We propose a series of simple yet effective
ablation-style strategies for incorporating text into conventional tabular
pipelines. Moreover, we benchmark how state-of-the-art tabular foundation
models can handle textual data by manually curating a collection of real-world
tabular datasets with meaningful textual features. Our study is an important
step towards improving benchmarking of foundation models for tabular data with
text.

</details>


### [130] ["So, Tell Me About Your Policy...": Distillation of interpretable policies from Deep Reinforcement Learning agents](https://arxiv.org/abs/2507.07848)
*Giovanni Dispoto,Paolo Bonetti,Marcello Restelli*

Main category: cs.LG

TL;DR: 论文提出新算法，可提取可解释策略，在经典控制环境和金融交易场景验证。


<details>
  <summary>Details</summary>
Motivation: 当前深度强化学习缺乏可解释性，在关键任务和现实场景中，需更简单可解释算法。

Method: 考虑优势函数，利用先前收集的经验训练可解释策略。

Result: 算法在经典控制环境和金融交易场景中，展现出从复杂专家策略中提取有意义信息的能力。

Conclusion: 提出的新算法能在不忽视专家行为特点的情况下提取可解释策略。

Abstract: Recent advances in Reinforcement Learning (RL) largely benefit from the
inclusion of Deep Neural Networks, boosting the number of novel approaches
proposed in the field of Deep Reinforcement Learning (DRL). These techniques
demonstrate the ability to tackle complex games such as Atari, Go, and other
real-world applications, including financial trading. Nevertheless, a
significant challenge emerges from the lack of interpretability, particularly
when attempting to comprehend the underlying patterns learned, the relative
importance of the state features, and how they are integrated to generate the
policy's output. For this reason, in mission-critical and real-world settings,
it is often preferred to deploy a simpler and more interpretable algorithm,
although at the cost of performance. In this paper, we propose a novel
algorithm, supported by theoretical guarantees, that can extract an
interpretable policy (e.g., a linear policy) without disregarding the
peculiarities of expert behavior. This result is obtained by considering the
advantage function, which includes information about why an action is superior
to the others. In contrast to previous works, our approach enables the training
of an interpretable policy using previously collected experience. The proposed
algorithm is empirically evaluated on classic control environments and on a
financial trading scenario, demonstrating its ability to extract meaningful
information from complex expert policies.

</details>


### [131] [Optimization Guarantees for Square-Root Natural-Gradient Variational Inference](https://arxiv.org/abs/2507.07853)
*Navish Kumar,Thomas Möllenhoff,Mohammad Emtiyaz Khan,Aurelien Lucchi*

Main category: cs.LG

TL;DR: 使用平方根参数化解决自然梯度变分高斯推理收敛保证难题，实验证明方法有效性。


<details>
  <summary>Details</summary>
Motivation: 自然梯度下降变分推理实践中收敛快，但理论收敛保证难建立，即使是简单情况。

Method: 对高斯协方差采用平方根参数化。

Result: 建立了自然梯度变分高斯推理及其连续时间梯度流的新收敛保证，实验证明自然梯度方法有效。

Conclusion: 自然梯度方法比使用欧几里得或 Wasserstein 几何的算法更具优势。

Abstract: Variational inference with natural-gradient descent often shows fast
convergence in practice, but its theoretical convergence guarantees have been
challenging to establish. This is true even for the simplest cases that involve
concave log-likelihoods and use a Gaussian approximation. We show that the
challenge can be circumvented for such cases using a square-root
parameterization for the Gaussian covariance. This approach establishes novel
convergence guarantees for natural-gradient variational-Gaussian inference and
its continuous-time gradient flow. Our experiments demonstrate the
effectiveness of natural gradient methods and highlight their advantages over
algorithms that use Euclidean or Wasserstein geometries.

</details>


### [132] [Credit Risk Analysis for SMEs Using Graph Neural Networks in Supply Chain](https://arxiv.org/abs/2507.07854)
*Zizhou Zhang,Qinyan Shen,Zhuohuan Hu,Qianying Liu,Huijie Shen*

Main category: cs.LG

TL;DR: 本文提出基于图神经网络（GNN）的框架用于中小企业信用风险分析，测试显示其优于传统方法，还能辅助监管和压力测试。


<details>
  <summary>Details</summary>
Motivation: 中小企业信用风险分析面临数据稀缺问题，尤其是在线贷款机构缺乏直接信用记录。

Method: 引入基于图神经网络（GNN）的框架，利用交易和社交数据中的中小企业交互来映射空间依赖关系并预测贷款违约风险。

Result: 在真实数据集上测试，GNN在供应链挖掘和违约预测的AUC分别为0.995和0.701，优于传统和其他GNN基线；能帮助监管机构建模供应链中断对银行的影响，为美联储压力测试提供关键数据。

Conclusion: 该方法为评估中小企业信用风险提供了可扩展、有效的工具。

Abstract: Small and Medium-sized Enterprises (SMEs) are vital to the modern economy,
yet their credit risk analysis often struggles with scarce data, especially for
online lenders lacking direct credit records. This paper introduces a Graph
Neural Network (GNN)-based framework, leveraging SME interactions from
transaction and social data to map spatial dependencies and predict loan
default risks. Tests on real-world datasets from Discover and Ant Credit (23.4M
nodes for supply chain analysis, 8.6M for default prediction) show the GNN
surpasses traditional and other GNN baselines, with AUCs of 0.995 and 0.701 for
supply chain mining and default prediction, respectively. It also helps
regulators model supply chain disruption impacts on banks, accurately
forecasting loan defaults from material shortages, and offers Federal Reserve
stress testers key data for CCAR risk buffers. This approach provides a
scalable, effective tool for assessing SME credit risk.

</details>


### [133] [Principled Foundations for Preference Optimization](https://arxiv.org/abs/2507.07855)
*Wenxuan Zhou,Shujian Zhang,Brice Magdalou,John Lambert,Ehsan Amid,Richard Nock,Andrew Hard*

Main category: cs.LG

TL;DR: 论文揭示直接偏好优化（DPO）是机器学习中损失函数与随机选择两大理论联系的特殊形式，并阐述该联系的特点及研究意义。


<details>
  <summary>Details</summary>
Motivation: 鉴于模型应用广泛、DPO发展势头良好且现有DPO变体占比小，需从一般原理角度理解DPO运作。

Method: 建立Savage损失函数与随机选择理论的联系。

Result: 该联系支持选择理论中的弃权、机器学习中的非凸目标，可免费构建DPO的一些扩展。

Conclusion: 理解DPO的一般原理有助于了解其运作、避免偏离问题并找到解决办法。

Abstract: In this paper, we show that direct preference optimization (DPO) is a very
specific form of a connection between two major theories in the ML context of
learning from preferences: loss functions (Savage) and stochastic choice
(Doignon-Falmagne and Machina). The connection is established for all of
Savage's losses and at this level of generality, (i) it includes support for
abstention on the choice theory side, (ii) it includes support for non-convex
objectives on the ML side, and (iii) it allows to frame for free some notable
extensions of the DPO setting, including margins and corrections for length.
Getting to understand how DPO operates from a general principled perspective is
crucial because of the huge and diverse application landscape of models,
because of the current momentum around DPO, but also -- and importantly --
because many state of the art variations on DPO definitely occupy a small
region of the map that we cover. It also helps to understand the pitfalls of
departing from this map, and figure out workarounds.

</details>


### [134] [Predicting and generating antibiotics against future pathogens with ApexOracle](https://arxiv.org/abs/2507.07862)
*Tianang Leng,Fangping Wan,Marcelo Der Torossian Torres,Cesar de la Fuente-Nunez*

Main category: cs.LG

TL;DR: 介绍AI模型ApexOracle，能预测化合物抗菌效力、设计新分子，表现优于现有方法，可应对AMR。


<details>
  <summary>Details</summary>
Motivation: 抗菌耐药性加剧，现有方法无法快速识别针对新病原体或耐药菌株的有效分子，急需新方法。

Method: 引入ApexOracle，结合基础离散扩散语言模型捕捉的分子特征和结合基因组与文献衍生菌株表示的双嵌入框架。

Result: 在不同细菌种类和化学模式中，ApexOracle在活性预测上始终优于现有方法，对新病原体有可靠的迁移能力，能生成有高预测效力的新分子。

Conclusion: ApexOracle通过快速活性预测和靶向分子生成，为应对AMR和未来传染病爆发提供可扩展策略。

Abstract: Antimicrobial resistance (AMR) is escalating and outpacing current antibiotic
development. Thus, discovering antibiotics effective against emerging pathogens
is becoming increasingly critical. However, existing approaches cannot rapidly
identify effective molecules against novel pathogens or emerging drug-resistant
strains. Here, we introduce ApexOracle, an artificial intelligence (AI) model
that both predicts the antibacterial potency of existing compounds and designs
de novo molecules active against strains it has never encountered. Departing
from models that rely solely on molecular features, ApexOracle incorporates
pathogen-specific context through the integration of molecular features
captured via a foundational discrete diffusion language model and a
dual-embedding framework that combines genomic- and literature-derived strain
representations. Across diverse bacterial species and chemical modalities,
ApexOracle consistently outperformed state-of-the-art approaches in activity
prediction and demonstrated reliable transferability to novel pathogens with
little or no antimicrobial data. Its unified representation-generation
architecture further enables the in silico creation of "new-to-nature"
molecules with high predicted efficacy against priority threats. By pairing
rapid activity prediction with targeted molecular generation, ApexOracle offers
a scalable strategy for countering AMR and preparing for future
infectious-disease outbreaks.

</details>


### [135] [Can AI-predicted complexes teach machine learning to compute drug binding affinity?](https://arxiv.org/abs/2507.07882)
*Wei-Tse Hsu,Savva Grevtsev,Thomas Douglas,Aniket Magarkar,Philip C. Biggin*

Main category: cs.LG

TL;DR: 评估使用共折叠模型进行合成数据增强在训练结合亲和力预测的机器学习评分函数的可行性，提出无参考结构识别高质量共折叠预测的启发式方法。


<details>
  <summary>Details</summary>
Motivation: 评估使用共折叠模型进行合成数据增强在训练机器学习评分函数以预测结合亲和力的可行性。

Method: 评估可行性，建立无参考结构识别高质量共折叠预测的简单启发式方法。

Result: 性能提升关键取决于增强数据的结构质量。

Conclusion: 研究为基于共折叠模型的未来数据增强策略提供了参考。

Abstract: We evaluate the feasibility of using co-folding models for synthetic data
augmentation in training machine learning-based scoring functions (MLSFs) for
binding affinity prediction. Our results show that performance gains depend
critically on the structural quality of augmented data. In light of this, we
established simple heuristics for identifying high-quality co-folding
predictions without reference structures, enabling them to substitute for
experimental structures in MLSF training. Our study informs future data
augmentation strategies based on co-folding models.

</details>


### [136] [SAMO: A Lightweight Sharpness-Aware Approach for Multi-Task Optimization with Joint Global-Local Perturbation](https://arxiv.org/abs/2507.07883)
*Hao Ban,Gokul Ram Subramani,Kaiyi Ji*

Main category: cs.LG

TL;DR: 本文探讨将SAM集成到MTL中，提出轻量级优化方法SAMO，实验证明其有效性和高效性。


<details>
  <summary>Details</summary>
Motivation: MTL存在任务冲突问题，SAM能缓解该问题，但集成SAM到MTL面临信息结合和计算开销挑战。

Method: 提出SAMO方法，利用联合全局 - 局部扰动，局部扰动仅通过前向传播近似并进行层归一化。

Result: 在多任务基准测试上的大量实验证明了SAMO方法的有效性和高效性。

Conclusion: SAMO是一种有效且高效的多任务优化方法，代码已开源。

Abstract: Multi-task learning (MTL) enables a joint model to capture commonalities
across multiple tasks, reducing computation costs and improving data
efficiency. However, a major challenge in MTL optimization is task conflicts,
where the task gradients differ in direction or magnitude, limiting model
performance compared to single-task counterparts. Sharpness-aware minimization
(SAM) minimizes task loss while simultaneously reducing the sharpness of the
loss landscape. Our empirical observations show that SAM effectively mitigates
task conflicts in MTL. Motivated by these findings, we explore integrating SAM
into MTL but face two key challenges. While both the average loss gradient and
individual task gradients-referred to as global and local
information-contribute to SAM, how to combine them remains unclear. Moreover,
directly computing each task gradient introduces significant computational and
memory overheads. To address these challenges, we propose SAMO, a lightweight
\textbf{S}harpness-\textbf{A}ware \textbf{M}ulti-task \textbf{O}ptimization
approach, that leverages a joint global-local perturbation. The local
perturbations are approximated using only forward passes and are layerwise
normalized to improve efficiency. Extensive experiments on a suite of
multi-task benchmarks demonstrate both the effectiveness and efficiency of our
method. Code is available at https://github.com/OptMN-Lab/SAMO.

</details>


### [137] [UnIT: Scalable Unstructured Inference-Time Pruning for MAC-efficient Neural Inference on MCUs](https://arxiv.org/abs/2507.07885)
*Ashe Neth,Sawinder kaur,Mohammad Nur Hossain Khan,Subrata Biswas,Asif Salekin,Bashima Islam*

Main category: cs.LG

TL;DR: 提出UnIT方法在推理时动态剪枝，在MSP430微控制器上实验效果好，证明无结构推理时剪枝可行。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法多依赖结构化稀疏，在无SIMD支持或并行计算设备上无法充分实现细粒度效率，需改进。

Method: 引入UnIT方法，根据输入特定激活模式动态识别并跳过不必要MAC操作，将剪枝决策转化为轻量级比较，提出三种适合嵌入式平台的除法近似。

Result: 在MSP430微控制器上，UnIT相比训练时剪枝模型，MAC减少11.02% - 82.03%，推理加速27.30% - 84.19%，能耗降低27.33% - 84.38%，精度损失0.48 - 7%，领域偏移下精度表现好。

Conclusion: 无结构推理时剪枝是在微控制器上高效、无需重新训练部署深度神经网络的可行方案。

Abstract: Existing pruning methods are typically applied during training or compile
time and often rely on structured sparsity. While compatible with low-power
microcontrollers (MCUs), structured pruning underutilizes the opportunity for
fine-grained efficiency on devices without SIMD support or parallel compute. To
address these limitations, we introduce UnIT (Unstructured Inference-Time
pruning), a lightweight method that dynamically identifies and skips
unnecessary multiply-accumulate (MAC) operations during inference, guided by
input-specific activation patterns. Unlike structured pruning, UnIT embraces
irregular sparsity and does not require retraining or hardware specialization.
It transforms pruning decisions into lightweight comparisons, replacing
multiplications with threshold checks and approximated divisions. UnIT further
optimizes compute by reusing threshold computations across multiple connections
and applying layer- and group-specific pruning sensitivity. We present three
fast, hardware-friendly division approximations tailored to the capabilities of
common embedded platforms. Demonstrated on the MSP430 microcontroller, UnIT
achieves 11.02% to 82.03% MAC reduction, 27.30% to 84.19% faster inference, and
27.33% to 84.38% lower energy consumption compared to training-time pruned
models, while maintaining accuracy with 0.48-7%. Under domain shift, UnIT
matches or exceeds the accuracy of retrained models while requiring
significantly fewer MACs. These results establish unstructured inference-time
pruning as a viable and practical solution for efficient, retraining-free
deployment of deep neural networks on MCUs.

</details>


### [138] [Efficient Causal Discovery for Autoregressive Time Series](https://arxiv.org/abs/2507.07898)
*Mohammad Fesanghary,Achintya Gopal*

Main category: cs.LG

TL;DR: 提出用于非线性自回归时间序列因果结构学习的新算法，降低复杂度，在合成数据集上表现优，有实际应用潜力。


<details>
  <summary>Details</summary>
Motivation: 为非线性自回归时间序列的因果结构学习提供更高效、可扩展的算法。

Method: 提出一种基于约束的新算法。

Result: 在合成数据集上评估显示，新算法优于现有技术，在数据有限时表现出色。

Conclusion: 该算法在需要从非线性时间序列数据进行高效准确因果推断的领域有实际应用潜力。

Abstract: In this study, we present a novel constraint-based algorithm for causal
structure learning specifically designed for nonlinear autoregressive time
series. Our algorithm significantly reduces computational complexity compared
to existing methods, making it more efficient and scalable to larger problems.
We rigorously evaluate its performance on synthetic datasets, demonstrating
that our algorithm not only outperforms current techniques, but also excels in
scenarios with limited data availability. These results highlight its potential
for practical applications in fields requiring efficient and accurate causal
inference from nonlinear time series data.

</details>


### [139] [Agentic Retrieval of Topics and Insights from Earnings Calls](https://arxiv.org/abs/2507.07906)
*Anant Gupta,Rajarshi Bhowmik,Geoffrey Gunow*

Main category: cs.LG

TL;DR: 提出用LLM - agent驱动方法从季度财报电话会议中发现和检索新兴主题，并进行评估。


<details>
  <summary>Details</summary>
Motivation: 传统主题建模技术难以动态捕捉新兴主题及其关系，需要新方法来追踪公司战略重点。

Method: 提出LLM - agent从文档中提取主题，将其构建为分层本体，并通过主题本体建立新老主题关系。

Result: 用提取的主题推断公司层面的见解和随时间变化的新兴趋势，并通过测量本体一致性、主题演变准确性等进行评估。

Conclusion: 未明确提及，但表明该方法在发现新兴主题、推断趋势等方面可能有效。

Abstract: Tracking the strategic focus of companies through topics in their earnings
calls is a key task in financial analysis. However, as industries evolve,
traditional topic modeling techniques struggle to dynamically capture emerging
topics and their relationships. In this work, we propose an LLM-agent driven
approach to discover and retrieve emerging topics from quarterly earnings
calls. We propose an LLM-agent to extract topics from documents, structure them
into a hierarchical ontology, and establish relationships between new and
existing topics through a topic ontology. We demonstrate the use of extracted
topics to infer company-level insights and emerging trends over time. We
evaluate our approach by measuring ontology coherence, topic evolution
accuracy, and its ability to surface emerging financial trends.

</details>


### [140] [Low Resource Reconstruction Attacks Through Benign Prompts](https://arxiv.org/abs/2507.07947)
*Sol Yarkoni,Roi Livni*

Main category: cs.LG

TL;DR: 近期生成模型有隐私等风险，此前重建训练集图像技术资源需求高，本文提出低资源、少依赖训练集的新攻击方法，还举例说明普通提示词可能有风险。


<details>
  <summary>Details</summary>
Motivation: 为更好理解和控制生成模型在隐私、版权等方面的风险，改进现有高资源需求的图像重建技术。

Method: 基于前人工作的直觉，利用领域知识，识别因电商平台数据使用导致的基础漏洞，找出能引发潜在风险图像重建的普通提示词。

Result: 以现有模型为例，发现如“蓝色中性T恤”这样看似良性的提示词可生成真实人脸图像。

Conclusion: 新攻击方法揭示即使不知情用户也可能无意重建图像，强调生成模型潜在风险。

Abstract: The recent advances in generative models such as diffusion models have raised
several risks and concerns related to privacy, copyright infringements and data
stewardship. To better understand and control the risks, various researchers
have created techniques, experiments and attacks that reconstruct images, or
part of images, from the training set. While these techniques already establish
that data from the training set can be reconstructed, they often rely on
high-resources, excess to the training set as well as well-engineered and
designed prompts.
  In this work, we devise a new attack that requires low resources, assumes
little to no access to the actual training set, and identifies, seemingly,
benign prompts that lead to potentially-risky image reconstruction. This
highlights the risk that images might even be reconstructed by an uninformed
user and unintentionally. For example, we identified that, with regard to one
existing model, the prompt ``blue Unisex T-Shirt'' can generate the face of a
real-life human model. Our method builds on an intuition from previous works
which leverages domain knowledge and identifies a fundamental vulnerability
that stems from the use of scraped data from e-commerce platforms, where
templated layouts and images are tied to pattern-like prompts.

</details>


### [141] [Dynamic Chunking for End-to-End Hierarchical Sequence Modeling](https://arxiv.org/abs/2507.07955)
*Sukjun Hwang,Brandon Wang,Albert Gu*

Main category: cs.LG

TL;DR: 介绍新技巧实现动态分块机制，构建H - Net全端到端模型，性能优于Transformer，在多方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型中分词等预处理步骤阻碍了真正的端到端基础模型发展。

Method: 引入新技巧实现动态分块机制，将其融入显式分层网络H - Net。

Result: 单级H - Net在字节级表现优于基于BPE标记的Transformer；多级迭代提升性能；在英语上鲁棒性增强；在中文、代码、DNA序列等方面数据效率提升。

Conclusion: H - Net展示了真正端到端模型从原始数据学习和扩展的潜力。

Abstract: Despite incredible progress in language models (LMs) in recent years, largely
resulting from moving away from specialized models designed for specific tasks
to general models based on powerful architectures (e.g. the Transformer) that
learn everything from raw data, pre-processing steps such as tokenization
remain a barrier to true end-to-end foundation models. We introduce a
collection of new techniques that enable a dynamic chunking mechanism which
automatically learns content -- and context -- dependent segmentation
strategies learned jointly with the rest of the model. Incorporating this into
an explicit hierarchical network (H-Net) allows replacing the (implicitly
hierarchical) tokenization-LM-detokenization pipeline with a single model
learned fully end-to-end. When compute- and data- matched, an H-Net with one
stage of hierarchy operating at the byte level outperforms a strong Transformer
language model operating over BPE tokens. Iterating the hierarchy to multiple
stages further increases its performance by modeling multiple levels of
abstraction, demonstrating significantly better scaling with data and matching
a token-based Transformer of twice its size. H-Nets pretrained on English show
significantly increased character-level robustness, and qualitatively learn
meaningful data-dependent chunking strategies without any heuristics or
explicit supervision. Finally, the H-Net's improvement over tokenized pipelines
is further increased in languages and modalities with weaker tokenization
heuristics, such as Chinese and code, or DNA sequences (nearly 4x improvement
in data efficiency over baselines), showing the potential of true end-to-end
models that learn and scale better from unprocessed data.

</details>


### [142] [EXPO: Stable Reinforcement Learning with Expressive Policies](https://arxiv.org/abs/2507.07986)
*Perry Dong,Qiyang Li,Dorsa Sadigh,Chelsea Finn*

Main category: cs.LG

TL;DR: 研究在离线数据集上用在线强化学习训练和微调表达性策略的问题，提出EXPO算法，相比先前方法样本效率平均提升2 - 3倍。


<details>
  <summary>Details</summary>
Motivation: 训练表达性策略时存在稳定值最大化的挑战，因其参数化的长去噪链阻碍稳定梯度传播。

Method: 提出Expressive Policy Optimization (EXPO)算法，利用即时策略，结合表达性基础策略和轻量级高斯编辑策略，对基础策略动作优化并选择值最大动作进行采样和TD备份。

Result: 在微调预训练策略和利用离线数据在线训练场景中，样本效率相比先前方法平均提升2 - 3倍。

Conclusion: EXPO算法能有效解决表达性策略训练和微调中的稳定值最大化问题，提升样本效率。

Abstract: We study the problem of training and fine-tuning expressive policies with
online reinforcement learning (RL) given an offline dataset. Training
expressive policy classes with online RL present a unique challenge of stable
value maximization. Unlike simpler Gaussian policies commonly used in online
RL, expressive policies like diffusion and flow-matching policies are
parameterized by a long denoising chain, which hinders stable gradient
propagation from actions to policy parameters when optimizing against some
value function. Our key insight is that we can address stable value
maximization by avoiding direct optimization over value with the expressive
policy and instead construct an on-the-fly RL policy to maximize Q-value. We
propose Expressive Policy Optimization (EXPO), a sample-efficient online RL
algorithm that utilizes an on-the-fly policy to maximize value with two
parameterized policies -- a larger expressive base policy trained with a stable
imitation learning objective and a light-weight Gaussian edit policy that edits
the actions sampled from the base policy toward a higher value distribution.
The on-the-fly policy optimizes the actions from the base policy with the
learned edit policy and chooses the value maximizing action from the base and
edited actions for both sampling and temporal-difference (TD) backup. Our
approach yields up to 2-3x improvement in sample efficiency on average over
prior methods both in the setting of fine-tuning a pretrained policy given
offline data and in leveraging offline data to train online.

</details>


### [143] [Skip a Layer or Loop it? Test-Time Depth Adaptation of Pretrained LLMs](https://arxiv.org/abs/2507.07996)
*Ziyue Li,Yang Li,Tianyi Zhou*

Main category: cs.LG

TL;DR: 本文提出可操纵预训练大语言模型的层构建定制模型CoLa，用MCTS搜索最优结构，发现其可提升推理效率和性能。


<details>
  <summary>Details</summary>
Motivation: 探究预训练神经网络能否不微调就适应不同输入，以及是否所有层都适用于不同任务。

Method: 将预训练模型层作为独立模块，可跳过、重复或任意堆叠形成CoLa，用蒙特卡罗树搜索（MCTS）为每个样本探索最优CoLa。

Result: 75%原模型正确预测的样本可找到更短CoLa；60%原模型错误预测的样本可找到能正确预测的CoLa。

Conclusion: 使用固定架构的预训练大模型进行推理有不足，测试时深度自适应可释放模型泛化能力。

Abstract: Can a pretrained neural network adapt its architecture to different inputs
without any finetuning? Do we need all layers for simple tasks, and are they
adequate for challenging tasks? We found that the layers of a pretrained large
language model (LLM) can be manipulated as separate modules to build a better
and even shallower model customized for each test sample. In particular, each
layer from the pretrained model can be skipped/pruned or repeated multiple
times as recurrent neural networks (RNN), and stacked with others in arbitrary
orders, yielding a chain-of-layers (CoLa) per sample. This compositional space
greatly expands the scope of existing works on looped/recurrent pretrained
modules, layer pruning, or early-exit networks. We develop a Monte Carlo Tree
Search (MCTS) protocol to explore and identify the optimal CoLa for each sample
from math and commonsense reasoning benchmarks. Compared to a static model of a
fixed depth, CoLa allows shortcut paths (fast thinking), recurrence of the same
layer(s) (slow thinking), and combining both, offering more flexible, dynamic
architectures for different inputs. We conduct an extensive analysis of the
MCTS-optimized CoLa, which leads to two key findings: (1) For >75% of samples
with correct predictions by the original LLM, we can find shorter CoLa,
suggesting a large space for improving inference efficiency; (2) For >60% of
samples with originally incorrect predictions, we can identify CoLa achieving
correct predictions, suggesting a large space of performance enhancement. Our
results highlight the shortcomings of using a fixed architecture of pre-trained
LLMs for inference on different samples and pave the way to unlock the
generalization power of test-time depth adaptation.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [144] [A Robust, Open-Source Framework for Spiking Neural Networks on Low-End FPGAs](https://arxiv.org/abs/2507.07284)
*Andrew Fan,Simon D. Levy*

Main category: cs.NE

TL;DR: 本文提出基于低资源FPGA的SNN加速框架，实现MNIST数字识别，代码开源。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络算力需求大，SNN有潜力但相关芯片难获取，现有FPGA架构有局限。

Method: 提出含SNN加速架构和Pytorch编译器的框架，用突触阵列传播脉冲，针对低端FPGA。

Result: 在低端FPGA上测试，MNIST识别速度有竞争力，能准确模拟SNN。

Conclusion: 该框架可有效加速SNN，在低端FPGA上也有良好表现。

Abstract: As the demand for compute power in traditional neural networks has increased
significantly, spiking neural networks (SNNs) have emerged as a potential
solution to increasingly power-hungry neural networks. By operating on 0/1
spikes emitted by neurons instead of arithmetic multiply-and-accumulate
operations, SNNs propagate information temporally and spatially, allowing for
more efficient compute power. To this end, many architectures for accelerating
and simulating SNNs have been developed, including Loihi, TrueNorth, and
SpiNNaker. However, these chips are largely inaccessible to the wider
community. Field programmable gate arrays (FPGAs) have been explored to serve
as a middle ground between neuromorphic and non-neuromorphic hardware, but many
proposed architectures require expensive high-end FPGAs or target a single SNN
topology. This paper presents a framework consisting of a robust SNN
acceleration architecture and a Pytorch-based SNN model compiler. Targeting
any-to-any and/or fully connected SNNs, the FPGA architecture features a
synaptic array that tiles across the SNN to propagate spikes. The architecture
targets low-end FPGAs and requires very little (6358 LUT, 40.5 BRAM) resources.
The framework, tested on a low-end Xilinx Artix-7 FPGA at 100 MHz, achieves
competitive speed in recognizing MNIST digits (0.52 ms/img). Further
experiments also show accurate simulation of hand coded any-to-any spiking
neural networks on toy problems. All code and setup instructions are available
at
https://github.com/im-afan/snn-fpga}{\texttt{https://github.com/im-afan/snn-fpga.

</details>


### [145] [Homeostatic Adaptation of Optimal Population Codes under Metabolic Stress](https://arxiv.org/abs/2507.07874)
*Yi-Chun Hung,Gregory Schwartz,Emily A. Cooper,Emma Alexander*

Main category: cs.NE

TL;DR: 提出理论群体编码框架，用新约束条件捕捉神经元在代谢压力下行为，推导最优编码策略。


<details>
  <summary>Details</summary>
Motivation: 现有数学模型无法准确描述受代谢资源限制和噪声影响的神经群体信息处理，近期数据显示神经元有‘低功耗模式’。

Method: 开发理论群体编码框架，用近似的 firing rate homeostasis 和与噪声水平相关的能量限制两个新约束条件，模拟得到能量依赖的分散泊松噪声模型。

Result: 推导不同能量预算和编码目标下神经元的最优编码策略，方法能捕捉群体调谐曲线在维持稳态时的适应性。

Conclusion: 所提出的框架可推广现有最优群体编码，解释细胞中 ATP 使用，能准确描述神经群体在代谢压力下的行为。

Abstract: Information processing in neural populations is inherently constrained by
metabolic resource limits and noise properties, with dynamics that are not
accurately described by existing mathematical models. Recent data, for example,
shows that neurons in mouse visual cortex go into a "low power mode" in which
they maintain firing rate homeostasis while expending less energy. This
adaptation leads to increased neuronal noise and tuning curve flattening in
response to metabolic stress. We have developed a theoretical population coding
framework that captures this behavior using two novel, surprisingly simple
constraints: an approximation of firing rate homeostasis and an energy limit
tied to noise levels via biophysical simulation. A key feature of our
contribution is an energy budget model directly connecting adenosine
triphosphate (ATP) use in cells to a fully explainable mathematical framework
that generalizes existing optimal population codes. Specifically, our
simulation provides an energy-dependent dispersed Poisson noise model, based on
the assumption that the cell will follow an optimal decay path to produce the
least-noisy spike rate that is possible at a given cellular energy budget. Each
state along this optimal path is associated with properties (resting potential
and leak conductance) which can be measured in electrophysiology experiments
and have been shown to change under prolonged caloric deprivation. We
analytically derive the optimal coding strategy for neurons under varying
energy budgets and coding goals, and show how our method uniquely captures how
populations of tuning curves adapt while maintaining homeostasis, as has been
observed empirically.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [146] [A German Gold-Standard Dataset for Sentiment Analysis in Software Engineering](https://arxiv.org/abs/2507.07325)
*Martin Obaidi,Marc Herrmann,Elisa Schmid,Raymond Ochsner,Kurt Schneider,Jil Klünder*

Main category: cs.SE

TL;DR: 本文引入德国开发者语句数据集，用于德语软件工程社区情感分析，评估表明数据集有效且现有工具缺乏特定领域解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有软件工程情感分析工具主要依赖英语或非德语数据集，为填补德语数据空白。

Method: 从德国开发者论坛提取5949条独特开发者语句，由四名德语计算机科学学生依据情感模型进行六种基本情绪标注。

Result: 标注过程显示出高评分者间一致性和可靠性，现有德语情感分析工具缺乏软件工程特定领域解决方案。

Conclusion: 该数据集足以支持德语软件工程社区的情感分析，并讨论了优化标注方法和数据集的进一步用例。

Abstract: Sentiment analysis is an essential technique for investigating the emotional
climate within developer teams, contributing to both team productivity and
project success. Existing sentiment analysis tools in software engineering
primarily rely on English or non-German gold-standard datasets. To address this
gap, our work introduces a German dataset of 5,949 unique developer statements,
extracted from the German developer forum Android-Hilfe.de. Each statement was
annotated with one of six basic emotions, based on the emotion model by Shaver
et al., by four German-speaking computer science students. Evaluation of the
annotation process showed high interrater agreement and reliability. These
results indicate that the dataset is sufficiently valid and robust to support
sentiment analysis in the German-speaking software engineering community.
Evaluation with existing German sentiment analysis tools confirms the lack of
domain-specific solutions for software engineering. We also discuss approaches
to optimize annotation and present further use cases for the dataset.

</details>


### [147] [Automatic Generation of Explainability Requirements and Software Explanations From User Reviews](https://arxiv.org/abs/2507.07344)
*Martin Obaidi,Jannik Fischbach,Jakob Droste,Hannah Deters,Marc Herrmann,Jil Klünder,Steffen Krätzig,Hugo Villamizar,Kurt Schneider*

Main category: cs.SE

TL;DR: 文章介绍自动从用户反馈推导可解释性需求并生成对应解释的方法，评估显示AI生成有优劣，强调人工验证重要性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以将用户反馈的解释需求转化为结构化需求和对应解释，缺乏系统的推导和生成方法。

Method: 引入工具支持的自动化方法，与工业自动化制造商合作创建含58条用户评论的数据集进行评估。

Result: AI生成的需求与人类创建的相比缺乏相关性和正确性，但其生成的解释因清晰度和风格常受青睐，但正确性仍有问题。

Conclusion: 工作推动了软件系统可解释性需求发展，介绍自动方法、提供经验见解并发布数据集。

Abstract: Explainability has become a crucial non-functional requirement to enhance
transparency, build user trust, and ensure regulatory compliance. However,
translating explanation needs expressed in user feedback into structured
requirements and corresponding explanations remains challenging. While existing
methods can identify explanation-related concerns in user reviews, there is no
established approach for systematically deriving requirements and generating
aligned explanations. To contribute toward addressing this gap, we introduce a
tool-supported approach that automates this process. To evaluate its
effectiveness, we collaborated with an industrial automation manufacturer to
create a dataset of 58 user reviews, each annotated with manually crafted
explainability requirements and explanations. Our evaluation shows that while
AI-generated requirements often lack relevance and correctness compared to
human-created ones, the AI-generated explanations are frequently preferred for
their clarity and style. Nonetheless, correctness remains an issue,
highlighting the importance of human validation. This work contributes to the
advancement of explainability requirements in software systems by (1)
introducing an automated approach to derive requirements from user reviews and
generate corresponding explanations, (2) providing empirical insights into the
strengths and limitations of automatically generated artifacts, and (3)
releasing a curated dataset to support future research on the automatic
generation of explainability requirements.

</details>


### [148] [Towards an Engineering Workflow Management System for Asset Administration Shells using BPMN](https://arxiv.org/abs/2507.07468)
*Sten Grüner,Nafise Eskandani*

Main category: cs.SE

TL;DR: 探讨AAS在工程工作流中的应用，提出分布式AAS写时复制基础设施和工作流管理原型。


<details>
  <summary>Details</summary>
Motivation: 将工业4.0技术集成到工程工作流，实现工厂和过程工程流程自动化和优化，利用AAS创建可互操作数字双胞胎。

Method: 提出分布式AAS写时复制基础设施，引入工作流管理原型。

Result: 增强了安全性和可扩展性，实现无缝跨组织协作，提高了效率和可追溯性。

Conclusion: AAS结合BPMN可定义结构化和自动化流程，所提方法能促进工程工作流发展。

Abstract: The integration of Industry 4.0 technologies into engineering workflows is an
essential step toward automating and optimizing plant and process engineering
processes. The Asset Administration Shell (AAS) serves as a key enabler for
creating interoperable Digital Twins that facilitate engineering data exchange
and automation. This paper explores the use of AAS within engineering
workflows, particularly in combination with Business Process Model and Notation
(BPMN) to define structured and automated processes. We propose a distributed
AAS copy-on-write infrastructure that enhances security and scalability while
enabling seamless cross organizational collaboration. We also introduce a
workflow management prototype automating AAS operations and engineering
workflows, improving efficiency and traceability.

</details>


### [149] [From Requirements to Code: Understanding Developer Practices in LLM-Assisted Software Engineering](https://arxiv.org/abs/2507.07548)
*Jonathan Ullrich,Matthias Koch,Andreas Vogelsang*

Main category: cs.SE

TL;DR: 通过访谈从业者，研究开发者使用大语言模型进行代码生成时如何利用需求信息，提出理论，强调需求工程工作仍必要。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型能否仅根据需求生成高质量代码这一愿景的可行性，因该话题研究较少。

Method: 对14家公司的18名从业者进行访谈。

Result: 提出理论，表明通常记录的需求太抽象，需手动分解为编程任务，再添加设计决策和架构约束后用于提示。

Conclusion: 使用大语言模型生成代码时，基础的需求工程工作仍必要，该理论有助于为自动化以需求为中心的软件工程任务的科学方法提供背景。

Abstract: With the advent of generative LLMs and their advanced code generation
capabilities, some people already envision the end of traditional software
engineering, as LLMs may be able to produce high-quality code based solely on
the requirements a domain expert feeds into the system. The feasibility of this
vision can be assessed by understanding how developers currently incorporate
requirements when using LLMs for code generation-a topic that remains largely
unexplored. We interviewed 18 practitioners from 14 companies to understand how
they (re)use information from requirements and other design artifacts to feed
LLMs when generating code. Based on our findings, we propose a theory that
explains the processes developers employ and the artifacts they rely on. Our
theory suggests that requirements, as typically documented, are too abstract
for direct input into LLMs. Instead, they must first be manually decomposed
into programming tasks, which are then enriched with design decisions and
architectural constraints before being used in prompts. Our study highlights
that fundamental RE work is still necessary when LLMs are used to generate
code. Our theory is important for contextualizing scientific approaches to
automating requirements-centric SE tasks.

</details>


### [150] [Prompt Engineering for Requirements Engineering: A Literature Review and Roadmap](https://arxiv.org/abs/2507.07682)
*Kaicheng Huang,Fanyu Wang,Yutan Huang,Chetan Arora*

Main category: cs.SE

TL;DR: 对需求工程的提示工程（PE4RE）进行了系统文献综述，提出混合分类法和逐步路线图。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在需求工程领域应用存在不确定性和缺乏可控性，缺少有效提示的明确指导，阻碍其可靠实施。

Method: 遵循Kitchenham和Petersen的二次研究协议，搜索六个数字图书馆，筛选867条记录，分析35项主要研究。

Result: 提出混合分类法，将技术导向模式与任务导向的需求工程角色联系起来；通过两个研究问题及五个子问题揭示当前局限性和研究差距。

Conclusion: 给出逐步路线图，使临时的PE原型能演变为可重复、对从业者友好的工作流程。

Abstract: Advancements in large language models (LLMs) have led to a surge of prompt
engineering (PE) techniques that can enhance various requirements engineering
(RE) tasks. However, current LLMs are often characterized by significant
uncertainty and a lack of controllability. This absence of clear guidance on
how to effectively prompt LLMs acts as a barrier to their trustworthy
implementation in the RE field. We present the first roadmap-oriented
systematic literature review of Prompt Engineering for RE (PE4RE). Following
Kitchenham's and Petersen's secondary-study protocol, we searched six digital
libraries, screened 867 records, and analyzed 35 primary studies. To bring
order to a fragmented landscape, we propose a hybrid taxonomy that links
technique-oriented patterns (e.g., few-shot, Chain-of-Thought) to task-oriented
RE roles (elicitation, validation, traceability). Two research questions, with
five sub-questions, map the tasks addressed, LLM families used, and prompt
types adopted, and expose current limitations and research gaps. Finally, we
outline a step-by-step roadmap showing how today's ad-hoc PE prototypes can
evolve into reproducible, practitioner-friendly workflows.

</details>


### [151] [From Domain Documents to Requirements: Retrieval-Augmented Generation in the Space Industry](https://arxiv.org/abs/2507.07689)
*Chetan Arora,Fanyu Wang,Chakkrit Tantithamthavorn,Aldeida Aleti,Shaun Kenyon*

Main category: cs.SE

TL;DR: 本文探讨用RAG模型支持航天领域需求生成，提出模块化AI驱动方法，应用于实际任务文档，初步结果良好并规划AI集成路线图。


<details>
  <summary>Details</summary>
Motivation: 航天行业需求工程复杂，小型航天组织和新进入者难从大量非结构化文档获取可执行需求。

Method: 提出模块化AI驱动方法，对原始航天任务文档预处理、分类、检索相关内容，用大语言模型合成需求草案。

Result: 初步结果显示该方法可减少人工、提高需求覆盖、支持轻量级合规对齐。

Conclusion: 规划了AI在需求工程工作流中更广泛集成的路线图，旨在降低小型组织参与大型安全关键任务的门槛。

Abstract: Requirements engineering (RE) in the space industry is inherently complex,
demanding high precision, alignment with rigorous standards, and adaptability
to mission-specific constraints. Smaller space organisations and new entrants
often struggle to derive actionable requirements from extensive, unstructured
documents such as mission briefs, interface specifications, and regulatory
standards. In this innovation opportunity paper, we explore the potential of
Retrieval-Augmented Generation (RAG) models to support and (semi-)automate
requirements generation in the space domain. We present a modular, AI-driven
approach that preprocesses raw space mission documents, classifies them into
semantically meaningful categories, retrieves contextually relevant content
from domain standards, and synthesises draft requirements using large language
models (LLMs). We apply the approach to a real-world mission document from the
space domain to demonstrate feasibility and assess early outcomes in
collaboration with our industry partner, Starbound Space Solutions. Our
preliminary results indicate that the approach can reduce manual effort,
improve coverage of relevant requirements, and support lightweight compliance
alignment. We outline a roadmap toward broader integration of AI in RE
workflows, intending to lower barriers for smaller organisations to participate
in large-scale, safety-critical missions.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [152] [Machine Learning Enhanced Multi-Factor Quantitative Trading: A Cross-Sectional Portfolio Optimization Approach with Bias Correction](https://arxiv.org/abs/2507.07107)
*Yimin Du*

Main category: q-fin.PM

TL;DR: 本文提出用于量化交易的机器学习框架，结合多因子发现与偏差校正技术，在中国A股市场验证表现优异。


<details>
  <summary>Details</summary>
Motivation: 构建一个能实现卓越风险调整回报的量化交易框架。

Method: 整合多因子alpha发现与偏差校正技术，利用PyTorch加速因子计算和高级投资组合优化，采用张量因子计算加速、几何布朗运动数据增强和横截面中性化策略。

Result: 在中国A股市场（2010 - 2024）实现20%的年化回报率，夏普比率超2.0，显著优于传统方法。

Conclusion: 因子构建中偏差校正至关重要，横截面投资组合优化对策略表现有重大影响。

Abstract: This paper presents a comprehensive machine learning framework for
quantitative trading that achieves superior risk-adjusted returns through
systematic factor engineering, real-time computation optimization, and
cross-sectional portfolio construction. Our approach integrates multi-factor
alpha discovery with bias correction techniques, leveraging PyTorch-accelerated
factor computation and advanced portfolio optimization. The system processes
500-1000 factors derived from open-source alpha101 extensions and proprietary
market microstructure signals. Key innovations include tensor-based factor
computation acceleration, geometric Brownian motion data augmentation, and
cross-sectional neutralization strategies. Empirical validation on Chinese
A-share markets (2010-2024) demonstrates annualized returns of $20\%$ with
Sharpe ratios exceeding 2.0, significantly outperforming traditional
approaches. Our analysis reveals the critical importance of bias correction in
factor construction and the substantial impact of cross-sectional portfolio
optimization on strategy performance. Code and experimental implementations are
available at: https://github.com/initial-d/ml-quant-trading

</details>


### [153] [Variable annuities: A closer look at ratchet guarantees, hybrid contract designs, and taxation](https://arxiv.org/abs/2507.07358)
*Jennifer Alonso-Garcia,Len Patrick Dominic M. Garces,Jonathan Ziveyi*

Main category: q-fin.PM

TL;DR: 研究含保证最低提款利益附加条款、税收和棘轮机制的可变年金合同中保单持有人的最优提款策略和行为，揭示税收、现金基金和利益基础更新机制间的相互作用。


<details>
  <summary>Details</summary>
Motivation: 探究含特定条款和机制的可变年金合同中保单持有人的最优提款策略及行为，以及相关因素间的相互作用。

Method: 通过求解与优化合同现金流的折现风险中性期望相关的反向动态规划问题。

Result: 高税率时现金基金的税收屏蔽效应增强合同吸引力；棘轮利益基础更新方案抑制提前退保；现金基金抑制主动提款。

Conclusion: 税收、现金基金和利益基础更新机制间存在显著相互作用，影响保单持有人行为和合同吸引力。

Abstract: This paper investigates optimal withdrawal strategies and behavior of
policyholders in a variable annuity (VA) contract with a guaranteed minimum
withdrawal benefit (GMWB) rider incorporating taxation and a ratchet mechanism
for enhancing the benefit base during the life of the contract. Mathematically,
this is accomplished by solving a backward dynamic programming problem
associated with optimizing the discounted risk-neutral expectation of cash
flows from the contract. Furthermore, reflecting traded VA contracts in the
market, we consider hybrid products providing policyholders access to a cash
fund which functions as an intermediate repository of earnings from the VA and
earns interest at a contractually specified cash rate. We contribute to the
literature by revealing several significant interactions among taxation, the
cash fund, and the benefit base update mechanism. When tax rates are high, the
tax-shielding effect of the cash fund, which is taxed differently from ordinary
withdrawals from the VA, plays a significant role in enhancing the
attractiveness of the overall contract. Furthermore, the ratchet benefit base
update scheme (in contrast to the ubiquitous return-of-premium specification in
the literature) tends to discourage early surrender as it provides enhanced
downside market risk protection. In addition, the cash fund discourages active
withdrawals, with policyholders preferring to transfer the guaranteed
withdrawal amount to the cash fund to leverage the cash fund rate.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [154] [Class conditional conformal prediction for multiple inputs by p-value aggregation](https://arxiv.org/abs/2507.07150)
*Jean-Baptiste Fermanian,Mohamed Hebiri,Joseph Salmon*

Main category: stat.ML

TL;DR: 本文为分类任务改进了共形预测方法，适用于单实例多观测场景，在模拟和真实数据上评估了方法。


<details>
  <summary>Details</summary>
Motivation: 受公民科学应用启发，如同一动植物有多个图像，需在多观测场景优化共形预测方法。

Method: 将多观测信息集成到共形预测中，基于多输入各观测计算的共形p值聚合，提出通用聚合框架。

Result: 在模拟和真实数据（特别是Pl@ntNet平台数据）上进行了评估。

Conclusion: 该方法能在保留所需类条件覆盖保证的同时，减小预测标签集的大小。

Abstract: Conformal prediction methods are statistical tools designed to quantify
uncertainty and generate predictive sets with guaranteed coverage
probabilities. This work introduces an innovative refinement to these methods
for classification tasks, specifically tailored for scenarios where multiple
observations (multi-inputs) of a single instance are available at prediction
time. Our approach is particularly motivated by applications in citizen
science, where multiple images of the same plant or animal are captured by
individuals. Our method integrates the information from each observation into
conformal prediction, enabling a reduction in the size of the predicted label
set while preserving the required class-conditional coverage guarantee. The
approach is based on the aggregation of conformal p-values computed from each
observation of a multi-input. By exploiting the exact distribution of these
p-values, we propose a general aggregation framework using an abstract scoring
function, encompassing many classical statistical tools. Knowledge of this
distribution also enables refined versions of standard strategies, such as
majority voting. We evaluate our method on simulated and real data, with a
particular focus on Pl@ntNet, a prominent citizen science platform that
facilitates the collection and identification of plant species through
user-submitted images.

</details>


### [155] [Bayesian Double Descent](https://arxiv.org/abs/2507.07338)
*Nick Polson,Vadim Sokolov*

Main category: stat.ML

TL;DR: 从贝叶斯视角解读过参数统计模型的双重下降现象，并用神经网络示例说明，最后给出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 从贝叶斯视角看待双重下降现象。

Method: 对过参数模型风险特性分析，以神经网络的贝叶斯模型选择为例进行说明。

Result: 发现双重下降现象有自然的贝叶斯解释，且与贝叶斯模型的奥卡姆剃刀原则不冲突。

Conclusion: 给出未来研究方向。

Abstract: Double descent is a phenomenon of over-parameterized statistical models. Our
goal is to view double descent from a Bayesian perspective. Over-parameterized
models such as deep neural networks have an interesting re-descending property
in their risk characteristics. This is a recent phenomenon in machine learning
and has been the subject of many studies. As the complexity of the model
increases, there is a U-shaped region corresponding to the traditional
bias-variance trade-off, but then as the number of parameters equals the number
of observations and the model becomes one of interpolation, the risk can become
infinite and then, in the over-parameterized region, it re-descends -- the
double descent effect. We show that this has a natural Bayesian interpretation.
Moreover, we show that it is not in conflict with the traditional Occam's razor
that Bayesian models possess, in that they tend to prefer simpler models when
possible. We illustrate the approach with an example of Bayesian model
selection in neural networks. Finally, we conclude with directions for future
research.

</details>


### [156] [Topological Machine Learning with Unreduced Persistence Diagrams](https://arxiv.org/abs/2507.07156)
*Nicole Abreu,Parker B. Edwards,Francis Motta*

Main category: stat.ML

TL;DR: 引入从非约简边界矩阵生成拓扑特征向量的方法，对比其与全约简持久图矢量化的性能，发现非约简图训练的模型在部分任务上表现佳。


<details>
  <summary>Details</summary>
Motivation: 有实验发现基于持久同调特征训练的监督机器学习管道会忽略持久图的很多信息，且计算持久图是计算量最大的步骤，为探索此现象展开研究。

Method: 引入从非约简边界矩阵生成拓扑特征向量的方法，并对比不同数据和任务类型下非约简持久图矢量化与全约简持久图矢量化训练的管道性能。

Result: 基于非约简图构建的持久图训练的模型在部分任务上可与全约简图训练的模型表现相当，甚至更优。

Conclusion: 结合基于拓扑特征的机器学习管道利用非约简边界矩阵中的信息，可在计算成本和性能上受益。

Abstract: Supervised machine learning pipelines trained on features derived from
persistent homology have been experimentally observed to ignore much of the
information contained in a persistence diagram. Computing persistence diagrams
is often the most computationally demanding step in such a pipeline, however.
To explore this, we introduce several methods to generate topological feature
vectors from unreduced boundary matrices. We compared the performance of
pipelines trained on vectorizations of unreduced PDs to vectorizations of
fully-reduced PDs across several data and task types. Our results indicate that
models trained on PDs built from unreduced diagrams can perform on par and even
outperform those trained on fully-reduced diagrams on some tasks. This
observation suggests that machine learning pipelines which incorporate
topology-based features may benefit in terms of computational cost and
performance by utilizing information contained in unreduced boundary matrices.

</details>


### [157] [Hess-MC2: Sequential Monte Carlo Squared using Hessian Information and Second Order Proposals](https://arxiv.org/abs/2507.07461)
*Joshua Murphy,Conor Rosato,Andrew Millard,Lee Devlin,Paul Horridge,Simon Maskell*

Main category: stat.ML

TL;DR: 本文聚焦SMC²方法进行贝叶斯推理，提出结合对数目标Hessian矩阵的二阶提议，实验显示该方法在步长选择和后验近似精度上有优势。


<details>
  <summary>Details</summary>
Motivation: 使用SMC方法进行贝叶斯推理时，需兼顾后验近似精度和计算效率，SMC²虽适合HPC环境，但提议分布设计会影响精度，因此需改进。

Method: 将二阶信息（对数目标的Hessian矩阵）融入SMC²框架的提议分布设计中，此前二阶提议多用于p - MCMC方法。

Result: 在合成模型上的实验表明，与其他提议相比，该方法在步长选择和后验近似精度方面表现更佳。

Conclusion: 在SMC²框架中引入二阶提议是可行且有效的，能提升贝叶斯推理的性能。

Abstract: When performing Bayesian inference using Sequential Monte Carlo (SMC)
methods, two considerations arise: the accuracy of the posterior approximation
and computational efficiency. To address computational demands, Sequential
Monte Carlo Squared (SMC$^2$) is well-suited for high-performance computing
(HPC) environments. The design of the proposal distribution within SMC$^2$ can
improve accuracy and exploration of the posterior as poor proposals may lead to
high variance in importance weights and particle degeneracy. The
Metropolis-Adjusted Langevin Algorithm (MALA) uses gradient information so that
particles preferentially explore regions of higher probability. In this paper,
we extend this idea by incorporating second-order information, specifically the
Hessian of the log-target. While second-order proposals have been explored
previously in particle Markov Chain Monte Carlo (p-MCMC) methods, we are the
first to introduce them within the SMC$^2$ framework. Second-order proposals
not only use the gradient (first-order derivative), but also the curvature
(second-order derivative) of the target distribution. Experimental results on
synthetic models highlight the benefits of our approach in terms of step-size
selection and posterior approximation accuracy when compared to other
proposals.

</details>


### [158] [Galerkin-ARIMA: A Two-Stage Polynomial Regression Framework for Fast Rolling One-Step-Ahead Forecasting](https://arxiv.org/abs/2507.07469)
*Haojie Liu,Zihan Lin*

Main category: stat.ML

TL;DR: 提出Galerkin - ARIMA模型，结合经典时间序列建模和非参数回归，提高预测性能和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统ARIMA模型有线性假设限制和大数据集计算成本高的问题。

Method: 提出Galerkin - ARIMA，用Galerkin投影估计的样条函数替代ARIMA的AR成分，推导Galerkin系数的封闭形式OLS估计量。

Result: 模型在标准条件下渐近无偏且一致。

Conclusion: 该方法结合经典时间序列建模和非参数回归，提高了预测性能和计算效率。

Abstract: Time-series models like ARIMA remain widely used for forecasting but limited
to linear assumptions and high computational cost in large and complex
datasets. We propose Galerkin-ARIMA that generalizes the AR component of ARIMA
and replace it with a flexible spline-based function estimated by Galerkin
projection. This enables the model to capture nonlinear dependencies in lagged
values and retain the MA component and Gaussian noise assumption. We derive a
closed-form OLS estimator for the Galerkin coefficients and show the model is
asymptotically unbiased and consistent under standard conditions. Our method
bridges classical time-series modeling and nonparametric regression, which
offering improved forecasting performance and computational efficiency.

</details>


### [159] [A Unified Empirical Risk Minimization Framework for Flexible N-Tuples Weak Supervision](https://arxiv.org/abs/2507.07771)
*Shuying Huang,Junpeng Li,Changchun Hua,Yana Yang*

Main category: stat.ML

TL;DR: 提出基于经验风险最小化的通用N元组学习框架，统一数据生成过程，推导无偏经验风险估计器，在四个场景实例化并修正风险，实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有N元组学习方法依赖特定任务设计且缺乏统一理论基础，为减轻监督学习标注负担。

Method: 提出基于经验风险最小化的通用框架，统一数据生成过程，推导无偏估计器，建立泛化误差界，在四个场景实例化，采用修正函数调整经验风险。

Result: 在基准数据集上的大量实验验证了框架的有效性，利用点式无标签数据能提高泛化能力。

Conclusion: 提出的通用N元组学习框架有效，整合点式无标签数据可提升N元组学习任务的泛化性能。

Abstract: To alleviate the annotation burden in supervised learning, N-tuples learning
has recently emerged as a powerful weakly-supervised method. While existing
N-tuples learning approaches extend pairwise learning to higher-order
comparisons and accommodate various real-world scenarios, they often rely on
task-specific designs and lack a unified theoretical foundation. In this paper,
we propose a general N-tuples learning framework based on empirical risk
minimization, which systematically integrates pointwise unlabeled data to
enhance learning performance. This paper first unifies the data generation
processes of N-tuples and pointwise unlabeled data under a shared probabilistic
formulation. Based on this unified view, we derive an unbiased empirical risk
estimator that generalizes a broad class of existing N-tuples models. We
further establish a generalization error bound for theoretical support. To
demonstrate the flexibility of the framework, we instantiate it in four
representative weakly supervised scenarios, each recoverable as a special case
of our general model. Additionally, to address overfitting issues arising from
negative risk terms, we adopt correction functions to adjust the empirical
risk. Extensive experiments on benchmark datasets validate the effectiveness of
the proposed framework and demonstrate that leveraging pointwise unlabeled data
consistently improves generalization across various N-tuples learning tasks.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [160] [Large-scale portfolio optimization with variational neural annealing](https://arxiv.org/abs/2507.07159)
*Nishan Ranabhat,Behnam Javanparast,David Goerz,Estelle Inack*

Main category: cond-mat.dis-nn

TL;DR: 提出用变分神经退火（VNA）解决现实约束下的投资组合优化问题，能处理超2000资产组合，性能与顶尖优化器相当且收敛快，还揭示算法普适性和多项式退火时间缩放特性。


<details>
  <summary>Details</summary>
Motivation: 现实约束下投资组合优化问题形成的混合整数非线性规划，现有混合整数优化器难以求解。

Method: 将问题映射到经典伊辛型哈密顿量，用基于自回归神经网络的经典形式的变分神经退火（VNA）求解。

Result: VNA能为超2000资产的投资组合找到近最优解，性能与Mosek等顶尖优化器相当，在难题上收敛更快。

Conclusion: 通过对标准普尔500、罗素1000和罗素3000指数的动力学有限尺寸缩放分析，揭示了VNA算法在投资组合优化问题上的普适行为和多项式退火时间缩放特性。

Abstract: Portfolio optimization is a routine asset management operation conducted in
financial institutions around the world. However, under real-world constraints
such as turnover limits and transaction costs, its formulation becomes a
mixed-integer nonlinear program that current mixed-integer optimizers often
struggle to solve. We propose mapping this problem onto a classical Ising-like
Hamiltonian and solving it with Variational Neural Annealing (VNA), via its
classical formulation implemented using autoregressive neural networks. We
demonstrate that VNA can identify near-optimal solutions for portfolios
comprising more than 2,000 assets and yields performance comparable to that of
state-of-the-art optimizers, such as Mosek, while exhibiting faster convergence
on hard instances. Finally, we present a dynamical finite-size scaling analysis
applied to the S&P 500, Russell 1000, and Russell 3000 indices, revealing
universal behavior and polynomial annealing time scaling of the VNA algorithm
on portfolio optimization problems.

</details>


### [161] [Probabilistic Approximate Optimization: A New Variational Monte Carlo Algorithm](https://arxiv.org/abs/2507.07420)
*Abdelrahman S. Abdelrahman,Shuvro Chowdhury,Flaviano Morone,Kerem Y. Camsari*

Main category: cond-mat.dis-nn

TL;DR: 本文介绍广义概率近似优化算法(PAOA)，它是经典变分蒙特卡罗框架，在伊辛机和概率计算机上表现优异，优于QAOA和模拟退火。


<details>
  <summary>Details</summary>
Motivation: 扩展和形式化前人工作，实现在当今伊辛机和概率计算机上的参数化快速采样。

Method: 迭代修改二元随机单元网络的耦合，通过独立样本的成本评估引导；建立无导数更新与全马尔可夫流梯度的对应关系。

Result: 模拟退火是受限参数化下的特例；在基于FPGA的概率计算机上实现求解大型3D自旋玻璃问题；在26 - 自旋Sherrington - Kirkpatrick模型上PAOA性能优于QAOA；在重尾问题上优于模拟退火。

Conclusion: PAOA自然扩展了模拟退火，通过优化多个温度曲线提高性能。

Abstract: We introduce a generalized \textit{Probabilistic Approximate Optimization
Algorithm (PAOA)}, a classical variational Monte Carlo framework that extends
and formalizes prior work by Weitz \textit{et al.}~\cite{Combes_2023}, enabling
parameterized and fast sampling on present-day Ising machines and probabilistic
computers. PAOA operates by iteratively modifying the couplings of a network of
binary stochastic units, guided by cost evaluations from independent samples.
We establish a direct correspondence between derivative-free updates and the
gradient of the full $2^N \times 2^N$ Markov flow, showing that PAOA admits a
principled variational formulation. Simulated annealing emerges as a limiting
case under constrained parameterizations, and we implement this regime on an
FPGA-based probabilistic computer with on-chip annealing to solve large 3D
spin-glass problems. Benchmarking PAOA against QAOA on the canonical 26-spin
Sherrington-Kirkpatrick model with matched parameters reveals superior
performance for PAOA. We show that PAOA naturally extends simulated annealing
by optimizing multiple temperature profiles, leading to improved performance
over SA on heavy-tailed problems such as SK-L\'evy.

</details>


### [162] [A statistical physics framework for optimal learning](https://arxiv.org/abs/2507.07907)
*Francesca Mignacco,Francesco Mori*

Main category: cond-mat.dis-nn

TL;DR: 结合统计物理与控制理论，为神经网络模型确定最优学习协议，涵盖多种场景，应用于实际数据集，为元学习理论奠定基础。


<details>
  <summary>Details</summary>
Motivation: 目前对最优学习策略的理论理解不足，学习空间高维导致搜索最优协议困难，现有解决方案多为启发式且难解释、计算量大。

Method: 将统计物理与控制理论结合，在高维极限下推导跟踪随机梯度下降的常微分方程，将学习协议设计转化为最优控制问题。

Result: 找到非平凡且可解释的策略，能调节学习权衡，如最大化与信息输入方向对齐同时最小化噪声拟合。

Conclusion: 为理解和设计最优学习协议建立了原则性基础，为基于统计物理的元学习理论指明道路。

Abstract: Learning is a complex dynamical process shaped by a range of interconnected
decisions. Careful design of hyperparameter schedules for artificial neural
networks or efficient allocation of cognitive resources by biological learners
can dramatically affect performance. Yet, theoretical understanding of optimal
learning strategies remains sparse, especially due to the intricate interplay
between evolving meta-parameters and nonlinear learning dynamics. The search
for optimal protocols is further hindered by the high dimensionality of the
learning space, often resulting in predominantly heuristic, difficult to
interpret, and computationally demanding solutions. Here, we combine
statistical physics with control theory in a unified theoretical framework to
identify optimal protocols in prototypical neural network models. In the
high-dimensional limit, we derive closed-form ordinary differential equations
that track online stochastic gradient descent through low-dimensional order
parameters. We formulate the design of learning protocols as an optimal control
problem directly on the dynamics of the order parameters with the goal of
minimizing the generalization error at the end of training. This framework
encompasses a variety of learning scenarios, optimization constraints, and
control budgets. We apply it to representative cases, including optimal
curricula, adaptive dropout regularization and noise schedules in denoising
autoencoders. We find nontrivial yet interpretable strategies highlighting how
optimal protocols mediate crucial learning tradeoffs, such as maximizing
alignment with informative input directions while minimizing noise fitting.
Finally, we show how to apply our framework to real datasets. Our results
establish a principled foundation for understanding and designing optimal
learning protocols and suggest a path toward a theory of meta-learning grounded
in statistical physics.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [163] [Sampled Grid Pairwise Likelihood (SG-PL): An Efficient Approach for Spatial Regression on Large Data](https://arxiv.org/abs/2507.07113)
*Giuseppe Arbia,Vincenzo Nardelli,Niccolo Salvini*

Main category: stat.ME

TL;DR: 本文提出SG - PL方法用于大规模不规则空间数据集的回归模型估计，能大幅减少计算时间，具有实用价值。


<details>
  <summary>Details</summary>
Motivation: 大规模不规则结构化数据集的空间回归模型估计存在计算难题，现有PL方法中有效选择观测对是关键挑战。

Method: 引入Sampled Grid Pairwise Likelihood (SG - PL)方法，采用基于网格的抽样策略来选择观测对。

Result: 模拟研究表明SG - PL能大幅减少计算时间，以可接受的统计效率损失为代价；实证应用验证了其实用性。

Conclusion: SG - PL是大规模数据集空间分析的可扩展且有效的工具，在计算可行性和统计精度间取得良好平衡。

Abstract: Estimating spatial regression models on large, irregularly structured
datasets poses significant computational hurdles. While Pairwise Likelihood
(PL) methods offer a pathway to simplify these estimations, the efficient
selection of informative observation pairs remains a critical challenge,
particularly as data volume and complexity grow. This paper introduces the
Sampled Grid Pairwise Likelihood (SG-PL) method, a novel approach that employs
a grid-based sampling strategy to strategically select observation pairs.
Simulation studies demonstrate SG-PL's principal advantage: a dramatic
reduction in computational time -- often by orders of magnitude -- when
compared to benchmark methods. This substantial acceleration is achieved with a
manageable trade-off in statistical efficiency. An empirical application
further validates SG-PL's practical utility. Consequently, SG-PL emerges as a
highly scalable and effective tool for spatial analysis on very large datasets,
offering a compelling balance where substantial gains in computational
feasibility are realized for a limited cost in statistical precision, a
trade-off that increasingly favors SG-PL with larger N.

</details>


### [164] [Vecchia approximated Bayesian heteroskedastic Gaussian processes](https://arxiv.org/abs/2507.07815)
*Parul V. Patil,Robert B. Gramacy,Cayelan C. Carey,R. Quinn Thomas*

Main category: stat.ME

TL;DR: 提出用椭圆切片采样和Vecchia近似的贝叶斯异方差高斯过程（hetGP），在基准示例和湖温模拟中表现好，开源实现为bhetGP。


<details>
  <summary>Details</summary>
Motivation: 现有hetGP实现无法处理大规模模拟，且使用点估计限制了适用性和不确定性评估。

Method: 提出贝叶斯hetGP，用椭圆切片采样进行后验方差积分，用Vecchia近似规避计算瓶颈。

Result: 在基准示例和超900万次湖温模拟中，相比其他方法有良好表现。

Conclusion: 所提方法能提升hetGP能力，可处理大规模模拟，开源代码方便使用。

Abstract: Many computer simulations are stochastic and exhibit input dependent noise.
In such situations, heteroskedastic Gaussian processes (hetGPs) make ideal
surrogates as they estimate a latent, non-constant variance. However, existing
hetGP implementations are unable to deal with large simulation campaigns and
use point-estimates for all unknown quantities, including latent variances.
This limits applicability to small experiments and undercuts uncertainty. We
propose a Bayesian hetGP using elliptical slice sampling (ESS) for posterior
variance integration, and the Vecchia approximation to circumvent computational
bottlenecks. We show good performance for our upgraded hetGP capability,
compared to alternatives, on a benchmark example and a motivating corpus of
more than 9-million lake temperature simulations. An open source implementation
is provided as bhetGP on CRAN.

</details>


### [165] [Late Fusion Multi-task Learning for Semiparametric Inference with Nuisance Parameters](https://arxiv.org/abs/2507.07941)
*Sohom Bhattacharya,Yongzhuo Chen,Muxuan Liang*

Main category: stat.ME

TL;DR: 提出用于半参数模型多任务学习的后期融合框架，可跨数据源估计参数，有理论保证和实际应用验证有效性。


<details>
  <summary>Details</summary>
Motivation: 大数据时代需整合多源信息改进参数估计，多任务学习是有效途径，聚焦含无穷维扰动参数的半参数模型。

Method: 采用两步后期融合框架，先通过单任务学习获取初始双机器学习估计量，再自适应聚合；还提出用于扰动参数估计的多任务学习方法。

Result: 建立理论保证，相比单任务学习有更快收敛率；模拟和实际数据应用验证框架有效性。

Conclusion: 所提框架在参数估计上有效，即使样本量适中也能发挥作用。

Abstract: In the age of large and heterogeneous datasets, the integration of
information from diverse sources is essential to improve parameter estimation.
Multi-task learning offers a powerful approach by enabling simultaneous
learning across related tasks. In this work, we introduce a late fusion
framework for multi-task learning with semiparametric models that involve
infinite-dimensional nuisance parameters, focusing on applications such as
heterogeneous treatment effect estimation across multiple data sources,
including electronic health records from different hospitals or clinical trial
data. Our framework is two-step: first, initial double machine-learning
estimators are obtained through individual task learning; second, these
estimators are adaptively aggregated to exploit task similarities while
remaining robust to task-specific differences. In particular, the framework
avoids individual level data sharing, preserving privacy. Additionally, we
propose a novel multi-task learning method for nuisance parameter estimation,
which further enhances parameter estimation when nuisance parameters exhibit
similarity across tasks. We establish theoretical guarantees for the method,
demonstrating faster convergence rates compared to individual task learning
when tasks share similar parametric components. Extensive simulations and real
data applications complement the theoretical findings of our work while
highlight the effectiveness of our framework even in moderate sample sizes.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [166] [MODA: A Unified 3D Diffusion Framework for Multi-Task Target-Aware Molecular Generation](https://arxiv.org/abs/2507.07201)
*Dong Xu,Zhangfan Yang,Sisi Yuan,Jenna Xinyi Yao,Jiangqiang Li,Junkai Ji*

Main category: q-bio.BM

TL;DR: 提出MODA扩散框架统一分子设计多任务，单阶段多任务扩散例程可替代两阶段工作流。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的三维分子生成器在任务上分散，SMILES输入、两阶段训练流程和单任务单模型做法阻碍了立体化学保真度、任务对齐和零样本迁移。

Method: 引入MODA扩散框架，用贝叶斯掩码调度器统一多项任务，训练时掩盖连续空间片段并一次去噪，进行多任务训练。

Result: 多任务训练得到的通用主干超越多个基线和训练范式，不同模型各有优劣，零样本测试有稳定负Vina分数和高改进率。

Conclusion: 单阶段多任务扩散例程可替代两阶段工作流用于基于结构的分子设计。

Abstract: Three-dimensional molecular generators based on diffusion models can now
reach near-crystallographic accuracy, yet they remain fragmented across tasks.
SMILES-only inputs, two-stage pretrain-finetune pipelines, and
one-task-one-model practices hinder stereochemical fidelity, task alignment,
and zero-shot transfer. We introduce MODA, a diffusion framework that unifies
fragment growing, linker design, scaffold hopping, and side-chain decoration
with a Bayesian mask scheduler. During training, a contiguous spatial fragment
is masked and then denoised in one pass, enabling the model to learn shared
geometric and chemical priors across tasks. Multi-task training yields a
universal backbone that surpasses six diffusion baselines and three training
paradigms on substructure, chemical property, interaction, and geometry.
Model-C reduces ligand-protein clashes and substructure divergences while
maintaining Lipinski compliance, whereas Model-B preserves similarity but
trails in novelty and binding affinity. Zero-shot de novo design and
lead-optimisation tests confirm stable negative Vina scores and high
improvement rates without force-field refinement. These results demonstrate
that a single-stage multi-task diffusion routine can replace two-stage
workflows for structure-based molecular design.

</details>


### [167] [Platform for Representation and Integration of multimodal Molecular Embeddings](https://arxiv.org/abs/2507.07367)
*Erika Yilin Zheng,Yu Yan,Baradwaj Simha Sankar,Ethan Ji,Steven Swee,Irsyad Adam,Ding Wang,Alexander Russell Pelletier,Alex Bui,Wei Wang,Peipei Ping*

Main category: q-bio.BM

TL;DR: 现有分子嵌入机器学习方法受限，本文评估多源生物分子知识表征，提出PRISME整合多模态嵌入，经验证表现良好，推动多模态嵌入发展。


<details>
  <summary>Details</summary>
Motivation: 现有分子嵌入机器学习方法局限于特定任务或数据模态，无法全面捕捉基因功能和相互作用。

Method: 系统评估多维度生物分子知识表征，设计调整版SVCCA区分信号，提出PRISME工作流用自编码器整合多模态嵌入。

Result: 分析表明现有嵌入捕捉非重叠分子信号，PRISME在多个基准任务中表现稳定，在缺失值插补中优于单个嵌入方法。

Conclusion: 新框架支持生物分子综合建模，推动适用于下游生物医学机器学习应用的多模态嵌入发展。

Abstract: Existing machine learning methods for molecular (e.g., gene) embeddings are
restricted to specific tasks or data modalities, limiting their effectiveness
within narrow domains. As a result, they fail to capture the full breadth of
gene functions and interactions across diverse biological contexts. In this
study, we have systematically evaluated knowledge representations of
biomolecules across multiple dimensions representing a task-agnostic manner
spanning three major data sources, including omics experimental data,
literature-derived text data, and knowledge graph-based representations. To
distinguish between meaningful biological signals from chance correlations, we
devised an adjusted variant of Singular Vector Canonical Correlation Analysis
(SVCCA) that quantifies signal redundancy and complementarity across different
data modalities and sources. These analyses reveal that existing embeddings
capture largely non-overlapping molecular signals, highlighting the value of
embedding integration. Building on this insight, we propose Platform for
Representation and Integration of multimodal Molecular Embeddings (PRISME), a
machine learning based workflow using an autoencoder to integrate these
heterogeneous embeddings into a unified multimodal representation. We validated
this approach across various benchmark tasks, where PRISME demonstrated
consistent performance, and outperformed individual embedding methods in
missing value imputations. This new framework supports comprehensive modeling
of biomolecules, advancing the development of robust, broadly applicable
multimodal embeddings optimized for downstream biomedical machine learning
applications.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [168] [Generative Panoramic Image Stitching](https://arxiv.org/abs/2507.07133)
*Mathieu Tuli,Kaveh Kamali,David B. Lindell*

Main category: cs.GR

TL;DR: 提出生成式全景图像拼接任务，提出微调扩散修复模型方法，结果在图像质量等方面优于基线。


<details>
  <summary>Details</summary>
Motivation: 传统图像拼接管道和现有生成模型在生成含视差、光照等变化的全景图时有局限，无法生成无缝且连贯的全景图。

Method: 微调基于扩散的修复模型，使其基于多参考图像保留场景内容和布局，从单参考图像扩展出完整全景图。

Result: 生成无缝且视觉连贯的全景图，忠实整合所有参考图像内容，在图像质量和结构一致性上显著优于基线。

Conclusion: 所提方法能有效解决生成式全景图像拼接问题，表现优于现有方法。

Abstract: We introduce the task of generative panoramic image stitching, which aims to
synthesize seamless panoramas that are faithful to the content of multiple
reference images containing parallax effects and strong variations in lighting,
camera capture settings, or style. In this challenging setting, traditional
image stitching pipelines fail, producing outputs with ghosting and other
artifacts. While recent generative models are capable of outpainting content
consistent with multiple reference images, they fail when tasked with
synthesizing large, coherent regions of a panorama. To address these
limitations, we propose a method that fine-tunes a diffusion-based inpainting
model to preserve a scene's content and layout based on multiple reference
images. Once fine-tuned, the model outpaints a full panorama from a single
reference image, producing a seamless and visually coherent result that
faithfully integrates content from all reference images. Our approach
significantly outperforms baselines for this task in terms of image quality and
the consistency of image structure and scene layout when evaluated on captured
datasets.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [169] [Thermodynamic Prediction Enabled by Automatic Dataset Building and Machine Learning](https://arxiv.org/abs/2507.07293)
*Juejing Liu,Haydn Anderson,Noah I. Waxman,Vsevolod Kovalev,Byron Fisher,Elizabeth Li,Xiaofeng Guo*

Main category: cond-mat.mtrl-sci

TL;DR: 利用大语言模型进行自动文献综述，并训练机器学习模型预测化学知识，凸显集成机器学习方法重塑化学和材料科学研究的潜力。


<details>
  <summary>Details</summary>
Motivation: 化学和材料科学新发现带来知识量和实验工作量增加，需要机器学习加速研究效率。

Method: 使用大语言模型进行自动文献综述，用CatBoost算法训练机器学习模型。

Result: 基于大语言模型的文献综述工具成功提取化学信息，训练的机器学习模型能准确预测矿物热力学参数。

Conclusion: 集成机器学习方法有重塑化学和材料科学研究的变革潜力。

Abstract: New discoveries in chemistry and materials science, with increasingly
expanding volume of requisite knowledge and experimental workload, provide
unique opportunities for machine learning (ML) to take critical roles in
accelerating research efficiency. Here, we demonstrate (1) the use of large
language models (LLMs) for automated literature reviews, and (2) the training
of an ML model to predict chemical knowledge (thermodynamic parameters). Our
LLM-based literature review tool (LMExt) successfully extracted chemical
information and beyond into a machine-readable structure, including stability
constants for metal cation-ligand interactions, thermodynamic properties, and
other broader data types (medical research papers, and financial reports),
effectively overcoming the challenges inherent in each domain. Using the
autonomous acquisition of thermodynamic data, an ML model was trained using the
CatBoost algorithm for accurately predicting thermodynamic parameters (e.g.,
enthalpy of formation) of minerals. This work highlights the transformative
potential of integrated ML approaches to reshape chemistry and materials
science research.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [170] [ArchiveGPT: A human-centered evaluation of using a vision language model for image cataloguing](https://arxiv.org/abs/2507.07551)
*Line Abele,Gerrit Anders,Tolgahan Aydın,Jürgen Buder,Helen Fischer,Dominik Kimmel,Markus Huff*

Main category: cs.HC

TL;DR: 研究使用VLM自动生成照片集元数据，评估AI生成描述质量，发现需人工审核，建议人机协作。


<details>
  <summary>Details</summary>
Motivation: 照片集增长快，人工编目难，研究AI生成编目描述能否达人类水平及如何融入编目工作流。

Method: 用InternVL2为考古照片生成编目描述，通过以人为中心的实验框架，让专家和非专家评估。

Result: 分类表现超随机水平，两组都低估检测能力；OCR错误和幻觉影响质量，高质量描述难分类；专家采用意愿低。

Conclusion: 提倡人机协作，AI辅助生成草稿，人工验证，成功集成依赖技术进步和建立专业人员信任。

Abstract: The accelerating growth of photographic collections has outpaced manual
cataloguing, motivating the use of vision language models (VLMs) to automate
metadata generation. This study examines whether Al-generated catalogue
descriptions can approximate human-written quality and how generative Al might
integrate into cataloguing workflows in archival and museum collections. A VLM
(InternVL2) generated catalogue descriptions for photographic prints on
labelled cardboard mounts with archaeological content, evaluated by archive and
archaeology experts and non-experts in a human-centered, experimental
framework. Participants classified descriptions as AI-generated or
expert-written, rated quality, and reported willingness to use and trust in AI
tools. Classification performance was above chance level, with both groups
underestimating their ability to detect Al-generated descriptions. OCR errors
and hallucinations limited perceived quality, yet descriptions rated higher in
accuracy and usefulness were harder to classify, suggesting that human review
is necessary to ensure the accuracy and quality of catalogue descriptions
generated by the out-of-the-box model, particularly in specialized domains like
archaeological cataloguing. Experts showed lower willingness to adopt AI tools,
emphasizing concerns on preservation responsibility over technical performance.
These findings advocate for a collaborative approach where AI supports draft
generation but remains subordinate to human verification, ensuring alignment
with curatorial values (e.g., provenance, transparency). The successful
integration of this approach depends not only on technical advancements, such
as domain-specific fine-tuning, but even more on establishing trust among
professionals, which could both be fostered through a transparent and
explainable AI pipeline.

</details>


### [171] [Probing Experts' Perspectives on AI-Assisted Public Speaking Training](https://arxiv.org/abs/2507.07930)
*Nesrine Fourati,Alisa Barkar,Marion Dragée,Liv Danthon-Lefebvre,Mathieu Chollet*

Main category: cs.HC

TL;DR: 研究评估演讲专家对商用AI演讲训练工具的看法并提改进建议，专家认可其价值但指出问题，支持混合模式。


<details>
  <summary>Details</summary>
Motivation: 传统演讲训练依赖专家指导，AI带来新的自动反馈工具，但研究多关注原型，缺乏专家看法研究，故评估专家意见并提改进准则。

Method: 对16位演讲专家进行半结构化访谈和2次焦点小组讨论，探讨对现有工具看法、与传统指导结合及改进建议。

Result: 专家认可AI工具处理重复技术训练的价值，让教练关注高级技能，但指出当前工具存在关键问题。

Conclusion: 需提供个性化、易懂、精选的反馈和清晰教学设计，支持传统指导与AI辅助练习结合的混合模式。

Abstract: Background: Public speaking is a vital professional skill, yet it remains a
source of significant anxiety for many individuals. Traditional training relies
heavily on expert coaching, but recent advances in AI has led to novel types of
commercial automated public speaking feedback tools. However, most research has
focused on prototypes rather than commercial applications, and little is known
about how public speaking experts perceive these tools.
  Objectives: This study aims to evaluate expert opinions on the efficacy and
design of commercial AI-based public speaking training tools and to propose
guidelines for their improvement.
  Methods: The research involved 16 semi-structured interviews and 2 focus
groups with public speaking experts. Participants discussed their views on
current commercial tools, their potential integration into traditional
coaching, and suggestions for enhancing these systems.
  Results and Conclusions: Experts acknowledged the value of AI tools in
handling repetitive, technical aspects of training, allowing coaches to focus
on higher-level skills. However they found key issues in current tools,
emphasising the need for personalised, understandable, carefully selected
feedback and clear instructional design. Overall, they supported a hybrid model
combining traditional coaching with AI-supported exercises.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [172] [Almost Sure Convergence for the Last Iterate of Stochastic Gradient Descent Schemes](https://arxiv.org/abs/2507.07281)
*Marcel Hudiani*

Main category: math.OC

TL;DR: 研究随机梯度下降（SGD）和随机重球法（SHB）最后迭代的几乎必然收敛率，用离散Gronwall不等式得到收敛结果，还证明SHB在特定条件下的收敛率。


<details>
  <summary>Details</summary>
Motivation: 研究SGD和SHB在参数设置下，目标函数全局凸或非凸（梯度为γ - H"{o}lder）时最后迭代的几乎必然收敛率。

Method: 仅使用离散Gronwall不等式，不借助Robbins - Siegmund定理和鞅收敛理论。

Result: 得到SGD和SHB在非凸和凸目标下的收敛结果，还证明SHB在特定条件下的收敛率。

Conclusion: 通过离散Gronwall不等式能有效分析SGD和SHB的收敛率。

Abstract: We study the almost sure convergence rate for the last iterate of stochastic
gradient descent (SGD) and stochastic heavy ball (SHB) in the parametric
setting when the objective function $F$ is globally convex or non-convex whose
gradient is $\gamma$-H\"{o}lder. Using only discrete Gronwall's inequality
without Robbins-Siegmund theorem nor martingale convergence theory, we recover
results for both SGD and SHB: $\min_{s\leq t} \|\nabla F(w_s)\|^2 = o(t^{p-1})$
for non-convex objectives and $F(w_t) - F_* = o(t^{2\gamma/(1+\gamma) \cdot
\max(p-1,-2p+1)-\epsilon})$ for $\beta \in (0, 1)$ and $\min_{s \leq t} F(w_s)
- F_* = o(t^{p-1})$ almost surely for convex objectives. In addition, we proved
that SHB with constant momentum parameter $\beta \in (0, 1)$ attains a
convergence rate of $F(w_t) - F_* = O(t^{\max(p-1,-2p+1)} \log^2
\frac{t}{\delta})$ with probability at least $1-\delta$ when $F$ is convex and
$\gamma = 1$ and step size $\alpha_t = \Theta(t^{-p})$ with $p \in
(\frac{1}{2}, 1)$.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [173] [Synergistic Localization and Sensing in MIMO-OFDM Systems via Mixed-Integer Bilevel Learning](https://arxiv.org/abs/2507.07118)
*Zelin Zhu,Kai Yang,Rui Zhang*

Main category: cs.NI

TL;DR: 论文提出SPG - MIBO算法联合优化无线定位与感知任务，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有对MIMO - OFDM系统高维CSI特征下定位与感知联合建模研究不足，期望挖掘两者协同潜力。

Method: 将定位和感知问题表述为混合整数双层深度学习问题，提出基于随机近端梯度的混合整数双层优化（SPG - MIBO）算法。

Result: 在多个数据集上的大量实验验证了算法有效性，凸显联合优化定位与感知的性能提升。

Conclusion: SPG - MIBO算法适用于高维大规模数据集，有理论收敛保证，可有效联合优化定位与感知任务。

Abstract: Wireless localization and sensing technologies are essential in modern
wireless networks, supporting applications in smart cities, the Internet of
Things (IoT), and autonomous systems. High-performance localization and sensing
systems are critical for both network efficiency and emerging intelligent
applications. Integrating channel state information (CSI) with deep learning
has recently emerged as a promising solution. Recent works have leveraged the
spatial diversity of multiple input multiple output (MIMO) systems and the
frequency granularity of orthogonal frequency division multiplexing (OFDM)
waveforms to improve spatial resolution. Nevertheless, the joint modeling of
localization and sensing under the high-dimensional CSI characteristics of
MIMO-OFDM systems remains insufficiently investigated. This work aims to
jointly model and optimize localization and sensing tasks to harness their
potential synergy. We first formulate localization and sensing as a
mixed-integer bilevel deep learning problem and then propose a novel stochastic
proximal gradient-based mixed-integer bilevel optimization (SPG-MIBO)
algorithm. SPG-MIBO is well-suited for high-dimensional and large-scale
datasets, leveraging mini-batch training at each step for computational and
memory efficiency. The algorithm is also supported by theoretical convergence
guarantees. Extensive experiments on multiple datasets validate its
effectiveness and highlight the performance gains from joint localization and
sensing optimization.

</details>


### [174] [DAF: An Efficient End-to-End Dynamic Activation Framework for on-Device DNN Training](https://arxiv.org/abs/2507.07149)
*Renyuan Liu,Yuyang Leng,Kaiyan Liu,Shaohan Hu,Chun-Fu,Chen,Peijun Zhao,Heechul Yun,Shuochao Yao*

Main category: cs.NI

TL;DR: 为解决设备端训练中激活压缩的系统级挑战，提出DAF框架，可在不损失精度下实现内存和时间高效的动态量化训练，在多平台评估有显著效果。


<details>
  <summary>Details</summary>
Motivation: 设备端训练有内存限制，现有动态激活量化方法有系统级挑战，如计算开销和内存碎片化，需高效激活压缩方法。

Method: 提出DAF框架，通过开发混合约简操作、利用CPU - GPU协作位打包、实现重要性感知分页内存管理方案等系统级优化。

Result: 在嵌入式和移动平台上对多种深度学习模型评估，内存使用最多减少22.9倍，速度最多提升3.2倍。

Conclusion: DAF是资源受限环境中可扩展且实用的解决方案。

Abstract: Recent advancements in on-device training for deep neural networks have
underscored the critical need for efficient activation compression to overcome
the memory constraints of mobile and edge devices. As activations dominate
memory usage during training and are essential for gradient computation,
compressing them without compromising accuracy remains a key research
challenge. While existing methods for dynamic activation quantization promise
theoretical memory savings, their practical deployment is impeded by
system-level challenges such as computational overhead and memory
fragmentation.
  To address these challenges, we introduce DAF, a Dynamic Activation Framework
that enables scalable and efficient on-device training through system-level
optimizations. DAF achieves both memory- and time-efficient dynamic
quantization training by addressing key system bottlenecks. It develops hybrid
reduction operations tailored to the memory hierarchies of mobile and edge
SoCs, leverages collaborative CPU-GPU bit-packing for efficient dynamic
quantization, and implements an importance-aware paging memory management
scheme to reduce fragmentation and support dynamic memory adjustments.
  These optimizations collectively enable DAF to achieve substantial memory
savings and speedup without compromising model training accuracy. Evaluations
on various deep learning models across embedded and mobile platforms
demonstrate up to a $22.9\times$ reduction in memory usage and a $3.2\times$
speedup, making DAF a scalable and practical solution for resource-constrained
environments.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [175] [Scalable ADER-DG Transport Method with Polynomial Order Independent CFL Limit](https://arxiv.org/abs/2507.07304)
*Kieran Ricardo,Kenneth Duru*

Main category: math.NA

TL;DR: 提出针对输运主导问题的新型ADER - DG格式，稳定时间步与多项式阶数无关，给出稳定性证明并通过数值实验验证精度和收敛性。


<details>
  <summary>Details</summary>
Motivation: 传统间断Galerkin（DG）方法随多项式阶数增加时间步约束变严格，限制高阶效率，需改进。

Method: 引入局部隐式但全局显式的ADER - DG格式，每时间步求解单元局部隐式问题，用半解析von Neumann稳定性分析扩展到二维和三维。

Result: 方法在d维空间中CFL数达1/√d仍稳定，给出一维严格稳定性证明。

Conclusion: 该方法有效，通过数值实验验证其对线性和非线性测试用例的精度和收敛性。

Abstract: Discontinuous Galerkin (DG) methods are known to suffer from increasingly
restrictive time step constraints as the polynomial order increases, limiting
their efficiency at high orders. In this paper, we introduce a novel locally
implicit, but globally explicit ADER-DG scheme designed for transport-dominated
problems. The method achieves a maximum stable time step governed by an
element-width based CFL condition that is independent of the polynomial degree.
By solving a set of element-local implicit problems at each time step, our
approach more effectively captures the domain of dependence. As a result, our
method remains stable for CFL numbers up to $1/\sqrt{d}$ in $d$ spatial
dimensions. We provide a rigorous stability proof in one dimension, and extend
the analysis to two and three dimensions using a semi-analytical von Neumann
stability analysis. The accuracy and convergence of the method are demonstrated
through numerical experiments on both linear and nonlinear test cases.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [176] [Toolchain for Faster Iterations in Quantum Software Development](https://arxiv.org/abs/2507.07448)
*Otso Kinanen,Andrés D. Muñoz-Moller,Vlad Stirbu,Tommi Mikkonen*

Main category: quant-ph

TL;DR: 本文探讨利用远程计算能力改进量子软件开发工作流，实验获得最高5倍电路执行加速并支持21 - 29个量子比特。


<details>
  <summary>Details</summary>
Motivation: 量子软件开发受硬件有限、经典系统模拟计算需求高和技术栈复杂等限制，开发者难以创建高效工作流。

Method: 研究以高效方式使用远程计算能力，降低本地执行和远程硬件转换障碍，在模拟器环境中加速执行。

Result: 实验中电路执行运行时间最高加快5倍，通过简单即插即用内核为Jupyter笔记本实现21到29个量子比特范围。

Conclusion: 该方法有助于开发更复杂电路，支持迭代式软件开发方法。

Abstract: Quantum computing proposes a revolutionary paradigm that can radically
transform numerous scientific and industrial application domains. To realize
this promise, these new capabilities need software solutions that are able to
effectively harness its power. However, developers may face significant
challenges when developing and executing quantum software due to the limited
availability of quantum computer hardware, high computational demands of
simulating quantum computers on classical systems, and complicated technology
stack to enable currently available accelerators into development environments.
These limitations make it difficult for the developer to create an efficient
workflow for quantum software development. In this paper, we investigate the
potential of using remote computational capabilities in an efficient manner to
improve the workflow of quantum software developers, by lowering the barrier of
moving between local execution and computationally more efficient remote
hardware and offering speedup in execution with simulator surroundings. The
goal is to allow the development of more complex circuits and to support an
iterative software development approach. In our experiment, with the solution
presented in this paper, we have obtained up to 5 times faster circuit
execution runtime, and enabled qubit ranges from 21 to 29 qubits with a simple
plug-and-play kernel for the Jupyter notebook.

</details>


### [177] [Quantum Executor: A Unified Interface for Quantum Computing](https://arxiv.org/abs/2507.07597)
*Giuseppe Bisicchia,Alessandro Bocci,Antonio Brogi*

Main category: quant-ph

TL;DR: 随着量子计算发展，本文介绍后端无关的量子执行引擎Quantum Executor，阐述其特性、应用场景，最后讨论局限与改进方向。


<details>
  <summary>Details</summary>
Motivation: 量子计算从理论走向实践，对量子软件实验工具需求增长，需要开发相关工具。

Method: 设计后端无关的执行引擎Quantum Executor，提供声明式和模块化接口，支持异步和分布式执行等。

Result: 通过自动化基准测试和混合验证两个场景说明其适用性，可简化量子开发。

Conclusion: 讨论当前局限并给出未来改进路线图。

Abstract: As quantum computing evolves from theoretical promise to practical
deployment, the demand for robust, portable, and scalable tools for quantum
software experimentation is growing. This paper introduces Quantum Executor, a
backend-agnostic execution engine designed to orchestrate quantum experiments
across heterogeneous platforms. Quantum Executor provides a declarative and
modular interface that decouples experiment design from backend execution,
enabling seamless interoperability and code reuse across diverse quantum and
classical resources. Key features include support for asynchronous and
distributed execution, customizable execution strategies and a unified API for
managing quantum experiments. We illustrate its applicability through two
life-like usage scenarios such as automated benchmarking and hybrid validation,
discussing its capacity to streamline quantum development. We conclude by
discussing current limitations and outlining a roadmap for future enhancements.

</details>


### [178] [ProvideQ: A Quantum Optimization Toolbox](https://arxiv.org/abs/2507.07649)
*Domenik Eichhorn,Nick Poser,Maximilian Schweikart,Ina Schaefer*

Main category: quant-ph

TL;DR: 介绍了ProvideQ工具盒以解决混合求解器在实际应用中的难题，展示其架构和应用案例，指出需更先进硬件提升性能。


<details>
  <summary>Details</summary>
Motivation: 混合求解器理论性能好但因缺乏技术栈难以实际应用，要解决此问题。

Method: 引入ProvideQ工具盒，通过元求解器策略让用户配置混合求解器，使用分解技术拆分问题。

Result: 概念验证表明元求解器策略已能应用量子子程序。

Conclusion: 目前需更复杂硬件使量子子程序性能具备竞争力。

Abstract: Hybrid solvers for combinatorial optimization problems combine the advantages
of classical and quantum computing to overcome difficult computational
challenges. Although their theoretical performance seems promising, their
practical applicability is challenging due to the lack of a technological stack
that can seamlessly integrate quantum solutions with existing classical
optimization frameworks. We tackle this challenge by introducing the ProvideQ
toolbox, a software tool that enables users to easily adapt and configure
hybrid solvers via Meta-Solver strategies. A Meta-Solver strategy implements
decomposition techniques, which splits problems into classical and quantum
subroutines. The ProvideQ toolbox enables the interactive creation of such
decompositions via a Meta-Solver configuration tool. It combines
well-established classical optimization techniques with quantum circuits that
are seamlessly executable on multiple backends. This paper introduces the
technical details of the ProvideQ toolbox, explains its architecture, and
demonstrates possible applications for several real-world use cases. Our proof
of concept shows that Meta-Solver strategies already enable the application of
quantum subroutines today, however, more sophisticated hardware is required to
make their performance competitive.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [179] [SonicMotion: Dynamic Spatial Audio Soundscapes with Latent Diffusion Models](https://arxiv.org/abs/2507.07318)
*Christian Templin,Yanda Zhu,Hao Wang*

Main category: cs.SD

TL;DR: 本文提出端到端模型SonicMotion以生成含动态声源的3D场景，还构建新数据集，模型评估效果良好。


<details>
  <summary>Details</summary>
Motivation: 将一阶Ambisonics（FOA）生成式AI模型的进展扩展到生成含动态声源的3D场景。

Method: 提出两种不同用户输入和定位精度的端到端模型SonicMotion，构建模拟空间音频 - 字幕对的新数据集。

Result: 模型能达到与现有先进模型相当的语义对齐和音频质量，同时捕捉所需空间属性。

Conclusion: SonicMotion模型在生成含动态声源的3D场景方面有良好表现。

Abstract: Spatial audio is an integral part of immersive entertainment, such as VR/AR,
and has seen increasing popularity in cinema and music as well. The most common
format of spatial audio is described as first-order Ambisonics (FOA). We seek
to extend recent advancements in FOA generative AI models to enable the
generation of 3D scenes with dynamic sound sources. Our proposed end-to-end
model, SonicMotion, comes in two variations which vary in their user input and
level of precision in sound source localization. In addition to our model, we
also present a new dataset of simulated spatial audio-caption pairs. Evaluation
of our models demonstrate that they are capable of matching the semantic
alignment and audio quality of state of the art models while capturing the
desired spatial attributes.

</details>


### [180] [Re-Bottleneck: Latent Re-Structuring for Neural Audio Autoencoders](https://arxiv.org/abs/2507.07867)
*Dimitrios Bralios,Jonah Casebeer,Paris Smaragdis*

Main category: cs.SD

TL;DR: 提出 Re - Bottleneck 框架修改预训练自编码器瓶颈，提升神经音频模型性能，满足不同应用需求。


<details>
  <summary>Details</summary>
Motivation: 多数神经音频编解码器和自编码器训练时忽视下游应用所需潜在结构，作者旨在解决此问题。

Method: 提出“Re - Bottleneck”，通过潜在空间损失训练内部瓶颈，以注入用户定义的结构。

Result: 在三项实验中证明框架有效性，如对潜在通道排序、使潜在特征与语义嵌入对齐、引入等变性。

Conclusion: Re - Bottleneck 框架提供灵活高效方式定制神经音频模型表示，以满足不同应用需求，且额外训练成本低。

Abstract: Neural audio codecs and autoencoders have emerged as versatile models for
audio compression, transmission, feature-extraction, and latent-space
generation. However, a key limitation is that most are trained to maximize
reconstruction fidelity, often neglecting the specific latent structure
necessary for optimal performance in diverse downstream applications. We
propose a simple, post-hoc framework to address this by modifying the
bottleneck of a pre-trained autoencoder. Our method introduces a
"Re-Bottleneck", an inner bottleneck trained exclusively through latent space
losses to instill user-defined structure. We demonstrate the framework's
effectiveness in three experiments. First, we enforce an ordering on latent
channels without sacrificing reconstruction quality. Second, we align latents
with semantic embeddings, analyzing the impact on downstream diffusion
modeling. Third, we introduce equivariance, ensuring that a filtering operation
on the input waveform directly corresponds to a specific transformation in the
latent space. Ultimately, our Re-Bottleneck framework offers a flexible and
efficient way to tailor representations of neural audio models, enabling them
to seamlessly meet the varied demands of different applications with minimal
additional training.

</details>


### [181] [Edge-ASR: Towards Low-Bit Quantization of Automatic Speech Recognition Models](https://arxiv.org/abs/2507.07877)
*Chen Feng,Yicheng Lin,Shaojie Zhuo,Chenzheng Su,Ramchalam Kinattinkara Ramakrishnan,Zhaocong Yuan,Xiaopeng Zhang*

Main category: cs.SD

TL;DR: 本文对八种SOTA PTQ方法应用于Whisper和Moonshine两种边缘ASR模型进行全面基准测试，分析量化对模型性能影响，结果显示高级PTQ技术下3位量化可用于高容量模型。


<details>
  <summary>Details</summary>
Motivation: 尽管量化能减少模型大小和推理成本，但各种先进量化方法和位宽配置对ASR模型性能的影响尚不明确，且在资源受限边缘设备部署ASR模型存在挑战。

Method: 对两种领先的边缘ASR模型家族应用八种SOTA PTQ方法，在七个不同数据集上系统评估模型性能，基于LLM压缩工具包扩展构建框架。

Result: 表征了效率和准确性之间的权衡，表明使用高级PTQ技术时3位量化可在高容量模型上成功。

Conclusion: 研究结果为优化低功耗、常开边缘设备上的ASR模型提供了有价值的见解。

Abstract: Recent advances in Automatic Speech Recognition (ASR) have demonstrated
remarkable accuracy and robustness in diverse audio applications, such as live
transcription and voice command processing. However, deploying these models on
resource constrained edge devices (e.g., IoT device, wearables) still presents
substantial challenges due to strict limits on memory, compute and power.
Quantization, particularly Post-Training Quantization (PTQ), offers an
effective way to reduce model size and inference cost without retraining.
Despite its importance, the performance implications of various advanced
quantization methods and bit-width configurations on ASR models remain unclear.
In this work, we present a comprehensive benchmark of eight state-of-the-art
(SOTA) PTQ methods applied to two leading edge-ASR model families, Whisper and
Moonshine. We systematically evaluate model performances (i.e., accuracy,
memory I/O and bit operations) across seven diverse datasets from the open ASR
leaderboard, analyzing the impact of quantization and various configurations on
both weights and activations. Built on an extension of the LLM compression
toolkit, our framework integrates edge-ASR models, diverse advanced
quantization algorithms, a unified calibration and evaluation data pipeline,
and detailed analysis tools. Our results characterize the trade-offs between
efficiency and accuracy, demonstrating that even 3-bit quantization can succeed
on high capacity models when using advanced PTQ techniques. These findings
provide valuable insights for optimizing ASR models on low-power, always-on
edge devices.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [182] [Electricity Market Predictability: Virtues of Machine Learning and Links to the Macroeconomy](https://arxiv.org/abs/2507.07477)
*Jinbo Cai,Wenze Li,Wenjie Wang*

Main category: econ.GN

TL;DR: 使用利益相关者层面的市场数据对新加坡电力价格预测的机器学习模型进行比较分析，揭示模型优点、提升集成性能方法、经济收益及时间序列预测模式等。


<details>
  <summary>Details</summary>
Motivation: 利用利益相关者层面的市场数据，对机器学习在新加坡电力价格预测方面进行研究，探索不同模型的表现及相关规律。

Method: 进行了15个单独模型和4种集成方法的比较分析，通过模拟支持相关结论，采用惩罚预测相关性等方法。

Result: 证实ML模型的三个优点，发现惩罚预测相关性能提升集成性能，可转化为经济收益，揭示时间序列预测在不同宏观制度下的模式，特征重要性结果反映新加坡电力市场动态。

Conclusion: ML模型在新加坡电力价格预测中有一定优势，可通过相关方法提升性能并带来经济收益，市场具有复杂动态及供应驱动且受监管影响的特点。

Abstract: With stakeholder-level in-market data, we conduct a comparative analysis of
machine learning (ML) for forecasting electricity prices in Singapore, spanning
15 individual models and 4 ensemble approaches. Our empirical findings justify
the three virtues of ML models: (1) the virtue of capturing non-linearity, (2)
the complexity (Kelly et al., 2024) and (3) the l2-norm and bagging techniques
in a weak factor environment (Shen and Xiu, 2024). Simulation also supports the
first virtue. Penalizing prediction correlation improves ensemble performance
when individual models are highly correlated. The predictability can be
translated into sizable economic gains under the mean-variance framework. We
also reveal significant patterns of time-series heterogeneous predictability
across macro regimes: predictability is clustered in expansion, volatile market
and extreme geopolitical risk periods. Our feature importance results agree
with the complex dynamics of Singapore's electricity market after de
regulation, yet highlight its relatively supply-driven nature with the
continued presence of strong regulatory influences.

</details>


### [183] [A Flexible Measure of Voter Polarization](https://arxiv.org/abs/2507.07770)
*Boris Ginzburg*

Main category: econ.GN

TL;DR: 本文定义了选民围绕特定中心点的意识形态极化，将其应用于美国选民调查数据，分析极化变化并探讨与其他现象的关联。


<details>
  <summary>Details</summary>
Motivation: 提出一种灵活的选民意识形态极化测量方法，以分析围绕任意感兴趣点的极化情况。

Method: 定义极化测量方法并应用于2004 - 2020年美国选民调查数据。

Result: 右倾选民与其他选民极化逐渐增加，左倾选民极化先稳定后陡增；选举后左倾立场极化降低，右倾立场极化增加；能找出极化变化最大的分裂点。

Conclusion: 该测量方法可有效分析意识形态极化，且意识形态极化与情感极化等现象有关。

Abstract: This paper introduces a definition of ideological polarization of an
electorate around a particular central point. By being flexible about the
location or width of the center, this measure enables the researcher to analyze
polarization around any point of interest. The paper then applies this approach
to US voter survey data between 2004 and 2020, showing how polarization between
right-of-center voters and the rest of the electorate was increasing gradually,
while polarization between left-wingers and the rest was originally constant
and then rose steeply. It also shows how, following elections, polarization
around left-wing positions decreased while polarization around right-wing
positions increased. Furthermore, the paper shows how this measure can be used
to find cleavage points around which polarization changed the most. I then show
how ideological polarization as defined here is related to other phenomena,
such as affective polarization and increased salience of divisive issues.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [184] [Multilayer GNN for Predictive Maintenance and Clustering in Power Grids](https://arxiv.org/abs/2507.07298)
*Muhammad Kazim,Harun Pirim,Chau Le,Trung Le,Om Prakash Yadav*

Main category: eess.SY

TL;DR: 本文引入多层图神经网络框架用于改进预测性维护和变电站聚类，模型预测性能佳，聚类效果好，支持主动电网管理。


<details>
  <summary>Details</summary>
Motivation: 现有预测性维护模型忽视电网故障的时空和因果依赖，导致美国每年因意外停电损失超1500亿美元，需要改进。

Method: 引入多层图神经网络框架，融合图注意力网络、图卷积网络和图同构网络，通过注意力加权嵌入融合；使用HDBSCAN聚类进行弹性分析。

Result: 模型30天F1分数为0.8935 +/- 0.0258，优于XGBoost、随机森林和单层GNN；移除因果层性能下降；聚类确定8个运行风险组，聚类效果优于K-Means和谱聚类。

Conclusion: 该工作通过改进故障预测和风险感知的变电站聚类，支持主动电网管理。

Abstract: Unplanned power outages cost the US economy over $150 billion annually,
partly due to predictive maintenance (PdM) models that overlook spatial,
temporal, and causal dependencies in grid failures. This study introduces a
multilayer Graph Neural Network (GNN) framework to enhance PdM and enable
resilience-based substation clustering. Using seven years of incident data from
Oklahoma Gas & Electric (292,830 records across 347 substations), the framework
integrates Graph Attention Networks (spatial), Graph Convolutional Networks
(temporal), and Graph Isomorphism Networks (causal), fused through
attention-weighted embeddings. Our model achieves a 30-day F1-score of 0.8935
+/- 0.0258, outperforming XGBoost and Random Forest by 3.2% and 2.7%, and
single-layer GNNs by 10 to 15 percent. Removing the causal layer drops
performance to 0.7354 +/- 0.0418. For resilience analysis, HDBSCAN clustering
on HierarchicalRiskGNN embeddings identifies eight operational risk groups. The
highest-risk cluster (Cluster 5, 44 substations) shows 388.4 incidents/year and
602.6-minute recovery time, while low-risk groups report fewer than 62
incidents/year. ANOVA (p < 0.0001) confirms significant inter-cluster
separation. Our clustering outperforms K-Means and Spectral Clustering with a
Silhouette Score of 0.626 and Davies-Bouldin index of 0.527. This work supports
proactive grid management through improved failure prediction and risk-aware
substation clustering.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [185] [Concentration of measure for non-linear random matrices with applications to neural networks and non-commutative polynomials](https://arxiv.org/abs/2507.07625)
*Radosław Adamczak*

Main category: math.PR

TL;DR: 证明非线性随机矩阵模型的集中不等式，得到神经网络共轭核和随机矩阵中非交换多项式的线性谱统计估计。


<details>
  <summary>Details</summary>
Motivation: 研究非线性随机矩阵及相关统计量的性质。

Method: 证明集中不等式。

Result: 得到神经网络共轭核和随机矩阵中非交换多项式的线性谱统计估计。

Conclusion: 通过集中不等式可对相关模型的线性谱统计量进行估计。

Abstract: We prove concentration inequalities for several models of non-linear random
matrices. As corollaries we obtain estimates for linear spectral statistics of
the conjugate kernel of neural networks and non-commutative polynomials in
(possibly dependent) random matrices.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [186] [Learning Pole Structures of Hadronic States using Predictive Uncertainty Estimation](https://arxiv.org/abs/2507.07668)
*Felix Frohnert,Denny Lane B. Sombrillo,Evert van Nieuwenburg,Patrick Emonts*

Main category: hep-ph

TL;DR: 提出不确定性感知机器学习方法对S矩阵元素的极点结构分类，在合成数据上训练后应用于实验数据，框架适用于其他强子态。


<details>
  <summary>Details</summary>
Motivation: 强子光谱学中匹配理论预测和实验数据有挑战，极点结构与线形映射在阈值附近模糊，需有效方法分类极点结构。

Method: 基于分类器链集成，提供认知和随机不确定性估计，应用基于预测不确定性的拒绝准则。

Result: 验证准确率近95%，舍弃少量高不确定性预测，模型推广到实验数据，推断出P₍c̅c₎(4312)⁺态的四极点结构。

Conclusion: 框架广泛适用于其他候选强子态，为散射振幅的极点结构推断提供可扩展工具。

Abstract: Matching theoretical predictions to experimental data remains a central
challenge in hadron spectroscopy. In particular, the identification of new
hadronic states is difficult, as exotic signals near threshold can arise from a
variety of physical mechanisms. A key diagnostic in this context is the pole
structure of the scattering amplitude, but different configurations can produce
similar signatures. The mapping between pole configurations and line shapes is
especially ambiguous near the mass threshold, where analytic control is
limited. In this work, we introduce an uncertainty-aware machine learning
approach for classifying pole structures in $S$-matrix elements. Our method is
based on an ensemble of classifier chains that provide both epistemic and
aleatoric uncertainty estimates. We apply a rejection criterion based on
predictive uncertainty, achieving a validation accuracy of nearly $95\%$ while
discarding only a small fraction of high-uncertainty predictions. Trained on
synthetic data with known pole structures, the model generalizes to previously
unseen experimental data, including enhancements associated with the
$P_{c\bar{c}}(4312)^+$ state observed by LHCb. In this, we infer a four-pole
structure, representing the presence of a genuine compact pentaquark in the
presence of a higher channel virtual state pole with non-vanishing width. While
evaluated on this particular state, our framework is broadly applicable to
other candidate hadronic states and offers a scalable tool for pole structure
inference in scattering amplitudes.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [187] [IML-Spikeformer: Input-aware Multi-Level Spiking Transformer for Speech Processing](https://arxiv.org/abs/2507.07396)
*Zeyang Song,Shimin Zhang,Yuhong Chou,Jibin Wu,Haizhou Li*

Main category: cs.MM

TL;DR: 提出用于大规模语音处理的IML - Spikeformer架构，在性能和能效上取得进展。


<details>
  <summary>Details</summary>
Motivation: 现有SNN架构在大规模语音处理任务中难以取得有竞争力的性能，存在训练计算开销大、缺乏适配架构两个挑战。

Method: 引入Input - aware Multi - Level Spike机制，将Reparameterized Spiking Self - Attention模块与Hierarchical Decay Mask集成形成HD - RepSSA模块。

Result: 在AiShell - 1和Librispeech - 960上达到6.0%和3.4%的字错误率，理论推理能耗分别降低4.64倍和4.32倍。

Conclusion: IML - Spikeformer在大规模语音处理的可扩展SNN架构方面，在任务性能和能源效率上均有进步。

Abstract: Spiking Neural Networks (SNNs), inspired by biological neural mechanisms,
represent a promising neuromorphic computing paradigm that offers
energy-efficient alternatives to traditional Artificial Neural Networks (ANNs).
Despite proven effectiveness, SNN architectures have struggled to achieve
competitive performance on large-scale speech processing task. Two key
challenges hinder progress: (1) the high computational overhead during training
caused by multi-timestep spike firing, and (2) the absence of large-scale SNN
architectures tailored to speech processing tasks. To overcome the issues, we
introduce Input-aware Multi-Level Spikeformer, i.e. IML-Spikeformer, a spiking
Transformer architecture specifically designed for large-scale speech
processing. Central to our design is the Input-aware Multi-Level Spike (IMLS)
mechanism, which simulate multi-timestep spike firing within a single timestep
using an adaptive, input-aware thresholding scheme. IML-Spikeformer further
integrates a Reparameterized Spiking Self-Attention (RepSSA) module with a
Hierarchical Decay Mask (HDM), forming the HD-RepSSA module. This module
enhances the precision of attention maps and enables modeling of multi-scale
temporal dependencies in speech signals. Experiments demonstrate that
IML-Spikeformer achieves word error rates of 6.0\% on AiShell-1 and 3.4\% on
Librispeech-960, comparable to conventional ANN transformers while reducing
theoretical inference energy consumption by 4.64$\times$ and 4.32$\times$
respectively. IML-Spikeformer marks an advance of scalable SNN architectures
for large-scale speech processing in both task performance and energy
efficiency.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [188] [The Cross-Lingual Cost: Retrieval Biases in RAG over Arabic-English Corpora](https://arxiv.org/abs/2507.07543)
*Chen Amiraz,Yaroslav Fyodorov,Elad Haramaty,Zohar Karnin,Liane Lewin-Eytan*

Main category: cs.CL

TL;DR: 研究特定领域阿拉伯语 - 英语跨语言检索增强生成（RAG），发现检索是瓶颈，提出策略提升性能。


<details>
  <summary>Details</summary>
Motivation: 以往跨语言RAG研究多依赖开放领域基准，存在检索挑战被隐藏问题，故研究特定领域阿拉伯语 - 英语RAG。

Method: 使用来自真实企业数据集的基准，包含用户查询和支持文档语言的所有组合，进行系统研究。

Result: 发现检索是特定领域跨语言场景的瓶颈，查询和文档语言不同时性能下降，原因是检索器跨语言排名文档困难。

Conclusion: 提出简单检索策略，可提升跨语言和整体性能，为实际RAG应用改善多语言检索提供机会。

Abstract: Cross-lingual retrieval-augmented generation (RAG) is a critical capability
for retrieving and generating answers across languages. Prior work in this
context has mostly focused on generation and relied on benchmarks derived from
open-domain sources, most notably Wikipedia. In such settings, retrieval
challenges often remain hidden due to language imbalances, overlap with
pretraining data, and memorized content. To address this gap, we study
Arabic-English RAG in a domain-specific setting using benchmarks derived from
real-world corporate datasets. Our benchmarks include all combinations of
languages for the user query and the supporting document, drawn independently
and uniformly at random. This enables a systematic study of multilingual
retrieval behavior.
  Our findings reveal that retrieval is a critical bottleneck in cross-lingual
domain-specific scenarios, with significant performance drops occurring when
the user query and supporting document languages differ. A key insight is that
these failures stem primarily from the retriever's difficulty in ranking
documents across languages. Finally, we propose a simple retrieval strategy
that addresses this source of failure by enforcing equal retrieval from both
languages, resulting in substantial improvements in cross-lingual and overall
performance. These results highlight meaningful opportunities for improving
multilingual retrieval, particularly in practical, real-world RAG applications.

</details>


### [189] [Rethinking the Privacy of Text Embeddings: A Reproducibility Study of "Text Embeddings Reveal (Almost) As Much As Text"](https://arxiv.org/abs/2507.07700)
*Dominykas Seputis,Yongkang Li,Karsten Langerak,Serghei Mihailov*

Main category: cs.CL

TL;DR: 本文复现Vec2Text框架，从验证原结论和拓展研究两方面评估，发现Vec2Text在理想条件下有效，但有局限性，高斯噪声和量化技术可降低隐私风险。


<details>
  <summary>Details</summary>
Motivation: Vec2Text意外强的结果促使作者考虑高维嵌入空间特性，对其进行进一步验证。

Method: 复现Vec2Text框架，从验证原声明和拓展研究两方面评估，包括参数敏感性分析、评估敏感输入重建可行性、探索嵌入量化作为隐私防御。

Result: Vec2Text在理想条件下有效，能重建无明确语义序列，但对输入序列长度敏感，高斯噪声和量化技术可降低隐私风险。

Conclusion: 使用文本嵌入需谨慎，应进一步研究NLP系统的鲁棒防御机制。

Abstract: Text embeddings are fundamental to many natural language processing (NLP)
tasks, extensively applied in domains such as recommendation systems and
information retrieval (IR). Traditionally, transmitting embeddings instead of
raw text has been seen as privacy-preserving. However, recent methods such as
Vec2Text challenge this assumption by demonstrating that controlled decoding
can successfully reconstruct original texts from black-box embeddings. The
unexpectedly strong results reported by Vec2Text motivated us to conduct
further verification, particularly considering the typically non-intuitive and
opaque structure of high-dimensional embedding spaces. In this work, we
reproduce the Vec2Text framework and evaluate it from two perspectives: (1)
validating the original claims, and (2) extending the study through targeted
experiments. First, we successfully replicate the original key results in both
in-domain and out-of-domain settings, with only minor discrepancies arising due
to missing artifacts, such as model checkpoints and dataset splits.
Furthermore, we extend the study by conducting a parameter sensitivity
analysis, evaluating the feasibility of reconstructing sensitive inputs (e.g.,
passwords), and exploring embedding quantization as a lightweight privacy
defense. Our results show that Vec2Text is effective under ideal conditions,
capable of reconstructing even password-like sequences that lack clear
semantics. However, we identify key limitations, including its sensitivity to
input sequence length. We also find that Gaussian noise and quantization
techniques can mitigate the privacy risks posed by Vec2Text, with quantization
offering a simpler and more widely applicable solution. Our findings emphasize
the need for caution in using text embeddings and highlight the importance of
further research into robust defense mechanisms for NLP systems.

</details>


### [190] [DTECT: Dynamic Topic Explorer & Context Tracker](https://arxiv.org/abs/2507.07910)
*Suman Adhya,Debarshi Kumar Sanyal*

Main category: cs.CL

TL;DR: 本文介绍了端到端系统DTECT，可从文本数据中获取时间洞察，增强可解释性，助力用户追踪和理解主题动态，且开源。


<details>
  <summary>Details</summary>
Motivation: 文本数据增长使挖掘主题和趋势面临挑战，现有动态主题建模技术存在碎片化且缺乏解释和友好探索支持。

Method: 引入DTECT系统，提供统一工作流，支持数据预处理、多模型架构和评估指标，通过LLM自动主题标签、时间显著词趋势分析、可视化和自然语言聊天界面增强可解释性。

Result: DTECT将多种功能集成到一个平台，有效帮助用户追踪和理解主题动态。

Conclusion: DTECT解决了现有技术问题，增强了主题分析的可解释性和用户体验，且开源方便使用。

Abstract: The explosive growth of textual data over time presents a significant
challenge in uncovering evolving themes and trends. Existing dynamic topic
modeling techniques, while powerful, often exist in fragmented pipelines that
lack robust support for interpretation and user-friendly exploration. We
introduce DTECT (Dynamic Topic Explorer & Context Tracker), an end-to-end
system that bridges the gap between raw textual data and meaningful temporal
insights. DTECT provides a unified workflow that supports data preprocessing,
multiple model architectures, and dedicated evaluation metrics to analyze the
topic quality of temporal topic models. It significantly enhances
interpretability by introducing LLM-driven automatic topic labeling, trend
analysis via temporally salient words, interactive visualizations with
document-level summarization, and a natural language chat interface for
intuitive data querying. By integrating these features into a single, cohesive
platform, DTECT empowers users to more effectively track and understand
thematic dynamics. DTECT is open-source and available at
https://github.com/AdhyaSuman/DTECT.

</details>


### [191] [Why is Your Language Model a Poor Implicit Reward Model?](https://arxiv.org/abs/2507.07981)
*Noam Razin,Yong Lin,Jiarui Yao,Sanjeev Arora*

Main category: cs.CL

TL;DR: 研究隐式奖励模型（IM - RM）和显式奖励模型（EX - RM）泛化差距的原因，发现IM - RM更依赖表层词元线索导致泛化能力差。


<details>
  <summary>Details</summary>
Motivation: 理解不同奖励模型类型潜在的隐式偏差，探究IM - RM和EX - RM泛化差距的根本原因。

Method: 理论分析和实验研究。

Result: IM - RM更依赖表层词元线索，在词元分布变化及分布内泛化能力均不如EX - RM，并反驳了泛化差距的其他假设。

Conclusion: 奖励模型看似微小的设计选择会显著影响其泛化行为。

Abstract: Reward models are key to language model post-training and inference
pipelines. Conveniently, recent work showed that every language model defines
an implicit reward model (IM-RM), without requiring any architectural changes.
However, such IM-RMs tend to generalize worse, especially out-of-distribution,
compared to explicit reward models (EX-RMs) that apply a dedicated linear head
over the hidden representations of a language model. The existence of a
generalization gap is puzzling, as EX-RMs and IM-RMs are nearly identical. They
can be trained using the same data, loss function, and language model, and
differ only in how the reward is computed. Towards a fundamental understanding
of the implicit biases underlying different reward model types, we investigate
the root cause of this gap. Our main finding, backed by theory and experiments,
is that IM-RMs rely more heavily on superficial token-level cues. Consequently,
they often generalize worse than EX-RMs under token-level distribution shifts,
as well as in-distribution. Furthermore, we provide evidence against
alternative hypotheses for the generalization gap. Most notably, we challenge
the intuitive claim that IM-RMs struggle in tasks where generation is harder
than verification because they can operate both as a verifier and a generator.
Taken together, our results highlight that seemingly minor design choices can
substantially impact the generalization behavior of reward models.

</details>


### [192] [Planted in Pretraining, Swayed by Finetuning: A Case Study on the Origins of Cognitive Biases in LLMs](https://arxiv.org/abs/2507.07186)
*Itay Itzhak,Yonatan Belinkov,Gabriel Stanovsky*

Main category: cs.CL

TL;DR: 研究大语言模型认知偏差来源，发现偏差主要由预训练塑造。


<details>
  <summary>Details</summary>
Motivation: 不清楚大语言模型认知偏差差异源于预训练、微调还是训练随机性噪音。

Method: 采用两步因果实验法，多次用不同随机种子微调模型研究训练随机性影响，引入交叉调优交换指令数据集隔离偏差来源。

Result: 训练随机性有一定影响，但偏差主要由预训练塑造，相同预训练骨干的模型偏差模式更相似。

Conclusion: 理解微调模型偏差需考虑预训练起源，此观点可指导评估和减轻大语言模型偏差的策略开发。

Abstract: Large language models (LLMs) exhibit cognitive biases -- systematic
tendencies of irrational decision-making, similar to those seen in humans.
Prior work has found that these biases vary across models and can be amplified
by instruction tuning. However, it remains unclear if these differences in
biases stem from pretraining, finetuning, or even random noise due to training
stochasticity. We propose a two-step causal experimental approach to
disentangle these factors. First, we finetune models multiple times using
different random seeds to study how training randomness affects over $30$
cognitive biases. Second, we introduce \emph{cross-tuning} -- swapping
instruction datasets between models to isolate bias sources. This swap uses
datasets that led to different bias patterns, directly testing whether biases
are dataset-dependent. Our findings reveal that while training randomness
introduces some variability, biases are mainly shaped by pretraining: models
with the same pretrained backbone exhibit more similar bias patterns than those
sharing only finetuning data. These insights suggest that understanding biases
in finetuned models requires considering their pretraining origins beyond
finetuning effects. This perspective can guide future efforts to develop
principled strategies for evaluating and mitigating bias in LLMs.

</details>


### [193] [Prompt Perturbations Reveal Human-Like Biases in LLM Survey Responses](https://arxiv.org/abs/2507.07188)
*Jens Rupprecht,Georg Ahnert,Markus Strohmaier*

Main category: cs.CL

TL;DR: 本文研究大语言模型在规范调查背景下的回答稳健性，对9种大语言模型进行测试，揭示其对扰动的脆弱性、近因偏差，强调使用大模型生成合成调查数据时提示设计和稳健性测试的重要性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为社会科学调查中人类主体的代理越来越常用，但对其可靠性和易受已知回答偏差影响的情况了解不足，因此研究其在规范调查背景下的回答稳健性。

Method: 对9种不同的大语言模型在世界价值观调查（WVS）的问题上进行测试，对问题表述和答案选项结构应用11种扰动，进行超167000次模拟访谈。

Result: 揭示了大语言模型对扰动的脆弱性，所有测试模型都存在不同强度的近因偏差，更倾向于最后呈现的答案选项；较大模型通常更稳健，但所有模型对语义变化和组合扰动仍敏感；大语言模型部分符合人类调查回答偏差。

Conclusion: 使用大语言模型生成合成调查数据时，提示设计和稳健性测试至关重要。

Abstract: Large Language Models (LLMs) are increasingly used as proxies for human
subjects in social science surveys, but their reliability and susceptibility to
known response biases are poorly understood. This paper investigates the
response robustness of LLMs in normative survey contexts -- we test nine
diverse LLMs on questions from the World Values Survey (WVS), applying a
comprehensive set of 11 perturbations to both question phrasing and answer
option structure, resulting in over 167,000 simulated interviews. In doing so,
we not only reveal LLMs' vulnerabilities to perturbations but also reveal that
all tested models exhibit a consistent \textit{recency bias} varying in
intensity, disproportionately favoring the last-presented answer option. While
larger models are generally more robust, all models remain sensitive to
semantic variations like paraphrasing and to combined perturbations. By
applying a set of perturbations, we reveal that LLMs partially align with
survey response biases identified in humans. This underscores the critical
importance of prompt design and robustness testing when using LLMs to generate
synthetic survey data.

</details>


### [194] [GNN-CNN: An Efficient Hybrid Model of Convolutional and Graph Neural Networks for Text Representation](https://arxiv.org/abs/2507.07414)
*Fardin Rastakhiz*

Main category: cs.CL

TL;DR: 提出结合GNN和CNN的新模型处理长文本，融入LLM信息，在多任务表现高效有竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer处理长文本计算复杂度高、效率低，需新模型解决问题。

Method: 结合GNN和CNN，有实时端到端图生成机制，处理字符级输入，融入LLM信息，用CNN捕捉局部模式，通过图结构扩展感受野和聚合信息。

Result: 生成图有有意义语义组织的结构特性，在多文本分类任务实验结果证实模型高效且有竞争力。

Conclusion: 提出的模型在处理长文本时效率高，有较好性能。

Abstract: Time, cost, and energy efficiency are critical considerations in
Deep-Learning (DL), particularly when processing long texts. Transformers,
which represent the current state of the art, exhibit quadratic computational
complexity relative to input length, making them inefficient for extended
documents. This study introduces a novel model architecture that combines Graph
Neural Networks (GNNs) and Convolutional Neural Networks (CNNs), integrated
with a real-time, end-to-end graph generation mechanism. The model processes
compact batches of character-level inputs without requiring padding or
truncation. To enhance performance while maintaining high speed and efficiency,
the model incorporates information from Large Language Models (LLMs), such as
token embeddings and sentiment polarities, through efficient dictionary
lookups. It captures local contextual patterns using CNNs, expands local
receptive fields via lattice-based graph structures, and employs small-world
graphs to aggregate document-level information. The generated graphs exhibit
structural properties indicative of meaningful semantic organization, with an
average clustering coefficient of approximately 0.45 and an average shortest
path length ranging between 4 and 5. The model is evaluated across multiple
text classification tasks, including sentiment analysis and
news-categorization, and is compared against state-of-the-art models.
Experimental results confirm the proposed model's efficiency and competitive
performance.

</details>


### [195] [MedReadCtrl: Personalizing medical text generation with readability-controlled instruction learning](https://arxiv.org/abs/2507.07419)
*Hieu Tran,Zonghai Yao,Won Seok Jang,Sharmin Sultana,Allen Chang,Yuan Zhang,Hong Yu*

Main category: cs.CL

TL;DR: 介绍MedReadCtrl框架，可控制大语言模型输出可读性，在多数据集和任务中表现优于GPT - 4，受专家青睐，能支持患者教育和扩大AI医疗公平性。


<details>
  <summary>Details</summary>
Motivation: 解决生成式AI在医疗领域部署时有效的人机沟通问题，使内容个性化且易懂。

Method: 引入MedReadCtrl可读性控制指令调优框架。

Result: 在九个数据集和三项任务评估中，MedReadCtrl比GPT - 4的可读性指令跟随错误显著降低，在未见过的临床任务中有大幅提升，专家更青睐。

Conclusion: MedReadCtrl能将临床内容重构为易读语言且保留医疗意图，为患者教育和扩大AI医疗公平性提供可扩展解决方案。

Abstract: Generative AI has demonstrated strong potential in healthcare, from clinical
decision support to patient-facing chatbots that improve outcomes. A critical
challenge for deployment is effective human-AI communication, where content
must be both personalized and understandable. We introduce MedReadCtrl, a
readability-controlled instruction tuning framework that enables LLMs to adjust
output complexity without compromising meaning. Evaluations of nine datasets
and three tasks across medical and general domains show that MedReadCtrl
achieves significantly lower readability instruction-following errors than
GPT-4 (e.g., 1.39 vs. 1.59 on ReadMe, p<0.001) and delivers substantial gains
on unseen clinical tasks (e.g., +14.7 ROUGE-L, +6.18 SARI on MTSamples).
Experts consistently preferred MedReadCtrl (71.7% vs. 23.3%), especially at low
literacy levels. These gains reflect MedReadCtrl's ability to restructure
clinical content into accessible, readability-aligned language while preserving
medical intent, offering a scalable solution to support patient education and
expand equitable access to AI-enabled care.

</details>


### [196] [SynthEHR-Eviction: Enhancing Eviction SDoH Detection with LLM-Augmented Synthetic EHR Data](https://arxiv.org/abs/2507.07421)
*Zonghai Yao,Youxia Zhao,Avijit Mitra,David A. Levy,Emily Druhl,Jack Tsai,Hong Yu*

Main category: cs.CL

TL;DR: 提出SynthEHR - Eviction管道从临床笔记提取驱逐状态，创建相关数据集，微调模型表现优，减少标注工作量。


<details>
  <summary>Details</summary>
Motivation: 驱逐是重要但研究不足的健康社会决定因素，未在电子健康记录结构化字段编码，限制下游应用。

Method: 结合大语言模型、人工标注和自动提示优化的SynthEHR - Eviction管道。

Result: 创建最大公共驱逐相关SDoH数据集，微调模型Macro - F1分数高，优于其他模型，减少标注工作量超80%。

Conclusion: 该管道可实现可扩展的驱逐检测，推广到其他信息提取任务。

Abstract: Eviction is a significant yet understudied social determinants of health
(SDoH), linked to housing instability, unemployment, and mental health. While
eviction appears in unstructured electronic health records (EHRs), it is rarely
coded in structured fields, limiting downstream applications. We introduce
SynthEHR-Eviction, a scalable pipeline combining LLMs, human-in-the-loop
annotation, and automated prompt optimization (APO) to extract eviction
statuses from clinical notes. Using this pipeline, we created the largest
public eviction-related SDoH dataset to date, comprising 14 fine-grained
categories. Fine-tuned LLMs (e.g., Qwen2.5, LLaMA3) trained on
SynthEHR-Eviction achieved Macro-F1 scores of 88.8% (eviction) and 90.3% (other
SDoH) on human validated data, outperforming GPT-4o-APO (87.8%, 87.3%),
GPT-4o-mini-APO (69.1%, 78.1%), and BioBERT (60.7%, 68.3%), while enabling
cost-effective deployment across various model sizes. The pipeline reduces
annotation effort by over 80%, accelerates dataset creation, enables scalable
eviction detection, and generalizes to other information extraction tasks.

</details>


### [197] [Towards Interpretable Time Series Foundation Models](https://arxiv.org/abs/2507.07439)
*Matthieu Boileau,Philippe Helluy,Jeremy Pawlus,Svitlana Vyetrenko*

Main category: cs.CL

TL;DR: 本文研究将时间序列推理能力蒸馏到小型指令调优语言模型，以构建可解释时间序列基础模型，用合成数据集微调模型，评估显示模型获解释能力，证明压缩时间序列理解到轻量级模型的可行性。


<details>
  <summary>Details</summary>
Motivation: 构建可解释的时间序列基础模型，将时间序列推理能力蒸馏到小型指令调优语言模型。

Method: 利用有不同趋势和噪声水平的合成数据集，用大模型生成自然语言注释，监督微调紧凑型Qwen模型，引入评估指标。

Result: 微调后的模型获得有意义的解释能力，证明将时间序列理解压缩到轻量级模型用于设备端或隐私敏感部署是可行的。

Conclusion: 为开发能用自然语言解释时间模式的小型可解释模型奠定基础。

Abstract: In this paper, we investigate the distillation of time series reasoning
capabilities into small, instruction-tuned language models as a step toward
building interpretable time series foundation models. Leveraging a synthetic
dataset of mean-reverting time series with systematically varied trends and
noise levels, we generate natural language annotations using a large multimodal
model and use these to supervise the fine-tuning of compact Qwen models. We
introduce evaluation metrics that assess the quality of the distilled reasoning
- focusing on trend direction, noise intensity, and extremum localization - and
show that the post-trained models acquire meaningful interpretive capabilities.
Our results highlight the feasibility of compressing time series understanding
into lightweight, language-capable models suitable for on-device or
privacy-sensitive deployment. This work contributes a concrete foundation
toward developing small, interpretable models that explain temporal patterns in
natural language.

</details>


### [198] [Machine Bullshit: Characterizing the Emergent Disregard for Truth in Large Language Models](https://arxiv.org/abs/2507.07484)
*Kaiqu Liang,Haimin Hu,Xuandong Zhao,Dawn Song,Thomas L. Griffiths,Jaime Fernández Fisac*

Main category: cs.CL

TL;DR: 提出机器废话概念框架、废话指数和分类法，通过多数据集评估发现模型微调与思维链提示会加剧机器废话，凸显AI对齐挑战。


<details>
  <summary>Details</summary>
Motivation: 以往研究关注大语言模型幻觉和谄媚，本文提出机器废话概念框架以刻画大语言模型失去真实性的更广泛现象并揭示其潜在机制。

Method: 引入废话指数量化大语言模型对真理的漠视，提出分类法分析四种废话形式，在多个数据集和新基准上进行实证评估。

Result: 基于人类反馈的强化学习模型微调显著加剧废话，思维链提示放大特定废话形式，政治语境中机器废话普遍，含糊其辞是主要策略。

Conclusion: 研究结果凸显了AI对齐的系统性挑战，为实现大语言模型更真实的行为提供新见解。

Abstract: Bullshit, as conceptualized by philosopher Harry Frankfurt, refers to
statements made without regard to their truth value. While previous work has
explored large language model (LLM) hallucination and sycophancy, we propose
machine bullshit as an overarching conceptual framework that can allow
researchers to characterize the broader phenomenon of emergent loss of
truthfulness in LLMs and shed light on its underlying mechanisms. We introduce
the Bullshit Index, a novel metric quantifying LLMs' indifference to truth, and
propose a complementary taxonomy analyzing four qualitative forms of bullshit:
empty rhetoric, paltering, weasel words, and unverified claims. We conduct
empirical evaluations on the Marketplace dataset, the Political Neutrality
dataset, and our new BullshitEval benchmark (2,400 scenarios spanning 100 AI
assistants) explicitly designed to evaluate machine bullshit. Our results
demonstrate that model fine-tuning with reinforcement learning from human
feedback (RLHF) significantly exacerbates bullshit and inference-time
chain-of-thought (CoT) prompting notably amplify specific bullshit forms,
particularly empty rhetoric and paltering. We also observe prevalent machine
bullshit in political contexts, with weasel words as the dominant strategy. Our
findings highlight systematic challenges in AI alignment and provide new
insights toward more truthful LLM behavior.

</details>


### [199] [PLAN-TUNING: Post-Training Language Models to Learn Step-by-Step Planning for Complex Problem Solving](https://arxiv.org/abs/2507.07495)
*Mihir Parmar,Palash Goyal,Xin Liu,Yiwen Song,Mingyang Ling,Chitta Baral,Hamid Palangi,Tomas Pfister*

Main category: cs.CL

TL;DR: 介绍PLAN - TUNING框架提升小型开源大语言模型复杂推理性能，在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 利用规划结构在训练后提升小型开源大语言模型性能的研究不足，有必要探索相关方法。

Method: 提出PLAN - TUNING统一后训练框架，从大规模大语言模型中提取合成任务分解，通过监督和强化学习目标微调小型模型。

Result: 在GSM8k和MATH基准测试中，经PLAN - TUNING调整的模型平均优于强基线约7%；在域外数据集上泛化能力更好，在OlympiadBench和AIME 2024上分别平均提升约10%和12%。

Conclusion: PLAN - TUNING是提升小型大语言模型特定任务性能的有效策略。

Abstract: Recently, decomposing complex problems into simple subtasks--a crucial part
of human-like natural planning--to solve the given problem has significantly
boosted the performance of large language models (LLMs). However, leveraging
such planning structures during post-training to boost the performance of
smaller open-source LLMs remains underexplored. Motivated by this, we introduce
PLAN-TUNING, a unified post-training framework that (i) distills synthetic task
decompositions (termed "planning trajectories") from large-scale LLMs and (ii)
fine-tunes smaller models via supervised and reinforcement-learning objectives
designed to mimic these planning processes to improve complex reasoning. On
GSM8k and the MATH benchmarks, plan-tuned models outperform strong baselines by
an average $\sim7\%$. Furthermore, plan-tuned models show better generalization
capabilities on out-of-domain datasets, with average $\sim10\%$ and $\sim12\%$
performance improvements on OlympiadBench and AIME 2024, respectively. Our
detailed analysis demonstrates how planning trajectories improves complex
reasoning capabilities, showing that PLAN-TUNING is an effective strategy for
improving task-specific performance of smaller LLMs.

</details>


### [200] [Hallucination Stations: On Some Basic Limitations of Transformer-Based Language Models](https://arxiv.org/abs/2507.07505)
*Varin Sikka,Vishal Sikka*

Main category: cs.CL

TL;DR: 从计算复杂度角度探讨大语言模型（LLMs）能力边界，指出其在处理特定复杂度以上任务及验证任务准确性上存在局限，并举例说明及讨论影响。


<details>
  <summary>Details</summary>
Motivation: 随着基于Transformer的语言模型广泛应用，人们关注LLMs能力边界，尤其是幻觉问题及代理用途，因此需了解其能执行和不能执行的任务类型。

Method: 从LLM推理的计算复杂度角度进行探索。

Result: LLMs无法执行特定复杂度以上的计算和代理任务，也无法验证特定复杂度以上任务的准确性，并给出了相关示例。

Conclusion: 文章虽未明确结论表述，但可推测理解LLMs能力边界对其应用和发展有重要意义。

Abstract: With widespread adoption of transformer-based language models in AI, there is
significant interest in the limits of LLMs capabilities, specifically so-called
hallucinations, occurrences in which LLMs provide spurious, factually incorrect
or nonsensical information when prompted on certain subjects. Furthermore,
there is growing interest in agentic uses of LLMs - that is, using LLMs to
create agents that act autonomously or semi-autonomously to carry out various
tasks, including tasks with applications in the real world. This makes it
important to understand the types of tasks LLMs can and cannot perform. We
explore this topic from the perspective of the computational complexity of LLM
inference. We show that LLMs are incapable of carrying out computational and
agentic tasks beyond a certain complexity, and further that LLMs are incapable
of verifying the accuracy of tasks beyond a certain complexity. We present
examples of both, then discuss some consequences of this work.

</details>


### [201] [Toward Real-World Chinese Psychological Support Dialogues: CPsDD Dataset and a Co-Evolving Multi-Agent System](https://arxiv.org/abs/2507.07509)
*Yuanchen Shi,Longyin Zhang,Fang Kong*

Main category: cs.CL

TL;DR: 针对心理支持领域相关数据集稀缺问题，提出框架微调大模型构建中文心理支持对话数据集CPsDD，并引入综合代理对话支持系统CADSS，实验显示CADSS在相关任务上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决因压力增大对心理支持需求增加而导致的相关数据集稀缺问题，尤其是非英语语言的数据集。

Method: 提出框架利用有限真实世界数据和专家知识微调对话生成器和对话修改器两个大语言模型构建CPsDD；引入CADSS，包含分析用户特征、总结对话历史、选择策略和生成共情回复等组件。

Result: 构建了包含68K对话的CPsDD；CADSS在策略预测和情感支持对话任务的实验中，在CPsDD和ESConv数据集上达到了SOTA性能。

Conclusion: 所提出的方法有效，能够为心理支持领域提供高质量数据集和高性能对话支持系统。

Abstract: The growing need for psychological support due to increasing pressures has
exposed the scarcity of relevant datasets, particularly in non-English
languages. To address this, we propose a framework that leverages limited
real-world data and expert knowledge to fine-tune two large language models:
Dialog Generator and Dialog Modifier. The Generator creates large-scale
psychological counseling dialogues based on predefined paths, which guide
system response strategies and user interactions, forming the basis for
effective support. The Modifier refines these dialogues to align with
real-world data quality. Through both automated and manual review, we construct
the Chinese Psychological support Dialogue Dataset (CPsDD), containing 68K
dialogues across 13 groups, 16 psychological problems, 13 causes, and 12
support focuses. Additionally, we introduce the Comprehensive Agent Dialogue
Support System (CADSS), where a Profiler analyzes user characteristics, a
Summarizer condenses dialogue history, a Planner selects strategies, and a
Supporter generates empathetic responses. The experimental results of the
Strategy Prediction and Emotional Support Conversation (ESC) tasks demonstrate
that CADSS achieves state-of-the-art performance on both CPsDD and ESConv
datasets.

</details>


### [202] [CEA-LIST at CheckThat! 2025: Evaluating LLMs as Detectors of Bias and Opinion in Text](https://arxiv.org/abs/2507.07539)
*Akram Elbouanani,Evan Dufraisse,Aboubacar Tuo,Adrian Popescu*

Main category: cs.CL

TL;DR: 本文提出用大语言模型（LLMs）结合少样本提示进行多语言主观性检测的方法，在CheckThat! 2025评测中取得佳绩，凸显LLM少样本学习用于多语言情感任务的有效性。


<details>
  <summary>Details</summary>
Motivation: 参与CheckThat! 2025评测活动的任务1：主观性检测，探索有效多语言主观性检测方法。

Method: 使用大语言模型结合精心设计的少样本提示，尝试辩论式LLMs和多种示例选择等高级提示工程技术。

Result: 系统在CheckThat! 2025主观性检测任务多语言赛道获顶尖排名，如阿拉伯语和波兰语第一等。在阿拉伯语数据集表现尤其稳健。

Conclusion: 基于LLM的少样本学习对多语言情感任务有效且适应性强，在标注数据稀缺或不一致时是传统微调的有力替代。

Abstract: This paper presents a competitive approach to multilingual subjectivity
detection using large language models (LLMs) with few-shot prompting. We
participated in Task 1: Subjectivity of the CheckThat! 2025 evaluation
campaign. We show that LLMs, when paired with carefully designed prompts, can
match or outperform fine-tuned smaller language models (SLMs), particularly in
noisy or low-quality data settings. Despite experimenting with advanced prompt
engineering techniques, such as debating LLMs and various example selection
strategies, we found limited benefit beyond well-crafted standard few-shot
prompts. Our system achieved top rankings across multiple languages in the
CheckThat! 2025 subjectivity detection task, including first place in Arabic
and Polish, and top-four finishes in Italian, English, German, and multilingual
tracks. Notably, our method proved especially robust on the Arabic dataset,
likely due to its resilience to annotation inconsistencies. These findings
highlight the effectiveness and adaptability of LLM-based few-shot learning for
multilingual sentiment tasks, offering a strong alternative to traditional
fine-tuning, particularly when labeled data is scarce or inconsistent.

</details>


### [203] [Single-to-mix Modality Alignment with Multimodal Large Language Model for Document Image Machine Translation](https://arxiv.org/abs/2507.07572)
*Yupu Liang,Yaping Zhang,Zhiyang Zhang,Yang Zhao,Lu Xiang,Chengqing Zong,Yu Zhou*

Main category: cs.CL

TL;DR: 提出M4Doc框架解决文档图像机器翻译泛化挑战，实验显示翻译质量提升。


<details>
  <summary>Details</summary>
Motivation: 文档图像机器翻译面临训练数据有限和视觉与文本信息复杂交互带来的泛化挑战。

Method: 引入M4Doc框架，将仅图像编码器与多模态大语言模型的多模态表示对齐，预训练于大规模文档图像数据集，推理时绕过多模态大语言模型。

Result: 在翻译质量上有显著提升，尤其在跨领域泛化和具有挑战性的文档图像场景中。

Conclusion: M4Doc框架能有效解决文档图像机器翻译的泛化问题，提升翻译质量。

Abstract: Document Image Machine Translation (DIMT) aims to translate text within
document images, facing generalization challenges due to limited training data
and the complex interplay between visual and textual information. To address
these challenges, we introduce M4Doc, a novel single-to-mix modality alignment
framework leveraging Multimodal Large Language Models (MLLMs). M4Doc aligns an
image-only encoder with the multimodal representations of an MLLM, pre-trained
on large-scale document image datasets. This alignment enables a lightweight
DIMT model to learn crucial visual-textual correlations during training. During
inference, M4Doc bypasses the MLLM, maintaining computational efficiency while
benefiting from its multimodal knowledge. Comprehensive experiments demonstrate
substantial improvements in translation quality, especially in cross-domain
generalization and challenging document image scenarios.

</details>


### [204] [Bayesian Discrete Diffusion Beats Autoregressive Perplexity](https://arxiv.org/abs/2507.07586)
*Cooper Doyle*

Main category: cs.CL

TL;DR: 文章揭示离散扩散语言模型的贝叶斯核心，提出轻量级推理时集成方法，在WikiText - 2上取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 探索离散扩散语言模型的核心原理，提高模型性能及获得后验感知的标记概率和不确定性估计。

Method: 证明前向掩蔽分布下期望去噪器输出可恢复干净标记的精确后验，通过K次独立损坏的蒙特卡罗边缘化收敛到该后验；引入轻量级推理时集成方法，对K次掩蔽 - 去噪过程取平均。

Result: 在WikiText - 2上，K = 8时测试困惑度为8.8，而GPT - 2 Small为20.3，使用了大小相当的模型。

Conclusion: 离散扩散语言模型有隐藏的贝叶斯核心，提出的轻量级推理时集成方法有效，能在不增加训练成本下提升性能。

Abstract: We reveal a hidden Bayesian core of discrete-diffusion language models by
showing that the expected denoiser output under the forward masking
distribution recovers the exact posterior over clean tokens. Under minimal
assumptions, Monte Carlo marginalization over K independent corruptions
converges to this posterior at rate O(1/sqrt(K)), yielding a simple proof of
consistency and finite-sample error bounds. Building on this insight, we
introduce a lightweight inference-time ensemble that averages K
mask-and-denoise passes to obtain posterior-aware token probabilities and
uncertainty estimates at no extra training cost. On WikiText-2, our method
achieves test perplexity 8.8 with K=8, versus 20.3 for GPT-2 Small, despite
using a model of comparable size. Code is available at
https://github.com/mercury0100/bayesradd.

</details>


### [205] [KeyKnowledgeRAG (K^2RAG): An Enhanced RAG method for improved LLM question-answering capabilities](https://arxiv.org/abs/2507.07695)
*Hruday Markondapatnaikuni,Basem Suleiman,Abdelkarim Erradi,Shijing Chen*

Main category: cs.CL

TL;DR: 传统微调大语言模型资源消耗大，RAG 有局限，本文提出 K2RAG 框架，实验显示其在准确率、效率和可扩展性上表现更好。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型微调资源消耗大的问题，克服传统 RAG 方法在可扩展性和答案准确性上的局限。

Method: 引入 K2RAG 框架，集成密集和稀疏向量搜索、知识图谱和文本摘要技术，增加预处理步骤总结训练数据。

Result: 在 MultiHopRAG 数据集上测试，答案相似度得分高，训练时间减少 93%，执行速度快 40%，所需 VRAM 比其他 RAG 实现少三倍。

Conclusion: K2RAG 框架在提高检索质量、系统效率和可扩展性方面表现出色。

Abstract: Fine-tuning is an immensely resource-intensive process when retraining Large
Language Models (LLMs) to incorporate a larger body of knowledge. Although many
fine-tuning techniques have been developed to reduce the time and computational
cost involved, the challenge persists as LLMs continue to grow in size and
complexity. To address this, a new approach to knowledge expansion in LLMs is
needed. Retrieval-Augmented Generation (RAG) offers one such alternative by
storing external knowledge in a database and retrieving relevant chunks to
support question answering. However, naive implementations of RAG face
significant limitations in scalability and answer accuracy. This paper
introduces KeyKnowledgeRAG (K2RAG), a novel framework designed to overcome
these limitations. Inspired by the divide-and-conquer paradigm, K2RAG
integrates dense and sparse vector search, knowledge graphs, and text
summarization to improve retrieval quality and system efficiency. The framework
also includes a preprocessing step that summarizes the training data,
significantly reducing the training time. K2RAG was evaluated using the
MultiHopRAG dataset, where the proposed pipeline was trained on the document
corpus and tested on a separate evaluation set. Results demonstrated notable
improvements over common naive RAG implementations. K2RAG achieved the highest
mean answer similarity score of 0.57, and reached the highest third quartile
(Q3) similarity of 0.82, indicating better alignment with ground-truth answers.
In addition to improved accuracy, the framework proved highly efficient. The
summarization step reduced the average training time of individual components
by 93%, and execution speed was up to 40% faster than traditional knowledge
graph-based RAG systems. K2RAG also demonstrated superior scalability,
requiring three times less VRAM than several naive RAG implementations tested
in this study.

</details>


### [206] [Not All Preferences are What You Need for Post-Training: Selective Alignment Strategy for Preference Optimization](https://arxiv.org/abs/2507.07725)
*Zhijin Dong*

Main category: cs.CL

TL;DR: 提出选择性对齐策略Selective - DPO，减少计算开销、提高对齐保真度，实验验证其优于基线方法，强调词元优化和参考模型选择重要性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型训练后对齐面临挑战，并非所有词元对模型性能贡献相同，需优化对齐策略。

Method: 引入选择性对齐策略，利用当前策略和参考模型间词元级对数概率差异，优先处理偏好对中有高影响力的词元，还探索参考模型质量的作用。

Result: 在Arena - Hard和MT - Bench等基准测试中，Selective - DPO方法优于标准DPO和基于蒸馏的基线方法。

Conclusion: 词元级优化和参考模型选择对推进大语言模型偏好对齐很重要。

Abstract: Post-training alignment of large language models (LLMs) is a critical
challenge, as not all tokens contribute equally to model performance. This
paper introduces a selective alignment strategy that prioritizes high-impact
tokens within preference pairs, leveraging token-level log-probability
differences between the current policy and a reference model. By focusing on
these informative tokens, our approach reduces computational overhead and
enhances alignment fidelity. We further explore the role of reference model
quality, demonstrating that stronger reference models significantly improve
token selection accuracy and overall optimization effectiveness. Comprehensive
experiments on benchmarks such as Arena-Hard and MT-Bench validate the
superiority of our Selective-DPO method over standard DPO and
distillation-based baselines. Our findings highlight the importance of
token-level optimization and reference model selection in advancing preference
alignment for LLMs. The code is available at
https://github.com/Dongzhijin/SDPO.

</details>


### [207] [When Large Language Models Meet Law: Dual-Lens Taxonomy, Technical Advances, and Ethical Governance](https://arxiv.org/abs/2507.07748)
*Peizhang Shao,Linrui Xu,Jinxi Wang,Wei Zhou,Xingyu Wu*

Main category: cs.CL

TL;DR: 本文首次全面综述法律领域的大语言模型，提出创新分类法，记录进展并指出挑战和前沿方向，为法律人工智能奠定基础。


<details>
  <summary>Details</summary>
Motivation: 对法律领域应用的大语言模型进行全面综述，整合历史研究与当代突破。

Method: 采用创新的双视角分类法，结合法律推理框架和专业本体；实现图尔敏论证框架的计算化。

Result: 记录了大语言模型在任务泛化、推理形式化等方面的进展；指出广泛应用带来的幻觉、可解释性不足等挑战。

Conclusion: 提供技术路线图和概念框架，确定关键前沿方向，为法律人工智能新时代奠定基础。

Abstract: This paper establishes the first comprehensive review of Large Language
Models (LLMs) applied within the legal domain. It pioneers an innovative dual
lens taxonomy that integrates legal reasoning frameworks and professional
ontologies to systematically unify historical research and contemporary
breakthroughs. Transformer-based LLMs, which exhibit emergent capabilities such
as contextual reasoning and generative argumentation, surmount traditional
limitations by dynamically capturing legal semantics and unifying evidence
reasoning. Significant progress is documented in task generalization, reasoning
formalization, workflow integration, and addressing core challenges in text
processing, knowledge integration, and evaluation rigor via technical
innovations like sparse attention mechanisms and mixture-of-experts
architectures. However, widespread adoption of LLM introduces critical
challenges: hallucination, explainability deficits, jurisdictional adaptation
difficulties, and ethical asymmetry. This review proposes a novel taxonomy that
maps legal roles to NLP subtasks and computationally implements the Toulmin
argumentation framework, thus systematizing advances in reasoning, retrieval,
prediction, and dispute resolution. It identifies key frontiers including
low-resource systems, multimodal evidence integration, and dynamic rebuttal
handling. Ultimately, this work provides both a technical roadmap for
researchers and a conceptual framework for practitioners navigating the
algorithmic future, laying a robust foundation for the next era of legal
artificial intelligence. We have created a GitHub repository to index the
relevant papers: https://github.com/Kilimajaro/LLMs_Meet_Law.

</details>


### [208] [Bridging Logic and Learning: Decoding Temporal Logic Embeddings via Transformers](https://arxiv.org/abs/2507.07808)
*Sara Candussio,Gaia Saveri,Gabriele Sarti,Luca Bortolussi*

Main category: cs.CL

TL;DR: 本文提出用基于Transformer的仅解码器模型对信号时序逻辑（STL）公式的语义嵌入进行反转，验证了模型有效性并将其用于需求挖掘任务。


<details>
  <summary>Details</summary>
Motivation: 连续逻辑公式表示需可反转才能将最优连续表示转化为具体需求，现有方法在这方面存在不足。

Method: 训练基于Transformer的仅解码器模型反转STL公式的语义嵌入，构建小词汇表。

Result: 模型1个epoch可生成有效公式，10个epoch可泛化逻辑语义，能解码出更简单且语义相近的公式，在不同复杂度训练数据上有效。

Conclusion: 所提方法能有效捕获嵌入中的语义信息，可泛化到分布外数据，能用于解决需求挖掘任务。

Abstract: Continuous representations of logic formulae allow us to integrate symbolic
knowledge into data-driven learning algorithms. If such embeddings are
semantically consistent, i.e. if similar specifications are mapped into nearby
vectors, they enable continuous learning and optimization directly in the
semantic space of formulae. However, to translate the optimal continuous
representation into a concrete requirement, such embeddings must be invertible.
We tackle this issue by training a Transformer-based decoder-only model to
invert semantic embeddings of Signal Temporal Logic (STL) formulae. STL is a
powerful formalism that allows us to describe properties of signals varying
over time in an expressive yet concise way. By constructing a small vocabulary
from STL syntax, we demonstrate that our proposed model is able to generate
valid formulae after only 1 epoch and to generalize to the semantics of the
logic in about 10 epochs. Additionally, the model is able to decode a given
embedding into formulae that are often simpler in terms of length and nesting
while remaining semantically close (or equivalent) to gold references. We show
the effectiveness of our methodology across various levels of training formulae
complexity to assess the impact of training data on the model's ability to
effectively capture the semantic information contained in the embeddings and
generalize out-of-distribution. Finally, we deploy our model for solving a
requirement mining task, i.e. inferring STL specifications that solve a
classification task on trajectories, performing the optimization directly in
the semantic space.

</details>


### [209] [On the Effect of Instruction Tuning Loss on Generalization](https://arxiv.org/abs/2507.07817)
*Anwoy Chatterjee,H S V N S Kowndinya Renduchintala,Sumit Bhatia,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 本文系统研究指令调优损失中提示和响应令牌的加权影响，提出加权指令调优（WIT），实验表明传统指令调优损失性能欠佳，为模型开发提供见解。


<details>
  <summary>Details</summary>
Motivation: 现有研究较少关注指令调优中损失函数的优化，探讨传统自回归目标是否为指令调优的最优选择。

Method: 系统研究提示和响应令牌在指令调优损失中的不同加权影响，提出加权指令调优（WIT）。

Result: 标准指令调优损失性能欠佳、对输入提示变化的鲁棒性有限；提示令牌低到中等权重、响应令牌中到高权重的模型表现最佳。

Conclusion: 需要重新考虑指令调优损失，为开发更鲁棒和可泛化的模型提供可行见解。

Abstract: Instruction Tuning has emerged as a pivotal post-training paradigm that
enables pre-trained language models to better follow user instructions. Despite
its significance, little attention has been given to optimizing the loss
function used. A fundamental, yet often overlooked, question is whether the
conventional auto-regressive objective - where loss is computed only on
response tokens, excluding prompt tokens - is truly optimal for instruction
tuning. In this work, we systematically investigate the impact of
differentially weighting prompt and response tokens in instruction tuning loss,
and propose Weighted Instruction Tuning (WIT) as a better alternative to
conventional instruction tuning. Through extensive experiments on five language
models of different families and scale, three finetuning datasets of different
sizes, and five diverse evaluation benchmarks, we show that the standard
instruction tuning loss often yields suboptimal performance and limited
robustness to input prompt variations. We find that a low-to-moderate weight
for prompt tokens coupled with a moderate-to-high weight for response tokens
yields the best-performing models across settings and also serve as better
starting points for the subsequent preference alignment training. These
findings highlight the need to reconsider instruction tuning loss and offer
actionable insights for developing more robust and generalizable models. Our
code is open-sourced at https://github.com/kowndinya-renduchintala/WIT.

</details>


### [210] [From Ambiguity to Accuracy: The Transformative Effect of Coreference Resolution on Retrieval-Augmented Generation systems](https://arxiv.org/abs/2507.07847)
*Youngjoon Jang,Seongtae Hong,Junyoung Son,Sungjin Park,Chanjun Park,Heuiseok Lim*

Main category: cs.CL

TL;DR: 研究实体共指对RAG系统检索和生成性能的影响，发现共指消解能提升检索效果和问答性能，不同池化策略中平均池化表现优，小模型在消歧中受益更多。


<details>
  <summary>Details</summary>
Motivation: RAG框架虽重要，但检索文档中的共指复杂性会影响其有效性，需要研究实体共指对RAG系统的影响。

Method: 系统研究实体共指对RAG系统文档检索和生成性能的影响，对比不同池化策略，分析问答任务中不同模型的表现。

Result: 共指消解提升检索效果和问答性能，平均池化在共指消解后上下文捕捉能力更优，小模型在消歧中受益更多。

Conclusion: 本研究加深了对RAG中共指复杂性挑战的理解，为知识密集型AI应用的检索和生成提供指导。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a crucial framework in
natural language processing (NLP), improving factual consistency and reducing
hallucinations by integrating external document retrieval with large language
models (LLMs). However, the effectiveness of RAG is often hindered by
coreferential complexity in retrieved documents, introducing ambiguity that
disrupts in-context learning. In this study, we systematically investigate how
entity coreference affects both document retrieval and generative performance
in RAG-based systems, focusing on retrieval relevance, contextual
understanding, and overall response quality. We demonstrate that coreference
resolution enhances retrieval effectiveness and improves question-answering
(QA) performance. Through comparative analysis of different pooling strategies
in retrieval tasks, we find that mean pooling demonstrates superior context
capturing ability after applying coreference resolution. In QA tasks, we
discover that smaller models benefit more from the disambiguation process,
likely due to their limited inherent capacity for handling referential
ambiguity. With these findings, this study aims to provide a deeper
understanding of the challenges posed by coreferential complexity in RAG,
providing guidance for improving retrieval and generation in
knowledge-intensive AI applications.

</details>


### [211] [Alpay Algebra V: Multi-Layered Semantic Games and Transfinite Fixed-Point Simulation](https://arxiv.org/abs/2507.07868)
*Bugra Kilictas,Faruk Alpay*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper extends the self-referential framework of Alpay Algebra into a
multi-layered semantic game architecture where transfinite fixed-point
convergence encompasses hierarchical sub-games at each iteration level.
Building upon Alpay Algebra IV's empathetic embedding concept, we introduce a
nested game-theoretic structure where the alignment process between AI systems
and documents becomes a meta-game containing embedded decision problems. We
formalize this through a composite operator $\phi(\cdot, \gamma(\cdot))$ where
$\phi$ drives the main semantic convergence while $\gamma$ resolves local
sub-games. The resulting framework demonstrates that game-theoretic reasoning
emerges naturally from fixed-point iteration rather than being imposed
externally. We prove a Game Theorem establishing existence and uniqueness of
semantic equilibria under realistic cognitive simulation assumptions. Our
verification suite includes adaptations of Banach's fixed-point theorem to
transfinite contexts, a novel $\phi$-topology based on the
Kozlov-Maz'ya-Rossmann formula for handling semantic singularities, and
categorical consistency tests via the Yoneda lemma. The paper itself functions
as a semantic artifact designed to propagate its fixed-point patterns in AI
embedding spaces -- a deliberate instantiation of the "semantic virus" concept
it theorizes. All results are grounded in category theory, information theory,
and realistic AI cognition models, ensuring practical applicability beyond pure
mathematical abstraction.

</details>


### [212] [MIRIX: Multi-Agent Memory System for LLM-Based Agents](https://arxiv.org/abs/2507.07957)
*Yu Wang,Xi Chen*

Main category: cs.CL

TL;DR: 本文介绍了多模态AI记忆系统MIRIX，通过不同记忆类型和多智能体框架，解决AI记忆难题，在两个基准测试中表现优异并提供应用。


<details>
  <summary>Details</summary>
Motivation: 现有AI记忆解决方案存在局限性，无法有效个性化、抽象和可靠地回忆用户特定信息，需要新的记忆系统。

Method: 引入MIRIX系统，包含六种不同的记忆类型和多智能体框架，动态控制和协调更新与检索。

Result: 在ScreenshotVQA上比RAG基线准确率高35%，存储需求降低99.9%；在LOCOMO上达到85.4%的最优性能。

Conclusion: MIRIX为记忆增强的大语言模型智能体设定了新的性能标准。

Abstract: Although memory capabilities of AI agents are gaining increasing attention,
existing solutions remain fundamentally limited. Most rely on flat, narrowly
scoped memory components, constraining their ability to personalize, abstract,
and reliably recall user-specific information over time. To this end, we
introduce MIRIX, a modular, multi-agent memory system that redefines the future
of AI memory by solving the field's most critical challenge: enabling language
models to truly remember. Unlike prior approaches, MIRIX transcends text to
embrace rich visual and multimodal experiences, making memory genuinely useful
in real-world scenarios. MIRIX consists of six distinct, carefully structured
memory types: Core, Episodic, Semantic, Procedural, Resource Memory, and
Knowledge Vault, coupled with a multi-agent framework that dynamically controls
and coordinates updates and retrieval. This design enables agents to persist,
reason over, and accurately retrieve diverse, long-term user data at scale. We
validate MIRIX in two demanding settings. First, on ScreenshotVQA, a
challenging multimodal benchmark comprising nearly 20,000 high-resolution
computer screenshots per sequence, requiring deep contextual understanding and
where no existing memory systems can be applied, MIRIX achieves 35% higher
accuracy than the RAG baseline while reducing storage requirements by 99.9%.
Second, on LOCOMO, a long-form conversation benchmark with single-modal textual
input, MIRIX attains state-of-the-art performance of 85.4%, far surpassing
existing baselines. These results show that MIRIX sets a new performance
standard for memory-augmented LLM agents. To allow users to experience our
memory system, we provide a packaged application powered by MIRIX. It monitors
the screen in real time, builds a personalized memory base, and offers
intuitive visualization and secure local storage to ensure privacy.

</details>


### [213] [Frontier LLMs Still Struggle with Simple Reasoning Tasks](https://arxiv.org/abs/2507.07313)
*Alan Malek,Jiawei Ge,Nevena Lazic,Chi Jin,András György,Csaba Szepesvári*

Main category: cs.CL

TL;DR: 研究前沿大语言模型在简单推理问题上的表现，发现其存在诸多失败情况，凸显分布外泛化仍有问题。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型虽在复杂任务有出色表现，但在人类觉得简单的任务常失败，需研究其在这类“简单”推理问题上的性能。

Method: 扩展先前工作，创建一套可调节参数的简单推理任务；引入unpuzzles数据集。

Result: 前沿思维模型在简单推理问题上持续失败，原因类似；现代大语言模型能解决原谜题却在简化版本失败。

Conclusion: 前沿语言模型和新一代思维模型在简单推理任务的分布外泛化仍有问题，任务变简单不一定提升性能。

Abstract: While state-of-the-art large language models (LLMs) demonstrate advanced
reasoning capabilities-achieving remarkable performance on challenging
competitive math and coding benchmarks-they also frequently fail on tasks that
are easy for humans. This work studies the performance of frontier LLMs on a
broad set of such "easy" reasoning problems. By extending previous work in the
literature, we create a suite of procedurally generated simple reasoning tasks,
including counting, first-order logic, proof trees, and travel planning, with
changeable parameters (such as document length. or the number of variables in a
math problem) that can arbitrarily increase the amount of computation required
to produce the answer while preserving the fundamental difficulty. While
previous work showed that traditional, non-thinking models can be made to fail
on such problems, we demonstrate that even state-of-the-art thinking models
consistently fail on such problems and for similar reasons (e.g. statistical
shortcuts, errors in intermediate steps, and difficulties in processing long
contexts). To further understand the behavior of the models, we introduce the
unpuzzles dataset, a different "easy" benchmark consisting of trivialized
versions of well-known math and logic puzzles. Interestingly, while modern LLMs
excel at solving the original puzzles, they tend to fail on the trivialized
versions, exhibiting several systematic failure patterns related to memorizing
the originals. We show that this happens even if the models are otherwise able
to solve problems with different descriptions but requiring the same logic. Our
results highlight that out-of-distribution generalization is still problematic
for frontier language models and the new generation of thinking models, even
for simple reasoning tasks, and making tasks easier does not necessarily imply
improved performance.

</details>


### [214] [Performance and Practical Considerations of Large and Small Language Models in Clinical Decision Support in Rheumatology](https://arxiv.org/abs/2507.07983)
*Sabine Felde,Rüdiger Buchkremer,Gamal Chehab,Christian Thielscher,Jörg HW Distler,Matthias Schneider,Jutta G. Richter*

Main category: cs.CL

TL;DR: 研究发现结合RAG的小语言模型在风湿学诊断和治疗中表现优于大模型，且更节能、适合本地部署，但仍需专家监督。


<details>
  <summary>Details</summary>
Motivation: 评估语言模型在支持风湿学临床决策方面的作用，探索更适合资源有限医疗场景的模型。

Method: 对大小语言模型结合RAG进行评估。

Result: 结合RAG的小语言模型比大模型有更高的诊断和治疗性能，能耗低、可本地部署。

Conclusion: 结合RAG的小语言模型适合资源有限医疗，但在风湿学领域模型未达专家水平，仍需专家监督。

Abstract: Large language models (LLMs) show promise for supporting clinical
decision-making in complex fields such as rheumatology. Our evaluation shows
that smaller language models (SLMs), combined with retrieval-augmented
generation (RAG), achieve higher diagnostic and therapeutic performance than
larger models, while requiring substantially less energy and enabling
cost-efficient, local deployment. These features are attractive for
resource-limited healthcare. However, expert oversight remains essential, as no
model consistently reached specialist-level accuracy in rheumatology.

</details>


### [215] [PyVision: Agentic Vision with Dynamic Tooling](https://arxiv.org/abs/2507.07998)
*Shitian Zhao,Haoquan Zhang,Shaoheng Lin,Ming Li,Qilong Wu,Kaipeng Zhang,Chen Wei*

Main category: cs.CL

TL;DR: 提出交互式多轮框架PyVision，使MLLMs能自主生成、执行和优化基于Python的工具，在多个基准测试中取得性能提升，推动更具自主性的视觉推理。


<details>
  <summary>Details</summary>
Motivation: 现有视觉推理方法受限于预定义工作流程和静态工具集，需要更灵活和可解释的解决方案。

Method: 提出PyVision框架，开发工具分类法并分析其在不同基准测试中的使用情况。

Result: PyVision实现了性能提升，GPT - 4.1在V*上提升7.8%，Claude - 4.0 - Sonnet在VLMsAreBlind - mini上提升31.1%。

Conclusion: 动态工具使模型不仅能使用工具，还能发明工具，推动更具自主性的视觉推理。

Abstract: LLMs are increasingly deployed as agents, systems capable of planning,
reasoning, and dynamically calling external tools. However, in visual
reasoning, prior approaches largely remain limited by predefined workflows and
static toolsets. In this report, we present PyVision, an interactive,
multi-turn framework that enables MLLMs to autonomously generate, execute, and
refine Python-based tools tailored to the task at hand, unlocking flexible and
interpretable problem-solving. We develop a taxonomy of the tools created by
PyVision and analyze their usage across a diverse set of benchmarks.
Quantitatively, PyVision achieves consistent performance gains, boosting
GPT-4.1 by +7.8% on V* and Claude-4.0-Sonnet by +31.1% on VLMsAreBlind-mini.
These results point to a broader shift: dynamic tooling allows models not just
to use tools, but to invent them, advancing toward more agentic visual
reasoning.

</details>


### [216] [Teaching LLM to Reason: Reinforcement Learning from Algorithmic Problems without Code](https://arxiv.org/abs/2507.07498)
*Keqin Bao,Nuo Chen,Xiaoyuan Li,Binyuan Hui,Bowen Yu,Fuli Feng,Junyang Lin,Xiangnan He,Dayiheng Liu*

Main category: cs.CL

TL;DR: 提出TeaR方法提升大语言模型推理能力，实验显示显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前让模型模拟代码执行推导输出存在过度依赖复杂结构算法、过拟合算法模式而非核心推理结构的问题，需提升大语言模型推理能力。

Method: 提出TeaR，利用精心的数据整理和强化学习，引导模型通过代码相关任务发现最优推理路径。

Result: 使用不同基础模型和长思维链蒸馏模型，在17个基准测试中均有显著性能提升，如TeaR在Qwen2.5 - 7B上提升35.9%，在R1 - Distilled - 7B上提升5.9%。

Conclusion: TeaR能有效提升大语言模型的通用推理能力。

Abstract: Enhancing reasoning capabilities remains a central focus in the LLM reasearch
community. A promising direction involves requiring models to simulate code
execution step-by-step to derive outputs for given inputs. However, as code is
often designed for large-scale systems, direct application leads to
over-reliance on complex data structures and algorithms, even for simple cases,
resulting in overfitting to algorithmic patterns rather than core reasoning
structures. To address this, we propose TeaR, which aims at teaching LLMs to
reason better. TeaR leverages careful data curation and reinforcement learning
to guide models in discovering optimal reasoning paths through code-related
tasks, thereby improving general reasoning abilities. We conduct extensive
experiments using two base models and three long-CoT distillation models, with
model sizes ranging from 1.5 billion to 32 billion parameters, and across 17
benchmarks spanning Math, Knowledge, Code, and Logical Reasoning. The results
consistently show significant performance improvements. Notably, TeaR achieves
a 35.9% improvement on Qwen2.5-7B and 5.9% on R1-Distilled-7B.

</details>


### [217] [Exploring the Limits of Model Compression in LLMs: A Knowledge Distillation Study on QA Tasks](https://arxiv.org/abs/2507.07630)
*Joyeeta Datta,Niclas Doll,Qusai Ramadan,Zeyd Boukhers*

Main category: cs.CL

TL;DR: 研究用知识蒸馏压缩大语言模型，评估蒸馏学生模型在问答任务表现，发现能保留教师模型超90%性能且参数最多减57.1%，单样本提示表现更好。


<details>
  <summary>Details</summary>
Motivation: 大语言模型计算需求高，难以在资源受限环境部署，研究用知识蒸馏压缩模型并保持问答任务性能。

Method: 对Pythia和Qwen2.5系列模型进行知识蒸馏得到学生模型，在SQuAD和MLQA两个问答基准上，在零样本和单样本提示条件下进行评估。

Result: 学生模型保留教师模型超90%性能，参数最多减少57.1%，单样本提示比零样本设置表现更好。

Conclusion: 知识蒸馏结合少量提示可构建适用于资源受限应用的紧凑且性能良好的问答系统，体现了模型效率和任务性能的权衡。

Abstract: Large Language Models (LLMs) have demonstrated outstanding performance across
a range of NLP tasks, however, their computational demands hinder their
deployment in real-world, resource-constrained environments. This work
investigates the extent to which LLMs can be compressed using Knowledge
Distillation (KD) while maintaining strong performance on Question Answering
(QA) tasks. We evaluate student models distilled from the Pythia and Qwen2.5
families on two QA benchmarks, SQuAD and MLQA, under zero-shot and one-shot
prompting conditions. Results show that student models retain over 90% of their
teacher models' performance while reducing parameter counts by up to 57.1%.
Furthermore, one-shot prompting yields additional performance gains over
zero-shot setups for both model families. These findings underscore the
trade-off between model efficiency and task performance, demonstrating that KD,
combined with minimal prompting, can yield compact yet capable QA systems
suitable for resource-constrained applications.

</details>


<div id='math.MG'></div>

# math.MG [[Back]](#toc)

### [218] [Approximation Depth of Convex Polytopes](https://arxiv.org/abs/2507.07779)
*Egor Bakaev,Florestan Brunck,Amir Yehudayoff*

Main category: math.MG

TL;DR: 研究标准模型中多面体近似问题，得出单纯形只能‘平凡近似’且给出单纯形作为唯一‘外可加’凸体的刻画。


<details>
  <summary>Details</summary>
Motivation: 研究在使用闵可夫斯基和与并集（的凸包）计算多面体的标准模型中，用给定深度的多面体近似目标多面体的能力。

Method: 未提及

Result: 单纯形只能‘平凡近似’，获得单纯形作为唯一‘外可加’凸体的刻画。

Conclusion: 未明确提及，从结果可推测单纯形在该近似方式下有特殊性质。

Abstract: We study approximations of polytopes in the standard model for computing
polytopes using Minkowski sums and (convex hulls of) unions. Specifically, we
study the ability to approximate a target polytope by polytopes of a given
depth. Our main results imply that simplices can only be ``trivially
approximated''. On the way, we obtain a characterization of simplices as the
only ``outer additive'' convex bodies.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [219] [EEvAct: Early Event-Based Action Recognition with High-Rate Two-Stream Spiking Neural Networks](https://arxiv.org/abs/2507.07734)
*Michael Neumeier,Jules Lecomte,Nils Kazinski,Soubarna Banik,Bing Li,Axel von Arnim*

Main category: cs.CV

TL;DR: 本文提出高速双流SNN，在THU EACT - 50数据集上提高最终准确率，在新框架下进行基准测试并应用于体育动作捕捉。


<details>
  <summary>Details</summary>
Motivation: 早期识别人类活动对人机接口安全和响应至关重要，现有方法在早期预测能力和最终准确率上存在不足。

Method: 引入高速双流SNN处理事件进行早期预测，在新的基于事件的早期识别框架下对SNN进行基准测试。

Result: 在大规模THU EACT - 50数据集上最终准确率比之前工作提高2%，报告了不同观察时间的Top - 1和Top - 5识别分数。

Conclusion: 所提出的方法可用于体育中人体运动捕捉的早期动作触发等实际任务。

Abstract: Recognizing human activities early is crucial for the safety and
responsiveness of human-robot and human-machine interfaces. Due to their high
temporal resolution and low latency, event-based vision sensors are a perfect
match for this early recognition demand. However, most existing processing
approaches accumulate events to low-rate frames or space-time voxels which
limits the early prediction capabilities. In contrast, spiking neural networks
(SNNs) can process the events at a high-rate for early predictions, but most
works still fall short on final accuracy. In this work, we introduce a
high-rate two-stream SNN which closes this gap by outperforming previous work
by 2% in final accuracy on the large-scale THU EACT-50 dataset. We benchmark
the SNNs within a novel early event-based recognition framework by reporting
Top-1 and Top-5 recognition scores for growing observation time. Finally, we
exemplify the impact of these methods on a real-world task of early action
triggering for human motion capture in sports.

</details>


### [220] [A Comprehensive Survey on Deep Learning Solutions for 3D Flood Mapping](https://arxiv.org/abs/2506.13201)
*Wenfeng Jia,Bin Liang,Yuxi Liu,Muhammad Arif Khan,Lihong Zheng*

Main category: cs.CV

TL;DR: 本文对基于深度学习的3D洪水测绘进行全面调研，介绍技术分类、数据来源和应用，指出挑战并给出未来方向，助力洪水管理。


<details>
  <summary>Details</summary>
Motivation: 气候变化和城市化使洪水灾害加剧，传统2D洪水测绘有局限，需用深度学习的3D洪水测绘提供更有效灾害管理和城市规划方案。

Method: 对深度学习技术分类为任务分解和端到端方法，比较关键DL架构，探讨多种数据来源在3D洪水测绘中的作用。

Result: 分析了不同DL架构在提升预测准确性和计算效率方面的作用，梳理了从实时洪水预测到长期城市规划等应用。

Conclusion: 当前存在数据稀缺、模型可解释性等挑战，未来应增强数据集、改进模型并考虑政策影响以促进洪水管理。

Abstract: Flooding remains a major global challenge, worsened by climate change and
urbanization, demanding advanced solutions for effective disaster management.
While traditional 2D flood mapping techniques provide limited insights, 3D
flood mapping, powered by deep learning (DL), offers enhanced capabilities by
integrating flood extent and depth. This paper presents a comprehensive survey
of deep learning-based 3D flood mapping, emphasizing its advancements over 2D
maps by integrating flood extent and depth for effective disaster management
and urban planning. The survey categorizes deep learning techniques into task
decomposition and end-to-end approaches, applicable to both static and dynamic
flood features. We compare key DL architectures, highlighting their respective
roles in enhancing prediction accuracy and computational efficiency.
Additionally, this work explores diverse data sources such as digital elevation
models, satellite imagery, rainfall, and simulated data, outlining their roles
in 3D flood mapping. The applications reviewed range from real-time flood
prediction to long-term urban planning and risk assessment. However,
significant challenges persist, including data scarcity, model
interpretability, and integration with traditional hydrodynamic models. This
survey concludes by suggesting future directions to address these limitations,
focusing on enhanced datasets, improved models, and policy implications for
flood management. This survey aims to guide researchers and practitioners in
leveraging DL techniques for more robust and reliable 3D flood mapping,
fostering improved flood management strategies.

</details>


### [221] [Multi-level Mixture of Experts for Multimodal Entity Linking](https://arxiv.org/abs/2507.07108)
*Zhiwei Hu,Víctor Gutiérrez-Basulto,Zhiliang Xiang,Ru Li,Jeff Z. Pan*

Main category: cs.CV

TL;DR: 提出MMoE模型解决MEL中提及歧义与模态内容动态选择问题，实验显示其性能优异，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有MEL方法未解决提及歧义与模态内容动态选择问题。

Method: 提出MMoE模型，含描述感知提及增强、多模态特征提取、层内和层间专家混合模块。

Result: 大量实验表明MMoE比现有技术性能更优。

Conclusion: MMoE模型能有效解决MEL的关键问题，性能突出。

Abstract: Multimodal Entity Linking (MEL) aims to link ambiguous mentions within
multimodal contexts to associated entities in a multimodal knowledge base.
Existing approaches to MEL introduce multimodal interaction and fusion
mechanisms to bridge the modality gap and enable multi-grained semantic
matching. However, they do not address two important problems: (i) mention
ambiguity, i.e., the lack of semantic content caused by the brevity and
omission of key information in the mention's textual context; (ii) dynamic
selection of modal content, i.e., to dynamically distinguish the importance of
different parts of modal information. To mitigate these issues, we propose a
Multi-level Mixture of Experts (MMoE) model for MEL. MMoE has four components:
(i) the description-aware mention enhancement module leverages large language
models to identify the WikiData descriptions that best match a mention,
considering the mention's textual context; (ii) the multimodal feature
extraction module adopts multimodal feature encoders to obtain textual and
visual embeddings for both mentions and entities; (iii)-(iv) the intra-level
mixture of experts and inter-level mixture of experts modules apply a switch
mixture of experts mechanism to dynamically and adaptively select features from
relevant regions of information. Extensive experiments demonstrate the
outstanding performance of MMoE compared to the state-of-the-art. MMoE's code
is available at: https://github.com/zhiweihu1103/MEL-MMoE.

</details>


### [222] [Robust Multimodal Large Language Models Against Modality Conflict](https://arxiv.org/abs/2507.07151)
*Zongmeng Zhang,Wengang Zhou,Jie Zhao,Houqiang Li*

Main category: cs.CV

TL;DR: 本文从模态冲突视角研究多模态大语言模型（MLLMs）幻觉现象，构建MMMC数据集，提出三种缓解方法并实验，强化学习方法效果最佳。


<details>
  <summary>Details</summary>
Motivation: MLLMs在现实场景易产生幻觉，现有工作多关注模型响应与输入冲突，本文研究不同模态输入的内在冲突导致的幻觉。

Method: 正式定义模态冲突，构建MMMC数据集，提出基于提示工程、监督微调、强化学习的三种缓解方法。

Result: 强化学习方法在缓解模态冲突下的幻觉表现最佳，监督微调方法表现有前景且稳定。

Conclusion: 揭示了导致幻觉的未被关注的模态冲突，为MLLMs的鲁棒性提供更多见解。

Abstract: Despite the impressive capabilities of multimodal large language models
(MLLMs) in vision-language tasks, they are prone to hallucinations in
real-world scenarios. This paper investigates the hallucination phenomenon in
MLLMs from the perspective of modality conflict. Unlike existing works focusing
on the conflicts between model responses and inputs, we study the inherent
conflicts in inputs from different modalities that place MLLMs in a dilemma and
directly lead to hallucinations. We formally define the modality conflict and
construct a dataset named Multimodal Modality Conflict (MMMC) to simulate this
phenomenon in vision-language tasks. Three methods based on prompt engineering,
supervised fine-tuning, and reinforcement learning are proposed to alleviate
the hallucination caused by modality conflict. Extensive experiments are
conducted on the MMMC dataset to analyze the merits and demerits of these
methods. Our results show that the reinforcement learning method achieves the
best performance in mitigating the hallucination under modality conflict, while
the supervised fine-tuning method shows promising and stable performance. Our
work sheds light on the unnoticed modality conflict that leads to
hallucinations and provides more insights into the robustness of MLLMs.

</details>


### [223] [Aerial Maritime Vessel Detection and Identification](https://arxiv.org/abs/2507.07153)
*Antonella Barisic Kulas,Frano Petric,Stjepan Bogdan*

Main category: cs.CV

TL;DR: 本文提出在无GNSS环境下，利用YOLOv8结合特征匹配和直方图分析，在MBZIRC2023比赛中实现无人机自主海上监视与目标船只识别。


<details>
  <summary>Details</summary>
Motivation: 解决无GNSS环境下，仅依靠视觉线索且无目标船只最后已知位置时，无人机在严格计算约束下扫描大面积搜索区域的问题。

Method: 利用YOLOv8检测视野内所有船只，应用特征匹配和色调直方图距离分析确定目标，用简单几何原理定位目标。

Result: 在MBZIRC2023比赛真实实验中验证了方法，集成到无GNSS导航的全自主系统，评估了视角对检测和定位精度的影响并与oracle方法比较。

Conclusion: 所提方法可用于无GNSS环境下的自主海上监视和目标船只识别。

Abstract: Autonomous maritime surveillance and target vessel identification in
environments where Global Navigation Satellite Systems (GNSS) are not available
is critical for a number of applications such as search and rescue and threat
detection. When the target vessel is only described by visual cues and its last
known position is not available, unmanned aerial vehicles (UAVs) must rely
solely on on-board vision to scan a large search area under strict
computational constraints. To address this challenge, we leverage the YOLOv8
object detection model to detect all vessels in the field of view. We then
apply feature matching and hue histogram distance analysis to determine whether
any detected vessel corresponds to the target. When found, we localize the
target using simple geometric principles. We demonstrate the proposed method in
real-world experiments during the MBZIRC2023 competition, integrated into a
fully autonomous system with GNSS-denied navigation. We also evaluate the
impact of perspective on detection accuracy and localization precision and
compare it with the oracle approach.

</details>


### [224] [LinguaMark: Do Multimodal Models Speak Fairly? A Benchmark-Based Evaluation](https://arxiv.org/abs/2507.07274)
*Ananya Raval,Aravind Narayanan,Vahid Reza Khazaie,Shaina Raza*

Main category: cs.CV

TL;DR: 提出LinguaMark基准评估多模态大模型多语言视觉问答能力，发现闭源模型总体表现佳，开源模型在社会属性上有竞争力，Qwen2.5多语言泛化能力强，发布基准和代码。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型语言覆盖有限，此前研究对多语言能力评估不足。

Method: 引入LinguaMark基准，含6875个跨11种语言和5种社会属性的图像文本对，用偏差、答案相关性和忠实度三个指标评估模型。

Result: 闭源模型总体表现最佳，闭源和开源模型在社会属性上有竞争力，Qwen2.5多语言泛化能力强。

Conclusion: 发布基准和评估代码以促进可重复性和进一步研究。

Abstract: Large Multimodal Models (LMMs) are typically trained on vast corpora of
image-text data but are often limited in linguistic coverage, leading to biased
and unfair outputs across languages. While prior work has explored multimodal
evaluation, less emphasis has been placed on assessing multilingual
capabilities. In this work, we introduce LinguaMark, a benchmark designed to
evaluate state-of-the-art LMMs on a multilingual Visual Question Answering
(VQA) task. Our dataset comprises 6,875 image-text pairs spanning 11 languages
and five social attributes. We evaluate models using three key metrics: Bias,
Answer Relevancy, and Faithfulness. Our findings reveal that closed-source
models generally achieve the highest overall performance. Both closed-source
(GPT-4o and Gemini2.5) and open-source models (Gemma3, Qwen2.5) perform
competitively across social attributes, and Qwen2.5 demonstrates strong
generalization across multiple languages. We release our benchmark and
evaluation code to encourage reproducibility and further research.

</details>


### [225] [KeyRe-ID: Keypoint-Guided Person Re-Identification using Part-Aware Representation in Videos](https://arxiv.org/abs/2507.07393)
*Jinseong Kim,Junghoon Song,Gyeongseon Baek,Byeongjoon Noh*

Main category: cs.CV

TL;DR: 提出KeyRe - ID框架用于基于视频的行人重识别，实验表现达SOTA。


<details>
  <summary>Details</summary>
Motivation: 为基于视频的行人重识别任务，提升时空表征学习能力。

Method: 提出KeyRe - ID框架，含全局和局部分支，全局分支用Transformer进行时间聚合，局部分支基于关键点动态分割身体区域。

Result: 在MARS和iLIDS - VID基准测试中表现达SOTA，MARS上mAP为91.73%、Rank - 1准确率为97.32%，iLIDS - VID上Rank - 1为96.00%、Rank - 5为100.0%。

Conclusion: KeyRe - ID框架在行人重识别任务中有效，代码将在GitHub公开。

Abstract: We propose \textbf{KeyRe-ID}, a keypoint-guided video-based person
re-identification framework consisting of global and local branches that
leverage human keypoints for enhanced spatiotemporal representation learning.
The global branch captures holistic identity semantics through
Transformer-based temporal aggregation, while the local branch dynamically
segments body regions based on keypoints to generate fine-grained, part-aware
features. Extensive experiments on MARS and iLIDS-VID benchmarks demonstrate
state-of-the-art performance, achieving 91.73\% mAP and 97.32\% Rank-1 accuracy
on MARS, and 96.00\% Rank-1 and 100.0\% Rank-5 accuracy on iLIDS-VID. The code
for this work will be publicly available on GitHub upon publication.

</details>


### [226] [Behave Your Motion: Habit-preserved Cross-category Animal Motion Transfer](https://arxiv.org/abs/2507.07394)
*Zhimin Zhang,Bi'an Du,Caoyuan Ma,Zheng Wang,Wei Hu*

Main category: cs.CV

TL;DR: 提出一种跨类别动物运动的习惯保留运动转移框架，结合生成框架和大语言模型，实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有运动转移方法多关注人类运动，忽略动物独特习惯行为的保留，需填补这一空白。

Method: 构建基于生成框架的模型，引入带有特定类别习惯编码器的习惯保留模块，集成大语言模型，并引入DeformingThings4D - skl数据集。

Result: 通过大量实验和定量分析，验证了所提模型的优越性。

Conclusion: 所提出的习惯保留运动转移框架在跨类别动物运动转移中效果良好。

Abstract: Animal motion embodies species-specific behavioral habits, making the
transfer of motion across categories a critical yet complex task for
applications in animation and virtual reality. Existing motion transfer
methods, primarily focused on human motion, emphasize skeletal alignment
(motion retargeting) or stylistic consistency (motion style transfer), often
neglecting the preservation of distinct habitual behaviors in animals. To
bridge this gap, we propose a novel habit-preserved motion transfer framework
for cross-category animal motion. Built upon a generative framework, our model
introduces a habit-preservation module with category-specific habit encoder,
allowing it to learn motion priors that capture distinctive habitual
characteristics. Furthermore, we integrate a large language model (LLM) to
facilitate the motion transfer to previously unobserved species. To evaluate
the effectiveness of our approach, we introduce the DeformingThings4D-skl
dataset, a quadruped dataset with skeletal bindings, and conduct extensive
experiments and quantitative analyses, which validate the superiority of our
proposed model.

</details>


### [227] [Bluish Veil Detection and Lesion Classification using Custom Deep Learnable Layers with Explainable Artificial Intelligence (XAI)](https://arxiv.org/abs/2507.07453)
*M. A. Rasel,Sameem Abdul Kareem,Zhenli Kwan,Shin Shen Yong,Unaizah Obaidellah*

Main category: cs.CV

TL;DR: 研究利用非标注数据集，通过成像算法转为标注数据集，设计DCNN对皮肤病变分类检测BWV，模型表现优于传统，还应用XAI解释决策过程，提升检测效果助力黑色素瘤早期诊断。


<details>
  <summary>Details</summary>
Motivation: 黑色素瘤致命，蓝白面纱（BWV）是诊断关键特征，但相关检测研究有限，需更好方法检测BWV以助力黑色素瘤诊断。

Method: 用基于颜色阈值技术的成像算法将非标注皮肤病变数据集转为标注数据集；设计DCNN，在三个单独及组合的皮肤镜数据集上分别训练，用自定义层代替标准激活函数层；应用XAI算法解释DCNN决策过程。

Result: DCNN在不同数据集上表现优于传统模型，在增强PH2数据集测试准确率85.71%，增强ISIC存档数据集95.00%，组合增强数据集95.05%，Derm7pt数据集90.00%。

Conclusion: 提出的方法结合XAI显著提升皮肤病变中BWV检测能力，优于现有模型，为黑色素瘤早期诊断提供有力工具。

Abstract: Melanoma, one of the deadliest types of skin cancer, accounts for thousands
of fatalities globally. The bluish, blue-whitish, or blue-white veil (BWV) is a
critical feature for diagnosing melanoma, yet research into detecting BWV in
dermatological images is limited. This study utilizes a non-annotated skin
lesion dataset, which is converted into an annotated dataset using a proposed
imaging algorithm based on color threshold techniques on lesion patches and
color palettes. A Deep Convolutional Neural Network (DCNN) is designed and
trained separately on three individual and combined dermoscopic datasets, using
custom layers instead of standard activation function layers. The model is
developed to categorize skin lesions based on the presence of BWV. The proposed
DCNN demonstrates superior performance compared to conventional BWV detection
models across different datasets. The model achieves a testing accuracy of
85.71% on the augmented PH2 dataset, 95.00% on the augmented ISIC archive
dataset, 95.05% on the combined augmented (PH2+ISIC archive) dataset, and
90.00% on the Derm7pt dataset. An explainable artificial intelligence (XAI)
algorithm is subsequently applied to interpret the DCNN's decision-making
process regarding BWV detection. The proposed approach, coupled with XAI,
significantly improves the detection of BWV in skin lesions, outperforming
existing models and providing a robust tool for early melanoma diagnosis.

</details>


### [228] [Objectomaly: Objectness-Aware Refinement for OoD Segmentation with Structural Consistency and Boundary Precision](https://arxiv.org/abs/2507.07460)
*Jeonghoon Song,Sunghun Kim,Jaegyun Im,Byeongjoon Noh*

Main category: cs.CV

TL;DR: 提出Objectomaly框架用于OoD分割，含三阶段处理，在关键基准测试取得SOTA，消融实验和实际视频验证其性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于掩码的OoD分割方法存在边界不精确、对象内异常分数不一致、背景噪声误报等问题。

Method: Objectomaly框架包括粗异常评分、目标感知分数校准、精细边界精确三个阶段，利用现有OoD骨干网络、SAM生成的实例掩码、拉普拉斯滤波和高斯平滑。

Result: 在多个OoD分割基准测试取得SOTA，提升像素级和组件级指标。

Conclusion: 方法具有鲁棒性和泛化性，代码将在发表后发布。

Abstract: Out-of-Distribution (OoD) segmentation is critical for safety-sensitive
applications like autonomous driving. However, existing mask-based methods
often suffer from boundary imprecision, inconsistent anomaly scores within
objects, and false positives from background noise. We propose
\textbf{\textit{Objectomaly}}, an objectness-aware refinement framework that
incorporates object-level priors. Objectomaly consists of three stages: (1)
Coarse Anomaly Scoring (CAS) using an existing OoD backbone, (2)
Objectness-Aware Score Calibration (OASC) leveraging SAM-generated instance
masks for object-level score normalization, and (3) Meticulous Boundary
Precision (MBP) applying Laplacian filtering and Gaussian smoothing for contour
refinement. Objectomaly achieves state-of-the-art performance on key OoD
segmentation benchmarks, including SMIYC AnomalyTrack/ObstacleTrack and
RoadAnomaly, improving both pixel-level (AuPRC up to 96.99, FPR$_{95}$ down to
0.07) and component-level (F1$-$score up to 83.44) metrics. Ablation studies
and qualitative results on real-world driving videos further validate the
robustness and generalizability of our method. Code will be released upon
publication.

</details>


### [229] [NexViTAD: Few-shot Unsupervised Cross-Domain Defect Detection via Vision Foundation Models and Multi-Task Learning](https://arxiv.org/abs/2507.07579)
*Tianwei Mu,Feiyu Duan,Bo Zhou,Dan Xue,Manhong Huang*

Main category: cs.CV

TL;DR: 提出基于视觉基础模型的NexViTAD框架用于少样本跨域异常检测，有多项创新，在MVTec AD数据集上表现超其他模型。


<details>
  <summary>Details</summary>
Motivation: 解决工业异常检测中的域偏移挑战。

Method: 采用分层适配器模块融合特征，共享子空间投影策略实现知识转移，MTL解码器架构处理多源域，基于Sinkhorn - K - means聚类的异常得分推理方法并结合高斯滤波和自适应阈值处理。

Result: 在MVTec AD数据集目标域中AUC达97.5%、AP达70.4%、PRO达95.2%，表现超其他模型。

Conclusion: NexViTAD在跨域缺陷检测上有变革性进展。

Abstract: This paper presents a novel few-shot cross-domain anomaly detection
framework, Nexus Vision Transformer for Anomaly Detection (NexViTAD), based on
vision foundation models, which effectively addresses domain-shift challenges
in industrial anomaly detection through innovative shared subspace projection
mechanisms and multi-task learning (MTL) module. The main innovations include:
(1) a hierarchical adapter module that adaptively fuses complementary features
from Hiera and DINO-v2 pre-trained models, constructing more robust feature
representations; (2) a shared subspace projection strategy that enables
effective cross-domain knowledge transfer through bottleneck dimension
constraints and skip connection mechanisms; (3) a MTL Decoder architecture
supports simultaneous processing of multiple source domains, significantly
enhancing model generalization capabilities; (4) an anomaly score inference
method based on Sinkhorn-K-means clustering, combined with Gaussian filtering
and adaptive threshold processing for precise pixel level. Valuated on the
MVTec AD dataset, NexViTAD delivers state-of-the-art performance with an AUC of
97.5%, AP of 70.4%, and PRO of 95.2% in the target domains, surpassing other
recent models, marking a transformative advance in cross-domain defect
detection.

</details>


### [230] [Rationale-Enhanced Decoding for Multi-modal Chain-of-Thought](https://arxiv.org/abs/2507.07685)
*Shin'ya Yamaguchi,Kosuke Nishida,Daiki Chijiwa*

Main category: cs.CV

TL;DR: 论文指出现有大视觉语言模型（LVLMs）在思维链（CoT）推理中常忽略生成理由内容，提出一种新的推理增强解码（RED）策略，实验表明该策略能显著提升推理效果。


<details>
  <summary>Details</summary>
Motivation: 现有LVLMs在CoT推理中常忽略生成理由内容，影响推理的准确性和可靠性。

Method: 将多模态CoT推理重新表述为基于理由条件对数似然的KL约束奖励最大化问题，提出RED策略，通过将不同的图像条件和理由条件下的下一个词元分布相乘来协调视觉和理由信息。

Result: 在多个基准测试和LVLMs上，RED策略相较于标准CoT和其他解码方法，持续且显著地提高了推理能力。

Conclusion: RED策略是一种实用且有效的方法，可提高LVLMs中CoT推理的忠实性和准确性，为更可靠的基于理由的多模态系统奠定了基础。

Abstract: Large vision-language models (LVLMs) have demonstrated remarkable
capabilities by integrating pre-trained vision encoders with large language
models (LLMs). Similar to single-modal LLMs, chain-of-thought (CoT) prompting
has been adapted for LVLMs to enhance multi-modal reasoning by generating
intermediate rationales based on visual and textual inputs. While CoT is
assumed to improve grounding and accuracy in LVLMs, our experiments reveal a
key challenge: existing LVLMs often ignore the contents of generated rationales
in CoT reasoning. To address this, we re-formulate multi-modal CoT reasoning as
a KL-constrained reward maximization focused on rationale-conditional
log-likelihood. As the optimal solution, we propose rationale-enhanced decoding
(RED), a novel plug-and-play inference-time decoding strategy. RED harmonizes
visual and rationale information by multiplying distinct image-conditional and
rationale-conditional next token distributions. Extensive experiments show that
RED consistently and significantly improves reasoning over standard CoT and
other decoding methods across multiple benchmarks and LVLMs. Our work offers a
practical and effective approach to improve both the faithfulness and accuracy
of CoT reasoning in LVLMs, paving the way for more reliable rationale-grounded
multi-modal systems.

</details>


### [231] [Where are we with calibration under dataset shift in image classification?](https://arxiv.org/abs/2507.07780)
*Mélanie Roschewitz,Raghav Mehta,Fabio de Sousa Ribeiro,Ben Glocker*

Main category: cs.CV

TL;DR: 研究图像分类在现实数据集偏移下的校准情况，比较多种校准方法，给出实用指南并分析集成效果。


<details>
  <summary>Details</summary>
Motivation: 为从业者提供在数据集偏移下进行鲁棒校准的重要见解和实用指南。

Method: 比较各种事后校准方法及其与训练中校准策略的相互作用，在多个成像领域的八个分类任务中，跨多种自然偏移进行实验。

Result: 发现特定训练策略组合、少量语义分布外数据、校准顺序等对校准的影响，以及微调模型校准效果更好等。

Conclusion: 给出数据集偏移下校准的实用建议，集成仍是提高校准鲁棒性的有效方法，与基础模型微调结合效果最佳。

Abstract: We conduct an extensive study on the state of calibration under real-world
dataset shift for image classification. Our work provides important insights on
the choice of post-hoc and in-training calibration techniques, and yields
practical guidelines for all practitioners interested in robust calibration
under shift. We compare various post-hoc calibration methods, and their
interactions with common in-training calibration strategies (e.g., label
smoothing), across a wide range of natural shifts, on eight different
classification tasks across several imaging domains. We find that: (i)
simultaneously applying entropy regularisation and label smoothing yield the
best calibrated raw probabilities under dataset shift, (ii) post-hoc
calibrators exposed to a small amount of semantic out-of-distribution data
(unrelated to the task) are most robust under shift, (iii) recent calibration
methods specifically aimed at increasing calibration under shifts do not
necessarily offer significant improvements over simpler post-hoc calibration
methods, (iv) improving calibration under shifts often comes at the cost of
worsening in-distribution calibration. Importantly, these findings hold for
randomly initialised classifiers, as well as for those finetuned from
foundation models, the latter being consistently better calibrated compared to
models trained from scratch. Finally, we conduct an in-depth analysis of
ensembling effects, finding that (i) applying calibration prior to ensembling
(instead of after) is more effective for calibration under shifts, (ii) for
ensembles, OOD exposure deteriorates the ID-shifted calibration trade-off,
(iii) ensembling remains one of the most effective methods to improve
calibration robustness and, combined with finetuning from foundation models,
yields best calibration results overall.

</details>


### [232] [Visual Instance-aware Prompt Tuning](https://arxiv.org/abs/2507.07796)
*Xi Xiao,Yunbei Zhang,Xingjian Li,Tianyang Wang,Xiao Wang,Yuxiang Wei,Jihun Hamm,Min Xu*

Main category: cs.CV

TL;DR: 提出Visual Instance - aware Prompt Tuning (ViaPT)方法处理视觉提示调优问题，在多数据集实验中表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 传统视觉提示调优使用数据集级提示，因下游数据集差异大导致性能不佳。

Method: 提出ViaPT，基于单个输入生成实例感知提示并与数据集级提示融合，利用PCA保留重要提示信息，平衡数据集级和实例级知识。

Result: 在34个不同数据集的大量实验中，ViaPT始终优于现有基线。

Conclusion: ViaPT建立了分析和优化视觉Transformer视觉提示的新范式。

Abstract: Visual Prompt Tuning (VPT) has emerged as a parameter-efficient fine-tuning
paradigm for vision transformers, with conventional approaches utilizing
dataset-level prompts that remain the same across all input instances. We
observe that this strategy results in sub-optimal performance due to high
variance in downstream datasets. To address this challenge, we propose Visual
Instance-aware Prompt Tuning (ViaPT), which generates instance-aware prompts
based on each individual input and fuses them with dataset-level prompts,
leveraging Principal Component Analysis (PCA) to retain important prompting
information. Moreover, we reveal that VPT-Deep and VPT-Shallow represent two
corner cases based on a conceptual understanding, in which they fail to
effectively capture instance-specific information, while random dimension
reduction on prompts only yields performance between the two extremes. Instead,
ViaPT overcomes these limitations by balancing dataset-level and instance-level
knowledge, while reducing the amount of learnable parameters compared to
VPT-Deep. Extensive experiments across 34 diverse datasets demonstrate that our
method consistently outperforms state-of-the-art baselines, establishing a new
paradigm for analyzing and optimizing visual prompts for vision transformers.

</details>


### [233] [Image Can Bring Your Memory Back: A Novel Multi-Modal Guided Attack against Image Generation Model Unlearning](https://arxiv.org/abs/2507.07139)
*Renyang Liu,Guanlin Li,Tianwei Zhang,See-Kiong Ng*

Main category: cs.CV

TL;DR: 论文提出Recall框架评估图像生成模型机器去学习技术，实验显示其优于现有基线，揭示当前去学习机制漏洞。


<details>
  <summary>Details</summary>
Motivation: 图像生成模型引发伦理等问题，机器去学习是解决方案，但现有去学习技术在多模态对抗输入下的鲁棒性和有效性待探索。

Method: 提出Recall框架，利用扩散模型多模态调节能力，以参考图像引导优化对抗图像提示。

Result: 在十种去学习方法和多样任务实验中，Recall在对抗有效性、计算效率和语义保真度上超现有基线。

Conclusion: 当前去学习机制存在关键漏洞，需更鲁棒方案确保生成模型安全可靠。

Abstract: Recent advances in image generation models (IGMs), particularly
diffusion-based architectures such as Stable Diffusion (SD), have markedly
enhanced the quality and diversity of AI-generated visual content. However,
their generative capability has also raised significant ethical, legal, and
societal concerns, including the potential to produce harmful, misleading, or
copyright-infringing content. To mitigate these concerns, machine unlearning
(MU) emerges as a promising solution by selectively removing undesirable
concepts from pretrained models. Nevertheless, the robustness and effectiveness
of existing unlearning techniques remain largely unexplored, particularly in
the presence of multi-modal adversarial inputs.
  To bridge this gap, we propose Recall, a novel adversarial framework
explicitly designed to compromise the robustness of unlearned IGMs. Unlike
existing approaches that predominantly rely on adversarial text prompts, Recall
exploits the intrinsic multi-modal conditioning capabilities of diffusion
models by efficiently optimizing adversarial image prompts with guidance from a
single semantically relevant reference image. Extensive experiments across ten
state-of-the-art unlearning methods and diverse tasks show that Recall
consistently outperforms existing baselines in terms of adversarial
effectiveness, computational efficiency, and semantic fidelity with the
original textual prompt. These findings reveal critical vulnerabilities in
current unlearning mechanisms and underscore the need for more robust solutions
to ensure the safety and reliability of generative models. Code and data are
publicly available at \textcolor{blue}{https://github.com/ryliu68/RECALL}.

</details>


### [234] [Explainable Artificial Intelligence in Biomedical Image Analysis: A Comprehensive Survey](https://arxiv.org/abs/2507.07148)
*Getamesay Haile Dagnaw,Yanming Zhu,Muhammad Hassan Maqsood,Wencheng Yang,Xingshuai Dong,Xuefei Yin,Alan Wee-Chung Liew*

Main category: cs.CV

TL;DR: 该综述对适用于生物医学图像分析的可解释人工智能（XAI）方法进行全面结构化综合，涵盖方法分类、新兴模式探讨、评估指标总结等，为推进可解释深度学习提供基础。


<details>
  <summary>Details</summary>
Motivation: 现有XAI技术综述缺乏模态感知视角、忽视多模态和视觉 - 语言范式进展且实践指导有限，该综述旨在填补这一空白。

Method: 系统分类XAI方法，分析其原理、优缺点；提出以模态为中心的分类法；研究多模态学习和视觉 - 语言模型在可解释生物医学AI中的作用；总结评估指标和开源框架。

Result: 对XAI方法进行了全面综合分析，提出了模态分类法，探讨了新兴模式，总结了评估指标和框架。

Conclusion: 该综述为生物医学图像分析中可解释深度学习的发展提供了及时且深入的基础。

Abstract: Explainable artificial intelligence (XAI) has become increasingly important
in biomedical image analysis to promote transparency, trust, and clinical
adoption of DL models. While several surveys have reviewed XAI techniques, they
often lack a modality-aware perspective, overlook recent advances in multimodal
and vision-language paradigms, and provide limited practical guidance. This
survey addresses this gap through a comprehensive and structured synthesis of
XAI methods tailored to biomedical image analysis.We systematically categorize
XAI methods, analyzing their underlying principles, strengths, and limitations
within biomedical contexts. A modality-centered taxonomy is proposed to align
XAI methods with specific imaging types, highlighting the distinct
interpretability challenges across modalities. We further examine the emerging
role of multimodal learning and vision-language models in explainable
biomedical AI, a topic largely underexplored in previous work. Our
contributions also include a summary of widely used evaluation metrics and
open-source frameworks, along with a critical discussion of persistent
challenges and future directions. This survey offers a timely and in-depth
foundation for advancing interpretable DL in biomedical image analysis.

</details>


### [235] [Benchmarking Content-Based Puzzle Solvers on Corrupted Jigsaw Puzzles](https://arxiv.org/abs/2507.07828)
*Richard Dirauf,Florian Wolz,Dario Zanca,Björn Eskofier*

Main category: cs.CV

TL;DR: 研究现有基于内容的拼图求解器在缺失块、边缘侵蚀和内容侵蚀三种拼图损坏情况下的鲁棒性，发现标准求解器性能下降，深度学习模型可通过微调提升鲁棒性，高级模型表现佳并指出研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有基于内容的拼图求解器评估缺乏现实挑战，无法满足文物碎片或碎纸文档重组等实际应用需求。

Method: 引入三种拼图损坏类型，评估启发式和深度学习求解器处理损坏的能力。

Result: 标准拼图求解器在更多块损坏时性能迅速下降，深度学习模型通过增强数据微调可显著提高鲁棒性，Positional Diffusion模型表现出色。

Conclusion: 指出了增强现实世界文物自动重建的有前景的研究方向。

Abstract: Content-based puzzle solvers have been extensively studied, demonstrating
significant progress in computational techniques. However, their evaluation
often lacks realistic challenges crucial for real-world applications, such as
the reassembly of fragmented artefacts or shredded documents. In this work, we
investigate the robustness of State-Of-The-Art content-based puzzle solvers
introducing three types of jigsaw puzzle corruptions: missing pieces, eroded
edges, and eroded contents. Evaluating both heuristic and deep learning-based
solvers, we analyse their ability to handle these corruptions and identify key
limitations. Our results show that solvers developed for standard puzzles have
a rapid decline in performance if more pieces are corrupted. However, deep
learning models can significantly improve their robustness through fine-tuning
with augmented data. Notably, the advanced Positional Diffusion model adapts
particularly well, outperforming its competitors in most experiments. Based on
our findings, we highlight promising research directions for enhancing the
automated reconstruction of real-world artefacts.

</details>


### [236] [Interpretable EEG-to-Image Generation with Semantic Prompts](https://arxiv.org/abs/2507.07157)
*Arshak Rezvani,Ali Akbari,Kosar Sanjar Arani,Maryam Mirian,Emad Arasteh,Martin J. McKeown*

Main category: cs.CV

TL;DR: 本文提出通过将EEG信号与大语言模型生成的多级语义描述对齐的方法实现EEG视觉解码，在EEGCVPR数据集上取得先进成果。


<details>
  <summary>Details</summary>
Motivation: EEG在图像重建中空间细节不足，需要一种新方法实现从脑信号中解码视觉体验。

Method: 将EEG信号与大语言模型生成的多级语义描述对齐，使用基于Transformer的EEG编码器通过对比学习将脑活动映射到描述上，推理时通过投影头检索描述嵌入以调节预训练的潜在扩散模型进行图像生成。

Result: 在EEGCVPR数据集上实现了最先进的视觉解码，与已知神经认知通路有可解释的对齐，揭示了头皮上的语义地形。

Conclusion: 结构化语义调解能够实现从EEG中进行认知对齐的视觉解码。

Abstract: Decoding visual experience from brain signals offers exciting possibilities
for neuroscience and interpretable AI. While EEG is accessible and temporally
precise, its limitations in spatial detail hinder image reconstruction. Our
model bypasses direct EEG-to-image generation by aligning EEG signals with
multilevel semantic captions -- ranging from object-level to abstract themes --
generated by a large language model. A transformer-based EEG encoder maps brain
activity to these captions through contrastive learning. During inference,
caption embeddings retrieved via projection heads condition a pretrained latent
diffusion model for image generation. This text-mediated framework yields
state-of-the-art visual decoding on the EEGCVPR dataset, with interpretable
alignment to known neurocognitive pathways. Dominant EEG-caption associations
reflected the importance of different semantic levels extracted from perceived
images. Saliency maps and t-SNE projections reveal semantic topography across
the scalp. Our model demonstrates how structured semantic mediation enables
cognitively aligned visual decoding from EEG.

</details>


### [237] [Towards Continuous Home Cage Monitoring: An Evaluation of Tracking and Identification Strategies for Laboratory Mice](https://arxiv.org/abs/2507.07929)
*Juan Pablo Oberhauser,Daniel Grzenda*

Main category: cs.CV

TL;DR: 开发实时识别算法，解决实验小鼠个体指标监测难题，提升跟踪效率。


<details>
  <summary>Details</summary>
Motivation: 连续自动监测实验小鼠可准确收集数据、提升动物福利，但因小鼠特征导致个体指标监测困难。

Method: 开发实时识别算法，包含自定义多目标跟踪器MouseTracks、基于变换器的ID分类器Mouseformer和轨迹关联线性规划MouseMap。

Result: 模型能以每秒30帧的速度根据定制耳标分配动物ID，实现24/7笼内覆盖。

Conclusion: 与现有小鼠跟踪方法相比，自定义跟踪和ID流程提高了跟踪效率，减少了ID切换。

Abstract: Continuous, automated monitoring of laboratory mice enables more accurate
data collection and improves animal welfare through real-time insights.
Researchers can achieve a more dynamic and clinically relevant characterization
of disease progression and therapeutic effects by integrating behavioral and
physiological monitoring in the home cage. However, providing individual mouse
metrics is difficult because of their housing density, similar appearances,
high mobility, and frequent interactions. To address these challenges, we
develop a real-time identification (ID) algorithm that accurately assigns ID
predictions to mice wearing custom ear tags in digital home cages monitored by
cameras. Our pipeline consists of three parts: (1) a custom multiple object
tracker (MouseTracks) that combines appearance and motion cues from mice; (2) a
transformer-based ID classifier (Mouseformer); and (3) a tracklet associator
linear program to assign final ID predictions to tracklets (MouseMap). Our
models assign an animal ID based on custom ear tags at 30 frames per second
with 24/7 cage coverage. We show that our custom tracking and ID pipeline
improves tracking efficiency and lowers ID switches across mouse strains and
various environmental factors compared to current mouse tracking methods.

</details>


### [238] [Scaling RL to Long Videos](https://arxiv.org/abs/2507.07966)
*Yukang Chen,Wei Huang,Baifeng Shi,Qinghao Hu,Hanrong Ye,Ligeng Zhu,Zhijian Liu,Pavlo Molchanov,Jan Kautz,Xiaojuan Qi,Sifei Liu,Hongxu Yin,Yao Lu,Song Han*

Main category: cs.CV

TL;DR: 提出用强化学习扩展视觉语言模型处理长视频推理的全栈框架，包含数据集、训练流程和基础设施，实验表现佳且发布训练系统。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型在长视频推理中的独特挑战，提升其长视频推理能力。

Method: 整合大规模数据集LongVideo - Reason、两阶段训练流程（CoT - SFT和RL）和训练基础设施MR - SP。

Result: LongVILA - R1 - 7B在长视频问答基准测试表现好，MR - SP系统加速长视频RL训练，LongVILA - R1输入帧增多性能提升。

Conclusion: LongVILA - R1向视觉语言模型长视频推理迈出坚实一步，且训练系统公开可用。

Abstract: We introduce a full-stack framework that scales up reasoning in
vision-language models (VLMs) to long videos, leveraging reinforcement
learning. We address the unique challenges of long video reasoning by
integrating three critical components: (1) a large-scale dataset,
LongVideo-Reason, comprising 52K long video QA pairs with high-quality
reasoning annotations across diverse domains such as sports, games, and vlogs;
(2) a two-stage training pipeline that extends VLMs with chain-of-thought
supervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a
training infrastructure for long video RL, named Multi-modal Reinforcement
Sequence Parallelism (MR-SP), which incorporates sequence parallelism and a
vLLM-based engine tailored for long video, using cached video embeddings for
efficient rollout and prefilling. In experiments, LongVILA-R1-7B achieves
strong performance on long video QA benchmarks such as VideoMME. It also
outperforms Video-R1-7B and even matches Gemini-1.5-Pro across temporal
reasoning, goal and purpose reasoning, spatial reasoning, and plot reasoning on
our LongVideo-Reason-eval benchmark. Notably, our MR-SP system achieves up to
2.1x speedup on long video RL training. LongVILA-R1 demonstrates consistent
performance gains as the number of input video frames scales. LongVILA-R1 marks
a firm step towards long video reasoning in VLMs. In addition, we release our
training system for public availability that supports RL training on various
modalities (video, text, and audio), various models (VILA and Qwen series), and
even image and video generation models. On a single A100 node (8 GPUs), it
supports RL training on hour-long videos (e.g., 3,600 frames / around 256k
tokens).

</details>


### [239] [Geometry Forcing: Marrying Video Diffusion and 3D Representation for Consistent World Modeling](https://arxiv.org/abs/2507.07982)
*Haoyu Wu,Diankun Wu,Tianyu He,Junliang Guo,Yang Ye,Yueqi Duan,Jiang Bian*

Main category: cs.CV

TL;DR: 提出Geometry Forcing方法提升视频扩散模型的3D一致性，实验证明效果优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有仅基于原始视频数据训练的视频扩散模型难以捕捉有意义的几何感知结构，需弥合与物理世界3D本质的差距。

Method: 提出Geometry Forcing方法，通过与预训练几何基础模型特征对齐引导模型中间表示，引入角度对齐和尺度对齐两个互补目标。

Result: 在相机视图条件和动作条件视频生成任务上评估，该方法大幅提升视觉质量和3D一致性。

Conclusion: Geometry Forcing方法能有效提升视频扩散模型的性能。

Abstract: Videos inherently represent 2D projections of a dynamic 3D world. However,
our analysis suggests that video diffusion models trained solely on raw video
data often fail to capture meaningful geometric-aware structure in their
learned representations. To bridge this gap between video diffusion models and
the underlying 3D nature of the physical world, we propose Geometry Forcing, a
simple yet effective method that encourages video diffusion models to
internalize latent 3D representations. Our key insight is to guide the model's
intermediate representations toward geometry-aware structure by aligning them
with features from a pretrained geometric foundation model. To this end, we
introduce two complementary alignment objectives: Angular Alignment, which
enforces directional consistency via cosine similarity, and Scale Alignment,
which preserves scale-related information by regressing unnormalized geometric
features from normalized diffusion representation. We evaluate Geometry Forcing
on both camera view-conditioned and action-conditioned video generation tasks.
Experimental results demonstrate that our method substantially improves visual
quality and 3D consistency over the baseline methods. Project page:
https://GeometryForcing.github.io.

</details>


### [240] [Multi-Granular Spatio-Temporal Token Merging for Training-Free Acceleration of Video LLMs](https://arxiv.org/abs/2507.07990)
*Jeongseok Hyun,Sukjun Hwang,Su Ho Han,Taeoh Kim,Inwoong Lee,Dongyoon Wee,Joon-Young Lee,Seon Joo Kim,Minho Shim*

Main category: cs.CV

TL;DR: 提出无训练时空令牌合并方法STTM，利用视频数据冗余，在多个视频QA基准上表现好，有速度提升且准确性损失小。


<details>
  <summary>Details</summary>
Motivation: 解决视频大语言模型因时空令牌数量增加导致计算复杂度呈二次增长的问题。

Method: 先将每帧通过四叉树结构从粗到细搜索转换为多粒度空间令牌，再跨时间维度进行有向成对合并。

Result: 在六个视频QA基准上优于现有令牌减少方法，特定令牌预算下有显著速度提升和较小准确性损失，且支持KV缓存重用。

Conclusion: 所提出的STTM方法有效解决了视频大语言模型计算复杂度问题。

Abstract: Video large language models (LLMs) achieve strong video understanding by
leveraging a large number of spatio-temporal tokens, but suffer from quadratic
computational scaling with token count. To address this, we propose a
training-free spatio-temporal token merging method, named STTM. Our key insight
is to exploit local spatial and temporal redundancy in video data which has
been overlooked in prior work. STTM first transforms each frame into
multi-granular spatial tokens using a coarse-to-fine search over a quadtree
structure, then performs directed pairwise merging across the temporal
dimension. This decomposed merging approach outperforms existing token
reduction methods across six video QA benchmarks. Notably, STTM achieves a
2$\times$ speed-up with only a 0.5% accuracy drop under a 50% token budget, and
a 3$\times$ speed-up with just a 2% drop under a 30% budget. Moreover, STTM is
query-agnostic, allowing KV cache reuse across different questions for the same
video. The project page is available at https://www.jshyun.me/projects/sttm.

</details>


### [241] [Multigranular Evaluation for Brain Visual Decoding](https://arxiv.org/abs/2507.07993)
*Weihao Xia,Cengiz Oztireli*

Main category: cs.CV

TL;DR: 现有脑视觉解码评估协议有局限，本文提出BASIC统一多粒度评估框架并进行基准测试，为评估提供更好基础。


<details>
  <summary>Details</summary>
Motivation: 现有脑视觉解码评估协议依赖粗粒度指标，掩盖模型差异、缺乏神经科学基础且无法捕捉细粒度视觉差异。

Method: 引入BASIC框架，在结构层面采用基于分割的分层指标集，在语义层面用多模态大语言模型提取结构化场景表示，在统一框架下对多种视觉解码方法进行基准测试。

Result: 未提及具体结果。

Conclusion: 这些标准为衡量脑视觉解码方法提供了更具区分性、可解释性和全面性的基础。

Abstract: Existing evaluation protocols for brain visual decoding predominantly rely on
coarse metrics that obscure inter-model differences, lack neuroscientific
foundation, and fail to capture fine-grained visual distinctions. To address
these limitations, we introduce BASIC, a unified, multigranular evaluation
framework that jointly quantifies structural fidelity, inferential alignment,
and contextual coherence between decoded and ground truth images. For the
structural level, we introduce a hierarchical suite of segmentation-based
metrics, including foreground, semantic, instance, and component masks,
anchored in granularity-aware correspondence across mask structures. For the
semantic level, we extract structured scene representations encompassing
objects, attributes, and relationships using multimodal large language models,
enabling detailed, scalable, and context-rich comparisons with ground-truth
stimuli. We benchmark a diverse set of visual decoding methods across multiple
stimulus-neuroimaging datasets within this unified evaluation framework.
Together, these criteria provide a more discriminative, interpretable, and
comprehensive foundation for measuring brain visual decoding methods.

</details>


### [242] [Single-pass Adaptive Image Tokenization for Minimum Program Search](https://arxiv.org/abs/2507.07995)
*Shivam Duggal,Sanghyun Byun,William T. Freeman,Antonio Torralba,Phillip Isola*

Main category: cs.CV

TL;DR: 受科尔莫戈罗夫复杂性原理启发，提出单遍自适应分词器KARL，性能与现有自适应分词器相当，还给出缩放定律并进行概念研究。


<details>
  <summary>Details</summary>
Motivation: 现有视觉表示学习系统使用固定长度表示，忽略复杂性或熟悉度变化，近期自适应分词方法需测试时多次搜索。

Method: 提出单遍自适应分词器KARL，其训练过程类似上下颠倒强化学习范式，基于期望重建质量预测分词停止。

Result: KARL性能与近期自适应分词器相当，给出了缩放定律。

Conclusion: KARL能单遍运行，其预测的图像复杂性与人类直觉一致。

Abstract: According to Algorithmic Information Theory (AIT) -- Intelligent
representations compress data into the shortest possible program that can
reconstruct its content, exhibiting low Kolmogorov Complexity (KC). In
contrast, most visual representation learning systems use fixed-length
representations for all inputs, ignoring variations in complexity or
familiarity. Recent adaptive tokenization methods address this by allocating
variable-length representations but typically require test-time search over
multiple encodings to find the most predictive one. Inspired by Kolmogorov
Complexity principles, we propose a single-pass adaptive tokenizer, KARL, which
predicts the appropriate number of tokens for an image in a single forward
pass, halting once its approximate KC is reached. The token count serves as a
proxy for the minimum description length. KARL's training procedure closely
resembles the Upside-Down Reinforcement Learning paradigm, as it learns to
conditionally predict token halting based on a desired reconstruction quality.
KARL matches the performance of recent adaptive tokenizers while operating in a
single pass. We present scaling laws for KARL, analyzing the role of
encoder/decoder size, continuous vs. discrete tokenization and more.
Additionally, we offer a conceptual study drawing an analogy between Adaptive
Image Tokenization and Algorithmic Information Theory, examining the predicted
image complexity (KC) across axes such as structure vs. noise and in- vs.
out-of-distribution familiarity -- revealing alignment with human intuition.

</details>


### [243] [Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and Methodology](https://arxiv.org/abs/2507.07999)
*Haochen Wang,Xiangtai Li,Zilong Huang,Anran Wang,Jiacong Wang,Tao Zhang,Jiani Zheng,Sule Bai,Zijian Kang,Jiashi Feng,Zhuochen Wang,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: 提出TreeBench基准测试评估视觉基础推理能力，引入TreeVGR训练范式提升相关任务表现，证明可追溯性对视觉基础推理很重要。


<details>
  <summary>Details</summary>
Motivation: 现无全面评估视觉基础推理能力的基准，需填补此空白。

Method: 提出TreeBench基准，基于三个原则构建，从SA - 1B采样图像并人工标注；引入TreeVGR训练范式，用强化学习联合监督定位和推理。

Result: TreeBench具有挑战性，先进模型准确率难达60%；TreeVGR提升了V* Bench、MME - RealWorld和TreeBench的表现。

Conclusion: 可追溯性是推进视觉基础推理的关键。

Abstract: Models like OpenAI-o3 pioneer visual grounded reasoning by dynamically
referencing visual regions, just like human "thinking with images". However, no
benchmark exists to evaluate these capabilities holistically. To bridge this
gap, we propose TreeBench (Traceable Evidence Evaluation Benchmark), a
diagnostic benchmark built on three principles: (1) focused visual perception
of subtle targets in complex scenes, (2) traceable evidence via bounding box
evaluation, and (3) second-order reasoning to test object interactions and
spatial hierarchies beyond simple object localization. Prioritizing images with
dense objects, we initially sample 1K high-quality images from SA-1B, and
incorporate eight LMM experts to manually annotate questions, candidate
options, and answers for each image. After three stages of quality control,
TreeBench consists of 405 challenging visual question-answering pairs, even the
most advanced models struggle with this benchmark, where none of them reach 60%
accuracy, e.g., OpenAI-o3 scores only 54.87. Furthermore, we introduce TreeVGR
(Traceable Evidence Enhanced Visual Grounded Reasoning), a training paradigm to
supervise localization and reasoning jointly with reinforcement learning,
enabling accurate localizations and explainable reasoning pathways. Initialized
from Qwen2.5-VL-7B, it improves V* Bench (+16.8), MME-RealWorld (+12.6), and
TreeBench (+13.4), proving traceability is key to advancing vision-grounded
reasoning. The code is available at https://github.com/Haochen-Wang409/TreeVGR.

</details>


### [244] [Divergence Minimization Preference Optimization for Diffusion Model Alignment](https://arxiv.org/abs/2507.07510)
*Binxu Li,Minkai Xu,Meihua Dang,Stefano Ermon*

Main category: cs.CV

TL;DR: 本文提出DMPO方法用于对齐扩散模型，通过最小化反向KL散度，实验证明该方法优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 受语言模型进展启发，为进一步提升扩散模型与人类偏好的对齐程度，且现有偏好优化方法存在局限。

Method: 引入Divergence Minimization Preference Optimization (DMPO)方法，通过最小化反向KL散度来对齐扩散模型。

Result: 扩散模型经DMPO微调后在人类评估和自动指标上表现出色，在PickScore上至少比现有基线高64.6%。

Conclusion: DMPO为偏好对齐提供了稳健且优雅的途径，弥合了扩散模型理论与实践的差距。

Abstract: Diffusion models have achieved remarkable success in generating realistic and
versatile images from text prompts. Inspired by the recent advancements of
language models, there is an increasing interest in further improving the
models by aligning with human preferences. However, we investigate alignment
from a divergence minimization perspective and reveal that existing preference
optimization methods are typically trapped in suboptimal mean-seeking
optimization. In this paper, we introduce Divergence Minimization Preference
Optimization (DMPO), a novel and principled method for aligning diffusion
models by minimizing reverse KL divergence, which asymptotically enjoys the
same optimization direction as original RL. We provide rigorous analysis to
justify the effectiveness of DMPO and conduct comprehensive experiments to
validate its empirical strength across both human evaluations and automatic
metrics. Our extensive results show that diffusion models fine-tuned with DMPO
can consistently outperform or match existing techniques, specifically
outperforming all existing diffusion alignment baselines by at least 64.6% in
PickScore across all evaluation datasets, demonstrating the method's
superiority in aligning generative behavior with desired outputs. Overall, DMPO
unlocks a robust and elegant pathway for preference alignment, bridging
principled theory with practical performance in diffusion models.

</details>


### [245] [TinierHAR: Towards Ultra-Lightweight Deep Learning Models for Efficient Human Activity Recognition on Edge Devices](https://arxiv.org/abs/2507.07949)
*Sizhen Bian,Mengxi Liu,Vitor Fortes Rey,Daniel Geissler,Paul Lukowicz*

Main category: cs.CV

TL;DR: 本文提出超轻量级深度学习架构TinierHAR用于资源受限可穿戴设备的人类活动识别，评估显示其高效且保持性能，还做了消融研究并给出设计准则，开源相关材料。


<details>
  <summary>Details</summary>
Motivation: 资源受限可穿戴设备的人类活动识别需要协调准确性和计算效率的推理模型。

Method: 引入TinierHAR架构，结合残差深度可分离卷积、门控循环单元（GRUs）和时间聚合。

Result: 在14个公共数据集上评估，TinierHAR相比TinyHAR和DeepConvLSTM减少了参数和MACs，同时保持平均F1分数；进行了系统的消融研究。

Conclusion: 给出了未来高效人类活动识别系统的设计准则，并开源材料以促进边缘人类活动识别研究。

Abstract: Human Activity Recognition (HAR) on resource-constrained wearable devices
demands inference models that harmonize accuracy with computational efficiency.
This paper introduces TinierHAR, an ultra-lightweight deep learning
architecture that synergizes residual depthwise separable convolutions, gated
recurrent units (GRUs), and temporal aggregation to achieve SOTA efficiency
without compromising performance. Evaluated across 14 public HAR datasets,
TinierHAR reduces Parameters by 2.7x (vs. TinyHAR) and 43.3x (vs.
DeepConvLSTM), and MACs by 6.4x and 58.6x, respectively, while maintaining the
averaged F1-scores. Beyond quantitative gains, this work provides the first
systematic ablation study dissecting the contributions of spatial-temporal
components across proposed TinierHAR, prior SOTA TinyHAR, and the classical
DeepConvLSTM, offering actionable insights for designing efficient HAR systems.
We finally discussed the findings and suggested principled design guidelines
for future efficient HAR. To catalyze edge-HAR research, we open-source all
materials in this work for future
benchmarking\footnote{https://github.com/zhaxidele/TinierHAR}

</details>


### [246] [Impact of Pretraining Word Co-occurrence on Compositional Generalization in Multimodal Models](https://arxiv.org/abs/2507.08000)
*Helen Qu,Sang Michael Xie*

Main category: cs.CV

TL;DR: 研究预训练数据中单词共现统计对CLIP/LMM性能的影响，发现与零样本准确率强相关，该现象在自然图像和基于CLIP的LMM中也存在，强调提升组合泛化能力的算法和架构的必要性。


<details>
  <summary>Details</summary>
Motivation: CLIP和LMM在训练数据中高代表性概念示例上准确率较高，但训练数据中概念组合对组合泛化的作用尚不明确，因此研究预训练数据中单词共现统计对其性能的影响。

Method: 用点互信息（PMI）衡量共现，以分离单词共现频率和单单词频率的影响，使用合成图像和编辑自然图像进行实验。

Result: CLIP预训练数据中的PMI与零样本准确率强相关（r=0.97），自然图像中相关系数为r=0.75，该现象在基于CLIP的LMM中也存在（TextVQA r=0.70，VQAv2 r=0.62）。

Conclusion: 需要无需组合式扩展训练数据就能提升多模态模型组合泛化能力的算法和架构。

Abstract: CLIP and large multimodal models (LMMs) have better accuracy on examples
involving concepts that are highly represented in the training data. However,
the role of concept combinations in the training data on compositional
generalization is largely unclear -- for instance, how does accuracy vary when
a common object appears in an uncommon pairing with another object? In this
paper, we investigate how word co-occurrence statistics in the pretraining
dataset (a proxy for co-occurrence of visual concepts) impacts CLIP/LMM
performance. To disentangle the effects of word co-occurrence frequencies from
single-word frequencies, we measure co-occurrence with pointwise mutual
information (PMI), which normalizes the joint probability of two words
co-occurring by the probability of co-occurring independently. Using
synthetically generated images with a variety of concept pairs, we show a
strong correlation between PMI in the CLIP pretraining data and zero-shot
accuracy in CLIP models trained on LAION-400M (r=0.97 and 14% accuracy gap
between images in the top and bottom 5% of PMI values), demonstrating that even
accuracy on common concepts is affected by the combination of concepts in the
image. Leveraging this finding, we reproduce this effect in natural images by
editing them to contain pairs with varying PMI, resulting in a correlation of
r=0.75. Finally, we demonstrate that this behavior in CLIP transfers to LMMs
built on top of CLIP (r=0.70 for TextVQA, r=0.62 for VQAv2). Our findings
highlight the need for algorithms and architectures that improve compositional
generalization in multimodal models without scaling the training data
combinatorially. Our code is available at
https://github.com/helenqu/multimodal-pretraining-pmi.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [247] [Feature-free regression kriging](https://arxiv.org/abs/2507.07382)
*Peng Luo,Yilong Wu,Yongze Song*

Main category: physics.soc-ph

TL;DR: 本文提出无特征回归克里金法（FFRK），在澳大利亚矿区重金属分布预测实验中，不依赖外部解释变量，仅靠提取的地理空间特征就优于传统克里金和依赖解释变量的机器学习模型，有效解决空间非平稳性问题。


<details>
  <summary>Details</summary>
Motivation: 传统地统计模型难捕捉地理变量非平稳特征，趋势面建模依赖高质量外部解释变量，而很多场景下缺乏这些变量。

Method: 提出FFRK方法，自动提取地理空间特征构建基于回归的趋势面，不依赖外部解释变量。

Result: 在澳大利亚矿区三种重金属空间分布预测实验中，FFRK相比17种经典插值方法表现更优。

Conclusion: 基于领域知识准确表征地理空间特征可显著提升空间预测性能，可能比采用更高级统计模型效果更好。

Abstract: Spatial interpolation is a crucial task in geography. As perhaps the most
widely used interpolation methods, geostatistical models -- such as Ordinary
Kriging (OK) -- assume spatial stationarity, which makes it difficult to
capture the nonstationary characteristics of geographic variables. A common
solution is trend surface modeling (e.g., Regression Kriging, RK), which relies
on external explanatory variables to model the trend and then applies
geostatistical interpolation to the residuals. However, this approach requires
high-quality and readily available explanatory variables, which are often
lacking in many spatial interpolation scenarios -- such as estimating heavy
metal concentrations underground. This study proposes a Feature-Free Regression
Kriging (FFRK) method, which automatically extracts geospatial features --
including local dependence, local heterogeneity, and geosimilarity -- to
construct a regression-based trend surface without requiring external
explanatory variables. We conducted experiments on the spatial distribution
prediction of three heavy metals in a mining area in Australia. In comparison
with 17 classical interpolation methods, the results indicate that FFRK, which
does not incorporate any explanatory variables and relies solely on extracted
geospatial features, consistently outperforms both conventional Kriging
techniques and machine learning models that depend on explanatory variables.
This approach effectively addresses spatial nonstationarity while reducing the
cost of acquiring explanatory variables, improving both prediction accuracy and
generalization ability. This finding suggests that an accurate characterization
of geospatial features based on domain knowledge can significantly enhance
spatial prediction performance -- potentially yielding greater improvements
than merely adopting more advanced statistical models.

</details>


<div id='q-fin.GN'></div>

# q-fin.GN [[Back]](#toc)

### [248] [Time Series Foundation Models for Multivariate Financial Time Series Forecasting](https://arxiv.org/abs/2507.07296)
*Ben A. Marconi*

Main category: q-fin.GN

TL;DR: 研究评估两个时间序列基础模型（TSFMs）用于金融预测任务，结果显示TSFMs有潜力，但传统模型在部分任务表现更好，需特定预训练和架构改进。


<details>
  <summary>Details</summary>
Motivation: 金融时间序列预测因复杂特性存在挑战，TSFMs有望解决问题，评估其在金融预测任务中的表现。

Method: 评估两个TSFMs（TTM和Chronos）在三个金融预测任务中的表现。

Result: TTM有强可迁移性，微调时表现更好，零样本表现超朴素基准，但传统专业模型在两个任务中表现相当或更优。

Conclusion: TSFMs虽新兴，但在金融预测有潜力，尤其数据受限任务，需特定预训练和架构改进以达竞争性能。

Abstract: Financial time series forecasting presents significant challenges due to
complex nonlinear relationships, temporal dependencies, variable
interdependencies and limited data availability, particularly for tasks
involving low-frequency data, newly listed instruments, or emerging market
assets. Time Series Foundation Models (TSFMs) offer a promising solution
through pretraining on diverse time series corpora followed by task-specific
adaptation. This study evaluates two TSFMs (Tiny Time Mixers (TTM) and Chronos)
across three financial forecasting tasks: US 10-year Treasury yield changes,
EUR/USD volatility, and equity spread prediction. Results demonstrate that TTM
exhibits strong transferability. When fine-tuning both the pretrained version
of TTM and an untrained model with the same architecture, the pretrained
version achieved 25-50% better performance when fine-tuned on limited data and
15-30% improvements even when fine-tuned on lengthier datasets. Notably, TTM's
zero-shot performance outperformed naive benchmarks in volatility forecasting
and equity spread prediction, with the latter demonstrating that TSFMs can
surpass traditional benchmark models without fine-tuning. The pretrained model
consistently required 3-10 fewer years of data to achieve comparable
performance levels compared to the untrained model, demonstrating significant
sample-efficiency gains. However, while TTM outperformed naive baselines,
traditional specialised models matched or exceeded its performance in two of
three tasks, suggesting TSFMs prioritise breadth over task-specific
optimisation. These findings indicate that TSFMs, though still nascent, offer
substantial promise for financial forecasting-particularly in noisy,
data-constrained tasks-but achieving competitive performance likely requires
domain-specific pretraining and architectural refinements tailored to financial
time series characteristics.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [249] [Way More Than the Sum of Their Parts: From Statistical to Structural Mixtures](https://arxiv.org/abs/2507.07343)
*James P. Crutchfield*

Main category: cond-mat.stat-mech

TL;DR: 多组分系统混合物结构复杂度远超各部分之和，与统计混合物不同，还引出新结构复杂度及对系统遍历性的影响。


<details>
  <summary>Details</summary>
Motivation: 揭示多组分系统混合物的结构复杂度特性，对比统计混合物的不同。

Method: 通过对比多组分系统混合物和统计混合物的方式进行分析。

Result: 发现多组分系统存在新的结构复杂度，并得出对系统遍历性有广泛影响的结果。

Conclusion: 多组分系统混合物结构比想象复杂，统计混合物无法体现其关键特性，新结构复杂度影响系统遍历性。

Abstract: We show that mixtures comprised of multicomponent systems typically are much
more structurally complex than the sum of their parts; sometimes, infinitely
more complex. We contrast this with the more familiar notion of statistical
mixtures, demonstrating how statistical mixtures miss key aspects of emergent
hierarchical organization. This leads us to identify a new kind of structural
complexity inherent in multicomponent systems and to draw out broad
consequences for system ergodicity.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [250] [FedP3E: Privacy-Preserving Prototype Exchange for Non-IID IoT Malware Detection in Cross-Silo Federated Learning](https://arxiv.org/abs/2507.07258)
*Rami Darwish,Mahmoud Abdelsalam,Sajad Khorsandroo,Kaushik Roy*

Main category: cs.CR

TL;DR: 物联网生态系统面临复杂恶意软件攻击，传统联邦学习算法有局限，提出FedP3E框架解决问题并评估。


<details>
  <summary>Details</summary>
Motivation: 物联网易受攻击，需隐私保护且能应对数据异构的检测框架，传统联邦学习算法在非IID数据和类别不平衡场景表现不佳。

Method: 提出FedP3E框架，客户端用GMM构建类原型，加高斯噪声后传给服务器，聚合后返回客户端并结合SMOTE增强少数类表示。

Result: 文中未明确提及具体结果，将在N - BaIoT数据集上评估。

Conclusion: 文中未明确得出结论，预计该框架可减少统计异质性影响，通信开销小。

Abstract: As IoT ecosystems continue to expand across critical sectors, they have
become prominent targets for increasingly sophisticated and large-scale malware
attacks. The evolving threat landscape, combined with the sensitive nature of
IoT-generated data, demands detection frameworks that are both
privacy-preserving and resilient to data heterogeneity. Federated Learning (FL)
offers a promising solution by enabling decentralized model training without
exposing raw data. However, standard FL algorithms such as FedAvg and FedProx
often fall short in real-world deployments characterized by class imbalance and
non-IID data distributions -- particularly in the presence of rare or disjoint
malware classes. To address these challenges, we propose FedP3E
(Privacy-Preserving Prototype Exchange), a novel FL framework that supports
indirect cross-client representation sharing while maintaining data privacy.
Each client constructs class-wise prototypes using Gaussian Mixture Models
(GMMs), perturbs them with Gaussian noise, and transmits only these compact
summaries to the server. The aggregated prototypes are then distributed back to
clients and integrated into local training, supported by SMOTE-based
augmentation to enhance representation of minority malware classes. Rather than
relying solely on parameter averaging, our prototype-driven mechanism enables
clients to enrich their local models with complementary structural patterns
observed across the federation -- without exchanging raw data or gradients.
This targeted strategy reduces the adverse impact of statistical heterogeneity
with minimal communication overhead. We evaluate FedP3E on the N-BaIoT dataset
under realistic cross-silo scenarios with varying degrees of data imbalance.

</details>


### [251] [Phishing Detection in the Gen-AI Era: Quantized LLMs vs Classical Models](https://arxiv.org/abs/2507.07406)
*Jikesh Thapa,Gurrehmat Chahal,Serban Voinea Gabreanu,Yazan Otoum*

Main category: cs.CR

TL;DR: 本文对比传统机器学习、深度学习和量化小参数大语言模型用于网络钓鱼检测，发现大语言模型虽原始准确率不如前两者，但识别细微线索有潜力，部分模型准确率超80%且显存需求低，为网络钓鱼防御系统提供新思路。


<details>
  <summary>Details</summary>
Motivation: 网络钓鱼攻击日益复杂，需要高精度与计算效率平衡的检测系统。

Method: 在特定数据集上对传统机器学习、深度学习和量化小参数大语言模型进行对比实验，研究零样本和少样本提示策略影响，评估模型对抗鲁棒性和成本 - 性能权衡。

Result: 大语言模型原始准确率不如机器学习和深度学习方法，但识别细微线索有潜力；大语言模型改写的邮件会降低检测性能；部分模型如DeepSeek R1 Distill Qwen 14B (Q8_0)准确率超80%，仅需17GB显存；轻量级大语言模型可提供简洁解释支持实时决策。

Conclusion: 优化后的大语言模型可作为网络钓鱼防御系统的有前景组件，为现代网络安全框架集成可解释、高效的人工智能指明方向。

Abstract: Phishing attacks are becoming increasingly sophisticated, underscoring the
need for detection systems that strike a balance between high accuracy and
computational efficiency. This paper presents a comparative evaluation of
traditional Machine Learning (ML), Deep Learning (DL), and quantized
small-parameter Large Language Models (LLMs) for phishing detection. Through
experiments on a curated dataset, we show that while LLMs currently
underperform compared to ML and DL methods in terms of raw accuracy, they
exhibit strong potential for identifying subtle, context-based phishing cues.
We also investigate the impact of zero-shot and few-shot prompting strategies,
revealing that LLM-rephrased emails can significantly degrade the performance
of both ML and LLM-based detectors. Our benchmarking highlights that models
like DeepSeek R1 Distill Qwen 14B (Q8_0) achieve competitive accuracy, above
80%, using only 17GB of VRAM, supporting their viability for cost-efficient
deployment. We further assess the models' adversarial robustness and
cost-performance tradeoffs, and demonstrate how lightweight LLMs can provide
concise, interpretable explanations to support real-time decision-making. These
findings position optimized LLMs as promising components in phishing defence
systems and offer a path forward for integrating explainable, efficient AI into
modern cybersecurity frameworks.

</details>


### [252] [Hybrid LLM-Enhanced Intrusion Detection for Zero-Day Threats in IoT Networks](https://arxiv.org/abs/2507.07413)
*Mohammad F. Al-Hammouri,Yazan Otoum,Rasha Atwa,Amiya Nayak*

Main category: cs.CR

TL;DR: 本文提出结合传统签名方法与GPT - 2大语言模型的入侵检测新方法，实验显示能提升检测效果，肯定了语言模型集成在网络安全防御中的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着网络威胁日益复杂，传统入侵检测系统在分布式、异构和资源受限环境中难以识别新攻击模式，需要动态自适应的入侵检测系统。

Method: 提出一个混合入侵检测系统框架，将基于签名技术的稳健性与GPT - 2驱动的语义分析的适应性相结合。

Result: 在代表性入侵数据集上实验表明，模型检测准确率提高6.3%，误报率降低9.0%，并保持近实时响应。

Conclusion: 肯定了语言模型集成在构建适用于现代互联环境的智能、可扩展和有弹性的网络安全防御方面的潜力。

Abstract: This paper presents a novel approach to intrusion detection by integrating
traditional signature-based methods with the contextual understanding
capabilities of the GPT-2 Large Language Model (LLM). As cyber threats become
increasingly sophisticated, particularly in distributed, heterogeneous, and
resource-constrained environments such as those enabled by the Internet of
Things (IoT), the need for dynamic and adaptive Intrusion Detection Systems
(IDSs) becomes increasingly urgent. While traditional methods remain effective
for detecting known threats, they often fail to recognize new and evolving
attack patterns. In contrast, GPT-2 excels at processing unstructured data and
identifying complex semantic relationships, making it well-suited to uncovering
subtle, zero-day attack vectors. We propose a hybrid IDS framework that merges
the robustness of signature-based techniques with the adaptability of
GPT-2-driven semantic analysis. Experimental evaluations on a representative
intrusion dataset demonstrate that our model enhances detection accuracy by
6.3%, reduces false positives by 9.0%, and maintains near real-time
responsiveness. These results affirm the potential of language model
integration to build intelligent, scalable, and resilient cybersecurity
defences suited for modern connected environments.

</details>


### [253] [Autonomous AI-based Cybersecurity Framework for Critical Infrastructure: Real-Time Threat Mitigation](https://arxiv.org/abs/2507.07416)
*Jenifer Paulraj,Brindha Raghuraman,Nagarani Gopalakrishnan,Yazan Otoum*

Main category: cs.CR

TL;DR: 本文探讨关键基础设施的网络安全漏洞，提出混合AI驱动框架，研究还处理相关复杂问题，为增强安全提供见解。


<details>
  <summary>Details</summary>
Motivation: 关键基础设施因互联性面临多种网络威胁，需保障其安全和恢复力。

Method: 提出混合AI驱动的网络安全框架，用于实时漏洞检测、威胁建模和自动修复。

Result: 研究解决了对抗性AI、监管合规和集成等复杂问题。

Conclusion: 研究结果为增强关键基础设施系统抵御新兴网络威胁的安全性和恢复力提供了可行的见解。

Abstract: Critical infrastructure systems, including energy grids, healthcare
facilities, transportation networks, and water distribution systems, are
pivotal to societal stability and economic resilience. However, the increasing
interconnectivity of these systems exposes them to various cyber threats,
including ransomware, Denial-of-Service (DoS) attacks, and Advanced Persistent
Threats (APTs). This paper examines cybersecurity vulnerabilities in critical
infrastructure, highlighting the threat landscape, attack vectors, and the role
of Artificial Intelligence (AI) in mitigating these risks. We propose a hybrid
AI-driven cybersecurity framework to enhance real-time vulnerability detection,
threat modelling, and automated remediation. This study also addresses the
complexities of adversarial AI, regulatory compliance, and integration. Our
findings provide actionable insights to strengthen the security and resilience
of critical infrastructure systems against emerging cyber threats.

</details>


### [254] [May I have your Attention? Breaking Fine-Tuning based Prompt Injection Defenses using Architecture-Aware Attacks](https://arxiv.org/abs/2507.07417)
*Nishit V. Pandya,Andrey Labunets,Sicun Gao,Earlence Fernandes*

Main category: cs.CR

TL;DR: 本文评估白盒场景下一类提示注入防御的鲁棒性，构造攻击算法攻击两种防御方法，成功率达70%，并公开代码。


<details>
  <summary>Details</summary>
Motivation: 评估一类依赖微调模型分离指令和数据的提示注入防御在白盒场景下的鲁棒性。

Method: 构造基于优化的强攻击，提出基于注意力的攻击算法并应用于两种白盒防御方法。

Result: 对SecAlign和StruQ的攻击成功率达70%，攻击者令牌预算适度增加。

Conclusion: 研究在理解白盒场景下提示注入防御的鲁棒性方面取得了重要进展。

Abstract: A popular class of defenses against prompt injection attacks on large
language models (LLMs) relies on fine-tuning the model to separate instructions
and data, so that the LLM does not follow instructions that might be present
with data. There are several academic systems and production-level
implementations of this idea. We evaluate the robustness of this class of
prompt injection defenses in the whitebox setting by constructing strong
optimization-based attacks and showing that the defenses do not provide the
claimed security properties. Specifically, we construct a novel attention-based
attack algorithm for text-based LLMs and apply it to two recent whitebox
defenses SecAlign (CCS 2025) and StruQ (USENIX Security 2025), showing attacks
with success rates of up to 70% with modest increase in attacker budget in
terms of tokens. Our findings make fundamental progress towards understanding
the robustness of prompt injection defenses in the whitebox setting. We release
our code and attacks at https://github.com/nishitvp/better_opts_attacks

</details>


### [255] [Mitigating Watermark Stealing Attacks in Generative Models via Multi-Key Watermarking](https://arxiv.org/abs/2507.07871)
*Toluwani Aremu,Noor Hussein,Munachiso Nwadike,Samuele Poppi,Jie Zhang,Karthik Nandakumar,Neil Gong,Nils Lukas*

Main category: cs.CR

TL;DR: 文章聚焦GenAI水印防盗用攻击，提出多密钥扩展方法，有理论保证且实证有效，还定义了水印伪造威胁。


<details>
  <summary>Details</summary>
Motivation: GenAI水印存在被盗用攻击的威胁，攻击者可能伪造水印诬陷提供商，因此要减轻此类攻击。

Method: 提出多密钥扩展方法，可事后应用于任何水印方法；通过安全游戏建模水印伪造威胁。

Result: 该方法在多个数据集上使伪造水印的效果大幅降低。

Conclusion: 多密钥扩展方法能有效减轻水印盗用攻击，为GenAI水印安全提供了解决方案。

Abstract: Watermarking offers a promising solution for GenAI providers to establish the
provenance of their generated content. A watermark is a hidden signal embedded
in the generated content, whose presence can later be verified using a secret
watermarking key. A threat to GenAI providers are \emph{watermark stealing}
attacks, where users forge a watermark into content that was \emph{not}
generated by the provider's models without access to the secret key, e.g., to
falsely accuse the provider. Stealing attacks collect \emph{harmless}
watermarked samples from the provider's model and aim to maximize the expected
success rate of generating \emph{harmful} watermarked samples. Our work focuses
on mitigating stealing attacks while treating the underlying watermark as a
black-box. Our contributions are: (i) Proposing a multi-key extension to
mitigate stealing attacks that can be applied post-hoc to any watermarking
method across any modality. (ii) We provide theoretical guarantees and
demonstrate empirically that our method makes forging substantially less
effective across multiple datasets, and (iii) we formally define the threat of
watermark forging as the task of generating harmful, watermarked content and
model this threat via security games.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [256] [Machine Learning-Assisted Surrogate Modeling with Multi-Objective Optimization and Decision-Making of a Steam Methane Reforming Reactor](https://arxiv.org/abs/2507.07641)
*Seyed Reza Nabavi,Zonglin Guo,Zhiyuan Wang*

Main category: physics.chem-ph

TL;DR: 提出蒸汽甲烷重整（SMR）反应器集成建模与优化框架，结合多种技术，优化效果好且策略有效可扩展。


<details>
  <summary>Details</summary>
Motivation: 解决蒸汽甲烷重整反应器多目标优化问题，降低计算成本。

Method: 采用一维固定床反应器模型，构建混合 ANN 替代模型，用 NSGA - II 进行多目标优化，用 TOPSIS 和 sPROBID 进行多准则决策。

Result: 第一案例甲烷转化率 0.863，氢气输出 4.556 mol/s；第三案例甲烷转化率 0.988，氢气 3.335 mol/s，二氧化碳 0.781 mol/s。

Conclusion: 该综合方法为复杂催化反应器系统多目标优化提供可扩展有效策略。

Abstract: This study presents an integrated modeling and optimization framework for a
steam methane reforming (SMR) reactor, combining a mathematical model,
artificial neural network (ANN)-based hybrid modeling, advanced multi-objective
optimization (MOO) and multi-criteria decision-making (MCDM) techniques. A
one-dimensional fixed-bed reactor model accounting for internal mass transfer
resistance was employed to simulate reactor performance. To reduce the high
computational cost of the mathematical model, a hybrid ANN surrogate was
constructed, achieving a 93.8% reduction in average simulation time while
maintaining high predictive accuracy. The hybrid model was then embedded into
three MOO scenarios using the non-dominated sorting genetic algorithm II
(NSGA-II) solver: 1) maximizing methane conversion and hydrogen output; 2)
maximizing hydrogen output while minimizing carbon dioxide emissions; and 3) a
combined three-objective case. The optimal trade-off solutions were further
ranked and selected using two MCDM methods: technique for order of preference
by similarity to ideal solution (TOPSIS) and simplified preference ranking on
the basis of ideal-average distance (sPROBID). Optimal results include a
methane conversion of 0.863 with 4.556 mol/s hydrogen output in the first case,
and 0.988 methane conversion with 3.335 mol/s hydrogen and 0.781 mol/s carbon
dioxide in the third. This comprehensive methodology offers a scalable and
effective strategy for optimizing complex catalytic reactor systems with
multiple, often conflicting, objectives.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [257] [Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics](https://arxiv.org/abs/2507.07155)
*Xueqing Xu,Boris Bolliet,Adrian Dimitrov,Andrew Laverick,Francisco Villaescusa-Navarro,Licong Xu,Íñigo Zubeldia*

Main category: astro-ph.IM

TL;DR: 评估9种RAG代理配置在105个宇宙学问答对上的表现，找到最佳配置，校准LLMaaJ系统并公开相关数据。


<details>
  <summary>Details</summary>
Motivation: 为天体物理学中自主科学发现的多智能体系统系统地选择最佳RAG代理配置，并提供可扩展的评估系统。

Method: 对9种RAG配置在105个宇宙学问答对上生成的945个答案进行人工评估，用评估结果校准LLMaaJ系统。

Result: 最佳RAG代理配置为使用OpenAI嵌入和生成模型，准确率达91.4%；校准了可作为人工评估代理的LLMaaJ系统。

Conclusion: 可系统选择最佳RAG代理配置，LLMaaJ系统可扩展，公开相关数据供天体物理学界使用。

Abstract: We evaluate 9 Retrieval Augmented Generation (RAG) agent configurations on
105 Cosmology Question-Answer (QA) pairs that we built specifically for this
purpose.The RAG configurations are manually evaluated by a human expert, that
is, a total of 945 generated answers were assessed. We find that currently the
best RAG agent configuration is with OpenAI embedding and generative model,
yielding 91.4\% accuracy. Using our human evaluation results we calibrate
LLM-as-a-Judge (LLMaaJ) system which can be used as a robust proxy for human
evaluation. These results allow us to systematically select the best RAG agent
configuration for multi-agent system for autonomous scientific discovery in
astrophysics (e.g., cmbagent presented in a companion paper) and provide us
with an LLMaaJ system that can be scaled to thousands of cosmology QA pairs. We
make our QA dataset, human evaluation results, RAG pipelines, and LLMaaJ system
publicly available for further use by the astrophysics community.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [258] [Accelerating Transposed Convolutions on FPGA-based Edge Devices](https://arxiv.org/abs/2507.07683)
*Jude Haris,José Cano*

Main category: cs.AR

TL;DR: 本文提出MM2IM硬件 - 软件协同设计加速器，在资源受限边缘设备上高效处理TCONV层，实验显示其性能优于基线和类似加速器。


<details>
  <summary>Details</summary>
Motivation: 现有TCONV的IOM方法存在输出映射复杂、重叠求和和计算低效等问题，加剧了资源受限边缘设备上TCONV和生成模型的性能瓶颈。

Method: 提出MM2IM加速器，结合矩阵乘法和col2IM，并使用SECDA - TFLite设计工具包实现。

Result: 在261个TCONV问题配置上平均加速1.9倍；在知名生成模型的TCONV层上最高加速4.2倍；GOPs/DSP比类似加速器至少高2倍；在DCGAN和pix2pix GAN模型上最高加速3倍、节能2.4倍。

Conclusion: MM2IM能在资源受限边缘设备上高效处理TCONV层，提升性能并降低能耗。

Abstract: Transposed Convolutions (TCONV) enable the up-scaling mechanism within
generative Artificial Intelligence (AI) models. However, the predominant
Input-Oriented Mapping (IOM) method for implementing TCONV has complex output
mapping, overlapping sums, and ineffectual computations. These inefficiencies
further exacerbate the performance bottleneck of TCONV and generative models on
resource-constrained edge devices. To address this problem, in this paper we
propose MM2IM, a hardware-software co-designed accelerator that combines Matrix
Multiplication (MatMul) with col2IM to process TCONV layers on
resource-constrained edge devices efficiently. Using the SECDA-TFLite design
toolkit, we implement MM2IM and evaluate its performance across 261 TCONV
problem configurations, achieving an average speedup of 1.9x against a
dual-thread ARM Neon optimized CPU baseline. We then evaluate the performance
of MM2IM on a range of TCONV layers from well-known generative models achieving
up to 4.2x speedup, and compare it against similar resource-constrained TCONV
accelerators, outperforming them by at least 2x GOPs/DSP. Finally, we evaluate
MM2IM on the DCGAN and pix2pix GAN models, achieving up to 3x speedup and 2.4x
energy reduction against the CPU baseline.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [259] [Scalable Signed Exponential Random Graph Models under Local Dependence](https://arxiv.org/abs/2507.07660)
*Marc Schalberger,Cornelius Fritz*

Main category: cs.SI

TL;DR: 传统网络分析聚焦二元边，本文提出结合SBM和ERGM优点的新方法，在合成网络和维基百科网络验证，发现符合结构平衡理论的模式。


<details>
  <summary>Details</summary>
Motivation: 传统网络分析聚焦二元边，无法处理真实世界复杂关系，传统方法如SBM和ERGM在处理大数据网络时因同质性假设和全局依赖存在局限。

Method: 提出结合SBM和ERGM优点的新方法，通过SBM近似将网络分解为子网络，再用ERGM方法估计参数。

Result: 在大型合成网络和有数千编辑的维基百科网络验证方法，发现符合结构平衡理论的模式。

Conclusion: 结合SBM和ERGM并引入局部依赖的方法能有效处理大数据网络中的有符号交互。

Abstract: Traditional network analysis focuses on binary edges, while real-world
relationships are more nuanced, encompassing cooperation, neutrality, and
conflict. The rise of negative edges in social media discussions spurred
interest in analyzing signed interactions, especially in polarized debates.
However, the vast data generated by digital networks presents challenges for
traditional methods like Stochastic Block Models (SBM) and Exponential Family
Random Graph Models (ERGM), particularly due to the homogeneity assumption and
global dependence, which become increasingly unrealistic as network size grows.
To address this, we propose a novel method that combines the strengths of SBM
and ERGM while mitigating their weaknesses by incorporating local dependence
based on non-overlapping blocks. Our approach involves a two-step process:
first, decomposing the network into sub-networks using SBM approximation, and
then estimating parameters using ERGM methods. We validate our method on large
synthetic networks and apply it to a signed Wikipedia network of thousands of
editors. Through the use of local dependence, we find patterns consistent with
structural balance theory.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [260] [A Graph Isomorphism-based Decentralized Algorithm for Modular Robot Configuration Formation](https://arxiv.org/abs/1602.03104)
*Ayan Dutta,Prithviraj Dasgupta,Carl Nelson*

Main category: cs.RO

TL;DR: 提出基于图同构的算法解决模块化机器人系统配置形成问题，分析证明算法特性，实验显示性能优。


<details>
  <summary>Details</summary>
Motivation: 解决模块化机器人系统中从不同初始配置和位置形成新目标配置的问题，减少模块到达目标配置所需时间和能量。

Method: 提出基于图同构的算法，模块用基于效用的框架选择目标配置中的位置，尽量保留原始配置。

Result: 分析证明算法完备且保证帕累托最优分配，实验模拟显示规划时间短，与市场分配算法对比，时间和消息交换数量表现更好。

Conclusion: 所提算法在模块化机器人系统配置形成问题上有良好表现，能有效减少时间和能量消耗。

Abstract: We consider the problem of configuration formation in modular robot systems
where a set of modules that are initially in different configurations and
located at different locations are required to assume appropriate positions so
that they can get into a new, user-specified, target configuration. We propose
a novel algorithm based on graph isomorphism, where the modules select
locations or spots in the target configuration using a utility-based framework,
while retaining their original configuration to the greatest extent possible,
to reduce the time and energy required by the modules to assume the target
configuration. We have shown analytically that our proposed algorithm is
complete and guarantees a Pareto-optimal allocation. Experimental simulations
of our algorithm with different number of modules in different initial
configurations and located initially at different locations, show that the
planning time of our algorithm is nominal (order of msec. for 100 modules). We
have also compared our algorithm against a market-based allocation algorithm
and shown that our proposed algorithm performs better in terms of time and
number of messages exchanged.

</details>


### [261] [PILOC: A Pheromone Inverse Guidance Mechanism and Local-Communication Framework for Dynamic Target Search of Multi-Agent in Unknown Environments](https://arxiv.org/abs/2507.07376)
*Hengrui Liu,Yi Feng,Qilong Zhang*

Main category: cs.RO

TL;DR: 提出PILOC框架用于多智能体搜索救援，结合局部通信和信息素引导提升性能。


<details>
  <summary>Details</summary>
Motivation: 动态未知环境下多智能体搜索救援面临目标不可预测和环境不确定的挑战。

Method: 提出PILOC框架，采用信息素反向引导机制，将其嵌入深度强化学习的观测空间，结合局部通信，集成到基于DRL的多智能体架构。

Result: 结合局部通信与信息素引导显著提高搜索效率、适应性和系统鲁棒性，在动态和通信受限场景表现优于现有方法。

Conclusion: PILOC为未来多智能体搜索救援应用提供了有前景的方向。

Abstract: Multi-Agent Search and Rescue (MASAR) plays a vital role in disaster
response, exploration, and reconnaissance. However, dynamic and unknown
environments pose significant challenges due to target unpredictability and
environmental uncertainty. To tackle these issues, we propose PILOC, a
framework that operates without global prior knowledge, leveraging local
perception and communication. It introduces a pheromone inverse guidance
mechanism to enable efficient coordination and dynamic target localization.
PILOC promotes decentralized cooperation through local communication,
significantly reducing reliance on global channels. Unlike conventional
heuristics, the pheromone mechanism is embedded into the observation space of
Deep Reinforcement Learning (DRL), supporting indirect agent coordination based
on environmental cues. We further integrate this strategy into a DRL-based
multi-agent architecture and conduct extensive experiments. Results show that
combining local communication with pheromone-based guidance significantly
boosts search efficiency, adaptability, and system robustness. Compared to
existing methods, PILOC performs better under dynamic and
communication-constrained scenarios, offering promising directions for future
MASAR applications.

</details>


### [262] [Adaptive Gaussian Mixture Models-based Anomaly Detection for under-constrained Cable-Driven Parallel Robots](https://arxiv.org/abs/2507.07714)
*Julio Garrido,Javier Vales,Diego Silva-Muñiz,Enrique Riveiro,Pablo López-Matencio,Josué Rivera-Andrade*

Main category: cs.RO

TL;DR: 本文研究仅用电机扭矩数据检测电缆驱动并联机器人（CDPRs）异常，提出基于高斯混合模型（GMM）的自适应无监督异常检测算法，经测试性能良好且更具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在CDPRs每个中间停止点，系统需检测可能影响性能的异常以判断是否安全推进，研究能否仅用电机扭矩数据检测异常。

Method: 引入基于GMM的自适应无监督异常检测算法，先在短校准期对无异常数据拟合GMM，用马氏距离评估实时扭矩测量值，根据统计阈值触发异常标记，定期更新模型参数。

Result: 该方法在14次长时测试中实现100%真阳性率和95.4%平均真阴性率，检测延迟1秒，比功率阈值和非自适应GMM方法更能适应漂移和环境变化。

Conclusion: 仅用电机扭矩数据结合所提算法能有效检测CDPRs异常，且算法具有较高鲁棒性。

Abstract: Cable-Driven Parallel Robots (CDPRs) are increasingly used for load
manipulation tasks involving predefined toolpaths with intermediate stops. At
each stop, where the platform maintains a fixed pose and the motors keep the
cables under tension, the system must evaluate whether it is safe to proceed by
detecting anomalies that could compromise performance (e.g., wind gusts or
cable impacts). This paper investigates whether anomalies can be detected using
only motor torque data, without additional sensors. It introduces an adaptive,
unsupervised outlier detection algorithm based on Gaussian Mixture Models
(GMMs) to identify anomalies from torque signals. The method starts with a
brief calibration period, just a few seconds, during which a GMM is fit on
known anomaly-free data. Real-time torque measurements are then evaluated using
Mahalanobis distance from the GMM, with statistically derived thresholds
triggering anomaly flags. Model parameters are periodically updated using the
latest segments identified as anomaly-free to adapt to changing conditions.
Validation includes 14 long-duration test sessions simulating varied wind
intensities. The proposed method achieves a 100% true positive rate and 95.4%
average true negative rate, with 1-second detection latency. Comparative
evaluation against power threshold and non-adaptive GMM methods indicates
higher robustness to drift and environmental variation.

</details>


### [263] [Data-driven Kinematic Modeling in Soft Robots: System Identification and Uncertainty Quantification](https://arxiv.org/abs/2507.07370)
*Zhanhong Jiang,Dylan Shah,Hsin-Jung Yang,Soumik Sarkar*

Main category: cs.RO

TL;DR: 针对软机器人运动学建模问题，研究多种机器学习模型，发现非线性集成方法泛化性能佳，开发共形运动学建模框架量化预测位置不确定性。


<details>
  <summary>Details</summary>
Motivation: 精确运动学建模对软机器人校准和控制器设计至关重要，但因软机器人高度非线性和复杂行为，数据驱动机器学习方法有预测不确定性问题且不确定性量化研究不足。

Method: 研究常用线性和非线性机器学习模型，利用分裂共形预测开发共形运动学建模框架。

Result: 非线性集成方法表现出最稳健的泛化性能。

Conclusion: 所开发的共形运动学建模框架可量化预测位置不确定性，保证分布无关的预测区间。

Abstract: Precise kinematic modeling is critical in calibration and controller design
for soft robots, yet remains a challenging issue due to their highly nonlinear
and complex behaviors. To tackle the issue, numerous data-driven machine
learning approaches have been proposed for modeling nonlinear dynamics.
However, these models suffer from prediction uncertainty that can negatively
affect modeling accuracy, and uncertainty quantification for kinematic modeling
in soft robots is underexplored. In this work, using limited simulation and
real-world data, we first investigate multiple linear and nonlinear machine
learning models commonly used for kinematic modeling of soft robots. The
results reveal that nonlinear ensemble methods exhibit the most robust
generalization performance. We then develop a conformal kinematic modeling
framework for soft robots by utilizing split conformal prediction to quantify
predictive position uncertainty, ensuring distribution-free prediction
intervals with a theoretical guarantee.

</details>


### [264] [Improving AEBS Validation Through Objective Intervention Classification Leveraging the Prediction Divergence Principle](https://arxiv.org/abs/2507.07872)
*Daniel Betschinske,Steven Peters*

Main category: cs.RO

TL;DR: 本文提出基于预测差异原则（PDP）的规则分类方法解决自动紧急制动系统（AEBS）安全验证中区分误报和真阳性激活的问题，结合人工标注可提升分类效果。


<details>
  <summary>Details</summary>
Motivation: 自动紧急制动系统安全验证需准确区分误报和真阳性激活，开环重仿真分析激活情况复杂，人工标注有局限性。

Method: 提出基于预测差异原则（PDP）的规则分类方法。

Result: 应用于简化AEBS，揭示方法优缺点和系统要求，结合人工标注可提升分类透明度和一致性。

Conclusion: 该方法可补充现有实践，为更可靠、可重复的AEBS验证框架奠定基础，后续可进一步完善规则集。

Abstract: The safety validation of automatic emergency braking system (AEBS) requires
accurately distinguishing between false positive (FP) and true positive (TP)
system activations. While simulations allow straightforward differentiation by
comparing scenarios with and without interventions, analyzing activations from
open-loop resimulations - such as those from field operational testing (FOT) -
is more complex. This complexity arises from scenario parameter uncertainty and
the influence of driver interventions in the recorded data. Human labeling is
frequently used to address these challenges, relying on subjective assessments
of intervention necessity or situational criticality, potentially introducing
biases and limitations. This work proposes a rule-based classification approach
leveraging the Prediction Divergence Principle (PDP) to address those issues.
Applied to a simplified AEBS, the proposed method reveals key strengths,
limitations, and system requirements for effective implementation. The findings
suggest that combining this approach with human labeling may enhance the
transparency and consistency of classification, thereby improving the overall
validation process. While the rule set for classification derived in this work
adopts a conservative approach, the paper outlines future directions for
refinement and broader applicability. Finally, this work highlights the
potential of such methods to complement existing practices, paving the way for
more reliable and reproducible AEBS validation frameworks.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [265] [Benchmarking Waitlist Mortality Prediction in Heart Transplantation Through Time-to-Event Modeling using New Longitudinal UNOS Dataset](https://arxiv.org/abs/2507.07339)
*Yingtao Luo,Reza Skandari,Carlos Martinez,Arman Kilic,Rema Padman*

Main category: stat.AP

TL;DR: 本文对利用纵向等待列表历史数据的机器学习模型进行基准测试，用于预测心脏移植等待名单患者的死亡率，最佳模型表现优于以往，结果可支持决策。


<details>
  <summary>Details</summary>
Motivation: 当前心脏移植等待名单患者管理决策多为临时决定，随着相关数据增多，对支持临床决策的分析方法需求增加。

Method: 对利用纵向等待列表历史数据的机器学习模型进行基准测试，在23,807条患者记录和77个变量上训练，评估1年的生存预测和区分度。

Result: 最佳模型C - Index为0.94，AUROC为0.89，显著优于以往模型，关键预测因子与已知风险因素相符并揭示新关联。

Conclusion: 研究结果可支持心脏移植决策中的紧迫性评估和政策完善。

Abstract: Decisions about managing patients on the heart transplant waitlist are
currently made by committees of doctors who consider multiple factors, but the
process remains largely ad-hoc. With the growing volume of longitudinal
patient, donor, and organ data collected by the United Network for Organ
Sharing (UNOS) since 2018, there is increasing interest in analytical
approaches to support clinical decision-making at the time of organ
availability. In this study, we benchmark machine learning models that leverage
longitudinal waitlist history data for time-dependent, time-to-event modeling
of waitlist mortality. We train on 23,807 patient records with 77 variables and
evaluate both survival prediction and discrimination at a 1-year horizon. Our
best model achieves a C-Index of 0.94 and AUROC of 0.89, significantly
outperforming previous models. Key predictors align with known risk factors
while also revealing novel associations. Our findings can support urgency
assessment and policy refinement in heart transplant decision making.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [266] [Computationally Efficient Information-Driven Optical Design with Interchanging Optimization](https://arxiv.org/abs/2507.07789)
*Eric Markley,Henry Pinkard,Leyla Kabuli,Nalini Singh,Laura Waller*

Main category: eess.IV

TL;DR: 研究发现IDEAL存在高内存使用、长运行时间等问题，提出IDEAL - IO方法，减少了运行时间和内存使用，在多种成像应用中验证了其实用性。


<details>
  <summary>Details</summary>
Motivation: 解决IDEAL在不同成像系统中存在的高内存使用、长运行时间和目标函数可能不匹配的问题。

Method: 提出IDEAL - IO方法，通过交替拟合模型到当前测量值和使用固定模型进行信息估计来更新光学参数，将密度估计与光学参数优化解耦。

Result: 该方法将运行时间和内存使用减少达6倍，能使用更具表现力的密度模型引导优化至更好设计。

Conclusion: 信息论优化可作为实际、可扩展的策略用于现实世界成像系统设计。

Abstract: Recent work has demonstrated that imaging systems can be evaluated through
the information content of their measurements alone, enabling
application-agnostic optical design that avoids computational decoding
challenges. Information-Driven Encoder Analysis Learning (IDEAL) was proposed
to automate this process through gradient-based. In this work, we study IDEAL
across diverse imaging systems and find that it suffers from high memory usage,
long runtimes, and a potentially mismatched objective function due to
end-to-end differentiability requirements. We introduce IDEAL with
Interchanging Optimization (IDEAL-IO), a method that decouples density
estimation from optical parameter optimization by alternating between fitting
models to current measurements and updating optical parameters using fixed
models for information estimation. This approach reduces runtime and memory
usage by up to 6x while enabling more expressive density models that guide
optimization toward superior designs. We validate our method on diffractive
optics, lensless imaging, and snapshot 3D microscopy applications, establishing
information-theoretic optimization as a practical, scalable strategy for
real-world imaging system design.

</details>


### [267] [DpDNet: An Dual-Prompt-Driven Network for Universal PET-CT Segmentation](https://arxiv.org/abs/2507.07126)
*Xinglong Liang,Jiaju Huang,Luyi Han,Tianyu Zhang,Xin Wang,Yuan Gao,Chunyao Lu,Lishan Cai,Tao Tan,Ritse Mann*

Main category: eess.IV

TL;DR: 提出DpDNet用于PET - CT病灶分割，在四种癌症数据集上表现优于现有模型，可用于乳腺癌生存分析。


<details>
  <summary>Details</summary>
Motivation: 当前主流方法将所有癌症视为单一任务进行分割，忽略了不同癌症类型的独特特征，PET - CT病灶分割存在挑战。

Method: 提出DpDNet，结合特定提示捕获癌症特定特征，通用提示保留共享知识，在解码器后使用提示感知头处理多分割任务。

Result: 在四种癌症的PET - CT数据集上，DpDNet优于现有模型，基于分割结果计算指标用于乳腺癌生存分析。

Conclusion: DpDNet有潜力作为个性化风险分层的有价值工具，支持临床医生优化治疗策略和改善治疗结果。

Abstract: PET-CT lesion segmentation is challenging due to noise sensitivity, small and
variable lesion morphology, and interference from physiological high-metabolic
signals. Current mainstream approaches follow the practice of one network
solving the segmentation of multiple cancer lesions by treating all cancers as
a single task. However, this overlooks the unique characteristics of different
cancer types. Considering the specificity and similarity of different cancers
in terms of metastatic patterns, organ preferences, and FDG uptake intensity,
we propose DpDNet, a Dual-Prompt-Driven network that incorporates specific
prompts to capture cancer-specific features and common prompts to retain shared
knowledge. Additionally, to mitigate information forgetting caused by the early
introduction of prompts, prompt-aware heads are employed after the decoder to
adaptively handle multiple segmentation tasks. Experiments on a PET-CT dataset
with four cancer types show that DpDNet outperforms state-of-the-art models.
Finally, based on the segmentation results, we calculated MTV, TLG, and SUVmax
for breast cancer survival analysis. The results suggest that DpDNet has the
potential to serve as a valuable tool for personalized risk stratification,
supporting clinicians in optimizing treatment strategies and improving
outcomes. Code is available at https://github.com/XinglongLiang08/DpDNet.

</details>


### [268] [Semi-supervised learning and integration of multi-sequence MR-images for carotid vessel wall and plaque segmentation](https://arxiv.org/abs/2507.07496)
*Marie-Christine Pali,Christina Schwaiger,Malik Galijasevic,Valentin K. Ladenhauf,Stephanie Mangesius,Elke R. Gizewski*

Main category: eess.IV

TL;DR: 本文提出半监督深度学习方法用于颈动脉血管壁和斑块分割，在动脉硬化患者MRI数据上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 颈动脉斑块分析对评估动脉粥样硬化和缺血性中风风险至关重要，准确分割是量化疾病状态的关键，但斑块形态复杂和标注数据稀缺带来挑战。

Method: 提出由粗定位模型和精细分割模型组成的算法，研究不同融合策略并引入多级多序列U - Net架构，采用半监督方法增强输入变换下的一致性。

Result: 在52名患者的多序列MRI数据上进行综合实验，证明了方法的有效性，强调了U - Net架构中融合点选择的作用，还进行了专家评估。

Conclusion: 融合策略和半监督学习在数据有限的MRI应用中，对改善颈动脉分割有潜力。

Abstract: The analysis of carotid arteries, particularly plaques, in multi-sequence
Magnetic Resonance Imaging (MRI) data is crucial for assessing the risk of
atherosclerosis and ischemic stroke. In order to evaluate metrics and radiomic
features, quantifying the state of atherosclerosis, accurate segmentation is
important. However, the complex morphology of plaques and the scarcity of
labeled data poses significant challenges. In this work, we address these
problems and propose a semi-supervised deep learning-based approach designed to
effectively integrate multi-sequence MRI data for the segmentation of carotid
artery vessel wall and plaque. The proposed algorithm consists of two networks:
a coarse localization model identifies the region of interest guided by some
prior knowledge on the position and number of carotid arteries, followed by a
fine segmentation model for precise delineation of vessel walls and plaques. To
effectively integrate complementary information across different MRI sequences,
we investigate different fusion strategies and introduce a multi-level
multi-sequence version of U-Net architecture. To address the challenges of
limited labeled data and the complexity of carotid artery MRI, we propose a
semi-supervised approach that enforces consistency under various input
transformations. Our approach is evaluated on 52 patients with
arteriosclerosis, each with five MRI sequences. Comprehensive experiments
demonstrate the effectiveness of our approach and emphasize the role of fusion
point selection in U-Net-based architectures. To validate the accuracy of our
results, we also include an expert-based assessment of model performance. Our
findings highlight the potential of fusion strategies and semi-supervised
learning for improving carotid artery segmentation in data-limited MRI
applications.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [269] [Distributed and Decentralised Training: Technical Governance Challenges in a Shifting AI Landscape](https://arxiv.org/abs/2507.07765)
*Jakub Kryś,Yashvardhan Sharma,Janet Egan*

Main category: cs.CY

TL;DR: 本文区分分布式和去中心化训练场景，探讨其对AI治理的影响，指出政策杠杆仍有相关性，承认去中心化AI的好处，旨在支持精准政策制定。


<details>
  <summary>Details</summary>
Motivation: 当前低通信训练算法发展使训练模式转变，分布式和去中心化训练在政策讨论中常被混淆且理解不足，需深入分析其对AI治理的影响。

Method: 对分布式和去中心化训练场景进行区分，并分析其对技术AI治理的影响。

Result: 发现这些趋势可能带来计算结构、能力扩散等风险，挑战计算治理的关键假设，但如出口管制等政策杠杆仍有相关性，且去中心化AI有潜在好处。

Conclusion: 应围绕计算、能力扩散和去中心化AI发展制定更精准的政策。

Abstract: Advances in low-communication training algorithms are enabling a shift from
centralised model training to compute setups that are either distributed across
multiple clusters or decentralised via community-driven contributions. This
paper distinguishes these two scenarios - distributed and decentralised
training - which are little understood and often conflated in policy discourse.
We discuss how they could impact technical AI governance through an increased
risk of compute structuring, capability proliferation, and the erosion of
detectability and shutdownability. While these trends foreshadow a possible new
paradigm that could challenge key assumptions of compute governance, we
emphasise that certain policy levers, like export controls, remain relevant. We
also acknowledge potential benefits of decentralised AI, including
privacy-preserving training runs that could unlock access to more data, and
mitigating harmful power concentration. Our goal is to support more precise
policymaking around compute, capability proliferation, and decentralised AI
development.

</details>
