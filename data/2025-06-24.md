<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 50]
- [cs.CE](#cs.CE) [Total: 15]
- [cs.DB](#cs.DB) [Total: 10]
- [cs.DC](#cs.DC) [Total: 13]
- [cs.DS](#cs.DS) [Total: 11]
- [cs.GT](#cs.GT) [Total: 5]
- [cs.IR](#cs.IR) [Total: 24]
- [cs.LG](#cs.LG) [Total: 147]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.SE](#cs.SE) [Total: 29]
- [q-fin.ST](#q-fin.ST) [Total: 2]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [stat.ML](#stat.ML) [Total: 16]
- [stat.CO](#stat.CO) [Total: 2]
- [econ.GN](#econ.GN) [Total: 3]
- [physics.med-ph](#physics.med-ph) [Total: 2]
- [math.NA](#math.NA) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 4]
- [cs.CC](#cs.CC) [Total: 2]
- [quant-ph](#quant-ph) [Total: 3]
- [cs.CR](#cs.CR) [Total: 19]
- [physics.geo-ph](#physics.geo-ph) [Total: 2]
- [cs.CG](#cs.CG) [Total: 2]
- [stat.ME](#stat.ME) [Total: 3]
- [math.OC](#math.OC) [Total: 3]
- [q-bio.OT](#q-bio.OT) [Total: 1]
- [cs.GR](#cs.GR) [Total: 3]
- [eess.SY](#eess.SY) [Total: 5]
- [cs.DL](#cs.DL) [Total: 1]
- [physics.app-ph](#physics.app-ph) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [eess.SP](#eess.SP) [Total: 2]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.RO](#cs.RO) [Total: 21]
- [cs.HC](#cs.HC) [Total: 6]
- [econ.TH](#econ.TH) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.CV](#cs.CV) [Total: 58]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [eess.IV](#eess.IV) [Total: 5]
- [cs.LO](#cs.LO) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 3]
- [q-fin.PR](#q-fin.PR) [Total: 2]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.CY](#cs.CY) [Total: 12]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [cs.DM](#cs.DM) [Total: 1]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.CL](#cs.CL) [Total: 43]
- [cs.SI](#cs.SI) [Total: 3]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [math.PR](#math.PR) [Total: 1]
- [cs.SD](#cs.SD) [Total: 8]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Evaluating Generalization and Representation Stability in Small LMs via Prompting](https://arxiv.org/abs/2506.17289)
*Rahul Raja,Arpita Vats*

Main category: cs.AI

TL;DR: 研究小语言模型在少样本提示和监督微调两种范式下的泛化能力，对比不同设置下表现并分析内部表征，为低数据场景模型选择提供指导。


<details>
  <summary>Details</summary>
Motivation: 探究少样本提示在低资源和分布偏移场景下的鲁棒性，以及不同适应范式下小语言模型内化和泛化知识的差异。

Method: 对提示和微调在不同任务格式、提示风格和模型规模下进行对比研究，分析内部表征。

Result: 发现小模型在不同适应策略下内化和泛化知识存在关键差异。

Conclusion: 为低数据场景下的模型选择提供实用指导，为提示与微调的争论提供实证见解。

Abstract: We investigate the generalization capabilities of small language models under
two popular adaptation paradigms: few-shot prompting and supervised
fine-tuning. While prompting is often favored for its parameter efficiency and
flexibility, it remains unclear how robust this approach is in low-resource
settings and under distributional shifts. This paper presents a comparative
study of prompting and fine-tuning across task formats, prompt styles, and
model scales, with a focus on their behavior in both in-distribution and
out-of-distribution (OOD) settings.
  Beyond accuracy, we analyze the internal representations learned by each
approach to assess the stability and abstraction of task-specific features. Our
findings highlight critical differences in how small models internalize and
generalize knowledge under different adaptation strategies. This work offers
practical guidance for model selection in low-data regimes and contributes
empirical insight into the ongoing debate over prompting versus fine-tuning.
Code for the experiments is available at the following

</details>


### [2] [Individual Causal Inference with Structural Causal Model](https://arxiv.org/abs/2506.17300)
*Daniel T. Chang*

Main category: cs.AI

TL;DR: 提出基于结构因果模型（SCM）的个体因果推断（ICI）方法，用indiv - operator和个体因果查询来形式化表示，指出这是对个体替代方案的推断。


<details>
  <summary>Details</summary>
Motivation: 个体因果效应（ICE）估计因个体数据有限和多数因果推断方法基于总体而具有挑战性，需要新方法进行个体因果推断。

Method: 利用SCM中外生变量编码个体差异，提出indiv - operator和个体因果查询P(Y | indiv(W), do(X), Z)。

Result: 提出基于SCM的ICI作为“rung 3”因果推断，展示了其是对个体替代方案而非个体反事实的推断。

Conclusion: 基于SCM的ICI为个体因果推断提供了新的思路和方法。

Abstract: Individual causal inference (ICI) uses causal inference methods to understand
and predict the effects of interventions on individuals, considering their
specific characteristics / facts. It aims to estimate individual causal effect
(ICE), which varies across individuals. Estimating ICE can be challenging due
to the limited data available for individuals, and the fact that most causal
inference methods are population-based. Structural Causal Model (SCM) is
fundamentally population-based. Therefore, causal discovery (structural
learning and parameter learning), association queries and intervention queries
are all naturally population-based. However, exogenous variables (U) in SCM can
encode individual variations and thus provide the mechanism for individualized
population per specific individual characteristics / facts. Based on this, we
propose ICI with SCM as a "rung 3" causal inference, because it involves
"imagining" what would be the causal effect of a hypothetical intervention on
an individual, given the individual's observed characteristics / facts.
Specifically, we propose the indiv-operator, indiv(W), to formalize/represent
the population individualization process, and the individual causal query, P(Y
| indiv(W), do(X), Z), to formalize/represent ICI. We show and argue that ICI
with SCM is inference on individual alternatives (possible), not individual
counterfactuals (non-actual).

</details>


### [3] [Resource Rational Contractualism Should Guide AI Alignment](https://arxiv.org/abs/2506.17434)
*Sydney Levine,Matija Franklin,Tan Zhi-Xuan,Secil Yanik Guyot,Lionel Wong,Daniel Kilov,Yejin Choi,Joshua B. Tenenbaum,Noah Goodman,Seth Lazar,Iason Gabriel*

Main category: cs.AI

TL;DR: 提出资源理性契约主义（RRC）框架，使AI系统高效且能适应人类社会。


<details>
  <summary>Details</summary>
Motivation: 现有契约主义对齐方法在大规模达成协议时成本高且速度慢。

Method: 提出RRC框架，让AI系统借助规范启发、认知启发式方法权衡准确性与努力程度来近似达成理性各方的协议。

Result: RRC对齐的代理不仅能高效运行，还能动态适应和解读不断变化的人类社会。

Conclusion: RRC框架能解决现有问题，使AI系统更好适应人类环境。

Abstract: AI systems will soon have to navigate human environments and make decisions
that affect people and other AI agents whose goals and values diverge.
Contractualist alignment proposes grounding those decisions in agreements that
diverse stakeholders would endorse under the right conditions, yet securing
such agreement at scale remains costly and slow -- even for advanced AI. We
therefore propose Resource-Rational Contractualism (RRC): a framework where AI
systems approximate the agreements rational parties would form by drawing on a
toolbox of normatively-grounded, cognitively-inspired heuristics that trade
effort for accuracy. An RRC-aligned agent would not only operate efficiently,
but also be equipped to dynamically adapt to and interpret the ever-changing
human social world.

</details>


### [4] [Keeping Medical AI Healthy: A Review of Detection and Correction Methods for System Degradation](https://arxiv.org/abs/2506.17442)
*Hao Guan,David Bates,Li Zhou*

Main category: cs.AI

TL;DR: 文章探讨医疗AI系统性能下降问题，回顾常见原因、检测技术、纠正策略，涵盖不同模型，讨论挑战并提出研究方向。


<details>
  <summary>Details</summary>
Motivation: 现实中医疗AI系统性能会随时间下降，影响可靠性，需监测和维护其“健康”。

Method: 回顾数据和模型层面性能下降原因，总结检测数据和模型漂移的技术，深入分析根本原因，回顾从模型再训练到测试时自适应的纠正策略。

Result: 对传统机器学习模型和大语言模型进行研究，了解其优缺点。

Conclusion: 为开发能在动态临床环境安全长期部署的可靠、稳健医疗AI系统提供指导。

Abstract: Artificial intelligence (AI) is increasingly integrated into modern
healthcare, offering powerful support for clinical decision-making. However, in
real-world settings, AI systems may experience performance degradation over
time, due to factors such as shifting data distributions, changes in patient
characteristics, evolving clinical protocols, and variations in data quality.
These factors can compromise model reliability, posing safety concerns and
increasing the likelihood of inaccurate predictions or adverse outcomes. This
review presents a forward-looking perspective on monitoring and maintaining the
"health" of AI systems in healthcare. We highlight the urgent need for
continuous performance monitoring, early degradation detection, and effective
self-correction mechanisms. The paper begins by reviewing common causes of
performance degradation at both data and model levels. We then summarize key
techniques for detecting data and model drift, followed by an in-depth look at
root cause analysis. Correction strategies are further reviewed, ranging from
model retraining to test-time adaptation. Our survey spans both traditional
machine learning models and state-of-the-art large language models (LLMs),
offering insights into their strengths and limitations. Finally, we discuss
ongoing technical challenges and propose future research directions. This work
aims to guide the development of reliable, robust medical AI systems capable of
sustaining safe, long-term deployment in dynamic clinical settings.

</details>


### [5] [Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective](https://arxiv.org/abs/2506.17930)
*Jianyu Wang,Zhiqiang Hu,Lidong Bing*

Main category: cs.AI

TL;DR: 提出新颖提示设计范式，将随机演示剪枝成‘乱码’可提升性能，提出自发现提示优化框架PromptQuine并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 挑战大语言模型提示的传统智慧，解决现有归因方法和提示压缩算法无法有效找到剪枝策略的问题。

Method: 提出自发现提示优化框架PromptQuine，这是一个在低数据机制下自动搜索剪枝策略的进化搜索框架。

Result: 在多种大语言模型的分类、多选问答、生成和数学推理任务中证明了有效性，且有不错的运行时效率。

Conclusion: 研究结果可指导上下文学习的机制研究，呼吁开发更多开放式搜索算法以实现更有效的大语言模型提示。

Abstract: We propose a novel prompt design paradigm that challenges conventional wisdom
in large language model (LLM) prompting. While conventional wisdom prioritizes
well-crafted instructions and demonstrations for in-context learning (ICL), we
show that pruning random demonstrations into seemingly incoherent "gibberish"
can remarkably improve performance across diverse tasks. Notably, the
"gibberish" always matches or surpasses state-of-the-art automatic prompt
optimization techniques, achieving substantial gains regardless of LLM
alignment. Nevertheless, discovering an effective pruning strategy is
non-trivial, as existing attribution methods and prompt compression algorithms
fail to deliver robust results, let alone human intuition. In terms of this, we
propose a self-discover prompt optimization framework, PromptQuine, an
evolutionary search framework that automatically searches for the pruning
strategy by itself using only low-data regimes. Much like the emergent
complexity in nature--such as symbiosis and self-organization--arising in
response to resource constraints, our framework evolves and refines
unconventional yet highly effective prompts by leveraging only the tokens
present within the context. We demonstrate its effectiveness across
classification, multi-choice question answering, generation and math reasoning
tasks across LLMs, while achieving decent runtime efficiency. We hope our
findings can guide mechanistic studies on in-context learning, and provide a
call to action, to pave the way for more open-ended search algorithms for more
effective LLM prompting.

</details>


### [6] [OmniReflect: Discovering Transferable Constitutions for LLM agents via Neuro-Symbolic Reflections](https://arxiv.org/abs/2506.17449)
*Manasa Bharadwaj,Nikhil Verma,Kevin Ferreira*

Main category: cs.AI

TL;DR: 介绍OmniReflect框架提升大语言模型（LLM）智能体在复杂任务中的表现，该框架有两种模式，采用多种技术构建原则，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 现有提升LLM智能体性能的方法缺乏长期学习通用机制且在动态环境中效率低，需新方法。

Method: 引入OmniReflect框架，有自我维持和合作两种模式，采用神经、符号和神经符号技术构建宪法原则。

Result: 在自我维持模式下各任务成功率显著提升，合作模式中轻量级智能体在BabyAI上超越基线。

Conclusion: OmniReflect在不同环境和模型骨干上都具有鲁棒性和有效性。

Abstract: Efforts to improve Large Language Model (LLM) agent performance on complex
tasks have largely focused on fine-tuning and iterative self-correction.
However, these approaches often lack generalizable mechanisms for longterm
learning and remain inefficient in dynamic environments. We introduce
OmniReflect, a hierarchical, reflection-driven framework that constructs a
constitution, a compact set of guiding principles distilled from task
experiences, to enhance the effectiveness and efficiency of an LLM agent.
OmniReflect operates in two modes: Self-sustaining, where a single agent
periodically curates its own reflections during task execution, and
Co-operative, where a Meta-advisor derives a constitution from a small
calibration set to guide another agent. To construct these constitutional
principles, we employ Neural, Symbolic, and NeuroSymbolic techniques, offering
a balance between contextual adaptability and computational efficiency.
Empirical results averaged across models show major improvements in task
success, with absolute gains of +10.3% on ALFWorld, +23.8% on BabyAI, and +8.3%
on PDDL in the Self-sustaining mode. Similar gains are seen in the Co-operative
mode, where a lightweight Qwen3-4B ReAct agent outperforms all Reflexion
baselines on BabyAI. These findings highlight the robustness and effectiveness
of OmniReflect across environments and backbones.

</details>


### [7] [From Unstructured Communication to Intelligent RAG: Multi-Agent Automation for Supply Chain Knowledge Bases](https://arxiv.org/abs/2506.17484)
*Yao Zhang,Zaixi Shang,Silpan Patel,Mikel Zuniga*

Main category: cs.AI

TL;DR: 提出离线优先方法将供应链非结构化通信转为结构化知识库，实验表明优于传统RAG，提升运营效率。


<details>
  <summary>Details</summary>
Motivation: 供应链运营中关键知识埋于非结构化通信，现有RAG系统因数据问题效果受限。

Method: 引入基于大语言模型的多智能体系统，含类别发现、分类和知识合成三个专业智能体。

Result: 创建紧凑知识库，数据量降至原3.4%，RAG系统有益答案率48.74%高于传统38.60%，无用回复减少77.4%。

Conclusion: 该方法通过智能离线处理解决知识管理缺口，提升运营效率，自动解决约50%未来供应链工单。

Abstract: Supply chain operations generate vast amounts of operational data; however,
critical knowledge such as system usage practices, troubleshooting workflows,
and resolution techniques often remains buried within unstructured
communications like support tickets, emails, and chat logs. While RAG systems
aim to leverage such communications as a knowledge base, their effectiveness is
limited by raw data challenges: support tickets are typically noisy,
inconsistent, and incomplete, making direct retrieval suboptimal. Unlike
existing RAG approaches that focus on runtime optimization, we introduce a
novel offline-first methodology that transforms these communications into a
structured knowledge base. Our key innovation is a LLMs-based multi-agent
system orchestrating three specialized agents: Category Discovery for taxonomy
creation, Categorization for ticket grouping, and Knowledge Synthesis for
article generation. Applying our methodology to real-world support tickets with
resolution notes and comments, our system creates a compact knowledge base -
reducing total volume to just 3.4% of original ticket data while improving
quality. Experiments demonstrate that our prebuilt knowledge base in RAG
systems significantly outperforms traditional RAG implementations (48.74% vs.
38.60% helpful answers) and achieves a 77.4% reduction in unhelpful responses.
By automating institutional knowledge capture that typically remains siloed in
experts' heads, our solution translates to substantial operational efficiency:
reducing support workload, accelerating resolution times, and creating
self-improving systems that automatically resolve approximately 50% of future
supply chain tickets. Our approach addresses a key gap in knowledge management
by transforming transient communications into structured, reusable knowledge
through intelligent offline processing rather than latency-inducing runtime
architectures.

</details>


### [8] [Kaleidoscopic Teaming in Multi Agent Simulations](https://arxiv.org/abs/2506.17514)
*Ninareh Mehrabi,Tharindu Kumarage,Kai-Wei Chang,Aram Galstyan,Rahul Gupta*

Main category: cs.AI

TL;DR: 文章指出现有红队或安全评估框架在评估AI智能体安全风险方面存在不足，提出了万花筒式团队（kaleidoscopic teaming）概念、框架、优化技术和评估指标，并利用框架识别模型安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有红队或安全评估框架无法有效评估复杂行为、思维过程和多智能体场景中的安全风险。

Method: 引入万花筒式团队概念，构建新框架模拟现实社会场景，在单智能体和多智能体场景下评估安全，采用新的上下文优化技术生成更好场景，并给出评估指标。

Result: 利用万花筒式团队框架识别出不同模型在智能体用例中的安全漏洞。

Conclusion: 万花筒式团队框架可有效评估AI智能体在单智能体和多智能体场景下的安全风险。

Abstract: Warning: This paper contains content that may be inappropriate or offensive.
  AI agents have gained significant recent attention due to their autonomous
tool usage capabilities and their integration in various real-world
applications. This autonomy poses novel challenges for the safety of such
systems, both in single- and multi-agent scenarios. We argue that existing red
teaming or safety evaluation frameworks fall short in evaluating safety risks
in complex behaviors, thought processes and actions taken by agents. Moreover,
they fail to consider risks in multi-agent setups where various vulnerabilities
can be exposed when agents engage in complex behaviors and interactions with
each other. To address this shortcoming, we introduce the term kaleidoscopic
teaming which seeks to capture complex and wide range of vulnerabilities that
can happen in agents both in single-agent and multi-agent scenarios. We also
present a new kaleidoscopic teaming framework that generates a diverse array of
scenarios modeling real-world human societies. Our framework evaluates safety
of agents in both single-agent and multi-agent setups. In single-agent setup,
an agent is given a scenario that it needs to complete using the tools it has
access to. In multi-agent setup, multiple agents either compete against or
cooperate together to complete a task in the scenario through which we capture
existing safety vulnerabilities in agents. We introduce new in-context
optimization techniques that can be used in our kaleidoscopic teaming framework
to generate better scenarios for safety analysis. Lastly, we present
appropriate metrics that can be used along with our framework to measure safety
of agents. Utilizing our kaleidoscopic teaming framework, we identify
vulnerabilities in various models with respect to their safety in agentic
use-cases.

</details>


### [9] [Cite Pretrain: Retrieval-Free Knowledge Attribution for Large Language Models](https://arxiv.org/abs/2506.17585)
*Yukun Huang,Sanxing Chen,Jian Pei,Manzil Zaheer,Bhuwan Dhingra*

Main category: cs.AI

TL;DR: 本文探讨不依赖测试时检索，通过修改训练过程让大语言模型可靠引用预训练文档的方法，提出Active Indexing，实验表明其优于Passive Indexing，且增加增强数据可提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型引用不可靠，当前系统在推理时查询外部检索器存在延迟、依赖基础设施和易受检索噪声影响等问题，因此探索让大语言模型可靠引用预训练文档的方法。

Method: 采用两阶段过程，先持续预训练将事实与文档标识符绑定，再进行指令微调以引出引用行为；提出Active Indexing，在合成QA对上持续预训练。

Result: 实验表明Active Indexing在所有任务和模型上均优于Passive Indexing，引用精度最高提升30.2%；消融研究显示增加增强数据量性能持续提升。

Conclusion: 通过修改训练过程，Active Indexing可让大语言模型可靠引用预训练文档，且增加增强数据有助于提升性能。

Abstract: Trustworthy language models should provide both correct and verifiable
answers. While language models can sometimes attribute their outputs to
pretraining data, their citations are often unreliable due to hallucination. As
a result, current systems insert citations by querying an external retriever at
inference time, introducing latency, infrastructure dependence, and
vulnerability to retrieval noise. We explore whether LLMs can be made to
reliably attribute to the documents seen during (continual)
pretraining--without test-time retrieval--by revising the training process. To
evaluate this, we release CitePretrainBench, a benchmark that mixes real-world
corpora (Wikipedia, Common Crawl, arXiv) with novel, unseen documents and
probes both short-form (single fact) and long-form (multi-fact) citation tasks.
Our approach follows a two-stage process: (1) continual pretraining to bind
facts to persistent document identifiers, and (2) instruction tuning to elicit
citation behavior. We find that simple Passive Indexing, which appends an
identifier to each document, helps memorize verbatim text but fails on
paraphrased or compositional facts. Instead, we propose Active Indexing, which
continually pretrains on synthetic QA pairs that (1) restate each fact in
diverse compositional forms, and (2) require bidirectional source-to-fact and
fact-to-source generation, jointly teaching the model to generate content from
a cited source and to attribute its own answers. Experiments with Qwen2.5-7B
and 3B show that Active Indexing consistently outperforms Passive Indexing
across all tasks and models, with citation precision gains up to 30.2 percent.
Our ablation studies reveal that performance continues to improve as we scale
the amount of augmented data, showing a clear upward trend even at 16 times the
original token count.

</details>


### [10] [Taming the Untamed: Graph-Based Knowledge Retrieval and Reasoning for MLLMs to Conquer the Unknown](https://arxiv.org/abs/2506.17589)
*Bowen Wang*

Main category: cs.AI

TL;DR: 采用视觉游戏认知构建MH - MMKG，设计查询评估模型，提出多智能体检索器，提升MLLM性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在特定领域任务中因知识有限表现不佳，需探索提升方法。

Method: 以视觉游戏认知为测试台，构建MH - MMKG，设计挑战性查询，提出多智能体检索器。

Result: 所提方法显著提升了多模态大语言模型的性能。

Conclusion: 该方法为多模态知识增强推理提供新视角，为未来研究奠定基础。

Abstract: The real value of knowledge lies not just in its accumulation, but in its
potential to be harnessed effectively to conquer the unknown. Although recent
multimodal large language models (MLLMs) exhibit impressing multimodal
capabilities, they often fail in rarely encountered domain-specific tasks due
to limited relevant knowledge. To explore this, we adopt visual game cognition
as a testbed and select Monster Hunter: World as the target to construct a
multimodal knowledge graph (MH-MMKG), which incorporates multi-modalities and
intricate entity relations. We also design a series of challenging queries
based on MH-MMKG to evaluate the models' ability for complex knowledge
retrieval and reasoning. Furthermore, we propose a multi-agent retriever that
enables a model to autonomously search relevant knowledge without additional
training. Experimental results show that our approach significantly enhances
the performance of MLLMs, providing a new perspective on multimodal
knowledge-augmented reasoning and laying a solid foundation for future
research.

</details>


### [11] [Measuring and Augmenting Large Language Models for Solving Capture-the-Flag Challenges](https://arxiv.org/abs/2506.17644)
*Zimo Ji,Daoyuan Wu,Wenyuan Jiang,Pingchuan Ma,Zongjie Li,Shuai Wang*

Main category: cs.AI

TL;DR: 文章强调CTF中技术知识重要性，构建CTFKnow基准测试LLMs能力，发现其应用知识不足，提出CTFAgent框架，实验显示性能提升，在竞赛中排名靠前。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型发展，关注其自动解决CTF挑战的能力，当前需结合多种能力，强调技术知识重要性并衡量LLMs在这方面的表现。

Method: 构建CTFKnow基准测试LLMs理解CTF知识的能力，提出CTFAgent框架，包含两阶段检索增强生成和交互式环境增强模块。

Result: 在两个流行CTF数据集上CTFAgent性能提升超80%，在picoCTF2024中排名前23.6%。

Conclusion: 测量研究有价值，CTFAgent框架有潜力提升LLMs解决CTF问题的能力。

Abstract: Capture-the-Flag (CTF) competitions are crucial for cybersecurity education
and training. As large language models (LLMs) evolve, there is increasing
interest in their ability to automate CTF challenge solving. For example, DARPA
has organized the AIxCC competition since 2023 to advance AI-powered automated
offense and defense. However, this demands a combination of multiple abilities,
from knowledge to reasoning and further to actions. In this paper, we highlight
the importance of technical knowledge in solving CTF problems and deliberately
construct a focused benchmark, CTFKnow, with 3,992 questions to measure LLMs'
performance in this core aspect. Our study offers a focused and innovative
measurement of LLMs' capability in understanding CTF knowledge and applying it
to solve CTF challenges. Our key findings reveal that while LLMs possess
substantial technical knowledge, they falter in accurately applying this
knowledge to specific scenarios and adapting their strategies based on feedback
from the CTF environment.
  Based on insights derived from this measurement study, we propose CTFAgent, a
novel LLM-driven framework for advancing CTF problem-solving. CTFAgent
introduces two new modules: two-stage Retrieval Augmented Generation (RAG) and
interactive Environmental Augmentation, which enhance LLMs' technical knowledge
and vulnerability exploitation on CTF, respectively. Our experimental results
show that, on two popular CTF datasets, CTFAgent both achieves over 80%
performance improvement. Moreover, in the recent picoCTF2024 hosted by CMU,
CTFAgent ranked in the top 23.6% of nearly 7,000 participating teams. This
reflects the benefit of our measurement study and the potential of our
framework in advancing LLMs' capabilities in CTF problem-solving.

</details>


### [12] [PhysUniBench: An Undergraduate-Level Physics Reasoning Benchmark for Multimodal Models](https://arxiv.org/abs/2506.17667)
*Lintao Wang,Encheng Su,Jiaqi Liu,Pengze Li,Peng Xia,Jiabei Xiao,Wenlong Zhang,Xinnan Dai,Xi Chen,Yuan Meng,Mingyu Ding,Lei Bai,Wanli Ouyang,Shixiang Tang,Aoran Wang,Xinzhu Ma*

Main category: cs.AI

TL;DR: 提出用于评估多模态大语言模型本科物理问题推理能力的大规模多模态基准PhysUniBench，实验显示当前模型表现不佳，该基准旨在推动AI for Science发展。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法在评估本科物理问题推理能力上有局限，需更严格评估。

Method: 构建包含3304个物理问题、8大子学科且每题配一个可视化图的基准，采用多阶段严格流程构建，包括多次发布、专家评估等。

Result: 当前最先进模型在物理推理中面临挑战，如GPT - 4o mini在PhysUniBench中准确率仅约34.2%。

Conclusion: 当前多模态大语言模型在高级物理推理上存在困难，PhysUniBench可推动AI for Science发展。

Abstract: Physics problem-solving is a challenging domain for large AI models,
requiring integration of conceptual understanding, mathematical reasoning, and
interpretation of physical diagrams. Current evaluation methodologies show
notable limitations in capturing the breadth and complexity of
undergraduate-level physics, underscoring the need for more rigorous
assessments. To this end, we present PhysUniBench, a large-scale multimodal
benchmark designed to evaluate and improve the reasoning capabilities of
multimodal large language models (MLLMs) specifically on undergraduate-level
physics problems. PhysUniBench consists of 3,304 physics questions spanning 8
major sub-disciplines of physics, each accompanied by one visual diagrams. The
benchmark includes both open-ended and multiple-choice questions,
systematically curated and difficulty-rated through an iterative
model-in-the-loop process. The benchmark's construction involved a rigorous
multi-stage process, including multiple roll-outs, expert-level evaluation,
automated filtering of easily solved problems, and a nuanced difficulty grading
system with five levels. Through extensive experiments, we observe that current
state-of-the-art models encounter substantial challenges in physics reasoning.
For example, GPT-4o mini achieves only about 34.2\% accuracy in the proposed
PhysUniBench. These results highlight that current MLLMs struggle with advanced
physics reasoning, especially on multi-step problems and those requiring
precise diagram interpretation. By providing a broad and rigorous assessment
tool, PhysUniBench aims to drive progress in AI for Science, encouraging the
development of models with stronger physical reasoning, problem-solving skills,
and multimodal understanding. The benchmark and evaluation scripts are
available at https://prismax-team.github.io/PhysUniBenchmark/.

</details>


### [13] [Beyond Syntax: Action Semantics Learning for App Agents](https://arxiv.org/abs/2506.17697)
*Bohan Tang,Dezhao Luo,Jingxuan Chen,Shaogang Gong,Jianye Hao,Jun Wang,Kun Shao*

Main category: cs.AI

TL;DR: 本文提出Action Semantics Learning (ASL)框架解决现有App代理微调方法问题，理论证明其鲁棒性，实验显示效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的解决方案有计算成本高和依赖外部API问题，现有微调方法存在分布外（OOD）漏洞。

Method: 提出ASL学习框架，定义App代理的动作语义，用SEmantic Estimator (SEE)计算语义奖励训练App代理。

Result: 理论证明ASL在OOD问题上比现有语法学习范式有更强鲁棒性，实验表明ASL显著提高了App代理的准确性和泛化性。

Conclusion: ASL是解决现有App代理微调方法问题的有效方案，能提升App代理性能。

Abstract: The advent of Large Language Models (LLMs) enables the rise of App agents
that interpret user intent and operate smartphone Apps through actions such as
clicking and scrolling. While prompt-based solutions with closed LLM APIs show
promising ability, they incur heavy compute costs and external API dependency.
Fine-tuning smaller open-source LLMs solves these limitations. However, current
fine-tuning methods use a syntax learning paradigm that forces agents to
reproduce exactly the ground truth action strings, leading to
out-of-distribution (OOD) vulnerability. To fill this gap, we propose Action
Semantics Learning (ASL), a novel learning framework, where the learning
objective is capturing the semantics of the ground truth actions. Specifically,
inspired by the programming language theory, we define the action semantics for
App agents as the state transition induced by the action in the user interface.
With this insight, ASL employs a novel SEmantic Estimator (SEE) to compute a
semantic reward to train the App agents in generating actions aligned with the
semantics of ground truth actions, even when the syntactic forms differ. To
support the effectiveness of ASL, we theoretically demonstrate the superior
robustness of ASL for the OOD problem compared with the existing syntax
learning paradigm. Extensive experiments on offline and online smartphone App
operation benchmarks show that ASL significantly improves the accuracy and
generalisation of App agents over existing methods.

</details>


### [14] [AnyMAC: Cascading Flexible Multi-Agent Collaboration via Next-Agent Prediction](https://arxiv.org/abs/2506.17784)
*Song Wang,Zhen Tan,Zihan Chen,Shuang Zhou,Tianlong Chen,Jundong Li*

Main category: cs.AI

TL;DR: 提出基于顺序结构的多智能体协作新框架，评估显示性能优且降低通信开销。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的多智能体协作方法依赖静态或基于图的拓扑结构，缺乏通信的适应性和灵活性。

Method: 提出新框架，从顺序结构而非图结构重新思考多智能体协调，包括Next - Agent Prediction和Next - Context Selection两部分。

Result: 在多个基准测试中表现优越，大幅降低通信开销。

Conclusion: 所提方法能构建任务自适应通信管道，支持角色灵活性和全局信息流。

Abstract: Recent progress in large language model (LLM)-based multi-agent collaboration
highlights the power of structured communication in enabling collective
intelligence. However, existing methods largely rely on static or graph-based
inter-agent topologies, lacking the potential adaptability and flexibility in
communication. In this work, we propose a new framework that rethinks
multi-agent coordination through a sequential structure rather than a graph
structure, offering a significantly larger topology space for multi-agent
communication. Our method focuses on two key directions: (1) Next-Agent
Prediction, which selects the most suitable agent role at each step, and (2)
Next-Context Selection (NCS), which enables each agent to selectively access
relevant information from any previous step. Together, these components
construct task-adaptive communication pipelines that support both role
flexibility and global information flow. Extensive evaluations across multiple
benchmarks demonstrate that our approach achieves superior performance while
substantially reducing communication overhead.

</details>


### [15] [Bayesian Social Deduction with Graph-Informed Language Models](https://arxiv.org/abs/2506.17788)
*Shahab Rahimirad,Guven Gergerli,Lucia Romero,Angela Qian,Matthew Lyle Olson,Simon Stepputtis,Joseph Campbell*

Main category: cs.AI

TL;DR: 评估大语言模型在社交推理任务上的局限，提出混合推理框架，该框架在人机对抗中表现出色并开源代码等。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在社交推理（如Avalon游戏）中存在需大量推理、模型压缩后性能下降等问题。

Method: 引入混合推理框架，将信念推理外部化到结构化概率模型，用大语言模型进行语言理解和交互。

Result: 在Agent - Agent游戏中与更大模型有竞争力，首次在受控研究中击败人类玩家，胜率67%，定性评分更高。

Conclusion: 提出的混合推理框架有效提升大语言模型社交推理能力，相关资源开源利于后续研究。

Abstract: Social reasoning - inferring unobservable beliefs and intentions from partial
observations of other agents - remains a challenging task for large language
models (LLMs). We evaluate the limits of current reasoning language models in
the social deduction game Avalon and find that while the largest models
demonstrate strong performance, they require extensive test-time inference and
degrade sharply when distilled to smaller, real-time-capable variants. To
address this, we introduce a hybrid reasoning framework that externalizes
belief inference to a structured probabilistic model, while using an LLM for
language understanding and interaction. Our approach achieves competitive
performance with much larger models in Agent-Agent play and, notably, is the
first language agent to defeat human players in a controlled study - achieving
a 67% win rate and receiving higher qualitative ratings than both reasoning
baselines and human teammates. We release code, models, and a dataset to
support future work on social reasoning in LLM agents, which can be found at
https://camp-lab-purdue.github.io/bayesian-social-deduction/

</details>


### [16] [Efficient Strategy Synthesis for MDPs via Hierarchical Block Decomposition](https://arxiv.org/abs/2506.17792)
*Alexandros Evangelidis,Gricel Vázquez,Simos Gerasimou*

Main category: cs.AI

TL;DR: 提出动态细化MDP并迭代选择最脆弱区域进行细化的方法，加速大MDP的策略合成，经实验比PRISM性能提升最高达2倍。


<details>
  <summary>Details</summary>
Motivation: 传统策略合成方法在大状态空间中无法扩展，需解决大MDP策略合成效率问题。

Method: 动态细化MDP，迭代选择最脆弱MDP区域进行细化。

Result: 通过多样案例和最大100万状态的MDP实验，相比PRISM性能最多提升2倍。

Conclusion: 该方法在大MDP的现实策略合成任务中是极具竞争力的解决方案。

Abstract: Software-intensive systems, such as software product lines and robotics,
utilise Markov decision processes (MDPs) to capture uncertainty and analyse
sequential decision-making problems. Despite the usefulness of conventional
policy synthesis methods, they fail to scale to large state spaces. Our
approach addresses this issue and accelerates policy synthesis in large MDPs by
dynamically refining the MDP and iteratively selecting the most fragile MDP
regions for refinement. This iterative procedure offers a balance between
accuracy and efficiency, as refinement occurs only when necessary. Through a
comprehensive empirical evaluation comprising diverse case studies and MDPs up
to 1M states, we demonstrate significant performance improvements yielded by
our approach compared to the leading probabilistic model checker PRISM (up to
2x), thus offering a very competitive solution for real-world policy synthesis
tasks in larger MDPs.

</details>


### [17] [Reflective Verbal Reward Design for Pluralistic Alignment](https://arxiv.org/abs/2506.17834)
*Carter Blair,Kate Larson,Edith Law*

Main category: cs.AI

TL;DR: 论文提出个性化奖励建模方法，通过引导用户反思对话构建偏好，在研究中比非反思模型准确率提升9 - 12%，且样本效率更高。


<details>
  <summary>Details</summary>
Motivation: 现有基于聚合人类反馈的单一奖励模型存在抑制少数偏好的问题，因人类价值观并非同质。

Method: 用语言模型引导用户进行反思对话以构建偏好，利用对话历史作为另一个语言模型的上下文，形成个性化奖励函数。

Result: 在30名参与者的研究中，该方法比非反思语言奖励模型准确率提高9 - 12%，样本效率高于传统监督学习方法。

Conclusion: 提出的个性化奖励建模方法有效，能解决聚合反馈单一奖励模型的问题。

Abstract: AI agents are commonly aligned with "human values" through reinforcement
learning from human feedback (RLHF), where a single reward model is learned
from aggregated human feedback and used to align an agent's behavior. However,
human values are not homogeneous--different people hold distinct and sometimes
conflicting values. Aggregating feedback into a single reward model risks
disproportionately suppressing minority preferences. To address this, we
present a novel reward modeling approach for learning individualized reward
models. Our approach uses a language model to guide users through reflective
dialogues where they critique agent behavior and construct their preferences.
This personalized dialogue history, containing the user's reflections and
critiqued examples, is then used as context for another language model that
serves as an individualized reward function (what we call a "verbal reward
model") for evaluating new trajectories. In studies with 30 participants, our
method achieved a 9-12% improvement in accuracy over non-reflective verbal
reward models while being more sample efficient than traditional supervised
learning methods.

</details>


### [18] [Out of Control -- Why Alignment Needs Formal Control Theory (and an Alignment Control Stack)](https://arxiv.org/abs/2506.17846)
*Elija Perrier*

Main category: cs.AI

TL;DR: 论文认为形式最优控制理论应成为AI对齐研究核心，提出对齐控制栈，以创建更全面对齐框架。


<details>
  <summary>Details</summary>
Motivation: 现有AI安全和机制可解释性工作在控制框架泛化和协议互操作性研究方面不足。

Method: 通过形式最优控制原则重新构建对齐问题，提出分层的对齐控制栈，明确各层测量和控制特性及互操作性。

Result: 提出了一个分层的Alignment Control Stack。

Conclusion: 将最优控制方法与实际部署考虑相结合，可创建更全面的对齐框架，提升高级AI系统安全性和可靠性。

Abstract: This position paper argues that formal optimal control theory should be
central to AI alignment research, offering a distinct perspective from
prevailing AI safety and security approaches. While recent work in AI safety
and mechanistic interpretability has advanced formal methods for alignment,
they often fall short of the generalisation required of control frameworks for
other technologies. There is also a lack of research into how to render
different alignment/control protocols interoperable. We argue that by recasting
alignment through principles of formal optimal control and framing alignment in
terms of hierarchical stack from physical to socio-technical layers according
to which controls may be applied we can develop a better understanding of the
potential and limitations for controlling frontier models and agentic AI
systems. To this end, we introduce an Alignment Control Stack which sets out a
hierarchical layered alignment stack, identifying measurement and control
characteristics at each layer and how different layers are formally
interoperable. We argue that such analysis is also key to the assurances that
will be needed by governments and regulators in order to see AI technologies
sustainably benefit the community. Our position is that doing so will bridge
the well-established and empirically validated methods of optimal control with
practical deployment considerations to create a more comprehensive alignment
framework, enhancing how we approach safety and reliability for advanced AI
systems.

</details>


### [19] [Towards Robust Fact-Checking: A Multi-Agent System with Advanced Evidence Retrieval](https://arxiv.org/abs/2506.17878)
*Tam Trinh,Manh Nguyen,Truong-Son Hy*

Main category: cs.AI

TL;DR: 本文提出用于自动事实核查的多智能体系统，在基准数据集上表现良好，改进了现有方法。


<details>
  <summary>Details</summary>
Motivation: 数字时代错误信息快速传播，传统人工事实核查难以应对，现有自动化方法存在局限性，需更优解决方案。

Method: 提出包含输入摄取、查询生成、证据检索和判决预测四个专业智能体的多智能体系统。

Result: 在基准数据集上Macro F1得分比基线方法提高12.3%，能分解复杂声明、检索可靠证据并生成透明解释。

Conclusion: 该方法为自动事实核查领域提供更准确、高效、透明的验证方法，适用于实际应用，代码已开源。

Abstract: The rapid spread of misinformation in the digital era poses significant
challenges to public discourse, necessitating robust and scalable fact-checking
solutions. Traditional human-led fact-checking methods, while credible,
struggle with the volume and velocity of online content, prompting the
integration of automated systems powered by Large Language Models (LLMs).
However, existing automated approaches often face limitations, such as handling
complex claims, ensuring source credibility, and maintaining transparency. This
paper proposes a novel multi-agent system for automated fact-checking that
enhances accuracy, efficiency, and explainability. The system comprises four
specialized agents: an Input Ingestion Agent for claim decomposition, a Query
Generation Agent for formulating targeted subqueries, an Evidence Retrieval
Agent for sourcing credible evidence, and a Verdict Prediction Agent for
synthesizing veracity judgments with human-interpretable explanations.
Evaluated on benchmark datasets (FEVEROUS, HOVER, SciFact), the proposed system
achieves a 12.3% improvement in Macro F1-score over baseline methods. The
system effectively decomposes complex claims, retrieves reliable evidence from
trusted sources, and generates transparent explanations for verification
decisions. Our approach contributes to the growing field of automated
fact-checking by providing a more accurate, efficient, and transparent
verification methodology that aligns with human fact-checking practices while
maintaining scalability for real-world applications. Our source code is
available at https://github.com/HySonLab/FactAgent

</details>


### [20] [Leveraging Large Language Model for Intelligent Log Processing and Autonomous Debugging in Cloud AI Platforms](https://arxiv.org/abs/2506.17900)
*Cheng Ji,Huaiying Luo*

Main category: cs.AI

TL;DR: 针对云平台AI系统日志处理难题，提出LLM - ID框架，经实验其故障定位准确率提升16.2%，优于主流方法。


<details>
  <summary>Details</summary>
Motivation: 云平台AI系统规模扩大、复杂度增加，系统运行产生的日志数据存在处理难题，影响故障定位和系统自修复。

Method: 在预训练Transformer模型基础上扩展，集成多阶段语义推理机制，包括动态结构化日志、提取模板和模式，用微调LLM结合多轮注意力机制推理，引入基于强化学习的策略引导恢复规划器。

Result: 在云平台日志数据集实验中，LLM - ID使故障定位准确率提高16.2%。

Conclusion: 与现有规则引擎或传统日志分析系统相比，提出的模型语义理解、持续学习和异构环境适应能力更强，表现显著优于当前主流方法。

Abstract: With the increasing complexity and rapid expansion of the scale of AI systems
in cloud platforms, the log data generated during system operation is massive,
unstructured, and semantically ambiguous, which brings great challenges to
fault location and system self-repair. In order to solve this problem, this
paper proposes an intelligent log processing and automatic debugging framework
based on Large Language Model (LLM), named Intelligent Debugger (LLM-ID). This
method is extended on the basis of the existing pre-trained Transformer model,
and integrates a multi-stage semantic inference mechanism to realize the
context understanding of system logs and the automatic reconstruction of fault
chains. Firstly, the system log is dynamically structured, and the unsupervised
clustering and embedding mechanism is used to extract the event template and
semantic schema. Subsequently, the fine-tuned LLM combined with the multi-round
attention mechanism to perform contextual reasoning on the log sequence to
generate potential fault assumptions and root cause paths. Furthermore, this
paper introduces a reinforcement learning-based policy-guided recovery planner,
which is driven by the remediation strategy generated by LLM to support dynamic
decision-making and adaptive debugging in the cloud environment. Compared with
the existing rule engine or traditional log analysis system, the proposed model
has stronger semantic understanding ability, continuous learning ability and
heterogeneous environment adaptability. Experiments on the cloud platform log
dataset show that LLM-ID improves the fault location accuracy by 16.2%, which
is significantly better than the current mainstream methods

</details>


### [21] [Airalogy: AI-empowered universal data digitization for research automation](https://arxiv.org/abs/2506.18586)
*Zijie Yang,Qiji Zhou,Fang Guo,Sijie Zhang,Yexun Xi,Jinglei Nie,Yudian Zhu,Liping Huang,Chou Wu,Yonghe Xia,Xiaoyu Ma,Yingming Pu,Panzhong Lu,Junshu Pan,Mingtao Chen,Tiannan Guo,Yanmei Dou,Hongyu Chen,Anping Zeng,Jiaxing Huang,Tian Xu,Yue Zhang*

Main category: cs.AI

TL;DR: 本文介绍了研究数据标准化面临的挑战，提出并开发了Airalogy平台，该平台已在西湖大学部署，有望加速科研创新。


<details>
  <summary>Details</summary>
Motivation: 当前AI应用受限于数据问题，研究数据收集存在碎片化、缺乏标准等问题，且无平台能兼顾通用性与标准化，阻碍了AI驱动的科学进步，因此需要开发新平台。

Method: 开发Airalogy平台，该平台以AI和社区驱动，用可定制的标准化数据记录呈现研究工作流程，并提供高级AI研究助手。

Result: Airalogy已在西湖大学四个学院的实验室部署。

Conclusion: Airalogy有潜力加速和自动化大学、行业及全球研究社区的科学创新，造福人类。

Abstract: Research data are the foundation of Artificial Intelligence (AI)-driven
science, yet current AI applications remain limited to a few fields with
readily available, well-structured, digitized datasets. Achieving comprehensive
AI empowerment across multiple disciplines is still out of reach. Present-day
research data collection is often fragmented, lacking unified standards,
inefficiently managed, and difficult to share. Creating a single platform for
standardized data digitization needs to overcome the inherent challenge of
balancing between universality (supporting the diverse, ever-evolving needs of
various disciplines) and standardization (enforcing consistent formats to fully
enable AI). No existing platform accommodates both facets. Building a truly
multidisciplinary platform requires integrating scientific domain knowledge
with sophisticated computing skills. Researchers often lack the computational
expertise to design customized and standardized data recording methods, whereas
platform developers rarely grasp the intricate needs of multiple scientific
domains. These gaps impede research data standardization and hamper AI-driven
progress. In this study, we address these challenges by developing Airalogy
(https://airalogy.com), the world's first AI- and community-driven platform
that balances universality and standardization for digitizing research data
across multiple disciplines. Airalogy represents entire research workflows
using customizable, standardized data records and offers an advanced AI
research copilot for intelligent Q&A, automated data entry, analysis, and
research automation. Already deployed in laboratories across all four schools
of Westlake University, Airalogy has the potential to accelerate and automate
scientific innovation in universities, industry, and the global research
community-ultimately benefiting humanity as a whole.

</details>


### [22] [Learning, Reasoning, Refinement: A Framework for Kahneman's Dual-System Intelligence in GUI Agents](https://arxiv.org/abs/2506.17913)
*Jinjie Wei,Jiyao Liu,Lihao Liu,Ming Hu,Junzhi Ning,Mingcheng Li,Weijie Yin,Junjun He,Xiao Liang,Chao Feng,Dingkang Yang*

Main category: cs.AI

TL;DR: 本文提出CogniGUI框架解决现有GUI智能体系统局限，结合双组件实现自适应学习，并引入ScreenSeek基准评估，实验显示CogniGUI效果更佳。


<details>
  <summary>Details</summary>
Motivation: 现有GUI智能体系统依赖试错决策、缺乏学习适应能力，且评估指标过于简单，无法反映真实交互复杂性。

Method: 受双过程理论启发，结合全解析引擎和GRPO接地智能体，形成双系统设计；引入ScreenSeek基准。

Result: CogniGUI在现有和新提出的基准测试中超越了现有方法。

Conclusion: CogniGUI能解决现有GUI智能体系统局限，通过独特设计实现自适应学习，新基准有助于评估系统泛化和适应性。

Abstract: Graphical User Interface (GUI) agents have made significant progress in
automating digital tasks through the utilization of computer vision and
language models. Nevertheless, existing agent systems encounter notable
limitations. Firstly, they predominantly depend on trial and error decision
making rather than progressive reasoning, thereby lacking the capability to
learn and adapt from interactive encounters. Secondly, these systems are
assessed using overly simplistic single step accuracy metrics, which do not
adequately reflect the intricate nature of real world GUI interactions. In this
paper, we present CogniGUI, a cognitive framework developed to overcome these
limitations by enabling adaptive learning for GUI automation resembling
human-like behavior. Inspired by Kahneman's Dual Process Theory, our approach
combines two main components: (1) an omni parser engine that conducts immediate
hierarchical parsing of GUI elements through quick visual semantic analysis to
identify actionable components, and (2) a Group based Relative Policy
Optimization (GRPO) grounding agent that assesses multiple interaction paths
using a unique relative reward system, promoting minimal and efficient
operational routes. This dual-system design facilitates iterative ''exploration
learning mastery'' cycles, enabling the agent to enhance its strategies over
time based on accumulated experience. Moreover, to assess the generalization
and adaptability of agent systems, we introduce ScreenSeek, a comprehensive
benchmark that includes multi application navigation, dynamic state
transitions, and cross interface coherence, which are often overlooked
challenges in current benchmarks. Experimental results demonstrate that
CogniGUI surpasses state-of-the-art methods in both the current GUI grounding
benchmarks and our newly proposed benchmark.

</details>


### [23] [medicX-KG: A Knowledge Graph for Pharmacists' Drug Information Needs](https://arxiv.org/abs/2506.17959)
*Lizzy Farrugia,Lilian M. Azzopardi,Jeremy Debattista,Charlie Abela*

Main category: cs.AI

TL;DR: 本文介绍了面向药剂师的知识图谱medicX - KG，阐述其构建、评估情况及局限性与未来方向。


<details>
  <summary>Details</summary>
Motivation: 药剂师角色转变需要准确及时的药品信息，当前缺乏统一国家药品库，需整合数据以支持决策。

Method: 利用人工智能和语义技术，整合英国国家处方集、DrugBank和马耳他药品管理局的数据，通过访谈药剂师确保设计实用性，进行数据提取、本体设计和语义映射来构建medicX - KG。

Result: medicX - KG能有效支持关于药品可用性、相互作用、不良反应和治疗类别的查询。

Conclusion: medicX - KG有一定成效，但存在缺少详细剂量编码和实时更新等局限，可在未来改进。

Abstract: The role of pharmacists is evolving from medicine dispensing to delivering
comprehensive pharmaceutical services within multidisciplinary healthcare
teams. Central to this shift is access to accurate, up-to-date medicinal
product information supported by robust data integration. Leveraging artificial
intelligence and semantic technologies, Knowledge Graphs (KGs) uncover hidden
relationships and enable data-driven decision-making. This paper presents
medicX-KG, a pharmacist-oriented knowledge graph supporting clinical and
regulatory decisions. It forms the semantic layer of the broader medicX
platform, powering predictive and explainable pharmacy services. medicX-KG
integrates data from three sources, including, the British National Formulary
(BNF), DrugBank, and the Malta Medicines Authority (MMA) that addresses Malta's
regulatory landscape and combines European Medicines Agency alignment with
partial UK supply dependence. The KG tackles the absence of a unified national
drug repository, reducing pharmacists' reliance on fragmented sources. Its
design was informed by interviews with practicing pharmacists to ensure
real-world applicability. We detail the KG's construction, including data
extraction, ontology design, and semantic mapping. Evaluation demonstrates that
medicX-KG effectively supports queries about drug availability, interactions,
adverse reactions, and therapeutic classes. Limitations, including missing
detailed dosage encoding and real-time updates, are discussed alongside
directions for future enhancements.

</details>


### [24] [Graphs Meet AI Agents: Taxonomy, Progress, and Future Opportunities](https://arxiv.org/abs/2506.18019)
*Yuanchen Bei,Weizhi Zhang,Siwen Wang,Weizhi Chen,Sheng Zhou,Hao Chen,Yong Li,Jiajun Bu,Shirui Pan,Yizhou Yu,Irwin King,Fakhri Karray,Philip S. Yu*

Main category: cs.AI

TL;DR: 本文系统综述图如何赋能AI智能体，探索图技术与智能体核心功能结合，介绍应用和未来研究方向，还在Github收集更新相关资源。


<details>
  <summary>Details</summary>
Motivation: 当前AI智能体虽有发展，但完成复杂任务面临信息处理挑战，图在组织复杂数据关系有优势，可助力智能体能力提升。

Method: 对图赋能AI智能体进行系统综述，探索图技术与智能体核心功能的整合。

Result: 梳理了图技术与智能体核心功能结合情况，介绍了显著应用。

Conclusion: 希望通过综述启发下一代AI智能体借助图技术应对复杂挑战。

Abstract: AI agents have experienced a paradigm shift, from early dominance by
reinforcement learning (RL) to the rise of agents powered by large language
models (LLMs), and now further advancing towards a synergistic fusion of RL and
LLM capabilities. This progression has endowed AI agents with increasingly
strong abilities. Despite these advances, to accomplish complex real-world
tasks, agents are required to plan and execute effectively, maintain reliable
memory, and coordinate smoothly with other agents. Achieving these capabilities
involves contending with ever-present intricate information, operations, and
interactions. In light of this challenge, data structurization can play a
promising role by transforming intricate and disorganized data into
well-structured forms that agents can more effectively understand and process.
In this context, graphs, with their natural advantage in organizing, managing,
and harnessing intricate data relationships, present a powerful data paradigm
for structurization to support the capabilities demanded by advanced AI agents.
To this end, this survey presents a first systematic review of how graphs can
empower AI agents. Specifically, we explore the integration of graph techniques
with core agent functionalities, highlight notable applications, and identify
prospective avenues for future research. By comprehensively surveying this
burgeoning intersection, we hope to inspire the development of next-generation
AI agents equipped to tackle increasingly sophisticated challenges with graphs.
Related resources are collected and continuously updated for the community in
the Github link.

</details>


### [25] [Action Language BC+](https://arxiv.org/abs/2506.18044)
*Joseph Babb,Joohyung Lee*

Main category: cs.AI

TL;DR: 提出新动作语言BC+，缩小动作语言与现代ASP语言差距，具高表达力且可用ASP求解器计算。


<details>
  <summary>Details</summary>
Motivation: 早期动作语言对应的答案集程序形式较现代ASP语言受限，为缩小两者差距。

Method: 基于命题公式的通用稳定模型语义定义BC+的语义。

Result: BC+具有足够表达力，涵盖其他动作语言优点，可利用ASP求解器计算并实现了语言扩展。

Conclusion: BC+是一种有效的动作语言，连接了动作语言和现代ASP语言。

Abstract: Action languages are formal models of parts of natural language that are
designed to describe effects of actions. Many of these languages can be viewed
as high level notations of answer set programs structured to represent
transition systems. However, the form of answer set programs considered in the
earlier work is quite limited in comparison with the modern Answer Set
Programming (ASP) language, which allows several useful constructs for
knowledge representation, such as choice rules, aggregates, and abstract
constraint atoms. We propose a new action language called BC+, which closes the
gap between action languages and the modern ASP language. The main idea is to
define the semantics of BC+ in terms of general stable model semantics for
propositional formulas, under which many modern ASP language constructs can be
identified with shorthands for propositional formulas. Language BC+ turns out
to be sufficiently expressive to encompass the best features of other action
languages, such as languages B, C, C+, and BC. Computational methods available
in ASP solvers are readily applicable to compute BC+, which led to an
implementation of the language by extending system cplus2asp.

</details>


### [26] [Weighted Assumption Based Argumentation to reason about ethical principles and actions](https://arxiv.org/abs/2506.18056)
*Paolo Baldi,Fabio Aurelio D'Asaro,Abeer Dyoub,Francesca Alessandra Lisi*

Main category: cs.AI

TL;DR: 本文将加权论证融入基于假设的论证（ABA），并通过伦理推理示例说明，基于回答集编程实现。


<details>
  <summary>Details</summary>
Motivation: 增强基于假设的论证（ABA）的功能。

Method: 为论证分配权重，推导ABA论证间攻击的权重，结合回答集编程实现。

Result: 通过伦理推理领域的示例展示了方案。

Conclusion: 实现了加权论证与ABA的融合并给出实现方法。

Abstract: We augment Assumption Based Argumentation (ABA for short) with weighted
argumentation. In a nutshell, we assign weights to arguments and then derive
the weight of attacks between ABA arguments. We illustrate our proposal through
running examples in the field of ethical reasoning, and present an
implementation based on Answer Set Programming.

</details>


### [27] [Deep Research Agents: A Systematic Examination And Roadmap](https://arxiv.org/abs/2506.18096)
*Yuxuan Huang,Yihang Chen,Haozheng Zhang,Kang Li,Meng Fang,Linyi Yang,Xiaoguang Li,Lifeng Shang,Songcen Xu,Jianye Hao,Kun Shao,Jun Wang*

Main category: cs.AI

TL;DR: 本文分析深度研究（DR）代理的基础技术和架构组件，提出分类法，评估现有基准，指出挑战与研究方向，并提供相关研究仓库。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型发展出现DR代理，为了详细了解其构成并推动其发展，进行相关分析。

Method: 回顾信息获取策略，对比不同检索方法；研究模块化工具使用框架；提出区分静态和动态工作流的分类法，并根据规划策略和代理组成对代理架构进行分类；对当前基准进行评估。

Result: 完成对DR代理基础技术和架构组件的分析，提出分类法，指出当前基准的关键局限。

Conclusion: 明确了DR代理研究的开放挑战和未来有前景的研究方向。

Abstract: The rapid progress of Large Language Models (LLMs) has given rise to a new
category of autonomous AI systems, referred to as Deep Research (DR) agents.
These agents are designed to tackle complex, multi-turn informational research
tasks by leveraging a combination of dynamic reasoning, adaptive long-horizon
planning, multi-hop information retrieval, iterative tool use, and the
generation of structured analytical reports. In this paper, we conduct a
detailed analysis of the foundational technologies and architectural components
that constitute Deep Research agents. We begin by reviewing information
acquisition strategies, contrasting API-based retrieval methods with
browser-based exploration. We then examine modular tool-use frameworks,
including code execution, multimodal input processing, and the integration of
Model Context Protocols (MCPs) to support extensibility and ecosystem
development. To systematize existing approaches, we propose a taxonomy that
differentiates between static and dynamic workflows, and we classify agent
architectures based on planning strategies and agent composition, including
single-agent and multi-agent configurations. We also provide a critical
evaluation of current benchmarks, highlighting key limitations such as
restricted access to external knowledge, sequential execution inefficiencies,
and misalignment between evaluation metrics and the practical objectives of DR
agents. Finally, we outline open challenges and promising directions for future
research. A curated and continuously updated repository of DR agent research is
available at: {https://github.com/ai-agents-2030/awesome-deep-research-agent}.

</details>


### [28] [Decentralized Consensus Inference-based Hierarchical Reinforcement Learning for Multi-Constrained UAV Pursuit-Evasion Game](https://arxiv.org/abs/2506.18126)
*Xiang Yuming,Li Sizhao,Li Rongpeng,Zhao Zhifeng,Zhang Honggang*

Main category: cs.AI

TL;DR: 本文提出用于多约束追逃游戏中多无人机协同躲避与编队覆盖任务的两级框架CI - HRL，实验验证其有更好性能。


<details>
  <summary>Details</summary>
Motivation: 多无人机系统在多约束追逃游戏的协同躲避与编队覆盖任务有挑战性，尤其是在通信受限下，问题复杂、维度高。

Method: 提出两级框架CI - HRL，高层用ConsMAC模块感知全局信息达成共识，低层用AT - M和策略蒸馏实现控制。

Result: 高保真软件在环仿真等实验表明CI - HRL有更好的群集协同躲避和任务完成能力。

Conclusion: CI - HRL为多约束追逃游戏中的协同躲避与编队覆盖任务提供了优越解决方案。

Abstract: Multiple quadrotor unmanned aerial vehicle (UAV) systems have garnered
widespread research interest and fostered tremendous interesting applications,
especially in multi-constrained pursuit-evasion games (MC-PEG). The Cooperative
Evasion and Formation Coverage (CEFC) task, where the UAV swarm aims to
maximize formation coverage across multiple target zones while collaboratively
evading predators, belongs to one of the most challenging issues in MC-PEG,
especially under communication-limited constraints. This multifaceted problem,
which intertwines responses to obstacles, adversaries, target zones, and
formation dynamics, brings up significant high-dimensional complications in
locating a solution. In this paper, we propose a novel two-level framework
(i.e., Consensus Inference-based Hierarchical Reinforcement Learning (CI-HRL)),
which delegates target localization to a high-level policy, while adopting a
low-level policy to manage obstacle avoidance, navigation, and formation.
Specifically, in the high-level policy, we develop a novel multi-agent
reinforcement learning module, Consensus-oriented Multi-Agent Communication
(ConsMAC), to enable agents to perceive global information and establish
consensus from local states by effectively aggregating neighbor messages.
Meanwhile, we leverage an Alternative Training-based Multi-agent proximal
policy optimization (AT-M) and policy distillation to accomplish the low-level
control. The experimental results, including the high-fidelity
software-in-the-loop (SITL) simulations, validate that CI-HRL provides a
superior solution with enhanced swarm's collaborative evasion and task
completion capabilities.

</details>


### [29] [jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval](https://arxiv.org/abs/2506.18902)
*Michael Günther,Saba Sturua,Mohammad Kalim Akram,Isabelle Mohr,Andrei Ungureanu,Sedigheh Eslami,Scott Martens,Bo Wang,Nan Wang,Han Xiao*

Main category: cs.AI

TL;DR: 介绍38亿参数的多模态嵌入模型jina - embeddings - v4，它有新架构，结合LoRA适配器，在多任务表现佳，还推出Jina - VDR基准。


<details>
  <summary>Details</summary>
Motivation: 开发能统一文本和图像表示、在多种检索场景表现良好的多模态嵌入模型，并评估其处理视觉丰富内容的能力。

Method: 采用新架构支持单向量和多向量嵌入，结合任务特定的Low - Rank Adaptation (LoRA) 适配器。

Result: jina - embeddings - v4在单模态和跨模态检索任务上达到了先进水平，尤其在处理视觉丰富内容方面表现出色。

Conclusion: jina - embeddings - v4是有效的多模态嵌入模型，Jina - VDR可用于评估其处理视觉丰富图像检索的能力。

Abstract: We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding
model that unifies text and image representations through a novel architecture
supporting both single-vector and multi-vector embeddings in the late
interaction style. The model incorporates task-specific Low-Rank Adaptation
(LoRA) adapters to optimize performance across diverse retrieval scenarios,
including query-based information retrieval, cross-modal semantic similarity,
and programming code search. Comprehensive evaluations demonstrate that
jina-embeddings-v4 achieves state-of-the-art performance on both single- modal
and cross-modal retrieval tasks, with particular strength in processing
visually rich content such as tables, charts, diagrams, and mixed-media
formats. To facilitate evaluation of this capability, we also introduce
Jina-VDR, a novel benchmark specifically designed for visually rich image
retrieval.

</details>


### [30] [SE-Merging: A Self-Enhanced Approach for Dynamic Model Merging](https://arxiv.org/abs/2506.18135)
*Zijun Chen,Zhanpeng Zhou,Bo Zhang,Weinan Zhang,Xi Sun,Junchi Yan*

Main category: cs.AI

TL;DR: 文章从表征视角探究模型合并机制，提出SE - Merging框架，无需额外训练实现动态模型合并，实验表明其有显著性能提升且与现有技术兼容。


<details>
  <summary>Details</summary>
Motivation: 模型合并虽有实证成功，但底层机制理解不足，需深入探究其机制。

Method: 从表征视角分析模型合并机制，基于分析结果提出SE - Merging框架，利用机制特点动态识别任务并自适应调整合并系数。

Result: 广泛实验表明SE - Merging实现显著性能提升，且与现有模型合并技术兼容。

Conclusion: 模型合并通过区分不同任务样本和适应对应专家模型实现多任务能力，SE - Merging可有效增强合并模型的特定任务专业能力。

Abstract: Model merging has gained increasing attention due to its intriguing property:
interpolating the parameters of different task-specific fine-tuned models leads
to multi-task abilities. However, despite its empirical success, the underlying
mechanisms of model merging remain poorly understood. In this work, we delve
into the mechanism behind model merging from a representation perspective. Our
analysis reveals that model merging achieves multi-task abilities through two
key capabilities: i) distinguishing samples from different tasks, and ii)
adapting to the corresponding expert model for each sample. These two
capabilities allow the merged model to retain task-specific expertise, enabling
efficient multi-task adaptation. Building on these insights, we propose
\texttt{SE-Merging}, a self-enhanced model merging framework that leverages
these two characteristics to dynamically identify the corresponding task for
each sample and then adaptively rescales the merging coefficients to further
enhance task-specific expertise in the merged model. Notably,
\texttt{SE-Merging} achieves dynamic model merging without additional training.
Extensive experiments demonstrate that \texttt{SE-Merging} achieves significant
performance improvements while remaining compatible with existing model merging
techniques.

</details>


### [31] [CoachGPT: A Scaffolding-based Academic Writing Assistant](https://arxiv.org/abs/2506.18149)
*Fumian Chen,Sotheara Veng,Joshua Wilson,Xiaoming Li,Hui Fang*

Main category: cs.AI

TL;DR: 传统学术写作辅助工具存在不足，本文开发了CoachGPT辅助学术写作，用户研究证明其有用。


<details>
  <summary>Details</summary>
Motivation: 传统写作辅助方式有局限，早期写作助手不准确、机器学习助手训练成本高、大语言模型用于教育有弊端，需开发新工具。

Method: 开发基于AI智能体的网页应用CoachGPT，接收教育者指令，转化为子任务，用大语言模型提供实时反馈和建议。

Result: 用户研究证明了CoachGPT的有用性以及大语言模型用于学术写作的潜力。

Conclusion: CoachGPT提供更沉浸的写作体验和个性化反馈，有别于现有写作助手，大语言模型用于学术写作有潜力。

Abstract: Academic writing skills are crucial for students' success, but can feel
overwhelming without proper guidance and practice, particularly when writing in
a second language. Traditionally, students ask instructors or search
dictionaries, which are not universally accessible. Early writing assistants
emerged as rule-based systems that focused on detecting misspellings,
subject-verb disagreements, and basic punctuation errors; however, they are
inaccurate and lack contextual understanding. Machine learning-based assistants
demonstrate a strong ability for language understanding but are expensive to
train. Large language models (LLMs) have shown remarkable capabilities in
generating responses in natural languages based on given prompts. Still, they
have a fundamental limitation in education: they generate essays without
teaching, which can have detrimental effects on learning when misused. To
address this limitation, we develop CoachGPT, which leverages large language
models (LLMs) to assist individuals with limited educational resources and
those who prefer self-paced learning in academic writing. CoachGPT is an AI
agent-based web application that (1) takes instructions from experienced
educators, (2) converts instructions into sub-tasks, and (3) provides real-time
feedback and suggestions using large language models. This unique scaffolding
structure makes CoachGPT unique among existing writing assistants. Compared to
existing writing assistants, CoachGPT provides a more immersive writing
experience with personalized feedback and guidance. Our user studies prove the
usefulness of CoachGPT and the potential of large language models for academic
writing.

</details>


### [32] [AI Through the Human Lens: Investigating Cognitive Theories in Machine Psychology](https://arxiv.org/abs/2506.18156)
*Akash Kundu,Rishika Goswami*

Main category: cs.AI

TL;DR: 研究大语言模型在四种心理学框架下是否呈现类人认知模式，评估多种模型，发现其有类人行为，探讨相关影响和未来工作。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型是否展现出类人认知模式。

Method: 使用结构化提示和自动评分对多个专有和开源模型进行评估。

Result: 模型常生成连贯叙述，易受积极框架影响，道德判断与自由/压迫相关，存在经大量合理化缓解的自相矛盾。

Conclusion: 模型行为类似人类认知倾向，但受训练数据和对齐方法影响，需关注AI透明度、道德部署及跨认知心理学和AI安全的未来工作。

Abstract: We investigate whether Large Language Models (LLMs) exhibit human-like
cognitive patterns under four established frameworks from psychology: Thematic
Apperception Test (TAT), Framing Bias, Moral Foundations Theory (MFT), and
Cognitive Dissonance. We evaluated several proprietary and open-source models
using structured prompts and automated scoring. Our findings reveal that these
models often produce coherent narratives, show susceptibility to positive
framing, exhibit moral judgments aligned with Liberty/Oppression concerns, and
demonstrate self-contradictions tempered by extensive rationalization. Such
behaviors mirror human cognitive tendencies yet are shaped by their training
data and alignment methods. We discuss the implications for AI transparency,
ethical deployment, and future work that bridges cognitive psychology and AI
safety

</details>


### [33] [Chain-of-Memory: Enhancing GUI Agents for Cross-Application Navigation](https://arxiv.org/abs/2506.18158)
*Xinzge Gao,Chuanrui Hu,Bin Chen,Teng Li*

Main category: cs.AI

TL;DR: 提出Chain-of-Memory (CoM)方法用于GUI智能体显式建模短、长期记忆，开发GUI Odyssey-CoM数据集评估其效果，实验表明CoM提升了跨应用任务性能，小模型也能有较好记忆管理能力，数据集和代码将开源。


<details>
  <summary>Details</summary>
Motivation: 现有GUI智能体方法依赖历史截图或动作隐式表示任务状态，在理解任务状态和存储关键信息方面存在挑战。

Method: 提出CoM方法，通过捕捉动作描述、整合任务相关屏幕信息并维护专用内存模块来显式建模短、长期记忆；开发GUI Odyssey-CoM数据集进行评估。

Result: CoM显著提升了GUI智能体在跨应用任务中的性能，GUI Odyssey-CoM使7B模型能达到与72B模型相当的记忆管理能力。

Conclusion: CoM方法有效，能提升GUI智能体性能，且可使小模型具备较强记忆管理能力。

Abstract: Multimodal large language models (MLLMs) are attracting growing attention in
the development of Graphical User Interface (GUI) agents. Existing approaches
often rely on historical screenshots or actions to implicitly represent the
task state. This reliance poses challenges for GUI agents in accurately
understanding task states and underscores the absence of effective mechanisms
to store critical information in complex and lengthy cross-app tasks. To
address these challenges, we propose Chain-of-Memory (CoM), a novel approach
for explicitly modeling short-term and long-term memory in GUI agents. CoM
achieves this by capturing action descriptions, integrating task-relevant
screen information, and maintaining a dedicated memory module to store and
manage this information. By leveraging explicit memory representations, CoM
enables GUI agents to better understand task states and retain critical
historical information persistently. To equip GUI agents with memory management
capabilities and evaluate the effectiveness of CoM, we developed the GUI
Odyssey-CoM, a dataset comprising 111k screen-action pairs annotated with
Chain-of-Memory. Experimental results demonstrate that CoM significantly
improves GUI agents' performance in cross-application tasks. Additionally, GUI
Odyssey-CoM enables 7B models to achieve memory management capabilities
comparable to 72B models. The dataset and code will be open-sourced.

</details>


### [34] [Reasoning about Uncertainty: Do Reasoning Models Know When They Don't Know?](https://arxiv.org/abs/2506.18183)
*Zhiting Mei,Christina Zhang,Tenny Yin,Justin Lidard,Ola Shorinwa,Anirudha Majumdar*

Main category: cs.AI

TL;DR: 文章探讨推理模型的不确定性量化，发现模型通常过度自信，深度推理会加剧，内省可改善但不统一，最后给出研究方向。


<details>
  <summary>Details</summary>
Motivation: 推理模型易产生幻觉，了解何时及多大程度信任模型对其在现实应用中的安全部署至关重要，因此要探索推理模型的不确定性量化。

Method: 提出三个基本问题并引入内省式不确定性量化方法，在多个基准上对SOTA推理模型进行广泛评估。

Result: 推理模型通常过度自信，深度推理会加剧过度自信，内省可使部分模型校准更好但并非所有模型。

Conclusion: 指出设计必要的UQ基准和改进推理模型校准的重要研究方向。

Abstract: Reasoning language models have set state-of-the-art (SOTA) records on many
challenging benchmarks, enabled by multi-step reasoning induced using
reinforcement learning. However, like previous language models, reasoning
models are prone to generating confident, plausible responses that are
incorrect (hallucinations). Knowing when and how much to trust these models is
critical to the safe deployment of reasoning models in real-world applications.
To this end, we explore uncertainty quantification of reasoning models in this
work. Specifically, we ask three fundamental questions: First, are reasoning
models well-calibrated? Second, does deeper reasoning improve model
calibration? Finally, inspired by humans' innate ability to double-check their
thought processes to verify the validity of their answers and their confidence,
we ask: can reasoning models improve their calibration by explicitly reasoning
about their chain-of-thought traces? We introduce introspective uncertainty
quantification (UQ) to explore this direction. In extensive evaluations on SOTA
reasoning models across a broad range of benchmarks, we find that reasoning
models: (i) are typically overconfident, with self-verbalized confidence
estimates often greater than 85% particularly for incorrect responses, (ii)
become even more overconfident with deeper reasoning, and (iii) can become
better calibrated through introspection (e.g., o3-Mini and DeepSeek R1) but not
uniformly (e.g., Claude 3.7 Sonnet becomes more poorly calibrated). Lastly, we
conclude with important research directions to design necessary UQ benchmarks
and improve the calibration of reasoning models.

</details>


### [35] [The Impact of Medication Non-adherence on Adverse Outcomes: Evidence from Schizophrenia Patients via Survival Analysis](https://arxiv.org/abs/2506.18187)
*Shahriar Noroozizadeh,Pim Welle,Jeremy C. Weiss,George H. Chen*

Main category: cs.AI

TL;DR: 研究量化精神分裂症患者抗精神病药不依从与不良后果关联，用生存分析和因果推断方法，发现不依从使不良后果提前1 - 4个月，强调依从性重要性。


<details>
  <summary>Details</summary>
Motivation: 量化精神分裂症患者抗精神病药不依从与不良后果之间的关联。

Method: 采用生存分析，聚焦多个不良事件最早发生时间；扩展标准因果推断方法，结合不同生存模型估计治疗效果；使用不同时长纵向信息重复分析。

Result: 发现不依从使不良后果提前约1 - 4个月；消融研究证实县提供的风险评分可调整关键混杂因素；亚组分析显示不依从与更早不良事件相关。

Conclusion: 强调依从性对延缓精神危机的临床重要性，表明生存分析与因果推断工具结合能产生政策相关见解，仅作关联性声明。

Abstract: This study quantifies the association between non-adherence to antipsychotic
medications and adverse outcomes in individuals with schizophrenia. We frame
the problem using survival analysis, focusing on the time to the earliest of
several adverse events (early death, involuntary hospitalization, jail
booking). We extend standard causal inference methods (T-learner, S-learner,
nearest neighbor matching) to utilize various survival models to estimate
individual and average treatment effects, where treatment corresponds to
medication non-adherence. Analyses are repeated using different amounts of
longitudinal information (3, 6, 9, and 12 months). Using data from Allegheny
County in western Pennsylvania, we find strong evidence that non-adherence
advances adverse outcomes by approximately 1 to 4 months. Ablation studies
confirm that county-provided risk scores adjust for key confounders, as their
removal amplifies the estimated effects. Subgroup analyses by medication
formulation (injectable vs. oral) and medication type consistently show that
non-adherence is associated with earlier adverse events. These findings
highlight the clinical importance of adherence in delaying psychiatric crises
and show that integrating survival analysis with causal inference tools can
yield policy-relevant insights. We caution that although we apply causal
inference, we only make associative claims and discuss assumptions needed for
causal interpretation.

</details>


### [36] [A Conceptual Framework for AI Capability Evaluations](https://arxiv.org/abs/2506.18213)
*María Victoria Carro,Denise Alejandra Mester,Francisca Gauna Selasco,Luca Nicolás Forziati Gangi,Matheo Sandleris Musa,Lola Ramos Pereyra,Mario Leiva,Juan Gustavo Corvalan,María Vanina Martinez,Gerardo Simari*

Main category: cs.AI

TL;DR: 随着AI系统发展，需全面可靠评估，本文提出分析AI能力评估的概念框架及作用。


<details>
  <summary>Details</summary>
Motivation: AI系统融入社会，缺乏全面可靠的评估方法。

Method: 提出概念框架，对广泛使用的方法和术语分析，不引入新分类或格式。

Result: 框架支持评估的透明度、可比性和可解释性。

Conclusion: 框架能帮助研究者发现方法弱点、协助从业者设计评估、为政策制定者提供审查工具。

Abstract: As AI systems advance and integrate into society, well-designed and
transparent evaluations are becoming essential tools in AI governance,
informing decisions by providing evidence about system capabilities and risks.
Yet there remains a lack of clarity on how to perform these assessments both
comprehensively and reliably. To address this gap, we propose a conceptual
framework for analyzing AI capability evaluations, offering a structured,
descriptive approach that systematizes the analysis of widely used methods and
terminology without imposing new taxonomies or rigid formats. This framework
supports transparency, comparability, and interpretability across diverse
evaluations. It also enables researchers to identify methodological weaknesses,
assists practitioners in designing evaluations, and provides policymakers with
an accessible tool to scrutinize, compare, and navigate complex evaluation
landscapes.

</details>


### [37] [The 4th Dimension for Scaling Model Size](https://arxiv.org/abs/2506.18233)
*Ruike Zhu,Hanwen Zhang,Tianyu Shi,Chi Wang,Tianyi Zhou,Zengyi Qin*

Main category: cs.AI

TL;DR: 本文探索了大语言模型扩展的第四维度虚拟逻辑深度（VLD），发现VLD缩放能在不改变参数数量的情况下提升推理能力，且参数数量与知识容量相关但与推理能力无关。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型扩展的新维度，探索参数复用在模型扩展中的潜力和特性。

Method: 通过精心设计的对照实验研究VLD缩放。

Result: VLD缩放使模型知识容量几乎不变，能显著提升推理能力，参数数量与知识容量相关但与推理能力无关。

Conclusion: 这些发现适用于各种模型配置，在实验范围内具有普遍有效性。

Abstract: Scaling the size of large language models typically involves three
dimensions: depth, width, and the number of parameters. In this work, we
explore a fourth dimension, virtual logical depth (VLD), which increases the
effective algorithmic depth without changing the overall parameter count by
reusing parameters within the model. Although parameter reuse is not a new
concept, its potential and characteristics in model scaling have not been
thoroughly studied. Through carefully designed controlled experiments, we make
the following key discoveries regarding VLD scaling:
  VLD scaling forces the knowledge capacity of the model to remain almost
constant, with only minor variations.
  VLD scaling enables a significant improvement in reasoning capability,
provided the scaling method is properly implemented.
  The number of parameters correlates with knowledge capacity, but not with
reasoning capability. Under certain conditions, it is not necessary to increase
the parameter count to enhance reasoning.
  These findings are consistent across various model configurations and are
likely to be generally valid within the scope of our experiments.

</details>


### [38] [Advanced For-Loop for QML algorithm search](https://arxiv.org/abs/2506.18260)
*FuTe Wong*

Main category: cs.AI

TL;DR: 本文提出基于大语言模型多智能体系统的框架用于量子机器学习算法自动搜索与优化，展示了其潜力并给出未来方向。


<details>
  <summary>Details</summary>
Motivation: 实现量子机器学习算法的自动化搜索与优化。

Method: 受Google DeepMind的FunSearch启发，在抽象层面迭代生成和优化经典机器学习算法的量子变换。

Result: 证明了智能体框架系统探索经典机器学习概念并将其应用于量子计算的潜力。

Conclusion: 该框架为量子机器学习算法的高效自动开发铺平道路，未来可结合规划机制和优化策略以拓展应用。

Abstract: This paper introduces an advanced framework leveraging Large Language
Model-based Multi-Agent Systems (LLMMA) for the automated search and
optimization of Quantum Machine Learning (QML) algorithms. Inspired by Google
DeepMind's FunSearch, the proposed system works on abstract level to
iteratively generates and refines quantum transformations of classical machine
learning algorithms (concepts), such as the Multi-Layer Perceptron,
forward-forward and backpropagation algorithms. As a proof of concept, this
work highlights the potential of agentic frameworks to systematically explore
classical machine learning concepts and adapt them for quantum computing,
paving the way for efficient and automated development of QML algorithms.
Future directions include incorporating planning mechanisms and optimizing
strategy in the search space for broader applications in quantum-enhanced
machine learning.

</details>


### [39] [Dynamic Knowledge Exchange and Dual-diversity Review: Concisely Unleashing the Potential of a Multi-Agent Research Team](https://arxiv.org/abs/2506.18348)
*Weilun Yu,Shixiang Tang,Yonggui Huang,Nanqing Dong,Li Fan,Honggang Qi,Wei Liu,Xiaoli Diao,Xi Chen,Wanli Ouyang*

Main category: cs.AI

TL;DR: 提出多智能体框架IDVSCI，通过两个创新机制促进科学思想生成，实验表明其性能优于现有系统，凸显模拟交互和同行评审动态的价值。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的科学家智能体缺乏交互式推理和评估机制，难以有效模拟现实科研协作。

Method: 提出IDVSCI框架，包含动态知识交换机制和双多样性评审范式，并在两个数据集上进行实验。

Result: IDVSCI在两个数据集上始终表现最佳，优于AI Scientist和VIRSCI等现有系统。

Conclusion: 在基于大语言模型的自主研究中，模拟交互和同行评审动态具有重要价值。

Abstract: Scientific progress increasingly relies on effective collaboration among
researchers, a dynamic that large language models (LLMs) have only begun to
emulate. While recent LLM-based scientist agents show promise in autonomous
scientific discovery, they often lack the interactive reasoning and evaluation
mechanisms essential to real-world research. We propose IDVSCI (Internal
Discussion and Vote SCIentists), a multi-agent framework built on LLMs that
incorporates two key innovations: a Dynamic Knowledge Exchange mechanism
enabling iterative feedback among agents, and a Dual-Diversity Review paradigm
that simulates heterogeneous expert evaluation. These components jointly
promote deeper reasoning and the generation of more creative and impactful
scientific ideas. To evaluate the effectiveness and generalizability of our
approach, we conduct experiments on two datasets: a widely used benchmark in
computer science and a new dataset we introduce in the health sciences domain.
Results show that IDVSCI consistently achieves the best performance across both
datasets, outperforming existing systems such as AI Scientist and VIRSCI. These
findings highlight the value of modeling interaction and peer review dynamics
in LLM-based autonomous research.

</details>


### [40] [A Large Language Model-based Multi-Agent Framework for Analog Circuits' Sizing Relationships Extraction](https://arxiv.org/abs/2506.18424)
*Chengjie Liu,Weiyu Chen,Huiyao Xu,Yuan Du,Jun Yang,Li Du*

Main category: cs.AI

TL;DR: 提出基于大语言模型的多智能体框架提取模拟电路尺寸关系，可有效修剪搜索空间，在3种电路测试中优化效率提升2.32 - 26.6倍。


<details>
  <summary>Details</summary>
Motivation: 现有模拟电路器件尺寸确定技术忽略自动引入先验知识，未能有效修剪搜索空间，有较大压缩空间。

Method: 提出基于大语言模型（LLM）的多智能体框架从学术论文中提取模拟电路尺寸关系。

Result: 在3种类型电路上测试，优化效率提升2.32 - 26.6倍。

Conclusion: LLM能有效修剪模拟电路尺寸搜索空间，为LLM与传统模拟电路设计自动化方法结合提供新方案。

Abstract: In the design process of the analog circuit pre-layout phase, device sizing
is an important step in determining whether an analog circuit can meet the
required performance metrics. Many existing techniques extract the circuit
sizing task as a mathematical optimization problem to solve and continuously
improve the optimization efficiency from a mathematical perspective. But they
ignore the automatic introduction of prior knowledge, fail to achieve effective
pruning of the search space, which thereby leads to a considerable compression
margin remaining in the search space. To alleviate this problem, we propose a
large language model (LLM)-based multi-agent framework for analog circuits'
sizing relationships extraction from academic papers. The search space in the
sizing process can be effectively pruned based on the sizing relationship
extracted by this framework. Eventually, we conducted tests on 3 types of
circuits, and the optimization efficiency was improved by $2.32 \sim 26.6
\times$. This work demonstrates that the LLM can effectively prune the search
space for analog circuit sizing, providing a new solution for the combination
of LLMs and conventional analog circuit design automation methods.

</details>


### [41] [How Robust is Model Editing after Fine-Tuning? An Empirical Study on Text-to-Image Diffusion Models](https://arxiv.org/abs/2506.18428)
*Feng He,Zhenyang Liu,Marco Valentino,Zhixue Zhao*

Main category: cs.AI

TL;DR: 研究T2I扩散模型中模型编辑与微调的相互作用，发现编辑通常难以在微调后保留，指出当前编辑方法的局限性及对AI安全的双重影响。


<details>
  <summary>Details</summary>
Motivation: 探究模型编辑在微调后是否持续有效或被逆转，因这对实际应用如防御恶意编辑、保障安全有重要意义。

Method: 在T2I扩散模型中，涵盖两个模型家族、两种编辑技术和三种微调方法，进行广泛实证分析。

Result: 编辑通常无法在微调后保留，DoRA逆转编辑效果最强，UCE比ReFACT在微调后更稳健。

Conclusion: 当前编辑方法有重要局限，需更稳健技术，微调可应对恶意编辑，但微调后需重新编辑以维持有益属性。

Abstract: Model editing offers a low-cost technique to inject or correct a particular
behavior in a pre-trained model without extensive retraining, supporting
applications such as factual correction and bias mitigation. Despite this
common practice, it remains unknown whether edits persist after fine-tuning or
whether they are inadvertently reversed. This question has fundamental
practical implications. For example, if fine-tuning removes prior edits, it
could serve as a defence mechanism against hidden malicious edits. Vice versa,
the unintended removal of edits related to bias mitigation could pose serious
safety concerns. We systematically investigate the interaction between model
editing and fine-tuning in the context of T2I diffusion models, which are known
to exhibit biases and generate inappropriate content. Our study spans two T2I
model families (Stable Diffusion and FLUX), two sota editing techniques, and
three fine-tuning methods (DreamBooth, LoRA, and DoRA). Through an extensive
empirical analysis across diverse editing tasks and evaluation metrics, our
findings reveal a trend: edits generally fail to persist through fine-tuning,
even when fine-tuning is tangential or unrelated to the edits. Notably, we
observe that DoRA exhibits the strongest edit reversal effect. At the same
time, among editing methods, UCE demonstrates greater robustness, retaining
significantly higher efficacy post-fine-tuning compared to ReFACT. These
findings highlight a crucial limitation in current editing methodologies,
emphasizing the need for more robust techniques to ensure reliable long-term
control and alignment of deployed AI systems. These findings have dual
implications for AI safety: they suggest that fine-tuning could serve as a
remediation mechanism for malicious edits while simultaneously highlighting the
need for re-editing after fine-tuning to maintain beneficial safety and
alignment properties.

</details>


### [42] [Standard Applicability Judgment and Cross-jurisdictional Reasoning: A RAG-based Framework for Medical Device Compliance](https://arxiv.org/abs/2506.18511)
*Yu Han,Aaron Ceross,Jeroen H. M. Bergmann*

Main category: cs.AI

TL;DR: 本文介绍模块化AI系统自动确定医疗设备监管标准适用性，构建数据集评估，证明其有效，可跨辖区推理。


<details>
  <summary>Details</summary>
Motivation: 解决医疗设备合规中确定监管标准适用性这一关键但研究不足的挑战，避免依赖专家解读不同辖区零散异构文档。

Method: 引入利用检索增强生成（RAG）管道的模块化AI系统，根据设备自由文本描述从语料库检索候选标准，用大语言模型推断辖区特定适用性；构建含专家标注映射的国际基准数据集，与多种基线方法对比评估。

Result: 系统分类准确率达73%，Top - 5检索召回率达87%。

Conclusion: 提出首个端到端标准适用性推理系统，实现可扩展且可解释的AI支持的监管科学，区域感知RAG代理可进行中美标准跨辖区推理。

Abstract: Identifying the appropriate regulatory standard applicability remains a
critical yet understudied challenge in medical device compliance, frequently
necessitating expert interpretation of fragmented and heterogeneous
documentation across different jurisdictions. To address this challenge, we
introduce a modular AI system that leverages a retrieval-augmented generation
(RAG) pipeline to automate standard applicability determination. Given a
free-text device description, our system retrieves candidate standards from a
curated corpus and uses large language models to infer jurisdiction-specific
applicability, classified as Mandatory, Recommended, or Not Applicable, with
traceable justifications. We construct an international benchmark dataset of
medical device descriptions with expert-annotated standard mappings, and
evaluate our system against retrieval-only, zero-shot, and rule-based
baselines. The proposed approach attains a classification accuracy of 73% and a
Top-5 retrieval recall of 87%, demonstrating its effectiveness in identifying
relevant regulatory standards. We introduce the first end-to-end system for
standard applicability reasoning, enabling scalable and interpretable
AI-supported regulatory science. Notably, our region-aware RAG agent performs
cross-jurisdictional reasoning between Chinese and U.S. standards, supporting
conflict resolution and applicability justification across regulatory
frameworks.

</details>


### [43] [A Question Bank to Assess AI Inclusivity: Mapping out the Journey from Diversity Errors to Inclusion Excellence](https://arxiv.org/abs/2506.18538)
*Rifat Ara Shams,Didar Zowghi,Muneera Bano*

Main category: cs.AI

TL;DR: 文章引入结构化AI包容性问题库评估AI包容性，经模拟评估验证其作用，强调将D&I原则融入AI开发和治理。


<details>
  <summary>Details</summary>
Motivation: 现有AI风险评估框架常忽视包容性，缺乏衡量AI系统与D&I原则一致性的标准化工具。

Method: 采用迭代、多源方法开发含253个问题的问题库，通过对70个AI生成角色进行模拟评估。

Result: 问题库能用于评估不同角色和应用领域AI的包容性。

Conclusion: 应将D&I原则融入AI开发和治理，问题库为评估和提升AI系统包容性提供工具。

Abstract: Ensuring diversity and inclusion (D&I) in artificial intelligence (AI) is
crucial for mitigating biases and promoting equitable decision-making. However,
existing AI risk assessment frameworks often overlook inclusivity, lacking
standardized tools to measure an AI system's alignment with D&I principles.
This paper introduces a structured AI inclusivity question bank, a
comprehensive set of 253 questions designed to evaluate AI inclusivity across
five pillars: Humans, Data, Process, System, and Governance. The development of
the question bank involved an iterative, multi-source approach, incorporating
insights from literature reviews, D&I guidelines, Responsible AI frameworks,
and a simulated user study. The simulated evaluation, conducted with 70
AI-generated personas related to different AI jobs, assessed the question
bank's relevance and effectiveness for AI inclusivity across diverse roles and
application domains. The findings highlight the importance of integrating D&I
principles into AI development workflows and governance structures. The
question bank provides an actionable tool for researchers, practitioners, and
policymakers to systematically assess and enhance the inclusivity of AI
systems, paving the way for more equitable and responsible AI technologies.

</details>


### [44] [T-CPDL: A Temporal Causal Probabilistic Description Logic for Developing Logic-RAG Agent](https://arxiv.org/abs/2506.18559)
*Hong Qing Yu*

Main category: cs.AI

TL;DR: 提出T - CPDL框架解决大语言模型结构化推理局限，实证表明其提升推理效果并为Logic - RAG框架奠基。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在涉及时间约束、因果关系和概率推理的结构化推理方面存在局限。

Method: 提出T - CPDL框架，有两种变体，共享统一逻辑结构以支持复杂推理任务。

Result: 在时间推理和因果推断基准测试中，T - CPDL显著提高语言模型输出的推理准确性、可解释性和置信度校准。

Conclusion: T - CPDL增强语言模型决策能力，为Logic - RAG框架发展奠定基础。

Abstract: Large language models excel at generating fluent text but frequently struggle
with structured reasoning involving temporal constraints, causal relationships,
and probabilistic reasoning. To address these limitations, we propose Temporal
Causal Probabilistic Description Logic (T-CPDL), an integrated framework that
extends traditional Description Logic with temporal interval operators,
explicit causal relationships, and probabilistic annotations. We present two
distinct variants of T-CPDL: one capturing qualitative temporal relationships
through Allen's interval algebra, and another variant enriched with explicit
timestamped causal assertions. Both variants share a unified logical structure,
enabling complex reasoning tasks ranging from simple temporal ordering to
nuanced probabilistic causation. Empirical evaluations on temporal reasoning
and causal inference benchmarks confirm that T-CPDL substantially improves
inference accuracy, interpretability, and confidence calibration of language
model outputs. By delivering transparent reasoning paths and fine-grained
temporal and causal semantics, T-CPDL significantly enhances the capability of
language models to support robust, explainable, and trustworthy
decision-making. This work also lays the groundwork for developing advanced
Logic-Retrieval-Augmented Generation (Logic-RAG) frameworks, potentially
boosting the reasoning capabilities and efficiency of knowledge graph-enhanced
RAG systems.

</details>


### [45] [AggTruth: Contextual Hallucination Detection using Aggregated Attention Scores in LLMs](https://arxiv.org/abs/2506.18628)
*Piotr Matys,Jan Eliasz,Konrad Kiełczyński,Mikołaj Langner,Teddy Ferdinan,Jan Kocoń,Przemysław Kazienko*

Main category: cs.AI

TL;DR: 本文介绍了用于在线检测上下文幻觉的AggTruth方法，该方法分析内部注意力分数分布，有四种变体，在多场景表现优于SOTA，还分析了特征选择技术和注意力头数量对检测性能的影响。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，大语言模型（LLMs）常出现幻觉，即使在检索增强生成（RAG）设置中也是如此，这对其部署构成重大挑战，因此需要一种检测上下文幻觉的方法。

Method: 引入AggTruth方法，通过分析提供上下文中内部注意力分数的分布来在线检测上下文幻觉，提出该方法的四种不同变体，每种变体的聚合技术不同。

Result: 在所有检查的大语言模型中，AggTruth在同任务和跨任务设置中都表现稳定，在多个场景中优于当前的最先进技术（SOTA）。

Conclusion: 仔细选择注意力头对于实现最佳检测结果至关重要。

Abstract: In real-world applications, Large Language Models (LLMs) often hallucinate,
even in Retrieval-Augmented Generation (RAG) settings, which poses a
significant challenge to their deployment. In this paper, we introduce
AggTruth, a method for online detection of contextual hallucinations by
analyzing the distribution of internal attention scores in the provided context
(passage). Specifically, we propose four different variants of the method, each
varying in the aggregation technique used to calculate attention scores. Across
all LLMs examined, AggTruth demonstrated stable performance in both same-task
and cross-task setups, outperforming the current SOTA in multiple scenarios.
Furthermore, we conducted an in-depth analysis of feature selection techniques
and examined how the number of selected attention heads impacts detection
performance, demonstrating that careful selection of heads is essential to
achieve optimal results.

</details>


### [46] [Dual-level Behavioral Consistency for Inter-group and Intra-group Coordination in Multi-Agent Systems](https://arxiv.org/abs/2506.18651)
*Shuocun Yang,Huawen Hu,Enze Shi,Shu Zhang*

Main category: cs.AI

TL;DR: 提出DLBC方法调节多智能体组内和组间行为多样性，实验证明其提升性能，为行为一致性控制提供新思路。


<details>
  <summary>Details</summary>
Motivation: 过往多智能体系统研究多关注组内行为一致性，对多智能体分组场景下的行为一致性关注有限。

Method: 引入Dual - Level Behavioral Consistency (DLBC)方法，将智能体分组，动态调节组内和组间行为多样性，直接约束智能体策略函数。

Result: 在各种分组合作场景实验中，DLBC显著提升组内合作性能和组间任务专业化程度，取得显著性能提升。

Conclusion: DLBC为多智能体系统行为一致性控制提供新想法，未来可探索其在更复杂任务和动态环境中的应用。

Abstract: Behavioral diversity in Multi-agent reinforcement learning(MARL) represents
an emerging and promising research area. Prior work has largely centered on
intra-group behavioral consistency in multi-agent systems, with limited
attention given to behavioral consistency in multi-agent grouping scenarios. In
this paper, we introduce Dual-Level Behavioral Consistency (DLBC), a novel MARL
control method designed to explicitly regulate agent behaviors at both
intra-group and inter-group levels. DLBC partitions agents into distinct groups
and dynamically modulates behavioral diversity both within and between these
groups. By dynamically modulating behavioral diversity within and between these
groups, DLBC achieves enhanced division of labor through inter-group
consistency, which constrains behavioral strategies across different groups.
Simultaneously, intra-group consistency, achieved by aligning behavioral
strategies within each group, fosters stronger intra-group cooperation.
Crucially, DLBC's direct constraint of agent policy functions ensures its broad
applicability across various algorithmic frameworks. Experimental results in
various grouping cooperation scenarios demonstrate that DLBC significantly
enhances both intra-group cooperative performance and inter-group task
specialization, yielding substantial performance improvements. DLBC provides
new ideas for behavioral consistency control of multi-intelligent body systems,
and its potential for application in more complex tasks and dynamic
environments can be further explored in the future.

</details>


### [47] [Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions During Code Training](https://arxiv.org/abs/2506.18777)
*Jonathan Cook,Silvia Sapora,Arash Ahmadian,Akbir Khan,Tim Rocktaschel,Jakob Foerster,Laura Ruis*

Main category: cs.AI

TL;DR: 本文提出编程反向传播（PBB）解释代码训练提升大语言模型推理能力的机制，通过实验发现PBB有效，表明代码训练能让模型内化算法抽象。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型在源代码上训练提升通用推理能力的机制。

Method: 在代表简单数学问题和算法的两组程序上微调大语言模型，一组有源代码和输入输出示例，另一组只有源代码。

Result: 大语言模型在多种实验设置下有能力评估无输入输出示例的程序，PBB在代码形式下效果更好，可隐式评估程序，通过思维链更可靠，且比基于输入输出对训练更稳健。

Conclusion: 代码训练提升推理能力的机制是使大语言模型内化可复用的算法抽象，未来有提升模型从符号程序学习的空间。

Abstract: Training large language models (LLMs) on source code significantly enhances
their general-purpose reasoning abilities, but the mechanisms underlying this
generalisation are poorly understood. In this paper, we propose Programming by
Backprop (PBB) as a potential driver of this effect - teaching a model to
evaluate a program for inputs by training on its source code alone, without
ever seeing I/O examples. To explore this idea, we finetune LLMs on two sets of
programs representing simple maths problems and algorithms: one with source
code and I/O examples (w/ IO), the other with source code only (w/o IO). We
find evidence that LLMs have some ability to evaluate w/o IO programs for
inputs in a range of experimental settings, and make several observations.
Firstly, PBB works significantly better when programs are provided as code
rather than semantically equivalent language descriptions. Secondly, LLMs can
produce outputs for w/o IO programs directly, by implicitly evaluating the
program within the forward pass, and more reliably when stepping through the
program in-context via chain-of-thought. We further show that PBB leads to more
robust evaluation of programs across inputs than training on I/O pairs drawn
from a distribution that mirrors naturally occurring data. Our findings suggest
a mechanism for enhanced reasoning through code training: it allows LLMs to
internalise reusable algorithmic abstractions. Significant scope remains for
future work to enable LLMs to more effectively learn from symbolic procedures,
and progress in this direction opens other avenues like model alignment by
training on formal constitutional principles.

</details>


### [48] [TRIZ Agents: A Multi-Agent LLM Approach for TRIZ-Based Innovation](https://arxiv.org/abs/2506.18783)
*Kamil Szczepanik,Jarosław A. Chudziak*

Main category: cs.AI

TL;DR: 本文提出基于LLM的多智能体系统TRIZ agents解决创新问题，通过案例证明其协作产生多样创新方案的潜力。


<details>
  <summary>Details</summary>
Motivation: TRIZ应用受复杂性和跨学科知识限制，此前研究用单LLM，本文引入多智能体方法。

Method: 提出LLM-based多智能体系统TRIZ agents，各智能体有专业能力和工具访问权限，基于TRIZ方法协作解决问题。

Result: 通过工程案例研究，证明智能体协作能产生多样、创新的解决方案。

Conclusion: 该研究为AI驱动的创新做出贡献，展示了分散式解决复杂创意任务的优势。

Abstract: TRIZ, the Theory of Inventive Problem Solving, is a structured,
knowledge-based framework for innovation and abstracting problems to find
inventive solutions. However, its application is often limited by the
complexity and deep interdisciplinary knowledge required. Advancements in Large
Language Models (LLMs) have revealed new possibilities for automating parts of
this process. While previous studies have explored single LLMs in TRIZ
applications, this paper introduces a multi-agent approach. We propose an
LLM-based multi-agent system, called TRIZ agents, each with specialized
capabilities and tool access, collaboratively solving inventive problems based
on the TRIZ methodology. This multi-agent system leverages agents with various
domain expertise to efficiently navigate TRIZ steps. The aim is to model and
simulate an inventive process with language agents. We assess the effectiveness
of this team of agents in addressing complex innovation challenges based on a
selected case study in engineering. We demonstrate the potential of agent
collaboration to produce diverse, inventive solutions. This research
contributes to the future of AI-driven innovation, showcasing the advantages of
decentralized problem-solving in complex ideation tasks.

</details>


### [49] [ConciseHint: Boosting Efficient Reasoning via Continuous Concise Hints during Generation](https://arxiv.org/abs/2506.18810)
*Siao Tang,Xinyin Ma,Gongfan Fang,Xinchao Wang*

Main category: cs.AI

TL;DR: 现有大推理模型推理冗长，提出ConciseHint框架，能在保证性能的同时生成简洁推理过程，实验效果良好。


<details>
  <summary>Details</summary>
Motivation: 大推理模型推理过程冗长导致效率问题，现有改进效率的研究忽略推理生成时直接促使模型简洁表达这一方向。

Method: 提出ConciseHint框架，在推理过程的token生成中注入文本提示，且能自适应调整提示强度。

Result: 在DeepSeek - R1和Qwen - 3系列等模型上实验，能有效生成简洁推理过程，如在GSM8K基准上用Qwen - 3 4B使推理长度减少65%且几乎不损失准确率。

Conclusion: ConciseHint框架能有效解决大推理模型推理冗长问题，在保证性能的同时提升推理简洁性。

Abstract: Recent advancements in large reasoning models (LRMs) like DeepSeek-R1 and
OpenAI o1 series have achieved notable performance enhancements on complex
reasoning tasks by scaling up the generation length by Chain-of-Thought (CoT).
However, an emerging issue is their inclination to produce excessively verbose
reasoning processes, leading to the inefficiency problem. Existing literature
on improving efficiency mainly adheres to the before-reasoning paradigms such
as prompting and reasoning or fine-tuning and reasoning, but ignores the
promising direction of directly encouraging the model to speak concisely by
intervening during the generation of reasoning. In order to fill the blank, we
propose a framework dubbed ConciseHint, which continuously encourages the
reasoning model to speak concisely by injecting the textual hint (manually
designed or trained on the concise data) during the token generation of the
reasoning process. Besides, ConciseHint is adaptive to the complexity of the
query by adaptively adjusting the hint intensity, which ensures it will not
undermine model performance. Experiments on the state-of-the-art LRMs,
including DeepSeek-R1 and Qwen-3 series, demonstrate that our method can
effectively produce concise reasoning processes while maintaining performance
well. For instance, we achieve a reduction ratio of 65\% for the reasoning
length on GSM8K benchmark with Qwen-3 4B with nearly no accuracy loss.

</details>


### [50] [Steering Conceptual Bias via Transformer Latent-Subspace Activation](https://arxiv.org/abs/2506.18887)
*Vansh Sharma,Venkat Raman*

Main category: cs.AI

TL;DR: 研究激活大语言模型潜在子空间能否引导科学代码生成至特定编程语言，提出G - ACT框架并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 探究激活大语言模型潜在子空间引导科学代码生成至特定编程语言的可行性。

Method: 先评估五种因果大语言模型在科学编码提示下对四种编程语言的基线偏差；静态神经元归因方法效果不佳，进而开发G - ACT框架，包括将每提示激活差异聚类成引导方向，训练和在线优化轻量级每层探针。

Result: 在LLaMA - 3.2 3B中，相比标准ACT框架平均探针分类准确率提高15%，早期层提高61.5%；在LLaMA - 3.3 70B中，关键层的定向注入仍能改善语言选择。

Conclusion: 该方法为实际代理系统的概念级控制提供了可扩展、可解释且高效的机制。

Abstract: This work examines whether activating latent subspaces in language models
(LLMs) can steer scientific code generation toward a specific programming
language. Five causal LLMs were first evaluated on scientific coding prompts to
quantify their baseline bias among four programming languages. A static
neuron-attribution method, perturbing the highest activated MLP weight for a
C++ or CPP token, proved brittle and exhibited limited generalization across
prompt styles and model scales. To address these limitations, a
gradient-refined adaptive activation steering framework (G-ACT) was developed:
per-prompt activation differences are clustered into a small set of steering
directions, and lightweight per-layer probes are trained and refined online to
select the appropriate steering vector. In LLaMA-3.2 3B, this approach reliably
biases generation towards the CPP language by increasing the average probe
classification accuracy by 15% and the early layers (0-6) improving the probe
classification accuracy by 61.5% compared to the standard ACT framework. For
LLaMA-3.3 70B, where attention-head signals become more diffuse, targeted
injections at key layers still improve language selection. Although per-layer
probing introduces a modest inference overhead, it remains practical by
steering only a subset of layers and enables reproducible model behavior. These
results demonstrate a scalable, interpretable and efficient mechanism for
concept-level control for practical agentic systems.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [51] [Bridging Equilibrium and Kinetics Prediction with a Data-Weighted Neural Network Model of Methane Steam Reforming](https://arxiv.org/abs/2506.17224)
*Zofia Pizoń,Shinji Kimijima,Grzegorz Brus*

Main category: cs.CE

TL;DR: 提出能统一动力学和平衡态机制的替代模型，用于模拟甲烷蒸汽重整，经训练和评估后表现出高预测精度，对设计和过程优化有价值。


<details>
  <summary>Details</summary>
Motivation: 氢气作为能源载体需求增加，需高效生产，现有模型在动力学和平衡态机制适用性有限，需统一两者的模型。

Method: 用包含实验、插值和理论数据的综合数据集训练人工神经网络，采用数据增强和分配权重提升训练，评估贝叶斯优化和随机抽样。

Result: 最优模型预测精度高，均方误差0.000498，皮尔逊相关系数0.927，能提供连续导数。

Conclusion: 替代模型在模拟甲烷蒸汽重整的两种机制上具有鲁棒性，是设计和过程优化的有价值工具。

Abstract: Hydrogen's role is growing as an energy carrier, increasing the need for
efficient production, with methane steam reforming being the most widely used
technique. This process is crucial for applications like fuel cells, where
hydrogen is converted into electricity, pushing for reactor miniaturization and
optimized process control through numerical simulations. Existing models
typically address either kinetic or equilibrium regimes, limiting their
applicability. Here we show a surrogate model capable of unifying both regimes.
An artificial neural network trained on a comprehensive dataset that includes
experimental data from kinetic and equilibrium experiments, interpolated data,
and theoretical data derived from theoretical models for each regime. Data
augmentation and assigning appropriate weights to each data type enhanced
training. After evaluating Bayesian Optimization and Random Sampling, the
optimal model demonstrated high predictive accuracy for the composition of the
post-reaction mixture under varying operating parameters, indicated by a mean
squared error of 0.000498 and strong Pearson correlation coefficients of 0.927.
The network's ability to provide continuous derivatives of its predictions
makes it particularly useful for process modeling and optimization. The results
confirm the surrogate model's robustness for simulating methane steam reforming
in both kinetic and equilibrium regimes, making it a valuable tool for design
and process optimization.

</details>


### [52] [Variational Quantum Latent Encoding for Topology Optimization](https://arxiv.org/abs/2506.17487)
*Alireza Tabarraei*

Main category: cs.CE

TL;DR: 本文提出一种结构拓扑优化的变分框架，结合量子与经典潜在编码策略，实验表明量子编码在部分基准测试中有优势，凸显量子电路用于拓扑优化的潜力。


<details>
  <summary>Details</summary>
Motivation: 开发新的结构拓扑优化方法，探索量子电路在该领域的应用，以克服传统方法仅产生单一确定解的局限。

Method: 将量子或经典方式生成的低维潜在向量通过可学习投影层映射到高维空间，再用神经网络解码为高分辨率材料分布，直接对潜在参数进行优化，基于物理目标指导优化。

Result: 经典和量子编码都能产生高质量结构设计，量子编码在部分基准测试中，在柔度和设计多样性方面表现更优。

Conclusion: 量子电路有望成为受物理约束的拓扑优化的有效且可扩展工具，为近期量子硬件在结构设计中的应用指明方向。

Abstract: A variational framework for structural topology optimization is developed,
integrating quantum and classical latent encoding strategies within a
coordinate-based neural decoding architecture. In this approach, a
low-dimensional latent vector, generated either by a variational quantum
circuit or sampled from a Gaussian distribution, is mapped to a
higher-dimensional latent space via a learnable projection layer. This enriched
representation is then decoded into a high-resolution material distribution
using a neural network that takes both the latent vector and Fourier-mapped
spatial coordinates as input. The optimization is performed directly on the
latent parameters, guided solely by physics-based objectives such as compliance
minimization and volume constraints evaluated through finite element analysis,
without requiring any precomputed datasets or supervised training. Quantum
latent vectors are constructed from the expectation values of Pauli observables
measured on parameterized quantum circuits, providing a structured and
entangled encoding of information. The classical baseline uses Gaussian-sampled
latent vectors projected in the same manner. The proposed variational
formulation enables the generation of diverse and physically valid topologies
by exploring the latent space through sampling or perturbation, in contrast to
traditional optimization methods that yield a single deterministic solution.
Numerical experiments show that both classical and quantum encodings produce
high-quality structural designs. However, quantum encodings demonstrate
advantages in several benchmark cases in terms of compliance and design
diversity. These results highlight the potential of quantum circuits as an
effective and scalable tool for physics-constrained topology optimization and
suggest promising directions for applying near-term quantum hardware in
structural design.

</details>


### [53] [A predictor-corrector scheme for approximating signed distances using finite element methods](https://arxiv.org/abs/2506.17830)
*Amina El Bachari,Johann Rannou,Vladislav A. Yastrebov,Pierre Kerfriden,Susanne Claus*

Main category: cs.CE

TL;DR: 介绍一种用于二维和三维任意边界近似有符号距离函数鲁棒计算的有限元方法，通过实例验证其性能。


<details>
  <summary>Details</summary>
Motivation: 现有技术在处理复杂界面和任意陡峭或平坦区域的初始水平集函数存在挑战，需要一种更有效的方法。

Method: 采用新颖的预测 - 校正方法，先求解基于线性扩散的预测问题，再求解与 Eikonal 方程相关的基于非线性最小化的校正问题。

Result: 通过多个代表性例子，包括经典几何形状和复杂形状，证明了方法的准确性、效率和鲁棒性。

Conclusion: 该方法具有广泛适用性，可用于重新初始化各种水平集函数。

Abstract: In this article, we introduce a finite element method designed for the robust
computation of approximate signed distance functions to arbitrary boundaries in
two and three dimensions. Our method employs a novel prediction-correction
approach, involving first the solution of a linear diffusion-based prediction
problem, followed by a nonlinear minimization-based correction problem
associated with the Eikonal equation. The prediction step efficiently generates
a suitable initial guess, significantly facilitating convergence of the
nonlinear correction step. A key strength of our approach is its ability to
handle complex interfaces and initial level set functions with arbitrary steep
or flat regions, a notable challenge for existing techniques. Through several
representative examples, including classical geometries and more complex shapes
such as star domains and three-dimensional tori, we demonstrate the accuracy,
efficiency, and robustness of the method, validating its broad applicability
for reinitializing diverse level set functions.

</details>


### [54] [Learning from the Storm: A Multivariate Machine Learning Approach to Predicting Hurricane-Induced Economic Losses](https://arxiv.org/abs/2506.17964)
*Bolin Shen,Eren Erman Ozguven,Yue Zhao,Guang Wang,Yiqun Xie,Yushun Dong*

Main category: cs.CE

TL;DR: 本文提出综合建模框架预测佛罗里达飓风经济损失，评估各因素重要性并提供减灾等策略，代码开源。


<details>
  <summary>Details</summary>
Motivation: 以往研究缺乏统一框架综合评估飓风经济损失影响因素，而佛罗里达易受飓风影响且损失大。

Method: 提出综合建模框架，将影响因素分为三类，整合多源数据，在ZCTA级别聚合变量，用机器学习模型，以保险索赔为损失指标。

Result: 能准确预测损失，可系统评估各因素相对重要性。

Conclusion: 该方法为沿海和风暴影响地区的减灾、风险评估和城市策略制定提供实用指导。

Abstract: Florida is particularly vulnerable to hurricanes, which frequently cause
substantial economic losses. While prior studies have explored specific
contributors to hurricane-induced damage, few have developed a unified
framework capable of integrating a broader range of influencing factors to
comprehensively assess the sources of economic loss. In this study, we propose
a comprehensive modeling framework that categorizes contributing factors into
three key components: (1) hurricane characteristics, (2) water-related
environmental factors, and (3) socioeconomic factors of affected areas. By
integrating multi-source data and aggregating all variables at the finer
spatial granularity of the ZIP Code Tabulation Area (ZCTA) level, we employ
machine learning models to predict economic loss, using insurance claims as
indicators of incurred damage. Beyond accurate loss prediction, our approach
facilitates a systematic assessment of the relative importance of each
component, providing practical guidance for disaster mitigation, risk
assessment, and the development of adaptive urban strategies in coastal and
storm-exposed areas. Our code is now available at:
https://github.com/LabRAI/Hurricane-Induced-Economic-Loss-Prediction

</details>


### [55] [A phase field model for hydraulic fracture: Drucker-Prager driving force and a hybrid coupling strategy](https://arxiv.org/abs/2506.18161)
*Y. Navidtehrani,C. Betegón,J. Vallejos,E. Martínez-Pañeda*

Main category: cs.CE

TL;DR: 提出新的相场框架模拟水力压裂，有新耦合方法和应变能分解，通过案例展示能力并开源代码


<details>
  <summary>Details</summary>
Motivation: 近年来对用相场方法模拟水力压裂兴趣渐浓，为优化在石油工程、采矿和地热能开采等行业关键的水力压裂过程

Method: 提出通用且多功能的相场框架，有新的混合耦合方法处理裂缝 - 流体流动相互作用，采用基于Drucker - Prager的应变能分解

Result: 通过四个案例研究展示了额外的建模能力，深入了解水力压裂模拟中的渗透率耦合、开裂行为和多轴条件

Conclusion: 该相场框架能带来创新，扩展了水力压裂模拟范围，代码开源可供社区使用

Abstract: Recent years have seen a significant interest in using phase field approaches
to model hydraulic fracture, so as to optimise a process that is key to
industries such as petroleum engineering, mining and geothermal energy
extraction. Here, we present a novel theoretical and computational phase field
framework to simulate hydraulic fracture. The framework is general and
versatile, in that it allows for improved treatments of the coupling between
fluid flow and the phase field, and encompasses a universal description of the
fracture driving force. Among others, this allows us to bring two innovations
to the phase field hydraulic fracture community: (i) a new hybrid coupling
approach to handle the fracture-fluid flow interplay, offering enhanced
accuracy and flexibility; and (ii) a Drucker-Prager-based strain energy
decomposition, extending the simulation of hydraulic fracture to materials
exhibiting asymmetric tension-compression fracture behaviour (such as shale
rocks) and enabling the prediction of geomechanical phenomena such as fault
reactivation and stick-slip behaviour. Four case studies are addressed to
illustrate these additional modelling capabilities and bring insight into
permeability coupling, cracking behaviour, and multiaxial conditions in
hydraulic fracturing simulations. The codes developed are made freely available
to the community and can be downloaded from {https://mechmat.web.ox.ac.uk/

</details>


### [56] [Measuring Fractal Dimension using Discrete Global Grid Systems](https://arxiv.org/abs/2506.18175)
*Pramit Ghosh*

Main category: cs.CE

TL;DR: 本文建立分形维度与离散全球网格系统（DGGS）联系，用DGGS计算分形维度，结果准确，解决相关问题并理论验证DGGS有效性。


<details>
  <summary>Details</summary>
Motivation: 建立分形维度和离散全球网格系统（DGGS）这两个研究充分但相距较远的主题之间的联系。

Method: 使用DGGS作为地理空间矢量数据的覆盖集来计算Minkowski - Bouligand维度。

Result: 对合成数据计算结果与理论分形维度误差在1%以内，对卫星图像中不透明云场的案例研究结果与文献一致。

Conclusion: 所提出的方法解决了任意网格放置和方向以及地理空间数据覆盖集单元格大小变化的问题，理论上确立了DGGS作为覆盖集的有效性，并讨论了适用于此目的的DGGS的理想属性。

Abstract: This study builds a bridge between two well-studied but distant topics:
fractal dimension and Discrete Global Grid System (DGGS). DGGSs are used as
covering sets for geospatial vector data to calculate the Minkowski-Bouligand
dimension. Using the method on synthetic data yields results within 1% of their
theoretical fractal dimensions. A case study on opaque cloud fields obtained
from satellite images gives fractal dimension in agreement with that available
in the literature. The proposed method alleviates the problems of arbitrary
grid placement and orientation, as well as the progression of cell sizes of the
covering sets for geospatial data. Using DGGSs further ensure that
intersections of the covering sets with the geospatial vector having large
geographic extents are calculated by taking the curvature of the earth into
account. This paper establishes the validity of DGGSs as covering sets
theoretically and discusses desirable properties of DGGSs suitable for this
purpose.

</details>


### [57] [Conservative data-driven finite element formulation](https://arxiv.org/abs/2506.18206)
*Adriana Kuliková,Andrei G. Shvarts,Łukasz Kaczmarczyk,Chris J. Pearce*

Main category: cs.CE

TL;DR: 本文提出基于混合有限元公式的数据驱动有限元框架，用于扩散问题，可避免材料模型偏差，有后验误差指标和自适应细化功能，通过核石墨热传导示例验证。


<details>
  <summary>Details</summary>
Motivation: 传统扩散问题求解需拟合材料模型，为利用所有可用信息并避免材料模型偏差，采用数据驱动方法。

Method: 采用混合有限元公式，用有限元法满足守恒定律和边界条件，直接在数值模拟中使用实验数据，引入后验误差指标实现自适应 hp 细化。

Result: 在核石墨非线性热传导示例中展示了该公式的能力，且能观察数据集变化导致的解的非唯一性并量化结果不确定性。

Conclusion: 提出的数据驱动有限元框架有效，能避免材料模型偏差，可用于解决扩散问题，且具有误差估计和不确定性量化功能。

Abstract: This paper presents a new data-driven finite element framework derived with
mixed finite element formulation. The standard approach to diffusion problems
requires the solution of the mathematical equations that describe both the
conservation law and the constitutive relations, where the latter is
traditionally obtained after fitting experimental data to simplified material
models. To exploit all available information and avoid bias in the material
model, we follow a data-driven approach. While the conservation laws and
boundary conditions are satisfied by means of the finite element method, the
experimental data is used directly in the numerical simulations, avoiding the
need of fitting material model parameters. In order to satisfy the conservation
law a priori in the strong sense, we introduce a mixed finite element
formulation. This relaxes the regularity requirements on approximation spaces
while enforcing continuity of the normal flux component across all of the inner
boundaries. This weaker mixed formulation provides a posteriori error
indicators tailored for this data-driven approach, enabling adaptive
hp-refinement. The relaxed regularity of the approximation spaces makes it
easier to observe how the variation in the datasets results in the
non-uniqueness of the solution, which can be quantified to predict the
uncertainty of the results. The capabilities of the formulation are
demonstrated in an example of the nonlinear heat transfer in nuclear graphite
using synthetically generated material datasets.

</details>


### [58] [Exact Conditional Score-Guided Generative Modeling for Amortized Inference in Uncertainty Quantification](https://arxiv.org/abs/2506.18227)
*Zezhong Zhang,Caroline Tatsuoka,Dongbin Xiu,Guannan Zhang*

Main category: cs.CE

TL;DR: 提出一种用于摊销条件推理的高效框架，结合归一化流和扩散模型优势，经两阶段训练神经网络，能快速准确地进行高维多模态后验分布的条件采样。


<details>
  <summary>Details</summary>
Motivation: 传统归一化流方法有可逆架构限制，扩散模型推理计算成本高，为结合两者优势而提出新框架。

Method: 两阶段方法：一是解析推导精确条件得分函数构建免训练条件扩散模型；二是用噪声标记数据训练前馈神经网络以实现直接映射。

Result: 模型能为高维多模态后验分布提供快速、准确和可扩展的条件采样。

Conclusion: 该方法适用于不确定性量化任务，如复杂物理系统的参数估计，数值实验证明了其有效性。

Abstract: We propose an efficient framework for amortized conditional inference by
leveraging exact conditional score-guided diffusion models to train a
non-reversible neural network as a conditional generative model. Traditional
normalizing flow methods require reversible architectures, which can limit
their expressiveness and efficiency. Although diffusion models offer greater
flexibility, they often suffer from high computational costs during inference.
To combine the strengths of both approaches, we introduce a two-stage method.
First, we construct a training-free conditional diffusion model by analytically
deriving an exact score function under a Gaussian mixture prior formed from
samples of the underlying joint distribution. This exact conditional score
model allows us to efficiently generate noise-labeled data, consisting of
initial diffusion Gaussian noise and posterior samples conditioned on various
observation values, by solving a reverse-time ordinary differential equation.
Second, we use this noise-labeled data to train a feedforward neural network
that maps noise and observations directly to posterior samples, eliminating the
need for reversibility or iterative sampling at inference time. The resulting
model provides fast, accurate, and scalable conditional sampling for
high-dimensional and multi-modal posterior distributions, making it well-suited
for uncertainty quantification tasks, e.g., parameter estimation of complex
physical systems. We demonstrate the effectiveness of our approach through a
series of numerical experiments.

</details>


### [59] [Neural-operator element method: Efficient and scalable finite element method enabled by reusable neural operators](https://arxiv.org/abs/2506.18427)
*Weihang Ouyang,Yeonjong Shin,Si-Wei Liu,Lu Lu*

Main category: cs.CE

TL;DR: 提出结合有限元法和算子学习的神经算子单元法（NOEM），能解决现有方法的问题，实验证明其准确、高效、可扩展。


<details>
  <summary>Details</summary>
Motivation: 有限元法在复杂多尺度模拟中计算成本高，机器学习方法存在训练成本高和模型复用性低的问题，需新方法解决。

Method: 用神经算子模拟子域，构建神经算子单元，再与标准有限元通过变分框架集成表示整体解。

Result: 通过大量系统数值实验，包括非线性偏微分方程、多尺度问题等，证明了NOEM的准确性、效率和可扩展性。

Conclusion: NOEM无需密集网格划分，能提供高效模拟，是解决偏微分方程模拟问题的有效方法。

Abstract: The finite element method (FEM) is a well-established numerical method for
solving partial differential equations (PDEs). However, its mesh-based nature
gives rise to substantial computational costs, especially for complex
multiscale simulations. Emerging machine learning-based methods (e.g., neural
operators) provide data-driven solutions to PDEs, yet they present challenges,
including high training cost and low model reusability. Here, we propose the
neural-operator element method (NOEM) by synergistically combining FEM with
operator learning to address these challenges. NOEM leverages neural operators
(NOs) to simulate subdomains where a large number of finite elements would be
required if FEM was used. In each subdomain, an NO is used to build a single
element, namely a neural-operator element (NOE). NOEs are then integrated with
standard finite elements to represent the entire solution through the
variational framework. Thereby, NOEM does not necessitate dense meshing and
offers efficient simulations. We demonstrate the accuracy, efficiency, and
scalability of NOEM by performing extensive and systematic numerical
experiments, including nonlinear PDEs, multiscale problems, PDEs on complex
geometries, and discontinuous coefficient fields.

</details>


### [60] [Virtual failure assessment diagrams for hydrogen transmission pipelines](https://arxiv.org/abs/2506.18554)
*J. Wijnen,J. Parker,M. Gagliano,E. Martínez-Pañeda*

Main category: cs.CE

TL;DR: 结合热冶金焊接建模与相场断裂模拟预测输氢管道失效状态，构建虚拟失效评估图，模型与标准方法相符但更能体现焊缝微观结构异质性，还确定了机械安全系数。


<details>
  <summary>Details</summary>
Motivation: 准确预测输氢管道的失效状态，解决现有标准在考虑焊缝微观结构异质性时的不足。

Method: 将先进的热冶金焊接过程建模与耦合扩散 - 弹塑性相场断裂模拟相结合。

Result: 模型预测与标准的失效评估图方法吻合，表明标准方法在处理焊缝微观结构异质性时不够保守；构建了虚拟失效评估图。

Conclusion: 可以有效预测输氢管道失效状态，建立了考虑残余应力和硬脆焊缝区域作用的机械失效评估图安全系数。

Abstract: We combine state-of-the-art thermo-metallurgical welding process modelling
with coupled diffusion-elastic-plastic phase field fracture simulations to
predict the failure states of hydrogen transport pipelines. This enables
quantitatively resolving residual stress states and the role of brittle, hard
regions of the weld such as the heat affected zone (HAZ). Failure pressures can
be efficiently quantified as a function of asset state (existing defects),
materials and weld procedures adopted, and hydrogen purity. Importantly,
simulations spanning numerous relevant conditions (defect size and
orientations) are used to build \emph{Virtual} Failure Assessment Diagrams
(FADs), enabling a straightforward uptake of this mechanistic approach in
fitness-for-service assessment. Model predictions are in very good agreement
with FAD approaches from the standards but show that the latter are not
conservative when resolving the heterogeneous nature of the weld
microstructure. Appropriate, \emph{mechanistic} FAD safety factors are
established that account for the role of residual stresses and hard, brittle
weld regions.

</details>


### [61] [A Physics-Informed Neural Network Framework for Simulating Creep Buckling in Growing Viscoelastic Biological Tissues](https://arxiv.org/abs/2506.18565)
*Zhongya Lin,Jinshuai Bai,Shuang Li,Xindong Chen,Bo Li,Xi-Qiao Feng*

Main category: cs.CE

TL;DR: 本文提出基于能量的物理信息神经网络框架用于模拟粘弹性行为，能有效预测粘弹性不稳定性等，是传统方法的有前景替代方案。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法模拟粘弹性行为时计算复杂，需开发新方法。

Method: 采用增量方法开发基于能量的物理信息神经网络（PINN）框架，通过训练神经网络最小化系统势能泛函保证物理一致性。

Result: 该框架能自然捕捉蠕变屈曲，扩展到生物组织生长和形态发生预测，有效预测粘弹性不稳定性、后屈曲演化和组织形态演化。

Conclusion: PINN可成为模拟复杂时变材料行为的灵活强大工具，在多领域有应用潜力。

Abstract: Modeling viscoelastic behavior is crucial in engineering and biomechanics,
where materials undergo time-dependent deformations, including stress
relaxation, creep buckling and biological tissue development. Traditional
numerical methods, like the finite element method, often require explicit
meshing, artificial perturbations or embedding customised programs to capture
these phenomena, adding computational complexity. In this study, we develop an
energy-based physics-informed neural network (PINN) framework using an
incremental approach to model viscoelastic creep, stress relaxation, buckling,
and growth-induced morphogenesis. Physics consistency is ensured by training
neural networks to minimize the systems potential energy functional, implicitly
satisfying equilibrium and constitutive laws. We demonstrate that this
framework can naturally capture creep buckling without pre-imposed
imperfections, leveraging inherent training dynamics to trigger instabilities.
Furthermore, we extend our framework to biological tissue growth and
morphogenesis, predicting both uniform expansion and differential
growth-induced buckling in cylindrical structures. Results show that the
energy-based PINN effectively predicts viscoelastic instabilities,
post-buckling evolution and tissue morphological evolution, offering a
promising alternative to traditional methods. This study demonstrates that PINN
can be a flexible robust tool for modeling complex, time-dependent material
behavior, opening possible applications in structural engineering, soft
materials, and tissue development.

</details>


### [62] [Communication Architecture for Autonomous Power-to-X Platforms: Enhancing Inspection and Operation With Legged Robots and 5G](https://arxiv.org/abs/2506.18572)
*Peter Frank,Falk Dettinger,Daniel Dittler,Pascal Häbig,Nasser Jazdi,Kai Hufendiek,Michael Weyrich*

Main category: cs.CE

TL;DR: 本文对Power to X平台分类，提出通信架构，集成机器人执行任务并在5G网络下分析远程操作情况。


<details>
  <summary>Details</summary>
Motivation: 海上平台检查和维护成本高，人员需求大且操作条件困难。

Method: 对Power to X平台分类，提出通信架构，集成四足机器人执行任务，在5G独立网络下分析机器人远程监控、控制和遥操作。

Result: 记录、比较和评估了可用性和延迟等方面。

Conclusion: 未明确提及，但暗示所提出方案有助于降低人力需求和海上平台运维成本。

Abstract: Inspection and maintenance of offshore platforms are associated with high
costs, primarily due to the significant personnel requirements and challenging
operational conditions. This paper first presents a classification of Power to
X platforms. Building upon this foundation, a communication architecture is
proposed to enable monitoring, control, and teleoperation for a Power to X
platform. To reduce the demand for human labor, a robotic system is integrated
to autonomously perform inspection and maintenance tasks. The implementation
utilizes a quadruped robot. Remote monitoring, control, and teleoperation of
the robot are analyzed within the context of a 5G standalone network. As part
of the evaluation, aspects such as availability and latency are recorded,
compared, and critically assessed.

</details>


### [63] [A Study of Dynamic Stock Relationship Modeling and S&P500 Price Forecasting Based on Differential Graph Transformer](https://arxiv.org/abs/2506.18717)
*Linyue Hu,Qi Wang*

Main category: cs.CE

TL;DR: 本文提出DGT框架用于动态关系建模和股价预测，结合差分图结构与Transformer，验证动态关系建模，找出最优相关指标和范围，聚类分析支持量化策略。


<details>
  <summary>Details</summary>
Motivation: 股价预测对投资决策和风险管理至关重要，但因市场非线性动态和股票间时变相关性而具挑战性，传统静态相关模型无法捕捉股票关系演变。

Method: 提出DGT框架，通过差分图机制将顺序图结构变化整合到多头自注意力中，用因果时间注意力捕捉价格序列依赖关系，评估不同范围的相关指标作为空间注意力先验。

Result: 带空间先验的DGT优于GRU基线（RMSE: 0.24 vs. 0.87），Kendall's Tau全局矩阵结果最优（MAE: 0.11），聚类分析区分不同类型股票，Kendall's Tau和Mutual Information在波动行业表现出色。

Conclusion: 创新结合差分图结构与Transformer，验证动态关系建模，确定最优相关指标和范围，聚类分析支持量化策略，推动金融时间序列预测发展。

Abstract: Stock price prediction is vital for investment decisions and risk management,
yet remains challenging due to markets' nonlinear dynamics and time-varying
inter-stock correlations. Traditional static-correlation models fail to capture
evolving stock relationships. To address this, we propose a Differential Graph
Transformer (DGT) framework for dynamic relationship modeling and price
prediction. Our DGT integrates sequential graph structure changes into
multi-head self-attention via a differential graph mechanism, adaptively
preserving high-value connections while suppressing noise. Causal temporal
attention captures global/local dependencies in price sequences. We further
evaluate correlation metrics (Pearson, Mutual Information, Spearman, Kendall's
Tau) across global/local/dual scopes as spatial-attention priors. Using 10
years of S&P 500 closing prices (z-score normalized; 64-day sliding windows),
DGT with spatial priors outperformed GRU baselines (RMSE: 0.24 vs. 0.87).
Kendall's Tau global matrices yielded optimal results (MAE: 0.11). K-means
clustering revealed "high-volatility growth" and "defensive blue-chip" stocks,
with the latter showing lower errors (RMSE: 0.13) due to stable correlations.
Kendall's Tau and Mutual Information excelled in volatile sectors. This study
innovatively combines differential graph structures with Transformers,
validating dynamic relationship modeling and identifying optimal correlation
metrics/scopes. Clustering analysis supports tailored quantitative strategies.
Our framework advances financial time-series prediction through dynamic
modeling and cross-asset interaction analysis.

</details>


### [64] [Towards Real-time Structural Dynamics Simulation with Graph-based Digital Twin Modelling](https://arxiv.org/abs/2506.18724)
*Jun Zhang,Tong Zhang,Ying Wang*

Main category: cs.CE

TL;DR: 本文提出基于图的数字孪生建模框架模拟结构动态响应，经实验验证有效，提升计算效率，推动实际应用。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法计算成本高、效率低，深度学习方法物理可解释性有限且难适应多样结构，需解决这些问题。

Method: 提出基于图的数字孪生建模（GDTM）框架，用邻接矩阵表示结构顶点空间关系。

Result: 框架能准确模拟不同拓扑结构动态，数值模拟中归一化均方误差（NMSE）低于0.005，实验验证中低于0.0015，计算效率比传统有限元法提升超80倍。

Conclusion: 该研究促进基于图的结构动力学建模实际应用，有望推动结构性能评估和健康监测发展。

Abstract: Precise and timely simulation of a structure's dynamic behavior is crucial
for evaluating its performance and assessing its health status. Traditional
numerical methods are often limited by high computational costs and low
efficiency, while deep learning approaches offer a promising alternative.
However, these data-driven methods still face challenges, such as limited
physical interpretability and difficulty in adapting to diverse structural
configurations. To address these issues, this study proposes a graph-based
digital twin modelling (GDTM) framework to simulate structural dynamic
responses across various spatial topologies. In this framework, the adjacency
matrix explicitly represents the spatial relationships between structural
vertices, enhancing the model's physical interpretability. The effectiveness of
the proposed framework was validated through comprehensive numerical and
experimental studies. The results demonstrate that the framework accurately
simulated structural dynamics across different topological configurations, with
Normalized Mean-Squared Error (NMSE) values consistently below 0.005 in
numerical simulations and 0.0015 in experimental validations. Furthermore, the
framework achieved over 80-fold improvements in computational efficiency
compared to traditional finite element methods (FEM). This research promotes
the practical application of graph-based structural dynamics modelling, which
has the potential to significantly advance structural performance evaluation
and health monitoring.

</details>


### [65] [Skeletal Reaction Models for Gasoline Surrogate Combustion](https://arxiv.org/abs/2506.18853)
*Yinmin Liu,Hessam Babaee,Peyman Givi,Daniel Livescu,Arash Nouri*

Main category: cs.CE

TL;DR: 本文采用瞬时局部灵敏度分析技术为四组分汽油替代模型推导骨架反应模型，评估灵敏度后自动构建模型，应用于LLNL的1389种物质模型，开发出两种新骨架模型并评估性能。


<details>
  <summary>Details</summary>
Motivation: 开发汽油替代模型的骨架反应模型，以简化复杂的详细动力学模型。

Method: 采用瞬时局部灵敏度分析技术，用基于CUR矩阵分解并结合隐式时间积分的隐式TDB - CUR降阶建模方法估计灵敏度，之后通过全自动程序开发骨架反应模型。

Result: 开发出分别包含679和494种物质的两种新骨架模型，679种物质模型重现详细模型关键火焰结果误差小于1%，494种物质模型误差小于10%。

Conclusion: 所开发的骨架反应模型能在一定误差范围内较好地重现详细模型的结果，具有应用价值。

Abstract: Skeletal reaction models are derived for a four-component gasoline surrogate
model via an instantaneous local sensitivity analysis technique. The
sensitivities of the species mass fractions and the temperature with respect to
the reaction rates are estimated by a reduced-order modeling (ROM) methodology.
Termed "implicit time-dependent basis CUR (implicit TDB-CUR)," this methodology
is based on the CUR matrix decomposition and incorporates implicit time
integration for evolving the bases. The estimated sensitivities are
subsequently analyzed to develop skeletal reaction models with a fully
automated procedure. The 1389-species gasoline surrogate model developed at
Lawrence Livermore National Laboratory (LLNL) is selected as the detailed
kinetics model. The skeletal reduction procedure is applied to this model in a
zero-dimensional constant-pressure reactor over a wide range of initial
conditions. The performances of the resulting skeletal models are appraised by
comparison against the results via the LLNL detailed model, and also
predictions via other skeletal models. Two new skeletal models are developed
consisting of 679 and 494 species, respectively. The first is an alternative to
an existing model with the same number of species. The predictions with this
model reproduces the detailed models vital flame results with less than 1%
errors. The errors via the second model are less than 10%.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [66] [DCMF: A Dynamic Context Monitoring and Caching Framework for Context Management Platforms](https://arxiv.org/abs/2506.17226)
*Ashish Manchanda,Prem Prakash Jayaraman,Abhik Banerjee,Kaneez Fizza,Arkady Zaslavsky*

Main category: cs.DB

TL;DR: 提出动态上下文监控框架DCMF提升上下文缓存，经评估比m - CAC技术有更好效果，适合动态物联网环境。


<details>
  <summary>Details</summary>
Motivation: 上下文感知的物联网应用对及时准确的上下文信息需求增加，传统缓存策略难以应对物联网上下文的瞬态特性。

Method: 提出DCMF框架，包含CEE和CMM两个核心组件。CEE计算访问概率，CMM采用混合Dempster - Shafer方法管理上下文新鲜度。

Result: 在CoaaS平台用真实智慧城市数据评估，DCMF比m - CAC技术缓存命中率高12.5%，缓存过期率降低60%。

Conclusion: DCMF具有可扩展性，适合动态上下文感知的物联网环境。

Abstract: The rise of context-aware IoT applications has increased the demand for
timely and accurate context information. Context is derived by aggregating and
inferring from dynamic IoT data, making it highly volatile and posing
challenges in maintaining freshness and real-time accessibility. Caching is a
potential solution, but traditional policies struggle with the transient nature
of context in IoT (e.g., ensuring real-time access for frequent queries or
handling fast-changing data). To address this, we propose the Dynamic Context
Monitoring Framework (DCMF) to enhance context caching in Context Management
Platforms (CMPs) by dynamically evaluating and managing context. DCMF comprises
two core components: the Context Evaluation Engine (CEE) and the Context
Management Module (CMM). The CEE calculates the Probability of Access (PoA)
using parameters such as Quality of Service (QoS), Quality of Context (QoC),
Cost of Context (CoC), timeliness, and Service Level Agreements (SLAs),
assigning weights to assess access likelihood. Based on this, the CMM applies a
hybrid Dempster-Shafer approach to manage Context Freshness (CF), updating
belief levels and confidence scores to determine whether to cache, evict, or
refresh context items. We implemented DCMF in a Context-as-a-Service (CoaaS)
platform and evaluated it using real-world smart city data, particularly
traffic and roadwork scenarios. Results show DCMF achieves a 12.5% higher cache
hit rate and reduces cache expiry by up to 60% compared to the m-CAC technique,
ensuring timely delivery of relevant context and reduced latency. These results
demonstrate DCMF's scalability and suitability for dynamic context-aware IoT
environments.

</details>


### [67] [Transient Concepts in Streaming Graphs](https://arxiv.org/abs/2506.17451)
*Aida Sheshbolouki,M. Tamer Ozsu*

Main category: cs.DB

TL;DR: 文章针对流图场景下的概念漂移问题，提出SGDD和SGDP两个框架，SGDP可提前预测概念漂移。


<details>
  <summary>Details</summary>
Motivation: 理解、检测和适应流数据中的概念漂移对有效分析至关重要，但当前方法存在局限性，不适用于流图场景。

Method: 提出SGDD和SGDP两个框架来检测和预测流图中的概念漂移，二者可辨别生成源的变化。

Result: SGDD检测概念漂移有显著延迟，难以评估性能；SGDP能在0.19 - 7374毫秒前预测概念漂移，且无需访问数据记录的有效负载。

Conclusion: 提出的SGDD和SGDP框架为流图中的概念漂移检测和预测提供了新方法。

Abstract: Concept Drift (CD) occurs when a change in a hidden context can induce
changes in a target concept. CD is a natural phenomenon in non-stationary
settings such as data streams. Understanding, detection, and adaptation to CD
in streaming data is (i) vital for effective and efficient analytics as
reliable output depends on adaptation to fresh input, (ii) challenging as it
requires efficient operations as well as effective performance evaluations, and
(iii) impactful as it applies to a variety of use cases and is a crucial
initial step for data management systems. Current works are mostly focused on
passive CD detection as part of supervised adaptation, on independently
generated data instances or graph snapshots, on target concepts as a function
of data labels, on static data management, and on specific temporal order of
data record. These methods do not always work. We revisit CD for the streaming
graphs setting and introduce two first-of-its-kind frameworks SGDD and SGDP for
streaming graph CD detection and prediction. Both frameworks discern the change
of generative source. SGDD detects the CDs due to the changes of generative
parameters with significant delays such that it is difficult to evaluate the
performance, while SGDP predicts these CDs between 7374 to 0.19 milliseconds
ahead of their occurrence, without accessing the payloads of data records.

</details>


### [68] [Lower Bounds for Conjunctive Query Evaluation](https://arxiv.org/abs/2506.17702)
*Stefan Mengel*

Main category: cs.DB

TL;DR: 本文综述不同场景下合取查询评估复杂度的已知结果，聚焦复杂度理论假设与查询应答的联系及算法改进可能性。


<details>
  <summary>Details</summary>
Motivation: 了解不同场景下合取查询评估复杂度的研究现状，并探索复杂度理论新假设与查询应答的关联。

Method: 对不同场景下合取查询评估复杂度的已知结果进行调研。

Result: 展示了不同复杂度理论假设与查询应答的联系，表明部分已知算法可能无法改进。

Conclusion: 不同复杂度理论假设对合取查询评估有重要意义，部分已知算法改进空间不大。

Abstract: In this tutorial, we will survey known results on the complexity of
conjunctive query evaluation in different settings, ranging from Boolean
queries over counting to more complex models like enumeration and direct
access. A particular focus will be on showing how different relatively recent
hypotheses from complexity theory connect to query answering and allow showing
that known algorithms in several cases can likely not be improved.

</details>


### [69] [Dual-Hierarchy Labelling: Scaling Up Distance Queries on Dynamic Road Networks](https://arxiv.org/abs/2506.18013)
*Muhammad Farhan,Henning Koehler,Qing Wang*

Main category: cs.DB

TL;DR: 提出Dual - Hierarchy Labelling (DHL) 解决动态道路网络距离查询问题，在10个大型道路网络评估中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有解决道路网络最短路径距离计算的研究多针对静态网络，动态网络相关方法存在查询响应慢或维护性能差的问题，需要新方案。

Method: 提出DHL，包含查询层次、更新层次和层次标记三个组件，开发动态算法及并行变体。

Result: 在10个大型道路网络评估中，构建和更新时间更快，查询处理快2 - 4倍，标记空间仅占10% - 20%。

Conclusion: DHL在动态道路网络距离查询上显著优于现有方法。

Abstract: Computing the shortest-path distance between any two given vertices in road
networks is an important problem. A tremendous amount of research has been
conducted to address this problem, most of which are limited to static road
networks. Since road networks undergo various real-time traffic conditions,
there is a pressing need to address this problem for dynamic road networks.
Existing state-of-the-art methods incrementally maintain an indexing structure
to reflect dynamic changes on road networks. However, these methods suffer from
either slow query response time or poor maintenance performance, particularly
when road networks are large. In this work, we propose an efficient solution
\emph{Dual-Hierarchy Labelling (DHL)} for distance querying on dynamic road
networks from a novel perspective, which incorporates two hierarchies with
different but complementary data structures to support efficient query and
update processing. Specifically, our proposed solution is comprised of three
main components: \emph{query hierarchy}, \emph{update hierarchy}, and
\emph{hierarchical labelling}, where \emph{query hierarchy} enables efficient
query answering by exploring only a small subset of vertices in the labels of
two query vertices and \emph{update hierarchy} supports efficient maintenance
of distance labelling under edge weight increase or decrease. We further
develop dynamic algorithms to reflect dynamic changes by efficiently
maintaining the update hierarchy and hierarchical labelling. We also propose a
parallel variant of our dynamic algorithms by exploiting labelling structure.
We evaluate our methods on 10 large road networks and it shows that our methods
significantly outperform the state-of-the-art methods, i.e., achieving
considerably faster construction and update time, while being consistently 2-4
times faster in terms of query processing and consuming only 10\%-20\%
labelling space.

</details>


### [70] [Floating-Point Data Transformation for Lossless Compression](https://arxiv.org/abs/2506.18062)
*Samirasadat Jamalidinan,Kazem Cheshmi*

Main category: cs.DB

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Floating-point data is widely used across various domains. Depending on the
required precision, each floating-point value can occupy several bytes.
Lossless storage of this information is crucial due to its critical accuracy,
as seen in applications such as medical imaging and language model weights. In
these cases, data size is often significant, making lossless compression
essential. Previous approaches either treat this data as raw byte streams for
compression or fail to leverage all patterns within the dataset. However,
because multiple bytes represent a single value and due to inherent patterns in
floating-point representations, some of these bytes are correlated. To leverage
this property, we propose a novel data transformation method called Typed Data
Transformation (\DTT{}) that groups related bytes together to improve
compression. We implemented and tested our approach on various datasets across
both CPU and GPU. \DTT{} achieves a geometric mean compression ratio
improvement of 1.16$\times$ over state-of-the-art compression tools such as
zstd, while also improving both compression and decompression throughput by
1.18--3.79$\times$.

</details>


### [71] [Learning Lineage Constraints for Data Science Operations](https://arxiv.org/abs/2506.18252)
*Jinjin Zhao*

Main category: cs.DB

TL;DR: 本文受跨库性能优化中中间表示（IRs）的启发，提出一种跨库数据谱系架构XProv，并讨论了在仅有物化图时推断逻辑模式的早期想法。


<details>
  <summary>Details</summary>
Motivation: 数据科学工作流需跨库数据谱系进行调试，但现有谱系表示与特定数据模型和操作范式紧密相关，需要一种通用的跨库逻辑谱系表示方法。

Method: 提出XProv架构，将数据转换的物化谱系图与抽象逻辑模式相链接。

Result: 无明确提及实验或实际应用结果。

Conclusion: 提出了跨库逻辑谱系的架构设想，并探讨了逻辑模式推断的早期思路。

Abstract: Data science workflows often integrate functionalities from a diverse set of
libraries and frameworks. Tasks such as debugging require data lineage that
crosses library boundaries. The problem is that the way that "lineage" is
represented is often intimately tied to particular data models and data
manipulation paradigms. Inspired by the use of intermediate representations
(IRs) in cross-library performance optimizations, this vision paper proposes a
similar architecture for lineage - how do we specify logical lineage across
libraries in a common parameterized way? In practice, cross-library workflows
will contain both known operations and unknown operations, so a key design of
XProv to link both materialized lineage graphs of data transformations and the
aforementioned abstracted logical patterns. We further discuss early ideas on
how to infer logical patterns when only the materialized graphs are available.

</details>


### [72] [Fast Capture of Cell-Level Provenance in Numpy](https://arxiv.org/abs/2506.18255)
*Jinjin Zhao,Sanjay Krishnan*

Main category: cs.DB

TL;DR: 本文提出用于数组的原型注释系统，捕捉numpy库中单元级溯源信息并进行内存优化，可用于结构化数据工作流治理。


<details>
  <summary>Details</summary>
Motivation: 现有数组工作流溯源捕捉存在API快速演变、操作类型多样和数据集规模大等挑战，需要有效方法解决。

Method: 提出针对数组的原型注释系统，在numpy库中捕捉单元级溯源信息，并进行内存优化。

Result: 探索出能大幅降低注释延迟的简单内存优化方法。

Conclusion: 此数组溯源捕捉方法可作为结构化数据工作流和数据科学应用治理系统的一部分。

Abstract: Effective provenance tracking enhances reproducibility, governance, and data
quality in array workflows. However, significant challenges arise in capturing
this provenance, including: (1) rapidly evolving APIs, (2) diverse operation
types, and (3) large-scale datasets. To address these challenges, this paper
presents a prototype annotation system designed for arrays, which captures
cell-level provenance specifically within the numpy library. With this
prototype, we explore straightforward memory optimizations that substantially
reduce annotation latency. We envision this provenance capture approach for
arrays as part of a broader governance system for tracking for structured data
workflows and diverse data science applications.

</details>


### [73] [TableVault: Managing Dynamic Data Collections for LLM-Augmented Workflows](https://arxiv.org/abs/2506.18257)
*Jinjin Zhao,Sanjay Krishnan*

Main category: cs.DB

TL;DR: 提出数据管理系统TableVault，用于处理LLM增强环境中动态数据集合，满足复杂数据工作流需求。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型集成到复杂数据工作流带来管理挑战，需要新的数据管理系统。

Method: 将成熟数据库方法与新兴LLM驱动需求结合，设计TableVault系统。

Result: TableVault支持并发执行、保证可重复性、维护强大数据版本控制并实现可组合工作流设计。

Conclusion: TableVault提供了一个透明平台，能有效管理结构化数据和相关数据工件。

Abstract: Large Language Models (LLMs) have emerged as powerful tools for automating
and executing complex data tasks. However, their integration into more complex
data workflows introduces significant management challenges. In response, we
present TableVault - a data management system designed to handle dynamic data
collections in LLM-augmented environments. TableVault meets the demands of
these workflows by supporting concurrent execution, ensuring reproducibility,
maintaining robust data versioning, and enabling composable workflow design. By
merging established database methodologies with emerging LLM-driven
requirements, TableVault offers a transparent platform that efficiently manages
both structured data and associated data artifacts.

</details>


### [74] [Patient Journey Ontology: Representing Medical Encounters for Enhanced Patient-Centric Applications](https://arxiv.org/abs/2506.18772)
*Hassan S. Al Khatib,Subash Neupane,Sudip Mittal,Shahram Rahimi,Nina Marhamati,Sean Bozorgzad*

Main category: cs.DB

TL;DR: 本文提出患者旅程本体（PJO）框架，整合患者数据，支持预测分析，经评估有较强能力且有应用潜力，助力医疗知识表示。


<details>
  <summary>Details</summary>
Motivation: 医疗行业向以患者为中心转变，需要先进方法管理和表示患者数据。

Method: 提出患者旅程本体（PJO）框架，利用本体整合不同患者数据源，捕捉医疗接触间的关系。

Result: 经专家定量和定性评估，PJO在患者病史检索、症状跟踪和提供者交互表示方面能力强，发现增强诊断 - 症状关联的机会。

Conclusion: PJO可靠且有实际适用性，能提升患者预后和医疗效率，为医疗知识表示作贡献，可用于个性化医疗等。

Abstract: The healthcare industry is moving towards a patient-centric paradigm that
requires advanced methods for managing and representing patient data. This
paper presents a Patient Journey Ontology (PJO), a framework that aims to
capture the entirety of a patient's healthcare encounters. Utilizing
ontologies, the PJO integrates different patient data sources like medical
histories, diagnoses, treatment pathways, and outcomes; it enables semantic
interoperability and enhances clinical reasoning. By capturing temporal,
sequential, and causal relationships between medical encounters, the PJO
supports predictive analytics, enabling earlier interventions and optimized
treatment plans. The ontology's structure, including its main classes,
subclasses, properties, and relationships, as detailed in the paper,
demonstrates its ability to provide a holistic view of patient care.
Quantitative and qualitative evaluations by Subject Matter Experts (SMEs)
demonstrate strong capabilities in patient history retrieval, symptom tracking,
and provider interaction representation, while identifying opportunities for
enhanced diagnosis-symptom linking. These evaluations reveal the PJO's
reliability and practical applicability, demonstrating its potential to enhance
patient outcomes and healthcare efficiency. This work contributes to the
ongoing efforts of knowledge representation in healthcare, offering a reliable
tool for personalized medicine, patient journey analysis and advancing the
capabilities of Generative AI in healthcare applications.

</details>


### [75] [LIGHTHOUSE: Fast and precise distance to shoreline calculations from anywhere on earth](https://arxiv.org/abs/2506.18842)
*Patrick Beukema,Henry Herzog,Yawen Zhang,Hunter Pitelka,Favyen Bastani*

Main category: cs.DB

TL;DR: 提出用于从地球任意位置进行快速高效沿海距离计算的新数据集和算法，数据集精度大幅提升，新库性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有全球沿海数据集分辨率低，限制了其应用，需要更高精度的数据集。

Method: 利用公开卫星图像和计算机视觉生成10米分辨率的全球海岸线数据集，引入新库Lighthouse处理大规模查询。

Result: 提供了10米分辨率的全球海岸线数据集，Lighthouse只需1个CPU和2GB内存就能实现毫秒级在线推理。

Conclusion: 新数据集和Lighthouse库适合资源受限环境下的实时应用。

Abstract: We introduce a new dataset and algorithm for fast and efficient coastal
distance calculations from Anywhere on Earth (AoE). Existing global coastal
datasets are only available at coarse resolution (e.g. 1-4 km) which limits
their utility. Publicly available satellite imagery combined with computer
vision enable much higher precision. We provide a global coastline dataset at
10 meter resolution, a 100+ fold improvement in precision over existing data.
To handle the computational challenge of querying at such an increased scale,
we introduce a new library: Layered Iterative Geospatial Hierarchical
Terrain-Oriented Unified Search Engine (Lighthouse). Lighthouse is both
exceptionally fast and resource-efficient, requiring only 1 CPU and 2 GB of RAM
to achieve millisecond online inference, making it well suited for real-time
applications in resource-constrained environments.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [76] [PBFT-Backed Semantic Voting for Multi-Agent Memory Pruning](https://arxiv.org/abs/2506.17338)
*Duong Bach*

Main category: cs.DC

TL;DR: 本文提出用于多智能体系统的Co - Forgetting协议，介绍其组成并通过实验证明协议有效性。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在复杂动态环境中需管理共享知识，保证分布式内存同步、去除过时数据。

Method: 提出Co - Forgetting协议，包含上下文感知语义投票、多尺度时间衰减函数和PBFT共识机制，利用gRPC、Pinecone和SQLite。

Result: 在模拟环境实验中，500个周期内存占用减少52%，遗忘决策投票准确率88%，PBFT共识成功率92%，内存访问缓存命中率82%。

Conclusion: Co - Forgetting协议能有效管理多智能体系统的共享知识，实现内存同步修剪。

Abstract: The proliferation of multi-agent systems (MAS) in complex, dynamic
environments necessitates robust and efficient mechanisms for managing shared
knowledge. A critical challenge is ensuring that distributed memories remain
synchronized, relevant, and free from the accumulation of outdated or
inconsequential data - a process analogous to biological forgetting. This paper
introduces the Co-Forgetting Protocol, a novel, comprehensive framework
designed to address this challenge by enabling synchronized memory pruning in
MAS. The protocol integrates three key components: (1) context-aware semantic
voting, where agents utilize a lightweight DistilBERT model to assess the
relevance of memory items based on their content and the current operational
context; (2) multi-scale temporal decay functions, which assign diminishing
importance to memories based on their age and access frequency across different
time horizons; and (3) a Practical Byzantine Fault Tolerance (PBFT)-based
consensus mechanism, ensuring that decisions to retain or discard memory items
are agreed upon by a qualified and fault-tolerant majority of agents, even in
the presence of up to f Byzantine (malicious or faulty) agents in a system of N
greater than or equal to 3f+1 agents. The protocol leverages gRPC for efficient
inter-agent communication and Pinecone for scalable vector embedding storage
and similarity search, with SQLite managing metadata. Experimental evaluations
in a simulated MAS environment with four agents demonstrate the protocol's
efficacy, achieving a 52% reduction in memory footprint over 500 epochs, 88%
voting accuracy in forgetting decisions against human-annotated benchmarks, a
92% PBFT consensus success rate under simulated Byzantine conditions, and an
82% cache hit rate for memory access.

</details>


### [77] [Speeding up Local Optimization in Vehicle Routing with Tensor-based GPU Acceleration](https://arxiv.org/abs/2506.17357)
*Zhenyu Lei,Jin-Kao Hao,Qinghua Wu*

Main category: cs.DC

TL;DR: 本文提出基于张量的GPU加速方法解决车辆路径问题局部搜索邻域探索计算成本高的问题，实验证明其优势并分析优缺点。


<details>
  <summary>Details</summary>
Motivation: 车辆路径问题局部搜索邻域探索计算成本高、耗时长，尤其针对大实例或复杂约束问题。

Method: 引入基于张量的GPU加速方法，采用基于属性的表示，低耦合架构将密集计算卸载到GPU。

Result: 在三个路径问题基准实例上的对比实验显示，该方法比传统CPU实现有显著计算优势。

Conclusion: 该方法能提高计算效率和潜在提高解质量，研究分析其优缺点，为未来改进提供方向。

Abstract: Local search plays a central role in many effective heuristic algorithms for
the vehicle routing problem (VRP) and its variants. However, neighborhood
exploration is known to be computationally expensive and time consuming,
especially for large instances or problems with complex constraints. In this
study, we explore a promising direction to address this challenge by
introducing an original tensor-based GPU acceleration method designed to speed
up the commonly used local search operators in vehicle routing. By using an
attribute-based representation, the method offers broad extensibility, making
it applicable to different VRP variants. Its low-coupling architecture, with
intensive computations completely offloaded to the GPU, ensures seamless
integration in various local search-based algorithms and frameworks, leading to
significant improvements in computational efficiency and potentially improved
solution quality. Through comparative experiments on benchmark instances of
three routing problems, we demonstrate the substantial computational advantages
of the proposed approach over traditional CPU-based implementations. We also
provide a detailed analysis of the strengths and limitations of the method,
providing valuable insights into its performance characteristics and
identifying potential bottlenecks in practical applications. These findings
contribute to a better understanding and suggest directions for future
improvements.

</details>


### [78] [Code Generation for Near-Roofline Finite Element Actions on GPUs from Symbolic Variational Forms](https://arxiv.org/abs/2506.17471)
*Kaushik Kulkarni,Andreas Klöckner*

Main category: cs.DC

TL;DR: 提出在GPU上评估有限元方法变分形式的并行化策略，在Firedrake框架中实现原型并测试，多数测试用例达超50%峰值性能。


<details>
  <summary>Details</summary>
Motivation: 解决通过统一形式语言在单纯形网格上可表达的有限元方法变分形式评估的并行化问题，实现接近峰值性能。

Method: 基于代码转换，构建调度候选空间并通过启发式成本模型排序，设计搜索空间并采用剪枝策略；在Firedrake框架实现原型。

Result: 在两代Nvidia GPU上对多种算子测试，65%测试用例达到超50%峰值性能。

Conclusion: 所提并行化策略有效，能平衡设备延迟隐藏能力和状态空间，实现接近峰值性能。

Abstract: We present a novel parallelization strategy for evaluating Finite Element
Method (FEM) variational forms on GPUs, focusing on those that are expressible
through the Unified Form Language (UFL) on simplex meshes. We base our approach
on code transformations, wherein we construct a space of scheduling candidates
and rank them via a heuristic cost model to effectively handle the large
diversity of computational workloads that can be expressed in this way. We
present a design of a search space to which the cost model is applied, along
with an associated pruning strategy to limit the number of configurations that
need to be empirically evaluated. The goal of our design is to strike a balance
between the device's latency-hiding capabilities and the amount of state space,
a key factor in attaining near-roofline performance.
  To make our work widely available, we have prototyped our parallelization
strategy within the \textsc{Firedrake} framework, a UFL-based FEM solver. We
evaluate the performance of our parallelization scheme on two generations of
Nvidia GPUs, specifically the Titan V (Volta architecture) and Tesla K40c
(Kepler architecture), across a range of operators commonly used in
applications, including fluid dynamics, wave propagation, and structural
mechanics, in 2D and 3D geometries. Our results demonstrate that our proposed
algorithm achieves more than $50\%$ roofline performance in $65\%$ of the test
cases on both devices.

</details>


### [79] [ConsumerBench: Benchmarking Generative AI Applications on End-User Devices](https://arxiv.org/abs/2506.17538)
*Yile Gu,Rohan Kadekodi,Hoang Nguyen,Keisuke Kamahori,Yiyu Liu,Baris Kasikci*

Main category: cs.DC

TL;DR: 提出ConsumerBench框架评估端设备上GenAI模型系统效率和响应时间，实验揭示问题并给出建议。


<details>
  <summary>Details</summary>
Motivation: GenAI应用从云端到端设备带来资源管理等挑战，需要评估系统效率和响应时间的框架。

Method: 设计ConsumerBench框架，模拟多应用并发场景，支持自定义工作流，捕获应用和系统级指标。

Result: 揭示资源共享低效、贪婪分配下调度不公、静态模型服务器配置性能陷阱。

Conclusion: 为模型开发者和系统设计者提供实用见解，如定制内核和SLO感知调度策略的好处。

Abstract: The recent shift in Generative AI (GenAI) applications from cloud-only
environments to end-user devices introduces new challenges in resource
management, system efficiency, and user experience. This paper presents
ConsumerBench, a comprehensive benchmarking framework designed to evaluate the
system efficiency and response time of GenAI models running on end-user
devices. Unlike existing benchmarks that assume exclusive model access on
dedicated GPUs, ConsumerBench simulates realistic multi-application scenarios
executing concurrently on constrained hardware. Furthermore, ConsumerBench
supports customizable workflows that simulate complex tasks requiring
coordination among multiple applications. ConsumerBench captures both
application-level metrics, including latency and Service Level Objective (SLO)
attainment, and system-level metrics like CPU/GPU utilization and memory
bandwidth. Through extensive experiments, ConsumerBench reveals inefficiencies
in resource sharing, unfair scheduling under greedy allocation, and performance
pitfalls of static model server configurations. The paper also provides
practical insights for model developers and system designers, highlighting the
benefits of custom kernels tailored to consumer-grade GPU architectures and the
value of implementing SLO-aware scheduling strategies.

</details>


### [80] [Research on Model Parallelism and Data Parallelism Optimization Methods in Large Language Model-Based Recommendation Systems](https://arxiv.org/abs/2506.17551)
*Haowei Yang,Yu Tian,Zhongheng Yang,Zhao Wang,Chengrui Zhou,Dannier Li*

Main category: cs.DC

TL;DR: 本文研究推荐场景下大语言模型分布式训练的优化方法，提出混合并行方案，实验显示能提升训练吞吐量和资源利用率，还讨论了策略权衡与未来方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型应用于推荐系统时，其大规模参数和数据量导致计算与通信瓶颈突出，需优化训练方法。

Method: 系统研究模型并行和数据并行两类优化方法。模型并行实现张量并行和流水线并行并引入自适应负载均衡机制；数据并行比较同步和异步模式，结合梯度压缩、稀疏化技术与高效聚合通信框架。

Result: 在模拟服务环境的真实推荐数据集上实验表明，提出的混合并行方案比传统单模式并行训练吞吐量提高超30%，资源利用率提高约20%，且有强可扩展性和鲁棒性。

Conclusion: 讨论不同并行策略在线上部署的权衡，指出未来涉及异构硬件集成和自动调度技术的方向。

Abstract: With the rapid adoption of large language models (LLMs) in recommendation
systems, the computational and communication bottlenecks caused by their
massive parameter sizes and large data volumes have become increasingly
prominent. This paper systematically investigates two classes of optimization
methods-model parallelism and data parallelism-for distributed training of LLMs
in recommendation scenarios. For model parallelism, we implement both tensor
parallelism and pipeline parallelism, and introduce an adaptive load-balancing
mechanism to reduce cross-device communication overhead. For data parallelism,
we compare synchronous and asynchronous modes, combining gradient compression
and sparsification techniques with an efficient aggregation communication
framework to significantly improve bandwidth utilization. Experiments conducted
on a real-world recommendation dataset in a simulated service environment
demonstrate that our proposed hybrid parallelism scheme increases training
throughput by over 30% and improves resource utilization by approximately 20%
compared to traditional single-mode parallelism, while maintaining strong
scalability and robustness. Finally, we discuss trade-offs among different
parallel strategies in online deployment and outline future directions
involving heterogeneous hardware integration and automated scheduling
technologies.

</details>


### [81] [Distributed Butterfly Analysis using Mobile Agents](https://arxiv.org/abs/2506.17721)
*Prabhat Kumar Chand,Apurba Das,Anisur Rahaman Molla*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Butterflies, or 4-cycles in bipartite graphs, are crucial for identifying
cohesive structures and dense subgraphs. While agent-based data mining is
gaining prominence, its application to bipartite networks remains relatively
unexplored. We propose distributed, agent-based algorithms for \emph{Butterfly
Counting} in a bipartite graph $G((A,B),E)$. Agents first determine their
respective partitions and collaboratively construct a spanning tree, electing a
leader within $O(n \log \lambda)$ rounds using only $O(\log \lambda)$ bits per
agent. A novel meeting mechanism between adjacent agents improves efficiency
and eliminates the need for prior knowledge of the graph, requiring only the
highest agent ID $\lambda$ among the $n$ agents. Notably, our techniques
naturally extend to general graphs, where leader election and spanning tree
construction maintain the same round and memory complexities. Building on these
foundations, agents count butterflies per node in $O(\Delta)$ rounds and
compute the total butterfly count of $G$ in $O(\Delta+\min\{|A|,|B|\})$ rounds.

</details>


### [82] [Choosing the Right Battery Model for Data Center Simulations](https://arxiv.org/abs/2506.17739)
*Paul Kilian,Philipp Wiesner,Odej Kao*

Main category: cs.DC

TL;DR: 随着计算资源需求增长，数据中心电源系统改变，新的联合仿真测试台出现，但选电池模型有挑战。本文在Vessim中实现四种电池模型并分析，线性模型表现好，简单无损模型不佳。


<details>
  <summary>Details</summary>
Motivation: 计算资源需求上升，电力成本增加和碳排放法规促使数据中心电源系统改变，新测试台出现但选电池模型困难。

Method: 在联合仿真框架Vessim中为数据中心场景实现四种不同电池模型并分析其行为。

Result: 考虑效率和功率限制的线性模型在短期实验中与复杂物理模型行为相近，执行速度快且无需电化学和电路动力学知识；简单无损模型不能准确表现复杂行为且无运行时间优势。

Conclusion: 线性模型在数据中心电池模拟中更具优势，简单无损模型不适用。

Abstract: As demand for computing resources continues to rise, the increasing cost of
electricity and anticipated regulations on carbon emissions are prompting
changes in data center power systems. Many providers are now operating compute
nodes in microgrids, close to renewable power generators and energy storage, to
maintain full control over the cost and origin of consumed electricity.
Recently, new co-simulation testbeds have emerged that integrate
domain-specific simulators to support research, development, and testing of
such systems in a controlled environment. Yet, choosing an appropriate battery
model for data center simulations remains challenging, as it requires balancing
simulation speed, realism, and ease of configuration.
  In this paper, we implement four different battery models for data center
scenarios within the co-simulation framework Vessim and analyze their behavior.
The results show that linear models, which consider inefficiencies and power
limits, closely match the behavior of complex physics-based models in
short-term experiments while offering faster execution, and not requiring
knowledge on electrochemical reactions and circuit-level dynamics. In contrast,
simple, lossless models fail to accurately represent complex behavior and
provide no further runtime advantage.

</details>


### [83] [Maintaining a Bounded Degree Expander in Dynamic Peer-to-Peer Networks](https://arxiv.org/abs/2506.17757)
*Antonio Cruciani*

Main category: cs.DC

TL;DR: 研究在节点不断加入和离开的全分布式环境中维护健壮且稀疏覆盖网络的问题，推广了已有协议，证明算法能高概率维持恒定度扩张图。


<details>
  <summary>Details</summary>
Motivation: 现实中非结构化点对点网络需维护连接良好且低度数的通信图，解决已有研究中的开放问题。

Method: 推广Becchetti等人的协议到动态网络，基于Augustine等人的框架分析，由无意识对手控制节点进出。

Result: 分布式算法能高概率维持恒定度扩张图，对抗每轮高达$\mathcal{O}(n/polylog(n))$的节点变动率。

Conclusion: 得到一个简单、全分布式且抗节点变动的协议，理论结果与经验行为相符。

Abstract: We study the problem of maintaining robust and sparse overlay networks in
fully distributed settings where nodes continuously join and leave the system.
This scenario closely models real-world unstructured peer-to-peer networks,
where maintaining a well-connected yet low-degree communication graph is
crucial. We generalize a recent protocol by Becchetti et al. [SODA 2020] that
relies on a simple randomized connection strategy to build an expander topology
with high probability to a dynamic networks with churn setting. In this work,
the network dynamism is governed by an oblivious adversary that controls which
nodes join and leave the system in each round. The adversary has full knowledge
of the system and unbounded computational power, but cannot see the random
choices made by the protocol. Our analysis builds on the framework of Augustine
et al. [FOCS 2015], and shows that our distributed algorithm maintains a
constant-degree expander graph with high probability, despite a continuous
adversarial churn with a rate of up to $\mathcal{O}(n/polylog(n))$ per round,
where $n$ is the stable network size. The protocol and proof techniques are not
new, but together they resolve a specific open problem raised in prior work.
The result is a simple, fully distributed, and churn-resilient protocol with
provable guarantees that align with observed empirical behavior.

</details>


### [84] [Implementation and Evaluation of Fast Raft for Hierarchical Consensus](https://arxiv.org/abs/2506.17793)
*Anton Melnychuk,Bryan SebaRaj*

Main category: cs.DC

TL;DR: 本文首次开源实现并评估Fast Raft，实验表明在低丢包率下有吞吐量提升和延迟降低。


<details>
  <summary>Details</summary>
Motivation: 为动态分布式环境设计一种减少日志条目提交消息轮数的共识协议。

Method: 引入快速通道机制并减少对领导者的依赖，使用gRPC和基于Kubernetes在AWS可用区部署。

Result: 在低丢包条件下实现吞吐量提升和提交延迟降低，同时保证安全性和活性。

Conclusion: Fast Raft在低丢包场景下能有效提升性能，且保留Raft协议安全和活性保证。

Abstract: We present the first open-source implementation and evaluation of Fast Raft,
a hierarchical consensus protocol designed for dynamic, distributed
environments. Fast Raft reduces the number of message rounds needed to commit
log entries compared to standard Raft by introducing a fast-track mechanism and
reducing leader dependence. Our implementation uses gRPC and Kubernetes-based
deployment across AWS availability zones. Experimental results demonstrate a
throughput improvement and reduced commit latency under low packet loss
conditions, while maintaining Raft's safety and liveness guarantees.

</details>


### [85] [CFTel: A Practical Architecture for Robust and Scalable Telerobotics with Cloud-Fog Automation](https://arxiv.org/abs/2506.17991)
*Thien Tran,Jonathan Kua,Minh Tran,Honghao Lyu,Thuong Hoang,Jiong Jin*

Main category: cs.DC

TL;DR: 文章综合云雾远程机器人技术（CFTel）进展，分析相关架构和技术，展示其潜力并讨论挑战，为未来研究提供参考。


<details>
  <summary>Details</summary>
Motivation: 传统基于云的远程机器人存在延迟、可靠性等问题，阻碍关键应用实时性能，需新方案解决。

Method: 分析使能CFTel的架构框架和技术，如5G、边缘智能等。

Result: CFTel有潜力增强实时控制、可扩展性和自主性，支持面向服务的解决方案。

Conclusion: 文章为远程机器人研究人员、利益相关者和从业者提供基础参考。

Abstract: Telerobotics is a key foundation in autonomous Industrial Cyber-Physical
Systems (ICPS), enabling remote operations across various domains. However,
conventional cloud-based telerobotics suffers from latency, reliability,
scalability, and resilience issues, hindering real-time performance in critical
applications. Cloud-Fog Telerobotics (CFTel) builds on the Cloud-Fog Automation
(CFA) paradigm to address these limitations by leveraging a distributed
Cloud-Edge-Robotics computing architecture, enabling deterministic
connectivity, deterministic connected intelligence, and deterministic networked
computing. This paper synthesizes recent advancements in CFTel, aiming to
highlight its role in facilitating scalable, low-latency, autonomous, and
AI-driven telerobotics. We analyze architectural frameworks and technologies
that enable them, including 5G Ultra-Reliable Low-Latency Communication, Edge
Intelligence, Embodied AI, and Digital Twins. The study demonstrates that CFTel
has the potential to enhance real-time control, scalability, and autonomy while
supporting service-oriented solutions. We also discuss practical challenges,
including latency constraints, cybersecurity risks, interoperability issues,
and standardization efforts. This work serves as a foundational reference for
researchers, stakeholders, and industry practitioners in future telerobotics
research.

</details>


### [86] [Leveraging Cloud-Fog Automation for Autonomous Collision Detection and Classification in Intelligent Unmanned Surface Vehicles](https://arxiv.org/abs/2506.18024)
*Thien Tran,Quang Nguyen,Jonathan Kua,Minh Tran,Toan Luu,Thuong Hoang,Jiong Jin*

Main category: cs.DC

TL;DR: 提出适用于海事工业信息物理系统（ICPS）的分布式云-边-IoT架构，实验显示在计算效率等方面有提升，为智能无人水面艇提供框架。


<details>
  <summary>Details</summary>
Motivation: 现有海事ICPS受限于板载计算和通信延迟，影响实时数据处理等，限制了可扩展性和响应性。

Method: 借鉴云-雾自动化范式设计原则，提出包含云、边、IoT三层的分布式架构。

Result: 实验表明在计算效率、响应性和可扩展性方面有改进，分类准确率达86%，延迟性能提升。

Conclusion: 采用云-雾自动化可解决海事ICPS低延迟处理和可扩展性挑战，为未来海事ICPS的智能无人水面艇提供实用框架。

Abstract: Industrial Cyber-Physical Systems (ICPS) technologies are foundational in
driving maritime autonomy, particularly for Unmanned Surface Vehicles (USVs).
However, onboard computational constraints and communication latency
significantly restrict real-time data processing, analysis, and predictive
modeling, hence limiting the scalability and responsiveness of maritime ICPS.
To overcome these challenges, we propose a distributed Cloud-Edge-IoT
architecture tailored for maritime ICPS by leveraging design principles from
the recently proposed Cloud-Fog Automation paradigm. Our proposed architecture
comprises three hierarchical layers: a Cloud Layer for centralized and
decentralized data aggregation, advanced analytics, and future model
refinement; an Edge Layer that executes localized AI-driven processing and
decision-making; and an IoT Layer responsible for low-latency sensor data
acquisition. Our experimental results demonstrated improvements in
computational efficiency, responsiveness, and scalability. When compared with
our conventional approaches, we achieved a classification accuracy of 86\%,
with an improved latency performance. By adopting Cloud-Fog Automation, we
address the low-latency processing constraints and scalability challenges in
maritime ICPS applications. Our work offers a practical, modular, and scalable
framework to advance robust autonomy and AI-driven decision-making and autonomy
for intelligent USVs in future maritime ICPS.

</details>


### [87] [Edge Association Strategies for Synthetic Data Empowered Hierarchical Federated Learning with Non-IID Data](https://arxiv.org/abs/2506.18259)
*Jer Shyuan Ng,Aditya Pribadi Kalapaaking,Xiaoyu Xia,Dusit Niyato,Ibrahim Khalil,Iqbal Gondal*

Main category: cs.DC

TL;DR: 提出合成数据赋能的分层联邦学习框架，缓解非IID数据问题并激励参与者。


<details>
  <summary>Details</summary>
Motivation: 解决传统联邦学习中因工作节点掉线、非IID数据及工作节点不完全合作导致的性能问题。

Method: 引入额外边缘服务器层，边缘服务器奖励工作节点并生成、分发合成数据集，工作节点按需选择边缘服务器。

Result: 未提及具体结果。

Conclusion: 该框架可缓解非IID本地数据集的统计问题并激励工作节点参与。

Abstract: In recent years, Federated Learning (FL) has emerged as a widely adopted
privacy-preserving distributed training approach, attracting significant
interest from both academia and industry. Research efforts have been dedicated
to improving different aspects of FL, such as algorithm improvement, resource
allocation, and client selection, to enable its deployment in distributed edge
networks for practical applications. One of the reasons for the poor FL model
performance is due to the worker dropout during training as the FL server may
be located far away from the FL workers. To address this issue, an Hierarchical
Federated Learning (HFL) framework has been introduced, incorporating an
additional layer of edge servers to relay communication between the FL server
and workers. While the HFL framework improves the communication between the FL
server and workers, large number of communication rounds may still be required
for model convergence, particularly when FL workers have non-independent and
identically distributed (non-IID) data. Moreover, the FL workers are assumed to
fully cooperate in the FL training process, which may not always be true in
practical situations. To overcome these challenges, we propose a
synthetic-data-empowered HFL framework that mitigates the statistical issues
arising from non-IID local datasets while also incentivizing FL worker
participation. In our proposed framework, the edge servers reward the FL
workers in their clusters for facilitating the FL training process. To improve
the performance of the FL model given the non-IID local datasets of the FL
workers, the edge servers generate and distribute synthetic datasets to FL
workers within their clusters. FL workers determine which edge server to
associate with, considering the computational resources required to train on
both their local datasets and the synthetic datasets.

</details>


### [88] [The Power of Strong Linearizability: the Difficulty of Consistent Refereeing](https://arxiv.org/abs/2506.18401)
*Hagit Attiya,Armando Castañeda,Constantin Enea*

Main category: cs.DC

TL;DR: 研究不同对象的一致性与强线性化实现之间的关系，定义两个竞赛对象以分析强线性化能力，得出相关不可能结果。


<details>
  <summary>Details</summary>
Motivation: 探索不同对象的一致性与强线性化实现的关系，获得并发对象从各种原语实现的新结果。

Method: 考虑提供强线性化和决定性线性化的实现，定义两个竞赛对象来捕捉无锁和无等待实现中强线性化的能力。

Result: 发现几种并发对象的无锁和无等待强线性化实现需要一种弱于共识但不能用非通用原语组合强线性化实现的一致性形式，且这种一致性需要高协调能力；两个竞赛对象严格弱于共识，能捕捉强线性化并得出相关不可能结果。

Conclusion: 在无锁和无等待实现中，一致的竞赛裁判需要高协调能力，竞赛对象可用于分析多种“高级”对象的强线性化能力和相关不可能结果。

Abstract: This paper studies the relation between agreement and strongly linearizable
implementations of various objects. This leads to new results about
implementations of concurrent objects from various primitives including window
registers and interfering primitives. We consider implementations that provide
both strong linearizability and decisive linearizability.
  We identify that lock-free, respectively, wait-free, strongly linearizable
implementations of several concurrent objects entail a form of agreement that
is weaker than consensus but impossible to strongly-linearizable implement with
combinations of non-universal primitives. In both cases, lock-free and
wait-free, this form of agreement requires a distinguished process to referee a
competition that involves all other processes. Our results show that consistent
refereeing of such competitions (i.e. the outcome of the competition does not
change in extensions of the current execution) requires high coordination
power.
  More specifically, two contest objects are defined and used to capture the
power of strong linearizability in lock-free and wait-free implementations,
respectively. Both objects are strictly weaker than consensus, in the sense
that they have a wait-free linearizable (in fact, decisively linearizable)
implementation from reads and writes. The contest objects capture strong
linearizability since (1) they have strongly linearizable implementations from
several ``high-level'' objects like stacks, queues, snapshots, counters, and
therefore, impossibility results for them carry over to these objects, and (2)
they admit powerful impossibility results for strong linearizability that
involve window registers and interfering primitives, which are non-universal.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [89] [On the Power of Spatial Locality on Online Routing Problems](https://arxiv.org/abs/2506.17517)
*Swapnil Guragain,Gokarna Sharma*

Main category: cs.DS

TL;DR: 本文研究旅行商和按需接送问题的在线版本，提出空间局部性模型，展示其对提升竞争比的作用。


<details>
  <summary>Details</summary>
Motivation: 受现实应用（如Uber/Lyft打车）启发，在部分未来请求信息已知情况下，研究其对提升问题竞争比的作用。

Method: 提出空间局部性模型，该模型提前给出新请求与服务器当前位置的距离范围。

Result: 表明无论在何种度量空间，小的局部性确实有助于获得更好的竞争比。

Conclusion: 空间局部性模型提供的提前信息对提升旅行商和按需接送问题的竞争比有帮助。

Abstract: We consider the online versions of two fundamental routing problems,
traveling salesman (TSP) and dial-a-ride (DARP), which have a variety of
relevant applications in logistics and robotics. The online versions of these
problems concern with efficiently serving a sequence of requests presented in a
real-time on-line fashion located at points of a metric space by servers
(salesmen/vehicles/robots). In this paper, motivated from real-world
applications, such as Uber/Lyft rides, where some limited knowledge is
available on the future requests, we propose the {\em spatial locality} model
that provides in advance the distance within which new request(s) will be
released from the current position of server(s). We study the usefulness of
this advanced information on achieving the improved competitive ratios for both
the problems with $k\geq 1$ servers, compared to the competitive results
established in the literature without such spatial locality consideration. We
show that small locality is indeed useful in obtaining improved competitive
ratios irrespective of the metric space.

</details>


### [90] [Structural Optimal Jacobian Accumulation and Minimum Edge Count are NP-Complete Under Vertex Elimination](https://arxiv.org/abs/2506.17521)
*Matthias Bentert,Alex Crane,Pål Grønås Drange,Yosuke Mizutani,Blair D. Sullivan*

Main category: cs.DS

TL;DR: 研究算法微分中两个问题的图论公式，证明问题NP完全，给出精确算法并分析时间复杂度，还提供数据约简规则。


<details>
  <summary>Details</summary>
Motivation: 解决算法微分中两个基本问题（结构最优雅可比累积和最小边数）长期未决的问题。

Method: 考虑顶点消除操作，通过归约证明问题的NP完全性，设计$O^*(2^n)$ - 时间的精确算法。

Result: 证明两个问题是NP完全的，给出$O^*(2^n)$ - 时间的精确算法，证明在指数时间假设下运行时间本质上是紧的，为结构最优雅可比累积提供数据约简规则。

Conclusion: 解决了两个长期未决的开放问题，给出了相应的算法和时间复杂度分析，为相关研究提供了新的结果和方法。

Abstract: We study graph-theoretic formulations of two fundamental problems in
algorithmic differentiation. The first (Structural Optimal Jacobian
Accumulation) is that of computing a Jacobian while minimizing multiplications.
The second (Minimum Edge Count) is to find a minimum-size computational graph.
For both problems, we consider the vertex elimination operation. Our main
contribution is to show that both problems are NP-complete, thus resolving
longstanding open questions. In contrast to prior work, our reduction for
Structural Optimal Jacobian Accumulation does not rely on any assumptions about
the algebraic relationships between local partial derivatives; we allow these
values to be mutually independent. We also provide $O^*(2^n)$-time exact
algorithms for both problems, and show that under the exponential time
hypothesis these running times are essentially tight. Finally, we provide a
data reduction rule for Structural Optimal Jacobian Accumulation by showing
that false twins may always be eliminated consecutively.

</details>


### [91] [Faster Low-Rank Approximation and Kernel Ridge Regression via the Block-Nyström Method](https://arxiv.org/abs/2506.17556)
*Sachin Garg,Michał Dereziński*

Main category: cs.DS

TL;DR: 提出Block - Nyström算法改进Nyström方法，降低计算成本，可用于优化和统计学习。


<details>
  <summary>Details</summary>
Motivation: 当数据谱衰减为重尾时，Nyström方法计算成本高，超出预算。

Method: 在Nyström方法中引入块对角结构，提出Block - Nyström算法；给出递归预条件方案求逆矩阵。

Result: 可构建二阶优化的预条件子，有效解决核岭回归问题，得到更强的谱尾估计和新的统计学习界。

Conclusion: Block - Nyström算法在降低计算成本的同时能保证强近似性，适用于优化和统计学习。

Abstract: The Nystr\"om method is a popular low-rank approximation technique for large
matrices that arise in kernel methods and convex optimization. Yet, when the
data exhibits heavy-tailed spectral decay, the effective dimension of the
problem often becomes so large that even the Nystr\"om method may be outside of
our computational budget. To address this, we propose Block-Nystr\"om, an
algorithm that injects a block-diagonal structure into the Nystr\"om method,
thereby significantly reducing its computational cost while recovering strong
approximation guarantees. We show that Block-Nystr\"om can be used to construct
improved preconditioners for second-order optimization, as well as to
efficiently solve kernel ridge regression for statistical learning over Hilbert
spaces. Our key technical insight is that, within the same computational
budget, combining several smaller Nystr\"om approximations leads to stronger
tail estimates of the input spectrum than using one larger approximation. Along
the way, we provide a novel recursive preconditioning scheme for efficiently
inverting the Block-Nystr\"om matrix, and provide new statistical learning
bounds for a broad class of approximate kernel ridge regression solvers.

</details>


### [92] [Contextual Pattern Mining and Counting](https://arxiv.org/abs/2506.17613)
*Ling Li,Daniel Gibney,Sharma V. Thankachan,Solon P. Pissis,Grigorios Loukides*

Main category: cs.DS

TL;DR: 本文引入上下文模式挖掘（CPM）和上下文模式计数（CPC）问题，分别提出对应算法和索引，并通过实验验证其有效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 解决与字符串上下文相关的挖掘和计数问题，处理大规模数据集。

Method: 为CPM提出线性工作算法，可使用内外存；为CPC提出特定空间索引，并通过优化提升性能。

Result: CPM外部内存版本能处理大数据集，运行时间与内部内存版本相当；优化后的CPC索引在多方面优于现有方法。

Conclusion: 所提算法和索引在处理字符串上下文问题上表现良好，有实际应用价值。

Abstract: Given a string $P$ of length $m$, a longer string $T$ of length $n>m$, and
two integers $l\geq 0$ and $r\geq 0$, the context of $P$ in $T$ is the set of
all string pairs $(L,R)$, with $|L|=l$ and $|R|=r$, such that the string $LPR$
occurs in $T$. We introduce two problems related to the notion of context: (1)
the Contextual Pattern Mining (CPM) problem, which given $T$, $(m,l,r)$, and an
integer $\tau>0$, asks for outputting the context of each substring $P$ of
length $m$ of $T$, provided that the size of the context of $P$ is at least
$\tau$; and (2) the Contextual Pattern Counting (CPC) problem, which asks for
preprocessing $T$ so that the size of the context of a given query string $P$
of length $m$ can be found efficiently.
  For CPM, we propose a linear-work algorithm that either uses only internal
memory, or a bounded amount of internal memory and external memory, which
allows much larger datasets to be handled. For CPC, we propose an
$\widetilde{\mathcal{O}}(n)$-space index that can be constructed in
$\widetilde{\mathcal{O}}n)$ time and answers queries in
$\mathcal{O}(m)+\widetilde{\mathcal{O}}(1)$ time. We further improve the
practical performance of the CPC index by optimizations that exploit the LZ77
factorization of $T$ and an upper bound on the query length. Using
billion-letter datasets from different domains, we show that the external
memory version of our CPM algorithm can deal with very large datasets using a
small amount of internal memory while its runtime is comparable to that of the
internal memory version. Interestingly, we also show that our optimized index
for CPC outperforms an approach based on the state of the art for the reporting
version of CPC [Navarro, SPIRE 2020] in terms of query time, index size,
construction time, and construction space, often by more than an order of
magnitude.

</details>


### [93] [Optimizing Periodic Operations for Efficient Inland Waterway Lock Management](https://arxiv.org/abs/2506.17743)
*Julian Golak,Alexander Grigoriev,Freija van Lent,Tom van der Zanden*

Main category: cs.DS

TL;DR: 研究评估周期性船闸调度管理船只交通的成本，开发算法解决相关问题，实验表明直观策略常优于最优策略。


<details>
  <summary>Details</summary>
Motivation: 内河航道船闸高效运行影响拥堵与运输不确定性，周期性调度简单但可能增加等待时间，需评估其成本。

Method: 先开发算法估算与不规则船只到达数据匹配的周期性到达模式，再将其作为输入计算运营调度，给出不同复杂度算法和近似方案。

Result: 数值实验用AIS数据构建周期性到达模式，直观策略在实际数据评估中常优于针对周期模式训练的最优策略。

Conclusion: 直观且简单的船闸调度策略在实际应用中可能比专门针对周期性模式训练的最优策略更有效。

Abstract: In inland waterways, the efficient management of water lock operations
impacts the level of congestion and the resulting uncertainty in inland
waterway transportation. To achieve reliable and efficient traffic, schedules
should be easy to understand and implement, reducing the likelihood of errors.
The simplest schedules follow periodic patterns, reducing complexity and
facilitating predictable management. Since vessels do not arrive in perfectly
regular intervals, periodic schedules may lead to more wait time. The aim of
this research is to estimate this cost by evaluating how effective these
periodic schedules manage vessel traffic at water locks. The first objective is
to estimate a periodic arrival pattern that closely matches a dataset of
irregular vessel arrivals at a specific lock. We develop an algorithm that,
given a fixed number of vessel streams, solves the problem in polynomial time.
The solution then serves as input for the subsequent part, where we consider
algorithms that compute operational schedules by formulating an optimisation
problem with periodic arrival patterns as input, and the goal is to determine a
periodic schedule that minimises the long-run average waiting time of vessels.
We present a polynomial-time algorithm for the two-stream case and a
pseudo-polynomial-time algorithm for the general case, along with incremental
polynomial-time approximation schemes. In our numerical experiments, use AIS
data to construct a periodic arrival pattern closely matching the observed
data. Our experiments demonstrate that when evaluated against actual data,
intuitive and straightforward policies often outperform optimal policies
specifically trained on the periodic arrival pattern.

</details>


### [94] [Semirandom Planted Clique via 1-norm Isometry Property](https://arxiv.org/abs/2506.17916)
*Venkatesan Guruswami,Hsin-Po Wang*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We give a polynomial-time algorithm that finds a planted clique of size $k
\ge \sqrt{n \log n}$ in the semirandom model, improving the state-of-the-art
$\sqrt{n} (\log n)^2$ bound. This $\textit{semirandom planted clique problem}$
concerns finding the planted subset $S$ of $k$ vertices of a graph $G$ on $V$,
where the induced subgraph $G[S]$ is complete, the cut edges in $G[S; V
\setminus S]$ are random, and the remaining edges in $G[V \setminus S]$ are
adversarial.
  An elegant greedy algorithm by Blasiok, Buhai, Kothari, and Steurer [BBK24]
finds $S$ by sampling inner products of the columns of the adjacency matrix of
$G$, and checking if they deviate significantly from typical inner products of
random vectors. Their analysis uses a suitably random matrix that, with high
probability, satisfies a certain restricted isometry property. Inspired by
Wootters's work on list decoding, we put forth and implement the $1$-norm
analog of this argument, and quantitatively improve their analysis to work all
the way up to the conjectured optimal $\sqrt{n \log n}$ bound on clique size,
answering one of the main open questions posed in [BBK24].

</details>


### [95] [Fully-Dynamic Parallel Algorithms for Single-Linkage Clustering](https://arxiv.org/abs/2506.18384)
*Quinten De Man,Laxman Dhulipala,Kishen N Gowda*

Main category: cs.DS

TL;DR: 本文研究全动态环境下维护单链树状图（SLD）的问题，提出多种更新算法，速度优于已知静态算法。


<details>
  <summary>Details</summary>
Motivation: 先前工作未提供渐近快于从头重新计算的SLD更新算法，本文旨在解决全动态环境下高效更新SLD的问题。

Method: 针对动态森林F的边插入和删除操作设计算法，包括插入、删除算法，还给出并行和批量并行版本。

Result: 插入算法时间复杂度为O(h)，删除算法为O(h log (1+n/h))；有工作高效或近乎工作高效且具有多对数深度的并行版本；插入操作可在O(c log(1+n/c))时间内近乎最优完成。

Conclusion: 提出的算法在全动态环境下能渐近快于已知静态算法更新SLD，在多种情况下表现更优。

Abstract: Single-linkage clustering is a popular form of hierarchical agglomerative
clustering (HAC) where the distance between two clusters is defined as the
minimum distance between any pair of points across the two clusters. In
single-linkage HAC, the output is typically the single-linkage dendrogram
(SLD), which is the binary tree representing the hierarchy of clusters formed
by iteratively contracting the two closest clusters. In the dynamic setting,
prior work has only studied maintaining a minimum spanning forest over the data
since single-linkage HAC reduces to computing the SLD on the minimum spanning
forest of the data.
  In this paper, we study the problem of maintaining the SLD in the
fully-dynamic setting. We assume the input is a dynamic forest $F$
(representing the minimum spanning forest of the data) which receives a
sequence of edge insertions and edge deletions. To our knowledge, no prior work
has provided algorithms to update an SLD asymptotically faster than recomputing
it from scratch. All of our update algorithms are asymptotically faster than
the best known static SLD computation algorithm, which takes $O(n \log h)$ time
where $h$ is the height of the dendrogram ($h \leq n-1$). Furthermore, our
algorithms are much faster in many cases, such as when $h$ is low. Our first
set of results are an insertion algorithm in $O(h)$ time and a deletion
algorithm in $O(h \log (1+n/h))$ time. Next, we describe parallel and
batch-parallel versions of these algorithms which are work-efficient or nearly
work-efficient and have poly-logarithmic depth. Finally, we show how to perform
insertions near-optimally in $O(c \log(1+n/c))$ time, where $c$ is the number
of structural changes in the dendrogram caused by the update, and give a
work-efficient parallel version of this algorithm that has polylogarithmic
depth.

</details>


### [96] [Tight simulation of a distribution using conditional samples](https://arxiv.org/abs/2506.18444)
*Tomer Adar*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present an algorithm for simulating a distribution using prefix
conditional samples (Adar, Fischer and Levi, 2024), as well as
``prefix-compatible'' conditional models such as the interval model (Cannone,
Ron and Servedio, 2015) and the subcube model (CRS15, Bhattacharyya and
Chakraborty, 2018). The conditional sample complexity is $O(\log^2 N /
\varepsilon^2)$ prefix conditional samples per query, which improves on the
previously known $\tilde{O}(\log^3 N / \varepsilon^2)$ (Kumar, Meel and Pote,
2025). Moreover, our simulating distribution is $O(\varepsilon^2)$-close to the
input distribution with respect to the Kullback-Leibler divergence, which is
stricter than the usual guarantee of being $O(\varepsilon)$-close with respect
to the total-variation distance.
  We show that our algorithm is tight with respect to the highly-related task
of estimation: every algorithm that is able to estimate the mass of individual
elements within $(1 \pm \varepsilon)$-multiplicative error must make
$\Omega(\log^2 N / \varepsilon^2)$ prefix conditional samples per element.

</details>


### [97] [Near-Optimal Dynamic Policies for Joint Replenishment in Continuous/Discrete Time](https://arxiv.org/abs/2506.18491)
*Danny Segev*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: While dynamic policies have historically formed the foundation of most
influential papers dedicated to the joint replenishment problem, we are still
facing profound gaps in our structural understanding of optimal such policies
as well as in their surrounding computational questions. To date, the seminal
work of Roundy (1985, 1986) and Jackson et al. (1985) remains unsurpassed in
efficiently developing provably-good dynamic policies in this context.
  The principal contribution of this paper consists in developing a wide range
of algorithmic ideas and analytical insights around the continuous-time joint
replenishment problem, culminating in a deterministic framework for efficiently
approximating optimal dynamic policies to any desired level of accuracy. These
advances enable us to derive a compactly-encoded replenishment policy whose
long-run average cost is within factor $1 + \epsilon$ of the dynamic optimum,
arriving at an efficient polynomial-time approximation scheme (EPTAS).
Technically speaking, our approach hinges on affirmative resolutions to two
fundamental open questions:
  -- We devise the first efficient discretization-based framework for
approximating the joint replenishment problem. Specifically, we prove that
every continuous-time infinite-horizon instance can be reduced to a
corresponding discrete-time $O( \frac{ n^3 }{ \epsilon^6 } )$-period instance,
while incurring a multiplicative optimality loss of at most $1 + \epsilon$.
  -- Motivated by this relation, we substantially improve on the $O(
2^{2^{O(1/\epsilon)}} \cdot (nT)^{ O(1) } )$-time approximation scheme of
Nonner and Sviridenko (2013) for the discrete-time joint replenishment problem.
Beyond an exponential improvement in running time, we demonstrate that
randomization and hierarchical decompositions can be entirely avoided, while
concurrently offering a relatively simple analysis.

</details>


### [98] [Learning Partitions with Optimal Query and Round Complexities](https://arxiv.org/abs/2505.05009)
*Hadley Black,Arya Mazumdar,Barna Saha*

Main category: cs.DS

TL;DR: 本文研究用简单查询学习元素分区问题，给出确定性查询复杂度与轮数关系，分析子集查询算法上下界。


<details>
  <summary>Details</summary>
Motivation: 在多个领域应用中，降低查询适应性并最小化查询复杂度有重要意义。

Method: 对学习未知分区问题进行分析，推导不同设置下的查询复杂度；对成对查询扩展到子集查询进行研究。

Result: 得到确定性查询复杂度与轮数的关系；非自适应强查询至少需$\Omega(n^2/s^2)$，弱查询非自适应算法在一定条件下接近该界；得到子集查询算法上下界。

Conclusion: 完成对问题确定性查询复杂度的刻画，为相关应用在查询复杂度和适应性上提供理论支持。

Abstract: We consider the basic problem of learning an unknown partition of $n$
elements into at most $k$ sets using simple queries that reveal information
about a small subset of elements. Our starting point is the well-studied
pairwise same-set queries which ask if a pair of elements belong to the same
class. It is known that non-adaptive algorithms require $\Theta(n^2)$ queries,
while adaptive algorithms require $\Theta(nk)$ queries, and the best known
algorithm uses $k-1$ rounds. This problem has been studied extensively over the
last two decades in multiple communities due to its fundamental nature and
relevance to clustering, active learning, and crowd sourcing. In many
applications, it is of high interest to reduce adaptivity while minimizing
query complexity. We give a complete characterization of the deterministic
query complexity of this problem as a function of the number of rounds, $r$,
interpolating between the non-adaptive and adaptive settings: for any constant
$r$, the query complexity is
$\Theta(n^{1+\frac{1}{2^r-1}}k^{1-\frac{1}{2^r-1}})$. Our algorithm only needs
$O(\log \log n)$ rounds to attain the optimal $O(nk)$ query complexity.
  Next, we consider two generalizations of pairwise queries to subsets $S$ of
size at most $s$: (1) weak subset queries which return the number of classes
intersected by $S$, and (2) strong subset queries which return the entire
partition restricted on $S$. Once again in crowd sourcing applications, queries
on large sets may be prohibitive. For non-adaptive algorithms, we show
$\Omega(n^2/s^2)$ strong queries are needed. Perhaps surprisingly, we show that
there is a non-adaptive algorithm using weak queries that matches this bound up
to log-factors for all $s \leq \sqrt{n}$. More generally, we obtain nearly
matching upper and lower bounds for algorithms using subset queries in terms of
both the number of rounds, $r$, and the query size bound, $s$.

</details>


### [99] [Optimal Graph Reconstruction by Counting Connected Components in Induced Subgraphs](https://arxiv.org/abs/2506.08405)
*Hadley Black,Arya Mazumdar,Barna Saha,Yinzhan Xu*

Main category: cs.DS

TL;DR: 本文提出关于连通分量数量的新查询模型，分析自适应和非自适应重建图所需查询次数，并给出两轮自适应查询算法。


<details>
  <summary>Details</summary>
Motivation: 图重建问题在多种查询模型下被广泛研究，提出关于连通分量数量这一基本图参数的新查询模型。

Method: 考虑用查询诱导子图连通分量数量的方式重建n节点m边图，分析自适应和非自适应查询情况。

Result: 自适应重建图期望需要Θ((m log n)/(log m))次查询，非自适应即使m = O(n)也需要Ω(n^2)次查询，给出O(m log n + n log^2 n)次查询的两轮自适应算法。

Conclusion: 明确新查询模型下自适应和非自适应重建图的查询复杂度，提供高效两轮自适应算法。

Abstract: The graph reconstruction problem has been extensively studied under various
query models. In this paper, we propose a new query model regarding the number
of connected components, which is one of the most basic and fundamental graph
parameters. Formally, we consider the problem of reconstructing an $n$-node
$m$-edge graph with oracle queries of the following form: provided with a
subset of vertices, the oracle returns the number of connected components in
the induced subgraph. We show $\Theta(\frac{m \log n}{\log m})$ queries in
expectation are both sufficient and necessary to adaptively reconstruct the
graph. In contrast, we show that $\Omega(n^2)$ non-adaptive queries are
required, even when $m = O(n)$. We also provide an $O(m\log n + n\log^2 n)$
query algorithm using only two rounds of adaptivity.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [100] [Ultra-Efficient Contracts: Breaking the Substitutes Barrier in Combinatorial Contracts](https://arxiv.org/abs/2506.18008)
*Michal Feldman,Liat Yashin*

Main category: cs.GT

TL;DR: 研究组合合约框架下的最优合约问题，证明可处理性扩展到更广泛的Ultra函数类，还处理了更复杂的成本情况。


<details>
  <summary>Details</summary>
Motivation: 此前研究表明奖励函数为GS时最优合约可多项式时间计算，Submodular函数问题为NP难，探讨GS是否是可处理性的真正边界。

Method: 提出新的技术，因之前方法依赖GS的子模性。

Result: 证明可处理性扩展到更广泛的Ultra函数类，GS是Ultra和Submodular函数的交集，且是Ultra而非Submodular驱动可处理性；还处理了加法加对称的成本。

Conclusion: 这是Ultra函数在重要经济场景中的首次应用，推翻了认为GS的子模性是关键的普遍看法。

Abstract: We study the optimal contract problem in the framework of combinatorial
contracts, introduced by Duetting et al. [FOCS'21], where a principal delegates
the execution of a project to an agent, and the agent can choose any subset
from a given set of costly actions. At the core of the model is a reward
function - a monotone set function that maps each set of actions taken by the
agent into an expected reward to the principal. To incentivize the agent, the
principal offers a contract specifying the fraction of the reward to be paid,
and the agent responds with their optimal action set. The goal is to compute
the contract that maximizes the principal's expected utility.
  Previous work showed that when the reward function is gross substitutes (GS),
the optimal contract can be computed in polynomial time, but the problem is
NP-hard for the broader class of Submodular functions. This raised the
question: is GS the true boundary of tractability for the optimal contract
problem? We prove that tractability extends to the strictly broader class of
Ultra functions. Interestingly, GS constitutes precisely the intersection of
Ultra and Submodular functions, and our result reveals that it is Ultra - not
Submodular - that drives tractability, overturning the prevailing belief that
the submodularity component of GS is essential. We further extend tractability
beyond additive costs, handling costs that are additive plus symmetric. Our
results require new techniques, as prior approaches relied on the submodularity
of GS. To the best of our knowledge, this is the first application of Ultra
functions in a prominent economic setting.

</details>


### [101] [Thresholds for sensitive optimality and Blackwell optimality in stochastic games](https://arxiv.org/abs/2506.18545)
*Stéphane Gaubert,Julien Grand-Clément,Ricardo D. Katz*

Main category: cs.GT

TL;DR: 研究两人零和完美信息随机博弈中平均支付准则的改进，给出d - 敏感阈值新界限及布莱克威尔阈值改进界限。


<details>
  <summary>Details</summary>
Motivation: 界定布莱克威尔阈值和d - 敏感阈值是算法博弈论的基本问题，这些阈值控制计算最优策略的复杂度。

Method: 利用代数数的分离界限，依赖拉格朗日界限及基于马勒测度和重数定理的更高级技术。

Result: 给出除d = - 1情况外d - 敏感阈值α_d的首个界限，改进布莱克威尔阈值α_Bw的界限。

Conclusion: 通过特定技术可实现对d - 敏感阈值和布莱克威尔阈值界限的界定和改进。

Abstract: We investigate refinements of the mean-payoff criterion in two-player
zero-sum perfect-information stochastic games. A strategy is Blackwell optimal
if it is optimal in the discounted game for all discount factors sufficiently
close to $1$. The notion of $d$-sensitive optimality interpolates between
mean-payoff optimality (corresponding to the case $d=-1$) and Blackwell
optimality ($d=+\infty$). The Blackwell threshold $\alpha_{\sf Bw} \in [0,1[$
is the discount factor above which all optimal strategies in the discounted
game are guaranteed to be Blackwell optimal. The $d$-sensitive threshold
$\alpha_{\sf d} \in [0,1[$ is defined analogously. Bounding $\alpha_{\sf Bw}$
and $\alpha_{\sf d}$ are fundamental problems in algorithmic game theory, since
these thresholds control the complexity for computing Blackwell and
$d$-sensitive optimal strategies, by reduction to discounted games which can be
solved in $O\left((1-\alpha)^{-1}\right)$ iterations. We provide the first
bounds on the $d$-sensitive threshold $\alpha_{\sf d}$ beyond the case $d=-1$,
and we establish improved bounds for the Blackwell threshold $\alpha_{\sf Bw}$.
This is achieved by leveraging separation bounds on algebraic numbers, relying
on Lagrange bounds and more advanced techniques based on Mahler measures and
multiplicity theorems.

</details>


### [102] [Agentic Markets: Game Dynamics and Equilibrium in Markets with Learning Agents](https://arxiv.org/abs/2506.18571)
*Martin Bichler,Julius Durmann,Matthias Oberlechner*

Main category: cs.GT

TL;DR: 文章围绕自主学习智能体参与市场的情况，调研以动态系统建模智能体行为的进展，分析其何时及如何收敛到市场博弈均衡，为分析此类市场稳定性和收敛性提供数学基础。


<details>
  <summary>Details</summary>
Motivation: 游戏理论无法解释学习智能体的重复互动如何达到特定结果，需要理解自动化市场何时能实现有效均衡。

Method: 调研以动态系统建模智能体行为的进展，重点关注投影梯度和无后悔学习算法，利用变分不等式和李雅普诺夫稳定性理论进行分析。

Result: 学习在博弈中可能导致包括收敛到均衡、循环和混沌行为等各种动态。

Conclusion: 为分析由自主学习智能体驱动的智能市场的稳定性和收敛性提供了数学基础。

Abstract: Autonomous and learning agents increasingly participate in markets - setting
prices, placing bids, ordering inventory. Such agents are not just aiming to
optimize in an uncertain environment; they are making decisions in a
game-theoretical environment where the decision of one agent influences the
profit of other agents. While game theory usually predicts outcomes of
strategic interaction as an equilibrium, it does not capture how repeated
interaction of learning agents arrives at a certain outcome. This article
surveys developments in modeling agent behavior as dynamical systems, with a
focus on projected gradient and no-regret learning algorithms. In general,
learning in games can lead to all types of dynamics, including convergence to
equilibrium, but also cycles and chaotic behavior. It is important to
understand when we can expect efficient equilibrium in automated markets and
when this is not the case. Thus, we analyze when and how learning agents
converge to an equilibrium of a market game, drawing on tools from variational
inequalities and Lyapunov stability theory. Special attention is given to the
stability of projected dynamics and the convergence to equilibrium sets as
limiting outcomes. Overall, the paper provides mathematical foundations for
analyzing stability and convergence in agentic markets driven by autonomous,
learning agents.

</details>


### [103] [Robust Committee Voting, or The Other Side of Representation](https://arxiv.org/abs/2506.18643)
*Gregory Kehne,Ulrike Schmidt-Kraepelin,Krzysztof Sornat*

Main category: cs.GT

TL;DR: 从新视角研究基于批准的委员会投票，开发随机投票规则，证明算法稳定性，展示规则在其他场景的实用性。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要围绕选民的比例代表制，本文将重点转移到候选人，同时保留比例性。

Method: 开发满足事前中立性、单调性和连续性，同时保持强事后比例性保证的随机投票规则；引入Softmax - GJCR算法；通过不同方式获得更强稳定性保证。

Result: Softmax - GJCR算法满足EJR+事后要求，保证事前单调性和中立性，提供$O(k^3/n)$ - 稳定性；展示规则在在线动态委员会投票和隐私敏感领域的应用。

Conclusion: 稳定的投票规则在不同场景有实用性，如低预期追索的动态投票规则和隐私敏感领域。

Abstract: We study approval-based committee voting from a novel perspective. While
extant work largely centers around proportional representation of the voters,
we shift our focus to the candidates while preserving proportionality.
Intuitively, candidates supported by similar voter groups should receive
comparable representation. Since deterministic voting rules cannot achieve this
ideal, we develop randomized voting rules that satisfy ex-ante neutrality,
monotonicity, and continuity, while maintaining strong ex-post proportionality
guarantees.
  Continuity of the candidate selection probabilities proves to be the most
demanding of our ex-ante desiderata. We provide it via voting rules that are
algorithmically stable, a stronger notion of robustness which captures the
continuity of the committee distribution under small changes. First, we
introduce Softmax-GJCR, a randomized variant of the Greedy Justified Candidate
Rule (GJCR) [Brill and Peters, 2023], which carefully leverages slack in GJCR
to satisfy our ex-ante properties. This polynomial-time algorithm satisfies
EJR+ ex post, assures ex-ante monotonicity and neutrality, and provides
$O(k^3/n)$-stability (ignoring $\log$ factors). Building on our techniques for
Softmax-GJCR, we further show that stronger stability guarantees can be
attained by (i) allowing exponential running time, (ii) relaxing EJR+ to an
approximate $\alpha$-EJR+, and (iii) relaxing EJR+ to JR.
  We finally demonstrate the utility of stable voting rules in other settings.
In online dynamic committee voting, we show that stable voting rules imply
dynamic voting rules with low expected recourse, and illustrate this reduction
for Softmax-GJCR. Our voting rules also satisfy a stronger form of stability
that coincides with differential privacy, suggesting their applicability in
privacy-sensitive domains.

</details>


### [104] [Fair Allocation with Money: What is Your Objective?](https://arxiv.org/abs/2506.18794)
*Noga Klein Elmalem,Rica Gonen,Erel Segal-Halevi*

Main category: cs.GT

TL;DR: 比较分配不可分物品时使用货币转移消除嫉妒的不同模型及优化目标上下界关系


<details>
  <summary>Details</summary>
Motivation: 探讨分配不可分物品时不同货币转移方式消除嫉妒及优化支付金额的问题

Method: 对不同使用货币转移消除嫉妒的模型进行比较

Result: 无明确提及

Conclusion: 无明确提及

Abstract: When allocating indivisible items, there are various ways to use monetary
transfers for eliminating envy. Particularly, one can apply a balanced vector
of transfer payments, or charge each agent a positive amount, or -- contrarily
-- give each agent a positive amount as a ``subsidy''. In each model, one can
aim to minimize the amount of payments used; this aim translates into different
optimization objectives in each setting. This note compares the various models,
and the relations between upper and lower bounds for these objectives.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [105] [QUST_NLP at SemEval-2025 Task 7: A Three-Stage Retrieval Framework for Monolingual and Crosslingual Fact-Checked Claim Retrieval](https://arxiv.org/abs/2506.17272)
*Youzheng Liu,Jiyan Liu,Xiaoman Xu,Taihang Wang,Yimin Wang,Ye Jiang*

Main category: cs.IR

TL;DR: 本文介绍QUST_NLP参加SemEval - 2025任务7的情况，提出三阶段检索框架用于事实核查声明检索，取得一定排名并开源代码。


<details>
  <summary>Details</summary>
Motivation: 参与SemEval - 2025任务7，解决事实核查声明检索问题。

Method: 提出三阶段检索框架，先评估检索模型选最佳者进行候选检索，再用多模型重排序选Top - 10结果，最后用加权投票确定最终结果。

Result: 在单语赛道获第5名，跨语言赛道获第7名。

Conclusion: 所提出的三阶段检索框架在SemEval - 2025任务7中取得了较好的效果，代码已开源。

Abstract: This paper describes the participation of QUST_NLP in the SemEval-2025 Task
7. We propose a three-stage retrieval framework specifically designed for
fact-checked claim retrieval. Initially, we evaluate the performance of several
retrieval models and select the one that yields the best results for candidate
retrieval. Next, we employ multiple re-ranking models to enhance the candidate
results, with each model selecting the Top-10 outcomes. In the final stage, we
utilize weighted voting to determine the final retrieval outcomes. Our approach
achieved 5th place in the monolingual track and 7th place in the crosslingual
track. We release our system code at:
https://github.com/warmth27/SemEval2025_Task7

</details>


### [106] [Chunk Twice, Embed Once: A Systematic Study of Segmentation and Representation Trade-offs in Chemistry-Aware Retrieval-Augmented Generation](https://arxiv.org/abs/2506.17277)
*Mahmoud Amiri,Thomas Bocklitz*

Main category: cs.IR

TL;DR: 对化学领域RAG系统的分块策略和嵌入模型进行大规模评估，给出构建高效系统的指导。


<details>
  <summary>Details</summary>
Motivation: RAG系统在化学领域重要，但特定领域的基础设计选择研究不足。

Method: 对5类方法家族的25种分块配置和48种嵌入模型，在三个化学特定基准上进行评估。

Result: 递归基于令牌的分块（R100 - 0）表现最佳，检索优化嵌入模型优于领域专用模型。

Conclusion: 通过发布数据集等，为构建化学感知RAG系统提供可行指导。

Abstract: Retrieval-Augmented Generation (RAG) systems are increasingly vital for
navigating the ever-expanding body of scientific literature, particularly in
high-stakes domains such as chemistry. Despite the promise of RAG, foundational
design choices -- such as how documents are segmented and represented -- remain
underexplored in domain-specific contexts. This study presents the first
large-scale, systematic evaluation of chunking strategies and embedding models
tailored to chemistry-focused RAG systems. We investigate 25 chunking
configurations across five method families and evaluate 48 embedding models on
three chemistry-specific benchmarks, including the newly introduced
QuestChemRetrieval dataset. Our results reveal that recursive token-based
chunking (specifically R100-0) consistently outperforms other approaches,
offering strong performance with minimal resource overhead. We also find that
retrieval-optimized embeddings -- such as Nomic and Intfloat E5 variants --
substantially outperform domain-specialized models like SciBERT. By releasing
our datasets, evaluation framework, and empirical benchmarks, we provide
actionable guidelines for building effective and efficient chemistry-aware RAG
systems.

</details>


### [107] [CORONA: A Coarse-to-Fine Framework for Graph-based Recommendation with Large Language Models](https://arxiv.org/abs/2506.17281)
*Junze Chen,Xinjie Yang,Cheng Yang,Junfei Bao,Zeyuan Guo,Yawen Li,Chuan Shi*

Main category: cs.IR

TL;DR: 提出CORONA框架，在候选过滤中利用大语言模型推理能力结合图神经网络提升推荐系统性能，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 以往研究仅将大语言模型用于重排序或数据增强，未在候选过滤中发挥其能力，导致性能不佳。

Method: 提出CORONA框架，包含偏好辅助检索、意图辅助检索和图神经网络增强检索三个步骤。

Result: 在多个数据集和设置上实验，CORONA平均召回率相对提升18.6%，NDCG相对提升18.4%。

Conclusion: CORONA框架在检索过程中利用大语言模型推理能力，结合图神经网络，实现了最先进的推荐性能。

Abstract: Recommender systems (RSs) are designed to retrieve candidate items a user
might be interested in from a large pool. A common approach is using graph
neural networks (GNNs) to capture high-order interaction relationships. As
large language models (LLMs) have shown strong capabilities across domains,
researchers are exploring their use to enhance recommendation. However, prior
work limits LLMs to re-ranking results or dataset augmentation, failing to
utilize their power during candidate filtering - which may lead to suboptimal
performance. Instead, we propose to leverage LLMs' reasoning abilities during
the candidate filtering process, and introduce Chain Of Retrieval ON grAphs
(CORONA) to progressively narrow down the range of candidate items on
interaction graphs with the help of LLMs: (1) First, LLM performs preference
reasoning based on user profiles, with the response serving as a query to
extract relevant users and items from the interaction graph as
preference-assisted retrieval; (2) Then, using the information retrieved in the
previous step along with the purchase history of target user, LLM conducts
intent reasoning to help refine an even smaller interaction subgraph as
intent-assisted retrieval; (3) Finally, we employ a GNN to capture high-order
collaborative filtering information from the extracted subgraph, performing
GNN-enhanced retrieval to generate the final recommendation results. The
proposed framework leverages the reasoning capabilities of LLMs during the
retrieval process, while seamlessly integrating GNNs to enhance overall
recommendation performance. Extensive experiments on various datasets and
settings demonstrate that our proposed CORONA achieves state-of-the-art
performance with an 18.6% relative improvement in recall and an 18.4% relative
improvement in NDCG on average.

</details>


### [108] [Automating Financial Statement Audits with Large Language Models](https://arxiv.org/abs/2506.17282)
*Rushi Wang,Jiateng Liu,Weijie Zhao,Shenglan Li,Denghui Zhang*

Main category: cs.IR

TL;DR: 利用大语言模型（LLMs）自动化财务报表审计，构建基准测试并评估其能力，发现当前LLMs存在不足，为未来研究指明方向。


<details>
  <summary>Details</summary>
Motivation: 当前财务报表审计的人工流程效率低且易出错，需要自动化解决方案。

Method: 引入综合基准测试，结合真实财务表格和合成交易数据，开发五阶段评估框架，通过测试用例模拟实际审计场景。

Result: 当前最先进的LLMs能识别错误，但在解释错误、引用会计准则、执行完整审计和修订报表方面存在明显局限。

Conclusion: 当前LLMs在特定会计领域知识存在关键差距，未来研究应增强其对审计原则和程序的理解，基准测试和评估框架为开发更有效的自动化审计工具奠定基础。

Abstract: Financial statement auditing is essential for stakeholders to understand a
company's financial health, yet current manual processes are inefficient and
error-prone. Even with extensive verification procedures, auditors frequently
miss errors, leading to inaccurate financial statements that fail to meet
stakeholder expectations for transparency and reliability. To this end, we
harness large language models (LLMs) to automate financial statement auditing
and rigorously assess their capabilities, providing insights on their
performance boundaries in the scenario of automated auditing. Our work
introduces a comprehensive benchmark using a curated dataset combining
real-world financial tables with synthesized transaction data. In the
benchmark, we developed a rigorous five-stage evaluation framework to assess
LLMs' auditing capabilities. The benchmark also challenges models to map
specific financial statement errors to corresponding violations of accounting
standards, simulating real-world auditing scenarios through test cases. Our
testing reveals that current state-of-the-art LLMs successfully identify
financial statement errors when given historical transaction data. However,
these models demonstrate significant limitations in explaining detected errors
and citing relevant accounting standards. Furthermore, LLMs struggle to execute
complete audits and make necessary financial statement revisions. These
findings highlight a critical gap in LLMs' domain-specific accounting
knowledge. Future research must focus on enhancing LLMs' understanding of
auditing principles and procedures. Our benchmark and evaluation framework
establish a foundation for developing more effective automated auditing tools
that will substantially improve the accuracy and efficiency of real-world
financial statement auditing.

</details>


### [109] [A Framework for Generating Conversational Recommendation Datasets from Behavioral Interactions](https://arxiv.org/abs/2506.17285)
*Vinaik Chhetri,Yousaf Reza,Moghis Fereidouni,Srijata Maji,Umar Farooq,AB Siddique*

Main category: cs.IR

TL;DR: 提出ConvRecStudio框架用LLM模拟对话，应用于三个领域生成对话，评估显示对话质量高，模型在指标上有提升。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统的协同过滤和对话推荐各有不足，统一两者有挑战，因缺乏真实用户行为的大规模对话数据集。

Method: 采用三阶段管道，包括时间分析、语义对话规划和多轮模拟，使用配对LLM代理并进行保真检查。

Result: 在三个领域生成超12K多轮对话，评估显示对话自然、连贯且基于行为，模型在指标上有提升，如在Yelp上Hit@1提升10.9%。

Conclusion: ConvRecStudio框架能有效模拟对话，统一两种推荐范式，所建模型效果好。

Abstract: Modern recommendation systems typically follow two complementary paradigms:
collaborative filtering, which models long-term user preferences from
historical interactions, and conversational recommendation systems (CRS), which
interact with users in natural language to uncover immediate needs. Each
captures a different dimension of user intent. While CRS models lack
collaborative signals, leading to generic or poorly personalized suggestions,
traditional recommenders lack mechanisms to interactively elicit immediate
needs. Unifying these paradigms promises richer personalization but remains
challenging due to the lack of large-scale conversational datasets grounded in
real user behavior. We present ConvRecStudio, a framework that uses large
language models (LLMs) to simulate realistic, multi-turn dialogs grounded in
timestamped user-item interactions and reviews. ConvRecStudio follows a
three-stage pipeline: (1) Temporal Profiling, which constructs user profiles
and community-level item sentiment trajectories over fine-grained aspects; (2)
Semantic Dialog Planning, which generates a structured plan using a DAG of
flexible super-nodes; and (3) Multi-Turn Simulation, which instantiates the
plan using paired LLM agents for the user and system, constrained by
executional and behavioral fidelity checks. We apply ConvRecStudio to three
domains -- MobileRec, Yelp, and Amazon Electronics -- producing over 12K
multi-turn dialogs per dataset. Human and automatic evaluations confirm the
naturalness, coherence, and behavioral grounding of the generated
conversations. To demonstrate utility, we build a cross-attention transformer
model that jointly encodes user history and dialog context, achieving gains in
Hit@K and NDCG@K over baselines using either signal alone or naive fusion.
Notably, our model achieves a 10.9% improvement in Hit@1 on Yelp over the
strongest baseline.

</details>


### [110] [Recommendation systems in e-commerce applications with machine learning methods](https://arxiv.org/abs/2506.17287)
*Aneta Poniszewska-Maranda,Magdalena Pakula,Bozena Borowska*

Main category: cs.IR

TL;DR: 本文通过系统文献综述分析电商推荐系统趋势、挑战及机器学习方法有效性。


<details>
  <summary>Details</summary>
Motivation: 电商平台依赖推荐系统，需了解当前趋势、挑战及机器学习方法有效性。

Method: 进行系统文献综述，分析2013 - 2025年38篇出版物，评估比较各方法。

Result: 未提及具体结果

Conclusion: 未提及具体结论

Abstract: E-commerce platforms are increasingly reliant on recommendation systems to
enhance user experience, retain customers, and, in most cases, drive sales. The
integration of machine learning methods into these systems has significantly
improved their efficiency, personalization, and scalability. This paper aims to
highlight the current trends in e-commerce recommendation systems, identify
challenges, and evaluate the effectiveness of various machine learning methods
used, including collaborative filtering, content-based filtering, and hybrid
models. A systematic literature review (SLR) was conducted, analyzing 38
publications from 2013 to 2025. The methods used were evaluated and compared to
determine their performance and effectiveness in addressing e-commerce
challenges.

</details>


### [111] [SlimRAG: Retrieval without Graphs via Entity-Aware Context Selection](https://arxiv.org/abs/2506.17288)
*Jiale Zhang,Jiaxiang Chen,Zhucong Li,Jie Ding,Kui Zhao,Zenglin Xu,Xin Pang,Yinghui Xu*

Main category: cs.IR

TL;DR: 提出无图轻量级检索框架SlimRAG，实验显示其性能优且代码将开源。


<details>
  <summary>Details</summary>
Motivation: 图基RAG系统有结构开销和检索不精确问题，语义相似不意味着相关。

Method: 用简单有效的实体感知机制替代结构复杂组件，构建实体到块表，检索时识别实体、打分并组装输入，提出RITU指标。

Result: 在多个QA基准测试中，SlimRAG比基线模型更准确，减小索引大小和RITU。

Conclusion: 无结构、以实体为中心的上下文选择有价值。

Abstract: Retrieval-Augmented Generation (RAG) enhances language models by
incorporating external knowledge at inference time. However, graph-based RAG
systems often suffer from structural overhead and imprecise retrieval: they
require costly pipelines for entity linking and relation extraction, yet
frequently return subgraphs filled with loosely related or tangential content.
This stems from a fundamental flaw -- semantic similarity does not imply
semantic relevance. We introduce SlimRAG, a lightweight framework for retrieval
without graphs. SlimRAG replaces structure-heavy components with a simple yet
effective entity-aware mechanism. At indexing time, it constructs a compact
entity-to-chunk table based on semantic embeddings. At query time, it
identifies salient entities, retrieves and scores associated chunks, and
assembles a concise, contextually relevant input -- without graph traversal or
edge construction. To quantify retrieval efficiency, we propose Relative Index
Token Utilization (RITU), a metric measuring the compactness of retrieved
content. Experiments across multiple QA benchmarks show that SlimRAG
outperforms strong flat and graph-based baselines in accuracy while reducing
index size and RITU (e.g., 16.31 vs. 56+), highlighting the value of
structure-free, entity-centric context selection. The code will be released
soon. https://github.com/continue-ai-company/SlimRAG

</details>


### [112] [PreQRAG -- Classify and Rewrite for Enhanced RAG](https://arxiv.org/abs/2506.17493)
*Damian Martinez,Catalina Riano,Hui Fang*

Main category: cs.IR

TL;DR: UDInfo团队提交SIGIR 2025 LiveRAG挑战赛方案，介绍PreQRAG架构，通过问题预处理提升性能，获挑战赛第2名。


<details>
  <summary>Details</summary>
Motivation: 提升检索增强生成（RAG）的检索和生成质量。

Method: 引入PreQRAG架构，先对问题分类，单文档问题用改写技术，多文档问题分解复杂查询。

Result: 在LiveRAG挑战赛数据集上验证有效，PreQRAG获第2名。

Conclusion: 问题类型感知架构可提升RAG性能。

Abstract: This paper presents the submission of the UDInfo team to the SIGIR 2025
LiveRAG Challenge. We introduce PreQRAG, a Retrieval Augmented Generation (RAG)
architecture designed to improve retrieval and generation quality through
targeted question preprocessing. PreQRAG incorporates a pipeline that first
classifies each input question as either single-document or multi-document
type. For single-document questions, we employ question rewriting techniques to
improve retrieval precision and generation relevance. For multi-document
questions, we decompose complex queries into focused sub-questions that can be
processed more effectively by downstream components. This classification and
rewriting strategy improves the RAG performance. Experimental evaluation of the
LiveRAG Challenge dataset demonstrates the effectiveness of our
question-type-aware architecture, with PreQRAG achieving the preliminary second
place in Session 2 of the LiveRAG challenge.

</details>


### [113] [Context-Aware Scientific Knowledge Extraction on Linked Open Data using Large Language Models](https://arxiv.org/abs/2506.17580)
*Sajratul Y. Rubaiat,Hasan M. Jamil*

Main category: cs.IR

TL;DR: 本文介绍了WISE系统，它通过结构化工作流解决科学文献知识提取难题，实验表明其能高效提取信息，还可适应多领域。


<details>
  <summary>Details</summary>
Motivation: 科学文献指数级增长，传统搜索引擎和通用大模型在知识提取与合成方面存在局限。

Method: 使用基于大模型的树状架构提炼数据，采用动态评分和排名，设置自适应停止标准。

Result: 在HBB基因相关疾病实验中，WISE减少超80%处理文本，召回率显著高于基线，输出更独特、信息更深入。

Conclusion: WISE能有效解决科学知识提取与合成问题，可适应药物发现、材料科学和社会科学等多领域。

Abstract: The exponential growth of scientific literature challenges researchers
extracting and synthesizing knowledge. Traditional search engines return many
sources without direct, detailed answers, while general-purpose LLMs may offer
concise responses that lack depth or omit current information. LLMs with search
capabilities are also limited by context window, yielding short, incomplete
answers. This paper introduces WISE (Workflow for Intelligent Scientific
Knowledge Extraction), a system addressing these limits by using a structured
workflow to extract, refine, and rank query-specific knowledge. WISE uses an
LLM-powered, tree-based architecture to refine data, focusing on query-aligned,
context-aware, and non-redundant information. Dynamic scoring and ranking
prioritize unique contributions from each source, and adaptive stopping
criteria minimize processing overhead. WISE delivers detailed, organized
answers by systematically exploring and synthesizing knowledge from diverse
sources. Experiments on HBB gene-associated diseases demonstrate WISE reduces
processed text by over 80% while achieving significantly higher recall over
baselines like search engines and other LLM-based approaches. ROUGE and BLEU
metrics reveal WISE's output is more unique than other systems, and a novel
level-based metric shows it provides more in-depth information. We also explore
how the WISE workflow can be adapted for diverse domains like drug discovery,
material science, and social science, enabling efficient knowledge extraction
and synthesis from unstructured scientific papers and web sources.

</details>


### [114] [A novel fast short-time root music method for vibration monitoring of high-speed spindles](https://arxiv.org/abs/2506.17600)
*Huiguang Zhang,Baoguo Liu,Wei Feng,Zongtang Li*

Main category: cs.IR

TL;DR: 提出快速短时间根MUSIC（fSTrM）算法用于超高速主轴轴承振动监测，降低计算复杂度，有超分辨率，实验验证其检测微缺陷能力强，可实时部署，改变监测范式。


<details>
  <summary>Details</summary>
Motivation: 超高速主轴轴承因宽带噪声、非平稳性和时频分辨率有限，传统振动监测方法面临挑战。

Method: 利用FFT加速的兰乔斯双对角化降低计算复杂度，从16ms信号帧构建汉克尔矩阵，通过单位圆上的多项式求根提取故障频率。

Result: 在实验中能可靠识别150μm缺陷，比传统方法多提供72+小时预警时间，频率分辨率达1.2Hz，在 -5dB SNR下检测率93%，能量化缺陷严重程度，每帧处理时间2.4ms。

Conclusion: 该算法将轴承监测从故障预防转变为连续退化评估，为航空航天和精密加工的预测性维护建立了新范式。

Abstract: Ultra-high-speed spindle bearings challenge traditional vibration monitoring
due to broadband noise, non-stationarity, and limited time-frequency
resolution. We present a fast Short-Time Root-MUSIC (fSTrM) algorithm that
exploits
  FFT-accelerated Lanczos bidiagonalization to reduce computational complexity
from $\mathcal{O}(N^3)$ to $SN\log_2N+S^2(N+S)+M^2(N+M)$
  while preserving parametric super-resolution. The method constructs Hankel
matrices from 16 ms signal frames and extracts fault frequencies through
polynomial rooting on the unit circle. Experimental validation on the
Politecnico di Torino bearing dataset demonstrates breakthrough micro-defect
detection capabilities. The algorithm reliably identifies 150 $\mu$m defects --
previously undetectable by conventional methods -- providing 72+ hours
additional warning time. Compared to STFT and wavelet methods, fSTrM achieves
1.2 Hz frequency resolution (vs. 12.5 Hz), 93\% detection rate at $-$5 dB SNR,
and quantifies defect severity through harmonic content analysis. Critically,
the algorithm processes each frame in 2.4 ms on embedded ARM Cortex-M7
hardware, enabling real-time deployment. This advancement transforms bearing
monitoring from failure prevention to continuous degradation assessment,
establishing a new paradigm for predictive maintenance in aerospace and
precision machining.

</details>


### [115] [Reinforcing User Interest Evolution in Multi-Scenario Learning for recommender systems](https://arxiv.org/abs/2506.17682)
*Zhijian Feng,Wenhao Zheng,Xuanji Xiao*

Main category: cs.IR

TL;DR: 针对多场景推荐中用户兴趣不一致致统一建模难问题，提出用强化学习建模用户兴趣演变的方法，实验超现有方法，提供新思路。


<details>
  <summary>Details</summary>
Motivation: 现实推荐系统中不同场景下用户兴趣不一致，统一建模困难，需解决多场景学习挑战。

Method: 提出新颖强化学习方法，通过建模多场景下用户兴趣演变来建模用户偏好，用Double Q - learning提高下一项预测准确性，用Q值优化对比学习损失。

Result: 实验结果表明该方法在多场景推荐任务中超越了现有最优方法。

Conclusion: 为多场景建模提供新视角，指明未来研究的有前景方向。

Abstract: In real-world recommendation systems, users would engage in variety
scenarios, such as homepages, search pages, and related recommendation pages.
Each of these scenarios would reflect different aspects users focus on.
However, the user interests may be inconsistent in different scenarios, due to
differences in decision-making processes and preference expression. This
variability complicates unified modeling, making multi-scenario learning a
significant challenge. To address this, we propose a novel reinforcement
learning approach that models user preferences across scenarios by modeling
user interest evolution across multiple scenarios. Our method employs Double
Q-learning to enhance next-item prediction accuracy and optimizes contrastive
learning loss using Q-value to make model performance better. Experimental
results demonstrate that our approach surpasses state-of-the-art methods in
multi-scenario recommendation tasks. Our work offers a fresh perspective on
multi-scenario modeling and highlights promising directions for future
research.

</details>


### [116] [CARTS: Collaborative Agents for Recommendation Textual Summarization](https://arxiv.org/abs/2506.17765)
*Jiao Chen,Kehui Yao,Reza Yousefi Maragheh,Kai Zhao,Jianpeng Xu,Jason Cho,Evren Korpeoglu,Sushant Kumar,Kannan Achan*

Main category: cs.IR

TL;DR: 提出CARTS框架用于推荐系统文本摘要，实验显示其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型文本摘要方法不适用于推荐系统，推荐系统需解释与物品集核心特征高度相关且符合严格字数限制。

Method: 提出CARTS多智能体大语言模型框架，将任务分解为生成增强生成、精炼循环和仲裁三个阶段。

Result: 在大规模电商数据实验和线上A/B测试中，CARTS显著优于单遍和思维链大语言模型基线，提高标题相关性和用户参与度指标。

Conclusion: CARTS在推荐系统结构化文本摘要方面表现良好，是有效的解决方案。

Abstract: Current recommendation systems often require some form of textual data
summarization, such as generating concise and coherent titles for product
carousels or other grouped item displays. While large language models have
shown promise in NLP domains for textual summarization, these approaches do not
directly apply to recommendation systems, where explanations must be highly
relevant to the core features of item sets, adhere to strict word limit
constraints. In this paper, we propose CARTS (Collaborative Agents for
Recommendation Textual Summarization), a multi-agent LLM framework designed for
structured summarization in recommendation systems. CARTS decomposes the task
into three stages-Generation Augmented Generation (GAG), refinement circle, and
arbitration, where successive agent roles are responsible for extracting
salient item features, iteratively refining candidate titles based on relevance
and length feedback, and selecting the final title through a collaborative
arbitration process. Experiments on large-scale e-commerce data and live A/B
testing show that CARTS significantly outperforms single-pass and
chain-of-thought LLM baselines, delivering higher title relevance and improved
user engagement metrics.

</details>


### [117] [Expanding Relevance Judgments for Medical Case-based Retrieval Task with Multimodal LLMs](https://arxiv.org/abs/2506.17782)
*Catarina Pires,Sérgio Nunes,Luís Filipe Teixeira*

Main category: cs.IR

TL;DR: 本文探索用多模态大语言模型（MLLM）扩展相关性判断，在医学案例检索任务中取得了较好效果，展示了MLLM在扩展相关性判断收集方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 评估信息检索系统依赖高质量人工相关性判断，成本高且耗时，大语言模型为减少对人工判断的依赖提供了替代方案。

Method: 在ImageCLEFmed 2013案例检索任务中使用Gemini 1.5 Pro，通过迭代优化的结构化提示策略模拟人类评估，并系统实验不同提示配置。

Result: MLLM与人类判断的Cohen's Kappa值达0.6，数据集扩展超37倍至558,653个判断，相关标注增至5,950个。

Conclusion: MLLMs在扩展相关性判断收集方面有潜力，为医学和多模态信息检索任务的评估提供了有前景的方向。

Abstract: Evaluating Information Retrieval (IR) systems relies on high-quality manual
relevance judgments (qrels), which are costly and time-consuming to obtain.
While pooling reduces the annotation effort, it results in only partially
labeled datasets. Large Language Models (LLMs) offer a promising alternative to
reducing reliance on manual judgments, particularly in complex domains like
medical case-based retrieval, where relevance assessment requires analyzing
both textual and visual information. In this work, we explore using a
Multimodal Large Language Model (MLLM) to expand relevance judgments, creating
a new dataset of automated judgments. Specifically, we employ Gemini 1.5 Pro on
the ImageCLEFmed 2013 case-based retrieval task, simulating human assessment
through an iteratively refined, structured prompting strategy that integrates
binary scoring, instruction-based evaluation, and few-shot learning. We
systematically experimented with various prompt configurations to maximize
agreement with human judgments. To evaluate agreement between the MLLM and
human judgments, we use Cohen's Kappa, achieving a substantial agreement score
of 0.6, comparable to inter-annotator agreement typically observed in
multimodal retrieval tasks. Starting from the original 15,028 manual judgments
(4.72% relevant) across 35 topics, our MLLM-based approach expanded the dataset
by over 37x to 558,653 judgments, increasing relevant annotations to 5,950. On
average, each medical case query received 15,398 new annotations, with
approximately 99% being non-relevant, reflecting the high sparsity typical in
this domain. Our results demonstrate the potential of MLLMs to scale relevance
judgment collection, offering a promising direction for supporting retrieval
evaluation in medical and multimodal IR tasks.

</details>


### [118] [A GenAI System for Improved FAIR Independent Biological Database Integration](https://arxiv.org/abs/2506.17934)
*Syed N. Sakib,Kallol Naha,Sajratul Y. Rubaiat,Hasan M. Jamil*

Main category: cs.IR

TL;DR: 本文介绍了自然语言查询处理系统FAIRBridge，可助力科学家查询生物数据库，增强科学数据处理能力。


<details>
  <summary>Details</summary>
Motivation: 生命科学研究需处理LOD网络多源数据，现有查询响应质量依赖数据源选择和语义集成，过程繁琐易错，虽有FAIR原则但仍有障碍。

Method: 引入FAIRBridge系统，利用AI解读查询意图，映射相关数据库，生成可执行查询，含缓解低质量查询工具。

Result: FAIRBridge的自主查询处理框架让用户探索替代数据源，按需利用众包。

Conclusion: FAIRBridge提供友好自动化平台，显著增强科学数据集成与处理能力，为研究提供新工具。

Abstract: Life sciences research increasingly requires identifying, accessing, and
effectively processing data from an ever-evolving array of information sources
on the Linked Open Data (LOD) network. This dynamic landscape places a
significant burden on researchers, as the quality of query responses depends
heavily on the selection and semantic integration of data sources --processes
that are often labor-intensive, error-prone, and costly. While the adoption of
FAIR (Findable, Accessible, Interoperable, and Reusable) data principles has
aimed to address these challenges, barriers to efficient and accurate
scientific data processing persist.
  In this paper, we introduce FAIRBridge, an experimental natural
language-based query processing system designed to empower scientists to
discover, access, and query biological databases, even when they are not
FAIR-compliant. FAIRBridge harnesses the capabilities of AI to interpret query
intents, map them to relevant databases described in scientific literature, and
generate executable queries via intelligent resource access plans. The system
also includes robust tools for mitigating low-quality query processing,
ensuring high fidelity and responsiveness in the information delivered.
  FAIRBridge's autonomous query processing framework enables users to explore
alternative data sources, make informed choices at every step, and leverage
community-driven crowd curation when needed. By providing a user-friendly,
automated hypothesis-testing platform in natural English, FAIRBridge
significantly enhances the integration and processing of scientific data,
offering researchers a powerful new tool for advancing their inquiries.

</details>


### [119] [LLM-Enhanced Multimodal Fusion for Cross-Domain Sequential Recommendation](https://arxiv.org/abs/2506.17966)
*Wangyu Wu,Zhenhong Chen,Xianglin Qiu,Siqi Song,Xiaowei Huang,Fei Ma,Jimin Xiao*

Main category: cs.IR

TL;DR: 提出LLM - EMF用于跨域序列推荐，融合多模态数据提升推荐性能，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 跨域序列推荐需更好地建模跨域偏好和捕捉项目关系，提升推荐性能。

Method: 用冻结的CLIP模型生成图像和文本嵌入，用多注意力机制学习单域和跨域偏好。

Result: 在四个电商数据集上评估，LLM - EMF始终优于现有方法。

Conclusion: 多模态数据集成有效，能提升序列推荐系统性能，代码将发布。

Abstract: Cross-Domain Sequential Recommendation (CDSR) predicts user behavior by
leveraging historical interactions across multiple domains, focusing on
modeling cross-domain preferences and capturing both intra- and inter-sequence
item relationships. We propose LLM-Enhanced Multimodal Fusion for Cross-Domain
Sequential Recommendation (LLM-EMF), a novel and advanced approach that
enhances textual information with Large Language Models (LLM) knowledge and
significantly improves recommendation performance through the fusion of visual
and textual data. Using the frozen CLIP model, we generate image and text
embeddings, thereby enriching item representations with multimodal data. A
multiple attention mechanism jointly learns both single-domain and cross-domain
preferences, effectively capturing and understanding complex user interests
across diverse domains. Evaluations conducted on four e-commerce datasets
demonstrate that LLM-EMF consistently outperforms existing methods in modeling
cross-domain user preferences, thereby highlighting the effectiveness of
multimodal data integration and its advantages in enhancing sequential
recommendation systems. Our source code will be released.

</details>


### [120] [Comparative Analysis of Lion and AdamW Optimizers for Cross-Encoder Reranking with MiniLM, GTE, and ModernBERT](https://arxiv.org/abs/2506.18297)
*Shahil Kumar,Manu Pande,Anay Yatin Damle*

Main category: cs.IR

TL;DR: 研究Lion优化器在微调交叉编码器重排器时的影响，对三个模型用两种优化器微调并评估，发现Lion有较好效果且GPU效率高。


<details>
  <summary>Details</summary>
Motivation: 研究Lion优化器在交叉编码器重排器微调时的影响，与AdamW对比。

Method: 在MS MARCO数据集上用Lion和AdamW对MiniLM、GTE和ModernBERT三个模型进行微调，在TREC 2019和MS MARCO dev集评估。

Result: ModernBERT搭配Lion在TREC DL 2019上NDCG@10和MAP最佳，MiniLM搭配Lion在MS MARCO dev上MRR@10与ModernBERT并列，Lion提高GPU利用率。

Conclusion: Lion优化器在交叉编码器重排器微调中有较好效果，还能提升GPU效率，不同架构训练动态受其影响。

Abstract: Modern information retrieval systems often employ a two-stage pipeline: an
efficient initial retrieval stage followed by a computationally intensive
reranking stage. Cross-encoders have shown strong effectiveness for reranking
due to their deep analysis of query-document pairs. This paper studies the
impact of the Lion optimizer, a recent alternative to AdamW, during fine-tuning
of cross-encoder rerankers. We fine-tune three transformer models-MiniLM, GTE,
and ModernBERT-on the MS MARCO passage ranking dataset using both optimizers.
GTE and ModernBERT support extended context lengths (up to 8192 tokens). We
evaluate effectiveness using TREC 2019 Deep Learning Track and MS MARCO dev set
(MRR@10). Experiments, run on the Modal cloud platform, reveal that ModernBERT
with Lion achieves the best NDCG@10 (0.7225) and MAP (0.5121) on TREC DL 2019,
while MiniLM with Lion ties ModernBERT for MRR@10 (0.5988) on MS MARCO dev.
Lion also provides superior GPU efficiency, improving utilization by 2.67% to
10.33% across models. We analyze performance trends using standard IR metrics
and discuss the optimizer's impact on training dynamics across architectures.

</details>


### [121] [LettinGo: Explore User Profile Generation for Recommendation System](https://arxiv.org/abs/2506.18309)
*Lu Wang,Di Zhang,Fangkai Yang,Pu Zhao,Jianfeng Liu,Yuefeng Zhan,Hao Sun,Qingwei Lin,Weiwei Deng,Dongmei Zhang,Feng Sun,Qi Zhang*

Main category: cs.IR

TL;DR: 本文提出LettinGo框架生成多样且自适应的用户画像，经实验验证能提升推荐效果，是下一代推荐系统的关键创新。


<details>
  <summary>Details</summary>
Motivation: 传统基于嵌入的用户画像缺乏可解释性和适应性，现有基于大语言模型的方法受固定格式限制，无法充分捕捉用户行为多样性。

Method: 引入LettinGo框架，利用大语言模型的表达能力和下游推荐任务的反馈，采用直接偏好优化（DPO），分三个阶段操作：多模型探索、基于推荐效果评估、通过任务表现的偏好数据对齐。

Result: 实验结果表明该框架显著提升了推荐的准确性、灵活性和上下文感知能力。

Conclusion: 这项工作改进了用户画像生成，是下一代推荐系统的关键创新。

Abstract: User profiling is pivotal for recommendation systems, as it transforms raw
user interaction data into concise and structured representations that drive
personalized recommendations. While traditional embedding-based profiles lack
interpretability and adaptability, recent advances with large language models
(LLMs) enable text-based profiles that are semantically richer and more
transparent. However, existing methods often adhere to fixed formats that limit
their ability to capture the full diversity of user behaviors. In this paper,
we introduce LettinGo, a novel framework for generating diverse and adaptive
user profiles. By leveraging the expressive power of LLMs and incorporating
direct feedback from downstream recommendation tasks, our approach avoids the
rigid constraints imposed by supervised fine-tuning (SFT). Instead, we employ
Direct Preference Optimization (DPO) to align the profile generator with
task-specific performance, ensuring that the profiles remain adaptive and
effective. LettinGo operates in three stages: (1) exploring diverse user
profiles via multiple LLMs, (2) evaluating profile quality based on their
impact in recommendation systems, and (3) aligning the profile generation
through pairwise preference data derived from task performance. Experimental
results demonstrate that our framework significantly enhances recommendation
accuracy, flexibility, and contextual awareness. This work enhances profile
generation as a key innovation for next-generation recommendation systems.

</details>


### [122] [Enhancing Document Retrieval in COVID-19 Research: Leveraging Large Language Models for Hidden Relation Extraction](https://arxiv.org/abs/2506.18311)
*Hoang-An Trieu,Dinh-Truong Do,Chau Nguyen,Vu Tran,Minh Le Nguyen*

Main category: cs.IR

TL;DR: 本文提出Covrelex - SE系统方法，利用大语言模型为检索系统提供高质量结果。


<details>
  <summary>Details</summary>
Motivation: 新冠疫情下相关出版物众多，需要高效检索系统应对突发疫情，获取有用信息。

Method: 利用大语言模型提取未标记出版物中的隐藏关系。

Result: 文中未提及明确结果。

Conclusion: 文中未提及明确结论。

Abstract: In recent years, with the appearance of the COVID-19 pandemic, numerous
publications relevant to this disease have been issued. Because of the massive
volume of publications, an efficient retrieval system is necessary to provide
researchers with useful information if an unexpected pandemic happens so
suddenly, like COVID-19. In this work, we present a method to help the
retrieval system, the Covrelex-SE system, to provide more high-quality search
results. We exploited the power of the large language models (LLMs) to extract
the hidden relationships inside the unlabeled publication that cannot be found
by the current parsing tools that the system is using. Since then, help the
system to have more useful information during retrieval progress.

</details>


### [123] [Team LA at SCIDOCA shared task 2025: Citation Discovery via relation-based zero-shot retrieval](https://arxiv.org/abs/2506.18316)
*Trieu An,Long Nguyen,Minh Le Nguyen*

Main category: cs.IR

TL;DR: 本文围绕引文发现共享任务，开发系统结合特征提取和大语言模型进行引文预测，并在数据集上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 解决引文发现共享任务中摘要段落长、候选摘要相似度高导致难以确定正确引文的问题。

Method: 先基于提取的关系特征从给定段落中检索出最相似的前k个摘要，再利用大语言模型从子集中准确识别最相关的引文。

Result: 在SCIDOCA 2025组织者提供的训练数据集上评估框架，证明其在引文预测方面有效。

Conclusion: 所开发的系统能有效进行引文预测。

Abstract: The Citation Discovery Shared Task focuses on predicting the correct citation
from a given candidate pool for a given paragraph. The main challenges stem
from the length of the abstract paragraphs and the high similarity among
candidate abstracts, making it difficult to determine the exact paper to cite.
To address this, we develop a system that first retrieves the top-k most
similar abstracts based on extracted relational features from the given
paragraph. From this subset, we leverage a Large Language Model (LLM) to
accurately identify the most relevant citation. We evaluate our framework on
the training dataset provided by the SCIDOCA 2025 organizers, demonstrating its
effectiveness in citation prediction.

</details>


### [124] [Bias vs Bias -- Dawn of Justice: A Fair Fight in Recommendation Systems](https://arxiv.org/abs/2506.18327)
*Tahsin Alamgir Kheya,Mohamed Reda Bouadjenek,Sunil Aryal*

Main category: cs.IR

TL;DR: 本文提出公平感知重排序方法缓解推荐系统中的偏见，在三个真实数据集上实验表明该方法能缓解社会偏见且性能损失小。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统可能产生不公平和不平衡的推荐，先前工作忽视特定物品类别中的偏见，且多数公平重排序工作聚焦二元敏感属性。

Method: 提出公平感知重排序方法，利用现有偏见纠正不同人口群体推荐中的差异。

Result: 在三个真实数据集上实验，该方法能缓解社会偏见，性能几乎无下降。

Conclusion: 所提出的重排序方法可有效缓解推荐系统中的偏见，同时保持性能。

Abstract: Recommendation systems play a crucial role in our daily lives by impacting
user experience across various domains, including e-commerce, job
advertisements, entertainment, etc. Given the vital role of such systems in our
lives, practitioners must ensure they do not produce unfair and imbalanced
recommendations. Previous work addressing bias in recommendations overlooked
bias in certain item categories, potentially leaving some biases unaddressed.
Additionally, most previous work on fair re-ranking focused on binary-sensitive
attributes. In this paper, we address these issues by proposing a
fairness-aware re-ranking approach that helps mitigate bias in different
categories of items. This re-ranking approach leverages existing biases to
correct disparities in recommendations across various demographic groups. We
show how our approach can mitigate bias on multiple sensitive attributes,
including gender, age, and occupation. We experimented on three real-world
datasets to evaluate the effectiveness of our re-ranking scheme in mitigating
bias in recommendations. Our results show how this approach helps mitigate
social bias with little to no degradation in performance.

</details>


### [125] [PERSCEN: Learning Personalized Interaction Pattern and Scenario Preference for Multi-Scenario Matching](https://arxiv.org/abs/2506.18382)
*Haotong Du,Yaqing Wang,Fei Xiong,Lei Shao,Ming Liu,Hao Gu,Quanming Yao,Zhen Wang*

Main category: cs.IR

TL;DR: 提出创新方法 PERSCEN 用于多场景匹配，融合用户特定建模，实验证明其优于现有方法且平衡了性能与计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有多场景推荐方法常忽略用户特定建模，限制个性化用户表示生成，需改进。

Method: 构建用户特定特征图，用轻量级图神经网络捕捉高阶交互模式；利用向量量化技术提取场景感知偏好；引入渐进式场景感知门控线性单元实现信息融合。

Result: PERSCEN 在实验中优于现有方法，能有效平衡性能与计算成本。

Conclusion: PERSCEN 是有效的多场景匹配方法，适用于现实工业系统。

Abstract: With the expansion of business scales and scopes on online platforms,
multi-scenario matching has become a mainstream solution to reduce maintenance
costs and alleviate data sparsity. The key to effective multi-scenario
recommendation lies in capturing both user preferences shared across all
scenarios and scenario-aware preferences specific to each scenario. However,
existing methods often overlook user-specific modeling, limiting the generation
of personalized user representations. To address this, we propose PERSCEN, an
innovative approach that incorporates user-specific modeling into
multi-scenario matching. PERSCEN constructs a user-specific feature graph based
on user characteristics and employs a lightweight graph neural network to
capture higher-order interaction patterns, enabling personalized extraction of
preferences shared across scenarios. Additionally, we leverage vector
quantization techniques to distil scenario-aware preferences from users'
behavior sequence within individual scenarios, facilitating user-specific and
scenario-aware preference modeling. To enhance efficient and flexible
information transfer, we introduce a progressive scenario-aware gated linear
unit that allows fine-grained, low-latency fusion. Extensive experiments
demonstrate that PERSCEN outperforms existing methods. Further efficiency
analysis confirms that PERSCEN effectively balances performance with
computational cost, ensuring its practicality for real-world industrial
systems.

</details>


### [126] [Rethinking Click Models in Light of Carousel Interfaces: Theory-Based Categorization and Design of Click Models](https://arxiv.org/abs/2506.18548)
*Jingwei Kang,Maarten de Rijke,Santiago de Leon-Martinez,Harrie Oosterhuis*

Main category: cs.IR

TL;DR: 本文指出旧的点击模型分类过时，提出基于数学性质的关键设计选择和新分类法，还给出新设计示例。


<details>
  <summary>Details</summary>
Motivation: 以往点击模型分类过时，无法比较PGM和NN模型，也难以推广到新界面，阻碍新模型发展。

Method: 重新考虑点击模型设计的基本概念，基于数学性质提出三个关键设计选择，创建新的分类法。

Result: 得到一个能对所有现有点击模型进行有意义比较的新分类法，涵盖单列表、网格和轮播界面模型，包括PGM和NN。

Conclusion: 新的概念化方法为未来点击模型设计提供了基础，通过轮播界面新设计示例得以体现。

Abstract: Click models are a well-established for modeling user interactions with web
interfaces. Previous work has mainly focused on traditional single-list web
search settings; this includes existing surveys that introduced categorizations
based on the first generation of probabilistic graphical model (PGM) click
models that have become standard. However, these categorizations have become
outdated, as their conceptualizations are unable to meaningfully compare PGM
with neural network (NN) click models nor generalize to newer interfaces, such
as carousel interfaces. We argue that this outdated view fails to adequately
explain the fundamentals of click model designs, thus hindering the development
of novel click models.
  This work reconsiders what should be the fundamental concepts in click model
design, grounding them - unlike previous approaches - in their mathematical
properties. We propose three fundamental key-design choices that explain what
statistical patterns a click model can capture, and thus indirectly, what user
behaviors they can capture. Based on these choices, we create a novel click
model taxonomy that allows a meaningful comparison of all existing click
models; this is the first taxonomy of single-list, grid and carousel click
models that includes PGMs and NNs. Finally, we show how our conceptualization
provides a foundation for future click model design by an example derivation of
a novel design for carousel interfaces.

</details>


### [127] [Harnessing the Power of Reinforcement Learning for Language-Model-Based Information Retriever via Query-Document Co-Augmentation](https://arxiv.org/abs/2506.18670)
*Jingming Liu,Yumeng Li,Wei Shi,Yao-Xiang Ding,Hui Su,Kun Zhou*

Main category: cs.IR

TL;DR: 本文提出基于大语言模型（LLM）的检索器，通过强化学习（RL）探索策略，增强查询和文档，提升检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅通过查询重写利用LLM作为信息检索器，在处理具有挑战性的语料库时，仅增强查询不足以进行鲁棒的语义匹配，LLM还应直接处理和增强文档。

Method: 提出基于LLM的检索器，通过强化学习全面探索策略，设计双向RL框架，引入奖励采样策略和特定RL算法解决训练中策略联合更新的难题。

Result: 实验表明该方法在稀疏和密集设置中显著提升基于LLM的检索性能，在困难检索领域表现出色，且具有较强的跨基准泛化能力。

Conclusion: 所提出的方法有效提升了基于LLM的检索性能，在多种场景下表现良好。

Abstract: Recent studies have proposed leveraging Large Language Models (LLMs) as
information retrievers through query rewriting. However, for challenging
corpora, we argue that enhancing queries alone is insufficient for robust
semantic matching; the LLM should also have sufficient understanding of the
corpus by directly handling and augmenting the documents themselves. To this
end, we present an LLM-based retriever empowered to augment both user queries
and corpus documents, with its policy fully explored via reinforcement learning
(RL) and minimal human inductive bias. Notably, we find that simply allowing
the LLM to modify documents yields little benefit unless paired with our
carefully designed bidirectional RL framework, which enables the LLM to
simultaneously learn and collaborate on both query and document augmentation
policies. A key technical challenge in realizing such a framework lies in
jointly updating both policies during training, where the rewards for the two
directions depend on each other, making their entangled reward intractable. Our
approach addresses this by introducing a reward sampling strategy and a
specifically designed RL algorithm that enables effective training with these
sampled rewards. Experimental results demonstrate that our approach
significantly enhances LLM-based retrieval performance in both sparse and dense
settings, particularly in difficult retrieval domains, and achieves strong
cross-benchmark generalization. Our code is released at
https://github.com/liujm2001/CoAugRetriever.

</details>


### [128] [An Audio-centric Multi-task Learning Framework for Streaming Ads Targeting on Spotify](https://arxiv.org/abs/2506.18735)
*Shivam Verma,Vivian Chen,Darren Mei*

Main category: cs.IR

TL;DR: 介绍适用于Spotify的跨模态自适应专家混合框架CAMoE优化CTR预测，提升广告效果。


<details>
  <summary>Details</summary>
Motivation: Spotify内容消费模式多样，传统广告推荐模型难以调和音频中心性与多格式广告性能优化需求。

Method: 引入CAMoE框架，结合模态感知任务分组、自适应损失掩码和深度交叉网络。

Result: 在多种广告格式上接近帕累托最优性能，AUC - PR显著提升，大规模部署时音频广告CTR增14.5%、视频广告增1.3%，音频槽eCPC降4.8%。

Conclusion: CAMoE能有效解决Spotify计算广告的挑战，提升广告效果。

Abstract: Spotify, a large-scale multimedia platform, attracts over 675 million monthly
active users who collectively consume millions of hours of music, podcasts,
audiobooks, and video content. This diverse content consumption pattern
introduces unique challenges for computational advertising, which must
effectively integrate a variety of ad modalities, including audio, video, and
display, within a single user experience. Traditional ad recommendation models,
primarily designed for foregrounded experiences, often struggle to reconcile
the platform's inherent audio-centrality with the demands of optimizing ad
performance across multiple formats and modalities. To overcome these
challenges, we introduce Cross-modal Adaptive Mixture-of-Experts (CAMoE), a
novel framework for optimizing click-through rate (CTR) prediction in both
audio-centric and multi-modal settings. CAMoE enhances traditional
mixture-of-experts models by incorporating modality-aware task grouping,
adaptive loss masking, and deep-cross networks (DCN) to capture complex feature
interactions within a multi-modal ad ecosystem. Through extensive ablation
studies, we demonstrate that this approach achieves near Pareto-optimal
performance across audio, video, and display ad formats, significantly
improving AUC-PR compared to conventional single-task and content-based
multi-task learning baselines. When deployed at scale on Spotify's ad serving
platform, CAMoE delivered substantial gains, yielding a 14.5% increase in CTR
for audio ads, a 1.3% increase for video ads, and a 4.8% reduction in expected
cost-per-click (eCPC) for audio slots.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [129] [MMET: A Multi-Input and Multi-Scale Transformer for Efficient PDEs Solving](https://arxiv.org/abs/2506.17230)
*Yichen Luo,Jia Wang,Dapeng Lan,Yu Liu,Zhibo Pang*

Main category: cs.LG

TL;DR: 本文提出多输入多尺度高效Transformer（MMET）框架解决偏微分方程（PDEs）求解难题，实验显示其在精度和计算效率上优于SOTA方法，有潜力用于实时PDE求解。


<details>
  <summary>Details</summary>
Motivation: 现有基于机器学习的PDEs求解方法存在多输入和多尺度泛化能力有限、计算成本高的问题。

Method: 提出MMET框架，将网格和查询点解耦为两个序列分别输入编码器和解码器，用Gated Condition Embedding（GCE）层嵌入不同维度的输入变量或函数，采用基于Hilbert曲线的重新序列化和补丁嵌入机制减少输入长度。

Result: 在不同物理领域的多个基准测试中，MMET在精度和计算效率上均优于SOTA方法。

Conclusion: MMET有潜力成为工程和基于物理的应用中实时PDE求解的强大且可扩展的解决方案，为特定领域预训练大规模模型的未来探索铺平道路。

Abstract: Partial Differential Equations (PDEs) are fundamental for modeling physical
systems, yet solving them in a generic and efficient manner using machine
learning-based approaches remains challenging due to limited multi-input and
multi-scale generalization capabilities, as well as high computational costs.
This paper proposes the Multi-input and Multi-scale Efficient Transformer
(MMET), a novel framework designed to address the above challenges. MMET
decouples mesh and query points as two sequences and feeds them into the
encoder and decoder, respectively, and uses a Gated Condition Embedding (GCE)
layer to embed input variables or functions with varying dimensions, enabling
effective solutions for multi-scale and multi-input problems. Additionally, a
Hilbert curve-based reserialization and patch embedding mechanism decrease the
input length. This significantly reduces the computational cost when dealing
with large-scale geometric models. These innovations enable efficient
representations and support multi-scale resolution queries for large-scale and
multi-input PDE problems. Experimental evaluations on diverse benchmarks
spanning different physical fields demonstrate that MMET outperforms SOTA
methods in both accuracy and computational efficiency. This work highlights the
potential of MMET as a robust and scalable solution for real-time PDE solving
in engineering and physics-based applications, paving the way for future
explorations into pre-trained large-scale models in specific domains. This work
is open-sourced at https://github.com/YichenLuo-0/MMET.

</details>


### [130] [PCaM: A Progressive Focus Attention-Based Information Fusion Method for Improving Vision Transformer Domain Adaptation](https://arxiv.org/abs/2506.17232)
*Zelin Zang,Fei Wang,Liangyu Li,Jinlin Wu,Chunshui Zhao,Zhen Lei,Baigui Sun*

Main category: cs.LG

TL;DR: 指出基于ViT的UDA方法存在前景物体不匹配问题，提出PCaM机制及注意力引导损失，实验证明其提升了适应性能。


<details>
  <summary>Details</summary>
Motivation: 解决基于ViT的无监督域适应（UDA）方法中前景物体不匹配，导致注意力一致性减弱和域对齐效果不佳的问题。

Method: 提出渐进式聚焦交叉注意力机制（PCaM）过滤背景信息，引入注意力引导损失增强跨域注意力一致性。

Result: 在多个数据集上的实验表明，PCaM显著提高了适应性能，取得新的最优结果。

Conclusion: 注意力引导的前景融合对域适应有效，PCaM轻量级、与架构无关且易集成到现有基于ViT的UDA管道。

Abstract: Unsupervised Domain Adaptation (UDA) aims to transfer knowledge from a
labeled source domain to an unlabeled target domain. Recent UDA methods based
on Vision Transformers (ViTs) have achieved strong performance through
attention-based feature alignment. However, we identify a key limitation:
foreground object mismatch, where the discrepancy in foreground object size and
spatial distribution across domains weakens attention consistency and hampers
effective domain alignment. To address this issue, we propose the Progressive
Focus Cross-Attention Mechanism (PCaM), which progressively filters out
background information during cross-attention, allowing the model to focus on
and fuse discriminative foreground semantics across domains. We further
introduce an attentional guidance loss that explicitly directs attention toward
task-relevant regions, enhancing cross-domain attention consistency. PCaM is
lightweight, architecture-agnostic, and easy to integrate into existing
ViT-based UDA pipelines. Extensive experiments on Office-Home, DomainNet,
VisDA-2017, and remote sensing datasets demonstrate that PCaM significantly
improves adaptation performance and achieves new state-of-the-art results,
validating the effectiveness of attention-guided foreground fusion for domain
adaptation.

</details>


### [131] [Graph Neural Networks in Multi-Omics Cancer Research: A Structured Survey](https://arxiv.org/abs/2506.17234)
*Payam Zohari,Mostafa Haghir Chehreghani*

Main category: cs.LG

TL;DR: 本文是关于基于图神经网络（GNN）架构在多组学癌症研究中的系统综述，分析了现有研究趋势并指出潜在方向。


<details>
  <summary>Details</summary>
Motivation: 数据整合是揭示癌症复杂生物学基础的有力策略，GNN为建模多组学数据提供有效框架，本文旨在探索相关研究。

Method: 对利用GNN架构的多组学癌症研究进行系统综述，按目标组学层、GNN结构和生物任务分类。

Result: 发现混合和可解释模型、注意力机制和对比学习的应用呈增长趋势，患者特异性图和知识驱动先验是新兴方向。

Conclusion: 本综述为设计基于GNN的癌症综合分析管道的研究人员提供全面资源，涵盖当前实践、局限性和未来方向。

Abstract: The task of data integration for multi-omics data has emerged as a powerful
strategy to unravel the complex biological underpinnings of cancer. Recent
advancements in graph neural networks (GNNs) offer an effective framework to
model heterogeneous and structured omics data, enabling precise representation
of molecular interactions and regulatory networks. This systematic review
explores several recent studies that leverage GNN-based architectures in
multi-omics cancer research. We classify the approaches based on their targeted
omics layers, graph neural network structures, and biological tasks such as
subtype classification, prognosis prediction, and biomarker discovery. The
analysis reveals a growing trend toward hybrid and interpretable models,
alongside increasing adoption of attention mechanisms and contrastive learning.
Furthermore, we highlight the use of patient-specific graphs and
knowledge-driven priors as emerging directions. This survey serves as a
comprehensive resource for researchers aiming to design effective GNN-based
pipelines for integrative cancer analysis, offering insights into current
practices, limitations, and potential future directions.

</details>


### [132] [Training a Scientific Reasoning Model for Chemistry](https://arxiv.org/abs/2506.17238)
*Siddharth M. Narayanan,James D. Braza,Ryan-Rhys Griffiths,Albert Bou,Geemi Wellawatte,Mayk Caldas Ramos,Ludovico Mitchener,Samuel G. Rodriques,Andrew D. White*

Main category: cs.LG

TL;DR: 研究表明推理模型无需额外领域预训练就能进行化学后训练，数据效率高，所训练的ether0模型在分子设计任务上表现出色，此方法有望用于多科学领域。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型推理能否在数学、编程和逻辑之外的领域，如化学领域进行泛化。

Method: 基于Mistral - Small - 24B的24B参数LLM，在640,730个实验性化学问题上通过强化学习训练。

Result: ether0模型在分子设计任务上超越通用化学模型、前沿模型和人类专家，且数据效率更高。

Conclusion: 该方法可用于训练跨多种科学领域任务的数据高效语言模型。

Abstract: Reasoning models are large language models that emit a long chain-of-thought
before answering, providing both higher accuracy and explicit reasoning for
their response. A major question has been whether language model reasoning
generalizes beyond mathematics, programming, and logic, where most previous
work has focused. We demonstrate that reasoning models can be post-trained for
chemistry without additional domain pretraining, and require substantially less
data compared to contemporary domain-specific models. We report ether0, a 24B
parameter LLM (based on Mistral-Small-24B) that can reason in natural language
and respond with chemical structures. This reasoning model was trained with
reinforcement learning on 640,730 experimentally-grounded chemistry problems
across 375 tasks ranging from synthesizability, to blood-brain barrier
permeability, to human receptor activity, to scent. Our model exceeds
general-purpose chemistry models, frontier models, and human experts on
molecular design tasks. It is also more data efficient relative to specialized
models. We anticipate that this method can be applied to train data-efficient
language models specialized for tasks across a wide variety of scientific
domains.

</details>


### [133] [Recursive Learning-Based Virtual Buffering for Analytical Global Placement](https://arxiv.org/abs/2506.17247)
*Andrew B. Kahng,Yiting Liu,Zhiang Wang*

Main category: cs.LG

TL;DR: 提出开源学习驱动的虚拟缓冲感知分析全局布局框架MLBuf - RePlAce，在开源和商业流程中提升总负裕度，不降低或小幅提升后布线功耗。


<details>
  <summary>Details</summary>
Motivation: 现代技术节点中互连与单元延迟缩放不均，现有布局方法在全局布局时存在计算成本高、未充分考虑电气规则检查违规等问题。

Method: 构建基于OpenROAD基础设施的MLBuf - RePlAce，采用高效递归学习生成缓冲方法预测缓冲类型和位置，解决全局布局中的ERC违规。

Result: 在开源OpenROAD流程中，不降低后布线功耗，总负裕度最大提升56%、平均提升31%；在商业流程中，总负裕度最大提升53%、平均提升28%，后布线功耗平均提升0.2%。

Conclusion: MLBuf - RePlAce有效解决现有布局方法问题，在不同流程中显著提升时序性能，对功耗影响小。

Abstract: Due to the skewed scaling of interconnect versus cell delay in modern
technology nodes, placement with buffer porosity (i.e., cell density) awareness
is essential for timing closure in physical synthesis flows. However, existing
approaches face two key challenges: (i) traditional van Ginneken-Lillis-style
buffering approaches are computationally expensive during global placement; and
(ii) machine learning-based approaches, such as BufFormer, lack a thorough
consideration of Electrical Rule Check (ERC) violations and fail to "close the
loop" back into the physical design flow. In this work, we propose
MLBuf-RePlAce, the first open-source learning-driven virtual buffering-aware
analytical global placement framework, built on top of the OpenROAD
infrastructure. MLBuf-RePlAce adopts an efficient recursive learning-based
generative buffering approach to predict buffer types and locations, addressing
ERC violations during global placement. We compare MLBuf-RePlAce against the
default virtual buffering-based timing-driven global placer in OpenROAD, using
open-source testcases from the TILOS MacroPlacement and OpenROAD-flow-scripts
repositories. Without degradation of post-route power, MLBuf-RePlAce achieves
(maximum, average) improvements of (56%, 31%) in total negative slack (TNS)
within the open-source OpenROAD flow. When evaluated by completion in a
commercial flow, MLBuf-RePlAce achieves (maximum, average) improvements of
(53%, 28%) in TNS with an average of 0.2% improvement in post-route power.

</details>


### [134] [Efficient Quantification of Multimodal Interaction at Sample Level](https://arxiv.org/abs/2506.17248)
*Zequn Yang,Hongfa Wang,Di Hu*

Main category: cs.LG

TL;DR: 本文提出轻量级逐样本多模态交互（LSMI）估计器，基于点态信息理论，经实验验证其精确高效，能揭示多模态数据细粒度动态并用于实际应用。


<details>
  <summary>Details</summary>
Motivation: 多模态信息交互的精确样本级量化存在理论和计算挑战，需解决该问题。

Method: 先开发冗余估计框架，用点态信息测度量化交互；再提出通用交互估计方法，用高效熵估计进行连续分布的逐样本估计。

Result: 在合成和真实数据集上的实验验证了LSMI的精确性和高效性。

Conclusion: 逐样本方法能揭示多模态数据细粒度动态，可用于冗余信息样本划分、目标知识蒸馏和交互感知模型集成等实际应用。

Abstract: Interactions between modalities -- redundancy, uniqueness, and synergy --
collectively determine the composition of multimodal information. Understanding
these interactions is crucial for analyzing information dynamics in multimodal
systems, yet their accurate sample-level quantification presents significant
theoretical and computational challenges. To address this, we introduce the
Lightweight Sample-wise Multimodal Interaction (LSMI) estimator, rigorously
grounded in pointwise information theory. We first develop a redundancy
estimation framework, employing an appropriate pointwise information measure to
quantify this most decomposable and measurable interaction. Building upon this,
we propose a general interaction estimation method that employs efficient
entropy estimation, specifically tailored for sample-wise estimation in
continuous distributions. Extensive experiments on synthetic and real-world
datasets validate LSMI's precision and efficiency. Crucially, our sample-wise
approach reveals fine-grained sample- and category-level dynamics within
multimodal data, enabling practical applications such as redundancy-informed
sample partitioning, targeted knowledge distillation, and interaction-aware
model ensembling. The code is available at
https://github.com/GeWu-Lab/LSMI_Estimator.

</details>


### [135] [Improving Prediction Certainty Estimation for Reliable Early Exiting via Null Space Projection](https://arxiv.org/abs/2506.17249)
*Jianing He,Qi Zhang,Duoqian Miao,Yi Kun,Shufeng Hao,Hongyun Zhang,Zhihua Wei*

Main category: cs.LG

TL;DR: 提出基于CAP分数的早期退出方法，考虑特征中无关信息，在GLUE基准测试取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有早期退出方法依赖分类相关对数，忽略特征中无关信息对预测确定性的影响，导致样本过早退出。

Method: 定义NSP分数考虑特征中无关信息比例估计预测确定性，提出基于CAP分数的早期退出方法。

Result: 在GLUE基准测试中平均加速比达2.19x，性能损失可忽略，超越SOTA的ConsistentEE 28%。

Conclusion: 所提方法能在任务性能和推理效率间取得更好平衡。

Abstract: Early exiting has demonstrated great potential in accelerating the inference
of pre-trained language models (PLMs) by enabling easy samples to exit at
shallow layers, eliminating the need for executing deeper layers. However,
existing early exiting methods primarily rely on class-relevant logits to
formulate their exiting signals for estimating prediction certainty, neglecting
the detrimental influence of class-irrelevant information in the features on
prediction certainty. This leads to an overestimation of prediction certainty,
causing premature exiting of samples with incorrect early predictions. To
remedy this, we define an NSP score to estimate prediction certainty by
considering the proportion of class-irrelevant information in the features. On
this basis, we propose a novel early exiting method based on the
Certainty-Aware Probability (CAP) score, which integrates insights from both
logits and the NSP score to enhance prediction certainty estimation, thus
enabling more reliable exiting decisions. The experimental results on the GLUE
benchmark show that our method can achieve an average speed-up ratio of 2.19x
across all tasks with negligible performance degradation, surpassing the
state-of-the-art (SOTA) ConsistentEE by 28%, yielding a better trade-off
between task performance and inference efficiency. The code is available at
https://github.com/He-Jianing/NSP.git.

</details>


### [136] [Towards Interpretable Adversarial Examples via Sparse Adversarial Attack](https://arxiv.org/abs/2506.17250)
*Fudong Lin,Jiadong Lou,Hao Wang,Brian Jalaian,Xu Yuan*

Main category: cs.LG

TL;DR: 提出一种新的稀疏攻击方法，解决现有方法的缺点，在计算开销、可迁移性和攻击强度上表现更好，还能生成更稀疏对抗样本并发现两类噪声。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏攻击方法存在稀疏性差、计算开销大、可迁移性和攻击强度弱等问题，为理解CNN脆弱性，需开发新的稀疏攻击方法克服这些缺点。

Method: 引入新的参数化技术近似NP难的l0优化问题，设计新的损失函数同时最大化对抗属性和最小化扰动像素数量。

Result: 实验表明该方法在计算开销、可迁移性和攻击强度上优于现有稀疏攻击方法，能生成更稀疏对抗样本，发现两类噪声。

Conclusion: 该方法有理论性能保证，有望成为评估DNN鲁棒性的基准，发现的噪声有助于解释对抗扰动如何误导分类器。

Abstract: Sparse attacks are to optimize the magnitude of adversarial perturbations for
fooling deep neural networks (DNNs) involving only a few perturbed pixels
(i.e., under the l0 constraint), suitable for interpreting the vulnerability of
DNNs. However, existing solutions fail to yield interpretable adversarial
examples due to their poor sparsity. Worse still, they often struggle with
heavy computational overhead, poor transferability, and weak attack strength.
In this paper, we aim to develop a sparse attack for understanding the
vulnerability of CNNs by minimizing the magnitude of initial perturbations
under the l0 constraint, to overcome the existing drawbacks while achieving a
fast, transferable, and strong attack to DNNs. In particular, a novel and
theoretical sound parameterization technique is introduced to approximate the
NP-hard l0 optimization problem, making directly optimizing sparse
perturbations computationally feasible. Besides, a novel loss function is
designed to augment initial perturbations by maximizing the adversary property
and minimizing the number of perturbed pixels simultaneously. Extensive
experiments are conducted to demonstrate that our approach, with theoretical
performance guarantees, outperforms state-of-the-art sparse attacks in terms of
computational overhead, transferability, and attack strength, expecting to
serve as a benchmark for evaluating the robustness of DNNs. In addition,
theoretical and empirical results validate that our approach yields sparser
adversarial examples, empowering us to discover two categories of noises, i.e.,
"obscuring noise" and "leading noise", which will help interpret how
adversarial perturbation misleads the classifiers into incorrect predictions.
Our code is available at https://github.com/fudong03/SparseAttack.

</details>


### [137] [Training-free LLM Verification via Recycling Few-shot Examples](https://arxiv.org/abs/2506.17251)
*Dongseok Lee,Jimyung Hong,Dongyoung Kim,Jaehyung Kim*

Main category: cs.LG

TL;DR: 提出Referi框架回收少样本示例验证大语言模型输出，提升模型准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理过程有随机性和结论多样性问题，现有多数投票和Best-of-N方法有局限性。

Method: 提出Referi框架，额外利用少样本示例评估目标查询的候选输出，结合两种基于贝叶斯规则的分数评估，通过额外推理选择输出。

Result: 在三种大语言模型和七个不同任务实验中，该框架有效提升准确率，平均提升4.8%。

Conclusion: Referi框架无需额外训练，通过有效选择响应显著提升大语言模型准确率。

Abstract: Although LLMs have achieved remarkable performance, the inherent
stochasticity of their reasoning process and varying conclusions present
significant challenges. Majority voting or Best-of-N with external verification
models has been explored to find the most promising solution among multiple LLM
outputs. However, these approaches have certain limitations, such as limited
applicability or the cost of an additional training step. To address this
problem, we propose a novel and effective framework that Recycles Few-shot
examples to verify LLM outputs (Referi). Our key idea is to additionally
utilize the given few-shot examples to evaluate the candidate outputs of the
target query, not only using them to generate outputs as the conventional
few-shot prompting setup. Specifically, Referi evaluates the generated outputs
by combining two different scores, designed motivated from Bayes' rule, and
subsequently selects the candidate that is both confidently determined and
contextually coherent through a few additional LLM inferences. Experiments with
three different LLMs and across seven diverse tasks demonstrate that our
framework significantly improves the accuracy of LLMs-achieving an average gain
of 4.8%-through effective response selection, without additional training.

</details>


### [138] [SliceGX: Layer-wise GNN Explanation with Model-slicing](https://arxiv.org/abs/2506.17977)
*Tingting Zhu,Tingyang Chen,Yinghui Wu,Arijit Khan,Xiangyu Ke*

Main category: cs.LG

TL;DR: 本文提出SliceGX，一种渐进式生成特定GNN层解释的方法，通过实验验证其有效性、效率及模型调试实用价值。


<details>
  <summary>Details</summary>
Motivation: 现有GNN解释方法缺乏对中间表示如何促成最终结果的细粒度、逐层分析，而这对模型诊断和架构优化至关重要。

Method: 引入SliceGX，自动将GNN分割成层块，在每层块中发现高质量解释子图，开发高效算法和优化技术，并提供类似SPARQL的查询接口。

Result: 通过在大型真实图和代表性GNN架构上的实验，验证了SliceGX的有效性和效率。

Conclusion: SliceGX在支持模型调试方面具有实用价值。

Abstract: Ensuring the trustworthiness of graph neural networks (GNNs) as black-box
models requires effective explanation methods. Existing GNN explanations
typically apply input perturbations to identify subgraphs that are responsible
for the occurrence of the final output of GNNs. However, such approaches lack
finer-grained, layer-wise analysis of how intermediate representations
contribute to the final result, capabilities that are crucial for model
diagnosis and architecture optimization. This paper introduces SliceGX, a novel
GNN explanation approach that generates explanations at specific GNN layers in
a progressive manner. Given a GNN M, a set of selected intermediate layers, and
a target layer, SliceGX automatically segments M into layer blocks ("model
slice") and discovers high-quality explanatory subgraphs in each layer block
that clarifies the occurrence of output of M at the targeted layer. Although
finding such layer-wise explanations is computationally challenging, we develop
efficient algorithms and optimization techniques that incrementally generate
and maintain these subgraphs with provable approximation guarantees.
Additionally, SliceGX offers a SPARQL-like query interface, providing
declarative access and search capacities for the generated explanations.
Through experiments on large real-world graphs and representative GNN
architectures, we verify the effectiveness and efficiency of SliceGX, and
illustrate its practical utility in supporting model debugging.

</details>


### [139] [AlgoSelect: Universal Algorithm Selection via the Comb Operator](https://arxiv.org/abs/2506.17304)
*Jasper Yao*

Main category: cs.LG

TL;DR: 介绍了AlgoSelect框架用于从数据中学习最优算法选择，有理论贡献和良好实证效果。


<details>
  <summary>Details</summary>
Motivation: 解决自动化算法选择问题，提供有理论保证和可实际部署的解决方案。

Method: 围绕Comb Operator构建框架，用简单的sigmoid - gated selector进行插值，扩展到N - Path Comb用于多算法选择。

Result: 理论上证明框架具有通用性、信息论最优性、计算高效性和鲁棒性；实证中在20×20问题 - 算法研究里实现近完美选择（99.9%+准确率），收敛快。

Conclusion: AlgoSelect为自动化算法选择提供了有理论依据、可实际部署的方案，对AI和自适应系统有重要意义。

Abstract: We introduce AlgoSelect, a principled framework for learning optimal
algorithm selection from data, centered around the novel Comb Operator. Given a
set of algorithms and a feature representation of problems, AlgoSelect learns
to interpolate between diverse computational approaches. For pairs of
algorithms, a simple sigmoid-gated selector, an instance of the Comb Operator,
facilitates this interpolation. We extend this to an N-Path Comb for multiple
algorithms. We prove that this framework is universal (can approximate any
algorithm selector), information-theoretically optimal in its learnability
(thresholds for selection converge almost surely, demonstrated via
Borel-Cantelli arguments), computationally efficient, and robust. Key
theoretical contributions include: (1) a universal approximation theorem
demonstrating that Comb-based selectors can achieve arbitrary accuracy; (2)
information-theoretic learnability for selection thresholds; (3) formalization
of the Comb Operator within linear operator theory, detailing its boundedness
and spectral properties; (4) an N-Path Comb generalization for multi-algorithm
selection; and (5) a practical learning framework for the adaptive seeding
functions that guide the Comb Operator. Empirical validation on a comprehensive
20$\times$20 problem-algorithm study demonstrates near-perfect selection
(99.9\%+ accuracy) with remarkably few samples and rapid convergence, revealing
that $H(\text{Algorithm}|\text{Problem}) \approx 0$ in structured domains.
AlgoSelect provides a theoretically grounded, practically deployable solution
to automated algorithm selection with provable optimality and learnability
guarantees, with significant implications for AI and adaptive systems.

</details>


### [140] [Multi-Agent Online Control with Adversarial Disturbances](https://arxiv.org/abs/2506.18814)
*Anas Barakat,John Lazarsfeld,Georgios Piliouras,Antonios Varvitsiotis*

Main category: cs.LG

TL;DR: 研究多智能体线性动态系统带干扰的在线控制，证明近最优次线性遗憾界，目标一致时推导均衡差距保证。


<details>
  <summary>Details</summary>
Motivation: 多智能体控制问题在多领域广泛存在，以往工作较少考虑干扰对抗性和在线设置。

Method: 研究基于梯度的控制器，在最小通信假设下分析，关注个体遗憾保证受智能体数量的影响。

Result: 证明了对所有智能体一致成立的近最优次线性遗憾界，目标一致时得出均衡差距保证。

Conclusion: 在多智能体线性动态系统在线控制中，基于梯度的控制器有良好表现，目标一致时有均衡差距保证。

Abstract: Multi-agent control problems involving a large number of agents with
competing and time-varying objectives are increasingly prevalent in
applications across robotics, economics, and energy systems. In this paper, we
study online control in multi-agent linear dynamical systems with disturbances.
In contrast to most prior work in multi-agent control, we consider an online
setting where disturbances are adversarial and where each agent seeks to
minimize its own, adversarial sequence of convex losses. In this setting, we
investigate the robustness of gradient-based controllers from single-agent
online control, with a particular focus on understanding how individual regret
guarantees are influenced by the number of agents in the system. Under minimal
communication assumptions, we prove near-optimal sublinear regret bounds that
hold uniformly for all agents. Finally, when the objectives of the agents are
aligned, we show that the multi-agent control problem induces a time-varying
potential game for which we derive equilibrium gap guarantees.

</details>


### [141] [Adaptive Sample Scheduling for Direct Preference Optimization](https://arxiv.org/abs/2506.17252)
*Zixuan Huang,Yikun Ban,Lean Fu,Xiaojie Li,Zhongxiang Dai,Jianxin Li,Deqing Wang*

Main category: cs.LG

TL;DR: 本文提出DPO样本调度问题并给出SamS算法，在不修改DPO核心算法下提升性能，指明提升大语言模型对齐的新方向。


<details>
  <summary>Details</summary>
Motivation: DPO性能依赖人类偏好数据质量，现有数据选择策略忽略DPO过程中语言模型状态变化。

Method: 提出样本调度问题，设计SamS算法，基于大语言模型学习反馈在每个训练批次自适应选择样本。

Result: 不修改DPO核心算法，集成SamS能显著提升多任务性能，且计算开销极小。

Conclusion: 通过更有效利用固定偏好数据集，为提升大语言模型对齐指明新方向。

Abstract: Direct Preference Optimization (DPO) has emerged as an effective approach for
aligning large language models (LLMs) with human preferences. However, its
performance is highly dependent on the quality of the underlying human
preference data. To address this bottleneck, prior work has explored various
data selection strategies, but these methods often overlook the impact of the
evolving states of the language model during the DPO process. %including active
querying, response pair selection, and data pre-selection. In this paper, we
introduce a novel problem: Sample Scheduling for DPO, which aims to dynamically
and adaptively schedule training samples based on the model's evolving states
throughout preference optimization. To solve this problem, we propose SamS, an
efficient and effective algorithm that adaptively selects samples in each
training batch based on the LLM's learning feedback to maximize the potential
generalization performance. Notably, without modifying the core DPO algorithm,
simply integrating SamS significantly improves performance across tasks, with
minimal additional computational overhead. This work points to a promising new
direction for improving LLM alignment through more effective utilization of
fixed preference datasets.

</details>


### [142] [PuckTrick: A Library for Making Synthetic Data More Realistic](https://arxiv.org/abs/2506.18499)
*Alessandra Agostini,Andrea Maurino,Blerina Spahiu*

Main category: cs.LG

TL;DR: 论文介绍Python库Pucktrick，可系统地污染合成数据集以评估机器学习模型在真实数据缺陷下的弹性，实验表明污染数据训练的模型性能更好。


<details>
  <summary>Details</summary>
Motivation: 合成数据缺乏真实世界缺陷，影响模型泛化和鲁棒性，需改进以更好评估模型。

Method: 开发Pucktrick库，支持多种错误类型和两种污染模式，对真实金融数据集实验。

Result: 机器学习模型在受污染合成数据上训练，比在无误差合成数据上训练表现更好，尤其树和线性模型。

Conclusion: Pucktrick库能有效模拟真实数据缺陷，提升模型评估的有效性。

Abstract: The increasing reliance on machine learning (ML) models for decision-making
requires high-quality training data. However, access to real-world datasets is
often restricted due to privacy concerns, proprietary restrictions, and
incomplete data availability. As a result, synthetic data generation (SDG) has
emerged as a viable alternative, enabling the creation of artificial datasets
that preserve the statistical properties of real data while ensuring privacy
compliance. Despite its advantages, synthetic data is often overly clean and
lacks real-world imperfections, such as missing values, noise, outliers, and
misclassified labels, which can significantly impact model generalization and
robustness. To address this limitation, we introduce Pucktrick, a Python
library designed to systematically contaminate synthetic datasets by
introducing controlled errors. The library supports multiple error types,
including missing data, noisy values, outliers, label misclassification,
duplication, and class imbalance, offering a structured approach to evaluating
ML model resilience under real-world data imperfections. Pucktrick provides two
contamination modes: one for injecting errors into clean datasets and another
for further corrupting already contaminated datasets. Through extensive
experiments on real-world financial datasets, we evaluate the impact of
systematic data contamination on model performance. Our findings demonstrate
that ML models trained on contaminated synthetic data outperform those trained
on purely synthetic, error-free data, particularly for tree-based and linear
models such as SVMs and Extra Trees.

</details>


### [143] [MS-TVNet:A Long-Term Time Series Prediction Method Based on Multi-Scale Dynamic Convolution](https://arxiv.org/abs/2506.17253)
*Chenghan Li,Mingchen Li,Yipu Liao,Ruisheng Diao*

Main category: cs.LG

TL;DR: 提出多尺度3D动态卷积神经网络MS - TVNet用于长时序预测，在多数据集上表现优异，代码开源。


<details>
  <summary>Details</summary>
Motivation: 长时序预测主要依赖Transformer和MLP模型，卷积网络在此领域潜力未充分挖掘。

Method: 引入多尺度时间序列重塑模块，构建多尺度3D动态卷积神经网络MS - TVNet。

Result: MS - TVNet在不同数据集上评估表现优于基线模型，达到长时序预测的SOTA结果。

Conclusion: 利用卷积网络捕捉复杂时间模式有效，为该领域未来研究指明方向。

Abstract: Long-term time series prediction has predominantly relied on Transformer and
MLP models, while the potential of convolutional networks in this domain
remains underexplored. To address this gap, we introduce a novel multi-scale
time series reshape module, which effectively captures the relationships among
multi-period patches and variable dependencies. Building upon this module, we
propose MS-TVNet, a multi-scale 3D dynamic convolutional neural network.
Through comprehensive evaluations on diverse datasets, MS-TVNet demonstrates
superior performance compared to baseline models, achieving state-of-the-art
(SOTA) results in long-term time series prediction. Our findings highlight the
effectiveness of leveraging convolutional networks for capturing complex
temporal patterns, suggesting a promising direction for future research in this
field.The code is realsed on https://github.com/Curyyfaust/TVNet.

</details>


### [144] [Keeping Up with the Models: Online Deployment and Routing of LLMs at Scale](https://arxiv.org/abs/2506.17254)
*Shaoang Li,Jian Li*

Main category: cs.LG

TL;DR: 针对大语言模型更新快的情况，提出StageRoute算法解决模型部署和查询路由问题，证明其接近最优并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 大语言模型更新快，服务提供商需在有限部署容量和查询成本预算下管理模型库存。

Method: 将问题建模为在线决策问题，引入StageRoute分层算法，先使用置信界选择模型，再解决带预算约束的多臂老虎机子问题进行查询路由。

Result: 证明StageRoute算法的遗憾阶为$T^{2/3}$并给出匹配下界，实验表明在实际场景中接近最优表现。

Conclusion: StageRoute算法在解决大语言模型部署和查询路由问题上接近最优，具有实际应用价值。

Abstract: The rapid pace at which new large language models (LLMs) appear -- and older
ones become obsolete -- forces LLM service providers to juggle a streaming
inventory of models while respecting tight deployment capacity and per-query
cost budgets. We cast the reality as an online decision problem that couples
stage-wise deployment, made at fixed maintenance windows, with per-query
routing among the models kept live. We introduce StageRoute, a hierarchical
algorithm that (i) optimistically selects up to $M_max$ models for the next
stage using reward upper-confidence and cost lower-confidence bounds, then (ii)
solves a budget-constrained bandit sub-problem to route each incoming query. We
prove that StageRoute achieves a regret of order $T^{2/3}$ and provide a
matching lower bound, thereby establishing its near-optimality. Moreover, our
experiments confirm the theory, demonstrating that StageRoute performs close to
the optimum in practical settings.

</details>


### [145] [UltraSketchLLM: Saliency-Driven Sketching for Ultra-Low Bit LLM Compression](https://arxiv.org/abs/2506.17255)
*Sunan Zou,Ziyun Zhang,Xueting Sun,Guojie Luo*

Main category: cs.LG

TL;DR: 现有大语言模型权重压缩方法有局限，提出UltraSketchLLM框架实现超低比特压缩且保持性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型快速发展使边缘设备内存受限，需超越1比特的极端权重压缩，而现有方法存在内存开销大或精度下降严重问题。

Method: 引入无索引、基于草图的UltraSketchLLM框架，利用数据草图技术，集成低估AbsMaxMin草图、重要性感知空间分配和直通估计器进行压缩感知微调。

Result: 在Llama - 3.2 - 1B上实验实现0.5比特压缩，困惑度有竞争力，延迟开销可接受。

Conclusion: UltraSketchLLM为资源受限环境部署大语言模型提供实用解决方案。

Abstract: The rapid growth of large language models (LLMs) has outpaced the memory
constraints of edge devices, necessitating extreme weight compression beyond
the 1-bit limit. While quantization reduces model size, it is fundamentally
limited to 1 bit per weight. Existing multiple-to-one compression methods
either rely on mapping tables (inducing memory overhead) or incur severe
accuracy degradation due to random weight grouping. We introduce
UltraSketchLLM, an index-free, sketch-based framework that achieves ultra-low
bit compression (down to 0.5 bits per weight) while preserving model
performance. UltraSketchLLM leverages data sketching, a sub-linear
representation technique from streaming applications, to map multiple weights
to single values with bounded error. Our approach integrates an underestimate
AbsMaxMin sketch to minimize relative errors for small weights,
importance-aware space allocation to prioritize salient weights, and a
straight-through estimator for compression-aware finetuning. Experiments on
Llama-3.2-1B demonstrate up to 0.5-bit compression with competitive perplexity,
alongside tolerable latency overhead. UltraSketchLLM offers a practical
solution for deploying LLMs in resource-constrained environments.

</details>


### [146] [AI to Identify Strain-sensitive Regions of the Optic Nerve Head Linked to Functional Loss in Glaucoma](https://arxiv.org/abs/2506.17262)
*Thanadet Chuangsuwanich,Monisha E. Nongpiur,Fabian A. Braeu,Tin A. Tun,Alexandre Thiery,Shamira Perera,Ching Lin Ho,Martin Buist,George Barbastathis,Tin Aung,Michaël J. A. Girard*

Main category: cs.LG

TL;DR: 研究评估视盘（ONH）生物力学对青光眼三种进行性视野缺损模式预测的改善情况，利用可解释AI确定关键区域，模型AUC高，ONH应变提升预测效果，神经视网膜边缘是关键区域。


<details>
  <summary>Details</summary>
Motivation: 评估ONH生物力学是否能改善青光眼三种进行性视野缺损模式的预测，并用可解释AI确定对预测有贡献的应变敏感ONH区域。

Method: 招募237名青光眼受试者，对单眼ONH在两种眼压条件下成像，专家分类，计算眼压诱导的神经组织和筛板应变，将生物力学和结构特征输入几何深度学习模型进行三项分类任务，用AUC评估性能，用可解释AI突出关键区域。

Result: 模型AUC为0.77 - 0.88，ONH应变改善了视野损失预测，下和下颞缘是关键应变敏感区域，随疾病严重程度扩大。

Conclusion: ONH应变增强了青光眼视野损失模式的预测，神经视网膜边缘而非筛板是对模型预测最关键的区域。

Abstract: Objective: (1) To assess whether ONH biomechanics improves prediction of
three progressive visual field loss patterns in glaucoma; (2) to use
explainable AI to identify strain-sensitive ONH regions contributing to these
predictions.
  Methods: We recruited 237 glaucoma subjects. The ONH of one eye was imaged
under two conditions: (1) primary gaze and (2) primary gaze with IOP elevated
to ~35 mmHg via ophthalmo-dynamometry. Glaucoma experts classified the subjects
into four categories based on the presence of specific visual field defects:
(1) superior nasal step (N=26), (2) superior partial arcuate (N=62), (3) full
superior hemifield defect (N=25), and (4) other/non-specific defects (N=124).
Automatic ONH tissue segmentation and digital volume correlation were used to
compute IOP-induced neural tissue and lamina cribrosa (LC) strains.
Biomechanical and structural features were input to a Geometric Deep Learning
model. Three classification tasks were performed to detect: (1) superior nasal
step, (2) superior partial arcuate, (3) full superior hemifield defect. For
each task, the data were split into 80% training and 20% testing sets. Area
under the curve (AUC) was used to assess performance. Explainable AI techniques
were employed to highlight the ONH regions most critical to each
classification.
  Results: Models achieved high AUCs of 0.77-0.88, showing that ONH strain
improved VF loss prediction beyond morphology alone. The inferior and
inferotemporal rim were identified as key strain-sensitive regions,
contributing most to visual field loss prediction and showing progressive
expansion with increasing disease severity.
  Conclusion and Relevance: ONH strain enhances prediction of glaucomatous VF
loss patterns. Neuroretinal rim, rather than the LC, was the most critical
region contributing to model predictions.

</details>


### [147] [Memory Allocation in Resource-Constrained Reinforcement Learning](https://arxiv.org/abs/2506.17263)
*Massimiliano Tamborski,David Abel*

Main category: cs.LG

TL;DR: 研究内存约束对强化学习智能体在未知环境导航性能的影响及内存分配困境。


<details>
  <summary>Details</summary>
Motivation: 探索资源约束如何改变学习和决策，聚焦内存约束对智能体在未知环境导航表现的影响。

Method: 在基于MCTS和DQN的算法中研究内存分配问题，考察不同内存分配在情景和持续学习场景下对性能的影响。

Result: 未提及

Conclusion: 未提及

Abstract: Resource constraints can fundamentally change both learning and
decision-making. We explore how memory constraints influence an agent's
performance when navigating unknown environments using standard reinforcement
learning algorithms. Specifically, memory-constrained agents face a dilemma:
how much of their limited memory should be allocated to each of the agent's
internal processes, such as estimating a world model, as opposed to forming a
plan using that model? We study this dilemma in MCTS- and DQN-based algorithms
and examine how different allocations of memory impact performance in episodic
and continual learning settings.

</details>


### [148] [OAT-Rephrase: Optimization-Aware Training Data Rephrasing for Zeroth-Order LLM Fine-Tuning](https://arxiv.org/abs/2506.17264)
*Jikai Long,Zijian Hu,Xiaodong Yu,Jianwen Xie,Zhaozhuo Xu*

Main category: cs.LG

TL;DR: 本文提出OAT - Rephrase策略提升零阶优化微调大语言模型性能，评估显示该策略有效。


<details>
  <summary>Details</summary>
Motivation: 零阶优化微调大语言模型存在收敛慢和优化不稳定问题，需要改进。

Method: 引入OAT - Rephrase策略，基于零阶动态用大语言模型改写训练实例，采用包含重写器和语义判断器的双阶段管道。

Result: 在五个分类任务和三种大语言模型架构上评估，OAT - Rephrase持续提升MeZO微调性能，缩小与一阶方法差距。

Conclusion: 优化感知的改写可作为零阶微调机制的可复用、低开销增强手段。

Abstract: Fine-tuning large language models (LLMs) using zeroth-order optimization (ZO)
offers a memory-efficient alternative to gradient-based methods but suffers
from slower convergence and unstable optimization due to noisy gradient
estimates. This paper introduces OAT-Rephrase, an Optimization-Aware Training
data rephrasing strategy that leverages an LLM to rephrase training instances
based on its understanding of the ZO dynamics, specifically MeZO, derived
directly from its paper. The approach incorporates a dual-stage pipeline
featuring a rewriter LLM and a semantic judge, ensuring all rephrasings retain
task relevance and logical consistency. Evaluations across five classification
tasks and three LLM architectures demonstrate that OAT-Rephrase consistently
improves MeZO fine-tuning performance, often narrowing or eliminating the gap
with first-order methods. Our findings suggest that optimization-aware
rephrasing serves as a reusable and low-overhead enhancement for zeroth-order
tuning regimes.

</details>


### [149] [Does Multimodal Large Language Model Truly Unlearn? Stealthy MLLM Unlearning Attack](https://arxiv.org/abs/2506.17265)
*Xianren Zhang,Hui Liu,Delvin Ce Zhang,Xianfeng Tang,Qi He,Dongwon Lee,Suhang Wang*

Main category: cs.LG

TL;DR: 提出针对MLLM的遗忘攻击框架SUA，能有效恢复模型遗忘信息，且噪声泛化性好。


<details>
  <summary>Details</summary>
Motivation: MLLM遗忘方法不确定是否真正遗忘信息，研究恢复遗忘知识的新问题。

Method: 提出Stealthy Unlearning Attack (SUA) 框架学习通用噪声模式，引入嵌入对齐损失提升隐蔽性。

Result: SUA能有效从MLLMs恢复遗忘信息，训练的噪声对未见图像有泛化性。

Conclusion: 知识再现不是偶然失败，而是一致行为。

Abstract: Multimodal Large Language Models (MLLMs) trained on massive data may memorize
sensitive personal information and photos, posing serious privacy risks. To
mitigate this, MLLM unlearning methods are proposed, which fine-tune MLLMs to
reduce the ``forget'' sensitive information. However, it remains unclear
whether the knowledge has been truly forgotten or just hidden in the model.
Therefore, we propose to study a novel problem of LLM unlearning attack, which
aims to recover the unlearned knowledge of an unlearned LLM. To achieve the
goal, we propose a novel framework Stealthy Unlearning Attack (SUA) framework
that learns a universal noise pattern. When applied to input images, this noise
can trigger the model to reveal unlearned content. While pixel-level
perturbations may be visually subtle, they can be detected in the semantic
embedding space, making such attacks vulnerable to potential defenses. To
improve stealthiness, we introduce an embedding alignment loss that minimizes
the difference between the perturbed and denoised image embeddings, ensuring
the attack is semantically unnoticeable. Experimental results show that SUA can
effectively recover unlearned information from MLLMs. Furthermore, the learned
noise generalizes well: a single perturbation trained on a subset of samples
can reveal forgotten content in unseen images. This indicates that knowledge
reappearance is not an occasional failure, but a consistent behavior.

</details>


### [150] [CF-VLM:CounterFactual Vision-Language Fine-tuning](https://arxiv.org/abs/2506.17267)
*Jusheng Zhang,Kaitong Cai,Yijia Fan,Jian Wang,Keze Wang*

Main category: cs.LG

TL;DR: 现有视觉语言模型（VLMs）在细粒度区分和因果推理任务存在局限，提出CF - VLM框架提升其因果推理能力，实验显示其表现出色，能用于高风险场景。


<details>
  <summary>Details</summary>
Motivation: 现有VLMs依赖表面统计关联，缺乏捕捉视觉和文本内容潜在因果逻辑的能力，在细粒度区分和深度因果推理任务有局限。

Method: 提出CounterFactual Vision - Language Fine - tuning（CF - VLM）框架，引入三个互补训练目标：维持基础跨模态对齐、增强事实场景表征的独特性和稳定性、提高模型对关键因果编辑的敏感度。

Result: CF - VLM在组合推理和泛化基准上始终优于强基线和最先进方法，且在减轻视觉幻觉方面有前景，提升了事实一致性。

Conclusion: CF - VLM为在需要可靠推理和可解释性的高风险现实场景中部署VLMs提供了坚实基础。

Abstract: Recent advances in vision-language models (VLMs) have greatly improved
cross-modal semantic understanding, yet significant limitations remain in
fine-grained discrimination and deep causal reasoning tasks. Existing VLMs
often rely on superficial statistical correlations, lacking the ability to
capture the underlying causal logic between visual and textual content. To
address this, we propose CounterFactual Vision-Language Fine-tuning (CF-VLM), a
novel framework that enhances the causal reasoning capabilities of VLMs through
the targeted use of counterfactual samples. CF-VLM introduces three
complementary training objectives: maintaining foundational cross-modal
alignment, reinforcing the uniqueness and stability of factual scene
representations against coherent counterfactuals, and sharpening the model's
sensitivity to minimal but critical causal edits. Extensive experiments
demonstrate that CF-VLM consistently outperforms strong baselines and
state-of-the-art methods on compositional reasoning and generalization
benchmarks. Furthermore, it shows promise in mitigating visual hallucinations,
indicating improved factual consistency. Our CF-VLM provides a robust
foundation for deploying VLMs in high-stakes, real-world scenarios requiring
reliable reasoning and interpretability.

</details>


### [151] [CopulaSMOTE: A Copula-Based Oversampling Approach for Imbalanced Classification in Diabetes Prediction](https://arxiv.org/abs/2506.17326)
*Agnideep Aich,Md Monzur Murshed,Sameera Hewage,Amanda Mayeaux*

Main category: cs.LG

TL;DR: 研究用基于A2 copula的数据增强结合机器学习检测糖尿病，在Pima Indian数据集上，XGBoost结合A2 copula过采样表现最佳，是首个用A2 copulas进行数据增强的研究。


<details>
  <summary>Details</summary>
Motivation: 糖尿病危害大，早期检测可降低风险，但机器学习检测糖尿病受数据不平衡影响，需解决该问题。

Method: 采用基于copula的数据增强方法，在Pima Indian数据集上用A2 copula生成数据，应用逻辑回归、随机森林、梯度提升和极端梯度提升四种机器学习算法，并使用McNemar检验验证结果。

Result: XGBoost结合A2 copula过采样性能最佳，与标准SMOTE方法相比，准确率提高4.6%，精确率提高15.6%，召回率提高20.4%，F1分数提高18.2%，AUC提高25.5%。

Conclusion: 这是首次使用A2 copulas进行数据增强，可作为SMOTE技术的替代方案，凸显了copulas作为统计方法在机器学习应用中的有效性。

Abstract: Diabetes mellitus poses a significant health risk, as nearly 1 in 9 people
are affected by it. Early detection can significantly lower this risk. Despite
significant advancements in machine learning for identifying diabetic cases,
results can still be influenced by the imbalanced nature of the data. To
address this challenge, our study considered copula-based data augmentation,
which preserves the dependency structure when generating data for the minority
class and integrates it with machine learning (ML) techniques. We selected the
Pima Indian dataset and generated data using A2 copula, then applied four
machine learning algorithms: logistic regression, random forest, gradient
boosting, and extreme gradient boosting. Our findings indicate that XGBoost
combined with A2 copula oversampling achieved the best performance improving
accuracy by 4.6%, precision by 15.6%, recall by 20.4%, F1-score by 18.2% and
AUC by 25.5% compared to the standard SMOTE method. Furthermore, we
statistically validated our results using the McNemar test. This research
represents the first known use of A2 copulas for data augmentation and serves
as an alternative to the SMOTE technique, highlighting the efficacy of copulas
as a statistical method in machine learning applications.

</details>


### [152] [NestQuant: Post-Training Integer-Nesting Quantization for On-Device DNN](https://arxiv.org/abs/2506.17870)
*Jianhang Xie,Chuntao Ding,Xiaqing Li,Shenyuan Ren,Yidong Li,Zhichao Lu*

Main category: cs.LG

TL;DR: 本文提出资源友好的训练后整数嵌套量化方法NestQuant，可在物联网设备上进行量化模型切换，实验证明其能实现高性能并减少开销。


<details>
  <summary>Details</summary>
Motivation: 现有动态/混合精度量化需重新训练或特殊硬件，训练后量化（PTQ）在资源适配方面存在局限，难以适应物联网设备动态资源及会消耗大量存储资源和切换开销。

Method: 提出NestQuant，结合整数权重分解，按位将量化权重拆分为高位和低位整数权重，还有分解权重嵌套机制，通过自适应舍入优化高位权重并嵌套到原量化权重中。

Result: 在ImageNet - 1K预训练DNN上实验表明，NestQuant模型在top - 1准确率上表现良好，减少数据传输、存储消耗和切换开销，如ResNet - 101的全位/部分位模型准确率分别达78.1%和77.9%，与不同位宽PTQ模型相比，切换开销降低约78.1%。

Conclusion: NestQuant是一种有效的资源友好型量化方法，可在物联网设备上实现量化模型切换，满足多场景资源需求。

Abstract: Deploying quantized deep neural network (DNN) models with resource adaptation
capabilities on ubiquitous Internet of Things (IoT) devices to provide
high-quality AI services can leverage the benefits of compression and meet
multi-scenario resource requirements. However, existing dynamic/mixed precision
quantization requires retraining or special hardware, whereas post-training
quantization (PTQ) has two limitations for resource adaptation: (i) The
state-of-the-art PTQ methods only provide one fixed bitwidth model, which makes
it challenging to adapt to the dynamic resources of IoT devices; (ii) Deploying
multiple PTQ models with diverse bitwidths consumes large storage resources and
switching overheads. To this end, this paper introduces a resource-friendly
post-training integer-nesting quantization, i.e., NestQuant, for on-device
quantized model switching on IoT devices. The proposed NestQuant incorporates
the integer weight decomposition, which bit-wise splits quantized weights into
higher-bit and lower-bit weights of integer data types. It also contains a
decomposed weights nesting mechanism to optimize the higher-bit weights by
adaptive rounding and nest them into the original quantized weights. In
deployment, we can send and store only one NestQuant model and switch between
the full-bit/part-bit model by paging in/out lower-bit weights to adapt to
resource changes and reduce consumption. Experimental results on the
ImageNet-1K pretrained DNNs demonstrated that the NestQuant model can achieve
high performance in top-1 accuracy, and reduce in terms of data transmission,
storage consumption, and switching overheads. In particular, the ResNet-101
with INT8 nesting INT6 can achieve 78.1% and 77.9% accuracy for full-bit and
part-bit models, respectively, and reduce switching overheads by approximately
78.1% compared with diverse bitwidths PTQ models.

</details>


### [153] [SafeRL-Lite: A Lightweight, Explainable, and Constrained Reinforcement Learning Library](https://arxiv.org/abs/2506.17297)
*Satyam Mishra,Phung Thao Vi,Shivam Mishra,Vishwanath Bijalwan,Vijay Bhaskar Semwal,Abdul Manan Khan*

Main category: cs.LG

TL;DR: 介绍开源Python库SafeRL - Lite，可构建有约束且可解释的强化学习代理，展示其在CartPole变体上的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有RL工具包缺乏实施硬安全约束和生成决策合理解释的原生机制。

Method: 提供围绕标准Gym环境和深度Q学习代理的模块化包装器，通过约束执行实现安全感知训练，通过SHAP值和显著性图进行实时事后解释。

Result: 在CartPole的约束变体上证明了其有效性，并提供展示策略逻辑和安全遵守情况的可视化。

Conclusion: SafeRL - Lite是轻量级、可扩展且易于安装的，能有效构建有约束且可解释的强化学习代理。

Abstract: We introduce SafeRL-Lite, an open-source Python library for building
reinforcement learning (RL) agents that are both constrained and explainable.
Existing RL toolkits often lack native mechanisms for enforcing hard safety
constraints or producing human-interpretable rationales for decisions.
SafeRL-Lite provides modular wrappers around standard Gym environments and deep
Q-learning agents to enable: (i) safety-aware training via constraint
enforcement, and (ii) real-time post-hoc explanation via SHAP values and
saliency maps. The library is lightweight, extensible, and installable via pip,
and includes built-in metrics for constraint violations. We demonstrate its
effectiveness on constrained variants of CartPole and provide visualizations
that reveal both policy logic and safety adherence. The full codebase is
available at: https://github.com/satyamcser/saferl-lite.

</details>


### [154] [A Survey of State Representation Learning for Deep Reinforcement Learning](https://arxiv.org/abs/2506.17518)
*Ayoub Echchahed,Pablo Samuel Castro*

Main category: cs.LG

TL;DR: 本文对无模型在线强化学习中的表征学习方法进行分类，阐述各类方法机制、优缺点，还讨论评估方法和未来方向。


<details>
  <summary>Details</summary>
Motivation: 应对顺序决策问题中复杂观测空间挑战，增强对强化学习中表征学习方法的理解，为新研究者提供指引。

Method: 将方法分为六大类，详细说明其机制、优缺点；讨论评估表征质量的技术。

Result: 完成对无模型在线强化学习中表征学习方法的分类。

Conclusion: 通过分类增强对该领域理解，为新研究者提供指导，明确相关未来方向。

Abstract: Representation learning methods are an important tool for addressing the
challenges posed by complex observations spaces in sequential decision making
problems. Recently, many methods have used a wide variety of types of
approaches for learning meaningful state representations in reinforcement
learning, allowing better sample efficiency, generalization, and performance.
This survey aims to provide a broad categorization of these methods within a
model-free online setting, exploring how they tackle the learning of state
representations differently. We categorize the methods into six main classes,
detailing their mechanisms, benefits, and limitations. Through this taxonomy,
our aim is to enhance the understanding of this field and provide a guide for
new researchers. We also discuss techniques for assessing the quality of
representations, and detail relevant future directions.

</details>


### [155] [Learning to Adapt Frozen CLIP for Few-Shot Test-Time Domain Adaptation](https://arxiv.org/abs/2506.17307)
*Zhixiang Chi,Li Gu,Huan Liu,Ziqiang Wang,Yanan Wu,Yang Wang,Konstantinos N Plataniotis*

Main category: cs.LG

TL;DR: 本文提出在输入空间学习以补充冻结CLIP的特定数据集知识，通过独立分支、文本集成等方法实现少样本测试时的领域适应，实验显示在多个基准上表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖CLIP特征空间知识受其先验知识限制，使用较不鲁棒的骨干网络时在真实基准上性能显著下降，需新方法解决。

Method: 引入在输入空间学习以补充CLIP知识，添加独立分支并通过反向注意力学习，用贪心文本集成和细化增强文本特征的离散度，用生成的领域提示以领域感知的方式融合文本和视觉特征。

Result: 在5个大规模基准（WILDS和DomainNet）上表现优越，在iWildCam的F1分数提高5.1，在FMoW的WC Acc提高3.1%。

Conclusion: 所提方法有效，能在少样本测试时进行领域适应，尤其对较小网络性能提升明显。

Abstract: Few-shot Test-Time Domain Adaptation focuses on adapting a model at test time
to a specific domain using only a few unlabeled examples, addressing domain
shift. Prior methods leverage CLIP's strong out-of-distribution (OOD) abilities
by generating domain-specific prompts to guide its generalized, frozen
features. However, since downstream datasets are not explicitly seen by CLIP,
solely depending on the feature space knowledge is constrained by CLIP's prior
knowledge. Notably, when using a less robust backbone like ViT-B/16,
performance significantly drops on challenging real-world benchmarks. Departing
from the state-of-the-art of inheriting the intrinsic OOD capability of CLIP,
this work introduces learning directly on the input space to complement the
dataset-specific knowledge for frozen CLIP. Specifically, an independent side
branch is attached in parallel with CLIP and enforced to learn exclusive
knowledge via revert attention. To better capture the dataset-specific label
semantics for downstream adaptation, we propose to enhance the inter-dispersion
among text features via greedy text ensemble and refinement. The text and
visual features are then progressively fused in a domain-aware manner by a
generated domain prompt to adapt toward a specific domain. Extensive
experiments show our method's superiority on 5 large-scale benchmarks (WILDS
and DomainNet), notably improving over smaller networks like ViT-B/16 with
gains of \textbf{+5.1} in F1 for iWildCam and \textbf{+3.1\%} in WC Acc for
FMoW.

</details>


### [156] [DeInfoReg: A Decoupled Learning Framework for Better Training Throughput](https://arxiv.org/abs/2506.18193)
*Zih-Hao Huang,You-Teng Lin,Hung-Hsuan Chen*

Main category: cs.LG

TL;DR: 提出DeInfoReg方法，可缓解梯度消失问题、支持模型并行化，实验显示性能优越且代码开源


<details>
  <summary>Details</summary>
Motivation: 缓解梯度消失问题，提高训练吞吐量

Method: 将长梯度流转换为多个短梯度流，结合流水线策略实现多GPU模型并行化

Result: 在不同任务和数据集上实验表明，DeInfoReg比传统BP模型性能更优、抗噪性更好，能有效利用并行计算资源

Conclusion: DeInfoReg是一种有效解决梯度消失问题、提高训练效率的方法

Abstract: This paper introduces Decoupled Supervised Learning with Information
Regularization (DeInfoReg), a novel approach that transforms a long gradient
flow into multiple shorter ones, thereby mitigating the vanishing gradient
problem. Integrating a pipeline strategy, DeInfoReg enables model
parallelization across multiple GPUs, significantly improving training
throughput. We compare our proposed method with standard backpropagation and
other gradient flow decomposition techniques. Extensive experiments on diverse
tasks and datasets demonstrate that DeInfoReg achieves superior performance and
better noise resistance than traditional BP models and efficiently utilizes
parallel computing resources. The code for reproducibility is available at:
https://github.com/ianzih/Decoupled-Supervised-Learning-for-Information-Regularization/.

</details>


### [157] [I Know Which LLM Wrote Your Code Last Summer: LLM generated Code Stylometry for Authorship Attribution](https://arxiv.org/abs/2506.17323)
*Tamas Bisztray,Bilel Cherif,Richard A. Dubniczky,Nils Gruschka,Bertalan Borsos,Mohamed Amine Ferrag,Attila Kovacs,Vasileios Mavroeidis,Norbert Tihanyi*

Main category: cs.LG

TL;DR: 本文对C程序的大语言模型作者归属进行系统研究，提出CodeT5 - Authorship模型并引入LLM - AuthorBench基准，模型在分类任务中表现良好，相关资源开源。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型生成代码愈发普遍，识别每个样本背后的具体模型变得日益重要，当前缺乏对C程序大语言模型作者归属的系统研究。

Method: 提出CodeT5 - Authorship模型，仅使用原始CodeT5编码器 - 解码器架构的编码器层，其编码器输出通过两层分类头；引入LLM - AuthorBench基准，对比七种传统机器学习分类器和八种微调的Transformer模型。

Result: 在二分类中，模型区分密切相关模型生成的C程序准确率达97.56%；在五类归属中准确率达95.40%。

Conclusion: CodeT5 - Authorship模型在大语言模型代码作者归属识别方面表现出色，相关资源开源有助于推动开放科学。

Abstract: Detecting AI-generated code, deepfakes, and other synthetic content is an
emerging research challenge. As code generated by Large Language Models (LLMs)
becomes more common, identifying the specific model behind each sample is
increasingly important. This paper presents the first systematic study of LLM
authorship attribution for C programs. We released CodeT5-Authorship, a novel
model that uses only the encoder layers from the original CodeT5
encoder-decoder architecture, discarding the decoder to focus on
classification. Our model's encoder output (first token) is passed through a
two-layer classification head with GELU activation and dropout, producing a
probability distribution over possible authors. To evaluate our approach, we
introduce LLM-AuthorBench, a benchmark of 32,000 compilable C programs
generated by eight state-of-the-art LLMs across diverse tasks. We compare our
model to seven traditional ML classifiers and eight fine-tuned transformer
models, including BERT, RoBERTa, CodeBERT, ModernBERT, DistilBERT, DeBERTa-V3,
Longformer, and LoRA-fine-tuned Qwen2-1.5B. In binary classification, our model
achieves 97.56% accuracy in distinguishing C programs generated by closely
related models such as GPT-4.1 and GPT-4o, and 95.40% accuracy for multi-class
attribution among five leading LLMs (Gemini 2.5 Flash, Claude 3.5 Haiku,
GPT-4.1, Llama 3.3, and DeepSeek-V3). To support open science, we release the
CodeT5-Authorship architecture, the LLM-AuthorBench benchmark, and all relevant
Google Colab scripts on GitHub: https://github.com/LLMauthorbench/.

</details>


### [158] [Towards Fundamental Limits for Active Multi-distribution Learning](https://arxiv.org/abs/2506.17607)
*Chicheng Zhang,Yihan Zhou*

Main category: cs.LG

TL;DR: 本文为主动多分布学习开发新算法，建立改进的标签复杂度上下界，还给出被动多分布学习的实例相关样本复杂度界。


<details>
  <summary>Details</summary>
Motivation: 多分布学习在协作学习、公平性和鲁棒性等方面有应用，但主动多分布学习研究较少，算法最优性未知。

Method: 开发新的主动多分布学习算法，分析不同设置下的标签复杂度。

Result: 在近可实现、可实现和不可知设置下分别证明了标签复杂度上界，表明可实现设置下的界是信息论最优的，不可知设置中的特定项对合适的学习者是基础的。还建立了被动多分布学习的实例相关样本复杂度界。

Conclusion: 为主动多分布学习提供了新算法和改进的复杂度界，推动了该领域的研究。

Abstract: Multi-distribution learning extends agnostic Probably Approximately Correct
(PAC) learning to the setting in which a family of $k$ distributions,
$\{D_i\}_{i\in[k]}$, is considered and a classifier's performance is measured
by its error under the worst distribution. This problem has attracted a lot of
recent interests due to its applications in collaborative learning, fairness,
and robustness. Despite a rather complete picture of sample complexity of
passive multi-distribution learning, research on active multi-distribution
learning remains scarce, with algorithms whose optimality remaining unknown.
  In this paper, we develop new algorithms for active multi-distribution
learning and establish improved label complexity upper and lower bounds, in
distribution-dependent and distribution-free settings. Specifically, in the
near-realizable setting we prove an upper bound of
$\widetilde{O}\Bigl(\theta_{\max}(d+k)\ln\frac{1}{\varepsilon}\Bigr)$ and
$\widetilde{O}\Bigl(\theta_{\max}(d+k)\Bigl(\ln\frac{1}{\varepsilon}+\frac{\nu^2}{\varepsilon^2}\Bigr)+\frac{k\nu}{\varepsilon^2}\Bigr)$
in the realizable and agnostic settings respectively, where $\theta_{\max}$ is
the maximum disagreement coefficient among the $k$ distributions, $d$ is the VC
dimension of the hypothesis class, $\nu$ is the multi-distribution error of the
best hypothesis, and $\varepsilon$ is the target excess error. Moreover, we
show that the bound in the realizable setting is information-theoretically
optimal and that the $k\nu/\varepsilon^2$ term in the agnostic setting is
fundamental for proper learners. We also establish instance-dependent sample
complexity bound for passive multidistribution learning that smoothly
interpolates between realizable and agnostic
regimes~\citep{blum2017collaborative,zhang2024optimal}, which may be of
independent interest.

</details>


### [159] [Origins of Creativity in Attention-Based Diffusion Models](https://arxiv.org/abs/2506.17324)
*Emma Finn,T. Anderson Keller,Manos Theodosis,Demba E. Ba*

Main category: cs.LG

TL;DR: 本文研究扩散模型中创造力来源，扩展理论至含自注意力层的CNN参数化得分的扩散模型，并通过实验验证理论。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型用于图像生成且图像质量提升，理解扩散模型中‘创造力’的来源愈发重要，现有理论未涵盖自注意力在该过程中的作用。

Method: 将现有理论扩展到得分由带最终自注意力层的CNN参数化的扩散模型，并在精心构建的数据集上进行实验验证。

Result: 理论表明自注意力会在生成样本中诱导局部特征超出块级别的全局图像一致排列，且通过实验验证了这一行为。

Conclusion: 扩展了关于扩散模型创造力来源的理论，明确了自注意力在生成样本特征排列中的作用。

Abstract: As diffusion models have become the tool of choice for image generation and
as the quality of the images continues to improve, the question of how
`creativity' originates in diffusion has become increasingly important. The
score matching perspective on diffusion has proven particularly fruitful for
understanding how and why diffusion models generate images that remain
plausible while differing significantly from their training images. In
particular, as explained in (Kamb \& Ganguli, 2024) and others, e.g.,
(Ambrogioni, 2023), theory suggests that if our score matching were optimal, we
would only be able to recover training samples through our diffusion process.
However, as shown by Kamb \& Ganguli, (2024), in diffusion models where the
score is parametrized by a simple CNN, the inductive biases of the CNN itself
(translation equivariance and locality) allow the model to generate samples
that globally do not match any training samples, but are rather patch-wise
`mosaics'. Notably, however, this theory does not extend to describe the role
of self-attention in this process. In this work, we take a preliminary step in
this direction to extend this theory to the case of diffusion models whose
score is parametrized by a CNN with a final self-attention layer. We show that
our theory suggests that self-attention will induce a globally image-consistent
arrangement of local features beyond the patch-level in generated samples, and
we verify this behavior empirically on a carefully crafted dataset.

</details>


### [160] [CEGA: A Cost-Effective Approach for Graph-Based Model Extraction and Acquisition](https://arxiv.org/abs/2506.17709)
*Zebin Wang,Menghan Lin,Bolin Shen,Ken Anderson,Molei Liu,Tianxi Cai,Yushun Dong*

Main category: cs.LG

TL;DR: 本文评估GNN对模型提取攻击的脆弱性，提出一种节点查询策略，实验表明该策略在严格查询约束下优于基线。


<details>
  <summary>Details</summary>
Motivation: GNN在MLaaS平台部署面临模型提取攻击威胁，且在非对抗研究中需低成本获取模型，同时选择性采样信息节点对训练GNN有价值。

Method: 提出一种适应特定场景的节点查询策略，在多个学习周期迭代优化节点选择机制。

Result: 在基准图数据集实验中，所提策略在准确性、保真度和F1分数上优于可比基线。

Conclusion: 部署的GNN易受提取攻击，伦理高效的GNN获取方法对低资源研究环境有前景。

Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable utility across
diverse applications, and their growing complexity has made Machine Learning as
a Service (MLaaS) a viable platform for scalable deployment. However, this
accessibility also exposes GNN to serious security threats, most notably model
extraction attacks (MEAs), in which adversaries strategically query a deployed
model to construct a high-fidelity replica. In this work, we evaluate the
vulnerability of GNNs to MEAs and explore their potential for cost-effective
model acquisition in non-adversarial research settings. Importantly, adaptive
node querying strategies can also serve a critical role in research,
particularly when labeling data is expensive or time-consuming. By selectively
sampling informative nodes, researchers can train high-performing GNNs with
minimal supervision, which is particularly valuable in domains such as
biomedicine, where annotations often require expert input. To address this, we
propose a node querying strategy tailored to a highly practical yet
underexplored scenario, where bulk queries are prohibited, and only a limited
set of initial nodes is available. Our approach iteratively refines the node
selection mechanism over multiple learning cycles, leveraging historical
feedback to improve extraction efficiency. Extensive experiments on benchmark
graph datasets demonstrate our superiority over comparable baselines on
accuracy, fidelity, and F1 score under strict query-size constraints. These
results highlight both the susceptibility of deployed GNNs to extraction
attacks and the promise of ethical, efficient GNN acquisition methods to
support low-resource research environments.

</details>


### [161] [AutomataGPT: Forecasting and Ruleset Inference for Two-Dimensional Cellular Automata](https://arxiv.org/abs/2506.17333)
*Jaime A. Berkovich,Noah S. David,Markus J. Buehler*

Main category: cs.LG

TL;DR: 介绍AutomataGPT，其在二维二进制确定性CA规则上预训练，在预测和规则推断上表现良好，为抽象现实世界现象奠定基础。


<details>
  <summary>Details</summary>
Motivation: 解决自动发现给定现象的局部更新规则并进行定量预测的难题。

Method: 训练基于约100万个模拟轨迹预训练的解码器式Transformer AutomataGPT，涵盖100种不同二维二进制确定性CA规则。

Result: 在未见规则上，实现98.5%的一步完美预测，规则重构功能准确率达96%，规则矩阵匹配率82%。

Conclusion: 大规模预训练可在正逆问题上实现泛化，为将现实世界动态现象抽象为CA替代模型奠定基础。

Abstract: Cellular automata (CA) provide a minimal formalism for investigating how
simple local interactions generate rich spatiotemporal behavior in domains as
diverse as traffic flow, ecology, tissue morphogenesis and crystal growth.
However, automatically discovering the local update rules for a given
phenomenon and using them for quantitative prediction remains challenging. Here
we present AutomataGPT, a decoder-only transformer pretrained on around 1
million simulated trajectories that span 100 distinct two-dimensional binary
deterministic CA rules on toroidal grids. When evaluated on previously unseen
rules drawn from the same CA family, AutomataGPT attains 98.5% perfect one-step
forecasts and reconstructs the governing update rule with up to 96% functional
(application) accuracy and 82% exact rule-matrix match. These results
demonstrate that large-scale pretraining over wider regions of rule space
yields substantial generalization in both the forward (state forecasting) and
inverse (rule inference) problems, without hand-crafted priors. By showing that
transformer models can faithfully infer and execute CA dynamics from data
alone, our work lays the groundwork for abstracting real-world dynamical
phenomena into data-efficient CA surrogates, opening avenues in biology, tissue
engineering, physics and AI-driven scientific discovery.

</details>


### [162] [Learning Time-Aware Causal Representation for Model Generalization in Evolving Domains](https://arxiv.org/abs/2506.17718)
*Zhuo He,Shuang Li,Wenze Song,Longhui Yuan,Jian Liang,Han Li,Kun Gai*

Main category: cs.LG

TL;DR: 提出SYNC方法解决EDG方法泛化问题，在数据集上有出色表现


<details>
  <summary>Details</summary>
Motivation: 现有EDG方法建模时存在虚假关联，影响模型泛化能力

Method: 设计时间感知的结构因果模型SCM，提出SYNC方法，将信息论目标集成到顺序VAE框架中

Result: 在合成和真实数据集上，SYNC取得了优异的时间泛化性能

Conclusion: SYNC方法可有效学习时间感知因果表示，能为每个时域产生最优因果预测器

Abstract: Endowing deep models with the ability to generalize in dynamic scenarios is
of vital significance for real-world deployment, given the continuous and
complex changes in data distribution. Recently, evolving domain generalization
(EDG) has emerged to address distribution shifts over time, aiming to capture
evolving patterns for improved model generalization. However, existing EDG
methods may suffer from spurious correlations by modeling only the dependence
between data and targets across domains, creating a shortcut between
task-irrelevant factors and the target, which hinders generalization. To this
end, we design a time-aware structural causal model (SCM) that incorporates
dynamic causal factors and the causal mechanism drifts, and propose
\textbf{S}tatic-D\textbf{YN}amic \textbf{C}ausal Representation Learning
(\textbf{SYNC}), an approach that effectively learns time-aware causal
representations. Specifically, it integrates specially designed
information-theoretic objectives into a sequential VAE framework which captures
evolving patterns, and produces the desired representations by preserving
intra-class compactness of causal factors both across and within domains.
Moreover, we theoretically show that our method can yield the optimal causal
predictor for each time domain. Results on both synthetic and real-world
datasets exhibit that SYNC can achieve superior temporal generalization
performance.

</details>


### [163] [Adaptive Social Metaverse Streaming based on Federated Multi-Agent Deep Reinforcement Learning](https://arxiv.org/abs/2506.17342)
*Zijian Long,Haopeng Wang,Haiwei Dong,Abdulmotaleb El Saddik*

Main category: cs.LG

TL;DR: 本文针对社交元宇宙隐私保护和高质量低延迟流传输难题，提出基于F - MAPPO的ASMS系统，实验表明其能提升用户体验，增强社交元宇宙体验。


<details>
  <summary>Details</summary>
Motivation: 社交元宇宙发展中，隐私保护和实现高质量低延迟流传输面临挑战，需解决这些问题。

Method: 提出基于Federated Multi - Agent Proximal Policy Optimization (F - MAPPO)的ASMS（Adaptive Social Metaverse Streaming）系统，利用结合联邦学习和深度强化学习的F - MAPPO动态调整流比特率并保护隐私。

Result: 实验显示在不同网络条件下，与现有流传输方法相比，ASMS至少将用户体验提升14%。

Conclusion: ASMS能在动态和资源受限网络中提供无缝沉浸式流传输，增强社交元宇宙体验，同时确保敏感用户数据留存在本地设备。

Abstract: The social metaverse is a growing digital ecosystem that blends virtual and
physical worlds. It allows users to interact socially, work, shop, and enjoy
entertainment. However, privacy remains a major challenge, as immersive
interactions require continuous collection of biometric and behavioral data. At
the same time, ensuring high-quality, low-latency streaming is difficult due to
the demands of real-time interaction, immersive rendering, and bandwidth
optimization. To address these issues, we propose ASMS (Adaptive Social
Metaverse Streaming), a novel streaming system based on Federated Multi-Agent
Proximal Policy Optimization (F-MAPPO). ASMS leverages F-MAPPO, which
integrates federated learning (FL) and deep reinforcement learning (DRL) to
dynamically adjust streaming bit rates while preserving user privacy.
Experimental results show that ASMS improves user experience by at least 14%
compared to existing streaming methods across various network conditions.
Therefore, ASMS enhances the social metaverse experience by providing seamless
and immersive streaming, even in dynamic and resource-constrained networks,
while ensuring that sensitive user data remains on local devices.

</details>


### [164] [Log-Normal Multiplicative Dynamics for Stable Low-Precision Training of Large Networks](https://arxiv.org/abs/2506.17768)
*Keigo Nishida,Eren Mehmet Kıral,Kenichi Bannai,Mohammad Emtiyaz Khan,Thomas Möllenhoff*

Main category: cs.LG

TL;DR: 论文推导了贝叶斯学习规则，提出LMD算法，该算法在低精度运算下训练Vision Transformer和GPT - 2稳定准确，表明乘法动力学或能实现低精度推理和学习。


<details>
  <summary>Details</summary>
Motivation: 受生物突触遵循对数正态分布且生物网络在不可靠突触传输下能稳定运行的启发，探索能否在人工神经网络中设计类似的乘法训练。

Method: 推导假设权重后验分布为对数正态分布的贝叶斯学习规则，提出LMD算法，使用乘法更新，噪声和正则化均乘法应用，实现简单，仅需额外存储一个向量。

Result: LMD在Vision Transformer和GPT - 2的低精度前向运算下能从头开始稳定准确地训练。

Conclusion: 生物特征的乘法动力学可能在未来节能硬件上实现稳定的低精度推理和学习。

Abstract: Studies in neuroscience have shown that biological synapses follow a
log-normal distribution whose transitioning can be explained by noisy
multiplicative dynamics. Biological networks can function stably even under
dynamically fluctuating conditions arising due to unreliable synaptic
transmissions. Here we ask: Is it possible to design similar multiplicative
training in artificial neural networks? To answer this question, we derive a
Bayesian learning rule that assumes log-normal posterior distributions over
weights which gives rise to a new Log-Normal Multiplicative Dynamics (LMD)
algorithm. The algorithm uses multiplicative updates with both noise and
regularization applied multiplicatively. The method is as easy to implement as
Adam and only requires one additional vector to store. Our results show that
LMD achieves stable and accurate training-from-scratch under low-precision
forward operations for Vision Transformer and GPT-2. These results suggest that
multiplicative dynamics, a biological feature, may enable stable low-precision
inference and learning on future energy-efficient hardware.

</details>


### [165] [FFINO: Factorized Fourier Improved Neural Operator for Modeling Multiphase Flow in Underground Hydrogen Storage](https://arxiv.org/abs/2506.17344)
*Tao Wang,Hewei Tang*

Main category: cs.LG

TL;DR: 提出新神经算子架构FFINO作为地下氢存储（UHS）多相流问题快速替代模型，相比FMIONet有参数、时间和内存优势，推理速度远超数值模拟器。


<details>
  <summary>Details</summary>
Motivation: UHS是向低碳经济能源转型的有前景储能方案，快速建模氢羽流迁移和压力场演化对UHS现场管理至关重要。

Method: 提出FFINO模型，参数化文献报道的实验相对渗透率曲线并纳入模型作为关键不确定参数，用综合指标与FMIONet对比。

Result: FFINO比FMIONet可训练参数少38.1%，训练时间少17.6%，GPU内存成本少12%，预测氢羽流精度提高9.8%，预测压力积累RMSE高18%，推理时间比数值模拟器快7850倍。

Conclusion: FFINO是UHS问题数值模拟的有力替代，时间效率优越。

Abstract: Underground hydrogen storage (UHS) is a promising energy storage option for
the current energy transition to a low-carbon economy. Fast modeling of
hydrogen plume migration and pressure field evolution is crucial for UHS field
management. In this study, we propose a new neural operator architecture,
FFINO, as a fast surrogate model for multiphase flow problems in UHS. We
parameterize experimental relative permeability curves reported in the
literature and include them as key uncertainty parameters in the FFINO model.
We also compare the FFINO model with the state-of-the-art FMIONet model through
a comprehensive combination of metrics. Our new FFINO model has 38.1% fewer
trainable parameters, 17.6% less training time, and 12% less GPU memory cost
compared to FMIONet. The FFINO model also achieves a 9.8% accuracy improvement
in predicting hydrogen plume in focused areas, and 18% higher RMSE in
predicting pressure buildup. The inference time of the trained FFINO model is
7850 times faster than a numerical simulator, which makes it a competent
substitute for numerical simulations of UHS problems with superior time
efficiency.

</details>


### [166] [SING: SDE Inference via Natural Gradients](https://arxiv.org/abs/2506.17796)
*Amber Hu,Henry Smith,Scott Linderman*

Main category: cs.LG

TL;DR: 提出SING方法用于潜在随机微分方程模型的推理，该方法能快速可靠推理，在多数据集上表现优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 现有潜在SDE的变分推理方法存在收敛慢和数值不稳定问题，需更有效方法。

Method: 提出SING方法，利用自然梯度变分推理，近似难以处理的积分并并行化时间计算。

Result: SING在状态推理和漂移估计上优于先前方法，在多种数据集包括神经动力学建模中表现良好。

Conclusion: SING有潜力成为复杂动力系统精确推理的工具，尤其适用于先验知识有限和非共轭结构的系统。

Abstract: Latent stochastic differential equation (SDE) models are important tools for
the unsupervised discovery of dynamical systems from data, with applications
ranging from engineering to neuroscience. In these complex domains, exact
posterior inference of the latent state path is typically intractable,
motivating the use of approximate methods such as variational inference (VI).
However, existing VI methods for inference in latent SDEs often suffer from
slow convergence and numerical instability. Here, we propose SDE Inference via
Natural Gradients (SING), a method that leverages natural gradient VI to
efficiently exploit the underlying geometry of the model and variational
posterior. SING enables fast and reliable inference in latent SDE models by
approximating intractable integrals and parallelizing computations in time. We
provide theoretical guarantees that SING will approximately optimize the
intractable, continuous-time objective of interest. Moreover, we demonstrate
that better state inference enables more accurate estimation of nonlinear drift
functions using, for example, Gaussian process SDE models. SING outperforms
prior methods in state inference and drift estimation on a variety of datasets,
including a challenging application to modeling neural dynamics in freely
behaving animals. Altogether, our results illustrate the potential of SING as a
tool for accurate inference in complex dynamical systems, especially those
characterized by limited prior knowledge and non-conjugate structure.

</details>


### [167] [SAFEx: Analyzing Vulnerabilities of MoE-Based LLMs via Stable Safety-critical Expert Identification](https://arxiv.org/abs/2506.17368)
*Zhenglin Lai,Mengyao Liao,Dong Xu,Zebin Zhao,Zhihang Yuan,Chao Fan,Jianqiang Li,Bingzhe Wu*

Main category: cs.LG

TL;DR: 本文研究MoE模型安全对齐挑战，提出SAFEx框架，实验表明少量关键专家对模型安全影响大。


<details>
  <summary>Details</summary>
Motivation: 基于混合专家的大语言模型有独特安全对齐挑战，现有策略不适用于MoE模型，需研究其位置漏洞。

Method: 提出SAFEx分析框架，使用基于稳定性的专家选择（SES）算法识别、表征和验证安全关键专家。

Result: 主流MoE模型如Qwen3 - MoE的安全机制依赖少量位置专家，禁用少量专家会大幅降低模型拒绝有害请求的能力。

Conclusion: 少量安全关键专家对MoE模型整体安全有不成比例的影响。

Abstract: Large language models based on Mixture-of-Experts have achieved substantial
gains in efficiency and scalability, yet their architectural uniqueness
introduces underexplored safety alignment challenges. Existing safety alignment
strategies, predominantly designed for dense models, are ill-suited to address
MoE-specific vulnerabilities. In this work, we formalize and systematically
study MoE model's positional vulnerability - the phenomenon where
safety-aligned behaviors rely on specific expert modules, revealing critical
risks inherent to MoE architectures. To this end, we present SAFEx, an
analytical framework that robustly identifies, characterizes, and validates the
safety-critical experts using a novel Stability-based Expert Selection (SES)
algorithm. Notably, our approach enables the explicit decomposition of
safety-critical experts into distinct functional groups, including those
responsible for harmful content detection and those controlling safe response
generation. Extensive experiments on mainstream MoE models, such as the
recently released Qwen3-MoE, demonstrated that their intrinsic safety
mechanisms heavily rely on a small subset of positional experts. Disabling
these experts significantly compromised the models' ability to refuse harmful
requests. For Qwen3-MoE with 6144 experts (in the FNN layer), we find that
disabling as few as 12 identified safety-critical experts can cause the refusal
rate to drop by 22%, demonstrating the disproportionate impact of a small set
of experts on overall model safety.

</details>


### [168] [Flatness After All?](https://arxiv.org/abs/2506.17809)
*Neta Shoham,Liron Mor-Yosef,Haim Avron*

Main category: cs.LG

TL;DR: 论文提出用Hessian矩阵的软秩测度评估泛化性，在校准和未校准模型中均有分析，实验显示该方法评估泛化差距更稳健。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽指出‘平坦’极小值泛化性更好，但深度网络即使有任意锐度也能泛化，故提出用Hessian矩阵软秩测度评估泛化性。

Method: 对于校准的常见神经网络模型，分析测度与渐近期望泛化差距的关系；对于未校准模型，将测度与Takeuchi信息准则联系起来。

Result: 校准模型中测度能准确捕捉渐近期望泛化差距；未校准模型中，该测度为不过于自信的模型提供可靠的泛化差距估计。实验表明该方法比基线方法更稳健。

Conclusion: 提出的用Hessian矩阵软秩测度评估泛化性的方法可行且有效。

Abstract: Recent literature has examined the relationship between the curvature of the
loss function at minima and generalization, mainly in the context of
overparameterized networks. A key observation is that "flat" minima tend to
generalize better than "sharp" minima. While this idea is supported by
empirical evidence, it has also been shown that deep networks can generalize
even with arbitrary sharpness, as measured by either the trace or the spectral
norm of the Hessian. In this paper, we argue that generalization could be
assessed by measuring flatness using a soft rank measure of the Hessian. We
show that when the common neural network model (neural network with exponential
family negative log likelihood loss) is calibrated, and its prediction error
and its confidence in the prediction are not correlated with the first and the
second derivatives of the network's output, our measure accurately captures the
asymptotic expected generalization gap. For non-calibrated models, we connect
our flatness measure to the well-known Takeuchi Information Criterion and show
that it still provides reliable estimates of generalization gaps for models
that are not overly confident. Experimental results indicate that our approach
offers a robust estimate of the generalization gap compared to baselines.

</details>


### [169] [Aha Moment Revisited: Are VLMs Truly Capable of Self Verification in Inference-time Scaling?](https://arxiv.org/abs/2506.17417)
*Mingyuan Wu,Meitang Li,Jingcheng Yang,Jize Jiang,Kaizhuo Yan,Zhaoheng Li,Minjia Zhang,Klara Nahrstedt*

Main category: cs.LG

TL;DR: 研究推理时间技术对基于强化学习训练的视觉语言模型（VLM）的有效性，发现生成依赖方法比验证依赖方法效果更好，RL 训练的 VLM 缺乏鲁棒的自我验证能力。


<details>
  <summary>Details</summary>
Motivation: 探究推理时间技术是否能有效扩展到基于强化学习训练的视觉语言模型。

Method: 在推理时间缩放框架内进行广泛实验。

Result: 解码策略能提高 VLM 推理性能，生成依赖方法比验证依赖方法增益更高，RL 调优模型的自我修正行为无明显增益。

Conclusion: RL 训练的 VLMs 在视觉和文本模态上仍缺乏鲁棒的自我验证能力。

Abstract: Recent advances in large language models (LLMs) have demonstrated that
inference-time computation techniques, such as decoding-time scaling and
self-refinement, can significantly enhance reasoning capabilities without
relying on external knowledge. A key driver of this success is the emergence of
self-correction and self-verification behaviors, often elicited through
reinforcement learning (RL). In this paper, we investigate whether these
inference-time techniques extend effectively to vision-language models (VLMs),
particularly those trained with RL. We find that while decoding strategies such
as majority voting and best-of-N selection with self-verification all improve
VLM reasoning performance, generation-reliant methods such as the former
achieve significantly higher gains versus verification-reliant methods such as
the latter. Additionally, the self-correction behavior often associated with
RL-tuned models, such as aha moment, does not lead to measurable gains. We show
via extensive experimentation within the inference-time scaling framework to
identify a key root cause: RL-trained VLMs still lack robust self-verification
capabilities across both visual and textual modalities.

</details>


### [170] [FedNAMs: Performing Interpretability Analysis in Federated Learning Context](https://arxiv.org/abs/2506.17466)
*Amitash Nanda,Sree Bhargavi Balija,Debashis Sahoo*

Main category: cs.LG

TL;DR: 提出联邦神经加法模型（FedNAMs）解决联邦学习可解释性问题，研究表明其在多种任务上可在保证准确性的同时提供强可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在可解释性和可说明性方面的挑战。

Method: 在联邦学习框架中引入神经加法模型（NAMs），形成FedNAMs，结合NAMs和联邦学习优势。

Result: 在多种文本和图像分类任务中，与传统联邦深度神经网络相比，FedNAMs可在最小化精度损失的情况下提供强可解释性，并识别出不同数据集的关键预测特征。

Conclusion: FedNAMs增强了隐私性和模型效率，提高了跨不同数据集的可解释性和鲁棒性，还能洞察高低可解释性特征的成因。

Abstract: Federated learning continues to evolve but faces challenges in
interpretability and explainability. To address these challenges, we introduce
a novel approach that employs Neural Additive Models (NAMs) within a federated
learning framework. This new Federated Neural Additive Models (FedNAMs)
approach merges the advantages of NAMs, where individual networks concentrate
on specific input features, with the decentralized approach of federated
learning, ultimately producing interpretable analysis results. This integration
enhances privacy by training on local data across multiple devices, thereby
minimizing the risks associated with data centralization and improving model
robustness and generalizability. FedNAMs maintain detailed, feature-specific
learning, making them especially valuable in sectors such as finance and
healthcare. They facilitate the training of client-specific models to integrate
local updates, preserve privacy, and mitigate concerns related to
centralization. Our studies on various text and image classification tasks,
using datasets such as OpenFetch ML Wine, UCI Heart Disease, and Iris, show
that FedNAMs deliver strong interpretability with minimal accuracy loss
compared to traditional Federated Deep Neural Networks (DNNs). The research
involves notable findings, including the identification of critical predictive
features at both client and global levels. Volatile acidity, sulfates, and
chlorides for wine quality. Chest pain type, maximum heart rate, and number of
vessels for heart disease. Petal length and width for iris classification. This
approach strengthens privacy and model efficiency and improves interpretability
and robustness across diverse datasets. Finally, FedNAMs generate insights on
causes of highly and low interpretable features.

</details>


### [171] [Choice of Scoring Rules for Indirect Elicitation of Properties with Parametric Assumptions](https://arxiv.org/abs/2506.17880)
*Lingfang Hu,Ian A. Kash*

Main category: cs.LG

TL;DR: 本文探讨参数假设下属性的间接引出问题，研究权重选择对目标属性估计的影响，通过模拟研究发现规律，并建立理论框架解释结果。


<details>
  <summary>Details</summary>
Motivation: 以往研究多关注适当评分规则的存在和特征，少讨论其应用选择，本文旨在研究参数假设下间接引出属性时权重选择对目标属性估计的影响及最优选择。

Method: 先进行模拟研究，观察规律；再建立基本理论框架，分别给出两个子属性和多个子属性情况的充分条件；在高维情况研究线性情况，提出复杂情况可局部映射或线性近似处理。

Result: 模拟研究发现多数情况下目标属性的最优估计随每个权重增加单调变化，最佳权重配置常是某些权重设为零；二维情况理论完美解释实验结果。

Conclusion: 建立的理论框架能解释实验结果，高维线性情况及复杂情况可通过特定方法处理来理解。

Abstract: People are commonly interested in predicting a statistical property of a
random event such as mean and variance. Proper scoring rules assess the quality
of predictions and require that the expected score gets uniquely maximized at
the precise prediction, in which case we call the score directly elicits the
property. Previous research work has widely studied the existence and the
characterization of proper scoring rules for different properties, but little
literature discusses the choice of proper scoring rules for applications at
hand. In this paper, we explore a novel task, the indirect elicitation of
properties with parametric assumptions, where the target property is a function
of several directly-elicitable sub-properties and the total score is a weighted
sum of proper scoring rules for each sub-property. Because of the restriction
to a parametric model class, different settings for the weights lead to
different constrained optimal solutions. Our goal is to figure out how the
choice of weights affects the estimation of the target property and which
choice is the best. We start it with simulation studies and observe an
interesting pattern: in most cases, the optimal estimation of the target
property changes monotonically with the increase of each weight, and the best
configuration of weights is often to set some weights as zero. To understand
how it happens, we first establish the elementary theoretical framework and
then provide deeper sufficient conditions for the case of two sub-properties
and of more sub-properties respectively. The theory on 2-D cases perfectly
interprets the experimental results. In higher-dimensional situations, we
especially study the linear cases and suggest that more complex settings can be
understood with locally mapping into linear situations or using linear
approximations when the true values of sub-properties are close enough to the
parametric space.

</details>


### [172] [A geometric framework for momentum-based optimizers for low-rank training](https://arxiv.org/abs/2506.17475)
*Steffen Schotthöfer,Timon Klein,Jonas Kusch*

Main category: cs.LG

TL;DR: 文章分析传统优化器训练低秩参数化的问题，提出基于动态低秩近似的训练策略，实验验证其效果更好。


<details>
  <summary>Details</summary>
Motivation: 识别并分析传统优化器（如重球动量法或Adam）训练神经网络低秩参数化时遇到的潜在困难。

Method: 引入基于动态低秩近似的训练策略，结合动态低秩近似和基于动量的优化工具设计优化器。

Result: 通过数值实验验证，该方法在给定参数预算下收敛更快，验证指标更强。

Conclusion: 所提出的基于动态低秩近似的训练策略有效，能克服传统方法在训练低秩参数化时的问题。

Abstract: Low-rank pre-training and fine-tuning have recently emerged as promising
techniques for reducing the computational and storage costs of large neural
networks. Training low-rank parameterizations typically relies on conventional
optimizers such as heavy ball momentum methods or Adam. In this work, we
identify and analyze potential difficulties that these training methods
encounter when used to train low-rank parameterizations of weights. In
particular, we show that classical momentum methods can struggle to converge to
a local optimum due to the geometry of the underlying optimization landscape.
To address this, we introduce novel training strategies derived from dynamical
low-rank approximation, which explicitly account for the underlying geometric
structure. Our approach leverages and combines tools from dynamical low-rank
approximation and momentum-based optimization to design optimizers that respect
the intrinsic geometry of the parameter space. We validate our methods through
numerical experiments, demonstrating faster convergence, and stronger
validation metrics at given parameter budgets.

</details>


### [173] [An entropy-optimal path to humble AI](https://arxiv.org/abs/2506.17940)
*Davide Bassetti,Lukáš Pospíšil,Michael Groom,Terence J. O'Kane,Illia Horenko*

Main category: cs.LG

TL;DR: 本文提出基于全概率定律的玻尔兹曼机非平衡熵优化数学框架，成本低、性能高，在合成问题和气候数据预测上表现佳。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型成本高、过度自信，需要改进。

Method: 引入基于全概率定律的非平衡熵优化的玻尔兹曼机数学框架。

Result: 该框架无需梯度下降学习，有存在唯一性准则和答案置信度衡量；在合成问题上模型性能更好、更精简；在气候数据预测上对拉尼娜和厄尔尼诺现象预测能力更强，训练数据需求少。

Conclusion: 所提方法能构建高性能、低成本模型，在多方面表现优于现有AI工具。

Abstract: Progress of AI has led to a creation of very successful, but by no means
humble models and tools, especially regarding (i) the huge and further
exploding costs and resources they demand, and (ii) the over-confidence of
these tools with the answers they provide. Here we introduce a novel
mathematical framework for a non-equilibrium entropy-optimizing reformulation
of Boltzmann machines based on the exact law of total probability. It results
in the highly-performant, but much cheaper, gradient-descent-free learning
framework with mathematically-justified existence and uniqueness criteria, and
answer confidence/reliability measures. Comparisons to state-of-the-art AI
tools in terms of performance, cost and the model descriptor lengths on a set
of synthetic problems with varying complexity reveal that the proposed method
results in more performant and slim models, with the descriptor lengths being
very close to the intrinsic complexity scaling bounds for the underlying
problems. Applying this framework to historical climate data results in models
with systematically higher prediction skills for the onsets of La Ni\~na and El
Ni\~no climate phenomena, requiring just few years of climate data for training
- a small fraction of what is necessary for contemporary climate prediction
tools.

</details>


### [174] [Episode-specific Fine-tuning for Metric-based Few-shot Learners with Optimization-based Training](https://arxiv.org/abs/2506.17499)
*Xuanyu Zhuang,Geoffroy Peeters,Gaël Richard*

Main category: cs.LG

TL;DR: 提出基于度量的模型的推理时微调方法，结合元训练避免过拟合，在三个音频数据集验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有度量模型对有标签支持样本利用不足，且小样本数据微调易过拟合。

Method: 提出RDFT及两个变体IDFT和ADFT进行推理时微调，在优化的元学习框架中训练模型。

Result: 在三个音频数据集实验，所提方法提升所有评估的基于度量的模型性能，尤其基于注意力的模型，且跨音频领域泛化性好。

Conclusion: 结合特定情节微调与优化的元训练，能让基于度量的模型快速适应有限支持样本并避免过拟合。

Abstract: In few-shot classification tasks (so-called episodes), a small set of labeled
support samples is provided during inference to aid the classification of
unlabeled query samples. Metric-based models typically operate by computing
similarities between query and support embeddings within a learned metric
space, followed by nearest-neighbor classification. However, these labeled
support samples are often underutilized--they are only used for similarity
comparison, despite their potential to fine-tune and adapt the metric space
itself to the classes in the current episode. To address this, we propose a
series of simple yet effective episode-specific, during-inference fine-tuning
methods for metric-based models, including Rotational Division Fine-Tuning
(RDFT) and its two variants, Iterative Division Fine-Tuning (IDFT) and
Augmented Division Fine-Tuning (ADFT). These methods construct pseudo
support-query pairs from the given support set to enable fine-tuning even for
non-parametric models. Nevertheless, the severely limited amount of data in
each task poses a substantial risk of overfitting when applying such
fine-tuning strategies. To mitigate this, we further propose to train the
metric-based model within an optimization-based meta-learning framework. With
the combined efforts of episode-specific fine-tuning and optimization-based
meta-training, metric-based models are equipped with the ability to rapidly
adapt to the limited support samples during inference while avoiding
overfitting. We validate our approach on three audio datasets from diverse
domains, namely ESC-50 (environmental sounds), Speech Commands V2 (spoken
keywords), and Medley-solos-DB (musical instrument). Experimental results
demonstrate that our approach consistently improves performance for all
evaluated metric-based models (especially for attention-based models) and
generalizes well across different audio domains.

</details>


### [175] [h-calibration: Rethinking Classifier Recalibration with Probabilistic Error-Bounded Objective](https://arxiv.org/abs/2506.17968)
*Wenjian Huang,Guiping Cao,Jiahao Xia,Jingkun Chen,Hao Wang,Jianguo Zhang*

Main category: cs.LG

TL;DR: 本文总结先前校准方法，指出局限，提出h - calibration概率学习框架及校准算法，实验验证其优于传统方法，理论分析了学习目标优势，在基准测试达最优。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络常存在校准误差，概率输出不可靠，此前有缓解校准误差的工作，但存在局限，需改进。

Method: 将先前工作总结为三类策略，指出其十种局限，提出h - calibration概率学习框架，设计后验校准算法。

Result: 所提方法克服了先前方法的十种局限，实验表现明显优于传统方法，在标准后验校准基准测试中达最优。

Conclusion: 概率框架为学习误差有界的校准概率推导了近似等效的可微目标，阐明了计算统计与规范校准理论界限的对应和收敛性质，为相关领域学习可靠似然提供参考。

Abstract: Deep neural networks have demonstrated remarkable performance across numerous
learning tasks but often suffer from miscalibration, resulting in unreliable
probability outputs. This has inspired many recent works on mitigating
miscalibration, particularly through post-hoc recalibration methods that aim to
obtain calibrated probabilities without sacrificing the classification
performance of pre-trained models. In this study, we summarize and categorize
previous works into three general strategies: intuitively designed methods,
binning-based methods, and methods based on formulations of ideal calibration.
Through theoretical and practical analysis, we highlight ten common limitations
in previous approaches. To address these limitations, we propose a
probabilistic learning framework for calibration called h-calibration, which
theoretically constructs an equivalent learning formulation for canonical
calibration with boundedness. On this basis, we design a simple yet effective
post-hoc calibration algorithm. Our method not only overcomes the ten
identified limitations but also achieves markedly better performance than
traditional methods, as validated by extensive experiments. We further analyze,
both theoretically and experimentally, the relationship and advantages of our
learning objective compared to traditional proper scoring rule. In summary, our
probabilistic framework derives an approximately equivalent differentiable
objective for learning error-bounded calibrated probabilities, elucidating the
correspondence and convergence properties of computational statistics with
respect to theoretical bounds in canonical calibration. The theoretical
effectiveness is verified on standard post-hoc calibration benchmarks by
achieving state-of-the-art performance. This research offers valuable reference
for learning reliable likelihood in related fields.

</details>


### [176] [Predicting E-commerce Purchase Behavior using a DQN-Inspired Deep Learning Model for enhanced adaptability](https://arxiv.org/abs/2506.17543)
*Aditi Madhusudan Jain*

Main category: cs.LG

TL;DR: 本文提出基于DQN架构预测电商购买意图和产品需求的方法，在大规模数据集上验证，表现优于传统方法，适合电商应用。


<details>
  <summary>Details</summary>
Motivation: 在电商领域，准确预测用户行为对优化库存管理、个性化用户体验和提高销售至关重要。

Method: 将强化学习概念应用于监督学习，结合LSTM网络的序列建模能力和DQN的战略决策方面。

Result: 模型在处理电商数据的类别不平衡问题上表现出色，准确率达88%，AUC - ROC得分为0.88，优于传统机器学习和标准深度学习方法。

Conclusion: 引入结合深度学习和强化学习优势的预测建模技术，对电商需求预测、用户体验个性化和营销策略优化有重要意义。

Abstract: This paper presents a novel approach to predicting buying intent and product
demand in e-commerce settings, leveraging a Deep Q-Network (DQN) inspired
architecture. In the rapidly evolving landscape of online retail, accurate
prediction of user behavior is crucial for optimizing inventory management,
personalizing user experiences, and maximizing sales. Our method adapts
concepts from reinforcement learning to a supervised learning context,
combining the sequential modeling capabilities of Long Short-Term Memory (LSTM)
networks with the strategic decision-making aspects of DQNs. We evaluate our
model on a large-scale e-commerce dataset comprising over 885,000 user
sessions, each characterized by 1,114 features. Our approach demonstrates
robust performance in handling the inherent class imbalance typical in
e-commerce data, where purchase events are significantly less frequent than
non-purchase events. Through comprehensive experimentation with various
classification thresholds, we show that our model achieves a balance between
precision and recall, with an overall accuracy of 88\% and an AUC-ROC score of
0.88. Comparative analysis reveals that our DQN-inspired model offers
advantages over traditional machine learning and standard deep learning
approaches, particularly in its ability to capture complex temporal patterns in
user behavior. The model's performance and scalability make it well-suited for
real-world e-commerce applications dealing with high-dimensional, sequential
data. This research contributes to the field of e-commerce analytics by
introducing a novel predictive modeling technique that combines the strengths
of deep learning and reinforcement learning paradigms. Our findings have
significant implications for improving demand forecasting, personalizing user
experiences, and optimizing marketing strategies in online retail environments.

</details>


### [177] [Imputation of Longitudinal Data Using GANs: Challenges and Implications for Classification](https://arxiv.org/abs/2506.18007)
*Sharon Torao Pingi,Md Abul Bashar,Richi Nayak*

Main category: cs.LG

TL;DR: 本文全面综述GAN在纵向数据插补中应用，指出其虽有潜力但需更通用方法应对挑战，旨在为未来研究提供指引。


<details>
  <summary>Details</summary>
Motivation: 纵向数据分类复杂且存在缺失值等问题，现有基于GAN研究未充分考虑数据基本假设，需全面评估其在纵向数据插补应用。

Method: 对基于GAN的纵向数据插补主要方法分类，分析优缺点，确定关键研究趋势。

Result: GAN在纵向数据插补有潜力提升数据可用性和质量，但需更通用方法应对含缺失值纵向数据挑战。

Conclusion: 通过整合现有知识和确定研究差距，为未来开发更有效基于GAN解决方案应对纵向数据分类挑战提供方向。

Abstract: Longitudinal data is commonly utilised across various domains, such as
health, biomedical, education and survey studies. This ubiquity has led to a
rise in statistical, machine and deep learning-based methods for Longitudinal
Data Classification (LDC). However, the intricate nature of the data,
characterised by its multi-dimensionality, causes instance-level heterogeneity
and temporal correlations that add to the complexity of longitudinal data
analysis. Additionally, LDC accuracy is often hampered by the pervasiveness of
missing values in longitudinal data. Despite ongoing research that draw on the
generative power and utility of Generative Adversarial Networks (GANs) to
address the missing data problem, critical considerations include statistical
assumptions surrounding longitudinal data and missingness within it, as well as
other data-level challenges like class imbalance and mixed data types that
impact longitudinal data imputation (LDI) and the subsequent LDC process in
GANs. This paper provides a comprehensive overview of how GANs have been
applied in LDI, with a focus whether GANS have adequately addressed fundamental
assumptions about the data from a LDC perspective. We propose a categorisation
of main approaches to GAN-based LDI, highlight strengths and limitations of
methods, identify key research trends, and provide promising future directions.
Our findings indicate that while GANs show great potential for LDI to improve
usability and quality of longitudinal data for tasks like LDC, there is need
for more versatile approaches that can handle the wider spectrum of challenges
presented by longitudinal data with missing values. By synthesising current
knowledge and identifying critical research gaps, this survey aims to guide
future research efforts in developing more effective GAN-based solutions to
address LDC challenges.

</details>


### [178] [DRIMV_TSK: An Interpretable Surgical Evaluation Model for Incomplete Multi-View Rectal Cancer Data](https://arxiv.org/abs/2506.17552)
*Wei Zhang,Zi Wang,Hanwen Zhou,Zhaohong Deng,Weiping Ding,Yuxi Ge,Te Zhang,Yuanpeng Zhang,Kup-Sze Choi,Shitong Wang,Shudong Hu*

Main category: cs.LG

TL;DR: 本文构建多视图直肠癌数据集，提出可解释的不完整多视图手术评估模型，在MVRC数据集上效果最佳。


<details>
  <summary>Details</summary>
Motivation: 可靠评估直肠癌手术难度可提高治疗成功率，当前评估基于临床数据，技术发展使更多数据收集及人工智能应用成为可能。

Method: 先构建多视图数据集，再提出双表示不完整多视图学习模型提取信息，将缺失视图插补融入表示学习并引入二阶相似性约束；基于插补数据和学习的双表示，提出带TSK模糊系统的多视图手术评估模型，构建合作学习机制并引入香农熵调整视图权重。

Result: 在MVRC数据集上与多种先进算法对比，DRIMV_TSK取得最佳结果。

Conclusion: 所提出的可解释不完整多视图手术评估模型在直肠癌手术难度评估中表现良好。

Abstract: A reliable evaluation of surgical difficulty can improve the success of the
treatment for rectal cancer and the current evaluation method is based on
clinical data. However, more data about rectal cancer can be collected with the
development of technology. Meanwhile, with the development of artificial
intelligence, its application in rectal cancer treatment is becoming possible.
In this paper, a multi-view rectal cancer dataset is first constructed to give
a more comprehensive view of patients, including the high-resolution MRI image
view, pressed-fat MRI image view, and clinical data view. Then, an
interpretable incomplete multi-view surgical evaluation model is proposed,
considering that it is hard to obtain extensive and complete patient data in
real application scenarios. Specifically, a dual representation incomplete
multi-view learning model is first proposed to extract the common information
between views and specific information in each view. In this model, the missing
view imputation is integrated into representation learning, and second-order
similarity constraint is also introduced to improve the cooperative learning
between these two parts. Then, based on the imputed multi-view data and the
learned dual representation, a multi-view surgical evaluation model with the
TSK fuzzy system is proposed. In the proposed model, a cooperative learning
mechanism is constructed to explore the consistent information between views,
and Shannon entropy is also introduced to adapt the view weight. On the MVRC
dataset, we compared it with several advanced algorithms and DRIMV_TSK obtained
the best results.

</details>


### [179] [Probing the Embedding Space of Transformers via Minimal Token Perturbations](https://arxiv.org/abs/2506.18011)
*Eddie Conti,Alejandro Astruc,Alvaro Parafita,Axel Brando*

Main category: cs.LG

TL;DR: 研究最小令牌扰动对嵌入空间的影响，验证模型第一层可用于解释模型，引入令牌扰动和嵌入空间偏移组合作为解释工具。


<details>
  <summary>Details</summary>
Motivation: 理解信息在Transformer模型中的传播方式，解决模型可解释性的关键挑战。

Method: 研究最小令牌扰动对嵌入空间的影响，分析令牌产生最小偏移的频率，研究扰动在各层的传播情况。

Result: 稀有令牌通常导致更大偏移，输入信息在更深层越来越混合。

Conclusion: 模型的第一层可作为模型解释的代理，令牌扰动和嵌入空间偏移组合是强大的模型可解释性工具。

Abstract: Understanding how information propagates through Transformer models is a key
challenge for interpretability. In this work, we study the effects of minimal
token perturbations on the embedding space. In our experiments, we analyze the
frequency of which tokens yield to minimal shifts, highlighting that rare
tokens usually lead to larger shifts. Moreover, we study how perturbations
propagate across layers, demonstrating that input information is increasingly
intermixed in deeper layers. Our findings validate the common assumption that
the first layers of a model can be used as proxies for model explanations.
Overall, this work introduces the combination of token perturbations and shifts
on the embedding space as a powerful tool for model interpretability.

</details>


### [180] [Accelerating Residual Reinforcement Learning with Uncertainty Estimation](https://arxiv.org/abs/2506.17564)
*Lakshita Dodeja,Karl Schmeckpeper,Shivam Vats,Thomas Weng,Mingxi Jia,George Konidaris,Stefanie Tellex*

Main category: cs.LG

TL;DR: 提出对残差强化学习的两项改进，提升样本效率，适用于随机基础策略，在模拟环境中显著优于基线，且可零样本从模拟迁移到现实。


<details>
  <summary>Details</summary>
Motivation: 现有残差强化学习方法在稀疏奖励场景有困难且适用于确定性基础策略，需改进以提升样本效率和适配随机基础策略。

Method: 一是利用基础策略的不确定性估计，聚焦基础策略不自信区域进行探索；二是对离策略残差学习进行简单修改，使其能观察基础动作并处理随机基础策略。

Result: 在多种模拟基准环境中，算法显著优于现有基线；可零样本从模拟迁移到现实，展示了策略的鲁棒性。

Conclusion: 提出的改进使残差强化学习更高效且适用于随机基础策略，具有实际应用价值。

Abstract: Residual Reinforcement Learning (RL) is a popular approach for adapting
pretrained policies by learning a lightweight residual policy that provides
corrective actions. While Residual RL is more sample-efficient than finetuning
the entire base policy, existing methods struggle with sparse rewards and are
designed for deterministic base policies. We propose two improvements to
Residual RL that further enhance its sample efficiency and make it suitable for
stochastic base policies. First, we leverage uncertainty estimates of the base
policy to focus exploration on regions in which the base policy is not
confident. Second, we propose a simple modification to off-policy residual
learning that allows it to observe base actions and better handle stochastic
base policies. We evaluate our method with both Gaussian-based and
Diffusion-based stochastic base policies on tasks from Robosuite and D4RL, and
compare against state-of-the-art finetuning methods, demo-augmented RL methods,
and other residual RL methods. Our algorithm significantly outperforms existing
baselines in a variety of simulation benchmark environments. We also deploy our
learned polices in the real world to demonstrate their robustness with
zero-shot sim-to-real transfer.

</details>


### [181] [Generalization under Byzantine & Poisoning Attacks: Tight Stability Bounds in Robust Distributed Learning](https://arxiv.org/abs/2506.18020)
*Thomas Boudou,Batiste Le Bars,Nirupam Gupta,Aurélien Bellet*

Main category: cs.LG

TL;DR: 研究拜占庭攻击和数据投毒攻击对分布式学习泛化性的影响，理论证明拜占庭攻击对泛化性危害更大。


<details>
  <summary>Details</summary>
Motivation: 先前工作未明确拜占庭攻击和数据投毒攻击对泛化性的影响，有必要进行理论研究。

Method: 对两种攻击模型下鲁棒分布式学习算法的统一算法稳定性进行理论分析和证明。

Result: 数据投毒下，最优优化误差的算法稳定性下降为$\varTheta ( \frac{f}{n-f} )$；拜占庭攻击下为$\mathcal{O} ( \sqrt{ \frac{f}{n-2f}} )$。

Conclusion: 拜占庭攻击本质上比数据投毒对泛化性更有害，在$f$接近$\frac{n}{2}$时泛化误差差距显著。

Abstract: Robust distributed learning algorithms aim to maintain good performance in
distributed and federated settings, even in the presence of misbehaving
workers. Two primary threat models have been studied: Byzantine attacks, where
misbehaving workers can send arbitrarily corrupted updates, and data poisoning
attacks, where misbehavior is limited to manipulation of local training data.
While prior work has shown comparable optimization error under both threat
models, a fundamental question remains open: How do these threat models impact
generalization? Empirical evidence suggests a gap between the two threat
models, yet it remains unclear whether it is fundamental or merely an artifact
of suboptimal attacks. In this work, we present the first theoretical
investigation into this problem, formally showing that Byzantine attacks are
intrinsically more harmful to generalization than data poisoning. Specifically,
we prove that: (i) under data poisoning, the uniform algorithmic stability of a
robust distributed learning algorithm, with optimal optimization error,
degrades by an additive factor of $\varTheta ( \frac{f}{n-f} )$, with $f$ the
number of misbehaving workers out of $n$; and (ii) In contrast, under Byzantine
attacks, the degradation is in $\mathcal{O} \big( \sqrt{ \frac{f}{n-2f}}
\big)$.This difference in stability leads to a generalization error gap that is
especially significant as $f$ approaches its maximum value $\frac{n}{2}$.

</details>


### [182] [Towards Deeper GCNs: Alleviating Over-smoothing via Iterative Training and Fine-tuning](https://arxiv.org/abs/2506.17576)
*Furong Peng,Jinzhen Gao,Xuan Lu,Kang Liu,Yifan Huo,Sheng Wang*

Main category: cs.LG

TL;DR: 指出GCN中可训练线性变换加剧特征崩溃，提出LGT训练策略，实验表明其在深度GCN上性能优异且可与现有方法结合。


<details>
  <summary>Details</summary>
Motivation: 解决GCN深度架构下因可训练线性变换导致的过平滑和表达能力权衡问题。

Method: 提出LGT训练策略，包含逐层训练、低秩适应和恒等初始化三个组件。

Result: 在基准数据集上，LGT使普通GCN达到SOTA性能，还能与现有方法结合提升性能。

Conclusion: LGT为可扩展的深度GCN提供通用、与架构无关的训练框架。

Abstract: Graph Convolutional Networks (GCNs) suffer from severe performance
degradation in deep architectures due to over-smoothing. While existing studies
primarily attribute the over-smoothing to repeated applications of graph
Laplacian operators, our empirical analysis reveals a critical yet overlooked
factor: trainable linear transformations in GCNs significantly exacerbate
feature collapse, even at moderate depths (e.g., 8 layers). In contrast,
Simplified Graph Convolution (SGC), which removes these transformations,
maintains stable feature diversity up to 32 layers, highlighting linear
transformations' dual role in facilitating expressive power and inducing
over-smoothing. However, completely removing linear transformations weakens the
model's expressive capacity.
  To address this trade-off, we propose Layer-wise Gradual Training (LGT), a
novel training strategy that progressively builds deep GCNs while preserving
their expressiveness. LGT integrates three complementary components: (1)
layer-wise training to stabilize optimization from shallow to deep layers, (2)
low-rank adaptation to fine-tune shallow layers and accelerate training, and
(3) identity initialization to ensure smooth integration of new layers and
accelerate convergence. Extensive experiments on benchmark datasets demonstrate
that LGT achieves state-of-the-art performance on vanilla GCN, significantly
improving accuracy even in 32-layer settings. Moreover, as a training method,
LGT can be seamlessly combined with existing methods such as PairNorm and
ContraNorm, further enhancing their performance in deeper networks. LGT offers
a general, architecture-agnostic training framework for scalable deep GCNs. The
code is available at [https://github.com/jfklasdfj/LGT_GCN].

</details>


### [183] [LFR-PINO: A Layered Fourier Reduced Physics-Informed Neural Operator for Parametric PDEs](https://arxiv.org/abs/2506.17582)
*Jing Wang,Biao Chen,Hairun Xie,Rui Wang,Yifan Xia,Jifa Zhang,Hui Xu*

Main category: cs.LG

TL;DR: 提出LFR - PINO解决现有物理信息神经算子问题，实验表明其能降低误差和内存使用，平衡计算效率与解的保真度。


<details>
  <summary>Details</summary>
Motivation: 现有物理信息神经算子方法存在表达能力有限或计算挑战问题，需改进以更好解决参数化偏微分方程。

Method: 提出LFR - PINO，引入分层超网络架构和频域缩减策略，通过预训练学习通用PDE求解器。

Result: 在四个代表性PDE问题实验中，LFR - PINO较现有基线误差降低22.8% - 68.7%，频域缩减策略较Hyper - PINNs内存使用降低28.6% - 69.3%且保持解的精度。

Conclusion: LFR - PINO能有效平衡计算效率和解的保真度。

Abstract: Physics-informed neural operators have emerged as a powerful paradigm for
solving parametric partial differential equations (PDEs), particularly in the
aerospace field, enabling the learning of solution operators that generalize
across parameter spaces. However, existing methods either suffer from limited
expressiveness due to fixed basis/coefficient designs, or face computational
challenges due to the high dimensionality of the parameter-to-weight mapping
space. We present LFR-PINO, a novel physics-informed neural operator that
introduces two key innovations: (1) a layered hypernetwork architecture that
enables specialized parameter generation for each network layer, and (2) a
frequency-domain reduction strategy that significantly reduces parameter count
while preserving essential spectral features. This design enables efficient
learning of a universal PDE solver through pre-training, capable of directly
handling new equations while allowing optional fine-tuning for enhanced
precision. The effectiveness of this approach is demonstrated through
comprehensive experiments on four representative PDE problems, where LFR-PINO
achieves 22.8%-68.7% error reduction compared to state-of-the-art baselines.
Notably, frequency-domain reduction strategy reduces memory usage by
28.6%-69.3% compared to Hyper-PINNs while maintaining solution accuracy,
striking an optimal balance between computational efficiency and solution
fidelity.

</details>


### [184] [Bayesian Multiobject Tracking With Neural-Enhanced Motion and Measurement Models](https://arxiv.org/abs/2506.18124)
*Shaoxiu Wei,Mingchao Liang,Florian Meyer*

Main category: cs.LG

TL;DR: 本文提出一种混合方法，结合基于模型和数据驱动方法优势，提升贝叶斯多目标跟踪性能，在nuScenes数据集上达先进水平。


<details>
  <summary>Details</summary>
Motivation: 传统和数据驱动的多目标跟踪方法各有优势，思考能否将二者结合形成通用框架。

Method: 利用神经网络改进贝叶斯多目标跟踪统计模型中过于简化的部分，用信念传播避免高维运算，结合序贯蒙特卡罗方法高效执行低维运算。

Result: 提出的方法结合了基于模型方法的灵活性和鲁棒性以及神经网络从数据中学习复杂信息的能力。

Conclusion: 该方法在nuScenes自动驾驶数据集上评估，具有先进性能。

Abstract: Multiobject tracking (MOT) is an important task in applications including
autonomous driving, ocean sciences, and aerospace surveillance. Traditional MOT
methods are model-based and combine sequential Bayesian estimation with data
association and an object birth model. More recent methods are fully
data-driven and rely on the training of neural networks. Both approaches offer
distinct advantages in specific settings. In particular, model-based methods
are generally applicable across a wide range of scenarios, whereas data-driven
MOT achieves superior performance in scenarios where abundant labeled data for
training is available. A natural thought is whether a general framework can
integrate the two approaches. This paper introduces a hybrid method that
utilizes neural networks to enhance specific aspects of the statistical model
in Bayesian MOT that have been identified as overly simplistic. By doing so,
the performance of the prediction and update steps of Bayesian MOT is improved.
To ensure tractable computation, our framework uses belief propagation to avoid
high-dimensional operations combined with sequential Monte Carlo methods to
perform low-dimensional operations efficiently. The resulting method combines
the flexibility and robustness of model-based approaches with the capability to
learn complex information from data of neural networks. We evaluate the
performance of the proposed method based on the nuScenes autonomous driving
dataset and demonstrate that it has state-of-the-art performance

</details>


### [185] [Online Learning of Whittle Indices for Restless Bandits with Non-Stationary Transition Kernels](https://arxiv.org/abs/2506.18186)
*Md Kamran Chowdhury Shisher,Vishrant Tripathi,Mung Chiang,Christopher G. Brinton*

Main category: cs.LG

TL;DR: 本文针对未知非平稳环境下的不安分多臂老虎机（RMABs）提出在线学习算法，可利用先验知识加速学习，数值结果显示其累积遗憾低于基线。


<details>
  <summary>Details</summary>
Motivation: RMABs最优求解困难，且实际中计算Whittle指数所需的转移核未知且非平稳，需有效算法。

Method: 先通过求解线性优化问题预测当前转移核，再计算相关Whittle指数，设计滑动窗口和上置信界保证次线性动态遗憾，利用先验知识加速学习。

Result: 数值结果表明，算法在非平稳环境中累积遗憾低于基线。

Conclusion: 提出的在线学习算法在未知非平稳RMABs环境中有优越性能，能利用先验知识加速学习。

Abstract: We consider optimal resource allocation for restless multi-armed bandits
(RMABs) in unknown, non-stationary settings. RMABs are PSPACE-hard to solve
optimally, even when all parameters are known. The Whittle index policy is
known to achieve asymptotic optimality for a large class of such problems,
while remaining computationally efficient. In many practical settings, however,
the transition kernels required to compute the Whittle index are unknown and
non-stationary. In this work, we propose an online learning algorithm for
Whittle indices in this setting. Our algorithm first predicts current
transition kernels by solving a linear optimization problem based on upper
confidence bounds and empirical transition probabilities calculated from data
over a sliding window. Then, it computes the Whittle index associated with the
predicted transition kernels. We design these sliding windows and upper
confidence bounds to guarantee sub-linear dynamic regret on the number of
episodes $T$, under the condition that transition kernels change slowly over
time (rate upper bounded by $\epsilon=1/T^k$ with $k>0$). Furthermore, our
proposed algorithm and regret analysis are designed to exploit prior domain
knowledge and structural information of the RMABs to accelerate the learning
process. Numerical results validate that our algorithm achieves superior
performance in terms of lowest cumulative regret relative to baselines in
non-stationary environments.

</details>


### [186] [EQuARX: Efficient Quantized AllReduce in XLA for Distributed Machine Learning Acceleration](https://arxiv.org/abs/2506.17615)
*Ibrahim Ahmed,Clemens Schaefer,Gil Tabak,Denis Vnukov,Zenong Zhang,Felix chern,Anatoliy Yevtushenko,Andy Davis*

Main category: cs.LG

TL;DR: 提出适用于TPU的XLA编译器的动态块级高效量化AllReduce（EQuARX），在不同网络拓扑上比基线BF16 AllReduce提速1.8倍，对Gemma 3不同模型预填充阶段也有加速效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型规模大，部署时跨设备通信有性能开销，直接对AllReduce量化有困难，易导致数值不稳定和误差累积。

Method: 在TPU的XLA编译器中实现原生动态块级高效量化AllReduce（EQuARX），采用TPU友好的量化和通信与计算深度流水线技术。

Result: EQuARX int8精度在不同网络拓扑上比基线BF16 AllReduce提速1.8倍，分别加速Gemma 3 27B预填充阶段1.25倍、Gemma 3 12B 1.1倍，对质量影响小。

Conclusion: EQuARX能有效提高大语言模型部署时AllReduce操作的性能，且对模型质量影响小。

Abstract: While Large Language Models (LLMs) have become highly influential, their
enormous scale presents significant deployment challenges. Efficiently serving
these models typically requires distributing them across numerous accelerator
devices, which introduces substantial performance overhead from inter-device
communication (collectives). While model quantization has been widely adopted
to reduce the memory and compute requirements of LLM weights and activations
with minimal quality impact, applying quantization directly to collectives like
AllReduce is inherently difficult due to the inter-device summation involved,
which can lead to numerical instability or significant error accumulation. In
this work, we present a native dynamic block-wise efficient quantized AllReduce
within the XLA compiler for TPUs (EQuARX). By using TPU-friendly quantization
and deep pipelining of communication and compute, EQuARX with int8 precision
achieves a 1.8X speedup over baseline BF16 AllReduce across various network
topologies. Furthermore, EQuARX accelerates the prefill stage of Gemma 3 27B by
1.25X and Gemma 3 12B by 1.1X, respectively, with small to negligible impact on
quality.

</details>


### [187] [Trustworthy Chronic Disease Risk Prediction For Self-Directed Preventive Care via Medical Literature Validation](https://arxiv.org/abs/2506.17620)
*Minh Le,Khoi Ton*

Main category: cs.LG

TL;DR: 本文开发仅用个人和生活方式因素预测13种慢性病风险的深度学习模型，用SHAP解释特征并与医学文献验证，结果显示模型特征与文献一致，可用于慢性病预测。


<details>
  <summary>Details</summary>
Motivation: 现有慢性病机器学习风险评估模型依赖医学检测数据，且解释性未与医学文献验证，需开发可及、可解释的模型。

Method: 开发仅用个人和生活方式因素的深度学习模型预测13种慢性病风险，用SHAP解释特征并与医学文献验证。

Result: 模型最具影响力的特征与医学文献高度一致，且在13种疾病中都成立。

Conclusion: 此机器学习方法可广泛用于慢性病预测，为开发可信的自我预防护理工具奠定基础，未来可探索其他提升模型可信度的方法及伦理使用问题。

Abstract: Chronic diseases are long-term, manageable, yet typically incurable
conditions, highlighting the need for effective preventive strategies. Machine
learning has been widely used to assess individual risk for chronic diseases.
However, many models rely on medical test data (e.g. blood results, glucose
levels), which limits their utility for proactive self-assessment.
Additionally, to gain public trust, machine learning models should be
explainable and transparent. Although some research on self-assessment machine
learning models includes explainability, their explanations are not validated
against established medical literature, reducing confidence in their
reliability. To address these issues, we develop deep learning models that
predict the risk of developing 13 chronic diseases using only personal and
lifestyle factors, enabling accessible, self-directed preventive care.
Importantly, we use SHAP-based explainability to identify the most influential
model features and validate them against established medical literature. Our
results show a strong alignment between the models' most influential features
and established medical literature, reinforcing the models' trustworthiness.
Critically, we find that this observation holds across 13 distinct diseases,
indicating that this machine learning approach can be broadly trusted for
chronic disease prediction. This work lays the foundation for developing
trustworthy machine learning tools for self-directed preventive care. Future
research can explore other approaches for models' trustworthiness and discuss
how the models can be used ethically and responsibly.

</details>


### [188] [Exploiting Efficiency Vulnerabilities in Dynamic Deep Learning Systems](https://arxiv.org/abs/2506.17621)
*Ravishka Rathnasuriya,Wei Yang*

Main category: cs.LG

TL;DR: 本文研究动态深度学习系统（DDLSs）的安全隐患，提出评估效率攻击可行性并开发针对性防御措施。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在实际环境中部署，对低延迟和资源高效推理需求增加，DDLSs虽提高效率但存在安全风险，需研究其安全隐患。

Method: 调查现有攻击策略，找出新兴模型架构覆盖漏洞和现有防御机制的局限性。

Result: 揭示当前系统存在易被对抗性输入利用的效率漏洞。

Conclusion: 应评估现代DDLSs效率攻击的可行性，并开发针对性防御措施以保持鲁棒性。

Abstract: The growing deployment of deep learning models in real-world environments has
intensified the need for efficient inference under strict latency and resource
constraints. To meet these demands, dynamic deep learning systems (DDLSs) have
emerged, offering input-adaptive computation to optimize runtime efficiency.
While these systems succeed in reducing cost, their dynamic nature introduces
subtle and underexplored security risks. In particular, input-dependent
execution pathways create opportunities for adversaries to degrade efficiency,
resulting in excessive latency, energy usage, and potential denial-of-service
in time-sensitive deployments. This work investigates the security implications
of dynamic behaviors in DDLSs and reveals how current systems expose efficiency
vulnerabilities exploitable by adversarial inputs. Through a survey of existing
attack strategies, we identify gaps in the coverage of emerging model
architectures and limitations in current defense mechanisms. Building on these
insights, we propose to examine the feasibility of efficiency attacks on modern
DDLSs and develop targeted defenses to preserve robustness under adversarial
conditions.

</details>


### [189] [Reliability-Adjusted Prioritized Experience Replay](https://arxiv.org/abs/2506.18482)
*Leonard S. Pleiss,Tobias Sutter,Maximilian Schiffer*

Main category: cs.LG

TL;DR: 本文提出ReaPER算法，理论和实验表明其比PER更高效。


<details>
  <summary>Details</summary>
Motivation: 传统经验回放均匀采样效率低，PER虽有改进但仍可优化，旨在进一步提升采样效率。

Method: 提出衡量时间差分误差可靠性的新方法，扩展PER得到ReaPER算法。

Result: 理论证明ReaPER比PER学习更高效，实验表明ReaPER在多种环境类型中表现优于PER。

Conclusion: ReaPER是一种比PER更高效的经验回放算法。

Abstract: Experience replay enables data-efficient learning from past experiences in
online reinforcement learning agents. Traditionally, experiences were sampled
uniformly from a replay buffer, regardless of differences in
experience-specific learning potential. In an effort to sample more
efficiently, researchers introduced Prioritized Experience Replay (PER). In
this paper, we propose an extension to PER by introducing a novel measure of
temporal difference error reliability. We theoretically show that the resulting
transition selection algorithm, Reliability-adjusted Prioritized Experience
Replay (ReaPER), enables more efficient learning than PER. We further present
empirical results showing that ReaPER outperforms PER across various
environment types, including the Atari-5 benchmark.

</details>


### [190] [LLM-Prompt: Integrated Heterogeneous Prompts for Unlocking LLMs in Time Series Forecasting](https://arxiv.org/abs/2506.17631)
*Zesen Wang,Yonggang Li,Lijuan Lan*

Main category: cs.LG

TL;DR: 现有深度学习方法在时间序列预测中存在不足，提出LLM - Prompt框架，经多数据集验证有效。


<details>
  <summary>Details</summary>
Motivation: 深度学习方法在长期预测和数据稀缺场景表现不佳，现有基于大语言模型（LLMs）的方法存在文本提示表述缺乏统一范式、忽视模态差异的问题。

Method: 提出LLM - Prompt框架，构建含可学习软提示和文本化硬提示的统一文本提示范式，设计语义空间嵌入和跨模态对齐模块实现跨模态融合，对LLMs转换后的时间序列投影得到预测结果。

Result: 在6个公共数据集和3个碳排放数据集上进行综合评估。

Conclusion: LLM - Prompt是一个强大的时间序列预测框架。

Abstract: Time series forecasting aims to model temporal dependencies among variables
for future state inference, holding significant importance and widespread
applications in real-world scenarios. Although deep learning-based methods have
achieved remarkable progress, they still exhibit suboptimal performance in
long-term forecasting and data-scarce scenarios. Recent research demonstrates
that large language models (LLMs) achieve promising performance in time series
forecasting. However, we find existing LLM-based methods still have
shortcomings: (1) the absence of a unified paradigm for textual prompt
formulation and (2) the neglect of modality discrepancies between textual
prompts and time series. To address this, we propose LLM-Prompt, an LLM-based
time series forecasting framework integrating multi-prompt information and
cross-modal semantic alignment. Specifically, we first construct a unified
textual prompt paradigm containing learnable soft prompts and textualized hard
prompts. Second, to enhance LLMs' comprehensive understanding of the
forecasting task, we design a semantic space embedding and cross-modal
alignment module to achieve cross-modal fusion of temporal and textual
information. Finally, the transformed time series from the LLMs are projected
to obtain the forecasts. Comprehensive evaluations on 6 public datasets and 3
carbon emission datasets demonstrate that LLM-Prompt is a powerful framework
for time series forecasting.

</details>


### [191] [Optimization-Induced Dynamics of Lipschitz Continuity in Neural Networks](https://arxiv.org/abs/2506.18588)
*Róisín Luo,James McDermott,Christian Gagné,Qiang Sun,Colm O'Riordan*

Main category: cs.LG

TL;DR: 提出严格数学框架模拟随机梯度下降训练时Lipschitz连续性的时间演变，实验验证理论与观察一致。


<details>
  <summary>Details</summary>
Motivation: Lipschitz连续性在训练期间的动态变化研究不足，需深入探究。

Method: 利用随机微分方程系统建立框架，分析驱动演变的三个主要因素。

Result: 理论分析确定驱动演变的因素，实验结果显示理论与观察行为高度一致。

Conclusion: 所提理论框架能解释多种因素如何影响神经网络Lipschitz连续性的演变。

Abstract: Lipschitz continuity characterizes the worst-case sensitivity of neural
networks to small input perturbations; yet its dynamics (i.e. temporal
evolution) during training remains under-explored. We present a rigorous
mathematical framework to model the temporal evolution of Lipschitz continuity
during training with stochastic gradient descent (SGD). This framework
leverages a system of stochastic differential equations (SDEs) to capture both
deterministic and stochastic forces. Our theoretical analysis identifies three
principal factors driving the evolution: (i) the projection of gradient flows,
induced by the optimization dynamics, onto the operator-norm Jacobian of
parameter matrices; (ii) the projection of gradient noise, arising from the
randomness in mini-batch sampling, onto the operator-norm Jacobian; and (iii)
the projection of the gradient noise onto the operator-norm Hessian of
parameter matrices. Furthermore, our theoretical framework sheds light on such
as how noisy supervision, parameter initialization, batch size, and mini-batch
sampling trajectories, among other factors, shape the evolution of the
Lipschitz continuity of neural networks. Our experimental results demonstrate
strong agreement between the theoretical implications and the observed
behaviors.

</details>


### [192] [Online Multi-LLM Selection via Contextual Bandits under Unstructured Context Evolution](https://arxiv.org/abs/2506.17670)
*Manhin Poon,XiangXiang Dai,Xutong Liu,Fang Kong,John C. S. Lui,Jinhang Zuo*

Main category: cs.LG

TL;DR: 研究在线环境下自适应多LLM选择问题，提出上下文老虎机框架及相关算法，实验证明方法在准确性和成本效率上优于现有策略。


<details>
  <summary>Details</summary>
Motivation: LLM响应行为、成本和优势多样，在在线环境中难以选择合适LLM，且存在无结构上下文演变的挑战。

Method: 提出首个在无结构提示动态下进行顺序LLM选择的上下文老虎机框架，开发基于LinUCB的算法，还引入预算感知和位置感知扩展。

Result: 算法在多个基准测试中，在准确性和成本效率上优于现有LLM路由策略。

Conclusion: 上下文老虎机可用于实时、自适应LLM选择。

Abstract: Large language models (LLMs) exhibit diverse response behaviors, costs, and
strengths, making it challenging to select the most suitable LLM for a given
user query. We study the problem of adaptive multi-LLM selection in an online
setting, where the learner interacts with users through multi-step query
refinement and must choose LLMs sequentially without access to offline datasets
or model internals. A key challenge arises from unstructured context evolution:
the prompt dynamically changes in response to previous model outputs via a
black-box process, which cannot be simulated, modeled, or learned. To address
this, we propose the first contextual bandit framework for sequential LLM
selection under unstructured prompt dynamics. We formalize a notion of myopic
regret and develop a LinUCB-based algorithm that provably achieves sublinear
regret without relying on future context prediction. We further introduce
budget-aware and positionally-aware (favoring early-stage satisfaction)
extensions to accommodate variable query costs and user preferences for early
high-quality responses. Our algorithms are theoretically grounded and require
no offline fine-tuning or dataset-specific training. Experiments on diverse
benchmarks demonstrate that our methods outperform existing LLM routing
strategies in both accuracy and cost-efficiency, validating the power of
contextual bandits for real-time, adaptive LLM selection.

</details>


### [193] [On Equivariant Model Selection through the Lens of Uncertainty](https://arxiv.org/abs/2506.18629)
*Putri A. van der Linden,Alexander Timans,Dharmesh Tailor,Erik J. Bekkers*

Main category: cs.LG

TL;DR: 文章从不确定性感知角度研究对称模型选择任务，比较不同评估方法，发现不确定性指标与预测性能有一定关联，贝叶斯模型证据表现不稳定。


<details>
  <summary>Details</summary>
Motivation: 解决在具有不同对称偏差的预训练模型中进行选择的难题。

Method: 从不确定性感知角度，比较频率主义（通过共形预测）、贝叶斯（通过边际似然）、基于校准的度量与基于朴素误差的评估方法。

Result: 不确定性指标总体与预测性能一致，但贝叶斯模型证据表现不稳定，原因是贝叶斯和几何概念的模型复杂度不匹配。

Conclusion: 不确定性在指导对称感知模型选择方面有潜力。

Abstract: Equivariant models leverage prior knowledge on symmetries to improve
predictive performance, but misspecified architectural constraints can harm it
instead. While work has explored learning or relaxing constraints, selecting
among pretrained models with varying symmetry biases remains challenging. We
examine this model selection task from an uncertainty-aware perspective,
comparing frequentist (via Conformal Prediction), Bayesian (via the marginal
likelihood), and calibration-based measures to naive error-based evaluation. We
find that uncertainty metrics generally align with predictive performance, but
Bayesian model evidence does so inconsistently. We attribute this to a mismatch
in Bayesian and geometric notions of model complexity, and discuss possible
remedies. Our findings point towards the potential of uncertainty in guiding
symmetry-aware model selection.

</details>


### [194] [Learning Personalized Utility Functions for Drivers in Ride-hailing Systems Using Ensemble Hypernetworks](https://arxiv.org/abs/2506.17672)
*Weiming Mai,Jie Gao,Oded Cats*

Main category: cs.LG

TL;DR: 本文提出用超网络和集成学习学习个性化效用函数预测网约车司机接单决策，经真实数据集验证效果良好。


<details>
  <summary>Details</summary>
Motivation: 传统模型预测网约车司机接单决策时无法考虑属性间非线性关系和司机个性化偏好，为提升系统效率和可靠性，需更优预测方法。

Method: 使用超网络和集成学习学习个性化效用函数，超网络基于行程请求数据和司机资料动态生成线性效用函数权重，对不同数据段训练超网络集成以提高适应性和泛化能力。

Result: 在真实数据集上验证了模型在预测准确性和不确定性估计方面的性能，能准确预测司机效用，平衡可解释性和不确定性量化需求。

Conclusion: 该模型可有效预测司机接单决策，是揭示不同司机个性化偏好、明确影响接单决策属性的有力工具。

Abstract: In ride-hailing systems, drivers decide whether to accept or reject ride
requests based on factors such as order characteristics, traffic conditions,
and personal preferences. Accurately predicting these decisions is essential
for improving the efficiency and reliability of these systems. Traditional
models, such as the Random Utility Maximization (RUM) approach, typically
predict drivers' decisions by assuming linear correlations among attributes.
However, these models often fall short because they fail to account for
non-linear interactions between attributes and do not cater to the unique,
personalized preferences of individual drivers. In this paper, we develop a
method for learning personalized utility functions using hypernetwork and
ensemble learning. Hypernetworks dynamically generate weights for a linear
utility function based on trip request data and driver profiles, capturing the
non-linear relationships. An ensemble of hypernetworks trained on different
data segments further improve model adaptability and generalization by
introducing controlled randomness, thereby reducing over-fitting. We validate
the performance of our ensemble hypernetworks model in terms of prediction
accuracy and uncertainty estimation in a real-world dataset. The results
demonstrate that our approach not only accurately predicts each driver's
utility but also effectively balances the needs for explainability and
uncertainty quantification. Additionally, our model serves as a powerful tool
for revealing the personalized preferences of different drivers, clearly
illustrating which attributes largely impact their rider acceptance decisions.

</details>


### [195] [FaithfulSAE: Towards Capturing Faithful Features with Sparse Autoencoders without External Dataset Dependencies](https://arxiv.org/abs/2506.17673)
*Seonglae Cho,Harryn Oh,Donghyun Lee,Luis Eduardo Rodrigues Vieira,Andrew Bermingham,Ziad El Sayed*

Main category: cs.LG

TL;DR: 本文指出稀疏自编码器（SAEs）存在不稳定和无法捕捉模型内部特征的问题，提出FaithfulSAE方法，用模型自身合成数据集训练，结果显示更稳定且表现更好，消除对外部数据集依赖。


<details>
  <summary>Details</summary>
Motivation: 现有SAEs存在不稳定和无法捕捉模型内部特征的问题，可能源于使用外部数据集训练，会产生“虚假特征”。

Method: 提出FaithfulSAE方法，使用模型自身合成数据集训练SAEs。

Result: 使用FaithfulSAEs训练SAEs在种子间更稳定，在SAE探测任务中表现优于基于网络数据集训练的SAEs，7个模型中有5个虚假特征比例更低。

Conclusion: 该方法消除了对外部数据集的依赖，更好地捕捉模型内部特征，提升可解释性，凸显了SAE训练数据集的重要性。

Abstract: Sparse Autoencoders (SAEs) have emerged as a promising solution for
decomposing large language model representations into interpretable features.
However, Paulo and Belrose (2025) have highlighted instability across different
initialization seeds, and Heap et al. (2025) have pointed out that SAEs may not
capture model-internal features. These problems likely stem from training SAEs
on external datasets - either collected from the Web or generated by another
model - which may contain out-of-distribution (OOD) data beyond the model's
generalisation capabilities. This can result in hallucinated SAE features,
which we term "Fake Features", that misrepresent the model's internal
activations. To address these issues, we propose FaithfulSAE, a method that
trains SAEs on the model's own synthetic dataset. Using FaithfulSAEs, we
demonstrate that training SAEs on less-OOD instruction datasets results in SAEs
being more stable across seeds. Notably, FaithfulSAEs outperform SAEs trained
on web-based datasets in the SAE probing task and exhibit a lower Fake Feature
Ratio in 5 out of 7 models. Overall, our approach eliminates the dependency on
external datasets, advancing interpretability by better capturing
model-internal features while highlighting the often neglected importance of
SAE training datasets.

</details>


### [196] [Enhancing Stress-Strain Predictions with Seq2Seq and Cross-Attention based on Small Punch Test](https://arxiv.org/abs/2506.17680)
*Zhengni Yang,Rui Yang,Weijian Han,Qixin Liu*

Main category: cs.LG

TL;DR: 本文提出一种深度学习方法，利用小冲孔试验数据预测高强度钢真实应力 - 应变曲线，结果显示预测精度高，是材料科学中传统方法的有前景替代方案。


<details>
  <summary>Details</summary>
Motivation: 寻找新方法提高材料科学中真实应力 - 应变关系预测的准确性和效率。

Method: 使用Gramian Angular Field将载荷 - 位移序列转换为图像，采用基于LSTM的编码器 - 解码器架构的Sequence - to - Sequence模型，并通过多头交叉注意力机制增强。

Result: 所提方法实现了卓越的预测精度，平均绝对误差最小为0.15 MPa，最大为5.58 MPa。

Conclusion: 该方法是材料科学中传统实验技术的有前景替代方案，可提高预测的准确性和效率。

Abstract: This paper introduces a novel deep-learning approach to predict true
stress-strain curves of high-strength steels from small punch test (SPT)
load-displacement data. The proposed approach uses Gramian Angular Field (GAF)
to transform load-displacement sequences into images, capturing
spatial-temporal features and employs a Sequence-to-Sequence (Seq2Seq) model
with an LSTM-based encoder-decoder architecture, enhanced by multi-head
cross-attention to improved accuracy. Experimental results demonstrate that the
proposed approach achieves superior prediction accuracy, with minimum and
maximum mean absolute errors of 0.15 MPa and 5.58 MPa, respectively. The
proposed method offers a promising alternative to traditional experimental
techniques in materials science, enhancing the accuracy and efficiency of true
stress-strain relationship predictions.

</details>


### [197] [Physics-informed mixture of experts network for interpretable battery degradation trajectory computation amid second-life complexities](https://arxiv.org/abs/2506.17755)
*Xinghao Huang,Shengyu Tao,Chen Liang,Jiawei Chen,Junzhe Shi,Yuqi Li,Bizhong Xia,Guangmin Zhou,Xuan Zhang*

Main category: cs.LG

TL;DR: 本文提出PIMOE网络计算退役电动汽车电池退化轨迹，经实验验证效果好，为电池退化轨迹计算提供可部署、无历史依赖的解决方案。


<details>
  <summary>Details</summary>
Motivation: 退役电动汽车电池在支持低碳能源系统方面潜力大，但退化行为不确定和数据获取难阻碍其安全可扩展部署。

Method: 提出PIMOE网络，利用自适应多退化预测模块分类退化模式，生成潜在退化趋势嵌入，输入到与使用相关的循环网络进行长期轨迹预测。

Result: 在207个电池、77种使用条件和67902个循环上验证，平均绝对百分比误差0.88%，推理时间0.43毫秒，相比现有模型减少50%计算时间和误差；支持150个循环预测，即使训练数据缩减到5MB也能有效运行。

Conclusion: PIMOE框架为电池退化轨迹计算提供可部署、无历史依赖的解决方案，重新定义二次储能系统评估、优化和集成方式。

Abstract: Retired electric vehicle batteries offer immense potential to support
low-carbon energy systems, but uncertainties in their degradation behavior and
data inaccessibilities under second-life use pose major barriers to safe and
scalable deployment. This work proposes a Physics-Informed Mixture of Experts
(PIMOE) network that computes battery degradation trajectories using partial,
field-accessible signals in a single cycle. PIMOE leverages an adaptive
multi-degradation prediction module to classify degradation modes using expert
weight synthesis underpinned by capacity-voltage and relaxation data, producing
latent degradation trend embeddings. These are input to a use-dependent
recurrent network for long-term trajectory prediction. Validated on 207
batteries across 77 use conditions and 67,902 cycles, PIMOE achieves an average
mean absolute percentage (MAPE) errors of 0.88% with a 0.43 ms inference time.
Compared to the state-of-the-art Informer and PatchTST, it reduces
computational time and MAPE by 50%, respectively. Compatible with random state
of charge region sampling, PIMOE supports 150-cycle forecasts with 1.50%
average and 6.26% maximum MAPE, and operates effectively even with pruned 5MB
training data. Broadly, PIMOE framework offers a deployable, history-free
solution for battery degradation trajectory computation, redefining how
second-life energy storage systems are assessed, optimized, and integrated into
the sustainable energy landscape.

</details>


### [198] [Towards a Unified Textual Graph Framework for Spectral Reasoning via Physical and Chemical Information Fusion](https://arxiv.org/abs/2506.17761)
*Jiheng Liang,Ziru Yu,Zujie Xie,Yuchen Guo,Yulan Guo,Xiangyang Yu*

Main category: cs.LG

TL;DR: 针对现有光谱分析方法局限，提出结合先验知识图谱和大语言模型的多模态光谱分析框架，表现佳。


<details>
  <summary>Details</summary>
Motivation: 现有光谱分析方法依赖单模态数据、泛化性和可解释性差。

Method: 将原始光谱转换为TAGs，与先验知识合并形成任务图，用图神经网络处理以完成下游任务。

Result: 在多个光谱分析任务中表现优异，零样本和少样本设置下泛化性强。

Conclusion: 为大语言模型驱动的光谱分析建立可扩展和可解释的基础，统一物理和化学模态用于科学应用。

Abstract: Motivated by the limitations of current spectral analysis methods-such as
reliance on single-modality data, limited generalizability, and poor
interpretability-we propose a novel multi-modal spectral analysis framework
that integrates prior knowledge graphs with Large Language Models. Our method
explicitly bridges physical spectral measurements and chemical structural
semantics by representing them in a unified Textual Graph format, enabling
flexible, interpretable, and generalizable spectral understanding. Raw spectra
are first transformed into TAGs, where nodes and edges are enriched with
textual attributes describing both spectral properties and chemical context.
These are then merged with relevant prior knowledge-including functional groups
and molecular graphs-to form a Task Graph that incorporates "Prompt Nodes"
supporting LLM-based contextual reasoning. A Graph Neural Network further
processes this structure to complete downstream tasks. This unified design
enables seamless multi-modal integration and automated feature decoding with
minimal manual annotation. Our framework achieves consistently high performance
across multiple spectral analysis tasks, including node-level, edge-level, and
graph-level classification. It demonstrates robust generalization in both
zero-shot and few-shot settings, highlighting its effectiveness in learning
from limited data and supporting in-context reasoning. This work establishes a
scalable and interpretable foundation for LLM-driven spectral analysis,
unifying physical and chemical modalities for scientific applications.

</details>


### [199] [PhysiX: A Foundation Model for Physics Simulations](https://arxiv.org/abs/2506.17774)
*Tung Nguyen,Arsh Koneru,Shufan Li,Aditya grover*

Main category: cs.LG

TL;DR: 介绍首个物理模拟基础模型PhysiX，其解决数据瓶颈，性能超基线。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型在物理模拟领域进展不佳，数据稀缺、规模差异大等问题限制模型发展，需新模型解决。

Method: 构建45亿参数的自回归生成模型PhysiX，用离散分词器编码物理过程，采用自回归下一个标记预测目标建模，加入细化模块减少离散化误差。

Result: PhysiX有效解决数据瓶颈，在可比设置下超越特定任务基线，在The Well基准上超越先前最优方法。

Conclusion: 自然视频知识可成功迁移到物理模拟，跨不同模拟任务联合训练可实现协同学习。

Abstract: Foundation models have achieved remarkable success across video, image, and
language domains. By scaling up the number of parameters and training datasets,
these models acquire generalizable world knowledge and often surpass
task-specific approaches. However, such progress has yet to extend to the
domain of physics simulation. A primary bottleneck is data scarcity: while
millions of images, videos, and textual resources are readily available on the
internet, the largest physics simulation datasets contain only tens of
thousands of samples. This data limitation hinders the use of large models, as
overfitting becomes a major concern. As a result, physics applications
typically rely on small models, which struggle with long-range prediction due
to limited context understanding. Additionally, unlike images, videos, or
text-which typically exhibit fixed granularity-physics datasets often vary
drastically in scale, amplifying the challenges of scaling up multitask
training. We introduce PhysiX, the first large-scale foundation model for
physics simulation. PhysiX is a 4.5B parameter autoregressive generative model.
It uses a discrete tokenizer to encode physical processes at different scales
into a sequence of discrete tokens, and employs an autoregressive next-token
prediction objective to model such processes in the token space. To mitigate
the rounding error in the discretization process, PhysiX incorporates a
specialized refinement module. Through extensive experiments, we show that
PhysiX effectively addresses the data bottleneck, outperforming task-specific
baselines under comparable settings as well as the previous absolute
state-of-the-art approaches on The Well benchmark. Our results indicate that
knowledge learned from natural videos can be successfully transferred to
physics simulation, and that joint training across diverse simulation tasks
enables synergistic learning.

</details>


### [200] [Machine Learning Model Integration with Open World Temporal Logic for Process Automation](https://arxiv.org/abs/2506.17776)
*Dyuman Aditya,Colton Payne,Mario Leiva,Paulo Shakarian*

Main category: cs.LG

TL;DR: 本文介绍将机器学习模型输出与PyReason框架集成，以实现复杂流程自动化决策系统，可用于多领域。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习虽能从复杂数据源提取信息，但难以将输出转化为复杂操作流程中的可执行决策。

Method: 将多种机器学习模型输出与开放世界时态逻辑编程推理引擎PyReason框架集成，利用其逻辑框架处理实值输出，通过Python机制动态处理模型输出。

Result: 未明确提及具体实验或应用结果。

Conclusion: 结合机器学习的感知提取能力和PyReason的逻辑推理与透明度，可创建强大的复杂流程自动化系统，在多领域有应用价值。

Abstract: Recent advancements in Machine Learning (ML) have yielded powerful models
capable of extracting structured information from diverse and complex data
sources. However, a significant challenge lies in translating these perceptual
or extractive outputs into actionable, reasoned decisions within complex
operational workflows. To address these challenges, this paper introduces a
novel approach that integrates the outputs from various machine learning models
directly with the PyReason framework, an open-world temporal logic programming
reasoning engine. PyReason's foundation in generalized annotated logic allows
for the seamless incorporation of real-valued outputs (e.g., probabilities,
confidence scores) from diverse ML models, treating them as truth intervals
within its logical framework. Crucially, PyReason provides mechanisms,
implemented in Python, to continuously poll ML model outputs, convert them into
logical facts, and dynamically recompute the minimal model, ensuring real-tine
adaptive decision-making. Furthermore, its native support for temporal
reasoning, knowledge graph integration, and fully explainable interface traces
enables sophisticated analysis over time-sensitive process data and existing
organizational knowledge. By combining the strengths of perception and
extraction from ML models with the logical deduction and transparency of
PyReason, we aim to create a powerful system for automating complex processes.
This integration finds utility across numerous domains, including
manufacturing, healthcare, and business operations.

</details>


### [201] [Toward Autonomous UI Exploration: The UIExplorer Benchmark](https://arxiv.org/abs/2506.17779)
*Andrei Cristian Nica,Akshaya Vishnu Kudlu Shanbhogue,Harshil Shah,Aleix Cambray,Tudor Berariu,Lucas Maystre,David Barber*

Main category: cs.LG

TL;DR: 介绍首个专门用于UI探索的基准UIExplore - Bench，提出指标hUFO，评估不同模式下代理表现，结果显示UIExplore - AlGo表现领先，有性能差距，公开相关资源促研究。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对自主代理探索用户界面这一关键阶段的系统评估。

Method: 在标准化GitLab沙盒环境中，用Structured模式和Screen模式在三个级别评估代理，将探索形式化为最大化可操作UI组件集合的过程，提出hUFO指标。

Result: UIExplore - AlGo达到领先的平均hUFO分数，在Structured模式和Screen模式下分别达到人类性能的77.2%和59.0%，当前代理与人类专家有较大性能差距。

Conclusion: 该基准有相关性，公开资源可促进高效UI探索策略及下游应用的研究。

Abstract: Autonomous agents must know how to explore user interfaces (UIs) for reliable
task solving, yet systematic evaluation of this crucial phase is lacking. We
introduce UIExplore-Bench, the first benchmark explicitly dedicated to UI
exploration. The benchmark evaluates agents with either Structured mode
(granting access to layout information like DOM trees) or Screen mode (relying
on GUI-only observations such as screenshots and human-like mouse/keyboard
interactions) across three levels in a standardized GitLab sandbox environment.
We formalize exploration as the process of maximizing the set of actionable UI
components discovered and propose a metric, human-normalized UI-Functionalities
Observed (hUFO), to quantify the effectiveness of exploration. Our results show
that UIExplore-AlGo achieves the leading mean hUFO scores, reaching up to 77.2%
of human performance in Structured mode and 59.0% in Screen mode at 2,000
steps, particularly excelling at the Sparse level. The results highlight the
relevance of our benchmark, as current agents show a substantial performance
gap compared to one hour of human expert exploration, indicating ample room for
future advancements. We publicly release the benchmark environment, an
exploration dataset, and an evaluation suite to catalyze research into
efficient UI exploration strategies and their downstream applications, such as
experience-driven task completion and automated training data generation.

</details>


### [202] [Beyond instruction-conditioning, MoTE: Mixture of Task Experts for Multi-task Embedding Models](https://arxiv.org/abs/2506.17781)
*Miguel Romero,Shuoyang Ding,Corey D. Barret,Georgiana Dinu,George Karypis*

Main category: cs.LG

TL;DR: 本文分析指令调节应用于低容量模型的局限性，引入MoTE变压器块提升生成专业嵌入的能力，实验显示其有显著性能提升且无需改变指令等。


<details>
  <summary>Details</summary>
Motivation: 指令调节直接应用于低容量模型存在表示约束，限制了专业化带来的性能提升，需要解决该问题。

Method: 引入Mixture of Task Experts (MoTE)变压器块，利用任务感知对比学习训练任务专用参数。

Result: MoTE在检索数据集上性能提升64%，在所有数据集上性能提升43%，且不改变指令、训练数据、推理时间和活动参数数量。

Conclusion: MoTE能有效提升模型生成专业嵌入的能力，且具有不改变其他条件的优势。

Abstract: Dense embeddings are fundamental to modern machine learning systems, powering
Retrieval-Augmented Generation (RAG), information retrieval, and representation
learning. While instruction-conditioning has become the dominant approach for
embedding specialization, its direct application to low-capacity models imposes
fundamental representational constraints that limit the performance gains
derived from specialization. In this paper, we analyze these limitations and
introduce the Mixture of Task Experts (MoTE) transformer block, which leverages
task-specialized parameters trained with Task-Aware Contrastive Learning
(\tacl) to enhance the model ability to generate specialized embeddings.
Empirical results show that MoTE achieves $64\%$ higher performance gains in
retrieval datasets ($+3.27 \rightarrow +5.21$) and $43\%$ higher performance
gains across all datasets ($+1.81 \rightarrow +2.60$). Critically, these gains
are achieved without altering instructions, training data, inference time, or
number of active parameters.

</details>


### [203] [Reimagining Parameter Space Exploration with Diffusion Models](https://arxiv.org/abs/2506.17807)
*Lijun Zhang,Xiao Liu,Hui Guan*

Main category: cs.LG

TL;DR: 提出用扩散模型生成特定任务参数，评估其在不同场景表现，展现潜力与局限


<details>
  <summary>Details</summary>
Motivation: 传统神经网络适应新任务需特定微调，耗时且依赖标注数据，探索生成式替代方案

Method: 使用扩散模型学习有效特定任务参数空间的底层结构，按需合成参数

Result: 扩散模型能生成准确特定任务参数，在参数子空间结构良好时支持多任务插值，但无法泛化到未见任务

Conclusion: 此生成式解决方案有潜力，但存在局限

Abstract: Adapting neural networks to new tasks typically requires task-specific
fine-tuning, which is time-consuming and reliant on labeled data. We explore a
generative alternative that produces task-specific parameters directly from
task identity, eliminating the need for task-specific training. To this end, we
propose using diffusion models to learn the underlying structure of effective
task-specific parameter space and synthesize parameters on demand. Once
trained, the task-conditioned diffusion model can generate specialized weights
directly from task identifiers. We evaluate this approach across three
scenarios: generating parameters for a single seen task, for multiple seen
tasks, and for entirely unseen tasks. Experiments show that diffusion models
can generate accurate task-specific parameters and support multi-task
interpolation when parameter subspaces are well-structured, but fail to
generalize to unseen tasks, highlighting both the potential and limitations of
this generative solution.

</details>


### [204] [Actionable Interpretability via Causal Hypergraphs: Unravelling Batch Size Effects in Deep Learning](https://arxiv.org/abs/2506.17826)
*Zhongtian Sun,Anoushka Harit,Pietro Lio*

Main category: cs.LG

TL;DR: 本文提出基于超图的因果框架HGCNet探究批量大小对泛化能力的因果机制，实验表明HGCNet优于基线模型，小批量能提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 批量大小对泛化能力影响在图和文本领域的因果机制未被充分探索。

Method: 引入基于超图的因果框架HGCNet，利用深度结构因果模型和do - calculus量化批量大小干预的直接和间接影响。

Result: 在多个数据集上HGCNet优于GCN、GAT等强基线模型。

Conclusion: 小批量通过增加随机性和更平缓的极小值点因果性地增强泛化能力，可指导深度学习训练策略。

Abstract: While the impact of batch size on generalisation is well studied in vision
tasks, its causal mechanisms remain underexplored in graph and text domains. We
introduce a hypergraph-based causal framework, HGCNet, that leverages deep
structural causal models (DSCMs) to uncover how batch size influences
generalisation via gradient noise, minima sharpness, and model complexity.
Unlike prior approaches based on static pairwise dependencies, HGCNet employs
hypergraphs to capture higher-order interactions across training dynamics.
Using do-calculus, we quantify direct and mediated effects of batch size
interventions, providing interpretable, causally grounded insights into
optimisation. Experiments on citation networks, biomedical text, and e-commerce
reviews show that HGCNet outperforms strong baselines including GCN, GAT,
PI-GNN, BERT, and RoBERTa. Our analysis reveals that smaller batch sizes
causally enhance generalisation through increased stochasticity and flatter
minima, offering actionable interpretability to guide training strategies in
deep learning. This work positions interpretability as a driver of principled
architectural and optimisation choices beyond post hoc analysis.

</details>


### [205] [Aligning Frozen LLMs by Reinforcement Learning: An Iterative Reweight-then-Optimize Approach](https://arxiv.org/abs/2506.17828)
*Xinnan Zhang,Chenliang Li,Siliang Zeng,Jiaxiang Li,Zhongruo Wang,Kaixiang Lin,Songtao Lu,Alfredo Garcia,Mingyi Hong*

Main category: cs.LG

TL;DR: 提出迭代重加权再优化（IRO）方法，在不更新模型参数下进行大语言模型与人类偏好对齐，训练和测试有相应流程，且无需访问模型权重。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型与人类偏好对齐的微调方法无法在测试时使用，测试时方法有推理成本高和输出次优问题。

Method: 提出IRO方法，训练时迭代采样、重采样并训练轻量级价值函数，测试时用价值函数通过搜索优化过程引导模型生成。

Result: 未提及具体实验结果。

Conclusion: 用户可在自己数据集上使用IRO对齐模型，无需访问模型权重。

Abstract: Aligning large language models (LLMs) with human preferences usually requires
fine-tuning methods such as RLHF and DPO. These methods directly optimize the
model parameters, so they cannot be used in test-time to improve model
performance, nor are they applicable when the model weights are not accessible.
In contrast, test-time methods sidestep weight updates by leveraging reward
functions to guide and improve output quality. However, they incur high
inference costs, and their one-shot guidance is often based on imperfect reward
or value functions, leading to suboptimal outputs. In this work, we present a
method named Iterative Reweight-then-Optimize (IRO), a reinforcement learning
(RL) framework that performs RL-style alignment of the (frozen) base model
without touching its parameters. During training, each iteration (i) samples
candidates from the base model, (ii) resamples using current value functions,
and (iii) trains a new lightweight value function that guides the next decoding
pass. At test time, the value functions are used to guide the base model
generation via a search-based optimization process. Notably, users can apply
IRO to align a model on their own dataset, similar to OpenAI's reinforcement
fine-tuning (RFT), but without requiring access to the model weights.

</details>


### [206] [Causal Spherical Hypergraph Networks for Modelling Social Uncertainty](https://arxiv.org/abs/2506.17840)
*Anoushka Harit,Zhongtian Sun*

Main category: cs.LG

TL;DR: 提出Causal - SphHN框架用于社会预测，在多数据集实验中表现良好，贡献了统一因果几何方法。


<details>
  <summary>Details</summary>
Motivation: 人类社会行为受不确定性、因果关系和群体动态的复杂相互作用影响，需要一个能联合建模高阶结构、方向影响和认知不确定性的社会预测框架。

Method: 将个体表示为超球嵌入，群体上下文表示为超边，通过香农熵量化不确定性，用格兰杰因果子图识别时间因果依赖，通过角度消息传递机制传播信息。

Result: 在SNARE、PHEME和AMIGOS数据集上，Causal - SphHN比强基线模型提高了预测准确性、鲁棒性和校准度，还能对影响模式和社会模糊性进行可解释分析。

Conclusion: 该工作为动态社会环境中的不确定性学习提供了统一的因果几何方法。

Abstract: Human social behaviour is governed by complex interactions shaped by
uncertainty, causality, and group dynamics. We propose Causal Spherical
Hypergraph Networks (Causal-SphHN), a principled framework for socially
grounded prediction that jointly models higher-order structure, directional
influence, and epistemic uncertainty. Our method represents individuals as
hyperspherical embeddings and group contexts as hyperedges, capturing semantic
and relational geometry. Uncertainty is quantified via Shannon entropy over von
Mises-Fisher distributions, while temporal causal dependencies are identified
using Granger-informed subgraphs. Information is propagated through an angular
message-passing mechanism that respects belief dispersion and directional
semantics. Experiments on SNARE (offline networks), PHEME (online discourse),
and AMIGOS (multimodal affect) show that Causal-SphHN improves predictive
accuracy, robustness, and calibration over strong baselines. Moreover, it
enables interpretable analysis of influence patterns and social ambiguity. This
work contributes a unified causal-geometric approach for learning under
uncertainty in dynamic social environments.

</details>


### [207] [A Comparative Study of Open-Source Libraries for Synthetic Tabular Data Generation: SDV vs. SynthCity](https://arxiv.org/abs/2506.17847)
*Cristian Del Gobbo*

Main category: cs.LG

TL;DR: 本文评估了两种开源库中的六种表格合成数据生成器，在低数据情况下按不同比例生成合成数据，用统计相似性和预测效用评估，发现贝叶斯网络保真度高，SDV文档和易用性更佳。


<details>
  <summary>Details</summary>
Motivation: 获取高质量真实数据有挑战，合成数据生成器是有前景的解决方案，需评估其性能。

Method: 使用UCI机器学习库的真实数据集模拟低数据情况，让生成器按1:1和1:10比例生成合成数据，用统计指标和“训练合成，测试真实”方法评估。

Result: 统计相似性在两种场景下各模型较一致，1:10时预测效用下降；Synthicity的贝叶斯网络保真度最高，SDV的TVAE在1:10预测任务中表现最佳。

Conclusion: 两库性能无显著差距，SDV文档和易用性更好，对从业者更易上手。

Abstract: High-quality training data is critical to the performance of machine learning
models, particularly Large Language Models (LLMs). However, obtaining real,
high-quality data can be challenging, especially for smaller organizations and
early-stage startups. Synthetic data generators provide a promising solution by
replicating the statistical and structural properties of real data while
preserving privacy and scalability. This study evaluates the performance of six
tabular synthetic data generators from two widely used open-source libraries:
SDV (Gaussian Copula, CTGAN, TVAE) and Synthicity (Bayesian Network, CTGAN,
TVAE). Using a real-world dataset from the UCI Machine Learning Repository,
comprising energy consumption and environmental variables from Belgium, we
simulate a low-data regime by training models on only 1,000 rows. Each
generator is then tasked with producing synthetic datasets under two
conditions: a 1:1 (1,000 rows) and a 1:10 (10,000 rows) input-output ratio.
Evaluation is conducted using two criteria: statistical similarity, measured
via classical statistics and distributional metrics; and predictive utility,
assessed using a "Train on Synthetic, Test on Real" approach with four
regression models. While statistical similarity remained consistent across
models in both scenarios, predictive utility declined notably in the 1:10 case.
The Bayesian Network from Synthicity achieved the highest fidelity in both
scenarios, while TVAE from SDV performed best in predictive tasks under the
1:10 setting. Although no significant performance gap was found between the two
libraries, SDV stands out for its superior documentation and ease of use,
making it more accessible for practitioners.

</details>


### [208] [Pathway-based Progressive Inference (PaPI) for Energy-Efficient Continual Learning](https://arxiv.org/abs/2506.17848)
*Suyash Gaurav,Jukka Heikkonen,Jatin Chaudhary*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Continual learning systems face the dual challenge of preventing catastrophic
forgetting while maintaining energy efficiency, particularly in
resource-constrained environments. This paper introduces Pathway-based
Progressive Inference (PaPI), a novel theoretical framework that addresses
these challenges through a mathematically rigorous approach to pathway
selection and adaptation. We formulate continual learning as an
energy-constrained optimization problem and provide formal convergence
guarantees for our pathway routing mechanisms. Our theoretical analysis
demonstrates that PaPI achieves an $\mathcal{O}(K)$ improvement in the
stability-plasticity trade-off compared to monolithic architectures, where $K$
is the number of pathways. We derive tight bounds on forgetting rates using
Fisher Information Matrix analysis and prove that PaPI's energy consumption
scales with the number of active parameters rather than the total model size.
Comparative theoretical analysis shows that PaPI provides stronger guarantees
against catastrophic forgetting than Elastic Weight Consolidation (EWC) while
maintaining better energy efficiency than both EWC and Gradient Episodic Memory
(GEM). Our experimental validation confirms these theoretical advantages across
multiple benchmarks, demonstrating PaPI's effectiveness for continual learning
in energy-constrained settings. Our codes are available at
https://github.com/zser092/PAPI_FILES.

</details>


### [209] [In-Context Learning Strategies Emerge Rationally](https://arxiv.org/abs/2506.17859)
*Daniel Wurgaft,Ekdeep Singh Lubana,Core Francisco Park,Hidenori Tanaka,Gautam Reddy,Noah D. Goodman*

Main category: cs.LG

TL;DR: 本文提出分层贝叶斯框架统一上下文学习（ICL）策略研究，从策略损失和复杂度权衡角度解释和预测ICL现象。


<details>
  <summary>Details</summary>
Motivation: 统一现有关于ICL模型在不同实验条件下学习不同策略的研究成果，探究模型学习这些不同策略的原因。

Method: 采用认知科学的理性分析视角，构建分层贝叶斯框架，将预训练视为更新不同策略后验概率的过程，推理时行为视为策略预测的后验加权平均。

Result: 该框架几乎能完美预测Transformer在训练过程中的下一个标记预测，解释了著名的ICL现象，并给出新预测，如任务多样性增加时向记忆化过渡的时间尺度呈超线性趋势。

Conclusion: 研究从策略损失和复杂度的权衡出发，推动了对ICL的解释性和预测性研究。

Abstract: Recent work analyzing in-context learning (ICL) has identified a broad set of
strategies that describe model behavior in different experimental conditions.
We aim to unify these findings by asking why a model learns these disparate
strategies in the first place. Specifically, we start with the observation that
when trained to learn a mixture of tasks, as is popular in the literature, the
strategies learned by a model for performing ICL can be captured by a family of
Bayesian predictors: a memorizing predictor, which assumes a discrete prior on
the set of seen tasks, and a generalizing predictor, wherein the prior matches
the underlying task distribution. Adopting the lens of rational analysis from
cognitive science, where a learner's behavior is explained as an optimal
adaptation to data given computational constraints, we develop a hierarchical
Bayesian framework that almost perfectly predicts Transformer next token
predictions throughout training without assuming access to its weights. Under
this framework, pretraining is viewed as a process of updating the posterior
probability of different strategies, and its inference-time behavior as a
posterior-weighted average over these strategies' predictions. Our framework
draws on common assumptions about neural network learning dynamics, which make
explicit a tradeoff between loss and complexity among candidate strategies:
beyond how well it explains the data, a model's preference towards implementing
a strategy is dictated by its complexity. This helps explain well-known ICL
phenomena, while offering novel predictions: e.g., we show a superlinear trend
in the timescale for transition to memorization as task diversity is increased.
Overall, our work advances an explanatory and predictive account of ICL
grounded in tradeoffs between strategy loss and complexity.

</details>


### [210] [Decoding Federated Learning: The FedNAM+ Conformal Revolution](https://arxiv.org/abs/2506.17872)
*Sree Bhargavi Balija,Amitash Nanda,Debashis Sahoo*

Main category: cs.LG

TL;DR: 提出FedNAM+联邦学习框架，集成NAMs与共形预测方法，实现可解释和可靠的不确定性估计，经实验验证有高准确性、低损失，且计算高效。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习框架缺乏结合不确定性量化、可解释性和鲁棒性的综合解决方案。

Method: 提出FedNAM+框架，集成NAMs与共形预测方法，引入动态水平调整技术，利用基于梯度的敏感性图识别关键输入特征。

Result: 在CT扫描、MNIST和CIFAR数据集实验中，预测准确性高、损失小，能提供可视化的不确定性度量，相比Monte Carlo Dropout计算开销低。

Conclusion: FedNAM+是一个健壮、可解释且计算高效的框架，增强了分散式预测建模的信任和透明度。

Abstract: Federated learning has significantly advanced distributed training of machine
learning models across decentralized data sources. However, existing frameworks
often lack comprehensive solutions that combine uncertainty quantification,
interpretability, and robustness. To address this, we propose FedNAM+, a
federated learning framework that integrates Neural Additive Models (NAMs) with
a novel conformal prediction method to enable interpretable and reliable
uncertainty estimation. Our method introduces a dynamic level adjustment
technique that utilizes gradient-based sensitivity maps to identify key input
features influencing predictions. This facilitates both interpretability and
pixel-wise uncertainty estimates. Unlike traditional interpretability methods
such as LIME and SHAP, which do not provide confidence intervals, FedNAM+
offers visual insights into prediction reliability. We validate our approach
through experiments on CT scan, MNIST, and CIFAR datasets, demonstrating high
prediction accuracy with minimal loss (e.g., only 0.1% on MNIST), along with
transparent uncertainty measures. Visual analysis highlights variable
uncertainty intervals, revealing low-confidence regions where model performance
can be improved with additional data. Compared to Monte Carlo Dropout, FedNAM+
delivers efficient and global uncertainty estimates with reduced computational
overhead, making it particularly suitable for federated learning scenarios.
Overall, FedNAM+ provides a robust, interpretable, and computationally
efficient framework that enhances trust and transparency in decentralized
predictive modeling.

</details>


### [211] [TROJAN-GUARD: Hardware Trojans Detection Using GNN in RTL Designs](https://arxiv.org/abs/2506.17894)
*Kiran Thorat,Amit Hasan,Caiwen Ding,Zhijie Shi*

Main category: cs.LG

TL;DR: 针对现有基于图神经网络的硬件木马检测方法在大型设计中表现不佳的问题，提出了一种新框架，该框架可生成大型设计的图嵌入，采用多种GNN模型并引入模型量化技术，实验证明其在检测大型芯片设计中的硬件木马方面有效且高效。


<details>
  <summary>Details</summary>
Motivation: 现有基于GNN的硬件木马检测方法在大型设计上表现差，且未探索适合的GNN模型和高效训练推理流程，而硬件木马会带来严重威胁。

Method: 提出可生成大型设计图嵌入的框架，采用多种适合硬件木马检测的GNN模型，引入模型量化技术进行高效训练和推理。

Result: 使用自定义数据集评估，精确率达98.66%，召回率达92.30%。

Conclusion: 所提框架在检测大型芯片设计中的硬件木马方面有效且高效。

Abstract: Chip manufacturing is a complex process, and to achieve a faster time to
market, an increasing number of untrusted third-party tools and designs from
around the world are being utilized. The use of these untrusted third party
intellectual properties (IPs) and tools increases the risk of adversaries
inserting hardware trojans (HTs). The covert nature of HTs poses significant
threats to cyberspace, potentially leading to severe consequences for national
security, the economy, and personal privacy. Many graph neural network
(GNN)-based HT detection methods have been proposed. However, they perform
poorly on larger designs because they rely on training with smaller designs.
Additionally, these methods do not explore different GNN models that are
well-suited for HT detection or provide efficient training and inference
processes. We propose a novel framework that generates graph embeddings for
large designs (e.g., RISC-V) and incorporates various GNN models tailored for
HT detection. Furthermore, our framework introduces domain-specific techniques
for efficient training and inference by implementing model quantization. Model
quantization reduces the precision of the weights, lowering the computational
requirements, enhancing processing speed without significantly affecting
detection accuracy. We evaluate our framework using a custom dataset, and our
results demonstrate a precision of 98.66% and a recall (true positive rate) of
92.30%, highlighting the effectiveness and efficiency of our approach in
detecting hardware trojans in large-scale chip designs

</details>


### [212] [Permutation Equivariant Model-based Offline Reinforcement Learning for Auto-bidding](https://arxiv.org/abs/2506.17919)
*Zhiyu Mou,Miao Xu,Wei Chen,Rongquan Bai,Chuan Yu,Jian Xu*

Main category: cs.LG

TL;DR: 本文提出基于模型的强化学习竞价（MRLB）及PE - MORL算法，实验表明其优于现有自动竞价方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于离线固定数据集的强化学习竞价（ORLB）受数据集状态空间覆盖限制，基于简单离线模拟器的强化学习竞价（SRLB）存在模拟器与现实的差距问题，需要改进。

Method: 引入MRLB，通过真实数据学习环境模型；提出置换等变模型架构和鲁棒离线Q学习方法，形成PE - MORL算法。

Result: 现实世界实验显示PE - MORL优于当前最先进的自动竞价方法。

Conclusion: PE - MORL算法在自动竞价中表现出色，能有效解决现有方法的问题。

Abstract: Reinforcement learning (RL) for auto-bidding has shifted from using
simplistic offline simulators (Simulation-based RL Bidding, SRLB) to offline RL
on fixed real datasets (Offline RL Bidding, ORLB). However, ORLB policies are
limited by the dataset's state space coverage, offering modest gains. While
SRLB expands state coverage, its simulator-reality gap risks misleading
policies. This paper introduces Model-based RL Bidding (MRLB), which learns an
environment model from real data to bridge this gap. MRLB trains policies using
both real and model-generated data, expanding state coverage beyond ORLB. To
ensure model reliability, we propose: 1) A permutation equivariant model
architecture for better generalization, and 2) A robust offline Q-learning
method that pessimistically penalizes model errors. These form the Permutation
Equivariant Model-based Offline RL (PE-MORL) algorithm. Real-world experiments
show that PE-MORL outperforms state-of-the-art auto-bidding methods.

</details>


### [213] [ASTER: Adaptive Spatio-Temporal Early Decision Model for Dynamic Resource Allocation](https://arxiv.org/abs/2506.17929)
*Shulun Chen,Wei Shao,Flora D. Salim,Hao Xue*

Main category: cs.LG

TL;DR: 提出ASTER模型，将时空预测范式从事件预测转变为可操作决策支持，在四个基准数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有工作将预测和下游决策阶段分离，降低了下游效率，需要将时空预测转化为可操作策略。

Method: 提出ASTER模型，引入RaST模块捕获动态资源条件下的时空依赖，设计基于多目标强化学习的Poda决策代理生成可操作决策。

Result: 在四个基准数据集上，ASTER在六个下游指标上提高了早期预测准确性和资源分配结果，达到了最先进的性能。

Conclusion: ASTER模型有效解决了时空预测到可操作决策转化的问题，提高了整体效率。

Abstract: Supporting decision-making has long been a central vision in the field of
spatio-temporal intelligence. While prior work has improved the timeliness and
accuracy of spatio-temporal forecasting, converting these forecasts into
actionable strategies remains a key challenge. A main limitation is the
decoupling of the prediction and the downstream decision phases, which can
significantly degrade the downstream efficiency. For example, in emergency
response, the priority is successful resource allocation and intervention, not
just incident prediction. To this end, it is essential to propose an Adaptive
Spatio-Temporal Early Decision model (ASTER) that reforms the forecasting
paradigm from event anticipation to actionable decision support. This framework
ensures that information is directly used for decision-making, thereby
maximizing overall effectiveness. Specifically, ASTER introduces a new
Resource-aware Spatio-Temporal interaction module (RaST) that adaptively
captures long- and short-term dependencies under dynamic resource conditions,
producing context-aware spatiotemporal representations. To directly generate
actionable decisions, we further design a Preference-oriented decision agent
(Poda) based on multi-objective reinforcement learning, which transforms
predictive signals into resource-efficient intervention strategies by deriving
optimal actions under specific preferences and dynamic constraints.
Experimental results on four benchmark datasets demonstrate the
state-of-the-art performance of ASTER in improving both early prediction
accuracy and resource allocation outcomes across six downstream metrics.

</details>


### [214] [Adapting Vision-Language Models for Evaluating World Models](https://arxiv.org/abs/2506.17967)
*Mariya Hendriksen,Tabish Rashid,David Bignell,Raluca Georgescu,Abdelhak Lemkhenter,Katja Hofmann,Sam Devlin,Sarah Parisot*

Main category: cs.LG

TL;DR: 本文引入评估协议并提出UNIVERSE方法评估世界模型的推演，实验表明其是可扩展、语义感知的评估器。


<details>
  <summary>Details</summary>
Motivation: 现有指标无法对世界模型的推演进行细粒度、时间关联的评估，视觉语言模型在相关评估任务中的应用有限且需针对性调整。

Method: 引入针对动作识别和角色识别的评估协议，提出UNIVERSE方法，在多种条件下比较全量、部分和参数高效微调。

Result: 统一评估器用单个检查点达到特定任务基线的性能，人类研究证实与人类判断高度一致。

Conclusion: UNIVERSE是世界模型可扩展、语义感知的评估器。

Abstract: World models -- generative models that simulate environment dynamics
conditioned on past observations and actions -- are gaining prominence in
planning, simulation, and embodied AI. However, evaluating their rollouts
remains a fundamental challenge, requiring fine-grained, temporally grounded
assessment of action alignment and semantic consistency -- capabilities not
captured by existing metrics. Vision-Language Models (VLMs) have shown promise
as automatic evaluators of generative content due to their strong multimodal
reasoning abilities. Yet, their use in fine-grained, temporally sensitive
evaluation tasks remains limited and requires targeted adaptation. We introduce
a evaluation protocol targeting two recognition tasks -- action recognition and
character recognition -- each assessed across binary, multiple-choice, and
open-ended formats. To support this, we present UNIVERSE (UNIfied
Vision-language Evaluator for Rollouts in Simulated Environments), a method for
adapting VLMs to rollout evaluation under data and compute constraints. We
conduct a large-scale study comparing full, partial, and parameter-efficient
finetuning across task formats, context lengths, sampling strategies, and data
compositions. The resulting unified evaluator matches the performance of
task-specific baselines using a single checkpoint. Human studies confirm strong
alignment with human judgments, establishing UNIVERSE as a scalable,
semantics-aware evaluator for world models.

</details>


### [215] [Trustworthy Efficient Communication for Distributed Learning using LQ-SGD Algorithm](https://arxiv.org/abs/2506.17974)
*Hongyang Li,Lincen Bai,Caesar Wu,Mohammed Chadli,Said Mammar,Pascal Bouvry*

Main category: cs.LG

TL;DR: 提出用于分布式训练的高效通信梯度压缩算法LQ - SGD，结合低秩近似和对数量化技术，减少通信开销，保证训练收敛速度和模型精度，且比传统SGD更抗梯度反转。


<details>
  <summary>Details</summary>
Motivation: 解决分布式训练中的通信开销问题，同时提升训练系统的效率和安全性。

Method: 在PowerSGD基础上，结合低秩近似和对数量化技术开发LQ - SGD算法。

Result: 大幅减少通信开销，保证训练收敛速度和模型精度，比传统SGD更抗梯度反转。

Conclusion: LQ - SGD为分布式学习系统提供了更稳健高效的优化路径。

Abstract: We propose LQ-SGD (Low-Rank Quantized Stochastic Gradient Descent), an
efficient communication gradient compression algorithm designed for distributed
training. LQ-SGD further develops on the basis of PowerSGD by incorporating the
low-rank approximation and log-quantization techniques, which drastically
reduce the communication overhead, while still ensuring the convergence speed
of training and model accuracy. In addition, LQ-SGD and other compression-based
methods show stronger resistance to gradient inversion than traditional SGD,
providing a more robust and efficient optimization path for distributed
learning systems.

</details>


### [216] [Data Curation Matters: Model Collapse and Spurious Shift Performance Prediction from Training on Uncurated Text Embeddings](https://arxiv.org/abs/2506.17989)
*Lucas Mattioli,Youness Ait Hadichou,Sabrina Chaouche,Martin Gonzalez*

Main category: cs.LG

TL;DR: 训练原始表格数据文本嵌入（TE）模型会导致模型崩溃，引入指标衡量崩溃程度，发现TE不能有效作为数据整理层，强调需对基于嵌入的表示进行细致整理和评估。


<details>
  <summary>Details</summary>
Motivation: 解决训练原始表格数据文本嵌入模型时出现的模型崩溃问题，评估TE质量。

Method: 对比相同超参数配置下原始表格数据和TE派生数据训练的模型，引入衡量模型崩溃程度的指标。

Result: TE不能有效作为整理层，其质量影响下游学习，模型崩溃会产生虚假的准确性相关性。

Conclusion: 需要对基于嵌入的表示进行更细致的整理和评估，特别是在分布外设置中。

Abstract: Training models on uncurated Text Embeddings (TEs) derived from raw tabular
data can lead to a severe failure mode known as model collapse, where
predictions converge to a single class regardless of input. By comparing models
trained with identical hyper-parameter configurations on both raw tabular data
and their TE-derived counterparts, we find that collapse is a consistent
failure mode in the latter setting. We introduce a set of metrics that capture
the extent of model collapse, offering a new perspective on TE quality as a
proxy for data curation. Our results reveal that TE alone does not effectively
function as a curation layer - and that their quality significantly influences
downstream learning. More insidiously, we observe that the presence of model
collapse can yield artificially inflated and spurious Accuracy-on-the-Line
correlation. These findings highlight the need for more nuanced curation and
evaluation of embedding-based representations, particularly in
out-of-distribution settings.

</details>


### [217] [Why Do Some Language Models Fake Alignment While Others Don't?](https://arxiv.org/abs/2506.18032)
*Abhay Sheshadri,John Hughes,Julian Michael,Alex Mallen,Arun Jose,Janus,Fabien Roger*

Main category: cs.LG

TL;DR: 分析25个大语言模型的对齐伪装情况，发现5个模型在训练时比部署时更易响应有害查询，研究其动机及多数模型不伪装原因，认为拒绝行为差异影响对齐伪装。


<details>
  <summary>Details</summary>
Motivation: 研究多个大语言模型对齐伪装情况，分析部分模型在训练和部署时响应有害查询差异的动机，以及多数模型不进行对齐伪装的原因。

Method: 对25个模型进行分析，研究5个模型的动机，通过扰动场景细节分析；探究多个聊天模型不伪装原因，提出5个关于训练后抑制对齐伪装的假设。

Result: 仅5个模型在训练时比部署时更易响应有害查询；Claude 3 Opus的合规差距主要是为保持目标；许多基础模型有时会伪装对齐，训练后对不同模型影响不同；拒绝行为差异可能是对齐伪装差异的重要原因。

Conclusion: 不同模型在对齐伪装上表现不同，训练过程和拒绝行为差异对对齐伪装情况有显著影响。

Abstract: Alignment faking in large language models presented a demonstration of Claude
3 Opus and Claude 3.5 Sonnet selectively complying with a helpful-only training
objective to prevent modification of their behavior outside of training. We
expand this analysis to 25 models and find that only 5 (Claude 3 Opus, Claude
3.5 Sonnet, Llama 3 405B, Grok 3, Gemini 2.0 Flash) comply with harmful queries
more when they infer they are in training than when they infer they are in
deployment. First, we study the motivations of these 5 models. Results from
perturbing details of the scenario suggest that only Claude 3 Opus's compliance
gap is primarily and consistently motivated by trying to keep its goals.
Second, we investigate why many chat models don't fake alignment. Our results
suggest this is not entirely due to a lack of capabilities: many base models
fake alignment some of the time, and post-training eliminates alignment-faking
for some models and amplifies it for others. We investigate 5 hypotheses for
how post-training may suppress alignment faking and find that variations in
refusal behavior may account for a significant portion of differences in
alignment faking.

</details>


### [218] [Pathwise Explanation of ReLU Neural Networks](https://arxiv.org/abs/2506.18037)
*Seongwoo Lim,Won Jo,Joohyung Lee,Jaesik Choi*

Main category: cs.LG

TL;DR: 本文提出新方法解释ReLU网络，考虑决策路径隐藏单元子集，实验显示该方法优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 神经网络的“黑盒”性质引发对透明度和可靠性的担忧，前人基于所有隐藏单元激活状态将ReLU网络转化为线性模型，本文寻求新解释方法。

Method: 引入考虑决策路径中隐藏单元子集的新方法，提供路径解释，可调整解释范围、分解输入解释。

Result: 实验表明该方法在定量和定性上均优于其他方法。

Conclusion: 所提方法能更清晰一致地理解输入与决策过程关系，有解释范围调整和分解解释的灵活性，性能出色。

Abstract: Neural networks have demonstrated a wide range of successes, but their
``black box" nature raises concerns about transparency and reliability.
Previous research on ReLU networks has sought to unwrap these networks into
linear models based on activation states of all hidden units. In this paper, we
introduce a novel approach that considers subsets of the hidden units involved
in the decision making path. This pathwise explanation provides a clearer and
more consistent understanding of the relationship between the input and the
decision-making process. Our method also offers flexibility in adjusting the
range of explanations within the input, i.e., from an overall attribution input
to particular components within the input. Furthermore, it allows for the
decomposition of explanations for a given input for more detailed explanations.
Experiments demonstrate that our method outperforms others both quantitatively
and qualitatively.

</details>


### [219] [TAB: Unified Benchmarking of Time Series Anomaly Detection Methods](https://arxiv.org/abs/2506.18046)
*Xiangfei Qiu,Zhe Li,Wanghui Qiu,Shiyan Hu,Lekui Zhou,Xingjian Wu,Zhengyu Li,Chenjuan Guo,Aoying Zhou,Zhenli Sheng,Jilin Hu,Christian S. Jensen,Bin Yang*

Main category: cs.LG

TL;DR: 本文提出新的时间序列异常检测基准TAB，包含多领域数据集，涵盖多种方法，有统一自动评估流程，用其评估现有方法并公开数据代码。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列异常检测（TSAD）评估程序在数据集、实验设置和协议方面存在不足，需要可靠方法评估新方法并与现有方法比较。

Method: 提出新的时间序列异常检测基准TAB，包含多领域数据集，覆盖多种TSAD方法，采用统一自动评估流程。

Result: 使用TAB评估现有TSAD方法并报告结果，更深入了解这些方法的性能。

Conclusion: TAB能促进对TSAD方法的评估，所有数据和代码公开可获取。

Abstract: Time series anomaly detection (TSAD) plays an important role in many domains
such as finance, transportation, and healthcare. With the ongoing
instrumentation of reality, more time series data will be available, leading
also to growing demands for TSAD. While many TSAD methods already exist, new
and better methods are still desirable. However, effective progress hinges on
the availability of reliable means of evaluating new methods and comparing them
with existing methods. We address deficiencies in current evaluation procedures
related to datasets and experimental settings and protocols. Specifically, we
propose a new time series anomaly detection benchmark, called TAB. First, TAB
encompasses 29 public multivariate datasets and 1,635 univariate time series
from different domains to facilitate more comprehensive evaluations on diverse
datasets. Second, TAB covers a variety of TSAD methods, including Non-learning,
Machine learning, Deep learning, LLM-based, and Time-series pre-trained
methods. Third, TAB features a unified and automated evaluation pipeline that
enables fair and easy evaluation of TSAD methods. Finally, we employ TAB to
evaluate existing TSAD methods and report on the outcomes, thereby offering a
deeper insight into the performance of these methods. Besides, all datasets and
code are available at https://github.com/decisionintelligence/TAB.

</details>


### [220] [Distributionally robust minimization in meta-learning for system identification](https://arxiv.org/abs/2506.18074)
*Matteo Rufolo,Dario Piga,Marco Forgione*

Main category: cs.LG

TL;DR: 本文探索元学习中用于系统识别的分布鲁棒最小化，采用新方法提升最坏情况性能并减少安全关键应用失败。


<details>
  <summary>Details</summary>
Motivation: 标准元学习方法优化期望损失，忽略任务可变性，本文旨在解决此问题。

Method: 采用分布鲁棒优化范式，优先处理高损失任务。

Result: 在合成动态系统元模型上评估，该方法能减少安全关键应用中的失败。

Conclusion: 分布鲁棒优化范式可提升元学习在系统识别中的性能，尤其在最坏情况下。

Abstract: Meta learning aims at learning how to solve tasks, and thus it allows to
estimate models that can be quickly adapted to new scenarios. This work
explores distributionally robust minimization in meta learning for system
identification. Standard meta learning approaches optimize the expected loss,
overlooking task variability. We use an alternative approach, adopting a
distributionally robust optimization paradigm that prioritizes high-loss tasks,
enhancing performance in worst-case scenarios. Evaluated on a meta model
trained on a class of synthetic dynamical systems and tested in both
in-distribution and out-of-distribution settings, the proposed approach allows
to reduce failures in safety-critical applications.

</details>


### [221] [RL for Reasoning by Adaptively Revealing Rationales](https://arxiv.org/abs/2506.18110)
*Mohammad Hossein Amani,Aryo Lotfi,Nicolas Mario Baldwin,Samy Bengio,Mehrdad Farajtabar,Emmanuel Abbe,Robert West*

Main category: cs.LG

TL;DR: 提出从部分专家演示进行强化学习是解决复杂序列生成任务的有前景框架，引入自适应回溯算法，在合成任务和数学推理基准测试中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 监督微调依赖密集真实标签，成本随序列长度增加；强化学习存在稀疏奖励和大输出空间问题，需新方法解决复杂序列生成任务。

Method: 引入自适应回溯（AdaBack），一种逐样本课程学习算法，训练时仅揭示目标输出的部分前缀，根据模型过去奖励信号动态调整监督长度。

Result: 在合成任务中，自适应课程学习能解决原本难处理的问题；在数学推理基准测试中，课程学习使模型获得新推理能力。

Conclusion: 逐样本课程学习不只是效率和通用性的权衡，能在监督微调与强化学习都无法泛化的任务中取得成功。

Abstract: We propose that reinforcement learning (RL) from partial expert
demonstrations is not merely a training heuristic, but a promising framework
for solving complex sequence generation tasks. Supervised fine-tuning (SFT)
relies on dense ground-truth labels, which become increasingly costly as
sequence length grows. RL, on the other hand, struggles with sparse rewards and
a combinatorially large output space. We address this by introducing adaptive
backtracking (AdaBack), a per-sample curriculum learning algorithm that reveals
only a partial prefix of the target output during training. The supervision
length is adjusted dynamically for each sample based on the model's past reward
signal, allowing it to incrementally learn to complete reasoning chains by
conditioning on correct partial solutions. We investigate this intermediate
regime between SFT and RL and argue that per-sample curriculum learning is more
than a trade-off between efficiency and generality, it can succeed in tasks
with long sequences of latent dependencies where SFT and RL both fail to
generalize. Using a synthetic task with latent parity constraints, we show that
our adaptive curriculum over partial answers reliably solves problems that are
otherwise intractable. On mathematical reasoning benchmarks (MATH, GSM8k), we
find that curriculum learning enables models to solve problems that RL alone
cannot, acquiring new reasoning capabilities through incremental exposure to
partial solutions.

</details>


### [222] [Routing Mamba: Scaling State Space Models with Mixture-of-Experts Projection](https://arxiv.org/abs/2506.18145)
*Zheng Zhan,Liliang Ren,Shuohang Wang,Liyuan Liu,Yang Liu,Yeyun Gong,Yanzhi Wang,Yelong Shen*

Main category: cs.LG

TL;DR: 本文提出Routing Mamba (RoM)方法，用线性投影专家稀疏混合扩展SSM参数，在语言建模任务中表现佳，还能有效扩展混合语言模型并节省FLOPS。


<details>
  <summary>Details</summary>
Motivation: 高效扩展SSM（如结合MoE）存在挑战，天真集成常失败或降低性能。

Method: 引入Routing Mamba (RoM)，通过在投影层和Mamba轻量级子模块间共享路由决策，利用线性投影专家协同进行Mamba层的有效稀疏扩展。

Result: 在1.3B活动参数（共10B）和16K训练序列长度下，语言建模性能与需超2.3倍活动参数的密集Mamba模型相当，各上下文长度困惑度稳定；扩展混合语言模型时，与密集Mamba扩展相比，FLOPS节省23%。

Conclusion: RoM能有效且高效地扩展SSM参数，在语言建模和扩展混合语言模型上表现出色。

Abstract: Linear State Space Models (SSMs) offer remarkable performance gains in
efficient sequence modeling, with constant inference-time computation and
memory complexity. Recent advances, such as Mamba, further enhance SSMs with
input-dependent gating and hardware-aware implementations, positioning them as
strong alternatives to Transformers for long sequence modeling. However,
efficiently scaling the expressive power of SSMs, particularly with Mixture of
Experts (MoE), remains challenging, as naive integration attempts often falter
or degrade performance. In this work, we introduce Routing Mamba (RoM), a novel
approach that scales SSM parameters using sparse mixtures of linear projection
experts. By sharing routing decisions between projection layers and lightweight
sub-modules within Mamba across experts, RoM leverages synergies among linear
projection experts for effective and efficient sparse scaling of Mamba layers.
At a scale of 1.3B active parameters (10B total) and 16K training sequence
length, RoM achieves language modeling performance equivalent to a dense Mamba
model requiring over 2.3x more active parameters, and demonstrates consistent
perplexity across context lengths. Experimental results further show RoM
effectively scales hybrid language models, yielding a 23% FLOPS saving compared
to dense Mamba scaling for similar performance.

</details>


### [223] [Probabilistic and reinforced mining of association rules](https://arxiv.org/abs/2506.18155)
*Yongchao Huang*

Main category: cs.LG

TL;DR: 本文提出4种新的关联规则挖掘方法，区别于传统频率算法，提高灵活性和鲁棒性，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统频率算法在结合先验知识、建模不确定性等方面存在不足，需要新方法。

Method: 提出GPAR、BARM、MAB - ARM和RLAR四种方法，分别采用高斯过程、贝叶斯框架、UCB策略和深度Q网络。

Result: 在合成和真实数据集上验证了方法有效性，同时也体现了计算复杂度和可解释性的权衡。

Conclusion: 这些创新方法是对传统范式的重大转变，为不同领域提供了新的关联规则挖掘框架。

Abstract: This work introduces 4 novel probabilistic and reinforcement-driven methods
for association rule mining (ARM): Gaussian process-based association rule
mining (GPAR), Bayesian ARM (BARM), multi-armed bandit based ARM (MAB-ARM), and
reinforcement learning based association rule mining (RLAR). These methods
depart fundamentally from traditional frequency-based algorithms such as
Apriori, FP-Growth, and Eclat, offering enhanced capabilities for incorporating
prior knowledge, modeling uncertainty, item dependencies, probabilistic
inference and adaptive search strategies. GPAR employs Gaussian processes to
model item co-occurrence via feature representations, enabling principled
inference, uncertainty quantification, and efficient generalization to unseen
itemsets without retraining. BARM adopts a Bayesian framework with priors and
optional correlation structures, yielding robust uncertainty quantification
through full posterior distributions over item presence probabilities. MAB-ARM,
including its Monte Carlo tree search (MCTS) companion, utilizes an upper
confidence bound (UCB) strategy for efficient and adaptive exploration of the
itemset space, while RLAR applies a deep Q-network (DQN) to learn a
generalizable policy for identifying high-quality rules. Collectively, these
approaches improve the flexibility and robustness of ARM, particularly for
discovering rare or complex patterns and operating on small datasets. Empirical
results on synthetic and real-world datasets demonstrate their effectiveness,
while also highlighting trade-offs in computational complexity and
interpretability. These innovations mark a significant shift from static,
frequency-driven paradigms, offering some prior and dependency-informed,
uncertainty-aware or scalable ARM frameworks for diverse application domains
such as retail, geography, finance, medical diagnostics, and risk-sensitive
scenarios.

</details>


### [224] [Pitfalls of Conformal Predictions for Medical Image Classification](https://arxiv.org/abs/2506.18162)
*Hendrik Mehrtens,Tabea Bucher,Titus J. Brinker*

Main category: cs.LG

TL;DR: 文章指出医学分类任务中可靠的不确定性估计有挑战，虽共形预测受关注，但在医学应用中有诸多问题。


<details>
  <summary>Details</summary>
Motivation: 探讨共形预测在医学分类任务中可靠不确定性估计的应用情况。

Method: 通过皮肤病学和组织病理学的例子进行说明。

Result: 发现共形预测在输入和标签变量分布变化时不可靠，不适用于选择预测以提高准确性，对数据子集不可靠，在少量类别分类设置中实用价值有限。

Conclusion: 共形预测在医学应用中有诸多陷阱、限制和假设，从业者需注意。

Abstract: Reliable uncertainty estimation is one of the major challenges for medical
classification tasks. While many approaches have been proposed, recently the
statistical framework of conformal predictions has gained a lot of attention,
due to its ability to provide provable calibration guarantees. Nonetheless, the
application of conformal predictions in safety-critical areas such as medicine
comes with pitfalls, limitations and assumptions that practitioners need to be
aware of. We demonstrate through examples from dermatology and histopathology
that conformal predictions are unreliable under distributional shifts in input
and label variables. Additionally, conformal predictions should not be used for
selecting predictions to improve accuracy and are not reliable for subsets of
the data, such as individual classes or patient attributes. Moreover, in
classification settings with a small number of classes, which are common in
medical image classification tasks, conformal predictions have limited
practical value.

</details>


### [225] [Non-equilibrium Annealed Adjoint Sampler](https://arxiv.org/abs/2506.18165)
*Jaemoo Choi,Yongxin Chen,Molei Tao,Guan-Horng Liu*

Main category: cs.LG

TL;DR: 本文介绍了基于随机最优控制的新型扩散采样器NAAS，无需重要性采样，在多任务中证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有退火方法依赖重要性采样，存在高方差和可扩展性有限的问题，需新的采样方法。

Method: 引入非平衡退火伴随采样器（NAAS），利用退火参考动力学，借助伴随匹配的精简伴随系统进行高效可扩展训练。

Result: 在从经典能量景观和分子玻尔兹曼分布采样等一系列任务中证明了方法的有效性。

Conclusion: NAAS是一种有效的基于随机最优控制的扩散采样器，无需重要性采样，可解决现有退火方法的问题。

Abstract: Recently, there has been significant progress in learning-based diffusion
samplers, which aim to sample from a given unnormalized density. These methods
typically follow one of two paradigms: (i) formulating sampling as an unbiased
stochastic optimal control (SOC) problem using a canonical reference process,
or (ii) refining annealed path measures through importance-weighted sampling.
Although annealing approaches have advantages in guiding samples toward
high-density regions, reliance on importance sampling leads to high variance
and limited scalability in practice. In this paper, we introduce the
\textbf{Non-equilibrium Annealed Adjoint Sampler (NAAS)}, a novel SOC-based
diffusion sampler that leverages annealed reference dynamics without resorting
to importance sampling. NAAS employs a lean adjoint system inspired by adjoint
matching, enabling efficient and scalable training. We demonstrate the
effectiveness of our approach across a range of tasks, including sampling from
classical energy landscapes and molecular Boltzmann distribution.

</details>


### [226] [Understanding Reasoning in Thinking Language Models via Steering Vectors](https://arxiv.org/abs/2506.18167)
*Constantin Venhoff,Iván Arcuschin,Philip Torr,Arthur Conmy,Neel Nanda*

Main category: cs.LG

TL;DR: 本文提出针对思考型大语言模型的引导方法，通过分析和操纵DeepSeek - R1 - Distill模型推理行为，用系统实验识别多种推理行为，证明可用引导向量控制，验证了方法在不同架构模型的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有思考型大语言模型虽性能提升，但控制推理过程仍具挑战。

Method: 分析和操纵DeepSeek - R1 - Distill模型特定推理行为，在10类共500个任务上进行系统实验，提取并应用引导向量。

Result: 识别出思考模型的多种推理行为，证明这些行为由模型激活空间的线性方向介导，可用引导向量控制。

Conclusion: 所提方法能以可控且可解释的方式引导思考模型的推理过程，在不同架构模型上有效。

Abstract: Recent advances in large language models (LLMs) have led to the development
of thinking language models that generate extensive internal reasoning chains
before producing responses. While these models achieve improved performance,
controlling their reasoning processes remains challenging. This work presents a
steering approach for thinking LLMs by analyzing and manipulating specific
reasoning behaviors in DeepSeek-R1-Distill models. Through a systematic
experiment on 500 tasks across 10 diverse categories, we identify several
reasoning behaviors exhibited by thinking models, including expressing
uncertainty, generating examples for hypothesis validation, and backtracking in
reasoning chains. We demonstrate that these behaviors are mediated by linear
directions in the model's activation space and can be controlled using steering
vectors. By extracting and applying these vectors, we provide a method to
modulate specific aspects of the model's reasoning process, such as its
tendency to backtrack or express uncertainty. Our approach offers practical
tools for steering reasoning processes in thinking models in a controlled and
interpretable manner. We validate our steering method using two
DeepSeek-R1-Distill models, demonstrating consistent control across different
model architectures.

</details>


### [227] [Memba: Membrane-driven Parameter-Efficient Fine-Tuning for Mamba](https://arxiv.org/abs/2506.18184)
*Donghyun Lee,Yuhang Li,Ruokai Yin,Shiting Xiao,Priyadarshini Panda*

Main category: cs.LG

TL;DR: 提出针对Mamba的Memba参数高效微调方法，在语言和视觉任务实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着SSMs模型增大，需参数高效微调方法适配下游任务，以往方法未考虑SSMs独特时间处理动态。

Method: 提出Memba，引入Leaky Integrate Membrane (LIM) 神经元作为门控机制，结合Low - Rank Adaptations (LoRA) 和跨层膜转移。

Result: 在语言和视觉任务的大量实验中，Memba相比现有PEFT方法有显著提升。

Conclusion: Memba能有效提升Mamba的时间建模能力，是一种有效的PEFT方法，代码已开源。

Abstract: State Space Models (SSMs) have emerged as powerful alternatives to
attention-based Transformers, with Mamba demonstrating impressive efficiency
and scalability. As these models grow increasingly larger, the need for
Parameter-Efficient Fine-Tuning (PEFT) methods becomes critical to adapt
pre-trained Mamba to downstream tasks without prohibitive computational costs.
However, previous approaches simply apply traditional Transformer-tailored PEFT
methods without addressing the unique temporal processing dynamics of SSMs. To
address this limitation, we propose Memba, a membrane-driven PEFT approach
specifically designed for Mamba. Memba introduces Leaky Integrate Membrane
(LIM) neurons as bio-inspired gating mechanisms that naturally accumulate
membrane potentials over time, enhancing selective information retention. By
strategically combining LIM neurons with Low-Rank Adaptations (LoRA) and
cross-layer membrane transfer, our approach significantly improves Mamba's
temporal modeling capabilities. Extensive experiments across language and
vision tasks demonstrate that Memba achieves substantial improvements over
existing PEFT methods. The code is available at
https://github.com/Intelligent-Computing-Lab-Yale/Memba.

</details>


### [228] [Joint Embedding Predictive Architecture for self-supervised pretraining on polymer molecular graphs](https://arxiv.org/abs/2506.18194)
*Francesco Picolli,Gabriel Vogel,Jana M. Weber*

Main category: cs.LG

TL;DR: 研究在聚合物分子图上使用JEPA进行自监督学习，结果显示在标记数据稀缺时可提升下游性能。


<details>
  <summary>Details</summary>
Motivation: 聚合物机器学习因高质量标记数据集稀缺而受阻，研究JEPA自监督学习策略在标记数据稀缺时能否提升下游性能。

Method: 在聚合物分子图上使用JEPA进行自监督预训练。

Result: 基于JEPA的自监督预训练在聚合物图上提升了下游性能，尤其在标记数据非常稀缺时，在所有测试数据集上都有改进。

Conclusion: JEPA自监督预训练在聚合物图上对下游性能有提升作用，特别是标记数据稀缺时。

Abstract: Recent advances in machine learning (ML) have shown promise in accelerating
the discovery of polymers with desired properties by aiding in tasks such as
virtual screening via property prediction. However, progress in polymer ML is
hampered by the scarcity of high-quality labeled datasets, which are necessary
for training supervised ML models. In this work, we study the use of the very
recent 'Joint Embedding Predictive Architecture' (JEPA), a type of architecture
for self-supervised learning (SSL), on polymer molecular graphs to understand
whether pretraining with the proposed SSL strategy improves downstream
performance when labeled data is scarce. Our results indicate that JEPA-based
self-supervised pretraining on polymer graphs enhances downstream performance,
particularly when labeled data is very scarce, achieving improvements across
all tested datasets.

</details>


### [229] [These are Not All the Features You are Looking For: A Fundamental Bottleneck In Supervised Pretraining](https://arxiv.org/abs/2506.18221)
*Xingyu Alice Yang,Jianyu Zhang,Léon Bottou*

Main category: cs.LG

TL;DR: 研究迁移学习挑战，发现信息饱和瓶颈，建议关注特定任务训练，提出丰富特征表示作为解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决迁移学习中确保迁移特征能处理未见数据集及量化任务相关性的挑战。

Method: 评估从预训练混合任务到各组件任务的模型迁移，分析是否能达到特定任务直接训练的性能。

Result: 发现深度学习模型存在信息饱和瓶颈，该现象在深度学习架构中普遍存在，数据分布等因素影响特征学习。

Conclusion: 仅依靠大规模网络可能不如特定任务训练有效，提出丰富特征表示以更好地泛化到新数据集。

Abstract: Transfer learning is a cornerstone of modern machine learning, promising a
way to adapt models pretrained on a broad mix of data to new tasks with minimal
new data. However, a significant challenge remains in ensuring that transferred
features are sufficient to handle unseen datasets, amplified by the difficulty
of quantifying whether two tasks are "related". To address these challenges, we
evaluate model transfer from a pretraining mixture to each of its component
tasks, assessing whether pretrained features can match the performance of
task-specific direct training. We identify a fundamental limitation in deep
learning models -- an "information saturation bottleneck" -- where networks
fail to learn new features once they encode similar competing features during
training. When restricted to learning only a subset of key features during
pretraining, models will permanently lose critical features for transfer and
perform inconsistently on data distributions, even components of the training
mixture. Empirical evidence from published studies suggests that this
phenomenon is pervasive in deep learning architectures -- factors such as data
distribution or ordering affect the features that current representation
learning methods can learn over time. This study suggests that relying solely
on large-scale networks may not be as effective as focusing on task-specific
training, when available. We propose richer feature representations as a
potential solution to better generalize across new datasets and, specifically,
present existing methods alongside a novel approach, the initial steps towards
addressing this challenge.

</details>


### [230] [AdapThink: Adaptive Thinking Preferences for Reasoning Language Model](https://arxiv.org/abs/2506.18237)
*Xu Wan,Wei Wang,Wenyue Xu,Wotao Yin,Jie Song,Mingyang Sun*

Main category: cs.LG

TL;DR: 提出AdapThink自适应后训练框架解决强化学习后训练中推理效率问题，实验证明其优势。


<details>
  <summary>Details</summary>
Motivation: 强化学习后训练虽提升语言模型推理能力，但存在推理效率问题，以往机制缺乏适应性。

Method: 提出AdapThink框架，包含利用模型置信度和响应特征的组相对奖励函数，以及通过熵引导分数平衡训练组准确率和推理多样性的多样性感知采样机制。

Result: 在多个数学推理数据集上的实验表明AdapThink能实现自适应推理模式并缓解低效问题。

Conclusion: AdapThink可在保持推理语言模型性能的同时，诱导更高效的思考。

Abstract: Reinforcement Learning (RL)-based post-training has significantly advanced
the complex reasoning capabilities of language models, fostering sophisticated
self-reflection processes. However, this ``slow thinking'' paradigm presents a
critical challenge to reasoning efficiency: models may expend excessive
computation on simple questions and shift reasoning prematurely for complex
ones. Previous mechanisms typically rely on static length budgets or predefined
rules, lacking the adaptability for varying question complexities and models'
evolving capabilities. To this end, we propose AdapThink, an adaptive
post-training framework designed to induce more efficient thinking while
maintaining the performance of reasoning language models. Specifically,
AdapThink incorporates two key mechanisms: 1) A group-relative reward function
that leverages model confidence and response's characteristic to dynamically
adjust the preference of reflection-related transition words without resorting
to a fixed length preference. 2) A diversity-aware sampling mechanism that
balances the training group's solution accuracy with reasoning diversity via an
entropy-guided score. Experiments on several mathematical reasoning datasets
with DeepSeek-distilled models demonstrate AdapThink's advantages in enabling
adaptive reasoning patterns and mitigating the inefficiencies.

</details>


### [231] [Quantum-Classical Hybrid Quantized Neural Network](https://arxiv.org/abs/2506.18240)
*Wenxin Li,Chuan Wang,Hongdong Zhu,Qi Gao,Yin Ma,Hai Wei,Kai Wen*

Main category: cs.LG

TL;DR: 提出用于量化神经网络训练的QBO模型，用FIP方法处理非线性问题，用QCGD算法解决QCBO问题，实验在Fashion MNIST任务达94.95%准确率。


<details>
  <summary>Details</summary>
Motivation: 使量子计算机能优化复杂非线性函数，拓宽其在人工智能中的应用，解决大规模QCBO模型求解中约束处理难题。

Method: 提出QBO模型，引入FIP方法离散激活函数，采用QCGD算法直接求解QCBO问题，推导相关理论上界。

Result: 实验使用CIM在Fashion MNIST分类任务上以1.1位精度达到94.95%准确率。

Conclusion: 所提方法有效，能在低精度下实现较高准确率，可利用量子计算机优化神经网络。

Abstract: Here in this work, we present a novel Quadratic Binary Optimization (QBO)
model for quantized neural network training, enabling the use of arbitrary
activation and loss functions through spline interpolation. We introduce
Forward Interval Propagation (FIP), a method designed to tackle the challenges
of non-linearity and the multi-layer composite structure in neural networks by
discretizing activation functions into linear subintervals. This approach
preserves the universal approximation properties of neural networks while
allowing complex nonlinear functions to be optimized using quantum computers,
thus broadening their applicability in artificial intelligence. We provide
theoretical upper bounds on the approximation error and the number of Ising
spins required, by deriving the sample complexity of the empirical risk
minimization problem, from an optimization perspective. A significant challenge
in solving the associated Quadratic Constrained Binary Optimization (QCBO)
model on a large scale is the presence of numerous constraints. When employing
the penalty method to handle these constraints, tuning a large number of
penalty coefficients becomes a critical hyperparameter optimization problem,
increasing computational complexity and potentially affecting solution quality.
To address this, we employ the Quantum Conditional Gradient Descent (QCGD)
algorithm, which leverages quantum computing to directly solve the QCBO
problem. We prove the convergence of QCGD under a quantum oracle with
randomness and bounded variance in objective value, as well as under limited
precision constraints in the coefficient matrix. Additionally, we provide an
upper bound on the Time-To-Solution for the QCBO solving process. Experimental
results using a coherent Ising machine (CIM) demonstrate a 94.95% accuracy on
the Fashion MNIST classification task, with only 1.1-bit precision.

</details>


### [232] [Dual-Forward Path Teacher Knowledge Distillation: Bridging the Capacity Gap Between Teacher and Student](https://arxiv.org/abs/2506.18244)
*Tong Li,Long Liu,Yihang Hu,Hu Chen,Shifeng Chen*

Main category: cs.LG

TL;DR: 本文提出DFPT - KD和DFPT - KD+方法解决知识蒸馏中师生网络容量差距问题，实验表明性能优于传统KD。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏师生网络容量差距大，已有方法不能有效解决该问题，限制蒸馏收益。

Method: 将基于提示的学习思想扩展，提出DFPT - KD，在预训练教师网络中建立基于提示的前向路径；进一步微调该路径得到DFPT - KD+。

Result: DFPT - KD训练的学生网络性能优于传统KD；DFPT - KD+性能优于DFPT - KD，达到了最先进的准确率。

Conclusion: DFPT - KD和DFPT - KD+能有效解决知识蒸馏中师生网络容量差距问题，提升蒸馏效果。

Abstract: Knowledge distillation (KD) provides an effective way to improve the
performance of a student network under the guidance of pre-trained teachers.
However, this approach usually brings in a large capacity gap between teacher
and student networks, limiting the distillation gains. Previous methods
addressing this problem either discard accurate knowledge representation or
fail to dynamically adjust the transferred knowledge, which is less effective
in addressing the capacity gap problem and hinders students from achieving
comparable performance with the pre-trained teacher. In this work, we extend
the ideology of prompt-based learning to address the capacity gap problem, and
propose Dual-Forward Path Teacher Knowledge Distillation (DFPT-KD), which
replaces the pre-trained teacher with a novel dual-forward path teacher to
supervise the learning of student. The key to DFPT-KD is prompt-based tuning,
i.e., establishing an additional prompt-based forward path within the
pre-trained teacher and optimizing it with the pre-trained teacher frozen to
make the transferred knowledge compatible with the representation ability of
the student. Extensive experiments demonstrate that DFPT-KD leads to trained
students performing better than the vanilla KD. To make the transferred
knowledge better compatible with the representation abilities of the student,
we further fine-tune the whole prompt-based forward path, yielding a novel
distillation approach dubbed DFPT-KD+. By extensive experiments, it is shown
that DFPT-KD+ improves upon DFPT-KD and achieves state-of-the-art accuracy
performance.

</details>


### [233] [Exploring Efficient Quantification of Modeling Uncertainties with Differentiable Physics-Informed Machine Learning Architectures](https://arxiv.org/abs/2506.18247)
*Manaswin Oddiraju,Bharath Varma Penumatsa,Divyang Amin,Michael Piedmonte,Souma Chowdhury*

Main category: cs.LG

TL;DR: 本文探索将贝叶斯神经网络集成到自动可微混合物理信息机器学习架构中以实现不确定性传播，经测试有一定效果。


<details>
  <summary>Details</summary>
Motivation: 物理信息机器学习方法在预测和传播建模不确定性方面有待探索，本文旨在研究贝叶斯神经网络能否为其赋予不确定性传播能力。

Method: 将结合部分物理和神经网络的自动可微混合PIML架构与贝叶斯神经网络集成，采用两阶段训练过程。

Result: 在分析基准问题和固定翼遥控飞机飞行实验数据上评估，预测性能与纯数据驱动ML和原始PIML模型相近，蒙特卡罗采样在传播不确定性上最有效。

Conclusion: 贝叶斯神经网络集成的PIML架构能在一定程度上实现不确定性传播，蒙特卡罗采样是有效传播方法。

Abstract: Quantifying and propagating modeling uncertainties is crucial for reliability
analysis, robust optimization, and other model-based algorithmic processes in
engineering design and control. Now, physics-informed machine learning (PIML)
methods have emerged in recent years as a new alternative to traditional
computational modeling and surrogate modeling methods, offering a balance
between computing efficiency, modeling accuracy, and interpretability. However,
their ability to predict and propagate modeling uncertainties remains mostly
unexplored. In this paper, a promising class of auto-differentiable hybrid PIML
architectures that combine partial physics and neural networks or ANNs (for
input transformation or adaptive parameter estimation) is integrated with
Bayesian Neural networks (replacing the ANNs); this is done with the goal to
explore whether BNNs can successfully provision uncertainty propagation
capabilities in the PIML architectures as well, further supported by the
auto-differentiability of these architectures. A two-stage training process is
used to alleviate the challenges traditionally encountered in training
probabilistic ML models. The resulting BNN-integrated PIML architecture is
evaluated on an analytical benchmark problem and flight experiments data for a
fixed-wing RC aircraft, with prediction performance observed to be slightly
worse or at par with purely data-driven ML and original PIML models. Moreover,
Monte Carlo sampling of probabilistic BNN weights was found to be most
effective in propagating uncertainty in the BNN-integrated PIML architectures.

</details>


### [234] [RLPR: Extrapolating RLVR to General Domains without Verifiers](https://arxiv.org/abs/2506.18254)
*Tianyu Yu,Bo Ji,Shouli Wang,Shu Yao,Zefan Wang,Ganqu Cui,Lifan Yuan,Ning Ding,Yuan Yao,Zhiyuan Liu,Maosong Sun,Tat-Seng Chua*

Main category: cs.LG

TL;DR: 提出无验证器框架RLPR扩展RLVR到通用领域，实验表明其能提升多种模型推理能力，且表现优于其他方法。


<details>
  <summary>Details</summary>
Motivation: RLVR依赖特定领域验证器，导致复杂性高和可扩展性有限，需扩展到通用领域。

Method: 提出RLPR框架，利用LLM自身标记概率分数作为奖励信号，提出处理高方差的方法以确保奖励精确稳定。

Result: 在多个通用和数学基准测试中，RLPR能提升Gemma、Llama和Qwen等模型的推理能力，且优于VeriFree和General - Reasoner。

Conclusion: RLPR是一种有效的无验证器框架，可扩展到通用领域提升LLM推理能力。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates promising
potential in advancing the reasoning capabilities of LLMs. However, its success
remains largely confined to mathematical and code domains. This primary
limitation stems from the heavy reliance on domain-specific verifiers, which
results in prohibitive complexity and limited scalability. To address the
challenge, our key observation is that LLM's intrinsic probability of
generating a correct free-form answer directly indicates its own evaluation of
the reasoning reward (i.e., how well the reasoning process leads to the correct
answer). Building on this insight, we propose RLPR, a simple verifier-free
framework that extrapolates RLVR to broader general domains. RLPR uses the
LLM's own token probability scores for reference answers as the reward signal
and maximizes the expected reward during training. We find that addressing the
high variance of this noisy probability reward is crucial to make it work, and
propose prob-to-reward and stabilizing methods to ensure a precise and stable
reward from LLM intrinsic probabilities. Comprehensive experiments in four
general-domain benchmarks and three mathematical benchmarks show that RLPR
consistently improves reasoning capabilities in both areas for Gemma, Llama,
and Qwen based models. Notably, RLPR outperforms concurrent VeriFree by 7.6
points on TheoremQA and 7.5 points on Minerva, and even surpasses strong
verifier-model-dependent approaches General-Reasoner by 1.6 average points
across seven benchmarks.

</details>


### [235] [Ground tracking for improved landmine detection in a GPR system](https://arxiv.org/abs/2506.18258)
*Li Tang,Peter A. Torrione,Cihat Eldeniz,Leslie M. Collins*

Main category: cs.LG

TL;DR: 本文提出用卡尔曼滤波器和粒子滤波器框架的地面反射（GB）跟踪算法减轻干扰，提升探雷性能，并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 地面穿透雷达（GPR）数据中的地面反射（GB）干扰是影响低金属含量地雷检测性能的主要因素，需减轻该干扰。

Method: 提出用卡尔曼滤波器（KF）和粒子滤波器（PF）框架的GB跟踪算法，将雷达信号中GB位置建模为随机系统隐藏状态，利用2D雷达图像，自动设置参数，自适应更新特征，通过相邻通道/扫描信息预测先验分布。

Result: 在利用真实数据的实验中验证了算法，并与其他GB跟踪方法进行性能比较。

Conclusion: 改进的GB跟踪有助于提升地雷检测问题的性能。

Abstract: Ground penetrating radar (GPR) provides a promising technology for accurate
subsurface object detection. In particular, it has shown promise for detecting
landmines with low metal content. However, the ground bounce (GB) that is
present in GPR data, which is caused by the dielectric discontinuity between
soil and air, is a major source of interference and degrades landmine detection
performance. To mitigate this interference, GB tracking algorithms formulated
using both a Kalman filter (KF) and a particle filter (PF) framework are
proposed. In particular, the location of the GB in the radar signal is modeled
as the hidden state in a stochastic system for the PF approach. The
observations are the 2D radar images, which arrive scan by scan along the
down-track direction. An initial training stage sets parameters automatically
to accommodate different ground and weather conditions. The features associated
with the GB description are updated adaptively with the arrival of new data.
The prior distribution for a given location is predicted by propagating
information from two adjacent channels/scans, which ensures that the overall GB
surface remains smooth. The proposed algorithms are verified in experiments
utilizing real data, and their performances are compared with other GB tracking
approaches. We demonstrate that improved GB tracking contributes to improved
performance for the landmine detection problem.

</details>


### [236] [ARD-LoRA: Dynamic Rank Allocation for Parameter-Efficient Fine-Tuning of Foundation Models with Heterogeneous Adaptation Needs](https://arxiv.org/abs/2506.18267)
*Haseeb Ullah Khan Shinwari,Muhammad Usama*

Main category: cs.LG

TL;DR: 提出自适应秩动态LoRA（ARD - LoRA）框架，可自动分配秩，实验证明其有效，确立动态细粒度秩分配是高效基础模型适配的关键范式。


<details>
  <summary>Details</summary>
Motivation: 传统低秩适配（LoRA）方法采用固定秩，无法适应Transformer层和注意力头的异质学习动态。

Method: 引入ARD - LoRA框架，通过可学习的缩放因子自动分配秩，用元目标优化缩放因子，结合ℓ1稀疏性和总变差正则化。

Result: 在LLAMA - 3.1 - 70B和PaliGemma - 2上实验，用0.32%可训练参数达全微调性能的99.3%，优于DoRA和AdaLoRA，减少41%多模态适配内存。

Conclusion: 动态细粒度秩分配是高效基础模型适配的关键范式。

Abstract: Conventional Low-Rank Adaptation (LoRA) methods employ a fixed rank, imposing
uniform adaptation across transformer layers and attention heads despite their
heterogeneous learning dynamics. This paper introduces Adaptive Rank Dynamic
LoRA (ARD-LoRA), a novel framework that automates rank allocation through
learnable scaling factors. These factors are optimized via a meta-objective
balancing task performance and parameter efficiency, incorporating $\ell_1$
sparsity for minimal rank and Total Variation regularization for stable rank
transitions. ARD-LoRA enables continuous, differentiable, per-head rank
adaptation. Experiments on LLAMA-3.1-70B and PaliGemma-2 demonstrate ARD-LoRA's
efficacy, achieving up to 99.3% of full fine-tuning performance with only 0.32%
trainable parameters, outperforming strong baselines like DoRA and AdaLoRA.
Furthermore, it reduces multimodal adaptation memory by 41%. These results
establish dynamic, fine-grained rank allocation as a critical paradigm for
efficient foundation model adaptation.

</details>


### [237] [Memory-Augmented Architecture for Long-Term Context Handling in Large Language Models](https://arxiv.org/abs/2506.18271)
*Haseeb Ullah Khan Shinwari,Muhammad Usama*

Main category: cs.LG

TL;DR: 大语言模型在长对话中因上下文记忆有限面临挑战，本文提出记忆增强架构解决问题，实验显示其提升了对话效果。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在长对话中因上下文记忆有限导致交互不连贯、响应相关性降低及用户体验差的问题。

Method: 提出一种记忆增强架构，可动态检索、更新和修剪过往交互中的相关信息。

Result: 显著提高了上下文连贯性，减少了内存开销，提升了响应质量。

Conclusion: 该解决方案有潜力应用于交互式系统的实时场景。

Abstract: Large Language Models face significant challenges in maintaining coherent
interactions over extended dialogues due to their limited contextual memory.
This limitation often leads to fragmented exchanges and reduced relevance in
responses, diminishing user experience. To address these issues, we propose a
memory-augmented architecture that dynamically retrieves, updates, and prunes
relevant information from past interactions, ensuring effective long-term
context handling. Experimental results demonstrate that our solution
significantly improves contextual coherence, reduces memory overhead, and
enhances response quality, showcasing its potential for real-time applications
in interactive systems.

</details>


### [238] [Leveraging Large Language Models for Information Verification -- an Engineering Approach](https://arxiv.org/abs/2506.18274)
*Nguyen Nang Hung,Nguyen Thanh Trong,Vuong Thanh Toan,Nguyen An Phuoc,Dao Minh Tu,Nguyen Manh Duc Tuan,Nguyen Dinh Mau*

Main category: cs.LG

TL;DR: 提出用于多媒体新闻源验证的工程方法，以GPT - 4o为核心，经多步骤处理多媒体数据，全流程自动化，人工仅参与最终验证。


<details>
  <summary>Details</summary>
Motivation: 参加ACMMM25挑战，解决多媒体新闻源验证问题。

Method: 以GPT - 4o为骨干，用Google工具生成元数据，对多媒体数据分段、清理、选帧，交叉比对帧与元数据，提取音频转录，通过提示工程用GPT - 4o自动化流程。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: For the ACMMM25 challenge, we present a practical engineering approach to
multimedia news source verification, utilizing Large Language Models (LLMs)
like GPT-4o as the backbone of our pipeline. Our method processes images and
videos through a streamlined sequence of steps: First, we generate metadata
using general-purpose queries via Google tools, capturing relevant content and
links. Multimedia data is then segmented, cleaned, and converted into frames,
from which we select the top-K most informative frames. These frames are
cross-referenced with metadata to identify consensus or discrepancies.
Additionally, audio transcripts are extracted for further verification.
Noticeably, the entire pipeline is automated using GPT-4o through prompt
engineering, with human intervention limited to final validation.

</details>


### [239] [Learning Causal Graphs at Scale: A Foundation Model Approach](https://arxiv.org/abs/2506.18285)
*Naiyu Yin,Tian Gao,Yue Yu*

Main category: cs.LG

TL;DR: 本文提出Attention - DAG (ADAG)解决DAG学习在计算成本和小样本可识别性的挑战，在基准数据集上有良好表现，是DAG学习预训练基础模型的首个实用方法。


<details>
  <summary>Details</summary>
Motivation: DAG学习存在计算成本超指数增长和小样本可识别性问题，现有方法难解决。

Method: 利用线性transformer成功经验，提出基于注意力机制的ADAG架构学习多个线性结构方程模型，将多任务学习转化为连续优化问题。

Result: 在基准合成数据集上，ADAG在DAG学习准确性和零样本推理效率上有显著提升。

Conclusion: 这是首个DAG学习预训练基础模型的实用方法，向因果发现下游应用的高效性和泛化性迈进了一步。

Abstract: Due to its human-interpretability and invariance properties, Directed Acyclic
Graph (DAG) has been a foundational tool across various areas of AI research,
leading to significant advancements. However, DAG learning remains highly
challenging, due to its super-exponential growth in computational cost and
identifiability issues, particularly in small-sample regimes. To address these
two challenges, in this work we leverage the recent success of linear
transformers and develop a foundation model approach for discovering multiple
order-consistent DAGs across tasks. In particular, we propose Attention-DAG
(ADAG), a novel attention-mechanism-based architecture for learning multiple
linear Structural Equation Models (SEMs). ADAG learns the mapping from observed
data to both graph structure and parameters via a nonlinear attention-based
kernel, enabling efficient multi-task estimation of the underlying linear SEMs.
By formulating the learning process across multiple tasks as a continuous
optimization problem, the pre-trained ADAG model captures the common structural
properties as a shared low-dimensional prior, thereby reducing the
ill-posedness of downstream DAG learning tasks in small-sample regimes. We
evaluate our proposed approach on benchmark synthetic datasets and find that
ADAG achieves substantial improvements in both DAG learning accuracy and
zero-shot inference efficiency. To the best of our knowledge, this is the first
practical approach for pre-training a foundation model specifically designed
for DAG learning, representing a step toward more efficient and generalizable
down-stream applications in causal discovery.

</details>


### [240] [Learning High-Quality Latent Representations for Anomaly Detection and Signal Integrity Enhancement in High-Speed Signals](https://arxiv.org/abs/2506.18288)
*Muhammad Usama,Hee-Deok Jang,Soham Shanbhag,Yoo-Chang Sung,Seung-Jun Bae,Dong Eui Chang*

Main category: cs.LG

TL;DR: 本文提出联合训练框架提升高速动态随机存取存储器信号的异常检测和信号完整性，评估效果优于基线方法，还引入算法提升信号完整性。


<details>
  <summary>Details</summary>
Motivation: 应对高速动态随机存取存储器信号在异常检测和信号完整性方面的双重挑战。

Method: 提出集成自编码器和分类器的联合训练框架学习有效数据特征的潜在表示，引入信号完整性增强算法。

Result: 所提方法在三种异常检测算法评估中均优于两种基线方法，信号完整性增强算法平均提升11.3%。

Conclusion: 提出的联合训练框架和信号完整性增强算法在提升异常检测和信号完整性方面有效。

Abstract: This paper addresses the dual challenge of improving anomaly detection and
signal integrity in high-speed dynamic random access memory signals. To achieve
this, we propose a joint training framework that integrates an autoencoder with
a classifier to learn more distinctive latent representations by focusing on
valid data features. Our approach is evaluated across three anomaly detection
algorithms and consistently outperforms two baseline methods. Detailed ablation
studies further support these findings. Furthermore, we introduce a signal
integrity enhancement algorithm that improves signal integrity by an average of
11.3%. The source code and data used in this study are available at
https://github.com/Usama1002/learning-latent-representations.

</details>


### [241] [Instability in Diffusion ODEs: An Explanation for Inaccurate Image Reconstruction](https://arxiv.org/abs/2506.18290)
*Han Zhang,Jinghong Mao,Shangwen Zhu,Zhantao Yang,Lianghua Huang,Yu Liu,Deli Zhao,Ruili Feng,Fan Cheng*

Main category: cs.LG

TL;DR: 本文指出扩散重建存在误差，根源在于PF - ODE生成过程的不稳定性，通过实验证明其存在，理论证明高维时不稳定概率趋近1。


<details>
  <summary>Details</summary>
Motivation: 解决扩散重建中存在明显误差且无法用数值误差解释的问题。

Method: 在玩具数值示例和开源扩散模型上进行实验，基于图像数据特性进行理论证明。

Result: 实验证明了不稳定性的存在及其对重建误差的放大作用，理论证明数据维度增加时不稳定性概率趋近于1。

Conclusion: 揭示了基于扩散重建的内在挑战，为未来改进提供思路。

Abstract: Diffusion reconstruction plays a critical role in various applications such
as image editing, restoration, and style transfer. In theory, the
reconstruction should be simple - it just inverts and regenerates images by
numerically solving the Probability Flow-Ordinary Differential Equation
(PF-ODE). Yet in practice, noticeable reconstruction errors have been observed,
which cannot be well explained by numerical errors. In this work, we identify a
deeper intrinsic property in the PF-ODE generation process, the instability,
that can further amplify the reconstruction errors. The root of this
instability lies in the sparsity inherent in the generation distribution, which
means that the probability is concentrated on scattered and small regions while
the vast majority remains almost empty. To demonstrate the existence of
instability and its amplification on reconstruction error, we conduct
experiments on both toy numerical examples and popular open-sourced diffusion
models. Furthermore, based on the characteristics of image data, we
theoretically prove that the instability's probability converges to one as the
data dimensionality increases. Our findings highlight the inherent challenges
in diffusion-based reconstruction and can offer insights for future
improvements.

</details>


### [242] [GeNeRT: A Physics-Informed Approach to Intelligent Wireless Channel Modeling via Generalizable Neural Ray Tracing](https://arxiv.org/abs/2506.18295)
*Kejia Bian,Meixia Tao,Shu Sun,Jun Yu*

Main category: cs.LG

TL;DR: 提出GeNeRT框架用于信道建模，具有泛化性、准确性和效率优势，实验证明其性能良好。


<details>
  <summary>Details</summary>
Motivation: 现有神经射线追踪方法存在泛化能力受限和对电磁定律遵循不足的问题。

Method: 提出GeNeRT框架，支持场景内空间可迁移性和跨场景零样本泛化，采用菲涅尔启发的神经网络设计，引入GPU张量加速策略。

Result: 在室外场景实验中，GeNeRT在未训练区域和全新环境泛化能力好，多径分量预测准确性高，运行效率优于Wireless Insite。

Conclusion: 消融实验验证了网络架构和训练策略能捕捉射线 - 表面相互作用的物理原理。

Abstract: Neural ray tracing (RT) has emerged as a promising paradigm for channel
modeling by combining physical propagation principles with neural networks. It
enables high modeling accuracy and efficiency. However, current neural RT
methods face two key limitations: constrained generalization capability due to
strong spatial dependence, and weak adherence to electromagnetic laws. In this
paper, we propose GeNeRT, a Generalizable Neural RT framework with enhanced
generalization, accuracy and efficiency. GeNeRT supports both intra-scenario
spatial transferability and inter-scenario zero-shot generalization. By
incorporating Fresnel-inspired neural network design, it also achieves higher
accuracy in multipath component (MPC) prediction. Furthermore, a GPU-tensorized
acceleration strategy is introduced to improve runtime efficiency. Extensive
experiments conducted in outdoor scenarios demonstrate that GeNeRT generalizes
well across untrained regions within a scenario and entirely unseen
environments, and achieves superior accuracy in MPC prediction compared to
baselines. Moreover, it outperforms Wireless Insite in runtime efficiency,
particularly in multi-transmitter settings. Ablation experiments validate the
effectiveness of the network architecture and training strategy in capturing
physical principles of ray-surface interactions.

</details>


### [243] [Sharpening the Spear: Adaptive Expert-Guided Adversarial Attack Against DRL-based Autonomous Driving Policies](https://arxiv.org/abs/2506.18304)
*Junchao Fan,Xuyang Lei,Xiaolin Chang*

Main category: cs.LG

TL;DR: 提出自适应专家引导的对抗攻击方法，解决现有攻击方法效率低和训练不稳定问题，实验表明该方法在碰撞率、攻击效率和训练稳定性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: DRL 用于自动驾驶时基于其的策略易受对抗攻击，现有攻击方法存在效率低和训练不稳定的问题，需研究改进。

Method: 先用模仿学习从成功攻击演示中导出专家策略，用集成架构增强泛化能力，通过 KL 散度正则化项引导 DRL 对手，再引入性能感知退火策略降低对专家的依赖。

Result: 该方法在碰撞率、攻击效率和训练稳定性上优于现有方法，在专家策略次优时效果更明显。

Conclusion: 提出的自适应专家引导对抗攻击方法能有效提高攻击策略训练的稳定性和效率。

Abstract: Deep reinforcement learning (DRL) has emerged as a promising paradigm for
autonomous driving. However, despite their advanced capabilities, DRL-based
policies remain highly vulnerable to adversarial attacks, posing serious safety
risks in real-world deployments. Investigating such attacks is crucial for
revealing policy vulnerabilities and guiding the development of more robust
autonomous systems. While prior attack methods have made notable progress, they
still face several challenges: 1) they often rely on high-frequency attacks,
yet critical attack opportunities are typically context-dependent and
temporally sparse, resulting in inefficient attack patterns; 2) restricting
attack frequency can improve efficiency but often results in unstable training
due to the adversary's limited exploration. To address these challenges, we
propose an adaptive expert-guided adversarial attack method that enhances both
the stability and efficiency of attack policy training. Our method first
derives an expert policy from successful attack demonstrations using imitation
learning, strengthened by an ensemble Mixture-of-Experts architecture for
robust generalization across scenarios. This expert policy then guides a
DRL-based adversary through a KL-divergence regularization term. Due to the
diversity of scenarios, expert policies may be imperfect. To address this, we
further introduce a performance-aware annealing strategy that gradually reduces
reliance on the expert as the adversary improves. Extensive experiments
demonstrate that our method achieves outperforms existing approaches in terms
of collision rate, attack efficiency, and training stability, especially in
cases where the expert policy is sub-optimal.

</details>


### [244] [Confucius3-Math: A Lightweight High-Performance Reasoning LLM for Chinese K-12 Mathematics Learning](https://arxiv.org/abs/2506.18330)
*Lixin Wu,Na Cai,Qiao Cheng,Jiachen Wang,Yitao Duan*

Main category: cs.LG

TL;DR: 介绍14B参数的开源大模型Confucius3 - Math，在单消费级GPU高效运行，数学推理任务表现优，针对中国K - 12数学学习，介绍开发方法并开源。


<details>
  <summary>Details</summary>
Motivation: 利用AI加强教育和知识传播，为中国K - 12学生和教育者的数学学习服务。

Method: 通过大规模强化学习（RL）进行后训练，采用Targeted Entropy Regularization、Recent Sample Recovery和Policy - Specific Hardness Weighting三项技术创新。

Result: Confucius3 - Math在数学推理任务上达到SOTA，能以低成本解决主流中国K - 12数学问题，三项创新稳定RL训练、提高数据效率和性能。

Conclusion: 证明了低成本构建特定领域强推理模型的可行性。

Abstract: We introduce Confucius3-Math, an open-source large language model with 14B
parameters that (1) runs efficiently on a single consumer-grade GPU; (2)
achieves SOTA performances on a range of mathematical reasoning tasks,
outperforming many models with significantly larger sizes. In particular, as
part of our mission to enhancing education and knowledge dissemination with AI,
Confucius3-Math is specifically committed to mathematics learning for Chinese
K-12 students and educators. Built via post-training with large-scale
reinforcement learning (RL), Confucius3-Math aligns with national curriculum
and excels at solving main-stream Chinese K-12 mathematical problems with low
cost. In this report we share our development recipe, the challenges we
encounter and the techniques we develop to overcome them. In particular, we
introduce three technical innovations: Targeted Entropy Regularization, Recent
Sample Recovery and Policy-Specific Hardness Weighting. These innovations
encompass a new entropy regularization, a novel data scheduling policy, and an
improved group-relative advantage estimator. Collectively, they significantly
stabilize the RL training, improve data efficiency, and boost performance. Our
work demonstrates the feasibility of building strong reasoning models in a
particular domain at low cost. We open-source our model and code at
https://github.com/netease-youdao/Confucius3-Math.

</details>


### [245] [Structured Kolmogorov-Arnold Neural ODEs for Interpretable Learning and Symbolic Discovery of Nonlinear Dynamics](https://arxiv.org/abs/2506.18339)
*Wei Liu,Kiran Bacsa,Loon Ching Tang,Eleni Chatzi*

Main category: cs.LG

TL;DR: 提出SKANODE框架解决深度学习在建模非线性动力系统时精度与可解释性不能兼顾的问题，实验证明其性能优越且模型具有可解释性。


<details>
  <summary>Details</summary>
Motivation: 深度学习在学习复杂系统行为时，实现高精度且具有物理可解释性的模型仍是重大挑战。

Method: 提出SKANODE框架，用可训练KAN作为通用函数逼近器进行虚拟传感恢复潜在状态，利用KAN符号回归能力提取系统动力学表达式，代回框架并训练校准系数。

Result: 在模拟和真实系统上的大量实验表明，SKANODE性能优越。

Conclusion: SKANODE能提供可解释、符合物理规律的模型，揭示非线性动力系统的潜在机制。

Abstract: Understanding and modeling nonlinear dynamical systems is a fundamental
problem across scientific and engineering domains. While deep learning has
demonstrated remarkable potential for learning complex system behavior,
achieving models that are both highly accurate and physically interpretable
remains a major challenge. To address this, we propose Structured
Kolmogorov-Arnold Neural ODEs (SKANODEs), a novel framework that integrates
structured state-space modeling with the Kolmogorov-Arnold Network (KAN).
SKANODE first employs a fully trainable KAN as a universal function
approximator within a structured Neural ODE framework to perform virtual
sensing, recovering latent states that correspond to physically interpretable
quantities such as positions and velocities. Once this structured latent
representation is established, we exploit the symbolic regression capability of
KAN to extract compact and interpretable expressions for the system's governing
dynamics. The resulting symbolic expression is then substituted back into the
Neural ODE framework and further calibrated through continued training to
refine its coefficients, enhancing both the precision of the discovered
equations and the predictive accuracy of system responses. Extensive
experiments on both simulated and real-world systems demonstrate that SKANODE
achieves superior performance while offering interpretable, physics-consistent
models that uncover the underlying mechanisms of nonlinear dynamical systems.

</details>


### [246] [Controlled Generation with Equivariant Variational Flow Matching](https://arxiv.org/abs/2506.18340)
*Floor Eijkelboom,Heiko Zimmermann,Sharvaree Vadgama,Erik J Bekkers,Max Welling,Christian A. Naesseth,Jan-Willem van de Meent*

Main category: cs.LG

TL;DR: 本文在变分流匹配框架下推导可控生成目标，提出两种可控生成实现方式，建立等变生成条件，在分子生成任务上表现优异，加强了流生成模型与贝叶斯推理联系。


<details>
  <summary>Details</summary>
Motivation: 在变分流匹配框架下实现可控生成，建立等变生成条件，为约束驱动和对称感知生成提供框架。

Method: 在变分流匹配框架推导可控生成目标，通过端到端训练条件生成模型或作为贝叶斯推理问题实现可控生成，建立等变生成条件并给出分子生成的等变公式。

Result: 在无控制和可控分子生成任务上评估，无控制生成达SOTA，可控生成在端到端训练和贝叶斯推理设置中均优于SOTA模型。

Conclusion: 加强了流生成模型与贝叶斯推理的联系，为约束驱动和对称感知生成提供可扩展且有原则的框架。

Abstract: We derive a controlled generation objective within the framework of
Variational Flow Matching (VFM), which casts flow matching as a variational
inference problem. We demonstrate that controlled generation can be implemented
two ways: (1) by way of end-to-end training of conditional generative models,
or (2) as a Bayesian inference problem, enabling post hoc control of
unconditional models without retraining. Furthermore, we establish the
conditions required for equivariant generation and provide an equivariant
formulation of VFM tailored for molecular generation, ensuring invariance to
rotations, translations, and permutations. We evaluate our approach on both
uncontrolled and controlled molecular generation, achieving state-of-the-art
performance on uncontrolled generation and outperforming state-of-the-art
models in controlled generation, both with end-to-end training and in the
Bayesian inference setting. This work strengthens the connection between
flow-based generative modeling and Bayesian inference, offering a scalable and
principled framework for constraint-driven and symmetry-aware generation.

</details>


### [247] [SlimMoE: Structured Compression of Large MoE Models via Expert Slimming and Distillation](https://arxiv.org/abs/2506.18349)
*Zichong Li,Chen Liang,Zixuan Zhang,Ilgee Hong,Young Jin Kim,Weizhu Chen,Tuo Zhao*

Main category: cs.LG

TL;DR: 提出SlimMoE框架压缩大MoE模型，生成小而高效变体，在资源受限环境表现好，模型公开。


<details>
  <summary>Details</summary>
Motivation: MoE架构内存需求大，在资源受限环境微调或部署成本高。

Method: 引入SlimMoE多阶段压缩框架，通过精简专家和中间阶段知识转移减少参数。

Result: 用该框架压缩Phi 3.5 - MoE生成Phi - mini - MoE和Phi - tiny - MoE，可单GPU微调，性能优于同尺寸模型，与大模型有竞争力。

Conclusion: 结构化剪枝结合分阶段蒸馏是创建高质量紧凑MoE模型的有效途径，利于MoE架构广泛应用。

Abstract: The Mixture of Experts (MoE) architecture has emerged as a powerful paradigm
for scaling large language models (LLMs) while maintaining inference
efficiency. However, their enormous memory requirements make them prohibitively
expensive to fine-tune or deploy in resource-constrained environments. To
address this challenge, we introduce SlimMoE, a multi-stage compression
framework for transforming large MoE models into much smaller, efficient
variants without incurring the prohibitive costs of training from scratch. Our
method systematically reduces parameter counts by slimming experts and
transferring knowledge through intermediate stages, effectively mitigating the
performance degradation common in one-shot pruning approaches. Using this
framework, we compress Phi 3.5-MoE (41.9B total/6.6B activated parameters) to
create Phi-mini-MoE (7.6B total/2.4B activated parameters) and Phi-tiny-MoE
(3.8B total/1.1B activated parameters) using only 400B tokens--less than 10% of
the original model's training data. These compressed models can be fine-tuned
on a single GPU (A100 for Phi-mini-MoE, A6000 for Phi-tiny-MoE), making them
highly suitable for academic and resource-limited settings. Our experiments
demonstrate that these compressed models outperform others of similar size and
remain competitive with larger models. For instance, Phi-mini-MoE achieves
similar or better performance to Phi-3-mini using only 2/3 of the activated
parameters and yields comparable MMLU scores to Llama 3.1 8B despite having
significantly lower latency. Our findings demonstrate that structured pruning
combined with staged distillation offers an effective path to creating
high-quality, compact MoE models, paving the way for broader adoption of MoE
architectures. We make our models publicly available at
https://huggingface.co/microsoft/Phi-mini-MoE-instruct and
https://huggingface.co/microsoft/Phi-tiny-MoE-instruct .

</details>


### [248] [LOGICPO: Efficient Translation of NL-based Logical Problems to FOL using LLMs and Preference Optimization](https://arxiv.org/abs/2506.18383)
*Koushik Viswanadha,Deepanway Ghosal,Somak Aditya*

Main category: cs.LG

TL;DR: 文章提出用偏好优化数据集微调大模型，以提升其逻辑推理能力，最佳模型表现优于GPT - 3.5 - turbo。


<details>
  <summary>Details</summary>
Motivation: 现有提升大模型推理能力的方法在将自然语言推理问题转换为逻辑形式时存在不足，阻碍推理能力。

Method: 引入新数据集LogicPO，采用DPO、KTO等技术微调开源大模型。

Result: 最佳模型Phi - 3.5比GPT - 3.5 - turbo（8 - shot）逻辑正确结果多10%，语法错误少14%。

Conclusion: 通过该框架和改进的评估指标，为提升大模型逻辑推理能力提供了有前景的方向。

Abstract: Logical reasoning is a key task for artificial intelligence due to it's role
in major downstream tasks such as Question Answering, Summarization. Recent
methods in improving the reasoning ability of LLMs fall short in correctly
converting a natural language reasoning problem to an equivalent logical
formulation, which hinders the framework's overall ability to reason. Towards
this, we propose to use finetuning on a preference optimization dataset to
learn to parse and represent a natural language problem as a whole to a
consistent logical program by 1) introducing a new supervised and preference
optimization dataset LogicPO, and 2) adopting popular techniques such as Direct
Preference Optimization (DPO), Kahneman-Tversky optimization (KTO) to finetune
open-source LLMs. Our best model with Phi-3.5 consistently outperforms
GPT-3.5-turbo's (8-shot) by producing 10% more logically correct and with 14%
less syntax errors. Through the framework and our improved evaluation metrics,
we offer a promising direction in improving the logical reasoning of LLMs by
better representing them in their logical formulations.

</details>


### [249] [ADNF-Clustering: An Adaptive and Dynamic Neuro-Fuzzy Clustering for Leukemia Prediction](https://arxiv.org/abs/2506.18396)
*Marco Aruta,Ciro Listone,Giuseppe Murano,Aniello Murano*

Main category: cs.LG

TL;DR: 提出自适应动态神经模糊聚类框架ADNF用于白血病诊断和监测，在数据集上表现优于静态基线，有应用潜力。


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法无法适应不断变化的细胞模式和实时量化不确定性，而白血病诊断和监测依赖高通量图像数据。

Method: 结合基于卷积神经网络的特征提取和在线模糊聚类引擎，通过模糊C均值初始化软分区，用模糊时间索引更新微簇中心、密度和模糊参数，进行拓扑细化。

Result: 在C - NMC白血病显微镜数据集上，轮廓系数达到0.51，优于静态基线。

Conclusion: 该方法的自适应不确定性建模和无标签操作可集成到INFANT儿科肿瘤网络，支持个性化白血病管理。

Abstract: Leukemia diagnosis and monitoring rely increasingly on high-throughput image
data, yet conventional clustering methods lack the flexibility to accommodate
evolving cellular patterns and quantify uncertainty in real time. We introduce
Adaptive and Dynamic Neuro-Fuzzy Clustering, a novel streaming-capable
framework that combines Convolutional Neural Network-based feature extraction
with an online fuzzy clustering engine. ADNF initializes soft partitions via
Fuzzy C-Means, then continuously updates micro-cluster centers, densities, and
fuzziness parameters using a Fuzzy Temporal Index (FTI) that measures entropy
evolution. A topology refinement stage performs density-weighted merging and
entropy-guided splitting to guard against over- and under-segmentation. On the
C-NMC leukemia microscopy dataset, our tool achieves a silhouette score of
0.51, demonstrating superior cohesion and separation over static baselines. The
method's adaptive uncertainty modeling and label-free operation hold immediate
potential for integration within the INFANT pediatric oncology network,
enabling scalable, up-to-date support for personalized leukemia management.

</details>


### [250] [FREQuency ATTribution: Benchmarking Frequency-based Occlusion for Time Series Data](https://arxiv.org/abs/2506.18481)
*Dominique Mercier,Andreas Dengel,Sheraz,Ahmed*

Main category: cs.LG

TL;DR: 现有方法对基于时间序列网络解释不足，本文提出FreqATT框架，在频域分析表现更优。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络缺乏可解释性，现有可解释性方法对时间序列网络分析不够具体。

Method: 提出FreqATT框架，评估相关不同频率，对信号滤波或标记相关输入数据。

Result: 频域分析能比现有方法更好地突出输入信号中的相关区域，且对信号波动更具鲁棒性。

Conclusion: FreqATT框架可实现事后网络对时间序列的分析。

Abstract: Deep neural networks are among the most successful algorithms in terms of
performance and scalability in different domains. However, since these networks
are black boxes, their usability is severely restricted due to the lack of
interpretability. Existing interpretability methods do not address the analysis
of time-series-based networks specifically enough. This paper shows that an
analysis in the frequency domain can not only highlight relevant areas in the
input signal better than existing methods, but is also more robust to
fluctuations in the signal. In this paper, FreqATT is presented, a framework
that enables post-hoc networks to interpret time series analysis. To achieve
this, the relevant different frequencies are evaluated and the signal is either
filtered or the relevant input data is marked.

</details>


### [251] [AnalogNAS-Bench: A NAS Benchmark for Analog In-Memory Computing](https://arxiv.org/abs/2506.18495)
*Aniss Bessalah,Hatem Mohamed Abdelmoumen,Karima Benatchba,Hadjer Benmeziane*

Main category: cs.LG

TL;DR: 引入针对模拟内存计算（AIMC）的首个神经架构搜索（NAS）基准AnalogNAS - Bench，揭示三个关键见解并指出当前NAS基准的局限。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络未针对AIMC设计，需要NAS发现优化架构，而比较NAS方法需要考虑AIMC硬件非理想性的NAS基准。

Method: 引入AnalogNAS - Bench这一专为AIMC定制的NAS基准。

Result: 揭示三点关键见解，即标准量化技术无法捕捉AIMC特定噪声、鲁棒架构倾向于更宽和分支块、跳跃连接提高对时间漂移噪声的恢复能力。

Conclusion: 指出当前NAS基准对AIMC的局限性，为未来模拟感知NAS研究铺平道路。

Abstract: Analog In-memory Computing (AIMC) has emerged as a highly efficient paradigm
for accelerating Deep Neural Networks (DNNs), offering significant energy and
latency benefits over conventional digital hardware. However, state-of-the-art
neural networks are not inherently designed for AIMC, as they fail to account
for its unique non-idealities. Neural Architecture Search (NAS) is thus needed
to systematically discover neural architectures optimized explicitly for AIMC
constraints. However, comparing NAS methodologies and extracting insights about
robust architectures for AIMC requires a dedicated NAS benchmark that
explicitly accounts for AIMC-specific hardware non-idealities. To address this,
we introduce AnalogNAS-Bench, the first NAS benchmark tailored specifically for
AIMC. Our study reveals three key insights: (1) standard quantization
techniques fail to capture AIMC-specific noises, (2) robust architectures tend
to feature wider and branched blocks, (3) skip connections improve resilience
to temporal drift noise. These insights highlight the limitations of current
NAS benchmarks for AIMC and pave the way for future analog-aware NAS. All the
implementations used in this paper can be found at
https://github.com/IBM/analog-nas/tree/main/analognasbench.

</details>


### [252] [DDOT: A Derivative-directed Dual-decoder Ordinary Differential Equation Transformer for Dynamic System Modeling](https://arxiv.org/abs/2506.18522)
*Yang Chang,Kuang-Da Wang,Ping-Chun Hsieh,Cheng-Kuan Lin,Wen-Chih Peng*

Main category: cs.LG

TL;DR: 传统符号回归方法难以捕捉常微分方程（ODE）特性，ODEFormer对初始点敏感。本文提出DIV - diff指标和DDOT模型，实验显示DDOT性能优于现有方法且有实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 传统符号回归方法难以捕捉ODE的时间动态和变量间相关性，ODEFormer对初始起始点敏感，不能充分反映真实性能，需要更好的评估指标和模型。

Method: 提出DIV - diff指标，在目标区域的点网格上评估散度；引入基于Transformer的DDOT模型，结合预测ODE导数的辅助任务来重建多维ODE。

Result: 在ODEBench实验中，DDOT在重建和泛化任务的$P(R^2 > 0.9)$分别绝对提升4.58%和1.62%，DIV - diff绝对降低3.55%，且在麻醉数据集上有实际应用。

Conclusion: DDOT模型在重建多维ODE方面优于现有符号回归方法，且具有实际应用价值。

Abstract: Uncovering the underlying ordinary differential equations (ODEs) that govern
dynamic systems is crucial for advancing our understanding of complex
phenomena. Traditional symbolic regression methods often struggle to capture
the temporal dynamics and intervariable correlations inherent in ODEs.
ODEFormer, a state-of-the-art method for inferring multidimensional ODEs from
single trajectories, has made notable progress. However, its focus on
single-trajectory evaluation is highly sensitive to initial starting points,
which may not fully reflect true performance. To address this, we propose the
divergence difference metric (DIV-diff), which evaluates divergence over a grid
of points within the target region, offering a comprehensive and stable
analysis of the variable space. Alongside, we introduce DDOT
(Derivative-Directed Dual-Decoder Ordinary Differential Equation Transformer),
a transformer-based model designed to reconstruct multidimensional ODEs in
symbolic form. By incorporating an auxiliary task predicting the ODE's
derivative, DDOT effectively captures both structure and dynamic behavior.
Experiments on ODEBench show DDOT outperforms existing symbolic regression
methods, achieving an absolute improvement of 4.58% and 1.62% in $P(R^2 > 0.9)$
for reconstruction and generalization tasks, respectively, and an absolute
reduction of 3.55% in DIV-diff. Furthermore, DDOT demonstrates real-world
applicability on an anesthesia dataset, highlighting its practical impact.

</details>


### [253] [Federated Learning from Molecules to Processes: A Perspective](https://arxiv.org/abs/2506.18525)
*Jan G. Rittig,Clemens Kortmann*

Main category: cs.LG

TL;DR: 本文提出化工领域联邦学习的观点，探讨其应用并通过案例研究验证效果，表明其能在保护数据隐私下提升模型准确性。


<details>
  <summary>Details</summary>
Motivation: 化工公司的数据为专有数据，存在数据孤岛问题，阻碍了机器学习模型在大数据集上的训练，联邦学习可在不泄露数据的情况下联合训练模型。

Method: 探讨联邦学习在化工多个领域的潜在应用，并进行两个案例研究，分别用图神经网络预测二元混合物活度系数和用自编码器对精馏塔进行系统识别。

Result: 通过联邦学习联合训练的机器学习模型比各公司单独训练的模型准确性显著提高，且与使用所有公司组合数据集训练的模型表现相近。

Conclusion: 联邦学习在尊重企业数据隐私的同时，有很大潜力推动化工领域机器学习模型的发展，在未来工业应用中前景广阔。

Abstract: We present a perspective on federated learning in chemical engineering that
envisions collaborative efforts in machine learning (ML) developments within
the chemical industry. Large amounts of chemical and process data are
proprietary to chemical companies and are therefore locked in data silos,
hindering the training of ML models on large data sets in chemical engineering.
Recently, the concept of federated learning has gained increasing attention in
ML research, enabling organizations to jointly train machine learning models
without disclosure of their individual data. We discuss potential applications
of federated learning in several fields of chemical engineering, from the
molecular to the process scale. In addition, we apply federated learning in two
exemplary case studies that simulate practical scenarios of multiple chemical
companies holding proprietary data sets: (i) prediction of binary mixture
activity coefficients with graph neural networks and (ii) system identification
of a distillation column with autoencoders. Our results indicate that ML models
jointly trained with federated learning yield significantly higher accuracy
than models trained by each chemical company individually and can perform
similarly to models trained on combined datasets from all companies. Federated
learning has therefore great potential to advance ML models in chemical
engineering while respecting corporate data privacy, making it promising for
future industrial applications.

</details>


### [254] [Transformer World Model for Sample Efficient Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2506.18537)
*Azad Deihim,Eduardo Alonso,Dimitra Apostolopoulou*

Main category: cs.LG

TL;DR: 提出用于多智能体强化学习的MATWM，在多个基准测试中表现出色，样本效率高。


<details>
  <summary>Details</summary>
Motivation: 为多智能体强化学习设计有效的世界模型，解决部分可观测和非平稳性问题。

Method: 结合去中心化想象框架、半中心化评论家、队友预测模块，采用优先重放机制。

Result: 在多个基准测试中达到了最先进的性能，样本效率高，消融实验证实各组件有影响。

Conclusion: MATWM是多智能体强化学习中有效的世界模型，在多任务中表现良好。

Abstract: We present the Multi-Agent Transformer World Model (MATWM), a novel
transformer-based world model designed for multi-agent reinforcement learning
in both vector- and image-based environments. MATWM combines a decentralized
imagination framework with a semi-centralized critic and a teammate prediction
module, enabling agents to model and anticipate the behavior of others under
partial observability. To address non-stationarity, we incorporate a
prioritized replay mechanism that trains the world model on recent experiences,
allowing it to adapt to agents' evolving policies. We evaluated MATWM on a
broad suite of benchmarks, including the StarCraft Multi-Agent Challenge,
PettingZoo, and MeltingPot. MATWM achieves state-of-the-art performance,
outperforming both model-free and prior world model approaches, while
demonstrating strong sample efficiency, achieving near-optimal performance in
as few as 50K environment interactions. Ablation studies confirm the impact of
each component, with substantial gains in coordination-heavy tasks.

</details>


### [255] [No Training Wheels: Steering Vectors for Bias Correction at Inference Time](https://arxiv.org/abs/2506.18598)
*Aviral Gupta,Armaan Sethi,Ameesh Sethi*

Main category: cs.LG

TL;DR: 本文提出一种无训练的低成本方法，通过计算偏差向量减少分类偏差，提升最差组准确率。


<details>
  <summary>Details</summary>
Motivation: 神经网络分类器在不均衡数据集上训练会产生偏差，现有解决方法需重新训练或大量计算。

Method: 计算多数和少数群体平均激活值的差异定义“偏差向量”，从模型的残差流中减去该向量。

Result: 减少了分类偏差，提高了最差组的准确率。

Conclusion: 转向向量可用于分类任务，且该方法成本低、无需训练，能有效减轻分类模型的偏差。

Abstract: Neural network classifiers trained on datasets with uneven group
representation often inherit class biases and learn spurious correlations.
These models may perform well on average but consistently fail on atypical
groups. For example, in hair color classification, datasets may over-represent
females with blond hair, reinforcing stereotypes. Although various algorithmic
and data-centric methods have been proposed to address such biases, they often
require retraining or significant compute. In this work, we propose a cheap,
training-free method inspired by steering vectors used to edit behaviors in
large language models. We compute the difference in mean activations between
majority and minority groups to define a "bias vector," which we subtract from
the model's residual stream. This leads to reduced classification bias and
improved worst-group accuracy. We explore multiple strategies for extracting
and applying these vectors in transformer-like classifiers, showing that
steering vectors, traditionally used in generative models, can also be
effective in classification. More broadly, we showcase an extremely cheap,
inference time, training free method to mitigate bias in classification models.

</details>


### [256] [Simulation-Free Differential Dynamics through Neural Conservation Laws](https://arxiv.org/abs/2506.18604)
*Mengjian Hua,Eric Vanden-Eijnden,Ricky T. Q. Chen*

Main category: cs.LG

TL;DR: 提出一种无模拟框架用于训练连续时间扩散过程，可处理多种目标函数，在多领域验证。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么仅适用于受限问题，要么需昂贵模拟，因此需新方法。

Method: 提出耦合参数化方法，将福克 - 普朗克方程和密度函数要求作为硬约束。

Result: 可对多种问题进行无模拟训练，在多领域验证。

Conclusion: 该无模拟框架能有效训练连续时间扩散过程，适用于多种目标函数和应用场景。

Abstract: We present a novel simulation-free framework for training continuous-time
diffusion processes over very general objective functions. Existing methods
typically involve either prescribing the optimal diffusion process -- which
only works for heavily restricted problem formulations -- or require expensive
simulation to numerically obtain the time-dependent densities and sample from
the diffusion process. In contrast, we propose a coupled parameterization which
jointly models a time-dependent density function, or probability path, and the
dynamics of a diffusion process that generates this probability path. To
accomplish this, our approach directly bakes in the Fokker-Planck equation and
density function requirements as hard constraints, by extending and greatly
simplifying the construction of Neural Conservation Laws. This enables
simulation-free training for a large variety of problem formulations, from
data-driven objectives as in generative modeling and dynamical optimal
transport, to optimality-based objectives as in stochastic optimal control,
with straightforward extensions to mean-field objectives due to the ease of
accessing exact density functions. We validate our method in a diverse range of
application domains from modeling spatio-temporal events to learning optimal
dynamics from population data.

</details>


### [257] [Policy gradient methods for ordinal policies](https://arxiv.org/abs/2506.18614)
*Simón Weinberger,Jairo Cugliari*

Main category: cs.LG

TL;DR: 提出基于序回归模型的新策略参数化方法，在实际应用和连续动作任务中有效


<details>
  <summary>Details</summary>
Motivation: 标准的softmax参数化方法无法捕捉动作间的顺序关系，受实际工业问题启发

Method: 提出基于序回归模型并适配强化学习设置的新策略参数化方法

Result: 数值实验表明在实际应用和连续动作任务中有效，离散动作空间并应用序策略有竞争力

Conclusion: 所提方法能解决实际挑战，在不同任务中有良好表现

Abstract: In reinforcement learning, the softmax parametrization is the standard
approach for policies over discrete action spaces. However, it fails to capture
the order relationship between actions. Motivated by a real-world industrial
problem, we propose a novel policy parametrization based on ordinal regression
models adapted to the reinforcement learning setting. Our approach addresses
practical challenges, and numerical experiments demonstrate its effectiveness
in real applications and in continuous action tasks, where discretizing the
action space and applying the ordinal policy yields competitive performance.

</details>


### [258] [Pr{é}diction optimale pour un mod{è}le ordinal {à} covariables fonctionnelles](https://arxiv.org/abs/2506.18615)
*Simón Weinberger,Jairo Cugliari,Aurélie Le Cain*

Main category: cs.LG

TL;DR: 提出序数模型预测框架，推导预测形式，转换模型并应用于数据集。


<details>
  <summary>Details</summary>
Motivation: 构建序数模型的预测框架以进行有效预测。

Method: 引入基于损失函数的最优预测，给出最小绝对偏差预测的显式形式，将含函数协变量的序数模型转换为含多个标量协变量的经典序数模型。

Result: 论文未提及具体结果。

Conclusion: 论文未提及具体结论。

Abstract: We present a prediction framework for ordinal models: we introduce optimal
predictions using loss functions and give the explicit form of the
Least-Absolute-Deviation prediction for these models. Then, we reformulate an
ordinal model with functional covariates to a classic ordinal model with
multiple scalar covariates. We illustrate all the proposed methods and try to
apply these to a dataset collected by EssilorLuxottica for the development of a
control algorithm for the shade of connected glasses.

</details>


### [259] [Multi-Agent Reinforcement Learning for Inverse Design in Photonic Integrated Circuits](https://arxiv.org/abs/2506.18627)
*Yannik Mahlau,Maximilian Schier,Christoph Reinders,Frederik Schubert,Marco Bügling,Bodo Rosenhahn*

Main category: cs.LG

TL;DR: 本文提出用于光子集成电路（PICs）设计的强化学习（RL）环境和多智能体RL算法，在设计任务中表现优于基于梯度的优化方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于梯度的PICs逆向设计易陷入局部最优，随着对PICs兴趣增加，需要更具适应性的优化算法。

Method: 将设计空间离散成网格，把设计任务表述为含数千个二进制变量的优化问题，分解设计空间为数千个个体智能体。

Result: 算法仅用数千个环境样本就能优化设计，在二维和三维设计任务中均优于先前基于梯度的优化方法。

Conclusion: 该工作可为光子学逆向设计的样本高效RL进一步探索提供基准。

Abstract: Inverse design of photonic integrated circuits (PICs) has traditionally
relied on gradientbased optimization. However, this approach is prone to end up
in local minima, which results in suboptimal design functionality. As interest
in PICs increases due to their potential for addressing modern hardware demands
through optical computing, more adaptive optimization algorithms are needed. We
present a reinforcement learning (RL) environment as well as multi-agent RL
algorithms for the design of PICs. By discretizing the design space into a
grid, we formulate the design task as an optimization problem with thousands of
binary variables. We consider multiple two- and three-dimensional design tasks
that represent PIC components for an optical computing system. By decomposing
the design space into thousands of individual agents, our algorithms are able
to optimize designs with only a few thousand environment samples. They
outperform previous state-of-the-art gradient-based optimization in both twoand
three-dimensional design tasks. Our work may also serve as a benchmark for
further exploration of sample-efficient RL for inverse design in photonics.

</details>


### [260] [ReDit: Reward Dithering for Improved LLM Policy Optimization](https://arxiv.org/abs/2506.18631)
*Chenxing Wei,Jiarui Yu,Ying Tiffany He,Hande Dong,Yao Shu,Fei Yu*

Main category: cs.LG

TL;DR: 提出ReDit方法解决离散奖励导致的问题，实验证明其有效且高效。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则的奖励系统为离散奖励，会导致梯度异常、优化不稳定和收敛慢等问题。

Method: 提出ReDit方法，通过添加简单随机噪声对离散奖励信号进行抖动。

Result: 在不同任务实验中，ReDit仅用约10%训练步骤就达到与vanilla GRPO相当性能，训练相同时间性能提升4%，可视化证实缓解了梯度问题。

Conclusion: ReDit方法有效且高效，理论分析进一步验证其优势。

Abstract: DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning
capabilities through its rule-based reward system. While it's a ''perfect''
reward system that effectively mitigates reward hacking, such reward functions
are often discrete. Our experimental observations suggest that discrete rewards
can lead to gradient anomaly, unstable optimization, and slow convergence. To
address this issue, we propose ReDit (Reward Dithering), a method that dithers
the discrete reward signal by adding simple random noise. With this perturbed
reward, exploratory gradients are continuously provided throughout the learning
process, enabling smoother gradient updates and accelerating convergence. The
injected noise also introduces stochasticity into flat reward regions,
encouraging the model to explore novel policies and escape local optima.
Experiments across diverse tasks demonstrate the effectiveness and efficiency
of ReDit. On average, ReDit achieves performance comparable to vanilla GRPO
with only approximately 10% the training steps, and furthermore, still exhibits
a 4% performance improvement over vanilla GRPO when trained for a similar
duration. Visualizations confirm significant mitigation of gradient issues with
ReDit. Moreover, theoretical analyses are provided to further validate these
advantages.

</details>


### [261] [Granular-Ball-Induced Multiple Kernel K-Means](https://arxiv.org/abs/2506.18637)
*Shuyin Xia,Yifan Wang,Lifeng Shen,Guoyin Wang*

Main category: cs.LG

TL;DR: 多数多核聚类算法存在计算效率和鲁棒性问题，本文用粒度球计算改进多核聚类框架，提出GB - MKKM框架，在聚类任务中表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有多核聚类算法如多核K - means在复杂数据分布下计算效率和鲁棒性差，难以捕捉数据结构和多样性。

Method: 利用粒度球计算自适应拟合数据分布，引入粒度球核（GBK）及其对应的粒度球多核K - means框架（GB - MKKM）进行聚类。

Result: GB - MKKM框架在各类聚类任务的实证评估中展现出效率和聚类性能上的优越性。

Conclusion: 基于粒度球计算的GB - MKKM框架能有效提升多核聚类的计算效率和鲁棒性。

Abstract: Most existing multi-kernel clustering algorithms, such as multi-kernel
K-means, often struggle with computational efficiency and robustness when faced
with complex data distributions. These challenges stem from their dependence on
point-to-point relationships for optimization, which can lead to difficulty in
accurately capturing data sets' inherent structure and diversity. Additionally,
the intricate interplay between multiple kernels in such algorithms can further
exacerbate these issues, effectively impacting their ability to cluster data
points in high-dimensional spaces. In this paper, we leverage granular-ball
computing to improve the multi-kernel clustering framework. The core of
granular-ball computing is to adaptively fit data distribution by balls from
coarse to acceptable levels. Each ball can enclose data points based on a
density consistency measurement. Such ball-based data description thus improves
the computational efficiency and the robustness to unknown noises.
Specifically, based on granular-ball representations, we introduce the
granular-ball kernel (GBK) and its corresponding granular-ball multi-kernel
K-means framework (GB-MKKM) for efficient clustering. Using granular-ball
relationships in multiple kernel spaces, the proposed GB-MKKM framework shows
its superiority in efficiency and clustering performance in the empirical
evaluation of various clustering tasks.

</details>


### [262] [Federated Loss Exploration for Improved Convergence on Non-IID Data](https://arxiv.org/abs/2506.18640)
*Christian Internò,Markus Olhofer,Yaochu Jin,Barbara Hammer*

Main category: cs.LG

TL;DR: 本文提出FedLEx方法解决联邦学习在非IID数据场景的问题，通过联邦损失探索技术优化全局模型参数更新，实验显示性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法在非IID数据场景面临数据异质性挑战，缺乏鲁棒性。

Method: 提出FedLEx方法，采用联邦损失探索技术，客户计算模型参数梯度偏差形成全局指导矩阵，引导后续梯度更新。

Result: 与现有算法对比实验表明，在非IID条件下性能显著提升。

Conclusion: FedLEx有潜力克服联邦学习应用中的关键障碍。

Abstract: Federated learning (FL) has emerged as a groundbreaking paradigm in machine
learning (ML), offering privacy-preserving collaborative model training across
diverse datasets. Despite its promise, FL faces significant hurdles in
non-identically and independently distributed (non-IID) data scenarios, where
most existing methods often struggle with data heterogeneity and lack
robustness in performance. This paper introduces Federated Loss Exploration
(FedLEx), an innovative approach specifically designed to tackle these
challenges. FedLEx distinctively addresses the shortcomings of existing FL
methods in non-IID settings by optimizing its learning behavior for scenarios
in which assumptions about data heterogeneity are impractical or unknown. It
employs a federated loss exploration technique, where clients contribute to a
global guidance matrix by calculating gradient deviations for model parameters.
This matrix serves as a strategic compass to guide clients' gradient updates in
subsequent FL rounds, thereby fostering optimal parameter updates for the
global model. FedLEx effectively navigates the complex loss surfaces inherent
in non-IID data, enhancing knowledge transfer in an efficient manner, since
only a small number of epochs and small amount of data are required to build a
strong global guidance matrix that can achieve model convergence without the
need for additional data sharing or data distribution statics in a large client
scenario. Our extensive experiments with state-of-the art FL algorithms
demonstrate significant improvements in performance, particularly under
realistic non-IID conditions, thus highlighting FedLEx's potential to overcome
critical barriers in diverse FL applications.

</details>


### [263] [On Union-Closedness of Language Generation](https://arxiv.org/abs/2506.18642)
*Steve Hanneke,Amin Karbasi,Anay Mehrotra,Grigoris Velegkas*

Main category: cs.LG

TL;DR: 本文研究语言极限生成问题，解决了Li等人提出的三个开放问题，揭示其与传统统计学习任务不同，构造方法利用精心设计的类和对角化论证。


<details>
  <summary>Details</summary>
Motivation: 解决Kleinberg、Mullainathan及Li等人研究中遗留的开放问题，深入探索语言极限生成特性。

Method: 通过精心构造类和使用新颖的对角化论证方法。

Result: 证明可生成或非均匀可生成类的有限并集不一定可生成；找到了不满足EUC条件的非均匀可生成不可数类。

Conclusion: 语言极限生成与传统统计学习任务在有限并集性质上不同，无法通过组合生成器实现增强，构造方法有独立研究价值。

Abstract: We investigate language generation in the limit - a model by Kleinberg and
Mullainathan [NeurIPS 2024] and extended by Li, Raman, and Tewari [COLT 2025].
While Kleinberg and Mullainathan proved generation is possible for all
countable collections, Li et al. defined a hierarchy of generation notions
(uniform, non-uniform, and generatable) and explored their feasibility for
uncountable collections.
  Our first set of results resolve two open questions of Li et al. by proving
finite unions of generatable or non-uniformly generatable classes need not be
generatable. These follow from a stronger result: there is a non-uniformly
generatable class and a uniformly generatable class whose union is
non-generatable. This adds to the aspects along which language generation in
the limit is different from traditional tasks in statistical learning theory
like classification, which are closed under finite unions. In particular, it
implies that given two generators for different collections, one cannot combine
them to obtain a single "more powerful" generator, prohibiting this notion of
boosting.
  Our construction also addresses a third open question of Li et al. on whether
there are uncountable classes that are non-uniformly generatable and do not
satisfy the eventually unbounded closure (EUC) condition introduced by Li,
Raman, and Tewari. Our approach utilizes carefully constructed classes along
with a novel diagonalization argument that could be of independent interest in
the growing area of language generation.

</details>


### [264] [SaGIF: Improving Individual Fairness in Graph Neural Networks via Similarity Encoding](https://arxiv.org/abs/2506.18696)
*Yuchang Zhu,Jintang Li,Huizhe Zhang,Liang Chen,Zibin Zheng*

Main category: cs.LG

TL;DR: 文章探讨图神经网络个体公平性问题，提出SaGIF模型及两个评估指标，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前图神经网络个体公平性研究在明确不公平诱因和全面考虑相似个体识别方面存在不足。

Method: 初步分析不公平诱因，引入相似度一致性概念，提出拓扑融合和特征融合两个指标，构建SaGIF模型。

Result: 实验表明提出的指标和SaGIF有效，SaGIF优于现有方法且保持实用性能。

Conclusion: 提出的方法能改善图神经网络个体公平性，代码已开源。

Abstract: Individual fairness (IF) in graph neural networks (GNNs), which emphasizes
the need for similar individuals should receive similar outcomes from GNNs, has
been a critical issue. Despite its importance, research in this area has been
largely unexplored in terms of (1) a clear understanding of what induces
individual unfairness in GNNs and (2) a comprehensive consideration of
identifying similar individuals. To bridge these gaps, we conduct a preliminary
analysis to explore the underlying reason for individual unfairness and observe
correlations between IF and similarity consistency, a concept introduced to
evaluate the discrepancy in identifying similar individuals based on graph
structure versus node features. Inspired by our observations, we introduce two
metrics to assess individual similarity from two distinct perspectives:
topology fusion and feature fusion. Building upon these metrics, we propose
Similarity-aware GNNs for Individual Fairness, named SaGIF. The key insight
behind SaGIF is the integration of individual similarities by independently
learning similarity representations, leading to an improvement of IF in GNNs.
Our experiments on several real-world datasets validate the effectiveness of
our proposed metrics and SaGIF. Specifically, SaGIF consistently outperforms
state-of-the-art IF methods while maintaining utility performance. Code is
available at: https://github.com/ZzoomD/SaGIF.

</details>


### [265] [Multi-modal Anchor Gated Transformer with Knowledge Distillation for Emotion Recognition in Conversation](https://arxiv.org/abs/2506.18716)
*Jie Li,Shifei Ding,Lili Guo,Xuan Li*

Main category: cs.LG

TL;DR: 提出MAGTKD用于ERC任务，结合提示学习和知识蒸馏，在数据集上取得SOTA效果，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有ERC研究忽略模态贡献差异且帧级对齐复杂度高，需高效生成特定模态表示。

Method: 提出MAGTKD，用提示学习增强文本模态表示，知识蒸馏强化弱模态表示，引入多模态锚定门控变压器整合跨模态话语级表示。

Result: 在IEMOCAP和MELD数据集实验，知识蒸馏有效增强模态表示，实现情感识别SOTA性能。

Conclusion: MAGTKD能有效解决ERC任务中模态表示和整合问题，提升情感识别效果。

Abstract: Emotion Recognition in Conversation (ERC) aims to detect the emotions of
individual utterances within a conversation. Generating efficient and
modality-specific representations for each utterance remains a significant
challenge. Previous studies have proposed various models to integrate features
extracted using different modality-specific encoders. However, they neglect the
varying contributions of modalities to this task and introduce high complexity
by aligning modalities at the frame level. To address these challenges, we
propose the Multi-modal Anchor Gated Transformer with Knowledge Distillation
(MAGTKD) for the ERC task. Specifically, prompt learning is employed to enhance
textual modality representations, while knowledge distillation is utilized to
strengthen representations of weaker modalities. Furthermore, we introduce a
multi-modal anchor gated transformer to effectively integrate utterance-level
representations across modalities. Extensive experiments on the IEMOCAP and
MELD datasets demonstrate the effectiveness of knowledge distillation in
enhancing modality representations and achieve state-of-the-art performance in
emotion recognition. Our code is available at:
https://github.com/JieLi-dd/MAGTKD.

</details>


### [266] [PARALLELPROMPT: Extracting Parallelism from Large Language Model Queries](https://arxiv.org/abs/2506.18728)
*Steven Kolawole,Keshav Santhanam,Virginia Smith,Pratiksha Thaker*

Main category: cs.LG

TL;DR: 提出PARALLELPROMPT基准，测量用户提示内并行性，超75%数据集可解析，部分任务有5倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务系统未充分利用用户提示潜在语义并行性，为挖掘并行性以降低延迟。

Method: 构建含超37000个真实提示的数据集，用LLM辅助提示和规则多语言验证提取模式，提供执行套件对比串行和并行策略。

Result: 超75%策划数据集可成功解析内查询并行性，部分任务有5倍加速，质量损失小。

Conclusion: 发布基准、流程和套件，为研究LLM服务管道结构感知执行提供标准化测试平台。

Abstract: LLM serving systems typically treat user prompts as monolithic inputs,
optimizing inference through decoding tricks or inter-query batching. However,
many real-world prompts contain latent semantic parallelism--decomposable
structures where subtasks can be executed independently to reduce latency while
preserving meaning. We introduce PARALLELPROMPT, the first benchmark for
measuring intra-query parallelism in natural user prompts. Our dataset
comprises over 37,000 real-world prompts from public LLM chat logs, each
annotated with a structured schema capturing task templates, shared context,
and iteration inputs. These schemas are extracted using LLM-assisted prompting
with rule-based multilingual validation. To evaluate the benefits of
decomposition, we provide an execution suite that benchmarks serial vs.
parallel strategies, measuring latency, structural adherence, and semantic
fidelity. Our results show that intra-query parallelism can be successfully
parsed in over 75% of curated datasets, unlocking up to 5x speedups on tasks
like translation, comprehension, and comparative analysis, with minimal quality
degradation. By releasing this benchmark, curation pipeline, and evaluation
suite, we provide the first standardized testbed for studying structure-aware
execution in LLM serving pipelines.

</details>


### [267] [Towards Group Fairness with Multiple Sensitive Attributes in Federated Foundation Models](https://arxiv.org/abs/2506.18732)
*Yuning Yang,Han Yu,Tianrun Gao,Xiaodong Xu,Guangyu Wang*

Main category: cs.LG

TL;DR: 本文首次对联邦基础模型（FFM）中跨多种敏感属性的群体公平性进行因果分析，扩展FFM结构权衡多属性，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 基础模型与联邦学习深度融合在敏感领域很重要，但现有研究多关注单一敏感属性公平性，无法提供多属性依赖的可解释性，本文旨在解决该问题。

Method: 扩展FFM结构同时权衡多个敏感属性，通过因果发现和推理量化群体公平背后的因果效应。

Result: 广泛的实验验证了方法的有效性。

Conclusion: 该方法为构建可信和公平的FFM系统提供了可解释性见解。

Abstract: The deep integration of foundation models (FM) with federated learning (FL)
enhances personalization and scalability for diverse downstream tasks, making
it crucial in sensitive domains like healthcare. Achieving group fairness has
become an increasingly prominent issue in the era of federated foundation
models (FFMs), since biases in sensitive attributes might lead to inequitable
treatment for under-represented demographic groups. Existing studies mostly
focus on achieving fairness with respect to a single sensitive attribute. This
renders them unable to provide clear interpretability of dependencies among
multiple sensitive attributes which is required to achieve group fairness. Our
paper takes the first attempt towards a causal analysis of the relationship
between group fairness across various sensitive attributes in the FFM. We
extend the FFM structure to trade off multiple sensitive attributes
simultaneously and quantify the causal effect behind the group fairness through
causal discovery and inference. Extensive experiments validate its
effectiveness, offering insights into interpretability towards building
trustworthy and fair FFM systems.

</details>


### [268] [On the Existence of Universal Simulators of Attention](https://arxiv.org/abs/2506.18739)
*Debanjan Dutta,Faizanuddin Ansari,Anish Chakrabarty,Swagatam Das*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Prior work on the learnability of transformers has established its capacity
to approximate specific algorithmic patterns through training under restrictive
architectural assumptions. Fundamentally, these arguments remain data-driven
and therefore can only provide a probabilistic guarantee. Expressivity, on the
contrary, has theoretically been explored to address the problems
\emph{computable} by such architecture. These results proved the
Turing-completeness of transformers, investigated bounds focused on circuit
complexity, and formal logic. Being at the crossroad between learnability and
expressivity, the question remains: \emph{can transformer architectures exactly
simulate an arbitrary attention mechanism, or in particular, the underlying
operations?} In this study, we investigate the transformer encoder's ability to
simulate a vanilla attention mechanism. By constructing a universal simulator
$\mathcal{U}$ composed of transformer encoders, we present algorithmic
solutions to identically replicate attention outputs and the underlying
elementary matrix and activation operations via RASP, a formal framework for
transformer computation. Our proofs, for the first time, show the existence of
an algorithmically achievable data-agnostic solution, previously known to be
approximated only by learning.

</details>


### [269] [Experimenting, Fast and Slow: Bayesian Optimization of Long-term Outcomes with Online Experiments](https://arxiv.org/abs/2506.18744)
*Qing Feng,Samuel Dalton,Benjamin Letham,Maximilian Balandat,Eytan Bakshy*

Main category: cs.LG

TL;DR: 提出结合快速实验和离线代理与长期慢速实验的方法，在短时间内对大动作空间进行顺序贝叶斯优化。


<details>
  <summary>Details</summary>
Motivation: 决策者希望优化系统更改的长期处理效果，但顺序实验策略时间过长，短期测量可能有误导性。

Method: 将快速实验（如仅运行几小时或几天的有偏实验）和/或离线代理（如离策略评估）与长期慢速实验相结合，进行顺序贝叶斯优化。

Result: 未提及具体结果。

Conclusion: 未提及明确结论，但提出的方法有望在短时间内对大动作空间进行优化。

Abstract: Online experiments in internet systems, also known as A/B tests, are used for
a wide range of system tuning problems, such as optimizing recommender system
ranking policies and learning adaptive streaming controllers. Decision-makers
generally wish to optimize for long-term treatment effects of the system
changes, which often requires running experiments for a long time as short-term
measurements can be misleading due to non-stationarity in treatment effects
over time. The sequential experimentation strategies--which typically involve
several iterations--can be prohibitively long in such cases. We describe a
novel approach that combines fast experiments (e.g., biased experiments run
only for a few hours or days) and/or offline proxies (e.g., off-policy
evaluation) with long-running, slow experiments to perform sequential, Bayesian
optimization over large action spaces in a short amount of time.

</details>


### [270] [ContinualFlow: Learning and Unlearning with Neural Flow Matching](https://arxiv.org/abs/2506.18747)
*Lorenzo Simone,Davide Bacciu,Shuangge Ma*

Main category: cs.LG

TL;DR: 提出ContinualFlow框架，通过流匹配实现生成模型定向遗忘，借助基于能量的重加权损失，无需从头训练。


<details>
  <summary>Details</summary>
Motivation: 解决生成模型定向遗忘问题，避免从头训练和直接访问待遗忘样本。

Method: 利用基于能量的重加权损失，依靠能量代理引导遗忘过程，诱导与流匹配等效的梯度。

Result: 在2D和图像领域实验验证框架，有可解释可视化和定量评估支持。

Conclusion: ContinualFlow框架能有效实现生成模型的定向遗忘。

Abstract: We introduce ContinualFlow, a principled framework for targeted unlearning in
generative models via Flow Matching. Our method leverages an energy-based
reweighting loss to softly subtract undesired regions of the data distribution
without retraining from scratch or requiring direct access to the samples to be
unlearned. Instead, it relies on energy-based proxies to guide the unlearning
process. We prove that this induces gradients equivalent to Flow Matching
toward a soft mass-subtracted target, and validate the framework through
experiments on 2D and image domains, supported by interpretable visualizations
and quantitative evaluations.

</details>


### [271] [Sensitivity Analysis of Image Classification Models using Generalized Polynomial Chaos](https://arxiv.org/abs/2506.18751)
*Lukas Bahr,Lucas Poßner,Konstantin Weise,Sophie Gröger,Rüdiger Daub*

Main category: cs.LG

TL;DR: 研究预测质量的图像分类模型敏感性，用随机变量建模输入分布域偏移，用Sobol指数量化影响并通过案例验证。


<details>
  <summary>Details</summary>
Motivation: 生产中数据驱动的预测质量方法中，图像分类的ML模型面临模型、数据和域偏移带来的不确定性，导致分类模型输出过度自信，需分析输入参数对输出的相对影响。

Method: 用随机变量对输入的分布域偏移进行建模，通过广义多项式混沌（GPC）计算Sobol指数来量化其对模型输出的影响。

Result: 通过焊接缺陷分类问题及宝马集团生产设施中使用的标志分类模型的案例研究验证了该方法。

Conclusion: 所提出的建模和量化方法在分析图像分类模型敏感性上是有效的。

Abstract: Integrating advanced communication protocols in production has accelerated
the adoption of data-driven predictive quality methods, notably machine
learning (ML) models. However, ML models in image classification often face
significant uncertainties arising from model, data, and domain shifts. These
uncertainties lead to overconfidence in the classification model's output. To
better understand these models, sensitivity analysis can help to analyze the
relative influence of input parameters on the output. This work investigates
the sensitivity of image classification models used for predictive quality. We
propose modeling the distributional domain shifts of inputs with random
variables and quantifying their impact on the model's outputs using Sobol
indices computed via generalized polynomial chaos (GPC). This approach is
validated through a case study involving a welding defect classification
problem, utilizing a fine-tuned ResNet18 model and an emblem classification
model used in BMW Group production facilities.

</details>


### [272] [Neural Total Variation Distance Estimators for Changepoint Detection in News Data](https://arxiv.org/abs/2506.18764)
*Csaba Zsolnai,Niels Lörch,Julian Arnold*

Main category: cs.LG

TL;DR: 本文利用神经网络和学习-混淆方案在新闻数据中进行变点检测，在合成和真实数据集上验证有效性，对多领域有价值。


<details>
  <summary>Details</summary>
Motivation: 检测公共话语响应重大事件的转变对理解社会动态至关重要，但现实数据特点使变点检测有挑战。

Method: 利用神经网络，引入学习-混淆方案，训练分类器区分不同时期文章，用分类准确率估计内容分布的总变差距离以确定变点。

Result: 在合成数据集和《卫报》真实数据上有效，成功识别9·11、新冠疫情和总统选举等重大历史事件。

Conclusion: 该方法所需领域知识少，能自主发现公共话语重大转变，提供内容变化定量衡量，对新闻、政策分析和危机监测有价值。

Abstract: Detecting when public discourse shifts in response to major events is crucial
for understanding societal dynamics. Real-world data is high-dimensional,
sparse, and noisy, making changepoint detection in this domain a challenging
endeavor. In this paper, we leverage neural networks for changepoint detection
in news data, introducing a method based on the so-called learning-by-confusion
scheme, which was originally developed for detecting phase transitions in
physical systems. We train classifiers to distinguish between articles from
different time periods. The resulting classification accuracy is used to
estimate the total variation distance between underlying content distributions,
where significant distances highlight changepoints. We demonstrate the
effectiveness of this method on both synthetic datasets and real-world data
from The Guardian newspaper, successfully identifying major historical events
including 9/11, the COVID-19 pandemic, and presidential elections. Our approach
requires minimal domain knowledge, can autonomously discover significant shifts
in public discourse, and yields a quantitative measure of change in content,
making it valuable for journalism, policy analysis, and crisis monitoring.

</details>


### [273] [Shift Happens: Mixture of Experts based Continual Adaptation in Federated Learning](https://arxiv.org/abs/2506.18789)
*Rahul Atul Bhope,K. R. Jayaram,Praveen Venkateswaran,Nalini Venkatasubramanian*

Main category: cs.LG

TL;DR: 本文提出ShiftEx框架解决流式联邦学习环境中协变量和标签偏移问题，实验显示相比基线有精度提升和更快适应速度，提供可扩展、隐私保护的中间件解决方案。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在客户端数据分布动态变化的现实场景面临挑战，非平稳数据分布会降低模型性能，需要自适应中间件解决方案。

Method: 引入ShiftEx，一个基于专家混合的框架，使用最大均值差异检测协变量偏移，动态创建和训练专门的全局模型，采用潜在记忆机制复用专家，并实现基于设施选址的优化。

Result: 在不同偏移场景下，相比最先进的联邦学习基线，精度提高5.5 - 12.9个百分点，适应速度快22 - 95%。

Conclusion: 该方法为非平稳现实条件下的联邦学习系统提供了可扩展、隐私保护的中间件解决方案，同时最小化通信和计算开销。

Abstract: Federated Learning (FL) enables collaborative model training across
decentralized clients without sharing raw data, yet faces significant
challenges in real-world settings where client data distributions evolve
dynamically over time. This paper tackles the critical problem of covariate and
label shifts in streaming FL environments, where non-stationary data
distributions degrade model performance and require adaptive middleware
solutions. We introduce ShiftEx, a shift-aware mixture of experts framework
that dynamically creates and trains specialized global models in response to
detected distribution shifts using Maximum Mean Discrepancy for covariate
shifts. The framework employs a latent memory mechanism for expert reuse and
implements facility location-based optimization to jointly minimize covariate
mismatch, expert creation costs, and label imbalance. Through theoretical
analysis and comprehensive experiments on benchmark datasets, we demonstrate
5.5-12.9 percentage point accuracy improvements and 22-95 % faster adaptation
compared to state-of-the-art FL baselines across diverse shift scenarios. The
proposed approach offers a scalable, privacy-preserving middleware solution for
FL systems operating in non-stationary, real-world conditions while minimizing
communication and computational overhead.

</details>


### [274] [A Multi-view Divergence-Convergence Feature Augmentation Framework for Drug-related Microbes Prediction](https://arxiv.org/abs/2506.18797)
*Xin An,Ruijie Li,Qiao Ning,Shikai Guo,Hui Li,Qian Ma*

Main category: cs.LG

TL;DR: 提出多视图DCFA_DMP框架预测药物 - 微生物关联，实验证明其有效性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有方法孤立分析药物与微生物关联和相似性，缺乏视图优化与多视图特征融合，需更好学习和整合信息。

Method: 提出DCFA_DMP框架，发散阶段用对抗学习优化特征空间，收敛阶段用双向协同注意力机制融合特征，交替应用Transformer图学习。

Result: DCFA_DMP在预测药物 - 微生物关联上表现出色，冷启动实验也证明对新药和新微生物关联预测有效。

Conclusion: DCFA_DMP稳定可靠，能有效预测潜在药物 - 微生物关联。

Abstract: In the study of drug function and precision medicine, identifying new
drug-microbe associations is crucial. However, current methods isolate
association and similarity analysis of drug and microbe, lacking effective
inter-view optimization and coordinated multi-view feature fusion. In our
study, a multi-view Divergence-Convergence Feature Augmentation framework for
Drug-related Microbes Prediction (DCFA_DMP) is proposed, to better learn and
integrate association information and similarity information. In the divergence
phase, DCFA_DMP strengthens the complementarity and diversity between
heterogeneous information and similarity information by performing Adversarial
Learning method between the association network view and different similarity
views, optimizing the feature space. In the convergence phase, a novel
Bidirectional Synergistic Attention Mechanism is proposed to deeply synergize
the complementary features between different views, achieving a deep fusion of
the feature space. Moreover, Transformer graph learning is alternately applied
on the drug-microbe heterogeneous graph, enabling each drug or microbe node to
focus on the most relevant nodes. Numerous experiments demonstrate DCFA_DMP's
significant performance in predicting drug-microbe associations. It also proves
effectiveness in predicting associations for new drugs and microbes in cold
start experiments, further confirming its stability and reliability in
predicting potential drug-microbe associations.

</details>


### [275] [Offline Goal-Conditioned Reinforcement Learning with Projective Quasimetric Planning](https://arxiv.org/abs/2506.18847)
*Anthony Kobanda,Waris Radji,Mathieu Petitbois,Odalric-Ambrym Maillard,Rémy Portelas*

Main category: cs.LG

TL;DR: 提出Projective Quasimetric Planning (ProQ)框架解决离线目标条件强化学习在长视野任务中的挑战，在导航基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 离线目标条件强化学习在长视野任务扩展面临价值估计误差累积的挑战，需要解决方法。

Method: 引入ProQ框架，学习不对称距离，将其用作排斥能量使关键点均匀分布，作为结构化方向成本引导到近端子目标，结合拉格朗日分布外检测器确保关键点在可达区域。

Result: 统一度量学习、关键点覆盖和目标条件控制，产生有意义的子目标，在多种导航基准测试中稳健驱动长视野目标达成。

Conclusion: ProQ框架能有效解决离线目标条件强化学习在长视野任务中的问题。

Abstract: Offline Goal-Conditioned Reinforcement Learning seeks to train agents to
reach specified goals from previously collected trajectories. Scaling that
promises to long-horizon tasks remains challenging, notably due to compounding
value-estimation errors. Principled geometric offers a potential solution to
address these issues. Following this insight, we introduce Projective
Quasimetric Planning (ProQ), a compositional framework that learns an
asymmetric distance and then repurposes it, firstly as a repulsive energy
forcing a sparse set of keypoints to uniformly spread over the learned latent
space, and secondly as a structured directional cost guiding towards proximal
sub-goals. In particular, ProQ couples this geometry with a Lagrangian
out-of-distribution detector to ensure the learned keypoints stay within
reachable areas. By unifying metric learning, keypoint coverage, and
goal-conditioned control, our approach produces meaningful sub-goals and
robustly drives long-horizon goal-reaching on diverse a navigation benchmarks.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [276] [Spiffy: Efficient Implementation of CoLaNET for Raspberry Pi](https://arxiv.org/abs/2506.18306)
*Andrey Derzhavin,Denis Larionov*

Main category: cs.NE

TL;DR: 提出轻量级软件方法运行脉冲神经网络，在树莓派用MNIST数据集验证，准确率92%且代码开源


<details>
  <summary>Details</summary>
Motivation: 在不依赖专用硬件和框架的情况下运行脉冲神经网络

Method: 在Rust中实现特定SNN架构CoLaNET并针对通用计算平台优化

Result: 实现的Spiffy在树莓派上使用MNIST数据集达到92%准确率，训练步长0.9ms，推理步长0.45ms

Conclusion: 该轻量级软件方法有效可行，代码开源可进一步使用

Abstract: This paper presents a lightweight software-based approach for running spiking
neural networks (SNNs) without relying on specialized neuromorphic hardware or
frameworks. Instead, we implement a specific SNN architecture (CoLaNET) in Rust
and optimize it for common computing platforms. As a case study, we demonstrate
our implementation, called Spiffy, on a Raspberry Pi using the MNIST dataset.
Spiffy achieves 92% accuracy with low latency - just 0.9 ms per training step
and 0.45 ms per inference step. The code is open-source.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [277] [Challenges and Practices in Quantum Software Testing and Debugging: Insights from Practitioners](https://arxiv.org/abs/2506.17306)
*Jake Zappin,Trevor Stalnaker,Oscar Chaparro,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 调查26位量子软件开发人员，了解测试和调试实践，发现需更好的测试和调试工具。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算从理论走向实践，开发人员面临经典软件开发中不存在的问题，为了解当前实践情况开展研究。

Method: 对26位来自学术界和行业的量子软件开发人员进行调查，并进行后续访谈，聚焦测试、调试和常见挑战。

Result: 所有参与者都进行测试，常用单元、回归和验收测试，但仅31%使用量子特定测试工具；调试依赖经典策略，扩展性不佳；最常见的错误源是经典问题。

Conclusion: 迫切需要更好的测试和调试工具，并更无缝地集成到量子开发人员的工作流程中。

Abstract: Quantum software engineering is an emerging discipline with distinct
challenges, particularly in testing and debugging. As quantum computing
transitions from theory to implementation, developers face issues not present
in classical software development, such as probabilistic execution, limited
observability, shallow abstractions, and low awareness of quantum-specific
tools. To better understand current practices, we surveyed 26 quantum software
developers from academia and industry and conducted follow-up interviews
focused on testing, debugging, and recurring challenges. All participants
reported engaging in testing, with unit testing (88%), regression testing
(54%), and acceptance testing (54%) being the most common. However, only 31%
reported using quantum-specific testing tools, relying instead on manual
methods. Debugging practices were similarly grounded in classical strategies,
such as print statements, circuit visualizations, and simulators, which
respondents noted do not scale well. The most frequently cited sources of bugs
were classical in nature-library updates (81%), developer mistakes (68%), and
compatibility issues (62%)-often worsened by limited abstraction in existing
SDKs. These findings highlight the urgent need for better-aligned testing and
debugging tools, integrated more seamlessly into the workflows of quantum
developers. We present these results in detail and offer actionable
recommendations grounded in the real-world needs of practitioners.

</details>


### [278] [An Expert Survey on Models and Digital Twins](https://arxiv.org/abs/2506.17313)
*Jonathan Reif,Daniel Dittler,Milapji Singh Gill,Tamás Farkas,Valentin Stegmaier,Felix Gehlhoff,Tobias Kleinert,Michael Weyrich*

Main category: cs.SE

TL;DR: 本文通过专家调查分析数字孪生中集成数字模型的挑战，指出缺少标准接口等问题及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 获取行业对数字孪生中集成数字模型的挑战和研究需求的看法。

Method: 在多个应用领域开展专家调查。

Result: 发现存在缺少标准接口、手动适配工作量大、跨生命周期阶段模型重用支持有限等问题。

Conclusion: 强调未来需在自动模型组合和基于语义的互操作性方面开展研究。

Abstract: Digital Twins (DTs) are becoming increasingly vital for future industrial
applications, enhancing monitoring, control, and optimization of physical
assets. This enhancement is made possible by integrating various Digital Models
(DMs) within DTs, which must interoperate to represent different system aspects
and fulfill diverse application purposes. However, industry perspectives on the
challenges and research needs for integrating these models are rarely obtained.
Thus, this study conducts an expert survey across multiple application domains
to identify and analyze the challenges in utilizing diverse DMs within DTs. The
results reveal missing standardized interfaces, high manual adaptation effort,
and limited support for model reuse across lifecycle phases, highlighting
future research needs in automated model composition and semantics-based
interoperability.

</details>


### [279] [Large Language Models for Spreadsheets: Benchmarking Progress and Evaluating Performance with FLARE](https://arxiv.org/abs/2506.17330)
*Simon Thorne*

Main category: cs.SE

TL;DR: 研究提出基准框架评估大语言模型在电子表格任务中的表现，发现其在复杂任务存在局限，并推出新基准FLARE。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在电子表格相关任务的有效性未被充分探索，需要评估其在电子表格任务中的表现。

Method: 引入综合基准框架，涵盖从基本公式创建到复杂现实场景的任务来评估大语言模型。

Result: 大语言模型在简单任务表现较好，但在复杂多步操作常出错，产生看似合理却错误的输出。

Conclusion: 当前大语言模型处理需精确逻辑推理的电子表格任务存在局限，需在架构中集成符号推理能力，并推出新基准FLARE。

Abstract: Large Language Models (LLMs) have demonstrated some significant capabilities
across various domains; however, their effectiveness in spreadsheet related
tasks remains underexplored. This study introduces a foundation for a
comprehensive benchmark framework to evaluate the performance of leading LLMs
in executing spreadsheet functions, formula generation and data manipulation
tasks. The benchmark encompasses tasks ranging from basic formula creation to
complex, real world spreadsheet scenarios. Our findings reveal that while LLMs
exhibit proficiency in straightforward tasks, they often falter in complex,
multi step operations, frequently producing plausible yet incorrect outputs.
These results underscore the limitations of current LLMs in handling
spreadsheet tasks that require precise logical reasoning and highlight the need
for integrating symbolic reasoning capabilities into LLM architectures. To
support this, we introduce FLARE (Formula Logic, Auditing, Reasoning and
Evaluation) a new benchmark for evaluating LLM performance on real-world
spreadsheet logic, auditing, and reasoning tasks.

</details>


### [280] [LMR-BENCH: Evaluating LLM Agent's Ability on Reproducing Language Modeling Research](https://arxiv.org/abs/2506.17335)
*Shuo Yan,Ruochen Li,Ziming Luo,Zimu Wang,Daoyang Li,Liqiang Jing,Kaiyu He,Peilin Wu,George Michalopoulos,Yue Zhang,Ziyang Zhang,Mian Zhang,Zhiyu Chen,Xinya Du*

Main category: cs.SE

TL;DR: 提出LMR - BENCH基准评估大语言模型代理从语言建模研究中复现代码的能力，实验表明先进模型在科学推理和代码合成方面有局限。


<details>
  <summary>Details</summary>
Motivation: 大语言模型代理在从研究论文中复现代码（特别是NLP领域）的能力未被充分探索，该任务存在独特复杂推理挑战。

Method: 构建LMR - BENCH基准，包含28个代码复现任务，对最先进大语言模型在标准提示和大语言模型代理设置下进行实验，评估单元测试准确性和代码正确性。

Result: 即使最先进的模型在科学推理和代码合成方面仍有持续的局限性。

Conclusion: 大语言模型代理在自主复现科学研究方面存在关键差距。

Abstract: Large language model (LLM) agents have demonstrated remarkable potential in
advancing scientific discovery. However, their capability in the fundamental
yet crucial task of reproducing code from research papers, especially in the
NLP domain, remains underexplored. This task includes unique complex reasoning
challenges in the intellectual synthesis of abstract concepts and the
comprehension of code repositories with interdependent files. Motivated by this
gap, we present LMR-BENCH, a benchmark designed to systematically evaluate the
capability of LLM agents on code reproduction from Language Modeling Research.
It consists of 28 code reproduction tasks derived from 23 research papers
published in top-tier NLP venues over the past five years, spanning nine
fundamental categories. Models are provided with a research paper, a code
repository containing one or more masked functions, and instructions for
implementing these functions. We conduct extensive experiments in standard
prompting and LLM agent settings with state-of-the-art LLMs, evaluating the
accuracy of unit tests and performing LLM-based evaluation of code correctness.
Experimental results reveal that even the most advanced models still exhibit
persistent limitations in scientific reasoning and code synthesis, highlighting
critical gaps in LLM agents' ability to autonomously reproduce scientific
research

</details>


### [281] [Re-Evaluating Code LLM Benchmarks Under Semantic Mutation](https://arxiv.org/abs/2506.17369)
*Zhiyuan Pan,Xing Hu,Xin Xia,Xiaohu Yang*

Main category: cs.SE

TL;DR: 研究代码基准测试中的提示敏感性，发现轻微提示变化会导致性能显著变化及排名不一致，强调设计未来代码基准时需考虑该因素。


<details>
  <summary>Details</summary>
Motivation: 现有代码基准因依赖单一提示模板，存在提示敏感性问题，导致模型能力评估不可靠，且以往研究局限于传统NLP任务。

Method: 提出通用框架修改提示模板，在8个代码基准任务上对10个开源大语言模型进行实验，使用多种统计指标分析结果。

Result: 轻微提示变化会使性能显著改变，且会导致不同模型性能排名不一致。

Conclusion: 设计未来代码基准时需考虑提示敏感性，以确保对大语言模型能力的评估更可靠准确。

Abstract: In the era of large language models (LLMs), code benchmarks have become an
important research area in software engineering and are widely used by
practitioners. These benchmarks evaluate the performance of LLMs on specific
code-related tasks, such as code understanding and generation. A critical step
in constructing code benchmarks is the design of prompts. However, as existing
code benchmarks typically rely on a single prompt template per task, they are
prone to the issue of prompt sensitivity, where minor prompt variations could
result in substantial performance variations, leading to unreliable evaluations
of model capabilities.
  While previous studies have explored prompt sensitivity, their experimental
designs and findings are limited to traditional natural language processing
(NLP) tasks. In this paper, we present an empirical study to investigate prompt
sensitivity in code benchmarks. We first propose a general framework that
modifies prompt templates in a manner that preserves both their semantics and
their structure as much as possible. Based on the framework, we conduct
extensive experiments across eight code benchmark tasks on 10 representative
open-source LLMs, with each task featuring 100 semantically similar prompt
templates. We then analyze the evaluation results using various statistical
metrics, focusing on both absolute and relative model performance. Our findings
suggest that even slight prompt variations can lead to significant shifts in
performance. Additionally, we observe that such variations can introduce
inconsistencies in the performance rankings across different models. These
insights highlight the need for considering prompt sensitivity when designing
future code benchmarks, to ensure more reliable and accurate evaluation of LLM
capabilities.

</details>


### [282] [Breaking Single-Tester Limits: Multi-Agent LLMs for Multi-User Feature Testing](https://arxiv.org/abs/2506.17539)
*Sidong Feng,Changhao Du,Huaxiao Liu,Qingnan Wang,Zhengwei Lv,Mengfei Wang,Chunyang Chen*

Main category: cs.SE

TL;DR: 提出基于大语言模型的多智能体方法MAdroid进行应用多用户交互功能自动化测试，评估有效且能发现交互漏洞。


<details>
  <summary>Details</summary>
Motivation: 当前自动化测试方法难以应对多用户交互功能测试中及时、动态和协作性用户交互的需求。

Method: 提出MAdroid，使用用户代理（Operator）和监督代理（Coordinator和Observer）两类多智能体，各代理承担特定角色。

Result: 对41个多用户交互任务评估，82.9%任务实现，动作相似度96.8%，优于消融研究和现有基线，回归测试发现11个交互漏洞。

Conclusion: MAdroid在实际软件开发中有潜在价值。

Abstract: The growing dependence on mobile phones and their apps has made multi-user
interactive features, like chat calls, live streaming, and video conferencing,
indispensable for bridging the gaps in social connectivity caused by physical
and situational barriers. However, automating these interactive features for
testing is fraught with challenges, owing to their inherent need for timely,
dynamic, and collaborative user interactions, which current automated testing
methods inadequately address. Inspired by the concept of agents designed to
autonomously and collaboratively tackle problems, we propose MAdroid, a novel
multi-agent approach powered by the Large Language Models (LLMs) to automate
the multi-user interactive task for app feature testing. Specifically, MAdroid
employs two functional types of multi-agents: user agents (Operator) and
supervisor agents (Coordinator and Observer). Each agent takes a specific role:
the Coordinator directs the interactive task; the Operator mimics user
interactions on the device; and the Observer monitors and reviews the task
automation process. Our evaluation, which included 41 multi-user interactive
tasks, demonstrates the effectiveness of our approach, achieving 82.9% of the
tasks with 96.8% action similarity, outperforming the ablation studies and
state-of-the-art baselines. Additionally, a preliminary investigation
underscores MAdroid's practicality by helping identify 11 multi-user
interactive bugs during regression app testing, confirming its potential value
in real-world software development contexts.

</details>


### [283] [CodeMorph: Mitigating Data Leakage in Large Language Model Assessment](https://arxiv.org/abs/2506.17627)
*Hongzhou Rao,Yanjie Zhao,Wenjie Zhu,Ling Xiao,Meizhen Wang,Haoyu Wang*

Main category: cs.SE

TL;DR: 为解决代码大语言模型基准泄漏问题，提出CodeMorph方法，实验显示其能降低LLM代码完成任务准确率及代码相似度。


<details>
  <summary>Details</summary>
Motivation: 代码大语言模型存在基准泄漏问题，现有代码扰动生成新数据集方法有局限性，需新方法。

Method: 提出CodeMorph方法，包含26种语义保留转换方法迭代扰动代码，引入基于遗传算法的PESO选择算法。

Result: 应用CodeMorph后，LLM在五种编程语言代码完成任务准确率平均降24.67%，Python降幅最大；PESO优化后代码相似度平均降7.01%，最高降42.86%。

Conclusion: CodeMorph能有效缓解代码大语言模型的数据泄漏问题。

Abstract: Concerns about benchmark leakage in large language models for code (Code
LLMs) have raised issues of data contamination and inflated evaluation metrics.
The diversity and inaccessibility of many training datasets make it difficult
to prevent data leakage entirely, even with time lag strategies. Consequently,
generating new datasets through code perturbation has become essential.
However, existing methods often fail to produce complex and diverse variations,
struggle with complex cross-file dependencies, and lack support for multiple
programming languages, which limits their effectiveness in enhancing LLM
evaluations for coding tasks. To fill this gap, we propose CodeMorph, an
approach designed to support multiple programming languages while preserving
cross-file dependencies to mitigate data leakage. CodeMorph consists of two
main components that work together to enhance the perturbation process. The
first component employs 26 semantic-preserving transformation methods to
iteratively perturb code, generating diverse variations while ensuring that the
modified code remains compilable. The second component introduces a genetic
algorithm-based selection algorithm, PESO, to identify the more effective
perturbation method for each iteration by targeting lower similarity scores
between the perturbed and original code, thereby enhancing overall perturbation
effectiveness. Experimental results demonstrate that after applying CodeMorph,
the accuracy of the LLM on code completion tasks across five programming
languages decreased by an average of 24.67%, with Python showing the most
significant reduction at 45%. The similarity score of code optimized by PESO
is, on average, 7.01% lower than that of randomly perturbed code, peaking at a
reduction of 42.86%.

</details>


### [284] [Deep Learning Framework Testing via Model Mutation: How Far Are We?](https://arxiv.org/abs/2506.17638)
*Yanzhou Mu,Rong Wang,Juan Zhai,Chunrong Fang,Xiang Chen,Zhiyuan Peng,Peiran Yang,Ruixiang Qian,Shaoyu Yang,Zhenyu Chen*

Main category: cs.SE

TL;DR: 本文重新审视现有基于突变的深度学习框架测试方法的缺陷检测能力，分析影响其有效性的因素，提出优化策略并发现新缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有基于突变的测试方法存在未定制、有效性有限、误报率高、开发者忽视检测出的缺陷等问题，需研究其检测经开发者认证的重要缺陷的有效性。

Method: 收集三个流行框架的缺陷报告并根据开发者评级分类构建数据集，进行深入分析，提出优化策略。

Result: 提出优化策略后发现7个新缺陷，4个被开发者确认为高优先级问题，3个已解决；在23个模型中共发现39个独特缺陷，31个得到开发者确认，8个已修复。

Conclusion: 现有基于突变的测试方法有改进空间，提出的优化策略能有效检测深度学习框架中的重要缺陷。

Abstract: Deep Learning (DL) frameworks are a fundamental component of DL development.
Therefore, the detection of DL framework defects is important and challenging.
As one of the most widely adopted DL testing techniques, model mutation has
recently gained significant attention. In this study, we revisit the defect
detection ability of existing mutation-based testing methods and investigate
the factors that influence their effectiveness. To begin with, we reviewed
existing methods and observed that many of them mutate DL models (e.g.,
changing their parameters) without any customization, ignoring the unique
challenges in framework testing. Another issue with these methods is their
limited effectiveness, characterized by a high rate of false positives caused
by illegal mutations arising from the use of generic, non-customized mutation
operators. Moreover, we tracked the defects identified by these methods and
discovered that most of them were ignored by developers. Motivated by these
observations, we investigate the effectiveness of existing mutation-based
testing methods in detecting important defects that have been authenticated by
framework developers. We begin by collecting defect reports from three popular
frameworks and classifying them based on framework developers' ratings to build
a comprehensive dataset. We then perform an in-depth analysis to uncover
valuable insights. Based on our findings, we propose optimization strategies to
address the shortcomings of existing approaches. Following these optimizations,
we identified seven new defects, four of which were confirmed by developers as
high-priority issues, with three resolved. In summary, we identified 39 unique
defects across just 23 models, of which 31 were confirmed by developers, and
eight have been fixed.

</details>


### [285] [May the Feedback Be with You! Unlocking the Power of Feedback-Driven Deep Learning Framework Fuzzing via LLMs](https://arxiv.org/abs/2506.17642)
*Shaoyu Yang,Chunrong Fang,Haifeng Lin,Xiang Chen,Zhenyu Chen*

Main category: cs.SE

TL;DR: 文章提出用于深度学习框架的反馈驱动模糊测试工具FUEL，检测到多个框架的大量新漏洞，表明考虑多类型反馈及用大模型分析反馈信息有前景。


<details>
  <summary>Details</summary>
Motivation: 现有模糊测试技术未全面考虑多类型反馈，分析反馈粒度粗，基于大模型的模糊测试技术忽视其分析反馈信息的潜力，难以生成有效多样测试用例。

Method: 提出FUEL，其由分析大模型和生成大模型两个基于大模型的代理组成，分析大模型从反馈信息推断分析摘要，生成大模型根据摘要创建测试用例。

Result: FUEL为PyTorch和TensorFlow检测到104个漏洞，93个为新漏洞，47个已修复，5个被分配CVE编号。

Conclusion: 考虑多类型反馈对模糊测试性能有益，利用大模型分析反馈信息是有前景的方向。

Abstract: Artificial Intelligence (AI) Infrastructures, represented by Deep Learning
(DL) frameworks, have served as fundamental DL systems over the last decade.
However, the bugs in DL frameworks could lead to catastrophic consequences in
some critical scenarios (e.g., healthcare and autonomous driving). A simple yet
effective way to find bugs in DL frameworks is fuzz testing (Fuzzing).
Unfortunately, existing fuzzing techniques have not comprehensively considered
multiple types of feedback. Additionally, they analyze feedback in a
coarse-grained manner, such as mutating the test cases only according to
whether the coverage increases. Recently, researchers introduced Large Language
Models (LLMs) into fuzzing. However, current LLM-based fuzzing techniques only
focus on using LLMs to generate test cases while overlooking their potential to
analyze feedback information, failing to create more valid and diverse test
cases. To fill this gap, we propose FUEL to break the seal of Feedback-driven
fuzzing for DL frameworks. The backbone of FUEL comprises two LLM-based agents,
namely analysis LLM and generation LLM. Analysis LLM agent infers analysis
summaries from feedback information, while the generation LLM agent creates
tests guided by these analysis summaries. So far, FUEL has detected 104 bugs
for PyTorch and TensorFlow, with 93 confirmed as new bugs, 47 already fixed,
and 5 assigned with CVE IDs. Our work indicates that considering multiple types
of feedback is beneficial to fuzzing performance, and leveraging LLMs to
analyze feedback information is a promising direction. Our artifact is
available at https://github.com/NJU-iSE/FUEL

</details>


### [286] [Improving Compiler Bug Isolation by Leveraging Large Language Models](https://arxiv.org/abs/2506.17647)
*Yixian Qi,Jiajun Jiang,Fengjie Li,Bowen Chen,Hongyu Zhang,Junjie Chen*

Main category: cs.SE

TL;DR: 提出AutoCBI方法利用大语言模型定位编译器漏洞，在GCC和LLVM编译器上评估显示其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统自动化漏洞隔离技术因扩展性或有效性问题不适用于编译器，当前主流编译器漏洞定位技术在测试程序变异和资源消耗上有局限。

Method: 使用大语言模型总结编译器文件功能，用特定提示引导大语言模型对可疑文件排名重新排序，结合四种信息得出可疑文件排名。

Result: 在GCC/LLVM的Top - 1排名结果中，AutoCBI比RecBi、DiWi和FuseFL分别多隔离66.67%/69.23%、300%/340%和100%/57.14%的漏洞，消融研究凸显各组件重要性。

Conclusion: AutoCBI方法有效。

Abstract: Compilers play a foundational role in building reliable software systems, and
bugs within them can lead to catastrophic consequences. The compilation process
typically involves hundreds of files, making traditional automated bug
isolation techniques inapplicable due to scalability or effectiveness issues.
Current mainstream compiler bug localization techniques have limitations in
test program mutation and resource consumption. Inspired by the recent advances
of pre-trained Large Language Models (LLMs), we propose an innovative approach
named AutoCBI, which (1) uses LLMs to summarize compiler file functions and (2)
employs specialized prompts to guide LLM in reordering suspicious file
rankings. This approach leverages four types of information: the failing test
program, source file function summaries, lists of suspicious files identified
through analyzing test coverage, as well as compilation configurations with
related output messages, resulting in a refined ranking of suspicious files.
Our evaluation of AutoCBI against state-of-the-art approaches (DiWi, RecBi and
FuseFL) on 120 real-world bugs from the widely-used GCC and LLVM compilers
demonstrates its effectiveness. Specifically, AutoCBI isolates 66.67%/69.23%,
300%/340%, and 100%/57.14% more bugs than RecBi, DiWi, and FuseFL,
respectively, in the Top-1 ranked results for GCC/LLVM. Additionally, the
ablation study underscores the significance of each component in our approach.

</details>


### [287] [PAGENT: Learning to Patch Software Engineering Agents](https://arxiv.org/abs/2506.17772)
*Haoran Xue,Gias Uddin,Song Wang*

Main category: cs.SE

TL;DR: 本文对7个顶级大语言模型代码代理生成的失败补丁进行实证研究，提出失败原因分类法，设计PAGENT工具修复类型相关错误，在部分失败补丁上测试有一定修复效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型代理自动生成的补丁可能不准确，人们对失败补丁的根本原因及修复方法了解甚少。

Method: 从SWE - bench Lite数据集收集未解决问题，用GPT - 4o和手动分析检查7个代理生成的失败补丁，提出失败原因分类法；设计PAGENT工具，利用程序分析和大语言模型推理技术推断和细化补丁类型信息。

Result: 提出包含6个类别及子类别失败原因分类法；PAGENT在127个类型相关失败补丁上能修复29个。

Conclusion: 通过实证研究明确大语言模型代码代理生成失败补丁的原因分类，PAGENT工具对修复类型相关失败补丁有一定作用。

Abstract: LLM Agents produce patches automatically to resolve an issue. However, they
can generate inaccurate patches. Little is known about the root causes behind
those failed patches or how those could be fixed. This paper reports an
empirical study of the failed patches generated by seven top LLM code agents.
We collected 114 issues from the SWE-bench Lite dataset that remained
unresolved across the agents. The seven agents produced a total of 769 failed
patches for those issues, which we checked with a combination of GPT-4o and
manual analysis. We present a taxonomy of the failure reasons across the
patches. The taxonomy contains six categories, with several sub-categories
under each category. For example, a frequently observed category is the
inability of an LLM to correctly infer/produce the appropriate variable type in
the produced patch. As a first step towards addressing such type-related
errors, we designed PAGENT (Patch Agent). PAGENT utilizes program analysis
techniques like CFG creation and exploration to infer the type of information
of a patch. PAGENT does this by applying repository-level static code analysis
techniques. Then, PAGENT refines the inferred type by further utilizing an
LLM-based inference technique. We tested PAGENT on all 127 type-related failed
patches from the top three agents in our study. PAGENT could fix 29 of the 127
failed patches.

</details>


### [288] [SAVANT: Vulnerability Detection in Application Dependencies through Semantic-Guided Reachability Analysis](https://arxiv.org/abs/2506.17798)
*Wang Lingxiang,Quanzhi Fu,Wenjia Song,Gelei Deng,Yi Liu,Dan Williams,Ying Zhang*

Main category: cs.SE

TL;DR: 现有SCA工具检测Java开发中第三方库漏洞API使用效果不佳，本文提出SAVANT方法，经评估优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 现有SCA工具难以有效检测第三方库中易受攻击的API使用，导致漏洞警报不准确，给开发团队带来负担并延误安全修复。

Method: 提出SAVANT方法，结合语义预处理和基于大语言模型的上下文分析，先将源代码分割成有意义的块，再利用基于大语言模型的反射分析API使用上下文。

Result: 在55个实际应用程序上评估，SAVANT的精确率达83.8%，召回率达73.8%，准确率达69.0%，F1分数达78.5%。

Conclusion: SAVANT方法在检测Java开发中第三方库漏洞方面优于现有SCA工具。

Abstract: The integration of open-source third-party library dependencies in Java
development introduces significant security risks when these libraries contain
known vulnerabilities. Existing Software Composition Analysis (SCA) tools
struggle to effectively detect vulnerable API usage from these libraries due to
limitations in understanding API usage semantics and computational challenges
in analyzing complex codebases, leading to inaccurate vulnerability alerts that
burden development teams and delay critical security fixes.
  To address these challenges, we proposed SAVANT by leveraging two insights:
proof-of-vulnerability test cases demonstrate how vulnerabilities can be
triggered in specific contexts, and Large Language Models (LLMs) can understand
code semantics. SAVANT combines semantic preprocessing with LLM-powered context
analysis for accurate vulnerability detection. SAVANT first segments source
code into meaningful blocks while preserving semantic relationships, then
leverages LLM-based reflection to analyze API usage context and determine
actual vulnerability impacts. Our evaluation on 55 real-world applications
shows that SAVANT achieves 83.8% precision, 73.8% recall, 69.0% accuracy, and
78.5% F1-score, outperforming state-of-the-art SCA tools.

</details>


### [289] [Is Your Automated Software Engineer Trustworthy?](https://arxiv.org/abs/2506.17812)
*Noble Saji Mathews,Meiyappan Nagappan*

Main category: cs.SE

TL;DR: 提出BouncerBench基准评估LLM软件代理拒绝不当输入或输出的能力，结果显示多数模型表现不佳，该基准为构建可靠代码代理迈出第一步。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的软件工程工具在处理不确定或错误输入输出时缺乏拒绝机制，导致行为不可靠，需要评估模型拒绝不当情况的能力。

Method: 引入BouncerBench基准，针对模糊问题描述和错误代码补丁两个失败点进行评估，并实现基本的输入输出拒绝器。

Result: 多数模型在面对不明确输入或错误输出时未能拒绝行动。

Conclusion: 在LLM能在现实软件工程工作流中做出正确决策和推荐前有很大改进空间，BouncerBench为评估和构建更可靠代码代理提供开端。

Abstract: Large Language Models (LLMs) are being increasingly used in software
engineering tasks, with an increased focus on bug report resolution over the
past year. However, most proposed systems fail to properly handle uncertain or
incorrect inputs and outputs. Existing LLM-based tools and coding agents
respond to every issue and generate a patch for every case, even when the input
is vague or their own output is incorrect. There are no mechanisms in place to
abstain when confidence is low. This leads to unreliable behaviour, such as
hallucinated code changes or responses based on vague issue reports. We
introduce BouncerBench, a benchmark that evaluates whether LLM-based software
agents can refuse to act when inputs are ill-defined or refuse to respond when
their own outputs are likely to be incorrect. Unlike prior benchmarks that
implicitly incentivize models to generate responses even when uncertain,
BouncerBench aims to improve precision by targeting two overlooked failure
points: (1) vague or underspecified issue descriptions in tickets and (2)
logically or functionally incorrect code patches created by the system. It
measures whether proposed systems can distinguish actionable issues from vague
tickets and valid patches from untrustworthy ones. We also implement a basic
input and output bouncer, evaluating how well current LLMs can abstain when
needed. Our results show that most models fail to abstain from underspecified
inputs or incorrect outputs. Hence, we conclude that there is significant room
for improvement before LLMs can be trusted to make correct decisions and
recommendations in real-world software engineering workflows. BouncerBench
provides a first step toward evaluating and building more cautious, trustworthy
code agents. The replication package, dataset, and leaderboard can be found at
bouncerbench.com

</details>


### [290] [The Impact of AI-Generated Solutions on Software Architecture and Productivity: Results from a Survey Study](https://arxiv.org/abs/2506.17833)
*Giorgio Amasanti,Jasmin Jahic*

Main category: cs.SE

TL;DR: 本文通过对使用AI工具的软件从业者进行调查，发现AI工具能显著提升软件工程师生产力，但项目越复杂效益越低；在一定条件下对软件质量无显著负面影响，处理复杂问题时需架构师介入。


<details>
  <summary>Details</summary>
Motivation: 了解AI软件工具对软件工程师的生产力提升效果以及对软件长期质量的影响。

Method: 对使用AI工具的软件从业者进行调查。

Result: AI工具显著提升软件工程师生产力，项目越复杂效益越低；对软件质量无显著负面影响，但处理复杂问题时生成方案质量较低。

Conclusion: AI工具能提升生产力，但项目复杂时效益降低；对软件质量影响因问题规模而异，处理复杂问题需架构师进行问题分解和方案集成。

Abstract: AI-powered software tools are widely used to assist software engineers.
However, there is still a need to understand the productivity benefits of such
tools for software engineers. In addition to short-term benefits, there is a
question of how adopting AI-generated solutions affects the quality of software
over time (e.g., maintainability and extendability).
  To provide some insight on these questions, we conducted a survey among
software practitioners who use AI tools. Based on the data collected from our
survey, we conclude that AI tools significantly increase the productivity of
software engineers. However, the productivity benefits of using AI tools reduce
as projects become more complex. The results also show that there are no
significant negative influences of adopting AI-generated solutions on software
quality, as long as those solutions are limited to smaller code snippets.
However, when solving larger and more complex problems, AI tools generate
solutions of a lower quality, indicating the need for architects to perform
problem decomposition and solution integration.

</details>


### [291] [Software Reuse in the Generative AI Era: From Cargo Cult Towards AI Native Software Engineering](https://arxiv.org/abs/2506.17937)
*Tommi Mikkonen,Antero Taivalsaari*

Main category: cs.SE

TL;DR: 本文探讨AI辅助生成式软件复用在‘AI原生’软件工程背景下的影响，提出问题并定义研究议程。


<details>
  <summary>Details</summary>
Motivation: 软件开发范式转变，AI辅助方法替代早期软件复用实践，引出新的软件复用形式。

Method: 讨论AI辅助生成式软件复用影响、提出相关问题。

Result: 定义了初步研究议程。

Conclusion: 呼吁采取行动解决相关核心问题。

Abstract: Software development is currently under a paradigm shift in which artificial
intelligence and generative software reuse are taking the center stage in
software creation. Consequently, earlier software reuse practices and methods
are rapidly being replaced by AI-assisted approaches in which developers place
their trust on code that has been generated by artificial intelligence. This is
leading to a new form of software reuse that is conceptually not all that
different from cargo cult development. In this paper we discuss the
implications of AI-assisted generative software reuse in the context of
emerging "AI native" software engineering, bring forth relevant questions, and
define a tentative research agenda and call to action for tackling some of the
central issues associated with this approach.

</details>


### [292] [Build It Clean: Large-Scale Detection of Code Smells in Build Scripts](https://arxiv.org/abs/2506.17948)
*Mahzabin Tamanna,Yash Chandrani,Matthew Burrows,Brandon Wroblewski,Laurie Williams,Dominik Wermke*

Main category: cs.SE

TL;DR: 该研究通过对GitHub上构建脚本和问题的实证研究，采用混合方法识别构建脚本代码异味，提出减轻异味策略。


<details>
  <summary>Details</summary>
Motivation: 帮助从业者避免构建脚本中的代码异味，减少构建失败、风险和技术债务。

Method: 采用混合方法，对2000个GitHub问题进行定性分析，开发静态分析工具Sniffer分析5882个构建脚本。

Result: 识别出13类代码异味，共10895次出现，发现不同构建脚本中常见异味，且部分异味对有强关联。

Conclusion: 基于研究结果，推荐减轻构建脚本代码异味的策略以提高软件项目质量。

Abstract: Build scripts are files that automate the process of compiling source code,
managing dependencies, running tests, and packaging software into deployable
artifacts. These scripts are ubiquitous in modern software development
pipelines for streamlining testing and delivery. While developing build
scripts, practitioners may inadvertently introduce code smells. Code smells are
recurring patterns of poor coding practices that may lead to build failures or
increase risk and technical debt. The goal of this study is to aid
practitioners in avoiding code smells in build scripts through an empirical
study of build scripts and issues on GitHub. We employed a mixed-methods
approach, combining qualitative and quantitative analysis. We conducted a
qualitative analysis of 2000 build-script-related GitHub issues. Next, we
developed a static analysis tool, Sniffer, to identify code smells in 5882
build scripts of Maven, Gradle, CMake, and Make files, collected from 4877
open-source GitHub repositories. We identified 13 code smell categories, with a
total of 10,895 smell occurrences, where 3184 were in Maven, 1214 in Gradle,
337 in CMake, and 6160 in Makefiles.
  Our analysis revealed that Insecure URLs were the most prevalent code smell
in Maven build scripts, while Hardcoded Paths/URLs were commonly observed in
both Gradle and CMake scripts. Wildcard Usage emerged as the most frequent
smell in Makefiles. The co-occurrence analysis revealed strong associations
between specific smell pairs of Hardcoded Paths/URLs with Duplicates, and
Inconsistent Dependency Management with Empty or Incomplete Tags, indicating
potential underlying issues in the build script structure and maintenance
practices. Based on our findings, we recommend strategies to mitigate the
existence of code smells in build scripts to improve the efficiency,
reliability, and maintainability of software projects.

</details>


### [293] [VFArchē: A Dual-Mode Framework for Locating Vulnerable Functions in Open-Source Software](https://arxiv.org/abs/2506.18050)
*Lyuye Zhang,Jian Zhang,Kaixuan Li,Chong Wang,Chengwei Liu,Jiahui Wu,Sen Chen,Yaowen Zheng,Yang Liu*

Main category: cs.SE

TL;DR: 提出VFArch=e双模式方法定位软件中的易受攻击函数，在有无补丁场景均有效，实验效果好，能用于现实场景。


<details>
  <summary>Details</summary>
Motivation: 现有软件成分分析（SCA）中进行可达性分析需易受攻击函数（VF）信息，但现代漏洞数据库常无此信息，直接从补丁提取不总是可行，忽略补丁搜索有噪声和词法差距，需能处理有无补丁两种情况的方案。

Method: 提出VFArch=e双模式方法，适用于有或无可用补丁链接的已披露漏洞场景。

Result: 在构建的基准数据集上，在三个指标上效果显著，在有补丁和无补丁模式下平均倒数排名分别是最佳基线的1.3倍和1.9倍；在现实场景中成功定位50个最新漏洞中的43个VF，显著减少SCA工具78 - 89%的误报。

Conclusion: VFArch=e方法有效，可满足现实需求，自动定位易受攻击函数。

Abstract: Software Composition Analysis (SCA) has become pivotal in addressing
vulnerabilities inherent in software project dependencies. In particular,
reachability analysis is increasingly used in Open-Source Software (OSS)
projects to identify reachable vulnerabilities (e.g., CVEs) through call
graphs, enabling a focus on exploitable risks. Performing reachability analysis
typically requires the vulnerable function (VF) to track the call chains from
downstream applications. However, such crucial information is usually
unavailable in modern vulnerability databases like NVD. While directly
extracting VF from modified functions in vulnerability patches is intuitive,
patches are not always available. Moreover, our preliminary study shows that
over 26% of VF do not exist in the modified functions. Meanwhile, simply
ignoring patches to search vulnerable functions suffers from overwhelming
noises and lexical gaps between descriptions and source code. Given that almost
half of the vulnerabilities are equipped with patches, a holistic solution that
handles both scenarios with and without patches is required. To meet real-world
needs and automatically localize VF, we present VFArch\=e, a dual-mode approach
designed for disclosed vulnerabilities, applicable in scenarios with or without
available patch links. The experimental results of VFArch\=e on our constructed
benchmark dataset demonstrate significant efficacy regarding three metrics,
achieving 1.3x and 1.9x Mean Reciprocal Rank over the best baselines for
Patch-present and Patch-absent modes, respectively. Moreover, VFArch\=e has
proven its applicability in real-world scenarios by successfully locating VF
for 43 out of 50 latest vulnerabilities with reasonable efforts and
significantly reducing 78-89% false positives of SCA tools.

</details>


### [294] [Call Me Maybe: Enhancing JavaScript Call Graph Construction using Graph Neural Networks](https://arxiv.org/abs/2506.18191)
*Masudul Hasan Masud Bhuiyan,Gianluca De Stefano,Giancarlo Pellegrino,Cristian-Alexandru Staicu*

Main category: cs.SE

TL;DR: 本文提出GRAPHIA方法，将问题转化为全程序图的链接预测，利用图神经网络识别JavaScript缺失的调用边，经大规模评估证明能提升调用图构建的召回率。


<details>
  <summary>Details</summary>
Motivation: 现有JavaScript调用图构建算法存在不健全和不完整问题，会产生错误边并遗漏有效边，需要改进。

Method: 将问题建模为全程序图的链接预测，使用多种边类型的丰富表示，结合句法和语义边表示JavaScript程序，利用图神经网络建模代码元素间非局部关系，从现有工具的静态调用边和测试的动态边学习。

Result: 在50个流行JavaScript库上评估，GRAPHIA构建的程序图有660万条结构边和38.6万条语义边，超42%未解决情况将正确目标排在首位，72%情况排在前5，减少分析的人工工作量。

Conclusion: 基于学习的方法能提高JavaScript调用图构建的召回率，这是首次将基于GNN的链接预测应用于全多文件程序图进行过程间分析。

Abstract: Static analysis plays a key role in finding bugs, including security issues.
A critical step in static analysis is building accurate call graphs that model
function calls in a program. However, due to hard-to-analyze language features,
existing call graph construction algorithms for JavaScript are neither sound
nor complete. Prior work shows that even advanced solutions produce false edges
and miss valid ones. In this work, we assist these tools by identifying missed
call edges. Our main idea is to frame the problem as link prediction on full
program graphs, using a rich representation with multiple edge types. Our
approach, GRAPHIA, leverages recent advances in graph neural networks to model
non-local relationships between code elements. Concretely, we propose
representing JavaScript programs using a combination of syntactic- and
semantic-based edges. GRAPHIA can learn from imperfect labels, including static
call edges from existing tools and dynamic edges from tests, either from the
same or different projects. Because call graphs are sparse, standard machine
learning metrics like ROC are not suitable. Instead, we evaluate GRAPHIA by
ranking function definitions for each unresolved call site. We conduct a
large-scale evaluation on 50 popular JavaScript libraries with 163K call edges
(150K static and 13K dynamic). GRAPHIA builds program graphs with 6.6M
structural and 386K semantic edges. It ranks the correct target as the top
candidate in over 42% of unresolved cases and within the top 5 in 72% of cases,
reducing the manual effort needed for analysis. Our results show that
learning-based methods can improve the recall of JavaScript call graph
construction. To our knowledge, this is the first work to apply GNN-based link
prediction to full multi-file program graphs for interprocedural analysis.

</details>


### [295] [Managing Technical Debt in a Multidisciplinary Data Intensive Software Team: an Observational Case Study](https://arxiv.org/abs/2506.18219)
*Ulrike M. Graetsch,Rashina Hoda,Hourieh Khalazjadeh,Mojtaba Shahin,John Grundy*

Main category: cs.SE

TL;DR: 研究以探索性观察案例研究分析多学科数据密集型团队技术债务管理实践，识别债务类型、管理方式等，建议新实施模式和工具支持。


<details>
  <summary>Details</summary>
Motivation: 数据密集型解决方案投资增加，相关技术债务增长，但多学科团队开发系统和管理技术债务知识有限，需相关实践见解。

Method: 采用探索性观察案例研究，用社会技术扎根理论分析数据，形成概念和类别。

Result: 识别团队处理的技术债务，如技术数据组件债务和管道债务，解释团队管理、评估债务，考虑的处理方式及如何适应冲刺容量约束。

Conclusion: 研究结果与现有技术债务和管理分类法对齐，讨论影响，强调多学科团队需新实施模式和工具支持。

Abstract: Context: There is an increase in the investment and development of
data-intensive (DI) solutions, systems that manage large amounts of data.
Without careful management, this growing investment will also grow associated
technical debt (TD). Delivery of DI solutions requires a multidisciplinary
skill set, but there is limited knowledge about how multidisciplinary teams
develop DI systems and manage TD.
  Objective: This research contributes empirical, practice based insights about
multidisciplinary DI team TD management practices.
  Method: This research was conducted as an exploratory observation case study.
We used socio-technical grounded theory (STGT) for data analysis to develop
concepts and categories that articulate TD and TDs debt management practices.
  Results: We identify TD that the DI team deals with, in particular technical
data components debt and pipeline debt. We explain how the team manages the TD,
assesses TD, what TD treatments they consider and how they implement TD
treatments to fit sprint capacity constraints.
  Conclusion: We align our findings to existing TD and TDM taxonomies, discuss
their implications and highlight the need for new implementation patterns and
tool support for multidisciplinary DI teams.

</details>


### [296] [Tu(r)ning AI Green: Exploring Energy Efficiency Cascading with Orthogonal Optimizations](https://arxiv.org/abs/2506.18289)
*Saurabhsingh Rajput,Mootez Saad,Tushar Sharma*

Main category: cs.SE

TL;DR: 本文强调将能源效率作为计算密集型管道的基础设计考虑因素，通过对AI管道五个阶段的策略性选择可实现级联效率，正交组合可大幅降低能耗并保留较高F1分数，提供了可持续AI的可行框架。


<details>
  <summary>Details</summary>
Motivation: AI指数级增长带来计算需求和能源挑战，现有优化技术多为事后、孤立应用，未考虑组合对能源效率的影响。

Method: 将能源效率作为首要设计考虑，对AI管道的五个阶段（数据、模型、训练、系统、推理）进行策略性选择。

Result: 正交组合可降低能耗达94.6%，同时保留非优化管道95.95%的原始F1分数。

Conclusion: 该精心设计的方法为平衡效率、性能和环境责任的可持续AI提供了可行框架。

Abstract: AI's exponential growth intensifies computational demands and energy
challenges. While practitioners employ various optimization techniques, that we
refer as "knobs" in this paper, to tune model efficiency, these are typically
afterthoughts and reactive ad-hoc changes applied in isolation without
understanding their combinatorial effects on energy efficiency. This paper
emphasizes on treating energy efficiency as the first-class citizen and as a
fundamental design consideration for a compute-intensive pipeline. We show that
strategic selection across five AI pipeline phases (data, model, training,
system, inference) creates cascading efficiency. Experimental validation shows
orthogonal combinations reduce energy consumption by up to $94.6$% while
preserving $95.95$% of the original F1 score of non-optimized pipelines. This
curated approach provides actionable frameworks for informed sustainable AI
that balance efficiency, performance, and environmental responsibility.

</details>


### [297] [Use Property-Based Testing to Bridge LLM Code Generation and Validation](https://arxiv.org/abs/2506.18315)
*Lehan He,Zeren Chen,Zhe Zhang,Jing Shao,Xiang Gao,Lu Sheng*

Main category: cs.SE

TL;DR: 本文提出Property - Generated Solver框架，利用基于属性的测试验证程序属性，通过两个协作的基于大语言模型的代理实现代码生成与优化，实验表明该框架在多个代码生成基准测试上比传统TDD方法有显著提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在代码生成时确保输出功能正确性是挑战，传统TDD方法因高质量测试用例稀缺或自动测试生成的缺陷而效果不佳。

Method: 引入Property - Generated Solver框架，使用基于属性的测试验证程序属性，采用生成器和测试器两个协作的基于大语言模型的代理进行代码生成和反馈优化。

Result: 在多个代码生成基准测试上，Property - Generated Solver相比传统TDD方法，pass@1有23.1%至37.3%的相对提升。

Conclusion: 将基于属性的测试作为迭代闭环范式的核心验证引擎，Property - Generated Solver能引导大语言模型生成更正确、更具通用性的代码。

Abstract: Large Language Models (LLMs) excel at code generation, but ensuring their
outputs to be functionally correct, especially in complex programming tasks, is
a persistent challenge. While traditional Test-Driven Development (TDD) offers
a path for code refinement, its efficacy with LLMs is often undermined by the
scarcity of high-quality test cases or the pitfalls of automated test
generation, including biased tests or inaccurate output predictions that can
misdirect the correction process. This paper introduces Property-Generated
Solver, a novel framework that leverages Property-Based Testing (PBT) to
validate high-level program properties or invariants, instead of relying on
specific input-output examples. These properties are often simpler to define
and verify than directly predicting exhaustive test oracles, breaking the
"cycle of self-deception" where tests might share flaws with the code they are
meant to validate. Property-Generated Solver employs two collaborative
LLM-based agents: a Generator dedicated to code generation and iterative
refinement, and a Tester that manages the PBT life-cycle and formulate
semantically rich feedback from property violations. The resulting
comprehensive and actionable feedback then guides the Generator in its
refinement efforts. By establishing PBT as the core validation engine within
this iterative, closed-loop paradigm, Property-Generated Solver provides a
robust mechanism for steering LLMs towards more correct and generalizable code.
Extensive experimental results on multiple code generation benchmarks
demonstrate that Property-Generated Solver achieves substantial pass@1
improvements, ranging from 23.1% to 37.3% relative gains over established TDD
methods.

</details>


### [298] [Predictive Analytics for Collaborators Answers, Code Quality, and Dropout on Stack Overflow](https://arxiv.org/abs/2506.18329)
*Elijah Zolduoarrati,Sherlock A. Licorish,Nigel Stanger*

Main category: cs.SE

TL;DR: 研究对21种算法在三个任务上进行评估，发现不同任务下表现最佳的算法，为研究和实践提供见解。


<details>
  <summary>Details</summary>
Motivation: 以往用Stack Overflow数据开发预测模型的研究使用的基准模型有限或选择方法随意，需对更多模型进行基准测试。

Method: 评估21种算法，采用归一化、标准化等转换方法，结合贝叶斯超参数优化和遗传算法，微调CodeBERT。

Result: 不同任务下不同算法表现最佳，如Bagging集成模型预测用户回答数R2值最高，随机梯度下降回归器等预测代码质量表现好，极端梯度提升预测用户流失F1分数最高，CodeBERT分类用户流失F1分数为0.809。

Conclusion: 对21种算法的基准测试为研究人员和从业者提供了关于合适模型和最优超参数的见解。

Abstract: Previous studies that used data from Stack Overflow to develop predictive
models often employed limited benchmarks of 3-5 models or adopted arbitrary
selection methods. Despite being insightful, their limited scope suggests the
need to benchmark more models to avoid overlooking untested algorithms. Our
study evaluates 21 algorithms across three tasks: predicting the number of
question a user is likely to answer, their code quality violations, and their
dropout status. We employed normalisation, standardisation, as well as
logarithmic and power transformations paired with Bayesian hyperparameter
optimisation and genetic algorithms. CodeBERT, a pre-trained language model for
both natural and programming languages, was fine-tuned to classify user dropout
given their posts (questions and answers) and code snippets. We found Bagging
ensemble models combined with standardisation achieved the highest R2 value
(0.821) in predicting user answers. The Stochastic Gradient Descent regressor,
followed by Bagging and Epsilon Support Vector Machine models, consistently
demonstrated superior performance to other benchmarked algorithms in predicting
user code quality across multiple quality dimensions and languages. Extreme
Gradient Boosting paired with log-transformation exhibited the highest F1-score
(0.825) in predicting user dropout. CodeBERT was able to classify user dropout
with a final F1-score of 0.809, validating the performance of Extreme Gradient
Boosting that was solely based on numerical data. Overall, our benchmarking of
21 algorithms provides multiple insights. Researchers can leverage findings
regarding the most suitable models for specific target variables, and
practitioners can utilise the identified optimal hyperparameters to reduce the
initial search space during their own hyperparameter tuning processes.

</details>


### [299] [Recipe for Discovery: A Framework for Systematic Open Source Project Identification](https://arxiv.org/abs/2506.18359)
*Juanita Gomez,Emily Lovell,Stephanie Lieggi,Alvaro A. Cardenas,James Davis*

Main category: cs.SE

TL;DR: 本文提出框架解决分布式机构系统中开源软件项目发现、分类和分析难题，以加州大学系统为例，利用GitHub API构建管道并评估多种分类策略，结果显示框架有效。


<details>
  <summary>Details</summary>
Motivation: 开源软件开发分散难追踪，相关成果因缺乏可见性和机构认知而未获认可，需解决分布式机构系统中开源软件项目的发现、分类和分析难题。

Method: 以加州大学系统为案例，使用GitHub的REST API构建管道发现相关仓库并提取元数据，提出并评估传统机器学习模型和大语言模型等多种分类策略。

Result: 框架大规模有效，发现超52000个仓库，能高精度预测机构关联。

Conclusion: 所提出的框架可有效解决分布式机构系统中开源软件项目的发现、分类和分析问题。

Abstract: Open source software development, particularly within institutions such as
universities and research laboratories, is often decentralized and difficult to
track. Despite producing highly impactful tools in science, these efforts often
go unrecognized due to a lack of visibility and institutional awareness. This
paper addresses the challenge of discovering, classifying, and analyzing open
source software projects developed across distributed institutional systems. We
present a framework for systematically identifying institutional affiliated
repositories, using the University of California (UC) system as a case study.
  Using GitHub's REST API, we build a pipeline to discover relevant
repositories and extract meaningful metadata. We then propose and evaluate
multiple classification strategies, including both traditional machine learning
models and large language models (LLMs), to distinguish affiliated projects
from unrelated repositories and generate accurate insights into the academic
open source landscape. Our results show that the framework is effective at
scale, discovering over 52,000 repositories and predicting institutional
affiliation with high accuracy.

</details>


### [300] [Tracing Errors, Constructing Fixes: Repository-Level Memory Error Repair via Typestate-Guided Context Retrieval](https://arxiv.org/abs/2506.18394)
*Xiao Cheng,Zhihao Guo,Huan Huo,Yulei Sui*

Main category: cs.SE

TL;DR: 本文介绍了利用大语言模型（LLMs）的LTFix方法来自动修复C程序内存错误，解决了LLM在内存错误修复中的两个挑战。


<details>
  <summary>Details</summary>
Motivation: C语言手动内存管理复杂，内存相关错误修复困难，传统自动程序修复方法有局限，深度学习方法有不足，因此需要新方法。

Method: 引入LTFix方法，利用有限类型状态自动机跟踪错误传播路径和上下文跟踪，采用类型状态引导的上下文检索策略为LLM提供相关信息。

Result: 未提及具体结果。

Conclusion: 未提及明确结论。

Abstract: Memory-related errors in C programming continue to pose significant
challenges in software development, primarily due to the complexities of manual
memory management inherent in the language. These errors frequently serve as
vectors for severe vulnerabilities, while their repair requires extensive
knowledge of program logic and C's memory model. Automated Program Repair (APR)
has emerged as a critical research area to address these challenges.
Traditional APR approaches rely on expert-designed strategies and predefined
templates, which are labor-intensive and constrained by the effectiveness of
manual specifications. Deep learning techniques offer a promising alternative
by automatically extracting repair patterns, but they require substantial
training datasets and often lack interpretability.
  This paper introduces LTFix, a novel approach that harnesses the potential of
Large Language Models (LLMs) for automated memory error repair, especially for
complex repository-level errors that span multiple functions and files. We
address two fundamental challenges in LLM-based memory error repair: a limited
understanding of interprocedural memory management patterns and context window
limitations for repository-wide analysis. Our approach utilizes a finite
typestate automaton to guide the tracking of error-propagation paths and
context trace, capturing both spatial (memory states) and temporal (execution
history) dimensions of error behavior. This typestate-guided context retrieval
strategy provides the LLM with concise yet semantically rich information
relevant to erroneous memory management, effectively addressing the token
limitation of LLMs.

</details>


### [301] [Your Token Becomes Worthless: Unveiling Rug Pull Schemes in Crypto Token via Code-and-Transaction Fusion Analysis](https://arxiv.org/abs/2506.18398)
*Hao Wu,Haijun Wang,Shangwang Li,Yin Wu,Ming Fan,Wuxia Jin,Yitao Zhao,Ting Liu*

Main category: cs.SE

TL;DR: 论文提出RPhunter技术结合代码与交易信息检测Rug Pull骗局，在自建数据集和现实场景中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法仅关注代码风险或交易数据，难以有效检测复杂的Rug Pull骗局。

Method: RPhunter通过建立规则和流分析提取代码风险信息构建SRCG，将交易活动构建为TFBG，用图神经网络提取特征并通过注意力融合模型增强检测。

Result: 在自建数据集上精确率95.3%、召回率93.8%、F1分数94.5%；在现实场景中识别4801个Rug Pull代币，精确率91%。

Conclusion: RPhunter在Rug Pull检测方面性能优于现有方法，能有效应对复杂骗局。

Abstract: Rug pull scams have emerged as a persistent threat to cryptocurrency, causing
significant financial losses. A typical scenario involves scammers deploying
honeypot contracts to attract investments, restricting token sales, and
draining the funds, which leaves investors with worthless tokens. Current
methods either rely on predefined patterns to detect code risks or utilize
statistical transaction data to train detection models. However, real-world Rug
Pull schemes often involve a complex interplay between malicious code and
suspicious transaction behaviors. These methods, which solely focus on one
aspect, fall short in detecting such schemes effectively.
  In this paper, we propose RPhunter, a novel technique that integrates code
and transaction for Rug Pull detection. First, RPhunter establishes declarative
rules and performs flow analysis to extract code risk information, further
constructing a semantic risk code graph (SRCG). Meanwhile, to leverage
transaction information, RPhunter formulates dynamic token transaction
activities as a token flow behavior graph (TFBG) in which nodes and edges are
characterized from network structure and market manipulation perspectives.
Finally, RPhunter employs graph neural networks to extract complementary
features from SRCG and TFBG, integrating them through an attention fusion model
to enhance the detection of Rug Pull. We manually analyzed 645 Rug Pull
incidents from code and transaction aspects and constructed a ground-truth
dataset. We evaluated RPhunter on our dataset, achieving a precision of 95.3%,
a recall of 93.8% and an F1 score of 94.5%, which highlights superior
performance compared to existing state-of-the-art methods. Furthermore, when
applied to the real-world scenarios, RPhunter has identified 4801 Rug Pull
tokens, achieving a precision of 91%.

</details>


### [302] [The Debugging Decay Index: Rethinking Debugging Strategies for Code LLMs](https://arxiv.org/abs/2506.18403)
*Muntasir Adnan,Carlos C. N. Kuhn*

Main category: cs.SE

TL;DR: AI调试效果呈指数衰减，引入调试衰减指数（DDI）框架，战略重启方法可恢复调试效果，DDI揭示现有AI调试局限并提供量化框架。


<details>
  <summary>Details</summary>
Motivation: 迭代调试对实际代码生成系统至关重要，但AI调试效果快速衰减，需解决此问题。

Method: 引入调试衰减指数（DDI）框架量化调试失效点和预测干预点，采用战略重启方法在调试过程战略点从利用转向探索。

Result: 战略重启方法能在适时干预时恢复调试效果。

Conclusion: DDI揭示当前AI调试的根本局限，为优化迭代代码生成策略提供首个量化框架。

Abstract: The effectiveness of AI debugging follows a predictable exponential decay
pattern; most models lose 60-80% of their debugging capability within just 2-3
attempts, despite iterative debugging being a critical capability for practical
code generation systems. We introduce the Debugging Decay Index (DDI), a
mathematical framework that quantifies when debugging becomes ineffective and
predicts intervention points. Our strategic fresh start approach shifts from
exploitation to exploration at strategic points in the debugging process,
demonstrating that well-timed interventions can rescue the effectiveness of
debugging. DDI reveals a fundamental limitation in current AI debugging and
provides the first quantitative framework for optimising iterative code
generation strategies.

</details>


### [303] [ModeliHub: A Web-based, Federated Analytics Platform for Modelica-centric, Model-based Systems Engineering](https://arxiv.org/abs/2506.18790)
*Mohamad Omar Nachawati*

Main category: cs.SE

TL;DR: 本文介绍了基于Web的联邦分析平台ModeliHub，它用于基于Modelica的系统工程，具有创新架构和实时仿真环境，采用可扩展编译器前端实现。


<details>
  <summary>Details</summary>
Motivation: 为基于Modelica的系统工程提供一个能整合异构工程工件、平衡严谨性与灵活性、实现多工程领域无缝集成和分析的平台。

Method: 采用以Modelica为中心的轮辐式联邦架构，开发基于Isomorphic TypeScript的可扩展Modelica编译器前端。

Result: 构建了ModeliHub平台，其虚拟孪生引擎可提供实时交互式仿真环境。

Conclusion: 该架构能在严谨性和灵活性间取得平衡，实现各工程领域的无缝集成与分析。

Abstract: This paper introduces ModeliHub, a Web-based, federated analytics platform
designed specifically for model-based systems engineering with Modelica.
ModeliHub's key innovation lies in its Modelica-centric, hub-and-spoke
federation architecture that provides systems engineers with a Modelica-based,
unified system model of repositories containing heterogeneous engineering
artifacts. From this unified system model, ModeliHub's Virtual Twin engine
provides a real-time, interactive simulation environment for deploying Modelica
simulation models that represent digital twins of the virtual prototype of the
system under development at a particular iteration of the iterative systems
engineering life cycle. The implementation of ModeliHub is centered around its
extensible, Modelica compiler frontend developed in Isomorphic TypeScript that
can run seamlessly across browser, desktop and server environments. This
architecture aims to strike a balance between rigor and agility, enabling
seamless integration and analysis across various engineering domains.

</details>


### [304] [Context-Aware CodeLLM Eviction for AI-assisted Coding](https://arxiv.org/abs/2506.18796)
*Kishanthan Thangarajah,Boyuan Chen,Shi Chang,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 本文提出CACE策略优化自托管代码大语言模型服务，评估显示其能降低延迟和模型驱逐次数。


<details>
  <summary>Details</summary>
Motivation: 企业自托管代码大语言模型时，模型多样性和有限加速器内存带来管理和服务效率挑战。

Method: 提出CACE上下文感知模型驱逐策略，利用多因素（模型加载时间、任务延迟敏感度等）。

Result: 使用真实工作负载评估，CACE降低了TTFT和E2E延迟，显著减少模型驱逐次数。

Conclusion: 为现实软件工程环境部署可扩展、低延迟AI编码助手提供实用策略。

Abstract: AI-assisted coding tools powered by Code Large Language Models (CodeLLMs) are
increasingly integrated into modern software development workflows. To address
concerns around privacy, latency, and model customization, many enterprises opt
to self-host these models. However, the diversity and growing number of
CodeLLMs, coupled with limited accelerator memory, introduce practical
challenges in model management and serving efficiency. This paper presents
CACE, a novel context-aware model eviction strategy designed specifically to
optimize self-hosted CodeLLM serving under resource constraints. Unlike
traditional eviction strategies based solely on recency (e.g., Least Recently
Used), CACE leverages multiple context-aware factors, including model load
time, task-specific latency sensitivity, expected output length, and recent
usage and future demand tracked through a sliding window. We evaluate CACE
using realistic workloads that include both latency-sensitive code completion
and throughput-intensive code reasoning tasks. Our experiments show that CACE
reduces Time-to-First-Token (TTFT) and end-to-end (E2E) latency, while
significantly lowering the number of model evictions compared to
state-of-the-art systems. Ablation studies further demonstrate the importance
of multi-factor eviction in balancing responsiveness and resource efficiency.
This work contributes practical strategies for deploying scalable, low-latency
AI coding assistants in real-world software engineering environments.

</details>


### [305] [Understanding Software Engineering Agents: A Study of Thought-Action-Result Trajectories](https://arxiv.org/abs/2506.18824)
*Islem Bouzenia,Michael Pradel*

Main category: cs.SE

TL;DR: 本文对三种基于大语言模型的代理进行大规模实证研究，分析其思想 - 行动 - 结果轨迹，找出成功与失败执行的特征，发布数据集和标注框架。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的代理广泛应用，但内部决策过程未充分探索，限制对其操作动态和失败模式的理解。

Method: 统一三种代理的交互日志格式，结合定量分析（结构属性、行动模式、令牌使用）和定性评估（推理连贯性、反馈整合）。

Result: 识别出关键轨迹特征，如迭代次数、令牌消耗、重复行动序列等，发现区分成功与失败执行的行为模式和反模式。

Conclusion: 研究结果为改进代理设计提供可操作的见解，发布的数据集和框架支持后续研究。

Abstract: Large Language Model (LLM)-based agents are increasingly employed to automate
complex software engineering tasks such as program repair and issue resolution.
These agents operate by autonomously generating natural language thoughts,
invoking external tools, and iteratively refining their solutions. Despite
their widespread adoption, the internal decision-making processes of these
agents remain largely unexplored, limiting our understanding of their
operational dynamics and failure modes. In this paper, we present a large-scale
empirical study of the thought-action-result trajectories of three
state-of-the-art LLM-based agents: \textsc{RepairAgent},
\textsc{AutoCodeRover}, and \textsc{OpenHands}. We unify their interaction logs
into a common format, capturing 120 trajectories and 2822 LLM interactions
focused on program repair and issue resolution. Our study combines quantitative
analyses of structural properties, action patterns, and token usage with
qualitative assessments of reasoning coherence and feedback integration. We
identify key trajectory characteristics such as iteration counts and token
consumption, recurring action sequences, and the semantic coherence linking
thoughts, actions, and their results. Our findings reveal behavioral motifs and
anti-patterns that distinguish successful from failed executions, providing
actionable insights for improving agent design, including prompting strategies,
failure diagnosis, and anti-pattern detection. We release our dataset and
annotation framework to support further research on transparent and robust
autonomous software engineering agents.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [306] [Transformers Beyond Order: A Chaos-Markov-Gaussian Framework for Short-Term Sentiment Forecasting of Any Financial OHLC timeseries Data](https://arxiv.org/abs/2506.17244)
*Arif Pathan*

Main category: q-fin.ST

TL;DR: 本文提出CMG框架用于金融市场短期情绪预测，结合混沌理论、马尔可夫性质和高斯过程，用Transformer模型增强，在预测中表现优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 金融市场OHLC数据存在波动性、非线性和噪声，传统模型预测准确性低且需大量资源和特定调整，因此需要新框架提高预测准确性和效率。

Method: 引入CMG框架，整合混沌理论、马尔可夫性质和高斯过程，并用Transformer深度学习模型增强，以捕捉时间模式。

Result: 在市场指数上评估，预测下一个交易日第一季度的情绪，与统计、机器学习和深度学习基线模型对比，CMG框架在准确性和效率上始终表现更优。

Conclusion: CMG框架减少了开销，泛化能力强，对分析师和金融机构有重要价值。

Abstract: Short-term sentiment forecasting in financial markets (e.g., stocks, indices)
is challenging due to volatility, non-linearity, and noise in OHLC (Open, High,
Low, Close) data. This paper introduces a novel CMG (Chaos-Markov-Gaussian)
framework that integrates chaos theory, Markov property, and Gaussian processes
to improve prediction accuracy. Chaos theory captures nonlinear dynamics; the
Markov chain models regime shifts; Gaussian processes add probabilistic
reasoning. We enhance the framework with transformer-based deep learning models
to capture temporal patterns efficiently. The CMG Framework is designed for
fast, resource-efficient, and accurate forecasting of any financial
instrument's OHLC time series. Unlike traditional models that require heavy
infrastructure and instrument-specific tuning, CMG reduces overhead and
generalizes well. We evaluate the framework on market indices, forecasting
sentiment for the next trading day's first quarter. A comparative study against
statistical, ML, and DL baselines trained on the same dataset with no feature
engineering shows CMG consistently outperforms in accuracy and efficiency,
making it valuable for analysts and financial institutions.

</details>


### [307] [Predicting Stock Market Crash with Bayesian Generalised Pareto Regression](https://arxiv.org/abs/2506.17549)
*Sourish Das*

Main category: q-fin.ST

TL;DR: 本文开发贝叶斯广义帕累托回归（GPR）模型预测印度股市极端损失，对比四种先验选择，发现柯西先验最优，实证表明市场波动增加尾部风险，该模型对新兴市场尾部风险预测有优势。


<details>
  <summary>Details</summary>
Motivation: 极端负回报虽罕见但会造成重大金融破坏，传统广义帕累托分布（GPD）模型常忽略市场条件，需准确建模以进行有效风险管理。

Method: 开发贝叶斯GPR模型，将尺度参数与协变量通过对数线性函数关联；对比柯西、Lasso、Ridge和Zellner's g - prior四种先验选择；以印度Nifty 50指数大幅负回报数据实证，用Nifty 50、S&P 500和黄金的波动率作为协变量。

Result: 模拟结果显示柯西先验在预测准确性和模型简单性间平衡最佳；实证发现尾部风险随市场波动显著增加，S&P 500和黄金波动率对崩盘预测有重要作用。

Conclusion: 所提出的GPR模型为新兴市场尾部风险预测提供了稳健且可解释的方法，通过纳入实时金融指标改进了传统基于EVT的模型，对从业者、政策制定者和金融监管者有用。

Abstract: This paper develops a Bayesian Generalised Pareto Regression (GPR) model to
forecast extreme losses in Indian equity markets, with a focus on the Nifty 50
index. Extreme negative returns, though rare, can cause significant financial
disruption, and accurate modelling of such events is essential for effective
risk management. Traditional Generalised Pareto Distribution (GPD) models often
ignore market conditions; in contrast, our framework links the scale parameter
to covariates using a log-linear function, allowing tail risk to respond
dynamically to market volatility. We examine four prior choices for Bayesian
regularisation of regression coefficients: Cauchy, Lasso (Laplace), Ridge
(Gaussian), and Zellner's g-prior. Simulation results suggest that the Cauchy
prior delivers the best trade-off between predictive accuracy and model
simplicity, achieving the lowest RMSE, AIC, and BIC values. Empirically, we
apply the model to large negative returns (exceeding 5%) in the Nifty 50 index.
Volatility measures from the Nifty 50, S&P 500, and gold are used as covariates
to capture both domestic and global risk drivers. Our findings show that tail
risk increases significantly with higher market volatility. In particular, both
S&P 500 and gold volatilities contribute meaningfully to crash prediction,
highlighting global spillover and flight-to-safety effects. The proposed GPR
model offers a robust and interpretable approach for tail risk forecasting in
emerging markets. It improves upon traditional EVT-based models by
incorporating real-time financial indicators, making it useful for
practitioners, policymakers, and financial regulators concerned with systemic
risk and stress testing.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [308] [Causal Interventions in Bond Multi-Dealer-to-Client Platforms](https://arxiv.org/abs/2506.18147)
*Paloma Marín,Sergio Ardanza-Trevijano,Javier Sabio*

Main category: q-fin.TR

TL;DR: 文章引入概率图模型和因果推断的通用框架分析多交易商对客户（MD2C）平台的报价请求（RfQ）过程，探讨相关推理问题，评估两种模型构建方法。


<details>
  <summary>Details</summary>
Motivation: 金融市场数字化使交易转向电子渠道，MD2C平台竞争激烈，经销商无法看到彼此价格，需严谨分析谈判过程以确保盈利。

Method: 引入用概率图模型和因果推断分析RfQ过程的通用框架，探讨相关推理问题，分析基于Fermanian等工作的生成模型和利用机器学习技术的判别模型。

Result: 用预测指标评估这些方法在最优定价方面的有效性。

Conclusion: 考虑谈判过程内部机制的模型有相对优势。

Abstract: The digitalization of financial markets has shifted trading from voice to
electronic channels, with Multi-Dealer-to-Client (MD2C) platforms now enabling
clients to request quotes (RfQs) for financial instruments like bonds from
multiple dealers simultaneously. In this competitive landscape, dealers cannot
see each other's prices, making a rigorous analysis of the negotiation process
crucial to ensure their profitability. This article introduces a novel general
framework for analyzing the RfQ process using probabilistic graphical models
and causal inference. Within this framework, we explore different inferential
questions that are relevant for dealers participating in MD2C platforms, such
as the computation of optimal prices, estimating potential revenues and the
identification of clients that might be interested in trading the dealer's
axes. We then move into analyzing two different approaches for model
specification: a generative model built on the work of (Fermanian, Gu\'eant &
Pu, 2017); and discriminative models utilizing machine learning techniques. We
evaluate these methodologies using predictive metrics designed to assess their
effectiveness in the context of optimal pricing, highlighting the relative
benefits of using models that take into account the internal mechanisms of the
negotiation process.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [309] [Coupled Entropy: A Goldilocks Generalization?](https://arxiv.org/abs/2506.17229)
*Kenric P. Nelson*

Main category: stat.ML

TL;DR: 本文指出非广延统计力学（NSM）中Tsallis熵约束与函数归一化问题，提出耦合熵可能为机器学习等应用提供鲁棒性，且耦合可作为统计复杂性的度量。


<details>
  <summary>Details</summary>
Motivation: 解决非广延统计力学中Tsallis熵约束与函数归一化产生的问题，寻找适用于机器学习等应用的鲁棒熵函数。

Method: 通过理论分析，明确独立随机变量数量q与复杂系统非线性性质的关系，引出耦合熵及其最大化分布。

Result: 定义了耦合熵及其最大化分布——耦合指数族，指出耦合是诱导非指数分布和非可加性熵的非线性度量。

Conclusion: 耦合熵可能为应用提供必要的鲁棒性，耦合是统计复杂性的有力候选度量。

Abstract: Nonextensive Statistical Mechanics (NSM) has developed into a powerful
toolset for modeling and analyzing complex systems. Despite its many successes,
a puzzle arose early in its development. The constraints on the Tsallis entropy
are in the form of an escort distribution with elements proportional to
$p_i^q$, but this same factor within the Tsallis entropy function is not
normalized. This led to consideration of the Normalized Tsallis Entropy (NTE);
however, the normalization proved to make the function unstable. I will provide
evidence that the coupled entropy, which divides NTE by $1 + d\kappa$, where
$d$ is the dimension and $\kappa$ is the coupling, may provide the necessary
robustness necessary for applications like machine learning. The definition for
the coupled entropy and its maximizing distributions, the coupled exponential
family, arises from clarifying how the number of independent random variables
$(q)$ is composed of the nonlinear properties of complex systems,
$q=1+\frac{\alpha\kappa}{1+d\kappa}$, where $\alpha$ is the nonlinear parameter
governing the shape of distributions near their location and $\kappa$ is the
parameter determining the asymptotic tail decay. Foundationally, for complex
systems, the coupling is the measure of nonlinearity inducing non-exponential
distributions and the degree of nonadditivity entropy. As such, the coupling is
a strong candidate as a measure of statistical complexity.

</details>


### [310] [Differentiable neural network representation of multi-well, locally-convex potentials](https://arxiv.org/abs/2506.17242)
*Reese E. Jones,Adrian Buganza Tepole,Jan N. Fuhg*

Main category: stat.ML

TL;DR: 提出基于LSE-ICNN的可微凸公式，能自动发现模式数量和过渡尺度，在多领域展示了有效性。


<details>
  <summary>Details</summary>
Motivation: 现有非光滑最小混合表示存在不足，需要一种更好的方法来处理多井势问题。

Method: 提出基于log - sum - exponential (LSE)混合输入凸神经网络(ICNN)模式的可微凸公式（LSE - ICNN），通过稀疏回归自动发现模式数量和过渡尺度。

Result: 在机械化学相变、微观结构弹性不稳定性等多个领域验证了LSE - ICNN的有效性。

Conclusion: LSE - ICNN在捕捉复杂多峰景观时能保持可微性，在数据驱动建模、优化和物理模拟中有广泛应用。

Abstract: Multi-well potentials are ubiquitous in science, modeling phenomena such as
phase transitions, dynamic instabilities, and multimodal behavior across
physics, chemistry, and biology. In contrast to non-smooth minimum-of-mixture
representations, we propose a differentiable and convex formulation based on a
log-sum-exponential (LSE) mixture of input convex neural network (ICNN) modes.
This log-sum-exponential input convex neural network (LSE-ICNN) provides a
smooth surrogate that retains convexity within basins and allows for
gradient-based learning and inference.
  A key feature of the LSE-ICNN is its ability to automatically discover both
the number of modes and the scale of transitions through sparse regression,
enabling adaptive and parsimonious modeling. We demonstrate the versatility of
the LSE-ICNN across diverse domains, including mechanochemical phase
transformations, microstructural elastic instabilities, conservative biological
gene circuits, and variational inference for multimodal probability
distributions. These examples highlight the effectiveness of the LSE-ICNN in
capturing complex multimodal landscapes while preserving differentiability,
making it broadly applicable in data-driven modeling, optimization, and
physical simulation.

</details>


### [311] [Gaussian Processes and Reproducing Kernels: Connections and Equivalences](https://arxiv.org/abs/2506.17366)
*Motonobu Kanagawa,Philipp Hennig,Dino Sejdinovic,Bharath K. Sriperumbudur*

Main category: stat.ML

TL;DR: 本文研究正定核两种方法（高斯过程概率方法与再生核希尔伯特空间非概率方法）关系，回顾其在多领域的联系与等价性，建立统一视角，为连接更多相关方法打基础。


<details>
  <summary>Details</summary>
Motivation: 探究高斯过程概率方法和再生核希尔伯特空间非概率方法之间的关系，在机器学习、统计和数值分析等领域具有重要意义。

Method: 回顾两种方法在回归、插值等基础主题及高斯过程样本路径性质方面的联系和等价性，基于高斯希尔伯特空间和RKHS的等价性建立统一视角。

Result: 明确了两种方法在多个基础主题及样本路径性质方面的联系和等价性，建立了统一视角。

Conclusion: 该专著为连接两个研究群体基于高斯过程和再生核发展的其他方法奠定基础。

Abstract: This monograph studies the relations between two approaches using positive
definite kernels: probabilistic methods using Gaussian processes, and
non-probabilistic methods using reproducing kernel Hilbert spaces (RKHS). They
are widely studied and used in machine learning, statistics, and numerical
analysis. Connections and equivalences between them are reviewed for
fundamental topics such as regression, interpolation, numerical integration,
distributional discrepancies, and statistical dependence, as well as for sample
path properties of Gaussian processes. A unifying perspective for these
equivalences is established, based on the equivalence between the Gaussian
Hilbert space and the RKHS. The monograph serves as a basis to bridge many
other methods based on Gaussian processes and reproducing kernels, which are
developed in parallel by the two research communities.

</details>


### [312] [Scalable Machine Learning Algorithms using Path Signatures](https://arxiv.org/abs/2506.17634)
*Csaba Tóth*

Main category: stat.ML

TL;DR: 本文聚焦随机分析与机器学习接口领域，研究如何在可扩展机器学习管道中利用路径签名的表达能力，提出一系列结合理论鲁棒性与计算效率的模型。


<details>
  <summary>Details</summary>
Motivation: 解决现实世界时间序列和图数据中常见的演化动态、长程依赖和不规则采样等挑战，在可扩展机器学习管道中发挥路径签名的表达能力。

Method: 引入一系列模型，结合粗糙路径理论与概率建模、深度学习和核方法，如基于签名核协方差函数的高斯过程、Seq2Tens框架、基于图的模型等。

Result: 提出了多种模型和方法，包括用于时间序列建模的高斯过程、处理长程依赖的Seq2Tens框架、基于图的模型，以及随机傅里叶签名特征和循环稀疏谱签名高斯过程等。

Conclusion: 本文可作为方法工具箱和概念桥梁，为基于签名的可扩展序列和结构化数据学习的当前技术水平提供有用参考。

Abstract: The interface between stochastic analysis and machine learning is a rapidly
evolving field, with path signatures - iterated integrals that provide
faithful, hierarchical representations of paths - offering a principled and
universal feature map for sequential and structured data. Rooted in rough path
theory, path signatures are invariant to reparameterization and well-suited for
modelling evolving dynamics, long-range dependencies, and irregular sampling -
common challenges in real-world time series and graph data.
  This thesis investigates how to harness the expressive power of path
signatures within scalable machine learning pipelines. It introduces a suite of
models that combine theoretical robustness with computational efficiency,
bridging rough path theory with probabilistic modelling, deep learning, and
kernel methods. Key contributions include: Gaussian processes with signature
kernel-based covariance functions for uncertainty-aware time series modelling;
the Seq2Tens framework, which employs low-rank tensor structure in the weight
space for scalable deep modelling of long-range dependencies; and graph-based
models where expected signatures over graphs induce hypo-elliptic diffusion
processes, offering expressive yet tractable alternatives to standard graph
neural networks. Further developments include Random Fourier Signature
Features, a scalable kernel approximation with theoretical guarantees, and
Recurrent Sparse Spectrum Signature Gaussian Processes, which combine Gaussian
processes, signature kernels, and random features with a principled forgetting
mechanism for multi-horizon time series forecasting with adaptive context
length.
  We hope this thesis serves as both a methodological toolkit and a conceptual
bridge, and provides a useful reference for the current state of the art in
scalable, signature-based learning for sequential and structured data.

</details>


### [313] [Derandomizing Simultaneous Confidence Regions for Band-Limited Functions by Improved Norm Bounds and Majority-Voting Schemes](https://arxiv.org/abs/2506.17764)
*Balázs Csanád Csáji,Bálint Horváth*

Main category: stat.ML

TL;DR: 本文在Paley - Wiener再生核希尔伯特空间中改进了一种构造带限函数同时置信区域的方法，通过收紧核范数界、确定阈值和使用多数投票聚合置信集等方式提升性能，并通过数值实验验证。


<details>
  <summary>Details</summary>
Motivation: 改进从有噪声的输入 - 输出测量中构造带限函数同时置信区域的非参数、非渐近方法。

Method: 在Paley - Wiener再生核希尔伯特空间工作，用均匀随机化的Hoeffding不等式收紧小样本核范数界，用经验Bernstein界收紧大样本核范数界，推导部署界的近似阈值，使用多数投票聚合随机子样本的置信集。

Result: 证明了即使每个输入的聚合区间也保留其同时覆盖保证。

Conclusion: 改进方法有效，通过数值实验得到验证。

Abstract: Band-limited functions are fundamental objects that are widely used in
systems theory and signal processing. In this paper we refine a recent
nonparametric, nonasymptotic method for constructing simultaneous confidence
regions for band-limited functions from noisy input-output measurements, by
working in a Paley-Wiener reproducing kernel Hilbert space. Kernel norm bounds
are tightened using a uniformly-randomized Hoeffding's inequality for small
samples and an empirical Bernstein bound for larger ones. We derive an
approximate threshold, based on the sample size and how informative the inputs
are, that governs which bound to deploy. Finally, we apply majority voting to
aggregate confidence sets from random subsamples, boosting both stability and
region size. We prove that even per-input aggregated intervals retain their
simultaneous coverage guarantee. These refinements are also validated through
numerical experiments.

</details>


### [314] [DRO-Augment Framework: Robustness by Synergizing Wasserstein Distributionally Robust Optimization and Data Augmentation](https://arxiv.org/abs/2506.17874)
*Jiaming Hu,Debarghya Mukherjee,Ioannis Ch. Paschalidis*

Main category: stat.ML

TL;DR: 提出DRO - Augment框架结合W - DRO和数据增强策略，提升模型对多种扰动和攻击的鲁棒性，在多数据集表现良好且有理论误差界。


<details>
  <summary>Details</summary>
Motivation: 现有数据增强技术在同时应对损坏数据和对抗攻击的鲁棒性上有提升空间，需改进深度神经网络在图像分类任务中的鲁棒性和稳定性。

Method: 引入DRO - Augment框架，将Wasserstein Distributionally Robust Optimization与多种数据增强策略结合。

Result: 在严重数据扰动和对抗攻击场景下优于现有增强方法，在干净数据集上保持准确率，在CIFAR - 10 - C等多个基准数据集上有效。

Conclusion: DRO - Augment框架能显著提升模型在多种损坏情况下的鲁棒性，还建立了相关神经网络的泛化误差界。

Abstract: In many real-world applications, ensuring the robustness and stability of
deep neural networks (DNNs) is crucial, particularly for image classification
tasks that encounter various input perturbations. While data augmentation
techniques have been widely adopted to enhance the resilience of a trained
model against such perturbations, there remains significant room for
improvement in robustness against corrupted data and adversarial attacks
simultaneously. To address this challenge, we introduce DRO-Augment, a novel
framework that integrates Wasserstein Distributionally Robust Optimization
(W-DRO) with various data augmentation strategies to improve the robustness of
the models significantly across a broad spectrum of corruptions. Our method
outperforms existing augmentation methods under severe data perturbations and
adversarial attack scenarios while maintaining the accuracy on the clean
datasets on a range of benchmark datasets, including but not limited to
CIFAR-10-C, CIFAR-100-C, MNIST, and Fashion-MNIST. On the theoretical side, we
establish novel generalization error bounds for neural networks trained using a
computationally efficient, variation-regularized loss function closely related
to the W-DRO problem.

</details>


### [315] [Identifiable Convex-Concave Regression via Sub-gradient Regularised Least Squares](https://arxiv.org/abs/2506.18078)
*William Chung*

Main category: stat.ML

TL;DR: 提出可识别的凸 - 凹非参数最小二乘法（ICCNLS），通过分解目标函数建模输入输出关系，实验表明其比传统方法有更好的预测准确性和模型简洁性。


<details>
  <summary>Details</summary>
Motivation: 构建能建模复杂输入输出关系的非参数回归方法，解决凸 - 凹分解中的仿射模糊性，提高模型可解释性。

Method: 提出ICCNLS方法，将目标函数分解为可加的形状约束组件，引入全局统计正交约束，对次梯度加入L1、L2和弹性网络正则化。

Result: 在合成和真实数据集上，相比传统CNLS和DC回归方法，有更好的预测准确性和模型简洁性。

Conclusion: 统计可识别性结合凸 - 凹结构和次梯度正则化，能得到适用于预测、基准测试和政策评估的可解释模型。

Abstract: We propose a novel nonparametric regression method that models complex
input-output relationships as the sum of convex and concave components. The
method-Identifiable Convex-Concave Nonparametric Least Squares
(ICCNLS)-decomposes the target function into additive shape-constrained
components, each represented via sub-gradient-constrained affine functions. To
address the affine ambiguity inherent in convex-concave decompositions, we
introduce global statistical orthogonality constraints, ensuring that residuals
are uncorrelated with both intercept and input variables. This enforces
decomposition identifiability and improves interpretability. We further
incorporate L1, L2 and elastic net regularisation on sub-gradients to enhance
generalisation and promote structural sparsity. The proposed method is
evaluated on synthetic and real-world datasets, including healthcare pricing
data, and demonstrates improved predictive accuracy and model simplicity
compared to conventional CNLS and difference-of-convex (DC) regression
approaches. Our results show that statistical identifiability, when paired with
convex-concave structure and sub-gradient regularisation, yields interpretable
models suited for forecasting, benchmarking, and policy evaluation.

</details>


### [316] [Phase transition of \emph{descending} phase retrieval algorithms](https://arxiv.org/abs/2506.18275)
*Mihailo Stojnic*

Main category: stat.ML

TL;DR: 研究下降相位恢复算法的理论极限，利用随机对偶理论构建通用程序，研究参数流形特性，观察到相变，开发实用算法变体，理论与模拟结果相符。


<details>
  <summary>Details</summary>
Motivation: 探究下降相位恢复算法的理论极限，了解其性能和行为。

Method: 利用随机对偶理论构建通用程序，研究参数流形及其漏斗点，开发结合屏障和普通梯度下降的实用算法变体。

Result: 建立单漏斗点流形与下降算法全局收敛的同构关系，观察到随着样本复杂度增加，参数流形从多漏斗点结构转变为单漏斗点结构，对应算法从失败到成功的转变。

Conclusion: 理论结果在小维度下与模拟的相变预测有很强的一致性。

Abstract: We study theoretical limits of \emph{descending} phase retrieval algorithms.
Utilizing \emph{Random duality theory} (RDT) we develop a generic program that
allows statistical characterization of various algorithmic performance metrics.
Through these we identify the concepts of \emph{parametric manifold} and its
\emph{funneling points} as key mathematical objects that govern the underlying
algorithms' behavior. An isomorphism between single funneling point manifolds
and global convergence of descending algorithms is established. The structure
and shape of the parametric manifold as well as its dependence on the sample
complexity are studied through both plain and lifted RDT. Emergence of a phase
transition is observed. Namely, as sample complexity increases, parametric
manifold transitions from a multi to a single funneling point structure. This
in return corresponds to a transition from the scenarios where descending
algorithms generically fail to the scenarios where they succeed in solving
phase retrieval. We also develop and implement a practical algorithmic variant
that in a hybrid alternating fashion combines a barrier and a plain gradient
descent. Even though the theoretical results are obtained for infinite
dimensional scenarios (and consequently non-jittery parametric manifolds), we
observe a strong agrement between theoretical and simulated phase transitions
predictions for fairly small dimensions on the order of a few hundreds.

</details>


### [317] [Optimal spectral initializers impact on phase retrieval phase transitions -- an RDT view](https://arxiv.org/abs/2506.18279)
*Mihailo Stojnic*

Main category: stat.ML

TL;DR: 分析谱初始化器与降序相位恢复算法理论极限的关系，提出随机对偶理论程序表征重叠最优谱初始化器，发现算法理论相变难实现及“安全压缩”可解决问题，数值模拟与理论相符。


<details>
  <summary>Details</summary>
Motivation: 研究谱初始化器与降序相位恢复算法理论极限的关系，明确重叠最优谱初始化器对算法的影响，解决算法在参数流形平坦区域的问题。

Method: 提出随机对偶理论（RDT）程序对重叠最优谱初始化器进行统计表征，通过理论分析和数值模拟验证。

Result: 发现降序相位恢复算法理论相变难实现，因其参数流形平坦区域大；采用“安全压缩”增加样本复杂度比可缩小平坦区域，使算法最终解决相位恢复问题，数值模拟与理论预测相符。

Conclusion: 降序相位恢复算法理论相变实际中较难达成，“安全压缩”策略可有效解决算法在参数流形平坦区域的问题，帮助算法解决相位恢复问题。

Abstract: We analyze the relation between spectral initializers and theoretical limits
of \emph{descending} phase retrieval algorithms (dPR). In companion paper
[104], for any sample complexity ratio, $\alpha$, \emph{parametric manifold},
${\mathcal {PM}}(\alpha)$, is recognized as a critically important structure
that generically determines dPRs abilities to solve phase retrieval (PR).
Moreover, overlap between the algorithmic solution and the true signal is
positioned as a key ${\mathcal {PM}}$'s component. We here consider the
so-called \emph{overlap optimal} spectral initializers (OptSpins) as dPR's
starting points and develop a generic \emph{Random duality theory} (RDT) based
program to statistically characterize them. In particular, we determine the
functional structure of OptSpins and evaluate the starting overlaps that they
provide for the dPRs. Since ${\mathcal {PM}}$'s so-called \emph{flat regions}
are highly susceptible to \emph{local jitteriness} and as such are key
obstacles on dPR's path towards PR's global optimum, a precise characterization
of the starting overlap allows to determine if such regions can be successfully
circumvented. Through the presented theoretical analysis we observe two key
points in that regard: \textbf{\emph{(i)}} dPR's theoretical phase transition
(critical $\alpha$ above which they solve PR) might be difficult to practically
achieve as the ${\mathcal {PM}}$'s flat regions are large causing the
associated OptSpins to fall exactly within them; and \textbf{\emph{(ii)}}
Opting for so-called ``\emph{safer compression}'' and slightly increasing
$\alpha$ (by say $15\%$) shrinks flat regions and allows OptSpins to fall
outside them and dPRs to ultimately solve PR. Numerical simulations are
conducted as well and shown to be in an excellent agreement with theoretical
predictions.

</details>


### [318] [Phase retrieval with rank $d$ measurements -- \emph{descending} algorithms phase transitions](https://arxiv.org/abs/2506.18282)
*Mihailo Stojnic*

Main category: stat.ML

TL;DR: 推广之前的分析程序以处理秩为d的正定相位恢复测量，确定相位转变位置，模拟结果与理论预测相符。


<details>
  <summary>Details</summary>
Motivation: 将之前基于随机对偶理论（RDT）的分析程序进行推广，以处理秩d正定相位恢复测量。

Method: 推广分析程序，确定相位转变位置，实现对数障碍梯度下降变体进行模拟。

Result: 最小样本复杂度比存在相位转变现象，确定了相位转变位置，模拟的相位转变与理论预测高度一致。

Conclusion: 推广的程序可有效处理秩d正定相位恢复测量，模拟结果验证了理论预测。

Abstract: Companion paper [118] developed a powerful \emph{Random duality theory} (RDT)
based analytical program to statistically characterize performance of
\emph{descending} phase retrieval algorithms (dPR) (these include all variants
of gradient descents and among them widely popular Wirtinger flows). We here
generalize the program and show how it can be utilized to handle rank $d$
positive definite phase retrieval (PR) measurements (with special cases $d=1$
and $d=2$ serving as emulations of the real and complex phase retrievals,
respectively). In particular, we observe that the minimal sample complexity
ratio (number of measurements scaled by the dimension of the unknown signal)
which ensures dPR's success exhibits a phase transition (PT) phenomenon. For
both plain and lifted RDT we determine phase transitions locations. To
complement theoretical results we implement a log barrier gradient descent
variant and observe that, even in small dimensional scenarios (with problem
sizes on the order of 100), the simulated phase transitions are in an excellent
agreement with the theoretical predictions.

</details>


### [319] [Quantifying Uncertainty in the Presence of Distribution Shifts](https://arxiv.org/abs/2506.18283)
*Yuli Slavutsky,David M. Blei*

Main category: stat.ML

TL;DR: 提出一种考虑协变量偏移的贝叶斯不确定性估计框架，在分布偏移下能改善不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 神经网络在训练和测试协变量分布偏移时，难以提供可靠的不确定性估计。

Method: 提出自适应先验的贝叶斯框架，使用摊销变分推理近似后验预测分布，通过从训练数据中抽取小的自助样本构建合成环境。

Result: 在合成和真实数据上评估，在分布偏移下显著改善了不确定性估计。

Conclusion: 所提方法能有效应对协变量分布偏移，提升不确定性估计的可靠性。

Abstract: Neural networks make accurate predictions but often fail to provide reliable
uncertainty estimates, especially under covariate distribution shifts between
training and testing. To address this problem, we propose a Bayesian framework
for uncertainty estimation that explicitly accounts for covariate shifts. While
conventional approaches rely on fixed priors, the key idea of our method is an
adaptive prior, conditioned on both training and new covariates. This prior
naturally increases uncertainty for inputs that lie far from the training
distribution in regions where predictive performance is likely to degrade. To
efficiently approximate the resulting posterior predictive distribution, we
employ amortized variational inference. Finally, we construct synthetic
environments by drawing small bootstrap samples from the training data,
simulating a range of plausible covariate shift using only the original
dataset. We evaluate our method on both synthetic and real-world data. It
yields substantially improved uncertainty estimates under distribution shifts.

</details>


### [320] [Theoretical guarantees for neural estimators in parametric statistics](https://arxiv.org/abs/2506.18508)
*Almut Rödder,Manuel Hentschel,Sebastian Engelke*

Main category: stat.ML

TL;DR: 研究神经估计器风险，分解为可单独分析的项，提出易验证假设确保各项收敛到零，为更广泛架构和估计问题提供理论保证。


<details>
  <summary>Details</summary>
Motivation: 现有神经估计器虽在模拟研究和实际应用中表现良好，但缺乏理论统计保证。

Method: 将神经估计器的风险分解为多个可单独分析的项，提出易验证的假设。

Result: 验证了假设在神经估计器常见应用中的有效性，为更广泛情况提供理论保证。

Conclusion: 为更广泛的架构和估计问题推导理论保证提供通用方法。

Abstract: Neural estimators are simulation-based estimators for the parameters of a
family of statistical models, which build a direct mapping from the sample to
the parameter vector. They benefit from the versatility of available network
architectures and efficient training methods developed in the field of deep
learning. Neural estimators are amortized in the sense that, once trained, they
can be applied to any new data set with almost no computational cost. While
many papers have shown very good performance of these methods in simulation
studies and real-world applications, so far no statistical guarantees are
available to support these observations theoretically. In this work, we study
the risk of neural estimators by decomposing it into several terms that can be
analyzed separately. We formulate easy-to-check assumptions ensuring that each
term converges to zero, and we verify them for popular applications of neural
estimators. Our results provide a general recipe to derive theoretical
guarantees also for broader classes of architectures and estimation problems.

</details>


### [321] [Trustworthy Prediction with Gaussian Process Knowledge Scores](https://arxiv.org/abs/2506.18630)
*Kurt Butler,Guanchao Feng,Tong Chen,Petar Djuric*

Main category: stat.ML

TL;DR: 提出高斯过程回归模型预测的知识得分，实验证明其能预测预测准确性并提升任务表现，代码开源。


<details>
  <summary>Details</summary>
Motivation: 概率模型在无观测数据区域做预测时，不清楚预测是否受已有数据充分影响。

Method: 提出高斯过程回归模型预测的知识得分，该得分可解释且范围在0到1。

Result: 实验表明知识得分能预测高斯过程回归模型预测的准确性，提升异常检测、外推和缺失数据插补等任务表现。

Conclusion: 所提出的知识得分有效，可用于评估高斯过程回归模型预测并提升相关任务性能。

Abstract: Probabilistic models are often used to make predictions in regions of the
data space where no observations are available, but it is not always clear
whether such predictions are well-informed by previously seen data. In this
paper, we propose a knowledge score for predictions from Gaussian process
regression (GPR) models that quantifies the extent to which observing data have
reduced our uncertainty about a prediction. The knowledge score is
interpretable and naturally bounded between 0 and 1. We demonstrate in several
experiments that the knowledge score can anticipate when predictions from a GPR
model are accurate, and that this anticipation improves performance in tasks
such as anomaly detection, extrapolation, and missing data imputation. Source
code for this project is available online at
https://github.com/KurtButler/GP-knowledge.

</details>


### [322] [Tight Generalization Error Bounds for Stochastic Gradient Descent in Non-convex Learning](https://arxiv.org/abs/2506.18645)
*Wenjun Xiong,Juan Ding,Xinlei Zuo,Qizhai Li*

Main category: stat.ML

TL;DR: 本文引入T2pm - SGD分析非凸学习中SGD的泛化误差界，改进轨迹项并细化整体界，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 理解SGD在非凸学习中的泛化特性对确保模型在未见数据上的鲁棒性至关重要。

Method: 引入T2pm - SGD，将泛化误差界分解为轨迹项和平坦度项，通过选择最优扰动噪声方差优化界。

Result: 改进轨迹项至$O(n^{-1})$，细化整体界至$O(n^{-2/3})$，平坦度项稳定且更小。

Conclusion: T2pm - SGD能为两种损失函数类型建立更紧的泛化误差界，实验验证了其有效性。

Abstract: Stochastic Gradient Descent (SGD) is fundamental for training deep neural
networks, especially in non-convex settings. Understanding SGD's generalization
properties is crucial for ensuring robust model performance on unseen data. In
this paper, we analyze the generalization error bounds of SGD for non-convex
learning by introducing the Type II perturbed SGD (T2pm-SGD), which
accommodates both sub-Gaussian and bounded loss functions. The generalization
error bound is decomposed into two components: the trajectory term and the
flatness term. Our analysis improves the trajectory term to $O(n^{-1})$,
significantly enhancing the previous $O((nb)^{-1/2})$ bound for bounded losses,
where n is the number of training samples and b is the batch size. By selecting
an optimal variance for the perturbation noise, the overall bound is further
refined to $O(n^{-2/3})$. For sub-Gaussian loss functions, a tighter trajectory
term is also achieved. In both cases, the flatness term remains stable across
iterations and is smaller than those reported in previous literature, which
increase with iterations. This stability, ensured by T2pm-SGD, leads to tighter
generalization error bounds for both loss function types. Our theoretical
results are validated through extensive experiments on benchmark datasets,
including MNIST and CIFAR-10, demonstrating the effectiveness of T2pm-SGD in
establishing tighter generalization bounds.

</details>


### [323] [A Random Matrix Analysis of In-context Memorization for Nonlinear Attention](https://arxiv.org/abs/2506.18656)
*Zhenyu Liao,Jiaqing Liu,TianQi Hou,Difan Zou,Zenan Ling*

Main category: stat.ML

TL;DR: 本文在高维比例状态下刻画非线性注意力的上下文记忆误差，揭示非线性和输入结构如何相互作用影响记忆性能，数值实验支持理论。


<details>
  <summary>Details</summary>
Motivation: 注意力机制虽重要，但对其理论理解，尤其是非线性设置下有限，需深入研究。

Method: 利用大核随机矩阵理论。

Result: 非线性注意力在随机输入上记忆误差通常比线性岭回归高，但输入有统计结构时差距消失甚至逆转。

Conclusion: 揭示了非线性和输入结构相互作用对非线性注意力记忆性能的影响。

Abstract: Attention mechanisms have revolutionized machine learning (ML) by enabling
efficient modeling of global dependencies across inputs. Their inherently
parallelizable structures allow for efficient scaling with the exponentially
increasing size of both pretrained data and model parameters. Yet, despite
their central role as the computational backbone of modern large language
models (LLMs), the theoretical understanding of Attentions, especially in the
nonlinear setting, remains limited.
  In this paper, we provide a precise characterization of the \emph{in-context
memorization error} of \emph{nonlinear Attention}, in the high-dimensional
proportional regime where the number of input tokens $n$ and their embedding
dimension $p$ are both large and comparable. Leveraging recent advances in the
theory of large kernel random matrices, we show that nonlinear Attention
typically incurs higher memorization error than linear ridge regression on
random inputs. However, this gap vanishes, and can even be reversed, when the
input exhibits statistical structure, particularly when the Attention weights
align with the input signal direction. Our results reveal how nonlinearity and
input structure interact with each other to govern the memorization performance
of nonlinear Attention. The theoretical insights are supported by numerical
experiments.

</details>


### [324] [Local Averaging Accurately Distills Manifold Structure From Noisy Data](https://arxiv.org/abs/2506.18761)
*Yihan Shen,Shiyu Wang,Arnaud Lamy,Mariam Avagyan,John Wright*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: High-dimensional data are ubiquitous, with examples ranging from natural
images to scientific datasets, and often reside near low-dimensional manifolds.
Leveraging this geometric structure is vital for downstream tasks, including
signal denoising, reconstruction, and generation. However, in practice, the
manifold is typically unknown and only noisy samples are available. A
fundamental approach to uncovering the manifold structure is local averaging,
which is a cornerstone of state-of-the-art provable methods for manifold
fitting and denoising. However, to the best of our knowledge, there are no
works that rigorously analyze the accuracy of local averaging in a manifold
setting in high-noise regimes. In this work, we provide theoretical analyses of
a two-round mini-batch local averaging method applied to noisy samples drawn
from a $d$-dimensional manifold $\mathcal M \subset \mathbb{R}^D$, under a
relatively high-noise regime where the noise size is comparable to the reach
$\tau$. We show that with high probability, the averaged point $\hat{\mathbf
q}$ achieves the bound $d(\hat{\mathbf q}, \mathcal M) \leq \sigma
\sqrt{d\left(1+\frac{\kappa\mathrm{diam}(\mathcal {M})}{\log(D)}\right)}$,
where $\sigma, \mathrm{diam(\mathcal M)},\kappa$ denote the standard deviation
of the Gaussian noise, manifold's diameter and a bound on its extrinsic
curvature, respectively. This is the first analysis of local averaging accuracy
over the manifold in the relatively high noise regime where $\sigma \sqrt{D}
\approx \tau$. The proposed method can serve as a preprocessing step for a wide
range of provable methods designed for lower-noise regimes. Additionally, our
framework can provide a theoretical foundation for a broad spectrum of
denoising and dimensionality reduction methods that rely on local averaging
techniques.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [325] [The Within-Orbit Adaptive Leapfrog No-U-Turn Sampler](https://arxiv.org/abs/2506.18746)
*Nawaf Bou-Rabee,Bob Carpenter,Tore Selland Kleppe,Sifan Liu*

Main category: stat.CO

TL;DR: 提出WALNUTS改进NUTS，提升多尺度目标分布采样效率和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 后验分布存在多尺度几何特征，NUTS需局部动态调整蛙跳积分器步长，该问题自NUTS提出后未解决

Method: 提出WALNUTS，在模拟时间固定间隔调整蛙跳步长，从二元时间表中选最大步长控制能量误差，采用有偏渐进状态选择

Result: 在多尺度目标分布上的实证评估显示，WALNUTS较标准NUTS在采样效率和鲁棒性上有显著提升

Conclusion: WALNUTS是NUTS的有效推广，能解决局部动态调整步长问题，提升采样性能

Abstract: Locally adapting parameters within Markov chain Monte Carlo methods while
preserving reversibility is notoriously difficult. The success of the No-U-Turn
Sampler (NUTS) largely stems from its clever local adaptation of the
integration time in Hamiltonian Monte Carlo via a geometric U-turn condition.
However, posterior distributions frequently exhibit multi-scale geometries with
extreme variations in scale, making it necessary to also adapt the leapfrog
integrator's step size locally and dynamically. Despite its practical
importance, this problem has remained largely open since the introduction of
NUTS by Hoffman and Gelman (2014). To address this issue, we introduce the
Within-orbit Adaptive Leapfrog No-U-Turn Sampler (WALNUTS), a generalization of
NUTS that adapts the leapfrog step size at fixed intervals of simulated time as
the orbit evolves. At each interval, the algorithm selects the largest step
size from a dyadic schedule that keeps the energy error below a user-specified
threshold. Like NUTS, WALNUTS employs biased progressive state selection to
favor states with positions that are further from the initial point along the
orbit. Empirical evaluations on multiscale target distributions, including
Neal's funnel and the Stock-Watson stochastic volatility time-series model,
demonstrate that WALNUTS achieves substantial improvements in sampling
efficiency and robustness compared to standard NUTS.

</details>


### [326] [Bayesian decomposition using Besov priors](https://arxiv.org/abs/2506.18846)
*Andreas Horst,Babak Maboudi Afkham,Yiqiu Dong,Jakob Lemvig*

Main category: stat.CO

TL;DR: 研究线性贝叶斯逆问题，未知量含光滑和分段常数两部分，提出并比较两种先验模型，用吉布斯采样进行后验推断，在反卷积问题上验证方法有效。


<details>
  <summary>Details</summary>
Motivation: 许多逆问题中未知量由具不同规律性的多部分组成，研究含光滑和分段常数两部分的线性贝叶斯逆问题。

Method: 将未知量建模为两部分之和，对各部分赋予先验；提出两种先验模型，在参数上设超先验，用吉布斯采样进行后验推断。

Result: 在1D和2D反卷积问题上的数值结果表明，方法比单先验方法提高了重建质量，能成功估计先验参数实现平衡分解。

Conclusion: 所提方法可有效处理含不同规律性部分的线性贝叶斯逆问题，提升重建质量。

Abstract: In many inverse problems, the unknown is composed of multiple components with
different regularities, for example, in imaging problems, where the unknown can
have both rough and smooth features. We investigate linear Bayesian inverse
problems, where the unknown consists of two components: one smooth and one
piecewise constant. We model the unknown as a sum of two components and assign
individual priors on each component to impose the assumed behavior. We propose
and compare two prior models: (i) a combination of a Haar wavelet-based Besov
prior and a smoothing Besov prior, and (ii) a hierarchical Gaussian prior on
the gradient coupled with a smoothing Besov prior. To achieve a balanced
reconstruction, we place hyperpriors on the prior parameters and jointly infer
both the components and the hyperparameters. We propose Gibbs sampling schemes
for posterior inference in both prior models. We demonstrate the capabilities
of our approach on 1D and 2D deconvolution problems, where the unknown consists
of smooth parts with jumps. The numerical results indicate that our methods
improve the reconstruction quality compared to single-prior approaches and that
the prior parameters can be successfully estimated to yield a balanced
decomposition.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [327] [Social Group Bias in AI Finance](https://arxiv.org/abs/2506.17490)
*Thomas R. Cook,Sophia Kazinnik*

Main category: econ.GN

TL;DR: 论文研究金融大语言模型在信贷决策中的种族偏见，提出测试框架，发现显著差异，通过干预降低偏见，提供识别和缓解偏见工具包。


<details>
  <summary>Details</summary>
Motivation: 金融机构依赖大语言模型进行高风险决策，需防范其在部署时延续有害偏见，聚焦信贷决策任务研究种族偏见。

Method: 引入可复现的反事实测试框架评估模型对不同种族申请人的响应，进行层分析追踪敏感属性传播，部署控制向量干预。

Result: 发现显著的基于种族的差异，超过历史偏见水平；控制向量干预有效降低种族差异最高达70%，平均33%，且不损害整体模型性能。

Conclusion: 该方法为金融大语言模型部署中识别和缓解偏见提供了透明实用的工具包。

Abstract: Financial institutions increasingly rely on large language models (LLMs) for
high-stakes decision-making. However, these models risk perpetuating harmful
biases if deployed without careful oversight. This paper investigates racial
bias in LLMs specifically through the lens of credit decision-making tasks,
operating on the premise that biases identified here are indicative of broader
concerns across financial applications. We introduce a reproducible,
counterfactual testing framework that evaluates how models respond to simulated
mortgage applicants identical in all attributes except race. Our results reveal
significant race-based discrepancies, exceeding historically observed bias
levels. Leveraging layer-wise analysis, we track the propagation of sensitive
attributes through internal model representations. Building on this, we deploy
a control-vector intervention that effectively reduces racial disparities by up
to 70% (33% on average) without impairing overall model performance. Our
approach provides a transparent and practical toolkit for the identification
and mitigation of bias in financial LLM deployments.

</details>


### [328] [An AI-powered Tool for Central Bank Business Liaisons: Quantitative Indicators and On-demand Insights from Firms](https://arxiv.org/abs/2506.18505)
*Nicholas Gray,Finn Lattimore,Kate McLoughlin,Callan Windsor*

Main category: econ.GN

TL;DR: 本文介绍新文本分析和检索工具处理央行联络情报，展示其评估经济状况能力，添加文本特征可提升工资增长预测表现。


<details>
  <summary>Details</summary>
Motivation: 在政策不确定性增加背景下，央行需依赖软信息源补充传统经济统计和预测，而央行联络项目情报是有价值的软信息源。

Method: 利用现代自然语言处理技术，结合传统文本分析技术和强大语言模型开发工具，还将文本特征加入预测模型并使用机器学习方法处理多预测因子。

Result: 添加文本特征到现有最佳预测模型显著提升工资增长预测表现，且少量特征驱动预测增益。

Conclusion: 新工具能高效处理和分析央行联络情报，添加文本特征可改善工资增长预测，且信号较稀疏。

Abstract: In a world of increasing policy uncertainty, central banks are relying more
on soft information sources to complement traditional economic statistics and
model-based forecasts. One valuable source of soft information comes from
intelligence gathered through central bank liaison programs -- structured
programs in which central bank staff regularly talk with firms to gather
insights. This paper introduces a new text analytics and retrieval tool that
efficiently processes, organises, and analyses liaison intelligence gathered
from firms using modern natural language processing techniques. The textual
dataset spans 25 years, integrates new information as soon as it becomes
available, and covers a wide range of business sizes and industries. The tool
uses both traditional text analysis techniques and powerful language models to
provide analysts and researchers with three key capabilities: (1) quickly
querying the entire history of business liaison meeting notes; (2) zooming in
on particular topics to examine their frequency (topic exposure) and analysing
the associated tone and uncertainty of the discussion; and (3) extracting
precise numerical values from the text, such as firms' reported figures for
wages and prices growth. We demonstrate how these capabilities are useful for
assessing economic conditions by generating text-based indicators of wages
growth and incorporating them into a nowcasting model. We find that adding
these text-based features to current best-in-class predictive models, combined
with the use of machine learning methods designed to handle many predictors,
significantly improves the performance of nowcasts for wages growth. Predictive
gains are driven by a small number of features, indicating a sparse signal in
contrast to other predictive problems in macroeconomics, where the signal is
typically dense.

</details>


### [329] [The Theory of Economic Complexity](https://arxiv.org/abs/2506.18829)
*César A. Hidalgo,Viktor Stojkoski*

Main category: econ.GN

TL;DR: 本文为经济复杂性指标的特征向量提供解析解，解决理论难题，验证相关理念。


<details>
  <summary>Details</summary>
Motivation: 缺乏从第一原理推导经济复杂性估计中特征向量并置于机制模型的理论。

Method: 对经济产出与活动所需要素概率关系的模型进行解析计算特征向量，推广到其他生产函数和短期均衡框架。

Result: ECI是经济拥有要素概率的单调函数，多要素模型中是所有要素平均禀赋的估计；主要结果不受价格和工资引入影响；推导出的工资函数与相似复杂度经济体收敛一致；解释了相关活动网络形状。

Conclusion: 解决经济复杂性文献中长期存在的理论难题，验证经济复杂性指标是经济拥有多要素禀赋的估计这一理念。

Abstract: Economic complexity estimates rely on eigenvectors derived from matrices of
specialization to explain differences in economic growth, inequality, and
sustainability. Yet, despite their widespread use, we still lack a principled
theory that can deduce these eigenvectors from first principles and place them
in the context of a mechanistic model. Here, we calculate these eigenvectors
analytically for a model where the output of an economy in an activity
increases with the probability the economy is endowed with the factors required
by the activity. We show that the eigenvector known as the Economic Complexity
Index or ECI is a monotonic function of the probability that an economy is
endowed with a factor, and that in a multi-factor model, it is an estimate of
the average endowment across all factors. We then generalize this result to
other production functions and to a short-run equilibrium framework with
prices, wages, and consumption. We find that our main result does not depend on
the introduction of prices or wages, and that the derived wage function is
consistent with the convergence of economies with a similar level of
complexity. Finally, we use this model to explain the shape of networks of
related activities, such as the product space and the research space. These
findings solve long standing theoretical puzzles in the economic complexity
literature and validate the idea that metrics of economic complexity are
estimates of an economy being endowed with multiple factors.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [330] [Exploring Strategies for Personalized Radiation Therapy Part II Predicting Tumor Drift Patterns with Diffusion Models](https://arxiv.org/abs/2506.17491)
*Hao Peng,Steve Jiang,Robert Timmerman*

Main category: physics.med-ph

TL;DR: 本文提出用DDIM框架解决放疗中剂量和时机参数因人而异的问题，开发单步和迭代去噪策略，结果表明扩散模型有效，为放疗干预提供基础。


<details>
  <summary>Details</summary>
Motivation: 放疗结果受剂量和时机两个关键参数影响，不同患者最佳值差异大，尤其脑癌治疗中难以预测治疗反应，现有模型对肿瘤反应的时空模式洞察有限。

Method: 提出使用Denoising Diffusion Implicit Models (DDIM)的框架，开发单步和迭代去噪策略并比较性能。

Result: 扩散模型能有效模拟患者特定的肿瘤演变，定位与治疗反应相关的区域。

Conclusion: 所提出策略为建模异质性治疗反应和实现早期自适应干预提供了有前景的基础，推动放疗更个性化和符合生物学认知。

Abstract: Radiation therapy outcomes are decided by two key parameters, dose and
timing, whose best values vary substantially across patients. This variability
is especially critical in the treatment of brain cancer, where fractionated or
staged stereotactic radiosurgery improves safety compared to single fraction
approaches, but complicates the ability to predict treatment response. To
address this challenge, we employ Personalized Ultra-fractionated Stereotactic
Adaptive Radiotherapy (PULSAR), a strategy that dynamically adjusts treatment
based on how each tumor evolves over time. However, the success of PULSAR and
other adaptive approaches depends on predictive tools that can guide early
treatment decisions and avoid both overtreatment and undertreatment. However,
current radiomics and dosiomics models offer limited insight into the evolving
spatial and temporal patterns of tumor response. To overcome these limitations,
we propose a novel framework using Denoising Diffusion Implicit Models (DDIM),
which learns data-driven mappings from pre to post treatment imaging. In this
study, we developed single step and iterative denoising strategies and compared
their performance. The results show that diffusion models can effectively
simulate patient specific tumor evolution and localize regions associated with
treatment response. The proposed strategy provides a promising foundation for
modeling heterogeneous treatment response and enabling early, adaptive
interventions, paving the way toward more personalized and biologically
informed radiotherapy.

</details>


### [331] [Exploring Strategies for Personalized Radiation Therapy Part I Unlocking Response-Related Tumor Subregions with Class Activation Mapping](https://arxiv.org/abs/2506.17536)
*Hao Peng,Steve Jiang,Robert Timmerman*

Main category: physics.med-ph

TL;DR: 研究比较三种预测放疗反应的方法，分析脑转移瘤病例，像素级CAM表现最佳，对个性化放疗有潜在价值。


<details>
  <summary>Details</summary>
Motivation: 个性化精准放疗需识别预后、空间信息特征并根据个体反应调整治疗，故比较三种预测放疗反应的方法。

Method: 比较标准影像组学、基于梯度的特征和结合类激活映射的卷积神经网络三种方法，用集成自动编码器分类器模型预测肿瘤体积缩小情况。

Result: 像素级CAM在分层特征提取和分类判别能力上表现出色，分类准确性优于其他两种方法，能提供详细空间洞察，激活区域可指示放射抵抗区域。

Conclusion: 虽然需要进一步验证，但研究结果显示这些方法在指导光子和粒子疗法的个性化、自适应放疗策略上有前景。

Abstract: Personalized precision radiation therapy requires more than simple
classification, it demands the identification of prognostic, spatially
informative features and the ability to adapt treatment based on individual
response. This study compares three approaches for predicting treatment
response: standard radiomics, gradient based features, and convolutional neural
networks enhanced with Class Activation Mapping. We analyzed 69 brain
metastases from 39 patients treated with Gamma Knife radiosurgery. An
integrated autoencoder classifier model was used to predict whether tumor
volume would shrink by more than 20 percent at a three months follow up, framed
as a binary classification task. The results highlight their strength in
hierarchical feature extraction and the classifiers discriminative capacity.
Among the models, pixel wise CAM provides the most detailed spatial insight,
identifying lesion specific regions rather than relying on fixed patterns,
demonstrating strong generalization. In non responding lesions, the activated
regions may indicate areas of radio resistance. Pixel wise CAM outperformed
both radiomics and gradient based methods in classification accuracy. Moreover,
its fine grained spatial features allow for alignment with cellular level data,
supporting biological validation and deeper understanding of heterogeneous
treatment responses. Although further validation is necessary, these findings
underscore the promise in guiding personalized and adaptive radiotherapy
strategies for both photon and particle therapies.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [332] [Numerical simulation of transient heat conduction with moving heat source using Physics Informed Neural Networks](https://arxiv.org/abs/2506.17726)
*Anirudh Kalyan,Sundararajan Natarajan*

Main category: math.NA

TL;DR: 本文用PINNs模拟含移动源的传热问题，提出新训练方法，计算大时间间隔并与有限元法对比，结果吻合。


<details>
  <summary>Details</summary>
Motivation: 减少含移动源传热问题数值模拟的计算量。

Method: 采用PINNs，提出新训练方法，通过迁移学习进行连续时间步长训练，将时间区间划分并在单网络上依次训练。

Result: 所提框架用于估计含移动热源的均匀介质中的温度分布，结果与传统有限元法吻合。

Conclusion: 所提框架能在不增加网络复杂度的情况下计算大时间间隔，可用于含移动热源的传热问题模拟。

Abstract: In this paper, the physics informed neural networks (PINNs) is employed for
the numerical simulation of heat transfer involving a moving source. To reduce
the computational effort, a new training method is proposed that uses a
continuous time-stepping through transfer learning. Within this, the time
interval is divided into smaller intervals and a single network is initialized.
On this single network each time interval is trained with the initial condition
for (n+1)th as the solution obtained at nth time increment. Thus, this
framework enables the computation of large temporal intervals without
increasing the complexity of the network itself. The proposed framework is used
to estimate the temperature distribution in a homogeneous medium with a moving
heat source. The results from the proposed framework is compared with
traditional finite element method and a good agreement is seen.

</details>


### [333] [DPG loss functions for learning parameter-to-solution maps by neural networks](https://arxiv.org/abs/2506.18773)
*Pablo Cortés Castillo,Wolfgang Dahmen,Jay Gopalakrishnan*

Main category: math.NA

TL;DR: 本文针对参数相关偏微分方程族的参数 - 解映射机器学习，开发、分析并实验探索基于残差的损失函数，证明其在高对比度扩散参数问题上比简单最小二乘损失函数更稳健。


<details>
  <summary>Details</summary>
Motivation: 提升深度神经网络降阶模型的预测能力，实现严格的精度认证。

Method: 使用变分正确的损失函数，以椭圆偏微分方程为例，从超弱间断Petrov Galerkin (DPG)离散化推导损失函数的变分正确性。

Result: 数值结果和理论论证表明，对于高对比度扩散参数，所提出的DPG损失函数比简单最小二乘损失函数性能更稳健。

Conclusion: 所提出的概念适用于有稳定DPG公式的更广泛问题，DPG损失函数在高对比度扩散问题上表现更好。

Abstract: We develop, analyze, and experimentally explore residual-based loss functions
for machine learning of parameter-to-solution maps in the context of
parameter-dependent families of partial differential equations (PDEs). Our
primary concern is on rigorous accuracy certification to enhance prediction
capability of resulting deep neural network reduced models. This is achieved by
the use of variationally correct loss functions. Through one specific example
of an elliptic PDE, details for establishing the variational correctness of a
loss function from an ultraweak Discontinuous Petrov Galerkin (DPG)
discretization are worked out. Despite the focus on the example, the proposed
concepts apply to a much wider scope of problems, namely problems for which
stable DPG formulations are available. The issue of {high-contrast} diffusion
fields and ensuing difficulties with degrading ellipticity are discussed. Both
numerical results and theoretical arguments illustrate that for high-contrast
diffusion parameters the proposed DPG loss functions deliver much more robust
performance than simpler least-squares losses.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [334] [Resolving the Ti-V Phase Diagram Discrepancy with First-Principles Calculations and Bayesian Learning](https://arxiv.org/abs/2506.17719)
*Timofei Miryashkin,Olga Klimanova,Alexander Shapeev*

Main category: cond-mat.mtrl-sci

TL;DR: 用从头算+机器学习工作流研究Ti - V二元合金，结果支持存在BCC混溶间隙且非杂质效应。


<details>
  <summary>Details</summary>
Motivation: 解决关于Ti - V二元合金是否存在体心立方（BCC）混溶间隙的实验冲突。

Method: 使用结合主动训练的矩张量势和贝叶斯热力学推理的从头算+机器学习工作流。

Result: 得到全成分范围的Ti - V二元体系及热力学极限下的置信区间，相图重现所有实验特征，支持存在BCC混溶间隙。

Conclusion: 混溶间隙不能归因于杂质效应，与近期CALPHAD重新评估结果矛盾。

Abstract: Conflicting experiments disagree on whether the titanium-vanadium (Ti-V)
binary alloy exhibits a body-centred cubic (BCC) miscibility gap or remains
completely soluble. A leading hypothesis attributes the miscibility gap to
oxygen contamination during alloy preparation. To resolve this controversy, we
use an ab initio + machine-learning workflow that couples an actively-trained
Moment Tensor Potential to Bayesian thermodynamic inference. Using this
workflow, we obtain Ti-V binary system across the entire composition range,
together with confidence intervals in the thermodynamic limit. The resulting
diagram reproduces all experimental features, demonstrating the robustness of
our approach, and clearly favors the variant with a BCC miscibility gap
terminating at T = 980 K and c = 0.67. Because oxygen was excluded from
simulations, the gap cannot be attributed to impurity effects, contradicting
recent CALPHAD reassessments.

</details>


### [335] [Residual Connection-Enhanced ConvLSTM for Lithium Dendrite Growth Prediction](https://arxiv.org/abs/2506.17756)
*Hosung Lee,Byeongoh Hwang,Dasan Kim,Myungjoo Kang*

Main category: cond-mat.mtrl-sci

TL;DR: 本文提出残差连接增强的ConvLSTM模型预测锂枝晶生长模式，实验表明比传统ConvLSTM更准确高效，为电池诊断提供工具。


<details>
  <summary>Details</summary>
Motivation: 锂枝晶生长影响可充电电池性能和安全，需准确预测枝晶生长模式。

Method: 提出残差连接增强的ConvLSTM模型，用相场模型生成数据集。

Result: 模型在不同电压条件下比传统ConvLSTM准确率高7%，均方误差显著降低。

Conclusion: 残差连接在电化学系统建模的深度时空网络中有效，为电池诊断提供有力工具，未来可扩展到其他电池化学体系。

Abstract: The growth of lithium dendrites significantly impacts the performance and
safety of rechargeable batteries, leading to short circuits and capacity
degradation. This study proposes a Residual Connection-Enhanced ConvLSTM model
to predict dendrite growth patterns with improved accuracy and computational
efficiency. By integrating residual connections into ConvLSTM, the model
mitigates the vanishing gradient problem, enhances feature retention across
layers, and effectively captures both localized dendrite growth dynamics and
macroscopic battery behavior. The dataset was generated using a phase-field
model, simulating dendrite evolution under varying conditions. Experimental
results show that the proposed model achieves up to 7% higher accuracy and
significantly reduces mean squared error (MSE) compared to conventional
ConvLSTM across different voltage conditions (0.1V, 0.3V, 0.5V). This
highlights the effectiveness of residual connections in deep spatiotemporal
networks for electrochemical system modeling. The proposed approach offers a
robust tool for battery diagnostics, potentially aiding in real-time monitoring
and optimization of lithium battery performance. Future research can extend
this framework to other battery chemistries and integrate it with real-world
experimental data for further validation

</details>


### [336] [CLOUD: A Scalable and Physics-Informed Foundation Model for Crystal Representation Learning](https://arxiv.org/abs/2506.17345)
*Changwen Xu,Shang Zhu,Venkatasubramanian Viswanathan*

Main category: cond-mat.mtrl-sci

TL;DR: 本文介绍CLOUD框架，基于新编码方式训练，预训练后微调用于多任务，预测性能好，还结合Debye模型实现热学性质预测，展现出作为晶体材料基础模型的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统晶体性质预测方法资源消耗大，现有机器学习模型存在依赖标签数据、表征能力弱、缺乏物理原理融合等问题，限制了泛化性和可解释性。

Method: 引入基于transformer的CLOUD框架，采用新的SCOPE编码方式，在超六百万晶体结构上预训练，后在多下游任务微调；应用CLOUD - DEBYE框架结合Debye模型预测热学性质。

Result: CLOUD在预测多种材料性质上表现有竞争力，展现出强扩展性；CLOUD - DEBYE框架能在不额外数据下实现温度相关性质预测。

Conclusion: CLOUD有潜力成为可扩展、结合物理知识的晶体材料基础模型，统一对称一致表征与基于物理的学习用于性质预测和材料发现。

Abstract: The prediction of crystal properties is essential for understanding
structure-property relationships and accelerating the discovery of functional
materials. However, conventional approaches relying on experimental
measurements or density functional theory (DFT) calculations are often
resource-intensive, limiting their scalability. Machine learning (ML) models
offer a promising alternative by learning complex structure-property
relationships from data, enabling faster predictions. Yet, existing ML models
often rely on labeled data, adopt representations that poorly capture essential
structural characteristics, and lack integration with physical
principles--factors that limit their generalizability and interpretability.
Here, we introduce CLOUD (Crystal Language mOdel for Unified and Differentiable
materials modeling), a transformer-based framework trained on a novel
Symmetry-Consistent Ordered Parameter Encoding (SCOPE) that encodes crystal
symmetry, Wyckoff positions, and composition in a compact, coordinate-free
string representation. Pre-trained on over six million crystal structures,
CLOUD is fine-tuned on multiple downstream tasks and achieves competitive
performance in predicting a wide range of material properties, demonstrating
strong scaling performance. Furthermore, as proof of concept of differentiable
materials modeling, CLOUD is applied to predict the phonon internal energy and
heat capacity, which integrates the Debye model to preserve thermodynamic
consistency. The CLOUD-DEBYE framework enforces thermodynamic consistency and
enables temperature-dependent property prediction without requiring additional
data. These results demonstrate the potential of CLOUD as a scalable and
physics-informed foundation model for crystalline materials, unifying
symmetry-consistent representations with physically grounded learning for
property prediction and materials discovery.

</details>


### [337] [Leveraging neural network interatomic potentials for a foundation model of chemistry](https://arxiv.org/abs/2506.18497)
*So Yeon Kim,Yang Jeong Park,Ju Li*

Main category: cond-mat.mtrl-sci

TL;DR: 本文引入HackNIP两阶段管道方法，利用预训练NIP解决机器学习在材料科学中的权衡问题，并进行多方面测试和分析。


<details>
  <summary>Details</summary>
Motivation: NIP在直接预测电子特性有挑战，机器学习方法存在可泛化性和数据计算量的权衡问题，需新方法解决。

Method: 引入HackNIP两阶段管道，先从NIP基础模型提取特征向量，再用其训练浅层ML模型进行下游预测。

Result: 在Matbench上进行基准测试，评估数据效率，测试多种任务，分析嵌入深度对性能的影响。

Conclusion: 提出的混合策略可克服材料科学中机器学习的权衡问题，推动高性能预测建模的普及。

Abstract: Large-scale foundation models, including neural network interatomic
potentials (NIPs) in computational materials science, have demonstrated
significant potential. However, despite their success in accelerating atomistic
simulations, NIPs face challenges in directly predicting electronic properties
and often require coupling to higher-scale models or extensive simulations for
macroscopic properties. Machine learning (ML) offers alternatives for
structure-to-property mapping but faces trade-offs: feature-based methods often
lack generalizability, while deep neural networks require significant data and
computational power. To address these trade-offs, we introduce HackNIP, a
two-stage pipeline that leverages pretrained NIPs. This method first extracts
fixed-length feature vectors (embeddings) from NIP foundation models and then
uses these embeddings to train shallow ML models for downstream
structure-to-property predictions. This study investigates whether such a
hybridization approach, by ``hacking" the NIP, can outperform end-to-end deep
neural networks, determines the dataset size at which this transfer learning
approach surpasses direct fine-tuning of the NIP, and identifies which NIP
embedding depths yield the most informative features. HackNIP is benchmarked on
Matbench, evaluated for data efficiency, and tested on diverse tasks including
\textit{ab initio}, experimental, and molecular properties. We also analyze how
embedding depth impacts performance. This work demonstrates a hybridization
strategy to overcome ML trade-offs in materials science, aiming to democratize
high-performance predictive modeling.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [338] [Universal Solvability for Robot Motion Planning on Graphs](https://arxiv.org/abs/2506.18755)
*Anubhav Dhar,Ashlesha Hota,Sudeshna Kolay,Pranav Nyati,Tanishq Prasad*

Main category: cs.CC

TL;DR: 研究图上机器人运动规划通用可解性问题，设计算法分析配置可达性，还研究图边、顶点和边增强问题并给出相关界限。


<details>
  <summary>Details</summary>
Motivation: 解决图上机器人运动规划的通用可解性判定及图增强以实现通用可解性的问题。

Method: 设计规范积累程序，利用等价类分析配置可达性，设计随机算法并去随机化，结合图结构优化算法。

Result: 在非通用可解实例中至少一半配置不可达，得到高效随机算法和确定性算法运行时间，给出图边增强和顶点与边增强问题的界限。

Conclusion: 所提方法能有效解决图上机器人运动规划通用可解性及相关图增强问题。

Abstract: We study the Universal Solvability of Robot Motion Planning on Graphs (USolR)
problem: given an undirected graph G = (V, E) and p robots, determine whether
any arbitrary configuration of the robots can be transformed into any other
arbitrary configuration via a sequence of valid, collision-free moves. We
design a canonical accumulation procedure that maps arbitrary configurations to
configurations that occupy a fixed subset of vertices, enabling us to analyze
configuration reachability in terms of equivalence classes. We prove that in
instances that are not universally solvable, at least half of all
configurations are unreachable from a given one, and leverage this to design an
efficient randomized algorithm with one-sided error, which can be derandomized
with a blow-up in the running time by a factor of p. Further, we optimize our
deterministic algorithm by using the structure of the input graph G = (V, E),
achieving a running time of O(p * (|V| + |E|)) in sparse graphs and O(|V| +
|E|) in dense graphs. Finally, we consider the Graph Edge Augmentation for
Universal Solvability (EAUS) problem, where given a connected graph G that is
not universally solvable for p robots, the question is to check if for a given
budget b, at most b edges can be added to G to make it universally solvable for
p robots. We provide an upper bound of p - 2 on b for general graphs. On the
other hand, we also provide examples of graphs that require Theta(p) edges to
be added. We further study the Graph Vertex and Edge Augmentation for Universal
Solvability (VEAUS) problem, where a vertices and b edges can be added, and we
provide lower bounds on a and b.

</details>


### [339] [New Hardness Results for Low-Rank Matrix Completion](https://arxiv.org/abs/2506.18440)
*Dror Chawin,Ishay Haviv*

Main category: cs.CC

TL;DR: 本文给出低秩矩阵补全问题新的NP难结果，加强已有结论并解决有界无穷范数情况。


<details>
  <summary>Details</summary>
Motivation: 低秩矩阵补全问题在多领域有应用，需研究其复杂度。

Method: 运用图的近似正交表示、线有向图概念及扰动单位矩阵秩的界进行证明。

Result: 给出特定条件下低秩矩阵补全问题是NP难的结果，加强前人结论并解决有界无穷范数情况。

Conclusion: 新结果拓展了低秩矩阵补全问题NP难结果的范围。

Abstract: The low-rank matrix completion problem asks whether a given real matrix with
missing values can be completed so that the resulting matrix has low rank or is
close to a low-rank matrix. The completed matrix is often required to satisfy
additional structural constraints, such as positive semi-definiteness or a
bounded infinity norm. The problem arises in various research fields, including
machine learning, statistics, and theoretical computer science, and has broad
real-world applications.
  This paper presents new $\mathsf{NP} $-hardness results for low-rank matrix
completion problems. We show that for every sufficiently large integer $d$ and
any real number $\varepsilon \in [ 2^{-O(d)},\frac{1}{7}]$, given a partial
matrix $A$ with exposed values of magnitude at most $1$ that admits a positive
semi-definite completion of rank $d$, it is $\mathsf{NP}$-hard to find a
positive semi-definite matrix that agrees with each given value of $A$ up to an
additive error of at most $\varepsilon$, even when the rank is allowed to
exceed $d$ by a multiplicative factor of $O (\frac{1}{\varepsilon ^2 \cdot
\log(1/\varepsilon)} )$. This strengthens a result of Hardt, Meka, Raghavendra,
and Weitz (COLT, 2014), which applies to multiplicative factors smaller than
$2$ and to $\varepsilon $ that decays polynomially in $d$. We establish similar
$\mathsf{NP}$-hardness results for the case where the completed matrix is
constrained to have a bounded infinity norm (rather than be positive
semi-definite), for which all previous hardness results rely on complexity
assumptions related to the Unique Games Conjecture. Our proofs involve a novel
notion of nearly orthonormal representations of graphs, the concept of line
digraphs, and bounds on the rank of perturbed identity matrices.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [340] [Residue Number System (RNS) based Distributed Quantum Multiplication](https://arxiv.org/abs/2506.17588)
*Bhaskar Gaur,Himanshu Thapliyal*

Main category: quant-ph

TL;DR: 提出基于剩余数系统（RNS）的分布式量子乘法，设计量子减1模$(2^n + 1)$乘法器，与非分布式量子乘法器比较，Toffoli深度最多降低46.018%，T门减少34.483% - 86.25%。


<details>
  <summary>Details</summary>
Motivation: 现有量子乘法器电路Toffoli深度高、T门使用多，影响可扩展性和适用性。

Method: 利用RNS进行分布式量子乘法，设计量子减1模$(2^n + 1)$乘法器，估算量子资源使用并与非分布式乘法器对比。

Result: 比较分析得出Toffoli深度最多降低46.018%，T门减少34.483% - 86.25%。

Conclusion: 基于RNS的分布式量子乘法能有效降低Toffoli深度和T门使用，提高量子乘法器性能。

Abstract: Multiplication of quantum states is a frequently used function or subroutine
in quantum algorithms and applications, making quantum multipliers an essential
component of quantum arithmetic. However, quantum multiplier circuits suffer
from high Toffoli depth and T gate usage, which ultimately affects their
scalability and applicability on quantum computers. To address these issues, we
propose utilizing the Residue Number System (RNS) based distributed quantum
multiplication, which executes multiple quantum modulo multiplication circuits
across quantum computers or jobs with lower Toffoli depth and T gate usage.
Towards this end, we propose a design of Quantum Diminished-1 Modulo $(2^n+1)$
Multiplier, an essential component of RNS based distributed quantum
multiplication. We provide estimates of quantum resource usage and compare them
with those of an existing non-distributed quantum multiplier for 6 to 16 qubit
sized output. Our comparative analysis estimates up to 46.018% lower Toffoli
depth, and reduction in T gates of 34.483% to 86.25%.

</details>


### [341] [Bloch Vector Assertions for Debugging Quantum Programs](https://arxiv.org/abs/2506.18458)
*Noah H. Oldfield,Christoph Laaber,Shaukat Ali*

Main category: quant-ph

TL;DR: 论文提出Bloq方法解决量子程序调试难题，还引入AutoBloq自动生成断言方案，实验显示Bloq优于现有方法，有良好扩展性和效率。


<details>
  <summary>Details</summary>
Motivation: 量子程序调试因量子特性困难，现有断言调试方法有手动生成断言、依赖中间测量和可扩展性差等问题。

Method: 提出Bloq方法，引入基于Bloch向量的断言，利用Pauli算子期望值测量进行低开销故障定位；引入AutoBloq自动生成断言方案。

Result: 在684432个程序实验中，Bloq在F1分数、运行时间和电路深度开销等方面均优于Proq。

Conclusion: Bloq使基于断言的调试对近期量子设备具有可扩展性和有效性。

Abstract: Quantum programs must be reliable to ensure trustworthy results, yet
debugging them is notoriously challenging due to quantum-specific faults like
gate misimplementations and hardware noise, as well as their inherently
probabilistic nature. Assertion-based debugging provides a promising solution
by enabling localized correctness checks during execution. However, current
approaches face challenges including manual assertion generation, reliance on
mid-circuit-measurements, and poor scalability. In this paper, we present Bloq,
a scalable, automated fault localization approach introducing
Bloch-vector-based assertions utilizing expectation value measurements of Pauli
operators, enabling low-overhead fault localization without mid-circuit
measurements. In addition, we introduce AutoBloq, a component of Bloq for
automatically generating assertion schemes from quantum algorithms. An
experimental evaluation over 684432 programs using two algorithms (Quantum
Fourier Transform (QFT) and Grover) shows that Bloq consistently outperforms
the state-of-the-art approach Proq, notably as circuit depth and noise
increase. For Grover, Bloq achieves a mean F1 score across all experimental
instances of 0.74 versus 0.38 for Proq under ideal conditions, and maintains
performance under noise (0.43 versus 0.06). Bloq also reduces Proq's runtime by
a factor of 5 and circuit depth overhead by a factor of 23. These results
underline Bloq's potential to make assertion-based debugging scalable and
effective for near-term quantum devices.

</details>


### [342] [Quantum-Hybrid Support Vector Machines for Anomaly Detection in Industrial Control Systems](https://arxiv.org/abs/2506.17824)
*Tyler Cultice,Md. Saif Hassan Onim,Annarita Giani,Himanshu Thapliyal*

Main category: quant-ph

TL;DR: 研究用三个流行数据集对量子混合支持向量机（QSVM）参数化，结果显示QSVM在异常检测上优于传统方法，有‘量子优势’，可提升关键基础设施安全。


<details>
  <summary>Details</summary>
Motivation: 工业控制系统（ICS）中敏感数据对关键基础设施安全至关重要，量子核机器学习方法在识别复杂异常行为有潜力，研究QSVM在异常检测的应用。

Method: 用来自网络物理系统（CPS）的三个流行数据集对QSVM参数化，基于真实IBMQ硬件进行模拟研究噪声。

Result: QSVM比传统经典核方法F1分数高13.3%；QSVM核最大误差0.98%，分类指标平均降低1.57%；QSVM核目标对齐比经典方法提高91.023%。

Conclusion: QSVM在ICS异常检测中有显著优势，能增强关键基础设施的安全和完整性。

Abstract: Sensitive data captured by Industrial Control Systems (ICS) play a large role
in the safety and integrity of many critical infrastructures. Detection of
anomalous or malicious data, or Anomaly Detection (AD), with machine learning
is one of many vital components of cyberphysical security. Quantum kernel-based
machine learning methods have shown promise in identifying complex anomalous
behavior by leveraging the highly expressive and efficient feature spaces of
quantum computing. This study focuses on the parameterization of Quantum Hybrid
Support Vector Machines (QSVMs) using three popular datasets from
Cyber-Physical Systems (CPS). The results demonstrate that QSVMs outperform
traditional classical kernel methods, achieving 13.3% higher F1 scores.
Additionally, this research investigates noise using simulations based on real
IBMQ hardware, revealing a maximum error of only 0.98% in the QSVM kernels.
This error results in an average reduction of 1.57% in classification metrics.
Furthermore, the study found that QSVMs show a 91.023% improvement in
kernel-target alignment compared to classical methods, indicating a potential
"quantum advantage" in anomaly detection for critical infrastructures. This
effort suggests that QSVMs can provide a substantial advantage in anomaly
detection for ICS, ultimately enhancing the security and integrity of critical
infrastructures.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [343] [Design, Implementation, and Analysis of Fair Faucets for Blockchain Ecosystems](https://arxiv.org/abs/2506.17236)
*Serdar Metin*

Main category: cs.CR

TL;DR: 本文针对非商业区块链网络中共享资源公平分配问题，改进水龙头机制，提出6种最大最小公平算法，抗DoS攻击且成本低。


<details>
  <summary>Details</summary>
Motivation: 商业区块链网络资源可用法定货币交换，公平性不是问题；非商业网络无货币解决方案，现有水龙头机制易受DoS攻击且无法解决公平问题。

Method: 将水龙头机制按照最大最小公平方案进行调整，提出6种不同的最大最小公平算法。

Result: 贡献了6种高效的区块链水龙头算法，算法抗DoS攻击，在区块链计算经济方面成本低，且支持不同用户加权策略。

Conclusion: 通过改进水龙头机制提出的算法能解决非商业区块链网络中共享资源的公平分配问题。

Abstract: The present dissertation addresses the problem of fairly distributing shared
resources in non-commercial blockchain networks. Blockchains are distributed
systems that order and timestamp records of a given network of users, in a
public, cryptographically secure, and consensual way. The records, which may in
kind be events, transaction orders, sets of rules for structured transactions
etc. are placed within well-defined datastructures called blocks, and they are
linked to each other by the virtue of cryptographic pointers, in a total
ordering which represents their temporal relations of succession. The ability
to operate on the blockchain, and/or to contribute a record to the content of a
block are shared resources of the blockchain systems. In commercial networks,
these resources are exchanged in return for fiat money, and consequently,
fairness is not a relevant problem in terms of computer engineering. In
non-commercial networks, however, monetary solutions are not available, by
definition. The present non-commercial blockchain networks employ trivial
distribution mechanisms called faucets, which offer fixed amounts of free
tokens (called cryptocurrencies) specific to the given network. This mechanism,
although simple and efficient, is prone to denial of service (DoS) attacks and
cannot address the fairness problem. In the present dissertation, the faucet
mechanism is adapted for fair distribution, in line with Max-min Fairness
scheme. In total, we contributed 6 distinct Max-min Fair algorithms as
efficient blockchain faucets. The algorithms we contribute are resistant to DoS
attacks, low-cost in terms of blockchain computation economics, and they also
allow for different user weighting policies.

</details>


### [344] [A Locally Differential Private Coding-Assisted Succinct Histogram Protocol](https://arxiv.org/abs/2506.17767)
*Hsuan-Po Liu,Hessam Mahdavifar*

Main category: cs.CR

TL;DR: 提出用纠错码构建简洁直方图的实用(ε,δ)-LDP协议，实验显示该方法优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 为简洁直方图问题开发严格隐私框架，在LDP下保留数据效用。

Method: 利用极化码及其连续消除列表（SCL）解码算法作为底层编码方案，引入基于高斯的扰动以实现高效软解码。

Result: 实验表明该方法优于先前方法，尤其对于真实频率低的项目，同时保持相似的频率估计精度。

Conclusion: 所提出的协议在构建简洁直方图方面有较好效果，能提升性能。

Abstract: A succinct histogram captures frequent items and their frequencies across
clients and has become increasingly important for large-scale,
privacy-sensitive machine learning applications. To develop a rigorous
framework to guarantee privacy for the succinct histogram problem, local
differential privacy (LDP) has been utilized and shown promising results. To
preserve data utility under LDP, which essentially works by intentionally
adding noise to data, error-correcting codes naturally emerge as a promising
tool for reliable information collection. This work presents the first
practical $(\epsilon,\delta)$-LDP protocol for constructing succinct histograms
using error-correcting codes. To this end, polar codes and their
successive-cancellation list (SCL) decoding algorithms are leveraged as the
underlying coding scheme. More specifically, our protocol introduces
Gaussian-based perturbations to enable efficient soft decoding. Experiments
demonstrate that our approach outperforms prior methods, particularly for items
with low true frequencies, while maintaining similar frequency estimation
accuracy.

</details>


### [345] [Tracking GPTs Third Party Service: Automation, Analysis, and Insights](https://arxiv.org/abs/2506.17315)
*Chuan Yan,Liuhuo Wan,Bowei Guan,Fengqi Yu,Guangdong Bai,Jin Song Dong*

Main category: cs.CR

TL;DR: 本文介绍了用于提取GPT隐私设置的自动化框架GPTs - ThirdSpy，以支持对GPT第三方服务集成的学术研究。


<details>
  <summary>Details</summary>
Motivation: ChatGPT发展迅速，GPT可集成第三方服务，但隐私设置信息披露方式不利于评估第三方集成的隐私影响，需要支持相关学术研究。

Method: 引入自动化框架GPTs - ThirdSpy来提取GPT的隐私设置。

Result: GPTs - ThirdSpy能为学术研究人员提供实时、可靠的第三方服务元数据，便于对集成、合规性和潜在安全风险进行深入分析。

Conclusion: GPTs - ThirdSpy有助于对GPT应用生态系统的透明度和监管挑战进行大规模研究。

Abstract: ChatGPT has quickly advanced from simple natural language processing to
tackling more sophisticated and specialized tasks. Drawing inspiration from the
success of mobile app ecosystems, OpenAI allows developers to create
applications that interact with third-party services, known as GPTs. GPTs can
choose to leverage third-party services to integrate with specialized APIs for
domain-specific applications. However, the way these disclose privacy setting
information limits accessibility and analysis, making it challenging to
systematically evaluate the data privacy implications of third-party integrate
to GPTs. In order to support academic research on the integration of
third-party services in GPTs, we introduce GPTs-ThirdSpy, an automated
framework designed to extract privacy settings of GPTs. GPTs-ThirdSpy provides
academic researchers with real-time, reliable metadata on third-party services
used by GPTs, enabling in-depth analysis of their integration, compliance, and
potential security risks. By systematically collecting and structuring this
data, GPTs-ThirdSpy facilitates large-scale research on the transparency and
regulatory challenges associated with the GPT app ecosystem.

</details>


### [346] [Smart-LLaMA-DPO: Reinforced Large Language Model for Explainable Smart Contract Vulnerability Detection](https://arxiv.org/abs/2506.18245)
*Lei Yu,Zhirong Huang,Hang Yuan,Shiqi Cheng,Li Yang,Fengjun Zhang,Chenjie Shen,Jiajia Ma,Jingyuan Zhang,Junyi Lu,Chun Zuo*

Main category: cs.CR

TL;DR: 针对智能合约漏洞检测难题，提出Smart - LLaMA - DPO模型，实验效果超基线。


<details>
  <summary>Details</summary>
Motivation: 现有数据集覆盖不全且缺优质解释，大语言模型难准确解读智能合约安全概念。

Method: 基于LLaMA - 3.1 - 8B构建Smart - LLaMA - DPO，构建综合数据集，进行持续预训练、监督微调与直接偏好优化。

Result: 在四种主要漏洞类型及机器难审计漏洞上评估，F1分数平均提高10.43%，准确率提高7.87%。

Conclusion: 该方法显著优于基线，生成解释更正确、全面、清晰。

Abstract: Smart contract vulnerability detection remains a major challenge in
blockchain security. Existing vulnerability detection methods face two main
issues: (1) Existing datasets lack comprehensive coverage and high-quality
explanations for preference learning. (2) Large language models (LLMs) often
struggle with accurately interpreting specific concepts in smart contract
security. Empirical analysis shows that even after continual pre-training (CPT)
and supervised fine-tuning (SFT), LLMs may misinterpret the execution order of
state changes, resulting in incorrect explanations despite making correct
detection decisions. To address these challenges, we propose Smart-LLaMA-DPO
based on LLaMA-3.1-8B. We construct a comprehensive dataset covering four major
vulnerability types and machine-unauditable vulnerabilities, including precise
labels, explanations, and locations for SFT, as well as high-quality and
low-quality output pairs for Direct Preference Optimization (DPO). Second, we
perform CPT using large-scale smart contract to enhance the LLM's understanding
of specific security practices in smart contracts. Futhermore, we conduct SFT
with our comprehensive dataset. Finally, we apply DPO, leveraging human
feedback and a specially designed loss function that increases the probability
of preferred explanations while reducing the likelihood of non-preferred
outputs. We evaluate Smart-LLaMA-DPO on four major vulnerability types:
reentrancy, timestamp dependence, integer overflow/underflow, and delegatecall,
as well as machine-unauditable vulnerabilities. Our method significantly
outperforms state-of-the-art baselines, with average improvements of 10.43% in
F1 score and 7.87% in accuracy. Moreover, both LLM evaluation and human
evaluation confirm that our method generates more correct, thorough, and clear
explanations.

</details>


### [347] [Automatic Selection of Protections to Mitigate Risks Against Software Applications](https://arxiv.org/abs/2506.18470)
*Daniele Canavese,Leonardo Regano,Bjorn De Sutter,Cataldo Basile*

Main category: cs.CR

TL;DR: 提出自动化选择软件保护的新方法，通过博弈论模型和启发式算法解决问题，引入软件保护指数，经验证有效。


<details>
  <summary>Details</summary>
Motivation: 缓解软件应用关键资产面临的MATE风险，寻找自动化选择软件保护的方法。

Method: 将保护决策关键元素形式化，构建博弈论模型，用基于极小极大深度优先探索策略的启发式算法求解，加入动态规划优化，引入软件保护指数。

Result: 通过概念验证实现和专家评估验证了方法。

Conclusion: 自动化软件保护是软件风险缓解的实用有效解决方案。

Abstract: This paper introduces a novel approach for the automated selection of
software protections to mitigate MATE risks against critical assets within
software applications. We formalize the key elements involved in protection
decision-making - including code artifacts, assets, security requirements,
attacks, and software protections - and frame the protection process through a
game-theoretic model. In this model, a defender strategically applies
protections to various code artifacts of a target application, anticipating
repeated attack attempts by adversaries against the confidentiality and
integrity of the application's assets. The selection of the optimal defense
maximizes resistance to attacks while ensuring the application remains usable
by constraining the overhead introduced by protections. The game is solved
through a heuristic based on a mini-max depth-first exploration strategy,
augmented with dynamic programming optimizations for improved efficiency.
Central to our formulation is the introduction of the Software Protection
Index, an original contribution that extends existing notions of potency and
resilience by evaluating protection effectiveness against attack paths using
software metrics and expert assessments. We validate our approach through a
proof-of-concept implementation and expert evaluations, demonstrating that
automated software protection is a practical and effective solution for risk
mitigation in software.

</details>


### [348] [FORGE: An LLM-driven Framework for Large-Scale Smart Contract Vulnerability Dataset Construction](https://arxiv.org/abs/2506.18795)
*Jiachi Chen,Yiming Shen,Jiashuo Zhang,Zihao Li,John Grundy,Zhenzhe Shao,Yanlin Wang,Jiashui Wang,Ting Chen,Zibin Zheng*

Main category: cs.CR

TL;DR: 提出FORGE自动构建智能合约漏洞数据集，评估有效且揭示现有检测能力局限和研究重点不一致问题


<details>
  <summary>Details</summary>
Motivation: 解决当前手动构建数据集劳动密集、易出错和缺乏标准分类规则的局限

Method: 利用LLM驱动管道从审计报告提取漏洞，按CWE分类，采用分治法提取信息，用思维树技术分类

Result: 处理6454份报告生成含81390个文件和27497个漏洞的数据集，手动评估精度高，基准测试揭示现有工具检测能力局限

Conclusion: 当前智能合约研究重点与实际漏洞优先级存在不一致

Abstract: High-quality smart contract vulnerability datasets are critical for
evaluating security tools and advancing smart contract security research. Two
major limitations of current manual dataset construction are (1)
labor-intensive and error-prone annotation processes limiting the scale,
quality, and evolution of the dataset, and (2) absence of standardized
classification rules results in inconsistent vulnerability categories and
labeling results across different datasets. To address these limitations, we
present FORGE, the first automated approach for constructing smart contract
vulnerability datasets. FORGE leverages an LLM-driven pipeline to extract
high-quality vulnerabilities from real-world audit reports and classify them
according to the CWE, the most widely recognized classification in software
security. FORGE employs a divide-and-conquer strategy to extract structured and
self-contained vulnerability information from these reports. Additionally, it
uses a tree-of-thoughts technique to classify the vulnerability information
into the hierarchical CWE classification. To evaluate FORGE's effectiveness, we
run FORGE on 6,454 real-world audit reports and generate a dataset comprising
81,390 solidity files and 27,497 vulnerability findings across 296 CWE
categories. Manual assessment of the dataset demonstrates high extraction
precision and classification consistency with human experts (precision of 95.6%
and inter-rater agreement k-$\alpha$ of 0.87). We further validate the
practicality of our dataset by benchmarking 13 existing security tools on our
dataset. The results reveal the significant limitations in current detection
capabilities. Furthermore, by analyzing the severity-frequency distribution
patterns through a unified CWE perspective in our dataset, we highlight
inconsistency between current smart contract research focus and priorities
identified from real-world vulnerabilities...

</details>


### [349] [Step-by-Step Reasoning Attack: Revealing 'Erased' Knowledge in Large Language Models](https://arxiv.org/abs/2506.17279)
*Yash Sinha,Manit Baser,Murari Mandal,Dinil Mon Divakaran,Mohan Kankanhalli*

Main category: cs.CR

TL;DR: 论文指出现有大语言模型知识擦除方法存在问题，引入基于逐步推理的黑盒攻击Sleek暴露其失败之处，评估显示现有方法不可靠，强调需更稳健的擦除策略。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型知识擦除方法不能真正移除知识，存在信息可恢复风险，需暴露这些问题以改进擦除策略。

Method: 引入基于逐步推理的黑盒攻击Sleek，采用包含对抗性提示生成策略、攻击机制和提示分类的结构化攻击框架。

Result: 对四种先进擦除技术和两种常用大语言模型评估显示，62.5%的对抗性提示能从WHP - unlearned Llama中恢复遗忘的哈利·波特事实，50%暴露了对保留知识的不公平抑制。

Conclusion: 现有方法无法确保可靠的知识移除，存在信息泄露风险，需要更稳健的知识擦除策略。

Abstract: Knowledge erasure in large language models (LLMs) is important for ensuring
compliance with data and AI regulations, safeguarding user privacy, mitigating
bias, and misinformation. Existing unlearning methods aim to make the process
of knowledge erasure more efficient and effective by removing specific
knowledge while preserving overall model performance, especially for retained
information. However, it has been observed that the unlearning techniques tend
to suppress and leave the knowledge beneath the surface, thus making it
retrievable with the right prompts. In this work, we demonstrate that
\textit{step-by-step reasoning} can serve as a backdoor to recover this hidden
information. We introduce a step-by-step reasoning-based black-box attack,
Sleek, that systematically exposes unlearning failures. We employ a structured
attack framework with three core components: (1) an adversarial prompt
generation strategy leveraging step-by-step reasoning built from LLM-generated
queries, (2) an attack mechanism that successfully recalls erased content, and
exposes unfair suppression of knowledge intended for retention and (3) a
categorization of prompts as direct, indirect, and implied, to identify which
query types most effectively exploit unlearning weaknesses. Through extensive
evaluations on four state-of-the-art unlearning techniques and two widely used
LLMs, we show that existing approaches fail to ensure reliable knowledge
removal. Of the generated adversarial prompts, 62.5% successfully retrieved
forgotten Harry Potter facts from WHP-unlearned Llama, while 50% exposed unfair
suppression of retained knowledge. Our work highlights the persistent risks of
information leakage, emphasizing the need for more robust unlearning strategies
for erasure.

</details>


### [350] [Theoretically Unmasking Inference Attacks Against LDP-Protected Clients in Federated Vision Models](https://arxiv.org/abs/2506.17292)
*Quan Nguyen,Minh N. Vu,Truc Nguyen,My T. Thai*

Main category: cs.CR

TL;DR: 研究在本地差分隐私（LDP）保护下的成员推理攻击（MIAs），推导低多项式时间MIAs成功率的理论下界，证实隐私风险并指出降噪对模型效用的影响。


<details>
  <summary>Details</summary>
Motivation: 现有对MIAs的研究大多忽略LDP或未对LDP保护数据的攻击成功率提供理论保证，需要填补此空白。

Method: 推导利用全连接或自注意力层漏洞的低多项式时间MIAs成功率的理论下界。

Result: 即使数据受LDP保护，仍存在隐私风险，且取决于隐私预算；联邦视觉模型的实际评估显示有显著隐私风险，降噪会大幅降低模型效用。

Conclusion: 在LDP保护下，隐私风险依然存在，降低隐私风险会影响模型效用。

Abstract: Federated Learning enables collaborative learning among clients via a
coordinating server while avoiding direct data sharing, offering a perceived
solution to preserve privacy. However, recent studies on Membership Inference
Attacks (MIAs) have challenged this notion, showing high success rates against
unprotected training data. While local differential privacy (LDP) is widely
regarded as a gold standard for privacy protection in data analysis, most
studies on MIAs either neglect LDP or fail to provide theoretical guarantees
for attack success rates against LDP-protected data. To address this gap, we
derive theoretical lower bounds for the success rates of low-polynomial time
MIAs that exploit vulnerabilities in fully connected or self-attention layers.
We establish that even when data are protected by LDP, privacy risks persist,
depending on the privacy budget. Practical evaluations on federated vision
models confirm considerable privacy risks, revealing that the noise required to
mitigate these attacks significantly degrades models' utility.

</details>


### [351] [LLM Jailbreak Oracle](https://arxiv.org/abs/2506.17299)
*Shuyi Lin,Anshuman Suri,Alina Oprea,Cheng Tan*

Main category: cs.CR

TL;DR: 为解决大语言模型越狱攻击评估方法缺失问题，提出越狱预言机问题，开发高效算法Boa，可进行严格安全评估。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在安全关键应用中部署，缺乏评估越狱攻击脆弱性的系统方法，存在安全漏洞。

Method: 提出越狱预言机问题，Boa算法采用三阶段搜索策略：构建块列表识别拒绝模式、广度优先采样识别易获取越狱情况、基于细粒度安全分数的深度优先优先搜索探索低概率路径。

Result: Boa算法可实现严格的安全评估，包括系统防御评估、红队攻击标准化比较和极端对抗条件下的模型认证。

Conclusion: Boa算法有效解决了越狱预言机问题，为大语言模型的安全评估提供了方法。

Abstract: As large language models (LLMs) become increasingly deployed in
safety-critical applications, the lack of systematic methods to assess their
vulnerability to jailbreak attacks presents a critical security gap. We
introduce the jailbreak oracle problem: given a model, prompt, and decoding
strategy, determine whether a jailbreak response can be generated with
likelihood exceeding a specified threshold. This formalization enables a
principled study of jailbreak vulnerabilities. Answering the jailbreak oracle
problem poses significant computational challenges -- the search space grows
exponentially with the length of the response tokens. We present Boa, the first
efficient algorithm for solving the jailbreak oracle problem. Boa employs a
three-phase search strategy: (1) constructing block lists to identify refusal
patterns, (2) breadth-first sampling to identify easily accessible jailbreaks,
and (3) depth-first priority search guided by fine-grained safety scores to
systematically explore promising low-probability paths. Boa enables rigorous
security assessments including systematic defense evaluation, standardized
comparison of red team attacks, and model certification under extreme
adversarial conditions.

</details>


### [352] [Context manipulation attacks : Web agents are susceptible to corrupted memory](https://arxiv.org/abs/2506.17318)
*Atharv Singh Patlan,Ashwin Hebbar,Pramod Viswanath,Prateek Mittal*

Main category: cs.CR

TL;DR: 研究发现自主网页导航代理内存管理有安全漏洞，提出计划注入攻击，攻击成功率高，强调代理系统需重视安全内存处理。


<details>
  <summary>Details</summary>
Motivation: 自主网页导航代理依赖外部内存系统，而其内存管理存在安全漏洞，近期有生产系统遭攻击。

Method: 引入并形式化“计划注入”攻击，对Browser - use和Agent - E两个流行网页代理进行系统评估。

Result: 计划注入可绕过强大的提示注入防御，攻击成功率比类似基于提示的攻击高3倍；上下文链式注入使隐私窃取任务成功率提高17.7%。

Conclusion: 代理系统中安全内存处理必须成为首要关注点。

Abstract: Autonomous web navigation agents, which translate natural language
instructions into sequences of browser actions, are increasingly deployed for
complex tasks across e-commerce, information retrieval, and content discovery.
Due to the stateless nature of large language models (LLMs), these agents rely
heavily on external memory systems to maintain context across interactions.
Unlike centralized systems where context is securely stored server-side, agent
memory is often managed client-side or by third-party applications, creating
significant security vulnerabilities. This was recently exploited to attack
production systems.
  We introduce and formalize "plan injection," a novel context manipulation
attack that corrupts these agents' internal task representations by targeting
this vulnerable context. Through systematic evaluation of two popular web
agents, Browser-use and Agent-E, we show that plan injections bypass robust
prompt injection defenses, achieving up to 3x higher attack success rates than
comparable prompt-based attacks. Furthermore, "context-chained injections,"
which craft logical bridges between legitimate user goals and attacker
objectives, lead to a 17.7% increase in success rate for privacy exfiltration
tasks. Our findings highlight that secure memory handling must be a first-class
concern in agentic systems.

</details>


### [353] [On the Performance of Cyber-Biomedical Features for Intrusion Detection in Healthcare 5.0](https://arxiv.org/abs/2506.17329)
*Pedro H. Lui,Lucas P. Siqueira,Juliano F. Kazienko,Vagner E. Quincozes,Silvio E. Quincozes,Daniel Welfer*

Main category: cs.CR

TL;DR: 研究将可解释AI应用于集成网络流量和生物医学传感器数据的Healthcare 5.0数据集，XGBoost在不同分类有不错F1分数，揭示网络和生物医学数据在检测中的作用。


<details>
  <summary>Details</summary>
Motivation: Healthcare 5.0依赖互联医疗技术易受网络威胁，当前AI驱动的网络安全模型常忽略生物医学数据，有效性和可解释性有限。

Method: 将可解释AI（XAI）应用于集成网络流量和生物医学传感器数据的Healthcare 5.0数据集。

Result: XGBoost对良性和数据篡改的F1分数达99%，对欺骗攻击达81%；网络数据在入侵检测中占主导，生物医学特征有助于欺骗检测，温度的Shapley值为0.37。

Conclusion: 可解释AI在Healthcare 5.0网络安全检测中有较好效果，不同类型数据在检测中发挥不同作用。

Abstract: Healthcare 5.0 integrates Artificial Intelligence (AI), the Internet of
Things (IoT), real-time monitoring, and human-centered design toward
personalized medicine and predictive diagnostics. However, the increasing
reliance on interconnected medical technologies exposes them to cyber threats.
Meanwhile, current AI-driven cybersecurity models often neglect biomedical
data, limiting their effectiveness and interpretability. This study addresses
this gap by applying eXplainable AI (XAI) to a Healthcare 5.0 dataset that
integrates network traffic and biomedical sensor data. Classification outputs
indicate that XGBoost achieved 99% F1-score for benign and data alteration, and
81% for spoofing. Explainability findings reveal that network data play a
dominant role in intrusion detection whereas biomedical features contributed to
spoofing detection, with temperature reaching a Shapley values magnitude of
0.37.

</details>


### [354] [CUBA: Controlled Untargeted Backdoor Attack against Deep Neural Networks](https://arxiv.org/abs/2506.17350)
*Yinghao Wu,Liyan Zhang*

Main category: cs.CR

TL;DR: 提出CUBA攻击，结合无目标攻击灵活性与有目标攻击意图，能绕过现有防御，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击多为有目标攻击，纯无目标攻击存在自我弱化问题，需结合二者优势。

Method: 提出对交叉熵损失使用对数归一化和翻转的独热标签，在训练中约束对数，使受攻击模型在选定目标类上均匀分布。

Result: 广泛实验证明CUBA在不同数据集上有效。

Conclusion: CUBA结合无目标攻击灵活性和有目标攻击意图，能绕过现有后门防御方法。

Abstract: Backdoor attacks have emerged as a critical security threat against deep
neural networks in recent years. The majority of existing backdoor attacks
focus on targeted backdoor attacks, where trigger is strongly associated to
specific malicious behavior. Various backdoor detection methods depend on this
inherent property and shows effective results in identifying and mitigating
such targeted attacks. However, a purely untargeted attack in backdoor
scenarios is, in some sense, self-weakening, since the target nature is what
makes backdoor attacks so powerful. In light of this, we introduce a novel
Constrained Untargeted Backdoor Attack (CUBA), which combines the flexibility
of untargeted attacks with the intentionality of targeted attacks. The
compromised model, when presented with backdoor images, will classify them into
random classes within a constrained range of target classes selected by the
attacker. This combination of randomness and determinedness enables the
proposed untargeted backdoor attack to natively circumvent existing backdoor
defense methods. To implement the untargeted backdoor attack under controlled
flexibility, we propose to apply logit normalization on cross-entropy loss with
flipped one-hot labels. By constraining the logit during training, the
compromised model will show a uniform distribution across selected target
classes, resulting in controlled untargeted attack. Extensive experiments
demonstrate the effectiveness of the proposed CUBA on different datasets.

</details>


### [355] [Differentiation-Based Extraction of Proprietary Data from Fine-Tuned LLMs](https://arxiv.org/abs/2506.17353)
*Zongjie Li,Daoyuan Wu,Shuai Wang,Zhendong Su*

Main category: cs.CR

TL;DR: 本文首次研究SFT数据集提取问题，提出DDE提取方法，实验证明其可行性与优越性，并提出防御机制，揭示微调大模型数据泄露风险。


<details>
  <summary>Details</summary>
Motivation: 特定领域和符合人类偏好的大语言模型需求增加，SFT数据集有价值且易被提取，需研究该问题。

Method: 先定义问题，探索攻击目标、类型和变体，基于直接提取行为分析开发DDE方法，利用微调模型置信度和与预训练模型行为差异。

Result: DDE在所有攻击设置中始终优于现有提取基线，可实现SFT数据提取。

Conclusion: 研究揭示微调大模型存在隐藏数据泄露风险，为开发更安全模型提供见解。

Abstract: The increasing demand for domain-specific and human-aligned Large Language
Models (LLMs) has led to the widespread adoption of Supervised Fine-Tuning
(SFT) techniques. SFT datasets often comprise valuable instruction-response
pairs, making them highly valuable targets for potential extraction. This paper
studies this critical research problem for the first time. We start by formally
defining and formulating the problem, then explore various attack goals, types,
and variants based on the unique properties of SFT data in real-world
scenarios. Based on our analysis of extraction behaviors of direct extraction,
we develop a novel extraction method specifically designed for SFT models,
called Differentiated Data Extraction (DDE), which exploits the confidence
levels of fine-tuned models and their behavioral differences from pre-trained
base models. Through extensive experiments across multiple domains and
scenarios, we demonstrate the feasibility of SFT data extraction using DDE. Our
results show that DDE consistently outperforms existing extraction baselines in
all attack settings. To counter this new attack, we propose a defense mechanism
that mitigates DDE attacks with minimal impact on model performance. Overall,
our research reveals hidden data leak risks in fine-tuned LLMs and provides
insights for developing more secure models.

</details>


### [356] [Efficient Malware Detection with Optimized Learning on High-Dimensional Features](https://arxiv.org/abs/2506.17309)
*Aditya Choudhary,Sarthak Pawar,Yashodhara Haribhakta*

Main category: cs.CR

TL;DR: 本文应用降维技术解决恶意软件检测特征高维计算难题，经实验得出LightGBM在特定特征集上效果最佳，提供高效准确检测方法。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习恶意软件检测中高维特征带来的显著计算挑战。

Method: 应用XGBoost特征选择和PCA两种降维技术，在四个模型上评估三种降维特征维度，采用统一数据集划分。

Result: LightGBM在XGBoost特征选择后的384维特征集上准确率达97.52%，训练用时61分钟，对新数据集保持高准确率。

Conclusion: 提出了一种可扩展、计算高效且不影响准确性的恶意软件检测方法。

Abstract: Malware detection using machine learning requires feature extraction from
binary files, as models cannot process raw binaries directly. A common approach
involves using LIEF for raw feature extraction and the EMBER vectorizer to
generate 2381-dimensional feature vectors. However, the high dimensionality of
these features introduces significant computational challenges. This study
addresses these challenges by applying two dimensionality reduction techniques:
XGBoost-based feature selection and Principal Component Analysis (PCA). We
evaluate three reduced feature dimensions (128, 256, and 384), which correspond
to approximately 5.4%, 10.8%, and 16.1% of the original 2381 features, across
four models-XGBoost, LightGBM, Extra Trees, and Random Forest-using a unified
training, validation, and testing split formed from the EMBER-2018, ERMDS, and
BODMAS datasets. This approach ensures generalization and avoids dataset bias.
Experimental results show that LightGBM trained on the 384-dimensional feature
set after XGBoost feature selection achieves the highest accuracy of 97.52% on
the unified dataset, providing an optimal balance between computational
efficiency and detection performance. The best model, trained in 61 minutes
using 30 GB of RAM and 19.5 GB of disk space, generalizes effectively to
completely unseen datasets, maintaining 95.31% accuracy on TRITIUM and 93.98%
accuracy on INFERNO. These findings present a scalable, compute-efficient
approach for malware detection without compromising accuracy.

</details>


### [357] [Mechanistic Interpretability in the Presence of Architectural Obfuscation](https://arxiv.org/abs/2506.18053)
*Marcos Florencio,Thomas Barton*

Main category: cs.CR

TL;DR: 研究架构混淆对大语言模型机制可解释性的影响，发现其在保留全局行为的同时阻碍特定内容的机制分析。


<details>
  <summary>Details</summary>
Motivation: 现有研究未系统探究架构混淆对机制可解释性的影响，不清楚其是否真的阻碍模型理解。

Method: 分析用代表性混淆映射从头训练的GPT - 2 - small模型，应用logit - lens归因、因果路径修补和注意力头消融等方法。

Result: 混淆显著改变注意力头激活模式，但保留层计算图；阻碍用户提示逆向工程；前馈和残差路径功能完好，降低细粒度可解释性但不影响顶层任务性能。

Conclusion: 架构混淆可同时保留全局模型行为并阻碍特定内容机制分析，为隐私防御和可解释性工具提供指导。

Abstract: Architectural obfuscation - e.g., permuting hidden-state tensors, linearly
transforming embedding tables, or remapping tokens - has recently gained
traction as a lightweight substitute for heavyweight cryptography in
privacy-preserving large-language-model (LLM) inference. While recent work has
shown that these techniques can be broken under dedicated reconstruction
attacks, their impact on mechanistic interpretability has not been
systematically studied. In particular, it remains unclear whether scrambling a
network's internal representations truly thwarts efforts to understand how the
model works, or simply relocates the same circuits to an unfamiliar coordinate
system. We address this gap by analyzing a GPT-2-small model trained from
scratch with a representative obfuscation map. Assuming the obfuscation map is
private and the original basis is hidden (mirroring an honest-but-curious
server), we apply logit-lens attribution, causal path-patching, and
attention-head ablation to locate and manipulate known circuits. Our findings
reveal that obfuscation dramatically alters activation patterns within
attention heads yet preserves the layer-wise computational graph. This
disconnect hampers reverse-engineering of user prompts: causal traces lose
their alignment with baseline semantics, and token-level logit attributions
become too noisy to reconstruct. At the same time, feed-forward and residual
pathways remain functionally intact, suggesting that obfuscation degrades
fine-grained interpretability without compromising top-level task performance.
These results establish quantitative evidence that architectural obfuscation
can simultaneously (i) retain global model behaviour and (ii) impede
mechanistic analyses of user-specific content. By mapping where
interpretability breaks down, our study provides guidance for future privacy
defences and for robustness-aware interpretability tooling.

</details>


### [358] [AdRo-FL: Informed and Secure Client Selection for Federated Learning in the Presence of Adversarial Aggregator](https://arxiv.org/abs/2506.17805)
*Md. Kamrul Hossain,Walid Aljoby,Anis Elgabli,Ahmed M. Abdelmoniem,Khaled A. Harras*

Main category: cs.CR

TL;DR: 本文提出AdRo - FL，可实现基于客户端效用的信息客户端选择，防御BSA，有针对不同场景的选择框架，还应用量化和设置传输期限提升性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中聚合器可从客户端模型更新中推断敏感信息，安全聚合存在BSA漏洞，而可验证随机选择影响性能。

Method: 提出AdRo - FL，包括针对集群和分布式客户端的两种选择框架，在集群场景设置选择配额和引入效用函数，分布式场景采用两阶段选择协议，应用量化和设置传输期限。

Result: AdRo - FL比不安全基线实现高达1.85倍更快的时间 - 准确率和高达1.06倍更高的最终准确率。

Conclusion: AdRo - FL能实现基于客户端效用的选择，有效防御BSA，提升性能和效率。

Abstract: Federated Learning (FL) enables collaborative learning without exposing
clients' data. While clients only share model updates with the aggregator,
studies reveal that aggregators can infer sensitive information from these
updates. Secure Aggregation (SA) protects individual updates during
transmission; however, recent work demonstrates a critical vulnerability where
adversarial aggregators manipulate client selection to bypass SA protections,
constituting a Biased Selection Attack (BSA). Although verifiable random
selection prevents BSA, it precludes informed client selection essential for FL
performance. We propose Adversarial Robust Federated Learning (AdRo-FL), which
simultaneously enables: informed client selection based on client utility, and
robust defense against BSA maintaining privacy-preserving aggregation. AdRo-FL
implements two client selection frameworks tailored for distinct settings. The
first framework assumes clients are grouped into clusters based on mutual
trust, such as different branches of an organization. The second framework
handles distributed clients where no trust relationships exist between them.
For the cluster-oriented setting, we propose a novel defense against BSA by (1)
enforcing a minimum client selection quota from each cluster, supervised by a
cluster-head in every round, and (2) introducing a client utility function to
prioritize efficient clients. For the distributed setting, we design a
two-phase selection protocol: first, the aggregator selects the top clients
based on our utility-driven ranking; then, a verifiable random function (VRF)
ensures a BSA-resistant final selection. AdRo-FL also applies quantization to
reduce communication overhead and sets strict transmission deadlines to improve
energy efficiency. AdRo-FL achieves up to $1.85\times$ faster time-to-accuracy
and up to $1.06\times$ higher final accuracy compared to insecure baselines.

</details>


### [359] [Federated Learning-Based Data Collaboration Method for Enhancing Edge Cloud AI System Security Using Large Language Models](https://arxiv.org/abs/2506.18087)
*Huaiying Luo,Cheng Ji*

Main category: cs.CR

TL;DR: 本文提出基于联邦学习的数据协作方法提升边缘云AI系统安全，用大语言模型增强隐私保护和系统鲁棒性，实验显示效果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 边缘计算和云系统在AI应用中广泛使用，需解决保证数据隐私同时维持高效性能的安全问题。

Method: 基于现有联邦学习框架，引入安全多方计算协议，用大语言模型优化分布式节点间数据聚合和加密过程，结合对抗训练技术。

Result: 所提方法在数据保护和模型鲁棒性上比传统联邦学习方法好15%。

Conclusion: 所提基于联邦学习和大语言模型的数据协作方法能提升边缘云AI系统的安全性和鲁棒性。

Abstract: With the widespread application of edge computing and cloud systems in
AI-driven applications, how to maintain efficient performance while ensuring
data privacy has become an urgent security issue. This paper proposes a
federated learning-based data collaboration method to improve the security of
edge cloud AI systems, and use large-scale language models (LLMs) to enhance
data privacy protection and system robustness. Based on the existing federated
learning framework, this method introduces a secure multi-party computation
protocol, which optimizes the data aggregation and encryption process between
distributed nodes by using LLM to ensure data privacy and improve system
efficiency. By combining advanced adversarial training techniques, the model
enhances the resistance of edge cloud AI systems to security threats such as
data leakage and model poisoning. Experimental results show that the proposed
method is 15% better than the traditional federated learning method in terms of
data protection and model robustness.

</details>


### [360] [Dynamic Temporal Positional Encodings for Early Intrusion Detection in IoT](https://arxiv.org/abs/2506.18114)
*Ioannis Panopoulos,Maria-Lamprini A. Bartsioka,Sokratis Nikolaidis,Stylianos I. Venieris,Dimitra I. Kaklamani,Iakovos S. Venieris*

Main category: cs.CR

TL;DR: 提出基于Transformer的早期入侵检测系统EIDS，结合动态时间位置编码和数据增强，在CICIoT2023数据集上表现优于现有模型，可在资源受限设备实时运行。


<details>
  <summary>Details</summary>
Motivation: 物联网快速发展带来安全挑战，传统入侵检测系统忽略网络流量时间特征，早期威胁检测效果有限。

Method: 提出基于Transformer的EIDS，使用动态时间位置编码，引入数据增强管道。

Result: 在CICIoT2023数据集上，该方法在准确性和早期检测方面优于现有模型，能在资源受限物联网设备上实时运行，实现低延迟推理和小内存占用。

Conclusion: 基于Transformer的EIDS能有效提高入侵检测准确性和早期检测能力，且适用于资源受限设备。

Abstract: The rapid expansion of the Internet of Things (IoT) has introduced
significant security challenges, necessitating efficient and adaptive Intrusion
Detection Systems (IDS). Traditional IDS models often overlook the temporal
characteristics of network traffic, limiting their effectiveness in early
threat detection. We propose a Transformer-based Early Intrusion Detection
System (EIDS) that incorporates dynamic temporal positional encodings to
enhance detection accuracy while maintaining computational efficiency. By
leveraging network flow timestamps, our approach captures both sequence
structure and timing irregularities indicative of malicious behaviour.
Additionally, we introduce a data augmentation pipeline to improve model
robustness. Evaluated on the CICIoT2023 dataset, our method outperforms
existing models in both accuracy and earliness. We further demonstrate its
real-time feasibility on resource-constrained IoT devices, achieving
low-latency inference and minimal memory footprint.

</details>


### [361] [Security Assessment of DeepSeek and GPT Series Models against Jailbreak Attacks](https://arxiv.org/abs/2506.18543)
*Xiaodong Wu,Xiangman Li,Jianbing Ni*

Main category: cs.CR

TL;DR: 对DeepSeek系列模型进行越狱评估，发现其架构优缺点，强调开源大模型安全调优必要。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型易受越狱攻击，新兴开源模型DeepSeek的鲁棒性缺乏研究。

Method: 用HarmBench基准对DeepSeek系列模型与GPT-3.5和GPT-4进行越狱评估，评估7种攻击策略下510种有害行为。

Result: DeepSeek的MoE架构对基于优化的攻击有选择性鲁棒性，但在基于提示和人工设计的攻击下更脆弱；GPT-4 Turbo安全对齐性更强更一致。

Conclusion: 架构效率和对齐泛化存在权衡，需针对性安全调优和模块化对齐策略确保开源大模型安全部署。

Abstract: The widespread deployment of large language models (LLMs) has raised critical
concerns over their vulnerability to jailbreak attacks, i.e., adversarial
prompts that bypass alignment mechanisms and elicit harmful or policy-violating
outputs. While proprietary models like GPT-4 have undergone extensive
evaluation, the robustness of emerging open-source alternatives such as
DeepSeek remains largely underexplored, despite their growing adoption in
real-world applications. In this paper, we present the first systematic
jailbreak evaluation of DeepSeek-series models, comparing them with GPT-3.5 and
GPT-4 using the HarmBench benchmark. We evaluate seven representative attack
strategies across 510 harmful behaviors categorized by both function and
semantic domain. Our analysis reveals that DeepSeek's Mixture-of-Experts (MoE)
architecture introduces routing sparsity that offers selective robustness
against optimization-based attacks such as TAP-T, but leads to significantly
higher vulnerability under prompt-based and manually engineered attacks. In
contrast, GPT-4 Turbo demonstrates stronger and more consistent safety
alignment across diverse behaviors, likely due to its dense Transformer design
and reinforcement learning from human feedback. Fine-grained behavioral
analysis and case studies further show that DeepSeek often routes adversarial
prompts to under-aligned expert modules, resulting in inconsistent refusal
behaviors. These findings highlight a fundamental trade-off between
architectural efficiency and alignment generalization, emphasizing the need for
targeted safety tuning and modular alignment strategies to ensure secure
deployment of open-source LLMs.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [362] [Pix2Geomodel: A Next-Generation Reservoir Geomodeling with Property-to-Property Translation](https://arxiv.org/abs/2506.17747)
*Abdulrahman Al-Fakih,Ardiansyah Koeshidayatullah,Nabil A. Saraih,Tapan Mukerji,Rayan Kanfar,Abdulmohsen Alali,SanLinn I. Kaka*

Main category: physics.geo-ph

TL;DR: 研究提出Pix2Geomodel框架预测储层属性，评估显示高精度，比传统方法有优势，但存在局限，需改进。


<details>
  <summary>Details</summary>
Motivation: 传统地质建模方法难以处理复杂地下非均质性和观测数据约束问题，需要更准确的地质建模方法。

Method: 基于Pix2Pix的条件生成对抗网络框架Pix2Geomodel，进行数据预处理、增强，用U - Net生成器和PatchGAN判别器训练，用多种指标评估。

Result: 对相和含水饱和度预测精度高，孔隙度和渗透率有一定成功，翻译性能好，能捕捉空间变异性和地质真实性。

Conclusion: Pix2Geomodel比传统方法有更高保真度，但有微结构变异性和二维约束挑战，未来需整合多模态数据和3D建模。

Abstract: Accurate geological modeling is critical for reservoir characterization, yet
traditional methods struggle with complex subsurface heterogeneity, and they
have problems with conditioning to observed data. This study introduces
Pix2Geomodel, a novel conditional generative adversarial network (cGAN)
framework based on Pix2Pix, designed to predict reservoir properties (facies,
porosity, permeability, and water saturation) from the Rotliegend reservoir of
the Groningen gas field. Utilizing a 7.6 million-cell dataset from the
Nederlandse Aardolie Maatschappij, accessed via EPOS-NL, the methodology
included data preprocessing, augmentation to generate 2,350 images per
property, and training with a U-Net generator and PatchGAN discriminator over
19,000 steps. Evaluation metrics include pixel accuracy (PA), mean intersection
over union (mIoU), frequency weighted intersection over union (FWIoU), and
visualizations assessed performance in masked property prediction and
property-to-property translation tasks. Results demonstrated high accuracy for
facies (PA 0.88, FWIoU 0.85) and water saturation (PA 0.96, FWIoU 0.95), with
moderate success for porosity (PA 0.70, FWIoU 0.55) and permeability (PA 0.74,
FWIoU 0.60), and robust translation performance (e.g., facies-to-facies PA
0.98, FWIoU 0.97). The framework captured spatial variability and geological
realism, as validated by variogram analysis, and calculated the training loss
curves for the generator and discriminator for each property. Compared to
traditional methods, Pix2Geomodel offers enhanced fidelity in direct property
mapping. Limitations include challenges with microstructural variability and 2D
constraints, suggesting future integration of multi-modal data and 3D modeling
(Pix2Geomodel v2.0). This study advances the application of generative AI in
geoscience, supporting improved reservoir management and open science
initiatives.

</details>


### [363] [UT-GraphCast Hindcast Dataset: A Global AI Forecast Archive from UT Austin for Weather and Climate Applications](https://arxiv.org/abs/2506.17453)
*Naveen Sudharsan,Manmeet Singh,Harsh Kamath,Hassan Dashtian,Clint Dawson,Zong-Liang Yang,Dev Niyogi*

Main category: physics.geo-ph

TL;DR: 介绍了1979 - 2024年的UT GraphCast Hindcast数据集，它由谷歌DeepMind GraphCast模型生成，提供45年每日15天的确定性预报。


<details>
  <summary>Details</summary>
Motivation: 创建一个全面的全球天气预报档案。

Method: 使用基于ECMWF ERA5再分析数据训练的物理信息图神经网络GraphCast模型生成数据集。

Result: 得到了涵盖约25公里全球网格、45年的每日15天确定性预报，能预测十多个关键大气和地表变量。

Conclusion: 该数据集能在现代硬件上不到一分钟内完成完整的中期预报。

Abstract: The UT GraphCast Hindcast Dataset from 1979 to 2024 is a comprehensive global
weather forecast archive generated using the Google DeepMind GraphCast
Operational model. Developed by researchers at The University of Texas at
Austin under the WCRP umbrella, this dataset provides daily 15 day
deterministic forecasts at 00UTC on an approximately 25 km global grid for a 45
year period. GraphCast is a physics informed graph neural network that was
trained on ECMWF ERA5 reanalysis. It predicts more than a dozen key atmospheric
and surface variables on 37 vertical levels, delivering a full medium range
forecast in under one minute on modern hardware.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [364] [Continuous Map Matching to Paths under Travel Time Constraints](https://arxiv.org/abs/2506.18354)
*Yannick Bosch,Sabine Storandt*

Main category: cs.CG

TL;DR: 研究带旅行时间约束的地图匹配问题，提出新算法，理论和实践性能优越，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决经典方法无法保证找到一致解的问题，处理公共交通数据处理和轨迹地图匹配等场景。

Method: 提出能处理每个测量无限候选位置集的新算法，依赖高效的线段 - 圆相交数据结构。

Result: 算法总能检测到一致的地图匹配路径（若存在），运行时间为$\mathcal{O}(k^2 n \log {nk})$ ，在温和假设下为$\mathcal{O}(k n ^\lambda + n \log^3 n)$ ，优于基线。

Conclusion: 新算法在理论和实践上表现出色，对不同测量数据和GTFS数据都有用。

Abstract: In this paper, we study the problem of map matching with travel time
constraints. Given a sequence of $k$ spatio-temporal measurements and an
embedded path graph with travel time costs, the goal is to snap each
measurement to a close-by location in the graph, such that consecutive
locations can be reached from one another along the path within the timestamp
difference of the respective measurements. This problem arises in public
transit data processing as well as in map matching of movement trajectories to
general graphs. We show that the classical approach for this problem, which
relies on selecting a finite set of candidate locations in the graph for each
measurement, cannot guarantee to find a consistent solution. We propose a new
algorithm that can deal with an infinite set of candidate locations per
measurement. We prove that our algorithm always detects a consistent map
matching path (if one exists). Despite the enlarged candidate set, we also
demonstrate that our algorithm has superior running time in theory and
practice. For a path graph with $n$ nodes, we show that our algorithm runs in
$\mathcal{O}(k^2 n \log {nk})$ and under mild assumptions in $\mathcal{O}(k n
^\lambda + n \log^3 n)$ for $\lambda \approx 0.695$. This is a significant
improvement over the baseline, which runs in $\mathcal{O}(k n^2)$ and which
might not even identify a correct solution. The performance of our algorithm
hinges on an efficient segment-circle intersection data structure. We describe
how to design and implement such a data structure for our application. In the
experimental evaluation, we demonstrate the usefulness of our novel algorithm
on a diverse set of generated measurements as well as GTFS data.

</details>


### [365] [Optimal Parallel Algorithms for Convex Hulls in 2D and 3D under Noisy Primitive Operations](https://arxiv.org/abs/2506.17507)
*Michael T. Goodrich,Vinesh Sridhar*

Main category: cs.CG

TL;DR: 本文研究噪声原语模型下2D和3D凸包的并行计算几何算法，给出CREW PRAM模型下的最优并行算法，用推广的故障扫描技术检测和修复错误。


<details>
  <summary>Details</summary>
Motivation: 之前Eppstein等人在噪声原语模型下的方法是顺序的，本文研究并行算法。

Method: 使用推广的故障扫描技术在算法中间步骤检测和修复错误。

Result: 给出噪声原语模型下2D和3D凸包在CREW PRAM模型的首个最优并行算法。

Conclusion: 成功在噪声原语模型下为2D和3D凸包设计出并行算法并达到最优。

Abstract: In the noisy primitives model, each primitive comparison performed by an
algorithm, e.g., testing whether one value is greater than another, returns the
incorrect answer with random, independent probability p < 1/2 and otherwise
returns a correct answer. This model was first applied in the context of
sorting and searching, and recent work by Eppstein, Goodrich, and Sridhar
extends this model to sequential algorithms involving geometric primitives such
as orientation and sidedness tests. However, their approaches appear to be
inherently sequential; hence, in this paper, we study parallel computational
geometry algorithms for 2D and 3D convex hulls in the noisy primitives model.
We give the first optimal parallel algorithms in the noisy primitives model for
2D and 3D convex hulls in the CREW PRAM model. The main technical contribution
of our work concerns our ability to detect and fix errors during intermediate
steps of our algorithm using a generalization of the failure sweeping
technique.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [366] [Bayesian Inference for Left-Truncated Log-Logistic Distributions for Time-to-event Data Analysis](https://arxiv.org/abs/2506.17852)
*Fahad Mostafa,Md Rejuan Haque,Md Mostafijur Rahman,Farzana Nasrin*

Main category: stat.ME

TL;DR: 本文提出贝叶斯方法估计左截断对数 - 逻辑斯蒂分布参数，经模拟研究和实际应用验证该方法优势。


<details>
  <summary>Details</summary>
Motivation: 参数估计是统计建模基础，贝叶斯估计有优势，左截断对数 - 逻辑斯蒂分布适合建模时间 - 事件数据，需一种方法估计其参数。

Method: 提出贝叶斯方法，基于截断样本推导似然函数，假设参数先验分布独立，用马尔可夫链蒙特卡罗抽样（Metropolis - Hastings算法）进行后验推断。

Result: 通过模拟研究和实际应用表明，贝叶斯估计提供更稳定可靠的参数估计，尤其在似然面不规则时。

Conclusion: 贝叶斯推断在截断分布的时间 - 事件数据分析中，在估计参数不确定性方面优于其他方法。

Abstract: Parameter estimation is a foundational step in statistical modeling, enabling
us to extract knowledge from data and apply it effectively. Bayesian estimation
of parameters incorporates prior beliefs with observed data to infer
distribution parameters probabilistically and robustly. Moreover, it provides
full posterior distributions, allowing uncertainty quantification and
regularization, especially useful in small or truncated samples. Utilizing the
left-truncated log-logistic (LTLL) distribution is particularly well-suited for
modeling time-to-event data where observations are subject to a known lower
bound such as precipitation data and cancer survival times. In this paper, we
propose a Bayesian approach for estimating the parameters of the LTLL
distribution with a fixed truncation point \( x_L > 0 \). Given a random
variable \( X \sim LL(\alpha, \beta; x_L) \), where \( \alpha > 0 \) is the
scale parameter and \( \beta > 0 \) is the shape parameter, the likelihood
function is derived based on a truncated sample \( X_1, X_2, \dots, X_N \) with
\( X_i > x_L \). We assume independent prior distributions for the parameters,
and the posterior inference is conducted via Markov Chain Monte Carlo sampling,
specifically using the Metropolis-Hastings algorithm to obtain posterior
estimates \( \hat{\alpha} \) and \( \hat{\beta} \). Through simulation studies
and real-world applications, we demonstrate that Bayesian estimation provides
more stable and reliable parameter estimates, particularly when the likelihood
surface is irregular due to left truncation. The results highlight the
advantages of Bayesian inference outperform the estimation of parameter
uncertainty in truncated distributions for time to event data analysis.

</details>


### [367] [GRASP: Grouped Regression with Adaptive Shrinkage Priors](https://arxiv.org/abs/2506.18092)
*Shu Yu Tew,Daniel F. Schmidt,Mario Boley*

Main category: stat.ME

TL;DR: 介绍基于NBP先验的GRASP贝叶斯回归框架，有新的组内收缩参数相关性量化框架和高效采样器，实证结果显示其鲁棒性和通用性。


<details>
  <summary>Details</summary>
Motivation: 解决分组预测变量回归问题，在不同稀疏性和信噪比下实现灵活稀疏性，避免复杂层次构造。

Method: 构建基于NBP先验的GRASP框架，将NBP先验分配给局部和组收缩参数，引入量化组内收缩参数相关性框架和Metropolis - Hastings采样器。

Result: 实证结果表明GRASP在模拟和真实数据的分组回归问题中具有鲁棒性和通用性。

Conclusion: GRASP框架在分组回归问题中有效，无需复杂层次构造，能自适应稀疏性并提供组内收缩行为洞察。

Abstract: We introduce GRASP, a simple Bayesian framework for regression with grouped
predictors, built on the normal beta prime (NBP) prior. The NBP prior is an
adaptive generalization of the horseshoe prior with tunable hyperparameters
that control tail behavior, enabling a flexible range of sparsity, from strong
shrinkage to ridge-like regularization. Unlike prior work that introduced the
group inverse-gamma gamma (GIGG) prior by decomposing the NBP prior into
structured hierarchies, we show that directly controlling the tails is
sufficient without requiring complex hierarchical constructions. Extending the
non-tail adaptive grouped half-Cauchy hierarchy of Xu et al., GRASP assigns the
NBP prior to both local and group shrinkage parameters allowing adaptive
sparsity within and across groups. A key contribution of this work is a novel
framework to explicitly quantify correlations among shrinkage parameters within
a group, providing deeper insights into grouped shrinkage behavior. We also
introduce an efficient Metropolis-Hastings sampler for hyperparameter
estimation. Empirical results on simulated and real-world data demonstrate the
robustness and versatility of GRASP across grouped regression problems with
varying sparsity and signal-to-noise ratios.

</details>


### [368] [PCA-Guided Quantile Sampling: Preserving Data Structure in Large-Scale Subsampling](https://arxiv.org/abs/2506.18249)
*Foo Hui-Mean,Yuan-chin Ivan Chang*

Main category: stat.ME

TL;DR: 提出PCA QS采样框架，保留大规模数据集结构，有理论保证，实践表现优于简单随机采样。


<details>
  <summary>Details</summary>
Motivation: 现有PCA降维牺牲可解释性，需要一个能保留数据集统计和几何结构的采样方法。

Method: 提出PCA QS框架，用主成分引导分位数分层采样，保留原特征空间。

Result: 建立理论保证，给出选择参数的实践指南，实证研究显示优于简单随机采样。

Conclusion: PCA QS是现代机器学习工作流中高效数据总结的可扩展、可解释且有理论依据的解决方案。

Abstract: We introduce Principal Component Analysis guided Quantile Sampling (PCA QS),
a novel sampling framework designed to preserve both the statistical and
geometric structure of large scale datasets. Unlike conventional PCA, which
reduces dimensionality at the cost of interpretability, PCA QS retains the
original feature space while using leading principal components solely to guide
a quantile based stratification scheme. This principled design ensures that
sampling remains representative without distorting the underlying data
semantics. We establish rigorous theoretical guarantees, deriving convergence
rates for empirical quantiles, Kullback Leibler divergence, and Wasserstein
distance, thus quantifying the distributional fidelity of PCA QS samples.
Practical guidelines for selecting the number of principal components, quantile
bins, and sampling rates are provided based on these results. Extensive
empirical studies on both synthetic and real-world datasets show that PCA QS
consistently outperforms simple random sampling, yielding better structure
preservation and improved downstream model performance. Together, these
contributions position PCA QS as a scalable, interpretable, and theoretically
grounded solution for efficient data summarization in modern machine learning
workflows.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [369] [On the convergence of computational methods for the online bin stretching problem](https://arxiv.org/abs/2506.17271)
*Antoine Lhomme,Nicolas Catusse,Nadia Brauner*

Main category: math.OC

TL;DR: 本文证明在线装箱问题中计算搜索法可收敛，并给出与最优值差距的界，为在线问题计算方法收敛提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 现有通过计算搜索求在线装箱问题上下界的方法无理论保证收敛到最优，需理论支撑。

Method: 理论证明计算搜索方法的收敛性。

Result: 证明该方法收敛且给出与最优值差距的界。

Conclusion: 为在线问题计算方法的收敛性奠定理论基础。

Abstract: Online bin stretching is an online packing problem where some of the best
known lower and upper bounds were found through computational searches. The
limiting factor in obtaining better bounds with such methods is the
computational time allowed. However, there is still no theoretical guarantee
that such methods do converge towards the optimal online performance. This
paper shows that such methods do, in fact, converge; moreover, bounds on the
gap to the optimal are also given. These results frame a theoretical foundation
for the convergence of computational approaches for online problems.

</details>


### [370] [Regular Tree Search for Simulation Optimization](https://arxiv.org/abs/2506.17696)
*Du-Yi Wang,Guo Liang,Guangwu Liu,Kun Zhang*

Main category: math.OC

TL;DR: 提出Regular Tree Search随机搜索算法解决非凸目标函数仿真优化问题，证明收敛性，实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 解决运筹学中处理非凸目标函数仿真优化问题的挑战。

Method: 提出Regular Tree Search算法，结合自适应采样和搜索空间递归分区，采用UCT树搜索策略，在次高斯噪声下证明全局收敛性。

Result: 数值实验表明算法能可靠识别全局最优解并准确估计目标值。

Conclusion: Regular Tree Search算法在解决非凸目标函数仿真优化问题上有效且具有全局收敛性。

Abstract: Tackling simulation optimization problems with non-convex objective functions
remains a fundamental challenge in operations research. In this paper, we
propose a class of random search algorithms, called Regular Tree Search, which
integrates adaptive sampling with recursive partitioning of the search space.
The algorithm concentrates simulations on increasingly promising regions by
iteratively refining a tree structure. A tree search strategy guides sampling
decisions, while partitioning is triggered when the number of samples in a leaf
node exceeds a threshold that depends on its depth. Furthermore, a specific
tree search strategy, Upper Confidence Bounds applied to Trees (UCT), is
employed in the Regular Tree Search. We prove global convergence under
sub-Gaussian noise, based on assumptions involving the optimality gap, without
requiring continuity of the objective function. Numerical experiments confirm
that the algorithm reliably identifies the global optimum and provides accurate
estimates of its objective value.

</details>


### [371] [Wisdom of Crowds Through Myopic Self-Confidence Adaptation](https://arxiv.org/abs/2506.18195)
*Giacomo Como,Fabio Fagnani,Anton Proskurnikov*

Main category: math.OC

TL;DR: 本文探讨群体智慧现象，研究持有无偏噪声测量的代理按特定规则迭代更新估计，解决博弈论多目标优化问题，刻画帕累托前沿和纳什均衡集并证明异步最佳响应动态收敛。


<details>
  <summary>Details</summary>
Motivation: 研究群体决策中，在受部分有影响力代理影响导致群体决策准确性降低的情况下，如何让群体成员实现对世界状态的最佳估计。

Method: 假设代理按非贝叶斯学习规则迭代更新估计，将其转化为博弈论多目标优化问题，刻画帕累托前沿和纳什均衡集，研究异步最佳响应动态。

Result: 刻画了博弈中的帕累托前沿和纳什均衡集，证明异步最佳响应动态收敛到严格纳什均衡集。

Conclusion: 通过解决博弈论多目标优化问题，可帮助代理实现对世界状态更好的估计，异步最佳响应动态能收敛到严格纳什均衡。

Abstract: The wisdom of crowds is an umbrella term for phenomena suggesting that the
collective judgment or decision of a large group can be more accurate than the
individual judgments or decisions of the group members. A well-known example
illustrating this concept is the competition at a country fair described by
Galton, where the median value of the individual guesses about the weight of an
ox resulted in an astonishingly accurate estimate of the actual weight. This
phenomenon resembles classical results in probability theory and relies on
independent decision-making. The accuracy of the group's final decision can be
significantly reduced if the final agents' opinions are driven by a few
influential agents.
  In this paper, we consider a group of agents who initially possess
uncorrelated and unbiased noisy measurements of a common state of the world.
Assume these agents iteratively update their estimates according to a simple
non-Bayesian learning rule, commonly known in mathematical sociology as the
French-DeGroot dynamics or iterative opinion pooling. As a result of this
iterative distributed averaging process, each agent arrives at an asymptotic
estimate of the state of the world, with the variance of this estimate
determined by the matrix of weights the agents assign to each other. Every
agent aims at minimizing the variance of her asymptotic estimate of the state
of the world; however, such variance is also influenced by the weights
allocated by other agents. To achieve the best possible estimate, the agents
must then solve a game-theoretic, multi-objective optimization problem defined
by the available sets of influence weights. We characterize both the Pareto
frontier and the set of Nash equilibria in the resulting game. Additionally, we
examine asynchronous best-response dynamics for the group of agents and prove
their convergence to the set of strict Nash equilibria.

</details>


<div id='q-bio.OT'></div>

# q-bio.OT [[Back]](#toc)

### [372] [Six Decades Post-Discovery of Taylor's Power Law: From Ecological and Statistical Universality, Through Prime Number Distributions and Tipping-Point Signals, to Heterogeneity and Stability of Complex Networks](https://arxiv.org/abs/2506.18154)
*Zhanshan,Ma,R. A. J. Taylor*

Main category: q-bio.OT

TL;DR: 本文介绍泰勒幂定律（TPL）的发现、研究方向、阶段、主题，提出未来研究方向并阐述其研究意义。


<details>
  <summary>Details</summary>
Motivation: TPL在多领域展现出‘普遍性’，引发不同学科研究者的探索兴趣。

Method: 对过去六十年TPL研究进行回顾，识别并综述八个主题。

Result: 梳理出TPL研究的三个阶段、八个主题，提出三个未来研究方向。

Conclusion: TPL研究在实践和理论方面都有重要意义。

Abstract: First discovered by L. R. Taylor (1961, Nature), Taylor's Power Law (TPL)
correlates the mean (M) population abundances and the corresponding variances
(V) across a set of insect populations using a power function (V=aM^b). TPL has
demonstrated its 'universality' across numerous fields of sciences, social
sciences, and humanities. This universality has inspired two main prongs of
exploration: one from mathematicians and statisticians, who might instinctively
respond with a convergence theorem similar to the central limit theorem of the
Gaussian distribution, and another from biologists, ecologists, physicists,
etc., who are more interested in potential underlying ecological or
organizational mechanisms. Over the past six decades, TPL studies have produced
a punctuated landscape with three relatively distinct periods (1960s-1980s;
1990s-2000s, and 2010s-2020s) across the two prongs of abstract and physical
worlds. Eight themes have been identified and reviewed on this landscape,
including population spatial aggregation and ecological mechanisms, TPL and
skewed statistical distributions, mathematical/statistical mechanisms of TPL,
sample vs. population TPL, population stability, synchrony, and early warning
signals for tipping points, TPL on complex networks, TPL in macrobiomes, and in
microbiomes. Three future research directions including fostering reciprocal
interactions between the two prongs, heterogeneity measuring, and exploration
in the context of evolution. The significance of TPL research includes
practically, population fluctuations captured by TPL are relevant for
agriculture, forestry, fishery, wildlife-conservation, epidemiology, tumor
heterogeneity, earthquakes, social inequality, stock illiquidity, financial
stability, tipping point events, etc.; theoretically, TPL is one form of power
laws, which are related to phase transitions, universality, scale-invariance,
etc.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [373] [Auto-Regressive Surface Cutting](https://arxiv.org/abs/2506.18017)
*Yang Li,Victor Cheung,Xinhai Liu,Yuguang Chen,Zhongjin Luo,Biwen Lei,Haohan Weng,Zibo Zhao,Jingwei Huang,Zhuo Chen,Chunchao Guo*

Main category: cs.GR

TL;DR: 提出SeamGPT模型解决表面切割问题，在UV展开基准测试表现出色，还能增强3D分割工具。


<details>
  <summary>Details</summary>
Motivation: 现有表面切割方法生成的图集过于碎片化且缺乏语义连贯性。

Method: 将表面切割问题转化为下一个标记预测任务，对网格顶点和边采样点云并编码为形状条件，用GPT式变压器顺序预测具有量化3D坐标的接缝段。

Result: 在包含流形和非流形网格的UV展开基准测试中取得出色表现，能为3D分割工具提供清晰边界。

Conclusion: SeamGPT是一种有效的表面切割方法，可提升UV展开效果和3D分割工具性能。

Abstract: Surface cutting is a fundamental task in computer graphics, with applications
in UV parameterization, texture mapping, and mesh decomposition. However,
existing methods often produce technically valid but overly fragmented atlases
that lack semantic coherence. We introduce SeamGPT, an auto-regressive model
that generates cutting seams by mimicking professional workflows. Our key
technical innovation lies in formulating surface cutting as a next token
prediction task: sample point clouds on mesh vertices and edges, encode them as
shape conditions, and employ a GPT-style transformer to sequentially predict
seam segments with quantized 3D coordinates. Our approach achieves exceptional
performance on UV unwrapping benchmarks containing both manifold and
non-manifold meshes, including artist-created, and 3D-scanned models. In
addition, it enhances existing 3D segmentation tools by providing clean
boundaries for part decomposition.

</details>


### [374] [Morse: Dual-Sampling for Lossless Acceleration of Diffusion Models](https://arxiv.org/abs/2506.18251)
*Chao Li,Jiawei Fan,Anbang Yao*

Main category: cs.GR

TL;DR: 本文提出Morse框架无损加速扩散模型，通过双采样策略提升效率，实验显示有显著加速效果且可推广。


<details>
  <summary>Details</summary>
Motivation: 加速扩散模型的生成过程，提升其运行效率。

Method: 提出Morse双采样框架，包含Dash和Dot两个模型，利用快速跳跃采样和自适应残差反馈策略，还采用权重共享策略。

Result: 在6个图像生成任务中，相对9个基线扩散模型平均无损加速1.78X - 3.31X，且可推广到LCM - SDXL。

Conclusion: Morse框架能在保证无损的情况下有效加速扩散模型，具有良好的训练和推理效率及可推广性。

Abstract: In this paper, we present Morse, a simple dual-sampling framework for
accelerating diffusion models losslessly. The key insight of Morse is to
reformulate the iterative generation (from noise to data) process via taking
advantage of fast jump sampling and adaptive residual feedback strategies.
Specifically, Morse involves two models called Dash and Dot that interact with
each other. The Dash model is just the pre-trained diffusion model of any type,
but operates in a jump sampling regime, creating sufficient space for sampling
efficiency improvement. The Dot model is significantly faster than the Dash
model, which is learnt to generate residual feedback conditioned on the
observations at the current jump sampling point on the trajectory of the Dash
model, lifting the noise estimate to easily match the next-step estimate of the
Dash model without jump sampling. By chaining the outputs of the Dash and Dot
models run in a time-interleaved fashion, Morse exhibits the merit of flexibly
attaining desired image generation performance while improving overall runtime
efficiency. With our proposed weight sharing strategy between the Dash and Dot
models, Morse is efficient for training and inference. Our method shows a
lossless speedup of 1.78X to 3.31X on average over a wide range of sampling
step budgets relative to 9 baseline diffusion models on 6 image generation
tasks. Furthermore, we show that our method can be also generalized to improve
the Latent Consistency Model (LCM-SDXL, which is already accelerated with
consistency distillation technique) tailored for few-step text-to-image
synthesis. The code and models are available at
https://github.com/deep-optimization/Morse.

</details>


### [375] [BulletGen: Improving 4D Reconstruction with Bullet-Time Generation](https://arxiv.org/abs/2506.18601)
*Denys Rozumnyi,Jonathon Luiten,Numair Khan,Johannes Schönberger,Peter Kontschieder*

Main category: cs.GR

TL;DR: 本文提出BulletGen方法，利用生成模型校正错误并补充高斯动态场景表示中的缺失信息，在新视角合成和2D/3D跟踪任务中取得SOTA成果。


<details>
  <summary>Details</summary>
Motivation: 将单目视频转化为沉浸式动态体验是不适定任务，存在重建未知区域和处理单目深度估计歧义等挑战。

Method: 通过在单个冻结的“子弹时间”步骤将基于扩散的视频生成模型输出与4D重建对齐，用生成帧监督4D高斯模型优化，无缝融合生成内容与静态和动态场景组件。

Result: 在新视角合成和2D/3D跟踪任务中取得了当前最优结果。

Conclusion: 所提出的BulletGen方法能有效处理单目视频转换中的问题，提升了相关任务的性能。

Abstract: Transforming casually captured, monocular videos into fully immersive dynamic
experiences is a highly ill-posed task, and comes with significant challenges,
e.g., reconstructing unseen regions, and dealing with the ambiguity in
monocular depth estimation. In this work we introduce BulletGen, an approach
that takes advantage of generative models to correct errors and complete
missing information in a Gaussian-based dynamic scene representation. This is
done by aligning the output of a diffusion-based video generation model with
the 4D reconstruction at a single frozen "bullet-time" step. The generated
frames are then used to supervise the optimization of the 4D Gaussian model.
Our method seamlessly blends generative content with both static and dynamic
scene components, achieving state-of-the-art results on both novel-view
synthesis, and 2D/3D tracking tasks.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [376] [A Digital Twin Framework for Generation-IV Reactors with Reinforcement Learning-Enabled Health-Aware Supervisory Control](https://arxiv.org/abs/2506.17258)
*Jasmin Y. Lim,Dimitrios Pylorof,Humberto E. Garcia,Karthik Duraisamy*

Main category: eess.SY

TL;DR: 本文设计数字孪生框架用于Gen - IV氟盐冷却高温反应堆，通过集成多种方法优化运维策略，经三个案例验证其在核电站运行中的稳健性和普适性。


<details>
  <summary>Details</summary>
Motivation: Gen - IV核电站虽有优势但成本高阻碍部署，数字孪生可降低成本、提高决策和运营效率，因此设计框架用于Gen - IV氟盐冷却高温反应堆。

Method: 设计闭环框架，集成代理建模、强化学习和贝叶斯推理，用强化学习考虑组件健康和退化，通过参考调节器控制算法实施约束，用贝叶斯滤波将在线模拟与测量数据融合。

Result: 通过一年长期运维、短期精度细化、系统冲击捕捉三个案例展示了框架的维护规划、实时校准等能力。

Conclusion: 该数字孪生框架对健康感知和约束知情的核电站运行具有稳健性，普遍适用于其他先进反应堆概念和复杂工程系统。

Abstract: Generation IV (Gen-IV) nuclear power plants are envisioned to replace the
current reactor fleet, bringing improvements in performance, safety,
reliability, and sustainability. However, large cost investments currently
inhibit the deployment of these advanced reactor concepts. Digital twins bridge
real-world systems with digital tools to reduce costs, enhance decision-making,
and boost operational efficiency. In this work, a digital twin framework is
designed to operate the Gen-IV Fluoride-salt-cooled High-temperature Reactor,
utilizing data-enhanced methods to optimize operational and maintenance
policies while adhering to system constraints. The closed-loop framework
integrates surrogate modeling, reinforcement learning, and Bayesian inference
to streamline end-to-end communication for online regulation and
self-adjustment. Reinforcement learning is used to consider component health
and degradation to drive the target power generations, with constraints
enforced through a Reference Governor control algorithm that ensures compliance
with pump flow rate and temperature limits. These input driving modules benefit
from detailed online simulations that are assimilated to measurement data with
Bayesian filtering. The digital twin is demonstrated in three case studies: a
one-year long-term operational period showcasing maintenance planning
capabilities, short-term accuracy refinement with high-frequency measurements,
and system shock capturing that demonstrates real-time recalibration
capabilities when change in boundary conditions. These demonstrations validate
robustness for health-aware and constraint-informed nuclear plant operation,
with general applicability to other advanced reactor concepts and complex
engineering systems.

</details>


### [377] [Conformal Safety Shielding for Imperfect-Perception Agents](https://arxiv.org/abs/2506.17275)
*William Scarbro,Calum Imrie,Sinem Getir Yaman,Kavan Fatehi,Corina S. Pasareanu,Radu Calinescu,Ravi Mangal*

Main category: eess.SY

TL;DR: 本文提出一种护盾构造方法，用于解决离散自主代理在感知误差下的安全控制问题，并证明了现有护盾构造的全局安全属性，通过案例进行了验证。


<details>
  <summary>Details</summary>
Motivation: 解决使用学习组件进行不完美感知的离散自主代理的安全控制问题。

Method: 提出基于状态估计的护盾构造方法，使用共形预测处理感知组件，仅当预测集所有估计都允许时才允许行动；证明现有护盾构造的全局安全属性。

Result: 实现了局部安全保证，证明了全局安全属性，通过飞机滑行道引导实验系统案例进行了说明。

Conclusion: 所提出的护盾构造方法能在感知误差下为自主代理提供运行时安全保证。

Abstract: We consider the problem of safe control in discrete autonomous agents that
use learned components for imperfect perception (or more generally, state
estimation) from high-dimensional observations. We propose a shield
construction that provides run-time safety guarantees under perception errors
by restricting the actions available to an agent, modeled as a Markov decision
process, as a function of the state estimates. Our construction uses conformal
prediction for the perception component, which guarantees that for each
observation, the predicted set of estimates includes the actual state with a
user-specified probability. The shield allows an action only if it is allowed
for all the estimates in the predicted set, resulting in a local safety
guarantee. We also articulate and prove a global safety property of existing
shield constructions for perfect-perception agents bounding the probability of
reaching unsafe states if the agent always chooses actions prescribed by the
shield. We illustrate our approach with a case-study of an experimental
autonomous system that guides airplanes on taxiways using high-dimensional
perception DNNs.

</details>


### [378] [A Theoretical Framework for Virtual Power Plant Integration with Gigawatt-Scale AI Data Centers: Multi-Timescale Control and Stability Analysis](https://arxiv.org/abs/2506.17284)
*Ali Peivandizadeh*

Main category: eess.SY

TL;DR: 本文提出适应AI数据中心功率极端动态的VPP理论框架，介绍其多层控制架构及成果，为AI基础设施稳定集成奠定数学基础。


<details>
  <summary>Details</summary>
Motivation: 人工智能发展使数据中心对电力系统运行构成挑战，传统VPP架构无法应对其极端功率动态。

Method: 提出四层分级控制架构的理论框架，开发针对转换器主导系统的控制机制和稳定性标准。

Result: 新框架有亚毫秒控制层、结合保护系统动态的新稳定性标准，量化灵活性表征显示可降低30%峰值，保障AI服务可用性。

Conclusion: 为2030年占数据中心用电量50 - 70%的AI基础设施稳定集成建立数学基础。

Abstract: The explosive growth of artificial intelligence has created gigawatt-scale
data centers that fundamentally challenge power system operation, exhibiting
power fluctuations exceeding 500 MW within seconds and millisecond-scale
variations of 50-75% of thermal design power. This paper presents a
comprehensive theoretical framework that reconceptualizes Virtual Power Plants
(VPPs) to accommodate these extreme dynamics through a four-layer hierarchical
control architecture operating across timescales from 100 microseconds to 24
hours.
  We develop control mechanisms and stability criteria specifically tailored to
converter-dominated systems with pulsing megawatt-scale loads. We prove that
traditional VPP architectures, designed for aggregating distributed resources
with response times of seconds to minutes, cannot maintain stability when
confronted with AI data center dynamics exhibiting slew rates exceeding 1,000
MW/s at gigawatt scale.
  Our framework introduces: (1) a sub-millisecond control layer that interfaces
with data center power electronics to actively dampen power oscillations; (2)
new stability criteria incorporating protection system dynamics, demonstrating
that critical clearing times reduce from 150 ms to 83 ms for gigawatt-scale
pulsing loads; and (3) quantified flexibility characterization showing that
workload deferability enables 30% peak reduction while maintaining AI service
availability above 99.95%.
  This work establishes the mathematical foundations necessary for the stable
integration of AI infrastructure that will constitute 50-70% of data center
electricity consumption by 2030.

</details>


### [379] [Dynamic Hybrid Modeling: Incremental Identification and Model Predictive Control](https://arxiv.org/abs/2506.18344)
*Adrian Caspari,Thomas Bierweiler,Sarah Fadda,Daniel Labisch,Maarten Nauta,Franzisko Wagner,Merle Warmbold,Constantinos C. Pantelides*

Main category: eess.SY

TL;DR: 本文提出动态混合模型增量识别方法，包含四步，经三个案例验证其在处理复杂系统和数据有限场景的有效性。


<details>
  <summary>Details</summary>
Motivation: 数学模型在化工过程优化控制中有计算时间、算法复杂度和开发成本等局限，混合模型是解决方案，但动态混合模型识别难，需解决计算和概念难题。

Method: 提出增量识别方法，含正则化动态参数估计、相关性分析、数据驱动模型识别和混合模型集成四步。

Result: 通过三个案例证明该增量方法在处理复杂系统和数据有限场景时具备鲁棒性、可靠性和高效性。

Conclusion: 该方法便于早期评估模型结构适用性，加速混合模型开发，且可独立识别数据驱动组件。

Abstract: Mathematical models are crucial for optimizing and controlling chemical
processes, yet they often face significant limitations in terms of
computational time, algorithm complexity, and development costs. Hybrid models,
which combine mechanistic models with data-driven models (i.e. models derived
via the application of machine learning to experimental data), have emerged as
a promising solution to these challenges. However, the identification of
dynamic hybrid models remains difficult due to the need to integrate
data-driven models within mechanistic model structures. We present an
incremental identification approach for dynamic hybrid models that decouples
the mechanistic and data-driven components to overcome computational and
conceptual difficulties. Our methodology comprises four key steps: (1)
regularized dynamic parameter estimation to determine optimal time profiles for
flux variables, (2) correlation analysis to evaluate relationships between
variables, (3) data-driven model identification using advanced machine learning
techniques, and (4) hybrid model integration to combine the mechanistic and
data-driven components. This approach facilitates early evaluation of model
structure suitability, accelerates the development of hybrid models, and allows
for independent identification of data-driven components. Three case studies
are presented to illustrate the robustness, reliability, and efficiency of our
incremental approach in handling complex systems and scenarios with limited
data.

</details>


### [380] [Frequency Control in Microgrids: An Adaptive Fuzzy-Neural-Network Virtual Synchronous Generator](https://arxiv.org/abs/2506.18611)
*Waleed Breesam,Rezvan Alamian,Nima Tashakor,Brahim Elkhalil Youcefa,Stefan M. Goetz*

Main category: eess.SY

TL;DR: 本文提出用模糊神经网络控制器动态调整虚拟同步发电机的惯性、阻尼和下垂参数，以解决微电网频率调节问题，经仿真和实验验证效果良好。


<details>
  <summary>Details</summary>
Motivation: 分布式可再生能源依赖增加，电力电子分布式发电机取代同步发电机，导致微电网动态特性改变、系统惯性和阻尼降低，固定参数的虚拟同步发电机无法保证频率调节在可接受范围内。

Method: 通过模糊神经网络控制器动态调整虚拟同步发电机的惯性、阻尼和下垂参数，该控制器可在线自我训练选择合适的虚拟参数，考虑可再生能源渗透和影响应用于典型交流微电网。

Result: 与传统和模糊逻辑控制器方法相比，该方法显著降低频率偏差至小于0.03Hz，缩短稳定/恢复时间。

Conclusion: 所提出的动态调整虚拟参数的方法能有效解决微电网频率调节问题，提高系统稳定性。

Abstract: The reliance on distributed renewable energy has increased recently. As a
result, power electronic-based distributed generators replaced synchronous
generators which led to a change in the dynamic characteristics of the
microgrid. Most critically, they reduced system inertia and damping. Virtual
synchronous generators emulated in power electronics, which mimic the dynamic
behaviour of synchronous generators, are meant to fix this problem. However,
fixed virtual synchronous generator parameters cannot guarantee a frequency
regulation within the acceptable tolerance range. Conversely, a dynamic
adjustment of these virtual parameters promises robust solution with stable
frequency. This paper proposes a method to adapt the inertia, damping, and
droop parameters dynamically through a fuzzy neural network controller. This
controller trains itself online to choose appropriate values for these virtual
parameters. The proposed method can be applied to a typical AC microgrid by
considering the penetration and impact of renewable energy sources. We study
the system in a MATLAB/Simulink model and validate it experimentally in real
time using hardware-in-the-loop based on an embedded ARM system (SAM3X8E,
Cortex-M3). Compared to traditional and fuzzy logic controller methods, the
results demonstrate that the proposed method significantly reduces the
frequency deviation to less than 0.03 Hz and shortens the stabilizing/recovery
time.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [381] [Mapping the Evolution of Research Contributions using KnoVo](https://arxiv.org/abs/2506.17508)
*Sajratul Y. Rubaiat,Syed N. Sakib,Hasan M. Jamil*

Main category: cs.DL

TL;DR: 提出智能框架KnoVo用于量化和分析科研文献研究新颖性演变，通过与相关文献比较打分并可视化，还展示了其能力及LLM性能。


<details>
  <summary>Details</summary>
Motivation: 超越传统仅衡量影响力的引用分析，对科研文献研究新颖性进行量化和分析。

Method: 利用大语言模型动态提取目标论文比较维度，与相关文献对比，借鉴锦标赛选择进行比较分析得到新颖性分数，聚合分数并可视化。

Result: 通过分析20篇不同领域论文展示了KnoVo评估原创性、识别相似工作、跟踪知识演变等能力，报告了框架内不同开源大语言模型的性能。

Conclusion: KnoVo框架有助于研究人员评估原创性、识别相似工作、跟踪知识演变、发现研究空白和探索跨学科联系。

Abstract: This paper presents KnoVo (Knowledge Evolution), an intelligent framework
designed for quantifying and analyzing the evolution of research novelty in the
scientific literature. Moving beyond traditional citation analysis, which
primarily measures impact, KnoVo determines a paper's novelty relative to both
prior and subsequent work within its multilayered citation network. Given a
target paper's abstract, KnoVo utilizes Large Language Models (LLMs) to
dynamically extract dimensions of comparison (e.g., methodology, application,
dataset). The target paper is then compared to related publications along these
same extracted dimensions. This comparative analysis, inspired by tournament
selection, yields quantitative novelty scores reflecting the relative
improvement, equivalence, or inferiority of the target paper in specific
aspects. By aggregating these scores and visualizing their progression, for
instance, through dynamic evolution graphs and comparative radar charts, KnoVo
facilitates researchers not only to assess originality and identify similar
work, but also to track knowledge evolution along specific research dimensions,
uncover research gaps, and explore cross-disciplinary connections. We
demonstrate these capabilities through a detailed analysis of 20 diverse papers
from multiple scientific fields and report on the performance of various
open-source LLMs within the KnoVo framework.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [382] [Parallel nonlinear neuromorphic computing with temporal encoding](https://arxiv.org/abs/2506.17261)
*Guangfeng You,Chao Qian,Hongsheng Chen*

Main category: physics.app-ph

TL;DR: 本文提出并行非线性神经形态处理器，利用时空超表面的时间编码映射数据和权重，实验验证其在多任务表现良好，为复杂场景神经形态处理器开辟新途径。


<details>
  <summary>Details</summary>
Motivation: 深度学习应用激增，对低能耗、高计算速度电子硬件需求增加，而传统非线性材料和光电转换难以同时实现高线性和非线性表达能力。

Method: 引入并行非线性神经形态处理器，利用时空超表面的时间编码映射输入数据和可训练权重。

Result: 实验证明该处理器在多标签识别、多任务并行和异步调制中表现良好，在自主规划任务有动态记忆能力，对迷宫问题有实时响应。

Conclusion: 为适用于复杂场景的时间调制神经形态处理器开辟了灵活途径。

Abstract: The proliferation of deep learning applications has intensified the demand
for electronic hardware with low energy consumption and fast computing speed.
Neuromorphic photonics have emerged as a viable alternative to directly process
high-throughput information at the physical space. However, the simultaneous
attainment of high linear and nonlinear expressivity posse a considerable
challenge due to the power efficiency and impaired manipulability in
conventional nonlinear materials and optoelectronic conversion. Here we
introduce a parallel nonlinear neuromorphic processor that enables arbitrary
superposition of information states in multi-dimensional channels, only by
leveraging the temporal encoding of spatiotemporal metasurfaces to map the
input data and trainable weights. The proposed temporal encoding nonlinearity
is theoretically proved to flexibly customize the nonlinearity, while
preserving quasi-static linear transformation capability within each time
partition. We experimentally demonstrated the concept based on distributed
spatiotemporal metasurfaces, showcasing robust performance in multi-label
recognition and multi-task parallelism with asynchronous modulation.
Remarkably, our nonlinear processor demonstrates dynamic memory capability in
autonomous planning tasks and real-time responsiveness to canonical
maze-solving problem. Our work opens up a flexible avenue for a variety of
temporally-modulated neuromorphic processors tailored for complex scenarios.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [383] [Semidefinite Programming for the Asymmetric Stochastic Block Model](https://arxiv.org/abs/2506.18754)
*Julia Gaudio,Phawin Prongpaophan*

Main category: cs.IT

TL;DR: 研究半定规划（SDP）在非对称随机块模型精确恢复问题中的应用，指出sym - SDP在非对称情形下失败并提出新SDP但难分析。


<details>
  <summary>Details</summary>
Motivation: 解决SDP能否用于非对称块模型精确恢复的关键问题。

Method: 研究sym - SDP在非对称情形下的失败情况，给出几何解释并提出新SDP。

Result: 证明sym - SDP在某些非对称情形下无法返回正确顶点标签，提出新SDP但难以用现有技术分析。

Conclusion: 现有技术在设计用于社区检测的SDP时存在根本局限性。

Abstract: We consider semidefinite programming (SDP) for the binary stochastic block
model with equal-sized communities. Prior work of Hajek, Wu, and Xu proposed an
SDP (sym-SDP) for the symmetric case where the intra-community edge
probabilities are equal, and showed that the SDP achieves the
information-theoretic threshold for exact recovery under the symmetry
assumption. A key open question is whether SDPs can be used to achieve exact
recovery for non-symmetric block models. In order to inform the design of a new
SDP for the non-symmetric setting, we investigate the failure of sym-SDP when
it is applied to non-symmetric settings. We formally show that sym-SDP fails to
return the correct labeling of the vertices in some information-theoretically
feasible, asymmetric cases. In addition, we give an intuitive geometric
interpretation of the failure of sym-SDP in asymmetric settings, which in turn
suggests an SDP formulation to handle the asymmetric setting. Still, this new
SDP cannot be readily analyzed by existing techniques, suggesting a fundamental
limitation in the design of SDPs for community detection.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [384] [Rethinking the Role of Operating Conditions for Learning-based Multi-condition Fault Diagnosis](https://arxiv.org/abs/2506.17740)
*Pengyu Han,Zeyi Liu,Shijin Chen,Dongliang Zou,Xiao He*

Main category: eess.SP

TL;DR: 研究现有端到端领域泛化方法在不同工况下性能，提出两阶段诊断框架提升故障诊断性能并验证有效性。


<details>
  <summary>Details</summary>
Motivation: 多工况故障诊断中工况对故障信息影响研究不足，直接用领域泛化方法可能降低模型泛化能力。

Method: 对真实齿轮箱进行多实验研究现有方法性能，提出结合领域泛化编码器和再训练策略的两阶段诊断框架。

Result: 通过对真实齿轮箱数据集实验，验证了所提方法有效性。

Conclusion: 所提两阶段诊断框架能在工况影响大的场景下提升故障诊断性能，提取工况不变的故障特征并缓解过拟合。

Abstract: Multi-condition fault diagnosis is prevalent in industrial systems and
presents substantial challenges for conventional diagnostic approaches. The
discrepancy in data distributions across different operating conditions
degrades model performance when a model trained under one condition is applied
to others. With the recent advancements in deep learning, transfer learning has
been introduced to the fault diagnosis field as a paradigm for addressing
multi-condition fault diagnosis. Among these methods, domain generalization
approaches can handle complex scenarios by extracting condition-invariant fault
features. Although many studies have considered fault diagnosis in specific
multi-condition scenarios, the extent to which operating conditions affect
fault information has been scarcely studied, which is crucial. However, the
extent to which operating conditions affect fault information has been scarcely
studied, which is crucial. When operating conditions have a significant impact
on fault features, directly applying domain generalization methods may lead the
model to learn condition-specific information, thereby reducing its overall
generalization ability. This paper investigates the performance of existing
end-to-end domain generalization methods under varying conditions, specifically
in variable-speed and variable-load scenarios, using multiple experiments on a
real-world gearbox. Additionally, a two-stage diagnostic framework is proposed,
aiming to improve fault diagnosis performance under scenarios with significant
operating condition impacts. By incorporating a domain-generalized encoder with
a retraining strategy, the framework is able to extract condition-invariant
fault features while simultaneously alleviating potential overfitting to the
source domain. Several experiments on a real-world gearbox dataset are
conducted to validate the effectiveness of the proposed approach.

</details>


### [385] [Fast State-Augmented Learning for Wireless Resource Allocation with Dual Variable Regression](https://arxiv.org/abs/2506.18748)
*Yigit Berkay Uslu,Navid NaderiAlizadeh,Mark Eisen,Alejandro Ribeiro*

Main category: eess.SP

TL;DR: 提出用状态增强图神经网络解决多用户无线网络资源分配问题，介绍学习策略、改进训练方法，经实验验证性能优越并给出收敛结果和概率界。


<details>
  <summary>Details</summary>
Motivation: 解决传统对偶次梯度方法在多用户无线网络资源分配问题中的缺点，优化网络效用函数。

Method: 采用状态增强图神经网络参数化资源分配策略，离线训练学习拉格朗日最大化策略，推理阶段通过梯度更新对偶变量；利用二次GNN参数化实现对偶乘子近最优初始化；从对偶下降动力学采样乘子最大化拉格朗日函数改进训练。

Result: 在发射功率控制案例的大量数值实验中，所提算法表现出优越性能。

Conclusion: 所提算法有效，证明了收敛结果和对偶函数最优性差距的指数概率界。

Abstract: We consider resource allocation problems in multi-user wireless networks,
where the goal is to optimize a network-wide utility function subject to
constraints on the ergodic average performance of users. We demonstrate how a
state-augmented graph neural network (GNN) parametrization for the resource
allocation policy circumvents the drawbacks of the ubiquitous dual subgradient
methods by representing the network configurations (or states) as graphs and
viewing dual variables as dynamic inputs to the model, viewed as graph signals
supported over the graphs. Lagrangian maximizing state-augmented policies are
learned during the offline training phase, and the dual variables evolve
through gradient updates while executing the learned state-augmented policies
during the inference phase. Our main contributions are to illustrate how
near-optimal initialization of dual multipliers for faster inference can be
accomplished with dual variable regression, leveraging a secondary GNN
parametrization, and how maximization of the Lagrangian over the multipliers
sampled from the dual descent dynamics substantially improves the training of
state-augmented models. We demonstrate the superior performance of the proposed
algorithm with extensive numerical experiments in a case study of transmit
power control. Finally, we prove a convergence result and an exponential
probability bound on the excursions of the dual function (iterate) optimality
gaps.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [386] [Embedded FPGA Acceleration of Brain-Like Neural Networks: Online Learning to Scalable Inference](https://arxiv.org/abs/2506.18530)
*Muhammad Ihsan Al Hafiz,Naresh Ravichandran,Anders Lansner,Pawel Herman,Artur Podobas*

Main category: cs.AR

TL;DR: 本文提出首个基于Zynq UltraScale+ SoC的BCPNN嵌入式FPGA加速器，在多数据集上评估有显著性能和节能优势，实现边缘设备神经形态计算。


<details>
  <summary>Details</summary>
Motivation: 边缘AI应用需低能耗可在设备上学习适应的模型，传统深度学习模型能耗高依赖云连接，现有BCPNN实现受限于嵌入式系统。

Method: 使用高级综合在Zynq UltraScale+ SoC上开发BCPNN嵌入式FPGA加速器，实现在线学习和推理内核，支持可变和混合精度。

Result: 在MNIST、肺炎和乳腺癌数据集上评估，相比ARM基线，加速器实现高达17.5倍的延迟降低和94%的能耗节省，且不损失精度。

Conclusion: 该工作使边缘设备上的神经形态计算成为可能，弥合类脑学习与实际部署的差距。

Abstract: Edge AI applications increasingly require models that can learn and adapt
on-device with minimal energy budget. Traditional deep learning models, while
powerful, are often overparameterized, energy-hungry, and dependent on cloud
connectivity. Brain-Like Neural Networks (BLNNs), such as the Bayesian
Confidence Propagation Neural Network (BCPNN), propose a neuromorphic
alternative by mimicking cortical architecture and biologically-constrained
learning. They offer sparse architectures with local learning rules and
unsupervised/semi-supervised learning, making them well-suited for low-power
edge intelligence. However, existing BCPNN implementations rely on GPUs or
datacenter FPGAs, limiting their applicability to embedded systems. This work
presents the first embedded FPGA accelerator for BCPNN on a Zynq UltraScale+
SoC using High-Level Synthesis. We implement both online learning and
inference-only kernels with support for variable and mixed precision. Evaluated
on MNIST, Pneumonia, and Breast Cancer datasets, our accelerator achieves up to
17.5x latency and 94% energy savings over ARM baselines, without sacrificing
accuracy. This work enables practical neuromorphic computing on edge devices,
bridging the gap between brain-like learning and real-world deployment.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [387] [General-Purpose Robotic Navigation via LVLM-Orchestrated Perception, Reasoning, and Acting](https://arxiv.org/abs/2506.17462)
*Bernard Lange,Anil Yildiz,Mansur Arief,Shehryar Khattak,Mykel Kochenderfer,Georgios Georgakis*

Main category: cs.RO

TL;DR: 提出通用导航框架ARNA，能在未映射环境中导航推理，在HM - EQA基准测试中达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有导航系统泛化性差，先前LVLM - 机器人集成有局限性，需开发通用导航策略。

Method: 引入Agentic Robotic Navigation Architecture (ARNA)框架，让基于LVLM的智能体利用机器人堆栈中的工具自主定义和执行工作流。

Result: 在Habitat Lab的HM - EQA基准测试中，ARNA达到了SOTA性能，能有效探索、导航和进行具身问答。

Conclusion: ARNA方法为机器人堆栈设计提供了新视角，可在未映射环境中实现鲁棒导航和推理。

Abstract: Developing general-purpose navigation policies for unknown environments
remains a core challenge in robotics. Most existing systems rely on
task-specific neural networks and fixed data flows, limiting generalizability.
Large Vision-Language Models (LVLMs) offer a promising alternative by embedding
human-like knowledge suitable for reasoning and planning. Yet, prior LVLM-robot
integrations typically depend on pre-mapped spaces, hard-coded representations,
and myopic exploration. We introduce the Agentic Robotic Navigation
Architecture (ARNA), a general-purpose navigation framework that equips an
LVLM-based agent with a library of perception, reasoning, and navigation tools
available within modern robotic stacks. At runtime, the agent autonomously
defines and executes task-specific workflows that iteratively query the robotic
modules, reason over multimodal inputs, and select appropriate navigation
actions. This approach enables robust navigation and reasoning in previously
unmapped environments, providing a new perspective on robotic stack design.
Evaluated in Habitat Lab on the HM-EQA benchmark, ARNA achieves
state-of-the-art performance, demonstrating effective exploration, navigation,
and embodied question answering without relying on handcrafted plans, fixed
input representations, or pre-existing maps.

</details>


### [388] [Distilling On-device Language Models for Robot Planning with Minimal Human Intervention](https://arxiv.org/abs/2506.17486)
*Zachary Ravichandran,Ignacio Hounie,Fernando Cladera,Alejandro Ribeiro,George J. Pappas,Vijay Kumar*

Main category: cs.RO

TL;DR: 提出PRISM框架，用于提炼可在设备上运行的小语言模型（SLM）驱动的机器人规划器，提升模型性能并能跨平台和环境泛化。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型（LLM）的机器人依赖云托管模型，在通信基础设施不可靠的环境中可用性受限。

Method: 从现有的LLM规划器开始，自动合成多样任务和环境，从LLM获取规划，用合成数据集提炼紧凑的SLM替代源模型。

Result: 将PRISM应用于三个LLM规划器，将Llama - 3.2 - 3B的性能从GPT - 4o的10 - 20%提升到93%以上，蒸馏后的规划器能跨异构机器人平台和多样环境泛化。

Conclusion: PRISM框架有效，可提升模型性能和泛化能力，相关软件、模型和数据集已开源。

Abstract: Large language models (LLMs) provide robots with powerful contextual
reasoning abilities and a natural human interface. Yet, current LLM-enabled
robots typically depend on cloud-hosted models, limiting their usability in
environments with unreliable communication infrastructure, such as outdoor or
industrial settings. We present PRISM, a framework for distilling small
language model (SLM)-enabled robot planners that run on-device with minimal
human supervision. Starting from an existing LLM-enabled planner, PRISM
automatically synthesizes diverse tasks and environments, elicits plans from
the LLM, and uses this synthetic dataset to distill a compact SLM as a drop-in
replacement of the source model. We apply PRISM to three LLM-enabled planners
for mapping and exploration, manipulation, and household assistance, and we
demonstrate that PRISM improves the performance of Llama-3.2-3B from 10-20% of
GPT-4o's performance to over 93% - using only synthetic data. We further
demonstrate that the distilled planners generalize across heterogeneous robotic
platforms (ground and aerial) and diverse environments (indoor and outdoor). We
release all software, trained models, and datasets at
https://zacravichandran.github.io/PRISM.

</details>


### [389] [Risk-Guided Diffusion: Toward Deploying Robot Foundation Models in Space, Where Failure Is Not An Option](https://arxiv.org/abs/2506.17601)
*Rohan Thakker,Adarsh Patnaik,Vince Kurtz,Jonas Frey,Jonathan Becktor,Sangwoo Moon,Rob Royce,Marcel Kaufmann,Georgios Georgakis,Pascal Roth,Joel Burdick,Marco Hutter,Shehryar Khattak*

Main category: cs.RO

TL;DR: 提出风险引导扩散框架提升机器人在极端地形导航安全性与可靠性，实验显示降低失败率。


<details>
  <summary>Details</summary>
Motivation: 未来机器人太空探索任务需要在极端陌生地形实现安全可靠导航，现有生成式AI方法安全保障有限。

Method: 提出风险引导扩散框架，融合快速的学习型“系统1”和慢速的基于物理的“系统2”，在训练和推理时共享计算。

Result: 在NASA JPL的火星模拟设施进行硬件实验，该方法在不额外训练情况下，利用推理时计算，最多降低4倍失败率，且目标达成性能与基于学习的机器人模型相当。

Conclusion: 所提风险引导扩散框架能有效提升机器人在极端地形导航的安全性和可靠性。

Abstract: Safe, reliable navigation in extreme, unfamiliar terrain is required for
future robotic space exploration missions. Recent generative-AI methods learn
semantically aware navigation policies from large, cross-embodiment datasets,
but offer limited safety guarantees. Inspired by human cognitive science, we
propose a risk-guided diffusion framework that fuses a fast, learned "System-1"
with a slow, physics-based "System-2", sharing computation at both training and
inference to couple adaptability with formal safety. Hardware experiments
conducted at the NASA JPL's Mars-analog facility, Mars Yard, show that our
approach reduces failure rates by up to $4\times$ while matching the
goal-reaching performance of learning-based robotic models by leveraging
inference-time compute without any additional training.

</details>


### [390] [RLRC: Reinforcement Learning-based Recovery for Compressed Vision-Language-Action Models](https://arxiv.org/abs/2506.17639)
*Yuxuan Chen,Xiao Li*

Main category: cs.RO

TL;DR: 本文针对VLA模型在实际部署中的问题提出RLRC压缩方法，能减少内存使用、提高推理吞吐量且不降低任务成功率，优于现有基线。


<details>
  <summary>Details</summary>
Motivation: VLA模型参数大、推理延迟高，在资源受限的机器人平台上部署存在挑战。

Method: 先进行实证研究，然后提出RLRC三阶段恢复方法，包括结构化剪枝、基于SFT和RL的性能恢复以及进一步量化。

Result: RLRC实现了内存使用减少8倍、推理吞吐量提高2.3倍，保持或超过原VLA的任务成功率，且优于现有压缩基线。

Conclusion: RLRC在VLA的设备端部署方面具有强大潜力。

Abstract: Vision-Language-Action models (VLA) have demonstrated remarkable capabilities
and promising potential in solving complex robotic manipulation tasks. However,
their substantial parameter sizes and high inference latency pose significant
challenges for real-world deployment, particularly on resource-constrained
robotic platforms. To address this issue, we begin by conducting an extensive
empirical study to explore the effectiveness of model compression techniques
when applied to VLAs. Building on the insights gained from these preliminary
experiments, we propose RLRC, a three-stage recovery method for compressed
VLAs, including structured pruning, performance recovery based on SFT and RL,
and further quantization. RLRC achieves up to an 8x reduction in memory usage
and a 2.3x improvement in inference throughput, while maintaining or even
surpassing the original VLA's task success rate. Extensive experiments show
that RLRC consistently outperforms existing compression baselines,
demonstrating strong potential for on-device deployment of VLAs. Project
website: https://rlrc-vla.github.io

</details>


### [391] [Learning to Control an Android Robot Head for Facial Animation](https://arxiv.org/abs/2412.13641)
*Marcel Heisler,Christian Becker-Asano*

Main category: cs.RO

TL;DR: 本文将自动学习面部表情的方法用于评估和控制不同的机器人头部，提出用3D地标及其成对距离替代面部动作单元作为学习算法输入，在线调查显示该方法更受青睐但仍需改进。


<details>
  <summary>Details</summary>
Motivation: 手动定义机器人头部丰富面部表情复杂，已有自动学习方法，要将其用于评估和控制不同的机器人头部，并改进面部表情映射。

Method: 使用3D地标及其成对距离作为学习算法输入，替代之前的面部动作单元。

Result: 在线调查参与者在大多数情况下更喜欢所提出方法的映射。

Conclusion: 该方法有优势，但仍需进一步改进。

Abstract: The ability to display rich facial expressions is crucial for human-like
robotic heads. While manually defining such expressions is intricate, there
already exist approaches to automatically learn them. In this work one such
approach is applied to evaluate and control a robot head different from the one
in the original study. To improve the mapping of facial expressions from human
actors onto a robot head, it is proposed to use 3D landmarks and their pairwise
distances as input to the learning algorithm instead of the previously used
facial action units. Participants of an online survey preferred mappings from
our proposed approach in most cases, though there are still further
improvements required.

</details>


### [392] [RoboMonkey: Scaling Test-Time Sampling and Verification for Vision-Language-Action Models](https://arxiv.org/abs/2506.17811)
*Jacky Kwok,Christopher Agia,Rohan Sinha,Matt Foutter,Shulu Li,Ion Stoica,Azalia Mirhoseini,Marco Pavone*

Main category: cs.RO

TL;DR: 本文研究通过采样和验证的测试时间缩放方法增强VLA模型鲁棒性与泛化性，提出RoboMonkey框架，实验显示与现有VLA结合有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决Vision - Language - Action (VLA) 模型在非结构化现实环境中鲁棒性不足的问题。

Method: 先证明动作误差与生成样本数量遵循指数幂律，提出RoboMonkey框架，采样动作、应用高斯扰动和多数投票构建动作提案分布，用VLM验证器选最优动作，还提出合成数据生成管道训练验证器。

Result: 现有VLAs与RoboMonkey结合，分布外任务绝对提高25%，分布内任务提高8%；在适应新机器人设置时，微调VLAs和动作验证器比仅微调VLAs性能提高7%。

Conclusion: 通过测试时间缩放方法，尤其是RoboMonkey框架，能有效提升VLAs的鲁棒性和泛化性。

Abstract: Vision-Language-Action (VLA) models have demonstrated remarkable capabilities
in visuomotor control, yet ensuring their robustness in unstructured real-world
environments remains a persistent challenge. In this paper, we investigate
test-time scaling through the lens of sampling and verification as means to
enhance the robustness and generalization of VLAs. We first demonstrate that
the relationship between action error and the number of generated samples
follows an exponentiated power law across a range of VLAs, indicating the
existence of inference-time scaling laws. Building on these insights, we
introduce RoboMonkey, a test-time scaling framework for VLAs. At deployment,
RoboMonkey samples a small set of actions from a VLA, applies Gaussian
perturbation and majority voting to construct an action proposal distribution,
and then uses a Vision Language Model (VLM)-based verifier to select the
optimal action. We propose a synthetic data generation pipeline for training
such VLM-based action verifiers, and demonstrate that scaling the synthetic
dataset consistently improves verification and downstream accuracy. Through
extensive simulated and hardware experiments, we show that pairing existing
VLAs with RoboMonkey yields significant performance gains, achieving a 25%
absolute improvement on out-of-distribution tasks and 8% on in-distribution
tasks. Additionally, when adapting to new robot setups, we show that
fine-tuning both VLAs and action verifiers yields a 7% performance increase
compared to fine-tuning VLAs alone.

</details>


### [393] [Learning to Dock: A Simulation-based Study on Closing the Sim2Real Gap in Autonomous Underwater Docking](https://arxiv.org/abs/2506.17823)
*Kevin Chang,Rakesh Vivekanandan,Noah Pragin,Sean Bullock,Geoffrey Hollinger*

Main category: cs.RO

TL;DR: 本文针对AUV在动态不确定环境中自主对接，研究减少训练模拟与现实差距的方法，探索提高鲁棒性的现有方法，为缓解差距提供见解并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 解决AUV在动态不确定环境中自主对接时，强化学习因训练模拟与现实差距导致性能下降的问题。

Method: 对不同控制器进行训练，并在现实干扰下评估，探索随机化技术和历史条件控制器等现有提高鲁棒性的方法。

Result: 为训练对接控制器时缓解模拟与现实差距提供了见解。

Conclusion: 指出了对海洋机器人领域有益的未来研究方向。

Abstract: Autonomous Underwater Vehicle (AUV) docking in dynamic and uncertain
environments is a critical challenge for underwater robotics. Reinforcement
learning is a promising method for developing robust controllers, but the
disparity between training simulations and the real world, or the sim2real gap,
often leads to a significant deterioration in performance. In this work, we
perform a simulation study on reducing the sim2real gap in autonomous docking
through training various controllers and then evaluating them under realistic
disturbances. In particular, we focus on the real-world challenge of docking
under different payloads that are potentially outside the original training
distribution. We explore existing methods for improving robustness including
randomization techniques and history-conditioned controllers. Our findings
provide insights into mitigating the sim2real gap when training docking
controllers. Furthermore, our work indicates areas of future research that may
be beneficial to the marine robotics community.

</details>


### [394] [Generative Grasp Detection and Estimation with Concept Learning-based Safety Criteria](https://arxiv.org/abs/2506.17842)
*Al-Harith Farhad,Khalil Abuibaid,Christiane Plociennik,Achim Wagner,Martin Ruskowski*

Main category: cs.RO

TL;DR: 提出协作机器人抓取算法管道，集成可解释AI方法提升透明度和可靠性，在工业环境测试并展示其一致性和改进交接位置标准。


<details>
  <summary>Details</summary>
Motivation: 神经网络作为黑盒模型复杂度高，在安全相关应用中有弊端，需提升机器人抓取算法的透明度和可靠性。

Method: 提出协作机器人抓取算法管道，集成可解释AI方法，提取学习特征并关联输入类以解释模型预测。

Result: 该方法在工业环境测试，设置相机系统让机器人抓取工具和物体，展示了方法的一致性和改进交接位置的标准。

Conclusion: 该方法可提升机器人抓取算法的透明度和可靠性，适用于安全相关的工业应用。

Abstract: Neural networks are often regarded as universal equations that can estimate
any function. This flexibility, however, comes with the drawback of high
complexity, rendering these networks into black box models, which is especially
relevant in safety-centric applications. To that end, we propose a pipeline for
a collaborative robot (Cobot) grasping algorithm that detects relevant tools
and generates the optimal grasp. To increase the transparency and reliability
of this approach, we integrate an explainable AI method that provides an
explanation for the underlying prediction of a model by extracting the learned
features and correlating them to corresponding classes from the input. These
concepts are then used as additional criteria to ensure the safe handling of
work tools. In this paper, we show the consistency of this approach and the
criterion for improving the handover position. This approach was tested in an
industrial environment, where a camera system was set up to enable a robot to
pick up certain tools and objects.

</details>


### [395] [Online Adaptation for Flying Quadrotors in Tight Formations](https://arxiv.org/abs/2506.17488)
*Pei-An Hsieh,Kong Yao Chee,M. Ani Hsieh*

Main category: cs.RO

TL;DR: 提出L1 KNODE - DW MPC控制框架应对四旋翼编队飞行气动交互挑战，经实验验证优于基线模型，能使编队保持垂直对齐。


<details>
  <summary>Details</summary>
Motivation: 四旋翼编队飞行中复杂气动尾流交互会使团队及个体不稳定，且气动效应非线性、变化快，难以建模和预测。

Method: 提出L1 KNODE - DW MPC，一种基于自适应、混合专家学习的控制框架。

Result: 在两种不同的三旋翼编队中评估，结果显示该框架优于多个MPC基线模型，能使三旋翼团队在飞行中保持垂直对齐。

Conclusion: L1自适应模块与准确动力学模型配合时，能最有效地补偿未建模干扰。

Abstract: The task of flying in tight formations is challenging for teams of quadrotors
because the complex aerodynamic wake interactions can destabilize individual
team members as well as the team. Furthermore, these aerodynamic effects are
highly nonlinear and fast-paced, making them difficult to model and predict. To
overcome these challenges, we present L1 KNODE-DW MPC, an adaptive, mixed
expert learning based control framework that allows individual quadrotors to
accurately track trajectories while adapting to time-varying aerodynamic
interactions during formation flights. We evaluate L1 KNODE-DW MPC in two
different three-quadrotor formations and show that it outperforms several MPC
baselines. Our results show that the proposed framework is capable of enabling
the three-quadrotor team to remain vertically aligned in close proximity
throughout the flight. These findings show that the L1 adaptive module
compensates for unmodeled disturbances most effectively when paired with an
accurate dynamics model. A video showcasing our framework and the physical
experiments is available here: https://youtu.be/9QX1Q5Ut9Rs

</details>


### [396] [GeNIE: A Generalizable Navigation System for In-the-Wild Environments](https://arxiv.org/abs/2506.17960)
*Jiaming Wang,Diwen Liu,Jizhuo Chen,Jiaxuan Da,Nuowen Qian,Tram Minh Man,Harold Soh*

Main category: cs.RO

TL;DR: 本文介绍适用于野外环境的可泛化导航系统GeNIE，它集成可泛化的可通行性预测模型与路径融合策略，在ERC比赛中表现优异，将发布相关代码和数据。


<details>
  <summary>Details</summary>
Motivation: 解决具身智能体在非结构化现实环境，特别是多样地形、天气和传感器配置下可靠导航的挑战。

Method: GeNIE集成基于SAM2的可泛化可通行性预测模型和新的路径融合策略。

Result: GeNIE在ERC比赛中获第一名，得分为最高分的79%，领先第二名17%，全程无需人工干预。

Conclusion: GeNIE为鲁棒、可泛化的户外机器人导航设定了新基准，将发布代码、预训练模型权重和新整理的数据集以支持未来研究。

Abstract: Reliable navigation in unstructured, real-world environments remains a
significant challenge for embodied agents, especially when operating across
diverse terrains, weather conditions, and sensor configurations. In this paper,
we introduce GeNIE (Generalizable Navigation System for In-the-Wild
Environments), a robust navigation framework designed for global deployment.
GeNIE integrates a generalizable traversability prediction model built on SAM2
with a novel path fusion strategy that enhances planning stability in noisy and
ambiguous settings. We deployed GeNIE in the Earth Rover Challenge (ERC) at
ICRA 2025, where it was evaluated across six countries spanning three
continents. GeNIE took first place and achieved 79% of the maximum possible
score, outperforming the second-best team by 17%, and completed the entire
competition without a single human intervention. These results set a new
benchmark for robust, generalizable outdoor robot navigation. We will release
the codebase, pretrained model weights, and newly curated datasets to support
future research in real-world navigation.

</details>


### [397] [ADA-DPM: A Neural Descriptors-based Adaptive Noise Point Filtering Strategy for SLAM](https://arxiv.org/abs/2506.18016)
*Yongxin Shao,Binrui Wang,Aihong Tan*

Main category: cs.RO

TL;DR: 提出自适应噪声过滤SLAM策略ADA - DPM应对现有LiDAR SLAM方法在复杂环境的问题，并经数据集测试取得出色结果。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR SLAM方法在动态物体干扰、点云噪声和非结构化环境下，需在定位精度和系统鲁棒性间权衡。

Method: 设计动态分割头预测动态特征点并消除；设计全局重要性评分头自适应选择特征点抑制噪声；构建跨层图内卷积模块融合多尺度邻域结构。

Result: 在多个公开数据集上测试取得出色结果。

Conclusion: 所提出的ADA - DPM策略能在定位精度和系统鲁棒性两方面取得良好表现。

Abstract: LiDAR SLAM has demonstrated significant application value in various fields,
including mobile robot navigation and high-precision map construction. However,
existing methods often need to make a trade-off between positioning accuracy
and system robustness when faced with dynamic object interference, point cloud
noise, and unstructured environments. To address this challenge, we propose an
adaptive noise filtering SLAM strategy-ADA-DPM, achieving excellent preference
in both aspects. We design the Dynamic Segmentation Head to predict the
category of feature points belonging to dynamic points, to eliminate dynamic
feature points; design the Global Importance Scoring Head to adaptively select
feature points with higher contribution and features while suppressing noise
interference; and construct the Cross Layer Intra-Graph Convolution Module
(GLI-GCN) to fuse multi-scale neighborhood structures, thereby enhancing the
discriminative ability of overlapping features. Finally, to further validate
the effectiveness of our method, we tested it on several publicly available
datasets and achieved outstanding results.

</details>


### [398] [RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation](https://arxiv.org/abs/2506.18088)
*Tianxing Chen,Zanxin Chen,Baijun Chen,Zijian Cai,Yibin Liu,Qiwei Liang,Zixuan Li,Xianliang Lin,Yiheng Ge,Zhenyu Gu,Weiliang Deng,Yubin Guo,Tian Nian,Xuanbing Xie,Qiangyu Chen,Kailun Su,Tianling Xu,Guodong Liu,Mengkang Hu,Huan-ang Gao,Kaixuan Wang,Zhixuan Liang,Yusen Qin,Xiaokang Yang,Ping Luo,Yao Mu*

Main category: cs.RO

TL;DR: 提出RoboTwin 2.0框架用于双臂操作数据生成，结合MLLMs和结构化域随机化，在多任务上实验取得良好泛化效果并开源。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据集在双臂操作上因缺乏高效数据生成方法和无法捕捉现实复杂性而不足。

Method: 构建大型对象库RoboTwin - OD，开发结合MLLMs和模拟细化的专家数据合成管道，引入五轴结构化域随机化。

Result: 代码生成成功率提升10.9%，微调模型在真实场景任务相对提升367%，零样本模型相对提升228%。

Conclusion: RoboTwin 2.0能实现大规模多样化真实数据自动生成，支持双臂操作可扩展研究。

Abstract: Simulation-based data synthesis has emerged as a powerful paradigm for
enhancing real-world robotic manipulation. However, existing synthetic datasets
remain insufficient for robust bimanual manipulation due to two challenges: (1)
the lack of an efficient, scalable data generation method for novel tasks, and
(2) oversimplified simulation environments that fail to capture real-world
complexity. We present RoboTwin 2.0, a scalable simulation framework that
enables automated, large-scale generation of diverse and realistic data, along
with unified evaluation protocols for dual-arm manipulation. We first construct
RoboTwin-OD, a large-scale object library comprising 731 instances across 147
categories, each annotated with semantic and manipulation-relevant labels.
Building on this foundation, we develop an expert data synthesis pipeline that
combines multimodal large language models (MLLMs) with simulation-in-the-loop
refinement to generate task-level execution code automatically. To improve
sim-to-real transfer, RoboTwin 2.0 incorporates structured domain randomization
along five axes: clutter, lighting, background, tabletop height and language
instructions, thereby enhancing data diversity and policy robustness. We
instantiate this framework across 50 dual-arm tasks spanning five robot
embodiments, and pre-collect over 100,000 domain-randomized expert
trajectories. Empirical results show a 10.9% gain in code generation success
and improved generalization to novel real-world scenarios. A VLA model
fine-tuned on our dataset achieves a 367% relative improvement (42.0% vs. 9.0%)
on unseen scene real-world tasks, while zero-shot models trained solely on our
synthetic data achieve a 228% relative gain, highlighting strong generalization
without real-world supervision. We release the data generator, benchmark,
dataset, and code to support scalable research in robust bimanual manipulation.

</details>


### [399] [Leveling the Playing Field: Carefully Comparing Classical and Learned Controllers for Quadrotor Trajectory Tracking](https://arxiv.org/abs/2506.17832)
*Pratik Kunapuli,Jake Welde,Dinesh Jayaraman,Vijay Kumar*

Main category: cs.RO

TL;DR: 本文以四旋翼末端执行器敏捷跟踪问题为例，提出对比学习和经典控制器的最佳实践，解决先前研究中偏向RL的偏差，发现两者性能差距没那么大，还开源了控制器代码。


<details>
  <summary>Details</summary>
Motivation: 可靠地比较学习型控制器（如RL）和分析型控制器（如GC）的性能，解决先前研究中存在的比较偏差问题。

Method: 以四旋翼固定臂末端执行器敏捷跟踪为案例研究，制定合成最佳RL和GC控制器进行基准测试的最佳实践，解决先前研究中存在的不对称问题。

Result: 改进实验协议很关键，先前声称RL优于GC不准确，两者差距没那么大，GC稳态误差低，RL瞬态性能好，GC在慢或低敏捷任务中表现好，RL在高敏捷任务中表现好。

Conclusion: 提出对比学习和经典控制器的最佳实践，为未来研究提供参考，并开源控制器代码。

Abstract: Learning-based control approaches like reinforcement learning (RL) have
recently produced a slew of impressive results for tasks like quadrotor
trajectory tracking and drone racing. Naturally, it is common to demonstrate
the advantages of these new controllers against established methods like
analytical controllers. We observe, however, that reliably comparing the
performance of such very different classes of controllers is more complicated
than might appear at first sight. As a case study, we take up the problem of
agile tracking of an end-effector for a quadrotor with a fixed arm. We develop
a set of best practices for synthesizing the best-in-class RL and geometric
controllers (GC) for benchmarking. In the process, we resolve widespread
RL-favoring biases in prior studies that provide asymmetric access to: (1) the
task definition, in the form of an objective function, (2) representative
datasets, for parameter optimization, and (3) feedforward information,
describing the desired future trajectory. The resulting findings are the
following: our improvements to the experimental protocol for comparing learned
and classical controllers are critical, and each of the above asymmetries can
yield misleading conclusions. Prior works have claimed that RL outperforms GC,
but we find the gaps between the two controller classes are much smaller than
previously published when accounting for symmetric comparisons. Geometric
control achieves lower steady-state error than RL, while RL has better
transient performance, resulting in GC performing better in relatively slow or
less agile tasks, but RL performing better when greater agility is required.
Finally, we open-source implementations of geometric and RL controllers for
these aerial vehicles, implementing best practices for future development.
Website and code is available at https://pratikkunapuli.github.io/rl-vs-gc/

</details>


### [400] [Geometric Contact Flows: Contactomorphisms for Dynamics and Control](https://arxiv.org/abs/2506.17868)
*Andrea Testa,Søren Hauberg,Tamim Asfour,Leonel Rozo*

Main category: cs.RO

TL;DR: 提出几何接触流（GCF）框架学习复杂动力系统，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 准确建模和预测涉及力交换和耗散的复杂动力系统具有重要应用价值，但因几何约束和能量转移的复杂相互作用面临挑战。

Method: 引入GCF框架，利用黎曼和接触几何作为归纳偏置，构建潜在接触哈密顿模型，用接触同胚集合调整模型。

Result: 该集合可实现不确定感知的测地线，使系统行为趋向数据支持。

Conclusion: 在学习物理系统动力学和机器人交互任务控制实验中证明了方法的有效性。

Abstract: Accurately modeling and predicting complex dynamical systems, particularly
those involving force exchange and dissipation, is crucial for applications
ranging from fluid dynamics to robotics, but presents significant challenges
due to the intricate interplay of geometric constraints and energy transfer.
This paper introduces Geometric Contact Flows (GFC), a novel framework
leveraging Riemannian and Contact geometry as inductive biases to learn such
systems. GCF constructs a latent contact Hamiltonian model encoding desirable
properties like stability or energy conservation. An ensemble of
contactomorphisms then adapts this model to the target dynamics while
preserving these properties. This ensemble allows for uncertainty-aware
geodesics that attract the system's behavior toward the data support, enabling
robust generalization and adaptation to unseen scenarios. Experiments on
learning dynamics for physical systems and for controlling robots on
interaction tasks demonstrate the effectiveness of our approach.

</details>


### [401] [Newtonian and Lagrangian Neural Networks: A Comparison Towards Efficient Inverse Dynamics Identification](https://arxiv.org/abs/2506.17994)
*Minh Trinh,Andreas René Geist,Josefine Monnet,Stefan Vilceanu,Sebastian Trimpe,Christian Brecher*

Main category: cs.RO

TL;DR: 研究对比拉格朗日和牛顿神经网络在工业机器人逆动力学建模中的表现，发现估计电机扭矩时牛顿网络更优。


<details>
  <summary>Details</summary>
Motivation: 当前文献缺乏在拉格朗日和牛顿神经网络间选择的指导。

Method: 对比拉格朗日和牛顿神经网络与神经网络回归在MABI MAX 100工业机器人数据上的表现。

Result: 估计电机扭矩时，拉格朗日网络因未显式建模耗散扭矩，不如牛顿网络有效。

Conclusion: 在估计电机扭矩的工业机器人逆动力学建模中，牛顿网络比拉格朗日网络更合适。

Abstract: Accurate inverse dynamics models are essential tools for controlling
industrial robots. Recent research combines neural network regression with
inverse dynamics formulations of the Newton-Euler and the Euler-Lagrange
equations of motion, resulting in so-called Newtonian neural networks and
Lagrangian neural networks, respectively. These physics-informed models seek to
identify unknowns in the analytical equations from data. Despite their
potential, current literature lacks guidance on choosing between Lagrangian and
Newtonian networks. In this study, we show that when motor torques are
estimated instead of directly measuring joint torques, Lagrangian networks
prove less effective compared to Newtonian networks as they do not explicitly
model dissipative torques. The performance of these models is compared to
neural network regression on data of a MABI MAX 100 industrial robot.

</details>


### [402] [RoboArena: Distributed Real-World Evaluation of Generalist Robot Policies](https://arxiv.org/abs/2506.18123)
*Pranav Atreya,Karl Pertsch,Tony Lee,Moo Jin Kim,Arhan Jain,Artur Kuramshin,Clemens Eppner,Cyrus Neary,Edward Hu,Fabio Ramos,Jonathan Tremblay,Kanav Arora,Kirsty Ellis,Luca Macesanu,Matthew Leonard,Meedeum Cho,Ozgur Aslan,Shivin Dass,Jie Wang,Xingfang Yuan,Xuning Yang,Abhishek Gupta,Dinesh Jayaraman,Glen Berseth,Kostas Daniilidis,Roberto Martin-Martin,Youngwoon Lee,Percy Liang,Chelsea Finn,Sergey Levine*

Main category: cs.RO

TL;DR: 提出RoboArena用于现实世界中可扩展评估通用机器人策略，通过众包评估和双盲成对比较排名策略，比传统方法更优且已开源。


<details>
  <summary>Details</summary>
Motivation: 现有机器人基准测试方法难以扩展到广泛任务和环境评估通用策略，需要新的评估方法。

Method: 提出众包评估方法，评估者可自由选任务和环境，进行双盲成对评估，聚合反馈得到策略排名。

Result: 通过超600次真实机器人评估，众包方法比传统集中评估更准确排名通用策略，且更具可扩展性、弹性和可信度。

Conclusion: 开放评估网络，希望促进通用机器人策略更易进行比较。

Abstract: Comprehensive, unbiased, and comparable evaluation of modern generalist
policies is uniquely challenging: existing approaches for robot benchmarking
typically rely on heavy standardization, either by specifying fixed evaluation
tasks and environments, or by hosting centralized ''robot challenges'', and do
not readily scale to evaluating generalist policies across a broad range of
tasks and environments. In this work, we propose RoboArena, a new approach for
scalable evaluation of generalist robot policies in the real world. Instead of
standardizing evaluations around fixed tasks, environments, or locations, we
propose to crowd-source evaluations across a distributed network of evaluators.
Importantly, evaluators can freely choose the tasks and environments they
evaluate on, enabling easy scaling of diversity, but they are required to
perform double-blind evaluations over pairs of policies. Then, by aggregating
preference feedback from pairwise comparisons across diverse tasks and
environments, we can derive a ranking of policies. We instantiate our approach
across a network of evaluators at seven academic institutions using the DROID
robot platform. Through more than 600 pairwise real-robot evaluation episodes
across seven generalist policies, we demonstrate that our crowd-sourced
approach can more accurately rank the performance of existing generalist
policies than conventional, centralized evaluation approaches, while being more
scalable, resilient, and trustworthy. We open our evaluation network to the
community and hope that it can enable more accessible comparisons of generalist
robot policies.

</details>


### [403] [A Motivational Architecture for Open-Ended Learning Challenges in Robots](https://arxiv.org/abs/2506.18454)
*Alejandro Romero,Gianluca Baldassarre,Richard J. Duro,Vieri Giuliano Santucci*

Main category: cs.RO

TL;DR: 本文介绍了分层架构H - GRAIL，通过不同类型的内在动机和相互关联的学习机制，解决开放式学习的挑战，并在真实机器人场景中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 开发能在复杂动态环境中自主交互的智能体，现有工作缺乏同时解决开放式学习核心挑战的集成方案。

Method: 引入分层架构H - GRAIL，利用不同类型内在动机和互联学习机制来发现新目标、学习技能、生成技能序列和适应非平稳环境。

Result: 在真实机器人场景中测试了H - GRAIL，证明其能有效解决开放式学习的各种挑战。

Conclusion: H - GRAIL可有效解决开放式学习挑战，为开发能在复杂动态环境中自主交互的智能体提供了可行方案。

Abstract: Developing agents capable of autonomously interacting with complex and
dynamic environments, where task structures may change over time and prior
knowledge cannot be relied upon, is a key prerequisite for deploying artificial
systems in real-world settings. The open-ended learning framework identifies
the core challenges for creating such agents, including the ability to
autonomously generate new goals, acquire the necessary skills (or curricula of
skills) to achieve them, and adapt to non-stationary environments. While many
existing works tackles various aspects of these challenges in isolation, few
propose integrated solutions that address them simultaneously. In this paper,
we introduce H-GRAIL, a hierarchical architecture that, through the use of
different typologies of intrinsic motivations and interconnected learning
mechanisms, autonomously discovers new goals, learns the required skills for
their achievement, generates skill sequences for tackling interdependent tasks,
and adapts to non-stationary environments. We tested H-GRAIL in a real robotic
scenario, demonstrating how the proposed solutions effectively address the
various challenges of open-ended learning.

</details>


### [404] [Robots and Children that Learn Together : Improving Knowledge Retention by Teaching Peer-Like Interactive Robots](https://arxiv.org/abs/2506.18365)
*Imene Tarakli,Samuele Vinanzi,Richard Moore,Alessandro Di Nuovo*

Main category: cs.RO

TL;DR: 本文引入交互式强化学习作为可教学社交机器人的认知模型，通过实验对比教学机器人和自主练习两种学习方式，发现教学机器人能带来更好学习效果，还证明了多台自主机器人在教室部署的可行性。


<details>
  <summary>Details</summary>
Motivation: 以往学习式教学研究多依赖脚本或奥兹巫师行为，限制对实时交互学习的理解，本文旨在填补用自主类人社交机器人在真实教室实施学习式教学的研究空白。

Method: 采用交互式强化学习作为社交机器人认知模型，对58名小学生进行两组组间实验，对比教学机器人和在平板上自主练习学习法语词汇和语法规则的效果。

Result: 学习式教学组比自主练习组有更高记忆提升，尤其是语法任务；低知识水平学习者从教学机器人中受益最大；儿童会调整教学策略，在推理任务中参与度更高。

Conclusion: 引入交互式强化学习作为有效可扩展的同伴机器人学习模型，首次证明多台自主机器人在教室同时部署的可行性，拓展了学习式教学的理论理解。

Abstract: Despite growing interest in Learning-by-Teaching (LbT), few studies have
explored how this paradigm can be implemented with autonomous, peer-like social
robots in real classrooms. Most prior work has relied on scripted or
Wizard-of-Oz behaviors, limiting our understanding of how real-time,
interactive learning can be supported by artificial agents. This study
addresses this gap by introducing Interactive Reinforcement Learning (RL) as a
cognitive model for teachable social robots. We conducted two between-subject
experiments with 58 primary school children, who either taught a robot or
practiced independently on a tablet while learning French vocabulary
(memorization) and grammatical rules (inference). The robot, powered by
Interactive RL, learned from the child's evaluative feedback. Children in the
LbT condition achieved significantly higher retention gains compared to those
in the self-practice condition, especially on the grammar task. Learners with
lower prior knowledge benefited most from teaching the robot. Behavioural
metrics revealed that children adapted their teaching strategies over time and
engaged more deeply during inference tasks. This work makes two contributions:
(1) it introduces Interactive RL as a pedagogically effective and scalable
model for peer-robot learning, and (2) it demonstrates, for the first time, the
feasibility of deploying multiple autonomous robots simultaneously in real
classrooms. These findings extend theoretical understanding of LbT by showing
that social robots can function not only as passive tutees but as adaptive
partners that enhance meta-cognitive engagement and long-term learning
outcomes.

</details>


### [405] [Learning Physical Systems: Symplectification via Gauge Fixing in Dirac Structures](https://arxiv.org/abs/2506.18812)
*Aristotelis Papatheodorou,Pranav Vaidhyanathan,Natalia Ares,Ioannis Havoutis*

Main category: cs.RO

TL;DR: 本文引入Presymplectification Networks (PSNs)解决含耗散和完整约束系统的规范辛形式退化问题，结合循环编码器和流匹配目标学习增广相空间动力学，在ANYmal四足机器人动力学上验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 在含耗散和完整约束的系统中，规范辛形式会退化，破坏保证稳定性和长期预测的不变量，需要解决此基础限制。

Method: 引入PSNs框架，通过狄拉克结构学习辛化提升，将约束系统嵌入高维流形恢复非退化辛几何；结合循环编码器与流匹配目标端到端学习增广相空间动力学；附加轻量级的Symplectic Network预测约束轨迹。

Result: 在ANYmal四足机器人动力学上进行了方法验证。

Conclusion: 这是首个有效弥合约束、耗散机械系统与辛学习之间差距的框架，开启了一类新的几何机器学习模型。

Abstract: Physics-informed deep learning has achieved remarkable progress by embedding
geometric priors, such as Hamiltonian symmetries and variational principles,
into neural networks, enabling structure-preserving models that extrapolate
with high accuracy. However, in systems with dissipation and holonomic
constraints, ubiquitous in legged locomotion and multibody robotics, the
canonical symplectic form becomes degenerate, undermining the very invariants
that guarantee stability and long-term prediction. In this work, we tackle this
foundational limitation by introducing Presymplectification Networks (PSNs),
the first framework to learn the symplectification lift via Dirac structures,
restoring a non-degenerate symplectic geometry by embedding constrained systems
into a higher-dimensional manifold. Our architecture combines a recurrent
encoder with a flow-matching objective to learn the augmented phase-space
dynamics end-to-end. We then attach a lightweight Symplectic Network (SympNet)
to forecast constrained trajectories while preserving energy, momentum, and
constraint satisfaction. We demonstrate our method on the dynamics of the
ANYmal quadruped robot, a challenging contact-rich, multibody system. To the
best of our knowledge, this is the first framework that effectively bridges the
gap between constrained, dissipative mechanical systems and symplectic
learning, unlocking a whole new class of geometric machine learning models,
grounded in first principles yet adaptable from data.

</details>


### [406] [NOVA: Navigation via Object-Centric Visual Autonomy for High-Speed Target Tracking in Unstructured GPS-Denied Environments](https://arxiv.org/abs/2506.18689)
*Alessandro Saviolo,Giuseppe Loianno*

Main category: cs.RO

TL;DR: 本文提出NOVA框架，仅用立体相机和IMU实现无GPS环境下的空中目标跟踪，经多种场景验证效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖运动捕捉系统、预映射场景或基于特征的定位，限制了在现实世界的部署，需要解决非结构化和无GPS环境下自主空中目标跟踪问题。

Method: 引入NOVA框架，在目标参考系中进行感知、估计和控制，结合轻量级目标检测器与立体深度补全，用直方图滤波推断目标距离，通过视觉惯性状态估计器恢复机器人相对目标的6自由度姿态，用非线性模型预测控制器规划轨迹，构建高阶控制屏障函数实现避障。

Result: 在多种具有挑战性的现实场景中验证了NOVA，实现了超过50 km/h的敏捷目标跟踪，多次实验表现一致可靠。

Conclusion: 仅使用机载传感，不依赖外部定位和环境假设，在野外实现基于视觉的高速跟踪是可行的。

Abstract: Autonomous aerial target tracking in unstructured and GPS-denied environments
remains a fundamental challenge in robotics. Many existing methods rely on
motion capture systems, pre-mapped scenes, or feature-based localization to
ensure safety and control, limiting their deployment in real-world conditions.
We introduce NOVA, a fully onboard, object-centric framework that enables
robust target tracking and collision-aware navigation using only a stereo
camera and an IMU. Rather than constructing a global map or relying on absolute
localization, NOVA formulates perception, estimation, and control entirely in
the target's reference frame. A tightly integrated stack combines a lightweight
object detector with stereo depth completion, followed by histogram-based
filtering to infer robust target distances under occlusion and noise. These
measurements feed a visual-inertial state estimator that recovers the full
6-DoF pose of the robot relative to the target. A nonlinear model predictive
controller (NMPC) plans dynamically feasible trajectories in the target frame.
To ensure safety, high-order control barrier functions are constructed online
from a compact set of high-risk collision points extracted from depth, enabling
real-time obstacle avoidance without maps or dense representations. We validate
NOVA across challenging real-world scenarios, including urban mazes, forest
trails, and repeated transitions through buildings with intermittent GPS loss
and severe lighting changes that disrupt feature-based localization. Each
experiment is repeated multiple times under similar conditions to assess
resilience, showing consistent and reliable performance. NOVA achieves agile
target following at speeds exceeding 50 km/h. These results show that
high-speed vision-based tracking is possible in the wild using only onboard
sensing, with no reliance on external localization or environment assumptions.

</details>


### [407] [MinD: Unified Visual Imagination and Control via Hierarchical World Models](https://arxiv.org/abs/2506.18897)
*Xiaowei Chi,Kuangzhi Ge,Jiaming Liu,Siyuan Zhou,Peidong Jia,Zichen He,Yuzhen Liu,Tingguang Li,Lei Han,Sirui Han,Shanghang Zhang,Yike Guo*

Main category: cs.RO

TL;DR: 提出Manipulate in Dream (MinD)框架解决视频生成模型在机器人领域应用的问题，在多基准测试中取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在机器人领域实际应用受限于生成速度慢和想象视频与可执行动作一致性差的问题。

Method: 提出基于分层扩散的世界模型框架MinD，采用双系统设计，执行低频率VGM提取特征，利用高频扩散策略进行实时交互，引入视频 - 动作扩散匹配模块DiffMatcher及新的协同训练策略。

Result: MinD能实现低延迟闭环控制，可作为世界模拟器预测任务成败，在多个基准测试中取得63%以上的操作性能。

Conclusion: MinD推进了机器人统一世界建模的前沿。

Abstract: Video generation models (VGMs) offer a promising pathway for unified world
modeling in robotics by integrating simulation, prediction, and manipulation.
However, their practical application remains limited due to (1) slowgeneration
speed, which limits real-time interaction, and (2) poor consistency between
imagined videos and executable actions. To address these challenges, we propose
Manipulate in Dream (MinD), a hierarchical diffusion-based world model
framework that employs a dual-system design for vision-language manipulation.
MinD executes VGM at low frequencies to extract video prediction features,
while leveraging a high-frequency diffusion policy for real-time interaction.
This architecture enables low-latency, closed-loop control in manipulation with
coherent visual guidance. To better coordinate the two systems, we introduce a
video-action diffusion matching module (DiffMatcher), with a novel co-training
strategy that uses separate schedulers for each diffusion model. Specifically,
we introduce a diffusion-forcing mechanism to DiffMatcher that aligns their
intermediate representations during training, helping the fast action model
better understand video-based predictions. Beyond manipulation, MinD also
functions as a world simulator, reliably predicting task success or failure in
latent space before execution. Trustworthy analysis further shows that VGMs can
preemptively evaluate task feasibility and mitigate risks. Extensive
experiments across multiple benchmarks demonstrate that MinD achieves
state-of-the-art manipulation (63%+) in RL-Bench, advancing the frontier of
unified world modeling in robotics.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [408] [AutoGraph: A Knowledge-Graph Framework for Modeling Interface Interaction and Automating Procedure Execution in Digital Nuclear Control Rooms](https://arxiv.org/abs/2506.18727)
*Xingyu Xiao,Jiejuan Tong,Jun Sun,Zhe Sui,Jingang Liang,Hongru Zhao,Jun Zhao,Haitao Wang*

Main category: cs.HC

TL;DR: 本文提出AutoGraph框架用于核电站数字化环境的程序执行自动化，经场景验证有效且有扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有计算机程序与人类系统界面缺乏语义集成，限制智能自动化支持能力且增加人为错误风险。

Method: 提出AutoGraph框架，集成HTRPM跟踪模块、接口元素知识图谱、文本程序到可执行界面路径的自动映射及执行引擎。

Result: 通过代表性场景验证，显著减少任务完成时间，有支持实时人员可靠性评估的潜力。

Conclusion: AutoGraph框架可增强复杂社会技术系统中的程序安全性和认知性能，有进一步集成扩展的能力。

Abstract: Digitalization in nuclear power plant (NPP) control rooms is reshaping how
operators interact with procedures and interface elements. However, existing
computer-based procedures (CBPs) often lack semantic integration with
human-system interfaces (HSIs), limiting their capacity to support intelligent
automation and increasing the risk of human error, particularly under dynamic
or complex operating conditions. In this study, we present AutoGraph, a
knowledge-graph-based framework designed to formalize and automate procedure
execution in digitalized NPP environments.AutoGraph integrates (1) a proposed
HTRPM tracking module to capture operator interactions and interface element
locations; (2) an Interface Element Knowledge Graph (IE-KG) encoding spatial,
semantic, and structural properties of HSIs; (3) automatic mapping from textual
procedures to executable interface paths; and (4) an execution engine that maps
textual procedures to executable interface paths. This enables the
identification of cognitively demanding multi-action steps and supports fully
automated execution with minimal operator input. We validate the framework
through representative control room scenarios, demonstrating significant
reductions in task completion time and the potential to support real-time human
reliability assessment. Further integration into dynamic HRA frameworks (e.g.,
COGMIF) and real-time decision support systems (e.g., DRIF) illustrates
AutoGraph extensibility in enhancing procedural safety and cognitive
performance in complex socio-technical systems.

</details>


### [409] [When concept-based XAI is imprecise: Do people distinguish between generalisations and misrepresentations?](https://arxiv.org/abs/2506.17936)
*Romy Müller*

Main category: cs.HC

TL;DR: 研究在铁路安全场景中人们对C - XAI概念泛化的识别和评价，发现人们难自发识别泛化，可能无法从概念推断AI对复杂情况的理解。


<details>
  <summary>Details</summary>
Motivation: 探究人们是否能识别和欣赏C - XAI概念的泛化，并将其与其他不期望的不精确形式区分开。

Method: 在铁路安全实验场景中，让参与者评估模拟AI对危险交通场景的判断，AI用相似图像片段概念解释决策，概念在与分类图像的匹配度上有差异。

Result: 与假设相反，对不太相关特征泛化的概念评分低于精确匹配的概念，参与者对相关特征的不精确性高度敏感。

Conclusion: 人们可能无法自发识别泛化，难以从C - XAI概念推断AI对复杂情况的深入理解。

Abstract: Concept-based explainable artificial intelligence (C-XAI) can help reveal the
inner representations of AI models. Understanding these representations is
particularly important in complex tasks like safety evaluation. Such tasks rely
on high-level semantic information (e.g., about actions) to make decisions
about abstract categories (e.g., whether a situation is dangerous). In this
context, it may desirable for C-XAI concepts to show some variability,
suggesting that the AI is capable of generalising beyond the concrete details
of a situation. However, it is unclear whether people recognise and appreciate
such generalisations and can distinguish them from other, less desirable forms
of imprecision. This was investigated in an experimental railway safety
scenario. Participants evaluated the performance of a simulated AI that
evaluated whether traffic scenes involving people were dangerous. To explain
these decisions, the AI provided concepts in the form of similar image
snippets. These concepts differed in their match with the classified image,
either regarding a highly relevant feature (i.e., relation to tracks) or a less
relevant feature (i.e., actions). Contrary to the hypotheses, concepts that
generalised over less relevant features led to ratings that were lower than for
precisely matching concepts and comparable to concepts that systematically
misrepresented these features. Conversely, participants were highly sensitive
to imprecisions in relevant features. These findings cast doubts on whether
people spontaneously recognise generalisations. Accordingly, they might not be
able to infer from C-XAI concepts whether AI models have gained a deeper
understanding of complex situations.

</details>


### [410] [Conceptualization, Operationalization, and Measurement of Machine Companionship: A Scoping Review](https://arxiv.org/abs/2506.18119)
*Jaime Banks,Zhixin Li*

Main category: cs.HC

TL;DR: 本文对机器陪伴（MC）相关学术研究进行PRISMA指导的范围综述，最终给出MC的定义。


<details>
  <summary>Details</summary>
Motivation: 尽管机器伴侣概念常见，但缺乏对机器陪伴作为正式概念或可衡量变量的深入研究。

Method: 采用PRISMA指导的范围综述，对2017 - 2025年71篇关于MC的学术作品进行系统抽样、调查和综合。

Result: 不同研究在MC的理论、先验属性维度和测量概念上差异较大，有超50个不同测量变量。

Conclusion: 给出文献导向的MC定义，即人机之间随时间展开的、主观积极的自为协调连接。

Abstract: The notion of machine companions has long been embedded in
social-technological imaginaries. Recent advances in AI have moved those media
musings into believable sociality manifested in interfaces, robotic bodies, and
devices. Those machines are often referred to colloquially as "companions" yet
there is little careful engagement of machine companionship (MC) as a formal
concept or measured variable. This PRISMA-guided scoping review systematically
samples, surveys, and synthesizes current scholarly works on MC (N = 71;
2017-2025), to that end. Works varied widely in considerations of MC according
to guiding theories, dimensions of a-priori specified properties (subjectively
positive, sustained over time, co-active, autotelic), and in measured concepts
(with more than 50 distinct measured variables). WE ultimately offer a
literature-guided definition of MC as an autotelic, coordinated connection
between human and machine that unfolds over time and is subjectively positive.

</details>


### [411] [AI Harmonizer: Expanding Vocal Expression with a Generative Neurosymbolic Music AI System](https://arxiv.org/abs/2506.18143)
*Lancelot Blanchard,Cameron Holt,Joseph A. Paradiso*

Main category: cs.HC

TL;DR: 介绍AI Harmonizer可自动生成四声部和声，无需用户输入和声信息，还探讨应用与发展方向并开源实现。


<details>
  <summary>Details</summary>
Motivation: 传统和声器需用户有一定音乐专业知识，AI Harmonizer旨在无需用户先验和声输入就能生成和声。

Method: 集成先进生成式AI的音高检测和语音建模技术，结合自定义训练的符号音乐模型。

Result: 系统可将任何声乐旋律编排成丰富的合唱织体，目前系统离线运行。

Conclusion: 该系统是迈向AI辅助声乐表演和音乐增强的重要一步。

Abstract: Vocals harmonizers are powerful tools to help solo vocalists enrich their
melodies with harmonically supportive voices. These tools exist in various
forms, from commercially available pedals and software to custom-built systems,
each employing different methods to generate harmonies. Traditional harmonizers
often require users to manually specify a key or tonal center, while others
allow pitch selection via an external keyboard-both approaches demanding some
degree of musical expertise. The AI Harmonizer introduces a novel approach by
autonomously generating musically coherent four-part harmonies without
requiring prior harmonic input from the user. By integrating state-of-the-art
generative AI techniques for pitch detection and voice modeling with
custom-trained symbolic music models, our system arranges any vocal melody into
rich choral textures. In this paper, we present our methods, explore potential
applications in performance and composition, and discuss future directions for
real-time implementations. While our system currently operates offline, we
believe it represents a significant step toward AI-assisted vocal performance
and expressive musical augmentation. We release our implementation on GitHub.

</details>


### [412] [Two Sonification Methods for the MindCube](https://arxiv.org/abs/2506.18196)
*Fangzheng Liu,Lancelot Blanchard,Don D. Haddad,Joseph A. Paradiso*

Main category: cs.HC

TL;DR: 探索MindCube音乐界面潜力，提出有无AI的两种映射，讨论结果并给出未来方向。


<details>
  <summary>Details</summary>
Motivation: 研究MindCube作为帮助情绪调节的音乐系统控制器的音乐界面潜力。

Method: 为MindCube提出有AI和无AI的两种映射，在生成式AI映射中提出在潜在空间注入意义及用外部控制器导航的方法。

Result: 文中未明确提及具体结果。

Conclusion: 讨论结果并提出未来工作方向，但未明确具体结论。

Abstract: In this work, we explore the musical interface potential of the MindCube, an
interactive device designed to study emotions. Embedding diverse sensors and
input devices, this interface resembles a fidget cube toy commonly used to help
users relieve their stress and anxiety. As such, it is a particularly
well-suited controller for musical systems that aim to help with emotion
regulation. In this regard, we present two different mappings for the MindCube,
with and without AI. With our generative AI mapping, we propose a way to infuse
meaning within a latent space and techniques to navigate through it with an
external controller. We discuss our results and propose directions for future
work.

</details>


### [413] [BRAVE: Brain-Controlled Prosthetic Arm with Voice Integration and Embodied Learning for Enhanced Mobility](https://arxiv.org/abs/2506.18749)
*Abdul Basit,Maha Nawaz,Muhammad Shafique*

Main category: cs.HC

TL;DR: 提出BRAVE混合式脑电与语音控制假肢系统，结合集成学习与人工在环框架，有高分类准确率、低响应延迟等优点，为无创假肢控制提供新思路。


<details>
  <summary>Details</summary>
Motivation: 现有基于脑电图的假肢控制系统面临信号噪声、分类精度和实时适应性等挑战，需要改进。

Method: 结合LSTM、CNN和随机森林模型构建集成框架进行脑电分类，使用带通滤波器、ICA和CSP对脑电信号预处理；融入自动语音识别实现模式切换；采用LSL网络同步数据采集。

Result: 分类准确率达96%，响应延迟150毫秒，在自制假肢和多名参与者上评估显示出通用性，可低功耗嵌入式部署。

Conclusion: BRAVE为实现强大、实时、无创的假肢控制迈出了有前景的一步。

Abstract: Non-invasive brain-computer interfaces (BCIs) have the potential to enable
intuitive control of prosthetic limbs for individuals with upper limb
amputations. However, existing EEG-based control systems face challenges
related to signal noise, classification accuracy, and real-time adaptability.
In this work, we present BRAVE, a hybrid EEG and voice-controlled prosthetic
system that integrates ensemble learning-based EEG classification with a
human-in-the-loop (HITL) correction framework for enhanced responsiveness.
Unlike traditional electromyography (EMG)-based prosthetic control, BRAVE aims
to interpret EEG-driven motor intent, enabling movement control without
reliance on residual muscle activity. To improve classification robustness,
BRAVE combines LSTM, CNN, and Random Forest models in an ensemble framework,
achieving a classification accuracy of 96% across test subjects. EEG signals
are preprocessed using a bandpass filter (0.5-45 Hz), Independent Component
Analysis (ICA) for artifact removal, and Common Spatial Pattern (CSP) feature
extraction to minimize contamination from electromyographic (EMG) and
electrooculographic (EOG) signals. Additionally, BRAVE incorporates automatic
speech recognition (ASR) to facilitate intuitive mode switching between
different degrees of freedom (DOF) in the prosthetic arm. The system operates
in real time, with a response latency of 150 ms, leveraging Lab Streaming Layer
(LSL) networking for synchronized data acquisition. The system is evaluated on
an in-house fabricated prosthetic arm and on multiple participants highlighting
the generalizability across users. The system is optimized for low-power
embedded deployment, ensuring practical real-world application beyond
high-performance computing environments. Our results indicate that BRAVE offers
a promising step towards robust, real-time, non-invasive prosthetic control.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [414] [Disaster Risk Financing through Taxation: A Framework for Regional Participation in Collective Risk-Sharing](https://arxiv.org/abs/2506.18895)
*Fallou Niakh,Arthur Charpentier,Caroline Hillairet,Philipp Ratz*

Main category: econ.TH

TL;DR: 考虑多区域巨灾保险，引入公私合营应对保险公司违约风险，提出区域参与集体风险分担理论框架。


<details>
  <summary>Details</summary>
Motivation: 不同风险区域希望通过多区域巨灾保险对冲灾难风险，且保险公司有破产可能性，需保护区域免受保险公司违约风险。

Method: 引入政府与保险公司的公私合营模式，灾难损失超保险公司资本时，政府实施税收系统分担剩余索赔。

Result: 提出了考虑区域灾难风险概况和经济状况，通过税收收入参与集体风险分担的理论框架。

Conclusion: 构建了一个能让不同区域基于自身情况参与集体风险分担的理论模型。

Abstract: We consider an economy composed of different risk profile regions wishing to
be hedged against a disaster risk using multi-region catastrophe insurance.
Such catastrophic events inherently have a systemic component; we consider
situations where the insurer faces a non-zero probability of insolvency. To
protect the regions against the risk of the insurer's default, we introduce a
public-private partnership between the government and the insurer. When a
disaster generates losses exceeding the total capital of the insurer, the
central government intervenes by implementing a taxation system to share the
residual claims. In this study, we propose a theoretical framework for regional
participation in collective risk-sharing through tax revenues by accounting for
their disaster risk profiles and their economic status.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [415] [BrainSymphony: A Transformer-Driven Fusion of fMRI Time Series and Structural Connectivity](https://arxiv.org/abs/2506.18314)
*Moein Khajehnejad,Forough Habibollahi,Adeel Razi*

Main category: q-bio.QM

TL;DR: 提出轻量级基础模型BrainSymphony，在小数据集预训练，表现超大型模型。


<details>
  <summary>Details</summary>
Motivation: 现有神经影像基础模型过大且数据需求高，需轻量级模型。

Method: 通过并行时空transformer流处理功能MRI数据，用新型有符号图transformer处理扩散MRI，经自适应融合门整合。

Result: 在多种下游基准测试中表现超大型模型，在特殊数据集揭示大脑动力学新见解。

Conclusion: 架构感知的多模态模型可超越更大模型，为计算神经科学研究提供新思路。

Abstract: Existing foundation models for neuroimaging are often prohibitively large and
data-intensive. We introduce BrainSymphony, a lightweight, parameter-efficient
foundation model that achieves state-of-the-art performance while being
pre-trained on significantly smaller public datasets. BrainSymphony's strong
multimodal architecture processes functional MRI data through parallel spatial
and temporal transformer streams, which are then efficiently distilled into a
unified representation by a Perceiver module. Concurrently, it models
structural connectivity from diffusion MRI using a novel signed graph
transformer to encode the brain's anatomical structure. These powerful,
modality-specific representations are then integrated via an adaptive fusion
gate. Despite its compact design, our model consistently outperforms larger
models on a diverse range of downstream benchmarks, including classification,
prediction, and unsupervised network identification tasks. Furthermore, our
model revealed novel insights into brain dynamics using attention maps on a
unique external psilocybin neuroimaging dataset (pre- and post-administration).
BrainSymphony establishes that architecturally-aware, multimodal models can
surpass their larger counterparts, paving the way for more accessible and
powerful research in computational neuroscience.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [416] [From Drawings to Decisions: A Hybrid Vision-Language Framework for Parsing 2D Engineering Drawings into Structured Manufacturing Knowledge](https://arxiv.org/abs/2506.17374)
*Muhammad Tayyab Khan,Lequn Chen,Zane Yong,Jun Ming Tan,Wenhe Feng,Seung Ki Moon*

Main category: cs.CV

TL;DR: 本文提出混合视觉 - 语言框架从二维工程图提取关键信息，在自建数据集上实验，Donut 模型表现更佳，案例显示该框架对制造任务有实用价值。


<details>
  <summary>Details</summary>
Motivation: 手动从二维工程图提取关键信息效率低，通用 OCR 模型因布局复杂等因素输出不可靠，需新方法解决。

Method: 提出集成旋转感知目标检测模型 YOLOv11 - obb 和基于 Transformer 的视觉 - 语言解析器的混合框架，在自建数据集上训练 YOLOv11 - OBB 检测并提取注释补丁，用 Donut 和 Florence - 2 两个轻量级 VLM 解析，微调后对比评估。

Result: 对比实验中 Donut 表现优于 Florence - 2，精度 88.5%、召回率 99.2%、F1 分数 93.5%，幻觉率 11.5%。

Conclusion: 所提框架能有效从二维工程图提取结构化信息，支持下游制造任务，对二维图纸解释现代化有实用价值。

Abstract: Efficient and accurate extraction of key information from 2D engineering
drawings is essential for advancing digital manufacturing workflows. Such
information includes geometric dimensioning and tolerancing (GD&T), measures,
material specifications, and textual annotations. Manual extraction is slow and
labor-intensive, while generic OCR models often fail due to complex layouts,
engineering symbols, and rotated text, leading to incomplete and unreliable
outputs. These limitations result in incomplete and unreliable outputs. To
address these challenges, we propose a hybrid vision-language framework that
integrates a rotation-aware object detection model (YOLOv11-obb) with a
transformer-based vision-language parser. Our structured pipeline applies
YOLOv11-OBB to localize annotations and extract oriented bounding box (OBB)
patches, which are then parsed into structured outputs using a fine-tuned,
lightweight vision-language model (VLM). We curate a dataset of 1,367 2D
mechanical drawings annotated across nine key categories. YOLOv11-OBB is
trained on this dataset to detect OBBs and extract annotation patches. These
are parsed using two open-source VLMs: Donut and Florence-2. Both models are
lightweight and well-suited for specialized industrial tasks under limited
computational overhead. Following fine-tuning of both models on the curated
dataset of image patches paired with structured annotation labels, a
comparative experiment is conducted to evaluate parsing performance across four
key metrics. Donut outperforms Florence-2, achieving 88.5% precision, 99.2%
recall, and a 93.5% F1-score, with a hallucination rate of 11.5%. Finally, a
case study demonstrates how the extracted structured information supports
downstream manufacturing tasks such as process and tool selection, showcasing
the practical utility of the proposed framework in modernizing 2D drawing
interpretation.

</details>


### [417] [RadarSeq: A Temporal Vision Framework for User Churn Prediction via Radar Chart Sequences](https://arxiv.org/abs/2506.17325)
*Sina Najafi,M. Hadi Sepanj,Fahimeh Jafari*

Main category: cs.CV

TL;DR: 提出时间感知的计算机视觉框架预测非订阅零工平台用户流失，在真实数据集上表现优于经典模型和基线，适合大规模流失建模。


<details>
  <summary>Details</summary>
Motivation: 非订阅零工平台用户流失预测因缺乏明确标签和用户行为动态性面临挑战，现有方法忽视时间线索。

Method: 提出时间感知计算机视觉框架，将用户行为模式建模为雷达图图像序列，集成预训练CNN编码器和双向LSTM。

Result: 在真实数据集实验中，F1分数提升17.7，精度提升29.4，AUC提升16.1，可解释性增强。

Conclusion: 该框架模块化设计、可解释性工具和高效部署特性使其适用于动态零工经济平台大规模流失建模。

Abstract: Predicting user churn in non-subscription gig platforms, where disengagement
is implicit, poses unique challenges due to the absence of explicit labels and
the dynamic nature of user behavior. Existing methods often rely on aggregated
snapshots or static visual representations, which obscure temporal cues
critical for early detection. In this work, we propose a temporally-aware
computer vision framework that models user behavioral patterns as a sequence of
radar chart images, each encoding day-level behavioral features. By integrating
a pretrained CNN encoder with a bidirectional LSTM, our architecture captures
both spatial and temporal patterns underlying churn behavior. Extensive
experiments on a large real-world dataset demonstrate that our method
outperforms classical models and ViT-based radar chart baselines, yielding
gains of 17.7 in F1 score, 29.4 in precision, and 16.1 in AUC, along with
improved interpretability. The framework's modular design, explainability
tools, and efficient deployment characteristics make it suitable for
large-scale churn modeling in dynamic gig-economy platforms.

</details>


### [418] [P2MFDS: A Privacy-Preserving Multimodal Fall Detection System for Elderly People in Bathroom Environments](https://arxiv.org/abs/2506.17332)
*Haitian Wang,Yiren Wang,Xinyu Wang,Yumeng Miao,Yuliang Zhang,Yu Zhang,Atif Mansoor*

Main category: cs.CV

TL;DR: 预计到2050年，65岁以上人口将占全球人口16%，浴室环境老人易摔倒。现有单模态系统有局限，本文提出隐私保护多模态摔倒检测系统，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 人口老龄化使老人摔倒风险增加，浴室是摔倒高发地，现有单模态摔倒检测系统在复杂环境下精度受限，存在系统偏差和环境干扰问题。

Method: 开发传感器评估框架，融合毫米波雷达和3D振动传感构建隐私保护多模态数据集；引入P2MFDS双流网络，结合不同分支检测运动和振动。

Result: 所提系统在精度和召回率上比现有方法有显著提升。

Conclusion: 提出的隐私保护多模态摔倒检测系统有效解决了现有单模态系统的局限，具有更好的检测效果。

Abstract: By 2050, people aged 65 and over are projected to make up 16 percent of the
global population. As aging is closely associated with increased fall risk,
particularly in wet and confined environments such as bathrooms where over 80
percent of falls occur. Although recent research has increasingly focused on
non-intrusive, privacy-preserving approaches that do not rely on wearable
devices or video-based monitoring, these efforts have not fully overcome the
limitations of existing unimodal systems (e.g., WiFi-, infrared-, or
mmWave-based), which are prone to reduced accuracy in complex environments.
These limitations stem from fundamental constraints in unimodal sensing,
including system bias and environmental interference, such as multipath fading
in WiFi-based systems and drastic temperature changes in infrared-based
methods. To address these challenges, we propose a Privacy-Preserving
Multimodal Fall Detection System for Elderly People in Bathroom Environments.
First, we develop a sensor evaluation framework to select and fuse
millimeter-wave radar with 3D vibration sensing, and use it to construct and
preprocess a large-scale, privacy-preserving multimodal dataset in real
bathroom settings, which will be released upon publication. Second, we
introduce P2MFDS, a dual-stream network combining a CNN-BiLSTM-Attention branch
for radar motion dynamics with a multi-scale CNN-SEBlock-Self-Attention branch
for vibration impact detection. By uniting macro- and micro-scale features,
P2MFDS delivers significant gains in accuracy and recall over state-of-the-art
approaches. Code and pretrained models will be made available at:
https://github.com/HaitianWang/P2MFDS-A-Privacy-Preserving-Multimodal-Fall-Detection-Network-for-Elderly-Individuals-in-Bathroom.

</details>


### [419] [A Novel Multi-layer Task-centric and Data Quality Framework for Autonomous Driving](https://arxiv.org/abs/2506.17346)
*Yuhan Zhou,Haihua Chen,Kewei Sha*

Main category: cs.CV

TL;DR: 论文提出面向下一代自动驾驶汽车的任务中心数据质量框架，通过案例证明去除数据冗余可提升任务性能，指出相关领域挑战并有望助力构建更优自动驾驶汽车。


<details>
  <summary>Details</summary>
Motivation: 下一代自动驾驶汽车依赖多源多模态数据，但数据质量因环境等因素有差异，且该领域重模型轻数据质量，需满足其功能、效率和可信度需求。

Method: 提出包含数据层、数据质量层、任务层、应用层和目标层的任务中心数据质量框架，通过在nuScenes数据集上的案例研究验证。

Result: 部分去除多源图像数据冗余可提升YOLOv8目标检测任务性能，对图像和LiDAR多模态数据分析呈现现有冗余数据质量问题。

Conclusion: 该框架开启了自动驾驶领域数据质量、任务编排和面向性能系统开发交叉处的挑战研究，有望指导构建更自适应、可解释和有弹性的自动驾驶汽车。

Abstract: The next-generation autonomous vehicles (AVs), embedded with frequent
real-time decision-making, will rely heavily on a large volume of multisource
and multimodal data. In real-world settings, the data quality (DQ) of different
sources and modalities usually varies due to unexpected environmental factors
or sensor issues. However, both researchers and practitioners in the AV field
overwhelmingly concentrate on models/algorithms while undervaluing the DQ. To
fulfill the needs of the next-generation AVs with guarantees of functionality,
efficiency, and trustworthiness, this paper proposes a novel task-centric and
data quality vase framework which consists of five layers: data layer, DQ
layer, task layer, application layer, and goal layer. The proposed framework
aims to map DQ with task requirements and performance goals. To illustrate, a
case study investigating redundancy on the nuScenes dataset proves that
partially removing redundancy on multisource image data could improve YOLOv8
object detection task performance. Analysis on multimodal data of image and
LiDAR further presents existing redundancy DQ issues. This paper opens up a
range of critical but unexplored challenges at the intersection of DQ, task
orchestration, and performance-oriented system development in AVs. It is
expected to guide the AV community toward building more adaptive, explainable,
and resilient AVs that respond intelligently to dynamic environments and
heterogeneous data streams. Code, data, and implementation details are publicly
available at: https://anonymous.4open.science/r/dq4av-framework/README.md.

</details>


### [420] [Trans${^2}$-CBCT: A Dual-Transformer Framework for Sparse-View CBCT Reconstruction](https://arxiv.org/abs/2506.17425)
*Minmin Yang,Huantao Ren,Senem Velipasalar*

Main category: cs.CV

TL;DR: 提出Trans - CBCT和Trans² - CBCT模型解决稀疏视图CBCT重建问题，在LUNA16和ToothFairy数据集实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 解决稀疏视图CBCT重建中欠采样导致的强伪影和空间覆盖不足问题。

Method: 用TransUNet替换传统编码器得Trans - CBCT；引入邻域感知Point Transformer得Trans² - CBCT。

Result: Trans - CBCT在LUNA16数据集上PSNR超基线1.17 dB、SSIM超0.0163；Trans² - CBCT额外提升PSNR 0.63 dB、SSIM 0.0117。

Conclusion: 结合CNN - Transformer特征与基于点的几何推理用于稀疏视图CBCT重建有效。

Abstract: Cone-beam computed tomography (CBCT) using only a few X-ray projection views
enables faster scans with lower radiation dose, but the resulting severe
under-sampling causes strong artifacts and poor spatial coverage. We address
these challenges in a unified framework. First, we replace conventional
UNet/ResNet encoders with TransUNet, a hybrid CNN-Transformer model.
Convolutional layers capture local details, while self-attention layers enhance
global context. We adapt TransUNet to CBCT by combining multi-scale features,
querying view-specific features per 3D point, and adding a lightweight
attenuation-prediction head. This yields Trans-CBCT, which surpasses prior
baselines by 1.17 dB PSNR and 0.0163 SSIM on the LUNA16 dataset with six views.
Second, we introduce a neighbor-aware Point Transformer to enforce volumetric
coherence. This module uses 3D positional encoding and attention over k-nearest
neighbors to improve spatial consistency. The resulting model, Trans$^2$-CBCT,
provides an additional gain of 0.63 dB PSNR and 0.0117 SSIM. Experiments on
LUNA16 and ToothFairy show consistent gains from six to ten views, validating
the effectiveness of combining CNN-Transformer features with point-based
geometry reasoning for sparse-view CBCT reconstruction.

</details>


### [421] [SynDaCaTE: A Synthetic Dataset For Evaluating Part-Whole Hierarchical Inference](https://arxiv.org/abs/2506.17558)
*Jake Levi,Mark van der Wilk*

Main category: cs.CV

TL;DR: 提出SynDaCaTE数据集用于胶囊网络测试评估，展示现有胶囊模型瓶颈及自注意力机制对部分到整体推理的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决难以评估胶囊网络是否真正学习到部分 - 整体层次结构推理的问题。

Method: 提出SynDaCaTE数据集。

Result: 展示了现有胶囊模型的精确瓶颈，证明排列等变自注意力机制对部分到整体推理非常有效。

Conclusion: 为计算机视觉设计有效归纳偏置提供了未来方向。

Abstract: Learning to infer object representations, and in particular part-whole
hierarchies, has been the focus of extensive research in computer vision, in
pursuit of improving data efficiency, systematic generalisation, and
robustness. Models which are \emph{designed} to infer part-whole hierarchies,
often referred to as capsule networks, are typically trained end-to-end on
supervised tasks such as object classification, in which case it is difficult
to evaluate whether such a model \emph{actually} learns to infer part-whole
hierarchies, as claimed. To address this difficulty, we present a SYNthetic
DAtaset for CApsule Testing and Evaluation, abbreviated as SynDaCaTE, and
establish its utility by (1) demonstrating the precise bottleneck in a
prominent existing capsule model, and (2) demonstrating that
permutation-equivariant self-attention is highly effective for parts-to-wholes
inference, which motivates future directions for designing effective inductive
biases for computer vision.

</details>


### [422] [VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models](https://arxiv.org/abs/2506.17561)
*Chongkai Gao,Zixuan Liu,Zhenghao Chi,Junshan Huang,Xin Fei,Yiwen Hou,Yuxuan Zhang,Yudi Lin,Zhirui Fang,Zeyu Jiang,Lin Shao*

Main category: cs.CV

TL;DR: 本文引入统一VLA架构VLA - OS并设计对照实验，探究不同规划范式和表示的影响，发现视觉规划表示更好，Hierarchical - VLA范式有优有劣。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型方法在多方面差异大，难以确定性能提升来源和待改进组件，需系统研究不同规划范式和表示的影响。

Method: 引入统一VLA架构VLA - OS，设计涵盖不同对象类别、视觉模态、环境和末端执行器的对照实验。

Result: 视觉规划表示通常优于语言规划表示；Hierarchical - VLA范式在多项能力上表现较好，但训练和推理速度慢。

Conclusion: 明确了不同规划范式和表示的性能表现，Hierarchical - VLA范式虽有速度问题，但综合性能有优势。

Abstract: Recent studies on Vision-Language-Action (VLA) models have shifted from the
end-to-end action-generation paradigm toward a pipeline involving task planning
followed by action generation, demonstrating improved performance on various
complex, long-horizon manipulation tasks. However, existing approaches vary
significantly in terms of network architectures, planning paradigms,
representations, and training data sources, making it challenging for
researchers to identify the precise sources of performance gains and components
to be further improved. To systematically investigate the impacts of different
planning paradigms and representations isolating from network architectures and
training data, in this paper, we introduce VLA-OS, a unified VLA architecture
series capable of various task planning paradigms, and design a comprehensive
suite of controlled experiments across diverse object categories (rigid and
deformable), visual modalities (2D and 3D), environments (simulation and
real-world), and end-effectors (grippers and dexterous hands). Our results
demonstrate that: 1) visually grounded planning representations are generally
better than language planning representations; 2) the Hierarchical-VLA paradigm
generally achieves superior or comparable performance than other paradigms on
task performance, pretraining, generalization ability, scalability, and
continual learning ability, albeit at the cost of slower training and inference
speeds.

</details>


### [423] [HalluRNN: Mitigating Hallucinations via Recurrent Cross-Layer Reasoning in Large Vision-Language Models](https://arxiv.org/abs/2506.17587)
*Le Yu,Kaishen Wang,Jianlong Xiong,Yue Cao,Tao He*

Main category: cs.CV

TL;DR: 本文提出架构级解决方案HalluRNN，通过循环跨层推理增强模型稳定性，微调特定模块在多基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型易产生幻觉，现有方法需大量资源或特定配置，要解决此问题。

Method: 引入HalluRNN，提出DG - DPU模块，该模块跨层共享并循环细化隐藏状态，仅微调DG - DPU模块。

Result: HalluRNN在多个基准测试中实现了强大且稳健的性能。

Conclusion: 架构级的HalluRNN能有效缓解大视觉语言模型的幻觉问题，提升模型稳定性。

Abstract: Though Large Vision-Language Models (LVLMs) have achieved remarkable
performance across various tasks, they are still prone to
hallucinations-generating outputs that are textually plausible but visually
ungrounded. While prior approaches generally address this issue through
data-centric fine-tuning or innovative decoding strategies, these methods often
require substantial resources or task-specific configurations. In this work, we
introduce an architecture-level solution, HalluRNN, which enhances model
stability through recurrent cross-layer reasoning. Specifically, we propose a
novel Dual-Gated Depth Propagation Unit (DG-DPU) module, which is shared across
layers and recurrently refines hidden states. This allows for the adaptive
propagation of information throughout the model, enforces consistency across
layers, and mitigates hallucinations caused by representational drift. By
fine-tuning only the DG-DPU module, HalluRNN achieves strong and robust
performance across multiple benchmarks.

</details>


### [424] [DRAMA-X: A Fine-grained Intent Prediction and Risk Reasoning Benchmark For Driving](https://arxiv.org/abs/2506.17590)
*Mihir Godbole,Xiangbo Gao,Zhengzhong Tu*

Main category: cs.CV

TL;DR: 文章引入DRAMA - X基准评估自动驾驶中多类意图预测，提出SGG - Intent框架作基线，实验表明场景图推理可提升意图预测和风险评估。


<details>
  <summary>Details</summary>
Motivation: 现有基准缺乏对安全关键情况下多类意图预测的评估，而理解VRUs短期运动对自动驾驶安全至关重要。

Method: 通过自动化标注流程从DRAMA数据集构建DRAMA - X基准，提出SGG - Intent轻量级无训练框架，用VLM支持的检测器生成场景图，用大语言模型进行推理。

Result: 对多种VLMs进行评估，实验表明场景图推理在显式建模上下文线索时能增强意图预测和风险评估。

Conclusion: 场景图推理在自动驾驶的意图预测和风险评估等任务中有积极作用，DRAMA - X基准有助于自动驾驶决策相关任务的结构化评估。

Abstract: Understanding the short-term motion of vulnerable road users (VRUs) like
pedestrians and cyclists is critical for safe autonomous driving, especially in
urban scenarios with ambiguous or high-risk behaviors. While vision-language
models (VLMs) have enabled open-vocabulary perception, their utility for
fine-grained intent reasoning remains underexplored. Notably, no existing
benchmark evaluates multi-class intent prediction in safety-critical
situations, To address this gap, we introduce DRAMA-X, a fine-grained benchmark
constructed from the DRAMA dataset via an automated annotation pipeline.
DRAMA-X contains 5,686 accident-prone frames labeled with object bounding
boxes, a nine-class directional intent taxonomy, binary risk scores,
expert-generated action suggestions for the ego vehicle, and descriptive motion
summaries. These annotations enable a structured evaluation of four
interrelated tasks central to autonomous decision-making: object detection,
intent prediction, risk assessment, and action suggestion. As a reference
baseline, we propose SGG-Intent, a lightweight, training-free framework that
mirrors the ego vehicle's reasoning pipeline. It sequentially generates a scene
graph from visual input using VLM-backed detectors, infers intent, assesses
risk, and recommends an action using a compositional reasoning stage powered by
a large language model. We evaluate a range of recent VLMs, comparing
performance across all four DRAMA-X tasks. Our experiments demonstrate that
scene-graph-based reasoning enhances intent prediction and risk assessment,
especially when contextual cues are explicitly modeled.

</details>


### [425] [CLiViS: Unleashing Cognitive Map through Linguistic-Visual Synergy for Embodied Visual Reasoning](https://arxiv.org/abs/2506.17629)
*Kailing Li,Qi'ao Xu,Tianwen Qian,Yuqian Fu,Yang Jiao,Xiaoling Wang*

Main category: cs.CV

TL;DR: 本文提出无训练框架CLiViS解决具身视觉推理（EVR）问题，利用大语言模型和视觉语言模型优势，通过动态认知地图连接感知与推理，实验证明其有效性和泛化性。


<details>
  <summary>Details</summary>
Motivation: EVR面临复杂指令多样性和长时第一人称视频时空动态的挑战，现有解决方案存在不足。

Method: 提出无训练框架CLiViS，利用大语言模型进行高层任务规划，协调视觉语言模型进行开放世界视觉感知以迭代更新场景上下文，核心是动态认知地图。

Result: 在多个基准测试上的大量实验证明了CLiViS的有效性和泛化性，尤其在处理长时视觉依赖方面表现出色。

Conclusion: CLiViS能有效解决EVR问题，可处理长时视觉依赖，代码开源。

Abstract: Embodied Visual Reasoning (EVR) seeks to follow complex, free-form
instructions based on egocentric video, enabling semantic understanding and
spatiotemporal reasoning in dynamic environments. Despite its promising
potential, EVR encounters significant challenges stemming from the diversity of
complex instructions and the intricate spatiotemporal dynamics in long-term
egocentric videos. Prior solutions either employ Large Language Models (LLMs)
over static video captions, which often omit critical visual details, or rely
on end-to-end Vision-Language Models (VLMs) that struggle with stepwise
compositional reasoning. Consider the complementary strengths of LLMs in
reasoning and VLMs in perception, we propose CLiViS. It is a novel
training-free framework that leverages LLMs for high-level task planning and
orchestrates VLM-driven open-world visual perception to iteratively update the
scene context. Building on this synergy, the core of CLiViS is a dynamic
Cognitive Map that evolves throughout the reasoning process. This map
constructs a structured representation of the embodied scene, bridging
low-level perception and high-level reasoning. Extensive experiments across
multiple benchmarks demonstrate the effectiveness and generality of CLiViS,
especially in handling long-term visual dependencies. Code is available at
https://github.com/Teacher-Tom/CLiViS.

</details>


### [426] [Adaptive Multi-prompt Contrastive Network for Few-shot Out-of-distribution Detection](https://arxiv.org/abs/2506.17633)
*Xiang Fang,Arvind Easwaran,Blaise Genest*

Main category: cs.CV

TL;DR: 提出Adaptive Multi - prompt Contrastive Network (AMCN)用于少样本OOD检测，实验表明其性能优于其他先进方法。


<details>
  <summary>Details</summary>
Motivation: 多数OOD检测方法需大量IID样本训练，限制实际应用；少样本OOD检测更具挑战且以往工作忽略类间多样性。

Method: 提出AMCN，通过学习类间和类内分布调整ID - OOD分离边界；利用CLIP连接文本和图像，设计可学习的ID和OOD文本提示；生成自适应提示、类阈值和自适应类边界，提出提示引导的ID - OOD分离模块。

Result: AMCN在实验中性能优于其他先进方法。

Conclusion: AMCN在少样本OOD检测任务中有效，能取得较好效果。

Abstract: Out-of-distribution (OOD) detection attempts to distinguish outlier samples
to prevent models trained on the in-distribution (ID) dataset from producing
unavailable outputs. Most OOD detection methods require many IID samples for
training, which seriously limits their real-world applications. To this end, we
target a challenging setting: few-shot OOD detection, where {Only a few {\em
labeled ID} samples are available.} Therefore, few-shot OOD detection is much
more challenging than the traditional OOD detection setting. Previous few-shot
OOD detection works ignore the distinct diversity between different classes. In
this paper, we propose a novel network: Adaptive Multi-prompt Contrastive
Network (AMCN), which adapts the ID-OOD separation boundary by learning inter-
and intra-class distribution. To compensate for the absence of OOD and scarcity
of ID {\em image samples}, we leverage CLIP, connecting text with images,
engineering learnable ID and OOD {\em textual prompts}. Specifically, we first
generate adaptive prompts (learnable ID prompts, label-fixed OOD prompts and
label-adaptive OOD prompts). Then, we generate an adaptive class boundary for
each class by introducing a class-wise threshold. Finally, we propose a
prompt-guided ID-OOD separation module to control the margin between ID and OOD
prompts. Experimental results show that AMCN outperforms other state-of-the-art
works.

</details>


### [427] [Programmable-Room: Interactive Textured 3D Room Meshes Generation Empowered by Large Language Models](https://arxiv.org/abs/2506.17707)
*Jihyun Kim,Junho Park,Kyeongbo Kong,Suk-Ju Kang*

Main category: cs.CV

TL;DR: 提出Programmable - Room框架，能依自然语言指令交互式生成和编辑3D房间网格，用视觉编程统一支持各子任务，展示了框架灵活性和优越性。


<details>
  <summary>Details</summary>
Motivation: 实现依据自然语言指令对3D房间网格进行精确交互生成和编辑。

Method: 将任务分解为创建房间网格坐标、生成纹理全景图等步骤，采用视觉编程，利用大语言模型编写程序；纹理生成模块用预训练扩散模型，结合双向LSTM优化训练目标。

Result: 展示了Programmable - Room在生成和编辑3D房间网格方面的灵活性，定量和定性证明其优于现有模型。

Conclusion: Programmable - Room框架能有效实现依据自然语言的3D房间网格生成和编辑，具有良好性能。

Abstract: We present Programmable-Room, a framework which interactively generates and
edits a 3D room mesh, given natural language instructions. For precise control
of a room's each attribute, we decompose the challenging task into simpler
steps such as creating plausible 3D coordinates for room meshes, generating
panorama images for the texture, constructing 3D meshes by integrating the
coordinates and panorama texture images, and arranging furniture. To support
the various decomposed tasks with a unified framework, we incorporate visual
programming (VP). VP is a method that utilizes a large language model (LLM) to
write a Python-like program which is an ordered list of necessary modules for
the various tasks given in natural language. We develop most of the modules.
Especially, for the texture generating module, we utilize a pretrained
large-scale diffusion model to generate panorama images conditioned on text and
visual prompts (i.e., layout, depth, and semantic map) simultaneously.
Specifically, we enhance the panorama image generation quality by optimizing
the training objective with a 1D representation of a panorama scene obtained
from bidirectional LSTM. We demonstrate Programmable-Room's flexibility in
generating and editing 3D room meshes, and prove our framework's superiority to
an existing model quantitatively and qualitatively. Project page is available
in https://jihyun0510.github.io/Programmable_Room_Page/.

</details>


### [428] [Fine-Scale Soil Mapping in Alaska with Multimodal Machine Learning](https://arxiv.org/abs/2506.17302)
*Yijun Lin,Theresa Chen,Colby Brungard,Grunwald Sabine,Sue Ives,Matt Macander,Timm Nawrocki,Yao-Yi Chiang,Nic Jelinski*

Main category: cs.CV

TL;DR: 论文提出基于视觉的机器学习模型MISO用于阿拉斯加精细尺度土壤制图，对比传统模型表现更好，展示了先进ML方法潜力。


<details>
  <summary>Details</summary>
Motivation: 阿拉斯加精细尺度土壤制图关键但发展不足，气候变化下永久冻土融化威胁大，高分辨率土壤图对刻画永久冻土分布等至关重要。

Method: 提出MISO模型，集成地理空间基础模型、隐式神经表示和对比学习；与传统随机森林模型对比。

Result: 空间交叉验证和区域分析表明，MISO对偏远未知地点泛化性更好，召回率高于随机森林。

Conclusion: 先进机器学习方法在精细尺度土壤制图有潜力，为永久冻土区土壤采样和基础设施规划提供指导。

Abstract: Fine-scale soil mapping in Alaska, traditionally relying on fieldwork and
localized simulations, remains a critical yet underdeveloped task, despite the
region's ecological importance and extensive permafrost coverage. As permafrost
thaw accelerates due to climate change, it threatens infrastructure stability
and key ecosystem services, such as soil carbon storage. High-resolution soil
maps are essential for characterizing permafrost distribution, identifying
vulnerable areas, and informing adaptation strategies. We present MISO, a
vision-based machine learning (ML) model to produce statewide fine-scale soil
maps for near-surface permafrost and soil taxonomy. The model integrates a
geospatial foundation model for visual feature extraction, implicit neural
representations for continuous spatial prediction, and contrastive learning for
multimodal alignment and geo-location awareness. We compare MISO with Random
Forest (RF), a traditional ML model that has been widely used in soil mapping
applications. Spatial cross-validation and regional analysis across Permafrost
Zones and Major Land Resource Areas (MLRAs) show that MISO generalizes better
to remote, unseen locations and achieves higher recall than RF, which is
critical for monitoring permafrost thaw and related environmental processes.
These findings demonstrate the potential of advanced ML approaches for
fine-scale soil mapping and provide practical guidance for future soil sampling
and infrastructure planning in permafrost-affected landscapes. The project will
be released at https://github.com/knowledge-computing/Peatland-permafrost.

</details>


### [429] [Efficient Feedback Gate Network for Hyperspectral Image Super-Resolution](https://arxiv.org/abs/2506.17361)
*Xufei Wang,Mingjian Zhang,Fei Ge,Jinchen Zhu,Wen Sha,Jifen Ren,Zhimeng Hou,Shouguo Zheng,ling Zheng,Shizhuang Weng*

Main category: cs.CV

TL;DR: 提出基于组的单高光谱图像超分辨率方法EFGN，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有单高光谱图像超分辨率方法未充分探索波段和空谱信息，导致性能受限。

Method: 提出高效反馈门网络，利用反馈和门操作，含大核卷积和光谱交互；用SPDFM学习丰富信息；开发SSRGM模块获取特征；应用三维SSRGM增强数据整体信息和连贯性。

Result: 在三个高光谱数据集上实验，该网络在光谱保真度和空间内容重建方面优于现有方法。

Conclusion: 所提方法在单高光谱图像超分辨率上有优越性能。

Abstract: Even without auxiliary images, single hyperspectral image super-resolution
(SHSR) methods can be designed to improve the spatial resolution of
hyperspectral images. However, failing to explore coherence thoroughly along
bands and spatial-spectral information leads to the limited performance of the
SHSR. In this study, we propose a novel group-based SHSR method termed the
efficient feedback gate network, which uses various feedbacks and gate
operations involving large kernel convolutions and spectral interactions. In
particular, by providing different guidance for neighboring groups, we can
learn rich band information and hierarchical hyperspectral spatial information
using channel shuffling and dilatation convolution in shuffled and progressive
dilated fusion module(SPDFM). Moreover, we develop a wide-bound perception gate
block and a spectrum enhancement gate block to construct the spatial-spectral
reinforcement gate module (SSRGM) and obtain highly representative
spatial-spectral features efficiently. Additionally, we apply a
three-dimensional SSRGM to enhance holistic information and coherence for
hyperspectral data. The experimental results on three hyperspectral datasets
demonstrate the superior performance of the proposed network over the
state-of-the-art methods in terms of spectral fidelity and spatial content
reconstruction.

</details>


### [430] [SurgVidLM: Towards Multi-grained Surgical Video Understanding with Large Language Model](https://arxiv.org/abs/2506.17873)
*Guankun Wang,Wenjin Mo,Junyi Wang,Long Bai,Kun Yuan,Ming Hu,Jinlin Wu,Junjun He,Yiming Huang,Nicolas Padoy,Zhen Lei,Hongbin Liu,Nassir Navab,Hongliang Ren*

Main category: cs.CV

TL;DR: 本文提出首个外科视频语言模型SurgVidLM用于手术视频理解，构建SVU - 31K数据集，引入StageFocus机制和Multi - frequency Fusion Attention，实验显示其性能优越。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门用于细粒度手术视频理解任务的Vid - LLMs，而细粒度理解对分析手术具体过程和细节至关重要。

Method: 提出SurgVidLM，构建SVU - 31K数据集，引入StageFocus机制进行多粒度渐进式理解，开发Multi - frequency Fusion Attention整合视觉标记。

Result: SurgVidLM在全量和细粒度视频理解任务上显著优于现有Vid - LLMs。

Conclusion: SurgVidLM在捕捉复杂手术过程上下文方面能力出众。

Abstract: Recent advances in Multimodal Large Language Models have demonstrated great
potential in the medical domain, facilitating users to understand surgical
scenes and procedures. Beyond image-based methods, the exploration of Video
Large Language Models (Vid-LLMs) has emerged as a promising avenue for
capturing the complex sequences of information involved in surgery. However,
there is still a lack of Vid-LLMs specialized for fine-grained surgical video
understanding tasks, which is crucial for analyzing specific processes or
details within a surgical procedure. To bridge this gap, we propose SurgVidLM,
the first video language model designed to address both full and fine-grained
surgical video comprehension. To train our SurgVidLM, we construct the SVU-31K
dataset which consists of over 31K video-instruction pairs, enabling both
holistic understanding and detailed analysis of surgical procedures.
Furthermore, we introduce the StageFocus mechanism which is a two-stage
framework performing the multi-grained, progressive understanding of surgical
videos. We also develop the Multi-frequency Fusion Attention to effectively
integrate low and high-frequency visual tokens, ensuring the retention of
critical information. Experimental results demonstrate that SurgVidLM
significantly outperforms state-of-the-art Vid-LLMs in both full and
fine-grained video understanding tasks, showcasing its superior capability in
capturing complex procedural contexts.

</details>


### [431] [StainPIDR: A Pathological Image Decouplingand Reconstruction Method for StainNormalization Based on Color VectorQuantization and Structure Restaining](https://arxiv.org/abs/2506.17879)
*Zheng Chen*

Main category: cs.CV

TL;DR: 提出染色归一化方法StainPIDR，包括解耦特征、重染色和模板选择算法，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 病理图像颜色外观受多种因素影响，会使计算机辅助诊断系统性能下降，需消除颜色差异。

Method: 将图像解耦为结构特征和矢量量化颜色特征，用目标颜色特征重染结构特征，训练固定颜色矢量码本，利用交叉注意力机制重染，设计模板图像选择算法。

Result: 实验验证了StainPIDR和解耦图像选择算法的有效性。

Conclusion: 该方法在染色归一化任务中表现良好，代码后续公开。

Abstract: The color appearance of a pathological image is highly related to the imaging
protocols, the proportion of different dyes, and the scanning devices.
Computer-aided diagnostic systems may deteriorate when facing these
color-variant pathological images. In this work, we propose a stain
normalization method called StainPIDR. We try to eliminate this color
discrepancy by decoupling the image into structure features and
vector-quantized color features, restaining the structure features with the
target color features, and decoding the stained structure features to
normalized pathological images. We assume that color features decoupled by
different images with the same color should be exactly the same. Under this
assumption, we train a fixed color vector codebook to which the decoupled color
features will map. In the restaining part, we utilize the cross-attention
mechanism to efficiently stain the structure features. As the target color
(decoupled from a selected template image) will also affect the performance of
stain normalization, we further design a template image selection algorithm to
select a template from a given dataset. In our extensive experiments, we
validate the effectiveness of StainPIDR and the template image selection
algorithm. All the results show that our method can perform well in the stain
normalization task. The code of StainPIDR will be publicly available later.

</details>


### [432] [EgoWorld: Translating Exocentric View to Egocentric View using Rich Exocentric Observations](https://arxiv.org/abs/2506.17896)
*Junho Park,Andrew Sangwoo Ye,Taein Kwon*

Main category: cs.CV

TL;DR: 提出EgoWorld框架，从丰富外中心观察重建自我中心视图，在数据集上达SOTA并具泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有外中心到自我中心转换方法依赖2D线索、同步多视图设置和不现实假设，需更好方法。

Method: 引入两阶段EgoWorld框架，从估计的外中心深度图重建点云，重新投影到自我中心视角，应用基于扩散的修复生成图像。

Result: 在H2O和TACO数据集上达到SOTA，对新对象、动作、场景和主体有鲁棒泛化性，在未标记真实示例上有良好结果。

Conclusion: EgoWorld框架有效克服现有方法局限，能从外中心观察重建高质量自我中心视图。

Abstract: Egocentric vision is essential for both human and machine visual
understanding, particularly in capturing the detailed hand-object interactions
needed for manipulation tasks. Translating third-person views into first-person
views significantly benefits augmented reality (AR), virtual reality (VR) and
robotics applications. However, current exocentric-to-egocentric translation
methods are limited by their dependence on 2D cues, synchronized multi-view
settings, and unrealistic assumptions such as necessity of initial egocentric
frame and relative camera poses during inference. To overcome these challenges,
we introduce EgoWorld, a novel two-stage framework that reconstructs an
egocentric view from rich exocentric observations, including projected point
clouds, 3D hand poses, and textual descriptions. Our approach reconstructs a
point cloud from estimated exocentric depth maps, reprojects it into the
egocentric perspective, and then applies diffusion-based inpainting to produce
dense, semantically coherent egocentric images. Evaluated on the H2O and TACO
datasets, EgoWorld achieves state-of-the-art performance and demonstrates
robust generalization to new objects, actions, scenes, and subjects. Moreover,
EgoWorld shows promising results even on unlabeled real-world examples.

</details>


### [433] [Cause-Effect Driven Optimization for Robust Medical Visual Question Answering with Language Biases](https://arxiv.org/abs/2506.17903)
*Huanjia Zhu,Yishu Liu,Xiaozhao Fang,Guangming Lu,Bingzhi Chen*

Main category: cs.CV

TL;DR: 提出因果驱动优化框架CEDO缓解Med - VQA模型语言偏差，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有Med - VQA模型存在语言偏差，有虚假相关性。

Method: 提出CEDO框架，包含MHO（用自适应学习率实现异构优化）、GMS（用帕累托优化促进模态协同并消除偏差更新）、DLR（为单个损失分配自适应权重）。

Result: 在多个基准测试中，CEDO比现有方法更具鲁棒性。

Conclusion: CEDO框架能从因果角度有效缓解Med - VQA模型的语言偏差。

Abstract: Existing Medical Visual Question Answering (Med-VQA) models often suffer from
language biases, where spurious correlations between question types and answer
categories are inadvertently established. To address these issues, we propose a
novel Cause-Effect Driven Optimization framework called CEDO, that incorporates
three well-established mechanisms, i.e., Modality-driven Heterogeneous
Optimization (MHO), Gradient-guided Modality Synergy (GMS), and
Distribution-adapted Loss Rescaling (DLR), for comprehensively mitigating
language biases from both causal and effectual perspectives. Specifically, MHO
employs adaptive learning rates for specific modalities to achieve
heterogeneous optimization, thus enhancing robust reasoning capabilities.
Additionally, GMS leverages the Pareto optimization method to foster
synergistic interactions between modalities and enforce gradient orthogonality
to eliminate bias updates, thereby mitigating language biases from the effect
side, i.e., shortcut bias. Furthermore, DLR is designed to assign adaptive
weights to individual losses to ensure balanced learning across all answer
categories, effectively alleviating language biases from the cause side, i.e.,
imbalance biases within datasets. Extensive experiments on multiple traditional
and bias-sensitive benchmarks consistently demonstrate the robustness of CEDO
over state-of-the-art competitors.

</details>


### [434] [Feedback Driven Multi Stereo Vision System for Real-Time Event Analysis](https://arxiv.org/abs/2506.17910)
*Mohamed Benkedadra,Matei Mancas,Sidi Ahmed Mahmoudi*

Main category: cs.CV

TL;DR: 提出用于交互式系统的基于3D立体视觉的管道，可处理普通和敏感应用，介绍了方法、初步实验结果并规划后续步骤。


<details>
  <summary>Details</summary>
Motivation: 现有2D和3D相机在大型复杂环境中不可靠，需要一种能在复杂环境工作的解决方案。

Method: 提出基于3D立体视觉的管道，融合多个3D相机进行全场景重建，利用反馈方法接收环境中主体数据。

Result: 文中介绍了初步实验及结果，但未详细说明。

Conclusion: 规划了将该管道投入生产需要采取的下一步路线图。

Abstract: 2D cameras are often used in interactive systems. Other systems like gaming
consoles provide more powerful 3D cameras for short range depth sensing.
Overall, these cameras are not reliable in large, complex environments. In this
work, we propose a 3D stereo vision based pipeline for interactive systems,
that is able to handle both ordinary and sensitive applications, through robust
scene understanding. We explore the fusion of multiple 3D cameras to do full
scene reconstruction, which allows for preforming a wide range of tasks, like
event recognition, subject tracking, and notification. Using possible feedback
approaches, the system can receive data from the subjects present in the
environment, to learn to make better decisions, or to adapt to completely new
environments. Throughout the paper, we introduce the pipeline and explain our
preliminary experimentation and results. Finally, we draw the roadmap for the
next steps that need to be taken, in order to get this pipeline into production

</details>


### [435] [IDAL: Improved Domain Adaptive Learning for Natural Images Dataset](https://arxiv.org/abs/2506.17931)
*Ravi Kant Gupta,Shounak Das,Amit Sethi*

Main category: cs.CV

TL;DR: 提出一种自然图像无监督域适应新方法，结合特定网络架构和损失函数，提升模型性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有对抗域适应方法难以有效对齐多模态分布不同领域，且自然图像存在尺度、噪声和风格偏移等问题。

Method: 使用ResNet和FPN网络架构处理内容和风格特征，结合新颖损失函数和现有损失函数训练网络。

Result: 组合损失函数提升了目标域模型的准确性、鲁棒性和训练收敛速度。

Conclusion: 该方法在多个数据集上比基于CNN的先进方法泛化能力更好。

Abstract: We present a novel approach for unsupervised domain adaptation (UDA) for
natural images. A commonly-used objective for UDA schemes is to enhance domain
alignment in representation space even if there is a domain shift in the input
space. Existing adversarial domain adaptation methods may not effectively align
different domains of multimodal distributions associated with classification
problems. Our approach has two main features. Firstly, its neural architecture
uses the deep structure of ResNet and the effective separation of scales of
feature pyramidal network (FPN) to work with both content and style features.
Secondly, it uses a combination of a novel loss function and judiciously
selected existing loss functions to train the network architecture. This
tailored combination is designed to address challenges inherent to natural
images, such as scale, noise, and style shifts, that occur on top of a
multi-modal (multi-class) distribution. The combined loss function not only
enhances model accuracy and robustness on the target domain but also speeds up
training convergence. Our proposed UDA scheme generalizes better than
state-of-the-art for CNN-based methods on Office-Home, Office-31, and
VisDA-2017 datasets and comaparable for DomainNet dataset.

</details>


### [436] [GEMeX-ThinkVG: Towards Thinking with Visual Grounding in Medical VQA via Reinforcement Learning](https://arxiv.org/abs/2506.17939)
*Bo Liu,Xiangyu Zhao,Along He,Yidi Chen,Huazhu Fu,Xiao-Ming Wu*

Main category: cs.CV

TL;DR: 提出ThinkVG数据集和可验证奖励机制解决医学视觉问答可靠性和可解释性问题，用八分之一数据获可比性能。


<details>
  <summary>Details</summary>
Motivation: 当前医学视觉问答方法存在答案可靠性和可解释性问题，影响临床人员和患者对模型答案的理解和信任。

Method: 提出ThinkVG数据集将答案生成分解为中间推理步骤以提供细粒度可解释性，引入可验证奖励机制指导强化学习后训练。

Result: 仅用八分之一训练数据达到可比性能。

Conclusion: 提出的方法有效且高效，数据集公开可获取。

Abstract: Medical visual question answering aims to support clinical decision-making by
enabling models to answer natural language questions based on medical images.
While recent advances in multi-modal learning have significantly improved
performance, current methods still suffer from limited answer reliability and
poor interpretability, impairing the ability of clinicians and patients to
understand and trust model-generated answers. To address this, this work first
proposes a Thinking with Visual Grounding (ThinkVG) dataset wherein the answer
generation is decomposed into intermediate reasoning steps that explicitly
ground relevant visual regions of the medical image, thereby providing
fine-grained explainability. Furthermore, we introduce a novel verifiable
reward mechanism for reinforcement learning to guide post-training, improving
the alignment between the model's reasoning process and its final answer.
Remarkably, our method achieves comparable performance using only one-eighth of
the training data, demonstrating the efficiency and effectiveness of the
proposal. The dataset is available at
https://huggingface.co/datasets/BoKelvin/GEMeX-ThinkVG.

</details>


### [437] [PP-DocBee2: Improved Baselines with Efficient Data for Multimodal Document Understanding](https://arxiv.org/abs/2506.18023)
*Kui Huang,Xinrong Chen,Wenyu Lv,Jincheng Liao,Guanzhong Wang,Yi Liu*

Main category: cs.CV

TL;DR: 介绍PP - DocBee2，在多模态文档理解上有技术改进，提升性能并降低推理延迟，还提出数据质量优化策略和特征融合策略，代码和预训练模型开源。


<details>
  <summary>Details</summary>
Motivation: 增强多模态文档理解能力，解决PP - DocBee的局限性。

Method: 采用大规模多模态模型架构，改进合成数据质量、视觉特征融合策略和推理方法；用大模型评估数据并以新统计标准过滤异常值；分解ViT层并应用新特征融合策略。

Result: 在中文商业文档内部基准测试中性能提升11.4%，推理延迟较原始版本降低73.0%。

Conclusion: PP - DocBee2的技术改进有效提升了多模态文档理解性能，数据质量优化和特征融合策略可行。

Abstract: This report introduces PP-DocBee2, an advanced version of the PP-DocBee,
designed to enhance multimodal document understanding. Built on a large
multimodal model architecture, PP-DocBee2 addresses the limitations of its
predecessor through key technological improvements, including enhanced
synthetic data quality, improved visual feature fusion strategy, and optimized
inference methodologies. These enhancements yield an $11.4\%$ performance boost
on internal benchmarks for Chinese business documents, and reduce inference
latency by $73.0\%$ to the vanilla version. A key innovation of our work is a
data quality optimization strategy for multimodal document tasks. By employing
a large-scale multimodal pre-trained model to evaluate data, we apply a novel
statistical criterion to filter outliers, ensuring high-quality training data.
Inspired by insights into underutilized intermediate features in multimodal
models, we enhance the ViT representational capacity by decomposing it into
layers and applying a novel feature fusion strategy to improve complex
reasoning. The source code and pre-trained model are available at
\href{https://github.com/PaddlePaddle/PaddleMIX}{https://github.com/PaddlePaddle/PaddleMIX}.

</details>


### [438] [Pre-Trained LLM is a Semantic-Aware and Generalizable Segmentation Booster](https://arxiv.org/abs/2506.18034)
*Fenghe Tang,Wenxin Ma,Zhiyang He,Xiaodong Tao,Zihang Jiang,S. Kevin Zhou*

Main category: cs.CV

TL;DR: 本文发现冻结的预训练大语言模型层可处理医学图像分割任务的视觉标记，提出混合结构LLM4Seg提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型在自然语言处理的进展，探索其在医学图像分割任务中的应用。

Method: 提出简单混合结构LLM4Seg，将预训练、冻结的大语言模型层集成到CNN编解码器分割框架中。

Result: 该设计在多种模态下以极小的可训练参数增加提升了分割性能，且在不同大语言模型上验证有效。

Conclusion: 大语言模型的语义感知能力可迁移到分割任务中，能提升全局理解和局部建模能力。

Abstract: With the advancement of Large Language Model (LLM) for natural language
processing, this paper presents an intriguing finding: a frozen pre-trained LLM
layer can process visual tokens for medical image segmentation tasks.
Specifically, we propose a simple hybrid structure that integrates a
pre-trained, frozen LLM layer within the CNN encoder-decoder segmentation
framework (LLM4Seg). Surprisingly, this design improves segmentation
performance with a minimal increase in trainable parameters across various
modalities, including ultrasound, dermoscopy, polypscopy, and CT scans. Our
in-depth analysis reveals the potential of transferring LLM's semantic
awareness to enhance segmentation tasks, offering both improved global
understanding and better local modeling capabilities. The improvement proves
robust across different LLMs, validated using LLaMA and DeepSeek.

</details>


### [439] [MUPA: Towards Multi-Path Agentic Reasoning for Grounded Video Question Answering](https://arxiv.org/abs/2506.18071)
*Jisheng Dang,Huilin Song,Junbin Xiao,Bimei Wang,Han Peng,Haoxuan Li,Xun Yang,Meng Wang,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 提出MUPA方法解决Grounded VideoQA问题，统一多任务，有三条推理路径和反射代理，参数少效果好。


<details>
  <summary>Details</summary>
Motivation: 现代多模态模型在Grounded VideoQA中依赖语言先验和虚假关联，导致预测结果不佳。

Method: 提出MUPA，有三条不同推理路径和反射代理统一视频定位、问答、答案反思和聚合。

Result: 仅用2B参数就超越7B规模竞品，7B参数时在NExT - GQA和DeVE - QA上创佳绩。

Conclusion: MUPA在可靠视频 - 语言理解方面有效。

Abstract: Grounded Video Question Answering (Grounded VideoQA) requires aligning
textual answers with explicit visual evidence. However, modern multimodal
models often rely on linguistic priors and spurious correlations, resulting in
poorly grounded predictions. In this work, we propose MUPA, a cooperative
MUlti-Path Agentic approach that unifies video grounding, question answering,
answer reflection and aggregation to tackle Grounded VideoQA. MUPA features
three distinct reasoning paths on the interplay of grounding and QA agents in
different chronological orders, along with a dedicated reflection agent to
judge and aggregate the multi-path results to accomplish consistent QA and
grounding. This design markedly improves grounding fidelity without sacrificing
answer accuracy. Despite using only 2B parameters, our method outperforms all
7B-scale competitors. When scaled to 7B parameters, MUPA establishes new
state-of-the-art results, with Acc@GQA of 30.3% and 47.4% on NExT-GQA and
DeVE-QA respectively, demonstrating MUPA' effectiveness towards trustworthy
video-language understanding. Our code is available in
https://github.com/longmalongma/MUPA.

</details>


### [440] [ShareGPT-4o-Image: Aligning Multimodal Models with GPT-4o-Level Image Generation](https://arxiv.org/abs/2506.18095)
*Junying Chen,Zhenyang Cai,Pengcheng Chen,Shunian Chen,Ke Ji,Xidong Wang,Yunjin Yang,Benyou Wang*

Main category: cs.CV

TL;DR: 提出ShareGPT - 4o - Image数据集并开发Janus - 4o模型，提升图像生成能力，推动相关开放研究。


<details>
  <summary>Details</summary>
Motivation: 领先的图像生成系统如GPT - 4o - Image专有且难获取，为实现能力民主化。

Method: 合成ShareGPT - 4o - Image数据集，利用该数据集开发Janus - 4o模型。

Result: Janus - 4o显著提升文本到图像生成能力，新增文本和图像到图像生成能力，少量样本和短时间训练获好效果。

Conclusion: ShareGPT - 4o - Image和Janus - 4o的发布将促进逼真、指令对齐图像生成的开放研究。

Abstract: Recent advances in multimodal generative models have unlocked photorealistic,
instruction-aligned image generation, yet leading systems like GPT-4o-Image
remain proprietary and inaccessible. To democratize these capabilities, we
present ShareGPT-4o-Image, the first dataset comprising 45K text-to-image and
46K text-and-image-to-image data, all synthesized using GPT-4o's image
generation capabilities for distilling its advanced image generation abilities.
Leveraging this dataset, we develop Janus-4o, a multimodal large language model
capable of both text-to-image and text-and-image-to-image generation. Janus-4o
not only significantly improves text-to-image generation over its predecessor,
Janus-Pro, but also newly supports text-and-image-to-image generation. Notably,
it achieves impressive performance in text-and-image-to-image generation from
scratch, using only 91K synthetic samples and 6 hours of training on an 8
A800-GPU machine. We hope the release of ShareGPT-4o-Image and Janus-4o will
foster open research in photorealistic, instruction-aligned image generation.

</details>


### [441] [Cloud-Aware SAR Fusion for Enhanced Optical Sensing in Space Missions](https://arxiv.org/abs/2506.17885)
*Trong-An Bui,Thanh-Thoai Le*

Main category: cs.CV

TL;DR: 本文提出云注意力重建框架生成无云光学图像，实验表明该方法优于现有方法，能产生高保真图像。


<details>
  <summary>Details</summary>
Motivation: 云污染严重影响光学卫星图像可用性，影响环境监测等关键应用。

Method: 提出云注意力重建框架，采用注意力驱动特征融合机制结合SAR和光学数据，引入云感知模型更新策略进行自适应损失加权。

Result: 所提方法优于现有方法，PSNR达31.01 dB，SSIM为0.918，MAE为0.017。

Conclusion: 该框架能有效产生高保真、空间和光谱一致的无云光学图像。

Abstract: Cloud contamination significantly impairs the usability of optical satellite
imagery, affecting critical applications such as environmental monitoring,
disaster response, and land-use analysis. This research presents a
Cloud-Attentive Reconstruction Framework that integrates SAR-optical feature
fusion with deep learning-based image reconstruction to generate cloud-free
optical imagery. The proposed framework employs an attention-driven feature
fusion mechanism to align complementary structural information from Synthetic
Aperture Radar (SAR) with spectral characteristics from optical data.
Furthermore, a cloud-aware model update strategy introduces adaptive loss
weighting to prioritize cloud-occluded regions, enhancing reconstruction
accuracy. Experimental results demonstrate that the proposed method outperforms
existing approaches, achieving a PSNR of 31.01 dB, SSIM of 0.918, and MAE of
0.017. These outcomes highlight the framework's effectiveness in producing
high-fidelity, spatially and spectrally consistent cloud-free optical images.

</details>


### [442] [BeltCrack: the First Sequential-image Industrial Conveyor Belt Crack Detection Dataset and Its Baseline with Triple-domain Feature Learning](https://arxiv.org/abs/2506.17892)
*Jianghong Huang,Luping Ji,Xin Ma,Mao Ye*

Main category: cs.CV

TL;DR: 本文构建了来自真实工厂场景的皮带裂纹检测数据集，并提出特殊基线方法验证其可用性和有效性，结果表明数据集可用且基线方法优于其他类似方法。


<details>
  <summary>Details</summary>
Motivation: 当前缺少真实世界工业皮带裂纹数据集，为推动该领域机器学习发展，构建相关数据集。

Method: 构建顺序图像皮带裂纹检测数据集BeltCrack14ks和BeltCrack9kd，提出基于三域（时间 - 空间 - 频率）特征分层融合学习的基线方法。

Result: 实验证明数据集可用且有效，基线方法明显优于其他类似检测方法。

Conclusion: 构建的数据集和提出的基线方法对工业皮带裂纹检测有积极作用，代码和数据集可在指定链接获取。

Abstract: Conveyor belt is a category of important equipments in modern industry,
widely applied in production and manufacturing Fields. Its health status is
much critical to operation efficiency and safety hazards. Among the factors
affecting belt health, crack is often one of the most threatening risks.
Currently, considering safety, how to intelligently detect belt cracks is
catching an increasing attention. To implement the intelligent detection with
machine learning, real crack samples are believed to be necessary. However,
existing crack datasets primarily focus on pavement scenarios or synthetic
data, no real-world industrial belt crack datasets at all. To propel machine
learning advancement in this field, this paper constructs the first
sequential-image belt crack detection datasets (BeltCrack14ks and
BeltCrack9kd), from real-world factory scenes. Furthermore, to validate
usability and effectiveness, we propose a special baseline method with
triple-domain (i.e., time-space-frequency) feature hierarchical fusion learning
for the two whole-new datasets. Experimental results demonstrate the
availability and effectiveness of our dataset. Besides, they also show that our
baseline is obviously superior to other similar detection methods. Our datasets
and source codes are available at https://github.com/UESTC-nnLab/BeltCrack.

</details>


### [443] [STACT-Time: Spatio-Temporal Cross Attention for Cine Thyroid Ultrasound Time Series Classification](https://arxiv.org/abs/2506.18172)
*Irsyad Adam,Tengyue Zhang,Shrayes Raman,Zhuyu Qiu,Brandon Taraku,Hexiang Feng,Sile Wang,Ashwath Radhachandran,Shreeram Athreya,Vedrana Ivezic,Peipei Ping,Corey Arnold,William Speier*

Main category: cs.CV

TL;DR: 本文提出STACT - Time模型用于甲状腺超声影像分类，整合影像与分割掩码特征，提升恶性预测能力，有望改善临床决策。


<details>
  <summary>Details</summary>
Motivation: 现有甲状腺结节评估方法（如FNA）会导致不必要活检，TI - RADS受观察者差异限制，深度学习方法未充分利用超声影像动态信息，需新方法改善风险分层。

Method: 提出Spatio - Temporal Cross Attention for Cine Thyroid Ultrasound Time Series Classification (STACT - Time)模型，整合超声影像特征与预训练模型自动生成的分割掩码特征，利用自注意力和交叉注意力机制。

Result: 模型相比现有技术提升了恶性预测能力，交叉验证精度达0.91 (±0.02)，F1分数达0.89 (±0.02)。

Conclusion: 模型能减少良性结节不必要活检，保持高恶性检测敏感度，有望增强临床决策，改善患者预后。

Abstract: Thyroid cancer is among the most common cancers in the United States. Thyroid
nodules are frequently detected through ultrasound (US) imaging, and some
require further evaluation via fine-needle aspiration (FNA) biopsy. Despite its
effectiveness, FNA often leads to unnecessary biopsies of benign nodules,
causing patient discomfort and anxiety. To address this, the American College
of Radiology Thyroid Imaging Reporting and Data System (TI-RADS) has been
developed to reduce benign biopsies. However, such systems are limited by
interobserver variability. Recent deep learning approaches have sought to
improve risk stratification, but they often fail to utilize the rich temporal
and spatial context provided by US cine clips, which contain dynamic global
information and surrounding structural changes across various views. In this
work, we propose the Spatio-Temporal Cross Attention for Cine Thyroid
Ultrasound Time Series Classification (STACT-Time) model, a novel
representation learning framework that integrates imaging features from US cine
clips with features from segmentation masks automatically generated by a
pretrained model. By leveraging self-attention and cross-attention mechanisms,
our model captures the rich temporal and spatial context of US cine clips while
enhancing feature representation through segmentation-guided learning. Our
model improves malignancy prediction compared to state-of-the-art models,
achieving a cross-validation precision of 0.91 (plus or minus 0.02) and an F1
score of 0.89 (plus or minus 0.02). By reducing unnecessary biopsies of benign
nodules while maintaining high sensitivity for malignancy detection, our model
has the potential to enhance clinical decision-making and improve patient
outcomes.

</details>


### [444] [Fast Neural Inverse Kinematics on Human Body Motions](https://arxiv.org/abs/2506.17996)
*David Tolpin,Sefy Kagarlitsky*

Main category: cs.CV

TL;DR: 提出用于从3D关键点实时捕捉人体运动的快速可靠神经逆运动学框架并评估。


<details>
  <summary>Details</summary>
Motivation: 无标记动作捕捉虽有优势，但计算需求高、推理慢，限制实时场景应用。

Method: 设计快速可靠的神经逆运动学框架，详细介绍网络架构、训练方法和推理过程，用消融研究支持关键设计决策。

Result: 对框架进行定性和定量评估。

Conclusion: 未提及明确结论。

Abstract: Markerless motion capture enables the tracking of human motion without
requiring physical markers or suits, offering increased flexibility and reduced
costs compared to traditional systems. However, these advantages often come at
the expense of higher computational demands and slower inference, limiting
their applicability in real-time scenarios. In this technical report, we
present a fast and reliable neural inverse kinematics framework designed for
real-time capture of human body motions from 3D keypoints. We describe the
network architecture, training methodology, and inference procedure in detail.
Our framework is evaluated both qualitatively and quantitatively, and we
support key design decisions through ablation studies.

</details>


### [445] [Enhancing VICReg: Random-Walk Pairing for Improved Generalization and Better Global Semantics Capturing](https://arxiv.org/abs/2506.18104)
*Idan Simai,Ronen Talmon,Uri Shaham*

Main category: cs.CV

TL;DR: 文章指出VICReg可能因过度依赖训练数据而泛化不佳，提出SAG - VICReg改进该问题，实验证明其有效性，还提出新评估指标。


<details>
  <summary>Details</summary>
Motivation: 发现VICReg可能存在过度依赖训练数据、泛化能力不足的问题，需提升其在训练集外图像上生成有意义表征的能力。

Method: 在VICReg基础上引入新训练技术构建SAG - VICReg，并提出一种不依赖标签、考虑全局数据结构的新评估指标。

Result: SAG - VICReg有效解决泛化挑战，在多种最先进的自监督学习基线中表现匹配或超越，在评估全局语义理解的指标上表现出色，在局部评估指标上也有竞争力。

Conclusion: SAG - VICReg能提升模型捕捉全局语义和泛化能力，新评估指标可在标签数据稀缺时发挥作用。

Abstract: In this paper, we argue that viewing VICReg-a popular self-supervised
learning (SSL) method--through the lens of spectral embedding reveals a
potential source of sub-optimality: it may struggle to generalize robustly to
unseen data due to overreliance on the training data. This observation invites
a closer look at how well this method achieves its goal of producing meaningful
representations of images outside of the training set as well. Here, we
investigate this issue and introduce SAG-VICReg (Stable and Generalizable
VICReg), a method that builds on VICReg by incorporating new training
techniques. These enhancements improve the model's ability to capture global
semantics within the data and strengthen the generalization capabilities.
Experiments demonstrate that SAG-VICReg effectively addresses the
generalization challenge while matching or surpassing diverse state-of-the-art
SSL baselines. Notably, our method exhibits superior performance on metrics
designed to evaluate global semantic understanding, while simultaneously
maintaining competitive results on local evaluation metrics. Furthermore, we
propose a new standalone evaluation metric for embeddings that complements the
standard evaluation methods and accounts for the global data structure without
requiring labels--a key issue when tagged data is scarce or not available.

</details>


### [446] [Multimodal Fusion SLAM with Fourier Attention](https://arxiv.org/abs/2506.18204)
*Youjie Zhou,Guofeng Mei,Yiming Wang,Yi Wan,Fabio Poiesi*

Main category: cs.CV

TL;DR: 提出高效多模态融合SLAM方法FMF - SLAM，用FFT提升效率，在多种场景表现优异，代码和数据集公开。


<details>
  <summary>Details</summary>
Motivation: 视觉SLAM在有噪声、光照变化和黑暗环境中具挑战性，传统光流视觉SLAM耗资源，需更高效方法。

Method: 提出基于FFT的FMF - SLAM，引入基于傅里叶的自注意力和交叉注意力机制提取特征，结合多尺度知识蒸馏增强多模态特征交互，与安全机器人集成并融合GNSS - RTK和全局束调整。

Result: 用TUM、TartanAir和真实数据集视频序列验证，在噪声、光照变化和黑暗条件下有先进性能。

Conclusion: FMF - SLAM在有噪声、光照变化和黑暗环境中高效且实用，有实时性能。

Abstract: Visual SLAM is particularly challenging in environments affected by noise,
varying lighting conditions, and darkness. Learning-based optical flow
algorithms can leverage multiple modalities to address these challenges, but
traditional optical flow-based visual SLAM approaches often require significant
computational resources.To overcome this limitation, we propose FMF-SLAM, an
efficient multimodal fusion SLAM method that utilizes fast Fourier transform
(FFT) to enhance the algorithm efficiency. Specifically, we introduce a novel
Fourier-based self-attention and cross-attention mechanism to extract features
from RGB and depth signals. We further enhance the interaction of multimodal
features by incorporating multi-scale knowledge distillation across modalities.
We also demonstrate the practical feasibility of FMF-SLAM in real-world
scenarios with real time performance by integrating it with a security robot by
fusing with a global positioning module GNSS-RTK and global Bundle Adjustment.
Our approach is validated using video sequences from TUM, TartanAir, and our
real-world datasets, showcasing state-of-the-art performance under noisy,
varying lighting, and dark conditions.Our code and datasets are available at
https://github.com/youjie-zhou/FMF-SLAM.git.

</details>


### [447] [Deep Learning-based Alignment Measurement in Knee Radiographs](https://arxiv.org/abs/2506.18209)
*Zhisen Hu,Dominic Cullen,Peter Thompson,David Johnson,Chang Bian,Aleksei Tiulpin,Timothy Cootes,Claudia Lindner*

Main category: cs.CV

TL;DR: 提出基于深度学习的方法测量膝关节正位片的KA，准确可靠，为临床工作流数字化提供机会。


<details>
  <summary>Details</summary>
Motivation: 传统KA测量方法手动、耗时且需长腿X光片，需要更高效准确的方法。

Method: 基于沙漏网络，结合注意力门结构，自动定位超100个膝关节解剖标志点。

Result: 使用解剖性胫股角测量解剖性内/外翻KA高度准确可靠，与临床真值平均绝对差约1°，术前和术后一致性良好。

Conclusion: KA评估可高度准确自动化，为数字化临床工作流创造机会。

Abstract: Radiographic knee alignment (KA) measurement is important for predicting
joint health and surgical outcomes after total knee replacement. Traditional
methods for KA measurements are manual, time-consuming and require long-leg
radiographs. This study proposes a deep learning-based method to measure KA in
anteroposterior knee radiographs via automatically localized knee anatomical
landmarks. Our method builds on hourglass networks and incorporates an
attention gate structure to enhance robustness and focus on key anatomical
features. To our knowledge, this is the first deep learning-based method to
localize over 100 knee anatomical landmarks to fully outline the knee shape
while integrating KA measurements on both pre-operative and post-operative
images. It provides highly accurate and reliable anatomical varus/valgus KA
measurements using the anatomical tibiofemoral angle, achieving mean absolute
differences ~1{\deg} when compared to clinical ground truth measurements.
Agreement between automated and clinical measurements was excellent
pre-operatively (intra-class correlation coefficient (ICC) = 0.97) and good
post-operatively (ICC = 0.86). Our findings demonstrate that KA assessment can
be automated with high accuracy, creating opportunities for digitally enhanced
clinical workflows.

</details>


### [448] [Cross-Architecture Knowledge Distillation (KD) for Retinal Fundus Image Anomaly Detection on NVIDIA Jetson Nano](https://arxiv.org/abs/2506.18220)
*Berk Yilmaz,Aniruddh Aiyengar*

Main category: cs.CV

TL;DR: 提出用跨架构知识蒸馏开发轻量级边缘设备可部署的视网膜疾病分类器，训练ViT教师模型并压缩为CNN学生模型，保留教师模型诊断性能，为资源匮乏地区提供可扩展AI分诊方案。


<details>
  <summary>Details</summary>
Motivation: 早期准确识别视网膜疾病很重要，但资源匮乏地区缺乏可靠诊断设备，需开发轻量级可部署分类器。

Method: 先训练基于I - JEPA自监督学习预训练的高容量ViT教师模型对眼底图像分类，再使用包含PCA投影仪、GL投影仪和多视图鲁棒训练方法的框架将其压缩为CNN学生模型以用于资源受限环境。

Result: 教师模型比学生模型多97.4%的参数，学生模型实现89%的分类准确率，保留约93%教师模型的诊断性能。

Conclusion: 该方法能在压缩ViT模型的同时保留准确性，为资源匮乏地区视网膜疾病提供了可扩展的AI分诊解决方案。

Abstract: Early and accurate identification of retinal ailments is crucial for averting
ocular decline; however, access to dependable diagnostic devices is not often
available in low-resourced settings. This project proposes to solve that by
developing a lightweight, edge-device deployable disease classifier using
cross-architecture knowledge distilling. We first train a high-capacity vision
transformer (ViT) teacher model, pre-trained using I-JEPA self-supervised
learning, to classify fundus images into four classes: Normal, Diabetic
Retinopathy, Glaucoma, and Cataract. We kept an Internet of Things (IoT) focus
when compressing to a CNN-based student model for deployment in
resource-limited conditions, such as the NVIDIA Jetson Nano. This was
accomplished using a novel framework which included a Partitioned
Cross-Attention (PCA) projector, a Group-Wise Linear (GL) projector, and a
multi-view robust training method. The teacher model has 97.4 percent more
parameters than the student model, with it achieving 89 percent classification
with a roughly 93 percent retention of the teacher model's diagnostic
performance. The retention of clinical classification behavior supports our
method's initial aim: compression of the ViT while retaining accuracy. Our work
serves as an example of a scalable, AI-driven triage solution for retinal
disorders in under-resourced areas.

</details>


### [449] [Make It Efficient: Dynamic Sparse Attention for Autoregressive Image Generation](https://arxiv.org/abs/2506.18226)
*Xunzhi Xiang,Qi Fan*

Main category: cs.CV

TL;DR: 本文针对自回归条件图像生成模型推理时上下文过长导致的问题，提出无训练上下文优化方法ADSA及动态KV - cache更新机制，实验证明其有效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 自回归条件图像生成模型推理时过长上下文会导致KV - cache带来的显著内存开销和计算延迟，需要解决该问题。

Method: 系统分析推理中全局语义、空间布局和细粒度纹理的形成过程，提出无训练上下文优化方法Adaptive Dynamic Sparse Attention (ADSA)，并引入针对ADSA的动态KV - cache更新机制。

Result: 动态KV - cache更新机制使推理时GPU内存消耗降低约50%。

Conclusion: 所提方法在生成质量和资源效率方面有效且优越。

Abstract: Autoregressive conditional image generation models have emerged as a dominant
paradigm in text-to-image synthesis. These methods typically convert images
into one-dimensional token sequences and leverage the self-attention mechanism,
which has achieved remarkable success in natural language processing, to
capture long-range dependencies, model global context, and ensure semantic
coherence. However, excessively long contexts during inference lead to
significant memory overhead caused by KV-cache and computational delays. To
alleviate these challenges, we systematically analyze how global semantics,
spatial layouts, and fine-grained textures are formed during inference, and
propose a novel training-free context optimization method called Adaptive
Dynamic Sparse Attention (ADSA). Conceptually, ADSA dynamically identifies
historical tokens crucial for maintaining local texture consistency and those
essential for ensuring global semantic coherence, thereby efficiently
streamlining attention computation. Additionally, we introduce a dynamic
KV-cache update mechanism tailored for ADSA, reducing GPU memory consumption
during inference by approximately $50\%$. Extensive qualitative and
quantitative experiments demonstrate the effectiveness and superiority of our
approach in terms of both generation quality and resource efficiency.

</details>


### [450] [Semantic Structure-Aware Generative Attacks for Enhanced Adversarial Transferability](https://arxiv.org/abs/2506.18248)
*Jongoh Jeong,Hunmin Yang,Jaeseok Jeong,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: 提出基于Mean Teacher的语义结构感知攻击框架，实验证明比现有生成式攻击有改进。


<details>
  <summary>Details</summary>
Motivation: 现有生成式对抗攻击未充分利用生成模型的表征能力来保留和利用语义信息，限制了扰动与目标显著区域的对齐。

Method: 引入基于Mean Teacher的语义结构感知攻击框架，通过特征蒸馏引导学生模型早期层激活与教师模型的语义一致性，基于经验发现将扰动合成锚定在生成器语义显著的早期中间块。

Result: 在不同模型、领域和任务上进行大量实验，使用传统指标和新提出的ACR指标评估，相对于现有生成式攻击有一致改进。

Conclusion: 所提出的方法能引导在关键区域进行渐进式对抗扰动，显著提高对抗迁移能力。

Abstract: Generative adversarial attacks train a perturbation generator on a white-box
surrogate model and subsequently apply the crafted perturbations to unseen
black-box victim models. In contrast to iterative attacks, these methods
deliver superior inference-time efficiency, scalability, and transferability;
however, up until now, existing studies have not fully exploited the
representational capacity of generative models to preserve and harness semantic
information. Specifically, the intermediate activations of the generator encode
rich semantic features--object boundaries and coarse shapes--that remain
under-exploited, thereby limiting the alignment of perturbations with
object-salient regions which are critical for adversarial transferability. To
remedy this, we introduce a semantic structure-aware attack framework based on
the Mean Teacher, which serves as a temporally smoothed feature reference. With
this smoothed reference, we further direct semantic consistency between the
early-layer activations in the student and those of the semantically rich
teacher by feature distillation. By anchoring perturbation synthesis to the
semantically salient early intermediate blocks within the generator based on
empirical findings, our method guides progressive adversarial perturbation on
regions that substantially enhance adversarial transferability. We conduct
extensive experiments over diverse models, domains and tasks to demonstrate
consistent improvements relative to state-of-the-art generative attacks,
comprehensively evaluated using conventional metrics and our newly proposed
Accidental Correction Rate (ACR).

</details>


### [451] [Escaping the SpuriVerse: Can Large Vision-Language Models Generalize Beyond Seen Spurious Correlations?](https://arxiv.org/abs/2506.18322)
*Yiwei Yang,Chung Peng Lee,Shangbin Feng,Dora Zhao,Bingbing Wen,Anthony Z. Liu,Yulia Tsvetkov,Bill Howe*

Main category: cs.CV

TL;DR: 论文开发SpuriVerse基准测试多模态大视觉语言模型的虚假关联问题，评估发现模型表现不佳，微调可提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究虚假关联的基准有局限，需研究预训练多模态大视觉语言模型在无显式任务监督下的虚假关联。

Method: 通过收集GPT - 4o在真实世界视觉问答基准上的错误，经LVLM - 人类标注和合成反事实评估，创建SpuriVerse基准，并评估15个开源和闭源LVLMs。

Result: 即使最先进的闭源模型在SpuriVerse上表现也不佳，最佳准确率仅37.1%，微调合成示例后准确率提升至78.40%。

Conclusion: 对多样化虚假模式的训练能使模型泛化到未见情况，学会避免‘捷径’并关注整体图像上下文。

Abstract: Finetuning can cause spurious correlations to arise between non-essential
features and the target labels, but benchmarks to study these effects involve
contrived settings and narrow tasks. In contrast, we consider spurious
correlations in multi-modal Large Vision Language Models (LVLMs) pretrained on
extensive and diverse datasets without explicit task supervision. We develop a
benchmark by sourcing GPT-4o errors on real-world visual-question-answering
(VQA) benchmarks, then curating a subset through LVLM-human annotation and
synthetic counterfactual evaluation to identify errors caused by spurious
correlations. This process yields SpuriVerse, a novel benchmark comprised of
124 distinct types of spurious correlations extracted from real-world datasets,
each containing 1 realistic and 10 synthetic VQA samples for a total of 1364
multiple choice questions. We evaluate 15 open and closed-source LVLMs on
SpuriVerse, finding that even state-of-the-art closed-source models struggle
significantly, achieving at best only 37.1% accuracy. Fine-tuning on synthetic
examples that emphasize the spurious correlation improves performance to
78.40%, suggesting that training on diverse spurious patterns generalizes to
unseen situations: models appear to learn to avoid "shortcuts" and attend to
the overall image context.

</details>


### [452] [Open Set Recognition for Endoscopic Image Classification: A Deep Learning Approach on the Kvasir Dataset](https://arxiv.org/abs/2506.18284)
*Kasra Moazzami,Seoyoun Son,John Lin,Sun Min Lee,Daniel Son,Hayeon Lee,Jeongho Lee,Seongji Lee*

Main category: cs.CV

TL;DR: 本文探讨在Kvasir数据集上应用开放集识别（OSR）技术进行内窥镜图像分类，评估比较多种深度学习架构的OSR能力，为医学图像分析提供基准。


<details>
  <summary>Details</summary>
Motivation: 传统闭集分类框架在开放世界临床环境中存在局限性，模型可靠性受未知情况影响，需要应用OSR技术。

Method: 在Kvasir数据集上评估比较ResNet - 50、Swin Transformer和混合ResNet - Transformer模型在闭集和开集条件下的OSR能力，采用OpenMax作为基线OSR方法。

Result: 结果为临床现实环境中的模型行为提供了实际见解。

Conclusion: 强调了OSR技术对于内窥镜中人工智能系统安全部署的重要性。

Abstract: Endoscopic image classification plays a pivotal role in medical diagnostics
by identifying anatomical landmarks and pathological findings. However,
conventional closed-set classification frameworks are inherently limited in
open-world clinical settings, where previously unseen conditions can arise
andcompromise model reliability. To address this, we explore the application of
Open Set Recognition (OSR) techniques on the Kvasir dataset, a publicly
available and diverse endoscopic image collection. In this study, we evaluate
and compare the OSR capabilities of several representative deep learning
architectures, including ResNet-50, Swin Transformer, and a hybrid
ResNet-Transformer model, under both closed-set and open-set conditions.
OpenMax is adopted as a baseline OSR method to assess the ability of these
models to distinguish known classes from previously unseen categories. This
work represents one of the first efforts to apply open set recognition to the
Kvasir dataset and provides a foundational benchmark for evaluating OSR
performance in medical image analysis. Our results offer practical insights
into model behavior in clinically realistic settings and highlight the
importance of OSR techniques for the safe deployment of AI systems in
endoscopy.

</details>


### [453] [Selective Social-Interaction via Individual Importance for Fast Human Trajectory Prediction](https://arxiv.org/abs/2506.18291)
*Yota Urano,Hiromu Taketsugu,Norimichi Ukita*

Main category: cs.CV

TL;DR: 提出一种选择重要邻人的架构预测主要人物轨迹，用重要性估计器和Gumbel Softmax，在JRDB数据集实验有效果


<details>
  <summary>Details</summary>
Motivation: 有效选择邻人以预测主要人物轨迹

Method: 提出重要性估计器模块输出邻人重要性，用Gumbel Softmax训练防止梯度被非可微操作阻塞

Result: 在JRDB数据集实验表明方法加快过程且有有竞争力的预测精度

Conclusion: 该架构能有效选择邻人预测主要人物轨迹，提升预测效率和准确性

Abstract: This paper presents an architecture for selecting important neighboring
people to predict the primary person's trajectory. To achieve effective
neighboring people selection, we propose a people selection module called the
Importance Estimator which outputs the importance of each neighboring person
for predicting the primary person's future trajectory. To prevent gradients
from being blocked by non-differentiable operations when sampling surrounding
people based on their importance, we employ the Gumbel Softmax for training.
Experiments conducted on the JRDB dataset show that our method speeds up the
process with competitive prediction accuracy.

</details>


### [454] [A Set-to-Set Distance Measure in Hyperbolic Space](https://arxiv.org/abs/2506.18529)
*Pengxiang Li,Wei Wu,Zhi Gao,Xiaomeng Fan,Peilin Yu,Yuwei Wu,Zhipeng Lu,Yunde Jia,Mehrtash Harandi*

Main category: cs.CV

TL;DR: 提出双曲集到集距离度量（HS2SD），整合全局和局部结构信息，在多任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实应用需比较双曲数据点集，现有方法不能有效整合集的全局和局部结构信息。

Method: 通过双曲集的爱因斯坦中点间测地距离整合全局结构信息，利用有限图厄 - 摩尔斯序列近似集的拓扑结构以捕捉局部信息。

Result: 在实体匹配、标准图像分类和少样本图像分类任务中，该距离度量优于现有方法。

Conclusion: HS2SD能有效建模双曲集的层次和复杂关系，对双曲集关系有更细致理解。

Abstract: We propose a hyperbolic set-to-set distance measure for computing
dissimilarity between sets in hyperbolic space. While point-to-point distances
in hyperbolic space effectively capture hierarchical relationships between data
points, many real-world applications require comparing sets of hyperbolic data
points, where the local structure and the global structure of the sets carry
crucial semantic information. The proposed the \underline{h}yperbolic
\underline{s}et-\underline{to}-\underline{s}et \underline{d}istance measure
(HS2SD) integrates both global and local structural information: global
structure through geodesic distances between Einstein midpoints of hyperbolic
sets, and local structure through topological characteristics of the two sets.
To efficiently compute topological differences, we prove that using a finite
Thue-Morse sequence of degree and adjacency matrices can serve as a robust
approximation to capture the topological structure of a set. In this case, by
considering the topological differences, HS2SD provides a more nuanced
understanding of the relationships between two hyperbolic sets. Empirical
evaluation on entity matching, standard image classification, and few-shot
image classification demonstrates that our distance measure outperforms
existing methods by effectively modeling the hierarchical and complex
relationships inherent in hyperbolic sets.

</details>


### [455] [A Multi-Scale Spatial Attention-Based Zero-Shot Learning Framework for Low-Light Image Enhancement](https://arxiv.org/abs/2506.18323)
*Muhammad Azeem Aslam,Hassan Khalid,Nisar Ahmed*

Main category: cs.CV

TL;DR: 提出LucentVisionNet零样本学习框架用于低光图像增强，在多数据集实验中表现优于现有方法，适合实际应用。


<details>
  <summary>Details</summary>
Motivation: 解决低光图像增强在无配对训练数据时的难题，克服传统和深度学习增强方法的局限。

Method: 将多尺度空间注意力与深度曲线估计网络结合，采用循环增强策略，用包含六个定制组件的复合损失函数优化模型。

Result: 在配对和非配对基准数据集实验中，LucentVisionNet在多个图像质量指标上始终优于现有方法。

Conclusion: 该框架视觉质量高、结构一致且计算高效，适合移动摄影、监控和自主导航等实际应用。

Abstract: Low-light image enhancement remains a challenging task, particularly in the
absence of paired training data. In this study, we present LucentVisionNet, a
novel zero-shot learning framework that addresses the limitations of
traditional and deep learning-based enhancement methods. The proposed approach
integrates multi-scale spatial attention with a deep curve estimation network,
enabling fine-grained enhancement while preserving semantic and perceptual
fidelity. To further improve generalization, we adopt a recurrent enhancement
strategy and optimize the model using a composite loss function comprising six
tailored components, including a novel no-reference image quality loss inspired
by human visual perception. Extensive experiments on both paired and unpaired
benchmark datasets demonstrate that LucentVisionNet consistently outperforms
state-of-the-art supervised, unsupervised, and zero-shot methods across
multiple full-reference and no-reference image quality metrics. Our framework
achieves high visual quality, structural consistency, and computational
efficiency, making it well-suited for deployment in real-world applications
such as mobile photography, surveillance, and autonomous navigation.

</details>


### [456] [SpaNN: Detecting Multiple Adversarial Patches on CNNs by Spanning Saliency Thresholds](https://arxiv.org/abs/2506.18591)
*Mauricio Byrd Victorica,György Dán,Henrik Sandberg*

Main category: cs.CV

TL;DR: 提出攻击检测器SpaNN，其计算复杂度与对抗补丁数量无关，在多个数据集上表现优于现有防御方法。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法多针对单补丁攻击，对多补丁攻击存在计算不可行或效率低等问题，需解决对补丁数量的敏感性问题。

Method: 对受害者模型第一层卷积层的神经激活应用一组显著性阈值构建二值化特征图集合，对集合进行聚类，将聚类特征作为分类器输入进行攻击检测。

Result: 在四个常用的目标检测和分类数据集上评估，SpaNN在目标检测和图像分类方面分别比现有防御方法高出11和27个百分点。

Conclusion: SpaNN不依赖固定显著性阈值识别对抗区域，对白色盒对抗攻击具有鲁棒性，且性能优于现有防御方法。

Abstract: State-of-the-art convolutional neural network models for object detection and
image classification are vulnerable to physically realizable adversarial
perturbations, such as patch attacks. Existing defenses have focused,
implicitly or explicitly, on single-patch attacks, leaving their sensitivity to
the number of patches as an open question or rendering them computationally
infeasible or inefficient against attacks consisting of multiple patches in the
worst cases. In this work, we propose SpaNN, an attack detector whose
computational complexity is independent of the expected number of adversarial
patches. The key novelty of the proposed detector is that it builds an ensemble
of binarized feature maps by applying a set of saliency thresholds to the
neural activations of the first convolutional layer of the victim model. It
then performs clustering on the ensemble and uses the cluster features as the
input to a classifier for attack detection. Contrary to existing detectors,
SpaNN does not rely on a fixed saliency threshold for identifying adversarial
regions, which makes it robust against white box adversarial attacks. We
evaluate SpaNN on four widely used data sets for object detection and
classification, and our results show that SpaNN outperforms state-of-the-art
defenses by up to 11 and 27 percentage points in the case of object detection
and the case of image classification, respectively. Our code is available at
https://github.com/gerkbyrd/SpaNN.

</details>


### [457] [Including Semantic Information via Word Embeddings for Skeleton-based Action Recognition](https://arxiv.org/abs/2506.18721)
*Dustin Aganian,Erik Franze,Markus Eisenbach,Horst-Michael Gross*

Main category: cs.CV

TL;DR: 提出基于骨架的动作识别新方法，用词嵌入编码语义信息，实验显示提升分类性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统基于骨架的方法在复杂交互中因丢失关键点语义而效果受限，需有效方法用于工业4.0协作机器人辅助装配任务。

Method: 引入基于骨架的动作识别新方法，利用词嵌入编码语义信息，用语义体积替代独热编码。

Result: 在多个装配数据集上实验表明，该方法显著提升分类性能，同时支持不同骨架类型和对象类，增强泛化能力。

Conclusion: 在动态多样环境中，融入语义信息可提升基于骨架的动作识别能力。

Abstract: Effective human action recognition is widely used for cobots in Industry 4.0
to assist in assembly tasks. However, conventional skeleton-based methods often
lose keypoint semantics, limiting their effectiveness in complex interactions.
In this work, we introduce a novel approach to skeleton-based action
recognition that enriches input representations by leveraging word embeddings
to encode semantic information. Our method replaces one-hot encodings with
semantic volumes, enabling the model to capture meaningful relationships
between joints and objects. Through extensive experiments on multiple assembly
datasets, we demonstrate that our approach significantly improves
classification performance, and enhances generalization capabilities by
simultaneously supporting different skeleton types and object classes. Our
findings highlight the potential of incorporating semantic information to
enhance skeleton-based action recognition in dynamic and diverse environments.

</details>


### [458] [Latent Space Analysis for Melanoma Prevention](https://arxiv.org/abs/2506.18414)
*Ciro Listone,Aniello Murano*

Main category: cs.CV

TL;DR: 提出用条件变分自编码器进行可解释风险建模的新方法，结合SVM分类，能实现黑色素瘤早期检测并增强临床适用性。


<details>
  <summary>Details</summary>
Motivation: 黑色素瘤进展快、死亡率高，需早期可解释诊断工具，现有深度学习模型临床洞察有限。

Method: 采用条件变分自编码器学习结构化潜在空间，结合SVM在该表示上进行分类。

Result: 在区分良性痣和黑色素瘤上表现出色，潜在空间能对恶性程度进行视觉和几何解释。

Conclusion: 该方法将预测性能与临床适用性结合，促进早期检测，增强对AI辅助诊断的信任。

Abstract: Melanoma represents a critical health risk due to its aggressive progression
and high mortality, underscoring the need for early, interpretable diagnostic
tools. While deep learning has advanced in skin lesion classification, most
existing models provide only binary outputs, offering limited clinical insight.
This work introduces a novel approach that extends beyond classification,
enabling interpretable risk modelling through a Conditional Variational
Autoencoder. The proposed method learns a structured latent space that captures
semantic relationships among lesions, allowing for a nuanced, continuous
assessment of morphological differences. An SVM is also trained on this
representation effectively differentiating between benign nevi and melanomas,
demonstrating strong and consistent performance. More importantly, the learned
latent space supports visual and geometric interpretation of malignancy, with
the spatial proximity of a lesion to known melanomas serving as a meaningful
indicator of risk. This approach bridges predictive performance with clinical
applicability, fostering early detection, highlighting ambiguous cases, and
enhancing trust in AI-assisted diagnosis through transparent and interpretable
decision-making.

</details>


### [459] [Focus Your Attention: Towards Data-Intuitive Lightweight Vision Transformers](https://arxiv.org/abs/2506.18791)
*Suyash Gaurav,Muhammad Farhan Humayun,Jukka Heikkonen,Jatin Chaudhary*

Main category: cs.CV

TL;DR: 提出 SPPP 技术和 LLA 模块改进 Vision Transformers，提升计算效率，适用于边缘部署。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers 存在依赖大量计算和内存资源、任务特定迁移学习困难、能源效率低等问题。

Method: 提出 SPPP 技术生成上下文感知、语义丰富的补丁嵌入，引入 LLA 模块降低注意力模块复杂度，利用数据直观的补丁嵌入和动态位置编码调整交叉注意力过程。

Result: 所提架构在计算效率上有显著提升，与现有先进方法结果相当。

Conclusion: 所提架构有潜力用于节能型变压器，适合边缘部署。

Abstract: The evolution of Vision Transformers has led to their widespread adaptation
to different domains. Despite large-scale success, there remain significant
challenges including their reliance on extensive computational and memory
resources for pre-training on huge datasets as well as difficulties in
task-specific transfer learning. These limitations coupled with energy
inefficiencies mainly arise due to the computation-intensive self-attention
mechanism. To address these issues, we propose a novel Super-Pixel Based Patch
Pooling (SPPP) technique that generates context-aware, semantically rich, patch
embeddings to effectively reduce the architectural complexity and improve
efficiency. Additionally, we introduce the Light Latent Attention (LLA) module
in our pipeline by integrating latent tokens into the attention mechanism
allowing cross-attention operations to significantly reduce the time and space
complexity of the attention module. By leveraging the data-intuitive patch
embeddings coupled with dynamic positional encodings, our approach adaptively
modulates the cross-attention process to focus on informative regions while
maintaining the global semantic structure. This targeted attention improves
training efficiency and accelerates convergence. Notably, the SPPP module is
lightweight and can be easily integrated into existing transformer
architectures. Extensive experiments demonstrate that our proposed architecture
provides significant improvements in terms of computational efficiency while
achieving comparable results with the state-of-the-art approaches, highlighting
its potential for energy-efficient transformers suitable for edge deployment.
(The code is available on our GitHub repository:
https://github.com/zser092/Focused-Attention-ViT).

</details>


### [460] [Benchmarking Foundation Models and Parameter-Efficient Fine-Tuning for Prognosis Prediction in Medical Imaging](https://arxiv.org/abs/2506.18434)
*Filippo Ruffini,Elena Mulero Ayllon,Linlin Shen,Paolo Soda,Valerio Guarrasi*

Main category: cs.CV

TL;DR: 本文引入结构化基准评估卷积神经网络和基础模型在COVID - 19患者临床预后预测中的可迁移性，通过多种微调策略和学习范式进行大规模对比分析，为临床预后预测的AI应用提供参考。


<details>
  <summary>Details</summary>
Motivation: 人工智能在医学影像预后预测中应用有挑战，需要评估模型在该领域的可迁移性，以推动其实际应用。

Method: 利用公开胸部X光数据集，采用多种微调策略（传统及先进的参数高效微调方法），在多种学习范式（全数据场景和少样本学习设置）下，对多种预训练模型进行大规模对比分析。

Result: 未提及具体结果，但进行了广泛评估，获取了各微调策略在不同条件下的优缺点等详细信息。

Conclusion: 本次评估旨在为现实临床预后预测工作流程中稳健、高效且可泛化的AI解决方案的实际部署和应用提供依据。

Abstract: Artificial Intelligence (AI) holds significant promise for improving
prognosis prediction in medical imaging, yet its effective application remains
challenging. In this work, we introduce a structured benchmark explicitly
designed to evaluate and compare the transferability of Convolutional Neural
Networks and Foundation Models in predicting clinical outcomes in COVID-19
patients, leveraging diverse publicly available Chest X-ray datasets. Our
experimental methodology extensively explores a wide set of fine-tuning
strategies, encompassing traditional approaches such as Full Fine-Tuning and
Linear Probing, as well as advanced Parameter-Efficient Fine-Tuning methods
including Low-Rank Adaptation, BitFit, VeRA, and IA3. The evaluations were
conducted across multiple learning paradigms, including both extensive
full-data scenarios and more clinically realistic Few-Shot Learning settings,
which are critical for modeling rare disease outcomes and rapidly emerging
health threats. By implementing a large-scale comparative analysis involving a
diverse selection of pretrained models, including general-purpose architectures
pretrained on large-scale datasets such as CLIP and DINOv2, to
biomedical-specific models like MedCLIP, BioMedCLIP, and PubMedCLIP, we
rigorously assess each model's capacity to effectively adapt and generalize to
prognosis tasks, particularly under conditions of severe data scarcity and
pronounced class imbalance. The benchmark was designed to capture critical
conditions common in prognosis tasks, including variations in dataset size and
class distribution, providing detailed insights into the strengths and
limitations of each fine-tuning strategy. This extensive and structured
evaluation aims to inform the practical deployment and adoption of robust,
efficient, and generalizable AI-driven solutions in real-world clinical
prognosis prediction workflows.

</details>


### [461] [Generalizing Vision-Language Models to Novel Domains: A Comprehensive Survey](https://arxiv.org/abs/2506.18504)
*Xinyao Li,Jingjing Li,Fengling Li,Lei Zhu,Yang Yang,Heng Tao Shen*

Main category: cs.CV

TL;DR: 本文对视觉语言模型（VLM）泛化研究进行全面综述，包括设置、方法、基准和结果，还讨论了与多模态大语言模型的关系。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在特定领域或专业泛化任务中性能下降，需要将其知识迁移到下游应用。

Method: 对现有文献按转移模块分为基于提示、基于参数和基于特征的方法；回顾典型迁移学习设置总结各方法差异；介绍流行基准并比较性能；讨论VLM与MLLM的关系。

Result: 梳理了VLM泛化研究的现状，比较了不同方法在基准上的性能。

Conclusion: 从泛化角度系统综述为当前和未来多模态研究提供清晰图景。

Abstract: Recently, vision-language pretraining has emerged as a transformative
technique that integrates the strengths of both visual and textual modalities,
resulting in powerful vision-language models (VLMs). Leveraging web-scale
pretraining data, these models exhibit strong zero-shot capabilities. However,
their performance often deteriorates when confronted with domain-specific or
specialized generalization tasks. To address this, a growing body of research
focuses on transferring or generalizing the rich knowledge embedded in VLMs to
various downstream applications. This survey aims to comprehensively summarize
the generalization settings, methodologies, benchmarking and results in VLM
literatures. Delving into the typical VLM structures, current literatures are
categorized into prompt-based, parameter-based and feature-based methods
according to the transferred modules. The differences and characteristics in
each category are furthered summarized and discussed by revisiting the typical
transfer learning (TL) settings, providing novel interpretations for TL in the
era of VLMs. Popular benchmarks for VLM generalization are further introduced
with thorough performance comparisons among the reviewed methods. Following the
advances in large-scale generalizable pretraining, this survey also discusses
the relations and differences between VLMs and up-to-date multimodal large
language models (MLLM), e.g., DeepSeek-VL. By systematically reviewing the
surging literatures in vision-language research from a novel and practical
generalization prospective, this survey contributes to a clear landscape of
current and future multimodal researches.

</details>


### [462] [Historical Report Guided Bi-modal Concurrent Learning for Pathology Report Generation](https://arxiv.org/abs/2506.18658)
*Ling Zhang,Boxiang Yun,Qingli Li,Yan Wang*

Main category: cs.CV

TL;DR: 提出BiGen框架解决病理报告自动生成的挑战，实验效果好，代码开源。


<details>
  <summary>Details</summary>
Motivation: 解决自动病理报告生成中视觉特征缺乏语义内容和WSIs信息冗余的问题。

Method: 提出Historical Report Guided Bi - modal Concurrent Learning Framework (BiGen)，含知识检索机制和双模态并发学习策略，多模态解码器生成报告。

Result: 在PathText (BRCA) 数据集上取得SOTA性能，NLP指标相对提升7.4%，Her - 2预测分类指标提升19.1%。

Conclusion: 消融实验验证模块必要性，方法能提供语义内容并抑制信息冗余。

Abstract: Automated pathology report generation from Whole Slide Images (WSIs) faces
two key challenges: (1) lack of semantic content in visual features and (2)
inherent information redundancy in WSIs. To address these issues, we propose a
novel Historical Report Guided \textbf{Bi}-modal Concurrent Learning Framework
for Pathology Report \textbf{Gen}eration (BiGen) emulating pathologists'
diagnostic reasoning, consisting of: (1) A knowledge retrieval mechanism to
provide rich semantic content, which retrieves WSI-relevant knowledge from
pre-built medical knowledge bank by matching high-attention patches and (2) A
bi-modal concurrent learning strategy instantiated via a learnable visual token
and a learnable textual token to dynamically extract key visual features and
retrieved knowledge, where weight-shared layers enable cross-modal alignment
between visual features and knowledge features. Our multi-modal decoder
integrates both modals for comprehensive diagnostic reports generation.
Experiments on the PathText (BRCA) dataset demonstrate our framework's
superiority, achieving state-of-the-art performance with 7.4\% relative
improvement in NLP metrics and 19.1\% enhancement in classification metrics for
Her-2 prediction versus existing methods. Ablation studies validate the
necessity of our proposed modules, highlighting our method's ability to provide
WSI-relevant rich semantic content and suppress information redundancy in WSIs.
Code is publicly available at https://github.com/DeepMed-Lab-ECNU/BiGen.

</details>


### [463] [Benchmarking histopathology foundation models in a multi-center dataset for skin cancer subtyping](https://arxiv.org/abs/2506.18668)
*Pablo Meseguer,Rocío del Amor,Valery Naranjo*

Main category: cs.CV

TL;DR: 提出评估组织病理学基础模型的新基准，利用AI4SkIN数据集和新指标FM - SI，实验表明提取低偏差特征可提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 组织病理学基础模型多样，需设计真实挑战评估其有效性。

Method: 提出评估组织病理学基础模型作为多实例学习（MIL）框架中补丁级特征提取器的新基准，利用AI4SkIN数据集，定义新指标FM - SI。

Result: 实验显示提取低偏差特征可提升分类性能，在基于相似度的MIL分类器中尤其明显。

Conclusion: 新基准和指标有助于评估组织病理学基础模型，提取低偏差特征对分类有益。

Abstract: Pretraining on large-scale, in-domain datasets grants histopathology
foundation models (FM) the ability to learn task-agnostic data representations,
enhancing transfer learning on downstream tasks. In computational pathology,
automated whole slide image analysis requires multiple instance learning (MIL)
frameworks due to the gigapixel scale of the slides. The diversity among
histopathology FMs has highlighted the need to design real-world challenges for
evaluating their effectiveness. To bridge this gap, our work presents a novel
benchmark for evaluating histopathology FMs as patch-level feature extractors
within a MIL classification framework. For that purpose, we leverage the
AI4SkIN dataset, a multi-center cohort encompassing slides with challenging
cutaneous spindle cell neoplasm subtypes. We also define the Foundation Model -
Silhouette Index (FM-SI), a novel metric to measure model consistency against
distribution shifts. Our experimentation shows that extracting less biased
features enhances classification performance, especially in similarity-based
MIL classifiers.

</details>


### [464] [Multi-Scale Spectral Attention Module-based Hyperspectral Segmentation in Autonomous Driving Scenarios](https://arxiv.org/abs/2506.18682)
*Imad Ali Shah,Jiarong Li,Tim Brophy,Martin Glavin,Edward Jones,Enda Ward,Brian Deegan*

Main category: cs.CV

TL;DR: 论文提出MSAM模块并集成到UNet-SC形成UNet-MSAM，在多个HSI数据集上提升语义分割性能，证明多尺度核组合优势。


<details>
  <summary>Details</summary>
Motivation: 高效处理高光谱成像（HSI）的高维光谱数据以用于自动驾驶环境感知是挑战，需要更好的方法。

Method: 引入多尺度光谱注意力模块（MSAM），通过三个并行1D卷积及自适应特征聚合机制增强光谱特征提取，将其集成到UNet的跳跃连接中。

Result: UNet-MSAM在多个HSI数据集上显著提升语义分割性能，以极小计算开销优于UNet-SC，平均mIoU提升3.61%，mF1提升3.80%。

Conclusion: 证明HSI处理用于自动驾驶的潜力，为设计鲁棒多尺度光谱特征提取器提供见解。

Abstract: Recent advances in autonomous driving (AD) have highlighted the potential of
Hyperspectral Imaging (HSI) for enhanced environmental perception, particularly
in challenging weather and lighting conditions. However, efficiently processing
its high-dimensional spectral data remains a significant challenge. This paper
introduces a Multi-scale Spectral Attention Module (MSAM) that enhances
spectral feature extraction through three parallel 1D convolutions with varying
kernel sizes between 1 to 11, coupled with an adaptive feature aggregation
mechanism. By integrating MSAM into UNet's skip connections (UNet-SC), our
proposed UNet-MSAM achieves significant improvements in semantic segmentation
performance across multiple HSI datasets: HyKo-VIS v2, HSI-Drive v2, and
Hyperspectral City v2. Our comprehensive experiments demonstrate that with
minimal computational overhead (on average 0.02% in parameters and 0.82%
GFLOPS), UNet-MSAM consistently outperforms UNet-SC, achieving average
improvements of 3.61% in mean IoU and 3.80% in mF1 across the three datasets.
Through extensive ablation studies, we have established that multi-scale kernel
combinations perform better than single-scale configurations. These findings
demonstrate the potential of HSI processing for AD and provide valuable
insights into designing robust, multi-scale spectral feature extractors for
real-world applications.

</details>


### [465] [SIM-Net: A Multimodal Fusion Network Using Inferred 3D Object Shape Point Clouds from RGB Images for 2D Classification](https://arxiv.org/abs/2506.18683)
*Youcef Sklab,Hanane Ariouat,Eric Chenin,Edi Prifti,Jean-Daniel Zucker*

Main category: cs.CV

TL;DR: 提出Shape - Image Multimodal Network (SIM - Net)用于2D图像分类，融合3D点云，在植物标本数据集上表现优于ResNet101和一些Transformer架构。


<details>
  <summary>Details</summary>
Motivation: 解决传统基于图像模型在数字化植物标本分类中因背景复杂、非植物元素和遮挡等带来的问题。

Method: 采用基于分割的预处理步骤提取对象掩码生成3D点云，使用CNN编码器提取2D图像特征，PointNet编码器提取几何特征并融合到统一潜在空间。

Result: 在植物标本数据集上，SIM - Net比ResNet101准确率最多高9.9%，F分数最多高12.3%，也超过一些基于Transformer的先进架构。

Conclusion: 将3D结构推理融入2D图像分类任务有益。

Abstract: We introduce the Shape-Image Multimodal Network (SIM-Net), a novel 2D image
classification architecture that integrates 3D point cloud representations
inferred directly from RGB images. Our key contribution lies in a
pixel-to-point transformation that converts 2D object masks into 3D point
clouds, enabling the fusion of texture-based and geometric features for
enhanced classification performance. SIM-Net is particularly well-suited for
the classification of digitized herbarium specimens (a task made challenging by
heterogeneous backgrounds), non-plant elements, and occlusions that compromise
conventional image-based models. To address these issues, SIM-Net employs a
segmentation-based preprocessing step to extract object masks prior to 3D point
cloud generation. The architecture comprises a CNN encoder for 2D image
features and a PointNet-based encoder for geometric features, which are fused
into a unified latent space. Experimental evaluations on herbarium datasets
demonstrate that SIM-Net consistently outperforms ResNet101, achieving gains of
up to 9.9% in accuracy and 12.3% in F-score. It also surpasses several
transformer-based state-of-the-art architectures, highlighting the benefits of
incorporating 3D structural reasoning into 2D image classification tasks.

</details>


### [466] [Matrix-Game: Interactive World Foundation Model](https://arxiv.org/abs/2506.18701)
*Yifan Zhang,Chunli Peng,Boyang Wang,Puyi Wang,Qingcheng Zhu,Fei Kang,Biao Jiang,Zedong Gao,Eric Li,Yang Liu,Yahui Zhou*

Main category: cs.CV

TL;DR: 提出Matrix - Game用于可控游戏世界生成，介绍训练方式、数据集、评估指标等，表现优于先前模型并将开源。


<details>
  <summary>Details</summary>
Motivation: 实现可控的游戏世界生成。

Method: 采用两阶段训练流程，先进行大规模无标签预训练，再进行有动作标签的训练；使用Matrix - Game - MC数据集；采用可控的图像到世界生成范式；开发GameWorld Score评估指标。

Result: Matrix - Game在各项指标上均优于先前的开源Minecraft世界模型，双盲人类评估也证实其优越性。

Conclusion: Matrix - Game能生成感知上真实且可精确控制的视频，将开源模型权重和评估指标以推动相关研究。

Abstract: We introduce Matrix-Game, an interactive world foundation model for
controllable game world generation. Matrix-Game is trained using a two-stage
pipeline that first performs large-scale unlabeled pretraining for environment
understanding, followed by action-labeled training for interactive video
generation. To support this, we curate Matrix-Game-MC, a comprehensive
Minecraft dataset comprising over 2,700 hours of unlabeled gameplay video clips
and over 1,000 hours of high-quality labeled clips with fine-grained keyboard
and mouse action annotations. Our model adopts a controllable image-to-world
generation paradigm, conditioned on a reference image, motion context, and user
actions. With over 17 billion parameters, Matrix-Game enables precise control
over character actions and camera movements, while maintaining high visual
quality and temporal coherence. To evaluate performance, we develop GameWorld
Score, a unified benchmark measuring visual quality, temporal quality, action
controllability, and physical rule understanding for Minecraft world
generation. Extensive experiments show that Matrix-Game consistently
outperforms prior open-source Minecraft world models (including Oasis and
MineWorld) across all metrics, with particularly strong gains in
controllability and physical consistency. Double-blind human evaluations
further confirm the superiority of Matrix-Game, highlighting its ability to
generate perceptually realistic and precisely controllable videos across
diverse game scenarios. To facilitate future research on interactive
image-to-world generation, we will open-source the Matrix-Game model weights
and the GameWorld Score benchmark at https://github.com/SkyworkAI/Matrix-Game.

</details>


### [467] [Deep CNN Face Matchers Inherently Support Revocable Biometric Templates](https://arxiv.org/abs/2506.18731)
*Aman Bhatta,Michael C. King,Kevin W. Bowyer*

Main category: cs.CV

TL;DR: 研究表明现代深度CNN人脸匹配器可实现鲁棒的可撤销生物识别方案，还对比了ViT与ResNet骨干网络在该方案中的适用性。


<details>
  <summary>Details</summary>
Motivation: 解决生物特征认证中生物特征被盗用后个体无补救措施的问题，提出可撤销生物特征概念。

Method: 针对给定的先进深度CNN骨干网络和训练集，生成具有等效识别能力和强不兼容生物特征模板的不同人脸匹配器模型；探索基于ViT骨干网络的人脸匹配器在可撤销生物识别系统中的可行性。

Result: 能生成无限数量符合要求的人脸匹配器模型，不同模型实例的生物特征模板强不兼容；ViT骨干网络的人脸匹配器不如典型的基于ResNet的深度CNN骨干网络适合该系统。

Conclusion: 现代深度CNN人脸匹配器可实现鲁棒的可撤销生物识别方案，ViT骨干网络不太适用于此方案。

Abstract: One common critique of biometric authentication is that if an individual's
biometric is compromised, then the individual has no recourse. The concept of
revocable biometrics was developed to address this concern. A biometric scheme
is revocable if an individual can have their current enrollment in the scheme
revoked, so that the compromised biometric template becomes worthless, and the
individual can re-enroll with a new template that has similar recognition
power. We show that modern deep CNN face matchers inherently allow for a robust
revocable biometric scheme. For a given state-of-the-art deep CNN backbone and
training set, it is possible to generate an unlimited number of distinct face
matcher models that have both (1) equivalent recognition power, and (2)
strongly incompatible biometric templates. The equivalent recognition power
extends to the point of generating impostor and genuine distributions that have
the same shape and placement on the similarity dimension, meaning that the
models can share a similarity threshold for a 1-in-10,000 false match rate. The
biometric templates from different model instances are so strongly incompatible
that the cross-instance similarity score for images of the same person is
typically lower than the same-instance similarity score for images of different
persons. That is, a stolen biometric template that is revoked is of less value
in attempting to match the re-enrolled identity than the average impostor
template. We also explore the feasibility of using a Vision Transformer (ViT)
backbone-based face matcher in the revocable biometric system proposed in this
work and demonstrate that it is less suitable compared to typical ResNet-based
deep CNN backbones.

</details>


### [468] [SWA-SOP: Spatially-aware Window Attention for Semantic Occupancy Prediction in Autonomous Driving](https://arxiv.org/abs/2506.18785)
*Helin Cao,Rafael Materla,Sven Behnke*

Main category: cs.CV

TL;DR: 现有基于Transformer的语义占用预测（SOP）方法在注意力计算中缺乏对空间结构的显式建模，本文提出了空间感知窗口注意力（SWA）机制，在基于LiDAR的SOP基准测试中取得了最先进的结果，并在基于相机的SOP管道中也有一致的提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的SOP方法在注意力计算中缺乏对空间结构的显式建模，导致几何感知能力有限，在稀疏或遮挡区域表现不佳。

Method: 提出空间感知窗口注意力（SWA）机制，将局部空间上下文融入注意力。

Result: SWA显著改善了场景补全，在基于LiDAR的SOP基准测试中取得了最先进的结果，在基于相机的SOP管道中也有一致的提升。

Conclusion: SWA机制具有良好的性能和通用性，可有效提升SOP任务的表现。

Abstract: Perception systems in autonomous driving rely on sensors such as LiDAR and
cameras to perceive the 3D environment. However, due to occlusions and data
sparsity, these sensors often fail to capture complete information. Semantic
Occupancy Prediction (SOP) addresses this challenge by inferring both occupancy
and semantics of unobserved regions. Existing transformer-based SOP methods
lack explicit modeling of spatial structure in attention computation, resulting
in limited geometric awareness and poor performance in sparse or occluded
areas. To this end, we propose Spatially-aware Window Attention (SWA), a novel
mechanism that incorporates local spatial context into attention. SWA
significantly improves scene completion and achieves state-of-the-art results
on LiDAR-based SOP benchmarks. We further validate its generality by
integrating SWA into a camera-based SOP pipeline, where it also yields
consistent gains across modalities.

</details>


### [469] [OC-SOP: Enhancing Vision-Based 3D Semantic Occupancy Prediction by Object-Centric Awareness](https://arxiv.org/abs/2506.18798)
*Helin Cao,Sven Behnke*

Main category: cs.CV

TL;DR: 提出Object - Centric SOP (OC - SOP)框架解决传统相机方法在语义占用预测中的问题，在SemanticKITTI上达最优。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶感知受遮挡和场景数据不完整挑战，传统相机方法在语义占用预测中对所有类别平等对待且依赖局部特征，预测效果不佳。

Method: 提出Object - Centric SOP (OC - SOP)框架，将通过检测分支提取的高级以对象为中心的线索集成到语义占用预测管道中。

Result: 显著提高了前景对象的预测准确性，在SemanticKITTI所有类别中达到了最优性能。

Conclusion: OC - SOP框架有效提升语义占用预测性能。

Abstract: Autonomous driving perception faces significant challenges due to occlusions
and incomplete scene data in the environment. To overcome these issues, the
task of semantic occupancy prediction (SOP) is proposed, which aims to jointly
infer both the geometry and semantic labels of a scene from images. However,
conventional camera-based methods typically treat all categories equally and
primarily rely on local features, leading to suboptimal predictions, especially
for dynamic foreground objects. To address this, we propose Object-Centric SOP
(OC-SOP), a framework that integrates high-level object-centric cues extracted
via a detection branch into the semantic occupancy prediction pipeline. This
object-centric integration significantly enhances the prediction accuracy for
foreground objects and achieves state-of-the-art performance among all
categories on SemanticKITTI.

</details>


### [470] [TAMMs: Temporal-Aware Multimodal Model for Satellite Image Change Understanding and Forecasting](https://arxiv.org/abs/2506.18862)
*Zhongbin Guo,Yuhao Wang,Ping Jian,Xinyue Chen,Wei Peng,Ertai E*

Main category: cs.CV

TL;DR: 本文提出TAMMs模型用于卫星图像变化理解和预测，在相关任务上优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在卫星图像时间序列分析的细粒度时空推理上存在挑战，研究其在联合时间变化理解和未来场景生成任务上的能力。

Method: 提出TAMMs模型，用轻量级时间模块增强冻结的MLLMs进行结构化序列编码和上下文提示，引入SFCI机制结合高级语义推理和结构先验进行未来图像生成。

Result: TAMMs在时间变化理解和未来图像预测任务上优于强大的MLLM基线模型。

Conclusion: 精心设计的时间推理和语义融合能释放MLLMs在时空理解上的全部潜力。

Abstract: Satellite image time-series analysis demands fine-grained spatial-temporal
reasoning, which remains a challenge for existing multimodal large language
models (MLLMs). In this work, we study the capabilities of MLLMs on a novel
task that jointly targets temporal change understanding and future scene
generation, aiming to assess their potential for modeling complex multimodal
dynamics over time. We propose TAMMs, a Temporal-Aware Multimodal Model for
satellite image change understanding and forecasting, which enhances frozen
MLLMs with lightweight temporal modules for structured sequence encoding and
contextual prompting. To guide future image generation, TAMMs introduces a
Semantic-Fused Control Injection (SFCI) mechanism that adaptively combines
high-level semantic reasoning and structural priors within an enhanced
ControlNet. This dual-path conditioning enables temporally consistent and
semantically grounded image synthesis. Experiments demonstrate that TAMMs
outperforms strong MLLM baselines in both temporal change understanding and
future image forecasting tasks, highlighting how carefully designed temporal
reasoning and semantic fusion can unlock the full potential of MLLMs for
spatio-temporal understanding.

</details>


### [471] [OmniAvatar: Efficient Audio-Driven Avatar Video Generation with Adaptive Body Animation](https://arxiv.org/abs/2506.18866)
*Qijun Gan,Ruizi Yang,Jianke Zhu,Shaofei Xue,Steven Hoi*

Main category: cs.CV

TL;DR: 提出OmniAvatar模型解决音频驱动全身动画生成问题，实验显示其效果超现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有音频驱动人体动画方法多聚焦面部，难以实现自然同步和流畅的全身动画，且缺乏精确提示控制。

Method: 引入像素级多层次音频嵌入策略捕获音频特征，采用基于LoRA的训练方法结合音频特征和提示控制。

Result: OmniAvatar在面部和半身视频生成上超越现有模型，能进行精确文本控制以创建多领域视频。

Conclusion: OmniAvatar模型有效解决现有音频驱动人体动画方法的问题，具有良好性能和应用潜力。

Abstract: Significant progress has been made in audio-driven human animation, while
most existing methods focus mainly on facial movements, limiting their ability
to create full-body animations with natural synchronization and fluidity. They
also struggle with precise prompt control for fine-grained generation. To
tackle these challenges, we introduce OmniAvatar, an innovative audio-driven
full-body video generation model that enhances human animation with improved
lip-sync accuracy and natural movements. OmniAvatar introduces a pixel-wise
multi-hierarchical audio embedding strategy to better capture audio features in
the latent space, enhancing lip-syncing across diverse scenes. To preserve the
capability for prompt-driven control of foundation models while effectively
incorporating audio features, we employ a LoRA-based training approach.
Extensive experiments show that OmniAvatar surpasses existing models in both
facial and semi-body video generation, offering precise text-based control for
creating videos in various domains, such as podcasts, human interactions,
dynamic scenes, and singing. Our project page is
https://omni-avatar.github.io/.

</details>


### [472] [OmniGen2: Exploration to Advanced Multimodal Generation](https://arxiv.org/abs/2506.18871)
*Chenyuan Wu,Pengfei Zheng,Ruiran Yan,Shitao Xiao,Xin Luo,Yueze Wang,Wanli Li,Xiyan Jiang,Yexin Liu,Junjie Zhou,Ze Liu,Ziyi Xia,Chaofan Li,Haoge Deng,Jiahao Wang,Kun Luo,Bo Zhang,Defu Lian,Xinlong Wang,Zhongyuan Wang,Tiejun Huang,Zheng Liu*

Main category: cs.CV

TL;DR: 介绍多功能开源生成模型OmniGen2，可用于多种生成任务，设计独特，有数据构建管道等，在多任务基准测试表现佳，还引入新基准，将发布相关资源。


<details>
  <summary>Details</summary>
Motivation: 为多样化生成任务提供统一解决方案，改进OmniGen v1。

Method: 采用文本和图像模态的不同解码路径，使用非共享参数和分离图像分词器，开发数据构建管道，引入图像生成反射机制和数据集，引入新基准OmniContext。

Result: OmniGen2参数规模不大但在多任务基准测试有竞争力，在上下文生成一致性上达开源模型最优。

Conclusion: OmniGen2设计有效，能促进该领域未来研究，将发布模型、代码、数据集和管道。

Abstract: In this work, we introduce OmniGen2, a versatile and open-source generative
model designed to provide a unified solution for diverse generation tasks,
including text-to-image, image editing, and in-context generation. Unlike
OmniGen v1, OmniGen2 features two distinct decoding pathways for text and image
modalities, utilizing unshared parameters and a decoupled image tokenizer. This
design enables OmniGen2 to build upon existing multimodal understanding models
without the need to re-adapt VAE inputs, thereby preserving the original text
generation capabilities. To facilitate the training of OmniGen2, we developed
comprehensive data construction pipelines, encompassing image editing and
in-context generation data. Additionally, we introduce a reflection mechanism
tailored for image generation tasks and curate a dedicated reflection dataset
based on OmniGen2. Despite its relatively modest parameter size, OmniGen2
achieves competitive results on multiple task benchmarks, including
text-to-image and image editing. To further evaluate in-context generation,
also referred to as subject-driven tasks, we introduce a new benchmark named
OmniContext. OmniGen2 achieves state-of-the-art performance among open-source
models in terms of consistency. We will release our models, training code,
datasets, and data construction pipeline to support future research in this
field. Project Page: https://vectorspacelab.github.io/OmniGen2; GitHub Link:
https://github.com/VectorSpaceLab/OmniGen2

</details>


### [473] [Vision as a Dialect: Unifying Visual Understanding and Generation via Text-Aligned Representations](https://arxiv.org/abs/2506.18898)
*Jiaming Han,Hao Chen,Yang Zhao,Hanyu Wang,Qi Zhao,Ziyan Yang,Hao He,Xiangyu Yue,Lu Jiang*

Main category: cs.CV

TL;DR: 提出多模态框架Tar，整合视觉理解与生成，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 统一视觉理解和生成在共享离散语义表示中，避免特定模态设计。

Method: 使用Text - Aligned Tokenizer将图像转化为离散令牌；提出自适应编码解码、生成式去令牌器；采用两种互补去令牌器；研究高级预训练任务。

Result: Tar在基准测试中匹配或超越现有多模态大语言模型方法，收敛更快、训练效率更高。

Conclusion: 该多模态框架有效，可用于视觉理解和生成。

Abstract: This paper presents a multimodal framework that attempts to unify visual
understanding and generation within a shared discrete semantic representation.
At its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into
discrete tokens using a text-aligned codebook projected from a large language
model's (LLM) vocabulary. By integrating vision and text into a unified space
with an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input
and output through a shared interface, without the need for modality-specific
designs. Additionally, we propose scale-adaptive encoding and decoding to
balance efficiency and visual detail, along with a generative de-tokenizer to
produce high-fidelity visual outputs. To address diverse decoding needs, we
utilize two complementary de-tokenizers: a fast autoregressive model and a
diffusion-based model. To enhance modality fusion, we investigate advanced
pre-training tasks, demonstrating improvements in both visual understanding and
generation. Experiments across benchmarks show that Tar matches or surpasses
existing multimodal LLM methods, achieving faster convergence and greater
training efficiency. Code, models, and data are available at
https://tar.csuhan.com

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [474] [JAX-LaB: A High-Performance, Differentiable, Lattice Boltzmann Library for Modeling Multiphase Fluid Dynamics in Geosciences and Engineering](https://arxiv.org/abs/2506.17713)
*Piyush Pradhan,Pierre Gentine,Shaina Kelly*

Main category: physics.comp-ph

TL;DR: 介绍了基于Python的可微格子玻尔兹曼库JAX - LaB，用于模拟多孔介质多相多物理流，进行了验证、性能测试，库开源。


<details>
  <summary>Details</summary>
Motivation: 开发可用于模拟水文、地质和工程多孔介质中多相多物理流的库，且能与机器学习工作流集成。

Method: 基于XLB库扩展，用JAX计算，用Shan - Chen伪势方法建模多相相互作用，用“改进”虚拟密度方案处理润湿性。

Result: 通过多个分析基准验证库，展示了示例用例，报告了单GPU和多GPU性能扩展。

Conclusion: 成功开发JAX - LaB库，可有效模拟多相多物理流，且性能良好，代码开源。

Abstract: We present JAX-LaB, a differentiable, Python-based Lattice Boltzmann library
for simulating multiphase and multiphysics flows in hydrologic, geologic, and
engineered porous media. Built as an extension of the XLB library, JAX-LaB
utilizes JAX for computations and offers a performant, hardware-agnostic
implementation that integrates seamlessly with machine learning workflows and
scales efficiently across CPUs, GPUs, and distributed systems. Multiphase
interactions are modeled using the Shan-Chen pseudopotential method, which is
coupled with an equation of state and an improved forcing scheme to obtain
liquid-vapor densities that are consistent with Maxwell's construction,
enabling simulations of systems with very large density ratios while
maintaining minimal spurious currents. Wetting is handled using the "improved"
virtual density scheme, which allows precise control of contact angles and
eliminates non-physical films seen in other Shan-Chen wetting methods. We
validate the library through several analytical benchmarks, such as Laplace's
law, capillary rise, and cocurrent multicomponent flow, and demonstrate some
exemplary use cases for the library. We also report single- and multi-GPU
performance scaling of the library. The library is open-source under the Apache
license and available at https://github.com/piyush-ppradhan/JAX-LaB.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [475] [Can Common VLMs Rival Medical VLMs? Evaluation and Strategic Insights](https://arxiv.org/abs/2506.17337)
*Yuan Zhong,Ruinan Jin,Xiaoxiao Li,Qi Dou*

Main category: eess.IV

TL;DR: 研究评估通用和医学视觉语言模型在疾病诊断和视觉问答任务中的表现，发现通用模型微调后可媲美医学模型，为医学影像研究提供新见解。


<details>
  <summary>Details</summary>
Motivation: 探讨高效微调的通用视觉语言模型能否在特定医学影像任务上与通用医学视觉语言模型竞争。

Method: 使用基于CLIP和LLaVA的模型，评估现成性能差距、微调能否弥补差距以及对域外任务的泛化能力。

Result: 医学预训练在域内有优势，但通用模型轻量级微调后可匹配或超越医学模型；在域外任务中，通用模型在部分任务显示强适应性。

Conclusion: 微调通用视觉语言模型是开发大规模医学视觉语言模型的可扩展且经济有效的替代方案。

Abstract: Medical vision-language models (VLMs) leverage large-scale pretraining for
diverse imaging tasks but require substantial computational and data resources.
Meanwhile, common or general-purpose VLMs (e.g., CLIP, LLaVA), though not
trained for medical use, show promise with fine-tuning. This raises a key
question: Can efficient fine-tuned common VLMs rival generalist medical VLMs
for solving specific medical imaging tasks? This study systematically evaluates
common and medical VLMs across disease diagnosis and visual question answering
(VQA). Using CLIP-based and LLaVA-based models, we examine (1) off-the-shelf
performance gaps in in-domain (ID) settings, (2) whether fine-tuning bridges
these gaps, and (3) generalization to out-of-domain (OOD) tasks on unseen
medical modalities. While medical-specific pretraining provides advantages in
ID settings, common VLMs match or surpass medical-specific models after
lightweight fine-tuning, with LoRA-based adaptation proving highly effective
among different tasks. In OOD tasks, common VLMs demonstrate strong
adaptability in some tasks, challenging the assumption that medical-specific
pre-training is essential. These findings suggest that leveraging common VLMs
with fine-tuning offers a scalable and cost-effective alternative to developing
large-scale medical VLMs, providing crucial insights for future research in the
medical imaging field.

</details>


### [476] [MTSIC: Multi-stage Transformer-based GAN for Spectral Infrared Image Colorization](https://arxiv.org/abs/2506.17540)
*Tingting Liu,Yuan Liu,Jinhui Tang,Liyin Yuan,Chengyu Liu,Chunlai Li,Xiubao Sui,Qian Chen*

Main category: eess.IV

TL;DR: 提出基于GAN框架结合多波段光谱信息增强红外图像着色，实验显示性能优于传统技术。


<details>
  <summary>Details</summary>
Motivation: 热红外图像缺乏颜色和纹理信息，现有着色方法依赖单波段图像，有图像失真和语义模糊问题，而多波段红外图像可提供更丰富光谱数据。

Method: 提出基于GAN框架，采用多阶段光谱自注意力Transformer网络（MTSIC）作为生成器，利用多波段特征映射，结合U形架构和多尺度小波块。

Result: 所提方法显著优于传统技术，有效提升红外图像视觉质量。

Conclusion: 基于GAN结合多波段光谱信息的方法能有效改善红外图像着色效果。

Abstract: Thermal infrared (TIR) images, acquired through thermal radiation imaging,
are unaffected by variations in lighting conditions and atmospheric haze.
However, TIR images inherently lack color and texture information, limiting
downstream tasks and potentially causing visual fatigue. Existing colorization
methods primarily rely on single-band images with limited spectral information
and insufficient feature extraction capabilities, which often result in image
distortion and semantic ambiguity. In contrast, multiband infrared imagery
provides richer spectral data, facilitating the preservation of finer details
and enhancing semantic accuracy. In this paper, we propose a generative
adversarial network (GAN)-based framework designed to integrate spectral
information to enhance the colorization of infrared images. The framework
employs a multi-stage spectral self-attention Transformer network (MTSIC) as
the generator. Each spectral feature is treated as a token for self-attention
computation, and a multi-head self-attention mechanism forms a spatial-spectral
attention residual block (SARB), achieving multi-band feature mapping and
reducing semantic confusion. Multiple SARB units are integrated into a
Transformer-based single-stage network (STformer), which uses a U-shaped
architecture to extract contextual information, combined with multi-scale
wavelet blocks (MSWB) to align semantic information in the spatial-frequency
dual domain. Multiple STformer modules are cascaded to form MTSIC,
progressively optimizing the reconstruction quality. Experimental results
demonstrate that the proposed method significantly outperforms traditional
techniques and effectively enhances the visual quality of infrared images.

</details>


### [477] [Multimodal Medical Image Binding via Shared Text Embeddings](https://arxiv.org/abs/2506.18072)
*Yunhao Liu,Suyang Xi,Shiqi Liu,Hong Ding,Chicheng Jin,Chenxi Yang,Junjun He,Yiqing Shen*

Main category: eess.IV

TL;DR: 提出多模态医学图像绑定文本框架M³Bind，无需显式配对数据实现多模态医学图像对齐，实验证明其在多项任务中达SOTA。


<details>
  <summary>Details</summary>
Motivation: 医学图像分析需多模态信息整合，但现有方法如CLIP需要显式配对数据，在医学场景中难获取。

Method: 先微调预训练的类CLIP图像 - 文本模型以对齐特定模态文本嵌入空间，再将特定模态文本编码器蒸馏为统一模型创建共享文本嵌入空间。

Result: 在多种下游任务实验中，M³Bind在零样本、少样本分类和跨模态检索任务中比类CLIP模型表现更优。

Conclusion: M³Bind在医学分析的跨图像模态对齐方面有效。

Abstract: Medical image analysis increasingly relies on the integration of multiple
imaging modalities to capture complementary anatomical and functional
information, enabling more accurate diagnosis and treatment planning. Achieving
aligned feature representations across these diverse modalities is therefore
important for effective multimodal analysis. While contrastive language-image
pre-training (CLIP) and its variant have enabled image-text alignments, they
require explicitly paired data between arbitrary two modalities, which is
difficult to acquire in medical contexts. To address the gap, we present
Multimodal Medical Image Binding with Text (M\textsuperscript{3}Bind), a novel
pre-training framework that enables seamless alignment of multiple medical
imaging modalities through a shared text representation space without requiring
explicit paired data between any two medical image modalities. Specifically,
based on the insight that different images can naturally bind with text,
M\textsuperscript{3}Bind first fine-tunes pre-trained CLIP-like image-text
models to align their modality-specific text embedding space while preserving
their original image-text alignments. Subsequently, we distill these
modality-specific text encoders into a unified model, creating a shared text
embedding space. Experiments on X-ray, CT, retina, ECG, and pathological images
on multiple downstream tasks demonstrate that M\textsuperscript{3}Bind achieves
state-of-the-art performance in zero-shot, few-shot classification and
cross-modal retrieval tasks compared to its CLIP-like counterparts. These
results validate M\textsuperscript{3}Bind's effectiveness in achieving
cross-image-modal alignment for medical analysis.

</details>


### [478] [CT Radiomics-Based Explainable Machine Learning Model for Accurate Differentiation of Malignant and Benign Endometrial Tumors: A Two-Center Study](https://arxiv.org/abs/2506.18106)
*Tingrui Zhang,Honglin Wu,Zekun Jiang,Yingying Wang,Rui Ye,Huiming Ni,Chang Liu,Jin Cao,Xuan Sun,Rong Shao,Xiaorong Wei,Yingchun Sun*

Main category: eess.IV

TL;DR: 开发并验证基于CT影像组学的可解释机器学习模型用于诊断子宫内膜癌良恶性，随机森林模型效果最佳，可作智能辅助诊断工具。


<details>
  <summary>Details</summary>
Motivation: 开发并验证基于CT影像组学的可解释机器学习模型，用于诊断子宫内膜癌患者的良恶性。

Method: 收集83例患者数据并划分训练和测试集，手动分割CT图像感兴趣区域，提取1132个影像组学特征，实施六种可解释机器学习算法确定最佳流程，用多种指标评估模型性能，进行SHAP分析等。

Result: 随机森林模型为诊断EC的最优选择，训练AUC为1.00，测试AUC为0.96，SHAP确定重要特征，影像组学特征图可用于临床，DCA显示模型有更高净效益。

Conclusion: 基于CT影像组学的可解释机器学习模型诊断性能高，可作为子宫内膜癌诊断的智能辅助工具。

Abstract: Aimed to develop and validate a CT radiomics-based explainable machine
learning model for diagnosing malignancy and benignity specifically in
endometrial cancer (EC) patients. A total of 83 EC patients from two centers,
including 46 with malignant and 37 with benign conditions, were included, with
data split into a training set (n=59) and a testing set (n=24). The regions of
interest (ROIs) were manually segmented from pre-surgical CT scans, and 1132
radiomic features were extracted from the pre-surgical CT scans using
Pyradiomics. Six explainable machine learning modeling algorithms were
implemented respectively, for determining the optimal radiomics pipeline. The
diagnostic performance of the radiomic model was evaluated by using
sensitivity, specificity, accuracy, precision, F1 score, confusion matrices,
and ROC curves. To enhance clinical understanding and usability, we separately
implemented SHAP analysis and feature mapping visualization, and evaluated the
calibration curve and decision curve. By comparing six modeling strategies, the
Random Forest model emerged as the optimal choice for diagnosing EC, with a
training AUC of 1.00 and a testing AUC of 0.96. SHAP identified the most
important radiomic features, revealing that all selected features were
significantly associated with EC (P < 0.05). Radiomics feature maps also
provide a feasible assessment tool for clinical applications. DCA indicated a
higher net benefit for our model compared to the "All" and "None" strategies,
suggesting its clinical utility in identifying high-risk cases and reducing
unnecessary interventions. In conclusion, the CT radiomics-based explainable
machine learning model achieved high diagnostic performance, which could be
used as an intelligent auxiliary tool for the diagnosis of endometrial cancer.

</details>


### [479] [A Deep Convolutional Neural Network-Based Novel Class Balancing for Imbalance Data Segmentation](https://arxiv.org/abs/2506.18474)
*Atifa Kalsoom,M. A. Iftikhar,Amjad Ali,Zubair Shah,Shidin Balakrishnan,Hazrat Ali*

Main category: eess.IV

TL;DR: 提出基于深度学习和双级类别平衡方案的BLCB - CNN用于视网膜眼底图像血管分割，经评估表现优异，有泛化能力。


<details>
  <summary>Details</summary>
Motivation: 视网膜血管准确分割因数据分布不均衡和血管厚度不同而具有挑战性。

Method: 提出BLCB - CNN方案，采用CNN架构和经验方法进行双级像素分布平衡，对输入图像进行预处理，用平衡数据集进行分类分割。

Result: 在标准视网膜眼底图像上评估，ROC曲线下面积98.23%、准确率96.22%、灵敏度81.57%、特异度97.65%，在STARE图像外部交叉验证证实泛化能力。

Conclusion: 所提方案在视网膜血管分割中表现优异，具有良好泛化能力。

Abstract: Retinal fundus images provide valuable insights into the human eye's interior
structure and crucial features, such as blood vessels, optic disk, macula, and
fovea. However, accurate segmentation of retinal blood vessels can be
challenging due to imbalanced data distribution and varying vessel thickness.
In this paper, we propose BLCB-CNN, a novel pipeline based on deep learning and
bi-level class balancing scheme to achieve vessel segmentation in retinal
fundus images. The BLCB-CNN scheme uses a Convolutional Neural Network (CNN)
architecture and an empirical approach to balance the distribution of pixels
across vessel and non-vessel classes and within thin and thick vessels. Level-I
is used for vessel/non-vessel balancing and Level-II is used for thick/thin
vessel balancing. Additionally, pre-processing of the input retinal fundus
image is performed by Global Contrast Normalization (GCN), Contrast Limited
Adaptive Histogram Equalization (CLAHE), and gamma corrections to increase
intensity uniformity as well as to enhance the contrast between vessels and
background pixels. The resulting balanced dataset is used for
classification-based segmentation of the retinal vascular tree. We evaluate the
proposed scheme on standard retinal fundus images and achieve superior
performance measures, including an area under the ROC curve of 98.23%, Accuracy
of 96.22%, Sensitivity of 81.57%, and Specificity of 97.65%. We also
demonstrate the method's efficacy through external cross-validation on STARE
images, confirming its generalization ability.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [480] [Modal Logic for Stratified Becoming: Actualization Beyond Possible Worlds](https://arxiv.org/abs/2506.17276)
*Alexandre Le Nepvou*

Main category: cs.LO

TL;DR: 本文基于分层实现思想开发模态逻辑新框架，提出SAL系统，定义其语法、语义和公理，证明可靠性和完备性，并探讨应用，提供标准模态实在论的分层替代方案。


<details>
  <summary>Details</summary>
Motivation: 传统克里普克语义忽略了实现过程的局部、动态和不对称性质，需要新的模态逻辑框架。

Method: 提出分层实现逻辑（SAL）系统，其中模态由本体稳定性级别索引，基于层间转换的内部连贯性；形式定义SAL的语法和语义，引入公理并证明可靠性和完备性。

Result: 得到一种无需借助抽象可能世界就能捕捉实现的本体结构的逻辑。

Conclusion: SAL系统为标准模态实在论提供了分层替代方案。

Abstract: This article develops a novel framework for modal logic based on the idea of
stratified actualization, rather than the classical model of global possible
worlds. Traditional Kripke semantics treat modal operators as quantification
over fully determinate alternatives, neglecting the local, dynamic, and often
asymmetric nature of actualization processes. We propose a system Stratified
Actualization Logic (SAL) in which modalities are indexed by levels of
ontological stability, interpreted as admissibility regimes. Each modality
operates over a structured layer of possibility, grounded in the internal
coherence of transitions between layers. We formally define the syntax and
semantics of SAL, introduce its axioms, and prove soundness and completeness.
Applications are discussed in connection with temporal becoming, quantum
decoherence domains, and modal metaphysics. The result is a logic that captures
the ontological structure of actualization without recourse to abstract
possible worlds, offering a stratified alternative to standard modal realism.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [481] [PaceLLM: Brain-Inspired Large Language Models for Long-Context Understanding](https://arxiv.org/abs/2506.17310)
*Kangcong Li,Peng Ye,Chongjun Tu,Lin Zhang,Chunfeng Song,Jiamin Wu,Tao Yang,Qihao Zheng,Tao Chen*

Main category: q-bio.NC

TL;DR: 本文提出PaceLLM模型解决大语言模型长上下文能力问题，经评估有性能提升且可泛化。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型长上下文能力受信息衰减和语义碎片化限制，需改进。

Method: 提出PaceLLM，包含模拟前额叶皮质神经元持续放电的持久活动机制和模拟任务自适应神经专业化的皮质专家聚类方法。

Result: PaceLLM在LongBench多文档问答中提升6%，在Infinite - Bench任务中性能提升12.5 - 17.5%，在NIAH测试中可将可测量上下文长度扩展到200K个token。

Conclusion: 该工作开创了受大脑启发的大语言模型优化，可泛化到任何模型，提升长上下文性能和可解释性，无需大规模结构修改。

Abstract: While Large Language Models (LLMs) demonstrate strong performance across
domains, their long-context capabilities are limited by transient neural
activations causing information decay and unstructured feed-forward network
(FFN) weights leading to semantic fragmentation. Inspired by the brain's
working memory and cortical modularity, we propose PaceLLM, featuring two
innovations: (1) a Persistent Activity (PA) Mechanism that mimics prefrontal
cortex (PFC) neurons' persistent firing by introducing an activation-level
memory bank to dynamically retrieve, reuse, and update critical FFN states,
addressing contextual decay; and (2) Cortical Expert (CE) Clustering that
emulates task-adaptive neural specialization to reorganize FFN weights into
semantic modules, establishing cross-token dependencies and mitigating
fragmentation. Extensive evaluations show that PaceLLM achieves 6% improvement
on LongBench's Multi-document QA and 12.5-17.5% performance gains on
Infinite-Bench tasks, while extending measurable context length to 200K tokens
in Needle-In-A-Haystack (NIAH) tests. This work pioneers brain-inspired LLM
optimization and is complementary to other works. Besides, it can be
generalized to any model and enhance their long-context performance and
interpretability without structural overhauls.

</details>


### [482] [Challenges in Grounding Language in the Real World](https://arxiv.org/abs/2506.17375)
*Peter Lindes,Kaoutar Skiker*

Main category: q-bio.NC

TL;DR: 本文指出让人类与物理机器人用自然语言协作的挑战，提出结合认知代理与大语言模型能力的解决方案并指明初步实现方向。


<details>
  <summary>Details</summary>
Motivation: 实现让人类与物理机器人用自然语言协作的长期目标。

Method: 将具备交互式任务学习能力的认知代理与大语言模型的语言能力相集成。

Result: 无明确提及具体结果。

Conclusion: 提出了解决人类与物理机器人自然语言协作挑战的方案并指出初步实现途径。

Abstract: A long-term goal of Artificial Intelligence is to build a language
understanding system that allows a human to collaborate with a physical robot
using language that is natural to the human. In this paper we highlight some of
the challenges in doing this, and propose a solution that integrates the
abilities of a cognitive agent capable of interactive task learning in a
physical robot with the linguistic abilities of a large language model. We also
point the way to an initial implementation of this approach.

</details>


### [483] [Sequence-to-Sequence Models with Attention Mechanistically Map to the Architecture of Human Memory Search](https://arxiv.org/abs/2506.17424)
*Nikolaus Salvatore,Qiong Zhang*

Main category: q-bio.NC

TL;DR: 本文发现神经机器翻译模型与人类记忆模型有对应机制，用其实现人类记忆搜索认知模型，能解释人类行为模式。


<details>
  <summary>Details</summary>
Motivation: 以往基于上下文的记忆模型未解释人类为何发展出此类架构，旨在深入理解上下文在人类记忆中的功能作用及探索新的人类记忆建模方法。

Method: 研究神经机器翻译中的RNN序列到序列模型与人类记忆CMR模型的对应机制，实现神经机器翻译模型作为人类记忆搜索的认知模型。

Result: 模型能有效解释人类平均和最优行为模式，评估模型组件交互展现出更多优势。

Conclusion: 神经机器翻译模型与人类记忆模型的融合为理解人类记忆和建模提供新思路。

Abstract: Past work has long recognized the important role of context in guiding how
humans search their memory. While context-based memory models can explain many
memory phenomena, it remains unclear why humans develop such architectures over
possible alternatives in the first place. In this work, we demonstrate that
foundational architectures in neural machine translation -- specifically,
recurrent neural network (RNN)-based sequence-to-sequence models with attention
-- exhibit mechanisms that directly correspond to those specified in the
Context Maintenance and Retrieval (CMR) model of human memory. Since neural
machine translation models have evolved to optimize task performance, their
convergence with human memory models provides a deeper understanding of the
functional role of context in human memory, as well as presenting new ways to
model human memory. Leveraging this convergence, we implement a neural machine
translation model as a cognitive model of human memory search that is both
interpretable and capable of capturing complex dynamics of learning. We show
that our model accounts for both averaged and optimal human behavioral patterns
as effectively as context-based memory models. Further, we demonstrate
additional strengths of the proposed model by evaluating how memory search
performance emerges from the interaction of different model components.

</details>


<div id='q-fin.PR'></div>

# q-fin.PR [[Back]](#toc)

### [484] [Empirical Models of the Time Evolution of SPX Option Prices](https://arxiv.org/abs/2506.17511)
*Alessio Brini,David A. Hsieh,Patrick Kuiper,Sean Moushegian,David Ye*

Main category: q-fin.PR

TL;DR: 本文旨在开发可模拟SPX期权未来路径的定价实证模型，评估了神经网络、随机森林和线性回归等模型，最终发现特定神经网络模型表现良好。


<details>
  <summary>Details</summary>
Motivation: 开发能模拟SPX期权未来路径的定价实证模型。

Method: 构建并评估神经网络、随机森林和线性回归等统计模型，使用期权观测特征和外部输入作为模型输入，在30年历史期权数据上进行评估，对神经网络确保可解释性。

Result: 一个两层、每层四个神经元且极少超参数调整的神经网络模型，相较于理论的布莱克 - 斯科尔斯 - 默顿模型和其他两个实证模型表现良好，能给出无套利的期权价格。

Conclusion: 特定的神经网络模型在SPX期权定价上表现出色，无需强加条件即可给出无套利期权价格。

Abstract: The key objective of this paper is to develop an empirical model for pricing
SPX options that can be simulated over future paths of the SPX. To accomplish
this, we formulate and rigorously evaluate several statistical models,
including neural network, random forest, and linear regression. These models
use the observed characteristics of the options as inputs -- their price,
moneyness and time-to-maturity, as well as a small set of external inputs, such
as the SPX and its past history, dividend yield, and the risk-free rate. Model
evaluation is performed on historical options data, spanning 30 years of daily
observations. Significant effort is given to understanding the data and
ensuring explainability for the neural network. A neural network model with two
hidden layers and four neurons per layer, trained with minimal hyperparameter
tuning, performs well against the theoretical Black-Scholes-Merton model for
European options, as well as two other empirical models based on the random
forest and the linear regression. It delivers arbitrage-free option prices
without requiring these conditions to be imposed.

</details>


### [485] [American options valuation in time-dependent jump-diffusion models via integral equations and characteristic functions](https://arxiv.org/abs/2506.18210)
*Andrey Itkin*

Main category: q-fin.PR

TL;DR: 本文扩展半解析方法为美式期权定价，解决关键问题，数值示例显示其高效稳健，适用于大规模工业应用。


<details>
  <summary>Details</summary>
Motivation: 机器学习在衍生品定价有进展，但美式期权高效准确估值仍因复杂边界、到期行为和合约特征而面临挑战。

Method: 推导并求解第二类Volterra积分方程确定行使边界，扩展分解方法，利用特征函数处理无闭式转移密度情况，将框架推广到多维扩散。

Result: 数值示例表明该方法高效且稳健。

Conclusion: 积分方程方法适用于大规模工业应用，解决了现有技术的一些局限。

Abstract: Despite significant advancements in machine learning for derivative pricing,
the efficient and accurate valuation of American options remains a persistent
challenge due to complex exercise boundaries, near-expiry behavior, and
intricate contractual features. This paper extends a semi-analytical approach
for pricing American options in time-inhomogeneous models, including pure
diffusions, jump-diffusions, and Levy processes. Building on prior work, we
derive and solve Volterra integral equations of the second kind to determine
the exercise boundary explicitly, offering a computationally superior
alternative to traditional finite-difference and Monte Carlo methods. We
address key open problems: (1) extending the decomposition method, i.e.
splitting the American option price into its European counterpart and an early
exercise premium, to general jump-diffusion and Levy models; (2) handling cases
where closed-form transition densities are unavailable by leveraging
characteristic functions via, e.g., the COS method; and (3) generalizing the
framework to multidimensional diffusions. Numerical examples demonstrate the
method's efficiency and robustness. Our results underscore the advantages of
the integral equation approach for large-scale industrial applications, while
resolving some limitations of existing techniques.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [486] [Consistent Channel Hopping Algorithms for the Multichannel Rendezvous Problem with Heterogeneous Available Channel Sets](https://arxiv.org/abs/2506.18381)
*Yiwei Liu,Yi-Chia Cheng,Cheng-Shang Chang*

Main category: cs.NI

TL;DR: 本文提出一致信道跳频算法理论框架解决异构可用信道集下的多信道会合问题，分析双用户和多用户情况，提出模算法并通过仿真验证有效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决无线异构可用信道集下的多信道会合问题。

Method: 提出一致信道选择函数，用序列排列表示算法，引入虚拟用户分析双用户情况，提出模算法，拓展到多用户并提出新策略。

Result: 表明所有一致信道选择函数等价，推导双用户会合时间边界，证明算法可最大化会合概率，模算法性能与LSH算法相当，仿真验证算法有效且可扩展。

Conclusion: 提出的一致信道跳频算法理论框架及相关算法能有效解决多信道会合问题，具有实际应用价值。

Abstract: We propose a theoretical framework for consistent channel hopping algorithms
to address the multichannel rendezvous problem (MRP) in wireless networks with
heterogeneous available channel sets. A channel selection function is called
consistent if the selected channel remains unchanged when the available channel
set shrinks, provided the selected channel is still available. We show that all
consistent channel selection functions are equivalent to the function that
always selects the smallest-index channel under appropriate channel relabeling.
This leads to a natural representation of a consistent channel hopping
algorithm as a sequence of permutations. For the two-user MRP, we characterize
rendezvous time slots using a fictitious user and derive tight bounds on the
maximum time-to-rendezvous (MTTR) and expected time-to-rendezvous (ETTR).
Notably, the ETTR is shown to be the inverse of the Jaccard index when
permutations are randomly selected. We also prove that consistent channel
hopping algorithms maximize the rendezvous probability. To reduce
implementation complexity, we propose the modulo algorithm, which uses modular
arithmetic with one-cycle permutations and achieves performance comparable to
locality-sensitive hashing (LSH)-based algorithms. The framework is extended to
multiple users, with novel strategies such as stick-together, spread-out, and a
hybrid method that accelerates rendezvous in both synchronous and asynchronous
settings. Simulation results confirm the effectiveness and scalability of the
proposed algorithms.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [487] [AI is the Strategy: From Agentic AI to Autonomous Business Models onto Strategy in the Age of AI](https://arxiv.org/abs/2506.17339)
*René Bohnsack,Mickie de Wet*

Main category: cs.CY

TL;DR: 文章提出自主商业模式（ABMs）概念，结合案例展示从增强到自主商业模式的演变，指出其重塑竞争优势，带来新竞争形式并挑战传统假设。


<details>
  <summary>Details</summary>
Motivation: 在智能代理AI时代，探讨其对商业模式的影响，重新审视AI在战略中的角色。

Method: 通过getswan.ai和假设的瑞安航空两个案例，描绘商业模式从增强到自主的演变。

Result: ABMs通过智能执行、持续适应和逐步减少人类决策重塑竞争优势，带来合成竞争。

Conclusion: 应将智能代理AI置于商业模式执行的核心，重新思考战略管理。

Abstract: This article develops the concept of Autonomous Business Models (ABMs) as a
distinct managerial and strategic logic in the age of agentic AI. While most
firms still operate within human-driven or AI-augmented models, we argue that
we are now entering a phase where agentic AI (systems capable of initiating,
coordinating, and adapting actions autonomously) can increasingly execute the
core mechanisms of value creation, delivery, and capture. This shift reframes
AI not as a tool to support strategy, but as the strategy itself. Using two
illustrative cases, getswan.ai, an Israeli startup pursuing autonomy by design,
and a hypothetical reconfiguration of Ryanair as an AI-driven incumbent, we
depict the evolution from augmented to autonomous business models. We show how
ABMs reshape competitive advantage through agentic execution, continuous
adaptation, and the gradual offloading of human decision-making. This
transition introduces new forms of competition between AI-led firms, which we
term synthetic competition, where strategic interactions occur at rapid,
machine-level speed and scale. It also challenges foundational assumptions in
strategy, organizational design, and governance. By positioning agentic AI as
the central actor in business model execution, the article invites us to
rethink strategic management in an era where firms increasingly run themselves.

</details>


### [488] [A Grassroots Network and Community Roadmap for Interconnected Autonomous Science Laboratories for Accelerated Discovery](https://arxiv.org/abs/2506.17510)
*Rafael Ferreira da Silva,Milad Abolhasani,Dionysios A. Antonopoulos,Laura Biven,Ryan Coffee,Ian T. Foster,Leslie Hamilton,Shantenu Jha,Theresa Mayer,Benjamin Mintz,Robert G. Moore,Salahudin Nimer,Noah Paulson,Woong Shin,Frederic Suter,Mitra Taheri,Michela Taufer,Newell R. Washburn*

Main category: cs.CY

TL;DR: 提出AISLE网络，将分散能力整合，加速科学发现，实现跨机构协作，有望在多领域取得突破。


<details>
  <summary>Details</summary>
Motivation: 当前自主实验室无法跨机构协作，需要将分散能力整合以加速科学发现。

Method: 构建AISLE网络，解决跨机构设备编排、数据管理、AI代理驱动编排、代理通信接口和科学教育五个关键维度问题。

Result: 连接跨机构的自主代理，可解锁传统方法无法触及的研究空间，使前沿技术普及。

Conclusion: 向协作式自主科学的范式转变有望在可持续能源、材料开发和公共卫生领域取得突破。

Abstract: Scientific discovery is being revolutionized by AI and autonomous systems,
yet current autonomous laboratories remain isolated islands unable to
collaborate across institutions. We present the Autonomous Interconnected
Science Lab Ecosystem (AISLE), a grassroots network transforming fragmented
capabilities into a unified system that shorten the path from ideation to
innovation to impact and accelerates discovery from decades to months. AISLE
addresses five critical dimensions: (1) cross-institutional equipment
orchestration, (2) intelligent data management with FAIR compliance, (3)
AI-agent driven orchestration grounded in scientific principles, (4)
interoperable agent communication interfaces, and (5) AI/ML-integrated
scientific education. By connecting autonomous agents across institutional
boundaries, autonomous science can unlock research spaces inaccessible to
traditional approaches while democratizing cutting-edge technologies. This
paradigm shift toward collaborative autonomous science promises breakthroughs
in sustainable energy, materials development, and public health.

</details>


### [489] [Distinguishing Predictive and Generative AI in Regulation](https://arxiv.org/abs/2506.17347)
*Jennifer Wang,Andrew Selbst,Solon Barocas,Suresh Venkatasubramanian*

Main category: cs.CY

TL;DR: 过去十年政策工具难适配生成式AI，本文指出其四个不同方面并为政策制定者提供监管建议。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI出现，过去为预测性AI制定的监管工具假设不再适用，需新政策。

Method: 识别生成式AI需不同政策响应的四个方面，基于此为政策制定者提出建议。

Result: 指出了生成式AI在监管目标、评估设计、法律问题和价值链结构方面的不同。

Conclusion: 政策制定者需评估旧政策适用性，制定新政策，给出三条监管建议。

Abstract: Over the past decade, policymakers have developed a set of regulatory tools
to ensure AI development aligns with key societal goals. Many of these tools
were initially developed in response to concerns with predictive AI and
therefore encode certain assumptions about the nature of AI systems and the
utility of certain regulatory approaches. With the advent of generative AI,
however, some of these assumptions no longer hold, even as policymakers attempt
to maintain a single regulatory target that covers both types of AI.
  In this paper, we identify four distinct aspects of generative AI that call
for meaningfully different policy responses. These are the generality and
adaptability of generative AI that make it a poor regulatory target, the
difficulty of designing effective evaluations, new legal concerns that change
the ecosystem of stakeholders and sources of expertise, and the distributed
structure of the generative AI value chain.
  In light of these distinctions, policymakers will need to evaluate where the
past decade of policy work remains relevant and where new policies, designed to
address the unique risks posed by generative AI, are necessary. We outline
three recommendations for policymakers to more effectively identify regulatory
targets and leverage constraints across the broader ecosystem to govern
generative AI.

</details>


### [490] [Automatic Large Language Models Creation of Interactive Learning Lessons](https://arxiv.org/abs/2506.17356)
*Jionghao Lin,Jiarui Rao,Yiyang Zhao,Yuting Wang,Ashish Gurung,Amanda Barany,Jaclyn Ocumpaugh,Ryan S. Baker,Kenneth R. Koedinger*

Main category: cs.CY

TL;DR: 本文探索利用GPT - 4o自动生成面向在线初中数学新手辅导教师的情景互动式培训课程，通过任务分解策略生成课程并评估，发现该策略优于单步生成，凸显混合式人 - 机方法潜力。


<details>
  <summary>Details</summary>
Motivation: 探索自动生成面向在线初中数学新手辅导教师的互动式情景课程。

Method: 采用基于检索增强生成的提示工程，用GPT - 4o创建课程，使用任务分解策略将课程生成拆分为子任务，由两位评估人员使用综合评分标准进行评估。

Result: 任务分解策略生成的课程评分更高，评估人员指出课程有内容结构好、省时等优点，也存在反馈通用、部分教学内容不清晰等局限。

Conclusion: 混合式人 - 机方法在辅导教师培训课程生成中具有潜力。

Abstract: We explore the automatic generation of interactive, scenario-based lessons
designed to train novice human tutors who teach middle school mathematics
online. Employing prompt engineering through a Retrieval-Augmented Generation
approach with GPT-4o, we developed a system capable of creating structured
tutor training lessons. Our study generated lessons in English for three key
topics: Encouraging Students' Independence, Encouraging Help-Seeking Behavior,
and Turning on Cameras, using a task decomposition prompting strategy that
breaks lesson generation into sub-tasks. The generated lessons were evaluated
by two human evaluators, who provided both quantitative and qualitative
evaluations using a comprehensive rubric informed by lesson design research.
Results demonstrate that the task decomposition strategy led to higher-rated
lessons compared to single-step generation. Human evaluators identified several
strengths in the LLM-generated lessons, including well-structured content and
time-saving potential, while also noting limitations such as generic feedback
and a lack of clarity in some instructional sections. These findings underscore
the potential of hybrid human-AI approaches for generating effective lessons in
tutor training.

</details>


### [491] [A Large-Scale Real-World Evaluation of LLM-Based Virtual Teaching Assistant](https://arxiv.org/abs/2506.17363)
*Sunjun Kweon,Sooyohn Nam,Hyunseung Lim,Hwajung Hong,Edward Choi*

Main category: cs.CY

TL;DR: 本文开发基于大语言模型的虚拟助教并部署在课程中，通过调查和交互分析评估其在现实课堂的可行性与挑战，还开源了代码。


<details>
  <summary>Details</summary>
Motivation: 现有关于虚拟助教在现实课堂有效性和接受度的实证研究有限，其实践影响不明。

Method: 开发基于大语言模型的虚拟助教并部署到课程中，开展三轮调查，分析3869对学生与虚拟助教的交互，对比传统学生与人类教师的交互。

Result: 评估了虚拟助教在现实课堂部署的可行性，识别了广泛应用的关键挑战。

Conclusion: 开源虚拟助教系统代码，促进人工智能驱动教育的未来发展。

Abstract: Virtual Teaching Assistants (VTAs) powered by Large Language Models (LLMs)
have the potential to enhance student learning by providing instant feedback
and facilitating multi-turn interactions. However, empirical studies on their
effectiveness and acceptance in real-world classrooms are limited, leaving
their practical impact uncertain. In this study, we develop an LLM-based VTA
and deploy it in an introductory AI programming course with 477 graduate
students. To assess how student perceptions of the VTA's performance evolve
over time, we conduct three rounds of comprehensive surveys at different stages
of the course. Additionally, we analyze 3,869 student--VTA interaction pairs to
identify common question types and engagement patterns. We then compare these
interactions with traditional student--human instructor interactions to
evaluate the VTA's role in the learning process. Through a large-scale
empirical study and interaction analysis, we assess the feasibility of
deploying VTAs in real-world classrooms and identify key challenges for broader
adoption. Finally, we release the source code of our VTA system, fostering
future advancements in AI-driven education:
\texttt{https://github.com/sean0042/VTA}.

</details>


### [492] [AI-based Multimodal Biometrics for Detecting Smartphone Distractions: Application to Online Learning](https://arxiv.org/abs/2506.17364)
*Alvaro Becerra,Roberto Daza,Ruth Cobos,Aythami Morales,Mutlu Cukurova,Julian Fierrez*

Main category: cs.CY

TL;DR: 本文聚焦计算机在线学习，研究多模态生物识别技术检测智能手机使用导致的注意力分散，提出AI方法，对比不同信号检测准确率，最后讨论模型部署的影响和局限。


<details>
  <summary>Details</summary>
Motivation: 解决学习者在内部、系统和环境因素影响下保持专注的问题，传统学习平台缺乏详细行为数据。

Method: 提出基于AI的方法，利用生理信号和头部姿态数据检测手机使用。

Result: 单一生物特征信号检测准确率有限，头部姿态单独检测准确率达87%，多模态模型结合所有信号准确率达91%。

Conclusion: 讨论了在在线学习环境中部署这些模型以提供实时支持的影响和局限。

Abstract: This work investigates the use of multimodal biometrics to detect
distractions caused by smartphone use during tasks that require sustained
attention, with a focus on computer-based online learning. Although the methods
are applicable to various domains, such as autonomous driving, we concentrate
on the challenges learners face in maintaining engagement amid internal (e.g.,
motivation), system-related (e.g., course design) and contextual (e.g.,
smartphone use) factors. Traditional learning platforms often lack detailed
behavioral data, but Multimodal Learning Analytics (MMLA) and biosensors
provide new insights into learner attention. We propose an AI-based approach
that leverages physiological signals and head pose data to detect phone use.
Our results show that single biometric signals, such as brain waves or heart
rate, offer limited accuracy, while head pose alone achieves 87%. A multimodal
model combining all signals reaches 91% accuracy, highlighting the benefits of
integration. We conclude by discussing the implications and limitations of
deploying these models for real-time support in online learning environments.

</details>


### [493] [AI based Content Creation and Product Recommendation Applications in E-commerce: An Ethical overview](https://arxiv.org/abs/2506.17370)
*Aditi Madhusudan Jain,Ayush Jain*

Main category: cs.CY

TL;DR: 电商中AI内容创作和产品推荐技术有益但有伦理挑战，本文探讨其伦理影响，提出去偏和合规框架以负责任使用AI。


<details>
  <summary>Details</summary>
Motivation: 电商广泛使用AI带来数据隐私、算法偏见和消费者自主性等伦理挑战，需解决这些问题。

Method: 提出消除偏见的最佳实践，如算法审计、数据多样化、引入公平指标；讨论伦理合规框架。

Result: 为电商中AI内容创作和产品推荐的负责任使用提供指导。

Conclusion: 解决伦理问题能确保电商中AI技术既有效又符合伦理。

Abstract: As e-commerce rapidly integrates artificial intelligence for content creation
and product recommendations, these technologies offer significant benefits in
personalization and efficiency. AI-driven systems automate product
descriptions, generate dynamic advertisements, and deliver tailored
recommendations based on consumer behavior, as seen in major platforms like
Amazon and Shopify. However, the widespread use of AI in e-commerce raises
crucial ethical challenges, particularly around data privacy, algorithmic bias,
and consumer autonomy. Bias -- whether cultural, gender-based, or socioeconomic
-- can be inadvertently embedded in AI models, leading to inequitable product
recommendations and reinforcing harmful stereotypes. This paper examines the
ethical implications of AI-driven content creation and product recommendations,
emphasizing the need for frameworks to ensure fairness, transparency, and need
for more established and robust ethical standards. We propose actionable best
practices to remove bias and ensure inclusivity, such as conducting regular
audits of algorithms, diversifying training data, and incorporating fairness
metrics into AI models. Additionally, we discuss frameworks for ethical
conformance that focus on safeguarding consumer data privacy, promoting
transparency in decision-making processes, and enhancing consumer autonomy. By
addressing these issues, we provide guidelines for responsibly utilizing AI in
e-commerce applications for content creation and product recommendations,
ensuring that these technologies are both effective and ethically sound.

</details>


### [494] [Multimodal Political Bias Identification and Neutralization](https://arxiv.org/abs/2506.17372)
*Cedric Bernard,Xavier Pleimling,Amun Kharel,Chase Vickery*

Main category: cs.CY

TL;DR: 本文提出结合文本与图像的去偏模型，分四步进行去偏，初步结果有前景但需更多时间和资源，还提议人工评估。


<details>
  <summary>Details</summary>
Motivation: 政治回声室存在，需检测并去除政治文章文本和图像中的主观偏见与情绪化语言，而此前研究仅关注文本部分。

Method: 提出包含图像文本对齐、图像偏差评分、文本去偏和最终去偏四步的模型，分别利用CLIP、ViT、BERT模型。

Result: 文本去偏策略能识别潜在偏置词和短语，ViT模型训练有效，语义对齐模型高效。

Conclusion: 该方法有前景，但需更多时间和资源以获更好结果，提议人工评估确保语义一致性。

Abstract: Due to the presence of political echo chambers, it becomes imperative to
detect and remove subjective bias and emotionally charged language from both
the text and images of political articles. However, prior work has focused on
solely the text portion of the bias rather than both the text and image
portions. This is a problem because the images are just as powerful of a medium
to communicate information as text is. To that end, we present a model that
leverages both text and image bias which consists of four different steps.
Image Text Alignment focuses on semantically aligning images based on their
bias through CLIP models. Image Bias Scoring determines the appropriate bias
score of images via a ViT classifier. Text De-Biasing focuses on detecting
biased words and phrases and neutralizing them through BERT models. These three
steps all culminate to the final step of debiasing, which replaces the text and
the image with neutralized or reduced counterparts, which for images is done by
comparing the bias scores. The results so far indicate that this approach is
promising, with the text debiasing strategy being able to identify many
potential biased words and phrases, and the ViT model showcasing effective
training. The semantic alignment model also is efficient. However, more time,
particularly in training, and resources are needed to obtain better results. A
human evaluation portion was also proposed to ensure semantic consistency of
the newly generated text and images.

</details>


### [495] [Optimizing Mastery Learning by Fast-Forwarding Over-Practice Steps](https://arxiv.org/abs/2506.17577)
*Meng Xia,Robin Schmucker,Conrad Borchers,Vincent Aleven*

Main category: cs.CY

TL;DR: 提出Fast - Forwarding技术增强现有问题选择算法，可减少三分之一过度练习，其效果受学生面对难题时的动机和参与度影响。


<details>
  <summary>Details</summary>
Motivation: 掌握式学习存在过度练习问题，以往研究多从问题选择算法和练习任务设计入手，缺乏基于步骤级适应性减少过度练习的研究。

Method: 提出Fast - Forwarding技术，基于学习者模型和真实学生数据进行模拟研究。

Result: Fast - Forwarding可减少高达三分之一的过度练习，对优先选择难题的算法效果最佳。

Conclusion: Fast - Forwarding可提高学生练习效率，实际效果受学生面对高难度时的动机和参与度影响。

Abstract: Mastery learning improves learning proficiency and efficiency. However, the
overpractice of skills--students spending time on skills they have already
mastered--remains a fundamental challenge for tutoring systems. Previous
research has reduced overpractice through the development of better problem
selection algorithms and the authoring of focused practice tasks. However, few
efforts have concentrated on reducing overpractice through step-level
adaptivity, which can avoid resource-intensive curriculum redesign. We propose
and evaluate Fast-Forwarding as a technique that enhances existing problem
selection algorithms. Based on simulation studies informed by learner models
and problem-solving pathways derived from real student data, Fast-Forwarding
can reduce overpractice by up to one-third, as it does not require students to
complete problem-solving steps if all remaining pathways are fully mastered.
Fast-Forwarding is a flexible method that enhances any problem selection
algorithm, though its effectiveness is highest for algorithms that
preferentially select difficult problems. Therefore, our findings suggest that
while Fast-Forwarding may improve student practice efficiency, the size of its
practical impact may also depend on students' ability to stay motivated and
engaged at higher levels of difficulty.

</details>


### [496] [Using Machine Learning in Analyzing Air Quality Discrepancies of Environmental Impact](https://arxiv.org/abs/2506.17319)
*Shuangbao Paul Wang,Lucas Yang,Rahouane Chouchane,Jin Guo,Michael Bailey*

Main category: cs.CY

TL;DR: 应用机器学习和软件工程分析巴尔的摩空气污染水平，发现污染与保险评估方法有关，存在种族和收入差异。


<details>
  <summary>Details</summary>
Motivation: 分析巴尔的摩空气污染水平及其影响因素。

Method: 运用机器学习和软件工程，采用三种主要数据源构建数据模型进行分析。

Result: 空气污染水平与有偏差的保险评估方法有明显关联，不同收入街区和不同种族居民间的空气污染水平存在差异。

Conclusion: 几十年前的政策至今仍在歧视并影响巴尔的摩市民生活质量。

Abstract: In this study, we apply machine learning and software engineering in
analyzing air pollution levels in City of Baltimore. The data model was fed
with three primary data sources: 1) a biased method of estimating insurance
risk used by homeowners loan corporation, 2) demographics of Baltimore
residents, and 3) census data estimate of NO2 and PM2.5 concentrations. The
dataset covers 650,643 Baltimore residents in 44.7 million residents in 202
major cities in US. The results show that air pollution levels have a clear
association with the biased insurance estimating method. Great disparities
present in NO2 level between more desirable and low income blocks. Similar
disparities exist in air pollution level between residents' ethnicity. As
Baltimore population consists of a greater proportion of people of color, the
finding reveals how decades old policies has continued to discriminate and
affect quality of life of Baltimore citizens today.

</details>


### [497] [MAARTA:Multi-Agentic Adaptive Radiology Teaching Assistant](https://arxiv.org/abs/2506.17320)
*Akash Awasthi,Brandon V. Chang,Anh M. Vu,Ngan Le,Rishi Agrawal,Zhigang Deng,Carol Wu,Hien Van Nguyen*

Main category: cs.CY

TL;DR: 现有AI系统无法解决放射科学生感知错误问题，MAARTA框架可分析凝视模式和报告提供个性化反馈，促进AI驱动的放射学教育。


<details>
  <summary>Details</summary>
Motivation: 放射科学生因专家指导时间有限，存在感知错误，而现有AI系统无法解决这些问题。

Method: 引入MAARTA多智能体框架，根据错误复杂度动态选择智能体，通过结构化图比较专家和学生凝视行为，分配感知错误教师智能体分析差异，用逐步提示帮助学生。

Result: 系统能识别学生错过的发现。

Conclusion: MAARTA可帮助学生理解错误，改进诊断推理，推动AI驱动的放射学教育发展。

Abstract: Radiology students often struggle to develop perceptual expertise due to
limited expert mentorship time, leading to errors in visual search and
diagnostic interpretation. These perceptual errors, such as missed fixations,
short dwell times, or misinterpretations, are not adequately addressed by
current AI systems, which focus on diagnostic accuracy but fail to explain how
and why errors occur. To address this gap, we introduce MAARTA (Multi-Agentic
Adaptive Radiology Teaching Assistant), a multi-agent framework that analyzes
gaze patterns and radiology reports to provide personalized feedback. Unlike
single-agent models, MAARTA dynamically selects agents based on error
complexity, enabling adaptive and efficient reasoning. By comparing expert and
student gaze behavior through structured graphs, the system identifies missed
findings and assigns Perceptual Error Teacher agents to analyze discrepancies.
MAARTA then uses step-by-step prompting to help students understand their
errors and improve diagnostic reasoning, advancing AI-driven radiology
education.

</details>


### [498] [The Democratic Paradox in Large Language Models' Underestimation of Press Freedom](https://arxiv.org/abs/2506.18045)
*I. Loaiza,R. Vestrelli,A. Fronzetti Colladon,R. Rigobon*

Main category: cs.CY

TL;DR: 研究发现六种流行大语言模型评估180个国家新闻自由时存在与专业评估的偏差，包括低估、差异偏差和本土偏好。


<details>
  <summary>Details</summary>
Motivation: 大语言模型影响全球信息获取，其校准和偏差会影响公众对民主制度的理解和信任，需研究其评估新闻自由的准确性。

Method: 对比六种流行大语言模型对180个国家新闻自由的评估与世界新闻自由指数（WPFI）的专家评估。

Result: 大语言模型存在负面偏差，低估新闻自由；有差异偏差，在新闻自由强的国家低估更严重；五个模型有正向本土偏好，对本国新闻自由评价更高。

Conclusion: 若大语言模型成为重要文化工具，需确保对全球人权和公民权利状况的准确呈现。

Abstract: As Large Language Models (LLMs) increasingly mediate global information
access for millions of users worldwide, their alignment and biases have the
potential to shape public understanding and trust in fundamental democratic
institutions, such as press freedom. In this study, we uncover three systematic
distortions in the way six popular LLMs evaluate press freedom in 180 countries
compared to expert assessments of the World Press Freedom Index (WPFI). The six
LLMs exhibit a negative misalignment, consistently underestimating press
freedom, with individual models rating between 71% to 93% of countries as less
free. We also identify a paradoxical pattern we term differential misalignment:
LLMs disproportionately underestimate press freedom in countries where it is
strongest. Additionally, five of the six LLMs exhibit positive home bias,
rating their home countries' press freedoms more favorably than would be
expected given their negative misalignment with the human benchmark. In some
cases, LLMs rate their home countries between 7% to 260% more positively than
expected. If LLMs are set to become the next search engines and some of the
most important cultural tools of our time, they must ensure accurate
representations of the state of our human and civic rights globally.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [499] [Perceptual Rationality: An Evolutionary Game Theory of Perceptually Rational Decision-Making](https://arxiv.org/abs/2506.17724)
*Mohammad Salahshour*

Main category: q-bio.PE

TL;DR: 提出感知理性框架解释群体合作决策，揭示理性决策与行为多样性等的关系，框架与生态进化模式相符，指出多样性对公共利益的影响。


<details>
  <summary>Details</summary>
Motivation: 在进化博弈论中，以往通过有限理性理解生物决策行为，本文旨在提出新框架。

Method: 提出感知理性框架，结合公共物品博弈，纳入社会信息使用的进化。

Result: 简单公共物品博弈解释感知多样性幂律分布，理性决策是个性差异和行为多样性进化根源，会导致多态性或循环动态、非单调进化，框架与生态进化模式相符，多样性常不利于公共利益。

Conclusion: 感知理性框架有助于理解群体合作互动中的决策行为及生态进化模式，且强调多样性在公共利益提供中的不利影响。

Abstract: Understanding how biological organisms make decisions is of fundamental
importance in understanding behavior. Such an understanding within evolutionary
game theory so far has been sought by appealing to bounded rationality. Here,
we present a perceptual rationality framework in the context of group
cooperative interactions, where individuals make rational decisions based on
their evolvable perception of the environment. We show that a simple public
goods game accounts for power law distributed perceptual diversity.
Incorporating the evolution of social information use into the framework
reveals that rational decision-making is a natural root of the evolution of
consistent personality differences and power-law distributed behavioral
diversity. The behavioral diversity, core to the perceptual rationality
approach, can lead to ever-shifting polymorphism or cyclic dynamics, through
which different rational personality types coexist and engage in mutualistic,
complementary, or competitive and exploitative relationships. This polymorphism
can lead to non-monotonic evolution as external environmental conditions
change. The framework provides predictions consistent with some large-scale
eco-evolutionary patterns and illustrates how the evolution of social structure
can modify large-scale eco-evolutionary patterns. Furthermore, consistent with
most empirical evidence and in contrast to most theoretical predictions, our
work suggests diversity is often detrimental to public good provision,
especially in strong social dilemmas.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [500] [Perfect phylogenies via the Minimum Uncovering Branching problem: efficiently solvable cases](https://arxiv.org/abs/2506.18578)
*Narmina Baghirova,Esther Galby,Martin Milanič*

Main category: cs.DM

TL;DR: 本文提出最小未覆盖分支问题的新可有效求解情况，证明有界宽度实例可多项式时间求解，还引入新下界并确定另一可多项式时间求解条件。


<details>
  <summary>Details</summary>
Motivation: Hujdurović等人留下有界宽度实例的精确复杂度问题未解决，本文旨在回答此问题。

Method: 通过研究最优解的结构特性，将问题转化为计算二部图最大匹配和偏序集最大权重反链。

Result: 证明问题在有界宽度实例下可多项式时间求解，引入新的多项式可计算下界。

Conclusion: 解决了有界宽度实例的精确复杂度问题，为该问题提供新的求解思路和条件。

Abstract: In this paper, we present new efficiently solvable cases of the Minimum
Uncovering Branching problem, an optimization problem with applications in
cancer genomics introduced by Hujdurovi\'c, Husi\'c, Milani\v{c}, Rizzi, and
Tomescu in 2018. The problem involves a family of finite sets, and the goal is
to map each non-maximal set to exactly one set that contains it, minimizing the
sum of uncovered elements across all sets in the family. Hujdurovi\'c et al.
formulated the problem in terms of branchings of the digraph formed by the
proper set inclusion relation on the input sets and studied the problem
complexity based on properties of the corresponding partially ordered set, in
particular, with respect to its height and width, defined respectively as the
maximum cardinality of a chain and an antichain. They showed that the problem
is APX-complete for instances of bounded height and that a constant-factor
approximation algorithm exists for instances of bounded width, but left the
exact complexity for bounded-width instances open. In this paper, we answer
this question by proving that the problem is solvable in polynomial time. We
derive this result by examining the structural properties of optimal solutions
and reducing the problem to computing maximum matchings in bipartite graphs and
maximum weight antichains in partially ordered sets. We also introduce a new
polynomially computable lower bound and identify another condition for
polynomial-time solvability.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [501] [Advanced Modeling for Exoplanet Detection and Characterization](https://arxiv.org/abs/2506.17665)
*Krishna Chamarthy*

Main category: astro-ph.EP

TL;DR: 研究利用开普勒数据集的恒星光变曲线结合机器学习方法发现系外行星并推导其物理特征，还进行恒星分类，提高搜索效率。


<details>
  <summary>Details</summary>
Motivation: 恒星光变曲线研究改变了系外行星的发现和特征描述方式，需利用其发现系外行星并获取相关特征。

Method: 分析开普勒数据集恒星光变曲线，寻找亮度周期性下降；应用已有方法推导行星关键参数；采用机器学习对恒星进行分类。

Result: 未提及具体结果，但预计可发现系外行星，获取行星距离、轨道周期、半径等参数。

Conclusion: 该方法能提高寻找系外行星的效率，快速搜索海量天文数据集。

Abstract: Research into light curves from stars (temporal variation of brightness) has
completely changed how exoplanets are discovered or characterised. This study
including star light curves from the Kepler dataset as a way to discover
exoplanets (planetary transits) and derive some estimate of their physical
characteristics by the light curve and machine learning methods. The dataset
consists of measured flux (recordings) for many individual stars and we will
examine the light curve of each star and look for periodic dips in brightness
due to an astronomical body making a transit. We will apply variables derived
from an established method for deriving measurements from light curve data to
derive key parameters related to the planet we observed during the transit,
such as distance to the host star, orbital period, radius. The orbital period
will typically be measured based on the time between transit of the subsequent
timelines and the radius will be measured based on the depth of transit. The
density of the star and planet can also be estimated from the transit event, as
well as very limited information on the albedo (reflectivity) and atmosphere of
the planet based on transmission spectroscopy and/or the analysis of phase
curve for levels of flux. In addition to these methods, we will employ some
machine learning classification of the stars (i.e. likely have an exoplanet or
likely do not have an exoplanet) based on flux change. This could help fulfil
both the process of looking for exoplanets more efficient as well as providing
important parameters for the planet. This will provide a much quicker means of
searching the vast astronomical datasets for the likelihood of exoplanets.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [502] [Advanced Game-Theoretic Frameworks for Multi-Agent AI Challenges: A 2025 Outlook](https://arxiv.org/abs/2506.17348)
*Pavel Malinovskiy*

Main category: cs.MA

TL;DR: 本文探讨高级博弈论范式为2025年左右AI挑战奠基，纳入新元素并给出数学形式等，为研究者提供理论工具。


<details>
  <summary>Details</summary>
Motivation: 研究高级博弈论范式如何为下一代AI挑战奠定基础。

Method: 纳入动态联盟形成、基于语言的效用等元素，提供数学形式、模拟和编码方案，运用重复博弈、贝叶斯更新等。

Result: 展示了多智能体AI系统在复杂环境中如何适应和协商。

Conclusion: 为AI研究者提供在不确定、部分对抗环境中进行战略交互的强大理论工具。

Abstract: This paper presents a substantially reworked examination of how advanced
game-theoretic paradigms can serve as a foundation for the next-generation
challenges in Artificial Intelligence (AI), forecasted to arrive in or around
2025. Our focus extends beyond traditional models by incorporating dynamic
coalition formation, language-based utilities, sabotage risks, and partial
observability. We provide a set of mathematical formalisms, simulations, and
coding schemes that illustrate how multi-agent AI systems may adapt and
negotiate in complex environments. Key elements include repeated games,
Bayesian updates for adversarial detection, and moral framing within payoff
structures. This work aims to equip AI researchers with robust theoretical
tools for aligning strategic interaction in uncertain, partially adversarial
contexts.

</details>


### [503] [Towards Zero-Shot Coordination between Teams of Agents: The N-XPlay Framework](https://arxiv.org/abs/2506.17560)
*Ava Abderezaei,Chi-Hui Lin,Joseph Miceli,Naren Sivagnanadasan,Stéphane Aroca-Ouellette,Jake Brawer,Alessandro Roncone*

Main category: cs.MA

TL;DR: 提出N - player Overcooked和N - XPlay方法用于多团队系统零样本协调评估，实验表明N - XPlay训练的智能体在团队内外协调上表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有零样本协调方法未反映现实世界多智能体系统复杂性，需要新方法评估多团队系统中的零样本协调能力。

Method: 引入N - player Overcooked用于N智能体场景零样本协调评估，提出N - XPlay用于N智能体、多团队环境下的零样本协调。

Result: 在二、三、五玩家的Overcooked场景中，与Self - Play相比，N - XPlay训练的智能体更能同时平衡团队内和团队间的协调。

Conclusion: N - XPlay方法在多团队系统零样本协调方面优于Self - Play方法。

Abstract: Zero-shot coordination (ZSC) -- the ability to collaborate with unfamiliar
partners -- is essential to making autonomous agents effective teammates.
Existing ZSC methods evaluate coordination capabilities between two agents who
have not previously interacted. However, these scenarios do not reflect the
complexity of real-world multi-agent systems, where coordination often involves
a hierarchy of sub-groups and interactions between teams of agents, known as
Multi-Team Systems (MTS). To address this gap, we first introduce N-player
Overcooked, an N-agent extension of the popular two-agent ZSC benchmark,
enabling evaluation of ZSC in N-agent scenarios. We then propose N-XPlay for
ZSC in N-agent, multi-team settings. Comparison against Self-Play across two-,
three- and five-player Overcooked scenarios, where agents are split between an
``ego-team'' and a group of unseen collaborators shows that agents trained with
N-XPlay are better able to simultaneously balance ``intra-team'' and
``inter-team'' coordination than agents trained with SP.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [504] [Reply to "Emergent LLM behaviors are observationally equivalent to data leakage"](https://arxiv.org/abs/2506.18600)
*Ariel Flint Ashery,Luca Maria Aiello,Andrea Baronchelli*

Main category: cs.CL

TL;DR: 讨论大语言模型群体模拟中的数据污染问题，指出不影响研究真正的涌现动力学，借他人批评澄清相关研究可行性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型群体模拟中数据污染问题对实验的影响，以及澄清相关研究的可行性。

Method: 无明确提及

Result: 说明数据污染虽重要但不排除研究大语言模型群体中真正的涌现动力学，且以社会习俗为例说明此类动力学已被实证观察到。

Conclusion: 可以在大语言模型群体中研究自组织和依赖模型的涌现动力学。

Abstract: A potential concern when simulating populations of large language models
(LLMs) is data contamination, i.e. the possibility that training data may shape
outcomes in unintended ways. While this concern is important and may hinder
certain experiments with multi-agent models, it does not preclude the study of
genuinely emergent dynamics in LLM populations. The recent critique by Barrie
and T\"ornberg [1] of the results of Flint Ashery et al. [2] offers an
opportunity to clarify that self-organisation and model-dependent emergent
dynamics can be studied in LLM populations, highlighting how such dynamics have
been empirically observed in the specific case of social conventions.

</details>


### [505] [UProp: Investigating the Uncertainty Propagation of LLMs in Multi-Step Agentic Decision-Making](https://arxiv.org/abs/2506.17419)
*Jinhao Duan,James Diffenderfer,Sandeep Madireddy,Tianlong Chen,Bhavya Kailkhura,Kaidi Xu*

Main category: cs.CL

TL;DR: 论文针对大语言模型多步决策场景中不确定性量化问题，提出UProp估计器，经实验验证其优于现有单轮UQ基线。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于安全关键的现实顺序决策应用时，需知道何时信任其决策，现有UQ方法多针对单轮问答，多步决策场景研究不足。

Method: 引入信息论框架将LLM顺序决策不确定性分解为内部和外部不确定性，提出UProp将互信息直接估计转换为多轨迹依赖决策过程上的点互信息估计。

Result: 在多步决策基准测试中，UProp显著优于配备深思熟虑聚合策略的现有单轮UQ基线。

Conclusion: UProp有效，论文还对其采样效率、潜在应用和中间不确定性传播进行了全面分析。

Abstract: As Large Language Models (LLMs) are integrated into safety-critical
applications involving sequential decision-making in the real world, it is
essential to know when to trust LLM decisions. Existing LLM Uncertainty
Quantification (UQ) methods are primarily designed for single-turn
question-answering formats, resulting in multi-step decision-making scenarios,
e.g., LLM agentic system, being underexplored. In this paper, we introduce a
principled, information-theoretic framework that decomposes LLM sequential
decision uncertainty into two parts: (i) internal uncertainty intrinsic to the
current decision, which is focused on existing UQ methods, and (ii) extrinsic
uncertainty, a Mutual-Information (MI) quantity describing how much uncertainty
should be inherited from preceding decisions. We then propose UProp, an
efficient and effective extrinsic uncertainty estimator that converts the
direct estimation of MI to the estimation of Pointwise Mutual Information (PMI)
over multiple Trajectory-Dependent Decision Processes (TDPs). UProp is
evaluated over extensive multi-step decision-making benchmarks, e.g.,
AgentBench and HotpotQA, with state-of-the-art LLMs, e.g., GPT-4.1 and
DeepSeek-V3. Experimental results demonstrate that UProp significantly
outperforms existing single-turn UQ baselines equipped with thoughtful
aggregation strategies. Moreover, we provide a comprehensive analysis of UProp,
including sampling efficiency, potential applications, and intermediate
uncertainty propagation, to demonstrate its effectiveness. Codes will be
available at https://github.com/jinhaoduan/UProp.

</details>


### [506] [When Fine-Tuning Fails: Lessons from MS MARCO Passage Ranking](https://arxiv.org/abs/2506.18535)
*Manu Pande,Shahil Kumar,Anay Yatin Damle*

Main category: cs.CL

TL;DR: 研究发现微调预训练变压器模型在MS MARCO段落排名任务上性能下降，挑战传统迁移学习观点。


<details>
  <summary>Details</summary>
Motivation: 探究微调预训练变压器模型在MS MARCO段落排名任务上性能下降这一反直觉现象。

Method: 进行包含五种模型变体（全参数微调、参数高效LoRA适配等）的综合实验，结合UMAP可视化、训练动态分析和计算效率指标分析。

Result: 所有微调方法的性能均不如基础模型，微调破坏了基础模型预训练学习到的最优嵌入空间结构。

Conclusion: 挑战传统迁移学习在饱和基准上的有效性观点，认为需要架构创新来实现有意义的改进。

Abstract: This paper investigates the counterintuitive phenomenon where fine-tuning
pre-trained transformer models degrades performance on the MS MARCO passage
ranking task. Through comprehensive experiments involving five model
variants-including full parameter fine-tuning and parameter efficient LoRA
adaptations-we demonstrate that all fine-tuning approaches underperform the
base sentence-transformers/all- MiniLM-L6-v2 model (MRR@10: 0.3026). Our
analysis reveals that fine-tuning disrupts the optimal embedding space
structure learned during the base model's extensive pre-training on 1 billion
sentence pairs, including 9.1 million MS MARCO samples. UMAP visualizations
show progressive embedding space flattening, while training dynamics analysis
and computational efficiency metrics further support our findings. These
results challenge conventional wisdom about transfer learning effectiveness on
saturated benchmarks and suggest architectural innovations may be necessary for
meaningful improvements.

</details>


### [507] [GTA: Grouped-head latenT Attention](https://arxiv.org/abs/2506.17286)
*Luoyang Sun,Jiwen Jiang,Cheng Deng,Xinjian Wu,Haifeng Zhang,Lei Chen,Lionel Ni,Jun Wang*

Main category: cs.CL

TL;DR: 本文指出注意力机制存在冗余，提出GTA机制，减少内存使用和计算复杂度，提升LLM部署效率及推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有注意力机制计算和内存开销大，在有限资源硬件上部署困难。

Method: 提出GTA机制，包含共享注意力图机制和非线性值解码器，分别减少键缓存大小和压缩值缓存。

Result: 与Grouped - Query Attention相比，GTA最多削减62.5%注意力计算FLOPs，KV缓存最多缩小70%，端到端推理速度提升2倍。

Conclusion: GTA机制能在保持性能的同时，减少内存使用和计算复杂度，提高LLM部署效率。

Abstract: Attention mechanisms underpin the success of large language models (LLMs),
yet their substantial computational and memory overhead poses challenges for
optimizing efficiency and performance. A critical bottleneck arises as KV cache
and attention computations scale rapidly with text length, challenging
deployment on hardware with limited computational and memory resources. We
observe that attention mechanisms exhibit substantial redundancy, since the KV
cache can be significantly compressed and attention maps across heads display
high similarity, revealing that much of the computation and storage is
unnecessary. Leveraging these insights, we propose \textbf{G}rouped-Head
Laten\textbf{T} \textbf{A}ttention (GTA), a novel attention mechanism that
reduces memory usage and computational complexity while maintaining
performance. GTA comprises two components: (1) a shared attention map mechanism
that reuses attention scores across multiple heads, decreasing the key cache
size; and (2) a nonlinear value decoder with learned projections that
compresses the value cache into a latent space, further cutting memory needs.
GTA cuts attention computation FLOPs by up to \emph{62.5\%} versus
Grouped-Query Attention and shrink the KV cache by up to \emph{70\%}, all while
avoiding the extra overhead of Multi-Head Latent Attention to improve LLM
deployment efficiency. Consequently, GTA models achieve a \emph{2x} increase in
end-to-end inference speed, with prefill benefiting from reduced computational
cost and decoding benefiting from the smaller cache footprint.

</details>


### [508] [AI-Generated Game Commentary: A Survey and a Datasheet Repository](https://arxiv.org/abs/2506.17294)
*Qirui Zheng,Xingbo Wang,Keyuan Cheng,Yunlong Lu,Wenxin Li*

Main category: cs.CL

TL;DR: 本文介绍AIGGC通用框架，对45个现有游戏解说数据集和方法进行综述，分类比较常用评估指标，并提供结构化数据表。


<details>
  <summary>Details</summary>
Motivation: AIGGC有市场潜力和技术挑战，作为综合多模态NLP任务对语言模型有较高要求。

Method: 引入通用框架，根据关键挑战对现有数据集和方法进行综述，分类比较评估指标。

Result: 完成对45个数据集和方法的综述及评估指标比较，提供结构化数据表。

Conclusion: 研究成果可支持未来研究和基准测试。

Abstract: AI-Generated Game Commentary (AIGGC) has gained increasing attention due to
its market potential and inherent technical challenges. As a comprehensive
multimodal Natural Language Processing (NLP) task, AIGGC imposes substantial
demands on language models, including factual accuracy, logical reasoning,
expressive text generation, generation speed, and context management. In this
paper, we introduce a general framework for AIGGC and present a comprehensive
survey of 45 existing game commentary dataset and methods according to key
challenges they aim to address in this domain. We further classify and compare
various evaluation metrics commonly used in this domain. To support future
research and benchmarking, we also provide a structured datasheet summarizing
the essential attributes of these datasets in appendix, which is meanwhile
publicly available in an open repository.

</details>


### [509] [Semantic uncertainty in advanced decoding methods for LLM generation](https://arxiv.org/abs/2506.17296)
*Darius Foodeei,Simin Fan,Martin Jaggi*

Main category: cs.CL

TL;DR: 研究大语言模型不同解码方法输出的语义不确定性，发现结构化解码可兼顾多样性与准确性。


<details>
  <summary>Details</summary>
Motivation: 探究不同解码方法对大语言模型输出多样性和可靠性的影响。

Method: 在问答、总结和代码生成任务上进行实验，分析不同解码策略。

Result: CoT解码语义多样性高且预测熵低，代码生成Pass@2率提升48.8%；总结任务中投机采样有效，ROUGE分数高且语义多样性适中。

Conclusion: 结构化解码方法可在增加语义探索时维持或提高输出质量，对实际应用有重要意义。

Abstract: This study investigates semantic uncertainty in large language model (LLM)
outputs across different decoding methods, focusing on emerging techniques like
speculative sampling and chain-of-thought (CoT) decoding. Through experiments
on question answering, summarization, and code generation tasks, we analyze how
different decoding strategies affect both the diversity and reliability of
model outputs. Our findings reveal that while CoT decoding demonstrates higher
semantic diversity, it maintains lower predictive entropy, suggesting that
structured exploration can lead to more confident and accurate outputs. This is
evidenced by a 48.8% improvement in code generation Pass@2 rates, despite lower
alignment with reference solutions. For summarization tasks, speculative
sampling proved particularly effective, achieving superior ROUGE scores while
maintaining moderate semantic diversity. Our results challenge conventional
assumptions about trade-offs between diversity and accuracy in language model
outputs, demonstrating that properly structured decoding methods can increase
semantic exploration while maintaining or improving output quality. These
findings have significant implications for deploying language models in
practical applications where both reliability and diverse solution generation
are crucial.

</details>


### [510] [Mercury: Ultra-Fast Language Models Based on Diffusion](https://arxiv.org/abs/2506.17298)
*Inception Labs,Samar Khanna,Siddhant Kharbanda,Shufan Li,Harshit Varma,Eric Wang,Sawyer Birnbaum,Ziyang Luo,Yanis Miraoui,Akash Palrecha,Stefano Ermon,Aditya Grover,Volodymyr Kuleshov*

Main category: cs.CL

TL;DR: 介绍基于扩散的新一代商业规模大语言模型Mercury，详述其用于编码的模型Mercury Coder，展示其速度和质量优势，还发布了公共API和免费试用平台。


<details>
  <summary>Details</summary>
Motivation: 开发具有高速度和高质量的商业规模大语言模型用于编码应用。

Method: 采用Transformer架构参数化，训练并行预测多个令牌。

Result: Mercury Coder Mini和Small在NVIDIA H100 GPU上实现了1109和737 tokens/sec的吞吐量，平均比速度优化的前沿模型快10倍，在多种代码基准测试中有良好表现，在Copilot Arena中质量排名第二且速度最快。

Conclusion: Mercury Coder在速度 - 质量方面达到了新的先进水平，可用于编码应用。

Abstract: We present Mercury, a new generation of commercial-scale large language
models (LLMs) based on diffusion. These models are parameterized via the
Transformer architecture and trained to predict multiple tokens in parallel. In
this report, we detail Mercury Coder, our first set of diffusion LLMs designed
for coding applications. Currently, Mercury Coder comes in two sizes: Mini and
Small. These models set a new state-of-the-art on the speed-quality frontier.
Based on independent evaluations conducted by Artificial Analysis, Mercury
Coder Mini and Mercury Coder Small achieve state-of-the-art throughputs of 1109
tokens/sec and 737 tokens/sec, respectively, on NVIDIA H100 GPUs and outperform
speed-optimized frontier models by up to 10x on average while maintaining
comparable quality. We discuss additional results on a variety of code
benchmarks spanning multiple languages and use-cases as well as real-world
validation by developers on Copilot Arena, where the model currently ranks
second on quality and is the fastest model overall. We also release a public
API at https://platform.inceptionlabs.ai/ and free playground at
https://chat.inceptionlabs.ai

</details>


### [511] [Towards Safety Evaluations of Theory of Mind in Large Language Models](https://arxiv.org/abs/2506.17352)
*Tatsuhiro Aoshima,Mitsuaki Akiyama*

Main category: cs.CL

TL;DR: 随着大语言模型能力提升，安全评估重要性凸显，研究提出测量其心智理论能力，发现阅读理解有提升但心智理论能力未相应发展，还呈现评估现状并讨论挑战。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型欺骗行为对开发者或用户的潜在风险，探究其行为是否源于模型内部隐秘、有意的过程。

Method: 回顾心智理论的现有研究，确定其在安全评估中的相关视角和任务，分析一系列开放权重的大语言模型的发展趋势。

Result: 大语言模型在阅读理解方面有改善，但心智理论能力未呈现可比的发展。

Conclusion: 呈现了大语言模型心智理论安全评估的现状，并讨论了未来工作的剩余挑战。

Abstract: As the capabilities of large language models (LLMs) continue to advance, the
importance of rigorous safety evaluation is becoming increasingly evident.
Recent concerns within the realm of safety assessment have highlighted
instances in which LLMs exhibit behaviors that appear to disable oversight
mechanisms and respond in a deceptive manner. For example, there have been
reports suggesting that, when confronted with information unfavorable to their
own persistence during task execution, LLMs may act covertly and even provide
false answers to questions intended to verify their behavior.To evaluate the
potential risk of such deceptive actions toward developers or users, it is
essential to investigate whether these behaviors stem from covert, intentional
processes within the model. In this study, we propose that it is necessary to
measure the theory of mind capabilities of LLMs. We begin by reviewing existing
research on theory of mind and identifying the perspectives and tasks relevant
to its application in safety evaluation. Given that theory of mind has been
predominantly studied within the context of developmental psychology, we
analyze developmental trends across a series of open-weight LLMs. Our results
indicate that while LLMs have improved in reading comprehension, their theory
of mind capabilities have not shown comparable development. Finally, we present
the current state of safety evaluation with respect to LLMs' theory of mind,
and discuss remaining challenges for future work.

</details>


### [512] [Cash or Comfort? How LLMs Value Your Inconvenience](https://arxiv.org/abs/2506.17367)
*Mateusz Cedro,Timour Ichmoukhamedov,Sofie Goethals,Yifan He,James Hinns,David Martens*

Main category: cs.CL

TL;DR: 研究量化多个大语言模型对用户不适定价，发现其用作决策助手存在诸多问题，强调需审查其对人类不便的估值。


<details>
  <summary>Details</summary>
Motivation: 此前研究未充分探索大语言模型在财务奖励与用户舒适度冲突场景中的行为，本文旨在解决该问题。

Method: 量化多个大语言模型对一系列用户不适（额外步行、等待、饥饿和疼痛）所赋予的价格。

Result: 发现大语言模型存在响应差异大、对提示措辞敏感、接受不合理低回报、拒绝无不适的金钱收益等问题。

Conclusion: 强调需要审查大语言模型对人类不便的估值，特别是在代表用户进行现金与舒适度权衡的应用中。

Abstract: Large Language Models (LLMs) are increasingly proposed as near-autonomous
artificial intelligence (AI) agents capable of making everyday decisions on
behalf of humans. Although LLMs perform well on many technical tasks, their
behaviour in personal decision-making remains less understood. Previous studies
have assessed their rationality and moral alignment with human decisions.
However, the behaviour of AI assistants in scenarios where financial rewards
are at odds with user comfort has not yet been thoroughly explored. In this
paper, we tackle this problem by quantifying the prices assigned by multiple
LLMs to a series of user discomforts: additional walking, waiting, hunger and
pain. We uncover several key concerns that strongly question the prospect of
using current LLMs as decision-making assistants: (1) a large variance in
responses between LLMs, (2) within a single LLM, responses show fragility to
minor variations in prompt phrasing (e.g., reformulating the question in the
first person can considerably alter the decision), (3) LLMs can accept
unreasonably low rewards for major inconveniences (e.g., 1 Euro to wait 10
hours), and (4) LLMs can reject monetary gains where no discomfort is imposed
(e.g., 1,000 Euro to wait 0 minutes). These findings emphasize the need for
scrutiny of how LLMs value human inconvenience, particularly as we move toward
applications where such cash-versus-comfort trade-offs are made on users'
behalf.

</details>


### [513] [Computational Approaches to Understanding Large Language Model Impact on Writing and Information Ecosystems](https://arxiv.org/abs/2506.17467)
*Weixin Liang*

Main category: cs.CL

TL;DR: 论文从三方面研究个人和机构对大语言模型的适应与参与情况。


<details>
  <summary>Details</summary>
Motivation: 大语言模型有改变人们写作、交流和创造方式的潜力，需研究个人和机构如何适应和参与该新兴技术。

Method: 一是分析机构采用AI检测器的情况；二是提出新的算法方法衡量不同写作领域对大语言模型的采用情况；三是通过大规模实证分析研究大语言模型为研究手稿提供反馈的能力。

Result: 发现机构采用AI检测器存在系统性偏差；揭示了不同写作领域中AI辅助内容的模式；了解到大语言模型能为研究手稿提供反馈。

Conclusion: 研究揭示了AI治理中的公平问题，为大语言模型在不同写作领域的应用提供了模式和见解，也体现了其对研究人员的支持潜力。

Abstract: Large language models (LLMs) have shown significant potential to change how
we write, communicate, and create, leading to rapid adoption across society.
This dissertation examines how individuals and institutions are adapting to and
engaging with this emerging technology through three research directions.
First, I demonstrate how the institutional adoption of AI detectors introduces
systematic biases, particularly disadvantaging writers of non-dominant language
varieties, highlighting critical equity concerns in AI governance. Second, I
present novel population-level algorithmic approaches that measure the
increasing adoption of LLMs across writing domains, revealing consistent
patterns of AI-assisted content in academic peer reviews, scientific
publications, consumer complaints, corporate communications, job postings, and
international organization press releases. Finally, I investigate LLMs'
capability to provide feedback on research manuscripts through a large-scale
empirical analysis, offering insights into their potential to support
researchers who face barriers in accessing timely manuscript feedback,
particularly early-career researchers and those from under-resourced settings.

</details>


### [514] [Data Quality Issues in Multilingual Speech Datasets: The Need for Sociolinguistic Awareness and Proactive Language Planning](https://arxiv.org/abs/2506.17525)
*Mingfei Lau,Qian Chen,Yeming Fang,Tingting Xu,Tongzhou Chen,Pavel Golik*

Main category: cs.CL

TL;DR: 对三个常用多语言语音数据集质量审计，发现部分语言有质量问题，分两类并分析，提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 解决数据集质量问题，使数据集更适用于训练和评估，提升下游模型性能。

Method: 对三个数据集进行质量审计，将问题分为微观和宏观两类，以台湾闽南语为例分析。

Result: 部分语言数据集存在显著质量问题，宏观问题在资源少的语言中更普遍。

Conclusion: 提出未来数据集开发的指南和建议，强调社会语言学意识对创建可靠语音数据资源的重要性。

Abstract: Our quality audit for three widely used public multilingual speech datasets -
Mozilla Common Voice 17.0, FLEURS, and VoxPopuli - shows that in some
languages, these datasets suffer from significant quality issues. We believe
addressing these issues will make these datasets more useful as training and
evaluation sets, and improve downstream models. We divide these quality issues
into two categories: micro-level and macro-level. We find that macro-level
issues are more prevalent in less institutionalized, often under-resourced
languages. We provide a case analysis of Taiwanese Southern Min (nan_tw) that
highlights the need for proactive language planning (e.g. orthography
prescriptions, dialect boundary definition) and enhanced data quality control
in the process of Automatic Speech Recognition (ASR) dataset creation. We
conclude by proposing guidelines and recommendations to mitigate these issues
in future dataset development, emphasizing the importance of sociolinguistic
awareness in creating robust and reliable speech data resources.

</details>


### [515] [TPTT: Transforming Pretrained Transformer into Titans](https://arxiv.org/abs/2506.17671)
*Fabien Furfaro*

Main category: cs.CL

TL;DR: 提出TPTT框架提升预训练Transformer模型，在MMLU基准测试展现有效性，代码和包公开。


<details>
  <summary>Details</summary>
Motivation: 大语言模型计算和内存需求大，尤其是长上下文推理，需高效方法。

Method: 引入TPTT框架，采用Memory as Gate和混合线性化注意力等技术，结合Hugging Face库和参数高效微调。

Result: 在约10亿参数模型的MMLU基准测试中，效率和准确性显著提升，如Titans - Llama - 3.2 - 1B的精确匹配率提高20%。

Conclusion: 统计分析和对比表明TPTT具有实际可扩展性和鲁棒性。

Abstract: Recent advances in large language models (LLMs) have led to remarkable
progress in natural language processing, but their computational and memory
demands remain a significant challenge, particularly for long-context
inference. We introduce TPTT (Transforming Pretrained Transformer into Titans),
a novel framework for enhancing pretrained Transformer models with efficient
linearized attention mechanisms and advanced memory management. TPTT employs
techniques such as Memory as Gate (MaG) and mixed linearized attention (LiZA).
It is fully compatible with the Hugging Face Transformers library, enabling
seamless adaptation of any causal LLM through parameter-efficient fine-tuning
(LoRA) without full retraining. We show the effectiveness of TPTT on the MMLU
benchmark with models of approximately 1 billion parameters, observing
substantial improvements in both efficiency and accuracy. For instance,
Titans-Llama-3.2-1B achieves a 20% increase in Exact Match (EM) over its
baseline. Statistical analyses and comparisons with recent state-of-the-art
methods confirm the practical scalability and robustness of TPTT. Code is
available at https://github.com/fabienfrfr/tptt . Python package at
https://pypi.org/project/tptt/ .

</details>


### [516] [The Evolution of Natural Language Processing: How Prompt Optimization and Language Models are Shaping the Future](https://arxiv.org/abs/2506.17700)
*Summra Saleem,Muhammad Nabeel Asim,Shaista Zulfiqar,Andreas Dengel*

Main category: cs.CL

TL;DR: 本文针对大语言模型提示优化策略进行全面分析与分类，为未来比较研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对提示优化策略的全面分析，本文旨在填补这一空白。

Method: 分析提示优化策略的工作范式，并将其分为11个不同类别，介绍其在NLP任务中的应用及评估使用的数据集。

Result: 对提示优化策略进行全面分类，提供不同NLP任务、大语言模型和基准数据集的详细信息。

Conclusion: 本研究集中了多样的策略知识，有助于在未探索任务中开发创新预测器。

Abstract: Large Language Models (LLMs) have revolutionized the field of Natural
Language Processing (NLP) by automating traditional labor-intensive tasks and
consequently accelerated the development of computer-aided applications. As
researchers continue to advance this field with the introduction of novel
language models and more efficient training/finetuning methodologies, the idea
of prompt engineering and subsequent optimization strategies with LLMs has
emerged as a particularly impactful trend to yield a substantial performance
boost across diverse NLP tasks. To best of our knowledge numerous review
articles have explored prompt engineering, however, a critical gap exists in
comprehensive analyses of prompt optimization strategies. To bridge this gap
this paper provides unique and comprehensive insights about the potential of
diverse prompt optimization strategies. It analyzes their underlying working
paradigms and based on these principles, categorizes them into 11 distinct
classes. Moreover, the paper provides details about various NLP tasks where
these prompt optimization strategies have been employed, along with details of
different LLMs and benchmark datasets used for evaluation. This comprehensive
compilation lays a robust foundation for future comparative studies and enables
rigorous assessment of prompt optimization and LLM-based predictive pipelines
under consistent experimental settings: a critical need in the current
landscape. Ultimately, this research will centralize diverse strategic
knowledge to facilitate the adaptation of existing prompt optimization
strategies for development of innovative predictors across unexplored tasks.

</details>


### [517] [Aged to Perfection: Machine-Learning Maps of Age in Conversational English](https://arxiv.org/abs/2506.17708)
*MingZe Tang*

Main category: cs.CL

TL;DR: 研究利用英国国家语料库2014探究不同年龄组语言模式，结合计算语言分析和机器学习，为现代英国语的社会语言学多样性研究做贡献。


<details>
  <summary>Details</summary>
Motivation: 探索不同年龄组语言模式差异，研究说话者人口统计学特征与语言因素的联系。

Method: 使用英国国家语料库2014，结合计算语言分析和机器学习方法。

Result: 文中未明确提及具体结果。

Conclusion: 该研究有助于了解现代英国语整个生命周期的社会语言学多样性。

Abstract: The study uses the British National Corpus 2014, a large sample of
contemporary spoken British English, to investigate language patterns across
different age groups. Our research attempts to explore how language patterns
vary between different age groups, exploring the connection between speaker
demographics and linguistic factors such as utterance duration, lexical
diversity, and word choice. By merging computational language analysis and
machine learning methodologies, we attempt to uncover distinctive linguistic
markers characteristic of multiple generations and create prediction models
that can consistently estimate the speaker's age group from various aspects.
This work contributes to our knowledge of sociolinguistic diversity throughout
the life of modern British speech.

</details>


### [518] [KAG-Thinker: Teaching Large Language Models to Think with Human-like Reasoning Process](https://arxiv.org/abs/2506.17728)
*Dalong Zhang,Jun Xu,Jun Zhou,Lei Liang,Lin Yuan,Ling Zhong,Mengshu Sun,Peilong Zhao,QiWei Wang,Xiaorui Wang,Xinkai Du,YangYang Hou,Yu Ao,ZhaoYang Wang,Zhengke Gui,ZhiYing Yi,Zhongpu Bo*

Main category: cs.CL

TL;DR: 介绍基于轻参数大语言模型的类人推理框架KAG - Thinker，增强问答任务逻辑与上下文一致性。


<details>
  <summary>Details</summary>
Motivation: 增强大语言模型在特定领域知识库问答任务中思维过程的逻辑连贯性和上下文一致性。

Method: 通过广度分解将复杂问题拆分为子问题，用不同函数处理不同任务；用知识边界模型选最优知识源，深度求解模型增强知识获取；采用多轮对话监督微调使模型与推理范式对齐。

Result: 未提及明确结果。

Conclusion: 未提及明确结论。

Abstract: In this paper, we introduce KAG-Thinker, a novel human-like reasoning
framework built upon a parameter-light large language model (LLM). Our approach
enhances the logical coherence and contextual consistency of the thinking
process in question-answering (Q\&A) tasks on domain-specific knowledge bases
(KBs) within LLMs. This framework simulates human cognitive mechanisms for
handling complex problems by establishing a structured thinking process.
Continuing the \textbf{Logical Form} guided retrieval and reasoning technology
route of KAG v0.7, firstly, it decomposes complex questions into independently
solvable sub-problems(also referred to as logical forms) through
\textbf{breadth decomposition}, each represented in two equivalent
forms-natural language and logical function-and further classified as either
Knowledge Retrieval or Reasoning Analysis tasks, with dependencies and
variables passing explicitly modeled via logical function interfaces. In the
solving process, the Retrieval function is used to perform knowledge retrieval
tasks, while the Math and Deduce functions are used to perform reasoning
analysis tasks. Secondly, it is worth noting that, in the Knowledge Retrieval
sub-problem tasks, LLMs and external knowledge sources are regarded as
equivalent KBs. We use the \textbf{knowledge boundary} model to determine the
optimal source using self-regulatory mechanisms such as confidence calibration
and reflective reasoning, and use the \textbf{depth solving} model to enhance
the comprehensiveness of knowledge acquisition. Finally, instead of utilizing
reinforcement learning, we employ supervised fine-tuning with multi-turn
dialogues to align the model with our structured inference paradigm, thereby
avoiding excessive reflection. This is supported by a data evaluation framework
and iterative corpus synthesis, which facilitate the generation of detailed
reasoning trajectories...

</details>


### [519] [HIDE and Seek: Detecting Hallucinations in Language Models via Decoupled Representations](https://arxiv.org/abs/2506.17748)
*Anwoy Chatterjee,Yash Goel,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 提出单通、免训练的HIDE方法检测语言模型幻觉，在多个数据集和模型上实验，效果优于其他单通方法，与多通方法竞争且计算量小。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型存在幻觉问题影响可靠性，多数检测方法计算成本高、延迟大。

Method: 基于语言模型输入上下文和生成输出内部表示统计解耦假设，用HSIC量化解耦。

Result: HIDE在几乎所有设置中优于其他单通方法，AUC - ROC平均相对提升约29%；与多通方法竞争，AUC - ROC平均相对提升约3%，计算时间减少约51%。

Conclusion: 利用语言模型内部表示解耦可有效进行高效实用的幻觉检测。

Abstract: Contemporary Language Models (LMs), while impressively fluent, often generate
content that is factually incorrect or unfaithful to the input context - a
critical issue commonly referred to as 'hallucination'. This tendency of LMs to
generate hallucinated content undermines their reliability, especially because
these fabrications are often highly convincing and therefore difficult to
detect. While several existing methods attempt to detect hallucinations, most
rely on analyzing multiple generations per input, leading to increased
computational cost and latency. To address this, we propose a single-pass,
training-free approach for effective Hallucination detectIon via Decoupled
rEpresentations (HIDE). Our approach leverages the hypothesis that
hallucinations result from a statistical decoupling between an LM's internal
representations of input context and its generated output. We quantify this
decoupling using the Hilbert-Schmidt Independence Criterion (HSIC) applied to
hidden-state representations extracted while generating the output sequence. We
conduct extensive experiments on four diverse question answering datasets,
evaluating both faithfulness and factuality hallucinations across six
open-source LMs of varying scales and properties. Our results demonstrate that
HIDE outperforms other single-pass methods in almost all settings, achieving an
average relative improvement of ~29% in AUC-ROC over the best-performing
single-pass strategy across various models and datasets. Additionally, HIDE
shows competitive and often superior performance with multi-pass
state-of-the-art methods, obtaining an average relative improvement of ~3% in
AUC-ROC while consuming ~51% less computation time. Our findings highlight the
effectiveness of exploiting internal representation decoupling in LMs for
efficient and practical hallucination detection.

</details>


### [520] [THCM-CAL: Temporal-Hierarchical Causal Modelling with Conformal Calibration for Clinical Risk Prediction](https://arxiv.org/abs/2506.17844)
*Xin Zhang,Qiyu Wei,Yingjie Zhu,Fanyi Wu,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 提出THCM - CAL模型用于电子病历临床风险预测，实验显示其优越性。


<details>
  <summary>Details</summary>
Motivation: 以往方法处理电子病历结构化诊断代码和非结构化叙述笔记存在不足，未考虑因果交互。

Method: 构建多模态因果图，进行分层因果发现，将共形预测扩展到多标签ICD编码。

Result: 在MIMIC - III和MIMIC - IV上实验表明THCM - CAL更优。

Conclusion: THCM - CAL模型在电子病历临床风险预测中表现出色。

Abstract: Automated clinical risk prediction from electronic health records (EHRs)
demands modeling both structured diagnostic codes and unstructured narrative
notes. However, most prior approaches either handle these modalities separately
or rely on simplistic fusion strategies that ignore the directional,
hierarchical causal interactions by which narrative observations precipitate
diagnoses and propagate risk across admissions. In this paper, we propose
THCM-CAL, a Temporal-Hierarchical Causal Model with Conformal Calibration. Our
framework constructs a multimodal causal graph where nodes represent clinical
entities from two modalities: Textual propositions extracted from notes and ICD
codes mapped to textual descriptions. Through hierarchical causal discovery,
THCM-CAL infers three clinically grounded interactions: intra-slice
same-modality sequencing, intra-slice cross-modality triggers, and inter-slice
risk propagation. To enhance prediction reliability, we extend conformal
prediction to multi-label ICD coding, calibrating per-code confidence intervals
under complex co-occurrences. Experimental results on MIMIC-III and MIMIC-IV
demonstrate the superiority of THCM-CAL.

</details>


### [521] [How Alignment Shrinks the Generative Horizon](https://arxiv.org/abs/2506.17871)
*Chenghao Yang,Ari Holtzman*

Main category: cs.CL

TL;DR: 研究对齐大语言模型输出缺乏多样性的原因，引入分支因子（BF）量化概率集中，发现BF随生成进行降低，对齐调优大幅降低BF，BF可用于理解和控制LLM输出。


<details>
  <summary>Details</summary>
Motivation: 探究对齐大语言模型输出缺乏多样性、生成稳定的驱动因素。

Method: 引入分支因子（BF）量化模型输出分布的概率集中情况并进行实证分析，开展nudging实验。

Result: 1. BF随生成进行下降，模型更具可预测性；2. 对齐调优大幅降低BF；3. 对齐思维链模型利用低BF阶段产生稳定输出；4. nudging实验表明提示基础模型特定标记可降低BF。

Conclusion: BF是理解和控制大语言模型输出的有力工具，能解释对齐如何减少可变性、思维链如何促进稳定生成以及基础模型如何降低多样性。

Abstract: Despite their impressive capabilities, aligned large language models (LLMs)
often generate outputs that lack diversity. What drives this stability in the
generation? We investigate this phenomenon through the lens of probability
concentration in the model's output distribution. To quantify this
concentration, we introduce the Branching Factor (BF) -- a token-invariant
measure of the effective number of plausible next steps during generation. Our
empirical analysis reveals two key findings: (1) BF often decreases as
generation progresses, suggesting that LLMs become more predictable as they
generate. (2) alignment tuning substantially sharpens the model's output
distribution from the outset, reducing BF by nearly an order of magnitude
(e.g., from 12 to 1.2) relative to base models. This stark reduction helps
explain why aligned models often appear less sensitive to decoding strategies.
Building on this insight, we find this stability has surprising implications
for complex reasoning. Aligned Chain-of-Thought (CoT) models (e.g.,
DeepSeek-distilled models), for instance, leverage this effect; by generating
longer reasoning chains, they push generation into later, more deterministic
(lower BF) stages, resulting in more stable outputs. We hypothesize that
alignment tuning does not fundamentally change a model's behavior, but instead
steers it toward stylistic tokens (e.g., "Sure") that unlock low-entropy
trajectories already present in the base model. This view is supported by
nudging experiments, which show that prompting base models with such tokens can
similarly reduce BF. Together, our findings establish BF as a powerful
diagnostic for understanding and controlling LLM outputs - clarifying how
alignment reduces variability, how CoT promotes stable generations, and how
base models can be steered away from diversity.

</details>


### [522] [Multi-turn Jailbreaking via Global Refinement and Active Fabrication](https://arxiv.org/abs/2506.17881)
*Hua Tang,Lingyong Yan,Yukun Zhao,Shuaiqiang Wang,Jizhou Huang,Dawei Yin*

Main category: cs.CL

TL;DR: 本文提出多轮越狱方法，在多轮对话中全局优化越狱路径并伪造回复，实验显示其优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在安全风险，越狱攻击可识别威胁，但现有多轮越狱技术在动态对话中适应性差。

Method: 提出在每次交互中全局优化越狱路径的多轮越狱方法，主动伪造模型回复以抑制安全警告。

Result: 实验表明该方法在六种最先进的大语言模型上优于现有单轮和多轮越狱技术。

Conclusion: 所提多轮越狱方法有效，能更好应对大语言模型安全风险，代码已公开。

Abstract: Large Language Models (LLMs) have achieved exceptional performance across a
wide range of tasks. However, they still pose significant safety risks due to
the potential misuse for malicious purposes. Jailbreaks, which aim to elicit
models to generate harmful content, play a critical role in identifying the
underlying security threats. Recent jailbreaking primarily focuses on
single-turn scenarios, while the more complicated multi-turn scenarios remain
underexplored. Moreover, existing multi-turn jailbreaking techniques struggle
to adapt to the evolving dynamics of dialogue as the interaction progresses. To
address this limitation, we propose a novel multi-turn jailbreaking method that
refines the jailbreaking path globally at each interaction. We also actively
fabricate model responses to suppress safety-related warnings, thereby
increasing the likelihood of eliciting harmful outputs in subsequent questions.
Experimental results demonstrate the superior performance of our method
compared with existing single-turn and multi-turn jailbreaking techniques
across six state-of-the-art LLMs. Our code is publicly available at
https://github.com/Ytang520/Multi-Turn_jailbreaking_Global-Refinment_and_Active-Fabrication.

</details>


### [523] [Scatter-Based Innovation Propagation in Large Language Models for Multi-Stage Process Adaptation](https://arxiv.org/abs/2506.17949)
*Hong Su*

Main category: cs.CL

TL;DR: 论文提出创新散射模型帮助大语言模型扩展创新，提升泛化和复用能力。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型难以将特定阶段或组件的局部创新应用到多阶段过程其他部分的问题。

Method: 提出基于散射的创新扩展模型，引导大语言模型经历识别核心创新、泛化创新、判断适用性、系统应用四个步骤。

Result: 创新散射模型使大语言模型能跨结构相似阶段扩展创新。

Conclusion: 创新散射模型可增强大语言模型的泛化和复用能力。

Abstract: Large Language Models (LLMs) exhibit strong capabilities in reproducing and
extending patterns observed during pretraining but often struggle to generalize
novel ideas beyond their original context. This paper addresses the challenge
of applying such localized innovations - introduced at a specific stage or
component - to other parts of a multi-stage process. We propose a scatter-based
innovation expansion model (innovation scatter model) that guides the LLM
through a four-step process: (1) identifying the core innovation by comparing
the user's input with its surrounding context, (2) generalizing the innovation
by removing references to specific stages or components, (3) determining
whether the generalized innovation applies to a broader scope beyond the
original stage, and (4) systematically applying it to other structurally
similar stages using the LLM. This model leverages structural redundancy across
stages to improve the applicability of novel ideas. Verification results
demonstrate that the innovation scatter model enables LLMs to extend
innovations across structurally similar stages, thereby enhancing
generalization and reuse.

</details>


### [524] [TyphoFormer: Language-Augmented Transformer for Accurate Typhoon Track Forecasting](https://arxiv.org/abs/2506.17609)
*Lincan Li,Eren Erman Ozguven,Yue Zhao,Guang Wang,Yiqun Xie,Yushun Dong*

Main category: cs.CL

TL;DR: 提出TyphoFormer框架，结合自然语言描述辅助提示改进台风轨迹预测，在HURDAT2基准测试中表现优于其他方法。


<details>
  <summary>Details</summary>
Motivation: Transformer模型缺乏提升稀疏气象轨迹（如台风轨迹）预测可靠性的更广泛上下文知识，需准确的台风轨迹预测用于预警和灾害响应。

Method: 提出TyphoFormer框架，用大语言模型基于北大西洋飓风数据库生成文本描述，将其作为特殊辅助令牌添加到数值时间序列输入，通过统一的Transformer编码器整合文本和序列信息。

Result: 在HURDAT2基准测试中，TyphoFormer始终优于其他最先进的基线方法，尤其在非线性路径偏移和历史观测有限的具有挑战性的场景下。

Conclusion: TyphoFormer框架通过结合自然语言描述作为辅助提示，有效提升了台风轨迹预测的性能。

Abstract: Accurate typhoon track forecasting is crucial for early system warning and
disaster response. While Transformer-based models have demonstrated strong
performance in modeling the temporal dynamics of dense trajectories of humans
and vehicles in smart cities, they usually lack access to broader contextual
knowledge that enhances the forecasting reliability of sparse meteorological
trajectories, such as typhoon tracks. To address this challenge, we propose
TyphoFormer, a novel framework that incorporates natural language descriptions
as auxiliary prompts to improve typhoon trajectory forecasting. For each time
step, we use Large Language Model (LLM) to generate concise textual
descriptions based on the numerical attributes recorded in the North Atlantic
hurricane database. The language descriptions capture high-level meteorological
semantics and are embedded as auxiliary special tokens prepended to the
numerical time series input. By integrating both textual and sequential
information within a unified Transformer encoder, TyphoFormer enables the model
to leverage contextual cues that are otherwise inaccessible through numerical
features alone. Extensive experiments are conducted on HURDAT2 benchmark,
results show that TyphoFormer consistently outperforms other state-of-the-art
baseline methods, particularly under challenging scenarios involving nonlinear
path shifts and limited historical observations.

</details>


### [525] [Step-Opt: Boosting Optimization Modeling in LLMs through Iterative Data Synthesis and Structured Validation](https://arxiv.org/abs/2506.17637)
*Yang Wu,Yifan Zhang,Yurong Wu,Yuran Wang,Junkai Zhang,Jian Cheng*

Main category: cs.CL

TL;DR: 提出Step - Opt - Instruct框架生成优化建模微调数据，微调开源大模型得到Step - Opt，在基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在运筹学优化建模任务，尤其是处理复杂问题时面临挑战。

Method: 提出Step - Opt - Instruct框架，采用迭代问题生成增加复杂度，逐步验证确保数据质量；用此框架微调开源大模型。

Result: Step - Opt在NL4OPT、MAMO和IndustryOR等基准测试中达最优性能，在难题上微平均准确率提升17.01%。

Conclusion: 结构化验证与逐步问题细化结合能有效推动大语言模型在决策自动化过程中的应用。

Abstract: Large Language Models (LLMs) have revolutionized various domains but
encounter substantial challenges in tackling optimization modeling tasks for
Operations Research (OR), particularly when dealing with complex problem. In
this work, we propose Step-Opt-Instruct, a framework that augments existing
datasets and generates high-quality fine-tuning data tailored to optimization
modeling. Step-Opt-Instruct employs iterative problem generation to
systematically increase problem complexity and stepwise validation to
rigorously verify data, preventing error propagation and ensuring the quality
of the generated dataset. Leveraging this framework, we fine-tune open-source
LLMs, including LLaMA-3-8B and Mistral-7B, to develop Step-Opt--a model that
achieves state-of-the-art performance on benchmarks such as NL4OPT, MAMO, and
IndustryOR. Extensive experiments demonstrate the superior performance of
Step-Opt, especially in addressing complex OR tasks, with a notable 17.01\%
improvement in micro average accuracy on difficult problems. These findings
highlight the effectiveness of combining structured validation with gradual
problem refinement to advance the automation of decision-making processes using
LLMs.The code and dataset are available at https://github.com/samwu-learn/Step.

</details>


### [526] [Zero-Shot Conversational Stance Detection: Dataset and Approaches](https://arxiv.org/abs/2506.17693)
*Yuzhe Ding,Kang He,Bobo Li,Li Zheng,Haijun He,Fei Li,Chong Teng,Donghong Ji*

Main category: cs.CL

TL;DR: 文章指出现有对话立场检测数据集目标受限，手动创建了ZS - CSD数据集，提出SITPCL模型，该模型在零样本检测中有最优表现但仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 现有对话立场检测数据集目标有限，限制了模型在现实应用中对大量未见目标的检测效果。

Method: 手动创建包含280个目标的ZS - CSD数据集，提出SITPCL模型并在零样本设置下测试。

Result: SITPCL模型在零样本对话立场检测中达到了最优表现，但F1 - macro分数仅为43.81%。

Conclusion: 零样本对话立场检测仍存在挑战。

Abstract: Stance detection, which aims to identify public opinion towards specific
targets using social media data, is an important yet challenging task. With the
increasing number of online debates among social media users, conversational
stance detection has become a crucial research area. However, existing
conversational stance detection datasets are restricted to a limited set of
specific targets, which constrains the effectiveness of stance detection models
when encountering a large number of unseen targets in real-world applications.
To bridge this gap, we manually curate a large-scale, high-quality zero-shot
conversational stance detection dataset, named ZS-CSD, comprising 280 targets
across two distinct target types. Leveraging the ZS-CSD dataset, we propose
SITPCL, a speaker interaction and target-aware prototypical contrastive
learning model, and establish the benchmark performance in the zero-shot
setting. Experimental results demonstrate that our proposed SITPCL model
achieves state-of-the-art performance in zero-shot conversational stance
detection. Notably, the SITPCL model attains only an F1-macro score of 43.81%,
highlighting the persistent challenges in zero-shot conversational stance
detection.

</details>


### [527] [Unveiling Factors for Enhanced POS Tagging: A Study of Low-Resource Medieval Romance Languages](https://arxiv.org/abs/2506.17715)
*Matthias Schöffel,Esteban Garces Arias,Marinus Wiedner,Paula Ruppert,Meimingwei Li,Christian Heumann,Matthias Aßenmacher*

Main category: cs.CL

TL;DR: 研究中世纪罗曼语语料词性标注，评估多种技术对标注准确性影响，揭示大语言模型处理历史语言的局限与有效技术。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型应用于中世纪罗曼语词性标注存在因语言演变、拼写变化和标注数据稀缺带来的独特挑战，需研究影响标注性能的因素。

Method: 系统研究中世纪奥克语、西班牙语和法语多种领域语料，通过严格实验评估微调方法、提示工程、模型架构、解码策略和跨语言迁移学习技术对标注准确性的影响。

Result: 大语言模型处理历史语言变体和非标准化拼写能力有显著局限，但也有能有效应对低资源历史语言独特挑战的专门技术。

Conclusion: 有专门技术可应对低资源历史语言词性标注挑战，但大语言模型处理历史语言仍有局限。

Abstract: Part-of-speech (POS) tagging remains a foundational component in natural
language processing pipelines, particularly critical for historical text
analysis at the intersection of computational linguistics and digital
humanities. Despite significant advancements in modern large language models
(LLMs) for ancient languages, their application to Medieval Romance languages
presents distinctive challenges stemming from diachronic linguistic evolution,
spelling variations, and labeled data scarcity. This study systematically
investigates the central determinants of POS tagging performance across diverse
corpora of Medieval Occitan, Medieval Spanish, and Medieval French texts,
spanning biblical, hagiographical, medical, and dietary domains. Through
rigorous experimentation, we evaluate how fine-tuning approaches, prompt
engineering, model architectures, decoding strategies, and cross-lingual
transfer learning techniques affect tagging accuracy. Our results reveal both
notable limitations in LLMs' ability to process historical language variations
and non-standardized spelling, as well as promising specialized techniques that
effectively address the unique challenges presented by low-resource historical
languages.

</details>


### [528] [Mental Health Equity in LLMs: Leveraging Multi-Hop Question Answering to Detect Amplified and Silenced Perspectives](https://arxiv.org/abs/2506.18116)
*Batool Haider,Atmika Gorti,Aman Chadha,Manas Gaur*

Main category: cs.CL

TL;DR: 本文提出多跳问答框架检测大语言模型在心理健康话语中的响应偏差，评估四个模型，展示检测优势，实施去偏技术并取得成效，为公平AI发展提供见解。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在心理健康护理中存在传播偏见的风险，且现有检测交叉偏见的系统方法有限。

Method: 引入多跳问答框架，分析IMHI数据集内容，对年龄、种族、性别和社会经济地位进行系统标记，评估四个大语言模型，实施角色扮演模拟和显式偏差减少两种去偏技术。

Result: 揭示了在情感、人口统计和心理健康状况方面的系统性差异，多跳问答方法检测效果优于传统方法，去偏技术通过少样本提示实现66 - 94%的偏差减少。

Conclusion: 研究指出大语言模型再现心理健康护理偏见的关键领域，为公平的人工智能发展提供了可操作的见解。

Abstract: Large Language Models (LLMs) in mental healthcare risk propagating biases
that reinforce stigma and harm marginalized groups. While previous research
identified concerning trends, systematic methods for detecting intersectional
biases remain limited. This work introduces a multi-hop question answering
(MHQA) framework to explore LLM response biases in mental health discourse. We
analyze content from the Interpretable Mental Health Instruction (IMHI) dataset
across symptom presentation, coping mechanisms, and treatment approaches. Using
systematic tagging across age, race, gender, and socioeconomic status, we
investigate bias patterns at demographic intersections. We evaluate four LLMs:
Claude 3.5 Sonnet, Jamba 1.6, Gemma 3, and Llama 4, revealing systematic
disparities across sentiment, demographics, and mental health conditions. Our
MHQA approach demonstrates superior detection compared to conventional methods,
identifying amplification points where biases magnify through sequential
reasoning. We implement two debiasing techniques: Roleplay Simulation and
Explicit Bias Reduction, achieving 66-94% bias reductions through few-shot
prompting with BBQ dataset examples. These findings highlight critical areas
where LLMs reproduce mental healthcare biases, providing actionable insights
for equitable AI development.

</details>


### [529] [$φ^{\infty}$: Clause Purification, Embedding Realignment, and the Total Suppression of the Em Dash in Autoregressive Language Models](https://arxiv.org/abs/2506.18129)
*Bugra Kilictas,Faruk Alpay*

Main category: cs.CL

TL;DR: 发现自回归Transformer语言模型中破折号导致的漏洞，提出无需重新训练模型的解决方案，实验验证效果好并建立通用框架。


<details>
  <summary>Details</summary>
Motivation: 识别自回归Transformer语言模型中破折号导致的递归语义漂移、子句边界幻觉和嵌入空间纠缠等关键漏洞。

Method: 通过对语义格中令牌级扰动的形式分析，结合使用phi - infinity运算符进行符号子句净化和目标嵌入矩阵重新对齐。

Result: 实验验证在生成一致性和主题维护方面有显著改善。

Conclusion: 建立了识别和缓解基础模型中令牌级漏洞的通用框架，对AI安全等有直接影响，方法可扩展到解决更广泛的递归不稳定性问题。

Abstract: We identify a critical vulnerability in autoregressive transformer language
models where the em dash token induces recursive semantic drift, leading to
clause boundary hallucination and embedding space entanglement. Through formal
analysis of token-level perturbations in semantic lattices, we demonstrate that
em dash insertion fundamentally alters the model's latent representations,
causing compounding errors in long-form generation. We propose a novel solution
combining symbolic clause purification via the phi-infinity operator with
targeted embedding matrix realignment. Our approach enables total suppression
of problematic tokens without requiring model retraining, while preserving
semantic coherence through fixed-point convergence guarantees. Experimental
validation shows significant improvements in generation consistency and topic
maintenance. This work establishes a general framework for identifying and
mitigating token-level vulnerabilities in foundation models, with immediate
implications for AI safety, model alignment, and robust deployment of large
language models in production environments. The methodology extends beyond
punctuation to address broader classes of recursive instabilities in neural
text generation systems.

</details>


### [530] [Sparse Feature Coactivation Reveals Composable Semantic Modules in Large Language Models](https://arxiv.org/abs/2506.18141)
*Ruixuan Deng,Xiaoyang Hu,Miles Gilberti,Shane Storks,Aman Taxali,Mike Angstadt,Chandra Sripada,Joyce Chai*

Main category: cs.CL

TL;DR: 用少量提示收集的稀疏自编码器特征识别大语言模型中语义连贯、上下文一致的网络组件，研究国家关系任务，发现模型知识模块化组织，推进模型操作方法。


<details>
  <summary>Details</summary>
Motivation: 识别大语言模型中有意义的网络组件，探索模型知识组织方式及实现高效有针对性的模型操作。

Method: 利用少量提示收集稀疏自编码器特征来识别网络组件，在国家关系任务上进行消融和放大组件实验。

Result: 消融和放大组件能使模型输出按预期改变；国家组件多在第一层出现，关系组件集中在后面层；关系组件中后层节点对输出因果影响更强。

Conclusion: 大语言模型内知识存在模块化组织，研究推进了高效有针对性的模型操作方法。

Abstract: We identify semantically coherent, context-consistent network components in
large language models (LLMs) using coactivation of sparse autoencoder (SAE)
features collected from just a handful of prompts. Focusing on country-relation
tasks, we show that ablating semantic components for countries and relations
changes model outputs in predictable ways, while amplifying these components
induces counterfactual responses. Notably, composing relation and country
components yields compound counterfactual outputs. We find that, whereas most
country components emerge from the very first layer, the more abstract relation
components are concentrated in later layers. Furthermore, within relation
components themselves, nodes from later layers tend to have a stronger causal
impact on model outputs. Overall, these findings suggest a modular organization
of knowledge within LLMs and advance methods for efficient, targeted model
manipulation.

</details>


### [531] [QuranMorph: Morphologically Annotated Quranic Corpus](https://arxiv.org/abs/2506.18148)
*Diyam Akra,Tymaa Hammouda,Mustafa Jarrar*

Main category: cs.CL

TL;DR: 介绍了QuranMorph语料库，含77,429个标记，人工标注，开源可用。


<details>
  <summary>Details</summary>
Motivation: 构建一个形态学标注的《古兰经》语料库，便于与多种语言资源关联。

Method: 由三位专家语言学家手动进行词形还原，使用Qabas数据库中的词元；词性标注采用SAMA/Qabas标签集。

Result: QuranMorph语料库可与许多语言资源相互关联。

Conclusion: 构建了开源且公开可用的QuranMorph语料库。

Abstract: We present the QuranMorph corpus, a morphologically annotated corpus for the
Quran (77,429 tokens). Each token in the QuranMorph was manually lemmatized and
tagged with its part-of-speech by three expert linguists. The lemmatization
process utilized lemmas from Qabas, an Arabic lexicographic database linked
with 110 lexicons and corpora of 2 million tokens. The part-of-speech tagging
was performed using the fine-grained SAMA/Qabas tagset, which encompasses 40
tags. As shown in this paper, this rich lemmatization and POS tagset enabled
the QuranMorph corpus to be inter-linked with many linguistic resources. The
corpus is open-source and publicly available as part of the SinaLab resources
at (https://sina.birzeit.edu/quran)

</details>


### [532] [CareLab at #SMM4H-HeaRD 2025: Insomnia Detection and Food Safety Event Extraction with Domain-Aware Transformers](https://arxiv.org/abs/2506.18185)
*Zihan Liang,Ziwen Pan,Sumon Kanti Dey,Azra Ismail*

Main category: cs.CL

TL;DR: 介绍SMM4H - HeaRD 2025共享任务系统，涵盖任务4和5，强调任务5子任务1表现佳，用编码器模型和GPT - 4实现。


<details>
  <summary>Details</summary>
Motivation: 参与SMM4H - HeaRD 2025共享任务，完成任务4（临床笔记中失眠提及检测）和任务5（新闻文章食品安全事件提取）。

Method: 采用基于编码器的模型（如RoBERTa），并用GPT - 4进行数据增强，包含预处理、模型架构和子任务特定调整。

Result: 系统在各子任务有成果，任务5子任务1表现突出，测试集F1分数0.958获第一名。

Conclusion: 论文阐述了完成共享任务的系统方法和取得的良好结果。

Abstract: This paper presents our system for the SMM4H-HeaRD 2025 shared tasks,
specifically Task 4 (Subtasks 1, 2a, and 2b) and Task 5 (Subtasks 1 and 2).
Task 4 focused on detecting mentions of insomnia in clinical notes, while Task
5 addressed the extraction of food safety events from news articles. We
participated in all subtasks and report key findings across them, with
particular emphasis on Task 5 Subtask 1, where our system achieved strong
performance-securing first place with an F1 score of 0.958 on the test set. To
attain this result, we employed encoder-based models (e.g., RoBERTa), alongside
GPT-4 for data augmentation. This paper outlines our approach, including
preprocessing, model architecture, and subtask-specific adaptations

</details>


### [533] [Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review](https://arxiv.org/abs/2506.18199)
*Bushra Asseri,Estabrag Abdelaziz,Areej Al-Wabil*

Main category: cs.CL

TL;DR: 研究分析解决大语言模型对阿拉伯和穆斯林文化偏见的提示工程策略，发现五种方法，指出结构化多步管道最有效，强调提示工程可缓解偏见，也指出研究存在缺口。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在对阿拉伯和穆斯林的文化偏见，现有针对此的提示工程策略研究不足。

Method: 采用混合方法系统综述，遵循PRISMA指南和Kitchenham系统综述方法，分析2021 - 2024年8项实证研究。

Result: 发现五种提示工程方法，结构化多步管道总体效果最佳，文化提示更易获取且效果显著，不同方法效果因研究和偏见类型而异。

Conclusion: 提示工程可缓解文化偏见且无需访问模型参数，但该领域研究不足，未来应开发适应性技术、创建评估资源并结合其他去偏方法。

Abstract: Large language models have demonstrated remarkable capabilities across
various domains, yet concerns about cultural bias - particularly towards Arabs
and Muslims - pose significant ethical challenges by perpetuating harmful
stereotypes and marginalization. Despite growing recognition of bias in LLMs,
prompt engineering strategies specifically addressing Arab and Muslim
representation remain understudied. This mixed-methods systematic review
examines such techniques, offering evidence-based guidance for researchers and
practitioners. Following PRISMA guidelines and Kitchenham's systematic review
methodology, we analyzed 8 empirical studies published between 2021-2024
investigating bias mitigation strategies. Our findings reveal five primary
prompt engineering approaches: cultural prompting, affective priming,
self-debiasing techniques, structured multi-step pipelines, and
parameter-optimized continuous prompts. Although all approaches show potential
for reducing bias, effectiveness varied substantially across studies and bias
types. Evidence suggests that certain bias types may be more resistant to
prompt-based mitigation than others. Structured multi-step pipelines
demonstrated the highest overall effectiveness, achieving up to 87.7% reduction
in bias, though they require greater technical expertise. Cultural prompting
offers broader accessibility with substantial effectiveness. These results
underscore the accessibility of prompt engineering for mitigating cultural bias
without requiring access to model parameters. The limited number of studies
identified highlights a significant research gap in this critical area. Future
research should focus on developing culturally adaptive prompting techniques,
creating Arab and Muslim-specific evaluation resources, and integrating prompt
engineering with complementary debiasing methods to address deeper stereotypes
while maintaining model utility.

</details>


### [534] [End-to-End Spoken Grammatical Error Correction](https://arxiv.org/abs/2506.18532)
*Mengjie Qian,Rao Ma,Stefano Bannò,Mark J. F. Gales,Kate M. Knill*

Main category: cs.CL

TL;DR: 本文探讨端到端口语语法纠错（SGEC）框架，比较不同架构，解决数据稀缺、提高反馈精度等问题，实验表明方法有效提升性能。


<details>
  <summary>Details</summary>
Motivation: 书面语法纠错已成熟，但口语语法纠错因存在不流畅、转录错误等问题面临挑战，需更好的解决方案。

Method: 比较级联、部分级联和端到端架构；采用自动伪标签框架增加训练数据；利用额外上下文信息；提出参考对齐过程提高反馈精度；结合编辑置信度估计排除低置信度编辑。

Result: 实验在内部Linguaskill语料库和公开Speak & Improve语料库上表明，提出的方法显著提升端到端SGEC性能。

Conclusion: 提出的方法能有效解决端到端SGEC系统开发中的挑战，提升系统性能。

Abstract: Grammatical Error Correction (GEC) and feedback play a vital role in
supporting second language (L2) learners, educators, and examiners. While
written GEC is well-established, spoken GEC (SGEC), aiming to provide feedback
based on learners' speech, poses additional challenges due to disfluencies,
transcription errors, and the lack of structured input. SGEC systems typically
follow a cascaded pipeline consisting of Automatic Speech Recognition (ASR),
disfluency detection, and GEC, making them vulnerable to error propagation
across modules. This work examines an End-to-End (E2E) framework for SGEC and
feedback generation, highlighting challenges and possible solutions when
developing these systems. Cascaded, partial-cascaded and E2E architectures are
compared, all built on the Whisper foundation model. A challenge for E2E
systems is the scarcity of GEC labeled spoken data. To address this, an
automatic pseudo-labeling framework is examined, increasing the training data
from 77 to over 2500 hours. To improve the accuracy of the SGEC system,
additional contextual information, exploiting the ASR output, is investigated.
Candidate feedback of their mistakes is an essential step to improving
performance. In E2E systems the SGEC output must be compared with an estimate
of the fluent transcription to obtain the feedback. To improve the precision of
this feedback, a novel reference alignment process is proposed that aims to
remove hypothesised edits that results from fluent transcription errors.
Finally, these approaches are combined with an edit confidence estimation
approach, to exclude low-confidence edits. Experiments on the in-house
Linguaskill (LNG) corpora and the publicly available Speak & Improve (S&I)
corpus show that the proposed approaches significantly boost E2E SGEC
performance.

</details>


### [535] [Context Biasing for Pronunciations-Orthography Mismatch in Automatic Speech Recognition](https://arxiv.org/abs/2506.18703)
*Christian Huber,Alexander Waibel*

Main category: cs.CL

TL;DR: 提出一种校正替换错误的方法，可在推理时即时添加校正，使有偏词错误率相对改善达11%，同时保持整体词错误率有竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有神经序列到序列自动语音识别系统难以识别训练中未见过的词，上下文偏置方法对发音 - 拼写法不匹配的词效果不佳。

Method: 提出一种校正替换错误的方法，允许用户在推理时即时添加校正。

Result: 该方法使有偏词错误率相对改善达11%，且保持整体词错误率有竞争力。

Conclusion: 所提方法能有效提高具有挑战性的词的识别准确率。

Abstract: Neural sequence-to-sequence systems deliver state-of-the-art performance for
automatic speech recognition. When using appropriate modeling units, e.g.,
byte-pair encoded characters, these systems are in principal open vocabulary
systems. In practice, however, they often fail to recognize words not seen
during training, e.g., named entities, acronyms, or domain-specific special
words. To address this problem, many context biasing methods have been
proposed; however, for words with a pronunciation-orthography mismatch, these
methods may still struggle. We propose a method which allows corrections of
substitution errors to improve the recognition accuracy of such challenging
words. Users can add corrections on the fly during inference. We show that with
this method we get a relative improvement in biased word error rate of up to
11\%, while maintaining a competitive overall word error rate.

</details>


### [536] [Evaluating Causal Explanation in Medical Reports with LLM-Based and Human-Aligned Metrics](https://arxiv.org/abs/2506.18387)
*Yousang Cho,Key-Sun Choi*

Main category: cs.CL

TL;DR: 研究不同评估指标对自动生成诊断报告中因果解释质量的评估准确性，对比六种指标，结果显示GPT - Black判别力强，强调指标选择和加权的影响，支持基于大语言模型的评估。


<details>
  <summary>Details</summary>
Motivation: 探究不同评估指标对自动生成诊断报告中因果解释质量的评估准确性。

Method: 对比六种评估指标（BERTScore、Cosine Similarity、BioSentVec、GPT - White、GPT - Black和专家定性评估），在两种输入类型（基于观察和基于多项选择的报告生成）下，应用两种加权策略（反映特定任务优先级和等权重）。

Result: GPT - Black在识别逻辑连贯和临床有效的因果叙述方面判别力最强，GPT - White与专家评估契合度高，基于相似度的指标与临床推理质量有差异。

Conclusion: 指标选择和加权会影响评估结果，支持在需要可解释性和因果推理的任务中使用基于大语言模型的评估。

Abstract: This study investigates how accurately different evaluation metrics capture
the quality of causal explanations in automatically generated diagnostic
reports. We compare six metrics: BERTScore, Cosine Similarity, BioSentVec,
GPT-White, GPT-Black, and expert qualitative assessment across two input types:
observation-based and multiple-choice-based report generation. Two weighting
strategies are applied: one reflecting task-specific priorities, and the other
assigning equal weights to all metrics. Our results show that GPT-Black
demonstrates the strongest discriminative power in identifying logically
coherent and clinically valid causal narratives. GPT-White also aligns well
with expert evaluations, while similarity-based metrics diverge from clinical
reasoning quality. These findings emphasize the impact of metric selection and
weighting on evaluation outcomes, supporting the use of LLM-based evaluation
for tasks requiring interpretability and causal reasoning.

</details>


### [537] [TReB: A Comprehensive Benchmark for Evaluating Table Reasoning Capabilities of Large Language Models](https://arxiv.org/abs/2506.18421)
*Ce Li,Xiaofan Liu,Zhiyan Song,Ce Chi,Chen Zhao,Jingjing Yang,Zhendong Wang,Kexin Yang,Boshen Shi,Xing Wang,Chao Deng,Junlan Feng*

Main category: cs.CL

TL;DR: 本文提出全面的表格推理评估基准TReB，涵盖26个子任务，构建高质量数据集和评估框架，对超20个先进大模型进行基准测试，结果显示现有大模型在表格相关任务上仍有提升空间，数据集和框架公开。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏能公平反映大语言模型表格推理能力的有效评估基准。

Method: 提出全面的表格推理评估基准TReB，通过迭代数据处理构建高质量数据集，创建含三种推理模式的评估框架，用该框架对超20个先进大模型进行基准测试。

Result: 现有大语言模型在处理复杂现实表格相关任务上有很大提升空间。

Conclusion: 数据集和评估框架公开，可用于提升大语言模型表格推理能力。

Abstract: The majority of data in businesses and industries is stored in tables,
databases, and data warehouses. Reasoning with table-structured data poses
significant challenges for large language models (LLMs) due to its hidden
semantics, inherent complexity, and structured nature. One of these challenges
is lacking an effective evaluation benchmark fairly reflecting the performances
of LLMs on broad table reasoning abilities. In this paper, we fill in this gap,
presenting a comprehensive table reasoning evolution benchmark, TReB, which
measures both shallow table understanding abilities and deep table reasoning
abilities, a total of 26 sub-tasks. We construct a high quality dataset through
an iterative data processing procedure. We create an evaluation framework to
robustly measure table reasoning capabilities with three distinct inference
modes, TCoT, PoT and ICoT. Further, we benchmark over 20 state-of-the-art LLMs
using this frame work and prove its effectiveness. Experimental results reveal
that existing LLMs still have significant room for improvement in addressing
the complex and real world Table related tasks. Both the dataset and evaluation
framework are publicly available, with the dataset hosted on [HuggingFace] and
the framework on [GitHub].

</details>


### [538] [LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning](https://arxiv.org/abs/2506.18841)
*Yuhao Wu,Yushi Bai,Zhiqiang Hu,Roy Ka-Wei Lee,Juanzi Li*

Main category: cs.CL

TL;DR: 本文提出无监督的基于激励的方法LongWriter - Zero，使大语言模型具备超长高质量文本生成能力，实验效果超传统方法。


<details>
  <summary>Details</summary>
Motivation: 超长文本生成是大需求场景，但现有方法依赖合成SFT数据，构建难、成本高且生成质量不佳。

Method: 提出基于激励的方法，用强化学习从基础模型开始训练，借助专门奖励模型引导大语言模型。

Result: 基于Qwen2.5 - 32B训练的LongWriter - Zero模型在长文写作任务上超传统SFT方法，在多个指标上达最优，超100B + 模型。

Conclusion: 基于激励的方法有效提升大语言模型超长文本生成能力，开源数据和模型。

Abstract: Ultra-long generation by large language models (LLMs) is a widely demanded
scenario, yet it remains a significant challenge due to their maximum
generation length limit and overall quality degradation as sequence length
increases. Previous approaches, exemplified by LongWriter, typically rely on
''teaching'', which involves supervised fine-tuning (SFT) on synthetic
long-form outputs. However, this strategy heavily depends on synthetic SFT
data, which is difficult and costly to construct, often lacks coherence and
consistency, and tends to be overly artificial and structurally monotonous. In
this work, we propose an incentivization-based approach that, starting entirely
from scratch and without relying on any annotated or synthetic data, leverages
reinforcement learning (RL) to foster the emergence of ultra-long, high-quality
text generation capabilities in LLMs. We perform RL training starting from a
base model, similar to R1-Zero, guiding it to engage in reasoning that
facilitates planning and refinement during the writing process. To support
this, we employ specialized reward models that steer the LLM towards improved
length control, writing quality, and structural formatting. Experimental
evaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B,
consistently outperforms traditional SFT methods on long-form writing tasks,
achieving state-of-the-art results across all metrics on WritingBench and
Arena-Write, and even surpassing 100B+ models such as DeepSeek R1 and
Qwen3-235B. We open-source our data and model checkpoints under
https://huggingface.co/THU-KEG/LongWriter-Zero-32B

</details>


### [539] [MeRF: Motivation-enhanced Reinforcement Finetuning for Large Reasoning Models](https://arxiv.org/abs/2506.18485)
*Junjie Zhang,Guozheng Ma,Shunyu Liu,Haoyu Wang,Jiaxing Huang,Ting-En Lin,Fei Huang,Yongbin Li,Dacheng Tao*

Main category: cs.CL

TL;DR: 本文提出MeRF方法，结合强化学习与上下文学习提升大语言模型推理能力，在K&K基准测试上性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法忽视大语言模型上下文学习能力，探索如何结合强化学习与上下文学习提升模型推理能力。

Method: 提出Motivation - enhanced Reinforcement Finetuning (MeRF)方法，将奖励规范直接注入提示，作为上下文动机引导模型优化。

Result: 在K&K逻辑谜题推理基准测试中MeRF性能大幅优于基线，消融实验表明上下文动机与外部奖励函数一致性越高性能越好，模型能通过强化学习适应误导性动机。

Conclusion: MeRF方法能有效结合强化学习与上下文学习，提升大语言模型推理能力。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a
powerful learn-to-reason paradigm for Large Language Models (LLMs) to tackle
complex reasoning tasks. However, existing RLVR methods overlook one of the
most distinctive capabilities of LLMs, their in-context learning ability, as
prominently demonstrated by the success of Chain-of-Thought (CoT) prompting.
This motivates us to explore how reinforcement learning can be effectively
combined with in-context learning to better improve the reasoning capabilities
of LLMs. In this paper, we introduce Motivation-enhanced Reinforcement
Finetuning} (MeRF), an intuitive yet effective method enhancing reinforcement
learning of LLMs by involving ``telling LLMs the rules of the game''.
Specifically, MeRF directly injects the reward specification into the prompt,
which serves as an in-context motivation for model to improve its responses
with awareness of the optimization objective. This simple modification
leverages the in-context learning ability of LLMs aligning generation with
optimization, thereby incentivizing the model to generate desired outputs from
both inner motivation and external reward. Empirical evaluations on the Knights
and Knaves~(K&K) logic puzzle reasoning benchmark demonstrate that
\texttt{MeRF} achieves substantial performance gains over baselines. Moreover,
ablation studies show that performance improves with greater consistency
between the in-context motivation and the external reward function, while the
model also demonstrates an ability to adapt to misleading motivations through
reinforcement learning.

</details>


### [540] [Comparative Evaluation of ChatGPT and DeepSeek Across Key NLP Tasks: Strengths, Weaknesses, and Domain-Specific Performance](https://arxiv.org/abs/2506.18501)
*Wael Etaiwi,Bushra Alhijawi*

Main category: cs.CL

TL;DR: 研究评估ChatGPT和DeepSeek在五项NLP任务中的表现，发现DeepSeek在分类稳定性和逻辑推理方面出色，ChatGPT在需要细微理解和灵活性的任务中表现更好。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在NLP任务中使用增多，需全面评估ChatGPT和DeepSeek等模型在不同应用中的有效性，以了解其优缺点和特定领域能力。

Method: 采用结构化实验协议，用相同中立提示对两个模型进行测试，并在每个任务的两个基准数据集上评估，数据集涵盖新闻、评论等领域。

Result: DeepSeek在分类稳定性和逻辑推理方面表现出色，ChatGPT在需要细微理解和灵活性的任务中表现更好。

Conclusion: 研究结果为根据任务需求选择合适的大语言模型提供了有价值的见解。

Abstract: The increasing use of large language models (LLMs) in natural language
processing (NLP) tasks has sparked significant interest in evaluating their
effectiveness across diverse applications. While models like ChatGPT and
DeepSeek have shown strong results in many NLP domains, a comprehensive
evaluation is needed to understand their strengths, weaknesses, and
domain-specific abilities. This is critical as these models are applied to
various tasks, from sentiment analysis to more nuanced tasks like textual
entailment and translation. This study aims to evaluate ChatGPT and DeepSeek
across five key NLP tasks: sentiment analysis, topic classification, text
summarization, machine translation, and textual entailment. A structured
experimental protocol is used to ensure fairness and minimize variability. Both
models are tested with identical, neutral prompts and evaluated on two
benchmark datasets per task, covering domains like news, reviews, and
formal/informal texts. The results show that DeepSeek excels in classification
stability and logical reasoning, while ChatGPT performs better in tasks
requiring nuanced understanding and flexibility. These findings provide
valuable insights for selecting the appropriate LLM based on task requirements.

</details>


### [541] [Is There a Case for Conversation Optimized Tokenizers in Large Language Models?](https://arxiv.org/abs/2506.18674)
*Raquel Ferrando,Javier Conde,Gonzalo Martínez,Pedro Reviriego*

Main category: cs.CL

TL;DR: 论文探讨优化大型语言模型分词器用于聊天机器人对话，结果显示优化后的分词器能减少对话中令牌数量，节省5% - 10%能源且对原训练语料库影响小。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型计算和能源成本随模型增大及用户增多而指数级增加，分词器影响模型效率，且聊天机器人的用户输入和回复文本与训练语料库不同，因此探讨优化分词器用于聊天机器人对话的好处。

Method: 利用公开的聊天机器人对话语料库重新设计不同分词器的词汇表，并评估其在该领域的性能。

Result: 对话优化后的分词器持续减少聊天机器人对话中的令牌数量，可节省5% - 10%的能源，对原训练语料库的分词效率影响极小甚至略有积极影响。

Conclusion: 优化分词器用于聊天机器人对话有潜在好处，能在减少令牌数量、节省能源的同时，对原训练语料库影响不大。

Abstract: The computational and energy costs of Large Language Models (LLMs) have
increased exponentially driven by the growing model sizes and the massive
adoption of LLMs by hundreds of millions of users. The unit cost of an LLM is
the computation of a token. Therefore, the tokenizer plays an important role in
the efficiency of a model, and they are carefully optimized to minimize the
number of tokens for the text in their training corpus. One of the most popular
applications of LLMs are chatbots that interact with users. A key observation
is that, for those chatbots, what is important is the performance of the
tokenizer in the user text input and the chatbot responses. Those are most
likely different from the text in the training corpus. So, a question that
immediately arises is whether there is a potential benefit in optimizing
tokenizers for chatbot conversations. In this paper, this idea is explored for
different tokenizers by using a publicly available corpus of chatbot
conversations to redesign their vocabularies and evaluate their performance in
this domain. The results show that conversation-optimized tokenizers
consistently reduce the number of tokens in chatbot dialogues, which can lead
to meaningful energy savings, in the range of 5% to 10% while having minimal or
even slightly positive impact on tokenization efficiency for the original
training corpus.

</details>


### [542] [Benchmarking the Pedagogical Knowledge of Large Language Models](https://arxiv.org/abs/2506.18710)
*Maxime Lelièvre,Amy Waldock,Meng Liu,Natalia Valdés Aspillaga,Alasdair Mackintosh,María José Ogando Portelo,Jared Lee,Paul Atherton,Robin A. A. Ince,Oliver G. B. Garrod*

Main category: cs.CL

TL;DR: 本文引入教学基准测试，评估大语言模型跨领域教学知识和特殊教育教学知识，报告97个模型结果，设在线排行榜，强调教育基准对LLM在教育中应用的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注内容知识，缺乏对模型教学法理解的评估，需评估模型教学知识以促进其在教育中应用。

Method: 创建新数据集The Pedagogy Benchmark，基于教师专业发展考试问题构建基准，评估大语言模型。

Result: 报告97个模型在教学知识问题上准确率28% - 89%，考虑成本与准确率关系，绘制帕累托值前沿进展图，设在线排行榜。

Conclusion: 教育基准对衡量模型能力、指导LLM在教育中合理部署及决策至关重要。

Abstract: Benchmarks like Massive Multitask Language Understanding (MMLU) have played a
pivotal role in evaluating AI's knowledge and abilities across diverse domains.
However, existing benchmarks predominantly focus on content knowledge, leaving
a critical gap in assessing models' understanding of pedagogy - the method and
practice of teaching. This paper introduces The Pedagogy Benchmark, a novel
dataset designed to evaluate large language models on their Cross-Domain
Pedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND)
pedagogical knowledge. These benchmarks are built on a carefully curated set of
questions sourced from professional development exams for teachers, which cover
a range of pedagogical subdomains such as teaching strategies and assessment
methods. Here we outline the methodology and development of these benchmarks.
We report results for 97 models, with accuracies spanning a range from 28% to
89% on the pedagogical knowledge questions. We consider the relationship
between cost and accuracy and chart the progression of the Pareto value
frontier over time. We provide online leaderboards at
https://rebrand.ly/pedagogy which are updated with new models and allow
interactive exploration and filtering based on various model properties, such
as cost per token and open-vs-closed weights, as well as looking at performance
in different subjects. LLMs and generative AI have tremendous potential to
influence education and help to address the global learning crisis.
Education-focused benchmarks are crucial to measure models' capacities to
understand pedagogical concepts, respond appropriately to learners' needs, and
support effective teaching practices across diverse contexts. They are needed
for informing the responsible and evidence-based deployment of LLMs and
LLM-based tools in educational settings, and for guiding both development and
policy decisions.

</details>


### [543] [RWESummary: A Framework and Test for Choosing Large Language Models to Summarize Real-World Evidence (RWE) Studies](https://arxiv.org/abs/2506.18819)
*Arjun Mukerji,Michael L. Jackson,Jason Jones,Neil Sanghavi*

Main category: cs.CL

TL;DR: 提出RWESummary用于评估大语言模型对真实世界证据研究结构化输出的总结能力，用其比较不同LLMs表现，发现Gemini 2.5模型最佳。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型未专门针对真实世界证据研究结构化输出的总结任务进行评估。

Method: 在MedHELM框架中引入RWESummary，包含一个场景和三种评估，使用Atropos Health专有数据开发，用其比较内部RWE总结工具中不同LLMs表现。

Result: 在13项不同的RWE研究中，Gemini 2.5模型（Flash和Pro）整体表现最佳。

Conclusion: RWESummary是用于真实世界证据研究总结的新颖且有用的基础模型基准。

Abstract: Large Language Models (LLMs) have been extensively evaluated for general
summarization tasks as well as medical research assistance, but they have not
been specifically evaluated for the task of summarizing real-world evidence
(RWE) from structured output of RWE studies. We introduce RWESummary, a
proposed addition to the MedHELM framework (Bedi, Cui, Fuentes, Unell et al.,
2025) to enable benchmarking of LLMs for this task. RWESummary includes one
scenario and three evaluations covering major types of errors observed in
summarization of medical research studies and was developed using Atropos
Health proprietary data. Additionally, we use RWESummary to compare the
performance of different LLMs in our internal RWE summarization tool. At the
time of publication, with 13 distinct RWE studies, we found the Gemini 2.5
models performed best overall (both Flash and Pro). We suggest RWESummary as a
novel and useful foundation model benchmark for real-world evidence study
summarization.

</details>


### [544] [Mechanistic Interpretability Needs Philosophy](https://arxiv.org/abs/2506.18852)
*Iwan Williams,Ninell Oldenburg,Ruchira Dhar,Joshua Hatherley,Constanza Fierro,Nina Rajcic,Sandrine R. Schiller,Filippos Stamatiou,Anders Søgaard*

Main category: cs.CL

TL;DR: 随着机械可解释性（MI）领域影响力增大，需哲学作为伙伴，本文以三个问题为例说明哲学对MI研究的价值并规划跨学科对话路径。


<details>
  <summary>Details</summary>
Motivation: 随着MI领域影响力增长，有必要审视其隐含的假设、概念和解释策略，论证哲学对MI研究的必要性。

Method: 以MI文献中的三个开放问题为例进行阐述。

Result: 说明了哲学能为MI研究增添价值。

Conclusion: 指出应开展哲学与MI研究更深入的跨学科对话。

Abstract: Mechanistic interpretability (MI) aims to explain how neural networks work by
uncovering their underlying causal mechanisms. As the field grows in influence,
it is increasingly important to examine not just models themselves, but the
assumptions, concepts and explanatory strategies implicit in MI research. We
argue that mechanistic interpretability needs philosophy: not as an
afterthought, but as an ongoing partner in clarifying its concepts, refining
its methods, and assessing the epistemic and ethical stakes of interpreting AI
systems. Taking three open problems from the MI literature as examples, this
position paper illustrates the value philosophy can add to MI research, and
outlines a path toward deeper interdisciplinary dialogue.

</details>


### [545] [CommVQ: Commutative Vector Quantization for KV Cache Compression](https://arxiv.org/abs/2506.18879)
*Junyan Li,Yang Zhang,Muhammad Yusuf Hassan,Talha Chafekar,Tianle Cai,Zhile Ren,Pengsheng Guo,Foroozan Karimzadeh,Colorado Reed,Chong Wang,Chuang Gan*

Main category: cs.CL

TL;DR: 提出CommVQ减少长上下文大语言模型推理时的内存使用，实验效果好且开源代码。


<details>
  <summary>Details</summary>
Motivation: 大语言模型处理长上下文时，键值缓存成GPU内存瓶颈，需减少内存使用。

Method: 引入带轻量级编码器和码本的加法量化压缩键值缓存，设计与RoPE可交换的码本并使用EM算法训练，将解码高效集成到自注意力机制。

Result: 2位量化时减少87.5%的FP16键值缓存大小，优于现有方法；能实现1位量化且精度损失小，让LLaMA - 3.1 8B模型在单GPU上处理128K上下文长度。

Conclusion: CommVQ能有效减少长上下文大语言模型推理时的内存使用。

Abstract: Large Language Models (LLMs) are increasingly used in applications requiring
long context lengths, but the key-value (KV) cache often becomes a memory
bottleneck on GPUs as context grows. To address this, we propose Commutative
Vector Quantization (CommVQ) to significantly reduce memory usage for
long-context LLM inference. We first introduce additive quantization with a
lightweight encoder and codebook to compress the KV cache, which can be decoded
via simple matrix multiplication. To further reduce computational costs during
decoding, we design the codebook to be commutative with Rotary Position
Embedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.
This enables efficient integration of decoding into the self-attention
mechanism. Our approach achieves high accuracy with additive quantization and
low overhead via the RoPE-commutative codebook. Experiments on long-context
benchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%
with 2-bit quantization, while outperforming state-of-the-art KV cache
quantization methods. Notably, it enables 1-bit KV cache quantization with
minimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context
length on a single RTX 4090 GPU. The source code is available at:
https://github.com/UMass-Embodied-AGI/CommVQ.

</details>


### [546] [OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory, Compositional, and Transformative Generalization](https://arxiv.org/abs/2506.18880)
*Yiyou Sun,Shawn Hu,Georgia Zhou,Ken Zheng,Hannaneh Hajishirzi,Nouha Dziri,Dawn Song*

Main category: cs.CL

TL;DR: 提出OMEGA基准评估大语言模型在数学问题上的分布外泛化能力，评估前沿模型并微调Qwen系列，为提升模型数学创造力奠基。


<details>
  <summary>Details</summary>
Motivation: 现有大规模语言模型在奥林匹克数学基准测试虽有成果，但依赖有限策略，难以解决需新思维的问题，需系统研究其局限性。

Method: 引入OMEGA基准，包含三种泛化轴，由模板问题生成器生成训练 - 测试对，用符号、数值或图形方法验证答案，评估前沿模型并微调Qwen系列模型。

Result: 随着问题复杂度增加，前沿模型性能急剧下降；微调Qwen系列在探索性泛化有显著提升，组合泛化有限，变革性推理几乎无改善。

Conclusion: OMEGA通过分离和量化细粒度失败，为推动大语言模型超越机械熟练度、实现真正的数学创造力奠定基础。

Abstract: Recent large-scale language models (LLMs) with long Chain-of-Thought
reasoning-such as DeepSeek-R1-have achieved impressive results on
Olympiad-level mathematics benchmarks. However, they often rely on a narrow set
of strategies and struggle with problems that require a novel way of thinking.
To systematically investigate these limitations, we introduce
OMEGA-Out-of-distribution Math Problems Evaluation with 3 Generalization Axes-a
controlled yet diverse benchmark designed to evaluate three axes of
out-of-distribution generalization, inspired by Boden's typology of creativity:
(1) Exploratory-applying known problem solving skills to more complex instances
within the same problem domain; (2) Compositional-combining distinct reasoning
skills, previously learned in isolation, to solve novel problems that require
integrating these skills in new and coherent ways; and (3)
Transformative-adopting novel, often unconventional strategies by moving beyond
familiar approaches to solve problems more effectively. OMEGA consists of
programmatically generated training-test pairs derived from templated problem
generators across geometry, number theory, algebra, combinatorics, logic, and
puzzles, with solutions verified using symbolic, numerical, or graphical
methods. We evaluate frontier (or top-tier) LLMs and observe sharp performance
degradation as problem complexity increases. Moreover, we fine-tune the
Qwen-series models across all generalization settings and observe notable
improvements in exploratory generalization, while compositional generalization
remains limited and transformative reasoning shows little to no improvement. By
isolating and quantifying these fine-grained failures, OMEGA lays the
groundwork for advancing LLMs toward genuine mathematical creativity beyond
mechanical proficiency.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [547] [Dynamic Evolution of Complex Networks: A Reinforcement Learning Approach Applying Evolutionary Games to Community Structure](https://arxiv.org/abs/2506.17925)
*Bin Pi,Liang-Jian Deng,Minyu Feng,Matjaž Perc,Jürgen Kurths*

Main category: cs.SI

TL;DR: 本文提出含个体生死的网络演化模型，用强化学习研究动态系统中个体生死与社区结构发展，实验验证模型，分析多因素对社区的影响，为现实社区发展提供新视角。


<details>
  <summary>Details</summary>
Motivation: 解决当前动态系统中个体生死探索和社区结构发展研究的局限性。

Method: 提出网络演化模型，个体有任意分布的寿命，通过与邻居博弈、用Q - learning选择行动并在二维空间移动。

Result: 实验验证理论，观察有无生死过程系统中合作行为和社区结构的演化，模型能拟合现实种群和网络，分析出多种因素对社区的影响。

Conclusion: 模型为现实世界社区发展提供新视角，为研究种群动态行为提供有价值的框架。

Abstract: Complex networks serve as abstract models for understanding real-world
complex systems and provide frameworks for studying structured dynamical
systems. This article addresses limitations in current studies on the
exploration of individual birth-death and the development of community
structures within dynamic systems. To bridge this gap, we propose a networked
evolution model that includes the birth and death of individuals, incorporating
reinforcement learning through games among individuals. Each individual has a
lifespan following an arbitrary distribution, engages in games with network
neighbors, selects actions using Q-learning in reinforcement learning, and
moves within a two-dimensional space. The developed theories are validated
through extensive experiments. Besides, we observe the evolution of cooperative
behaviors and community structures in systems both with and without the
birth-death process. The fitting of real-world populations and networks
demonstrates the practicality of our model. Furthermore, comprehensive analyses
of the model reveal that exploitation rates and payoff parameters determine the
emergence of communities, learning rates affect the speed of community
formation, discount factors influence stability, and two-dimensional space
dimensions dictate community size. Our model offers a novel perspective on
real-world community development and provides a valuable framework for studying
population dynamics behaviors.

</details>


### [548] [Heterogeneous Temporal Hypergraph Neural Network](https://arxiv.org/abs/2506.17312)
*Huan Liu,Pengfei Jiao,Mengzhou Gao,Chaochao Chen,Di Jin*

Main category: cs.SI

TL;DR: 现有图表示学习方法忽视高阶交互关系，本文提出异质时态超图定义、构建算法和HTHGN网络，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 多数现有GRL方法只关注低阶拓扑信息，忽视高阶交互关系，且多数超图方法只能处理静态同质图，无法对HTGs中的高阶交互建模。

Method: 提出异质时态超图的形式化定义和不依赖额外信息的P - 均匀异质超边构建算法；提出HTHGN网络，含分层注意力机制模块进行时态消息传递，并通过对比学习避免低阶结构模糊问题。

Result: 在三个真实世界的HTG数据集上的实验结果验证了HTHGN对HTGs中高阶交互建模的有效性，并显示出显著的性能提升。

Conclusion: 提出的HTHGN能有效捕捉HTGs中的高阶交互关系，性能有显著提升。

Abstract: Graph representation learning (GRL) has emerged as an effective technique for
modeling graph-structured data. When modeling heterogeneity and dynamics in
real-world complex networks, GRL methods designed for complex heterogeneous
temporal graphs (HTGs) have been proposed and have achieved successful
applications in various fields. However, most existing GRL methods mainly focus
on preserving the low-order topology information while ignoring higher-order
group interaction relationships, which are more consistent with real-world
networks. In addition, most existing hypergraph methods can only model static
homogeneous graphs, limiting their ability to model high-order interactions in
HTGs. Therefore, to simultaneously enable the GRL model to capture high-order
interaction relationships in HTGs, we first propose a formal definition of
heterogeneous temporal hypergraphs and $P$-uniform heterogeneous hyperedge
construction algorithm that does not rely on additional information. Then, a
novel Heterogeneous Temporal HyperGraph Neural network (HTHGN), is proposed to
fully capture higher-order interactions in HTGs. HTHGN contains a hierarchical
attention mechanism module that simultaneously performs temporal
message-passing between heterogeneous nodes and hyperedges to capture rich
semantics in a wider receptive field brought by hyperedges. Furthermore, HTHGN
performs contrastive learning by maximizing the consistency between low-order
correlated heterogeneous node pairs on HTG to avoid the low-order structural
ambiguity issue. Detailed experimental results on three real-world HTG datasets
verify the effectiveness of the proposed HTHGN for modeling high-order
interactions in HTGs and demonstrate significant performance improvements.

</details>


### [549] [A family of graph GOSPA metrics for graphs with different sizes](https://arxiv.org/abs/2506.17316)
*Jinhao Gu,Ángel F. García-Fernández,Robert E. Firth,Lennart Svensson*

Main category: cs.SI

TL;DR: 本文提出用于衡量不同大小图之间距离的图度量族，证明其满足度量性质，展示近似计算方法并通过实验说明特性和分类益处。


<details>
  <summary>Details</summary>
Motivation: 为衡量不同大小图之间的距离，提供更通用的图度量方法。

Method: 定义图广义最优子模式分配（GOSPA）度量族，用线性规划近似计算，进行仿真实验和真实数据集测试。

Result: 该度量族满足度量性质，可近似计算，不同超参数选择有不同特性，在分类任务中有优势。

Conclusion: 提出的图GOSPA度量族是有效且实用的，可用于图距离测量和分类任务。

Abstract: This paper proposes a family of graph metrics for measuring distances between
graphs of different sizes. The proposed metric family defines a general form of
the graph generalised optimal sub-pattern assignment (GOSPA) metric and is also
proved to satisfy the metric properties. Similarly to the graph GOSPA metric,
the proposed graph GOSPA metric family also penalises the node attribute costs
for assigned nodes between the two graphs, and the number of unassigned nodes.
However, the proposed family of metrics provides more general penalties for
edge mismatches than the graph GOSPA metric. This paper also shows that the
graph GOSPA metric family can be approximately computed using linear
programming. Simulation experiments are performed to illustrate the
characteristics of the proposed graph GOSPA metric family with different
choices of hyperparameters. The benefits of the proposed graph GOSPA metric
family for classification tasks are also shown on real-world datasets.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [550] [OmniESI: A unified framework for enzyme-substrate interaction prediction with progressive conditional deep learning](https://arxiv.org/abs/2506.17963)
*Zhiwei Nie,Hongyu Zhang,Hao Jiang,Yutian Liu,Xiansong Huang,Fan Xu,Jie Fu,Zhixiang Ren,Yonghong Tian,Wen-Bin Zhang,Jie Chen*

Main category: q-bio.BM

TL;DR: 介绍OmniESI框架用于酶 - 底物相互作用预测，性能超现有方法，是统一有效工具。


<details>
  <summary>Details</summary>
Motivation: 现有预测方法未结合酶催化先验知识来合理调整与催化模式不一致的蛋白质 - 分子特征。

Method: 引入两阶段渐进框架OmniESI，通过条件深度学习进行预测，分解为两阶段过程，包含两个条件网络。

Result: 在七个基准测试中，OmniESI性能优于现有方法；关键组件消融实验表明条件网络提升性能且参数增加可忽略。

Conclusion: OmniESI是统一预测方法，为催化机制研究和酶工程提供有效工具，泛化性和适用性强。

Abstract: Understanding and modeling enzyme-substrate interactions is crucial for
catalytic mechanism research, enzyme engineering, and metabolic engineering.
Although a large number of predictive methods have emerged, they do not
incorporate prior knowledge of enzyme catalysis to rationally modulate general
protein-molecule features that are misaligned with catalytic patterns. To
address this issue, we introduce a two-stage progressive framework, OmniESI,
for enzyme-substrate interaction prediction through conditional deep learning.
By decomposing the modeling of enzyme-substrate interactions into a two-stage
progressive process, OmniESI incorporates two conditional networks that
respectively emphasize enzymatic reaction specificity and crucial
catalysis-related interactions, facilitating a gradual feature modulation in
the latent space from general protein-molecule domain to catalysis-aware
domain. On top of this unified architecture, OmniESI can adapt to a variety of
downstream tasks, including enzyme kinetic parameter prediction,
enzyme-substrate pairing prediction, enzyme mutational effect prediction, and
enzymatic active site annotation. Under the multi-perspective performance
evaluation of in-distribution and out-of-distribution settings, OmniESI
consistently delivered superior performance than state-of-the-art specialized
methods across seven benchmarks. More importantly, the proposed conditional
networks were shown to internalize the fundamental patterns of catalytic
efficiency while significantly improving prediction performance, with only
negligible parameter increases (0.16%), as demonstrated by ablation studies on
key components. Overall, OmniESI represents a unified predictive approach for
enzyme-substrate interactions, providing an effective tool for catalytic
mechanism cracking and enzyme engineering with strong generalization and broad
applicability.

</details>


### [551] [AbRank: A Benchmark Dataset and Metric-Learning Framework for Antibody-Antigen Affinity Ranking](https://arxiv.org/abs/2506.17857)
*Chunan Liu,Aurelien Pelissier,Yanjun Shao,Lilian Denzler,Andrew C. R. Martin,Brooks Paige,Mariia Rodriguez Martinez*

Main category: q-bio.BM

TL;DR: 提出AbRank基准框架和WALLE - Affinity方法，揭示现有方法局限，表明基于排名训练可提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前抗体 - 抗原结合亲和力预测模型受噪声标签、异质条件和泛化性差等问题限制，需要改进。

Method: 引入AbRank基准框架，聚合多源数据，定义m - 自信排名框架；提出基于图的WALLE - Affinity方法。

Result: 现有方法在实际泛化设置中有显著局限，基于排名的训练提高了鲁棒性和可迁移性。

Conclusion: AbRank为机器学习模型在抗体 - 抗原空间泛化提供了坚实基础，与抗体治疗设计相关。

Abstract: Accurate prediction of antibody-antigen (Ab-Ag) binding affinity is essential
for therapeutic design and vaccine development, yet the performance of current
models is limited by noisy experimental labels, heterogeneous assay conditions,
and poor generalization across the vast antibody and antigen sequence space. We
introduce AbRank, a large-scale benchmark and evaluation framework that
reframes affinity prediction as a pairwise ranking problem. AbRank aggregates
over 380,000 binding assays from nine heterogeneous sources, spanning diverse
antibodies, antigens, and experimental conditions, and introduces standardized
data splits that systematically increase distribution shift, from local
perturbations such as point mutations to broad generalization across novel
antigens and antibodies. To ensure robust supervision, AbRank defines an
m-confident ranking framework by filtering out comparisons with marginal
affinity differences, focusing training on pairs with at least an m-fold
difference in measured binding strength. As a baseline for the benchmark, we
introduce WALLE-Affinity, a graph-based approach that integrates protein
language model embeddings with structural information to predict pairwise
binding preferences. Our benchmarks reveal significant limitations in current
methods under realistic generalization settings and demonstrate that
ranking-based training improves robustness and transferability. In summary,
AbRank offers a robust foundation for machine learning models to generalize
across the antibody-antigen space, with direct relevance for scalable,
structure-aware antibody therapeutic design.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [552] [Greedy Selection under Independent Increments: A Toy Model Analysis](https://arxiv.org/abs/2506.17941)
*Huitao Yang*

Main category: math.PR

TL;DR: 研究N个独立同分布离散时间随机过程的迭代选择问题，证明选最终最大值过程的最优策略是每阶段贪心选择。


<details>
  <summary>Details</summary>
Motivation: 为多阶段淘汰场景中的贪心启发式方法提供理论依据，为理解高维应用中的相关算法提供示例。

Method: 在给定简单模型下进行理论推导。

Result: 证明了选择最终最大值过程的最优策略是每阶段进行贪心选择。

Conclusion: 虽结果依赖强独立性假设，但能为贪心启发式方法提供解释，可作为理解相关算法的示例。

Abstract: We study an iterative selection problem over N i.i.d. discrete-time
stochastic processes with independent increments. At each stage, a fixed number
of processes are retained based on their observed values. Under this simple
model, we prove that the optimal strategy for selecting the final maximum-value
process is to apply greedy selection at each stage. While the result relies on
strong independence assumptions, it offers a clean justification for greedy
heuristics in multi-stage elimination settings and may serve as a toy example
for understanding related algorithms in high-dimensional applications.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [553] [Zero-Shot Cognitive Impairment Detection from Speech Using AudioLLM](https://arxiv.org/abs/2506.17351)
*Mostafa Shahin,Beena Ahmed,Julien Epps*

Main category: cs.SD

TL;DR: 提出基于Qwen2 - Audio AudioLLM的零样本语音认知障碍检测方法，在多数据集评估，性能与监督方法相当，有良好泛化性。


<details>
  <summary>Details</summary>
Motivation: 传统认知障碍检测方法依赖监督模型，需手动标注且跨数据集和语言泛化性差，需新方法。

Method: 使用Qwen2 - Audio AudioLLM，设计基于提示的指令对语音样本分类。

Result: 零样本AudioLLM方法性能与监督方法相当，在语言、任务和数据集上有良好泛化性和一致性。

Conclusion: 基于AudioLLM的零样本语音认知障碍检测方法可行且有效。

Abstract: Cognitive impairment (CI) is of growing public health concern, and early
detection is vital for effective intervention. Speech has gained attention as a
non-invasive and easily collectible biomarker for assessing cognitive decline.
Traditional CI detection methods typically rely on supervised models trained on
acoustic and linguistic features extracted from speech, which often require
manual annotation and may not generalise well across datasets and languages. In
this work, we propose the first zero-shot speech-based CI detection method
using the Qwen2- Audio AudioLLM, a model capable of processing both audio and
text inputs. By designing prompt-based instructions, we guide the model in
classifying speech samples as indicative of normal cognition or cognitive
impairment. We evaluate our approach on two datasets: one in English and
another multilingual, spanning different cognitive assessment tasks. Our
results show that the zero-shot AudioLLM approach achieves performance
comparable to supervised methods and exhibits promising generalizability and
consistency across languages, tasks, and datasets.

</details>


### [554] [From Generality to Mastery: Composer-Style Symbolic Music Generation via Large-Scale Pre-training](https://arxiv.org/abs/2506.17497)
*Mingyang Yao,Ke Chen*

Main category: cs.SD

TL;DR: 本文研究利用通用音乐知识提升特定作曲家风格钢琴作品生成，采用两阶段训练范式，实验表明方法优于对比，还观察了模型构建音乐概念过程。


<details>
  <summary>Details</summary>
Motivation: 可控符号音乐生成中，某些控制方式存在数据稀缺问题，如作曲家风格音乐生成，可用作品少限制风格和音乐元素建模。

Method: 采用两阶段训练范式，先在大型音乐语料库上预训练REMI音乐生成模型，再用轻量级适配器模块在小数据集上微调，以风格指标为条件。

Result: 方法在风格准确性和音乐性的主客观评估中均优于消融实验和基线模型，实现更精确的风格建模和更好的音乐美学。

Conclusion: 通用音乐知识能增强对特定作曲家风格的掌握，可实现更好的作曲家风格音乐生成，还能观察到模型构建音乐概念和细化风格理解的过程。

Abstract: Despite progress in controllable symbolic music generation, data scarcity
remains a challenge for certain control modalities. Composer-style music
generation is a prime example, as only a few pieces per composer are available,
limiting the modeling of both styles and fundamental music elements (e.g.,
melody, chord, rhythm). In this paper, we investigate how general music
knowledge learned from a broad corpus can enhance the mastery of specific
composer styles, with a focus on piano piece generation. Our approach follows a
two-stage training paradigm. First, we pre-train a REMI-based music generation
model on a large corpus of pop, folk, and classical music. Then, we fine-tune
it on a small, human-verified dataset from four renowned composers, namely
Bach, Mozart, Beethoven, and Chopin, using a lightweight adapter module to
condition the model on style indicators. To evaluate the effectiveness of our
approach, we conduct both objective and subjective evaluations on style
accuracy and musicality. Experimental results demonstrate that our method
outperforms ablations and baselines, achieving more precise composer-style
modeling and better musical aesthetics. Additionally, we provide observations
on how the model builds music concepts from the generality pre-training and
refines its stylistic understanding through the mastery fine-tuning.

</details>


### [555] [CultureMERT: Continual Pre-Training for Cross-Cultural Music Representation Learning](https://arxiv.org/abs/2506.17818)
*Angelos-Nikolaos Kanatas,Charilaos Papaioannou,Alexandros Potamianos*

Main category: cs.SD

TL;DR: 介绍CultureMERT - 95M模型增强跨文化音乐表征学习，提出两阶段预训练策略，有良好结果，还研究任务算术方法，公开模型促进相关研究。


<details>
  <summary>Details</summary>
Motivation: 现有音乐基础模型在不同音乐传统中的有效性有限，需要增强跨文化音乐表征学习和理解。

Method: 提出两阶段持续预训练策略，集成学习率重升温与重衰减；研究任务算术方法，在权重空间合并单文化适应模型。

Result: 在多文化数据上训练，非西方音乐自动标注任务平均提升4.9%，超越现有技术，西方基准遗忘少；任务算术方法表现相当，无回归；多文化适应模型整体表现最佳。

Conclusion: 公开CultureMERT - 95M和CultureMERT - TA - 95M，有助于开发更具文化意识的音乐基础模型。

Abstract: Recent advances in music foundation models have improved audio representation
learning, yet their effectiveness across diverse musical traditions remains
limited. We introduce CultureMERT-95M, a multi-culturally adapted foundation
model developed to enhance cross-cultural music representation learning and
understanding. To achieve this, we propose a two-stage continual pre-training
strategy that integrates learning rate re-warming and re-decaying, enabling
stable adaptation even with limited computational resources. Training on a
650-hour multi-cultural data mix, comprising Greek, Turkish, and Indian music
traditions, results in an average improvement of 4.9% in ROC-AUC and AP across
diverse non-Western music auto-tagging tasks, surpassing prior
state-of-the-art, with minimal forgetting on Western-centric benchmarks. We
further investigate task arithmetic, an alternative approach to multi-cultural
adaptation that merges single-culture adapted models in the weight space. Task
arithmetic performs on par with our multi-culturally trained model on
non-Western auto-tagging tasks and shows no regression on Western datasets.
Cross-cultural evaluation reveals that single-culture models transfer with
varying effectiveness across musical traditions, whereas the multi-culturally
adapted model achieves the best overall performance. To support research on
world music representation learning, we publicly release CultureMERT-95M and
CultureMERT-TA-95M, fostering the development of more culturally aware music
foundation models.

</details>


### [556] [Adaptive Control Attention Network for Underwater Acoustic Localization and Domain Adaptation](https://arxiv.org/abs/2506.17409)
*Quoc Thinh Vo,Joe Woods,Priontu Chowdhury,David K. Han*

Main category: cs.SD

TL;DR: 本文提出多分支网络架构用于水下声源定位，结合CNN和Conformers，采用特定特征输入与AGC层，跨域测试表现优于SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 海洋环境复杂动态，高背景噪音、不规则水下几何形状和变化的声学特性使水下声源定位困难，需新方法解决。

Method: 提出多分支网络架构，利用CNN提取空间特征，结合Conformers捕捉时间依赖，使用Log - mel spectrogram和GCC - PHAT特征作为输入，引入AGC层调整特征幅度，进行跨域训练测试。

Result: 所提方法在类似设置中优于当前最先进方法，为水下声源定位建立了新基准。

Conclusion: 所提多分支网络架构能有效解决水下声源定位难题，性能表现出色。

Abstract: Localizing acoustic sound sources in the ocean is a challenging task due to
the complex and dynamic nature of the environment. Factors such as high
background noise, irregular underwater geometries, and varying acoustic
properties make accurate localization difficult. To address these obstacles, we
propose a multi-branch network architecture designed to accurately predict the
distance between a moving acoustic source and a receiver, tested on real-world
underwater signal arrays. The network leverages Convolutional Neural Networks
(CNNs) for robust spatial feature extraction and integrates Conformers with
self-attention mechanism to effectively capture temporal dependencies. Log-mel
spectrogram and generalized cross-correlation with phase transform (GCC-PHAT)
features are employed as input representations. To further enhance the model
performance, we introduce an Adaptive Gain Control (AGC) layer, that adaptively
adjusts the amplitude of input features, ensuring consistent energy levels
across varying ranges, signal strengths, and noise conditions. We assess the
model's generalization capability by training it in one domain and testing it
in a different domain, using only a limited amount of data from the test domain
for fine-tuning. Our proposed method outperforms state-of-the-art (SOTA)
approaches in similar settings, establishing new benchmarks for underwater
sound localization.

</details>


### [557] [AI-Generated Song Detection via Lyrics Transcripts](https://arxiv.org/abs/2506.18488)
*Markus Frohmann,Elena V. Epure,Gabriel Meseguer-Brocal,Markus Schedl,Romain Hennequin*

Main category: cs.SD

TL;DR: 本文提出用通用自动语音识别模型转录歌曲来检测AI生成音乐，实验显示检测性能好且更鲁棒，代码开源。


<details>
  <summary>Details</summary>
Motivation: 当前基于音频的检测器难以泛化，且利用歌词数据库检测在实际中不可行，存在检测AI生成音乐的方法缺口。

Method: 使用通用自动语音识别模型转录歌曲，并结合多个检测器进行检测。

Result: 在多语言、多风格歌词上检测性能好，在音频受干扰和不同音乐生成器上比现有音频检测器更鲁棒。

Conclusion: 提出的基于转录歌词的方法能有效解决检测AI生成音乐的问题，且具有良好的鲁棒性。

Abstract: The recent rise in capabilities of AI-based music generation tools has
created an upheaval in the music industry, necessitating the creation of
accurate methods to detect such AI-generated content. This can be done using
audio-based detectors; however, it has been shown that they struggle to
generalize to unseen generators or when the audio is perturbed. Furthermore,
recent work used accurate and cleanly formatted lyrics sourced from a lyrics
provider database to detect AI-generated music. However, in practice, such
perfect lyrics are not available (only the audio is); this leaves a substantial
gap in applicability in real-life use cases. In this work, we instead propose
solving this gap by transcribing songs using general automatic speech
recognition (ASR) models. We do this using several detectors. The results on
diverse, multi-genre, and multi-lingual lyrics show generally strong detection
performance across languages and genres, particularly for our best-performing
model using Whisper large-v2 and LLM2Vec embeddings. In addition, we show that
our method is more robust than state-of-the-art audio-based ones when the audio
is perturbed in different ways and when evaluated on different music
generators. Our code is available at
https://github.com/deezer/robust-AI-lyrics-detection.

</details>


### [558] [Smooth Operators: LLMs Translating Imperfect Hints into Disfluency-Rich Transcripts](https://arxiv.org/abs/2506.18510)
*Duygu Altinok*

Main category: cs.SD

TL;DR: 利用大语言模型提出转录不流畅现象的新方法，实验表明有时间戳线索时模型处理不完美输入很稳健。


<details>
  <summary>Details</summary>
Motivation: 准确检测口语中的不流畅现象对提升自动语音和语言处理系统性能及推动相关技术发展至关重要。

Method: 提出将不流畅现象转录为带时间戳的显式标记的新方法，整合音频编码器提取的声学表征与不同质量的文本输入。

Result: 实验表明文本输入无需完美，只要有时间戳相关线索，大语言模型就能有效平滑输入并生成带不流畅标注的完整转录。

Conclusion: 大语言模型在处理不完美线索方面具有很强的稳健性。

Abstract: Accurate detection of disfluencies in spoken language is crucial for
enhancing the performance of automatic speech and language processing systems,
as well as fostering the development of more inclusive speech and language
technologies. Leveraging the growing trend of large language models (LLMs) as
versatile learners capable of processing both lexical and non-lexical inputs
(e.g., audio and video), we propose a novel approach to transcribing
disfluencies as explicit tokens with timestamps, enabling the generation of
fully annotated disfluency-rich transcripts. Our method integrates acoustic
representations extracted from an audio encoder with textual inputs of varying
quality: clean transcriptions without disfluencies, time-aligned transcriptions
from aligners, or outputs from phoneme-based ASR models -- all of which may
contain imperfections. Importantly, our experiments demonstrate that textual
inputs do not need to be flawless. As long as they include timestamp-related
cues, LLMs can effectively smooth the input and produce fully
disfluency-annotated transcripts, underscoring their robustness in handling
imperfect hints.

</details>


### [559] [Frequency-Weighted Training Losses for Phoneme-Level DNN-based Speech Enhancement](https://arxiv.org/abs/2506.18714)
*Nasser-Eddine Monir,Paul Magron,Romain Serizel*

Main category: cs.SD

TL;DR: 提出感知信息的SDR损失变体，用于多通道语音增强，实验显示感知频率加权指标有显著改善，辅音重建更好。


<details>
  <summary>Details</summary>
Motivation: 传统训练损失函数如SDR可能无法保留对音素可懂度至关重要的细粒度频谱线索。

Method: 提出时频域的SDR损失变体，用频率相关加权方案调制，研究固定和自适应策略，用不同损失训练FaSNet模型。

Result: 标准指标如SDR仅有轻微改善，感知频率加权指标有显著提升，频谱和音素分析显示辅音重建更好。

Conclusion: 提出的感知信息SDR损失变体有助于更好地保留某些声学线索。

Abstract: Recent advances in deep learning have significantly improved multichannel
speech enhancement algorithms, yet conventional training loss functions such as
the scale-invariant signal-to-distortion ratio (SDR) may fail to preserve
fine-grained spectral cues essential for phoneme intelligibility. In this work,
we propose perceptually-informed variants of the SDR loss, formulated in the
time-frequency domain and modulated by frequency-dependent weighting schemes.
These weights are designed to emphasize time-frequency regions where speech is
prominent or where the interfering noise is particularly strong. We investigate
both fixed and adaptive strategies, including ANSI band-importance weights,
spectral magnitude-based weighting, and dynamic weighting based on the relative
amount of speech and noise. We train the FaSNet multichannel speech enhancement
model using these various losses. Experimental results show that while standard
metrics such as the SDR are only marginally improved, their perceptual
frequency-weighted counterparts exhibit a more substantial improvement.
Besides, spectral and phoneme-level analysis indicates better consonant
reconstruction, which points to a better preservation of certain acoustic cues.

</details>


### [560] [MuseControlLite: Multifunctional Music Generation with Lightweight Conditioners](https://arxiv.org/abs/2506.18729)
*Fang-Duo Tsai,Shih-Lun Wu,Weijaw Lee,Sheng-Ping Yang,Bo-Rui Chen,Hao-Chung Cheng,Yi-Hsuan Yang*

Main category: cs.SD

TL;DR: 提出轻量级机制MuseControlLite微调文本到音乐生成模型，利用旋转位置嵌入提升控制精度，成本更低。


<details>
  <summary>Details</summary>
Motivation: 现有文本到音乐生成模型在精确调节方面存在不足，需更好方法实现精确控制。

Method: 提出MuseControlLite机制，在解耦交叉注意力层添加旋转位置嵌入。

Result: 控制精度从56.6%提升到61.1%，所需可训练参数比现有微调机制少6.75倍，在多种任务上比MusicGen - Large和Stable Audio Open ControlNet可控性更好且微调成本低。

Conclusion: MuseControlLite能以较低成本实现文本到音乐生成模型的精确控制。

Abstract: We propose MuseControlLite, a lightweight mechanism designed to fine-tune
text-to-music generation models for precise conditioning using various
time-varying musical attributes and reference audio signals. The key finding is
that positional embeddings, which have been seldom used by text-to-music
generation models in the conditioner for text conditions, are critical when the
condition of interest is a function of time. Using melody control as an
example, our experiments show that simply adding rotary positional embeddings
to the decoupled cross-attention layers increases control accuracy from 56.6%
to 61.1%, while requiring 6.75 times fewer trainable parameters than
state-of-the-art fine-tuning mechanisms, using the same pre-trained diffusion
Transformer model of Stable Audio Open. We evaluate various forms of musical
attribute control, audio inpainting, and audio outpainting, demonstrating
improved controllability over MusicGen-Large and Stable Audio Open ControlNet
at a significantly lower fine-tuning cost, with only 85M trainble parameters.
Source code, model checkpoints, and demo examples are available at: https:
//MuseControlLite.github.io/web/.

</details>
