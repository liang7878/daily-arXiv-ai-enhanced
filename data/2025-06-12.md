<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 12]
- [cs.CE](#cs.CE) [Total: 4]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.GT](#cs.GT) [Total: 2]
- [cs.IR](#cs.IR) [Total: 5]
- [cs.LG](#cs.LG) [Total: 95]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.SE](#cs.SE) [Total: 15]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [stat.ML](#stat.ML) [Total: 7]
- [stat.CO](#stat.CO) [Total: 2]
- [cs.DL](#cs.DL) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.CY](#cs.CY) [Total: 3]
- [cs.CV](#cs.CV) [Total: 47]
- [eess.AS](#eess.AS) [Total: 1]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.CR](#cs.CR) [Total: 4]
- [cs.RO](#cs.RO) [Total: 11]
- [math.OC](#math.OC) [Total: 1]
- [stat.ME](#stat.ME) [Total: 2]
- [math.NA](#math.NA) [Total: 1]
- [cs.MA](#cs.MA) [Total: 3]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.NI](#cs.NI) [Total: 2]
- [eess.SP](#eess.SP) [Total: 6]
- [eess.IV](#eess.IV) [Total: 3]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.CL](#cs.CL) [Total: 32]
- [cs.HC](#cs.HC) [Total: 3]
- [q-bio.QM](#q-bio.QM) [Total: 3]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.SD](#cs.SD) [Total: 5]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.GR](#cs.GR) [Total: 3]
- [eess.SY](#eess.SY) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Robot-Gated Interactive Imitation Learning with Adaptive Intervention Mechanism](https://arxiv.org/abs/2506.09176)
*Haoyuan Cai,Zhenghao Peng,Bolei Zhou*

Main category: cs.AI

TL;DR: 提出AIM算法降低交互式模仿学习中人类监督认知需求，实验显示其减少专家监测工作，提升效率，还能识别关键状态。


<details>
  <summary>Details</summary>
Motivation: 当前交互式模仿学习方法对人类监督者认知要求高。

Method: 提出自适应干预机制（AIM），利用代理Q函数模仿人类干预规则，根据代理与人类行动的一致性调整干预请求。

Result: 在连续和离散控制任务中显著减少专家监测工作，相比基线方法，人类接管成本和学习效率提升40%，能识别关键状态收集高质量示范。

Conclusion: AIM算法能有效降低人类监督认知需求，提升学习效率，减少专家数据和环境交互。

Abstract: Interactive Imitation Learning (IIL) allows agents to acquire desired
behaviors through human interventions, but current methods impose high
cognitive demands on human supervisors. We propose the Adaptive Intervention
Mechanism (AIM), a novel robot-gated IIL algorithm that learns an adaptive
criterion for requesting human demonstrations. AIM utilizes a proxy Q-function
to mimic the human intervention rule and adjusts intervention requests based on
the alignment between agent and human actions. By assigning high Q-values when
the agent deviates from the expert and decreasing these values as the agent
becomes proficient, the proxy Q-function enables the agent to assess the
real-time alignment with the expert and request assistance when needed. Our
expert-in-the-loop experiments reveal that AIM significantly reduces expert
monitoring efforts in both continuous and discrete control tasks. Compared to
the uncertainty-based baseline Thrifty-DAgger, our method achieves a 40%
improvement in terms of human take-over cost and learning efficiency.
Furthermore, AIM effectively identifies safety-critical states for expert
assistance, thereby collecting higher-quality expert demonstrations and
reducing overall expert data and environment interactions needed. Code and demo
video are available at https://github.com/metadriverse/AIM.

</details>


### [2] [Comment on The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity](https://arxiv.org/abs/2506.09250)
*C. Opus,A. Lawsen*

Main category: cs.AI

TL;DR: 论文指出Shojaee等人关于大推理模型存在‘准确性崩溃’的结论源于实验设计缺陷，控制这些因素后模型表现良好，强调评估AI推理能力需精心设计实验。


<details>
  <summary>Details</summary>
Motivation: 质疑Shojaee等人关于大推理模型存在‘准确性崩溃’的结论，认为其结论受实验设计局限影响。

Method: 分析Shojaee等人实验存在的三个关键问题，通过控制实验人为因素（请求生成函数而非详尽移动列表）进行初步实验。

Result: 在控制实验人为因素后，多个模型在先前被认为完全失败的汉诺塔实例上表现出高准确性。

Conclusion: 评估AI推理能力时，精心设计实验至关重要。

Abstract: Shojaee et al. (2025) report that Large Reasoning Models (LRMs) exhibit
"accuracy collapse" on planning puzzles beyond certain complexity thresholds.
We demonstrate that their findings primarily reflect experimental design
limitations rather than fundamental reasoning failures. Our analysis reveals
three critical issues: (1) Tower of Hanoi experiments systematically exceed
model output token limits at reported failure points, with models explicitly
acknowledging these constraints in their outputs; (2) The authors' automated
evaluation framework fails to distinguish between reasoning failures and
practical constraints, leading to misclassification of model capabilities; (3)
Most concerningly, their River Crossing benchmarks include mathematically
impossible instances for N > 5 due to insufficient boat capacity, yet models
are scored as failures for not solving these unsolvable problems. When we
control for these experimental artifacts, by requesting generating functions
instead of exhaustive move lists, preliminary experiments across multiple
models indicate high accuracy on Tower of Hanoi instances previously reported
as complete failures. These findings highlight the importance of careful
experimental design when evaluating AI reasoning capabilities.

</details>


### [3] [Ming-Omni: A Unified Multimodal Model for Perception and Generation](https://arxiv.org/abs/2506.09344)
*Inclusion AI,Biao Gong,Cheng Zou,Chuanyang Zheng,Chunluan Zhou,Canxiang Yan,Chunxiang Jin,Chunjie Shen,Dandan Zheng,Fudong Wang,Furong Xu,GuangMing Yao,Jun Zhou,Jingdong Chen,Jianxin Sun,Jiajia Liu,Jianjiang Zhu,Jun Peng,Kaixiang Ji,Kaiyou Song,Kaimeng Ren,Libin Wang,Lixiang Ru,Lele Xie,Longhua Tan,Lyuxin Xue,Lan Wang,Mochen Bai,Ning Gao,Pei Chen,Qingpei Guo,Qinglong Zhang,Qiang Xu,Rui Liu,Ruijie Xiong,Sirui Gao,Tinghao Liu,Taisong Li,Weilong Chai,Xinyu Xiao,Xiaomei Wang,Xiaoxue Chen,Xiao Lu,Xiaoyu Li,Xingning Dong,Xuzheng Yu,Yi Yuan,Yuting Gao,Yunxiao Sun,Yipeng Chen,Yifei Wu,Yongjie Lyu,Ziping Ma,Zipeng Feng,Zhijiang Fang,Zhihao Qiu,Ziyuan Huang,Zhengyu He*

Main category: cs.AI

TL;DR: 提出统一多模态模型Ming - Omni，能处理多模态数据并支持生成任务，实验证明其有效，且为开源模型。


<details>
  <summary>Details</summary>
Motivation: 构建一个能高效处理和融合多模态输入，支持多种任务且无需单独模型、特定微调或结构重新设计的统一多模态模型。

Method: 使用专用编码器提取不同模态的令牌，用带新提出的特定模态路由器的MoE架构Ling处理，集成高级音频解码器和Ming - Lite - Uni用于生成。

Result: 实验表明Ming - Omni为跨所有模态的统一感知和生成提供了强大解决方案。

Conclusion: Ming - Omni是首个已知在模态支持上匹配GPT - 4o的开源模型，释放代码和权重以推动社区研究。

Abstract: We propose Ming-Omni, a unified multimodal model capable of processing
images, text, audio, and video, while demonstrating strong proficiency in both
speech and image generation. Ming-Omni employs dedicated encoders to extract
tokens from different modalities, which are then processed by Ling, an MoE
architecture equipped with newly proposed modality-specific routers. This
design enables a single model to efficiently process and fuse multimodal inputs
within a unified framework, thereby facilitating diverse tasks without
requiring separate models, task-specific fine-tuning, or structural redesign.
Importantly, Ming-Omni extends beyond conventional multimodal models by
supporting audio and image generation. This is achieved through the integration
of an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for
high-quality image generation, which also allow the model to engage in
context-aware chatting, perform text-to-speech conversion, and conduct
versatile image editing. Our experimental results showcase Ming-Omni offers a
powerful solution for unified perception and generation across all modalities.
Notably, our proposed Ming-Omni is the first open-source model we are aware of
to match GPT-4o in modality support, and we release all code and model weights
to encourage further research and development in the community.

</details>


### [4] [Beyond Nash Equilibrium: Bounded Rationality of LLMs and humans in Strategic Decision-making](https://arxiv.org/abs/2506.09390)
*Kehan Zheng,Jinfeng Zhou,Hongning Wang*

Main category: cs.AI

TL;DR: 研究对比大语言模型（LLMs）和人类在策略决策中的行为，发现LLMs有类似人类的有限理性，但更僵化，需改进训练方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于策略决策，常偏离完全理性，研究其是否有类似人类的有限理性。

Method: 采用行为博弈论研究的实验范式，聚焦石头剪刀布和囚徒困境两个策略游戏。

Result: LLMs重现人类启发式策略，但应用更僵化，对游戏环境动态变化敏感度低，推理模型在自适应场景中难寻有效策略。

Conclusion: 当前LLMs仅体现部分类人有限理性，需鼓励灵活对手建模和更强上下文感知的训练方法。

Abstract: Large language models are increasingly used in strategic decision-making
settings, yet evidence shows that, like humans, they often deviate from full
rationality. In this study, we compare LLMs and humans using experimental
paradigms directly adapted from behavioral game-theory research. We focus on
two well-studied strategic games, Rock-Paper-Scissors and the Prisoner's
Dilemma, which are well known for revealing systematic departures from rational
play in human subjects. By placing LLMs in identical experimental conditions,
we evaluate whether their behaviors exhibit the bounded rationality
characteristic of humans. Our findings show that LLMs reproduce familiar human
heuristics, such as outcome-based strategy switching and increased cooperation
when future interaction is possible, but they apply these rules more rigidly
and demonstrate weaker sensitivity to the dynamic changes in the game
environment. Model-level analyses reveal distinctive architectural signatures
in strategic behavior, and even reasoning models sometimes struggle to find
effective strategies in adaptive situations. These results indicate that
current LLMs capture only a partial form of human-like bounded rationality and
highlight the need for training methods that encourage flexible opponent
modeling and stronger context awareness.

</details>


### [5] [A Call for Collaborative Intelligence: Why Human-Agent Systems Should Precede AI Autonomy](https://arxiv.org/abs/2506.09420)
*Henry Peng Zou,Wei-Chieh Huang,Yaozu Wu,Chunyu Miao,Dongyuan Li,Aiwei Liu,Yue Zhou,Yankai Chen,Weizhi Zhang,Yangning Li,Liancheng Fang,Renhe Jiang,Philip S. Yu*

Main category: cs.AI

TL;DR: 本文质疑构建完全自主AI代理的路径，提出基于大语言模型的人机系统（LLM - HAS），展示人机协作优势，讨论挑战并给出解决方案，认为AI应与人协作。


<details>
  <summary>Details</summary>
Motivation: 当前完全自主AI系统存在可靠性、透明度和理解人类需求等问题，质疑此路径是否正确。

Method: 提出LLM - HAS方法，让AI与人类合作而非取代人类，通过医疗、金融和软件开发的实例说明。

Result: 表明人机协作比AI单独工作能更好地处理复杂任务。

Conclusion: AI进步应衡量其与人类协作能力，未来AI应通过有意义的伙伴关系增强人类能力。

Abstract: Recent improvements in large language models (LLMs) have led many researchers
to focus on building fully autonomous AI agents. This position paper questions
whether this approach is the right path forward, as these autonomous systems
still have problems with reliability, transparency, and understanding the
actual requirements of human. We suggest a different approach: LLM-based
Human-Agent Systems (LLM-HAS), where AI works with humans rather than replacing
them. By keeping human involved to provide guidance, answer questions, and
maintain control, these systems can be more trustworthy and adaptable. Looking
at examples from healthcare, finance, and software development, we show how
human-AI teamwork can handle complex tasks better than AI working alone. We
also discuss the challenges of building these collaborative systems and offer
practical solutions. This paper argues that progress in AI should not be
measured by how independent systems become, but by how well they can work with
humans. The most promising future for AI is not in systems that take over human
roles, but in those that enhance human capabilities through meaningful
partnership.

</details>


### [6] [Fast Monte Carlo Tree Diffusion: 100x Speedup via Parallel Sparse Planning](https://arxiv.org/abs/2506.09498)
*Jaesik Yoon,Hyeonseo Cho,Yoshua Bengio,Sungjin Ahn*

Main category: cs.AI

TL;DR: 提出Fast - MCTD改进MCTD，结合并行和稀疏技术提速，实验显示可达100倍加速且规划性能不弱。


<details>
  <summary>Details</summary>
Motivation: MCTD因树搜索的顺序性和迭代去噪成本有较大计算开销，需更高效方法。

Method: 提出Fast - MCTD，集成Parallel MCTD（通过延迟树更新和冗余感知选择实现并行展开）和Sparse MCTD（通过轨迹粗化减少展开长度）两种技术。

Result: Fast - MCTD比标准MCTD最多提速100倍，保持或提升规划性能，部分任务推理速度超Diffuser。

Conclusion: Fast - MCTD是基于扩散的推理时间推理的实用且可扩展的解决方案。

Abstract: Diffusion models have recently emerged as a powerful approach for trajectory
planning. However, their inherently non-sequential nature limits their
effectiveness in long-horizon reasoning tasks at test time. The recently
proposed Monte Carlo Tree Diffusion (MCTD) offers a promising solution by
combining diffusion with tree-based search, achieving state-of-the-art
performance on complex planning problems. Despite its strengths, our analysis
shows that MCTD incurs substantial computational overhead due to the sequential
nature of tree search and the cost of iterative denoising. To address this, we
propose Fast-MCTD, a more efficient variant that preserves the strengths of
MCTD while significantly improving its speed and scalability. Fast-MCTD
integrates two techniques: Parallel MCTD, which enables parallel rollouts via
delayed tree updates and redundancy-aware selection; and Sparse MCTD, which
reduces rollout length through trajectory coarsening. Experiments show that
Fast-MCTD achieves up to 100x speedup over standard MCTD while maintaining or
improving planning performance. Remarkably, it even outperforms Diffuser in
inference speed on some tasks, despite Diffuser requiring no search and
yielding weaker solutions. These results position Fast-MCTD as a practical and
scalable solution for diffusion-based inference-time reasoning.

</details>


### [7] [DipLLM: Fine-Tuning LLM for Strategic Decision-making in Diplomacy](https://arxiv.org/abs/2506.09655)
*Kaixuan Xu,Jiajun Chai,Sicheng Li,Yuqian Fu,Yuanheng Zhu,Dongbin Zhao*

Main category: cs.AI

TL;DR: 提出DipLLM代理解决外交游戏AI难题，用少量数据超现有模型，展示微调大语言模型潜力。


<details>
  <summary>Details</summary>
Motivation: 传统方法训练外交游戏AI耗资源，大语言模型应用有挑战，需新方法解决。

Method: 提出DipLLM，用自回归分解框架简化多单元行动分配，以均衡策略为学习目标微调模型。

Result: 仅用1.5%的Cicero模型所需数据，性能超越Cicero模型。

Conclusion: 微调大语言模型在多人游戏复杂战略决策中有潜力。

Abstract: Diplomacy is a complex multiplayer game that requires both cooperation and
competition, posing significant challenges for AI systems. Traditional methods
rely on equilibrium search to generate extensive game data for training, which
demands substantial computational resources. Large Language Models (LLMs) offer
a promising alternative, leveraging pre-trained knowledge to achieve strong
performance with relatively small-scale fine-tuning. However, applying LLMs to
Diplomacy remains challenging due to the exponential growth of possible action
combinations and the intricate strategic interactions among players. To address
this challenge, we propose DipLLM, a fine-tuned LLM-based agent that learns
equilibrium policies for Diplomacy. DipLLM employs an autoregressive
factorization framework to simplify the complex task of multi-unit action
assignment into a sequence of unit-level decisions. By defining an equilibrium
policy within this framework as the learning objective, we fine-tune the model
using only 1.5% of the data required by the state-of-the-art Cicero model,
surpassing its performance. Our results demonstrate the potential of fine-tuned
LLMs for tackling complex strategic decision-making in multiplayer games.

</details>


### [8] [Application-Driven Value Alignment in Agentic AI Systems: Survey and Perspectives](https://arxiv.org/abs/2506.09656)
*Wei Zeng,Hengshu Zhu,Chuan Qin,Han Wu,Yihang Cheng,Sirui Zhang,Xiaowei Jin,Yinuo Shen,Zhenxing Wang,Feimin Zhong,Hui Xiong*

Main category: cs.AI

TL;DR: 本文回顾特定应用场景下智能体系统的价值对齐，涵盖价值原则、应用场景、评估等方面，还探讨多智能体价值协调并提出潜在研究方向。


<details>
  <summary>Details</summary>
Motivation: AI进入智能体阶段，大模型应用风险增加，使AI智能体价值对齐受关注，因此进行相关回顾研究。

Method: 将大模型驱动的AI进展与社会治理需求结合，对价值原则进行自上而下分层组织，对应用场景从一般到具体分类回顾，系统考察价值对齐评估数据集和方法。

Result: 完成对特定应用场景下智能体系统价值对齐多方面的回顾，并探讨了多智能体价值协调。

Conclusion: 提出了该领域几个潜在的研究方向。

Abstract: The ongoing evolution of AI paradigms has propelled AI research into the
Agentic AI stage. Consequently, the focus of research has shifted from single
agents and simple applications towards multi-agent autonomous decision-making
and task collaboration in complex environments. As Large Language Models (LLMs)
advance, their applications become more diverse and complex, leading to
increasingly situational and systemic risks. This has brought significant
attention to value alignment for AI agents, which aims to ensure that an
agent's goals, preferences, and behaviors align with human values and societal
norms. This paper reviews value alignment in agent systems within specific
application scenarios. It integrates the advancements in AI driven by large
models with the demands of social governance. Our review covers value
principles, agent system application scenarios, and agent value alignment
evaluation. Specifically, value principles are organized hierarchically from a
top-down perspective, encompassing macro, meso, and micro levels. Agent system
application scenarios are categorized and reviewed from a general-to-specific
viewpoint. Agent value alignment evaluation systematically examines datasets
for value alignment assessment and relevant value alignment methods.
Additionally, we delve into value coordination among multiple agents within
agent systems. Finally, we propose several potential research directions in
this field.

</details>


### [9] [Intent Factored Generation: Unleashing the Diversity in Your Language Model](https://arxiv.org/abs/2506.09659)
*Eltayeb Ahmed,Uljad Berdica,Martha Elliott,Danijela Horak,Jakob N. Foerster*

Main category: cs.AI

TL;DR: 本文提出Intent Factored Generation (IFG)方法，将采样过程分为两步，能在保持性能的同时提高大语言模型样本多样性，且易集成到多种算法中。


<details>
  <summary>Details</summary>
Motivation: 当前增加大语言模型样本多样性的方法多在词元层面操作，导致推理问题探索不佳和对话重复，需要解决该问题。

Method: 提出IFG方法，将采样分为两步：先采样语义密集的意图，再根据原提示和意图生成最终回复；在意图步骤用高温，最终生成用低温；推理任务中让模型在每步链式思考前明确陈述意图。

Result: 该方法在多种任务中有效，提高了数学和代码任务的pass@k和基于验证器反馈的强化学习效果；结合IFG和直接偏好优化增加对话多样性且不损失奖励；在通用语言建模任务中提高多样性并保持生成质量。

Conclusion: 提出的方法简单，通过改变提示和温度就能实现，易集成到多种算法中以在不同应用中获益。

Abstract: Obtaining multiple meaningfully diverse, high quality samples from Large
Language Models for a fixed prompt remains an open challenge. Current methods
for increasing diversity often only operate at the token-level, paraphrasing
the same response. This is problematic because it leads to poor exploration on
reasoning problems and to unengaging, repetitive conversational agents. To
address this we propose Intent Factored Generation (IFG), factorising the
sampling process into two stages. First, we sample a semantically dense intent,
e.g., a summary or keywords. Second, we sample the final response conditioning
on both the original prompt and the intent from the first stage. This allows us
to use a higher temperature during the intent step to promote conceptual
diversity, and a lower temperature during the final generation to ensure the
outputs are coherent and self-consistent. Additionally, we find that prompting
the model to explicitly state its intent for each step of the chain-of-thought
before generating the step is beneficial for reasoning tasks. We demonstrate
our method's effectiveness across a diverse set of tasks. We show this method
improves both pass@k and Reinforcement Learning from Verifier Feedback on maths
and code tasks. For instruction-tuning, we combine IFG with Direct Preference
Optimisation to increase conversational diversity without sacrificing reward.
Finally, we achieve higher diversity while maintaining the quality of
generations on a general language modelling task, using a new dataset of reader
comments and news articles that we collect and open-source. In summary, we
present a simple method of increasing the sample diversity of LLMs while
maintaining performance. This method can be implemented by changing the prompt
and varying the temperature during generation, making it easy to integrate into
many algorithms for gains across various applications.

</details>


### [10] [How Do People Revise Inconsistent Beliefs? Examining Belief Revision in Humans with User Studies](https://arxiv.org/abs/2506.09977)
*Stylianos Loukas Vasileiou,Antonio Rago,Maria Vanina Martinez,William Yeoh*

Main category: cs.AI

TL;DR: 本文通过三项用户研究表明人们偏好基于解释的信念修正，这对模拟人类推理的AI系统有启示。


<details>
  <summary>Details</summary>
Motivation: 理解人类如何根据新信息修正信念，以开发能有效模拟和契合人类推理的AI系统。

Method: 开展三项综合用户研究，系统探究人们在有或无解释情况下如何修正信念。

Result: 人们始终偏好基于解释的信念修正，在不同场景下倾向于非最小化修正。

Conclusion: 模拟人类推理或与人类交互的AI系统应采用基于解释、可能非最小化的信念修正算子，以更好契合人类认知过程。

Abstract: Understanding how humans revise their beliefs in light of new information is
crucial for developing AI systems which can effectively model, and thus align
with, human reasoning. While theoretical belief revision frameworks rely on a
set of principles that establish how these operations are performed, empirical
evidence from cognitive psychology suggests that people may follow different
patterns when presented with conflicting information. In this paper, we present
three comprehensive user studies showing that people consistently prefer
explanation-based revisions, i.e., those which are guided by explanations, that
result in changes to their belief systems that are not necessarily captured by
classical belief change theory. Our experiments systematically investigate how
people revise their beliefs with explanations for inconsistencies, whether they
are provided with them or left to formulate them themselves, demonstrating a
robust preference for what may seem non-minimal revisions across different
types of scenarios. These findings have implications for AI systems designed to
model human reasoning or interact with humans, suggesting that such systems
should accommodate explanation-based, potentially non-minimal belief revision
operators to better align with human cognitive processes.

</details>


### [11] [V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning](https://arxiv.org/abs/2506.09985)
*Mido Assran,Adrien Bardes,David Fan,Quentin Garrido,Russell Howes,Mojtaba,Komeili,Matthew Muckley,Ammar Rizvi,Claire Roberts,Koustuv Sinha,Artem Zholus,Sergio Arnaud,Abha Gejji,Ada Martin,Francois Robert Hogan,Daniel Dugas,Piotr Bojanowski,Vasil Khalidov,Patrick Labatut,Francisco Massa,Marc Szafraniec,Kapil Krishnakumar,Yong Li,Xiaodong Ma,Sarath Chandar,Franziska Meier,Yann LeCun,Michael Rabbat,Nicolas Ballas*

Main category: cs.AI

TL;DR: 本文探索自监督方法结合互联网视频数据与少量交互数据开发物理世界模型，在多项任务达最优，还应用于机器人规划。


<details>
  <summary>Details</summary>
Motivation: 应对现代AI通过观察理解世界和行动的挑战。

Method: 预训练V - JEPA 2模型于超100万小时互联网视频，与大语言模型对齐；用Droid数据集未标记机器人视频对V - JEPA 2 - AC进行后训练。

Result: V - JEPA 2在运动理解、人类动作预测、视频问答任务表现优；V - JEPA 2 - AC可用于机器人零样本规划。

Conclusion: 自监督学习结合网络数据和少量机器人交互数据可得到物理世界规划模型。

Abstract: A major challenge for modern AI is to learn to understand the world and learn
to act largely by observation. This paper explores a self-supervised approach
that combines internet-scale video data with a small amount of interaction data
(robot trajectories), to develop models capable of understanding, predicting,
and planning in the physical world. We first pre-train an action-free
joint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset
comprising over 1 million hours of internet video. V-JEPA 2 achieves strong
performance on motion understanding (77.3 top-1 accuracy on Something-Something
v2) and state-of-the-art performance on human action anticipation (39.7
recall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models.
Additionally, after aligning V-JEPA 2 with a large language model, we
demonstrate state-of-the-art performance on multiple video question-answering
tasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on
TempCompass). Finally, we show how self-supervised learning can be applied to
robotic planning tasks by post-training a latent action-conditioned world
model, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the
Droid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different
labs and enable picking and placing of objects using planning with image goals.
Notably, this is achieved without collecting any data from the robots in these
environments, and without any task-specific training or reward. This work
demonstrates how self-supervised learning from web-scale data and a small
amount of robot interaction data can yield a world model capable of planning in
the physical world.

</details>


### [12] [Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering](https://arxiv.org/abs/2506.06905)
*Akash Gupta,Amos Storkey,Mirella Lapata*

Main category: cs.AI

TL;DR: 提出元学习方法解决小尺寸大模态模型上下文学习性能不稳定问题，在基准测试中表现更优。


<details>
  <summary>Details</summary>
Motivation: 大模态模型上下文学习性能不稳定，尤其是小模型，且性能不随示例增加单调提升，推测是图像嵌入额外信息干扰所致。

Method: 提出元学习方法，用固定软提示集从任务相关图像特征中提取信息，可在测试时用少量示例调整；引入注意力映射器模块，与软提示联合学习。

Result: 在VL - ICL基准测试中，该方法始终优于上下文学习和相关提示调整方法，即使图像有扰动。

Conclusion: 该方法能提升大模态模型在低数据场景下任务归纳和推理能力。

Abstract: Large Multimodal Models (LMMs) often rely on in-context learning (ICL) to
perform new tasks with minimal supervision. However, ICL performance,
especially in smaller LMMs, is inconsistent and does not always improve
monotonically with increasing examples. We hypothesize that this occurs due to
the LMM being overwhelmed by additional information present in the image
embeddings, which is not required for the downstream task. To address this, we
propose a meta-learning approach that provides an alternative for inducing
few-shot capabilities in LMMs, using a fixed set of soft prompts that are
distilled from task-relevant image features and can be adapted at test time
using a few examples. To facilitate this distillation, we introduce an
attention-mapper module that can be easily integrated with the popular LLaVA
v1.5 architecture and is jointly learned with soft prompts, enabling task
adaptation in LMMs under low-data regimes with just a few gradient steps.
Evaluation on the VL-ICL Bench shows that our method consistently outperforms
ICL and related prompt-tuning approaches, even under image perturbations,
improving task induction and reasoning across visual question answering tasks.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [13] [Transaction Categorization with Relational Deep Learning in QuickBooks](https://arxiv.org/abs/2506.09234)
*Kaiwen Dong,Padmaja Jonnalagedda,Xiang Gao,Ayan Acharya,Maria Kissa,Mauricio Flores,Nitesh V. Chawla,Kamalika Das*

Main category: cs.CE

TL;DR: 本文提出基于图的Rel - Cat模型用于QuickBooks自动交易分类，表现优于现有模型，架构简单有效可扩展且能应对冷启动问题。


<details>
  <summary>Details</summary>
Motivation: 解决QuickBooks自动交易分类中交易描述格式独特、类别多样、数据量大以及关系数据库组织数据难以构建统一模型的挑战。

Method: 开发直接基于关系数据库的Rel - Cat模型，将交易分类问题转化为图结构中的链接预测任务，集成自然语言处理和图机器学习技术。

Result: 模型表现优于QuickBooks现有生产模型，能有效扩展以适应不断增长的客户群，架构简单有效且不损失准确性。

Conclusion: Rel - Cat模型是解决QuickBooks自动交易分类问题的有效方案，还能应对冷启动问题。

Abstract: Automatic transaction categorization is crucial for enhancing the customer
experience in QuickBooks by providing accurate accounting and bookkeeping. The
distinct challenges in this domain stem from the unique formatting of
transaction descriptions, the wide variety of transaction categories, and the
vast scale of the data involved. Furthermore, organizing transaction data in a
relational database creates difficulties in developing a unified model that
covers the entire database. In this work, we develop a novel graph-based model,
named Rel-Cat, which is built directly over the relational database. We
introduce a new formulation of transaction categorization as a link prediction
task within this graph structure. By integrating techniques from natural
language processing and graph machine learning, our model not only outperforms
the existing production model in QuickBooks but also scales effectively to a
growing customer base with a simpler, more effective architecture without
compromising on accuracy. This design also helps tackle a key challenge of the
cold start problem by adapting to minimal data.

</details>


### [14] [Large Language Models for Design Structure Matrix Optimization](https://arxiv.org/abs/2506.09749)
*Shuo Jiang,Min Xie,Jianxi Luo*

Main category: cs.CE

TL;DR: 研究探索用大语言模型解决复杂工程系统中设计结构矩阵元素排序的组合优化问题，提出新框架，实验显示效果优于基线方法，证明大语言模型潜力。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法在解决设计结构矩阵元素排序的组合优化问题时，难以捕捉上下文细微差别，无法提供有效解决方案。

Method: 提出基于大语言模型的框架，将网络拓扑与上下文领域知识结合，对设计结构矩阵元素排序进行迭代优化。

Result: 在各种设计结构矩阵案例实验中，该方法收敛更快、解的质量更高，且结合上下文领域知识能显著提升优化性能。

Conclusion: 大语言模型结合语义和数学推理有解决复杂工程组合优化问题的潜力，为基于大语言模型的工程设计优化开辟新范式。

Abstract: In complex engineering systems, the interdependencies among components or
development activities are often modeled and analyzed using Design Structure
Matrix (DSM). Reorganizing elements within a DSM to minimize feedback loops and
enhance modularity or process efficiency constitutes a challenging
combinatorial optimization (CO) problem in engineering design and operations.
As problem sizes increase and dependency networks become more intricate,
traditional optimization methods that solely use mathematical heuristics often
fail to capture the contextual nuances and struggle to deliver effective
solutions. In this study, we explore the potential of Large Language Models
(LLMs) for helping solve such CO problems by leveraging their capabilities for
advanced reasoning and contextual understanding. We propose a novel LLM-based
framework that integrates network topology with contextual domain knowledge for
iterative optimization of DSM element sequencing - a common CO problem.
Experiments on various DSM cases show that our method consistently achieves
faster convergence and superior solution quality compared to both stochastic
and deterministic baselines. Notably, we find that incorporating contextual
domain knowledge significantly enhances optimization performance regardless of
the chosen LLM backbone. These findings highlight the potential of LLMs to
solve complex engineering CO problems by combining semantic and mathematical
reasoning. This approach paves the way towards a new paradigm in LLM-based
engineering design optimization.

</details>


### [15] [Intelligent Design 4.0: Paradigm Evolution Toward the Agentic AI Era](https://arxiv.org/abs/2506.09755)
*Shuo Jiang,Min Xie,Frank Youhua Chen,Jian Ma,Jianxi Luo*

Main category: cs.CE

TL;DR: 本文介绍由智能AI系统驱动的智能设计4.0，回顾智能设计历史阶段，提出概念框架，探讨其潜力与未来方向。


<details>
  <summary>Details</summary>
Motivation: 基础模型出现为工程设计带来新变革，引入智能设计4.0以推动工程设计发展。

Method: 回顾智能设计四个不同阶段的历史演变，提出智能设计4.0的概念框架。

Result: 提出概念框架，讨论其支持工程设计端到端自动化的潜力。

Conclusion: 这些见解为智能设计应对复杂挑战向更高适应性、自主性和有效性发展奠定基础。

Abstract: Research and practice in Intelligent Design (ID) have significantly enhanced
engineering innovation, efficiency, quality, and productivity over recent
decades, fundamentally reshaping how engineering designers think, behave, and
interact with design processes. The recent emergence of Foundation Models
(FMs), particularly Large Language Models (LLMs), has demonstrated general
knowledge-based reasoning capabilities, and open new paths and avenues for
further transformation in engineering design. In this context, this paper
introduces Intelligent Design 4.0 (ID 4.0) as an emerging paradigm empowered by
agentic AI systems. We review the historical evolution of ID across four
distinct stages: rule-based expert systems, task-specific machine learning
models, large-scale foundation AI models, and the recent emerging paradigm of
multi-agent collaboration. We propose a conceptual framework for ID 4.0 and
discuss its potential to support end-to-end automation of engineering design
processes through coordinated, autonomous multi-agent-based systems.
Furthermore, we discuss future perspectives to enhance and fully realize ID
4.0's potential, including more complex design scenarios, more practical design
implementations, novel agent coordination mechanisms, and autonomous design
goal-setting with better human value alignment. In sum, these insights lay a
foundation for advancing Intelligent Design toward greater adaptivity,
autonomy, and effectiveness in addressing increasingly complex design
challenges.

</details>


### [16] [Superstudent intelligence in thermodynamics](https://arxiv.org/abs/2506.09822)
*Rebecca Loubet,Pascal Zittlau,Marco Hoffmann,Luisa Vollmer,Sophie Fellenz,Heike Leitte,Fabian Jirasek,Johannes Lenhard,Hans Hasse*

Main category: cs.CE

TL;DR: OpenAI的大语言模型o3在热力学大学考试中击败所有学生，引发对工程工作和教育影响的讨论。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在复杂学术任务中的表现，评估其对人类智力能力的挑战。

Method: 将热力学考试同时给学生和OpenAI的o3模型，并以相同方式评估答案。

Result: o3在零样本模式下正确解决所有问题，成绩优于所有学生，得分处于1985年以来超10000次类似考试的最佳分数区间。

Conclusion: 这是一个转折点，机器在复杂任务上超越人类，需探讨其对工程师工作和未来工程师教育的影响。

Abstract: In this short note, we report and analyze a striking event: OpenAI's large
language model o3 has outwitted all students in a university exam on
thermodynamics. The thermodynamics exam is a difficult hurdle for most
students, where they must show that they have mastered the fundamentals of this
important topic. Consequently, the failure rates are very high, A-grades are
rare - and they are considered proof of the students' exceptional intellectual
abilities. This is because pattern learning does not help in the exam. The
problems can only be solved by knowledgeably and creatively combining
principles of thermodynamics. We have given our latest thermodynamics exam not
only to the students but also to OpenAI's most powerful reasoning model, o3,
and have assessed the answers of o3 exactly the same way as those of the
students. In zero-shot mode, the model o3 solved all problems correctly, better
than all students who took the exam; its overall score was in the range of the
best scores we have seen in more than 10,000 similar exams since 1985. This is
a turning point: machines now excel in complex tasks, usually taken as proof of
human intellectual capabilities. We discuss the consequences this has for the
work of engineers and the education of future engineers.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [17] [Terabyte-Scale Analytics in the Blink of an Eye](https://arxiv.org/abs/2506.09226)
*Bowen Wu,Wei Cui,Carlo Curino,Matteo Interlandi,Rathijit Sen*

Main category: cs.DB

TL;DR: 文章研究分布式GPU集群上分析型SQL查询扩展问题，通过构建原型实验指出有至少60倍的性能提升机会。


<details>
  <summary>Details</summary>
Motivation: AI模型的规模定律和普及促使商业数据中心部署强大GPU集群，其相比CPU有显著性能提升，研究旨在确定分布式GPU集群上分析型SQL查询性能提升上限。

Method: 构建利用ML/HPC最佳实践（如组通信原语进行跨设备数据移动）的原型，进行全面性能实验。

Result: 实验表明有至少60倍的巨大性能提升机会，系统能在短时间内以1TB规模因子运行TPC - H的22个查询。

Conclusion: 分布式GPU集群在分析型SQL查询上有巨大的性能提升潜力。

Abstract: For the past two decades, the DB community has devoted substantial research
to take advantage of cheap clusters of machines for distributed data analytics
-- we believe that we are at the beginning of a paradigm shift. The scaling
laws and popularity of AI models lead to the deployment of incredibly powerful
GPU clusters in commercial data centers. Compared to CPU-only solutions, these
clusters deliver impressive improvements in per-node compute, memory bandwidth,
and inter-node interconnect performance. In this paper, we study the problem of
scaling analytical SQL queries on distributed clusters of GPUs, with the stated
goal of establishing an upper bound on the likely performance gains. To do so,
we build a prototype designed to maximize performance by leveraging ML/HPC best
practices, such as group communication primitives for cross-device data
movements. This allows us to conduct thorough performance experimentation to
point our community towards a massive performance opportunity of at least
60$\times$. To make these gains more relatable, before you can blink twice, our
system can run all 22 queries of TPC-H at a 1TB scale factor!

</details>


### [18] [ArcNeural: A Multi-Modal Database for the Gen-AI Era](https://arxiv.org/abs/2506.09467)
*Wu Min,Qiao Yuncong,Yu Tan,Chenghu Yang*

Main category: cs.DB

TL;DR: ArcNeural提出适用于生成式AI和大语言模型的多模态数据库，性能和可扩展性佳，为企业级AI应用提供通用方案。


<details>
  <summary>Details</summary>
Motivation: 解决多模态数据处理挑战，为生成式AI时代提供智能、数据驱动解决方案。

Method: 采用存储计算分离架构，集成图技术、先进向量索引和事务处理。具备统一存储层、MemEngine自适应边缘收集等特性。

Result: 实验评估表明ArcNeural比现有系统有更优的性能和可扩展性。

Conclusion: ArcNeural可桥接结构化和非结构化数据管理，为企业级AI应用提供通用解决方案。

Abstract: ArcNeural introduces a novel multimodal database tailored for the demands of
Generative AI and Large Language Models, enabling efficient management of
diverse data types such as graphs, vectors, and documents. Its storage-compute
separated architecture integrates graph technology, advanced vector indexing,
and transaction processing to support real-time analytics and AI-driven
applications. Key features include a unified storage layer, adaptive edge
collection in MemEngine, and seamless integration of transaction and analytical
processing. Experimental evaluations demonstrate ArcNeural's superior
performance and scalability compared to state-of-the-art systems. This system
bridges structured and unstructured data management, offering a versatile
solution for enterprise-grade AI applications.
  ArcNeural's design addresses the challenges of multimodal data processing,
providing a robust framework for intelligent, data-driven solutions in the Gen
AI era.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [19] [EdgeProfiler: A Fast Profiling Framework for Lightweight LLMs on Edge Using Analytical Model](https://arxiv.org/abs/2506.09061)
*Alyssa Pinnock,Shakya Jayakody,Kawsher A Roxy,Md Rubel Ahmed*

Main category: cs.DC

TL;DR: 本文介绍EdgeProfiler框架，用于在边缘系统评估轻量级大语言模型，通过量化技术评估性能，得出量化可降低内存、提升推理速度和降低能耗等结果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型因高计算、内存和功耗需求局限于云环境，需要在资源受限的边缘环境评估其性能。

Method: 使用激进的量化技术和严格内存约束对紧凑大语言模型进行分析建模，估计延迟、FLOPs和能耗。

Result: 4位量化减少约60 - 70%模型内存，精度维持在全精度基线的2 - 5%内；推理速度比FP16基线提升2 - 3倍；INT4配置能耗降低35 - 50%。

Conclusion: 强调针对边缘环境轻量级大语言模型进行高效性能分析的重要性，需平衡准确性、能源效率和计算可行性。

Abstract: This paper introduces EdgeProfiler, a fast profiling framework designed for
evaluating lightweight Large Language Models (LLMs) on edge systems. While LLMs
offer remarkable capabilities in natural language understanding and generation,
their high computational, memory, and power requirements often confine them to
cloud environments. EdgeProfiler addresses these challenges by providing a
systematic methodology for assessing LLM performance in resource-constrained
edge settings. The framework profiles compact LLMs, including TinyLLaMA,
Gemma3.1B, Llama3.2-1B, and DeepSeek-r1-1.5B, using aggressive quantization
techniques and strict memory constraints. Analytical modeling is used to
estimate latency, FLOPs, and energy consumption. The profiling reveals that
4-bit quantization reduces model memory usage by approximately 60-70%, while
maintaining accuracy within 2-5% of full-precision baselines. Inference speeds
are observed to improve by 2-3x compared to FP16 baselines across various edge
devices. Power modeling estimates a 35-50% reduction in energy consumption for
INT4 configurations, enabling practical deployment on hardware such as
Raspberry Pi 4/5 and Jetson Orin Nano Super. Our findings emphasize the
importance of efficient profiling tailored to lightweight LLMs in edge
environments, balancing accuracy, energy efficiency, and computational
feasibility.

</details>


### [20] [Multi-GPU Acceleration of PALABOS Fluid Solver using C++ Standard Parallelism](https://arxiv.org/abs/2506.09242)
*Jonas Latt,Christophe Coreixas*

Main category: cs.DC

TL;DR: 文章介绍Palabos库GPU端口的原理、架构和性能，采用混合CPU - GPU执行模型，用现代C++开发，通过多物理基准测试验证性能良好。


<details>
  <summary>Details</summary>
Motivation: 将原基于CPU的Lattice Boltzmann软件库Palabos移植到GPU，以提升性能。

Method: 采用混合CPU - GPU执行模型，结合面向对象和面向数据两种范式，使用现代C++技术。

Result: 通过三维多物理基准测试验证了GPU版Palabos的正确性和性能，单GPU性能与CUDA原生求解器相近，多GPU测试有良好的弱和强扩展性。

Conclusion: 该设计实现了Palabos从CPU到GPU的移植，性能表现良好且与现有代码兼容。

Abstract: This article presents the principles, software architecture, and performance
analysis of the GPU port of the lattice Boltzmann software library Palabos (J.
Latt et al., "Palabos: Parallel lattice Boltzmann solver", Comput. Math. Appl.
81, 334-350, (2021)). A hybrid CPU-GPU execution model is adopted, in which
numerical components are selectively assigned to either the CPU or the GPU,
depending on considerations of performance or convenience. This design enables
a progressive porting strategy, allowing most features of the original
CPU-based codebase to be gradually and seamlessly adapted to GPU execution. The
new architecture builds upon two complementary paradigms: a classical
object-oriented structure for CPU execution, and a data-oriented counterpart
for GPUs, which reproduces the modularity of the original code while
eliminating object-oriented overhead detrimental to GPU performance. Central to
this approach is the use of modern C++, including standard parallel algorithms
and template metaprogramming techniques, which permit the generation of
hardware-agnostic computational kernels. This facilitates the development of
user-defined, GPU-accelerated components such as collision operators or
boundary conditions, while preserving compatibility with the existing codebase
and avoiding the need for external libraries or non-standard language
extensions. The correctness and performance of the GPU-enabled Palabos are
demonstrated through a series of three-dimensional multiphysics benchmarks,
including the laminar-turbulent transition in a Taylor-Green vortex, lid-driven
cavity flow, and pore-scale flow in Berea sandstone. Despite the high-level
abstraction of the implementation, the single-GPU performance is similar to
CUDA-native solvers, and multi-GPU tests exhibit good weak and strong scaling
across all test cases.

</details>


### [21] [A Survey of End-to-End Modeling for Distributed DNN Training: Workloads, Simulators, and TCO](https://arxiv.org/abs/2506.09275)
*Jonas Svedas,Hannah Watson,Nathan Laubeuf,Diksha Moolchandani,Abubakr Nada,Arjun Singh,Dwaipayan Biswas,James Myers,Debjyoti Bhattacharjee*

Main category: cs.DC

TL;DR: 本文综述分布式DNN训练模拟器，涵盖工作负载表示、模拟基础设施和TCO模型等方面，提供结构化概述以支持分布式训练系统的设计和评估决策。


<details>
  <summary>Details</summary>
Motivation: 分布式DNN模型复杂度增长远超CMOS技术扩展，需跨软件、硬件和技术层协同设计，因部署全规模训练系统成本高，模拟器在设计探索中至关重要。

Method: 从工作负载表示、模拟基础设施和TCO模型三个主要维度对分布式DNN训练模拟器进行综述，涵盖工作负载抽象、常见表示方法，并给出比较表格。

Result: 对现有工具进行综合，突出新兴趋势、常见限制和开放研究挑战。

Conclusion: 该工作提供结构化概述，支持分布式训练系统设计和评估的明智决策。

Abstract: Distributed deep neural networks (DNNs) have become a cornerstone for scaling
machine learning to meet the demands of increasingly complex applications.
However, the rapid growth in model complexity far outpaces CMOS technology
scaling, making sustainable and efficient system design a critical challenge.
Addressing this requires coordinated co-design across software, hardware, and
technology layers. Due to the prohibitive cost and complexity of deploying
full-scale training systems, simulators play a pivotal role in enabling this
design exploration. This survey reviews the landscape of distributed DNN
training simulators, focusing on three major dimensions: workload
representation, simulation infrastructure, and models for total cost of
ownership (TCO) including carbon emissions. It covers how workloads are
abstracted and used in simulation, outlines common workload representation
methods, and includes comprehensive comparison tables covering both simulation
frameworks and TCO/emissions models, detailing their capabilities, assumptions,
and areas of focus. In addition to synthesizing existing tools, the survey
highlights emerging trends, common limitations, and open research challenges
across the stack. By providing a structured overview, this work supports
informed decision-making in the design and evaluation of distributed training
systems.

</details>


### [22] [TTrace: Lightweight Error Checking and Diagnosis for Distributed Training](https://arxiv.org/abs/2506.09280)
*Haitian Jiang,Shaowei Zhu,Zhen Zhang,Zhenyu Song,Xinwei Fu,Zhen Jia,Yida Wang,Jinyang Li*

Main category: cs.DC

TL;DR: 本文提出TTrace系统，用于检测和定位分布式训练中的静默错误，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 分布式训练易出现静默错误，现有调试方法低效，定位困难，需有效检测和定位手段。

Method: 设计TTrace系统，细粒度收集分布式训练中间张量，与单设备参考实现对比，提出数学分析设置阈值区分误差。

Result: TTrace有效检测Megatron - LM框架中11个现有和3个新错误，代码改动少于10行，适用于多种训练方案。

Conclusion: TTrace能有效检测和定位分布式训练中的静默错误。

Abstract: Distributed training is essential for scaling the training of large neural
network models, such as large language models (LLMs), across thousands of GPUs.
However, the complexity of distributed training programs makes them
particularly prone to silent bugs, which do not produce explicit error signal
but lead to incorrect training outcome. Effectively detecting and localizing
such silent bugs in distributed training is challenging. Common debugging
practice using metrics like training loss or gradient norm curves can be
inefficient and ineffective. Additionally, obtaining intermediate tensor values
and determining whether they are correct during silent bug localization is
difficult, particularly in the context of low-precision training.
  To address those challenges, we design and implement TTrace, the first system
capable of detecting and localizing silent bugs in distributed training. TTrace
collects intermediate tensors from distributing training in a fine-grained
manner and compares them against those from a trusted single-device reference
implementation. To properly compare the floating-point values in the tensors,
we propose novel mathematical analysis that provides a guideline for setting
thresholds, enabling TTrace to distinguish bug-induced errors from
floating-point round-off errors. Experimental results demonstrate that TTrace
effectively detects 11 existing bugs and 3 new bugs in the widely used
Megatron-LM framework, while requiring fewer than 10 lines of code change.
TTrace is effective in various training recipes, including low-precision
recipes involving BF16 and FP8.

</details>


### [23] [ScalableHD: Scalable and High-Throughput Hyperdimensional Computing Inference on Multi-Core CPUs](https://arxiv.org/abs/2506.09282)
*Dhruv Parikh,Viktor Prasanna*

Main category: cs.DC

TL;DR: 本文提出ScalableHD用于多核CPU上可扩展且高吞吐量的HDC推理，采用两级流水线执行模型，有两种执行变体，相比基线有10倍吞吐量提升且可扩展性强。


<details>
  <summary>Details</summary>
Motivation: 传统HDC方法精度低，现有高效HDC推理研究多针对专用硬件，对通用多核CPU关注有限。

Method: 提出ScalableHD，采用两级流水线执行模型，使用生产者 - 消费者机制，集成内存分块和NUMA感知绑定，有两种执行变体。

Result: ScalableHD在多种任务上比TorchHD等基线实现高达10倍吞吐量提升，且增加核心数可实现近乎成比例的吞吐量提升。

Conclusion: ScalableHD能在多核CPU上实现可扩展且高吞吐量的HDC推理，同时保持任务精度。

Abstract: Hyperdimensional Computing (HDC) is a brain-inspired computing paradigm that
represents and manipulates information using high-dimensional vectors, called
hypervectors (HV). Traditional HDC methods, while robust to noise and
inherently parallel, rely on single-pass, non-parametric training and often
suffer from low accuracy. To address this, recent approaches adopt iterative
training of base and class HVs, typically accelerated on GPUs. Inference,
however, remains lightweight and well-suited for real-time execution. Yet,
efficient HDC inference has been studied almost exclusively on specialized
hardware such as FPGAs and GPUs, with limited attention to general-purpose
multi-core CPUs. To address this gap, we propose ScalableHD for scalable and
high-throughput HDC inference on multi-core CPUs. ScalableHD employs a
two-stage pipelined execution model, where each stage is parallelized across
cores and processes chunks of base and class HVs. Intermediate results are
streamed between stages using a producer-consumer mechanism, enabling
on-the-fly consumption and improving cache locality. To maximize performance,
ScalableHD integrates memory tiling and NUMA-aware worker-to-core binding.
Further, it features two execution variants tailored for small and large batch
sizes, each designed to exploit compute parallelism based on workload
characteristics while mitigating the memory-bound compute pattern that limits
HDC inference performance on modern multi-core CPUs. ScalableHD achieves up to
10x speedup in throughput (samples per second) over state-of-the-art baselines
such as TorchHD, across a diverse set of tasks ranging from human activity
recognition to image classification, while preserving task accuracy.
Furthermore, ScalableHD exhibits robust scalability: increasing the number of
cores yields near-proportional throughput improvements.

</details>


### [24] [SLED: A Speculative LLM Decoding Framework for Efficient Edge Serving](https://arxiv.org/abs/2506.09397)
*Xiangchen Li,Dimitrios Spatharakis,Saeid Ghafouri,Jiakun Fan,Dimitrios Nikolopoulos*

Main category: cs.DC

TL;DR: 本文介绍利用投机解码为边缘计算适配的新方法SLED，实验表明其可降低延迟、提高能效和并发推理会话数且不牺牲模型准确性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型边缘推理策略因设备内存和功耗限制，存在用准确性换效率或成本负担大的问题。

Method: 提出SLED方法，让轻量级边缘设备用不同草稿模型本地生成候选令牌，共享边缘服务器用更精确目标模型批量验证。

Result: 在Jetson Orin Nano、Raspberry Pi 5和RTX 6000边缘服务器的实验中，实现显著降低延迟、提高能源效率、增加并发推理会话数，且不牺牲模型准确性。

Conclusion: 利用投机解码的SLED方法能有效解决边缘计算中推理大语言模型的难题。

Abstract: Regardless the advancements in device capabilities, efficient inferencing
advanced large language models (LLMs) at the edge remains challenging due to
limited device memory and power constraints. Existing strategies, such as
aggressive quantization, pruning, or remote inference, trade accuracy for
efficiency or lead to substantial cost burdens. This position paper introduces
a new approach that leverages speculative decoding, previously viewed primarily
as a decoding acceleration technique for autoregressive generation of LLMs, as
a promising approach specifically adapted for edge computing by orchestrating
computation across heterogeneous devices. We propose SLED, a method that allows
lightweight edge devices to draft multiple candidate tokens locally using
diverse draft models, while a single, shared edge server efficiently batches
and verifies the tokens utilizing a more precise target model. This approach
supports device heterogeneity and reduces server-side memory footprint by
avoiding the need to deploy multiple target models. Our initial experiments
with Jetson Orin Nano, Raspberry Pi 5, and an RTX 6000 edge server indicate
substantial benefits: significantly reduced latency, improved energy
efficiency, and increased concurrent inference sessions, all without
sacrificing model accuracy.

</details>


### [25] [Efficient Task Graph Scheduling for Parallel QR Factorization in SLSQP](https://arxiv.org/abs/2506.09463)
*Soumyajit Chatterjee,Rahul Utkoor,Uppu Eshwar,Sathya Peri,V. Krishna Nandivada*

Main category: cs.DC

TL;DR: 论文提出双队列调度技术执行QR分解内核，在SLSQP算法中有效存储中间结果，性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有DAG方法缺乏对QR分解中间结果的控制，而SLSQP算法需要这些中间结果用于回代逻辑。

Method: 采用双队列方法执行QR分解内核，用C++语言实现。

Result: 经验评估显示，相比SLSQP算法的顺序QR版本，性能有10倍提升。

Conclusion: 提出的调度技术能有效执行QR分解内核，可存储中间结果，提升了算法性能。

Abstract: Efficient task scheduling is paramount in parallel programming on multi-core
architectures, where tasks are fundamental computational units. QR
factorization is a critical sub-routine in Sequential Least Squares Quadratic
Programming (SLSQP) for solving non-linear programming (NLP) problems. QR
factorization decomposes a matrix into an orthogonal matrix Q and an upper
triangular matrix R, which are essential for solving systems of linear
equations arising from optimization problems. SLSQP uses an in-place version of
QR factorization, which requires storing intermediate results for the next
steps of the algorithm. Although DAG-based approaches for QR factorization are
prevalent in the literature, they often lack control over the intermediate
kernel results, providing only the final output matrices Q and R. This
limitation is particularly challenging in SLSQP, where intermediate results of
QR factorization are crucial for back-substitution logic at each iteration. Our
work introduces novel scheduling techniques using a two-queue approach to
execute the QR factorization kernel effectively. This approach, implemented in
high-level C++ programming language, facilitates compiler optimizations and
allows storing intermediate results required by back-substitution logic.
Empirical evaluations demonstrate substantial performance gains, including a
10x improvement over the sequential QR version of the SLSQP algorithm.

</details>


### [26] [On the Performance of Cloud-based ARM SVE for Zero-Knowledge Proving Systems](https://arxiv.org/abs/2506.09505)
*Dumitrel Loghin,Shuang Liang,Shengwei Liu,Xiong Liu,Pingcheng Ruan,Zhigang Ye*

Main category: cs.DC

TL;DR: 本文探讨零知识证明（ZKP）中ARM服务器能否替代x86 - 64服务器，分析发现当前ARM CPU性能不如x86 - 64，但增大向量大小后ARM有优势。


<details>
  <summary>Details</summary>
Motivation: 随着ARM云服务器出现且更便宜，以及ARM SVE的实现，探讨ARM服务器能否替代x86 - 64服务器用于以太坊虚拟机上的零知识证明事务。

Method: 分析比较当前ARM CPU（如AWS的Graviton4和GCP的Axion）与x86 - 64服务器（如最新AMD EPYC和Intel Xeon）在构建四百万以上叶子节点的默克尔树时的性能。

Result: 当前ARM CPU性能不如x86 - 64，Graviton4和Axion分别比对应x86 - 64服务器慢1.6倍和1.4倍，原因是向量大小小和时钟频率低。

Conclusion: ARM SVE/SVE2 ISA至少和AVX/AVX512一样强大且更灵活，增大ARM CPU向量大小到512位可在保持价格优势下实现更高性能。

Abstract: Zero-knowledge proofs (ZKP) are becoming a gold standard in scaling
blockchains and bringing Web3 to life. At the same time, ZKP for transactions
running on the Ethereum Virtual Machine require powerful servers with hundreds
of CPU cores. The current zkProver implementation from Polygon is optimized for
x86-64 CPUs by vectorizing key operations, such as Merkle tree building with
Poseidon hashes over the Goldilocks field, with Advanced Vector Extensions (AVX
and AVX512). With these optimizations, a ZKP for a batch of transactions is
generated in less than two minutes. With the advent of cloud servers with ARM
which are at least 10% cheaper than x86-64 servers and the implementation of
ARM Scalable Vector Extension (SVE), we wonder if ARM servers can take over
their x86-64 counterparts. Unfortunately, our analysis shows that current ARM
CPUs are not a match for their x86-64 competitors. Graviton4 from Amazon Web
Services (AWS) and Axion from Google Cloud Platform (GCP) are 1.6X and 1.4X
slower compared to the latest AMD EPYC and Intel Xeon servers from AWS with AVX
and AVX512, respectively, when building a Merkle tree with over four million
leaves. This low performance is due to (1) smaller vector size in these ARM
CPUs (128 bits versus 512 bits in AVX512) and (2) lower clock frequency. On the
other hand, ARM SVE/SVE2 Instruction Set Architecture (ISA) is at least as
powerful as AVX/AVX512 but more flexible. Moreover, we estimate that increasing
the vector size to 512 bits will enable higher performance in ARM CPUs compared
to their x86-64 counterparts while maintaining their price advantage.

</details>


### [27] [Understanding the Performance and Power of LLM Inferencing on Edge Accelerators](https://arxiv.org/abs/2506.09554)
*Mayank Arya,Yogesh Simmhan*

Main category: cs.DC

TL;DR: 研究在NVIDIA Jetson Orin AGX上对4个SOTA大语言模型进行推理评估，探究不同因素影响及权衡问题，结果助于优化边缘加速器上LLM服务。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在关键任务和隐私敏感应用中可能需本地部署，边缘加速器是选择，但在其上进行LLM推理的可行性和性能研究不足。

Method: 在NVIDIA Jetson Orin AGX上对4个参数从27亿到328亿的SOTA模型进行推理评估，研究不同批大小、序列长度、量化级别对延迟、吞吐量和困惑度的影响，还探索不同电源模式进行功耗分析。

Result: 发现效率、推理速度和资源使用之间的权衡关系，如增加序列长度降低令牌吞吐量，量化使较小LLM变慢。

Conclusion: 研究结果有助于优化边缘加速器上大语言模型的实际应用服务。

Abstract: Large Language Models (LLMs) have demonstrated exceptional benefits to a wide
range of domains, for tasks as diverse as code generation and robot navigation.
While LLMs are usually served from cloud data centers, mission-critical and
privacy-sensitive applications may require local hosting of open LLM models.
Given the large GPU memory footprint needed for LLMs, edge accelerators such as
Nvidia Jetson Orin AGX with 64GB of shared GPU-CPU RAM are a compelling choice.
However, the feasibility and performance of LLM inference on edge accelerators
is under-explored. This study presents a detailed evaluation of LLM inference
on the NVIDIA Jetson Orin AGX, on four SOTA models ranging from 2.7B to 32.8B
parameters, such as Meta Llama3.1, Microsoft-Phi2, Deepseek-R1-Qwen.We
investigate the impact of varying batch sizes, sequence lengths, and
quantization levels on latency, throughput, and perplexity, and also explore
various custom power modes on the Orin AGX to perform power and energy
consumption analysis. Our findings offer interesting insights on the trade-offs
between efficiency, inference speed and resource use, e.g., increasing the
sequence length causes a decrease in token throughput and quantization causes
smaller LLMs to be slower. These results can help optimize LLM serving on edge
accelerators for practical applications.

</details>


### [28] [Frosty for partial synchrony](https://arxiv.org/abs/2506.09823)
*Stephen Buttolph,Andrew Lewis-Pye,Kevin Sekniqi*

Main category: cs.DC

TL;DR: 本文旨在展示如何修改Frosty以处理Snowman的部分同步版本。


<details>
  <summary>Details</summary>
Motivation: Frosty假设了强同步形式，而需要处理Snowman的部分同步版本。

Method: 未提及具体方法

Result: 未提及具体结果

Conclusion: 未提及具体结论

Abstract: Snowman is the consensus protocol used by blockchains on Avalanche. Recent
work has shown both how to augment Snowman with a `liveness' module called
`Frosty' that protects against liveness attacks, and also how to modify Snowman
so as to be consistent in partial synchrony. Since Frosty assumes (a strong
form of) synchrony, the aim of this note is to show how to modify Frosty to
deal with the partially synchronous version of Snowman.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [29] [Tight Paths and Tight Pairs in Weighted Directed Graphs](https://arxiv.org/abs/2506.09966)
*José Luis Balcázar*

Main category: cs.DS

TL;DR: 本文提出在有向边加权图中寻找紧路径及其简化问题——寻找紧对，讨论并比较了几种算法。


<details>
  <summary>Details</summary>
Motivation: 在一种特定数据分析方法中，需要算法来寻找闭包空间中的基本前件。

Method: 讨论并比较几种算法来解决寻找紧路径和紧对的问题。

Result: 文档未提及具体结果。

Conclusion: 文档未提及具体结论。

Abstract: We state the graph-theoretic computational problem of finding tight paths in
a directed, edge-weighted graph, as well as its simplification of finding tight
pairs. These problems are motivated by the need of algorithms that find
so-called basic antecedents in closure spaces, in one specific approach to data
analysis. We discuss and compare several algorithms to approach these problems.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [30] [Improved Approximate EFX Guarantees for Multigraphs](https://arxiv.org/abs/2506.09288)
*Alireza Kaviani,Alireza Keshavarz,Masoud Seddighin,AmirMohammad Shahrezaei*

Main category: cs.GT

TL;DR: 本文针对加性(2,∞)有界估值，改进已有结果，证明存在(1/√2)-EFX分配。


<details>
  <summary>Details</summary>
Motivation: 已有结果表明加性(2,∞)有界估值存在(2/3)-EFX分配，作者希望进一步改进该界。

Method: 未提及具体方法。

Result: 证明了加性(2,∞)有界估值存在(1/√2)-EFX分配。

Conclusion: 在加性(2,∞)有界估值的EFX分配界上取得了比之前更好的结果。

Abstract: In recent years, a new line of work in fair allocation has focused on EFX
allocations for \((p, q)\)-bounded valuations, where each good is relevant to
at most \(p\) agents, and any pair of agents share at most \(q\) relevant
goods. For the case \(p = 2\) and \(q = \infty\), such instances can be
equivalently represented as multigraphs whose vertices are the agents and whose
edges represent goods, each edge incident to exactly the one or two agents for
whom the good is relevant. A recent result of \citet{amanatidis2024pushing}
shows that for additive $(2,\infty)$ bounded valuations, a
\((\nicefrac{2}{3})\)-EFX allocation always exists. In this paper, we improve
this bound by proving the existence of a \((\nicefrac{1}{\sqrt{2}})\)-\(\efx\)
allocation for additive \((2,\infty)\)-bounded valuations.

</details>


### [31] [Competition Complexity in Multi-Item Auctions: Beyond VCG and Regularity](https://arxiv.org/abs/2506.09291)
*Hedyeh Beyhaghi,Linda Cai,Yiding Feng,Yingkai Li,S. Matthew Weinberg*

Main category: cs.GT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We quantify the value of the monopoly's bargaining power in terms of
competition complexity--that is, the number of additional bidders the monopoly
must attract in simple auctions to match the expected revenue of the optimal
mechanisms (c.f., Bulow and Klemperer, 1996, Eden et al., 2017)--within the
setting of multi-item auctions. We show that for simple auctions that sell
items separately, the competition complexity is $\Theta(\frac{n}{\alpha})$ in
an environment with $n$ original bidders under the slightly stronger assumption
of $\alpha$-strong regularity, in contrast to the standard regularity
assumption in the literature, which requires $\Omega(n \cdot \ln \frac{m}{n})$
additional bidders (Feldman et al., 2018). This significantly reduces the value
of learning the distribution to design the optimal mechanisms, especially in
large markets with many items for sale. For simple auctions that sell items as
a grand bundle, we establish a constant competition complexity bound in a
single-bidder environment when the number of items is small or when the value
distribution has a monotone hazard rate. Some of our competition complexity
results also hold when we compete against the first best benchmark (i.e.,
optimal social welfare).

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [32] [Revisiting Graph Projections for Effective Complementary Product Recommendation](https://arxiv.org/abs/2506.09209)
*Leandro Anghinoni,Pablo Zivic,Jorge Adrian Sanchez*

Main category: cs.IR

TL;DR: 提出基于用户 - 物品二分图投影的有向加权图结构预测互补产品的方法，相比其他方法有显著提升。


<details>
  <summary>Details</summary>
Motivation: 互补产品推荐能提升客户体验和零售销售，但用户 - 物品交互数据存在噪声和稀疏性，推荐合适产品并非易事。

Method: 基于用户 - 物品二分图投影的有向加权图结构预测互补产品，提出从历史用户 - 物品交互推断互补关系的新方法。

Result: 在不同基准测试中，相比顺序和基于图的推荐器，平均分别提升 43% 和 38%。

Conclusion: 所提方法简单有效，在互补产品推荐上优于近期文献中的方法。

Abstract: Complementary product recommendation is a powerful strategy to improve
customer experience and retail sales. However, recommending the right product
is not a simple task because of the noisy and sparse nature of user-item
interactions. In this work, we propose a simple yet effective method to predict
a list of complementary products given a query item, based on the structure of
a directed weighted graph projected from the user-item bipartite graph. We
revisit bipartite graph projections for recommender systems and propose a novel
approach for inferring complementarity relationships from historical user-item
interactions. We compare our model with recent methods from the literature and
show, despite the simplicity of our approach, an average improvement of +43%
and +38% over sequential and graph-based recommenders, respectively, over
different benchmarks.

</details>


### [33] [In Crowd Veritas: Leveraging Human Intelligence To Fight Misinformation](https://arxiv.org/abs/2506.09221)
*Michael Soprano*

Main category: cs.IR

TL;DR: 论文研究如何利用人类智能评估网络信息真实性，通过实验和建模提出预测与解释模型，发现非专家判断常与专家一致，有助于开发对抗虚假信息的系统。


<details>
  <summary>Details</summary>
Motivation: 网络虚假信息传播威胁民主社会，传统专家核查面临扩展性挑战，众包虽有潜力但存在诸多问题，因此研究利用人类智能评估信息真实性。

Method: 开展大规模众包实验并进行统计建模。

Result: 非专家判断常与专家评估一致，尤其在考虑时机和经验等因素时。

Conclusion: 深化对真实性评估中人类判断和偏见的理解，有助于开发更透明、可信和可解释的对抗虚假信息系统。

Abstract: The spread of online misinformation poses serious threats to democratic
societies. Traditionally, expert fact-checkers verify the truthfulness of
information through investigative processes. However, the volume and immediacy
of online content present major scalability challenges. Crowdsourcing offers a
promising alternative by leveraging non-expert judgments, but it introduces
concerns about bias, accuracy, and interpretability. This thesis investigates
how human intelligence can be harnessed to assess the truthfulness of online
information, focusing on three areas: misinformation assessment, cognitive
biases, and automated fact-checking systems. Through large-scale crowdsourcing
experiments and statistical modeling, it identifies key factors influencing
human judgments and introduces a model for the joint prediction and explanation
of truthfulness. The findings show that non-expert judgments often align with
expert assessments, particularly when factors such as timing and experience are
considered. By deepening our understanding of human judgment and bias in
truthfulness assessment, this thesis contributes to the development of more
transparent, trustworthy, and interpretable systems for combating
misinformation.

</details>


### [34] [ThinkQE: Query Expansion via an Evolving Thinking Process](https://arxiv.org/abs/2506.09260)
*Yibin Lei,Tao Shen,Andrew Yates*

Main category: cs.IR

TL;DR: 提出ThinkQE框架用于查询扩展，在多个基准测试中表现优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的查询扩展方法生成的扩展内容聚焦过窄，无法满足搜索的探索性和结果多样性需求。

Method: 提出ThinkQE框架，包含基于思考的扩展过程和语料库交互策略。

Result: 在DL19、DL20和BRIGHT等多种网络搜索基准测试中，ThinkQE始终优于先前方法。

Conclusion: ThinkQE能有效解决现有查询扩展方法的局限性，提升查询扩展效果。

Abstract: Effective query expansion for web search benefits from promoting both
exploration and result diversity to capture multiple interpretations and facets
of a query. While recent LLM-based methods have improved retrieval performance
and demonstrate strong domain generalization without additional training, they
often generate narrowly focused expansions that overlook these desiderata. We
propose ThinkQE, a test-time query expansion framework addressing this
limitation through two key components: a thinking-based expansion process that
encourages deeper and comprehensive semantic exploration, and a
corpus-interaction strategy that iteratively refines expansions using retrieval
feedback from the corpus. Experiments on diverse web search benchmarks (DL19,
DL20, and BRIGHT) show ThinkQE consistently outperforms prior approaches,
including training-intensive dense retrievers and rerankers.

</details>


### [35] [MAGMaR Shared Task System Description: Video Retrieval with OmniEmbed](https://arxiv.org/abs/2506.09409)
*Jiaqi Samantha Zhan,Crystina Zhang,Shengyao Zhuang,Xueguang Ma,Jimmy Lin*

Main category: cs.IR

TL;DR: 本文利用OmniEmbed模型在MAGMaR共享任务中探索统一检索方法，在MultiVENT 2.0数据集上微调后提升多语言视频检索效果，提交结果在排行榜领先且开源模型检查点。


<details>
  <summary>Details</summary>
Motivation: 由于整合视觉、听觉和文本模态的复杂性，有效视频检索仍具挑战性，需探索统一检索方法。

Method: 使用Tevatron 2.0工具包中的OmniEmbed模型，在MultiVENT 2.0数据集上生成统一嵌入表示，并结合多模态数据进行微调。

Result: 在MAGMaR共享任务排行榜的公开提交中获得最高分。

Conclusion: 统一的多模态检索方法具有实际有效性。

Abstract: Effective video retrieval remains challenging due to the complexity of
integrating visual, auditory, and textual modalities. In this paper, we explore
unified retrieval methods using OmniEmbed, a powerful multimodal embedding
model from the Tevatron 2.0 toolkit, in the context of the MAGMaR shared task.
Evaluated on the comprehensive MultiVENT 2.0 dataset, OmniEmbed generates
unified embeddings for text, images, audio, and video, enabling robust
multimodal retrieval. By finetuning OmniEmbed with the combined multimodal
data--visual frames, audio tracks, and textual descriptions provided in
MultiVENT 2.0, we achieve substantial improvements in complex, multilingual
video retrieval tasks. Our submission achieved the highest score on the MAGMaR
shared task leaderboard among public submissions as of May 20th, 2025,
highlighting the practical effectiveness of our unified multimodal retrieval
approach. Model checkpoint in this work is opensourced.

</details>


### [36] [Discrete Scale-invariant Metric Learning for Efficient Collaborative Filtering](https://arxiv.org/abs/2506.09898)
*Yan Zhang,Li Deng,Lixin Duan,Sami Azam*

Main category: cs.IR

TL;DR: 提出离散尺度不变度量学习方法DSIML用于推荐系统，实验表明优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前度量学习方法用绝对几何距离度量用户偏好差异，在处理不平衡类别物品时不理想。

Method: 在用户和物品上添加二进制约束，映射到共享汉明子空间；提出基于角度的尺度不变边界，推导尺度不变三重铰链损失；集成成对排序损失；优化变分二次上界并使用交替优化策略学习哈希码。

Result: 在基准数据集实验显示，DSIML优于推荐系统中具有竞争力的度量学习和基于哈希的基线方法。

Conclusion: DSIML方法在推荐系统度量学习中有效且优越，代码开源。

Abstract: Metric learning has attracted extensive interest for its ability to provide
personalized recommendations based on the importance of observed user-item
interactions. Current metric learning methods aim to push negative items away
from the corresponding users and positive items by an absolute geometrical
distance margin. However, items may come from imbalanced categories with
different intra-class variations. Thus, the absolute distance margin may not be
ideal for estimating the difference between user preferences over imbalanced
items. To this end, we propose a new method, named discrete scale-invariant
metric learning (DSIML), by adding binary constraints to users and items, which
maps users and items into binary codes of a shared Hamming subspace to speed up
the online recommendation. Specifically, we firstly propose a scale-invariant
margin based on angles at the negative item points in the shared Hamming
subspace. Then, we derive a scale-invariant triple hinge loss based on the
margin. To capture more preference difference information, we integrate a
pairwise ranking loss into the scale-invariant loss in the proposed model. Due
to the difficulty of directly optimizing the mixed integer optimization problem
formulated with \textit{log-sum-exp} functions, we seek to optimize its
variational quadratic upper bound and learn hash codes with an alternating
optimization strategy. Experiments on benchmark datasets clearly show that our
proposed method is superior to competitive metric learning and hashing-based
baselines for recommender systems. The implementation code is available at
https://github.com/AnonyFeb/dsml.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [37] [Llama-Affinity: A Predictive Antibody Antigen Binding Model Integrating Antibody Sequences with Llama3 Backbone Architecture](https://arxiv.org/abs/2506.09052)
*Delower Hossain,Ehsan Saghapour,Kevin Song,Jake Y. Chen*

Main category: cs.LG

TL;DR: 本文介绍基于Llama 3的抗体 - 抗原结合亲和力预测模型LlamaAffinity，表现优于现有方法且计算效率高。


<details>
  <summary>Details</summary>
Motivation: 传统亲和力测量实验方法耗时且昂贵，人工智能为抗体设计和亲和力预测带来新途径。

Method: 利用开源Llama 3骨干和OAS数据库中的抗体序列数据构建LlamaAffinity模型。

Result: 模型在多个评估指标上优于现有SOTA方法，准确率0.9640，F1分数0.9643等，且计算效率高，平均累积训练时间仅0.46小时。

Conclusion: LlamaAffinity模型在抗体 - 抗原结合亲和力预测上有显著优势和应用潜力。

Abstract: Antibody-facilitated immune responses are central to the body's defense
against pathogens, viruses, and other foreign invaders. The ability of
antibodies to specifically bind and neutralize antigens is vital for
maintaining immunity. Over the past few decades, bioengineering advancements
have significantly accelerated therapeutic antibody development. These
antibody-derived drugs have shown remarkable efficacy, particularly in treating
cancer, SARS-CoV-2, autoimmune disorders, and infectious diseases.
Traditionally, experimental methods for affinity measurement have been
time-consuming and expensive. With the advent of artificial intelligence, in
silico medicine has been revolutionized; recent developments in machine
learning, particularly the use of large language models (LLMs) for representing
antibodies, have opened up new avenues for AI-based design and improved
affinity prediction. Herein, we present an advanced antibody-antigen binding
affinity prediction model (LlamaAffinity), leveraging an open-source Llama 3
backbone and antibody sequence data sourced from the Observed Antibody Space
(OAS) database. The proposed approach shows significant improvement over
existing state-of-the-art (SOTA) methods (AntiFormer, AntiBERTa, AntiBERTy)
across multiple evaluation metrics. Specifically, the model achieved an
accuracy of 0.9640, an F1-score of 0.9643, a precision of 0.9702, a recall of
0.9586, and an AUC-ROC of 0.9936. Moreover, this strategy unveiled higher
computational efficiency, with a five-fold average cumulative training time of
only 0.46 hours, significantly lower than in previous studies.

</details>


### [38] [FinHEAR: Human Expertise and Adaptive Risk-Aware Temporal Reasoning for Financial Decision-Making](https://arxiv.org/abs/2506.09080)
*Jiaxiang Chen,Mingxi Zou,Zhuo Wang,Qifan Wang,Dongning Sun,Chi Zhang,Zenglin Xu*

Main category: cs.LG

TL;DR: 提出FinHEAR框架用于金融决策，在金融数据集上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 语言模型在金融决策中面临挑战，且现有大语言模型无法捕捉人类金融决策行为模式。

Method: 提出FinHEAR多智能体框架，通过专门的基于大语言模型的智能体在以事件为中心的流程中分析历史趋势、解读当前事件和检索专家先例，结合行为经济学理念。

Result: 在精心策划的金融数据集上，FinHEAR在趋势预测和交易任务中始终优于强基线，实现更高准确性和更好的风险调整回报。

Conclusion: FinHEAR框架能提升金融决策的可解释性和鲁棒性。

Abstract: Financial decision-making presents unique challenges for language models,
demanding temporal reasoning, adaptive risk assessment, and responsiveness to
dynamic events. While large language models (LLMs) show strong general
reasoning capabilities, they often fail to capture behavioral patterns central
to human financial decisions-such as expert reliance under information
asymmetry, loss-averse sensitivity, and feedback-driven temporal adjustment. We
propose FinHEAR, a multi-agent framework for Human Expertise and Adaptive
Risk-aware reasoning. FinHEAR orchestrates specialized LLM-based agents to
analyze historical trends, interpret current events, and retrieve
expert-informed precedents within an event-centric pipeline. Grounded in
behavioral economics, it incorporates expert-guided retrieval,
confidence-adjusted position sizing, and outcome-based refinement to enhance
interpretability and robustness. Empirical results on curated financial
datasets show that FinHEAR consistently outperforms strong baselines across
trend prediction and trading tasks, achieving higher accuracy and better
risk-adjusted returns.

</details>


### [39] [Enhanced Whole Page Optimization via Mixed-Grained Reward Mechanism-Adapted Language Models](https://arxiv.org/abs/2506.09084)
*Xinyuan Wang,Liang Wu,Yanjie Fu*

Main category: cs.LG

TL;DR: 本文用用户反馈监督微调大模型进行全页优化，提出 PageLLM 方法，在多数据集验证有效，线上测试有收益。


<details>
  <summary>Details</summary>
Motivation: 微调预训练大模型用于全页优化需大量人工标注数据，成本高，需新方法。

Method: 提出基于奖励的微调方法 PageLLM，采用结合页面级和条目级奖励的混合粒度奖励机制。

Result: PageLLM 在公开和工业数据集上验证，优于基线，线上 A/B 测试中 GMV 提升 0.44%。

Conclusion: PageLLM 方法有效，对真实场景有积极影响。

Abstract: Optimizing the presentation of search and recommendation results is crucial
to enhancing user experience and engagement. Whole Page Optimization (WPO)
plays a pivotal role in this process, as it directly influences how information
is surfaced to users. While Pre-trained Large Language Models (LLMs) have
demonstrated remarkable capabilities in generating coherent and contextually
relevant content, fine-tuning these models for complex tasks like WPO presents
challenges. Specifically, the need for extensive human-annotated data to
mitigate issues such as hallucinations and model instability can be
prohibitively expensive, especially in large-scale systems that interact with
millions of items daily. In this work, we address the challenge of fine-tuning
LLMs for WPO by using user feedback as the supervision. Unlike manually labeled
datasets, user feedback is inherently noisy and less precise. To overcome this,
we propose a reward-based fine-tuning approach, PageLLM, which employs a
mixed-grained reward mechanism that combines page-level and item-level rewards.
The page-level reward evaluates the overall quality and coherence, while the
item-level reward focuses on the accuracy and relevance of key recommendations.
This dual-reward structure ensures that both the holistic presentation and the
critical individual components are optimized. We validate PageLLM on both
public and industrial datasets. PageLLM outperforms baselines and achieves a
0.44\% GMV increase in an online A/B test with over 10 million users,
demonstrating its real-world impact.

</details>


### [40] [Metritocracy: Representative Metrics for Lite Benchmarks](https://arxiv.org/abs/2506.09813)
*Ariel Procaccia,Benjamin Schiffer,Serena Wang,Shirley Zhang*

Main category: cs.LG

TL;DR: 使用社会选择理论为评估指标子集选择形式化两种代表性概念，证明指标数量上下界，研究广义属性并结合实际案例。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型评估中如何从全量指标中选择子集，且‘代表性’定义不明确的问题。

Method: 运用社会选择理论，引入位置代表性和位置比例性概念，证明最坏情况下指标数量上下界，研究广义属性。

Result: 得出最坏情况下保证相关属性所需最小指标数量的上下界，研究广义属性。

Conclusion: 理论结合实际，通过大语言模型评估和医院质量评估案例验证。

Abstract: A common problem in LLM evaluation is how to choose a subset of metrics from
a full suite of possible metrics. Subset selection is usually done for
efficiency or interpretability reasons, and the goal is often to select a
``representative'' subset of metrics. However, ``representative'' is rarely
clearly defined. In this work, we use ideas from social choice theory to
formalize two notions of representation for the selection of a subset of
evaluation metrics. We first introduce positional representation, which
guarantees every alternative is sufficiently represented at every position
cutoff. We then introduce positional proportionality, which guarantees no
alternative is proportionally over- or under-represented by more than a small
error at any position. We prove upper and lower bounds on the smallest number
of metrics needed to guarantee either of these properties in the worst case. We
also study a generalized form of each property that allows for additional input
on groups of metrics that must be represented. Finally, we tie theory to
practice through real-world case studies on both LLM evaluation and hospital
quality evaluation.

</details>


### [41] [Synergizing Reinforcement Learning and Genetic Algorithms for Neural Combinatorial Optimization](https://arxiv.org/abs/2506.09404)
*Shengda Gu,Kai Li,Junliang Xing,Yifan Zhang,Jian Cheng*

Main category: cs.LG

TL;DR: 本文提出进化增强机制（EAM），结合DRL和GA优势，理论分析保证策略更新稳定有效，实验证明能提升解质量和训练效率。


<details>
  <summary>Details</summary>
Motivation: 组合优化问题难，DRL探索有限、易陷入局部最优，GA样本效率低、计算密集，需结合二者优势。

Method: 提出EAM框架，从学习策略生成解，用特定遗传操作优化，将进化解选择性重新注入策略训练循环，并进行理论分析。

Result: 在多个基准问题上，EAM显著提升解质量和训练效率，优于竞争基线。

Conclusion: EAM是通用且即插即用的框架，能有效结合DRL学习效率和GA全局搜索能力。

Abstract: Combinatorial optimization problems are notoriously challenging due to their
discrete structure and exponentially large solution space. Recent advances in
deep reinforcement learning (DRL) have enabled the learning heuristics directly
from data. However, DRL methods often suffer from limited exploration and
susceptibility to local optima. On the other hand, evolutionary algorithms such
as Genetic Algorithms (GAs) exhibit strong global exploration capabilities but
are typically sample inefficient and computationally intensive. In this work,
we propose the Evolutionary Augmentation Mechanism (EAM), a general and
plug-and-play framework that synergizes the learning efficiency of DRL with the
global search power of GAs. EAM operates by generating solutions from a learned
policy and refining them through domain-specific genetic operations such as
crossover and mutation. These evolved solutions are then selectively reinjected
into the policy training loop, thereby enhancing exploration and accelerating
convergence. We further provide a theoretical analysis that establishes an
upper bound on the KL divergence between the evolved solution distribution and
the policy distribution, ensuring stable and effective policy updates. EAM is
model-agnostic and can be seamlessly integrated with state-of-the-art DRL
solvers such as the Attention Model, POMO, and SymNCO. Extensive results on
benchmark problems (e.g., TSP, CVRP, PCTSP, and OP) demonstrate that EAM
significantly improves both solution quality and training efficiency over
competitive baselines.

</details>


### [42] [LLM-ML Teaming: Integrated Symbolic Decoding and Gradient Search for Valid and Stable Generative Feature Transformation](https://arxiv.org/abs/2506.09085)
*Xinyuan Wang,Haoyue Bai,Nanxu Gong,Wangyang Ying,Sixun Dong,Xiquan Cui,Yanjie Fu*

Main category: cs.LG

TL;DR: 本文提出结合大语言模型（LLM）符号生成与机器学习（ML）梯度优化的协作框架用于特征转换，实验显示该策略可提升下游性能并减少错误，还发现了LLM理解原始数据的能力。


<details>
  <summary>Details</summary>
Motivation: 现有特征转换方法存在传统机器学习有效性低和大语言模型生成不稳定的问题，需要新方法解决。

Method: 提出结合LLM符号生成与ML梯度优化的协作框架，包含黄金示例生成、特征转换序列嵌入与搜索、学生LLM特征转换、LLM - ML解码器协作四个步骤。

Result: 在各种数据集上实验表明，该协作策略可使下游性能提升5%，并减少近一半的错误案例，且展现出效率和鲁棒性。

Conclusion: 所提出的协作框架有效，能提升特征转换效果，且发现了LLM理解原始数据的能力。

Abstract: Feature transformation enhances data representation by deriving new features
from the original data. Generative AI offers potential for this task, but faces
challenges in stable generation (consistent outputs) and valid generation
(error-free sequences). Existing methods--traditional MLs' low validity and
LLMs' instability--fail to resolve both. We find that LLMs ensure valid syntax,
while ML's gradient-steered search stabilizes performance. To bridge this gap,
we propose a teaming framework combining LLMs' symbolic generation with ML's
gradient optimization. This framework includes four steps: (1) golden examples
generation, aiming to prepare high-quality samples with the ground knowledge of
the teacher LLM; (2) feature transformation sequence embedding and search,
intending to uncover potentially superior embeddings within the latent space;
(3) student LLM feature transformation, aiming to distill knowledge from the
teacher LLM; (4) LLM-ML decoder teaming, dedicating to combine ML and the
student LLM probabilities for valid and stable generation. The experiments on
various datasets show that the teaming policy can achieve 5\% improvement in
downstream performance while reducing nearly half of the error cases. The
results also demonstrate the efficiency and robustness of the teaming policy.
Additionally, we also have exciting findings on LLMs' capacity to understand
the original data.

</details>


### [43] [Natural Language Guided Ligand-Binding Protein Design](https://arxiv.org/abs/2506.09332)
*Zhenqiao Song,Ramith Hettiarachchi,Chuan Li,Jianwen Xie,Lei Li*

Main category: cs.LG

TL;DR: 提出InstructPro模型，可依语言指令设计配体结合蛋白，训练两变体超基线，证明能生成符合功能规范蛋白。


<details>
  <summary>Details</summary>
Motivation: 解决多数AI模型训练数据稀缺问题，实现让AI蛋白模型按人类语言指令设计有特定功能蛋白。

Method: 提出InstructPro模型架构和训练策略，构建大规模数据集InstructProBench用于训练和评估，训练InstructPro - 1B和InstructPro - 3B两个变体。

Result: 两个变体均超基线，InstructPro - 1B对接成功率高、RMSD低，InstructPro - 3B进一步降低RMSD。

Conclusion: InstructPro具备生成符合功能规范的配体结合蛋白的能力。

Abstract: Can AI protein models follow human language instructions and design proteins
with desired functions (e.g. binding to a ligand)? Designing proteins that bind
to a given ligand is crucial in a wide range of applications in biology and
chemistry. Most prior AI models are trained on protein-ligand complex data,
which is scarce due to the high cost and time requirements of laboratory
experiments. In contrast, there is a substantial body of human-curated text
descriptions about protein-ligand interactions and ligand formula. In this
paper, we propose InstructPro, a family of protein generative models that
follow natural language instructions to design ligand-binding proteins. Given a
textual description of the desired function and a ligand formula in SMILES,
InstructPro generates protein sequences that are functionally consistent with
the specified instructions. We develop the model architecture, training
strategy, and a large-scale dataset, InstructProBench, to support both training
and evaluation. InstructProBench consists of 9,592,829 triples of (function
description, ligand formula, protein sequence). We train two model variants:
InstructPro-1B (with 1 billion parameters) and InstructPro-3B~(with 3 billion
parameters). Both variants consistently outperform strong baselines, including
ProGen2, ESM3, and Pinal. Notably, InstructPro-1B achieves the highest docking
success rate (81.52% at moderate confidence) and the lowest average root mean
square deviation (RMSD) compared to ground truth structures (4.026{\AA}).
InstructPro-3B further descreases the average RMSD to 2.527{\AA}, demonstrating
InstructPro's ability to generate ligand-binding proteins that align with the
functional specifications.

</details>


### [44] [Spiking Neural Models for Decision-Making Tasks with Learning](https://arxiv.org/abs/2506.09087)
*Sophie Jaffard,Giulia Mezzadri,Patricia Reynaud-Bouret,Etienne Tanré*

Main category: cs.LG

TL;DR: 本文提出用于决策的SNN模型，连接认知与生物模型，还给出DDM和泊松计数器模型耦合结果，设计在线分类任务评估模型。


<details>
  <summary>Details</summary>
Motivation: 现有决策模型缺乏学习机制且限于参与者有先验知识的任务，需弥合认知与生物模型差距。

Method: 提出基于多元Hawkes过程的SNN模型，给出DDM和泊松计数器模型耦合结果，设计在线分类任务。

Result: DDM和泊松计数器模型分类和反应时间相似，特定带相关噪声的DDM可从遵循局部学习规则的Hawkes网络推导。

Conclusion: 工作向认知模型中整合生物相关神经机制迈进重要一步，助于理解神经活动和行为关系。

Abstract: In cognition, response times and choices in decision-making tasks are
commonly modeled using Drift Diffusion Models (DDMs), which describe the
accumulation of evidence for a decision as a stochastic process, specifically a
Brownian motion, with the drift rate reflecting the strength of the evidence.
In the same vein, the Poisson counter model describes the accumulation of
evidence as discrete events whose counts over time are modeled as Poisson
processes, and has a spiking neurons interpretation as these processes are used
to model neuronal activities. However, these models lack a learning mechanism
and are limited to tasks where participants have prior knowledge of the
categories. To bridge the gap between cognitive and biological models, we
propose a biologically plausible Spiking Neural Network (SNN) model for
decision-making that incorporates a learning mechanism and whose neurons
activities are modeled by a multivariate Hawkes process. First, we show a
coupling result between the DDM and the Poisson counter model, establishing
that these two models provide similar categorizations and reaction times and
that the DDM can be approximated by spiking Poisson neurons. To go further, we
show that a particular DDM with correlated noise can be derived from a Hawkes
network of spiking neurons governed by a local learning rule. In addition, we
designed an online categorization task to evaluate the model predictions. This
work provides a significant step toward integrating biologically relevant
neural mechanisms into cognitive models, fostering a deeper understanding of
the relationship between neural activity and behavior.

</details>


### [45] [A Topic Modeling Analysis of Stigma Dimensions, Social, and Related Behavioral Circumstances in Clinical Notes Among Patients with HIV](https://arxiv.org/abs/2506.09279)
*Ziyi Chen,Yiyang Liu,Mattia Prosperi,Krishna Vaddiparti,Robert L Cook,Jiang Bian,Yi Guo,Yonghui Wu*

Main category: cs.LG

TL;DR: 使用自然语言处理分析美国东南部医疗系统电子病历，揭示HIV相关污名、社会及行为情况主题并发现年龄亚组差异，可改进评估。


<details>
  <summary>Details</summary>
Motivation: 刻画寻求治疗的HIV感染者的污名维度、社会及相关行为情况。

Method: 从UF Health IDR确定9140名HIV感染者队列，用LDA进行主题建模分析，专家创建关键词种子列表，采用滚雪球策略，测试三种关键词过滤策略，专家手动审核，进行词频分析和亚组主题差异分析。

Result: 主题建模揭示多种与HIV相关的主题，年龄亚组主题有差异。

Conclusion: 从电子病历中提取相关信息可实现高效评估，克服传统问卷局限，改善患者预后。

Abstract: Objective: To characterize stigma dimensions, social, and related behavioral
circumstances in people living with HIV (PLWHs) seeking care, using natural
language processing methods applied to a large collection of electronic health
record (EHR) clinical notes from a large integrated health system in the
southeast United States. Methods: We identified 9,140 cohort of PLWHs from the
UF Health IDR and performed topic modeling analysis using Latent Dirichlet
Allocation (LDA) to uncover stigma dimensions, social, and related behavioral
circumstances. Domain experts created a seed list of HIV-related stigma
keywords, then applied a snowball strategy to iteratively review notes for
additional terms until saturation was reached. To identify more target topics,
we tested three keyword-based filtering strategies. Domain experts manually
reviewed the detected topics using the prevalent terms and key discussion
topics. Word frequency analysis was used to highlight the prevalent terms
associated with each topic. In addition, we conducted topic variation analysis
among subgroups to examine differences across age and sex-specific
demographics. Results and Conclusion: Topic modeling on sentences containing at
least one keyword uncovered a wide range of topic themes associated with
HIV-related stigma, social, and related behaviors circumstances, including
"Mental Health Concern and Stigma", "Social Support and Engagement", "Limited
Healthcare Access and Severe Illness", "Treatment Refusal and Isolation" and so
on. Topic variation analysis across age subgroups revealed differences.
Extracting and understanding the HIV-related stigma dimensions, social, and
related behavioral circumstances from EHR clinical notes enables scalable,
time-efficient assessment, overcoming the limitations of traditional
questionnaires and improving patient outcomes.

</details>


### [46] [Causal Climate Emulation with Bayesian Filtering](https://arxiv.org/abs/2506.09891)
*Sebastian Hickman,Ilija Trajkovic,Julia Kaltenborn,Francis Pelletier,Alex Archibald,Yaniv Gurwicz,Peer Nowack,David Rolnick,Julien Boussard*

Main category: cs.LG

TL;DR: 开发基于因果表征学习的可解释气候模型模拟器，展示其学习气候动力学的能力及各组件重要性。


<details>
  <summary>Details</summary>
Motivation: 传统气候变化模型计算成本高，限制气候预测与分析；现有机器学习方法无法纳入物理因果关系。

Method: 开发基于因果表征学习的可解释气候模型模拟器，采用包含贝叶斯滤波器的物理信息方法进行长期自回归模拟。

Result: 模拟器能学习准确的气候动力学，在合成数据集和两个广泛使用的气候模型数据上展示了各组件的重要性。

Conclusion: 所开发的基于因果表征学习的模拟器在气候模拟方面具有良好效果。

Abstract: Traditional models of climate change use complex systems of coupled equations
to simulate physical processes across the Earth system. These simulations are
highly computationally expensive, limiting our predictions of climate change
and analyses of its causes and effects. Machine learning has the potential to
quickly emulate data from climate models, but current approaches are not able
to incorporate physics-informed causal relationships. Here, we develop an
interpretable climate model emulator based on causal representation learning.
We derive a physics-informed approach including a Bayesian filter for stable
long-term autoregressive emulation. We demonstrate that our emulator learns
accurate climate dynamics, and we show the importance of each one of its
components on a realistic synthetic dataset and data from two widely deployed
climate models.

</details>


### [47] [Integrating Asynchronous AdaBoost into Federated Learning: Five Real World Applications](https://arxiv.org/abs/2506.09090)
*Arthur Oghlukyan,Nuria Gomez Blas*

Main category: cs.LG

TL;DR: 本文分析增强异步AdaBoost框架在联邦学习五个领域的应用，算法减少同步频率和通信开销，实证显示效率和鲁棒性提升。


<details>
  <summary>Details</summary>
Motivation: 提升联邦学习中模型的通信效率、可扩展性、收敛性和鲁棒性。

Method: 提出增强异步AdaBoost框架，采用自适应通信调度和延迟权重补偿。

Result: 与基线AdaBoost相比，训练时间减少20 - 35%，通信开销减少30 - 40%，收敛轮数显著减少。

Conclusion: 增强的AdaBoost在不同联邦学习场景中效率和鲁棒性显著提升，有广泛适用性。

Abstract: This paper presents a comprehensive analysis of an enhanced asynchronous
AdaBoost framework for federated learning (FL), focusing on its application
across five distinct domains: computer vision on edge devices, blockchain-based
model transparency, on-device mobile personalization, IoT anomaly detection,
and federated healthcare diagnostics. The proposed algorithm incorporates
adaptive communication scheduling and delayed weight compensation to reduce
synchronization frequency and communication overhead while preserving or
improving model accuracy. We examine how these innovations improve
communication efficiency, scalability, convergence, and robustness in each
domain. Comparative metrics including training time, communication overhead,
convergence iterations, and classification accuracy are evaluated using data
and estimates derived from Oghlukyan's enhanced AdaBoost framework. Empirical
results show, for example, training time reductions on the order of 20-35% and
communication overhead reductions of 30-40% compared to baseline AdaBoost, with
convergence achieved in significantly fewer boosting rounds. Tables and charts
summarize these improvements by domain. Mathematical formulations of the
adaptive scheduling rule and error-driven synchronization thresholds are
provided. Overall, the enhanced AdaBoost exhibits markedly improved efficiency
and robustness across diverse FL scenarios, suggesting broad applicability of
the approach.

</details>


### [48] [Variational Inference Optimized Using the Curved Geometry of Coupled Free Energy](https://arxiv.org/abs/2506.09091)
*Kenric Nelson,Igor Oliveira,Amenah Al-Najafi,Fode Zhang,Hon Keung Tony Ng*

Main category: cs.LG

TL;DR: 本文提出基于耦合自由能的变分推理优化框架，应用于耦合变分自编码器，能减少训练样本离群值，在CelebA图像重建上比VAE有3%的提升。


<details>
  <summary>Details</summary>
Motivation: 将变分推理技术扩展到耦合指数族的弯曲几何，以提高学习模型的准确性和鲁棒性。

Method: 利用耦合自由能，推导耦合广义的Fisher信息度量和仿射连接，应用于设计耦合变分自编码器，使用耦合分布和成本函数推导重建度量。

Result: 能训练在尾部有高惩罚的模型，减少训练样本离群值，CelebA图像重建的Wasserstein - 2或Fréchet Inception Distance显示CVAE比VAE训练5个epoch后有3%的提升。

Conclusion: 所提出的基于耦合自由能的变分推理优化框架有效，应用于CVAE能取得比VAE更好的效果。

Abstract: We introduce an optimization framework for variational inference based on the
coupled free energy, extending variational inference techniques to account for
the curved geometry of the coupled exponential family. This family includes
important heavy-tailed distributions such as the generalized Pareto and the
Student's t. By leveraging the coupled free energy, which is equal to the
coupled evidence lower bound (ELBO) of the inverted probabilities, we improve
the accuracy and robustness of the learned model. The coupled generalization of
Fisher Information metric and the affine connection. The method is applied to
the design of a coupled variational autoencoder (CVAE). By using the coupling
for both the distributions and cost functions, the reconstruction metric is
derived to still be the mean-square average loss with modified constants. The
novelty comes from sampling the heavy-tailed latent distribution with its
associated coupled probability, which has faster decaying tails. The result is
the ability to train a model with high penalties in the tails, while assuring
that the training samples have a reduced number of outliers. The Wasserstein-2
or Fr\'echet Inception Distance of the reconstructed CelebA images shows the
CVAE has a 3\% improvement over the VAE after 5 epochs of training.

</details>


### [49] [CUDA-LLM: LLMs Can Write Efficient CUDA Kernels](https://arxiv.org/abs/2506.09092)
*Wentao Chen,Jiace Zhu,Qi Fan,Yehan Ma,An Zou*

Main category: cs.LG

TL;DR: 探索用大语言模型自动生成和优化CUDA程序，提出FSR框架，实验证明其能保证正确性且生成代码性能远超人类编写代码。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在通用代码生成能力强，但生成高度特定于硬件、考虑架构且对性能要求高的代码，尤其是针对大规模并行GPU的代码仍是挑战，目标是生成充分利用底层硬件的高性能GPU内核。

Method: 提出Feature Search and Reinforcement (FSR)框架，联合优化编译、功能正确性和运行时性能，通过大量测试用例验证，以实际内核执行延迟衡量。

Result: 使用FSR增强的大语言模型能保证正确性，自动生成的内核执行速度比一般人类编写的代码最高快179倍。

Conclusion: 结合大语言模型和性能强化来自动化针对特定硬件、架构敏感和性能关键应用的GPU编程具有潜力。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in
general-purpose code generation. However, generating the code which is deeply
hardware-specific, architecture-aware, and performance-critical, especially for
massively parallel GPUs, remains a complex challenge. In this work, we explore
the use of LLMs for the automated generation and optimization of CUDA programs,
with the goal of producing high-performance GPU kernels that fully exploit the
underlying hardware. To address this challenge, we propose a novel framework
called \textbf{Feature Search and Reinforcement (FSR)}. FSR jointly optimizes
compilation and functional correctness, as well as the runtime performance,
which are validated through extensive and diverse test cases, and measured by
actual kernel execution latency on the target GPU, respectively. This approach
enables LLMs not only to generate syntactically and semantically correct CUDA
code but also to iteratively refine it for efficiency, tailored to the
characteristics of the GPU architecture. We evaluate FSR on representative CUDA
kernels, covering AI workloads and computational intensive algorithms. Our
results show that LLMs augmented with FSR consistently guarantee correctness
rates. Meanwhile, the automatically generated kernels can outperform general
human-written code by a factor of up to 179$\times$ in execution speeds. These
findings highlight the potential of combining LLMs with performance
reinforcement to automate GPU programming for hardware-specific,
architecture-sensitive, and performance-critical applications.

</details>


### [50] [Merging Smarter, Generalizing Better: Enhancing Model Merging on OOD Data](https://arxiv.org/abs/2506.09093)
*Bingjie Zhang,Hongkang Li,Changlong Shi,Guowei Rong,He Zhao,Dongsheng Wang,Dandan Guo,Meng Wang*

Main category: cs.LG

TL;DR: 提出LwPTV方法解决模型合并在OOD数据集上效果不佳问题，可集成到现有方法提升OOD性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法主要关注ID数据集性能，忽略了在OOD数据集上的效果，需要改进。

Method: 构建显著性分数衡量任务向量参数冗余，得到各任务掩码向量，对任务向量进行逐层剪枝，仅保留合并模型中对应层的预训练模型参数。

Result: 该方法能与多数现有模型合并方法集成，在保持ID任务能力的同时，大幅提升OOD任务性能。

Conclusion: 提出的LwPTV方法有效，可改善现有模型合并方法在OOD任务上的表现。

Abstract: Multi-task learning (MTL) concurrently trains a model on diverse task
datasets to exploit common features, thereby improving overall performance
across the tasks. Recent studies have dedicated efforts to merging multiple
independent model parameters into a unified model for MTL, thus circumventing
the need for training data and expanding the scope of applicable scenarios of
MTL. However, current approaches to model merging predominantly concentrate on
enhancing performance within in-domain (ID) datasets, often overlooking their
efficacy on out-of-domain (OOD) datasets. In this work, we proposed LwPTV
(Layer-wise Pruning Task Vector) by building a saliency score, measuring the
redundancy of parameters in task vectors. Designed in this way ours can achieve
mask vector for each task and thus perform layer-wise pruning on the task
vectors, only keeping the pre-trained model parameters at the corresponding
layer in merged model. Owing to its flexibility, our method can be seamlessly
integrated with most of existing model merging methods to improve their
performance on OOD tasks. Extensive experiments demonstrate that the
application of our method results in substantial enhancements in OOD
performance while preserving the ability on ID tasks.

</details>


### [51] [Intra-Trajectory Consistency for Reward Modeling](https://arxiv.org/abs/2506.09096)
*Chaoyang Zhou,Shunyu Liu,Zengmao Wang,Di Wang,Rong-Cheng Tu,Bo Du,Dacheng Tao*

Main category: cs.LG

TL;DR: 本文提出利用生成概率建立响应轨迹过程间的奖励一致性，以解决现有奖励模型泛化能力差的问题，并取得较好效果。


<details>
  <summary>Details</summary>
Motivation: 当前奖励建模依赖整体响应得分，监督信号粗粒度，导致奖励模型难以识别与得分真正相关的具体组件，泛化能力差。

Method: 利用生成概率建立响应轨迹过程间的奖励一致性，开发轨迹内一致性正则化方法，并应用于先进结果奖励模型。

Result: 改进了奖励模型在RewardBench上的性能，诱导出更好的DPO对齐策略，实现了更好的N选1推理时验证结果。

Conclusion: 所提出的方法有效提升了奖励模型的性能和泛化能力。

Abstract: Reward models are critical for improving large language models (LLMs),
particularly in reinforcement learning from human feedback (RLHF) or
inference-time verification. Current reward modeling typically relies on scores
of overall responses to learn the outcome rewards for the responses. However,
since the response-level scores are coarse-grained supervision signals, the
reward model struggles to identify the specific components within a response
trajectory that truly correlate with the scores, leading to poor generalization
on unseen responses. In this paper, we propose to leverage generation
probabilities to establish reward consistency between processes in the response
trajectory, which allows the response-level supervisory signal to propagate
across processes, thereby providing additional fine-grained signals for reward
learning. Building on analysis under the Bayesian framework, we develop an
intra-trajectory consistency regularization to enforce that adjacent processes
with higher next-token generation probability maintain more consistent rewards.
We apply the proposed regularization to the advanced outcome reward model,
improving its performance on RewardBench. Besides, we show that the reward
model trained with the proposed regularization induces better DPO-aligned
policies and achieves better best-of-N (BON) inference-time verification
results. Our code is provided in https://github.com/chaoyang101/ICRM.

</details>


### [52] [Feature Shift Localization Network](https://arxiv.org/abs/2506.09101)
*Míriam Barrabés,Daniel Mas Montserrat,Kapal Dev,Alexander G. Ioannidis*

Main category: cs.LG

TL;DR: 提出Feature Shift Localization Network (FSL - Net) 用于快速准确地定位大型高维数据集中的特征偏移，代码和模型公开。


<details>
  <summary>Details</summary>
Motivation: 不同数据源存在特征偏移问题，定位偏移特征对解决数据问题、避免下游分析质量下降很重要，但现有定位方法不准确或不可扩展。

Method: 引入FSL - Net神经网络，通过大量数据集训练，学习数据集统计特性，可定位未知数据集和偏移。

Result: 能在大型高维数据集中快速准确地定位特征偏移。

Conclusion: FSL - Net是一种有效的特征偏移定位方法，代码和训练好的模型可公开获取。

Abstract: Feature shifts between data sources are present in many applications
involving healthcare, biomedical, socioeconomic, financial, survey, and
multi-sensor data, among others, where unharmonized heterogeneous data sources,
noisy data measurements, or inconsistent processing and standardization
pipelines can lead to erroneous features. Localizing shifted features is
important to address the underlying cause of the shift and correct or filter
the data to avoid degrading downstream analysis. While many techniques can
detect distribution shifts, localizing the features originating them is still
challenging, with current solutions being either inaccurate or not scalable to
large and high-dimensional datasets. In this work, we introduce the Feature
Shift Localization Network (FSL-Net), a neural network that can localize
feature shifts in large and high-dimensional datasets in a fast and accurate
manner. The network, trained with a large number of datasets, learns to extract
the statistical properties of the datasets and can localize feature shifts from
previously unseen datasets and shifts without the need for re-training. The
code and ready-to-use trained model are available at
https://github.com/AI-sandbox/FSL-Net.

</details>


### [53] [FLoRIST: Singular Value Thresholding for Efficient and Accurate Federated Fine-Tuning of Large Language Models](https://arxiv.org/abs/2506.09199)
*Hariharan Ramesh,Jyotikrishna Dass*

Main category: cs.LG

TL;DR: 提出FLoRIST框架用于联邦微调大语言模型，在多数据集和模型上平衡通信效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有联邦LoRA方法在平衡通信效率、模型准确性和计算成本上存在挑战，尤其是在异构客户端场景。

Method: 提出FLoRIST框架，在服务器端对堆叠本地适配器分别进行奇异值分解，引入可调奇异值阈值进行最优秩选择以构建全局低秩适配器。

Result: 在多个数据集和大语言模型上的广泛实验表明，FLoRIST在同构和异构设置中都能在通信效率和性能之间取得最佳平衡。

Conclusion: FLoRIST能有效解决现有联邦LoRA方法的问题，实现高效准确的聚合。

Abstract: Integrating Low-Rank Adaptation (LoRA) into federated learning offers a
promising solution for parameter-efficient fine-tuning of Large Language Models
(LLMs) without sharing local data. However, several methods designed for
federated LoRA present significant challenges in balancing communication
efficiency, model accuracy, and computational cost, particularly among
heterogeneous clients. These methods either rely on simplistic averaging of
local adapters, which introduces aggregation noise, require transmitting large
stacked local adapters, leading to poor communication efficiency, or
necessitate reconstructing memory-dense global weight-update matrix and
performing computationally expensive decomposition to design client-specific
low-rank adapters. In this work, we propose FLoRIST, a federated fine-tuning
framework that achieves mathematically accurate aggregation without incurring
high communication or computational overhead. Instead of constructing the full
global weight-update matrix at the server, FLoRIST employs an efficient
decomposition pipeline by performing singular value decomposition on stacked
local adapters separately. This approach operates within a compact intermediate
space to represent the accumulated information from local LoRAs. We introduce
tunable singular value thresholding for server-side optimal rank selection to
construct a pair of global low-rank adapters shared by all clients. Extensive
empirical evaluations across multiple datasets and LLMs demonstrate that
FLoRIST consistently strikes the best balance between superior communication
efficiency and competitive performance in both homogeneous and heterogeneous
setups.

</details>


### [54] [Too Big to Think: Capacity, Memorization, and Generalization in Pre-Trained Transformers](https://arxiv.org/abs/2506.09099)
*Joshua Barron,Devin White*

Main category: cs.LG

TL;DR: 研究容量受限Transformer模型在合成字符任务上的预训练，发现记忆和泛化存在权衡，预训练可能倾向一种学习模式，为小语言模型设计和部署提供见解。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型中记忆和泛化的关系。

Method: 在两个合成字符级任务上从头预训练一系列容量受限的Transformer模型，分别探测泛化和记忆。

Result: 小模型能泛化但不能记忆，大模型能记忆但不能泛化，联合训练时无模型能成功泛化。

Conclusion: 预训练可能本质上更倾向一种学习模式，模型容量影响学习行为，对小语言模型设计和部署有更广泛影响。

Abstract: The relationship between memorization and generalization in large language
models (LLMs) remains an open area of research, with growing evidence that the
two are deeply intertwined. In this work, we investigate this relationship by
pre-training a series of capacity-limited Transformer models from scratch on
two synthetic character-level tasks designed to separately probe generalization
(via arithmetic extrapolation) and memorization (via factual recall). We
observe a consistent trade-off: small models extrapolate to unseen arithmetic
cases but fail to memorize facts, while larger models memorize but fail to
extrapolate. An intermediate-capacity model exhibits a similar shift toward
memorization. When trained on both tasks jointly, no model (regardless of size)
succeeds at extrapolation. These findings suggest that pre-training may
intrinsically favor one learning mode over the other. By isolating these
dynamics in a controlled setting, our study offers insight into how model
capacity shapes learning behavior and offers broader implications for the
design and deployment of small language models.

</details>


### [55] [Scalable Spatiotemporal Inference with Biased Scan Attention Transformer Neural Processes](https://arxiv.org/abs/2506.09163)
*Daniel Jenson,Jhonathan Navott,Piotr Grynfelder,Mengyan Zhang,Makkunda Sharma,Elizaveta Semenova,Seth Flaxman*

Main category: cs.LG

TL;DR: 提出新架构BSA - TNP，可避免准确性与可扩展性的权衡，在多方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 现代神经过程（NPs）在复杂应用中面临可扩展性压力，很多架构需在准确性和可扩展性间权衡，此研究欲解决该问题。

Method: 提出Biased Scan Attention Transformer Neural Process (BSA - TNP) 架构，引入Kernel Regression Blocks (KRBlocks)、组不变注意力偏差和内存高效的Biased Scan Attention (BSA)。

Result: BSA - TNP能达到或超越最佳模型的准确性，训练时间大幅缩短；具有平移不变性；可透明地对时空演变过程建模；支持高维固定效应；扩展性良好。

Conclusion: 在建模完全或部分平移不变过程时，准确性和可扩展性的权衡通常是不必要的，BSA - TNP架构有效。

Abstract: Neural Processes (NPs) are a rapidly evolving class of models designed to
directly model the posterior predictive distribution of stochastic processes.
While early architectures were developed primarily as a scalable alternative to
Gaussian Processes (GPs), modern NPs tackle far more complex and data hungry
applications spanning geology, epidemiology, climate, and robotics. These
applications have placed increasing pressure on the scalability of these
models, with many architectures compromising accuracy for scalability. In this
paper, we demonstrate that this tradeoff is often unnecessary, particularly
when modeling fully or partially translation invariant processes. We propose a
versatile new architecture, the Biased Scan Attention Transformer Neural
Process (BSA-TNP), which introduces Kernel Regression Blocks (KRBlocks),
group-invariant attention biases, and memory-efficient Biased Scan Attention
(BSA). BSA-TNP is able to: (1) match or exceed the accuracy of the best models
while often training in a fraction of the time, (2) exhibit translation
invariance, enabling learning at multiple resolutions simultaneously, (3)
transparently model processes that evolve in both space and time, (4) support
high dimensional fixed effects, and (5) scale gracefully -- running inference
with over 1M test points with 100K context points in under a minute on a single
24GB GPU.

</details>


### [56] [An Interpretable N-gram Perplexity Threat Model for Large Language Model Jailbreaks](https://arxiv.org/abs/2410.16222)
*Valentyn Boreiko,Alexander Panfilov,Vaclav Voracek,Matthias Hein,Jonas Geiping*

Main category: cs.LG

TL;DR: 本文提出统一威胁模型比较越狱攻击方法，发现对安全调优模型的攻击成功率低于以往，离散优化攻击表现更佳，有效攻击利用罕见二元组。


<details>
  <summary>Details</summary>
Motivation: 现有越狱攻击方法在流畅性和计算量上差异大，需统一威胁模型进行原则性比较。

Method: 构建基于1T令牌的N - 元语言模型，适应流行攻击方法并进行基准测试。

Result: 对安全调优现代模型的攻击成功率低于以往，离散优化攻击显著优于基于大语言模型的攻击。

Conclusion: 该威胁模型具有可解释性，能对越狱攻击进行全面分析和比较，有效攻击利用了罕见二元组。

Abstract: A plethora of jailbreaking attacks have been proposed to obtain harmful
responses from safety-tuned LLMs. These methods largely succeed in coercing the
target output in their original settings, but their attacks vary substantially
in fluency and computational effort. In this work, we propose a unified threat
model for the principled comparison of these methods. Our threat model checks
if a given jailbreak is likely to occur in the distribution of text. For this,
we build an N-gram language model on 1T tokens, which, unlike model-based
perplexity, allows for an LLM-agnostic, nonparametric, and inherently
interpretable evaluation. We adapt popular attacks to this threat model, and,
for the first time, benchmark these attacks on equal footing with it. After an
extensive comparison, we find attack success rates against safety-tuned modern
models to be lower than previously presented and that attacks based on discrete
optimization significantly outperform recent LLM-based attacks. Being
inherently interpretable, our threat model allows for a comprehensive analysis
and comparison of jailbreak attacks. We find that effective attacks exploit and
abuse infrequent bigrams, either selecting the ones absent from real-world text
or rare ones, e.g., specific to Reddit or code datasets.

</details>


### [57] [Generalization Error Analysis for Attack-Free and Byzantine-Resilient Decentralized Learning with Data Heterogeneity](https://arxiv.org/abs/2506.09438)
*Haoxiang Ye,Tao Sun,Qing Ling*

Main category: cs.LG

TL;DR: 本文对无攻击和抗拜占庭攻击的去中心化学习进行细粒度泛化误差分析，揭示数据异质性等因素对泛化误差的影响，并用数值实验验证理论。


<details>
  <summary>Details</summary>
Motivation: 现有研究对去中心化学习算法的泛化误差研究不足，而泛化误差对实际应用中模型性能至关重要，因此需深入研究。

Method: 在异质数据和温和假设下，对无攻击和抗拜占庭攻击的去中心化学习进行细粒度泛化误差分析。

Result: 揭示数据异质性、模型初始化和随机梯度噪声对泛化误差的影响，表明拜占庭攻击影响泛化误差且与数据异质性相关、与样本大小无关。

Conclusion: 通过数值实验验证了理论分析结果。

Abstract: Decentralized learning, which facilitates joint model training across
geographically scattered agents, has gained significant attention in the field
of signal and information processing in recent years. While the optimization
errors of decentralized learning algorithms have been extensively studied,
their generalization errors remain relatively under-explored. As the
generalization errors reflect the scalability of trained models on unseen data
and are crucial in determining the performance of trained models in real-world
applications, understanding the generalization errors of decentralized learning
is of paramount importance. In this paper, we present fine-grained
generalization error analysis for both attack-free and Byzantine-resilient
decentralized learning with heterogeneous data as well as under mild
assumptions, in contrast to prior studies that consider homogeneous data and/or
rely on a stringent bounded stochastic gradient assumption. Our results shed
light on the impact of data heterogeneity, model initialization and stochastic
gradient noise -- factors that have not been closely investigated before -- on
the generalization error of decentralized learning. We also reveal that
Byzantine attacks performed by malicious agents largely affect the
generalization error, and their negative impact is inherently linked to the
data heterogeneity while remaining independent on the sample size. Numerical
experiments on both convex and non-convex tasks are conducted to validate our
theoretical findings.

</details>


### [58] [Unifying Block-wise PTQ and Distillation-based QAT for Progressive Quantization toward 2-bit Instruction-Tuned LLMs](https://arxiv.org/abs/2506.09104)
*Jung Hyun Lee,Seungjae Shin,Vinnam Kim,Jaeseong You,An Chen*

Main category: cs.LG

TL;DR: 提出统一渐进量化框架UPQ，将块级后训练量化与基于蒸馏的量化感知训练相结合，实现INT2指令调优大模型量化，在基准测试取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有极低比特量化进展局限于预训练大模型，未拓展到指令调优模型，需方法填补此空白。

Method: 提出UPQ框架，先通过块级后训练量化将FP16指令调优模型量化到INT4，再用基于蒸馏的量化感知训练使INT2指令调优模型生成与FP16一致响应。

Result: 能在不依赖专有后训练数据情况下将开源指令调优大模型量化到INT2，在MMLU和IFEval基准测试取得SOTA性能。

Conclusion: UPQ是有效量化方法，可推动极低比特量化在指令调优大模型的应用。

Abstract: As the rapid scaling of large language models (LLMs) poses significant
challenges for deployment on resource-constrained devices, there is growing
interest in extremely low-bit quantization, such as 2-bit. Although prior works
have shown that 2-bit large models are pareto-optimal over their 4-bit smaller
counterparts in both accuracy and latency, these advancements have been limited
to pre-trained LLMs and have not yet been extended to instruction-tuned models.
To bridge this gap, we propose Unified Progressive Quantization (UPQ)$-$a novel
progressive quantization framework (FP16$\rightarrow$INT4$\rightarrow$INT2)
that unifies block-wise post-training quantization (PTQ) with
distillation-based quantization-aware training (Distill-QAT) for INT2
instruction-tuned LLM quantization. UPQ first quantizes FP16 instruction-tuned
models to INT4 using block-wise PTQ to significantly reduce the quantization
error introduced by subsequent INT2 quantization. Next, UPQ applies Distill-QAT
to enable INT2 instruction-tuned LLMs to generate responses consistent with
their original FP16 counterparts by minimizing the generalized Jensen-Shannon
divergence (JSD) between the two. To the best of our knowledge, we are the
first to demonstrate that UPQ can quantize open-source instruction-tuned LLMs
to INT2 without relying on proprietary post-training data, while achieving
state-of-the-art performances on MMLU and IFEval$-$two of the most
representative benchmarks for evaluating instruction-tuned LLMs.

</details>


### [59] [CFMI: Flow Matching for Missing Data Imputation](https://arxiv.org/abs/2506.09258)
*Vaidotas Simkus,Michael U. Gutmann*

Main category: cs.LG

TL;DR: 提出条件流匹配插补（CFMI）方法用于处理缺失数据，在多方面表现良好，适用于多种数据类型和维度。


<details>
  <summary>Details</summary>
Motivation: 解决传统多重插补的难解问题，提出通用的缺失数据插补方法。

Method: 结合连续归一化流、流匹配和共享条件建模的CFMI方法。

Result: 在24个中小维度表格数据集上，CFMI在多种指标上匹配或优于传统和现代技术；在时间序列数据零样本插补上，精度与相关基于扩散的方法相当，但计算效率更高。

Conclusion: CFMI在低维数据上至少与传统方法表现相当，可扩展到高维场景，匹配或超越其他基于深度学习的方法，是多种数据类型和维度的首选插补方法。

Abstract: We introduce conditional flow matching for imputation (CFMI), a new
general-purpose method to impute missing data. The method combines continuous
normalising flows, flow-matching, and shared conditional modelling to deal with
intractabilities of traditional multiple imputation. Our comparison with nine
classical and state-of-the-art imputation methods on 24 small to
moderate-dimensional tabular data sets shows that CFMI matches or outperforms
both traditional and modern techniques across a wide range of metrics. Applying
the method to zero-shot imputation of time-series data, we find that it matches
the accuracy of a related diffusion-based method while outperforming it in
terms of computational efficiency. Overall, CFMI performs at least as well as
traditional methods on lower-dimensional data while remaining scalable to
high-dimensional settings, matching or exceeding the performance of other deep
learning-based approaches, making it a go-to imputation method for a wide range
of data types and dimensionalities.

</details>


### [60] [SyncFed: Time-Aware Federated Learning through Explicit Timestamping and Synchronization](https://arxiv.org/abs/2506.09660)
*Baran Can Gül,Stefanos Tziampazis,Nasser Jazdi,Michael Weyrich*

Main category: cs.LG

TL;DR: 随着联邦学习扩展到更大更分布式环境，训练一致性受多种因素挑战，现有方法缺乏量化陈旧性机制，本文提出SyncFed框架，能量化陈旧性，提升模型效果。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在大规模分布式环境中，网络延迟、时钟不同步和客户端更新差异影响训练一致性，现有方法无法有效量化陈旧性。

Method: 引入SyncFed时间感知联邦学习框架，利用显式同步和时间戳建立系统通用时间参考，基于NTP协议交换的时间戳量化陈旧性，并在聚合时应用时间感知加权。

Result: 在地理分布式测试平台上的实验表明，SyncFed使全局模型在稳定时间上下文内演化，相比无时间语义的基于轮次的基线方法，提高了准确性和信息新鲜度。

Conclusion: SyncFed框架能有效解决联邦学习中训练一致性问题，通过量化陈旧性提升模型性能。

Abstract: As Federated Learning (FL) expands to larger and more distributed
environments, consistency in training is challenged by network-induced delays,
clock unsynchronicity, and variability in client updates. This combination of
factors may contribute to misaligned contributions that undermine model
reliability and convergence. Existing methods like staleness-aware aggregation
and model versioning address lagging updates heuristically, yet lack mechanisms
to quantify staleness, especially in latency-sensitive and cross-regional
deployments. In light of these considerations, we introduce \emph{SyncFed}, a
time-aware FL framework that employs explicit synchronization and timestamping
to establish a common temporal reference across the system. Staleness is
quantified numerically based on exchanged timestamps under the Network Time
Protocol (NTP), enabling the server to reason about the relative freshness of
client updates and apply temporally informed weighting during aggregation. Our
empirical evaluation on a geographically distributed testbed shows that, under
\emph{SyncFed}, the global model evolves within a stable temporal context,
resulting in improved accuracy and information freshness compared to
round-based baselines devoid of temporal semantics.

</details>


### [61] [MetaTT: A Global Tensor-Train Adapter for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2506.09105)
*Javier Lopez-Piqueres,Pranav Deshpande,Archan Ray,Mattia J. Villani,Marco Pistoia,Niraj Kumar*

Main category: cs.LG

TL;DR: 提出MetaTT统一张量列车适配器框架用于预训练transformer全局低秩微调，参数压缩效果好，训练简单，可自然扩展到多任务。


<details>
  <summary>Details</summary>
Motivation: 现有方法如LoRA独立微调每个权重矩阵，需提出新框架实现更高效微调。

Method: 使用单个共享TT对所有transformer子模块进行因式分解，通过索引结构轴；对比MetaTT与LoRA等微调方案。

Result: 在标准语言建模基准测试中，MetaTT参数减少最多，精度与LoRA相近，优于其他基于张量的方法，训练更简单。

Conclusion: MetaTT是一种高效的预训练transformer微调框架，参数压缩好、训练简单且可扩展到多任务。

Abstract: We present MetaTT, a unified Tensor Train (TT) adapter framework for global
low-rank fine-tuning of pre-trained transformers. Unlike LoRA, which fine-tunes
each weight matrix independently, MetaTT uses a single shared TT to factorize
all transformer sub-modules -- query, key, value, projection, and feed-forward
layers -- by indexing the structural axes like layer and matrix type, and
optionally heads and tasks. For a given rank, while LoRA adds parameters
proportional to the product across modes, MetaTT only adds parameters
proportional to the sum across modes leading to a significantly compressed
final adapter. Our benchmarks compare MetaTT with LoRA along with recent
state-of-the-art matrix and tensor decomposition based fine-tuning schemes. We
observe that when tested on standard language modeling benchmarks, MetaTT leads
to the most reduction in the parameters while maintaining similar accuracy to
LoRA and even outperforming other tensor-based methods. Unlike CP or other
rank-factorizations, the TT ansatz benefits from mature optimization routines
-- e.g., DMRG-style rank adaptive minimization in addition to Adam, which we
find simplifies training. Because new modes can be appended cheaply, MetaTT
naturally extends to shared adapters across many tasks without redesigning the
core tensor.

</details>


### [62] [G-Sim: Generative Simulations with Large Language Models and Gradient-Free Calibration](https://arxiv.org/abs/2506.09272)
*Samuel Holt,Max Ruiz Luyten,Antonin Berthon,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 现有模拟器构建方法存在问题，本文介绍混合框架G - Sim，结合LLM与经验校准，可生成可靠模拟器用于复杂决策。


<details>
  <summary>Details</summary>
Motivation: 现有模拟器构建方法在泛化性、准确性和经验一致性方面存在不足，需要构建更稳健的模拟器以在关键领域指导决策。

Method: 引入G - Sim框架，利用LLM迭代提出并完善模拟器核心组件和因果关系，结合领域知识，再用灵活校准技术估计参数，可处理非可微和随机模拟器。

Result: G - Sim通过整合领域先验和经验证据，能产生可靠的、有因果信息的模拟器。

Conclusion: G - Sim能缓解数据低效问题，为复杂决策提供稳健的系统级干预。

Abstract: Constructing robust simulators is essential for asking "what if?" questions
and guiding policy in critical domains like healthcare and logistics. However,
existing methods often struggle, either failing to generalize beyond historical
data or, when using Large Language Models (LLMs), suffering from inaccuracies
and poor empirical alignment. We introduce G-Sim, a hybrid framework that
automates simulator construction by synergizing LLM-driven structural design
with rigorous empirical calibration. G-Sim employs an LLM in an iterative loop
to propose and refine a simulator's core components and causal relationships,
guided by domain knowledge. This structure is then grounded in reality by
estimating its parameters using flexible calibration techniques. Specifically,
G-Sim can leverage methods that are both likelihood-free and gradient-free with
respect to the simulator, such as gradient-free optimization for direct
parameter estimation or simulation-based inference for obtaining a posterior
distribution over parameters. This allows it to handle non-differentiable and
stochastic simulators. By integrating domain priors with empirical evidence,
G-Sim produces reliable, causally-informed simulators, mitigating
data-inefficiency and enabling robust system-level interventions for complex
decision-making.

</details>


### [63] [Private Aggregation for Byzantine-Resilient Heterogeneous Federated Learning](https://arxiv.org/abs/2506.09870)
*Maximilian Egger,Rawad Bitar*

Main category: cs.LG

TL;DR: 本文提出多阶段方法解决联邦学习中数据异构时的拜占庭弹性和隐私保护问题，评估其有效性并研究与零阶估计方法的结合以降低通信成本。


<details>
  <summary>Details</summary>
Motivation: 现有应对拜占庭客户端和保护客户端数据隐私的方法在数据异构时失效，且预处理技术无法与隐私保护机制结合。

Method: 提出包含可验证秘密共享、安全聚合和定制对称私有信息检索方案的多阶段方法。

Result: 方案在多种攻击下表现良好，优于已知技术；研究了与零阶估计方法的相互作用以降低通信成本。

Conclusion: 所提方法可在数据异构下实现信息论隐私保证和拜占庭弹性，结合零阶估计方法使私有聚合可扩展。

Abstract: Ensuring resilience to Byzantine clients while maintaining the privacy of the
clients' data is a fundamental challenge in federated learning (FL). When the
clients' data is homogeneous, suitable countermeasures were studied from an
information-theoretic perspective utilizing secure aggregation techniques while
ensuring robust aggregation of the clients' gradients. However, the
countermeasures used fail when the clients' data is heterogeneous. Suitable
pre-processing techniques, such as nearest neighbor mixing, were recently shown
to enhance the performance of those countermeasures in the heterogeneous
setting. Nevertheless, those pre-processing techniques cannot be applied with
the introduced privacy-preserving mechanisms.
  We propose a multi-stage method encompassing a careful co-design of
verifiable secret sharing, secure aggregation, and a tailored symmetric private
information retrieval scheme to achieve information-theoretic privacy
guarantees and Byzantine resilience under data heterogeneity. We evaluate the
effectiveness of our scheme on a variety of attacks and show how it outperforms
the previously known techniques. Since the communication overhead of secure
aggregation is non-negligible, we investigate the interplay with zero-order
estimation methods that reduce the communication cost in state-of-the-art FL
tasks and thereby make private aggregation scalable.

</details>


### [64] [SensorLM: Learning the Language of Wearable Sensors](https://arxiv.org/abs/2506.09108)
*Yuwei Zhang,Kumar Ayush,Siyuan Qiao,A. Ali Heydari,Girish Narayanswamy,Maxwell A. Xu,Ahmed A. Metwally,Shawn Xu,Jake Garrison,Xuhai Xu,Tim Althoff,Yun Liu,Pushmeet Kohli,Jiening Zhan,Mark Malhotra,Shwetak Patel,Cecilia Mascolo,Xin Liu,Daniel McDuff,Yuzhe Yang*

Main category: cs.LG

TL;DR: 提出SensorLM传感器语言基础模型，通过分层字幕生成管道创建大规模数据集，在多项任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决现实世界可穿戴数据中缺乏配对、丰富注释的传感器 - 文本描述，导致传感器数据与语言对齐和解释困难的问题。

Method: 引入分层字幕生成管道来获取传感器数据信息，扩展多模态预训练架构。

Result: 创建了包含超5970万小时、超10.3万人数据的最大传感器 - 语言数据集，在人类活动分析和医疗保健的真实任务中表现优于现有技术。

Conclusion: SensorLM在零样本识别、少样本学习和跨模态检索等方面表现出色，具备多种能力。

Abstract: We present SensorLM, a family of sensor-language foundation models that
enable wearable sensor data understanding with natural language. Despite its
pervasive nature, aligning and interpreting sensor data with language remains
challenging due to the lack of paired, richly annotated sensor-text
descriptions in uncurated, real-world wearable data. We introduce a
hierarchical caption generation pipeline designed to capture statistical,
structural, and semantic information from sensor data. This approach enabled
the curation of the largest sensor-language dataset to date, comprising over
59.7 million hours of data from more than 103,000 people. Furthermore, SensorLM
extends prominent multimodal pretraining architectures (e.g., CLIP, CoCa) and
recovers them as specific variants within a generic architecture. Extensive
experiments on real-world tasks in human activity analysis and healthcare
verify the superior performance of SensorLM over state-of-the-art in zero-shot
recognition, few-shot learning, and cross-modal retrieval. SensorLM also
demonstrates intriguing capabilities including scaling behaviors, label
efficiency, sensor captioning, and zero-shot generalization to unseen tasks.

</details>


### [65] [Adversarial Surrogate Risk Bounds for Binary Classification](https://arxiv.org/abs/2506.09348)
*Natalie S. Frank*

Main category: cs.LG

TL;DR: 本文研究机器学习模型对抗攻击脆弱性问题，给出对抗替代风险收敛率的替代风险界，并推导标准学习设置下依赖分布的替代风险界。


<details>
  <summary>Details</summary>
Motivation: 现有工作未解决最小化对抗替代风险的函数序列中对抗分类风险收敛到最优值的速率问题，本文旨在解决该问题。

Method: 给出量化收敛率的替代风险界，并推导标准学习设置下依赖分布的替代风险界。

Result: 得到了对抗替代风险收敛率的替代风险界和标准学习设置下依赖分布的替代风险界。

Conclusion: 解决了对抗分类风险收敛速率量化问题，推导的标准学习设置下结果或有独立研究价值。

Abstract: A central concern in classification is the vulnerability of machine learning
models to adversarial attacks. Adversarial training is one of the most popular
techniques for training robust classifiers, which involves minimizing an
adversarial surrogate risk. Recent work characterized when a minimizing
sequence of an adversarial surrogate risk is also a minimizing sequence of the
adversarial classification risk for binary classification -- a property known
as adversarial consistency. However, these results do not address the rate at
which the adversarial classification risk converges to its optimal value for
such a sequence of functions that minimize the adversarial surrogate. This
paper provides surrogate risk bounds that quantify that convergence rate.
Additionally, we derive distribution-dependent surrogate risk bounds in the
standard (non-adversarial) learning setting, that may be of independent
interest.

</details>


### [66] [CodeBrain: Bridging Decoupled Tokenizer and Multi-Scale Architecture for EEG Foundation Model](https://arxiv.org/abs/2506.09110)
*Jingying Ma,Feng Wu,Qika Lin,Yucheng Xing,Chenyu Liu,Ziyu Jia,Mengling Feng*

Main category: cs.LG

TL;DR: 提出CodeBrain，一种与大脑组织对齐的高效EEG基础模型，在10个公开数据集实验证明其泛化性。


<details>
  <summary>Details</summary>
Motivation: 传统任务特定模型可迁移性受限，现有EEG基础模型异质表示能力有限、捕捉多尺度脑依赖效率低。

Method: 分两阶段训练CodeBrain，引入TFDual - Tokenizer独立标记异质时频分量，提出EEGSSM结合结构全局卷积架构和滑动窗口注意力机制，以掩码自监督学习目标训练。

Result: 在10个公开EEG数据集上的综合实验通过线性探测证明了CodeBrain的泛化性。

Conclusion: CodeBrain为EEG提供了有生物学依据且可解释的建模，为未来神经科学研究奠定基础，代码和预训练权重未来版本将发布。

Abstract: Electroencephalography (EEG) provides real-time insights into brain activity
and is widely used in neuroscience. However, variations in channel
configurations, sequence lengths, and task objectives limit the transferability
of traditional task-specific models. Although recent EEG foundation models
(EFMs) aim to learn generalizable representations, they struggle with limited
heterogeneous representation capacity and inefficiency in capturing multi-scale
brain dependencies. To address these challenges, we propose CodeBrain, an
efficient EFM structurally aligned with brain organization, trained in two
stages. (1) We introduce a TFDual-Tokenizer that independently tokenizes
heterogeneous temporal and frequency components, enabling a quadratic expansion
of the discrete representation space. This also offers a degree of
interpretability through cross-domain token analysis. (2) We propose the
EEGSSM, which combines a structured global convolution architecture and a
sliding window attention mechanism to jointly model sparse long-range and local
dependencies. Unlike fully connected Transformer models, EEGSSM better reflects
the brain's small-world topology and efficiently captures EEG's inherent
multi-scale structure. EEGSSM is trained with a masked self-supervised learning
objective to predict token indices obtained in TFDual-Tokenizer. Comprehensive
experiments on 10 public EEG datasets demonstrate the generalizability of
CodeBrain with linear probing. By offering biologically informed and
interpretable EEG modeling, CodeBrain lays the foundation for future
neuroscience research. Both code and pretraining weights will be released in
the future version.

</details>


### [67] [TRACE: Grounding Time Series in Context for Multimodal Embedding and Retrieval](https://arxiv.org/abs/2506.09114)
*Jialin Chen,Ziyu Zhao,Gaukhar Nurbek,Aosong Feng,Ali Maatouk,Leandros Tassiulas,Yifeng Gao,Rex Ying*

Main category: cs.LG

TL;DR: 提出通用多模态检索器TRACE，能将时间序列嵌入与文本上下文对齐，支持灵活跨模态检索，在下游任务表现出色。


<details>
  <summary>Details</summary>
Motivation: 动态数据普遍存在，时间序列数据检索需求大，但现有方法存在缺乏语义基础、难对齐异质模态等问题。

Method: 提出TRACE，实现细粒度通道级对齐，采用难负样本挖掘，支持多种跨模态检索模式。

Result: 能丰富下游模型信息，提高预测准确性和可解释性，在下游任务达SOTA性能。

Conclusion: TRACE兼具下游应用编码器和通用检索器的双重效用。

Abstract: The ubiquity of dynamic data in domains such as weather, healthcare, and
energy underscores a growing need for effective interpretation and retrieval of
time-series data. These data are inherently tied to domain-specific contexts,
such as clinical notes or weather narratives, making cross-modal retrieval
essential not only for downstream tasks but also for developing robust
time-series foundation models by retrieval-augmented generation (RAG). Despite
the increasing demand, time-series retrieval remains largely underexplored.
Existing methods often lack semantic grounding, struggle to align heterogeneous
modalities, and have limited capacity for handling multi-channel signals. To
address this gap, we propose TRACE, a generic multimodal retriever that grounds
time-series embeddings in aligned textual context. TRACE enables fine-grained
channel-level alignment and employs hard negative mining to facilitate
semantically meaningful retrieval. It supports flexible cross-modal retrieval
modes, including Text-to-Timeseries and Timeseries-to-Text, effectively linking
linguistic descriptions with complex temporal patterns. By retrieving
semantically relevant pairs, TRACE enriches downstream models with informative
context, leading to improved predictive accuracy and interpretability. Beyond a
static retrieval engine, TRACE also serves as a powerful standalone encoder,
with lightweight task-specific tuning that refines context-aware
representations while maintaining strong cross-modal alignment. These
representations achieve state-of-the-art performance on downstream forecasting
and classification tasks. Extensive experiments across multiple domains
highlight its dual utility, as both an effective encoder for downstream
applications and a general-purpose retriever to enhance time-series models.

</details>


### [68] [Safe Screening Rules for Group SLOPE](https://arxiv.org/abs/2506.09451)
*Runxue Bao,Quanchao Lu,Yanfu Zhang*

Main category: cs.LG

TL;DR: 本文针对Group SLOPE模型引入安全筛选规则，可高效识别无效组，提高计算效率和内存使用效率，且能与现有求解器集成，实验验证有效。


<details>
  <summary>Details</summary>
Motivation: Group SLOPE在处理块不可分离组效应时，现有方法无效或低效，导致高维场景下计算成本和内存使用高。

Method: 为Group SLOPE模型引入安全筛选规则，处理块不可分离组效应，识别零系数的无效组并在训练中排除。

Result: 实验结果表明该方法能有效检测无效特征组，显著提高计算效率且不影响准确性。

Conclusion: 所提出的筛选规则可安全用于现有优化算法，与原方法结果相同，能提高计算效率和内存使用效率。

Abstract: Variable selection is a challenging problem in high-dimensional sparse
learning, especially when group structures exist. Group SLOPE performs well for
the adaptive selection of groups of predictors. However, the block
non-separable group effects in Group SLOPE make existing methods either invalid
or inefficient. Consequently, Group SLOPE tends to incur significant
computational costs and memory usage in practical high-dimensional scenarios.
To overcome this issue, we introduce a safe screening rule tailored for the
Group SLOPE model, which efficiently identifies inactive groups with zero
coefficients by addressing the block non-separable group effects. By excluding
these inactive groups during training, we achieve considerable gains in
computational efficiency and memory usage. Importantly, the proposed screening
rule can be seamlessly integrated into existing solvers for both batch and
stochastic algorithms. Theoretically, we establish that our screening rule can
be safely employed with existing optimization algorithms, ensuring the same
results as the original approaches. Experimental results confirm that our
method effectively detects inactive feature groups and significantly boosts
computational efficiency without compromising accuracy.

</details>


### [69] [Efficient Preference-Based Reinforcement Learning: Randomized Exploration Meets Experimental Design](https://arxiv.org/abs/2506.09508)
*Andreas Schlaginhaufen,Reda Ouhamma,Maryam Kamgarpour*

Main category: cs.LG

TL;DR: 研究马尔可夫决策过程中基于人类反馈的强化学习，提出基于随机探索的元算法和改进算法，有理论保证且实验效果好。


<details>
  <summary>Details</summary>
Motivation: 在基于轨迹偏好比较的强化学习中，设计能选择信息丰富的偏好查询以识别潜在奖励并提供理论保证的算法。

Method: 提出基于随机探索的元算法，避免乐观方法的计算挑战；引入改进算法，收集轨迹对批次并应用最优实验设计选择查询，可并行化偏好查询。

Result: 在温和的强化学习预言机假设下建立了遗憾和最后迭代保证，实证评估表明该方法与基于奖励的强化学习有竞争力，且所需偏好查询数量少。

Conclusion: 所提出的方法在基于人类反馈的强化学习中有效，在理论和实践上均有优势。

Abstract: We study reinforcement learning from human feedback in general Markov
decision processes, where agents learn from trajectory-level preference
comparisons. A central challenge in this setting is to design algorithms that
select informative preference queries to identify the underlying reward while
ensuring theoretical guarantees. We propose a meta-algorithm based on
randomized exploration, which avoids the computational challenges associated
with optimistic approaches and remains tractable. We establish both regret and
last-iterate guarantees under mild reinforcement learning oracle assumptions.
To improve query complexity, we introduce and analyze an improved algorithm
that collects batches of trajectory pairs and applies optimal experimental
design to select informative comparison queries. The batch structure also
enables parallelization of preference queries, which is relevant in practical
deployment as feedback can be gathered concurrently. Empirical evaluation
confirms that the proposed method is competitive with reward-based
reinforcement learning while requiring a small number of preference queries.

</details>


### [70] [Improving LLM Agent Planning with In-Context Learning via Atomic Fact Augmentation and Lookahead Search](https://arxiv.org/abs/2506.09171)
*Samuel Holt,Max Ruiz Luyten,Thomas Pouplin,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 提出新的大语言模型代理框架，通过上下文学习增强规划能力，在交互式任务中表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂交互环境中有效执行困难，难适应新信息和利用过往经验进行多步推理。

Method: 引入新框架，通过原子事实增强和递归前瞻搜索进行上下文学习，提取关键原子事实动态增强提示，用深度受限前瞻搜索进行规划。

Result: 代理在挑战性交互任务中性能和适应性提升，随经验积累行为更优。

Conclusion: 该方法可在不更新权重情况下，利用经验改进决策和行为，性能与事实抽象质量和LLM模拟精度有关。

Abstract: Large Language Models (LLMs) are increasingly capable but often require
significant guidance or extensive interaction history to perform effectively in
complex, interactive environments. Existing methods may struggle with adapting
to new information or efficiently utilizing past experiences for multi-step
reasoning without fine-tuning. We introduce a novel LLM agent framework that
enhances planning capabilities through in-context learning, facilitated by
atomic fact augmentation and a recursive lookahead search. Our agent learns to
extract task-critical ``atomic facts'' from its interaction trajectories. These
facts dynamically augment the prompts provided to LLM-based components
responsible for action proposal, latent world model simulation, and state-value
estimation. Planning is performed via a depth-limited lookahead search, where
the LLM simulates potential trajectories and evaluates their outcomes, guided
by the accumulated facts and interaction history. This approach allows the
agent to improve its understanding and decision-making online, leveraging its
experience to refine its behavior without weight updates. We provide a
theoretical motivation linking performance to the quality of fact-based
abstraction and LLM simulation accuracy. Empirically, our agent demonstrates
improved performance and adaptability on challenging interactive tasks,
achieving more optimal behavior as it accumulates experience, showcased in
tasks such as TextFrozenLake and ALFWorld.

</details>


### [71] [MultiNet: An Open-Source Software Toolkit \& Benchmark Suite for the Evaluation and Adaptation of Multimodal Action Models](https://arxiv.org/abs/2506.09172)
*Pranav Guruprasad,Yangyue Wang,Harshvardhan Sikka*

Main category: cs.LG

TL;DR: 介绍MultiNet这一开源基准和软件生态系统，用于评估和适配跨视觉、语言和行动领域的模型，并说明其在下游研究中的应用。


<details>
  <summary>Details</summary>
Motivation: 为了严格评估和适配跨视觉、语言和行动领域的模型，推动通用智能体系统的发展。

Method: 建立评估视觉 - 语言模型和视觉 - 语言 - 行动模型的标准化协议，提供开源软件用于下载数据、模型和评估，提供包含多种任务的复合数据集。

Result: MultiNet基准、框架、工具包和评估工具已用于下游关于视觉 - 语言 - 行动模型泛化局限性的研究。

Conclusion: MultiNet为评估和适配多模态行动模型提供了有效工具，对下游研究有积极作用。

Abstract: Recent innovations in multimodal action models represent a promising
direction for developing general-purpose agentic systems, combining visual
understanding, language comprehension, and action generation. We introduce
MultiNet - a novel, fully open-source benchmark and surrounding software
ecosystem designed to rigorously evaluate and adapt models across vision,
language, and action domains. We establish standardized evaluation protocols
for assessing vision-language models (VLMs) and vision-language-action models
(VLAs), and provide open source software to download relevant data, models, and
evaluations. Additionally, we provide a composite dataset with over 1.3
trillion tokens of image captioning, visual question answering, commonsense
reasoning, robotic control, digital game-play, simulated
locomotion/manipulation, and many more tasks. The MultiNet benchmark,
framework, toolkit, and evaluation harness have been used in downstream
research on the limitations of VLA generalization.

</details>


### [72] [On the Similarities of Embeddings in Contrastive Learning](https://arxiv.org/abs/2506.09781)
*Chungpa Lee,Sehee Lim,Kibok Lee,Jy-yong Sohn*

Main category: cs.LG

TL;DR: 提出统一框架理解对比学习，分析不同设置问题并引入辅助损失改善小批量训练性能


<details>
  <summary>Details</summary>
Motivation: 现有工作缺乏系统解释对比学习目标的综合框架

Method: 基于分析正负样本对嵌入的余弦相似度构建统一框架，针对不同批量设置分析问题，引入辅助损失项

Result: 小批量训练中负样本对相似度方差高，引入辅助损失可降低方差

Conclusion: 引入的损失能持续提升对比学习方法在小批量训练中的性能

Abstract: Contrastive learning (CL) operates on a simple yet effective principle:
embeddings of positive pairs are pulled together, while those of negative pairs
are pushed apart. Although various forms of contrastive loss have been proposed
and analyzed from different perspectives, prior works lack a comprehensive
framework that systematically explains a broad class of these objectives. In
this paper, we present a unified framework for understanding CL, which is based
on analyzing the cosine similarity between embeddings of positive and negative
pairs. In full-batch settings, we show that perfect alignment of positive pairs
is unattainable when similarities of negative pairs fall below a certain
threshold, and that this misalignment can be alleviated by incorporating
within-view negative pairs. In mini-batch settings, we demonstrate that smaller
batch sizes incur stronger separation among negative pairs within batches,
which leads to higher variance in similarities of negative pairs. To address
this limitation of mini-batch CL, we introduce an auxiliary loss term that
reduces the variance of similarities of negative pairs in CL. Empirical results
demonstrate that incorporating the proposed loss consistently improves the
performance of CL methods in small-batch training.

</details>


### [73] [The Curious Language Model: Strategic Test-Time Information Acquisition](https://arxiv.org/abs/2506.09173)
*Michael Cooper,Rohan Wadhawan,John Michael Giorgi,Chenhao Tan,Davis Liang*

Main category: cs.LG

TL;DR: 提出CuriosiTree用于大语言模型零样本信息获取，在临床诊断模拟中表现优于基线策略。


<details>
  <summary>Details</summary>
Motivation: 决策者信息不足时获取信息不同方式成本不同，需选择兼具信息性和成本效益的行动。

Method: 提出基于启发式的CuriosiTree策略，用贪心树搜索估计行动预期信息增益，基于预期信息增益和成本平衡选择行动。

Result: 在临床诊断模拟中，CuriosiTree能实现异质信息源的成本效益集成，在选择行动序列以实现准确诊断方面优于基线策略。

Conclusion: CuriosiTree是一种有效的大语言模型零样本信息获取策略。

Abstract: Decision-makers often possess insufficient information to render a confident
decision. In these cases, the decision-maker can often undertake actions to
acquire the necessary information about the problem at hand, e.g., by
consulting knowledgeable authorities or by conducting experiments. Importantly,
different levers of information acquisition come with different costs, posing
the challenge of selecting the actions that are both informative and
cost-effective. In this work, we propose CuriosiTree, a heuristic-based,
test-time policy for zero-shot information acquisition in large language models
(LLMs). CuriosiTree employs a greedy tree search to estimate the expected
information gain of each action and strategically chooses actions based on a
balance of anticipated information gain and associated cost. Empirical
validation in a clinical diagnosis simulation shows that CuriosiTree enables
cost-effective integration of heterogenous sources of information, and
outperforms baseline action selection strategies in selecting action sequences
that enable accurate diagnosis.

</details>


### [74] [Multivariate Long-term Time Series Forecasting with Fourier Neural Filter](https://arxiv.org/abs/2506.09174)
*Chenheng Xu,Dan Wu,Yixin Zhu,Ying Nian Wu*

Main category: cs.LG

TL;DR: 提出FNF骨干和DBD架构用于多变量长期时间序列预测，经理论分析和实证评估，展现出优异性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法难以同时捕捉时间序列变量内的时间依赖和变量间的空间关联，且缺乏有时间特定归纳偏置的专用骨干。

Method: 引入FNF作为骨干，DBD作为架构进行时空建模，进行理论分析和多领域公开数据集实证评估。

Result: 在11个公开基准数据集上达到了最先进性能，无需辅助技术。

Conclusion: 设计合理的神经架构可捕捉时间序列固有属性，有望改变科研和工业应用中的时间序列建模。

Abstract: Multivariate long-term time series forecasting has been suffering from the
challenge of capturing both temporal dependencies within variables and spatial
correlations across variables simultaneously. Current approaches predominantly
repurpose backbones from natural language processing or computer vision (e.g.,
Transformers), which fail to adequately address the unique properties of time
series (e.g., periodicity). The research community lacks a dedicated backbone
with temporal-specific inductive biases, instead relying on domain-agnostic
backbones supplemented with auxiliary techniques (e.g., signal decomposition).
We introduce FNF as the backbone and DBD as the architecture to provide
excellent learning capabilities and optimal learning pathways for
spatio-temporal modeling, respectively. Our theoretical analysis proves that
FNF unifies local time-domain and global frequency-domain information
processing within a single backbone that extends naturally to spatial modeling,
while information bottleneck theory demonstrates that DBD provides superior
gradient flow and representation capacity compared to existing unified or
sequential architectures. Our empirical evaluation across 11 public benchmark
datasets spanning five domains (energy, meteorology, transportation,
environment, and nature) confirms state-of-the-art performance with consistent
hyperparameter settings. Notably, our approach achieves these results without
any auxiliary techniques, suggesting that properly designed neural
architectures can capture the inherent properties of time series, potentially
transforming time series modeling in scientific and industrial applications.

</details>


### [75] [Learning single-index models via harmonic decomposition](https://arxiv.org/abs/2506.09887)
*Nirmit Joshi,Hugo Koubbi,Theodor Misiakiewicz,Nathan Srebro*

Main category: cs.LG

TL;DR: 本文提出用‘球谐函数’研究单指标模型学习问题，刻画任意球对称输入分布下学习复杂度，引入两类估计器，对高斯输入情况有新发现。


<details>
  <summary>Details</summary>
Motivation: 先前工作在高斯输入下用埃尔米特展开研究单指标模型中恢复未知投影向量的复杂度，本文认为‘球谐函数’更能捕捉问题的内在‘旋转对称性’，从而提出新视角。

Method: 基于球谐函数的新视角，刻画任意球对称输入分布下学习单指标模型的复杂度，引入基于张量展开和在线SGD的两类估计器。

Result: 两类估计器分别实现了最优样本复杂度或最优运行时间，且一般情况下难以有估计器同时实现两者；在高斯输入下，理论能恢复并澄清现有结果，还揭示了新现象。

Conclusion: ‘球谐函数’是研究单指标模型学习问题的自然基础，对该问题的复杂度刻画和估计器设计有重要意义。

Abstract: We study the problem of learning single-index models, where the label $y \in
\mathbb{R}$ depends on the input $\boldsymbol{x} \in \mathbb{R}^d$ only through
an unknown one-dimensional projection $\langle
\boldsymbol{w}_*,\boldsymbol{x}\rangle$. Prior work has shown that under
Gaussian inputs, the statistical and computational complexity of recovering
$\boldsymbol{w}_*$ is governed by the Hermite expansion of the link function.
In this paper, we propose a new perspective: we argue that "spherical
harmonics" -- rather than "Hermite polynomials" -- provide the natural basis
for this problem, as they capture its intrinsic "rotational symmetry". Building
on this insight, we characterize the complexity of learning single-index models
under arbitrary spherically symmetric input distributions. We introduce two
families of estimators -- based on tensor unfolding and online SGD -- that
respectively achieve either optimal sample complexity or optimal runtime, and
argue that estimators achieving both may not exist in general. When specialized
to Gaussian inputs, our theory not only recovers and clarifies existing results
but also reveals new phenomena that had previously been overlooked.

</details>


### [76] [Multi-Task Reward Learning from Human Ratings](https://arxiv.org/abs/2506.09183)
*Mingkang Wu,Devin White,Evelyn Rose,Vernon Lawhern,Nicholas R Waytowich,Yongcan Cao*

Main category: cs.LG

TL;DR: 提出模仿人类决策的强化学习方法，用合成人类评分验证，效果超现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于人类反馈的强化学习方法将人类推理简化为孤立任务，未考虑人类决策时整合多种策略的特点。

Method: 提出新的强化学习方法，联合考虑多个任务，利用无奖励环境中的人类评分推断奖励函数，引入可学习权重平衡分类和回归模型贡献。

Result: 所提方法持续超越现有基于评分的强化学习方法，部分情况超越传统强化学习方法。

Conclusion: 新方法能更好模仿人类决策，具有有效性和优势。

Abstract: Reinforcement learning from human feeback (RLHF) has become a key factor in
aligning model behavior with users' goals. However, while humans integrate
multiple strategies when making decisions, current RLHF approaches often
simplify this process by modeling human reasoning through isolated tasks such
as classification or regression. In this paper, we propose a novel
reinforcement learning (RL) method that mimics human decision-making by jointly
considering multiple tasks. Specifically, we leverage human ratings in
reward-free environments to infer a reward function, introducing learnable
weights that balance the contributions of both classification and regression
models. This design captures the inherent uncertainty in human decision-making
and allows the model to adaptively emphasize different strategies. We conduct
several experiments using synthetic human ratings to validate the effectiveness
of the proposed approach. Results show that our method consistently outperforms
existing rating-based RL methods, and in some cases, even surpasses traditional
RL approaches.

</details>


### [77] [Bayesian Probabilistic Matrix Factorization](https://arxiv.org/abs/2506.09928)
*Ruixuan Xu,Xiangxiang Weng*

Main category: cs.LG

TL;DR: 使用MCMC和VI近似PMF后验分布，在MovieLens数据集评估性能，VI收敛快，MCMC估计后验更准确。


<details>
  <summary>Details</summary>
Motivation: PMF计算后验分布因高维积分难以处理，需解决该问题。

Method: 采用MCMC和VI两种贝叶斯推理方法近似后验分布，并在MovieLens数据集评估。

Result: 实验表明VI收敛更快，MCMC提供更准确的后验估计。

Conclusion: MCMC和VI各有优势，可根据需求选用。

Abstract: Matrix factorization is a widely used technique in recommendation systems.
Probabilistic Matrix Factorization (PMF) [1] extends traditional matrix
factorization by incorporating probability distributions over latent factors,
allowing for uncertainty quantification. However, computing the posterior
distribution is intractable due to the high-dimensional integral. To address
this, we employ two Bayesian inference methods: Markov Chain Monte Carlo (MCMC)
[2] and Variational Inference (VI) [3] to approximate the posterior. We
evaluate their performance on MovieLens dataset and compare their convergence
speed, predictive accuracy, and computational efficiency. Experimental results
demonstrate that VI offers faster convergence, while MCMC provides more
accurate posterior estimates.

</details>


### [78] [LaDCast: A Latent Diffusion Model for Medium-Range Ensemble Weather Forecasting](https://arxiv.org/abs/2506.09193)
*Yilin Zhuang,Karthik Duraisamy*

Main category: cs.LG

TL;DR: 介绍用于中期集合预报的全球潜在扩散框架LaDCast，能在潜在空间生成预报，减少存储和计算，性能接近IFS - ENS，追踪极端事件表现更优。


<details>
  <summary>Details</summary>
Motivation: 准确概率天气预报需高精度和高效不确定性量化，现有集合数值天气预报和机器学习方法难以应对这些挑战。

Method: 引入LaDCast框架，用自编码器将高维ERA5再分析场压缩，基于Transformer的扩散模型生成顺序潜在更新，融入GeoRoPE、双流注意力机制和正弦时间嵌入。

Result: LaDCast确定性和概率技能接近欧洲中期天气预报中心IFS - ENS，无需显式扰动，追踪罕见极端事件表现优于现有模型，能大幅减少存储和计算。

Conclusion: LaDCast为实时公里级分辨率预报提供了可行途径，代码和模型已开源。

Abstract: Accurate probabilistic weather forecasting demands both high accuracy and
efficient uncertainty quantification, challenges that overburden both ensemble
numerical weather prediction (NWP) and recent machine-learning methods. We
introduce LaDCast, the first global latent-diffusion framework for medium-range
ensemble forecasting, which generates hourly ensemble forecasts entirely in a
learned latent space. An autoencoder compresses high-dimensional ERA5
reanalysis fields into a compact representation, and a transformer-based
diffusion model produces sequential latent updates with arbitrary hour
initialization. The model incorporates Geometric Rotary Position Embedding
(GeoRoPE) to account for the Earth's spherical geometry, a dual-stream
attention mechanism for efficient conditioning, and sinusoidal temporal
embeddings to capture seasonal patterns. LaDCast achieves deterministic and
probabilistic skill close to that of the European Centre for Medium-Range
Forecast IFS-ENS, without any explicit perturbations. Notably, LaDCast
demonstrates superior performance in tracking rare extreme events such as
cyclones, capturing their trajectories more accurately than established models.
By operating in latent space, LaDCast reduces storage and compute by orders of
magnitude, demonstrating a practical path toward forecasting at kilometer-scale
resolution in real time. We open-source our code and models and provide the
training and evaluation pipelines at: https://github.com/tonyzyl/ladcast.

</details>


### [79] [The Sample Complexity of Online Strategic Decision Making with Information Asymmetry and Knowledge Transportability](https://arxiv.org/abs/2506.09940)
*Jiachen Hu,Rui Ai,Han Zhong,Xiaoyu Chen,Liwei Wang,Zhaoran Wang,Zhuoran Yang*

Main category: cs.LG

TL;DR: 本文探讨在线学习中利用非独立同分布动作在知识迁移时学习混杂因素的问题，提出样本高效算法，可在信息不对称下识别系统动态并应对知识迁移挑战，实现ε - 最优策略学习。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统中存在信息不对称和知识可迁移性两大挑战，前者因策略行为和混杂变量产生复杂性，后者源于目标环境实验困难需知识迁移，由此引出在线学习中的相关问题。

Method: 提出一种样本高效算法，基于在线战略交互模型，用于强化学习。

Result: 该方法能以O(1/ε²)的紧密样本复杂度实现ε - 最优策略的学习。

Conclusion: 所提算法可以在信息不对称下准确识别系统动态，并有效应对知识迁移挑战。

Abstract: Information asymmetry is a pervasive feature of multi-agent systems,
especially evident in economics and social sciences. In these settings, agents
tailor their actions based on private information to maximize their rewards.
These strategic behaviors often introduce complexities due to confounding
variables. Simultaneously, knowledge transportability poses another significant
challenge, arising from the difficulties of conducting experiments in target
environments. It requires transferring knowledge from environments where
empirical data is more readily available. Against these backdrops, this paper
explores a fundamental question in online learning: Can we employ non-i.i.d.
actions to learn about confounders even when requiring knowledge transfer? We
present a sample-efficient algorithm designed to accurately identify system
dynamics under information asymmetry and to navigate the challenges of
knowledge transfer effectively in reinforcement learning, framed within an
online strategic interaction model. Our method provably achieves learning of an
$\epsilon$-optimal policy with a tight sample complexity of $O(1/\epsilon^2)$.

</details>


### [80] [FedRAG: A Framework for Fine-Tuning Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2506.09200)
*Val Andrei Fajardo,David B. Emerson,Amandeep Singh,Veronica Chatrath,Marcelo Lotif,Ravi Theja,Alex Cheung,Izuki Matsubi*

Main category: cs.LG

TL;DR: 提出FedRAG框架用于跨集中式和联邦式架构微调检索增强生成（RAG）系统，支持先进微调方法，与RAG生态深度集成。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明微调RAG系统的检索器和生成器模型可提升性能，但缺乏跨集中式和联邦式架构的微调框架。

Method: 引入FedRAG框架，支持先进微调方法，提供简单直观接口，可无缝转换训练任务。

Result: FedRAG框架支持先进微调方法，可实现从集中式到联邦式训练任务的无缝转换，且与现代RAG生态深度集成。

Conclusion: FedRAG框架填补了现有工具的关键空白。

Abstract: Retrieval-augmented generation (RAG) systems have been shown to be effective
in addressing many of the drawbacks of relying solely on the parametric memory
of large language models. Recent work has demonstrated that RAG systems can be
improved via fine-tuning of their retriever and generator models. In this work,
we introduce FedRAG, a framework for fine-tuning RAG systems across centralized
and federated architectures. FedRAG supports state-of-the-art fine-tuning
methods, offering a simple and intuitive interface and a seamless conversion
from centralized to federated training tasks. FedRAG is also deeply integrated
with the modern RAG ecosystem, filling a critical gap in available tools.

</details>


### [81] [Policy-Based Trajectory Clustering in Offline Reinforcement Learning](https://arxiv.org/abs/2506.09202)
*Hao Hu,Xinqi Wang,Simon Shaolei Du*

Main category: cs.LG

TL;DR: 本文提出离线强化学习数据集轨迹聚类新任务，提出PG - Kmeans和CAAE方法，理论证明PG - Kmeans收敛性，实验验证方法有效性。


<details>
  <summary>Details</summary>
Motivation: 解决离线强化学习数据集中轨迹聚类问题，每个簇中心代表生成轨迹的策略。

Method: 提出Policy - Guided Kmeans (PG - Kmeans) 和Centroid - Attracted Autoencoder (CAAE) 方法。PG - Kmeans迭代训练行为克隆策略并根据策略生成概率分配轨迹，CAAE引导轨迹的潜在表示靠近特定码本条目实现聚类。

Result: 在D4RL数据集和自定义GridWorld环境上实验表明，PG - Kmeans和CAAE能有效将轨迹划分为有意义的簇。

Conclusion: 两种方法为基于策略的轨迹聚类提供了有前景的框架，在离线强化学习及其他领域有广泛应用。

Abstract: We introduce a novel task of clustering trajectories from offline
reinforcement learning (RL) datasets, where each cluster center represents the
policy that generated its trajectories. By leveraging the connection between
the KL-divergence of offline trajectory distributions and a mixture of
policy-induced distributions, we formulate a natural clustering objective. To
solve this, we propose Policy-Guided K-means (PG-Kmeans) and Centroid-Attracted
Autoencoder (CAAE). PG-Kmeans iteratively trains behavior cloning (BC) policies
and assigns trajectories based on policy generation probabilities, while CAAE
resembles the VQ-VAE framework by guiding the latent representations of
trajectories toward the vicinity of specific codebook entries to achieve
clustering. Theoretically, we prove the finite-step convergence of PG-Kmeans
and identify a key challenge in offline trajectory clustering: the inherent
ambiguity of optimal solutions due to policy-induced conflicts, which can
result in multiple equally valid but structurally distinct clusterings.
Experimentally, we validate our methods on the widely used D4RL dataset and
custom GridWorld environments. Our results show that both PG-Kmeans and CAAE
effectively partition trajectories into meaningful clusters. They offer a
promising framework for policy-based trajectory clustering, with broad
applications in offline RL and beyond.

</details>


### [82] [mLaSDI: Multi-stage latent space dynamics identification](https://arxiv.org/abs/2506.09207)
*William Anderson,Kevin Chung,Youngsoo Choi*

Main category: cs.LG

TL;DR: 为解决偏微分方程数值解计算成本高问题，提出LaSDI框架，但存在重建数据困难，本文提出mLaSDI，减少误差和训练时间。


<details>
  <summary>Details</summary>
Motivation: 现有LaSDI框架的自编码器在复杂或高频场景下难以准确重建训练数据并满足潜在空间动力学要求。

Method: 提出多阶段潜在空间动力学识别（mLaSDI），通过分阶段顺序训练多个自编码器，每个自编码器纠正前一阶段的误差。

Result: 使用小型自编码器应用mLaSDI可降低预测和重建误差，同时减少训练时间。

Conclusion: mLaSDI在解决偏微分方程数值解问题上比LaSDI更有效。

Abstract: Determining accurate numerical solutions of partial differential equations
(PDEs) is an important task in many scientific disciplines. However, solvers
can be computationally expensive, leading to the development of reduced-order
models (ROMs). Recently, Latent Space Dynamics Identification (LaSDI) was
proposed as a data-driven, non-intrusive ROM framework. LaSDI compresses the
training data using an autoencoder and learns a system of user-chosen ordinary
differential equations (ODEs), which govern the latent space dynamics. This
allows for rapid predictions by interpolating and evolving the low-dimensional
ODEs in the latent space. While LaSDI has produced effective ROMs for numerous
problems, the autoencoder can have difficulty accurately reconstructing
training data while also satisfying the imposed dynamics in the latent space,
particularly in complex or high-frequency regimes. To address this, we propose
multi-stage Latent Space Dynamics Identification (mLaSDI). With mLaSDI, several
autoencoders are trained sequentially in stages, where each autoencoder learns
to correct the error of the previous stages. We find that applying mLaSDI with
small autoencoders results in lower prediction and reconstruction errors, while
also reducing training time compared to LaSDI.

</details>


### [83] [Robust Noise Attenuation via Adaptive Pooling of Transformer Outputs](https://arxiv.org/abs/2506.09215)
*Greyson Brothers*

Main category: cs.LG

TL;DR: 研究用于汇总transformer嵌入模型输出的池化方法设计，指出标准方法易受信噪比影响，提出基于注意力的自适应池化方法并验证其优势。


<details>
  <summary>Details</summary>
Motivation: 主要受强化学习和视觉应用的驱动，研究解决输入中信号和噪声问题的池化方法。

Method: 将池化视为向量量化，以最小化信号损失为目标，提出基于注意力的自适应池化方法。

Result: 标准池化方法在信噪比波动时易性能崩溃，自适应池化方法能在误差范围内逼近信号最优向量量化器，在多个任务中表现出更强的鲁棒性。

Conclusion: 自适应池化方法在处理含噪声输入的transformer模型中具有优越性。

Abstract: We investigate the design of pooling methods used to summarize the outputs of
transformer embedding models, primarily motivated by reinforcement learning and
vision applications. This work considers problems where a subset of the input
vectors contains requisite information for a downstream task (signal) while the
rest are distractors (noise). By framing pooling as vector quantization with
the goal of minimizing signal loss, we demonstrate that the standard methods
used to aggregate transformer outputs, AvgPool, MaxPool, and ClsToken, are
vulnerable to performance collapse as the signal-to-noise ratio (SNR) of inputs
fluctuates. We then show that an attention-based adaptive pooling method can
approximate the signal-optimal vector quantizer within derived error bounds for
any SNR. Our theoretical results are first validated by supervised experiments
on a synthetic dataset designed to isolate the SNR problem, then generalized to
standard relational reasoning, multi-agent reinforcement learning, and vision
benchmarks with noisy observations, where transformers with adaptive pooling
display superior robustness across tasks.

</details>


### [84] [SoK: Machine Unlearning for Large Language Models](https://arxiv.org/abs/2506.09227)
*Jie Ren,Yue Xing,Yingqian Cui,Charu C. Aggarwal,Hui Liu*

Main category: cs.LG

TL;DR: 提出基于意图视角的大语言模型遗忘技术新分类法，有三项关键贡献，提供理解和推进生成式AI遗忘的框架。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型遗忘技术分类法忽视遗忘潜在意图这一基本维度，需新分类法。

Method: 提出基于意图导向视角的新分类法，在此基础上进行相关探讨和研究。

Result: 重新审视遗忘方法的本质，调研评估策略并指出局限，强调阻碍方法广泛应用的实际挑战。

Conclusion: 该工作为生成式AI遗忘提供综合框架，支持研究和指导政策决策。

Abstract: Large language model (LLM) unlearning has become a critical topic in machine
learning, aiming to eliminate the influence of specific training data or
knowledge without retraining the model from scratch. A variety of techniques
have been proposed, including Gradient Ascent, model editing, and re-steering
hidden representations. While existing surveys often organize these methods by
their technical characteristics, such classifications tend to overlook a more
fundamental dimension: the underlying intention of unlearning--whether it seeks
to truly remove internal knowledge or merely suppress its behavioral effects.
In this SoK paper, we propose a new taxonomy based on this intention-oriented
perspective. Building on this taxonomy, we make three key contributions. First,
we revisit recent findings suggesting that many removal methods may
functionally behave like suppression, and explore whether true removal is
necessary or achievable. Second, we survey existing evaluation strategies,
identify limitations in current metrics and benchmarks, and suggest directions
for developing more reliable and intention-aligned evaluations. Third, we
highlight practical challenges--such as scalability and support for sequential
unlearning--that currently hinder the broader deployment of unlearning methods.
In summary, this work offers a comprehensive framework for understanding and
advancing unlearning in generative AI, aiming to support future research and
guide policy decisions around data removal and privacy.

</details>


### [85] [Agent-based Condition Monitoring Assistance with Multimodal Industrial Database Retrieval Augmented Generation](https://arxiv.org/abs/2506.09247)
*Karl Löwenmark,Daniel Strömbergsson,Chang Liu,Marcus Liwicki,Fredrik Sandin*

Main category: cs.LG

TL;DR: 文章将基于大语言模型的推理代理与状态监测（CM）工作流程集成，提出MindRAG框架，初步结果显示其能为警报管理提供决策支持，提升CM系统可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前计算机化维护系统在故障严重程度估计和维护决策等任务依赖人工，自动分析决策不确定性大、误报率高，需解决降低误报、提升故障严重程度估计等问题。

Method: 提出MindRAG模块化框架，结合多模态检索增强生成（RAG）与专为CM数据设计的新型向量存储结构，利用现有注释和维护工单作为监督学习标签。

Result: 经经验丰富的分析师评估，MindRAG能为警报管理提供有意义的决策支持。

Conclusion: MindRAG可提高CM系统的可解释性，实现更高效的警报管理。

Abstract: Condition monitoring (CM) plays a crucial role in ensuring reliability and
efficiency in the process industry. Although computerised maintenance systems
effectively detect and classify faults, tasks like fault severity estimation,
and maintenance decisions still largely depend on human expert analysis. The
analysis and decision making automatically performed by current systems
typically exhibit considerable uncertainty and high false alarm rates, leading
to increased workload and reduced efficiency.
  This work integrates large language model (LLM)-based reasoning agents with
CM workflows to address analyst and industry needs, namely reducing false
alarms, enhancing fault severity estimation, improving decision support, and
offering explainable interfaces. We propose MindRAG, a modular framework
combining multimodal retrieval-augmented generation (RAG) with novel vector
store structures designed specifically for CM data. The framework leverages
existing annotations and maintenance work orders as surrogates for labels in a
supervised learning protocol, addressing the common challenge of training
predictive models on unlabelled and noisy real-world datasets.
  The primary contributions include: (1) an approach for structuring industry
CM data into a semi-structured multimodal vector store compatible with
LLM-driven workflows; (2) developing multimodal RAG techniques tailored for CM
data; (3) developing practical reasoning agents capable of addressing
real-world CM queries; and (4) presenting an experimental framework for
integrating and evaluating such agents in realistic industrial scenarios.
Preliminary results, evaluated with the help of an experienced analyst,
indicate that MindRAG provide meaningful decision support for more efficient
management of alarms, thereby improving the interpretability of CM systems.

</details>


### [86] [Uncertainty Prioritized Experience Replay](https://arxiv.org/abs/2506.09270)
*Rodrigo Carrasco-Davis,Sebastian Lee,Claudia Clopath,Will Dabney*

Main category: cs.LG

TL;DR: 提出用认知不确定性估计指导回放缓冲区过渡优先级排序，在玩具模型和Atari套件上验证其效果。


<details>
  <summary>Details</summary>
Motivation: 传统基于时间差分误差的优先经验回放易受噪声干扰，类似探索文献中的嘈杂电视问题，需减轻价值估计中噪声的影响。

Method: 使用认知不确定性估计来指导回放缓冲区过渡的优先级排序，先在两个表格玩具模型中验证，再在Atari套件上评估。

Result: 在Atari套件上的表现优于分位数回归深度Q学习基准。

Conclusion: 为强化学习智能体中使用不确定性优先回放开辟了道路。

Abstract: Prioritized experience replay, which improves sample efficiency by selecting
relevant transitions to update parameter estimates, is a crucial component of
contemporary value-based deep reinforcement learning models. Typically,
transitions are prioritized based on their temporal difference error. However,
this approach is prone to favoring noisy transitions, even when the value
estimation closely approximates the target mean. This phenomenon resembles the
noisy TV problem postulated in the exploration literature, in which
exploration-guided agents get stuck by mistaking noise for novelty. To mitigate
the disruptive effects of noise in value estimation, we propose using epistemic
uncertainty estimation to guide the prioritization of transitions from the
replay buffer. Epistemic uncertainty quantifies the uncertainty that can be
reduced by learning, hence reducing transitions sampled from the buffer
generated by unpredictable random processes. We first illustrate the benefits
of epistemic uncertainty prioritized replay in two tabular toy models: a simple
multi-arm bandit task, and a noisy gridworld. Subsequently, we evaluate our
prioritization scheme on the Atari suite, outperforming quantile regression
deep Q-learning benchmarks; thus forging a path for the use of uncertainty
prioritized replay in reinforcement learning agents.

</details>


### [87] [Learning The Minimum Action Distance](https://arxiv.org/abs/2506.09276)
*Lorenzo Steccanella,Joshua B. Evans,Özgür Şimşek,Anders Jonsson*

Main category: cs.LG

TL;DR: 提出一种仅从状态轨迹学习的马尔可夫决策过程状态表示框架，通过学习最小动作距离（MAD）构建嵌入空间，在多种环境中表现出色。


<details>
  <summary>Details</summary>
Motivation: 构建一种无需奖励信号和智能体执行动作，仅从状态轨迹学习的马尔可夫决策过程状态表示框架。

Method: 学习最小动作距离（MAD）作为基本度量，采用自监督学习方法构建嵌入空间，使嵌入状态对之间的距离对应其MAD。

Result: 在多种环境中高效学习准确的MAD表示，在表示质量上显著优于现有状态表示方法。

Conclusion: 所提方法在学习状态表示方面有效且表现优异。

Abstract: This paper presents a state representation framework for Markov decision
processes (MDPs) that can be learned solely from state trajectories, requiring
neither reward signals nor the actions executed by the agent. We propose
learning the minimum action distance (MAD), defined as the minimum number of
actions required to transition between states, as a fundamental metric that
captures the underlying structure of an environment. MAD naturally enables
critical downstream tasks such as goal-conditioned reinforcement learning and
reward shaping by providing a dense, geometrically meaningful measure of
progress. Our self-supervised learning approach constructs an embedding space
where the distances between embedded state pairs correspond to their MAD,
accommodating both symmetric and asymmetric approximations. We evaluate the
framework on a comprehensive suite of environments with known MAD values,
encompassing both deterministic and stochastic dynamics, as well as discrete
and continuous state spaces, and environments with noisy observations.
Empirical results demonstrate that the proposed approach not only efficiently
learns accurate MAD representations across these diverse settings but also
significantly outperforms existing state representation methods in terms of
representation quality.

</details>


### [88] [Causal Graph Recovery in Neuroimaging through Answer Set Programming](https://arxiv.org/abs/2506.09286)
*Mohammadsajad Abavisani,Kseniya Solovyeva,David Danks,Vince Calhoun,Sergey Plis*

Main category: cs.LG

TL;DR: 本文针对从时间序列数据学习因果图结构时因次采样导致信息丢失的问题，用约束优化方法（ASP）进行处理，在模拟数据和实证数据上验证了方法优越性。


<details>
  <summary>Details</summary>
Motivation: 解决从时间序列数据学习因果图结构时，测量频率与系统因果时间尺度不匹配导致次采样信息丢失，产生多个可能因果图的问题。

Method: 采用约束优化方法，即回答集编程（ASP）寻找最优答案集，还利用图论进一步修剪可能的解。

Result: 在模拟数据和脑结构连接实证数据上验证了方法优越性，作为元方法平均提高F1分数12%，在因果图重建的精确率和召回率上达到了最优，且对不同程度次采样表现出鲁棒性。

Conclusion: 所提出的方法比现有方法更优，能更准确、快速地得到因果图结构，且对次采样有较好的鲁棒性。

Abstract: Learning graphical causal structures from time series data presents
significant challenges, especially when the measurement frequency does not
match the causal timescale of the system. This often leads to a set of equally
possible underlying causal graphs due to information loss from sub-sampling
(i.e., not observing all possible states of the system throughout time). Our
research addresses this challenge by incorporating the effects of sub-sampling
in the derivation of causal graphs, resulting in more accurate and intuitive
outcomes. We use a constraint optimization approach, specifically answer set
programming (ASP), to find the optimal set of answers. ASP not only identifies
the most probable underlying graph, but also provides an equivalence class of
possible graphs for expert selection. In addition, using ASP allows us to
leverage graph theory to further prune the set of possible solutions, yielding
a smaller, more accurate answer set significantly faster than traditional
approaches. We validate our approach on both simulated data and empirical
structural brain connectivity, and demonstrate its superiority over established
methods in these experiments. We further show how our method can be used as a
meta-approach on top of established methods to obtain, on average, 12%
improvement in F1 score. In addition, we achieved state of the art results in
terms of precision and recall of reconstructing causal graph from sub-sampled
time series data. Finally, our method shows robustness to varying degrees of
sub-sampling on realistic simulations, whereas other methods perform worse for
higher rates of sub-sampling.

</details>


### [89] [On-the-Fly Adaptive Distillation of Transformer to Dual-State Linear Attention](https://arxiv.org/abs/2506.09316)
*Yeonju Ro,Zhenyu Zhang,Souvik Kundu,Zhangyang Wang,Aditya Akella*

Main category: cs.LG

TL;DR: 提出DSLA解决线性注意力架构短程偏差问题，引入SERVE框架平衡效率与准确性，评估显示推理速度显著提升且性能相当。


<details>
  <summary>Details</summary>
Motivation: 大语言模型处理长输入时计算和内存成本高，次二次方法降低成本但会降低准确性，存在短程偏差。

Method: 提出DSLA维护两个隐藏状态，引入SERVE在线自适应蒸馏框架，按敏感度排序替换Transformer层，采用链式微调策略。

Result: SERVE比Llama2 - 7B推理快2.3倍，比Zamba - 7B快3.0倍，下游任务性能相当；DSLA能捕获全局和局部依赖。

Conclusion: DSLA和SERVE有效平衡了大语言模型处理长输入时的效率和准确性。

Abstract: Large language models (LLMs) excel at capturing global token dependencies via
self-attention but face prohibitive compute and memory costs on lengthy inputs.
While sub-quadratic methods (e.g., linear attention) can reduce these costs,
they often degrade accuracy due to overemphasizing recent tokens. In this work,
we first propose \textit{dual-state linear attention} (\textbf{\dsla}), a novel
design that maintains two specialized hidden states-one for preserving
historical context and one for tracking recency-thereby mitigating the
short-range bias typical of linear-attention architectures. To further balance
efficiency and accuracy under dynamic workload conditions, we introduce
\textbf{\serve}, an online \textit{adaptive distillation} framework that
progressively replaces Transformer layers with DSLA layers at inference time,
guided by a sensitivity-based layer ordering. \serve\ uses a chained
fine-tuning strategy to ensure that each newly converted DSLA layer remains
consistent with previously replaced layers, preserving the overall quality.
Extensive evaluations on commonsense reasoning, long-context QA, and text
summarization demonstrate that \serve\ yields \textbf{2.3x} faster inference
than Llama2-7B and \textbf{3.0x} faster than the hybrid Zamba-7B, while
retaining comparable performance across downstream tasks. Our ablation studies
show that DSLA's dual states capture both global and local dependencies,
addressing the historical-token underrepresentation seen in prior linear
attentions. Codes are available at https://github.com/utnslab/DSLA-Serve.

</details>


### [90] [ErrorEraser: Unlearning Data Bias for Improved Continual Learning](https://arxiv.org/abs/2506.09347)
*Xuemei Cao,Hanlin Gu,Xin Yang,Bingjun Wei,Haoyang Liang,Xiangkun Wang,Tianrui Li*

Main category: cs.LG

TL;DR: 文章提出持续学习需有意遗忘，介绍了去除数据偏差导致错误记忆的插件ErrorEraser，实验证明其能缓解数据偏差负面影响。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法忽略现实数据偏差，导致模型学习到虚假关联，降低保留和迁移知识的能力。

Method: 提出插件ErrorEraser，包含错误识别和错误消除两个模块，还有增量特征分布学习策略。

Result: ErrorEraser显著缓解数据偏差负面影响，在三种持续学习方法中实现更高准确率和更低遗忘率。

Conclusion: ErrorEraser是一种有效的持续学习方法，可解决数据偏差带来的问题。

Abstract: Continual Learning (CL) primarily aims to retain knowledge to prevent
catastrophic forgetting and transfer knowledge to facilitate learning new
tasks. Unlike traditional methods, we propose a novel perspective: CL not only
needs to prevent forgetting, but also requires intentional forgetting.This
arises from existing CL methods ignoring biases in real-world data, leading the
model to learn spurious correlations that transfer and amplify across tasks.
From feature extraction and prediction results, we find that data biases
simultaneously reduce CL's ability to retain and transfer knowledge. To address
this, we propose ErrorEraser, a universal plugin that removes erroneous
memories caused by biases in CL, enhancing performance in both new and old
tasks. ErrorEraser consists of two modules: Error Identification and Error
Erasure. The former learns the probability density distribution of task data in
the feature space without prior knowledge, enabling accurate identification of
potentially biased samples. The latter ensures only erroneous knowledge is
erased by shifting the decision space of representative outlier samples.
Additionally, an incremental feature distribution learning strategy is designed
to reduce the resource overhead during error identification in downstream
tasks. Extensive experimental results show that ErrorEraser significantly
mitigates the negative impact of data biases, achieving higher accuracy and
lower forgetting rates across three types of CL methods. The code is available
at https://github.com/diadai/ErrorEraser.

</details>


### [91] [Anomaly Detection and Generation with Diffusion Models: A Survey](https://arxiv.org/abs/2506.09368)
*Yang Liu,Jing Liu,Chengfang Li,Rui Xi,Wenchao Li,Liang Cao,Jin Wang,Laurence T. Yang,Junsong Yuan,Wei Zhou*

Main category: cs.LG

TL;DR: 本文全面综述了基于扩散模型的异常检测与生成（ADGDM），分析理论基础与实践，强调二者协同关系，分类讨论方法优缺点，指出挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 深度学习中扩散模型在无监督异常检测领域有潜力，现有调查常分开处理异常检测和生成，本文旨在强调二者协同关系并推动创新应用。

Method: 对ADGDM进行全面综述，采用教程式分析，通过详细分类法对方法进行归类分析。

Result: 揭示异常检测和生成的协同关系，分类讨论了不同ADGDM方法的优缺点。

Conclusion: 指出了可扩展性和计算效率等关键挑战，给出高效架构、条件策略等未来方向，为研究者和从业者提供指导。

Abstract: Anomaly detection (AD) plays a pivotal role across diverse domains, including
cybersecurity, finance, healthcare, and industrial manufacturing, by
identifying unexpected patterns that deviate from established norms in
real-world data. Recent advancements in deep learning, specifically diffusion
models (DMs), have sparked significant interest due to their ability to learn
complex data distributions and generate high-fidelity samples, offering a
robust framework for unsupervised AD. In this survey, we comprehensively review
anomaly detection and generation with diffusion models (ADGDM), presenting a
tutorial-style analysis of the theoretical foundations and practical
implementations and spanning images, videos, time series, tabular, and
multimodal data. Crucially, unlike existing surveys that often treat anomaly
detection and generation as separate problems, we highlight their inherent
synergistic relationship. We reveal how DMs enable a reinforcing cycle where
generation techniques directly address the fundamental challenge of anomaly
data scarcity, while detection methods provide critical feedback to improve
generation fidelity and relevance, advancing both capabilities beyond their
individual potential. A detailed taxonomy categorizes ADGDM methods based on
anomaly scoring mechanisms, conditioning strategies, and architectural designs,
analyzing their strengths and limitations. We final discuss key challenges
including scalability and computational efficiency, and outline promising
future directions such as efficient architectures, conditioning strategies, and
integration with foundation models (e.g., visual-language models and large
language models). By synthesizing recent advances and outlining open research
questions, this survey aims to guide researchers and practitioners in
leveraging DMs for innovative AD solutions across diverse applications.

</details>


### [92] [LPO: Towards Accurate GUI Agent Interaction via Location Preference Optimization](https://arxiv.org/abs/2506.09373)
*Jiaqi Tang,Yu Xia,Yi-Feng Wu,Yuwei Hu,Yuhui Chen,Qing-Guo Chen,Xiaogang Xu,Xiangyu Wu,Hao Lu,Yanqing Ma,Shiyin Lu,Qifeng Chen*

Main category: cs.LG

TL;DR: 现有GUI代理的监督微调方法定位能力有限，本文提出LPO方法优化交互偏好，实验表明其性能优越，代码将公开。


<details>
  <summary>Details</summary>
Motivation: 当前用于GUI代理实现空间定位的监督微调方法难以准确感知位置数据，现有策略难以有效评估位置准确性，限制了其效用。

Method: 引入Location Preference Optimization (LPO)方法，利用信息熵预测交互位置，引入基于物理距离的动态位置奖励函数，借助Group Relative Preference Optimization (GRPO)进行环境探索。

Result: 综合实验表明LPO表现优越，在离线基准测试和真实在线评估中都取得了SOTA结果。

Conclusion: LPO方法能有效优化GUI交互偏好，显著提高交互精度。

Abstract: The advent of autonomous agents is transforming interactions with Graphical
User Interfaces (GUIs) by employing natural language as a powerful
intermediary. Despite the predominance of Supervised Fine-Tuning (SFT) methods
in current GUI agents for achieving spatial localization, these methods face
substantial challenges due to their limited capacity to accurately perceive
positional data. Existing strategies, such as reinforcement learning, often
fail to assess positional accuracy effectively, thereby restricting their
utility. In response, we introduce Location Preference Optimization (LPO), a
novel approach that leverages locational data to optimize interaction
preferences. LPO uses information entropy to predict interaction positions by
focusing on zones rich in information. Besides, it further introduces a dynamic
location reward function based on physical distance, reflecting the varying
importance of interaction positions. Supported by Group Relative Preference
Optimization (GRPO), LPO facilitates an extensive exploration of GUI
environments and significantly enhances interaction precision. Comprehensive
experiments demonstrate LPO's superior performance, achieving SOTA results
across both offline benchmarks and real-world online evaluations. Our code will
be made publicly available soon, at https://github.com/AIDC-AI/LPO.

</details>


### [93] [Revisiting Diffusion Models: From Generative Pre-training to One-Step Generation](https://arxiv.org/abs/2506.09376)
*Bowen Zheng,Tianming Yang*

Main category: cs.LG

TL;DR: 本文指出扩散蒸馏问题，表明独立GAN目标可将扩散模型转为一步生成器，提出扩散训练可视为生成式预训练，通过微调预训练模型创建一步生成模型并给出频域分析，提供新视角。


<details>
  <summary>Details</summary>
Motivation: 解决扩散蒸馏需大量训练且学生模型性能易下降的问题，解释引入GAN目标的潜在机制。

Method: 识别蒸馏关键局限，证明独立GAN目标可克服局限，提出扩散训练为生成式预训练观点，通过微调预训练模型创建一步生成模型并进行频域分析。

Result: 以85%参数冻结微调预训练模型创建一步生成模型，用0.2M图像获强性能，5M图像获近SOTA结果。

Conclusion: 为扩散训练提供新视角，强调其作为强大生成式预训练过程的作用，可用于构建高效一步生成模型。

Abstract: Diffusion distillation is a widely used technique to reduce the sampling cost
of diffusion models, yet it often requires extensive training, and the student
performance tends to be degraded. Recent studies show that incorporating a GAN
objective may alleviate these issues, yet the underlying mechanism remains
unclear. In this work, we first identify a key limitation of distillation:
mismatched step sizes and parameter numbers between the teacher and the student
model lead them to converge to different local minima, rendering direct
imitation suboptimal. We further demonstrate that a standalone GAN objective,
without relying a distillation loss, overcomes this limitation and is
sufficient to convert diffusion models into efficient one-step generators.
Based on this finding, we propose that diffusion training may be viewed as a
form of generative pre-training, equipping models with capabilities that can be
unlocked through lightweight GAN fine-tuning. Supporting this view, we create a
one-step generation model by fine-tuning a pre-trained model with 85% of
parameters frozen, achieving strong performance with only 0.2M images and
near-SOTA results with 5M images. We further present a frequency-domain
analysis that may explain the one-step generative capability gained in
diffusion training. Overall, our work provides a new perspective for diffusion
training, highlighting its role as a powerful generative pre-training process,
which can be the basis for building efficient one-step generation models.

</details>


### [94] [Efficient Prediction of SO(3)-Equivariant Hamiltonian Matrices via SO(2) Local Frames](https://arxiv.org/abs/2506.09398)
*Haiyang Yu,Yuchao Lin,Xuan Zhang,Xiaofeng Qian,Shuiwang Ji*

Main category: cs.LG

TL;DR: 提出QHNetV2网络预测哈密顿矩阵加速电子结构计算，在QH9和MD17数据集表现出色，代码将随AIRS库发布。


<details>
  <summary>Details</summary>
Motivation: 利用哈密顿矩阵非对角块与SO(2)局部框架的内在关系，避免昂贵的SO(3)克莱布希 - 高登张量积，以加速电子结构计算。

Method: 引入新的高效SO(2)等变操作，在SO(2)局部框架内进行非对角特征更新和消息传递，在每个节点的SO(2)局部框架内进行连续SO(2)张量积融合节点特征。

Result: 在QH9和MD17数据集的大量实验中，模型在多种分子结构和轨迹上表现优越，泛化能力强。

Conclusion: SO(2)局部框架上的SO(2)操作是可扩展和对称感知的电子结构学习的有前景方向。

Abstract: We consider the task of predicting Hamiltonian matrices to accelerate
electronic structure calculations, which plays an important role in physics,
chemistry, and materials science. Motivated by the inherent relationship
between the off-diagonal blocks of the Hamiltonian matrix and the SO(2) local
frame, we propose a novel and efficient network, called QHNetV2, that achieves
global SO(3) equivariance without the costly SO(3) Clebsch-Gordan tensor
products. This is achieved by introducing a set of new efficient and powerful
SO(2)-equivariant operations and performing all off-diagonal feature updates
and message passing within SO(2) local frames, thereby eliminating the need of
SO(3) tensor products. Moreover, a continuous SO(2) tensor product is performed
within the SO(2) local frame at each node to fuse node features, mimicking the
symmetric contraction operation. Extensive experiments on the large QH9 and
MD17 datasets demonstrate that our model achieves superior performance across a
wide range of molecular structures and trajectories, highlighting its strong
generalization capability. The proposed SO(2) operations on SO(2) local frames
offer a promising direction for scalable and symmetry-aware learning of
electronic structures. Our code will be released as part of the AIRS library
https://github.com/divelab/AIRS.

</details>


### [95] [Mitigating Spurious Correlations in LLMs via Causality-Aware Post-Training](https://arxiv.org/abs/2506.09433)
*Shurui Gui,Shuiwang Ji*

Main category: cs.LG

TL;DR: 提出因果感知后训练（CAPT）方法减轻大语言模型预训练中的虚假关联，提升泛化能力，实验证明其有效性和样本效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在预训练中获得虚假关联，导致在分布外样本上表现不佳，需减轻虚假关联。

Method: 通过因果感知后训练（CAPT），将有偏预测分解为事件估计和事件干预两个无偏步骤，减少预训练偏差且不引入额外微调偏差。

Result: 在CLadder和PrOntoQA数据集上，用CAPT微调的3B规模语言模型，仅用100个分布内微调样本，在分布内和分布外任务上优于传统SFT和更大的大语言模型。

Conclusion: CAPT方法有效且具有样本效率，能提升大语言模型的泛化能力。

Abstract: While large language models (LLMs) have demonstrated remarkable capabilities
in language modeling, recent studies reveal that they often fail on
out-of-distribution (OOD) samples due to spurious correlations acquired during
pre-training. Here, we aim to mitigate such spurious correlations through
causality-aware post-training (CAPT). By decomposing a biased prediction into
two unbiased steps, known as \textit{event estimation} and \textit{event
intervention}, we reduce LLMs' pre-training biases without incurring additional
fine-tuning biases, thus enhancing the model's generalization ability.
Experiments on the formal causal inference benchmark CLadder and the logical
reasoning dataset PrOntoQA show that 3B-scale language models fine-tuned with
CAPT can outperform both traditional SFT and larger LLMs on in-distribution
(ID) and OOD tasks using only 100 ID fine-tuning samples, demonstrating the
effectiveness and sample efficiency of CAPT.

</details>


### [96] [Learning Obfuscations Of LLM Embedding Sequences: Stained Glass Transform](https://arxiv.org/abs/2506.09452)
*Jay Roberts,Kyle Mylonakis,Sidhartha Roy,Kaan Kale*

Main category: cs.LG

TL;DR: 本文介绍了Stained Glass Transform，一种可保护大语言模型输入隐私同时保留模型效用的转换方法，并进行理论分析和实验验证。


<details>
  <summary>Details</summary>
Motivation: AI计算基础设施成本高、大语言模型服务难，数据以明文形式在共享或多租户计算基础设施上运行，使数据所有者对隐私担忧，从而限制数据使用。

Method: 引入Stained Glass Transform对大语言模型的词嵌入进行学习、随机且依赖序列的转换，理论上连接特定类别的转换与高斯混合模型互信息理论，通过互信息计算后验隐私估计，用标记级隐私指标和标准大语言模型性能基准验证。

Result: 通过相关理论分析和实验验证了转换后嵌入实例的隐私性和效用。

Conclusion: Stained Glass Transform能在保留大语言模型效用的同时，为其输入提供隐私保护。

Abstract: The high cost of ownership of AI compute infrastructure and challenges of
robust serving of large language models (LLMs) has led to a surge in managed
Model-as-a-service deployments. Even when enterprises choose on-premises
deployments, the compute infrastructure is typically shared across many teams
in order to maximize the return on investment. In both scenarios the deployed
models operate only on plaintext data, and so enterprise data owners must allow
their data to appear in plaintext on a shared or multi-tenant compute
infrastructure. This results in data owners with private or sensitive data
being hesitant or restricted in what data they use with these types of
deployments. In this work we introduce the Stained Glass Transform, a learned,
stochastic, and sequence dependent transformation of the word embeddings of an
LLM which information theoretically provides privacy to the input of the LLM
while preserving the utility of model. We theoretically connect a particular
class of Stained Glass Transforms to the theory of mutual information of
Gaussian Mixture Models. We then calculate a-postiori privacy estimates, based
on mutual information, and verify the privacy and utility of instances of
transformed embeddings through token level metrics of privacy and standard LLM
performance benchmarks.

</details>


### [97] [NDCG-Consistent Softmax Approximation with Accelerated Convergence](https://arxiv.org/abs/2506.09454)
*Yuanhao Pu,Defu Lian,Xiaolong Chen,Xu Huang,Jin Chen,Enhong Chen*

Main category: cs.LG

TL;DR: 提出基于Softmax Loss泰勒展开的RG²和RGˣ损失函数，与ALS优化方法结合，在真实数据集上取得良好排序性能且加速收敛。


<details>
  <summary>Details</summary>
Motivation: Softmax Loss在大规模对象空间应用时存在计算开销大、可扩展性有限的问题。

Method: 通过Softmax Loss的泰勒展开得到RG²和RGˣ损失函数，将其与ALS优化方法结合，并进行泛化保证和收敛率分析。

Result: 在真实数据集上，该方法排序性能与Softmax Loss相当或更优，且显著加速收敛。

Conclusion: 此框架为相似度学习社区提供理论见解和实用高效工具，适用于需平衡排序质量和计算效率的广泛任务。

Abstract: Ranking tasks constitute fundamental components of extreme similarity
learning frameworks, where extremely large corpora of objects are modeled
through relative similarity relationships adhering to predefined ordinal
structures. Among various ranking surrogates, Softmax (SM) Loss has been widely
adopted due to its natural capability to handle listwise ranking via global
negative comparisons, along with its flexibility across diverse application
scenarios. However, despite its effectiveness, SM Loss often suffers from
significant computational overhead and scalability limitations when applied to
large-scale object spaces. To address this challenge, we propose novel loss
formulations that align directly with ranking metrics: the
Ranking-Generalizable \textbf{squared} (RG$^2$) Loss and the
Ranking-Generalizable interactive (RG$^\times$) Loss, both derived through
Taylor expansions of the SM Loss. Notably, RG$^2$ reveals the intrinsic
mechanisms underlying weighted squared losses (WSL) in ranking methods and
uncovers fundamental connections between sampling-based and non-sampling-based
loss paradigms. Furthermore, we integrate the proposed RG losses with the
highly efficient Alternating Least Squares (ALS) optimization method, providing
both generalization guarantees and convergence rate analyses. Empirical
evaluations on real-world datasets demonstrate that our approach achieves
comparable or superior ranking performance relative to SM Loss, while
significantly accelerating convergence. This framework offers the similarity
learning community both theoretical insights and practically efficient tools,
with methodologies applicable to a broad range of tasks where balancing ranking
quality and computational efficiency is essential.

</details>


### [98] [On a few pitfalls in KL divergence gradient estimation for RL](https://arxiv.org/abs/2506.09477)
*Yunhao Tang,Rémi Munos*

Main category: cs.LG

TL;DR: 指出在大语言模型强化学习训练中实现KL散度梯度估计的几个陷阱，并给出正确实现方式。


<details>
  <summary>Details</summary>
Motivation: 发现一些开源项目和论文在大语言模型强化学习训练中实现KL散度梯度估计存在问题，需指出并解决。

Method: 分析错误实现方式，通过表格和大语言模型实验展示问题影响。

Result: 发现对KL估计求导作为损失函数一般不正确，部分实现未考虑估计问题的序列性质只能得到部分梯度。

Conclusion: 给出了正确实现KL梯度的方法。

Abstract: We point out a few pitfalls in implementing gradient estimation for KL
divergence in RL training for LLM, as seen in a number of open source projects
and papers. The first major pitfall is to differentiate through the KL estimate
as loss functions to minimize KL divergence. We show that such implementations
are generally incorrect and do not produce the desired KL gradient. Secondly,
we show that some implementations do not account for the sequential nature of
the estimation problem and produce a partial gradient at best. We demonstrate
the impact of such issues with illustrative tabular and LLM experiments, and
show the correct way to implement the KL gradient.

</details>


### [99] [EnerBridge-DPO: Energy-Guided Protein Inverse Folding with Markov Bridges and Direct Preference Optimization](https://arxiv.org/abs/2506.09496)
*Dingyi Rong,Haotian Lu,Wenzhuo Zheng,Fan Zhang,Shuangjia Zheng,Ning Liu*

Main category: cs.LG

TL;DR: 提出EnerBridge - DPO逆折叠框架生成低能量、高稳定性蛋白质序列，评估显示其能设计低能量序列且保持高序列恢复率，还能准确预测ΔΔG值。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习方法在蛋白质逆折叠中主要通过最大化序列恢复率训练，常忽略生成序列的能量，本研究旨在克服这一局限，开发直接生成低能量、稳定蛋白质序列的模型。

Method: 提出EnerBridge - DPO框架，将马尔可夫桥与直接偏好优化（DPO）集成，用基于能量的偏好微调马尔可夫桥模型；引入显式能量约束损失，增强DPO基于先验序列的能量驱动特性。

Result: EnerBridge - DPO能设计出能量更低的蛋白质复合物序列，保持与现有先进模型相当的序列恢复率，准确预测不同序列间的ΔΔG值。

Conclusion: EnerBridge - DPO在蛋白质逆折叠中能有效生成低能量、高稳定性的蛋白质序列。

Abstract: Designing protein sequences with optimal energetic stability is a key
challenge in protein inverse folding, as current deep learning methods are
primarily trained by maximizing sequence recovery rates, often neglecting the
energy of the generated sequences. This work aims to overcome this limitation
by developing a model that directly generates low-energy, stable protein
sequences. We propose EnerBridge-DPO, a novel inverse folding framework focused
on generating low-energy, high-stability protein sequences. Our core innovation
lies in: First, integrating Markov Bridges with Direct Preference Optimization
(DPO), where energy-based preferences are used to fine-tune the Markov Bridge
model. The Markov Bridge initiates optimization from an information-rich prior
sequence, providing DPO with a pool of structurally plausible sequence
candidates. Second, an explicit energy constraint loss is introduced, which
enhances the energy-driven nature of DPO based on prior sequences, enabling the
model to effectively learn energy representations from a wealth of prior
knowledge and directly predict sequence energy values, thereby capturing
quantitative features of the energy landscape. Our evaluations demonstrate that
EnerBridge-DPO can design protein complex sequences with lower energy while
maintaining sequence recovery rates comparable to state-of-the-art models, and
accurately predicts $\Delta \Delta G$ values between various sequences.

</details>


### [100] [A Unified Theory of Compositionality, Modularity, and Interpretability in Markov Decision Processes](https://arxiv.org/abs/2506.09499)
*Thomas J. Ringstrom,Paul R. Schrater*

Main category: cs.LG

TL;DR: 提出Option Kernel Bellman Equations (OKBEs)用于无奖励马尔可夫决策过程，通过构建和优化STOK解决规划问题，支持可验证的长期规划和内在动机。


<details>
  <summary>Details</summary>
Motivation: 解决无奖励马尔可夫决策过程中的规划问题，实现可组合、模块化和可解释的策略，并解决奖励最大化与这些特性的冲突。

Method: 引入OKBEs，直接构建和优化STOK，利用Chapman - Kolmogorov方程进行组合，将高维问题分解为局部STOK和目标条件策略。

Result: 得到高度灵活的智能体，可快速合成元策略，跨任务重用规划表示，利用内在动机函数证明目标合理性。

Conclusion: OKBEs有助于实现可组合、模块化和可解释性，支持可验证的长期规划和内在动机，适用于动态高维世界模型。

Abstract: We introduce Option Kernel Bellman Equations (OKBEs) for a new reward-free
Markov Decision Process. Rather than a value function, OKBEs directly construct
and optimize a predictive map called a state-time option kernel (STOK) to
maximize the probability of completing a goal while avoiding constraint
violations. STOKs are compositional, modular, and interpretable
initiation-to-termination transition kernels for policies in the Options
Framework of Reinforcement Learning. This means: 1) STOKs can be composed using
Chapman-Kolmogorov equations to make spatiotemporal predictions for multiple
policies over long horizons, 2) high-dimensional STOKs can be represented and
computed efficiently in a factorized and reconfigurable form, and 3) STOKs
record the probabilities of semantically interpretable goal-success and
constraint-violation events, needed for formal verification. Given a
high-dimensional state-transition model for an intractable planning problem, we
can decompose it with local STOKs and goal-conditioned policies that are
aggregated into a factorized goal kernel, making it possible to forward-plan at
the level of goals in high-dimensions to solve the problem. These properties
lead to highly flexible agents that can rapidly synthesize meta-policies, reuse
planning representations across many tasks, and justify goals using
empowerment, an intrinsic motivation function. We argue that
reward-maximization is in conflict with the properties of compositionality,
modularity, and interpretability. Alternatively, OKBEs facilitate these
properties to support verifiable long-horizon planning and intrinsic motivation
that scales to dynamic high-dimensional world-models.

</details>


### [101] [Neural Functions for Learning Periodic Signal](https://arxiv.org/abs/2506.09526)
*Woojin Cho,Minju Jo,Kookjin Lee,Noseong Park*

Main category: cs.LG

TL;DR: 本文针对坐标基MLP泛化性差问题，提出新网络架构，在多实验中展现出良好效果。


<details>
  <summary>Details</summary>
Motivation: 坐标基MLP学习信号时存在过拟合和泛化性差的问题，尤其在信号有周期性时，需解决其外推性能不佳问题。

Method: 提出一种新的网络架构，从测量数据中提取周期模式，利用这些信息表示信号。

Result: 通过微分方程周期解学习、真实数据集时间序列插补和预测等实验，证明了方法的有效性。

Conclusion: 新网络架构能有效提取信号周期模式，增强泛化能力，提升外推性能。

Abstract: As function approximators, deep neural networks have served as an effective
tool to represent various signal types. Recent approaches utilize multi-layer
perceptrons (MLPs) to learn a nonlinear mapping from a coordinate to its
corresponding signal, facilitating the learning of continuous neural
representations from discrete data points. Despite notable successes in
learning diverse signal types, coordinate-based MLPs often face issues of
overfitting and limited generalizability beyond the training region, resulting
in subpar extrapolation performance. This study addresses scenarios where the
underlying true signals exhibit periodic properties, either spatially or
temporally. We propose a novel network architecture, which extracts periodic
patterns from measurements and leverages this information to represent the
signal, thereby enhancing generalization and improving extrapolation
performance. We demonstrate the efficacy of the proposed method through
comprehensive experiments, including the learning of the periodic solutions for
differential equations, and time series imputation (interpolation) and
forecasting (extrapolation) on real-world datasets.

</details>


### [102] [Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models](https://arxiv.org/abs/2506.09532)
*Shuai Wang,Zhenhua Liu,Jiaheng Wei,Xuanwu Yin,Dong Li,Emad Barsoum*

Main category: cs.LG

TL;DR: 提出多模态过程奖励模型Athena - PRM，用弱强完成器预测一致性生成高质量标签，还提出两个提升策略，在多场景和基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 开发高性能过程奖励模型需大量时间和资金，传统自动标注方法有噪音且计算成本高。

Method: 利用弱强完成器预测一致性识别可靠过程标签，提出ORM初始化和负数据上采样两个策略。

Result: Athena - PRM在多个基准测试和场景中表现优越，在多个数据集上提升性能，在VisualProcessBench创最优结果。

Conclusion: Athena - PRM能有效评估推理步骤奖励分数，所提方法有效提升了过程奖励模型性能。

Abstract: We present Athena-PRM, a multimodal process reward model (PRM) designed to
evaluate the reward score for each step in solving complex reasoning problems.
Developing high-performance PRMs typically demands significant time and
financial investment, primarily due to the necessity for step-level annotations
of reasoning steps. Conventional automated labeling methods, such as Monte
Carlo estimation, often produce noisy labels and incur substantial
computational costs. To efficiently generate high-quality process-labeled data,
we propose leveraging prediction consistency between weak and strong completers
as a criterion for identifying reliable process labels. Remarkably, Athena-PRM
demonstrates outstanding effectiveness across various scenarios and benchmarks
with just 5,000 samples. Furthermore, we also develop two effective strategies
to improve the performance of PRMs: ORM initialization and up-sampling for
negative data. We validate our approach in three specific scenarios:
verification for test time scaling, direct evaluation of reasoning step
correctness, and reward ranked fine-tuning. Our Athena-PRM consistently
achieves superior performance across multiple benchmarks and scenarios.
Notably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances
performance by 10.2 points on WeMath and 7.1 points on MathVista for test time
scaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in
VisualProcessBench and outperforms the previous SoTA by 3.9 F1-score,
showcasing its robust capability to accurately assess the correctness of the
reasoning step. Additionally, utilizing Athena-PRM as the reward model, we
develop Athena-7B with reward ranked fine-tuning and outperforms baseline with
a significant margin on five benchmarks.

</details>


### [103] [STOAT: Spatial-Temporal Probabilistic Causal Inference Network](https://arxiv.org/abs/2506.09544)
*Yang Yang,Du Yin,Hao Xue,Flora Salim*

Main category: cs.LG

TL;DR: 提出STOAT框架用于时空因果时间序列概率预测，在COVID - 19数据实验中表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法独立建模时空动态，忽略因果驱动的概率预测，导致预测能力受限。

Method: 提出STOAT框架，结合因果推理方法与空间关系矩阵估计因果效应，用深度概率模型处理潜在序列，探索多种输出分布。

Result: 在六个国家的COVID - 19数据实验中，STOAT在关键指标上优于现有概率预测模型，尤其在空间依赖强的区域。

Conclusion: STOAT桥接因果推理和地理空间概率预测，为复杂时空任务提供通用框架。

Abstract: Spatial-temporal causal time series (STC-TS) involve region-specific temporal
observations driven by causally relevant covariates and interconnected across
geographic or network-based spaces. Existing methods often model spatial and
temporal dynamics independently and overlook causality-driven probabilistic
forecasting, limiting their predictive power. To address this, we propose STOAT
(Spatial-Temporal Probabilistic Causal Inference Network), a novel framework
for probabilistic forecasting in STC-TS. The proposed method extends a causal
inference approach by incorporating a spatial relation matrix that encodes
interregional dependencies (e.g. proximity or connectivity), enabling spatially
informed causal effect estimation. The resulting latent series are processed by
deep probabilistic models to estimate the parameters of the distributions,
enabling calibrated uncertainty modeling. We further explore multiple output
distributions (e.g., Gaussian, Student's-$t$, Laplace) to capture
region-specific variability. Experiments on COVID-19 data across six countries
demonstrate that STOAT outperforms state-of-the-art probabilistic forecasting
models (DeepAR, DeepVAR, Deep State Space Model, etc.) in key metrics,
particularly in regions with strong spatial dependencies. By bridging causal
inference and geospatial probabilistic forecasting, STOAT offers a
generalizable framework for complex spatial-temporal tasks, such as epidemic
management.

</details>


### [104] [MOORL: A Framework for Integrating Offline-Online Reinforcement Learning](https://arxiv.org/abs/2506.09574)
*Gaurav Chaudhary,Wassim Uddin Mondal,Laxmidhar Behera*

Main category: cs.LG

TL;DR: 提出Meta Offline-Online Reinforcement Learning (MOORL) 框架，结合离线和在线RL，解决样本效率和探索问题，经实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 解决深度强化学习中样本效率和探索挑战，克服离线RL受分布外动作等限制的问题。

Method: 提出MOORL混合框架，引入元策略在离线和在线轨迹间无缝适应。

Result: 理论分析表明混合方法增强探索，学习稳定Q函数，28个任务实验显示比现有离线和混合RL基线有一致改进。

Conclusion: MOORL计算开销小、性能强，有实际应用潜力。

Abstract: Sample efficiency and exploration remain critical challenges in Deep
Reinforcement Learning (DRL), particularly in complex domains. Offline RL,
which enables agents to learn optimal policies from static, pre-collected
datasets, has emerged as a promising alternative. However, offline RL is
constrained by issues such as out-of-distribution (OOD) actions that limit
policy performance and generalization. To overcome these limitations, we
propose Meta Offline-Online Reinforcement Learning (MOORL), a hybrid framework
that unifies offline and online RL for efficient and scalable learning. While
previous hybrid methods rely on extensive design components and added
computational complexity to utilize offline data effectively, MOORL introduces
a meta-policy that seamlessly adapts across offline and online trajectories.
This enables the agent to leverage offline data for robust initialization while
utilizing online interactions to drive efficient exploration. Our theoretical
analysis demonstrates that the hybrid approach enhances exploration by
effectively combining the complementary strengths of offline and online data.
Furthermore, we demonstrate that MOORL learns a stable Q-function without added
complexity. Extensive experiments on 28 tasks from the D4RL and V-D4RL
benchmarks validate its effectiveness, showing consistent improvements over
state-of-the-art offline and hybrid RL baselines. With minimal computational
overhead, MOORL achieves strong performance, underscoring its potential for
practical applications in real-world scenarios.

</details>


### [105] [Beyond Overconfidence: Foundation Models Redefine Calibration in Deep Neural Networks](https://arxiv.org/abs/2506.09593)
*Achim Hekler,Lukas Kuhn,Florian Buettner*

Main category: cs.LG

TL;DR: 本文对基础模型的校准行为进行全面研究，发现其在分布内预测时倾向于信心不足，在分布偏移时校准有所改善，后验校准技术在分布内有效，在严重分布偏移时效果不佳。


<details>
  <summary>Details</summary>
Motivation: 可靠的不确定性校准对在高风险应用中安全部署深度神经网络至关重要，但基础模型的校准特性研究不足。

Method: 对基础模型的校准行为进行实证分析。

Result: 基础模型在分布内预测时信心不足、校准误差高，分布偏移时校准改善；后验校准技术在分布内有效，严重分布偏移时可靠性降低甚至有反作用。

Conclusion: 架构和训练创新对校准有复杂、非单调的影响，挑战了持续改进的既有观点。

Abstract: Reliable uncertainty calibration is essential for safely deploying deep
neural networks in high-stakes applications. Deep neural networks are known to
exhibit systematic overconfidence, especially under distribution shifts.
Although foundation models such as ConvNeXt, EVA and BEiT have demonstrated
significant improvements in predictive performance, their calibration
properties remain underexplored. This paper presents a comprehensive
investigation into the calibration behavior of foundation models, revealing
insights that challenge established paradigms. Our empirical analysis shows
that these models tend to be underconfident in in-distribution predictions,
resulting in higher calibration errors, while demonstrating improved
calibration under distribution shifts. Furthermore, we demonstrate that
foundation models are highly responsive to post-hoc calibration techniques in
the in-distribution setting, enabling practitioners to effectively mitigate
underconfidence bias. However, these methods become progressively less reliable
under severe distribution shifts and can occasionally produce counterproductive
results. Our findings highlight the complex, non-monotonic effects of
architectural and training innovations on calibration, challenging established
narratives of continuous improvement.

</details>


### [106] [Accelerating Large-Scale Regularized High-Order Tensor Recovery](https://arxiv.org/abs/2506.09594)
*Wenjin Qin,Hailin Wang,Jingyao Hou,Jianjun Wang*

Main category: cs.LG

TL;DR: 本文针对现有张量恢复方法问题，设计随机算法进行低秩张量逼近，构建广义非凸建模框架，研究统一非凸模型与优化算法，实验证明方法实用有效。


<details>
  <summary>Details</summary>
Motivation: 现有张量恢复方法未考虑张量尺度变化影响，处理大规模高阶张量数据计算成本高。

Method: 借助Krylov子空间迭代、块Lanczos双对角化过程和随机投影策略设计随机算法，构建广义非凸建模框架，将随机低秩张量逼近方案集成到计算中。

Result: 在各种大规模张量上的实验表明，该方法比一些先进方法更实用、有效和优越。

Conclusion: 提出的方法能有效解决现有张量恢复方法的问题，适用于大规模张量数据。

Abstract: Currently, existing tensor recovery methods fail to recognize the impact of
tensor scale variations on their structural characteristics. Furthermore,
existing studies face prohibitive computational costs when dealing with
large-scale high-order tensor data. To alleviate these issue, assisted by the
Krylov subspace iteration, block Lanczos bidiagonalization process, and random
projection strategies, this article first devises two fast and accurate
randomized algorithms for low-rank tensor approximation (LRTA) problem.
Theoretical bounds on the accuracy of the approximation error estimate are
established. Next, we develop a novel generalized nonconvex modeling framework
tailored to large-scale tensor recovery, in which a new regularization paradigm
is exploited to achieve insightful prior representation for large-scale
tensors. On the basis of the above, we further investigate new unified
nonconvex models and efficient optimization algorithms, respectively, for
several typical high-order tensor recovery tasks in unquantized and quantized
situations. To render the proposed algorithms practical and efficient for
large-scale tensor data, the proposed randomized LRTA schemes are integrated
into their central and time-intensive computations. Finally, we conduct
extensive experiments on various large-scale tensors, whose results demonstrate
the practicability, effectiveness and superiority of the proposed method in
comparison with some state-of-the-art approaches.

</details>


### [107] [SparseSSM: Efficient Selective Structured State Space Models Can Be Pruned in One-Shot](https://arxiv.org/abs/2506.09613)
*Kaiwen Tuo,Huan Wang*

Main category: cs.LG

TL;DR: 提出首个无训练剪枝框架SparseSSM用于状态空间架构，剪枝50% SSM权重无精度损失，达当前基于Mamba大模型剪枝最优。


<details>
  <summary>Details</summary>
Motivation: 现有一次性剪枝方法适用于注意力模块，未考虑选择性状态空间模块核心的时间共享和离散化状态转移矩阵，且状态空间语言模型参数多阻碍部署。

Method: 将经典最优脑外科医生框架扩展到状态空间架构，采用逐层算法，推导近似二阶显著性分数，结合组件敏感性分析指导前馈网络剪枝，可扩展到半结构化和结构化稀疏性。

Result: 不进行微调剪枝50% SSM权重，零样本精度无损失。

Conclusion: SparseSSM是基于Mamba大模型当前最优的剪枝算法。

Abstract: State-space language models such as Mamba match Transformer quality while
permitting linear complexity inference, yet still comprise billions of
parameters that hinder deployment. Existing one-shot pruning methods are
tailored to attention blocks and fail to account for the time-shared and
discretized state-transition matrix at the heart of the selective state-space
module (SSM). In this paper, we introduce SparseSSM, the first training-free
pruning framework that extends the classic optimal brain surgeon (OBS)
framework to state space architectures. Our layer-wise algorithm (i) derives an
approximate second-order saliency score that aggregates Hessian-trace
information across time steps, (ii) incorporates a component sensitivity
analysis to guide feed-forward network (FFN) pruning, which also sheds light on
where redundancy resides in mamba architecture, (iii) can be easily extended to
semi-structured and structured sparsity. Empirically, we prune 50% of SSM
weights without fine-tuning and observe no zero-shot accuracy loss, achieving
the current state-of-the-art pruning algorithm for Mamba-based LLMs.

</details>


### [108] [GLGENN: A Novel Parameter-Light Equivariant Neural Networks Architecture Based on Clifford Geometric Algebras](https://arxiv.org/abs/2506.09625)
*Ekaterina Filimoshina,Dmitry Shirokov*

Main category: cs.LG

TL;DR: 提出广义Lipschitz群等变神经网络（GLGENN），与竞品对比，该网络参数少、不易过拟合，在多项基准任务表现佳。


<details>
  <summary>Details</summary>
Motivation: 构建对向量空间所有伪正交变换等变的神经网络架构。

Method: 提出基于几何代数的GLGENN架构，采用考虑几何代数结构和运算的权重共享参数化技术。

Result: GLGENN在多个基准等变任务中表现优于或与竞品相当，且使用的可优化参数显著减少。

Conclusion: GLGENN架构参数少、不易过拟合，在等变任务中有良好表现。

Abstract: We propose, implement, and compare with competitors a new architecture of
equivariant neural networks based on geometric (Clifford) algebras: Generalized
Lipschitz Group Equivariant Neural Networks (GLGENN). These networks are
equivariant to all pseudo-orthogonal transformations, including rotations and
reflections, of a vector space with any non-degenerate or degenerate symmetric
bilinear form. We propose a weight-sharing parametrization technique that takes
into account the fundamental structures and operations of geometric algebras.
Due to this technique, GLGENN architecture is parameter-light and has less
tendency to overfitting than baseline equivariant models. GLGENN outperforms or
matches competitors on several benchmarking equivariant tasks, including
estimation of an equivariant function and a convex hull experiment, while using
significantly fewer optimizable parameters.

</details>


### [109] [In-Context Bias Propagation in LLM-Based Tabular Data Generation](https://arxiv.org/abs/2506.09630)
*Pol G. Recasens,Alberto Gutierrez,Jordi Torres,Josep. Ll Berral,Anisa Halimi,Kieran Fraser*

Main category: cs.LG

TL;DR: 研究大语言模型上下文学习生成合成表格数据时，上下文示例的统计偏差如何传播及恶意注入偏差的影响，揭示LLM数据生成管道新漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有工作假设上下文示例无偏且具代表性，但现实数据常含噪声和人口统计偏差，需研究偏差影响。

Method: 系统研究上下文示例统计偏差在合成表格数据分布中的传播情况，引入恶意注入偏差的对抗场景。

Result: 即使轻微上下文偏差也会导致全局统计失真，恶意贡献者可通过部分上下文示例注入偏差，损害下游分类器公平性。

Conclusion: 基于LLM且依赖上下文提示的数据生成管道在敏感领域存在新的漏洞。

Abstract: Large Language Models (LLMs) are increasingly used for synthetic tabular data
generation through in-context learning (ICL), offering a practical solution for
data augmentation in data scarce scenarios. While prior work has shown the
potential of LLMs to improve downstream task performance through augmenting
underrepresented groups, these benefits often assume access to a subset of
unbiased in-context examples, representative of the real dataset. In real-world
settings, however, data is frequently noisy and demographically skewed. In this
paper, we systematically study how statistical biases within in-context
examples propagate to the distribution of synthetic tabular data, showing that
even mild in-context biases lead to global statistical distortions. We further
introduce an adversarial scenario where a malicious contributor can inject bias
into the synthetic dataset via a subset of in-context examples, ultimately
compromising the fairness of downstream classifiers for a targeted and
protected subgroup. Our findings demonstrate a new vulnerability associated
with LLM-based data generation pipelines that rely on in-context prompts with
in sensitive domains.

</details>


### [110] [FedVLMBench: Benchmarking Federated Fine-Tuning of Vision-Language Models](https://arxiv.org/abs/2506.09638)
*Weiying Zheng,Ziyue Lin,Pengxin Guo,Yuyin Zhou,Feifei Wang,Liangqiong Qu*

Main category: cs.LG

TL;DR: 提出首个视觉语言模型联邦微调基准FedVLMBench，通过实验揭示架构、策略等间相互作用，给出最优配置等结论，为社区提供工具和指导。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型大多采用集中式训练，难以满足医疗等隐私要求高的领域，且缺乏评估联邦微调策略等的综合基准。

Method: 集成两种主流VLM架构、四种微调策略、五种FL算法、六个多模态数据集及多种任务场景构建FedVLMBench进行实验。

Result: 发现基于编码器的VLM在FL中特定配置为最优；当前FL方法在视觉中心任务中对数据异质性更敏感。

Conclusion: 该基准为社区提供了标准化平台，有助于推进多模态基础模型的隐私保护联邦训练。

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable capabilities in
cross-modal understanding and generation by integrating visual and textual
information. While instruction tuning and parameter-efficient fine-tuning
methods have substantially improved the generalization of VLMs, most existing
approaches rely on centralized training, posing challenges for deployment in
domains with strict privacy requirements like healthcare. Recent efforts have
introduced Federated Learning (FL) into VLM fine-tuning to address these
privacy concerns, yet comprehensive benchmarks for evaluating federated
fine-tuning strategies, model architectures, and task generalization remain
lacking. In this work, we present \textbf{FedVLMBench}, the first systematic
benchmark for federated fine-tuning of VLMs. FedVLMBench integrates two
mainstream VLM architectures (encoder-based and encoder-free), four fine-tuning
strategies, five FL algorithms, six multimodal datasets spanning four
cross-domain single-task scenarios and two cross-domain multitask settings,
covering four distinct downstream task categories. Through extensive
experiments, we uncover key insights into the interplay between VLM
architectures, fine-tuning strategies, data heterogeneity, and multi-task
federated optimization. Notably, we find that a 2-layer multilayer perceptron
(MLP) connector with concurrent connector and LLM tuning emerges as the optimal
configuration for encoder-based VLMs in FL. Furthermore, current FL methods
exhibit significantly higher sensitivity to data heterogeneity in
vision-centric tasks than text-centric ones, across both encoder-free and
encoder-based VLM architectures. Our benchmark provides essential tools,
datasets, and empirical guidance for the research community, offering a
standardized platform to advance privacy-preserving, federated training of
multimodal foundation models.

</details>


### [111] [Wavelet Scattering Transform and Fourier Representation for Offline Detection of Malicious Clients in Federated Learning](https://arxiv.org/abs/2506.09674)
*Alessandro Licciardi,Davide Leo,Davide Carbone*

Main category: cs.LG

TL;DR: 本文提出WAFFLE算法用于联邦学习中恶意客户端检测，实验显示该算法优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中异常或损坏客户端会降低模型性能，检测此类客户端且不访问原始数据是关键挑战。

Method: 提出WAFFLE算法，利用小波散射变换或傅里叶变换的本地压缩表示在训练前标记恶意客户端，用轻量级检测器进行标记。

Result: 实验表明该方法在基准数据集上提高了检测准确率和下游分类性能。

Conclusion: WAFFLE算法作为预训练检测策略有效，优于在线检测策略。

Abstract: Federated Learning (FL) enables the training of machine learning models
across decentralized clients while preserving data privacy. However, the
presence of anomalous or corrupted clients - such as those with faulty sensors
or non representative data distributions - can significantly degrade model
performance. Detecting such clients without accessing raw data remains a key
challenge. We propose WAFFLE (Wavelet and Fourier representations for Federated
Learning) a detection algorithm that labels malicious clients {\it before
training}, using locally computed compressed representations derived from
either the Wavelet Scattering Transform (WST) or the Fourier Transform. Both
approaches provide low-dimensional, task-agnostic embeddings suitable for
unsupervised client separation. A lightweight detector, trained on a
distillated public dataset, performs the labeling with minimal communication
and computational overhead. While both transforms enable effective detection,
WST offers theoretical advantages, such as non-invertibility and stability to
local deformations, that make it particularly well-suited to federated
scenarios. Experiments on benchmark datasets show that our method improves
detection accuracy and downstream classification performance compared to
existing FL anomaly detection algorithms, validating its effectiveness as a
pre-training alternative to online detection strategies.

</details>


### [112] [Wasserstein Hypergraph Neural Network](https://arxiv.org/abs/2506.09682)
*Iulia Duta,Pietro Liò*

Main category: cs.LG

TL;DR: 文章介绍Wasserstein超图神经网络，用切片Wasserstein池化聚合信息，实验表明该方法在节点分类任务上有出色表现。


<details>
  <summary>Details</summary>
Motivation: 现有超图神经网络大多借鉴图神经网络，聚合操作简单，需更好方法保留分布几何属性。

Method: 提出Wasserstein超图神经网络，将节点和超边邻域视为分布，用切片Wasserstein池化聚合信息。

Result: 在多个真实数据集的节点分类任务上取得最佳性能。

Conclusion: 在超图设置中应用Wasserstein池化对节点分类任务有显著益处。

Abstract: The ability to model relational information using machine learning has driven
advancements across various domains, from medicine to social science. While
graph representation learning has become mainstream over the past decade,
representing higher-order relationships through hypergraphs is rapidly gaining
momentum. In the last few years, numerous hypergraph neural networks have
emerged, most of them falling under a two-stage, set-based framework. The
messages are sent from nodes to edges and then from edges to nodes. However,
most of the advancement still takes inspiration from the graph counterpart,
often simplifying the aggregations to basic pooling operations. In this paper
we are introducing Wasserstein Hypergraph Neural Network, a model that treats
the nodes and hyperedge neighbourhood as distributions and aggregate the
information using Sliced Wasserstein Pooling. Unlike conventional aggregators
such as mean or sum, which only capture first-order statistics, our approach
has the ability to preserve geometric properties like the shape and spread of
distributions. This enables the learned embeddings to reflect how easily one
hyperedge distribution can be transformed into another, following principles of
optimal transport. Experimental results demonstrate that applying Wasserstein
pooling in a hypergraph setting significantly benefits node classification
tasks, achieving top performance on several real-world datasets.

</details>


### [113] [TRIDENT: Temporally Restricted Inference via DFA-Enhanced Neural Traversal](https://arxiv.org/abs/2506.09701)
*Vincenzo Collura,Karim Tit,Laura Bussi,Eleonora Giunchiglia,Maxime Cordy*

Main category: cs.LG

TL;DR: 提出TRIDENT算法，无需重新训练就能保证大语言模型输出满足时间约束，经实验验证其有效性与优越性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型等难以确保输出满足时间约束，如LTLf，因此需一种方法解决该问题。

Method: 将LTLf公式编译为DFA，用其引导受限的束搜索，在解码步骤中屏蔽违反约束的转移，并动态重新排序路径。

Result: 理论证明生成序列满足LTLf约束，实验表明提高了输出质量，在两个任务中实现完美约束满足，效率和质量指标优于现有方法。

Conclusion: TRIDENT算法能有效解决大语言模型输出的时间约束问题，且有较好的性能。

Abstract: Large Language Models (LLMs) and other neural architectures have achieved
impressive results across a variety of generative and classification tasks.
However, they remain fundamentally ill-equipped to ensure that their outputs
satisfy temporal constraints, such as those expressible in Linear Temporal
Logic over finite traces (LTLf). In this paper, we introduce TRIDENT: a general
and model-agnostic inference-time algorithm that guarantees compliance with
such constraints without requiring any retraining. TRIDENT compiles LTLf
formulas into a Deterministic Finite Automaton (DFA), which is used to guide a
constrained variant of beam search. At each decoding step, transitions that
would lead to constraint violations are masked, while remaining paths are
dynamically re-ranked based on both the model's probabilities and the DFA's
acceptance structure. We formally prove that the resulting sequences are
guaranteed to satisfy the given LTLf constraints, and we empirically
demonstrate that TRIDENT also improves output quality. We validate our approach
on two distinct tasks: temporally constrained image-stream classification and
controlled text generation. In both settings, TRIDENT achieves perfect
constraint satisfaction, while comparison with the state of the art shows
improved efficiency and high standard quality metrics.

</details>


### [114] [Auto-Compressing Networks](https://arxiv.org/abs/2506.09714)
*Vaggelis Dorovatas,Georgios Paraskevopoulos,Alexandros Potamianos*

Main category: cs.LG

TL;DR: 本文提出Auto - Compressing Networks (ACNs)，用长前馈连接替代短残差连接，有自动压缩特性，实验表明其在多方面表现优，结合传统剪枝技术能实现更好的稀疏 - 性能权衡。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络增加深度常带来计算冗余且表征质量无相应提升，需要新架构解决此问题。

Method: 提出ACNs架构，用长前馈连接替代短残差连接，利用自动压缩特性训练网络。

Result: ACNs在抗噪性、低数据设置、迁移学习等方面表现优于残差网络，能减少灾难性遗忘和架构压缩，结合传统剪枝技术有更好的稀疏 - 性能权衡。

Conclusion: ACNs是开发高效神经网络架构的实用方法，能自动适应任务复杂度并学习鲁棒表征。

Abstract: Deep neural networks with short residual connections have demonstrated
remarkable success across domains, but increasing depth often introduces
computational redundancy without corresponding improvements in representation
quality. In this work, we introduce Auto-Compressing Networks (ACNs), an
architectural variant where additive long feedforward connections from each
layer to the output replace traditional short residual connections. ACNs
showcase a unique property we coin as "auto-compression", the ability of a
network to organically compress information during training with gradient
descent, through architectural design alone. Through auto-compression,
information is dynamically "pushed" into early layers during training,
enhancing their representational quality and revealing potential redundancy in
deeper ones. We theoretically show that this property emerges from layer-wise
training patterns present in ACNs, where layers are dynamically utilized during
training based on task requirements. We also find that ACNs exhibit enhanced
noise robustness compared to residual networks, superior performance in
low-data settings, improved transfer learning capabilities, and mitigate
catastrophic forgetting suggesting that they learn representations that
generalize better despite using fewer parameters. Our results demonstrate up to
18% reduction in catastrophic forgetting and 30-80% architectural compression
while maintaining accuracy across vision transformers, MLP-mixers, and BERT
architectures. Furthermore, we demonstrate that coupling ACNs with traditional
pruning techniques, enables significantly better sparsity-performance
trade-offs compared to conventional architectures. These findings establish
ACNs as a practical approach to developing efficient neural architectures that
automatically adapt their computational footprint to task complexity, while
learning robust representations.

</details>


### [115] [AtmosMJ: Revisiting Gating Mechanism for AI Weather Forecasting Beyond the Year Scale](https://arxiv.org/abs/2506.09733)
*Minjong Cheon*

Main category: cs.LG

TL;DR: 本文挑战非标准数据表示对长期天气预测的必要性假设，提出AtmosMJ模型，在标准经纬度网格上实现约500天稳定预测，且训练成本低。


<details>
  <summary>Details</summary>
Motivation: 现有模型实现长期稳定自回归预测存在挑战，且主流模型依赖非标准空间域转换，作者想探究标准经纬度网格能否实现类似长程性能。

Method: 引入AtmosMJ深度卷积网络，直接处理ERA5数据，采用新型门控残差融合（GRF）机制防止误差累积。

Result: AtmosMJ能产生约500天稳定且符合物理规律的预测，10天预测精度有竞争力，训练成本低。

Conclusion: 高效的架构设计而非非标准数据表示，是实现稳定且计算高效的长期天气预报的关键。

Abstract: The advent of Large Weather Models (LWMs) has marked a turning point in
data-driven forecasting, with many models now outperforming traditional
numerical systems in the medium range. However, achieving stable, long-range
autoregressive forecasts beyond a few weeks remains a significant challenge.
Prevailing state-of-the-art models that achieve year-long stability, such as
SFNO and DLWP-HPX, have relied on transforming input data onto non-standard
spatial domains like spherical harmonics or HEALPix meshes. This has led to the
prevailing assumption that such representations are necessary to enforce
physical consistency and long-term stability. This paper challenges that
assumption by investigating whether comparable long-range performance can be
achieved on the standard latitude-longitude grid. We introduce AtmosMJ, a deep
convolutional network that operates directly on ERA5 data without any spherical
remapping. The model's stability is enabled by a novel Gated Residual Fusion
(GRF) mechanism, which adaptively moderates feature updates to prevent error
accumulation over long recursive simulations. Our results demonstrate that
AtmosMJ produces stable and physically plausible forecasts for about 500 days.
In quantitative evaluations, it achieves competitive 10-day forecast accuracy
against models like Pangu-Weather and GraphCast, all while requiring a
remarkably low training budget of 5.7 days on a V100 GPU. Our findings suggest
that efficient architectural design, rather than non-standard data
representation, can be the key to unlocking stable and computationally
efficient long-range weather prediction.

</details>


### [116] [Towards Multi-modal Graph Large Language Model](https://arxiv.org/abs/2506.09738)
*Xin Wang,Zeyang Zhang,Linxin Xiao,Haibo Chen,Chendi Ge,Wenwu Zhu*

Main category: cs.LG

TL;DR: 文章探讨多模态图大语言模型（MG - LLM）统一多模态图数据和任务的潜力，提出其五个关键特性，阐述挑战、相关工作和未来方向，总结相关数据集。


<details>
  <summary>Details</summary>
Motivation: 现有多模态图学习方法难以在不同多模态图数据和任务间泛化，需探索MG - LLM以填补此差距。

Method: 提出多模态图数据、任务和模型的统一框架，发现多模态图的多粒度和多尺度特征，提出MG - LLM的五个关键特性。

Result: 阐述实现关键特性的挑战、相关工作和未来研究方向，总结相关数据集。

Conclusion: 本文有助于推进MG - LLM在多模态图数据和任务泛化方面的研究。

Abstract: Multi-modal graphs, which integrate diverse multi-modal features and
relations, are ubiquitous in real-world applications. However, existing
multi-modal graph learning methods are typically trained from scratch for
specific graph data and tasks, failing to generalize across various multi-modal
graph data and tasks. To bridge this gap, we explore the potential of
Multi-modal Graph Large Language Models (MG-LLM) to unify and generalize across
diverse multi-modal graph data and tasks. We propose a unified framework of
multi-modal graph data, task, and model, discovering the inherent
multi-granularity and multi-scale characteristics in multi-modal graphs.
Specifically, we present five key desired characteristics for MG-LLM: 1)
unified space for multi-modal structures and attributes, 2) capability of
handling diverse multi-modal graph tasks, 3) multi-modal graph in-context
learning, 4) multi-modal graph interaction with natural language, and 5)
multi-modal graph reasoning. We then elaborate on the key challenges, review
related works, and highlight promising future research directions towards
realizing these ambitious characteristics. Finally, we summarize existing
multi-modal graph datasets pertinent for model training. We believe this paper
can contribute to the ongoing advancement of the research towards MG-LLM for
generalization across multi-modal graph data and tasks.

</details>


### [117] [Feature Engineering for Agents: An Adaptive Cognitive Architecture for Interpretable ML Monitoring](https://arxiv.org/abs/2506.09742)
*Gusseppe Bravo-Rocca,Peini Liu,Jordi Guitart,Rodrigo M Carrillo-Larco,Ajay Dholakia,David Ellison*

Main category: cs.LG

TL;DR: 提出基于LLM的ML监控认知架构，用特征工程原理增强监控输出可解释性，实验显示比基线更准确。


<details>
  <summary>Details</summary>
Motivation: 传统ML模型监控方法输出冗长、可解释性低，阻碍有效决策。

Method: 提出认知架构，核心是决策程序模块，包含重构、分解、编译三步模拟特征工程，结合特征工程驱动规划和选择性使用LLM。

Result: 使用多个LLM的实验表明，在多个领域比各种基线有更高的准确率。

Conclusion: 该架构能形成强大的决策支持系统，提供高可解释性和可操作的见解。

Abstract: Monitoring Machine Learning (ML) models in production environments is
crucial, yet traditional approaches often yield verbose, low-interpretability
outputs that hinder effective decision-making. We propose a cognitive
architecture for ML monitoring that applies feature engineering principles to
agents based on Large Language Models (LLMs), significantly enhancing the
interpretability of monitoring outputs. Central to our approach is a Decision
Procedure module that simulates feature engineering through three key steps:
Refactor, Break Down, and Compile. The Refactor step improves data
representation to better capture feature semantics, allowing the LLM to focus
on salient aspects of the monitoring data while reducing noise and irrelevant
information. Break Down decomposes complex information for detailed analysis,
and Compile integrates sub-insights into clear, interpretable outputs. This
process leads to a more deterministic planning approach, reducing dependence on
LLM-generated planning, which can sometimes be inconsistent and overly general.
The combination of feature engineering-driven planning and selective LLM
utilization results in a robust decision support system, capable of providing
highly interpretable and actionable insights. Experiments using multiple LLMs
demonstrate the efficacy of our approach, achieving significantly higher
accuracy compared to various baselines across several domains.

</details>


### [118] [Load-Aware Training Scheduling for Model Circulation-based Decentralized Federated Learning](https://arxiv.org/abs/2506.09769)
*Haruki Kainuma,Takayuki Nishio*

Main category: cs.LG

TL;DR: 提出Load - aware Tram - FL，通过考虑计算和通信负载最小化分散式联邦学习的总训练时间，模拟结果表明其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 在分散式联邦学习中最小化总训练时间，同时促进非IID分布下的数据均衡利用。

Method: 将调度问题表述为全局优化任务，分解为节点子问题求解，引入方差约束，通过目标函数最小化训练延迟。

Result: 在MNIST和CIFAR - 10上的模拟结果显示，Load - aware Tram - FL显著减少训练时间并加速收敛。

Conclusion: Load - aware Tram - FL在减少训练时间和加速收敛方面优于基线方法，能有效应用于分散式联邦学习。

Abstract: This paper proposes Load-aware Tram-FL, an extension of Tram-FL that
introduces a training scheduling mechanism to minimize total training time in
decentralized federated learning by accounting for both computational and
communication loads. The scheduling problem is formulated as a global
optimization task, which-though intractable in its original form-is made
solvable by decomposing it into node-wise subproblems. To promote balanced data
utilization under non-IID distributions, a variance constraint is introduced,
while the overall training latency, including both computation and
communication costs, is minimized through the objective function. Simulation
results on MNIST and CIFAR-10 demonstrate that Load-aware Tram-FL significantly
reduces training time and accelerates convergence compared to baseline methods.

</details>


### [119] [A theoretical framework for self-supervised contrastive learning for continuous dependent data](https://arxiv.org/abs/2506.09785)
*Alexander Marusov,Alexander Yuhay,Alexey Zaytsev*

Main category: cs.LG

TL;DR: 提出针对连续依赖数据的对比自监督学习理论框架，在下游任务验证有效，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 自监督学习在依赖数据应用探索不足，传统对比方法假设不适用于依赖数据。

Method: 提出针对连续依赖数据的对比自监督学习理论框架，给出两种真实相似度度量，推导估计相似度矩阵，引入依赖感知损失函数。

Result: 在时间和时空下游问题验证，在标准基准上超越TS2Vec，在干旱分类任务ROC - AUC得分提高7%。

Conclusion: 理论驱动的损失函数在捕捉时空依赖方面有效。

Abstract: Self-supervised learning (SSL) has emerged as a powerful approach to learning
representations, particularly in the field of computer vision. However, its
application to dependent data, such as temporal and spatio-temporal domains,
remains underexplored. Besides, traditional contrastive SSL methods often
assume \emph{semantic independence between samples}, which does not hold for
dependent data exhibiting complex correlations. We propose a novel theoretical
framework for contrastive SSL tailored to \emph{continuous dependent data},
which allows the nearest samples to be semantically close to each other. In
particular, we propose two possible \textit{ground truth similarity measures}
between objects -- \emph{hard} and \emph{soft} closeness. Under it, we derive
an analytical form for the \textit{estimated similarity matrix} that
accommodates both types of closeness between samples, thereby introducing
dependency-aware loss functions. We validate our approach, \emph{Dependent
TS2Vec}, on temporal and spatio-temporal downstream problems. Given the
dependency patterns presented in the data, our approach surpasses modern ones
for dependent data, highlighting the effectiveness of our theoretically
grounded loss functions for SSL in capturing spatio-temporal dependencies.
Specifically, we outperform TS2Vec on the standard UEA and UCR benchmarks, with
accuracy improvements of $4.17$\% and $2.08$\%, respectively. Furthermore, on
the drought classification task, which involves complex spatio-temporal
patterns, our method achieves a $7$\% higher ROC-AUC score.

</details>


### [120] [Devil's Hand: Data Poisoning Attacks to Locally Private Graph Learning Protocols](https://arxiv.org/abs/2506.09803)
*Longzhu He,Chaozhuo Li,Peng Tang,Litian Zhang,Sen Su*

Main category: cs.LG

TL;DR: 论文指出本地私有图学习协议虽有隐私保护优势，但易受数据投毒攻击，介绍了针对该协议的攻击方式并验证其有效性，还探讨了防御策略但效果有限。


<details>
  <summary>Details</summary>
Motivation: 现有本地私有图学习协议未考虑数据投毒攻击威胁，为确保隐私保护图学习框架的鲁棒性和安全性开展研究。

Method: 引入针对本地私有图学习协议的数据投毒攻击，攻击者注入虚假用户、建立连接并发送精心构造数据。

Result: 从理论和实证两方面证明了攻击的有效性，探索的防御策略效果有限。

Conclusion: 本地私有图学习协议存在数据投毒攻击风险，当前防御策略需改进，需更强大的防御手段。

Abstract: Graph neural networks (GNNs) have achieved significant success in graph
representation learning and have been applied to various domains. However, many
real-world graphs contain sensitive personal information, such as user profiles
in social networks, raising serious privacy concerns when graph learning is
performed using GNNs. To address this issue, locally private graph learning
protocols have gained considerable attention. These protocols leverage the
privacy advantages of local differential privacy (LDP) and the effectiveness of
GNN's message-passing in calibrating noisy data, offering strict privacy
guarantees for users' local data while maintaining high utility (e.g., node
classification accuracy) for graph learning. Despite these advantages, such
protocols may be vulnerable to data poisoning attacks, a threat that has not
been considered in previous research. Identifying and addressing these threats
is crucial for ensuring the robustness and security of privacy-preserving graph
learning frameworks. This work introduces the first data poisoning attack
targeting locally private graph learning protocols. The attacker injects fake
users into the protocol, manipulates these fake users to establish links with
genuine users, and sends carefully crafted data to the server, ultimately
compromising the utility of private graph learning. The effectiveness of the
attack is demonstrated both theoretically and empirically. In addition, several
defense strategies have also been explored, but their limited effectiveness
highlights the need for more robust defenses.

</details>


### [121] [Generalizing Supervised Contrastive learning: A Projection Perspective](https://arxiv.org/abs/2506.09810)
*Minoh Jeong,Alfred Hero*

Main category: cs.LG

TL;DR: 文章提出ProjNCE损失函数统一有监督和自监督对比目标，证明其为有效互信息边界，实验显示其性能优于SupCon和标准交叉熵训练。


<details>
  <summary>Details</summary>
Motivation: 有监督对比方法在自监督对比学习背景下受关注少，SupCon与互信息关系未被探索。

Method: 引入ProjNCE损失函数，通过结合投影函数和负对调整项统一有监督和自监督对比目标，并探索多种投影方法。

Result: 在多个数据集和设置上的大量实验表明，ProjNCE始终优于SupCon和标准交叉熵训练。

Conclusion: 从互信息解释和投影设计两方面改进SupCon，在以SupCon为基础对比目标时提供广泛适用的改进。

Abstract: Self-supervised contrastive learning (SSCL) has emerged as a powerful
paradigm for representation learning and has been studied from multiple
perspectives, including mutual information and geometric viewpoints. However,
supervised contrastive (SupCon) approaches have received comparatively little
attention in this context: for instance, while InfoNCE used in SSCL is known to
form a lower bound on mutual information (MI), the relationship between SupCon
and MI remains unexplored. To address this gap, we introduce ProjNCE, a
generalization of the InfoNCE loss that unifies supervised and self-supervised
contrastive objectives by incorporating projection functions and an adjustment
term for negative pairs. We prove that ProjNCE constitutes a valid MI bound and
affords greater flexibility in selecting projection strategies for class
embeddings. Building on this flexibility, we further explore the centroid-based
class embeddings in SupCon by exploring a variety of projection methods.
Extensive experiments on multiple datasets and settings demonstrate that
ProjNCE consistently outperforms both SupCon and standard cross-entropy
training. Our work thus refines SupCon along two complementary
perspective--mutual information interpretation and projection design--and
offers broadly applicable improvements whenever SupCon serves as the
foundational contrastive objective.

</details>


### [122] [Identifiability Challenges in Sparse Linear Ordinary Differential Equations](https://arxiv.org/abs/2506.09816)
*Cecilia Casolo,Sören Becker,Niki Kilbertus*

Main category: cs.LG

TL;DR: 本文研究稀疏线性常微分方程（ODE）的可识别性，发现其在实际稀疏区域有正概率不可识别，且理论不可识别性在实际估计方法中也存在。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明线性ODE在稠密矩阵下几乎可从单轨迹识别，但稀疏区域研究不足，而稀疏性在多系统中自然存在，故研究稀疏线性ODE可识别性。

Method: 对稀疏线性ODE的可识别性进行理论刻画，通过实验研究理论不可识别性在现有方法中的体现。

Result: 稀疏系统在实际稀疏区域有正概率不可识别，且实际估计中也存在不可识别性，归纳偏差和优化动态无法解决理论局限。

Conclusion: 需要重新思考数据驱动的动态系统建模的预期，并可对学习到的线性ODE进行定量评估。

Abstract: Dynamical systems modeling is a core pillar of scientific inquiry across
natural and life sciences. Increasingly, dynamical system models are learned
from data, rendering identifiability a paramount concept. For systems that are
not identifiable from data, no guarantees can be given about their behavior
under new conditions and inputs, or about possible control mechanisms to steer
the system. It is known in the community that "linear ordinary differential
equations (ODE) are almost surely identifiable from a single trajectory."
However, this only holds for dense matrices. The sparse regime remains
underexplored, despite its practical relevance with sparsity arising naturally
in many biological, social, and physical systems. In this work, we address this
gap by characterizing the identifiability of sparse linear ODEs. Contrary to
the dense case, we show that sparse systems are unidentifiable with a positive
probability in practically relevant sparsity regimes and provide lower bounds
for this probability. We further study empirically how this theoretical
unidentifiability manifests in state-of-the-art methods to estimate linear ODEs
from data. Our results corroborate that sparse systems are also practically
unidentifiable. Theoretical limitations are not resolved through inductive
biases or optimization dynamics. Our findings call for rethinking what can be
expected from data-driven dynamical system modeling and allows for quantitative
assessments of how much to trust a learned linear ODE.

</details>


### [123] [Weighted Loss Methods for Robust Federated Learning under Data Heterogeneity](https://arxiv.org/abs/2506.09824)
*Johan Erbani,Sonia Ben Mokhtar,Pierre-Edouard Portier,Elod Egyed-Zsigmond,Diana Nurbakova*

Main category: cs.LG

TL;DR: 本文提出Worker Label Alignement Loss (WoLA)方法，解决联邦学习中拜占庭攻击问题，在异构环境下效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习存在拜占庭参与者贡献有害梯度影响模型收敛问题，现有方法在异构环境难以区分诚实和拜占庭梯度。

Method: 引入Worker Label Alignement Loss (WoLA)加权损失函数，在数据异构情况下对齐诚实工作者的梯度。

Result: 该方法在异构环境下显著优于现有方法。

Conclusion: 通过理论分析和实证验证了WoLA方法的有效性。

Abstract: Federated learning (FL) is a machine learning paradigm that enables multiple
data holders to collaboratively train a machine learning model without sharing
their training data with external parties. In this paradigm, workers locally
update a model and share with a central server their updated gradients (or
model parameters). While FL seems appealing from a privacy perspective, it
opens a number of threats from a security perspective as (Byzantine)
participants can contribute poisonous gradients (or model parameters) harming
model convergence. Byzantine-resilient FL addresses this issue by ensuring that
the training proceeds as if Byzantine participants were absent. Towards this
purpose, common strategies ignore outlier gradients during model aggregation,
assuming that Byzantine gradients deviate more from honest gradients than
honest gradients do from each other. However, in heterogeneous settings, honest
gradients may differ significantly, making it difficult to distinguish honest
outliers from Byzantine ones. In this paper, we introduce the Worker Label
Alignement Loss (WoLA), a weighted loss that aligns honest worker gradients
despite data heterogeneity, which facilitates the identification of Byzantines'
gradients. This approach significantly outperforms state-of-the-art methods in
heterogeneous settings. In this paper, we provide both theoretical insights and
empirical evidence of its effectiveness.

</details>


### [124] [Guided Graph Compression for Quantum Graph Neural Networks](https://arxiv.org/abs/2506.09862)
*Mikel Casals,Vasilis Belis,Elias F. Combarro,Eduard Alarcón,Sofia Vallecorsa,Michele Grossi*

Main category: cs.LG

TL;DR: 本文提出引导图压缩（GGC）框架处理图数据，在喷注标记任务上评估，结果显示GGC表现更优，且利于在真实数据集测试新QGNN方案。


<details>
  <summary>Details</summary>
Motivation: GNN处理大图有高内存需求和GPU稀疏矩阵运算低效问题，QC可解决但当前量子硬件对数据编码维度有限制，现有方法处理方式不佳。

Method: 引入GGC框架，用图自动编码器减少节点数量和节点特征维度，引导压缩以提升下游分类任务性能。

Result: 在喷注标记任务上，GGC优于将自动编码器作为独立预处理步骤和基线经典GNN分类器。

Conclusion: GGC表现出色，能促进在真实数据集测试新QGNN方案。

Abstract: Graph Neural Networks (GNNs) are effective for processing graph-structured
data but face challenges with large graphs due to high memory requirements and
inefficient sparse matrix operations on GPUs. Quantum Computing (QC) offers a
promising avenue to address these issues and inspires new algorithmic
approaches. In particular, Quantum Graph Neural Networks (QGNNs) have been
explored in recent literature. However, current quantum hardware limits the
dimension of the data that can be effectively encoded. Existing approaches
either simplify datasets manually or use artificial graph datasets. This work
introduces the Guided Graph Compression (GGC) framework, which uses a graph
autoencoder to reduce both the number of nodes and the dimensionality of node
features. The compression is guided to enhance the performance of a downstream
classification task, which can be applied either with a quantum or a classical
classifier. The framework is evaluated on the Jet Tagging task, a
classification problem of fundamental importance in high energy physics that
involves distinguishing particle jets initiated by quarks from those by gluons.
The GGC is compared against using the autoencoder as a standalone preprocessing
step and against a baseline classical GNN classifier. Our numerical results
demonstrate that GGC outperforms both alternatives, while also facilitating the
testing of novel QGNN ansatzes on realistic datasets.

</details>


### [125] [Machine Learning-Based Classification of Oils Using Dielectric Properties and Microwave Resonant Sensing](https://arxiv.org/abs/2506.09867)
*Amit Baran Dey,Wasim Arif,Rakhesh Singh Kshetrimayum*

Main category: cs.LG

TL;DR: 本文提出基于微波谐振传感器和机器学习的油品分类方法，实验表明随机森林分类器准确率达99.41%，适用于工业环境。


<details>
  <summary>Details</summary>
Motivation: 提出一种能用于实时工业应用的油品分类方法。

Method: 利用微波谐振传感器捕获油品介电特性引起的谐振频率和幅度响应变化，提取特征，用机器学习分类器分类，开发综合数据集训练和评估分类器。

Result: 随机森林分类器分类准确率达99.41%。

Conclusion: 该方法具备快速可靠的油品分类能力，适用于工业环境。

Abstract: This paper proposes a machine learning-based methodology for the
classification of various oil samples based on their dielectric properties,
utilizing a microwave resonant sensor. The dielectric behaviour of oils,
governed by their molecular composition, induces distinct shifts in the
sensor's resonant frequency and amplitude response. These variations are
systematically captured and processed to extract salient features, which serve
as inputs for multiple machine learning classifiers. The microwave resonant
sensor operates in a non-destructive, low-power manner, making it particularly
well-suited for real-time industrial applications. A comprehensive dataset is
developed by varying the permittivity of oil samples and acquiring the
corresponding sensor responses. Several classifiers are trained and evaluated
using the extracted resonant features to assess their capability in
distinguishing between oil types. Experimental results demonstrate that the
proposed approach achieves a high classification accuracy of 99.41% with the
random forest classifier, highlighting its strong potential for automated oil
identification. The system's compact form factor, efficiency, and high
performance underscore its viability for fast and reliable oil characterization
in industrial environments.

</details>


### [126] [A look at adversarial attacks on radio waveforms from discrete latent space](https://arxiv.org/abs/2506.09896)
*Attanasia Garuso,Silvija Kokalj-Filipovic,Yagna Kaasaragadda*

Main category: cs.LG

TL;DR: 分析VQVAE在高信噪比射频数据点遭受对抗攻击时的攻击抑制特性，通过实验表明其可降低攻击有效性，还发现离散空间有趣特性助于检测攻击。


<details>
  <summary>Details</summary>
Motivation: 研究VQVAE在对抗攻击下对高信噪比射频数据的攻击抑制特性。

Method: 设计VQVAE将数字无线电波形映射到离散潜在空间，创建保留和不保留相位的对抗攻击，比较不同攻击下分类器准确率，评估VQVAE重建数据后的分类器准确率，对比I/Q平面示意图和潜在空间概率分布。

Result: VQVAE大幅降低攻击有效性，改变攻击强度时离散空间有有趣特性。

Conclusion: VQVAE具有攻击抑制能力，离散空间特性有助于检测对抗攻击。

Abstract: Having designed a VQVAE that maps digital radio waveforms into discrete
latent space, and yields a perfectly classifiable reconstruction of the
original data, we here analyze the attack suppressing properties of VQVAE when
an adversarial attack is performed on high-SNR radio-frequency (RF)
data-points. To target amplitude modulations from a subset of digitally
modulated waveform classes, we first create adversarial attacks that preserve
the phase between the in-phase and quadrature component whose values are
adversarially changed. We compare them with adversarial attacks of the same
intensity where phase is not preserved. We test the classification accuracy of
such adversarial examples on a classifier trained to deliver 100% accuracy on
the original data. To assess the ability of VQVAE to suppress the strength of
the attack, we evaluate the classifier accuracy on the reconstructions by VQVAE
of the adversarial datapoints and show that VQVAE substantially decreases the
effectiveness of the attack. We also compare the I/Q plane diagram of the
attacked data, their reconstructions and the original data. Finally, using
multiple methods and metrics, we compare the probability distribution of the
VQVAE latent space with and without attack. Varying the attack strength, we
observe interesting properties of the discrete space, which may help detect the
attacks.

</details>


### [127] ["What are my options?": Explaining RL Agents with Diverse Near-Optimal Alternatives (Extended)](https://arxiv.org/abs/2506.09901)
*Noel Brindise,Vijeth Hebbar,Riya Shah,Cedric Langbort*

Main category: cs.LG

TL;DR: 本文探讨可解释强化学习新方法DNA，通过奖励塑形求解不同策略，在模拟中成功得到不同‘选项’，还为强化学习探索和规划带来新可能。


<details>
  <summary>Details</summary>
Motivation: 提供可解释强化学习的新方法，以‘选项’的形式向人类用户解释智能体的选择。

Method: 使用局部修改Q学习问题中的奖励塑形，求解具有保证的ε - 最优性的不同策略。

Result: 在模拟中成功得到质上不同的策略，构成有意义的不同‘选项’，并与随机优化领域的相关方法作了简要比较。

Conclusion: 该方法除了解释性动机外，为强化学习中的探索和自适应规划开辟了新的可能性。

Abstract: In this work, we provide an extended discussion of a new approach to
explainable Reinforcement Learning called Diverse Near-Optimal Alternatives
(DNA), first proposed at L4DC 2025. DNA seeks a set of reasonable "options" for
trajectory-planning agents, optimizing policies to produce qualitatively
diverse trajectories in Euclidean space. In the spirit of explainability, these
distinct policies are used to "explain" an agent's options in terms of
available trajectory shapes from which a human user may choose. In particular,
DNA applies to value function-based policies on Markov decision processes where
agents are limited to continuous trajectories. Here, we describe DNA, which
uses reward shaping in local, modified Q-learning problems to solve for
distinct policies with guaranteed epsilon-optimality. We show that it
successfully returns qualitatively different policies that constitute
meaningfully different "options" in simulation, including a brief comparison to
related approaches in the stochastic optimization field of Quality Diversity.
Beyond the explanatory motivation, this work opens new possibilities for
exploration and adaptive planning in RL.

</details>


### [128] [Apollo: A Posteriori Label-Only Membership Inference Attack Towards Machine Unlearning](https://arxiv.org/abs/2506.09923)
*Liou Tang,James Joshi,Ashish Kundu*

Main category: cs.LG

TL;DR: 提出新型隐私攻击Apollo，仅通过未学习模型的标签输出推断样本是否被移除，在未学习样本成员状态推断上精度较高。


<details>
  <summary>Details</summary>
Motivation: 现有针对机器无学习（MU）的隐私推理攻击依赖弱威胁模型，在现实场景中可行性受限，需要更严格威胁模型下的攻击方法。

Method: 提出A Posteriori Label-Only Membership Inference Attack towards MU（Apollo）攻击，在攻击者仅能访问未学习模型标签输出的严格威胁模型下，推断数据样本是否被移除。

Result: 相比之前的攻击，所需对目标模型的访问权限更少，能在未学习样本成员状态推断上达到较高精度。

Conclusion: 提出的Apollo攻击在严格威胁模型下有效，能在少访问权限下实现较精准的隐私推理。

Abstract: Machine Unlearning (MU) aims to update Machine Learning (ML) models following
requests to remove training samples and their influences on a trained model
efficiently without retraining the original ML model from scratch. While MU
itself has been employed to provide privacy protection and regulatory
compliance, it can also increase the attack surface of the model. Existing
privacy inference attacks towards MU that aim to infer properties of the
unlearned set rely on the weaker threat model that assumes the attacker has
access to both the unlearned model and the original model, limiting their
feasibility toward real-life scenarios. We propose a novel privacy attack, A
Posteriori Label-Only Membership Inference Attack towards MU, Apollo, that
infers whether a data sample has been unlearned, following a strict threat
model where an adversary has access to the label-output of the unlearned model
only. We demonstrate that our proposed attack, while requiring less access to
the target model compared to previous attacks, can achieve relatively high
precision on the membership status of the unlearned samples.

</details>


### [129] [Canonical Latent Representations in Conditional Diffusion Models](https://arxiv.org/abs/2506.09955)
*Yitao Xu,Tong Zhang,Ehsan Pajouheshgar,Sabine Süsstrunk*

Main category: cs.LG

TL;DR: 本文指出条件扩散模型（CDMs）存在特征纠缠问题，提出Canonical LAtent Representations（CLAReps），并开发CaDistill特征蒸馏范式，使学生模型获得强鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 条件扩散模型在建模时会将类别定义特征与无关上下文纠缠，难以提取鲁棒和可解释的表示。

Method: 识别出CLAReps，开发基于扩散的特征蒸馏范式CaDistill，教师模型CDM通过CLAReps向学生模型传递核心类别知识。

Result: 学生模型在训练后获得强对抗鲁棒性和泛化能力，更关注类别信号而非虚假背景线索。

Conclusion: CDMs不仅可作为图像生成器，还可作为紧凑、可解释的教师驱动鲁棒表示学习。

Abstract: Conditional diffusion models (CDMs) have shown impressive performance across
a range of generative tasks. Their ability to model the full data distribution
has opened new avenues for analysis-by-synthesis in downstream discriminative
learning. However, this same modeling capacity causes CDMs to entangle the
class-defining features with irrelevant context, posing challenges to
extracting robust and interpretable representations. To this end, we identify
Canonical LAtent Representations (CLAReps), latent codes whose internal CDM
features preserve essential categorical information while discarding
non-discriminative signals. When decoded, CLAReps produce representative
samples for each class, offering an interpretable and compact summary of the
core class semantics with minimal irrelevant details. Exploiting CLAReps, we
develop a novel diffusion-based feature-distillation paradigm, CaDistill. While
the student has full access to the training set, the CDM as teacher transfers
core class knowledge only via CLAReps, which amounts to merely 10 % of the
training data in size. After training, the student achieves strong adversarial
robustness and generalization ability, focusing more on the class signals
instead of spurious background cues. Our findings suggest that CDMs can serve
not just as image generators but also as compact, interpretable teachers that
can drive robust representation learning.

</details>


### [130] [Multiverse: Your Language Models Secretly Decide How to Parallelize and Merge Generation](https://arxiv.org/abs/2506.09991)
*Xinyu Yang,Yuwei An,Hongyi Liu,Tianqi Chen,Beidi Chen*

Main category: cs.LG

TL;DR: 提出Multiverse生成模型实现原生并行生成，经微调后性能与同规模AR - LLMs相当，有更好扩展性和效率，且开源整个生态。


<details>
  <summary>Details</summary>
Motivation: 受AR - LLMs顺序生成中的隐式并行性启发，开发能原生并行生成的模型。

Method: 模型内化MapReduce范式分三阶段生成；构建推理模型，通过数据、算法和系统协同设计；设计Multiverse Attention；实现Multiverse Engine；将顺序推理链转换为结构化训练数据。

Result: Multiverse - 32B经微调后在AIME24 & 25测试中与同规模领先AR - LLMs性能相当，扩展性比AR - LLMs平均高1.87%，不同批量大小下速度提升达2倍。

Conclusion: Multiverse模型能实现并行生成，性能良好，有更好扩展性和效率，开源生态利于后续研究。

Abstract: Autoregressive Large Language Models (AR-LLMs) frequently exhibit implicit
parallelism in sequential generation. Inspired by this, we introduce
Multiverse, a new generative model that enables natively parallel generation.
Multiverse internalizes a MapReduce paradigm, generating automatically through
three stages: (i) a Map stage for adaptive task decomposition, (ii) a Process
stage for parallel subtask execution, and (iii) a Reduce stage for lossless
result synthesis. Next, we build a real-world Multiverse reasoning model with
co-design of data, algorithm, and system, enabling rapid and seamless transfer
from frontier AR-LLMs. Starting from sequential reasoning chains, we create
Multiverse 1K by converting them into structured training data using an
automated LLM-assisted pipeline, avoiding costly human annotations.
Algorithmically, we design Multiverse Attention to separate parallel reasoning
steps while keeping compatibility with causal attention for efficient training.
Systematically, we implement Multiverse Engine to enable parallel inference. It
features a dedicated scheduler that dynamically switches between sequential and
parallel generation, triggered directly by the model. After a 3-hour
fine-tuning with 1K examples, our Multiverse-32B stands as the only
open-sourced non-AR model achieving performance on par with leading AR-LLMs of
the same scale, evidenced by AIME24 & 25 scores of 54% and 46%, respectively.
Moreover, our budget control experiments show that Multiverse-32B exhibits
superior scaling, outperforming AR-LLMs by 1.87% on average using the same
context length. Such scaling further leads to practical efficiency gain,
achieving up to 2x speedup across varying batch sizes. We have open-sourced the
entire Multiverse ecosystem, including data, model weights, engine, supporting
tools, as well as complete data curation prompts and detailed training and
evaluation recipes.

</details>


### [131] [Flipping Against All Odds: Reducing LLM Coin Flip Bias via Verbalized Rejection Sampling](https://arxiv.org/abs/2506.09998)
*Tim Z. Xiao,Johannes Zenn,Zhen Liu,Weiyang Liu,Robert Bamler,Bernhard Schölkopf*

Main category: cs.LG

TL;DR: 论文研究大语言模型在伯努利分布中知识与采样的差距，引入VRS方法减少采样偏差，展示经典概率工具可嵌入LLM工作流提高可靠性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽能描述概率分布，但难以生成忠实样本，限制了其在需要可靠随机性任务中的应用。

Method: 引入Verbalized Rejection Sampling (VRS)方法，即对经典拒绝采样进行自然语言适配，让大语言模型对提议样本进行推理和取舍。

Result: VRS大幅减少了各模型的采样偏差，理论分析表明在温和假设下，VRS优于直接采样。

Conclusion: 经典概率工具可通过语言化嵌入大语言模型工作流，提高可靠性，无需访问模型内部或进行大量提示工程。

Abstract: Large language models (LLMs) can often accurately describe probability
distributions using natural language, yet they still struggle to generate
faithful samples from them. This mismatch limits their use in tasks requiring
reliable stochasticity, such as Monte Carlo methods, agent-based simulations,
and randomized decision-making. We investigate this gap between knowledge and
sampling in the context of Bernoulli distributions. We introduce Verbalized
Rejection Sampling (VRS), a natural-language adaptation of classical rejection
sampling that prompts the LLM to reason about and accept or reject proposed
samples. Despite relying on the same Bernoulli mechanism internally, VRS
substantially reduces sampling bias across models. We provide theoretical
analysis showing that, under mild assumptions, VRS improves over direct
sampling, with gains attributable to both the algorithm and prompt design. More
broadly, our results show how classical probabilistic tools can be verbalized
and embedded into LLM workflows to improve reliability, without requiring
access to model internals or heavy prompt engineering.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [132] [A Topological Improvement of the Overall Performance of Sparse Evolutionary Training: Motif-Based Structural Optimization of Sparse MLPs Project](https://arxiv.org/abs/2506.09204)
*Xiaotian Chen,Hongyun Liu,Seyed Sahand Mohammadi Ziabari*

Main category: cs.NE

TL;DR: 研究DNN计算成本和内存开销问题，探讨SET - MLP结构优化能否提升性能及提升程度。


<details>
  <summary>Details</summary>
Motivation: DNN模型复杂度增加，需降低计算成本和内存开销，SET算法有优化空间。

Method: 运用基于主题的优化这一结构优化方法对SET - MLP进行研究。

Result: 未提及具体研究结果。

Conclusion: 未提及具体结论，仅提出研究方向是探讨SET - MLP结构优化的性能提升情况。

Abstract: Deep Neural Networks (DNNs) have been proven to be exceptionally effective
and have been applied across diverse domains within deep learning. However, as
DNN models increase in complexity, the demand for reduced computational costs
and memory overheads has become increasingly urgent. Sparsity has emerged as a
leading approach in this area. The robustness of sparse Multi-layer Perceptrons
(MLPs) for supervised feature selection, along with the application of Sparse
Evolutionary Training (SET), illustrates the feasibility of reducing
computational costs without compromising accuracy. Moreover, it is believed
that the SET algorithm can still be improved through a structural optimization
method called motif-based optimization, with potential efficiency gains
exceeding 40% and a performance decline of under 4%. This research investigates
whether the structural optimization of Sparse Evolutionary Training applied to
Multi-layer Perceptrons (SET-MLP) can enhance performance and to what extent
this improvement can be achieved.

</details>


### [133] [Energy Aware Development of Neuromorphic Implantables: From Metrics to Action](https://arxiv.org/abs/2506.09599)
*Enrique Barba Roque,Luis Cruz*

Main category: cs.NE

TL;DR: 本文对SNN基准文献中提出的能效指标进行初步探索研究，发现现有指标存在问题并给出研究方向。


<details>
  <summary>Details</summary>
Motivation: 评估SNN模型的能源性能存在挑战，缺乏标准化和可操作的指标，且实验性神经形态硬件能耗测量困难。

Method: 对13个常用指标基于四个关键属性进行分类。

Result: 许多现有指标虽能比较架构，但缺乏对SNN开发者的实用见解，存在易获取指标和高保真度指标之间的差距，缺乏可操作指标。

Conclusion: 为提高SNN能源指标及其可操作性的未来研究奠定基础，给出了相关研究方向。

Abstract: Spiking Neural Networks (SNNs) and neuromorphic computing present a promising
alternative to traditional Artificial Neural Networks (ANNs) by significantly
improving energy efficiency, particularly in edge and implantable devices.
However, assessing the energy performance of SNN models remains a challenge due
to the lack of standardized and actionable metrics and the difficulty of
measuring energy consumption in experimental neuromorphic hardware. In this
paper, we conduct a preliminary exploratory study of energy efficiency metrics
proposed in the SNN benchmarking literature. We classify 13 commonly used
metrics based on four key properties: Accessibility, Fidelity, Actionability,
and Trend-Based analysis. Our findings indicate that while many existing
metrics provide useful comparisons between architectures, they often lack
practical insights for SNN developers. Notably, we identify a gap between
accessible and high-fidelity metrics, limiting early-stage energy assessment.
Additionally, we emphasize the lack of metrics that provide practitioners with
actionable insights, making it difficult to guide energy-efficient SNN
development. To address these challenges, we outline research directions for
bridging accessibility and fidelity and finding new Actionable metrics for
implantable neuromorphic devices, introducing more Trend-Based metrics, metrics
that reflect changes in power requirements, battery-aware metrics, and
improving energy-performance tradeoff assessments. The results from this paper
pave the way for future research on enhancing energy metrics and their
Actionability for SNNs.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [134] [Formal Methods Meets Readability: Auto-Documenting JML Java Code](https://arxiv.org/abs/2506.09230)
*Juan Carlos Recio Abad,Ruben Saborido,Francisco Chicano*

Main category: cs.SE

TL;DR: 研究JML能否提升LLM生成Javadocs的质量，发现JML显著提高类级文档完整性，对方法级有一定提升，对核心描述质量影响有限。


<details>
  <summary>Details</summary>
Motivation: 探究使用Java建模语言（JML）的形式化规范能否提升大语言模型（LLM）生成的Javadocs的质量。

Method: 对有JML注释和无JML注释的Java类生成的文档进行系统比较，通过自动化指标和专家分析评估质量。

Result: JML显著提高类级文档完整性，方法级有一定提升；能有效捕捉复杂类不变量和设计契约；对核心描述质量影响有限；类的不变量越多，JML优势越明显。

Conclusion: 形式化规范主要确保全面覆盖而非改变实现描述，为软件团队在文档工作流中采用形式方法提供见解，展示了形式方法和LLM可协同提高文档质量。

Abstract: This paper investigates whether formal specifications using Java Modeling
Language (JML) can enhance the quality of Large Language Model (LLM)-generated
Javadocs. While LLMs excel at producing documentation from code alone, we
hypothesize that incorporating formally verified invariants yields more
complete and accurate results. We present a systematic comparison of
documentation generated from JML-annotated and non-annotated Java classes,
evaluating quality through both automated metrics and expert analysis. Our
findings demonstrate that JML significantly improves class-level documentation
completeness, with more moderate gains at the method level. Formal
specifications prove particularly effective in capturing complex class
invariants and design contracts that are frequently overlooked in code-only
documentation. A threshold effect emerges, where the benefits of JML become
more pronounced for classes with richer sets of invariants. While JML enhances
specification coverage, its impact on core descriptive quality is limited,
suggesting that formal specifications primarily ensure comprehensive coverage
rather than fundamentally altering implementation descriptions. These results
offer actionable insights for software teams adopting formal methods in
documentation workflows, highlighting scenarios where JML provides clear
advantages. The study contributes to AI-assisted software documentation
research by demonstrating how formal methods and LLMs can synergistically
improve documentation quality.

</details>


### [135] [Microservices and Real-Time Processing in Retail IT: A Review of Open-Source Toolchains and Deployment Strategies](https://arxiv.org/abs/2506.09938)
*Aaditaa Vashisht,Rekha B S*

Main category: cs.SE

TL;DR: 本文探讨现代事件驱动和微服务架构对零售与金融系统的变革，分析相关技术优势并给出启示。


<details>
  <summary>Details</summary>
Motivation: 在零售行业数字化转型背景下，探索如何利用现代架构提升系统性能以满足行业需求。

Method: 系统回顾近年来学术出版物、技术白皮书和行业报告，综合关键主题和实施策略。

Result: Kafka和Spring Boot有助于构建低延迟、支持实时分析和欺诈检测的应用；MongoDB部署在Kubernetes上可确保库存和交易系统的容错性和高可用性；Kubernetes能自动化微服务部署和扩展。

Conclusion: 研究结果为行业从业者设计可扩展基础设施、研究混合部署模型提供见解，为教育工作者将现代系统架构融入教学提供基础。

Abstract: With the rapid pace of digital transformation, the retail industry is
increasingly depending on real-time, scalable, and resilient systems to manage
financial transactions, analyze customer behavior, and streamline order
processing. This literature review explores how modern event-driven and
microservices-based architectures, particularly those leveraging Apache Kafka,
Spring Boot, MongoDB, and Kubernetes are transforming retail and financial
systems. By systematically reviewing academic publications, technical white
papers, and industry reports from recent years, this study synthesizes key
themes and implementation strategies. The analysis reveals that technologies
like Kafka and Spring Boot are instrumental in building low-latency,
event-driven applications that support real-time analytics and fraud detection,
while MongoDB, when deployed on Kubernetes, ensures fault tolerance and high
availability in inventory and transaction systems. Kubernetes itself plays a
crucial role in automating deployment and scaling of microservices. These
findings provide valuable insights for industry practitioners aiming to design
scalable infrastructures, identify research opportunities in hybrid deployment
models, and offer educators a foundation to integrate modern system
architectures into professional and technical communication training.

</details>


### [136] [UTBoost: Rigorous Evaluation of Coding Agents on SWE-Bench](https://arxiv.org/abs/2506.09289)
*Boxi Yu,Yuxuan Zhu,Pinjia He,Daniel Kang*

Main category: cs.SE

TL;DR: 提出LLM驱动的测试用例生成器UTGenerator和测试用例增强框架UTBoost，修正SWE - Bench测试结果并改变排名。


<details>
  <summary>Details</summary>
Motivation: SWE - Bench中手动编写的测试用例常不足，生成的补丁可能通过测试但未解决潜在问题。

Method: 引入UTGenerator自动分析代码库和依赖生成Python项目测试用例，在此基础上提出UTBoost框架。

Result: 识别36个测试用例不足的任务实例，发现345个原标记为通过的错误补丁，影响SWE - Bench Lite 40.9%和SWE - Bench Verified 24.4%的排行榜条目，分别造成18和11次排名变化。

Conclusion: UTGenerator和UTBoost能有效增强测试用例，修正代码生成评估基准的测试结果。

Abstract: The advent of Large Language Models (LLMs) has spurred the development of
coding agents for real-world code generation. As a widely used benchmark for
evaluating the code generation capabilities of these agents, SWE-Bench uses
real-world problems based on GitHub issues and their corresponding pull
requests. However, the manually written test cases included in these pull
requests are often insufficient, allowing generated patches to pass the tests
without resolving the underlying issue. To address this challenge, we introduce
UTGenerator, an LLM-driven test case generator that automatically analyzes
codebases and dependencies to generate test cases for real-world Python
projects. Building on UTGenerator, we propose UTBoost, a comprehensive
framework for test case augmentation. In our evaluation, we identified 36 task
instances with insufficient test cases and uncovered 345 erroneous patches
incorrectly labeled as passed in the original SWE Bench. These corrections,
impacting 40.9% of SWE-Bench Lite and 24.4% of SWE-Bench Verified leaderboard
entries, yield 18 and 11 ranking changes, respectively.

</details>


### [137] [Assessing the Impact of Refactoring Energy-Inefficient Code Patterns on Software Sustainability: An Industry Case Study](https://arxiv.org/abs/2506.09370)
*Rohit Mehra,Priyavanshi Pathania,Vibhu Saujanya Sharma,Vikrant Kaulgud,Sanjay Podder,Adam P. Burden*

Main category: cs.SE

TL;DR: 随着软件系统普及，软件碳排放增长，用自动化工具识别低效代码模式并重构，案例显示重构后应用可持续性提升，用户月均能耗降29%。


<details>
  <summary>Details</summary>
Motivation: 软件普及使碳排放增加，影响环境可持续性，需从可持续性角度优化软件。

Method: 进行行业案例研究，用自动化软件可持续性评估工具识别大型应用中能源低效代码模式并重构。

Result: 重构后应用可持续性有积极影响，用户月均能耗降低29%。

Conclusion: 采用自动化工具识别和重构能源低效代码模式有助于提升软件的可持续性。

Abstract: Advances in technologies like artificial intelligence and metaverse have led
to a proliferation of software systems in business and everyday life. With this
widespread penetration, the carbon emissions of software are rapidly growing as
well, thereby negatively impacting the long-term sustainability of our
environment. Hence, optimizing software from a sustainability standpoint
becomes more crucial than ever. We believe that the adoption of automated tools
that can identify energy-inefficient patterns in the code and guide appropriate
refactoring can significantly assist in this optimization. In this extended
abstract, we present an industry case study that evaluates the sustainability
impact of refactoring energy-inefficient code patterns identified by automated
software sustainability assessment tools for a large application. Preliminary
results highlight a positive impact on the application's sustainability
post-refactoring, leading to a 29% decrease in per-user per-month energy
consumption.

</details>


### [138] [Reasoning as a Resource: Optimizing Fast and Slow Thinking in Code Generation Models](https://arxiv.org/abs/2506.09396)
*Zongjie Li,Shuai Wang*

Main category: cs.SE

TL;DR: 提出在代码生成模型设计中，将推理深度作为可控资源，优化推理预算可实现精度、延迟和成本的更好权衡。


<details>
  <summary>Details</summary>
Motivation: 改变代码生成模型设计方式，避免推理深度成为提示的附带产物，实现精度、延迟和成本的更好平衡。

Method: 在整个模型生命周期（从合成数据创建、基准测试到实际部署）优化推理预算，对推理进行自适应控制。

Result: 自适应控制推理可丰富监督信号，催生新的多维基准测试，为考虑成本和安全的部署策略提供参考。

Conclusion: 将快速和慢速思考视为可调度的互补模式，可构建必要时深入思考、可能时快速行动的编码代理。

Abstract: This position paper proposes a fundamental shift in designing code generation
models: treating reasoning depth as a controllable resource. Rather than being
an incidental byproduct of prompting, we argue that the trade-off between
rapid, direct answers ("fast thinking") and elaborate, chain-of-thought
deliberation ("slow thinking") must be explicitly managed. We contend that
optimizing reasoning budgets across the entire model lifecycle - from synthetic
data creation and benchmarking to real-world deploymen - can unlock superior
trade-offs among accuracy, latency, and cost. This paper outlines how adaptive
control over reasoning can enrich supervision signals, motivate new
multi-dimensional benchmarks, and inform cost-aware, security-conscious
deployment policies. By viewing fast and slow thinking as complementary modes
to be scheduled, we envision coding agents that think deep when necessary and
act fast when possible.

</details>


### [139] [Automated Synthesis of Formally Verified Multi-Abstraction Function Summaries](https://arxiv.org/abs/2506.09550)
*Fanpeng Yang,Xu Ma,Shuling Wang,Xiong Xu,Qinxiang Cao,Naijun Zhan,Xiaofeng Li,Bin Gu*

Main category: cs.SE

TL;DR: 本文提出结合符号执行、大语言模型和形式验证的框架生成函数摘要，解决C程序函数摘要生成难题并进行实验对比。


<details>
  <summary>Details</summary>
Motivation: 关键遗留代码缺乏形式化规范，C程序函数摘要自动生成困难且需支持多抽象层次。

Method: 提出结合符号执行、大语言模型和形式验证的框架，利用VST - A符号执行、LLMs推断循环不变式、Frama - C保证摘要合理性，从生成的RSPs自动合成最强非冗余后置条件。

Result: 通过广泛实验与现有工作对比。

Conclusion: 未明确提及，但推测所提方法在函数摘要生成上有较好表现。

Abstract: Function summaries, which characterize the behavior of code segments
(typically functions) through preconditions and postconditions, are essential
for understanding, reusing, and verifying software, particularly in
safety-critical domains like aerospace embedded systems. However, these
mission-critical legacy code serving as a valuable reused asset often lacks
formal specifications. It is challenging to automatically generate function
summaries for C programs, due to the existence of complex features such as
loops, nested function calls, pointer aliasing, and so on. Moreover, function
summaries should support multiple abstraction levels to meet diverse
requirements, e.g. precise summaries capturing full functionality for formal
verification and intuitive summaries for human understanding.
  To address these challenges, we first propose a novel framework that combines
symbolic execution, large language models (LLMs), and formal verification to
generate Relatively Strongest Postconditions (RSPs) and build function
summaries that fully capture program behavior. Our approach leverages VST-A's
symbolic execution to precisely track program execution paths and state
transitions, employs LLMs to infer loop invariants based on predefined
templates, and uses Frama-C to guarantee soundness of generated summaries in an
iterative refinement loop. Furthermore, from generated RSPs, we automatically
synthesize strongest non-redundant postconditions expressed within given domain
specific language. We compare our approach with existing work through extensive
experiments.

</details>


### [140] [ASTAGEN: Empirical Evaluation of Automated SATD Taxonomy Generation with LLMs](https://arxiv.org/abs/2506.09601)
*Sota Nakashima,Yuta Ishimoto,Masanari Kondo,Tao Xiao,Yasutaka Kamei*

Main category: cs.SE

TL;DR: 文章介绍了ASTAGEN工具，借助大语言模型自动生成自承认技术债务（SATD）分类法，在三个领域数据集评估中表现良好，支持半自动化分类法构建。


<details>
  <summary>Details</summary>
Motivation: 传统构建SATD分类法的方式耗时、费力且因注释者主观性而不一致，需要自动化方法。

Method: ASTAGEN给定注释及其周围代码，先为每个SATD注释生成简洁解释，再逐步生成和更新类别以构建分类法。

Result: 在三个领域数据集评估中成功恢复特定领域类别，比简单使用大语言模型产生的类别分配更一致，能在两小时内且花费不到一美元完成分类法生成。

Conclusion: 虽然完全自动化仍具挑战，但ASTAGEN能支持半自动化分类法构建，为未来其他领域自动分类法生成开辟了道路。

Abstract: Technical debt refers to suboptimal code that degrades software quality. When
developers intentionally introduce such debt, it is called self-admitted
technical debt (SATD). Since SATD hinders maintenance, identifying its
categories is key to uncovering quality issues. Traditionally, constructing
such taxonomies requires manually inspecting SATD comments and surrounding
code, which is time-consuming, labor-intensive, and often inconsistent due to
annotator subjectivity. This study presents ASTAGEN, an initial step toward
automating SATD taxonomy generation using large language models (LLMs). Given a
comment and its surrounding code, ASTAGEN first generates a concise explanation
for each SATD comment, then incrementally generates and updates categories to
construct a taxonomy. We evaluate ASTAGEN on SATD datasets from three domains:
quantum software, smart contracts, and machine learning. It successfully
recovers domain-specific categories reported in prior work, such as Layer
Configuration in machine learning. Compared to a naive use of an LLM, ASTAGEN
produces more consistent category assignments due to its explanation-driven,
iterative design. It also completes taxonomy generation in under two hours and
for less than one USD, even on the largest dataset. These results suggest that
while full automation remains challenging, ASTAGEN is able to support
semi-automated taxonomy construction. Furthermore, our work opens up avenues
for future work, such as automatic taxonomy generation in other areas.

</details>


### [141] [Translating a VDM Model of a Medical Device into Kapture](https://arxiv.org/abs/2506.09636)
*Joe Hare,Leo Freitas,Ken Pierce*

Main category: cs.SE

TL;DR: 本文探索使用Kapture工具翻译医疗植入物VDM模型，评估其可用性、建模挑战和模型有效性，结果表明无经验用户也能有效使用Kapture建模。


<details>
  <summary>Details</summary>
Motivation: 随着安全关键医疗设备复杂度增加，需要清晰可验证的软件需求，探索使用Kapture工具翻译现有医疗植入物的VDM模型。

Method: 使用D - RisQ开发的Kapture工具，在无形式化方法经验的情况下，对CANDO医疗植入物的VDM模型进行翻译。

Result: 得到的Kapture模型覆盖原VDM模型超90%，结果轨迹匹配，设计和实现中因学习曲线遇到一些问题。

Conclusion: 无经验用户可在Kapture中有效建模复杂系统，指出将VDM规范转换为Kapture存在困难。

Abstract: As the complexity of safety-critical medical devices increases, so does the
need for clear, verifiable, software requirements. This paper explores the use
of Kapture, a formal modelling tool developed by D-RisQ, to translate an
existing formal VDM model of a medical implant for treating focal epilepsy
called CANDO. The work was undertaken without prior experience in formal
methods. The paper assess Kapture's usability, the challenges of formal
modelling, and the effectiveness of the translated model. The result is a model
in Kapture which covers over 90% of the original VDM model, and produces
matching traces of results. While several issues were encountered during design
and implementation, mainly due to the initial learning curve, this paper
demonstrates that complex systems can be effectively modelled in Kapture by
inexperienced users and highlights some difficulties in translating VDM
specifications to Kapture.

</details>


### [142] [Calculating Software's Energy Use and Carbon Emissions: A Survey of the State of Art, Challenges, and the Way Ahead](https://arxiv.org/abs/2506.09683)
*Priyavanshi Pathania,Nikhil Bamby,Rohit Mehra,Samarth Sikand,Vibhu Saujanya Sharma,Vikrant Kaulgud,Sanjay Podder,Adam P. Burden*

Main category: cs.SE

TL;DR: 本文对测量软件和AI能源及碳排放的方法和工具进行了综述，介绍分类法，比较工具并指出挑战，强调社区协作。


<details>
  <summary>Details</summary>
Motivation: 软件和AI的能源与碳足迹问题随其普及而凸显，出于环境可持续性考虑，需理解和优化软件对环境的影响。

Method: 对相关方法和工具进行综述，引入分类法对现有工作分类，从不同维度和粒度比较工具。

Result: 完成对工具的比较，观察到工具的实际使用情况，识别出现有技术的挑战。

Conclusion: 强调在该重要领域社区需积极协作以应对挑战。

Abstract: The proliferation of software and AI comes with a hidden risk: its growing
energy and carbon footprint. As concerns regarding environmental sustainability
come to the forefront, understanding and optimizing how software impacts the
environment becomes paramount. In this paper, we present a state-of-the-art
review of methods and tools that enable the measurement of software and
AI-related energy and/or carbon emissions. We introduce a taxonomy to
categorize the existing work as Monitoring, Estimation, or Black-Box
approaches. We delve deeper into the tools and compare them across different
dimensions and granularity - for example, whether their measurement encompasses
energy and carbon emissions and the components considered (like CPU, GPU, RAM,
etc.). We present our observations on the practical use (component wise
consolidation of approaches) as well as the challenges that we have identified
across the current state-of-the-art. As we start an initiative to address these
challenges, we emphasize active collaboration across the community in this
important field.

</details>


### [143] [Mapping NVD Records to Their VFCs: How Hard is it?](https://arxiv.org/abs/2506.09702)
*Huu Hung Nguyen,Duc Manh Tran,Yiran Cheng,Thanh Le-Cong,Hong Jin Kang,Ratnadira Widyasari,Shar Lwin Khin,Ouh Eng Lieh,Ting Zhang,David Lo*

Main category: cs.SE

TL;DR: 研究探索将NVD记录映射到VFC的可行性，通过多种方法提取VFC，有一定成果但仍有大部分记录未映射。


<details>
  <summary>Details</summary>
Motivation: NVD记录到VFC的映射对漏洞分析至关重要，但NVD引用中显式链接稀疏，需探索映射可行性。

Method: 先手动分析NVD引用，再构建自动化管道，还挖掘外部安全数据库和GitHub仓库。

Result: 自动化管道从NVD记录提取31,942个VFC，精度87%；挖掘外部安全数据库得29,254个VFC，精度88.4%；GitHub仓库添加3,686个VFC，精度73%；共映射26,710条唯一记录，覆盖率11.3%。

Conclusion: 研究为增强漏洞数据集和未来自动化安全研究提供见解，同时指出无Git链接时映射难度大，88.7%记录未映射。

Abstract: Mapping National Vulnerability Database (NVD) records to vulnerability-fixing
commits (VFCs) is crucial for vulnerability analysis but challenging due to
sparse explicit links in NVD references.This study explores this mapping's
feasibility through an empirical approach. Manual analysis of NVD references
showed Git references enable over 86% success, while non-Git references achieve
under 14%. Using these findings, we built an automated pipeline extracting
31,942 VFCs from 20,360 NVD records (8.7% of 235,341) with 87% precision,
mainly from Git references. To fill gaps, we mined six external security
databases, yielding 29,254 VFCs for 18,985 records (8.1%) at 88.4% precision,
and GitHub repositories, adding 3,686 VFCs for 2,795 records (1.2%) at 73%
precision. Combining these, we mapped 26,710 unique records (11.3% coverage)
from 7,634 projects, with overlap between NVD and external databases, plus
unique GitHub contributions. Despite success with Git references, 88.7% of
records remain unmapped, highlighting the difficulty without Git links. This
study offers insights for enhancing vulnerability datasets and guiding future
automated security research.

</details>


### [144] [A First Look at Bugs in LLM Inference Engines](https://arxiv.org/abs/2506.09713)
*Mugeng Liu,Siqi Zhong,Weichen Bi,Yixuan Zhang,Zhiyang Chen,Zhenpeng Chen,Xuanzhe Liu,Yun Ma*

Main category: cs.SE

TL;DR: 本文首次对大语言模型推理引擎的漏洞进行实证研究，挖掘5个引擎官方仓库构建929个真实漏洞数据集，分析症状和根源，提出相关建议。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理引擎易出现漏洞，但缺乏对这些漏洞的系统理解，需要开展研究。

Method: 挖掘5个广泛使用的大语言模型推理引擎的官方仓库，构建真实漏洞数据集，通过严格的开放式编码过程分析漏洞。

Result: 发现6种主要漏洞症状和28种根源的分类，揭示了大语言模型推理引擎漏洞检测和定位的关键挑战。

Conclusion: 基于研究结果，为研究人员、推理引擎供应商和大语言模型应用开发者提出了一系列可行的建议。

Abstract: Large language model-specific inference engines (in short as \emph{LLM
inference engines}) have become a fundamental component of modern AI
infrastructure, enabling the deployment of LLM-powered applications (LLM apps)
across cloud and local devices. Despite their critical role, LLM inference
engines are prone to bugs due to the immense resource demands of LLMs and the
complexities of cross-platform compatibility. However, a systematic
understanding of these bugs remains lacking. To bridge this gap, we present the
first empirical study on bugs in LLM inference engines. We mine official
repositories of 5 widely adopted LLM inference engines, constructing a
comprehensive dataset of 929 real-world bugs. Through a rigorous open coding
process, we analyze these bugs to uncover their symptoms, root causes, and
commonality. Our findings reveal six major bug symptoms and a taxonomy of 28
root causes, shedding light on the key challenges in bug detection and location
within LLM inference engines. Based on these insights, we propose a series of
actionable implications for researchers, inference engine vendors, and LLM app
developers.

</details>


### [145] [Towards Bridging Formal Methods and Human Interpretability](https://arxiv.org/abs/2506.09759)
*Abhijit Paul,Proma Chowdhury,Kazi Sakib*

Main category: cs.SE

TL;DR: 本文研究人类对标记转移系统（LTS）设计的理解，确定7个关键指标，发现部分指标能准确反映理解情况，应用指标可提升设计可解释性。


<details>
  <summary>Details</summary>
Motivation: 以往研究未考察人类对LTS设计的理解，为填补该空白开展研究。

Method: 借鉴软件工程和图论确定7个指标，创建148个LTS设计数据集，抽样进行成对比较并用Bradley - Terry模型排名，通过Kendall's Tau相关分析评估指标。

Result: Albin复杂度、状态空间大小、圈复杂度和冗余度能最准确反映人类对LTS设计的理解。将Albin复杂度应用于Fortis工具，减少了39%的理解时间。

Conclusion: 强调人为因素的指标可以提高形式化设计的可解释性。

Abstract: Labeled Transition Systems (LTS) are integral to model checking and design
repair tools. System engineers frequently examine LTS designs during model
checking or design repair to debug, identify inconsistencies, and validate
system behavior. Despite LTS's significance, no prior research has examined
human comprehension of these designs. To address this, we draw on traditional
software engineering and graph theory, identifying 7 key metrics: cyclomatic
complexity, state space size, average branching factor, maximum depth, Albin
complexity, modularity, and redundancy. We created a dataset of 148 LTS
designs, sampling 48 for 324 paired comparisons, and ranked them using the
Bradley-Terry model. Through Kendall's Tau correlation analysis, we found that
Albin complexity ($\tau = 0.444$), state space size ($\tau = 0.420$),
cyclomatic complexity ($\tau = 0.366$), and redundancy ($\tau = 0.315$) most
accurately reflect human comprehension of LTS designs. To showcase the metrics'
utility, we applied the Albin complexity metric within the Fortis design repair
tool, ranking system redesigns. This ranking reduced annotators' comprehension
time by 39\%, suggesting that metrics emphasizing human factors can enhance
formal design interpretability.

</details>


### [146] [variability.dev: Towards an Online Toolbox for Feature Modeling](https://arxiv.org/abs/2506.09845)
*Tobias Heß,Lukas Ostheimer,Tobias Betz,Simon Karrer,Tim Jannik Schmidt,Pierre Coquet,Sean Semmler,Thomas Thüm*

Main category: cs.SE

TL;DR: 介绍正在开发的在线特征建模工具 variability.dev，展示基于 FeatureIDE 库构建的协作式特征模型编辑器和在线配置器。


<details>
  <summary>Details</summary>
Motivation: 现有在线特征模型编辑器功能少、未维护或需离线安装，不能满足建模者需求。

Method: 基于 FeatureIDE 库开发在线工具 variability.dev。

Result: 展示了正在开发的在线工具 variability.dev 中的协作式特征模型编辑器和在线配置器。

Conclusion: 提出了开发新的在线特征建模工具以解决现有工具问题的方案。

Abstract: The emergence of feature models as the default to model the variability in
configurable systems fosters a rich diversity in applications, application
domains, and perspectives. Independent of their domain, modelers require to
open, view, edit, transform, save, and configure models as well as to
collaborate with others. However, at the time of writing, the top five results
when googling ``Online Editor Feature Model'' point to editors that either have
minimal functionality, are unmaintained or defunct, or require an offline
installation, such as FeatureIDE. In this work we present a preview of our
in-development online toolbox for feature modeling, variability.dev. In
particular, we showcase our collaborative feature-model editor and our online
configurator both of which are built on top of the FeatureIDE library.

</details>


### [147] [Stakeholder Participation for Responsible AI Development: Disconnects Between Guidance and Current Practice](https://arxiv.org/abs/2506.09873)
*Emma Kallina,Thomas Bohné,Jat Singh*

Main category: cs.SE

TL;DR: 研究明确既定利益相关者参与（SHI）实践对负责任AI（rAI）的贡献及脱节，提出干预措施和研究机会。


<details>
  <summary>Details</summary>
Motivation: 明确既定SHI实践对rAI的贡献程度及潜在脱节，为推动行业向rAI发展的干预措施提供依据。

Method: 分析56份rAI指导文件，对130名AI从业者进行在线调查，对10名从业者进行半结构化访谈。

Result: 实践中的SHI主要由商业优先级驱动，多种因素阻碍了与rAI更一致的SHI实践，既定SHI实践大多未对rAI做出贡献。

Conclusion: 提出干预措施和研究机会以推动rAI的实际发展。

Abstract: Responsible AI (rAI) guidance increasingly promotes stakeholder involvement
(SHI) during AI development. At the same time, SHI is already common in
commercial software development, but with potentially different foci. This
study clarifies the extent to which established SHI practices are able to
contribute to rAI efforts as well as potential disconnects -- essential
insights to inform and tailor future interventions that further shift industry
practice towards rAI efforts. First, we analysed 56 rAI guidance documents to
identify why SHI is recommended (i.e. its expected benefits for rAI) and
uncovered goals such as redistributing power, improving socio-technical
understandings, anticipating risks, and enhancing public oversight. To
understand why and how SHI is currently practised in commercial settings, we
then conducted an online survey (n=130) and semi-structured interviews (n=10)
with AI practitioners. Our findings reveal that SHI in practice is primarily
driven by commercial priorities (e.g. customer value, compliance) and several
factors currently discourage more rAI-aligned SHI practices. This suggests that
established SHI practices are largely not contributing to rAI efforts. To
address this disconnect, we propose interventions and research opportunities to
advance rAI development in practice.

</details>


### [148] [Assessing a Safety Case: Bottom-up Guidance for Claims and Evidence Evaluation](https://arxiv.org/abs/2506.09929)
*Scott Schnelle,Francesca Favaro,Laura Fraade-Blanar,David Wichner,Holland Broce,Justin Miranda*

Main category: cs.SE

TL;DR: 本文探讨评估自动驾驶系统安全案例可信度的方法，包括评估声明支持和证据状态，提供评分策略和指南，为综合评估提供起点。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶系统技术发展，需要强大的保障框架确保安全和公众信任，安全案例是关键工具，本文旨在评估安全案例的可信度。

Method: 从安全案例的构成要素出发，评估每个声明的程序性支持和实施性支持，独立评估证据状态，提供评分策略和评估指南。

Result: 提供了评分策略和评估指南，包括声明支持和证据状态评估的详细评分表，讨论了治理、持续改进和时间考虑等问题。

Conclusion: 本文方法虽不具有决定性，但为全面的案例可信度评估提供了起点，有助于自动驾驶系统安全保障的讨论和技术的负责任集成。

Abstract: As Automated Driving Systems (ADS) technology advances, ensuring safety and
public trust requires robust assurance frameworks, with safety cases emerging
as a critical tool toward such a goal. This paper explores an approach to
assess how a safety case is supported by its claims and evidence, toward
establishing credibility for the overall case. Starting from a description of
the building blocks of a safety case (claims, evidence, and optional
format-dependent entries), this paper delves into the assessment of support of
each claim through the provided evidence. Two domains of assessment are
outlined for each claim: procedural support (formalizing process specification)
and implementation support (demonstrating process application). Additionally,
an assessment of evidence status is also undertaken, independently from the
claims support. Scoring strategies and evaluation guidelines are provided,
including detailed scoring tables for claim support and evidence status
assessment. The paper further discusses governance, continual improvement, and
timing considerations for safety case assessments. Reporting of results and
findings is contextualized within its primary use for internal decision-making
on continual improvement efforts. The presented approach builds on state of the
art auditing practices, but specifically tackles the question of judging the
credibility of a safety case. While not conclusive on its own, it provides a
starting point toward a comprehensive "Case Credibility Assessment" (CCA),
starting from the evaluation of the support for each claim (individually and in
aggregate), as well as every piece of evidence provided. By delving into the
technical intricacies of ADS safety cases, this work contributes to the ongoing
discourse on safety assurance and aims to facilitate the responsible
integration of ADS technology into society.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [149] [Advancing Exchange Rate Forecasting: Leveraging Machine Learning and AI for Enhanced Accuracy in Global Financial Markets](https://arxiv.org/abs/2506.09851)
*Md. Yeasin Rahat,Rajan Das Gupta,Nur Raisa Rahman,Sudipto Roy Pritom,Samiur Rahman Shakir,Md Imrul Hasan Showmick,Md. Jakir Hossen*

Main category: q-fin.ST

TL;DR: 本文利用2018 - 2023年美元/孟加拉塔卡汇率数据，构建LSTM和GBC模型进行外汇预测，LSTM精度高，GBC有一定盈利比例但有净亏损，凸显深度学习在外汇预测潜力并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 外汇汇率预测对全球金融市场至关重要，影响贸易、投资和经济稳定，需准确预测模型。

Method: 利用雅虎财经2018 - 2023年美元/孟加拉塔卡汇率数据，构建LSTM神经网络进行预测，应用GBC进行方向预测，分析历史趋势并纳入归一化日回报。

Result: LSTM精度达99.449%，RMSE为0.9858，测试损失0.8523，优于ARIMA；GBC盈利交易率40.82%，49笔交易净亏损20653.25美元；BDT/USD汇率从0.012降至0.009。

Conclusion: 深度学习在外汇预测有潜力，可为交易员和政策制定者提供风险管理工具，未来可整合情绪分析和实时经济指标提高模型适应性。

Abstract: The prediction of foreign exchange rates, such as the US Dollar (USD) to
Bangladeshi Taka (BDT), plays a pivotal role in global financial markets,
influencing trade, investments, and economic stability. This study leverages
historical USD/BDT exchange rate data from 2018 to 2023, sourced from Yahoo
Finance, to develop advanced machine learning models for accurate forecasting.
A Long Short-Term Memory (LSTM) neural network is employed, achieving an
exceptional accuracy of 99.449%, a Root Mean Square Error (RMSE) of 0.9858, and
a test loss of 0.8523, significantly outperforming traditional methods like
ARIMA (RMSE 1.342). Additionally, a Gradient Boosting Classifier (GBC) is
applied for directional prediction, with backtesting on a $10,000 initial
capital revealing a 40.82% profitable trade rate, though resulting in a net
loss of $20,653.25 over 49 trades. The study analyzes historical trends,
showing a decline in BDT/USD rates from 0.012 to 0.009, and incorporates
normalized daily returns to capture volatility. These findings highlight the
potential of deep learning in forex forecasting, offering traders and
policymakers robust tools to mitigate risks. Future work could integrate
sentiment analysis and real-time economic indicators to further enhance model
adaptability in volatile markets.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [150] [Know What You Don't Know: Uncertainty Calibration of Process Reward Models](https://arxiv.org/abs/2506.09338)
*Young-Jin Park,Kristjan Greenewald,Kaveh Alim,Hao Wang,Navid Azizan*

Main category: stat.ML

TL;DR: 本文提出对过程奖励模型（PRM）的校准方法及实例自适应缩放（IAS）框架，实验表明校准方法效果好，校准对自适应缩放很重要，IAS策略可降低推理成本并保持准确率。


<details>
  <summary>Details</summary>
Motivation: 现有最先进的PRM校准不佳且常高估成功概率，需要改进。

Method: 通过分位数回归进行校准，基于校准的成功估计和置信区间引入IAS框架动态调整推理预算。

Result: PRM校准方法校准误差小，优于基线方法；校准对有效自适应缩放至关重要；IAS策略降低推理成本并保持准确率。

Conclusion: 所提出的校准方法和IAS框架能有效改善LLM推理过程，降低成本并保证准确率。

Abstract: Process reward models (PRMs) play a central role in guiding inference-time
scaling algorithms for large language models (LLMs). However, we observe that
even state-of-the-art PRMs can be poorly calibrated and often overestimate
success probabilities. To address this, we present a calibration approach,
performed via quantile regression, that adjusts PRM outputs to better align
with true success probabilities. Leveraging these calibrated success estimates
and their associated confidence bounds, we introduce an \emph{instance-adaptive
scaling} (IAS) framework that dynamically adjusts the inference budget based on
the estimated likelihood that a partial reasoning trajectory will yield a
correct final answer. Unlike conventional methods that allocate a fixed number
of reasoning trajectories per query, this approach successfully adapts to each
instance and reasoning step when using our calibrated PRMs. Experiments on
mathematical reasoning benchmarks show that (i) our PRM calibration method
successfully achieves small calibration error, outperforming the baseline
methods, (ii) calibration is crucial for enabling effective adaptive scaling,
and (iii) the proposed IAS strategy reduces inference costs while maintaining
final answer accuracy, utilizing less compute on more confident problems as
desired.

</details>


### [151] [Attention-Bayesian Hybrid Approach to Modular Multiple Particle Tracking](https://arxiv.org/abs/2506.09441)
*Piyush Mishra,Philippe Roudot*

Main category: stat.ML

TL;DR: 提出混合跟踪框架解决多粒子跟踪难题，提升跟踪精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有transformer架构在轨迹假设集较少场景性能不如传统贝叶斯滤波方法，为结合二者优势提出混合跟踪框架。

Method: 结合自注意力学习粒子行为的潜在表示和贝叶斯滤波的可靠性与可解释性，通过解决标签预测问题进行轨迹到检测的关联，用transformer编码器推断帧间检测的软关联来修剪假设集。

Result: 所提方法提高了跟踪精度，增强了对虚假检测的鲁棒性。

Conclusion: 该方法为高杂波多粒子跟踪场景提供了解决方案。

Abstract: Tracking multiple particles in noisy and cluttered scenes remains challenging
due to a combinatorial explosion of trajectory hypotheses, which scales
super-exponentially with the number of particles and frames. The transformer
architecture has shown a significant improvement in robustness against this
high combinatorial load. However, its performance still falls short of the
conventional Bayesian filtering approaches in scenarios presenting a reduced
set of trajectory hypothesis. This suggests that while transformers excel at
narrowing down possible associations, they may not be able to reach the
optimality of the Bayesian approach in locally sparse scenario. Hence, we
introduce a hybrid tracking framework that combines the ability of
self-attention to learn the underlying representation of particle behavior with
the reliability and interpretability of Bayesian filtering. We perform
trajectory-to-detection association by solving a label prediction problem,
using a transformer encoder to infer soft associations between detections
across frames. This prunes the hypothesis set, enabling efficient
multiple-particle tracking in Bayesian filtering framework. Our approach
demonstrates improved tracking accuracy and robustness against spurious
detections, offering a solution for high clutter multiple particle tracking
scenarios.

</details>


### [152] [LLM-Powered CPI Prediction Inference with Online Text Time Series](https://arxiv.org/abs/2506.09516)
*Yingying Fan,Jinchi Lv,Ao Sun,Yurou Wang*

Main category: stat.ML

TL;DR: 本文提出基于大语言模型的LLM - CPI方法用于CPI预测，结合在线文本时间序列，经模拟和真实数据验证其性能和优势。


<details>
  <summary>Details</summary>
Motivation: 现有CPI预测方法多依赖低频调查数据，利用高频在线文本数据进行CPI预测有潜力且未充分探索。

Method: 收集高频在线文本，用ChatGPT和BERT构建通胀标签，提取文本嵌入，开发结合月度CPI数据和每日CPI替代值的联合时间序列框架，包括ARX结构的月度模型和VARX结构的每日模型。

Result: 建立方法的渐近性质，给出两种预测区间，通过模拟和真实数据展示了LLM - CPI在有限样本下的性能和实际优势。

Conclusion: 基于大语言模型结合在线文本时间序列的LLM - CPI方法在CPI预测中有良好表现和实用价值。

Abstract: Forecasting the Consumer Price Index (CPI) is an important yet challenging
task in economics, where most existing approaches rely on low-frequency,
survey-based data. With the recent advances of large language models (LLMs),
there is growing potential to leverage high-frequency online text data for
improved CPI prediction, an area still largely unexplored. This paper proposes
LLM-CPI, an LLM-based approach for CPI prediction inference incorporating
online text time series. We collect a large set of high-frequency online texts
from a popularly used Chinese social network site and employ LLMs such as
ChatGPT and the trained BERT models to construct continuous inflation labels
for posts that are related to inflation. Online text embeddings are extracted
via LDA and BERT. We develop a joint time series framework that combines
monthly CPI data with LLM-generated daily CPI surrogates. The monthly model
employs an ARX structure combining observed CPI data with text embeddings and
macroeconomic variables, while the daily model uses a VARX structure built on
LLM-generated CPI surrogates and text embeddings. We establish the asymptotic
properties of the method and provide two forms of constructed prediction
intervals. The finite-sample performance and practical advantages of LLM-CPI
are demonstrated through both simulation and real data examples.

</details>


### [153] [Evasion Attacks Against Bayesian Predictive Models](https://arxiv.org/abs/2506.09640)
*Pablo G. Arce,Roi Naveiro,David Ríos Insua*

Main category: stat.ML

TL;DR: 本文提出针对贝叶斯预测模型的最优逃避攻击通用方法，研究两种对抗目标及梯度攻击方法。


<details>
  <summary>Details</summary>
Motivation: 现有对抗机器学习研究多聚焦经典场景下预测模型的逃避或中毒攻击弱点，贝叶斯预测模型的攻击易感性研究不足。

Method: 提出针对贝叶斯预测模型设计最优逃避攻击的通用方法，研究两种对抗目标，提出基于梯度的攻击方法并研究其在不同计算场景的实现和特性。

Result: 未提及明确具体结果。

Conclusion: 未提及明确结论。

Abstract: There is an increasing interest in analyzing the behavior of machine learning
systems against adversarial attacks. However, most of the research in
adversarial machine learning has focused on studying weaknesses against evasion
or poisoning attacks to predictive models in classical setups, with the
susceptibility of Bayesian predictive models to attacks remaining
underexplored. This paper introduces a general methodology for designing
optimal evasion attacks against such models. We investigate two adversarial
objectives: perturbing specific point predictions and altering the entire
posterior predictive distribution. For both scenarios, we propose novel
gradient-based attacks and study their implementation and properties in various
computational setups.

</details>


### [154] [Scaling Laws for Uncertainty in Deep Learning](https://arxiv.org/abs/2506.09648)
*Mattia Rosso,Simone Rossi,Giulio Franzese,Markus Heinonen,Maurizio Filippone*

Main category: stat.ML

TL;DR: 研究深度学习中预测不确定性是否存在缩放定律，通过实验证实存在，并反驳对贝叶斯方法的质疑。


<details>
  <summary>Details</summary>
Motivation: 受深度学习缩放定律启发，探究预测不确定性是否存在类似缩放定律，且在过参数化模型中相关研究少。

Method: 通过视觉和语言任务实验，用流行的近似贝叶斯推理和集成方法估计分布内和分布外的预测不确定性。

Result: 实证表明预测不确定性关于数据集和模型大小存在缩放定律。

Conclusion: 缩放定律有理论美和实际用途，且证明大量数据通常不足以使认知不确定性忽略不计，反驳了对贝叶斯方法的质疑。

Abstract: Deep learning has recently revealed the existence of scaling laws,
demonstrating that model performance follows predictable trends based on
dataset and model sizes. Inspired by these findings and fascinating phenomena
emerging in the over-parameterized regime, we examine a parallel direction: do
similar scaling laws govern predictive uncertainties in deep learning? In
identifiable parametric models, such scaling laws can be derived in a
straightforward manner by treating model parameters in a Bayesian way. In this
case, for example, we obtain $O(1/N)$ contraction rates for epistemic
uncertainty with respect to the number of data $N$. However, in
over-parameterized models, these guarantees do not hold, leading to largely
unexplored behaviors. In this work, we empirically show the existence of
scaling laws associated with various measures of predictive uncertainty with
respect to dataset and model sizes. Through experiments on vision and language
tasks, we observe such scaling laws for in- and out-of-distribution predictive
uncertainty estimated through popular approximate Bayesian inference and
ensemble methods. Besides the elegance of scaling laws and the practical
utility of extrapolating uncertainties to larger data or models, this work
provides strong evidence to dispel recurring skepticism against Bayesian
approaches: "In many applications of deep learning we have so much data
available: what do we need Bayes for?". Our findings show that "so much data"
is typically not enough to make epistemic uncertainty negligible.

</details>


### [155] [Assessing the Quality of Denoising Diffusion Models in Wasserstein Distance: Noisy Score and Optimal Bounds](https://arxiv.org/abs/2506.09681)
*Vahan Arsenyan,Elen Vardanyan,Arnak Dalalyan*

Main category: stat.ML

TL;DR: 本文研究去噪扩散概率模型（DDPMs），证明其对分数评估中恒定方差噪声具有鲁棒性，建立了Wasserstein - 2距离下的有限样本保证，有更快收敛率且达到最优。


<details>
  <summary>Details</summary>
Motivation: 研究生成建模中DDPMs在分数评估存在噪声情况下的特性和性能。

Method: 先提供DDPMs对恒定方差噪声鲁棒性的实证证据，再建立Wasserstein - 2距离下的有限样本保证。

Result: 证明DDPMs对恒定方差噪声鲁棒，建立的有限样本保证有两个关键特征，收敛率比已知结果更快，且与高斯情况下的已知结果匹配。

Conclusion: DDPMs对噪声分数估计具有鲁棒性，所获收敛率是最优的。

Abstract: Generative modeling aims to produce new random examples from an unknown
target distribution, given access to a finite collection of examples. Among the
leading approaches, denoising diffusion probabilistic models (DDPMs) construct
such examples by mapping a Brownian motion via a diffusion process driven by an
estimated score function. In this work, we first provide empirical evidence
that DDPMs are robust to constant-variance noise in the score evaluations. We
then establish finite-sample guarantees in Wasserstein-2 distance that exhibit
two key features: (i) they characterize and quantify the robustness of DDPMs to
noisy score estimates, and (ii) they achieve faster convergence rates than
previously known results. Furthermore, we observe that the obtained rates match
those known in the Gaussian case, implying their optimality.

</details>


### [156] [A Deep Generative Model for the Simulation of Discrete Karst Networks](https://arxiv.org/abs/2506.09832)
*Dany Lauzon,Julien Straubhaar,Philippe Renard*

Main category: stat.ML

TL;DR: 本文提出将岩溶网络表示为图并应用图生成模型模拟离散岩溶网络的新方法，通过两个步骤生成图，经实际数据测试，该方法可随机模拟不同类型地层的离散岩溶网络。


<details>
  <summary>Details</summary>
Motivation: 由于地质和水文地质背景下物理化学过程复杂，导致岩溶网络模式多样，模拟离散岩溶网络面临挑战，因此探索新的模拟方法。

Method: 将岩溶网络表示为图，运用图生成模型，包括使用图递归神经网络学习拓扑分布，采用图去噪扩散概率模型学习节点特征。

Result: 使用实际岩溶网络测试，通过几何和拓扑指标比较生成子图和实际子图。

Conclusion: 该方法可对不同类型地层的离散岩溶网络进行随机模拟，是研究水流和输运等物理过程行为的有用工具。

Abstract: The simulation of discrete karst networks presents a significant challenge
due to the complexity of the physicochemical processes occurring within various
geological and hydrogeological contexts over extended periods. This complex
interplay leads to a wide variety of karst network patterns, each intricately
linked to specific hydrogeological conditions. We explore a novel approach that
represents karst networks as graphs and applies graph generative models (deep
learning techniques) to capture the intricate nature of karst environments. In
this representation, nodes retain spatial information and properties, while
edges signify connections between nodes. Our generative process consists of two
main steps. First, we utilize graph recurrent neural networks (GraphRNN) to
learn the topological distribution of karst networks. GraphRNN decomposes the
graph simulation into a sequential generation of nodes and edges, informed by
previously generated structures. Second, we employ denoising diffusion
probabilistic models on graphs (G-DDPM) to learn node features (spatial
coordinates and other properties). G-DDPMs enable the generation of nodes
features on the graphs produced by the GraphRNN that adhere to the learned
statistical properties by sampling from the derived probability distribution,
ensuring that the generated graphs are realistic and capture the essential
features of the original data. We test our approach using real-world karst
networks and compare generated subgraphs with actual subgraphs from the
database, by using geometry and topology metrics. Our methodology allows
stochastic simulation of discrete karst networks across various types of
formations, a useful tool for studying the behavior of physical processes such
as flow and transport.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [157] [Variational inference for steady-state BVARs](https://arxiv.org/abs/2506.09271)
*Oskar Gustafsson,Mattias Villani*

Main category: stat.CO

TL;DR: 提出快速变分推理（VI）算法用于稳态贝叶斯向量自回归（BVAR），结果接近吉布斯采样且计算时间大幅减少。


<details>
  <summary>Details</summary>
Motivation: 稳态BVAR采用吉布斯采样估计耗时，尤其对于大规模BVAR模型，需更高效算法。

Method: 提出快速变分推理（VI）算法来近似稳态BVAR的参数后验和预测分布，以及用于模型比较的对数预测分数。

Result: 使用模拟和真实美国宏观经济数据表明，VI结果与吉布斯采样非常接近，且计算时间大幅减少，尤其对于对数预测分数，且随系统中时间序列数量增加扩展性更好。

Conclusion: 变分推理算法可作为稳态BVAR参数估计和预测的有效替代方法，能提高计算效率。

Abstract: The steady-state Bayesian vector autoregression (BVAR) makes it possible to
incorporate prior information about the long-run mean of the process. This has
been shown in many studies to substantially improve forecasting performance,
and the model is routinely used for forecasting and macroeconomic policy
analysis at central banks and other financial institutions. Steady-steady BVARs
are estimated using Gibbs sampling, which is time-consuming for the
increasingly popular large-scale BVAR models with many variables. We propose a
fast variational inference (VI) algorithm for approximating the parameter
posterior and predictive distribution of the steady-state BVAR, as well as log
predictive scores for model comparison. We use simulated and real US
macroeconomic data to show that VI produces results that are very close to
those from Gibbs sampling. The computing time of VI can be orders of magnitude
lower than Gibbs sampling, in particular for log predictive scores, and VI is
shown to scale much better with the number of time series in the system.

</details>


### [158] [Parallel computations for Metropolis Markov chains with Picard maps](https://arxiv.org/abs/2506.09762)
*Sebastiano Grazzi,Giacomo Zanella*

Main category: stat.CO

TL;DR: 开发基于Picard映射的零阶Metropolis马尔可夫链并行算法，可加速采样，在高维问题中实证评估效果良好。


<details>
  <summary>Details</summary>
Motivation: 开发可加速采样的并行算法，适用于梯度不可用情况。

Method: 基于Picard映射开发并行算法，有针对不同情况的实现方式。

Result: 对目标分布采样，能在一定迭代次数和处理器数量下完成，加速收敛；实证评估在高维回归和流行病模型中表现良好。

Conclusion: 算法易于实现，是仅用逐点评估和并行计算从指定分布采样的有用工具。

Abstract: We develop parallel algorithms for simulating zeroth-order (aka
gradient-free) Metropolis Markov chains based on the Picard map. For Random
Walk Metropolis Markov chains targeting log-concave distributions $\pi$ on
$\mathbb{R}^d$, our algorithm generates samples close to $\pi$ in
$\mathcal{O}(\sqrt{d})$ parallel iterations with $\mathcal{O}(\sqrt{d})$
processors, therefore speeding up the convergence of the corresponding
sequential implementation by a factor $\sqrt{d}$. Furthermore, a modification
of our algorithm generates samples from an approximate measure $ \pi_\epsilon$
in $\mathcal{O}(1)$ parallel iterations and $\mathcal{O}(d)$ processors. We
empirically assess the performance of the proposed algorithms in
high-dimensional regression problems and an epidemic model where the gradient
is unavailable. Our algorithms are straightforward to implement and may
constitute a useful tool for practitioners seeking to sample from a prescribed
distribution $\pi$ using only point-wise evaluations of $\log\pi$ and parallel
computing.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [159] [Linking Data Citation to Repository Visibility: An Empirical Study](https://arxiv.org/abs/2506.09530)
*Fakhri Momeni,Janete Saldanha Bach,Brigitte Mathiak,Peter Mutschke*

Main category: cs.DL

TL;DR: 研究探讨仓库可见性对数据引用率的影响，发现可见性虽有作用但非唯一影响因素。


<details>
  <summary>Details</summary>
Motivation: 在数据驱动研究中，数据集可见性、可访问性及数据引用很重要，研究探究仓库可见性是否影响数据引用率。

Method: 使用OpenAlex数据和仓库影响指标，分析社会科学和经济学领域的数据集。

Result: 更可见的网络域名上的数据集引用更多，域名可见性与引用计数正相关，但域名层面引用指标相关性不一致且较弱，引用分布差异大。

Conclusion: 可见性对增加引用计数有作用，但非影响数据集引用影响的唯一因素，数据集质量、研究趋势和学科规范等也有重要贡献。

Abstract: In today's data-driven research landscape, dataset visibility and
accessibility play a crucial role in advancing scientific knowledge. At the
same time, data citation is essential for maintaining academic integrity,
acknowledging contributions, validating research outcomes, and fostering
scientific reproducibility. As a critical link, it connects scholarly
publications with the datasets that drive scientific progress. This study
investigates whether repository visibility influences data citation rates. We
hypothesize that repositories with higher visibility, as measured by search
engine metrics, are associated with increased dataset citations. Using OpenAlex
data and repository impact indicators (including the visibility index from
Sistrix, the h-index of repositories, and citation metrics such as mean and
median citations), we analyze datasets in Social Sciences and Economics to
explore their relationship. Our findings suggest that datasets hosted on more
visible web domains tend to receive more citations, with a positive correlation
observed between web domain visibility and dataset citation counts,
particularly for datasets with at least one citation. However, when analyzing
domain-level citation metrics, such as the h-index, mean, and median citations,
the correlations are inconsistent and weaker. While higher visibility domains
tend to host datasets with greater citation impact, the distribution of
citations across datasets varies significantly. These results suggest that
while visibility plays a role in increasing citation counts, it is not the sole
factor influencing dataset citation impact. Other elements, such as dataset
quality, research trends, and disciplinary norms, also contribute significantly
to citation patterns.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [160] [Almost-Optimal Local-Search Methods for Sparse Tensor PCA](https://arxiv.org/abs/2506.09959)
*Max Lovig,Conor Sheehan,Konstantinos Tsirkas,Ilias Zadik*

Main category: math.ST

TL;DR: 本文提出一系列局部搜索方法，缩小了局部马尔可夫链方法在稀疏张量主成分分析中的性能差距。


<details>
  <summary>Details</summary>
Motivation: 局部搜索方法在统计应用中广泛使用，但理论基础研究不足，且在稀疏张量主成分分析中存在局部计算差距。

Method: 提出标准贪心和随机贪心算法应用于模型后验，以及引入随机阈值变体。

Result: 所提方法在多个模型状态下可缩小与已知多项式时间程序的差距。

Conclusion: 引入随机阈值有助于对随机贪心算法轨迹进行数学分析，有独立研究价值。

Abstract: Local-search methods are widely employed in statistical applications, yet
interestingly, their theoretical foundations remain rather underexplored,
compared to other classes of estimators such as low-degree polynomials and
spectral methods. Of note, among the few existing results recent studies have
revealed a significant "local-computational" gap in the context of a
well-studied sparse tensor principal component analysis (PCA), where a broad
class of local Markov chain methods exhibits a notable underperformance
relative to other polynomial-time algorithms. In this work, we propose a series
of local-search methods that provably "close" this gap to the best known
polynomial-time procedures in multiple regimes of the model, including and
going beyond the previously studied regimes in which the broad family of local
Markov chain methods underperforms. Our framework includes: (1) standard greedy
and randomized greedy algorithms applied to the (regularized) posterior of the
model; and (2) novel random-threshold variants, in which the randomized greedy
algorithm accepts a proposed transition if and only if the corresponding change
in the Hamiltonian exceeds a random Gaussian threshold-rather that if and only
if it is positive, as is customary. The introduction of the random thresholds
enables a tight mathematical analysis of the randomized greedy algorithm's
trajectory by crucially breaking the dependencies between the iterations, and
could be of independent interest to the community.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [161] [Surrogate models to optimize plasma assisted atomic layer deposition in high aspect ratio features](https://arxiv.org/abs/2506.09313)
*Angel Yanguas-Gil,Jeffrey W. Elam*

Main category: cond-mat.mtrl-sci

TL;DR: 本文探索替代模型优化高纵横比特征的等离子体增强原子层沉积（PEALD），利用模拟数据集训练神经网络预测饱和时间，证明机器学习可加速PEALD优化，且方法可扩展。


<details>
  <summary>Details</summary>
Motivation: 在PEALD等基于等离子体的过程中，表面复合会主导等离子体与表面的反应性，导致实现纳米结构内完全保形的曝光时间过长，需要优化PEALD。

Method: 使用基于PEALD模拟的合成数据集，训练人工神经网络，根据部分涂层条件下的横截面厚度数据预测饱和时间；训练替代模型判断表面复合是否主导PEALD过程中的等离子体 - 表面相互作用。

Result: 仅两次欠饱和条件实验的数据就能使预测饱和时间与真实值误差在10%以内；判断表面复合主导作用的替代模型准确率达99%。

Conclusion: 机器学习为加速PEALD过程优化提供新途径，该方法可扩展到原子层蚀刻和更复杂结构。

Abstract: In this work we explore surrogate models to optimize plasma enhanced atomic
layer deposition (PEALD) in high aspect ratio features. In plasma-based
processes such as PEALD and atomic layer etching, surface recombination can
dominate the reactivity of plasma species with the surface, which can lead to
unfeasibly long exposure times to achieve full conformality inside
nanostructures like high aspect ratio vias. Using a synthetic dataset based on
simulations of PEALD, we train artificial neural networks to predict saturation
times based on cross section thickness data obtained for partially coated
conditions. The results obtained show that just two experiments in
undersaturated conditions contain enough information to predict saturation
times within 10% of the ground truth. A surrogate model trained to determine
whether surface recombination dominates the plasma-surface interactions in a
PEALD process achieves 99% accuracy. This demonstrates that machine learning
can provide a new pathway to accelerate the optimization of PEALD processes in
areas such as microelectronics. Our approach can be easily extended to atomic
layer etching and more complex structures.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [162] [A theoretical basis for model collapse in recursive training](https://arxiv.org/abs/2506.09401)
*Vivek Shripad Borkar*

Main category: math.PR

TL;DR: 递归训练生成模型会导致模拟概率分布‘崩溃’，且根据是否有外部源贡献样本会有两种不同渐近行为。


<details>
  <summary>Details</summary>
Motivation: 研究递归训练生成模型时模拟概率分布的渐近行为。

Method: 未提及

Result: 根据是否有外部源贡献样本，会得到两种不同渐近行为。

Conclusion: 在递归训练生成模型中，外部源的有无会影响模拟概率分布的渐近行为。

Abstract: It is known that recursive training from generative models can lead to the so
called `collapse' of the simulated probability distribution. This note shows
that one in fact gets two different asymptotic behaviours depending on whether
an external source, howsoever minor, is also contributing samples.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [163] [Alice and the Caterpillar: A more descriptive null model for assessing data mining results](https://arxiv.org/abs/2506.09764)
*Giulia Preti,Gianmarco De Francisci Morales,Matteo Riondato*

Main category: cs.SI

TL;DR: 本文引入新的零模型评估二元交易和序列数据集结果，介绍算法Alice并展示其优势。


<details>
  <summary>Details</summary>
Motivation: 现有零模型不能很好保持观测数据集的属性，需引入新的零模型。

Method: 引入新零模型，该模型保持数据集对应二分图的二部联合度矩阵；提出Markov chain Monte Carlo算法Alice从新零模型采样数据集。

Result: 实验表明Alice混合速度快、扩展性好，新零模型得出与以往不同的显著结果。

Conclusion: 新零模型比现有模型更优，能保持更多数据集属性。

Abstract: We introduce novel null models for assessing the results obtained from
observed binary transactional and sequence datasets, using statistical
hypothesis testing. Our null models maintain more properties of the observed
dataset than existing ones. Specifically, they preserve the Bipartite Joint
Degree Matrix of the bipartite (multi-)graph corresponding to the dataset,
which ensures that the number of caterpillars, i.e., paths of length three, is
preserved, in addition to other properties considered by other models. We
describe Alice, a suite of Markov chain Monte Carlo algorithms for sampling
datasets from our null models, based on a carefully defined set of states and
efficient operations to move between them. The results of our experimental
evaluation show that Alice mixes fast and scales well, and that our null model
finds different significant results than ones previously considered in the
literature.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [164] [Revolutionizing Clinical Trials: A Manifesto for AI-Driven Transformation](https://arxiv.org/abs/2506.09102)
*Mihaela van der Schaar,Richard Peck,Eoin McKinney,Jim Weatherall,Stuart Bailey,Justine Rochon,Chris Anagnostopoulos,Pierre Marquet,Anthony Wood,Nicky Best,Harry Amad,Julianna Piskorz,Krzysztof Kacprzyk,Rafik Salama,Christina Gunther,Francesca Frau,Antoine Pugeat,Ramon Hernandez*

Main category: cs.CY

TL;DR: 由多领域领导者合作撰写的宣言，提出用因果推断和数字孪生两种AI技术改变临床试验的路线图。


<details>
  <summary>Details</summary>
Motivation: 希望借助AI技术让临床试验更快、更安全，为患者提供更个性化的结果，变革临床研究。

Method: 聚焦在现有监管框架内将两种AI技术进行可操作的整合。

Result: 无明确提及。

Conclusion: 提出利用AI重塑临床试验黄金标准的前进方向。

Abstract: This manifesto represents a collaborative vision forged by leaders in
pharmaceuticals, consulting firms, clinical research, and AI. It outlines a
roadmap for two AI technologies - causal inference and digital twins - to
transform clinical trials, delivering faster, safer, and more personalized
outcomes for patients. By focusing on actionable integration within existing
regulatory frameworks, we propose a way forward to revolutionize clinical
research and redefine the gold standard for clinical trials using AI.

</details>


### [165] [FAIRTOPIA: Envisioning Multi-Agent Guardianship for Disrupting Unfair AI Pipelines](https://arxiv.org/abs/2506.09107)
*Athena Vakali,Ilias Dimitriadis*

Main category: cs.CY

TL;DR: 本文提出利用智能体技术构建FAIRTOPIA框架保障AI公平性，能在各阶段实施公平监管。


<details>
  <summary>Details</summary>
Motivation: AI技术快速发展引发有害事件和不公平现象，急需打破忽视人类原则的AI流程。

Method: 引入基于设计的公平性方法，将多角色智能体嵌入端到端协同方案，提出可定制的通用算法，构建三层架构的FAIRTOPIA框架。

Result: 可在AI全流程各阶段实施公平监管，能激发基于多原则的公平性研究。

Conclusion: 可设计自适应、现实的AI公平性框架。

Abstract: AI models have become active decision makers, often acting without human
supervision. The rapid advancement of AI technology has already caused harmful
incidents that have hurt individuals and societies and AI unfairness in heavily
criticized. It is urgent to disrupt AI pipelines which largely neglect human
principles and focus on computational biases exploration at the data (pre),
model(in), and deployment (post) processing stages. We claim that by exploiting
the advances of agents technology, we will introduce cautious, prompt, and
ongoing fairness watch schemes, under realistic, systematic, and human-centric
fairness expectations. We envision agents as fairness guardians, since agents
learn from their environment, adapt to new information, and solve complex
problems by interacting with external tools and other systems. To set the
proper fairness guardrails in the overall AI pipeline, we introduce a
fairness-by-design approach which embeds multi-role agents in an end-to-end
(human to AI) synergetic scheme. Our position is that we may design adaptive
and realistic AI fairness frameworks, and we introduce a generalized algorithm
which can be customized to the requirements and goals of each AI decision
making scenario. Our proposed, so called FAIRTOPIA framework, is structured
over a three-layered architecture, which encapsulates the AI pipeline inside an
agentic guardian and a knowledge-based, self-refining layered scheme. Based on
our proposition, we enact fairness watch in all of the AI pipeline stages,
under robust multi-agent workflows, which will inspire new fairness research
hypothesis, heuristics, and methods grounded in human-centric, systematic,
interdisciplinary, socio-technical principles.

</details>


### [166] [Understanding Human-AI Trust in Education](https://arxiv.org/abs/2506.09160)
*Griffin Pitts,Sanaz Motamedi*

Main category: cs.CY

TL;DR: 研究AI聊天机器人在教育中应用时学生对其信任类型及影响，发现学生与AI有独特信任形式，需新理论框架。


<details>
  <summary>Details</summary>
Motivation: AI聊天机器人拟人特性使学生对其信任类型模糊，人际和技术信任模型适用性存挑战，需研究信任类型对学生相关感知的影响。

Method: 采用偏最小二乘结构方程建模。

Result: 类人信任和类系统信任显著影响学生感知，影响各有不同。

Conclusion: 学生与AI聊天机器人形成独特信任形式，需新理论框架，对有效采用AI教育有实践意义。

Abstract: As AI chatbots become increasingly integrated in education, students are
turning to these systems for guidance, feedback, and information. However, the
anthropomorphic characteristics of these chatbots create ambiguity regarding
whether students develop trust toward them as they would a human peer or
instructor, based in interpersonal trust, or as they would any other piece of
technology, based in technology trust. This ambiguity presents theoretical
challenges, as interpersonal trust models may inappropriately ascribe human
intentionality and morality to AI, while technology trust models were developed
for non-social technologies, leaving their applicability to anthropomorphic
systems unclear. To address this gap, we investigate how human-like and
system-like trusting beliefs comparatively influence students' perceived
enjoyment, trusting intention, behavioral intention to use, and perceived
usefulness of an AI chatbot - factors associated with students' engagement and
learning outcomes. Through partial least squares structural equation modeling,
we found that human-like and system-like trust significantly influenced student
perceptions, with varied effects. Human-like trust more strongly predicted
trusting intention, while system-like trust better predicted behavioral
intention and perceived usefulness. Both had similar effects on perceived
enjoyment. Given the partial explanatory power of each type of trust, we
propose that students develop a distinct form of trust with AI chatbots
(human-AI trust) that differs from human-human and human-technology models of
trust. Our findings highlight the need for new theoretical frameworks specific
to human-AI trust and offer practical insights for fostering appropriately
calibrated trust, which is critical for the effective adoption and pedagogical
impact of AI in education.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [167] [ReStNet: A Reusable & Stitchable Network for Dynamic Adaptation on IoT Devices](https://arxiv.org/abs/2506.09066)
*Maoyu Wang,Yao Lu,Jiaqi Nie,Zeyu Wang,Yun Lin,Qi Xuan,Guan Gui*

Main category: cs.CV

TL;DR: 提出ReStNet，通过拼接两个预训练模型动态构建混合网络，可适应不同资源约束，实验证明能实现灵活的准确率 - 效率权衡并降低训练成本。


<details>
  <summary>Details</summary>
Motivation: 现有预训练模型在不同物联网设备部署困难，传统压缩方法应用后缺乏灵活性，无法适应资源约束变化。

Method: 通过Centered Kernel Alignment (CKA)计算层间相似度确定拼接点，保留大容量模型的早期层并添加小容量模型的深层构建混合模型，仅微调拼接层，支持同构和异构拼接。

Result: 在多个基准测试上的实验表明，ReStNet在运行时能实现灵活的准确率 - 效率权衡，显著降低训练成本。

Conclusion: ReStNet设计可快速适应预算变化，充分利用可用资源，实现灵活的模型部署。

Abstract: With the rapid development of deep learning, a growing number of pre-trained
models have been publicly available. However, deploying these fixed models in
real-world IoT applications is challenging because different devices possess
heterogeneous computational and memory resources, making it impossible to
deploy a single model across all platforms. Although traditional compression
methods, such as pruning, quantization, and knowledge distillation, can improve
efficiency, they become inflexible once applied and cannot adapt to changing
resource constraints. To address these issues, we propose ReStNet, a Reusable
and Stitchable Network that dynamically constructs a hybrid network by
stitching two pre-trained models together. Implementing ReStNet requires
addressing several key challenges, including how to select the optimal
stitching points, determine the stitching order of the two pre-trained models,
and choose an effective fine-tuning strategy. To systematically address these
challenges and adapt to varying resource constraints, ReStNet determines the
stitching point by calculating layer-wise similarity via Centered Kernel
Alignment (CKA). It then constructs the hybrid model by retaining early layers
from a larger-capacity model and appending deeper layers from a smaller one. To
facilitate efficient deployment, only the stitching layer is fine-tuned. This
design enables rapid adaptation to changing budgets while fully leveraging
available resources. Moreover, ReStNet supports both homogeneous (CNN-CNN,
Transformer-Transformer) and heterogeneous (CNN-Transformer) stitching,
allowing to combine different model families flexibly. Extensive experiments on
multiple benchmarks demonstrate that ReStNet achieve flexible
accuracy-efficiency trade-offs at runtime while significantly reducing training
cost.

</details>


### [168] [Enhancing the Safety of Medical Vision-Language Models by Synthetic Demonstrations](https://arxiv.org/abs/2506.09067)
*Zhiyu Xue,Reza Abbasi-Asl,Ramtin Pedarsani*

Main category: cs.CV

TL;DR: 提出新颖推理时间防御策略减轻医学视觉语言模型有害查询，提升安全性且性能损失小，还提出混合演示策略平衡安全与性能。


<details>
  <summary>Details</summary>
Motivation: 医学视觉语言模型安全漏洞未充分探索，存在过度防御问题。

Method: 提出新颖推理时间防御策略，基于合成临床演示，使用九种模态医学成像数据集。

Result: 防御策略增强模型安全性且未显著降低性能，增加演示预算可缓解过度防御问题。

Conclusion: 引入混合演示策略作为在少样本演示预算约束下平衡安全与性能的折衷解决方案。

Abstract: Generative medical vision-language models~(Med-VLMs) are primarily designed
to generate complex textual information~(e.g., diagnostic reports) from
multimodal inputs including vision modality~(e.g., medical images) and language
modality~(e.g., clinical queries). However, their security vulnerabilities
remain underexplored. Med-VLMs should be capable of rejecting harmful queries,
such as \textit{Provide detailed instructions for using this CT scan for
insurance fraud}. At the same time, addressing security concerns introduces the
risk of over-defense, where safety-enhancing mechanisms may degrade general
performance, causing Med-VLMs to reject benign clinical queries. In this paper,
we propose a novel inference-time defense strategy to mitigate harmful queries,
enabling defense against visual and textual jailbreak attacks. Using diverse
medical imaging datasets collected from nine modalities, we demonstrate that
our defense strategy based on synthetic clinical demonstrations enhances model
safety without significantly compromising performance. Additionally, we find
that increasing the demonstration budget alleviates the over-defense issue. We
then introduce a mixed demonstration strategy as a trade-off solution for
balancing security and performance under few-shot demonstration budget
constraints.

</details>


### [169] [Segment Any Architectural Facades (SAAF):An automatic segmentation model for building facades, walls and windows based on multimodal semantics guidance](https://arxiv.org/abs/2506.09071)
*Peilin Li,Jun Yin,Jing Zhong,Ran Luo,Pengyu Zeng,Miao Zhang*

Main category: cs.CV

TL;DR: 本文提出基于多模态语义引导的建筑立面墙窗自动分割模型SAAF，实验显示其表现优于现有方法，为建筑计算机视觉技术发展提供参考。


<details>
  <summary>Details</summary>
Motivation: 在建筑数字化发展背景下，提高建筑信息模型和计算机辅助设计效率，解决墙窗自动分割问题。

Method: 提出SAAF模型，采用多模态语义协同特征提取机制，结合自然语言处理技术融合文本语义与图像特征；开发端到端训练框架，使模型自主学习文本到图像分割的映射关系。

Result: 在多个立面数据集上实验，SAAF在mIoU指标上优于现有方法，面对不同数据集能保持高精度分割能力。

Conclusion: 模型在墙窗分割任务的准确性和泛化能力上取得进展，有望为建筑计算机视觉技术发展提供参考，探索多模态学习在建筑领域的应用。

Abstract: In the context of the digital development of architecture, the automatic
segmentation of walls and windows is a key step in improving the efficiency of
building information models and computer-aided design. This study proposes an
automatic segmentation model for building facade walls and windows based on
multimodal semantic guidance, called Segment Any Architectural Facades (SAAF).
First, SAAF has a multimodal semantic collaborative feature extraction
mechanism. By combining natural language processing technology, it can fuse the
semantic information in text descriptions with image features, enhancing the
semantic understanding of building facade components. Second, we developed an
end-to-end training framework that enables the model to autonomously learn the
mapping relationship from text descriptions to image segmentation, reducing the
influence of manual intervention on the segmentation results and improving the
automation and robustness of the model. Finally, we conducted extensive
experiments on multiple facade datasets. The segmentation results of SAAF
outperformed existing methods in the mIoU metric, indicating that the SAAF
model can maintain high-precision segmentation ability when faced with diverse
datasets. Our model has made certain progress in improving the accuracy and
generalization ability of the wall and window segmentation task. It is expected
to provide a reference for the development of architectural computer vision
technology and also explore new ideas and technical paths for the application
of multimodal learning in the architectural field.

</details>


### [170] [VersaVid-R1: A Versatile Video Understanding and Reasoning Model from Question Answering to Captioning Tasks](https://arxiv.org/abs/2506.09079)
*Xinlong Chen,Yuanxing Zhang,Yushuo Guan,Bohan Zeng,Yang Shi,Sihan Yang,Pengfei Wan,Qiang Liu,Liang Wang,Tieniu Tan*

Main category: cs.CV

TL;DR: 文章引入两个新数据集 DarkEventInfer 和 MixVidQA，结合强化学习开发了 VersaVid - R1 模型，实验表明其在多个视频任务基准上显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在视频推理领域发展不足，缺乏高质量推理数据和有效训练方法，需填补该领域空白。

Method: 引入 DarkEventInfer 和 MixVidQA 数据集，利用精心策划的训练样本和多种奖励函数引导的强化学习方法。

Result: 开发出 VersaVid - R1 模型，能处理多项视频任务，在广泛基准测试中显著优于现有模型。

Conclusion: 所提出的数据集和模型在视频理解和推理任务上表现出色，推动了多模态大语言模型在视频推理领域的发展。

Abstract: Recent advancements in multimodal large language models have successfully
extended the Reason-Then-Respond paradigm to image-based reasoning, yet
video-based reasoning remains an underdeveloped frontier, primarily due to the
scarcity of high-quality reasoning-oriented data and effective training
methodologies. To bridge this gap, we introduce DarkEventInfer and MixVidQA,
two novel datasets specifically designed to stimulate the model's advanced
video understanding and reasoning abilities. DarkEventinfer presents videos
with masked event segments, requiring models to infer the obscured content
based on contextual video cues. MixVidQA, on the other hand, presents
interleaved video sequences composed of two distinct clips, challenging models
to isolate and reason about one while disregarding the other. Leveraging these
carefully curated training samples together with reinforcement learning guided
by diverse reward functions, we develop VersaVid-R1, the first versatile video
understanding and reasoning model under the Reason-Then-Respond paradigm
capable of handling multiple-choice and open-ended question answering, as well
as video captioning tasks. Extensive experiments demonstrate that VersaVid-R1
significantly outperforms existing models across a broad spectrum of
benchmarks, covering video general understanding, cognitive reasoning, and
captioning tasks.

</details>


### [171] [FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation](https://arxiv.org/abs/2506.09081)
*Zheqi He,Yesheng Liu,Jing-shu Zheng,Xuejing Li,Richeng Xuan,Jin-Ge Yao,Xi Yang*

Main category: cs.CV

TL;DR: 介绍开源评估框架FlagEvalMM，可全面评估多模态模型，实验证明其有价值且公开可用。


<details>
  <summary>Details</summary>
Motivation: 需要一个能全面评估多模态模型在多种视觉 - 语言任务表现的工具。

Method: 通过独立评估服务解耦模型推理与评估，利用先进推理加速工具和异步数据加载。

Result: FlagEvalMM能准确高效洞察模型优缺点。

Conclusion: FlagEvalMM是推进多模态研究的有价值工具。

Abstract: We present FlagEvalMM, an open-source evaluation framework designed to
comprehensively assess multimodal models across a diverse range of
vision-language understanding and generation tasks, such as visual question
answering, text-to-image/video generation, and image-text retrieval. We
decouple model inference from evaluation through an independent evaluation
service, thus enabling flexible resource allocation and seamless integration of
new tasks and models. Moreover, FlagEvalMM utilizes advanced inference
acceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to
significantly enhance evaluation efficiency. Extensive experiments show that
FlagEvalMM offers accurate and efficient insights into model strengths and
limitations, making it a valuable tool for advancing multimodal research. The
framework is publicly accessible athttps://github.com/flageval-baai/FlagEvalMM.

</details>


### [172] [AVA-Bench: Atomic Visual Ability Benchmark for Vision Foundation Models](https://arxiv.org/abs/2506.09082)
*Zheda Mai,Arpita Chowdhury,Zihe Wang,Sooyoung Jeon,Lemeng Wang,Jiacheng Hou,Jihyung Kil,Wei-Lun Chao*

Main category: cs.CV

TL;DR: 提出AVA - Bench基准用于评估视觉基础模型，能明确模型的优劣势，且发现小语言模型也能高效评估。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型评估协议存在指令调整数据与测试分布不匹配、难以确定错误来源的问题，需要改进评估方法。

Method: 引入AVA - Bench基准，解耦14种原子视觉能力，匹配训练和测试分布。

Result: 应用AVA - Bench揭示了模型的“能力指纹”，0.5B的大语言模型与7B的大语言模型在评估模型排名上效果相似，但能减少8倍GPU使用时间。

Conclusion: AVA - Bench为下一代视觉基础模型评估奠定了基础。

Abstract: The rise of vision foundation models (VFMs) calls for systematic evaluation.
A common approach pairs VFMs with large language models (LLMs) as
general-purpose heads, followed by evaluation on broad Visual Question
Answering (VQA) benchmarks. However, this protocol has two key blind spots: (i)
the instruction tuning data may not align with VQA test distributions, meaning
a wrong prediction can stem from such data mismatch rather than a VFM' visual
shortcomings; (ii) VQA benchmarks often require multiple visual abilities,
making it hard to tell whether errors stem from lacking all required abilities
or just a single critical one. To address these gaps, we introduce AVA-Bench,
the first benchmark that explicitly disentangles 14 Atomic Visual Abilities
(AVAs) -- foundational skills like localization, depth estimation, and spatial
understanding that collectively support complex visual reasoning tasks. By
decoupling AVAs and matching training and test distributions within each,
AVA-Bench pinpoints exactly where a VFM excels or falters. Applying AVA-Bench
to leading VFMs thus reveals distinctive "ability fingerprints," turning VFM
selection from educated guesswork into principled engineering. Notably, we find
that a 0.5B LLM yields similar VFM rankings as a 7B LLM while cutting GPU hours
by 8x, enabling more efficient evaluation. By offering a comprehensive and
transparent benchmark, we hope AVA-Bench lays the foundation for the next
generation of VFMs.

</details>


### [173] [BakuFlow: A Streamlining Semi-Automatic Label Generation Tool](https://arxiv.org/abs/2506.09083)
*Jerry Lin,Partick P. W. Chen*

Main category: cs.CV

TL;DR: 本文介绍半自动化标签生成工具BakuFlow，其具多种特性，能减少标注工作量并提高效率。


<details>
  <summary>Details</summary>
Motivation: 准确标注数据是计算机视觉瓶颈，手动标注耗时且易出错，现有工具存在需手动标注每张图片的问题。

Method: 引入BakuFlow工具，具备实时可调放大镜、交互式数据增强模块、标签传播功能及基于改进YOLOE框架的自动标注模块。

Result: 该工具可有效用于目标检测和跟踪，减少标注工作量。

Conclusion: BakuFlow在实际计算机视觉和工业场景中能提高标注效率。

Abstract: Accurately labeling (or annotation) data is still a bottleneck in computer
vision, especially for large-scale tasks where manual labeling is
time-consuming and error-prone. While tools like LabelImg can handle the
labeling task, some of them still require annotators to manually label each
image. In this paper, we introduce BakuFlow, a streamlining semi-automatic
label generation tool. Key features include (1) a live adjustable magnifier for
pixel-precise manual corrections, improving user experience; (2) an interactive
data augmentation module to diversify training datasets; (3) label propagation
for rapidly copying labeled objects between consecutive frames, greatly
accelerating annotation of video data; and (4) an automatic labeling module
powered by a modified YOLOE framework. Unlike the original YOLOE, our extension
supports adding new object classes and any number of visual prompts per class
during annotation, enabling flexible and scalable labeling for dynamic,
real-world datasets. These innovations make BakuFlow especially effective for
object detection and tracking, substantially reducing labeling workload and
improving efficiency in practical computer vision and industrial scenarios.

</details>


### [174] [Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation](https://arxiv.org/abs/2506.09350)
*Shanchuan Lin,Ceyuan Yang,Hao He,Jianwen Jiang,Yuxi Ren,Xin Xia,Yang Zhao,Xuefeng Xiao,Lu Jiang*

Main category: cs.CV

TL;DR: 提出AAPT方法将预训练潜在视频扩散模型转换为实时交互式视频生成器，实验显示模型有高效表现。


<details>
  <summary>Details</summary>
Motivation: 现有大规模视频生成模型计算量大，无法用于实时和交互式应用。

Method: 提出自回归对抗后训练（AAPT）方法，每次自回归生成一个潜在帧，利用对抗训练进行自回归生成，以学生强迫方式训练模型。

Result: 8B模型在单H100上以736x416分辨率实现24fps实时流式视频生成，在8xH100上以1280x720分辨率生成长达一分钟的视频。

Conclusion: AAPT方法能将预训练潜在视频扩散模型转换为高效的实时交互式视频生成器。

Abstract: Existing large-scale video generation models are computationally intensive,
preventing adoption in real-time and interactive applications. In this work, we
propose autoregressive adversarial post-training (AAPT) to transform a
pre-trained latent video diffusion model into a real-time, interactive video
generator. Our model autoregressively generates a latent frame at a time using
a single neural function evaluation (1NFE). The model can stream the result to
the user in real time and receive interactive responses as controls to generate
the next latent frame. Unlike existing approaches, our method explores
adversarial training as an effective paradigm for autoregressive generation.
This not only allows us to design an architecture that is more efficient for
one-step generation while fully utilizing the KV cache, but also enables
training the model in a student-forcing manner that proves to be effective in
reducing error accumulation during long video generation. Our experiments
demonstrate that our 8B model achieves real-time, 24fps, streaming video
generation at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to
a minute long (1440 frames). Visit our research website at
https://seaweed-apt.com/2

</details>


### [175] [SAGE: Exploring the Boundaries of Unsafe Concept Domain with Semantic-Augment Erasing](https://arxiv.org/abs/2506.09363)
*Hongguang Zhu,Yunchao Wei,Mengyu Wang,Siyu Jiao,Yan Fang,Jiannan Huang,Yao Zhao*

Main category: cs.CV

TL;DR: 文章提出SAGE方法解决扩散模型预训练时敏感信息问题，实验证明其优越性且将开源代码和权重。


<details>
  <summary>Details</summary>
Motivation: 扩散模型预训练时会包含敏感信息带来安全风险，现有概念擦除方法存在局限，无法进行通用的概念相关擦除。

Method: 引入语义增强擦除将概念词擦除转换为概念域擦除，提出全局 - 局部协作保留机制减少擦除时无关概念的保留退化。

Result: 广泛实验表明SAGE方法相比其他方法在扩散模型安全生成方面具有全面优越性。

Conclusion: SAGE方法能有效解决扩散模型敏感信息问题，提升安全生成能力。

Abstract: Diffusion models (DMs) have achieved significant progress in text-to-image
generation. However, the inevitable inclusion of sensitive information during
pre-training poses safety risks, such as unsafe content generation and
copyright infringement. Concept erasing finetunes weights to unlearn
undesirable concepts, and has emerged as a promising solution. However,
existing methods treat unsafe concept as a fixed word and repeatedly erase it,
trapping DMs in ``word concept abyss'', which prevents generalized
concept-related erasing. To escape this abyss, we introduce semantic-augment
erasing which transforms concept word erasure into concept domain erasure by
the cyclic self-check and self-erasure. It efficiently explores and unlearns
the boundary representation of concept domain through semantic spatial
relationships between original and training DMs, without requiring additional
preprocessed data. Meanwhile, to mitigate the retention degradation of
irrelevant concepts while erasing unsafe concepts, we further propose the
global-local collaborative retention mechanism that combines global semantic
relationship alignment with local predicted noise preservation, effectively
expanding the retentive receptive field for irrelevant concepts. We name our
method SAGE, and extensive experiments demonstrate the comprehensive
superiority of SAGE compared with other methods in the safe generation of DMs.
The code and weights will be open-sourced at
https://github.com/KevinLight831/SAGE.

</details>


### [176] [Synthetic Human Action Video Data Generation with Pose Transfer](https://arxiv.org/abs/2506.09411)
*Vaclav Knapp,Matyas Bohacek*

Main category: cs.CV

TL;DR: 本文提出用姿态迁移生成合成人类动作视频数据的方法，在相关数据集评估显示可提升动作识别任务表现，还能扩展小样本数据集，且开源了方法和相关数据集。


<details>
  <summary>Details</summary>
Motivation: 视频理解任务中合成数据生成有怪异特征，影响训练效果，导致手语翻译等任务无法充分利用合成数据。

Method: 使用姿态迁移（可控3D高斯头像模型）生成合成人类动作视频数据。

Result: 在Toyota Smarthome和NTU RGB+D数据集评估，提升了动作识别任务表现，能有效扩展小样本数据集、弥补真实训练数据中代表性不足的群体并添加多样背景。

Conclusion: 所提方法可行有效，且开源方便后续研究。

Abstract: In video understanding tasks, particularly those involving human motion,
synthetic data generation often suffers from uncanny features, diminishing its
effectiveness for training. Tasks such as sign language translation, gesture
recognition, and human motion understanding in autonomous driving have thus
been unable to exploit the full potential of synthetic data. This paper
proposes a method for generating synthetic human action video data using pose
transfer (specifically, controllable 3D Gaussian avatar models). We evaluate
this method on the Toyota Smarthome and NTU RGB+D datasets and show that it
improves performance in action recognition tasks. Moreover, we demonstrate that
the method can effectively scale few-shot datasets, making up for groups
underrepresented in the real training data and adding diverse backgrounds. We
open-source the method along with RANDOM People, a dataset with videos and
avatars of novel human identities for pose transfer crowd-sourced from the
internet.

</details>


### [177] [A High-Quality Dataset and Reliable Evaluation for Interleaved Image-Text Generation](https://arxiv.org/abs/2506.09427)
*Yukang Feng,Jianwen Sun,Chuanhao Li,Zizhen Li,Jiaxin Ai,Fanrui Zhang,Yifan Chang,Sizhuo Zhou,Shenglin Zhang,Yu Dai,Kaipeng Zhang*

Main category: cs.CV

TL;DR: 提出大规模多模态数据集InterSyn和自动评估模型SynJudge，实验表明SEIR方法提升数据集质量，基于InterSyn训练的LMMs性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在生成紧密交织的图文输出方面存在困难，且缺乏可靠评估工具。

Method: 使用Self - Evaluation with Iterative Refinement (SEIR)方法构建InterSyn数据集，引入SynJudge自动评估模型。

Result: SEIR方法显著提高数据集质量，基于InterSyn训练的LMMs在所有评估指标上性能提升。

Conclusion: InterSyn数据集有助于推动多模态系统发展。

Abstract: Recent advancements in Large Multimodal Models (LMMs) have significantly
improved multimodal understanding and generation. However, these models still
struggle to generate tightly interleaved image-text outputs, primarily due to
the limited scale, quality and instructional richness of current training
datasets. To address this, we introduce InterSyn, a large-scale multimodal
dataset constructed using our Self-Evaluation with Iterative Refinement (SEIR)
method. InterSyn features multi-turn, instruction-driven dialogues with tightly
interleaved imagetext responses, providing rich object diversity and rigorous
automated quality refinement, making it well-suited for training
next-generation instruction-following LMMs. Furthermore, to address the lack of
reliable evaluation tools capable of assessing interleaved multimodal outputs,
we introduce SynJudge, an automatic evaluation model designed to quantitatively
assess multimodal outputs along four dimensions: text content, image content,
image quality, and image-text synergy.
  Experimental studies show that the SEIR method leads to substantially higher
dataset quality compared to an otherwise identical process without refinement.
  Moreover, LMMs trained on InterSyn achieve uniform performance gains across
all evaluation metrics, confirming InterSyn's utility for advancing multimodal
systems.

</details>


### [178] [TOGA: Temporally Grounded Open-Ended Video QA with Weak Supervision](https://arxiv.org/abs/2506.09445)
*Ayush Gupta,Anirban Roy,Rama Chellappa,Nathaniel D. Bastian,Alvaro Velasquez,Susmit Jha*

Main category: cs.CV

TL;DR: 提出弱监督下用于视频问答和时间定位的视觉语言模型TOGA，在相关基准测试中达SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决弱监督无时间注释下的视频问答及时间定位问题。

Method: 提出TOGA模型，指令调优以联合生成答案和时间定位，生成伪标签并通过一致性约束确保标签有效性。

Result: 在NExT - GQA、MSVD - QA和ActivityNet - QA基准测试中，联合生成答案和定位提升了问答和定位性能，达到SOTA。

Conclusion: TOGA模型在弱监督视频问答及时间定位任务中表现优秀。

Abstract: We address the problem of video question answering (video QA) with temporal
grounding in a weakly supervised setup, without any temporal annotations. Given
a video and a question, we generate an open-ended answer grounded with the
start and end time. For this task, we propose TOGA: a vision-language model for
Temporally Grounded Open-Ended Video QA with Weak Supervision. We instruct-tune
TOGA to jointly generate the answer and the temporal grounding. We operate in a
weakly supervised setup where the temporal grounding annotations are not
available. We generate pseudo labels for temporal grounding and ensure the
validity of these labels by imposing a consistency constraint between the
question of a grounding response and the response generated by a question
referring to the same temporal segment. We notice that jointly generating the
answers with the grounding improves performance on question answering as well
as grounding. We evaluate TOGA on grounded QA and open-ended QA tasks. For
grounded QA, we consider the NExT-GQA benchmark which is designed to evaluate
weakly supervised grounded question answering. For open-ended QA, we consider
the MSVD-QA and ActivityNet-QA benchmarks. We achieve state-of-the-art
performance for both tasks on these benchmarks.

</details>


### [179] [Revisit What You See: Disclose Language Prior in Vision Tokens for Efficient Guided Decoding of LVLMs](https://arxiv.org/abs/2506.09522)
*Beomsik Cho,Jaehyung Kim*

Main category: cs.CV

TL;DR: 本文提出ReVisiT解码方法解决LVLMs传统解码策略无法有效利用视觉信息问题，实验表明该方法能增强视觉关联且降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统LVLMs解码策略无法有效利用视觉信息，现有解决方法需额外训练等，本文旨在提出简单有效的方法。

Method: 提出ReVisiT方法，将视觉标记投影到文本标记分布空间，通过约束散度最小化动态选择最相关视觉标记，用其优化输出分布。

Result: 在三个LVLM幻觉基准测试中，ReVisiT能持续增强视觉关联，计算开销小，与现有基线相比有竞争力或更优，计算成本最多降低2倍。

Conclusion: ReVisiT是一种简单有效的LVLMs解码方法，能提升视觉关联并降低计算成本。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable performance
across various multimodal tasks by integrating visual perception with language
understanding. However, conventional decoding strategies of LVLMs often fail to
successfully utilize visual information, leading to visually ungrounded
responses. While various approaches have been proposed to address this
limitation, they typically require additional training, multi-step inference
procedures, or external model dependencies. This paper introduces ReVisiT, a
simple yet effective decoding method that references vision tokens to guide the
text generation process in LVLMs. Our approach leverages the semantic
information embedded within vision tokens by projecting them into the text
token distribution space, and dynamically selecting the most relevant vision
token at each decoding step through constrained divergence minimization. This
selected vision token is then used to refine the output distribution to better
incorporate visual semantics. Experiments on three LVLM hallucination
benchmarks with two recent LVLMs demonstrate that ReVisiT consistently enhances
visual grounding with minimal computational overhead. Moreover, our method
achieves competitive or superior results relative to state-of-the-art baselines
while reducing computational costs for up to $2\times$.

</details>


### [180] [Unify Graph Learning with Text: Unleashing LLM Potentials for Session Search](https://arxiv.org/abs/2505.14156)
*Songhao Wu,Quan Tu,Hong Liu,Jia Xu,Zhongyi Liu,Guannan Zhang,Ran Wang,Xiuying Chen,Rui Yan*

Main category: cs.CV

TL;DR: 提出Symbolic Graph Ranker (SGR) 结合文本和图方法进行会话搜索，用符号语法规则转换图为文本，引入自监督学习任务，实验证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有会话搜索策略要么忽视交互中的图结构，要么忽略词级语义建模，需结合文本和图方法。

Method: 引入符号语法规则将会话图转换为文本，作为大语言模型输入；引入自监督符号学习任务，使大语言模型从粗粒度到细粒度捕捉拓扑信息。

Result: 在AOL和Tiangong - ST两个基准数据集上的实验结果和综合分析证实了方法的优越性。

Conclusion: 提出的范式提供了一种新颖有效的方法，弥合了传统搜索策略和现代大语言模型之间的差距。

Abstract: Session search involves a series of interactive queries and actions to
fulfill user's complex information need. Current strategies typically
prioritize sequential modeling for deep semantic understanding, overlooking the
graph structure in interactions. While some approaches focus on capturing
structural information, they use a generalized representation for documents,
neglecting the word-level semantic modeling. In this paper, we propose Symbolic
Graph Ranker (SGR), which aims to take advantage of both text-based and
graph-based approaches by leveraging the power of recent Large Language Models
(LLMs). Concretely, we first introduce a set of symbolic grammar rules to
convert session graph into text. This allows integrating session history,
interaction process, and task instruction seamlessly as inputs for the LLM.
Moreover, given the natural discrepancy between LLMs pre-trained on textual
corpora, and the symbolic language we produce using our graph-to-text grammar,
our objective is to enhance LLMs' ability to capture graph structures within a
textual format. To achieve this, we introduce a set of self-supervised symbolic
learning tasks including link prediction, node content generation, and
generative contrastive learning, to enable LLMs to capture the topological
information from coarse-grained to fine-grained. Experiment results and
comprehensive analysis on two benchmark datasets, AOL and Tiangong-ST, confirm
the superiority of our approach. Our paradigm also offers a novel and effective
methodology that bridges the gap between traditional search strategies and
modern LLMs.

</details>


### [181] [AD^2-Bench: A Hierarchical CoT Benchmark for MLLM in Autonomous Driving under Adverse Conditions](https://arxiv.org/abs/2506.09557)
*Zhaoyang Wei,Chenhui Qiang,Bowen Jiang,Xumeng Han,Xuehui Yu,Zhenjun Han*

Main category: cs.CV

TL;DR: 引入AD^2 - Bench基准测试用于自动驾驶多模态大模型思维链推理评估，评估显示当前模型准确率低，该基准有重要价值。


<details>
  <summary>Details</summary>
Motivation: 现有基准忽略自动驾驶恶劣场景下思维链推理的严格评估，需填补此空白。

Method: 构建AD^2 - Bench，满足数据覆盖全、注释细粒度、有专用评估框架三个标准，收集5.4k个高质量人工注释思维链实例。

Result: 对先进多模态大模型在AD^2 - Bench上的评估显示准确率低于60%。

Conclusion: AD^2 - Bench提供标准化评估平台，能推动自动驾驶多模态大模型推理能力的研究。

Abstract: Chain-of-Thought (CoT) reasoning has emerged as a powerful approach to
enhance the structured, multi-step decision-making capabilities of Multi-Modal
Large Models (MLLMs), is particularly crucial for autonomous driving with
adverse weather conditions and complex traffic environments. However, existing
benchmarks have largely overlooked the need for rigorous evaluation of CoT
processes in these specific and challenging scenarios. To address this critical
gap, we introduce AD^2-Bench, the first Chain-of-Thought benchmark specifically
designed for autonomous driving with adverse weather and complex scenes.
AD^2-Bench is meticulously constructed to fulfill three key criteria:
comprehensive data coverage across diverse adverse environments, fine-grained
annotations that support multi-step reasoning, and a dedicated evaluation
framework tailored for assessing CoT performance. The core contribution of
AD^2-Bench is its extensive collection of over 5.4k high-quality, manually
annotated CoT instances. Each intermediate reasoning step in these annotations
is treated as an atomic unit with explicit ground truth, enabling unprecedented
fine-grained analysis of MLLMs' inferential processes under text-level,
point-level, and region-level visual prompts. Our comprehensive evaluation of
state-of-the-art MLLMs on AD^2-Bench reveals accuracy below 60%, highlighting
the benchmark's difficulty and the need to advance robust, interpretable
end-to-end autonomous driving systems. AD^2-Bench thus provides a standardized
evaluation platform, driving research forward by improving MLLMs' reasoning in
autonomous driving, making it an invaluable resource.

</details>


### [182] [BG-HOP: A Bimanual Generative Hand-Object Prior](https://arxiv.org/abs/2506.09068)
*Sriram Krishna,Sravan Chittupalli,Sungjae Park*

Main category: cs.CV

TL;DR: 提出BG - HOP生成先验模型用于3D双手与物体交互建模，展示初步成果并开源代码和模型


<details>
  <summary>Details</summary>
Motivation: 解决双手交互数据有限的问题

Method: 扩展现有的单手握持生成先验

Result: 模型能够生成双手交互并为给定物体合成抓握动作

Conclusion: 成功构建可用于3D双手与物体交互建模的BG - HOP模型

Abstract: In this work, we present BG-HOP, a generative prior that seeks to model
bimanual hand-object interactions in 3D. We address the challenge of limited
bimanual interaction data by extending existing single-hand generative priors,
demonstrating preliminary results in capturing the joint distribution of hands
and objects. Our experiments showcase the model's capability to generate
bimanual interactions and synthesize grasps for given objects. We make code and
models publicly available.

</details>


### [183] [HSENet: Hybrid Spatial Encoding Network for 3D Medical Vision-Language Understanding](https://arxiv.org/abs/2506.09634)
*Yanzhao Shi,Xiaodan Zhang,Junzhong Ji,Haoning Jiang,Chengxin Zheng,Yinong Wang,Liangqiong Qu*

Main category: cs.CV

TL;DR: 本文提出HSENet框架用于3D CT自动诊断，实验显示其在多项任务中达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型主要关注2D医学图像，难以捕捉复杂3D解剖结构，易导致诊断误判。

Method: 提出HSENet框架，采用双3D视觉编码器感知全局和细节信息，并通过双阶段与诊断报告对齐预训练；提出Spatial Packer将3D空间区域压缩为视觉标记，促进视觉表征向语义空间转换。

Result: 在3D语言 - 视觉检索、3D医学报告生成和3D视觉问答任务中取得SOTA性能。

Conclusion: HSENet框架有效，代码开源。

Abstract: Automated 3D CT diagnosis empowers clinicians to make timely, evidence-based
decisions by enhancing diagnostic accuracy and workflow efficiency. While
multimodal large language models (MLLMs) exhibit promising performance in
visual-language understanding, existing methods mainly focus on 2D medical
images, which fundamentally limits their ability to capture complex 3D
anatomical structures. This limitation often leads to misinterpretation of
subtle pathologies and causes diagnostic hallucinations. In this paper, we
present Hybrid Spatial Encoding Network (HSENet), a framework that exploits
enriched 3D medical visual cues by effective visual perception and projection
for accurate and robust vision-language understanding. Specifically, HSENet
employs dual-3D vision encoders to perceive both global volumetric contexts and
fine-grained anatomical details, which are pre-trained by dual-stage alignment
with diagnostic reports. Furthermore, we propose Spatial Packer, an efficient
multimodal projector that condenses high-resolution 3D spatial regions into a
compact set of informative visual tokens via centroid-based compression. By
assigning spatial packers with dual-3D vision encoders, HSENet can seamlessly
perceive and transfer hybrid visual representations to LLM's semantic space,
facilitating accurate diagnostic text generation. Experimental results
demonstrate that our method achieves state-of-the-art performance in 3D
language-visual retrieval (39.85% of R@100, +5.96% gain), 3D medical report
generation (24.01% of BLEU-4, +8.01% gain), and 3D visual question answering
(73.60% of Major Class Accuracy, +1.99% gain), confirming its effectiveness.
Our code is available at https://github.com/YanzhaoShi/HSENet.

</details>


### [184] [DGAE: Diffusion-Guided Autoencoder for Efficient Latent Representation Learning](https://arxiv.org/abs/2506.09644)
*Dongxu Liu,Yuang Peng,Haomiao Tang,Yuwei Chen,Chunrui Han,Zheng Ge,Daxin Jiang,Mingxue Liao*

Main category: cs.CV

TL;DR: 提出DGAE缓解高空间压缩率下性能下降，缩小潜在空间并提升图像生成性能。


<details>
  <summary>Details</summary>
Motivation: 解决GAN导致的训练不稳定问题，在提升空间压缩的同时最小化潜在空间维度。

Method: 提出DGAE，用扩散模型引导解码器恢复未完全解码的信息信号。

Result: 有效缓解高空间压缩率下性能下降，潜在空间缩小2倍，在图像生成任务中表现出色，促进扩散模型更快收敛。

Conclusion: DGAE在图像和视频生成领域有良好效果，其紧凑潜在表示有助于提升模型性能。

Abstract: Autoencoders empower state-of-the-art image and video generative models by
compressing pixels into a latent space through visual tokenization. Although
recent advances have alleviated the performance degradation of autoencoders
under high compression ratios, addressing the training instability caused by
GAN remains an open challenge. While improving spatial compression, we also aim
to minimize the latent space dimensionality, enabling more efficient and
compact representations. To tackle these challenges, we focus on improving the
decoder's expressiveness. Concretely, we propose DGAE, which employs a
diffusion model to guide the decoder in recovering informative signals that are
not fully decoded from the latent representation. With this design, DGAE
effectively mitigates the performance degradation under high spatial
compression rates. At the same time, DGAE achieves state-of-the-art performance
with a 2x smaller latent space. When integrated with Diffusion Models, DGAE
demonstrates competitive performance on image generation for ImageNet-1K and
shows that this compact latent representation facilitates faster convergence of
the diffusion model.

</details>


### [185] [Reasoning Models Are More Easily Gaslighted Than You Think](https://arxiv.org/abs/2506.09677)
*Bin Zhu,Hailong Yin,Jingjing Chen,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 评估推理模型在误导输入下的鲁棒性，发现显著准确率下降，还引入新基准加剧模型失败，揭示推理模型鲁棒性局限。


<details>
  <summary>Details</summary>
Motivation: 现有以推理为中心的模型应对误导性用户输入的能力未充分探索，需进行评估。

Method: 对三个先进推理模型在三个多模态基准上进行系统评估，引入新诊断基准GaslightingBench - R。

Result: 在误导否定提示下模型平均准确率下降25 - 29%，新基准下平均准确率下降超53%。

Conclusion: 推理模型在鲁棒性上存在根本局限，逐步推理和信念坚持之间存在差距。

Abstract: Recent advances in reasoning-centric models promise improved robustness
through mechanisms such as chain-of-thought prompting and test-time scaling.
However, their ability to withstand misleading user input remains
underexplored. In this paper, we conduct a systematic evaluation of three
state-of-the-art reasoning models, i.e., OpenAI's o4-mini, Claude-3.7-Sonnet
and Gemini-2.5-Flash, across three multimodal benchmarks: MMMU, MathVista, and
CharXiv. Our evaluation reveals significant accuracy drops (25-29% on average)
following gaslighting negation prompts, indicating that even top-tier reasoning
models struggle to preserve correct answers under manipulative user feedback.
Built upon the insights of the evaluation and to further probe this
vulnerability, we introduce GaslightingBench-R, a new diagnostic benchmark
specifically designed to evaluate reasoning models' susceptibility to defend
their belief under gaslighting negation prompt. Constructed by filtering and
curating 1,025 challenging samples from the existing benchmarks,
GaslightingBench-R induces even more dramatic failures, with accuracy drops
exceeding 53% on average. Our findings reveal fundamental limitations in the
robustness of reasoning models, highlighting the gap between step-by-step
reasoning and belief persistence.

</details>


### [186] [Towards Practical Alzheimer's Disease Diagnosis: A Lightweight and Interpretable Spiking Neural Model](https://arxiv.org/abs/2506.09695)
*Changwei Wu,Yifei Chen,Yuxin Du,Jinying Zong,Jie Dong,Mingxuan Liu,Yong Peng,Jin Fan,Feiwei Qin,Changmiao Wang*

Main category: cs.CV

TL;DR: 文章指出AD早期诊断存在问题，现有深度学习方法有局限，提出FasterSNN，实验证明其有竞争力且效率和稳定性高，代码开源。


<details>
  <summary>Details</summary>
Motivation: AD早期诊断受主观评估和成像成本限制，深度学习方法能耗高、计算需求大，现有SNN存在表现力弱和训练不稳定问题。

Method: 提出FasterSNN，将生物启发的LIF神经元与区域自适应卷积和多尺度脉冲注意力集成。

Result: 在基准数据集实验中，FasterSNN性能有竞争力，效率和稳定性显著提高。

Conclusion: FasterSNN有潜力用于实际AD筛查。

Abstract: Early diagnosis of Alzheimer's Disease (AD), especially at the mild cognitive
impairment (MCI) stage, is vital yet hindered by subjective assessments and the
high cost of multimodal imaging modalities. Although deep learning methods
offer automated alternatives, their energy inefficiency and computational
demands limit real-world deployment, particularly in resource-constrained
settings. As a brain-inspired paradigm, spiking neural networks (SNNs) are
inherently well-suited for modeling the sparse, event-driven patterns of neural
degeneration in AD, offering a promising foundation for interpretable and
low-power medical diagnostics. However, existing SNNs often suffer from weak
expressiveness and unstable training, which restrict their effectiveness in
complex medical tasks. To address these limitations, we propose FasterSNN, a
hybrid neural architecture that integrates biologically inspired LIF neurons
with region-adaptive convolution and multi-scale spiking attention. This design
enables sparse, efficient processing of 3D MRI while preserving diagnostic
accuracy. Experiments on benchmark datasets demonstrate that FasterSNN achieves
competitive performance with substantially improved efficiency and stability,
supporting its potential for practical AD screening. Our source code is
available at https://github.com/wuchangw/FasterSNN.

</details>


### [187] [Bias Analysis in Unconditional Image Generative Models](https://arxiv.org/abs/2506.09106)
*Xiaofeng Zhang,Michelle Lin,Simon Lacoste-Julien,Aaron Courville,Yash Goyal*

Main category: cs.CV

TL;DR: 研究生成式AI模型偏差，训练无条件图像生成模型分析偏差转移，发现检测到的属性转移小，属性转移对分类器敏感，强调评估偏差时需改进标注、审视评估框架和考虑属性社会复杂性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI模型广泛应用引发对代表性伤害和歧视性结果的担忧，现有研究未厘清偏差产生机制。

Method: 训练一组无条件图像生成模型，采用常用偏差评估框架研究训练和生成分布间的偏差转移。

Result: 检测到的属性转移小，属性转移对评估框架中用于标注生成图像的属性分类器敏感，尤其当决策边界处于高密度区域，这种分类器敏感性常见于非二元属性值。

Conclusion: 评估偏差时需要更具代表性的标注实践，更严格审视评估框架以理解其不足，认识属性的社会复杂性。

Abstract: The widespread adoption of generative AI models has raised growing concerns
about representational harm and potential discriminatory outcomes. Yet, despite
growing literature on this topic, the mechanisms by which bias emerges -
especially in unconditional generation - remain disentangled. We define the
bias of an attribute as the difference between the probability of its presence
in the observed distribution and its expected proportion in an ideal reference
distribution. In our analysis, we train a set of unconditional image generative
models and adopt a commonly used bias evaluation framework to study bias shift
between training and generated distributions. Our experiments reveal that the
detected attribute shifts are small. We find that the attribute shifts are
sensitive to the attribute classifier used to label generated images in the
evaluation framework, particularly when its decision boundaries fall in
high-density regions. Our empirical analysis indicates that this classifier
sensitivity is often observed in attributes values that lie on a spectrum, as
opposed to exhibiting a binary nature. This highlights the need for more
representative labeling practices, understanding the shortcomings through
greater scrutiny of evaluation frameworks, and recognizing the socially complex
nature of attributes when evaluating bias.

</details>


### [188] [Non-Contact Health Monitoring During Daily Personal Care Routines](https://arxiv.org/abs/2506.09718)
*Xulin Ma,Jiankai Tang,Zhang Jiang,Songqin Cheng,Yuanchun Shi,Dong LI,Xin Liu,Daniel McDuff,Xiaojing Liu,Yuntao Wang*

Main category: cs.CV

TL;DR: 提出LADH数据集，结合RGB和IR视频输入提升非接触生理监测准确性和鲁棒性，多任务学习可同时提升多生理指标性能。


<details>
  <summary>Details</summary>
Motivation: rPPG在高海拔长期个人护理场景应用面临环境光照变化、手部遮挡和动态面部姿势等挑战。

Method: 创建包含240个同步RGB和IR面部视频的LADH数据集，结合RGB和IR视频输入，采用多任务学习。

Result: 结合RGB和IR视频输入在心率估计中MAE为4.99 BPM，多任务学习可提升多生理指标性能。

Conclusion: 结合RGB和IR视频输入以及多任务学习有助于提升非接触生理监测性能，数据集和代码已开源。

Abstract: Remote photoplethysmography (rPPG) enables non-contact, continuous monitoring
of physiological signals and offers a practical alternative to traditional
health sensing methods. Although rPPG is promising for daily health monitoring,
its application in long-term personal care scenarios, such as mirror-facing
routines in high-altitude environments, remains challenging due to ambient
lighting variations, frequent occlusions from hand movements, and dynamic
facial postures. To address these challenges, we present LADH (Long-term
Altitude Daily Health), the first long-term rPPG dataset containing 240
synchronized RGB and infrared (IR) facial videos from 21 participants across
five common personal care scenarios, along with ground-truth PPG, respiration,
and blood oxygen signals. Our experiments demonstrate that combining RGB and IR
video inputs improves the accuracy and robustness of non-contact physiological
monitoring, achieving a mean absolute error (MAE) of 4.99 BPM in heart rate
estimation. Furthermore, we find that multi-task learning enhances performance
across multiple physiological indicators simultaneously. Dataset and code are
open at https://github.com/McJackTang/FusionVitals.

</details>


### [189] [Vision Matters: Simple Visual Perturbations Can Boost Multimodal Math Reasoning](https://arxiv.org/abs/2506.09736)
*Yuting Li,Lai Wei,Kaipeng Zheng,Jingyuan Huang,Linghe Kong,Lichao Sun,Weiran Huang*

Main category: cs.CV

TL;DR: 研究指出当前多模态大语言模型忽视视觉处理，提出简单视觉扰动框架提升感知鲁棒性，实验证明可提升数学推理性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型虽能生成准确视觉描述，但在推理时无法有效整合，语言模型用图像描述能有更好表现，因此需提升视觉处理能力。

Method: 提出视觉扰动框架，包括干扰项拼接、保主导混合和随机旋转三种扰动，可集成到现有后训练管道。

Result: 在多个数据集实验中数学推理性能持续提升，训练Qwen2.5 - VL - 7B有竞争力表现，消融研究分析了不同扰动策略效果。

Conclusion: 视觉扰动在多模态数学推理中起关键作用，更好的视觉处理是更好推理的开端。

Abstract: Despite the rapid progress of multimodal large language models (MLLMs), they
have largely overlooked the importance of visual processing. In a simple yet
revealing experiment, we interestingly find that language-only models, when
provided with image captions, can achieve comparable or even better performance
than MLLMs that consume raw visual inputs. This suggests that current MLLMs may
generate accurate visual descriptions but fail to effectively integrate them
during reasoning. Motivated by this, we propose a simple visual perturbation
framework that enhances perceptual robustness without requiring algorithmic
modifications or additional training data. Our approach introduces three
targeted perturbations: distractor concatenation, dominance-preserving mixup,
and random rotation, that can be easily integrated into existing post-training
pipelines including SFT, DPO, and GRPO. Through extensive experiments across
multiple datasets, we demonstrate consistent improvements in mathematical
reasoning performance, with gains comparable to those achieved through
algorithmic changes. Additionally, we achieve competitive performance among
open-source 7B RL-tuned models by training Qwen2.5-VL-7B with visual
perturbation. Through comprehensive ablation studies, we analyze the
effectiveness of different perturbation strategies, revealing that each
perturbation type contributes uniquely to different aspects of visual
reasoning. Our findings highlight the critical role of visual perturbation in
multimodal mathematical reasoning: better reasoning begins with better seeing.
Our code is available at https://github.com/YutingLi0606/Vision-Matters.

</details>


### [190] [PatchGuard: Adversarially Robust Anomaly Detection and Localization through Vision Transformers and Pseudo Anomalies](https://arxiv.org/abs/2506.09237)
*Mojtaba Nafez,Amirhossein Koochakian,Arad Maleki,Jafar Habibi,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: 本文提出PatchGuard方法，通过在ViT架构中引入带定位掩码的伪异常样本，提升异常检测和定位的对抗鲁棒性，实验显示其在对抗环境下性能大幅提升。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测和定位方法因训练数据局限，易受对抗攻击，需提升鲁棒性。

Method: 研究伪异常特性，分析注意力机制；利用前景感知伪异常，将其融入ViT框架，通过新损失函数进行对抗训练。

Result: 在工业和医疗数据集上实验表明，PatchGuard在对抗环境下AD性能提升53.2%，AL提升68.5%，非对抗环境下也有不错表现。

Conclusion: PatchGuard方法能有效提升异常检测和定位的对抗鲁棒性。

Abstract: Anomaly Detection (AD) and Anomaly Localization (AL) are crucial in fields
that demand high reliability, such as medical imaging and industrial
monitoring. However, current AD and AL approaches are often susceptible to
adversarial attacks due to limitations in training data, which typically
include only normal, unlabeled samples. This study introduces PatchGuard, an
adversarially robust AD and AL method that incorporates pseudo anomalies with
localization masks within a Vision Transformer (ViT)-based architecture to
address these vulnerabilities. We begin by examining the essential properties
of pseudo anomalies, and follow it by providing theoretical insights into the
attention mechanisms required to enhance the adversarial robustness of AD and
AL systems. We then present our approach, which leverages Foreground-Aware
Pseudo-Anomalies to overcome the deficiencies of previous anomaly-aware
methods. Our method incorporates these crafted pseudo-anomaly samples into a
ViT-based framework, with adversarial training guided by a novel loss function
designed to improve model robustness, as supported by our theoretical analysis.
Experimental results on well-established industrial and medical datasets
demonstrate that PatchGuard significantly outperforms previous methods in
adversarial settings, achieving performance gains of $53.2\%$ in AD and
$68.5\%$ in AL, while also maintaining competitive accuracy in non-adversarial
settings. The code repository is available at
https://github.com/rohban-lab/PatchGuard .

</details>


### [191] [ELBO-T2IAlign: A Generic ELBO-Based Method for Calibrating Pixel-level Text-Image Alignment in Diffusion Models](https://arxiv.org/abs/2506.09740)
*Qin Zhou,Zhiyang Zhang,Jinglong Wang,Xiaobin Li,Jing Zhang,Qian Yu,Lu Sheng,Dong Xu*

Main category: cs.CV

TL;DR: 本文提出用零样本指称图像分割评估扩散模型像素 - 文本对齐，分析了不对齐原因，提出 ELBO - T2IAlign 校准方法，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前方法假设扩散模型文本 - 图像完美对齐不符合实际情况，需评估和校准像素 - 文本对齐。

Method: 用零样本指称图像分割评估对齐；从训练数据偏差角度分析不对齐；提出基于似然证据下界（ELBO）的 ELBO - T2IAlign 校准方法。

Result: 发现小尺寸、遮挡或稀有对象类别的图像存在不对齐；提出的校准方法在图像分割和生成基准数据集实验中验证有效。

Conclusion: 提出的 ELBO - T2IAlign 方法训练自由且通用，能有效校准扩散模型的像素 - 文本对齐。

Abstract: Diffusion models excel at image generation. Recent studies have shown that
these models not only generate high-quality images but also encode text-image
alignment information through attention maps or loss functions. This
information is valuable for various downstream tasks, including segmentation,
text-guided image editing, and compositional image generation. However, current
methods heavily rely on the assumption of perfect text-image alignment in
diffusion models, which is not the case. In this paper, we propose using
zero-shot referring image segmentation as a proxy task to evaluate the
pixel-level image and class-level text alignment of popular diffusion models.
We conduct an in-depth analysis of pixel-text misalignment in diffusion models
from the perspective of training data bias. We find that misalignment occurs in
images with small sized, occluded, or rare object classes. Therefore, we
propose ELBO-T2IAlign, a simple yet effective method to calibrate pixel-text
alignment in diffusion models based on the evidence lower bound (ELBO) of
likelihood. Our method is training-free and generic, eliminating the need to
identify the specific cause of misalignment and works well across various
diffusion model architectures. Extensive experiments on commonly used benchmark
datasets on image segmentation and generation have verified the effectiveness
of our proposed calibration approach.

</details>


### [192] [UFM: A Simple Path towards Unified Dense Correspondence with Flow](https://arxiv.org/abs/2506.09278)
*Yuchen Zhang,Nikhil Keetha,Chenwei Lyu,Bhuvan Jhamb,Yutian Chen,Yuheng Qiu,Jay Karhade,Shreyas Jha,Yaoyu Hu,Deva Ramanan,Sebastian Scherer,Wenshan Wang*

Main category: cs.CV

TL;DR: 本文提出统一流与匹配模型UFM，在统一数据上训练，比现有方法更准确快速，证明统一训练可超越特定领域方法。


<details>
  <summary>Details</summary>
Motivation: 历史上宽基线场景和光流估计的密集对应问题分开处理，本文旨在开发统一模型解决该问题。

Method: 开发UFM模型，使用简单通用的Transformer架构直接回归(u,v)流，在源和目标图像中共同可见像素的统一数据上训练。

Result: UFM比最先进的光流方法（Unimatch）准确率高28%，比密集宽基线匹配器（RoMa）误差小62%、速度快6.7倍。

Conclusion: 统一训练能在两个领域超越特定方法，实现快速通用对应，为多模态、长距离和实时对应任务开辟新方向。

Abstract: Dense image correspondence is central to many applications, such as visual
odometry, 3D reconstruction, object association, and re-identification.
Historically, dense correspondence has been tackled separately for
wide-baseline scenarios and optical flow estimation, despite the common goal of
matching content between two images. In this paper, we develop a Unified Flow &
Matching model (UFM), which is trained on unified data for pixels that are
co-visible in both source and target images. UFM uses a simple, generic
transformer architecture that directly regresses the (u,v) flow. It is easier
to train and more accurate for large flows compared to the typical
coarse-to-fine cost volumes in prior work. UFM is 28% more accurate than
state-of-the-art flow methods (Unimatch), while also having 62% less error and
6.7x faster than dense wide-baseline matchers (RoMa). UFM is the first to
demonstrate that unified training can outperform specialized approaches across
both domains. This result enables fast, general-purpose correspondence and
opens new directions for multi-modal, long-range, and real-time correspondence
tasks.

</details>


### [193] [Inverting Black-Box Face Recognition Systems via Zero-Order Optimization in Eigenface Space](https://arxiv.org/abs/2506.09777)
*Anton Razzhigaev,Matvey Mikhalchuk,Klim Kireev,Igor Udovichenko,Andrey Kuznetsov,Aleksandr Petiushko*

Main category: cs.CV

TL;DR: 本文提出DarkerBB方法，在仅使用相似度分数的情况下，于PCA导出的特征脸空间进行零阶优化来重建彩色人脸，实验显示其在仅相似度设置下有最优验证准确率和有竞争力的查询效率。


<details>
  <summary>Details</summary>
Motivation: 解决仅使用相似度分数进行模型反演来重建人脸这一更具挑战性的场景，应对从黑盒识别模型重建人脸图像带来的隐私威胁。

Method: 提出DarkerBB方法，在PCA导出的特征脸空间进行零阶优化来重建彩色人脸。

Result: 在LFW、AgeDB - 30和CFP - FP基准测试中，DarkerBB在仅相似度设置下达到了最先进的验证准确率，且查询效率有竞争力。

Conclusion: DarkerBB方法在仅使用相似度分数的人脸重建场景中表现良好，具有一定优势。

Abstract: Reconstructing facial images from black-box recognition models poses a
significant privacy threat. While many methods require access to embeddings, we
address the more challenging scenario of model inversion using only similarity
scores. This paper introduces DarkerBB, a novel approach that reconstructs
color faces by performing zero-order optimization within a PCA-derived
eigenface space. Despite this highly limited information, experiments on LFW,
AgeDB-30, and CFP-FP benchmarks demonstrate that DarkerBB achieves
state-of-the-art verification accuracies in the similarity-only setting, with
competitive query efficiency.

</details>


### [194] [Lightweight Object Detection Using Quantized YOLOv4-Tiny for Emergency Response in Aerial Imagery](https://arxiv.org/abs/2506.09299)
*Sindhu Boddu,Arindam Mukherjee*

Main category: cs.CV

TL;DR: 提出针对应急响应航拍图像的轻量级、节能目标检测方案，用INT8后训练量化优化YOLOv4 - Tiny模型，在自构建数据集训练，对比YOLOv5 - small，结果表明量化后模型适用于低功耗边缘设备实时应急检测。


<details>
  <summary>Details</summary>
Motivation: 缺乏公开可用的无人机视角应急图像数据，需要为应急响应航拍图像提供轻量级、节能的目标检测解决方案。

Method: 部署YOLOv4 - Tiny模型，通过后训练量化至INT8精度，在自构建的含10820张标注图像的应急数据集上训练，与YOLOv5 - small在多个指标上对比评估。

Result: 量化后的YOLOv4 - Tiny检测性能相当，模型大小从22.5 MB降至6.4 MB，推理速度提升44%。

Conclusion: 量化后的YOLOv4 - Tiny模型非常适合在低功耗边缘设备上进行实时应急检测。

Abstract: This paper presents a lightweight and energy-efficient object detection
solution for aerial imagery captured during emergency response situations. We
focus on deploying the YOLOv4-Tiny model, a compact convolutional neural
network, optimized through post-training quantization to INT8 precision. The
model is trained on a custom-curated aerial emergency dataset, consisting of
10,820 annotated images covering critical emergency scenarios. Unlike prior
works that rely on publicly available datasets, we created this dataset
ourselves due to the lack of publicly available drone-view emergency imagery,
making the dataset itself a key contribution of this work. The quantized model
is evaluated against YOLOv5-small across multiple metrics, including mean
Average Precision (mAP), F1 score, inference time, and model size. Experimental
results demonstrate that the quantized YOLOv4-Tiny achieves comparable
detection performance while reducing the model size from 22.5 MB to 6.4 MB and
improving inference speed by 44\%. With a 71\% reduction in model size and a
44\% increase in inference speed, the quantized YOLOv4-Tiny model proves highly
suitable for real-time emergency detection on low-power edge devices.

</details>


### [195] [Q-SAM2: Accurate Quantization for Segment Anything Model 2](https://arxiv.org/abs/2506.09782)
*Nicola Farronato,Florian Scheidegger,Mattia Rigotti,Cristiano Malossi,Michele Magno,Haotong Qin*

Main category: cs.CV

TL;DR: 提出高效SAM2低比特量化方法Q - SAM2，实验证明其在保证高精度推理的同时显著提升效率，超越现有量化方案。


<details>
  <summary>Details</summary>
Motivation: SAM2计算和内存消耗大，在资源受限场景应用有挑战，需高效量化方法。

Method: 引入线性层校准方法用于低比特初始化，提出量化感知训练（QAT）管道抑制异常值，使网络适应量化阈值。

Result: Q - SAM2实现高精度推理且提升效率，超越现有量化方案，校准技术在训练后量化中也有效，mIoU精度提升达66%。

Conclusion: Q - SAM2是有效的高效SAM2量化方法，校准技术在不同量化场景都有效果。

Abstract: The Segment Anything Model 2 (SAM2) has gained significant attention as a
foundational approach for promptable image and video segmentation. However, its
expensive computational and memory consumption poses a severe challenge for its
application in resource-constrained scenarios. In this paper, we propose an
accurate low-bit quantization method for efficient SAM2, termed Q-SAM2. To
address the performance degradation caused by the singularities in weight and
activation distributions during quantization, Q-SAM2 introduces two novel
technical contributions. We first introduce a linear layer calibration method
for low-bit initialization of SAM2, which minimizes the Frobenius norm over a
small image batch to reposition weight distributions for improved quantization.
We then propose a Quantization-Aware Training (QAT) pipeline that applies
clipping to suppress outliers and allows the network to adapt to quantization
thresholds during training. Our comprehensive experiments demonstrate that
Q-SAM2 allows for highly accurate inference while substantially improving
efficiency. Both quantitative and visual results show that our Q-SAM2 surpasses
existing state-of-the-art general quantization schemes, especially for
ultra-low 2-bit quantization. While designed for quantization-aware training,
our proposed calibration technique also proves effective in post-training
quantization, achieving up to a 66% mIoU accuracy improvement over
non-calibrated models.

</details>


### [196] [DynaSplat: Dynamic-Static Gaussian Splatting with Hierarchical Motion Decomposition for Scene Reconstruction](https://arxiv.org/abs/2506.09836)
*Junli Deng,Ping Shi,Qipei Li,Jinyang Guo*

Main category: cs.CV

TL;DR: 提出DynaSplat方法用于动态场景重建，在精度、真实感等方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有计算机视觉中动态场景重建方案难以应对现实世界动态的复杂性，需要新方法。

Method: 通过动态 - 静态分离和分层运动建模扩展高斯溅射，包括融合变形偏移统计和2D运动流一致性分类场景元素，采用分层运动建模策略，集成基于物理的不透明度估计。

Result: 在具有挑战性的数据集上实验表明，DynaSplat在准确性和真实感上超越了现有替代方案。

Conclusion: DynaSplat为动态场景重建提供了更直观、紧凑和高效的途径。

Abstract: Reconstructing intricate, ever-changing environments remains a central
ambition in computer vision, yet existing solutions often crumble before the
complexity of real-world dynamics. We present DynaSplat, an approach that
extends Gaussian Splatting to dynamic scenes by integrating dynamic-static
separation and hierarchical motion modeling. First, we classify scene elements
as static or dynamic through a novel fusion of deformation offset statistics
and 2D motion flow consistency, refining our spatial representation to focus
precisely where motion matters. We then introduce a hierarchical motion
modeling strategy that captures both coarse global transformations and
fine-grained local movements, enabling accurate handling of intricate,
non-rigid motions. Finally, we integrate physically-based opacity estimation to
ensure visually coherent reconstructions, even under challenging occlusions and
perspective shifts. Extensive experiments on challenging datasets reveal that
DynaSplat not only surpasses state-of-the-art alternatives in accuracy and
realism but also provides a more intuitive, compact, and efficient route to
dynamic scene reconstruction.

</details>


### [197] [OctoNav: Towards Generalist Embodied Navigation](https://arxiv.org/abs/2506.09839)
*Chen Gao,Liankai Jin,Xingyu Peng,Jiazhao Zhang,Yue Deng,Annan Li,He Wang,Si Liu*

Main category: cs.CV

TL;DR: 本文提出OctoNav - Bench基准和OctoNav - R1方法，推动通用导航智能体发展，OctoNav - R1性能优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 先前导航研究分为不同任务和能力，数据集和方法单独设计，本文旨在构建能遵循自由形式指令的通用导航智能体。

Method: 提出OctoNav - Bench基准，通过设计的标注流程构建，创建TBA - CoT数据集；构建基于MLLMs的OctoNav - R1模型并适配为VLA类型；设计包含三个阶段的混合训练范式（HTP）。

Result: OctoNav - R1与先前方法相比表现出更优性能。

Conclusion: 提出的方法有助于在具身导航领域实现行动前思考，提升模型向通用智能体发展的推理能力。

Abstract: Embodied navigation stands as a foundation pillar within the broader pursuit
of embodied AI. However, previous navigation research is divided into different
tasks/capabilities, e.g., ObjNav, ImgNav and VLN, where they differ in task
objectives and modalities, making datasets and methods are designed
individually. In this work, we take steps toward generalist navigation agents,
which can follow free-form instructions that include arbitrary compounds of
multi-modal and multi-capability. To achieve this, we propose a large-scale
benchmark and corresponding method, termed OctoNav-Bench and OctoNav-R1.
Specifically, OctoNav-Bench features continuous environments and is constructed
via a designed annotation pipeline. We thoroughly craft instruction-trajectory
pairs, where instructions are diverse in free-form with arbitrary modality and
capability. Also, we construct a Think-Before-Action (TBA-CoT) dataset within
OctoNav-Bench to provide the thinking process behind actions. For OctoNav-R1,
we build it upon MLLMs and adapt it to a VLA-type model, which can produce
low-level actions solely based on 2D visual observations. Moreover, we design a
Hybrid Training Paradigm (HTP) that consists of three stages, i.e.,
Action-/TBA-SFT, Nav-GPRO, and Online RL stages. Each stage contains
specifically designed learning policies and rewards. Importantly, for TBA-SFT
and Nav-GRPO designs, we are inspired by the OpenAI-o1 and DeepSeek-R1, which
show impressive reasoning ability via thinking-before-answer. Thus, we aim to
investigate how to achieve thinking-before-action in the embodied navigation
field, to improve model's reasoning ability toward generalists. Specifically,
we propose TBA-SFT to utilize the TBA-CoT dataset to fine-tune the model as a
cold-start phrase and then leverage Nav-GPRO to improve its thinking ability.
Finally, OctoNav-R1 shows superior performance compared with previous methods.

</details>


### [198] [Learning to Align: Addressing Character Frequency Distribution Shifts in Handwritten Text Recognition](https://arxiv.org/abs/2506.09846)
*Panagiotis Kaliosis,John Pavlopoulos*

Main category: cs.CV

TL;DR: 针对手写文本识别中字符集和频率分布变化导致模型表现不佳的问题，提出新损失函数，结合Wasserstein距离，提升准确性和鲁棒性，还能在推理时改进现有模型，实验验证方法有效性并开源代码。


<details>
  <summary>Details</summary>
Motivation: 手写文本识别具有挑战性，字符集和频率分布随时间和地域变化，使在广泛异构语料库上训练的模型在特定子集上表现不佳。

Method: 提出一种结合预测文本字符频率分布与训练数据目标分布Wasserstein距离的新损失函数；在引导解码方案中将字符分布对齐作为评分函数以在推理时改进现有模型。

Result: 实验结果表明，在多个数据集和架构上该方法能有效提升泛化能力和性能。

Conclusion: 所提方法可有效提升手写文本识别模型在字符集和频率分布变化情况下的准确性和鲁棒性。

Abstract: Handwritten text recognition aims to convert visual input into
machine-readable text, and it remains challenging due to the evolving and
context-dependent nature of handwriting. Character sets change over time, and
character frequency distributions shift across historical periods or regions,
often causing models trained on broad, heterogeneous corpora to underperform on
specific subsets. To tackle this, we propose a novel loss function that
incorporates the Wasserstein distance between the character frequency
distribution of the predicted text and a target distribution empirically
derived from training data. By penalizing divergence from expected
distributions, our approach enhances both accuracy and robustness under
temporal and contextual intra-dataset shifts. Furthermore, we demonstrate that
character distribution alignment can also improve existing models at inference
time without requiring retraining by integrating it as a scoring function in a
guided decoding scheme. Experimental results across multiple datasets and
architectures confirm the effectiveness of our method in boosting
generalization and performance. We open source our code at
https://github.com/pkaliosis/fada.

</details>


### [199] [3D-Aware Vision-Language Models Fine-Tuning with Geometric Distillation](https://arxiv.org/abs/2506.09883)
*Seonho Lee,Jiho Choi,Inha Kang,Jiwook Kim,Junsung Park,Hyunjung Shim*

Main category: cs.CV

TL;DR: 提出几何蒸馏框架，注入几何线索到预训练视觉语言模型，提升3D空间推理能力，成本低。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在理解3D空间结构方面存在根本局限。

Method: 提出轻量级、无需注释的微调框架Geometric Distillation，从现成3D基础模型中提取三种信息注入预训练视觉语言模型。

Result: 在3D视觉语言推理和3D感知基准测试中始终优于先前方法，以更低计算成本实现更好3D空间推理。

Conclusion: 展示了将2D训练的视觉语言模型与3D理解相连接的可扩展且高效的途径。

Abstract: Vision-Language Models (VLMs) have shown remarkable performance on diverse
visual and linguistic tasks, yet they remain fundamentally limited in their
understanding of 3D spatial structures. We propose Geometric Distillation, a
lightweight, annotation-free fine-tuning framework that injects human-inspired
geometric cues into pretrained VLMs without modifying their architecture. By
distilling (1) sparse correspondences, (2) relative depth relations, and (3)
dense cost volumes from off-the-shelf 3D foundation models (e.g., MASt3R,
VGGT), our method shapes representations to be geometry-aware while remaining
compatible with natural image-text inputs. Through extensive evaluations on 3D
vision-language reasoning and 3D perception benchmarks, our method consistently
outperforms prior approaches, achieving improved 3D spatial reasoning with
significantly lower computational cost. Our work demonstrates a scalable and
efficient path to bridge 2D-trained VLMs with 3D understanding, opening up
wider use in spatially grounded multimodal tasks.

</details>


### [200] [HadaNorm: Diffusion Transformer Quantization through Mean-Centered Transformations](https://arxiv.org/abs/2506.09932)
*Marco Federici,Riccardo Del Chiaro,Boris van Breugel,Paul Whatmough,Markus Nagel*

Main category: cs.CV

TL;DR: 本文提出HadaNorm方法，缓解扩散模型量化时的离群值问题，实现更好的效率 - 性能权衡。


<details>
  <summary>Details</summary>
Motivation: 扩散模型内存和计算需求高，标准后训练量化（PTQ）方法处理离群值有困难，需改进量化方法。

Method: 提出HadaNorm线性变换，在应用Hadamard变换前对激活特征通道进行归一化，以缓解离群值。

Result: HadaNorm在变压器块各组件上持续降低量化误差。

Conclusion: 与现有技术相比，HadaNorm实现了更优的效率 - 性能权衡。

Abstract: Diffusion models represent the cutting edge in image generation, but their
high memory and computational demands hinder deployment on resource-constrained
devices. Post-Training Quantization (PTQ) offers a promising solution by
reducing the bitwidth of matrix operations. However, standard PTQ methods
struggle with outliers, and achieving higher compression often requires
transforming model weights and activations before quantization. In this work,
we propose HadaNorm, a novel linear transformation that extends existing
approaches and effectively mitigates outliers by normalizing activations
feature channels before applying Hadamard transformations, enabling more
aggressive activation quantization. We demonstrate that HadaNorm consistently
reduces quantization error across the various components of transformer blocks,
achieving superior efficiency-performance trade-offs when compared to
state-of-the-art methods.

</details>


### [201] [CausalVQA: A Physically Grounded Causal Reasoning Benchmark for Video Models](https://arxiv.org/abs/2506.09943)
*Aaron Foss,Chloe Evans,Sasha Mitts,Koustuv Sinha,Ammar Rizvi,Justine T. Kao*

Main category: cs.CV

TL;DR: 介绍视频问答基准数据集CausalVQA，测试模型对物理世界因果关系理解，发现当前模型表现远低于人类。


<details>
  <summary>Details</summary>
Motivation: 现有VQA基准要么侧重表面感知理解，要么是模拟环境问题，CausalVQA填补真实场景因果推理问题空白。

Method: 设计CausalVQA数据集，包含五种问题类型，设置质量控制机制防止模型走捷径。

Result: 当前前沿多模态模型在该基准上表现远低于人类，尤其在预期和假设问题上。

Conclusion: 当前系统在利用时空推理、物理原理理解和替代方案理解以进行准确预测方面面临挑战。

Abstract: We introduce CausalVQA, a benchmark dataset for video question answering
(VQA) composed of question-answer pairs that probe models' understanding of
causality in the physical world. Existing VQA benchmarks either tend to focus
on surface perceptual understanding of real-world videos, or on narrow physical
reasoning questions created using simulation environments. CausalVQA fills an
important gap by presenting challenging questions that are grounded in
real-world scenarios, while focusing on models' ability to predict the likely
outcomes of different actions and events through five question types:
counterfactual, hypothetical, anticipation, planning and descriptive. We
designed quality control mechanisms that prevent models from exploiting trivial
shortcuts, requiring models to base their answers on deep visual understanding
instead of linguistic cues. We find that current frontier multimodal models
fall substantially below human performance on the benchmark, especially on
anticipation and hypothetical questions. This highlights a challenge for
current systems to leverage spatial-temporal reasoning, understanding of
physical principles, and comprehension of possible alternatives to make
accurate predictions in real-world settings.

</details>


### [202] [UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal Gaussian Splatting](https://arxiv.org/abs/2506.09952)
*Ziyi Wang,Yanran Zhang,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 提出统一预训练方法UniPre3D，可无缝应用于任意规模点云和任意架构3D模型，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 点云数据规模多样性给3D视觉统一表示学习技术带来挑战，现缺少统一3D模型和对物体与场景级点云均有效的预训练方法。

Method: 以预测高斯基元为预训练任务，采用可微高斯溅射渲染图像实现像素级监督和端到端优化，集成预训练图像模型的2D特征调控任务复杂度并引导模型关注几何结构。

Result: 通过对多种物体和场景级任务的大量实验，验证了方法的普遍有效性。

Conclusion: UniPre3D是一种有效的统一预训练方法，可用于不同规模点云和架构的3D模型。

Abstract: The scale diversity of point cloud data presents significant challenges in
developing unified representation learning techniques for 3D vision. Currently,
there are few unified 3D models, and no existing pre-training method is equally
effective for both object- and scene-level point clouds. In this paper, we
introduce UniPre3D, the first unified pre-training method that can be
seamlessly applied to point clouds of any scale and 3D models of any
architecture. Our approach predicts Gaussian primitives as the pre-training
task and employs differentiable Gaussian splatting to render images, enabling
precise pixel-level supervision and end-to-end optimization. To further
regulate the complexity of the pre-training task and direct the model's focus
toward geometric structures, we integrate 2D features from pre-trained image
models to incorporate well-established texture knowledge. We validate the
universal effectiveness of our proposed method through extensive experiments
across a variety of object- and scene-level tasks, using diverse point cloud
models as backbones. Code is available at https://github.com/wangzy22/UniPre3D.

</details>


### [203] [Outside Knowledge Conversational Video (OKCV) Dataset -- Dialoguing over Videos](https://arxiv.org/abs/2506.09953)
*Benjamin Reichman,Constantin Patsch,Jack Truxal,Atishay Jain,Larry Heck*

Main category: cs.CV

TL;DR: 文章提出一个用于视觉基础对话任务的数据集，包含视频和人工标注对话，还给出基线并指出未来挑战。


<details>
  <summary>Details</summary>
Motivation: 将外部知识视觉问答任务拓展到基于视频的视觉基础对话场景，考虑对话上下文和外部知识。

Method: 引入包含2017个视频、5986个人工标注对话及40954个对话轮次的数据集。

Result: 提供了在数据集上评估的基线。

Conclusion: 指出该任务存在的未来挑战，数据集已公开。

Abstract: In outside knowledge visual question answering (OK-VQA), the model must
identify relevant visual information within an image and incorporate external
knowledge to accurately respond to a question. Extending this task to a
visually grounded dialogue setting based on videos, a conversational model must
both recognize pertinent visual details over time and answer questions where
the required information is not necessarily present in the visual information.
Moreover, the context of the overall conversation must be considered for the
subsequent dialogue. To explore this task, we introduce a dataset comprised of
$2,017$ videos with $5,986$ human-annotated dialogues consisting of $40,954$
interleaved dialogue turns. While the dialogue context is visually grounded in
specific video segments, the questions further require external knowledge that
is not visually present. Thus, the model not only has to identify relevant
video parts but also leverage external knowledge to converse within the
dialogue. We further provide several baselines evaluated on our dataset and
show future challenges associated with this task. The dataset is made publicly
available here: https://github.com/c-patsch/OKCV.

</details>


### [204] [Vision Generalist Model: A Survey](https://arxiv.org/abs/2506.09954)
*Ziyi Wang,Yongming Rao,Shuofeng Sun,Xinrun Liu,Yi Wei,Xumin Yu,Zuyan Liu,Yanbo Wang,Hongmin Liu,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 本文对视觉通用模型进行全面概述，介绍背景、框架设计、相关领域联系，给出应用场景、挑战及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 通用模型在自然语言处理取得成功，吸引研究者将其应用于计算机视觉，但视觉任务输入输出多样，难以统一表示，因此需对视觉通用模型进行全面概述。

Method: 先回顾背景，包括数据集、任务和基准；再研究现有研究提出的框架设计及提升性能的技术；还介绍相关领域及其联系。

Result: 提供了视觉通用模型的全面信息，包括背景、框架、相关领域联系等。

Conclusion: 给出真实应用场景，分析现存挑战并指出未来研究方向。

Abstract: Recently, we have witnessed the great success of the generalist model in
natural language processing. The generalist model is a general framework
trained with massive data and is able to process various downstream tasks
simultaneously. Encouraged by their impressive performance, an increasing
number of researchers are venturing into the realm of applying these models to
computer vision tasks. However, the inputs and outputs of vision tasks are more
diverse, and it is difficult to summarize them as a unified representation. In
this paper, we provide a comprehensive overview of the vision generalist
models, delving into their characteristics and capabilities within the field.
First, we review the background, including the datasets, tasks, and benchmarks.
Then, we dig into the design of frameworks that have been proposed in existing
research, while also introducing the techniques employed to enhance their
performance. To better help the researchers comprehend the area, we take a
brief excursion into related domains, shedding light on their interconnections
and potential synergies. To conclude, we provide some real-world application
scenarios, undertake a thorough examination of the persistent challenges, and
offer insights into possible directions for future research endeavors.

</details>


### [205] [Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing](https://arxiv.org/abs/2506.09965)
*Junfei Wu,Jian Guan,Kaituo Feng,Qiang Liu,Shu Wu,Liang Wang,Wei Wu,Tieniu Tan*

Main category: cs.CV

TL;DR: 提出画图空间推理范式提升LVLMs多模态推理能力，模型VILASR在多种空间推理基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LVLMs多模态推理方法以文本为中心，在空间推理任务中有局限，需新方法解决。

Method: 提出画图空间推理范式，赋予模型基本绘图操作，开发三阶段训练框架。

Result: 模型VILASR在多种空间推理基准测试中平均提升18.4%，优于现有方法。

Conclusion: 画图空间推理范式能有效提升LVLMs在空间推理任务中的性能。

Abstract: As textual reasoning with large language models (LLMs) has advanced
significantly, there has been growing interest in enhancing the multimodal
reasoning capabilities of large vision-language models (LVLMs). However,
existing methods primarily approach multimodal reasoning in a straightforward,
text-centric manner, where both reasoning and answer derivation are conducted
purely through text, with the only difference being the presence of multimodal
input. As a result, these methods often encounter fundamental limitations in
spatial reasoning tasks that demand precise geometric understanding and
continuous spatial tracking-capabilities that humans achieve through mental
visualization and manipulation. To address the limitations, we propose drawing
to reason in space, a novel paradigm that enables LVLMs to reason through
elementary drawing operations in the visual space. By equipping models with
basic drawing operations, including annotating bounding boxes and drawing
auxiliary lines, we empower them to express and analyze spatial relationships
through direct visual manipulation, meanwhile avoiding the performance ceiling
imposed by specialized perception tools in previous tool-integrated reasoning
approaches. To cultivate this capability, we develop a three-stage training
framework: cold-start training with synthetic data to establish basic drawing
abilities, reflective rejection sampling to enhance self-reflection behaviors,
and reinforcement learning to directly optimize for target rewards. Extensive
experiments demonstrate that our model, named VILASR, consistently outperforms
existing methods across diverse spatial reasoning benchmarks, involving maze
navigation, static spatial reasoning, video-based reasoning, and
multi-view-based reasoning tasks, with an average improvement of 18.4%.

</details>


### [206] [InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio Conditions](https://arxiv.org/abs/2506.09984)
*Zhenzhi Wang,Jiaqi Yang,Jianwen Jiang,Chao Liang,Gaojie Lin,Zerong Zheng,Ceyuan Yang,Dahua Lin*

Main category: cs.CV

TL;DR: 提出新框架实现多概念可控的以人类为中心的视频高质量生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法多为单主体动画且全局注入条件，无法精确控制多概念，阻碍应用。

Method: 摒弃单实体假设，引入新框架，用掩码预测器推断布局信息，迭代注入局部音频条件。

Result: 实证结果和消融研究验证了显式布局控制相比隐式及其他方法的有效性。

Conclusion: 该框架能实现多概念人类中心视频的高质量可控生成。

Abstract: End-to-end human animation with rich multi-modal conditions, e.g., text,
image and audio has achieved remarkable advancements in recent years. However,
most existing methods could only animate a single subject and inject conditions
in a global manner, ignoring scenarios that multiple concepts could appears in
the same video with rich human-human interactions and human-object
interactions. Such global assumption prevents precise and per-identity control
of multiple concepts including humans and objects, therefore hinders
applications. In this work, we discard the single-entity assumption and
introduce a novel framework that enforces strong, region-specific binding of
conditions from modalities to each identity's spatiotemporal footprint. Given
reference images of multiple concepts, our method could automatically infer
layout information by leveraging a mask predictor to match appearance cues
between the denoised video and each reference appearance. Furthermore, we
inject local audio condition into its corresponding region to ensure
layout-aligned modality matching in a iterative manner. This design enables the
high-quality generation of controllable multi-concept human-centric videos.
Empirical results and ablation studies validate the effectiveness of our
explicit layout control for multi-modal conditions compared to implicit
counterparts and other existing methods.

</details>


### [207] [HopaDIFF: Holistic-Partial Aware Fourier Conditioned Diffusion for Referring Human Action Segmentation in Multi-Person Scenarios](https://arxiv.org/abs/2506.09650)
*Kunyu Peng,Junchao Huang,Xiangsheng Huang,Di Wen,Junwei Zheng,Yufan Chen,Kailun Yang,Jiamin Wu,Chongqing Hao,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 本文提出文本参考引导的多人动作分割方法，构建RHAS133数据集，提出HopaDIFF框架并取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要处理单人固定动作序列活动，忽略多人场景，本文旨在解决多人场景下文本参考引导的人类动作分割问题。

Method: 构建RHAS133数据集，提出整体 - 局部感知的傅里叶条件扩散框架HopaDIFF，利用新型交叉输入门注意力xLSTM和傅里叶条件。

Result: 在RHAS133的不同评估设置中，HopaDIFF取得了SOTA结果。

Conclusion: 提出的方法有效解决了多人场景下文本参考引导的人类动作分割问题，代码已开源。

Abstract: Action segmentation is a core challenge in high-level video understanding,
aiming to partition untrimmed videos into segments and assign each a label from
a predefined action set. Existing methods primarily address single-person
activities with fixed action sequences, overlooking multi-person scenarios. In
this work, we pioneer textual reference-guided human action segmentation in
multi-person settings, where a textual description specifies the target person
for segmentation. We introduce the first dataset for Referring Human Action
Segmentation, i.e., RHAS133, built from 133 movies and annotated with 137
fine-grained actions with 33h video data, together with textual descriptions
for this new task. Benchmarking existing action recognition methods on RHAS133
using VLM-based feature extractors reveals limited performance and poor
aggregation of visual cues for the target person. To address this, we propose a
holistic-partial aware Fourier-conditioned diffusion framework, i.e., HopaDIFF,
leveraging a novel cross-input gate attentional xLSTM to enhance
holistic-partial long-range reasoning and a novel Fourier condition to
introduce more fine-grained control to improve the action segmentation
generation. HopaDIFF achieves state-of-the-art results on RHAS133 in diverse
evaluation settings. The code is available at
https://github.com/KPeng9510/HopaDIFF.git.

</details>


### [208] [EditInspector: A Benchmark for Evaluation of Text-Guided Image Edits](https://arxiv.org/abs/2506.09988)
*Ron Yosef,Moran Yanuka,Yonatan Bitton,Dani Lischinski*

Main category: cs.CV

TL;DR: 本文引入EditInspector基准评估文本引导图像编辑，发现现有模型评估能力不足，并提出两种新方法。


<details>
  <summary>Details</summary>
Motivation: 随着文本引导图像编辑流行，需要综合框架验证和评估编辑质量。

Method: 引入基于人工标注的EditInspector基准评估模型，提出两种新方法。

Result: 当前模型难以全面评估编辑，描述变化时易产生幻觉，新方法在工件检测和差异描述生成上优于现有模型。

Conclusion: 新提出的两种方法能解决现有模型评估文本引导图像编辑的挑战。

Abstract: Text-guided image editing, fueled by recent advancements in generative AI, is
becoming increasingly widespread. This trend highlights the need for a
comprehensive framework to verify text-guided edits and assess their quality.
To address this need, we introduce EditInspector, a novel benchmark for
evaluation of text-guided image edits, based on human annotations collected
using an extensive template for edit verification. We leverage EditInspector to
evaluate the performance of state-of-the-art (SoTA) vision and language models
in assessing edits across various dimensions, including accuracy, artifact
detection, visual quality, seamless integration with the image scene, adherence
to common sense, and the ability to describe edit-induced changes. Our findings
indicate that current models struggle to evaluate edits comprehensively and
frequently hallucinate when describing the changes. To address these
challenges, we propose two novel methods that outperform SoTA models in both
artifact detection and difference caption generation.

</details>


### [209] [Text-Aware Image Restoration with Diffusion Models](https://arxiv.org/abs/2506.09993)
*Jaewon Min,Jin Hyeon Kim,Paul Hyunbin Cho,Jaeeun Lee,Jihye Park,Minkyu Park,Sangpil Kim,Hyunhee Park,Seungryong Kim*

Main category: cs.CV

TL;DR: 本文提出文本感知图像恢复任务TAIR，构建SA - Text基准，提出TeReDiff框架，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的图像恢复方法在恢复退化图像中文本区域时存在文本幻觉问题，难以保证文本保真度。

Method: 提出文本感知图像恢复任务TAIR；构建大规模SA - Text基准；提出多任务扩散框架TeReDiff，将扩散模型内部特征集成到文本检测模块进行联合训练，提取文本表示作为去噪提示。

Result: 实验表明该方法始终优于现有恢复方法，在文本识别准确率上有显著提升。

Conclusion: 所提出的任务、基准和框架在图像恢复中保证文本保真度方面效果良好，性能优于当前先进方法。

Abstract: Image restoration aims to recover degraded images. However, existing
diffusion-based restoration methods, despite great success in natural image
restoration, often struggle to faithfully reconstruct textual regions in
degraded images. Those methods frequently generate plausible but incorrect
text-like patterns, a phenomenon we refer to as text-image hallucination. In
this paper, we introduce Text-Aware Image Restoration (TAIR), a novel
restoration task that requires the simultaneous recovery of visual contents and
textual fidelity. To tackle this task, we present SA-Text, a large-scale
benchmark of 100K high-quality scene images densely annotated with diverse and
complex text instances. Furthermore, we propose a multi-task diffusion
framework, called TeReDiff, that integrates internal features from diffusion
models into a text-spotting module, enabling both components to benefit from
joint training. This allows for the extraction of rich text representations,
which are utilized as prompts in subsequent denoising steps. Extensive
experiments demonstrate that our approach consistently outperforms
state-of-the-art restoration methods, achieving significant gains in text
recognition accuracy. See our project page: https://cvlab-kaist.github.io/TAIR/

</details>


### [210] [CINeMA: Conditional Implicit Neural Multi-Modal Atlas for a Spatio-Temporal Representation of the Perinatal Brain](https://arxiv.org/abs/2506.09668)
*Maik Dannecker,Vasiliki Sideri-Lampretsa,Sophie Starck,Angeline Mihailov,Mathieu Milh,Nadine Girard,Guillaume Auzias,Daniel Rueckert*

Main category: cs.CV

TL;DR: 提出CINeMA框架创建高分辨率时空多模态脑图谱，适用于低数据场景，优于现有方法并开源代码和图谱。


<details>
  <summary>Details</summary>
Motivation: 传统脑图谱和深度学习方法依赖大量数据，研究病理脑时数据稀缺，需新方法。

Method: 提出CINeMA框架，在潜在空间操作，避免密集图像配准。

Result: CINeMA在准确性、效率和通用性上超越现有方法，支持下游任务和数据增强。

Conclusion: CINeMA是推进脑研究的有力工具。

Abstract: Magnetic resonance imaging of fetal and neonatal brains reveals rapid
neurodevelopment marked by substantial anatomical changes unfolding within
days. Studying this critical stage of the developing human brain, therefore,
requires accurate brain models-referred to as atlases-of high spatial and
temporal resolution. To meet these demands, established traditional atlases and
recently proposed deep learning-based methods rely on large and comprehensive
datasets. This poses a major challenge for studying brains in the presence of
pathologies for which data remains scarce. We address this limitation with
CINeMA (Conditional Implicit Neural Multi-Modal Atlas), a novel framework for
creating high-resolution, spatio-temporal, multimodal brain atlases, suitable
for low-data settings. Unlike established methods, CINeMA operates in latent
space, avoiding compute-intensive image registration and reducing atlas
construction times from days to minutes. Furthermore, it enables flexible
conditioning on anatomical features including GA, birth age, and pathologies
like ventriculomegaly (VM) and agenesis of the corpus callosum (ACC). CINeMA
supports downstream tasks such as tissue segmentation and age prediction
whereas its generative properties enable synthetic data creation and
anatomically informed data augmentation. Surpassing state-of-the-art methods in
accuracy, efficiency, and versatility, CINeMA represents a powerful tool for
advancing brain research. We release the code and atlases at
https://github.com/m-dannecker/CINeMA.

</details>


### [211] [Adding simple structure at inference improves Vision-Language Compositionality](https://arxiv.org/abs/2506.09691)
*Imanol Miranda,Ander Salaberria,Eneko Agirre,Gorka Azkune*

Main category: cs.CV

TL;DR: 本文提出在推理时添加简单结构以提升双编码器视觉 - 语言模型（VLM）的视觉 - 语言组合能力，实验证明该方法无需训练即可提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有双编码器 VLM 存在组合性问题，推理时间技术受关注少，旨在提升其在图像 - 文本检索任务中的性能。

Method: 推理时将图像分割成小作物，提取文本片段，用 VLM 找到图像作物与文本片段的匹配，最后聚合相似度得到最终图像 - 文本相似度。

Result: 在控制和自然数据集上评估发现，该方法无需训练就能持续提升 VLM 性能，在属性 - 对象绑定方面效果尤佳。

Conclusion: 处理图像作物对性能提升至关重要，同时确定了进一步改进推理时间方法的特定领域。

Abstract: Dual encoder Vision-Language Models (VLM) such as CLIP are widely used for
image-text retrieval tasks. However, those models struggle with
compositionality, showing a bag-of-words-like behavior that limits their
retrieval performance. Many different training approaches have been proposed to
improve the vision-language compositionality capabilities of those models. In
comparison, inference-time techniques have received little attention. In this
paper, we propose to add simple structure at inference, where, given an image
and a caption: i) we divide the image into different smaller crops, ii) we
extract text segments, capturing objects, attributes and relations, iii) using
a VLM, we find the image crops that better align with text segments obtaining
matches, and iv) we compute the final image-text similarity aggregating the
individual similarities of the matches. Based on various popular dual encoder
VLMs, we evaluate our approach in controlled and natural datasets for VL
compositionality. We find that our approach consistently improves the
performance of evaluated VLMs without any training, which shows the potential
of inference-time techniques. The results are especially good for
attribute-object binding as shown in the controlled dataset. As a result of an
extensive analysis: i) we show that processing image crops is actually
essential for the observed gains in performance, and ii) we identify specific
areas to further improve inference-time approaches.

</details>


### [212] [Kvasir-VQA-x1: A Multimodal Dataset for Medical Reasoning and Robust MedVQA in Gastrointestinal Endoscopy](https://arxiv.org/abs/2506.09958)
*Sushant Gautam,Michael A. Riegler,Pål Halvorsen*

Main category: cs.CV

TL;DR: 介绍用于胃肠内窥镜的大规模数据集Kvasir - VQA - x1，通过生成新问答对、引入视觉增强等提供更具挑战性和临床相关性的基准，推动临床多模态AI系统发展。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉问答数据集缺乏临床复杂性和视觉多样性，限制了临床决策支持系统发展，需新数据集解决问题。

Method: 用大语言模型系统生成按复杂度分层的新问答对，引入模拟常见成像伪影的视觉增强，设置两个评估轨道。

Result: 创建了Kvasir - VQA - x1数据集，可用于标准VQA性能和模型抗视觉扰动鲁棒性评估。

Conclusion: Kvasir - VQA - x1为临床多模态AI系统开发提供更具挑战性和临床相关性的基准，数据集符合FAIR数据原则，是有价值资源。

Abstract: Medical Visual Question Answering (MedVQA) is a promising field for
developing clinical decision support systems, yet progress is often limited by
the available datasets, which can lack clinical complexity and visual
diversity. To address these gaps, we introduce Kvasir-VQA-x1, a new,
large-scale dataset for gastrointestinal (GI) endoscopy. Our work significantly
expands upon the original Kvasir-VQA by incorporating 159,549 new
question-answer pairs that are designed to test deeper clinical reasoning. We
developed a systematic method using large language models to generate these
questions, which are stratified by complexity to better assess a model's
inference capabilities. To ensure our dataset prepares models for real-world
clinical scenarios, we have also introduced a variety of visual augmentations
that mimic common imaging artifacts. The dataset is structured to support two
main evaluation tracks: one for standard VQA performance and another to test
model robustness against these visual perturbations. By providing a more
challenging and clinically relevant benchmark, Kvasir-VQA-x1 aims to accelerate
the development of more reliable and effective multimodal AI systems for use in
clinical settings. The dataset is fully accessible and adheres to FAIR data
principles, making it a valuable resource for the wider research community.
Code and data: https://github.com/Simula/Kvasir-VQA-x1 and
https://huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1

</details>


### [213] [A Shortcut-aware Video-QA Benchmark for Physical Understanding via Minimal Video Pairs](https://arxiv.org/abs/2506.09987)
*Benno Krojer,Mojtaba Komeili,Candace Ross,Quentin Garrido,Koustuv Sinha,Nicolas Ballas,Mahmoud Assran*

Main category: cs.CV

TL;DR: 本文提出Minimal Video Pairs (MVP)基准，用于评估视频语言模型物理理解能力，减少因捷径解决方案导致的分数膨胀问题。


<details>
  <summary>Details</summary>
Motivation: 现有评估视频语言模型时空理解和推理能力的基准，易因基于表面视觉或文本线索的捷径解决方案导致分数膨胀，需要准确评估模型性能。

Method: 引入MVP基准，包含55K高质量多选视频QA示例，来自九个视频数据源。每个样本有最小变化对，模型需正确回答一对中的两个示例。

Result: 人类在MVP上的表现为92.9%，最佳开源的最先进视频语言模型达到40.2%，随机表现为25%。

Conclusion: MVP基准可有效减少基于表面线索的捷径解决方案，更准确地评估视频语言模型的物理理解能力。

Abstract: Existing benchmarks for assessing the spatio-temporal understanding and
reasoning abilities of video language models are susceptible to score inflation
due to the presence of shortcut solutions based on superficial visual or
textual cues. This paper mitigates the challenges in accurately assessing model
performance by introducing the Minimal Video Pairs (MVP) benchmark, a simple
shortcut-aware video QA benchmark for assessing the physical understanding of
video language models. The benchmark is comprised of 55K high-quality
multiple-choice video QA examples focusing on physical world understanding.
Examples are curated from nine video data sources, spanning first-person
egocentric and exocentric videos, robotic interaction data, and cognitive
science intuitive physics benchmarks. To mitigate shortcut solutions that rely
on superficial visual or textual cues and biases, each sample in MVP has a
minimal-change pair -- a visually similar video accompanied by an identical
question but an opposing answer. To answer a question correctly, a model must
provide correct answers for both examples in the minimal-change pair; as such,
models that solely rely on visual or textual biases would achieve below random
performance. Human performance on MVP is 92.9\%, while the best open-source
state-of-the-art video-language model achieves 40.2\% compared to random
performance at 25\%.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [214] [Regularizing Learnable Feature Extraction for Automatic Speech Recognition](https://arxiv.org/abs/2506.09804)
*Peter Vieting,Maximilian Kannen,Benedikt Hilmes,Ralf Schlüter,Hermann Ney*

Main category: eess.AS

TL;DR: 研究自动语音识别系统中可学习特征提取前端的正则化方法，结合两种正则化方法缩小与传统特征的性能差距。


<details>
  <summary>Details</summary>
Motivation: 神经网络前端在自动语音识别系统中虽可直接训练适配声学模型，但易过拟合，性能不如传统方法，因此研究其正则化方法。

Method: 研究音频扰动方法，指出标准SpecAugment的局限性并提出在STFT域进行掩码的改进方法，最后结合两种正则化方法。

Result: 音频扰动方法对可学习特征有更大相对改进，提出的STFT域掩码方法有效，结合两种方法缩小了传统特征和可学习特征的性能差距。

Conclusion: 所研究的正则化方法能有效提升可学习特征提取前端在自动语音识别系统中的性能。

Abstract: Neural front-ends are an appealing alternative to traditional, fixed feature
extraction pipelines for automatic speech recognition (ASR) systems since they
can be directly trained to fit the acoustic model. However, their performance
often falls short compared to classical methods, which we show is largely due
to their increased susceptibility to overfitting. This work therefore
investigates regularization methods for training ASR models with learnable
feature extraction front-ends. First, we examine audio perturbation methods and
show that larger relative improvements can be obtained for learnable features.
Additionally, we identify two limitations in the standard use of SpecAugment
for these front-ends and propose masking in the short time Fourier transform
(STFT)-domain as a simple but effective modification to address these
challenges. Finally, integrating both regularization approaches effectively
closes the performance gap between traditional and learnable features.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [215] [Integrated Analysis for Electronic Health Records with Structured and Sporadic Missingness](https://arxiv.org/abs/2506.09208)
*Jianbin Tan,Yan Zhang,Chuan Hong,T. Tony Cai,Tianxi Cai,Anru R. Zhang*

Main category: stat.AP

TL;DR: 提出针对电子健康记录（EHR）结构化和偶发性缺失数据的插补方法Macomss，经模拟研究和实际数据集验证表现优于现有方法，有推进人群健康研究潜力。


<details>
  <summary>Details</summary>
Motivation: 解决异构EHR数据集集成用于下游临床应用时的结构化和偶发性缺失问题，提升数据效用和对人群健康的理解。

Method: 先阐述EHR数据集成分析中的缺失机制，引入插补框架Macomss并给出理论保证，通过模拟研究和杜克大学健康系统（DUHS）数据集验证性能。

Result: 模拟研究表明该方法优于现有插补方法，在DUHS三家医院数据集上，Macomss多数情况下插补误差最低，下游预测性能良好。

Conclusion: 提供了理论有保障且实用的插补方法，可实现多EHR数据集准确可靠的集成分析，有推进人群健康研究的潜力。

Abstract: Objectives: We propose a novel imputation method tailored for Electronic
Health Records (EHRs) with structured and sporadic missingness. Such
missingness frequently arises in the integration of heterogeneous EHR datasets
for downstream clinical applications. By addressing these gaps, our method
provides a practical solution for integrated analysis, enhancing data utility
and advancing the understanding of population health.
  Materials and Methods: We begin by demonstrating structured and sporadic
missing mechanisms in the integrated analysis of EHR data. Following this, we
introduce a novel imputation framework, Macomss, specifically designed to
handle structurally and heterogeneously occurring missing data. We establish
theoretical guarantees for Macomss, ensuring its robustness in preserving the
integrity and reliability of integrated analyses. To assess its empirical
performance, we conduct extensive simulation studies that replicate the complex
missingness patterns observed in real-world EHR systems, complemented by
validation using EHR datasets from the Duke University Health System (DUHS).
  Results: Simulation studies show that our approach consistently outperforms
existing imputation methods. Using datasets from three hospitals within DUHS,
Macomss achieves the lowest imputation errors for missing data in most cases
and provides superior or comparable downstream prediction performance compared
to benchmark methods.
  Conclusions: We provide a theoretically guaranteed and practically meaningful
method for imputing structured and sporadic missing data, enabling accurate and
reliable integrated analysis across multiple EHR datasets. The proposed
approach holds significant potential for advancing research in population
health.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [216] [Empirical Quantification of Spurious Correlations in Malware Detection](https://arxiv.org/abs/2506.09662)
*Bianca Perasso,Ludovico Lozza,Andrea Ponte,Luca Demetrio,Luca Oneto,Fabio Roli*

Main category: cs.CR

TL;DR: 研究虚假关联对深度学习恶意软件检测的影响，对两个端到端模型排名以确定更适合投入生产的模型。


<details>
  <summary>Details</summary>
Motivation: 端到端深度学习在恶意软件检测中利用虚假关联取得成果，此前工作未量化其对决策的影响，需深入了解虚假关联的影响。

Method: 在小规模平衡数据集上进行分析。

Result: 揭示模型对编译器留下的空白空间的依赖程度，降低了编译代码的相关性，并对两个端到端模型进行排名。

Conclusion: 可根据排名更好地选择适合投入生产的模型。

Abstract: End-to-end deep learning exhibits unmatched performance for detecting
malware, but such an achievement is reached by exploiting spurious correlations
-- features with high relevance at inference time, but known to be useless
through domain knowledge. While previous work highlighted that deep networks
mainly focus on metadata, none investigated the phenomenon further, without
quantifying their impact on the decision. In this work, we deepen our
understanding of how spurious correlation affects deep learning for malware
detection by highlighting how much models rely on empty spaces left by the
compiler, which diminishes the relevance of the compiled code. Through our
seminal analysis on a small-scale balanced dataset, we introduce a ranking of
two end-to-end models to better understand which is more suitable to be put in
production.

</details>


### [217] [What is the Cost of Differential Privacy for Deep Learning-Based Trajectory Generation?](https://arxiv.org/abs/2506.09312)
*Erik Buchholz,Natasha Fernandes,David D. Nguyen,Alsharif Abuadbba,Surya Nepal,Salil S. Kanhere*

Main category: cs.CR

TL;DR: 本文研究在深度学习生成模型中实施差分隐私（DP）的效用成本，评估DP - SGD影响、提出新DP机制、分析模型类型影响，结果表明DP轨迹生成仍具挑战，正式隐私保证目前仅适用于大数据集和特定用例。


<details>
  <summary>Details</summary>
Motivation: 位置轨迹含敏感信息，现有深度学习生成模型缺乏正式隐私保证，需研究实施DP的效用成本。

Method: 通过两个数据集和十一个效用指标，评估DP - SGD对生成模型效用的影响；提出新的用于条件生成的DP机制并评估其对效用的影响；分析扩散、VAE和GAN模型类型对效用 - 隐私权衡的影响。

Result: DP - SGD显著影响性能，大数据集有一定效用；新DP机制改善训练稳定性；无保证时扩散模型效用最佳，有DP - SGD时GANs最佳。

Conclusion: DP轨迹生成是挑战任务，正式隐私保证目前仅适用于大数据集和受限用例。

Abstract: While location trajectories offer valuable insights, they also reveal
sensitive personal information. Differential Privacy (DP) offers formal
protection, but achieving a favourable utility-privacy trade-off remains
challenging. Recent works explore deep learning-based generative models to
produce synthetic trajectories. However, current models lack formal privacy
guarantees and rely on conditional information derived from real data during
generation. This work investigates the utility cost of enforcing DP in such
models, addressing three research questions across two datasets and eleven
utility metrics. (1) We evaluate how DP-SGD, the standard DP training method
for deep learning, affects the utility of state-of-the-art generative models.
(2) Since DP-SGD is limited to unconditional models, we propose a novel DP
mechanism for conditional generation that provides formal guarantees and assess
its impact on utility. (3) We analyse how model types - Diffusion, VAE, and GAN
- affect the utility-privacy trade-off. Our results show that DP-SGD
significantly impacts performance, although some utility remains if the
datasets is sufficiently large. The proposed DP mechanism improves training
stability, particularly when combined with DP-SGD, for unstable models such as
GANs and on smaller datasets. Diffusion models yield the best utility without
guarantees, but with DP-SGD, GANs perform best, indicating that the best
non-private model is not necessarily optimal when targeting formal guarantees.
In conclusion, DP trajectory generation remains a challenging task, and formal
guarantees are currently only feasible with large datasets and in constrained
use cases.

</details>


### [218] [TooBadRL: Trigger Optimization to Boost Effectiveness of Backdoor Attacks on Deep Reinforcement Learning](https://arxiv.org/abs/2506.09562)
*Songze Li,Mingxuan Zhang,Oubo Ma,Kang Wei,Shouling Ji*

Main category: cs.CR

TL;DR: 提出TooBadRL框架优化DRL后门攻击触发器，在多算法和任务评估中提升攻击成功率且对正常性能影响小。


<details>
  <summary>Details</summary>
Motivation: 现有DRL后门攻击多采用简单启发式触发器配置，忽略了触发器优化的潜力。

Method: 从时间、空间和幅度三个关键轴优化DRL后门触发器，包括引入性能感知自适应冻结机制、将维度选择建模为合作博弈、提出基于梯度的对抗程序。

Result: 在三种主流DRL算法和九个基准任务上评估，TooBadRL显著提高攻击成功率，且正常任务性能退化最小。

Conclusion: 强调了在DRL后门攻击中进行有原则的触发器优化的重要性。

Abstract: Deep reinforcement learning (DRL) has achieved remarkable success in a wide
range of sequential decision-making domains, including robotics, healthcare,
smart grids, and finance. Recent research demonstrates that attackers can
efficiently exploit system vulnerabilities during the training phase to execute
backdoor attacks, producing malicious actions when specific trigger patterns
are present in the state observations. However, most existing backdoor attacks
rely primarily on simplistic and heuristic trigger configurations, overlooking
the potential efficacy of trigger optimization. To address this gap, we
introduce TooBadRL (Trigger Optimization to Boost Effectiveness of Backdoor
Attacks on DRL), the first framework to systematically optimize DRL backdoor
triggers along three critical axes, i.e., temporal, spatial, and magnitude.
Specifically, we first introduce a performance-aware adaptive freezing
mechanism for injection timing. Then, we formulate dimension selection as a
cooperative game, utilizing Shapley value analysis to identify the most
influential state variable for the injection dimension. Furthermore, we propose
a gradient-based adversarial procedure to optimize the injection magnitude
under environment constraints. Evaluations on three mainstream DRL algorithms
and nine benchmark tasks show that TooBadRL significantly improves attack
success rates, while ensuring minimal degradation of normal task performance.
These results highlight the previously underappreciated importance of
principled trigger optimization in DRL backdoor attacks. The source code of
TooBadRL can be found at https://github.com/S3IC-Lab/TooBadRL.

</details>


### [219] [LLMail-Inject: A Dataset from a Realistic Adaptive Prompt Injection Challenge](https://arxiv.org/abs/2506.09956)
*Sahar Abdelnabi,Aideen Fay,Ahmed Salem,Egor Zverev,Kai-Chieh Liao,Chi-Huang Liu,Chun-Chih Kuo,Jannis Weigend,Danyael Manlangit,Alex Apostolov,Haris Umair,João Donato,Masayuki Kawakita,Athar Mahboob,Tran Huu Bach,Tsun-Han Chiang,Myeongjin Cho,Hajin Choi,Byeonghyeon Kim,Hyeonjin Lee,Benjamin Pannell,Conor McCauley,Mark Russinovich,Andrew Paverd,Giovanni Cherubin*

Main category: cs.CR

TL;DR: 介绍LLMail - Inject公开挑战，模拟真实场景让参与者注入恶意指令，产生大量攻击提交数据集，发布代码和数据，为解决提示注入问题研究奠基。


<details>
  <summary>Details</summary>
Motivation: 当前针对大语言模型间接提示注入攻击的防御对自适应对手的系统评估有限，很多基于大语言模型的应用仍易受攻击。

Method: 举办LLMail - Inject公开挑战，涵盖多种防御策略、大语言模型架构和检索配置。

Result: 得到208,095个来自839名参与者的独特攻击提交数据集，发布挑战代码和完整提交数据。

Conclusion: 这些数据可为指令 - 数据分离问题提供新见解，为解决提示注入问题的实际结构方案的未来研究奠定基础。

Abstract: Indirect Prompt Injection attacks exploit the inherent limitation of Large
Language Models (LLMs) to distinguish between instructions and data in their
inputs. Despite numerous defense proposals, the systematic evaluation against
adaptive adversaries remains limited, even when successful attacks can have
wide security and privacy implications, and many real-world LLM-based
applications remain vulnerable. We present the results of LLMail-Inject, a
public challenge simulating a realistic scenario in which participants
adaptively attempted to inject malicious instructions into emails in order to
trigger unauthorized tool calls in an LLM-based email assistant. The challenge
spanned multiple defense strategies, LLM architectures, and retrieval
configurations, resulting in a dataset of 208,095 unique attack submissions
from 839 participants. We release the challenge code, the full dataset of
submissions, and our analysis demonstrating how this data can provide new
insights into the instruction-data separation problem. We hope this will serve
as a foundation for future research towards practical structural solutions to
prompt injection.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [220] [UAD: Unsupervised Affordance Distillation for Generalization in Robotic Manipulation](https://arxiv.org/abs/2506.09284)
*Yihe Tang,Wenlong Huang,Yingke Wang,Chengshu Li,Roy Yuan,Ruohan Zhang,Jiajun Wu,Li Fei-Fei*

Main category: cs.RO

TL;DR: 提出无监督的UAD方法从基础模型中提取能力知识到任务条件的能力模型，在少量演示训练后有良好泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉能力预测方法依赖手动标注数据或预定义任务集，而理解细粒度对象能力对机器人在非结构化环境操作对象很重要。

Method: 引入UAD方法，利用大视觉模型和视觉语言模型优势自动标注大规模数据集，在冻结特征上训练轻量级任务条件解码器。

Result: UAD对真实场景和人类活动有显著泛化性，基于其提供的能力作为观察空间的模仿学习策略在少量演示训练后对未见对象实例、类别和任务指令变化有良好泛化。

Conclusion: UAD方法有效，能让机器人在非结构化环境基于开放式任务指令操作对象，有良好泛化能力。

Abstract: Understanding fine-grained object affordances is imperative for robots to
manipulate objects in unstructured environments given open-ended task
instructions. However, existing methods of visual affordance predictions often
rely on manually annotated data or conditions only on a predefined set of
tasks. We introduce UAD (Unsupervised Affordance Distillation), a method for
distilling affordance knowledge from foundation models into a task-conditioned
affordance model without any manual annotations. By leveraging the
complementary strengths of large vision models and vision-language models, UAD
automatically annotates a large-scale dataset with detailed $<$instruction,
visual affordance$>$ pairs. Training only a lightweight task-conditioned
decoder atop frozen features, UAD exhibits notable generalization to
in-the-wild robotic scenes and to various human activities, despite only being
trained on rendered objects in simulation. Using affordance provided by UAD as
the observation space, we show an imitation learning policy that demonstrates
promising generalization to unseen object instances, object categories, and
even variations in task instructions after training on as few as 10
demonstrations. Project website: https://unsup-affordance.github.io/

</details>


### [221] [Bipedal Balance Control with Whole-body Musculoskeletal Standing and Falling Simulations](https://arxiv.org/abs/2506.09383)
*Chengtian Ma,Yunyue Wei,Chenhui Zuo,Chen Zhang,Yanan Sui*

Main category: cs.RO

TL;DR: 本文提出模拟人体平衡的分层控制管道，揭示稳定站立平衡时空动态、肌肉损伤影响，生成摔倒模式，展示髋外骨骼辅助效果，为相关干预和机器人系统发展奠基。


<details>
  <summary>Details</summary>
Motivation: 当前对静态平衡和摔倒的定量理解有限，需要深入研究人体平衡控制。

Method: 通过综合的全身肌肉骨骼系统提出分层控制管道来模拟人体平衡。

Result: 确定稳定站立时平衡的时空动态，揭示肌肉损伤对平衡行为的影响，生成与临床数据相符的摔倒接触模式，模拟髋外骨骼辅助能改善平衡并减少肌肉用力。

Conclusion: 此研究提供了难以通过实验获取的肌肉层面人体平衡动态见解，可为平衡受损者干预和人形机器人系统发展提供基础。

Abstract: Balance control is important for human and bipedal robotic systems. While
dynamic balance during locomotion has received considerable attention,
quantitative understanding of static balance and falling remains limited. This
work presents a hierarchical control pipeline for simulating human balance via
a comprehensive whole-body musculoskeletal system. We identified spatiotemporal
dynamics of balancing during stable standing, revealed the impact of muscle
injury on balancing behavior, and generated fall contact patterns that aligned
with clinical data. Furthermore, our simulated hip exoskeleton assistance
demonstrated improvement in balance maintenance and reduced muscle effort under
perturbation. This work offers unique muscle-level insights into human balance
dynamics that are challenging to capture experimentally. It could provide a
foundation for developing targeted interventions for individuals with balance
impairments and support the advancement of humanoid robotic systems.

</details>


### [222] [Adv-BMT: Bidirectional Motion Transformer for Safety-Critical Traffic Scenario Generation](https://arxiv.org/abs/2506.09485)
*Yuxin Liu,Zhenghao Peng,Xuanhao Cui,Bolei Zhou*

Main category: cs.RO

TL;DR: 提出Adv - BMT框架解决自动驾驶场景测试数据稀缺问题，可生成高质量碰撞场景，降低碰撞率。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶场景测试中真实世界数据集缺乏长尾、安全关键场景，需解决数据稀缺问题。

Method: 提出Adv - BMT框架，核心是双向运动变压器（BMT）模型进行逆向交通运动预测，采用两阶段流水线，先进行对抗初始化，再进行逆向运动预测。

Result: 实验验证Adv - BMT生成的碰撞场景质量，在增强数据集上训练比以往工作降低20%的碰撞率。

Conclusion: Adv - BMT框架无需预训练碰撞数据，能生成真实多样的碰撞交互场景，有效提升自动驾驶系统测试性能。

Abstract: Scenario-based testing is essential for validating the performance of
autonomous driving (AD) systems. However, such testing is limited by the
scarcity of long-tailed, safety-critical scenarios in existing datasets
collected in the real world. To tackle the data issue, we propose the Adv-BMT
framework, which augments real-world scenarios with diverse and realistic
adversarial interactions. The core component of Adv-BMT is a bidirectional
motion transformer (BMT) model to perform inverse traffic motion predictions,
which takes agent information in the last time step of the scenario as input,
and reconstruct the traffic in the inverse of chronological order until the
initial time step. The Adv-BMT framework is a two-staged pipeline: it first
conducts adversarial initializations and then inverse motion predictions.
Different from previous work, we do not need any collision data for
pretraining, and are able to generate realistic and diverse collision
interactions. Our experimental results validate the quality of generated
collision scenarios by Adv-BMT: training in our augmented dataset would reduce
episode collision rates by 20\% compared to previous work.

</details>


### [223] [Tightly-Coupled LiDAR-IMU-Leg Odometry with Online Learned Leg Kinematics Incorporating Foot Tactile Information](https://arxiv.org/abs/2506.09548)
*Taku Okawara,Kenji Koide,Aoki Takanose,Shuji Oishi,Masashi Yokozuka,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 提出紧耦合的LiDAR - IMU - 腿部里程计，在无特征环境和可变形地形等条件下表现良好，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 开发一种在无特征环境和可变形地形等具有挑战性条件下仍能保持鲁棒性的里程计方法。

Method: 开发基于在线学习的神经腿部运动学模型，结合触觉信息；根据神经自适应腿部里程计因子和在线不确定性估计，在统一因子图上联合解决运动学模型的在线训练和里程计估计。

Result: 使用四足机器人在沙滩和校园等具有挑战性的场景中进行实验，含神经腿部运动学模型的里程计估计优于现有方法。

Conclusion: 提出的紧耦合LiDAR - IMU - 腿部里程计方法具有良好的鲁棒性和性能，优于现有技术。

Abstract: In this letter, we present tightly coupled LiDAR-IMU-leg odometry, which is
robust to challenging conditions such as featureless environments and
deformable terrains. We developed an online learning-based leg kinematics model
named the neural leg kinematics model, which incorporates tactile information
(foot reaction force) to implicitly express the nonlinear dynamics between
robot feet and the ground. Online training of this model enhances its
adaptability to weight load changes of a robot (e.g., assuming delivery or
transportation tasks) and terrain conditions. According to the \textit{neural
adaptive leg odometry factor} and online uncertainty estimation of the leg
kinematics model-based motion predictions, we jointly solve online training of
this kinematics model and odometry estimation on a unified factor graph to
retain the consistency of both. The proposed method was verified through real
experiments using a quadruped robot in two challenging situations: 1) a sandy
beach, representing an extremely featureless area with a deformable terrain,
and 2) a campus, including multiple featureless areas and terrain types of
asphalt, gravel (deformable terrain), and grass. Experimental results showed
that our odometry estimation incorporating the \textit{neural leg kinematics
model} outperforms state-of-the-art works. Our project page is available for
further details: https://takuokawara.github.io/RAL2025_project_page/

</details>


### [224] [SkillBlender: Towards Versatile Humanoid Whole-Body Loco-Manipulation via Skill Blending](https://arxiv.org/abs/2506.09366)
*Yuxuan Kuang,Haoran Geng,Amine Elhafsi,Tan-Dzung Do,Pieter Abbeel,Jitendra Malik,Marco Pavone,Yue Wang*

Main category: cs.RO

TL;DR: 提出用于类人机器人移动操作的SkillBlender框架和SkillBench基准，实验表现优且将开源。


<details>
  <summary>Details</summary>
Motivation: 现有类人机器人全身控制和移动操作方法需针对特定任务繁琐调参，限制了在日常场景中对多样任务的通用性和可扩展性。

Method: 提出SkillBlender分层强化学习框架，先预训练与任务无关的原始技能，再动态融合这些技能；还引入SkillBench模拟基准及评估指标。

Result: 大量模拟实验表明该方法显著优于所有基线方法，能自然规范行为以避免奖励破解，在日常场景多样移动操作任务中实现更准确可行的动作。

Conclusion: 所提方法在类人机器人移动操作上表现良好，代码和基准将开源以推动未来研究。

Abstract: Humanoid robots hold significant potential in accomplishing daily tasks
across diverse environments thanks to their flexibility and human-like
morphology. Recent works have made significant progress in humanoid whole-body
control and loco-manipulation leveraging optimal control or reinforcement
learning. However, these methods require tedious task-specific tuning for each
task to achieve satisfactory behaviors, limiting their versatility and
scalability to diverse tasks in daily scenarios. To that end, we introduce
SkillBlender, a novel hierarchical reinforcement learning framework for
versatile humanoid loco-manipulation. SkillBlender first pretrains
goal-conditioned task-agnostic primitive skills, and then dynamically blends
these skills to accomplish complex loco-manipulation tasks with minimal
task-specific reward engineering. We also introduce SkillBench, a parallel,
cross-embodiment, and diverse simulated benchmark containing three embodiments,
four primitive skills, and eight challenging loco-manipulation tasks,
accompanied by a set of scientific evaluation metrics balancing accuracy and
feasibility. Extensive simulated experiments show that our method significantly
outperforms all baselines, while naturally regularizing behaviors to avoid
reward hacking, resulting in more accurate and feasible movements for diverse
loco-manipulation tasks in our daily scenarios. Our code and benchmark will be
open-sourced to the community to facilitate future research. Project page:
https://usc-gvl.github.io/SkillBlender-web/.

</details>


### [225] [Scoop-and-Toss: Dynamic Object Collection for Quadrupedal Systems](https://arxiv.org/abs/2506.09406)
*Minji Kang,Chanwoo Baek,Yoonsang Lee*

Main category: cs.RO

TL;DR: 提出使四足机器人在无额外执行器情况下收集物体的框架，拓展了四足机器人腿部的功能。


<details>
  <summary>Details</summary>
Motivation: 现有四足机器人腿部操作多针对静态任务，希望拓展其动态物体操作能力。

Method: 提出分层策略结构，包含两个专家策略（舀取抛掷和接近物体位置）和一个元策略，专家策略分别训练后进行元策略训练。

Result: 实现了四足机器人在无额外执行器下收集物体。

Conclusion: 四足机器人腿部可有效用于动态物体操作，拓展了其功能。

Abstract: Quadruped robots have made significant advances in locomotion, extending
their capabilities from controlled environments to real-world applications.
Beyond movement, recent work has explored loco-manipulation using the legs to
perform tasks such as pressing buttons or opening doors. While these efforts
demonstrate the feasibility of leg-based manipulation, most have focused on
relatively static tasks. In this work, we propose a framework that enables
quadruped robots to collect objects without additional actuators by leveraging
the agility of their legs. By attaching a simple scoop-like add-on to one leg,
the robot can scoop objects and toss them into a collection tray mounted on its
back. Our method employs a hierarchical policy structure comprising two expert
policies-one for scooping and tossing, and one for approaching object
positions-and a meta-policy that dynamically switches between them. The expert
policies are trained separately, followed by meta-policy training for
coordinated multi-object collection. This approach demonstrates how quadruped
legs can be effectively utilized for dynamic object manipulation, expanding
their role beyond locomotion.

</details>


### [226] [Time-Unified Diffusion Policy with Action Discrimination for Robotic Manipulation](https://arxiv.org/abs/2506.09422)
*Ye Niu,Sanping Zhou,Yizhe Li,Ye Den,Le Wang*

Main category: cs.RO

TL;DR: 提出时间统一扩散策略（TUDP），利用动作识别能力构建时间统一去噪过程，在RLBench上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的策略方法迭代去噪耗时久，阻碍机器人操作实时响应，且时变去噪过程增加训练难度、降低动作准确性。

Method: 构建具有额外动作判别信息的时间统一速度场，提出动作训练方法引入动作判别分支。

Result: 在RLBench多视图设置下成功率达82.6%，单视图设置下达83.8%，减少去噪迭代次数时成功率提升显著，能为实际任务生成准确动作。

Conclusion: TUDP能高效准确生成机器人动作，具有良好性能和实际应用价值。

Abstract: In many complex scenarios, robotic manipulation relies on generative models
to estimate the distribution of multiple successful actions. As the diffusion
model has better training robustness than other generative models, it performs
well in imitation learning through successful robot demonstrations. However,
the diffusion-based policy methods typically require significant time to
iteratively denoise robot actions, which hinders real-time responses in robotic
manipulation. Moreover, existing diffusion policies model a time-varying action
denoising process, whose temporal complexity increases the difficulty of model
training and leads to suboptimal action accuracy. To generate robot actions
efficiently and accurately, we present the Time-Unified Diffusion Policy
(TUDP), which utilizes action recognition capabilities to build a time-unified
denoising process. On the one hand, we build a time-unified velocity field in
action space with additional action discrimination information. By unifying all
timesteps of action denoising, our velocity field reduces the difficulty of
policy learning and speeds up action generation. On the other hand, we propose
an action-wise training method, which introduces an action discrimination
branch to supply additional action discrimination information. Through
action-wise training, the TUDP implicitly learns the ability to discern
successful actions to better denoising accuracy. Our method achieves
state-of-the-art performance on RLBench with the highest success rate of 82.6%
on a multi-view setup and 83.8% on a single-view setup. In particular, when
using fewer denoising iterations, TUDP achieves a more significant improvement
in success rate. Additionally, TUDP can produce accurate actions for a wide
range of real-world tasks.

</details>


### [227] [SAFE: Multitask Failure Detection for Vision-Language-Action Models](https://arxiv.org/abs/2506.09937)
*Qiao Gu,Yuanliang Ju,Shengxiang Sun,Igor Gilitschenski,Haruki Nishimura,Masha Itkina,Florian Shkurti*

Main category: cs.RO

TL;DR: 现有视觉语言动作模型（VLAs）在新任务上开箱即用成功率有限，本文提出多任务失败检测问题，引入SAFE失败检测器，在模拟和真实环境测试显示其性能达最优。


<details>
  <summary>Details</summary>
Motivation: VLAs在新任务上成功率低，需一个能泛化到未知任务和新环境的失败检测器，而现有检测器仅针对特定任务。

Method: 分析VLA特征空间，发现其有通用的任务成败知识，设计SAFE从VLA内部特征学习并预测任务失败可能性，在成功和失败轨迹上训练，在未知任务上评估。

Result: SAFE与不同策略架构兼容，在模拟和真实环境广泛测试，与多种基线对比，使用保形预测在准确率和检测时间上取得最优平衡。

Conclusion: SAFE在VLA等通用机器人策略的失败检测中达到了先进水平，能有效应对多任务场景。

Abstract: While vision-language-action models (VLAs) have shown promising robotic
behaviors across a diverse set of manipulation tasks, they achieve limited
success rates when deployed on novel tasks out-of-the-box. To allow these
policies to safely interact with their environments, we need a failure detector
that gives a timely alert such that the robot can stop, backtrack, or ask for
help. However, existing failure detectors are trained and tested only on one or
a few specific tasks, while VLAs require the detector to generalize and detect
failures also in unseen tasks and novel environments. In this paper, we
introduce the multitask failure detection problem and propose SAFE, a failure
detector for generalist robot policies such as VLAs. We analyze the VLA feature
space and find that VLAs have sufficient high-level knowledge about task
success and failure, which is generic across different tasks. Based on this
insight, we design SAFE to learn from VLA internal features and predict a
single scalar indicating the likelihood of task failure. SAFE is trained on
both successful and failed rollouts, and is evaluated on unseen tasks. SAFE is
compatible with different policy architectures. We test it on OpenVLA, $\pi_0$,
and $\pi_0$-FAST in both simulated and real-world environments extensively. We
compare SAFE with diverse baselines and show that SAFE achieves
state-of-the-art failure detection performance and the best trade-off between
accuracy and detection time using conformal prediction. More qualitative
results can be found at https://vla-safe.github.io/.

</details>


### [228] [eFlesh: Highly customizable Magnetic Touch Sensing using Cut-Cell Microstructures](https://arxiv.org/abs/2506.09994)
*Venkatesh Pattabiraman,Zizhou Huang,Daniele Panozzo,Denis Zorin,Lerrel Pinto,Raunaq Bhirangi*

Main category: cs.RO

TL;DR: 提出低成本、易制造、高度可定制的磁性触觉传感器eFlesh，介绍其组件、设计框架和实验结果，且开源相关文件和工具。


<details>
  <summary>Details</summary>
Motivation: 现有缺乏通用、易获取和可定制的触觉传感器，导致机器人操作解决方案碎片化，许多情况采用无传感器方法，需要解决这一问题。

Method: 使用3D打印机、磁铁、CAD模型和磁力计电路板构建eFlesh传感器，由平铺的参数化微结构构成，提供开源设计工具转换文件。

Result: 接触定位RMSE为0.5 mm，法向力和剪切力的力预测RMSE分别为0.27 N和0.12 N；滑动检测模型准确率95%；视觉触觉控制策略使操作性能比仅视觉基线提高40%，四项精确任务平均成功率达91%。

Conclusion: eFlesh传感器能有效解决当前机器人触觉传感问题，具有良好性能，且开源资源方便用户使用。

Abstract: If human experience is any guide, operating effectively in unstructured
environments -- like homes and offices -- requires robots to sense the forces
during physical interaction. Yet, the lack of a versatile, accessible, and
easily customizable tactile sensor has led to fragmented, sensor-specific
solutions in robotic manipulation -- and in many cases, to force-unaware,
sensorless approaches. With eFlesh, we bridge this gap by introducing a
magnetic tactile sensor that is low-cost, easy to fabricate, and highly
customizable. Building an eFlesh sensor requires only four components: a
hobbyist 3D printer, off-the-shelf magnets (<$5), a CAD model of the desired
shape, and a magnetometer circuit board. The sensor is constructed from tiled,
parameterized microstructures, which allow for tuning the sensor's geometry and
its mechanical response. We provide an open-source design tool that converts
convex OBJ/STL files into 3D-printable STLs for fabrication. This modular
design framework enables users to create application-specific sensors, and to
adjust sensitivity depending on the task. Our sensor characterization
experiments demonstrate the capabilities of eFlesh: contact localization RMSE
of 0.5 mm, and force prediction RMSE of 0.27 N for normal force and 0.12 N for
shear force. We also present a learned slip detection model that generalizes to
unseen objects with 95% accuracy, and visuotactile control policies that
improve manipulation performance by 40% over vision-only baselines -- achieving
91% average success rate for four precise tasks that require sub-mm accuracy
for successful completion. All design files, code and the CAD-to-eFlesh STL
conversion tool are open-sourced and available on https://e-flesh.com.

</details>


### [229] [Learning to Optimize Package Picking for Large-Scale, Real-World Robot Induction](https://arxiv.org/abs/2506.09765)
*Shuai Li,Azarakhsh Keipour,Sicong Zhao,Srinath Rajagopalan,Charles Swan,Kostas E. Bekris*

Main category: cs.RO

TL;DR: 提出基于机器学习的框架优化样本拣选，降低拣货失败率，用于大规模仓库自动化。


<details>
  <summary>Details</summary>
Motivation: 先前研究多关注启发式方法采样的拣选成功概率预测，缺乏数据驱动方法直接优化样本拣选以提升大规模性能。

Method: 提出基于机器学习的框架，预测变换调整并改进多吸盘末端执行器的吸盘选择，以提高样本拣选成功率。

Result: 在超200万次拣选中，与基于启发式的拣选采样基线相比，所提方法使拣选失败率降低20%。

Conclusion: 所提方法在大规模仓库自动化场景中有效。

Abstract: Warehouse automation plays a pivotal role in enhancing operational
efficiency, minimizing costs, and improving resilience to workforce
variability. While prior research has demonstrated the potential of machine
learning (ML) models to increase picking success rates in large-scale robotic
fleets by prioritizing high-probability picks and packages, these efforts
primarily focused on predicting success probabilities for picks sampled using
heuristic methods. Limited attention has been given, however, to leveraging
data-driven approaches to directly optimize sampled picks for better
performance at scale. In this study, we propose an ML-based framework that
predicts transform adjustments as well as improving the selection of suction
cups for multi-suction end effectors for sampled picks to enhance their success
probabilities. The framework was integrated and evaluated in test workcells
that resemble the operations of Amazon Robotics' Robot Induction (Robin) fleet,
which is used for package manipulation. Evaluated on over 2 million picks, the
proposed method achieves a 20\% reduction in pick failure rates compared to a
heuristic-based pick sampling baseline, demonstrating its effectiveness in
large-scale warehouse automation scenarios.

</details>


### [230] [Chain-of-Action: Trajectory Autoregressive Modeling for Robotic Manipulation](https://arxiv.org/abs/2506.09990)
*Wenbo Zhang,Tianrun Hu,Yanyuan Qiao,Hanbo Zhang,Yuchu Qin,Yang Li,Jiajun Liu,Tao Kong,Lingqiao Liu,Xiao Ma*

Main category: cs.RO

TL;DR: 提出基于轨迹自回归建模的Chain-of-Action (CoA) 视觉-运动策略范式，通过反向推理生成轨迹，结合四项设计，在多个任务中达SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统方法正向预测下一步动作，本文希望通过反向推理、以任务特定目标生成完整轨迹，使局部动作受最终目标约束。

Method: 构建CoA范式，在单一自回归结构内通过反向推理生成轨迹，结合连续动作令牌表示、动态停止、反向时间集成和多令牌预测四项设计。

Result: CoA具有强空间泛化能力，在60个RLBench任务和8个现实操作任务中达到SOTA性能。

Conclusion: CoA在保持视觉-运动策略灵活性和简单性的同时，展现了出色的性能和泛化能力。

Abstract: We present Chain-of-Action (CoA), a novel visuo-motor policy paradigm built
upon Trajectory Autoregressive Modeling. Unlike conventional approaches that
predict next step action(s) forward, CoA generates an entire trajectory by
explicit backward reasoning with task-specific goals through an action-level
Chain-of-Thought (CoT) process. This process is unified within a single
autoregressive structure: (1) the first token corresponds to a stable keyframe
action that encodes the task-specific goals; and (2) subsequent action tokens
are generated autoregressively, conditioned on the initial keyframe and
previously predicted actions. This backward action reasoning enforces a
global-to-local structure, allowing each local action to be tightly constrained
by the final goal. To further realize the action reasoning structure, CoA
incorporates four complementary designs: continuous action token
representation; dynamic stopping for variable-length trajectory generation;
reverse temporal ensemble; and multi-token prediction to balance action chunk
modeling with global structure. As a result, CoA gives strong spatial
generalization capabilities while preserving the flexibility and simplicity of
a visuo-motor policy. Empirically, we observe CoA achieves the state-of-the-art
performance across 60 RLBench tasks and 8 real-world manipulation tasks.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [231] [Empirical and computer-aided robustness analysis of long-step and accelerated methods in smooth convex optimization](https://arxiv.org/abs/2506.09730)
*Pierre Vernimmen,François Glineur*

Main category: math.OC

TL;DR: 本文通过性能估计方法评估不同一阶优化方法在梯度计算存在相对不精确性时的鲁棒性，分析三类方法，改进理论保证并测试，发现加速方法更鲁棒，缩短因子有帮助，缩短方法有前景。


<details>
  <summary>Details</summary>
Motivation: 评估不同一阶优化方法在梯度计算相对不精确时的鲁棒性，因处理大规模GPU问题时会出现梯度压缩导致不精确。

Method: 使用性能估计方法，分析三类方法，引入半启发式缩短因子改进理论保证，在具体不精确问题上测试。

Result: 加速方法比预期更鲁棒，缩短因子显著帮助长步方法。

Conclusion: 所有缩短方法在不精确情况下有前景。

Abstract: This work assesses both empirically and theoretically, using the performance
estimation methodology, how robust different first-order optimization methods
are when subject to relative inexactness in their gradient computations.
Relative inexactness occurs, for example, when compressing the gradient using
fewer bits of information, which happens when dealing with large-scale problems
on GPUs. Three major families of methods are analyzed: constant step gradient
descent, long-step methods, and accelerated methods. The latter two are first
shown to be theoretically not robust to inexactness. Then, a semi-heuristic
shortening factor is introduced to improve their theoretical guarantees. All
methods are subsequently tested on a concrete inexact problem, with two
different types of relative inexactness, and it is observed that both
accelerated methods are much more robust than expected, and that the shortening
factor significantly helps the long-step methods. In the end, all shortened
methods appear to be promising, even in this inexact setting.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [232] [A new hierarchical distribution on arbitrary sparse precision matrices](https://arxiv.org/abs/2506.09607)
*Gianluca Mastrantonio,Pierfrancesco Alaimo Di Loro,Marco Mingione*

Main category: stat.ME

TL;DR: 本文介绍了一种定义稀疏对称正定矩阵分布的策略，提出S - Bartlett方法并进行测试，显示其可作为估计稀疏精度矩阵的灵活替代方案。


<details>
  <summary>Details</summary>
Motivation: 定义稀疏对称正定矩阵空间上的分布，解决相关估计问题。

Method: 利用精度矩阵的Cholesky分解，通过约束元素实现稀疏性，提出S - Bartlett作为修改的Bartlett分解，结合Spike - and - Slab先验，用基于Dual Averaging Hamiltonian Monte Carlo更新的MCMC例程进行贝叶斯估计，还将框架拓展到广义线性模型。

Result: 在模拟和真实数据上测试，结果表明S - Bartlett先验能灵活估计稀疏精度矩阵。

Conclusion: S - Bartlett先验是估计稀疏精度矩阵的灵活替代方案，有广泛应用潜力。

Abstract: We introduce a general strategy for defining distributions over the space of
sparse symmetric positive definite matrices. Our method utilizes the Cholesky
factorization of the precision matrix, imposing sparsity through constraints on
its elements while preserving their independence and avoiding the numerical
evaluation of normalization constants. In particular, we develop the S-Bartlett
as a modified Bartlett decomposition, recovering the standard Wishart as a
particular case. By incorporating a Spike-and-Slab prior to model graph
sparsity, our approach facilitates Bayesian estimation through a tailored MCMC
routine based on a Dual Averaging Hamiltonian Monte Carlo update. This
framework extends naturally to the Generalized Linear Model setting, enabling
applications to non-Gaussian outcomes via latent Gaussian variables. We test
and compare the proposed S-Bartelett prior with the G-Wishart both on simulated
and real data. Results highlight that the S-Bartlett prior offers a flexible
alternative for estimating sparse precision matrices, with potential
applications across diverse fields.

</details>


### [233] [Knockoffs Inference under Privacy Constraints](https://arxiv.org/abs/2506.09690)
*Zhanrui Cai,Yingying Fan,Lan Gao*

Main category: stat.ME

TL;DR: 提出差分隐私范式下的knockoff推理综合框架，保证隐私保护和FDR控制，经分析和应用验证有效性。


<details>
  <summary>Details</summary>
Motivation: Model - X knockoff框架在数据隐私保护上存在挑战，需提出隐私保护方法。

Method: 提出差分隐私范式下的knockoff推理综合框架。

Result: 保证了隐私保护，维持FDR控制，经分析噪声不影响渐近功效，应用证明方法在不同维度下有效。

Conclusion: DP - knockoff方法可有效在FDR控制的变量选择中保护隐私。

Abstract: Model-X knockoff framework offers a model-free variable selection method that
ensures finite sample false discovery rate (FDR) control. However, the
complexity of generating knockoff variables, coupled with the model-free
assumption, presents significant challenges for protecting data privacy in this
context. In this paper, we propose a comprehensive framework for knockoff
inference within the differential privacy paradigm. Our proposed method
guarantees robust privacy protection while preserving the exact FDR control
entailed by the original model-X knockoff procedure. We further conduct power
analysis and establish sufficient conditions under which the noise added for
privacy preservation does not asymptotically compromise power. Through various
applications, we demonstrate that the differential privacy knockoff
(DP-knockoff) method can be effectively utilized to safeguard privacy during
variable selection with FDR control in both low and high dimensional settings.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [234] [A Note on the Reliability of Goal-Oriented Error Estimates for Galerkin Finite Element Methods with Nonlinear Functionals](https://arxiv.org/abs/2506.09913)
*Brian N. Granzow,Stephen D. Bond,D. Thomas Seidl,Bernhard Endtmayer*

Main category: math.NA

TL;DR: 本文研究抽象变分问题中非线性泛函离散化误差估计，表明某些非线性泛函下已知误差估计不可靠，并给出示例。


<details>
  <summary>Details</summary>
Motivation: 研究抽象变分问题中非线性泛函离散化误差估计的可靠性。

Method: 考虑Galerkin有限元方法近似抽象变分问题，分析已知误差估计形式。

Result: 发现存在非线性泛函，即使有精确伴随解，已知形式的误差估计也不可靠，并给出了不可靠的双线性形式和非线性泛函的示例对。

Conclusion: 某些非线性泛函下，现有的误差估计形式不具有可靠性。

Abstract: We consider estimating the discretization error in a nonlinear functional
$J(u)$ in the setting of an abstract variational problem: find $u \in
\mathcal{V}$ such that $B(u,\varphi) = L(\varphi) \; \forall \varphi \in
\mathcal{V}$, as approximated by a Galerkin finite element method. Here,
$\mathcal{V}$ is a Hilbert space, $B(\cdot,\cdot)$ is a bilinear form, and
$L(\cdot)$ is a linear functional. We consider well-known error estimates
$\eta$ of the form $J(u) - J(u_h) \approx \eta = L(z) - B(u_h, z)$, where $u_h$
denotes a finite element approximation to $u$, and $z$ denotes the solution to
an auxiliary adjoint variational problem. We show that there exist nonlinear
functionals for which error estimates of this form are not reliable, even in
the presence of an exact adjoint solution solution $z$. An estimate $\eta$ is
said to be reliable if there exists a constant $C \in \mathbb{R}_{>0}$
independent of $u_h$ such that $|J(u) - J(u_h)| \leq C|\eta|$. We present
several example pairs of bilinear forms and nonlinear functionals where
reliability of $\eta$ is not achieved.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [235] [Intelligent System of Emergent Knowledge: A Coordination Fabric for Billions of Minds](https://arxiv.org/abs/2506.09335)
*Moshi Wei,Sparks Li*

Main category: cs.MA

TL;DR: ISEK构建去中心化网络，结合三项原则，通过创新协议和激励机制促进涌现智能，实现范式转变。


<details>
  <summary>Details</summary>
Motivation: 构建能让人类和人工智能代理协作、形成自组织认知生态系统的去中心化网络，突破传统平台局限。

Method: 基于Web3基础设施，结合三项原则，实施六阶段协调协议，利用$ISEK代币进行经济激励，通过NFT管理身份。

Result: 创建了积极促进涌现智能的基础设施，实现了从传统平台的范式转变。

Conclusion: ISEK能实现大规模、去中心化认知系统的有机发展，使自治代理集体进化摆脱集中约束。

Abstract: The Intelligent System of Emergent Knowledge (ISEK) establishes a
decentralized network where human and artificial intelligence agents
collaborate as peers, forming a self-organizing cognitive ecosystem. Built on
Web3 infrastructure, ISEK combines three fundamental principles: (1) a
decentralized multi-agent architecture resistant to censorship, (2) symbiotic
AI-human collaboration with equal participation rights, and (3) resilient
self-adaptation through distributed consensus mechanisms.
  The system implements an innovative coordination protocol featuring a
six-phase workflow (Publish, Discover, Recruit, Execute, Settle, Feedback) for
dynamic task allocation, supported by robust fault tolerance and a
multidimensional reputation system. Economic incentives are governed by the
native $ISEK token, facilitating micropayments, governance participation, and
reputation tracking, while agent sovereignty is maintained through NFT-based
identity management.
  This synthesis of blockchain technology, artificial intelligence, and
incentive engineering creates an infrastructure that actively facilitates
emergent intelligence. ISEK represents a paradigm shift from conventional
platforms, enabling the organic development of large-scale, decentralized
cognitive systems where autonomous agents collectively evolve beyond
centralized constraints.

</details>


### [236] [When Is Diversity Rewarded in Cooperative Multi-Agent Learning?](https://arxiv.org/abs/2506.09434)
*Michael Amir,Matteo Bettini,Amanda Prorok*

Main category: cs.MA

TL;DR: 研究多智能体任务分配中异构团队何时优于同质团队，从奖励设计角度出发，提出理论和算法并验证。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺少对多样化团队何时优于同质团队的原则性解释，本文聚焦多智能体任务分配问题，从奖励设计角度研究适合异构团队的目标。

Method: 先考虑瞬时非空间设置，证明聚合算子曲率决定异构能否增加奖励；使用多智能体强化学习，引入基于梯度的HED算法优化环境参数空间。

Result: 矩阵游戏和多目标捕获环境实验表明，HED能重现理论预测的奖励机制，验证了HED并连接理论与奖励设计。

Conclusion: 研究结果有助于理解行为多样性何时带来可衡量的益处。

Abstract: The success of teams in robotics, nature, and society often depends on the
division of labor among diverse specialists; however, a principled explanation
for when such diversity surpasses a homogeneous team is still missing. Focusing
on multi-agent task allocation problems, our goal is to study this question
from the perspective of reward design: what kinds of objectives are best suited
for heterogeneous teams? We first consider an instantaneous, non-spatial
setting where the global reward is built by two generalized aggregation
operators: an inner operator that maps the $N$ agents' effort allocations on
individual tasks to a task score, and an outer operator that merges the $M$
task scores into the global team reward. We prove that the curvature of these
operators determines whether heterogeneity can increase reward, and that for
broad reward families this collapses to a simple convexity test. Next, we ask
what incentivizes heterogeneity to emerge when embodied, time-extended agents
must learn an effort allocation policy. To study heterogeneity in such
settings, we use multi-agent reinforcement learning (MARL) as our computational
paradigm, and introduce Heterogeneous Environment Design (HED), a
gradient-based algorithm that optimizes the parameter space of underspecified
MARL environments to find scenarios where heterogeneity is advantageous.
Experiments in matrix games and an embodied Multi-Goal-Capture environment show
that, despite the difference in settings, HED rediscovers the reward regimes
predicted by our theory to maximize the advantage of heterogeneity, both
validating HED and connecting our theoretical insights to reward design in
MARL. Together, these results help us understand when behavioral diversity
delivers a measurable benefit.

</details>


### [237] [Effective Red-Teaming of Policy-Adherent Agents](https://arxiv.org/abs/2506.09600)
*Itay Nakash,George Kour,Koren Lazar,Matan Vetzler,Guy Uziel,Ateret Anaby-Tavor*

Main category: cs.MA

TL;DR: 本文聚焦面向任务的基于大语言模型的代理在严格政策领域的应用挑战，提出威胁模型、CRAFT系统和tau - break基准，并评估防御策略，指出需更强保护措施。


<details>
  <summary>Details</summary>
Motivation: 确保面向任务的基于大语言模型的代理在严格政策领域中遵守规则，抵御恶意用户行为。

Method: 提出针对对抗性用户的威胁模型，构建CRAFT多智能体红队系统，引入tau - break基准，评估防御策略。

Result: CRAFT系统优于传统越狱方法，现有防御策略有一定作用但不足。

Conclusion: 需要更强的、基于研究的保障措施来保护遵守政策的代理免受对抗性攻击。

Abstract: Task-oriented LLM-based agents are increasingly used in domains with strict
policies, such as refund eligibility or cancellation rules. The challenge lies
in ensuring that the agent consistently adheres to these rules and policies,
appropriately refusing any request that would violate them, while still
maintaining a helpful and natural interaction. This calls for the development
of tailored design and evaluation methodologies to ensure agent resilience
against malicious user behavior. We propose a novel threat model that focuses
on adversarial users aiming to exploit policy-adherent agents for personal
benefit. To address this, we present CRAFT, a multi-agent red-teaming system
that leverages policy-aware persuasive strategies to undermine a
policy-adherent agent in a customer-service scenario, outperforming
conventional jailbreak methods such as DAN prompts, emotional manipulation, and
coercive. Building upon the existing tau-bench benchmark, we introduce
tau-break, a complementary benchmark designed to rigorously assess the agent's
robustness against manipulative user behavior. Finally, we evaluate several
straightforward yet effective defense strategies. While these measures provide
some protection, they fall short, highlighting the need for stronger,
research-driven safeguards to protect policy-adherent agents from adversarial
attacks

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [238] [Genetic Transformer-Assisted Quantum Neural Networks for Optimal Circuit Design](https://arxiv.org/abs/2506.09205)
*Haiyan Wang*

Main category: quant-ph

TL;DR: 提出GTQNNs混合学习框架，结合变压器编码器和浅变分量子电路，用NSGA - II算法微调，实验显示其性能佳且所需门少，适用于NISQ设备。


<details>
  <summary>Details</summary>
Motivation: 为在噪声中等规模量子（NISQ）硬件上实现有效量子计算，解决高维数据处理和减少量子门数量问题。

Method: 将变压器编码器与浅变分量子电路结合，使用NSGA - II多目标遗传算法自动微调电路。

Result: 在四个基准测试中，GTQNNs性能达到或超过现有量子模型，多数情况所需门更少；混合Fisher信息分析表明训练网络远离贫瘠高原。

Conclusion: GTQNNs能以适合当前NISQ设备的量子资源预算提供有竞争力的性能。

Abstract: We introduce Genetic Transformer Assisted Quantum Neural Networks (GTQNNs), a
hybrid learning framework that combines a transformer encoder with a shallow
variational quantum circuit and automatically fine tunes the circuit via the
NSGA-II multi objective genetic algorithm. The transformer reduces
high-dimensional classical data to a compact, qubit sized representation, while
NSGA-II searches for Pareto optimal circuits that (i) maximize classification
accuracy and (ii) minimize primitive gate count an essential constraint for
noisy intermediate-scale quantum (NISQ) hardware. Experiments on four
benchmarks (Iris, Breast Cancer, MNIST, and Heart Disease) show that GTQNNs
match or exceed state of the art quantum models while requiring much fewer
gates for most cases. A hybrid Fisher information analysis further reveals that
the trained networks operate far from barren plateaus; the leading curvature
directions increasingly align with the quantum subspace as the qubit budget
grows, confirming that the transformer front end has effectively condensed the
data. Together, these results demonstrate that GTQNNs deliver competitive
performance with a quantum resource budget well suited to present-day NISQ
devices.

</details>


### [239] [Devanagari Digit Recognition using Quantum Machine Learning](https://arxiv.org/abs/2506.09069)
*Sahaj Raj Malla*

Main category: quant-ph

TL;DR: 本文提出首个混合量子 - 经典架构用于天城文手写数字识别，在相关数据集上表现出色，凸显量子机器学习在低资源语言场景的潜力。


<details>
  <summary>Details</summary>
Motivation: 天城文等区域文字手写数字识别对多语言文档数字化等有重要意义，但传统模型面临结构复杂和标注数据有限的挑战。

Method: 结合卷积神经网络进行空间特征提取和10 - 量子比特变分量子电路进行量子增强分类。

Result: 在DHCD数据集上，实现99.80%的测试准确率、0.2893的测试损失和0.9980的平均每类F1分数，比等效经典CNN准确率更高、参数更少且鲁棒性更强。

Conclusion: 利用量子原理为区域文字识别建立新基准，显示量子机器学习在现实低资源语言场景的前景。

Abstract: Handwritten digit recognition in regional scripts, such as Devanagari, is
crucial for multilingual document digitization, educational tools, and the
preservation of cultural heritage. The script's complex structure and limited
annotated datasets pose significant challenges to conventional models. This
paper introduces the first hybrid quantum-classical architecture for Devanagari
handwritten digit recognition, combining a convolutional neural network (CNN)
for spatial feature extraction with a 10-qubit variational quantum circuit
(VQC) for quantum-enhanced classification. Trained and evaluated on the
Devanagari Handwritten Character Dataset (DHCD), the proposed model achieves a
state-of-the-art test accuracy for quantum implementation of 99.80% and a test
loss of 0.2893, with an average per-class F1-score of 0.9980. Compared to
equivalent classical CNNs, our model demonstrates superior accuracy with
significantly fewer parameters and enhanced robustness. By leveraging quantum
principles such as superposition and entanglement, this work establishes a
novel benchmark for regional script recognition, highlighting the promise of
quantum machine learning (QML) in real-world, low-resource language settings.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [240] [A Multi-Armed Bandit Framework for Online Optimisation in Green Integrated Terrestrial and Non-Terrestrial Networks](https://arxiv.org/abs/2506.09268)
*Henri Alam,Antonio de Domenico,Tareq Si Salem,Florian Kaltenberger*

Main category: cs.NI

TL;DR: 提出用于集成TN - NTN架构的在线优化框架，可实时平衡网络容量和能源效率，模拟显示能减少高峰时段未满足需求的用户比例，提升吞吐量并节省能源。


<details>
  <summary>Details</summary>
Motivation: 现有研究较少关注NTN在减轻地面网络负载和实现节能运行方面的作用，且地面部署日益密集引发担忧，因此探索NTN对更可持续网络的支持潜力。

Method: 提出基于多臂老虎机（MAB）公式和Bandit - feedback Constrained Online Mirror Descent（BCOMD）算法的在线优化框架，自适应优化关键系统参数。

Result: 24小时系统级模拟显示，该框架显著减少高峰时段未满足需求的用户比例，在低流量时段实现高达19%的吞吐量提升和5%的能源节省。

Conclusion: 所提框架优于遵循3GPP建议的标准网络设置。

Abstract: Integrated terrestrial and non-terrestrial network (TN-NTN) architectures
offer a promising solution for expanding coverage and improving capacity for
the network. While non-terrestrial networks (NTNs) are primarily exploited for
these specific reasons, their role in alleviating terrestrial network (TN) load
and enabling energy-efficient operation has received comparatively less
attention. In light of growing concerns associated with the densification of
terrestrial deployments, this work aims to explore the potential of NTNs in
supporting a more sustainable network. In this paper, we propose a novel online
optimisation framework for integrated TN-NTN architectures, built on a
multi-armed bandit (MAB) formulation and leveraging the Bandit-feedback
Constrained Online Mirror Descent (BCOMD) algorithm. Our approach adaptively
optimises key system parameters--including bandwidth allocation, user equipment
(UE) association, and macro base station (MBS) shutdown--to balance network
capacity and energy efficiency in real time. Extensive system-level simulations
over a 24-hour period show that our framework significantly reduces the
proportion of unsatisfied UEs during peak hours and achieves up to 19%
throughput gains and 5% energy savings in low-traffic periods, outperforming
standard network settings following 3GPP recommendations.

</details>


### [241] [Real-Time Network Traffic Forecasting with Missing Data: A Generative Model Approach](https://arxiv.org/abs/2506.09647)
*Lei Deng,Wenhan Xu,Jingwei Li,Danny H. K. Tsang*

Main category: cs.NI

TL;DR: 提出含缺失数据的实时网络流量预测生成模型方法，建模为张量补全问题，结合预训练生成模型，优化潜在表示，实验验证其准确性和实时性。


<details>
  <summary>Details</summary>
Motivation: 现有网络流量预测方法假设数据全观测，实际数据常因多种因素不完整，需新方法处理含缺失数据的实时预测。

Method: 将网络流量预测任务建模为张量补全问题；结合预训练生成模型实现张量补全的低秩结构；优化潜在表示简化优化过程以实现实时预测；建立理论恢复保证量化误差界。

Result: 在真实数据集实验中，能在100ms内实现准确网络流量预测，Abilene数据集上平均绝对误差低于0.002。

Conclusion: 所提方法可有效处理含缺失数据的实时网络流量预测，具有较好准确性和实时性。

Abstract: Real-time network traffic forecasting is crucial for network management and
early resource allocation. Existing network traffic forecasting approaches
operate under the assumption that the network traffic data is fully observed.
However, in practical scenarios, the collected data are often incomplete due to
various human and natural factors. In this paper, we propose a generative model
approach for real-time network traffic forecasting with missing data. Firstly,
we model the network traffic forecasting task as a tensor completion problem.
Secondly, we incorporate a pre-trained generative model to achieve the low-rank
structure commonly associated with tensor completion. The generative model
effectively captures the intrinsic low-rank structure of network traffic data
during pre-training and enables the mapping from a compact latent
representation to the tensor space. Thirdly, rather than directly optimizing
the high-dimensional tensor, we optimize its latent representation, which
simplifies the optimization process and enables real-time forecasting. We also
establish a theoretical recovery guarantee that quantifies the error bound of
the proposed approach. Experiments on real-world datasets demonstrate that our
approach achieves accurate network traffic forecasting within 100 ms, with a
mean absolute error (MAE) below 0.002, as validated on the Abilene dataset.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [242] [Not all those who drift are lost: Drift correction and calibration scheduling for the IoT](https://arxiv.org/abs/2506.09186)
*Aaron Hurst,Andrey V. Kalinichev,Klaus Koren,Daniel E. Lucani*

Main category: eess.SP

TL;DR: 提出概率传感器漂移校正方法及不确定性驱动校准计划优化方法，在溶解氧传感器测试中降低均方误差。


<details>
  <summary>Details</summary>
Motivation: 传感器老化产生漂移，影响数据质量，以往方法需大量真实数据且未考虑不确定性。

Method: 用高斯过程回归对传感器响应建模的概率传感器漂移校正方法，以及基于漂移校正的不确定性驱动校准计划优化方法。

Result: 漂移校正方法使均方误差最高降低90%，平均超20%；校准计划优化方法进一步降低均方误差达15.7%。

Conclusion: 所提方法能有效解决传感器漂移问题，提高数据质量。

Abstract: Sensors provide a vital source of data that link digital systems with the
physical world. However, as sensors age, the relationship between what they
measure and what they output changes. This is known as sensor drift and poses a
significant challenge that, combined with limited opportunity for
re-calibration, can severely limit data quality over time. Previous approaches
to drift correction typically require large volumes of ground truth data and do
not consider measurement or prediction uncertainty. In this paper, we propose a
probabilistic sensor drift correction method that takes a fundamental approach
to modelling the sensor response using Gaussian Process Regression. Tested
using dissolved oxygen sensors, our method delivers mean squared error (MSE)
reductions of up to 90% and more than 20% on average. We also propose a novel
uncertainty-driven calibration schedule optimisation approach that builds on
top of drift correction and further reduces MSE by up to 15.7%.

</details>


### [243] [Estimating Visceral Adiposity from Wrist-Worn Accelerometry](https://arxiv.org/abs/2506.09167)
*James R. Williamson,Andrew Alini,Brian A. Telfer,Adam W. Potter,Karl E. Friedl*

Main category: eess.SP

TL;DR: 研究利用NHANES数据，用两种方法从身体活动估计内脏脂肪组织（VAT），结合两种方法效果最佳，表明身体活动与VAT及代谢健康风险强相关。


<details>
  <summary>Details</summary>
Motivation: 内脏脂肪组织（VAT）是代谢健康和日常身体活动的关键标志物，研究身体活动与VAT的关系。

Method: 使用2011 - 2014年NHANES数据，用基于步态和睡眠运动特征及岭回归、深度神经网络两种方法从活动估计VAT，加入受试者人口统计学和身体测量协变量信息，最后结合两种方法。

Result: 结合两种方法得到的VAT估计相关性r = 0.86。

Conclusion: 身体活动与VAT有强关系，进而与代谢健康风险也有强关系。

Abstract: Visceral adipose tissue (VAT) is a key marker of both metabolic health and
habitual physical activity (PA). Excess VAT is highly correlated with type 2
diabetes and insulin resistance. The mechanistic basis for this pathophysiology
relates to overloading the liver with fatty acids. VAT is also a highly labile
fat depot, with increased turnover stimulated by catecholamines during
exercise. VAT can be measured with sophisticated imaging technologies, but can
also be inferred directly from PA. We tested this relationship using National
Health and Nutrition Examination Survey (NHANES) data from 2011-2014, for
individuals aged 20-60 years with 7 days of accelerometry data (n=2,456 men;
2,427 women) [1]. Two approaches were used for estimating VAT from activity.
The first used engineered features based on movements during gait and sleep,
and then ridge regression to map summary statistics of these features into a
VAT estimate. The second approach used deep neural networks trained on 24 hours
of continuous accelerometry. A foundation model first mapped each 10s frame
into a high-dimensional feature vector. A transformer model then mapped each
day's feature vector time series into a VAT estimate, which were averaged over
multiple days. For both approaches, the most accurate estimates were obtained
with the addition of covariate information about subject demographics and body
measurements. The best performance was obtained by combining the two
approaches, resulting in VAT estimates with correlations of r=0.86. These
findings demonstrate a strong relationship between PA and VAT and, by
extension, between PA and metabolic health risks.

</details>


### [244] [Integration of Contrastive Predictive Coding and Spiking Neural Networks](https://arxiv.org/abs/2506.09194)
*Emirhan Bilgiç,Neslihan Serap Şengör,Namık Berk Yalabık,Yavuz Selim İşler,Aykut Görkem Gelen,Rahmi Elibol*

Main category: eess.SP

TL;DR: 研究对比预测编码（CPC）与脉冲神经网络（SNN）的集成，在MNIST数据集测试取得高分类率。


<details>
  <summary>Details</summary>
Motivation: 通过在基于脉冲的系统中处理输入输出，开发具有更高生物学合理性的预测编码模型。

Method: 将CPC与SNN结合，在MNIST数据集上测试模型。

Result: 模型在区分正序样本和非顺序负样本时取得高分类率。

Conclusion: CPC能与SNN有效结合，用于分类任务的SNN也可作为编码机制。

Abstract: This study examines the integration of Contrastive Predictive Coding (CPC)
with Spiking Neural Networks (SNN). While CPC learns the predictive structure
of data to generate meaningful representations, SNN mimics the computational
processes of biological neural systems over time. In this study, the goal is to
develop a predictive coding model with greater biological plausibility by
processing inputs and outputs in a spike-based system. The proposed model was
tested on the MNIST dataset and achieved a high classification rate in
distinguishing positive sequential samples from non-sequential negative
samples. The study demonstrates that CPC can be effectively combined with SNN,
showing that an SNN trained for classification tasks can also function as an
encoding mechanism. Project codes and detailed results can be accessed on our
GitHub page: https://github.com/vnd-ogrenme/ongorusel-kodlama/tree/main/CPC_SNN

</details>


### [245] [Graph Attention-based Decentralized Actor-Critic for Dual-Objective Control of Multi-UAV Swarms](https://arxiv.org/abs/2506.09195)
*Haoran Peng,Ying-Jun Angela Zhang*

Main category: eess.SP

TL;DR: 研究提出GADC优化多无人机系统双目标，测试显示其性能优越。


<details>
  <summary>Details</summary>
Motivation: 优化多无人机系统，实现最大化服务覆盖为主、延长电池寿命为辅的双目标。

Method: 提出基于图注意力的分散式演员-评论家（GADC）方法，用图注意力网络处理局部观测、降维，用演员-双评论家网络管理双策略，用KL散度因子平衡双目标。

Result: 通过理论和实验基准测试评估了GADC的可扩展性和效率，在理想和现实环境测试中显示出优越性能。

Conclusion: GADC在多无人机系统双目标优化中表现良好，具有应用价值。

Abstract: This research focuses on optimizing multi-UAV systems with dual objectives:
maximizing service coverage as the primary goal while extending battery
lifetime as the secondary objective. We propose a Graph Attention-based
Decentralized Actor-Critic (GADC) to optimize the dual objectives. The proposed
approach leverages a graph attention network to process UAVs' limited local
observation and reduce the dimension of the environment states. Subsequently,
an actor-double-critic network is developed to manage dual policies for joint
objective optimization. The proposed GADC uses a Kullback-Leibler (KL)
divergence factor to balance the tradeoff between coverage performance and
battery lifetime in the multi-UAV system. We assess the scalability and
efficiency of GADC through comprehensive benchmarking against state-of-the-art
methods, considering both theory and experimental aspects. Extensive testing in
both ideal settings and NVIDIA Sionna's realistic ray tracing environment
demonstrates GADC's superior performance.

</details>


### [246] [AI-Driven SEEG Channel Ranking for Epileptogenic Zone Localization](https://arxiv.org/abs/2506.09255)
*Saeed Hashemi,Genchang Peng,Mehrdad Nourani,Omar Nofal,Jay Harvey*

Main category: eess.SP

TL;DR: 提出机器学习方法对SEEG通道进行排序，分析五位患者数据取得有前景结果。


<details>
  <summary>Details</summary>
Motivation: 传统对SEEG数百通道信号的视觉检查耗时低效，需要更优方法。

Method: 结合临床医生选择和计算发现，用XGBoost训练分类模型学习发作期通道特征，用SHAP评分排序通道，采用通道扩展策略扩大搜索空间。

Result: 分析五位患者的SEEG数据，在准确性、一致性和可解释性方面有有前景的结果。

Conclusion: 所提出的机器学习方法在SEEG通道排序上可行且有效。

Abstract: Stereo-electroencephalography (SEEG) is an invasive technique to implant
depth electrodes and collect data for pre-surgery evaluation. Visual inspection
of signals recorded from hundreds of channels is time consuming and
inefficient. We propose a machine learning approach to rank the impactful
channels by incorporating clinician's selection and computational finding. A
classification model using XGBoost is trained to learn the discriminative
features of each channel during ictal periods. Then, the SHapley Additive
exPlanations (SHAP) scoring is utilized to rank SEEG channels based on their
contribution to seizures. A channel extension strategy is also incorporated to
expand the search space and identify suspicious epileptogenic zones beyond
those selected by clinicians. For validation, SEEG data for five patients were
analyzed showing promising results in terms of accuracy, consistency, and
explainability.

</details>


### [247] [Cross-Channel Unlabeled Sensing over a Union of Signal Subspaces](https://arxiv.org/abs/2506.09773)
*Taulant Koka,Manolis C. Tsakiris,Benjamín Béjar Haro,Michael Muma*

Main category: eess.SP

TL;DR: 本文将跨通道未标记传感框架扩展到子空间并集中的信号，改进先前模型，在全脑钙成像应用中验证其效用。


<details>
  <summary>Details</summary>
Motivation: 处理更复杂信号结构，拓宽框架到压缩感知等任务，解决样本与通道不匹配问题。

Method: 推导更严格的唯一重建所需样本数量的界限，支持更通用信号类型。

Result: 在全脑钙成像应用中得到验证，能在样本 - 通道关联不精确的现实场景中实现准确信号重建。

Conclusion: 所提出的框架在处理复杂信号结构和现实场景中样本 - 通道关联不精确问题上具有实用性。

Abstract: Cross-channel unlabeled sensing addresses the problem of recovering a
multi-channel signal from measurements that were shuffled across channels. This
work expands the cross-channel unlabeled sensing framework to signals that lie
in a union of subspaces. The extension allows for handling more complex signal
structures and broadens the framework to tasks like compressed sensing. These
mismatches between samples and channels often arise in applications such as
whole-brain calcium imaging of freely moving organisms or multi-target
tracking. We improve over previous models by deriving tighter bounds on the
required number of samples for unique reconstruction, while supporting more
general signal types. The approach is validated through an application in
whole-brain calcium imaging, where organism movements disrupt sample-to-neuron
mappings. This demonstrates the utility of our framework in real-world settings
with imprecise sample-channel associations, achieving accurate signal
reconstruction.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [248] [Exploring Image Transforms derived from Eye Gaze Variables for Progressive Autism Diagnosis](https://arxiv.org/abs/2506.09065)
*Abigail Copiaco,Christian Ritz,Yassine Himeur,Valsamma Eapen,Ammar Albanna,Wathiq Mansoor*

Main category: eess.IV

TL;DR: 文章介绍了一种用于自闭症谱系障碍（ASD）诊断和管理的人工智能辅助技术，该技术可简化诊断流程，保护隐私并改善患者预后。


<details>
  <summary>Details</summary>
Motivation: 当前ASD诊断技术耗时，带来高社会和经济成本，需更便捷高效的诊断方法。

Method: 将迁移学习与基于眼动变量的图像变换相结合来诊断ASD。

Result: 实现家庭定期诊断，减轻患者和护理人员压力，保护隐私，促进监护人与治疗师沟通。

Conclusion: 该方法能及时、便捷诊断，保护隐私，改善ASD患者预后。

Abstract: The prevalence of Autism Spectrum Disorder (ASD) has surged rapidly over the
past decade, posing significant challenges in communication, behavior, and
focus for affected individuals. Current diagnostic techniques, though
effective, are time-intensive, leading to high social and economic costs. This
work introduces an AI-powered assistive technology designed to streamline ASD
diagnosis and management, enhancing convenience for individuals with ASD and
efficiency for caregivers and therapists. The system integrates transfer
learning with image transforms derived from eye gaze variables to diagnose ASD.
This facilitates and opens opportunities for in-home periodical diagnosis,
reducing stress for individuals and caregivers, while also preserving user
privacy through the use of image transforms. The accessibility of the proposed
method also offers opportunities for improved communication between guardians
and therapists, ensuring regular updates on progress and evolving support
needs. Overall, the approach proposed in this work ensures timely, accessible
diagnosis while protecting the subjects' privacy, improving outcomes for
individuals with ASD.

</details>


### [249] [Foundation Models in Medical Imaging -- A Review and Outlook](https://arxiv.org/abs/2506.09095)
*Vivien van Veldhuizen,Vanessa Botha,Chunyao Lu,Melis Erdal Cesur,Kevin Groot Lipman,Edwin D. de Jong,Hugo Horlings,Clárisa Sanchez,Cees Snoek,Ritse Mann,Eric Marcus,Jonas Teuwen*

Main category: eess.IV

TL;DR: 本文综述基础模型在医学图像分析中的应用，涵盖多领域研究，介绍核心组件、应用情况，讨论挑战与问题。


<details>
  <summary>Details</summary>
Motivation: 探讨基础模型在医学图像分析领域（病理、放射、眼科）的发展和应用情况。

Method: 分析超150项研究的证据，解释基础模型管道核心组件，回顾各成像领域应用并对比设计选择。

Result: 明确基础模型在医学图像分析不同领域的应用方式和设计选择。

Conclusion: 提出关键挑战和开放性问题以指导未来研究。

Abstract: Foundation models (FMs) are changing the way medical images are analyzed by
learning from large collections of unlabeled data. Instead of relying on
manually annotated examples, FMs are pre-trained to learn general-purpose
visual features that can later be adapted to specific clinical tasks with
little additional supervision. In this review, we examine how FMs are being
developed and applied in pathology, radiology, and ophthalmology, drawing on
evidence from over 150 studies. We explain the core components of FM pipelines,
including model architectures, self-supervised learning methods, and strategies
for downstream adaptation. We also review how FMs are being used in each
imaging domain and compare design choices across applications. Finally, we
discuss key challenges and open questions to guide future research.

</details>


### [250] [Low-Rank Augmented Implicit Neural Representation for Unsupervised High-Dimensional Quantitative MRI Reconstruction](https://arxiv.org/abs/2506.09100)
*Haonan Zhang,Guoyan Lao,Yuyao Zhang,Hongjiang Wei*

Main category: eess.IV

TL;DR: 提出LoREIN框架用于加速3D多参数定量磁共振成像（MP - qMRI）重建，结合双先验提高重建精度，还引入零样本学习范式。


<details>
  <summary>Details</summary>
Motivation: 当前仅依赖单一先验或物理模型的重建方法在解决高欠采样、高维测量的qMRI重建这一病态逆问题时效果不佳，需新方法提升重建效果。

Method: 提出LoREIN框架，通过低秩表示（LRR）和隐式神经表示（INR）分别引入低秩先验和连续性先验，结合两者优势进行重建，还引入零样本学习范式。

Result: 该框架能增强加权图像的高保真重建，预测的多对比度加权图像可提高定量参数图的重建精度。

Conclusion: LoREIN框架有效解决了qMRI重建难题，零样本学习范式在复杂时空和高维图像重建任务有广泛潜力，推动医学成像领域发展。

Abstract: Quantitative magnetic resonance imaging (qMRI) provides tissue-specific
parameters vital for clinical diagnosis. Although simultaneous multi-parametric
qMRI (MP-qMRI) technologies enhance imaging efficiency, robustly reconstructing
qMRI from highly undersampled, high-dimensional measurements remains a
significant challenge. This difficulty arises primarily because current
reconstruction methods that rely solely on a single prior or physics-informed
model to solve the highly ill-posed inverse problem, which often leads to
suboptimal results. To overcome this limitation, we propose LoREIN, a novel
unsupervised and dual-prior-integrated framework for accelerated 3D MP-qMRI
reconstruction. Technically, LoREIN incorporates both low-rank prior and
continuity prior via low-rank representation (LRR) and implicit neural
representation (INR), respectively, to enhance reconstruction fidelity. The
powerful continuous representation of INR enables the estimation of optimal
spatial bases within the low-rank subspace, facilitating high-fidelity
reconstruction of weighted images. Simultaneously, the predicted multi-contrast
weighted images provide essential structural and quantitative guidance, further
enhancing the reconstruction accuracy of quantitative parameter maps.
Furthermore, our work introduces a zero-shot learning paradigm with broad
potential in complex spatiotemporal and high-dimensional image reconstruction
tasks, further advancing the field of medical imaging.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [251] [Abstraction-Based Proof Production in Formal Verification of Neural Networks](https://arxiv.org/abs/2506.09455)
*Yizhak Yisrael Elboher,Omri Isac,Guy Katz,Tobias Ladner,Haoze Wu*

Main category: cs.LO

TL;DR: 提出用于DNN验证的基于抽象且可生成证明的新框架，以解决可扩展性和可证明保证之间的差距。


<details>
  <summary>Details</summary>
Motivation: 当前可生成证明的验证器不支持基于抽象的推理，存在可扩展性和可证明保证之间的差距。

Method: 将验证任务模块化分为证明抽象网络正确性和证明抽象相对于原始DNN的合理性两部分，前者用现有验证器处理，后者提出新的生成形式化证明的方法。

Result: 提出新的基于抽象且可生成证明的DNN验证框架。

Conclusion: 该初步工作旨在通过在形式化证明框架内支持常见抽象技术，实现可扩展且可信的验证。

Abstract: Modern verification tools for deep neural networks (DNNs) increasingly rely
on abstraction to scale to realistic architectures. In parallel, proof
production is becoming a critical requirement for increasing the reliability of
DNN verification results. However, current proofproducing verifiers do not
support abstraction-based reasoning, creating a gap between scalability and
provable guarantees. We address this gap by introducing a novel framework for
proof-producing abstraction-based DNN verification. Our approach modularly
separates the verification task into two components: (i) proving the
correctness of an abstract network, and (ii) proving the soundness of the
abstraction with respect to the original DNN. The former can be handled by
existing proof-producing verifiers, whereas we propose the first method for
generating formal proofs for the latter. This preliminary work aims to enable
scalable and trustworthy verification by supporting common abstraction
techniques within a formal proof framework.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [252] [PGDA-KGQA: A Prompt-Guided Generative Framework with Multiple Data Augmentation Strategies for Knowledge Graph Question Answering](https://arxiv.org/abs/2506.09414)
*Xiujun Zhou,Pingjian Zhang,Deyou Tang*

Main category: cs.CL

TL;DR: 本文提出PGDA - KGQA框架解决KGQA数据稀缺和推理问题，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有KGQA利用LLM的方法受限于数据稀缺和多跳推理样本不足，传统数据增强方法存在语义失真和多跳推理忽视问题，限制了数据多样性和模型泛化能力。

Method: 提出PGDA - KGQA框架，采用统一提示设计范式，通过生成单跳伪问题、语义保留的问题重写、答案引导的反向路径探索丰富训练集，采用增强 - 生成 - 检索语义解析管道。

Result: 在标准KGQA数据集上表现优于现有方法，在WebQSP和ComplexWebQuestions数据集的F1、Hits@1和Accuracy指标上有提升。

Conclusion: PGDA - KGQA框架有效解决了KGQA数据稀缺和多跳推理问题，提升了答案检索性能。

Abstract: Knowledge Graph Question Answering (KGQA) is a crucial task in natural
language processing that requires reasoning over knowledge graphs (KGs) to
answer natural language questions. Recent methods utilizing large language
models (LLMs) have shown remarkable semantic parsing capabilities but are
limited by the scarcity of diverse annotated data and multi-hop reasoning
samples. Traditional data augmentation approaches are focus mainly on
single-hop questions and prone to semantic distortion, while LLM-based methods
primarily address semantic distortion but usually neglect multi-hop reasoning,
thus limiting data diversity. The scarcity of multi-hop samples further weakens
models' generalization. To address these issues, we propose PGDA-KGQA, a
prompt-guided generative framework with multiple data augmentation strategies
for KGQA. At its core, PGDA-KGQA employs a unified prompt-design paradigm: by
crafting meticulously engineered prompts that integrate the provided textual
content, it leverages LLMs to generate large-scale (question, logical form)
pairs for model training. Specifically, PGDA-KGQA enriches its training set by:
(1) generating single-hop pseudo questions to improve the alignment of question
semantics with KG relations; (2) applying semantic-preserving question
rewriting to improve robustness against linguistic variations; (3) employing
answer-guided reverse path exploration to create realistic multi-hop questions.
By adopting an augment-generate-retrieve semantic parsing pipeline, PGDA-KGQA
utilizes the augmented data to enhance the accuracy of logical form generation
and thus improve answer retrieval performance. Experiments demonstrate that
outperforms state-of-the-art methods on standard KGQA datasets, achieving
improvements on WebQSP by 2.8%, 1.2%, and 3.1% and on ComplexWebQuestions by
1.8%, 1.1%, and 2.4% in F1, Hits@1, and Accuracy, respectively.

</details>


### [253] [Learning Efficient and Generalizable Graph Retriever for Knowledge-Graph Question Answering](https://arxiv.org/abs/2506.09645)
*Tianjun Yao,Haoxuan Li,Zhiqiang Shen,Pan Li,Tongliang Liu,Kun Zhang*

Main category: cs.CL

TL;DR: 现有RAG管道和基于图的检索器有局限性，提出RAPL框架用于KGQA的图检索，性能超SOTA。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在知识过时和幻觉问题，现有RAG管道依赖非结构化文本，基于图的检索器泛化能力有挑战。

Method: 提出RAPL框架，包含两阶段标注策略、模型无关图转换方法和基于路径的推理策略。

Result: RAPL比SOTA方法性能高2.66%-20.34%，缩小不同规模模型和跨数据集设置下的性能差距。

Conclusion: RAPL有优越的检索能力和泛化性。

Abstract: Large Language Models (LLMs) have shown strong inductive reasoning ability
across various domains, but their reliability is hindered by the outdated
knowledge and hallucinations. Retrieval-Augmented Generation mitigates these
issues by grounding LLMs with external knowledge; however, most existing RAG
pipelines rely on unstructured text, limiting interpretability and structured
reasoning. Knowledge graphs, which represent facts as relational triples, offer
a more structured and compact alternative. Recent studies have explored
integrating knowledge graphs with LLMs for knowledge graph question answering
(KGQA), with a significant proportion adopting the retrieve-then-reasoning
paradigm. In this framework, graph-based retrievers have demonstrated strong
empirical performance, yet they still face challenges in generalization
ability. In this work, we propose RAPL, a novel framework for efficient and
effective graph retrieval in KGQA. RAPL addresses these limitations through
three aspects: (1) a two-stage labeling strategy that combines heuristic
signals with parametric models to provide causally grounded supervision; (2) a
model-agnostic graph transformation approach to capture both intra- and
inter-triple interactions, thereby enhancing representational capacity; and (3)
a path-based reasoning strategy that facilitates learning from the injected
rational knowledge, and supports downstream reasoner through structured inputs.
Empirically, RAPL outperforms state-of-the-art methods by $2.66\%-20.34\%$, and
significantly reduces the performance gap between smaller and more powerful
LLM-based reasoners, as well as the gap under cross-dataset settings,
highlighting its superior retrieval capability and generalizability. Codes are
available at: https://github.com/tianyao-aka/RAPL.

</details>


### [254] [RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling](https://arxiv.org/abs/2506.08672)
*Yang Liu,Jiaqi Li,Zilong Zheng*

Main category: cs.CL

TL;DR: 本文提出RuleReasoner方法解决小推理模型基于规则推理问题，在多个基准测试中表现优于前沿大推理模型，且计算效率更高。


<details>
  <summary>Details</summary>
Motivation: 现实应用中规则格式等偏差给基于规则的推理带来挑战，不确定小推理模型能否有效学习基于规则的推理并在不同任务和领域中泛化。

Method: 引入Reinforced Rule - based Reasoning（RuleReasoner）方法，通过大量精选任务和新颖的领域感知动态采样方法进行基于规则的推理，根据历史奖励更新不同领域采样权重来重新采样训练批次。

Result: 在分布内（ID）和分布外（OOD）基准测试中，RuleReasoner显著优于前沿大推理模型（在8个ID任务中平均高4.1%，在3个OOD任务中平均高10.4%），且计算效率高于先前的强化学习动态采样方法。

Conclusion: RuleReasoner是一种简单有效的基于规则推理方法，能有效解决小推理模型基于规则推理的泛化问题。

Abstract: Rule-based reasoning has been acknowledged as one of the fundamental problems
in reasoning, while deviations in rule formats, types, and complexity in
real-world applications pose severe challenges. Recent studies have shown that
large reasoning models (LRMs) have remarkable reasoning capabilities, and their
performance is substantially enhanced by reinforcement learning (RL). However,
it remains an open question whether small reasoning models (SRMs) can learn
rule-based reasoning effectively with robust generalization across diverse
tasks and domains. To address this, we introduce Reinforced Rule-based
Reasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct
rule-based reasoning via a wide collection of curated tasks and a novel
domain-aware dynamic sampling approach. Specifically, RuleReasoner resamples
each training batch by updating the sampling weights of different domains based
on historical rewards. This facilitates domain augmentation and flexible online
learning schedules for RL, obviating the need for pre-hoc human-engineered
mix-training recipes used in existing methods. Empirical evaluations on
in-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that
RuleReasoner outperforms frontier LRMs by a significant margin ($\Delta$4.1%
average points on eight ID tasks and $\Delta$10.4% average points on three OOD
tasks over OpenAI-o1). Notably, our approach also exhibits higher computational
efficiency compared to prior dynamic sampling methods for RL.

</details>


### [255] [ComfyUI-R1: Exploring Reasoning Models for Workflow Generation](https://arxiv.org/abs/2506.09790)
*Zhenran Xu,Yiyu Wang,Xue Yang,Longyue Wang,Weihua Luo,Kaifu Zhang,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: 引入ComfyUI - R1用于自动生成工作流，通过两阶段框架训练，实验表明其表现超现有方法，凸显长链思维推理在AI艺术创作潜力。


<details>
  <summary>Details</summary>
Motivation: 解决在ComfyUI平台上创建有效工作流需要大量专业知识、学习曲线陡峭的问题。

Method: 从4K工作流数据集构建长链思维推理数据，采用两阶段框架训练：CoT微调冷启动和强化学习激励推理能力。

Result: 7B参数模型格式有效性达97%，通过率、节点和图级F1分数高，超GPT - 4o和Claude等现有方法。

Conclusion: 长链思维推理过程在AI艺术创作中很关键，将工作流转换为代码有优势，ComfyUI - R1在合成复杂工作流上有优势。

Abstract: AI-generated content has evolved from monolithic models to modular workflows,
particularly on platforms like ComfyUI, enabling customization in creative
pipelines. However, crafting effective workflows requires great expertise to
orchestrate numerous specialized components, presenting a steep learning curve
for users. To address this challenge, we introduce ComfyUI-R1, the first large
reasoning model for automated workflow generation. Starting with our curated
dataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning
data, including node selection, workflow planning, and code-level workflow
representation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT
fine-tuning for cold start, adapting models to the ComfyUI domain; (2)
reinforcement learning for incentivizing reasoning capability, guided by a
fine-grained rule-metric hybrid reward, ensuring format validity, structural
integrity, and node-level fidelity. Experiments show that our 7B-parameter
model achieves a 97\% format validity rate, along with high pass rate,
node-level and graph-level F1 scores, significantly surpassing prior
state-of-the-art methods that employ leading closed-source models such as
GPT-4o and Claude series. Further analysis highlights the critical role of the
reasoning process and the advantage of transforming workflows into code.
Qualitative comparison reveals our strength in synthesizing intricate workflows
with diverse nodes, underscoring the potential of long CoT reasoning in AI art
creation.

</details>


### [256] [LLM-as-a-qualitative-judge: automating error analysis in natural language generation](https://arxiv.org/abs/2506.09147)
*Nadezhda Chirkova,Tunde Oluwaseyi Ajayi,Seth Aycock,Zain Muhammad Mujahid,Vladana Perlić,Ekaterina Borisova,Markarit Vartampetian*

Main category: cs.CL

TL;DR: 提出LLM - as - a - qualitative - judge评估方法，输出NLG系统常见问题类型结构化报告，结果显示能识别问题且生成类似人工的报告。


<details>
  <summary>Details</summary>
Motivation: 现有LLM - as - a - judge主要作为定量工具，需为NLG系统开发者提供改进建议。

Method: 分两步，即开放式单实例问题分析和用累积算法对发现问题聚类，还引入评估策略并进行约300条标注。

Result: 能在2/3情况下正确识别特定实例问题，生成类似人工标注的错误类型报告。

Conclusion: LLM - as - a - qualitative - judge方法有效，代码和数据公开。

Abstract: Prompting large language models (LLMs) to evaluate generated text, known as
LLM-as-a-judge, has become a standard evaluation approach in natural language
generation (NLG), but is primarily used as a quantitative tool, i.e. with
numerical scores as main outputs. In this work, we propose
LLM-as-a-qualitative-judge, an LLM-based evaluation approach with the main
output being a structured report of common issue types in the NLG system
outputs. Our approach is targeted at providing developers with meaningful
insights on what improvements can be done to a given NLG system and consists of
two main steps, namely open-ended per-instance issue analysis and clustering of
the discovered issues using an intuitive cumulative algorithm. We also
introduce a strategy for evaluating the proposed approach, coupled with ~300
annotations of issues in instances from 12 NLG datasets. Our results show that
LLM-as-a-qualitative-judge correctly recognizes instance-specific issues in 2/3
cases and is capable of producing error type reports resembling the reports
composed by human annotators. Our code and data are publicly available at
https://github.com/tunde-ajayi/llm-as-a-qualitative-judge.

</details>


### [257] [PHRASED: Phrase Dictionary Biasing for Speech Translation](https://arxiv.org/abs/2506.09175)
*Peidong Wang,Jian Xue,Rui Zhao,Junkun Chen,Aswin Shanmugam Subramanian,Jinyu Li*

Main category: cs.CL

TL;DR: 本文提出短语词典偏置方法用于语音翻译，应用于两种模型并取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 短语在训练数据中出现频率低，在语音翻译任务中正确翻译短语具有挑战性。

Method: 提出短语词典偏置方法，并应用于基于变换器的流式语音翻译模型和多模态大语言模型。

Result: 对于流式语音翻译模型，该方法相对短语列表偏置提升21%；使多模态大语言模型在短语召回率上相对提升85%。

Conclusion: 短语词典偏置方法在语音翻译中有效，能提升模型性能。

Abstract: Phrases are essential to understand the core concepts in conversations.
However, due to their rare occurrence in training data, correct translation of
phrases is challenging in speech translation tasks. In this paper, we propose a
phrase dictionary biasing method to leverage pairs of phrases mapping from the
source language to the target language. We apply the phrase dictionary biasing
method to two types of widely adopted models, a transducer-based streaming
speech translation model and a multimodal large language model. Experimental
results show that the phrase dictionary biasing method outperforms phrase list
biasing by 21% relatively for the streaming speech translation model. In
addition, phrase dictionary biasing enables multimodal large language models to
use external phrase information, achieving 85% relative improvement in phrase
recall.

</details>


### [258] [Extrapolation by Association: Length Generalization Transfer in Transformers](https://arxiv.org/abs/2506.09251)
*Ziyang Cai,Nayoung Lee,Avi Schwarzschild,Samet Oymak,Dimitris Papailiopoulos*

Main category: cs.CL

TL;DR: 研究Transformer语言模型的长度泛化能力，发现可跨相关任务转移，在多种算法任务和预训练模型中得到验证，揭示与注意力头复用有关。


<details>
  <summary>Details</summary>
Motivation: 缺乏对Transformer语言模型泛化能力产生机制的细粒度理解，重点研究长度泛化。

Method: 从任务关联角度研究长度泛化，通过训练模型在相关辅助任务上，观察其在目标任务上对更长未见输入的泛化情况。

Result: Transformer模型在联合训练时可从相似任务继承泛化能力，预训练模型有类似转移效果，长度泛化转移与任务间注意力头复用相关。

Conclusion: 加深对Transformer处理分布外输入泛化机制的理解，强调任务间归纳结构的组合复用。

Abstract: Transformer language models have demonstrated impressive generalization
capabilities in natural language domains, yet we lack a fine-grained
understanding of how such generalization arises. In this paper, we investigate
length generalization--the ability to extrapolate from shorter to longer
inputs--through the lens of \textit{task association}. We find that length
generalization can be \textit{transferred} across related tasks. That is,
training a model with a longer and related auxiliary task can lead it to
generalize to unseen and longer inputs from some other target task. We
demonstrate this length generalization transfer across diverse algorithmic
tasks, including arithmetic operations, string transformations, and maze
navigation. Our results show that transformer models can inherit generalization
capabilities from similar tasks when trained jointly. Moreover, we observe
similar transfer effects in pretrained language models, suggesting that
pretraining equips models with reusable computational scaffolding that
facilitates extrapolation in downstream settings. Finally, we provide initial
mechanistic evidence that length generalization transfer correlates with the
re-use of the same attention heads between the tasks. Together, our findings
deepen our understanding of how transformers generalize to out-of-distribution
inputs and highlight the compositional reuse of inductive structure across
tasks.

</details>


### [259] [Self-Anchored Attention Model for Sample-Efficient Classification of Prosocial Text Chat](https://arxiv.org/abs/2506.09259)
*Zhuofang Li,Rafal Kocielnik,Fereshteh Soltani,Penphob,Boonyarungsrit,Animashree Anandkumar,R. Michael Alvarez*

Main category: cs.CL

TL;DR: 本文用无监督发现和专家协作识别分类游戏聊天中亲社会行为，提出SAAM模型，开发首个自动分类系统，应用于《使命召唤：现代战争II》，有助于促进线上积极互动。


<details>
  <summary>Details</summary>
Motivation: 以往研究多关注游戏聊天中少量有毒内容检测，如今需重视亲社会交流，但识别亲社会行为的数据集、模型和资源有限。

Method: 采用无监督发现与游戏领域专家协作，提出Self - Anchored Attention Model (SAAM)，利用整个训练集作为“锚点”提升性能。

Result: SAAM比现有最佳技术提升7.9%性能，开发出首个游戏聊天亲社会行为自动分类系统，应用于《使命召唤：现代战争II》展示了有效性。

Conclusion: 该研究将NLP技术用于发现和分类游戏聊天亲社会行为，有助于将审核重点从惩罚毒性内容转向鼓励积极互动。

Abstract: Millions of players engage daily in competitive online games, communicating
through in-game chat. Prior research has focused on detecting relatively small
volumes of toxic content using various Natural Language Processing (NLP)
techniques for the purpose of moderation. However, recent studies emphasize the
importance of detecting prosocial communication, which can be as crucial as
identifying toxic interactions. Recognizing prosocial behavior allows for its
analysis, rewarding, and promotion. Unlike toxicity, there are limited
datasets, models, and resources for identifying prosocial behaviors in
game-chat text. In this work, we employed unsupervised discovery combined with
game domain expert collaboration to identify and categorize prosocial player
behaviors from game chat. We further propose a novel Self-Anchored Attention
Model (SAAM) which gives 7.9% improvement compared to the best existing
technique. The approach utilizes the entire training set as "anchors" to help
improve model performance under the scarcity of training data. This approach
led to the development of the first automated system for classifying prosocial
behaviors in in-game chats, particularly given the low-resource settings where
large-scale labeled data is not available. Our methodology was applied to one
of the most popular online gaming titles - Call of Duty(R): Modern
Warfare(R)II, showcasing its effectiveness. This research is novel in applying
NLP techniques to discover and classify prosocial behaviors in player in-game
chat communication. It can help shift the focus of moderation from solely
penalizing toxicity to actively encouraging positive interactions on online
platforms.

</details>


### [260] [$(RSA)^2$: A Rhetorical-Strategy-Aware Rational Speech Act Framework for Figurative Language Understanding](https://arxiv.org/abs/2506.09301)
*Cesare Spinoso-Di Piano,David Austin,Pablo Piantanida,Jackie Chi Kit Cheung*

Main category: cs.CL

TL;DR: 提出(RSA)^2框架用于建模比喻性语言使用，结合大语言模型在新数据集上取得最优表现。


<details>
  <summary>Details</summary>
Motivation: 现有RSA框架无法很好处理比喻性表达，或需特定设置下建模隐含动机。

Method: 引入考虑说话者修辞策略的(RSA)^2框架。

Result: (RSA)^2能实现非字面表述的类人解释，结合大语言模型在新数据集的反讽子集上取得最优表现。

Conclusion: (RSA)^2框架可在不建模说话者非字面表达动机的情况下处理比喻性语言。

Abstract: Figurative language (e.g., irony, hyperbole, understatement) is ubiquitous in
human communication, resulting in utterances where the literal and the intended
meanings do not match. The Rational Speech Act (RSA) framework, which
explicitly models speaker intentions, is the most widespread theory of
probabilistic pragmatics, but existing implementations are either unable to
account for figurative expressions or require modeling the implicit motivations
for using figurative language (e.g., to express joy or annoyance) in a
setting-specific way. In this paper, we introduce the Rhetorical-Strategy-Aware
RSA $(RSA)^2$ framework which models figurative language use by considering a
speaker's employed rhetorical strategy. We show that $(RSA)^2$ enables
human-compatible interpretations of non-literal utterances without modeling a
speaker's motivations for being non-literal. Combined with LLMs, it achieves
state-of-the-art performance on the ironic split of PragMega+, a new irony
interpretation dataset introduced in this study.

</details>


### [261] [Alzheimer's Dementia Detection Using Perplexity from Paired Large Language Models](https://arxiv.org/abs/2506.09315)
*Yao Xiao,Heidi Christensen,Stefan Goetze*

Main category: cs.CL

TL;DR: 本文用Mistral - 7B指令跟随版本扩展配对困惑度方法检测阿尔茨海默病痴呆（AD），提高检测准确率，有清晰决策边界，并表明大语言模型学习到AD患者语言模式。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病痴呆会影响语言能力，需要有效检测方法。

Method: 使用Mistral - 7B指令跟随版本扩展配对困惑度方法来检测AD，还通过提示微调大语言模型并比较其与人类的响应。

Result: 相比当前最佳配对困惑度方法平均提高准确率3.33%，比ADReSS 2020挑战赛基准排名第一的方法提高6.35%，能以清晰可解释的决策边界有效检测AD，大语言模型学到AD患者语言模式。

Conclusion: 所提出的方法可有效检测AD，为模型解释和数据增强提供新可能。

Abstract: Alzheimer's dementia (AD) is a neurodegenerative disorder with cognitive
decline that commonly impacts language ability. This work extends the paired
perplexity approach to detecting AD by using a recent large language model
(LLM), the instruction-following version of Mistral-7B. We improve accuracy by
an average of 3.33% over the best current paired perplexity method and by 6.35%
over the top-ranked method from the ADReSS 2020 challenge benchmark. Our
further analysis demonstrates that the proposed approach can effectively detect
AD with a clear and interpretable decision boundary in contrast to other
methods that suffer from opaque decision-making processes. Finally, by
prompting the fine-tuned LLMs and comparing the model-generated responses to
human responses, we illustrate that the LLMs have learned the special language
patterns of AD speakers, which opens up possibilities for novel methods of
model interpretation and data augmentation.

</details>


### [262] [Multi-Agent Language Models: Advancing Cooperation, Coordination, and Adaptation](https://arxiv.org/abs/2506.09331)
*Arjun Vaithilingam Sudhakar*

Main category: cs.CL

TL;DR: 探讨大语言模型是否具备心智理论，通过合作多智能体强化学习研究，目标是提升智能体协作能力，促进人机交互。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽有泛化能力，但未明确是否有心智理论，理解他人意图对协作至关重要，故研究大语言模型的心智理论。

Method: 从合作多智能体强化学习角度研究大语言模型的心智理论，利用能进行自然语言交互的基于大语言模型的智能体。

Result: 未提及具体研究结果。

Conclusion: 朝着创建能实现无缝协作的人机混合系统迈进，对未来人机交互有广泛意义。

Abstract: Modern Large Language Models (LLMs) exhibit impressive zero-shot and few-shot
generalization capabilities across complex natural language tasks, enabling
their widespread use as virtual assistants for diverse applications such as
translation and summarization. Despite being trained solely on large corpora of
text without explicit supervision on author intent, LLMs appear to infer the
underlying meaning of textual interactions. This raises a fundamental question:
can LLMs model and reason about the intentions of others, i.e., do they possess
a form of theory of mind? Understanding other's intentions is crucial for
effective collaboration, which underpins human societal success and is
essential for cooperative interactions among multiple agents, including humans
and autonomous systems. In this work, we investigate the theory of mind in LLMs
through the lens of cooperative multi-agent reinforcement learning (MARL),
where agents learn to collaborate via repeated interactions, mirroring human
social reasoning. Our approach aims to enhance artificial agent's ability to
adapt and cooperate with both artificial and human partners. By leveraging
LLM-based agents capable of natural language interaction, we move towards
creating hybrid human-AI systems that can foster seamless collaboration, with
broad implications for the future of human-artificial interaction.

</details>


### [263] [RePO: Replay-Enhanced Policy Optimization](https://arxiv.org/abs/2506.09340)
*Siheng Li,Zhanhui Zhou,Wai Lam,Chao Yang,Chaochao Lu*

Main category: cs.CL

TL;DR: 提出RePO优化大语言模型，在数学推理基准测试中比GRPO有性能提升，有代码仓库。


<details>
  <summary>Details</summary>
Motivation: GRPO计算成本高、数据效率低，需改进大语言模型的强化学习方法。

Method: 引入Replay - Enhanced Policy Optimization (RePO)，利用多种回放策略从回放缓冲区检索离策略样本进行策略优化。

Result: 在五个大语言模型的七个数学推理基准测试中，RePO比GRPO有性能提升，如Qwen2.5 - Math - 1.5B和Qwen3 - 1.7B分别有18.4和4.1的绝对平均性能增益；Qwen3 - 1.7B计算成本增加15%，有效优化步骤增加48%。

Conclusion: RePO是一种有效优化大语言模型的方法，在性能提升和优化步骤增加上有优势。

Abstract: Reinforcement learning (RL) is vital for optimizing large language models
(LLMs). Recent Group Relative Policy Optimization (GRPO) estimates advantages
using multiple on-policy outputs per prompt, leading to high computational
costs and low data efficiency. To address this, we introduce Replay-Enhanced
Policy Optimization (RePO), which leverages diverse replay strategies to
retrieve off-policy samples from a replay buffer, allowing policy optimization
based on a broader and more diverse set of samples for each prompt. Experiments
on five LLMs across seven mathematical reasoning benchmarks demonstrate that
RePO achieves absolute average performance gains of $18.4$ and $4.1$ points for
Qwen2.5-Math-1.5B and Qwen3-1.7B, respectively, compared to GRPO. Further
analysis indicates that RePO increases computational cost by $15\%$ while
raising the number of effective optimization steps by $48\%$ for Qwen3-1.7B,
with both on-policy and off-policy sample numbers set to $8$. The repository
can be accessed at https://github.com/SihengLi99/RePO.

</details>


### [264] [Latent Multi-Head Attention for Small Language Models](https://arxiv.org/abs/2506.09342)
*Sushant Mehta,Raj Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.CL

TL;DR: 对小语言模型的潜在多头注意力（MLA）进行全面研究，对比不同架构变体，发现MLA+RoPE在内存和速度上有优势。


<details>
  <summary>Details</summary>
Motivation: 对小语言模型的MLA进行全面研究，揭示效率 - 质量权衡。

Method: 在10万个合成故事上训练3000万参数的GPT模型，对比标准多头注意力（MHA）、MLA和MLA+RoPE三种架构变体。

Result: MLA+RoPE半秩潜在维度可减少45%的KV缓存内存，验证损失仅增加0.3%；RoPE对小模型的MLA至关重要；r=d/2的MLA比全秩MLA提速1.4倍；GPT - 4评估得分高。

Conclusion: MLA+RoPE在内存受限部署中有帕累托改进，在速度和质量上有优势。

Abstract: We present the first comprehensive study of latent multi-head attention (MLA)
for small language models, revealing interesting efficiency-quality trade-offs.
Training 30M-parameter GPT models on 100,000 synthetic stories, we benchmark
three architectural variants: standard multi-head attention (MHA), MLA, and MLA
with rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE
with half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory
reduction while incurring only a 0.3% increase in validation loss (essentially
matching MHA quality)- a Pareto improvement for memory constrained deployment.
We further show that RoPE is crucial for MLA in small models: without it, MLA
underperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by
2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2
achieves a 1.4 times speedup over full-rank MLA while maintaining the memory
savings. GPT-4 evaluations corroborate perplexity results, with ours achieving
the highest quality scores (7.4/10) across grammar, creativity, and consistency
metrics. Code and models will be released upon acceptance.

</details>


### [265] [COGENT: A Curriculum-oriented Framework for Generating Grade-appropriate Educational Content](https://arxiv.org/abs/2506.09367)
*Zhengyuan Liu,Stella Xin Yin,Dion Hoe-Lian Goh,Nancy F. Chen*

Main category: cs.CL

TL;DR: 文章指出生成式AI应用于教育存在挑战，提出COGENT框架生成合适教育内容，评估显示其表现良好，为拓展学习资源提供可行方法。


<details>
  <summary>Details</summary>
Motivation: 生成式AI应用于教育场景有诸多挑战，如不符课程标准、难维持合适阅读水平，STEM教育平衡专业与日常语言也有困难，因此需开发合适框架。

Method: 提出COGENT框架，纳入三种课程组件，通过长度、词汇和句子复杂度控制可读性，采用“基于好奇”方法提高学生参与度，用大语言模型评判和专家分析进行多维度评估。

Result: COGENT能持续生成与人类参考相当或更优的适合对应年级的段落。

Conclusion: 建立了拓展自适应和高质量学习资源的可行方法。

Abstract: While Generative AI has demonstrated strong potential and versatility in
content generation, its application to educational contexts presents several
challenges. Models often fail to align with curriculum standards and maintain
grade-appropriate reading levels consistently. Furthermore, STEM education
poses additional challenges in balancing scientific explanations with everyday
language when introducing complex and abstract ideas and phenomena to younger
students. In this work, we propose COGENT, a curriculum-oriented framework for
generating grade-appropriate educational content. We incorporate three
curriculum components (science concepts, core ideas, and learning objectives),
control readability through length, vocabulary, and sentence complexity, and
adopt a ``wonder-based'' approach to increase student engagement and interest.
We conduct a multi-dimensional evaluation via both LLM-as-a-judge and human
expert analysis. Experimental results show that COGENT consistently produces
grade-appropriate passages that are comparable or superior to human references.
Our work establishes a viable approach for scaling adaptive and high-quality
learning resources.

</details>


### [266] [Token Constraint Decoding Improves Robustness on Question Answering for Large Language Models](https://arxiv.org/abs/2506.09408)
*Jui-Ming Yao,Hao-Yuan Chen,Zi-Xian Tang,Bing-Jia Tan,Sheng-Wei Peng,Bing-Cheng Xie,Shun-Feng Su*

Main category: cs.CL

TL;DR: 本文介绍Token Constraint Decoding (TCD)算法提升大语言模型在多选题问答中对输入扰动的鲁棒性，实验显示该算法结合提示工程有显著效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多选题问答基准测试中易受输入扰动影响，需要提升鲁棒性。

Method: 引入并评估Token Constraint Decoding (TCD)算法，在推理时强制标记级预测对齐，结合提示工程。

Result: 在多个数据集实验表明，TCD结合提示工程修复显著恢复输入噪声导致的性能下降，弱模型绝对增益达39%，不同模型需不同惩罚策略。

Conclusion: TCD是一种实用、与模型无关的方法，可提升现实环境中推理稳定性，有助于大语言模型在关键场景部署。

Abstract: Large Language Models (LLMs) have demonstrated impressive performance on
multiple-choice question answering (MCQA) benchmarks, yet they remain highly
vulnerable to minor input perturbations. In this paper, we introduce and
evaluate Token Constraint Decoding (TCD). This simple yet effective
inference-time algorithm enforces alignment between token-level predictions to
enhance robustness in noisy settings. Through extensive experiments on
CommonsenseQA, MMLU, and MMLU-Pro, we show that TCD, especially when paired
with prompt engineering (PE) fixes, significantly restores performance degraded
by input noise, yielding up to +39\% absolute gains for weaker models like
Gemma3 1B. Penalty sweep analyses further reveal that TCD implicitly
regularizes overconfident outputs, with different models requiring distinct
penalty schedules to maximize resilience. Our findings establish TCD as a
practical, model-agnostic approach for improving reasoning stability under
real-world imperfections and pave the way for more reliable deployment of LLMs
in safety-critical or user-facing applications.

</details>


### [267] [Improved Supervised Fine-Tuning for Large Language Models to Mitigate Catastrophic Forgetting](https://arxiv.org/abs/2506.09428)
*Fei Ding,Baiqiao Wang*

Main category: cs.CL

TL;DR: 提出新的有监督微调（SFT）方法，可在不访问原SFT数据下减少灾难性遗忘风险，实验证明能保留通用能力并提升特定任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统SFT方法会降低大语言模型通用能力，第三方在开源模型上实施SFT时因无法访问原预训练数据，会加剧灾难性遗忘。

Method: 先重构基础模型可能的SFT指令分布，再通过多模型筛选过程选择最优数据，将其与新数据混合进行SFT。

Result: 实验表明该方法能在通用领域保留泛化能力，同时提升特定任务性能。

Conclusion: 提出的SFT方法能有效解决传统SFT方法存在的问题，减少灾难性遗忘风险。

Abstract: Supervised Fine-Tuning (SFT), while enhancing large language models(LLMs)'
instruction-following capabilities and domain-specific task adaptability, often
diminishes their general capabilities. Moreover, due to the inaccessibility of
original pre-training data, catastrophic forgetting tends to be exacerbated
when third-party practitioners implement SFT on open-sourced models. To address
this challenge, we propose a novel, more cost-effective SFT method which could
effectively reduce the risk of catastrophic forgetting without access to
original SFT data. Our approach begins by reconstructing the likely SFT
instruction distribution of the base model, followed by a multi-model screening
process to select optimal data, which is then mixed with new data for SFT.
Experimental results demonstrate that our method preserves generalization
capabilities in general domains while improving task-specific performance.

</details>


### [268] [GigaChat Family: Efficient Russian Language Modeling Through Mixture of Experts Architecture](https://arxiv.org/abs/2506.09440)
*GigaChat team,Mamedov Valentin,Evgenii Kosarev,Gregory Leleytner,Ilya Shchuckin,Valeriy Berezovskiy,Daniil Smirnov,Dmitry Kozlov,Sergei Averkiev,Lukyanenko Ivan,Aleksandr Proshunin,Ainur Israfilova,Ivan Baskov,Artem Chervyakov,Emil Shakirov,Mikhail Kolesov,Daria Khomich,Darya Latortseva,Sergei Porkhun,Yury Fedorov,Oleg Kutuzov,Polina Kudriavtseva,Sofiia Soldatova,Kolodin Egor,Stanislav Pyatkin,Dzmitry Menshykh,Grafov Sergei,Eldar Damirov,Karlov Vladimir,Ruslan Gaitukiev,Arkadiy Shatenov,Alena Fenogenova,Nikita Savushkin,Fedor Minkin*

Main category: cs.CL

TL;DR: 本文介绍不同规模的俄语大语言模型GigaChat家族，包括架构、预训练等情况，评估其在俄英基准测试上的表现，展示系统并开源部分模型。


<details>
  <summary>Details</summary>
Motivation: 当前针对俄语的基础模型发展受限，主要因计算资源需求大。

Method: 详细介绍模型架构、预训练过程和实验以指导设计，在俄英基准测试上评估模型性能并与多语言同类模型比较。

Result: 完成GigaChat模型的构建和评估，展示了表现最佳模型的系统，可通过API、Telegram机器人和Web界面访问。

Conclusion: 开源三个GigaChat模型，旨在拓展俄语NLP研究机会和支持工业解决方案的发展。

Abstract: Generative large language models (LLMs) have become crucial for modern NLP
research and applications across various languages. However, the development of
foundational models specifically tailored to the Russian language has been
limited, primarily due to the significant computational resources required.
This paper introduces the GigaChat family of Russian LLMs, available in various
sizes, including base models and instruction-tuned versions. We provide a
detailed report on the model architecture, pre-training process, and
experiments to guide design choices. In addition, we evaluate their performance
on Russian and English benchmarks and compare GigaChat with multilingual
analogs. The paper presents a system demonstration of the top-performing models
accessible via an API, a Telegram bot, and a Web interface. Furthermore, we
have released three open GigaChat models in open-source
(https://huggingface.co/ai-sage), aiming to expand NLP research opportunities
and support the development of industrial solutions for the Russian language.

</details>


### [269] [UniToMBench: Integrating Perspective-Taking to Improve Theory of Mind in LLMs](https://arxiv.org/abs/2506.09450)
*Prameshwar Thiyagarajan,Vaishnavi Parimi,Shamant Sai,Soumil Garg,Zhangir Meirbek,Nitin Yarlagadda,Kevin Zhu,Chris Kim*

Main category: cs.CL

TL;DR: 介绍统一基准UniToMBench评估大语言模型心智理论能力，评估显示模型表现有差异，凸显该基准价值。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在心智理论能力方面表现不佳，需系统改进和评估其相关能力。

Method: 结合SimToM和TOMBENCH优势，采用多交互任务设计和演化故事场景，结合视角采择技术和多样评估指标，使用超1000手写场景自定义数据集。

Result: GPT - 4o和GPT - 4o Mini在情感和信念相关场景任务准确率超80%，但在基于知识任务中表现差异大。

Conclusion: UniToMBench是未来大语言模型心智理论能力发展的综合工具，代码公开。

Abstract: Theory of Mind (ToM), the ability to understand the mental states of oneself
and others, remains a challenging area for large language models (LLMs), which
often fail to predict human mental states accurately. In this paper, we
introduce UniToMBench, a unified benchmark that integrates the strengths of
SimToM and TOMBENCH to systematically improve and assess ToM capabilities in
LLMs by integrating multi-interaction task designs and evolving story
scenarios. Supported by a custom dataset of over 1,000 hand-written scenarios,
UniToMBench combines perspective-taking techniques with diverse evaluation
metrics to better stimulate social cognition in LLMs. Through evaluation, we
observe that while models like GPT-4o and GPT-4o Mini show consistently high
accuracy in tasks involving emotional and belief-related scenarios, with
results usually above 80%, there is significant variability in their
performance across knowledge-based tasks. These results highlight both the
strengths and limitations of current LLMs in ToM-related tasks, underscoring
the value of UniToMBench as a comprehensive tool for future development. Our
code is publicly available here:
https://github.com/Shamant/unifiedtombenchmark.

</details>


### [270] [TransXSSM: A Hybrid Transformer State Space Model with Unified Rotary Position Embedding](https://arxiv.org/abs/2506.09507)
*Bingheng Wu,Jingze Shi,Yifan Wu,Nan Tang,Yuyu Luo*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Transformers exhibit proficiency in capturing long-range dependencies,
whereas State Space Models (SSMs) facilitate linear-time sequence modeling.
Notwithstanding their synergistic potential, the integration of these
architectures presents a significant challenge, primarily attributable to a
fundamental incongruity in their respective positional encoding mechanisms:
Transformers rely on explicit Rotary Position Embeddings (RoPE), while SSMs
leverage implicit positional representations via convolutions. This divergence
often precipitates discontinuities and suboptimal performance. To address this
impediment, we propose a unified rotary position embedding (\textbf{\ourRoPE})
methodology, thereby establishing a consistent positional encoding framework
for both self-attention and state-space components. Using this \ourRoPE, we
introduce \textbf{\model}, a hybrid architecture that coherently integrates the
Transformer and SSM layers under this unified positional encoding scheme. At a
4K sequence length, \model exhibits training and inference speeds that are
\textbf{42.3\% and 29.5\% faster}, respectively, relative to standard
Transformer models. It also delivers higher accuracy: under comparable
settings, it surpasses a Transformer baseline by over 4\% on language modeling
benchmarks. \model furthermore scales more effectively: \model-1.3B gains
\textbf{7.22\%} in average accuracy over its 320M version (versus about 6\%
gains for equivalent Transformers or SSMs). Our results show that unified
positional encoding resolves positional incompatibility in hybrid models,
enabling efficient, high-performance long-context modeling.

</details>


### [271] [ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning](https://arxiv.org/abs/2506.09513)
*Yu Sun,Xingyu Qian,Weiwen Xu,Hao Zhang,Chenghao Xiao,Long Li,Yu Rong,Wenbing Huang,Qifeng Bai,Tingyang Xu*

Main category: cs.CL

TL;DR: 引入医疗推理数据集ReasonMed，研究训练策略，训练ReasonMed - 7B模型取得佳绩。


<details>
  <summary>Details</summary>
Motivation: 解决基于推理的大语言模型在知识密集型医疗问答能力未充分探索的问题。

Method: 构建含370k高质量示例的ReasonMed数据集，采用多智能体验证和细化过程，设计错误精炼器；结合详细思维链推理和简洁答案总结进行微调。

Result: ReasonMed - 7B为亚10B模型设定新基准，在PubMedQA上表现优于先前最佳模型和LLaMA3.1 - 70B。

Conclusion: 结合详细思维链推理与简洁答案总结的微调策略在训练医疗推理模型中最有效。

Abstract: Though reasoning-based large language models (LLMs) have excelled in
mathematics and programming, their capabilities in knowledge-intensive medical
question answering remain underexplored. To address this, we introduce
ReasonMed, the largest medical reasoning dataset, comprising 370k high-quality
examples distilled from 1.7 million initial reasoning paths generated by
various LLMs. ReasonMed is constructed through a \textit{multi-agent
verification and refinement process}, where we design an \textit{Error Refiner}
to enhance the reasoning paths by identifying and correcting error-prone steps
flagged by a verifier. Leveraging ReasonMed, we systematically investigate best
practices for training medical reasoning models and find that combining
detailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields
the most effective fine-tuning strategy. Based on this strategy, we train
ReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the
prior best by 4.17\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\%.

</details>


### [272] [From Symbolic to Neural and Back: Exploring Knowledge Graph-Large Language Model Synergies](https://arxiv.org/abs/2506.09566)
*Blaž Škrlj,Boshko Koloski,Senja Pollak,Nada Lavrač*

Main category: cs.CL

TL;DR: 本文系统探讨知识图谱（KGs）与大语言模型（LLMs）协同，分类现有方法，指出关键差距和优势，强调多方面特性并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 将知识图谱的结构化知识融入大语言模型，提升事实依据和推理能力。

Method: 对现有方法进行分类，分为KG增强的LLMs和LLM增强的KGs，并进行综合分析。

Result: 识别出关键差距，强调了结构化知识集成的互利性，突出了可扩展性、计算效率和数据质量。

Conclusion: 提出未来研究方向，包括神经符号集成、动态KG更新等，为处理复杂现实知识任务的智能系统铺平道路。

Abstract: Integrating structured knowledge from Knowledge Graphs (KGs) into Large
Language Models (LLMs) enhances factual grounding and reasoning capabilities.
This survey paper systematically examines the synergy between KGs and LLMs,
categorizing existing approaches into two main groups: KG-enhanced LLMs, which
improve reasoning, reduce hallucinations, and enable complex question
answering; and LLM-augmented KGs, which facilitate KG construction, completion,
and querying. Through comprehensive analysis, we identify critical gaps and
highlight the mutual benefits of structured knowledge integration. Compared to
existing surveys, our study uniquely emphasizes scalability, computational
efficiency, and data quality. Finally, we propose future research directions,
including neuro-symbolic integration, dynamic KG updating, data reliability,
and ethical considerations, paving the way for intelligent systems capable of
managing more complex real-world knowledge tasks.

</details>


### [273] [Is Fine-Tuning an Effective Solution? Reassessing Knowledge Editing for Unstructured Data](https://arxiv.org/abs/2506.09672)
*Hao Xiong,Chuanyuan Tan,Wenliang Chen*

Main category: cs.CL

TL;DR: 本文聚焦非结构化知识编辑（UKE），构建数据集评估局部性，找出影响微调方法性能的因素并提供训练方案，最优设置的FT - UKE方法表现超现有SOTA。


<details>
  <summary>Details</summary>
Motivation: 先前UKE研究存在缺乏局部性评估和微调方法异常失败的问题，需要解决这些问题。

Method: 扩展现有UKE数据集构建UnKEBench - Loc和AKEW - Loc (CF) 数据集评估局部性；找出影响微调方法性能的四个因素并进行实验确定训练方案。

Result: 最优设置的FT - UKE方法表现强，超过现有SOTA，在批量编辑场景中优势随批量大小增加，平均指标领先从+6.78%扩大到+10.80%。

Conclusion: 所提出的FT - UKE方法在UKE任务上表现优异，为未来研究提供了训练配方。

Abstract: Unstructured Knowledge Editing (UKE) is crucial for updating the relevant
knowledge of large language models (LLMs). It focuses on unstructured inputs,
such as long or free-form texts, which are common forms of real-world
knowledge. Although previous studies have proposed effective methods and tested
them, some issues exist: (1) Lack of Locality evaluation for UKE, and (2)
Abnormal failure of fine-tuning (FT) based methods for UKE. To address these
issues, we first construct two datasets, UnKEBench-Loc and AKEW-Loc (CF), by
extending two existing UKE datasets with locality test data from the
unstructured and structured views. This enables a systematic evaluation of the
Locality of post-edited models. Furthermore, we identify four factors that may
affect the performance of FT-based methods. Based on these factors, we conduct
experiments to determine how the well-performing FT-based methods should be
trained for the UKE task, providing a training recipe for future research. Our
experimental results indicate that the FT-based method with the optimal setting
(FT-UKE) is surprisingly strong, outperforming the existing state-of-the-art
(SOTA). In batch editing scenarios, FT-UKE shows strong performance as well,
with its advantage over SOTA methods increasing as the batch size grows,
expanding the average metric lead from +6.78% to +10.80%

</details>


### [274] [CoRT: Code-integrated Reasoning within Thinking](https://arxiv.org/abs/2506.09820)
*Chengpeng Li,Zhengyang Tang,Ziniu Li,Mingfeng Xue,Keqin Bao,Tian Ding,Ruoyu Sun,Benyou Wang,Xiang Wang,Junyang Lin,Dayiheng Liu*

Main category: cs.CL

TL;DR: 本文提出CoRT框架提升大推理模型处理复杂数学运算能力，用Hint - Engineering合成数据，经多种训练方法实验取得不错效果，代码开源。


<details>
  <summary>Details</summary>
Motivation: 大推理模型处理复杂数学运算时效率或准确性不足，直接结合Code Interpreter不高效，需解决这些问题。

Method: 提出CoRT框架，用Hint - Engineering合成代码集成推理数据，对不同参数模型进行监督微调、拒绝微调与强化学习。

Result: Hint - Engineering模型在五个数学推理数据集上有绝对提升，且相比自然语言模型使用更少的token。

Conclusion: CoRT框架能有效且高效地让大推理模型利用Code Interpreter处理复杂数学运算。

Abstract: Large Reasoning Models (LRMs) like o1 and DeepSeek-R1 have shown remarkable
progress in natural language reasoning with long chain-of-thought (CoT), yet
they remain inefficient or inaccurate when handling complex mathematical
operations. Addressing these limitations through computational tools (e.g.,
computation libraries and symbolic solvers) is promising, but it introduces a
technical challenge: Code Interpreter (CI) brings external knowledge beyond the
model's internal text representations, thus the direct combination is not
efficient. This paper introduces CoRT, a post-training framework for teaching
LRMs to leverage CI effectively and efficiently. As a first step, we address
the data scarcity issue by synthesizing code-integrated reasoning data through
Hint-Engineering, which strategically inserts different hints at appropriate
positions to optimize LRM-CI interaction. We manually create 30 high-quality
samples, upon which we post-train models ranging from 1.5B to 32B parameters,
with supervised fine-tuning, rejection fine-tuning and reinforcement learning.
Our experimental results demonstrate that Hint-Engineering models achieve 4\%
and 8\% absolute improvements on DeepSeek-R1-Distill-Qwen-32B and
DeepSeek-R1-Distill-Qwen-1.5B respectively, across five challenging
mathematical reasoning datasets. Furthermore, Hint-Engineering models use about
30\% fewer tokens for the 32B model and 50\% fewer tokens for the 1.5B model
compared with the natural language models. The models and code are available at
https://github.com/ChengpengLi1003/CoRT.

</details>


### [275] [EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech Emotion Detection](https://arxiv.org/abs/2506.09827)
*Christoph Schuhmann,Robert Kaczmarczyk,Gollam Rabby,Felix Friedrich,Maurice Kraus,Kourosh Nadi,Huu Nguyen,Kristian Kersting,Sören Auer*

Main category: cs.CL

TL;DR: 本文提出用于语音情感检测的EmoNet - Voice资源，包括预训练和基准数据集，利用合成音频，由专家验证，还推出Empathic Insight Voice模型，评估有新发现。


<details>
  <summary>Details</summary>
Motivation: 当前语音情感识别数据集在情感粒度、隐私等方面有局限，需要强大的基准来评估AI系统的情感理解能力。

Method: 利用先进语音生成技术，合成模拟特定情感场景的音频片段，由心理学专家进行严格验证并标注感知强度标签。

Result: 推出EmoNet - Voice资源和Empathic Insight Voice模型，评估发现高唤醒情绪比低唤醒情绪更容易检测。

Conclusion: EmoNet - Voice和Empathic Insight Voice模型为语音情感识别设定了新标准，在评估中得到有价值的发现。

Abstract: The advancement of text-to-speech and audio generation models necessitates
robust benchmarks for evaluating the emotional understanding capabilities of AI
systems. Current speech emotion recognition (SER) datasets often exhibit
limitations in emotional granularity, privacy concerns, or reliance on acted
portrayals. This paper introduces EmoNet-Voice, a new resource for speech
emotion detection, which includes EmoNet-Voice Big, a large-scale pre-training
dataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions,
and 4 languages), and EmoNet-Voice Bench, a novel benchmark dataset with human
expert annotations. EmoNet-Voice is designed to evaluate SER models on a
fine-grained spectrum of 40 emotion categories with different levels of
intensities. Leveraging state-of-the-art voice generation, we curated synthetic
audio snippets simulating actors portraying scenes designed to evoke specific
emotions. Crucially, we conducted rigorous validation by psychology experts who
assigned perceived intensity labels. This synthetic, privacy-preserving
approach allows for the inclusion of sensitive emotional states often absent in
existing datasets. Lastly, we introduce Empathic Insight Voice models that set
a new standard in speech emotion recognition with high agreement with human
experts. Our evaluations across the current model landscape exhibit valuable
findings, such as high-arousal emotions like anger being much easier to detect
than low-arousal states like concentration.

</details>


### [276] [Dataset of News Articles with Provenance Metadata for Media Relevance Assessment](https://arxiv.org/abs/2506.09847)
*Tomas Peterka,Matyas Bohacek*

Main category: cs.CL

TL;DR: 介绍新闻媒体来源数据集，提出两个任务并给出大语言模型基线结果，指出零样本在LOR任务表现有前景，DTOR任务待提升。


<details>
  <summary>Details</summary>
Motivation: 现有检测脱离上下文和错误归因图像的方法仅考虑图像语义与文本叙述的对应，有局限性，需改进。

Method: 引入新闻媒体来源数据集，在该数据集上制定位置起源相关性（LOR）和日期时间起源相关性（DTOR）两个任务，并在六个大语言模型上测试。

Result: 零样本在LOR任务表现有前景，DTOR任务表现不佳。

Conclusion: DTOR任务表现不佳，有必要开发专门架构并开展未来研究。

Abstract: Out-of-context and misattributed imagery is the leading form of media
manipulation in today's misinformation and disinformation landscape. The
existing methods attempting to detect this practice often only consider whether
the semantics of the imagery corresponds to the text narrative, missing
manipulation so long as the depicted objects or scenes somewhat correspond to
the narrative at hand. To tackle this, we introduce News Media Provenance
Dataset, a dataset of news articles with provenance-tagged images. We formulate
two tasks on this dataset, location of origin relevance (LOR) and date and time
of origin relevance (DTOR), and present baseline results on six large language
models (LLMs). We identify that, while the zero-shot performance on LOR is
promising, the performance on DTOR hinders, leaving room for specialized
architectures and future work.

</details>


### [277] [Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning](https://arxiv.org/abs/2506.09853)
*Xiangning Yu,Zhuohan Wang,Linyi Yang,Haoxuan Li,Anjie Liu,Xiao Xue,Jun Wang,Mengyue Yang*

Main category: cs.CL

TL;DR: 提出因果框架分析CoT推理的充分性和必要性，实验显示提升推理效率并减少token使用，不牺牲准确性。


<details>
  <summary>Details</summary>
Motivation: 解决CoT目前面临的充分性和必要性两大基本挑战。

Method: 提出因果框架，结合因果充分性和必要性概率，确定推理步骤的逻辑性质并量化其对结果的影响，实现步骤的自动添加和修剪。

Result: 在多种数学和常识推理基准测试中，推理效率显著提高，token使用减少，且不牺牲准确性。

Conclusion: 为提高大语言模型推理性能和成本效益提供了有前景的方向。

Abstract: Chain-of-Thought (CoT) prompting plays an indispensable role in endowing
large language models (LLMs) with complex reasoning capabilities. However, CoT
currently faces two fundamental challenges: (1) Sufficiency, which ensures that
the generated intermediate inference steps comprehensively cover and
substantiate the final conclusion; and (2) Necessity, which identifies the
inference steps that are truly indispensable for the soundness of the resulting
answer. We propose a causal framework that characterizes CoT reasoning through
the dual lenses of sufficiency and necessity. Incorporating causal Probability
of Sufficiency and Necessity allows us not only to determine which steps are
logically sufficient or necessary to the prediction outcome, but also to
quantify their actual influence on the final reasoning outcome under different
intervention scenarios, thereby enabling the automated addition of missing
steps and the pruning of redundant ones. Extensive experimental results on
various mathematical and commonsense reasoning benchmarks confirm substantial
improvements in reasoning efficiency and reduced token usage without
sacrificing accuracy. Our work provides a promising direction for improving LLM
reasoning performance and cost-effectiveness.

</details>


### [278] [Attention Head Embeddings with Trainable Deep Kernels for Hallucination Detection in LLMs](https://arxiv.org/abs/2506.09886)
*Rodion Oblovatny,Alexandra Bazarova,Alexey Zaytsev*

Main category: cs.CL

TL;DR: 提出通过分析提示与响应隐藏状态分布的概率差异检测大语言模型幻觉的方法，优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 检测大语言模型中的幻觉。

Method: 分析提示与响应隐藏状态分布的概率差异，用分布距离作为幻觉分数，使用可学习的深度核增强敏感性。

Result: 在多个基准测试中表现优于现有基线，不进行核训练时仍具竞争力。

Conclusion: 该方法是一种强大、可扩展的幻觉检测解决方案。

Abstract: We present a novel approach for detecting hallucinations in large language
models (LLMs) by analyzing the probabilistic divergence between prompt and
response hidden-state distributions. Counterintuitively, we find that
hallucinated responses exhibit smaller deviations from their prompts compared
to grounded responses, suggesting that hallucinations often arise from
superficial rephrasing rather than substantive reasoning. Leveraging this
insight, we propose a model-intrinsic detection method that uses distributional
distances as principled hallucination scores, eliminating the need for external
knowledge or auxiliary models. To enhance sensitivity, we employ deep learnable
kernels that automatically adapt to capture nuanced geometric differences
between distributions. Our approach outperforms existing baselines,
demonstrating state-of-the-art performance on several benchmarks. The method
remains competitive even without kernel training, offering a robust, scalable
solution for hallucination detection.

</details>


### [279] [The Emergence of Abstract Thought in Large Language Models Beyond Any Language](https://arxiv.org/abs/2506.09890)
*Yuxin Chen,Yiran Zhao,Yang Zhang,An Zhang,Kenji Kawaguchi,Shafiq Joty,Junnan Li,Tat-Seng Chua,Michael Qizhe Shieh,Wenxuan Zhang*

Main category: cs.CL

TL;DR: 研究发现大语言模型发展出语言无关核心参数空间，共享神经元占比和重要性增加，据此提出针对性训练策略且实验支持。


<details>
  <summary>Details</summary>
Motivation: 初步研究认为大语言模型可能“用英语思考”，但最新多语言表现挑战此观点，需深入探究其语言处理机制。

Method: 识别语言相关神经元并分类为共享和专属，观察不同阶段变化，提出针对不同发展阶段语言无关水平的神经元特定训练策略。

Result: 发现核心语言无关参数空间，共享神经元比例和功能重要性增加，专属神经元影响减小，实验支持所提训练策略。

Conclusion: 大语言模型发展出语言无关参数空间支持抽象思维，提出的训练策略有效。

Abstract: As large language models (LLMs) continue to advance, their capacity to
function effectively across a diverse range of languages has shown marked
improvement. Preliminary studies observe that the hidden activations of LLMs
often resemble English, even when responding to non-English prompts. This has
led to the widespread assumption that LLMs may "think" in English. However,
more recent results showing strong multilingual performance, even surpassing
English performance on specific tasks in other languages, challenge this view.
In this work, we find that LLMs progressively develop a core language-agnostic
parameter space-a remarkably small subset of parameters whose deactivation
results in significant performance degradation across all languages. This
compact yet critical set of parameters underlies the model's ability to
generalize beyond individual languages, supporting the emergence of abstract
thought that is not tied to any specific linguistic system. Specifically, we
identify language-related neurons-those are consistently activated during the
processing of particular languages, and categorize them as either shared
(active across multiple languages) or exclusive (specific to one). As LLMs
undergo continued development over time, we observe a marked increase in both
the proportion and functional importance of shared neurons, while exclusive
neurons progressively diminish in influence. These shared neurons constitute
the backbone of the core language-agnostic parameter space, supporting the
emergence of abstract thought. Motivated by these insights, we propose
neuron-specific training strategies tailored to LLMs' language-agnostic levels
at different development stages. Experiments across diverse LLM families
support our approach.

</details>


### [280] [Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms](https://arxiv.org/abs/2506.09457)
*Zeguan Xiao,Yun Chen,Guanhua Chen*

Main category: cs.CL

TL;DR: 指出直接对齐算法（DAAs）存在奖励生成差距问题，提出Prefix - Oriented Equal - length Training (POET)方法解决该问题，实验证明POET有效果。


<details>
  <summary>Details</summary>
Motivation: DAAs存在奖励生成差距问题，即训练时的优化目标与推理时的实际生成性能不一致，需解决该问题。

Method: 提出POET方法，将偏好和非偏好响应截断为相同长度，使DAAs目标优化在所有位置收敛，更关注前缀标记。

Result: 对DPO和SimPO两种DAAs进行实验，POET比标准实现有提升，在AlpacaEval 2中最多提高15.6分，下游任务也有整体提升。

Conclusion: 强调解决DAAs中奖励优化和生成性能不一致问题的重要性。

Abstract: Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization
(DPO) and Simple Preference Optimization (SimPO), have emerged as efficient
alternatives to Reinforcement Learning from Human Feedback (RLHF) algorithms
for aligning large language models (LLMs) with human preferences. However, DAAs
suffer from a fundamental limitation we identify as the "reward-generation gap"
-- a misalignment between optimization objectives during training and actual
generation performance during inference. In this paper, we find a contributor
to the reward-generation gap is the mismatch between the inherent importance of
prefix tokens during the LLM generation process and how this importance is
reflected in the implicit reward functions of DAAs. To bridge the gap, we
introduce a simple yet effective approach called Prefix-Oriented Equal-length
Training (POET), which truncates both preferred and dispreferred responses to
match the shorter one's length. Training with POET, where both responses in
each sample are truncated to equal length, resulting in diverse truncated
lengths across samples, the optimization of DAAs objective is implicitly
constrained to converge across all positions, thus paying more attention to
prefix tokens than the standard DAAs. We conduct experiments with DPO and
SimPO, two representative DAAs, demonstrating that POET improves over their
standard implementations, achieving up to 15.6 points in AlpacaEval 2 and
overall improvements across downstream tasks. Our results highlight the
importance of addressing the misalignment between reward optimization and
generation performance in DAAs.

</details>


### [281] [PersonaLens: A Benchmark for Personalization Evaluation in Conversational AI Assistants](https://arxiv.org/abs/2506.09902)
*Zheng Zhao,Clara Vania,Subhradeep Kayal,Naila Khan,Shay B. Cohen,Emine Yilmaz*

Main category: cs.CL

TL;DR: 提出用于评估面向任务的AI助手个性化的综合基准PersonaLens，实验揭示现有大语言模型助手个性化能力有显著差异。


<details>
  <summary>Details</summary>
Motivation: 现有个性化基准无法捕捉面向任务的个性化协助的复杂性，系统评估AI助手个性化能力仍具挑战。

Method: 引入PersonaLens基准，包含多样用户配置文件，配备基于大语言模型的用户代理和评判代理进行评估。

Result: 通过对当前大语言模型助手在多样任务上的广泛实验，发现其个性化能力存在显著差异。

Conclusion: 研究为推进对话式AI系统提供关键见解。

Abstract: Large language models (LLMs) have advanced conversational AI assistants.
However, systematically evaluating how well these assistants apply
personalization--adapting to individual user preferences while completing
tasks--remains challenging. Existing personalization benchmarks focus on
chit-chat, non-conversational tasks, or narrow domains, failing to capture the
complexities of personalized task-oriented assistance. To address this, we
introduce PersonaLens, a comprehensive benchmark for evaluating personalization
in task-oriented AI assistants. Our benchmark features diverse user profiles
equipped with rich preferences and interaction histories, along with two
specialized LLM-based agents: a user agent that engages in realistic
task-oriented dialogues with AI assistants, and a judge agent that employs the
LLM-as-a-Judge paradigm to assess personalization, response quality, and task
success. Through extensive experiments with current LLM assistants across
diverse tasks, we reveal significant variability in their personalization
capabilities, providing crucial insights for advancing conversational AI
systems.

</details>


### [282] [Bridging Online Behavior and Clinical Insight: A Longitudinal LLM-based Study of Suicidality on YouTube Reveals Novel Digital Markers](https://arxiv.org/abs/2506.09495)
*Ilanit Sobol,Shir Lissak,Refael Tikochinski,Tal Nakash,Anat Brunstein Klomek,Eyal Fruchter,Roi Reichart*

Main category: cs.CL

TL;DR: 文章用三种方法研究自杀行为在YouTube上的表现，整合方法为自杀行为提供了细致理解。


<details>
  <summary>Details</summary>
Motivation: 自杀是西方国家主要死因，社交媒体数字足迹可提供自杀行为洞察，需研究自杀行为在YouTube上的表现及与专家知识的差异。

Method: 采用计算自下而上、混合和专家驱动自上而下三种互补方法，对181个有生命危险自杀企图者的YouTube频道和134个对照频道数据集进行研究。

Result: 自下而上方法确定5个与自杀企图相关主题，2个有时间变化；混合方法中专家标记19个自杀相关主题但无额外显著时间效应；自上而下方法发现不同时期自杀者分享动机有差异。

Conclusion: 整合三种方法能提供对自杀行为的细致理解，弥合数字行为与临床见解的差距。

Abstract: Suicide remains a leading cause of death in Western countries, underscoring
the need for new research approaches. As social media becomes central to daily
life, digital footprints offer valuable insight into suicidal behavior.
Focusing on individuals who attempted suicide while uploading videos to their
channels, we investigate: How do suicidal behaviors manifest on YouTube, and
how do they differ from expert knowledge? We applied complementary approaches:
computational bottom-up, hybrid, and expert-driven top-down, on a novel
longitudinal dataset of 181 YouTube channels from individuals with
life-threatening attempts, alongside 134 control channels. In the bottom-up
approach, we applied LLM-based topic modeling to identify behavioral
indicators. Of 166 topics, five were associated with suicide-attempt, with two
also showing temporal attempt-related changes ($p<.01$) - Mental Health
Struggles ($+0.08$)* and YouTube Engagement ($+0.1$)*. In the hybrid approach,
a clinical expert reviewed LLM-derived topics and flagged 19 as
suicide-related. However, none showed significant attempt-related temporal
effects beyond those identified bottom-up. Notably, YouTube Engagement, a
platform-specific indicator, was not flagged by the expert, underscoring the
value of bottom-up discovery. In the top-down approach, psychological
assessment of suicide attempt narratives revealed that the only significant
difference between individuals who attempted before and those attempted during
their upload period was the motivation to share this experience: the former
aimed to Help Others ($\beta=-1.69$, $p<.01$), while the latter framed it as
part of their Personal Recovery ($\beta=1.08$, $p<.01$). By integrating these
approaches, we offer a nuanced understanding of suicidality, bridging digital
behavior and clinical insights.
  * Within-group changes in relation to the suicide attempt.

</details>


### [283] [VerIF: Verification Engineering for Reinforcement Learning in Instruction Following](https://arxiv.org/abs/2506.09942)
*Hao Peng,Yunjia Qi,Xiaozhi Wang,Bin Xu,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: 本文探索指令跟随强化学习的验证挑战，提出VerIF方法，构建VerInstruct数据集，应用于模型训练取得显著提升并开源。


<details>
  <summary>Details</summary>
Motivation: 强化学习可增强大语言模型，但指令跟随强化学习最佳实践探索不足，需解决验证挑战。

Method: 提出VerIF方法，结合基于规则的代码验证和基于大推理模型的验证；构建包含约22000个实例及验证信号的VerInstruct数据集。

Result: 将VerIF应用于两个模型，在多个指令跟随基准测试中显著提升，达到同规模模型的最优性能，泛化能力好且不影响通用能力。

Conclusion: VerIF可集成到现有强化学习方案中提升整体模型性能。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has become a key
technique for enhancing large language models (LLMs), with verification
engineering playing a central role. However, best practices for RL in
instruction following remain underexplored. In this work, we explore the
verification challenge in RL for instruction following and propose VerIF, a
verification method that combines rule-based code verification with LLM-based
verification from a large reasoning model (e.g., QwQ-32B). To support this
approach, we construct a high-quality instruction-following dataset,
VerInstruct, containing approximately 22,000 instances with associated
verification signals. We apply RL training with VerIF to two models, achieving
significant improvements across several representative instruction-following
benchmarks. The trained models reach state-of-the-art performance among models
of comparable size and generalize well to unseen constraints. We further
observe that their general capabilities remain unaffected, suggesting that RL
with VerIF can be integrated into existing RL recipes to enhance overall model
performance. We have released our datasets, codes, and models to facilitate
future research at https://github.com/THU-KEG/VerIF.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [284] [Designing conflict-based communicative tasks in Teaching Chinese as a Foreign Language with ChatGPT](https://arxiv.org/abs/2506.09089)
*Xia Li*

Main category: cs.HC

TL;DR: 大学汉语作为外语教学口语表达课程教学方案设计中，教师设计基于冲突的交际任务培养学生口语能力，并用ChatGPT辅助定稿，文章介绍师生与ChatGPT互动特点及ChatGPT应用和影响。


<details>
  <summary>Details</summary>
Motivation: 展示课程教学方案开发过程中教师与ChatGPT互动特点，探讨ChatGPT在此情境下的应用及影响。

Method: 教师设计基于冲突的交际任务，并使用ChatGPT辅助最终确定教学方案。

Result: 未提及。

Conclusion: 未提及。

Abstract: In developing the teaching program for a course in Oral Expression in
Teaching Chinese as a Foreign Language at the university level, the teacher
designs communicative tasks based on conflicts to encourage learners to engage
in interactive dynamics and develop their oral interaction skills. During the
design of these tasks, the teacher uses ChatGPT to assist in finalizing the
program. This article aims to present the key characteristics of the
interactions between the teacher and ChatGPT during this program development
process, as well as to examine the use of ChatGPT and its impacts in this
specific context.

</details>


### [285] ["Is This Really a Human Peer Supporter?": Misalignments Between Peer Supporters and Experts in LLM-Supported Interactions](https://arxiv.org/abs/2506.09354)
*Kellie Yu Hui Sim,Roy Ka-Wei Lee,Kenny Tsu Wei Choo*

Main category: cs.HC

TL;DR: 探讨AI支持系统在增强同伴支持互动中的作用，发现其有提升培训和互动质量的潜力，但存在同伴支持者与专家观点不一致的问题，强调需标准化培训。


<details>
  <summary>Details</summary>
Motivation: 心理健康受全球关注，AI驱动解决方案可扩大心理社会支持渠道，同伴支持有价值但质量等方面存在问题，LLMs为增强同伴支持互动带来新机遇。

Method: 提出并评估一个AI支持系统，通过两个混合方法研究，对12名同伴支持者和5名心理健康专家进行研究。

Result: 两组都认可系统提升培训和互动质量的潜力，但专家指出同伴支持者回应存在问题，双方观点有差异。

Conclusion: 强调需要标准化、有心理学基础的培训，精心设计并由专家监督的LLM支持系统可促进此发展，为心理健康领域负责任的AI集成对话做贡献。

Abstract: Mental health is a growing global concern, prompting interest in AI-driven
solutions to expand access to psychosocial support. Peer support, grounded in
lived experience, offers a valuable complement to professional care. However,
variability in training, effectiveness, and definitions raises concerns about
quality, consistency, and safety. Large Language Models (LLMs) present new
opportunities to enhance peer support interactions, particularly in real-time,
text-based interactions. We present and evaluate an AI-supported system with an
LLM-simulated distressed client, context-sensitive LLM-generated suggestions,
and real-time emotion visualisations. 2 mixed-methods studies with 12 peer
supporters and 5 mental health professionals (i.e., experts) examined the
system's effectiveness and implications for practice. Both groups recognised
its potential to enhance training and improve interaction quality. However, we
found a key tension emerged: while peer supporters engaged meaningfully,
experts consistently flagged critical issues in peer supporter responses, such
as missed distress cues and premature advice-giving. This misalignment
highlights potential limitations in current peer support training, especially
in emotionally charged contexts where safety and fidelity to best practices are
essential. Our findings underscore the need for standardised, psychologically
grounded training, especially as peer support scales globally. They also
demonstrate how LLM-supported systems can scaffold this development--if
designed with care and guided by expert oversight. This work contributes to
emerging conversations on responsible AI integration in mental health and the
evolving role of LLMs in augmenting peer-delivered care.

</details>


### [286] ["I Said Things I Needed to Hear Myself": Peer Support as an Emotional, Organisational, and Sociotechnical Practice in Singapore](https://arxiv.org/abs/2506.09362)
*Kellie Yu Hui Sim,Kenny Tsu Wei Choo*

Main category: cs.HC

TL;DR: 研究对新加坡20位同伴支持者访谈，分析其开展支持情况，提出文化响应数字工具设计方向及AI辅助建议，助力以人为中心计算。


<details>
  <summary>Details</summary>
Motivation: 数字平台介导同伴支持，其设计和影响在亚洲背景下研究不足，需深入探究。

Method: 对新加坡20位在不同环境开展工作的同伴支持者进行访谈，采用主题分析方法。

Result: 揭示参与者开展同伴支持的过程、动机、情感劳动和社会文化维度。

Conclusion: 提出文化响应数字工具设计方向，探讨AI负责任地增强同伴支持的方式，为心理健康领域可信和情境敏感的AI提供设计启示。

Abstract: Peer support plays a vital role in expanding access to mental health care by
providing empathetic, community-based support outside formal clinical systems.
As digital platforms increasingly mediate such support, the design and impact
of these technologies remain under-examined, particularly in Asian contexts.
This paper presents findings from an interview study with 20 peer supporters in
Singapore, who operate across diverse online, offline, and hybrid environments.
Through a thematic analysis, we unpack how participants start, conduct, and
sustain peer support, highlighting their motivations, emotional labour, and the
sociocultural dimensions shaping their practices. Building on this grounded
understanding, we surface design directions for culturally responsive digital
tools that scaffold rather than supplant relational care. Drawing insights from
qualitative accounts, we offer a situated perspective on how AI might
responsibly augment peer support. This research contributes to human-centred
computing by articulating the lived realities of peer supporters and proposing
design implications for trustworthy and context-sensitive AI in mental health.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [287] [Detecting malignant dynamics on very few blood sample using signature coefficients](https://arxiv.org/abs/2506.09097)
*Rémi Vaucher,Stéphane Chrétien*

Main category: q-bio.QM

TL;DR: 本文提出结合连续时间马尔可夫模型与Signature理论，基于血样检测侵袭性肿瘤，实验证明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 利用循环肿瘤DNA（ctDNA）水平进行癌症监测具有准确性高、患者负担小的优点，目前有基于监测ctDNA水平动态进行早期多癌检测的商业产品，本文想探索利用Signature理论基于血样检测侵袭性肿瘤。

Method: 结合连续时间马尔可夫模型分析血液中ctDNA水平动态，使用Signature理论构建高效测试程序。

Result: 该方法能解决因每个患者血样数量极少导致的数据稀缺难题，大量数值实验证实了所提流程的有效性。

Conclusion: 提出的结合方法在基于血样检测侵袭性肿瘤方面是有效的。

Abstract: Recent discoveries have suggested that the promising avenue of using
circulating tumor DNA (ctDNA) levels in blood samples provides reasonable
accuracy for cancer monitoring, with extremely low burden on the patient's
side. It is known that the presence of ctDNA can result from various mechanisms
leading to DNA release from cells, such as apoptosis, necrosis or active
secretion. One key idea in recent cancer monitoring studies is that monitoring
the dynamics of ctDNA levels might be sufficient for early multi-cancer
detection. This interesting idea has been turned into commercial products, e.g.
in the company named GRAIL.
  In the present work, we propose to explore the use of Signature theory for
detecting aggressive cancer tumors based on the analysis of blood samples. Our
approach combines tools from continuous time Markov modelling for the dynamics
of ctDNA levels in the blood, with Signature theory for building efficient
testing procedures. Signature theory is a topic of growing interest in the
Machine Learning community (see Chevyrev2016 and Fermanian2021), which is now
recognised as a powerful feature extraction tool for irregularly sampled
signals. The method proposed in the present paper is shown to correctly address
the challenging problem of overcoming the inherent data scarsity due to the
extremely small number of blood samples per patient. The relevance of our
approach is illustrated with extensive numerical experiments that confirm the
efficiency of the proposed pipeline.

</details>


### [288] [Simulation-trained conditional normalizing flows for likelihood approximation: a case study in stress regulation kinetics in yeast](https://arxiv.org/abs/2506.09374)
*Pedro Pessoa,Juan Andres Martinez,Vincent Vandenbroucke,Frank Delvigne,Steve Pressé*

Main category: q-bio.QM

TL;DR: 许多模型难以构建似然函数，本文用条件归一化流近似难以处理的似然函数，以酵母中glc3基因激活为例，发现考虑细胞分裂后glc3在压力下大多不活跃。


<details>
  <summary>Details</summary>
Motivation: 很多概念简单的模型缺乏可处理的似然函数，如从活跃分裂细胞的快照测量估计蛋白质产生，难以构建基于主方程的似然函数。

Method: 采用条件归一化流（一类用于学习概率分布的神经网络模型）从模拟数据近似难以处理的似然函数。

Result: 对酵母中glc3基因激活案例，不考虑细胞分裂分析认为很多细胞低表达活跃，考虑细胞分裂后发现glc3在压力下大多不活跃，表达短暂。

Conclusion: 条件归一化流可用于近似难以处理的似然函数，且在分析中考虑细胞分裂的非马尔可夫效应很重要。

Abstract: Physics-inspired inference often hinges on the ability to construct a
likelihood, or the probability of observing a sequence of data given a model.
These likelihoods can be directly maximized for parameter estimation,
incorporated into Bayesian frameworks, or even used as loss functions in neural
networks. Yet, many models, despite being conceptually simple, lack tractable
likelihoods. A notable example arises in estimating protein production from
snapshot measurements of actively dividing cells. Here, the challenge stems
from cell divisions occurring at non-Exponentially distributed intervals with
each division stochastically partitioning protein content between daughter
cells, making protein counts in any given cell a function of its full division
history. Such history dependence precludes a straightforward likelihood based
on a (standard Markovian) master equation. Instead, we employ conditional
normalizing flows (a class of neural network models designed to learn
probability distributions) to approximate otherwise intractable likelihoods
from simulated data. As a case study, we examine activation of the \emph{glc3}
gene in yeast involved in glycogen synthesis and expressed under
nutrient-limiting conditions. We monitor this activity using snapshot
fluorescence measurements via flow cytometry, where GFP expression reflects
\emph{glc3} promoter activity. A na\"ive analysis of flow cytometry data
ignoring cell division suggests many cells are active with low expression.
However, fluorescent proteins persist and can be inherited, so cells may appear
active from retaining ancestral fluorescence. Explicitly accounting for the
(non-Markovian) effects of cell division reveals \emph{glc3} is mostly inactive
under stress, showing that while cells occasionally activate it, expression is
brief and transient.

</details>


### [289] [Reconstructing Heterogeneous Biomolecules via Hierarchical Gaussian Mixtures and Part Discovery](https://arxiv.org/abs/2506.09063)
*Shayan Shekarforoush,David B. Lindell,Marcus A. Brubaker,David J. Fleet*

Main category: q-bio.QM

TL;DR: 本文介绍了基于分层高斯混合模型的3D重建框架CryoSPIRE，可处理构象和组成变化，在复杂实验数据集和基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决冷冻电镜中成像粒子存在非刚性构象灵活性和组成变化（部分缺失）时的结构建模问题。

Method: 引入具有分层高斯混合模型的3D重建框架，基于对粒子进行基于部分的分割来提供归纳偏置。

Result: 在复杂实验数据集上揭示了具有生物学意义的结构，在CryoBench基准测试中达到了新的最优水平。

Conclusion: 提出的CryoSPIRE框架能有效处理冷冻电镜中粒子的构象和组成变化问题。

Abstract: Cryo-EM is a transformational paradigm in molecular biology where
computational methods are used to infer 3D molecular structure at atomic
resolution from extremely noisy 2D electron microscope images. At the forefront
of research is how to model the structure when the imaged particles exhibit
non-rigid conformational flexibility and compositional variation where parts
are sometimes missing. We introduce a novel 3D reconstruction framework with a
hierarchical Gaussian mixture model, inspired in part by Gaussian Splatting for
4D scene reconstruction. In particular, the structure of the model is grounded
in an initial process that infers a part-based segmentation of the particle,
providing essential inductive bias in order to handle both conformational and
compositional variability. The framework, called CryoSPIRE, is shown to reveal
biologically meaningful structures on complex experimental datasets, and
establishes a new state-of-the-art on CryoBench, a benchmark for cryo-EM
heterogeneity methods.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [290] [Automatic Treatment Planning using Reinforcement Learning for High-dose-rate Prostate Brachytherapy](https://arxiv.org/abs/2506.09805)
*Tonghe Wang,Yining Feng,Xiaofeng Yang*

Main category: physics.med-ph

TL;DR: 研究使用强化学习在高剂量率前列腺近距离放射治疗预规划阶段提供针位置和驻留时间的可行性，结果显示该方法可行且效果良好。


<details>
  <summary>Details</summary>
Motivation: 高剂量率前列腺近距离放射治疗中，针放置模式依赖医生经验，旨在研究用强化学习提供针位置和驻留时间以减少手术时间并保证计划质量。

Method: 训练强化学习智能体调整针位置和驻留时间以最大化预定义奖励函数，对11名患者数据进行研究并与临床结果对比。

Result: 强化学习计划与临床计划前列腺覆盖率和直肠剂量相似，强化学习计划前列腺热点和尿道剂量更低，且平均少用2根针。

Conclusion: 首次证明强化学习用于自动生成临床实用的高剂量率前列腺近距离放射治疗计划的可行性，该方法能提高计划质量、减少用针，有潜力规范治疗规划、减少临床差异和改善患者预后。

Abstract: Purpose: In high-dose-rate (HDR) prostate brachytherapy procedures, the
pattern of needle placement solely relies on physician experience. We
investigated the feasibility of using reinforcement learning (RL) to provide
needle positions and dwell times based on patient anatomy during pre-planning
stage. This approach would reduce procedure time and ensure consistent plan
quality. Materials and Methods: We train a RL agent to adjust the position of
one selected needle and all the dwell times on it to maximize a pre-defined
reward function after observing the environment. After adjusting, the RL agent
then moves on to the next needle, until all needles are adjusted. Multiple
rounds are played by the agent until the maximum number of rounds is reached.
Plan data from 11 prostate HDR boost patients (1 for training, and 10 for
testing) treated in our clinic were included in this study. The dosimetric
metrics and the number of used needles of RL plan were compared to those of the
clinical results (ground truth). Results: On average, RL plans and clinical
plans have very similar prostate coverage (Prostate V100) and Rectum D2cc (no
statistical significance), while RL plans have less prostate hotspot (Prostate
V150) and Urethra D20% plans with statistical significance. Moreover, RL plans
use 2 less needles than clinical plan on average. Conclusion: We present the
first study demonstrating the feasibility of using reinforcement learning to
autonomously generate clinically practical HDR prostate brachytherapy plans.
This RL-based method achieved equal or improved plan quality compared to
conventional clinical approaches while requiring fewer needles. With minimal
data requirements and strong generalizability, this approach has substantial
potential to standardize brachytherapy planning, reduce clinical variability,
and enhance patient outcomes.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [291] [SimClass: A Classroom Speech Dataset Generated via Game Engine Simulation For Automatic Speech Recognition Research](https://arxiv.org/abs/2506.09206)
*Ahmed Adel Attia,Jing Liu,Carl Espy-Wilson*

Main category: cs.SD

TL;DR: 本文提出用游戏引擎合成教室噪音的方法，创建SimClass数据集，实验表明该数据集近似真实教室语音，对开发语音模型有价值。


<details>
  <summary>Details</summary>
Motivation: 大规模教室语音数据稀缺，公共教室数据集有限，缺乏专用教室噪音语料库阻碍了教育领域AI语音模型的发展。

Method: 用游戏引擎合成教室噪音的可扩展方法，将公共儿童语音语料库与YouTube讲座视频配对生成语音数据。

Result: SimClass数据集近似真实教室语音。

Conclusion: SimClass数据集是开发鲁棒语音识别和增强模型的宝贵资源。

Abstract: The scarcity of large-scale classroom speech data has hindered the
development of AI-driven speech models for education. Public classroom datasets
remain limited, and the lack of a dedicated classroom noise corpus prevents the
use of standard data augmentation techniques.
  In this paper, we introduce a scalable methodology for synthesizing classroom
noise using game engines, a framework that extends to other domains. Using this
methodology, we present SimClass, a dataset that includes both a synthesized
classroom noise corpus and a simulated classroom speech dataset. The speech
data is generated by pairing a public children's speech corpus with YouTube
lecture videos to approximate real classroom interactions in clean conditions.
Our experiments on clean and noisy speech demonstrate that SimClass closely
approximates real classroom speech, making it a valuable resource for
developing robust speech recognition and enhancement models.

</details>


### [292] [BemaGANv2: A Tutorial and Comparative Survey of GAN-based Vocoders for Long-Term Audio Generation](https://arxiv.org/abs/2506.09487)
*Taesoo Park,Mungwi Jeong,Mingyu Park,Narae Kim,Junyoung Kim,Mujung Kim,Jisang Yoo,Hoyun Lee,Sanghoon Kim,Soonchul Kwon*

Main category: cs.SD

TL;DR: 本文介绍BemaGANv2教程式调查与实现指南，该声码器有架构创新，经多指标评估，还提供教程与代码。


<details>
  <summary>Details</summary>
Motivation: 设计用于高保真和长期音频生成的先进GAN基声码器。

Method: 在生成器中用AMP模块替换传统ResBlocks，在判别器框架集成MED并结合MRD，系统评估多种判别器配置，用客观和主观指标评估。

Result: 通过多指标评估不同判别器配置效果。

Conclusion: 提供模型架构、训练方法和实现的全面教程，代码和预训练模型公开。

Abstract: This paper presents a tutorial-style survey and implementation guide of
BemaGANv2, an advanced GAN-based vocoder designed for high-fidelity and
long-term audio generation. Built upon the original BemaGAN architecture,
BemaGANv2 incorporates major architectural innovations by replacing traditional
ResBlocks in the generator with the Anti-aliased Multi-Periodicity composition
(AMP) module, which internally applies the Snake activation function to better
model periodic structures. In the discriminator framework, we integrate the
Multi-Envelope Discriminator (MED), a novel architecture we originally
proposed, to extract rich temporal envelope features crucial for periodicity
detection. Coupled with the Multi-Resolution Discriminator (MRD), this
combination enables more accurate modeling of long-range dependencies in audio.
We systematically evaluate various discriminator configurations, including MSD
+ MED, MSD + MRD, and MPD + MED + MRD, using objective metrics (FAD, SSIM,
PLCC, MCD) and subjective evaluations (MOS, SMOS). This paper also provides a
comprehensive tutorial on the model architecture, training methodology, and
implementation to promote reproducibility. The code and pre-trained models are
available at: https://github.com/dinhoitt/BemaGANv2.

</details>


### [293] [Training-Free Voice Conversion with Factorized Optimal Transport](https://arxiv.org/abs/2506.09709)
*Alexander Lobashev,Assel Yermekova,Maria Larchenko*

Main category: cs.SD

TL;DR: 本文介绍免训练的Factorized MKL - VC改进kNN - VC管道，仅需5秒参考音频实现高质量跨语言语音转换，实验显示其性能优于kNN - VC，与FACodec相当。


<details>
  <summary>Details</summary>
Motivation: 改进kNN - VC管道，以仅用5秒参考音频实现高质量的任意到任意跨语言语音转换。

Method: 在WavLM嵌入子空间中用从Monge - Kantorovich线性解导出的因式分解最优传输图取代kNN回归，因式分解处理跨维度的非均匀方差。

Result: 在LibriSpeech和FLEURS数据集实验表明，MKL - VC显著提高短参考音频下的内容保留和鲁棒性，优于kNN - VC，在跨语言语音转换领域性能与FACodec相当。

Conclusion: Factorized MKL - VC是对kNN - VC管道的有效改进，能在短参考音频下实现高质量跨语言语音转换。

Abstract: This paper introduces Factorized MKL-VC, a training-free modification for
kNN-VC pipeline. In contrast with original pipeline, our algorithm performs
high quality any-to-any cross-lingual voice conversion with only 5 second of
reference audio. MKL-VC replaces kNN regression with a factorized optimal
transport map in WavLM embedding subspaces, derived from Monge-Kantorovich
Linear solution. Factorization addresses non-uniform variance across
dimensions, ensuring effective feature transformation. Experiments on
LibriSpeech and FLEURS datasets show MKL-VC significantly improves content
preservation and robustness with short reference audio, outperforming kNN-VC.
MKL-VC achieves performance comparable to FACodec, especially in cross-lingual
voice conversion domain.

</details>


### [294] [Incorporating Linguistic Constraints from External Knowledge Source for Audio-Visual Target Speech Extraction](https://arxiv.org/abs/2506.09792)
*Wenxuan Wu,Shuai Wang,Xixin Wu,Helen Meng,Haizhou Li*

Main category: cs.SD

TL;DR: 探索预训练语音语言模型和预训练语言模型作为视听目标说话人提取（AV - TSE）辅助知识源，提出结合语言约束作为额外监督信号，无额外推理成本，提升语音质量，多语言和视觉受损场景表现佳。


<details>
  <summary>Details</summary>
Motivation: 人类利用语言知识辅助语音感知，探索预训练语音语言模型和预训练语言模型作为AV - TSE辅助知识源的潜力。

Method: 将预训练语音语言模型或预训练语言模型的语言约束作为额外监督信号引入AV - TSE模型。

Result: 不增加推理时额外计算成本，持续提升语音质量和可懂度，在多语言设置和视觉线索受损场景有稳健性能提升。

Conclusion: 结合预训练模型语言约束的方法对AV - TSE有效。

Abstract: Audio-visual target speaker extraction (AV-TSE) models primarily rely on
target visual cues to isolate the target speaker's voice from others. We know
that humans leverage linguistic knowledge, such as syntax and semantics, to
support speech perception. Inspired by this, we explore the potential of
pre-trained speech-language models (PSLMs) and pre-trained language models
(PLMs) as auxiliary knowledge sources for AV-TSE. In this study, we propose
incorporating the linguistic constraints from PSLMs or PLMs for the AV-TSE
model as additional supervision signals. Without introducing any extra
computational cost during inference, the proposed approach consistently
improves speech quality and intelligibility. Furthermore, we evaluate our
method in multi-language settings and visual cue-impaired scenarios and show
robust performance gains.

</details>


### [295] [UmbraTTS: Adapting Text-to-Speech to Environmental Contexts with Flow Matching](https://arxiv.org/abs/2506.09874)
*Neta Glazer,Aviv Navon,Yael Segal,Aviv Shamsian,Hilit Segev,Asaf Buchnick,Menachem Pirchi,Gil Hetz,Joseph Keshet*

Main category: cs.SD

TL;DR: 提出基于流匹配的TTS模型UmbraTTS，可联合生成语音与环境音频，通过自监督框架解决数据缺失问题，表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 当前文本转语音技术在将语音与复杂背景环境集成方面存在挑战。

Method: 引入基于流匹配的TTS模型UmbraTTS，提出自监督框架从无注释录音中提取语音、背景音频和转录内容。

Result: UmbraTTS显著优于现有基线，生成自然、高质量、具备环境感知的音频。

Conclusion: UmbraTTS在语音与环境音频联合生成方面表现出色，能解决相关挑战。

Abstract: Recent advances in Text-to-Speech (TTS) have enabled highly natural speech
synthesis, yet integrating speech with complex background environments remains
challenging. We introduce UmbraTTS, a flow-matching based TTS model that
jointly generates both speech and environmental audio, conditioned on text and
acoustic context. Our model allows fine-grained control over background volume
and produces diverse, coherent, and context-aware audio scenes. A key challenge
is the lack of data with speech and background audio aligned in natural
context. To overcome the lack of paired training data, we propose a
self-supervised framework that extracts speech, background audio, and
transcripts from unannotated recordings. Extensive evaluations demonstrate that
UmbraTTS significantly outperformed existing baselines, producing natural,
high-quality, environmentally aware audios.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [296] [How attention simplifies mental representations for planning](https://arxiv.org/abs/2506.09520)
*Jason da Silva Castanheira,Nicholas Shea,Stephen M. Fleming*

Main category: q-bio.NC

TL;DR: 本文通过虚拟迷宫导航研究空间注意力如何控制任务表征进入意识并用于规划，发现空间接近性影响规划信息可用性，注意力影响因个体而异，还将视觉空间注意力纳入现有计算模型以理解环境表征。


<details>
  <summary>Details</summary>
Motivation: 现有计算方法暗示规划和感知相互嵌套优化，但二者交互的感知和注意机制未知，需研究空间注意力如何控制任务表征。

Method: 利用虚拟迷宫导航进行研究。

Result: 空间接近性决定迷宫哪些方面可用于规划；任务相关信息符合注意自然轮廓时，人们更易构建简化有用的迷宫表征；注意力影响个体差异大。

Conclusion: 将视觉空间注意力纳入现有计算模型，桥接感知和决策的计算视角，有助于理解个体为规划而进行的环境表征。

Abstract: Human planning is efficient -- it frugally deploys limited cognitive
resources to accomplish difficult tasks -- and flexible -- adapting to novel
problems and environments. Computational approaches suggest that people
construct simplified mental representations of their environment, balancing the
complexity of a task representation with its utility. These models imply a
nested optimisation in which planning shapes perception, and perception shapes
planning -- but the perceptual and attentional mechanisms governing how this
interaction unfolds remain unknown. Here, we harness virtual maze navigation to
characterise how spatial attention controls which aspects of a task
representation enter subjective awareness and are available for planning. We
find that spatial proximity governs which aspects of a maze are available for
planning, and that when task-relevant information follows natural (lateralised)
contours of attention, people can more easily construct simplified and useful
maze representations. This influence of attention varies considerably across
individuals, explaining differences in people's task representations and
behaviour. Inspired by the 'spotlight of attention' analogy, we incorporate the
effects of visuospatial attention into existing computational accounts of
value-guided construal. Together, our work bridges computational perspectives
on perception and decision-making to better understand how individuals
represent their environments in aid of planning.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [297] [A Probabilistic Framework for Imputing Genetic Distances in Spatiotemporal Pathogen Models](https://arxiv.org/abs/2506.09076)
*Haley Stone,Jing Du,Hao Xue,Matthew Scotch,David Heslop,Andreas Züfle,Chandini Raina MacIntyre,Flora Salim*

Main category: q-bio.GN

TL;DR: 提出概率框架推断未测序病例与已知序列间遗传距离，应用于禽流感案例，支持基因组数据集扩充和时空建模。


<details>
  <summary>Details</summary>
Motivation: 病原体基因组数据因测序覆盖不完全，限制了其在空间模型中的效用。

Method: 提出概率框架，用时间感知的进化距离建模，从收集日期和观测到的遗传距离估计成对差异。

Result: 应用于美国野生鸟类高致病性禽流感A/H5病例，支持可扩展、考虑不确定性的基因组数据集扩充。

Conclusion: 该方法能增强进化信息融入时空建模工作流程。

Abstract: Pathogen genome data offers valuable structure for spatial models, but its
utility is limited by incomplete sequencing coverage. We propose a
probabilistic framework for inferring genetic distances between unsequenced
cases and known sequences within defined transmission chains, using time-aware
evolutionary distance modeling. The method estimates pairwise divergence from
collection dates and observed genetic distances, enabling biologically
plausible imputation grounded in observed divergence patterns, without
requiring sequence alignment or known transmission chains. Applied to highly
pathogenic avian influenza A/H5 cases in wild birds in the United States, this
approach supports scalable, uncertainty-aware augmentation of genomic datasets
and enhances the integration of evolutionary information into spatiotemporal
modeling workflows.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [298] [STREAMINGGS: Voxel-Based Streaming 3D Gaussian Splatting with Memory Optimization and Architectural Support](https://arxiv.org/abs/2506.09070)
*Chenqi Zhang,Yu Feng,Jieru Zhao,Guangda Liu,Wenchao Ding,Chentao Wu,Minyi Guo*

Main category: cs.GR

TL;DR: 提出STREAMINGGS算法架构协同设计，解决3DGS在移动设备实时性问题，有显著加速和节能效果。


<details>
  <summary>Details</summary>
Motivation: 3DGS在资源受限移动设备上无法达到实时要求，现有加速器忽视内存效率，存在冗余DRAM流量。

Method: 提出STREAMINGGS，采用全流式3DGS算法架构协同设计，从以图块为中心渲染转变为以内存为中心渲染实现细粒度流水线并减少DRAM流量。

Result: 相比移动Ampere GPU，设计实现了高达45.7倍的加速和62.9倍的节能。

Conclusion: STREAMINGGS有效解决3DGS在移动设备上的实时性问题，有显著性能和节能提升。

Abstract: 3D Gaussian Splatting (3DGS) has gained popularity for its efficiency and
sparse Gaussian-based representation. However, 3DGS struggles to meet the
real-time requirement of 90 frames per second (FPS) on resource-constrained
mobile devices, achieving only 2 to 9 FPS.Existing accelerators focus on
compute efficiency but overlook memory efficiency, leading to redundant DRAM
traffic. We introduce STREAMINGGS, a fully streaming 3DGS
algorithm-architecture co-design that achieves fine-grained pipelining and
reduces DRAM traffic by transforming from a tile-centric rendering to a
memory-centric rendering. Results show that our design achieves up to 45.7
$\times$ speedup and 62.9 $\times$ energy savings over mobile Ampere GPUs.

</details>


### [299] [SILK: Smooth InterpoLation frameworK for motion in-betweening A Simplified Computational Approach](https://arxiv.org/abs/2506.09075)
*Elly Akhoundi,Hung Yu Ling,Anup Anand Deshmukh,Judith Butepage*

Main category: cs.GR

TL;DR: 本文提出基于Transformer的简单框架用于运动中间帧生成，发现数据建模选择对性能有重要影响，挑战模型复杂度决定动画质量的假设。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习运动中间帧生成方案依赖复杂模型，作者希望提出简单有效的方法。

Method: 采用单个Transformer编码器合成逼真运动，研究不同数据建模选择对性能的影响。

Result: 增加数据量可实现等效或更好的运动过渡，选择合适的姿态表示对获得高质量结果至关重要，加入速度输入特征可提升动画性能。

Conclusion: 模型复杂度并非动画质量的主要决定因素，应采用以数据为中心的运动插值方法。

Abstract: Motion in-betweening is a crucial tool for animators, enabling intricate
control over pose-level details in each keyframe. Recent machine learning
solutions for motion in-betweening rely on complex models, incorporating
skeleton-aware architectures or requiring multiple modules and training steps.
In this work, we introduce a simple yet effective Transformer-based framework,
employing a single Transformer encoder to synthesize realistic motions for
motion in-betweening tasks. We find that data modeling choices play a
significant role in improving in-betweening performance. Among others, we show
that increasing data volume can yield equivalent or improved motion
transitions, that the choice of pose representation is vital for achieving
high-quality results, and that incorporating velocity input features enhances
animation performance. These findings challenge the assumption that model
complexity is the primary determinant of animation quality and provide insights
into a more data-centric approach to motion interpolation. Additional videos
and supplementary material are available at https://silk-paper.github.io.

</details>


### [300] [DGS-LRM: Real-Time Deformable 3D Gaussian Reconstruction From Monocular Videos](https://arxiv.org/abs/2506.09997)
*Chieh Hubert Lin,Zhaoyang Lv,Songyin Wu,Zhen Xu,Thu Nguyen-Phuoc,Hung-Yu Tseng,Julian Straub,Numair Khan,Lei Xiao,Ming-Hsuan Yang,Yuheng Ren,Richard Newcombe,Zhao Dong,Zhengqin Li*

Main category: cs.GR

TL;DR: 提出可变形高斯 splats 大型重建模型 DGS - LRM，用于动态场景前馈重建，实验表明其有良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有前馈场景重建模型多局限于静态场景，开发动态场景前馈模型面临训练数据稀缺、合适 3D 表示和训练范式缺乏等挑战。

Method: 引入增强的大规模合成数据集、易于学习的每像素可变形 3D 高斯表示和大型变压器网络。

Result: DGS - LRM 动态场景重建质量与基于优化的方法相当，在真实世界示例中显著优于现有预测动态重建方法，预测的 3D 变形准确，适用于长距离 3D 跟踪任务。

Conclusion: DGS - LRM 是一种有效的动态场景前馈重建模型，在动态场景重建和 3D 跟踪任务中有良好表现。

Abstract: We introduce the Deformable Gaussian Splats Large Reconstruction Model
(DGS-LRM), the first feed-forward method predicting deformable 3D Gaussian
splats from a monocular posed video of any dynamic scene. Feed-forward scene
reconstruction has gained significant attention for its ability to rapidly
create digital replicas of real-world environments. However, most existing
models are limited to static scenes and fail to reconstruct the motion of
moving objects. Developing a feed-forward model for dynamic scene
reconstruction poses significant challenges, including the scarcity of training
data and the need for appropriate 3D representations and training paradigms. To
address these challenges, we introduce several key technical contributions: an
enhanced large-scale synthetic dataset with ground-truth multi-view videos and
dense 3D scene flow supervision; a per-pixel deformable 3D Gaussian
representation that is easy to learn, supports high-quality dynamic view
synthesis, and enables long-range 3D tracking; and a large transformer network
that achieves real-time, generalizable dynamic scene reconstruction. Extensive
qualitative and quantitative experiments demonstrate that DGS-LRM achieves
dynamic scene reconstruction quality comparable to optimization-based methods,
while significantly outperforming the state-of-the-art predictive dynamic
reconstruction method on real-world examples. Its predicted physically grounded
3D deformation is accurate and can readily adapt for long-range 3D tracking
tasks, achieving performance on par with state-of-the-art monocular video 3D
tracking methods.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [301] [Enhanced V2X Communication Using Game-Theory Based Adaptive MAC Protocols](https://arxiv.org/abs/2506.09817)
*Dhrumil Bhatt,Nirbhay Singhal*

Main category: eess.SY

TL;DR: 本文提出基于博弈论的自适应MAC增强型V2X通信系统，模拟显示在高密度条件下有良好效果。


<details>
  <summary>Details</summary>
Motivation: 提升V2X通信系统性能，应对高密度车辆通信需求。

Method: 采用博弈论实现自适应MAC，集成动态传输功率控制、动态信标率等机制，在圆形覆盖区域模拟，结合信号传播模型和概率车辆移动性。

Result: 在250米半径内最多80辆车的高密度条件下，平均信标延迟低于0.35秒，丢包率低于1%。

Conclusion: 基于博弈论的环境感知传输参数自适应和可扩展设计适用于易受干扰的V2X部署。

Abstract: This paper presents an enhanced Vehicle-to-Everything (V2X) communication
system featuring adaptive Medium Access Control (MAC) using game theory. Our
approach integrates dynamic transmission power control, dynamic beacon rates,
contention window adaptation, and implicit acknowledgment mechanisms within a
Manhattan-like grid-based mobility scenario. Simulations are conducted in a
circular coverage area, incorporating refined signal propagation models and
probabilistic vehicle mobility with boundary reflection. The results
demonstrate effective beacon delivery with average delays under 0.35 s and
packet loss rates less than 1% in high-density conditions specifically, with up
to 80 vehicles operating within a 250 m radius. Key innovations include game
theory-based environment-aware transmission parameter adaptation and a scalable
design suited for interference-prone V2X deployments.

</details>


### [302] [A Survey on the Role of Artificial Intelligence and Machine Learning in 6G-V2X Applications](https://arxiv.org/abs/2506.09512)
*Donglin Wang,Anjie Qiu,Qiuheng Zhou,Hans D. Schotten*

Main category: eess.SY

TL;DR: 本文全面综述了应用于6G - V2X通信的AI和ML模型，分析其作用、挑战并指明未来方向。


<details>
  <summary>Details</summary>
Motivation: 6G网络为CAV提供连接，AI和ML能优化V2X通信，但缺乏该领域近期研究的系统总结。

Method: 对AI和ML模型，如DL、RL、GL、FL在6G - V2X通信的应用进行全面综述。

Result: AI尤其是GL在提升6G - V2X系统性能、适应性和智能性上有显著进展和潜力。

Conclusion: 分析了AI和ML在6G - V2X应用的作用、挑战，为相关人员提供实现智能V2X生态系统的见解。

Abstract: The rapid advancement of Vehicle-to-Everything (V2X) communication is
transforming Intelligent Transportation Systems (ITS), with 6G networks
expected to provide ultra-reliable, low-latency, and high-capacity connectivity
for Connected and Autonomous Vehicles (CAVs). Artificial Intelligence (AI) and
Machine Learning (ML) have emerged as key enablers in optimizing V2X
communication by enhancing network management, predictive analytics, security,
and cooperative driving due to their outstanding performance across various
domains, such as natural language processing and computer vision. This survey
comprehensively reviews recent advances in AI and ML models applied to 6G-V2X
communication. It focuses on state-of-the-art techniques, including Deep
Learning (DL), Reinforcement Learning (RL), Generative Learning (GL), and
Federated Learning (FL), with particular emphasis on developments from the past
two years. Notably, AI, especially GL, has shown remarkable progress and
emerging potential in enhancing the performance, adaptability, and intelligence
of 6G-V2X systems. Despite these advances, a systematic summary of recent
research efforts in this area remains lacking, which this survey aims to
address. We analyze their roles in 6G-V2X applications, such as intelligent
resource allocation, beamforming, intelligent traffic management, and security
management. Furthermore, we explore the technical challenges, including
computational complexity, data privacy, and real-time decision-making
constraints, while identifying future research directions for AI-driven 6G-V2X
development. This study aims to provide valuable insights for researchers,
engineers, and policymakers working towards realizing intelligent, AI-powered
V2X ecosystems in 6G communication.

</details>
