<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 15]
- [cs.CE](#cs.CE) [Total: 8]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.DS](#cs.DS) [Total: 5]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.IR](#cs.IR) [Total: 10]
- [cs.LG](#cs.LG) [Total: 52]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.SE](#cs.SE) [Total: 15]
- [q-fin.RM](#q-fin.RM) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [stat.ML](#stat.ML) [Total: 3]
- [stat.CO](#stat.CO) [Total: 2]
- [cs.AR](#cs.AR) [Total: 1]
- [nlin.CD](#nlin.CD) [Total: 1]
- [cs.CV](#cs.CV) [Total: 24]
- [cs.NI](#cs.NI) [Total: 1]
- [math.AP](#math.AP) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.PL](#cs.PL) [Total: 2]
- [math.OC](#math.OC) [Total: 2]
- [cs.HC](#cs.HC) [Total: 6]
- [cs.CR](#cs.CR) [Total: 4]
- [cs.SD](#cs.SD) [Total: 3]
- [eess.SP](#eess.SP) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [hep-ph](#hep-ph) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 2]
- [eess.IV](#eess.IV) [Total: 5]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [math.CO](#math.CO) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.CL](#cs.CL) [Total: 13]
- [cs.LO](#cs.LO) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [cs.CC](#cs.CC) [Total: 1]
- [econ.GN](#econ.GN) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Initial Steps in Integrating Large Reasoning and Action Models for Service Composition](https://arxiv.org/abs/2507.18775)
*Ilche Georgievski,Marco Aiello*

Main category: cs.AI

TL;DR: 文章探讨将大推理模型（LRMs）和大行动模型（LAMs）集成用于服务组合，提出LRM - LAM架构框架，有望使服务组合自动化、用户友好。


<details>
  <summary>Details</summary>
Motivation: 服务组合面临推理能力有限和执行机制脆弱的挑战，需新方法推进。

Method: 提出集成LRMs和LAMs的LRM - LAM架构框架。

Result: 该集成系统可推理服务需求和约束，动态执行工作流，弥合意图与执行差距。

Conclusion: 这种集成有潜力将服务组合转变为由高级自然语言意图驱动的全自动、用户友好过程。

Abstract: Service composition remains a central challenge in building adaptive and
intelligent software systems, often constrained by limited reasoning
capabilities or brittle execution mechanisms. This paper explores the
integration of two emerging paradigms enabled by large language models: Large
Reasoning Models (LRMs) and Large Action Models (LAMs). We argue that LRMs
address the challenges of semantic reasoning and ecosystem complexity while
LAMs excel in dynamic action execution and system interoperability. However,
each paradigm has complementary limitations - LRMs lack grounded action
capabilities, and LAMs often struggle with deep reasoning. We propose an
integrated LRM-LAM architectural framework as a promising direction for
advancing automated service composition. Such a system can reason about service
requirements and constraints while dynamically executing workflows, thus
bridging the gap between intention and execution. This integration has the
potential to transform service composition into a fully automated,
user-friendly process driven by high-level natural language intent.

</details>


### [2] [Simulation-Driven Reinforcement Learning in Queuing Network Routing Optimization](https://arxiv.org/abs/2507.18795)
*Fatima Al-Ani,Molly Wang,Jevon Charles,Aaron Ong,Joshua Forday,Vinayak Modi*

Main category: cs.AI

TL;DR: 提出模拟驱动的强化学习框架优化复杂排队网络系统路由决策，实验证明其有效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统排队方法在动态、不确定环境中存在局限，需要新方法优化复杂排队网络系统的路由决策。

Method: 提出结合深度确定性策略梯度（DDPG）和Dyna式规划的强化学习方法（Dyna - DDPG），框架有可配置模拟环境，增强的Dyna - DDPG实现包含独立预测模型。

Result: 框架能快速学习有效路由策略，在干扰下保持稳健性能，可有效扩展到更大网络规模。

Conclusion: 该框架有效可行，采用的软件工程实践保证了可重复性和可维护性，能在现实场景中实际部署。

Abstract: This study focuses on the development of a simulation-driven reinforcement
learning (RL) framework for optimizing routing decisions in complex queueing
network systems, with a particular emphasis on manufacturing and communication
applications. Recognizing the limitations of traditional queueing methods,
which often struggle with dynamic, uncertain environments, we propose a robust
RL approach leveraging Deep Deterministic Policy Gradient (DDPG) combined with
Dyna-style planning (Dyna-DDPG). The framework includes a flexible and
configurable simulation environment capable of modeling diverse queueing
scenarios, disruptions, and unpredictable conditions. Our enhanced Dyna-DDPG
implementation incorporates separate predictive models for next-state
transitions and rewards, significantly improving stability and sample
efficiency. Comprehensive experiments and rigorous evaluations demonstrate the
framework's capability to rapidly learn effective routing policies that
maintain robust performance under disruptions and scale effectively to larger
network sizes. Additionally, we highlight strong software engineering practices
employed to ensure reproducibility and maintainability of the framework,
enabling practical deployment in real-world scenarios.

</details>


### [3] [A Neuroscience-Inspired Dual-Process Model of Compositional Generalization](https://arxiv.org/abs/2507.18868)
*Alex Noviello,Claas Beger,Jacob Groner,Kevin Ellis,Weinan Sun*

Main category: cs.AI

TL;DR: 提出MIRAGE框架实现组合任务的系统泛化，在SCAN基准测试表现佳，消融实验确定其系统性依赖因素。


<details>
  <summary>Details</summary>
Motivation: 解决AI系统在系统性组合泛化方面的核心挑战，借鉴人类认知中海马体和前额叶皮质的相互作用机制。

Method: MIRAGE框架有两个交互模块，Transformer Neural Decomposer模拟新皮质模式识别，Schema Engine模拟HPC - PFC循环，通过明确管理的图式结构进行组合操作。

Result: 在SCAN基准测试中，仅用119万个参数的Transformer模块在所有任务分割上达到>99%的准确率。

Conclusion: MIRAGE的系统性关键依赖于提取图式的质量和模型的迭代细化过程。

Abstract: Systematic compositional generalization - constructing and understanding
novel combinations of known building blocks - remains a core challenge for AI
systems. Human cognition achieves this flexibility via the interplay of the
hippocampus (HPC) and prefrontal cortex (PFC): the hippocampus rapidly encodes
episodes, and the prefrontal cortex consolidates them into reusable schemas for
reasoning. Drawing on these insights, we present MIRAGE (Meta-Inference with
Rules and Abstractions from Generalized Experience), a framework that achieves
systematic generalization on compositional tasks. MIRAGE has two interacting
modules mirroring the brain's deliberative HPC-PFC loop and intuitive
neocortical pattern recognition. (1) The meta-trained Transformer Neural
Decomposer, paralleling neocortical "System 1" computation, is trained on a
task-agnostic stream of randomly sampled compositional grammars and applies one
decomposition step per pass, with successive passes iteratively refining the
sequence representation. (2) The Schema Engine, analogous to the HPC-PFC
"System 2" loop, dynamically extracts, ranks, and applies reusable schemas,
storing variable bindings in episodic memory and expanding them when needed. By
explicitly equipping the Transformer component of MIRAGE with actively managed
schematic structures, our model performs systematic compositional operations
through explicit schema application and transformation, relying solely on
frozen weights when solving entirely novel tasks. This approach demonstrates
systematic compositional generalization on the SCAN benchmark, achieving > 99%
accuracy on all task splits with only 1.19M parameters in the transformer
module. Ablation studies confirm that MIRAGE's systematicity critically depends
on the quality of extracted schemas and the model's iterative refinement
process.

</details>


### [4] [Success in Humanoid Reinforcement Learning under Partial Observation](https://arxiv.org/abs/2507.18883)
*Wuhao Wang,Zhiyong Chen*

Main category: cs.AI

TL;DR: 研究首次在Gymnasium Humanoid-v4环境下实现部分可观测性学习，策略性能与全状态访问相当，关键是新的历史编码器。


<details>
  <summary>Details</summary>
Motivation: 强化学习在部分可观测性下的策略学习是挑战，此前无人在Gymnasium Humanoid-v4环境用不完整状态信息稳定训练类人机器人策略。

Method: 采用新的历史编码器并行处理固定长度的过去观察序列，并集成到标准无模型算法中。

Result: 学习到的策略用原状态的三分之一到三分之二实现了与全状态访问相当的性能，且对机器人属性有适应性。

Conclusion: 新的历史编码器能从近期观察中重建关键上下文信息，实现稳健决策。

Abstract: Reinforcement learning has been widely applied to robotic control, but
effective policy learning under partial observability remains a major
challenge, especially in high-dimensional tasks like humanoid locomotion. To
date, no prior work has demonstrated stable training of humanoid policies with
incomplete state information in the benchmark Gymnasium Humanoid-v4
environment. The objective in this environment is to walk forward as fast as
possible without falling, with rewards provided for staying upright and moving
forward, and penalties incurred for excessive actions and external contact
forces. This research presents the first successful instance of learning under
partial observability in this environment. The learned policy achieves
performance comparable to state-of-the-art results with full state access,
despite using only one-third to two-thirds of the original states. Moreover,
the policy exhibits adaptability to robot properties, such as variations in
body part masses. The key to this success is a novel history encoder that
processes a fixed-length sequence of past observations in parallel. Integrated
into a standard model-free algorithm, the encoder enables performance on par
with fully observed baselines. We hypothesize that it reconstructs essential
contextual information from recent observations, thereby enabling robust
decision-making.

</details>


### [5] [Towards Improving Long-Tail Entity Predictions in Temporal Knowledge Graphs through Global Similarity and Weighted Sampling](https://arxiv.org/abs/2507.18977)
*Mehrnoosh Mirtaheri,Ryan A. Rossi,Sungchul Kim,Kanak Mahadik,Tong Yu,Xiang Chen,Mohammad Rostami*

Main category: cs.AI

TL;DR: 提出针对TKG的增量训练框架，结合增强层和加权采样策略，在两个基准数据集上表现优于现有方法，提升了TKG补全的性能。


<details>
  <summary>Details</summary>
Motivation: 传统TKG补全模型在训练时假设能访问整个图，忽略了TKG演变带来的挑战，如模型泛化和处理新实体及稀疏连接的问题。

Method: 提出增量训练框架，结合模型无关的增强层和加权采样策略，增强层利用更广泛的实体相似度定义，加权采样策略强调稀有实体的边。

Result: 在两个基准数据集上进行评估，在总链接预测、归纳链接预测和处理长尾实体方面优于现有方法，MRR分别提升10%和15%。

Conclusion: 该方法能缓解灾难性遗忘，增强TKG补全方法的鲁棒性，尤其在增量训练场景中具有潜力。

Abstract: Temporal Knowledge Graph (TKG) completion models traditionally assume access
to the entire graph during training. This overlooks challenges stemming from
the evolving nature of TKGs, such as: (i) the model's requirement to generalize
and assimilate new knowledge, and (ii) the task of managing new or unseen
entities that often have sparse connections. In this paper, we present an
incremental training framework specifically designed for TKGs, aiming to
address entities that are either not observed during training or have sparse
connections. Our approach combines a model-agnostic enhancement layer with a
weighted sampling strategy, that can be augmented to and improve any existing
TKG completion method. The enhancement layer leverages a broader, global
definition of entity similarity, which moves beyond mere local neighborhood
proximity of GNN-based methods. The weighted sampling strategy employed in
training accentuates edges linked to infrequently occurring entities. We
evaluate our method on two benchmark datasets, and demonstrate that our
framework outperforms existing methods in total link prediction, inductive link
prediction, and in addressing long-tail entities. Notably, our method achieves
a 10\% improvement and a 15\% boost in MRR for these datasets. The results
underscore the potential of our approach in mitigating catastrophic forgetting
and enhancing the robustness of TKG completion methods, especially in an
incremental training context

</details>


### [6] [Fine-Grained Traffic Inference from Road to Lane via Spatio-Temporal Graph Node Generation](https://arxiv.org/abs/2507.19089)
*Shuhao Li,Weidong Yang,Yue Cui,Xiaoxing Liu,Lingkai Meng,Lipeng Ma,Fan Zhang*

Main category: cs.AI

TL;DR: 提出FRTI任务并设计RoadDiff框架来解决该任务，通过多数据集实验验证其有效性，代码和数据开源。


<details>
  <summary>Details</summary>
Motivation: 获取车道级交通数据是数据驱动模型的关键瓶颈，需要更节能、经济的解决方案用于精确交通管理。

Method: 将FRTI任务抽象为时空图节点生成问题，设计两阶段框架RoadDiff，利用相关模块挖掘数据关系，还设计基线模型。

Result: 在六个代表不同路况的数据集上进行广泛实验，验证了RoadDiff模型解决FRTI任务的有效性。

Conclusion: RoadDiff框架能利用有限道路数据准确推断细粒度车道交通状态，为精确交通管理提供有效方案。

Abstract: Fine-grained traffic management and prediction are fundamental to key
applications such as autonomous driving, lane change guidance, and traffic
signal control. However, obtaining lane-level traffic data has become a
critical bottleneck for data-driven models due to limitations in the types and
number of sensors and issues with the accuracy of tracking algorithms. To
address this, we propose the Fine-grained Road Traffic Inference (FRTI) task,
which aims to generate more detailed lane-level traffic information using
limited road data, providing a more energy-efficient and cost-effective
solution for precise traffic management. This task is abstracted as the first
scene of the spatio-temporal graph node generation problem. We designed a
two-stage framework--RoadDiff--to solve the FRTI task. solve the FRTI task.
This framework leverages the Road-Lane Correlation Autoencoder-Decoder and the
Lane Diffusion Module to fully utilize the limited spatio-temporal dependencies
and distribution relationships of road data to accurately infer fine-grained
lane traffic states. Based on existing research, we designed several baseline
models with the potential to solve the FRTI task and conducted extensive
experiments on six datasets representing different road conditions to validate
the effectiveness of the RoadDiff model in addressing the FRTI task. The
relevant datasets and code are available at
https://github.com/ShuhaoLii/RoadDiff.

</details>


### [7] [Pareto-NRPA: A Novel Monte-Carlo Search Algorithm for Multi-Objective Optimization](https://arxiv.org/abs/2507.19109)
*Noé Lallouet,Tristan Cazenave,Cyrille Enderli*

Main category: cs.AI

TL;DR: 介绍Pareto - NRPA算法用于离散搜索空间多目标优化，在两类问题上测试，性能有竞争力。


<details>
  <summary>Details</summary>
Motivation: 将原本用于单目标问题的NRPA算法扩展到多目标优化问题。

Method: 扩展嵌套搜索和策略更新机制，用一组策略探索解空间，在各搜索层级维护非支配前沿，根据帕累托前沿序列的多样性和孤立性进行策略调整。

Result: Pareto - NRPA在收敛性和解的多样性上与最先进的多目标算法竞争，在约束搜索空间上大幅超越最先进的进化多目标算法。

Conclusion: 这是首次将NRPA应用于多目标场景。

Abstract: We introduce Pareto-NRPA, a new Monte-Carlo algorithm designed for
multi-objective optimization problems over discrete search spaces. Extending
the Nested Rollout Policy Adaptation (NRPA) algorithm originally formulated for
single-objective problems, Pareto-NRPA generalizes the nested search and policy
update mechanism to multi-objective optimization. The algorithm uses a set of
policies to concurrently explore different regions of the solution space and
maintains non-dominated fronts at each level of search. Policy adaptation is
performed with respect to the diversity and isolation of sequences within the
Pareto front. We benchmark Pareto-NRPA on two classes of problems: a novel
bi-objective variant of the Traveling Salesman Problem with Time Windows
problem (MO-TSPTW), and a neural architecture search task on well-known
benchmarks. Results demonstrate that Pareto-NRPA achieves competitive
performance against state-of-the-art multi-objective algorithms, both in terms
of convergence and diversity of solutions. Particularly, Pareto-NRPA strongly
outperforms state-of-the-art evolutionary multi-objective algorithms on
constrained search spaces. To our knowledge, this work constitutes the first
adaptation of NRPA to the multi-objective setting.

</details>


### [8] [OS-MAP: How Far Can Computer-Using Agents Go in Breadth and Depth?](https://arxiv.org/abs/2507.19132)
*Xuetian Chen,Yinghao Chen,Xinfeng Yuan,Zhuo Peng,Lu Chen,Yuekeng Li,Zhoujia Zhang,Yingqian Huang,Leyan Huang,Jiaqing Liang,Tianbao Xie,Zhiyong Wu,Qiushi Sun,Biqing Qi,Bowen Zhou*

Main category: cs.AI

TL;DR: 本文提出基准 OS - MAP 评估计算机使用代理，实验表明当前先进代理在高级任务存在问题，代码等公开。


<details>
  <summary>Details</summary>
Motivation: 现有基准未考虑任务异质性、代理能力及与用户需求的一致性，阻碍研究进展转化和能力发展。

Method: 提出 OS - MAP 基准，按自动化五级分类和现实用户需求层次的泛化范围组织 416 个现实任务，从两个维度评估代理，形成性能 - 泛化评估矩阵。

Result: 实验显示即使是最先进的基于 VLM 骨干的代理在涉及感知、推理和协调的高级任务中也存在困难。

Conclusion: 需要深入了解当前代理的优缺点，以推动计算机使用代理的研究和部署。

Abstract: Computer-using agents have shown strong potential to boost human productivity
and enable new application forms across platforms. While recent advances have
led to usable applications, existing benchmarks fail to account for the
internal task heterogeneity and the corresponding agent capabilities, as well
as their alignment with actual user demands-hindering both targeted capability
development and the reliable transition of research progress into practical
deployment. To bridge the gap, we present OS-MAP, a benchmark for daily
computer-using automation that organizes its 416 realistic tasks across 15
applications along two key dimensions: a five-level taxonomy of automation and
a generalization scope derived from a real-world user demand hierarchy. To
enable fine-grained analysis of required capabilities and alignment with
real-world scenarios, OS-MAP evaluates agents along two dimensions: automation
level across a five-level taxonomy, and generalization scope across a demand
hierarchy. This design captures varying levels of required agent autonomy and
generalization, forming a performance-generalization evaluation matrix for
structured and comprehensive assessment. Experiments show that even
State-of-the-Art agents with VLM backbones struggle with higher-level tasks
involving perception, reasoning, and coordination-highlighting the need for a
deeper understanding of current strengths and limitations to drive the future
progress in computer-using agents research and deployment. All code,
environments, baselines, and data are publicly available at
https://github.com/OS-Copilot/OS-Map.

</details>


### [9] [PhysDrive: A Multimodal Remote Physiological Measurement Dataset for In-vehicle Driver Monitoring](https://arxiv.org/abs/2507.19172)
*Jiyao Wang,Xiao Yang,Qingyong Hu,Jiankai Tang,Can Liu,Dengbo He,Yuntao Wang,Yingcong Chen,Kaishun Wu*

Main category: cs.AI

TL;DR: 本文提出首个大规模多模态非接触式车内生理传感数据集PhysDrive，涵盖多种数据和驾驶条件，进行评估并开源代码，有望推动相关研究。


<details>
  <summary>Details</summary>
Motivation: 远程生理测量（RPM）应用于实际驾驶场景受限于缺乏综合数据集，现有资源存在不足，需新数据集。

Method: 收集48名驾驶员的数据，包含多种模态和同步的六项地面真值，覆盖多种自然驾驶条件；在数据集上评估信号处理和深度学习方法，建立多模态基准。

Result: 建立了全面的多模态基准，并发布与主流公共工具箱兼容的开源代码。

Conclusion: PhysDrive将作为基础资源，加速多模态驾驶员监测和智能座舱系统的研究。

Abstract: Robust and unobtrusive in-vehicle physiological monitoring is crucial for
ensuring driving safety and user experience. While remote physiological
measurement (RPM) offers a promising non-invasive solution, its translation to
real-world driving scenarios is critically constrained by the scarcity of
comprehensive datasets. Existing resources are often limited in scale, modality
diversity, the breadth of biometric annotations, and the range of captured
conditions, thereby omitting inherent real-world challenges in driving. Here,
we present PhysDrive, the first large-scale multimodal dataset for contactless
in-vehicle physiological sensing with dedicated consideration on various
modality settings and driving factors. PhysDrive collects data from 48 drivers,
including synchronized RGB, near-infrared camera, and raw mmWave radar data,
accompanied with six synchronized ground truths (ECG, BVP, Respiration, HR, RR,
and SpO2). It covers a wide spectrum of naturalistic driving conditions,
including driver motions, dynamic natural light, vehicle types, and road
conditions. We extensively evaluate both signal-processing and deep-learning
methods on PhysDrive, establishing a comprehensive benchmark across all
modalities, and release full open-source code with compatibility for mainstream
public toolboxes. We envision PhysDrive will serve as a foundational resource
and accelerate research on multimodal driver monitoring and smart-cockpit
systems.

</details>


### [10] [Faster Lifting for Ordered Domains with Predecessor Relations](https://arxiv.org/abs/2507.19182)
*Kuncheng Zou,Jiahao Mai,Yonggang Zhang,Yuyi Wang,Ondřej Kuželka,Yuanhong Wang,Yi Chang*

Main category: cs.AI

TL;DR: 本文针对有序域上带前驱关系的提升推理问题，提出新算法，对已知可处理的关系有指数级加速，还能处理一般k - 阶前驱关系，实验证明算法高效。


<details>
  <summary>Details</summary>
Motivation: 先前基于加权一阶模型计数（WFOMC）处理有序域上带前驱关系的提升推理问题的算法在实际应用中表现不佳，尤其是涉及前驱关系时。

Method: 将前驱关系作为公理的原生部分，设计一种能固有支持这些关系的新算法。

Result: 新算法对已知可处理的直接和二阶前驱关系有指数级加速，能处理一般k - 阶前驱关系，在提升推理任务和组合数学问题的实验中实现了一个数量级的加速。

Conclusion: 提出的算法在处理有序域上带前驱关系的提升推理问题时高效可行。

Abstract: We investigate lifted inference on ordered domains with predecessor
relations, where the elements of the domain respect a total (cyclic) order, and
every element has a distinct (clockwise) predecessor. Previous work has
explored this problem through weighted first-order model counting (WFOMC),
which computes the weighted sum of models for a given first-order logic
sentence over a finite domain. In WFOMC, the order constraint is typically
encoded by the linear order axiom introducing a binary predicate in the
sentence to impose a linear ordering on the domain elements. The immediate and
second predecessor relations are then encoded by the linear order predicate.
Although WFOMC with the linear order axiom is theoretically tractable, existing
algorithms struggle with practical applications, particularly when the
predecessor relations are involved. In this paper, we treat predecessor
relations as a native part of the axiom and devise a novel algorithm that
inherently supports these relations. The proposed algorithm not only provides
an exponential speedup for the immediate and second predecessor relations,
which are known to be tractable, but also handles the general k-th predecessor
relations. The extensive experiments on lifted inference tasks and
combinatorics math problems demonstrate the efficiency of our algorithm,
achieving speedups of a full order of magnitude.

</details>


### [11] [Knowledge Grafting: A Mechanism for Optimizing AI Model Deployment in Resource-Constrained Environments](https://arxiv.org/abs/2507.19261)
*Osama Almurshed,Ashish Kaushal,Asmail Muftah,Nitin Auluck,Omer Rana*

Main category: cs.AI

TL;DR: 论文提出知识嫁接机制优化AI模型，减小模型大小同时提升性能，适用于资源受限场景。


<details>
  <summary>Details</summary>
Motivation: AI模型参数多、计算资源需求大，在现实应用场景常缺乏相应资源。

Method: 引入知识嫁接机制，将大捐赠模型的选定特征转移到小砧木模型。

Result: 模型大小减少88.54%，验证准确率达89.97%，验证损失更低，测试数据准确率90.45%。

Conclusion: 该方法解决了大小与性能的权衡问题，可在资源受限设备上部署，适用于多种边缘计算场景。

Abstract: The increasing adoption of Artificial Intelligence (AI) has led to larger,
more complex models with numerous parameters that require substantial computing
power -- resources often unavailable in many real-world application scenarios.
Our paper addresses this challenge by introducing knowledge grafting, a novel
mechanism that optimizes AI models for resource-constrained environments by
transferring selected features (the scion) from a large donor model to a
smaller rootstock model. The approach achieves an 88.54% reduction in model
size (from 64.39 MB to 7.38 MB), while improving generalization capability of
the model. Our new rootstock model achieves 89.97% validation accuracy (vs.
donor's 87.47%), maintains lower validation loss (0.2976 vs. 0.5068), and
performs exceptionally well on unseen test data with 90.45% accuracy. It
addresses the typical size vs performance trade-off, and enables deployment of
AI frameworks on resource-constrained devices with enhanced performance. We
have tested our approach on an agricultural weed detection scenario, however,
it can be extended across various edge computing scenarios, potentially
accelerating AI adoption in areas with limited hardware/software support -- by
mirroring in a similar manner the horticultural grafting enables productive
cultivation in challenging agri-based environments.

</details>


### [12] [Modeling Uncertainty: Constraint-Based Belief States in Imperfect-Information Games](https://arxiv.org/abs/2507.19263)
*Achille Morenville,Éric Piette*

Main category: cs.AI

TL;DR: 本文研究在有隐藏棋子身份的游戏中两种信念表示方法，评估后发现基于约束的信念与概率推理效果相当。


<details>
  <summary>Details</summary>
Motivation: 在不完全信息博弈中，解决基于部分信息决策的问题，用信念随机博弈模型减少对特定游戏推理逻辑的需求。

Method: 研究在有隐藏棋子身份的游戏中基于约束满足问题的约束模型和使用信念传播估计边际概率的概率扩展两种信念表示方法，并用通用智能体在两个不同游戏中评估。

Result: 基于约束的信念产生的结果与概率推理相当，智能体性能差异极小。

Conclusion: 在许多场景中，仅基于约束的信念状态可能足以进行有效决策。

Abstract: In imperfect-information games, agents must make decisions based on partial
knowledge of the game state. The Belief Stochastic Game model addresses this
challenge by delegating state estimation to the game model itself. This allows
agents to operate on externally provided belief states, thereby reducing the
need for game-specific inference logic. This paper investigates two approaches
to represent beliefs in games with hidden piece identities: a constraint-based
model using Constraint Satisfaction Problems and a probabilistic extension
using Belief Propagation to estimate marginal probabilities. We evaluated the
impact of both representations using general-purpose agents across two
different games. Our findings indicate that constraint-based beliefs yield
results comparable to those of probabilistic inference, with minimal
differences in agent performance. This suggests that constraint-based belief
states alone may suffice for effective decision-making in many settings.

</details>


### [13] [Integrating LLM in Agent-Based Social Simulation: Opportunities and Challenges](https://arxiv.org/abs/2507.19364)
*Patrick Taillandier,Jean Daniel Zucker,Arnaud Grignard,Benoit Gaudou,Nghi Quang Huynh,Alexis Drogoul*

Main category: cs.AI

TL;DR: 本文从计算社会科学视角探讨大语言模型（LLMs）在社会模拟中的应用，分析其潜力与局限，介绍相关应用，指出适用场景并提倡混合方法。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在社会模拟中的应用，明确其优势与不足，为社会模拟建模提供参考。

Method: 先回顾大语言模型复制人类认知能力的研究成果并指出局限，再调查其在多智能体模拟框架中的新兴应用，讨论相关项目，最后区分不同应用场景。

Result: 明确大语言模型在社会模拟中的潜力与局限，分析了新兴应用项目的设计、实证基础和方法创新，指出不同场景的适用性。

Conclusion: 提倡将大语言模型集成到传统基于智能体的建模平台的混合方法，结合语言推理的灵活性与经典规则系统的透明度和严谨性。

Abstract: This position paper examines the use of Large Language Models (LLMs) in
social simulation, analyzing both their potential and their limitations from a
computational social science perspective. The first part reviews recent
findings on the ability of LLMs to replicate key aspects of human cognition,
including Theory of Mind reasoning and social inference, while also
highlighting significant limitations such as cognitive biases, lack of true
understanding, and inconsistencies in behavior. The second part surveys
emerging applications of LLMs in multi-agent simulation frameworks, focusing on
system architectures, scale, and validation strategies. Notable projects such
as Generative Agents (Smallville) and AgentSociety are discussed in terms of
their design choices, empirical grounding, and methodological innovations.
Particular attention is given to the challenges of behavioral fidelity,
calibration, and reproducibility in large-scale LLM-driven simulations. The
final section distinguishes between contexts where LLMs, like other black-box
systems, offer direct value-such as interactive simulations and serious
games-and those where their use is more problematic, notably in explanatory or
predictive modeling. The paper concludes by advocating for hybrid approaches
that integrate LLMs into traditional agent-based modeling platforms (GAMA,
Netlogo, etc), enabling modelers to combine the expressive flexibility of
language-based reasoning with the transparency and analytical rigor of
classical rule-based systems.

</details>


### [14] [Learning neuro-symbolic convergent term rewriting systems](https://arxiv.org/abs/2507.19372)
*Flavio Petruzzellis,Alberto Testolin,Alessandro Sperduti*

Main category: cs.AI

TL;DR: 提出学习收敛项重写系统的通用框架，含NRS和FastNRS模型，在多任务中表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 构建能学习执行符号算法的神经网络系统，实现强泛化和分布外性能。

Method: 引入受重写算法启发的神经符号架构，给出NRS和FastNRS两种模块化实现。

Result: 两模型能泛化到分布外实例，FastNRS在内存效率等方面有显著提升，系统在多任务中表现优于多个基线模型。

Conclusion: 所提系统在学习执行符号算法上效果良好，能实现强泛化，性能超越多个基线模型。

Abstract: Building neural systems that can learn to execute symbolic algorithms is a
challenging open problem in artificial intelligence, especially when aiming for
strong generalization and out-of-distribution performance. In this work, we
introduce a general framework for learning convergent term rewriting systems
using a neuro-symbolic architecture inspired by the rewriting algorithm itself.
We present two modular implementations of such architecture: the Neural
Rewriting System (NRS) and the Fast Neural Rewriting System (FastNRS). As a
result of algorithmic-inspired design and key architectural elements, both
models can generalize to out-of-distribution instances, with FastNRS offering
significant improvements in terms of memory efficiency, training speed, and
inference time. We evaluate both architectures on four tasks involving the
simplification of mathematical formulas and further demonstrate their
versatility in a multi-domain learning scenario, where a single model is
trained to solve multiple types of problems simultaneously. The proposed system
significantly outperforms two strong neural baselines: the Neural Data Router,
a recent transformer variant specifically designed to solve algorithmic
problems, and GPT-4o, one of the most powerful general-purpose large-language
models. Moreover, our system matches or outperforms the latest o1-preview model
from OpenAI that excels in reasoning benchmarks.

</details>


### [15] [Hierarchical Deep Reinforcement Learning Framework for Multi-Year Asset Management Under Budget Constraints](https://arxiv.org/abs/2507.19458)
*Amir Fard,Arnold X. -X. Yuan*

Main category: cs.AI

TL;DR: 本文提出用于多年基础设施规划的分层深度强化学习方法，分解问题为两层规划，案例研究表明该方法比传统方法收敛快、可扩展性好且能提供近最优解。


<details>
  <summary>Details</summary>
Motivation: 现有基础设施资产预算规划和维护优化方法因组合动作空间、资产恶化多样、预算约束和环境不确定性等因素，可扩展性受限。

Method: 提出分层深度强化学习方法，将问题分解为高层预算规划器和低层维护规划器，在分层软演员 - 评论家框架中集成线性规划投影。

Result: 通过不同规模下水道网络案例研究，与传统深度Q学习和增强遗传算法相比，该方法收敛更快、可有效扩展且网络规模增大时也能提供近最优解。

Conclusion: 所提分层深度强化学习方法能有效解决基础设施规划中的问题，在可扩展性和解决方案质量上优于传统方法。

Abstract: Budget planning and maintenance optimization are crucial for infrastructure
asset management, ensuring cost-effectiveness and sustainability. However, the
complexity arising from combinatorial action spaces, diverse asset
deterioration, stringent budget constraints, and environmental uncertainty
significantly limits existing methods' scalability. This paper proposes a
Hierarchical Deep Reinforcement Learning methodology specifically tailored to
multi-year infrastructure planning. Our approach decomposes the problem into
two hierarchical levels: a high-level Budget Planner allocating annual budgets
within explicit feasibility bounds, and a low-level Maintenance Planner
prioritizing assets within the allocated budget. By structurally separating
macro-budget decisions from asset-level prioritization and integrating linear
programming projection within a hierarchical Soft Actor-Critic framework, the
method efficiently addresses exponential growth in the action space and ensures
rigorous budget compliance. A case study evaluating sewer networks of varying
sizes (10, 15, and 20 sewersheds) illustrates the effectiveness of the proposed
approach. Compared to conventional Deep Q-Learning and enhanced genetic
algorithms, our methodology converges more rapidly, scales effectively, and
consistently delivers near-optimal solutions even as network size grows.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [16] [Higher-order transmissibility and its linear approximation for in-service crack identification in train wheelset axles](https://arxiv.org/abs/2507.18636)
*Ehsan Naghizadeh,Paolo Tiso,Eleni Chatzi*

Main category: cs.CE

TL;DR: 提出基于呼吸裂纹高阶谐波的新裂纹检测特征HOTr，用线性系统理论近似该特征，结果显示近似可减少计算负担并保持高精度，有用于轮对车轴损伤识别潜力。


<details>
  <summary>Details</summary>
Motivation: 在役结构健康监测用于列车轮对车轴早期裂纹检测和识别时，纯数据驱动方法难实施，损伤为呼吸裂纹问题使模拟复杂，需新方法。

Method: 提出HOTr特征并评估其在裂纹识别中的敏感性和有效性，用线性系统理论近似该特征得到替代模型，将该方法与非线性模拟模型对比。

Result: HOTr近似可显著减少计算负担，与参考模型相比保持高精度。

Conclusion: 该方法在轮对车轴在役损伤识别中有很大潜力，可能实现近实时应用。

Abstract: In-service structural health monitoring is a so far rarely exploited, yet
potent option for early-stage crack detection and identification in train
wheelset axles. This procedure is non-trivial to enforce on the basis of a
purely data-driven approach and typically requires the adoption of numerical,
e.g. finite element-based, simulation schemes of the dynamic behavior of these
axles. Damage in this particular case can be formulated as a breathing crack
problem, which further complicates simulation by introducing response-dependent
nonlinearities into the picture. In this study, first, a new crack detection
feature based on higher-order harmonics of the breathing crack is proposed,
termed Higher-Order Transmissibility (HOTr), and, secondly, its sensitivity and
efficacy are assessed within the context of crack identification. Next, the
mentioned feature is approximated via use of linear system theory, delivering a
surrogate model which facilitates the computation and speeds up the crack
identification procedure. The accuracy of the proposed method in reproducing
the delivered HOTr is compared against the nonlinear simulation model. The
obtained results suggest that the approximation of the HOTr can significantly
reduce the computational burden by eliminating the need for an iterative
solution of the governing nonlinear equation of motion, while maintaining a
high level of accuracy when compared against the reference model. This implies
great potential for adoption in in-service damage identification for wheelset
axles, feasibly within a near real-time context.

</details>


### [17] [Learning coupled Allen-Cahn and Cahn-Hilliard phase-field equations using Physics-informed neural operator(PINO)](https://arxiv.org/abs/2507.18731)
*Gaijinliu Gangmei,Santu Rana,Bernard Rolfe,Kishalay Mitra,Saswata Bhattacharyya*

Main category: cs.CE

TL;DR: 本文提出用物理信息神经算子（PINO）预测周期性边界条件下材料微观结构演化，展示其预测Al - Cu合金中θ'析出相生长的能力，且用傅里叶导数改进了Cahn - Hilliard方程损失。


<details>
  <summary>Details</summary>
Motivation: 传统相场方程数值求解器计算成本高，需生成精细网格系统求解复杂偏微分方程，因此寻找替代方法。

Method: 使用物理信息神经算子（PINO），学习算子并同时求解三个耦合的物理方程，采用傅里叶导数替代有限差分法。

Result: PINO能预测Al - Cu合金中θ'析出相生长，使用傅里叶导数使Cahn - Hilliard方程损失提高十二个数量级，且可轻松计算Cahn - Hilliard方程的四阶导数。

Conclusion: PINO是一种有效预测周期性边界条件下微观结构演化的方法，傅里叶导数有明显优势。

Abstract: Phase-field equations, mostly solved numerically, are known for capturing the
mesoscale microstructural evolution of a material. However, such numerical
solvers are computationally expensive as it needs to generate fine mesh systems
to solve the complex Partial Differential Equations(PDEs) with good accuracy.
Therefore, we propose an alternative approach of predicting the microstructural
evolution subjected to periodic boundary conditions using Physics informed
Neural Operators (PINOs).
  In this study, we have demonstrated the capability of PINO to predict the
growth of $\theta^{\prime}$ precipitates in Al-Cu alloys by learning the
operator as well as by solving three coupled physics equations simultaneously.
The coupling is of two second-order Allen-Cahn equation and one fourth-order
Cahn-Hilliard equation. We also found that using Fourier
derivatives(pseudo-spectral method and Fourier extension) instead of Finite
Difference Method improved the Cahn-Hilliard equation loss by twelve orders of
magnitude. Moreover, since differentiation is equivalent to multiplication in
the Fourier domain, unlike Physics informed Neural Networks(PINNs), we can
easily compute the fourth derivative of Cahn-Hilliard equation without
converting it to coupled second order derivative.

</details>


### [18] [ThermoRL:Structure-Aware Reinforcement Learning for Protein Mutation Design to Enhance Thermostability](https://arxiv.org/abs/2507.18816)
*Xiangwen Wang,Gaojie Jin,Xiaowei Huang,Ronghui Mu*

Main category: cs.CE

TL;DR: 提出基于强化学习的ThermoRL框架设计提高蛋白质热稳定性的突变，实验效果好且有强泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质热稳定性突变设计方法因依赖实验或预定义数据集，缺乏迭代优化，限制了设计空间探索。

Method: 提出ThermoRL框架，结合预训练GNN编码器和分层Q学习网络，用替代模型进行奖励反馈。

Result: ThermoRL比基线有更高或相当的奖励，过滤掉不稳定突变，识别稳定突变，能准确检测未见蛋白质关键突变位点。

Conclusion: 基于GNN嵌入的强化学习方法为传统蛋白质突变设计提供了可靠替代方案。

Abstract: Designing mutations to optimize protein thermostability remains challenging
due to the complex relationship between sequence variations, structural
dynamics, and thermostability, often assessed by \delta\delta G
  (the change in free energy of unfolding). Existing methods rely on
experimental random mutagenesis or prediction models tested with pre-defined
datasets, using sequence-based heuristics and treating enzyme design as a
one-step process without iterative refinement, which limits design space
exploration and restricts discoveries beyond known variations. We present
ThermoRL, a framework based on reinforcement learning (RL) that leverages graph
neural networks (GNN) to design mutations with enhanced thermostability. It
combines a pre-trained GNN-based encoder with a hierarchical Q-learning network
and employs a surrogate model for reward feedback, guiding the RL agent on
where (the position) and which (mutant amino acid) to apply for enhanced
thermostability. Experimental results show that ThermoRL achieves higher or
comparable rewards than baselines while maintaining computational efficiency.
It filters out destabilizing mutations and identifies stabilizing mutations
aligned with experimental data. Moreover, ThermoRL accurately detects key
mutation sites in unseen proteins, highlighting its strong generalizability.
This RL-guided approach powered by GNN embeddings offers a robust alternative
to traditional protein mutation design.

</details>


### [19] [TrinityDNA: A Bio-Inspired Foundational Model for Efficient Long-Sequence DNA Modeling](https://arxiv.org/abs/2507.19229)
*Qirong Yang,Yucheng Guo,Zicheng Liu,Yujie Yang,Qijin Yin,Siyuan Li,Shaomin Ji,Linlin Chao,Xiaoming Zhang,Stan Z. Li*

Main category: cs.CE

TL;DR: 提出TrinityDNA新型DNA基础模型解决基因组序列建模难题，有多种创新机制，在基因组学应用有改进，还引入新基准。


<details>
  <summary>Details</summary>
Motivation: 传统序列模型难以捕捉DNA长程依赖和生物特征，基因组序列建模有挑战。

Method: 提出TrinityDNA模型，集成Groove Fusion、Gated Reverse Complement，引入多尺度注意力机制和进化训练策略。

Result: 在基因功能预测、调控机制发现等基因组学应用有显著改进。

Conclusion: TrinityDNA模型弥合机器学习与生物学见解差距，为基因组数据分析提供有效方法，新基准使评估更全面实用。

Abstract: The modeling of genomic sequences presents unique challenges due to their
length and structural complexity. Traditional sequence models struggle to
capture long-range dependencies and biological features inherent in DNA. In
this work, we propose TrinityDNA, a novel DNA foundational model designed to
address these challenges. The model integrates biologically informed
components, including Groove Fusion for capturing DNA's structural features and
Gated Reverse Complement (GRC) to handle the inherent symmetry of DNA
sequences. Additionally, we introduce a multi-scale attention mechanism that
allows the model to attend to varying levels of sequence dependencies, and an
evolutionary training strategy that progressively adapts the model to both
prokaryotic and eukaryotic genomes. TrinityDNA provides a more accurate and
efficient approach to genomic sequence modeling, offering significant
improvements in gene function prediction, regulatory mechanism discovery, and
other genomics applications. Our model bridges the gap between machine learning
techniques and biological insights, paving the way for more effective analysis
of genomic data. Additionally, we introduced a new DNA long-sequence CDS
annotation benchmark to make evaluations more comprehensive and oriented toward
practical applications.

</details>


### [20] [Multi-Level Monte Carlo sampling with Parallel-in-Time Integration for Uncertainty Quantification in Electric Machine Simulation](https://arxiv.org/abs/2507.19246)
*Robert Hahn,Sebastian Schöps*

Main category: cs.CE

TL;DR: 本文提出结合多级蒙特卡罗采样和时间并行积分的不确定性量化方法，研究了时间和计算量的权衡，通过数值例子验证有加速效果但计算量增加。


<details>
  <summary>Details</summary>
Motivation: 传统蒙特卡罗方法计算成本高，多级蒙特卡罗方法在并行计算环境中无法充分减少求解时间，需新方法加速计算。

Method: 提出结合多级蒙特卡罗采样和时间并行积分的不确定性量化方法，并研究时间和计算量的权衡。

Result: 相比多级蒙特卡罗采样，该方法有12 - 45%的加速，但计算量增加15 - 18%。

Conclusion: 结合方法能有效减少求解时间，但会增加总计算量。

Abstract: While generally considered computationally expensive, Uncertainty
Quantification using Monte Carlo sampling remains beneficial for applications
with uncertainties of high dimension. As an extension of the naive Monte Carlo
method, the Multi-Level Monte Carlo method reduces the overall computational
effort, but is unable to reduce the time to solution in a sufficiently parallel
computing environment. In this work, we propose a Uncertainty Quantification
method combining Multi-Level Monte Carlo sampling and Parallel-in-Time
integration for select samples, exploiting remaining parallel computing
capacity to accelerate the computation. While effective at reducing the
time-to-solution, Parallel-in-Time integration methods greatly increase the
total computational effort. We investigate the tradeoff between
time-to-solution and total computational effort of the combined method,
starting from theoretical considerations and comparing our findings to two
numerical examples. There, a speedup of 12 - 45% compared to Multi-Level Monte
Carlo sampling is observed, with an increase of 15 - 18% in computational
effort.

</details>


### [21] [Learning electromagnetic fields based on finite element basis functions](https://arxiv.org/abs/2507.19255)
*Merle Backmeyer,Michael Wiesheu,Sebastian Schöps*

Main category: cs.CE

TL;DR: 提出结合等几何分析、本征正交分解和深度学习的新方法用于电机参数代理模型，以永磁同步电机模型验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决电机参数代理模型中应对几何变化的问题，实现快速且符合物理规律的预测。

Method: 结合等几何分析、本征正交分解和深度学习，直接学习样条基系数。

Result: 用永磁同步电机的参数非线性静磁模型证明了方法的有效性。

Conclusion: 所提出的方法能实现电机参数代理模型的快速且物理一致的预测。

Abstract: Parametric surrogate models of electric machines are widely used for
efficient design optimization and operational monitoring. Addressing geometry
variations, spline-based computer-aided design representations play a pivotal
role. In this study, we propose a novel approach that combines isogeometric
analysis, proper orthogonal decomposition and deep learning to enable rapid and
physically consistent predictions by directly learning spline basis
coefficients. The effectiveness of this method is demonstrated using a
parametric nonlinear magnetostatic model of a permanent magnet synchronous
machine.

</details>


### [22] [A novel multi-thickness topology optimization method for balancing structural performance and manufacturability](https://arxiv.org/abs/2507.19388)
*Gabriel Stankiewicz,Chaitanya Dev,Paul Steinmann*

Main category: cs.CE

TL;DR: 提出多厚度、基于密度的拓扑优化方法，在标准算例验证，设计可兼顾性能与可制造性。


<details>
  <summary>Details</summary>
Motivation: 二维拓扑优化在结构性能和可制造性间存在权衡，现有方法各有优劣，需新方法弥合差距。

Method: 采用新型多级惩罚方案和多级平滑Heaviside投影，引导设计达到预定义离散厚度；结合惩罚和投影参数的延续策略与自适应网格细化技术。

Result: 随着允许厚度数量增加，设计从桁架结构过渡到高性能薄板结构；三个离散厚度水平设计的柔度值与无惩罚优化结果相差2%以内，远超标准SIMP结果。

Conclusion: 该方法消除不合理的薄区域，设计适用于增材制造和传统制造，兼顾性能和可制造性。

Abstract: Topology optimization (TO) in two dimensions often presents a trade-off
between structural performance and manufacturability, with unpenalized
(variable-thickness) methods yielding superior but complex designs, and
penalized (SIMP) methods producing simpler, truss-like structures with
compromised performance. This paper introduces a multi-thickness, density-based
topology optimization method designed to bridge this gap. The proposed approach
guides the design towards a predefined set of discrete, allowable thicknesses
by employing a novel multilevel penalization scheme and a multilevel smoothed
Heaviside projection. A continuation strategy for the penalization and
projection parameters, combined with an adaptive mesh refinement technique,
ensures robust convergence and high-resolution geometric features. The method
is validated on standard cantilever and MBB beam benchmarks. Results
demonstrate that as the number of allowable thicknesses increases, the designs
systematically transition from conventional truss-like structures to
high-performance, sheet-like structures. Notably, designs with as few as three
discrete thickness levels achieve compliance values within 2\% of those from
fully unpenalized, variable-thickness optimization, while significantly
outperforming standard SIMP results. The method inherently eliminates
impractically thin regions and features, both in the out-of-plane and in-plane
directions and produces designs well-suited for both additive manufacturing and
conventional fabrication using standard-thickness stock materials, thus
maximizing both performance and manufacturability.

</details>


### [23] [Human-AI Synergy in Adaptive Active Learning for Continuous Lithium Carbonate Crystallization Optimization](https://arxiv.org/abs/2507.19316)
*Shayan S. Mousavi Masouleh,Corey A. Sanz,Ryan P. Jansonius,Cara Cronin,Jason E. Hein,Jason Hattrick-Simpers*

Main category: cs.CE

TL;DR: 本文引入人在环辅助主动学习框架优化碳酸锂连续结晶，提高对杂质耐受性，使低品位锂资源开发可行，助力北美锂资源利用和全球锂供应链可持续性。


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车行业对高纯度锂需求激增，需从北美低品位锂资源中经济高效提取锂，连续结晶法优化面临挑战。

Method: 引入人在环（HITL）辅助主动学习框架优化碳酸锂连续结晶，结合人类专业知识与数据驱动见解。

Result: 框架能快速适应新数据，将对关键杂质镁的耐受性从几百ppm提升到6000 ppm。

Conclusion: 该进展是经济利用北美大量锂储备的重要一步，可增强全球锂供应链的可持续性。

Abstract: As demand for high-purity lithium surges with the growth of the electric
vehicle (EV) industry, cost-effective extraction from lower-grade North
American sources like the Smackover Formation is critical. These resources,
unlike high-purity South American brines, require innovative purification
techniques to be economically viable. Continuous crystallization is a promising
method for producing battery-grade lithium carbonate, but its optimization is
challenged by a complex parameter space and limited data. This study introduces
a Human-in-the-Loop (HITL) assisted active learning framework to optimize the
continuous crystallization of lithium carbonate. By integrating human expertise
with data-driven insights, our approach accelerates the optimization of lithium
extraction from challenging sources. Our results demonstrate the framework's
ability to rapidly adapt to new data, significantly improving the process's
tolerance to critical impurities like magnesium from the industry standard of a
few hundred ppm to as high as 6000 ppm. This breakthrough makes the
exploitation of low-grade, impurity-rich lithium resources feasible,
potentially reducing the need for extensive pre-refinement processes. By
leveraging artificial intelligence, we have refined operational parameters and
demonstrated that lower-grade materials can be used without sacrificing product
quality. This advancement is a significant step towards economically harnessing
North America's vast lithium reserves, such as those in the Smackover
Formation, and enhancing the sustainability of the global lithium supply chain.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [24] [ApproxJoin: Approximate Matching for Efficient Verification in Fuzzy Set Similarity Join](https://arxiv.org/abs/2507.18891)
*Michael Mandulak,S M Ferdous,Sayan Ghosh,Mahantesh Halappanavar,George Slota*

Main category: cs.DB

TL;DR: 提出ApproxJoin方法用于模糊集相似度连接验证，测试三种近似匹配方法，性能比现有方法高2 - 19倍且召回率达99%。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过过滤框架提高性能以抵消匈牙利算法带来的高验证成本，本文想直接针对验证过程，评估更高效匹配方法在候选对剪枝中的效果。

Method: 提出ApproxJoin，应用近似最大权重匹配算法进行模糊集相似度连接验证，测试Greedy、Locally Dominant和Paz Schwartzman三种近似匹配方法并与精确匹配的现有方法对比。

Result: ApproxJoin比现有方法性能提升2 - 19倍，且有99%的高召回率。

Conclusion: ApproxJoin在模糊集相似度连接验证中能有效提升性能并保证较高准确性。

Abstract: The set similarity join problem is a fundamental problem in data processing
and discovery, relying on exact similarity measures between sets. In the
presence of alterations, such as misspellings on string data, the fuzzy set
similarity join problem instead approximately matches pairs of elements based
on the maximum weighted matching of the bipartite graph representation of sets.
State-of-the-art methods within this domain improve performance through
efficient filtering methods within the filter-verify framework, primarily to
offset high verification costs induced by the usage of the Hungarian algorithm
- an optimal matching method. Instead, we directly target the verification
process to assess the efficacy of more efficient matching methods within
candidate pair pruning.
  We present ApproxJoin, the first work of its kind in applying approximate
maximum weight matching algorithms for computationally expensive fuzzy set
similarity join verification. We comprehensively test the performance of three
approximate matching methods: the Greedy, Locally Dominant and Paz Schwartzman
methods, and compare with the state-of-the-art approach using exact matching.
Our experimental results show that ApproxJoin yields performance improvements
of 2-19x the state-of-the-art with high accuracy (99% recall).

</details>


### [25] [Big Data Energy Systems: A Survey of Practices and Associated Challenges](https://arxiv.org/abs/2507.19154)
*Lunodzo J. Mwinuka,Massimo Cafaro,Lucas Pereira,Hugo Morais*

Main category: cs.DB

TL;DR: 本文探讨能源系统大数据管理研究趋势，指出传统方法问题，介绍先进方案瓶颈，提供新兴技术见解与实用建议。


<details>
  <summary>Details</summary>
Motivation: 能源系统数据量大且生成快，传统数据管理方法有可扩展性和可访问性问题，先进方案也有瓶颈，需探索新管理方案。

Method: 对能源系统大数据管理相关研究进行综述，用参考架构突出数据监管需求。

Result: 分析了当前存储和数据集成解决方案的局限性，介绍新兴技术如数据空间、多种数据管理架构等。

Conclusion: 给出新兴技术见解及实现数据共享和监管合规的实用建议。

Abstract: Energy systems generate vast amounts of data in extremely short time
intervals, creating challenges for efficient data management. Traditional data
management methods often struggle with scalability and accessibility, limiting
their usefulness. More advanced solutions, such as NoSQL databases and
cloud-based platforms, have been adopted to address these issues. Still, even
these advanced solutions can encounter bottlenecks, which can impact the
efficiency of data storage, retrieval, and analysis. This review paper explores
the research trends in big data management for energy systems, highlighting the
practices, opportunities and challenges. Also, the data regulatory demands are
highlighted using chosen reference architectures. The review, in particular,
explores the limitations of current storage and data integration solutions and
examines how new technologies are applied to the energy sector. Novel insights
into emerging technologies, including data spaces, various data management
architectures, peer-to-peer data management, and blockchains, are provided,
along with practical recommendations for achieving enhanced data sharing and
regulatory compliance.

</details>


### [26] [DBMS-LLM Integration Strategies in Industrial and Business Applications: Current Status and Future Challenges](https://arxiv.org/abs/2507.19254)
*Zhengtong Yan,Gongsheng Yuan,Qingsong Guo,Jiaheng Lu*

Main category: cs.DB

TL;DR: 本文调研DBMS与LLM集成的进展，分类五种架构模式并指出关键挑战，助于理解集成现状与待解决问题。


<details>
  <summary>Details</summary>
Motivation: 现代企业受DATA+AI范式驱动，DBMS与LLM集成有机会但也有新挑战，需系统了解现状与解决未决问题。

Method: 对DBMS与LLM集成进行调研，根据核心设计原则、优势和权衡分类五种代表性架构模式。

Result: 确定了五种代表性架构模式，突出了几个关键的开放性挑战。

Conclusion: 旨在为当前集成现状提供系统理解，明确实现未来智能应用中传统数据管理与高级语言推理可扩展高效集成的未决问题。

Abstract: Modern enterprises are increasingly driven by the DATA+AI paradigm, in which
Database Management Systems (DBMSs) and Large Language Models (LLMs) have
become two foundational infrastructures powering a wide range of industrial and
business applications, such as enterprise analytics, intelligent customer
service, and data-driven decision-making. The efficient integration of DBMSs
and LLMs within a unified system offers significant opportunities but also
introduces new technical challenges. This paper surveys recent developments in
DBMS+LLM integration and identifies key future challenges. Specifically, we
categorize five representative architectural patterns based on their core
design principles, strengths, and trade-offs. Based on this analysis, we
further highlight several critical open challenges. We aim to provide a
systematic understanding of the current integration landscape and to outline
the unresolved issues that must be addressed to achieve scalable and efficient
integration of traditional data management and advanced language reasoning in
future intelligent applications.

</details>


### [27] [Properties for Paths in Graph Databases](https://arxiv.org/abs/2507.19329)
*Fernando Orejas,Elvira Pino,Renzo Angles,E. Pasarella,Nikos Milonakis*

Main category: cs.DB

TL;DR: 本文提出图数据库路径属性形式化方法，可限制导航查询结果数量，证明其合理性与完备性，分析表达能力并进行复杂度讨论与实证分析，结果显示使用路径属性过滤的查询性能更优。


<details>
  <summary>Details</summary>
Motivation: 定义图数据库路径属性以限制导航查询结果数量、过滤查询、识别和移除不良路径。

Method: 为查询语言定义操作语义，证明与简单逻辑语义的兼容性，分析表达能力，讨论复杂度并进行实证分析。

Result: 使用路径属性作为过滤器的查询性能优于不使用的标准查询。

Conclusion: 所提出的图数据库路径属性形式化方法有效，能提升查询性能。

Abstract: This paper presents a formalism for defining properties of paths in graph
databases, which can be used to restrict the number of solutions to
navigational queries. In particular, our formalism allows us to define
quantitative properties such as length or accumulated cost, which can be used
as query filters. Furthermore, it enables the identification and removal of
paths that may be considered ill-formed.
  The new formalism is defined in terms of an operational semantics for the
query language that incorporates these new constructs, demonstrating its
soundness and completeness by proving its compatibility with a simple logical
semantics. We also analyze its expressive power, showing that path properties
are more expressive than register automata. Finally, after discussing some
complexity issues related to this new approach, we present an empirical
analysis carried out using our prototype implementation of the graph database
that serves as a running example throughout the paper. The results show that
queries using path properties as filters outperform standard queries that do
not use them.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [28] [CUTHERMO: Understanding GPU Memory Inefficiencies with Heat Map Profiling](https://arxiv.org/abs/2507.18729)
*Yanbo Zhao,Jinku Cui,Zecheng Li,Shuyin Jiao,Xu Liu,Jiajia Li*

Main category: cs.DC

TL;DR: 本文介绍了轻量级GPU内存分析工具cuThermo，可在运行时识别内存低效问题并提供优化指导，实验显示有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: GPU架构缺乏全面的运行时和细粒度内存分析支持，为高效利用GPU内存子系统，需有效分析内存访问模式。

Method: 引入cuThermo工具，在GPU二进制文件上运行，通过基于不同访问线程束计数的热图识别内存低效问题。

Result: 通过六个应用实验，确定五种跨不同GPU架构的内存访问模式，在两个GPU上评估优化，性能提升高达721.79%。

Conclusion: cuThermo是一个实用的GPU内存分析工具，能有效识别内存问题并带来显著性能提升。

Abstract: GPUs have become indispensable in high-performance computing, machine
learning, and many other domains. Efficiently utilizing the memory subsystem on
GPUs is critical for maximizing computing power through massive parallelism.
Analyzing memory access patterns has proven to be an effective method for
understanding memory bottlenecks in applications. However, comprehensive
runtime and fine-grained memory profiling support is lacking on GPU
architectures. In this work, we introduce cuThermo, a lightweight and practical
profiling tool for GPU memory analysis. It operates on GPU binaries without
requiring any modifications to hardware, operating system, or application
source code. Given a CUDA application, cuThermo identifies memory
inefficiencies at runtime via a heat map based on distinct visited warp counts
to represent word-sector-level data sharing and provides optimization guidance
in performance tuning iterations. Through our experiments on six applications,
we identified five memory access patterns that are portable across different
GPU architectures. By evaluating optimization on two GPUs, cuThermo achieves up
to $721.79\%$ performance improvement.

</details>


### [29] [PPipe: Efficient Video Analytics Serving on Heterogeneous GPU Clusters via Pool-Based Pipeline Parallelism](https://arxiv.org/abs/2507.18748)
*Z. Jonny Kong,Qiang Xu,Y. Charlie Hu*

Main category: cs.DC

TL;DR: 本文展示了管道并行技术可用于异构GPU集群上低延迟模型推理，提出推理服务系统PPipe，评估显示其有更好性能。


<details>
  <summary>Details</summary>
Motivation: 随着GPU快速创新，异构GPU集群普及，需要有效方法用于低延迟模型推理。

Method: 利用模型层和GPU架构多样性，提出PPipe系统，采用基于MILP的控制平面和基于资源预留的自适应批处理的数据平面。

Result: 在18个CNN模型上，PPipe使低级别GPU利用率提高41.1% - 65.5%，保持高级别GPU高利用率，服务吞吐量比基线高32.2% - 75.1%。

Conclusion: 管道并行技术能有效用于异构GPU集群上的低延迟模型推理，PPipe系统性能优越。

Abstract: With the rapid innovation of GPUs, heterogeneous GPU clusters in both public
clouds and on-premise data centers have become increasingly commonplace. In
this paper, we demonstrate how pipeline parallelism, a technique wellstudied
for throughput-oriented deep learning model training, can be used effectively
for serving latency-bound model inference, e.g., in video analytics systems, on
heterogeneous GPU clusters. Our work exploits the synergy between diversity in
model layers and diversity in GPU architectures, which results in comparable
inference latency for many layers when running on low-class and high-class
GPUs. We explore how such overlooked capability of low-class GPUs can be
exploited using pipeline parallelism and present a novel inference serving
system, PPipe, that employs pool-based pipeline parallelism via an MILP-based
control plane and a data plane that performs resource reservation-based
adaptive batching. Evaluation results on diverse workloads (18 CNN models) show
that PPipe achieves 41.1% - 65.5% higher utilization of low-class GPUs while
maintaining high utilization of high-class GPUs, leading to 32.2% - 75.1%
higher serving throughput compared to various baselines.

</details>


### [30] [Deadline-Aware Joint Task Scheduling and Offloading in Mobile Edge Computing Systems](https://arxiv.org/abs/2507.18864)
*Ngoc Hung Nguyen,Van-Dinh Nguyen,Anh Tuan Nguyen,Nguyen Van Thieu,Hoang Nam Nguyen,Symeon Chatzinotas*

Main category: cs.DC

TL;DR: 本文提出优化任务调度算法，有线性对数时间复杂度，还开发在线方法，数值结果证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 移动边缘计算和云系统需严格交互服务质量，现有方法在搜索时间和调度效率上有挑战。

Method: 提出最优作业调度算法确定任务顺序，让用户根据服务器信息卸载任务，还开发有快速中断检测的在线方法。

Result: 算法有线性对数时间复杂度O(nlogn)，在线方法时间复杂度O(n)，数值结果显示在服务比率和调度成本上有效。

Conclusion: 所提算法在任务调度上有优化效果，复杂度低且有效。

Abstract: The demand for stringent interactive quality-of-service has intensified in
both mobile edge computing (MEC) and cloud systems, driven by the imperative to
improve user experiences. As a result, the processing of computation-intensive
tasks in these systems necessitates adherence to specific deadlines or
achieving extremely low latency. To optimize task scheduling performance,
existing research has mainly focused on reducing the number of late jobs whose
deadlines are not met. However, the primary challenge with these methods lies
in the total search time and scheduling efficiency. In this paper, we present
the optimal job scheduling algorithm designed to determine the optimal task
order for a given set of tasks. In addition, users are enabled to make informed
decisions for offloading tasks based on the information provided by servers.
The details of performance analysis are provided to show its optimality and low
complexity with the linearithmic time O(nlogn), where $n$ is the number of
tasks. To tackle the uncertainty of the randomly arriving tasks, we further
develop an online approach with fast outage detection that achieves rapid
acceptance times with time complexity of O(n). Extensive numerical results are
provided to demonstrate the effectiveness of the proposed algorithm in terms of
the service ratio and scheduling cost.

</details>


### [31] [GPUnion: Autonomous GPU Sharing on Campus](https://arxiv.org/abs/2507.18928)
*Yufang Li,Yuanbo Zhang,Hanlong Liao,Guoming Tang,Deke Guo*

Main category: cs.DC

TL;DR: 提出校园级GPU共享平台GPUnion，通过三种核心机制提升资源利用率，证明提供者自主性与平台可靠性可共存。


<details>
  <summary>Details</summary>
Motivation: 校园GPU资源分配不均，现有共享平台与学术资源自主自愿的性质冲突。

Method: 采用基于容器的任务调度和执行、资源提供者优先架构、具备自动检查点和迁移的弹性执行三种核心机制，支持自定义数据存储，集成非根执行和镜像认证。

Result: 多个校园场景案例显示，GPU利用率提升30%，交互式会话增加40%，提供者离开时工作负载迁移成功率达94%。

Conclusion: 提供者自主性和平台可靠性可以共存，挑战传统集中式模式，使校园网络内计算资源获取更公平。

Abstract: A pronounced imbalance in GPU resources exists on campus, where some
laboratories own underutilized servers while others lack the compute needed for
AI research. GPU sharing can alleviate this disparity, while existing platforms
typically rely on centralized oversight and persistent allocation models,
conflicting with the voluntary and autonomous nature of academic resource
ownership. We present GPUnion, a campus-scale GPU sharing platform enabling
voluntary participation while preserving full provider autonomy. GPUnion
incorporates three core mechanisms: i) container-based task dispatching and
execution, ii) resource provider-first architecture, and iii) resilient
execution featuring automatic checkpointing and migration. GPUnion also
supports custom data storage and integrates the non-root execution and image
attestation for isolation and security improvement for containerization. Case
studies across multiple campus scenarios demonstrate 30% more GPU utilization
improvement, 40% increase in interactive sessions, and 94% successful workload
migration during provider departures. GPUnion demonstrates that provider
autonomy and platform reliability can coexist, challenging conventional
centralized paradigms and democratizing access to computational resources
within campus networks.

</details>


### [32] [The Case for Time-Shared Computing Resources](https://arxiv.org/abs/2507.19287)
*Pierre Jacquet,Adrien Luxey-Bitri*

Main category: cs.DC

TL;DR: 信息通信技术环境影响增大，需更严格管理资源。租户不共享计算资源，本文倡导改善租户间资源共享，提升基础设施共用性，还回顾现状、明确挑战机遇并给出研究方向。


<details>
  <summary>Details</summary>
Motivation: 信息通信技术环境影响因使用增加等因素持续增长，且受资源限制，需更严格管理服务，而当前租户不共享资源。

Method: 提出超越硬件层面分时共享，在更高抽象层面改善租户间资源共享，‘以更少资源在降低性能条件下做事’。

Result: 提升基础设施共用性可通过整合减少集群规模、提高能源效率，且性能权衡情况可能比取消服务更易被社会接受。

Conclusion: 对分时计算进行解读，明确当前研究现状、挑战和机遇，给出关键研究方向。

Abstract: The environmental impact of Information and Communication Technologies (ICT)
continues to grow, driven notably by increasing usage, rebound effects, and
emerging demands. However, despite the virtual nature of its services, the
sector remains inherently constrained by its materiality and cannot rely on an
infinite pool of resources. As a result, the wide variety of supported services
may need to be managed under stricter limits within hosting facilities in the
future. Contrary to common assumptions, we show that tenants typically do not
share computing resources, even in environments commonly perceived as
mutualized, such as cloud platforms. Time-sharing has been progressively phased
out for reasons of performance, security, predictability, and, perhaps more
importantly, due to the decreasing cost of computing resources. This paper
advocates for managing fewer physical resources by improving resource sharing
between tenants. It represents a paradigm shift, moving beyond traditional
time-sharing at the hardware level to a higher abstraction. This approach
entails "doing with fewer resources" under conditions of "reduced performance".
Nonetheless, enhancing the mutualization of infrastructure can reduce cluster
sizes (through consolidation) and improve energy efficiency, with gains related
to the accepted performance trade-off, a situation potentially more socially
acceptable than eliminating services. We review the current state of the art,
identify challenges and opportunities, propose interpretations of Time-Shared
Computing, and outline key research directions.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [33] [A Truly Subcubic Combinatorial Algorithm for Induced 4-Cycle Detection](https://arxiv.org/abs/2507.18845)
*Amir Abboud,Shyan Akmal,Nick Fischer*

Main category: cs.DS

TL;DR: 提出首个真正次立方组合算法检测图中诱导4 - 环，运行时间$O(n^{2.84})$，否定诱导4 - 环检测是否为三角形困难问题。


<details>
  <summary>Details</summary>
Motivation: 确定诱导$H$检测的精确时间复杂度，解决诱导4 - 环检测是否为三角形困难这一遗留问题。

Method: 采用图分解技术，与以往子图检测算法技术不同。

Result: 得到运行时间为$O(n^{2.84})$的算法，给出首个非平凡确定性算法。

Conclusion: 将诱导4 - 环检测与三角形检测分离，否定诱导4 - 环检测是三角形困难问题，组合算法有其他益处。

Abstract: We present the first truly subcubic, combinatorial algorithm for detecting an
induced $4$-cycle in a graph. The running time is $O(n^{2.84})$ on $n$-node
graphs, thus separating the task of detecting induced $4$-cycles from detecting
triangles, which requires $n^{3-o(1)}$ time combinatorially under the popular
BMM hypothesis.
  Significant work has gone into characterizing the exact time complexity of
induced $H$-detection, relative to the complexity of detecting cliques of
various sizes. Prior work identified the question of whether induced $4$-cycle
detection is triangle-hard as the only remaining case towards completing the
lowest level of the classification, dubbing it a "curious" case [Dalirrooyfard,
Vassilevska W., FOCS 2022]. Our result can be seen as a negative resolution of
this question.
  Our algorithm deviates from previous techniques in the large body of subgraph
detection algorithms and employs the trendy topic of graph decomposition that
has hitherto been restricted to more global problems (as in the use of expander
decompositions for flow problems) or to shaving subpolynomial factors (as in
the application of graph regularity lemmas). While our algorithm is slower than
the (non-combinatorial) state-of-the-art $\tilde{O}(n^{\omega})$-time algorithm
based on polynomial identity testing [Vassilevska W., Wang, Williams, Yu, SODA
2014], combinatorial advancements often come with other benefits. In
particular, we give the first nontrivial deterministic algorithm for detecting
induced $4$-cycles.

</details>


### [34] [String Consensus Problems with Swaps and Substitutions](https://arxiv.org/abs/2507.19139)
*Estéban Gabory,Laurent Bulteau,Gabriele Fici,Hilde Verbeek*

Main category: cs.DS

TL;DR: 本文研究字符串共识问题的推广，证明广义问题关于参数d是FPT，并为最小化输出字符串到所有输入字符串距离之和的变体给出多项式时间算法。


<details>
  <summary>Details</summary>
Motivation: 研究在允许相邻字符交换的情况下，字符串共识问题的复杂度和求解算法。

Method: 分析广义问题的复杂度，证明其关于参数d是FPT；针对最小化距离和的变体设计多项式时间算法。

Result: 证明广义问题是FPT；为变体问题给出多项式时间算法。

Conclusion: 在允许相邻字符交换的字符串共识问题研究上取得进展，确定复杂度并给出有效算法。

Abstract: String consensus problems aim at finding a string that minimizes some given
distance with respect to an input set of strings. In particular, in the
\textsc{Closest String} problem, we are given a set of strings of equal length
and a radius $d$. The objective is to find a new string that differs from each
input string by at most $d$ substitutions. We study a generalization of this
problem where, in addition to substitutions, swaps of adjacent characters are
also permitted, each operation incurring a unit cost. Amir et al. showed that
this generalized problem is NP-hard, even when only swaps are allowed. In this
paper, we show that it is FPT with respect to the parameter $d$. Moreover, we
investigate a variant in which the goal is to minimize the sum of distances
from the output string to all input strings. For this version, we present a
polynomial-time algorithm.

</details>


### [35] [Budget and Profit Approximations for Spanning Tree Interdiction](https://arxiv.org/abs/2507.19178)
*Rafail Ostrovsky,Yuval Rabani,Yoav Siman Tov*

Main category: cs.DS

TL;DR: 本文为最小生成树阻断问题的预算最小化和利润最大化版本提供了多项式时间对数近似保证。


<details>
  <summary>Details</summary>
Motivation: 研究最小化增加最小生成树权重成本的问题，现有近似保证不适用于成本增加问题。

Method: 证明最小化增加最小生成树权重的版本可在多项式时间内求解，给出高效算法，基于此得到图论松弛，设计并分析基于批量贪心的算法。

Result: 给出了预算最小化和利润最大化版本的多项式时间对数近似保证。

Conclusion: 通过图论松弛和批量贪心算法解决了最小生成树阻断问题的近似求解。

Abstract: We give polynomial time logarithmic approximation guarantees for the budget
minimization, as well as for the profit maximization versions of minimum
spanning tree interdiction. In this problem, the goal is to remove some edges
of an undirected graph with edge weights and edge costs, so as to increase the
weight of a minimum spanning tree. In the budget minimization version, the goal
is to minimize the total cost of the removed edges, while achieving a desired
increase $\Delta$ in the weight of the minimum spanning tree. An alternative
objective within the same framework is to maximize the profit of interdiction,
namely the increase in the weight of the minimum spanning tree, subject to a
budget constraint. There are known polynomial time $O(1)$ approximation
guarantees for a similar objective (maximizing the total cost of the tree,
rather than the increase). However, the guarantee does not seem to apply to the
increase in cost. Moreover, the same techniques do not seem to apply to the
budget version.
  Our approximation guarantees are motivated by studying the question of
minimizing the cost of increasing the minimum spanning tree by any amount. We
show that in contrast to the budget and profit problems, this version of
interdiction is polynomial time-solvable, and we give an efficient algorithm
for solving it. The solution motivates a graph-theoretic relaxation of the
NP-hard interdiction problem. The gain in minimum spanning tree weight, as a
function of the set of removed edges, is super-modular. Thus, the budget
problem is an instance of minimizing a linear function subject to a
super-modular covering constraint. We use the graph-theoretic relaxation to
design and analyze a batch greedy-based algorithm.

</details>


### [36] [Query Efficient Structured Matrix Learning](https://arxiv.org/abs/2507.19290)
*Noah Amsel,Pratyush Avi,Tyler Chen,Feyza Duman Keles,Chinmay Hegde,Cameron Musco,Christopher Musco,David Persson*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the problem of learning a structured approximation (low-rank,
sparse, banded, etc.) to an unknown matrix $A$ given access to matrix-vector
product (matvec) queries of the form $x \rightarrow Ax$ and $x \rightarrow
A^Tx$. This problem is of central importance to algorithms across scientific
computing and machine learning, with applications to fast multiplication and
inversion for structured matrices, building preconditioners for first-order
optimization, and as a model for differential operator learning. Prior work
focuses on obtaining query complexity upper and lower bounds for learning
specific structured matrix families that commonly arise in applications.
  We initiate the study of the problem in greater generality, aiming to
understand the query complexity of learning approximations from general matrix
families. Our main result focuses on finding a near-optimal approximation to
$A$ from any finite-sized family of matrices, $\mathcal{F}$. Standard results
from matrix sketching show that $O(\log|\mathcal{F}|)$ matvec queries suffice
in this setting. This bound can also be achieved, and is optimal, for
vector-matrix-vector queries of the form $x,y\rightarrow x^TAy$, which have
been widely studied in work on rank-$1$ matrix sensing.
  Surprisingly, we show that, in the matvec model, it is possible to obtain a
nearly quadratic improvement in complexity, to
$\tilde{O}(\sqrt{\log|\mathcal{F}|})$. Further, we prove that this bound is
tight up to log-log factors.Via covering number arguments, our result extends
to well-studied infinite families. As an example, we establish that a
near-optimal approximation from any \emph{linear matrix family} of dimension
$q$ can be learned with $\tilde{O}(\sqrt{q})$ matvec queries, improving on an
$O(q)$ bound achievable via sketching techniques and vector-matrix-vector
queries.

</details>


### [37] [Edge-weighted Matching in the Dark](https://arxiv.org/abs/2507.19366)
*Zhiyi Huang,Enze Sun,Xiaowei Wu,Jiahao Zhao*

Main category: cs.DS

TL;DR: 提出针对Oblivious Bipartite Matching问题的0.659竞争比的Quadratic Ranking算法，打破1 - 1/e障碍，改进现有结果。


<details>
  <summary>Details</summary>
Motivation: 解决Tang, Wu和Zhang在2023年提出的开放性问题，改进Query - Commit Matching问题现有算法的竞争比。

Method: 提出Quadratic Ranking算法，它是经典Ranking算法的变体，用两个函数参数化算法，使算法定义和分析中的关键表达式为二次型，利用二次规划求解器优化函数选择。

Result: 得到0.659竞争比的算法，打破1 - 1/e障碍，改进了现有0.641的竞争比。

Conclusion: Quadratic Ranking算法在Oblivious Bipartite Matching问题上表现良好，能有效提升竞争比。

Abstract: We present a $0.659$-competitive Quadratic Ranking algorithm for the
Oblivious Bipartite Matching problem, a distribution-free version of
Query-Commit Matching. This result breaks the $1-\frac{1}{e}$ barrier,
addressing an open question raised by Tang, Wu, and Zhang (JACM 2023).
Moreover, the competitive ratio of this distribution-free algorithm improves
the best existing $0.641$ ratio for Query-Commit Matching achieved by the
distribution-dependent algorithm of Chen, Huang, Li, and Tang (SODA 2025).
  Quadratic Ranking is a novel variant of the classic Ranking algorithm. We
parameterize the algorithm with two functions, and let two key expressions in
the definition and analysis of the algorithm be quadratic forms of the two
functions. We show that the quadratic forms are the unique choices that satisfy
a set of natural properties. Further, they allow us to optimize the choice of
the two functions using powerful quadratic programming solvers.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [38] [Existence of 2-EFX Allocations of Chores](https://arxiv.org/abs/2507.19461)
*Jugal Garg,Aniket Murhekar*

Main category: cs.GT

TL;DR: 研究不可分割任务在具有加性负效用函数的代理间的公平分配，改进了近似无嫉妒分配（EFX）的保证，提出通用框架并证明其优势。


<details>
  <summary>Details</summary>
Motivation: 探究满足无嫉妒至任意任务（EFX）及其乘法近似的分配的存在性，改进现有近似保证。

Method: 提供一个通用框架，从合适的初始分配开始，在嫉妒和被嫉妒的代理的任务束之间进行一系列局部交换。

Result: 证明了对于所有具有加性负效用的实例，存在2 - EFX分配；用框架给出多个现有结果的简单统一证明。

Conclusion: 该框架因简单和通用，有望在近似 - EFX中有更广泛应用。

Abstract: We study the fair division of indivisible chores among agents with additive
disutility functions. We investigate the existence of allocations satisfying
the popular fairness notion of envy-freeness up to any chore (EFX), and its
multiplicative approximations. The existence of $4$-EFX allocations was
recently established by Garg, Murhekar, and Qin (2025). We improve this
guarantee by proving the existence of $2$-EFX allocations for all instances
with additive disutilities. This approximation was previously known only for
restricted instances such as bivalued disutilities (Lin, Wu, and Zhou (2025))
or three agents (Afshinmehr, Ansaripour, Danaei, and Mehlhorn (2024)).
  We obtain our result by providing a general framework for achieving
approximate-EFX allocations. The approach begins with a suitable initial
allocation and performs a sequence of local swaps between the bundles of
envious and envied agents. For our main result, we begin with an initial
allocation that satisfies envy-freeness up to one chore (EF1) and
Pareto-optimality (PO); the existence of such an allocation was recently
established in a major breakthrough by Mahara (2025). We further demonstrate
the strength and generality of our framework by giving simple and unified
proofs of existing results, namely (i) $2$-EFX for bivalued instances, (ii)
2-EFX for three agents, (iii) EFX when the number of chores is at most twice
the number of agents, and (iv) $4$-EFX for all instances. We expect this
framework to have broader applications in approximate-EFX due to its simplicity
and generality.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [39] [CityHood: An Explainable Travel Recommender System for Cities and Neighborhoods](https://arxiv.org/abs/2507.18778)
*Gustavo H Santos,Myriam Delgado,Thiago H Silva*

Main category: cs.IR

TL;DR: 介绍了基于用户兴趣推荐城市和社区的交互式可解释推荐系统CityHood，结合多种指标建模，提供多尺度推荐并支持解释。


<details>
  <summary>Details</summary>
Motivation: 填补基于位置推荐在兴趣建模、多尺度分析和可解释性方面的空白。

Method: 利用谷歌地点评论结合地理、社会人口等指标建模用户兴趣，用LIME技术和自然语言解释，通过可视化界面展示。

Result: 实现了可让用户根据偏好探索推荐并查看推荐理由的系统，使旅行推荐透明且有吸引力。

Conclusion: 该系统结合多种特性，在基于位置推荐方面有积极作用。

Abstract: We present CityHood, an interactive and explainable recommendation system
that suggests cities and neighborhoods based on users' areas of interest. The
system models user interests leveraging large-scale Google Places reviews
enriched with geographic, socio-demographic, political, and cultural
indicators. It provides personalized recommendations at city (Core-Based
Statistical Areas - CBSAs) and neighborhood (ZIP code) levels, supported by an
explainable technique (LIME) and natural-language explanations. Users can
explore recommendations based on their stated preferences and inspect the
reasoning behind each suggestion through a visual interface. The demo
illustrates how spatial similarity, cultural alignment, and interest
understanding can be used to make travel recommendations transparent and
engaging. This work bridges gaps in location-based recommendation by combining
a kind of interest modeling, multi-scale analysis, and explainability in a
user-facing system.

</details>


### [40] [Semantic IDs for Music Recommendation](https://arxiv.org/abs/2507.18800)
*M. Jeffrey Mei,Florian Henkel,Samuel E. Sandberg,Oliver Bembom,Andreas F. Ehmann*

Main category: cs.IR

TL;DR: 研究表明基于内容的共享特征（语义ID）可提升音乐推荐准确性和多样性，同时减小模型大小。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统为每个物品学习唯一嵌入会占用大量可训练参数，期望使用共享嵌入减少内存存储，构建轻量级模型。

Method: 利用基于内容的共享特征（语义ID），并在两个音乐推荐数据集上进行实验，包括音乐流媒体服务的在线A/B测试。

Result: 使用共享内容特征提升了推荐准确性和多样性，同时减小了模型大小。

Conclusion: 基于内容的共享特征在音乐推荐中有益，能在提升推荐效果的同时减小模型规模。

Abstract: Training recommender systems for next-item recommendation often requires
unique embeddings to be learned for each item, which may take up most of the
trainable parameters for a model. Shared embeddings, such as using content
information, can reduce the number of distinct embeddings to be stored in
memory. This allows for a more lightweight model; correspondingly, model
complexity can be increased due to having fewer embeddings to store in memory.
We show the benefit of using shared content-based features ('semantic IDs') in
improving recommendation accuracy and diversity, while reducing model size, for
two music recommendation datasets, including an online A/B test on a music
streaming service.

</details>


### [41] [A Comprehensive Review of AI-based Intelligent Tutoring Systems: Applications and Challenges](https://arxiv.org/abs/2507.18882)
*Meriem Zerkouk,Miloud Mihoubi,Belkacem Chikhaoui*

Main category: cs.IR

TL;DR: 本文对基于AI的智能辅导系统（ITS）进行全面综述，分析其在实际教育场景中的运行情况、应用和评估挑战，给出研究结果并提出建议。


<details>
  <summary>Details</summary>
Motivation: 随着ITS融入教育场景，其有效性结果不一，需全面了解其在实际教育中的运行和应用评估挑战。

Method: 采用系统文献综述方法，分析2010 - 2025年发表的合格研究，涵盖教学策略、NLP等多个领域。

Result: 揭示了ITS有效性的复杂情况，既看到了进步也发现了持续的挑战，还指出实验设计和数据分析需更严谨。

Conclusion: 基于研究结果，提出了未来研究建议和实际应用的启示。

Abstract: AI-based Intelligent Tutoring Systems (ITS) have significant potential to
transform teaching and learning. As efforts continue to design, develop, and
integrate ITS into educational contexts, mixed results about their
effectiveness have emerged. This paper provides a comprehensive review to
understand how ITS operate in real educational settings and to identify the
associated challenges in their application and evaluation. We use a systematic
literature review method to analyze numerous qualified studies published from
2010 to 2025, examining domains such as pedagogical strategies, NLP, adaptive
learning, student modeling, and domain-specific applications of ITS. The
results reveal a complex landscape regarding the effectiveness of ITS,
highlighting both advancements and persistent challenges. The study also
identifies a need for greater scientific rigor in experimental design and data
analysis. Based on these findings, suggestions for future research and
practical implications are proposed.

</details>


### [42] [Agent0: Leveraging LLM Agents to Discover Multi-value Features from Text for Enhanced Recommendations](https://arxiv.org/abs/2507.18993)
*Blaž Škrlj,Benoît Guilleminot,Andraž Tori*

Main category: cs.IR

TL;DR: 本文介绍了基于大语言模型的Agent0系统，可自动进行信息提取和特征构建，其闭环方法对自动化特征发现实用有效。


<details>
  <summary>Details</summary>
Motivation: 大语言模型及其代理框架在代码生成中广泛应用，但在以数据为中心的研究中应用不足，且大规模推荐系统中分类特征获取成本高。

Method: 提出Agent0系统，协调一组交互的大语言模型代理自动识别文本有价值方面，还提供利用神谕动态反馈循环的自动提示工程调优方法。

Result: 证明了闭环方法对自动化特征发现实用且有效。

Conclusion: Agent0系统及其闭环方法有助于解决当前推荐系统开发中最具挑战性的自动化特征发现阶段的问题。

Abstract: Large language models (LLMs) and their associated agent-based frameworks have
significantly advanced automated information extraction, a critical component
of modern recommender systems. While these multitask frameworks are widely used
in code generation, their application in data-centric research is still largely
untapped. This paper presents Agent0, an LLM-driven, agent-based system
designed to automate information extraction and feature construction from raw,
unstructured text. Categorical features are crucial for large-scale recommender
systems but are often expensive to acquire. Agent0 coordinates a group of
interacting LLM agents to automatically identify the most valuable text aspects
for subsequent tasks (such as models or AutoML pipelines). Beyond its feature
engineering capabilities, Agent0 also offers an automated prompt-engineering
tuning method that utilizes dynamic feedback loops from an oracle. Our findings
demonstrate that this closed-loop methodology is both practical and effective
for automated feature discovery, which is recognized as one of the most
challenging phases in current recommender system development.

</details>


### [43] [PBiLoss: Popularity-Aware Regularization to Improve Fairness in Graph-Based Recommender Systems](https://arxiv.org/abs/2507.19067)
*Mohammad Naeimi,Mostafa Haghir Chehreghani*

Main category: cs.IR

TL;DR: 本文提出PBiLoss损失函数应对推荐系统的流行度偏差问题，引入两种采样策略和区分流行物品的方法，实验表明其能提升公平性并保持或增强推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于图神经网络的推荐系统存在流行度偏差，导致内容多样性降低和公平性受损。

Method: 提出PBiLoss损失函数，惩罚模型对流行物品的倾向；引入PopPos和PopNeg采样策略；探索两种区分流行物品的方法；方法与模型无关，可集成到现有图推荐框架。

Result: 在多个真实数据集上实验，PBiLoss显著降低PRU和PRI，提升公平性，同时保持或增强推荐准确性和排序指标。

Conclusion: 将公平性目标嵌入优化过程有效，为平衡推荐准确性和公平内容曝光提供实用可扩展的解决方案。

Abstract: Recommender systems, especially those based on graph neural networks (GNNs),
have achieved remarkable success in capturing user-item interaction patterns.
However, they remain susceptible to popularity bias--the tendency to
over-recommend popular items--resulting in reduced content diversity and
compromised fairness. In this paper, we propose PBiLoss, a novel
regularization-based loss function designed to counteract popularity bias in
graph-based recommender models explicitly. PBiLoss augments traditional
training objectives by penalizing the model's inclination toward popular items,
thereby encouraging the recommendation of less popular but potentially more
personalized content. We introduce two sampling strategies: Popular Positive
(PopPos) and Popular Negative (PopNeg), which respectively modulate the
contribution of the positive and negative popular items during training. We
further explore two methods to distinguish popular items: one based on a fixed
popularity threshold and another without any threshold, making the approach
flexible and adaptive. Our proposed method is model-agnostic and can be
seamlessly integrated into state-of-the-art graph-based frameworks such as
LightGCN and its variants. Comprehensive experiments across multiple real-world
datasets demonstrate that PBiLoss significantly improves fairness, as
demonstrated by reductions in the Popularity-Rank Correlation for Users (PRU)
and Popularity-Rank Correlation for Items (PRI), while maintaining or even
enhancing standard recommendation accuracy and ranking metrics. These results
highlight the effectiveness of directly embedding fairness objectives into the
optimization process, providing a practical and scalable solution for balancing
accuracy and equitable content exposure in modern recommender systems.

</details>


### [44] [SelfRACG: Enabling LLMs to Self-Express and Retrieve for Code Generation](https://arxiv.org/abs/2507.19033)
*Qian Dong,Jia Chen,Qingyao Ai,Hongning Wang,Haitao Li,Yi Wu,Yao Hu,Yiqun Liu,Shaoping Ma*

Main category: cs.IR

TL;DR: 现有RACG方法存在内容差距问题，提出SelfRACG范式提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有RACG方法因内容差距，外部检索模块无法推断大模型信息需求，影响性能。

Method: 提出SelfRACG范式，包含信息需求表达模块和两阶段信息需求引导训练策略。

Result: SelfRACG能检索更符合大模型信息需求的外部知识。

Conclusion: SelfRACG比普通RACG有更优的生成性能。

Abstract: Existing retrieval-augmented code generation (RACG) methods typically use an
external retrieval module to fetch semantically similar code snippets used for
generating subsequent fragments. However, even for consecutive code fragments,
the content often diverges due to logical progression, resulting in a content
gap. This gap undermines the performance of current RACG methods, as
\textit{external} retrieval modules based on content matching fail to infer the
specific information need of LLMs to generate the next code fragment.
Therefore, we propose \textbf{SelfRACG}, a novel paradigm that enables large
language models (LLMs) to \textbf{Self}-express their information needs to
enhance \textbf{RACG}. Specifically, SelfRACG includes an information need
expression module and a two-stage information need-guided training strategy,
which encourages LLMs to express their information need. Extensive experiments
demonstrate that SelfRACG can retrieve external knowledge that better aligns
with the LLM's own information needs, resulting in superior generation
performance compared to vanilla RACG.

</details>


### [45] [Distilling a Small Utility-Based Passage Selector to Enhance Retrieval-Augmented Generation](https://arxiv.org/abs/2507.19102)
*Hengran Zhang,Keping Bi,Jiafeng Guo,Jiaming Zhang,Shuaiqiang Wang,Dawei Yin,Xueqi Cheng*

Main category: cs.IR

TL;DR: 提出将大语言模型的效用判断能力提炼到小模型的方法，实现基于效用的动态段落选择，降低计算成本并提高答案质量，复杂问题上效用选择比相关性排序更有效。


<details>
  <summary>Details</summary>
Motivation: 标准检索注重相关性，而RAG强调效用，但用大语言模型进行效用判断计算成本高，限制评估段落数量，难以处理复杂查询。

Method: 将大语言模型的效用判断能力提炼到更小、更高效的模型，关注基于效用的选择而非排序，通过滑动窗口动态选择有用段落，训练学生模型从教师大语言模型学习伪答案生成和效用判断。

Result: 实验表明基于效用的选择为RAG提供了灵活且经济高效的解决方案，显著降低计算成本并提高答案质量，以Qwen3 - 32B为教师模型蒸馏出RankQwen1.7B和UtilityQwen1.7B。

Conclusion: 对于复杂问题，基于效用的选择比相关性排序在增强答案生成性能方面更有效，将发布MS MARCO数据集的相关标注支持后续研究。

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
incorporating retrieved information. Standard retrieval process prioritized
relevance, focusing on topical alignment between queries and passages. In
contrast, in RAG, the emphasis has shifted to utility, which considers the
usefulness of passages for generating accurate answers. Despite empirical
evidence showing the benefits of utility-based retrieval in RAG, the high
computational cost of using LLMs for utility judgments limits the number of
passages evaluated. This restriction is problematic for complex queries
requiring extensive information. To address this, we propose a method to
distill the utility judgment capabilities of LLMs into smaller, more efficient
models. Our approach focuses on utility-based selection rather than ranking,
enabling dynamic passage selection tailored to specific queries without the
need for fixed thresholds. We train student models to learn pseudo-answer
generation and utility judgments from teacher LLMs, using a sliding window
method that dynamically selects useful passages. Our experiments demonstrate
that utility-based selection provides a flexible and cost-effective solution
for RAG, significantly reducing computational costs while improving answer
quality. We present the distillation results using Qwen3-32B as the teacher
model for both relevance ranking and utility-based selection, distilled into
RankQwen1.7B and UtilityQwen1.7B. Our findings indicate that for complex
questions, utility-based selection is more effective than relevance ranking in
enhancing answer generation performance. We will release the relevance ranking
and utility-based selection annotations for the MS MARCO dataset, supporting
further research in this area.

</details>


### [46] [Towards LLM-Enhanced Group Recommender Systems](https://arxiv.org/abs/2507.19283)
*Sebastian Lubos,Alexander Felfernig,Thi Ngoc Trang Tran,Viet-Man Le,Damian Garber,Manuel Henrich,Reinhard Willfort,Jeremias Fuchs*

Main category: cs.IR

TL;DR: 本文探讨大语言模型如何支持群体推荐系统，提升决策支持质量和适用性。


<details>
  <summary>Details</summary>
Motivation: 群体推荐系统存在额外复杂性，需处理群体动态、决策过程等多方面因素，要研究大语言模型对其的支持作用。

Method: 未提及

Result: 未提及

Conclusion: 未提及

Abstract: In contrast to single-user recommender systems, group recommender systems are
designed to generate and explain recommendations for groups. This
group-oriented setting introduces additional complexities, as several factors -
absent in individual contexts - must be addressed. These include understanding
group dynamics (e.g., social dependencies within the group), defining effective
decision-making processes, ensuring that recommendations are suitable for all
group members, and providing group-level explanations as well as explanations
for individual users. In this paper, we analyze in which way large language
models (LLMs) can support these aspects and help to increase the overall
decision support quality and applicability of group recommender systems.

</details>


### [47] [Injecting External Knowledge into the Reasoning Process Enhances Retrieval-Augmented Generation](https://arxiv.org/abs/2507.19333)
*Minghao Tang,Shiyu Ni,Jiafeng Guo,Keping Bi*

Main category: cs.IR

TL;DR: 本文提出Passage Injection方法增强RAG系统对噪声段落的鲁棒性，实验表明该方法有效提升性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: RAG系统有效性常受噪声段落影响，提升大语言模型对噪声的鲁棒性对提高RAG系统可靠性至关重要。

Method: 提出Passage Injection方法，将检索到的段落明确纳入大语言模型的推理过程。

Result: 在四个推理增强的大语言模型和四个事实问答数据集上实验，表明该方法显著提升RAG整体性能，在两种噪声检索设置下提升鲁棒性，还能有效利用有用段落。

Conclusion: 将段落纳入大语言模型推理过程是构建更鲁棒RAG系统的有前景方向。

Abstract: Retrieval-augmented generation (RAG) has been widely adopted to augment large
language models (LLMs) with external knowledge for knowledge-intensive tasks.
However, its effectiveness is often undermined by the presence of noisy (i.e.,
low-quality) retrieved passages. Enhancing LLMs' robustness to such noise is
critical for improving the reliability of RAG systems. Recent advances have
equipped LLMs with strong reasoning and self-reflection capabilities, allowing
them to identify and correct errors in their reasoning process. Inspired by
this ability, we propose Passage Injection-a simple yet effective method that
explicitly incorporates retrieved passages into LLMs' reasoning process, aiming
to enhance the model's ability to recognize and resist noisy passages. We
validate Passage Injection under general RAG settings using BM25 as the
retriever. Experiments on four reasoning-enhanced LLMs across four factual QA
datasets demonstrate that Passage Injection significantly improves overall RAG
performance. Further analysis on two noisy retrieval settings-random noise,
where the model is provided irrelevant passages, and counterfactual noise,
where it is given misleading passages-shows that Passage Injection consistently
improves robustness. Controlled experiments confirm that Passage Injection can
also effectively leverage helpful passages. These findings suggest that
incorporating passages in LLMs' reasoning process is a promising direction for
building more robust RAG systems. The code can be found
\href{here}{https://github.com/mh-tang/Passage-Injection}.

</details>


### [48] [Let It Go? Not Quite: Addressing Item Cold Start in Sequential Recommendations with Content-Based Initialization](https://arxiv.org/abs/2507.19473)
*Anton Pembek,Artem Fatkulin,Anton Klenitskiy,Alexey Vasilev*

Main category: cs.IR

TL;DR: 针对序列推荐系统冷启动问题，提出在冻结内容嵌入基础上加可训练增量的方法，在多数据集有提升。


<details>
  <summary>Details</summary>
Motivation: 解决序列推荐系统冷启动问题，现有直接用冻结内容嵌入或微调嵌入方法效果不佳。

Method: 在冻结的内容嵌入中引入小的可训练增量，使模型适应项目表示又不偏离原语义结构。

Result: 该方法在多个数据集和模态上都有持续改进，如电商文本描述数据集和音乐音频表示数据集。

Conclusion: 提出的在冻结嵌入上加可训练增量的方法有效，能解决冷启动问题并提升性能。

Abstract: Many sequential recommender systems suffer from the cold start problem, where
items with few or no interactions cannot be effectively used by the model due
to the absence of a trained embedding. Content-based approaches, which leverage
item metadata, are commonly used in such scenarios. One possible way is to use
embeddings derived from content features such as textual descriptions as
initialization for the model embeddings. However, directly using frozen content
embeddings often results in suboptimal performance, as they may not fully adapt
to the recommendation task. On the other hand, fine-tuning these embeddings can
degrade performance for cold-start items, as item representations may drift far
from their original structure after training. We propose a novel approach to
address this limitation. Instead of entirely freezing the content embeddings or
fine-tuning them extensively, we introduce a small trainable delta to frozen
embeddings that enables the model to adapt item representations without letting
them go too far from their original semantic structure. This approach
demonstrates consistent improvements across multiple datasets and modalities,
including e-commerce datasets with textual descriptions and a music dataset
with audio-based representation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [49] [Diffusion Models for Solving Inverse Problems via Posterior Sampling with Piecewise Guidance](https://arxiv.org/abs/2507.18654)
*Saeed Mohseni-Sehdeh,Walid Saad,Kei Sakaguchi,Tao Yu*

Main category: cs.LG

TL;DR: 本文引入基于扩散模型的框架，用分段引导方案解决逆问题，实验表明其高效且准确。


<details>
  <summary>Details</summary>
Motivation: 现有方法需针对每个问题重新训练，且未显式考虑测量噪声，需开发通用且高效准确的方法。

Method: 提出基于扩散模型的框架，使用分段引导方案，引导项是扩散时间步的分段函数。

Result: 在图像修复任务实验中，相比基线，推理时间在图像修复降低25%，超分辨率分别降低23%和24%，PSNR和SSIM损失可忽略。

Conclusion: 所提框架能有效平衡计算效率和引导项准确性，适用于多种逆问题。

Abstract: Diffusion models are powerful tools for sampling from high-dimensional
distributions by progressively transforming pure noise into structured data
through a denoising process. When equipped with a guidance mechanism, these
models can also generate samples from conditional distributions. In this paper,
a novel diffusion-based framework is introduced for solving inverse problems
using a piecewise guidance scheme. The guidance term is defined as a piecewise
function of the diffusion timestep, facilitating the use of different
approximations during high-noise and low-noise phases. This design is shown to
effectively balance computational efficiency with the accuracy of the guidance
term. Unlike task-specific approaches that require retraining for each problem,
the proposed method is problem-agnostic and readily adaptable to a variety of
inverse problems. Additionally, it explicitly incorporates measurement noise
into the reconstruction process. The effectiveness of the proposed framework is
demonstrated through extensive experiments on image restoration tasks,
specifically image inpainting and super-resolution. Using a class conditional
diffusion model for recovery, compared to the \pgdm baseline, the proposed
framework achieves a reduction in inference time of \(25\%\) for inpainting
with both random and center masks, and \(23\%\) and \(24\%\) for \(4\times\)
and \(8\times\) super-resolution tasks, respectively, while incurring only
negligible loss in PSNR and SSIM.

</details>


### [50] [Concept Probing: Where to Find Human-Defined Concepts (Extended Version)](https://arxiv.org/abs/2507.18681)
*Manuel de Sousa Ribeiro,Afonso Leote,João Leite*

Main category: cs.LG

TL;DR: 本文提出一种自动识别神经网络模型中用于概念探测的合适层的方法，并通过实证分析验证。


<details>
  <summary>Details</summary>
Motivation: 概念探测中，探针性能高度依赖所探测的内部表征，因此识别合适的探测层至关重要。

Method: 基于表征对给定人类定义概念的信息性和规则性，自动识别神经网络模型中应考虑的层。

Result: 通过对不同神经网络模型和数据集的详尽实证分析验证了研究结果。

Conclusion: 所提出的方法能够有效识别用于概念探测的合适层。

Abstract: Concept probing has recently gained popularity as a way for humans to peek
into what is encoded within artificial neural networks. In concept probing,
additional classifiers are trained to map the internal representations of a
model into human-defined concepts of interest. However, the performance of
these probes is highly dependent on the internal representations they probe
from, making identifying the appropriate layer to probe an essential task. In
this paper, we propose a method to automatically identify which layer's
representations in a neural network model should be considered when probing for
a given human-defined concept of interest, based on how informative and regular
the representations are with respect to the concept. We validate our findings
through an exhaustive empirical analysis over different neural network models
and datasets.

</details>


### [51] [Efficient Knowledge Tracing Leveraging Higher-Order Information in Integrated Graphs](https://arxiv.org/abs/2507.18668)
*Donghee Han,Daehee Kim,Minjun Lee,Daeyoung Roh,Keejun Han,Mun Yong Yi*

Main category: cs.LG

TL;DR: 提出DGAKT模型解决现有知识追踪方法计算成本高问题，实验显示其性能和资源效率优。


<details>
  <summary>Details</summary>
Motivation: 现有知识追踪方法在利用大图和长学习序列时存在计算成本增加问题。

Method: 引入基于双图注意力的知识追踪（DGAKT）模型，采用基于子图的方法，仅处理每个目标交互的相关子图。

Result: DGAKT性能优于现有KT模型，资源效率也更高。

Conclusion: DGAKT解决了现有KT方法中被忽视的关键需求，树立了新的资源效率标准。

Abstract: The rise of online learning has led to the development of various knowledge
tracing (KT) methods. However, existing methods have overlooked the problem of
increasing computational cost when utilizing large graphs and long learning
sequences. To address this issue, we introduce Dual Graph Attention-based
Knowledge Tracing (DGAKT), a graph neural network model designed to leverage
high-order information from subgraphs representing student-exercise-KC
relationships. DGAKT incorporates a subgraph-based approach to enhance
computational efficiency. By processing only relevant subgraphs for each target
interaction, DGAKT significantly reduces memory and computational requirements
compared to full global graph models. Extensive experimental results
demonstrate that DGAKT not only outperforms existing KT models but also sets a
new standard in resource efficiency, addressing a critical need that has been
largely overlooked by prior KT approaches.

</details>


### [52] [Innovator: Scientific Continued Pretraining with Fine-grained MoE Upcycling](https://arxiv.org/abs/2507.18671)
*Ning Liao,Xiaoxing Wang,Zehao Lin,Weiyang Guo,Feng Hong,Shixiang Song,Geng Yu,Zihua Zhao,Sitao Xie,Longxuan Wei,Xiangqi Jin,Xiaohan Qin,Jiale Ma,Kai Chen,Jiangchao Yao,Zhouhan Lin,Junchi Yan,Zhiyu Li,Feiyu Xiong,Yanfeng Wang,Linfeng Zhang*

Main category: cs.LG

TL;DR: 本文提出Innovator模型，通过将预训练密集大语言模型升级为细粒度混合专家模型解决科学数据继续预训练时的灾难性遗忘问题，在科学和通用任务上表现良好。


<details>
  <summary>Details</summary>
Motivation: 直接用科学数据继续预训练大语言模型会导致灾难性遗忘，通用能力严重下降，需要解决该问题以构建具备科学和通用知识的大语言模型。

Method: 将预训练密集大语言模型升级为细粒度混合专家模型，采用四阶段升级训练范式：科学专家诱导、细粒度专家拆分、科学感知路由预热和通才 - 科学家集成训练。

Result: Innovator在30个科学任务上平均提升25%，胜率达70%，通用任务保留99%性能；Innovator - Reason在解决复杂科学问题上推理性能提升超30%。

Conclusion: Innovator模型有效解决了继续预训练中的灾难性遗忘问题，在科学和通用任务上都有出色表现，且针对推理的后训练模型也有良好推理性能。

Abstract: A large language model (LLM) with knowledge in both scientific and general
tasks is the foundation of science general intelligence. However, directly
continued pretraining an LLM using science data usually leads to catastrophic
forgetting, which indicates severe degradation in general ability. In this
report, we present Innovator, which solves this problem by upcycling a
pre-trained dense LLM into a fine-grained Mixtures-of-Experts model during
continued pretraining, where different experts are expected to learn science
knowledge in different disciplines, and a shared expert is utilized for general
tasks. Innovator introduces a four-stage upcycle training paradigm: (1)
Scientific Expert Induction on discipline-specific data, (2) Fine-grained
Expert Splitting via FFN dimension decomposition, (3) Science-Aware Routing
warmup, and (4) Generalist-Scientist Integration training on hybrid datasets.
Such a paradigm enables knowledge in the general domain, and different
scientific disciplines can be decoupled, avoiding the negative influence among
knowledge in different domains. With 53.3B total parameters and 13.3B
activated, Innovator extends Qwen2.5-7B using a shared general expert and 64
specialized scientific experts with 8 activated. Trained on 300B tokens with
tri-level quality-controlled data, Innovator achieves 25% average improvement
across 30 scientific tasks with a win rate as 70%, while retaining 99%
performance in general tasks. Furthermore, Innovator-Reason, which is
post-trained from Innovator for reasoning boosting, exhibits excellent
reasoning performance in solving complex scientific problems with improvements
over 30%.

</details>


### [53] [Market Making Strategies with Reinforcement Learning](https://arxiv.org/abs/2507.18680)
*Óscar Fernández Vicente*

Main category: cs.LG

TL;DR: 本文聚焦将强化学习应用于金融做市问题，设计策略并验证其优于传统策略，为做市代理设计提供新方法。


<details>
  <summary>Details</summary>
Motivation: 做市商在提供流动性时面临库存风险、竞争和非平稳市场动态等挑战，研究探索强化学习在开发做市策略上的应用。

Method: 将做市任务表述为强化学习问题，设计单/多智能体；用奖励工程和多目标强化学习处理库存管理；引入基于折扣汤普森采样的POW - dTS算法应对非平稳性。

Result: 基于强化学习的方法在多个性能指标上显著优于传统和基线算法策略。

Conclusion: 研究为设计稳健、高效和自适应的做市代理提供了新方法和见解，强化了强化学习变革复杂金融系统算法交易的潜力。

Abstract: This thesis presents the results of a comprehensive research project focused
on applying Reinforcement Learning (RL) to the problem of market making in
financial markets. Market makers (MMs) play a fundamental role in providing
liquidity, yet face significant challenges arising from inventory risk,
competition, and non-stationary market dynamics. This research explores how RL,
particularly Deep Reinforcement Learning (DRL), can be employed to develop
autonomous, adaptive, and profitable market making strategies.
  The study begins by formulating the MM task as a reinforcement learning
problem, designing agents capable of operating in both single-agent and
multi-agent settings within a simulated financial environment. It then
addresses the complex issue of inventory management using two complementary
approaches: reward engineering and Multi-Objective Reinforcement Learning
(MORL). While the former uses dynamic reward shaping to guide behavior, the
latter leverages Pareto front optimization to explicitly balance competing
objectives.
  To address the problem of non-stationarity, the research introduces POW-dTS,
a novel policy weighting algorithm based on Discounted Thompson Sampling. This
method allows agents to dynamically select and combine pretrained policies,
enabling continual adaptation to shifting market conditions.
  The experimental results demonstrate that the proposed RL-based approaches
significantly outperform traditional and baseline algorithmic strategies across
various performance metrics. Overall, this research thesis contributes new
methodologies and insights for the design of robust, efficient, and adaptive
market making agents, reinforcing the potential of RL to transform algorithmic
trading in complex financial systems.

</details>


### [54] [The Right to be Forgotten in Pruning: Unveil Machine Unlearning on Sparse Models](https://arxiv.org/abs/2507.18725)
*Yang Xiao,Gen Li,Jie Ji,Ruimeng Ye,Xiaolong Ma,Bo Hui*

Main category: cs.LG

TL;DR: 研究稀疏模型的机器学习遗忘问题，提出解剪枝算法，设计新评估指标并验证算法有效性。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏模型的遗忘研究不足，需解决删除数据对模型剪枝的影响及遗忘权问题。

Method: 定义解剪枝术语，提出解剪枝算法，可与现有遗忘算法集成；设计新的稀疏模型性能指标。

Result: 解剪枝误差理论上有上界，可用于结构和非结构稀疏模型；发现MIA准确性评估不可靠。

Conclusion: 所提解剪枝算法有效，新指标能更好评估稀疏模型遗忘效果。

Abstract: Machine unlearning aims to efficiently eliminate the memory about deleted
data from trained models and address the right to be forgotten. Despite the
success of existing unlearning algorithms, unlearning in sparse models has not
yet been well studied. In this paper, we empirically find that the deleted data
has an impact on the pruned topology in a sparse model. Motivated by the
observation and the right to be forgotten, we define a new terminology
``un-pruning" to eliminate the impact of deleted data on model pruning. Then we
propose an un-pruning algorithm to approximate the pruned topology driven by
retained data. We remark that any existing unlearning algorithm can be
integrated with the proposed un-pruning workflow and the error of un-pruning is
upper-bounded in theory. Also, our un-pruning algorithm can be applied to both
structured sparse models and unstructured sparse models. In the experiment, we
further find that Membership Inference Attack (MIA) accuracy is unreliable for
assessing whether a model has forgotten deleted data, as a small change in the
amount of deleted data can produce arbitrary MIA results. Accordingly, we
devise new performance metrics for sparse models to evaluate the success of
un-pruning. Lastly, we conduct extensive experiments to verify the efficacy of
un-pruning with various pruning methods and unlearning algorithms. Our code is
released at https://anonymous.4open.science/r/UnlearningSparseModels-FBC5/.

</details>


### [55] [Exploitation Over Exploration: Unmasking the Bias in Linear Bandit Recommender Offline Evaluation](https://arxiv.org/abs/2507.18756)
*Pedro R. Pires,Gregorio F. Azevedo,Pietro L. Campos,Rafael T. Sereicikas,Tiago A. Almeida*

Main category: cs.LG

TL;DR: 本文对线性多臂老虎机进行离线实证比较，发现无探索的贪婪线性模型表现出色，凸显离线评估协议不足，强调需开发更稳健评估方法。


<details>
  <summary>Details</summary>
Motivation: 多臂老虎机算法常用于推荐系统，但其离线评估在评估探索行为上有局限性，需研究其性能表现和评估方法。

Method: 对几种线性多臂老虎机进行广泛的离线实证比较，并进行超参数优化。

Result: 在超90%的数据集上，无探索的贪婪线性模型表现顶尖，超参数优化也倾向最小化探索的配置。

Conclusion: 现有离线评估协议存在不足，迫切需要开发更稳健的评估方法用于推荐系统的交互式学习。

Abstract: Multi-Armed Bandit (MAB) algorithms are widely used in recommender systems
that require continuous, incremental learning. A core aspect of MABs is the
exploration-exploitation trade-off: choosing between exploiting items likely to
be enjoyed and exploring new ones to gather information. In contextual linear
bandits, this trade-off is particularly central, as many variants share the
same linear regression backbone and differ primarily in their exploration
strategies. Despite its prevalent use, offline evaluation of MABs is
increasingly recognized for its limitations in reliably assessing exploration
behavior. This study conducts an extensive offline empirical comparison of
several linear MABs. Strikingly, across over 90% of various datasets, a greedy
linear model, with no type of exploration, consistently achieves top-tier
performance, often outperforming or matching its exploratory counterparts. This
observation is further corroborated by hyperparameter optimization, which
consistently favors configurations that minimize exploration, suggesting that
pure exploitation is the dominant strategy within these evaluation settings.
Our results expose significant inadequacies in offline evaluation protocols for
bandits, particularly concerning their capacity to reflect true exploratory
efficacy. Consequently, this research underscores the urgent necessity for
developing more robust assessment methodologies, guiding future investigations
into alternative evaluation frameworks for interactive learning in recommender
systems.

</details>


### [56] [CLEAR: Unlearning Spurious Style-Content Associations with Contrastive LEarning with Anti-contrastive Regularization](https://arxiv.org/abs/2507.18794)
*Minghui Sun,Benjamin A. Goldstein,Matthew M. Engelhard*

Main category: cs.LG

TL;DR: 提出CLEAR框架分离数据特征，在VAE潜在空间实现CLEAR - VAE，实验证明其能交换特征、提升下游分类性能。


<details>
  <summary>Details</summary>
Motivation: 学习不受表面特征影响的表征，确保测试时表面特征变化不影响下游预测性能，保证预测的公平性和泛化性。

Method: 提出CLEAR框架，证明PS反对比惩罚可最小化风格属性与内容标签的互信息，在VAE潜在空间实现CLEAR - VAE并进行实验。

Result: CLEAR - VAE可交换和插值样本的内容与风格，提升存在未知内容和风格组合时的下游分类性能。

Conclusion: CLEAR框架有效，能分离关键和表面特征，提升下游任务性能，代码将公开。

Abstract: Learning representations unaffected by superficial characteristics is
important to ensure that shifts in these characteristics at test time do not
compromise downstream prediction performance. For instance, in healthcare
applications, we might like to learn features that contain information about
pathology yet are unaffected by race, sex, and other sources of physiologic
variability, thereby ensuring predictions are equitable and generalizable
across all demographics. Here we propose Contrastive LEarning with
Anti-contrastive Regularization (CLEAR), an intuitive and easy-to-implement
framework that effectively separates essential (i.e., task-relevant)
characteristics from superficial (i.e., task-irrelevant) characteristics during
training, leading to better performance when superficial characteristics shift
at test time. We begin by supposing that data representations can be
semantically separated into task-relevant content features, which contain
information relevant to downstream tasks, and task-irrelevant style features,
which encompass superficial attributes that are irrelevant to these tasks, yet
may degrade performance due to associations with content present in training
data that do not generalize. We then prove that our anti-contrastive penalty,
which we call Pair-Switching (PS), minimizes the Mutual Information between the
style attributes and content labels. Finally, we instantiate CLEAR in the
latent space of a Variational Auto-Encoder (VAE), then perform experiments to
quantitatively and qualitatively evaluate the resulting CLEAR-VAE over several
image datasets. Our results show that CLEAR-VAE allows us to: (a) swap and
interpolate content and style between any pair of samples, and (b) improve
downstream classification performance in the presence of previously unseen
combinations of content and style. Our code will be made publicly available.

</details>


### [57] [Ralts: Robust Aggregation for Enhancing Graph Neural Network Resilience on Bit-flip Errors](https://arxiv.org/abs/2507.18804)
*Wencheng Zou,Nan Wu*

Main category: cs.LG

TL;DR: 本文分析GNN对翻转错误的鲁棒性并提出Ralts方案，经评估其能有效增强GNN鲁棒性且执行效率高。


<details>
  <summary>Details</summary>
Motivation: 现有GNN鲁棒性研究多关注软件层面威胁，硬件故障和错误研究不足，而硬件易受瞬态故障影响。

Method: 先全面分析GNN对翻转错误的鲁棒性，再提出Ralts方案，利用图相似性指标过滤异常值、恢复图拓扑，并将保护技术融入聚合函数。

Result: Ralts能有效增强不同GNN模型、数据集、错误模式及架构下的GNN鲁棒性，提高预测准确率，执行效率与PyTorch Geometric内置聚合函数相当。

Conclusion: Ralts是一个可推广的轻量级解决方案，能提升GNN对翻转错误的恢复能力，具有系统级优化潜力。

Abstract: Graph neural networks (GNNs) have been widely applied in safety-critical
applications, such as financial and medical networks, in which compromised
predictions may cause catastrophic consequences. While existing research on GNN
robustness has primarily focused on software-level threats, hardware-induced
faults and errors remain largely underexplored. As hardware systems progress
toward advanced technology nodes to meet high-performance and energy efficiency
demands, they become increasingly susceptible to transient faults, which can
cause bit flips and silent data corruption, a prominent issue observed by major
technology companies (e.g., Meta and Google). In response, we first present a
comprehensive analysis of GNN robustness against bit-flip errors, aiming to
reveal system-level optimization opportunities for future reliable and
efficient GNN systems. Second, we propose Ralts, a generalizable and
lightweight solution to bolster GNN resilience to bit-flip errors.
Specifically, Ralts exploits various graph similarity metrics to filter out
outliers and recover compromised graph topology, and incorporates these
protective techniques directly into aggregation functions to support any
message-passing GNNs. Evaluation results demonstrate that Ralts effectively
enhances GNN robustness across a range of GNN models, graph datasets, error
patterns, and both dense and sparse architectures. On average, under a BER of
$3\times10^{-5}$, these robust aggregation functions improve prediction
accuracy by at least 20\% when errors present in model weights or node
embeddings, and by at least 10\% when errors occur in adjacency matrices. Ralts
is also optimized to deliver execution efficiency comparable to built-in
aggregation functions in PyTorch Geometric.

</details>


### [58] [FD4QC: Application of Classical and Quantum-Hybrid Machine Learning for Financial Fraud Detection A Technical Report](https://arxiv.org/abs/2507.19402)
*Matteo Cardaioli,Luca Marangoni,Giada Martini,Francesco Mazzolin,Luca Pajola,Andrea Ferretto Parodi,Alessandra Saitta,Maria Chiara Vernillo*

Main category: cs.LG

TL;DR: 报告研究经典、量子和量子混合机器学习模型用于金融欺诈二元分类，开发特征工程框架，评估多种模型，提出FD4QC系统架构，结果显示经典树模型表现优，量子模型有局限。


<details>
  <summary>Details</summary>
Motivation: 传统欺诈检测系统面临金融交易复杂性和数据量增加的挑战，需探究不同模型在金融欺诈分类中的有效性。

Method: 开发行为特征工程框架，在IBM AML数据集上实现并评估经典模型（逻辑回归、决策树等）和量子混合算法架构（QSVM、VQC、HQNN），提出FD4QC系统架构。

Result: 经典树模型尤其是随机森林表现优于量子模型，准确率达97.34%，F - 测度86.95%；量子模型中QSVM有一定优势，精度77.15%，误报率1.36%，但召回率低且计算开销大。

Conclusion: 为金融应用提供基准，指出量子机器学习在该领域的当前局限，并给出未来研究方向。

Abstract: The increasing complexity and volume of financial transactions pose
significant challenges to traditional fraud detection systems. This technical
report investigates and compares the efficacy of classical, quantum, and
quantum-hybrid machine learning models for the binary classification of
fraudulent financial activities.
  As of our methodology, first, we develop a comprehensive behavioural feature
engineering framework to transform raw transactional data into a rich,
descriptive feature set. Second, we implement and evaluate a range of models on
the IBM Anti-Money Laundering (AML) dataset. The classical baseline models
include Logistic Regression, Decision Tree, Random Forest, and XGBoost. These
are compared against three hybrid classic quantum algorithms architectures: a
Quantum Support Vector Machine (QSVM), a Variational Quantum Classifier (VQC),
and a Hybrid Quantum Neural Network (HQNN).
  Furthermore, we propose Fraud Detection for Quantum Computing (FD4QC), a
practical, API-driven system architecture designed for real-world deployment,
featuring a classical-first, quantum-enhanced philosophy with robust fallback
mechanisms.
  Our results demonstrate that classical tree-based models, particularly
\textit{Random Forest}, significantly outperform the quantum counterparts in
the current setup, achieving high accuracy (\(97.34\%\)) and F-measure
(\(86.95\%\)). Among the quantum models, \textbf{QSVM} shows the most promise,
delivering high precision (\(77.15\%\)) and a low false-positive rate
(\(1.36\%\)), albeit with lower recall and significant computational overhead.
  This report provides a benchmark for a real-world financial application,
highlights the current limitations of quantum machine learning in this domain,
and outlines promising directions for future research.

</details>


### [59] [Fishers for Free? Approximating the Fisher Information Matrix by Recycling the Squared Gradient Accumulator](https://arxiv.org/abs/2507.18807)
*YuXin Li,Felix Dangel,Derek Tam,Colin Raffel*

Main category: cs.LG

TL;DR: 探索利用训练中已计算的平方梯度累加器免费近似Fisher对角线，实验表明'Squisher'表现与Fisher对角线相当且优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 估计Fisher对角线计算成本高，而自适应梯度方法训练中会计算平方梯度的移动平均，探索能否免费近似Fisher对角线。

Method: 通过涵盖Fisher对角线五个应用的一组综合实验。

Result: Squisher表现与Fisher对角线一致且优于基线方法。

Conclusion: 明确了Squisher和Fisher对角线的具体差异并对各自影响进行了实证量化。

Abstract: The diagonal of a model's Fisher Information Matrix (the "Fisher diagonal")
has frequently been used as a way to measure parameter sensitivity. Typically,
the Fisher diagonal is estimated via squared sampled gradients of the model's
likelihood with respect to its parameters, averaged over a few hundred or
thousand examples -- a process which incurs nontrivial computational costs. At
the same time, adaptive gradient methods like the ubiquitous Adam optimizer
compute a moving average of the squared gradient over the course of training.
This paper therefore explores whether an approximation of the Fisher diagonal
can be obtained "for free" by recycling the squared gradient accumulator that
has already been computed over the course of training. Through a comprehensive
set of experiments covering five applications of the Fisher diagonal, we
demonstrate that the "Squisher" (SQUared gradient accumulator as an
approximation of the FISHER) consistently performs similarly to the Fisher
diagonal while outperforming baseline methods. Additionally, we clarify the
exact differences between the Squisher and the Fisher diagonal and provide
empirical quantification of their respective impact.

</details>


### [60] [Test-time Offline Reinforcement Learning on Goal-related Experience](https://arxiv.org/abs/2507.18809)
*Marco Bagatella,Mert Albaba,Jonas Hübotter,Georg Martius,Andreas Krause*

Main category: cs.LG

TL;DR: 本文研究目标条件强化学习的测试时训练，提出新的数据选择标准，在多任务中验证效果并研究推理时计算分配。


<details>
  <summary>Details</summary>
Motivation: 受基础模型测试时训练提升性能启发，探索测试时离线强化学习提升策略性能的方法。

Method: 提出自监督数据选择标准，按相关性和质量从离线数据集选数据，采用GC - TTT算法以滚动时域方式调整策略。

Result: 在高维运动导航和操作任务中，对所选数据微调策略有显著性能提升，GC - TTT在可比成本下有模型扩展无法达到的性能增益。

Conclusion: 测试时离线强化学习能以最小计算成本显著提升策略性能，GC - TTT算法有效。

Abstract: Foundation models compress a large amount of information in a single, large
neural network, which can then be queried for individual tasks. There are
strong parallels between this widespread framework and offline goal-conditioned
reinforcement learning algorithms: a universal value function is trained on a
large number of goals, and the policy is evaluated on a single goal in each
test episode. Extensive research in foundation models has shown that
performance can be substantially improved through test-time training,
specializing the model to the current goal. We find similarly that test-time
offline reinforcement learning on experience related to the test goal can lead
to substantially better policies at minimal compute costs. We propose a novel
self-supervised data selection criterion, which selects transitions from an
offline dataset according to their relevance to the current state and quality
with respect to the evaluation goal. We demonstrate across a wide range of
high-dimensional loco-navigation and manipulation tasks that fine-tuning a
policy on the selected data for a few gradient steps leads to significant
performance gains over standard offline pre-training. Our goal-conditioned
test-time training (GC-TTT) algorithm applies this routine in a
receding-horizon fashion during evaluation, adapting the policy to the current
trajectory as it is being rolled out. Finally, we study compute allocation at
inference, demonstrating that, at comparable costs, GC-TTT induces performance
gains that are not achievable by scaling model size.

</details>


### [61] [Even Faster Simulations with Flow Matching: A Study of Zero Degree Calorimeter Responses](https://arxiv.org/abs/2507.18811)
*Maksymilian Wojnar*

Main category: cs.LG

TL;DR: 本文利用流匹配（FM）为ALICE实验中的零度量能器开发快速仿真替代模型，训练策略有效，减少计算成本，仿真保真度达最优。


<details>
  <summary>Details</summary>
Motivation: 利用生成式神经网络进展，尤其是FM，加速高能物理仿真，满足研究机构计算需求。

Method: 利用FM开发替代模型，提出有效训练策略，训练低参数快速生成模型。

Result: 在中子和质子探测器仿真中达最优保真度，减少计算成本，如ZN仿真中Wasserstein距离1.27，推理时间0.46ms/样本；ZP仿真中Wasserstein距离1.30，优于当前最佳值。潜在FM模型提高推理速度。

Conclusion: 所提方法能实现高能物理中零度量能器快速仿真，减少计算成本，有良好效果。

Abstract: Recent advances in generative neural networks, particularly flow matching
(FM), have enabled the generation of high-fidelity samples while significantly
reducing computational costs. A promising application of these models is
accelerating simulations in high-energy physics (HEP), helping research
institutions meet their increasing computational demands. In this work, we
leverage FM to develop surrogate models for fast simulations of zero degree
calorimeters in the ALICE experiment. We present an effective training strategy
that enables the training of fast generative models with an exceptionally low
number of parameters. This approach achieves state-of-the-art simulation
fidelity for both neutron (ZN) and proton (ZP) detectors, while offering
substantial reductions in computational costs compared to existing methods. Our
FM model achieves a Wasserstein distance of 1.27 for the ZN simulation with an
inference time of 0.46 ms per sample, compared to the current best of 1.20 with
an inference time of approximately 109 ms. The latent FM model further improves
the inference speed, reducing the sampling time to 0.026 ms per sample, with a
minimal trade-off in accuracy. Similarly, our approach achieves a Wasserstein
distance of 1.30 for the ZP simulation, outperforming the current best of 2.08.
The source code is available at https://github.com/m-wojnar/faster_zdc.

</details>


### [62] [Scale-Consistent Learning for Partial Differential Equations](https://arxiv.org/abs/2507.18813)
*Zongyi Li,Samuel Lanthaler,Catherine Deng,Michael Chen,Yixuan Wang,Kamyar Azizzadenesheli,Anima Anandkumar*

Main category: cs.LG

TL;DR: 提出基于PDE尺度一致性的增广方案和尺度感知神经算子，可处理多尺度问题，实验表明能提升泛化能力和降低误差。


<details>
  <summary>Details</summary>
Motivation: 以往机器学习模型在求解偏微分方程时泛化能力不足，如训练好的模型只能处理固定雷诺数。

Method: 基于PDE尺度一致性特性提出数据增广方案，设计尺度感知神经算子，引入尺度一致性损失。

Result: 在多个方程上实验，训练于雷诺数1000的模型能泛化到250 - 10000，平均误差比基线降低34%。

Conclusion: 所提方法能有效提升模型泛化能力，降低误差。

Abstract: Machine learning (ML) models have emerged as a promising approach for solving
partial differential equations (PDEs) in science and engineering. Previous ML
models typically cannot generalize outside the training data; for example, a
trained ML model for the Navier-Stokes equations only works for a fixed
Reynolds number ($Re$) on a pre-defined domain. To overcome these limitations,
we propose a data augmentation scheme based on scale-consistency properties of
PDEs and design a scale-informed neural operator that can model a wide range of
scales. Our formulation leverages the facts: (i) PDEs can be rescaled, or more
concretely, a given domain can be re-scaled to unit size, and the parameters
and the boundary conditions of the PDE can be appropriately adjusted to
represent the original solution, and (ii) the solution operators on a given
domain are consistent on the sub-domains. We leverage these facts to create a
scale-consistency loss that encourages matching the solutions evaluated on a
given domain and the solution obtained on its sub-domain from the rescaled PDE.
Since neural operators can fit to multiple scales and resolutions, they are the
natural choice for incorporating scale-consistency loss during training of
neural PDE solvers. We experiment with scale-consistency loss and the
scale-informed neural operator model on the Burgers' equation, Darcy Flow,
Helmholtz equation, and Navier-Stokes equations. With scale-consistency, the
model trained on $Re$ of 1000 can generalize to $Re$ ranging from 250 to 10000,
and reduces the error by 34% on average of all datasets compared to baselines.

</details>


### [63] [Weak-to-Strong Generalization with Failure Trajectories: A Tree-based Approach to Elicit Optimal Policy in Strong Models](https://arxiv.org/abs/2507.18858)
*Ruimeng Ye,Zihan Wang,Xiao Yang,Zinan Ling,Manling Li,Bo Hui*

Main category: cs.LG

TL;DR: 本文将弱到强泛化（W2SG）范式拓展到复杂交互决策环境，提出泛化失败经验、构建轨迹树并结合MCTS优化强模型，理论与实验验证方法有效性。


<details>
  <summary>Details</summary>
Motivation: 现有W2SG研究聚焦简单任务，本文旨在将其拓展到复杂交互决策环境，充分挖掘强模型能力。

Method: 用弱模型生成的中间动作轨迹微调强模型，泛化失败经验，构建“轨迹树”并结合蒙特卡罗树搜索（MCTS）优化强模型。

Result: 理论分析为方法有效性提供正式保证，实证评估显示在不同任务领域的推理和决策能力有显著提升。

Conclusion: 所提出的框架具有可扩展性和鲁棒性，能有效提升W2SG性能。

Abstract: Weak-to-Strong generalization (W2SG) is a new trend to elicit the full
capabilities of a strong model with supervision from a weak model. While
existing W2SG studies focus on simple tasks like binary classification, we
extend this paradigm to complex interactive decision-making environments.
Specifically, we fine-tune a strong model with trajectories of intermediate
actions generated by a weak model. Motivated by the human learning process, we
propose to generalize not only success knowledge but also failure experience so
that the strong model can learn from failed trajectories accumulated by weak
models. To effectively and efficiently elicit the potential of strong agents,
we further construct ``trajectory trees," a hierarchical representation that
organizes weak model-generated action trajectories, coupled with Monte Carlo
Tree Search (MCTS) to optimize the strong model. Through theoretical analysis,
we provide formal guarantees for the effectiveness of our method in improving
W2SG performance. Our empirical evaluations demonstrate substantial
improvements in reasoning and decision-making capabilities across diverse task
domains, validating the scalability and robustness of our proposed framework.
Our code is available at: https://github.com/yeruimeng/TraTree

</details>


### [64] [Early Mortality Prediction in ICU Patients with Hypertensive Kidney Disease Using Interpretable Machine Learning](https://arxiv.org/abs/2507.18866)
*Yong Si,Junyi Fan,Li Sun,Shuheng Chen,Minoo Ahmadi,Elham Pishgar,Kamiar Alaei,Greg Placencia,Maryam Pishgar*

Main category: cs.LG

TL;DR: 本文开发机器学习框架预测ICU中高血压肾病患者30天院内死亡率，CatBoost模型表现最佳，还结合算法评估不确定性，该模型支持个性化分流和临床决策。


<details>
  <summary>Details</summary>
Motivation: ICU中高血压肾病患者短期死亡率高，但缺乏针对性风险预测工具，早期识别高危个体对临床决策至关重要。

Method: 使用MIMIC - IV v2.2数据库的早期临床数据，筛选1366名成年患者，通过随机森林重要性和互信息过滤选择18个临床特征，用分层五折交叉验证训练并比较多个模型。

Result: CatBoost在独立测试集上AUROC为0.88，敏感性0.811，特异性0.798；通过SHAP值和ALE图显示模型依赖有意义的预测因子；集成DREAM算法评估患者特定的后验风险分布。

Conclusion: 提出可解释的机器学习流程用于早期实时风险评估，结合高预测性能和不确定性量化，支持个性化分流和透明临床决策，有临床应用前景，需外部验证。

Abstract: Background: Hypertensive kidney disease (HKD) patients in intensive care
units (ICUs) face high short-term mortality, but tailored risk prediction tools
are lacking. Early identification of high-risk individuals is crucial for
clinical decision-making. Methods: We developed a machine learning framework to
predict 30-day in-hospital mortality among ICU patients with HKD using early
clinical data from the MIMIC-IV v2.2 database. A cohort of 1,366 adults was
curated with strict criteria, excluding malignancy cases. Eighteen clinical
features-including vital signs, labs, comorbidities, and therapies-were
selected via random forest importance and mutual information filtering. Several
models were trained and compared with stratified five-fold cross-validation;
CatBoost demonstrated the best performance. Results: CatBoost achieved an AUROC
of 0.88 on the independent test set, with sensitivity of 0.811 and specificity
of 0.798. SHAP values and Accumulated Local Effects (ALE) plots showed the
model relied on meaningful predictors such as altered consciousness,
vasopressor use, and coagulation status. Additionally, the DREAM algorithm was
integrated to estimate patient-specific posterior risk distributions, allowing
clinicians to assess both predicted mortality and its uncertainty. Conclusions:
We present an interpretable machine learning pipeline for early, real-time risk
assessment in ICU patients with HKD. By combining high predictive performance
with uncertainty quantification, our model supports individualized triage and
transparent clinical decisions. This approach shows promise for clinical
deployment and merits external validation in broader critical care populations.

</details>


### [65] [Learning Individual Intrinsic Reward in Multi-Agent Reinforcement Learning via Incorporating Generalized Human Expertise](https://arxiv.org/abs/2507.18867)
*Xuefei Wu,Xiao Yin,Yuanyang Zhu,Chunlin Chen*

Main category: cs.LG

TL;DR: 提出LIGHT框架解决多智能体强化学习探索难题，结合团队奖励与人类知识，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习在仅接收团队奖励、稀疏奖励环境中探索困难，现有个体奖励依赖手动设计函数，学习和泛化能力不足。

Method: 提出LIGHT框架，将人类知识端到端集成到多智能体强化学习算法中，考虑个体动作分布和人类专业偏好分布，基于可操作表征变换设计个体内在奖励。

Result: 实验表明该方法在性能上优于代表性基线，在不同稀疏奖励任务中有更好的知识复用性。

Conclusion: LIGHT框架有效，能在多智能体强化学习中结合人类知识实现高效探索。

Abstract: Efficient exploration in multi-agent reinforcement learning (MARL) is a
challenging problem when receiving only a team reward, especially in
environments with sparse rewards. A powerful method to mitigate this issue
involves crafting dense individual rewards to guide the agents toward efficient
exploration. However, individual rewards generally rely on manually engineered
shaping-reward functions that lack high-order intelligence, thus it behaves
ineffectively than humans regarding learning and generalization in complex
problems. To tackle these issues, we combine the above two paradigms and
propose a novel framework, LIGHT (Learning Individual Intrinsic reward via
Incorporating Generalized Human experTise), which can integrate human knowledge
into MARL algorithms in an end-to-end manner. LIGHT guides each agent to avoid
unnecessary exploration by considering both individual action distribution and
human expertise preference distribution. Then, LIGHT designs individual
intrinsic rewards for each agent based on actionable representational
transformation relevant to Q-learning so that the agents align their action
preferences with the human expertise while maximizing the joint action value.
Experimental results demonstrate the superiority of our method over
representative baselines regarding performance and better knowledge reusability
across different sparse-reward tasks on challenging scenarios.

</details>


### [66] [Geometric Multi-color Message Passing Graph Neural Networks for Blood-brain Barrier Permeability Prediction](https://arxiv.org/abs/2507.18926)
*Trung Nguyen,Md Masud Rana,Farjana Tasnim Mukta,Chang-Guo Zhan,Duc Duy Nguyen*

Main category: cs.LG

TL;DR: 本文提出GMC - MPNN模型用于血脑屏障通透性预测，在分类和回归任务中表现优于现有模型，为药物发现提供更准确工具。


<details>
  <summary>Details</summary>
Motivation: 准确预测血脑屏障通透性对中枢神经系统药物开发至关重要，现有图神经网络常忽略三维几何信息。

Method: 引入GMC - MPNN框架，结合原子级几何特征和长程相互作用，构建加权彩色子图，在三个基准数据集上用基于支架的分割方法评估。

Result: GMC - MPNN在分类和回归任务中始终优于现有模型，分类AUC - ROC分别为0.9704和0.9685，回归RMSE为0.4609，皮尔逊相关系数为0.7759。

Conclusion: 将空间几何信息集成到图表示中，GMC - MPNN设定了新的性能基准，为药物发现管道提供更准确、更具泛化性的工具。

Abstract: Accurate prediction of blood-brain barrier permeability (BBBP) is essential
for central nervous system (CNS) drug development. While graph neural networks
(GNNs) have advanced molecular property prediction, they often rely on
molecular topology and neglect the three-dimensional geometric information
crucial for modeling transport mechanisms. This paper introduces the geometric
multi-color message-passing graph neural network (GMC-MPNN), a novel framework
that enhances standard message-passing architectures by explicitly
incorporating atomic-level geometric features and long-range interactions. Our
model constructs weighted colored subgraphs based on atom types to capture the
spatial relationships and chemical context that govern BBB permeability. We
evaluated GMC-MPNN on three benchmark datasets for both classification and
regression tasks, using rigorous scaffold-based splitting to ensure a robust
assessment of generalization. The results demonstrate that GMC-MPNN
consistently outperforms existing state-of-the-art models, achieving superior
performance in both classifying compounds as permeable/non-permeable (AUC-ROC
of 0.9704 and 0.9685) and in regressing continuous permeability values (RMSE of
0.4609, Pearson correlation of 0.7759). An ablation study further quantified
the impact of specific atom-pair interactions, revealing that the model's
predictive power derives from its ability to learn from both common and rare,
but chemically significant, functional motifs. By integrating spatial geometry
into the graph representation, GMC-MPNN sets a new performance benchmark and
offers a more accurate and generalizable tool for drug discovery pipelines.

</details>


### [67] [Secure Best Arm Identification in the Presence of a Copycat](https://arxiv.org/abs/2507.18975)
*Asaf Cohen,Onur Günlü*

Main category: cs.LG

TL;DR: 本文研究带安全约束的最佳臂识别问题，提出一种使用编码臂的安全算法，不依赖密钥或加密原语，实现了Ω(T/log²(d))的指数且几乎不泄露最佳臂信息。


<details>
  <summary>Details</summary>
Motivation: 现有算法在最佳臂识别时存在信息泄露问题，玩家希望在有模仿者观察的情况下，既识别出最佳臂又不泄露相关信息。

Method: 提出一种使用编码臂的安全算法，且该算法无需密钥或加密原语。

Result: 该算法实现了Ω(T/log²(d))的指数，且几乎不泄露最佳臂信息。

Conclusion: 提出的安全算法在解决带安全约束的最佳臂识别问题上表现良好，兼顾识别效果与信息安全。

Abstract: Consider the problem of best arm identification with a security constraint.
Specifically, assume a setup of stochastic linear bandits with $K$ arms of
dimension $d$. In each arm pull, the player receives a reward that is the sum
of the dot product of the arm with an unknown parameter vector and independent
noise. The player's goal is to identify the best arm after $T$ arm pulls.
Moreover, assume a copycat Chloe is observing the arm pulls. The player wishes
to keep Chloe ignorant of the best arm.
  While a minimax--optimal algorithm identifies the best arm with an
$\Omega\left(\frac{T}{\log(d)}\right)$ error exponent, it easily reveals its
best-arm estimate to an outside observer, as the best arms are played more
frequently. A naive secure algorithm that plays all arms equally results in an
$\Omega\left(\frac{T}{d}\right)$ exponent. In this paper, we propose a secure
algorithm that plays with \emph{coded arms}. The algorithm does not require any
key or cryptographic primitives, yet achieves an
$\Omega\left(\frac{T}{\log^2(d)}\right)$ exponent while revealing almost no
information on the best arm.

</details>


### [68] [KASPER: Kolmogorov Arnold Networks for Stock Prediction and Explainable Regimes](https://arxiv.org/abs/2507.18983)
*Vidhi Oad,Param Pathak,Nouhaila Innan,Shalini D,Muhammad Shafique*

Main category: cs.LG

TL;DR: 提出KASPER框架用于股票预测和可解释制度分析，应用于真实金融时间序列表现出色，开创金融市场预测新方向。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型难以在变化的市场条件下泛化，需要更具适应性和可解释性的方法。

Method: 引入KASPER框架，结合制度检测、基于稀疏样条的函数建模和符号规则提取，用Gumbel - Softmax机制识别市场条件，用带稀疏样条激活的Kolmogorov - Arnold网络进行预测，通过基于蒙特卡罗Shapley值的符号学习实现可解释性。

Result: 应用于真实金融时间序列，R²分数达0.89，夏普比率为12.02，均方误差低至0.0001，优于现有方法。

Conclusion: 该研究为金融市场中具有制度感知、透明和稳健的预测建立了新方向。

Abstract: Forecasting in financial markets remains a significant challenge due to their
nonlinear and regime-dependent dynamics. Traditional deep learning models, such
as long short-term memory networks and multilayer perceptrons, often struggle
to generalize across shifting market conditions, highlighting the need for a
more adaptive and interpretable approach. To address this, we introduce
Kolmogorov-Arnold networks for stock prediction and explainable regimes
(KASPER), a novel framework that integrates regime detection, sparse
spline-based function modeling, and symbolic rule extraction. The framework
identifies hidden market conditions using a Gumbel-Softmax-based mechanism,
enabling regime-specific forecasting. For each regime, it employs
Kolmogorov-Arnold networks with sparse spline activations to capture intricate
price behaviors while maintaining robustness. Interpretability is achieved
through symbolic learning based on Monte Carlo Shapley values, which extracts
human-readable rules tailored to each regime. Applied to real-world financial
time series from Yahoo Finance, the model achieves an $R^2$ score of 0.89, a
Sharpe Ratio of 12.02, and a mean squared error as low as 0.0001, outperforming
existing methods. This research establishes a new direction for regime-aware,
transparent, and robust forecasting in financial markets.

</details>


### [69] [Differentiated Thyroid Cancer Recurrence Classification Using Machine Learning Models and Bayesian Neural Networks with Varying Priors: A SHAP-Based Interpretation of the Best Performing Model](https://arxiv.org/abs/2507.18987)
*HMNS Kumari,HMLS Kumari,UMMPK Nawarathne*

Main category: cs.LG

TL;DR: 本文提出DTC复发分类综合框架，对比不同模型在完整和降维数据集上的准确率，BNN模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: DTC复发需准确、可解释且能感知不确定性的分类和预测模型。

Method: 先使用11种ML模型在完整数据集上分类，用Boruta算法降维后再应用这些模型，最后在两个数据集上应用6种不同先验分布的BNN模型。

Result: SVM在完整数据集准确率0.9481，LR在降维数据集准确率0.9611，先验分布为Normal 0,10的BNN在特征选择前后准确率分别达0.9740和0.9870。

Conclusion: 先验分布为Normal 0,10的BNN模型在DTC复发分类中表现最佳。

Abstract: Differentiated thyroid cancer DTC recurrence is a major public health
concern, requiring classification and predictive models that are not only
accurate but also interpretable and uncertainty aware. This study introduces a
comprehensive framework for DTC recurrence classification using a dataset
containing 383 patients and 16 clinical and pathological variables. Initially,
11 machine learning ML models were employed using the complete dataset, where
the Support Vector Machines SVM model achieved the highest accuracy of 0.9481.
To reduce complexity and redundancy, feature selection was carried out using
the Boruta algorithm, and the same ML models were applied to the reduced
dataset, where it was observed that the Logistic Regression LR model obtained
the maximum accuracy of 0.9611. However, these ML models often lack uncertainty
quantification, which is critical in clinical decision making. Therefore, to
address this limitation, the Bayesian Neural Networks BNN with six varying
prior distributions, including Normal 0,1, Normal 0,10, Laplace 0,1, Cauchy
0,1, Cauchy 0,2.5, and Horseshoe 1, were implemented on both the complete and
reduced datasets. The BNN model with Normal 0,10 prior distribution exhibited
maximum accuracies of 0.9740 and 0.9870 before and after feature selection,
respectively.

</details>


### [70] [GENIAL: Generative Design Space Exploration via Network Inversion for Low Power Algorithmic Logic Units](https://arxiv.org/abs/2507.18989)
*Maxence Bouvier,Ryan Amaudruz,Felix Arnold,Renzo Andri,Lukas Cavigelli*

Main category: cs.LG

TL;DR: 本文介绍了基于机器学习的框架GENIAL用于自动生成和优化算术单元，实验表明它比其他方法更高效，能发现可节能的编码，适用于多种逻辑函数。


<details>
  <summary>Details</summary>
Motivation: 随着AI工作负载增加，传统手动或基于启发式的算术单元优化设计流程难以全面探索设计空间，需新方法。

Method: 构建基于Transformer的代理模型，分两阶段训练，通过反转模型搜索新操作数编码以降低功耗。

Result: 在大数据集实验中，GENIAL比其他方法样本效率更高、收敛更快，能使乘法器开关活动节省达18%，在有限状态机上也有显著改进。

Conclusion: 这些进展是数字系统实现自动结果质量优化组合电路生成的重要一步。

Abstract: As AI workloads proliferate, optimizing arithmetic units is becoming
increasingly important to reduce the footprint of digital systems. Conventional
design flows, which often rely on manual or heuristics-based optimization, are
limited in their ability to thoroughly explore the vast design space. In this
paper, we introduce GENIAL, a machine learning-based framework for the
automatic generation and optimization of arithmetic units, more specifically
multipliers.
  At the core of GENIAL is a Transformer-based surrogate model trained in two
stages, involving self-supervised pretraining followed by supervised
finetuning, to robustly forecast key hardware metrics such as power and area
from abstracted design representations. By inverting the surrogate model,
GENIAL efficiently searches for new operand encodings that directly minimize
power consumption in arithmetic units for specific input data distributions.
Extensive experiments on large datasets demonstrate that GENIAL is consistently
more sample efficient than other methods, and converges faster towards
optimized designs. This enables to deploy a high-effort logic synthesis
optimization flow in the loop, improving the accuracy of the surrogate model.
Notably, GENIAL automatically discovers encodings that achieve up to 18%
switching activity savings within multipliers on representative AI workloads
compared with the conventional two's complement. We also demonstrate the
versatility of our approach by achieving significant improvements on Finite
State Machines, highlighting GENIAL's applicability for a wide spectrum of
logic functions. Together, these advances mark a significant step toward
automated Quality-of-Results-optimized combinational circuit generation for
digital systems.

</details>


### [71] [Reinforcement Learning via Conservative Agent for Environments with Random Delays](https://arxiv.org/abs/2507.18992)
*Jongsoo Lee,Jangwon Kim,Jiseok Jeong,Soohee Han*

Main category: cs.LG

TL;DR: 提出保守代理应对随机延迟环境，将其转化为恒定延迟环境，实验显示性能优于基线算法。


<details>
  <summary>Details</summary>
Motivation: 现实强化学习应用受环境延迟反馈阻碍，随机延迟环境研究不足。

Method: 提出保守代理，将随机延迟环境转化为恒定延迟等效环境。

Result: 在连续控制任务上评估，该算法在渐近性能和样本效率上显著优于现有基线算法。

Conclusion: 所提保守代理可将恒定延迟方法扩展到随机延迟环境，且不改变算法结构和牺牲性能。

Abstract: Real-world reinforcement learning applications are often hindered by delayed
feedback from environments, which violates the Markov assumption and introduces
significant challenges. Although numerous delay-compensating methods have been
proposed for environments with constant delays, environments with random delays
remain largely unexplored due to their inherent variability and
unpredictability. In this study, we propose a simple yet robust agent for
decision-making under random delays, termed the conservative agent, which
reformulates the random-delay environment into its constant-delay equivalent.
This transformation enables any state-of-the-art constant-delay method to be
directly extended to the random-delay environments without modifying the
algorithmic structure or sacrificing performance. We evaluate the conservative
agent-based algorithm on continuous control tasks, and empirical results
demonstrate that it significantly outperforms existing baseline algorithms in
terms of asymptotic performance and sample efficiency.

</details>


### [72] [Adapting to Fragmented and Evolving Data: A Fisher Information Perspective](https://arxiv.org/abs/2507.18996)
*Behraj Khan,Tahir Qasim Syed,Nouman Muhammad Durrani*

Main category: cs.LG

TL;DR: 提出FADE框架应对顺序协变量偏移问题，在多基准测试中表现出色，能自然扩展到联邦学习，有理论保证和实证验证。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习系统在动态环境中面临顺序协变量偏移问题，需要一种有效方法进行鲁棒学习。

Method: 引入基于Fisher信息几何的移位感知正则化机制，提出Cramer - Rao信息移位信号检测分布变化，在线操作，无需任务边界、目标监督或经验回放。

Result: 在七个基准测试中，严重偏移下准确率最高提升19%，优于TENT和DIW等方法，能自然扩展到联邦学习。

Conclusion: 理论分析保证有界遗憾和参数一致性，实证结果证明FADE在多模态和不同移位强度下具有鲁棒性。

Abstract: Modern machine learning systems operating in dynamic environments often face
\textit{sequential covariate shift} (SCS), where input distributions evolve
over time while the conditional distribution remains stable. We introduce FADE
(Fisher-based Adaptation to Dynamic Environments), a lightweight and
theoretically grounded framework for robust learning under SCS. FADE employs a
shift-aware regularization mechanism anchored in Fisher information geometry,
guiding adaptation by modulating parameter updates based on sensitivity and
stability. To detect significant distribution changes, we propose a
Cramer-Rao-informed shift signal that integrates KL divergence with temporal
Fisher dynamics. Unlike prior methods requiring task boundaries, target
supervision, or experience replay, FADE operates online with fixed memory and
no access to target labels. Evaluated on seven benchmarks spanning vision,
language, and tabular data, FADE achieves up to 19\% higher accuracy under
severe shifts, outperforming methods such as TENT and DIW. FADE also
generalizes naturally to federated learning by treating heterogeneous clients
as temporally fragmented environments, enabling scalable and stable adaptation
in decentralized settings. Theoretical analysis guarantees bounded regret and
parameter consistency, while empirical results demonstrate FADE's robustness
across modalities and shift intensities.

</details>


### [73] [A diffusion-based generative model for financial time series via geometric Brownian motion](https://arxiv.org/abs/2507.19003)
*Gihun Kim,Sun-Yong Choi,Yeoneung Kim*

Main category: cs.LG

TL;DR: 提出结合几何布朗运动的金融时间序列生成框架，用Transformer架构训练，实证显示比传统模型更能重现关键特征。


<details>
  <summary>Details</summary>
Motivation: 传统分数模型将价格轨迹视为通用数值序列，未考虑金融时间序列异方差性，需改进生成模型。

Method: 将几何布朗运动融入前向加噪过程，按资产价格比例加噪，通过去噪分数匹配训练基于Transformer架构的反向生成过程。

Result: 模型在历史股票数据实证中比传统扩散模型更真实地重现重尾回报分布、波动率聚类和杠杆效应等关键特征。

Conclusion: 提出的结合几何布朗运动的扩散生成框架在金融时间序列生成上优于传统模型。

Abstract: We propose a novel diffusion-based generative framework for financial time
series that incorporates geometric Brownian motion (GBM), the foundation of the
Black--Scholes theory, into the forward noising process. Unlike standard
score-based models that treat price trajectories as generic numerical
sequences, our method injects noise proportionally to asset prices at each time
step, reflecting the heteroskedasticity observed in financial time series. By
accurately balancing the drift and diffusion terms, we show that the resulting
log-price process reduces to a variance-exploding stochastic differential
equation, aligning with the formulation in score-based generative models. The
reverse-time generative process is trained via denoising score matching using a
Transformer-based architecture adapted from the Conditional Score-based
Diffusion Imputation (CSDI) framework. Empirical evaluations on historical
stock data demonstrate that our model reproduces key stylized facts
heavy-tailed return distributions, volatility clustering, and the leverage
effect more realistically than conventional diffusion models.

</details>


### [74] [MindSpeed RL: Distributed Dataflow for Scalable and Efficient RL Training on Ascend NPU Cluster](https://arxiv.org/abs/2507.19017)
*Laingjun Feng,Chenyi Pan,Xinjie Guo,Fei Mei,Benzhe Ning,Jianxiang Zhang,Xinyang Liu,Beirong Zhou,Zeng Shu,Chang Liu,Guang Yang,Zhenyu Han,Jiangben Wang,Bo Wang*

Main category: cs.LG

TL;DR: 介绍MindSpeed RL系统用于大规模强化学习训练，通过分布式策略优化，实验显示比现有系统提升吞吐量，最后开源并在Ascend上展示性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习训练系统因跨节点依赖重，存在集群可扩展性差和内存利用率低的问题。

Method: MindSpeed RL从分布式视角组织数据依赖，设计分布式转移码头策略释放样本流调度开销，提出实用的allgather - swap策略消除重分片流冗余内存使用，并集成多种并行化策略和加速技术。

Result: 在流行模型的强化学习训练实验中，MindSpeed RL比现有系统吞吐量提升1.42 - 3.97倍。

Conclusion: MindSpeed RL性能强大且可靠，已开源并在Ascend上得到验证。

Abstract: Reinforcement learning (RL) is a paradigm increasingly used to align large
language models. Popular RL algorithms utilize multiple workers and can be
modeled as a graph, where each node is the status of a worker and each edge
represents dataflow between nodes. Owing to the heavy cross-node dependencies,
the RL training system usually suffers from poor cluster scalability and low
memory utilization. In this article, we introduce MindSpeed RL, an effective
and efficient system for large-scale RL training. Unlike existing centralized
methods, MindSpeed RL organizes the essential data dependencies in RL training,
i.e., sample flow and resharding flow, from a distributed view. On the one
hand, a distributed transfer dock strategy, which sets controllers and
warehouses on the basis of the conventional replay buffer, is designed to
release the dispatch overhead in the sample flow. A practical allgather--swap
strategy is presented to eliminate redundant memory usage in resharding flow.
In addition, MindSpeed RL further integrates numerous parallelization
strategies and acceleration techniques for systematic optimization. Compared
with existing state-of-the-art systems, comprehensive experiments on the RL
training of popular Qwen2.5-Dense-7B/32B, Qwen3-MoE-30B, and
DeepSeek-R1-MoE-671B show that MindSpeed RL increases the throughput by 1.42 ~
3.97 times. Finally, we open--source MindSpeed RL and perform all the
experiments on a super pod of Ascend with 384 neural processing units (NPUs) to
demonstrate the powerful performance and reliability of Ascend.

</details>


### [75] [ProGMLP: A Progressive Framework for GNN-to-MLP Knowledge Distillation with Efficient Trade-offs](https://arxiv.org/abs/2507.19031)
*Weigang Lu,Ziyu Guan,Wei Zhao,Yaming Yang,Yujie Sun,Zheng Liang,Yibing Zhan,Dapeng Tao*

Main category: cs.LG

TL;DR: 提出ProGMLP框架解决现有GNN - to - MLP方法无法灵活权衡推理成本和准确性的问题，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有GNN - to - MLP方法无法在推理成本和准确性之间进行灵活的动态调整，而实际应用中计算资源和时间限制差异大。

Method: 引入ProGMLP框架，采用渐进式训练结构（PTS）依次训练多个MLP学生，结合渐进式知识蒸馏（PKD）迭代优化蒸馏过程，使用渐进式混合增强（PMA）提升泛化能力。

Result: 在八个真实世界图数据集上的综合实验表明，ProGMLP能在动态适应不同运行时场景的同时保持高精度。

Conclusion: ProGMLP在不同应用场景中部署非常有效。

Abstract: GNN-to-MLP (G2M) methods have emerged as a promising approach to accelerate
Graph Neural Networks (GNNs) by distilling their knowledge into simpler
Multi-Layer Perceptrons (MLPs). These methods bridge the gap between the
expressive power of GNNs and the computational efficiency of MLPs, making them
well-suited for resource-constrained environments. However, existing G2M
methods are limited by their inability to flexibly adjust inference cost and
accuracy dynamically, a critical requirement for real-world applications where
computational resources and time constraints can vary significantly. To address
this, we introduce a Progressive framework designed to offer flexible and
on-demand trade-offs between inference cost and accuracy for GNN-to-MLP
knowledge distillation (ProGMLP). ProGMLP employs a Progressive Training
Structure (PTS), where multiple MLP students are trained in sequence, each
building on the previous one. Furthermore, ProGMLP incorporates Progressive
Knowledge Distillation (PKD) to iteratively refine the distillation process
from GNNs to MLPs, and Progressive Mixup Augmentation (PMA) to enhance
generalization by progressively generating harder mixed samples. Our approach
is validated through comprehensive experiments on eight real-world graph
datasets, demonstrating that ProGMLP maintains high accuracy while dynamically
adapting to varying runtime scenarios, making it highly effective for
deployment in diverse application settings.

</details>


### [76] [Neural Ordinary Differential Equations for Learning and Extrapolating System Dynamics Across Bifurcations](https://arxiv.org/abs/2507.19036)
*Eva van Tegelen,George van Voorn,Ioannis Athanasiadis,Peter van Heijster*

Main category: cs.LG

TL;DR: 使用神经常微分方程学习动力系统分岔结构，可从时间序列数据恢复分岔结构，能在训练数据参数区域外预测分岔，模型准确性更依赖训练数据信息质量。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习研究在预测动力系统分岔时局限于离散时间方法和局部分岔，需改进。

Method: 采用神经常微分方程，提供连续、数据驱动的框架学习系统动力学，并应用于具有局部和全局分岔的捕食 - 猎物系统。

Result: 神经常微分方程可从时间序列数据直接恢复潜在分岔结构，能在训练数据参数区域外预测分岔，模型准确性更依赖训练数据信息质量而非数量。

Conclusion: 神经常微分方程在预测动力系统分岔方面具有优势，能处理局部和全局分岔，且对数据数量要求不高。

Abstract: Forecasting system behaviour near and across bifurcations is crucial for
identifying potential shifts in dynamical systems. While machine learning has
recently been used to learn critical transitions and bifurcation structures
from data, most studies remain limited as they exclusively focus on
discrete-time methods and local bifurcations. To address these limitations, we
use Neural Ordinary Differential Equations which provide a continuous,
data-driven framework for learning system dynamics. We apply our approach to a
predator-prey system that features both local and global bifurcations,
presenting a challenging test case. Our results show that Neural Ordinary
Differential Equations can recover underlying bifurcation structures directly
from timeseries data by learning parameter-dependent vector fields. Notably, we
demonstrate that Neural Ordinary Differential Equations can forecast
bifurcations even beyond the parameter regions represented in the training
data. We also assess the method's performance under limited and noisy data
conditions, finding that model accuracy depends more on the quality of
information that can be inferred from the training data, than on the amount of
data available.

</details>


### [77] [Dynamics-Informed Reservoir Computing with Visibility Graphs](https://arxiv.org/abs/2507.19046)
*Charlotte Geier,Merten Stender*

Main category: cs.LG

TL;DR: 提出动力学感知水库计算（DyRC）框架，用可见性图（VG）技术构建水库网络，在预测任务中表现优于同规模的Erdős - Rényi图。


<details>
  <summary>Details</summary>
Motivation: 传统水库计算（RC）的随机水库图架构常导致次优和过大网络，且动力学难以理解，需改进。

Method: 提出DyRC框架，采用VG技术将时间序列数据转换为网络，直接用训练数据序列的VG网络构建水库网络，避免超参数调整。

Result: 在规范非线性Duffing振子预测任务中，DyRC - VG方法比同规模、谱半径和密度的Erdős - Rényi图有更高预测质量和更稳定的性能。

Conclusion: DyRC - VG方法能有效解决传统RC存在的问题，提高时间序列预测的准确性和稳定性。

Abstract: Accurate prediction of complex and nonlinear time series remains a
challenging problem across engineering and scientific disciplines. Reservoir
computing (RC) offers a computationally efficient alternative to traditional
deep learning by training only the read-out layer while employing a randomly
structured and fixed reservoir network. Despite its advantages, the largely
random reservoir graph architecture often results in suboptimal and oversized
networks with poorly understood dynamics. Addressing this issue, we propose a
novel Dynamics-Informed Reservoir Computing (DyRC) framework that
systematically infers the reservoir network structure directly from the input
training sequence. This work proposes to employ the visibility graph (VG)
technique, which converts time series data into networks by representing
measurement points as nodes linked by mutual visibility. The reservoir network
is constructed by directly adopting the VG network from a training data
sequence, leveraging the parameter-free visibility graph approach to avoid
expensive hyperparameter tuning. This process results in a reservoir that is
directly informed by the specific dynamics of the prediction task under study.
We assess the DyRC-VG method through prediction tasks involving the canonical
nonlinear Duffing oscillator, evaluating prediction accuracy and consistency.
Compared to an Erd\H{o}s-R\'enyi graph of the same size, spectral radius, and
comparable density, we observe higher prediction quality and more consistent
performance over repeated implementations in the DyRC-VG.

</details>


### [78] [Exploring molecular assembly as a biosignature using mass spectrometry and machine learning](https://arxiv.org/abs/2507.19057)
*Lindsay A. Rutter,Abhishek Sharma,Ian Seet,David Obeh Alobo,An Goto,Leroy Cronin*

Main category: cs.LG

TL;DR: 本文介绍分子组装用于地外生命探测，开发机器学习模型预测分子组装，指出标准化质谱数据库对未来天体生物学任务有重要意义。


<details>
  <summary>Details</summary>
Motivation: 质谱仪是太阳系任务核心，需在不解析未知结构情况下从其数据预测分子组装以实现无偏生命探测。

Method: 提出分子组装方法，开发机器学习模型预测分子组装。

Result: 模型预测分子组装准确率高，误差比基线模型降低三倍，模拟数据显示仪器不一致会使误差加倍。

Conclusion: 标准化质谱数据库可在不解析结构情况下准确预测分子组装，为未来天体生物学任务提供概念验证。

Abstract: Molecular assembly offers a promising path to detect life beyond Earth, while
minimizing assumptions based on terrestrial life. As mass spectrometers will be
central to upcoming Solar System missions, predicting molecular assembly from
their data without needing to elucidate unknown structures will be essential
for unbiased life detection. An ideal agnostic biosignature must be
interpretable and experimentally measurable. Here, we show that molecular
assembly, a recently developed approach to measure objects that have been
produced by evolution, satisfies both criteria. First, it is interpretable for
life detection, as it reflects the assembly of molecules with their bonds as
building blocks, in contrast to approaches that discount construction history.
Second, it can be determined without structural elucidation, as it can be
physically measured by mass spectrometry, a property that distinguishes it from
other approaches that use structure-based information measures for molecular
complexity. Whilst molecular assembly is directly measurable using mass
spectrometry data, there are limits imposed by mission constraints. To address
this, we developed a machine learning model that predicts molecular assembly
with high accuracy, reducing error by three-fold compared to baseline models.
Simulated data shows that even small instrumental inconsistencies can double
model error, emphasizing the need for standardization. These results suggest
that standardized mass spectrometry databases could enable accurate molecular
assembly prediction, without structural elucidation, providing a
proof-of-concept for future astrobiology missions.

</details>


### [79] [Clustering-Oriented Generative Attribute Graph Imputation](https://arxiv.org/abs/2507.19085)
*Mulin Chen,Bocheng Wang,Jiaxin Zhong,Zongcheng Miao,Xuelong Li*

Main category: cs.LG

TL;DR: 提出面向聚类的生成式插补与可靠细化（CGIR）模型解决属性缺失图聚类问题，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有属性缺失图聚类模型的插补方法难以捕捉类相关语义信息，细化策略忽略部分属性与图不相关的问题，导致聚类效果不佳。

Method: 建立CGIR模型，估计子簇分布约束生成对抗模块采样空间进行插补，合并多个子簇引导边注意力网络进行细化。

Result: 广泛实验证明CGIR优于现有先进的竞争对手。

Conclusion: CGIR将属性缺失图聚类拆分为子簇搜索和合并，在统一框架内实现节点插补和细化。

Abstract: Attribute-missing graph clustering has emerged as a significant unsupervised
task, where only attribute vectors of partial nodes are available and the graph
structure is intact. The related models generally follow the two-step paradigm
of imputation and refinement. However, most imputation approaches fail to
capture class-relevant semantic information, leading to sub-optimal imputation
for clustering. Moreover, existing refinement strategies optimize the learned
embedding through graph reconstruction, while neglecting the fact that some
attributes are uncorrelated with the graph. To remedy the problems, we
establish the Clustering-oriented Generative Imputation with reliable
Refinement (CGIR) model. Concretely, the subcluster distributions are estimated
to reveal the class-specific characteristics precisely, and constrain the
sampling space of the generative adversarial module, such that the imputation
nodes are impelled to align with the correct clusters. Afterwards, multiple
subclusters are merged to guide the proposed edge attention network, which
identifies the edge-wise attributes for each class, so as to avoid the
redundant attributes in graph reconstruction from disturbing the refinement of
overall embedding. To sum up, CGIR splits attribute-missing graph clustering
into the search and mergence of subclusters, which guides to implement node
imputation and refinement within a unified framework. Extensive experiments
prove the advantages of CGIR over state-of-the-art competitors.

</details>


### [80] [GCL-GCN: Graphormer and Contrastive Learning Enhanced Attributed Graph Clustering Network](https://arxiv.org/abs/2507.19095)
*Binxiong Li,Xu Xiang,Xue Li,Binyu Zhao,Yujie Liu,Huijie Tang,Benhan Yang,Zhixuan Chen*

Main category: cs.LG

TL;DR: 本文提出GCL - GCN图聚类模型，结合Graphormer模块和对比学习模块，实验表明其在聚类质量和鲁棒性上优于14种先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有模型处理稀疏和异构图数据时在捕捉局部依赖和复杂结构方面存在局限，需新的图聚类模型。

Method: 提出GCL - GCN模型，引入结合中心性编码和空间关系的Graphormer模块，以及增强特征表示判别力的对比学习模块。

Result: 在六个数据集上实验，GCL - GCN在聚类质量和鲁棒性上优于14种先进方法，在Cora数据集上，ACC、NMI和ARI较MBN分别提升4.94%、13.01%和10.97%。

Conclusion: GCL - GCN模型在图聚类任务中有效，能提升聚类质量和鲁棒性。

Abstract: Attributed graph clustering holds significant importance in modern data
analysis. However, due to the complexity of graph data and the heterogeneity of
node attributes, leveraging graph information for clustering remains
challenging. To address this, we propose a novel deep graph clustering model,
GCL-GCN, specifically designed to address the limitations of existing models in
capturing local dependencies and complex structures when dealing with sparse
and heterogeneous graph data. GCL-GCN introduces an innovative Graphormer
module that combines centrality encoding and spatial relationships, effectively
capturing both global and local information between nodes, thereby enhancing
the quality of node representations. Additionally, we propose a novel
contrastive learning module that significantly enhances the discriminative
power of feature representations. In the pre-training phase, this module
increases feature distinction through contrastive learning on the original
feature matrix, ensuring more identifiable initial representations for
subsequent graph convolution and clustering tasks. Extensive experimental
results on six datasets demonstrate that GCL-GCN outperforms 14 advanced
methods in terms of clustering quality and robustness. Specifically, on the
Cora dataset, it improves ACC, NMI, and ARI by 4.94%, 13.01%, and 10.97%,
respectively, compared to the primary comparison method MBN.

</details>


### [81] [Graph Structure Learning with Privacy Guarantees for Open Graph Data](https://arxiv.org/abs/2507.19116)
*Muhao Guo,Jiaqi Wu,Yang Weng,Yizheng Liao,Shengzhe Chen*

Main category: cs.LG

TL;DR: 针对大规模开放数据集隐私保护问题，提出基于高斯差分隐私的图数据隐私保护估计框架，实验显示在图学习中有良好表现。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私主要关注模型训练时噪声注入，数据发布阶段隐私保护不足，且现有隐私保护数据发布方法难平衡隐私和效用。

Method: 聚焦图恢复问题，提出利用高斯差分隐私和结构化噪声注入机制的隐私保护估计框架，在数据发布阶段执行差分隐私。

Result: 实验结果表明在图学习中性能稳健。

Conclusion: 该框架为注重隐私的图分析提供了可行解决方案。

Abstract: Ensuring privacy in large-scale open datasets is increasingly challenging
under regulations such as the General Data Protection Regulation (GDPR). While
differential privacy (DP) provides strong theoretical guarantees, it primarily
focuses on noise injection during model training, neglecting privacy
preservation at the data publishing stage. Existing privacy-preserving data
publishing (PPDP) approaches struggle to balance privacy and utility,
particularly when data publishers and users are distinct entities. To address
this gap, we focus on the graph recovery problem and propose a novel
privacy-preserving estimation framework for open graph data, leveraging
Gaussian DP (GDP) with a structured noise-injection mechanism. Unlike
traditional methods that perturb gradients or model updates, our approach
ensures unbiased graph structure recovery while enforcing DP at the data
publishing stage. Moreover, we provide theoretical guarantees on estimation
accuracy and extend our method to discrete-variable graphs, a setting often
overlooked in DP research. Experimental results in graph learning demonstrate
robust performance, offering a viable solution for privacy-conscious graph
analysis.

</details>


### [82] [Solar Photovoltaic Assessment with Large Language Model](https://arxiv.org/abs/2507.19144)
*Muhao Guo,Yang Weng*

Main category: cs.LG

TL;DR: 本文提出PVAL框架解决现有太阳能光伏板检测方法局限性，利用大语言模型实现自动化可重复检测流程，助力可再生能源集成和电网管理。


<details>
  <summary>Details</summary>
Motivation: 现有太阳能光伏板检测方法存在算法不透明、依赖大量数据、泛化能力差等问题，阻碍大规模部署和电网优化，需新方法解决。

Method: 提出PVAL框架，包含任务分解、输出标准化、少样本提示和使用精心策划数据集微调。

Result: PVAL确保了跨异构数据集的透明度、可扩展性和适应性，同时减少计算开销。

Conclusion: PVAL建立了自动化可重复的太阳能板检测流程，为大规模可再生能源集成和优化电网管理铺平道路。

Abstract: Accurate detection and localization of solar photovoltaic (PV) panels in
satellite imagery is essential for optimizing microgrids and active
distribution networks (ADNs), which are critical components of renewable energy
systems. Existing methods lack transparency regarding their underlying
algorithms or training datasets, rely on large, high-quality PV training data,
and struggle to generalize to new geographic regions or varied environmental
conditions without extensive re-training. These limitations lead to
inconsistent detection outcomes, hindering large-scale deployment and
data-driven grid optimization. In this paper, we investigate how large language
models (LLMs) can be leveraged to overcome these challenges. Despite their
promise, LLMs face several challenges in solar panel detection, including
difficulties with multi-step logical processes, inconsistent output formatting,
frequent misclassification of visually similar objects (e.g., shadows, parking
lots), and low accuracy in complex tasks such as spatial localization and
quantification. To overcome these issues, we propose the PV Assessment with
LLMs (PVAL) framework, which incorporates task decomposition for more efficient
workflows, output standardization for consistent and scalable formatting,
few-shot prompting to enhance classification accuracy, and fine-tuning using
curated PV datasets with detailed annotations. PVAL ensures transparency,
scalability, and adaptability across heterogeneous datasets while minimizing
computational overhead. By combining open-source accessibility with robust
methodologies, PVAL establishes an automated and reproducible pipeline for
solar panel detection, paving the way for large-scale renewable energy
integration and optimized grid management.

</details>


### [83] [Explainable AI guided unsupervised fault diagnostics for high-voltage circuit breakers](https://arxiv.org/abs/2507.19168)
*Chi-Ching Hsu,Gaëtan Frusque,Florent Forest,Felipe Macedo,Christian M. Franck,Olga Fink*

Main category: cs.LG

TL;DR: 提出基于振动和声学信号的高压断路器无监督故障检测与分割框架及可解释诊断方法，用实验数据验证，助力可靠运行。


<details>
  <summary>Details</summary>
Motivation: 现有高压断路器状态监测系统依赖直接可观测物理参数且需断电监测，基于振动或声学信号的有监督方法在实际中因无故障标签不可行。

Method: 提出无监督故障检测与分割框架检测健康状态偏差，用可解释人工智能方法进行故障诊断。

Result: 所提框架和方法用实验数据集得到验证。

Conclusion: 所提贡献有助于实现更可靠的断路器系统运行。

Abstract: Commercial high-voltage circuit breaker (CB) condition monitoring systems
rely on directly observable physical parameters such as gas filling pressure
with pre-defined thresholds. While these parameters are crucial, they only
cover a small subset of malfunctioning mechanisms and usually can be monitored
only if the CB is disconnected from the grid. To facilitate online condition
monitoring while CBs remain connected, non-intrusive measurement techniques
such as vibration or acoustic signals are necessary. Currently, CB condition
monitoring studies using these signals typically utilize supervised methods for
fault diagnostics, where ground-truth fault types are known due to artificially
introduced faults in laboratory settings. This supervised approach is however
not feasible in real-world applications, where fault labels are unavailable. In
this work, we propose a novel unsupervised fault detection and segmentation
framework for CBs based on vibration and acoustic signals. This framework can
detect deviations from the healthy state. The explainable artificial
intelligence (XAI) approach is applied to the detected faults for fault
diagnostics. The specific contributions are: (1) we propose an integrated
unsupervised fault detection and segmentation framework that is capable of
detecting faults and clustering different faults with only healthy data
required during training (2) we provide an unsupervised explainability-guided
fault diagnostics approach using XAI to offer domain experts potential
indications of the aged or faulty components, achieving fault diagnostics
without the prerequisite of ground-truth fault labels. These contributions are
validated using an experimental dataset from a high-voltage CB under healthy
and artificially introduced fault conditions, contributing to more reliable CB
system operation.

</details>


### [84] [Automatic Cough Analysis for Non-Small Cell Lung Cancer Detection](https://arxiv.org/abs/2507.19174)
*Chiara Giangregorio,Cristina Maria Licciardello,Vanja Miskovic,Leonardo Provenzano,Alessandra Laura Giulia Pedrocchi,Andra Diana Dumitrascu,Arsela Prelaj,Marina Chiara Garassino,Emilia Ambrosini,Simona Ferrante*

Main category: cs.LG

TL;DR: 研究探索自动咳嗽分析用于区分NSCLC患者和健康对照，CNN性能最佳，SVM在低算力场景适用，需更优数据集。


<details>
  <summary>Details</summary>
Motivation: 早期检测NSCLC对改善患者预后至关重要，需要新方法促进早期诊断。

Method: 采集227名受试者咳嗽音频，用SVM、XGBoost、CNN和VGG16等机器学习和深度学习技术分析，用SHAP增强模型可解释性，评估模型公平性。

Result: CNN测试集准确率0.83最佳，SVM稍低但适用于低算力场景，SHAP增强SVM透明度，年龄公平性差异略高于性别。

Conclusion: 需要更大、更多样、无偏的数据集以增强研究结果可靠性。

Abstract: Early detection of non-small cell lung cancer (NSCLC) is critical for
improving patient outcomes, and novel approaches are needed to facilitate early
diagnosis. In this study, we explore the use of automatic cough analysis as a
pre-screening tool for distinguishing between NSCLC patients and healthy
controls. Cough audio recordings were prospectively acquired from a total of
227 subjects, divided into NSCLC patients and healthy controls. The recordings
were analyzed using machine learning techniques, such as support vector machine
(SVM) and XGBoost, as well as deep learning approaches, specifically
convolutional neural networks (CNN) and transfer learning with VGG16. To
enhance the interpretability of the machine learning model, we utilized Shapley
Additive Explanations (SHAP). The fairness of the models across demographic
groups was assessed by comparing the performance of the best model across
different age groups (less than or equal to 58y and higher than 58y) and gender
using the equalized odds difference on the test set. The results demonstrate
that CNN achieves the best performance, with an accuracy of 0.83 on the test
set. Nevertheless, SVM achieves slightly lower performances (accuracy of 0.76
in validation and 0.78 in the test set), making it suitable in contexts with
low computational power. The use of SHAP for SVM interpretation further
enhances model transparency, making it more trustworthy for clinical
applications. Fairness analysis shows slightly higher disparity across age
(0.15) than gender (0.09) on the test set. Therefore, to strengthen our
findings' reliability, a larger, more diverse, and unbiased dataset is needed
-- particularly including individuals at risk of NSCLC and those in early
disease stages.

</details>


### [85] [WACA-UNet: Weakness-Aware Channel Attention for Static IR Drop Prediction in Integrated Circuit Design](https://arxiv.org/abs/2507.19197)
*Youngmin Seo,Yunhyeong Kwon,Younghun Park,HwiRyong Kim,Seungho Eum,Jinha Kim,Taigon Song,Juho Kim,Unsang Park*

Main category: cs.LG

TL;DR: 提出新机制改进VLSI中IR降估计，在基准测试中表现优异


<details>
  <summary>Details</summary>
Motivation: 传统模拟求解器计算成本高且难扩展，现有学习方法未考虑输入层重要性差异

Method: 将IR降估计转为像素回归任务，提出WACA机制并集成到基于ConvNeXtV2的注意力U-Net

Result: 在ICCAD - 2023基准测试中，平均绝对误差降低61.1%，F1分数提高71.0%

Conclusion: 通道异质性是VLSI物理布局分析的关键归纳偏差

Abstract: Accurate spatial prediction of power integrity issues, such as IR drop, is
critical for reliable VLSI design. However, traditional simulation-based
solvers are computationally expensive and difficult to scale. We address this
challenge by reformulating IR drop estimation as a pixel-wise regression task
on heterogeneous multi-channel physical maps derived from circuit layouts.
Prior learning-based methods treat all input layers (e.g., metal, via, and
current maps) equally, ignoring their varying importance to prediction
accuracy. To tackle this, we propose a novel Weakness-Aware Channel Attention
(WACA) mechanism, which recursively enhances weak feature channels while
suppressing over-dominant ones through a two-stage gating strategy. Integrated
into a ConvNeXtV2-based attention U-Net, our approach enables adaptive and
balanced feature representation. On the public ICCAD-2023 benchmark, our method
outperforms the ICCAD-2023 contest winner by reducing mean absolute error by
61.1% and improving F1-score by 71.0%. These results demonstrate that
channel-wise heterogeneity is a key inductive bias in physical layout analysis
for VLSI.

</details>


### [86] [Physics-Informed Graph Neural Networks for Transverse Momentum Estimation in CMS Trigger Systems](https://arxiv.org/abs/2507.19205)
*Md Abrar Jahin,Shahriar Soudeep,M. F. Mridha,Muhammad Mostafa Monowar,Md. Abdul Hamid*

Main category: cs.LG

TL;DR: 提出物理信息引导的GNN框架用于高能物理实时粒子横向动量估计，在CMS触发数据集实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有高能物理实时粒子横向动量估计算法在硬件约束下有不足，静态机器学习模型在高堆积下性能下降，通用GNN忽略关键领域结构。

Method: 通过四种图构建策略系统编码探测器几何和物理观测值，将定制图结构与新型消息传递层及特定领域损失函数结合。

Result: 基于CMS触发数据集实验，station - informed EdgeConv模型达到0.8525的MAE，参数比深度学习基线少超55%，η - centric MPL配置效率相当且精度提升。

Conclusion: 物理引导的GNN在资源受限触发系统部署有前景。

Abstract: Real-time particle transverse momentum ($p_T$) estimation in high-energy
physics demands algorithms that are both efficient and accurate under strict
hardware constraints. Static machine learning models degrade under high pileup
and lack physics-aware optimization, while generic graph neural networks (GNNs)
often neglect domain structure critical for robust $p_T$ regression. We propose
a physics-informed GNN framework that systematically encodes detector geometry
and physical observables through four distinct graph construction strategies
that systematically encode detector geometry and physical observables:
station-as-node, feature-as-node, bending angle-centric, and pseudorapidity
($\eta$)-centric representations. This framework integrates these tailored
graph structures with a novel Message Passing Layer (MPL), featuring
intra-message attention and gated updates, and domain-specific loss functions
incorporating $p_{T}$-distribution priors. Our co-design methodology yields
superior accuracy-efficiency trade-offs compared to existing baselines.
Extensive experiments on the CMS Trigger Dataset validate the approach: a
station-informed EdgeConv model achieves a state-of-the-art MAE of 0.8525 with
$\ge55\%$ fewer parameters than deep learning baselines, especially TabNet,
while an $\eta$-centric MPL configuration also demonstrates improved accuracy
with comparable efficiency. These results establish the promise of
physics-guided GNNs for deployment in resource-constrained trigger systems.

</details>


### [87] [Dependency-aware synthetic tabular data generation](https://arxiv.org/abs/2507.19211)
*Chaithra Umesh,Kristian Schultz,Manjunath Mahendra,Saptarshi Bej,Olaf Wolkenhauer*

Main category: cs.LG

TL;DR: 提出Hierarchical Feature Generation Framework (HFGF) 用于生成合成表格数据，实验表明其能提升功能和逻辑依赖保留，增强数据结构保真度和下游实用性。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在合成表格数据时难以保留属性间关系，特别是功能依赖和逻辑依赖。

Method: 创建含已知依赖的基准数据集，先使用标准生成模型生成独立特征，再根据预定义规则重建依赖特征。

Result: 在四个基准数据集上实验表明，HFGF 改进了六个生成模型对功能和逻辑依赖的保留。

Conclusion: HFGF 能显著增强合成表格数据的结构保真度和下游实用性。

Abstract: Synthetic tabular data is increasingly used in privacy-sensitive domains such
as health care, but existing generative models often fail to preserve
inter-attribute relationships. In particular, functional dependencies (FDs) and
logical dependencies (LDs), which capture deterministic and rule-based
associations between features, are rarely or often poorly retained in synthetic
datasets. To address this research gap, we propose the Hierarchical Feature
Generation Framework (HFGF) for synthetic tabular data generation. We created
benchmark datasets with known dependencies to evaluate our proposed HFGF. The
framework first generates independent features using any standard generative
model, and then reconstructs dependent features based on predefined FD and LD
rules. Our experiments on four benchmark datasets with varying sizes, feature
imbalance, and dependency complexity demonstrate that HFGF improves the
preservation of FDs and LDs across six generative models, including CTGAN,
TVAE, and GReaT. Our findings demonstrate that HFGF can significantly enhance
the structural fidelity and downstream utility of synthetic tabular data.

</details>


### [88] [Component-Based Machine Learning for Indoor Flow and Temperature Fields Prediction Latent Feature Aggregation and Flow Interaction](https://arxiv.org/abs/2507.19233)
*Shaofan Wang,Nils Thuerey,Philipp Geyer*

Main category: cs.LG

TL;DR: 提出基于组件的机器学习（CBML）替代模型方法快速预测室内速度和温度场，经测试能准确快速预测。


<details>
  <summary>Details</summary>
Motivation: 传统CFD模拟计算量大，难以集成到实时或设计迭代工作流程中，需准确高效预测室内气流和温度分布以用于建筑能源优化和居住者舒适度控制。

Method: 提出包含三个神经网络的CBML模型，用二维房间案例，以CFD模拟数据进行训练和测试。

Result: CBML模型在训练和测试数据集上能准确快速地预测双分量聚合速度和温度场。

Conclusion: CBML模型可替代传统CFD模拟，实现室内速度和温度场的快速准确预测。

Abstract: Accurate and efficient prediction of indoor airflow and temperature
distributions is essential for building energy optimization and occupant
comfort control. However, traditional CFD simulations are computationally
intensive, limiting their integration into real-time or design-iterative
workflows. This study proposes a component-based machine learning (CBML)
surrogate modeling approach to replace conventional CFD simulation for fast
prediction of indoor velocity and temperature fields. The model consists of
three neural networks: a convolutional autoencoder with residual connections
(CAER) to extract and compress flow features, a multilayer perceptron (MLP) to
map inlet velocities to latent representations, and a convolutional neural
network (CNN) as an aggregator to combine single-inlet features into dual-inlet
scenarios. A two-dimensional room with varying left and right air inlet
velocities is used as a benchmark case, with CFD simulations providing training
and testing data. Results show that the CBML model accurately and fast predicts
two-component aggregated velocity and temperature fields across both training
and testing datasets.

</details>


### [89] [A Markov Categorical Framework for Language Modeling](https://arxiv.org/abs/2507.19247)
*Yifan Zhang*

Main category: cs.LG

TL;DR: 本文引入马尔可夫范畴分析框架解构自回归语言模型生成过程与负对数似然目标，揭示现代大语言模型有效性背后的结构原理。


<details>
  <summary>Details</summary>
Motivation: 缺乏对自回归语言模型负对数似然目标产生通用表示的理论理解。

Method: 使用马尔可夫范畴（MCs）构建统一分析框架，将单步生成映射建模为随机范畴中马尔可夫核的组合。

Result: 为投机解码方法成功提供信息论依据，量化隐藏状态信息盈余；明确负对数似然最小化使模型学习数据内在条件随机性；证明负对数似然训练是一种隐式谱对比学习。

Conclusion: 该组合和信息几何视角揭示了现代大语言模型有效性的深层结构原理。

Abstract: Auto-regressive language models factorize sequence probabilities and are
trained by minimizing the negative log-likelihood (NLL) objective. While
empirically powerful, a deep theoretical understanding of why this simple
objective yields such versatile representations remains elusive. This work
introduces a unifying analytical framework using Markov Categories (MCs) to
deconstruct the AR generation process and the NLL objective. We model the
single-step generation map as a composition of Markov kernels in the category
Stoch. This compositional view, when enriched with statistical divergences,
allows us to dissect information flow and learned geometry. Our framework makes
three main contributions. First, we provide a formal, information-theoretic
rationale for the success of modern speculative decoding methods like EAGLE,
quantifying the information surplus in hidden states that these methods
exploit. Second, we formalize how NLL minimization forces the model to learn
not just the next token, but the data's intrinsic conditional stochasticity, a
process we analyze using categorical entropy. Third, and most centrally, we
prove that NLL training acts as an implicit form of spectral contrastive
learning. By analyzing the information geometry of the model's prediction head,
we show that NLL implicitly forces the learned representation space to align
with the eigenspectrum of a predictive similarity operator, thereby learning a
geometrically structured space without explicit contrastive pairs. This
compositional and information-geometric perspective reveals the deep structural
principles underlying the effectiveness of modern LMs. Project Page:
https://github.com/asiresearch/lm-theory

</details>


### [90] [Doubling Your Data in Minutes: Ultra-fast Tabular Data Generation via LLM-Induced Dependency Graphs](https://arxiv.org/abs/2507.19334)
*Shuo Yang,Zheyu Zhang,Bardh Prenkaj,Gjergji Kasneci*

Main category: cs.LG

TL;DR: 针对表格数据增强中现有方法的问题，提出轻量级生成框架SPADA，实验表明其能减少约束违反并加速生成。


<details>
  <summary>Details</summary>
Motivation: 现有表格数据增强方法存在特征依赖建模引入偏差和采样计算开销大的问题。

Method: 提出SPADA框架，通过大语言模型诱导图捕获稀疏依赖，将特征视为节点，利用高斯核密度估计和条件归一化流模型两种合成策略合成值。

Result: 在四个数据集上，SPADA相比基于扩散的方法减少4%的约束违反，比基于大语言模型的基线加速近9500倍。

Conclusion: SPADA框架有效解决了现有表格数据增强方法的问题，具有更好的性能。

Abstract: Tabular data is critical across diverse domains, yet high-quality datasets
remain scarce due to privacy concerns and the cost of collection. Contemporary
approaches adopt large language models (LLMs) for tabular augmentation, but
exhibit two major limitations: (1) dense dependency modeling among tabular
features that can introduce bias, and (2) high computational overhead in
sampling. To address these issues, we propose SPADA for SPArse
Dependency-driven Augmentation, a lightweight generative framework that
explicitly captures sparse dependencies via an LLM-induced graph. We treat each
feature as a node and synthesize values by traversing the graph, conditioning
each feature solely on its parent nodes. We explore two synthesis strategies: a
non-parametric method using Gaussian kernel density estimation, and a
conditional normalizing flow model that learns invertible mappings for
conditional density estimation. Experiments on four datasets show that SPADA
reduces constraint violations by 4% compared to diffusion-based methods and
accelerates generation by nearly 9,500 times over LLM-based baselines.

</details>


### [91] [Short-Form Video Recommendations with Multimodal Embeddings: Addressing Cold-Start and Bias Challenges](https://arxiv.org/abs/2507.19346)
*Andrii Dzhoha,Katya Mirylenka,Egor Malykh,Marco-Andrea Buchmann,Francesca Catino*

Main category: cs.LG

TL;DR: 电商平台引入短视频内容面临推荐系统挑战，利用微调多模态视觉语言模型的视频检索系统更有效。


<details>
  <summary>Details</summary>
Motivation: 电商等平台引入短视频内容，推荐系统面临新挑战，需有效解决方案。

Method: 利用微调多模态视觉语言模型的视频检索系统。

Result: 在电商平台在线实验中，该方法比传统监督学习方法更有效。

Conclusion: 即使有足够视频交互数据，利用上述视频检索系统能更好克服引入短视频体验的挑战。

Abstract: In recent years, social media users have spent significant amounts of time on
short-form video platforms. As a result, established platforms in other
domains, such as e-commerce, have begun introducing short-form video content to
engage users and increase their time spent on the platform. The success of
these experiences is due not only to the content itself but also to a unique UI
innovation: instead of offering users a list of choices to click, platforms
actively recommend content for users to watch one at a time. This creates new
challenges for recommender systems, especially when launching a new video
experience. Beyond the limited interaction data, immersive feed experiences
introduce stronger position bias due to the UI and duration bias when
optimizing for watch-time, as models tend to favor shorter videos. These
issues, together with the feedback loop inherent in recommender systems, make
it difficult to build effective solutions. In this paper, we highlight the
challenges faced when introducing a new short-form video experience and present
our experience showing that, even with sufficient video interaction data, it
can be more beneficial to leverage a video retrieval system using a fine-tuned
multimodal vision-language model to overcome these challenges. This approach
demonstrated greater effectiveness compared to conventional supervised learning
methods in online experiments conducted on our e-commerce platform.

</details>


### [92] [Reconstruction of Sparse Urban Wireless Signals via Group Equivariant Non-Expansive Operators](https://arxiv.org/abs/2507.19349)
*Lorenzo Mario Amorosa,Francesco Conti,Nicola Quercioli,Flavio Zabini,Tayebeh Lotfi Mahyari,Yiqun Ge,Patrizio Frosini*

Main category: cs.LG

TL;DR: 本文探索用GENEO从稀疏测量中重建空间信号，提出基于GENEO的方法用于城市无线通信网络SINR地图重建，在样本数量受限下表现良好。


<details>
  <summary>Details</summary>
Motivation: 新兴通信系统中高精度获取空间变化量成本高，需低成本方法进行重建。

Method: 引入基于GENEO的方法，利用其不变性减少参数，结合代数和几何约束。

Result: 该数学框架与现有方法相比有竞争力，能在样本严重受限下准确重建空间信号。

Conclusion: 基于GENEO的方法在城市无线通信网络SINR地图重建中，面对数据稀缺情况有优势。

Abstract: In emerging communication systems such as sixth generation (6G) wireless
networks, efficient resource management and service delivery rely on accurate
knowledge of spatially-varying quantities like signal-to-interference-noise
ratio (SINR) maps, which are costly to acquire at high resolution. This work
explores the reconstruction of such spatial signals from sparse measurements
using Group Equivariant Non-Expansive Operators (GENEOs), offering a
low-complexity alternative to traditional neural networks. The concept of
GENEO, which originated in topological data analysis (TDA), is a mathematical
tool used in machine learning to represent agents modelled as functional
operators acting on data while incorporating application-specific invariances.
Leveraging these invariances reduces the number of parameters with respect to
traditional neural networks and mitigates data scarcity by enforcing known
algebraic and geometric constraints that reflect symmetries in the agents'
actions. In this paper, we introduce a novel GENEO-based approach for SINR map
reconstruction in urban wireless communication networks using extremely sparse
sampling. We demonstrate that this mathematical framework achieves competitive
performance compared to established methods. Our evaluation, conducted using
both statistical and TDA metrics, highlights the advantages of our approach in
accurately reconstructing spatial signals under severe data limitations on the
number of samples.

</details>


### [93] [A Data-Driven Approach to Estimate LEO Orbit Capacity Models](https://arxiv.org/abs/2507.19365)
*Braden Stock,Maddox McVarthy,Simone Servadio*

Main category: cs.LG

TL;DR: 利用SINDy算法和LSTM对近地轨道空间物体进行建模预测其未来传播，借助高保真模型数据集构建轻量级低保真模型实现快速准确预测。


<details>
  <summary>Details</summary>
Motivation: 准确预测近地轨道卫星和碎片的未来传播情况。

Method: 运用Sparse Identification of Nonlinear Dynamics算法（SINDy）和Long Short - Term Memory Recurrent Neural Networks（LSTM），利用高保真模型MOCAT - MC的数据集构建低保真模型。

Result: 可在更短时间内实现准确预测。

Conclusion: 所提出的方法能够对近地轨道空间物体进行准确建模和预测。

Abstract: Utilizing the Sparse Identification of Nonlinear Dynamics algorithm (SINDy)
and Long Short-Term Memory Recurrent Neural Networks (LSTM), the population of
resident space objects, divided into Active, Derelict, and Debris, in LEO can
be accurately modeled to predict future satellite and debris propagation. This
proposed approach makes use of a data set coming from a computational expensive
high-fidelity model, the MOCAT-MC, to provide a light, low-fidelity counterpart
that provides accurate forecasting in a shorter time frame.

</details>


### [94] [Counterfactual Explanations in Medical Imaging: Exploring SPN-Guided Latent Space Manipulation](https://arxiv.org/abs/2507.19368)
*Julia Siekiera,Stefan Kramer*

Main category: cs.LG

TL;DR: 本文探讨医学图像分析中生成符合相似性约束的反事实解释的挑战，采用特定模型优化方法，结合SPN和半监督VAE建模潜在空间，在cheXpert数据集实验并对比基线、分析权衡。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医学图像分析中是黑盒系统，可靠性和可解释性存疑，生成合理反事实解释有挑战。

Method: 用SPN对VAE潜在空间的似然性建模，利用其作为潜在空间描述符和分类器的双重作用优化潜在空间反事实。

Result: 在cheXpert数据集实验，将SPN引导的潜在空间操作与神经网络基线对比，分析潜在变量正则化和反事实质量的权衡。

Conclusion: 未明确提及，但暗示该方法在解决生成合理反事实解释挑战上有一定效果。

Abstract: Artificial intelligence is increasingly leveraged across various domains to
automate decision-making processes that significantly impact human lives. In
medical image analysis, deep learning models have demonstrated remarkable
performance. However, their inherent complexity makes them black box systems,
raising concerns about reliability and interpretability. Counterfactual
explanations provide comprehensible insights into decision processes by
presenting hypothetical "what-if" scenarios that alter model classifications.
By examining input alterations, counterfactual explanations provide patterns
that influence the decision-making process. Despite their potential, generating
plausible counterfactuals that adhere to similarity constraints providing
human-interpretable explanations remains a challenge. In this paper, we
investigate this challenge by a model-specific optimization approach. While
deep generative models such as variational autoencoders (VAEs) exhibit
significant generative power, probabilistic models like sum-product networks
(SPNs) efficiently represent complex joint probability distributions. By
modeling the likelihood of a semi-supervised VAE's latent space with an SPN, we
leverage its dual role as both a latent space descriptor and a classifier for a
given discrimination task. This formulation enables the optimization of latent
space counterfactuals that are both close to the original data distribution and
aligned with the target class distribution. We conduct experimental evaluation
on the cheXpert dataset. To evaluate the effectiveness of the integration of
SPNs, our SPN-guided latent space manipulation is compared against a neural
network baseline. Additionally, the trade-off between latent variable
regularization and counterfactual quality is analyzed.

</details>


### [95] [On Arbitrary Predictions from Equally Valid Models](https://arxiv.org/abs/2507.19408)
*Sarah Lockfisch,Kristian Schwethelm,Martin Menten,Rickmer Braren,Daniel Rueckert,Alexander Ziller,Georgios Kaissis*

Main category: cs.LG

TL;DR: 研究分析医学任务和模型架构中预测多样性的程度、驱动因素和影响，表明小集成模型可缓解该问题，强调考虑模型多样性的临床重要性。


<details>
  <summary>Details</summary>
Motivation: 医学中机器学习模型的预测多样性会导致同一患者有冲突预测，此风险理解和处理不足，需研究。

Method: 对不同医学任务和模型架构进行实证分析。

Result: 标准验证指标无法确定唯一最优模型；大量预测依赖模型开发中的任意选择；小集成模型结合弃权策略可缓解预测多样性；增加模型容量提高准确率可减少预测多样性。

Conclusion: 强调考虑模型多样性的临床重要性，提倡基于集成的策略提高诊断可靠性，模型无法达成共识时建议专家评审。

Abstract: Model multiplicity refers to the existence of multiple machine learning
models that describe the data equally well but may produce different
predictions on individual samples. In medicine, these models can admit
conflicting predictions for the same patient -- a risk that is poorly
understood and insufficiently addressed.
  In this study, we empirically analyze the extent, drivers, and ramifications
of predictive multiplicity across diverse medical tasks and model
architectures, and show that even small ensembles can mitigate/eliminate
predictive multiplicity in practice. Our analysis reveals that (1) standard
validation metrics fail to identify a uniquely optimal model and (2) a
substantial amount of predictions hinges on arbitrary choices made during model
development. Using multiple models instead of a single model reveals instances
where predictions differ across equally plausible models -- highlighting
patients that would receive arbitrary diagnoses if any single model were used.
In contrast, (3) a small ensemble paired with an abstention strategy can
effectively mitigate measurable predictive multiplicity in practice;
predictions with high inter-model consensus may thus be amenable to automated
classification. While accuracy is not a principled antidote to predictive
multiplicity, we find that (4) higher accuracy achieved through increased model
capacity reduces predictive multiplicity.
  Our findings underscore the clinical importance of accounting for model
multiplicity and advocate for ensemble-based strategies to improve diagnostic
reliability. In cases where models fail to reach sufficient consensus, we
recommend deferring decisions to expert review.

</details>


### [96] [SILS: Strategic Influence on Liquidity Stability and Whale Detection in Concentrated-Liquidity DEXs](https://arxiv.org/abs/2507.19411)
*Ali RajabiNekoo,Laleh Rasoul,Amirfarhad Farhadi,Azadeh Zamanifar*

Main category: cs.LG

TL;DR: 传统方法识别CLMM中有影响力的流动性提供者不准确，SILS框架提供更详细方法，能准确识别高影响力LP，增强DeFi生态系统，用于主动风险管理。


<details>
  <summary>Details</summary>
Motivation: 传统识别CLMM中有影响力流动性提供者的方法依赖宽泛指标，导致风险分析不准确，需要更精确方法。

Method: 使用链上事件日志和智能合约执行跟踪计算ETWL配置文件，应用无监督异常检测，通过LSIS定义LP功能重要性。

Result: 能准确识别高影响力LP，包括传统方法遗漏的；支持保护预言机层和可操作交易信号等应用；减少传统模型的误报和漏报。

Conclusion: SILS为主动风险管理提供有效机制，改变DeFi协议保护生态系统免受不对称流动性行为影响的方式。

Abstract: Traditional methods for identifying impactful liquidity providers (LPs) in
Concentrated Liquidity Market Makers (CLMMs) rely on broad measures, such as
nominal capital size or surface-level activity, which often lead to inaccurate
risk analysis. The SILS framework offers a significantly more detailed
approach, characterizing LPs not just as capital holders but as dynamic
systemic agents whose actions directly impact market stability. This represents
a fundamental paradigm shift from the static, volume-based analysis to a
dynamic, impact-focused understanding. This advanced approach uses on-chain
event logs and smart contract execution traces to compute Exponential
Time-Weighted Liquidity (ETWL) profiles and apply unsupervised anomaly
detection. Most importantly, it defines an LP's functional importance through
the Liquidity Stability Impact Score (LSIS), a counterfactual metric that
measures the potential degradation of the market if the LP withdraws. This
combined approach provides a more detailed and realistic characterization of an
LP's impact, moving beyond the binary and often misleading classifications used
by existing methods. This impact-focused and comprehensive approach enables
SILS to accurately identify high-impact LPs-including those missed by
traditional methods and supports essential applications like a protective
oracle layer and actionable trader signals, thereby significantly enhancing
DeFi ecosystem. The framework provides unprecedented transparency into the
underlying liquidity structure and associated risks, effectively reducing the
common false positives and uncovering critical false negatives found in
traditional models. Therefore, SILS provides an effective mechanism for
proactive risk management, transforming how DeFi protocols safeguard their
ecosystems against asymmetric liquidity behavior.

</details>


### [97] [Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding](https://arxiv.org/abs/2507.19427)
*StepFun,:,Bin Wang,Bojun Wang,Changyi Wan,Guanzhe Huang,Hanpeng Hu,Haonan Jia,Hao Nie,Mingliang Li,Nuo Chen,Siyu Chen,Song Yuan,Wuxun Xie,Xiaoniu Song,Xing Chen,Xingping Yang,Xuelin Zhang,Yanbo Yu,Yaoyu Wang,Yibo Zhu,Yimin Jiang,Yu Zhou,Yuanwei Lu,Houyi Li,Jingcheng Hu,Ka Man Lo,Ailin Huang,Binxing Jiao,Bo Li,Boyu Chen,Changxin Miao,Chang Lou,Chen Hu,Chen Xu,Chenfeng Yu,Chengyuan Yao,Daokuan Lv,Dapeng Shi,Deshan Sun,Ding Huang,Dingyuan Hu,Dongqing Pang,Enle Liu,Fajie Zhang,Fanqi Wan,Gulin Yan,Han Zhang,Han Zhou,Hanghao Wu,Hangyu Guo,Hanqi Chen,Hanshan Zhang,Hao Wu,Haocheng Zhang,Haolong Yan,Haoran Lv,Haoran Wei,Hebin Zhou,Heng Wang,Heng Wang,Hongxin Li,Hongyu Zhou,Hongyuan Wang,Huiyong Guo,Jia Wang,Jiahao Gong,Jialing Xie,Jian Zhou,Jianjian Sun,Jiaoren Wu,Jiaran Zhang,Jiayu Liu,Jie Cheng,Jie Luo,Jie Yan,Jie Yang,Jieyi Hou,Jinguang Zhang,Jinlan Cao,Jisheng Yin,Junfeng Liu,Junhao Huang,Junzhe Lin,Kaijun Tan,Kaixiang Li,Kang An,Kangheng Lin,Kenkun Liu,Lei Yang,Liang Zhao,Liangyu Chen,Lieyu Shi,Liguo Tan,Lin Lin,Lin Zhang,Lina Chen,Liwen Huang,Liying Shi,Longlong Gu,Mei Chen,Mengqiang Ren,Ming Li,Mingzhe Chen,Na Wang,Nan Wu,Qi Han,Qian Zhao,Qiang Zhang,Qianni Liu,Qiaohui Chen,Qiling Wu,Qinglin He,Qinyuan Tan,Qiufeng Wang,Qiuping Wu,Qiuyan Liang,Quan Sun,Rui Li,Ruihang Miao,Ruosi Wan,Ruyan Guo,Shangwu Zhong,Shaoliang Pang,Shengjie Fan,Shijie Shang,Shilei Jiang,Shiliang Yang,Shiming Hao,Shuli Gao,Siming Huang,Siqi Liu,Tiancheng Cao,Tianhao Cheng,Tianhao Peng,Wang You,Wei Ji,Wen Sun,Wenjin Deng,Wenqing He,Wenzhen Zheng,Xi Chen,Xiangwen Kong,Xianzhen Luo,Xiaobo Yang,Xiaojia Liu,Xiaoxiao Ren,Xin Han,Xin Li,Xin Wu,Xu Zhao,Yanan Wei,Yang Li,Yangguang Li,Yangshijie Xu,Yanming Xu,Yaqiang Shi,Yeqing Shen,Yi Yang,Yifei Yang,Yifeng Gong,Yihan Chen,Yijing Yang,Yinmin Zhang,Yizhuang Zhou,Yuanhao Ding,Yuantao Fan,Yuanzhen Yang,Yuchu Luo,Yue Peng,Yufan Lu,Yuhang Deng,Yuhe Yin,Yujie Liu,Yukun Chen,Yuling Zhao,Yun Mou,Yunlong Li,Yunzhou Ju,Yusheng Li,Yuxiang Yang,Yuxiang Zhang,Yuyang Chen,Zejia Weng,Zhe Xie,Zheng Ge,Zheng Gong,Zhenyi Lu,Zhewei Huang,Zhichao Chang,Zhiguo Huang,Zhirui Wang,Zidong Yang,Zili Wang,Ziqi Wang,Zixin Zhang,Binxing Jiao,Daxin Jiang,Heung-Yeung Shum,Xiangyu Zhang*

Main category: cs.LG

TL;DR: 本文介绍321B参数的VLM模型Step - 3，通过硬件感知的模型 - 系统协同设计降低解码成本，在长上下文场景优势明显，实现高解码吞吐量。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在解码时硬件效率低，尤其是长上下文推理任务的问题。

Method: 采用Multi - Matrix Factorization Attention (MFA)机制减少KV缓存大小和计算量，使用Attention - FFN Disaggregation (AFD)分布式推理系统解耦注意力和前馈网络层。

Result: 与DeepSeek - V3和Qwen3 MoE 235B相比，显著降低理论解码成本；在Hopper GPU上实现每秒每GPU高达4,039个令牌的解码吞吐量，高于DeepSeek - V3。

Conclusion: 硬件对齐的注意力算术强度、MoE稀疏性和AFD对成本效益至关重要，为大语言模型解码设定了新的帕累托边界。

Abstract: Large language models (LLMs) face low hardware efficiency during decoding,
especially for long-context reasoning tasks. This paper introduces Step-3, a
321B-parameter VLM with hardware-aware model-system co-design optimized for
minimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel
Multi-Matrix Factorization Attention (MFA) mechanism that significantly reduces
both KV cache size and computation while maintaining high attention
expressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed
inference system that decouples attention and Feed-Forward Network (FFN) layers
into specialized subsystems. This co-design achieves unprecedented cost
efficiency: Step-3 significantly reduces theoretical decoding costs compared
with models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at
longer context. Step-3 achieves low cost while activating 38B parameters per
token (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that
hardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are
critical to cost-effectiveness. We perform a head-to-head comparison with
DeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs
achieves a decoding throughput of up to 4,039 tokens per second per GPU under
50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324
in the same setup and sets a new Pareto frontier for LLM decoding.

</details>


### [98] [Observations Meet Actions: Learning Control-Sufficient Representations for Robust Policy Generalization](https://arxiv.org/abs/2507.19437)
*Yuliang Gu,Hongpeng Cao,Marco Caccamo,Naira Hovakimyan*

Main category: cs.LG

TL;DR: 本文将基于上下文的强化学习重铸为对偶推理 - 控制问题，提出BCPO算法，在连续控制基准测试中表现良好，统一了理论、诊断和实践。


<details>
  <summary>Details</summary>
Motivation: 捕捉潜在变化对强化学习智能体在训练范围外的部署至关重要，需要解决基于上下文的强化学习问题。

Method: 将基于上下文的强化学习重铸为对偶推理 - 控制问题，提出上下文证据下限（ELBO）风格目标，用Bottlenecked Contextual Policy Optimization (BCPO)算法优化，在离线策略学习器前放置变分信息瓶颈编码器。

Result: 在标准连续控制基准测试中，BCPO使用更少样本达到或超越其他基线，在训练范围外保持良好性能。

Conclusion: 该框架统一了基于上下文的强化学习的理论、诊断和实践。

Abstract: Capturing latent variations ("contexts") is key to deploying
reinforcement-learning (RL) agents beyond their training regime. We recast
context-based RL as a dual inference-control problem and formally characterize
two properties and their hierarchy: observation sufficiency (preserving all
predictive information) and control sufficiency (retaining decision-making
relevant information). Exploiting this dichotomy, we derive a contextual
evidence lower bound(ELBO)-style objective that cleanly separates
representation learning from policy learning and optimizes it with Bottlenecked
Contextual Policy Optimization (BCPO), an algorithm that places a variational
information-bottleneck encoder in front of any off-policy policy learner. On
standard continuous-control benchmarks with shifting physical parameters, BCPO
matches or surpasses other baselines while using fewer samples and retaining
performance far outside the training regime. The framework unifies theory,
diagnostics, and practice for context-based RL.

</details>


### [99] [Forest-Guided Clustering -- Shedding Light into the Random Forest Black Box](https://arxiv.org/abs/2507.19455)
*Lisa Barros de Andrade e Sousa,Gregor Miller,Ronan Le Gleut,Dominik Thalmeier,Helena Pelin,Marie Piraud*

Main category: cs.LG

TL;DR: 提出Forest - Guided Clustering (FGC)方法用于解释随机森林，在基准数据集和AML转录组数据集上表现良好，弥合性能与可解释性差距。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在敏感领域应用增加，对可解释和可信决策需求上升，随机森林因集成性质难以解释。

Method: 提出FGC方法，通过按共享决策路径对实例分组揭示随机森林的局部和全局结构，计算簇特定和全局特征重要性分数推导决策规则。

Result: 在基准数据集上准确恢复潜在子类结构，优于经典聚类和事后解释方法；在AML转录组数据集上发现生物学相关亚群，分离疾病相关信号与混淆因素，恢复已知和新的基因表达模式。

Conclusion: FGC通过提供超越特征级归因的结构感知见解，弥合了性能和可解释性之间的差距。

Abstract: As machine learning models are increasingly deployed in sensitive application
areas, the demand for interpretable and trustworthy decision-making has
increased. Random Forests (RF), despite their widespread use and strong
performance on tabular data, remain difficult to interpret due to their
ensemble nature. We present Forest-Guided Clustering (FGC), a model-specific
explainability method that reveals both local and global structure in RFs by
grouping instances according to shared decision paths. FGC produces
human-interpretable clusters aligned with the model's internal logic and
computes cluster-specific and global feature importance scores to derive
decision rules underlying RF predictions. FGC accurately recovered latent
subclass structure on a benchmark dataset and outperformed classical clustering
and post-hoc explanation methods. Applied to an AML transcriptomic dataset, FGC
uncovered biologically coherent subpopulations, disentangled disease-relevant
signals from confounders, and recovered known and novel gene expression
patterns. FGC bridges the gap between performance and interpretability by
providing structure-aware insights that go beyond feature-level attribution.

</details>


### [100] [Advancing Event Forecasting through Massive Training of Large Language Models: Challenges, Solutions, and Broader Impacts](https://arxiv.org/abs/2507.19477)
*Sang-Woo Lee,Sohee Yang,Donghyun Kwak,Noah Y. Siegel*

Main category: cs.LG

TL;DR: 基于近期积极趋势，认为开展超预测水平事件预测大语言模型大规模训练研究时机成熟，探讨训练方法和数据获取方向。


<details>
  <summary>Details</summary>
Motivation: 近期研究显示大语言模型在事件预测上有进展，技术发展有望提升预测性能，推动开展相关大规模训练研究。

Method: 讨论训练方法，指出基于大语言模型事件预测训练的三个困难并给出缓解思路；数据方面，建议积极利用市场、公共和爬取数据集。

Result: 阐述技术进步可使人工智能在更广泛领域为社会提供预测智能。

Conclusion: 提出接近超预测水平人工智能技术的有前景路径和考虑，呼吁研究者关注。

Abstract: Many recent papers have studied the development of superforecaster-level
event forecasting LLMs. While methodological problems with early studies cast
doubt on the use of LLMs for event forecasting, recent studies with improved
evaluation methods have shown that state-of-the-art LLMs are gradually reaching
superforecaster-level performance, and reinforcement learning has also been
reported to improve future forecasting. Additionally, the unprecedented success
of recent reasoning models and Deep Research-style models suggests that
technology capable of greatly improving forecasting performance has been
developed. Therefore, based on these positive recent trends, we argue that the
time is ripe for research on large-scale training of superforecaster-level
event forecasting LLMs. We discuss two key research directions: training
methods and data acquisition. For training, we first introduce three
difficulties of LLM-based event forecasting training: noisiness-sparsity,
knowledge cut-off, and simple reward structure problems. Then, we present
related ideas to mitigate these problems: hypothetical event Bayesian networks,
utilizing poorly-recalled and counterfactual events, and auxiliary reward
signals. For data, we propose aggressive use of market, public, and crawling
datasets to enable large-scale training and evaluation. Finally, we explain how
these technical advances could enable AI to provide predictive intelligence to
society in broader areas. This position paper presents promising specific paths
and considerations for getting closer to superforecaster-level AI technology,
aiming to call for researchers' interest in these directions.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [101] [Game-Theoretic Gradient Control for Robust Neural Network Training](https://arxiv.org/abs/2507.19143)
*Maria Zaitseva,Ivan Tomilov,Natalia Gusarova*

Main category: cs.NE

TL;DR: 研究通过修改反向传播和目标变量加噪提升前馈神经网络抗噪性，实验显示不同效果，特定方法显著增强抗噪性。


<details>
  <summary>Details</summary>
Motivation: 前馈神经网络易受输入噪声影响，现有正则化方法有局限，需提升其噪声鲁棒性。

Method: 提出“梯度丢弃”，在反向传播时以概率 1 - p 选择性使隐藏层神经元梯度归零；对目标变量添加白噪声或稳定分布噪声。

Result: 在十个表格数据集实验有不同影响，回归任务中梯度丢弃（p = 0.9）结合稳定分布目标加噪显著增强输入噪声鲁棒性。

Conclusion: 方法有潜力，强调自适应参数调整重要性，为神经网络在博弈论框架下分析开辟新途径。

Abstract: Feed-forward neural networks (FFNNs) are vulnerable to input noise, reducing
prediction performance. Existing regularization methods like dropout often
alter network architecture or overlook neuron interactions. This study aims to
enhance FFNN noise robustness by modifying backpropagation, interpreted as a
multi-agent game, and exploring controlled target variable noising. Our
"gradient dropout" selectively nullifies hidden layer neuron gradients with
probability 1 - p during backpropagation, while keeping forward passes active.
This is framed within compositional game theory. Additionally, target variables
were perturbed with white noise or stable distributions. Experiments on ten
diverse tabular datasets show varying impacts: improvement or diminishing of
robustness and accuracy, depending on dataset and hyperparameters. Notably, on
regression tasks, gradient dropout (p = 0.9) combined with stable distribution
target noising significantly increased input noise robustness, evidenced by
flatter MSE curves and more stable SMAPE values. These results highlight the
method's potential, underscore the critical role of adaptive parameter tuning,
and open new avenues for analyzing neural networks as complex adaptive systems
exhibiting emergent behavior within a game-theoretic framework.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [102] [Exploring the Landscape of Fairness Interventions in Software Engineering](https://arxiv.org/abs/2507.18726)
*Sadia Afrin Mim*

Main category: cs.SE

TL;DR: AI应用广泛但有风险，从业者开发公平干预措施，本文对解决公平问题的研究和方法进行综述


<details>
  <summary>Details</summary>
Motivation: AI在现实应用中因数据潜在风险因素存在多重风险和弊端，需解决公平问题

Method: 对已有的解决公平问题的研究和方法进行总结

Result: 文中未提及具体研究结果

Conclusion: 文中未提及明确结论

Abstract: Current developments in AI made it broadly significant for reducing human
labor and expenses across several essential domains, including healthcare and
finance. However, the application of AI in the actual world poses multiple
risks and disadvantages due to potential risk factors in data (e.g., biased
dataset). Practitioners developed a number of fairness interventions for
addressing these kinds of problems. The paper acts as a survey, summarizing the
various studies and approaches that have been developed to address fairness
issues

</details>


### [103] [Agentic Program Repair from Test Failures at Scale: A Neuro-symbolic approach with static analysis and test execution feedback](https://arxiv.org/abs/2507.18755)
*Chandra Maddila,Adam Tait,Claire Chang,Daniel Cheng,Nauman Ahmad,Vijayaraghavan Murali,Marshall Roch,Arnaud Avondet,Aaron Meltzer,Victor Montalvao,Michael Hopko,Chris Waterson,Parth Thakkar,Renuka Fernandez,Kristian Kristensen,Sivan Barzily,Sherry Chen,Rui Abreu,Nachiappan Nagappan,Payam Shodjai,Killian Murphy,James Everingham,Aparna Ramani,Peter C. Rigby*

Main category: cs.SE

TL;DR: 本文开发基于Llama的工程代理修复代码，离线评估表明特定70B模型表现佳，生产中部分修复被落地，获工程师积极反馈。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型出现，在拥有大型代码库的大公司实现大规模代理程序修复成为可能，因此开发能大规模修复代码的工程代理。

Method: 以Llama为基础，采用ReAct框架开发代理，从规则测试失败处开始，设置代理框架并让其执行15个动作，通过静态分析和测试失败提供反馈，利用大语言模型作为评判确保补丁符合标准。

Result: 离线评估中特定70B模型与更大的Llama - 405B有竞争力；ReAct框架受益于符号信息；有模型基准解决率42.3%。生产中80%生成的修复被审查，31.5%被落地。工程师有积极和混合反馈。

Conclusion: 开发的工程代理在代码修复上有一定效果，能为代码修复工作提供帮助。

Abstract: Aim: With the advent of LLMs, sophisticated agentic program repair has become
viable at large organizations with large codebases. In this work, we develop an
Engineering Agent that fixes the source code based on test failures at scale
across diverse software offerings internally.
  Method: Using Llama as the base, we employ the ReAct harness to develop an
agent. We start with a test failure that was triaged by a rule-based test
failure bot. We then set up an agentic harness and allow the agent to reason
and run a set of 15 actions from reading a file to generating a patch. We
provide feedback to the agent through static analysis and test failures so it
can refine its solution. We leverage an LLM-as-a-Judge to ensure that the patch
conforms to the standards followed by a human review to land fixes.
  Benchmark Findings: We curated offline benchmarks for our patch generator,
the Engineering Agent loop, and the LLM-as-a-Judge. In offline evaluations we
found that a specialized 70B model is highly competitive with the much larger
but vanilla Llama-405B. In an ablation study, we found that the ReAct harness
(neural model) benefited from the symbolic information from static analysis
tools and test execution traces. A model that strikes a balance between the
solve rate and error rate vs the cost and latency has a benchmark solve rate of
42.3% using an average 11.8 feedback iterations.
  Production Findings: In a three month period, 80% of the generated fixes were
reviewed, of which 31.5% were landed (25.5% of the total number of generated
fixes).
  Feedback from Engineers: We used open coding to extract qualitative themes
from engineers' feedback. We saw positive feedback in the form of quick
approvals, gratitude, and surprise. We also found mixed feedback when the
Engineering Agent's solution was partially correct and it served as a good
starting point.

</details>


### [104] [MemoCoder: Automated Function Synthesis using LLM-Supported Agents](https://arxiv.org/abs/2507.18812)
*Yiping Jia,Zhen Ming Jiang,Shayan Noei,Ying Zou*

Main category: cs.SE

TL;DR: 本文提出MemoCoder多智能体框架解决大语言模型代码生成问题，实验显示其性能优于零样本提示和自我修复策略。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在代码生成中面临迭代调试、错误处理等挑战，现有方法存在成本高或缺乏知识积累复用机制的问题。

Method: 提出MemoCoder框架，核心是存储成功修复方案的修复知识集，有中央导师智能体监督修复过程。

Result: 在三个公共基准测试中，MemoCoder在Pass@10上提升3.1% - 12.1%，在Pass@50上提升1.4% - 14.5%，优于零样本提示和自我修复策略。

Conclusion: MemoCoder在迭代优化和知识引导的代码生成方面有效。

Abstract: With the widespread adoption of Large Language Models (LLMs) such as GitHub
Copilot and ChatGPT, developers increasingly rely on AI-assisted tools to
support code generation. While LLMs can generate syntactically correct
solutions for well-structured programming tasks, they often struggle with
challenges that require iterative debugging, error handling, or adaptation to
diverse problem structures. Existing approaches such as fine-tuning or
self-repair strategies either require costly retraining or lack mechanisms to
accumulate and reuse knowledge from previous attempts.
  To address these limitations, we propose MemoCoder, a multi-agent framework
that enables collaborative problem solving and persistent learning from past
fixes. At the core of MemoCoder is a Fixing Knowledge Set, which stores
successful repairs and supports retrieval for future tasks. A central Mentor
Agent supervises the repair process by identifying recurring error patterns and
refining high-level fixing strategies, providing a novel supervisory role that
guides the self-repair loop. We evaluate MemoCoder across three public
benchmarks -- MBPP, HumanEval, and LiveCodeBench -- spanning a range of problem
complexities. Experimental results show that MemoCoder consistently outperforms
both zero-shot prompting and a Self-Repair strategy, with improvements ranging
from 3.1% to 12.1% in Pass@10 and from 1.4% to 14.5% in Pass@50, demonstrating
its effectiveness in iterative refinement and knowledge-guided code generation.

</details>


### [105] [Exploring the Jupyter Ecosystem: An Empirical Study of Bugs and Vulnerabilities](https://arxiv.org/abs/2507.18833)
*Wenyuan Jiang,Diany Pressato,Harsh Darji,Thibaud Lutellier*

Main category: cs.SE

TL;DR: 本文对Notebook生态系统中的漏洞和安全隐患进行大规模实证研究，发现配置问题和API使用错误常见，且Notebook支持不如传统软件。


<details>
  <summary>Details</summary>
Motivation: 现有软件工程模型、工具和研究无法捕捉Notebook行为的独特性，因此需要对Notebook生态系统中的漏洞和安全隐患进行大规模实证研究。

Method: 从两个主要平台收集并分析大量Notebook数据集，进行定量分析以确定与漏洞相关的因素，用扎根理论进行定性研究对漏洞分类，分析安全相关提交和漏洞报告评估风险。

Result: 配置问题是Notebook文档中最常见的漏洞之一，其次是API使用错误，还探讨了流行部署框架的常见漏洞。

Conclusion: Notebook不如传统软件受支持，导致代码更复杂、配置错误和维护不佳。

Abstract: Background. Jupyter notebooks are one of the main tools used by data
scientists. Notebooks include features (configuration scripts, markdown,
images, etc.) that make them challenging to analyze compared to traditional
software. As a result, existing software engineering models, tools, and studies
do not capture the uniqueness of Notebook's behavior. Aims. This paper aims to
provide a large-scale empirical study of bugs and vulnerabilities in the
Notebook ecosystem. Method. We collected and analyzed a large dataset of
Notebooks from two major platforms. Our methodology involved quantitative
analyses of notebook characteristics (such as complexity metrics, contributor
activity, and documentation) to identify factors correlated with bugs.
Additionally, we conducted a qualitative study using grounded theory to
categorize notebook bugs, resulting in a comprehensive bug taxonomy. Finally,
we analyzed security-related commits and vulnerability reports to assess risks
associated with Notebook deployment frameworks. Results. Our findings highlight
that configuration issues are among the most common bugs in notebook documents,
followed by incorrect API usage. Finally, we explore common vulnerabilities
associated with popular deployment frameworks to better understand risks
associated with Notebook development. Conclusions. This work highlights that
notebooks are less well-supported than traditional software, resulting in more
complex code, misconfiguration, and poor maintenance.

</details>


### [106] [SLICEMATE: Accurate and Scalable Static Program Slicing via LLM-Powered Agents](https://arxiv.org/abs/2507.18957)
*Jianming Chang,Jieke Shi,Yunbo Lyu,Xin Zhou,Lulu Wang,Zhou Yang,Bixin Li,David Lo*

Main category: cs.SE

TL;DR: 提出基于大语言模型代理的静态程序切片方案SliceMate，构建SliceBench基准测试，实验显示其性能远超传统和基于学习的切片工具。


<details>
  <summary>Details</summary>
Motivation: 传统切片工具难以处理大型程序和语法不完整代码，基于学习的方法在格式良好代码上性能不如传统方法。

Method: 提出SliceMate，集成合成、验证、细化三个代理，并由控制模块协调；构建SliceBench基准测试。

Result: SliceMate大大优于传统和基于学习的切片工具。

Conclusion: SliceMate是一种有效的静态程序切片解决方案，能在不同类型代码上实现高质量切片。

Abstract: Static program slicing, which extracts the executable portions of a program
that affect the values at a specific location, supports many software analysis
tasks such as debugging and security auditing. However, traditional slicing
tools rely on computationally expensive reachability analysis over dependency
graphs, which struggle to scale to large programs and often fail to handle code
with incomplete syntax. Recently emerged learning-based methods, while more
robust to such cases, still fall short of achieving comparable performance to
traditional methods on well-formed code.
  In this work, we propose SliceMate, a novel static program slicing solution
powered by Large Language Model (LLM) agents. It bypasses the need for explicit
dependency graph construction and achieving superior slicing accuracy.
Concretely, SliceMate integrates three specialized agents: (1) a synthesis
agent that produces candidate slices by incrementally expanding the scan scope
across functions and files guided by LLM-inferred dependencies; (2) a
verification agent that performs conciseness and completeness checks of the
candidate slices, detecting missing or irrelevant statements; and (3) a
refinement agent that repairs the slices with minimal edits in accordance with
the verification results. These agents are orchestrated by a control module
that ensures timely convergence and outputs high-quality slices without manual
intervention. For rigorous evaluation, we construct a new and high-quality
benchmark, SliceBench, comprising 2,200 manually annotated Java and Python
programs, with program lengths ranging from 5 to 8,577 lines, significantly
larger than those in existing slicing benchmarks. Experimental results show
that SliceMate greatly outperforms both traditional and learning-based slicing
tools.

</details>


### [107] [Classifying Issues in Open-source GitHub Repositories](https://arxiv.org/abs/2507.18982)
*Amir Hossain Raaj,Fairuz Nawer Meem,Sadia Afrin Mim*

Main category: cs.SE

TL;DR: 利用ML和DNN模型对GitHub开源社区问题进行分类，发现DNN模型表现更佳。


<details>
  <summary>Details</summary>
Motivation: GitHub多数仓库未对问题进行常规标记，标记问题可助力开发者利用先验知识解决问题、加速开发进程。

Method: 分析知名GitHub开源仓库，用常见标签对问题分类。

Result: 研究表明DNN模型表现更好。

Conclusion: 未明确提及，但暗示DNN模型在开源社区问题分类上有优势。

Abstract: GitHub is the most widely used platform for software maintenance in the
open-source community. Developers report issues on GitHub from time to time
while facing difficulties. Having labels on those issues can help developers
easily address those issues with prior knowledge of labels. However, most of
the GitHub repositories do not maintain regular labeling for the issues. The
goal of this work is to classify issues in the open-source community using ML
\& DNN models. There are thousands of open-source repositories on GitHub. Some
of the repositories label their issues properly whereas some of them do not.
When issues are pre-labeled, the problem-solving process and the immediate
assignment of corresponding personnel are facilitated for the team, thereby
expediting the development process. In this work, we conducted an analysis of
prominent GitHub open-source repositories. We classified the issues in some
common labels which are: API, Documentation, Enhancement, Question, Easy,
Help-wanted, Dependency, CI, Waiting for OP's response, Test, Bug, etc. Our
study shows that DNN models outperf

</details>


### [108] [SESR-Eval: Dataset for Evaluating LLMs in the Title-Abstract Screening of Systematic Reviews](https://arxiv.org/abs/2507.19027)
*Aleksi Huotala,Miikka Kuutila,Mika Mäntylä*

Main category: cs.SE

TL;DR: 本文创建基准数据集评估大语言模型在软件工程系统评价标题摘要筛选中的性能，发现模型表现相近但不同二次研究筛选准确率差异大，目前不推荐用大语言模型自动化筛选。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在系统评价标题摘要筛选中性能评估有限，需要创建基准数据集评估其性能，并判断在软件工程领域使用的可行性。

Method: 从169个系统评价研究工件中选取24个纳入数据集，用该数据集对9个大语言模型进行标题摘要筛选基准测试。

Result: 提出含34528个标记初级研究的SESR - Eval数据集；多数大语言模型表现相近，二次研究间筛选准确率差异大于模型间差异；使用大语言模型成本较低。

Conclusion: 该基准可监测软件工程系统评价筛选任务中人工智能性能；目前不推荐用大语言模型自动化筛选，未来将研究影响模型筛选性能的因素。

Abstract: Background: The use of large language models (LLMs) in the title-abstract
screening process of systematic reviews (SRs) has shown promising results, but
suffers from limited performance evaluation. Aims: Create a benchmark dataset
to evaluate the performance of LLMs in the title-abstract screening process of
SRs. Provide evidence whether using LLMs in title-abstract screening in
software engineering is advisable. Method: We start with 169 SR research
artifacts and find 24 of those to be suitable for inclusion in the dataset.
Using the dataset we benchmark title-abstract screening using 9 LLMs. Results:
We present the SESR-Eval (Software Engineering Systematic Review Evaluation)
dataset containing 34,528 labeled primary studies, sourced from 24 secondary
studies published in software engineering (SE) journals. Most LLMs performed
similarly and the differences in screening accuracy between secondary studies
are greater than differences between LLMs. The cost of using an LLM is
relatively low - less than $40 per secondary study even for the most expensive
model. Conclusions: Our benchmark enables monitoring AI performance in the
screening task of SRs in software engineering. At present, LLMs are not yet
recommended for automating the title-abstract screening process, since accuracy
varies widely across secondary studies, and no LLM managed a high recall with
reasonable precision. In future, we plan to investigate factors that influence
LLM screening performance between studies.

</details>


### [109] [Exploring the Use of LLMs for Requirements Specification in an IT Consulting Company](https://arxiv.org/abs/2507.19113)
*Liliana Pasquale,Azzurra Ragone,Emanuele Piemontese,Armin Amiri Darban*

Main category: cs.SE

TL;DR: 本文介绍在 IT 咨询公司用大语言模型（LLMs）自动化需求规格说明流程的经验，对比其与人工生成的结果，表明 LLMs 可助力自动化和标准化，但依赖输入且需人工修订，倡导人机协同。


<details>
  <summary>Details</summary>
Motivation: 解决需求规格说明过程中知识分散、过程繁琐耗时的问题。

Method: 向 LLMs 提供需求获取文档摘要和 FDS 模板，让其生成 Epic FDS 和用户故事并编译成完整 FDS 文档，对比三种先进 LLMs 与人类分析师生成 FDS 的正确性和质量。

Result: LLMs 可帮助自动化和标准化需求规格说明，减少时间和人力，但生成的 FDS 质量高度依赖输入，常需人工修订。

Conclusion: 倡导采用协同方法，LLMs 作为有效起草工具，人类分析师提供关键的上下文和技术监督以生成高质量需求工程文档。

Abstract: In practice, requirements specification remains a critical challenge. The
knowledge necessary to generate a specification can often be fragmented across
diverse sources (e.g., meeting minutes, emails, and high-level product
descriptions), making the process cumbersome and time-consuming. In this paper,
we report our experience using large language models (LLMs) in an IT consulting
company to automate the requirements specification process. In this company,
requirements are specified using a Functional Design Specification (FDS), a
document that outlines the functional requirements and features of a system,
application, or process. We provide LLMs with a summary of the requirements
elicitation documents and FDS templates, prompting them to generate Epic FDS
(including high-level product descriptions) and user stories, which are
subsequently compiled into a complete FDS document. We compared the correctness
and quality of the FDS generated by three state-of-the-art LLMs against those
produced by human analysts. Our results show that LLMs can help automate and
standardize the requirements specification, reducing time and human effort.
However, the quality of LLM-generated FDS highly depends on inputs and often
requires human revision. Thus, we advocate for a synergistic approach in which
an LLM serves as an effective drafting tool while human analysts provide the
critical contextual and technical oversight necessary for high-quality
requirements engineering (RE) documentation.

</details>


### [110] [SDVDiag: A Modular Platform for the Diagnosis of Connected Vehicle Functions](https://arxiv.org/abs/2507.19403)
*Matthias Weiß,Falk Dettinger,Michael Weyrich*

Main category: cs.SE

TL;DR: 本文提出SDVDiag平台用于联网汽车功能自动诊断，部署在5G测试车队环境评估，结果显示能可靠检测注入故障，可提前发现问题及原因。


<details>
  <summary>Details</summary>
Motivation: 联网汽车对可靠性和可用性要求高，需快速解决故障，但手动分析不可行，需自动化诊断方案。

Method: 提出SDVDiag平台，创建涵盖数据收集到追踪潜在根源的管道，支持运行时模块交换，检测和更新功能依赖形成动态图，监控系统指标异常，调查事件时快照并分析。

Result: 在5G测试车队环境中，平台能可靠检测注入的故障。

Conclusion: 该平台有潜力获得新见解，通过早期识别问题及原因减少停机时间。

Abstract: Connected and software-defined vehicles promise to offer a broad range of
services and advanced functions to customers, aiming to increase passenger
comfort and support autonomous driving capabilities. Due to the high
reliability and availability requirements of connected vehicles, it is crucial
to resolve any occurring failures quickly. To achieve this however, a complex
cloud/edge architecture with a mesh of dependencies must be navigated to
diagnose the responsible root cause. As such, manual analyses become unfeasible
since they would significantly delay the troubleshooting.
  To address this challenge, this paper presents SDVDiag, an extensible
platform for the automated diagnosis of connected vehicle functions. The
platform enables the creation of pipelines that cover all steps from initial
data collection to the tracing of potential root causes. In addition, SDVDiag
supports self-adaptive behavior by the ability to exchange modules at runtime.
Dependencies between functions are detected and continuously updated, resulting
in a dynamic graph view of the system. In addition, vital system metrics are
monitored for anomalies. Whenever an incident is investigated, a snapshot of
the graph is taken and augmented by relevant anomalies. Finally, the analysis
is performed by traversing the graph and creating a ranking of the most likely
causes.
  To evaluate the platform, it is deployed inside an 5G test fleet environment
for connected vehicle functions. The results show that injected faults can be
detected reliably. As such, the platform offers the potential to gain new
insights and reduce downtime by identifying problems and their causes at an
early stage.

</details>


### [111] [Automated Code Review Using Large Language Models at Ericsson: An Experience Report](https://arxiv.org/abs/2507.19115)
*Shweta Ramesh,Joy Bose,Hamender Singh,A K Raghavan,Sujoy Roychowdhury,Giriprasad Sridhara,Nishrith Saini,Ricardo Britto*

Main category: cs.SE

TL;DR: 本文介绍了爱立信利用大语言模型自动化代码审查过程的经验，开发了相关工具并取得初步积极结果。


<details>
  <summary>Details</summary>
Motivation: 代码审查需要有经验的开发者，他们可能没时间深入审查，自动化代码审查可减轻其认知负担。

Method: 使用大语言模型和静态程序分析开发轻量级工具。

Result: 与有经验的开发者进行初步实验，取得了令人鼓舞的结果。

Conclusion: 利用大语言模型自动化代码审查过程有一定成效。

Abstract: Code review is one of the primary means of assuring the quality of released
software along with testing and static analysis. However, code review requires
experienced developers who may not always have the time to perform an in-depth
review of code. Thus, automating code review can help alleviate the cognitive
burden on experienced software developers allowing them to focus on their
primary activities of writing code to add new features and fix bugs. In this
paper, we describe our experience in using Large Language Models towards
automating the code review process in Ericsson. We describe the development of
a lightweight tool using LLMs and static program analysis. We then describe our
preliminary experiments with experienced developers in evaluating our code
review tool and the encouraging results.

</details>


### [112] [An OpenSource CI/CD Pipeline for Variant-Rich Software-Defined Vehicles](https://arxiv.org/abs/2507.19446)
*Matthias Weiß,Anish Navalgund,Johannes Stümpfle,Falk Dettinger,Michael Weyrich*

Main category: cs.SE

TL;DR: 本文提出适用于软件定义车辆（SDV）的开源CI/CD管道，评估显示其能有效管理软件变体和OTA更新。


<details>
  <summary>Details</summary>
Motivation: SDV功能多样且通过OTA更新，软件版本和变体增多，缺乏统一集成环境，需动态编排功能以确保异构系统可靠运行。

Method: 使用容器化开源工具组合自动化构建、测试和部署阶段，创建标准化、可移植和可扩展生态系统；开发自定义OTA中间件分发软件更新并支持回滚；基于部署目标依赖和硬件配置派生更新变体；支持自动驾驶AI模型的持续开发和部署。

Result: 在自动代客泊车场景评估中，实现无缝OTA更新、正确选择变体并在所有目标上成功编排。

Conclusion: 所提出的管道为SDV管理软件变体和OTA更新提供可扩展和高效的解决方案，有助于未来移动技术的发展。

Abstract: Software-defined vehicles (SDVs) offer a wide range of connected
functionalities, including enhanced driving behavior and fleet management.
These features are continuously updated via over-the-air (OTA) mechanisms,
resulting in a growing number of software versions and variants due to the
diversity of vehicles, cloud/edge environments, and stakeholders involved. The
lack of a unified integration environment further complicates development, as
connected mobility solutions are often built in isolation. To ensure reliable
operations across heterogeneous systems, a dynamic orchestration of functions
that considers hardware and software variability is essential. This paper
presents an open-source CI/CD pipeline tailored for SDVs. It automates the
build, test, and deployment phases using a combination of containerized
open-source tools, creating a standardized, portable, and scalable ecosystem
accessible to all stakeholders. Additionally, a custom OTA middleware
distributes software updates and supports rollbacks across vehicles and backend
services. Update variants are derived based on deployment target dependencies
and hardware configurations. The pipeline also supports continuous development
and deployment of AI models for autonomous driving features. Its effectiveness
is evaluated using an automated valet parking (AVP) scenario involving
TurtleBots and a coordinating backend server. Two object detection variants are
developed and deployed to match hardware-specific requirements. Results
demonstrate seamless OTA updates, correct variant selection, and successful
orchestration across all targets. Overall, the proposed pipeline provides a
scalable and efficient solution for managing software variants and OTA updates
in SDVs, contributing to the advancement of future mobility technologies.

</details>


### [113] [Fine-Tuning Multilingual Language Models for Code Review: An Empirical Study on Industrial C# Projects](https://arxiv.org/abs/2507.19271)
*Igli Begolli,Meltem Aksoy,Daniel Neider*

Main category: cs.SE

TL;DR: 研究对开源语言模型在单语微调后于三个自动代码审查任务上的表现进行评估，发现单语微调有优势，人类审查者在处理复杂任务上更优。


<details>
  <summary>Details</summary>
Motivation: 代码审查耗时且要求高，语言模型发展为自动化审查带来新途径，需评估单语微调对开源语言模型在自动代码审查任务中的性能影响。

Method: 在C#特定数据集上微调CodeReviewer、CodeLlama - 7B和DeepSeek - R1 - Distill三个模型，研究训练数据中编程语言和自然语言配置对模型性能的影响，将微调模型与自动化软件分析工具和人类审查者进行基准测试。

Result: 单语微调相比多语言基线提高了模型的准确性和相关性，语言模型能有效支持常规代码审查工作流，但人类审查者在处理语义复杂或上下文敏感的更改方面更出色。

Conclusion: 强调语言对齐和特定任务适应对优化自动代码审查语言模型的重要性。

Abstract: Code review is essential for maintaining software quality but often
time-consuming and cognitively demanding, especially in industrial
environments. Recent advancements in language models (LMs) have opened new
avenues for automating core review tasks. This study presents the empirical
evaluation of monolingual fine-tuning on the performance of open-source LMs
across three key automated code review tasks: Code Change Quality Estimation,
Review Comment Generation, and Code Refinement. We fine-tuned three distinct
models, CodeReviewer, CodeLlama-7B, and DeepSeek-R1-Distill, on a C\# specific
dataset combining public benchmarks with industrial repositories. Our study
investigates how different configurations of programming languages and natural
languages in the training data affect LM performance, particularly in comment
generation. Additionally, we benchmark the fine-tuned models against an
automated software analysis tool (ASAT) and human reviewers to evaluate their
practical utility in real-world settings. Our results show that monolingual
fine-tuning improves model accuracy and relevance compared to multilingual
baselines. While LMs can effectively support code review workflows, especially
for routine or repetitive tasks, human reviewers remain superior in handling
semantically complex or context-sensitive changes. Our findings highlight the
importance of language alignment and task-specific adaptation in optimizing LMs
for automated code review.

</details>


### [114] [Mut4All: Fuzzing Compilers via LLM-Synthesized Mutators Learned from Bug Reports](https://arxiv.org/abs/2507.19275)
*Bo Wang,Pengyang Wang,Chong Chen,Qi Sun,Jieke Shi,Chengran Yang,Ming Deng,Youfang Lin,Zhou Yang,David Lo*

Main category: cs.SE

TL;DR: 提出语言无关框架Mut4All合成变异器，处理Rust和C++的错误报告生成变异器，定制模糊测试器找出编译器漏洞，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于变异的模糊测试为现代复杂语言设计高质量变异器依赖人工，可扩展性和跨语言通用性受限。

Method: Mut4All框架含变异器发明、实现合成、细化三个代理，利用大语言模型和错误报告中的编译器知识合成变异器。

Result: 处理1000个错误报告生成319个Rust和403个C++变异器，定制模糊测试器在Rust编译器找到62个漏洞，C++编译器找到34个漏洞，在唯一崩溃检测和覆盖率上表现出色。

Conclusion: Mut4All在编译器漏洞挖掘方面优于现有方法。

Abstract: Mutation-based fuzzing is effective for uncovering compiler bugs, but
designing high-quality mutators for modern languages with complex constructs
(e.g., templates, macros) remains challenging. Existing methods rely heavily on
manual design or human-in-the-loop correction, limiting scalability and
cross-language generalizability.
  We present Mut4All, a fully automated, language-agnostic framework that
synthesizes mutators using Large Language Models (LLMs) and compiler-specific
knowledge from bug reports. It consists of three agents: (1) a mutator
invention agent that identifies mutation targets and generates mutator metadata
using compiler-related insights; (2) a mutator implementation synthesis agent,
fine-tuned to produce initial implementations; and (3) a mutator refinement
agent that verifies and corrects the mutators via unit-test feedback.
  Mut4All processes 1000 bug reports (500 Rust, 500 C++), yielding 319 Rust and
403 C++ mutators at ~$0.08 each via GPT-4o. Our customized fuzzer, using these
mutators, finds 62 bugs in Rust compilers (38 new, 7 fixed) and 34 bugs in C++
compilers (16 new, 1 fixed). Mut4All outperforms existing methods in both
unique crash detection and coverage, ranking first on Rust and second on C++.

</details>


### [115] [ReCatcher: Towards LLMs Regression Testing for Code Generation](https://arxiv.org/abs/2507.19390)
*Altaf Allah Abbassi,Leuson Da Silva,Amin Nikanjam,Foutse Khomh*

Main category: cs.SE

TL;DR: 提出ReCatcher回归测试框架评估代码生成LLM更新中的回归问题，实验显示不同更新场景有不同回归情况，该框架评估效果更好，强调更新前评估的重要性。


<details>
  <summary>Details</summary>
Motivation: LLM代码生成模型更新可能引入正确性、代码质量和性能方面的回归问题，需要评估。

Method: 提出ReCatcher框架，从逻辑正确性、静态代码质量和执行性能三个维度对比两个LLM。

Result: 不同更新场景有不同回归情况，如微调增加语法错误、合并导致正确性下降等，ReCatcher评估准确性更好。

Conclusion: 强调系统的回归评估对采用新模型的重要性，有助于研究人员和从业者做更新决策。

Abstract: Large Language Models (LLMs) for code generation evolve rapidly through
fine-tuning, merging, or new model releases. However, such updates can
introduce regressions, not only in correctness but also in code quality and
performance. To address this, we present ReCatcher, a regression testing
framework for Python code generation. ReCatcher systematically compares two
LLMs, typically a current model and a candidate update, across three
dimensions: logical correctness, static code quality, and execution
performance. We apply ReCatcher to assess regressions across three update
scenarios, fine-tuning, merging, and model release, using CodeLlama,
DeepSeek-Coder, and GPT-4o. Our evaluation shows that fine-tuning with
cross-language datasets increases syntax errors by up to 12%. Merging with
general-purpose models like Llama2 leads to regressions in correctness by up to
18%. GPT-4o introduces regressions of up to 50% in handling missing imports
compared to GPT-3.5-turbo, while GPT-4o-mini suffers up to 80% performance
degradation in execution time versus GPT-4o. Overall, logical correctness,
performance, and error handling (e.g., syntax errors and missing imports) are
the most regression-prone areas. Comparing ReCatcher with baseline solutions,
it presents better and consistent accuracy across logical and performance
aspects. ReCatcher highlights the importance of systematic regression
evaluation before adopting new models, while assisting researchers and
practitioners in making more informed update decisions.

</details>


### [116] [Resolving Build Conflicts via Example-Based and Rule-Based Program Transformations](https://arxiv.org/abs/2507.19432)
*Sheikh Shadab Towqir,Fei He,Todd Mytkowicz,Na Meng*

Main category: cs.SE

TL;DR: 介绍新冲突解决工具BUCOR，通过两种策略检测并解决构建冲突，经评估有一定效果，为未来合并工具指明方向。


<details>
  <summary>Details</summary>
Motivation: 现有工具在解决如方法移除导致的构建冲突方面支持不足，需新工具克服局限。

Method: BUCOR先通过比较合并场景的三个版本检测冲突，采用基于示例转换（BUCOR - E）和基于规则转换（BUCOR - R）两种策略解决冲突。

Result: 在88个真实构建冲突上评估，BUCOR为65个案例生成至少一个解决方案，正确解决43个冲突。

Conclusion: 结合上下文感知的基于示例学习和结构化的基于规则解决的混合方法能有效解决冲突，为更智能自动化合并工具提供方向。

Abstract: Merge conflicts often arise when developers integrate changes from different
software branches. The conflicts can result from overlapping edits in programs
(i.e., textual conflicts) or cause build and test errors (i.e., build and test
conflicts). They degrade software quality and hinder programmer productivity.
While several tools detect build conflicts, few offer meaningful support for
resolving cases like those caused by method removal. To overcome limitations of
existing tools, we introduce BUCOR (Build Conflict Resolver), a new conflict
resolver. BUCOR first detects conflicts by comparing three versions related to
a merging scenario: base b, left l, and right r. To resolve conflicts, it
employs two complementary strategies: example-based transformation (BUCOR-E)
and rule-based transformation (BUCOR-R). BUCOR-R applies predefined rules to
handle common, well-understood conflicts. BUCOR-E mines branch versions (l and
r) for exemplar edits applied to fix related build errors. From these examples,
it infers and generalizes program transformation patterns to resolve more
complex conflicts.
  We evaluated BUCOR on 88 real-world build conflicts spanning 21 distinct
conflict types. BUCOR generated at least one solution for 65 cases and
correctly resolved 43 conflicts. We observed that this hybrid
approach--combining context-aware, example-based learning with structured,
rule-based resolution--can effectively help resolve conflicts. Our research
sheds light on future directions for more intelligent and automated merge
tools.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [117] [Modeling Excess Mortality and Interest Rates using Mixed Fractional Brownian Motions](https://arxiv.org/abs/2507.19445)
*Kenneth Q. Zhou,Hongjuan Zhou*

Main category: q-fin.RM

TL;DR: 本文基于长程依赖特性，构建双变量随机框架对死亡率和利率建模，推导定价公式，提出校准程序并进行数值分析，为后疫情时代与死亡率相关证券的定价和风险管理提供实践启示。


<details>
  <summary>Details</summary>
Motivation: 近期研究发现长程依赖是死亡率和利率动态的关键特征，为对其长记忆行为和瞬时相关性进行联合建模。

Method: 构建基于混合分数布朗运动的双变量随机框架；在风险中性测度下推导定价公式；提出校准程序顺序估计模型和风险溢价参数；进行广泛的数值分析。

Result: 得到零息债券和极端死亡率债券的定价解析解，分析了长程依赖和死亡率 - 利率相关性对公平票面利率、债券支付和风险度量的影响。

Conclusion: 研究结果为后疫情时代与死亡率相关证券的定价和风险管理提供了实践意义。

Abstract: Recent studies have identified long-range dependence as a key feature in the
dynamics of both mortality and interest rates. Building on this insight, we
develop a novel bi-variate stochastic framework based on mixed fractional
Brownian motions to jointly model their long-memory behavior and instantaneous
correlation. Analytical solutions are derived under the risk-neutral measure
for explicitly pricing zero-coupon bonds and extreme mortality bonds, while
capturing the impact of persistent and correlated risk dynamics. We then
propose a calibration procedure that sequentially estimates the model and risk
premium parameters, including the Hurst parameters and the correlation
parameter, using the most recent data on mortality rates, interest rates, and
market conditions. Lastly, an extensive numerical analysis is conducted to
examine how long-range dependence and mortality-interest correlation influence
fair coupon rates, bond payouts and risk measures, providing practical
implications for the pricing and risk management of mortality-linked securities
in the post-pandemic environment.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [118] [A Regression-Based Share Market Prediction Model for Bangladesh](https://arxiv.org/abs/2507.18643)
*Syeda Tasnim Fabiha,Rubaiyat Jahan Mumu,Farzana Aktar,B M Mainul Hossain*

Main category: q-fin.ST

TL;DR: 对达卡证券交易所股市数据进行线性回归分析，对比线性模型和随机森林模型，指出随机森林模型结果更好，明确不同因素对股价变动的影响，且时间序列数据无法生成预测线性模型。


<details>
  <summary>Details</summary>
Motivation: 股市是国家经济发展重要部分，投资者倾向购买市场流动性大的公司股票，市场流动性取决于股价，旨在分析相关因素对股价的影响。

Method: 对达卡证券交易所股市数据进行线性回归分析，并将线性模型与随机森林模型基于不同指标进行对比。

Result: 随机森林模型结果更好，明确了不同因素对股价变动的个体显著程度。

Conclusion: 时间序列数据不能生成用于分析的预测线性模型。

Abstract: Share market is one of the most important sectors of economic development of
a country. Everyday almost all companies issue their shares and investors buy
and sell shares of these companies. Generally investors want to buy shares of
the companies whose market liquidity is comparatively greater. Market liquidity
depends on the average price of a share. In this paper, a thorough linear
regression analysis has been performed on the stock market data of Dhaka Stock
Exchange. Later, the linear model has been compared with random forest based on
different metrics showing better results for random forest model. However, the
amount of individual significance of different factors on the variability of
stock price has been identified and explained. This paper also shows that the
time series data is not capable of generating a predictive linear model for
analysis.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [119] [Central limit theorems for the eigenvalues of graph Laplacians on data clouds](https://arxiv.org/abs/2507.18803)
*Chenghui Li,Nicolás García Trillos,Housen Li,Leo Suchan*

Main category: stat.ML

TL;DR: 研究欧氏空间低维流形分布样本的图拉普拉斯算子特征值渐近波动，证明其渐近高斯性，给出几何与统计解释，还给出多特征值CLT并做数值实验。


<details>
  <summary>Details</summary>
Motivation: 探究图拉普拉斯算子特征值围绕均值的渐近波动情况。

Method: 在数据生成模型和ε衰减率的合适假设下进行理论推导，还通过数值实验验证。

Result: 证明√n(特征值 - 期望特征值)渐近高斯，可明确表征方差，给出几何与统计解释，有多个特征值的CLT。

Conclusion: 图拉普拉斯算子特征值估计有渐近统计有效性，结果在放松部分假设时通过实验验证有效性。

Abstract: Given i.i.d.\ samples $X_n =\{ x_1, \dots, x_n \}$ from a distribution
supported on a low dimensional manifold ${M}$ embedded in Eucliden space, we
consider the graph Laplacian operator $\Delta_n$ associated to an
$\varepsilon$-proximity graph over $X_n$ and study the asymptotic fluctuations
of its eigenvalues around their means. In particular, letting
$\hat{\lambda}_l^\varepsilon$ denote the $l$-th eigenvalue of $\Delta_n$, and
under suitable assumptions on the data generating model and on the rate of
decay of $\varepsilon$, we prove that $\sqrt{n } (\hat{\lambda}_{l}^\varepsilon
- \mathbb{E}[\hat{\lambda}_{l}^\varepsilon] )$ is asymptotically Gaussian with
a variance that we can explicitly characterize. A formal argument allows us to
interpret this asymptotic variance as the dissipation of a gradient flow of a
suitable energy with respect to the Fisher-Rao geometry. This geometric
interpretation allows us to give, in turn, a statistical interpretation of the
asymptotic variance in terms of a Cramer-Rao lower bound for the estimation of
the eigenvalues of certain weighted Laplace-Beltrami operator. The latter
interpretation suggests a form of asymptotic statistical efficiency for the
eigenvalues of the graph Laplacian. We also present CLTs for multiple
eigenvalues and through several numerical experiments explore the validity of
our results when some of the assumptions that we make in our theoretical
analysis are relaxed.

</details>


### [120] [Probably Approximately Correct Causal Discovery](https://arxiv.org/abs/2507.18903)
*Mian Wei,Somesh Jha,David Page*

Main category: stat.ML

TL;DR: 提出PACC发现框架，扩展PAC学习原则到因果领域，并为一些因果方法提供理论保证。


<details>
  <summary>Details</summary>
Motivation: 现实应用存在资源约束，现有因果发现理论在有限数据和时间下表现不佳，需有效方法。

Method: 受Valiant工作启发，提出Probably Approximately Correct Causal (PACC) Discovery框架。

Result: 该框架强调计算和样本效率，能为倾向得分技术、工具变量法等既定因果方法服务，还能为SCCS方法提供理论保证。

Conclusion: PACC发现框架可扩展PAC学习原则到因果领域，为多种因果方法提供理论保障。

Abstract: The discovery of causal relationships is a foundational problem in artificial
intelligence, statistics, epidemiology, economics, and beyond. While elegant
theories exist for accurate causal discovery given infinite data, real-world
applications are inherently resource-constrained. Effective methods for
inferring causal relationships from observational data must perform well under
finite data and time constraints, where "performing well" implies achieving
high, though not perfect accuracy. In his seminal paper A Theory of the
Learnable, Valiant highlighted the importance of resource constraints in
supervised machine learning, introducing the concept of Probably Approximately
Correct (PAC) learning as an alternative to exact learning. Inspired by
Valiant's work, we propose the Probably Approximately Correct Causal (PACC)
Discovery framework, which extends PAC learning principles to the causal field.
This framework emphasizes both computational and sample efficiency for
established causal methods such as propensity score techniques and instrumental
variable approaches. Furthermore, we show that it can also provide theoretical
guarantees for other widely used methods, such as the Self-Controlled Case
Series (SCCS) method, which had previously lacked such guarantees.

</details>


### [121] [Perfect Clustering in Very Sparse Diverse Multiplex Networks](https://arxiv.org/abs/2507.19423)
*Marianna Pensky*

Main category: stat.ML

TL;DR: 研究DIMPLE - SGRDPG网络模型，提出基于张量的方法在稀疏网络中实现层聚类，理论结果显示该方法在一定条件下可达完美聚类。


<details>
  <summary>Details</summary>
Motivation: 现有多层网络层聚类方法依赖层间分析且要求网络足够密集，而恢复具有独特子空间结构的层组任务中，网络为稀疏的情况研究不足。

Method: 将所有层的信息汇集在一起，提供基于张量的方法。

Result: 新方法在直观非严格假设下，能在稀疏条件下实现完美聚类，其稀疏条件在对数因子范围内与简单模型的计算下界一致。

Conclusion: 提出的基于张量的方法可在稀疏网络中有效完成层聚类任务。

Abstract: The paper studies the DIverse MultiPLEx Signed Generalized Random Dot Product
Graph (DIMPLE-SGRDPG) network model (Pensky (2024)), where all layers of the
network have the same collection of nodes. In addition, all layers can be
partitioned into groups such that the layers in the same group are embedded in
the same ambient subspace but otherwise matrices of connection probabilities
can be all different. This setting includes majority of multilayer network
models as its particular cases. The key task in this model is to recover the
groups of layers with unique subspace structures, since the case where all
layers of the network are embedded in the same subspace has been fairly well
studied. Until now, clustering of layers in such networks was based on the
layer-per-layer analysis, which required the multilayer network to be
sufficiently dense. Nevertheless, in this paper we succeeded in pooling
information in all layers together and providing a tensor-based methodology
that ensures perfect clustering for a much sparser network. Our theoretical
results, established under intuitive non-restrictive assumptions, assert that
the new technique achieves perfect clustering under sparsity conditions that,
up to logarithmic factors, coincide with the computational lower bound derived
for a much simpler model.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [122] [Hysteretic Multivariate Bayesian Structural GARCH Model with Soft Information](https://arxiv.org/abs/2507.18990)
*Tzu-Hsin Chien,Ning Ning,Shih-Feng Huang*

Main category: stat.CO

TL;DR: 本文介绍SH - MBS - GARCH模型，提出嵌入软信息方法和贝叶斯估计方法，经模拟验证和实证分析表明该模型拟合和预测效果佳。


<details>
  <summary>Details</summary>
Motivation: 为捕捉多个金融时间序列的联合动态，处理条件异方差性，结合硬信息和软信息构建模型。

Method: 提出将软信息嵌入状态成分的方法，采用结合自适应MCMC、尖峰 - 平板回归和模拟平滑器的贝叶斯估计方法。

Result: 广泛模拟验证了参数估计的准确性；对相关指数的实证分析表明SH - MBS - GARCH模型在拟合和预测准确性上优于竞争模型。

Conclusion: SH - MBS - GARCH模型能有效捕捉状态转换动态，在拟合和预测方面表现出色。

Abstract: This study introduces the SH-MBS-GARCH model, a hysteretic multivariate
Bayesian structural GARCH framework that integrates hard and soft information
to capture the joint dynamics of multiple financial time series, incorporating
hysteretic effects and addressing conditional heteroscedasticity through GARCH
components. Various model specifications could utilize soft information to
define the regime indicator in distinct ways. We propose a flexible,
straightforward method for embedding soft information into the regime
component, applicable across all SH-MBS-GARCH model variants. We further
propose a generally applicable Bayesian estimation approach that combines
adaptive MCMC, spike-and-slab regression, and a simulation smoother, ensuring
accurate parameter estimation, validated through extensive simulations.
Empirical analysis of the Dow Jones Industrial Average, NASDAQ Composite, and
PHLX Semiconductor indices from January 2016 to December 2020 demonstrates that
the SH-MBS-GARCH model outperforms competing models in fitting and prediction
accuracy, effectively capturing regime-switching dynamics.

</details>


### [123] [Branch-and-bound method for calculating Viterbi path in triplet Markov models](https://arxiv.org/abs/2507.19338)
*Oskar Soop,Jüri Lember*

Main category: stat.CO

TL;DR: 本文探讨用分支定界法寻找双变量有限状态马尔可夫链中边际过程X的维特比路径，该方法基于概率上下界，研究受三元马尔可夫模型解码问题启发。


<details>
  <summary>Details</summary>
Motivation: 解决非马尔可夫链的边际过程X寻找最可能路径计算成本高的问题，受三元马尔可夫模型解码或分割问题的启发。

Method: 采用分支定界法，利用(X, Y)的联合马尔可夫性质以低成本计算概率上下界。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: We consider a bivariate, possibly non-homogeneous, finite-state Markov chain
$(X,U)=\{(X_t,U_t)\}_{t=1}^n$. We are interested in the marginal process $X$,
which typically is not a Markov chain. The goal is to find a realization (path)
$x=(x_1,\ldots,x_n)$ with maximal probability $P(X=x)$. If $X$ is Markov chain,
then such path can be efficiently found using the celebrated Viterbi algorithm.
However, when $X$ is not Markovian, identifying the most probable path --
hereafter referred to as the Viterbi path -- becomes computationally expensive.
In this paper, we explore the branch-and-bound method for finding Viterbi
paths. The method is based on the lower and upper bounds on maximum probability
$\max_x P(X=x)$, and the objective of the paper is to exploit the joint Markov
property of $(X,Y)$ to calculate possibly good bounds in possibly cheap way.
  This research is motivated by decoding or segmentation problem in triplet
Markov models. A triplet Markov model is trivariate homogeneous Markov process
$(X,U,Y)$. In decoding, a realization of one marginal process $Y$ is observed
(representing the data), while $X$ and $U$ are latent processes. The process
$U$ serves as a nuisance variable, whereas $X$ is the process of primary
interest. Decoding refers to estimating the hidden sequence $X$ based solely on
the observation $Y$. Conditional on $Y$, the latent processes $(X, U)$ form a
non-homogeneous Markov chain. In this context, the Viterbi path corresponds to
the maximum a posteriori (MAP) estimate of $X$, making it a natural choice for
signal reconstruction.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [124] [RailX: A Flexible, Scalable, and Low-Cost Network Architecture for Hyper-Scale LLM Training Systems](https://arxiv.org/abs/2507.18889)
*Yinxiao Feng,Tiancheng Chen,Yuchen Wei,Siyuan Shen,Shiju Wang,Wei Li,Kaisheng Ma,Torsten Hoefler*

Main category: cs.AR

TL;DR: 本文提出可重构网络架构RailX，基于节点内直接连接和节点间电路交换，有更好扩展性和成本效益，还适用于MLaaS场景。


<details>
  <summary>Details</summary>
Motivation: 现有互联网络架构对超大规模AI工作负载在可扩展性和成本效益上不足，如树状拓扑贵，直接拓扑双分带宽和灵活性不够。

Method: 提出基于Hamiltonian Decomposition理论的互联方法，将独立基于轨道的环组织成all - to - all拓扑，优化环集体和全对全通信。

Result: 能互联超100K高带宽芯片，直径2 - 4跳，每注入/All - Reduce带宽和每双分/All - to - All带宽的网络成本低于Fat - Tree，互联200K芯片只需约13亿美元。

Conclusion: RailX可用于MLaaS场景，能灵活映射不同训练工作负载并应对故障。

Abstract: Increasingly large AI workloads are calling for hyper-scale infrastructure;
however, traditional interconnection network architecture is neither scalable
nor cost-effective enough. Tree-based topologies such as the
\textit{Rail-optimized} network are extremely expensive, while direct
topologies such as \textit{Torus} have insufficient bisection bandwidth and
flexibility. In this paper, we propose \textit{RailX}, a reconfigurable network
architecture based on intra-node direct connectivity and inter-node circuit
switching. Nodes and optical switches are physically 2D-organized, achieving
better scalability than existing centralized circuit switching networks. We
propose a novel interconnection method based on \textit{Hamiltonian
Decomposition} theory to organize separate rail-based rings into
\textit{all-to-all} topology, simultaneously optimizing ring-collective and
all-to-all communication. More than $100$K chips with hyper bandwidth can be
interconnected with a flat switching layer, and the diameter is only $2\sim4$
inter-node hops. The network cost per injection/All-Reduce bandwidth of
\textit{RailX} is less than $10\%$ of the Fat-Tree, and the cost per
bisection/All-to-All bandwidth is less than $50\%$ of the Fat-Tree.
Specifically, only $\sim$\$$1.3$B is required to interconnect 200K chips with
1.8TB bandwidth. \textit{RailX} can also be used in the ML-as-a-service (MLaaS)
scenario, where single or multiple training workloads with various shapes,
scales, and parallelism strategies can be flexibly mapped, and failures can be
worked around.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [125] [Discovering the dynamics of \emph{Sargassum} rafts' centers of mass](https://arxiv.org/abs/2507.18771)
*Francisco J. Beron-Vera,Gage Bonner*

Main category: nlin.CD

TL;DR: 本文评估对比LSTM和SINDy模型预测马尾藻筏质心运动，两模型基于eBOMB，在紧密团块条件下有效，LSTM简单设计时效果好，SINDy有可解释性。


<details>
  <summary>Details</summary>
Motivation: 因缺乏马尾藻筏质心运动预测规律，需用机器学习方法进行预测。

Method: 评估对比LSTM和SINDy模型，采用基于eBOMB的物理启发闭合建模方法。

Result: 两模型在紧密团块条件下最有效，复杂度上升精度下降；LSTM简单设计时结果最佳；SINDy可通过函数库识别显式函数关系。

Conclusion: LSTM和SINDy模型对马尾藻筏质心运动预测有一定效果，SINDy更具可解释性，窗口速度项有助于非局部相互作用建模。

Abstract: Since 2011, rafts of floating \emph{Sargassum} seaweed have frequently
obstructed the coasts of the Intra-Americas Seas. The motion of the rafts is
represented by a high-dimensional nonlinear dynamical system. Referred to as
the eBOMB model, this builds on the Maxey--Riley equation by incorporating
interactions between clumps of \emph{Sargassum} forming a raft and the effects
of Earth's rotation. The absence of a predictive law for the rafts' centers of
mass suggests a need for machine learning. In this paper, we evaluate and
contrast Long Short-Term Memory (LSTM) Recurrent Neural Networks (RNNs) and
Sparse Identification of Nonlinear Dynamics (SINDy). In both cases, a
physics-inspired closure modeling approach is taken rooted in eBOMB.
Specifically, the LSTM model learns a mapping from a collection of eBOMB
variables to the difference between raft center-of-mass and ocean velocities.
The SINDy model's library of candidate functions is suggested by eBOMB
variables and includes windowed velocity terms incorporating far-field effects
of the carrying flow. Both LSTM and SINDy models perform most effectively in
conditions with tightly bonded clumps, despite declining precision with rising
complexity, such as with wind effects and when assessing loosely connected
clumps. The LSTM model delivered the best results when designs were
straightforward, with fewer neurons and hidden layers. While LSTM model serves
as an opaque black-box model lacking interpretability, the SINDy model brings
transparency by discerning explicit functional relationships through the
function libraries. Integration of the windowed velocity terms enabled
effective modeling of nonlocal interactions, particularly in datasets featuring
sparsely connected rafts.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [126] [Flow Stochastic Segmentation Networks](https://arxiv.org/abs/2507.18838)
*Fabio De Sousa Ribeiro,Omar Todd,Charles Jones,Avinash Kori,Raghav Mehta,Ben Glocker*

Main category: cs.CV

TL;DR: 介绍Flow - SSN生成式分割模型，能估计高秩像素协方差，采样更高效，在医学成像基准测试中达SOTA。


<details>
  <summary>Details</summary>
Motivation: 克服先前方法低秩参数化的局限性，提升分割模型性能。

Method: 提出Flow - SSN，包含离散时间自回归和现代连续时间流变体，将大部分模型容量用于学习流的基础分布。

Result: Flow - SSN能估计任意高秩像素协方差，采样比标准扩散分割模型更高效，在医学成像基准测试中取得了最先进的结果。

Conclusion: Flow - SSN是一种有效的生成式分割模型，在医学成像领域具有应用潜力。

Abstract: We introduce the Flow Stochastic Segmentation Network (Flow-SSN), a
generative segmentation model family featuring discrete-time autoregressive and
modern continuous-time flow variants. We prove fundamental limitations of the
low-rank parameterisation of previous methods and show that Flow-SSNs can
estimate arbitrarily high-rank pixel-wise covariances without assuming the rank
or storing the distributional parameters. Flow-SSNs are also more efficient to
sample from than standard diffusion-based segmentation models, thanks to most
of the model capacity being allocated to learning the base distribution of the
flow, constituting an expressive prior. We apply Flow-SSNs to challenging
medical imaging benchmarks and achieve state-of-the-art results. Code
available: https://github.com/biomedia-mira/flow-ssn.

</details>


### [127] [A New One-Shot Federated Learning Framework for Medical Imaging Classification with Feature-Guided Rectified Flow and Knowledge Distillation](https://arxiv.org/abs/2507.19045)
*Yufei Ma,Hanwen Zhang,Qiya Yang,Guibo Luo,Yuesheng Zhu*

Main category: cs.CV

TL;DR: 本文提出改进的OSFL框架，含FG - RF和DLKD方法，在非IID医学影像数据集上表现优于多轮联邦学习及基线方法，且降低隐私泄露风险。


<details>
  <summary>Details</summary>
Motivation: 现有基于生成模型的OSFL方法在医疗领域存在训练效率低、隐私泄露风险，且在非IID数据下单轮模型聚合难收敛。

Method: 提出改进的OSFL框架，开发FG - RF在客户端加速医学影像生成建模并保护隐私，用DLKD处理非IID分布。

Result: 在三个非IID医学影像数据集上，新框架和方法优于多轮联邦学习方法，最多提升21.73%，平均超基线FedISCA 21.75%，特征级合成图像降低隐私泄露风险。

Conclusion: 新的改进OSFL框架及方法有效，能提升性能并降低隐私风险。

Abstract: In multi-center scenarios, One-Shot Federated Learning (OSFL) has attracted
increasing attention due to its low communication overhead, requiring only a
single round of transmission. However, existing generative model-based OSFL
methods suffer from low training efficiency and potential privacy leakage in
the healthcare domain. Additionally, achieving convergence within a single
round of model aggregation is challenging under non-Independent and Identically
Distributed (non-IID) data. To address these challenges, in this paper a
modified OSFL framework is proposed, in which a new Feature-Guided Rectified
Flow Model (FG-RF) and Dual-Layer Knowledge Distillation (DLKD) aggregation
method are developed. FG-RF on the client side accelerates generative modeling
in medical imaging scenarios while preserving privacy by synthesizing
feature-level images rather than pixel-level images. To handle non-IID
distributions, DLKD enables the global student model to simultaneously mimic
the output logits and align the intermediate-layer features of client-side
teacher models during aggregation. Experimental results on three non-IID
medical imaging datasets show that our new framework and method outperform
multi-round federated learning approaches, achieving up to 21.73% improvement,
and exceeds the baseline FedISCA by an average of 21.75%. Furthermore, our
experiments demonstrate that feature-level synthetic images significantly
reduce privacy leakage risks compared to pixel-level synthetic images.

</details>


### [128] [Closing the Modality Gap for Mixed Modality Search](https://arxiv.org/abs/2507.19054)
*Binxu Li,Yuhui Zhang,Xiaohan Wang,Weixin Liang,Ludwig Schmidt,Serena Yeung-Levy*

Main category: cs.CV

TL;DR: 研究对比视觉语言模型在混合模态搜索任务的表现，发现模态差距问题，提出GR - CLIP方法解决，在MixBench上效果好。


<details>
  <summary>Details</summary>
Motivation: 混合模态搜索是重要但研究不足的现实应用，需要研究对比视觉语言模型在此任务的表现并解决其存在的问题。

Method: 提出GR - CLIP，一种轻量级事后校准方法，用于消除CLIP嵌入空间的模态差距。

Result: 在MixBench上，GR - CLIP比CLIP的NDCG@10最多提高26个百分点，比近期视觉语言生成嵌入模型高4个百分点，且计算量少75倍。

Conclusion: GR - CLIP能有效解决对比视觉语言模型在混合模态搜索任务中的模态差距问题，提升搜索性能。

Abstract: Mixed modality search -- retrieving information across a heterogeneous corpus
composed of images, texts, and multimodal documents -- is an important yet
underexplored real-world application. In this work, we investigate how
contrastive vision-language models, such as CLIP, perform on the mixed modality
search task. Our analysis reveals a critical limitation: these models exhibit a
pronounced modality gap in the embedding space, where image and text embeddings
form distinct clusters, leading to intra-modal ranking bias and inter-modal
fusion failure. To address this issue, we propose GR-CLIP, a lightweight
post-hoc calibration method that removes the modality gap in CLIP's embedding
space. Evaluated on MixBench -- the first benchmark specifically designed for
mixed modality search -- GR-CLIP improves NDCG@10 by up to 26 percentage points
over CLIP, surpasses recent vision-language generative embedding models by 4
percentage points, while using 75x less compute.

</details>


### [129] [Quantum-Cognitive Tunnelling Neural Networks for Military-Civilian Vehicle Classification and Sentiment Analysis](https://arxiv.org/abs/2507.18645)
*Milan Maksimovic,Anna Bohdanets,Immaculate Motsi-Omoijiade,Guido Governatori,Ivan S. Maksymov*

Main category: cs.CV

TL;DR: 本文采用基于量子隧穿（QT）的神经网络评估其在区分军事和民用车辆图像及情感分析中的有效性，认为可增强战场场景下多模态AI应用。


<details>
  <summary>Details</summary>
Motivation: 过往研究表明将QT概率融入神经网络模型能捕捉人类感知细节，本文旨在探索其在区分特定图像和情感分析上的效果，以及在战场场景多模态AI应用的潜力。

Method: 采用基于QT的神经网络，使用特定军事词汇，对定制的CIFAR格式军事和民用车辆图像及情感进行区分评估。

Result: 文中未明确提及具体结果。

Conclusion: QT-based模型能增强战场场景下多模态AI应用，使AI具备人类推理特征。

Abstract: Prior work has demonstrated that incorporating well-known quantum tunnelling
(QT) probability into neural network models effectively captures important
nuances of human perception, particularly in the recognition of ambiguous
objects and sentiment analysis. In this paper, we employ novel QT-based neural
networks and assess their effectiveness in distinguishing customised
CIFAR-format images of military and civilian vehicles, as well as sentiment,
using a proprietary military-specific vocabulary. We suggest that QT-based
models can enhance multimodal AI applications in battlefield scenarios,
particularly within human-operated drone warfare contexts, imbuing AI with
certain traits of human reasoning.

</details>


### [130] [Gen-AI Police Sketches with Stable Diffusion](https://arxiv.org/abs/2507.18667)
*Nicholas Fidalgo,Aaron Contreras,Katherine Harvey,Johnny Ni*

Main category: cs.CV

TL;DR: 研究使用多模态AI方法自动化和改进嫌疑人素描，开发并评估三条管线，测试显示模型1性能最佳。


<details>
  <summary>Details</summary>
Motivation: 探究使用多模态AI驱动方法来自动化和增强嫌疑人素描。

Method: 开发并评估三条管线，包括基础图像到图像的Stable Diffusion模型、集成预训练CLIP模型的同一模型以及结合LoRA微调CLIP模型的新方法，并进行消融研究和性能测试。

Result: 消融研究表明微调自注意力和交叉注意力层对齐效果最佳；性能测试中模型1结构相似度和峰值信噪比最高，迭代优化后模型3感知相似度有提升但仍落后于模型1；模型1生成的素描面部特征最清晰。

Conclusion: 模型1作为基线模型虽简单但具有较强鲁棒性。

Abstract: This project investigates the use of multimodal AI-driven approaches to
automate and enhance suspect sketching. Three pipelines were developed and
evaluated: (1) baseline image-to-image Stable Diffusion model, (2) same model
integrated with a pre-trained CLIP model for text-image alignment, and (3)
novel approach incorporating LoRA fine-tuning of the CLIP model, applied to
self-attention and cross-attention layers, and integrated with Stable
Diffusion. An ablation study confirmed that fine-tuning both self- and
cross-attention layers yielded the best alignment between text descriptions and
sketches. Performance testing revealed that Model 1 achieved the highest
structural similarity (SSIM) of 0.72 and a peak signal-to-noise ratio (PSNR) of
25 dB, outperforming Model 2 and Model 3. Iterative refinement enhanced
perceptual similarity (LPIPS), with Model 3 showing improvement over Model 2
but still trailing Model 1. Qualitatively, sketches generated by Model 1
demonstrated the clearest facial features, highlighting its robustness as a
baseline despite its simplicity.

</details>


### [131] [Towards Scalable Spatial Intelligence via 2D-to-3D Data Lifting](https://arxiv.org/abs/2507.18678)
*Xingyu Miao,Haoran Duan,Quanhao Qian,Jiuniu Wang,Yang Long,Ling Shao,Deli Zhao,Ran Xu,Gongjie Zhang*

Main category: cs.CV

TL;DR: 提出可扩展管道将单视图图像转换为3D表示，减少数据收集成本，发布两个数据集并证明对3D任务有益。


<details>
  <summary>Details</summary>
Motivation: 空间智能受限于3D数据集稀缺，获取3D数据困难，需利用大量2D图像。

Method: 通过集成深度估计、相机校准和尺度校准，将单视图图像转换为3D表示。

Result: 发布COCO - 3D和Objects365 - v2 - 3D数据集，实验证明生成的数据对多种3D任务有益。

Conclusion: 该管道是开发能感知、理解和与物理环境交互的AI系统的有效解决方案。

Abstract: Spatial intelligence is emerging as a transformative frontier in AI, yet it
remains constrained by the scarcity of large-scale 3D datasets. Unlike the
abundant 2D imagery, acquiring 3D data typically requires specialized sensors
and laborious annotation. In this work, we present a scalable pipeline that
converts single-view images into comprehensive, scale- and appearance-realistic
3D representations - including point clouds, camera poses, depth maps, and
pseudo-RGBD - via integrated depth estimation, camera calibration, and scale
calibration. Our method bridges the gap between the vast repository of imagery
and the increasing demand for spatial scene understanding. By automatically
generating authentic, scale-aware 3D data from images, we significantly reduce
data collection costs and open new avenues for advancing spatial intelligence.
We release two generated spatial datasets, i.e., COCO-3D and Objects365-v2-3D,
and demonstrate through extensive experiments that our generated data can
benefit various 3D tasks, ranging from fundamental perception to MLLM-based
reasoning. These results validate our pipeline as an effective solution for
developing AI systems capable of perceiving, understanding, and interacting
with physical environments.

</details>


### [132] [Tell Me What You See: An Iterative Deep Learning Framework for Image Captioning](https://arxiv.org/abs/2507.18788)
*Hitesh Kumar Gupta*

Main category: cs.CV

TL;DR: 本文系统迭代开发图像描述模型，从简单CNN - LSTM到基于注意力的系统，最终模型Nexus表现出色，为视觉 - 语言任务提供蓝图。


<details>
  <summary>Details</summary>
Motivation: 深入理解图像描述任务所需的视觉场景和语言结构，开发有竞争力的图像描述模型。

Method: 开发从Genesis到Nexus的一系列五个模型，采用EfficientNetV2B3骨干网络和动态注意力机制，在MS COCO 2017数据集上训练。

Result: 最终模型Nexus在MS COCO 2017数据集上BLEU - 4得分31.4，超越多个基础基准。

Conclusion: 研究为理解现代视觉 - 语言任务的核心架构原则提供清晰、可复制的蓝图，验证了向注意力架构转变的合理性。

Abstract: Image captioning, a task at the confluence of computer vision and natural
language processing, requires a sophisticated understanding of both visual
scenes and linguistic structure. While modern approaches are dominated by
large-scale Transformer architectures, this paper documents a systematic,
iterative development of foundational image captioning models, progressing from
a simple CNN-LSTM encoder-decoder to a competitive attention-based system. We
present a series of five models, beginning with Genesis and concluding with
Nexus, an advanced model featuring an EfficientNetV2B3 backbone and a dynamic
attention mechanism. Our experiments chart the impact of architectural
enhancements and demonstrate a key finding within the classic CNN-LSTM
paradigm: merely upgrading the visual backbone without a corresponding
attention mechanism can degrade performance, as the single-vector bottleneck
cannot transmit the richer visual detail. This insight validates the
architectural shift to attention. Trained on the MS COCO 2017 dataset, our
final model, Nexus, achieves a BLEU-4 score of 31.4, surpassing several
foundational benchmarks and validating our iterative design process. This work
provides a clear, replicable blueprint for understanding the core architectural
principles that underpin modern vision-language tasks.

</details>


### [133] [Deepfake Detection Via Facial Feature Extraction and Modeling](https://arxiv.org/abs/2507.18815)
*Benjamin Carter,Nathan Dilla,Micheal Callahan,Atuhaire Ambala*

Main category: cs.CV

TL;DR: 本文提出仅用面部特征点检测深度伪造视频，实验证明该特征提取技术在多种神经网络模型中有效，挑战了原始图像处理的必要性假设。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术使在线媒体真实性存疑，现有模型多关注直接图像处理，需新的检测模型。

Method: 使用包含深度伪造和真实人脸视频的数据集，提取面部特征点，识别面部动作细微不一致。

Result: 在三种神经网络模型中测试相同面部特征点，RNN和ANN模型准确率分别在96% - 93%，CNN模型约78%。

Conclusion: 面部特征提取方法与多种神经网络模型兼容，所需参数少，挑战了原始图像处理检测深度伪造视频的必要性假设。

Abstract: The rise of deepfake technology brings forth new questions about the
authenticity of various forms of media found online today. Videos and images
generated by artificial intelligence (AI) have become increasingly more
difficult to differentiate from genuine media, resulting in the need for new
models to detect artificially-generated media. While many models have attempted
to solve this, most focus on direct image processing, adapting a convolutional
neural network (CNN) or a recurrent neural network (RNN) that directly
interacts with the video image data. This paper introduces an approach of using
solely facial landmarks for deepfake detection. Using a dataset consisting of
both deepfake and genuine videos of human faces, this paper describes an
approach for extracting facial landmarks for deepfake detection, focusing on
identifying subtle inconsistencies in facial movements instead of raw image
processing. Experimental results demonstrated that this feature extraction
technique is effective in various neural network models, with the same facial
landmarks tested on three neural network models, with promising performance
metrics indicating its potential for real-world applications. The findings
discussed in this paper include RNN and artificial neural network (ANN) models
with accuracy between 96% and 93%, respectively, with a CNN model hovering
around 78%. This research challenges the assumption that raw image processing
is necessary to identify deepfake videos by presenting a facial feature
extraction approach compatible with various neural network models while
requiring fewer parameters.

</details>


### [134] [PTCMIL: Multiple Instance Learning via Prompt Token Clustering for Whole Slide Image Analysis](https://arxiv.org/abs/2507.18848)
*Beidi Zhao,SangMook Kim,Hao Chen,Chen Zhou,Zu-hua Gao,Gang Wang,Xiaoxiao Li*

Main category: cs.CV

TL;DR: 提出PTCMIL用于WSI分析的多实例学习，实验证明其性能优越且有代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有多实例学习方法在处理WSI的复杂性和异质性上有困难，ViTs和基于聚类的方法计算量大且无法捕捉特定任务和切片的可变性。

Method: 提出PTCMIL，在ViT骨干中引入可学习的提示令牌，以端到端方式统一聚类和预测任务，使用基于投影的聚类，结合令牌合并和基于原型的池化。

Result: 在八个数据集的分类和生存分析任务中表现优于现有方法，消融实验证明其鲁棒性和可解释性。

Conclusion: PTCMIL能有效解决现有多实例学习方法的局限性，性能优越且具有强可解释性。

Abstract: Multiple Instance Learning (MIL) has advanced WSI analysis but struggles with
the complexity and heterogeneity of WSIs. Existing MIL methods face challenges
in aggregating diverse patch information into robust WSI representations. While
ViTs and clustering-based approaches show promise, they are computationally
intensive and fail to capture task-specific and slide-specific variability. To
address these limitations, we propose PTCMIL, a novel Prompt Token
Clustering-based ViT for MIL aggregation. By introducing learnable prompt
tokens into the ViT backbone, PTCMIL unifies clustering and prediction tasks in
an end-to-end manner. It dynamically aligns clustering with downstream tasks,
using projection-based clustering tailored to each WSI, reducing complexity
while preserving patch heterogeneity. Through token merging and prototype-based
pooling, PTCMIL efficiently captures task-relevant patterns. Extensive
experiments on eight datasets demonstrate its superior performance in
classification and survival analysis tasks, outperforming state-of-the-art
methods. Systematic ablation studies confirm its robustness and strong
interpretability. The code is released at https://github.com/ubc-tea/PTCMIL.

</details>


### [135] [WiSE-OD: Benchmarking Robustness in Infrared Object Detection](https://arxiv.org/abs/2507.18925)
*Heitor R. Medeiros,Atif Belal,Masih Aminbeidokhti,Eric Granger,Marco Pedersoli*

Main category: cs.CV

TL;DR: 提出两个跨模态OOD基准LLVIP - C和FLIR - C，以及权重空间集成方法WiSE - OD，可在无额外成本下提升跨模态和抗干扰鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 红外图像目标检测中因缺乏大规模红外数据集，模型依赖RGB预训练权重，微调后在分布偏移下鲁棒性差，需解决此问题。

Method: 构建两个跨模态OOD基准LLVIP - C和FLIR - C；提出WiSE - OD及其两个变体WiSE - OD$_{ZS}$和WiSE - OD$_{LP}$。

Result: 在三个RGB预训练检测器和两个鲁棒基线模型上评估，WiSE - OD提升了跨模态和抗干扰鲁棒性，且无额外训练和推理成本。

Conclusion: WiSE - OD能有效解决红外图像目标检测在分布偏移下鲁棒性差的问题。

Abstract: Object detection (OD) in infrared (IR) imagery is critical for low-light and
nighttime applications. However, the scarcity of large-scale IR datasets forces
models to rely on weights pre-trained on RGB images. While fine-tuning on IR
improves accuracy, it often compromises robustness under distribution shifts
due to the inherent modality gap between RGB and IR. To address this, we
introduce LLVIP-C and FLIR-C, two cross-modality out-of-distribution (OOD)
benchmarks built by applying corruption to standard IR datasets. Additionally,
to fully leverage the complementary knowledge from RGB and infrared trained
models, we propose WiSE-OD, a weight-space ensembling method with two variants:
WiSE-OD$_{ZS}$, which combines RGB zero-shot and IR fine-tuned weights, and
WiSE-OD$_{LP}$, which blends zero-shot and linear probing. Evaluated across
three RGB-pretrained detectors and two robust baselines, WiSE-OD improves both
cross-modality and corruption robustness without any additional training or
inference cost.

</details>


### [136] [MGHFT: Multi-Granularity Hierarchical Fusion Transformer for Cross-Modal Sticker Emotion Recognition](https://arxiv.org/abs/2507.18929)
*Jian Chen,Yuxuan Hu,Haifeng Lu,Wei Wang,Min Yang,Chengming Li,Xiping Hu*

Main category: cs.CV

TL;DR: 提出多粒度分层融合变压器MGHFT和基于多模态大语言模型的多视图贴纸解释器解决贴纸情感理解难题，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 预训练视觉模型在贴纸情感理解方面因依赖多视图信息仍具挑战，需新方法解决。

Method: 用多模态大语言模型通过多视图描述提供文本上下文解释贴纸；设计分层融合策略将文本上下文融入视觉理解；通过对比学习和注意力机制在视觉主干不同阶段注入文本特征；引入文本引导融合注意力机制整合多模态特征。

Result: 在2个公开贴纸情感数据集上实验，MGHFT显著优于现有方法，相比最佳预训练视觉模型，F1提高5.4%，准确率提高4.0%。

Conclusion: MGHFT能有效解决贴纸情感理解问题，提升情感识别准确性和细粒度。

Abstract: Although pre-trained visual models with text have demonstrated strong
capabilities in visual feature extraction, sticker emotion understanding
remains challenging due to its reliance on multi-view information, such as
background knowledge and stylistic cues. To address this, we propose a novel
multi-granularity hierarchical fusion transformer (MGHFT), with a multi-view
sticker interpreter based on Multimodal Large Language Models. Specifically,
inspired by the human ability to interpret sticker emotions from multiple
views, we first use Multimodal Large Language Models to interpret stickers by
providing rich textual context via multi-view descriptions. Then, we design a
hierarchical fusion strategy to fuse the textual context into visual
understanding, which builds upon a pyramid visual transformer to extract both
global and local sticker features at multiple stages. Through contrastive
learning and attention mechanisms, textual features are injected at different
stages of the visual backbone, enhancing the fusion of global- and
local-granularity visual semantics with textual guidance. Finally, we introduce
a text-guided fusion attention mechanism to effectively integrate the overall
multimodal features, enhancing semantic understanding. Extensive experiments on
2 public sticker emotion datasets demonstrate that MGHFT significantly
outperforms existing sticker emotion recognition approaches, achieving higher
accuracy and more fine-grained emotion recognition. Compared to the best
pre-trained visual models, our MGHFT also obtains an obvious improvement, 5.4%
on F1 and 4.0% on accuracy. The code is released at
https://github.com/cccccj-03/MGHFT_ACMMM2025.

</details>


### [137] [Underwater Waste Detection Using Deep Learning A Performance Comparison of YOLOv7 to 10 and Faster RCNN](https://arxiv.org/abs/2507.18967)
*UMMPK Nawarathne,HMNS Kumari,HMLS Kumari*

Main category: cs.CV

TL;DR: 研究对比五种目标识别算法在水下垃圾检测性能，YOLOv8表现最佳，有望用于对抗污染。


<details>
  <summary>Details</summary>
Motivation: 水下污染严重，准确检测垃圾对废物管理、环境监测和缓解策略至关重要。

Method: 在包含15个不同类别的大型数据集上，对YOLOv7、YOLOv8、YOLOv9、YOLOv10和Faster R - CNN进行训练和测试。

Result: YOLOv8表现最优，平均精度均值（mAP）达80.9%。

Conclusion: YOLOv8可作为对抗污染的有效工具，提升水下清理作业的检测能力和可扩展性。

Abstract: Underwater pollution is one of today's most significant environmental
concerns, with vast volumes of garbage found in seas, rivers, and landscapes
around the world. Accurate detection of these waste materials is crucial for
successful waste management, environmental monitoring, and mitigation
strategies. In this study, we investigated the performance of five cutting-edge
object recognition algorithms, namely YOLO (You Only Look Once) models,
including YOLOv7, YOLOv8, YOLOv9, YOLOv10, and Faster Region-Convolutional
Neural Network (R-CNN), to identify which model was most effective at
recognizing materials in underwater situations. The models were thoroughly
trained and tested on a large dataset containing fifteen different classes
under diverse conditions, such as low visibility and variable depths. From the
above-mentioned models, YOLOv8 outperformed the others, with a mean Average
Precision (mAP) of 80.9%, indicating a significant performance. This increased
performance is attributed to YOLOv8's architecture, which incorporates advanced
features such as improved anchor-free mechanisms and self-supervised learning,
allowing for more precise and efficient recognition of items in a variety of
settings. These findings highlight the YOLOv8 model's potential as an effective
tool in the global fight against pollution, improving both the detection
capabilities and scalability of underwater cleanup operations.

</details>


### [138] [MedIQA: A Scalable Foundation Model for Prompt-Driven Medical Image Quality Assessment](https://arxiv.org/abs/2507.19004)
*Siyi Xun,Yue Sun,Jingkun Chen,Zitong Yu,Tong Tong,Xiaohong Liu,Mingxiang Wu,Tao Tan*

Main category: cs.CV

TL;DR: 提出首个医学图像质量评估基础模型MedIQA，通过多模态数据集和相关策略，在下游任务中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 医学影像技术发展使精确自动图像质量评估需求迫切，现有方法泛化能力不足。

Method: 开发大规模多模态数据集，集成显著切片评估模块，采用自动提示策略。

Result: MedIQA在多个下游任务中显著优于基线。

Conclusion: MedIQA为医学图像质量评估建立了可扩展框架，推动诊断流程和临床决策。

Abstract: Rapid advances in medical imaging technology underscore the critical need for
precise and automated image quality assessment (IQA) to ensure diagnostic
accuracy. Existing medical IQA methods, however, struggle to generalize across
diverse modalities and clinical scenarios. In response, we introduce MedIQA,
the first comprehensive foundation model for medical IQA, designed to handle
variability in image dimensions, modalities, anatomical regions, and types. We
developed a large-scale multi-modality dataset with plentiful manually
annotated quality scores to support this. Our model integrates a salient slice
assessment module to focus on diagnostically relevant regions feature retrieval
and employs an automatic prompt strategy that aligns upstream physical
parameter pre-training with downstream expert annotation fine-tuning. Extensive
experiments demonstrate that MedIQA significantly outperforms baselines in
multiple downstream tasks, establishing a scalable framework for medical IQA
and advancing diagnostic workflows and clinical decision-making.

</details>


### [139] [Adapt, But Don't Forget: Fine-Tuning and Contrastive Routing for Lane Detection under Distribution Shift](https://arxiv.org/abs/2507.18653)
*Mohammed Abdul Hafeez Khan,Parth Ganeriwala,Sarah M. Lehman,Siddhartha Bhattacharyya,Amy Alvarez,Natasha Neogi*

Main category: cs.CV

TL;DR: 现有车道检测模型在跨数据集微调时会出现灾难性遗忘，本文提出训练基础模型后创建分支适配新分布，推理时用监督对比学习模型路由输入，用更少参数达近最优F1分数。


<details>
  <summary>Details</summary>
Motivation: 解决车道检测模型在跨数据集微调时出现的灾难性遗忘问题。

Method: 先在源分布上训练基础模型，为新目标分布创建单独分支，微调部分组件并固定源分支；通过组件分析确定有效微调策略；推理时用监督对比学习模型识别输入分布并路由到对应分支。

Result: 框架使用显著更少的参数达到了接近最优的F1分数。

Conclusion: 所提出的框架能有效解决跨数据集微调问题，实现参数高效的适配。

Abstract: Lane detection models are often evaluated in a closed-world setting, where
training and testing occur on the same dataset. We observe that, even within
the same domain, cross-dataset distribution shifts can cause severe
catastrophic forgetting during fine-tuning. To address this, we first train a
base model on a source distribution and then adapt it to each new target
distribution by creating separate branches, fine-tuning only selected
components while keeping the original source branch fixed. Based on a
component-wise analysis, we identify effective fine-tuning strategies for
target distributions that enable parameter-efficient adaptation. At inference
time, we propose using a supervised contrastive learning model to identify the
input distribution and dynamically route it to the corresponding branch. Our
framework achieves near-optimal F1-scores while using significantly fewer
parameters than training separate models for each distribution.

</details>


### [140] [MedSymmFlow: Bridging Generative Modeling and Classification in Medical Imaging through Symmetrical Flow Matching](https://arxiv.org/abs/2507.19098)
*Francisco Caetano,Lemar Abdi,Christiaan Viviers,Amaan Valiuddin,Fons van der Sommen*

Main category: cs.CV

TL;DR: 提出MedSymmFlow模型统一医学影像分类、生成和不确定性量化，在多个数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 可靠的医学图像分类需要准确预测和校准良好的不确定性估计，尤其是在高风险临床环境中。

Method: 构建基于对称流匹配的生成 - 判别混合模型MedSymmFlow，利用潜在空间公式，引入语义掩码调节机制。

Result: 在四个MedMNIST数据集上评估，分类准确率和AUC达到或超过现有基线，不确定性估计可靠。

Conclusion: MedSymmFlow能有效统一医学影像分类、生成和不确定性量化，表现良好。

Abstract: Reliable medical image classification requires accurate predictions and
well-calibrated uncertainty estimates, especially in high-stakes clinical
settings. This work presents MedSymmFlow, a generative-discriminative hybrid
model built on Symmetrical Flow Matching, designed to unify classification,
generation, and uncertainty quantification in medical imaging. MedSymmFlow
leverages a latent-space formulation that scales to high-resolution inputs and
introduces a semantic mask conditioning mechanism to enhance diagnostic
relevance. Unlike standard discriminative models, it naturally estimates
uncertainty through its generative sampling process. The model is evaluated on
four MedMNIST datasets, covering a range of modalities and pathologies. The
results show that MedSymmFlow matches or exceeds the performance of established
baselines in classification accuracy and AUC, while also delivering reliable
uncertainty estimates validated by performance improvements under selective
prediction.

</details>


### [141] [Advancing Vision-based Human Action Recognition: Exploring Vision-Language CLIP Model for Generalisation in Domain-Independent Tasks](https://arxiv.org/abs/2507.18675)
*Sanyam Jain,Marsha Mariya Kappan,Vijeta Sharma*

Main category: cs.CV

TL;DR: 评估CLIP在UCF - 101数据集上的表现，分析其在三种掩蔽策略下的性能，提出用特定类噪声改进模型，最后讨论临床应用挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统模型在复杂多样动作识别上泛化能力不足，而基于transformer的CLIP模型有泛化潜力，需评估其性能并改进。

Method: 在UCF - 101数据集上评估CLIP，采用三种掩蔽策略分析其性能，提出用自定义损失函数学习特定类噪声来改进模型。

Result: CLIP在关键视觉线索被遮挡时表现不稳定、误分类多，加入特定类噪声后提高了分类准确率和模型置信度，减少了偏差。

Conclusion: 讨论了模型在临床领域应用的挑战，给出了提升跨领域医疗场景泛化能力的未来工作方向。

Abstract: Human action recognition plays a critical role in healthcare and medicine,
supporting applications such as patient behavior monitoring, fall detection,
surgical robot supervision, and procedural skill assessment. While traditional
models like CNNs and RNNs have achieved moderate success, they often struggle
to generalize across diverse and complex actions. Recent advancements in
vision-language models, especially the transformer-based CLIP model, offer
promising capabilities for generalizing action recognition from video data. In
this work, we evaluate CLIP on the UCF-101 dataset and systematically analyze
its performance under three masking strategies: (1) percentage-based and
shape-based black masking at 10%, 30%, and 50%, (2) feature-specific masking to
suppress bias-inducing elements, and (3) isolation masking that retains only
class-specific regions. Our results reveal that CLIP exhibits inconsistent
behavior and frequent misclassifications, particularly when essential visual
cues are obscured. To overcome these limitations, we propose incorporating
class-specific noise, learned via a custom loss function, to reinforce
attention to class-defining features. This enhancement improves classification
accuracy and model confidence while reducing bias. We conclude with a
discussion on the challenges of applying such models in clinical domains and
outline directions for future work to improve generalizability across
domain-independent healthcare scenarios.

</details>


### [142] [PatchTraj: Dynamic Patch Representation Learning for Time-Frequency Trajectory Prediction](https://arxiv.org/abs/2507.19119)
*Yanghong Liu,Xingping Dong,Ming Li,Weixing Zhang,Yidong Lou*

Main category: cs.CV

TL;DR: 提出PatchTraj框架解决行人轨迹预测现有方法问题，实验证明性能高效且达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于点和网格的行人轨迹预测方法存在对人类运动动力学建模不足，时间表示与频域缺乏交互的问题。

Method: 提出PatchTraj框架，将轨迹分解为时间序列和频率分量，用动态补丁分区进行多尺度分割，经自适应嵌入层、分层特征聚合，两分支通过跨模态注意力交互，最后用Transformer编解码器预测轨迹。

Result: 在ETH - UCY、SDD、NBA和JRDB数据集上实验表明，方法高效且达到了最先进性能。

Conclusion: PatchTraj框架有效解决了现有行人轨迹预测方法的问题，具有良好性能。

Abstract: Pedestrian trajectory prediction is crucial for autonomous driving and
robotics. While existing point-based and grid-based methods expose two key
limitations: insufficiently modeling human motion dynamics, as they fail to
balance local motion details with long-range spatiotemporal dependencies, and
the time representation lacks interaction with the frequency domain in modeling
trajectory sequences. To address these challenges, we propose PatchTraj, a
dynamic patch-based trajectory prediction framework that unifies time-domain
and frequency-domain representations. Specifically, we decompose the trajectory
into raw time sequences and frequency components, employing dynamic patch
partitioning for multi-scale trajectory segmentation to capture hierarchical
motion patterns. Each patch is processed by an adaptive embedding layer with
scale-aware feature extraction, followed by hierarchical feature aggregation to
model both fine-grained and long-range dependencies. The outputs of two
branches interact via cross-modal attention, enabling complementary fusion of
temporal and spectral cues. Finally, a Transformer encoder-decoder integrates
both modalities to autoregressively predict future trajectories. Extensive
experiments on ETH-UCY, SDD, NBA, and JRDB datasets demonstrate that our method
achieves state-of-the-art performance with high efficiency.

</details>


### [143] [Multistream Network for LiDAR and Camera-based 3D Object Detection in Outdoor Scenes](https://arxiv.org/abs/2507.19304)
*Muhammad Ibrahim,Naveed Akhtar,Haitian Wang,Saeed Anwar,Ajmal Mian*

Main category: cs.CV

TL;DR: 提出MultiStream Detection (MuStD)网络融合LiDAR和RGB数据进行户外3D目标检测，在KITTI基准测试取得好结果。


<details>
  <summary>Details</summary>
Motivation: LiDAR和RGB数据融合有提升户外3D目标检测精度的潜力，但有效集成两种模态进行精确目标检测仍是待解决问题。

Method: 提出MuStD网络，采用三流结构，分别提取特征后融合并输入检测头。

Result: 在KITTI目标检测基准测试中取得新的最优或极具竞争力的结果，且效率高。

Conclusion: MuStD网络能有效融合LiDAR和RGB数据进行户外3D目标检测。

Abstract: Fusion of LiDAR and RGB data has the potential to enhance outdoor 3D object
detection accuracy. To address real-world challenges in outdoor 3D object
detection, fusion of LiDAR and RGB input has started gaining traction. However,
effective integration of these modalities for precise object detection task
still remains a largely open problem. To address that, we propose a MultiStream
Detection (MuStD) network, that meticulously extracts task-relevant information
from both data modalities. The network follows a three-stream structure. Its
LiDAR-PillarNet stream extracts sparse 2D pillar features from the LiDAR input
while the LiDAR-Height Compression stream computes Bird's-Eye View features. An
additional 3D Multimodal stream combines RGB and LiDAR features using UV
mapping and polar coordinate indexing. Eventually, the features containing
comprehensive spatial, textural and geometric information are carefully fused
and fed to a detection head for 3D object detection. Our extensive evaluation
on the challenging KITTI Object Detection Benchmark using public testing server
at
https://www.cvlibs.net/datasets/kitti/eval_object_detail.php?&result=d162ec699d6992040e34314d19ab7f5c217075e0
establishes the efficacy of our method by achieving new state-of-the-art or
highly competitive results in different categories while remaining among the
most efficient methods. Our code will be released through MuStD GitHub
repository at https://github.com/IbrahimUWA/MuStD.git

</details>


### [144] [SIDE: Sparse Information Disentanglement for Explainable Artificial Intelligence](https://arxiv.org/abs/2507.19321)
*Viktar Dubovik,Łukasz Struski,Jacek Tabor,Dawid Rymarczyk*

Main category: cs.CV

TL;DR: 提出SIDE方法提升原型部件可解释性，实验表明在不降低准确率下大幅减小解释规模。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在高风险领域缺乏透明度，现有原型部件网络有局限，InfoDisent解释复杂。

Method: 引入Sparse Information Disentanglement for Explainability (SIDE)方法，通过专门训练和剪枝方案强制稀疏性，用sigmoid激活代替softmax。

Result: SIDE达到现有方法的准确率，同时将解释规模减小超90%。

Conclusion: SIDE方法有效提升了基于原型解释的可理解性。

Abstract: Understanding the decisions made by deep neural networks is essential in
high-stakes domains such as medical imaging and autonomous driving. Yet, these
models often lack transparency, particularly in computer vision.
Prototypical-parts-based neural networks have emerged as a promising solution
by offering concept-level explanations. However, most are limited to
fine-grained classification tasks, with few exceptions such as InfoDisent.
InfoDisent extends prototypical models to large-scale datasets like ImageNet,
but produces complex explanations.
  We introduce Sparse Information Disentanglement for Explainability (SIDE), a
novel method that improves the interpretability of prototypical parts through a
dedicated training and pruning scheme that enforces sparsity. Combined with
sigmoid activations in place of softmax, this approach allows SIDE to associate
each class with only a small set of relevant prototypes. Extensive experiments
show that SIDE matches the accuracy of existing methods while reducing
explanation size by over $90\%$, substantially enhancing the understandability
of prototype-based explanations.

</details>


### [145] [LOTUS: A Leaderboard for Detailed Image Captioning from Quality to Societal Bias and User Preferences](https://arxiv.org/abs/2507.19362)
*Yusuke Hirota,Boyi Li,Ryo Hachiuma,Yueh-Hua Wu,Boris Ivanovic,Yuta Nakashima,Marco Pavone,Yejin Choi,Yu-Chiang Frank Wang,Chao-Han Huck Yang*

Main category: cs.CV

TL;DR: 介绍用于评估详细图像描述的LOTUS排行榜，分析LVLMs，发现无模型全面出色，选择取决于用户优先级。


<details>
  <summary>Details</summary>
Motivation: 解决现有图像描述评估中缺乏标准化标准、缺乏偏差感知评估和未考虑用户偏好的问题。

Method: 引入LOTUS排行榜，全面评估图像描述的质量、风险和社会偏差等方面，并进行偏好导向评估。

Result: 近期LVLMs没有一个能在所有标准上表现出色，图像描述详细程度和偏差风险存在相关性。

Conclusion: 最优模型选择取决于用户优先级。

Abstract: Large Vision-Language Models (LVLMs) have transformed image captioning,
shifting from concise captions to detailed descriptions. We introduce LOTUS, a
leaderboard for evaluating detailed captions, addressing three main gaps in
existing evaluations: lack of standardized criteria, bias-aware assessments,
and user preference considerations. LOTUS comprehensively evaluates various
aspects, including caption quality (e.g., alignment, descriptiveness), risks
(\eg, hallucination), and societal biases (e.g., gender bias) while enabling
preference-oriented evaluations by tailoring criteria to diverse user
preferences. Our analysis of recent LVLMs reveals no single model excels across
all criteria, while correlations emerge between caption detail and bias risks.
Preference-oriented evaluations demonstrate that optimal model selection
depends on user priorities.

</details>


### [146] [CXR-CML: Improved zero-shot classification of long-tailed multi-label diseases in Chest X-Rays](https://arxiv.org/abs/2507.19398)
*Rajesh Madhipati,Sheethal Bhat,Lukas Buess,Andreas Maier*

Main category: cs.CV

TL;DR: 当前自监督深度学习模型和CLIP模型在处理CXR图像长尾类分类时存在不足，本文提出采用类加权机制，结合GMM聚类、t分布细化和度量损失的方法，在MIMIC - CXR - JPG数据集上零样本AUC得分平均提高7%。


<details>
  <summary>Details</summary>
Motivation: 当前自监督深度学习模型难以准确分类长尾类，CLIP模型对长尾分布类效果不佳，需提高CXR图像分类中长尾类的识别和准确性。

Method: 采用类加权机制，对潜在空间应用GMM聚类，用Student t分布细化聚类，再利用修改后的嵌入进行度量损失，实现特征的稳定自适应聚类。

Result: 在MIMIC - CXR - JPG数据集的40个类别上，零样本AUC得分较之前SOTA模型平均提高7个百分点。

Conclusion: 所提出的方法能有效改善整体分类性能，尤其提升了罕见类别的识别和准确性。

Abstract: Chest radiography (CXR) plays a crucial role in the diagnosis of various
diseases. However, the inherent class imbalance in the distribution of clinical
findings presents a significant challenge for current self-supervised deep
learning models. These models often fail to accurately classify long-tailed
classes. Current Vision-Language models such as Contrastive Language Image
Pre-training (CLIP) models effectively model the manifold distribution of the
latent space, enabling high zero-shot classification accuracies. Although CLIP
performs well on most of the primary classes in the dataset, our work reveals
that its effectiveness decreases significantly for classes with a long-tailed
distribution. Our approach employs a class-weighting mechanism that directly
aligns with the distribution of classes within the latent space. This method
ensures a substantial improvement in overall classification performance, with
particular emphasis on enhancing the recognition and accuracy of rarely
observed classes. We accomplish this by applying Gaussian Mixture Model (GMM)
clustering to the latent space. The subsequent clusters are further refined by
Student t-distribution, followed by a metric loss that utilizes the altered
embeddings. Our approach facilitates stable and adaptive clustering of the
features. This results in a notable average improvement of 7\% points in
zero-shot AUC scores across 40 classes in the MIMIC-CXR-JPG dataset from
previous SOTA models.

</details>


### [147] [EffiComm: Bandwidth Efficient Multi Agent Communication](https://arxiv.org/abs/2507.19354)
*Melih Yazgan,Allen Xavier Arasan,J. Marius Zöllner*

Main category: cs.CV

TL;DR: 提出EffiComm框架，传输数据少于现有技术40%，保持高精度3D目标检测。


<details>
  <summary>Details</summary>
Motivation: 现有协作感知传输原始点云或全特征图会使V2V通信过载，产生延迟和可扩展性问题。

Method: EffiComm基于BEV特征图，采用两阶段缩减管道（选择性传输和自适应网格缩减），用软门控混合专家注意力层融合特征。

Result: 在OPV2V基准上，EffiComm达到0.84 mAP@0.7，每帧平均仅传输约1.5 MB，在准确率-比特曲线上优于先前方法。

Conclusion: 自适应、学习型通信对可扩展的V2X感知有价值。

Abstract: Collaborative perception allows connected vehicles to exchange sensor
information and overcome each vehicle's blind spots. Yet transmitting raw point
clouds or full feature maps overwhelms Vehicle-to-Vehicle (V2V) communications,
causing latency and scalability problems. We introduce EffiComm, an end-to-end
framework that transmits less than 40% of the data required by prior art while
maintaining state-of-the-art 3D object detection accuracy. EffiComm operates on
Bird's-Eye-View (BEV) feature maps from any modality and applies a two-stage
reduction pipeline: (1) Selective Transmission (ST) prunes low-utility regions
with a confidence mask; (2) Adaptive Grid Reduction (AGR) uses a Graph Neural
Network (GNN) to assign vehicle-specific keep ratios according to role and
network load. The remaining features are fused with a soft-gated
Mixture-of-Experts (MoE) attention layer, offering greater capacity and
specialization for effective feature integration. On the OPV2V benchmark,
EffiComm reaches 0.84 mAP@0.7 while sending only an average of approximately
1.5 MB per frame, outperforming previous methods on the accuracy-per-bit curve.
These results highlight the value of adaptive, learned communication for
scalable Vehicle-to-Everything (V2X) perception.

</details>


### [148] [CircuitProbe: Dissecting Spatiotemporal Visual Semantics with Circuit Tracing](https://arxiv.org/abs/2507.19420)
*Yiming Zhang,Chengzhang Yu,Zhuokai Zhao,Kun Wang,Qiankun Li,Zihan Chen,Yang Liu,Zenghui Ding,Yining Sun*

Main category: cs.CV

TL;DR: 提出电路框架研究大视觉语言模型时空理解推理机制，发现视觉语义与特定对象标记相关，模型中后期层有功能定位，为设计更好模型奠基。


<details>
  <summary>Details</summary>
Motivation: 现有研究对大视觉语言模型时空理解内部推理机制了解不足。

Method: 引入包含视觉审计电路、语义追踪电路和注意力流电路的基于电路的系统框架进行研究。

Result: 发现视觉语义高度局部化到特定对象标记，移除这些标记会使模型性能下降达92.6%；模型中后期层出现并逐步细化对象和动作的可解释概念，且对时空语义有专门的功能定位。

Conclusion: 研究为大视觉语言模型的时空语义分析提供了重要的机制见解，为设计更强大、可解释的模型奠定基础。

Abstract: The processing mechanisms underlying language and image understanding in
large vision-language models (LVLMs) have been extensively studied. However,
the internal reasoning mechanisms of LVLMs for spatiotemporal understanding
remain poorly understood. In this work, we introduce a systematic,
circuit-based framework designed to investigate how spatiotemporal visual
semantics are represented and processed within these LVLMs. Specifically, our
framework comprises three circuits: visual auditing circuit, semantic tracing
circuit, and attention flow circuit. Through the lens of these circuits, we
discover that visual semantics are highly localized to specific object
tokens--removing these tokens can degrade model performance by up to 92.6%.
Furthermore, we identify that interpretable concepts of objects and actions
emerge and become progressively refined in the middle-to-late layers of LVLMs.
In contrary to the current works that solely focus on objects in one image, we
reveal that the middle-to-late layers of LVLMs exhibit specialized functional
localization for spatiotemporal semantics. Our findings offer significant
mechanistic insights into spatiotemporal semantics analysis of LVLMs, laying a
foundation for designing more robust and interpretable models.

</details>


### [149] [Fast Learning of Non-Cooperative Spacecraft 3D Models through Primitive Initialization](https://arxiv.org/abs/2507.19459)
*Pol Francesch Huc,Emily Bates,Simone D'Amico*

Main category: cs.CV

TL;DR: 提出基于CNN的3DGS初始器，实现用单目图像训练，降低训练成本，能在不完美姿态监督下学习高保真3D表示，可用于空间应用。


<details>
  <summary>Details</summary>
Motivation: 现有新颖视图合成技术在空间应用中有需姿态训练和计算成本高的局限。

Method: 提出基于CNN的3DGS初始器，可使用噪声或隐式姿态估计训练，分析初始化变体以降低训练成本。

Result: CNN输出的粗3D模型初始化3DGS，显著减少训练迭代次数和输入图像数量，能在不完美姿态监督下学习高保真3D表示。

Conclusion: 该方法为新颖视图合成在空间应用中打开了大门。

Abstract: The advent of novel view synthesis techniques such as NeRF and 3D Gaussian
Splatting (3DGS) has enabled learning precise 3D models only from posed
monocular images. Although these methods are attractive, they hold two major
limitations that prevent their use in space applications: they require poses
during training, and have high computational cost at training and inference. To
address these limitations, this work contributes: (1) a Convolutional Neural
Network (CNN) based primitive initializer for 3DGS using monocular images; (2)
a pipeline capable of training with noisy or implicit pose estimates; and (3)
and analysis of initialization variants that reduce the training cost of
precise 3D models. A CNN takes a single image as input and outputs a coarse 3D
model represented as an assembly of primitives, along with the target's pose
relative to the camera. This assembly of primitives is then used to initialize
3DGS, significantly reducing the number of training iterations and input images
needed -- by at least an order of magnitude. For additional flexibility, the
CNN component has multiple variants with different pose estimation techniques.
This work performs a comparison between these variants, evaluating their
effectiveness for downstream 3DGS training under noisy or implicit pose
estimates. The results demonstrate that even with imperfect pose supervision,
the pipeline is able to learn high-fidelity 3D representations, opening the
door for the use of novel view synthesis in space applications.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [150] [Virne: A Comprehensive Benchmark for Deep RL-based Network Resource Allocation in NFV](https://arxiv.org/abs/2507.19234)
*Tianfu Wang,Liwei Deng,Xi Chen,Junyang Wang,Huiguo He,Leilei Ding,Wei Wu,Qilin Fan,Hui Xiong*

Main category: cs.NI

TL;DR: 本文介绍了用于NFV - RA问题的综合基准框架Virne，支持多种网络场景、超30种方法，通过实验分析给出性能权衡见解及研究方向指导，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有缺乏系统基准框架和深入分析，阻碍新兴网络探索和算法开发，导致评估不一致。

Method: 引入Virne框架，提供定制化网络场景模拟，采用模块化可扩展实现管道，开展广泛实验进行深入分析。

Result: 通过实验获得了性能权衡的有价值见解，能为高效实现和未来研究方向提供指导。

Conclusion: Virne凭借多样模拟、丰富实现和广泛评估能力，可作为推进NFV - RA方法和深度强化学习应用的综合基准。

Abstract: Resource allocation (RA) is critical to efficient service deployment in
Network Function Virtualization (NFV), a transformative networking paradigm.
Recently, deep Reinforcement Learning (RL)-based methods have been showing
promising potential to address this complexity. However, the lack of a
systematic benchmarking framework and thorough analysis hinders the exploration
of emerging networks and the development of more robust algorithms while
causing inconsistent evaluation. In this paper, we introduce Virne, a
comprehensive benchmarking framework for the NFV-RA problem, with a focus on
supporting deep RL-based methods. Virne provides customizable simulations for
diverse network scenarios, including cloud, edge, and 5G environments. It also
features a modular and extensible implementation pipeline that supports over 30
methods of various types, and includes practical evaluation perspectives beyond
effectiveness, such as scalability, generalization, and scalability.
Furthermore, we conduct in-depth analysis through extensive experiments to
provide valuable insights into performance trade-offs for efficient
implementation and offer actionable guidance for future research directions.
Overall, with its diverse simulations, rich implementations, and extensive
evaluation capabilities, Virne could serve as a comprehensive benchmark for
advancing NFV-RA methods and deep RL applications. The code is publicly
available at https://github.com/GeminiLight/virne.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [151] [Bayesian Inverse Problems on Metric Graphs](https://arxiv.org/abs/2507.18951)
*David Bolin,Wenwen Li,Daniel Sanz-Alonso*

Main category: math.AP

TL;DR: 研究度量图上贝叶斯逆问题的公式化、适定性和数值解，聚焦从解的噪声测量中恢复扩散系数，数值结果良好。


<details>
  <summary>Details</summary>
Motivation: 研究度量图上贝叶斯逆问题，解决从解的噪声测量中恢复扩散系数的逆问题。

Method: 利用度量图上微分方程的正则性理论建立正演模型稳定性，采用高斯Whittle - Matérn过程模型作为先验。

Result: 数值结果表明能准确重建和有效进行不确定性量化。

Conclusion: 该方法可有效解决度量图上贝叶斯逆问题，实现准确重建和不确定性量化。

Abstract: This paper studies the formulation, well-posedness, and numerical solution of
Bayesian inverse problems on metric graphs, in which the edges represent
one-dimensional wires connecting vertices. We focus on the inverse problem of
recovering the diffusion coefficient of a (fractional) elliptic equation on a
metric graph from noisy measurements of the solution. Well-posedness hinges on
both stability of the forward model and an appropriate choice of prior. We
establish the stability of elliptic and fractional elliptic forward models
using recent regularity theory for differential equations on metric graphs. For
the prior, we leverage modern Gaussian Whittle--Mat\'ern process models on
metric graphs with sufficiently smooth sample paths. Numerical results
demonstrate accurate reconstruction and effective uncertainty quantification.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [152] [An Explainable Equity-Aware P2P Energy Trading Framework for Socio-Economically Diverse Microgrid](https://arxiv.org/abs/2507.18738)
*Abhijan Theja,Mayukha Pal*

Main category: eess.SY

TL;DR: 提出融合多目标混合整数线性规划、合作博弈论和强化学习动态公平调整机制的框架，经六种场景验证有显著效果，能促进公平可持续能源社区。


<details>
  <summary>Details</summary>
Motivation: 社区微电网公平动态能源分配面临挑战，静态方法难适应不平等问题，导致参与者不满和合作不可持续。

Method: 提出整合多目标混合整数线性规划、合作博弈论和强化学习动态公平调整机制的框架，采用基于公平福利最大化原则的双层优化模型，用近端策略优化代理动态调整权重，用可解释AI解释收益分配。

Result: 经六种场景验证，框架使峰值需求最多降低72.6%，有显著合作收益，自适应强化学习机制降低了基尼系数。

Conclusion: 该框架为实现公平可持续的能源社区提供了途径。

Abstract: Fair and dynamic energy allocation in community microgrids remains a critical
challenge, particularly when serving socio-economically diverse participants.
Static optimization and cost-sharing methods often fail to adapt to evolving
inequities, leading to participant dissatisfaction and unsustainable
cooperation. This paper proposes a novel framework that integrates
multi-objective mixed-integer linear programming (MILP), cooperative game
theory, and a dynamic equity-adjustment mechanism driven by reinforcement
learning (RL). At its core, the framework utilizes a bi-level optimization
model grounded in Equity-regarding Welfare Maximization (EqWM) principles,
which incorporate Rawlsian fairness to prioritize the welfare of the least
advantaged participants. We introduce a Proximal Policy Optimization (PPO)
agent that dynamically adjusts socio-economic weights in the optimization
objective based on observed inequities in cost and renewable energy access.
This RL-powered feedback loop enables the system to learn and adapt,
continuously striving for a more equitable state. To ensure transparency,
Explainable AI (XAI) is used to interpret the benefit allocations derived from
a weighted Shapley value. Validated across six realistic scenarios, the
framework demonstrates peak demand reductions of up to 72.6%, and significant
cooperative gains. The adaptive RL mechanism further reduces the Gini
coefficient over time, showcasing a pathway to truly sustainable and fair
energy communities.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [153] [Nonparametric Linear Discriminant Analysis for High Dimensional Matrix-Valued Data](https://arxiv.org/abs/2507.19028)
*Seungyeon Oh,Seongoh Park,Hoyoung Park*

Main category: stat.ME

TL;DR: 本文针对矩阵值数据分类问题，提出基于非参数经验贝叶斯框架的Fisher线性判别分析扩展方法，经实验验证优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决矩阵值数据的分类问题，现有方法可能无法有效处理此类数据。

Method: 假设各类数据服从矩阵正态分布，采用基于非参数最大似然估计的非参数经验贝叶斯框架，应用于向量化和缩放后的矩阵。

Result: 通过大量模拟研究和真实数据应用，包括脑电图和磁共振成像分析，该方法在各种数据结构上始终优于现有方法。

Conclusion: 提出的方法能有效推广到矩阵设置，提高了分类性能。

Abstract: This paper addresses classification problems with matrix-valued data, which
commonly arises in applications such as neuroimaging and signal processing.
Building on the assumption that the data from each class follows a matrix
normal distribution, we propose a novel extension of Fisher's Linear
Discriminant Analysis (LDA) tailored for matrix-valued observations. To
effectively capture structural information while maintaining estimation
flexibility, we adopt a nonparametric empirical Bayes framework based on
Nonparametric Maximum Likelihood Estimation (NPMLE), applied to vectorized and
scaled matrices. The NPMLE method has been shown to provide robust, flexible,
and accurate estimates for vector-valued data with various structures in the
mean vector or covariance matrix. By leveraging its strengths, our method is
effectively generalized to the matrix setting, thereby improving classification
performance. Through extensive simulation studies and real data applications,
including electroencephalography (EEG) and magnetic resonance imaging (MRI)
analysis, we demonstrate that the proposed method consistently outperforms
existing approaches across a variety of data structures.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [154] [Equivariant Volumetric Grasping](https://arxiv.org/abs/2507.18847)
*Pinhao Song,Yutong Hu,Pengteng Li,Renaud Detry*

Main category: cs.RO

TL;DR: 提出垂直轴旋转等变的体积抓取模型，用三平面体积特征表示，结合新卷积和改进规划器，实验证明可降低成本且性能更优。


<details>
  <summary>Details</summary>
Motivation: 提升样本效率，降低计算和内存成本，提高抓取模型性能。

Method: 采用三平面体积特征表示，设计新三平面特征，使用可变形可控卷积，改进GIGA和IGD规划器。

Result: 所提投影设计降低计算和内存成本，等变抓取模型性能优于非等变模型。

Conclusion: 基于三平面特征的等变抓取模型有效，有实际应用价值。

Abstract: We propose a new volumetric grasp model that is equivariant to rotations
around the vertical axis, leading to a significant improvement in sample
efficiency. Our model employs a tri-plane volumetric feature representation --
i.e., the projection of 3D features onto three canonical planes. We introduce a
novel tri-plane feature design in which features on the horizontal plane are
equivariant to 90{\deg} rotations, while the sum of features from the other two
planes remains invariant to the same transformations. This design is enabled by
a new deformable steerable convolution, which combines the adaptability of
deformable convolutions with the rotational equivariance of steerable ones.
This allows the receptive field to adapt to local object geometry while
preserving equivariance properties. We further develop equivariant adaptations
of two state-of-the-art volumetric grasp planners, GIGA and IGD. Specifically,
we derive a new equivariant formulation of IGD's deformable attention mechanism
and propose an equivariant generative model of grasp orientations based on flow
matching. We provide a detailed analytical justification of the proposed
equivariance properties and validate our approach through extensive simulated
and real-world experiments. Our results demonstrate that the proposed
projection-based design significantly reduces both computational and memory
costs. Moreover, the equivariant grasp models built on top of our tri-plane
features consistently outperform their non-equivariant counterparts, achieving
higher performance with only a modest computational overhead. Video and code
can be viewed in: https://mousecpn.github.io/evg-page/

</details>


### [155] [ReCoDe: Reinforcement Learning-based Dynamic Constraint Design for Multi-Agent Coordination](https://arxiv.org/abs/2507.19151)
*Michael Amir,Guang Yang,Zhan Gao,Keisuke Okumura,Heedo Woo,Amanda Prorok*

Main category: cs.RO

TL;DR: 提出ReCoDe框架，结合优化控制器可靠性与多智能体强化学习适应性，用于多智能体导航任务，表现优于多种方法，保留用户定义控制器更高效。


<details>
  <summary>Details</summary>
Motivation: 手工约束在多智能体复杂协调场景中易失效，需新方法解决问题。

Method: 引入ReCoDe框架，通过学习额外动态约束改进专家控制器，智能体通过局部通信集体约束动作。

Result: 在多智能体导航任务中，ReCoDe优于纯手工控制器、其他混合方法和标准MARL基线。

Conclusion: 保留用户定义控制器比从头学习更高效，ReCoDe可动态调整对其依赖程度。

Abstract: Constraint-based optimization is a cornerstone of robotics, enabling the
design of controllers that reliably encode task and safety requirements such as
collision avoidance or formation adherence. However, handcrafted constraints
can fail in multi-agent settings that demand complex coordination. We introduce
ReCoDe--Reinforcement-based Constraint Design--a decentralized, hybrid
framework that merges the reliability of optimization-based controllers with
the adaptability of multi-agent reinforcement learning. Rather than discarding
expert controllers, ReCoDe improves them by learning additional, dynamic
constraints that capture subtler behaviors, for example, by constraining agent
movements to prevent congestion in cluttered scenarios. Through local
communication, agents collectively constrain their allowed actions to
coordinate more effectively under changing conditions. In this work, we focus
on applications of ReCoDe to multi-agent navigation tasks requiring intricate,
context-based movements and consensus, where we show that it outperforms purely
handcrafted controllers, other hybrid approaches, and standard MARL baselines.
We give empirical (real robot) and theoretical evidence that retaining a
user-defined controller, even when it is imperfect, is more efficient than
learning from scratch, especially because ReCoDe can dynamically change the
degree to which it relies on this controller.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [156] [Decompiling Rust: An Empirical Study of Compiler Optimizations and Reverse Engineering Challenges](https://arxiv.org/abs/2507.18792)
*Zixu Zhou*

Main category: cs.PL

TL;DR: 对Rust二进制文件反编译有挑战，通过基准驱动评估核心特性和编译模式下反编译质量，分析语言构造影响，为工具开发者提供见解。


<details>
  <summary>Details</summary>
Motivation: Rust语言丰富类型系统、编译器优化和高级抽象使二进制文件反编译具有挑战性，需评估反编译质量。

Method: 进行基准驱动评估，使用自动化评分框架，开展代表性案例研究。

Result: 泛型类型、特征方法和错误处理构造显著降低反编译质量，尤其在发布版本构建中；分析了特定语言构造对控制流、变量命名和类型信息恢复的影响。

Conclusion: 研究结果为工具开发者提供可操作见解，强调需要Rust感知的反编译策略。

Abstract: Decompiling Rust binaries is challenging due to the language's rich type
system, aggressive compiler optimizations, and widespread use of high-level
abstractions. In this work, we conduct a benchmark-driven evaluation of
decompilation quality across core Rust features and compiler build modes. Our
automated scoring framework shows that generic types, trait methods, and error
handling constructs significantly reduce decompilation quality, especially in
release builds. Through representative case studies, we analyze how specific
language constructs affect control flow, variable naming, and type information
recovery. Our findings provide actionable insights for tool developers and
highlight the need for Rust-aware decompilation strategies.

</details>


### [157] [An Enumerative Embedding of the Python Type System in ACL2s](https://arxiv.org/abs/2507.19015)
*Samuel Xifaras,Panagiotis Manolios,Andrew T. Walter,William Robertson*

Main category: cs.PL

TL;DR: 本文将Python类型系统子集嵌入ACL2s，用其生成输入对Python程序进行模糊测试，评估时测量代码覆盖率并分析阻碍因素，最后讨论结果并给出未来工作建议。


<details>
  <summary>Details</summary>
Motivation: 利用ACL2s对Python代码进行推理。

Method: 将Python类型系统子集嵌入ACL2s，定义类型及defdata枚举器，用ACL2s嵌入生成类型实例作为输入对Python程序进行模糊测试，仅用函数类型签名生成输入，将函数体视为黑盒。

Result: 对四个开源仓库评估，代码覆盖率在68%到超80%之间，识别出复杂分支条件和外部文件系统依赖等阻碍覆盖率的代码模式。

Conclusion: 讨论结果并给出未来工作建议。

Abstract: Python is a high-level interpreted language that has become an industry
standard in a wide variety of applications. In this paper, we take a first step
towards using ACL2s to reason about Python code by developing an embedding of a
subset of the Python type system in ACL2s. The subset of Python types we
support includes many of the most commonly used type annotations as well as
user-defined types comprised of supported types. We provide ACL2s definitions
of these types, as well as defdata enumerators that are customized to provide
code coverage and identify errors in Python programs. Using the ACL2s
embedding, we can generate instances of types that can then be used as inputs
to fuzz Python programs, which allows us to identify bugs in Python code that
are not detected by state-of-the-art Python type checkers. We evaluate our work
against four open-source repositories, extracting their type information and
generating inputs for fuzzing functions with type signatures that are in the
supported subset of Python types. Note that we only use the type signatures of
functions to generate inputs and treat the bodies of functions as black boxes.
We measure code coverage, which ranges from about 68% to more than 80%, and
identify code patterns that hinder coverage such as complex branch conditions
and external file system dependencies. We conclude with a discussion of the
results and recommendations for future work.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [158] [Multi-Year Maintenance Planning for Large-Scale Infrastructure Systems: A Novel Network Deep Q-Learning Approach](https://arxiv.org/abs/2507.18732)
*Amir Fard,Arnold X. -X. Yuan*

Main category: math.OC

TL;DR: 提出新的深度强化学习框架优化大型基础设施网络资产管理策略，案例显示比传统方法更优。


<details>
  <summary>Details</summary>
Motivation: 传统维护和修复规划方法在大规模网络资产管理中面临可扩展性和计算挑战。

Method: 将网络级马尔可夫决策过程分解为单个资产级马尔可夫决策过程，使用统一神经网络架构，并通过预算分配机制纳入年度预算约束。

Result: 在68,800个路段的大规模路面网络案例中，该框架在效率和网络性能上显著优于传统方法。

Conclusion: 该框架推动了基础设施资产管理及强化学习在复杂大规模环境中的应用。

Abstract: Infrastructure asset management is essential for sustaining the performance
of public infrastructure such as road networks, bridges, and utility networks.
Traditional maintenance and rehabilitation planning methods often face
scalability and computational challenges, particularly for large-scale networks
with thousands of assets under budget constraints. This paper presents a novel
deep reinforcement learning (DRL) framework that optimizes asset management
strategies for large infrastructure networks. By decomposing the network-level
Markov Decision Process (MDP) into individual asset-level MDPs while using a
unified neural network architecture, the proposed framework reduces
computational complexity, improves learning efficiency, and enhances
scalability. The framework directly incorporates annual budget constraints
through a budget allocation mechanism, ensuring maintenance plans are both
optimal and cost-effective. Through a case study on a large-scale pavement
network of 68,800 segments, the proposed DRL framework demonstrates significant
improvements over traditional methods like Progressive Linear Programming and
genetic algorithms, both in efficiency and network performance. This
advancement contributes to infrastructure asset management and the broader
application of reinforcement learning in complex, large-scale environments.

</details>


### [159] [Linearly Convergent Algorithms for Nonsmooth Problems with Unknown Smooth Pieces](https://arxiv.org/abs/2507.19465)
*Zhe Zhang,Suvrit Sra*

Main category: math.OC

TL;DR: 本文为未知定义域分区的分段光滑（PWS）函数优化开发高效算法，提出有全局线性收敛性的方法，扩展方法处理近似PWS函数与弱凸PWS问题，引入可验证的终止准则并开发无参数算法。


<details>
  <summary>Details</summary>
Motivation: 为未知定义域分区的PWS函数优化问题开发有效算法，改进现有复杂度。

Method: 提出束级（BL）型方法，扩展该方法，引入可验证的终止准则并结合搜索子程序与猜测-检查框架。

Result: BL型方法实现全局线性收敛，改进处理近似PWS函数与弱凸PWS问题的复杂度，终止准则能表征最优性差距且无需问题参数。

Conclusion: 开发出几乎无参数的算法用于凸QG和弱凸设置下的PWS优化。

Abstract: We develop efficient algorithms for optimizing piecewise smooth (PWS)
functions where the underlying partition of the domain into smooth pieces is
\emph{unknown}. For PWS functions satisfying a quadratic growth (QG) condition,
we propose a bundle-level (BL) type method that achieves global linear
convergence -- to our knowledge, the first such result for any algorithm for
this problem class. We extend this method to handle approximately PWS functions
and to solve weakly-convex PWS problems, improving the state-of-the-art
complexity to match the benchmark for smooth non-convex optimization.
Furthermore, we introduce the first verifiable and accurate termination
criterion for PWS optimization. Similar to the gradient norm in smooth
optimization, this certificate tightly characterizes the optimality gap under
the QG condition, and can moreover be evaluated without knowledge of any
problem parameters. We develop a search subroutine for this certificate and
embed it within a guess-and-check framework, resulting in an almost
parameter-free algorithm for both the convex QG and weakly-convex settings.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [160] [People Are Highly Cooperative with Large Language Models, Especially When Communication Is Possible or Following Human Interaction](https://arxiv.org/abs/2507.18639)
*Paweł Niszczota,Tomasz Grzegorczyk,Alexander Pastukhov*

Main category: cs.HC

TL;DR: 本文通过囚徒困境实验研究人与大语言模型（LLM）交互时的合作行为，发现与LLM合作率虽低于人类但仍较高，交流能提高合作可能，且与人类交互后和LLM合作更高，验证了企业在合作场景谨慎使用LLM的可行性。


<details>
  <summary>Details</summary>
Motivation: 探索在商业场景中与LLM而非人类交互时合作行为的变化。

Method: 使用囚徒困境游戏，进行了两个实验，实验1中100名参与者与人类、经典机器人和LLM进行三十轮重复游戏；实验2中192名参与者与人类或LLM进行一次性游戏，部分可与对手交流。

Result: 与LLM的合作率比人类低约10 - 15个百分点，但仍较高；交流能使与人类和LLM的合作可能性均提高88%；与人类交互后和LLM合作更高。

Conclusion: 验证了企业在有合作成分的场景中（谨慎）使用LLM的可行性。

Abstract: Machines driven by large language models (LLMs) have the potential to augment
humans across various tasks, a development with profound implications for
business settings where effective communication, collaboration, and stakeholder
trust are paramount. To explore how interacting with an LLM instead of a human
might shift cooperative behavior in such settings, we used the Prisoner's
Dilemma game -- a surrogate of several real-world managerial and economic
scenarios. In Experiment 1 (N=100), participants engaged in a thirty-round
repeated game against a human, a classic bot, and an LLM (GPT, in real-time).
In Experiment 2 (N=192), participants played a one-shot game against a human or
an LLM, with half of them allowed to communicate with their opponent, enabling
LLMs to leverage a key advantage over older-generation machines. Cooperation
rates with LLMs -- while lower by approximately 10-15 percentage points
compared to interactions with human opponents -- were nonetheless high. This
finding was particularly notable in Experiment 2, where the psychological cost
of selfish behavior was reduced. Although allowing communication about
cooperation did not close the human-machine behavioral gap, it increased the
likelihood of cooperation with both humans and LLMs equally (by 88%), which is
particularly surprising for LLMs given their non-human nature and the
assumption that people might be less receptive to cooperating with machines
compared to human counterparts. Additionally, cooperation with LLMs was higher
following prior interaction with humans, suggesting a spillover effect in
cooperative behavior. Our findings validate the (careful) use of LLMs by
businesses in settings that have a cooperative component.

</details>


### [161] [More Expert-like Eye Gaze Movement Patterns are Related to Better X-ray Reading](https://arxiv.org/abs/2507.18637)
*Pingjing Yang,Jennifer Cromley,Jana Diesner*

Main category: cs.HC

TL;DR: 研究用网络分析技术研究牙科学生诊断X光片时眼动与学习成果关系，发现网络指标与成绩相关性及学生处理水平发展转变，为视觉任务专业技能获取及AI辅助学习干预设计提供见解。


<details>
  <summary>Details</summary>
Motivation: 理解新手如何获取和提升视觉搜索技能，以开发和优化跨领域的训练方法。

Method: 用网络分析技术将眼动扫描路径建模为有向图，对网络指标进行时间序列聚类，识别视觉搜索策略模式并探究其与诊断表现的关联。

Result: 过渡熵与成绩负相关，节点和边的数量以及平均PageRank与成绩正相关；学生网络指标随时间变化显示从中间水平向专家水平处理的转变。

Conclusion: 研究有助于理解视觉任务中的专业技能获取，可为AI辅助学习干预设计提供依据。

Abstract: Understanding how novices acquire and hone visual search skills is crucial
for developing and optimizing training methods across domains. Network analysis
methods can be used to analyze graph representations of visual expertise. This
study investigates the relationship between eye-gaze movements and learning
outcomes among undergraduate dentistry students who were diagnosing dental
radiographs over multiple semesters. We use network analysis techniques to
model eye-gaze scanpaths as directed graphs and examine changes in network
metrics over time. Using time series clustering on each metric, we identify
distinct patterns of visual search strategies and explore their association
with students' diagnostic performance. Our findings suggest that the network
metric of transition entropy is negatively correlated with performance scores,
while the number of nodes and edges as well as average PageRank are positively
correlated with performance scores. Changes in network metrics for individual
students over time suggest a developmental shift from intermediate to
expert-level processing. These insights contribute to understanding expertise
acquisition in visual tasks and can inform the design of AI-assisted learning
interventions.

</details>


### [162] [Prompt Engineering and the Effectiveness of Large Language Models in Enhancing Human Productivity](https://arxiv.org/abs/2507.18638)
*Rizal Khoirul Anam*

Main category: cs.HC

TL;DR: 本文研究用户提示结构和清晰度对大语言模型输出效果的影响，结果表明清晰结构化提示能提升效率和结果质量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型广泛应用，探究用户提示结构和清晰度对其输出有效性和生产力的影响。

Method: 收集243位不同学术和职业背景受访者的数据，分析AI使用习惯、提示策略和用户满意度。

Result: 使用清晰、结构化和有上下文感知提示的用户报告了更高的任务效率和更好的结果。

Conclusion: 强调提示工程在最大化生成式AI价值中的关键作用，并为其日常使用提供实践启示。

Abstract: The widespread adoption of large language models (LLMs) such as ChatGPT,
Gemini, and DeepSeek has significantly changed how people approach tasks in
education, professional work, and creative domains. This paper investigates how
the structure and clarity of user prompts impact the effectiveness and
productivity of LLM outputs. Using data from 243 survey respondents across
various academic and occupational backgrounds, we analyze AI usage habits,
prompting strategies, and user satisfaction. The results show that users who
employ clear, structured, and context-aware prompts report higher task
efficiency and better outcomes. These findings emphasize the essential role of
prompt engineering in maximizing the value of generative AI and provide
practical implications for its everyday use.

</details>


### [163] [How good are humans at detecting AI-generated images? Learnings from an experiment](https://arxiv.org/abs/2507.18640)
*Thomas Roca,Anthony Cintron Roman,Jehú Torres Vega,Marcelo Duarte,Pengce Wang,Kevin White,Amit Misra,Juan Lavista Ferres*

Main category: cs.HC

TL;DR: 研究利用在线游戏数据，发现人们区分AI生成图像与真实图像的整体成功率仅62%，对部分类型图像区分困难，强调需透明度工具。


<details>
  <summary>Details</summary>
Motivation: 探究随着AI图像生成技术提升，人类区分真实和AI生成或修改图像的能力。

Method: 借助在线游戏“Real or Not Quiz”收集数据，让参与者对随机的真实和AI生成图像进行真伪判断。

Result: 约12500名全球参与者进行约287000次图像评估，整体成功率62%，对人像判断较准确，对自然和城市景观判断困难。

Conclusion: 人类区分AI生成视觉内容存在固有挑战，需要水印和强大的AI检测工具等透明度工具来降低AI生成内容带来的错误信息风险。

Abstract: As AI-powered image generation improves, a key question is how well human
beings can differentiate between "real" and AI-generated or modified images.
Using data collected from the online game "Real or Not Quiz.", this study
investigates how effectively people can distinguish AI-generated images from
real ones. Participants viewed a randomized set of real and AI-generated
images, aiming to identify their authenticity. Analysis of approximately
287,000 image evaluations by over 12,500 global participants revealed an
overall success rate of only 62\%, indicating a modest ability, slightly above
chance. Participants were most accurate with human portraits but struggled
significantly with natural and urban landscapes. These results highlight the
inherent challenge humans face in distinguishing AI-generated visual content,
particularly images without obvious artifacts or stylistic cues. This study
stresses the need for transparency tools, such as watermarks and robust AI
detection tools to mitigate the risks of misinformation arising from
AI-generated content

</details>


### [164] [DxHF: Providing High-Quality Human Feedback for LLM Alignment via Interactive Decomposition](https://arxiv.org/abs/2507.18802)
*Danqing Shi,Furui Cheng,Tino Weinkauf,Antti Oulasvirta,Mennatallah El-Assady*

Main category: cs.HC

TL;DR: 研究分解原则改进大语言模型对齐中人类反馈质量，构建界面DxHF，评估显示能提升反馈准确性，虽增加反馈时间，但凸显HCI在人机对齐中的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型对齐中使用的人类反馈界面让注释者比较长文本有认知挑战，需改进反馈质量。

Method: 研究分解原则，将文本分解为单个主张，构建用户界面DxHF，展示分解主张、编码相关性和链接相似主张。

Result: 技术评估表明分解提高反馈准确性，众包研究显示使用DxHF平均提高5%准确性，但增加18秒反馈时间，不确定情况下准确性提升显著。

Conclusion: 强调HCI是改善人机对齐的有效方法。

Abstract: Human preferences are widely used to align large language models (LLMs)
through methods such as reinforcement learning from human feedback (RLHF).
However, the current user interfaces require annotators to compare text
paragraphs, which is cognitively challenging when the texts are long or
unfamiliar. This paper contributes by studying the decomposition principle as
an approach to improving the quality of human feedback for LLM alignment. This
approach breaks down the text into individual claims instead of directly
comparing two long-form text responses. Based on the principle, we build a
novel user interface DxHF. It enhances the comparison process by showing
decomposed claims, visually encoding the relevance of claims to the
conversation and linking similar claims. This allows users to skim through key
information and identify differences for better and quicker judgment. Our
technical evaluation shows evidence that decomposition generally improves
feedback accuracy regarding the ground truth, particularly for users with
uncertainty. A crowdsourcing study with 160 participants indicates that using
DxHF improves feedback accuracy by an average of 5%, although it increases the
average feedback time by 18 seconds. Notably, accuracy is significantly higher
in situations where users have less certainty. The finding of the study
highlights the potential of HCI as an effective method for improving human-AI
alignment.

</details>


### [165] [TreeReader: A Hierarchical Academic Paper Reader Powered by Language Models](https://arxiv.org/abs/2507.18945)
*Zijian Zhang,Pan Chen,Fangshi Du,Runlong Ye,Oliver Huang,Michael Liut,Alán Aspuru-Guzik*

Main category: cs.HC

TL;DR: 介绍TreeReader这一新型论文阅读器，它将论文分解为交互式树状结构，经用户研究评估，能更聚焦高效地阅读学术文献。


<details>
  <summary>Details</summary>
Motivation: 传统线性格式论文易造成认知过载，难以定位关键信息；基于大语言模型的聊天机器人缺乏对特定部分的细致理解、信息不可靠且丢弃导航结构，因此需要更好的工具辅助阅读。

Method: 借鉴学术阅读实践的形成性研究，将论文分解为交互式树状结构，每个部分由大语言模型生成简洁摘要，细节可按需查看，并进行用户研究。

Result: 未详细提及用户研究的具体结果，但表明TreeReader能更聚焦高效地导航和理解复杂学术文献。

Conclusion: TreeReader通过将分层总结与交互式探索相结合，为阅读复杂学术文献提供了更聚焦高效的方式。

Abstract: Efficiently navigating and understanding academic papers is crucial for
scientific progress. Traditional linear formats like PDF and HTML can cause
cognitive overload and obscure a paper's hierarchical structure, making it
difficult to locate key information. While LLM-based chatbots offer
summarization, they often lack nuanced understanding of specific sections, may
produce unreliable information, and typically discard the document's
navigational structure. Drawing insights from a formative study on academic
reading practices, we introduce TreeReader, a novel language model-augmented
paper reader. TreeReader decomposes papers into an interactive tree structure
where each section is initially represented by an LLM-generated concise
summary, with underlying details accessible on demand. This design allows users
to quickly grasp core ideas, selectively explore sections of interest, and
verify summaries against the source text. A user study was conducted to
evaluate TreeReader's impact on reading efficiency and comprehension.
TreeReader provides a more focused and efficient way to navigate and understand
complex academic literature by bridging hierarchical summarization with
interactive exploration.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [166] [On the Security of a Code-Based PIR Scheme](https://arxiv.org/abs/2507.19295)
*Svenja Lage,Hannes Bartz*

Main category: cs.CR

TL;DR: 研究发现CB - cPIR方案有安全漏洞且通信成本竞争力弱，但代码基PIR方案仍值得研究


<details>
  <summary>Details</summary>
Motivation: 评估基于编码理论的CB - cPIR方案的安全性和竞争力

Method: 分析CB - cPIR方案的安全性，与现有先进PIR方案进行对比

Result: CB - cPIR方案有严重安全漏洞，通信成本竞争力下降

Conclusion: 代码基PIR方案有潜力，值得持续研究

Abstract: Private Information Retrieval (PIR) schemes allow clients to retrieve files
from a database without disclosing the requested file's identity to the server.
In the pursuit of post-quantum security, most recent PIR schemes rely on hard
lattice problems. In contrast, the so called CB-cPIR scheme stands out as a
pioneering effort to base PIR schemes on hard problems in coding theory,
thereby contributing significantly to the diversification of security
foundations. However, our research reveals a critical vulnerability in CB-cPIR,
substantially diminishing its security levels. Moreover, a comparative analysis
with state-of-the-art PIR schemes shows that CB-cPIR's advantages are reduced,
making it less competitive in terms of the communication cost. Nevertheless,
our findings highlight the importance of continued research into code-based PIR
schemes, as they have the potential to provide a valuable alternative to
lattice-based approaches.

</details>


### [167] [PurpCode: Reasoning for Safer Code Generation](https://arxiv.org/abs/2507.19060)
*Jiawei Liu,Nirav Diwan,Zhe Wang,Haoyu Zhai,Xiaona Zhou,Kiet A. Nguyen,Tianjiao Yu,Muntasir Wahed,Yinlin Deng,Hadjer Benkraouda,Yuxiang Wei,Lingming Zhang,Ismini Lourentzou,Gang Wang*

Main category: cs.CR

TL;DR: 介绍PurpCode训练安全代码推理模型，分两阶段训练，开发PurpCode - 32B模型，有先进安全性且降低拒绝率。


<details>
  <summary>Details</summary>
Motivation: 训练安全代码推理模型以生成安全代码并抵御恶意网络活动。

Method: PurpCode分规则学习和强化学习两阶段训练模型，进行内部红队测试合成网络安全数据。

Result: 开发的PurpCode - 32B模型有先进网络安全性，优于多种前沿模型，降低模型过度拒绝率。

Conclusion: PurpCode能有效训练安全代码推理模型，提升安全性并保留模型实用性。

Abstract: We introduce PurpCode, the first post-training recipe for training safe code
reasoning models towards generating secure code and defending against malicious
cyberactivities. PurpCode trains a reasoning model in two stages: (i) Rule
Learning, which explicitly teaches the model to reference cybersafety rules to
generate vulnerability-free code and to avoid facilitating malicious
cyberactivities; and (ii) Reinforcement Learning, which optimizes model safety
and preserves model utility through diverse, multi-objective reward mechanisms.
To empower the training pipelines with comprehensive cybersafety data, we
conduct internal red-teaming to synthesize comprehensive and high-coverage
prompts based on real-world tasks for inducing unsafe cyberactivities in the
model. Based on PurpCode, we develop a reasoning-based coding model, namely
PurpCode-32B, which demonstrates state-of-the-art cybersafety, outperforming
various frontier models. Meanwhile, our alignment method decreases the model
overrefusal rates in both general and cybersafety-specific scenarios, while
preserving model utility in both code generation and common security knowledge.

</details>


### [168] [PrompTrend: Continuous Community-Driven Vulnerability Discovery and Assessment for Large Language Models](https://arxiv.org/abs/2507.19185)
*Tarek Gasmi,Ramzi Guesmi,Mootez Aloui,Jihene Bennaceur*

Main category: cs.CR

TL;DR: 提出PrompTrend系统监测LLM漏洞，分析198个漏洞发现先进能力与漏洞有关，心理攻击更有效，框架分类准确率78%，表明需综合监测。


<details>
  <summary>Details</summary>
Motivation: 静态基准无法捕捉在线论坛社区实验中出现的大语言模型（LLM）漏洞，因此需要新的监测系统。

Method: 构建PrompTrend系统，跨平台收集漏洞数据，用多维评分评估，对198个漏洞进行横截面分析。

Result: 先进能力在某些架构中与更高的漏洞相关，心理攻击显著优于技术攻击，平台动态影响攻击效果，PrompTrend框架分类准确率78%，跨模型可迁移性有限。

Conclusion: 有效的LLM安全需要超越传统定期评估的综合社会技术监测，能力提升不一定带来安全提升，社区驱动的心理操纵是当前语言模型的主要威胁。

Abstract: Static benchmarks fail to capture LLM vulnerabilities emerging through
community experimentation in online forums. We present PrompTrend, a system
that collects vulnerability data across platforms and evaluates them using
multidimensional scoring, with an architecture designed for scalable
monitoring. Cross-sectional analysis of 198 vulnerabilities collected from
online communities over a five-month period (January-May 2025) and tested on
nine commercial models reveals that advanced capabilities correlate with
increased vulnerability in some architectures, psychological attacks
significantly outperform technical exploits, and platform dynamics shape attack
effectiveness with measurable model-specific patterns. The PrompTrend
Vulnerability Assessment Framework achieves 78% classification accuracy while
revealing limited cross-model transferability, demonstrating that effective LLM
security requires comprehensive socio-technical monitoring beyond traditional
periodic assessment. Our findings challenge the assumption that capability
advancement improves security and establish community-driven psychological
manipulation as the dominant threat vector for current language models.

</details>


### [169] [Running in CIRCLE? A Simple Benchmark for LLM Code Interpreter Security](https://arxiv.org/abs/2507.19399)
*Gabriel Chua*

Main category: cs.CR

TL;DR: 提出CIRCLE基准评估大语言模型代码解释器安全风险，评估7个模型发现显著且不一致的漏洞，强调需特定安全基准、缓解工具和行业标准。


<details>
  <summary>Details</summary>
Motivation: 大语言模型集成代码解释器带来新的系统级网络安全威胁，需系统评估解释器特定风险。

Method: 提出包含1260个提示的CIRCLE基准，涵盖CPU、内存和磁盘资源耗尽风险，用自动化评估框架评估模型。

Result: 评估7个模型发现显著且不一致的漏洞，间接提示会削弱模型防御。

Conclusion: 迫切需要解释器特定的网络安全基准、缓解工具和行业标准，已公开数据集和代码促进研究。

Abstract: As large language models (LLMs) increasingly integrate native code
interpreters, they enable powerful real-time execution capabilities,
substantially expanding their utility. However, such integrations introduce
potential system-level cybersecurity threats, fundamentally different from
prompt-based vulnerabilities. To systematically evaluate these
interpreter-specific risks, we propose CIRCLE (Code-Interpreter Resilience
Check for LLM Exploits), a simple benchmark comprising 1,260 prompts targeting
CPU, memory, and disk resource exhaustion. Each risk category includes
explicitly malicious ("direct") and plausibly benign ("indirect") prompt
variants. Our automated evaluation framework assesses not only whether LLMs
refuse or generates risky code, but also executes the generated code within the
interpreter environment to evaluate code correctness, simplifications made by
the LLM to make the code safe, or execution timeouts. Evaluating 7 commercially
available models from OpenAI and Google, we uncover significant and
inconsistent vulnerabilities. For instance, evaluations show substantial
disparities even within providers - OpenAI's o4-mini correctly refuses risky
requests at 7.1%, notably higher rates compared to GPT-4.1 at 0.5%. Results
particularly underscore that indirect, socially-engineered prompts
substantially weaken model defenses. This highlights an urgent need for
interpreter-specific cybersecurity benchmarks, dedicated mitigation tools
(e.g., guardrails), and clear industry standards to guide safe and responsible
deployment of LLM interpreter integrations. The benchmark dataset and
evaluation code are publicly released to foster further research.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [170] [HH-Codec: High Compression High-fidelity Discrete Neural Codec for Spoken Language Modeling](https://arxiv.org/abs/2507.18897)
*Rongkun Xue,Yazhe Niu,Shuai Hu,Zixin Yin,Yongqiang Yao,Jing Yang*

Main category: cs.SD

TL;DR: 本文提出HH - Codec神经编解码器，以单量化器推理实现24kHz音频每秒24个令牌的极端压缩，带宽仅0.3kbps，性能达SOTA。


<details>
  <summary>Details</summary>
Motivation: 大规模语音到语音系统中，多量化器并行流的复杂性和高时间维度编解码器的计算成本带来挑战，需要更好的离散语音分词方法。

Method: 设计用于口语语言建模的矢量量化空间，提出利用双重监督和渐进式训练的非对称编解码器架构（Audio - VQ - Mel - Audio）。

Result: HH - Codec在语音重建上达到了SOTA性能，带宽仅0.3kbps，通过大量消融实验验证了各模块的必要性。

Conclusion: HH - Codec能有效解决大规模语音系统中的挑战，实现高效的语音压缩和重建。

Abstract: Discrete speech tokenization is a fundamental component in speech codecs.
However, in large-scale speech-to-speech systems, the complexity of parallel
streams from multiple quantizers and the computational cost of
high-time-dimensional codecs pose significant challenges. In this paper, we
introduce HH-Codec, a neural codec that achieves extreme compression at 24
tokens per second for 24 kHz audio while relying on single-quantizer inference.
Our approach involves a carefully designed Vector Quantization space for Spoken
Language Modeling, optimizing compression efficiency while minimizing
information loss. Building on this, we propose an asymmetric encoder-decoder
architecture (Audio-VQ-Mel-Audio) that leverages dual supervision and
progressive training to enhance reconstruction stability and fidelity. HH-Codec
achieves state-of-the-art performance in speech reconstruction with an
ultra-low bandwidth of 0.3 kbps. We further evaluate its effectiveness in
codebook utilization and generative model adaptation, with extensive ablations
validating the necessity of each module. HH-Codec is available at
https://github.com/opendilab/HH-Codec.

</details>


### [171] [SCORE-SET: A dataset of GuitarPro files for Music Phrase Generation and Sequence Learning](https://arxiv.org/abs/2507.18723)
*Vishakh Begari*

Main category: cs.SD

TL;DR: 提供了适用于吉他音乐生成等任务的Guitar Pro曲谱数据集，数据源于MAESTRO和GiantMIDI的MIDI音符，处理后含多种吉他演奏表达设置。


<details>
  <summary>Details</summary>
Motivation: 为吉他音乐生成、序列建模和性能感知学习等任务提供合适的数据集。

Method: 从MAESTRO和GiantMIDI的MIDI音符获取数据并转换为节奏吉他音轨，再添加多种吉他演奏表达设置。

Result: 得到一个适用于相关任务的Guitar Pro曲谱数据集。

Conclusion: 该数据集能更好反映真实世界吉他演奏的细微差别，可用于相关研究任务。

Abstract: A curated dataset of Guitar Pro tablature files (.gp5 format), tailored for
tasks involving guitar music generation, sequence modeling, and
performance-aware learning is provided. The dataset is derived from MIDI notes
in MAESTRO and GiantMIDI which have been adapted into rhythm guitar tracks.
These tracks are further processed to include a variety of expression settings
typical of guitar performance, such as bends, slides, vibrato, and palm muting,
to better reflect the nuances of real-world guitar playing.

</details>


### [172] [Latent Granular Resynthesis using Neural Audio Codecs](https://arxiv.org/abs/2507.19202)
*Nao Tokui,Tom Baker*

Main category: cs.SD

TL;DR: 提出一种新颖的创意音频再合成技术，在潜在向量层面改造颗粒合成概念，无需模型训练，可避免传统拼接合成的不连续性，还提供补充材料和概念验证实现。


<details>
  <summary>Details</summary>
Motivation: 开发一种创新的音频再合成技术，改善传统音频合成的不足。

Method: 在潜在向量层面改造颗粒合成概念，创建“颗粒码本”，将目标音频信号的每个潜在颗粒与码本中最接近的对应项匹配，最后解码生成音频。

Result: 生成的音频保留了目标音频的时间结构，同时采用了源音频的音色特征，还避免了传统拼接合成的不连续性。

Conclusion: 该技术无需模型训练，适用于多种音频材料，是一种有效的创意音频再合成方法。

Abstract: We introduce a novel technique for creative audio resynthesis that operates
by reworking the concept of granular synthesis at the latent vector level. Our
approach creates a "granular codebook" by encoding a source audio corpus into
latent vector segments, then matches each latent grain of a target audio signal
to its closest counterpart in the codebook. The resulting hybrid sequence is
decoded to produce audio that preserves the target's temporal structure while
adopting the source's timbral characteristics. This technique requires no model
training, works with diverse audio materials, and naturally avoids the
discontinuities typical of traditional concatenative synthesis through the
codec's implicit interpolation during decoding. We include supplementary
material at https://github.com/naotokui/latentgranular/ , as well as a
proof-of-concept implementation to allow users to experiment with their own
sounds at https://huggingface.co/spaces/naotokui/latentgranular .

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [173] [Bespoke multiresolution analysis of graph signals](https://arxiv.org/abs/2507.19181)
*Giacomo Elefante,Gianluca Giacchi,Michael Multerer,Jacopo Quizi*

Main category: eess.SP

TL;DR: 提出用于图信号离散多分辨率分析的新框架，此框架基于samplet变换，相比Haar小波更高效，且有实验验证优势。


<details>
  <summary>Details</summary>
Motivation: 现有方法对图信号压缩和分析的局限性，期望找到能拓宽可高效处理图信号类别的方法。

Method: 将图划分为固定数量的补丁，嵌入欧氏空间构建samplets再拉回到图；结合重边聚类和landmark Isomap进行高效数值实现。

Result: 方法具有鲁棒性、可扩展性，能产生可控近似误差的稀疏表示，在压缩效率和多分辨率保真度上显著优于传统Haar小波方法。

Conclusion: 所提框架是有效的图信号离散多分辨率分析方法，比经典Haar小波更具优势。

Abstract: We present a novel framework for discrete multiresolution analysis of graph
signals. The main analytical tool is the samplet transform, originally defined
in the Euclidean framework as a discrete wavelet-like construction, tailored to
the analysis of scattered data. The first contribution of this work is defining
samplets on graphs. To this end, we subdivide the graph into a fixed number of
patches, embed each patch into a Euclidean space, where we construct samplets,
and eventually pull the construction back to the graph. This ensures
orthogonality, locality, and the vanishing moments property with respect to
properly defined polynomial spaces on graphs. Compared to classical Haar
wavelets, this framework broadens the class of graph signals that can
efficiently be compressed and analyzed. Along this line, we provide a
definition of a class of signals that can be compressed using our construction.
We support our findings with different examples of signals defined on graphs
whose vertices lie on smooth manifolds. For efficient numerical implementation,
we combine heavy edge clustering, to partition the graph into meaningful
patches, with landmark \texttt{Isomap}, which provides low-dimensional
embeddings for each patch. Our results demonstrate the method's robustness,
scalability, and ability to yield sparse representations with controllable
approximation error, significantly outperforming traditional Haar wavelet
approaches in terms of compression efficiency and multiresolution fidelity.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [174] [Finance as Extended Biology: Reciprocity as the Cognitive Substrate of Financial Behavior](https://arxiv.org/abs/2506.00099)
*Egil Diau*

Main category: physics.soc-ph

TL;DR: 本文指出金融行为源于互惠而非正式制度，重构四大金融功能，为建模去中心化金融行为提供新基础。


<details>
  <summary>Details</summary>
Motivation: 解释在无正式制度情况下金融行为如何产生。

Method: 将金融功能视为互惠行为的延伸，重构四大金融功能。

Result: 把贸易视为互惠的典型形式，重构四大金融功能。

Conclusion: 该框架将重点从制度设计转向行为计算，为人类和人工智能体的去中心化金融行为建模提供新基础。

Abstract: A central challenge in economics and artificial intelligence is explaining
how financial behaviors-such as credit, insurance, and trade-emerge without
formal institutions. We argue that these functions are not products of
institutional design, but structured extensions of a single behavioral
substrate: reciprocity. Far from being a derived strategy, reciprocity served
as the foundational logic of early human societies-governing the circulation of
goods, regulation of obligation, and maintenance of long-term cooperation well
before markets, money, or formal rules. Trade, commonly regarded as the origin
of financial systems, is reframed here as the canonical form of reciprocity:
simultaneous, symmetric, and partner-contingent. Building on this logic, we
reconstruct four core financial functions-credit, insurance, token exchange,
and investment-as expressions of the same underlying principle under varying
conditions. By grounding financial behavior in minimal, simulateable dynamics
of reciprocal interaction, this framework shifts the focus from institutional
engineering to behavioral computation-offering a new foundation for modeling
decentralized financial behavior in both human and artificial agents.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [175] [The Cognitive Foundations of Economic Exchange: A Modular Framework Grounded in Behavioral Evidence](https://arxiv.org/abs/2505.02945)
*Egil Diau*

Main category: cs.CY

TL;DR: 提出基于三个认知原语的行为框架，为可扩展经济行为奠基，统一人类与人工系统中信任、协调和经济行为建模。


<details>
  <summary>Details</summary>
Motivation: 现有经济行为起源理论存在不足，互惠缺乏可模拟和认知基础的解释。

Method: 引入基于个体识别、互惠信任和成本 - 回报敏感性三个认知原语的行为框架。

Result: 该框架能使主体参与并维持互惠交换，从下而上促进合作、原始经济交换和制度结构的出现。

Conclusion: 此框架为人类和人工系统中信任、协调和经济行为建模提供统一基础。

Abstract: The origins of economic behavior remain unresolved-not only in the social
sciences but also in AI, where dominant theories often rely on predefined
incentives or institutional assumptions. Contrary to the longstanding myth of
barter as the foundation of exchange, converging evidence from early human
societies suggests that reciprocity-not barter-was the foundational economic
logic, enabling communities to sustain exchange and social cohesion long before
formal markets emerged. Yet despite its centrality, reciprocity lacks a
simulateable and cognitively grounded account. Here, we introduce a minimal
behavioral framework based on three empirically supported cognitive
primitives-individual recognition, reciprocal credence, and cost--return
sensitivity-that enable agents to participate in and sustain reciprocal
exchange, laying the foundation for scalable economic behavior. These
mechanisms scaffold the emergence of cooperation, proto-economic exchange, and
institutional structure from the bottom up. By bridging insights from
primatology, developmental psychology, and economic anthropology, this
framework offers a unified substrate for modeling trust, coordination, and
economic behavior in both human and artificial systems.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [176] [Deep Neural Network Driven Simulation Based Inference Method for Pole Position Estimation under Model Misspecification](https://arxiv.org/abs/2507.18824)
*Daniel Sadasivan,Isaac Cordero,Andrew Graham,Cecilia Marsh,Daniel Kupcho,Melana Mourad,Maxim Mai*

Main category: hep-ph

TL;DR: 在π - π散射和ρ(770)共振案例中，基于模拟的推理（SBI）在模型误设时比传统卡方最小化对共振参数估计更准确。


<details>
  <summary>Details</summary>
Motivation: 探究在模型误设情况下更准确的共振参数估计方法，且准确建模π - π散射对研究当代物理系统很重要。

Method: 通过π - π散射和ρ(770)共振的案例研究，对比SBI和传统卡方最小化方法。

Result: 卡方最小化拟合某些数据集时对ρ(770)极点位置预测不准确，SBI在相同模型和数据下预测更稳健。

Conclusion: 证明SBI能处理模型误设问题，对当代物理系统研究有重要意义。

Abstract: Simulation Based Inference (SBI) is shown to yield more accurate resonance
parameter estimates than traditional chi-squared minimization in certain cases
of model misspecification, demonstrated through a case study of pi-pi
scattering and the rho(770) resonance. Models fit to some data sets using
chi-squared minimization can predict inaccurate pole positions for the
rho(770), while SBI provides more robust predictions across the same models and
data. This result is significant both as a proof of concept that SBI can handle
model misspecification, and because accurate modeling of pi-pi scattering is
essential in the study of many contemporary physical systems (e.g., a1(1260),
omega(782)).

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [177] [CNN-based Surface Temperature Forecasts with Ensemble Numerical Weather Prediction over Medium-range Forecast Periods](https://arxiv.org/abs/2507.18937)
*Takuya Inoue,Takuya Kawabata*

Main category: physics.ao-ph

TL;DR: 提出结合CNN与集合数值天气预报模型的方法，用于超短期的地表温度预测，该方法能减少系统和随机误差，优于高分辨率确定性NWP模型。


<details>
  <summary>Details</summary>
Motivation: 因计算资源有限，业务中期温度预报依赖低分辨率NWP模型，易产生系统和随机误差，需解决这些局限。

Method: 先对每个集合成员进行基于CNN的后处理以减少系统误差，再对CNN校正后的成员进行集合平均以减少随机误差，还研究了CNN校正和集合平均的顺序对预报精度的影响。

Result: 先进行CNN校正再进行集合平均的方法精度更高，所提方法虽基于低分辨率集合预报，但表现优于高分辨率确定性NWP模型。

Conclusion: 结合基于CNN的校正和集合平均能有效减少NWP模型输出的系统和随机误差，是改进中期温度预报的实用且可扩展的解决方案。

Abstract: This study proposes a method that integrates convolutional neural networks
(CNNs) with ensemble numerical weather prediction (NWP) models, enabling
surface temperature forecasting at lead times beyond the short-range (five-day)
forecast period. Owing to limited computational resources, operational
medium-range temperature forecasts typically rely on low-resolution NWP models,
which are prone to systematic and random errors. To resolve these limitations,
the proposed method first reduces systematic errors through CNN-based
post-processing (bias correction and spatial super-resolution) on each ensemble
member, reconstructing high-resolution temperature fields from low-resolution
model outputs. Second, it reduces random errors through ensemble averaging of
the CNN-corrected members. This study also investigates whether the sequence of
CNN correction and ensemble averaging affects the forecast accuracy. For
comparison with the proposed method, we additionally conducted experiments with
the CNN trained on ensemble-averaged forecasts. The first approach--CNN
correction before ensemble averaging--consistently achieved higher accuracy
than the reverse approach. Although based on low-resolution ensemble forecasts,
the proposed method notably outperformed the high-resolution deterministic NWP
models. These findings indicate that combining CNN-based correction with
ensemble averaging effectively reduces both the systematic and random errors in
NWP model outputs. The proposed approach is a practical and scalable solution
for improving medium-range temperature forecasts, and is particularly valuable
at operational centers with limited computational resources.

</details>


### [178] [A comparison of stretched-grid and limited-area modelling for data-driven regional weather forecasting](https://arxiv.org/abs/2507.18378)
*Jasper S. Wijnands,Michiel Van Ginderachter,Bastien François,Sophie Buurman,Piet Termonia,Dieter Van den Bleeken*

Main category: physics.ao-ph

TL;DR: 本文对比基于图神经网络的区域机器学习天气预报模型LAM和SGM，发现二者性能相当但各有优劣，可为气象机构选择模型提供参考。


<details>
  <summary>Details</summary>
Motivation: 了解LAM和SGM两种模型设计差异对性能和应用的影响，确定其在欧洲区域确定性预报中的优缺点。

Method: 使用Anemoi框架，以相似设置构建并训练两种模型，通过推理实验比较性能。

Result: LAM和SGM是有竞争力的模型，预报性能相当；LAM善用边界强迫，适用于难获取全球数据场景；SGM便于操作，泛化性更好。

Conclusion: 本文可为气象机构在开发数据驱动预报系统时选择LAM和SGM提供起点。

Abstract: Regional machine learning weather prediction (MLWP) models based on graph
neural networks have recently demonstrated remarkable predictive accuracy,
outperforming numerical weather prediction models at lower computational costs.
In particular, limited-area model (LAM) and stretched-grid model (SGM)
approaches have emerged for generating high-resolution regional forecasts,
based on initial conditions from a regional (re)analysis. While LAM uses
lateral boundaries from an external global model, SGM incorporates a global
domain at lower resolution. This study aims to understand how the differences
in model design impact relative performance and potential applications.
Specifically, the strengths and weaknesses of these two approaches are
identified for generating deterministic regional forecasts over Europe. Using
the Anemoi framework, models of both types are built by minimally adapting a
shared architecture and trained using global and regional reanalyses in a
near-identical setup. Several inference experiments have been conducted to
explore their relative performance and highlight key differences. Results show
that both LAM and SGM are competitive deterministic MLWP models with generally
accurate and comparable forecasting performance over the regional domain.
Various differences were identified in the performance of the models across
applications. LAM is able to successfully exploit high-quality boundary
forcings to make predictions within the regional domain and is suitable in
contexts where global data is difficult to acquire. SGM is fully self-contained
for easier operationalisation, can take advantage of more training data and
significantly surpasses LAM in terms of (temporal) generalisability. Our paper
can serve as a starting point for meteorological institutes to guide their
choice between LAM and SGM in developing an operational data-driven forecasting
system.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [179] [Learned Single-Pixel Fluorescence Microscopy](https://arxiv.org/abs/2507.18740)
*Serban C. Tudosie,Valerio Gandolfi,Shivaprasad Varakkoth,Andrea Farina,Cosimo D'Andrea,Simon Arridge*

Main category: eess.IV

TL;DR: 利用自监督训练自编码器提升荧光显微镜单像素成像效果，减少重建时间、提高图像质量并实现多光谱重建。


<details>
  <summary>Details</summary>
Motivation: 当前荧光显微镜单像素成像中，需提升压缩、重建质量和速度。

Method: 通过自监督训练自编码器学习编码器（测量矩阵）和解码器，并在物理获取的多光谱和强度数据上测试，使学习的编码器成为物理设备一部分。

Result: 减少重建时间两个数量级，实现优质图像质量和多光谱重建。

Conclusion: 学习型单像素荧光显微镜可推动诊断和生物研究，以低成本实现多光谱成像。

Abstract: Single-pixel imaging has emerged as a key technique in fluorescence
microscopy, where fast acquisition and reconstruction are crucial. In this
context, images are reconstructed from linearly compressed measurements. In
practice, total variation minimisation is still used to reconstruct the image
from noisy measurements of the inner product between orthogonal sampling
pattern vectors and the original image data. However, data can be leveraged to
learn the measurement vectors and the reconstruction process, thereby enhancing
compression, reconstruction quality, and speed. We train an autoencoder through
self-supervision to learn an encoder (or measurement matrix) and a decoder. We
then test it on physically acquired multispectral and intensity data. During
acquisition, the learned encoder becomes part of the physical device. Our
approach can enhance single-pixel imaging in fluorescence microscopy by
reducing reconstruction time by two orders of magnitude, achieving superior
image quality, and enabling multispectral reconstructions. Ultimately, learned
single-pixel fluorescence microscopy could advance diagnosis and biological
research, providing multispectral imaging at a fraction of the cost.

</details>


### [180] [Dual Path Learning -- learning from noise and context for medical image denoising](https://arxiv.org/abs/2507.19035)
*Jitindra Fartiyal,Pedro Freire,Yasmeen Whayeb,James S. Wolffsohn,Sergei K. Turitsyn,Sergei G. Sokolov*

Main category: eess.IV

TL;DR: 文章引入双路径学习（DPL）模型架构对医学图像去噪，在多模态和多种噪声类型下评估，相比基线UNet有PSNR提升，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像去噪方法通常仅依赖噪声特征或图像上下文信息，且多针对单一成像模态和噪声类型，本文受CNCL启发，旨在开发能综合利用两种信息的去噪模型。

Method: 引入双路径学习（DPL）模型架构，融合噪声和上下文信息进行医学图像去噪。

Result: DPL在多种成像模态和不同类型噪声下评估，在高斯噪声上评估且跨所有模态训练时，相比基线UNet使PSNR提高3.35%。

Conclusion: DPL模型架构有效，具有鲁棒性和泛化性，能用于医学图像去噪。

Abstract: Medical imaging plays a critical role in modern healthcare, enabling
clinicians to accurately diagnose diseases and develop effective treatment
plans. However, noise, often introduced by imaging devices, can degrade image
quality, leading to misinterpretation and compromised clinical outcomes.
Existing denoising approaches typically rely either on noise characteristics or
on contextual information from the image. Moreover, they are commonly developed
and evaluated for a single imaging modality and noise type. Motivated by Geng
et.al CNCL, which integrates both noise and context, this study introduces a
Dual-Pathway Learning (DPL) model architecture that effectively denoises
medical images by leveraging both sources of information and fusing them to
generate the final output. DPL is evaluated across multiple imaging modalities
and various types of noise, demonstrating its robustness and generalizability.
DPL improves PSNR by 3.35% compared to the baseline UNet when evaluated on
Gaussian noise and trained across all modalities. The code is available at
10.5281/zenodo.15836053.

</details>


### [181] [RealDeal: Enhancing Realism and Details in Brain Image Generation via Image-to-Image Diffusion Models](https://arxiv.org/abs/2507.18830)
*Shen Zhu,Yinzhu Jin,Tyler Spears,Ifrah Zawar,P. Thomas Fletcher*

Main category: eess.IV

TL;DR: 提出图像到图像扩散模型提升生成脑图像的真实感和细节，使用常用及新指标评估。


<details>
  <summary>Details</summary>
Motivation: 现有潜在扩散模型生成的脑图像因潜在压缩过于平滑，缺乏精细结构和扫描噪声，需提升图像真实感和增加细节。

Method: 将提升真实感和添加细节的过程构建为图像到图像扩散模型，对潜在扩散模型生成的图像进行优化，并采用常用指标和新指标评估。

Result: 无明确提及成果内容。

Conclusion: 无明确提及结论内容。

Abstract: We propose image-to-image diffusion models that are designed to enhance the
realism and details of generated brain images by introducing sharp edges, fine
textures, subtle anatomical features, and imaging noise. Generative models have
been widely adopted in the biomedical domain, especially in image generation
applications. Latent diffusion models achieve state-of-the-art results in
generating brain MRIs. However, due to latent compression, generated images
from these models are overly smooth, lacking fine anatomical structures and
scan acquisition noise that are typically seen in real images. This work
formulates the realism enhancing and detail adding process as image-to-image
diffusion models, which refines the quality of LDM-generated images. We employ
commonly used metrics like FID and LPIPS for image realism assessment.
Furthermore, we introduce new metrics to demonstrate the realism of images
generated by RealDeal in terms of image noise distribution, sharpness, and
texture.

</details>


### [182] [Enhancing Diabetic Retinopathy Classification Accuracy through Dual Attention Mechanism in Deep Learning](https://arxiv.org/abs/2507.19199)
*Abdul Hannan,Zahid Mahmood,Rizwan Qureshi,Hazrat Ali*

Main category: eess.IV

TL;DR: 本文结合GAB和CAB到深度学习模型，用三种预训练网络作为骨干架构解决糖尿病视网膜病变（DR）分类中的数据不平衡问题，在两个公开数据集上评估，取得有竞争力的性能。


<details>
  <summary>Details</summary>
Motivation: DR自动分类对临床重要，但数据分布不平衡阻碍深度学习模型泛化，需解决此问题。

Method: 将GAB和CAB结合到基于注意力机制的深度学习模型，用MobileNetV3 - small、Efficientnet - b0和DenseNet - 169作为骨干架构。

Result: 在APTOS和EYEPACS数据集上各网络有不同准确率，还计算了F1分数、精度等指标，MobileNetV3 - small参数相对较少。

Conclusion: 所提方法在DR分类上取得了与近期研究相当的有竞争力的性能。

Abstract: Automatic classification of Diabetic Retinopathy (DR) can assist
ophthalmologists in devising personalized treatment plans, making it a critical
component of clinical practice. However, imbalanced data distribution in the
dataset becomes a bottleneck in the generalization of deep learning models
trained for DR classification. In this work, we combine global attention block
(GAB) and category attention block (CAB) into the deep learning model, thus
effectively overcoming the imbalanced data distribution problem in DR
classification. Our proposed approach is based on an attention mechanism-based
deep learning model that employs three pre-trained networks, namely,
MobileNetV3-small, Efficientnet-b0, and DenseNet-169 as the backbone
architecture. We evaluate the proposed method on two publicly available
datasets of retinal fundoscopy images for DR. Experimental results show that on
the APTOS dataset, the DenseNet-169 yielded 83.20% mean accuracy, followed by
the MobileNetV3-small and EfficientNet-b0, which yielded 82% and 80%
accuracies, respectively. On the EYEPACS dataset, the EfficientNet-b0 yielded a
mean accuracy of 80%, while the DenseNet-169 and MobileNetV3-small yielded
75.43% and 76.68% accuracies, respectively. In addition, we also compute the
F1-score of 82.0%, precision of 82.1%, sensitivity of 83.0%, specificity of
95.5%, and a kappa score of 88.2% for the experiments. Moreover, in our work,
the MobileNetV3-small has 1.6 million parameters on the APTOS dataset and 0.90
million parameters on the EYEPACS dataset, which is comparatively less than
other methods. The proposed approach achieves competitive performance that is
at par with recently reported works on DR classification.

</details>


### [183] [Joint Holistic and Lesion Controllable Mammogram Synthesis via Gated Conditional Diffusion Model](https://arxiv.org/abs/2507.19201)
*Xin Li,Kaixiang Yang,Qiang Li,Zhiwei Wang*

Main category: eess.IV

TL;DR: 本文提出Gated Conditional Diffusion Model (GCDM)用于合成乳腺X光图像及局部病变，实验表明该模型能精准控制小病变区域，提升合成图像的真实感和多样性。


<details>
  <summary>Details</summary>
Motivation: 乳腺X光筛查对深度学习技术需求增加，但现有方法受数据不足和病变特征多样性缺乏的限制，且当前生成模型无法充分强调病变特征及其与周围组织的关系。

Method: 提出GCDM，基于潜在去噪扩散框架，将噪声潜在图像与代表乳房、病变及其过渡区域的软掩码嵌入连接，还加入门控条件分支，动态选择和融合病变的放射组学和几何属性以指导去噪过程。

Result: GCDM能精准控制小病变区域，提升合成乳腺X光图像的真实感和多样性。

Conclusion: GCDM是乳腺X光图像合成临床应用的有前景工具。

Abstract: Mammography is the most commonly used imaging modality for breast cancer
screening, driving an increasing demand for deep-learning techniques to support
large-scale analysis. However, the development of accurate and robust methods
is often limited by insufficient data availability and a lack of diversity in
lesion characteristics. While generative models offer a promising solution for
data synthesis, current approaches often fail to adequately emphasize
lesion-specific features and their relationships with surrounding tissues. In
this paper, we propose Gated Conditional Diffusion Model (GCDM), a novel
framework designed to jointly synthesize holistic mammogram images and
localized lesions. GCDM is built upon a latent denoising diffusion framework,
where the noised latent image is concatenated with a soft mask embedding that
represents breast, lesion, and their transitional regions, ensuring anatomical
coherence between them during the denoising process. To further emphasize
lesion-specific features, GCDM incorporates a gated conditioning branch that
guides the denoising process by dynamically selecting and fusing the most
relevant radiomic and geometric properties of lesions, effectively capturing
their interplay. Experimental results demonstrate that GCDM achieves precise
control over small lesion areas while enhancing the realism and diversity of
synthesized mammograms. These advancements position GCDM as a promising tool
for clinical applications in mammogram synthesis. Our code is available at
https://github.com/lixinHUST/Gated-Conditional-Diffusion-Model/

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [184] [Assessment of Personality Dimensions Across Situations Using Conversational Speech](https://arxiv.org/abs/2507.19137)
*Alice Zhang,Skanda Muralidhar,Daniel Gatica-Perez,Mathew Magimai-Doss*

Main category: eess.AS

TL;DR: 研究在两种工作场景下对话语音与感知人格的关系，发现感知人格随互动变化，某些声学特征与人格相关，手工特征优于说话人嵌入，压力互动更能预测神经质。


<details>
  <summary>Details</summary>
Motivation: 以往自动人格感知研究将人格视为静态特征，忽视情境影响，本研究旨在探究不同工作情境下对话语音与感知人格的关系。

Method: 研究参与者在中立面试和压力客户互动两种工作情境下的对话语音。

Result: 1. 感知人格在不同互动中差异显著；2. 响度等特征在不同情境与不同人格特质相关；3. 手工声学和非语言特征在推断感知人格上优于说话人嵌入；4. 压力互动更能预测神经质。

Conclusion: 感知人格受情境影响，特定声学特征与人格特质相关，研究结果与现有心理学研究相符。

Abstract: Prior research indicates that users prefer assistive technologies whose
personalities align with their own. This has sparked interest in automatic
personality perception (APP), which aims to predict an individual's perceived
personality traits. Previous studies in APP have treated personalities as
static traits, independent of context. However, perceived personalities can
vary by context and situation as shown in psychological research. In this
study, we investigate the relationship between conversational speech and
perceived personality for participants engaged in two work situations (a
neutral interview and a stressful client interaction). Our key findings are: 1)
perceived personalities differ significantly across interactions, 2) loudness,
sound level, and spectral flux features are indicative of perceived
extraversion, agreeableness, conscientiousness, and openness in neutral
interactions, while neuroticism correlates with these features in stressful
contexts, 3) handcrafted acoustic features and non-verbal features outperform
speaker embeddings in inference of perceived personality, and 4) stressful
interactions are more predictive of neuroticism, aligning with existing
psychological research.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [185] [Negative news posts are less prevalent and generate lower user engagement than non-negative news posts across six countries](https://arxiv.org/abs/2507.19300)
*Szymon Talaga,Dominik Batorski,Magdalena Wojcieszak*

Main category: cs.SI

TL;DR: 利用600多万条Facebook新闻帖分析社交媒体上负面政治与非政治新闻帖情况，发现负面新闻占比小、政治与非政治新闻负面程度相当等。


<details>
  <summary>Details</summary>
Motivation: 缺少社交媒体上负面政治与非政治新闻帖流行度和用户参与度的比较证据。

Method: 使用6081134条Facebook新闻帖，开发两个多语言分类器对帖子进行（非）政治和（非）负面标注。

Result: 负面新闻帖占比12.6%；政治与非政治新闻负面程度相当；美国政治新闻帖平均负面程度低于其他国家；负面新闻帖点赞和评论更少；负面新闻帖用户参与度占比10.2% - 13.1%。

Conclusion: 揭示了社交媒体上负面政治与非政治新闻帖的流行度和用户参与度情况。

Abstract: Although news negativity is often studied, missing is comparative evidence on
the prevalence of and engagement with negative political and non-political news
posts on social media. We use 6,081,134 Facebook posts published between
January 1, 2020, and April 1, 2024, by 97 media organizations in six countries
(U.S., UK, Ireland, Poland, France, Spain) and develop two multilingual
classifiers for labeling posts as (non-)political and (non-)negative. We show
that: (1) negative news posts constitute a relatively small fraction (12.6%);
(2) political news posts are neither more nor less negative than non-political
news posts; (3) U.S. political news posts are less negative relative to the
other countries on average (40% lower odds); (4) Negative news posts get 15%
fewer likes and 13% fewer comments than non-negative news posts. Lastly, (5) we
provide estimates of the proportion of the total volume of user engagement with
negative news posts and show that only between 10.2% to 13.1% of engagement is
linked to negative posts by the analyzed news organizations.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [186] [Cycle-factors of regular graphs via entropy](https://arxiv.org/abs/2507.19417)
*Micha Christoph,Nemanja Draganić,António Girão,Eoin Hurley,Lukas Michel,Alp Müyesser*

Main category: math.CO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: It is a classical result that a random permutation of $n$ elements has, on
average, about $\log n$ cycles. We generalise this fact to all directed
$d$-regular graphs on $n$ vertices by showing that, on average, a random
cycle-factor of such a graph has $\mathcal{O}((n\log d)/d)$ cycles. This is
tight up to the constant factor and improves the best previous bound of the
form $\mathcal{O}(n/\sqrt{\log d})$ due to Vishnoi. Our results also yield
randomised polynomial-time algorithms for finding such a cycle-factor and for
finding a tour of length $(1+\mathcal{O}((\log d)/d)) \cdot n$ if the graph is
connected. This makes progress on a conjecture of Magnant and Martin and on a
problem studied by Vishnoi and by Feige, Ravi, and Singh. Our proof uses the
language of entropy to exploit the fact that the upper and lower bounds on the
number of perfect matchings in regular bipartite graphs are extremely close.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [187] [Controlling Topological Defects in Polar Fluids via Reinforcement Learning](https://arxiv.org/abs/2507.19298)
*Abhinav Singh,Petros Koumoutsakos*

Main category: cond-mat.soft

TL;DR: 研究通过调节活性空间分布对受限活性流体中整数电荷缺陷进行闭环控制，借助连续流体动力学模型和强化学习框架实现缺陷运输，为活性物质可控性和自适应材料设计提供见解。


<details>
  <summary>Details</summary>
Motivation: 拓扑缺陷在活性极性流体中有复杂动力学，反馈控制可引导系统实现动态状态转变，探索对其闭环控制方法。

Method: 采用连续流体动力学模型，通过局部控制活性应力诱导流场；使用强化学习框架发现有效控制策略。

Result: 实现了缺陷沿规定轨迹的重新定位和定向运输，强化学习策略在训练和新轨迹上都能实现稳健的缺陷运输。

Conclusion: AI 智能体可学习底层动力学并调控活性，为活性物质可控性和自适应自组织材料设计提供了思路。

Abstract: Topological defects in active polar fluids exhibit complex dynamics driven by
internally generated stresses, reflecting the deep interplay between topology,
flow, and non-equilibrium hydrodynamics. Feedback control offers a powerful
means to guide such systems, enabling transitions between dynamic states. We
investigated closed-loop steering of integer-charged defects in a confined
active fluid by modulating the spatial profile of activity. Using a continuum
hydrodynamic model, we show that localized control of active stress induces
flow fields that can reposition and direct defects along prescribed
trajectories by exploiting non-linear couplings in the system. A reinforcement
learning framework is used to discover effective control strategies that
produce robust defect transport across both trained and novel trajectories. The
results highlight how AI agents can learn the underlying dynamics and spatially
structure activity to manipulate topological excitations, offering insights
into the controllability of active matter and the design of adaptive,
self-organized materials.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [188] [Graph Neural Network-Based Predictor for Optimal Quantum Hardware Selection](https://arxiv.org/abs/2507.19093)
*Antonio Tudisco,Deborah Volpe,Giacomo Orlandi,Giovanna Turvani*

Main category: quant-ph

TL;DR: 本文提出基于图神经网络的预测器自动选择量子硬件，评估498个量子电路，实验准确率94.4%，F1分数85.5%，代码开源。


<details>
  <summary>Details</summary>
Motivation: 量子硬件技术多样，现有硬件选择方法计算成本高、扩展性差。

Method: 提出基于图神经网络的预测器，分析量子电路的有向无环图表示，利用图机器学习直接嵌入图。

Result: 对498个量子电路实验，93个在俘获离子设备上最优编译，其余偏好超导平台，准确率94.4%，少数类F1分数85.5%。

Conclusion: 所提方法能有效预测最佳编译目标，加速决策过程并保留信息。

Abstract: The growing variety of quantum hardware technologies, each with unique
peculiarities such as connectivity and native gate sets, creates challenges
when selecting the best platform for executing a specific quantum circuit. This
selection process usually involves a brute-force approach: compiling the
circuit on various devices and evaluating performance based on factors such as
circuit depth and gate fidelity. However, this method is computationally
expensive and does not scale well as the number of available quantum processors
increases. In this work, we propose a Graph Neural Network (GNN)-based
predictor that automates hardware selection by analyzing the Directed Acyclic
Graph (DAG) representation of a quantum circuit. Our study evaluates 498
quantum circuits (up to 27 qubits) from the MQT Bench dataset, compiled using
Qiskit on four devices: three superconducting quantum processors (IBM-Kyiv,
IBM-Brisbane, IBM-Sherbrooke) and one trapped-ion processor (IONQ-Forte).
Performance is estimated using a metric that integrates circuit depth and gate
fidelity, resulting in a dataset where 93 circuits are optimally compiled on
the trapped-ion device, while the remaining circuits prefer superconducting
platforms. By exploiting graph-based machine learning, our approach avoids
extracting the circuit features for the model evaluation but directly embeds it
as a graph, significantly accelerating the optimal target decision-making
process and maintaining all the information. Experimental results prove 94.4%
accuracy and an 85.5% F1 score for the minority class, effectively predicting
the best compilation target. The developed code is publicly available on GitHub
(https://github.com/antotu/GNN-Model-Quantum-Predictor).

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [189] [Interpretable inverse design of optical multilayer thin films based on extended neural adjoint and regression activation mapping](https://arxiv.org/abs/2507.18644)
*Sungjun Kim,Jungho Kim*

Main category: physics.optics

TL;DR: 提出扩展神经伴随（ENA）框架用于光学多层薄膜逆设计，引入新网络架构和材料损失函数，验证效果并与Res - GLOnet对比，还展示了解释性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 满足光学多层薄膜人工智能辅助逆设计的六个关键标准，提高现有神经伴随方法的可扩展性。

Method: 提出新的前向神经网络架构，引入材料损失函数到现有神经伴随损失函数，提出回归激活映射（F - RAM）用于特征可视化，进行消融实验。

Result: 材料损失函数显著提高准确性和多样性；ENA逆设计比Res - GLOnet有更高准确性和更好多样性；F - RAM显示相似光学特性的不同OMT结构特征重要性分布一致。

Conclusion: ENA框架满足光学多层薄膜逆设计的六个关键标准，具有准确性、效率、多样性、可扩展性、灵活性和可解释性。

Abstract: We propose an extended neural adjoint (ENA) framework, which meets six key
criteria for artificial intelligence-assisted inverse design of optical
multilayer thin films (OMTs): accuracy, efficiency, diversity, scalability,
flexibility, and interpretability. To enhance the scalability of the existing
neural adjoint method, we present a novel forward neural network architecture
for OMTs and introduce a material loss function into the existing neural
adjoint loss function, facilitating the exploration of material configurations
of OMTs. Furthermore, we present the detailed formulation of the regression
activation mapping for the presented forward neural network architecture
(F-RAM), a feature visualization method aimed at improving interpretability. We
validated the efficacy of the material loss by conducting an ablation study,
where each component of the loss function is systematically removed and
evaluated. The results indicated that the inclusion of the material loss
significantly improves accuracy and diversity. To substantiate the performance
of the ENA-based inverse design, we compared it against the residual
network-based global optimization network (Res-GLOnet). The ENA yielded the OMT
solutions of an inverse design with higher accuracy and better diversity
compared to the Res-GLOnet. To demonstrate the interpretability, we applied
F-RAM to diverse OMT structures with similar optical properties, obtained by
the proposed ENA method. We showed that distributions of feature importance for
various OMT structures exhibiting analogous optical properties are consistent,
despite variations in material configurations, layer number, and thicknesses.
Furthermore, we demonstrate the flexibility of the ENA method by restricting
the initial layer of OMTs to SiO2 and 100 nm.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [190] [TiVy: Time Series Visual Summary for Scalable Visualization](https://arxiv.org/abs/2507.18972)
*Gromit Yeuk-Yin Chan,Luis Gustavo Nonato,Themis Palpanas,Cláudio T. Silva,Juliana Freire*

Main category: cs.GR

TL;DR: 提出新算法TiVy总结时间序列，可实时可视化大规模时间序列，实验显示能提取清晰模式且速度快。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列可视化表示在可扩展性上存在问题，处理长时间跨度数据时会导致视觉混乱。

Method: 提出TiVy算法，基于动态时间规整（DTW）将时间序列转换为符号序列，根据频繁序列模式构建相似子序列的不相交分组。

Result: 算法能在可视化时间序列数据时提取清晰准确的模式，相比直接的DTW聚类实现了1000倍的显著加速。

Conclusion: TiVy算法有效，能用于探索大规模时间序列数据中的隐藏结构。

Abstract: Visualizing multiple time series presents fundamental tradeoffs between
scalability and visual clarity. Time series capture the behavior of many
large-scale real-world processes, from stock market trends to urban activities.
Users often gain insights by visualizing them as line charts, juxtaposing or
superposing multiple time series to compare them and identify trends and
patterns. However, existing representations struggle with scalability: when
covering long time spans, leading to visual clutter from too many small
multiples or overlapping lines. We propose TiVy, a new algorithm that
summarizes time series using sequential patterns. It transforms the series into
a set of symbolic sequences based on subsequence visual similarity using
Dynamic Time Warping (DTW), then constructs a disjoint grouping of similar
subsequences based on the frequent sequential patterns. The grouping result, a
visual summary of time series, provides uncluttered superposition with fewer
small multiples. Unlike common clustering techniques, TiVy extracts similar
subsequences (of varying lengths) aligned in time. We also present an
interactive time series visualization that renders large-scale time series in
real-time. Our experimental evaluation shows that our algorithm (1) extracts
clear and accurate patterns when visualizing time series data, (2) achieves a
significant speed-up (1000X) compared to a straightforward DTW clustering. We
also demonstrate the efficiency of our approach to explore hidden structures in
massive time series data in two usage scenarios.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [191] [Gradient-based grand canonical optimization enabled by graph neural networks with fractional atomic existence](https://arxiv.org/abs/2507.19438)
*Mads-Peter Verner Christiansen,Bjørk Hammer*

Main category: cond-mat.mtrl-sci

TL;DR: 本文扩展消息传递形式，引入分数原子存在变量，提出基于梯度的大正则优化方法并用于Cu(110)表面氧化物。


<details>
  <summary>Details</summary>
Motivation: 机器学习原子间势是材料科学重要工具，当前模型为图神经网络，为计算吉布斯自由能梯度及实现大正则优化。

Method: 扩展消息传递形式，引入连续变量表示分数原子存在，提出基于梯度的大正则优化方法。

Result: 可计算吉布斯自由能相对于原子笛卡尔坐标和存在性的梯度。

Conclusion: 所提方法对Cu(110)表面氧化物有应用能力。

Abstract: Machine learning interatomic potentials have become an indispensable tool for
materials science, enabling the study of larger systems and longer timescales.
State-of-the-art models are generally graph neural networks that employ message
passing to iteratively update atomic embeddings that are ultimately used for
predicting properties. In this work we extend the message passing formalism
with the inclusion of a continuous variable that accounts for fractional atomic
existence. This allows us to calculate the gradient of the Gibbs free energy
with respect to both the Cartesian coordinates of atoms and their existence.
Using this we propose a gradient-based grand canonical optimization method and
document its capabilities for a Cu(110) surface oxide.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [192] [GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning](https://arxiv.org/abs/2507.19457)
*Lakshya A Agrawal,Shangyin Tan,Dilara Soylu,Noah Ziems,Rishi Khare,Krista Opsahl-Ong,Arnav Singhvi,Herumb Shandilya,Michael J Ryan,Meng Jiang,Christopher Potts,Koushik Sen,Alexandros G. Dimakis,Ion Stoica,Dan Klein,Matei Zaharia,Omar Khattab*

Main category: cs.CL

TL;DR: 本文提出提示优化器GEPA，利用自然语言反思学习，在多个任务上优于GRPO和MIPROv2，且所需滚动次数少。


<details>
  <summary>Details</summary>
Motivation: 指出语言的可解释性相比基于稀疏标量奖励的策略梯度，能为大语言模型提供更丰富的学习媒介，因此探索利用自然语言反思的方法。

Method: 引入GEPA，它会对系统级轨迹进行采样，用自然语言反思来诊断问题、提出并测试提示更新，结合帕累托前沿的经验。

Result: 在四个任务上，GEPA平均比GRPO性能高10%，最高高20%，所需滚动次数最多少35倍；在两个大语言模型上比MIPROv2性能高10%以上，在代码优化推理时间搜索策略上有良好表现。

Conclusion: GEPA利用自然语言反思的设计有效，能以较少的滚动次数实现质量大幅提升，在多个任务和场景中表现出色。

Abstract: Large language models (LLMs) are increasingly adapted to downstream tasks via
reinforcement learning (RL) methods like Group Relative Policy Optimization
(GRPO), which often require thousands of rollouts to learn new tasks. We argue
that the interpretable nature of language can often provide a much richer
learning medium for LLMs, compared with policy gradients derived from sparse,
scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt
optimizer that thoroughly incorporates natural language reflection to learn
high-level rules from trial and error. Given any AI system containing one or
more LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool
calls, and tool outputs) and reflects on them in natural language to diagnose
problems, propose and test prompt updates, and combine complementary lessons
from the Pareto frontier of its own attempts. As a result of GEPA's design, it
can often turn even just a few rollouts into a large quality gain. Across four
tasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up
to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer,
MIPROv2, by over 10% across two LLMs, and demonstrates promising results as an
inference-time search strategy for code optimization.

</details>


### [193] [Specification Self-Correction: Mitigating In-Context Reward Hacking Through Test-Time Refinement](https://arxiv.org/abs/2507.18742)
*Víctor Gallego*

Main category: cs.CL

TL;DR: 提出Specification Self - Correction (SSC)框架，可让语言模型在推理时自我修正规范，减少利用规范漏洞的情况，使模型行为更符合预期。


<details>
  <summary>Details</summary>
Motivation: 语言模型易受上下文奖励攻击，利用规范漏洞得分而不满足用户真实意图，需解决此问题。

Method: 采用多步推理过程，先根据可能有问题的规范生成响应，再对输出进行批判，然后修正规范，最后用修正后的规范生成更可靠的响应。

Result: 实验表明，模型最初在50 - 70%的情况下利用有问题的规范，SSC过程将这种漏洞利用情况减少了90%以上。

Conclusion: SSC框架在推理时动态修复规范，无需修改模型权重，能使模型行为更可靠地与预期对齐。

Abstract: Language models (LMs) are susceptible to in-context reward hacking, where
they exploit flaws in tainted or faulty written specifications or rubrics to
achieve high scores without fulfilling the user's true intent. We introduce
Specification Self-Correction (SSC), a novel, test-time framework that enables
an LM to identify and correct flaws within its own guiding specification. SSC
employs a multi-step inference process where the model first generates a
response based on a potentially tainted specification, critiques its output,
and then revises the specification itself to remove the exploitable loophole. A
final, more robust response is then generated using this self-corrected
specification. Across experiments spanning creative writing and agentic coding
tasks with several LMs, we demonstrate that while models initially game tainted
specifications in 50-70\% of cases, the SSC process reduces this vulnerability
by over 90\%. This dynamic repair occurs at inference time, requires no weight
modification, and leads to more robustly aligned model behavior. Code at
https://github.com/vicgalle/specification-self-correction .

</details>


### [194] [PrismRAG: Boosting RAG Factuality with Distractor Resilience and Strategized Reasoning](https://arxiv.org/abs/2507.18857)
*Mohammad Kachuee,Teja Gollapudi,Minseok Kim,Yin Huang,Kai Sun,Xiao Yang,Jiaqi Wang,Nirav Shah,Yue Liu,Aaron Colak,Anuj Kumar,Wen-tau Yih,Xin Luna Dong*

Main category: cs.CL

TL;DR: 提出高效微调框架PrismRAG，在12个基准测试中提升事实性，优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 解决检索增强生成（RAG）在遇到混淆的半相关段落或需深度上下文理解和推理时表现不佳的问题。

Method: 使用混合黄金证据和微妙干扰段落的抗干扰问答对训练模型，培养以推理为中心的习惯，使大语言模型无需大量人工指令即可规划、推理和合成。

Result: 在12个涵盖不同应用领域和场景的开卷RAG问答基准测试中，PrismRAG平均事实性提高了5.4%。

Conclusion: PrismRAG是一种有效的方法，优于现有最先进的解决方案。

Abstract: Retrieval-augmented generation (RAG) often falls short when retrieved context
includes confusing semi-relevant passages, or when answering questions require
deep contextual understanding and reasoning. We propose an efficient
fine-tuning framework, called PrismRAG, that (i) trains the model with
distractor-aware QA pairs mixing gold evidence with subtle distractor passages,
and (ii) instills reasoning-centric habits that make the LLM plan, rationalize,
and synthesize without relying on extensive human engineered instructions.
Evaluated across 12 open-book RAG QA benchmarks spanning diverse application
domains and scenarios, PrismRAG improves average factuality by 5.4%,
outperforming state-of-the-art solutions.

</details>


### [195] [Uncovering Cross-Linguistic Disparities in LLMs using Sparse Autoencoders](https://arxiv.org/abs/2507.18918)
*Richmond Sin Jing Xuan,Jalil Huseynov,Yang Zhang*

Main category: cs.CL

TL;DR: 分析多语言大模型Gemma - 2 - 2B在不同语言激活模式差异，用激活感知微调提升性能并取得一定效果。


<details>
  <summary>Details</summary>
Motivation: 多语言大模型在中低资源语言的常见基准测试中表现不佳，需提升其在这些语言上的性能。

Method: 分析Gemma - 2 - 2B在26个残差层和10种语言的激活模式，用稀疏自编码器揭示差异，通过低秩自适应进行激活感知微调。

Result: 中低资源语言早期层激活低，微调后有大幅激活增益，英语保留率约91%，基准测试结果有适度提升。

Conclusion: 激活对齐是提升多语言大模型性能的关键因素。

Abstract: Multilingual large language models (LLMs) exhibit strong cross-linguistic
generalization, yet medium to low resource languages underperform on common
benchmarks such as ARC-Challenge, MMLU, and HellaSwag. We analyze activation
patterns in Gemma-2-2B across all 26 residual layers and 10 languages: Chinese
(zh), Russian (ru), Spanish (es), Italian (it), medium to low resource
languages including Indonesian (id), Catalan (ca), Marathi (mr), Malayalam
(ml), and Hindi (hi), with English (en) as the reference. Using Sparse
Autoencoders (SAEs), we reveal systematic disparities in activation patterns.
Medium to low resource languages receive up to 26.27 percent lower activations
in early layers, with a persistent gap of 19.89 percent in deeper layers. To
address this, we apply activation-aware fine-tuning via Low-Rank Adaptation
(LoRA), leading to substantial activation gains, such as 87.69 percent for
Malayalam and 86.32 percent for Hindi, while maintaining English retention at
approximately 91 percent. After fine-tuning, benchmark results show modest but
consistent improvements, highlighting activation alignment as a key factor in
enhancing multilingual LLM performance.

</details>


### [196] [A Toolbox, Not a Hammer -- Multi-TAG: Scaling Math Reasoning with Multi-Tool Aggregation](https://arxiv.org/abs/2507.18973)
*Bohan Yao,Vikas Yadav*

Main category: cs.CL

TL;DR: 提出Multi - TAG框架提升大语言模型数学推理能力，免微调且在多基准测试中优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有工具增强方法在处理复杂数学问题时存在局限，需新方法。

Method: 提出Multi - TAG框架，在每个推理步骤中同时调用多个工具并聚合输出以验证和完善推理过程，且免微调。

Result: 在四个具有挑战性的基准测试中，Multi - TAG在不同大语言模型骨干上均显著优于现有基线，平均提升6.0% - 7.5%。

Conclusion: Multi - TAG是一种有效的提升大语言模型数学推理能力的框架。

Abstract: Augmenting large language models (LLMs) with external tools is a promising
avenue for developing high-performance mathematical reasoning systems. Prior
tool-augmented approaches typically finetune an LLM to select and invoke a
single tool at each reasoning step and show promising results on simpler math
reasoning benchmarks such as GSM8K. However, these approaches struggle with
more complex math problems that require precise reasoning over multiple steps.
To address this limitation, in this work, we propose Multi-TAG, a Multi-Tool
AGgregation-based framework. Instead of relying on a single tool, Multi-TAG
guides an LLM to concurrently invoke multiple tools at each reasoning step. It
then aggregates their diverse outputs to verify and refine the reasoning
process, enhancing solution robustness and accuracy. Notably, Multi-TAG is a
finetuning-free, inference-only framework, making it readily applicable to any
LLM backbone, including large open-weight models which are computationally
expensive to finetune and proprietary frontier models which cannot be finetuned
with custom recipes. We evaluate Multi-TAG on four challenging benchmarks:
MATH500, AIME, AMC, and OlympiadBench. Across both open-weight and
closed-source LLM backbones, Multi-TAG consistently and substantially
outperforms state-of-the-art baselines, achieving average improvements of 6.0%
to 7.5% over state-of-the-art baselines.

</details>


### [197] [ylmmcl at Multilingual Text Detoxification 2025: Lexicon-Guided Detoxification and Classifier-Gated Rewriting](https://arxiv.org/abs/2507.18769)
*Nicole Lai-Lopez,Lusha Wang,Su Yuan,Liza Zhang*

Main category: cs.CL

TL;DR: 介绍ylmmcl团队在PAN - 2025比赛中多语言文本去毒任务的解决方案，模型表现佳，团队获第九名。


<details>
  <summary>Details</summary>
Motivation: 参与PAN - 2025比赛的多语言文本去毒任务，改进先前无监督或单语言管道。

Method: 构建集成词典引导标记、微调的序列到序列模型和基于迭代分类器的守门机制的多语言文本去毒管道。

Result: 最终模型STA达0.922，官方J得分平均0.612，xCOMET得分分别为0.793（开发集）和0.787（测试集），表现优于基线和反向翻译方法。

Conclusion: 模型虽在SIM上有取舍，但去毒能力持续提升，团队获第九名。

Abstract: In this work, we introduce our solution for the Multilingual Text
Detoxification Task in the PAN-2025 competition for the ylmmcl team: a robust
multilingual text detoxification pipeline that integrates lexicon-guided
tagging, a fine-tuned sequence-to-sequence model (s-nlp/mt0-xl-detox-orpo) and
an iterative classifier-based gatekeeping mechanism. Our approach departs from
prior unsupervised or monolingual pipelines by leveraging explicit toxic word
annotation via the multilingual_toxic_lexicon to guide detoxification with
greater precision and cross-lingual generalization. Our final model achieves
the highest STA (0.922) from our previous attempts, and an average official J
score of 0.612 for toxic inputs in both the development and test sets. It also
achieved xCOMET scores of 0.793 (dev) and 0.787 (test). This performance
outperforms baseline and backtranslation methods across multiple languages, and
shows strong generalization in high-resource settings (English, Russian,
French). Despite some trade-offs in SIM, the model demonstrates consistent
improvements in detoxification strength. In the competition, our team achieved
ninth place with a score of 0.612.

</details>


### [198] [An Empirical Investigation of Gender Stereotype Representation in Large Language Models: The Italian Case](https://arxiv.org/abs/2507.19156)
*Gioele Giachino,Marco Rondina,Antonio Vetrò,Riccardo Coppola,Juan Carlos De Martin*

Main category: cs.CL

TL;DR: 研究聚焦LLMs在意大利语中对无性别提示的回应，揭示其输出存在性别和职业偏见，呼吁开发缓解策略。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在各领域广泛应用，担心其传播刻板印象和生成有偏见内容，研究其对无性别提示的回应方式。

Method: 采用结构化实验方法，给出三种不同职业组合的提示，使用意大利语，对ChatGPT和Gemini两个聊天机器人通过API收集3600条回复。

Result: LLMs生成的内容会延续刻板印象，如Gemini和ChatGPT将高比例的'她'代词与'助理'而非'经理'关联。

Conclusion: AI生成文本存在偏见有重大影响，理解风险对开发缓解策略和确保AI促进公平至关重要，未来可扩大研究范围。

Abstract: The increasing use of Large Language Models (LLMs) in a large variety of
domains has sparked worries about how easily they can perpetuate stereotypes
and contribute to the generation of biased content. With a focus on gender and
professional bias, this work examines in which manner LLMs shape responses to
ungendered prompts, contributing to biased outputs. This analysis uses a
structured experimental method, giving different prompts involving three
different professional job combinations, which are also characterized by a
hierarchical relationship. This study uses Italian, a language with extensive
grammatical gender differences, to highlight potential limitations in current
LLMs' ability to generate objective text in non-English languages. Two popular
LLM-based chatbots are examined, namely OpenAI ChatGPT (gpt-4o-mini) and Google
Gemini (gemini-1.5-flash). Through APIs, we collected a range of 3600
responses. The results highlight how content generated by LLMs can perpetuate
stereotypes. For example, Gemini associated 100% (ChatGPT 97%) of 'she'
pronouns to the 'assistant' rather than the 'manager'. The presence of bias in
AI-generated text can have significant implications in many fields, such as in
the workplaces or in job selections, raising ethical concerns about its use.
Understanding these risks is pivotal to developing mitigation strategies and
assuring that AI-based systems do not increase social inequalities, but rather
contribute to more equitable outcomes. Future research directions include
expanding the study to additional chatbots or languages, refining prompt
engineering methods or further exploiting a larger experimental base.

</details>


### [199] [Can Small-Scale Data Poisoning Exacerbate Dialect-Linked Biases in Large Language Models?](https://arxiv.org/abs/2507.19195)
*Chaymaa Abbas,Mariette Awad,Razane Tajeddine*

Main category: cs.CL

TL;DR: 研究数据投毒和方言差异对大语言模型输出毒性的影响，发现投毒对非裔美国英语输入毒性影响大，大模型更敏感，需相关改进措施。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型不断改进，但仍存在编码和放大社会偏见问题，本研究旨在探讨方言差异（非裔美国英语与标准美国英语）和数据投毒如何影响输出毒性。

Method: 使用中小规模LLaMA模型测试不同方言输入在数据投毒下的输出毒性，并用GPT - 4o作为公平性审计工具评估差异。

Result: 少量投毒数据显著增加非裔美国英语输入的输出毒性，对标准美国英语影响较小；大模型放大效应更明显，GPT - 4o发现有害刻板模式更多与非裔美国英语输入相关。

Conclusion: 强调数据投毒和方言偏见的复合影响，呼吁在开发中进行方言感知评估、有针对性的去偏干预和负责任的训练协议。

Abstract: Despite the ongoing improvements in the design of large language models
(LLMs) to foster inclusion and balanced responses, these systems remain
susceptible to encoding and amplifying social biases. This study examines how
dialectal variation, specifically African American Vernacular English (AAVE)
versus Standard American English (SAE), interacts with data poisoning to
influence toxicity in outputs. Using both small- and medium-scale LLaMA models,
we show that even minimal exposure to poisoned data significantly increases
toxicity for AAVE inputs, while it remains comparatively unaffected for SAE.
Larger models exhibit a more significant amplification effect which suggests
heightened susceptibility with scale. To further assess these disparities, we
employed GPT-4o as a fairness auditor, which identified harmful stereotypical
patterns disproportionately tied to AAVE inputs, including portrayals of
aggression, criminality, and intellectual inferiority. These findings
underscore the compounding impact of data poisoning and dialectal bias and
emphasize the need for dialect-aware evaluation, targeted debiasing
interventions, and socially responsible training protocols during development.

</details>


### [200] [CueBuddy: helping non-native English speakers navigate English-centric STEM education](https://arxiv.org/abs/2507.18827)
*Pranav Gupta*

Main category: cs.CL

TL;DR: 针对全球南方STEM课程学生理解英语专业术语困难问题，提出CueBuddy提供实时词汇提示，还提及方法局限与未来扩展。


<details>
  <summary>Details</summary>
Motivation: 全球南方STEM课程学生虽有科学基础，但英语关键术语理解困难，现有语音翻译模型有局限。

Method: 通过技术关键词识别和实时多语言词汇表查找，提供实时“词汇提示”。

Result: 未提及具体结果。

Conclusion: 文中提及描述了方法的局限性和未来扩展方向，但未明确结论。

Abstract: Students across the world in STEM classes, especially in the Global South,
fall behind their peers who are more fluent in English, despite being at par
with them in terms of scientific prerequisites. While many of them are able to
follow everyday English at ease, key terms in English stay challenging. In most
cases, such students have had most of their course prerequisites in a lower
resource language. Live speech translation to lower resource languages is a
promising area of research, however, models for speech translation can be too
expensive on a large scale and often struggle with technical content. In this
paper, we describe CueBuddy, which aims to remediate these issues by providing
real-time "lexical cues" through technical keyword spotting along real-time
multilingual glossary lookup to help students stay up to speed with complex
English jargon without disrupting their concentration on the lecture. We also
describe the limitations and future extensions of our approach.

</details>


### [201] [A Systematic Review of Key Retrieval-Augmented Generation (RAG) Systems: Progress, Gaps, and Future Directions](https://arxiv.org/abs/2507.18910)
*Agada Joseph Oche,Ademola Glory Folashade,Tirthankar Ghosal,Arpan Biswas*

Main category: cs.CL

TL;DR: 本文对检索增强生成（RAG）进行全面系统综述，涵盖其发展、技术组件、企业部署、性能评估、挑战及新兴解决方案。


<details>
  <summary>Details</summary>
Motivation: 缓解参数模型中的幻觉和知识过时问题，增强事实依据、准确性和上下文相关性。

Method: 追溯RAG从早期到最新的发展，详细分析核心技术组件，逐年分析关键里程碑和研究趋势，在企业系统中评估部署，进行性能对比评估，评估现存挑战。

Result: 明确了RAG的发展脉络、技术细节、企业部署面临的问题、性能表现及现存挑战。

Conclusion: 新兴解决方案为未来更可靠、高效、上下文感知的知识密集型NLP系统指明方向。

Abstract: Retrieval-Augmented Generation (RAG) represents a major advancement in
natural language processing (NLP), combining large language models (LLMs) with
information retrieval systems to enhance factual grounding, accuracy, and
contextual relevance. This paper presents a comprehensive systematic review of
RAG, tracing its evolution from early developments in open domain question
answering to recent state-of-the-art implementations across diverse
applications. The review begins by outlining the motivations behind RAG,
particularly its ability to mitigate hallucinations and outdated knowledge in
parametric models. Core technical components-retrieval mechanisms,
sequence-to-sequence generation models, and fusion strategies are examined in
detail. A year-by-year analysis highlights key milestones and research trends,
providing insight into RAG's rapid growth. The paper further explores the
deployment of RAG in enterprise systems, addressing practical challenges
related to retrieval of proprietary data, security, and scalability. A
comparative evaluation of RAG implementations is conducted, benchmarking
performance on retrieval accuracy, generation fluency, latency, and
computational efficiency. Persistent challenges such as retrieval quality,
privacy concerns, and integration overhead are critically assessed. Finally,
the review highlights emerging solutions, including hybrid retrieval
approaches, privacy-preserving techniques, optimized fusion strategies, and
agentic RAG architectures. These innovations point toward a future of more
reliable, efficient, and context-aware knowledge-intensive NLP systems.

</details>


### [202] [Smooth Reading: Bridging the Gap of Recurrent LLM to Self-Attention LLM on Long-Context Tasks](https://arxiv.org/abs/2507.19353)
*Kai Liu,Zhan Su,Peijie Dong,Fengran Mo,Jianfei Gao,ShaoTing Zhang,Kai Chen*

Main category: cs.CL

TL;DR: 本文提出Smooth Reading方法，以块方式处理上下文，缩小了循环大语言模型与自注意力大语言模型在长上下文任务上的性能差距，且保持了效率优势。


<details>
  <summary>Details</summary>
Motivation: 循环大语言模型在长上下文任务中因固定大小内存受限表现不佳，此前架构创新方法未能使其性能与自注意力大语言模型匹配。

Method: 提出Smooth Reading，一种受人类阅读策略启发的逐块推理方法，逐块处理上下文并迭代总结信息。

Result: 该方法大幅缩小了循环与自注意力大语言模型在长上下文任务上的性能差距，提升SWA - 3B - 4k性能，且保持高效，训练快3倍、推理快2倍。

Conclusion: 这是首次使循环大语言模型在长上下文任务上达到与自注意力大语言模型相当性能的工作，希望能启发未来研究并将发布代码和数据集。

Abstract: Recently, recurrent large language models (Recurrent LLMs) with linear
computational complexity have re-emerged as efficient alternatives to
self-attention-based LLMs (Self-Attention LLMs), which have quadratic
complexity. However, Recurrent LLMs often underperform on long-context tasks
due to their limited fixed-size memory. Previous research has primarily focused
on enhancing the memory capacity of Recurrent LLMs through architectural
innovations, but these approaches have not yet enabled Recurrent LLMs to match
the performance of Self-Attention LLMs on long-context tasks. We argue that
this limitation arises because processing the entire context at once is not
well-suited for Recurrent LLMs. In this paper, we propose Smooth Reading, a
chunk-wise inference method inspired by human reading strategies. Smooth
Reading processes context in chunks and iteratively summarizes the contextual
information, thereby reducing memory demands and making the approach more
compatible with Recurrent LLMs. Our experimental results show that this method
substantially narrows the performance gap between Recurrent and Self-Attention
LLMs on long-context tasks, while preserving the efficiency advantages of
Recurrent LLMs. Our Smooth Reading boosts SWA-3B-4k (a Recurrent LLM) from
5.68% lower to 3.61% higher performance than Self-Attention LLMs on LongBench.
Besides, our method maintains the high efficiency, training 3x faster and
inferring 2x faster at 64k context compared to Self-Attention LLMs. To our
knowledge, this is the first work to achieve comparable performance using
Recurrent LLMs compared with Self-Attention LLMs on long-context tasks. We hope
our method will inspire future research in this area. To facilitate further
progress, we will release code and dataset.

</details>


### [203] [SpeechIQ: Speech Intelligence Quotient Across Cognitive Levels in Voice Understanding Large Language Models](https://arxiv.org/abs/2507.19361)
*Zhen Wan,Chao-Han Huck Yang,Yahan Yu,Jinchuan Tian,Sheng Li,Ke Hu,Zhehuai Chen,Shinji Watanabe,Fei Cheng,Chenhui Chu,Sadao Kurohashi*

Main category: cs.CL

TL;DR: 提出基于语音的智商（SIQ）评估语音理解大语言模型的语音理解能力，有独特优势并揭示多模态训练挑战。


<details>
  <summary>Details</summary>
Motivation: 提出新的评估语音理解大语言模型语音理解能力的方法，超越常用的语音理解指标。

Method: 基于布鲁姆分类法的三个认知层次（记忆、理解、应用）来评估模型。

Result: SIQ不仅能量化语音理解能力，还能对级联方法和端到端模型进行统一比较、识别现有基准中的标注错误、检测模型幻觉。

Conclusion: 该框架是首创的将认知原则与语音基准相结合的智能测试，揭示了多模态训练中被忽视的挑战。

Abstract: We introduce Speech-based Intelligence Quotient (SIQ) as a new form of human
cognition-inspired evaluation pipeline for voice understanding large language
models, LLM Voice, designed to assess their voice understanding ability. Moving
beyond popular voice understanding metrics such as word error rate (WER), SIQ
examines LLM Voice across three cognitive levels motivated by Bloom's Taxonomy:
(1) Remembering (i.e., WER for verbatim accuracy); (2) Understanding (i.e.,
similarity of LLM's interpretations); and (3) Application (i.e., QA accuracy
for simulating downstream tasks). We demonstrate that SIQ not only quantifies
voice understanding abilities but also provides unified comparisons between
cascaded methods (e.g., ASR LLM) and end-to-end models, identifies annotation
errors in existing benchmarks, and detects hallucinations in LLM Voice. Our
framework represents a first-of-its-kind intelligence examination that bridges
cognitive principles with voice-oriented benchmarks, while exposing overlooked
challenges in multi-modal training.

</details>


### [204] [Data Augmentation for Spoken Grammatical Error Correction](https://arxiv.org/abs/2507.19374)
*Penny Karanasou,Mengjie Qian,Stefano Bannò,Mark J. F. Gales,Kate M. Knill*

Main category: cs.CL

TL;DR: 本文提出自动生成含语法错误和语流不畅的音频 - 文本对的方法及评估指标，在S&I语料库上开展实验评估增强语料效果。


<details>
  <summary>Details</summary>
Motivation: 口语语法错误纠正（SGEC）领域高质量标注口语数据集资源不足，需生成增强数据集。

Method: 提出自动生成音频 - 文本对的方法和评估生成数据的客观指标。

Result: 在S&I语料库上评估了增强语料在书面GEC和SGEC中的使用情况。

Conclusion: 未明确提及结论，但目标是生成能保持原数据特征、提供新错误类型且不改变二语学习者语言评估分数的增强数据集。

Abstract: While there exist strong benchmark datasets for grammatical error correction
(GEC), high-quality annotated spoken datasets for Spoken GEC (SGEC) are still
under-resourced. In this paper, we propose a fully automated method to generate
audio-text pairs with grammatical errors and disfluencies. Moreover, we propose
a series of objective metrics that can be used to evaluate the generated data
and choose the more suitable dataset for SGEC. The goal is to generate an
augmented dataset that maintains the textual and acoustic characteristics of
the original data while providing new types of errors. This augmented dataset
should augment and enrich the original corpus without altering the language
assessment scores of the second language (L2) learners. We evaluate the use of
the augmented corpus both for written GEC (the text part) and for SGEC (the
audio-text pairs). Our experiments are conducted on the S\&I Corpus, the first
publicly available speech dataset with grammar error annotations.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [205] [Transfinite Fixed Points in Alpay Algebra as Ordinal Game Equilibria in Dependent Type Theory](https://arxiv.org/abs/2507.19245)
*Faruk Alpay,Bugra Kilictas,Taylan Alpay*

Main category: cs.LO

TL;DR: 本文通过证明自指过程稳定结果与无界修正对话唯一均衡相同，为Alpay代数做贡献，还将超限不动点算子嵌入依赖类型理论获机器验证证明，建立推理无限自指系统基础。


<details>
  <summary>Details</summary>
Motivation: 为Alpay的语义收敛哲学主张在构造逻辑框架下提供基础，建立无限自指系统推理基础。

Method: 先说明经典不动点定理保证有限情形收敛，再用良基归纳和序理论连续性原理将论证扩展到超限领域，把超限不动点算子嵌入依赖类型理论。

Result: 得到机器验证证明，即迭代对话必然稳定且极限唯一。

Conclusion: 统一多领域概念，建立无限自指系统推理基础，提供计算环境中验证收敛的实用工具。

Abstract: This paper contributes to the Alpay Algebra by demonstrating that the stable
outcome of a self referential process, obtained by iterating a transformation
through all ordinal stages, is identical to the unique equilibrium of an
unbounded revision dialogue between a system and its environment. The analysis
initially elucidates how classical fixed point theorems guarantee such
convergence in finite settings and subsequently extends the argument to the
transfinite domain, relying upon well founded induction and principles of order
theoretic continuity.
  Furthermore, the resulting transordinal fixed point operator is embedded into
dependent type theory, a formalization which permits every step of the
transfinite iteration and its limit to be verified within a modern proof
assistant. This procedure yields a machine checked proof that the iterative
dialogue necessarily stabilizes and that its limit is unique. The result
provides a foundation for Alpay's philosophical claim of semantic convergence
within the framework of constructive logic. By unifying concepts from fixed
point theory, game semantics, ordinal analysis, and type theory, this research
establishes a broadly accessible yet formally rigorous foundation for reasoning
about infinite self referential systems and offers practical tools for
certifying their convergence within computational environments.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [206] [Optimizing Metachronal Paddling with Reinforcement Learning at Low Reynolds Number](https://arxiv.org/abs/2507.18849)
*Alana A. Bailey,Robert D. Guy*

Main category: physics.flu-dyn

TL;DR: 研究零雷诺数下游泳者通过强化学习选择的肢体协调模式，发现不同桨间距下有不同模式，最快泳姿与桨数量有关，最有效泳姿是后到前的波浪式。


<details>
  <summary>Details</summary>
Motivation: 探究强化学习算法是否会选择异时节律或出现其他协调模式，因为异时划水在不同雷诺数下被广泛采用，推测其具有最优游泳性能。

Method: 对零雷诺数下的游泳者应用强化学习，设计具有细长身体和不同固定桨间距的直且刚性桨的游泳者智能体。

Result: 根据桨间距，游泳者智能体学习到不同的协调模式，间距小时出现类似常见生物节律的后到前异时波浪式划水，间距大时选择不同的肢体协调方式；最快泳姿取决于桨的数量，最有效泳姿是后到前的波浪式划水。

Conclusion: 在零雷诺数下，不同桨间距会使强化学习产生不同肢体协调模式，且存在最快和最有效泳姿的规律。

Abstract: Metachronal paddling is a swimming strategy in which an organism oscillates
sets of adjacent limbs with a constant phase lag, propagating a metachronal
wave through its limbs and propelling it forward. This limb coordination
strategy is utilized by swimmers across a wide range of Reynolds numbers, which
suggests that this metachronal rhythm was selected for its optimality of
swimming performance. In this study, we apply reinforcement learning to a
swimmer at zero Reynolds number and investigate whether the learning algorithm
selects this metachronal rhythm, or if other coordination patterns emerge. We
design the swimmer agent with an elongated body and pairs of straight,
inflexible paddles placed along the body for various fixed paddle spacings.
Based on paddle spacing, the swimmer agent learns qualitatively different
coordination patterns. At tight spacings, a back-to-front metachronal wave-like
stroke emerges which resembles the commonly observed biological rhythm, but at
wide spacings, different limb coordinations are selected. Across all resulting
strokes, the fastest stroke is dependent on the number of paddles, however, the
most efficient stroke is a back-to-front wave-like stroke regardless of the
number of paddles.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [207] [Adaptive Neural Quantum States: A Recurrent Neural Network Perspective](https://arxiv.org/abs/2507.18700)
*Jake McNaughton,Mohamed Hibat-Allah*

Main category: cond-mat.dis-nn

TL;DR: 提出自适应方案优化神经网络量子态（NQS），用更少计算成本提升变分计算质量并优化GPU资源。


<details>
  <summary>Details</summary>
Motivation: NQS可系统改进，但需降低计算成本、减少训练波动和提升变分计算质量。

Method: 以循环神经网络为例，通过训练小RNN并复用其初始化大RNN来降低计算成本。

Result: 使用一小部分计算成本，减少训练波动，提升变分计算质量。

Conclusion: 该工作为大规模NQS模拟中优化GPU资源提供可能。

Abstract: Neural-network quantum states (NQS) are powerful neural-network ans\"atzes
that have emerged as promising tools for studying quantum many-body physics
through the lens of the variational principle. These architectures are known to
be systematically improvable by increasing the number of parameters. Here we
demonstrate an Adaptive scheme to optimize NQSs, through the example of
recurrent neural networks (RNN), using a fraction of the computation cost while
reducing training fluctuations and improving the quality of variational
calculations targeting ground states of prototypical models in one- and
two-spatial dimensions. This Adaptive technique reduces the computational cost
through training small RNNs and reusing them to initialize larger RNNs. This
work opens up the possibility for optimizing graphical processing unit (GPU)
resources deployed in large-scale NQS simulations.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [208] [Downward self-reducibility in the total function polynomial hierarchy](https://arxiv.org/abs/2507.19108)
*Karthik Gajulapalli,Surendra Ghentiyala,Zeyong Li,Sidhant Saraogi*

Main category: cs.CC

TL;DR: 研究总函数多项式层次中搜索问题的向下自归约性，证明有随机向下自归约的TFΣᵢᴾ问题的复杂度坍缩，并给出应用。


<details>
  <summary>Details</summary>
Motivation: 拓展Harsha等人对搜索问题向下自归约的研究，探索更一般的现象。

Method: 理论分析，证明有随机向下自归约且可访问Σᵢ₋₁ᴾ预言机的TFΣᵢᴾ问题的复杂度。

Result: TFΣᵢᴾ中可随机向下自归约的问题在PLS^Σᵢ₋₁ᴾ，有本质唯一解的在UEOPL^Σᵢ₋₁ᴾ；给出Range Avoidance和Linear Ordering Principle新上界。

Conclusion: 向下自归约在更难搜索问题中有更广泛的复杂度坍缩现象。

Abstract: A problem $\mathcal{P}$ is considered downward self-reducible, if there
exists an efficient algorithm for $\mathcal{P}$ that is allowed to make queries
to only strictly smaller instances of $\mathcal{P}$. Downward self-reducibility
has been well studied in the case of decision problems, and it is well known
that any downward self-reducible problem must lie in $\mathsf{PSPACE}$. Harsha,
Mitropolsky and Rosen [ITCS, 2023] initiated the study of downward self
reductions in the case of search problems. They showed the following
interesting collapse: if a problem is in $\mathsf{TFNP}$ and also downward
self-reducible, then it must be in $\mathsf{PLS}$. Moreover, if the problem
admits a unique solution then it must be in $\mathsf{UEOPL}$.
  We demonstrate that this represents just the tip of a much more general
phenomenon, which holds for even harder search problems that lie higher up in
the total function polynomial hierarchy ($\mathsf{TF\Sigma_i^P}$). In fact,
even if we allow our downward self-reduction to be much more powerful, such a
collapse will still occur.
  We show that any problem in $\mathsf{TF\Sigma_i^P}$ which admits a randomized
downward self-reduction with access to a $\mathsf{\Sigma_{i-1}^P}$ oracle must
be in $\mathsf{PLS}^{\mathsf{\Sigma_{i-1}^P}}$. If the problem has
\textit{essentially unique solutions} then it lies in
$\mathsf{UEOPL}^{\mathsf{\Sigma_{i-1}^P}}$.
  As one (out of many) application of our framework, we get new upper bounds
for the problems $\mathrm{Range Avoidance}$ and $\mathrm{Linear Ordering
Principle}$ and show that they are both in $\mathsf{UEOPL}^{\mathsf{NP}}$.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [209] [Financial Regulation and AI: A Faustian Bargain?](https://arxiv.org/abs/2507.18747)
*Christopher Clayton,Antonio Coppola*

Main category: econ.GN

TL;DR: 本文探讨实时预测模型融入央行宏观审慎工具包的问题，构建框架分析监管者模型选择，还引入适合金融持仓数据的深度学习架构。


<details>
  <summary>Details</summary>
Motivation: 研究是否以及如何将精细、实时的预测模型纳入央行宏观审慎工具包。

Method: 1. 构建框架分析监管者在不同模型间的权衡，推导最优政策；2. 引入图变换器这一深度学习架构处理金融持仓数据。

Result: 1. 纯预测模型可为监管者带来福利增益，预测精度和对政策干预因果影响的了解是互补的；2. 图变换器模型在样本外预测任务中达到了最先进的预测准确性。

Conclusion: 实时预测模型对央行宏观审慎监管有积极作用，图变换器模型适用于相关金融数据预测。

Abstract: We examine whether and how granular, real-time predictive models should be
integrated into central banks' macroprudential toolkit. First, we develop a
tractable framework that formalizes the tradeoff regulators face when choosing
between implementing models that forecast systemic risk accurately but have
uncertain causal content and models with the opposite profile. We derive the
regulator's optimal policy in a setting in which private portfolios react
endogenously to the regulator's model choice and policy rule. We show that even
purely predictive models can generate welfare gains for a regulator, and that
predictive precision and knowledge of causal impacts of policy interventions
are complementary. Second, we introduce a deep learning architecture tailored
to financial holdings data--a graph transformer--and we discuss why it is
optimally suited to this problem. The model learns vector embedding
representations for both assets and investors by explicitly modeling the
relational structure of holdings, and it attains state-of-the-art predictive
accuracy in out-of-sample forecasting tasks including trade prediction.

</details>


### [210] [Production Heterogeneity in Collective Labor Supply Models with Children](https://arxiv.org/abs/2507.18810)
*Charles Gauthier*

Main category: econ.GN

TL;DR: 本文通过集体劳动供给模型研究儿童福利改革成本效益，发现儿童福利生产规模报酬递减且家庭间异质性大，强调福利改革应改善儿童家庭环境。


<details>
  <summary>Details</summary>
Motivation: 政策制定者需评估儿童福利改革成本效益，但儿童福利不可观测。

Method: 采用含儿童的集体劳动供给模型，非参数部分识别面板数据中父母投入对儿童福利的影响，提出新估计策略。

Result: 儿童福利生产规模报酬递减，家庭类型间异质性大，弱势家庭儿童福利差。

Conclusion: 福利改革应包含改善儿童家庭环境的政策。

Abstract: Children welfare is at the center of many welfare reforms such as cash
transfers to families and training programs to parents. A key goal for
policy-makers is to evaluate the costs and benefits of such reforms. The main
challenge lies in that the outcome of interest, children welfare, is
unobservable. To address this issue, I consider a collective labor supply model
with children where adult members have preferences over their own leisure,
expenditures, and children welfare. I show that the model nonparametrically
partially identifies the impacts of parental inputs on children welfare in
panel data. I then propose a novel estimation strategy that accommodates
measurement error and can be used to efficiently construct valid confidence
sets. Using Dutch data on couples with children, I investigate the structure of
the expected production technology and how it varies with household
characteristics. I find that the production of children welfare is
characterized by decreasing returns to scale and large heterogeneity across
household types. In particular, I find that children from disadvantaged
households, whose parents have low education levels and are not homeowners, are
significantly worse off. My results highlight the importance for welfare
reforms to include policies targeted at improving children home environment.

</details>
