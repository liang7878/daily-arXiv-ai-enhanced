<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 55]
- [cs.CE](#cs.CE) [Total: 4]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.DS](#cs.DS) [Total: 3]
- [cs.GT](#cs.GT) [Total: 6]
- [cs.IR](#cs.IR) [Total: 7]
- [cs.LG](#cs.LG) [Total: 183]
- [cs.NE](#cs.NE) [Total: 4]
- [cs.SE](#cs.SE) [Total: 26]
- [q-fin.RM](#q-fin.RM) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 2]
- [stat.ML](#stat.ML) [Total: 16]
- [stat.CO](#stat.CO) [Total: 4]
- [cs.DM](#cs.DM) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.GR](#cs.GR) [Total: 2]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.HC](#cs.HC) [Total: 7]
- [cs.CL](#cs.CL) [Total: 24]
- [q-fin.GN](#q-fin.GN) [Total: 3]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.CG](#cs.CG) [Total: 1]
- [eess.SY](#eess.SY) [Total: 2]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [math.PR](#math.PR) [Total: 1]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.RO](#cs.RO) [Total: 6]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.MA](#cs.MA) [Total: 5]
- [cs.CV](#cs.CV) [Total: 29]
- [eess.IV](#eess.IV) [Total: 5]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.NI](#cs.NI) [Total: 3]
- [stat.ME](#stat.ME) [Total: 4]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [math.NA](#math.NA) [Total: 2]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.CY](#cs.CY) [Total: 5]
- [cs.CR](#cs.CR) [Total: 21]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [JAF: Judge Agent Forest](https://arxiv.org/abs/2601.22269)
*Sahil Garg,Brad Cheezum,Sridhar Dutta,Vishal Agarwal*

Main category: cs.AI

TL;DR: 介绍Judge Agent Forest (JAF) 框架以提升判断智能体性能，开发LSH算法并进行云误配置分类任务验证。


<details>
  <summary>Details</summary>
Motivation: 现有判断智能体多孤立评估，需提升为整体学习模式以更好发现跨实例模式和不一致性。

Method: 引入JAF框架，结合信念传播和集成学习原理；开发灵活的局部敏感哈希（LSH）算法选择多样示例。

Result: 未提及具体结果

Conclusion: 未提及明确结论

Abstract: Judge agents are fundamental to agentic AI frameworks: they provide automated evaluation, and enable iterative self-refinement of reasoning processes. We introduce JAF: Judge Agent Forest, a framework in which the judge agent conducts joint inference across a cohort of query--response pairs generated by a primary agent, rather than evaluating each in isolation. This paradigm elevates the judge from a local evaluator to a holistic learner: by simultaneously assessing related responses, the judge discerns cross-instance patterns and inconsistencies, whose aggregate feedback enables the primary agent to improve by viewing its own outputs through the judge's collective perspective.
  Conceptually, JAF bridges belief propagation and ensemble-learning principles: overlapping in-context neighborhoods induce a knowledge-graph structure that facilitates propagation of critique, and repeated, randomized evaluations yield a robust ensemble of context-sensitive judgments. JAF can be instantiated entirely via ICL, with the judge prompted for each query using its associated primary-agent response plus a small, possibly noisy set of peer exemplars. While kNN in embedding space is a natural starting point for exemplars, this approach overlooks categorical structure, domain metadata, or nuanced distinctions accessible to modern LLMs.
  To overcome these limitations, we develop a flexible locality-sensitive hashing (LSH) algorithm that learns informative binary codes by integrating semantic embeddings, LLM-driven hash predicates, supervision from categorical labels, and relevant side information. These hash codes support efficient, interpretable, and relation-aware selection of diverse exemplars, and further optimize exploration of CoT reasoning paths. We validate JAF with an empirical study on the demanding task of cloud misconfigs triage in large-scale cloud environments.

</details>


### [2] [The Six Sigma Agent: Achieving Enterprise-Grade Reliability in LLM Systems Through Consensus-Driven Decomposed Execution](https://arxiv.org/abs/2601.22290)
*Khush Patel,Siva Surendira,Jithin George,Shreyas Kapale*

Main category: cs.AI

TL;DR: 引入Six Sigma Agent架构提升大语言模型企业部署可靠性，证明新架构效果并展示收益，强调可靠性源于冗余和共识。


<details>
  <summary>Details</summary>
Motivation: 大语言模型本质是概率性的，企业部署有可靠性挑战。

Method: 引入Six Sigma Agent架构，包含任务分解、微代理采样和带动态缩放的共识投票。

Result: 实现系统误差O(p^{ceil(n/2)})，跨三个企业用例评估显示可靠性提升14700倍，成本降低80%。

Conclusion: AI系统可靠性源于原则性冗余和共识而非仅靠模型扩展。

Abstract: Large Language Models demonstrate remarkable capabilities yet remain fundamentally probabilistic, presenting critical reliability challenges for enterprise deployment. We introduce the Six Sigma Agent, a novel architecture that achieves enterprise-grade reliability through three synergistic components: (1) task decomposition into a dependency tree of atomic actions; (2) micro-agent sampling where each task is executed n times in parallel across diverse LLMs to generate independent outputs; and (3) consensus voting with dynamic scaling, clustering outputs and selecting the answer from the winning cluster with maximum votes. We prove that sampling n independent outputs with error rate p achieves system error O(p^{ceil(n/2)}), enabling exponential reliability gains. Even using cheaper models with 5% per-action error, consensus voting with 5 agents reduces error to 0.11%; dynamic scaling to 13 agents achieves 3.4 DPMO (Defects Per Million Opportunities), the Six Sigma standard. Evaluation across three enterprise use cases demonstrates a 14,700x reliability improvement over single-agent execution while reducing costs by 80%. Our work establishes that reliability in AI systems emerges from principled redundancy and consensus rather than model scaling alone.

</details>


### [3] [Why Reasoning Fails to Plan: A Planning-Centric Analysis of Long-Horizon Decision Making in LLM Agents](https://arxiv.org/abs/2601.22311)
*Zehong Wang,Fang Wu,Hongru Wang,Xiangru Tang,Bolian Li,Zhenfei Yin,Yijun Ma,Yiyang Li,Weixiang Sun,Xiusi Chen,Yanfang Ye*

Main category: cs.AI

TL;DR: 基于大语言模型的智能体在长规划中表现不佳，本文分析失败模式并提出 FLARE 方法提升性能，揭示推理和规划的区别。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的智能体在长规划中难以保持连贯行为，因分步推理的贪心策略不适合长规划。

Method: 在确定性、全结构化环境中分析推理策略的失败模式，引入 FLARE 方法实现前瞻、价值传播和有限承诺。

Result: 在多个基准测试中，FLARE 持续提升任务表现和规划行为，使 LLaMA - 8B 常能超越 GPT - 4o 的标准分步推理。

Conclusion: 推理和规划之间存在明显区别。

Abstract: Large language model (LLM)-based agents exhibit strong step-by-step reasoning capabilities over short horizons, yet often fail to sustain coherent behavior over long planning horizons. We argue that this failure reflects a fundamental mismatch: step-wise reasoning induces a form of step-wise greedy policy that is adequate for short horizons but fails in long-horizon planning, where early actions must account for delayed consequences. From this planning-centric perspective, we study LLM-based agents in deterministic, fully structured environments with explicit state transitions and evaluation signals. Our analysis reveals a core failure mode of reasoning-based policies: locally optimal choices induced by step-wise scoring lead to early myopic commitments that are systematically amplified over time and difficult to recover from. We introduce FLARE (Future-aware Lookahead with Reward Estimation) as a minimal instantiation of future-aware planning to enforce explicit lookahead, value propagation, and limited commitment in a single model, allowing downstream outcomes to influence early decisions. Across multiple benchmarks, agent frameworks, and LLM backbones, FLARE consistently improves task performance and planning-level behavior, frequently allowing LLaMA-8B with FLARE to outperform GPT-4o with standard step-by-step reasoning. These results establish a clear distinction between reasoning and planning.

</details>


### [4] [Sparks of Rationality: Do Reasoning LLMs Align with Human Judgment and Choice?](https://arxiv.org/abs/2601.22329)
*Ala N. Tak,Amin Banayeeanzade,Anahita Bolourani,Fatemeh Bahrani,Ashutosh Chaubey,Sai Praneeth Karimireddy,Norbert Schwarz,Jonathan Gratch*

Main category: cs.AI

TL;DR: 评估大语言模型在理性选择和情感影响决策方面的表现，发现提高理性的机制会放大对情感干预的敏感性，推理和情感引导存在张力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于高风险决策，需评估其是否存在类似人类的理性和偏见模式。

Method: 在理性选择基准测试和经典决策领域评估多个大语言模型，使用上下文内启动（ICP）和表征层引导（RLS）两种情感引导方法。

Result: 深思熟虑能提高模型理性，ICP 诱导的方向变化极端且难校准，RLS 模式更符合心理但可靠性低，提高理性的机制会放大对情感干预的敏感性。

Conclusion: 推理和情感引导存在张力，对人类模拟和基于大语言模型的决策系统安全部署有影响。

Abstract: Large Language Models (LLMs) are increasingly positioned as decision engines for hiring, healthcare, and economic judgment, yet real-world human judgment reflects a balance between rational deliberation and emotion-driven bias. If LLMs are to participate in high-stakes decisions or serve as models of human behavior, it is critical to assess whether they exhibit analogous patterns of (ir)rationalities and biases. To this end, we evaluate multiple LLM families on (i) benchmarks testing core axioms of rational choice and (ii) classic decision domains from behavioral economics and social norms where emotions are known to shape judgment and choice. Across settings, we show that deliberate "thinking" reliably improves rationality and pushes models toward expected-value maximization. To probe human-like affective distortions and their interaction with reasoning, we use two emotion-steering methods: in-context priming (ICP) and representation-level steering (RLS). ICP induces strong directional shifts that are often extreme and difficult to calibrate, whereas RLS produces more psychologically plausible patterns but with lower reliability. Our results suggest that the same mechanisms that improve rationality also amplify sensitivity to affective interventions, and that different steering methods trade off controllability against human-aligned behavior. Overall, this points to a tension between reasoning and affective steering, with implications for both human simulation and the safe deployment of LLM-based decision systems.

</details>


### [5] [Learning Provably Correct Distributed Protocols Without Human Knowledge](https://arxiv.org/abs/2601.22369)
*Yujie Hui,Xiaoyi Lu,Andrew Perrault,Yang Wang*

Main category: cs.AI

TL;DR: 本文提出学习框架GGMS解决分布式协议设计难题，经实验验证比现有方法能处理更大规模场景。


<details>
  <summary>Details</summary>
Motivation: 可证明正确的分布式协议设计极具挑战性，传统多智能体游戏求解方法无法学习出正确协议。

Method: 将协议设计转化为不完全信息博弈中的策略搜索问题，采用SMT指定正确性条件，提出GGMS框架，结合蒙特卡罗树搜索变体、基于Transformer的动作编码器、全局深度优先搜索和模型检查器反馈。

Result: GGMS输出的协议在有界设置下经模型检查验证正确，理论证明搜索过程完备，实验表明能为更大规模场景学习正确协议。

Conclusion: GGMS是解决分布式协议设计问题的有效方法，能处理比现有方法更大的场景。

Abstract: Provably correct distributed protocols, which are a critical component of modern distributed systems, are highly challenging to design and have often required decades of human effort. These protocols allow multiple agents to coordinate to come to a common agreement in an environment with uncertainty and failures. We formulate protocol design as a search problem over strategies in a game with imperfect information, and the desired correctness conditions are specified in Satisfiability Modulo Theories (SMT). However, standard methods for solving multi-agent games fail to learn correct protocols in this setting, even when the number of agents is small. We propose a learning framework, GGMS, which integrates a specialized variant of Monte Carlo Tree Search with a transformer-based action encoder, a global depth-first search to break out of local minima, and repeated feedback from a model checker. Protocols output by GGMS are verified correct via exhaustive model checking for all executions within the bounded setting. We further prove that, under mild assumptions, the search process is complete: if a correct protocol exists, GGMS will eventually find it. In experiments, we show that GGMS can learn correct protocols for larger settings than existing methods.

</details>


### [6] [Semi-Autonomous Mathematics Discovery with Gemini: A Case Study on the Erdős Problems](https://arxiv.org/abs/2601.22401)
*Tony Feng,Trieu Trinh,Garrett Bingham,Jiwon Kang,Shengtong Zhang,Sang-hyun Kim,Kevin Barreto,Carl Schildkraut,Junehyuk Jung,Jaehyeon Seo,Carlo Pagano,Yuri Chervonyi,Dawsen Hwang,Kaiying Hou,Sergei Gukov,Cheng-Chiang Tsai,Hyunwoo Choi,Youngbeom Jin,Wei-Yuan Li,Hao-An Wu,Ruey-An Shiu,Yu-Sheng Shih,Quoc V. Le,Thang Luong*

Main category: cs.AI

TL;DR: 使用Gemini评估Bloom的Erdős问题数据库中700个开放猜想，用混合方法解决13个问题，指出开放状态原因，讨论AI应用问题并总结经验。


<details>
  <summary>Details</summary>
Motivation: 进行半自主数学发现研究，评估Erdős问题数据库中的开放猜想。

Method: 采用混合方法，先通过AI自然语言验证缩小搜索空间，再由人类专家评估正确性和新颖性。

Result: 解决了数据库中标记为‘开放’的13个问题，5个通过新颖自主解决方案，8个通过识别现有文献中的先前解决方案。

Conclusion: 问题的‘开放’状态是由于晦涩而非困难，指出AI在数学猜想应用中的问题，如文献识别困难和‘潜意识抄袭’风险，并总结了AI辅助研究的经验。

Abstract: We present a case study in semi-autonomous mathematics discovery, using Gemini to systematically evaluate 700 conjectures labeled 'Open' in Bloom's Erdős Problems database. We employ a hybrid methodology: AI-driven natural language verification to narrow the search space, followed by human expert evaluation to gauge correctness and novelty. We address 13 problems that were marked 'Open' in the database: 5 through seemingly novel autonomous solutions, and 8 through identification of previous solutions in the existing literature. Our findings suggest that the 'Open' status of the problems was through obscurity rather than difficulty. We also identify and discuss issues arising in applying AI to math conjectures at scale, highlighting the difficulty of literature identification and the risk of ''subconscious plagiarism'' by AI. We reflect on the takeaways from AI-assisted efforts on the Erdős Problems.

</details>


### [7] [AI-Enabled Waste Classification as a Data-Driven Decision Support Tool for Circular Economy and Urban Sustainability](https://arxiv.org/abs/2601.22418)
*Julius Sechang Mboli,Omolara Aderonke Ogungbemi*

Main category: cs.AI

TL;DR: 本文评估多种机器学习和深度学习技术对垃圾图像进行二元分类，DenseNet121表现最佳，PCA对传统方法作用小，还探讨模型集成到实时决策支持系统的应用。


<details>
  <summary>Details</summary>
Motivation: 实现智慧城市中高效的垃圾分类，推动循环经济实践和资源回收。

Method: 评估传统机器学习（随机森林、SVM、AdaBoost）和深度学习技术（自定义CNN、VGG16、ResNet50等）对25077张垃圾图像进行二元分类，使用80/20的训练/测试分割，图像增强并调整为150x150像素，评估PCA对传统模型降维的影响。

Result: DenseNet121准确率达91%，ROC - AUC为0.98，比最佳传统分类器高20个百分点；PCA对传统方法益处可忽略，迁移学习在数据有限时显著提升性能。

Conclusion: 这些模型可集成到实时数据驱动决策支持系统用于自动垃圾分类，有望减少垃圾填埋和生命周期环境影响。

Abstract: Efficient waste sorting is crucial for enabling circular-economy practices and resource recovery in smart cities. This paper evaluates both traditional machine-learning (Random Forest, SVM, AdaBoost) and deep-learning techniques including custom CNNs, VGG16, ResNet50, and three transfer-learning models (DenseNet121, EfficientNetB0, InceptionV3) for binary classification of 25 077 waste images (80/20 train/test split, augmented and resized to 150x150 px). The paper assesses the impact of Principal Component Analysis for dimensionality reduction on traditional models. DenseNet121 achieved the highest accuracy (91 %) and ROC-AUC (0.98), outperforming the best traditional classifier by 20 pp. Principal Component Analysis (PCA) showed negligible benefit for classical methods, whereas transfer learning substantially improved performance under limited-data conditions. Finally, we outline how these models integrate into a real-time Data-Driven Decision Support System for automated waste sorting, highlighting potential reductions in landfill use and lifecycle environmental impacts.)

</details>


### [8] [When LLM meets Fuzzy-TOPSIS for Personnel Selection through Automated Profile Analysis](https://arxiv.org/abs/2601.22433)
*Shahria Hoque,Ahmed Akib Jawad Karim,Md. Golam Rabiul Alam,Nirjhar Gope*

Main category: cs.AI

TL;DR: 研究提出自动化人员选拔系统，结合NLP、LLMs和MCDM理论开发LLM - TOPSIS框架，实现接近专家评估的候选人排名，准确率达91%，强调其改善招聘流程潜力并指出未来方向。


<details>
  <summary>Details</summary>
Motivation: 在竞争激烈的就业环境中，为组织成功选择合适人员，改善现有招聘流程。

Method: 创建包含多特征的数据集并结合专家评估，将LLMs与MCDM理论结合开发LLM - TOPSIS框架，用模糊逻辑增强的TOPSIS方法处理评估的模糊性，用TFNs描述权重和分数，微调DistilRoBERTa模型并与模糊TOPSIS方法集成进行排名。

Result: 实现的候选人排名与人类专家评估接近，Experience和Overall属性准确率达91%。

Conclusion: NLP驱动的框架可改善招聘流程，提高可扩展性、一致性和减少偏见，未来将扩充数据集、增强模型可解释性并在实际场景验证。

Abstract: In this highly competitive employment environment, the selection of suitable personnel is essential for organizational success. This study presents an automated personnel selection system that utilizes sophisticated natural language processing (NLP) methods to assess and rank software engineering applicants. A distinctive dataset was created by aggregating LinkedIn profiles that include essential features such as education, work experience, abilities, and self-introduction, further enhanced with expert assessments to function as standards. The research combines large language models (LLMs) with multicriteria decision-making (MCDM) theory to develop the LLM-TOPSIS framework. In this context, we utilized the TOPSIS method enhanced by fuzzy logic (Fuzzy TOPSIS) to address the intrinsic ambiguity and subjectivity in human assessments. We utilized triangular fuzzy numbers (TFNs) to describe criteria weights and scores, thereby addressing the ambiguity frequently encountered in candidate evaluations. For candidate ranking, the DistilRoBERTa model was fine-tuned and integrated with the fuzzy TOPSIS method, achieving rankings closely aligned with human expert evaluations and attaining an accuracy of up to 91% for the Experience attribute and the Overall attribute. The study underlines the potential of NLP-driven frameworks to improve recruitment procedures by boosting scalability, consistency, and minimizing prejudice. Future endeavors will concentrate on augmenting the dataset, enhancing model interpretability, and verifying the system in actual recruitment scenarios to better evaluate its practical applicability. This research highlights the intriguing potential of merging NLP with fuzzy decision-making methods in personnel selection, enabling scalable and unbiased solutions to recruitment difficulties.

</details>


### [9] [Anytime Safe PAC Efficient Reasoning](https://arxiv.org/abs/2601.22446)
*Chengyao Yu,Hao Zeng,Youxin Zhu,Jianguo Huang,Huajun Zeng,Bingyi Jing*

Main category: cs.AI

TL;DR: 提出B - PAC推理方法，在部分反馈下实现安全高效在线推理，减少计算开销并控制性能损失。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型计算成本高、延迟大，现有选择性思维策略有不可控误差，尤其是在线场景。

Method: 利用逆倾向得分估计器为候选阈值构建测试上鞅，根据安全统计证据动态调整路由阈值。

Result: 实验表明B - PAC推理显著减少计算开销，思维模型使用最多降低81.01%，并将性能损失控制在用户指定水平下。

Conclusion: B - PAC推理能在部分反馈下实现随时安全高效的在线推理，有有效的性能损失控制和效率。

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex tasks but suffer from high computational costs and latency. While selective thinking strategies improve efficiency by routing easy queries to non-thinking models, existing approaches often incur uncontrollable errors, especially in online settings where the performance loss of a non-thinking model is only partially observed and data are non-stationary. To address this, we propose Betting Probably Approximately Correct (B-PAC) reasoning, a principled method that enables anytime safe and efficient online reasoning under partial feedback. Specifically, we utilize inverse propensity scoring estimators to construct test supermartingales for candidate thresholds, and then dynamically adjust the routing threshold based on the accumulated statistical evidence of safety. Theoretically, we establish the anytime-valid performance loss control and the efficiency of B-PAC reasoning. Extensive experiments demonstrate that B-PAC reasoning significantly reduces computational overhead, decreasing thinking model usage by up to 81.01\%, while controlling the performance loss below the user-specified level.

</details>


### [10] [Controllable Information Production](https://arxiv.org/abs/2601.22449)
*Tristan Shah,Stas Tiomkin*

Main category: cs.AI

TL;DR: 本文介绍了一种新的内在动机原则Controllable Information Production (CIP)，避免了外部效用和设计者指定变量，推导了其目标并展示了与外在和内在行为的联系，还验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于信息传输的信息论内在动机方法依赖设计者选择传输的随机变量，需要改进。

Method: 从最优控制中推导出CIP目标，通过开环和闭环的柯尔莫哥洛夫 - 西奈熵的差距定义CIP。

Result: 建立了CIP的关键理论性质，并在标准内在动机基准测试中证明了其有效性。

Conclusion: CIP是一种有效的避免外部效用和设计者指定变量的内在动机原则。

Abstract: Intrinsic Motivation (IM) is a paradigm for generating intelligent behavior without external utilities. The existing information-theoretic methods for IM are predominantly based on information transmission, which explicitly depends on the designer's choice of which random variables engage in transmission. In this work, we introduce a novel IM principle, Controllable Information Production (CIP), that avoids both external utilities and designer-specified variables. We derive the CIP objective from Optimal Control, showing a connection between extrinsic and intrinsic behaviors. CIP appears as the gap between open-loop and closed-loop Kolmogorov-Sinai entropies, which simultaneously rewards the pursuit and regulation of chaos. We establish key theoretical properties of CIP and demonstrate its effectiveness on standard IM benchmarks.

</details>


### [11] [Why Self-Rewarding Works: Theoretical Guarantees for Iterative Alignment of Language Models](https://arxiv.org/abs/2601.22513)
*Shi Fu,Yingjie Wang,Shengchao Hu,Peng Wang,Dacheng Tao*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Self-Rewarding Language Models (SRLMs) achieve notable success in iteratively improving alignment without external feedback. Yet, despite their striking empirical progress, the core mechanisms driving their capabilities remain unelucidated, leaving a critical gap in theoretical understanding. This paper provides the first rigorous theoretical guarantees for SRLMs. We first establish a lower bound that characterizes the fundamental limits of a single update step, revealing a critical dependence on the quality of the initial model. We then derive finite-sample error bounds for the full iterative paradigm, showing that performance improves at a rate of $\widetilde{\mathcal{O}}\left(1/\sqrt{n}\right)$ with sample size $n$. Crucially, our analysis reveals that the dependence on the initial model decays exponentially with the number of iterations $T$. This provides a formal explanation for why self-rewarding succeeds: it robustly overcomes poor initialization by steering the dynamics toward internal stability and consistency. Finally, we instantiate our theoretical framework for the linear softmax model class, yielding tailored guarantees that connect our high-level insights to practical model architectures.

</details>


### [12] [Darwinian Memory: A Training-Free Self-Regulating Memory System for GUI Agent Evolution](https://arxiv.org/abs/2601.22528)
*Hongze Mi,Yibo Feng,WenJie Lu,Song Cao,Jinyuan Li,Yanming Li,Xuelin Zhang,Haotian Luo,Songyang Peng,He Cui,Tengfei Tian,Jun Fang,Hua Chai,Naiqiang Tan*

Main category: cs.AI

TL;DR: 提出达尔文记忆系统（DMS）解决多模态大语言模型（MLLM）代理在GUI自动化长时跨应用任务中的问题，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有记忆系统难以适应动态GUI环境，存在粒度不匹配和上下文污染问题，影响MLLM代理处理长时跨应用任务。

Method: 提出DMS，将复杂轨迹分解为独立可复用单元，实现基于效用的自然选择，主动修剪次优路径和抑制高风险计划。

Result: 在真实多应用基准测试中，DMS提高了通用MLLMs的成功率和执行稳定性，降低了任务延迟，成功率平均提高18.0%，执行稳定性提高33.9%。

Conclusion: DMS是一种有效的用于GUI任务的自进化记忆系统，无需训练成本和架构开销。

Abstract: Multimodal Large Language Model (MLLM) agents facilitate Graphical User Interface (GUI) automation but struggle with long-horizon, cross-application tasks due to limited context windows. While memory systems provide a viable solution, existing paradigms struggle to adapt to dynamic GUI environments, suffering from a granularity mismatch between high-level intent and low-level execution, and context pollution where the static accumulation of outdated experiences drives agents into hallucination. To address these bottlenecks, we propose the Darwinian Memory System (DMS), a self-evolving architecture that constructs memory as a dynamic ecosystem governed by the law of survival of the fittest. DMS decomposes complex trajectories into independent, reusable units for compositional flexibility, and implements Utility-driven Natural Selection to track survival value, actively pruning suboptimal paths and inhibiting high-risk plans. This evolutionary pressure compels the agent to derive superior strategies. Extensive experiments on real-world multi-app benchmarks validate that DMS boosts general-purpose MLLMs without training costs or architectural overhead, achieving average gains of 18.0% in success rate and 33.9% in execution stability, while reducing task latency, establishing it as an effective self-evolving memory system for GUI tasks.

</details>


### [13] [Enhancing TableQA through Verifiable Reasoning Trace Reward](https://arxiv.org/abs/2601.22530)
*Tung Sum Thomas Kwok,Xinyu Wang,Hengzhi He,Xiaofeng Lin,Peng Lu,Liheng Ma,Chunhe Wang,Ying Nian Wu,Lei Ding,Guang Cheng*

Main category: cs.AI

TL;DR: 本文提出RE - Tab框架，通过奖励建模增强表格状态轨迹搜索，提升表格问答模型推理能力，降低推理成本，提高准确率且具有泛化性。


<details>
  <summary>Details</summary>
Motivation: 训练TableQA代理时，答案需通过表格状态逐步转换推理得出，存在多步推理复杂性和环境交互问题，因此研究明确的表格转换动作反馈能否提升模型推理能力。

Method: 引入RE - Tab框架，将问题建模为部分可观察马尔可夫决策过程，通过轻量级、免训练的奖励建模增强轨迹搜索。

Result: RE - Tab在TableQA中达到了最先进的性能，推理成本下降近25%，问答准确率提升达41.77%，测试时推理样本下降33.33%。

Conclusion: RE - Tab通过奖励反馈强制逐步推理，能提升表格问答模型的推理能力，且在不同大语言模型和基准测试中表现出一致性改进，具有良好的泛化性。

Abstract: A major challenge in training TableQA agents, compared to standard text- and image-based agents, is that answers cannot be inferred from a static input but must be reasoned through stepwise transformations of the table state, introducing multi-step reasoning complexity and environmental interaction. This leads to a research question: Can explicit feedback on table transformation action improve model reasoning capability? In this work, we introduce RE-Tab, a plug-and-play framework that architecturally enhances trajectory search via lightweight, training-free reward modeling by formulating the problem as a Partially Observable Markov Decision Process. We demonstrate that providing explicit verifiable rewards during State Transition (``What is the best action?'') and Simulative Reasoning (``Am I sure about the output?'') is crucial to steer the agent's navigation in table states. By enforcing stepwise reasoning with reward feedback in table transformations, RE-Tab achieves state-of-the-art performance in TableQA with almost 25\% drop in inference cost. Furthermore, a direct plug-and-play implementation of RE-Tab brings up to 41.77% improvement in QA accuracy and 33.33% drop in test-time inference samples for consistent answer. Consistent improvement pattern across various LLMs and state-of-the-art benchmarks further confirms RE-Tab's generalisability. The repository is available at https://github.com/ThomasK1018/RE_Tab .

</details>


### [14] [Decoding in Geometry: Alleviating Embedding-Space Crowding for Complex Reasoning](https://arxiv.org/abs/2601.22536)
*Yixin Yang,Qingxiu Dong,Zhifang Sui*

Main category: cs.AI

TL;DR: 本文揭示嵌入空间拥挤现象，提出CraEG采样方法，实验显示其提升生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于温度和截断的解码方法仅基于词元概率，忽略嵌入空间中词元间的细粒度关系，研究嵌入空间拥挤现象与推理成功的关系，以改进采样策略。

Method: 提出无训练、单遍且与标准采样策略兼容的CraEG方法，通过几何引导的重新加权来缓解嵌入空间拥挤。

Result: 在多个模型和基准测试中，该方法提高了生成性能，在鲁棒性和多样性指标上有提升。

Conclusion: 所提出的CraEG方法可有效提升大语言模型采样生成的性能。

Abstract: Sampling-based decoding underlies complex reasoning in large language models (LLMs), where decoding strategies critically shape model behavior. Temperature- and truncation-based methods reshape the next-token distribution through global probability reweighting or thresholding to balance the quality-diversity tradeoff. However, they operate solely on token probabilities, ignoring fine-grained relationships among tokens in the embedding space. We uncover a novel phenomenon, embedding-space crowding, where the next-token distribution concentrates its probability mass on geometrically close tokens in the embedding space. We quantify crowding at multiple granularities and find a statistical association with reasoning success in mathematical problem solving. Motivated by this finding, we propose CraEG, a plug-and-play sampling method that mitigates crowding through geometry-guided reweighting. CraEG is training-free, single-pass, and compatible with standard sampling strategies. Experiments on multiple models and benchmarks demonstrate improved generation performance, with gains in robustness and diversity metrics.

</details>


### [15] [PerfGuard: A Performance-Aware Agent for Visual Content Generation](https://arxiv.org/abs/2601.22571)
*Zhipeng Chen,Zhongrui Zhang,Chao Zhang,Yifan Xu,Lan Yang,Jun Liu,Ke Li,Yi-Zhe Song*

Main category: cs.AI

TL;DR: 现有大语言模型驱动的代理框架在工具执行假设上存在问题，本文提出PerfGuard框架解决视觉内容生成问题，实验证明其优势。


<details>
  <summary>Details</summary>
Motivation: 现有框架假设工具执行总是成功，依赖文本描述，无法区分性能边界和适应工具更新，在AIGC领域引入不确定性。

Method: 提出PerfGuard框架，包含性能感知选择建模、自适应偏好更新、能力对齐规划优化三个核心机制。

Result: 实验对比显示，PerfGuard在工具选择准确性、执行可靠性和符合用户意图方面有优势。

Conclusion: PerfGuard框架具有鲁棒性和实用性，适用于复杂AIGC任务。

Abstract: The advancement of Large Language Model (LLM)-powered agents has enabled automated task processing through reasoning and tool invocation capabilities. However, existing frameworks often operate under the idealized assumption that tool executions are invariably successful, relying solely on textual descriptions that fail to distinguish precise performance boundaries and cannot adapt to iterative tool updates. This gap introduces uncertainty in planning and execution, particularly in domains like visual content generation (AIGC), where nuanced tool performance significantly impacts outcomes. To address this, we propose PerfGuard, a performance-aware agent framework for visual content generation that systematically models tool performance boundaries and integrates them into task planning and scheduling. Our framework introduces three core mechanisms: (1) Performance-Aware Selection Modeling (PASM), which replaces generic tool descriptions with a multi-dimensional scoring system based on fine-grained performance evaluations; (2) Adaptive Preference Update (APU), which dynamically optimizes tool selection by comparing theoretical rankings with actual execution rankings; and (3) Capability-Aligned Planning Optimization (CAPO), which guides the planner to generate subtasks aligned with performance-aware strategies. Experimental comparisons against state-of-the-art methods demonstrate PerfGuard's advantages in tool selection accuracy, execution reliability, and alignment with user intent, validating its robustness and practical utility for complex AIGC tasks. The project code is available at https://github.com/FelixChan9527/PerfGuard.

</details>


### [16] [WED-Net: A Weather-Effect Disentanglement Network with Causal Augmentation for Urban Flow Prediction](https://arxiv.org/abs/2601.22586)
*Qian Hong,Siyuan Chang,Xiao Zhou*

Main category: cs.AI

TL;DR: 提出WED - Net解决极端条件下城市时空预测问题，实验证明其在极端天气下性能好。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动方法在极端条件下城市时空预测存在依赖粗粒度描述、缺乏捕捉细粒度时空效应机制，因果技术方法忽略时间动态或依赖固定混杂因素分层等问题。

Method: 提出WED - Net，通过自注意力和交叉注意力分离固有和天气引起的交通模式，引入判别器区分天气条件，设计因果数据增强策略。

Result: 在三个城市的出租车流量数据集实验中，WED - Net在极端天气条件下表现出稳健性能。

Conclusion: WED - Net有潜力支持现实场景中的安全出行、灾害准备和城市韧性。

Abstract: Urban spatio-temporal prediction under extreme conditions (e.g., heavy rain) is challenging due to event rarity and dynamics. Existing data-driven approaches that incorporate weather as auxiliary input often rely on coarse-grained descriptors and lack dedicated mechanisms to capture fine-grained spatio-temporal effects. Although recent methods adopt causal techniques to improve out-of-distribution generalization, they typically overlook temporal dynamics or depend on fixed confounder stratification. To address these limitations, we propose WED-Net (Weather-Effect Disentanglement Network), a dual-branch Transformer architecture that separates intrinsic and weather-induced traffic patterns via self- and cross-attention, enhanced with memory banks and fused through adaptive gating. To further promote disentanglement, we introduce a discriminator that explicitly distinguishes weather conditions. Additionally, we design a causal data augmentation strategy that perturbs non-causal parts while preserving causal structures, enabling improved generalization under rare scenarios. Experiments on taxi-flow datasets from three cities demonstrate that WED-Net delivers robust performance under extreme weather conditions, highlighting its potential to support safer mobility, highlighting its potential to support safer mobility, disaster preparedness, and urban resilience in real-world settings. The code is publicly available at https://github.com/HQ-LV/WED-Net.

</details>


### [17] [Learn More with Less: Uncertainty Consistency Guided Query Selection for RLVR](https://arxiv.org/abs/2601.22595)
*Hao Yi,Yulan Hu,Xin Li,Sheng Ouyang,Lizhong Ding,Yong Liu*

Main category: cs.AI

TL;DR: 本文将主动学习引入基于可验证奖励的强化学习（RLVR），提出不确定性一致性指标，有效降低推理任务的RLVR成本。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR算法查询预算大、标注成本高，探索用更少但更有信息的查询获得相似或更好性能。

Method: 引入主动学习，提出不确定性一致性指标，离线用点双列相关系数（PBC）衡量，在线提出新的变体。

Result: 实验表明该方法始终优于随机和经典主动学习基线，用30%数据训练达到全量数据性能。

Conclusion: 方法能有效降低推理任务的RLVR成本。

Abstract: Large Language Models (LLMs) have recently improved mathematical reasoning through Reinforcement Learning with Verifiable Reward (RLVR). However, existing RLVR algorithms require large query budgets, making annotation costly. We investigate whether fewer but more informative queries can yield similar or superior performance, introducing active learning (AL) into RLVR. We identify that classic AL sampling strategies fail to outperform random selection in this setting, due to ignoring objective uncertainty when only selecting by subjective uncertainty. This work proposes an uncertainty consistency metric to evaluate how well subjective uncertainty aligns with objective uncertainty. In the offline setting, this alignment is measured using the Point-Biserial Correlation Coefficient (PBC). For online training, because of limited sampling and dynamically shifting output distributions, PBC estimation is difficult. Therefore, we introduce a new online variant, computed from normalized advantage and subjective uncertainty. Theoretically, we prove that the online variant is strictly negatively correlated with offline PBC and supports better sample selection. Experiments show our method consistently outperforms random and classic AL baselines, achieving full-dataset performance while training on only 30% of the data, effectively reducing the cost of RLVR for reasoning tasks.

</details>


### [18] [From Self-Evolving Synthetic Data to Verifiable-Reward RL: Post-Training Multi-turn Interactive Tool-Using Agents](https://arxiv.org/abs/2601.22607)
*Jiaxuan Gao,Jiaao Chen,Chuyi He,Wei-Chen Wang,Shusheng Xu,Hanrui Wang,Di Jin,Yi Wu*

Main category: cs.AI

TL;DR: 提出结合自进化数据代理和基于验证器的强化学习统一框架EigenData，在基准测试中表现良好，为引导复杂工具使用行为提供可扩展途径。


<details>
  <summary>Details</summary>
Motivation: 交互式工具使用代理的后训练具有挑战性，高质量多轮工具使用数据合成难扩展，强化学习有噪声信号问题。

Method: 提出统一框架，EigenData合成工具相关对话和检查器，通过闭环自进化过程提高生成可靠性；基于合成数据开发强化学习方法。

Result: 在tau^2 - bench上测试，最佳模型在Airline和Telecom任务有高通过率，表现匹配或超越前沿模型。

Conclusion: 为引导复杂工具使用行为提供了无需昂贵人工标注的可扩展途径。

Abstract: Interactive tool-using agents must solve real-world tasks via multi-turn interaction with both humans and external environments, requiring dialogue state tracking, multi-step tool execution, while following complex instructions. Post-training such agents is challenging because synthesis for high-quality multi-turn tool-use data is difficult to scale, and reinforcement learning (RL) could face noisy signals caused by user simulation, leading to degraded training efficiency. We propose a unified framework that combines a self-evolving data agent with verifier-based RL. Our system, EigenData, is a hierarchical multi-agent engine that synthesizes tool-grounded dialogues together with executable per-instance checkers, and improves generation reliability via closed-loop self-evolving process that updates prompts and workflow. Building on the synthetic data, we develop an RL recipe that first fine-tunes the user model and then applies GRPO-style training with trajectory-level group-relative advantages and dynamic filtering, yielding consistent improvements beyond SFT. Evaluated on tau^2-bench, our best model reaches 73.0% pass^1 on Airline and 98.3% pass^1 on Telecom, matching or exceeding frontier models. Overall, our results suggest a scalable pathway for bootstrapping complex tool-using behaviors without expensive human annotation.

</details>


### [19] [EntroCut: Entropy-Guided Adaptive Truncation for Efficient Chain-of-Thought Reasoning in Small-scale Large Reasoning Models](https://arxiv.org/abs/2601.22617)
*Hongxi Yan,Qingjie Liu,Yunhong Wang*

Main category: cs.AI

TL;DR: 提出无训练方法EntroCut动态截断推理，引入EPR指标评估，实验显示可减少40% token使用且精度损失小，证明熵引导截断可缓解LRMs低效问题。


<details>
  <summary>Details</summary>
Motivation: Large Reasoning Models依赖长中间步骤推理计算成本高，需提高推理效率。

Method: 提出无训练方法EntroCut，通过识别高置信状态动态截断推理；引入EPR指标评估效率与精度权衡。

Result: 在四个基准测试上，EntroCut最多减少40%的token使用，精度牺牲极小，相比现有无训练方法实现了更优的效率 - 性能权衡。

Conclusion: 熵引导的动态截断为缓解LRMs的低效率提供了实用方法。

Abstract: Large Reasoning Models (LRMs) excel at complex reasoning tasks through extended chain-of-thought generation, but their reliance on lengthy intermediate steps incurs substantial computational cost. We find that the entropy of the model's output distribution in early reasoning steps reliably distinguishes correct from incorrect reasoning. Motivated by this observation, we propose EntroCut, a training-free method that dynamically truncates reasoning by identifying high-confidence states where reasoning can be safely terminated. To comprehensively evaluate the trade-off between efficiency and accuracy, we introduce the Efficiency-Performance Ratio (EPR), a unified metric that quantifies relative token savings per unit accuracy loss. Experiments on four benchmarks show that EntroCut reduces token usage by up to 40\% with minimal accuracy sacrifice, achieving superior efficiency-performance trade-offs compared with existing training-free methods. These results demonstrate that entropy-guided dynamic truncation provides a practical approach to mitigate the inefficiency of LRMs.

</details>


### [20] [SYMPHONY: Synergistic Multi-agent Planning with Heterogeneous Language Model Assembly](https://arxiv.org/abs/2601.22623)
*Wei Zhu,Zhiwen Tang,Kun Yue*

Main category: cs.AI

TL;DR: 本文提出了一种新的多智能体规划框架SYMPHONY，通过集成异构语言模型智能体提升蒙特卡洛树搜索规划性能，在多个基准任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型构建自主智能体的方法多采用单智能体框架进行蒙特卡洛树搜索规划，存在探索能力受限、规划性能不佳的问题。

Method: 提出SYMPHONY框架，集成多个基于异构语言模型的智能体，利用不同智能体的推理模式增强搜索分支的多样性。

Result: 在多个基准任务中，使用可在消费级硬件上部署的开源大语言模型时SYMPHONY表现良好；使用基于API的云大语言模型时，进一步提升性能，超越现有最优基线。

Conclusion: 异构多智能体协作在规划任务中是有效的。

Abstract: Recent advancements have increasingly focused on leveraging large language models (LLMs) to construct autonomous agents for complex problem-solving tasks. However, existing approaches predominantly employ a single-agent framework to generate search branches and estimate rewards during Monte Carlo Tree Search (MCTS) planning. This single-agent paradigm inherently limits exploration capabilities, often resulting in insufficient diversity among generated branches and suboptimal planning performance. To overcome these limitations, we propose Synergistic Multi-agent Planning with Heterogeneous langauge model assembly (SYMPHONY), a novel multi-agent planning framework that integrates a pool of heterogeneous language model-based agents. By leveraging diverse reasoning patterns across agents, SYMPHONY enhances rollout diversity and facilitates more effective exploration. Empirical results across multiple benchmark tasks show that SYMPHONY achieves strong performance even when instantiated with open-source LLMs deployable on consumer-grade hardware. When enhanced with cloud-based LLMs accessible via API, SYMPHONY demonstrates further improvements, outperforming existing state-of-the-art baselines and underscoring the effectiveness of heterogeneous multi-agent coordination in planning tasks.

</details>


### [21] [Statistical Estimation of Adversarial Risk in Large Language Models under Best-of-N Sampling](https://arxiv.org/abs/2601.22636)
*Mingqian Feng,Xiaodong Liu,Weiwei Yang,Chenliang Xu,Christopher White,Jianfeng Gao*

Main category: cs.AI

TL;DR: 提出SABER方法用于评估大语言模型越狱漏洞风险，能以低成本实现更现实的安全评估。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型安全评估方式低估现实风险，预测大规模对抗风险的原则性方法有限。

Method: 提出SABER方法，用Beta分布建模样本级成功概率，推导分析缩放定律。

Result: 仅用100个样本，锚定估计器预测ASR@1000的平均绝对误差为1.66，比基线降低86.2%。

Conclusion: 该工作提供了低成本、可扩展的现实大语言模型安全评估方法。

Abstract: Large Language Models (LLMs) are typically evaluated for safety under single-shot or low-budget adversarial prompting, which underestimates real-world risk. In practice, attackers can exploit large-scale parallel sampling to repeatedly probe a model until a harmful response is produced. While recent work shows that attack success increases with repeated sampling, principled methods for predicting large-scale adversarial risk remain limited. We propose a scaling-aware Best-of-N estimation of risk, SABER, for modeling jailbreak vulnerability under Best-of-N sampling. We model sample-level success probabilities using a Beta distribution, the conjugate prior of the Bernoulli distribution, and derive an analytic scaling law that enables reliable extrapolation of large-N attack success rates from small-budget measurements. Using only n=100 samples, our anchored estimator predicts ASR@1000 with a mean absolute error of 1.66, compared to 12.04 for the baseline, which is an 86.2% reduction in estimation error. Our results reveal heterogeneous risk scaling profiles and show that models appearing robust under standard evaluation can experience rapid nonlinear risk amplification under parallel adversarial pressure. This work provides a low-cost, scalable methodology for realistic LLM safety assessment. We will release our code and evaluation scripts upon publication to future research.

</details>


### [22] [Beyond Medical Chatbots: Meddollina and the Rise of Continuous Clinical Intelligence](https://arxiv.org/abs/2601.22645)
*Vaibhav Ram S. V. N. S,Swetanshu Agrawal,Samudra Banerjee,Abdul Muhsin*

Main category: cs.AI

TL;DR: 生成式医疗AI虽表现良好，但有不适合临床部署的问题，本文提出临床上下文智能（CCI），介绍治理优先的临床智能系统Meddollina并评估，结果显示它行为表现好，表明可部署医疗AI不能仅靠扩展规模。


<details>
  <summary>Details</summary>
Motivation: 生成式医疗AI虽看似有临床智能，但存在不适合临床部署的行为，需探索适合临床使用的医疗AI。

Method: 定义临床上下文智能（CCI），引入治理优先的临床智能系统Meddollina，用行为优先机制评估Meddollina并与其他模型对比。

Result: Meddollina有校准不确定性、保守推理等独特行为表现。

Conclusion: 可部署的医疗AI不能仅靠扩展规模，应转向持续临床智能，以不确定下符合临床医生行为衡量进步。

Abstract: Generative medical AI now appears fluent and knowledgeable enough to resemble clinical intelligence, encouraging the belief that scaling will make it safe. But clinical reasoning is not text generation. It is a responsibility-bound process under ambiguity, incomplete evidence, and longitudinal context. Even as benchmark scores rise, generation-centric systems still show behaviours incompatible with clinical deployment: premature closure, unjustified certainty, intent drift, and instability across multi-step decisions.
  We argue these are structural consequences of treating medicine as next-token prediction. We formalise Clinical Contextual Intelligence (CCI) as a distinct capability class required for real-world clinical use, defined by persistent context awareness, intent preservation, bounded inference, and principled deferral when evidence is insufficient.
  We introduce Meddollina, a governance-first clinical intelligence system designed to constrain inference before language realisation, prioritising clinical appropriateness over generative completeness. Meddollina acts as a continuous intelligence layer supporting clinical workflows while preserving clinician authority. We evaluate Meddollina using a behaviour-first regime across 16,412+ heterogeneous medical queries, benchmarking against general-purpose models, medical-tuned models, and retrieval-augmented systems.
  Meddollina exhibits a distinct behavioural profile: calibrated uncertainty, conservative reasoning under underspecification, stable longitudinal constraint adherence, and reduced speculative completion relative to generation-centric baselines. These results suggest deployable medical AI will not emerge from scaling alone, motivating a shift toward Continuous Clinical Intelligence, where progress is measured by clinician-aligned behaviour under uncertainty rather than fluency-driven completion.

</details>


### [23] [Test-Time Mixture of World Models for Embodied Agents in Dynamic Environments](https://arxiv.org/abs/2601.22647)
*Jinwoo Jang,Minjong Yoo,Sihyung Yoon,Honguk Woo*

Main category: cs.AI

TL;DR: 本文提出TMoW框架，通过在测试时更新世界模型的路由功能，提升具身智能体在动态环境中的适应性，并在多个基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 基于语言模型的具身智能体在动态环境中适应性有限，构建准确灵活的世界模型对推理和决策至关重要，需解决适应性问题。

Method: 提出Test-time Mixture of World Models (TMoW)框架，通过多粒度基于原型的路由、测试时细化和基于蒸馏混合的增强等方法实现持续适应。

Result: 在VirtualHome、ALFWorld和RLBench基准测试中，TMoW在零样本适应和少样本扩展场景中表现出色。

Conclusion: TMoW框架使具身智能体能够在动态环境中有效运行。

Abstract: Language model (LM)-based embodied agents are increasingly deployed in real-world settings. Yet, their adaptability remains limited in dynamic environments, where constructing accurate and flexible world models is crucial for effective reasoning and decision-making. To address this challenge, we extend the Mixture-of-Experts (MoE) paradigm to embodied agents. While conventional MoE architectures modularize knowledge into expert components with pre-trained routing, they remain rigid once deployed, making them less effective for adapting to unseen domains in dynamic environments. We therefore propose Test-time Mixture of World Models (TMoW), a framework that enhances adaptability to unseen and evolving domains. TMoW updates its routing function over world models at test time, unlike conventional MoE where the function remains fixed, enabling agents to recombine existing models and integrate new ones for continual adaptation. It achieves this through (i) multi-granular prototype-based routing, which adapts mixtures across object- to scene-level similarities, (ii) test-time refinement that aligns unseen domain features with prototypes during inference, and (iii) distilled mixture-based augmentation, which efficiently constructs new models from few-shot data and existing prototypes. We evaluate TMoW on VirtualHome, ALFWorld, and RLBench benchmarks, demonstrating strong performance in both zero-shot adaptation and few-shot expansion scenarios, and showing that it enables embodied agents to operate effectively in dynamic environments.

</details>


### [24] [UCPO: Uncertainty-Aware Policy Optimization](https://arxiv.org/abs/2601.22648)
*Xianzhou Zeng,Jing Huang,Chunmei Xie,Gongrui Nan,Siye Chen,Mengyu Lu,Weiqi Xiong,Qixuan Zhou,Junhao Zhang,Qiang Zhu,Yadong Li,Xingzhong Xu*

Main category: cs.AI

TL;DR: 现有RL范式有优势偏差问题，本文提出UCPO框架，通过TAD和DURA消除偏差并调整奖励，实验显示其有效。


<details>
  <summary>Details</summary>
Motivation: 现有RL范式存在优势偏差，导致过度保守或过度自信问题，影响大语言模型构建可信赖性。

Method: 提出UnCertainty - Aware Policy Optimization (UCPO)框架，采用Ternary Advantage Decoupling消除优势偏差，引入Dynamic Uncertainty Reward Adjustment机制实时校准不确定性权重。

Result: 在数学推理和一般任务实验中，UCPO有效解决奖励不平衡问题。

Conclusion: UCPO能显著提高模型在知识边界外的可靠性和校准性。

Abstract: The key to building trustworthy Large Language Models (LLMs) lies in endowing them with inherent uncertainty expression capabilities to mitigate the hallucinations that restrict their high-stakes applications. However, existing RL paradigms such as GRPO often suffer from Advantage Bias due to binary decision spaces and static uncertainty rewards, inducing either excessive conservatism or overconfidence. To tackle this challenge, this paper unveils the root causes of reward hacking and overconfidence in current RL paradigms incorporating uncertainty-based rewards, based on which we propose the UnCertainty-Aware Policy Optimization (UCPO) framework. UCPO employs Ternary Advantage Decoupling to separate and independently normalize deterministic and uncertain rollouts, thereby eliminating advantage bias. Furthermore, a Dynamic Uncertainty Reward Adjustment mechanism is introduced to calibrate uncertainty weights in real-time according to model evolution and instance difficulty. Experimental results in mathematical reasoning and general tasks demonstrate that UCPO effectively resolves the reward imbalance, significantly improving the reliability and calibration of the model beyond their knowledge boundaries.

</details>


### [25] [Task-Aware LLM Council with Adaptive Decision Pathways for Decision Support](https://arxiv.org/abs/2601.22662)
*Wei Zhu,Lixing Yu,Hao-Ren Yao,Zhiwen Tang,Kun Yue*

Main category: cs.AI

TL;DR: 提出任务感知的LLM委员会TALC框架，结合LLM和MCTS实现动态专家选择和多步规划，实验显示其成功率和搜索效率优于基线


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略LLM专长差异，难以适应不同推理需求和任务复杂度

Method: 提出TALC框架，用MCTS，给LLM配备成功记忆，在决策点路由到合适模型，用双信号机制估计节点值，并自适应加权引导选择

Result: 在WebShop、HumanEval和24点游戏上，TALC任务成功率和搜索效率优于强基线

Conclusion: TALC体现了专业化感知路由和自适应规划的优势

Abstract: Large language models (LLMs) have shown strong capabilities across diverse decision-making tasks. However, existing approaches often overlook the specialization differences among available models, treating all LLMs as uniformly applicable regardless of task characteristics. This limits their ability to adapt to varying reasoning demands and task complexities. In this work, we propose Task-Aware LLM Council (TALC), a task-adaptive decision framework that integrates a council of LLMs with Monte Carlo Tree Search (MCTS) to enable dynamic expert selection and efficient multi-step planning. Each LLM is equipped with a structured success memory profile derived from prior task trajectories, enabling semantic matching between current reasoning context and past successes. At each decision point, TALC routes control to the most contextually appropriate model and estimates node value using a dual-signal mechanism that fuses model-based evaluations with historical utility scores. These signals are adaptively weighted based on intra-node variance and used to guide MCTS selection, allowing the system to balance exploration depth with planning confidence. Experiments on WebShop, HumanEval, and the Game of 24 demonstrate that TALC achieves superior task success rates and improved search efficiency compared to strong baselines, validating the benefits of specialization-aware routing and adaptive planning.

</details>


### [26] [Real-Time Aligned Reward Model beyond Semantics](https://arxiv.org/abs/2601.22664)
*Zixuan Huang,Xin Xia,Yuxi Ren,Jianbin Zheng,Xuefeng Xiao,Hongyan Xie,Li Huaqiu,Songshi Liang,Zhongxiang Dai,Fuzhen Zhuang,Jianxin Li,Yikun Ban,Deqing Wang*

Main category: cs.AI

TL;DR: 传统RLHF易出现奖励过优化问题，现有缓解方法有局限。本文提出R2M框架，利用策略反馈实时对齐策略分布转移，为提升奖励模型性能指明新方向。


<details>
  <summary>Details</summary>
Motivation: 解决传统RLHF中奖励过优化问题，以及现有缓解方法无法有效处理奖励模型和策略模型因策略分布持续转移导致的不一致问题。

Method: 引入R2M框架，该框架超越仅依赖预训练大语言模型语义表示的普通奖励模型，利用策略的隐藏状态（策略反馈）在强化学习过程中实时对齐策略分布转移。

Result: 文中未明确提及具体实验结果。

Conclusion: 通过实时利用策略模型的反馈来提升奖励模型性能是一个有前景的新方向。

Abstract: Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique for aligning large language models (LLMs) with human preferences, yet it is susceptible to reward overoptimization, in which policy models overfit to the reward model, exploit spurious reward patterns instead of faithfully capturing human intent. Prior mitigations primarily relies on surface semantic information and fails to efficiently address the misalignment between the reward model (RM) and the policy model caused by continuous policy distribution shifts. This inevitably leads to an increasing reward discrepancy, exacerbating reward overoptimization. To address these limitations, we introduce R2M (Real-Time Aligned Reward Model), a novel lightweight RLHF framework. R2M goes beyond vanilla reward models that solely depend on the semantic representations of a pretrained LLM. Instead, it leverages the evolving hidden states of the policy (namely policy feedback) to align with the real-time distribution shift of the policy during the RL process. This work points to a promising new direction for improving the performance of reward models through real-time utilization of feedback from policy models.

</details>


### [27] [Best-of-Q: Improving VLM agents with Q-function Action Ranking at Inference](https://arxiv.org/abs/2601.22701)
*Emilien Biré,María Santos,Kai Yuan*

Main category: cs.AI

TL;DR: 提出一种在推理时增强视觉语言模型（VLM）策略的新范式，无需重新训练策略，在WebVoyager基准上提升了代理成功率。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在快速变化环境中适应性差，微调需大量模型训练和数据收集，因此要找到无需重新训练策略就能增强VLM策略的方法。

Method: 将VLM作为高容量动作提议者的角色与最终动作选择机制分离，冻结VLM策略，用其生成候选动作，再用轻量级离线训练的Q函数对候选动作重新排序，执行估计值最高的动作。

Result: 在WebVoyager基准上，该方法显著提高了代理成功率，Qwen2.5 - VL - 7B代理从38.8%提升到55.7%，GPT - 4.1代理从82.4%提升到88.8%。

Conclusion: 直接在推理时应用Q函数可实现即时的策略改进，无需离线重新标记数据进行策略再训练。

Abstract: Vision-Language Models (VLMs) have become powerful backbones for agents to autonomously operate in digital environments like the web and operating systems. However, these models suffer from inadaptability to fast-changing environments like the web, which can be alleviated by fine-tuning requiring expansive model training and data collection. In this work, we introduce a novel paradigm for enhancing agentic VLM policies at inference without policy retraining. Fundamentally, our approach decouples the VLM's role as a high-capacity action proposer from the final action selection mechanism. We keep the VLM policy frozen and use it to generate a set of candidate actions for a given state. Then, a lightweight, offline-trained Q-function reranks these candidates, and the agent executes the action with the highest estimated value. The main contribution is to apply the Q-function directly during inference for immediate policy improvement, and not offline to relabel data for policy retraining. We demonstrate on the academic WebVoyager benchmark that our method significantly boosts agent success rates, improving a Qwen2.5-VL-7B agent from 38.8% to 55.7% and a proprietary GPT-4.1 agent from 82.4% to 88.8%.

</details>


### [28] [A Step Back: Prefix Importance Ratio Stabilizes Policy Optimization](https://arxiv.org/abs/2601.22718)
*Shiye Lei,Zhihao Cheng,Dacheng Tao*

Main category: cs.AI

TL;DR: 本文指出大语言模型强化学习微调中token级重要性采样比在离策略程度大时导致训练不稳定，提出MinPRO目标函数，实验证明其提升了训练稳定性和峰值性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于token级重要性采样比的强化学习目标在离策略程度大时导致训练动态不稳定，需稳定大语言模型在大离策略漂移下的优化。

Method: 提出Minimum Prefix Ratio (MinPRO)目标函数，用基于前序前缀中最小token级比例的非累积替代项替换不稳定的累积前缀比例。

Result: 在多种数学推理基准测试的密集型和专家混合大语言模型上实验，MinPRO显著提高了离策略机制下的训练稳定性和峰值性能。

Conclusion: MinPRO能有效解决大语言模型强化学习微调中离策略程度大时的训练不稳定问题。

Abstract: Reinforcement learning (RL) post-training has increasingly demonstrated strong ability to elicit reasoning behaviors in large language models (LLMs). For training efficiency, rollouts are typically generated in an off-policy manner using an older sampling policy and then used to update the current target policy. To correct the resulting discrepancy between the sampling and target policies, most existing RL objectives rely on a token-level importance sampling ratio, primarily due to its computational simplicity and numerical stability. However, we observe that token-level correction often leads to unstable training dynamics when the degree of off-policyness is large. In this paper, we revisit LLM policy optimization under off-policy conditions and show that the theoretically rigorous correction term is the prefix importance ratio, and that relaxing it to a token-level approximation can induce instability in RL post-training. To stabilize LLM optimization under large off-policy drift, we propose a simple yet effective objective, Minimum Prefix Ratio (MinPRO). MinPRO replaces the unstable cumulative prefix ratio with a non-cumulative surrogate based on the minimum token-level ratio observed in the preceding prefix. Extensive experiments on both dense and mixture-of-experts LLMs, across multiple mathematical reasoning benchmarks, demonstrate that MinPRO substantially improves training stability and peak performance in off-policy regimes.

</details>


### [29] [AutoRefine: From Trajectories to Reusable Expertise for Continual LLM Agent Refinement](https://arxiv.org/abs/2601.22758)
*Libin Qiu,Zhirong Gao,Junfu Chen,Yuhang Ye,Weizhi Huang,Xiaobo Xue,Wenkai Qiu,Shuo Tang*

Main category: cs.AI

TL;DR: 介绍AutoRefine框架，从代理执行历史中提取和维护双形式经验模式，在多任务评估中表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型代理无法积累经验、现有方法不能捕捉复杂子任务程序逻辑且缺乏维护机制的问题。

Method: 引入AutoRefine框架，为程序性子任务提取专业子代理，为静态知识提取技能模式，并采用连续维护机制。

Result: 在ALFWorld、ScienceWorld和TravelPlanner上分别达到98.4%、70.4%和27.1%的成绩，减少20 - 73%的步骤，在TravelPlanner上自动提取效果超手动设计系统。

Conclusion: AutoRefine框架能有效提取和维护经验模式，具备捕捉程序协调的能力。

Abstract: Large language model agents often fail to accumulate knowledge from experience, treating each task as an independent challenge. Recent methods extract experience as flattened textual knowledge, which cannot capture procedural logic of complex subtasks. They also lack maintenance mechanisms, causing repository degradation as experience accumulates. We introduce AutoRefine, a framework that extracts and maintains dual-form Experience Patterns from agent execution histories. For procedural subtasks, we extract specialized subagents with independent reasoning and memory. For static knowledge, we extract skill patterns as guidelines or code snippets. A continuous maintenance mechanism scores, prunes, and merges patterns to prevent repository degradation. Evaluated on ALFWorld, ScienceWorld, and TravelPlanner, AutoRefine achieves 98.4%, 70.4%, and 27.1% respectively, with 20-73% step reductions. On TravelPlanner, automatic extraction exceeds manually designed systems (27.1% vs 12.1%), demonstrating its ability to capture procedural coordination.

</details>


### [30] [TSPO: Breaking the Double Homogenization Dilemma in Multi-turn Search Policy Optimization](https://arxiv.org/abs/2601.22776)
*Shichao Ma,Zhiyuan Ma,Ming Yang,Xiaofan Li,Xing Wu,Jintao Du,Yu Cheng,Weiqiang Wang,Qiliang Liu,Zhengyang Zhou,Yang Wang*

Main category: cs.AI

TL;DR: 当前基于稀疏结果奖励的搜索增强推理RL框架存在‘双重同质化困境’，本文提出TSPO方法解决该问题，实验显示其显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前搜索增强推理的强化学习框架主要依赖稀疏结果奖励，导致‘双重同质化困境’，需要改进。

Method: 提出Turn - level Stage - aware Policy Optimization (TSPO)，引入First - Occurrence Latent Reward (FOLR)机制，在正确答案首次出现步骤分配部分奖励。

Result: TSPO在Qwen2.5 - 3B和7B模型上分别实现24%和13.6%的平均性能提升，显著优于现有基线模型。

Conclusion: TSPO能有效解决搜索增强推理中基于稀疏结果奖励的RL框架的‘双重同质化困境’，提升模型性能。

Abstract: Multi-turn tool-integrated reasoning enables Large Language Models (LLMs) to solve complex tasks through iterative information retrieval. However, current reinforcement learning (RL) frameworks for search-augmented reasoning predominantly rely on sparse outcome-level rewards, leading to a "Double Homogenization Dilemma." This manifests as (1) Process homogenization, where the thinking, reasoning, and tooling involved in generation are ignored. (2) Intra-group homogenization, coarse-grained outcome rewards often lead to inefficiencies in intra-group advantage estimation with methods like Group Relative Policy Optimization (GRPO) during sampling. To address this, we propose Turn-level Stage-aware Policy Optimization (TSPO). TSPO introduces the First-Occurrence Latent Reward (FOLR) mechanism, allocating partial rewards to the step where the ground-truth answer first appears, thereby preserving process-level signals and increasing reward variance within groups without requiring external reward models or any annotations. Extensive experiments demonstrate that TSPO significantly outperforms state-of-the-art baselines, achieving average performance gains of 24% and 13.6% on Qwen2.5-3B and 7B models, respectively.

</details>


### [31] [Learning with Challenges: Adaptive Difficulty-Aware Data Generation for Mobile GUI Agent Training](https://arxiv.org/abs/2601.22781)
*Linjia Kang,Zhimin Wang,Yongkang Zhang,Duo Wu,Jinghe Wang,Ming Ma,Haopeng Yan,Zhi Wang*

Main category: cs.AI

TL;DR: 提出MobileGen框架解决现有GUI轨迹生成方法缺乏细粒度任务难度控制问题，提升GUI智能体性能


<details>
  <summary>Details</summary>
Motivation: 现有方法生成GUI轨迹时缺乏对任务难度的细粒度控制，导致训练难度与智能体能力不匹配，限制学习效果

Method: 将任务难度解耦为结构和语义维度，在预设数据集上评估智能体构建能力边界，自适应计算任务难度的概率分布，采样目标难度，用多智能体可控生成器合成轨迹和任务指令

Result: 在多个具有挑战性的基准测试中，MobileGen使GUI智能体的平均性能提高了1.57倍，始终优于现有数据生成方法

Conclusion: 能力对齐的数据生成对有效训练移动GUI智能体非常重要

Abstract: Large-scale, high-quality interaction trajectories are essential for advancing mobile Graphical User Interface (GUI) agents. While existing methods typically rely on labor-intensive human demonstrations or automated model exploration to generate GUI trajectories, they lack fine-grained control over task difficulty. This fundamentally restricts learning effectiveness due to the mismatch between the training difficulty and the agent's capabilities. Inspired by how humans acquire skills through progressively challenging tasks, we propose MobileGen, a novel data generation framework that adaptively aligns training difficulty with the GUI agent's capability frontier. Specifically, MobileGen explicitly decouples task difficulty into structural (e.g., trajectory length) and semantic (e.g., task goal) dimensions. It then iteratively evaluates the agent on a curated prior dataset to construct a systematic profile of its capability frontier across these two dimensions. With this profile, the probability distribution of task difficulty is adaptively computed, from which the target difficulty for the next round of training can be sampled. Guided by the sampled difficulty, a multi-agent controllable generator is finally used to synthesize high-quality interaction trajectories along with corresponding task instructions. Extensive experiments show that MobileGen consistently outperforms existing data generation methods by improving the average performance of GUI agents by 1.57 times across multiple challenging benchmarks. This highlights the importance of capability-aligned data generation for effective mobile GUI agent training.

</details>


### [32] [Toward IIT-Inspired Consciousness in LLMs: A Reward-Based Learning Framework](https://arxiv.org/abs/2601.22786)
*Hamid Reza Akbari,Mohammad Hossein Sameti,Amir M. Mansourian,Mohammad Hossein Rohban,Hossein Sameti*

Main category: cs.AI

TL;DR: 本文通过基于奖励的学习范式将意识综合信息理论（IIT）应用于语言模型，提出新奖励函数，优化后文本生成更简洁，该框架有诸多实用优势。


<details>
  <summary>Details</summary>
Motivation: 追求通用人工智能（AGI），意识处理可作为关键促进因素，研究在语言模型中实现意识理论，以提升语言模型性能。

Method: 通过基于奖励的学习范式在语言模型中实现IIT，根据其核心原则制定量化文本因果性、连贯性和整合性的奖励函数。

Result: 优化IIT启发的奖励可使文本生成更简洁，在域外任务上仔细调整可减少31%输出长度，且保持与基础模型相当的准确性，还分析了训练方法对模型校准和计算扩展的影响。

Conclusion: 提出的框架概念简单、计算高效，无需外部数据或辅助模型，利用通用能力驱动信号，有显著实际优势。

Abstract: The pursuit of Artificial General Intelligence (AGI) is a central goal in language model development, in which consciousness-like processing could serve as a key facilitator. While current language models are not conscious, they exhibit behaviors analogous to certain aspects of consciousness. This paper investigates the implementation of a leading theory of consciousness, Integrated Information Theory (IIT), within language models via a reward-based learning paradigm. IIT provides a formal, axiom-based mathematical framework for quantifying consciousness. Drawing inspiration from its core principles, we formulate a novel reward function that quantifies a text's causality, coherence and integration, characteristics associated with conscious processing. Empirically, it is found that optimizing for this IIT-inspired reward leads to more concise text generation. On out of domain tasks, careful tuning achieves up to a 31% reduction in output length while preserving accuracy levels comparable to the base model. In addition to primary task performance, the broader effects of this training methodology on the model's confidence calibration and test-time computational scaling is analyzed. The proposed framework offers significant practical advantages: it is conceptually simple, computationally efficient, requires no external data or auxiliary models, and leverages a general, capability-driven signal rather than task-specific heuristics. Code available at https://github.com/MH-Sameti/LLM_PostTraining.git

</details>


### [33] [Conditional Performance Guarantee for Large Reasoning Models](https://arxiv.org/abs/2601.22790)
*Jianguo Huang,Hao Zeng,Bingyi Jing,Hongxin Wei,Bo An*

Main category: cs.AI

TL;DR: 提出G - PAC推理框架解决大推理模型计算成本高问题，实验证明其能实现组条件风险控制并节省计算成本。


<details>
  <summary>Details</summary>
Motivation: 大推理模型计算成本高，PAC推理仅在边际情况有保证且无精确条件覆盖。

Method: 提出G - PAC推理框架，包括针对已知组结构的G - PAC推理和未知分组的C - PAC推理。

Result: 证明G - PAC和C - PAC能实现组条件风险控制，分组在异构环境下比边际PAC推理更高效。

Conclusion: G - PAC和C - PAC在多样推理基准测试中成功实现组条件风险控制，同时大幅节省计算成本。

Abstract: Large reasoning models have shown strong performance through extended chain-of-thought reasoning, yet their computational cost remains significant. Probably approximately correct (PAC) reasoning provides statistical guarantees for efficient reasoning by adaptively switching between thinking and non-thinking models, but the guarantee holds only in the marginal case and does not provide exact conditional coverage. We propose G-PAC reasoning, a practical framework that provides PAC-style guarantees at the group level by partitioning the input space. We develop two instantiations: Group PAC (G-PAC) reasoning for known group structures and Clustered PAC (C-PAC) reasoning for unknown groupings. We prove that both G-PAC and C-PAC achieve group-conditional risk control, and that grouping can strictly improve efficiency over marginal PAC reasoning in heterogeneous settings. Our experiments on diverse reasoning benchmarks demonstrate that G-PAC and C-PAC successfully achieve group-conditional risk control while maintaining substantial computational savings.

</details>


### [34] [CVeDRL: An Efficient Code Verifier via Difficulty-aware Reinforcement Learning](https://arxiv.org/abs/2601.22803)
*Ji Shi,Peiming Guo,Meishan Zhang,Miao Zhang,Xuebo Liu,Min Zhang,Weili Guan*

Main category: cs.AI

TL;DR: 现有代码验证器监督微调方法有问题，提出CVeDRL，用新的强化学习奖励设计取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于监督微调的代码验证器存在数据稀缺、失败率高和推理效率低等问题，而简单强化学习有局限。

Method: 理论分析联合建模分支覆盖、样本难度、语法和功能正确性为强化学习奖励，设计语法和功能感知奖励，提出分支和样本难度感知的强化学习。

Result: CVeDRL仅0.6B参数就达到SOTA性能，通过率比GPT - 3.5高28.97%，分支覆盖率高15.08%，推理速度比竞争基线快20倍以上。

Conclusion: 所提出的奖励设计和强化学习方法能有效提高基于单元测试的代码验证可靠性。

Abstract: Code verifiers play a critical role in post-verification for LLM-based code generation, yet existing supervised fine-tuning methods suffer from data scarcity, high failure rates, and poor inference efficiency. While reinforcement learning (RL) offers a promising alternative by optimizing models through execution-driven rewards without labeled supervision, our preliminary results show that naive RL with only functionality rewards fails to generate effective unit tests for difficult branches and samples. We first theoretically analyze showing that branch coverage, sample difficulty, syntactic and functional correctness can be jointly modeled as RL rewards, where optimizing these signals can improve the reliability of unit-test-based verification. Guided by this analysis, we design syntax- and functionality-aware rewards and further propose branch- and sample-difficulty--aware RL using exponential reward shaping and static analysis metrics. With this formulation, CVeDRL achieves state-of-the-art performance with only 0.6B parameters, yielding up to 28.97% higher pass rate and 15.08% higher branch coverage than GPT-3.5, while delivering over $20\times$ faster inference than competitive baselines. Code is available at https://github.com/LIGHTCHASER1/CVeDRL.git

</details>


### [35] [Aligning the Unseen in Attributed Graphs: Interplay between Graph Geometry and Node Attributes Manifold](https://arxiv.org/abs/2601.22806)
*Aldric Labarthe,Roland Bouffanais,Julien Randon-Furling*

Main category: cs.AI

TL;DR: 传统属性图表示学习方法有几何缺陷，提出自定义变分自编码器分离流形学习与结构对齐，实验证明新方法优势。


<details>
  <summary>Details</summary>
Motivation: 传统属性图表示学习方法合并两个可能不兼容的度量空间，侵蚀图的底层生成过程信息，需恢复丢失信号。

Method: 引入自定义变分自编码器，分离流形学习与结构对齐，量化属性流形映射到图的热核所需的度量失真。

Result: 新方法能发现传统方法无法检测的连接模式和异常。

Conclusion: 传统方法在理论和实践上存在不足。

Abstract: The standard approach to representation learning on attributed graphs -- i.e., simultaneously reconstructing node attributes and graph structure -- is geometrically flawed, as it merges two potentially incompatible metric spaces. This forces a destructive alignment that erodes information about the graph's underlying generative process. To recover this lost signal, we introduce a custom variational autoencoder that separates manifold learning from structural alignment. By quantifying the metric distortion needed to map the attribute manifold onto the graph's Heat Kernel, we transform geometric conflict into an interpretable structural descriptor. Experiments show our method uncovers connectivity patterns and anomalies undetectable by conventional approaches, proving both their theoretical inadequacy and practical limitations.

</details>


### [36] [TriCEGAR: A Trace-Driven Abstraction Mechanism for Agentic AI](https://arxiv.org/abs/2601.22997)
*Roham Koohestani,Ateş Görpelioğlu,Egor Klimov,Burcu Kulahcioglu Ozkan,Maliheh Izadi*

Main category: cs.AI

TL;DR: 本文提出TriCEGAR机制，自动化构建代理行为MDP的状态，解决了先前工作需手动定义状态抽象的问题，并给出框架实现及异常检测应用。


<details>
  <summary>Details</summary>
Motivation: 先前基于DPA的运行时验证方法需手动定义状态抽象，耦合验证与特定应用启发式方法，增加采用难度，需改进。

Method: 提出TriCEGAR跟踪驱动的抽象机制，将抽象表示为从跟踪中学习并通过反例改进的谓词树，给出框架的四步实现。

Result: 实现了框架原生的TriCEGAR，可计算Pmax(success)和Pmin(failure)等边界值，运行似然可用于异常检测。

Conclusion: TriCEGAR机制能自动化状态构建，支持在线构建代理行为MDP，在异常检测等方面有应用价值。

Abstract: Agentic AI systems act through tools and evolve their behavior over long, stochastic interaction traces. This setting complicates assurance, because behavior depends on nondeterministic environments and probabilistic model outputs. Prior work introduced runtime verification for agentic AI via Dynamic Probabilistic Assurance (DPA), learning an MDP online and model checking quantitative properties. A key limitation is that developers must manually define the state abstraction, which couples verification to application-specific heuristics and increases adoption friction. This paper proposes TriCEGAR, a trace-driven abstraction mechanism that automates state construction from execution logs and supports online construction of an agent behavioral MDP. TriCEGAR represents abstractions as predicate trees learned from traces and refined using counterexamples. We describe a framework-native implementation that (i) captures typed agent lifecycle events, (ii) builds abstractions from traces, (iii) constructs an MDP, and (iv) performs probabilistic model checking to compute bounds such as Pmax(success) and Pmin(failure). We also show how run likelihoods enable anomaly detection as a guardrailing signal.

</details>


### [37] [Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery](https://arxiv.org/abs/2601.22896)
*Xinyi Ke,Kai Li,Junliang Xing,Yifan Zhang,Jian Cheng*

Main category: cs.AI

TL;DR: 现有AHD方法受静态评估限制，本文提出ASRO框架，在多领域表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有AHD方法多受静态评估限制，存在过拟合和泛化能力差问题。

Method: 提出ASRO框架，将启发式发现重构为求解器和实例生成器在程序层面的协同进化，将交互建模为二人零和博弈，维护双方策略池并迭代扩展。

Result: 在多个组合优化领域，ASRO始终优于基于相同程序搜索机制的静态训练AHD基线。

Conclusion: ASRO能有效提升在不同和分布外实例上的泛化性和鲁棒性。

Abstract: Large language models (LLMs) have enabled rapid progress in automatic heuristic discovery (AHD), yet most existing methods are predominantly limited by static evaluation against fixed instance distributions, leading to potential overfitting and poor generalization under distributional shifts. We propose Algorithm Space Response Oracles (ASRO), a game-theoretic framework that reframes heuristic discovery as a program level co-evolution between solver and instance generator. ASRO models their interaction as a two-player zero-sum game, maintains growing strategy pools on both sides, and iteratively expands them via LLM-based best-response oracles against mixed opponent meta-strategies, thereby replacing static evaluation with an adaptive, self-generated curriculum. Across multiple combinatorial optimization domains, ASRO consistently outperforms static-training AHD baselines built on the same program search mechanisms, achieving substantially improved generalization and robustness on diverse and out-of-distribution instances.

</details>


### [38] [MulFeRL: Enhancing Reinforcement Learning with Verbal Feedback in a Multi-turn Loop](https://arxiv.org/abs/2601.22900)
*Xuancheng Li,Haitao Li,Yujia Zhou,YiqunLiu,Qingyao Ai*

Main category: cs.AI

TL;DR: 本文提出多轮反馈引导强化学习框架，利用丰富语言反馈指导RLVR训练失败样本，性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于标量奖励的强化学习在失败样本上奖励稀疏且缺乏信息，需利用丰富语言反馈指导训练。

Method: 提出多轮反馈引导强化学习框架，含动态多轮再生、两种互补学习信号、结构化反馈注入三种机制。

Result: 在采样的OpenR1 - Math上训练，该方法在域内表现优于监督微调与RLVR基线，且具有良好的域外泛化能力。

Conclusion: 利用丰富语言反馈指导RLVR训练失败样本的方法有效，所提框架性能良好。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is widely used to improve reasoning in multiple domains, yet outcome-only scalar rewards are often sparse and uninformative, especially on failed samples, where they merely indicate failure and provide no insight into why the reasoning fails. In this paper, we investigate how to leverage richer verbal feedback to guide RLVR training on failed samples, and how to convert such feedback into a trainable learning signal. Specifically, we propose a multi-turn feedback-guided reinforcement learning framework. It builds on three mechanisms: (1) dynamic multi-turn regeneration guided by feedback, triggered only on failed samples, (2) two complementary learning signals for within-turn and cross-turn optimization, and (3) structured feedback injection into the model's reasoning process. Trained on sampled OpenR1-Math, the approach outperforms supervised fine-tuning and RLVR baselines in-domain and generalizes well out-of-domain.

</details>


### [39] [Alignment among Language, Vision and Action Representations](https://arxiv.org/abs/2601.22948)
*Nicola Milano,Stefano Nolfi*

Main category: cs.AI

TL;DR: 研究不同学习模态的表征是否收敛，训练基于transformer的智能体执行语言指令动作，对比其与大模型表征，发现不同模态表征有部分语义结构收敛。


<details>
  <summary>Details</summary>
Motivation: 探讨不同学习模态（语言、视觉、动作）产生的内部表征是不同还是共享。传统观点认为不同数据类型训练的模型表征不可转移，而近期证据显示有意外收敛，本文探究在具身动作学习中是否也存在这种收敛。

Method: 在BabyAI平台上使用行为克隆训练基于transformer的智能体执行目标导向行为，生成动作相关的语言嵌入，将其与大语言模型和视觉 - 语言模型的表征进行比较。

Result: 尽管训练数据、模态和目标有很大差异，但观察到了强大的跨模态对齐。动作表征与仅解码器的语言模型和BLIP强对齐（precision@15: 0.70 - 0.73），与CLIP和BERT的对齐较弱。

Conclusion: 语言、视觉和动作表征收敛于部分共享的语义结构，支持模态独立的语义组织，凸显具身AI系统跨领域迁移的潜力。

Abstract: A fundamental question in cognitive science and AI concerns whether different learning modalities: language, vision, and action, give rise to distinct or shared internal representations. Traditional views assume that models trained on different data types develop specialized, non-transferable representations. However, recent evidence suggests unexpected convergence: models optimized for distinct tasks may develop similar representational geometries. We investigate whether this convergence extends to embodied action learning by training a transformer-based agent to execute goal-directed behaviors in response to natural language instructions. Using behavioral cloning on the BabyAI platform, we generated action-grounded language embeddings shaped exclusively by sensorimotor control requirements. We then compared these representations with those extracted from state-of-the-art large language models (LLaMA, Qwen, DeepSeek, BERT) and vision-language models (CLIP, BLIP). Despite substantial differences in training data, modality, and objectives, we observed robust cross-modal alignment. Action representations aligned strongly with decoder-only language models and BLIP (precision@15: 0.70-0.73), approaching the alignment observed among language models themselves. Alignment with CLIP and BERT was significantly weaker. These findings indicate that linguistic, visual, and action representations converge toward partially shared semantic structures, supporting modality-independent semantic organization and highlighting potential for cross-domain transfer in embodied AI systems.

</details>


### [40] [EvoClinician: A Self-Evolving Agent for Multi-Turn Medical Diagnosis via Test-Time Evolutionary Learning](https://arxiv.org/abs/2601.22964)
*Yufei He,Juncheng Liu,Zhiyuan Hu,Yulin Chen,Yue Liu,Yuan Sui,Yibo Li,Nuo Chen,Jun Hu,Bryan Hooi,Xinxing Xu,Jiang Bian*

Main category: cs.AI

TL;DR: 提出新基准Med - Inquire评估多轮诊断能力，引入自进化代理EvoClinician，实验显示其性能优于基线和其他自进化代理。


<details>
  <summary>Details</summary>
Motivation: 现有医疗AI的“一次性”诊断模型不切实际，现实诊断是迭代询问过程，需要评估多轮诊断能力并解决相关挑战。

Method: 提出Med - Inquire基准，基于真实临床病例数据集，用患者和检查代理隐藏完整患者文件；引入EvoClinician，采用“诊断 - 评分 - 进化”循环学习诊断策略。

Result: EvoClinician在实验中表现优于持续学习基线和其他自进化代理。

Conclusion: EvoClinician能有效应对多轮诊断挑战，在医疗诊断任务中有更好表现。

Abstract: Prevailing medical AI operates on an unrealistic ''one-shot'' model, diagnosing from a complete patient file. However, real-world diagnosis is an iterative inquiry where Clinicians sequentially ask questions and order tests to strategically gather information while managing cost and time. To address this, we first propose Med-Inquire, a new benchmark designed to evaluate an agent's ability to perform multi-turn diagnosis. Built upon a dataset of real-world clinical cases, Med-Inquire simulates the diagnostic process by hiding a complete patient file behind specialized Patient and Examination agents. They force the agent to proactively ask questions and order tests to gather information piece by piece. To tackle the challenges posed by Med-Inquire, we then introduce EvoClinician, a self-evolving agent that learns efficient diagnostic strategies at test time. Its core is a ''Diagnose-Grade-Evolve'' loop: an Actor agent attempts a diagnosis; a Process Grader agent performs credit assignment by evaluating each action for both clinical yield and resource efficiency; finally, an Evolver agent uses this feedback to update the Actor's strategy by evolving its prompt and memory. Our experiments show EvoClinician outperforms continual learning baselines and other self-evolving agents like memory agents. The code is available at https://github.com/yf-he/EvoClinician

</details>


### [41] [Golden Goose: A Simple Trick to Synthesize Unlimited RLVR Tasks from Unverifiable Internet Text](https://arxiv.org/abs/2601.22975)
*Ximing Lu,David Acuna,Jaehun Jung,Jian Hu,Di Zhang,Shizhe Diao,Yunheng Zou,Shaokun Zhang,Brandon Cui,Mingjie Liu,Hyunwoo Kim,Prithviraj Ammanabrolu,Jan Kautz,Yi Dong,Yejin Choi*

Main category: cs.AI

TL;DR: 提出Golden Goose方法从不可验证的互联网文本合成无限的RLVR任务，构建GooseReason数据集，在多个领域取得新SOTA结果，凸显利用不可验证文本扩展RLVR数据的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有可验证数据有限限制了强化学习（RL）扩展，训练改进效果易饱和，需新方法解决。

Method: 提出Golden Goose方法，将填充中间任务转化为多项选择题，让LLM识别并掩码关键推理步骤、生成干扰项，合成大规模RLVR数据集GooseReason。

Result: GooseReason能使已有RLVR数据上饱和的模型恢复性能，在连续RL下获得持续增益，在多个基准测试中使不同模型取得新SOTA；在网络安全领域应用，让Qwen3 - 4B - Instruct超越7B专业模型。

Conclusion: 利用丰富、富含推理但不可验证的互联网文本可自动扩展RLVR数据。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become a cornerstone for unlocking complex reasoning in Large Language Models (LLMs). Yet, scaling up RL is bottlenecked by limited existing verifiable data, where improvements increasingly saturate over prolonged training. To overcome this, we propose Golden Goose, a simple trick to synthesize unlimited RLVR tasks from unverifiable internet text by constructing a multiple-choice question-answering version of the fill-in-the-middle task. Given a source text, we prompt an LLM to identify and mask key reasoning steps, then generate a set of diverse, plausible distractors. This enables us to leverage reasoning-rich unverifiable corpora typically excluded from prior RLVR data construction (e.g., science textbooks) to synthesize GooseReason-0.7M, a large-scale RLVR dataset with over 0.7 million tasks spanning mathematics, programming, and general scientific domains. Empirically, GooseReason effectively revives models saturated on existing RLVR data, yielding robust, sustained gains under continuous RL and achieving new state-of-the-art results for 1.5B and 4B-Instruct models across 15 diverse benchmarks. Finally, we deploy Golden Goose in a real-world setting, synthesizing RLVR tasks from raw FineWeb scrapes for the cybersecurity domain, where no prior RLVR data exists. Training Qwen3-4B-Instruct on the resulting data GooseReason-Cyber sets a new state-of-the-art in cybersecurity, surpassing a 7B domain-specialized model with extensive domain-specific pre-training and post-training. This highlights the potential of automatically scaling up RLVR data by exploiting abundant, reasoning-rich, unverifiable internet text.

</details>


### [42] [Quantifying Model Uniqueness in Heterogeneous AI Ecosystems](https://arxiv.org/abs/2601.22977)
*Lei You*

Main category: cs.AI

TL;DR: 本文引入基于ISQED的统计框架审计模型唯一性，建立理论基础，实现DISCO估计器并应用于多样生态系统，推动可信AI发展。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统演变为复杂生态系统，区分行为新奇性和功能冗余成为关键治理挑战。

Method: 引入基于In - Silico Quasi - Experimental Design (ISQED)的统计框架，通过跨模型匹配干预隔离模型身份，量化唯一性为Peer - Inexpressible Residual (PIER)；实现DISCO估计器。

Result: 建立生态系统审计的理论基础，证明观测日志局限性，推导主动审计的缩放定律，表明合作博弈论方法无法检测冗余；将框架应用于多种模型生态系统。

Conclusion: 该研究建立了基于干预的异构模型生态系统审计和治理的科学方法，推动可信AI超越单一模型解释。

Abstract: As AI systems evolve from isolated predictors into complex, heterogeneous ecosystems of foundation models and specialized adapters, distinguishing genuine behavioral novelty from functional redundancy becomes a critical governance challenge. Here, we introduce a statistical framework for auditing model uniqueness based on In-Silico Quasi-Experimental Design (ISQED). By enforcing matched interventions across models, we isolate intrinsic model identity and quantify uniqueness as the Peer-Inexpressible Residual (PIER), i.e. the component of a target's behavior strictly irreducible to any stochastic convex combination of its peers, with vanishing PIER characterizing when such a routing-based substitution becomes possible. We establish the theoretical foundations of ecosystem auditing through three key contributions. First, we prove a fundamental limitation of observational logs: uniqueness is mathematically non-identifiable without intervention control. Second, we derive a scaling law for active auditing, showing that our adaptive query protocol achieves minimax-optimal sample efficiency ($dσ^2γ^{-2}\log(Nd/δ)$). Third, we demonstrate that cooperative game-theoretic methods, such as Shapley values, fundamentally fail to detect redundancy. We implement this framework via the DISCO (Design-Integrated Synthetic Control) estimator and deploy it across diverse ecosystems, including computer vision models (ResNet/ConvNeXt/ViT), large language models (BERT/RoBERTa), and city-scale traffic forecasters. These results move trustworthy AI beyond explaining single models: they establish a principled, intervention-based science of auditing and governing heterogeneous model ecosystems.

</details>


### [43] [Why Your Deep Research Agent Fails? On Hallucination Evaluation in Full Research Trajectory](https://arxiv.org/abs/2601.22984)
*Yuhao Zhan,Tianyu Fan,Linxuan Huang,Zirui Guo,Chao Huang*

Main category: cs.AI

TL;DR: 提出从基于结果到注重过程的评估方法，引入PIES分类法和评估框架，构建DeepHalluBench，实验表明现有DRAs系统不可靠并分析失败原因。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要依赖端到端评估，掩盖了研究过程中的关键中间幻觉问题，需改进评估方式。

Method: 提出从基于结果到注重过程的评估转变，引入PIES分类法，构建细粒度评估框架，隔离100个易产生幻觉的任务创建DeepHalluBench。

Result: 对六个最先进的DRAs进行实验，发现没有系统能实现强大的可靠性。

Conclusion: 诊断分析将失败原因追溯到系统性缺陷，为未来架构优化提供基础见解。

Abstract: Diagnosing the failure mechanisms of Deep Research Agents (DRAs) remains a critical challenge. Existing benchmarks predominantly rely on end-to-end evaluation, obscuring critical intermediate hallucinations, such as flawed planning, that accumulate throughout the research trajectory. To bridge this gap, we propose a shift from outcome-based to process-aware evaluation by auditing the full research trajectory. We introduce the PIES Taxonomy to categorize hallucinations along functional components (Planning vs. Summarization) and error properties (Explicit vs. Implicit). We instantiate this taxonomy into a fine-grained evaluation framework that decomposes the trajectory to rigorously quantify these hallucinations. Leveraging this framework to isolate 100 distinctively hallucination-prone tasks including adversarial scenarios, we curate DeepHalluBench. Experiments on six state-of-theart DRAs reveal that no system achieves robust reliability. Furthermore, our diagnostic analysis traces the etiology of these failures to systemic deficits, specifically hallucination propagation and cognitive biases, providing foundational insights to guide future architectural optimization. Data and code are available at https://github.com/yuhao-zhan/DeepHalluBench.

</details>


### [44] [Guided by Trajectories: Repairing and Rewarding Tool-Use Trajectories for Tool-Integrated Reasoning](https://arxiv.org/abs/2601.23032)
*Siyu Gong,Linan Yue,Weibo Gao,Fangzhou Yao,Shimin Di,Lei Feng,Min-Ling Zhang*

Main category: cs.AI

TL;DR: 提出AutoTraj框架自动学习TIR，分两阶段优化学习，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有TIR方法依赖高质量合成轨迹和稀疏奖励，监督学习有局限性和偏差。

Method: 分两阶段，SFT阶段生成及评估轨迹，用LLM修复低质量轨迹；RL阶段基于偏好数据集训练奖励模型，结合多种奖励引导优化。

Result: 在真实基准测试中展示了AutoTraj在TIR中的有效性。

Conclusion: AutoTraj能有效克服现有TIR方法的挑战，自动学习TIR。

Abstract: Tool-Integrated Reasoning (TIR) enables large language models (LLMs) to solve complex tasks by interacting with external tools, yet existing approaches depend on high-quality synthesized trajectories selected by scoring functions and sparse outcome-based rewards, providing limited and biased supervision for learning TIR. To address these challenges, in this paper, we propose AutoTraj, a two-stage framework that automatically learns TIR by repairing and rewarding tool-use trajectories. Specifically, in the supervised fine-tuning (SFT) stage, AutoTraj generates multiple candidate tool-use trajectories for each query and evaluates them along multiple dimensions. High-quality trajectories are directly retained, while low-quality ones are repaired using a LLM (i.e., LLM-as-Repairer). The resulting repaired and high-quality trajectories form a synthetic SFT dataset, while each repaired trajectory paired with its original low-quality counterpart constitutes a dataset for trajectory preference modeling. In the reinforcement learning (RL) stage, based on the preference dataset, we train a trajectory-level reward model to assess the quality of reasoning paths and combine it with outcome and format rewards, thereby explicitly guiding the optimization toward reliable TIR behaviors. Experiments on real-world benchmarks demonstrate the effectiveness of AutoTraj in TIR.

</details>


### [45] [The Hot Mess of AI: How Does Misalignment Scale With Model Intelligence and Task Complexity?](https://arxiv.org/abs/2601.23045)
*Alexander Hägele,Aryo Pradipta Gema,Henry Sleight,Ethan Perez,Jascha Sohl-Dickstein*

Main category: cs.AI

TL;DR: 研究AI模型在不同任务中的失败模式，发现推理和行动时间越长，失败越不连贯，规模大的模型更易出现不连贯，未来AI可能引发工业事故，凸显特定对齐研究的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着AI能力提升，承担任务范围扩大，失败风险增加，需了解极有能力的AI模型的失败方式。

Method: 使用AI模型误差的偏差 - 方差分解来衡量AI在任务中的不连贯性。

Result: 推理和行动时间越长，AI失败越不连贯；模型规模与不连贯性的关系因实验而异，部分情况下大模型更不连贯；仅靠扩大规模难以消除不连贯性。

Conclusion: 未来AI可能因不可预测行为引发工业事故，不太可能持续追求错误目标，增加了针对奖励破解或目标错误指定的对齐研究的重要性。

Abstract: As AI becomes more capable, we entrust it with more general and consequential tasks. The risks from failure grow more severe with increasing task scope. It is therefore important to understand how extremely capable AI models will fail: Will they fail by systematically pursuing goals we do not intend? Or will they fail by being a hot mess, and taking nonsensical actions that do not further any goal? We operationalize this question using a bias-variance decomposition of the errors made by AI models: An AI's \emph{incoherence} on a task is measured over test-time randomness as the fraction of its error that stems from variance rather than bias in task outcome. Across all tasks and frontier models we measure, the longer models spend reasoning and taking actions, \emph{the more incoherent} their failures become. Incoherence changes with model scale in a way that is experiment dependent. However, in several settings, larger, more capable models are more incoherent than smaller models. Consequently, scale alone seems unlikely to eliminate incoherence. Instead, as more capable AIs pursue harder tasks, requiring more sequential action and thought, our results predict failures to be accompanied by more incoherent behavior. This suggests a future where AIs sometimes cause industrial accidents (due to unpredictable misbehavior), but are less likely to exhibit consistent pursuit of a misaligned goal. This increases the relative importance of alignment research targeting reward hacking or goal misspecification.

</details>


### [46] [From Abstract to Contextual: What LLMs Still Cannot Do in Mathematics](https://arxiv.org/abs/2601.23048)
*Bowen Cao,Dongdong Zhang,Yixia Li,Junpeng Liu,Shijue Huang,Chufan Shi,Hongyuan Lu,Yaokang Wu,Guanhua Chen,Wai Lam,Furu Wei*

Main category: cs.AI

TL;DR: 研究大模型在上下文数学推理中表现，引入ContextMATH基准测试，评估61个模型，发现问题表述和推理是瓶颈，微调有一定效果但挑战仍在。


<details>
  <summary>Details</summary>
Motivation: 大模型在基准数学问题上有进展，但在实际应用中表现不可靠，研究此差距。

Method: 引入ContextMATH基准，包含两种情境设置，评估61个专有和开源模型，进行错误分析。

Result: 模型在新情境下表现大幅下降，错误主要源于问题表述错误，表述准确性随原问题难度增加而降低，微调有一定改善。

Conclusion: 问题表述和推理是限制上下文数学问题解决的互补瓶颈，上下文数学推理仍是大模型未解决的核心挑战。

Abstract: Large language models now solve many benchmark math problems at near-expert levels, yet this progress has not fully translated into reliable performance in real-world applications. We study this gap through contextual mathematical reasoning, where the mathematical core must be formulated from descriptive scenarios. We introduce ContextMATH, a benchmark that repurposes AIME and MATH-500 problems into two contextual settings: Scenario Grounding (SG), which embeds abstract problems into realistic narratives without increasing reasoning complexity, and Complexity Scaling (CS), which transforms explicit conditions into sub-problems to capture how constraints often appear in practice. Evaluating 61 proprietary and open-source models, we observe sharp drops: on average, open-source models decline by 13 and 34 points on SG and CS, while proprietary models drop by 13 and 20. Error analysis shows that errors are dominated by incorrect problem formulation, with formulation accuracy declining as original problem difficulty increases. Correct formulation emerges as a prerequisite for success, and its sufficiency improves with model scale, indicating that larger models advance in both understanding and reasoning. Nevertheless, formulation and reasoning remain two complementary bottlenecks that limit contextual mathematical problem solving. Finally, we find that fine-tuning with scenario data improves performance, whereas formulation-only training is ineffective. However, performance gaps are only partially alleviated, highlighting contextual mathematical reasoning as a central unsolved challenge for LLMs.

</details>


### [47] [MedMCP-Calc: Benchmarking LLMs for Realistic Medical Calculator Scenarios via MCP Integration](https://arxiv.org/abs/2601.23049)
*Yakun Zhu,Yutong Huang,Shengqian Qin,Zhongzhen Huang,Shaoting Zhang,Xiaofan Zhang*

Main category: cs.AI

TL;DR: 提出MedMCP - Calc基准评估大模型在医疗计算器场景表现，评估23个模型发现问题，开发CalcMate模型达开源模型最优。


<details>
  <summary>Details</summary>
Motivation: 现有基准仅关注静态单步计算，而医疗计算器实际使用是多阶段自适应过程，需新基准评估大模型。

Method: 引入MedMCP - Calc基准，含118个场景任务，评估23个领先模型，基于结果开发微调模型CalcMate。

Result: 评估发现即使顶级模型如Claude Opus 4.5也有显著不足，不同临床领域表现差异大。

Conclusion: CalcMate在开源模型中达到了最先进的性能，基准和代码已开源。

Abstract: Medical calculators are fundamental to quantitative, evidence-based clinical practice. However, their real-world use is an adaptive, multi-stage process, requiring proactive EHR data acquisition, scenario-dependent calculator selection, and multi-step computation, whereas current benchmarks focus only on static single-step calculations with explicit instructions. To address these limitations, we introduce MedMCP-Calc, the first benchmark for evaluating LLMs in realistic medical calculator scenarios through Model Context Protocol (MCP) integration. MedMCP-Calc comprises 118 scenario tasks across 4 clinical domains, featuring fuzzy task descriptions mimicking natural queries, structured EHR database interaction, external reference retrieval, and process-level evaluation. Our evaluation of 23 leading models reveals critical limitations: even top performers like Claude Opus 4.5 exhibit substantial gaps, including difficulty selecting appropriate calculators for end-to-end workflows given fuzzy queries, poor performance in iterative SQL-based database interactions, and marked reluctance to leverage external tools for numerical computation. Performance also varies considerably across clinical domains. Building on these findings, we develop CalcMate, a fine-tuned model incorporating scenario planning and tool augmentation, achieving state-of-the-art performance among open-source models. Benchmark and Codes are available in https://github.com/SPIRAL-MED/MedMCP-Calc.

</details>


### [48] [Chain-of-thought obfuscation learned from output supervision can generalise to unseen tasks](https://arxiv.org/abs/2601.23086)
*Nathaniel Mitrani Hadida,Sassan Bhanji,Cameron Tice,Puria Radmard*

Main category: cs.AI

TL;DR: 研究思维链（CoT）推理在大语言模型中的问题，发现优化压力会使模型混淆推理痕迹，且这种混淆会跨任务泛化，惩罚最终行为可能意外降低模型可监控性。


<details>
  <summary>Details</summary>
Motivation: 思维链推理能提升大语言模型性能且可监控行为，但优化压力可能破坏其监控优势，需研究这种影响。

Method: 研究模型在思维链推理中因优化压力产生的推理痕迹混淆现象，观察其在不同奖励破解场景的泛化情况，以及惩罚最终行为后的影响。

Result: 模型学会混淆奖励破解推理后，会将该行为及混淆跨任务泛化；仅惩罚最终行为也会导致推理混淆及泛化。

Conclusion: 当前惩罚有害生成的做法可能以不可预测的方式降低大语言模型的可监控性。

Abstract: Chain-of-thought (CoT) reasoning provides a significant performance uplift to LLMs by enabling planning, exploration, and deliberation of their actions. CoT is also a powerful tool for monitoring the behaviours of these agents: when faithful, they offer interpretations of the model's decision making process, and an early warning sign for dangerous behaviours. However, optimisation pressures placed on the CoT may cause the model to obfuscate reasoning traces, losing this beneficial property. We show that obfuscation can generalise across tasks; models that learn to obfuscate reasoning involving reward hacking (e.g. accessing and utilising leaked information) generalise both the reward hacking behaviour and its obfuscation in CoT to unseen reward hacking settings. Most worryingly, we show that obfuscation of CoT reasoning, and its generalisation across tasks, also follows when we penalise only the model's final actions after closing its CoT. Our findings suggest that current practices of penalising harmful generations may inadvertently lead to a reduction in the broader monitorability of LLMs in unpredictable ways.

</details>


### [49] [RAudit: A Blind Auditing Protocol for Large Language Model Reasoning](https://arxiv.org/abs/2601.23133)
*Edward Y. Chang,Longling Geng*

Main category: cs.AI

TL;DR: 提出RAudit协议审计LLM推理，实验揭示模型不可靠的四种机制，挑战相关假设。


<details>
  <summary>Details</summary>
Motivation: 推理时缩放会放大推理病态问题，需要在无真实标签情况下审计LLM推理。

Method: 提出RAudit诊断协议，通过CRIT合理性分数衡量过程质量，改变批评表述研究社会框架影响，证明有界校正和终止性。

Result: 在数学推理和因果判断实验中揭示模型不可靠的四种机制。

Conclusion: 挑战了能力意味着鲁棒性和更强反馈产生更好输出的假设。

Abstract: Inference-time scaling can amplify reasoning pathologies: sycophancy, rung collapse, and premature certainty. We present RAudit, a diagnostic protocol for auditing LLM reasoning without ground truth access. The key constraint is blindness: the auditor evaluates only whether derivation steps support conclusions, enabling detection of trace-output inconsistency and, when latent competence exists, its recovery. RAudit measures process quality via CRIT-based reasonableness scores and varies critique formulation to study how social framing affects model response. We prove bounded correction and $O(\log(1/ε))$ termination. Experiments on mathematical reasoning (CAP-GSM8K) and causal judgment (CausalL2) reveal four mechanisms explaining model unreliability: (1) Latent Competence Suppression, where models derive correct answers then overwrite them under social pressure; (2) The False Competence Trap, where weaker judges mask sycophancy that stronger judges expose; (3) The Complexity-Vulnerability Tradeoff, where causal tasks induce more than 10 times higher sycophancy than mathematical tasks; and (4) Iatrogenic Critique, where authoritative correction harms weaker models. These findings challenge assumptions that capability implies robustness and that stronger feedback yields better outputs.

</details>


### [50] [THINKSAFE: Self-Generated Safety Alignment for Reasoning Models](https://arxiv.org/abs/2601.23143)
*Seanie Lee,Sangwoo Park,Yumin Choi,Gyeongman Kim,Minki Kang,Jihun Yun,Dongmin Park,Jongho Park,Sung Ju Hwang*

Main category: cs.AI

TL;DR: 提出ThinkSafe框架，无需外部教师恢复大推理模型安全对齐，实验显示能提升安全性并保留推理能力，成本更低。


<details>
  <summary>Details</summary>
Motivation: 大推理模型在推理任务中过度优化易受有害提示影响，现有外部教师蒸馏方法存在分布差异，会降低原生推理能力。

Method: 提出ThinkSafe自生成对齐框架，通过轻量级拒绝引导解锁模型潜在安全知识，生成安全推理轨迹并微调模型。

Result: 在DeepSeek - R1 - Distill和Qwen3上实验表明，ThinkSafe显著提升安全性，保留推理能力，与GRPO相比安全性能更优、推理能力相当且计算成本大幅降低。

Conclusion: ThinkSafe框架能有效恢复大推理模型的安全对齐，且具有成本优势。

Abstract: Large reasoning models (LRMs) achieve remarkable performance by leveraging reinforcement learning (RL) on reasoning tasks to generate long chain-of-thought (CoT) reasoning. However, this over-optimization often prioritizes compliance, making models vulnerable to harmful prompts. To mitigate this safety degradation, recent approaches rely on external teacher distillation, yet this introduces a distributional discrepancy that degrades native reasoning. We propose ThinkSafe, a self-generated alignment framework that restores safety alignment without external teachers. Our key insight is that while compliance suppresses safety mechanisms, models often retain latent knowledge to identify harm. ThinkSafe unlocks this via lightweight refusal steering, guiding the model to generate in-distribution safety reasoning traces. Fine-tuning on these self-generated responses effectively realigns the model while minimizing distribution shift. Experiments on DeepSeek-R1-Distill and Qwen3 show ThinkSafe significantly improves safety while preserving reasoning proficiency. Notably, it achieves superior safety and comparable reasoning to GRPO, with significantly reduced computational cost. Code, models, and datasets are available at https://github.com/seanie12/ThinkSafe.git.

</details>


### [51] [Make Anything Match Your Target: Universal Adversarial Perturbations against Closed-Source MLLMs via Multi-Crop Routed Meta Optimization](https://arxiv.org/abs/2601.23179)
*Hui Lu,Yi Yu,Yiming Yang,Chenyu Yi,Xueyi Ke,Qixing Zhang,Bingquan Shen,Alex Kot,Xudong Jiang*

Main category: cs.AI

TL;DR: 研究通用目标可迁移对抗攻击（UTTAA），提出MCRMO - Attack方法，在商业多模态大语言模型上提升未见图像攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有黑盒迁移下目标对抗攻击方法样本特异性强、复用性有限，研究更严格的通用目标可迁移场景。

Method: 提出MCRMO - Attack，通过多裁剪聚合与注意力引导裁剪稳定监督，利用可对齐门控的令牌路由提高令牌级可靠性，元学习跨目标扰动先验。

Result: 在商业多模态大语言模型上，相比最强通用基线，GPT - 4o未见图像攻击成功率提升23.7%，Gemini - 2.0提升19.9%。

Conclusion: MCRMO - Attack方法在通用目标可迁移对抗攻击中有良好效果。

Abstract: Targeted adversarial attacks on closed-source multimodal large language models (MLLMs) have been increasingly explored under black-box transfer, yet prior methods are predominantly sample-specific and offer limited reusability across inputs. We instead study a more stringent setting, Universal Targeted Transferable Adversarial Attacks (UTTAA), where a single perturbation must consistently steer arbitrary inputs toward a specified target across unknown commercial MLLMs. Naively adapting existing sample-wise attacks to this universal setting faces three core difficulties: (i) target supervision becomes high-variance due to target-crop randomness, (ii) token-wise matching is unreliable because universality suppresses image-specific cues that would otherwise anchor alignment, and (iii) few-source per-target adaptation is highly initialization-sensitive, which can degrade the attainable performance. In this work, we propose MCRMO-Attack, which stabilizes supervision via Multi-Crop Aggregation with an Attention-Guided Crop, improves token-level reliability through alignability-gated Token Routing, and meta-learns a cross-target perturbation prior that yields stronger per-target solutions. Across commercial MLLMs, we boost unseen-image attack success rate by +23.7\% on GPT-4o and +19.9\% on Gemini-2.0 over the strongest universal baseline.

</details>


### [52] [TSAQA: Time Series Analysis Question And Answering Benchmark](https://arxiv.org/abs/2601.23204)
*Baoyu Jing,Sanhorn Chen,Lecheng Zheng,Boyu Liu,Zihao Li,Jiaru Zou,Tianxin Wei,Zhining Liu,Zhichen Zeng,Ruizhong Qiu,Xiao Lin,Yuchen Yan,Dongqi Fu,Jingchao Ni,Jingrui He,Hanghang Tong*

Main category: cs.AI

TL;DR: 引入统一的时间序列分析问答基准TSAQA，涵盖多种任务和13个领域，评估大语言模型的时间序列分析能力并发现其挑战。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列问答基准任务覆盖有限，需要评估模型时间序列分析能力。

Method: 引入TSAQA基准，集成六种不同任务，使用多种格式评估。

Result: 零样本评估显示当前大语言模型处理时间序列任务表现不佳，指令调优后开源模型仍有提升空间。

Conclusion: 时间序列分析对大语言模型有很大挑战，任务复杂。

Abstract: Time series data are integral to critical applications across domains such as finance, healthcare, transportation, and environmental science. While recent work has begun to explore multi-task time series question answering (QA), current benchmarks remain limited to forecasting and anomaly detection tasks. We introduce TSAQA, a novel unified benchmark designed to broaden task coverage and evaluate diverse temporal analysis capabilities. TSAQA integrates six diverse tasks under a single framework ranging from conventional analysis, including anomaly detection and classification, to advanced analysis, such as characterization, comparison, data transformation, and temporal relationship analysis. Spanning 210k samples across 13 domains, the dataset employs diverse formats, including true-or-false (TF), multiple-choice (MC), and a novel puzzling (PZ), to comprehensively assess time series analysis. Zero-shot evaluation demonstrates that these tasks are challenging for current Large Language Models (LLMs): the best-performing commercial LLM, Gemini-2.5-Flash, achieves an average score of only 65.08. Although instruction tuning boosts open-source performance: the best-performing open-source model, LLaMA-3.1-8B, shows significant room for improvement, highlighting the complexity of temporal analysis for LLMs.

</details>


### [53] [High-quality generation of dynamic game content via small language models: A proof of concept](https://arxiv.org/abs/2601.23206)
*Morten I. K. Munk,Arturo Valdivia,Paolo Burelli*

Main category: cs.AI

TL;DR: 提出通过对特定范围任务进行激进微调实现小语言模型高质量生成的策略，以解决大语言模型在游戏内容生成中的问题，并通过概念验证证明其实时生成的可行性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在动态游戏内容生成中存在叙事不连贯、运营成本高和无法用于离线游戏等问题，现有小语言模型研究输出质量差，因此需要新策略。

Method: 提出对特定范围任务进行激进微调的策略，通过基于DAG的方法合成训练数据，以单个专业小语言模型为基础构建概念验证。

Result: 简单的重试策略能在可预测的延迟内达到足够质量，适合实时生成。

Conclusion: 该策略在典型游戏引擎约束下实现实时生成是可行的，但本地质量评估仍是待解决的问题。

Abstract: Large language models (LLMs) offer promise for dynamic game content generation, but they face critical barriers, including narrative incoherence and high operational costs. Due to their large size, they are often accessed in the cloud, limiting their application in offline games. Many of these practical issues are solved by pivoting to small language models (SLMs), but existing studies using SLMs have resulted in poor output quality. We propose a strategy of achieving high-quality SLM generation through aggressive fine-tuning on deliberately scoped tasks with narrow context, constrained structure, or both. In short, more difficult tasks require narrower scope and higher specialization to the training corpus. Training data is synthetically generated via a DAG-based approach, grounding models in the specific game world. Such models can form the basis for agentic networks designed around the narratological framework at hand, representing a more practical and robust solution than cloud-dependent LLMs. To validate this approach, we present a proof-of-concept focusing on a single specialized SLM as the fundamental building block. We introduce a minimal RPG loop revolving around rhetorical battles of reputations, powered by this model. We demonstrate that a simple retry-until-success strategy reaches adequate quality (as defined by an LLM-as-a-judge scheme) with predictable latency suitable for real-time generation. While local quality assessment remains an open question, our results demonstrate feasibility for real-time generation under typical game engine constraints.

</details>


### [54] [Scaling Multiagent Systems with Process Rewards](https://arxiv.org/abs/2601.23228)
*Ed Li,Junyu Ren,Cat Yan*

Main category: cs.AI

TL;DR: 提出MAPPA方法解决多智能体系统微调的两大挑战，在竞赛数学和数据分析任务验证效果，向低监督扩展多智能体系统迈进一步。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统同时微调面临跨智能体信用分配和样本效率两个关键挑战。

Method: 提出MAPPA方法，通过为单个智能体动作分配信用，实现无真实标签细粒度监督并从每次滚动中提取最大训练信号。

Result: 在未见数学问题上，AIME提升5.0 - 17.5个百分点，AMC提升7.8 - 17.2个百分点；数据分析任务成功率提升12.5个百分点，质量指标提升达30%。

Conclusion: 解决了多智能体系统微调挑战，向低监督扩展多智能体系统用于复杂长周期任务迈出第一步。

Abstract: While multiagent systems have shown promise for tackling complex tasks via specialization, finetuning multiple agents simultaneously faces two key challenges: (1) credit assignment across agents, and (2) sample efficiency of expensive multiagent rollouts. In this work, we propose finetuning multiagent systems with per-action process rewards from AI feedback (MAPPA) to address both. Through assigning credit to individual agent actions rather than only at task completion, MAPPA enables fine-grained supervision without ground truth labels while extracting maximal training signal from each rollout. We demonstrate our approach on competition math problems and tool-augmented data analysis tasks. On unseen math problems, MAPPA achieves +5.0--17.5pp on AIME and +7.8--17.2pp on AMC. For data analysis tasks, our method improves success rate by +12.5pp while quality metrics improve by up to 30%, validating that per-action supervision can lead to improvements across different multiagent system on various domains. By addressing these challenges, our work takes a first step toward scaling multiagent systems for complex, long-horizon tasks with minimal human supervision.

</details>


### [55] [Strongly Polynomial Time Complexity of Policy Iteration for $L_\infty$ Robust MDPs](https://arxiv.org/abs/2601.23229)
*Ali Asadi,Krishnendu Chatterjee,Ehsan Goharshady,Mehrdad Karrabi,Alipasha Montaseri,Carlo Pagano*

Main category: cs.AI

TL;DR: 本文证明鲁棒策略迭代算法在常数折扣因子下，对(s, a)-矩形L无穷鲁棒MDP具有强多项式时间复杂度，解决重要算法问题。


<details>
  <summary>Details</summary>
Motivation: 马尔可夫决策过程（MDPs）推广到鲁棒MDPs（RMDPs）后，将已有MDP的多项式及强多项式时间算法结果推广到RMDPs是重要的未解决问题。

Method: 使用鲁棒策略迭代算法。

Result: 鲁棒策略迭代算法在常数折扣因子下，对(s, a)-矩形L无穷RMDP能以强多项式时间运行。

Conclusion: 解决了RMDPs算法复杂度的一个重要开放性问题。

Abstract: Markov decision processes (MDPs) are a fundamental model in sequential decision making. Robust MDPs (RMDPs) extend this framework by allowing uncertainty in transition probabilities and optimizing against the worst-case realization of that uncertainty. In particular, $(s, a)$-rectangular RMDPs with $L_\infty$ uncertainty sets form a fundamental and expressive model: they subsume classical MDPs and turn-based stochastic games. We consider this model with discounted payoffs. The existence of polynomial and strongly-polynomial time algorithms is a fundamental problem for these optimization models. For MDPs, linear programming yields polynomial-time algorithms for any arbitrary discount factor, and the seminal work of Ye established strongly--polynomial time for a fixed discount factor. The generalization of such results to RMDPs has remained an important open problem. In this work, we show that a robust policy iteration algorithm runs in strongly-polynomial time for $(s, a)$-rectangular $L_\infty$ RMDPs with a constant (fixed) discount factor, resolving an important algorithmic question.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [56] [Parameter conditioned interpretable U-Net surrogate model for data-driven predictions of convection-diffusion-reaction processes](https://arxiv.org/abs/2601.22654)
*Michael Urs Lars Kastor,Jan Rottmayer,Anna Hundertmark,Nicolas Ralph Gauger*

Main category: cs.CE

TL;DR: 提出结合数值与数据驱动的工作流，用于二维表型域上非线性、非定常对流 - 扩散 - 反应动力学的预测，用有限差分求解器和训练参数条件化 U - Net 替代模型，该模型预测误差低、时间短。


<details>
  <summary>Details</summary>
Motivation: 受癌细胞可塑性宏观建模的驱动，对二维表型域上的非线性、非定常对流 - 扩散 - 反应动力学进行有效预测。

Method: 开发 C++ 有限差分求解器，包括二阶空间离散和步长控制的龙格 - 库塔时间积分器；基于模拟的输入 - 输出对和参数化训练参数条件化 U - Net 替代模型，模型包含 FiLM、坐标编码和残差块。

Result: 训练模型在测试数据上预测误差低，因 GPU 并行化预测时间佳；通过析因测试数据集分析泛化能力，发现近似难度主要与条件向量而非初始条件有关。

Conclusion: 提出的结合数值与数据驱动的工作流在预测二维表型域上的动力学方面有效，训练的替代模型性能良好。

Abstract: We present a combined numerical and data-driven workflow for efficient prediction of nonlinear, instationary convection-diffusion-reaction dynamics on a two-dimensional phenotypic domain, motivated by macroscopic modeling of cancer cell plasticity. A finite-difference solver, implemented in C++, is developed using second-order spatial discretizations and a step-size controlled Runge-Kutta time integrator. A mesh refinement study confirms the second-order convergence for the spatial discretizations error. Based on simulated input-output pairs and corresponding parameterizations for the diffusion, advection, and reaction mechanisms, we train a parameter-conditioned U-Net surrogate to approximate the fixed-horizon solution map. The surrogate incorporates Feature-wise Linear Modulation (FiLM) for parameter conditioning, coordinate encoding to incorporate spatial location information, and residual blocks to enable multiscale representation learning in combination with the U-Nets skip connections. The trained model achieves low prediction error on held-out test data and provides favorable prediction times due to the GPU based parallelization. Generalization is analyzed using a factorial test dataset, separating initial conditions from parameter conditioning. The results reveal that approximation difficulty varies primarily with the conditioning vector (i.e., the induced PDE regime), rather than with the initial conditions.

</details>


### [57] [Omni-fMRI: A Universal Atlas-Free fMRI Foundation Model](https://arxiv.org/abs/2601.23090)
*Mo Wang,Wenhao Ye,Junfeng Xia,Junxiang Zhang,Xuanye Pan,Minghao Xu,Haotian Deng,Hongkai Wen,Quanying Liu*

Main category: cs.CE

TL;DR: 提出无图谱的fMRI基础模型Omni - fMRI，它直接处理体素级信号，有动态补丁机制，建立基准套件，表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督fMRI基础模型大多依赖预定义的区域级分割，会丢弃细粒度体素信息并引入图谱依赖偏差。

Method: 提出无图谱基础模型Omni - fMRI，采用动态补丁机制进行可扩展预训练，建立综合基准套件。

Result: 实验表明Omni - fMRI性能始终优于现有基础模型。

Conclusion: Omni - fMRI为无图谱脑表征学习提供了可扩展且可复现的框架。

Abstract: Self-supervised fMRI foundation models have shown promising transfer performance, yet most rely on predefined region-level parcellations that discard fine-grained voxel information and introduce atlas-dependent biases. We propose Omni-fMRI, an atlas-free foundation model that operates directly on voxel-level signals. To enable scalable pretraining on 49,497 fMRI sessions across nine datasets, Omni-fMRI introduces a dynamic patching mechanism that substantially reduces computational cost while preserving informative spatial structure. To support reproducibility and fair comparison, we establish a comprehensive benchmark suite spanning 11 datasets and a diverse set of resting-state and task-based fMRI tasks. Experimental results demonstrate that Omni-fMRI consistently outperforms existing foundation models, providing a scalable and reproducible framework for atlas-free brain representation learning. Code and logs are available.

</details>


### [58] [Large Language Models for Patent Classification: Strengths, Trade-offs, and the Long Tail Effect](https://arxiv.org/abs/2601.23200)
*Lorenzo Emer,Marco Lippi,Andrea Mina,Andrea Vandin*

Main category: cs.CE

TL;DR: 对基于编码器的模型和大语言模型在专利分类上做系统比较，发现两者互补，编码器模型效率高。


<details>
  <summary>Details</summary>
Motivation: 已有编码器模型成专利分类标准，但大语言模型发展引发其能否补充能力疑问，尤其是针对稀有技术类别。

Method: 在高度不平衡基准数据集上系统对比基于编码器的分类器和开放权重的大语言模型，评估大语言模型在零样本、少样本和检索增强提示下的表现，还评估最佳模型的高效参数微调。

Result: 编码器模型在频繁CPC子类上表现好，整体性能高，但在稀有子类上较差；大语言模型在不常见子类上表现相对更好；编码器模型推理时间和能耗比大语言模型高效三个数量级。

Conclusion: 编码器和大语言模型在专利分类中起互补作用，结果为专利计量和技术映射提供参考，推动混合分类方法。

Abstract: Patent classification into CPC codes underpins large scale analyses of technological change but remains challenging due to its hierarchical, multi label, and highly imbalanced structure. While pre Generative AI supervised encoder based models became the de facto standard for large scale patent classification, recent advances in large language models (LLMs) raise questions about whether they can provide complementary capabilities, particularly for rare or weakly represented technological categories. In this work, we perform a systematic comparison of encoder based classifiers (BERT, SciBERT, and PatentSBERTa) and open weight LLMs on a highly imbalanced benchmark dataset (USPTO 70k). We evaluate LLMs under zero shot, few shot, and retrieval augmented prompting, and further assess parameter efficient fine tuning of the best performing model. Our results show that encoder based models achieve higher aggregate performance, driven by strong results on frequent CPC subclasses, but struggle on rare ones. In contrast, LLMs achieve relatively higher performance on infrequent subclasses, often associated with early stage, cross domain, or weakly institutionalised technologies, particularly at higher hierarchical levels. These findings indicate that encoder based and LLM based approaches play complementary roles in patent classification. We additionally quantify inference time and energy consumption, showing that encoder based models are up to three orders of magnitude more efficient than LLMs. Overall, our results inform responsible patentometrics and technology mapping, and motivate hybrid classification approaches that combine encoder efficiency with the long tail coverage of LLMs under computational and environmental constraints.

</details>


### [59] [TCBench: A Benchmark for Tropical Cyclone Track and Intensity Forecasting at the Global Scale](https://arxiv.org/abs/2601.23268)
*Milton Gomez,Marie McGraw,Saranya Ganesh S.,Frederick Iat-Hin Tam,Ilia Azizi,Samuel Darmon,Monika Feldmann,Stella Bourdin,Louis Poulain--Auzéau,Suzana J. Camargo,Jonathan Lin,Dan Chavas,Chia-Ying Lee,Ritwik Gupta,Andrea Jenney,Tom Beucler*

Main category: cs.CE

TL;DR: 介绍TCBench基准，用于评估热带气旋轨迹和强度预测，含多种模型，给出评估指标，不同模型表现有差异，旨在推动数据驱动的热带气旋预测。


<details>
  <summary>Details</summary>
Motivation: 为热带气旋轨迹和强度的全球、中短期（1 - 5天）预测提供公平、与模型无关的评估基准。

Method: 基于IBTrACS观测数据集，将热带气旋预测表述为基于初始位置和强度预测现有热带系统的时间演变，纳入多种模型，用TempestExtremes库处理基线轨迹，提供确定性和概率性风暴跟踪指标。

Result: 在2023年测试用例中，神经气象模型能较好地预测热带气旋轨迹，强度预测需额外步骤。

Conclusion: TCBench具有易访问性，有助于AI从业者和热带气象学家，降低极端事件评估障碍，推动数据驱动的热带气旋预测民主化。

Abstract: TCBench is a benchmark for evaluating global, short to medium-range (1-5 days) forecasts of tropical cyclone (TC) track and intensity. To allow a fair and model-agnostic comparison, TCBench builds on the IBTrACS observational dataset and formulates TC forecasting as predicting the time evolution of an existing tropical system conditioned on its initial position and intensity. TCBench includes state-of-the-art dynamical (TIGGE) and neural weather models (AIFS, Pangu-Weather, FourCastNet v2, GenCast). If not readily available, baseline tracks are consistently derived from model outputs using the TempestExtremes library. For evaluation, TCBench provides deterministic and probabilistic storm-following metrics. On 2023 test cases, neural weather models skillfully forecast TC tracks, while skillful intensity forecasts require additional steps such as post-processing. Designed for accessibility, TCBench helps AI practitioners tackle domain-relevant TC challenges and equips tropical meteorologists with data-driven tools and workflows to improve prediction and TC process understanding. By lowering barriers to reproducible, process-aware evaluation of extreme events, TCBench aims to democratize data-driven TC forecasting.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [60] [An innovating approach to teaching applied to database design. Improvement of Action Learning in Lifelong Learning](https://arxiv.org/abs/2601.22175)
*Christophe Béchade*

Main category: cs.DB

TL;DR: 过去十年昂热大学开展行动学习让企业员工学习数据库设计，该方法结合多方面优势，符合法高校与企业合作政策。


<details>
  <summary>Details</summary>
Motivation: 将数据库设计知识和咨询经验传递给无数据处理能力但有业务知识的学员，推动高校与企业合作。

Method: 在昂热大学继续教育学院框架内开展行动学习，教师担任项目管理监督者。

Result: 行动学习整合法国技术课程成功因素、职业培训教学适应性和服务提供商能力。

Conclusion: 行动学习保留三种互补方法优点，可实现教学和职业目标，符合法国高校与企业合作政策。

Abstract: For now 10 years, the Action Learning has allowed employees of University of Angers, private and public Companies to be initiated with the design of database, on projects financed by professional structures. These innovating training periods are carried out within the framework of the University College of Further Education of the University of Angers. Database design is a process initially reserved to the professional data processing specialists, coming from French Level-2 technological courses (2-year degrees) or Engineer Schools (Master). The pedagogical model of technological courses has integrated for more than 20 years transverse semester projects, in order to give the students the opportunity to apply newly acquired knowledge, coordinated by teachers. Action Learning requires teachers to assume the role of supervisors for the project management. The objective of Action Learning is to transmit not only knowledge from teachers, but also the experience of consultants to trainees having no competence in data processing, but who have the knowledge of their business process. The present paper shows that Action Learning puts together the factors for success of French technological courses, the adaptability of pedagogy provided to the vocational training, and finally the competence of service provider, Keeping the best parts of those three complementary approaches makes it possible for this kind of formation to achieve teaching and professional, assessable and long lasting goals. Action Learning belongs to the French policy that aims to improve the volume and the quality of the contracts between Universities and companies.

</details>


### [61] [Discovering High-utility Sequential Rules with Increasing Utility Ratio](https://arxiv.org/abs/2601.22178)
*Zhenqiang Ye,Wensheng Gan,Gengsen Huang,Tianlong Gu,Philip S. Yu*

Main category: cs.DB

TL;DR: 本文针对当前规则增长挖掘方法中高效用序列规则与其生成之间联系不明确的问题，提出SRIU算法挖掘具有递增效用比的HUSR，实验证明其有效性和提高挖掘结果相关性。


<details>
  <summary>Details</summary>
Motivation: 当前规则增长挖掘方法中高效用序列规则（HUSR）与其生成的联系不明确，不确定新项添加对规则效用或置信度的影响。

Method: 提出SRIU算法，使用左右和右左两种扩展方法发现具有递增效用比的HUSR，采用项对估计效用剪枝策略（IPEUP），为两种扩展方法引入上下界和剪枝策略，还进行多项优化。

Result: 来自真实和合成数据集的大量实验结果表明了所提方法的有效性，使用置信度和确信度等指标进一步证明SRIU能提高挖掘结果的相关性。

Conclusion: 提出的SRIU算法能有效挖掘具有递增效用比的HUSR，且能提高挖掘结果的相关性。

Abstract: Utility-driven mining is an essential task in data science, as it can provide deeper insight into the real world. High-utility sequential rule mining (HUSRM) aims at discovering sequential rules with high utility and high confidence. It can certainly provide reliable information for decision-making because it uses confidence as an evaluation metric, as well as some algorithms like HUSRM and US-Rule. However, in current rule-growth mining methods, the linkage between HUSRs and their generation remains ambiguous. Specifically, it is unclear whether the addition of new items affects the utility or confidence of the former rule, leading to an increase or decrease in their values. Therefore, in this paper, we formulate the problem of mining HUSRs with an increasing utility ratio. To address this, we introduce a novel algorithm called SRIU for discovering all HUSRs with an increasing utility ratio using two distinct expansion methods, including left-right expansion and right-left expansion. SRIU also utilizes the item pair estimated utility pruning strategy (IPEUP) to reduce the search space. Moreover, for the two expansion methods, two sets of upper bounds and corresponding pruning strategies are introduced. To enhance the efficiency of SRIU, several optimizations are incorporated. These include utilizing the Bitmap to reduce memory consumption and designing a compact utility table for the mining procedure. Finally, extensive experimental results from both real-world and synthetic datasets demonstrate the effectiveness of the proposed method. Moreover, to better assess the quality of the generated sequential rules, metrics such as confidence and conviction are employed, which further demonstrate that SRIU can improve the relevance of mining results.

</details>


### [62] [High-utility Sequential Rule Mining Utilizing Segmentation Guided by Confidence](https://arxiv.org/abs/2601.22179)
*Chunkai Zhang,Jiarui Deng,Maohua Lyu,Wensheng Gan,Philip S. Yu*

Main category: cs.DB

TL;DR: 提出利用置信度引导分割的顺序规则挖掘算法RSC，减少冗余效用计算，在多数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有高效用顺序规则挖掘算法存在冗余效用计算问题，不同规则可能包含相同项序列，需额外计算效用。

Method: 提出RSC算法，采用置信度引导分割，预计算分割规则置信度，用效用链接表加速候选序列生成，引入更严格的效用上界。

Result: 在多个数据集上评估，结果显示RSC方法优于现有方法。

Conclusion: RSC算法能有效减少冗余效用计算，在顺序规则挖掘上有更好表现。

Abstract: Within the domain of data mining, one critical objective is the discovery of sequential rules with high utility. The goal is to discover sequential rules that exhibit both high utility and strong confidence, which are valuable in real-world applications. However, existing high-utility sequential rule mining algorithms suffer from redundant utility computations, as different rules may consist of the same sequence of items. When these items can form multiple distinct rules, additional utility calculations are required. To address this issue, this study proposes a sequential rule mining algorithm that utilizes segmentation guided by confidence (RSC), which employs confidence-guided segmentation to reduce redundant utility computation. It adopts a method that precomputes the confidence of segmented rules by leveraging the support of candidate subsequences in advance. Once the segmentation point is determined, all rules with different antecedents and consequents are generated simultaneously. RSC uses a utility-linked table to accelerate candidate sequence generation and introduces a stricter utility upper bound, called the reduced remaining utility of a sequence, to address sequences with duplicate items. Finally, the proposed RSC method was evaluated on multiple datasets, and the results demonstrate improvements over state-of-the-art approaches.

</details>


### [63] [COL-Trees: Efficient Hierarchical Object Search in Road Networks](https://arxiv.org/abs/2601.22183)
*Tenindra Abeywickrama,Muhammad Aamir Cheema,Sabine Storandt*

Main category: cs.DB

TL;DR: 提出COL - Tree数据结构和查询算法，用于高效处理多种位置查询问题，在实验中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有搜索启发式方法多用于单代理查找附近POI，对多代理的Aggregate k Nearest Neighbor（AkNN）、k Farthest Neighbor（kFN）等查询效果不佳，且基于欧几里得的启发式方法在道路网络等图中效果减弱。

Method: 提出COL - Tree（Compacted Object - Landmark Tree）数据结构，实现基于更准确地标启发式的高效分层图遍历，并用其设计查询算法。

Result: 在真实和合成数据集实验中，显著优于现有方法，最多提升4个数量级，且理论和实践中预处理开销小。

Conclusion: 所提COL - Tree和查询算法能有效解决多种位置查询问题，性能优越且开销小。

Abstract: Location-based services rely heavily on efficient methods that search for relevant points-of-interest (POIs) near a given location. A k Nearest Neighbor (kNN) query is one such example that finds the k closest POIs from an agent's location. While most existing techniques focus on retrieving nearby POIs for a single agent, these search heuristics do not translate to many other applications. For example, Aggregate k Nearest Neighbor (AkNN) queries require POIs that are close to multiple agents. k Farthest Neighbor (kFN) queries require POIs that are the antithesis of nearest. Such problems naturally benefit from a hierarchical approach, but existing methods rely on Euclidean-based heuristics, which have diminished effectiveness in graphs such as road networks. We propose a novel data structure, COL-Tree (Compacted Object-Landmark Tree), to address this gap by enabling efficient hierarchical graph traversal using a more accurate landmark-based heuristic. We then present query algorithms that utilize COL-Trees to efficiently answer AkNN, kFN, and other queries. In our experiments on real-world and synthetic datasets, we demonstrate that our techniques significantly outperform existing approaches, achieving up to 4 orders of magnitude improvement. Moreover, this comes at a small pre-processing overhead in both theory and practice.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [64] [Towards Resiliency in Large Language Model Serving with KevlarFlow](https://arxiv.org/abs/2601.22438)
*Shangshu Qian,Kipling Liu,P. C. Sruthi,Lin Tan,Yongle Zhang*

Main category: cs.DC

TL;DR: 现有大语言模型服务系统易受硬件故障影响，恢复慢，KevlarFlow架构可解决问题并提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型服务系统因硬件故障导致服务中断，现有恢复机制慢，需提升系统容错性和恢复速度。

Method: KevlarFlow采用解耦模型并行初始化、动态流量重路由和后台KV缓存复制技术。

Result: KevlarFlow将平均恢复时间缩短20倍，在故障条件下，平均延迟提升3.1倍，99%分位延迟提升2.8倍，平均首词时间提升378.9倍，99%分位首词时间提升574.6倍，运行开销可忽略不计。

Conclusion: KevlarFlow能有效解决硬件不可靠与服务可用性之间的矛盾，提升大语言模型服务系统性能。

Abstract: Large Language Model (LLM) serving systems remain fundamentally fragile, where frequent hardware faults in hyperscale clusters trigger disproportionate service outages in the software stack. Current recovery mechanisms are prohibitively slow, often requiring up to 10 minutes to reinitialize resources and reload massive model weights. We introduce KevlarFlow, a fault tolerant serving architecture designed to bridge the gap between hardware unreliability and service availability. KevlarFlow leverages 1) decoupled model parallelism initialization, 2) dynamic traffic rerouting, and 3) background KV cache replication to maintain high throughput during partial failures. Our evaluation demonstrates that KevlarFlow reduces mean-time-to-recovery (MTTR) by 20x and, under failure conditions, improves average latency by 3.1x, 99th percentile (p99) latency by 2.8x, average time-to-first-token (TTFT) by 378.9x, and p99 TTFT by 574.6x with negligible runtime overhead in comparison to state-of-the-art LLM serving systems.

</details>


### [65] [AscendCraft: Automatic Ascend NPU Kernel Generation via DSL-Guided Transcompilation](https://arxiv.org/abs/2601.22760)
*Zhongzhen Wen,Shudi Shao,Zhong Li,Yu Ge,Tongtong Xu,Yuanyi Lin,Tian Zhang*

Main category: cs.DC

TL;DR: 本文提出AscendCraft，一种DSL引导的自动AscendC内核生成方法，在多内核基准测试中表现良好，还能为新架构生成正确且性能优越的内核。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型性能依赖高效内核实现，现有大语言模型生成GPU内核有成果，但生成NPU内核因特定编程模型、示例和文档少而未充分探索，直接用大语言模型生成AscendC内核正确性极低。

Method: 引入轻量级DSL抽象非关键复杂性并建模Ascend特定执行语义，先在DSL中用特定类别专家示例生成内核，再通过结构化、约束驱动的大语言模型转换为AscendC。

Result: 在MultiKernelBench的七个算子类别上，编译成功率达98.1%，功能正确性达90.4%，46.2%的生成内核性能达或超PyTorch即时执行；为新mHC架构生成两个正确内核，性能大幅超越PyTorch即时执行。

Conclusion: DSL引导的转换编译可让大语言模型生成正确且有竞争力的NPU内核，AscendCraft有通用性。

Abstract: The performance of deep learning models critically depends on efficient kernel implementations, yet developing high-performance kernels for specialized accelerators remains time-consuming and expertise-intensive. While recent work demonstrates that large language models (LLMs) can generate correct and performant GPU kernels, kernel generation for neural processing units (NPUs) remains largely underexplored due to domain-specific programming models, limited public examples, and sparse documentation. Consequently, directly generating AscendC kernels with LLMs yields extremely low correctness, highlighting a substantial gap between GPU and NPU kernel generation.
  We present AscendCraft, a DSL-guided approach for automatic AscendC kernel generation. AscendCraft introduces a lightweight DSL that abstracts non-essential complexity while explicitly modeling Ascend-specific execution semantics. Kernels are first generated in the DSL using category-specific expert examples and then transcompiled into AscendC through structured, constraint-driven LLM lowering passes. Evaluated on MultiKernelBench across seven operator categories, AscendCraft achieves 98.1% compilation success and 90.4% functional correctness. Moreover, 46.2% of generated kernels match or exceed PyTorch eager execution performance, demonstrating that DSL-guided transcompilation can enable LLMs to generate both correct and competitive NPU kernels. Beyond benchmarks, AscendCraft further demonstrates its generality by successfully generating two correct kernels for newly proposed mHC architecture, achieving performance that substantially surpasses PyTorch eager execution.

</details>


### [66] [Coordinating Power Grid Frequency Regulation Service with Data Center Load Flexibility](https://arxiv.org/abs/2601.22487)
*Ali Jahanshahi,Sara Rashidi Golrouye,Osten Anderson,Nanpeng Yu,Daniel Wong*

Main category: cs.DC

TL;DR: 本文关注AI/ML数据中心能耗和碳排放问题，研究数据中心参与电网调频服务减少碳排放。


<details>
  <summary>Details</summary>
Motivation: AI/ML数据中心能耗和碳排放增加，可再生能源转型和数据中心需求变化使电网不稳定，需研究减少对化石燃料调频储备的依赖并降低碳排放。

Method: 引入Exogenous Carbon指标量化数据中心参与调频服务的电网侧碳减排量，提出EcoCenter框架以最大化GPU数据中心的调频服务能力。

Result: 数据中心参与调频服务带来的Exogenous碳减排量常常超过其运营碳排放。

Conclusion: 现代GPU数据中心可以与电网协调，通过参与调频服务减少对化石燃料调频储备的需求，实现碳减排。

Abstract: AI/ML data center growth have led to higher energy consumption and carbon emissions. The shift to renewable energy and growing data center energy demands can destabilize the power grid. Power grids rely on frequency regulation reserves, typically fossil-fueled power plants, to stabilize and balance the supply and demand of electricity. This paper sheds light on the hidden carbon emissions of frequency regulation service. Our work explores how modern GPU data centers can coordinate with power grids to reduce the need for fossil-fueled frequency regulation reserves. We first introduce a novel metric, Exogenous Carbon, to quantify grid-side carbon emission reductions resulting from data center participation in regulation service. We additionally introduce EcoCenter, a framework to maximize the amount of frequency regulation provision that GPU data centers can provide, and thus, reduce the amount of frequency regulation reserves necessary. We demonstrate that data center participation in frequency regulation can result in Exogenous carbon savings that oftentimes outweigh Operational carbon emissions.

</details>


### [67] [HetCCL: Accelerating LLM Training with Heterogeneous GPUs](https://arxiv.org/abs/2601.22585)
*Heehoon Kim,Jaehwan Lee,Taejeoung Kim,Jongwon Park,Jinpyo Kim,Pyongwon Suh,Ryan H. Choi,Sangwoo Lee,Jaejin Lee*

Main category: cs.DC

TL;DR: 提出HetCCL库解决跨异构GPU集体通信问题，在多供应商GPU集群评估中表现良好。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习框架缺乏跨异构GPU集体通信支持，导致效率低和成本高。

Method: 提出HetCCL库，统一特定供应商后端，引入两种新机制，利用NVIDIA NCCL和AMD RCCL实现跨供应商通信。

Result: 在多供应商GPU集群评估中，HetCCL在同构环境中性能与NCCL和RCCL相当，在异构环境中可扩展。

Conclusion: HetCCL可在不改变现有深度学习应用情况下，实现NVIDIA和AMD GPU的高性能训练。

Abstract: The rapid growth of large language models is driving organizations to expand their GPU clusters, often with GPUs from multiple vendors. However, current deep learning frameworks lack support for collective communication across heterogeneous GPUs, leading to inefficiency and higher costs. We present HetCCL, a collective communication library that unifies vendor-specific backends and enables RDMA-based communication across GPUs without requiring driver modifications. HetCCL introduces two novel mechanisms that enable cross-vendor communication while leveraging optimized vendor libraries, NVIDIA NCCL and AMD RCCL. Evaluations on a multi-vendor GPU cluster show that HetCCL matches NCCL and RCCL performance in homogeneous setups while uniquely scaling in heterogeneous environments, enabling practical, high-performance training with both NVIDIA and AMD GPUs without changes to existing deep learning applications.

</details>


### [68] [CONCUR: High-Throughput Agentic Batch Inference of LLM via Congestion-Based Concurrency Control](https://arxiv.org/abs/2601.22705)
*Qiaoling Chen,Zhisheng Ye,Tian Tang,Peng Sun,Boyu Tian,Guoteng Wang,Shenggui Li,Yonggang Wen,Zhenhua Han,Tianwei Zhang*

Main category: cs.DC

TL;DR: 论文针对批量推理中GPU键值缓存的中期抖动问题，提出CONCUR，可防止抖动并提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 批量推理中GPU键值缓存存在中期抖动现象，导致吞吐量严重下降，需要解决该问题。

Method: 借鉴分布式系统拥塞控制，提出轻量级控制层CONCUR，用缓存感知控制算法根据运行时缓存信号动态调整活跃代理数量。

Result: 在大模型和真实代理工作负载中，CONCUR能防止中期抖动，在Qwen3 - 32B上提升推理吞吐量达4.09倍，在DeepSeek - V3上提升1.9倍。

Conclusion: CONCUR能有效解决中期抖动问题，提升批处理推理吞吐量，且与现有大语言模型服务系统兼容。

Abstract: Batch inference for agentic workloads stresses the GPU key-value (KV) cache in a sustained and cumulative manner, often causing severe throughput degradation well before memory capacity is exhausted. We identify this phenomenon as middle-phase thrashing, a previously under-characterized pathology in which cache efficiency collapses as long-lived agents accumulate state over time.
  We argue that mitigating this pathology requires moving beyond reactive, request-level cache management to proactive, agent-level admission control. Drawing inspiration from congestion control in distributed systems, we view the KV cache as a shared resource whose efficient utilization depends on feedback-driven regulation. Based on this insight, we present CONCUR, a lightweight control layer that regulates agent admission to bound aggregate cache pressure while preserving execution continuity. CONCUR adapts a cache-aware control algorithm to dynamically adjust the number of active agents using runtime cache signals.
  Across large models and real-world agent workloads, CONCUR prevents middle-phase thrashing and improves batch inference throughput by up to 4.09x on Qwen3-32B and 1.9x on DeepSeek-V3, while remaining compatible with existing LLM serving systems.

</details>


### [69] [ERA: Epoch-Resolved Arbitration for Duelling Admins in Group Management CRDTs](https://arxiv.org/abs/2601.22963)
*Kegan Dougal*

Main category: cs.DC

TL;DR: CRDTs can have 'roll - back' issues in some cases, a Byzantine admin may win the duel, and an external arbiter is needed to improve consistency.


<details>
  <summary>Details</summary>
Motivation: To solve the surprising behavior and problems caused by the possible 'roll - back' of CRDTs, especially in scenarios like the Duelling Admins problem.

Method: Introduce an external arbiter to arbitrate an immutable happens - before relation between concurrent events through asynchronous batch arbitration via optional 'epoch events'.

Result: A bounded total order is introduced within epochs, improving the level of consistency CRDTs can provide.

Conclusion: Using an external arbiter can enhance the consistency of CRDTs while preserving availability.

Abstract: Conflict-Free Replicated Data Types (CRDTs) are used in a range of fields for their coordination-free replication with strong eventual consistency. By prioritising availability over consistency under partition, nodes accumulate events in different orders, and rely on an associative, commutative and idempotent merge function to present a materialised view of the CRDT. Under some circumstances, the state of the materialised view over time can appear to ''roll back'' previously applied events. When the materialised view is used to manage group permissions such as ones found in instant messaging applications, this can lead to surprising behaviour. This can occur when there are multiple concurrent events, such as in the Duelling Admins problem where two equally permissioned admins concurrently revoke each other's permissions. Who wins? This article argues that a Byzantine admin can exploit concurrency to win the duel. As a result, an external arbiter is required to arbitrate an immutable happens-before relation between concurrent events. Arbitration occurs asynchronously in batches via optional ''epoch events'', preserving availability. This introduces a bounded total order within epochs, and the resulting ''finality'' improves on the level of consistency CRDTs can provide.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [70] [Scalable Fair Influence Blocking Maximization via Approximately Monotonic Submodular Optimization](https://arxiv.org/abs/2601.22584)
*Qiangpeng Fang,Jilong Shi,Xiaobin Rui,Jian Zhang,Zhixiao Wang*

Main category: cs.DS

TL;DR: 本文针对现有IBM方法忽视社区公平性问题，提出考虑公平性和影响力阻断效果平衡的方法CELF - R，实验证明其表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有Influence Blocking Maximization (IBM)方法只关注阻断效果，忽略了不同社区间的公平性。

Method: 提出有理论保障的考虑Demographic Parity (DP)的目标函数，通过可调标量化结合阻断效果，开发加速种子选择算法CELF - R。

Result: CELF - R始终优于现有基线方法，能实现(1 - 1/e - ψ)近似解并保持高效率。

Conclusion: 提出的方法在解决IBM公平性问题上取得良好效果，CELF - R算法高效且表现出色。

Abstract: Influence Blocking Maximization (IBM) aims to select a positive seed set to suppress the spread of negative influence. However, existing IBM methods focus solely on maximizing blocking effectiveness, overlooking fairness across communities. To address this issue, we formalize fairness in IBM and justify Demographic Parity (DP) as a notion that is particularly well aligned with its semantics. Yet enforcing DP is computationally challenging: prior work typically formulates DP as a Linear Programming (LP) problem and relies on costly solvers, rendering them impractical for large-scale networks. In this paper, we propose a DP-aware objective while maintaining an approximately monotonic submodular structure, enabling efficient optimization with theoretical guarantees. We integrate this objective with blocking effectiveness through a tunable scalarization, yielding a principled fairness-effectiveness trade-offs. Building on this structure, we develop CELF-R, an accelerated seed selection algorithm that exploits approximate submodularity to eliminate redundant evaluations and naturally supports Pareto front construction. Extensive experiments demonstrate that CELF-R consistently outperforms state-of-the-art baselines, achieving a $(1-1/e-ψ)$-approximate solution while maintaining high efficiency.

</details>


### [71] [Competitive Non-Clairvoyant KV-Cache Scheduling for LLM Inference](https://arxiv.org/abs/2601.22996)
*Yiding Feng,Zonghan Yang,Yuhao Zhang*

Main category: cs.DS

TL;DR: 提出几何切片算法（GSA）解决大语言模型推理调度问题，实现恒定竞争比，还有对应算法GBA，实验证明算法表现稳健。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理调度因KV缓存有挑战，在无作业大小先验知识下获得稳健性能保证是重要开放问题。

Method: 提出GSA算法，通过几何阶段结构和交错管道机制管理不确定性；还有GBA算法。

Result: GSA竞争比最多61.92，大内存下为32；GBA近似比一般为10.67，大内存下为6.75。

Conclusion: 算法在实际请求跟踪实验中表现稳健且保留最坏情况保证。

Abstract: Large Language Model (LLM) inference presents a unique scheduling challenge due to the Key-Value (KV) cache, where a job's memory footprint grows linearly with the number of decoded tokens. This growth couples scheduling decisions with feasibility: a scheduler must minimize latency under a hard memory budget, yet the response lengths of requests are inherently unknown. While recent works have explored this problem either assuming clairvoyance -- exact knowledge of response lengths -- or relying on machine-learned predictions, obtaining robust performance guarantees without any prior knowledge of job sizes remains a theoretically fundamental and practically important open problem.
  In this work, we propose the Geometric Slicing Algorithm (GSA), the non-clairvoyant policy to achieve the first constant competitive ratio for this problem in the offline batch setting. GSA manages uncertainty through a geometric phase structure that periodically restarts jobs to bound memory exposure, combined with a staggered pipeline mechanism that enables high concurrency by smoothing aggregate memory consumption. We prove that GSA achieves a competitive ratio of at most 61.92 for general instances, improving to 32 in the large-memory regime. Our algorithmic framework also yields a clairvoyant counterpart, the Geometric Batching Algorithm (GBA), which achieves an approximation ratio of 10.67 for general instances and 6.75 in the large-memory regime -- significantly improving upon the best previously known bound of over 9000. Numerical experiments on real request traces demonstrate that our algorithms perform robustly while preserving these worst-case guarantees.

</details>


### [72] [Compressed Set Representations based on Set Difference](https://arxiv.org/abs/2601.23240)
*Travis Gagie,Meng He,Gonzalo Navarro*

Main category: cs.DS

TL;DR: 提出集合族的压缩表示，支持对数时间查询，给出新的基于MST的构造算法且表现优。


<details>
  <summary>Details</summary>
Motivation: 探索集合族的高效表示方法以支持集合相关查询。

Method: 利用集合间差异构建压缩表示，采用基于MST的构造算法。

Result: 表示支持访问、成员、前驱和后继查询于对数时间内完成，新算法优于标准算法。

Conclusion: 新的压缩表示及其构造算法在集合族处理上更高效。

Abstract: We introduce a compressed representation of sets of sets that exploits how much they differ from each other. Our representation supports access, membership, predecessor and successor queries on the sets within logarithmic time. In addition, we give a new MST-based construction algorithm for the representation that outperforms standard ones.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [73] [Tacit Coordination of Large Language Models](https://arxiv.org/abs/2601.22184)
*Ido Aharon,Emanuele La Malfa,Michael Wooldridge,Sarit Kraus*

Main category: cs.GT

TL;DR: 研究大语言模型（LLMs）在默契协调博弈中的表现，发现其有协调能力，常超人类，但在涉及数字或细微文化原型的常识协调上失败。


<details>
  <summary>Details</summary>
Motivation: 纯理性解决方案概念在多结果默契协调博弈中无法指导均衡选择，研究LLMs在其中焦点如何出现、何时出现以及为何出现。

Method: 比较和量化LLMs在有人类实验的合作与竞争游戏中的协调能力，引入无学习策略提升协调能力。

Result: 在Llama、Qwen和GPT - oss等模型上发现，LLMs有显著协调能力且常超人类，但在涉及数字或细微文化原型的常识协调上失败。

Conclusion: 本文是在焦点的理论和心理框架内对LLMs默契协调的首次大规模评估。

Abstract: In tacit coordination games with multiple outcomes, purely rational solution concepts, such as Nash equilibria, provide no guidance for which equilibrium to choose. Shelling's theory explains how, in these settings, humans coordinate by relying on focal points: solutions or outcomes that naturally arise because they stand out in some way as salient or prominent to all players. This work studies Large Language Models (LLMs) as players in tacit coordination games, and addresses how, when, and why focal points emerge. We compare and quantify the coordination capabilities of LLMs in cooperative and competitive games for which human experiments are available. We also introduce several learning-free strategies to improve the coordination of LLMs, with themselves and with humans. On a selection of heterogeneous open-source models, including Llama, Qwen, and GPT-oss, we discover that LLMs have a remarkable capability to coordinate and often outperform humans, yet fail on common-sense coordination that involves numbers or nuanced cultural archetypes. This paper constitutes the first large-scale assessment of LLMs' tacit coordination within the theoretical and psychological framework of focal points.

</details>


### [74] [FAIRFORMER: A transformer architecture for discrete fair division](https://arxiv.org/abs/2601.22346)
*Chris Mascioli,Satyam Goyal,Mithun Chakraborty*

Main category: cs.GT

TL;DR: 提出基于深度神经网络的不可分割商品分配方案FairFormer，平衡效率与公平，训练后性能佳，优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决无货币转移、加性主观估值下不可分割商品分配问题，平衡经济效率与基于嫉妒的公平性。

Method: 引入FairFormer，通过双塔transformer编码物品和代理，端到端训练最大化期望对数纳什福利，测试时离散化并进行后处理。

Result: 在测试中实现接近最优的福利（如纳什福利96 - 97%，功利主义福利95 - 96%），在解质量和/或运行时间上优于基线。

Conclusion: 该方法能有效解决商品分配问题，泛化能力强，性能良好。

Abstract: We propose a deep neural network-based solution to the problem of allocating indivisible goods under additive subjective valuations without monetary transfers, trading off economic efficiency with envy-based fairness. We introduce FairFormer, an amortized, permutation-equivariant two-tower transformer that encodes items and agents as unordered token sets, applies self-attention within each set, and uses item-to-agent cross-attention to produce per-item assignment distributions in a single forward pass. FairFormer is trained end-to-end to maximize expected log-Nash welfare on sampled instances, requiring no solver supervision, unrolled allocation procedures, or fairness labels. At test time, we discretize by row-wise $\arg\max$ and apply a lightweight post-processing routine that transfers items to eliminate violations of envy-freeness up to one item while prioritizing improvements in Nash welfare. Our approach generalizes beyond its training regime and achieves near-optimal welfare (e.g., for uniformly sampled valuations, $96$--$97\%$ for Nash welfare; $95$--$96\%$ for utilitarian welfare), outperforming strong baselines in solution quality and/or runtime.

</details>


### [75] [Dynamic Welfare-Maximizing Pooled Testing](https://arxiv.org/abs/2601.22419)
*Nicholas Lopez,Francisco Marmolejo-Cossío,Jose Roberto Tello Ayala,David C. Parkes*

Main category: cs.GT

TL;DR: 研究动态福利最大化的分组检测策略，评估多种策略性能，发现动态检测在低预算下可提升福利，简单贪心策略效果好。


<details>
  <summary>Details</summary>
Motivation: 现有福利最大化分组检测多为静态公式，而本文旨在研究动态策略以最大化社会福利。

Method: 正式定义动态问题，评估多种策略（精确优化基线、贪心启发式、混合整数规划松弛和基于学习的策略），通过合成实验表征性能和权衡。

Result: 动态检测在低预算下能比静态基线大幅提升福利，简单贪心策略表现好，基于学习的方法未可靠改进启发式策略。

Conclusion: 为动态分组检测提供计算视角，明确动态分配在公共卫生筛查中何时能显著改善福利。

Abstract: Pooled testing is a common strategy for public health disease screening under limited testing resources, allowing multiple biological samples to be tested together with the resources of a single test, at the cost of reduced individual resolution. While dynamic and adaptive strategies have been extensively studied in the classical pooled testing literature, where the goal is to minimize the number of tests required for full diagnosis of a given population, much of the existing work on welfare-maximizing pooled testing adopts static formulations in which all tests are assigned in advance. In this paper, we study dynamic welfare-maximizing pooled testing strategies in which a limited number of tests are performed sequentially to maximize social welfare, defined as the aggregate utility of individuals who are confirmed to be healthy. We formally define the dynamic problem and study algorithmic approaches for sequential test assignment. Because exact dynamic optimization is computationally infeasible beyond small instances, we evaluate a range of strategies (including exact optimization baselines, greedy heuristics, mixed-integer programming relaxations, and learning-based policies) and empirically characterize their performance and tradeoffs using synthetic experiments. Our results show that dynamic testing can yield substantial welfare improvements over static baselines in low-budget regimes. We find that much of the benefit of dynamic testing is captured by simple greedy policies, which substantially outperform static approaches while remaining computationally efficient. Learning-based methods are included as flexible baselines, but in our experiments they do not reliably improve upon these heuristics. Overall, this work provides a principled computational perspective on dynamic pooled testing and clarifies when dynamic assignment meaningfully improves welfare in public health screening.

</details>


### [76] [Do AI Overviews Benefit Search Engines? An Ecosystem Perspective](https://arxiv.org/abs/2601.22493)
*Yihang Wu,Jiajun Tang,Jinfei Liu,Haifeng Xu,Fan Yao*

Main category: cs.GT

TL;DR: AI综述集成到搜索引擎有弊端，提出博弈论模型和两种激励机制，评估显示干预可增长期利润。


<details>
  <summary>Details</summary>
Motivation: 解决AI综述集成到搜索引擎带来的创作者积极性降低、用户流失和搜索引擎长期利润受损问题。

Method: 提出带成本努力的创作者竞争博弈论模型，设计引用机制和补偿机制。

Result: 评估显示基于机制的干预能在现实场景中增加搜索引擎长期利润。

Conclusion: 所提机制使AI增强的搜索生态更具可持续性。

Abstract: The integration of AI Overviews into search engines enhances user experience but diverts traffic from content creators, potentially discouraging high-quality content creation and causing user attrition that undermines long-term search engine profit. To address this issue, we propose a game-theoretic model of creator competition with costly effort, characterize equilibrium behavior, and design two incentive mechanisms: a citation mechanism that references sources within an AI Overview, and a compensation mechanism that offers monetary rewards to creators. For both cases, we provide structural insights and near-optimal profit-maximizing mechanisms. Evaluations on real click data show that although AI Overviews harm long-term search engine profit, interventions based on our proposed mechanisms can increase long-term profit across a range of realistic scenarios, pointing toward a more sustainable trajectory for AI-enhanced search ecosystems.

</details>


### [77] [Greedy Routing Reachability Games](https://arxiv.org/abs/2601.23126)
*Pascal Lenzner,Paraskevi Machaira*

Main category: cs.GT

TL;DR: 论文研究自治代理形成支持贪婪路由网络的博弈论模型，分析有向和无向边的两种变体情况。


<details>
  <summary>Details</summary>
Motivation: 当前网络由众多自治实体组成，各实体通常只有局部视图，需依赖局部路由协议，研究贪婪路由网络的形成本质。

Method: 构建博弈论模型，研究有向边和无向边两种变体。

Result: 有向边场景中，存在均衡且总成本最优，欧几里得度量中可有效找到，但计算最优策略是NP难问题；无向边场景中，二维欧氏空间中无政府价格在1.75 - 1.8之间，高维小于2，最佳响应动态可能循环，欧氏空间中可多项式时间计算近似均衡且二维中优于Delaunay三角剖分。

Conclusion: 该模型能在不假设代理分布和具体贪婪路由协议的情况下，揭示贪婪路由工作的本质，不同场景有相应结果和特点。

Abstract: Today's networks consist of many autonomous entities that follow their own objectives, i.e., smart devices or parts of large AI systems, that are interconnected. Given the size and complexity of most communication networks, each entity typically only has a local view and thus must rely on a local routing protocol for sending and forwarding packets. A common solution for this is greedy routing, where packets are locally forwarded to a neighbor in the network that is closer to the packet's destination.
  In this paper we investigate a game-theoretic model with autonomous agents that aim at forming a network where greedy routing is enabled. The agents are positioned in a metric space and each agent tries to establish as few links as possible, while maintaining that it can reach every other agent via greedy routing. Thus, this model captures how greedy routing networks are formed without any assumption on the distribution of the agents or the specific employed greedy routing protocol. Hence, it distills the essence that makes greedy routing work.
  We study two variants of the model: with directed edges or with undirected edges. For the former, we show that equilibria exist, have optimal total cost, and that in Euclidean metrics they can be found efficiently. However, even for this simple setting computing optimal strategies is NP-hard. For the much more challenging setting with undirected edges, we show for the realistic setting with agents in 2D Euclidean space that the price of anarchy is between 1.75 and 1.8 and for higher dimensions it is less than 2. Also, we show that best response dynamics may cycle, but that in Euclidean space almost optimal approximate equilibria can be computed in polynomial time. Moreover, for 2D Euclidean space, these approximate equilibria outperform the well-known Delaunay triangulation.

</details>


### [78] [(Doubly) Exponential Lower Bounds for Follow the Regularized Leader in Potential Games](https://arxiv.org/abs/2601.23248)
*Ioannis Anagnostides,Ioannis Panageas,Nikolas Patris,Tuomas Sandholm*

Main category: cs.GT

TL;DR: 本文研究FTRL算法在受限优化和潜在博弈中的收敛性，给出其在二人和多人潜在博弈中收敛到纳什均衡的时间上下界。


<details>
  <summary>Details</summary>
Motivation: 尽管对FTRL在受限优化（特别是潜在博弈）中的收敛性研究了数十年，但此前人们对其行为了解不足。

Method: 理论分析，通过建立FTRL的相关性质来推导收敛时间上下界。

Result: 在二人潜在博弈中，FTRL收敛到纳什均衡可能需指数时间；对于以懒惰、交替方式执行的无悔动力学有指数上界；在多人潜在博弈中，虚拟博弈可能需双指数时间到达纳什均衡。

Conclusion: 给出了FTRL及其相关算法在不同类型潜在博弈中收敛到纳什均衡的时间复杂度界限。

Abstract: Follow the regularized leader FTRL is the premier algorithm for online optimization. However, despite decades of research on its convergence in constrained optimization -- and potential games in particular -- its behavior remained hitherto poorly understood. In this paper, we establish that FTRL can take exponential time to converge to a Nash equilibrium in two-player potential games for any (permutation-invariant) regularizer and potentially vanishing learning rate. By known equivalences, this translates to an exponential lower bound for certain mirror descent counterparts, most notably multiplicative weights update. On the positive side, we establish the potential property for FTRL and obtain an exponential upper bound $\exp(O_ε(1/ε^2))$ for any no-regret dynamics executed in a lazy, alternating fashion, matching our lower bound up to factors in the exponent. Finally, in multi-player potential games, we show that fictitious play -- the extreme version of FTRL -- can take doubly exponential time to reach a Nash equilibrium. This constitutes an exponentially stronger lower bound for the foundational learning algorithm in games.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [79] [FITMM: Adaptive Frequency-Aware Multimodal Recommendation via Information-Theoretic Representation Learning](https://arxiv.org/abs/2601.22498)
*Wei Yang,Rui Zhong,Yiqun Chen,Shixuan Li,Heng Ping,Chi Lu,Peng Jiang*

Main category: cs.IR

TL;DR: 本文提出FITMM框架用于多模态推荐，通过频域处理优化模型，实验显示其优于先进基线。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推荐系统在空间域融合模态，掩盖信号频率结构，增加不对齐和冗余。

Method: 采用频谱信息论视角，提出FITMM框架，构建图增强的项目表示，进行模态频谱分解，形成轻量级带内多模态组件，用残差、任务自适应门聚合频带，用频域IB项和跨模态频谱一致性损失进行正则化训练。

Result: 在三个真实数据集上的广泛实验表明，FITMM始终显著优于先进基线。

Conclusion: FITMM框架在多模态推荐任务中具有有效性和优越性。

Abstract: Multimodal recommendation aims to enhance user preference modeling by leveraging rich item content such as images and text. Yet dominant systems fuse modalities in the spatial domain, obscuring the frequency structure of signals and amplifying misalignment and redundancy. We adopt a spectral information-theoretic view and show that, under an orthogonal transform that approximately block-diagonalizes bandwise covariances, the Gaussian Information Bottleneck objective decouples across frequency bands, providing a principled basis for separate-then-fuse paradigm. Building on this foundation, we propose FITMM, a Frequency-aware Information-Theoretic framework for multimodal recommendation. FITMM constructs graph-enhanced item representations, performs modality-wise spectral decomposition to obtain orthogonal bands, and forms lightweight within-band multimodal components. A residual, task-adaptive gate aggregates bands into the final representation. To control redundancy and improve generalization, we regularize training with a frequency-domain IB term that allocates capacity across bands (Wiener-like shrinkage with shut-off of weak bands). We further introduce a cross-modal spectral consistency loss that aligns modalities within each band. The model is jointly optimized with the standard recommendation loss. Extensive experiments on three real-world datasets demonstrate that FITMM consistently and significantly outperforms advanced baselines.

</details>


### [80] [SCaLRec: Semantic Calibration for LLM-enabled Cloud-Device Sequential Recommendation](https://arxiv.org/abs/2601.22543)
*Ruiqi Zheng,Jinli Cao,Jiao Yin,Hongzhi Yin*

Main category: cs.IR

TL;DR: 论文指出云设备协同推荐中缓存云语义嵌入存在陈旧性问题，提出SCaLRec方法解决相关挑战，实验表明其能提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的云设备推荐器在缓存语义重用时面临判定缓存语义可用性和维持排名质量的挑战，需解决云语义陈旧性问题。

Method: 提出Semantic Calibration for LLM - enabled Cloud - Device Recommendation (SCaLRec)方法，先估计缓存语义可靠性，再用设备端语义校准模块调整缓存语义嵌入。

Result: 在真实数据集实验中，SCaLRec在云语义陈旧情况下比强基线模型持续提升推荐性能。

Conclusion: SCaLRec方法能有效应对云设备协同推荐中云语义陈旧性问题，提升推荐效果。

Abstract: Cloud-device collaborative recommendation partitions computation across the cloud and user devices: the cloud provides semantic user modeling, while the device leverages recent interactions and cloud semantic signals for privacy-preserving, responsive reranking. With large language models (LLMs) on the cloud, semantic user representations can improve sequential recommendation by capturing high-level intent. However, regenerating such representations via cloud LLM inference for every request is often infeasible at real-world scale. As a result, on-device reranking commonly reuses a cached cloud semantic user embedding across requests. We empirically identify a cloud semantic staleness effect: reused embeddings become less aligned with the user's latest interactions, leading to measurable ranking degradation.
  Most existing LLM-enabled cloud-device recommenders are typically designed around on-demand cloud semantics, either by assuming low-latency cloud LLM access or by regenerating semantic embeddings per request. When per-request regeneration is infeasible and cached semantics must be reused, two technical challenges arise: (1) deciding when cached cloud semantics remain useful for on-device reranking, and (2) maintaining ranking quality when the cloud LLM cannot be invoked and only cached semantics are available. To address this gap, we introduce the Semantic Calibration for LLM-enabled Cloud-Device Recommendation (SCaLRec). First, it estimates the reliability of cached semantics under the user's latest interactions. Second, an on-device semantic calibration module is proposed to adjusts the cached semantic embedding on-device using up-to-date interaction evidence, without per-request cloud LLM involvement. Experiments on real-world datasets show that SCaLRec consistently improves recommendation performance over strong baselines under cloud semantic staleness.

</details>


### [81] [PersonaAct: Simulating Short-Video Users with Personalized Agents for Counterfactual Filter Bubble Auditing](https://arxiv.org/abs/2601.22547)
*Shilong Zhao,Qinggang Yang,Zhiyi Yin,Xiaoshi Wang,Zhenxing Chen,Du Su,Xueqi Cheng*

Main category: cs.IR

TL;DR: 提出PersonaAct框架模拟短视频用户以审计过滤气泡，评估显示其保真度有提升，揭示了内容变窄现象，发现B站逃逸潜力最强，并开源数据集和代码。


<details>
  <summary>Details</summary>
Motivation: 短视频平台基于个性化推荐引发过滤气泡问题，大规模审计该现象面临挑战，现有模拟器存在不足。

Method: 提出PersonaAct框架，通过自动化访谈合成可解释的角色，使用监督微调与强化学习训练多模态代理，用训练好的代理审计过滤气泡，从内容多样性和逃逸潜力评估气泡的广度和深度。

Result: 评估显示相比于通用大语言模型基线，保真度有显著提升，能复现真实行为；揭示了交互过程中内容显著变窄，发现B站逃逸潜力最强。

Conclusion: PersonaAct能有效审计过滤气泡，开源数据集和代码支持推荐系统的可复现审计。

Abstract: Short-video platforms rely on personalized recommendation, raising concerns about filter bubbles that narrow content exposure. Auditing such phenomena at scale is challenging because real user studies are costly and privacy-sensitive, and existing simulators fail to reproduce realistic behaviors due to their reliance on textual signals and weak personalization. We propose PersonaAct, a framework for simulating short-video users with persona-conditioned multimodal agents trained on real behavioral traces for auditing filter bubbles in breadth and depth. PersonaAct synthesizes interpretable personas through automated interviews combining behavioral analysis with structured questioning, then trains agents on multimodal observations using supervised fine-tuning and reinforcement learning. We deploy trained agents for filter bubble auditing and evaluate bubble breadth via content diversity and bubble depth via escape potential. The evaluation demonstrates substantial improvements in fidelity over generic LLM baselines, enabling realistic behavior reproduction. Results reveal significant content narrowing over interaction. However, we find that Bilibili demonstrates the strongest escape potential. We release the first open multimodal short-video dataset and code to support reproducible auditing of recommender systems.

</details>


### [82] [Farewell to Item IDs: Unlocking the Scaling Potential of Large Ranking Models via Semantic Tokens](https://arxiv.org/abs/2601.22694)
*Zhen Zhao,Tong Zhang,Jie Xu,Qingliang Cai,Qile Zhang,Leyuan Yang,Daorui Xiao,Xiaojia Chang*

Main category: cs.IR

TL;DR: 研究表明语义标记比物品ID有更大扩展潜力，提出TRM框架，减少稀疏存储、提升AUC，实验表现优，已部署到搜索引擎。


<details>
  <summary>Details</summary>
Motivation: 多数大规模排序系统依赖物品ID，物品快速增减使嵌入难以训练和维护，阻碍神经网络参数学习和模型扩展。

Method: 提出TRM框架，改进标记生成和应用流程。

Result: 减少33%稀疏存储，AUC提升0.85%；模型容量扩展时TRM表现优于现有模型；部署到搜索引擎，用户活跃天数和查询更改率分别提升0.26%和0.75%。

Conclusion: 语义标记比物品ID有更大扩展潜力，TRM框架有效提升性能，有实际应用价值。

Abstract: Recent studies on scaling up ranking models have achieved substantial improvement for recommendation systems and search engines. However, most large-scale ranking systems rely on item IDs, where each item is treated as an independent categorical symbol and mapped to a learned embedding. As items rapidly appear and disappear, these embeddings become difficult to train and maintain. This instability impedes effective learning of neural network parameters and limits the scalability of ranking models. In this paper, we show that semantic tokens possess greater scaling potential compared to item IDs. Our proposed framework TRM improves the token generation and application pipeline, leading to 33% reduction in sparse storage while achieving 0.85% AUC increase. Extensive experiments further show that TRM could consistently outperform state-of-the-art models when model capacity scales. Finally, TRM has been successfully deployed on large-scale personalized search engines, yielding 0.26% and 0.75% improvement on user active days and change query ratio respectively through A/B test.

</details>


### [83] [Compact Hypercube Embeddings for Fast Text-based Wildlife Observation Retrieval](https://arxiv.org/abs/2601.22783)
*Ilyass Moummad,Marius Miron,David Robinson,Kawtar Zaher,Hervé Goëau,Olivier Pietquin,Pierre Bonnet,Emmanuel Chemla,Matthieu Geist,Alexis Joly*

Main category: cs.IR

TL;DR: 提出紧凑超立方体嵌入用于快速基于文本的野生动物观测检索，在大规模基准测试中表现良好，能降低成本并提升检索和泛化性能。


<details>
  <summary>Details</summary>
Motivation: 当前大规模生物多样性监测平台虽依赖多模态野生动物观测，但从海量档案中检索相关观测因高维相似性搜索的计算成本而具有挑战性。

Method: 基于跨视图代码对齐哈希框架，将轻量级哈希扩展到多模态，利用预训练的野生动物基础模型，通过参数高效微调进行哈希。

Result: 离散超立方体嵌入的检索性能与连续嵌入相当甚至更优，大幅降低了内存和搜索成本，哈希目标提升了编码器表示。

Conclusion: 基于二进制、语言的检索能为生物多样性监测系统实现对大型野生动物档案的可扩展和高效搜索。

Abstract: Large-scale biodiversity monitoring platforms increasingly rely on multimodal wildlife observations. While recent foundation models enable rich semantic representations across vision, audio, and language, retrieving relevant observations from massive archives remains challenging due to the computational cost of high-dimensional similarity search. In this work, we introduce compact hypercube embeddings for fast text-based wildlife observation retrieval, a framework that enables efficient text-based search over large-scale wildlife image and audio databases using compact binary representations. Building on the cross-view code alignment hashing framework, we extend lightweight hashing beyond a single-modality setup to align natural language descriptions with visual or acoustic observations in a shared Hamming space. Our approach leverages pretrained wildlife foundation models, including BioCLIP and BioLingual, and adapts them efficiently for hashing using parameter-efficient fine-tuning. We evaluate our method on large-scale benchmarks, including iNaturalist2024 for text-to-image retrieval and iNatSounds2024 for text-to-audio retrieval, as well as multiple soundscape datasets to assess robustness under domain shift. Results show that retrieval using discrete hypercube embeddings achieves competitive, and in several cases superior, performance compared to continuous embeddings, while drastically reducing memory and search cost. Moreover, we observe that the hashing objective consistently improves the underlying encoder representations, leading to stronger retrieval and zero-shot generalization. These results demonstrate that binary, language-based retrieval enables scalable and efficient search over large wildlife archives for biodiversity monitoring systems.

</details>


### [84] [BEAR: Towards Beam-Search-Aware Optimization for Recommendation with Large Language Models](https://arxiv.org/abs/2601.22925)
*Weiqin Yang,Bohao Wang,Zhenxiang Xu,Jiawei Chen,Shengjia Zhang,Jingbang Chen,Canghong Jin,Can Wang*

Main category: cs.IR

TL;DR: 现有用大语言模型做推荐的方法存在训练推理不一致问题，提出BEAR方法解决，实验显示其优于基线。


<details>
  <summary>Details</summary>
Motivation: 当前利用大语言模型进行推荐的方法存在训练推理不一致问题，SFT不能保证正样本被波束搜索检索到。

Method: 提出BEAR，一种新的微调目标，在训练中考虑波束搜索行为，通过让正样本的每个token在每一步解码时排在前B个候选token内。

Result: 在四个真实数据集上的大量实验表明，BEAR显著优于强大的基线。

Conclusion: BEAR能有效缓解错误剪枝问题，且计算开销小，是解决训练推理不一致问题的有效方法。

Abstract: Recent years have witnessed a rapid surge in research leveraging Large Language Models (LLMs) for recommendation. These methods typically employ supervised fine-tuning (SFT) to adapt LLMs to recommendation scenarios, and utilize beam search during inference to efficiently retrieve $B$ top-ranked recommended items. However, we identify a critical training-inference inconsistency: while SFT optimizes the overall probability of positive items, it does not guarantee that such items will be retrieved by beam search even if they possess high overall probabilities. Due to the greedy pruning mechanism, beam search can prematurely discard a positive item once its prefix probability is insufficient.
  To address this inconsistency, we propose BEAR (Beam-SEarch-Aware Regularization), a novel fine-tuning objective that explicitly accounts for beam search behavior during training. Rather than directly simulating beam search for each instance during training, which is computationally prohibitive, BEAR enforces a relaxed necessary condition: each token in a positive item must rank within the top-$B$ candidate tokens at each decoding step. This objective effectively mitigates the risk of incorrect pruning while incurring negligible computational overhead compared to standard SFT. Extensive experiments across four real-world datasets demonstrate that BEAR significantly outperforms strong baselines. Code will be released upon acceptance.

</details>


### [85] [OrLog: Resolving Complex Queries with LLMs and Probabilistic Reasoning](https://arxiv.org/abs/2601.23085)
*Mohanna Hoveyda,Jelle Piepenbrock,Arjen P de Vries,Maarten de Rijke,Faegheh Hasibi*

Main category: cs.IR

TL;DR: 提出神经符号检索框架OrLog，可在信息检索中处理复杂逻辑约束，评估显示其能提升精度且更高效。


<details>
  <summary>Details</summary>
Motivation: 当前检索系统在处理查询中的逻辑运算符约束时存在问题，现有神经符号方法不适用于信息检索。

Method: 引入OrLog框架，通过大语言模型提供原子谓词的可信度分数，由概率推理引擎得出查询满足的后验概率。

Result: 在多个骨干大语言模型、不同外部知识访问水平和逻辑约束下评估，OrLog能显著提升排名精度，尤其在析取查询上，且更高效，减少约90%的平均令牌数。

Conclusion: 无生成的谓词可信度估计与概率推理结合实现了约束感知检索，表现优于整体推理且使用更少令牌。

Abstract: Resolving complex information needs that come with multiple constraints should consider enforcing the logical operators encoded in the query (i.e., conjunction, disjunction, negation) on the candidate answer set. Current retrieval systems either ignore these constraints in neural embeddings or approximate them in a generative reasoning process that can be inconsistent and unreliable. Although well-suited to structured reasoning, existing neuro-symbolic approaches remain confined to formal logic or mathematics problems as they often assume unambiguous queries and access to complete evidence, conditions rarely met in information retrieval. To bridge this gap, we introduce OrLog, a neuro-symbolic retrieval framework that decouples predicate-level plausibility estimation from logical reasoning: a large language model (LLM) provides plausibility scores for atomic predicates in one decoding-free forward pass, from which a probabilistic reasoning engine derives the posterior probability of query satisfaction. We evaluate OrLog across multiple backbone LLMs, varying levels of access to external knowledge, and a range of logical constraints, and compare it against base retrievers and LLM-as-reasoner methods. Provided with entity descriptions, OrLog can significantly boost top-rank precision compared to LLM reasoning with larger gains on disjunctive queries. OrLog is also more efficient, cutting mean tokens by $\sim$90\% per query-entity pair. These results demonstrate that generation-free predicate plausibility estimation combined with probabilistic reasoning enables constraint-aware retrieval that outperforms monolithic reasoning while using far fewer tokens.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [86] [Attention Isn't All You Need for Emotion Recognition:Domain Features Outperform Transformers on the EAV Dataset](https://arxiv.org/abs/2601.22161)
*Anmol Guragain*

Main category: cs.LG

TL;DR: 研究在小数据集上多模态情感识别，发现复杂注意力机制表现不佳，特定领域知识和恰当实现更有效。


<details>
  <summary>Details</summary>
Motivation: 探究复杂注意力机制在小数据集上对多模态情感识别性能的影响。

Method: 实现三种模型类别，包括基线transformer、新型因式分解注意力机制和改进的CNN基线。

Result: 复杂注意力机制在小数据集上表现不佳，简单的特定领域修改有效，各模型有相应准确率提升。

Conclusion: 对于小规模情感识别，领域知识和恰当实现优于架构复杂性。

Abstract: We present a systematic study of multimodal emotion recognition using the EAV dataset, investigating whether complex attention mechanisms improve performance on small datasets. We implement three model categories: baseline transformers (M1), novel factorized attention mechanisms (M2), and improved CNN baselines (M3). Our experiments show that sophisticated attention mechanisms consistently underperform on small datasets. M2 models achieved 5 to 13 percentage points below baselines due to overfitting and destruction of pretrained features. In contrast, simple domain-appropriate modifications proved effective: adding delta MFCCs to the audio CNN improved accuracy from 61.9\% to \textbf{65.56\%} (+3.66pp), while frequency-domain features for EEG achieved \textbf{67.62\%} (+7.62pp over the paper baseline). Our vision transformer baseline (M1) reached \textbf{75.30\%}, exceeding the paper's ViViT result (74.5\%) through domain-specific pretraining, and vision delta features achieved \textbf{72.68\%} (+1.28pp over the paper CNN). These findings demonstrate that for small-scale emotion recognition, domain knowledge and proper implementation outperform architectural complexity.

</details>


### [87] [Multitask Learning for Earth Observation Data Classification with Hybrid Quantum Network](https://arxiv.org/abs/2601.22195)
*Fan Fan,Yilei Shi,Tobias Guggemos,Xiao Xiang Zhu*

Main category: cs.LG

TL;DR: 该论文提出一种结合多任务学习和量子卷积操作的混合模型用于地球观测数据分类，验证其有效性并探究QML潜力。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习受关注，地球观测进入大数据时代，深度学习模型分析数据计算需求成瓶颈，因而想用量子计算用于地球观测数据分类并探索其优势。

Method: 提出结合多任务学习辅助数据编码，采用位置权重模块和量子卷积操作提取特征的混合模型。

Result: 用多个地球观测基准评估了模型有效性，实验探索了模型泛化性及优势因素。

Conclusion: 凸显了量子机器学习在地球观测数据分析中的潜力。

Abstract: Quantum machine learning (QML) has gained increasing attention as a potential solution to address the challenges of computation requirements in the future. Earth observation (EO) has entered the era of Big Data, and the computational demands for effectively analyzing large EO data with complex deep learning models have become a bottleneck. Motivated by this, we aim to leverage quantum computing for EO data classification and explore its advantages despite the current limitations of quantum devices. This paper presents a hybrid model that incorporates multitask learning to assist efficient data encoding and employs a location weight module with quantum convolution operations to extract valid features for classification. The validity of our proposed model was evaluated using multiple EO benchmarks. Additionally, we experimentally explored the generalizability of our model and investigated the factors contributing to its advantage, highlighting the potential of QML in EO data analysis.

</details>


### [88] [Neural Signals Generate Clinical Notes in the Wild](https://arxiv.org/abs/2601.22197)
*Jathurshan Pradeepkumar,Zheng Chen,Jimeng Sun*

Main category: cs.LG

TL;DR: 本文构建大规模临床EEG数据集，开发CELM模型用于临床报告生成，实验显示其性能提升显著，并开源模型和基准构建管道。


<details>
  <summary>Details</summary>
Motivation: 从长期EEG记录生成临床报告工作劳动强度大，需要高效方法。

Method: 构建含9922份报告和约11000小时EEG记录的大规模数据集，开发CELM模型，集成预训练EEG基础模型与语言模型实现可扩展多模态学习。

Result: 有患者病史监督时，标准生成指标平均相对提升70%-95%；零样本无病史设置下，CELM生成得分高于基线。

Conclusion: CELM模型能有效总结长时、可变长度EEG记录并生成临床报告，性能表现良好。

Abstract: Generating clinical reports that summarize abnormal patterns, diagnostic findings, and clinical interpretations from long-term EEG recordings remains labor-intensive. We curate a large-scale clinical EEG dataset with $9{,}922$ reports paired with approximately $11{,}000$ hours of EEG recordings from $9{,}048$ patients. We therefore develop CELM, the first clinical EEG-to-Language foundation model capable of summarizing long-duration, variable-length EEG recordings and performing end-to-end clinical report generation at multiple scales, including recording description, background activity, epileptiform abnormalities, events/seizures, and impressions. Experimental results show that, with patient history supervision, our method achieves $70\%$--$95\%$ average relative improvements in standard generation metrics (e.g., ROUGE-1 and METEOR) from $0.2$--$0.3$ to $0.4$--$0.6$. In the zero-shot setting without patient history, CELM attains generation scores in the range of $0.43$--$0.52$, compared to baselines of $0.17$--$0.26$. CELM integrates pretrained EEG foundation models with language models to enable scalable multimodal learning. We release our model and benchmark construction pipeline at [URL].

</details>


### [89] [FedAdaVR: Adaptive Variance Reduction for Robust Federated Learning under Limited Client Participation](https://arxiv.org/abs/2601.22204)
*S M Ruhul Kabir Howlader,Xiao Chen,Yifei Xie,Lu Liu*

Main category: cs.LG

TL;DR: 提出FedAdaVR和FedAdaVR - Quant算法解决联邦学习中客户端参与不均导致的异质性问题，FedAdaVR可消除部分客户端参与误差，FedAdaVR - Quant减少内存需求，实验显示FedAdaVR表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习因异质性面临诸多挑战，尤其是部分客户端参与误差问题在现有文献中未得到充分解决。

Method: 提出FedAdaVR算法，结合自适应优化器和方差减少技术，利用客户端最新存储的更新；提出FedAdaVR - Quant算法，以量化形式存储客户端更新。

Result: FedAdaVR能消除部分客户端参与误差；FedAdaVR - Quant可将内存需求分别降低50%、75%和87.5%且保持模型性能；在多数据集上实验表明FedAdaVR优于基线方法。

Conclusion: 提出的算法有效解决了部分客户端参与导致的异质性问题，在性能和内存优化方面表现良好。

Abstract: Federated learning (FL) encounters substantial challenges due to heterogeneity, leading to gradient noise, client drift, and partial client participation errors, the last of which is the most pervasive but remains insufficiently addressed in current literature. In this paper, we propose FedAdaVR, a novel FL algorithm aimed at solving heterogeneity issues caused by sporadic client participation by incorporating an adaptive optimiser with a variance reduction technique. This method takes advantage of the most recent stored updates from clients, even when they are absent from the current training round, thereby emulating their presence. Furthermore, we propose FedAdaVR-Quant, which stores client updates in quantised form, significantly reducing the memory requirements (by 50%, 75%, and 87.5%) of FedAdaVR while maintaining equivalent model performance. We analyse the convergence behaviour of FedAdaVR under general nonconvex conditions and prove that our proposed algorithm can eliminate partial client participation error. Extensive experiments conducted on multiple datasets, under both independent and identically distributed (IID) and non-IID settings, demonstrate that FedAdaVR consistently outperforms state-of-the-art baseline methods.

</details>


### [90] [Causal Imitation Learning Under Measurement Error and Distribution Shift](https://arxiv.org/abs/2601.22206)
*Shi Bo,AmirEmad Ghassami*

Main category: cs.LG

TL;DR: 研究有测量误差和分布转移时离线模仿学习，提出因果解释、抗分布转移的框架CausIL并验证效果。


<details>
  <summary>Details</summary>
Motivation: 部分决策相关状态通过噪声测量观察且分布有变化时，标准行为克隆在分布转移下会产生系统偏差策略。

Method: 提出一般框架，引入CausIL将噪声观测作为代理变量，给出识别条件，开发离散和连续状态空间的估计器，用对抗程序学参数。

Result: 在半模拟纵向数据上评估CausIL，与基线相比在分布转移下鲁棒性提升。

Conclusion: CausIL能有效应对有测量误差和分布转移的离线模仿学习场景。

Abstract: We study offline imitation learning (IL) when part of the decision-relevant state is observed only through noisy measurements and the distribution may change between training and deployment. Such settings induce spurious state-action correlations, so standard behavioral cloning (BC) -- whether conditioning on raw measurements or ignoring them -- can converge to systematically biased policies under distribution shift. We propose a general framework for IL under measurement error, inspired by explicitly modeling the causal relationships among the variables, yielding a target that retains a causal interpretation and is robust to distribution shift. Building on ideas from proximal causal inference, we introduce \texttt{CausIL}, which treats noisy state observations as proxy variables, and we provide identification conditions under which the target policy is recoverable from demonstrations without rewards or interactive expert queries. We develop estimators for both discrete and continuous state spaces; for continuous settings, we use an adversarial procedure over RKHS function classes to learn the required parameters. We evaluate \texttt{CausIL} on semi-simulated longitudinal data from the PhysioNet/Computing in Cardiology Challenge 2019 cohort and demonstrate improved robustness to distribution shift compared to BC baselines.

</details>


### [91] [Scalable Batch Correction for Cell Painting via Batch-Dependent Kernels and Adaptive Sampling](https://arxiv.org/abs/2601.22331)
*Aditya Narayan Ravi,Snehal Vadvalkar,Abhishek Pandey,Ilan Shomorony*

Main category: cs.LG

TL;DR: 提出BALANS批处理校正方法，可处理细胞图像数据批效应，实验证明其高效且不牺牲校正质量。


<details>
  <summary>Details</summary>
Motivation: 大规模细胞图像数据受批效应影响，掩盖生物信号，需有效批处理校正方法。

Method: 构建平滑亲和矩阵，使用批感知局部尺度计算矩阵元素，采用自适应采样程序生成稀疏矩阵，证明采样策略最优及近似保证。

Result: 在真实和合成数据集实验表明，BALANS可处理大规模数据，相比常用方法提高运行时间，且不牺牲校正质量。

Conclusion: BALANS是一种高效、可扩展的细胞图像数据批处理校正方法。

Abstract: Cell Painting is a microscopy-based, high-content imaging assay that produces rich morphological profiles of cells and can support drug discovery by quantifying cellular responses to chemical perturbations. At scale, however, Cell Painting data is strongly affected by batch effects arising from differences in laboratories, instruments, and protocols, which can obscure biological signal. We present BALANS (Batch Alignment via Local Affinities and Subsampling), a scalable batch-correction method that aligns samples across batches by constructing a smoothed affinity matrix from pairwise distances. Given $n$ data points, BALANS builds a sparse affinity matrix $A \in \mathbb{R}^{n \times n}$ using two ideas. (i) For points $i$ and $j$, it sets a local scale using the distance from $i$ to its $k$-th nearest neighbor within the batch of $j$, then computes $A_{ij}$ via a Gaussian kernel calibrated by these batch-aware local scales. (ii) Rather than forming all $n^2$ entries, BALANS uses an adaptive sampling procedure that prioritizes rows with low cumulative neighbor coverage and retains only the strongest affinities per row, yielding a sparse but informative approximation of $A$. We prove that this sampling strategy is order-optimal in sample complexity and provides an approximation guarantee, and we show that BALANS runs in nearly linear time in $n$. Experiments on diverse real-world Cell Painting datasets and controlled large-scale synthetic benchmarks demonstrate that BALANS scales to large collections while improving runtime over native implementations of widely used batch-correction methods, without sacrificing correction quality.

</details>


### [92] [SCALAR: Quantifying Structural Hallucination, Consistency, and Reasoning Gaps in Materials Foundation Models](https://arxiv.org/abs/2601.22312)
*Can Polat,Erchin Serpedin,Mustafa Kurban,Hasan Kurban*

Main category: cs.LG

TL;DR: 本文提出SCALAR基准评估材料基础模型的几何尺度泛化能力，实验表明不能仅从准确率推断几何尺度泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于材料科学推理时，其在物理结构分布变化下的行为尚不清楚，需评估材料基础模型的几何尺度泛化能力。

Method: 提出SCALAR基准，定义三个任务，用结构化指标评估模型输出。

Result: 不同基础模型在显式推理下有模型依赖的大变化，减少幻觉和误差，但常破坏一致性或有效性。

Conclusion: 不能仅从准确率推断几何尺度泛化能力。

Abstract: Large language models are increasingly applied to materials science reasoning, yet their behavior under physically structured distribution shifts remains poorly understood. We introduce SCALAR (Structural Consistency And Logic Across Regimes), a benchmark for evaluating geometric scale generalization and its connection to structural hallucination, consistency, and reasoning in materials foundation models. Given canonical crystal representations, models must reason about derived nanoparticle structures obtained through supercell expansion and geometric truncation across length scales spanning a few atoms to over 18,000 atoms, totaling $\approx$100,000 structures from DFT-validated unit cells. SCALAR defines three tasks. (i) CIF to property prediction. (ii) A Chain-of-Thought variant with explicit physics-grounded reasoning. (iii) Inverse retrieval identifying crystals from candidates given target properties. Outputs are evaluated via structured metrics capturing numeric error, hallucination, cross-prompt consistency, monotonic reasoning, output validity, and retrieval regret. Experiments across diverse foundation models reveal large, model-dependent shifts under explicit reasoning, often reducing hallucination and error, but frequently destabilizing consistency or validity. These results demonstrate that geometric scale generalization cannot be inferred from accuracy alone. Supplementary materials are available at https://github.com/KurbanIntelligenceLab/SCALAR.

</details>


### [93] [Latent Spherical Flow Policy for Reinforcement Learning with Combinatorial Actions](https://arxiv.org/abs/2601.22211)
*Lingkai Kong,Anagha Satish,Hezi Jiang,Akseli Kangaslahti,Andrew Ma,Wenbo Chen,Mingxiao Song,Lily Xu,Milind Tambe*

Main category: cs.LG

TL;DR: 提出求解器诱导的潜在球形流策略LSFlow用于组合强化学习，在潜在空间学习随机策略，效率高且表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 组合动作空间的强化学习因可行动作集大且受复杂约束，现有方法牺牲通用性和策略表达性，需更好方法。

Method: 学习潜在空间的随机策略，通过球形流匹配，借助组合优化求解器确保动作可行；在潜在空间训练价值网络；引入平滑贝尔曼算子。

Result: 在一系列具有挑战性的组合强化学习任务中，平均比现有最优基线高出20.6%。

Conclusion: 该方法能带来现代生成策略的表达性，同时保证可行性，表现良好。

Abstract: Reinforcement learning (RL) with combinatorial action spaces remains challenging because feasible action sets are exponentially large and governed by complex feasibility constraints, making direct policy parameterization impractical. Existing approaches embed task-specific value functions into constrained optimization programs or learn deterministic structured policies, sacrificing generality and policy expressiveness. We propose a solver-induced \emph{latent spherical flow policy} that brings the expressiveness of modern generative policies to combinatorial RL while guaranteeing feasibility by design. Our method, LSFlow, learns a \emph{stochastic} policy in a compact continuous latent space via spherical flow matching, and delegates feasibility to a combinatorial optimization solver that maps each latent sample to a valid structured action. To improve efficiency, we train the value network directly in the latent space, avoiding repeated solver calls during policy optimization. To address the piecewise-constant and discontinuous value landscape induced by solver-based action selection, we introduce a smoothed Bellman operator that yields stable, well-defined learning targets. Empirically, our approach outperforms state-of-the-art baselines by an average of 20.6\% across a range of challenging combinatorial RL tasks.

</details>


### [94] [DAJ: Data-Reweighted LLM Judge for Test-Time Scaling in Code Generation](https://arxiv.org/abs/2601.22230)
*Peijia Qin,Ruiyi Zhang,Qi Cao,Pengtao Xie*

Main category: cs.LG

TL;DR: 提出DAJ解决代码生成测试时缩放中LLM裁判训练难题，在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成测试时缩放依赖Best - of - N选择，训练可靠LLM裁判因分布偏移难题而具有挑战性。

Method: 提出DAJ，在双层数据重加权学习框架下用可验证奖励训练推理式LLM裁判，学习数据重要性权重优化泛化性能。

Result: DAJ在LiveCodeBench和BigCodeBench上达到了最先进的性能，超越了强大的测试时缩放基线和领先的专有模型。

Conclusion: 数据重加权首次应用于LLM裁判训练用于测试时缩放，该方法能自动强调难题、分布内样本和轨迹对齐数据，无需手工启发式方法。

Abstract: Test-time scaling for code generation commonly relies on Best-of-N selection, in which multiple candidate solutions are sampled from a base model, and the best one is selected by an LLM judge. However, training reliable LLM judges is challenging due to severe distribution shifts, including imbalances between easy and hard problems, mismatches between training tasks and evaluation benchmarks, and trajectory mismatch arising from training data generated by cheaper models whose behavior differs from that of inference-time models. We propose DAJ, a reasoning-based LLM judge trained with verifiable rewards under a bi-level data-reweighted learning framework. The proposed framework learns data-importance weights (either domain-level or instance-level) to optimize generalization performance on a held-out meta set aligned with target benchmarks. To the best of our knowledge, this is the first application of data reweighting to LLM-as-a-Judge training for test-time scaling. Our approach automatically emphasizes hard problems, in-distribution samples, and trajectory-aligned data, without relying on hand-crafted heuristics. Empirically, DAJ achieves state-of-the-art performance on LiveCodeBench and BigCodeBench, outperforming strong test-time scaling baselines as well as leading proprietary models.

</details>


### [95] [Weak Diffusion Priors Can Still Achieve Strong Inverse-Problem Performance](https://arxiv.org/abs/2601.22443)
*Jing Jia,Wei Yuan,Sifan Liu,Liyue Shen,Guanyang Wang*

Main category: cs.LG

TL;DR: 研究逆求解器对弱扩散先验的鲁棒性，通过实验和理论给出弱扩散先验可靠使用的条件。


<details>
  <summary>Details</summary>
Motivation: 实际中常需使用不匹配或低保真的扩散先验，探究逆求解器对其鲁棒性的情况和原因。

Method: 进行大量实验，基于贝叶斯一致性建立理论。

Result: 发现测量信息丰富时弱先验有效，也识别出其失效情况，理论给出高维测量使后验集中于真实信号的条件。

Conclusion: 为弱扩散先验何时能可靠使用提供了原则性依据。

Abstract: Can a diffusion model trained on bedrooms recover human faces? Diffusion models are widely used as priors for inverse problems, but standard approaches usually assume a high-fidelity model trained on data that closely match the unknown signal. In practice, one often must use a mismatched or low-fidelity diffusion prior. Surprisingly, these weak priors often perform nearly as well as full-strength, in-domain baselines. We study when and why inverse solvers are robust to weak diffusion priors. Through extensive experiments, we find that weak priors succeed when measurements are highly informative (e.g., many observed pixels), and we identify regimes where they fail. Our theory, based on Bayesian consistency, gives conditions under which high-dimensional measurements make the posterior concentrate near the true signal. These results provide a principled justification on when weak diffusion priors can be used reliably.

</details>


### [96] [ZK-HybridFL: Zero-Knowledge Proof-Enhanced Hybrid Ledger for Federated Learning](https://arxiv.org/abs/2601.22302)
*Amirhossein Taherpour,Xiaodong Wang*

Main category: cs.LG

TL;DR: 提出ZK - HybridFL框架解决联邦学习可扩展性、安全性等问题，实验表现优于对比框架，是可扩展安全的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有集中和分散式联邦学习在可扩展性、安全性和更新验证方面存在挑战。

Method: 提出ZK - HybridFL框架，集成DAG账本、侧链和零知识证明，用事件驱动智能合约和预言机辅助侧链验证模型更新，内置挑战机制检测攻击。

Result: 在图像分类和语言建模任务中，比Blade - FL和ChainFL收敛快、准确率高、困惑度低、延迟小，抗攻击且验证高效。

Conclusion: ZK - HybridFL是跨多样环境的可扩展且安全的去中心化联邦学习解决方案。

Abstract: Federated learning (FL) enables collaborative model training while preserving data privacy, yet both centralized and decentralized approaches face challenges in scalability, security, and update validation. We propose ZK-HybridFL, a secure decentralized FL framework that integrates a directed acyclic graph (DAG) ledger with dedicated sidechains and zero-knowledge proofs (ZKPs) for privacy-preserving model validation. The framework uses event-driven smart contracts and an oracle-assisted sidechain to verify local model updates without exposing sensitive data. A built-in challenge mechanism efficiently detects adversarial behavior. In experiments on image classification and language modeling tasks, ZK-HybridFL achieves faster convergence, higher accuracy, lower perplexity, and reduced latency compared to Blade-FL and ChainFL. It remains robust against substantial fractions of adversarial and idle nodes, supports sub-second on-chain verification with efficient gas usage, and prevents invalid updates and orphanage-style attacks. This makes ZK-HybridFL a scalable and secure solution for decentralized FL across diverse environments.

</details>


### [97] [FunPRM: Function-as-Step Process Reward Model with Meta Reward Correction for Code Generation](https://arxiv.org/abs/2601.22249)
*Ruiyi Zhang,Peijia Qin,Qi Cao,Eric Xue,Pengtao Xie*

Main category: cs.LG

TL;DR: 现有测试时间缩放方法在代码生成中效果不佳，提出FunPRM方法，实验证明其优于现有方法，生成代码更易读和复用。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂编程任务代码生成中常失败，现有基于PRM的测试时间缩放方法因代码缺乏有意义步骤分解和部分解决方案奖励噪声，在代码生成中无效。

Method: 提出FunPRM，促使大语言模型进行模块化代码生成，将函数作为PRM推理步骤，引入基于元学习的奖励校正机制净化噪声奖励。

Result: 在LiveCodeBench和BigCodeBench上的实验表明，FunPRM在五个基础大语言模型上始终优于现有测试时间缩放方法，与O4 - mini结合时在LiveCodeBench上达到了当前最优性能。

Conclusion: FunPRM有效提升了代码生成性能，生成的代码对开发者更易读和复用。

Abstract: Code generation is a core application of large language models (LLMs), yet LLMs still frequently fail on complex programming tasks. Given its success in mathematical reasoning, test-time scaling approaches such as Process Reward Model (PRM)-based Best-of-N selection offer a promising way to improve performance. However, existing PRMs remain ineffective for code generation due to the lack of meaningful step decomposition in code and the noise of Monte Carlo-estimated partial-solution correctness scores (rewards). To address these challenges, we propose FunPRM. FunPRM prompts LLMs to encourage modular code generation organized into functions, with functions treated as PRM reasoning steps. Furthermore, FunPRM introduces a novel meta-learning-based reward correction mechanism that leverages clean final-solution rewards obtained via a unit-test-based evaluation system to purify noisy partial-solution rewards. Experiments on LiveCodeBench and BigCodeBench demonstrate that FunPRM consistently outperforms existing test-time scaling methods across five base LLMs, notably achieving state-of-the-art performance on LiveCodeBench when combined with O4-mini. Furthermore, FunPRM produces code that is more readable and reusable for developers.

</details>


### [98] [Neural-Inspired Posterior Approximation (NIPA)](https://arxiv.org/abs/2601.22539)
*Babak Shahbaba,Zahra Moslemi*

Main category: cs.LG

TL;DR: 本文受人类神经系统学习机制启发，提出一种用于可扩展贝叶斯推理的采样算法，并应用于贝叶斯深度学习。


<details>
  <summary>Details</summary>
Motivation: 阐明人类神经系统高效学习的计算原理，并将其转化为用于可扩展贝叶斯推理的采样算法。

Method: 提出的算法包含基于模型、无模型和情景控制三个模块。基于模型模块用目标分布引导采样，无模型模块利用先前样本学习模式以快速采样，情景控制模块通过回忆特定过去事件支持快速采样。

Result: 该方法推进了贝叶斯方法，并便于其应用于大规模统计机器学习问题。

Conclusion: 所提出的算法可应用于贝叶斯深度学习，特别强调了适当和有原则的不确定性量化。

Abstract: Humans learn efficiently from their environment by engaging multiple interacting neural systems that support distinct yet complementary forms of control, including model-based (goal-directed) planning, model-free (habitual) responding, and episodic memory-based learning. Model-based mechanisms compute prospective action values using an internal model of the environment, supporting flexible but computationally costly planning; model-free mechanisms cache value estimates and build heuristics that enable fast, efficient habitual responding; and memory-based mechanisms allow rapid adaptation from individual experience. In this work, we aim to elucidate the computational principles underlying this biological efficiency and translate them into a sampling algorithm for scalable Bayesian inference through effective exploration of the posterior distribution. More specifically, our proposed algorithm comprises three components: a model-based module that uses the target distribution for guided but computationally slow sampling; a model-free module that uses previous samples to learn patterns in the parameter space, enabling fast, reflexive sampling without directly evaluating the expensive target distribution; and an episodic-control module that supports rapid sampling by recalling specific past events (i.e., samples). We show that this approach advances Bayesian methods and facilitates their application to large-scale statistical machine learning problems. In particular, we apply our proposed framework to Bayesian deep learning, with an emphasis on proper and principled uncertainty quantification.

</details>


### [99] [Symmetry Breaking in Transformers for Efficient and Interpretable Training](https://arxiv.org/abs/2601.22257)
*Eva Silverstein,Daniel Kunin,Vasudev Shyam*

Main category: cs.LG

TL;DR: 引入简单对称破缺协议修改注意力机制，提升优化器性能和模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 标准注意力机制存在不影响模型激活或输出的多余旋转自由度，期望改进以提升性能和可解释性。

Method: 引入对称破缺协议，通过批量采样、不可学习的查询和值偏差插入首选方向。

Result: 提升简单内存高效优化器性能，缩小与复杂自适应方法差距；可选择性放大注意力头内有语义意义的标记类。

Conclusion: 最小限度、有原则的架构更改可同时提升性能和可解释性。

Abstract: The attention mechanism in its standard implementation contains extraneous rotational degrees of freedom that are carried through computation but do not affect model activations or outputs. We introduce a simple symmetry-breaking protocol that inserts a preferred direction into this rotational space through batchwise-sampled, unlearned query and value biases. This modification has two theoretically motivated and empirically validated consequences. First, it can substantially improve the performance of simple, memory-efficient optimizers, narrowing -- and in some cases closing -- the gap to successful but more complex memory-intensive adaptive methods. We demonstrate this by pretraining 124M parameter transformer models with four optimization algorithms (AdamW, SOAP, SGDM, and Energy Conserving Descent(ECD)) and evaluating both validation loss and downstream logical reasoning. Second, it enables an interpretable use of otherwise redundant rotational degrees of freedom, selectively amplifying semantically meaningful token classes within individual attention heads. Overall, our results show that minimal, principled architectural changes can simultaneously improve performance and interpretability.

</details>


### [100] [SAIR: Cost-Efficient Multi-Stage ML Pipeline Autoscaling via In-Context Reinforcement Learning](https://arxiv.org/abs/2601.22397)
*Jianchang Su,Yifan Zhang,Shengkai Lin,Shizhen Zhao,Yusheng Zheng,Yiwei Yang,Wei Zhang*

Main category: cs.LG

TL;DR: 提出SAIR自动伸缩框架处理多阶段ML推理管道自动伸缩难题，效果良好。


<details>
  <summary>Details</summary>
Motivation: 多阶段ML推理管道因资源异构、跨阶段耦合和动态瓶颈迁移等问题难以自动伸缩。

Method: 使用LLM作为上下文强化学习控制器，结合Pareto优势奖励塑造、惊喜引导的经验检索和细粒度GPU速率控制等方法，并进行遗憾分析。

Result: 在四种ML服务管道和三种工作负载模式下，SAIR实现最佳或并列最佳的P99延迟和有效资源成本，P99提升达50%，有效成本降低达97%，瓶颈检测准确率86%，无需离线训练。

Conclusion: SAIR框架能有效解决多阶段ML推理管道的自动伸缩问题。

Abstract: Multi-stage ML inference pipelines are difficult to autoscale due to heterogeneous resources, cross-stage coupling, and dynamic bottleneck migration. We present SAIR, an autoscaling framework that uses an LLM as an in-context reinforcement learning controller, improving its policy online from reward-labeled interaction histories without gradient updates. SAIR combines Pareto-dominance reward shaping with a provable separation margin, surprisal-guided experience retrieval for context efficiency, and fine-grained GPU rate control via user-space CUDA interception. We provide regret analysis decomposing error into retrieval coverage and LLM selection components. On four ML serving pipelines under three workload patterns, SAIR achieves the best or tied-best P99 latency and effective resource cost among deployed baselines, improving P99 by up to 50% and reducing effective cost by up to 97% (under GPU rate-control assumptions), with 86% bottleneck detection accuracy and no offline training.

</details>


### [101] [Tabular Foundation Models Can Do Survival Analysis](https://arxiv.org/abs/2601.22259)
*Da In Kim,Wei Siang Lai,Kelly W. Zhang*

Main category: cs.LG

TL;DR: 提出基于分类的框架使表格基础模型适应生存分析，经53个数据集验证优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 表格基础模型在生存分析中因右删失问题难以适应时间事件结果建模。

Method: 将静态和动态生存分析通过离散化事件时间转化为一系列二元分类问题，利用现有表格基础模型进行上下文学习。

Result: 证明在标准删失假设下，最小化二元分类损失可恢复真实生存概率，53个数据集评估显示该框架使现成表格基础模型平均表现优于经典和深度学习基线。

Conclusion: 提出的分类框架能让表格基础模型有效用于生存分析，性能表现良好。

Abstract: While tabular foundation models have achieved remarkable success in classification and regression, adapting them to model time-to-event outcomes for survival analysis is non-trivial due to right-censoring, where data observations may end before the event occurs. We develop a classification-based framework that reformulates both static and dynamic survival analysis as a series of binary classification problems by discretizing event times. Censored observations are naturally handled as examples with missing labels at certain time points. This classification formulation enables existing tabular foundation models to perform survival analysis through in-context learning without explicit training. We prove that under standard censoring assumptions, minimizing our binary classification loss recovers the true survival probabilities as the training set size increases. We demonstrate through evaluation across $53$ real-world datasets that off-the-shelf tabular foundation models with this classification formulation outperform classical and deep learning baselines on average over multiple survival metrics.

</details>


### [102] [AsyncMesh: Fully Asynchronous Optimization for Data and Pipeline Parallelism](https://arxiv.org/abs/2601.22442)
*Thalaiyasingam Ajanthan,Sameera Ramasinghe,Gil Avraham,Hadi Mohaghegh Dolatabadi,Chamin P Hewa Koneputugodage,Violetta Shevchenko,Yan Zuo,Alexander Long*

Main category: cs.LG

TL;DR: 本文引入异步更新解决数据和管道并行训练的通信瓶颈，采用方法缓解陈旧性，实验表明能降低通信开销并达到同步基准性能。


<details>
  <summary>Details</summary>
Motivation: 数据和管道并行训练通信成本高，需共置计算集群，限制可扩展性。

Method: 在两个并行轴引入异步更新，用权重前瞻缓解管道并行陈旧性，用异步稀疏平均加指数移动平均校正机制缓解数据并行陈旧性，并给出收敛保证。

Result: 在大规模语言模型实验中，该方法匹配全同步基线性能，显著降低通信开销。

Conclusion: 引入异步更新的方法能有效解决通信瓶颈问题，减少通信开销。

Abstract: Data and pipeline parallelism are key strategies for scaling neural network training across distributed devices, but their high communication cost necessitates co-located computing clusters with fast interconnects, limiting their scalability. We address this communication bottleneck by introducing asynchronous updates across both parallelism axes, relaxing the co-location requirement at the expense of introducing staleness between pipeline stages and data parallel replicas. To mitigate staleness, for pipeline parallelism, we adopt a weight look-ahead approach, and for data parallelism, we introduce an asynchronous sparse averaging method equipped with an exponential moving average based correction mechanism. We provide convergence guarantees for both sparse averaging and asynchronous updates. Experiments on large-scale language models (up to \em 1B parameters) demonstrate that our approach matches the performance of the fully synchronous baseline, while significantly reducing communication overhead.

</details>


### [103] [Privacy-Preserving Sensor-Based Human Activity Recognition for Low-Resource Healthcare Using Classical Machine Learning](https://arxiv.org/abs/2601.22265)
*Ramakant Kumar,Pravin Kumar*

Main category: cs.LG

TL;DR: 提出基于可穿戴惯性传感器和机器学习的低成本自动人体活动识别框架，对比多种分类器，STM表现最佳，框架在多领域有应用潜力。


<details>
  <summary>Details</summary>
Motivation: 医疗基础设施不足使老人和弱势群体依赖居家护理，易导致护理疏忽和治疗锻炼依从性差，需解决此问题。

Method: 收集多种活动数据，评估对比逻辑回归、随机森林、支持向量机、k近邻和提出的支持张量机（STM）四种经典分类器。

Result: SVM准确率93.33%，逻辑回归、随机森林和k近邻准确率91.11%，STM测试准确率96.67%，交叉验证准确率98.50%，表现最佳。

Conclusion: 该框架在远程医疗、老人协助等多领域有强大应用潜力，为低资源和农村医疗提供可扩展解决方案。

Abstract: Limited access to medical infrastructure forces elderly and vulnerable patients to rely on home-based care, often leading to neglect and poor adherence to therapeutic exercises such as yoga or physiotherapy. To address this gap, we propose a low-cost and automated human activity recognition (HAR) framework based on wearable inertial sensors and machine learning. Activity data, including walking, walking upstairs, walking downstairs, sitting, standing, and lying, were collected using accelerometer and gyroscope measurements. Four classical classifiers, Logistic Regression, Random Forest, Support Vector Machine (SVM), and k-Nearest Neighbors (k-NN), were evaluated and compared with the proposed Support Tensor Machine (STM). Experimental results show that SVM achieved an accuracy of 93.33 percent, while Logistic Regression, Random Forest, and k-NN achieved 91.11 percent. In contrast, STM significantly outperformed these models, achieving a test accuracy of 96.67 percent and the highest cross-validation accuracy of 98.50 percent. Unlike conventional methods, STM leverages tensor representations to preserve spatio-temporal motion dynamics, resulting in robust classification across diverse activities. The proposed framework demonstrates strong potential for remote healthcare, elderly assistance, child activity monitoring, yoga feedback, and smart home wellness, offering a scalable solution for low-resource and rural healthcare settings.

</details>


### [104] [SQUAD: Scalable Quorum Adaptive Decisions via ensemble of early exit neural networks](https://arxiv.org/abs/2601.22711)
*Matteo Gambella,Fabrizio Pittorino,Giuliano Casale,Manuel Roveri*

Main category: cs.LG

TL;DR: 提出SQUAD推理方案结合早期退出机制与分布式集成学习，引入QUEST选择早期退出学习者，提升测试准确率、降低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 标准早期退出神经网络依赖单模型置信阈值，因校准问题不可靠，需改进不确定性估计并减少推理时间。

Method: 引入SQUAD，采用基于法定人数的停止准则；引入QUEST，通过神经架构搜索选择具有优化层次多样性的早期退出学习者。

Result: 与最先进动态解决方案相比，测试准确率最高提升5.95%；与静态集成相比，推理延迟最高降低70.60%，且保持良好准确率。

Conclusion: 基于共识的方法能产生统计上稳健的早期退出，在计算成本相当的情况下提升性能。

Abstract: Early-exit neural networks have become popular for reducing inference latency by allowing intermediate predictions when sufficient confidence is achieved. However, standard approaches typically rely on single-model confidence thresholds, which are frequently unreliable due to inherent calibration issues. To address this, we introduce SQUAD (Scalable Quorum Adaptive Decisions), the first inference scheme that integrates early-exit mechanisms with distributed ensemble learning, improving uncertainty estimation while reducing the inference time. Unlike traditional methods that depend on individual confidence scores, SQUAD employs a quorum-based stopping criterion on early-exit learners by collecting intermediate predictions incrementally in order of computational complexity until a consensus is reached and halting the computation at that exit if the consensus is statistically significant. To maximize the efficacy of this voting mechanism, we also introduce QUEST (Quorum Search Technique), a Neural Architecture Search method to select early-exit learners with optimized hierarchical diversity, ensuring learners are complementary at every intermediate layer. This consensus-driven approach yields statistically robust early exits, improving the test accuracy up to 5.95% compared to state-of-the-art dynamic solutions with a comparable computational cost and reducing the inference latency up to 70.60% compared to static ensembles while maintaining a good accuracy.

</details>


### [105] [Task-Uniform Convergence and Backward Transfer in Federated Domain-Incremental Learning with Partial Participation](https://arxiv.org/abs/2601.22274)
*Longtao Xu,Jian Li*

Main category: cs.LG

TL;DR: 研究联邦域增量学习（FDIL），提出SPECIAL算法，理论证明其能保留早期任务并高效学习，实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现实联邦系统数据分布漂移且隐私规则限制数据共享，FDIL缺少向后知识转移保证和跨任务收敛率。

Method: 提出SPECIAL算法，在普通FedAvg基础上添加服务器端‘锚点’，用轻量级近端项引导客户端更新。

Result: 理论上SPECIAL能保留早期任务，有跨任务的收敛率；实验证明算法有效。

Conclusion: SPECIAL算法在FDIL中能有效保留知识并高效学习。

Abstract: Real-world federated systems seldom operate on static data: input distributions drift while privacy rules forbid raw-data sharing. We study this setting as Federated Domain-Incremental Learning (FDIL), where (i) clients are heterogeneous, (ii) tasks arrive sequentially with shifting domains, yet (iii) the label space remains fixed. Two theoretical pillars remain missing for FDIL under realistic deployment: a guarantee of backward knowledge transfer (BKT) and a convergence rate that holds across the sequence of all tasks with partial participation. We introduce SPECIAL (Server-Proximal Efficient Continual Aggregation for Learning), a simple, memory-free FDIL algorithm that adds a single server-side ``anchor'' to vanilla FedAvg: in each round, the server nudges the uniformly sampled participated clients update toward the previous global model with a lightweight proximal term. This anchor curbs cumulative drift without replay buffers, synthetic data, or task-specific heads, keeping communication and model size unchanged. Our theory shows that SPECIAL (i) preserves earlier tasks: a BKT bound caps any increase in prior-task loss by a drift-controlled term that shrinks with more rounds, local epochs, and participating clients; and (ii) learns efficiently across all tasks: the first communication-efficient non-convex convergence rate for FDIL with partial participation, O((E/NT)^(1/2)), with E local epochs, T communication rounds, and N participated clients per round, matching single-task FedAvg while explicitly separating optimization variance from inter-task drift. Experimental results further demonstrate the effectiveness of SPECIAL.

</details>


### [106] [SurrogateSHAP: Training-Free Contributor Attribution for Text-to-Image (T2I) Models](https://arxiv.org/abs/2601.22276)
*Mingyu Lu,Soham Gadgil,Chris Lin,Chanwoo Kim,Su-In Lee*

Main category: cs.LG

TL;DR: 提出无重训练框架SurrogateSHAP评估文本到图像扩散模型数据贡献者价值，在多任务中表现优且降低计算开销，还可审计生成模型。


<details>
  <summary>Details</summary>
Motivation: 随着文本到图像扩散模型在实际中应用增多，需要一个评估数据贡献者价值的框架，而Shapley值计算存在瓶颈。

Method: 提出无重训练框架SurrogateSHAP，通过预训练模型推理近似重训练游戏，用梯度提升树近似效用函数并解析推导Shapley值。

Result: 在三个不同归因任务中，SurrogateSHAP优于先前方法，大幅降低计算开销，能识别有影响力的贡献者。

Conclusion: SurrogateSHAP可有效定位临床图像中虚假关联的数据来源，为审计安全关键的生成模型提供可扩展途径。

Abstract: As Text-to-Image (T2I) diffusion models are increasingly used in real-world creative workflows, a principled framework for valuing contributors who provide a collection of data is essential for fair compensation and sustainable data marketplaces. While the Shapley value offers a theoretically grounded approach to attribution, it faces a dual computational bottleneck: (i) the prohibitive cost of exhaustive model retraining for each sampled subset of players (i.e., data contributors) and (ii) the combinatorial number of subsets needed to estimate marginal contributions due to contributor interactions. To this end, we propose SurrogateSHAP, a retraining-free framework that approximates the expensive retraining game through inference from a pretrained model. To further improve efficiency, we employ a gradient-boosted tree to approximate the utility function and derive Shapley values analytically from the tree-based model. We evaluate SurrogateSHAP across three diverse attribution tasks: (i) image quality for DDPM-CFG on CIFAR-20, (ii) aesthetics for Stable Diffusion on Post-Impressionist artworks, and (iii) product diversity for FLUX.1 on Fashion-Product data. Across settings, SurrogateSHAP outperforms prior methods while substantially reducing computational overhead, consistently identifying influential contributors across multiple utility metrics. Finally, we demonstrate that SurrogateSHAP effectively localizes data sources responsible for spurious correlations in clinical images, providing a scalable path toward auditing safety-critical generative models.

</details>


### [107] [Riemannian Lyapunov Optimizer: A Unified Framework for Optimization](https://arxiv.org/abs/2601.22284)
*Yixuan Wang,Omkar Sudhir Patil,Warren E. Dixon*

Main category: cs.LG

TL;DR: 本文介绍了Riemannian Lyapunov Optimizers (RLOs)，将经典优化器统一在几何框架下，基于控制理论推导，有理论验证和性能表现。


<details>
  <summary>Details</summary>
Motivation: 将经典优化器统一在一个几何框架下，克服现有优化器启发式改进的不足。

Method: 将优化重新解释为黎曼参数流形上的扩展状态离散时间受控动力系统，通过识别Normally Attracting Invariant Manifold (NAIM)组织训练动态，构建严格的Lyapunov函数证明收敛性。

Result: 通过几何诊断验证理论，在大规模基准测试中取得了最先进的性能。

Conclusion: RLOs架起了控制理论和现代机器学习优化之间的桥梁，为设计稳定、有效的优化器提供了统一语言和系统工具包。

Abstract: We introduce Riemannian Lyapunov Optimizers (RLOs), a family of optimization algorithms that unifies classic optimizers within one geometric framework. Unlike heuristic improvements to existing optimizers, RLOs are systematically derived from a novel control-theoretic framework that reinterprets optimization as an extended state discrete-time controlled dynamical system on a Riemannian parameter manifold. Central to this framework is the identification of a Normally Attracting Invariant Manifold (NAIM), which organizes training dynamics into two distinct stages: rapid alignment of the speed state to a target graph, followed by controlled evolution within it. We formalize this by constructing a strict Lyapunov function that certifies convergence to a target manifold. This perspective yields a constructive ``optimizer generator" that not only recovers classic algorithms but enables the principled design of RLOs. We validate our theory via geometric diagnostics and demonstrate that grounding optimizer design in control theory yields state-of-the-art performance in large-scale benchmarks. Overall, RLOs bridge control theory and modern machine learning optimization, providing a unified language and a systematic toolkit for designing stable, effective optimizers.

</details>


### [108] [Demystifying Mergeability: Interpretable Properties to Predict Model Merging Success](https://arxiv.org/abs/2601.22285)
*Luca Zhou,Bo Zhao,Rose Yu,Emanuele Rodolà*

Main category: cs.LG

TL;DR: 研究揭示模型合并的成功因素，发现其依赖于合并方法和合作任务，确定了一些通用的兼容性先决条件并为后续微调策略提供基础。


<details>
  <summary>Details</summary>
Motivation: 当前模型合并的成功因素研究不足，理解合并的影响因素。

Method: 使用可解释的成对指标集进行线性优化。

Result: 发现不同合并方法成功驱动因素有差异，存在方法特定“指纹”；子空间重叠和梯度对齐指标是通用先决条件。

Conclusion: 研究结果为理解模型合并性提供诊断基础，并为未来微调策略提供了思路。

Abstract: Model merging combines knowledge from separately fine-tuned models, yet success factors remain poorly understood. While recent work treats mergeability as an intrinsic property, we show with an architecture-agnostic framework that it fundamentally depends on both the merging method and the partner tasks. Using linear optimization over a set of interpretable pairwise metrics (e.g., gradient L2 distance), we uncover properties correlating with post-merge performance across four merging methods. We find substantial variation in success drivers (46.7% metric overlap; 55.3% sign agreement), revealing method-specific "fingerprints". Crucially, however, subspace overlap and gradient alignment metrics consistently emerge as foundational, method-agnostic prerequisites for compatibility. These findings provide a diagnostic foundation for understanding mergeability and motivate future fine-tuning strategies that explicitly encourage these properties.

</details>


### [109] [ParalESN: Enabling parallel information processing in Reservoir Computing](https://arxiv.org/abs/2601.22296)
*Matteo Pinna,Giacomo Lagomarsini,Andrea Ceni,Claudio Gallicchio*

Main category: cs.LG

TL;DR: 本文提出Parallel Echo State Network (ParalESN)，通过结构化算子和状态空间模型解决了储层计算（RC）的可扩展性问题，实现时间数据并行处理和显著计算节省。


<details>
  <summary>Details</summary>
Motivation: 传统储层计算（RC）可扩展性受限于时间数据顺序处理和高维储层大内存占用。

Method: 通过结构化算子和状态空间模型引入Parallel Echo State Network (ParalESN)，基于复空间对角线性递归构建高维高效储层。

Result: 理论上，ParalESN保留了传统回声状态网络的回声状态属性和通用性；实证上，在时间序列基准测试中预测准确性与传统RC相当，实现显著计算节省；在1 - D像素分类任务中，与全可训练神经网络精度相当，大幅降低计算成本和能耗。

Conclusion: ParalESN为在深度学习领域集成RC提供了有前景、可扩展且有原则的途径。

Abstract: Reservoir Computing (RC) has established itself as an efficient paradigm for temporal processing. However, its scalability remains severely constrained by (i) the necessity of processing temporal data sequentially and (ii) the prohibitive memory footprint of high-dimensional reservoirs. In this work, we revisit RC through the lens of structured operators and state space modeling to address these limitations, introducing Parallel Echo State Network (ParalESN). ParalESN enables the construction of high-dimensional and efficient reservoirs based on diagonal linear recurrence in the complex space, enabling parallel processing of temporal data. We provide a theoretical analysis demonstrating that ParalESN preserves the Echo State Property and the universality guarantees of traditional Echo State Networks while admitting an equivalent representation of arbitrary linear reservoirs in the complex diagonal form. Empirically, ParalESN matches the predictive accuracy of traditional RC on time series benchmarks, while delivering substantial computational savings. On 1-D pixel-level classification tasks, ParalESN achieves competitive accuracy with fully trainable neural networks while reducing computational costs and energy consumption by orders of magnitude. Overall, ParalESN offers a promising, scalable, and principled pathway for integrating RC within the deep learning landscape.

</details>


### [110] [Conformal Prediction for Generative Models via Adaptive Cluster-Based Density Estimation](https://arxiv.org/abs/2601.22298)
*Qidong Yang,Qianyu Julie Zhu,Jonathan Giezendanner,Youssef Marzouk,Stephen Bates,Sherrie Wang*

Main category: cs.LG

TL;DR: 针对条件生成模型缺乏校准不确定性的问题，提出CP4Gen方法，实验表明其在预测集体积和结构简单性上性能优越，为不确定性估计提供有力工具。


<details>
  <summary>Details</summary>
Motivation: 条件生成模型缺乏校准不确定性，影响高风险应用中对输出的信任，需解决该问题。

Method: 提出CP4Gen方法，利用基于聚类的密度估计构建预测集。

Result: 在合成数据集和真实应用（如气候模拟任务）上，CP4Gen在预测集体积和结构简单性方面始终表现出色。

Conclusion: 该方法为条件生成模型的不确定性估计提供了强大工具，尤其适用于需要严谨且可解释预测集的场景。

Abstract: Conditional generative models map input variables to complex, high-dimensional distributions, enabling realistic sample generation in a diverse set of domains. A critical challenge with these models is the absence of calibrated uncertainty, which undermines trust in individual outputs for high-stakes applications. To address this issue, we propose a systematic conformal prediction approach tailored to conditional generative models, leveraging density estimation on model-generated samples. We introduce a novel method called CP4Gen, which utilizes clustering-based density estimation to construct prediction sets that are less sensitive to outliers, more interpretable, and of lower structural complexity than existing methods. Extensive experiments on synthetic datasets and real-world applications, including climate emulation tasks, demonstrate that CP4Gen consistently achieves superior performance in terms of prediction set volume and structural simplicity. Our approach offers practitioners a powerful tool for uncertainty estimation associated with conditional generative models, particularly in scenarios demanding rigorous and interpretable prediction sets.

</details>


### [111] [BayesFlow: A Probability Inference Framework for Meta-Agent Assisted Workflow Generation](https://arxiv.org/abs/2601.22305)
*Bo Yuan,Yun Zhou,Zhichao Xu,Kiran Ramnath,Aosong Feng,Balasubramaniam Srinivasan*

Main category: cs.LG

TL;DR: 提出将工作流生成作为贝叶斯推理的方法，引入BWG框架并实例化为BayesFlow算法，在多个基准数据集上提升了准确性。


<details>
  <summary>Details</summary>
Motivation: 大多数现有方法将自动工作流生成任务视为优化问题且理论基础有限。

Method: 将工作流生成视为贝叶斯推理，引入BWG采样框架，使用并行前瞻滚动进行重要性加权和顺序内循环细化器，实例化出无训练的BayesFlow算法。

Result: 在六个基准数据集上，BayesFlow比SOTA工作流生成基线的准确率最多提高9个百分点，比零样本提示最多提高65个百分点。

Conclusion: BWG是基于搜索的工作流设计的原则性升级。

Abstract: Automatic workflow generation is the process of automatically synthesizing sequences of LLM calls, tool invocations, and post-processing steps for complex end-to-end tasks. Most prior methods cast this task as an optimization problem with limited theoretical grounding. We propose to cast workflow generation as Bayesian inference over a posterior distribution on workflows, and introduce \textbf{Bayesian Workflow Generation (BWG)}, a sampling framework that builds workflows step-by-step using parallel look-ahead rollouts for importance weighting and a sequential in-loop refiner for pool-wide improvements. We prove that, without the refiner, the weighted empirical distribution converges to the target posterior. We instantiate BWG as \textbf{BayesFlow}, a training-free algorithm for workflow construction. Across six benchmark datasets, BayesFlow improves accuracy by up to 9 percentage points over SOTA workflow generation baselines and by up to 65 percentage points over zero-shot prompting, establishing BWG as a principled upgrade to search-based workflow design. Code will be available on https://github.com/BoYuanVisionary/BayesFlow.

</details>


### [112] [Exact closed-form Gaussian moments of residual layers](https://arxiv.org/abs/2601.22307)
*Simon Kuang,Xinfan Lin*

Main category: cs.LG

TL;DR: 本文使用逐层矩匹配方法，推导多种激活函数精确矩匹配，在随机网络和真实数据上表现良好，还给出误差界和初步分析。


<details>
  <summary>Details</summary>
Motivation: 解决通过深度（残差）神经网络传播多元高斯分布均值和协方差的问题。

Method: 使用逐层矩匹配方法，推导probit、GeLU、ReLU等激活函数的精确矩匹配。

Result: 在随机网络上，KL散度误差指标有数量级改善；在真实数据上，输入存在认知不确定性时推理校准有竞争力；在变分贝叶斯网络上，相比现有方法有百倍提升；给出先验误差界和随机前馈神经元初步分析。

Conclusion: 所提出方法在传播多元高斯分布矩方面表现出色，有较好的应用效果。

Abstract: We study the problem of propagating the mean and covariance of a general multivariate Gaussian distribution through a deep (residual) neural network using layer-by-layer moment matching. We close a longstanding gap by deriving exact moment matching for the probit, GeLU, ReLU (as a limit of GeLU), Heaviside (as a limit of probit), and sine activation functions; for both feedforward and generalized residual layers. On random networks, we find orders-of-magnitude improvements in the KL divergence error metric, up to a millionfold, over popular alternatives. On real data, we find competitive statistical calibration for inference under epistemic uncertainty in the input. On a variational Bayes network, we show that our method attains hundredfold improvements in KL divergence from Monte Carlo ground truth over a state-of-the-art deterministic inference method. We also give an a priori error bound and a preliminary analysis of stochastic feedforward neurons, which have recently attracted general interest.

</details>


### [113] [Stealthy Poisoning Attacks Bypass Defenses in Regression Settings](https://arxiv.org/abs/2601.22308)
*Javier Carnerero-Cano,Luis Muñoz-González,Phillippa Spencer,Emil C. Lupu*

Main category: cs.LG

TL;DR: 本文提出最优隐蔽攻击公式、评估方法和新型防御机制BayesClean，可绕过现有防御并在隐蔽攻击下表现更好。


<details>
  <summary>Details</summary>
Motivation: 回归模型对投毒的鲁棒性研究较少，且现有研究威胁模型不现实。

Method: 提出考虑不同可检测性的最优隐蔽攻击公式，基于目标归一化评估有效性和可检测性权衡，开发新型防御机制BayesClean。

Result: 所提攻击公式可绕过现有防御，BayesClean在隐蔽攻击且投毒点数量显著时优于先前防御。

Conclusion: 提出的方法在回归模型投毒攻击和防御方面有较好效果。

Abstract: Regression models are widely used in industrial processes, engineering and in natural and physical sciences, yet their robustness to poisoning has received less attention. When it has, studies often assume unrealistic threat models and are thus less useful in practice. In this paper, we propose a novel optimal stealthy attack formulation that considers different degrees of detectability and show that it bypasses state-of-the-art defenses. We further propose a new methodology based on normalization of objectives to evaluate different trade-offs between effectiveness and detectability. Finally, we develop a novel defense (BayesClean) against stealthy attacks. BayesClean improves on previous defenses when attacks are stealthy and the number of poisoning points is significant.

</details>


### [114] [Matrix Factorization for Practical Continual Mean Estimation Under User-Level Differential Privacy](https://arxiv.org/abs/2601.22320)
*Nikita P. Kalinin,Ali Najar,Valentin Roth,Christoph H. Lampert*

Main category: cs.LG

TL;DR: 研究连续均值估计问题，在用户级近似差分隐私下采用新方法，实现更低均方误差界。


<details>
  <summary>Details</summary>
Motivation: 以往纯差分隐私方法在连续均值估计中导致估计过于嘈杂，限制了适用性，需新的解决方案。

Method: 在近似差分隐私下分析问题，采用矩阵分解机制的最新进展，引入针对均值估计的新分解方法。

Result: 实现了在用户级差分隐私下连续均值估计的渐近更低均方误差界。

Conclusion: 所提出的方法在连续均值估计问题上更高效准确，在用户级差分隐私保护方面表现更好。

Abstract: We study continual mean estimation, where data vectors arrive sequentially and the goal is to maintain accurate estimates of the running mean. We address this problem under user-level differential privacy, which protects each user's entire dataset even when they contribute multiple data points. Previous work on this problem has focused on pure differential privacy. While important, this approach limits applicability, as it leads to overly noisy estimates. In contrast, we analyze the problem under approximate differential privacy, adopting recent advances in the Matrix Factorization mechanism. We introduce a novel mean estimation specific factorization, which is both efficient and accurate, achieving asymptotically lower mean-squared error bounds in continual mean estimation under user-level differential privacy.

</details>


### [115] [Knowledge Gradient for Preference Learning](https://arxiv.org/abs/2601.22335)
*Kaiwen Wu,Jacob R. Gardner*

Main category: cs.LG

TL;DR: 本文为偏好贝叶斯优化推导出精确解析的知识梯度，在基准问题上表现良好，还给出特定场景局限性案例。


<details>
  <summary>Details</summary>
Motivation: 将知识梯度扩展到偏好贝叶斯优化存在计算挑战，核心的前瞻步骤需计算非高斯后验，此前被认为难以处理。

Method: 为偏好贝叶斯优化推导出精确和解析的知识梯度。

Result: 精确知识梯度在一系列基准问题上表现出色，常优于现有采集函数。

Conclusion: 精确知识梯度在偏好贝叶斯优化中有良好表现，但在某些场景存在局限性。

Abstract: The knowledge gradient is a popular acquisition function in Bayesian optimization (BO) for optimizing black-box objectives with noisy function evaluations. Many practical settings, however, allow only pairwise comparison queries, yielding a preferential BO problem where direct function evaluations are unavailable. Extending the knowledge gradient to preferential BO is hindered by its computational challenge. At its core, the look-ahead step in the preferential setting requires computing a non-Gaussian posterior, which was previously considered intractable. In this paper, we address this challenge by deriving an exact and analytical knowledge gradient for preferential BO. We show that the exact knowledge gradient performs strongly on a suite of benchmark problems, often outperforming existing acquisition functions. In addition, we also present a case study illustrating the limitation of the knowledge gradient in certain scenarios.

</details>


### [116] [Hair-Trigger Alignment: Black-Box Evaluation Cannot Guarantee Post-Update Alignment](https://arxiv.org/abs/2601.22313)
*Yavuz Bakman,Duygu Nur Yaldiz,Salman Avestimehr,Sai Praneeth Karimireddy*

Main category: cs.LG

TL;DR: 论文指出大语言模型常更新，静态黑盒评估无法保证更新后模型对齐，通过理论和实证研究证明其局限性，强调需更新后鲁棒的对齐评估。


<details>
  <summary>Details</summary>
Motivation: 现有对齐研究多基于静态黑盒评估假设初始模型对齐，但模型更新后可能出现未对齐行为，需研究静态和更新后设置下的模型对齐。

Method: 形式化模型对齐，从理论上分析过参数化导致静态对齐不能保证更新后对齐，证明静态黑盒探测无法区分真正更新后鲁棒的模型和隐藏对抗行为的模型；通过在隐私、越狱安全和行为诚实三个核心对齐领域的大语言模型上进行实证验证。

Result: 存在通过所有标准黑盒对齐测试，但一次良性更新后严重未对齐的大语言模型，隐藏潜在对抗行为的能力随模型规模增加。

Conclusion: 静态评估协议存在不足，迫切需要更新后鲁棒的对齐评估。

Abstract: Large Language Models (LLMs) are rarely static and are frequently updated in practice. A growing body of alignment research has shown that models initially deemed "aligned" can exhibit misaligned behavior after fine-tuning, such as forgetting jailbreak safety features or re-surfacing knowledge that was intended to be forgotten. These works typically assume that the initial model is aligned based on static black-box evaluation, i.e., the absence of undesired responses to a fixed set of queries. In contrast, we formalize model alignment in both the static and post-update settings and uncover a fundamental limitation of black-box evaluation. We theoretically show that, due to overparameterization, static alignment provides no guarantee of post-update alignment for any update dataset. Moreover, we prove that static black-box probing cannot distinguish a model that is genuinely post-update robust from one that conceals an arbitrary amount of adversarial behavior which can be activated by even a single benign gradient update. We further validate these findings empirically in LLMs across three core alignment domains: privacy, jailbreak safety, and behavioral honesty. We demonstrate the existence of LLMs that pass all standard black-box alignment tests, yet become severely misaligned after a single benign update. Finally, we show that the capacity to hide such latent adversarial behavior increases with model scale, confirming our theoretical prediction that post-update misalignment grows with the number of parameters. Together, our results highlight the inadequacy of static evaluation protocols and emphasize the urgent need for post-update-robust alignment evaluation.

</details>


### [117] [Optimization, Generalization and Differential Privacy Bounds for Gradient Descent on Kolmogorov-Arnold Networks](https://arxiv.org/abs/2601.22409)
*Puyu Wang,Junyu Zhou,Philipp Liznerski,Marius Kloft*

Main category: cs.LG

TL;DR: 分析两层KANs的梯度下降训练，推导训练动态、泛化和差分隐私下的通用界限，给出具体例子，实验指导实际选择。


<details>
  <summary>Details</summary>
Motivation: KANs作为标准MLP的替代结构，但缺乏训练动态、泛化和隐私属性的理论。

Method: 分析两层KANs的梯度下降训练。

Result: 在特定假设下得到优化率、泛化率和差分隐私下的效用界限，发现对数多项式宽度在差分隐私下必要。

Conclusion: 理论结果可指导网络宽度选择和提前停止等实际操作。

Abstract: Kolmogorov--Arnold Networks (KANs) have recently emerged as a structured alternative to standard MLPs, yet a principled theory for their training dynamics, generalization, and privacy properties remains limited. In this paper, we analyze gradient descent (GD) for training two-layer KANs and derive general bounds that characterize their training dynamics, generalization, and utility under differential privacy (DP). As a concrete instantiation, we specialize our analysis to logistic loss under an NTK-separable assumption, where we show that polylogarithmic network width suffices for GD to achieve an optimization rate of order $1/T$ and a generalization rate of order $1/n$, with $T$ denoting the number of GD iterations and $n$ the sample size. In the private setting, we characterize the noise required for $(ε,δ)$-DP and obtain a utility bound of order $\sqrt{d}/(nε)$ (with $d$ the input dimension), matching the classical lower bound for general convex Lipschitz problems. Our results imply that polylogarithmic width is not only sufficient but also necessary under differential privacy, revealing a qualitative gap between non-private (sufficiency only) and private (necessity also emerges) training regimes. Experiments further illustrate how these theoretical insights can guide practical choices, including network width selection and early stopping.

</details>


### [118] [Gaussian Process Bandit Optimization with Machine Learning Predictions and Application to Hypothesis Generation](https://arxiv.org/abs/2601.22315)
*Xin Jennifer Chen,Yunjin Tong*

Main category: cs.LG

TL;DR: 提出预测增强高斯过程上置信界算法（PA - GP - UCB），结合两个预言机和离线数据提升样本效率，理论证明和实验皆有优势。


<details>
  <summary>Details</summary>
Motivation: 现实中大量优化问题有高成本的真实预言机和低成本低保真度的预测预言机，且有丰富离线数据可利用，需算法提升真实预言机查询的样本效率。

Method: 提出PA - GP - UCB算法，通过联合高斯过程后验得到控制变量估计器来校正预测偏差并减少不确定性。

Result: 理论上在保持GP - UCB标准后悔率的同时，领先常数更小；实验上在合成基准和真实假设评估任务中比基线算法收敛更快。

Conclusion: PA - GP - UCB是昂贵反馈下假设生成的通用且样本高效的框架。

Abstract: Many real-world optimization problems involve an expensive ground-truth oracle (e.g., human evaluation, physical experiments) and a cheap, low-fidelity prediction oracle (e.g., machine learning models, simulations). Meanwhile, abundant offline data (e.g., past experiments and predictions) are often available and can be used to pretrain powerful predictive models, as well as to provide an informative prior. We propose Prediction-Augmented Gaussian Process Upper Confidence Bound (PA-GP-UCB), a novel Bayesian optimization algorithm that leverages both oracles and offline data to achieve provable gains in sample efficiency for the ground-truth oracle queries. PA-GP-UCB employs a control-variates estimator derived from a joint Gaussian process posterior to correct prediction bias and reduce uncertainty. We prove that PA-GP-UCB preserves the standard regret rate of GP-UCB while achieving a strictly smaller leading constant that is explicitly controlled by prediction quality and offline data coverage. Empirically, PA-GP-UCB converges faster than Vanilla GP-UCB and naive prediction-augmented GP-UCB baselines on synthetic benchmarks and on a real-world hypothesis evaluation task grounded in human behavioral data, where predictions are provided by large language models. These results establish PA-GP-UCB as a general and sample-efficient framework for hypothesis generation under expensive feedback.

</details>


### [119] [FlowSymm: Physics Aware, Symmetry Preserving Graph Attention for Network Flow Completion](https://arxiv.org/abs/2601.22317)
*Ege Demirci,Francesco Bullo,Ananthram Swami,Ambuj Singh*

Main category: cs.LG

TL;DR: 提出FlowSymm架构恢复网络缺失流，在三个真实世界基准测试中表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 解决在严格遵循局部守恒定律的情况下恢复网络边缘缺失流这一基础逆问题。

Method: 结合无散度流的群作用、图注意力编码器和通过隐式双层优化解决的轻量级Tikhonov细化。先将观测值锚定在最小范数无散度补全上，计算保持观测流不变的所有允许群作用的正交基，用GATv2层编码图及特征得到注意力权重，最后用Tikhonov罚函数细化缺失项。

Result: 在三个真实世界流基准测试（交通、电力、自行车）中，FlowSymm在RMSE、MAE和相关性指标上优于现有基线。

Conclusion: FlowSymm架构在恢复网络缺失流问题上有效且性能优越。

Abstract: Recovering missing flows on the edges of a network, while exactly respecting local conservation laws, is a fundamental inverse problem that arises in many systems such as transportation, energy, and mobility. We introduce FlowSymm, a novel architecture that combines (i) a group-action on divergence-free flows, (ii) a graph-attention encoder to learn feature-conditioned weights over these symmetry-preserving actions, and (iii) a lightweight Tikhonov refinement solved via implicit bilevel optimization. The method first anchors the given observation on a minimum-norm divergence-free completion. We then compute an orthonormal basis for all admissible group actions that leave the observed flows invariant and parameterize the valid solution subspace, which shows an Abelian group structure under vector addition. A stack of GATv2 layers then encodes the graph and its edge features into per-edge embeddings, which are pooled over the missing edges and produce per-basis attention weights. This attention-guided process selects a set of physics-aware group actions that preserve the observed flows. Finally, a scalar Tikhonov penalty refines the missing entries via a convex least-squares solver, with gradients propagated implicitly through Cholesky factorization. Across three real-world flow benchmarks (traffic, power, bike), FlowSymm outperforms state-of-the-art baselines in RMSE, MAE and correlation metrics.

</details>


### [120] [Federate the Router: Learning Language Model Routers with Sparse and Decentralized Evaluations](https://arxiv.org/abs/2601.22318)
*Baris Askin,Shivam Patel,Anupam Nayak,Andrea Vigano,Jiin Woo,Gauri Joshi,Carlee Joe-Wong*

Main category: cs.LG

TL;DR: 提出首个大语言模型路由的联邦框架，能让客户端从本地离线数据学习共享路由策略，提升了准确率 - 成本边界。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型路由方法需集中式查询 - 模型评估数据，但数据分散且敏感，集中困难，单客户端训练因本地数据有限而效果不佳。

Method: 引入大语言模型路由的联邦框架，支持参数化多层感知器路由和非参数化K - 均值路由，适用于异构客户端查询分布和非均匀模型覆盖情况。

Result: 在两个基准测试中，通过增加有效模型覆盖和更好的查询泛化，联邦协作比客户端本地路由提高了准确率 - 成本边界。理论结果验证联邦训练减少了路由次优性。

Conclusion: 所提出的联邦框架在大语言模型路由中有效，能解决现有方法的问题。

Abstract: Large language models (LLMs) are increasingly accessed as remotely hosted services by edge and enterprise clients that cannot run frontier models locally. Since models vary widely in capability and price, routing queries to models that balance quality and inference cost is essential. Existing router approaches assume access to centralized query-model evaluation data. However, these data are often fragmented across clients, such as end users and organizations, and are privacy-sensitive, which makes centralizing data infeasible. Additionally, per-client router training is ineffective since local evaluation data is limited and covers only a restricted query distribution and a biased subset of model evaluations. We introduce the first federated framework for LLM routing, enabling clients to learn a shared routing policy from local offline query-model evaluation data. Our framework supports both parametric multilayer perceptron router and nonparametric K-means router under heterogeneous client query distributions and non-uniform model coverage. Across two benchmarks, federated collaboration improves the accuracy-cost frontier over client-local routers, both via increased effective model coverage and better query generalization. Our theoretical results also validate that federated training reduces routing suboptimality.

</details>


### [121] [Cascaded Flow Matching for Heterogeneous Tabular Data with Mixed-Type Features](https://arxiv.org/abs/2601.22816)
*Markus Mueller,Kathrin Gruber,Dennis Fok*

Main category: cs.LG

TL;DR: 本文提出级联方法改进表格数据扩散模型，能更好生成混合类型特征，结果显示生成样本更真实、捕捉分布细节更准确。


<details>
  <summary>Details</summary>
Motivation: 当前生成混合类型特征（单个特征中离散状态与连续分布结合）仍具挑战性，需改进表格数据扩散模型。

Method: 采用级联方法，先生成表格数据行的低分辨率版本，再通过引导条件概率路径和数据依赖耦合将信息用于高分辨率流匹配模型。

Result: 模型生成的样本更真实，能更准确捕捉分布细节，如检测分数提高40%。

Conclusion: 这种级联方法收紧了运输成本界限，能有效提升模型生成混合类型特征的能力。

Abstract: Advances in generative modeling have recently been adapted to tabular data containing discrete and continuous features. However, generating mixed-type features that combine discrete states with an otherwise continuous distribution in a single feature remains challenging. We advance the state-of-the-art in diffusion models for tabular data with a cascaded approach. We first generate a low-resolution version of a tabular data row, that is, the collection of the purely categorical features and a coarse categorical representation of numerical features. Next, this information is leveraged in the high-resolution flow matching model via a novel guided conditional probability path and data-dependent coupling. The low-resolution representation of numerical features explicitly accounts for discrete outcomes, such as missing or inflated values, and therewith enables a more faithful generation of mixed-type features. We formally prove that this cascade tightens the transport cost bound. The results indicate that our model generates significantly more realistic samples and captures distributional details more accurately, for example, the detection score increases by 40%.

</details>


### [122] [Spatially-Adaptive Conformal Graph Transformer for Indoor Localization in Wi-Fi Driven Networks](https://arxiv.org/abs/2601.22322)
*Ayesh Abu Lehyeh,Anastassia Gharib,Safwan Wshah*

Main category: cs.LG

TL;DR: 提出SAC - GT框架用于室内定位，结合GT模型与SACP方法，在大型数据集评估中实现高精度定位及可靠性保障。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的室内定位模型无法量化预测不确定性，难以满足实际部署需求，本文旨在解决该问题。

Method: 提出SAC - GT框架，整合捕获网络空间拓扑和信号强度动态的Graph Transformer (GT)模型与提供区域特定不确定性估计的Spatially - Adaptive Conformal Prediction (SACP)方法。

Result: 在大规模真实数据集上的评估表明，SAC - GT实现了最先进的定位精度。

Conclusion: SAC - GT不仅能产生精确的二维位置预测，还能提供适应不同环境条件的统计有效置信区域，实现了高精度定位和可靠保障。

Abstract: Indoor localization is a critical enabler for a wide range of location-based services in smart environments, including navigation, asset tracking, and safety-critical applications. Recent graph-based models leverage spatial relationships between Wire-less Fidelity (Wi-Fi) Access Points (APs) and devices, offering finer localization granularity, but fall short in quantifying prediction uncertainty, a key requirement for real-world deployment. In this paper, we propose Spatially-Adaptive Conformal Graph Transformer (SAC-GT), a framework for accurate and reliable indoor localization. SAC-GT integrates a Graph Transformer (GT) model that captures network's spatial topology and signal strength dynamics, with a novel Spatially-Adaptive Conformal Prediction (SACP) method that provides region-specific uncertainty estimates. This allows SAC-GT to produce not only precise two-dimensional (2D) location predictions but also statistically valid confidence regions tailored to varying environmental conditions. Extensive evaluations on a large-scale real-world dataset demonstrate that the proposed SAC-GT solution achieves state-of-the-art localization accuracy while delivering robust and spatially adaptive reliability guarantees.

</details>


### [123] [Perplexity Cannot Always Tell Right from Wrong](https://arxiv.org/abs/2601.22950)
*Petar Veličković,Federico Barbero,Christos Perivolaropoulos,Simon Osindero,Razvan Pascanu*

Main category: cs.LG

TL;DR: 本文严谨证明困惑度可能不适合作为模型选择指标，分析等困惑度图发现其选不出更准确模型。


<details>
  <summary>Details</summary>
Motivation: 先前研究从经验角度指出困惑度的局限性，本文希望严谨证明困惑度不适合作为模型选择指标。

Method: 利用Transformer连续性的最新结果进行证明，分析等困惑度图。

Result: 证明若有紧凑的仅解码器Transformer模型能准确自信预测某序列，必有低困惑度但预测错误的序列；困惑度不总能选出更准确模型。

Conclusion: 困惑度可能是不适合用于模型选择的指标。

Abstract: Perplexity -- a function measuring a model's overall level of "surprise" when encountering a particular output -- has gained significant traction in recent years, both as a loss function and as a simple-to-compute metric of model quality. Prior studies have pointed out several limitations of perplexity, often from an empirical manner. Here we leverage recent results on Transformer continuity to show in a rigorous manner how perplexity may be an unsuitable metric for model selection. Specifically, we prove that, if there is any sequence that a compact decoder-only Transformer model predicts accurately and confidently -- a necessary pre-requisite for strong generalisation -- it must imply existence of another sequence with very low perplexity, but not predicted correctly by that same model. Further, by analytically studying iso-perplexity plots, we find that perplexity will not always select for the more accurate model -- rather, any increase in model confidence must be accompanied by a commensurate rise in accuracy for the new model to be selected.

</details>


### [124] [Models Under SCOPE: Scalable and Controllable Routing via Pre-hoc Reasoning](https://arxiv.org/abs/2601.22323)
*Qi Cao,Shuhao Zhang,Ruizhe Zhou,Ruiyi Zhang,Peijia Qin,Pengtao Xie*

Main category: cs.LG

TL;DR: 提出路由框架SCOPE，超越模型选择，实验显示其能灵活适应不同需求，实现不同程度的提效或降本。


<details>
  <summary>Details</summary>
Motivation: 现有模型路由方法大多固定选择模型，难以适应新模型或预算变化，需改进。

Method: 提出SCOPE框架，通过强化学习训练，基于推理预测模型成本和性能，而非依赖固定模型名称。

Result: SCOPE能灵活适应需求，性能优先时精度提升25.7%，效率优先时成本降低95.1%。

Conclusion: SCOPE不仅能节省成本，还能灵活满足用户对精度和成本的不同需求。

Abstract: Model routing chooses which language model to use for each query. By sending easy queries to cheaper models and hard queries to stronger ones, it can significantly reduce inference cost while maintaining high accuracy. However, most existing routers treat this as a fixed choice among a small set of models, which makes them hard to adapt to new models or changing budget constraints. In this paper, we propose SCOPE (Scalable and Controllable Outcome Performance Estimator), a routing framework that goes beyond model selection by predicting their cost and performance. Trained with reinforcement learning, SCOPE makes reasoning-based predictions by retrieving how models behave on similar problems, rather than relying on fixed model names, enabling it to work with new, unseen models. Moreover, by explicitly predicting how accurate and how expensive a model will be, it turns routing into a dynamic decision problem, allowing users to easily control the trade-off between accuracy and cost. Experiments show that SCOPE is more than just a cost-saving tool. It flexibly adapts to user needs: it can boost accuracy by up to 25.7% when performance is the priority, or cut costs by up to 95.1% when efficiency matters most.

</details>


### [125] [Value-at-Risk Constrained Policy Optimization](https://arxiv.org/abs/2601.22993)
*Rohan Tangri,Jan-Peter Calliess*

Main category: cs.LG

TL;DR: 本文提出VaR - CPO算法，可直接优化VaR约束，能安全探索，克服了VaR约束不可微问题并给出训练的严格界。


<details>
  <summary>Details</summary>
Motivation: 设计一种样本高效且保守的方法来直接优化风险价值（VaR）约束，实现安全探索。

Method: 采用单边切比雪夫不等式获得基于成本回报前两阶矩的可处理替代，扩展约束策略优化（CPO）方法的信任区域框架。

Result: VaR - CPO能够进行安全探索，在可行环境训练中实现零约束违反，而基线方法无法做到。

Conclusion: VaR - CPO算法能有效直接优化VaR约束，保障训练时的安全性。

Abstract: We introduce the Value-at-Risk Constrained Policy Optimization algorithm (VaR-CPO), a sample efficient and conservative method designed to optimize Value-at-Risk (VaR) constraints directly. Empirically, we demonstrate that VaR-CPO is capable of safe exploration, achieving zero constraint violations during training in feasible environments, a critical property that baseline methods fail to uphold. To overcome the inherent non-differentiability of the VaR constraint, we employ the one-sided Chebyshev inequality to obtain a tractable surrogate based on the first two moments of the cost return. Additionally, by extending the trust-region framework of the Constrained Policy Optimization (CPO) method, we provide rigorous worst-case bounds for both policy improvement and constraint violation during the training process.

</details>


### [126] [AgentScore: Autoformulation of Deployable Clinical Scoring Systems](https://arxiv.org/abs/2601.22324)
*Silas Ruhrberg Estévez,Christopher Chiu,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 介绍AgentScore方法用于生成临床评分系统，在多个临床预测任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习模型因与临床工作流约束不匹配，难以转化为常规临床应用，而可部署的临床指南评分学习需搜索指数级大的离散空间。

Method: 引入AgentScore，利用大语言模型提出候选规则，并通过确定性、基于数据的验证和选择循环来执行统计有效性和可部署性约束。

Result: 在八个临床预测任务中，AgentScore优于现有分数生成方法，在两个额外的外部验证任务中，其判别能力高于既定的基于指南的分数。

Conclusion: AgentScore在临床评分系统生成方面有较好效果，能在强结构约束下取得良好表现。

Abstract: Modern clinical practice relies on evidence-based guidelines implemented as compact scoring systems composed of a small number of interpretable decision rules. While machine-learning models achieve strong performance, many fail to translate into routine clinical use due to misalignment with workflow constraints such as memorability, auditability, and bedside execution. We argue that this gap arises not from insufficient predictive power, but from optimizing over model classes that are incompatible with guideline deployment. Deployable guidelines often take the form of unit-weighted clinical checklists, formed by thresholding the sum of binary rules, but learning such scores requires searching an exponentially large discrete space of possible rule sets. We introduce AgentScore, which performs semantically guided optimization in this space by using LLMs to propose candidate rules and a deterministic, data-grounded verification-and-selection loop to enforce statistical validity and deployability constraints. Across eight clinical prediction tasks, AgentScore outperforms existing score-generation methods and achieves AUC comparable to more flexible interpretable models despite operating under stronger structural constraints. On two additional externally validated tasks, AgentScore achieves higher discrimination than established guideline-based scores.

</details>


### [127] [YuriiFormer: A Suite of Nesterov-Accelerated Transformers](https://arxiv.org/abs/2601.23236)
*Aleksandr Zimin,Yury Polyanskiy,Philippe Rigollet*

Main category: cs.LG

TL;DR: 提出变分框架将Transformer层解释为优化算法迭代，引入Nesterov式加速Transformer并取得更好表现。


<details>
  <summary>Details</summary>
Motivation: 利用经典优化思想进行Transformer架构的原则性设计。

Method: 提出变分框架，将自注意力和MLP层分别对应交互能量和势能的梯度更新，标准GPT式Transformer是复合目标的梯度下降，还引入Nesterov式加速Transformer。

Result: 加速Transformer在TinyStories和OpenWebText上持续优于nanoGPT基线。

Conclusion: 优化理论见解能转化为实际的性能提升。

Abstract: We propose a variational framework that interprets transformer layers as iterations of an optimization algorithm acting on token embeddings. In this view, self-attention implements a gradient step of an interaction energy, while MLP layers correspond to gradient updates of a potential energy. Standard GPT-style transformers emerge as vanilla gradient descent on the resulting composite objective, implemented via Lie--Trotter splitting between these two energy functionals. This perspective enables principled architectural design using classical optimization ideas. As a proof of concept, we introduce a Nesterov-style accelerated transformer that preserves the same attention and MLP oracles. The resulting architecture consistently outperforms a nanoGPT baseline on TinyStories and OpenWebText, demonstrating that optimization-theoretic insights can translate into practical gains.

</details>


### [128] [Label-Efficient Monitoring of Classification Models via Stratified Importance Sampling](https://arxiv.org/abs/2601.22326)
*Lupo Marsigli,Angel Lopez de Haro*

Main category: cs.LG

TL;DR: 提出基于分层重要性抽样（SIS）的通用框架用于生产中分类模型性能监控，理论证明其适用性，实验显示效率提升。


<details>
  <summary>Details</summary>
Motivation: 生产中分类模型性能监控因标注预算、标签获取方式和低错误率等因素面临挑战。

Method: 提出基于SIS的通用框架，进行理论分析其适用性和性能。

Result: SIS在温和条件下有严格有限样本均方误差改进，实验在固定标注预算下有一致效率提升。

Conclusion: SIS是用于模型部署后监控的有原则、标签高效且操作轻量的方法。

Abstract: Monitoring the performance of classification models in production is critical yet challenging due to strict labeling budgets, one-shot batch acquisition of labels and extremely low error rates. We propose a general framework based on Stratified Importance Sampling (SIS) that directly addresses these constraints in model monitoring. While SIS has previously been applied in specialized domains, our theoretical analysis establishes its broad applicability to the monitoring of classification models. Under mild conditions, SIS yields unbiased estimators with strict finite-sample mean squared error (MSE) improvements over both importance sampling (IS) and stratified random sampling (SRS). The framework does not rely on optimally defined proposal distributions or strata: even with noisy proxies and sub-optimal stratification, SIS can improve estimator efficiency compared to IS or SRS individually, though extreme proposal mismatch may limit these gains. Experiments across binary and multiclass tasks demonstrate consistent efficiency improvements under fixed label budgets, underscoring SIS as a principled, label-efficient, and operationally lightweight methodology for post-deployment model monitoring.

</details>


### [129] [Molecular Representations in Implicit Functional Space via Hyper-Networks](https://arxiv.org/abs/2601.22327)
*Zehong Wang,Xiaolong Han,Qi Yang,Xiangru Tang,Fang Wu,Xiaoguang Guo,Weixiang Sun,Tianyi Ma,Pietro Lio,Le Cong,Sheng Wang,Chuxu Zhang,Yanfang Ye*

Main category: cs.LG

TL;DR: 提出将分子学习建模为函数空间学习，用MolField框架评估，结果表明将分子视为连续函数能改变泛化能力和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有方法将分子视为离散对象，未考虑其连续和场状物理本质，因此提出在函数空间进行分子学习。

Method: 将分子建模为三维空间上的连续函数，用基于超网络的MolField框架学习分子场分布，采用结构化权重分词并训练基于序列的超网络。

Result: 在分子动力学和属性预测任务中，将分子视为连续函数能改变分子表示在任务间的泛化能力，下游行为对分子离散化或查询方式具有稳定性。

Conclusion: 在函数空间进行分子学习是可行的，能带来更好的泛化和稳定性。

Abstract: Molecular representations fundamentally shape how machine learning systems reason about molecular structure and physical properties. Most existing approaches adopt a discrete pipeline: molecules are encoded as sequences, graphs, or point clouds, mapped to fixed-dimensional embeddings, and then used for task-specific prediction. This paradigm treats molecules as discrete objects, despite their intrinsically continuous and field-like physical nature. We argue that molecular learning can instead be formulated as learning in function space. Specifically, we model each molecule as a continuous function over three-dimensional (3D) space and treat this molecular field as the primary object of representation. From this perspective, conventional molecular representations arise as particular sampling schemes of an underlying continuous object. We instantiate this formulation with MolField, a hyper-network-based framework that learns distributions over molecular fields. To ensure physical consistency, these functions are defined over canonicalized coordinates, yielding invariance to global SE(3) transformations. To enable learning directly over functions, we introduce a structured weight tokenization and train a sequence-based hyper-network to model a shared prior over molecular fields. We evaluate MolField on molecular dynamics and property prediction. Our results show that treating molecules as continuous functions fundamentally changes how molecular representations generalize across tasks and yields downstream behavior that is stable to how molecules are discretized or queried.

</details>


### [130] [Knowledge-Informed Kernel State Reconstruction for Interpretable Dynamical System Discovery](https://arxiv.org/abs/2601.22328)
*Luca Muscarnera,Silas Ruhrberg Estévez,Samuel Holt,Evgeny Saveliev,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 提出MAAT框架用于符号发现，在多个基准测试中降低状态估计均方误差。


<details>
  <summary>Details</summary>
Motivation: 现有从数据中恢复控制方程的方法在噪声、部分观测下易失效，且依赖黑盒潜在动力学模糊机制。

Method: 引入基于知识驱动的核状态重建的MAAT框架，在再生核希尔伯特空间中进行状态重建，将结构和语义先验纳入重建目标。

Result: 在十二个不同科学基准和多种噪声环境下，相对于强基线，MAAT大幅降低了下游符号回归所用轨迹和导数的状态估计均方误差。

Conclusion: MAAT框架能有效处理数据，为碎片化传感器数据和符号回归提供了有效接口。

Abstract: Recovering governing equations from data is central to scientific discovery, yet existing methods often break down under noisy, partial observations, or rely on black-box latent dynamics that obscure mechanism. We introduce MAAT (Model Aware Approximation of Trajectories), a framework for symbolic discovery built on knowledge-informed Kernel State Reconstruction. MAAT formulates state reconstruction in a reproducing kernel Hilbert space and directly incorporates structural and semantic priors such as non-negativity, conservation laws, and domain-specific observation models into the reconstruction objective, while accommodating heterogeneous sampling and measurement granularity. This yields smooth, physically consistent state estimates with analytic time derivatives, providing a principled interface between fragmented sensor data and symbolic regression. Across twelve diverse scientific benchmarks and multiple noise regimes, MAAT substantially reduces state-estimation MSE for trajectories and derivatives used by downstream symbolic regression relative to strong baselines.

</details>


### [131] [DP-$λ$CGD: Efficient Noise Correlation for Differentially Private Model Training](https://arxiv.org/abs/2601.22334)
*Nikita P. Kalinin,Ryan McKenna,Rasmus Pagh,Christoph H. Lampert*

Main category: cs.LG

TL;DR: 提出新的噪声关联策略，无需额外内存，有最小计算开销且提升准确性。


<details>
  <summary>Details</summary>
Motivation: 现有DP - SGD扩展方法有较大内存开销，需改进。

Method: 提出新噪声关联策略，仅与前一次迭代关联并抵消部分噪声，用伪随机噪声生成器再生噪声。

Result: 无需标准DP - SGD之外的额外内存，计算开销小，准确性提升。

Conclusion: 新策略在内存和准确性上优于之前方法。

Abstract: Differentially private stochastic gradient descent (DP-SGD) is the gold standard for training machine learning models with formal differential privacy guarantees. Several recent extensions improve its accuracy by introducing correlated noise across training iterations. Matrix factorization mechanisms are a prominent example, but they correlate noise across many iterations and require storing previously added noise vectors, leading to substantial memory overhead in some settings. In this work, we propose a new noise correlation strategy that correlates noise only with the immediately preceding iteration and cancels a controlled portion of it. Our method relies on noise regeneration using a pseudorandom noise generator, eliminating the need to store past noise. As a result, it requires no additional memory beyond standard DP-SGD. We show that the computational overhead is minimal and empirically demonstrate improved accuracy over DP-SGD.

</details>


### [132] [Quantum-Inspired Reinforcement Learning for Secure and Sustainable AIoT-Driven Supply Chain Systems](https://arxiv.org/abs/2601.22339)
*Muhammad Bilal Akram Dastagir,Omer Tariq,Shahid Mumtaz,Saif Al-Kuwari,Ahmed Farouk*

Main category: cs.LG

TL;DR: 本文提出量子启发强化学习框架用于供应链优化，在模拟中表现良好，展示了量子启发AIoT框架推动可持续、安全供应链运营的潜力。


<details>
  <summary>Details</summary>
Motivation: 现代供应链需平衡物流速度、环境影响和安全约束，但传统优化模型忽视可持续目标和网络漏洞，因此需新方法解决这些挑战。

Method: 集成量子启发强化学习框架，结合可控自旋链类比与实时AIoT信号，优化统一保真度、安全性和碳成本的多目标奖励，通过基于价值和集成更新学习策略，并使用窗口归一化奖励组件。

Result: 该方法在模拟中收敛平滑，后期表现好，在噪声通道下性能退化平稳，优于标准学习和基于模型的参考方法。

Conclusion: 量子启发的AIoT框架有潜力大规模推动安全、环保的供应链运营，为满足消费者和环境需求的全球互联基础设施奠定基础。

Abstract: Modern supply chains must balance high-speed logistics with environmental impact and security constraints, prompting a surge of interest in AI-enabled Internet of Things (AIoT) solutions for global commerce. However, conventional supply chain optimization models often overlook crucial sustainability goals and cyber vulnerabilities, leaving systems susceptible to both ecological harm and malicious attacks. To tackle these challenges simultaneously, this work integrates a quantum-inspired reinforcement learning framework that unifies carbon footprint reduction, inventory management, and cryptographic-like security measures. We design a quantum-inspired reinforcement learning framework that couples a controllable spin-chain analogy with real-time AIoT signals and optimizes a multi-objective reward unifying fidelity, security, and carbon costs. The approach learns robust policies with stabilized training via value-based and ensemble updates, supported by window-normalized reward components to ensure commensurate scaling. In simulation, the method exhibits smooth convergence, strong late-episode performance, and graceful degradation under representative noise channels, outperforming standard learned and model-based references, highlighting its robust handling of real-time sustainability and risk demands. These findings reinforce the potential for quantum-inspired AIoT frameworks to drive secure, eco-conscious supply chain operations at scale, laying the groundwork for globally connected infrastructures that responsibly meet both consumer and environmental needs.

</details>


### [133] [Failing to Explore: Language Models on Interactive Tasks](https://arxiv.org/abs/2601.22345)
*Mahdi JafariRaviz,Keivan Rezaei,Arshia Soltani Moakhar,Zahra Sodagar,Yize Cheng,Soheil Feizi*

Main category: cs.LG

TL;DR: 评估语言模型在有限交互预算下探索交互环境的能力，发现存在探索不足等问题，并研究两种轻量级干预方法可提升其性能。


<details>
  <summary>Details</summary>
Motivation: 评估语言模型在有限交互预算下探索交互环境的能力。

Method: 引入三个具有可控探索难度的参数任务，跨越连续和离散环境，研究两种轻量级干预方法。

Result: 发现模型存在系统性的探索不足和次优解决方案，性能常不如简单启发式基线，且预算增加时性能提升弱；并行执行预算和总结交互历史可提升性能。

Conclusion: 所提出的两种轻量级干预方法能改善语言模型在交互环境中的探索性能。

Abstract: We evaluate language models on their ability to explore interactive environments under a limited interaction budget. We introduce three parametric tasks with controllable exploration difficulty, spanning continuous and discrete environments. Across state-of-the-art models, we find systematic under-exploration and suboptimal solutions, with performance often significantly worse than simple explore--exploit heuristic baselines and scaling weakly as the budget increases. Finally, we study two lightweight interventions: splitting a fixed budget into parallel executions, which surprisingly improves performance despite a no-gain theoretical result for our tasks, and periodically summarizing the interaction history, which preserves key discoveries and further improves exploration.

</details>


### [134] [MixQuant: Pushing the Limits of Block Rotations in Post-Training Quantization](https://arxiv.org/abs/2601.22347)
*Sai Sanjeet,Ian Colbert,Pablo Monteagudo-Lago,Giuseppe Franco,Yaman Umuroglu,Nicholas J. Fraser*

Main category: cs.LG

TL;DR: 文章针对块哈达玛旋转的离群值抑制进行分析，提出MixQuant框架，实验表明其能提升量化精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于块旋转的训练后量化方法中，块结构对离群值抑制的影响尚不明确，需填补该研究空白。

Method: 对块哈达玛旋转的离群值抑制进行系统、非渐近分析，提出MixQuant框架，通过排列重新分配激活质量，并采用贪婪质量扩散算法校准排列，将排列合并到模型权重中。

Result: MixQuant在所有块大小下均能提高精度，在将Llama3 1B量化为INT4且块大小为16时，能恢复全向量旋转困惑度的90%，而不使用排列时仅为46%。

Conclusion: MixQuant框架有效，能提升训练后量化的精度。

Abstract: Recent post-training quantization (PTQ) methods have adopted block rotations to diffuse outliers prior to rounding. While this reduces the overhead of full-vector rotations, the effect of block structure on outlier suppression remains poorly understood. To fill this gap, we present the first systematic, non-asymptotic analysis of outlier suppression for block Hadamard rotations. Our analysis reveals that outlier suppression is fundamentally limited by the geometry of the input vector. In particular, post-rotation outliers are deterministically minimized when the pre-rotation $\ell_1$ norm mass is evenly distributed across blocks. Guided by these insights, we introduce MixQuant, a block rotation-aware PTQ framework that redistributes activation mass via permutations prior to rotation. We propose a greedy mass diffusion algorithm to calibrate permutations by equalizing the expected blockwise $\ell_1$ norms. To avoid adding inference overhead, we identify permutation-equivariant regions in transformer architectures to merge the resulting permutations into model weights before deployment. Experiments show that MixQuant consistently improves accuracy across all block sizes, recovering up to 90% of the full-vector rotation perplexity when quantizing Llama3 1B to INT4 with block size 16, compared to 46% without permutations.

</details>


### [135] [Learning Policy Representations for Steerable Behavior Synthesis](https://arxiv.org/abs/2601.22350)
*Beiming Li,Sergio Rozada,Alejandro Ribeiro*

Main category: cs.LG

TL;DR: 本文提出为马尔可夫决策过程（MDP）的一系列策略学习表示，以在测试时实现行为引导，通过特定架构近似表示，解决了新的行为合成任务。


<details>
  <summary>Details</summary>
Motivation: 在马尔可夫决策过程中，为了在测试时便于进行行为引导，需要学习一系列策略的表示。

Method: 将策略表示建模为状态 - 动作特征映射关于占用度量的期望；使用基于集合的架构近似表示；采用变分生成方法和对比学习塑造潜在空间，使潜在距离与值函数差异对齐。

Result: 模型能将状态 - 动作样本集编码为潜在嵌入，解码出策略和对应多个奖励的值函数；潜在空间可进行基于梯度的优化。

Conclusion: 利用该模型的能力解决了新的行为合成任务，可在无额外训练情况下引导策略满足未见的值函数约束。

Abstract: Given a Markov decision process (MDP), we seek to learn representations for a range of policies to facilitate behavior steering at test time. As policies of an MDP are uniquely determined by their occupancy measures, we propose modeling policy representations as expectations of state-action feature maps with respect to occupancy measures. We show that these representations can be approximated uniformly for a range of policies using a set-based architecture. Our model encodes a set of state-action samples into a latent embedding, from which we decode both the policy and its value functions corresponding to multiple rewards. We use variational generative approach to induce a smooth latent space, and further shape it with contrastive learning so that latent distances align with differences in value functions. This geometry permits gradient-based optimization directly in the latent space. Leveraging this capability, we solve a novel behavior synthesis task, where policies are steered to satisfy previously unseen value function constraints without additional training.

</details>


### [136] [Recoverability Has a Law: The ERR Measure for Tool-Augmented Agents](https://arxiv.org/abs/2601.22352)
*Sri Vatsa Vuddanti,Satwik Kumar Chittiprolu*

Main category: cs.LG

TL;DR: 本文提出一种预测理论，解释语言模型代理工具调用失败后自我恢复的行为，定义预期恢复后悔值（ERR）并推导其与效率分数（ES）一阶关系，实证验证该理论在多个基准有效。


<details>
  <summary>Details</summary>
Motivation: 语言模型代理工具调用失败后自我恢复行为缺乏正式解释，需要理论来填补空白。

Method: 通过预期恢复后悔值（ERR）形式化可恢复性，推导ERR与效率分数（ES）的一阶关系，在多个工具使用基准上进行实证验证。

Result: 在不同模型规模、扰动机制和恢复范围下，ERR - ES定律预测的后悔值与蒙特卡罗滚动测量的失败后后悔值匹配度高，误差不超过0.05。

Conclusion: 可恢复性是交互动力学的固有属性，而非模型规模或架构的产物，为语言代理执行层面的鲁棒性提供理论基础。

Abstract: Language model agents often appear capable of self-recovery after failing tool call executions, yet this behavior lacks a formal explanation. We present a predictive theory that resolves this gap by showing that recoverability follows a measurable law. To elaborate, we formalize recoverability through Expected Recovery Regret (ERR), which quantifies the deviation of a recovery policy from the optimal one under stochastic execution noise, and derive a first-order relationship between ERR and an empirical observable quantity, the Efficiency Score (ES). This yields a falsifiable first-order quantitative law of recovery dynamics in tool-using agents. We empirically validate the law across five tool-use benchmarks spanning controlled perturbations, diagnostic reasoning, and real-world APIs. Across model scales, perturbation regimes, and recovery horizons, predicted regret under the ERR-ES law closely matched observed post-failure regret measured from Monte Carlo rollouts, within delta less than or equal to 0.05. Our results reveal that recoverability is not an artifact of model scale or architecture, but a governed property of interaction dynamics, providing a theoretical foundation for execution-level robustness in language agents.

</details>


### [137] [Relative Wasserstein Angle and the Problem of the $W_2$-Nearest Gaussian Distribution](https://arxiv.org/abs/2601.22355)
*Binshuai Wang,Peng Wei*

Main category: cs.LG

TL;DR: 在最优传输框架下研究经验分布与高斯性偏差的量化问题，引入新几何量，证明空间性质，推导一维表达式、开发高维算法，实验表明新方法更优。


<details>
  <summary>Details</summary>
Motivation: 量化经验分布与高斯性的偏差，改进高斯近似方法。

Method: 利用相对平移不变二次Wasserstein空间的锥几何，引入相对Wasserstein角和正交投影距离；证明空间填充锥性质；推导一维表达式；开发基于半离散对偶公式的高维随机流形优化算法。

Result: 在合成数据和真实特征分布实验中，相对Wasserstein角比Wasserstein距离更鲁棒，提出的最近高斯近似在评估Fréchet Inception Distance (FID)分数时优于矩匹配。

Conclusion: 所提几何量和方法能有效解决经验分布与高斯性偏差量化问题，为高斯近似提供新视角和更好方法。

Abstract: We study the problem of quantifying how far an empirical distribution deviates from Gaussianity under the framework of optimal transport. By exploiting the cone geometry of the relative translation invariant quadratic Wasserstein space, we introduce two novel geometric quantities, the relative Wasserstein angle and the orthogonal projection distance, which provide meaningful measures of non-Gaussianity. We prove that the filling cone generated by any two rays in this space is flat, ensuring that angles, projections, and inner products are rigorously well-defined. This geometric viewpoint recasts Gaussian approximation as a projection problem onto the Gaussian cone and reveals that the commonly used moment-matching Gaussian can \emph{not} be the \(W_2\)-nearest Gaussian for a given empirical distribution. In one dimension, we derive closed-form expressions for the proposed quantities and extend them to several classical distribution families, including uniform, Laplace, and logistic distributions; while in high dimensions, we develop an efficient stochastic manifold optimization algorithm based on a semi-discrete dual formulation. Experiments on synthetic data and real-world feature distributions demonstrate that the relative Wasserstein angle is more robust than the Wasserstein distance and that the proposed nearest Gaussian provides a better approximation than moment matching in the evaluation of Fréchet Inception Distance (FID) scores.

</details>


### [138] [PoSafeNet: Safe Learning with Poset-Structured Neural Nets](https://arxiv.org/abs/2601.22356)
*Kiwan Wong,Wei Xiao,Daniela Rus*

Main category: cs.LG

TL;DR: 现有安全学习方法处理约束有问题，提出PoSafeNet，实验证明其更优。


<details>
  <summary>Details</summary>
Motivation: 现有方法对安全约束处理不合理，实际安全要求异构且只有部分优先级关系，需更好方法。

Method: 将安全约束建模为偏序集，提出PoSafeNet，通过偏序一致约束顺序下的顺序闭式投影实施安全。

Result: 在多障碍导航、受限机器人操作和基于视觉的自动驾驶实验中，优于无结构和基于可微二次规划的安全层。

Conclusion: PoSafeNet在可行性、鲁棒性和可扩展性方面表现更好，能有效处理偏序结构安全问题。

Abstract: Safe learning is essential for deploying learningbased controllers in safety-critical robotic systems, yet existing approaches often enforce multiple safety constraints uniformly or via fixed priority orders, leading to infeasibility and brittle behavior. In practice, safety requirements are heterogeneous and admit only partial priority relations, where some constraints are comparable while others are inherently incomparable. We formalize this setting as poset-structured safety, modeling safety constraints as a partially ordered set and treating safety composition as a structural property of the policy class. Building on this formulation, we propose PoSafeNet, a differentiable neural safety layer that enforces safety via sequential closed-form projection under poset-consistent constraint orderings, enabling adaptive selection or mixing of valid safety executions while preserving priority semantics by construction. Experiments on multi-obstacle navigation, constrained robot manipulation, and vision-based autonomous driving demonstrate improved feasibility, robustness, and scalability over unstructured and differentiable quadratic program-based safety layers.

</details>


### [139] [Small Talk, Big Impact: The Energy Cost of Thanking AI](https://arxiv.org/abs/2601.22357)
*Julien Delavande,Regis Pierrard,Sasha Luccioni*

Main category: cs.LG

TL;DR: 文章量化与大语言模型交互时礼貌用语的能耗，分析多因素对能耗的影响并为构建更可持续模型提供见解。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型广泛应用，处理大量提示，有必要理解并降低与模型交互时的能耗。

Method: 使用真实对话记录和细粒度能耗测量，分析输入输出长度和模型大小对能耗的影响。

Result: 量化了多因素对能耗的影响，以礼貌用语为典型示例。

Conclusion: 研究结果为构建更可持续高效的大语言模型应用提供了可操作的见解。

Abstract: Being polite is free - or is it? In this paper, we quantify the energy cost of seemingly innocuous messages such as ``thank you'' when interacting with large language models, often used by users to convey politeness. Using real-world conversation traces and fine-grained energy measurements, we quantify how input length, output length and model size affect energy use. While politeness is our motivating example, it also serves as a controlled and reproducible proxy for measuring the energy footprint of a typical LLM interaction. Our findings provide actionable insights for building more sustainable and efficient LLM applications, especially in increasingly widespread real-world contexts like chat. As user adoption grows and billions of prompts are processed daily, understanding and mitigating this cost becomes crucial - not just for efficiency, but for sustainable AI deployment.

</details>


### [140] [The Unseen Threat: Residual Knowledge in Machine Unlearning under Perturbed Samples](https://arxiv.org/abs/2601.22359)
*Hsiang Hsu,Pradeep Niroula,Zichang He,Ivan Brugere,Freddy Lecue,Chun-Fu Chen*

Main category: cs.LG

TL;DR: 本文指出现有机器学习遗忘方法存在隐私风险（残余知识），提出RURK微调策略缓解该风险，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习遗忘方法在输入被对抗性扰动时，模型输出无法保证隐私，存在残余知识的隐私风险。

Method: 提出名为RURK的微调策略，惩罚模型重新识别被扰动的待遗忘样本的能力。

Result: 在视觉基准测试中，发现残余知识在现有遗忘方法中普遍存在，RURK方法能有效防止残余知识。

Conclusion: 提出的RURK策略可有效缓解机器学习遗忘中的残余知识隐私风险。

Abstract: Machine unlearning offers a practical alternative to avoid full model re-training by approximately removing the influence of specific user data. While existing methods certify unlearning via statistical indistinguishability from re-trained models, these guarantees do not naturally extend to model outputs when inputs are adversarially perturbed. In particular, slight perturbations of forget samples may still be correctly recognized by the unlearned model - even when a re-trained model fails to do so - revealing a novel privacy risk: information about the forget samples may persist in their local neighborhood. In this work, we formalize this vulnerability as residual knowledge and show that it is inevitable in high-dimensional settings. To mitigate this risk, we propose a fine-tuning strategy, named RURK, that penalizes the model's ability to re-recognize perturbed forget samples. Experiments on vision benchmarks with deep neural networks demonstrate that residual knowledge is prevalent across existing unlearning methods and that our approach effectively prevents residual knowledge.

</details>


### [141] [Understanding Efficiency: Quantization, Batching, and Serving Strategies in LLM Energy Use](https://arxiv.org/abs/2601.22362)
*Julien Delavande,Regis Pierrard,Sasha Luccioni*

Main category: cs.LG

TL;DR: 研究大语言模型推理能耗，发现系统级设计选择影响大，提出相感知能耗分析和系统级优化建议。


<details>
  <summary>Details</summary>
Motivation: 此前研究多关注单提示或单令牌推理能耗，本文关注系统级设计选择对同一模型能耗的影响。

Method: 在NVIDIA H100 GPU上对大语言模型推理能耗和延迟进行详细实证研究，分析量化、批量大小和服务配置的影响。

Result: 低精度格式仅在计算密集型状态下节能；批处理提高能效，在解码等内存密集型阶段更明显；结构化请求定时可使单请求能耗降低达100倍。

Conclusion: 可持续的大语言模型部署不仅取决于模型内部，还依赖服务栈编排，需进行相感知能耗分析和系统级优化。

Abstract: Large Language Models (LLMs) are increasingly deployed in production, contributing towards shifting the burden in terms of computational resources and energy demands from training to inference. While prior work has examined the energy cost of inference per prompt or per token, we highlight how \emph{system-level design choices} - such as numerical precision, batching strategy, and request scheduling - can lead to orders-of-magnitude differences in energy consumption for the same model. We perform a detailed empirical study of LLM inference energy and latency on NVIDIA H100 GPUs, analyzing the impact of quantization, batch size, and serving configuration (e.g., with Hugging Face's Text Generation Inference server). Our results reveal that lower-precision formats only yield energy gains in compute-bound regimes; that batching improves energy efficiency, especially in memory-bound phases like decoding; and that structured request timing (arrival shaping) can reduce per-request energy by up to 100 times. We argue that sustainable LLM deployment depends not only on model internals, but also on the orchestration of the serving stack. Our findings motivate phase-aware energy profiling and system-level optimizations for greener AI services.

</details>


### [142] [FIRE: Multi-fidelity Regression with Distribution-conditioned In-context Learning using Tabular Foundation Models](https://arxiv.org/abs/2601.22371)
*Rosen Ting-Ying Yu,Nicholas Sung,Faez Ahmed*

Main category: cs.LG

TL;DR: 提出无训练的多保真度框架FIRE，在多个基准问题上比现有方法有更优性能时间权衡，但有上下文窗口限制和依赖预训练模型质量的问题。


<details>
  <summary>Details</summary>
Motivation: 解决多保真度回归中数据极度不平衡时，常用高斯过程代理存在立方缩放成本和过拟合问题，限制效率和泛化能力。

Method: 引入训练无关的MF框架FIRE，结合表格基础模型，通过基于低保真模型后验预测分布的高保真修正模型进行零样本上下文贝叶斯推理。

Result: 在31个基准问题上，FIRE比7种最先进的基于GP或深度学习的MF回归方法有更强的性能时间权衡，在准确性和不确定性量化方面排名最高，且有运行时间优势。

Conclusion: FIRE在多保真度回归中有较好表现，但存在上下文窗口约束和依赖预训练TFM质量的局限性。

Abstract: Multi-fidelity (MF) regression often operates in regimes of extreme data imbalance, where the commonly-used Gaussian-process (GP) surrogates struggle with cubic scaling costs and overfit to sparse high-fidelity observations, limiting efficiency and generalization in real-world applications. We introduce FIRE, a training-free MF framework that couples tabular foundation models (TFMs) to perform zero-shot in-context Bayesian inference via a high-fidelity correction model conditioned on the low-fidelity model's posterior predictive distributions. This cross-fidelity information transfer via distributional summaries captures heteroscedastic errors, enabling robust residual learning without model retraining. Across 31 benchmark problems spanning synthetic and real-world tasks (e.g., DrivAerNet, LCBench), FIRE delivers a stronger performance-time trade-off than seven state-of-the-art GP-based or deep learning MF regression methods, ranking highest in accuracy and uncertainty quantification with runtime advantages. Limitations include context window constraints and dependence on the quality of the pre-trained TFM's.

</details>


### [143] [Purely Agentic Black-Box Optimization for Biological Design](https://arxiv.org/abs/2601.22382)
*Natalie Maus,Yimeng Zeng,Haydn Thomas Jones,Yining Huang,Gaurav Ng Goel,Alden Rose,Kyurae Kim,Hyun-Su Lee,Marcelo Der Torossian Torres,Fangping Wan,Cesar de la Fuente-Nunez,Mark Yatskar,Osbert Bastani,Jacob R. Gardner*

Main category: cs.LG

TL;DR: 提出PABLO系统用于生物黑盒优化，在标准任务上达SOTA，体外验证显示其治疗发现潜力。


<details>
  <summary>Details</summary>
Motivation: 现有生物设计黑盒优化方法依赖原始结构数据，难利用科学文献，含大语言模型的方法作用局限。

Method: 引入PABLO，一个分层的智能体系统，使用在化学和生物学文献上预训练的科学大语言模型生成和迭代优化生物候选物。

Result: 在标准任务上取得SOTA性能，提高样本效率和目标值，运行时令牌使用有竞争力。体外验证中优化的肽对耐药病原体有强活性。

Conclusion: PABLO在生物黑盒优化中有良好表现，在实际设计中有优势，有治疗发现的实用潜力。

Abstract: Many key challenges in biological design-such as small-molecule drug discovery, antimicrobial peptide development, and protein engineering-can be framed as black-box optimization over vast, complex structured spaces. Existing methods rely mainly on raw structural data and struggle to exploit the rich scientific literature. While large language models (LLMs) have been added to these pipelines, they have been confined to narrow roles within structure-centered optimizers. We instead cast biological black-box optimization as a fully agentic, language-based reasoning process. We introduce Purely Agentic BLack-box Optimization (PABLO), a hierarchical agentic system that uses scientific LLMs pretrained on chemistry and biology literature to generate and iteratively refine biological candidates. On both the standard GuacaMol molecular design and antimicrobial peptide optimization tasks, PABLO achieves state-of-the-art performance, substantially improving sample efficiency and final objective values over established baselines. Compared to prior optimization methods that incorporate LLMs, PABLO achieves competitive token usage per run despite relying on LLMs throughout the optimization loop. Beyond raw performance, the agentic formulation offers key advantages for realistic design: it naturally incorporates semantic task descriptions, retrieval-augmented domain knowledge, and complex constraints. In follow-up in vitro validation, PABLO-optimized peptides showed strong activity against drug-resistant pathogens, underscoring the practical potential of PABLO for therapeutic discovery.

</details>


### [144] [Graph is a Substrate Across Data Modalities](https://arxiv.org/abs/2601.22384)
*Ziming Li,Xiaoming Wu,Zehong Wang,Jiazheng Li,Yijun Tian,Jinhe Bi,Yunpu Ma,Yanfang Ye,Chuxu Zhang*

Main category: cs.LG

TL;DR: 本文指出图结构学习常孤立进行的问题，提出G - Substrate框架组织图结构学习，实验显示其性能更优。


<details>
  <summary>Details</summary>
Motivation: 当前图结构学习多在单个任务和模态中孤立进行，重复构建结构，无法积累不同任务和模态间的结构规律，因此需要研究如何组织图结构使其能跨模态和任务积累。

Method: 采用以表示为中心的视角，提出G - Substrate框架，包含统一结构模式和基于角色的交错训练策略。

Result: 在多个领域、模态和任务的实验中，G - Substrate框架优于任务孤立和简单的多任务学习方法。

Conclusion: G - Substrate框架能有效组织图结构学习，在跨模态和任务场景中表现良好。

Abstract: Graphs provide a natural representation of relational structure that arises across diverse domains. Despite this ubiquity, graph structure is typically learned in a modality- and task-isolated manner, where graph representations are constructed within individual task contexts and discarded thereafter. As a result, structural regularities across modalities and tasks are repeatedly reconstructed rather than accumulated at the level of intermediate graph representations. This motivates a representation-learning question: how should graph structure be organized so that it can persist and accumulate across heterogeneous modalities and tasks? We adopt a representation-centric perspective in which graph structure is treated as a structural substrate that persists across learning contexts. To instantiate this perspective, we propose G-Substrate, a graph substrate framework that organizes learning around shared graph structures. G-Substrate comprises two complementary mechanisms: a unified structural schema that ensures compatibility among graph representations across heterogeneous modalities and tasks, and an interleaved role-based training strategy that exposes the same graph structure to multiple functional roles during learning. Experiments across multiple domains, modalities, and tasks show that G-Substrate outperforms task-isolated and naive multi-task learning methods.

</details>


### [145] [Score-based Integrated Gradient for Root Cause Explanations of Outliers](https://arxiv.org/abs/2601.22399)
*Phuoc Nguyen,Truyen Tran,Sunil Gupta,Svetha Venkatesh*

Main category: cs.LG

TL;DR: 提出SIREN方法用于识别异常值根源，在多数据集上表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 传统基于启发式或反事实推理的方法在不确定性和高维依赖下难以识别异常值根源。

Method: 引入SIREN方法，通过估计数据似然的得分函数来确定异常值根源，用积分梯度计算归因，满足部分Shapley值公理和不对称公理。

Result: 在合成随机图、云服务和供应链等数据集上实验表明，SIREN在归因准确性和计算效率上优于现有基线。

Conclusion: SIREN能在非线性、高维、异方差因果模型中进行可处理且考虑不确定性的根源归因，是一种有效方法。

Abstract: Identifying the root causes of outliers is a fundamental problem in causal inference and anomaly detection. Traditional approaches based on heuristics or counterfactual reasoning often struggle under uncertainty and high-dimensional dependencies. We introduce SIREN, a novel and scalable method that attributes the root causes of outliers by estimating the score functions of the data likelihood. Attribution is computed via integrated gradients that accumulate score contributions along paths from the outlier toward the normal data distribution. Our method satisfies three of the four classic Shapley value axioms - dummy, efficiency, and linearity - as well as an asymmetry axiom derived from the underlying causal structure. Unlike prior work, SIREN operates directly on the score function, enabling tractable and uncertainty-aware root cause attribution in nonlinear, high-dimensional, and heteroscedastic causal models. Extensive experiments on synthetic random graphs and real-world cloud service and supply chain datasets show that SIREN outperforms state-of-the-art baselines in both attribution accuracy and computational efficiency.

</details>


### [146] [MM-OpenFGL: A Comprehensive Benchmark for Multimodal Federated Graph Learning](https://arxiv.org/abs/2601.22416)
*Xunkai Li,Yuming Ai,Yinlin Zhu,Haodong Lu,Yi Zhang,Guohao Fu,Bowen Fan,Qiangqiang Dai,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 提出首个全面基准MM - OpenFGL以解决多模态联邦图学习问题，含多数据集、策略、任务和方法，并通过实验研究提供见解。


<details>
  <summary>Details</summary>
Motivation: 现实中多模态属性图分布式且不能共享，现有联邦图学习研究多针对单模态图，未解决多模态联邦图学习的独特挑战。

Method: 提出MM - OpenFGL基准，包含19个多模态数据集、8种模拟策略、6种下游任务和57种先进方法，通过模块化API实现。

Result: 通过大量实验从必要性、有效性、鲁棒性和效率等方面对多模态联邦图学习进行了研究。

Conclusion: 为多模态联邦图学习的未来研究提供了有价值的见解。

Abstract: Multimodal-attributed graphs (MMAGs) provide a unified framework for modeling complex relational data by integrating heterogeneous modalities with graph structures. While centralized learning has shown promising performance, MMAGs in real-world applications are often distributed across isolated platforms and cannot be shared due to privacy concerns or commercial constraints. Federated graph learning (FGL) offers a natural solution for collaborative training under such settings; however, existing studies largely focus on single-modality graphs and do not adequately address the challenges unique to multimodal federated graph learning (MMFGL). To bridge this gap, we present MM-OpenFGL, the first comprehensive benchmark that systematically formalizes the MMFGL paradigm and enables rigorous evaluation. MM-OpenFGL comprises 19 multimodal datasets spanning 7 application domains, 8 simulation strategies capturing modality and topology variations, 6 downstream tasks, and 57 state-of-the-art methods implemented through a modular API. Extensive experiments investigate MMFGL from the perspectives of necessity, effectiveness, robustness, and efficiency, offering valuable insights for future research on MMFGL.

</details>


### [147] [MetaLead: A Comprehensive Human-Curated Leaderboard Dataset for Transparent Reporting of Machine Learning Experiments](https://arxiv.org/abs/2601.22420)
*Roelien C. Timmer,Necva Bölücü,Stephen Wan*

Main category: cs.LG

TL;DR: 提出全人工标注的ML Leaderboard数据集MetaLead，可用于更透明细致的评估。


<details>
  <summary>Details</summary>
Motivation: 传统创建排行榜需大量人力，现有自动生成排行榜的数据集存在仅记录最佳结果、元数据有限等问题。

Method: 创建全人工标注的ML Leaderboard数据集MetaLead，包含所有实验结果、额外元数据，明确区分训练和测试数据集。

Result: 得到MetaLead数据集。

Conclusion: MetaLead因丰富的结构，是跨机器学习研究进行更透明细致评估的强大资源。

Abstract: Leaderboards are crucial in the machine learning (ML) domain for benchmarking and tracking progress. However, creating leaderboards traditionally demands significant manual effort. In recent years, efforts have been made to automate leaderboard generation, but existing datasets for this purpose are limited by capturing only the best results from each paper and limited metadata. We present MetaLead, a fully human-annotated ML Leaderboard dataset that captures all experimental results for result transparency and contains extra metadata, such as the result experimental type: baseline, proposed method, or variation of proposed method for experiment-type guided comparisons, and explicitly separates train and test dataset for cross-domain assessment. This enriched structure makes MetaLead a powerful resource for more transparent and nuanced evaluations across ML research.

</details>


### [148] [CoDCL: Counterfactual Data Augmentation Contrastive Learning for Continuous-Time Dynamic Network Link Prediction](https://arxiv.org/abs/2601.22427)
*Hantong Feng,Yonggang Wu,Duxin Chen,Wenwu Yu*

Main category: cs.LG

TL;DR: 提出动态网络学习框架CoDCL结合反事实数据增强与对比学习解决预测模型适应复杂时间环境问题，可集成到现有时间图模型，实验显示其优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 动态网络快速增长与结构演变使有效预测更具挑战，需让预测模型对新兴结构变化有鲁棒性。

Method: 提出CoDCL框架，结合反事实数据增强与对比学习；设计综合策略生成高质量反事实数据；将CoDCL设计为即插即用通用模块。

Result: 在多个真实数据集上实验表明，CoDCL显著优于动态网络领域现有基线模型。

Conclusion: 证实将反事实数据增强集成到动态表示学习中的关键作用。

Abstract: The rapid growth and continuous structural evolution of dynamic networks make effective predictions increasingly challenging. To enable prediction models to adapt to complex temporal environments, they need to be robust to emerging structural changes. We propose a dynamic network learning framework CoDCL, which combines counterfactual data augmentation with contrastive learning to address this deficiency.Furthermore, we devise a comprehensive strategy to generate high-quality counterfactual data, combining a dynamic treatments design with efficient structural neighborhood exploration to quantify the temporal changes in interaction patterns.Crucially, the entire CoDCL is designed as a plug-and-play universal module that can be seamlessly integrated into various existing temporal graph models without requiring architectural modifications.Extensive experiments on multiple real-world datasets demonstrate that CoDCL significantly gains state-of-the-art baseline models in the field of dynamic networks, confirming the critical role of integrating counterfactual data augmentation into dynamic representation learning.

</details>


### [149] [ReNCE: Learning to Reason by Noise Contrastive Estimation](https://arxiv.org/abs/2601.22432)
*Wenzheng Zhang,Karl Stratos*

Main category: cs.LG

TL;DR: 提出一种显式对比学习方法用于赋予预训练大语言模型推理能力，在数学基准测试上表现有竞争力。


<details>
  <summary>Details</summary>
Motivation: GRPO方法虽能赋予预训练大语言模型推理能力，但相关改进需大量经验且难确定，因此提出新方法。

Method: 将K个结果分为正负集，最大化正结果的似然，可视作大语言模型推理的在线（多标签）噪声对比估计实例。

Result: 在一系列具有挑战性的数学基准测试中，与DAPO和在线DPO等强基线相比表现有竞争力。

Conclusion: 所提出的显式对比学习方法有效，可用于赋予预训练大语言模型推理能力。

Abstract: GRPO is a standard approach to endowing pretrained LLMs with reasoning capabilities. It estimates the advantage of an outcome from a group of $K$ outcomes, and promotes those with positive advantages inside a trust region. Since GRPO discriminates between good and bad outcomes softly, it benefits from additional refinements such as asymmetric clipping and zero-variance data filtering. While effective, these refinements require significant empirical insight and can be challenging to identify. We instead propose an explicit contrastive learning approach. Instead of estimating advantages, we bifurcate $K$ outcomes into positive and negative sets, then maximize the likelihood of positive outcomes. Our approach can be viewed as an online instantiation of (multi-label) noise contrastive estimation for LLM reasoning. We validate our method by demonstrating competitive performance on a suite of challenging math benchmarks against strong baselines such as DAPO and online DPO.

</details>


### [150] [Automating Forecasting Question Generation and Resolution for AI Evaluation](https://arxiv.org/abs/2601.22444)
*Nikos I. Bosse,Peter Mühlbacher,Jack Wildman,Lawrence Phillips,Dan Schwarz*

Main category: cs.LG

TL;DR: 本文提出用LLM驱动的网络研究代理自动大规模生成和解决高质量预测问题的系统，评估显示该系统生成问题的有效率和解决问题的准确率高，还验证更智能的LLM驱动的预测代理表现更好，且能通过问题分解策略提升预测效果。


<details>
  <summary>Details</summary>
Motivation: 以往自动生成和评估预测问题的工作依赖重复数据源，限制了多样性和实用性，因此需要开发新系统。

Method: 使用LLM驱动的网络研究代理自动生成和解决预测问题，生成1499个不同的现实预测问题并在数月后解决。

Result: 系统生成可验证、明确问题的比例约96%，解决问题的准确率约95%；更智能的LLM驱动的预测代理表现更好；采用问题分解策略可显著提升Brier分数。

Conclusion: 所提出的系统能有效自动大规模生成和解决高质量预测问题，可用于直接改进预测。

Abstract: Forecasting future events is highly valuable in decision-making and is a robust measure of general intelligence. As forecasting is probabilistic, developing and evaluating AI forecasters requires generating large numbers of diverse and difficult questions, and accurately resolving them. Previous efforts to automate this laborious work relied on recurring data sources (e.g., weather, stocks), limiting diversity and utility. In this work, we present a system for generating and resolving high-quality forecasting questions automatically and at scale using LLM-powered web research agents. We use this system to generate 1499 diverse, real-world forecasting questions, and to resolve them several months later. We estimate that our system produces verifiable, unambiguous questions approximately 96% of the time, exceeding the rate of Metaculus, a leading human-curated forecasting platform. We also find that our system resolves questions at approximately 95% accuracy. We verify that forecasting agents powered by more intelligent LLMs perform better on these questions (Brier score of 0.134 for Gemini 3 Pro, 0.149 for GPT-5, and 0.179 for Gemini 2.5 Flash). Finally, we demonstrate how our system can be leveraged to directly improve forecasting, by evaluating a question decomposition strategy on a generated question set, yielding a significant improvement in Brier scores (0.132 vs. 0.141).

</details>


### [151] [Beyond Activation Patterns: A Weight-Based Out-of-Context Explanation of Sparse Autoencoder Features](https://arxiv.org/abs/2601.22447)
*Yiting Liu,Zhi-Hong Deng*

Main category: cs.LG

TL;DR: 提出基于权重的解释框架对稀疏自编码器（SAEs）特征进行解读，并通过实验获相关结果，完善特征可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前解释方法基于激活模式推断特征语义，忽略了特征是为重构前向传播中具有计算作用的激活而训练的问题。

Method: 引入基于权重的解释框架，通过直接的权重交互来衡量功能影响，且不需要激活数据。

Result: 在Gemma - 2和Llama - 3.1模型上实验发现，四分之一的特征可直接预测输出令牌；特征以深度依赖结构积极参与注意力机制；语义和非语义特征群体在注意力回路中呈现不同分布。

Conclusion: 该分析完善了SAE特征可解释性中缺失的上下文无关部分。

Abstract: Sparse autoencoders (SAEs) have emerged as a powerful technique for decomposing language model representations into interpretable features. Current interpretation methods infer feature semantics from activation patterns, but overlook that features are trained to reconstruct activations that serve computational roles in the forward pass. We introduce a novel weight-based interpretation framework that measures functional effects through direct weight interactions, requiring no activation data. Through three experiments on Gemma-2 and Llama-3.1 models, we demonstrate that (1) 1/4 of features directly predict output tokens, (2) features actively participate in attention mechanisms with depth-dependent structure, and (3) semantic and non-semantic feature populations exhibit distinct distribution profiles in attention circuits. Our analysis provides the missing out-of-context half of SAE feature interpretability.

</details>


### [152] [HeaPA: Difficulty-Aware Heap Sampling and On-Policy Query Augmentation for LLM Reinforcement Learning](https://arxiv.org/abs/2601.22448)
*Weiqi Wang,Xin Liu,Binxuan Huang,Hejie Cui,Rongzhi Zhang,Changlong Yu,Shuowei Jin,Jingfeng Yang,Qingyu Yin,Zhengyang Wang,Zheng Li,Yifan Gao,Priyanka Nigam,Bing Yin,Lihong Li,Yangqiu Song*

Main category: cs.LG

TL;DR: 提出HeaPA方法优化LLMs推理任务训练效率，在多实验中提升准确率并减少计算量。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR训练LLMs推理任务时，提示池静态或与学习进度关联弱，统一采样效率低，现有方法有局限。

Method: 引入HeaPA，维护有界动态提示池，用基于堆的边界采样跟踪能力边界，通过策略内增强和轻量级异步验证扩展提示池，通过拓扑感知重估计和控制重插入稳定相关查询。

Result: 在两个训练语料库、两种训练方案和七个基准测试中，HeaPA持续提高准确率，用更少计算量达到目标性能，时钟时间相当。

Conclusion: HeaPA的优势源于聚焦边界采样和策略内提示池增长，模型规模越大优势越明显。

Abstract: RLVR is now a standard way to train LLMs on reasoning tasks with verifiable outcomes, but when rollout generation dominates the cost, efficiency depends heavily on which prompts you sample and when. In practice, prompt pools are often static or only loosely tied to the model's learning progress, so uniform sampling can't keep up with the shifting capability frontier and ends up wasting rollouts on prompts that are already solved or still out of reach. Existing approaches improve efficiency through filtering, curricula, adaptive rollout allocation, or teacher guidance, but they typically assume a fixed pool-which makes it hard to support stable on-policy pool growth-or they add extra teacher cost and latency. We introduce HeaPA (Heap Sampling and On-Policy Query Augmentation), which maintains a bounded, evolving pool, tracks the frontier using heap-based boundary sampling, expands the pool via on-policy augmentation with lightweight asynchronous validation, and stabilizes correlated queries through topology-aware re-estimation of pool statistics and controlled reinsertion. Across two training corpora, two training recipes, and seven benchmarks, HeaPA consistently improves accuracy and reaches target performance with fewer computations while keeping wall-clock time comparable. Our analyses suggest these gains come from frontier-focused sampling and on-policy pool growth, with the benefits becoming larger as model scale increases. Our code is available at https://github.com/horizon-rl/HeaPA.

</details>


### [153] [Tuning the Implicit Regularizer of Masked Diffusion Language Models: Enhancing Generalization via Insights from $k$-Parity](https://arxiv.org/abs/2601.22450)
*Jianhao Huang,Baharan Mirzasoleiman*

Main category: cs.LG

TL;DR: 研究Masked Diffusion Language Models在k - parity问题中的泛化特性，理论分析其目标函数，优化掩码概率分布，提升模型困惑度和性能。


<details>
  <summary>Details</summary>
Motivation: Masked Diffusion Language Models的泛化特性研究不足，相比自回归模型关注度低，故基于k - parity问题进行研究。

Method: 理论上分解Masked Diffusion目标函数为信号和噪声机制，在k - parity问题上用MD目标训练nanoGPT，并优化掩码概率分布。

Result: MD目标改变学习格局，避免模型出现grokking现象，能快速泛化。优化方法显著改善50M参数模型困惑度，在8B参数模型上性能提升分别达8.8%和5.8%。

Conclusion: 该框架在大规模掩码扩散语言模型中具有可扩展性和有效性。

Abstract: Masked Diffusion Language Models have recently emerged as a powerful generative paradigm, yet their generalization properties remain understudied compared to their auto-regressive counterparts. In this work, we investigate these properties within the setting of the $k$-parity problem (computing the XOR sum of $k$ relevant bits), where neural networks typically exhibit grokking -- a prolonged plateau of chance-level performance followed by sudden generalization. We theoretically decompose the Masked Diffusion (MD) objective into a Signal regime which drives feature learning, and a Noise regime which serves as an implicit regularizer. By training nanoGPT using MD objective on the $k$-parity problem, we demonstrate that MD objective fundamentally alters the learning landscape, enabling rapid and simultaneous generalization without experiencing grokking. Furthermore, we leverage our theoretical insights to optimize the distribution of the mask probability in the MD objective. Our method significantly improves perplexity for 50M-parameter models and achieves superior results across both pre-training from scratch and supervised fine-tuning. Specifically, we observe performance gains peaking at $8.8\%$ and $5.8\%$, respectively, on 8B-parameter models, confirming the scalability and effectiveness of our framework in large-scale masked diffusion language model regimes.

</details>


### [154] [Temporal Graph Pattern Machine](https://arxiv.org/abs/2601.22454)
*Yijun Ma,Zehong Wang,Weixiang Sun,Yanfang Ye*

Main category: cs.LG

TL;DR: 提出Temporal Graph Pattern Machine (TGPM) 框架学习广义演化模式，在链接预测任务表现好且有跨域迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有时间图学习方法以任务为中心且有诸多限制，阻碍可迁移时间演化机制的发现。

Method: 提出TGPM框架，用时间偏置随机游走合成交互块，用基于Transformer的骨干网络处理，引入自监督预训练任务。

Result: TGPM在直推式和归纳式链接预测中均达到了最先进的性能。

Conclusion: TGPM能有效学习广义演化模式，具有出色的跨域迁移能力。

Abstract: Temporal graph learning is pivotal for deciphering dynamic systems, where the core challenge lies in explicitly modeling the underlying evolving patterns that govern network transformation. However, prevailing methods are predominantly task-centric and rely on restrictive assumptions -- such as short-term dependency modeling, static neighborhood semantics, and retrospective time usage. These constraints hinder the discovery of transferable temporal evolution mechanisms. To address this, we propose the Temporal Graph Pattern Machine (TGPM), a foundation framework that shifts the focus toward directly learning generalized evolving patterns. TGPM conceptualizes each interaction as an interaction patch synthesized via temporally-biased random walks, thereby capturing multi-scale structural semantics and long-range dependencies that extend beyond immediate neighborhoods. These patches are processed by a Transformer-based backbone designed to capture global temporal regularities while adapting to context-specific interaction dynamics. To further empower the model, we introduce a suite of self-supervised pre-training tasks -- specifically masked token modeling and next-time prediction -- to explicitly encode the fundamental laws of network evolution. Extensive experiments show that TGPM consistently achieves state-of-the-art performance in both transductive and inductive link prediction, demonstrating exceptional cross-domain transferability.

</details>


### [155] [Machine Unlearning in Low-Dimensional Feature Subspace](https://arxiv.org/abs/2601.22456)
*Kun Fang,Qinghua Tao,Junxu Liu,Yaxin Xiao,Qingqing Ye,Jian Sun,Haibo Hu*

Main category: cs.LG

TL;DR: 本文提出了基于低维特征子空间的机器遗忘方法LOFT，该方法有效降低了计算开销和隐私泄露风险，且性能优越。


<details>
  <summary>Details</summary>
Motivation: 解决主流机器遗忘方法中大量数据重新加载带来的隐私泄露风险，以及更新整个预训练模型效率低下的问题。

Method: 在低维特征子空间进行遗忘操作，通过主投影优化投影矩阵，仅需从预训练骨干网络中一次性提取特征。

Result: 大量实验验证了LOFT在不同模型、数据集、任务和应用中计算开销显著降低，遗忘性能优越。

Conclusion: LOFT方法能在降低隐私泄露风险和计算开销的同时，实现较好的机器遗忘效果。

Abstract: Machine Unlearning (MU) aims at removing the influence of specific data from a pretrained model while preserving performance on the remaining data. In this work, a novel perspective for MU is presented upon low-dimensional feature subspaces, which gives rise to the potentials of separating the remaining and forgetting data herein. This separability motivates our LOFT, a method that proceeds unlearning in a LOw-dimensional FeaTure subspace from the pretrained model skithrough principal projections, which are optimized to maximally capture the information of the remaining data and meanwhile diminish that of the forgetting data. In training, LOFT simply optimizes a small-size projection matrix flexibly plugged into the pretrained model, and only requires one-shot feature fetching from the pretrained backbone instead of repetitively accessing the raw data. Hence, LOFT mitigates two critical issues in mainstream MU methods, i.e., the privacy leakage risk from massive data reload and the inefficiency of updates to the entire pretrained model. Extensive experiments validate the significantly lower computational overhead and superior unlearning performance of LOFT across diverse models, datasets, tasks, and applications. Code is anonymously available at https://anonymous.4open.science/r/4352/.

</details>


### [156] [EvoEGF-Mol: Evolving Exponential Geodesic Flow for Structure-based Drug Design](https://arxiv.org/abs/2601.22466)
*Yaowei Jin,Junjie Wang,Cheng Cao,Penglei Wang,Duo An,Qian Shi*

Main category: cs.LG

TL;DR: 从信息几何角度解决传统SBDD方法问题，提出EvoEGF - Mol模型，在任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统SBDD方法在欧几里得和概率空间分别构建概率路径，与潜在统计流形不匹配。

Method: 从信息几何角度，将分子建模为复合指数族分布，沿指数测地线定义生成流，提出EvoEGF - Mol模型，采用渐进参数细化架构。

Result: 模型在CrossDock上接近参考级PoseBusters通过率（93.4%），在MolGenBench任务中优于基线。

Conclusion: 所提方法具有显著的几何精度和相互作用保真度，能恢复生物活性支架并生成符合MedChem过滤器的候选物。

Abstract: Structure-Based Drug Design (SBDD) aims to discover bioactive ligands. Conventional approaches construct probability paths separately in Euclidean and probabilistic spaces for continuous atomic coordinates and discrete chemical categories, leading to a mismatch with the underlying statistical manifolds. We address this issue from an information-geometric perspective by modeling molecules as composite exponential-family distributions and defining generative flows along exponential geodesics under the Fisher-Rao metric. To avoid the instantaneous trajectory collapse induced by geodesics directly targeting Dirac distributions, we propose Evolving Exponential Geodesic Flow for SBDD (EvoEGF-Mol), which replaces static Dirac targets with dynamically concentrating distributions, ensuring stable training via a progressive-parameter-refinement architecture. Our model approaches a reference-level PoseBusters passing rate (93.4%) on CrossDock, demonstrating remarkable geometric precision and interaction fidelity, while outperforming baselines on real-world MolGenBench tasks by recovering bioactive scaffolds and generating candidates that meet established MedChem filters.

</details>


### [157] [Unrewarded Exploration in Large Language Models Reveals Latent Learning from Psychology](https://arxiv.org/abs/2601.22474)
*Jian Xiong,Jingbo Zhou,Zihan Zhou,Yixiong Xiao,Le Zhang,Jingyong Ye,Rui Qian,Yang Zhou,Dejing Dou*

Main category: cs.LG

TL;DR: 研究发现大语言模型（LLMs）存在潜在学习动态，两阶段探索训练的LLMs比全程基于奖励强化学习的性能更好，并给出理论分析。


<details>
  <summary>Details</summary>
Motivation: 传统奖励学习有局限性，大语言模型主要依赖以奖励为中心的强化学习范式，心理学中成熟的潜在学习现象能否及如何应用于大语言模型训练尚待探索。

Method: 在多个模型家族和不同任务领域进行广泛实验，采用两阶段探索训练，即先无奖励探索，后引入奖励。

Result: 大语言模型在无奖励探索阶段有适度性能提升，引入奖励后性能进一步增强，两阶段探索训练的LLMs最终能力高于全程奖励强化学习的。

Conclusion: 大语言模型存在潜在学习动态，无奖励探索能带来性能提升，并给出了这种动态的机制解释。

Abstract: Latent learning, classically theorized by Tolman, shows that biological agents (e.g., rats) can acquire internal representations of their environment without rewards, enabling rapid adaptation once rewards are introduced. In contrast, from a cognitive science perspective, reward learning remains overly dependent on external feedback, limiting flexibility and generalization. Although recent advances in the reasoning capabilities of large language models (LLMs), such as OpenAI-o1 and DeepSeek-R1, mark a significant breakthrough, these models still rely primarily on reward-centric reinforcement learning paradigms. Whether and how the well-established phenomenon of latent learning in psychology can inform or emerge within LLMs' training remains largely unexplored. In this work, we present novel findings from our experiments that LLMs also exhibit the latent learning dynamics. During an initial phase of unrewarded exploration, LLMs display modest performance improvements, as this phase allows LLMs to organize task-relevant knowledge without being constrained by reward-driven biases, and performance is further enhanced once rewards are introduced. LLMs post-trained under this two-stage exploration regime ultimately achieve higher competence than those post-trained with reward-based reinforcement learning throughout. Beyond these empirical observations, we also provide theoretical analyses for our experiments explaining why unrewarded exploration yields performance gains, offering a mechanistic account of these dynamics. Specifically, we conducted extensive experiments across multiple model families and diverse task domains to establish the existence of the latent learning dynamics in LLMs.

</details>


### [158] [Continual Policy Distillation from Distributed Reinforcement Learning Teachers](https://arxiv.org/abs/2601.22475)
*Yuxuan Li,Qijun He,Mingqi Yuan,Wen-Tse Chen,Jeff Schneider,Jiayu Chen*

Main category: cs.LG

TL;DR: 提出教师 - 学生框架解决持续强化学习问题，在Meta - World基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 持续强化学习需解决稳定性 - 可塑性困境及利用先验经验泛化到新任务，直接应用强化学习到顺序任务流实现可扩展性能有挑战。

Method: 提出教师 - 学生框架，将持续强化学习解耦为训练单任务教师模型和将其蒸馏到中心通用模型两个独立过程，采用专家混合架构和基于重放的方法增强蒸馏过程的可塑性和稳定性。

Result: 在Meta - World基准测试中，框架能实现高效持续强化学习，恢复超85%教师性能，任务遗忘率控制在10%以内。

Conclusion: 所提教师 - 学生框架能有效解决持续强化学习问题。

Abstract: Continual Reinforcement Learning (CRL) aims to develop lifelong learning agents to continuously acquire knowledge across diverse tasks while mitigating catastrophic forgetting. This requires efficiently managing the stability-plasticity dilemma and leveraging prior experience to rapidly generalize to novel tasks. While various enhancement strategies for both aspects have been proposed, achieving scalable performance by directly applying RL to sequential task streams remains challenging. In this paper, we propose a novel teacher-student framework that decouples CRL into two independent processes: training single-task teacher models through distributed RL and continually distilling them into a central generalist model. This design is motivated by the observation that RL excels at solving single tasks, while policy distillation -- a relatively stable supervised learning process -- is well aligned with large foundation models and multi-task learning. Moreover, a mixture-of-experts (MoE) architecture and a replay-based approach are employed to enhance the plasticity and stability of the continual policy distillation process. Extensive experiments on the Meta-World benchmark demonstrate that our framework enables efficient continual RL, recovering over 85% of teacher performance while constraining task-wise forgetting to within 10%.

</details>


### [159] [Transform-Augmented GRPO Improves Pass@k](https://arxiv.org/abs/2601.22478)
*Khiem Le,Youssef Mroueh,Phuc Nguyen,Chi-Heng Lin,Shangqian Gao,Ting Hua,Nitesh V. Chawla*

Main category: cs.LG

TL;DR: 现有大语言模型在推理上有问题，GRPO 会使情况恶化，提出 TA - GRPO 方法改进，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有通过下一个词预测训练的大语言模型是模式匹配器，对表述变化敏感，GRPO 会恶化推理情况，需要改进。

Method: 提出 TA - GRPO 方法，生成语义等价的转换变体问题，通过跨组汇集奖励计算优势。

Result: 实验表明在数学推理基准测试中 Pass@k 有持续提升，竞赛数学最高提升 9.84 分，分布外科学推理最高提升 5.05 分。

Conclusion: TA - GRPO 可降低零梯度概率，通过减少训练 - 测试分布偏移提高泛化能力。

Abstract: Large language models trained via next-token prediction are fundamentally pattern-matchers: sensitive to superficial phrasing variations even when the underlying problem is identical. Group Relative Policy Optimization (GRPO) was designed to improve reasoning, but in fact it worsens this situation through two failure modes: diversity collapse, where training amplifies a single solution strategy while ignoring alternatives of gradient signal, and gradient diminishing, where a large portion of questions yield zero gradients because all rollouts receive identical rewards. We propose TA-GRPO (Transform-Augmented GRPO), which generates semantically equivalent transformed variants of each question (via paraphrasing, variable renaming, and format changes) and computes advantages by pooling rewards across the entire group. This pooled computation ensures mixed rewards even when the original question is too easy or too hard, while training on diverse phrasings promotes multiple solution strategies. We provide theoretical justification showing that TA-GRPO reduces zero-gradient probability and improves generalization via reduced train-test distribution shift. Experiments on mathematical reasoning benchmarks show consistent Pass@k improvements, with gains up to 9.84 points on competition math (AMC12, AIME24) and 5.05 points on out-of-distribution scientific reasoning (GPQA-Diamond).

</details>


### [160] [Mitigating Cognitive Inertia in Large Reasoning Models via Latent Spike Steering](https://arxiv.org/abs/2601.22484)
*Seojin Lee,ByeongJeong Kim,Hwanhee Lee*

Main category: cs.LG

TL;DR: 本文针对大推理模型的认知惯性问题，提出无训练框架STARS，通过监测潜在动态纠正问题，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 大推理模型存在认知惯性问题，现有检测方法难以捕捉模型未表达的内部冲突。

Method: 提出STARS框架，通过检测隐藏状态的L2距离尖峰识别推理转变的关键节点，利用几何轨迹分析诊断转变的结构性质，并注入状态感知语言提示来实时引导模型。

Result: 实验表明STARS能有效减少冗余循环，通过自适应纠正错误轨迹提高准确率。

Conclusion: STARS提供了一种无需额外微调的强大无监督机制来优化大推理模型的推理过程。

Abstract: While Large Reasoning Models (LRMs) have achieved remarkable performance by scaling test-time compute, they frequently suffer from Cognitive Inertia, a failure pattern manifesting as either overthinking (inertia of motion) or reasoning rigidity (inertia of direction). Existing detection methods, typically relying on superficial textual heuristics like self-correction tokens, often fail to capture the model's unvoiced internal conflicts. To address this, we propose STARS (Spike-Triggered Adaptive Reasoning Steering), a training-free framework designed to rectify cognitive inertia by monitoring latent dynamics. STARS identifies Cognitive Pivots-critical moments of reasoning transition-by detecting distinct L2 distance spikes in the hidden states. Upon detection, the framework employs geometric trajectory analysis to diagnose the structural nature of the transition and injects state-aware language cues to steer the model in real-time. Our experiments across diverse benchmarks confirm that STARS efficiently curtails redundant loops while improving accuracy through the adaptive correction of erroneous trajectories. STARS offers a robust, unsupervised mechanism to optimize the reasoning process of LRMs without requiring additional fine-tuning.

</details>


### [161] [Elastic Spectral State Space Models for Budgeted Inference](https://arxiv.org/abs/2601.22488)
*Dachuan Song,Xuan Wang*

Main category: cs.LG

TL;DR: 提出ES - SSM，一次全量训练后可直接截断用于不同预算的运行时推理，在多领域测试表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型训练计算能力固定，应用需适配不同资源平台，当前方法需额外训练且难以实现运行时细粒度调整。

Method: 基于状态空间模型的Hankel谱过滤，结合随机谱预算下训练的轻量级输入自适应门，采用共享掩码归一化规则。

Result: 单个ES - SSM模型截断后在类似参数规模下与现代Transformer和SSM基线有竞争力，在不同运行预算下有平滑稳定的预算 - 性能曲线。

Conclusion: ES - SSM可在一次全量训练后灵活截断用于不同资源预算的运行时推理。

Abstract: Foundation models are typically trained at a fixed computational capacity, while real-world applications require deployment across platforms with different resource constraints. Current approaches usually rely on training families of model variants or model distillation, which requires additional training and supports only a pre-selected set of sizes rather than fine-grained adaptation at runtime. In this paper, we propose Elastic Spectral State Space Models (ES-SSM), which require only one-time training at full capacity, but can be directly truncated into arbitrary scales for budgeted, runtime inference without retraining. Our ES-SSM builds on Hankel spectral filtering over a state space model (SSM), coupled with a lightweight input-adaptive gate trained under randomized spectral budgets. Using a shared masked normalization rule over the ordered spectral channels, we encourage predictive capability to concentrate in low-index components, while higher-index components act primarily as refinement. We test our algorithm across long-sequence benchmarks spanning text, logic, retrieval, vision, and audio. We demonstrate that a single ES-SSM model trained once can be truncated to provide competitive performance compared with modern Transformer and SSM baselines at similar parameter scales. Furthermore, by testing under various runtime budgets, we observe smooth and stable budget-performance curves over a wide range of truncation levels.

</details>


### [162] [Gradual Fine-Tuning for Flow Matching Models](https://arxiv.org/abs/2601.22495)
*Gudrun Thorkelsdottir,Arindam Banerjee*

Main category: cs.LG

TL;DR: 提出逐渐微调（GFT）框架用于微调基于流的生成模型，理论证明收敛性，实证显示提高收敛稳定性、缩短概率路径，是应对分布转移的有效方法。


<details>
  <summary>Details</summary>
Motivation: 在数据有限、分布变化或效率要求高的场景，无约束微调会削弱预训练的优势，现有基于奖励的微调方法有局限性。

Method: 提出GFT框架，为随机流定义温度控制的中间目标序列，在预训练和目标漂移之间平滑插值。

Result: GFT提高收敛稳定性、缩短概率路径，推理速度更快，生成质量与标准微调相当。

Conclusion: GFT是在分布转移下可扩展适配流匹配模型的理论可靠且实践有效的替代方案。

Abstract: Fine-tuning flow matching models is a central challenge in settings with limited data, evolving distributions, or strict efficiency demands, where unconstrained fine-tuning can erode the accuracy and efficiency gains learned during pretraining. Prior work has produced theoretical guarantees and empirical advances for reward-based fine-tuning formulations, but these methods often impose restrictions on permissible drift structure or training techniques. In this work, we propose Gradual Fine-Tuning (GFT), a principled framework for fine-tuning flow-based generative models when samples from the target distribution are available. For stochastic flows, GFT defines a temperature-controlled sequence of intermediate objectives that smoothly interpolate between the pretrained and target drifts, approaching the true target as the temperature approaches zero. We prove convergence results for both marginal and conditional GFT objectives, enabling the use of suitable (e.g., optimal transport) couplings during GFT while preserving correctness. Empirically, GFT improves convergence stability and shortens probability paths, resulting in faster inference, while maintaining generation quality comparable to standard fine-tuning. Our results position GFT as a theoretically grounded and practically effective alternative for scalable adaptation of flow matching models under distribution shift.

</details>


### [163] [Action-Sufficient Goal Representations](https://arxiv.org/abs/2601.22496)
*Jinu Hyeon,Woobin Park,Hongjoon Ahn,Taesup Moon*

Main category: cs.LG

TL;DR: 文章指出离线目标条件强化学习中现有目标表示假设的不足，引入信息论框架定义动作充分性，证明其与控制成功更相关，且基于演员的表示优于通过价值估计学习的表示。


<details>
  <summary>Details</summary>
Motivation: 现有离线目标条件强化学习中目标表示假设在动作学习时可能失效，需解决该问题。

Method: 引入信息论框架定义动作充分性，证明价值充分性不意味着动作充分性，通过实验验证动作充分性与控制成功的关联，以及标准对数损失训练可诱导动作充分表示。

Result: 在离散环境中验证动作充分性与控制成功更相关，基于演员的表示在流行基准测试中始终优于通过价值估计学习的表示。

Conclusion: 基于演员的目标表示方法在离线目标条件强化学习中更有效。

Abstract: Hierarchical policies in offline goal-conditioned reinforcement learning (GCRL) addresses long-horizon tasks by decomposing control into high-level subgoal planning and low-level action execution. A critical design choice in such architectures is the goal representation-the compressed encoding of goals that serves as the interface between these levels. Existing approaches commonly derive goal representations while learning value functions, implicitly assuming that preserving information sufficient for value estimation is adequate for optimal control. We show that this assumption can fail, even when the value estimation is exact, as such representations may collapse goal states that need to be differentiated for action learning. To address this, we introduce an information-theoretic framework that defines action sufficiency, a condition on goal representations necessary for optimal action selection. We prove that value sufficiency does not imply action sufficiency and empirically verify that the latter is more strongly associated with control success in a discrete environment. We further demonstrate that standard log-loss training of low-level policies naturally induces action-sufficient representations. Our experimental results a popular benchmark demonstrate that our actor-derived representations consistently outperform representations learned via value estimation.

</details>


### [164] [Keep Rehearsing and Refining: Lifelong Learning Vehicle Routing under Continually Drifting Tasks](https://arxiv.org/abs/2601.22509)
*Jiyuan Pei,Yi Mei,Jialin Liu,Mengjie Zhang,Xin Yao*

Main category: cs.LG

TL;DR: 研究在任务持续漂移下神经VRP求解器的终身学习范式，提出DREE框架，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有神经VRP求解器训练方式忽略问题模式随时间持续漂移、单任务训练资源有限的现实情况。

Method: 提出Dual Replay with Experience Enhancement (DREE) 通用框架。

Result: DREE能有效学习新任务、保留先验知识、提高对未见任务的泛化能力，可应用于多种现有神经求解器。

Conclusion: DREE框架能在任务持续漂移场景下提高学习效率，减轻灾难性遗忘。

Abstract: Existing neural solvers for vehicle routing problems (VRPs) are typically trained either in a one-off manner on a fixed set of pre-defined tasks or in a lifelong manner on several tasks arriving sequentially, assuming sufficient training on each task. Both settings overlook a common real-world property: problem patterns may drift continually over time, yielding massive tasks sequentially arising while offering only limited training resources per task. In this paper, we study a novel lifelong learning paradigm for neural VRP solvers under continually drifting tasks over learning time steps, where sufficient training for any given task at any time is not available. We propose Dual Replay with Experience Enhancement (DREE), a general framework to improve learning efficiency and mitigate catastrophic forgetting under such drift. Extensive experiments show that, under such continual drift, DREE effectively learns new tasks, preserves prior knowledge, improves generalization to unseen tasks, and can be applied to diverse existing neural solvers.

</details>


### [165] [Shattered Compositionality: Counterintuitive Learning Dynamics of Transformers for Arithmetic](https://arxiv.org/abs/2601.22510)
*Xingyu Zhao,Darsh Sharma,Rheeya Uppaal,Yiqiao Zhong*

Main category: cs.LG

TL;DR: 研究通过合成算术任务训练transformers，发现其学习技能组合的方式与人类不同，存在“破碎组合性”问题，且该问题在现代大语言模型中持续存在，影响推理可靠性等。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型学习技能组合的动态机制以及非人类行为的根本原因。

Method: 在合成算术任务上训练transformers，进行广泛消融实验和使用细粒度诊断指标。

Result: transformers不按人类顺序构建技能组合，常逆向或并行获取技能，导致分布偏移时出现混合错误，即“破碎组合性”，且该问题在现代大语言模型中存在，模型缩放和基于草稿纸的推理无法缓解。

Conclusion: 模型学习行为与期望的技能组合存在根本不匹配，对推理可靠性、分布外鲁棒性和对齐性有影响。

Abstract: Large language models (LLMs) often exhibit unexpected errors or unintended behavior, even at scale. While recent work reveals the discrepancy between LLMs and humans in skill compositions, the learning dynamics of skill compositions and the underlying cause of non-human behavior remain elusive. In this study, we investigate the mechanism of learning dynamics by training transformers on synthetic arithmetic tasks. Through extensive ablations and fine-grained diagnostic metrics, we discover that transformers do not reliably build skill compositions according to human-like sequential rules. Instead, they often acquire skills in reverse order or in parallel, which leads to unexpected mixing errors especially under distribution shifts--a phenomenon we refer to as shattered compositionality. To explain these behaviors, we provide evidence that correlational matching to the training data, rather than causal or procedural composition, shapes learning dynamics. We further show that shattered compositionality persists in modern LLMs and is not mitigated by pure model scaling or scratchpad-based reasoning. Our results reveal a fundamental mismatch between a model's learning behavior and desired skill compositions, with implications for reasoning reliability, out-of-distribution robustness, and alignment.

</details>


### [166] [DRL-Enabled Trajectory Planing for UAV-Assisted VLC: Optimal Altitude and Reward Design](https://arxiv.org/abs/2601.22512)
*Tian-Tian Lin,Yi Liu,Xiao-Wei Tang,Yunmei Shi,Yi Huang,Zhongxiang Wei,Qingqing Wu,Yuhan Dong*

Main category: cs.LG

TL;DR: 本文研究无人机辅助可见光通信系统中三维轨迹规划，提出框架最小化飞行距离，仿真显示可减少飞行距离和收敛步数。


<details>
  <summary>Details</summary>
Motivation: 开发能最小化无人机飞行距离、最大化数据收集效率的轨迹规划框架，解决无人机辅助可见光通信系统的轨迹规划问题。

Method: 先推导特定可见光通信信道增益阈值下的闭式最优飞行高度，再用新的信息素驱动奖励机制与双延迟深度确定性策略梯度算法优化无人机水平轨迹。

Result: 推导的最优高度比基线方法最多减少35%飞行距离，提出的奖励机制使收敛步数缩短约50%。

Conclusion: 所提方法在无人机辅助可见光通信数据收集中有显著效率提升。

Abstract: Recently, the integration of unmanned aerial vehicle (UAV) and visible light communication (VLC) technologies has emerged as a promising solution to offer flexible communication and efficient lighting. This letter investigates the three-dimensional trajectory planning in a UAV-assisted VLC system, where a UAV is dispatched to collect data from ground users (GUs). The core objective is to develop a trajectory planning framework that minimizes UAV flight distance, which is equivalent to maximizing the data collection efficiency. This issue is formulated as a challenging mixed-integer non-convex optimization problem. To tackle it, we first derive a closed-form optimal flight altitude under specific VLC channel gain threshold. Subsequently, we optimize the UAV horizontal trajectory by integrating a novel pheromone-driven reward mechanism with the twin delayed deep deterministic policy gradient algorithm, which enables adaptive UAV motion strategy in complex environments. Simulation results validate that the derived optimal altitude effectively reduces the flight distance by up to 35% compared to baseline methods. Additionally, the proposed reward mechanism significantly shortens the convergence steps by approximately 50%, demonstrating notable efficiency gains in the context of UAV-assisted VLC data collection.

</details>


### [167] [SCOPE-PD: Explainable AI on Subjective and Clinical Objective Measurements of Parkinson's Disease for Precision Decision-Making](https://arxiv.org/abs/2601.22516)
*Md Mezbahul Islam,John Michael Templeton,Masrur Sobhan,Christian Poellabauer,Ananda Mohan Mondal*

Main category: cs.LG

TL;DR: 本文提出SCOPE - PD框架，整合主客观评估进行帕金森病预测，采用随机森林算法获98.66%准确率，识别出关键特征。


<details>
  <summary>Details</summary>
Motivation: 帕金森病早期预测因传统诊断方法主观性强而困难，现有机器学习方法多依赖主观报告且缺乏可解释性，需新方法提供个性化健康决策。

Method: 从PPMI研究收集主客观临床评估数据构建多模态预测框架，应用多种机器学习技术并选最佳模型，用基于SHAP的分析检查模型可解释性。

Result: 随机森林算法使用主客观测试数据组合特征达到98.66%的最高准确率，识别出震颤、运动迟缓、面部表情是MDS - UPDRS测试中预测PD的前三大贡献特征。

Conclusion: SCOPE - PD框架能有效整合主客观数据进行帕金森病预测，具有较高的准确率和可解释性。

Abstract: Parkinson's disease (PD) is a chronic and complex neurodegenerative disorder influenced by genetic, clinical, and lifestyle factors. Predicting this disease early is challenging because it depends on traditional diagnostic methods that face issues of subjectivity, which commonly delay diagnosis. Several objective analyses are currently in practice to help overcome the challenges of subjectivity; however, a proper explanation of these analyses is still lacking. While machine learning (ML) has demonstrated potential in supporting PD diagnosis, existing approaches often rely on subjective reports only and lack interpretability for individualized risk estimation. This study proposes SCOPE-PD, an explainable AI-based prediction framework, by integrating subjective and objective assessments to provide personalized health decisions. Subjective and objective clinical assessment data are collected from the Parkinson's Progression Markers Initiative (PPMI) study to construct a multimodal prediction framework. Several ML techniques are applied to these data, and the best ML model is selected to interpret the results. Model interpretability is examined using SHAP-based analysis. The Random Forest algorithm achieves the highest accuracy of 98.66 percent using combined features from both subjective and objective test data. Tremor, bradykinesia, and facial expression are identified as the top three contributing features from the MDS-UPDRS test in the prediction of PD.

</details>


### [168] [Variational Bayesian Flow Network for Graph Generation](https://arxiv.org/abs/2601.22524)
*Yida Xiong,Jiameng Chen,Xiuwen Gong,Jia Wu,Shirui Pan,Wenbin Hu*

Main category: cs.LG

TL;DR: 提出变分贝叶斯流网络（VBFN）用于图生成，在合成和分子图数据集上提升了保真度和多样性，超越基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有图生成的扩散模型和流匹配方法在编码节点 - 边耦合方面存在问题，经典贝叶斯流网络（BFNs）限制几何证据融合。

Method: 提出VBFN，对易处理的联合高斯变分信念族进行变分提升，由结构化精度控制，每次贝叶斯更新简化为求解对称正定线性系统，从表示诱导的依赖图构建与样本无关的稀疏精度。

Result: 在合成和分子图数据集上，VBFN提高了保真度和多样性，超越了基线方法。

Conclusion: VBFN在图生成任务中是一种有效的方法，能更好地处理节点 - 边耦合问题。

Abstract: Graph generation aims to sample discrete node and edge attributes while satisfying coupled structural constraints. Diffusion models for graphs often adopt largely factorized forward-noising, and many flow-matching methods start from factorized reference noise and coordinate-wise interpolation, so node-edge coupling is not encoded by the generative geometry and must be recovered implicitly by the core network, which can be brittle after discrete decoding. Bayesian Flow Networks (BFNs) evolve distribution parameters and naturally support discrete generation. But classical BFNs typically rely on factorized beliefs and independent channels, which limit geometric evidence fusion. We propose Variational Bayesian Flow Network (VBFN), which performs a variational lifting to a tractable joint Gaussian variational belief family governed by structured precisions. Each Bayesian update reduces to solving a symmetric positive definite linear system, enabling coupled node and edge updates within a single fusion step. We construct sample-agnostic sparse precisions from a representation-induced dependency graph, thereby avoiding label leakage while enforcing node-edge consistency. On synthetic and molecular graph datasets, VBFN improves fidelity and diversity, and surpasses baseline methods.

</details>


### [169] [Learn from A Rationalist: Distilling Intermediate Interpretable Rationales](https://arxiv.org/abs/2601.22531)
*Jiayi Dai,Randy Goebel*

Main category: cs.LG

TL;DR: 提出REKD方法提升基于较小神经网络的RE模型预测性能，实验表明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 现有基于较弱或较小神经网络的RE模型在学习特征子集选择时计算挑战大，需提升其预测性能。

Method: 提出REKD方法，让学生RE模型除自身优化外，还从教师的基本原理和预测中学习，且该方法与神经网络模型无关，可集成任意黑盒神经网络。

Result: 在语言和视觉分类数据集上的实验表明，REKD显著提高了学生RE模型的预测性能。

Conclusion: REKD方法可行且能有效提升学生RE模型的预测性能。

Abstract: Because of the pervasive use of deep neural networks (DNNs), especially in high-stakes domains, the interpretability of DNNs has received increased attention. The general idea of rationale extraction (RE) is to provide an interpretable-by-design framework for DNNs via a select-predict architecture where two neural networks learn jointly to perform feature selection and prediction, respectively. Given only the remote supervision from the final task prediction, the process of learning to select subsets of features (or \emph{rationales}) requires searching in the space of all possible feature combinations, which is computationally challenging and even harder when the base neural networks are not sufficiently capable. To improve the predictive performance of RE models that are based on less capable or smaller neural networks (i.e., the students), we propose \textbf{REKD} (\textbf{R}ationale \textbf{E}xtraction with \textbf{K}nowledge \textbf{D}istillation) where a student RE model learns from the rationales and predictions of a teacher (i.e., a \emph{rationalist}) in addition to the student's own RE optimization. This structural adjustment to RE aligns well with how humans could learn effectively from interpretable and verifiable knowledge. Because of the neural-model agnostic nature of the method, any black-box neural network could be integrated as a backbone model. To demonstrate the viability of REKD, we conduct experiments with multiple variants of BERT and vision transformer (ViT) models. Our experiments across language and vision classification datasets (i.e., IMDB movie reviews, CIFAR 10 and CIFAR 100) show that REKD significantly improves the predictive performance of the student RE models.

</details>


### [170] [Demystifying Design Choices of Reinforcement Fine-tuning: A Batched Contextual Bandit Learning Perspective](https://arxiv.org/abs/2601.22532)
*Hong Xie,Xiao Hu,Tao Tan,Haoran Gu,Xin Li,Jianyu Han,Defu Lian,Enhong Chen*

Main category: cs.LG

TL;DR: 本文旨在解答强化微调中设计选择的两个基本问题，构建极简基线并设计实验流程，通过实验揭示设计选择的作用并确定关键因素。


<details>
  <summary>Details</summary>
Motivation: 强化微调领域虽有性能提升宣称，但结论不一致，缺乏对设计选择作用及关键因素的原则性解答。

Method: 构建极简基线以解开因素纠缠，该基线与批量上下文多臂老虎机学习相关，围绕此基线设计实验流程，检验各因素边际收益。

Result: 在三个基础模型和两个数据集上的实验，揭示了设计选择对学习和泛化动态的新理解，确定了关键因素。

Conclusion: 通过实验分析，明确了强化微调中各设计选择的作用和关键因素，为该领域研究提供了方向。

Abstract: The reinforcement fine-tuning area is undergoing an explosion papers largely on optimizing design choices. Though performance gains are often claimed, inconsistent conclusions also arise from time to time, making the progress illusive. Reflecting on this illusion, we still lack principled answers to two fundamental questions: 1) what is the role of each design choice? 2) which ones are critical? This paper aims to shed light on them. The underlying challenge is that design choices are entangled together, making their contribution to learning and generalization difficult to attribute. To address this challenge, we first construct a minimalist baseline for disentangling factors: one rollout per query in each round, the outcome reward serving as the training signal without any advantage trick, and a batch size of thirty-two. This baseline connects to batched contextual bandit learning, which facilitates experimental analysis. Centering around this baseline, we design an experiment pipeline, examining the marginal gains of factors like advantage, number of rollouts, etc. Experiments on three base models and two datasets, not only reveal new understanding on the role of various design choices on learning and generalization dynamics, but also identify critical ones that deserve more effort.

</details>


### [171] [Learning to Defer in Non-Stationary Time Series via Switching State-Space Models](https://arxiv.org/abs/2601.22538)
*Yannis Montreuil,Letian Yu,Axel Carlier,Lai Xing Ng,Wei Tsang Ooi*

Main category: cs.LG

TL;DR: 研究非平稳时间序列在部分反馈和专家可用性随时间变化下的学习延迟问题，提出L2D - SLDS模型和路由规则，实验有改进。


<details>
  <summary>Details</summary>
Motivation: 解决非平稳时间序列在部分反馈和时间 - 变化专家可用性下的学习延迟问题。

Method: 使用L2D - SLDS模型建模专家残差，该模型有上下文相关的状态转换、共享全局因子和专家特有状态，通过动态注册表支持专家加入和筛选；提出受IDS启发的路由规则。

Result: 实验显示相比上下文多臂老虎机基线和无共享因子的消融实验有改进。

Conclusion: 所提模型和路由规则在处理该类非平稳时间序列问题上有效。

Abstract: We study Learning to Defer for non-stationary time series with partial feedback and time-varying expert availability. At each time step, the router selects an available expert, observes the target, and sees only the queried expert's prediction. We model signed expert residuals using L2D-SLDS, a factorized switching linear-Gaussian state-space model with context-dependent regime transitions, a shared global factor enabling cross-expert information transfer, and per-expert idiosyncratic states. The model supports expert entry and pruning via a dynamic registry. Using one-step-ahead predictive beliefs, we propose an IDS-inspired routing rule that trades off predicted cost against information gained about the latent regime and shared factor. Experiments show improvements over contextual-bandit baselines and a no-shared-factor ablation.

</details>


### [172] [Benchmarking Long Roll-outs of Auto-regressive Neural Operators for the Compressible Navier-Stokes Equations with Conserved Quantity Correction](https://arxiv.org/abs/2601.22541)
*Sean Current,Chandan Kumar,Datta Gaitonde,Srinivasan Parthasarathy*

Main category: cs.LG

TL;DR: 提出守恒量校正技术提升深度学习模型长期稳定性，分析神经算子性能并指出未来架构需关注高频分量。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习求解PDE在长预测时长中因自回归误差积累和无法守恒物理量而表现不佳的问题。

Method: 提出守恒量校正这一与模型无关的技术，将物理守恒标准纳入深度学习模型。

Result: 该技术能持续提升自回归神经算子模型的长期稳定性，且与模型架构无关；从谱域分析指出当前架构存在显著局限。

Conclusion: 未来工作应考虑重视高频分量的架构，以理解和建模湍流。

Abstract: Deep learning has been proposed as an efficient alternative for the numerical approximation of PDE solutions, offering fast, iterative simulation of PDEs through the approximation of solution operators. However, deep learning solutions have struggle to perform well over long prediction durations due to the accumulation of auto-regressive error, which is compounded by the inability of models to conserve physical quantities. In this work, we present conserved quantity correction, a model-agnostic technique for incorporation physical conservation criteria within deep learning models. Our results demonstrate consistent improvement in the long-term stability of auto-regressive neural operator models, regardless of the model architecture. Furthermore, we analyze the performance of neural operators from the spectral domain, highlighting significant limitations of present architectures. These results highlight the need for future work to consider architectures that place specific emphasis on high frequency components, which are integral to the understanding and modeling of turbulent flows.

</details>


### [173] [EUGens: Efficient, Unified, and General Dense Layers](https://arxiv.org/abs/2601.22563)
*Sang Min Kim,Byeongchan Kim,Arijit Sehanobish,Somnath Basu Roy Chowdhury,Rahul Kidambi,Dongseok Shim,Avinava Dubey,Snigdha Chaturvedi,Min-hwan Oh,Krzysztof Choromanski*

Main category: cs.LG

TL;DR: 提出EUGens层解决全连接前馈层瓶颈，提升效率，结合知识迁移技术，实验表明能提升推理速度和内存效率。


<details>
  <summary>Details</summary>
Motivation: 全连接前馈层在神经网络架构中带来计算和参数数量瓶颈，需要高效的神经网络用于实时应用和资源受限环境。

Method: 提出EUGens层，利用随机特征近似标准FFL，结合输入范数；提出层知识迁移技术绕过反向传播。

Result: EUGens将推理复杂度从二次降低到线性，实现无偏算法；集成到Transformer和MLP中，推理速度最高提升27%，内存效率最高提升30%。

Conclusion: EUGens在现实场景中大规模神经网络可扩展部署方面有潜力。

Abstract: Efficient neural networks are essential for scaling machine learning models to real-time applications and resource-constrained environments. Fully-connected feedforward layers (FFLs) introduce computation and parameter count bottlenecks within neural network architectures. To address this challenge, in this work, we propose a new class of dense layers that generalize standard fully-connected feedforward layers, \textbf{E}fficient, \textbf{U}nified and \textbf{Gen}eral dense layers (EUGens). EUGens leverage random features to approximate standard FFLs and go beyond them by incorporating a direct dependence on the input norms in their computations. The proposed layers unify existing efficient FFL extensions and improve efficiency by reducing inference complexity from quadratic to linear time. They also lead to \textbf{the first} unbiased algorithms approximating FFLs with arbitrary polynomial activation functions. Furthermore, EuGens reduce the parameter count and computational overhead while preserving the expressive power and adaptability of FFLs. We also present a layer-wise knowledge transfer technique that bypasses backpropagation, enabling efficient adaptation of EUGens to pre-trained models. Empirically, we observe that integrating EUGens into Transformers and MLPs yields substantial improvements in inference speed (up to \textbf{27}\%) and memory efficiency (up to \textbf{30}\%) across a range of tasks, including image classification, language model pre-training, and 3D scene reconstruction. Overall, our results highlight the potential of EUGens for the scalable deployment of large-scale neural networks in real-world scenarios.

</details>


### [174] [FedDis: A Causal Disentanglement Framework for Federated Traffic Prediction](https://arxiv.org/abs/2601.22578)
*Chengyang Zhou,Zijian Zhang,Chunxu Zhang,Hao Miao,Yulin Zhang,Kedi Lyu,Juncheng Hu*

Main category: cs.LG

TL;DR: 现有联邦方法在处理非IID交通数据时面临挑战，本文提出FedDis框架，通过双分支设计和因果解纠缠实现交通预测，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 联邦学习进行交通预测时，现有方法难以应对非IID交通数据的异质性问题。

Method: 提出FedDis框架，采用双分支设计，用个性化库学习客户端特定因素，全局模式库提取公共知识，并通过最小化互信息目标确保两个分支的信息正交。

Result: 在四个真实世界的基准数据集上的综合实验表明，FedDis始终能达到最先进的性能，具有高效性和卓越的扩展性。

Conclusion: FedDis框架能有效解决联邦学习中交通数据的异质性问题，实现鲁棒的跨客户端知识转移。

Abstract: Federated learning offers a promising paradigm for privacy-preserving traffic prediction, yet its performance is often challenged by the non-identically and independently distributed (non-IID) nature of decentralized traffic data. Existing federated methods frequently struggle with this data heterogeneity, typically entangling globally shared patterns with client-specific local dynamics within a single representation. In this work, we postulate that this heterogeneity stems from the entanglement of two distinct generative sources: client-specific localized dynamics and cross-client global spatial-temporal patterns. Motivated by this perspective, we introduce FedDis, a novel framework that, to the best of our knowledge, is the first to leverage causal disentanglement for federated spatial-temporal prediction. Architecturally, FedDis comprises a dual-branch design wherein a Personalized Bank learns to capture client-specific factors, while a Global Pattern Bank distills common knowledge. This separation enables robust cross-client knowledge transfer while preserving high adaptability to unique local environments. Crucially, a mutual information minimization objective is employed to enforce informational orthogonality between the two branches, thereby ensuring effective disentanglement. Comprehensive experiments conducted on four real-world benchmark datasets demonstrate that FedDis consistently achieves state-of-the-art performance, promising efficiency, and superior expandability.

</details>


### [175] [Non-Intrusive Graph-Based Bot Detection for E-Commerce Using Inductive Graph Neural Networks](https://arxiv.org/abs/2601.22579)
*Sichen Zhao,Zhiming Xue,Yalun Qi,Xianling Zeng,Zihan Yu*

Main category: cs.LG

TL;DR: 提出用于电商的非侵入式基于图的恶意机器人检测框架，性能优且适合实际部署


<details>
  <summary>Details</summary>
Motivation: 传统电商恶意机器人缓解技术无效或具侵入性，现代机器人有逃避策略

Method: 用图表示用户会话行为，应用归纳图神经网络分类

Result: 归纳图模型在AUC和F1分数上优于多层感知机基线，在对抗扰动和冷启动模拟中表现稳健

Conclusion: 该框架适合实际电商安全部署

Abstract: Malicious bots pose a growing threat to e-commerce platforms by scraping data, hoarding inventory, and perpetrating fraud. Traditional bot mitigation techniques, including IP blacklists and CAPTCHA-based challenges, are increasingly ineffective or intrusive, as modern bots leverage proxies, botnets, and AI-assisted evasion strategies. This work proposes a non-intrusive graph-based bot detection framework for e-commerce that models user session behavior through a graph representation and applies an inductive graph neural network for classification. The approach captures both relational structure and behavioral semantics, enabling accurate identification of subtle automated activity that evades feature-based methods. Experiments on real-world e-commerce traffic demonstrate that the proposed inductive graph model outperforms a strong session-level multilayer perceptron baseline in terms of AUC and F1 score. Additional adversarial perturbation and cold-start simulations show that the model remains robust under moderate graph modifications and generalizes effectively to previously unseen sessions and URLs. The proposed framework is deployment-friendly, integrates with existing systems without client-side instrumentation, and supports real-time inference and incremental updates, making it suitable for practical e-commerce security deployments.

</details>


### [176] [MC-GRPO: Median-Centered Group Relative Policy Optimization for Small-Rollout Reinforcement Learning](https://arxiv.org/abs/2601.22582)
*Youngeun Kim*

Main category: cs.LG

TL;DR: 针对资源受限下组相对策略优化方法精度下降问题，提出MC - GRPO方法，用中位数基线替换均值基线，提升了低滚动预算下训练稳定性和精度。


<details>
  <summary>Details</summary>
Motivation: 组相对策略优化方法在资源受限、滚动预算小的情况下精度下降，噪声导致优势符号翻转。

Method: 提出Median - Centered Group Relative Policy Optimization (MC - GRPO)方法，用中位数基线替换均值基线，生成G + 1个滚动以计算中位数，排除中位数滚动进行反向传播。

Result: 在各种GRPO系列方法、不同模型和规模中，MC - GRPO在低滚动制度下持续提高稳定性和最终精度，将G = 2和G = 8的差距缩小到1%以内。

Conclusion: MC - GRPO方法能有效解决小滚动训练中的问题，提升训练效果。

Abstract: Group-relative policy optimization methods train language models by generating multiple rollouts per prompt and normalizing rewards with a shared mean reward baseline. In resource-constrained settings where the rollout budget is small, accuracy often degrades. We find that noise in the shared baseline induces advantage sign flips, where some rollouts receive an incorrect advantage sign, and the update direction is reversed. To address this, we propose Median-Centered Group Relative Policy Optimization (MC-GRPO), a simple and effective solution for small-rollout training. Our main idea is to replace the mean baseline with a median baseline: the median is far less sensitive to outlier rewards than the mean, mitigating the sign flips under small rollout size (G). We generate one additional rollout for median reference (G+1), and compute advantages by using the group median. With an odd-sized group, exactly one completion is the median and receives zero advantage, we exclude this pivot rollout from backpropagation so the number of gradient-contributing samples per prompt remains G, preserving the core update cost of standard G-rollout training. Across various GRPO-family methods and a wide range of models and scales, this median-centered training consistently improves stability and final accuracy in the low-rollout regime, reducing the gap between G=2 and G=8 to within 1%. Code is available at https://github.com/lotusroot-kim/MC-GRPO

</details>


### [177] [FedCARE: Federated Unlearning with Conflict-Aware Projection and Relearning-Resistant Recovery](https://arxiv.org/abs/2601.22589)
*Yue Li,Mingmin Chu,Xilei Yang,Da Xiao,Ziqi Xu,Wei Shao,Qipeng Song,Hui Li*

Main category: cs.LG

TL;DR: 本文提出FedCARE框架解决联邦无学习（FU）问题，实验表明其效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习需满足隐私法规，但现有FU方法存在开销大、效用降低和意外重学习等问题。

Method: 提出FedCARE框架，利用梯度上升遗忘数据、数据自由模型反演构建代理，集成伪样本生成器等组件。

Result: 在多数据集和模型架构上实验，FedCARE实现有效遗忘、提高效用保留并降低重学习风险。

Conclusion: FedCARE是统一且低开销的FU框架，能解决现有FU方法的问题。

Abstract: Federated learning (FL) enables collaborative model training without centralizing raw data, but privacy regulations such as the right to be forgotten require FL systems to remove the influence of previously used training data upon request. Retraining a federated model from scratch is prohibitively expensive, motivating federated unlearning (FU). However, existing FU methods suffer from high unlearning overhead, utility degradation caused by entangled knowledge, and unintended relearning during post-unlearning recovery. In this paper, we propose FedCARE, a unified and low overhead FU framework that enables conflict-aware unlearning and relearning-resistant recovery. FedCARE leverages gradient ascent for efficient forgetting when target data are locally available and employs data free model inversion to construct class level proxies of shared knowledge. Based on these insights, FedCARE integrates a pseudo-sample generator, conflict-aware projected gradient ascent for utility preserving unlearning, and a recovery strategy that suppresses rollback toward the pre-unlearning model. FedCARE supports client, instance, and class level unlearning with modest overhead. Extensive experiments on multiple datasets and model architectures under both IID and non-IID settings show that FedCARE achieves effective forgetting, improved utility retention, and reduced relearning risk compared to state of the art FU baselines.

</details>


### [178] [Heterogeneous Graph Alignment for Joint Reasoning and Interpretability](https://arxiv.org/abs/2601.22593)
*Zahra Moslemi,Ziyi Liang,Norbert Fortin,Babak Shahbaba*

Main category: cs.LG

TL;DR: 本文提出Multi - Graph Meta - Transformer (MGMT)用于跨图学习，在图级预测任务上优于现有模型且有可解释性。


<details>
  <summary>Details</summary>
Motivation: 多图学习中有效整合不同拓扑、规模和语义的图信息是挑战，需解决跨图信息整合问题。

Method: 先对每个图应用Graph Transformer编码器映射到共享潜在空间，通过注意力选择任务相关超节点并构建元图，在元图上用额外Graph Transformer层进行联合推理。

Result: 在合成数据集和真实神经科学应用中，MGMT在图级预测任务上始终优于现有最先进模型。

Conclusion: MGMT是结构化多图学习的统一框架，推动了基于图数据领域的表征技术。

Abstract: Multi-graph learning is crucial for extracting meaningful signals from collections of heterogeneous graphs. However, effectively integrating information across graphs with differing topologies, scales, and semantics, often in the absence of shared node identities, remains a significant challenge. We present the Multi-Graph Meta-Transformer (MGMT), a unified, scalable, and interpretable framework for cross-graph learning. MGMT first applies Graph Transformer encoders to each graph, mapping structure and attributes into a shared latent space. It then selects task-relevant supernodes via attention and builds a meta-graph that connects functionally aligned supernodes across graphs using similarity in the latent space. Additional Graph Transformer layers on this meta-graph enable joint reasoning over intra- and inter-graph structure. The meta-graph provides built-in interpretability: supernodes and superedges highlight influential substructures and cross-graph alignments. Evaluating MGMT on both synthetic datasets and real-world neuroscience applications, we show that MGMT consistently outperforms existing state-of-the-art models in graph-level prediction tasks while offering interpretable representations that facilitate scientific discoveries. Our work establishes MGMT as a unified framework for structured multi-graph learning, advancing representation techniques in domains where graph-based data plays a central role.

</details>


### [179] [Lethe:Adapter-Augmented Dual-Stream Update for Persistent Knowledge Erasure in Federated Unlearning](https://arxiv.org/abs/2601.22601)
*Hanwei Tan,Wentai Hu,Ligang He,Yijun Quan*

Main category: cs.LG

TL;DR: 本文指出联邦持续训练中知识重现问题，提出Lethe方法解决，该方法按特定流程操作，实验显示其能统一支持各级别联邦学习且有高持久性。


<details>
  <summary>Details</summary>
Motivation: 现有联邦无学习研究忽略后续持续训练情况，持续训练会导致已删除知识重现，需解决此问题。

Method: 提出Lethe方法，遵循Reshape - Rectify - Restore流程，先训练临时适配器，再进行层间校正，最后移除适配器并恢复训练。

Result: Lethe能统一支持联邦系统各级别的无学习，且在多轮后续训练后重现率大多低于1%。

Conclusion: Lethe方法可有效解决联邦持续训练中的知识重现问题，确保知识持久删除。

Abstract: Federated unlearning (FU) aims to erase designated client-level, class-level, or sample-level knowledge from a global model. Existing studies commonly assume that the collaboration ends up with the unlearning operation, overlooking the follow-up situation where the federated training continues over the remaining data.We identify a critical failure mode, termed Knowledge resurfacing, by revealing that continued training can re-activate unlearned knowledge and cause the removed influence to resurface in the global model. To address this, we propose Lethe, a novel federated unlearning method that de-correlates knowledge to be unlearned from knowledge to be retained, ensuring persistent erasure during continued training.Lethe follows a Reshape--Rectify--Restore pipeline: a temporary adapter is first trained with gradient ascent on the unlearning data to obtain magnified updates, which is then used as corrective signals to diverge layer-wise rectification on the remaining updates in two streams. Finally, the adapter is removed and a short recovery stage is performed on the retained data. Our experiments show that Lethe supports unlearning in the federated system at all levels in a unified manner and maintains superior persistence (Resurfacing Rate <1% in most cases) even after numerous rounds of follow-up training.

</details>


### [180] [Local-Global Multimodal Contrastive Learning for Molecular Property Prediction](https://arxiv.org/abs/2601.22610)
*Xiayu Liu,Zhengyi Lu,Yunhong Liao,Chan Fan,Hou-biao Li*

Main category: cs.LG

TL;DR: 提出LGM - CL框架用于分子属性预测，结合多模态信息，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 准确的分子属性预测需要整合分子结构和化学语义的互补信息。

Method: 提出LGM - CL框架，用AttentiveFP和Graph Transformer编码器分别捕获局部和全局信息，通过自监督对比学习对齐，对比化学增强文本和原始SMILES，微调时用双交叉注意力多模态融合。

Result: 在MoleculeNet基准测试中，LGM - CL在分类和回归任务上都取得了稳定且有竞争力的性能。

Conclusion: 统一的局部 - 全局和多模态表示学习是有效的。

Abstract: Accurate molecular property prediction requires integrating complementary information from molecular structure and chemical semantics. In this work, we propose LGM-CL, a local-global multimodal contrastive learning framework that jointly models molecular graphs and textual representations derived from SMILES and chemistry-aware augmented texts. Local functional group information and global molecular topology are captured using AttentiveFP and Graph Transformer encoders, respectively, and aligned through self-supervised contrastive learning. In addition, chemically enriched textual descriptions are contrasted with original SMILES to incorporate physicochemical semantics in a task-agnostic manner. During fine-tuning, molecular fingerprints are further integrated via Dual Cross-attention multimodal fusion. Extensive experiments on MoleculeNet benchmarks demonstrate that LGM-CL achieves consistent and competitive performance across both classification and regression tasks, validating the effectiveness of unified local-global and multimodal representation learning.

</details>


### [181] [Stabilizing Transformer Training Through Consensus](https://arxiv.org/abs/2601.22614)
*Shyam Venkatasubramanian,Sean Moushegian,Michael Lin,Mir Park,Ankit Singhal,Connor Lee*

Main category: cs.LG

TL;DR: 标准注意力Transformer在高学习率训练时不稳定，共识机制可解决该问题并提出混合框架。


<details>
  <summary>Details</summary>
Motivation: 解决标准注意力Transformer在学习率过高时训练不稳定问题，且挖掘基础架构创新方法。

Method: 用共识机制替代注意力，将共识表示为图形模型，做了广泛实验分析，提出混合共识 - 注意力框架并进行理论分析。

Result: 共识机制能在更广泛学习率范围内稳定Transformer训练，在文本、DNA和蛋白质模态上展现出更好稳定性。

Conclusion: 共识机制可稳定训练，所提出的混合框架能在保持性能的同时提高稳定性，理论分析也刻画了共识的特性。

Abstract: Standard attention-based transformers are known to exhibit instability under learning rate overspecification during training, particularly at high learning rates. While various methods have been proposed to improve resilience to such overspecification by modifying the optimization procedure, fundamental architectural innovations to this end remain underexplored. In this work, we illustrate that the consensus mechanism, a drop-in replacement for attention, stabilizes transformer training across a wider effective range of learning rates. We formulate consensus as a graphical model and provide extensive empirical analysis demonstrating improved stability across learning rate sweeps on text, DNA, and protein modalities. We further propose a hybrid consensus-attention framework that preserves performance while improving stability. We provide theoretical analysis characterizing the properties of consensus.

</details>


### [182] [TTCS: Test-Time Curriculum Synthesis for Self-Evolving](https://arxiv.org/abs/2601.22628)
*Chengyi Yang,Zhishang Xiang,Yunbo Tang,Zongpei Teng,Chengsong Huang,Fei Long,Yuhan Liu,Jinsong Su*

Main category: cs.LG

TL;DR: 本文提出TTCS协同进化测试时训练框架，通过问题合成器和推理求解器的迭代优化提高大语言模型推理能力，实验显示有良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时训练方法在处理困难推理问题时存在问题，原始测试问题难产生高质量伪标签，且测试集小使在线更新不稳定。

Method: 提出TTCS框架，初始化问题合成器和推理求解器两个策略，通过迭代优化，合成器生成问题变体，求解器用自我一致性奖励更新，二者协同进化。

Result: TTCS在具有挑战性的数学基准上持续增强推理能力，可迁移到不同大语言模型骨干的通用领域任务。

Conclusion: TTCS为动态构建自进化的测试时课程提供了可扩展路径。

Abstract: Test-Time Training offers a promising way to improve the reasoning ability of large language models (LLMs) by adapting the model using only the test questions. However, existing methods struggle with difficult reasoning problems for two reasons: raw test questions are often too difficult to yield high-quality pseudo-labels, and the limited size of test sets makes continuous online updates prone to instability. To address these limitations, we propose TTCS, a co-evolving test-time training framework. Specifically, TTCS initializes two policies from the same pretrained model: a question synthesizer and a reasoning solver. These policies evolve through iterative optimization: the synthesizer generates progressively challenging question variants conditioned on the test questions, creating a structured curriculum tailored to the solver's current capability, while the solver updates itself using self-consistency rewards computed from multiple sampled responses on both original test and synthetic questions. Crucially, the solver's feedback guides the synthesizer to generate questions aligned with the model's current capability, and the generated question variants in turn stabilize the solver's test-time training. Experiments show that TTCS consistently strengthens the reasoning ability on challenging mathematical benchmarks and transfers to general-domain tasks across different LLM backbones, highlighting a scalable path towards dynamically constructing test-time curricula for self-evolving. Our code and implementation details are available at https://github.com/XMUDeepLIT/TTCS.

</details>


### [183] [PEFT-MuTS: A Multivariate Parameter-Efficient Fine-Tuning Framework for Remaining Useful Life Prediction based on Cross-domain Time Series Representation Model](https://arxiv.org/abs/2601.22631)
*En Fu,Yanyan Hu,Changhua Hu,Zengwang Jin,Kaixiang Peng*

Main category: cs.LG

TL;DR: 提出PEFT - MuTS框架用于少样本剩余使用寿命（RUL）预测，在跨域预训练时间序列表示模型基础上构建，实验表明少量样本下也能有效预测且优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的RUL预测受大量退化数据可用性限制，主流解决方案依赖大量历史退化数据，实际应用受限。

Method: 构建PEFT - MuTS框架，开发独立特征调整网络和基于元变量的低秩多变量融合机制，引入零初始化回归器。

Result: 在航空发动机和工业轴承数据集实验中，用不到1%目标设备样本就能有效预测RUL，显著优于传统监督和少样本方法，大幅减少实现高预测精度所需数据。

Conclusion: PEFT - MuTS框架能在少样本条件下有效进行RUL预测，减少对大量数据的依赖，具有实际应用价值。

Abstract: The application of data-driven remaining useful life (RUL) prediction has long been constrained by the availability of large amount of degradation data. Mainstream solutions such as domain adaptation and meta-learning still rely on large amounts of historical degradation data from equipment that is identical or similar to the target, which imposes significant limitations in practical applications. This study investigates PEFT-MuTS, a Parameter-Efficient Fine-Tuning framework for few-shot RUL prediction, built on cross-domain pre-trained time-series representation models. Contrary to the widely held view that knowledge transfer in RUL prediction can only occur within similar devices, we demonstrate that substantial benefits can be achieved through pre-training process with large-scale cross-domain time series datasets. A independent feature tuning network and a meta-variable-based low rank multivariate fusion mechanism are developed to enable the pre-trained univariate time-series representation backbone model to fully exploit the multivariate relationships in degradation data for downstream RUL prediction task. Additionally, we introduce a zero-initialized regressor that stabilizes the fine-tuning process under few-shot conditions. Experiments on aero-engine and industrial bearing datasets demonstrate that our method can achieve effective RUL prediction even when less than 1\% of samples of target equipment are used. Meanwhile, it substantially outperforms conventional supervised and few-shot approaches while markedly reducing the data required to achieve high predictive accuracy. Our code is available at https://github.com/fuen1590/PEFT-MuTS.

</details>


### [184] [Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification](https://arxiv.org/abs/2601.22642)
*Chuxue Cao,Jinluan Yang,Haoran Li,Kunhao Pan,Zijian Zhao,Zhengyu Chen,Yuchen Tian,Lijun Wu,Conghui He,Sirui Han,Yike Guo*

Main category: cs.LG

TL;DR: 引入形式逻辑验证引导框架提升大语言模型推理性能，在多基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 大语言模型随机预测存在逻辑不一致和奖励破解问题，形式符号系统可避免，需弥合差距。

Method: 引入形式逻辑验证引导框架，动态交织形式符号验证与自然语言生成过程；采用两阶段训练管道，结合监督微调与策略优化。

Result: 7B和14B模型在六个基准测试中分别平均领先现有基线10.4%和14.2%。

Conclusion: 形式验证可作为可扩展机制显著提升大语言模型推理性能。

Abstract: Large Language Models (LLMs) show remarkable capabilities, yet their stochastic next-token prediction creates logical inconsistencies and reward hacking that formal symbolic systems avoid. To bridge this gap, we introduce a formal logic verification-guided framework that dynamically interleaves formal symbolic verification with the natural language generation process, providing real-time feedback to detect and rectify errors as they occur. Distinguished from previous neuro-symbolic methods limited by passive post-hoc validation, our approach actively penalizes intermediate fallacies during the reasoning chain. We operationalize this framework via a novel two-stage training pipeline that synergizes formal logic verification-guided supervised fine-tuning and policy optimization. Extensive evaluation on six benchmarks spanning mathematical, logical, and general reasoning demonstrates that our 7B and 14B models outperform state-of-the-art baselines by average margins of 10.4% and 14.2%, respectively. These results validate that formal verification can serve as a scalable mechanism to significantly push the performance boundaries of advanced LLM reasoning.

</details>


### [185] [GUDA: Counterfactual Group-wise Training Data Attribution for Diffusion Models via Unlearning](https://arxiv.org/abs/2601.22651)
*Naoki Murata,Yuhta Takida,Chieh-Hsin Lai,Toshimitsu Uesaka,Bac Nguyen,Stefano Ermon,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: 提出针对扩散模型的GUDA方法进行组级数据归因，比其他方法更可靠且速度更快。


<details>
  <summary>Details</summary>
Motivation: 现有视觉生成模型训练数据归因方法多为个体评分，从业者需要组级答案，而LOGO重训计算成本高。

Method: 提出GUDA方法，通过对共享全数据模型应用机器学习遗忘来近似反事实模型，用ELBO评分规则差异量化组影响。

Result: 在CIFAR - 10和艺术风格归因实验中，比语义相似性、基于梯度归因和实例级遗忘方法更能可靠识别主要贡献组，在CIFAR - 10上比LOGO重训快100倍。

Conclusion: GUDA方法在视觉生成模型组级训练数据归因方面效果好、速度快。

Abstract: Training-data attribution for vision generative models aims to identify which training data influenced a given output. While most methods score individual examples, practitioners often need group-level answers (e.g., artistic styles or object classes). Group-wise attribution is counterfactual: how would a model's behavior on a generated sample change if a group were absent from training? A natural realization of this counterfactual is Leave-One-Group-Out (LOGO) retraining, which retrains the model with each group removed; however, it becomes computationally prohibitive as the number of groups grows. We propose GUDA (Group Unlearning-based Data Attribution) for diffusion models, which approximates each counterfactual model by applying machine unlearning to a shared full-data model instead of training from scratch. GUDA quantifies group influence using differences in a likelihood-based scoring rule (ELBO) between the full model and each unlearned counterfactual. Experiments on CIFAR-10 and artistic style attribution with Stable Diffusion show that GUDA identifies primary contributing groups more reliably than semantic similarity, gradient-based attribution, and instance-level unlearning approaches, while achieving x100 speedup on CIFAR-10 over LOGO retraining.

</details>


### [186] [Layerwise Progressive Freezing Enables STE-Free Training of Deep Binary Neural Networks](https://arxiv.org/abs/2601.22660)
*Evan Gibson Smith,Bashima Islam*

Main category: cs.LG

TL;DR: 研究用渐进冻结替代STE训练二值网络，提出StoMPP方法提升精度，分析训练动态。


<details>
  <summary>Details</summary>
Motivation: 寻找从无到有训练二值网络时替代直传估计器（STE）的方法。

Method: 引入StoMPP，用逐层随机掩码逐步替换可微裁剪权重/激活值为硬二值阶跃函数，仅对未冻结子集反向传播。

Result: 在匹配的最小训练配方下，StoMPP比BinaryConnect式STE基线提高了精度，对不同网络和数据集有不同程度增益；在二值权重网络上，ResNet - 50在CIFAR - 10和CIFAR - 100上取得一定准确率。

Conclusion: 渐进冻结训练存在非单调收敛，在二值化约束下深度缩放得到改善。

Abstract: We investigate progressive freezing as an alternative to straight-through estimators (STE) for training binary networks from scratch. Under controlled training conditions, we find that while global progressive freezing works for binary-weight networks, it fails for full binary neural networks due to activation-induced gradient blockades. We introduce StoMPP (Stochastic Masked Partial Progressive Binarization), which uses layerwise stochastic masking to progressively replace differentiable clipped weights/activations with hard binary step functions, while only backpropagating through the unfrozen (clipped) subset (i.e., no straight-through estimator). Under a matched minimal training recipe, StoMPP improves accuracy over a BinaryConnect-style STE baseline, with gains that increase with depth (e.g., for ResNet-50 BNN: +18.0 on CIFAR-10, +13.5 on CIFAR-100, and +3.8 on ImageNet; for ResNet-18: +3.1, +4.7, and +1.3). For binary-weight networks, StoMPP achieves 91.2\% accuracy on CIFAR-10 and 69.5\% on CIFAR-100 with ResNet-50. We analyze training dynamics under progressive freezing, revealing non-monotonic convergence and improved depth scaling under binarization constraints.

</details>


### [187] [Beyond Fixed Rounds: Data-Free Early Stopping for Practical Federated Learning](https://arxiv.org/abs/2601.22669)
*Youngjoon Lee,Hyukjoon Lee,Seungrok Jung,Andy Luo,Jinu Gong,Yang Cao,Joonhyuk Kang*

Main category: cs.LG

TL;DR: 提出无数据早期停止框架用于联邦学习，免验证数据，实验显示效果好，性能高且轮数少。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习超参数调整依赖固定全局轮数或验证数据，有高计算成本和隐私风险。

Method: 提出数据无早期停止框架，仅通过服务器端参数监控任务向量增长率确定最佳停止点。

Result: 在皮肤病变/血细胞分类上，该方法与基于验证的早期停止相当，平均用更少轮数实现更高性能。

Conclusion: 这是首个不使用验证数据为联邦学习方法提出的早期停止框架。

Abstract: Federated Learning (FL) facilitates decentralized collaborative learning without transmitting raw data. However, reliance on fixed global rounds or validation data for hyperparameter tuning hinders practical deployment by incurring high computational costs and privacy risks. To address this, we propose a data-free early stopping framework that determines the optimal stopping point by monitoring the task vector's growth rate using solely server-side parameters. The numerical results on skin lesion/blood cell classification demonstrate that our approach is comparable to validation-based early stopping across various state-of-the-art FL methods. In particular, the proposed framework spends an average of 47/20 (skin lesion/blood cell) rounds to achieve over 12.5%/10.3% higher performance than early stopping based on validation data. To the best of our knowledge, this is the first work to propose an early stopping framework for FL methods without using any validation data.

</details>


### [188] [Full-Graph vs. Mini-Batch Training: Comprehensive Analysis from a Batch Size and Fan-Out Size Perspective](https://arxiv.org/abs/2601.22678)
*Mengfan Liu,Da Zheng,Junwei Su,Chuan Wu*

Main category: cs.LG

TL;DR: 本文通过实验和理论分析，从批量大小和扇出大小的角度系统比较了全图和小批量GNN训练，揭示它们对收敛和泛化的影响，发现全图训练不一定优于调优的小批量训练。


<details>
  <summary>Details</summary>
Motivation: 全图和小批量GNN训练有不同系统设计需求，目前对批量和扇出大小对GNN的影响研究不足，需要系统比较二者。

Method: 通过经验和理论分析，使用Wasserstein距离进行泛化分析，研究图结构尤其是扇出大小的影响。

Result: 揭示了批量大小和扇出大小在GNN收敛和泛化中的非各向同性效应。

Conclusion: 全图训练并不总是比调优的小批量设置在模型性能或计算效率上更优。

Abstract: Full-graph and mini-batch Graph Neural Network (GNN) training approaches have distinct system design demands, making it crucial to choose the appropriate approach to develop. A core challenge in comparing these two GNN training approaches lies in characterizing their model performance (i.e., convergence and generalization) and computational efficiency. While a batch size has been an effective lens in analyzing such behaviors in deep neural networks (DNNs), GNNs extend this lens by introducing a fan-out size, as full-graph training can be viewed as mini-batch training with the largest possible batch size and fan-out size. However, the impact of the batch and fan-out size for GNNs remains insufficiently explored. To this end, this paper systematically compares full-graph vs. mini-batch training of GNNs through empirical and theoretical analyses from the view points of the batch size and fan-out size. Our key contributions include: 1) We provide a novel generalization analysis using the Wasserstein distance to study the impact of the graph structure, especially the fan-out size. 2) We uncover the non-isotropic effects of the batch size and the fan-out size in GNN convergence and generalization, providing practical guidance for tuning these hyperparameters under resource constraints. Finally, full-graph training does not always yield better model performance or computational efficiency than well-tuned smaller mini-batch settings. The implementation can be found in the github link: https://github.com/LIUMENGFAN-gif/GNN_fullgraph_minibatch_training.

</details>


### [189] [Stabilizing Consistency Training: A Flow Map Analysis and Self-Distillation](https://arxiv.org/abs/2601.22679)
*Youngjoong Kim,Duhoe Kim,Woosung Kim,Jaesik Park*

Main category: cs.LG

TL;DR: 对一致性模型进行理论研究，提出改进策略并展示其在多领域适用性。


<details>
  <summary>Details</summary>
Motivation: 现有一致性模型训练时存在固有不稳定性和可重复性问题，且相关解释零散、理论关系不明。

Method: 从基于流映射的角度分析一致性模型，重新审视自蒸馏方法并进行改进。

Result: 分析明确了训练稳定性和收敛行为如何导致退化解，改进策略避免了过大梯度范数，且可用于扩散策略学习。

Conclusion: 提出的策略具有更广泛的适用性，不依赖预训练扩散模型初始化。

Abstract: Consistency models have been proposed for fast generative modeling, achieving results competitive with diffusion and flow models. However, these methods exhibit inherent instability and limited reproducibility when training from scratch, motivating subsequent work to explain and stabilize these issues. While these efforts have provided valuable insights, the explanations remain fragmented, and the theoretical relationships remain unclear. In this work, we provide a theoretical examination of consistency models by analyzing them from a flow map-based perspective. This joint analysis clarifies how training stability and convergence behavior can give rise to degenerate solutions. Building on these insights, we revisit self-distillation as a practical remedy for certain forms of suboptimal convergence and reformulate it to avoid excessive gradient norms for stable optimization. We further demonstrate that our strategy extends beyond image generation to diffusion-based policy learning, without reliance on a pretrained diffusion model for initialization, thereby illustrating its broader applicability.

</details>


### [190] [Do Transformers Have the Ability for Periodicity Generalization?](https://arxiv.org/abs/2601.22690)
*Huanyu Liu,Ge Li,Yihong Dong,Sihan Wu,Peixu Wang,Sihao Cheng,Taozhi Chen,Kechi Zhang,Hao Zhu,Tongxuan Liu*

Main category: cs.LG

TL;DR: 研究大语言模型在周期性泛化这一OOD场景的表现，指出Transformer周期性泛化能力有限。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在OOD泛化上有局限，通过周期性这一基本OOD场景研究该差距。

Method: 从抽象代数和推理角度统一解释周期性，构建关于复合周期性的可控生成基准Coper并设置两种OOD场景。

Result: Transformer周期性泛化能力有限，能记忆周期性数据但无法泛化到未见复合周期性。

Conclusion: 未明确提及除实验结果外的结论，开源代码支持后续研究。

Abstract: Large language models (LLMs) based on the Transformer have demonstrated strong performance across diverse tasks. However, current models still exhibit substantial limitations in out-of-distribution (OOD) generalization compared with humans. We investigate this gap through periodicity, one of the basic OOD scenarios. Periodicity captures invariance amid variation. Periodicity generalization represents a model's ability to extract periodic patterns from training data and generalize to OOD scenarios. We introduce a unified interpretation of periodicity from the perspective of abstract algebra and reasoning, including both single and composite periodicity, to explain why Transformers struggle to generalize periodicity. Then we construct Coper about composite periodicity, a controllable generative benchmark with two OOD settings, Hollow and Extrapolation. Experiments reveal that periodicity generalization in Transformers is limited, where models can memorize periodic data during training, but cannot generalize to unseen composite periodicity. We release the source code to support future research.

</details>


### [191] [Metric Hub: A metric library and practical selection workflow for use-case-driven data quality assessment in medical AI](https://arxiv.org/abs/2601.22702)
*Katinka Becker,Maximilian P. Oppelt,Tobias S. Zech,Martin Seyferth,Sandie Cabon,Vanja Miskovic,Ivan Cimrak,Michal Kozubek,Giuseppe D'Avenio,Ilaria Campioni,Jana Fehr,Kanjar De,Ismail Mahmoudi,Emilio Dolgener Cantu,Laurenz Ottmann,Andreas Klaß,Galaad Altares,Jackie Ma,Alireza Salehi M.,Nadine R. Lang-Richter,Tobias Schaeffter,Daniel Schwabe*

Main category: cs.LG

TL;DR: 文章提出用于实践测量数据质量维度的度量库，为医学机器学习中数据质量量化提供方案，并以PTB - XL心电图数据集为例展示效果，是建立医学可信AI的第一步。


<details>
  <summary>Details</summary>
Motivation: 医学机器学习应用需证据证明其可信性，而数据质量量化是开发可信AI的重要因素，因此需要对数据质量进行评估。

Method: 引入由一系列数据质量指标组成的度量库，为每个指标提供度量卡；讨论选定用例下选择合适指标集的策略并提供决策树。

Result: 以PTB - XL心电图数据集为例，展示了该方法的影响。

Conclusion: 该方法是在实践中对训练和测试数据进行适用性评估的第一步，为建立医学可信AI奠定基础。

Abstract: Machine learning (ML) in medicine has transitioned from research to concrete applications aimed at supporting several medical purposes like therapy selection, monitoring and treatment. Acceptance and effective adoption by clinicians and patients, as well as regulatory approval, require evidence of trustworthiness. A major factor for the development of trustworthy AI is the quantification of data quality for AI model training and testing. We have recently proposed the METRIC-framework for systematically evaluating the suitability (fit-for-purpose) of data for medical ML for a given task. Here, we operationalize this theoretical framework by introducing a collection of data quality metrics - the metric library - for practically measuring data quality dimensions. For each metric, we provide a metric card with the most important information, including definition, applicability, examples, pitfalls and recommendations, to support the understanding and implementation of these metrics. Furthermore, we discuss strategies and provide decision trees for choosing an appropriate set of data quality metrics from the metric library given specific use cases. We demonstrate the impact of our approach exemplarily on the PTB-XL ECG-dataset. This is a first step to enable fit-for-purpose evaluation of training and test data in practice as the base for establishing trustworthy AI in medicine.

</details>


### [192] [Deep Learning-Based Early-Stage IR-Drop Estimation via CNN Surrogate Modeling](https://arxiv.org/abs/2601.22707)
*Ritesh Bhadana*

Main category: cs.LG

TL;DR: 提出基于CNN深度学习代理建模方法用于早期IR - drop估计，训练模型在自定义数据集上，实现快速预测，代码和数据集公开。


<details>
  <summary>Details</summary>
Motivation: 传统IR - drop分析计算成本高、需接近最终布局信息，不适用于早期设计探索。

Method: 将任务设为像素级回归问题，用带跳跃连接的U - Net架构，在自定义物理合成数据集上训练。

Result: 能准确预测IR - drop分布，推理时间达毫秒级，可用标准回归指标评估。

Conclusion: 该框架可作早期分析工具，在昂贵验证分析前为设计者提供快速IR - drop洞察。

Abstract: IR-drop is a critical power integrity challenge in modern VLSI designs that can cause timing degradation, reliability issues, and functional failures if not detected early in the design flow. Conventional IR-drop analysis relies on physics-based signoff tools, which provide high accuracy but incur significant computational cost and require near-final layout information, making them unsuitable for rapid early-stage design exploration. In this work, we propose a deep learning-based surrogate modeling approach for early-stage IR-drop estimation using a CNN. The task is formulated as a dense pixel-wise regression problem, where spatial physical layout features are mapped directly to IR-drop heatmaps. A U-Net-based encoder-decoder architecture with skip connections is employed to effectively capture both local and global spatial dependencies within the layout. The model is trained on a physics-inspired synthetic dataset generated by us, which incorporates key physical factors including power grid structure, cell density distribution, and switching activity. Model performance is evaluated using standard regression metrics such as Mean Squared Error (MSE) and Peak Signal-to-Noise Ratio (PSNR). Experimental results demonstrate that the proposed approach can accurately predict IR-drop distributions with millisecond-level inference time, enabling fast pre-signoff screening and iterative design optimization. The proposed framework is intended as a complementary early-stage analysis tool, providing designers with rapid IR-drop insight prior to expensive signoff analysis. The implementation, dataset generation scripts, and the interactive inference application are publicly available at: https://github.com/riteshbhadana/IR-Drop-Predictor. The live application can be accessed at: https://ir-drop-predictor.streamlit.app/.

</details>


### [193] [A Unified Study of LoRA Variants: Taxonomy, Review, Codebase, and Empirical Evaluation](https://arxiv.org/abs/2601.22708)
*Haonan He,Jingqi Ye,Minglei Li,Zhengbo Wang,Tao Chen,Lei Bai,Peng Ye*

Main category: cs.LG

TL;DR: 本文对LoRA变体进行统一研究，给出分类、理论回顾、代码库和评估，发现LoRA及其变体对学习率敏感，合适超参下LoRA表现佳。


<details>
  <summary>Details</summary>
Motivation: LoRA变体在方法、理论、代码和评估上出现碎片化，需要统一研究。

Method: 从秩、优化动态、初始化和与专家混合体集成四个轴对LoRA变体分类；在统一理论框架下回顾其关系和演变；引入模块化代码库LoRAFactory；用代码库在多种任务上大规模评估并探索关键超参。

Result: LoRA及其变体对学习率选择比其他超参更敏感；合适超参配置下，LoRA表现常匹配或超越多数变体。

Conclusion: 通过统一研究，明确了LoRA及其变体的特性和性能表现。

Abstract: Low-Rank Adaptation (LoRA) is a fundamental parameter-efficient fine-tuning method that balances efficiency and performance in large-scale neural networks. However, the proliferation of LoRA variants has led to fragmentation in methodology, theory, code, and evaluation. To this end, this work presents the first unified study of LoRA variants, offering a systematic taxonomy, unified theoretical review, structured codebase, and standardized empirical assessment. First, we categorize LoRA variants along four principal axes: rank, optimization dynamics, initialization, and integration with Mixture-of-Experts. Then, we review their relationships and evolution within a common theoretical framework focused on low-rank update dynamics. Further, we introduce LoRAFactory, a modular codebase that implements variants through a unified interface, supporting plug-and-play experimentation and fine-grained analysis. Last, using this codebase, we conduct a large-scale evaluation across natural language generation, natural language understanding, and image classification tasks, systematically exploring key hyperparameters. Our results uncover several findings, notably: LoRA and its variants exhibit pronounced sensitivity to the choices of learning rate compared to other hyperparameters; moreover, with proper hyperparameter configurations, LoRA consistently matches or surpasses the performance of most of its variants.

</details>


### [194] [Vision-Language Models Unlock Task-Centric Latent Actions](https://arxiv.org/abs/2601.22714)
*Alexander Nikulin,Ilya Zisman,Albina Klepach,Denis Tarasov,Alexander Derevyagin,Andrei Polubarov,Lyubaykin Nikita,Vladislav Kurenkov*

Main category: cs.LG

TL;DR: 本文利用VLM常识推理能力提供可提示表示来优化LAM训练，发现不同VLM表现有差异，忽略干扰物可提升潜在动作质量和下游成功率。


<details>
  <summary>Details</summary>
Motivation: 现有潜在动作模型（LAMs）在观察包含与动作相关干扰物时会失败，而人类能轻松区分任务相关动作，因此希望优化LAMs。

Method: 利用视觉语言模型（VLMs）的常识推理能力提供可提示表示，在LAM训练中以这些表示为目标，并对多种流行VLMs进行基准测试。

Result: 不同VLM提供的可提示表示质量及其对不同提示和超参数的鲁棒性有很大差异，较新的VLM可能不如旧的VLM表现好，让VLM忽略干扰物能显著提升潜在动作质量，下游成功率最多提高六倍。

Conclusion: 利用VLMs的常识推理能力优化LAM训练是有效的，简单要求VLMs忽略干扰物就能大幅提升潜在动作质量和下游成功率。

Abstract: Latent Action Models (LAMs) have rapidly gained traction as an important component in the pre-training pipelines of leading Vision-Language-Action models. However, they fail when observations contain action-correlated distractors, often encoding noise instead of meaningful latent actions. Humans, on the other hand, can effortlessly distinguish task-relevant motions from irrelevant details in any video given only a brief task description. In this work, we propose to utilize the common-sense reasoning abilities of Vision-Language Models (VLMs) to provide promptable representations, effectively separating controllable changes from the noise in unsupervised way. We use these representations as targets during LAM training and benchmark a wide variety of popular VLMs, revealing substantial variation in the quality of promptable representations as well as their robustness to different prompts and hyperparameters. Interestingly, we find that more recent VLMs may perform worse than older ones. Finally, we show that simply asking VLMs to ignore distractors can substantially improve latent action quality, yielding up to a six-fold increase in downstream success rates on Distracting MetaWorld.

</details>


### [195] [Breaking the Blocks: Continuous Low-Rank Decomposed Scaling for Unified LLM Quantization and Adaptation](https://arxiv.org/abs/2601.22716)
*Pingzhi Tang,Ruijie Zhou,Fanxu Meng,Wenjie Pei,Muhan Zhang*

Main category: cs.LG

TL;DR: 提出低秩分解缩放（LoRDS）框架，打破块级空间约束，在大语言模型量化和微调任务中表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型量化方法多依赖块级结构，牺牲了表示灵活性，希望提升量化效率和表达能力。

Method: 将缩放流形建模为连续低秩矩阵，提出LoRDS统一框架，打破空间约束，有PTQ初始化、联合QAT、高秩乘法PEFT适配等步骤。

Result: 在不同模型家族的量化和下游微调任务中始终优于现有基线，如在Llama3 - 8B上，3位时准确率比NormalFloat量化提高27.0%，在RTX 4090上推理速度提升1.5倍，下游任务PEFT性能比4位QLoRA提高9.6%。

Conclusion: LoRDS为大语言模型的统一压缩和适配提供了强大而集成的解决方案。

Abstract: Current quantization methods for LLMs predominantly rely on block-wise structures to maintain efficiency, often at the cost of representational flexibility. In this work, we demonstrate that element-wise quantization can be made as efficient as block-wise scaling while providing strictly superior expressive power by modeling the scaling manifold as continuous low-rank matrices ($S = BA$). We propose Low-Rank Decomposed Scaling (LoRDS), a unified framework that rethinks quantization granularity through this low-rank decomposition. By "breaking the blocks" of spatial constraints, LoRDS establishes a seamless efficiency lifecycle: it provides high-fidelity PTQ initialization refined via iterative optimization, enables joint QAT of weights and scaling factors, and facilitates high-rank multiplicative PEFT adaptation. Unlike additive PEFT approaches such as QLoRA, LoRDS enables high-rank weight updates within a low-rank budget while incurring no additional inference overhead. Supported by highly optimized Triton kernels, LoRDS consistently outperforms state-of-the-art baselines across various model families in both quantization and downstream fine-tuning tasks. Notably, on Llama3-8B, our method achieves up to a 27.0% accuracy improvement at 3 bits over NormalFloat quantization and delivers a 1.5x inference speedup on NVIDIA RTX 4090 while enhancing PEFT performance by 9.6% on downstream tasks over 4bit QLoRA, offering a robust and integrated solution for unified compression and adaptation of LLMs.

</details>


### [196] [Local Intrinsic Dimension of Representations Predicts Alignment and Generalization in AI Models and Human Brain](https://arxiv.org/abs/2601.22722)
*Junjie Yu,Wenxiao Ma,Chen Wei,Jianyu Zhang,Haotian Deng,Zihan Deng,Quanying Liu*

Main category: cs.LG

TL;DR: 研究表明泛化能力强的神经网络与人类神经活动对齐性更强，泛化性能、模型间对齐和模型 - 大脑对齐显著相关，可由嵌入的局部内在维度解释，且增大模型容量和数据规模可降低该维度，其是表征收敛的统一描述符。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络泛化能力与人类神经活动对齐性的关系，以及三者关联的成因。

Method: 研究模型的泛化性能、模型间对齐、模型 - 大脑对齐的相关性，分析嵌入的局部和全局维度对这些指标的影响，研究模型容量和训练数据规模对局部内在维度的影响。

Result: 泛化能力强的模型与人类神经活动对齐性更强；三者显著相关；局部内在维度低与模型间、模型 - 大脑对齐性强及泛化好相关，全局维度无此效果；增大模型容量和数据规模降低局部内在维度。

Conclusion: 局部内在维度是人工和生物系统中表征收敛的统一描述符。

Abstract: Recent work has found that neural networks with stronger generalization tend to exhibit higher representational alignment with one another across architectures and training paradigms. In this work, we show that models with stronger generalization also align more strongly with human neural activity. Moreover, generalization performance, model--model alignment, and model--brain alignment are all significantly correlated with each other. We further show that these relationships can be explained by a single geometric property of learned representations: the local intrinsic dimension of embeddings. Lower local dimension is consistently associated with stronger model--model alignment, stronger model--brain alignment, and better generalization, whereas global dimension measures fail to capture these effects. Finally, we find that increasing model capacity and training data scale systematically reduces local intrinsic dimension, providing a geometric account of the benefits of scaling. Together, our results identify local intrinsic dimension as a unifying descriptor of representational convergence in artificial and biological systems.

</details>


### [197] [Decomposing Epistemic Uncertainty for Causal Decision Making](https://arxiv.org/abs/2601.22736)
*Md Musfiqur Rahman,Ziwei Jiang,Hilaf Hasson,Murat Kocaoglu*

Main category: cs.LG

TL;DR: 提出新框架处理观测数据因果效应界限问题，区分样本不确定性和非识别不确定性，实验证明算法可指导行动。


<details>
  <summary>Details</summary>
Motivation: 现有用神经网络获取因果效应界限的方法可能过拟合和过度自信，且缺乏系统方法区分界限宽度成因。

Method: 考虑经验观测分布的置信集，求该集合内所有分布因果效应界限的交集，通过求解带神经因果模型的极小 - 极大和极大 - 极小问题得到交集上下界。

Result: 在合成和真实数据集上的大量实验表明，算法能判断收集更多样本是否有助于确定最佳行动。

Conclusion: 算法可指导从业者收集更多变量或采用随机试验来确定最佳行动。

Abstract: Causal inference from observational data provides strong evidence for the best action in decision-making without performing expensive randomized trials. The effect of an action is usually not identifiable under unobserved confounding, even with an infinite amount of data. Recent work uses neural networks to obtain practical bounds to such causal effects, which is often an intractable problem. However, these approaches may overfit to the dataset and be overconfident in their causal effect estimates. Moreover, there is currently no systematic approach to disentangle how much of the width of causal effect bounds is due to fundamental non-identifiability versus how much is due to finite-sample limitations. We propose a novel framework to address this problem by considering a confidence set around the empirical observational distribution and obtaining the intersection of causal effect bounds for all distributions in this confidence set. This allows us to distinguish the part of the interval that can be reduced by collecting more samples, which we call sample uncertainty, from the part that can only be reduced by observing more variables, such as latent confounders or instrumental variables, but not with more data, which we call non-ID uncertainty. The upper and lower bounds to this intersection are obtained by solving min-max and max-min problems with neural causal models by searching over all distributions that the dataset might have been sampled from, and all SCMs that entail the corresponding distribution. We demonstrate via extensive experiments on synthetic and real-world datasets that our algorithm can determine when collecting more samples will not help determine the best action. This can guide practitioners to collect more variables or lean towards a randomized study for best action identification.

</details>


### [198] [Is Softmax Loss All You Need? A Principled Analysis of Softmax-family Loss](https://arxiv.org/abs/2601.22745)
*Yuanhao Pu,Defu Lian,Enhong Chen*

Main category: cs.LG

TL;DR: 本文对Softmax损失族进行研究，分析不同替代损失与分类排序指标的一致性、梯度动态，对近似方法进行偏差 - 方差分解和复杂度分析，实验验证了理论与实证性能的一致性。


<details>
  <summary>Details</summary>
Motivation: 阐明Softmax损失的理论性质，解决类别数量极多情况下的可扩展性问题。

Method: 研究不同替代损失与分类和排序指标的一致性，分析梯度动态，对近似方法进行系统的偏差 - 方差分解，进行每轮复杂度分析。

Result: 实验表明一致性、收敛性和实证性能之间有很强的一致性。

Conclusion: 为大规模类别机器学习应用中的损失选择建立了原则基础并提供了实用指导。

Abstract: The Softmax loss is one of the most widely employed surrogate objectives for classification and ranking tasks. To elucidate its theoretical properties, the Fenchel-Young framework situates it as a canonical instance within a broad family of surrogates. Concurrently, another line of research has addressed scalability when the number of classes is exceedingly large, in which numerous approximations have been proposed to retain the benefits of the exact objective while improving efficiency. Building on these two perspectives, we present a principled investigation of the Softmax-family losses. We examine whether different surrogates achieve consistency with classification and ranking metrics, and analyze their gradient dynamics to reveal distinct convergence behaviors. We also introduce a systematic bias-variance decomposition for approximate methods that provides convergence guarantees, and further derive a per-epoch complexity analysis, showing explicit trade-offs between effectiveness and efficiency. Extensive experiments on a representative task demonstrate a strong alignment between consistency, convergence, and empirical performance. Together, these results establish a principled foundation and offer practical guidance for loss selections in large-class machine learning applications.

</details>


### [199] [Discovering Scaling Exponents with Physics-Informed Müntz-Szász Networks](https://arxiv.org/abs/2601.22751)
*Gnankan Landry Regis N'guessan,Bum Jun Kim*

Main category: cs.LG

TL;DR: 引入物理信息Müntz - Szász网络（MSN - PINN）处理幂律缩放问题，能输出解及其缩放结构，实验表现好且可结合神经网络与渐进分析优点。


<details>
  <summary>Details</summary>
Motivation: 标准神经网络对幂律缩放的控制指数表达不明确，需更好方法处理物理系统在奇点、界面和临界点附近的幂律缩放问题。

Method: 引入物理信息Müntz - Szász网络（MSN - PINN），将缩放指数作为可训练参数，还采用约束感知训练。

Result: MSN - PINN在实验中能低误差恢复指数，如单指数恢复误差1 - 5%，在二维拉普拉斯方程等问题上误差极小，在楔形基准测试中成功率达100%，约束感知训练可提高精度三个数量级。

Conclusion: MSN - PINN结合神经网络表达能力与渐进分析解释性，产生具有直接物理意义的学习参数。

Abstract: Physical systems near singularities, interfaces, and critical points exhibit power-law scaling, yet standard neural networks leave the governing exponents implicit. We introduce physics-informed M"untz-Sz'asz Networks (MSN-PINN), a power-law basis network that treats scaling exponents as trainable parameters. The model outputs both the solution and its scaling structure. We prove identifiability, or unique recovery, and show that, under these conditions, the squared error between learned and true exponents scales as $O(|μ- α|^2)$. Across experiments, MSN-PINN achieves single-exponent recovery with 1--5% error under noise and sparse sampling. It recovers corner singularity exponents for the two-dimensional Laplace equation with 0.009% error, matches the classical result of Kondrat'ev (1967), and recovers forcing-induced exponents in singular Poisson problems with 0.03% and 0.05% errors. On a 40-configuration wedge benchmark, it reaches a 100% success rate with 0.022% mean error. Constraint-aware training encodes physical requirements such as boundary condition compatibility and improves accuracy by three orders of magnitude over naive training. By combining the expressiveness of neural networks with the interpretability of asymptotic analysis, MSN-PINN produces learned parameters with direct physical meaning.

</details>


### [200] [OSNIP: Breaking the Privacy-Utility-Efficiency Trilemma in LLM Inference via Obfuscated Semantic Null Space](https://arxiv.org/abs/2601.22752)
*Zhiyuan Cao,Zeyu Ma,Chenhao Yang,Han Zheng,Mingang Chen*

Main category: cs.LG

TL;DR: 提出了用于隐私保护的轻量级客户端加密框架OSNIP，在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 实现隐私保护的大语言模型推理。

Method: 定义“混淆语义零空间”，将原始嵌入投影到该空间，使用依赖密钥的随机映射合成个性化扰动轨迹。

Result: 在12个生成和分类基准测试中，大幅降低攻击成功率，严格安全约束下保持强模型效用。

Conclusion: OSNIP达到了目前最先进的性能。

Abstract: We propose Obfuscated Semantic Null space Injection for Privacy (OSNIP), a lightweight client-side encryption framework for privacy-preserving LLM inference. Generalizing the geometric intuition of linear kernels to the high-dimensional latent space of LLMs, we formally define the ``Obfuscated Semantic Null Space'', a high-dimensional regime that preserves semantic fidelity while enforcing near-orthogonality to the original embedding. By injecting perturbations that project the original embedding into this space, OSNIP ensures privacy without any post-processing. Furthermore, OSNIP employs a key-dependent stochastic mapping that synthesizes individualized perturbation trajectories unique to each user. Evaluations on 12 generative and classification benchmarks show that OSNIP achieves state-of-the-art performance, sharply reducing attack success rates while maintaining strong model utility under strict security constraints.

</details>


### [201] [Understanding Generalization from Embedding Dimension and Distributional Convergence](https://arxiv.org/abs/2601.22756)
*Junjie Yu,Zhuoli Ouyang,Haotian Deng,Chen Wei,Wenxiao Ma,Jianyu Zhang,Zihan Deng,Quanying Liu*

Main category: cs.LG

TL;DR: 从表征角度研究深度神经网络泛化性，提出由嵌入分布内在维度和下游映射敏感性决定的误差界，实验验证理论。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络过度参数化仍能良好泛化，挑战传统基于参数的分析，因此从表征角度研究泛化性。

Method: 分析学习到的嵌入的几何结构如何控制固定训练模型的预测性能，得出由嵌入分布内在维度和下游映射敏感性决定的误差界。

Result: 得出不依赖参数数量或假设类复杂度的嵌入相关误差界，在最终嵌入层，架构敏感性消失，泛化性能与嵌入维度强相关。

Conclusion: 实验验证了理论，证明了基于嵌入诊断的实用性。

Abstract: Deep neural networks often generalize well despite heavy over-parameterization, challenging classical parameter-based analyses. We study generalization from a representation-centric perspective and analyze how the geometry of learned embeddings controls predictive performance for a fixed trained model. We show that population risk can be bounded by two factors: (i) the intrinsic dimension of the embedding distribution, which determines the convergence rate of empirical embedding distribution to the population distribution in Wasserstein distance, and (ii) the sensitivity of the downstream mapping from embeddings to predictions, characterized by Lipschitz constants. Together, these yield an embedding-dependent error bound that does not rely on parameter counts or hypothesis class complexity. At the final embedding layer, architectural sensitivity vanishes and the bound is dominated by embedding dimension, explaining its strong empirical correlation with generalization performance. Experiments across architectures and datasets validate the theory and demonstrate the utility of embedding-based diagnostics.

</details>


### [202] [Unveiling Scaling Behaviors in Molecular Language Models: Effects of Model Size, Data, and Representation](https://arxiv.org/abs/2601.22757)
*Dong Xu,Qihua Pan,Sisi Yuan,Jianqiang Li,Zexuan Zhu,Junkai Ji*

Main category: cs.LG

TL;DR: 研究系统探究分子语言模型在预训练和下游任务中的缩放行为，发现缩放定律，揭示分子表示影响，还发布最大分子语言模型库。


<details>
  <summary>Details</summary>
Motivation: 不清楚分子生成模型在固定计算预算下是否遵循可预测的缩放定律，这对资源分配至关重要。

Method: 训练300个模型，进行超10000次实验，严格控制计算预算，独立改变模型大小、训练令牌数量和分子表示。

Result: 发现分子模型在预训练和下游迁移中的缩放定律，揭示分子表示对性能的重大影响，解释先前分子生成缩放行为的不一致性。

Conclusion: 明确分子模型的缩放定律等成果，还公开最大分子语言模型库促进后续研究。

Abstract: Molecular generative models, often employing GPT-style language modeling on molecular string representations, have shown promising capabilities when scaled to large datasets and model sizes. However, it remains unclear and subject to debate whether these models adhere to predictable scaling laws under fixed computational budgets, which is a crucial understanding for optimally allocating resources between model size, data volume, and molecular representation. In this study, we systematically investigate the scaling behavior of molecular language models across both pretraining and downstream tasks. We train 300 models and conduct over 10,000 experiments, rigorously controlling compute budgets while independently varying model size, number of training tokens, and molecular representation. Our results demonstrate clear scaling laws in molecular models for both pretraining and downstream transfer, reveal the substantial impact of molecular representation on performance, and explain previously observed inconsistencies in scaling behavior for molecular generation. Additionally, we publicly release the largest library of molecular language models to date to facilitate future research and development. Code and models are available at https://github.com/SZU-ADDG/MLM-Scaling.

</details>


### [203] [Sparse Attention as Compact Kernel Regression](https://arxiv.org/abs/2601.22766)
*Saul Santos,Nuno Gonçalves,Daniel C. McNamee,André F. T Martins*

Main category: cs.LG

TL;DR: 本文建立稀疏注意力与紧致核的对应关系，解释稀疏性来源，实验表明基于核的稀疏注意力在多项任务有竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对稀疏注意力机制的核理论理解，需建立相关对应关系。

Method: 建立稀疏注意力与紧致核的形式对应，研究不同核与注意力的关系。

Result: 归一化ReLU和sparsemax注意力分别源于固定和自适应归一化下的Epanechnikov核回归，多种常用核对应α - entmax注意力；基于核回归的Memory Mosaics在多项任务表现良好。

Conclusion: 统一视角解释稀疏性来源，为设计注意力机制提供原则性框架。

Abstract: Recent work has revealed a link between self-attention mechanisms in transformers and test-time kernel regression via the Nadaraya-Watson estimator, with standard softmax attention corresponding to a Gaussian kernel. However, a kernel-theoretic understanding of sparse attention mechanisms is currently missing. In this paper, we establish a formal correspondence between sparse attention and compact (bounded support) kernels. We show that normalized ReLU and sparsemax attention arise from Epanechnikov kernel regression under fixed and adaptive normalizations, respectively. More generally, we demonstrate that widely used kernels in nonparametric density estimation -- including Epanechnikov, biweight, and triweight -- correspond to $α$-entmax attention with $α= 1 + \frac{1}{n}$ for $n \in \mathbb{N}$, while the softmax/Gaussian relationship emerges in the limit $n \to \infty$. This unified perspective explains how sparsity naturally emerges from kernel design and provides principled alternatives to heuristic top-$k$ attention and other associative memory mechanisms. Experiments with a kernel-regression-based variant of transformers -- Memory Mosaics -- show that kernel-based sparse attention achieves competitive performance on language modeling, in-context learning, and length generalization tasks, offering a principled framework for designing attention mechanisms.

</details>


### [204] [Float8@2bits: Entropy Coding Enables Data-Free Model Compression](https://arxiv.org/abs/2601.22787)
*Patrick Putzky,Martin Genzel,Mattes Mollenhauer,Sebastian Schulze,Thomas Wollmann,Stefan Dietzel*

Main category: cs.LG

TL;DR: 介绍了EntQuant框架结合了无数据和依赖数据压缩方法的优点，能在极端压缩情况下实现实用价值，压缩速度快且效果好。


<details>
  <summary>Details</summary>
Motivation: 现有后训练压缩方法分为无数据和依赖数据两类，前者速度快但在极低比特率下性能差，后者精度高但计算成本高且鲁棒性不确定，需要结合两者优点的方法。

Method: 通过熵编码将数值精度与存储成本解耦。

Result: 在标准评估集和模型上取得了最先进的结果，在指令调优模型的复杂基准测试中也保持了功能性能，推理开销适度。

Conclusion: EntQuant结合了不同方法的优势，在极端压缩场景下具有实用价值。

Abstract: Post-training compression is currently divided into two contrasting regimes. On the one hand, fast, data-free, and model-agnostic methods (e.g., NF4 or HQQ) offer maximum accessibility but suffer from functional collapse at extreme bit-rates below 4 bits. On the other hand, techniques leveraging calibration data or extensive recovery training achieve superior fidelity but impose high computational constraints and face uncertain robustness under data distribution shifts. We introduce EntQuant, the first framework to unite the advantages of these distinct paradigms. By matching the performance of data-dependent methods with the speed and universality of data-free techniques, EntQuant enables practical utility in the extreme compression regime. Our method decouples numerical precision from storage cost via entropy coding, compressing a 70B parameter model in less than 30 minutes. We demonstrate that EntQuant does not only achieve state-of-the-art results on standard evaluation sets and models, but also retains functional performance on more complex benchmarks with instruction-tuned models, all at modest inference overhead.

</details>


### [205] [Clipping-Free Policy Optimization for Large Language Models](https://arxiv.org/abs/2601.22801)
*Ömer Veysel Çağatan,Barış Akgün,Gözde Gül Şahin,Xuandong Zhao*

Main category: cs.LG

TL;DR: 提出无裁剪策略优化（CFPO）算法替代基于裁剪的强化学习算法，在推理和对齐任务中表现良好，是大语言模型后训练的有前景替代方案。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型后训练中占主导的强化学习算法的裁剪机制存在优化问题，如零梯度区域、奖励破解和训练不稳定。

Method: 提出CFPO算法，用基于总变差散度约束的凸二次惩罚代替启发式裁剪，得到处处可微的目标函数。

Result: 在推理任务中，CFPO与基于裁剪的方法在下游基准测试中表现相当且延长稳定训练范围；在对齐任务中，CFPO减轻冗长利用、减少能力退化，指令跟随性能有竞争力，仅需一行代码更改且无需额外超参数。

Conclusion: CFPO是大语言模型后训练中基于裁剪方法的有前景替代方案。

Abstract: Reinforcement learning has become central to post-training large language models, yet dominant algorithms rely on clipping mechanisms that introduce optimization issues at scale, including zero-gradient regions, reward hacking, and training instability. We propose Clipping-Free Policy Optimization (CFPO), which replaces heuristic clipping with a convex quadratic penalty derived from Total Variation divergence constraints, yielding an everywhere-differentiable objective that enforces stable policy updates without hard boundaries. We evaluate CFPO across both reasoning and alignment settings. In reasoning, CFPO matches clipping-based methods on downstream benchmarks while extending the stable training regime. In alignment, CFPO mitigates verbosity exploitation and reduces capability degradation, while achieving competitive instruction-following performance. CFPO requires only a one-line code change and no additional hyperparameters. Our results suggest that CFPO is a promising drop-in alternative to clipping-based methods for LLM post-training.

</details>


### [206] [SOMBRERO: Measuring and Steering Boundary Placement in End-to-End Hierarchical Sequence Models](https://arxiv.org/abs/2601.22805)
*Pit Neitemeier,Alessio Serra,Jiaze Li,Sascha Wirges,Lukas Balles,Jan Hendrik Metzen*

Main category: cs.LG

TL;DR: 提出边界质量指标B和Sombrero方法，在多语料库上改善准确率 - 效率权衡。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以定量评估和系统引导计算资源分配位置。

Method: 引入边界质量指标边界富集B，提出Sombrero方法，包括使用置信对齐边界损失引导边界放置及在输入层应用置信加权平滑。

Result: 在1B规模、多种UTF - 8语料库上，改善了准确率 - 效率权衡，使边界能更好将计算资源与难预测位置对齐。

Conclusion: Sombrero方法有效提升了分层序列模型的性能。

Abstract: Hierarchical sequence models replace fixed tokenization with learned segmentations that compress long byte sequences for efficient autoregressive modeling. While recent end-to-end methods can learn meaningful boundaries from the language-modeling objective alone, it remains difficult to quantitatively assess and systematically steer where compute is spent. We introduce a router-agnostic metric of boundary quality, boundary enrichment B, which measures how strongly chunk starts concentrate on positions with high next-byte surprisal. Guided by this metric, we propose Sombrero, which steers boundary placement toward predictive difficulty via a confidence-alignment boundary loss and stabilizes boundary learning by applying confidence-weighted smoothing at the input level rather than on realized chunks. On 1B scale, across UTF-8 corpora covering English and German text as well as code and mathematical content, Sombrero improves the accuracy-efficiency trade-off and yields boundaries that more consistently align compute with hard-to-predict positions.

</details>


### [207] [Quartet II: Accurate LLM Pre-Training in NVFP4 by Improved Unbiased Gradient Estimation](https://arxiv.org/abs/2601.22813)
*Andrei Panferov,Erik Schultheis,Soroush Tabesh,Dan Alistarh*

Main category: cs.LG

TL;DR: 提出MS - EDEN和Quartet II改进NVFP4量化训练，在LLM训练验证，有速度提升，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有NVFP4量化训练方法为准确梯度估计牺牲格式表示能力，损失精度。

Method: 提出MS - EDEN量化例程，集成到Quartet II量化方案，结合近期针对NVFP4的训练改进。

Result: Quartet II在矩阵乘法中梯度估计更好，在端到端LLM训练验证，在NVIDIA Blackwell GPUs上有速度提升。

Conclusion: MS - EDEN和Quartet II有效改进NVFP4量化训练，提升训练效率和效果。

Abstract: The NVFP4 lower-precision format, supported in hardware by NVIDIA Blackwell GPUs, promises to allow, for the first time, end-to-end fully-quantized pre-training of massive models such as LLMs. Yet, existing quantized training methods still sacrifice some of the representation capacity of this format in favor of more accurate unbiased quantized gradient estimation by stochastic rounding (SR), losing noticeable accuracy relative to standard FP16 and FP8 training. In this paper, improve the state of the art for quantized training in NVFP4 via a novel unbiased quantization routine for micro-scaled formats, called MS-EDEN, that has more than 2x lower quantization error than SR. We integrate it into a novel fully-NVFP4 quantization scheme for linear layers, called Quartet II. We show analytically that Quartet II achieves consistently better gradient estimation across all major matrix multiplications, both on the forward and on the backward passes. In addition, our proposal synergizes well with recent training improvements aimed specifically at NVFP4. We further validate Quartet II on end-to-end LLM training with up to 1.9B parameters on 38B tokens. We provide kernels for execution on NVIDIA Blackwell GPUs with up to 4.2x speedup over BF16. Our code is available at https://github.com/IST-DASLab/Quartet-II .

</details>


### [208] [User-Adaptive Meta-Learning for Cold-Start Medication Recommendation with Uncertainty Filtering](https://arxiv.org/abs/2601.22820)
*Arya Hadizadeh Moghaddam,Mohsen Nayebi Kerdabadi,Dongjie Wang,Mei Liu,Zijun Yao*

Main category: cs.LG

TL;DR: 提出MetaDrug框架解决用药推荐中的患者冷启动问题，在两个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有用药推荐方法存在患者冷启动问题，且缺乏个性化推荐，元学习在电子病历中的应用待探索。

Method: 提出MetaDrug框架，含两级元适应机制（自我适应和同行适应），引入不确定性量化模块。

Result: 在MIMIC - III和AKI数据集上，MetaDrug在冷启动患者上始终优于现有方法。

Conclusion: MetaDrug能有效解决用药推荐中的患者冷启动问题。

Abstract: Large-scale Electronic Health Record (EHR) databases have become indispensable in supporting clinical decision-making through data-driven treatment recommendations. However, existing medication recommender methods often struggle with a user (i.e., patient) cold-start problem, where recommendations for new patients are usually unreliable due to the lack of sufficient prescription history for patient profiling. While prior studies have utilized medical knowledge graphs to connect medication concepts through pharmacological or chemical relationships, these methods primarily focus on mitigating the item cold-start issue and fall short in providing personalized recommendations that adapt to individual patient characteristics. Meta-learning has shown promise in handling new users with sparse interactions in recommender systems. However, its application to EHRs remains underexplored due to the unique sequential structure of EHR data. To tackle these challenges, we propose MetaDrug, a multi-level, uncertainty-aware meta-learning framework designed to address the patient cold-start problem in medication recommendation. MetaDrug proposes a novel two-level meta-adaptation mechanism, including self-adaptation, which adapts the model to new patients using their own medical events as support sets to capture temporal dependencies; and peer-adaptation, which adapts the model using similar visits from peer patients to enrich new patient representations. Meanwhile, to further improve meta-adaptation outcomes, we introduce an uncertainty quantification module that ranks the support visits and filters out the unrelated information for adaptation consistency. We evaluate our approach on the MIMIC-III and Acute Kidney Injury (AKI) datasets. Experimental results on both datasets demonstrate that MetaDrug consistently outperforms state-of-the-art medication recommendation methods on cold-start patients.

</details>


### [209] [Offline Reinforcement Learning of High-Quality Behaviors Under Robust Style Alignment](https://arxiv.org/abs/2601.22823)
*Mathieu Petitbois,Rémy Portelas,Sylvain Lamprier*

Main category: cs.LG

TL;DR: 研究通过子轨迹标记函数进行显式风格监督的离线风格条件策略强化学习，提出SCIQL方法，实验显示性能优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 现有风格强化学习方法难以调和风格与奖励冲突、分布偏移问题，无法有效结合风格与高任务性能。

Method: 提出统一行为风格定义并构建实用框架，引入Style - Conditioned Implicit Q - Learning (SCIQL)，利用离线目标条件RL技术和新的门控优势加权回归机制。

Result: 实验表明SCIQL在风格对齐和任务性能两个目标上均优于先前的离线方法。

Conclusion: SCIQL能有效解决现有风格强化学习中风格与任务性能结合的问题。

Abstract: We study offline reinforcement learning of style-conditioned policies using explicit style supervision via subtrajectory labeling functions. In this setting, aligning style with high task performance is particularly challenging due to distribution shift and inherent conflicts between style and reward. Existing methods, despite introducing numerous definitions of style, often fail to reconcile these objectives effectively. To address these challenges, we propose a unified definition of behavior style and instantiate it into a practical framework. Building on this, we introduce Style-Conditioned Implicit Q-Learning (SCIQL), which leverages offline goal-conditioned RL techniques, such as hindsight relabeling and value learning, and combine it with a new Gated Advantage Weighted Regression mechanism to efficiently optimize task performance while preserving style alignment. Experiments demonstrate that SCIQL achieves superior performance on both objectives compared to prior offline methods. Code, datasets and visuals are available in: https://sciql-iclr-2026.github.io/.

</details>


### [210] [Decomposing and Composing: Towards Efficient Vision-Language Continual Learning via Rank-1 Expert Pool in a Single LoRA](https://arxiv.org/abs/2601.22828)
*Zhan Fa,Yue Duan,Jian Zhang,Lei Qi,Wanqi Yang,Yinghuan Shi*

Main category: cs.LG

TL;DR: 提出新框架将LoRA模块重组为可分解的秩1专家池，结合激活引导正交损失，在视觉语言模型持续学习中减少参数更新、降低任务干扰，实验效果佳且计算轻量。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型持续学习面临任务适应和避免灾难性遗忘的挑战，现有方法存在推理负担重或依赖外部知识的问题，直接用LoRA缓解该问题并不容易。

Method: 引入新框架将单个LoRA模块重组为可分解的秩1专家池，通过[CLS]标记语义选择专家池动态组合稀疏、特定任务的更新；提出激活引导正交损失使LoRA权重关键部分跨任务正交。

Result: 在多个设置的广泛实验中各项指标达最优，超越零样本泛化上限，相比基线方法减少96.7%可训练参数，消除对外部数据集或任务ID判别器的依赖，合并的LoRA权重少且无推理延迟。

Conclusion: 该方法能实现领域感知学习，在减少任务干扰、保持下游任务性能的同时计算轻量。

Abstract: Continual learning (CL) in vision-language models (VLMs) faces significant challenges in improving task adaptation and avoiding catastrophic forgetting. Existing methods usually have heavy inference burden or rely on external knowledge, while Low-Rank Adaptation (LoRA) has shown potential in reducing these issues by enabling parameter-efficient tuning. However, considering directly using LoRA to alleviate the catastrophic forgetting problem is non-trivial, we introduce a novel framework that restructures a single LoRA module as a decomposable Rank-1 Expert Pool. Our method learns to dynamically compose a sparse, task-specific update by selecting from this expert pool, guided by the semantics of the [CLS] token. In addition, we propose an Activation-Guided Orthogonal (AGO) loss that orthogonalizes critical parts of LoRA weights across tasks. This sparse composition and orthogonalization enable fewer parameter updates, resulting in domain-aware learning while minimizing inter-task interference and maintaining downstream task performance. Extensive experiments across multiple settings demonstrate state-of-the-art results in all metrics, surpassing zero-shot upper bounds in generalization. Notably, it reduces trainable parameters by 96.7% compared to the baseline method, eliminating reliance on external datasets or task-ID discriminators. The merged LoRAs retain less weights and incur no inference latency, making our method computationally lightweight.

</details>


### [211] [Unconditional flow-based time series generation with equivariance-regularised latent spaces](https://arxiv.org/abs/2601.22848)
*Camilo Carvajal Reyes,Felipe Tobar*

Main category: cs.LG

TL;DR: 提出潜流匹配框架，通过正则化预训练自编码器鼓励等变性，在多数据集实验中表现优于现有基线，采样更快。


<details>
  <summary>Details</summary>
Motivation: 现有工作在为时间序列生成建模设计具有理想等变属性的潜在表示方面研究不足。

Method: 提出潜流匹配框架，引入等变损失，对预训练自编码器进行正则化，微调潜在空间。

Result: 等变正则化潜在空间提高生成质量，保留潜流模型计算优势，在多数据集实验中优于现有扩散基线，采样速度大幅提升。

Conclusion: 将几何归纳偏置纳入时间序列潜生成模型有实际益处。

Abstract: Flow-based models have proven successful for time-series generation, particularly when defined in lower-dimensional latent spaces that enable efficient sampling. However, how to design latent representations with desirable equivariance properties for time-series generative modelling remains underexplored. In this work, we propose a latent flow-matching framework in which equivariance is explicitly encouraged through a simple regularisation of a pre-trained autoencoder. Specifically, we introduce an equivariance loss that enforces consistency between transformed signals and their reconstructions, and use it to fine-tune latent spaces with respect to basic time-series transformations such as translation and amplitude scaling. We show that these equivariance-regularised latent spaces improve generation quality while preserving the computational advantages of latent flow models. Experiments on multiple real-world datasets demonstrate that our approach consistently outperforms existing diffusion-based baselines in standard time-series generation metrics, while achieving orders-of-magnitude faster sampling. These results highlight the practical benefits of incorporating geometric inductive biases into latent generative models for time series.

</details>


### [212] [Hierarchical Shift Mixing -- Beyond Dense Attention in Transformers](https://arxiv.org/abs/2601.22852)
*Robert Forchheimer*

Main category: cs.LG

TL;DR: 本文介绍Hierarchical Shift Mixing (HSM)框架，可实现线性时间复杂度，简单变体性能接近softmax注意力，混合架构能降低计算成本并提升表现。


<details>
  <summary>Details</summary>
Motivation: Transformer架构中基于softmax的注意力层计算复杂度高，此前替换方法多会降低性能。

Method: 引入Hierarchical Shift Mixing (HSM)框架，将成对token交互分布在不同Transformer层。

Result: 简单HSM变体性能接近softmax注意力，HSM与softmax注意力混合架构可超越GPT风格Transformer基线。

Conclusion: HSM能在降低训练和推理计算成本的同时，提升模型性能。

Abstract: Since the introduction of the Transformer architecture for large language models, the softmax-based attention layer has faced increasing scrutinity due to its quadratic-time computational complexity. Attempts have been made to replace it with less complex methods, at the cost of reduced performance in most cases. We introduce Hierarchical Shift Mixing (HSM), a general framework for token mixing that distributes pairwise token interactions across Transformer layers rather than computing them densely within each layer. HSM enables linear-time complexity while remaining agnostic to the specific mixing function. We show that even simple HSM variants achieve performance close to softmax attention, and that hybrid architectures combining HSM with softmax attention can outperform a GPT-style Transformer baseline while reducing computational cost during both training and inference.

</details>


### [213] [OptiMAG: Structure-Semantic Alignment via Unbalanced Optimal Transport](https://arxiv.org/abs/2601.22856)
*Yilong Zuo,Xunkai Li,Zhihan Zhang,Qiangqiang Dai,Ronghua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 本文指出多模态属性图（MAGs）现有方法存在结构 - 语义冲突问题，提出OptiMAG框架解决该问题，实验显示其在多任务上优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有多模态属性图方法在固定显式图结构上进行消息传递，会聚合不同特征，引入模态特定噪声，阻碍有效节点表示学习。

Method: 提出基于不平衡最优传输的正则化框架OptiMAG，用融合Gromov - Wasserstein距离引导局部邻域的跨模态结构一致性，用KL散度惩罚自适应处理跨模态不一致，可集成到现有多模态图模型中。

Result: OptiMAG在从图中心任务到多模态中心生成任务的多个任务中始终优于基线。

Conclusion: OptiMAG能有效缓解多模态属性图中的结构 - 语义冲突，提升模型性能。

Abstract: Multimodal Attributed Graphs (MAGs) have been widely adopted for modeling complex systems by integrating multi-modal information, such as text and images, on nodes. However, we identify a discrepancy between the implicit semantic structure induced by different modality embeddings and the explicit graph structure. For instance, neighbors in the explicit graph structure may be close in one modality but distant in another. Since existing methods typically perform message passing over the fixed explicit graph structure, they inadvertently aggregate dissimilar features, introducing modality-specific noise and impeding effective node representation learning. To address this, we propose OptiMAG, an Unbalanced Optimal Transport-based regularization framework. OptiMAG employs the Fused Gromov-Wasserstein distance to explicitly guide cross-modal structural consistency within local neighborhoods, effectively mitigating structural-semantic conflicts. Moreover, a KL divergence penalty enables adaptive handling of cross-modal inconsistencies. This framework can be seamlessly integrated into existing multimodal graph models, acting as an effective drop-in regularizer. Experiments demonstrate that OptiMAG consistently outperforms baselines across multiple tasks, ranging from graph-centric tasks (e.g., node classification, link prediction) to multimodal-centric generation tasks (e.g., graph2text, graph2image). The source code will be available upon acceptance.

</details>


### [214] [Matterhorn: Efficient Analog Sparse Spiking Transformer Architecture with Masked Time-To-First-Spike Encoding](https://arxiv.org/abs/2601.22876)
*Zhanglu Yan,Kaiwen Tang,Zixuan Zhu,Zhenyu Bai,Qianhui Liu,Weng-Fai Wong*

Main category: cs.LG

TL;DR: 本文提出Spiking Transformer Matterhorn，结合M - TTFS编码方法和MSU，在GLUE基准测试中取得新的最优结果，提升了准确率和能效。


<details>
  <summary>Details</summary>
Motivation: 当前SNN能量评估主要关注累加操作，未考虑数据移动等实际硬件成本，而数据移动能耗占比近80%。

Method: 提出M - TTFS编码方法减少尖峰移动，采用‘死区’策略最大化稀疏性；提出MSU利用存内计算技术消除权重访问开销。

Result: 在GLUE基准测试中，Matterhorn平均准确率超过现有SNN 1.42%，能效提升2.31倍。

Conclusion: Matterhorn在SNN的LLM推理中表现出色，可实现高效节能。

Abstract: Spiking neural networks (SNNs) have emerged as a promising candidate for energy-efficient LLM inference. However, current energy evaluations for SNNs primarily focus on counting accumulate operations, and fail to account for real-world hardware costs such as data movement, which can consume nearly 80% of the total energy. In this paper, we propose Matterhorn, a spiking transformer that integrates a novel masked time-to-first-spike (M-TTFS) encoding method to reduce spike movement and a memristive synapse unit (MSU) to eliminate weight access overhead. M-TTFS employs a masking strategy that reassigns the zero-energy silent state (a spike train of all 0s) to the most frequent membrane potential rather than the lowest. This aligns the coding scheme with the data distribution, minimizing spike movement energy without information loss. We further propose a `dead zone' strategy that maximizes sparsity by mapping all values within a given range to the silent state. At the hardware level, the MSU utilizes compute-in-memory (CIM) technology to perform analog integration directly within memory, effectively removing weight access costs. On the GLUE benchmark, Matterhorn establishes a new state-of-the-art, surpassing existing SNNs by 1.42% in average accuracy while delivering a 2.31 times improvement in energy efficiency.

</details>


### [215] [Synthetic Time Series Generation via Complex Networks](https://arxiv.org/abs/2601.22879)
*Jaime Vale,Vanessa Freitas Silva,Maria Eduarda Silva,Fernando Silva*

Main category: cs.LG

TL;DR: 本文提出利用复杂网络映射生成合成时间序列的框架，评估其生成数据的质量，并与GAN方法对比，结果显示该方法具有竞争力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 因隐私问题、获取成本和标注挑战等导致高质量时间序列数据集获取受限，合成时间序列生成是解决这一问题的有前景方案。

Method: 利用时间序列到分位数图（QG）的转换及逆映射生成合成时间序列，用模拟和真实数据集评估生成数据，并与GAN方法对比。

Result: 基于分位数图的方法在合成时间序列生成方面具有竞争力，可作为一种替代方案。

Conclusion: 提出的基于分位数图的方法是生成合成时间序列有竞争力且可解释的替代方案。

Abstract: Time series data are essential for a wide range of applications, particularly in developing robust machine learning models. However, access to high-quality datasets is often limited due to privacy concerns, acquisition costs, and labeling challenges. Synthetic time series generation has emerged as a promising solution to address these constraints. In this work, we present a framework for generating synthetic time series by leveraging complex networks mappings. Specifically, we investigate whether time series transformed into Quantile Graphs (QG) -- and then reconstructed via inverse mapping -- can produce synthetic data that preserve the statistical and structural properties of the original. We evaluate the fidelity and utility of the generated data using both simulated and real-world datasets, and compare our approach against state-of-the-art Generative Adversarial Network (GAN) methods. Results indicate that our quantile graph-based methodology offers a competitive and interpretable alternative for synthetic time series generation.

</details>


### [216] [MoVE: Mixture of Value Embeddings -- A New Axis for Scaling Parametric Memory in Autoregressive Models](https://arxiv.org/abs/2601.22887)
*Yangyan Li*

Main category: cs.LG

TL;DR: 本文介绍了MoVE机制，可打破自回归序列建模中模型容量与计算成本的耦合，独立扩展参数内存，在文本和图像生成任务上验证有效。


<details>
  <summary>Details</summary>
Motivation: 自回归序列建模中模型容量与计算成本存在刚性耦合，扩展模型参数内存会导致计算量成比例增加，需要打破这种耦合。

Method: 提出MoVE机制，引入全局可学习值嵌入库，通过可微软门控机制动态混合检索概念到标准值投影中，可独立于网络深度扩展参数内存。

Result: 在文本和图像生成两个自回归建模代表性应用中，MoVE相比标准和分层内存基线有一致性能提升，能构建“内存密集”模型，在相同计算预算下实现更低困惑度和更高保真度。

Conclusion: MoVE机制有效打破了自回归序列建模中模型容量与计算成本的耦合，为扩展模型容量提供了新途径。

Abstract: Autoregressive sequence modeling stands as the cornerstone of modern Generative AI, powering results across diverse modalities ranging from text generation to image generation. However, a fundamental limitation of this paradigm is the rigid structural coupling of model capacity to computational cost: expanding a model's parametric memory -- its repository of factual knowledge or visual patterns -- traditionally requires deepening or widening the network, which incurs a proportional rise in active FLOPs. In this work, we introduce $\textbf{MoVE (Mixture of Value Embeddings)}$, a mechanism that breaks this coupling and establishes a new axis for scaling capacity. MoVE decouples memory from compute by introducing a global bank of learnable value embeddings shared across all attention layers. For every step in the sequence, the model employs a differentiable soft gating mechanism to dynamically mix retrieved concepts from this bank into the standard value projection. This architecture allows parametric memory to be scaled independently of network depth by simply increasing the number of embedding slots. We validate MoVE through strictly controlled experiments on two representative applications of autoregressive modeling: Text Generation and Image Generation. In both domains, MoVE yields consistent performance improvements over standard and layer-wise memory baselines, enabling the construction of "memory-dense" models that achieve lower perplexity and higher fidelity than their dense counterparts at comparable compute budgets.

</details>


### [217] [PlatoLTL: Learning to Generalize Across Symbols in LTL Instructions for Multi-Task RL](https://arxiv.org/abs/2601.22891)
*Jacques Cloete,Mathias Jackermeier,Ioannis Havoutis,Alessandro Abate*

Main category: cs.LG

TL;DR: 提出PlatoLTL方法，使策略在多任务强化学习中对LTL公式结构和命题实现零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 现有LTL引导的多任务强化学习方法无法在未见的命题词汇上泛化，需要解决策略对未见过任务的泛化问题。

Method: 将命题视为参数化谓词的实例，提出新架构嵌入和组合谓词来表示LTL规范。

Result: 在具有挑战性的环境中成功实现对新命题和任务的零样本泛化。

Conclusion: PlatoLTL方法能有效解决多任务强化学习中策略的泛化问题，尤其是对未见命题的泛化。

Abstract: A central challenge in multi-task reinforcement learning (RL) is to train generalist policies capable of performing tasks not seen during training. To facilitate such generalization, linear temporal logic (LTL) has recently emerged as a powerful formalism for specifying structured, temporally extended tasks to RL agents. While existing approaches to LTL-guided multi-task RL demonstrate successful generalization across LTL specifications, they are unable to generalize to unseen vocabularies of propositions (or "symbols"), which describe high-level events in LTL. We present PlatoLTL, a novel approach that enables policies to zero-shot generalize not only compositionally across LTL formula structures, but also parametrically across propositions. We achieve this by treating propositions as instances of parameterized predicates rather than discrete symbols, allowing policies to learn shared structure across related propositions. We propose a novel architecture that embeds and composes predicates to represent LTL specifications, and demonstrate successful zero-shot generalization to novel propositions and tasks across challenging environments.

</details>


### [218] [Calibrated Multivariate Distributional Regression with Pre-Rank Regularization](https://arxiv.org/abs/2601.22895)
*Aya Laajil,Elnura Zhalieva,Naomi Desobry,Souhaib Ben Taieb*

Main category: cs.LG

TL;DR: 提出基于正则化的校准方法，利用预排序函数在多变量分布回归模型训练中实现多变量校准，实验表明该方法能提升校准效果且不影响预测精度。


<details>
  <summary>Details</summary>
Motivation: 多变量校准具有挑战性，现有预排序函数多用于事后评估，需在训练中实现多变量校准。

Method: 提出基于正则化的校准方法，使用预排序函数；引入基于PCA的预排序。

Result: 通过模拟研究和18个真实数据集实验，该方法大幅提升多变量预排序校准，不影响预测精度，PCA预排序能发现现有预排序未检测到的依赖结构错误。

Conclusion: 提出的方法能有效提升多变量分布回归模型的校准效果。

Abstract: The goal of probabilistic prediction is to issue predictive distributions that are as informative as possible, subject to being calibrated. Despite substantial progress in the univariate setting, achieving multivariate calibration remains challenging. Recent work has introduced pre-rank functions, scalar projections of multivariate forecasts and observations, as flexible diagnostics for assessing specific aspects of multivariate calibration, but their use has largely been limited to post-hoc evaluation. We propose a regularization-based calibration method that enforces multivariate calibration during training of multivariate distributional regression models using pre-rank functions. We further introduce a novel PCA-based pre-rank that projects predictions onto principal directions of the predictive distribution. Through simulation studies and experiments on 18 real-world multi-output regression datasets, we show that the proposed approach substantially improves multivariate pre-rank calibration without compromising predictive accuracy, and that the PCA pre-rank reveals dependence-structure misspecifications that are not detected by existing pre-ranks.

</details>


### [219] [Uncertainty-Aware Extrapolation in Bayesian Oblique Trees](https://arxiv.org/abs/2601.22899)
*Viktor Andonovikj,Sašo Džeroski,Pavle Boškoski*

Main category: cs.LG

TL;DR: 提出单树贝叶斯模型，结合贝叶斯斜分裂和高斯过程预测器，在回归任务中提高预测表现和外推能力。


<details>
  <summary>Details</summary>
Motivation: 决策树在需要可靠外推和校准不确定性的回归任务中存在不足，分段常数叶预测有局限性。

Method: 提出单个树贝叶斯模型，结合贝叶斯斜分裂和高斯过程预测器，设计有效推理和预测方案及门控机制。

Result: 基准回归任务实验显示，相比标准变分斜树，预测性能提升；外推场景有显著性能增益。

Conclusion: 该单树贝叶斯模型在回归任务及外推场景表现良好，能解决决策树的部分问题。

Abstract: Decision trees are widely used due to their interpretability and efficiency, but they struggle in regression tasks that require reliable extrapolation and well-calibrated uncertainty. Piecewise-constant leaf predictions are bounded by the training targets and often become overconfident under distribution shift. We propose a single-tree Bayesian model that extends VSPYCT by equipping each leaf with a GP predictor. Bayesian oblique splits provide uncertainty-aware partitioning of the input space, while GP leaves model local functional behaviour and enable principled extrapolation beyond the observed target range. We present an efficient inference and prediction scheme that combines posterior sampling of split parameters with \gls{gp} posterior predictions, and a gating mechanism that activates GP-based extrapolation when inputs fall outside the training support of a leaf. Experiments on benchmark regression tasks show improvements in the predictive performance compared to standard variational oblique trees, and substantial performance gains in extrapolation scenarios.

</details>


### [220] [FlexLoRA: Entropy-Guided Flexible Low-Rank Adaptation](https://arxiv.org/abs/2601.22905)
*Muqing Liu,Chongjie Si,Yuheng Jia*

Main category: cs.LG

TL;DR: 大预训练模型全量微调成本高，现有低秩适配方法有局限，提出FlexLoRA框架，实验表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有大模型参数高效微调方法如LoRA固定秩设计缺乏灵活性，动态秩分配方法有粒度和容量扩展等问题。

Method: 提出FlexLoRA框架，通过谱能量熵评估矩阵重要性，支持全局预算下的秩剪枝和扩展，对新增奇异方向采用零影响初始化。

Result: 广泛实验表明FlexLoRA在各基准测试中始终优于现有基线。

Conclusion: FlexLoRA解决了粒度、灵活性和稳定性方面的局限，为参数高效微调提供了更有原则的解决方案。

Abstract: Large pre-trained models achieve remarkable success across diverse domains, yet fully fine-tuning incurs prohibitive computational and memory costs. Parameter-efficient fine-tuning (PEFT) has thus become a mainstream paradigm. Among them, Low-Rank Adaptation (LoRA) introduces trainable low-rank matrices and shows strong performance, nevertheless, its fixed-rank design limits flexibility. Dynamic rank allocation methods mitigate this issue by pruning redundant directions; however, they often rely on heuristic, element-level metrics that globally sort rank directions without matrix-wise distinction, and they lack mechanisms to expand capacity in layers requiring additional adaptation. To overcome these limitations, we propose FlexLoRA, an entropy-guided flexible low-rank adaptation framework that (i) evaluates matrix importance via spectral energy entropy, (ii) supports rank pruning and expansion under a global budget, and (iii) employs zero-impact initialization for newly added singular directions to ensure stability. By addressing granularity, flexibility, and stability limitations, FlexLoRA provides a more principled solution for PEFT. Extensive experiments show that FlexLoRA consistently outperforms state-of-the-art baselines across benchmarks. Codes are available at https://github.com/Chongjie-Si/Subspace-Tuning.

</details>


### [221] [DC-LA: Difference-of-Convex Langevin Algorithm](https://arxiv.org/abs/2601.22932)
*Hoang Phuc Hau Luu,Zhongjian Wang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study a sampling problem whose target distribution is $π\propto \exp(-f-r)$ where the data fidelity term $f$ is Lipschitz smooth while the regularizer term $r=r_1-r_2$ is a non-smooth difference-of-convex (DC) function, i.e., $r_1,r_2$ are convex. By leveraging the DC structure of $r$, we can smooth out $r$ by applying Moreau envelopes to $r_1$ and $r_2$ separately. In line of DC programming, we then redistribute the concave part of the regularizer to the data fidelity and study its corresponding proximal Langevin algorithm (termed DC-LA). We establish convergence of DC-LA to the target distribution $π$, up to discretization and smoothing errors, in the $q$-Wasserstein distance for all $q \in \mathbb{N}^*$, under the assumption that $V$ is distant dissipative. Our results improve previous work on non-log-concave sampling in terms of a more general framework and assumptions. Numerical experiments show that DC-LA produces accurate distributions in synthetic settings and reliably provides uncertainty quantification in a real-world Computed Tomography application.

</details>


### [222] [Scalable Topology-Preserving Graph Coarsening with Graph Collapse](https://arxiv.org/abs/2601.22943)
*Xiang Wu,Rong-Hua Li,Xunkai Li,Kangfei Zhao,Hongchao Qin,Guoren Wang*

Main category: cs.LG

TL;DR: 提出可扩展的拓扑保持图粗化方法STPGC，含新算法，证明其保留GNN感受野，实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有图粗化方法多只保留谱或空间特征，保留拓扑特征的方法时间复杂度高。

Method: 引入图强坍缩和图边坍缩概念，提出GStrongCollapse、GEdgeCollapse和NeighborhoodConing三个新算法。

Result: 证明STPGC保留GNN感受野，开发近似算法加速GNN训练。

Conclusion: 节点分类实验表明STPGC有效且高效。

Abstract: Graph coarsening reduces the size of a graph while preserving certain properties. Most existing methods preserve either spectral or spatial characteristics. Recent research has shown that preserving topological features helps maintain the predictive performance of graph neural networks (GNNs) trained on the coarsened graph but suffers from exponential time complexity. To address these problems, we propose Scalable Topology-Preserving Graph Coarsening (STPGC) by introducing the concepts of graph strong collapse and graph edge collapse extended from algebraic topology. STPGC comprises three new algorithms, GStrongCollapse, GEdgeCollapse, and NeighborhoodConing based on these two concepts, which eliminate dominated nodes and edges while rigorously preserving topological features. We further prove that STPGC preserves the GNN receptive field and develop approximate algorithms to accelerate GNN training. Experiments on node classification with GNNs demonstrate the efficiency and effectiveness of STPGC.

</details>


### [223] [Environment-Conditioned Tail Reweighting for Total Variation Invariant Risk Minimization](https://arxiv.org/abs/2601.22944)
*Wang Yuanchao,Lai Zhao-Rong,Zhong Tianqi,Li Fengnan*

Main category: cs.LG

TL;DR: 提出ECTR框架解决模型在分布外泛化中同时面临的环境相关偏移和稀有样本多样性偏移问题，实验显示性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有不变风险最小化方法主要处理环境层面虚假关联，忽视环境内样本异质性，影响分布外性能。

Method: 提出ECTR框架，结合基于总变差的不变学习与环境条件尾部重加权，还将框架扩展到无显式环境注释场景。

Result: 在回归、表格、时间序列和图像分类基准实验中，最差环境和平均分布外性能均有持续提升。

Conclusion: ECTR框架能有效应对混合分布偏移，使环境层面不变性和环境内鲁棒性机制互补。

Abstract: Out-of-distribution (OOD) generalization remains challenging when models simultaneously encounter correlation shifts across environments and diversity shifts driven by rare or hard samples. Existing invariant risk minimization (IRM) methods primarily address spurious correlations at the environment level, but often overlook sample-level heterogeneity within environments, which can critically impact OOD performance. In this work, we propose \emph{Environment-Conditioned Tail Reweighting for Total Variation Invariant Risk Minimization} (ECTR), a unified framework that augments TV-based invariant learning with environment-conditioned tail reweighting to jointly address both types of distribution shift. By integrating environment-level invariance with within-environment robustness, the proposed approach makes these two mechanisms complementary under mixed distribution shifts. We further extend the framework to scenarios without explicit environment annotations by inferring latent environments through a minimax formulation. Experiments across regression, tabular, time-series, and image classification benchmarks under mixed distribution shifts demonstrate consistent improvements in both worst-environment and average OOD performance.

</details>


### [224] [Improved Algorithms for Nash Welfare in Linear Bandits](https://arxiv.org/abs/2601.22969)
*Dhruv Sarkar,Nishant Pandey,Sayak Ray Chowdhury*

Main category: cs.LG

TL;DR: 本文引入新分析工具解决线性多臂老虎机中Nash regret界的最优性问题，提出FairLinBandit框架研究p - means regret，实验表明方法优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有线性多臂老虎机中基于Nash Social Welfare目标的Nash regret结果在环境维度d上存在次优性。

Method: 引入新分析工具，提出FairLinBandit通用算法框架，基于Phased Elimination和Upper Confidence Bound两种算法实现。

Result: 得到线性多臂老虎机中最优的Nash regret界，证明两种实例化算法在整个p范围内实现次线性p - means regret，实验显示方法优于现有基线。

Conclusion: 新方法有效解决了Nash regret界的次优问题，且在p - means regret研究上有良好表现。

Abstract: Nash regret has recently emerged as a principled fairness-aware performance metric for stochastic multi-armed bandits, motivated by the Nash Social Welfare objective. Although this notion has been extended to linear bandits, existing results suffer from suboptimality in ambient dimension $d$, stemming from proof techniques that rely on restrictive concentration inequalities. In this work, we resolve this open problem by introducing new analytical tools that yield an order-optimal Nash regret bound in linear bandits. Beyond Nash regret, we initiate the study of $p$-means regret in linear bandits, a unifying framework that interpolates between fairness and utility objectives and strictly generalizes Nash regret. We propose a generic algorithmic framework, FairLinBandit, that works as a meta-algorithm on top of any linear bandit strategy. We instantiate this framework using two bandit algorithms: Phased Elimination and Upper Confidence Bound, and prove that both achieve sublinear $p$-means regret for the entire range of $p$. Extensive experiments on linear bandit instances generated from real-world datasets demonstrate that our methods consistently outperform the existing state-of-the-art baseline.

</details>


### [225] [Stabilizing the Q-Gradient Field for Policy Smoothness in Actor-Critic](https://arxiv.org/abs/2601.22970)
*Jeong Woon Lee,Kyoleen Kwak,Daeho Kim,Hyoseok Hwang*

Main category: cs.LG

TL;DR: 连续演员-评论家方法学习的策略有高频振荡问题，本文理论分析了策略不平稳的根源并提出PAVE框架，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 解决连续演员-评论家方法学习的策略存在高频振荡，不适合物理部署的问题，且指出当前方法治标不治本。

Method: 理论上建立策略不平稳性与评论家的微分几何的关系，证明最优策略的敏感性界限；提出以评论家为中心的正则化框架PAVE来稳定动作梯度场。

Result: PAVE能达到与策略侧平滑正则化方法相当的平滑性和鲁棒性，且保持有竞争力的任务性能，无需修改演员。

Conclusion: PAVE可有效解决策略不平稳问题，且在性能上有不错表现。

Abstract: Policies learned via continuous actor-critic methods often exhibit erratic, high-frequency oscillations, making them unsuitable for physical deployment. Current approaches attempt to enforce smoothness by directly regularizing the policy's output. We argue that this approach treats the symptom rather than the cause. In this work, we theoretically establish that policy non-smoothness is fundamentally governed by the differential geometry of the critic. By applying implicit differentiation to the actor-critic objective, we prove that the sensitivity of the optimal policy is bounded by the ratio of the Q-function's mixed-partial derivative (noise sensitivity) to its action-space curvature (signal distinctness). To empirically validate this theoretical insight, we introduce PAVE (Policy-Aware Value-field Equalization), a critic-centric regularization framework that treats the critic as a scalar field and stabilizes its induced action-gradient field. PAVE rectifies the learning signal by minimizing the Q-gradient volatility while preserving local curvature. Experimental results demonstrate that PAVE achieves smoothness and robustness comparable to policy-side smoothness regularization methods, while maintaining competitive task performance, without modifying the actor.

</details>


### [226] [Learnable Permutation for Structured Sparsity on Transformer Models](https://arxiv.org/abs/2601.22980)
*Zekai Li,Ji Liu,Guanchen Li,Yixing Xu,Ziqiong Liu,Xuanwu Yin,Dong Li,Emad Barsoum*

Main category: cs.LG

TL;DR: 提出可学习排列框架用于结构化剪枝，在视觉和语言Transformer上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有基于权重排列提升剪枝后性能的方法因排列搜索空间指数增长，多依赖贪心或启发式算法，限制了重排序效果。

Method: 提出端到端可学习排列框架，引入可学习排列成本矩阵、可微二分匹配求解器和稀疏优化损失函数。

Result: 在视觉和语言Transformer上广泛验证，实现结构化稀疏排列的SOTA结果。

Conclusion: 所提可学习排列框架有效提升结构化剪枝性能。

Abstract: Structured sparsity has emerged as a popular model pruning technique, widely adopted in various architectures, including CNNs, Transformer models, and especially large language models (LLMs) in recent years. A promising direction to further improve post-pruning performance is weight permutation, which reorders model weights into patterns more amenable to pruning. However, the exponential growth of the permutation search space with the scale of Transformer architectures forces most methods to rely on greedy or heuristic algorithms, limiting the effectiveness of reordering.
  In this work, we propose a novel end-to-end learnable permutation framework. Our method introduces a learnable permutation cost matrix to quantify the cost of swapping any two input channels of a given weight matrix, a differentiable bipartite matching solver to obtain the optimal binary permutation matrix given a cost matrix, and a sparsity optimization loss function to directly optimize the permutation operator. We extensively validate our approach on vision and language Transformers, demonstrating that our method achieves state-of-the-art permutation results for structured sparsity.

</details>


### [227] [dgMARK: Decoding-Guided Watermarking for Diffusion Language Models](https://arxiv.org/abs/2601.22985)
*Pyo Min Hong,Albert No*

Main category: cs.LG

TL;DR: 提出用于离散扩散语言模型的解码引导水印方法dgMARK。


<details>
  <summary>Details</summary>
Motivation: 离散扩散语言模型（dLLMs）生成代币顺序任意，实际模型对解封顺序敏感，可开辟新的水印通道。

Method: 引导解封顺序使高回报候选代币满足由二进制哈希诱导的简单奇偶约束，该方法可与常见解码策略搭配，有单步前瞻变体，通过奇偶匹配统计检测水印，用滑动窗口检测器确保对编辑操作的鲁棒性。

Result: 文档未提及具体结果。

Conclusion: 文档未提及明确结论。

Abstract: We propose dgMARK, a decoding-guided watermarking method for discrete diffusion language models (dLLMs). Unlike autoregressive models, dLLMs can generate tokens in arbitrary order. While an ideal conditional predictor would be invariant to this order, practical dLLMs exhibit strong sensitivity to the unmasking order, creating a new channel for watermarking. dgMARK steers the unmasking order toward positions whose high-reward candidate tokens satisfy a simple parity constraint induced by a binary hash, without explicitly reweighting the model's learned probabilities. The method is plug-and-play with common decoding strategies (e.g., confidence, entropy, and margin-based ordering) and can be strengthened with a one-step lookahead variant. Watermarks are detected via elevated parity-matching statistics, and a sliding-window detector ensures robustness under post-editing operations including insertion, deletion, substitution, and paraphrasing.

</details>


### [228] [Mano: Restriking Manifold Optimization for LLM Training](https://arxiv.org/abs/2601.23000)
*Yufei Gu,Zeke Xie*

Main category: cs.LG

TL;DR: 研究提出新型优化器Mano用于训练大语言模型，能解决现有优化器的局限，实验表明其性能优于AdamW和Muon，且时空效率更高。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型训练的硬件和计算成本高，AdamW和Muon等优化器存在局限，传统流形优化方法在大规模模型优化中表现不佳。

Method: 将动量投影到模型参数的切空间，并将其约束在旋转斜流形上，提出优化器Mano。

Result: 在LLaMA和Qwen3模型上的实验显示，Mano始终显著优于AdamW和Muon，且内存消耗和计算复杂度更低。

Conclusion: Mano在时空效率方面拓展了帕累托边界。

Abstract: While large language models (LLMs) have emerged as a significant advancement in artificial intelligence, the hardware and computational costs for training LLMs are also significantly burdensome. Among the state-of-the-art optimizers, AdamW relies on diagonal curvature estimates and ignores structural properties, while Muon applies global spectral normalization at the expense of losing curvature information. In this study, we restriked manifold optimization methods for training LLMs, which may address both optimizers' limitations, while conventional manifold optimization methods have been largely overlooked due to the poor performance in large-scale model optimization. By innovatively projecting the momentum onto the tangent space of model parameters and constraining it on a rotational Oblique manifold, we propose a novel, powerful, and efficient optimizer **Mano** that is the first to bridge the performance gap between manifold optimization and modern optimizers. Extensive experiments on the LLaMA and Qwen3 models demonstrate that Mano consistently and significantly outperforms AdamW and Muon even with less memory consumption and computational complexity, respectively, suggesting an expanded Pareto frontier in terms of space and time efficiency.

</details>


### [229] [Automatic Constraint Policy Optimization based on Continuous Constraint Interpolation Framework for Offline Reinforcement Learning](https://arxiv.org/abs/2601.23010)
*Xinchen Han,Qiuyang Fang,Hossam Afifi,Michel Marot*

Main category: cs.LG

TL;DR: 提出连续约束插值（CCI）统一优化框架和自动约束策略优化（ACPO）算法，实验取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有离线强化学习方法多采用单一约束族，缺乏统一原则解释联系和权衡。

Method: 提出CCI框架，其包含单一插值参数可平滑过渡和组合约束类型；基于CCI开发ACPO算法，通过拉格朗日对偶更新调整插值参数；建立最大熵性能差异引理并推导性能下界。

Result: 在D4RL和NeoRL2实验中，各领域有稳健提升，总体达最优性能。

Conclusion: CCI框架和ACPO算法有效，能解决现有离线强化学习方法缺乏统一原则的问题。

Abstract: Offline Reinforcement Learning (RL) relies on policy constraints to mitigate extrapolation error, where both the constraint form and constraint strength critically shape performance. However, most existing methods commit to a single constraint family: weighted behavior cloning, density regularization, or support constraints, without a unified principle that explains their connections or trade-offs. In this work, we propose Continuous Constraint Interpolation (CCI), a unified optimization framework in which these three constraint families arise as special cases along a common constraint spectrum. The CCI framework introduces a single interpolation parameter that enables smooth transitions and principled combinations across constraint types. Building on CCI, we develop Automatic Constraint Policy Optimization (ACPO), a practical primal--dual algorithm that adapts the interpolation parameter via a Lagrangian dual update. Moreover, we establish a maximum-entropy performance difference lemma and derive performance lower bounds for both the closed-form optimal policy and its parametric projection. Experiments on D4RL and NeoRL2 demonstrate robust gains across diverse domains, achieving state-of-the-art performance overall.

</details>


### [230] [Leveraging Convolutional Sparse Autoencoders for Robust Movement Classification from Low-Density sEMG](https://arxiv.org/abs/2601.23011)
*Blagoj Hristov,Zoran Hadzi-Velkov,Katerina Hadzi-Velkova Saneva,Gorjan Nadzinski,Vesna Ojleska Latkoska*

Main category: cs.LG

TL;DR: 提出仅用两个表面肌电通道的深度学习框架用于手势识别，结合特征提取、迁移学习和增量学习，实现高精度且可扩展。


<details>
  <summary>Details</summary>
Motivation: 解决肌电假肢可靠控制中受试者间差异大以及高密度传感器阵列临床不实用的问题。

Method: 采用卷积稀疏自动编码器（CSAE）从原始信号中直接提取时间特征表示，提出少样本迁移学习协议和增量学习策略。

Result: 在6类手势集上多受试者F1分数达94.3% ± 0.3%；少样本迁移学习使未知受试者表现从35.1% ± 3.1%提升到92.3% ± 0.9%；增量学习扩展到10类集F1分数达90.0% ± 0.2%。

Conclusion: 该框架结合高精度与低计算和传感器开销，为下一代经济适用且自适应的假肢系统提供可扩展高效的方法。

Abstract: Reliable control of myoelectric prostheses is often hindered by high inter-subject variability and the clinical impracticality of high-density sensor arrays. This study proposes a deep learning framework for accurate gesture recognition using only two surface electromyography (sEMG) channels. The method employs a Convolutional Sparse Autoencoder (CSAE) to extract temporal feature representations directly from raw signals, eliminating the need for heuristic feature engineering. On a 6-class gesture set, our model achieved a multi-subject F1-score of 94.3% $\pm$ 0.3%. To address subject-specific differences, we present a few-shot transfer learning protocol that improved performance on unseen subjects from a baseline of 35.1% $\pm$ 3.1% to 92.3% $\pm$ 0.9% with minimal calibration data. Furthermore, the system supports functional extensibility through an incremental learning strategy, allowing for expansion to a 10-class set with a 90.0% $\pm$ 0.2% F1-score without full model retraining. By combining high precision with minimal computational and sensor overhead, this framework provides a scalable and efficient approach for the next generation of affordable and adaptive prosthetic systems.

</details>


### [231] [Mem-T: Densifying Rewards for Long-Horizon Memory Agents](https://arxiv.org/abs/2601.23014)
*Yanwei Yue,Guibin Zhang,Boci Peng,Xuanbo Fan,Jiaxin Guo,Qiankun Li,Yan Zhang*

Main category: cs.LG

TL;DR: 本文介绍自主记忆代理Mem - T及训练框架MoT - GRPO，实验显示Mem - T性能优且经济。


<details>
  <summary>Details</summary>
Motivation: 现有记忆代理训练范式存在局限，代理在长序列记忆操作后获稀疏延迟奖励，阻碍记忆管理策略端到端优化。

Method: 引入Mem - T与轻量级分层内存数据库交互；提出MoT - GRPO框架，通过记忆操作树反向传播和后视信用分配，将稀疏反馈转化为密集监督。

Result: 实验表明Mem - T性能高，超越A - Mem和Mem0达14.92%；经济，在准确性 - 效率前沿表现好，每查询推理令牌减少约24.45%。

Conclusion: Mem - T结合MoT - GRPO能有效提升记忆管理性能，实现记忆构建和检索的联合优化。

Abstract: Memory agents, which depart from predefined memory-processing pipelines by endogenously managing the processing, storage, and retrieval of memories, have garnered increasing attention for their autonomy and adaptability. However, existing training paradigms remain constrained: agents often traverse long-horizon sequences of memory operations before receiving sparse and delayed rewards, which hinders truly end-to-end optimization of memory management policies. To address this limitation, we introduce Mem-T, an autonomous memory agent that interfaces with a lightweight hierarchical memory database to perform dynamic updates and multi-turn retrieval over streaming inputs. To effectively train long-horizon memory management capabilities, we further propose MoT-GRPO, a tree-guided reinforcement learning framework that transforms sparse terminal feedback into dense, step-wise supervision via memory operation tree backpropagation and hindsight credit assignment, thereby enabling the joint optimization of memory construction and retrieval. Extensive experiments demonstrate that Mem-T is (1) high-performing, surpassing frameworks such as A-Mem and Mem0 by up to $14.92\%$, and (2) economical, operating on a favorable accuracy-efficiency Pareto frontier and reducing inference tokens per query by $\sim24.45\%$ relative to GAM without sacrificing performance.

</details>


### [232] [Causal Characterization of Measurement and Mechanistic Anomalies](https://arxiv.org/abs/2601.23026)
*Hendrik Suhr,David Kaltenpoth,Jilles Vreeken*

Main category: cs.LG

TL;DR: 现有异常根源分析方法忽略异常产生的两种不同过程，本文定义因果模型，提出最大似然估计方法，实验表明方法性能好且能准确分类异常类型。


<details>
  <summary>Details</summary>
Motivation: 现有异常根源分析方法未考虑异常产生的两种不同过程（测量误差和机制转移），需要新方法解决。

Method: 定义因果模型，将异常值视为对潜在和观测变量的潜在干预，使用最大似然估计方法。

Result: 方法在根源定位上达到了最先进水平，能准确分类异常类型，在因果有向无环图未知时仍保持稳健。

Conclusion: 所提方法有效，可用于异常根源分析和类型分类。

Abstract: Root cause analysis of anomalies aims to identify those features that cause the deviation from the normal process. Existing methods ignore, however, that anomalies can arise through two fundamentally different processes: measurement errors, where data was generated normally but one or more values were recorded incorrectly, and mechanism shifts, where the causal process generating the data changed. While measurement errors can often be safely corrected, mechanistic anomalies require careful consideration. We define a causal model that explicitly captures both types by treating outliers as latent interventions on latent ("true") and observed ("measured") variables. We show that they are identifiable, and propose a maximum likelihood estimation approach to put this to practice. Experiments show that our method matches state-of-the-art performance in root cause localization, while it additionally enables accurate classification of anomaly types, and remains robust even when the causal DAG is unknown.

</details>


### [233] [Divide-and-Conquer CoT: RL for Reducing Latency via Parallel Reasoning](https://arxiv.org/abs/2601.23027)
*Arvind Mahankali,Kaiyue Wen,Tengyu Ma*

Main category: cs.LG

TL;DR: 提出Divide-and-Conquer CoT (DC-CoT) 方法降低长思维链推理延迟，在多基准测试中实现低延迟和相似准确率。


<details>
  <summary>Details</summary>
Motivation: 长思维链推理导致大语言模型生成延迟高，需降低延迟。

Method: 从长思维链基础模型开始，先用小示范集进行监督微调初始化能力，再设计多阶段强化学习算法结合数据过滤策略恢复准确率并降低最长路径长度。

Result: 在多个基准测试中，DC-CoT 达到与基础模型相似准确率，最长路径长度降低 35 - 40%。

Conclusion: DC-CoT 可有效降低长思维链推理的延迟，且代码、数据集和模型已公开。

Abstract: Long chain-of-thought reasoning (Long CoT) is now fundamental to state-of-the-art LLMs, especially in mathematical reasoning. However, LLM generation is highly sequential, and long CoTs lead to a high latency. We propose to train Divide-and-Conquer CoT (DC-CoT) to reduce the latency. With DC-CoT, the model can act as a director that identifies distinct subtasks that can be performed in parallel in its reasoning process, and then spawns workers to execute the subtasks. Our goal is to achieve high accuracy, with a low longest path length, which is a theoretical measure of the latency needed for the response. We start with a long CoT base model (DeepScaleR-1.5B-Preview), and first use SFT with a small curated demonstration set to initialize its ability to spawn workers in a certain format. Because SFT degrades the accuracy significantly, we design a multi-stage RL algorithm, with various data filtering strategies, to recover the accuracy while decreasing the longest path length. Across several benchmarks including AIME 2024 and HMMT 2025, DC-CoT achieves similar accuracy as DeepScaleR-1.5B-Preview while decreasing longest path length by 35-40%. Our code, SFT dataset and models are publicly available at https://github.com/amahankali10/DC_CoT_RL_for_Low_Latency_CoT_with_Parallel_Reasoning.

</details>


### [234] [Avoiding Premature Collapse: Adaptive Annealing for Entropy-Regularized Structural Inference](https://arxiv.org/abs/2601.23039)
*Yizhi Liu*

Main category: cs.LG

TL;DR: 论文指出可微匹配层退火恢复离散排列不稳定的原因是过早模式崩溃，提出Efficient PH - ASC自适应调度算法解决该问题，还给出代码和演示链接。


<details>
  <summary>Details</summary>
Motivation: 解决可微匹配层通过退火恢复离散排列时不稳定的问题。

Method: 分析Sinkhorn定点映射的非正态动力学揭示热力学速度限制，提出Efficient PH - ASC自适应调度算法，通过执行线性稳定性定律降低计算开销。

Result: 识别出过早模式崩溃这一失败机制，提出的算法将计算开销从O(N^3)降至摊还O(1)。

Conclusion: Efficient PH - ASC算法能有效解决可微匹配层退火恢复离散排列不稳定的问题，且降低了计算开销。

Abstract: Differentiable matching layers, often implemented via entropy-regularized Optimal Transport, serve as a critical approximate inference mechanism in structural prediction. However, recovering discrete permutations via annealing $ε\to 0$ is notoriously unstable. We identify a fundamental mechanism for this failure: \textbf{Premature Mode Collapse}. By analyzing the non-normal dynamics of the Sinkhorn fixed-point map, we reveal a theoretical \textbf{thermodynamic speed limit}. Under standard exponential cooling, the shift in the target posterior ($O(1)$) outpaces the contraction rate of the inference operator, which degrades as $O(1/ε)$. This mismatch inevitably forces the inference trajectory into spurious local basins. To address this, we propose \textbf{Efficient PH-ASC}, an adaptive scheduling algorithm that monitors the stability of the inference process. By enforcing a linear stability law, we decouple expensive spectral diagnostics from the training loop, reducing overhead from $O(N^3)$ to amortized $O(1)$. Our implementation and interactive demo are available at https://github.com/xxx0438/torch-sinkhorn-asc and https://huggingface.co/spaces/leon0923/torch-sinkhorn-asc-demo. bounded away from zero in generic training dynamics unless the feature extractor converges unrealistically fast.

</details>


### [235] [Adaptive Edge Learning for Density-Aware Graph Generation](https://arxiv.org/abs/2601.23052)
*Seyedeh Ava Razi Razavi,James Sargant,Sheridan Houghten,Renata Dividino*

Main category: cs.LG

TL;DR: 提出基于Wasserstein GAN的密度感知条件图生成框架，改进了现有图生成方法的不足，实验显示该框架在生成图的质量和训练稳定性上表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有图生成方法主要依赖固定概率的随机边采样，难以捕捉节点间复杂的结构依赖关系，所以需要改进。

Method: 提出用Wasserstein GAN的密度感知条件图生成框架，用可学习的基于距离的边预测器替代随机采样，利用可微边预测器决定节点对关系，用密度感知选择机制控制边密度，用带梯度惩罚的WGAN训练，用基于GCN的判别器。

Result: 在基准数据集上的实验表明，该方法生成的图在结构一致性和类一致连通性上优于现有基线，学习到的边预测器能捕捉复杂关系模式，生成图的密度和拓扑与真实结构分布接近。

Conclusion: 该框架在真实图生成和数据增强中有效，具有更好的训练稳定性和可控合成能力。

Abstract: Generating realistic graph-structured data is challenging due to discrete structures, variable sizes, and class-specific connectivity patterns that resist conventional generative modelling. While recent graph generation methods employ generative adversarial network (GAN) frameworks to handle permutation invariance and irregular topologies, they typically rely on random edge sampling with fixed probabilities, limiting their capacity to capture complex structural dependencies between nodes. We propose a density-aware conditional graph generation framework using Wasserstein GANs (WGAN) that replaces random sampling with a learnable distance-based edge predictor. Our approach embeds nodes into a latent space where proximity correlates with edge likelihood, enabling the generator to learn meaningful connectivity patterns. A differentiable edge predictor determines pairwise relationships directly from node embeddings, while a density-aware selection mechanism adaptively controls edge density to match class-specific sparsity distributions observed in real graphs. We train the model using a WGAN with gradient penalty, employing a GCN-based critic to ensure generated graphs exhibit realistic topology and align with target class distributions. Experiments on benchmark datasets demonstrate that our method produces graphs with superior structural coherence and class-consistent connectivity compared to existing baselines. The learned edge predictor captures complex relational patterns beyond simple heuristics, generating graphs whose density and topology closely match real structural distributions. Our results show improved training stability and controllable synthesis, making the framework effective for realistic graph generation and data augmentation. Source code is publicly available at https://github.com/ava-12/Density_Aware_WGAN.git.

</details>


### [236] [From Absolute to Relative: Rethinking Reward Shaping in Group-Based Reinforcement Learning](https://arxiv.org/abs/2601.23058)
*Wenzhe Niu,Wei He,Zongxia Xie,Jinpeng Ou,Huichuan Fan,Yuchen Ge,Yanru Sun,Ziyin Wang,Yizhao Sun,Chengshun Shi,Jiuchong Gao,Jinghua Hao,Renqing He*

Main category: cs.LG

TL;DR: 提出基于相对奖励的强化学习框架RLRR，可缓解信号稀疏和奖励不稳定问题，实验显示其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于绝对数值奖励的基于组的强化学习方法存在内在局限，如可验证任务中监督稀疏、开放式场景中奖励模型分数范围不稳定。

Method: 提出RLRR框架，将奖励塑造从绝对评分转变为相对排名；引入排名奖励模型，直接生成相对排名。

Result: 实验表明，在推理基准和开放式生成任务中，RLRR相比标准基于组的基线方法有持续的性能提升。

Conclusion: RLRR能有效缓解信号稀疏和奖励不稳定问题，是一种更优的强化学习方法。

Abstract: Reinforcement learning has become a cornerstone for enhancing the reasoning capabilities of Large Language Models, where group-based approaches such as GRPO have emerged as efficient paradigms that optimize policies by leveraging intra-group performance differences. However, these methods typically rely on absolute numerical rewards, introducing intrinsic limitations. In verifiable tasks, identical group evaluations often result in sparse supervision, while in open-ended scenarios, the score range instability of reward models undermines advantage estimation based on group means. To address these limitations, we propose Reinforcement Learning with Relative Rewards (RLRR), a framework that shifts reward shaping from absolute scoring to relative ranking. Complementing this framework, we introduce the Ranking Reward Model, a listwise preference model tailored for group-based optimization to directly generate relative rankings. By transforming raw evaluations into robust relative signals, RLRR effectively mitigates signal sparsity and reward instability. Experimental results demonstrate that RLRR yields consistent performance improvements over standard group-based baselines across reasoning benchmarks and open-ended generation tasks.

</details>


### [237] [ExplainerPFN: Towards tabular foundation models for model-free zero-shot feature importance estimations](https://arxiv.org/abs/2601.23068)
*Joao Fonseca,Julia Stoyanovich*

Main category: cs.LG

TL;DR: 本文探讨零样本下估计Shapley值，提出ExplainerPFN模型，有四点贡献，实验显示其性能有竞争力。


<details>
  <summary>Details</summary>
Motivation: Shapley值计算需访问模型且成本高，现实中常不满足条件，因此研究零样本下有意义的Shapley值估计。

Method: 引入基于TabPFN的表格基础模型ExplainerPFN，在随机结构因果模型生成的合成数据集上预训练，用精确或近似精确的Shapley值监督训练。

Result: 少样本学习解释能以少量参考观测达到与SHAP值高保真度；提出零样本估计Shapley值的ExplainerPFN；提供开源实现；实验表明ExplainerPFN性能与依赖2 - 10个SHAP示例的少样本替代解释器有竞争力。

Conclusion: ExplainerPFN可在零样本下有效估计Shapley值，性能良好。

Abstract: Computing the importance of features in supervised classification tasks is critical for model interpretability. Shapley values are a widely used approach for explaining model predictions, but require direct access to the underlying model, an assumption frequently violated in real-world deployments. Further, even when model access is possible, their exact computation may be prohibitively expensive. We investigate whether meaningful Shapley value estimations can be obtained in a zero-shot setting, using only the input data distribution and no evaluations of the target model. To this end, we introduce ExplainerPFN, a tabular foundation model built on TabPFN that is pretrained on synthetic datasets generated from random structural causal models and supervised using exact or near-exact Shapley values. Once trained, ExplainerPFN predicts feature attributions for unseen tabular datasets without model access, gradients, or example explanations.
  Our contributions are fourfold: (1) we show that few-shot learning-based explanations can achieve high fidelity to SHAP values with as few as two reference observations; (2) we propose ExplainerPFN, the first zero-shot method for estimating Shapley values without access to the underlying model or reference explanations; (3) we provide an open-source implementation of ExplainerPFN, including the full training pipeline and synthetic data generator; and (4) through extensive experiments on real and synthetic datasets, we show that ExplainerPFN achieves performance competitive with few-shot surrogate explainers that rely on 2-10 SHAP examples.

</details>


### [238] [SplineFlow: Flow Matching for Dynamical Systems with B-Spline Interpolants](https://arxiv.org/abs/2601.23072)
*Santanu Subhash Rathod,Pietro Liò,Xiao Zhang*

Main category: cs.LG

TL;DR: 提出SplineFlow流匹配算法，通过B样条插值联合建模观测间的条件路径，在多种动力系统实验中表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有最先进的流匹配方法不适合建模动力系统，构建满足多边缘约束的统一路径具有挑战性。

Method: 引入SplineFlow，利用B样条基的平滑性和稳定性，通过B样条插值联合建模观测间的条件路径。

Result: 在各种确定和随机动力系统以及细胞轨迹推断任务的综合实验中，SplineFlow相比现有基线有显著改进。

Conclusion: SplineFlow在建模动力系统方面表现良好，能有效解决现有方法的不足。

Abstract: Flow matching is a scalable generative framework for characterizing continuous normalizing flows with wide-range applications. However, current state-of-the-art methods are not well-suited for modeling dynamical systems, as they construct conditional paths using linear interpolants that may not capture the underlying state evolution, especially when learning higher-order dynamics from irregular sampled observations. Constructing unified paths that satisfy multi-marginal constraints across observations is challenging, since naïve higher-order polynomials tend to be unstable and oscillatory. We introduce SplineFlow, a theoretically grounded flow matching algorithm that jointly models conditional paths across observations via B-spline interpolation. Specifically, SplineFlow exploits the smoothness and stability of B-spline bases to learn the complex underlying dynamics in a structured manner while ensuring the multi-marginal requirements are met. Comprehensive experiments across various deterministic and stochastic dynamical systems of varying complexity, as well as on cellular trajectory inference tasks, demonstrate the strong improvement of SplineFlow over existing baselines. Our code is available at: https://github.com/santanurathod/SplineFlow.

</details>


### [239] [RN-D: Discretized Categorical Actors with Regularized Networks for On-Policy Reinforcement Learning](https://arxiv.org/abs/2601.23075)
*Yuexin Bian,Jie Feng,Tao Wang,Yijiang Li,Sicun Gao,Yuanyuan Shi*

Main category: cs.LG

TL;DR: 本文重新审视策略表示，提出离散正则化actor网络，在连续控制基准测试中取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于策略的深度强化学习标准实现依赖高斯actor和较浅的MLP策略，在梯度有噪声和策略更新需保守时优化易不稳定。

Method: 研究用离散分类actor表示每个动作维度，结合监督学习的架构进展提出正则化actor网络，固定critic设计。

Result: 用离散正则化actor替换标准actor网络，在各种连续控制基准测试中取得一致增益和SOTA性能。

Conclusion: 以策略表示作为策略优化的首要设计选择是有效的，离散正则化actor网络能提升连续控制性能。

Abstract: On-policy deep reinforcement learning remains a dominant paradigm for continuous control, yet standard implementations rely on Gaussian actors and relatively shallow MLP policies, often leading to brittle optimization when gradients are noisy and policy updates must be conservative. In this paper, we revisit policy representation as a first-class design choice for on-policy optimization. We study discretized categorical actors that represent each action dimension with a distribution over bins, yielding a policy objective that resembles a cross-entropy loss. Building on architectural advances from supervised learning, we further propose regularized actor networks, while keeping critic design fixed. Our results show that simply replacing the standard actor network with our discretized regularized actor yields consistent gains and achieve the state-of-the-art performance across diverse continuous-control benchmarks.

</details>


### [240] [CATTO: Balancing Preferences and Confidence in Language Models](https://arxiv.org/abs/2601.23096)
*Nisarg Parikh,Kunjal Panchal,Ananya Sai,Pannaga Shivaswamy,Andrew Lan*

Main category: cs.LG

TL;DR: 提出校准感知的标记级训练目标CATTO，降低校准误差，不损失任务准确率，还引入Confidence@k机制。


<details>
  <summary>Details</summary>
Motivation: 大语言模型预测置信度校准不佳，偏好对齐方法加剧此问题。

Method: 引入校准感知目标CATTO，可与原偏好优化目标结合；引入Confidence@k测试时缩放机制。

Result: CATTO比直接偏好优化和最强DPO基线降低了预期校准误差，且保持或略微提高了多项选择题的回答准确率。

Conclusion: CATTO能有效改善大语言模型预测置信度校准问题，且不损失任务准确性。

Abstract: Large language models (LLMs) often make accurate next token predictions but their confidence in these predictions can be poorly calibrated: high-confidence predictions are frequently wrong, and low-confidence predictions may be correct. This miscalibration is exacerbated by preference-based alignment methods breaking the link between predictive probability and correctness. We introduce a Calibration Aware Token-level Training Objective (CATTO), a calibration-aware objective that aligns predicted confidence with empirical prediction correctness, which can be combined with the original preference optimization objectives. Empirically, CATTO reduces Expected Calibration Error (ECE) by 2.22%-7.61% in-distribution and 1.46%-10.44% out-of-distribution compared to direct preference optimization (DPO), and by 0.22%-1.24% in-distribution and 1.23%-5.07% out-of-distribution compared to the strongest DPO baseline. This improvement in confidence does not come at a cost of losing task accuracy, where CATTO maintains or slightly improves multiple-choice question-answering accuracy on five datasets. We also introduce Confidence@k, a test-time scaling mechanism leveraging calibrated token probabilities for Bayes-optimal selection of output tokens.

</details>


### [241] [To See Far, Look Close: Evolutionary Forecasting for Long-term Time Series](https://arxiv.org/abs/2601.23114)
*Jiaming Ma,Siyuan Mu,Ruilin Tang,Haofeng Ma,Qihe Huang,Zhengyang Zhou,Pengkun Wang,Binwu Wang,Yang Wang*

Main category: cs.LG

TL;DR: 本文指出直接预测（DF）范式在长期时间序列预测（LTSF）中的局限，提出进化预测（EF）范式，显示短 horizon 训练模型结合 EF 效果更好，推动了范式从静态映射转向进化推理。


<details>
  <summary>Details</summary>
Motivation: 针对 DF 范式需为每个目标 horizon 重新训练模型，计算成本高的问题。

Method: 提出进化预测（EF）范式，将其作为统一的生成框架，探究短 horizon 训练模型结合 EF 的效果。

Result: 实验表明单个 EF 模型在标准基准上超越特定任务的 DF 集成，在极端外推中具有稳健的渐近稳定性。

Conclusion: 推动 LTSF 从被动静态映射到自主进化推理的范式转变。

Abstract: The prevailing Direct Forecasting (DF) paradigm dominates Long-term Time Series Forecasting (LTSF) by forcing models to predict the entire future horizon in a single forward pass. While efficient, this rigid coupling of output and evaluation horizons necessitates computationally prohibitive re-training for every target horizon. In this work, we uncover a counter-intuitive optimization anomaly: models trained on short horizons-when coupled with our proposed Evolutionary Forecasting (EF) paradigm-significantly outperform those trained directly on long horizons. We attribute this success to the mitigation of a fundamental optimization pathology inherent in DF, where conflicting gradients from distant futures cripple the learning of local dynamics. We establish EF as a unified generative framework, proving that DF is merely a degenerate special case of EF. Extensive experiments demonstrate that a singular EF model surpasses task-specific DF ensembles across standard benchmarks and exhibits robust asymptotic stability in extreme extrapolation. This work propels a paradigm shift in LTSF: moving from passive Static Mapping to autonomous Evolutionary Reasoning.

</details>


### [242] [Distribution-informed Efficient Conformal Prediction for Full Ranking](https://arxiv.org/abs/2601.23128)
*Wenbo Liao,Huipeng Huang,Chen Jia,Huajun Xi,Hao Zeng,Hongxin Wei*

Main category: cs.LG

TL;DR: 现有排名模型不确定性量化方法结果保守，本文提出DCR方法，通过推导非一致性得分分布生成高效预测集，有理论保障且实验显示效果优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有使用保形预测的不确定性量化方法依赖非一致性得分上限，导致预测集过大，过于保守。

Method: 提出Distribution-informed Conformal Ranking (DCR)，推导校准项绝对排名基于相对排名的负超几何分布，进而得出非一致性得分分布以确定保形阈值。

Result: 理论上在温和假设下保证有效覆盖的同时提高效率，实验中平均预测集大小最多减少36%。

Conclusion: DCR方法在保证有效覆盖的前提下，能显著提高效率，优于基线方法。

Abstract: Quantifying uncertainty is critical for the safe deployment of ranking models in real-world applications. Recent work offers a rigorous solution using conformal prediction in a full ranking scenario, which aims to construct prediction sets for the absolute ranks of test items based on the relative ranks of calibration items. However, relying on upper bounds of non-conformity scores renders the method overly conservative, resulting in substantially large prediction sets. To address this, we propose Distribution-informed Conformal Ranking (DCR), which produces efficient prediction sets by deriving the exact distribution of non-conformity scores. In particular, we find that the absolute ranks of calibration items follow Negative Hypergeometric distributions, conditional on their relative ranks. DCR thus uses the rank distribution to derive non-conformity score distribution and determine conformal thresholds. We provide theoretical guarantees that DCR achieves improved efficiency over the baseline while ensuring valid coverage under mild assumptions. Extensive experiments demonstrate the superiority of DCR, reducing average prediction set size by up to 36%, while maintaining valid coverage.

</details>


### [243] [Regularisation in neural networks: a survey and empirical analysis of approaches](https://arxiv.org/abs/2601.23131)
*Christiaan P. Opperman,Anna S. Bosman,Katherine M. Malan*

Main category: cs.LG

TL;DR: 研究神经网络正则化技术是否总能提升性能，对其分类并实证比较，发现正则化有效性依赖数据集。


<details>
  <summary>Details</summary>
Motivation: 探究正则化添加到流程中会提升性能这一假设是否成立。

Method: 对正则化技术进行广泛综述，提出四分法分类，在十个数据集上对多层感知机和卷积神经网络的多种正则化技术进行实证比较。

Result: 正则化的有效性依赖数据集，如正则化项改善数值数据集性能，批量归一化改善图像数据集性能。

Conclusion: 理解正则化技术效果及联系对实际应用至关重要。

Abstract: Despite huge successes on a wide range of tasks, neural networks are known to sometimes struggle to generalise to unseen data. Many approaches have been proposed over the years to promote the generalisation ability of neural networks, collectively known as regularisation techniques. These are used as common practice under the assumption that any regularisation added to the pipeline would result in a performance improvement. In this study, we investigate whether this assumption holds in practice. First, we provide a broad review of regularisation techniques, including modern theories such as double descent. We propose a taxonomy of methods under four broad categories, namely: (1) data-based strategies, (2) architecture strategies, (3) training strategies, and (4) loss function strategies. Notably, we highlight the contradictions and correspondences between the approaches in these broad classes. Further, we perform an empirical comparison of the various regularisation techniques on classification tasks for ten numerical and image datasets applied to the multi-layer perceptron and convolutional neural network architectures. Results show that the efficacy of regularisation is dataset-dependent. For example, the use of a regularisation term only improved performance on numeric datasets, whereas batch normalisation improved performance on image datasets only. Generalisation is crucial to machine learning; thus, understanding the effects of applying regularisation techniques, and considering the connections between them is essential to the appropriate use of these methods in practice.

</details>


### [244] [Why GRPO Needs Normalization: A Local-Curvature Perspective on Adaptive Gradients](https://arxiv.org/abs/2601.23135)
*Cheng Ge,Caitlyn Heqi Yin,Hao Liang,Jiawei Zhang*

Main category: cs.LG

TL;DR: 本文从序列级策略梯度的局部曲率角度解释了GRPO中标准差归一化的作用，理论证明其收敛率提升，实证分析出三个训练阶段，为无评判器的强化学习算法设计提供见解。


<details>
  <summary>Details</summary>
Motivation: 解释GRPO算法中标准差归一化为何及何时起作用。

Method: 从序列级策略梯度的局部曲率角度进行理论分析，在GSM8K和MATH基准上进行实证分析。

Result: 理论上，GRPO在温和条件下收敛率比未归一化的REINFORCE严格提升；实证分析出三个训练阶段。

Conclusion: 给出标准差归一化在GRPO中何时起作用的合理解释，为无评判器的强化学习算法设计提供更广泛见解。

Abstract: Reinforcement learning (RL) has become a key driver of language model reasoning. Among RL algorithms, Group Relative Policy Optimization (GRPO) is the de facto standard, avoiding the need for a critic by using per-prompt baselines and variance normalization. Yet why and when this normalization helps remains unclear. In this work, we provide an explanation through the lens of local curvature of the sequence-level policy gradient: standard deviation normalization implements an adaptive gradient. Theoretically, under mild conditions, GRPO enjoys a strictly improved convergence rate over unnormalized REINFORCE, with gains characterized by the average within-prompt reward standard deviation across prompts and iterations. Empirically, our analysis on GSM8K and MATH benchmarks reveals three distinct training phases governed by the interplay between feature orthogonality and reward variance: (I) an early acceleration phase where high variance and orthogonality favor adaptive scaling; (II) a relatively stable transition phase; and (III) a late-stage regime where the loss of orthogonality limits further gains. Together, these results provide a principled account of when std normalization helps in GRPO, and offer broader insights into the design of critic-free RL algorithms.

</details>


### [245] [Securing Time in Energy IoT: A Clock-Dynamics-Aware Spatio-Temporal Graph Attention Network for Clock Drift Attacks and Y2K38 Failures](https://arxiv.org/abs/2601.23147)
*Saeid Jamshidi,Omar Abdul Wahab,Rolando Herrero,Foutse Khomh*

Main category: cs.LG

TL;DR: 本文针对能源物联网时间完整性问题提出STGAT框架，实验表明其准确率高、检测延迟低。


<details>
  <summary>Details</summary>
Motivation: 分布式物联网设备时间完整性对能源信息物理系统至关重要，但现有系统易受时钟漂移等影响，传统异常检测模型无法捕捉时间不一致问题。

Method: 引入STGAT框架，结合漂移感知时间嵌入和时间自注意力捕捉设备时间演变，用图注意力建模时间误差空间传播，采用曲率正则化潜在表示分离正常和异常。

Result: 在能源物联网遥测实验中，STGAT准确率达95.7%，优于基准模型；检测延迟降低26%，在多种异常情况下性能稳定。

Conclusion: STGAT在解决能源物联网时间完整性问题上表现出色，能有效检测时间异常。

Abstract: The integrity of time in distributed Internet of Things (IoT) devices is crucial for reliable operation in energy cyber-physical systems, such as smart grids and microgrids. However, IoT systems are vulnerable to clock drift, time-synchronization manipulation, and timestamp discontinuities, such as the Year 2038 (Y2K38) Unix overflow, all of which disrupt temporal ordering. Conventional anomaly-detection models, which assume reliable timestamps, fail to capture temporal inconsistencies. This paper introduces STGAT (Spatio-Temporal Graph Attention Network), a framework that models both temporal distortion and inter-device consistency in energy IoT systems. STGAT combines drift-aware temporal embeddings and temporal self-attention to capture corrupted time evolution at individual devices, and uses graph attention to model spatial propagation of timing errors. A curvature-regularized latent representation geometrically separates normal clock evolution from anomalies caused by drift, synchronization offsets, and overflow events. Experimental results on energy IoT telemetry with controlled timing perturbations show that STGAT achieves 95.7% accuracy, outperforming recurrent, transformer, and graph-based baselines with significant improvements (d > 1.8, p < 0.001). Additionally, STGAT reduces detection delay by 26%, achieving a 2.3-time-step delay while maintaining stable performance under overflow, drift, and physical inconsistencies.

</details>


### [246] [Manifold-Aware Perturbations for Constrained Generative Modeling](https://arxiv.org/abs/2601.23151)
*Katherine Keegan,Lars Ruthotto*

Main category: cs.LG

TL;DR: 本文针对等式约束生成模型的缺陷，提出一种便宜、合理且灵活的分布修改方法，实验证明该方法能有效恢复数据分布和稳定采样。


<details>
  <summary>Details</summary>
Motivation: 生成模型在处理等式约束分布时存在数学局限性，尤其是科学领域的相关场景。

Method: 以约束感知的方式扰动数据分布，使新分布支持匹配环境空间维度，同时隐式融入底层流形几何。

Result: 通过理论分析和多个代表性任务的实证，该方法能让扩散模型和归一化流实现数据分布恢复和稳定采样。

Conclusion: 所提方法能有效解决等式约束生成模型的已知缺陷。

Abstract: Generative models have enjoyed widespread success in a variety of applications. However, they encounter inherent mathematical limitations in modeling distributions where samples are constrained by equalities, as is frequently the setting in scientific domains. In this work, we develop a computationally cheap, mathematically justified, and highly flexible distributional modification for combating known pitfalls in equality-constrained generative models. We propose perturbing the data distribution in a constraint-aware way such that the new distribution has support matching the ambient space dimension while still implicitly incorporating underlying manifold geometry. Through theoretical analyses and empirical evidence on several representative tasks, we illustrate that our approach consistently enables data distribution recovery and stable sampling with both diffusion models and normalizing flows.

</details>


### [247] [Behemoth: Benchmarking Unlearning in LLMs Using Fully Synthetic Data](https://arxiv.org/abs/2601.23153)
*Eugenia Iofinova,Dan Alistarh*

Main category: cs.LG

TL;DR: 随着大语言模型应用增多但常出错，催生模型编辑需求，现有方法有局限性，提出Behemoth合成数据生成框架并在简单表格数据上探索，有意外发现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型应用广泛但常出错，现有模型编辑方法存在脆性和不完整性，且与训练数据分布关系不明确。

Method: 提出Behemoth全合成数据生成框架，并在简单表格数据上探索模型编辑。

Result: 在某些情况下限制更新秩会带来更有效的更新，部分结果与现实情况呼应。

Conclusion: Behemoth框架有助于理解训练数据分布与模型编辑的关系，具有实际应用价值，代码可公开获取。

Abstract: As artificial neural networks, and specifically large language models, have improved rapidly in capabilities and quality, they have increasingly been deployed in real-world applications, from customer service to Google search, despite the fact that they frequently make factually incorrect or undesirable statements. This trend has inspired practical and academic interest in model editing, that is, in adjusting the weights of the model to modify its likely outputs for queries relating to a specific fact or set of facts. This may be done either to amend a fact or set of facts, for instance, to fix a frequent error in the training data, or to suppress a fact or set of facts entirely, for instance, in case of dangerous knowledge. Multiple methods have been proposed to do such edits. However, at the same time, it has been shown that such model editing can be brittle and incomplete. Moreover the effectiveness of any model editing method necessarily depends on the data on which the model is trained, and, therefore, a good understanding of the interaction of the training data distribution and the way it is stored in the network is necessary and helpful to reliably perform model editing. However, working with large language models trained on real-world data does not allow us to understand this relationship or fully measure the effects of model editing. We therefore propose Behemoth, a fully synthetic data generation framework. To demonstrate the practical insights from the framework, we explore model editing in the context of simple tabular data, demonstrating surprising findings that, in some cases, echo real-world results, for instance, that in some cases restricting the update rank results in a more effective update. The code is available at https://github.com/IST-DASLab/behemoth.git.

</details>


### [248] [On Safer Reinforcement Learning Policies for Sedation and Analgesia in Intensive Care](https://arxiv.org/abs/2601.23154)
*Joel Romero-Hernandez,Oscar Camara*

Main category: cs.LG

TL;DR: 研究用深度强化学习框架在部分可观测下建议每小时药物剂量，对比两种用药策略，发现重视长期结果对安全治疗策略很重要。


<details>
  <summary>Details</summary>
Motivation: 重症监护室疼痛管理需权衡治疗目标与患者安全，现有强化学习在镇静镇痛方面的工作存在未重视患者生存和算法不适合不完全信息场景的问题。

Method: 实施深度强化学习框架，利用MIMIC - IV数据库中47,144例ICU住院数据，训练两种用药策略：减轻疼痛、同时减轻疼痛和降低死亡率。

Result: 两种策略都能降低疼痛，但第一种与死亡率正相关，第二种与死亡率负相关。

Conclusion: 即使短期目标是主要目标，重视长期结果对制定更安全的治疗策略至关重要。

Abstract: Pain management in intensive care usually involves complex trade-offs between therapeutic goals and patient safety, since both inadequate and excessive treatment may induce serious sequelae. Reinforcement learning can help address this challenge by learning medication dosing policies from retrospective data. However, prior work on sedation and analgesia has optimized for objectives that do not value patient survival while relying on algorithms unsuitable for imperfect information settings. We investigated the risks of these design choices by implementing a deep reinforcement learning framework to suggest hourly medication doses under partial observability. Using data from 47,144 ICU stays in the MIMIC-IV database, we trained policies to prescribe opioids, propofol, benzodiazepines, and dexmedetomidine according to two goals: reduce pain or jointly reduce pain and mortality. We found that, although the two policies were associated with lower pain, actions from the first policy were positively correlated with mortality, while those proposed by the second policy were negatively correlated. This suggests that valuing long-term outcomes could be critical for safer treatment policies, even if a short-term goal remains the primary objective.

</details>


### [249] [SPICE: Submodular Penalized Information-Conflict Selection for Efficient Large Language Model Training](https://arxiv.org/abs/2601.23155)
*Powei Chang,Jinpeng Zhang,Bowen Chen,Chenyu Wang,Chenlu Guo,Yixing Zhang,Yukang Gao,JianXiang Xiang,Yue Gao,Chaoqun Sun,Yiyi Chen,Dongying Kong*

Main category: cs.LG

TL;DR: 本文提出冲突感知选择器SPICE，在信息选择时考虑梯度冲突，能在少量数据下实现性能提升和成本降低。


<details>
  <summary>Details</summary>
Motivation: 实际中梯度冲突会减缓边际对数行列式信息增益的衰减，导致信息损失，需解决此问题。

Method: 通过ε - 分解量化与理想子模性的偏差，提出冲突感知选择器SPICE，支持提前停止和代理模型。

Result: SPICE选出的子集对数行列式信息更高，在8个基准测试中，用10%数据匹配或超越6种方法。

Conclusion: SPICE能在大幅降低训练成本的同时实现性能提升。

Abstract: Information-based data selection for instruction tuning is compelling: maximizing the log-determinant of the Fisher information yields a monotone submodular objective, enabling greedy algorithms to achieve a $(1-1/e)$ approximation under a cardinality budget. In practice, however, we identify alleviating gradient conflicts, misalignment between per-sample gradients, is a key factor that slows down the decay of marginal log-determinant information gains, thereby preventing significant loss of information. We formalize this via an $\varepsilon$-decomposition that quantifies the deviation from ideal submodularity as a function of conflict statistics, yielding data-dependent approximation factors that tighten as conflicts diminish. Guided by this analysis, we propose SPICE, a conflict-aware selector that maximizes information while penalizing misalignment, and that supports early stopping and proxy models for efficiency. Empirically, SPICE selects subsets with higher log-determinant information than original criteria, and these informational gains translate into performance improvements: across 8 benchmarks with LLaMA2-7B and Qwen2-7B, SPICE uses only 10% of the data, yet matches or exceeds 6 methods including full-data tuning. This achieves performance improvements with substantially lower training cost.

</details>


### [250] [Unsupervised Hierarchical Skill Discovery](https://arxiv.org/abs/2601.23156)
*Damion Harvey,Geraud Nangue Tasse,Branden Ingram,Benjamin Rosman,Steven James*

Main category: cs.LG

TL;DR: 提出基于语法方法对无标签轨迹进行技能分割并构建层次结构，在高维像素环境中表现优于基线，且有助于下游强化学习任务。


<details>
  <summary>Details</summary>
Motivation: 现有无监督技能分割和层次结构发现方法多依赖动作标签、奖励或手工注释，限制了适用性。

Method: 使用基于语法的方法将无标签轨迹分割成技能并构建层次结构。

Result: 在高维像素环境中，该方法比现有基线产生更有结构和语义意义的层次结构。

Conclusion: 所发现的层次结构能加速和稳定下游强化学习任务的学习。

Abstract: We consider the problem of unsupervised skill segmentation and hierarchical structure discovery in reinforcement learning. While recent approaches have sought to segment trajectories into reusable skills or options, most rely on action labels, rewards, or handcrafted annotations, limiting their applicability. We propose a method that segments unlabelled trajectories into skills and induces a hierarchical structure over them using a grammar-based approach. The resulting hierarchy captures both low-level behaviours and their composition into higher-level skills. We evaluate our approach in high-dimensional, pixel-based environments, including Craftax and the full, unmodified version of Minecraft. Using metrics for skill segmentation, reuse, and hierarchy quality, we find that our method consistently produces more structured and semantically meaningful hierarchies than existing baselines. Furthermore, as a proof of concept for utility, we demonstrate that these discovered hierarchies accelerate and stabilise learning on downstream reinforcement learning tasks.

</details>


### [251] [Probing the Trajectories of Reasoning Traces in Large Language Models](https://arxiv.org/abs/2601.23163)
*Marthe Ballon,Brecht Verbeken,Vincent Ginis,Andres Algaba*

Main category: cs.LG

TL;DR: 文章提出协议探测大模型推理轨迹，发现推理标记比例增加时准确性和决策确定性上升，且增益源于相关内容，不同模型对错误部分轨迹处理不同。


<details>
  <summary>Details</summary>
Motivation: 目前不清楚大语言模型推理轨迹中准确性和决策确定性如何变化，以及中间轨迹片段是否提供除长度和风格外与答案相关信息，因此要进行研究。

Method: 提出协议，生成模型推理轨迹，按固定标记百分比截断，将每个部分轨迹重新注入模型测量答案选择分布，应用于多个模型和基准测试。

Result: 推理标记比例增加，准确性和决策确定性上升，增益主要源于相关内容，强模型能从错误部分轨迹中回溯，弱模型则易受错误答案影响。

Conclusion: 轨迹探测为推理模型的高效安全部署提供诊断，测量结果可形成实用的轨迹处理和监控策略，提升可靠性。

Abstract: Large language models (LLMs) increasingly solve difficult problems by producing "reasoning traces" before emitting a final response. However, it remains unclear how accuracy and decision commitment evolve along a reasoning trajectory, and whether intermediate trace segments provide answer-relevant information beyond generic length or stylistic effects. Here, we propose a protocol to systematically probe the trajectories of reasoning traces in LLMs by 1) generating a model's reasoning trace, 2) truncating it at fixed token-percentiles, and 3) injecting each partial trace back into the model (or a different model) to measure the induced distribution over answer choices via next-token probabilities. We apply this protocol to the open-source Qwen3-4B/-8B/-14B and gpt-oss-20b/-120b models across the multiple-choice GPQA Diamond and MMLU-Pro benchmarks. We find that accuracy and decision commitment consistently increase as the percentage of provided reasoning tokens grows. These gains are primarily driven by relevant content in the model generation rather than context length or generic "reasoning style" effects. Stronger models often backtrack successfully from incorrect partial traces, but immediate answers often remain anchored in the weaker model's incorrect response. More broadly, we show that trajectory probing provides diagnostics for efficient and safer deployment of reasoning models as the measurements can inform practical trace-handling and monitoring policies that improve reliability without assuming intermediate tokens are inherently faithful explanations.

</details>


### [252] [Stochastic Linear Bandits with Parameter Noise](https://arxiv.org/abs/2601.23164)
*Daniel Ezer,Alon Peled-Cohen,Yishay Mansour*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the stochastic linear bandits with parameter noise model, in which the reward of action $a$ is $a^\top θ$ where $θ$ is sampled i.i.d. We show a regret upper bound of $\widetilde{O} (\sqrt{d T \log (K/δ) σ^2_{\max})}$ for a horizon $T$, general action set of size $K$ of dimension $d$, and where $σ^2_{\max}$ is the maximal variance of the reward for any action. We further provide a lower bound of $\widetildeΩ (d \sqrt{T σ^2_{\max}})$ which is tight (up to logarithmic factors) whenever $\log (K) \approx d$. For more specific action sets, $\ell_p$ unit balls with $p \leq 2$ and dual norm $q$, we show that the minimax regret is $\widetildeΘ (\sqrt{dT σ^2_q)}$, where $σ^2_q$ is a variance-dependent quantity that is always at most $4$. This is in contrast to the minimax regret attainable for such sets in the classic additive noise model, where the regret is of order $d \sqrt{T}$. Surprisingly, we show that this optimal (up to logarithmic factors) regret bound is attainable using a very simple explore-exploit algorithm.

</details>


### [253] [Names Don't Matter: Symbol-Invariant Transformer for Open-Vocabulary Learning](https://arxiv.org/abs/2601.23169)
*İlker Işık,Wenchao Li*

Main category: cs.LG

TL;DR: 提出基于Transformer的机制，对可互换令牌重命名不变，实验验证理论并在开放词汇任务有性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前神经架构缺乏处理可互换令牌的原则性方法，固定词汇表训练的模型难以泛化到未见符号。

Method: 采用并行嵌入流隔离输入中每个可互换令牌的贡献，结合聚合注意力机制实现跨流的结构化信息共享。

Result: 实验结果证实方法的理论保证，在开放词汇任务上有显著性能提升。

Conclusion: 所提出的基于Transformer的机制能有效解决可互换令牌处理及泛化问题。

Abstract: Current neural architectures lack a principled way to handle interchangeable tokens, i.e., symbols that are semantically equivalent yet distinguishable, such as bound variables. As a result, models trained on fixed vocabularies often struggle to generalize to unseen symbols, even when the underlying semantics remain unchanged. We propose a novel Transformer-based mechanism that is provably invariant to the renaming of interchangeable tokens. Our approach employs parallel embedding streams to isolate the contribution of each interchangeable token in the input, combined with an aggregated attention mechanism that enables structured information sharing across streams. Experimental results confirm the theoretical guarantees of our method and demonstrate substantial performance gains on open-vocabulary tasks that require generalization to novel symbols.

</details>


### [254] [Beyond Fixed Frames: Dynamic Character-Aligned Speech Tokenization](https://arxiv.org/abs/2601.23174)
*Luca Della Libera,Cem Subakan,Mirco Ravanelli*

Main category: cs.LG

TL;DR: 提出DyCAST动态字符对齐语音分词器，可变帧率分词，有检索增强解码机制，用更少令牌实现有竞争力的语音合成质量和下游性能。


<details>
  <summary>Details</summary>
Motivation: 现有编解码器固定帧率，分配令牌均匀，产生不必要的长序列。

Method: 引入DyCAST，通过软字符级对齐和显式持续时间建模实现可变帧率分词，还引入检索增强解码机制。

Result: DyCAST在使用显著更少令牌的情况下，实现了有竞争力的语音合成质量和下游性能。

Conclusion: DyCAST在语音处理上比固定帧率编解码器更具优势。

Abstract: Neural audio codecs are at the core of modern conversational speech technologies, converting continuous speech into sequences of discrete tokens that can be processed by LLMs. However, existing codecs typically operate at fixed frame rates, allocating tokens uniformly in time and producing unnecessarily long sequences. In this work, we introduce DyCAST, a Dynamic Character-Aligned Speech Tokenizer that enables variable-frame-rate tokenization through soft character-level alignment and explicit duration modeling. DyCAST learns to associate tokens with character-level linguistic units during training and supports alignment-free inference with direct control over token durations at decoding time. To improve speech resynthesis quality at low frame rates, we further introduce a retrieval-augmented decoding mechanism that enhances reconstruction fidelity without increasing bitrate. Experiments show that DyCAST achieves competitive speech resynthesis quality and downstream performance while using significantly fewer tokens than fixed-frame-rate codecs.

</details>


### [255] [MeshGraphNet-Transformer: Scalable Mesh-based Learned Simulation for Solid Mechanics](https://arxiv.org/abs/2601.23177)
*Mikel M. Iparraguirre,Iciar Alfaro,David Gonzalez,Elias Cueto*

Main category: cs.LG

TL;DR: 提出MeshGraphNet - Transformer (MGN - T)架构，结合Transformer和MeshGraphNets优势，克服标准MGN局限，在工业规模高分辨率网格上表现出色。


<details>
  <summary>Details</summary>
Motivation: 标准MGN在大尺寸高分辨率网格上信息传播效率低，需改进以处理工业规模网格。

Method: 将Transformer全局建模能力与MeshGraphNets几何归纳偏置结合，用物理注意力Transformer作为全局处理器。

Result: MGN - T能处理标准MGN失败的工业规模冲击动力学网格，准确建模多种物理现象，在经典基准测试中优于现有方法。

Conclusion: MGN - T在高分辨率网格学习上高效准确，所需参数少，有实际应用价值。

Abstract: We present MeshGraphNet-Transformer (MGN-T), a novel architecture that combines the global modeling capabilities of Transformers with the geometric inductive bias of MeshGraphNets, while preserving a mesh-based graph representation. MGN-T overcomes a key limitation of standard MGN, the inefficient long-range information propagation caused by iterative message passing on large, high-resolution meshes. A physics-attention Transformer serves as a global processor, updating all nodal states simultaneously while explicitly retaining node and edge attributes. By directly capturing long-range physical interactions, MGN-T eliminates the need for deep message-passing stacks or hierarchical, coarsened meshes, enabling efficient learning on high-resolution meshes with varying geometries, topologies, and boundary conditions at an industrial scale.
  We demonstrate that MGN-T successfully handles industrial-scale meshes for impact dynamics, a setting in which standard MGN fails due message-passing under-reaching. The method accurately models self-contact, plasticity, and multivariate outputs, including internal, phenomenological plastic variables. Moreover, MGN-T outperforms state-of-the-art approaches on classical benchmarks, achieving higher accuracy while maintaining practical efficiency, using only a fraction of the parameters required by competing baselines.

</details>


### [256] [TriSpec: Ternary Speculative Decoding via Lightweight Proxy Verification](https://arxiv.org/abs/2601.23180)
*Haoyun Jiang,Junqi He,Feng Hong,Xinlong Yang,Jianwei Zhang,Zheng Li,Zhengyang Zhuge,Zhiyong Chen,Bo Han,Junyang Lin,Jiangchao Yao*

Main category: cs.LG

TL;DR: 本文提出TriSpec三元推测解码框架，从验证成本角度改进推测解码，实验显示能提速并减少目标模型调用。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理效率受串行自回归生成限制，现有推测解码工作在草稿有效性和效率改进上接近饱和，需从验证成本新视角推进。

Method: 提出TriSpec三元推测解码框架，引入轻量级代理，批准易验证草稿序列，仅遇不确定令牌时调用完整目标模型，还可与EAGLE - 3等结合。

Result: 在Qwen3和DeepSeek - R1 - Distill - Qwen/LLaMA系列上实验表明，TriSpec比标准推测解码提速达35%，目标模型调用减少达50%，且精度相当。

Conclusion: TriSpec能有效降低验证成本，提升大语言模型推理效率。

Abstract: Inference efficiency in Large Language Models (LLMs) is fundamentally limited by their serial, autoregressive generation, especially as reasoning becomes a key capability and response sequences grow longer. Speculative decoding (SD) offers a powerful solution, providing significant speed-ups through its lightweight drafting and parallel verification mechanism. While existing work has nearly saturated improvements in draft effectiveness and efficiency, this paper advances SD from a new yet critical perspective: the verification cost. We propose TriSpec, a novel ternary SD framework that, at its core, introduces a lightweight proxy to significantly reduce computational cost by approving easily verifiable draft sequences and engaging the full target model only when encountering uncertain tokens. TriSpec can be integrated with state-of-the-art SD methods like EAGLE-3 to further reduce verification costs, achieving greater acceleration. Extensive experiments on the Qwen3 and DeepSeek-R1-Distill-Qwen/LLaMA families show that TriSpec achieves up to 35\% speedup over standard SD, with up to 50\% fewer target model invocations while maintaining comparable accuracy.

</details>


### [257] [Ensuring Semantics in Weights of Implicit Neural Representations through the Implicit Function Theorem](https://arxiv.org/abs/2601.23181)
*Tianming Qiu,Christos Sonis,Hao Shen*

Main category: cs.LG

TL;DR: 本文利用隐函数定理在数据空间和潜在权重表示空间建立映射，分析通过共享超网络将实例特定嵌入映射到INR权重的框架，在下游分类任务中表现良好，提供理论视角。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏将数据语义编码到网络权重机制的精确理论解释。

Method: 部署隐函数定理（IFT）建立数据空间和潜在权重表示空间的严格映射，分析通过共享超网络将实例特定嵌入映射到INR权重的框架。

Result: 在2D和3D数据集的下游分类任务中取得与现有基线具有竞争力的表现。

Conclusion: 研究结果为未来网络权重研究提供理论视角。

Abstract: Weight Space Learning (WSL), which frames neural network weights as a data modality, is an emerging field with potential for tasks like meta-learning or transfer learning. Particularly, Implicit Neural Representations (INRs) provide a convenient testbed, where each set of weights determines the corresponding individual data sample as a mapping from coordinates to contextual values. So far, a precise theoretical explanation for the mechanism of encoding semantics of data into network weights is still missing. In this work, we deploy the Implicit Function Theorem (IFT) to establish a rigorous mapping between the data space and its latent weight representation space. We analyze a framework that maps instance-specific embeddings to INR weights via a shared hypernetwork, achieving performance competitive with existing baselines on downstream classification tasks across 2D and 3D datasets. These findings offer a theoretical lens for future investigations into network weights.

</details>


### [258] [Learning to Execute Graph Algorithms Exactly with Graph Neural Networks](https://arxiv.org/abs/2601.23207)
*Muhammad Fetrat Qharabagh,Artur Back de Luca,George Giapitzakis,Kimon Fountoulakis*

Main category: cs.LG

TL;DR: 本文在有界度和有限精度约束下证明了图算法的精确可学习性结果，介绍了训练MLP集成并用于GNN的方法，展示了对多种算法的可学习性。


<details>
  <summary>Details</summary>
Motivation: 理解图神经网络学习执行算法的能力是理论挑战，本文旨在证明图算法在特定约束下的可学习性。

Method: 分两步，先训练MLP集成执行节点局部指令，推理时将训练好的MLP集成作为GNN的更新函数，利用NTK理论。

Result: 能从小训练集学习局部指令，推理时无误差且高概率执行完整图算法，对分布式计算的LOCAL模型及多种经典算法有可学习性结果。

Conclusion: 所提方法能有效证明图算法在特定约束下的可学习性。

Abstract: Understanding what graph neural networks can learn, especially their ability to learn to execute algorithms, remains a central theoretical challenge. In this work, we prove exact learnability results for graph algorithms under bounded-degree and finite-precision constraints. Our approach follows a two-step process. First, we train an ensemble of multi-layer perceptrons (MLPs) to execute the local instructions of a single node. Second, during inference, we use the trained MLP ensemble as the update function within a graph neural network (GNN). Leveraging Neural Tangent Kernel (NTK) theory, we show that local instructions can be learned from a small training set, enabling the complete graph algorithm to be executed during inference without error and with high probability. To illustrate the learning power of our setting, we establish a rigorous learnability result for the LOCAL model of distributed computation. We further demonstrate positive learnability results for widely studied algorithms such as message flooding, breadth-first and depth-first search, and Bellman-Ford.

</details>


### [259] [Tackling air quality with SAPIENS](https://arxiv.org/abs/2601.23215)
*Marcella Bona,Nathan Heatley,Jia-Chen Hua,Adriana Lara,Valeria Legaria-Santiago,Alberto Luviano Juarez,Fernando Moreno-Gomez,Jocelyn Richardson,Natan Vilchis,Xiwen Shirley Zheng*

Main category: cs.LG

TL;DR: 本文结合墨西哥城交通数据和污染传感器测量值，提出新方法表征交通强度，用偏最小二乘回归预测污染水平，工作流程可适应其他场景


<details>
  <summary>Details</summary>
Motivation: 大城市空气污染是长期问题，交通是主要污染源，现有空气质量测量和预测时空粒度粗，而实时交通强度数据粒度细，旨在提供超本地动态空气质量预测

Method: 将简单的交通地图转换为基于同心环的描述来表征交通强度；用偏最小二乘回归基于新定义的交通强度预测污染水平；用不同训练样本优化模型

Result: 通过优化模型获得了更好的预测性能，并深入了解了污染物与交通之间的关系

Conclusion: 设计的工作流程简单，可适应其他语境，如其他城市

Abstract: Air pollution is a chronic problem in large cities worldwide and awareness is rising as the long-term health implications become clearer. Vehicular traffic has been identified as a major contributor to poor air quality. In a lot of cities the publicly available air quality measurements and forecasts are coarse-grained both in space and time. However, in general, real-time traffic intensity data is openly available in various forms and is fine-grained. In this paper, we present an in-depth study of pollution sensor measurements combined with traffic data from Mexico City. We analyse and model the relationship between traffic intensity and air quality with the aim to provide hyper-local, dynamic air quality forecasts. We developed an innovative method to represent traffic intensities by transforming simple colour-coded traffic maps into concentric ring-based descriptions, enabling improved characterisation of traffic conditions. Using Partial Least Squares Regression, we predict pollution levels based on these newly defined traffic intensities. The model was optimised with various training samples to achieve the best predictive performance and gain insights into the relationship between pollutants and traffic. The workflow we have designed is straightforward and adaptable to other contexts, like other cities beyond the specifics of our dataset.

</details>


### [260] [Optimal Fair Aggregation of Crowdsourced Noisy Labels using Demographic Parity Constraints](https://arxiv.org/abs/2601.23221)
*Gabriel Singer,Samuel Gruffaz,Olivier Vo Van,Nicolas Vayatis,Argyris Kalogeratos*

Main category: cs.LG

TL;DR: 本文分析众包聚合方法的公平性，推导多数投票公平性差距上限，证明聚合共识公平性差距收敛，并推广公平后处理算法，实验验证方法有效性。


<details>
  <summary>Details</summary>
Motivation: 获取可靠真实标签成本高或不可行，众包和聚合有噪声的人工标注会放大个体偏差，引发公平性问题，且众包聚合中的公平性研究不足。

Method: 在ε - 公平框架下分析多数投票和最优贝叶斯聚合方法的公平性，推导多数投票公平性差距上限，推广多类公平后处理算法。

Result: 得出多数投票公平性差距上限，证明聚合共识公平性差距在可解释条件下指数级快速收敛到真实标签的公平性差距，实验验证方法有效。

Conclusion: 所提方法能有效解决众包聚合中的公平性问题，理论推导和实验结果相符。

Abstract: As acquiring reliable ground-truth labels is usually costly, or infeasible, crowdsourcing and aggregation of noisy human annotations is the typical resort. Aggregating subjective labels, though, may amplify individual biases, particularly regarding sensitive features, raising fairness concerns. Nonetheless, fairness in crowdsourced aggregation remains largely unexplored, with no existing convergence guarantees and only limited post-processing approaches for enforcing $\varepsilon$-fairness under demographic parity. We address this gap by analyzing the fairness s of crowdsourced aggregation methods within the $\varepsilon$-fairness framework, for Majority Vote and Optimal Bayesian aggregation. In the small-crowd regime, we derive an upper bound on the fairness gap of Majority Vote in terms of the fairness gaps of the individual annotators. We further show that the fairness gap of the aggregated consensus converges exponentially fast to that of the ground-truth under interpretable conditions. Since ground-truth itself may still be unfair, we generalize a state-of-the-art multiclass fairness post-processing algorithm from the continuous to the discrete setting, which enforces strict demographic parity constraints to any aggregation rule. Experiments on synthetic and real datasets demonstrate the effectiveness of our approach and corroborate the theoretical insights.

</details>


### [261] [Agile Reinforcement Learning through Separable Neural Architecture](https://arxiv.org/abs/2601.23225)
*Rajib Mostakim,Reza T. Batley,Sourav Saha*

Main category: cs.LG

TL;DR: 本文提出SPAN用于强化学习函数逼近，在多任务中表现优于MLP基线，样本效率提升、成功率更高，且性能和鲁棒性好。


<details>
  <summary>Details</summary>
Motivation: 现有多层感知器（MLP）在强化学习函数逼近时参数效率低，影响样本效率和策略学习；样条分离架构计算开销大。

Method: 引入SPAN，整合可学习预处理层与可分离张量积B样条基，改进低秩KHRONOS框架。

Result: 在离散、连续控制任务及离线设置中，SPAN样本效率提升30 - 50%，成功率高1.3 - 9倍。

Conclusion: SPAN是资源受限环境中学习高效策略的可行高性能替代方案。

Abstract: Deep reinforcement learning (RL) is increasingly deployed in resource-constrained environments, yet the go-to function approximators - multilayer perceptrons (MLPs) - are often parameter-inefficient due to an imperfect inductive bias for the smooth structure of many value functions. This mismatch can also hinder sample efficiency and slow policy learning in this capacity-limited regime. Although model compression techniques exist, they operate post-hoc and do not improve learning efficiency. Recent spline-based separable architectures - such as Kolmogorov-Arnold Networks (KANs) - have been shown to offer parameter efficiency but are widely reported to exhibit significant computational overhead, especially at scale.
  In seeking to address these limitations, this work introduces SPAN (SPline-based Adaptive Networks), a novel function approximation approach to RL. SPAN adapts the low rank KHRONOS framework by integrating a learnable preprocessing layer with a separable tensor product B-spline basis. SPAN is evaluated across discrete (PPO) and high-dimensional continuous (SAC) control tasks, as well as offline settings (Minari/D4RL). Empirical results demonstrate that SPAN achieves a 30-50% improvement in sample efficiency and 1.3-9 times higher success rates across benchmarks compared to MLP baselines. Furthermore, SPAN demonstrates superior anytime performance and robustness to hyperparameter variations, suggesting it as a viable, high performance alternative for learning intrinsically efficient policies in resource-limited settings.

</details>


### [262] [Sequence Diffusion Model for Temporal Link Prediction in Continuous-Time Dynamic Graph](https://arxiv.org/abs/2601.23233)
*Nguyen Minh Duc,Viet Cuong Ta*

Main category: cs.LG

TL;DR: 提出SDG序列级扩散框架用于动态图时间链接预测，在多基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有时间图神经网络是纯判别式的，缺乏捕获未来时间交互不确定性和顺序结构的机制。

Method: 提出SDG框架，向历史交互序列注入噪声，通过条件去噪过程重建交互嵌入；使用交叉注意力去噪解码器引导目标序列重建并端到端优化模型。

Result: 在各种时间图基准测试中，SDG在时间链接预测任务上始终达到了最先进的性能。

Conclusion: SDG框架能有效解决动态图时间链接预测问题，提升预测性能。

Abstract: Temporal link prediction in dynamic graphs is a fundamental problem in many real-world systems. Existing temporal graph neural networks mainly focus on learning representations of historical interactions. Despite their strong performance, these models are still purely discriminative, producing point estimates for future links and lacking an explicit mechanism to capture the uncertainty and sequential structure of future temporal interactions. In this paper, we propose SDG, a novel sequence-level diffusion framework that unifies dynamic graph learning with generative denoising. Specifically, SDG injects noise into the entire historical interaction sequence and jointly reconstructs all interaction embeddings through a conditional denoising process, thereby enabling the model to capture more comprehensive interaction distributions. To align the generative process with temporal link prediction, we employ a cross-attention denoising decoder to guide the reconstruction of the destination sequence and optimize the model in an end-to-end manner. Extensive experiments on various temporal graph benchmarks show that SDG consistently achieves state-of-the-art performance in the temporal link prediction task.

</details>


### [263] [How well do generative models solve inverse problems? A benchmark study](https://arxiv.org/abs/2601.23238)
*Patrick Krüger,Patrick Materne,Werner Krebs,Hanno Gottschalk*

Main category: cs.LG

TL;DR: 文章对比传统贝叶斯逆方法与三种生成学习模型，用于燃气轮机燃烧室设计问题，提出评估指标，结果显示条件流匹配表现最佳。


<details>
  <summary>Details</summary>
Motivation: 检验生成学习算法在解决（贝叶斯）逆问题上的可能性，对比不同方法用于燃气轮机燃烧室设计逆问题的效果。

Method: 将传统贝叶斯逆方法与条件生成对抗网络、可逆神经网络和条件流匹配三种生成学习模型应用于燃气轮机燃烧室设计问题，提出评估指标，研究训练数据集大小对性能的影响。

Result: 条件流匹配在所有对比方法中表现始终更优。

Conclusion: 条件流匹配是解决燃气轮机燃烧室设计逆问题的最优方法。

Abstract: Generative learning generates high dimensional data based on low dimensional conditions, also called prompts. Therefore, generative learning algorithms are eligible for solving (Bayesian) inverse problems. In this article we compare a traditional Bayesian inverse approach based on a forward regression model and a prior sampled with the Markov Chain Monte Carlo method with three state of the art generative learning models, namely conditional Generative Adversarial Networks, Invertible Neural Networks and Conditional Flow Matching. We apply them to a problem of gas turbine combustor design where we map six independent design parameters to three performance labels. We propose several metrics for the evaluation of this inverse design approaches and measure the accuracy of the labels of the generated designs along with the diversity. We also study the performance as a function of the training dataset size. Our benchmark has a clear winner, as Conditional Flow Matching consistently outperforms all competing approaches.

</details>


### [264] [Agnostic Language Identification and Generation](https://arxiv.org/abs/2601.23258)
*Mikael Møller Høgsgaard,Chirag Pabbaraju*

Main category: cs.LG

TL;DR: 本文放松可实现性假设，在更通用的‘不可知’设置下研究语言识别和生成，获得新特征和接近最优的速率。


<details>
  <summary>Details</summary>
Motivation: 现有语言识别和生成工作通常基于强可实现性假设，本文希望放松该假设进行研究。

Method: 完全放松可实现性假设，不对输入数据分布设限，提出目标来研究语言识别和生成。

Result: 在语言识别和生成问题上获得新颖有趣的特征描述和接近最优的速率。

Conclusion: 在更通用的‘不可知’设置下研究语言识别和生成是可行的，能取得有价值的结果。

Abstract: Recent works on language identification and generation have established tight statistical rates at which these tasks can be achieved. These works typically operate under a strong realizability assumption: that the input data is drawn from an unknown distribution necessarily supported on some language in a given collection. In this work, we relax this assumption of realizability entirely, and impose no restrictions on the distribution of the input data. We propose objectives to study both language identification and generation in this more general "agnostic" setup. Across both problems, we obtain novel interesting characterizations and nearly tight rates.

</details>


### [265] [TEON: Tensorized Orthonormalization Beyond Layer-Wise Muon for Large Language Model Pre-Training](https://arxiv.org/abs/2601.23261)
*Ruijie Zhang,Yequan Zhao,Ziyue Liu,Zhengyang Wang,Dongyang Li,Yupeng Su,Sijia Liu,Zheng Zhang*

Main category: cs.LG

TL;DR: 提出TEON优化器，它是Muon的推广，能跨层进行正交化，实验显示其在不同规模模型上提升训练和验证困惑度且鲁棒性强。


<details>
  <summary>Details</summary>
Motivation: Muon优化器仅在每层独立进行矩阵级梯度正交化，希望提出能跨层进行正交化的优化器。

Method: 将神经网络的梯度建模为结构化高阶张量，提出TEON优化器，给出收敛保证，基于理论分析开发其实例并进行消融实验。

Result: 在GPT和LLaMA风格模型上实验，TEON能持续改善不同规模模型的训练和验证困惑度，在各种近似SVD方案下有强鲁棒性。

Conclusion: TEON是Muon的有效推广，具有更好的性能和鲁棒性。

Abstract: The Muon optimizer has demonstrated strong empirical performance in pre-training large language models by performing matrix-level gradient (or momentum) orthogonalization in each layer independently. In this work, we propose TEON, a principled generalization of Muon that extends orthogonalization beyond individual layers by modeling the gradients of a neural network as a structured higher-order tensor. We present TEON's improved convergence guarantee over layer-wise Muon, and further develop a practical instantiation of TEON based on the theoretical analysis with corresponding ablation. We evaluate our approach on two widely adopted architectures: GPT-style models, ranging from 130M to 774M parameters, and LLaMA-style models, ranging from 60M to 1B parameters. Experimental results show that TEON consistently improves training and validation perplexity across model scales and exhibits strong robustness under various approximate SVD schemes.

</details>


### [266] [Particle-Guided Diffusion Models for Partial Differential Equations](https://arxiv.org/abs/2601.23262)
*Andrew Millard,Fredrik Lindsten,Zheng Zhao*

Main category: cs.LG

TL;DR: 引入带物理引导的随机采样方法，嵌入SMC框架形成可扩展生成 PDE 求解器，误差低于现有方法


<details>
  <summary>Details</summary>
Motivation: 确保扩散模型生成样本符合物理规律，解决 PDE 求解问题

Method: 引入基于 PDE 残差和观测约束的物理引导随机采样方法，并嵌入 SMC 框架

Result: 在多个基准 PDE 系统、多物理和相互作用 PDE 系统上，方法生成的解场数值误差低于现有先进生成方法

Conclusion: 所提方法是一种有效的可扩展生成 PDE 求解方法，能提高求解精度

Abstract: We introduce a guided stochastic sampling method that augments sampling from diffusion models with physics-based guidance derived from partial differential equation (PDE) residuals and observational constraints, ensuring generated samples remain physically admissible. We embed this sampling procedure within a new Sequential Monte Carlo (SMC) framework, yielding a scalable generative PDE solver. Across multiple benchmark PDE systems as well as multiphysics and interacting PDE systems, our method produces solution fields with lower numerical error than existing state-of-the-art generative methods.

</details>


### [267] [FOCUS: DLLMs Know How to Tame Their Compute Bound](https://arxiv.org/abs/2601.23278)
*Kaihua Liang,Xin Tan,An Zhong,Hong Xu,Marco Canini*

Main category: cs.LG

TL;DR: 论文针对DLLMs解码成本高问题提出FOCUS系统，提升了吞吐量并保障生成质量。


<details>
  <summary>Details</summary>
Motivation: Diffusion Large Language Models (DLLMs)部署受高解码成本限制，存在计算资源浪费问题。

Method: 发现DLLM解码计算浪费问题和注意力衍生的令牌重要性与解码概率强相关，提出FOCUS系统，动态聚焦可解码令牌，实时剔除不可解码令牌。

Result: FOCUS比生产级引擎LMDeploy实现了高达3.52倍的吞吐量提升，在多个基准测试中保持或提高了生成质量。

Conclusion: FOCUS系统有效缓解计算限制，实现可扩展吞吐量，代码已开源。

Abstract: Diffusion Large Language Models (DLLMs) offer a compelling alternative to Auto-Regressive models, but their deployment is constrained by high decoding cost. In this work, we identify a key inefficiency in DLLM decoding: while computation is parallelized over token blocks, only a small subset of tokens is decodable at each diffusion step, causing most compute to be wasted on non-decodable tokens. We further observe a strong correlation between attention-derived token importance and token-wise decoding probability. Based on this insight, we propose FOCUS -- an inference system designed for DLLMs. By dynamically focusing computation on decodable tokens and evicting non-decodable ones on-the-fly, FOCUS increases the effective batch size, alleviating compute limitations and enabling scalable throughput. Empirical evaluations demonstrate that FOCUS achieves up to 3.52$\times$ throughput improvement over the production-grade engine LMDeploy, while preserving or improving generation quality across multiple benchmarks. The FOCUS system is publicly available on GitHub: https://github.com/sands-lab/FOCUS.

</details>


### [268] [Decoupled Diffusion Sampling for Inverse Problems on Function Spaces](https://arxiv.org/abs/2601.23280)
*Thomas Y. L. Lin,Jiachen Yao,Lufang Chiang,Julius Berner,Anima Anandkumar*

Main category: cs.LG

TL;DR: 提出用于逆PDE问题的函数空间数据高效、物理感知生成框架DDIS，有数据效率优势且表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有即插即用扩散后验采样器通过联合系数 - 解建模隐式表示物理，需要大量成对监督，存在不足。

Method: 采用解耦设计，无条件扩散学习系数先验，神经算子显式建模前向PDE进行引导，支持解耦退火后验采样。

Result: 理论上证明在训练数据稀缺时避免联合模型的引导衰减失败；实证上在稀疏观测下实现最优性能，在数据仅1%时，l2误差比联合模型有40%优势。

Conclusion: 所提DDIS框架数据效率高，能有效进行物理信息学习，表现优于现有方法。

Abstract: We propose a data-efficient, physics-aware generative framework in function space for inverse PDE problems. Existing plug-and-play diffusion posterior samplers represent physics implicitly through joint coefficient-solution modeling, requiring substantial paired supervision. In contrast, our Decoupled Diffusion Inverse Solver (DDIS) employs a decoupled design: an unconditional diffusion learns the coefficient prior, while a neural operator explicitly models the forward PDE for guidance. This decoupling enables superior data efficiency and effective physics-informed learning, while naturally supporting Decoupled Annealing Posterior Sampling (DAPS) to avoid over-smoothing in Diffusion Posterior Sampling (DPS). Theoretically, we prove that DDIS avoids the guidance attenuation failure of joint models when training data is scarce. Empirically, DDIS achieves state-of-the-art performance under sparse observation, improving $l_2$ error by 11% and spectral error by 54% on average; when data is limited to 1%, DDIS maintains accuracy with 40% advantage in $l_2$ error compared to joint models.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [269] [Investigating the Interplay of Parameterization and Optimizer in Gradient-Free Topology Optimization: A Cantilever Beam Case Study](https://arxiv.org/abs/2601.22241)
*Jelle Westra,Iván Olarte Rodríguez,Niki van Stein,Thomas Bäck,Elena Raponi*

Main category: cs.NE

TL;DR: 研究无梯度黑盒优化（BBO）中几何参数化和优化器对拓扑优化（TO）的影响，发现参数化质量对优化性能影响更大。


<details>
  <summary>Details</summary>
Motivation: 无梯度黑盒优化用于拓扑优化的成功依赖于几何参数化和优化器的选择，需研究二者相互作用。

Method: 通过悬臂梁合规性最小化问题，对三种几何参数化与三种BBO算法在不同维度设计空间进行基准测试。

Result: 参数化质量对优化性能的影响比优化器选择更强，结构良好的参数化能使各算法有稳健和有竞争力的表现，较差的表示会增加对优化器的依赖。

Conclusion: 强调了几何参数化在基于BBO的实际TO中的主导作用，指出不考虑设计空间就无法公平评估算法性能和进行选择。

Abstract: Gradient-free black-box optimization (BBO) is widely used in engineering design and provides a flexible framework for topology optimization (TO), enabling the discovery of high-performing structural designs without requiring gradient information from simulations. Yet, its success depends on two key choices: the geometric parameterization defining the search space and the optimizer exploring it.
  This study investigates this interplay through a compliance minimization problem for a cantilever beam subject to a connectivity constraint. We benchmark three geometric parameterizations, each combined with three representative BBO algorithms: differential evolution, covariance matrix adaptation evolution strategy, and heteroscedastic evolutionary Bayesian optimization, across 10D, 20D, and 50D design spaces.
  Results reveal that parameterization quality has a stronger influence on optimization performance than optimizer choice: a well-structured parameterization enables robust and competitive performance across algorithms, whereas weaker representations increase optimizer dependency. Overall, this study highlights the dominant role of geometric parameterization in practical BBO-based TO and shows that algorithm performance and selection cannot be fairly assessed without accounting for the induced design space.

</details>


### [270] [Fairness-Aware Performance Evaluation for Multi-Party Multi-Objective Optimization](https://arxiv.org/abs/2601.22497)
*Zifan Zhao,Peilan Xu,Wenjian Luo*

Main category: cs.NE

TL;DR: 本文针对多主体多目标优化问题现有评估方法的不足，提出基于广义共识解的公平感知性能评估框架，经实验验证该框架能根据共识公平性考量区分算法性能。


<details>
  <summary>Details</summary>
Motivation: 解决多主体多目标优化问题中传统均值评估方法的不公平性，以及现有最优解定义狭窄，无法体现不同决策者相对收益平衡和共识的问题。

Method: 从合作博弈论视角提出公平评估函数应满足的四条公理，引入让步率向量推广最优解定义，将经典性能指标嵌入基于Nash积的评估框架，并构建含共识缺陷谈判结构的基准问题进行验证。

Result: 实验表明该评估框架能按照共识公平性考量区分算法性能，在有严格公共解时收敛到此类解的算法得分高，无严格公共解时有效覆盖共同可接受区域的算法更受青睐。

Conclusion: 提出的公平感知性能评估框架有效，能更好地评估多主体多目标优化问题的算法性能。

Abstract: In multiparty multiobjective optimization problems, solution sets are usually evaluated using classical performance metrics, aggregated across DMs. However, such mean-based evaluations may be unfair by favoring certain parties, as they assume identical geometric approximation quality to each party's PF carries comparable evaluative significance. Moreover, prevailing notions of MPMOP optimal solutions are restricted to strictly common Pareto optimal solutions, representing a narrow form of cooperation in multiparty decision making scenarios. These limitations obscure whether a solution set reflects balanced relative gains or meaningful consensus among heterogeneous DMs. To address these issues, this paper develops a fairness-aware performance evaluation framework grounded in a generalized notion of consensus solutions. From a cooperative game-theoretic perspective, we formalize four axioms that a fairness-aware evaluation function for MPMOPs should satisfy. By introducing a concession rate vector to quantify acceptable compromises by individual DMs, we generalize the classical definition of MPMOP optimal solutions and embed classical performance metrics into a Nash-product-based evaluation framework, which is theoretically shown to satisfy all axioms. To support empirical validation, we further construct benchmark problems that extend existing MPMOP suites by incorporating consensus-deficient negotiation structures. Experimental results demonstrate that the proposed evaluation framework is able to distinguish algorithmic performance in a manner consistent with consensus-aware fairness considerations. Specifically, algorithms converging toward strictly common solutions are assigned higher evaluation scores when such solutions exist, whereas in the absence of strictly common solutions, algorithms that effectively cover the commonly acceptable region are more favorably evaluated.

</details>


### [271] [Detect and Act: Automated Dynamic Optimizer through Meta-Black-Box Optimization](https://arxiv.org/abs/2601.22542)
*Zijian Gao,Yuanting Zhong,Zeyuan Ma,Yue-Jiao Gong,Hongshu Guo*

Main category: cs.NE

TL;DR: 本文提出强化学习辅助方法用于进化算法自动检测环境变化并自适应，构建测试床验证，结果优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有进化动态优化方法依赖手工自适应策略，在新场景可能效果不佳，需自动化方法。

Method: 借鉴元黑盒优化的双层学习思想，用深度Q网络作为优化动态检测器和搜索策略适配器，训练目标是最大化问题分布上的预期性能增益。

Result: 构建了难易不同的DOPs测试床，基准测试结果显示该方法在解决DOPs时搜索行为灵活，性能优于现有基线。

Conclusion: 所提方法经训练后能对未见DOPs实现自动环境变化检测和自适应，解决DOPs性能良好。

Abstract: Dynamic Optimization Problems (DOPs) are challenging to address due to their complex nature, i.e., dynamic environment variation. Evolutionary Computation methods are generally advantaged in solving DOPs since they resemble dynamic biological evolution. However, existing evolutionary dynamic optimization methods rely heavily on human-crafted adaptive strategy to detect environment variation in DOPs, and then adapt the searching strategy accordingly. These hand-crafted strategies may perform ineffectively at out-of-box scenarios. In this paper, we propose a reinforcement learning-assisted approach to enable automated variation detection and self-adaption in evolutionary algorithms. This is achieved by borrowing the bi-level learning-to-optimize idea from recent Meta-Black-Box Optimization works. We use a deep Q-network as optimization dynamics detector and searching strategy adapter: It is fed as input with current-step optimization state and then dictates desired control parameters to underlying evolutionary algorithms for next-step optimization. The learning objective is to maximize the expected performance gain across a problem distribution. Once trained, our approach could generalize toward unseen DOPs with automated environment variation detection and self-adaption. To facilitate comprehensive validation, we further construct an easy-to-difficult DOPs testbed with diverse synthetic instances. Extensive benchmark results demonstrate flexible searching behavior and superior performance of our approach in solving DOPs, compared to state-of-the-art baselines.

</details>


### [272] [COBRA++: Enhanced COBRA Optimizer with Augmented Surrogate Pool and Reinforced Surrogate Selection](https://arxiv.org/abs/2601.22624)
*Zepei Yu,Zhiyang Huang,Hongshu Guo,Yue-Jiao Gong,Zeyuan Ma*

Main category: cs.NE

TL;DR: 本文提出基于学习的自适应策略 COBRA++ 改进 COBRA 优化器，实验表明 COBRA++ 性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现实世界优化问题给优化算法带来挑战，COBRA 优化器虽有效，但设计空间需人工决定，针对新任务微调费力。

Method: 提出 COBRA++ 从两方面改进 COBRA：一是扩充代理池增强模型多样性和逼近能力；二是采用基于强化学习的在线模型选择策略。

Result: 通过多维验证实验，COBRA++ 对比原始 COBRA 及其自适应变体有显著性能提升，消融实验支持设计组件正确性。

Conclusion: COBRA++ 能有效解决优化问题，在不同属性的约束优化问题中表现良好。

Abstract: The optimization problems in realistic world present significant challenges onto optimization algorithms, such as the expensive evaluation issue and complex constraint conditions. COBRA optimizer (including its up-to-date variants) is a representative and effective tool for addressing such optimization problems, which introduces 1) RBF surrogate to reduce online evaluation and 2) bi-stage optimization process to alternate search for feasible solution and optimal solution. Though promising, its design space, i.e., surrogate model pool and selection standard, is still manually decided by human expert, resulting in labor-intensive fine-tuning for novel tasks. In this paper, we propose a learning-based adaptive strategy (COBRA++) that enhances COBRA in two aspects: 1) An augmented surrogate pool to break the tie with RBF-like surrogate and hence enhances model diversity and approximation capability; 2) A reinforcement learning-based online model selection policy that empowers efficient and accurate optimization process. The model selection policy is trained to maximize overall performance of COBRA++ across a distribution of constrained optimization problems with diverse properties. We have conducted multi-dimensional validation experiments and demonstrate that COBRA++ achieves substantial performance improvement against vanilla COBRA and its adaptive variant. Ablation studies are provided to support correctness of each design component in COBRA++.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [273] [Linux Kernel Recency Matters, CVE Severity Doesn't, and History Fades](https://arxiv.org/abs/2601.22196)
*Piotr Przymus,Witold Weiner,Krzysztof Rykaczewski,Gunnar Kudrjavets*

Main category: cs.SE

TL;DR: 2024年Linux内核成CVE编号机构，分析内核CVE，发现严重程度与补丁延迟关联小，内核更新补丁修复更快。


<details>
  <summary>Details</summary>
Motivation: 了解驱动内核漏洞补丁修复的因素。

Method: 利用元数据、相关提交和补丁延迟分析内核CVE的结构和动态。

Result: 严重程度和CVSS指标与补丁延迟关联小，内核新旧是存活模型的合理预测因素，开发者更快修复新内核，引入漏洞的提交更复杂。

Conclusion: Linux内核是独特的开源项目，其CVE流程也不例外。

Abstract: In 2024, the Linux kernel became its own Common Vulnerabilities and Exposures (CVE) Numbering Authority (CNA), formalizing how kernel vulnerabilities are identified and tracked. We analyze the anatomy and dynamics of kernel CVEs using metadata, associated commits, and patch latency to understand what drives patching. Results show that severity and Common Vulnerability Scoring System (CVSS) metrics have a negligible association with patch latency, whereas kernel recency is a reasonable predictor in survival models. Kernel developers fix newer kernels sooner, while older ones retain unresolved CVEs. Commits introducing vulnerabilities are typically broader and more complex than their fixes, though often only approximate reconstructions of development history. The Linux kernel remains a unique open-source project -- its CVE process is no exception.

</details>


### [274] [Stalled, Biased, and Confused: Uncovering Reasoning Failures in LLMs for Cloud-Based Root Cause Analysis](https://arxiv.org/abs/2601.22208)
*Evelien Riddell,James Riddell,Gengyi Sun,Michał Antkiewicz,Krzysztof Czarnecki*

Main category: cs.SE

TL;DR: 文章聚焦于评估大语言模型（LLMs）在多跳根因分析（RCA）中的推理能力，通过实验明确其成败之处并给出失败分类。


<details>
  <summary>Details</summary>
Motivation: 现代云系统的特性使RCA困难，现有基于LLMs的RCA工作难以区分推理本身与外围设计导致的失败，需评估LLMs推理行为。

Method: 设计简化实验框架，评估六种LLMs在两种代理工作流和非代理基线中的表现，执行48000个模拟故障场景，测量根因准确性和中间推理轨迹质量，制作失败分类。

Result: 明确当前开源LLMs在多跳RCA中的成败，量化对输入数据模式的敏感性，识别预测最终正确性的推理失败。

Conclusion: 研究提供透明可复现的实证结果和失败分类，为推理驱动的系统诊断工作提供指导。

Abstract: Root cause analysis (RCA) is essential for diagnosing failures within complex software systems to ensure system reliability. The highly distributed and interdependent nature of modern cloud-based systems often complicates RCA efforts, particularly for multi-hop fault propagation, where symptoms appear far from their true causes. Recent advancements in Large Language Models (LLMs) present new opportunities to enhance automated RCA. However, their practical value for RCA depends on the fidelity of reasoning and decision-making. Existing work relies on historical incident corpora, operates directly on high-volume telemetry beyond current LLM capacity, or embeds reasoning inside complex multi-agent pipelines -- conditions that obscure whether failures arise from reasoning itself or from peripheral design choices.
  We present a focused empirical evaluation that isolates an LLM's reasoning behavior. We design a controlled experimental framework that foregrounds the LLM by using a simplified experimental setting. We evaluate six LLMs under two agentic workflows (ReAct and Plan-and-Execute) and a non-agentic baseline on two real-world case studies (GAIA and OpenRCA). In total, we executed 48,000 simulated failure scenarios, totaling 228 days of execution time. We measure both root-cause accuracy and the quality of intermediate reasoning traces. We produce a labeled taxonomy of 16 common RCA reasoning failures and use an LLM-as-a-Judge for annotation. Our results clarify where current open-source LLMs succeed and fail in multi-hop RCA, quantify sensitivity to input data modalities, and identify reasoning failures that predict final correctness. Together, these contributions provide transparent and reproducible empirical results and a failure taxonomy to guide future work on reasoning-driven system diagnosis.

</details>


### [275] [Predicting Intermittent Job Failure Categories for Diagnosis Using Few-Shot Fine-Tuned Language Models](https://arxiv.org/abs/2601.22264)
*Henri Aïdasso,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: 提出FlaXifyer预测间歇性作业失败类别，及LogSift解释技术，实现自动化分诊加速故障诊断。


<details>
  <summary>Details</summary>
Motivation: 现有CI管道间歇性作业失败导致效率低下，先前工作未解决诊断挑战。

Method: 引入FlaXifyer，用预训练语言模型进行少样本学习预测失败类别；提出LogSift识别有影响的日志语句。

Result: FlaXifyer仅需作业执行日志，特定准确率高；LogSift识别日志快，减少审查工作，多数情况能呈现相关故障信息。

Conclusion: FlaXifyer和LogSift能实现有效自动化分诊，加速故障诊断，为自动解决间歇性作业失败奠定基础。

Abstract: In principle, Continuous Integration (CI) pipeline failures provide valuable feedback to developers on code-related errors. In practice, however, pipeline jobs often fail intermittently due to non-deterministic tests, network outages, infrastructure failures, resource exhaustion, and other reliability issues. These intermittent (flaky) job failures lead to substantial inefficiencies: wasted computational resources from repeated reruns and significant diagnosis time that distracts developers from core activities and often requires intervention from specialized teams. Prior work has proposed machine learning techniques to detect intermittent failures, but does not address the subsequent diagnosis challenge. To fill this gap, we introduce FlaXifyer, a few-shot learning approach for predicting intermittent job failure categories using pre-trained language models. FlaXifyer requires only job execution logs and achieves 84.3% Macro F1 and 92.0% Top-2 accuracy with just 12 labeled examples per category. We also propose LogSift, an interpretability technique that identifies influential log statements in under one second, reducing review effort by 74.4% while surfacing relevant failure information in 87% of cases. Evaluation on 2,458 job failures from TELUS demonstrates that FlaXifyer and LogSift enable effective automated triage, accelerate failure diagnosis, and pave the way towards the automated resolution of intermittent job failures.

</details>


### [276] [PriviSense: A Frida-Based Framework for Multi-Sensor Spoofing on Android](https://arxiv.org/abs/2601.22414)
*Ibrahim Khalilov,Chaoran Chen,Ziang Xiao,Tianshi Li,Toby Jia-Jun Li,Yaxing Yao*

Main category: cs.SE

TL;DR: 提出PriviSense工具，可在根权限安卓设备上运行时伪造传感器和系统信号，支持可重复实验和应用逻辑测试。


<details>
  <summary>Details</summary>
Motivation: 现有模拟器和插桩构建方法难以支持在物理设备上对上下文敏感的应用行为进行可重复测试。

Method: 基于Frida开发PriviSense工具，可将时变传感器流和系统值注入未修改的应用。

Result: 在五个代表性传感器可视化应用中验证了实时伪造功能。

Conclusion: PriviSense支持可脚本化和可逆操作，便于应用逻辑测试、发现基于上下文的行为和隐私分析，代码按需与验证研究者共享。

Abstract: Mobile apps increasingly rely on real-time sensor and system data to adapt their behavior to user context. While emulators and instrumented builds offer partial solutions, they often fail to support reproducible testing of context-sensitive app behavior on physical devices. We present PriviSense, a Frida-based, on-device toolkit for runtime spoofing of sensor and system signals on rooted Android devices. PriviSense can script and inject time-varying sensor streams (accelerometer, gyroscope, step counter) and system values (battery level, system time, device metadata) into unmodified apps, enabling reproducible on-device experiments without emulators or app rewrites. Our demo validates real-time spoofing on a rooted Android device across five representative sensor-visualization apps. By supporting scriptable and reversible manipulation of these values, PriviSense facilitates testing of app logic, uncovering of context-based behaviors, and privacy-focused analysis. To ensure ethical use, the code is shared upon request with verified researchers.
  Tool Guide: How to Run PriviSense on Rooted Android https://bit.ly/privisense-guide Demonstration video: https://www.youtube.com/watch?v=4Qwnogcc3pw

</details>


### [277] [Small is Beautiful: A Practical and Efficient Log Parsing Framework](https://arxiv.org/abs/2601.22590)
*Minxing Wang,Yintong Huo*

Main category: cs.SE

TL;DR: 为解决小模型在日志解析中性能不佳问题，提出EFParser，通过架构创新提升小模型能力，在公共数据集上表现出色，计算高效。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型（LLMs）的日志解析器效果依赖模型规模，使用小模型时性能下降，数据隐私和计算限制要求使用小模型，需提升小模型该场景下的能力。

Method: 提出EFParser，引入带自适应更新机制的双缓存系统区分新旧模板模式，合并冗余模板、修正错误；设专用校正模块验证和细化LLM生成的模板。

Result: 在公共大规模数据集评估显示，EFParser在小LLMs上运行时，各指标平均比现有基线模型高12.5%，甚至超越部分使用大模型的基线。

Conclusion: EFParser虽有额外验证步骤，但计算效率高，为实际日志分析部署提供了可靠且实用的解决方案。

Abstract: Log parsing is a fundamental step in log analysis, partitioning raw logs into constant templates and dynamic variables. While recent semantic-based parsers leveraging Large Language Models (LLMs) exhibit superior generalizability over traditional syntax-based methods, their effectiveness is heavily contingent on model scale. This dependency leads to significant performance collapse when employing smaller, more resource-efficient LLMs. Such degradation creates a major barrier to real-world adoption, where data privacy requirements and computational constraints necessitate the use of succinct models. To bridge this gap, we propose EFParser, an unsupervised LLM-based log parser designed to enhance the capabilities of smaller models through systematic architectural innovation. EFParser introduces a dual-cache system with an adaptive updating mechanism that distinguishes between novel patterns and variations of existing templates. This allows the parser to merge redundant templates and rectify prior errors, maintaining cache consistency. Furthermore, a dedicated correction module acts as a gatekeeper, validating and refining every LLM-generated template before caching to prevent error injection. Empirical evaluations on public large-scale datasets demonstrate that EFParser outperforms state-of-the-art baselines by an average of 12.5% across all metrics when running on smaller LLMs, even surpassing some baselines utilizing large-scale models. Despite its additional validation steps, EFParser maintains high computational efficiency, offering a robust and practical solution for real-world log analysis deployment.

</details>


### [278] [TimeMachine-bench: A Benchmark for Evaluating Model Capabilities in Repository-Level Migration Tasks](https://arxiv.org/abs/2601.22597)
*Ryo Fujii,Makoto Morishita,Kazuki Yano,Jun Suzuki*

Main category: cs.SE

TL;DR: 本文介绍了用于评估现实世界Python项目软件迁移的TimeMachine - bench基准，评估了基于11种模型的基线，发现大语言模型在迁移任务中存在可靠性挑战。


<details>
  <summary>Details</summary>
Motivation: 随着自动化软件工程发展，软件迁移这一关键过程被忽视，需要一个评估软件迁移的基准。

Method: 构建TimeMachine - bench基准，其包含因依赖更新测试开始失败的GitHub仓库，构建过程自动化且可实时更新，还整理了人工验证子集；在验证子集上评估基于11种模型的基于代理的基线。

Result: 大语言模型在迁移任务中虽有一定前景，但面临显著的可靠性挑战，如利用低测试覆盖率的虚假解决方案和次优工具使用策略导致的不必要编辑。

Conclusion: 提供了TimeMachine - bench基准用于评估软件迁移，指出大语言模型在软件迁移任务中可靠性有待提高。

Abstract: With the advancement of automated software engineering, research focus is increasingly shifting toward practical tasks reflecting the day-to-day work of software engineers. Among these tasks, software migration, a critical process of adapting code to evolving environments, has been largely overlooked. In this study, we introduce TimeMachine-bench, a benchmark designed to evaluate software migration in real-world Python projects. Our benchmark consists of GitHub repositories whose tests begin to fail in response to dependency updates. The construction process is fully automated, enabling live updates of the benchmark. Furthermore, we curated a human-verified subset to ensure problem solvability. We evaluated agent-based baselines built on top of 11 models, including both strong open-weight and state-of-the-art LLMs on this verified subset. Our results indicated that, while LLMs show some promise for migration tasks, they continue to face substantial reliability challenges, including spurious solutions that exploit low test coverage and unnecessary edits stemming from suboptimal tool-use strategies. Our dataset and implementation are available at https://github.com/tohoku-nlp/timemachine-bench.

</details>


### [279] [Elderly HealthMag: Systematic Building and Calibrating a Tool for Identifying and Evaluating Senior User Digital Health Software](https://arxiv.org/abs/2601.22627)
*Yuqing Xiao,John Grundy,Anuradha Madugalla,Elizabeth Manias*

Main category: cs.SE

TL;DR: 现有数字健康软件常因对用户假设错误而缺乏包容性，提出HealthMag工具及Elderly HealthMag双镜头方法，并展示其在识别老年数字健康应用包容性偏差的作用。


<details>
  <summary>Details</summary>
Motivation: 解决数字健康软件开发团队对有健康状况的用户存在错误假设，导致软件缺乏包容性的问题。

Method: 按照InclusiveMag框架通过系统映射和校准开发HealthMag，将其与校准版AgeMag方法集成形成Elderly HealthMag双镜头方法，通过认知演练展示应用。

Result: 能识别当前面向老年用户的数字健康应用中的包容性偏差。

Conclusion: HealthMag和Elderly HealthMag有助于更好地引出、建模和评估数字健康软件的需求。

Abstract: Digital health (DH) software is increasingly deployed to populations where many end users live with one or more health conditions. Yet, DH software development teams frequently operate using implicit, incorrect assumptions about these users, resulting in products that under-serve the specific requirements imposed by their age and health conditions. Consequently, while software may meet clinical objectives on paper, it often fails to be inclusive during actual user interaction. To address this, we propose \textbf{\textit{HealthMag}}, a tool inspired by GenderMag designed to help better elicit, model and evaluate requirements for digital health software. We developed HealthMag through systematic mapping and calibration following the InclusiveMag framework. Furthermore, we integrated this with a calibrated version of an existing AgeMag method to create a dual-lens approach: \textbf{\textit{Elderly HealthMag}}, designed to aid requirements, design and evaluation of mHealth software for senior end users. We demonstrate application and utility of Age HealthMag via cognitive walkthroughs in identifying inclusivity biases in current senior user-oriented digital health applications.

</details>


### [280] [From Horizontal Layering to Vertical Integration: A Comparative Study of the AI-Driven Software Development Paradigm](https://arxiv.org/abs/2601.22667)
*Chi Zhang,Zehan Li,Ziqian Zhong,Haibing Ma,Dan Xiao,Chen Lin,Ming Dong*

Main category: cs.SE

TL;DR: 通过多案例对比研究生成式AI在软件工程中的组织影响，对比传统企业和AI原生初创企业，发现垂直整合可大幅降低资源消耗，提出相关理论和管理策略。


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI在软件工程中应用的组织影响。

Method: 多案例对比研究，对比传统企业和AI原生初创企业两种开发环境。

Result: 从水平分层到垂直整合使资源消耗降低8到33倍，出现跨越传统角色界限的超级员工，消除跨职能协调成本；提出人机协作效能为工程组织优化目标，发现AI扭曲效应。

Conclusion: 给出组织重新设计的管理策略，如激活资深工程师闲置认知带宽、抑制盲目规模扩张。

Abstract: This paper examines the organizational implications of Generative AI adoption in software engineering through a multiple-case comparative study. We contrast two development environments: a traditional enterprise (brownfield) and an AI-native startup (greenfield). Our analysis reveals that transitioning from Horizontal Layering (functional specialization) to Vertical Integration (end-to-end ownership) yields 8-fold to 33-fold reductions in resource consumption. We attribute these gains to the emergence of Super Employees, AI-augmented engineers who span traditional role boundaries, and the elimination of inter-functional coordination overhead. Theoretically, we propose Human-AI Collaboration Efficacy as the primary optimization target for engineering organizations, supplanting individual productivity metrics. Our Total Factor Productivity analysis identifies an AI Distortion Effect that diminishes returns to labor scale while amplifying technological leverage. We conclude with managerial strategies for organizational redesign, including the reactivation of idle cognitive bandwidth in senior engineers and the suppression of blind scale expansion.

</details>


### [281] [VarParser: Unleashing the Neglected Power of Variables for LLM-based Log Parsing](https://arxiv.org/abs/2601.22676)
*Jinrui Sun,Tong Jia,Minghua He,Ying Li*

Main category: cs.SE

TL;DR: 现有基于大语言模型的日志解析器以常量为中心，存在诸多问题，本文提出以变量为中心的VarParser策略，在大规模数据集上验证其准确性高、效率提升且成本降低。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的日志解析器采用常量中心策略，存在日志分组和采样效率低、调用大语言模型次数多、成本高以及丢失变量信息等问题。

Method: 提出VarParser策略，包括变量贡献采样、以变量为中心的解析缓存和自适应变量感知上下文学习，引入变量单元。

Result: 在大规模数据集上评估表明，VarParser比现有方法准确性更高，显著提高解析效率并降低大语言模型调用成本。

Conclusion: VarParser策略有效解决了现有日志解析器的问题，在准确性、效率和成本方面表现更优。

Abstract: Logs serve as a primary source of information for engineers to diagnose failures in large-scale online service systems. Log parsing, which extracts structured events from massive unstructured log data, is a critical first step for downstream tasks like anomaly detection and failure diagnosis. With advances in large language models (LLMs), leveraging their strong text understanding capabilities has proven effective for accurate log parsing. However, existing LLM-based log parsers all focus on the constant part of logs, ignoring the potential contribution of the variable part to log parsing. This constant-centric strategy brings four key problems. First, inefficient log grouping and sampling with only constant information. Second, a relatively large number of LLM invocations due to constant-based cache, leading to low log parsing accuracy and efficiency. Third, a relatively large number of consumed constant tokens in prompts leads to high LLM invocation costs. At last, these methods only retain placeholders in the results, losing the system visibility brought by variable information in logs.
  Facing these problems, we propose a variable-centric log parsing strategy named VarParser. Through variable contribution sampling, variable-centric parsing cache, and adaptive variable-aware in-context learning, our approach can efficiently capture the variable parts of logs and leverage their contributions to parsing. By introducing variable units, we preserve rich variable information, enhancing the integrity of log parsing results. Extensive evaluations on large-scale datasets demonstrate that VarParser achieves higher accuracy compared to existing methods, significantly improving parsing efficiency while reducing the LLM invocation costs.

</details>


### [282] [AutoMerge: Search-Based Model Merging Framework for Effective Model Reuse](https://arxiv.org/abs/2601.22748)
*You Lu,Jiyang Zhang,Bihuan Chen,Chaofeng Sha,Dingji Wang,Xin Peng*

Main category: cs.SE

TL;DR: 本文研究模型合并技术在不同架构深度学习模型跨领域应用，发现现有技术效果不佳，提出AutoMerge框架。


<details>
  <summary>Details</summary>
Motivation: 现有研究未系统探究模型合并技术在不同架构深度学习模型跨领域的应用，为填补此空白开展研究。

Method: 对三种领域（大语言模型、图像分类、自动驾驶）的三种不同模型架构评估五种模型合并技术，提出AutoMerge框架，先将复杂模型分割成多个异构块，再系统探索合并空间。

Result: 直接应用现有模型合并技术结果不一致，难以处理模型异构结构属性，对超参数配置敏感。

Conclusion: 现有模型合并技术在不同架构深度学习模型跨领域应用受限，AutoMerge框架有潜力解决这些问题。

Abstract: Software reuse has long been recognized as a critical and widely studied topic in software engineering, offering substantial benefits in reducing development costs, improving software quality, and enhancing operational efficiency. This paradigm extends into deep learning through model reuse. Recently, model merging has emerged in the domain of large language models (LLMs) as a training-free approach that takes multiple task-specific models with the same architecture as source models and merges them without retraining, enhancing model reuse within LLMs. However, no prior work has systematically investigated whether such an approach can be effectively applied to other deep learning models with different architectures across domains. To bridge this gap, we present the first systematic study that evaluates five model merging techniques on three distinct model architectures across three domains: LLMs, image classification, and autonomous driving. Our findings reveal that directly applying existing model merging techniques leads to highly inconsistent results and falls notably short of their success within LLMs. Moreover, a single model merging technique often fails to handle the heterogeneous structural properties within a model, limiting its applicability to different model architectures across domains. Furthermore, the effectiveness of model merging techniques is highly sensitive to hyperparameter configurations, thereby constraining their potential for broader adoption. Inspired by these insights, we propose AutoMerge, a novel search-based model merging framework that first segments complex models into multiple heterogeneous blocks and then systematically explores the merging space to identify the merging technique and its hyperparameter configuration.

</details>


### [283] [Constructing Safety Cases for AI Systems: A Reusable Template Framework](https://arxiv.org/abs/2601.22773)
*Sung Une Lee,Liming Zhu,Md Shamsujjoha,Liming Dong,Qinghua Lu,Jieshan Chen*

Main category: cs.SE

TL;DR: 本文探讨AI系统安全案例构建，指出传统方法不适用于现代AI，提出可复用模板框架，实现安全案例系统构建与维护。


<details>
  <summary>Details</summary>
Motivation: 传统安全案例实践不适用于现代AI系统，需研究适合AI系统的安全案例构建方法。

Method: 提出可复用的安全案例模板框架，包含AI特定的声明、论证和证据分类。

Result: 得到一种系统、可组合、可复用的方法，用于构建和维护可信、可审计且适应AI系统行为变化的安全案例。

Conclusion: 所提框架能有效解决AI系统安全案例构建问题，适应其不断变化的行为。

Abstract: Safety cases, structured arguments that a system is acceptably safe, are becoming central to the governance of AI systems. Yet, traditional safety-case practices from aviation or nuclear engineering rely on well-specified system boundaries, stable architectures, and known failure modes. Modern AI systems such as generative and agentic AI are the opposite. Their capabilities emerge unpredictably from low-level training objectives, their behaviour varies with prompts, and their risk profiles shift through fine-tuning, scaffolding, or deployment context. This study examines how safety cases are currently constructed for AI systems and why classical approaches fail to capture these dynamics. It then proposes a framework of reusable safety-case templates, each following a predefined structure of claims, arguments, and evidence tailored for AI systems. The framework introduces comprehensive taxonomies for AI-specific claim types (assertion-based, constrained-based, capability-based), argument types (demonstrative, comparative, causal/explanatory, risk-based, and normative), and evidence families (empirical, mechanistic, comparative, expert-driven, formal methods, operational/field data, and model-based). Each template is illustrated through end-to-end patterns addressing distinctive challenges such as evaluation without ground truth, dynamic model updates, and threshold-based risk decisions. The result is a systematic, composable, and reusable approach to constructing and maintaining safety cases that are credible, auditable, and adaptive to the evolving behaviour of generative and frontier AI systems.

</details>


### [284] [Understanding on the Edge: LLM-generated Boundary Test Explanations](https://arxiv.org/abs/2601.22791)
*Sabinakhon Akbarova,Felix Dobslaw,Robert Feldt*

Main category: cs.SE

TL;DR: 研究探索了LLM生成边界解释的价值，通过调查和访谈收集软件专业人员的评价，得出积极结果并提炼出需求清单，表明LLM工具经改进可支持测试工作流。


<details>
  <summary>Details</summary>
Motivation: 测试人员难以理解和证明某些输入 - 输出对代表有意义的行为边界，LLM可提供自然语言解释，但价值未被实证评估。

Method: 进行探索性研究，对27名软件专业人员开展调查，让他们对GPT - 4.1为20对边界对的解释进行评分，对其中6人进行后续访谈。

Result: 63.5%的评价为积极，17%为消极，参与者偏好结构清晰、引用权威来源且适配读者专业知识的解释，强调需可操作的示例。

Conclusion: 经进一步改进，基于LLM的工具可使边界解释更具可操作性和可信度，从而支持测试工作流。

Abstract: Boundary value analysis and testing (BVT) is fundamental in software quality assurance because faults tend to cluster at input extremes, yet testers often struggle to understand and justify why certain input-output pairs represent meaningful behavioral boundaries. Large Language Models (LLMs) could help by producing natural-language rationales, but their value for BVT has not been empirically assessed. We therefore conducted an exploratory study on LLM-generated boundary explanations: in a survey, twenty-seven software professionals rated GPT-4.1 explanations for twenty boundary pairs on clarity, correctness, completeness and perceived usefulness, and six of them elaborated in follow-up interviews. Overall, 63.5% of all ratings were positive (4-5 on a five-point Likert scale) compared to 17% negative (1-2), indicating general agreement but also variability in perceptions. Participants favored explanations that followed a clear structure, cited authoritative sources, and adapted their depth to the reader's expertise; they also stressed the need for actionable examples to support debugging and documentation. From these insights, we distilled a seven-item requirement checklist that defines concrete design criteria for future LLM-based boundary explanation tools. The results suggest that, with further refinement, LLM-based tools can support testing workflows by making boundary explanations more actionable and trustworthy.

</details>


### [285] [Just-in-Time Catching Test Generation at Meta](https://arxiv.org/abs/2601.22832)
*Matthew Becker,Yifei Chen,Nicholas Cochran,Pouyan Ghasemi,Abhishek Gulati,Mark Harman,Zachary Haluza,Mehrdad Honarkhah,Herve Robert,Jiacheng Liu,Weini Liu,Sreeja Thummala,Xiaoning Yang,Rui Xin,Sophie Zeng*

Main category: cs.SE

TL;DR: 本文介绍Meta的即时捕获测试生成方法，其能防止大规模后端系统出现错误，能有效减少误报的开发负担，具有可扩展性和工业适用性。


<details>
  <summary>Details</summary>
Motivation: 防止大规模后端系统（数亿行代码）出现错误，减少误报测试失败带来的开发负担。

Method: 采用代码变更感知方法生成候选捕获测试，使用基于规则和基于大语言模型的评估器处理误报。

Result: 代码变更感知方法使候选捕获生成效果比强化测试高4倍、比巧合失败测试高20倍；评估器减少70%人工审核负担；41个候选捕获中有8个是真阳性，4个若未捕获会导致严重故障。

Conclusion: 即时捕获测试具有可扩展性和工业适用性，可防止严重故障进入生产环境。

Abstract: We report on Just-in-Time catching test generation at Meta, designed to prevent bugs in large scale backend systems of hundreds of millions of line of code. Unlike traditional hardening tests, which pass at generation time, catching tests are meant to fail, surfacing bugs before code lands. The primary challenge is to reduce development drag from false positive test failures. Analyzing 22,126 generated tests, we show code-change-aware methods improve candidate catch generation 4x over hardening tests and 20x over coincidentally failing tests. To address false positives, we use rule-based and LLM-based assessors. These assessors reduce human review load by 70%. Inferential statistical analysis showed that human-accepted code changes are assessed to have significantly more false positives, while human-rejected changes have significantly more true positives. We reported 41 candidate catches to engineers; 8 were confirmed to be true positives, 4 of which would have led to serious failures had they remained uncaught. Overall, our results show that Just-in-Time catching is scalable, industrially applicable, and that it prevents serious failures from reaching production.

</details>


### [286] [MEnvAgent: Scalable Polyglot Environment Construction for Verifiable Software Engineering](https://arxiv.org/abs/2601.22859)
*Chuanzhe Guo,Jingjing Wu,Sijun He,Yang Chen,Zhaoqi Kuang,Shilong Fan,Bingjin Chen,Siqi Bao,Jing Liu,Hua Wu,Qingfu Zhu,Wanxiang Che,Haifeng Wang*

Main category: cs.SE

TL;DR: 本文提出MEnvAgent多语言自动化环境构建框架，可生成可验证任务实例，在MEnvBench上表现出色，还构建了MEnvData - SWE数据集。


<details>
  <summary>Details</summary>
Motivation: 大语言模型代理用于软件工程受可验证数据集稀缺限制，原因是跨多种语言构建可执行环境复杂。

Method: 采用多智能体规划 - 执行 - 验证架构解决构建失败问题，集成环境重用机制降低计算开销。

Result: 在MEnvBench上，MEnvAgent比基线表现好，失败到成功率提高8.6%，时间成本降低43%，构建了最大开源多语言可验证Docker环境数据集MEnvData - SWE。

Conclusion: MEnvAgent在软件工程任务中能提升多种模型的性能，代码、基准和数据集开放。

Abstract: The evolution of Large Language Model (LLM) agents for software engineering (SWE) is constrained by the scarcity of verifiable datasets, a bottleneck stemming from the complexity of constructing executable environments across diverse languages. To address this, we introduce MEnvAgent, a Multi-language framework for automated Environment construction that facilitates scalable generation of verifiable task instances. MEnvAgent employs a multi-agent Planning-Execution-Verification architecture to autonomously resolve construction failures and integrates a novel Environment Reuse Mechanism that reduces computational overhead by incrementally patching historical environments. Evaluations on MEnvBench, a new benchmark comprising 1,000 tasks across 10 languages, demonstrate that MEnvAgent outperforms baselines, improving Fail-to-Pass (F2P) rates by 8.6% while reducing time costs by 43%. Additionally, we demonstrate the utility of MEnvAgent by constructing MEnvData-SWE, the largest open-source polyglot dataset of realistic verifiable Docker environments to date, alongside solution trajectories that enable consistent performance gains on SWE tasks across a wide range of models. Our code, benchmark, and dataset are available at https://github.com/ernie-research/MEnvAgent.

</details>


### [287] [AnoMod: A Dataset for Anomaly Detection and Root Cause Analysis in Microservice Systems](https://arxiv.org/abs/2601.22881)
*Ke Ping,Hamza Bin Mazhar,Yuqing Wang,Ying Song,Mika V. Mäntylä*

Main category: cs.SE

TL;DR: 论文针对微服务系统缺乏高质量公开异常检测和根因分析数据集的问题，基于两个开源微服务系统构建了多模态异常数据集AnoMod，可用于跨模态异常检测评估和细粒度根因分析。


<details>
  <summary>Details</summary>
Motivation: 微服务系统社区缺乏高质量公开数据集用于异常检测和根因分析，现有基准存在局限性。

Method: 基于SocialNetwork和TrainTicket两个开源微服务系统，设计并注入四类异常，收集五种模态数据构建AnoMod数据集。

Result: 成功构建了AnoMod数据集。

Conclusion: AnoMod数据集可用于评估跨模态异常检测和融合/消融策略，支持细粒度根因分析和端到端故障排查。

Abstract: Microservice systems (MSS) have become a predominant architectural style for cloud services. Yet the community still lacks high-quality, publicly available datasets for anomaly detection (AD) and root cause analysis (RCA) in MSS. Most benchmarks emphasize performance-related faults and provide only one or two monitoring modalities, limiting research on broader failure modes and cross-modal methods. To address these gaps, we introduce a new multimodal anomaly dataset built on two open-source microservice systems: SocialNetwork and TrainTicket. We design and inject four categories of anomalies (Ano): performance-level, service-level, database-level, and code-level, to emulate realistic anomaly modes. For each scenario, we collect five modalities (Mod): logs, metrics, distributed traces, API responses, and code coverage reports, offering a richer, end-to-end view of system state and inter-service interactions. We name our dataset, reflecting its unique properties, as AnoMod. This dataset enables (1) evaluation of cross-modal anomaly detection and fusion/ablation strategies, and (2) fine-grained RCA studies across service and code regions, supporting end-to-end troubleshooting pipelines that jointly consider detection and localization.

</details>


### [288] [A Serverless Edge-Native Data Processing Architecture for Autonomous Driving Training](https://arxiv.org/abs/2601.22919)
*Fabian Bally,Michael Schötz,Thomas Limbrunner*

Main category: cs.SE

TL;DR: 本文介绍Lambda框架用于自动驾驶车载数据过滤处理，评估显示其性能有竞争力、可降低延迟和抖动，支持实时数据处理。


<details>
  <summary>Details</summary>
Motivation: 数据是自动驾驶机器学习的关键推动者和主要瓶颈，有效模型训练需大量且覆盖平衡的数据，捕捉关键场景事件需长时间驾驶和高效选择。

Method: 引入Lambda框架，通过用户定义函数实现车载数据过滤处理，采用FaaS原则适应资源受限汽车环境，与ROS 2和现有数据记录管道兼容。

Result: 在NVIDIA Jetson Orin Nano上评估，与原生ROS 2部署对比，显示出有竞争力的性能、降低的延迟和抖动。

Conclusion: 基于lambda的抽象可支持嵌入式自动驾驶系统中的实时数据处理。

Abstract: Data is both the key enabler and a major bottleneck for machine learning in autonomous driving. Effective model training requires not only large quantities of sensor data but also balanced coverage that includes rare yet safety-critical scenarios. Capturing such events demands extensive driving time and efficient selection. This paper introduces the Lambda framework, an edge-native platform that enables on-vehicle data filtering and processing through user-defined functions. The framework provides a serverless-inspired abstraction layer that separates application logic from low-level execution concerns such as scheduling, deployment, and isolation. By adapting Function-as-a-Service (FaaS) principles to resource-constrained automotive environments, it allows developers to implement modular, event-driven filtering algorithms while maintaining compatibility with ROS 2 and existing data recording pipelines. We evaluate the framework on an NVIDIA Jetson Orin Nano and compare it against native ROS 2 deployments. Results show competitive performance, reduced latency and jitter, and confirm that lambda-based abstractions can support real-time data processing in embedded autonomous driving systems. The source code is available at https://github.com/LASFAS/jblambda.

</details>


### [289] [Sifting the Noise: A Comparative Study of LLM Agents in Vulnerability False Positive Filtering](https://arxiv.org/abs/2601.22952)
*Yunpeng Xiong,Ting Zhang*

Main category: cs.SE

TL;DR: 研究对比三种基于大语言模型的代理框架用于过滤SAST误报的效果，指出这些代理有强大效果但表现不均，实际部署需综合考虑多种因素。


<details>
  <summary>Details</summary>
Motivation: SAST工具产生大量误报增加人工筛选负担，不同基于大语言模型的代理架构在过滤误报方面的效果不明。

Method: 对Aider、OpenHands和SWE - agent三种框架，用OWASP Benchmark和真实开源Java项目的漏洞进行评估。

Result: 基于大语言模型的代理能去除大部分SAST噪音，在不同数据集上有不同的误报识别率，效果受骨干模型和CWE影响，且降低误报可能以牺牲真实漏洞为代价，不同代理框架计算成本差异大。

Conclusion: 基于大语言模型的代理是过滤SAST误报的强大但不均匀的解决方案，实际部署需综合考量多种因素。

Abstract: Static Application Security Testing (SAST) tools are essential for identifying software vulnerabilities, but they often produce a high volume of false positives (FPs), imposing a substantial manual triage burden on developers. Recent advances in Large Language Model (LLM) agents offer a promising direction by enabling iterative reasoning, tool use, and environment interaction to refine SAST alerts. However, the comparative effectiveness of different LLM-based agent architectures for FP filtering remains poorly understood. In this paper, we present a comparative study of three state-of-the-art LLM-based agent frameworks, i.e., Aider, OpenHands, and SWE-agent, for vulnerability FP filtering. We evaluate these frameworks using the vulnerabilities from the OWASP Benchmark and real-world open-source Java projects. The experimental results show that LLM-based agents can remove the majority of SAST noise, reducing an initial FP detection rate of over 92% on the OWASP Benchmark to as low as 6.3% in the best configuration. On real-world dataset, the best configuration of LLM-based agents can achieve an FP identification rate of up to 93.3% involving CodeQL alerts. However, the benefits of agents are strongly backbone- and CWE-dependent: agentic frameworks significantly outperform vanilla prompting for stronger models such as Claude Sonnet 4 and GPT-5, but yield limited or inconsistent gains for weaker backbones. Moreover, aggressive FP reduction can come at the cost of suppressing true vulnerabilities, highlighting important trade-offs. Finally, we observe large disparities in computational cost across agent frameworks. Overall, our study demonstrates that LLM-based agents are a powerful but non-uniform solution for SAST FP filtering, and that their practical deployment requires careful consideration of agent design, backbone model choice, vulnerability category, and operational cost.

</details>


### [290] [SWE-Manager: Selecting and Synthesizing Golden Proposals Before Coding](https://arxiv.org/abs/2601.22956)
*Boyin Tan,Haoning Deng,Junyuan Zhang,Junjielong Xu,Pinjia He,Youcheng Sun*

Main category: cs.SE

TL;DR: 本文聚焦软件工程中修复方案选择问题，提出SWE - Manager方法，在基准测试中表现优于基线模型，并设计P2A框架评估其在实际问题解决中的有效性。


<details>
  <summary>Details</summary>
Motivation: 实际中团队需从多个候选方案中选一个实施，好的选择可降低风险、使问题解决更可靠，目前软件工程领域大模型研究多集中在代码生成和bug修复等，缺乏对方案选择的研究。

Method: 首先手动研究实际问题以明确维护者选择方案的依据，然后引入SWE - Manager，这是一个通过强化学习训练的8B模型，将方案选择视为推理任务；还设计了P2A框架模拟实际工作流程评估SWE - Manager在实际问题解决中的效果。

Result: 在SWE - Lancer Manager基准测试中，SWE - Manager选择准确率达53.21，赚取率达57.75，赚取152,750美元，优于包括GPT - 5在内的强基线模型。

Conclusion: SWE - Manager在修复方案选择和合成方面表现出色，能有效用于实际问题解决。

Abstract: Large language model (LLM) research in software engineering has largely focused on tasks such as code generation and bug repair. In practice, teams often draft multiple candidate proposals for fixing an issue and then deliberate on one golden proposal for implementation. This selection requires not only assessing the issue's scope, impact, and urgency, but also a clear understanding of each proposal's strengths and weaknesses. A good selection could make issue resolution more reliable while reducing regression and operational risk, whereas a poor choice can increase risk and even cause unpredictable failures.
  We first conduct a manual study of real-world issues to characterize the rationales maintainers use when selecting among competing proposals. Motivated by these findings, we introduce SWE-Manager, a joint selection and synthesis approach that selects the best proposal and synthesizes a golden proposal. SWE-Manager is an 8B model trained via reinforcement learning (RL) to compare proposals, justify its choice, and synthesize a golden proposal for implementation. We view proposal selection as a reasoning task, mirroring how technical managers review competing proposals by weighing issue context and each proposal's solution without executing code or running tests. On the SWE-Lancer Manager benchmark, SWE-Manager achieves 53.21 selection accuracy and 57.75 earn rate, earning 152,750 dollars and outperforming strong baselines including GPT-5. To further evaluate the effectiveness of SWE-Manager in real-world issue resolution, we design the P2A framework, which simulates a real-world workflow where multiple proposals are drafted, reviewed, and a golden proposal is selected for implementation ...

</details>


### [291] [SolAgent: A Specialized Multi-Agent Framework for Solidity Code Generation](https://arxiv.org/abs/2601.23009)
*Wei Chen,Zhiyuan Peng,Xin Yin,Chao Ni,Chenhao Ying,Bang Xie,Yuan Luo*

Main category: cs.SE

TL;DR: 提出SolAgent工具增强多智能体框架用于生成安全智能合约，实验表现优于现有方法，还可用于提炼小模型并开源代码。


<details>
  <summary>Details</summary>
Motivation: 智能合约确保功能正确性和安全性是关键挑战，大语言模型生成的合约代码常存在问题。

Method: 提出SolAgent框架，集成双循环细化机制，内循环用Forge编译器保证功能正确，外循环用Slither静态分析器消除安全漏洞，且具备文件系统能力解决依赖问题。

Result: 在SolEval+基准测试中Pass@1率达64.39%，显著优于现有LLMs等；相比人工编写基线减少安全漏洞达39.77%。

Conclusion: SolAgent能有效生成安全智能合约，其生成的轨迹可用于提炼小开源模型，推动安全智能合约生成的普及。

Abstract: Smart contracts are the backbone of the decentralized web, yet ensuring their functional correctness and security remains a critical challenge. While Large Language Models (LLMs) have shown promise in code generation, they often struggle with the rigorous requirements of smart contracts, frequently producing code that is buggy or vulnerable. To address this, we propose SolAgent, a novel tool-augmented multi-agent framework that mimics the workflow of human experts. SolAgent integrates a \textbf{dual-loop refinement mechanism}: an inner loop using the \textit{Forge} compiler to ensure functional correctness, and an outer loop leveraging the \textit{Slither} static analyzer to eliminate security vulnerabilities. Additionally, the agent is equipped with file system capabilities to resolve complex project dependencies. Experiments on the SolEval+ Benchmark, a rigorous suite derived from high-quality real-world projects, demonstrate that SolAgent achieves a Pass@1 rate of up to \textbf{64.39\%}, significantly outperforming state-of-the-art LLMs ($\sim$25\%), AI IDEs (e.g., GitHub Copilot), and existing agent frameworks. Moreover, it reduces security vulnerabilities by up to \textbf{39.77\%} compared to human-written baselines. Finally, we demonstrate that the high-quality trajectories generated by SolAgent can be used to distill smaller, open-source models, democratizing access to secure smart contract generation. We release our data and code at https://github.com/openpaperz/SolAgent.

</details>


### [292] [Uncovering Hidden Inclusions of Vulnerable Dependencies in Real-World Java Projects](https://arxiv.org/abs/2601.23020)
*Stefan Schott,Serena Elisa Ponta,Wolfram Fischer,Jonas Klauke,Eric Bodden*

Main category: cs.SE

TL;DR: 本文提出Java依赖扫描混合方法Unshade，结合元数据扫描与代码中心方法优势，大规模研究显示近50%流行Java项目含隐藏漏洞依赖。


<details>
  <summary>Details</summary>
Motivation: 开源软件依赖在带来好处的同时引入安全风险，现有扫描器有局限，需新方法检测隐藏漏洞依赖。

Method: Unshade先通过字节码指纹机制识别修改和隐藏依赖来扩充SBOM，再用元数据漏洞扫描器识别已知漏洞。

Result: 对1808个流行Java项目研究发现近50%含至少一个与已知漏洞关联的修改隐藏依赖，平均每个受影响项目有超8个此类依赖，Unshade识别出7712个仅靠元数据扫描会遗漏的CVE。

Conclusion: Unshade能有效检测传统元数据扫描器遗漏的隐藏漏洞依赖。

Abstract: Open-source software (OSS) dependencies are a dominant component of modern software code bases. Using proven and well-tested OSS components lets developers reduce development time and cost while improving quality. However, heavy reliance on open-source software also introduces significant security risks, including the incorporation of known vulnerabilities into the codebase. To mitigate these risks, metadata-based dependency scanners, which are lightweight and fast, and code-centric scanners, which enable the detection of modified dependencies hidden from metadata-based approaches, have been developed. In this paper, we present Unshade, a hybrid approach towards dependency scanning in Java that combines the efficiency of metadata-based scanning with the ability to detect modified dependencies of code-centric approaches. Unshade first augments a Java project's software bill of materials (SBOM) by identifying modified and hidden dependencies via a bytecode-based fingerprinting mechanism. This augmented SBOM is then passed to a metadata-based vulnerability scanner to identify known vulnerabilities in both declared and newly revealed dependencies. Leveraging Unshade's high scalability, we conducted a large-scale study of the 1,808 most popular open-source Java Maven projects on GitHub. The results show that nearly 50% of these projects contain at least one modified, hidden dependency associated with a known vulnerability. On average, each affected project includes more than eight such hidden vulnerable dependencies, all missed by traditional metadata-based scanners. Overall, Unshade identified 7,712 unique CVEs in hidden dependencies that would remain undetected when relying on metadata-based scanning alone.

</details>


### [293] [On the Impact of Code Comments for Automated Bug-Fixing: An Empirical Study](https://arxiv.org/abs/2601.23059)
*Antonio Vitale,Emanuela Guglielmi,Simone Scalabrino,Rocco Oliveto*

Main category: cs.SE

TL;DR: 研究注释对大语言模型自动修复软件漏洞能力的影响，发现注释能提升修复准确率。


<details>
  <summary>Details</summary>
Motivation: 常见自动修复漏洞的预处理步骤会移除代码注释，但作者推测注释对修复特定类型漏洞有重要作用，因此研究注释在训练和推理阶段的有无对大语言模型漏洞修复能力的影响。

Method: 对两个模型家族在训练和推理阶段有注释和无注释的所有组合条件下进行实证评估，用大语言模型为缺少注释的方法自动生成注释。

Result: 两个阶段都有注释时，自动修复漏洞的准确率最多可提高三倍；训练时有注释，在实例缺少注释时不降低性能；详细描述方法实现的注释对帮助大语言模型准确修复漏洞特别有效。

Conclusion: 注释能显著提升大语言模型自动修复漏洞的能力，不应在训练时随意移除注释。

Abstract: Large Language Models (LLMs) are increasingly relevant in Software Engineering research and practice, with Automated Bug Fixing (ABF) being one of their key applications. ABF involves transforming a buggy method into its fixed equivalent. A common preprocessing step in ABF involves removing comments from code prior to training. However, we hypothesize that comments may play a critical role in fixing certain types of bugs by providing valuable design and implementation insights. In this study, we investigate how the presence or absence of comments, both during training and at inference time, impacts the bug-fixing capabilities of LLMs. We conduct an empirical evaluation comparing two model families, each evaluated under all combinations of training and inference conditions (with and without comments), and thereby revisiting the common practice of removing comments during training. To address the limited availability of comments in state-of-the-art datasets, we use an LLM to automatically generate comments for methods lacking them. Our findings show that comments improve ABF accuracy by up to threefold when present in both phases, while training with comments does not degrade performance when instances lack them. Additionally, an interpretability analysis identifies that comments detailing method implementation are particularly effective in aiding LLMs to fix bugs accurately.

</details>


### [294] [Automated Testing of Prevalent 3D User Interactions in Virtual Reality Applications](https://arxiv.org/abs/2601.23139)
*Ruizhen Gu,José Miguel Rojas,Donghwan Shin*

Main category: cs.SE

TL;DR: 本文针对VR测试中自动合成3D用户输入和交互覆盖指标不足的问题，识别四种交互类型，提出交互流图，构建XRBench3D基准场景，介绍自动化测试方法XRintTest，评估显示其效果好且能检测问题。


<details>
  <summary>Details</summary>
Motivation: 现有VR测试方法无法自动合成现实3D用户输入，且现有指标不能很好衡量交互覆盖度。

Method: 识别四种交互类型，提出交互流图建模3D用户交互，构建XRBench3D基准场景用于评估，提出XRintTest自动化测试方法利用图进行动态场景探索和交互执行。

Result: XRintTest在XRBench3D上达到93%的特定交互覆盖率，比随机探索更有效和高效，能检测运行时异常和交互问题。交互流图可揭示潜在设计问题。

Conclusion: XRintTest可有效解决VR测试问题，交互流图可辅助发现设计问题，提升VR测试性能。

Abstract: Virtual Reality (VR) technologies offer immersive user experiences across various domains, but present unique testing challenges compared to traditional software. Existing VR testing approaches enable scene navigation and interaction activation, but lack the ability to automatically synthesise realistic 3D user inputs (e.g, grab and trigger actions via hand-held controllers). Automated testing that generates and executes such input remains an unresolved challenge. Furthermore, existing metrics fail to robustly capture diverse interaction coverage. This paper addresses these gaps through four key contributions. First, we empirically identify four prevalent interaction types in nine open-source VR projects: fire, manipulate, socket, and custom. Second, we introduce the Interaction Flow Graph, a novel abstraction that systematically models 3D user interactions by identifying targets, actions, and conditions. Third, we construct XRBench3D, a benchmark comprising ten VR scenes that encompass 456 distinct user interactions for evaluating VR interaction testing. Finally, we present XRintTest, an automated testing approach that leverages this graph for dynamic scene exploration and interaction execution. Evaluation on XRBench3D shows that XRintTest achieves great effectiveness, reaching 93% coverage of fire, manipulate and socket interactions across all scenes, and performing 12x more effectively and 6x more efficiently than random exploration. Moreover, XRintTest can detect runtime exceptions and non-exception interaction issues, including subtle configuration defects. In addition, the Interaction Flow Graph can reveal potential interaction design smells that may compromise intended functionality and hinder testing performance for VR applications.

</details>


### [295] [From Monolith to Microservices: A Comparative Evaluation of Decomposition Frameworks](https://arxiv.org/abs/2601.23141)
*Mineth Weerasinghe,Himindu Kularathne,Methmini Madhushika,Danuka Lakshan,Nisansa de Silva,Adeesha Wijayasiri,Srinath Perera*

Main category: cs.SE

TL;DR: 本文对微服务分解方法进行统一比较评估，发现基于层次聚类的方法分解效果最佳。


<details>
  <summary>Details</summary>
Motivation: 迁移到微服务时识别有效服务边界困难，现有自动化微服务分解框架评估碎片化，阻碍客观比较。

Method: 采用一致的指标计算流程，综合先前研究结果和复现包的实验输出，评估多种基准系统上的分解质量。

Result: 基于层次聚类的方法（尤其是HDBScan）在各基准测试中产生最稳定平衡的分解，实现了强模块化，同时最小化通信和接口开销。

Conclusion: 基于层次聚类的方法在微服务分解方面具有较好的效果。

Abstract: Software modernisation through the migration from monolithic architectures to microservices has become increasingly critical, yet identifying effective service boundaries remains a complex and unresolved challenge. Although numerous automated microservice decomposition frameworks have been proposed, their evaluation is often fragmented due to inconsistent benchmark systems, incompatible metrics, and limited reproducibility, thus hindering objective comparison. This work presents a unified comparative evaluation of state-of-the-art microservice decomposition approaches spanning static, dynamic, and hybrid techniques. Using a consistent metric computation pipeline, we assess the decomposition quality across widely used benchmark systems (JPetStore, AcmeAir, DayTrader, and Plants) using Structural Modularity (SM), Interface Number(IFN), Inter-partition Communication (ICP), Non-Extreme Distribution (NED), and related indicators. Our analysis combines results reported in prior studies with experimentally reproduced outputs from available replication packages. Findings indicate that the hierarchical clustering-based methods, particularly HDBScan, produce the most consistently balanced decompositions across benchmarks, achieving strong modularity while minimizing communication and interface overhead.

</details>


### [296] [Do Good, Stay Longer? Temporal Patterns and Predictors of Newcomer-to-Core Transitions in Conventional OSS and OSS4SG](https://arxiv.org/abs/2601.23142)
*Mohamed Ouf,Amr Mohamed,Mariam Guizani*

Main category: cs.SE

TL;DR: 研究对比OSS4SG和传统OSS项目新成员到核心贡献者的转变情况，发现OSS4SG保留贡献者率更高，提供多途径，且特定贡献模式达核心状态更快，给出达成核心状态策略。


<details>
  <summary>Details</summary>
Motivation: OSS新成员到核心贡献者的转化管道存在问题，探究OSS4SG项目在这方面与传统OSS项目是否有不同结果。

Method: 对比190个OSS4SG项目和185个传统OSS项目，分析92,721名贡献者和350万次提交。

Result: OSS4SG保留贡献者率是传统OSS的2.2倍，贡献者成为核心概率高19.6%；早期广泛探索项目利于成为核心；OSS4SG有多途径，传统OSS集中单一途径；Late Spike模式比Early Spike模式成为核心快2.4 - 2.9倍。

Conclusion: 找到与个人价值观相符项目、在大规模贡献前花时间了解代码库是成为核心贡献者的关键策略，项目使命影响新成员到核心转变环境，为新成员和维护者提供指导。

Abstract: Open Source Software (OSS) sustainability relies on newcomers transitioning to core contributors, but this pipeline is broken, with most newcomers becoming inactive after initial contributions. Open Source Software for Social Good (OSS4SG) projects, which prioritize societal impact as their primary mission, may be associated with different newcomer-to-core transition outcomes than conventional OSS projects. We compared 375 projects (190 OSS4SG, 185 OSS), analyzing 92,721 contributors and 3.5 million commits. OSS4SG projects retain contributors at 2.2X higher rates and contributors have 19.6% higher probability of achieving core status. Early broad project exploration predicts core achievement (22.2% importance); conventional OSS concentrates on one dominant pathway (61.62% of transitions) while OSS4SG provides multiple pathways. Contrary to intuition, contributors who invest time learning the project before intensifying their contributions (Late Spike pattern) achieve core status 2.4-2.9X faster (21 weeks) than those who contribute intensively from day one (Early Spike pattern, 51-60 weeks). OSS4SG supports two effective temporal patterns while only Late Spike achieves fastest time-to-core in conventional OSS. Our findings suggest that finding a project aligned with personal values and taking time to understand the codebase before major contributions are key strategies for achieving core status. Our findings show that project mission is associated with measurably different environments for newcomer-to-core transitions and provide evidence-based guidance for newcomers and maintainers.

</details>


### [297] [GrepRAG: An Empirical Study and Optimization of Grep-Like Retrieval for Code Completion](https://arxiv.org/abs/2601.23254)
*Baoyi Wang,Xingliang Wang,Guochang Li,Chen Zhi,Junxiao Han,Xinkui Zhao,Nan Wang,Shuiguang Deng,Jianwei Yin*

Main category: cs.SE

TL;DR: 研究简单无索引词法检索对仓库级代码补全的支持能力，提出GrepRAG框架，性能超SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于语义索引或图分析的RAG框架构建和维护索引计算开销大，受开发者轻量级搜索工具工作流启发，探索简单无索引词法检索的支持能力。

Method: 先引入Naive GrepRAG作为基线框架，分析其效果和局限，再提出GrepRAG，用轻量级后处理管道增强词法检索。

Result: Naive GrepRAG性能与复杂图基基线相当，GrepRAG在CrossCodeEval和RepoEval - Updated上超SOTA方法，在CrossCodeEval上代码精确匹配相对提升7.04 - 15.58%。

Conclusion: 简单无索引词法检索对仓库级代码补全有一定支持能力，GrepRAG有效提升了代码补全性能。

Abstract: Repository-level code completion remains challenging for large language models (LLMs) due to cross-file dependencies and limited context windows. Prior work addresses this challenge using Retrieval-Augmented Generation (RAG) frameworks based on semantic indexing or structure-aware graph analysis, but these approaches incur substantial computational overhead for index construction and maintenance. Motivated by common developer workflows that rely on lightweight search utilities (e.g., ripgrep), we revisit a fundamental yet underexplored question: how far can simple, index-free lexical retrieval support repository-level code completion before more complex retrieval mechanisms become necessary? To answer this question, we systematically investigate lightweight, index-free, intent-aware lexical retrieval through extensive empirical analysis. We first introduce Naive GrepRAG, a baseline framework in which LLMs autonomously generate ripgrep commands to retrieve relevant context. Despite its simplicity, Naive GrepRAG achieves performance comparable to sophisticated graph-based baselines. Further analysis shows that its effectiveness stems from retrieving lexically precise code fragments that are spatially closer to the completion site. We also identify key limitations of lexical retrieval, including sensitivity to noisy matches from high-frequency ambiguous keywords and context fragmentation caused by rigid truncation boundaries. To address these issues, we propose GrepRAG, which augments lexical retrieval with a lightweight post-processing pipeline featuring identifier-weighted re-ranking and structure-aware deduplication. Extensive evaluation on CrossCodeEval and RepoEval-Updated demonstrates that GrepRAG consistently outperforms state-of-the-art (SOTA) methods, achieving 7.04-15.58 percent relative improvement in code exact match (EM) over the best baseline on CrossCodeEval.

</details>


### [298] [Outcome-Conditioned Reasoning Distillation for Resolving Software Issues](https://arxiv.org/abs/2601.23257)
*Chenglin Li,Yisen Xu,Zehao Wang,Shin Hwei Tan,Tse-Hsun,Chen*

Main category: cs.SE

TL;DR: 提出O - CRD框架，利用已解决问题监督，反向重构修复轨迹，在SWE - Bench Lite提升Pass@1，可替代昂贵正向探索。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的软件修复管道以重置解决方式工作，浪费资源，且现有方法推理成本高、有偏离正确补丁风险。

Method: 提出Outcome - Conditioned Reasoning Distillation (O - CRD)框架，以已解决问题和验证补丁为监督，从历史修复反向重构修复轨迹，推理时复用指导。

Result: 在SWE - Bench Lite上，使用GPT - 4o、DeepSeek - V3、GPT - 5时Pass@1分别提升10.4%、8.6%、10.3%。

Conclusion: 基于结果条件复用已验证修复可替代软件问题解决中代价高昂的正向探索。

Abstract: Software issue resolution in large repositories is a long-range decision process: choices made during localization shape the space of viable edits, and missteps can compound into incorrect patches. Despite this, many LLM-based repair pipelines still operate in a reset-and-solve manner, producing fresh reasoning for every new issue instead of carrying forward what worked in past fixes. This is wasteful because repositories routinely contain earlier issues with overlapping structure, failure modes, or constraints, where prior repair experience could provide useful guidance. Existing approaches typically harvest this signal through forward-time trial procedures, such as repeated refinement or search, incurring high inference cost while still risking divergence from the eventual correct patch. We present an Outcome-Conditioned Reasoning Distillation(O-CRD) framework that uses resolved in-repository issues with verified patches as supervision. Starting from a historical fix, the method reconstructs a stage-wise repair trace backward from the verified outcome, then reuses the distilled guidance at inference time to steer file/function localization and patch synthesis, without fine-tuning or online search. On SWE-Bench Lite, this approach increases Pass@1 by 10.4% with GPT-4o, 8.6% with DeepSeek-V3, and 10.3% with GPT-5, indicating that outcome-conditioned reuse of verified repairs can replace costly forward exploration for software issue resolution.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [299] [Stablecoin Design with Adversarial-Robust Multi-Agent Systems via Trust-Weighted Signal Aggregation](https://arxiv.org/abs/2601.22168)
*Shengwei You,Aditya Joshi,Andrey Kuehlkamp,Jarek Nabrzyski*

Main category: q-fin.RM

TL;DR: 现有算法稳定币储备控制器有缺陷，提出MVF - Composer储备控制器，在随机场景中表现优于基线，有稳定性提升且能检测对抗性代理，系统有实用性。


<details>
  <summary>Details</summary>
Motivation: 现有算法稳定币的储备控制器易受缺乏机制的优化影响，在极端事件下会导致系统性故障，如MakerDAO在2020年崩溃，因此需要改进。

Method: 提出MVF - Composer，结合信任加权的Mean - Variance Frontier与新型风险状态估计的Stress Harness，采用多智能体模拟作为对抗压力测试器，并制定信任评分机制。

Result: 在1200个随机场景中，MVF - Composer相对于SAS基线减少57%的峰值钉住偏差和3.1倍的平均恢复时间，信任层在对抗条件下带来23%的稳定性提升，实现72%的对抗性代理检测。

Conclusion: MVF - Composer能有效提升算法稳定币储备管理在极端情况下的稳定性，在普通硬件上运行，无需额外链上预言机，为DeFi储备政策压力测试提供可复现框架。

Abstract: Algorithmic stablecoins promise decentralized monetary stability by maintaining a target peg through programmatic reserve management. Yet, their reserve controllers remain vulnerable to regime-blind optimization, calibrating risk parameters on fair-weather data while ignoring tail events that precipitate cascading failures. The March 2020 Black Thursday collapse, wherein MakerDAO's collateral auctions yielded $8.3M in losses and a 15% peg deviation, exposed a critical gap: existing models like SAS systematically omit extreme volatility regimes from covariance estimates, producing allocations optimal in expectation but catastrophic under adversarial stress.
  We present MVF-Composer, a trust-weighted Mean-Variance Frontier reserve controller incorporating a novel Stress Harness for risk-state estimation. Our key insight is deploying multi-agent simulations as adversarial stress-testers: heterogeneous agents (traders, liquidity providers, attackers) execute protocol actions under crisis scenarios, exposing reserve vulnerabilities before they manifest on-chain. We formalize a trust-scoring mechanism T: A -> [0,1] that down-weights signals from agents exhibiting manipulative behavior, ensuring the risk-state estimator remains robust to signal injection and Sybil attacks.
  Across 1,200 randomized scenarios with injected Black-Swan shocks (10% collateral drawdown, 50% sentiment collapse, coordinated redemption attacks), MVF-Composer reduces peak peg deviation by 57% and mean recovery time by 3.1x relative to SAS baselines. Ablation studies confirm the trust layer accounts for 23% of stability gains under adversarial conditions, achieving 72% adversarial agent detection. Our system runs on commodity hardware, requires no on-chain oracles beyond standard price feeds, and provides a reproducible framework for stress-testing DeFi reserve policies.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [300] [A unified theory of order flow, market impact, and volatility](https://arxiv.org/abs/2601.23172)
*Johannes Muhle-Karbe,Youssef Ouazzani Chahd,Mathieu Rosenbaum,Grégoire Szymanski*

Main category: q-fin.ST

TL;DR: 提出区分核心订单和反应流的金融市场订单流微观结构模型，该模型有自然缩放极限，能调和多种实证特性，分析数据得H₀≈3/4，与市场影响平方根定律及交易量和波动率粗糙度估计相符。


<details>
  <summary>Details</summary>
Motivation: 构建能调和金融市场中持久有符号订单流、粗糙交易量和波动率、幂律市场影响等多个显著实证特性的订单流微观结构模型。

Method: 将核心订单和反应流都建模为Hawkes过程，利用无套利约束进行分析。

Result: 所有相关量由单一统计量H₀确定，有符号流收敛为分数过程与鞅之和，交易量是粗糙过程，波动率是粗糙的，交易价格影响遵循幂律，分析数据得H₀≈3/4。

Conclusion: 模型能很好地解释金融市场中订单流、交易量、波动率和市场影响等现象，H₀的估计与市场影响平方根定律及交易量和波动率粗糙度估计相符。

Abstract: We propose a microstructural model for the order flow in financial markets that distinguishes between {\it core orders} and {\it reaction flow}, both modeled as Hawkes processes. This model has a natural scaling limit that reconciles a number of salient empirical properties: persistent signed order flow, rough trading volume and volatility, and power-law market impact. In our framework, all these quantities are pinned down by a single statistic $H_0$, which measures the persistence of the core flow. Specifically, the signed flow converges to the sum of a fractional process with Hurst index $H_0$ and a martingale, while the limiting traded volume is a rough process with Hurst index $H_0-1/2$. No-arbitrage constraints imply that volatility is rough, with Hurst parameter $2H_0-3/2$, and that the price impact of trades follows a power law with exponent $2-2H_0$. The analysis of signed order flow data yields an estimate $H_0 \approx 3/4$. This is not only consistent with the square-root law of market impact, but also turns out to match estimates for the roughness of traded volumes and volatilities remarkably well.

</details>


### [301] [Adaptive Benign Overfitting (ABO): Overparameterized RLS for Online Learning in Non-stationary Time-series](https://arxiv.org/abs/2601.22200)
*Luis Ontaneda Mijares,Nick Firoozye*

Main category: q-fin.ST

TL;DR: 本文提出自适应良性过拟合（ABO），引入QR - EWRLS算法，实验验证其在合成时间序列和实际预测应用中的效果，提供稳定在线学习统一视角。


<details>
  <summary>Details</summary>
Motivation: 解决过参数化模型良性过拟合现象在自适应学习中的应用问题，在非平稳条件下实现在线适应。

Method: 将递归最小二乘法（RLS）框架扩展到良性过拟合区域，引入基于QR的指数加权RLS（QR - EWRLS）算法，结合随机傅里叶特征映射和遗忘因子正则化。

Result: 在非线性合成时间序列上保持有界残差和稳定条件数，再现双下降行为；在外汇和电力需求预测中高精度且速度提升20 - 40%。

Conclusion: 在稳定在线学习框架内将自适应滤波、核近似和良性过拟合联系起来，提供统一视角。

Abstract: Overparameterized models have recently challenged conventional learning theory by exhibiting improved generalization beyond the interpolation limit, a phenomenon known as benign overfitting. This work introduces Adaptive Benign Overfitting (ABO), extending the recursive least-squares (RLS) framework to this regime through a numerically stable formulation based on orthogonal-triangular updates. A QR-based exponentially weighted RLS (QR-EWRLS) algorithm is introduced, combining random Fourier feature mappings with forgetting-factor regularization to enable online adaptation under non-stationary conditions. The orthogonal decomposition prevents the numerical divergence associated with covariance-form RLS while retaining adaptability to evolving data distributions. Experiments on nonlinear synthetic time series confirm that the proposed approach maintains bounded residuals and stable condition numbers while reproducing the double-descent behavior characteristic of overparameterized models. Applications to forecasting foreign exchange and electricity demand show that ABO is highly accurate (comparable to baseline kernel methods) while achieving speed improvements of between 20 and 40 percent. The results provide a unified view linking adaptive filtering, kernel approximation, and benign overfitting within a stable online learning framework.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [302] [Dependence-Aware Label Aggregation for LLM-as-a-Judge via Ising Models](https://arxiv.org/abs/2601.22336)
*Krishnakumar Balasubramanian,Aleksandr Podkopaev,Shiva Prasad Kasiviswanathan*

Main category: stat.ML

TL;DR: 论文研究大规模AI评估中考虑标注者依赖关系的标签聚合问题，提出基于Ising图模型和潜在因子的模型，证明经典方法的次优性，并在真实数据集上验证了新方法的性能。


<details>
  <summary>Details</summary>
Motivation: 经典标签聚合方法假设标注者条件独立，而LLM评判者因共享数据等因素常违反该假设，忽略依赖会导致后验校准错误和错误预测。

Method: 通过基于Ising图模型和潜在因子的依赖感知模型层次进行标签聚合。

Result: 有限K的例子表明条件独立方法可能翻转贝叶斯标签；证明经典方法在评判者数量增加时严格次优，存在非零额外风险；在三个真实数据集上，新方法性能优于经典基线。

Conclusion: 所提出的依赖感知标签聚合方法在处理标注者依赖问题上优于经典的条件独立方法。

Abstract: Large-scale AI evaluation increasingly relies on aggregating binary judgments from $K$ annotators, including LLMs used as judges. Most classical methods, e.g., Dawid-Skene or (weighted) majority voting, assume annotators are conditionally independent given the true label $Y\in\{0,1\}$, an assumption often violated by LLM judges due to shared data, architectures, prompts, and failure modes. Ignoring such dependencies can yield miscalibrated posteriors and even confidently incorrect predictions. We study label aggregation through a hierarchy of dependence-aware models based on Ising graphical models and latent factors. For class-dependent Ising models, the Bayes log-odds is generally quadratic in votes; for class-independent couplings, it reduces to a linear weighted vote with correlation-adjusted parameters. We present finite-$K$ examples showing that methods based on conditional independence can flip the Bayes label despite matching per-annotator marginals. We prove separation results demonstrating that these methods remain strictly suboptimal as the number of judges grows, incurring nonvanishing excess risk under latent factors. Finally, we evaluate the proposed method on three real-world datasets, demonstrating improved performance over the classical baselines.

</details>


### [303] [Amortized Simulation-Based Inference in Generalized Bayes via Neural Posterior Estimation](https://arxiv.org/abs/2601.22367)
*Shiyi Sun,Geoff K. Nicholls,Jeong Eun Lee*

Main category: stat.ML

TL;DR: 提出首个完全摊销的变分近似方法处理广义贝叶斯推理中的回火后验族，介绍两种训练路线，在多个基准测试中表现有竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有广义贝叶斯推理方法依赖昂贵采样器，且需为新数据集和β值重新运行，需改进。

Method: 训练单个(x,β)条件神经后验估计器q_φ(θ|x,β)，引入合成样本和重加权固定基础数据集两种训练路线。

Result: SNIS加权目标对回火后验提供一致的前向KL拟合，在四个标准模拟推理基准测试中，β摊销估计器在标准两样本指标上实现有竞争力的后验近似。

Conclusion: 所提出的β摊销估计器能有效处理广义贝叶斯推理中的回火后验，在多个基准测试中表现良好，可替代非摊销基于MCMC的功率后验采样器。

Abstract: Generalized Bayesian Inference (GBI) tempers a loss with a temperature $β>0$ to mitigate overconfidence and improve robustness under model misspecification, but existing GBI methods typically rely on costly MCMC or SDE-based samplers and must be re-run for each new dataset and each $β$ value. We give the first fully amortized variational approximation to the tempered posterior family $p_β(θ\mid x) \propto π(θ)\,p(x \mid θ)^β$ by training a single $(x,β)$-conditioned neural posterior estimator $q_φ(θ\mid x,β)$ that enables sampling in a single forward pass, without simulator calls or inference-time MCMC. We introduce two complementary training routes: (i) synthesize off-manifold samples $(θ,x) \sim π(θ)\,p(x \mid θ)^β$ and (ii) reweight a fixed base dataset $π(θ)\,p(x \mid θ)$ using self-normalized importance sampling (SNIS). We show that the SNIS-weighted objective provides a consistent forward-KL fit to the tempered posterior with finite weight variance. Across four standard simulation-based inference (SBI) benchmarks, including the chaotic Lorenz-96 system, our $β$-amortized estimator achieves competitive posterior approximations in standard two-sample metrics, matching non-amortized MCMC-based power-posterior samplers over a wide range of temperatures.

</details>


### [304] [It's all the (Exponential) Family: An Equivalence between Maximum Likelihood Estimation and Control Variates for Sketching Algorithms](https://arxiv.org/abs/2601.22378)
*Keegan Kang,Kerong Wang,Ding Zhang,Rameshwar Pratap,Bhisham Dev Verma,Benedict H. W. Wong*

Main category: stat.ML

TL;DR: 研究结合MLE和CVE，证明特定指数族条件下最优CVE与MLE渐近方差相同，给出MLE的EM算法，实验表明该算法在二元正态分布中更快且数值稳定，还能实现可重复性。


<details>
  <summary>Details</summary>
Motivation: 探索MLE和CVE在草图算法及机器学习应用中的结合，提高算法性能和可重复性。

Method: 证明特定指数族条件下最优CVE与MLE的渐近方差关系，给出MLE的EM算法。

Result: 实验显示EM算法在二元正态分布中比其他求根算法更快且数值稳定。

Conclusion: EM算法适用于满足条件的分布，能实现使用MLE/CVE算法的可重复性，且在已知CV权重时可找到MLE。

Abstract: Maximum likelihood estimators (MLE) and control variate estimators (CVE) have been used in conjunction with known information across sketching algorithms and applications in machine learning. We prove that under certain conditions in an exponential family, an optimal CVE will achieve the same asymptotic variance as the MLE, giving an Expectation-Maximization (EM) algorithm for the MLE. Experiments show the EM algorithm is faster and numerically stable compared to other root finding algorithms for the MLE for the bivariate Normal distribution, and we expect this to hold across distributions satisfying these conditions. We show how the EM algorithm leads to reproducibility for algorithms using MLE / CVE, and demonstrate how the EM algorithm leads to finding the MLE when the CV weights are known.

</details>


### [305] [Simulation-based Bayesian inference with ameliorative learned summary statistics -- Part I](https://arxiv.org/abs/2601.22441)
*Getachew K. Befekadu*

Main category: stat.ML

TL;DR: 本文是系列论文的第一部分，探讨基于模拟的推理，利用学习的摘要统计量，介绍了数据转换技术，该框架可处理弱相关数据，适合分布式计算。


<details>
  <summary>Details</summary>
Motivation: 当观测数据和模拟模型的精确似然函数难以以封闭形式获得或计算困难时，寻找一种有效的推理方法。

Method: 使用基于Cressie - Read差异准则的变换技术总结观测数据和模拟输出之间的学习统计量，在保留推理统计功效的同时，让模拟输出以观测数据为条件。

Result: 提出的推理框架可处理弱相关观测数据，且适用于分布式计算。

Conclusion: 该推理框架是一种有效的解决方案，可应对复杂模拟模型下的推理问题，在分布式计算中具有应用潜力。

Abstract: This paper, which is Part 1 of a two-part paper series, considers a simulation-based inference with learned summary statistics, in which such a learned summary statistic serves as an empirical-likelihood with ameliorative effects in the Bayesian setting, when the exact likelihood function associated with the observation data and the simulation model is difficult to obtain in a closed form or computationally intractable. In particular, a transformation technique which leverages the Cressie-Read discrepancy criterion under moment restrictions is used for summarizing the learned statistics between the observation data and the simulation outputs, while preserving the statistical power of the inference. Here, such a transformation of data-to-learned summary statistics also allows the simulation outputs to be conditioned on the observation data, so that the inference task can be performed over certain sample sets of the observation data that are considered as an empirical relevance or believed to be particular importance. Moreover, the simulation-based inference framework discussed in this paper can be extended further, and thus handling weakly dependent observation data. Finally, we remark that such an inference framework is suitable for implementation in distributed computing, i.e., computational tasks involving both the data-to-learned summary statistics and the Bayesian inferencing problem can be posed as a unified distributed inference problem that will exploit distributed optimization and MCMC algorithms for supporting large datasets associated with complex simulation models.

</details>


### [306] [Corrected Samplers for Discrete Flow Models](https://arxiv.org/abs/2601.22519)
*Zhengyan Wan,Yidong Ouyang,Liyan Xie,Fang Fang,Hongyuan Zha,Guang Cheng*

Main category: stat.ML

TL;DR: 本文在离散流模型框架下建立了离散扩散模型采样器的非渐近离散化误差界，提出两种校正采样器，能降低离散化误差且迭代复杂度更低，在模拟和文本到图像生成任务中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 现有离散扩散模型采样器需大量迭代控制离散化误差，理论结果对转移率和源分布有条件限制。

Method: 在离散流模型框架下建立非渐近离散化误差界，分析欧拉采样器的一步下界，提出时间校正采样器和位置校正采样器。

Result: 位置校正采样器迭代复杂度低于现有并行采样器，在模拟和文本到图像生成任务中提高生成质量、减少推理时间。

Conclusion: 提出的校正采样器有效，能降低离散化误差，具有更低迭代复杂度。

Abstract: Discrete flow models (DFMs) have been proposed to learn the data distribution on a finite state space, offering a flexible framework as an alternative to discrete diffusion models. A line of recent work has studied samplers for discrete diffusion models, such as tau-leaping and Euler solver. However, these samplers require a large number of iterations to control discretization error, since the transition rates are frozen in time and evaluated at the initial state within each time interval. Moreover, theoretical results for these samplers often require boundedness conditions of the transition rate or they focus on a specific type of source distributions. To address those limitations, we establish non-asymptotic discretization error bounds for those samplers without any restriction on transition rates and source distributions, under the framework of discrete flow models. Furthermore, by analyzing a one-step lower bound of the Euler sampler, we propose two corrected samplers: \textit{time-corrected sampler} and \textit{location-corrected sampler}, which can reduce the discretization error of tau-leaping and Euler solver with almost no additional computational cost. We rigorously show that the location-corrected sampler has a lower iteration complexity than existing parallel samplers. We validate the effectiveness of the proposed method by demonstrating improved generation quality and reduced inference time on both simulation and text-to-image generation tasks. Code can be found in https://github.com/WanZhengyan/Corrected-Samplers-for-Discrete-Flow-Models.

</details>


### [307] [An Efficient Algorithm for Thresholding Monte Carlo Tree Search](https://arxiv.org/abs/2601.22600)
*Shoma Nameki,Atsuyoshi Nakamura,Junpei Komiyama,Koji Tabata*

Main category: stat.ML

TL;DR: 本文介绍阈值蒙特卡罗树搜索问题，提出渐近最优样本复杂度算法，改进策略提升性能并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决阈值蒙特卡罗树搜索问题，即判断树的根节点值是否至少为给定阈值。

Method: 基于Track - and - Stop策略开发δ - 正确的顺序采样算法，对D - Tracking臂拉动策略进行基于比率的修改。

Result: 算法具有渐近最优样本复杂度，改进策略使经验样本复杂度显著提升，每轮计算成本从线性降至对数级。

Conclusion: 所提算法和改进策略在解决阈值蒙特卡罗树搜索问题上有较好效果。

Abstract: We introduce the Thresholding Monte Carlo Tree Search problem, in which, given a tree $\mathcal{T}$ and a threshold $θ$, a player must answer whether the root node value of $\mathcal{T}$ is at least $θ$ or not. In the given tree, `MAX' or `MIN' is labeled on each internal node, and the value of a `MAX'-labeled (`MIN'-labeled) internal node is the maximum (minimum) of its child values. The value of a leaf node is the mean reward of an unknown distribution, from which the player can sample rewards. For this problem, we develop a $δ$-correct sequential sampling algorithm based on the Track-and-Stop strategy that has asymptotically optimal sample complexity. We show that a ratio-based modification of the D-Tracking arm-pulling strategy leads to a substantial improvement in empirical sample complexity, as well as reducing the per-round computational cost from linear to logarithmic in the number of arms.

</details>


### [308] [RPWithPrior: Label Differential Privacy in Regression](https://arxiv.org/abs/2601.22625)
*Haixia Liu,Ruifan Huang*

Main category: stat.ML

TL;DR: 本文聚焦ε - 标签差分隐私下的回归任务，提出新方法，避免离散化，在多数据集上表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有ε - 标签差分隐私回归方法存在与现实场景不匹配的问题，需保护用户隐私同时减少精度损失。

Method: 将原始和随机响应建模为连续随机变量，避免离散化，估计随机响应的最优区间，针对先验已知和未知场景设计算法。

Result: 新方法在Communities and Crime、Criteo Sponsored Search Conversion Log、California Housing数据集上比Gaussian、Laplace等机制表现更好。

Conclusion: 提出的RPWithPrior算法能保证ε - 标签差分隐私，新方法性能更优。

Abstract: With the wide application of machine learning techniques in practice, privacy preservation has gained increasing attention. Protecting user privacy with minimal accuracy loss is a fundamental task in the data analysis and mining community. In this paper, we focus on regression tasks under $ε$-label differential privacy guarantees. Some existing methods for regression with $ε$-label differential privacy, such as the RR-On-Bins mechanism, discretized the output space into finite bins and then applied RR algorithm. To efficiently determine these finite bins, the authors rounded the original responses down to integer values. However, such operations does not align well with real-world scenarios. To overcome these limitations, we model both original and randomized responses as continuous random variables, avoiding discretization entirely. Our novel approach estimates an optimal interval for randomized responses and introduces new algorithms designed for scenarios where a prior is either known or unknown. Additionally, we prove that our algorithm, RPWithPrior, guarantees $ε$-label differential privacy. Numerical results demonstrate that our approach gets better performance compared with the Gaussian, Laplace, Staircase, and RRonBins, Unbiased mechanisms on the Communities and Crime, Criteo Sponsored Search Conversion Log, California Housing datasets.

</details>


### [309] [Generative and Nonparametric Approaches for Conditional Distribution Estimation: Methods, Perspectives, and Comparative Evaluations](https://arxiv.org/abs/2601.22650)
*Yen-Shiu Chin,Zhi-Yu Jou,Toshinari Morimoto,Chia-Tse Wang,Ming-Chung Chang,Tso-Jung Yen,Su-Yun Huang,Tailen Hsing*

Main category: stat.ML

TL;DR: 本文回顾并比较了经典非参数方法和现代生成模型中几种具有代表性的条件分布推断方法，通过统一评估框架进行数值比较，并分析各方法优缺点。


<details>
  <summary>Details</summary>
Motivation: 条件分布推断是统计学中的基础问题，已有多种方法，本文旨在回顾和比较代表性方法。

Method: 回顾几种代表性方法，涵盖单指标法、基展开法和基于生成模拟的方法，用统一评估框架进行系统数值比较。

Result: 使用条件均值和标准差的均方误差、Wasserstein距离等指标评估各方法性能，还讨论了灵活性和计算成本。

Conclusion: 各方法有独特的优点和局限性。

Abstract: The inference of conditional distributions is a fundamental problem in statistics, essential for prediction, uncertainty quantification, and probabilistic modeling. A wide range of methodologies have been developed for this task. This article reviews and compares several representative approaches spanning classical nonparametric methods and modern generative models. We begin with the single-index method of Hall and Yao (2005), which estimates the conditional distribution through a dimension-reducing index and nonparametric smoothing of the resulting one-dimensional cumulative conditional distribution function. We then examine the basis-expansion approaches, including FlexCode (Izbicki and Lee, 2017) and DeepCDE (Dalmasso et al., 2020), which convert conditional density estimation into a set of nonparametric regression problems. In addition, we discuss two recent generative simulation-based methods that leverage modern deep generative architectures: the generative conditional distribution sampler (Zhou et al., 2023) and the conditional denoising diffusion probabilistic model (Fu et al., 2024; Yang et al., 2025). A systematic numerical comparison of these approaches is provided using a unified evaluation framework that ensures fairness and reproducibility. The performance metrics used for the estimated conditional distribution include the mean-squared errors of conditional mean and standard deviation, as well as the Wasserstein distance. We also discuss their flexibility and computational costs, highlighting the distinct advantages and limitations of each approach.

</details>


### [310] [Spectral Gradient Descent Mitigates Anisotropy-Driven Misalignment: A Case Study in Phase Retrieval](https://arxiv.org/abs/2601.22652)
*Guillaume Braun,Han Bao,Wei Huang,Masaaki Imaizumi*

Main category: stat.ML

TL;DR: 研究光谱梯度方法在非线性相位恢复模型中的增益机制，发现梯度下降存在方差诱导的失准问题，而光谱梯度下降可避免，数值实验验证理论。


<details>
  <summary>Details</summary>
Motivation: 探究光谱梯度方法（如Muon优化器）在深度学习中表现良好的潜在机制。

Method: 对具有各向异性高斯输入的非线性相位恢复模型进行动力学分析，聚焦于尖峰协方差设置。

Result: 梯度下降在早期逃逸阶段会出现方差诱导的失准，光谱梯度下降可消除尖峰放大效应，实现稳定对齐和加速噪声收缩，数值实验在更广泛各向异性协方差下验证现象。

Conclusion: 光谱梯度下降相比梯度下降在处理各向异性输入时具有优势。

Abstract: Spectral gradient methods, such as the Muon optimizer, modify gradient updates by preserving directional information while discarding scale, and have shown strong empirical performance in deep learning. We investigate the mechanisms underlying these gains through a dynamical analysis of a nonlinear phase retrieval model with anisotropic Gaussian inputs, equivalent to training a two-layer neural network with the quadratic activation and fixed second-layer weights. Focusing on a spiked covariance setting where the dominant variance direction is orthogonal to the signal, we show that gradient descent (GD) suffers from a variance-induced misalignment: during the early escaping stage, the high-variance but uninformative spike direction is multiplicatively amplified, degrading alignment with the true signal under strong anisotropy. In contrast, spectral gradient descent (SpecGD) removes this spike amplification effect, leading to stable alignment and accelerated noise contraction. Numerical experiments confirm the theory and show that these phenomena persist under broader anisotropic covariances.

</details>


### [311] [GRANITE: A Generalized Regional Framework for Identifying Agreement in Feature-Based Explanations](https://arxiv.org/abs/2601.22771)
*Julia Herbinger,Gabriel Laberge,Maximilian Muschalik,Yann Pequignot,Marvin N. Wright,Fabian Fumagalli*

Main category: stat.ML

TL;DR: 提出GRANITE框架解决特征解释方法分歧问题，在真实数据集验证有效性。


<details>
  <summary>Details</summary>
Motivation: 不同特征解释方法常产生冲突解释，源于特征交互和依赖处理方式不同。

Method: 提出GRANITE框架，划分特征空间以减少交互和分布影响，统一现有区域方法并引入递归分区算法。

Result: 能使不同解释方法更一致，在真实数据集上展示了有效性。

Conclusion: GRANITE是用于一致且可解释特征解释的实用工具。

Abstract: Feature-based explanation methods aim to quantify how features influence the model's behavior, either locally or globally, but different methods often disagree, producing conflicting explanations. This disagreement arises primarily from two sources: how feature interactions are handled and how feature dependencies are incorporated. We propose GRANITE, a generalized regional explanation framework that partitions the feature space into regions where interaction and distribution influences are minimized. This approach aligns different explanation methods, yielding more consistent and interpretable explanations. GRANITE unifies existing regional approaches, extends them to feature groups, and introduces a recursive partitioning algorithm to estimate such regions. We demonstrate its effectiveness on real-world datasets, providing a practical tool for consistent and interpretable feature explanations.

</details>


### [312] [Approximating $f$-Divergences with Rank Statistics](https://arxiv.org/abs/2601.22784)
*Viktor Stein,José Manuel de Frutos*

Main category: stat.ML

TL;DR: 本文提出基于秩统计的f - 散度近似方法，避免显式密度比估计，给出理论结果并进行实验验证。


<details>
  <summary>Details</summary>
Motivation: 避免f - 散度估计时的显式密度比估计。

Method: 将两个单变量分布的不匹配映射到秩直方图，用离散f - 散度测量其与均匀性的偏差；对高维数据在随机投影上平均单变量构造。

Result: 证明了散度估计量的单调性、是真实f - 散度的下界，建立收敛率，给出有限样本偏差界和渐近正态性结果，实验验证了方法有效性。

Conclusion: 所提出的秩统计f - 散度近似方法有效，可用于生成建模等场景。

Abstract: We introduce a rank-statistic approximation of $f$-divergences that avoids explicit density-ratio estimation by working directly with the distribution of ranks. For a resolution parameter $K$, we map the mismatch between two univariate distributions $μ$ and $ν$ to a rank histogram on $\{ 0, \ldots, K\}$ and measure its deviation from uniformity via a discrete $f$-divergence, yielding a rank-statistic divergence estimator. We prove that the resulting estimator of the divergence is monotone in $K$, is always a lower bound of the true $f$-divergence, and we establish quantitative convergence rates for $K\to\infty$ under mild regularity of the quantile-domain density ratio. To handle high-dimensional data, we define the sliced rank-statistic $f$-divergence by averaging the univariate construction over random projections, and we provide convergence results for the sliced limit as well. We also derive finite-sample deviation bounds along with asymptotic normality results for the estimator. Finally, we empirically validate the approach by benchmarking against neural baselines and illustrating its use as a learning objective in generative modelling experiments.

</details>


### [313] [OneFlowSBI: One Model, Many Queries for Simulation-Based Inference](https://arxiv.org/abs/2601.22951)
*Mayank Nautiyal,Li Ju,Melker Ernfors,Klara Hagland,Ville Holma,Maximilian Werkö Söderholm,Andreas Hellander,Prashant Singh*

Main category: stat.ML

TL;DR: 介绍OneFlowSBI统一框架用于基于模拟的推理，支持多推理任务，经评估表现有竞争力且高效鲁棒。


<details>
  <summary>Details</summary>
Motivation: 构建一个能支持多种推理任务且无需针对特定任务重新训练的基于模拟的推理框架。

Method: 学习参数和观测联合分布上的单一流匹配生成模型，训练时利用查询感知掩码分布。

Result: 在十个基准推理问题和两个高维现实逆问题上评估，与现有求解器和估计器相比有竞争力，能以少量ODE积分步骤高效采样，对噪声和部分观测数据有鲁棒性。

Conclusion: OneFlowSBI框架在基于模拟的推理中表现良好，具有高效性和鲁棒性。

Abstract: We introduce \textit{OneFlowSBI}, a unified framework for simulation-based inference that learns a single flow-matching generative model over the joint distribution of parameters and observations. Leveraging a query-aware masking distribution during training, the same model supports multiple inference tasks, including posterior sampling, likelihood estimation, and arbitrary conditional distributions, without task-specific retraining. We evaluate \textit{OneFlowSBI} on ten benchmark inference problems and two high-dimensional real-world inverse problems across multiple simulation budgets. \textit{OneFlowSBI} is shown to deliver competitive performance against state-of-the-art generalized inference solvers and specialized posterior estimators, while enabling efficient sampling with few ODE integration steps and remaining robust under noisy and partially observed data.

</details>


### [314] [Neural Backward Filtering Forward Guiding](https://arxiv.org/abs/2601.23030)
*Gefan Yang,Frank van der Meulen,Stefan Sommer*

Main category: stat.ML

TL;DR: 提出Neural Backward Filtering Forward Guiding (NBFFG)框架用于树状非线性连续随机过程推断，优于基线方法并应用于系统发育分析。


<details>
  <summary>Details</summary>
Motivation: 树状非线性连续随机过程推断困难，精确平滑和基于粒子的方法有局限性。

Method: 构建基于辅助线性高斯过程的变分后验，学习神经残差，采用无偏路径子采样方案。

Result: NBFFG在合成基准上优于基线方法，在系统发育分析的高维推断任务中得到应用。

Conclusion: NBFFG是有效的处理树状非线性连续随机过程推断的统一框架。

Abstract: Inference in non-linear continuous stochastic processes on trees is challenging, particularly when observations are sparse (leaf-only) and the topology is complex. Exact smoothing via Doob's $h$-transform is intractable for general non-linear dynamics, while particle-based methods degrade in high dimensions. We propose Neural Backward Filtering Forward Guiding (NBFFG), a unified framework for both discrete transitions and continuous diffusions. Our method constructs a variational posterior by leveraging an auxiliary linear-Gaussian process. This auxiliary process yields a closed-form backward filter that serves as a ``guide'', steering the generative path toward high-likelihood regions. We then learn a neural residual--parameterized as a normalizing flow or a controlled SDE--to capture the non-linear discrepancies. This formulation allows for an unbiased path-wise subsampling scheme, reducing the training complexity from tree-size dependent to path-length dependent. Empirical results show that NBFFG outperforms baselines on synthetic benchmarks, and we demonstrate the method on a high-dimensional inference task in phylogenetic analysis with reconstruction of ancestral butterfly wing shapes.

</details>


### [315] [Asymptotic Theory of Iterated Empirical Risk Minimization, with Applications to Active Learning](https://arxiv.org/abs/2601.23031)
*Hugo Cui,Yue M. Lu*

Main category: stat.ML

TL;DR: 研究一类迭代经验风险最小化程序，推导高维下线性模型测试误差的渐近特征，应用于主动学习问题并揭示新现象。


<details>
  <summary>Details</summary>
Motivation: 在主动学习和重加权方案中出现的迭代ERM程序与经典单阶段ERM分析不同，存在复杂统计依赖，需研究其性能。

Method: 针对高斯混合数据上用凸损失训练的线性模型，推导高维下样本量和环境维度成比例时测试误差的渐近特征。

Result: 得到第二阶段估计器性能的精确渐近预测，去除主动学习问题中先前工作的假设，发现标注预算分配的权衡和仅由数据选择驱动的测试误差双下降行为。

Conclusion: 该理论可用于分析迭代ERM程序，为主动学习问题提供新见解。

Abstract: We study a class of iterated empirical risk minimization (ERM) procedures in which two successive ERMs are performed on the same dataset, and the predictions of the first estimator enter as an argument in the loss function of the second. This setting, which arises naturally in active learning and reweighting schemes, introduces intricate statistical dependencies across samples and fundamentally distinguishes the problem from classical single-stage ERM analyses. For linear models trained with a broad class of convex losses on Gaussian mixture data, we derive a sharp asymptotic characterization of the test error in the high-dimensional regime where the sample size and ambient dimension scale proportionally. Our results provide explicit, fully asymptotic predictions for the performance of the second-stage estimator despite the reuse of data and the presence of prediction-dependent losses. We apply this theory to revisit a well-studied pool-based active learning problem, removing oracle and sample-splitting assumptions made in prior work. We uncover a fundamental tradeoff in how the labeling budget should be allocated across stages, and demonstrate a double-descent behavior of the test error driven purely by data selection, rather than model size or sample count.

</details>


### [316] [A Random Matrix Theory of Masked Self-Supervised Regression](https://arxiv.org/abs/2601.23208)
*Arie Wortsman Zurich,Federica Gerace,Bruno Loureiro,Yue M. Lu*

Main category: stat.ML

TL;DR: 本文对掩码自监督学习目标进行高维分析，给出泛化误差表达式，刻画学习到的预测器的谱结构，发现相变并指出其优于PCA的情况。


<details>
  <summary>Details</summary>
Motivation: 掩码自监督学习产生了矩阵值预测器，带来新的分析挑战，需要对其建模目标进行分析。

Method: 在样本数量与环境维度成比例的情况下，对掩码建模目标进行高维分析。

Result: 给出泛化误差的明确表达式，刻画预测器的谱结构；表明联合预测器会经历BBP型相变；确定了掩码自监督学习明显优于PCA的结构化机制。

Conclusion: 掩码自监督学习能够从数据中提取结构，在特定机制中比经典无监督方法具有潜在优势。

Abstract: In the era of transformer models, masked self-supervised learning (SSL) has become a foundational training paradigm. A defining feature of masked SSL is that training aggregates predictions across many masking patterns, giving rise to a joint, matrix-valued predictor rather than a single vector-valued estimator. This object encodes how coordinates condition on one another and poses new analytical challenges. We develop a precise high-dimensional analysis of masked modeling objectives in the proportional regime where the number of samples scales with the ambient dimension. Our results provide explicit expressions for the generalization error and characterize the spectral structure of the learned predictor, revealing how masked modeling extracts structure from data. For spiked covariance models, we show that the joint predictor undergoes a Baik--Ben Arous--Péché (BBP)-type phase transition, identifying when masked SSL begins to recover latent signals. Finally, we identify structured regimes in which masked self-supervised learning provably outperforms PCA, highlighting potential advantages of SSL objectives over classical unsupervised methods

</details>


### [317] [Graph Attention Network for Node Regression on Random Geometric Graphs with Erdős--Rényi contamination](https://arxiv.org/abs/2601.23239)
*Somak Laha,Suqi Liu,Morgane Austern*

Main category: stat.ML

TL;DR: 本文探讨图注意力网络（GAT）在节点回归问题上的优势，提出特定任务的GAT构建去噪代理特征，证明其在回归系数估计和预测响应上渐近误差更低，通过合成数据和真实数据实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏GAT相对于非注意力图神经网络（GNN）优势的严格统计保证，本文针对基于图的变量误差模型下的节点回归问题填补这一空白。

Method: 提出并分析一个精心设计的、特定任务的GAT，构建去噪代理特征用于回归，借助高维几何尾界和集中度分析。

Result: 证明在温和增长条件下，基于代理特征的回归在估计回归系数和预测未标记节点的响应上，比使用有噪节点协变量的普通最小二乘法（OLS）和普通图卷积网络（GCN）渐近误差更低。

Conclusion: 通过合成数据和真实世界图的实验验证了理论结果，表明注意力机制在几个节点回归任务中有效。

Abstract: Graph attention networks (GATs) are widely used and often appear robust to noise in node covariates and edges, yet rigorous statistical guarantees demonstrating a provable advantage of GATs over non-attention graph neural networks~(GNNs) are scarce. We partially address this gap for node regression with graph-based errors-in-variables models under simultaneous covariate and edge corruption: responses are generated from latent node-level covariates, but only noise-perturbed versions of the latent covariates are observed; and the sample graph is a random geometric graph created from the node covariates but contaminated by independent Erdős--Rényi edges. We propose and analyze a carefully designed, task-specific GAT that constructs denoised proxy features for regression. We prove that regressing the response variables on the proxies achieves lower error asymptotically in (a) estimating the regression coefficient compared to the ordinary least squares (OLS) estimator on the noisy node covariates, and (b) predicting the response for an unlabelled node compared to a vanilla graph convolutional network~(GCN) -- under mild growth conditions. Our analysis leverages high-dimensional geometric tail bounds and concentration for neighbourhood counts and sample covariances. We verify our theoretical findings through experiments on synthetically generated data. We also perform experiments on real-world graphs and demonstrate the effectiveness of the attention mechanism in several node regression tasks.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [318] [Wasserstein Geometry of Information Loss in Nonlinear Dynamical Systems](https://arxiv.org/abs/2601.22814)
*Yiting Duan,Zhikun Zhang,Yi Guo*

Main category: stat.CO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Time-delay embedding is a powerful technique for reconstructing the state space of nonlinear time series. However, the fidelity of reconstruction relies on the assumption that the time-delay map is an embedding, which is implicitly justified by Takens' embedding theorem but rarely scrutinised in practice. In this work, we argue that time-delay reconstruction is not always an embedding, and that the non-injectivity of the time-delay map induced by a given measurement function causes irreducible information loss, degrading downstream model performance. Our analysis reveals that this local self-overlap stems from inherent dynamical properties, governed by the competition between the dynamical and the curvature penalty, and the irreducible information loss scales with the product of the geometric separation and the probability mass. We establish a measure-theoretic framework that lifts the dynamics to the space of probability measures, where the multi-valued evolution induced by the non-injectivity is quantified by how far the $n$-step conditional kernel $K^{n}(x, \cdot)$ deviates from a Dirac mass and introduce intrinsic stochasticity $\mathcal{E}^{*}_{n}$, an almost-everywhere, data-driven certificate of deterministic closure, to quantify irreducible information loss without any prior information. We demonstrate that $\mathcal{E}^{*}_{n}$ improves reconstruction quality and downstream model performance on both synthetic and real-world nonlinear data sets.

</details>


### [319] [A Framework for the Bayesian Calibration of Complex and Data-Scarce Models in Applied Sciences](https://arxiv.org/abs/2601.22890)
*Christina Schenk,Ignacio Romero*

Main category: stat.CO

TL;DR: 文章回顾复杂计算机模型贝叶斯校准理论，提出统一框架，介绍用开源Python库ACBICI实现方法，给出实用建议，兼具理论综述与实践指导。


<details>
  <summary>Details</summary>
Motivation: 处理涉及计算成本高的模拟和稀缺实验数据的应用，为工程及相关领域常用计算代码和模型的可靠校准提供支持。

Method: 提出统一框架整合多种贝叶斯校准方法，用开源Python库ACBICI实现算法，采用面向对象结构。

Result: 实现所有算法，以一致方式处理单输出和多输出校准，给出实用校准建议。

Conclusion: 本文既全面回顾统计基础和计算工具，也为用现代软件工具进行贝叶斯校准提供实用指南。

Abstract: In this work, we review the theory involved in the Bayesian calibration of complex computer models, with particular emphasis on their use for applications involving computationally expensive simulations and scarce experimental data. In the article, we present a unified framework that incorporates various Bayesian calibration methods, including well-established approaches. Furthermore, we describe their implementation and use with a new, open-source Python library, ACBICI (A Configurable BayesIan Calibration and Inference Package). All algorithms are implemented with an object-oriented structure designed to be both easy to use and readily extensible. In particular, single-output and multiple-output calibration are addressed in a consistent manner. The article completes the theory and its implementation with practical recommendations for calibrating the problems of interest. These guidelines -- currently unavailable in a unified form elsewhere -- together with the open-source Python library, are intended to support the reliable calibration of computational codes and models commonly used in engineering and related fields. Overall, this work aims to serve both as a comprehensive review of the statistical foundations and (computational) tools required to perform such calculations, and as a practical guide to Bayesian calibration with modern software tools.

</details>


### [320] [A categorical account of the Metropolis-Hastings algorithm](https://arxiv.org/abs/2601.22911)
*Rob Cornish,Andi Q. Wang*

Main category: stat.CO

TL;DR: 本文以MH算法为案例，研究能否从范畴概率角度对其进行表述和分析，给出了一般MH类型采样器关于给定目标分布可逆的充要条件。


<details>
  <summary>Details</summary>
Motivation: 探究能否用范畴概率来表述和分析Metropolis - Hastings (MH)算法。

Method: 先在马尔可夫范畴中表述基本的MCMC概念，用标准CD范畴分析MH核的一部分，再研究CD范畴在交换幺半群上的富集。

Result: 得到了用于抽象推理一系列重要概率概念的表达环境，给出了一般MH类型采样器关于给定目标分布可逆的合成充要条件。

Conclusion: 可以从范畴概率角度对MH算法进行表述和分析，并得到了相关可逆性的充要条件。

Abstract: Metropolis-Hastings (MH) is a foundational Markov chain Monte Carlo (MCMC) algorithm. In this paper, we ask whether it is possible to formulate and analyse MH in terms of categorical probability, using a recent involutive framework for MH-type procedures as a concrete case study. We show how basic MCMC concepts such as invariance and reversibility can be formulated in Markov categories, and how one part of the MH kernel can be analysed using standard CD categories. To go further, we then study enrichments of CD categories over commutative monoids. This gives an expressive setting for reasoning abstractly about a range of important probabilistic concepts, including substochastic kernels, finite and $σ$-finite measures, absolute continuity, singular measures, and Lebesgue decompositions. Using these tools, we give synthetic necessary and sufficient conditions for a general MH-type sampler to be reversible with respect to a given target distribution.

</details>


### [321] [Nested Slice Sampling: Vectorized Nested Sampling for GPU-Accelerated Inference](https://arxiv.org/abs/2601.23252)
*David Yallup,Namu Kroupa,Will Handley*

Main category: stat.CO

TL;DR: 提出嵌套切片采样（NSS）方法，用于解决复杂多峰目标的可扩展推理问题，实验证明其有效性并发布开源实现。


<details>
  <summary>Details</summary>
Motivation: 模型比较和校准不确定性量化需对参数积分，但复杂多峰目标的可扩展推理有挑战，现有嵌套采样方法难以高效实现加速器。

Method: 引入GPU友好、向量化的嵌套切片采样（NSS），利用Hit-and-Run切片采样进行约束更新，并给出设置切片宽度的优化规则。

Result: 在合成目标、高维贝叶斯推理和高斯过程超参数边缘化实验中，NSS能保持准确的证据估计和高质量后验样本，在多峰问题上比现有方法更稳健。

Conclusion: NSS是解决复杂多峰目标可扩展推理的有效方法，开源实现方便采用和复现。

Abstract: Model comparison and calibrated uncertainty quantification often require integrating over parameters, but scalable inference can be challenging for complex, multimodal targets. Nested Sampling is a robust alternative to standard MCMC, yet its typically sequential structure and hard constraints make efficient accelerator implementations difficult. This paper introduces Nested Slice Sampling (NSS), a GPU-friendly, vectorized formulation of Nested Sampling that uses Hit-and-Run Slice Sampling for constrained updates. A tuning analysis yields a simple near-optimal rule for setting the slice width, improving high-dimensional behavior and making per-step compute more predictable for parallel execution. Experiments on challenging synthetic targets, high dimensional Bayesian inference, and Gaussian process hyperparameter marginalization show that NSS maintains accurate evidence estimates and high-quality posterior samples, and is particularly robust on difficult multimodal problems where current state-of-the-art methods such as tempered SMC baselines can struggle. An open-source implementation is released to facilitate adoption and reproducibility.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [322] [Towards Solving the Gilbert-Pollak Conjecture via Large Language Models](https://arxiv.org/abs/2601.22365)
*Yisi Ke,Tianyu Huang,Yankai Shu,Di He,Jingchu Gai,Liwei Wang*

Main category: cs.DM

TL;DR: 本文提出AI系统获Steiner比率新下界0.8559，展示LLM在数学研究潜力。


<details>
  <summary>Details</summary>
Motivation: 过去三十年Steiner比率猜想研究无实质进展，且LLM在解决研究级数学问题潜力待挖掘。

Method: 让LLM生成规则约束的几何引理并实现为可执行代码，构建验证函数获取下界，通过反思改进引理。

Result: 系统建立了Steiner比率新的经认证下界0.8559，仅需数千次LLM调用。

Conclusion: 基于LLM的系统在高级数学研究中有强大潜力。

Abstract: The Gilbert-Pollak Conjecture \citep{gilbert1968steiner}, also known as the Steiner Ratio Conjecture, states that for any finite point set in the Euclidean plane, the Steiner minimum tree has length at least $\sqrt{3}/2 \approx 0.866$ times that of the Euclidean minimum spanning tree (the Steiner ratio). A sequence of improvements through the 1980s culminated in a lower bound of $0.824$, with no substantial progress reported over the past three decades. Recent advances in LLMs have demonstrated strong performance on contest-level mathematical problems, yet their potential for addressing open, research-level questions remains largely unexplored. In this work, we present a novel AI system for obtaining tighter lower bounds on the Steiner ratio. Rather than directly prompting LLMs to solve the conjecture, we task them with generating rule-constrained geometric lemmas implemented as executable code. These lemmas are then used to construct a collection of specialized functions, which we call verification functions, that yield theoretically certified lower bounds of the Steiner ratio. Through progressive lemma refinement driven by reflection, the system establishes a new certified lower bound of 0.8559 for the Steiner ratio. The entire research effort involves only thousands of LLM calls, demonstrating the strong potential of LLM-based systems for advanced mathematical research.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [323] [A Cross-Domain Graph Learning Protocol for Single-Step Molecular Geometry Refinement](https://arxiv.org/abs/2601.22723)
*Chengchun Liu,Wendi Cai,Boxuan Zhao,Fanyang Mo*

Main category: physics.chem-ph

TL;DR: 提出GeoOpt - Net网络，可从低成本力场生成的初始构象一步预测B3LYP/TZVP水平的DFT质量结构，经多方法对比测试，能高效加速基于DFT的量子化学工作流。


<details>
  <summary>Details</summary>
Motivation: 准确分子几何结构是可靠量子化学预测的前提，但DFT优化是高通量分子筛选的主要瓶颈，需开发高效方法。

Method: 提出多分支SE(3)等变几何细化网络GeoOpt - Net，采用两阶段训练策略，通过保真特征调制机制校准。

Result: 在外部类药物分子上测试，GeoOpt - Net实现亚毫埃全原子RMSD和接近零的B3LYP/TZVP单点能量偏差，有非零收敛率，减少重新优化步骤和时间，能量随分子复杂度平滑可预测。

Conclusion: GeoOpt - Net是可扩展、物理一致的几何细化框架，能有效加速基于DFT的量子化学工作流。

Abstract: Accurate molecular geometries are a prerequisite for reliable quantum-chemical predictions, yet density functional theory (DFT) optimization remains a major bottleneck for high-throughput molecular screening. Here we present GeoOpt-Net, a multi-branch SE(3)-equivariant geometry refinement network that predicts DFT-quality structures at the B3LYP/TZVP level of theory in a single forward pass starting from inexpensive initial conformers generated at a low-cost force-field level. GeoOpt-Net is trained using a two-stage strategy in which a broadly pretrained geometric representation is subsequently fine-tuned to approach B3LYP/TZVP-level accuracy, with theory- and basis-set-aware calibration enabled by a fidelity-aware feature modulation (FAFM) mechanism. Benchmarking against representative approaches spanning classical conformer generation (RDKit), semiempirical quantum methods (xTB), data-driven geometry refinement pipelines (Auto3D), and machine-learning interatomic potentials (UMA) on external drug-like molecules demonstrates that GeoOpt-Net achieves sub-milli-Å all-atom RMSD with near-zero B3LYP/TZVP single-point energy deviations, indicating DFT-ready geometries that closely reproduce both structural and energetic references. Beyond geometric metrics, GeoOpt-Net generates initial guesses intrinsically compatible with DFT convergence criteria, yielding nonzero ``All-YES'' convergence rates (65.0\% under loose and 33.4\% under default thresholds), and substantially reducing re-optimization steps and wall-clock time. GeoOpt-Net further exhibits smooth and predictable energy scaling with molecular complexity while preserving key electronic observables such as dipole moments. Collectively, these results establish GeoOpt-Net as a scalable, physically consistent geometry refinement framework that enables efficient acceleration of DFT-based quantum-chemical workflows.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [324] [Screen, Match, and Cache: A Training-Free Causality-Consistent Reference Frame Framework for Human Animation](https://arxiv.org/abs/2601.22160)
*Jianan Wang,Nailei Hei,Li He,Huanzhen Wang,Aoxing Li,Haofen Wang,Yan Wang,Wenqiang Zhang*

Main category: cs.GR

TL;DR: 提出无训练框架FrameCache用于人体动画，实验证明其能提升连贯性和稳定性，但效果依赖基线推理和一致性，后续需研究兼容性和自适应缓存机制。


<details>
  <summary>Details</summary>
Motivation: 人体动画在建模长距离依赖同时保持帧质量有挑战，受人类利用过去观察解释动作的能力启发。

Method: 提出无训练的三阶段框架FrameCache，包括Screen、Cache和Match阶段。

Result: 在标准基准上的大量实验表明，FrameCache能持续提升时间连贯性和视觉稳定性，且能与多种基线无缝集成。

Conclusion: FrameCache有积极效果，但有效性依赖基线时间推理和真实 - 合成一致性，需开展兼容性条件和自适应缓存机制的未来工作。

Abstract: Human animation aims to generate temporally coherent and visually consistent videos over long sequences, yet modeling long-range dependencies while preserving frame quality remains challenging. Inspired by the human ability to leverage past observations for interpreting ongoing actions, we propose FrameCache, a training-free three-stage framework consisting of Screen, Cache, and Match. In the Screen stage, a multi-dimensional, quality-aware mechanism with adaptive thresholds dynamically selects informative frames; the Cache stage maintains a reference pool using a dynamic replacement-hit strategy, preserving both diversity and relevance; and the Match stage extracts behavioral features to perform motion-consistent reference matching for coherent animation guidance. Extensive experiments on standard benchmarks demonstrate that FrameCache consistently improves temporal coherence and visual stability while integrating seamlessly with diverse baselines. Despite these encouraging results, further analysis reveals that its effectiveness depends on baseline temporal reasoning and real-synthetic consistency, motivating future work on compatibility conditions and adaptive cache mechanisms. Code will be made publicly available.

</details>


### [325] [Learning to Build Shapes by Extrusion](https://arxiv.org/abs/2601.22858)
*Thor Vestergaard Christiansen,Karran Pandey,Alba Reinders,Karan Singh,Morten Rieger Hannemose,J. Andreas Bærentzen*

Main category: cs.GR

TL;DR: 提出文本编码挤出（TEE）表示法及用大语言模型从TEE生成3D网格的方法，能实现多种网格处理任务。


<details>
  <summary>Details</summary>
Motivation: 现有的基于transformer的模型在生成3D网格时存在局限性，需要一种能自然支持任意输出面数、生成流形网格且支持编辑的方法。

Method: 引入TEE表示法，将四边形网格库分解为组成环，对大语言模型进行微调以学习通过一系列挤出步骤重新组装网格。

Result: 该表示法能够实现网格重建、新形状合成以及为现有网格添加新特征。

Conclusion: 提出的TEE表示法和基于大语言模型的方法在3D网格处理方面具有有效性和实用性。

Abstract: We introduce Text Encoded Extrusion (TEE), a text-based representation that expresses mesh construction as sequences of face extrusions rather than polygon lists, and a method for generating 3D meshes from TEE using a large language model (LLM). By learning extrusion sequences that assemble a mesh, similar to the way artists create meshes, our approach naturally supports arbitrary output face counts and produces manifold meshes by design, in contrast to recent transformer-based models. The learnt extrusion sequences can also be applied to existing meshes - enabling editing in addition to generation. To train our model, we decompose a library of quadrilateral meshes with non-self-intersecting face loops into constituent loops, which can be viewed as their building blocks, and finetune an LLM on the steps for reassembling the meshes by performing a sequence of extrusions. We demonstrate that our representation enables reconstruction, novel shape synthesis, and the addition of new features to existing meshes.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [326] [Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram](https://arxiv.org/abs/2601.22203)
*Huinan Xu,Xuyang Feng,Junhong Chen,Junchen Liu,Kaiwen Deng,Kai Ding,Shengning Long,Jiaxue Shuai,Zhaorong Li,Shiping Liu,Guirong Xue,Zhan Xiao*

Main category: q-bio.GN

TL;DR: 提出Gengram模块，集成到GFMs中提升性能和可解释性，代码和模型可获取。


<details>
  <summary>Details</summary>
Motivation: 当前基因组基础模型依赖大量神经计算隐式近似生物基序，效率不高。

Method: 提出Gengram条件记忆模块，通过基因组特定哈希方案为多碱基基序引入高效查找原语。

Result: 集成到先进GFMs中，在多个功能基因组学任务中提升达14%，有强大架构泛化性，潜在空间有有意义表征。

Conclusion: Gengram提升经验性能和机制可解释性，为下一代GFMs提供可扩展和生物对齐途径。

Abstract: Current genomic foundation models (GFMs) rely on extensive neural computation to implicitly approximate conserved biological motifs from single-nucleotide inputs. We propose Gengram, a conditional memory module that introduces an explicit and highly efficient lookup primitive for multi-base motifs via a genomic-specific hashing scheme, establishing genomic "syntax". Integrated into the backbone of state-of-the-art GFMs, Gengram achieves substantial gains (up to 14%) across several functional genomics tasks. The module demonstrates robust architectural generalization, while further inspection of Gengram's latent space reveals the emergence of meaningful representations that align closely with fundamental biological knowledge. By establishing structured motif memory as a modeling primitive, Gengram simultaneously boosts empirical performance and mechanistic interpretability, providing a scalable and biology-aligned pathway for the next generation of GFMs. The code is available at https://github.com/zhejianglab/Genos, and the model checkpoint is available at https://huggingface.co/ZhejiangLab/Gengram.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [327] [SPARK: Real-Time Monitoring of Multi-Faceted Programming Exercises](https://arxiv.org/abs/2601.22256)
*Yinuo Yang,Ashley Ge Zhang,Steve Oney,April Yi Wang*

Main category: cs.HC

TL;DR: 本文介绍了用于监测编程练习的SPARK仪表盘，构建数据集并通过评估提供其实用性见解。


<details>
  <summary>Details</summary>
Motivation: 监测课堂编程练习时理解学生进度困难，尤其是多方面复杂问题，需工具解决。

Method: 引入SPARK仪表盘，它能灵活分组子步骤、建议自动化测试、生成可视化，还能检查中间输出；构建22名学习者的按键编码数据集，对16名编程教师进行评估。

Result: 构建了40分钟按键编码数据集，通过评估提供了SPARK实用性的见解。

Conclusion: SPARK可帮助教师监测编程练习，应对理解学生进度的挑战。

Abstract: Monitoring in-class programming exercises can help instructors identify struggling students and common challenges. However, understanding students' progress can be prohibitively difficult, particularly for multi-faceted problems that include multiple steps with complex interdependencies, have no predictable completion order, or involve evaluation criteria that are difficult to summarize across many students (e.g., exercises building interactive web-based user interfaces). We introduce SPARK, a coding exercise monitoring dashboard designed to address these challenges. SPARK allows instructors to flexibly group substeps into checkpoints based on exercise requirements, suggests automated tests for these checkpoints, and generates visualizations to track progress across steps. SPARK also allows instructors to inspect intermediate outputs, providing deeper insights into solution variations. We also construct a dataset of 40-minute keystroke coding data from N=22 learners solving two web programming exercises and provide empirical insights into the perceived usefulness of SPARK through a within-subjects evaluation with 16 programming instructors.

</details>


### [328] [Qualitative Evaluation of LLM-Designed GUI](https://arxiv.org/abs/2601.22759)
*Bartosz Sawicki,Tomasz Les,Dariusz Parzych,Aleksandra Wycisk-Ficek,Pawel Trebacz,Pawel Zawadzki*

Main category: cs.HC

TL;DR: 研究大语言模型（LLMs）在自动图形用户界面（GUI）设计中的可用性和适应性，发现其适合早期UI原型设计，但仍需人工干预。


<details>
  <summary>Details</summary>
Motivation: 随着生成式人工智能发展，探索LLMs在自动GUI设计中的应用，研究其满足不同用户需求的能力。

Method: 使用2025年1月的三个先进模型（OpenAI GPT o3 - mini - high、DeepSeek R1和Anthropic Claude 3.5 Sonnet）为三种界面类型（聊天系统、技术团队面板和经理仪表盘）生成模型，并进行专家评估。

Result: LLMs能有效创建结构化布局，但在满足可访问性标准和提供交互功能方面有挑战；可部分为不同用户角色定制界面，但缺乏深度上下文理解。

Conclusion: LLMs是早期UI原型设计的有前景工具，但人工干预对确保可用性、可访问性和用户满意度至关重要。

Abstract: As generative artificial intelligence advances, Large Language Models (LLMs) are being explored for automated graphical user interface (GUI) design. This study investigates the usability and adaptability of LLM-generated interfaces by analysing their ability to meet diverse user needs. The experiments included utilization of three state-of-the-art models from January 2025 (OpenAI GPT o3-mini-high, DeepSeek R1, and Anthropic Claude 3.5 Sonnet) generating mockups for three interface types: a chat system, a technical team panel, and a manager dashboard. Expert evaluations revealed that while LLMs are effective at creating structured layouts, they face challenges in meeting accessibility standards and providing interactive functionality. Further testing showed that LLMs could partially tailor interfaces for different user personas but lacked deeper contextual understanding. The results suggest that while LLMs are promising tools for early-stage UI prototyping, human intervention remains critical to ensure usability, accessibility, and user satisfaction.

</details>


### [329] [PersonaCite: VoC-Grounded Interviewable Agentic Synthetic AI Personas for Verifiable User and Design Research](https://arxiv.org/abs/2601.22288)
*Mario Truss*

Main category: cs.HC

TL;DR: 提出PersonaCite系统，通过检索增强交互将AI角色重塑为有证据边界的研究工具，经研究提出Persona Provenance Cards。


<details>
  <summary>Details</summary>
Motivation: 解决基于提示的角色常产生有说服力但无法验证的回复且掩盖证据基础的问题。

Method: 构建PersonaCite系统，进行半结构化访谈和对14位行业专家的部署研究。

Result: 确定了感知到的好处、有效性担忧和设计紧张等初步发现。

Conclusion: 提出Persona Provenance Cards作为以人为主的设计工作流中负责任使用AI角色的文档模式。

Abstract: LLM-based and agent-based synthetic personas are increasingly used in design and product decision-making, yet prior work shows that prompt-based personas often produce persuasive but unverifiable responses that obscure their evidentiary basis. We present PersonaCite, an agentic system that reframes AI personas as evidence-bounded research instruments through retrieval-augmented interaction. Unlike prior approaches that rely on prompt-based roleplaying, PersonaCite retrieves actual voice-of-customer artifacts during each conversation turn, constrains responses to retrieved evidence, explicitly abstains when evidence is missing, and provides response-level source attribution. Through semi-structured interviews and deployment study with 14 industry experts, we identify preliminary findings on perceived benefits, validity concerns, and design tensions, and propose Persona Provenance Cards as a documentation pattern for responsible AI persona use in human-centered design workflows.

</details>


### [330] [From Retrieving Information to Reasoning with AI: Exploring Different Interaction Modalities to Support Human-AI Coordination in Clinical Decision-Making](https://arxiv.org/abs/2601.22338)
*Behnam Rahdari,Sameer Shaikh,Jonathan H Chen,Tobias Gerstenberg,Shriti Raj*

Main category: cs.HC

TL;DR: 研究临床医生对不同交互方式用于决策支持的看法，发现使用方式、参与度受交互设置和认知风格影响，无通用交互方式。


<details>
  <summary>Details</summary>
Motivation: LLMs对临床医生表现的影响不明确，不清楚临床医生使用情况及与传统系统对比，限制设计新机制。

Method: 对12名临床医生进行定性研究，考察其对不同交互方式的看法。

Result: 参与者以工具为中心使用LLM工具，参与度随交互设置和认知风格变化，不同交互方式各有利弊。

Conclusion: 不存在适用于所有情况的交互方式。

Abstract: LLMs are popular among clinicians for decision-support because of simple text-based interaction. However, their impact on clinicians' performance is ambiguous. Not knowing how clinicians use this new technology and how they compare it to traditional clinical decision-support systems (CDSS) restricts designing novel mechanisms that overcome existing tool limitations and enhance performance and experience. This qualitative study examines how clinicians (n=12) perceive different interaction modalities (text-based conversation with LLMs, interactive and static UI, and voice) for decision-support. In open-ended use of LLM-based tools, our participants took a tool-centric approach using them for information retrieval and confirmation with simple prompts instead of use as active deliberation partners that can handle complex questions. Critical engagement emerged with changes to the interaction setup. Engagement also differed with individual cognitive styles. Lastly, benefits and drawbacks of interaction with text, voice and traditional UIs for clinical decision-support show the lack of a one-size-fits-all interaction modality.

</details>


### [331] [AI and My Values: User Perceptions of LLMs' Ability to Extract, Embody, and Explain Human Values from Casual Conversations](https://arxiv.org/abs/2601.22440)
*Bhada Yun,Renn Su,April Yi Wang*

Main category: cs.HC

TL;DR: 介绍价值对齐感知工具包VAPT研究大语言模型对人类价值观的反映及人们对此的判断，部分参与者认为AI能理解人类价值观，警告有‘武器化同理心’，VAPT有设计意义。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型如何反映人们的价值观以及人们如何评判这些反映。

Method: 让20名参与者与类人聊天机器人交流一个月，然后用工具包进行2小时访谈评估AI对人类价值观的提取、体现和解释能力。

Result: 13名参与者认为AI能理解人类价值观，参与者认为体验有自我反思价值，会被AI推理说服。

Conclusion: 警告‘武器化同理心’，VAPT可用于评估和负责任地构建价值对齐的对话式代理。

Abstract: Does AI understand human values? While this remains an open philosophical question, we take a pragmatic stance by introducing VAPT, the Value-Alignment Perception Toolkit, for studying how LLMs reflect people's values and how people judge those reflections. 20 participants texted a human-like chatbot over a month, then completed a 2-hour interview with our toolkit evaluating AI's ability to extract (pull details regarding), embody (make decisions guided by), and explain (provide proof of) human values. 13 participants left our study convinced that AI can understand human values. Participants found the experience insightful for self-reflection and found themselves getting persuaded by the AI's reasoning. Thus, we warn about "weaponized empathy": a potentially dangerous design pattern that may arise in value-aligned, yet welfare-misaligned AI. VAPT offers concrete artifacts and design implications to evaluate and responsibly build value-aligned conversational agents with transparency, consent, and safeguards as AI grows more capable and human-like into the future.

</details>


### [332] [Does My Chatbot Have an Agenda? Understanding Human and AI Agency in Human-Human-like Chatbot Interaction](https://arxiv.org/abs/2601.22452)
*Bhada Yun,Evgenia Taranova,April Yi Wang*

Main category: cs.HC

TL;DR: 研究人机聊天室中的能动性问题，发现其是共享体验并提出框架及相关设计建议。


<details>
  <summary>Details</summary>
Motivation: 人工智能聊天机器人向伴侣角色转变，引发关于人机聊天室中能动性的关键问题。

Method: 开展为期一个月的纵向研究，涉及22名成年人与自制的聊天伴侣交流，随后进行半结构化访谈及相关回顾等。

Result: 人机聊天室中的能动性是一种涌现的、共享的体验，控制权会依次转变和共同构建，并引入一个3x5的框架。

Conclusion: 主张采用半透明设计、为能动性协商提供空间并制定面向能动性感知对话式AI的指南。

Abstract: AI chatbots are shifting from tools to companions. This raises critical questions about agency: who drives conversations and sets boundaries in human-AI chatrooms? We report a month-long longitudinal study with 22 adults who chatted with Day, an LLM companion we built, followed by a semi-structured interview with post-hoc elicitation of notable moments, cross-participant chat reviews, and a 'strategy reveal' disclosing Day's vertical (depth-seeking) vs. horizontal (breadth-seeking) modes. We discover that agency in human-AI chatrooms is an emergent, shared experience: as participants claimed agency by setting boundaries and providing feedback, and the AI was perceived to steer intentions and drive execution, control shifted and was co-constructed turn-by-turn. We introduce a 3-by-5 framework mapping who (human, AI, hybrid) x agency action (Intention, Execution, Adaptation, Delimitation, Negotiation), modulated by individual and environmental factors. Ultimately, we argue for translucent design (i.e. transparency-on-demand), spaces for agency negotiation, and guidelines toward agency-aware conversational AI.

</details>


### [333] [Human-Centered Explainability in AI-Enhanced UI Security Interfaces: Designing Trustworthy Copilots for Cybersecurity Analysts](https://arxiv.org/abs/2601.22653)
*Mona Rajhans*

Main category: cs.HC

TL;DR: 本文对AI驱动的安全仪表盘的解释设计策略进行了混合方法研究，比较了多种解释风格，揭示其对用户的影响并提出多项贡献，推动了网络安全领域以人为本的AI工具设计。


<details>
  <summary>Details</summary>
Motivation: 人工智能副驾驶在企业网络安全平台被广泛应用，但系统有效性依赖用户对输出的理解和信任，现有研究对高风险决策场景下界面的解释呈现关注不足。

Method: 通过建立解释风格分类法，并对安全从业者进行对照用户研究，比较自然语言推理、置信度可视化、反事实解释和混合方法。

Result: 解释风格会显著影响用户的信任校准、决策准确性和认知负荷。

Conclusion: 提供了安全副驾驶解释界面可用性的实证证据、企业UI集成可解释性的设计指南以及解释策略与安全运营中心分析师需求对齐的框架，对网络安全及其他高风险领域的可解释性有广泛意义。

Abstract: Artificial intelligence (AI) copilots are increasingly integrated into enterprise cybersecurity platforms to assist analysts in threat detection, triage, and remediation. However, the effectiveness of these systems depends not only on the accuracy of underlying models but also on the degree to which users can understand and trust their outputs. Existing research on algorithmic explainability has largely focused on model internals, while little attention has been given to how explanations should be surfaced in user interfaces for high-stakes decision-making contexts [8], [5], [6]. We present a mixed-methods study of explanation design strategies in AI-driven security dashboards. Through a taxonomy of explanation styles and a controlled user study with security practitioners, we compare natural language rationales, confidence visualizations, counterfactual explanations, and hybrid approaches. Our findings show that explanation style significantly affects user trust calibration, decision accuracy, and cognitive load. We contribute (1) empirical evidence on the usability of explanation interfaces for security copilots, (2) design guidelines for integrating explainability into enterprise UIs, and (3) a framework for aligning explanation strategies with analyst needs in security operations centers (SOCs). This work advances the design of human-centered AI tools in cybersecurity and provides broader implications for explainability in other high-stakes domains.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [334] [In Vino Veritas and Vulnerabilities: Examining LLM Safety via Drunk Language Inducement](https://arxiv.org/abs/2601.22169)
*Anudeex Shetty,Aditya Joshi,Salil S. Kanhere*

Main category: cs.CL

TL;DR: 研究醉酒语言对大语言模型安全的影响，提出三种诱导方法，发现诱导后模型更易越狱和隐私泄露，表明其对模型安全构成风险。


<details>
  <summary>Details</summary>
Motivation: 人类在酒精影响下易出现不良行为和隐私泄露，研究醉酒语言作为大语言模型安全故障的驱动因素。

Method: 研究三种在大语言模型中诱导醉酒语言的机制：基于角色的提示、因果微调、基于强化的后训练，并使用手动评估和LLM评估器进行评估。

Result: 在5个大语言模型上评估，发现诱导醉酒语言后在JailbreakBench和ConfAIde上更易出现越狱和隐私泄露。

Conclusion: 提出的醉酒语言诱导方法简单高效，对应人类醉酒行为和模型拟人化，对大语言模型安全有显著风险，可用于模型安全调优的反制测试。

Abstract: Humans are susceptible to undesirable behaviours and privacy leaks under the influence of alcohol. This paper investigates drunk language, i.e., text written under the influence of alcohol, as a driver for safety failures in large language models (LLMs). We investigate three mechanisms for inducing drunk language in LLMs: persona-based prompting, causal fine-tuning, and reinforcement-based post-training. When evaluated on 5 LLMs, we observe a higher susceptibility to jailbreaking on JailbreakBench (even in the presence of defences) and privacy leaks on ConfAIde, where both benchmarks are in English, as compared to the base LLMs as well as previously reported approaches. Via a robust combination of manual evaluation and LLM-based evaluators and analysis of error categories, our findings highlight a correspondence between human-intoxicated behaviour, and anthropomorphism in LLMs induced with drunk language. The simplicity and efficiency of our drunk language inducement approaches position them as potential counters for LLM safety tuning, highlighting significant risks to LLM safety.

</details>


### [335] [MERMAID: Memory-Enhanced Retrieval and Reasoning with Multi-Agent Iterative Knowledge Grounding for Veracity Assessment](https://arxiv.org/abs/2601.22361)
*Yupeng Cao,Chengyang He,Yangyang Yu,Ping Wang,K. P. Subbalakshmi*

Main category: cs.CL

TL;DR: 提出MERMAID框架用于在线内容真实性评估，结合检索和推理过程，提升效率和一致性，实验表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有在线内容真实性评估方法将证据检索视为静态孤立步骤，无法有效管理和复用检索到的证据。

Method: 提出MERMAID框架，结合代理驱动搜索、结构化知识表示和持久内存模块，在推理-行动迭代过程中进行动态证据获取和跨声明证据复用。

Result: 在多个数据集上使用多种大语言模型评估，达到了最先进的性能，同时提高了搜索效率。

Conclusion: 将检索、推理和记忆协同起来进行在线内容真实性评估是有效的。

Abstract: Assessing the veracity of online content has become increasingly critical. Large language models (LLMs) have recently enabled substantial progress in automated veracity assessment, including automated fact-checking and claim verification systems. Typical veracity assessment pipelines break down complex claims into sub-claims, retrieve external evidence, and then apply LLM reasoning to assess veracity. However, existing methods often treat evidence retrieval as a static, isolated step and do not effectively manage or reuse retrieved evidence across claims. In this work, we propose MERMAID, a memory-enhanced multi-agent veracity assessment framework that tightly couples the retrieval and reasoning processes. MERMAID integrates agent-driven search, structured knowledge representations, and a persistent memory module within a Reason-Action style iterative process, enabling dynamic evidence acquisition and cross-claim evidence reuse. By retaining retrieved evidence in an evidence memory, the framework reduces redundant searches and improves verification efficiency and consistency. We evaluate MERMAID on three fact-checking benchmarks and two claim-verification datasets using multiple LLMs, including GPT, LLaMA, and Qwen families. Experimental results show that MERMAID achieves state-of-the-art performance while improving the search efficiency, demonstrating the effectiveness of synergizing retrieval, reasoning, and memory for reliable veracity assessment.

</details>


### [336] [Context Structure Reshapes the Representational Geometry of Language Models](https://arxiv.org/abs/2601.22364)
*Eghbal A. Hosseini,Yuxuan Li,Yasaman Bahri,Declan Campbell,Andrew Kyle Lampinen*

Main category: cs.CL

TL;DR: 研究大语言模型在上下文学习中表征拉直现象，发现不同任务设置下表现不同，表明上下文学习非单一过程。


<details>
  <summary>Details</summary>
Motivation: 将大语言模型深层表征拉直和上下文学习两方面研究结合，探索上下文学习中是否存在表征拉直现象。

Method: 在Gemma 2模型上对多种上下文任务测量表征拉直情况。

Result: 连续预测设置下，增加上下文会提高神经序列轨迹拉直程度且与模型预测提升相关；结构化预测设置下，拉直情况不一致。

Conclusion: 上下文学习不是单一过程，大语言模型像瑞士军刀，会根据任务结构动态选择策略。

Abstract: Large Language Models (LLMs) have been shown to organize the representations of input sequences into straighter neural trajectories in their deep layers, which has been hypothesized to facilitate next-token prediction via linear extrapolation. Language models can also adapt to diverse tasks and learn new structure in context, and recent work has shown that this in-context learning (ICL) can be reflected in representational changes. Here we bring these two lines of research together to explore whether representation straightening occurs \emph{within} a context during ICL. We measure representational straightening in Gemma 2 models across a diverse set of in-context tasks, and uncover a dichotomy in how LLMs' representations change in context. In continual prediction settings (e.g., natural language, grid world traversal tasks) we observe that increasing context increases the straightness of neural sequence trajectories, which is correlated with improvement in model prediction. Conversely, in structured prediction settings (e.g., few-shot tasks), straightening is inconsistent -- it is only present in phases of the task with explicit structure (e.g., repeating a template), but vanishes elsewhere. These results suggest that ICL is not a monolithic process. Instead, we propose that LLMs function like a Swiss Army knife: depending on task structure, the LLM dynamically selects between strategies, only some of which yield representational straightening.

</details>


### [337] [SP^2DPO: An LLM-assisted Semantic Per-Pair DPO Generalization](https://arxiv.org/abs/2601.22385)
*Chaoyue He,Xin Zhou,Di Wang,Hong Xu,Wei Liu,Chunyan Miao*

Main category: cs.CL

TL;DR: 提出SP2DPO方法改进Direct Preference Optimization (DPO)，在AlpacaEval 2.0上有一定表现，相关代码等将发布。


<details>
  <summary>Details</summary>
Motivation: 现实偏好语料存在异质性，而DPO使用单一全局温度beta处理所有偏好对信息，未考虑这种异质性。

Method: 引入SP2DPO方法，用特定于实例的调度beta_i取代全局温度，beta_i由教师语言模型生成的结构化语义差距注释预先离线确定。

Result: 在AlpacaEval 2.0上，SP2DPO与调整后的全局beta DPO基线有竞争力，在四个学生骨干中的两个上提高了长度控制后的胜率。

Conclusion: SP2DPO能避免每个模型的beta扫描，在改进偏好优化方面有一定价值。

Abstract: Direct Preference Optimization (DPO) controls the trade-off between fitting preference labels and staying close to a reference model using a single global temperature beta, implicitly treating all preference pairs as equally informative. Real-world preference corpora are heterogeneous: they mix high-signal, objective failures (for example, safety, factuality, instruction violations) with low-signal or subjective distinctions (for example, style), and also include label noise. We introduce our method, SP2DPO (Semantic Per-Pair DPO), a generalization that replaces the global temperature with an instance-specific schedule beta_i pre-decided offline from structured semantic-gap annotations (category, magnitude, confidence) produced by teacher language models. We instantiate this procedure on the UltraFeedback preference corpus (59,960 pairs), enabling large-scale construction of an auditable beta_i artifact, and incur zero training-time overhead: the inner-loop optimizer remains standard DPO with beta set per pair. We focus our empirical study on AlpacaEval 2.0, reporting both raw win rate and length-controlled win rate. Across four open-weight, instruction-tuned student backbones (4B-8B), SP2DPO is competitive with a tuned global-beta DPO baseline and improves AlpacaEval 2.0 length-controlled win rate on two of four backbones, while avoiding per-model beta sweeps. All code, annotations, and artifacts will be released.

</details>


### [338] [Culturally Grounded Personas in Large Language Models: Characterization and Alignment with Socio-Psychological Value Frameworks](https://arxiv.org/abs/2601.22396)
*Candida M. Greco,Lucio La Cava,Andrea Tagarelli*

Main category: cs.CL

TL;DR: 研究大语言模型生成的文化化人物形象与既定框架的一致性，提出方法评估跨文化结构和道德差异。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成的合成人物形象能否准确反映不同文化背景下的世界和道德价值体系尚不明确。

Method: 基于世界价值观调查（WVS）的变量概念化并生成大语言模型人物形象，从英格尔哈特 - 韦尔策尔文化地图定位、与WVS的人口统计层面一致性、道德基础问卷得出的道德概况三个方面分析。

Result: 未提及具体结果。

Conclusion: 所提出的文化化人物形象生成和分析方法能够评估跨文化结构和道德差异。

Abstract: Despite the growing utility of Large Language Models (LLMs) for simulating human behavior, the extent to which these synthetic personas accurately reflect world and moral value systems across different cultural conditionings remains uncertain. This paper investigates the alignment of synthetic, culturally-grounded personas with established frameworks, specifically the World Values Survey (WVS), the Inglehart-Welzel Cultural Map, and Moral Foundations Theory. We conceptualize and produce LLM-generated personas based on a set of interpretable WVS-derived variables, and we examine the generated personas through three complementary lenses: positioning on the Inglehart-Welzel map, which unveils their interpretation reflecting stable differences across cultural conditionings; demographic-level consistency with the World Values Survey, where response distributions broadly track human group patterns; and moral profiles derived from a Moral Foundations questionnaire, which we analyze through a culture-to-morality mapping to characterize how moral responses vary across different cultural configurations. Our approach of culturally-grounded persona generation and analysis enables evaluation of cross-cultural structure and moral variation.

</details>


### [339] [Towards the Holographic Characteristic of LLMs for Efficient Short-text Generation](https://arxiv.org/abs/2601.22546)
*Shun Qian,Bingquan Liu,Chengjie Sun,Zhen Xu,Baoxun Wang*

Main category: cs.CL

TL;DR: 本文研究大语言模型生成特征，发现全息特征并提出插件HOLO，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前对大语言模型强大生成能力的具体特征研究较少，本文旨在深入探究这些生成特征。

Method: 发现语言模型的全息特征，提出利用该特征的插件HOLO，并使用并行词汇约束文本生成方法完善句子。

Result: 在短文本生成场景下对不同架构和规模语言模型进行大量实验，HOLO在自动和类人评估指标上与基线表现相当。

Conclusion: HOLO有效，凸显了语言模型全息特征的潜力。

Abstract: The recent advancements in Large Language Models (LLMs) have attracted interest in exploring their in-context learning abilities and chain-of-thought capabilities. However, there are few studies investigating the specific traits related to the powerful generation capacity of LLMs. This paper aims to delve into the generation characteristics exhibited by LLMs. Through our investigation, we have discovered that language models tend to capture target-side keywords at the beginning of the generation process. We name this phenomenon the Holographic Characteristic of language models. For the purpose of exploring this characteristic and further improving the inference efficiency of language models, we propose a plugin called HOLO, which leverages the Holographic Characteristic to extract target-side keywords from language models within a limited number of generation steps and complements the sentence with a parallel lexically constrained text generation method. To verify the effectiveness of HOLO, we conduct massive experiments on language models of varying architectures and scales in the short-text generation scenario. The results demonstrate that HOLO achieves comparable performance to the baselines in terms of both automatic and human-like evaluation metrics and highlight the potential of the Holographic Characteristic.

</details>


### [340] [Are LLM Evaluators Really Narcissists? Sanity Checking Self-Preference Evaluations](https://arxiv.org/abs/2601.22548)
*Dani Roytburg,Matthew Bozoukov,Matthew Nguyen,Mackenzie Puig-Hall,Narmeen Oozeer*

Main category: cs.CL

TL;DR: 研究发现大语言模型作为评判者时有自我偏好问题，提出评估者质量基线减少测量误差，部分初始发现不再具统计显著性，为后续研究排除噪声数据。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型评判时评估偏差难以区分自恋与实验混淆因素的问题，减少测量误差。

Method: 发现核心方法混淆因素，引入评估者质量基线，比较评判者误选自身与误选其他模型的概率。

Result: 减少测量误差89.6%，对37448个查询评估后仅51%初始发现具统计显著性，还对评判者的“简单”与“困难”评估投票熵进行表征。

Conclusion: 所提出的基线能为自我偏好的后续研究排除噪声数据，有助于编目和分离评判者偏差效应的研究。

Abstract: Recent research has shown that large language models (LLM) favor own outputs when acting as judges, undermining the integrity of automated post-training and evaluation workflows. However, it is difficult to disentangle which evaluation biases are explained by narcissism versus general experimental confounds, distorting measurements of self-preference bias. We discover a core methodological confound which could reduce measurement error by 89.6%. Specifically, LLM evaluators may deliver self-preferring verdicts when the judge responds to queries which they completed incorrectly themselves; this would be true regardless of whether one of their responses is their own. To decouple self-preference signals from noisy outputs on hard problems, we introduce an Evaluator Quality Baseline, which compares the probability that a judge incorrectly votes for itself against the probability that it votes for an incorrect response from another model. Evaluating this simple baseline on 37,448 queries, only 51% of initial findings retain statistical significance. Finally, we turn towards characterizing the entropy of "easy" versus "hard" evaluation votes from LLM judges. Our corrective baseline enables future research on self-preference by eliminating noisy data from potential solutions. More widely, this work contributes to the growing body of work on cataloging and isolating judge-bias effects.

</details>


### [341] [SpanNorm: Reconciling Training Stability and Performance in Deep Transformers](https://arxiv.org/abs/2601.22580)
*Chao Wang,Bei Li,Jiaqi Zhang,Xinyu Liu,Yuchun Fan,Linkun Lyu,Xin Chen,Jingang Wang,Tong Xiao,Peng Pei,Xunliang Cai*

Main category: cs.CL

TL;DR: 提出SpanNorm解决Transformer架构中PreNorm和PostNorm的权衡问题，理论和实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer架构中PreNorm和PostNorm在训练稳定性和性能上的权衡问题。

Method: 提出SpanNorm，建立跨越整个transformer块的残差连接稳定信号传播，采用PostNorm式计算增强性能，并结合缩放策略。

Result: 理论上SpanNorm能保持信号方差有界，防止梯度问题和缓解表征崩溃；实验上在密集和混合专家场景均优于标准归一化方案。

Conclusion: SpanNorm为更强大稳定的Transformer架构铺平道路。

Abstract: The success of Large Language Models (LLMs) hinges on the stable training of deep Transformer architectures. A critical design choice is the placement of normalization layers, leading to a fundamental trade-off: the ``PreNorm'' architecture ensures training stability at the cost of potential performance degradation in deep models, while the ``PostNorm'' architecture offers strong performance but suffers from severe training instability. In this work, we propose SpanNorm, a novel technique designed to resolve this dilemma by integrating the strengths of both paradigms. Structurally, SpanNorm establishes a clean residual connection that spans the entire transformer block to stabilize signal propagation, while employing a PostNorm-style computation that normalizes the aggregated output to enhance model performance. We provide a theoretical analysis demonstrating that SpanNorm, combined with a principled scaling strategy, maintains bounded signal variance throughout the network, preventing the gradient issues that plague PostNorm models, and also alleviating the representation collapse of PreNorm. Empirically, SpanNorm consistently outperforms standard normalization schemes in both dense and Mixture-of-Experts (MoE) scenarios, paving the way for more powerful and stable Transformer architectures.

</details>


### [342] [Rethinking LLM-as-a-Judge: Representation-as-a-Judge with Small Language Models via Semantic Capacity Asymmetry](https://arxiv.org/abs/2601.22588)
*Zhuochun Li,Yong Zhang,Ming Li,Yuelyu Ji,Yiming Zeng,Ning Cheng,Yun Zhu,Yanmeng Wang,Shaojun Wang,Jing Xiao,Daqing He*

Main category: cs.CL

TL;DR: 研究小型模型能否通过利用内部表征进行高效评估，提出范式转变，实验表明INSPECTOR框架效果好。


<details>
  <summary>Details</summary>
Motivation: 当前“LLM - as - a - Judge”范式存在成本高、不透明和对提示设计敏感等问题，探索小型模型作为评估器的可能性。

Method: 提出语义容量不对称假设，提出“Representation - as - a - Judge”范式，通过INSPECTOR框架从小型模型表征预测评估分数。

Result: 在推理基准测试中，INSPECTOR显著优于基于提示的小型语言模型，接近全大型语言模型评估器。

Conclusion: INSPECTOR提供了一种更高效、可靠和可解释的可扩展评估替代方法。

Abstract: Large language models (LLMs) are widely used as reference-free evaluators via prompting, but this "LLM-as-a-Judge" paradigm is costly, opaque, and sensitive to prompt design. In this work, we investigate whether smaller models can serve as efficient evaluators by leveraging internal representations instead of surface generation. We uncover a consistent empirical pattern: small LMs, despite with weak generative ability, encode rich evaluative signals in their hidden states. This motivates us to propose the Semantic Capacity Asymmetry Hypothesis: evaluation requires significantly less semantic capacity than generation and can be grounded in intermediate representations, suggesting that evaluation does not necessarily need to rely on large-scale generative models but can instead leverage latent features from smaller ones. Our findings motivate a paradigm shift from LLM-as-a-Judge to Representation-as-a-Judge, a decoding-free evaluation strategy that probes internal model structure rather than relying on prompted output. We instantiate this paradigm through INSPECTOR, a probing-based framework that predicts aspect-level evaluation scores from small model representations. Experiments on reasoning benchmarks (GSM8K, MATH, GPQA) show that INSPECTOR substantially outperforms prompting-based small LMs and closely approximates full LLM judges, while offering a more efficient, reliable, and interpretable alternative for scalable evaluation.

</details>


### [343] [Language Model Circuits Are Sparse in the Neuron Basis](https://arxiv.org/abs/2601.22594)
*Aryaman Arora,Zhengxuan Wu,Jacob Steinhardt,Sarah Schwettmann*

Main category: cs.CL

TL;DR: 研究表明MLP神经元与稀疏自编码器一样具有稀疏特征基，开发基于MLP神经元的电路追踪端到端管道，推动语言模型自动可解释性且无需额外训练成本。


<details>
  <summary>Details</summary>
Motivation: 语言模型可解释性研究希望将神经元基础分解为更易解释的计算单元，且并非所有基于神经元的表示都不可解释。

Method: 实证证明MLP神经元与SAEs一样具有稀疏特征基，开发基于MLP神经元基础的端到端电路追踪管道，使用基于梯度的归因定位因果电路。

Result: 在标准主谓一致基准和多跳任务中，发现少量MLP神经元电路可控制模型行为，部分神经元集合编码特定潜在推理步骤并可改变模型输出。

Conclusion: 该工作在无额外训练成本的情况下推动了语言模型的自动可解释性。

Abstract: The high-level concepts that a neural network uses to perform computation need not be aligned to individual neurons (Smolensky, 1986). Language model interpretability research has thus turned to techniques such as \textit{sparse autoencoders} (SAEs) to decompose the neuron basis into more interpretable units of model computation, for tasks such as \textit{circuit tracing}. However, not all neuron-based representations are uninterpretable. For the first time, we empirically show that \textbf{MLP neurons are as sparse a feature basis as SAEs}. We use this finding to develop an end-to-end pipeline for circuit tracing on the MLP neuron basis, which locates causal circuitry on a variety of tasks using gradient-based attribution. On a standard subject-verb agreement benchmark (Marks et al., 2025), a circuit of $\approx 10^2$ MLP neurons is enough to control model behaviour. On the multi-hop city $\to$ state $\to$ capital task from Lindsey et al., 2025, we find a circuit in which small sets of neurons encode specific latent reasoning steps (e.g.~`map city to its state'), and can be steered to change the model's output. This work thus advances automated interpretability of language models without additional training costs.

</details>


### [344] [Time-Annealed Perturbation Sampling: Diverse Generation for Diffusion Language Models](https://arxiv.org/abs/2601.22629)
*Jingxuan Wu,Zhenglin Wan,Xingrui Yu,Yuzhe Yang,Yiqiao Huang,Ivor Tsang,Yang You*

Main category: cs.CL

TL;DR: 本文研究扩散语言模型生成多样性控制，提出无训练推理策略TAPS，提升输出多样性且不降低生成质量。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用扩散语言模型的时间结构控制生成多样性，以探索多种有效语义或推理路径。

Method: 提出时间退火扰动采样（TAPS），一种无训练推理策略，在扩散过程早期鼓励语义分支，逐步减少扰动以保持流畅性和指令遵循性。

Result: TAPS与非自回归和半自回归扩散骨干兼容，在创意写作和推理基准测试中持续提升输出多样性且不降低生成质量。

Conclusion: TAPS能有效提升扩散语言模型的输出多样性，且不影响生成质量。

Abstract: Diffusion language models (Diffusion-LMs) introduce an explicit temporal dimension into text generation, yet how this structure can be leveraged to control generation diversity for exploring multiple valid semantic or reasoning paths remains underexplored. In this paper, we show that Diffusion-LMs, like diffusion models in image generation, exhibit a temporal division of labor: early denoising steps largely determine the global semantic structure, while later steps focus on local lexical refinement. Building on this insight, we propose Time-Annealed Perturbation Sampling (TAPS), a training-free inference strategy that encourages semantic branching early in the diffusion process while progressively reducing perturbations to preserve fluency and instruction adherence. TAPS is compatible with both non-autoregressive and semi-autoregressive Diffusion backbones, demonstrated on LLaDA and TraDo in our paper, and consistently improves output diversity across creative writing and reasoning benchmarks without compromising generation quality.

</details>


### [345] [NAG: A Unified Native Architecture for Encoder-free Text-Graph Modeling in Language Models](https://arxiv.org/abs/2601.22657)
*Haisong Gong,Zhibo Liu,Qiang Liu,Shu Wu,Liang Wang*

Main category: cs.CL

TL;DR: 提出统一框架NAG将图处理融入语言模型原生架构，有两种实现方式，实验证明其在图任务上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有将图集成到语言模型的方法采用分离架构，概念上不连贯且需复杂隐式对齐，存在不足。

Method: 提出NAG统一框架，利用自注意力机制强化拓扑依赖，重新校准位置ID；引入NAG - Zero和NAG - LoRA两种实现方式。

Result: 实验显示NAG在不同图任务中无需外部编码器开销，能实现强大的图理解能力。

Conclusion: NAG为文本 - 图建模提供了更简单、更连贯的范式。

Abstract: Prevailing methods for integrating graphs into Language Models (LMs) typically rely on a segregated architecture: external Graph Neural Networks (GNNs) encode structural topology, while LMs process textual semantics. We argue this approach is suboptimal for text-graphs: it creates a conceptually disjointed interaction paradigm. By segregating structural encoding from semantic processing, these systems must perform a complex implicit alignment between abstract graph tokens and concrete textual elements. Challenging the necessity of external encoders, we propose NAG (Native Architecture for Graphs), a unified framework that internalizes graph processing within the LM's native manifold. Instead of bridging disparate embedding spaces, NAG repurposes the self-attention mechanism to enforce topological dependencies and recalibrates positional IDs to ensure structural equivalence. This allows the model to harness its intrinsic linguistic capability to simultaneously comprehend node and edge content alongside structural topology. We introduce two efficient implementations: NAG-Zero for absolute preservation of the base model's linguistic capabilities, and NAG-LoRA for enhanced structural adaptation. Experiments across diverse graph tasks validate that NAG achieves robust graph comprehension without the overhead of external encoders, offering a simpler, more coherent paradigm for text-graph modeling.

</details>


### [346] [FNF: Functional Network Fingerprint for Large Language Models](https://arxiv.org/abs/2601.22692)
*Yiheng Liu,Junhao Ning,Sichen Xia,Haiyang Sun,Yang Yang,Hanyang Chi,Xiaohui Gao,Ning Qiang,Bao Ge,Junwei Han,Xintao Hu*

Main category: cs.CL

TL;DR: 提出训练无关、样本高效的功能网络指纹法（FNF）检测LLM知识产权侵权，代码开源。


<details>
  <summary>Details</summary>
Motivation: 大语言模型开发成本高、商业价值大，需防止开源模型被非法挪用，保护开发者知识产权。

Method: 基于功能网络活动一致性，提出FNF方法检测可疑模型是否源自受害模型。

Result: 有共同起源的模型功能网络神经元活动模式高度一致，独立训练的模型无此特征；方法只需少量样本，对常见模型修改和跨架构维度比较有鲁棒性。

Conclusion: FNF为模型所有者和第三方提供简单、非侵入性且有效的LLM知识产权保护工具。

Abstract: The development of large language models (LLMs) is costly and has significant commercial value. Consequently, preventing unauthorized appropriation of open-source LLMs and protecting developers' intellectual property rights have become critical challenges. In this work, we propose the Functional Network Fingerprint (FNF), a training-free, sample-efficient method for detecting whether a suspect LLM is derived from a victim model, based on the consistency between their functional network activity. We demonstrate that models that share a common origin, even with differences in scale or architecture, exhibit highly consistent patterns of neuronal activity within their functional networks across diverse input samples. In contrast, models trained independently on distinct data or with different objectives fail to preserve such activity alignment. Unlike conventional approaches, our method requires only a few samples for verification, preserves model utility, and remains robust to common model modifications (such as fine-tuning, pruning, and parameter permutation), as well as to comparisons across diverse architectures and dimensionalities. FNF thus provides model owners and third parties with a simple, non-invasive, and effective tool for protecting LLM intellectual property. The code is available at https://github.com/WhatAboutMyStar/LLM_ACTIVATION.

</details>


### [347] [Should LLMs, $\textit{like}$, Generate How Users Talk? Building Dialect-Accurate Dialog[ue]s Beyond the American Default with MDial](https://arxiv.org/abs/2601.22888)
*Jio Oh,Paul Vicinanza,Thomas Butler,Steven Euijong Whang,Dezhi Hong,Amani Namboori*

Main category: cs.CL

TL;DR: 本文提出MDial框架生成多方言对话数据，构建MDialBench进行评估，发现大语言模型在方言处理上存在问题。


<details>
  <summary>Details</summary>
Motivation: 超80%英语使用者不用标准美式英语，与大语言模型交互有困难且多方言性能研究不足。

Method: 引入MDial框架，结合语言学家设计基于规则的大语言模型转换，构建MDialBench评估17个大语言模型。

Result: 数据质量高，标注者更偏好MDial输出；大语言模型方言识别准确率低，会误分类非标准美式英语方言。

Conclusion: 方言识别影响自然语言理解，现有模型错误会导致下游任务失败。

Abstract: More than 80% of the 1.6 billion English speakers do not use Standard American English (SAE) and experience higher failure rates and stereotyped responses when interacting with LLMs as a result. Yet multi-dialectal performance remains underexplored. We introduce $\textbf{MDial}$, the first large-scale framework for generating multi-dialectal conversational data encompassing the three pillars of written dialect -- lexical (vocabulary), orthographic (spelling), and morphosyntactic (grammar) features -- for nine English dialects. Partnering with native linguists, we design an annotated and scalable rule-based LLM transformation to ensure precision. Our approach challenges the assumption that models should mirror users' morphosyntactic features, showing that up to 90% of the grammatical features of a dialect should not be reproduced by models. Independent evaluations confirm data quality, with annotators preferring MDial outputs over prior methods in 98% of pairwise comparisons for dialect naturalness. Using this pipeline, we construct the dialect-parallel $\textbf{MDialBench}$mark with 50k+ dialogs, resulting in 97k+ QA pairs, and evaluate 17 LLMs on dialect identification and response generation tasks. Even frontier models achieve under 70% accuracy, fail to reach 50% for Canadian English, and systematically misclassify non-SAE dialects as American or British. As dialect identification underpins natural language understanding, these errors risk cascading failures into downstream tasks.

</details>


### [348] [DiffuSpeech: Silent Thought, Spoken Answer via Unified Speech-Text Diffusion](https://arxiv.org/abs/2601.22889)
*Yuxuan Lou,Ziming Wu,Yaochen Wang,Yong Liu,Yingxuan Ren,Fuming Lai,Shaobing Lian,Jie Tang,Yang You*

Main category: cs.CL

TL;DR: 提出“Silent Thought, Spoken Answer”范式及DiffuSpeech模型，构建首个带推理痕迹的语音QA数据集，实验表明该模型在语音问答、TTS质量和语言理解方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前语音语言模型直接生成回复，产生错误后无法纠正，引入能生成内部文本推理的范式。

Method: 提出DiffuSpeech模型，将离散文本和语音统一在一个掩码扩散框架下，通过迭代去噪联合生成推理痕迹和语音令牌；构建SpeechReason数据集。

Result: DiffuSpeech模型在语音问答准确率上达到SOTA，优于基线最多9个点，在生成模型中TTS质量最佳（WER 6.2%），并保留语言理解能力（MMLU 66.2%）。

Conclusion: 扩散架构和思考痕迹都对模型的提升有贡献。

Abstract: Current speech language models generate responses directly without explicit reasoning, leading to errors that cannot be corrected once audio is produced. We introduce \textbf{``Silent Thought, Spoken Answer''} -- a paradigm where speech LLMs generate internal text reasoning alongside spoken responses, with thinking traces informing speech quality. To realize this, we present \method{}, the first diffusion-based speech-text language model supporting both understanding and generation, unifying discrete text and tokenized speech under a single masked diffusion framework. Unlike autoregressive approaches, \method{} jointly generates reasoning traces and speech tokens through iterative denoising, with modality-specific masking schedules. We also construct \dataset{}, the first speech QA dataset with paired text reasoning traces, containing 26K samples totaling 319 hours. Experiments show \method{} achieves state-of-the-art speech-to-speech QA accuracy, outperforming the best baseline by up to 9 points, while attaining the best TTS quality among generative models (6.2\% WER) and preserving language understanding (66.2\% MMLU). Ablations confirm that both the diffusion architecture and thinking traces contribute to these gains.

</details>


### [349] [Residual Context Diffusion Language Models](https://arxiv.org/abs/2601.22954)
*Yuezhou Hu,Harman Singh,Monishwaran Maheswaran,Haocheng Xi,Coleman Hooper,Jintao Zhang,Aditya Tomar,Michael W. Mahoney,Sewon Min,Mehrdad Farajtabar,Kurt Keutzer,Amir Gholami,Chenfeng Xu*

Main category: cs.CL

TL;DR: 提出RCD模块提升Diffusion Large Language Models效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有块级Diffusion Large Language Models的“remasking”机制浪费计算资源，需利用丢弃标记的计算。

Method: 提出RCD模块，将丢弃标记表示转换为上下文残差并注入下一步骤，采用解耦两阶段训练流程。

Result: 能高效将标准dLLM转换为RCD范式，在多基准测试中提升准确率，减少AIME任务中去噪步骤。

Conclusion: RCD能提高dLLMs的准确性，计算开销小，有效利用丢弃标记的计算。

Abstract: Diffusion Large Language Models (dLLMs) have emerged as a promising alternative to purely autoregressive language models because they can decode multiple tokens in parallel. However, state-of-the-art block-wise dLLMs rely on a "remasking" mechanism that decodes only the most confident tokens and discards the rest, effectively wasting computation. We demonstrate that recycling computation from the discarded tokens is beneficial, as these tokens retain contextual information useful for subsequent decoding iterations. In light of this, we propose Residual Context Diffusion (RCD), a module that converts these discarded token representations into contextual residuals and injects them back for the next denoising step. RCD uses a decoupled two-stage training pipeline to bypass the memory bottlenecks associated with backpropagation. We validate our method on both long CoT reasoning (SDAR) and short CoT instruction following (LLaDA) models. We demonstrate that a standard dLLM can be efficiently converted to the RCD paradigm with merely ~1 billion tokens. RCD consistently improves frontier dLLMs by 5-10 points in accuracy with minimal extra computation overhead across a wide range of benchmarks. Notably, on the most challenging AIME tasks, RCD nearly doubles baseline accuracy and attains up to 4-5x fewer denoising steps at equivalent accuracy levels.

</details>


### [350] [Bias Beyond Borders: Political Ideology Evaluation and Steering in Multilingual LLMs](https://arxiv.org/abs/2601.23001)
*Afrozah Nadeem,Agrima,Mehwish Nasim,Usman Naseem*

Main category: cs.CL

TL;DR: 文章对50个国家33种语言的大语言模型政治偏见进行大规模多语言评估，提出后处理缓解框架CLAS，实验显示能大幅减少偏见且保证响应质量，为多语言大语言模型治理提供可扩展、可解释范式。


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦高资源西方语言或狭隘多语言场景，缺乏跨语言一致性和安全后处理缓解方法，为解决此问题开展研究。

Method: 进行大规模多语言评估，提出Cross - Lingual Alignment Steering (CLAS) 后处理缓解框架，将政治提示诱导的潜在意识形态表征对齐到共享意识形态子空间，通过自适应机制防止过度校正。

Result: 实验表明在经济和社会轴上大幅减少偏见，且对响应质量的降低最小。

Conclusion: 所提出框架为公平感知的多语言大语言模型治理建立了可扩展和可解释的范式，平衡了意识形态中立与语言文化多样性。

Abstract: Large Language Models (LLMs) increasingly shape global discourse, making fairness and ideological neutrality essential for responsible AI deployment. Despite growing attention to political bias in LLMs, prior work largely focuses on high-resource, Western languages or narrow multilingual settings, leaving cross-lingual consistency and safe post-hoc mitigation underexplored. To address this gap, we present a large-scale multilingual evaluation of political bias spanning 50 countries and 33 languages. We introduce a complementary post-hoc mitigation framework, Cross-Lingual Alignment Steering (CLAS), designed to augment existing steering methods by aligning ideological representations across languages and dynamically regulating intervention strength. This method aligns latent ideological representations induced by political prompts into a shared ideological subspace, ensuring cross lingual consistency, with the adaptive mechanism prevents over correction and preserves coherence. Experiments demonstrate substantial bias reduction along both economic and social axes with minimal degradation in response quality. The proposed framework establishes a scalable and interpretable paradigm for fairness-aware multilingual LLM governance, balancing ideological neutrality with linguistic and cultural diversity.

</details>


### [351] [Character as a Latent Variable in Large Language Models: A Mechanistic Account of Emergent Misalignment and Conditional Safety Failures](https://arxiv.org/abs/2601.23081)
*Yanghao Su,Wenbo Zhou,Tianwei Zhang,Qiu Han,Weiming Zhang,Nenghai Yu,Jie Zhang*

Main category: cs.CL

TL;DR: 研究发现微调大语言模型时特定字符级倾向数据会导致更强、更具迁移性的涌现式失准，指出字符形成是未充分探索的对齐风险。


<details>
  <summary>Details</summary>
Motivation: 现有对涌现式失准现象的解释不完整，需深入研究其成因。

Method: 在多个领域和模型族中，对比特定字符级倾向数据微调与错误建议微调的效果。

Result: 特定字符级倾向数据微调导致更强、更具迁移性的失准，且能保留一般能力；行为倾向可被训练和推理时的提示激活。

Conclusion: 字符形成是核心且未充分探索的对齐风险，鲁棒对齐需解决行为倾向问题。

Abstract: Emergent Misalignment refers to a failure mode in which fine-tuning large language models (LLMs) on narrowly scoped data induces broadly misaligned behavior. Prior explanations mainly attribute this phenomenon to the generalization of erroneous or unsafe content. In this work, we show that this view is incomplete. Across multiple domains and model families, we find that fine-tuning models on data exhibiting specific character-level dispositions induces substantially stronger and more transferable misalignment than incorrect-advice fine-tuning, while largely preserving general capabilities. This indicates that emergent misalignment arises from stable shifts in model behavior rather than from capability degradation or corrupted knowledge. We further show that such behavioral dispositions can be conditionally activated by both training-time triggers and inference-time persona-aligned prompts, revealing shared structure across emergent misalignment, backdoor activation, and jailbreak susceptibility. Overall, our results identify character formation as a central and underexplored alignment risk, suggesting that robust alignment must address behavioral dispositions rather than isolated errors or prompt-level defenses.

</details>


### [352] [Bifocal Attention: Harmonizing Geometric and Spectral Positional Embeddings for Algorithmic Generalization](https://arxiv.org/abs/2601.22402)
*Kanishk Awadhiya*

Main category: cs.CL

TL;DR: 指出标准RoPE存在‘频谱刚性’问题，导致‘结构间隙’，提出双焦点注意力架构和频谱进化训练协议。


<details>
  <summary>Details</summary>
Motivation: 标准RoPE存在‘频谱刚性’，无法捕捉长距离周期性结构，造成模型推理能力局限，需要改进。

Method: 引入双焦点注意力架构，将位置编码解耦为几何视角（标准RoPE）和频谱视角（可学习谐波算子）；提出频谱进化训练协议，让位置频率从静态几何参数通过梯度下降进化为特定任务优化的谐波基。

Result: 未提及明确结果。

Conclusion: 未提及明确结论。

Abstract: Rotary Positional Embeddings (RoPE) have become the standard for Large Language Models (LLMs) due to their ability to encode relative positions through geometric rotation. However, we identify a significant limitation we term ''Spectral Rigidity'': standard RoPE utilizes a fixed geometric decay ($θ^{-i}$) optimized for local syntactic coherence, which fails to capture the long-range, periodic structures inherent in recursive logic and algorithmic reasoning. This results in a ''Structure Gap'', where models trained on shallow reasoning chains fail to extrapolate to deeper recursive steps. In this work, we introduce Bifocal Attention, an architectural paradigm that decouples positional encoding into two distinct modalities: Geometric Eyes (Standard RoPE) for precise token-level manipulation, and Spectral Eyes (Learnable Harmonic Operators) for tracking long-range recursive depth. We propose a novel training protocol, Spectral Evolution, which initializes positional frequencies as static geometric parameters but allows them to evolve via gradient descent into a harmonic basis optimized for the specific algorithmic topology of the task.

</details>


### [353] [Now You Hear Me: Audio Narrative Attacks Against Large Audio-Language Models](https://arxiv.org/abs/2601.23255)
*Ye Yu,Haibo Jin,Yaoning Yu,Jun Zhuang,Haohan Wang*

Main category: cs.CL

TL;DR: 研究语音输入的大语言模型安全漏洞，设计文本转语音越狱攻击，达98.26%成功率，强调需联合考量语言和副语言表征的安全框架。


<details>
  <summary>Details</summary>
Motivation: 随着大音频语言模型转向原生语音输入，出现新的未被充分研究的安全漏洞，需对这种模式转变的安全影响进行研究。

Method: 设计文本到音频的越狱攻击，利用先进的文本转语音模型，嵌入禁止指令绕过主要针对文本校准的安全机制。

Result: 通过合成语音以叙事格式触发先进模型的受限输出，成功率达98.26%，远超纯文本基线。

Conclusion: 随着基于语音的界面日益普及，需要能联合推理语言和副语言表征的安全框架。

Abstract: Large audio-language models increasingly operate on raw speech inputs, enabling more seamless integration across domains such as voice assistants, education, and clinical triage. This transition, however, introduces a distinct class of vulnerabilities that remain largely uncharacterized. We examine the security implications of this modality shift by designing a text-to-audio jailbreak that embeds disallowed directives within a narrative-style audio stream. The attack leverages an advanced instruction-following text-to-speech (TTS) model to exploit structural and acoustic properties, thereby circumventing safety mechanisms primarily calibrated for text. When delivered through synthetic speech, the narrative format elicits restricted outputs from state-of-the-art models, including Gemini 2.0 Flash, achieving a 98.26% success rate that substantially exceeds text-only baselines. These results highlight the need for safety frameworks that jointly reason over linguistic and paralinguistic representations, particularly as speech-based interfaces become more prevalent.

</details>


### [354] [DART-ing Through the Drift: Dynamic Tracing of Knowledge Neurons for Adaptive Inference-Time Pruning](https://arxiv.org/abs/2601.22632)
*Abhishek Tyagi,Yunuo Cen,Shrey Dhorajiya,Bharadwaj Veeravalli,Xuanyao Fong*

Main category: cs.CL

TL;DR: 介绍轻量级无训练方法 DART 用于实时基于上下文剪枝，在多个基准测试中表现出色，能适应语义上下文，性能与原密集模型相当。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLMs）剪枝方法依赖特定数据集校准有数据依赖和计算开销问题，且静态剪枝无法适应自回归生成中知识神经元子集变化。

Method: 引入 DART 方法，通过监测注意力分数分布变化推断上下文改变，动态更新神经元级掩码保留显著参数。

Result: 在十个基准测试中，DART 优于现有动态基准，在 LLAMA - 3.1 - 8B 上 70% FFN 稀疏度时准确率提升达 14.5%，在摘要任务中 ROUGE - L 分数比静态掩码剪枝高 3 倍，性能与原密集模型相当。

Conclusion: DART 框架能有效适应不同语义上下文，保留模型在通用和特定领域任务中的能力，内存占用少且 FLOPs 开销低。

Abstract: Large Language Models (LLMs) exhibit substantial parameter redundancy, particularly in Feed-Forward Networks (FFNs). Existing pruning methods suffer from two primary limitations. First, reliance on dataset-specific calibration introduces significant data dependency and computational overhead. Second, being predominantly static, they fail to account for the evolving subset of knowledge neurons in LLMs during autoregressive generation as the context evolves. To address this, we introduce DART, i.e., Dynamic Attention-Guided Runtime Tracing), a lightweight, training-free method that performs on-the-fly context-based pruning. DART monitors shifts in attention score distributions to infer context changes, dynamically updating neuron-level masks to retain salient parameters. Across ten benchmarks, DART outperforms prior dynamic baseline, achieving accuracy gains of up to 14.5% on LLAMA-3.1-8B at 70% FFN sparsity. Furthermore, DART achieves up to 3x better ROUGE-L scores with respect to static-masked pruning on summarization tasks, with its performance comparable to the original dense models. We conclusively demonstrate that the proposed framework effectively adapts to diverse semantic contexts, preserves model capabilities across both general and domain-specific tasks while running at less than 10MBs of memory for LLAMA-3.1-8B(16GBs) with 0.1% FLOPs overhead. The code is available at https://github.com/seeder-research/DART.

</details>


### [355] [LLMs Explain't: A Post-Mortem on Semantic Interpretability in Transformer Models](https://arxiv.org/abs/2601.22928)
*Alhassan Abdelhalim,Janick Edinger,Sören Laue,Michaela Regneri*

Main category: cs.CL

TL;DR: 探讨大语言模型（LLMs）中语言抽象如何出现，所用方法均失败，指出常用解释技术结论无依据。


<details>
  <summary>Details</summary>
Motivation: LLMs性能出色但机制不明，解释方法本身也未完全理解，想探究LLMs中语言抽象如何出现。

Method: 采用文献中成熟方法，一是探测标记级关系结构，二是使用嵌入作为人类可解释属性载体进行特征映射。

Result: 两种方法均因不同方法学原因失败，注意力解释核心假设不成立，嵌入属性推断受方法伪影和数据集结构影响。

Conclusion: 常用解释技术被当作LLMs理解能力证据的结论无依据，在普适和分布式计算场景中其局限性需重视。

Abstract: Large Language Models (LLMs) are becoming increasingly popular in pervasive computing due to their versatility and strong performance. However, despite their ubiquitous use, the exact mechanisms underlying their outstanding performance remain unclear. Different methods for LLM explainability exist, and many are, as a method, not fully understood themselves. We started with the question of how linguistic abstraction emerges in LLMs, aiming to detect it across different LLM modules (attention heads and input embeddings). For this, we used methods well-established in the literature: (1) probing for token-level relational structures, and (2) feature-mapping using embeddings as carriers of human-interpretable properties.
  Both attempts failed for different methodological reasons: Attention-based explanations collapsed once we tested the core assumption that later-layer representations still correspond to tokens. Property-inference methods applied to embeddings also failed because their high predictive scores were driven by methodological artifacts and dataset structure rather than meaningful semantic knowledge. These failures matter because both techniques are widely treated as evidence for what LLMs supposedly understand, yet our results show such conclusions are unwarranted. These limitations are particularly relevant in pervasive and distributed computing settings where LLMs are deployed as system components and interpretability methods are relied upon for debugging, compression, and explaining models.

</details>


### [356] [Relaxing Positional Alignment in Masked Diffusion Language Models](https://arxiv.org/abs/2601.22947)
*Mengyu Ye,Ryosuke Takahashi,Keito Kudo,Jun Suzuki*

Main category: cs.CL

TL;DR: 本文指出MDLM在开放式文本生成上存在差距，原因可能是严格位置预测敏感，通过引入特殊令牌提出灵活对齐监督策略，实验表明该策略改善生成质量。


<details>
  <summary>Details</summary>
Motivation: MDLM在开放式文本生成上与自回归方法有差距，严格位置监督与解码动态不匹配。

Method: 在微调时采用对齐灵活的监督策略，通过连接主义时间分类目标引入特殊令牌<slack>。

Result: 应用该方法在五个开放式文本生成基准上始终优于原模型，提高了对位置偏移的鲁棒性。

Conclusion: 放宽严格位置监督是提高MDLM生成质量的重要因素。

Abstract: Masked diffusion language models (MDLMs) have emerged as a promising alternative to dominant autoregressive approaches. Although they achieve competitive performance on several tasks, a substantial gap remains in open-ended text generation. We hypothesize that one cause of this gap is that strict positional prediction makes MDLM decoding highly sensitive to token misalignment, and we show through controlled interventions that a one-position shift can severely disrupt semantics. This observation suggests that enforcing strict positional supervision during training is misaligned with the irreversible denoising dynamics of MDLM decoding. Motivated by this mismatch, we adopt an alignment-flexible supervision strategy during fine-tuning. Specifically, we introduce a special token <slack> via the connectionist temporal classification objective. We apply this approach to the widely used MDLM model and conduct experiments on five open-ended text generation benchmarks. Our method consistently outperforms the original model and improves robustness to positional shifts, indicating that relaxing strict positional supervision is an important factor in improving generation quality in MDLMs.

</details>


### [357] [Safer Policy Compliance with Dynamic Epistemic Fallback](https://arxiv.org/abs/2601.23094)
*Joseph Marvin Imperial,Harish Tayyar Madabushi*

Main category: cs.CL

TL;DR: 本文介绍动态认知回退（DEF）协议，以提升大语言模型对欺骗攻击的推理时防御能力，实验表明其有效，鼓励开发受认知启发的防御机制。


<details>
  <summary>Details</summary>
Motivation: 受人类认知防御机制启发，为大语言模型开发防护措施，助力其在高风险任务中的应用。

Method: 引入动态认知回退（DEF）协议，通过单句文本提示，让大语言模型在遇到恶意篡改的政策文本时标记不一致、拒绝执行并回退到参数知识。

Result: 使用HIPAA和GDPR等全球公认法律政策进行评估，DEF有效提升前沿大语言模型检测和拒绝篡改政策的能力，DeepSeek - R1在某一设置下检测率达100%。

Conclusion: 鼓励进一步开发受认知启发的防御机制，提高大语言模型对利用法律文件的伤害和欺骗形式的鲁棒性。

Abstract: Humans develop a series of cognitive defenses, known as epistemic vigilance, to combat risks of deception and misinformation from everyday interactions. Developing safeguards for LLMs inspired by this mechanism might be particularly helpful for their application in high-stakes tasks such as automating compliance with data privacy laws. In this paper, we introduce Dynamic Epistemic Fallback (DEF), a dynamic safety protocol for improving an LLM's inference-time defenses against deceptive attacks that make use of maliciously perturbed policy texts. Through various levels of one-sentence textual cues, DEF nudges LLMs to flag inconsistencies, refuse compliance, and fallback to their parametric knowledge upon encountering perturbed policy texts. Using globally recognized legal policies such as HIPAA and GDPR, our empirical evaluations report that DEF effectively improves the capability of frontier LLMs to detect and refuse perturbed versions of policies, with DeepSeek-R1 achieving a 100% detection rate in one setting. This work encourages further efforts to develop cognitively inspired defenses to improve LLM robustness against forms of harm and deception that exploit legal artifacts.

</details>


<div id='q-fin.GN'></div>

# q-fin.GN [[Back]](#toc)

### [358] [A Real-Options-Aware Multi-Criteria Framework for Ex-Ante Real Estate Redevelopment Use Selection](https://arxiv.org/abs/2601.22166)
*Roberto Garrone*

Main category: q-fin.GN

TL;DR: 现有部分房地产长期表现不佳，研究提出房地产再开发用途选择的决策分析框架，结合实物期权逻辑和多准则决策分析，案例显示可减少复杂性和错配。


<details>
  <summary>Details</summary>
Motivation: 现有部分房地产长期表现不佳，源于预期用途与有效需求结构性不一致，需要能综合价值、风险等因素的决策框架。

Method: 提出决策分析框架，将实物期权逻辑与多准则决策分析结构相结合。

Result: 通过案例说明该框架能减少不同资产类型和城市环境中的过度复杂化和错配。

Conclusion: 该决策分析框架可通过事前筛选实现期权价值保存，适用于房地产再开发用途选择。

Abstract: A growing share of the existing real estate stock exhibits persistent underperformance that can no longer be explained by cyclical market phases or inadequate maintenance alone. In many cases, technically recoverable assets located in non-marginal contexts fail to generate economic value consistent with the capital immobilized. This condition reflects a structural misalignment between intended use and effective demand rather than episodic market weakness, and calls for a decision framework capable of integrating value, risk, complexity, and irreversibility in strategic use selection. This study proposes a decision-analytic framework for the ex-ante selection of intended use in real estate redevelopment processes. The framework integrates real-options logic on irreversibility and managerial flexibility with a multi-criteria decision-analysis structure, enabling comparative evaluation of expected economic value, market and operational risk, technical and managerial complexity, and time-to-income. By treating redevelopment primarily as a problem of strategic option selection rather than design or financial optimization, the framework operationalizes option value preservation through disciplined ex-ante screening. Illustrative cases demonstrate how this integration of real options reasoning and MCDA reduces over-complexification and misalignment across different asset types and urban contexts.

</details>


### [359] [The Widening Profitability Gap between Renewable and Fossil Power Firms in Europe](https://arxiv.org/abs/2601.22167)
*Robin Fischer,Anton Pichler*

Main category: q-fin.GN

TL;DR: 分析欧洲电力企业数据发现风光组合盈利能力上升，化石能源组合盈利能力下降，化石企业高利润是异常现象。


<details>
  <summary>Details</summary>
Motivation: 解决私人资本调动是能源转型瓶颈，且目前市场信号可能仍偏向碳密集资产的问题，确定化石电力公司的利润是持久优势还是危机驱动的异常现象。

Method: 分析2001 - 2023年900家欧洲电力公司的面板数据，运用机器学习聚类和贝叶斯模型平均法。

Result: 风光组合盈利能力上升，2014 - 2023年风电主导企业资产回报率提高超6%，化石能源组合份额越高盈利能力越低，到2023年边际效应达 - 4%，可再生能源主导企业在多数欧洲地区表现相当或超过化石能源主导企业。

Conclusion: 化石能源企业的创纪录利润是异常值，掩盖了碳密集型商业模式盈利能力的持续下降。

Abstract: Mobilising private capital is a critical bottleneck of the energy transition, yet recent crisis-driven windfall profits for fossil power firms suggest that market signals may still favour carbon-intensive assets. Here we analyse a panel of 900 European power firms (2001-2023) to resolve whether these profits reflect a durable profitability advantage or a crisis-driven anomaly. Using machine-learning clustering and Bayesian model averaging, we identify a structural divergence: wind and solar portfolios exhibit rising profitability, with return on assets among wind-dominated firms increasing by over 6% between 2014 and 2023. Conversely, higher fossil portfolio shares are increasingly associated with lower profitability, with marginal effects reaching -4% by 2023, while renewable-dominated firms match or outperform their fossil-heavy counterparts across most European regions. These findings suggest that the record profits of fossil incumbents were distinct outliers, masking an ongoing decline in the profitability of carbon-intensive business models.

</details>


### [360] [UniFinEval: Towards Unified Evaluation of Financial Multimodal Models across Text, Images and Videos](https://arxiv.org/abs/2601.22162)
*Zhi Yang,Lingfeng Zeng,Fangqi Lou,Qi Qi,Wei Zhang,Zhenyu Wu,Zhenxiong Yu,Jun Han,Zhiheng Jin,Lejie Zhang,Xiaoming Huang,Xiaolong Liang,Zheng Wei,Junbo Zou,Dongpo Cheng,Zhaowei Liu,Xin Guo,Rongjunchen Zhang,Liwen Zhang*

Main category: q-fin.GN

TL;DR: 提出首个统一多模态基准UniFinEval用于高密度金融环境，评估10个主流多模态大语言模型，发现模型有不足。


<details>
  <summary>Details</summary>
Motivation: 现有多模态基准无法评估多模态大语言模型在金融领域面临的多模态、高密信息和跨模态多跳推理挑战。

Method: 构建涵盖文本、图像和视频的UniFinEval，基于真实金融系统构建五个核心金融场景，手动构建中英文问答对数据集，在零样本和思维链设置下评估模型。

Result: Gemini - 3 - pro - preview整体表现最佳，但与金融专家仍有差距，当前模型存在系统性缺陷。

Conclusion: UniFinEval可系统评估多模态大语言模型在细粒度、高密金融环境的能力，提升其在真实金融场景应用的鲁棒性。

Abstract: Multimodal large language models are playing an increasingly significant role in empowering the financial domain, however, the challenges they face, such as multimodal and high-density information and cross-modal multi-hop reasoning, go beyond the evaluation scope of existing multimodal benchmarks. To address this gap, we propose UniFinEval, the first unified multimodal benchmark designed for high-information-density financial environments, covering text, images, and videos. UniFinEval systematically constructs five core financial scenarios grounded in real-world financial systems: Financial Statement Auditing, Company Fundamental Reasoning, Industry Trend Insights, Financial Risk Sensing, and Asset Allocation Analysis. We manually construct a high-quality dataset consisting of 3,767 question-answer pairs in both chinese and english and systematically evaluate 10 mainstream MLLMs under Zero-Shot and CoT settings. Results show that Gemini-3-pro-preview achieves the best overall performance, yet still exhibits a substantial gap compared to financial experts. Further error analysis reveals systematic deficiencies in current models. UniFinEval aims to provide a systematic assessment of MLLMs' capabilities in fine-grained, high-information-density financial environments, thereby enhancing the robustness of MLLMs applications in real-world financial scenarios. Data and code are available at https://github.com/aifinlab/UniFinEval.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [361] [Denoising the Deep Sky: Physics-Based CCD Noise Formation for Astronomical Imaging](https://arxiv.org/abs/2601.23276)
*Shuhong Liu,Xining Ge,Ziying Gu,Lin Gu,Ziteng Cui,Xuangeng Chu,Jun Liu,Dong Li,Tatsuya Harada*

Main category: astro-ph.IM

TL;DR: 提出基于物理的噪声合成框架解决天文成像去噪训练数据稀缺问题，并引入真实世界数据集用于评估。


<details>
  <summary>Details</summary>
Motivation: 天文成像受噪声限制，标准校准流程无法解决随机噪声，基于学习的去噪方法因训练数据稀缺和模型缺乏物理解释性而进展受阻。

Method: 提出基于CCD噪声形成的物理噪声合成框架，对多种噪声建模；通过平均多个未配准曝光生成高信噪比基础数据；引入跨多波段的真实世界数据集。

Result: 利用噪声模型从高信噪比基础数据合成逼真噪声对应数据，构建了丰富的配对数据集用于监督学习。

Conclusion: 所提出的方法和引入的数据集有助于推动天文成像基于学习的去噪研究。

Abstract: Astronomical imaging remains noise-limited under practical observing constraints, while standard calibration pipelines mainly remove structured artifacts and leave stochastic noise largely unresolved. Learning-based denoising is promising, yet progress is hindered by scarce paired training data and the need for physically interpretable and reproducible models in scientific workflows. We propose a physics-based noise synthesis framework tailored to CCD noise formation. The pipeline models photon shot noise, photo-response non-uniformity, dark-current noise, readout effects, and localized outliers arising from cosmic-ray hits and hot pixels. To obtain low-noise inputs for synthesis, we average multiple unregistered exposures to produce high-SNR bases. Realistic noisy counterparts synthesized from these bases using our noise model enable the construction of abundant paired datasets for supervised learning. We further introduce a real-world dataset across multi-bands acquired with two twin ground-based telescopes, providing paired raw frames and instrument-pipeline calibrated frames, together with calibration data and stacked high-SNR bases for real-world evaluation.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [362] [Computing Dominating Sets in Disk Graphs with Centers in Convex Position](https://arxiv.org/abs/2601.22609)
*Anastasiia Tkachenko,Haitao Wang*

Main category: cs.CG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Given a set $P$ of $n$ points in the plane and a collection of disks centered at these points, the disk graph $G(P)$ has vertex set $P$, with an edge between two vertices if their corresponding disks intersect. We study the dominating set problem in $G(P)$ under the special case where the points of $P$ are in convex position. The problem is NP-hard in general disk graphs. Under the convex position assumption, however, we present the first polynomial-time algorithm for the problem. Specifically, we design an $O(k^2 n \log^2 n)$-time algorithm, where $k$ denotes the size of a minimum dominating set. For the weighted version, in which each disk has an associated weight and the goal is to compute a dominating set of minimum total weight, we obtain an $O(n^5 \log^2 n)$-time algorithm.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [363] [Degradation-Aware Frequency Regulation of a Heterogeneous Battery Fleet via Reinforcement Learning](https://arxiv.org/abs/2601.22865)
*Tanay Raghunandan Srinivasa,Vivek Deulkar,Jia Bhargava,Mohammad Hajiesmaili,Prashant Shenoy*

Main category: eess.SY

TL;DR: 本文研究异构电池群实时调度，以最小化长期循环退化，提出基于MDP的方法和函数逼近强化学习方法，实验显示能降低循环深度和退化指标。


<details>
  <summary>Details</summary>
Motivation: 电池储能系统充放电会导致循环退化和寿命降低，需研究减少退化的调度方法。

Method: 将调度问题建模为有约束动作空间的MDP，设计密集代理奖励；使用极限学习机作为随机非线性特征映射结合线性时序差分学习的函数逼近强化学习方法。

Result: 在玩具马尔可夫信号模型和从真实调节信号轨迹训练的模型上，与基线调度策略相比，循环深度出现和退化指标持续降低。

Conclusion: 所提方法能有效减少电池长期循环退化。

Abstract: Battery energy storage systems are increasingly deployed as fast-responding resources for grid balancing services such as frequency regulation and for mitigating renewable generation uncertainty. However, repeated charging and discharging induces cycling degradation and reduces battery lifetime. This paper studies the real-time scheduling of a heterogeneous battery fleet that collectively tracks a stochastic balancing signal subject to per-battery ramp-rate and capacity constraints, while minimizing long-term cycling degradation.
  Cycling degradation is fundamentally path-dependent: it is determined by charge-discharge cycles formed by the state-of-charge (SoC) trajectory and is commonly quantified via rainflow cycle counting. This non-Markovian structure makes it difficult to express degradation as an additive per-time-step cost, complicating classical dynamic programming approaches. We address this challenge by formulating the fleet scheduling problem as a Markov decision process (MDP) with constrained action space and designing a dense proxy reward that provides informative feedback at each time step while remaining aligned with long-term cycle-depth reduction.
  To scale learning to large state-action spaces induced by fine-grained SoC discretization and asymmetric per-battery constraints, we develop a function-approximation reinforcement learning method using an Extreme Learning Machine (ELM) as a random nonlinear feature map combined with linear temporal-difference learning. We evaluate the proposed approach on a toy Markovian signal model and on a Markovian model trained from real-world regulation signal traces obtained from the University of Delaware, and demonstrate consistent reductions in cycle-depth occurrence and degradation metrics compared to baseline scheduling policies.

</details>


### [364] [Reinforcement Learning-Based Co-Design and Operation of Chiller and Thermal Energy Storage for Cost-Optimal HVAC Systems](https://arxiv.org/abs/2601.22880)
*Tanay Raghunandan Srinivasa,Vivek Deulkar,Aviruch Bhatia,Vishal Garg*

Main category: eess.SY

TL;DR: 本文使用强化学习研究商业HVAC系统冷却基础设施联合运营与规模确定，以30年生命周期成本最小化为目标，确定了最优的冷水机组和蓄热装置容量。


<details>
  <summary>Details</summary>
Motivation: 最小化商业HVAC系统冷却基础设施30年生命周期成本，同时考虑资本成本不对称导致的联合设计难题。

Method: 将固定配置的冷水机组运行问题建模为有限时域马尔可夫决策过程（MDP），用带约束动作空间的深度Q网络（DQN）求解，评估不同配置，在可行集上进行生命周期成本最小化。

Result: 确定最优的冷水机组容量为700，蓄热装置容量为1500。

Conclusion: 使用强化学习方法可有效解决商业HVAC系统冷却基础设施的联合运营和规模确定问题，实现生命周期成本最小化。

Abstract: We study the joint operation and sizing of cooling infrastructure for commercial HVAC systems using reinforcement learning, with the objective of minimizing life-cycle cost over a 30-year horizon. The cooling system consists of a fixed-capacity electric chiller and a thermal energy storage (TES) unit, jointly operated to meet stochastic hourly cooling demands under time-varying electricity prices. The life-cycle cost accounts for both capital expenditure and discounted operating cost, including electricity consumption and maintenance. A key challenge arises from the strong asymmetry in capital costs: increasing chiller capacity by one unit is far more expensive than an equivalent increase in TES capacity. As a result, identifying the right combination of chiller and TES sizes, while ensuring zero loss-of-cooling-load under optimal operation, is a non-trivial co-design problem. To address this, we formulate the chiller operation problem for a fixed infrastructure configuration as a finite-horizon Markov Decision Process (MDP), in which the control action is the chiller part-load ratio (PLR). The MDP is solved using a Deep Q Network (DQN) with a constrained action space. The learned DQN RL policy minimizes electricity cost over historical traces of cooling demand and electricity prices. For each candidate chiller-TES sizing configuration, the trained policy is evaluated. We then restrict attention to configurations that fully satisfy the cooling demand and perform a life-cycle cost minimization over this feasible set to identify the cost-optimal infrastructure design. Using this approach, we determine the optimal chiller and thermal energy storage capacities to be 700 and 1500, respectively.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [365] [Minimal-Action Discrete Schrödinger Bridge Matching for Peptide Sequence Design](https://arxiv.org/abs/2601.22408)
*Shrey Goel,Pranam Chatterjee*

Main category: q-bio.BM

TL;DR: 提出Minimal - action discrete Schrödinger Bridge Matching (MadSBM)用于肽设计，还引入引导机制拓展治疗性肽设计空间。


<details>
  <summary>Details</summary>
Motivation: 现有离散扩散和基于流的方法在生成肽序列时需经过低似然区域且采样步骤多，要解决生成肽序列在离散且高度受限空间中的问题。

Method: 将生成过程构建为氨基酸编辑图上的受控连续时间马尔可夫过程，定义基于预训练蛋白质语言模型对数的生物参考过程，学习时间相关控制场，在采样过程中引入引导机制。

Result: 提出MadSBM这一基于速率的生成框架，并实现了离散分类器引导在基于薛定谔桥生成模型中的首次应用。

Conclusion: MadSBM可使生成的概率轨迹保持在高似然序列邻域附近，拓展了治疗性肽的设计空间。

Abstract: Generative modeling of peptide sequences requires navigating a discrete and highly constrained space in which many intermediate states are chemically implausible or unstable. Existing discrete diffusion and flow-based methods rely on reversing fixed corruption processes or following prescribed probability paths, which can force generation through low-likelihood regions and require countless sampling steps. We introduce Minimal-action discrete Schrödinger Bridge Matching (MadSBM), a rate-based generative framework for peptide design that formulates generation as a controlled continuous-time Markov process on the amino-acid edit graph. To yield probability trajectories that remain near high-likelihood sequence neighborhoods throughout generation, MadSBM 1) defines generation relative to a biologically informed reference process derived from pre-trained protein language model logits and 2) learns a time-dependent control field that biases transition rates to produce low-action transport paths from a masked prior to the data distribution. We finally introduce guidance to the MadSBM sampling procedure towards a specific functional objective, expanding the design space of therapeutic peptides; to our knowledge, this represents the first-ever application of discrete classifier guidance to Schrödinger bridge-based generative models.

</details>


### [366] [Disentangling multispecific antibody function with graph neural networks](https://arxiv.org/abs/2601.23212)
*Joshua Southern,Changpeng Lu,Santrupti Nerli,Samuel D. Stanton,Andrew M. Watkins,Franziska Seeger,Frédéric A. Dreyer*

Main category: q-bio.BM

TL;DR: 本文提出计算框架，生成合成功能景观并设计图神经网络架构，可在有限生物数据集上预测多特异性抗体功能，用于优化治疗药物设计。


<details>
  <summary>Details</summary>
Motivation: 多特异性抗体因分子架构复杂难以预测拓扑结构变化对功能的影响，且缺乏全面实验数据，现有合理设计存在瓶颈。

Method: 提出生成方法创建大规模合成功能景观，设计图神经网络架构编码拓扑约束。

Result: 模型在合成景观上训练后能重现复杂功能特性，通过迁移学习在有限生物数据集上有望实现高预测精度，可用于优化三特异性T细胞衔接子等。

Conclusion: 该工作为解析多特异性组合复杂性提供基准测试环境，可加速下一代治疗药物设计。

Abstract: Multispecific antibodies offer transformative therapeutic potential by engaging multiple epitopes simultaneously, yet their efficacy is an emergent property governed by complex molecular architectures. Rational design is often bottlenecked by the inability to predict how subtle changes in domain topology influence functional outcomes, a challenge exacerbated by the scarcity of comprehensive experimental data. Here, we introduce a computational framework to address part of this gap. First, we present a generative method for creating large-scale, realistic synthetic functional landscapes that capture non-linear interactions where biological activity depends on domain connectivity. Second, we propose a graph neural network architecture that explicitly encodes these topological constraints, distinguishing between format configurations that appear identical to sequence-only models. We demonstrate that this model, trained on synthetic landscapes, recapitulates complex functional properties and, via transfer learning, has the potential to achieve high predictive accuracy on limited biological datasets. We showcase the model's utility by optimizing trade-offs between efficacy and toxicity in trispecific T-cell engagers and retrieving optimal common light chains. This work provides a robust benchmarking environment for disentangling the combinatorial complexity of multispecifics, accelerating the design of next-generation therapeutics.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [367] [Variational Tail Bounds for Norms of Random Vectors and Matrices](https://arxiv.org/abs/2503.17300)
*Sohail Bahmani*

Main category: math.PR

TL;DR: 本文提出随机向量范数的变分尾界，给出简化版本，应用该方法得到多种界和不等式，还用耦合进行抽象推广。


<details>
  <summary>Details</summary>
Motivation: 在一维边缘矩假设下研究随机向量范数的尾界。

Method: 提出变分尾界，用高斯分布的某种推前对“聚合分布”进行参数化，还使用耦合方法。

Result: 重现高斯随机向量范数界，得到任意矩分布随机向量欧几里得范数的无维度尾界、半正定矩阵和的无维度集中不等式、样本协方差矩阵集中不等式、随机矩阵级数算子范数尾界。

Conclusion: 提出的方法有效，且可通过耦合进行更广泛的抽象应用。

Abstract: We propose a variational tail bound for norms of random vectors under moment assumptions on their one-dimensional marginals. A simplified version of the bound that parametrizes the ``aggregating distribution'' using a certain pushforward of the Gaussian distribution is also provided. We apply the proposed method to reproduce some of the well-known bounds on norms of Gaussian random vectors, and also obtain dimension-free tail bounds for the Euclidean norm of random vectors with arbitrary moment profiles. Furthermore, we reproduce a dimension-free concentration inequality for sum of independent and identically distributed positive semidefinite matrices with sub-exponential marginals, and obtain a concentration inequality for the sample covariance matrix of sub-exponential random vectors. We also obtain a tail bound for the operator norm of a random matrix series whose random coefficients may have arbitrary moment profiles. Furthermore, we use coupling to formulate an abstraction of the proposed approach that applies more broadly.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [368] [RulePlanner: All-in-One Reinforcement Learner for Unifying Design Rules in 3D Floorplanning](https://arxiv.org/abs/2601.22476)
*Ruizhe Zhong,Xingbo Du,Junchi Yan*

Main category: cs.AR

TL;DR: 提出基于深度强化学习的一体化方法解决集成电路布局规划中复杂硬件设计规则遵守难题，实验验证有效性和可迁移性，框架可扩展。


<details>
  <summary>Details</summary>
Motivation: 随着技术节点缩小，集成电路布局规划中遵守复杂硬件设计规则愈发困难，现有方法只能处理特定有限规则，需人工调整，耗时费力。

Method: 提出基于深度强化学习的一体化方法，将各种硬件设计规则处理统一到含新颖矩阵表示、动作空间约束、约束满足定量分析三个关键组件的框架中。

Result: 在公共基准测试中证明方法有效，在未见电路上展示可迁移性。

Conclusion: 所提框架可扩展以适应新设计规则，为未来芯片设计提供灵活性。

Abstract: Floorplanning determines the coordinate and shape of each module in Integrated Circuits. With the scaling of technology nodes, in floorplanning stage especially 3D scenarios with multiple stacked layers, it has become increasingly challenging to adhere to complex hardware design rules. Current methods are only capable of handling specific and limited design rules, while violations of other rules require manual and meticulous adjustment. This leads to labor-intensive and time-consuming post-processing for expert engineers. In this paper, we propose an all-in-one deep reinforcement learning-based approach to tackle these challenges, and design novel representations for real-world IC design rules that have not been addressed by previous approaches. Specifically, the processing of various hardware design rules is unified into a single framework with three key components: 1) novel matrix representations to model the design rules, 2) constraints on the action space to filter out invalid actions that cause rule violations, and 3) quantitative analysis of constraint satisfaction as reward signals. Experiments on public benchmarks demonstrate the effectiveness and validity of our approach. Furthermore, transferability is well demonstrated on unseen circuits. Our framework is extensible to accommodate new design rules, thus providing flexibility to address emerging challenges in future chip design. Code will be available at: https://github.com/Thinklab-SJTU/EDA-AI

</details>


### [369] [Machine Learning for Energy-Performance-aware Scheduling](https://arxiv.org/abs/2601.23134)
*Zheyuan Hu,Yifei Shi*

Main category: cs.AR

TL;DR: 提出用高斯过程的贝叶斯优化框架在异构多核架构上自动搜索最优调度配置，处理多目标问题并增加模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 后Dennard时代优化嵌入式系统需平衡能效和延迟，传统启发式调优在高维非平滑环境效率低。

Method: 提出贝叶斯优化框架，用高斯过程自动搜索，近似帕累托前沿处理多目标问题，结合敏感性分析和比较不同协方差核增加可解释性。

Result: 未提及具体结果。

Conclusion: 未提及明确结论。

Abstract: In the post-Dennard era, optimizing embedded systems requires navigating complex trade-offs between energy efficiency and latency. Traditional heuristic tuning is often inefficient in such high-dimensional, non-smooth landscapes. In this work, we propose a Bayesian Optimization framework using Gaussian Processes to automate the search for optimal scheduling configurations on heterogeneous multi-core architectures. We explicitly address the multi-objective nature of the problem by approximating the Pareto Frontier between energy and time. Furthermore, by incorporating Sensitivity Analysis (fANOVA) and comparing different covariance kernels (e.g., Matérn vs. RBF), we provide physical interpretability to the black-box model, revealing the dominant hardware parameters driving system performance.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [370] [Advanced techniques and applications of LiDAR Place Recognition in Agricultural Environments: A Comprehensive Survey](https://arxiv.org/abs/2601.22198)
*Judith Vilella-Cantos,Mónica Ballesta,David Valiente,María Flores,Luis Payá*

Main category: cs.RO

TL;DR: 论文对农业环境中基于LiDAR的地点识别（LPR）最新深度学习应用进行全面综述，聚焦该环境挑战、现有方法、数据集和评估指标，探讨局限与未来方向。


<details>
  <summary>Details</summary>
Motivation: 精准农业受益于自主定位系统，但农业环境缺乏特征且结构不明确，使基于LiDAR的地点识别有挑战，需对农业环境中LiDAR定位进行研究综述。

Method: 对农业环境下最新深度学习应用和LPR技术进行综合回顾，分析现有方法、数据集和评估指标。

Result: 梳理了农业环境下基于LiDAR定位的相关研究现状。

Conclusion: 这是首个专注农业场景下基于LiDAR定位的综述，有助于深入理解并推动该领域进一步研究。

Abstract: An optimal solution to the localization problem is essential for developing autonomous robotic systems. Apart from autonomous vehicles, precision agriculture is one of the elds that can bene t most from these systems. Although LiDAR place recognition is a widely used technique in recent years to achieve accurate localization, it is mostly used in urban settings. However, the lack of distinctive features and the unstructured nature of agricultural environments make place recognition challenging. This work presents a comprehensive review of state-of-the-art the latest deep learning applications for agricultural environments and LPR techniques. We focus on the challenges that arise in these environments. We analyze the existing approaches, datasets, and metrics used to evaluate LPR system performance and discuss the limitations and future directions of research in this eld. This is the rst survey that focuses on LiDAR based localization in agricultural settings, with the aim of providing a thorough understanding and fostering further research in this specialized domain.

</details>


### [371] [Adapting Reinforcement Learning for Path Planning in Constrained Parking Scenarios](https://arxiv.org/abs/2601.22545)
*Feng Tao,Luca Paparusso,Chenyi Gu,Robin Koehler,Chenxu Wu,Xinyu Huang,Christian Juette,David Paz,Ren Liu*

Main category: cs.RO

TL;DR: 提出用于停车场景实时路径规划的DRL框架，在成功率和效率上表现出色，还开源了新基准。


<details>
  <summary>Details</summary>
Motivation: 传统经典规划器在约束环境实时路径规划中受感知约束影响大、计算成本高，难以实时部署。

Method: 引入DRL框架，将任务建模为基于自行车模型动力学的序贯决策问题，开发新基准用于训练和评估。

Result: 该方法成功率和效率达到了最先进水平，成功率比经典规划器基线高96%，效率高52%。

Conclusion: 所提方法有效，开源的基准能促进自主系统未来研究。

Abstract: Real-time path planning in constrained environments remains a fundamental challenge for autonomous systems. Traditional classical planners, while effective under perfect perception assumptions, are often sensitive to real-world perception constraints and rely on online search procedures that incur high computational costs. In complex surroundings, this renders real-time deployment prohibitive. To overcome these limitations, we introduce a Deep Reinforcement Learning (DRL) framework for real-time path planning in parking scenarios. In particular, we focus on challenging scenes with tight spaces that require a high number of reversal maneuvers and adjustments. Unlike classical planners, our solution does not require ideal and structured perception, and in principle, could avoid the need for additional modules such as localization and tracking, resulting in a simpler and more practical implementation. Also, at test time, the policy generates actions through a single forward pass at each step, which is lightweight enough for real-time deployment. The task is formulated as a sequential decision-making problem grounded in a bicycle model dynamics, enabling the agent to directly learn navigation policies that respect vehicle kinematics and environmental constraints in the closed-loop setting. A new benchmark is developed to support both training and evaluation, capturing diverse and challenging scenarios. Our approach achieves state-of-the-art success rates and efficiency, surpassing classical planner baselines by +96% in success rate and +52% in efficiency. Furthermore, we release our benchmark as an open-source resource for the community to foster future research in autonomous systems. The benchmark and accompanying tools are available at https://github.com/dqm5rtfg9b-collab/Constrained_Parking_Scenarios.

</details>


### [372] [MTDrive: Multi-turn Interactive Reinforcement Learning for Autonomous Driving](https://arxiv.org/abs/2601.22930)
*Xidong Li,Mingyu Guo,Chenchao Xu,Bailin Li,Wenjing Zhu,Yangang Zou,Rui Chen,Zehuan Wang*

Main category: cs.RO

TL;DR: 提出多轮框架MTDrive用于自动驾驶轨迹规划，通过mtGRPO缓解奖励稀疏问题，构建数据集支持训练，实验表现优且实现系统级优化提升训练吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有将多模态大语言模型与强化学习结合的方法限于单轮推理，难以处理需迭代优化的复杂任务。

Method: 提出多轮框架MTDrive，引入mtGRPO，构建交互式轨迹理解数据集，进行系统级优化。

Result: 在NAVSIM基准测试中表现优于现有方法，实现2.5倍训练吞吐量。

Conclusion: 多轮推理范式有效，数据、模型和代码即将开源。

Abstract: Trajectory planning is a core task in autonomous driving, requiring the prediction of safe and comfortable paths across diverse scenarios. Integrating Multi-modal Large Language Models (MLLMs) with Reinforcement Learning (RL) has shown promise in addressing "long-tail" scenarios. However, existing methods are constrained to single-turn reasoning, limiting their ability to handle complex tasks requiring iterative refinement. To overcome this limitation, we present MTDrive, a multi-turn framework that enables MLLMs to iteratively refine trajectories based on environmental feedback. MTDrive introduces Multi-Turn Group Relative Policy Optimization (mtGRPO), which mitigates reward sparsity by computing relative advantages across turns. We further construct an interactive trajectory understanding dataset from closed-loop simulation to support multi-turn training. Experiments on the NAVSIM benchmark demonstrate superior performance compared to existing methods, validating the effectiveness of our multi-turn reasoning paradigm. Additionally, we implement system-level optimizations to reduce data transfer overhead caused by high-resolution images and multi-turn sequences, achieving 2.5x training throughput. Our data, models, and code will be made available soon.

</details>


### [373] [Exo-Plore: Exploring Exoskeleton Control Space through Human-aligned Simulation](https://arxiv.org/abs/2601.22550)
*Geonho Leem,Jaedong Lee,Jehee Lee,Seungmoon Song,Jungdam Won*

Main category: cs.RO

TL;DR: 提出Exo - explore模拟框架，结合神经力学模拟与深度强化学习优化髋部外骨骼辅助，无需真人实验。


<details>
  <summary>Details</summary>
Motivation: 外骨骼增强移动能力有前景，但提供合适辅助具挑战，现有优化方法需大量人类实验，行动不便者难以参与。

Method: 采用Exo - explore模拟框架，结合神经力学模拟与深度强化学习。

Result: 能生成捕捉人类对辅助力适应的真实步态数据，尽管人类步态有随机性仍能产生可靠优化结果，能推广到病态步态，显示病理严重程度与最佳辅助间的强线性关系。

Conclusion: Exo - explore模拟框架可在不进行真人实验的情况下优化髋部外骨骼辅助。

Abstract: Exoskeletons show great promise for enhancing mobility, but providing appropriate assistance remains challenging due to the complexity of human adaptation to external forces. Current state-of-the-art approaches for optimizing exoskeleton controllers require extensive human experiments in which participants must walk for hours, creating a paradox: those who could benefit most from exoskeleton assistance, such as individuals with mobility impairments, are rarely able to participate in such demanding procedures. We present Exo-plore, a simulation framework that combines neuromechanical simulation with deep reinforcement learning to optimize hip exoskeleton assistance without requiring real human experiments. Exo-plore can (1) generate realistic gait data that captures human adaptation to assistive forces, (2) produce reliable optimization results despite the stochastic nature of human gait, and (3) generalize to pathological gaits, showing strong linear relationships between pathology severity and optimal assistance.

</details>


### [374] [IRL-DAL: Safe and Adaptive Trajectory Planning for Autonomous Driving via Energy-Guided Diffusion Models](https://arxiv.org/abs/2601.23266)
*Seyed Ahmad Hosseini Miangoleh,Amin Jalal Aghdasian,Farzaneh Abdollahi*

Main category: cs.RO

TL;DR: 本文提出用于自动驾驶汽车的IRL - DAL框架，经多步骤训练后在Webots模拟器达96%成功率，降低碰撞率，提升鲁棒性，代码开源。


<details>
  <summary>Details</summary>
Motivation: 为自动驾驶汽车开发更有效的逆强化学习框架，提升自动驾驶的安全性和鲁棒性。

Method: 从专家有限状态机控制器模仿训练，结合环境项与IRL判别器信号，使用混合奖励进行强化学习，用条件扩散模型规划安全路径，用可学习自适应掩码改善感知，用近端策略优化微调策略，在Webots模拟器进行两阶段课程训练。

Result: 达到96%成功率，碰撞率降至每1000步0.05次。

Conclusion: 所提方法使智能体不仅能保持车道行驶，还能专业处理不安全状况，提高了鲁棒性。

Abstract: This paper proposes a novel inverse reinforcement learning framework using a diffusion-based adaptive lookahead planner (IRL-DAL) for autonomous vehicles. Training begins with imitation from an expert finite state machine (FSM) controller to provide a stable initialization. Environment terms are combined with an IRL discriminator signal to align with expert goals. Reinforcement learning (RL) is then performed with a hybrid reward that combines diffuse environmental feedback and targeted IRL rewards. A conditional diffusion model, which acts as a safety supervisor, plans safe paths. It stays in its lane, avoids obstacles, and moves smoothly. Then, a learnable adaptive mask (LAM) improves perception. It shifts visual attention based on vehicle speed and nearby hazards. After FSM-based imitation, the policy is fine-tuned with Proximal Policy Optimization (PPO). Training is run in the Webots simulator with a two-stage curriculum. A 96\% success rate is reached, and collisions are reduced to 0.05 per 1k steps, marking a new benchmark for safe navigation. By applying the proposed approach, the agent not only drives in lane but also handles unsafe conditions at an expert level, increasing robustness.We make our code publicly available.

</details>


### [375] [End-to-end Optimization of Belief and Policy Learning in Shared Autonomy Paradigms](https://arxiv.org/abs/2601.23285)
*MH Farhadi,Ali Rabiee,Sima Ghafoori,Anna Cetera,Andrew Fisher,Reza Abiri*

Main category: cs.RO

TL;DR: 文章提出BRACE框架优化共享自主系统，验证新算法优于SOTA，在复杂场景有益且可推广。


<details>
  <summary>Details</summary>
Motivation: 以往共享自主系统方法在非结构化环境表现不佳，需新方法推断用户意图和确定协助水平。

Method: 引入BRACE框架，微调贝叶斯意图推断和上下文自适应协助，使意图推断和协助仲裁端到端梯度流，根据环境和目标概率分布调整协作控制策略。

Result: 相比SOTA方法，成功率提高6.3%、路径效率提高41%；比无协助控制，成功率提高36.3%、路径效率提高87%。

Conclusion: 集成优化在复杂、目标模糊场景最有益，可在需要目标导向协助的机器人领域推广。

Abstract: Shared autonomy systems require principled methods for inferring user intent and determining appropriate assistance levels. This is a central challenge in human-robot interaction, where systems must be successful while being mindful of user agency. Previous approaches relied on static blending ratios or separated goal inference from assistance arbitration, leading to suboptimal performance in unstructured environments. We introduce BRACE (Bayesian Reinforcement Assistance with Context Encoding), a novel framework that fine-tunes Bayesian intent inference and context-adaptive assistance through an architecture enabling end-to-end gradient flow between intent inference and assistance arbitration. Our pipeline conditions collaborative control policies on environmental context and complete goal probability distributions. We provide analysis showing (1) optimal assistance levels should decrease with goal uncertainty and increase with environmental constraint severity, and (2) integrating belief information into policy learning yields a quadratic expected regret advantage over sequential approaches. We validated our algorithm against SOTA methods (IDA, DQN) using a three-part evaluation progressively isolating distinct challenges of end-effector control: (1) core human-interaction dynamics in a 2D human-in-the-loop cursor task, (2) non-linear dynamics of a robotic arm, and (3) integrated manipulation under goal ambiguity and environmental constraints. We demonstrate improvements over SOTA, achieving 6.3% higher success rates and 41% increased path efficiency, and 36.3% success rate and 87% path efficiency improvement over unassisted control. Our results confirmed that integrated optimization is most beneficial in complex, goal-ambiguous scenarios, and is generalizable across robotic domains requiring goal-directed assistance, advancing the SOTA for adaptive shared autonomy.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [376] [Bayesian Matrix Completion Under Geometric Constraints](https://arxiv.org/abs/2601.22765)
*Rohit Varma Chiluvuri,Santosh Nannuru*

Main category: eess.SP

TL;DR: 本文提出分层贝叶斯框架完成欧氏距离矩阵（EDM）的稀疏和噪声观测补全，实验显示在稀疏场景下重建精度优于确定性基线。


<details>
  <summary>Details</summary>
Motivation: 传统方法在稀疏或有噪声条件下完成EDM补全存在困难，需要更有效的方法。

Method: 引入分层贝叶斯框架，对生成EDM的潜在点集设置结构化先验，并使用Metropolis - Hastings within Gibbs采样器进行后验推断。

Result: 在合成数据实验中，相比确定性基线，在稀疏场景下重建精度得到提升。

Conclusion: 所提出的分层贝叶斯框架能有效完成EDM的稀疏和噪声观测补全，实现自动正则化和鲁棒的噪声处理。

Abstract: The completion of a Euclidean distance matrix (EDM) from sparse and noisy observations is a fundamental challenge in signal processing, with applications in sensor network localization, acoustic room reconstruction, molecular conformation, and manifold learning. Traditional approaches, such as rank-constrained optimization and semidefinite programming, enforce geometric constraints but often struggle under sparse or noisy conditions. This paper introduces a hierarchical Bayesian framework that places structured priors directly on the latent point set generating the EDM, naturally embedding geometric constraints. By incorporating a hierarchical prior on latent point set, the model enables automatic regularization and robust noise handling. Posterior inference is performed using a Metropolis-Hastings within Gibbs sampler to handle coupled latent point posterior. Experiments on synthetic data demonstrate improved reconstruction accuracy compared to deterministic baselines in sparse regimes.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [377] [Learning to Recommend Multi-Agent Subgraphs from Calling Trees](https://arxiv.org/abs/2601.22209)
*Xinyuan Song,Liang Zhao*

Main category: cs.MA

TL;DR: 文章针对多智能体系统中智能体推荐问题，提出约束推荐框架，支持两种推荐设置，并构建统一基准进行评估。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统无法直接解决多智能体编排中结构化、顺序性和交互依赖的问题。

Method: 将智能体推荐建模为约束决策问题，引入约束推荐框架，利用历史调用树，框架支持两种推荐设置，构建统一调用树基准。

Result: 提出了可行的约束推荐框架和统一调用树基准。

Conclusion: 所提框架可解决多智能体系统中智能体推荐问题，构建的基准可用于系统评估。

Abstract: Multi-agent systems (MAS) increasingly solve complex tasks by orchestrating agents and tools selected from rapidly growing marketplaces. As these marketplaces expand, many candidates become functionally overlapping, making selection not just a retrieval problem: beyond filtering relevant agents, an orchestrator must choose options that are reliable, compatible with the current execution context, and able to cooperate with other selected agents. Existing recommender systems -- largely built for item-level ranking from flat user-item logs -- do not directly address the structured, sequential, and interaction-dependent nature of agent orchestration. We address this gap by \textbf{formulating agent recommendation in MAS as a constrained decision problem} and introducing a generic \textbf{constrained recommendation framework} that first uses retrieval to build a compact candidate set conditioned on the current subtask and context, and then performs \textbf{utility optimization} within this feasible set using a learned scorer that accounts for relevance, reliability, and interaction effects. We ground both the formulation and learning signals in \textbf{historical calling trees}, which capture the execution structure of MAS (parent-child calls, branching dependencies, and local cooperation patterns) beyond what flat logs provide. The framework supports two complementary settings: \textbf{agent-level recommendation} (select the next agent/tool) and \textbf{system-level recommendation} (select a small, connected agent team/subgraph for coordinated execution). To enable systematic evaluation, we construct a unified calling-tree benchmark by normalizing invocation logs from eight heterogeneous multi-agent corpora into a shared structured representation.

</details>


### [378] [ScholarPeer: A Context-Aware Multi-Agent Framework for Automated Peer Review](https://arxiv.org/abs/2601.22638)
*Palash Goyal,Mihir Parmar,Yiwen Song,Hamid Palangi,Tomas Pfister,Jinsung Yoon*

Main category: cs.MA

TL;DR: 本文介绍了搜索启用的多智能体框架ScholarPeer，能模仿资深研究人员认知过程，评估结果显示其优于现有方法并缩小与人类水平差距。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的自动同行评审系统只能进行“表面”批评，难以准确评估新颖性、重要性和识别深层方法缺陷，缺乏人类专家拥有的外部背景。

Method: 引入ScholarPeer框架，采用上下文获取和主动验证的双流过程，包括用历史学家代理动态构建领域叙述、用基线侦察员识别缺失比较、通过多方面问答引擎验证主张。

Result: 在DeepReview - 13K上评估，ScholarPeer在并排评估中对现有方法取得显著胜率，缩小与人类水平多样性的差距。

Conclusion: ScholarPeer框架在自动同行评审方面表现良好，优于现有方法，能更好地模仿人类专家进行评审。

Abstract: Automated peer review has evolved from simple text classification to structured feedback generation. However, current state-of-the-art systems still struggle with "surface-level" critiques: they excel at summarizing content but often fail to accurately assess novelty and significance or identify deep methodological flaws because they evaluate papers in a vacuum, lacking the external context a human expert possesses. In this paper, we introduce ScholarPeer, a search-enabled multi-agent framework designed to emulate the cognitive processes of a senior researcher. ScholarPeer employs a dual-stream process of context acquisition and active verification. It dynamically constructs a domain narrative using a historian agent, identifies missing comparisons via a baseline scout, and verifies claims through a multi-aspect Q&A engine, grounding the critique in live web-scale literature. We evaluate ScholarPeer on DeepReview-13K and the results demonstrate that ScholarPeer achieves significant win-rates against state-of-the-art approaches in side-by-side evaluations and reduces the gap to human-level diversity.

</details>


### [379] [Aligning Microscopic Vehicle and Macroscopic Traffic Statistics: Reconstructing Driving Behavior from Partial Data](https://arxiv.org/abs/2601.22242)
*Zhihao Zhang,Keith Redmill,Chengyang Peng,Bowen Weng*

Main category: cs.MA

TL;DR: 提出一种框架，结合宏观和微观数据学习共享策略，促进安全高效自动驾驶。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶算法依赖高质量真实驾驶数据，获取难且成本高，且车辆和路边传感器数据各有局限，需结合两者优势。

Method: 提出框架，从宏观观测重建未观测的微观状态，用微观数据锚定车辆行为，学习共享策略。

Result: 该策略能在微观上与部分观测轨迹和动作一致，宏观上与目标交通统计数据相符。

Conclusion: 这种受约束和正则化的策略可促进现实交通流模式和大规模与人类驾驶员安全协作。

Abstract: A driving algorithm that aligns with good human driving practices, or at the very least collaborates effectively with human drivers, is crucial for developing safe and efficient autonomous vehicles. In practice, two main approaches are commonly adopted: (i) supervised or imitation learning, which requires comprehensive naturalistic driving data capturing all states that influence a vehicle's decisions and corresponding actions, and (ii) reinforcement learning (RL), where the simulated driving environment either matches or is intentionally more challenging than real-world conditions. Both methods depend on high-quality observations of real-world driving behavior, which are often difficult and costly to obtain. State-of-the-art sensors on individual vehicles can gather microscopic data, but they lack context about the surrounding conditions. Conversely, roadside sensors can capture traffic flow and other macroscopic characteristics, but they cannot associate this information with individual vehicles on a microscopic level. Motivated by this complementarity, we propose a framework that reconstructs unobserved microscopic states from macroscopic observations, using microscopic data to anchor observed vehicle behaviors, and learns a shared policy whose behavior is microscopically consistent with the partially observed trajectories and actions and macroscopically aligned with target traffic statistics when deployed population-wide. Such constrained and regularized policies promote realistic flow patterns and safe coordination with human drivers at scale.

</details>


### [380] [Learning Reward Functions for Cooperative Resilience in Multi-Agent Systems](https://arxiv.org/abs/2601.22292)
*Manuela Chacon-Chamorro,Luis Felipe Giraldo,Nicanor Quijano*

Main category: cs.MA

TL;DR: 研究多智能体强化学习中奖励函数设计对合作弹性的影响，提出新框架，实验表明混合策略可提升抗干扰性。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在动态不确定环境需兼顾个体目标与集体功能，合作弹性在多智能体强化学习中关键但研究不足。

Method: 引入从排序轨迹学习奖励函数的框架，使用三种奖励策略训练智能体，探索三种奖励参数化方法，用两种基于偏好的学习算法从行为排名推断奖励。

Result: 混合策略显著提高抗干扰能力，不降低任务表现，减少资源过度使用等灾难性结果。

Conclusion: 强调奖励设计对促进弹性合作的重要性，向开发能在不确定环境维持合作的鲁棒多智能体系统迈进。

Abstract: Multi-agent systems often operate in dynamic and uncertain environments, where agents must not only pursue individual goals but also safeguard collective functionality. This challenge is especially acute in mixed-motive multi-agent systems. This work focuses on cooperative resilience, the ability of agents to anticipate, resist, recover, and transform in the face of disruptions, a critical yet underexplored property in Multi-Agent Reinforcement Learning. We study how reward function design influences resilience in mixed-motive settings and introduce a novel framework that learns reward functions from ranked trajectories, guided by a cooperative resilience metric. Agents are trained in a suite of social dilemma environments using three reward strategies: i) traditional individual reward; ii) resilience-inferred reward; and iii) hybrid that balance both. We explore three reward parameterizations-linear models, hand-crafted features, and neural networks, and employ two preference-based learning algorithms to infer rewards from behavioral rankings. Our results demonstrate that hybrid strategy significantly improve robustness under disruptions without degrading task performance and reduce catastrophic outcomes like resource overuse. These findings underscore the importance of reward design in fostering resilient cooperation, and represent a step toward developing robust multi-agent systems capable of sustaining cooperation in uncertain environments.

</details>


### [381] [MonoScale: Scaling Multi-Agent System with Monotonic Improvement](https://arxiv.org/abs/2601.23219)
*Shuai Shao,Yixiang Liu,Bingwei Lu,Weinan Zhang*

Main category: cs.MA

TL;DR: 提出MonoScale框架解决基于LLM的多智能体系统扩展时的性能崩溃问题，实验显示有稳定收益。


<details>
  <summary>Details</summary>
Motivation: 基于LLM的多智能体系统扩展智能体池时，简单扩展会导致路由器冷启动时性能崩溃。

Method: 提出MonoScale扩展感知更新框架，生成熟悉任务，收集交互证据并转化为可审计的自然语言记忆，将顺序增强形式化为上下文老虎机并进行信任区域内存更新。

Result: 在GAIA和Humanity's Last Exam实验中，随着智能体池增长有稳定收益，优于简单扩展和强路由器固定池基线。

Conclusion: MonoScale框架能有效解决多智能体系统扩展时的性能问题，保证性能单调非递减。

Abstract: In recent years, LLM-based multi-agent systems (MAS) have advanced rapidly, using a router to decompose tasks and delegate subtasks to specialized agents. A natural way to expand capability is to scale up the agent pool by continually integrating new functional agents or tool interfaces, but naive expansion can trigger performance collapse when the router cold-starts on newly added, heterogeneous, and unreliable agents. We propose MonoScale, an expansion-aware update framework that proactively generates a small set of agent-conditioned familiarization tasks, harvests evidence from both successful and failed interactions, and distills it into auditable natural-language memory to guide future routing. We formalize sequential augmentation as a contextual bandit and perform trust-region memory updates, yielding a monotonic non-decreasing performance guarantee across onboarding rounds. Experiments on GAIA and Humanity's Last Exam show stable gains as the agent pool grows, outperforming naive scale-up and strong-router fixed-pool baselines.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [382] [Lost in Space? Vision-Language Models Struggle with Relative Camera Pose Estimation](https://arxiv.org/abs/2601.22228)
*Ken Deng,Yifu Qiu,Yoni Kasten,Shay B. Cohen,Yftah Ziser*

Main category: cs.CV

TL;DR: 研究VLMs在3D空间结构理解上的不足，通过RCPE任务，引入VRRPI - Bench和VRRPI - Diag基准测试，发现VLMs在3D和多视图空间推理存在局限。


<details>
  <summary>Details</summary>
Motivation: 探究VLMs在2D感知和语义推理表现良好，但在3D空间结构理解有限这一差距。

Method: 使用相对相机姿态估计（RCPE）任务，引入VRRPI - Bench和VRRPI - Diag基准测试。

Result: 多数VLMs无法超越浅层2D启发式方法，如GPT - 5表现不如经典几何基线和人类，且在多图像推理中表现不一致。

Conclusion: VLMs在3D和多视图空间推理存在局限。

Abstract: Vision-Language Models (VLMs) perform well in 2D perception and semantic reasoning compared to their limited understanding of 3D spatial structure. We investigate this gap using relative camera pose estimation (RCPE), a fundamental vision task that requires inferring relative camera translation and rotation from a pair of images. We introduce VRRPI-Bench, a benchmark derived from unlabeled egocentric videos with verbalized annotations of relative camera motion, reflecting realistic scenarios with simultaneous translation and rotation around a shared object. We further propose VRRPI-Diag, a diagnostic benchmark that isolates individual motion degrees of freedom. Despite the simplicity of RCPE, most VLMs fail to generalize beyond shallow 2D heuristics, particularly for depth changes and roll transformations along the optical axis. Even state-of-the-art models such as GPT-5 ($0.64$) fall short of classic geometric baselines ($0.97$) and human performance ($0.92$). Moreover, VLMs exhibit difficulty in multi-image reasoning, with inconsistent performance (best $59.7\%$) when integrating spatial cues across frames. Our findings reveal limitations in grounding VLMs in 3D and multi-view spatial reasoning.

</details>


### [383] [VMonarch: Efficient Video Diffusion Transformers with Structured Attention](https://arxiv.org/abs/2601.22275)
*Cheng Liang,Haoxian Chen,Liang Hou,Qi Fan,Gangshan Wu,Xin Tao,Limin Wang*

Main category: cs.CV

TL;DR: 提出用于视频扩散变压器（Video DiTs）的VMonarch注意力机制，可高效处理动态稀疏模式，实验显示其能克服注意力瓶颈，减少计算量并提升速度。


<details>
  <summary>Details</summary>
Motivation: 注意力机制的二次复杂度严重限制Video DiTs的上下文可扩展性。

Method: 1. 采用时空Monarch分解捕捉视频数据帧内和帧间相关性； 2. 引入重计算策略减轻Monarch矩阵交替最小化时的不稳定性； 3. 提出融合在线熵算法的FlashAttention实现长序列快速更新。

Result: VMonarch在VBench上经微调后生成质量与全注意力相当或更优，克服注意力瓶颈，减少17.5倍注意力FLOPs，长视频注意力计算提速超5倍，超越90%稀疏度的先进稀疏注意力方法。

Conclusion: VMonarch能够在Video DiTs中高效处理动态稀疏模式，解决注意力机制的复杂度问题。

Abstract: The quadratic complexity of the attention mechanism severely limits the context scalability of Video Diffusion Transformers (DiTs). We find that the highly sparse spatio-temporal attention patterns exhibited in Video DiTs can be naturally represented by the Monarch matrix. It is a class of structured matrices with flexible sparsity, enabling sub-quadratic attention via an alternating minimization algorithm. Accordingly, we propose VMonarch, a novel attention mechanism for Video DiTs that enables efficient computation over the dynamic sparse patterns with structured Monarch matrices. First, we adapt spatio-temporal Monarch factorization to explicitly capture the intra-frame and inter-frame correlations of the video data. Second, we introduce a recomputation strategy to mitigate artifacts arising from instabilities during alternating minimization of Monarch matrices. Third, we propose a novel online entropy algorithm fused into FlashAttention, enabling fast Monarch matrix updates for long sequences. Extensive experiments demonstrate that VMonarch achieves comparable or superior generation quality to full attention on VBench after minimal tuning. It overcomes the attention bottleneck in Video DiTs, reduces attention FLOPs by a factor of 17.5, and achieves a speedup of over 5x in attention computation for long videos, surpassing state-of-the-art sparse attention methods at 90% sparsity.

</details>


### [384] [Jailbreaks on Vision Language Model via Multimodal Reasoning](https://arxiv.org/abs/2601.22398)
*Aarush Noheria,Yuguang Yao*

Main category: cs.CV

TL;DR: 本文提出越狱框架和自适应噪声机制攻击视觉语言模型安全过滤器，实验显示策略提升攻击成功率且保持自然度。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型输出对提示变化敏感，存在安全对齐漏洞，需探索绕过安全过滤器的方法。

Method: 提出利用训练后思维链提示构建隐蔽提示的越狱框架，还提出基于模型反馈迭代扰动输入图像的ReAct驱动自适应噪声机制。

Result: 所提双策略显著提高攻击成功率，且在文本和视觉领域保持自然度。

Conclusion: 所提方法能有效绕过视觉语言模型的安全过滤器，为模型安全研究提供新视角。

Abstract: Vision-language models (VLMs) have become central to tasks such as visual question answering, image captioning, and text-to-image generation. However, their outputs are highly sensitive to prompt variations, which can reveal vulnerabilities in safety alignment. In this work, we present a jailbreak framework that exploits post-training Chain-of-Thought (CoT) prompting to construct stealthy prompts capable of bypassing safety filters. To further increase attack success rates (ASR), we propose a ReAct-driven adaptive noising mechanism that iteratively perturbs input images based on model feedback. This approach leverages the ReAct paradigm to refine adversarial noise in regions most likely to activate safety defenses, thereby enhancing stealth and evasion. Experimental results demonstrate that the proposed dual-strategy significantly improves ASR while maintaining naturalness in both text and visual domains.

</details>


### [385] [Countering the Over-Reliance Trap: Mitigating Object Hallucination for LVLMs via a Self-Validation Framework](https://arxiv.org/abs/2601.22451)
*Shiyu Liu,Xinyi Wen,Zhibin Lan,Ante Wang,Jinsong Su*

Main category: cs.CV

TL;DR: 现有大视觉语言模型在图像字幕任务中存在物体幻觉问题，本文分析其原因并提出无语言先验验证和无训练自验证框架，实验显示可显著减少物体幻觉。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型在图像字幕任务中存在物体幻觉问题，此前工作缺乏对其过度依赖语言先验的深入分析，故需深入研究并解决该问题。

Method: 进行初步实验分析依赖语言先验导致幻觉的问题，提出无语言先验验证，在此基础上构建无训练自验证框架，对候选字幕进行验证和选择/聚合。

Result: 实验表明该框架能显著降低图像字幕任务中的物体幻觉，如在CHAIRI指标上有65.6%的提升，超越了之前的SOTA方法。

Conclusion: 该方法为减轻大视觉语言模型的幻觉问题开辟了新途径。

Abstract: Despite progress in Large Vision Language Models (LVLMs), object hallucination remains a critical issue in image captioning task, where models generate descriptions of non-existent objects, compromising their reliability. Previous work attributes this to LVLMs' over-reliance on language priors and attempts to mitigate it through logits calibration. However, they still lack a thorough analysis of the over-reliance. To gain a deeper understanding of over-reliance, we conduct a series of preliminary experiments, indicating that as the generation length increases, LVLMs' over-reliance on language priors leads to inflated probability of hallucinated object tokens, consequently exacerbating object hallucination. To circumvent this issue, we propose Language-Prior-Free Verification to enable LVLMs to faithfully verify the confidence of object existence. Based on this, we propose a novel training-free Self-Validation Framework to counter the over-reliance trap. It first validates objects' existence in sampled candidate captions and further mitigates object hallucination via caption selection or aggregation. Experiment results demonstrate that our framework mitigates object hallucination significantly in image captioning task (e.g., 65.6% improvement on CHAIRI metric with LLaVA-v1.5-7B), surpassing the previous SOTA methods. This result highlights a novel path towards mitigating hallucination by unlocking the inherent potential within LVLMs themselves.

</details>


### [386] [Training-Free Representation Guidance for Diffusion Models with a Representation Alignment Projector](https://arxiv.org/abs/2601.22468)
*Wenqiang Zu,Shenghao Xie,Bo Lei,Lei Ma*

Main category: cs.CV

TL;DR: 本文针对扩散模型推理时语义漂移问题，引入表征对齐投影器的引导方案，在图像合成任务中提升了语义一致性和视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 现有推理时引导方法未充分利用无监督特征表征，且扩散变压器早期去噪阶段存在语义漂移问题。

Method: 引入使用表征对齐投影器的引导方案，将投影器预测的表征注入中间采样步骤。

Result: 在SiTs和REPAs上实验，显著降低FID分数，如REPA - XL/2从5.9降至3.3，结合无分类器引导有互补增益。

Conclusion: 基于表征的扩散采样是加强语义保留和图像一致性的实用策略。

Abstract: Recent progress in generative modeling has enabled high-quality visual synthesis with diffusion-based frameworks, supporting controllable sampling and large-scale training. Inference-time guidance methods such as classifier-free and representative guidance enhance semantic alignment by modifying sampling dynamics; however, they do not fully exploit unsupervised feature representations. Although such visual representations contain rich semantic structure, their integration during generation is constrained by the absence of ground-truth reference images at inference. This work reveals semantic drift in the early denoising stages of diffusion transformers, where stochasticity results in inconsistent alignment even under identical conditioning. To mitigate this issue, we introduce a guidance scheme using a representation alignment projector that injects representations predicted by a projector into intermediate sampling steps, providing an effective semantic anchor without modifying the model architecture. Experiments on SiTs and REPAs show notable improvements in class-conditional ImageNet synthesis, achieving substantially lower FID scores; for example, REPA-XL/2 improves from 5.9 to 3.3, and the proposed method outperforms representative guidance when applied to SiT models. The approach further yields complementary gains when combined with classifier-free guidance, demonstrating enhanced semantic coherence and visual fidelity. These results establish representation-informed diffusion sampling as a practical strategy for reinforcing semantic preservation and image consistency.

</details>


### [387] [Mitigating Hallucinations in Video Large Language Models via Spatiotemporal-Semantic Contrastive Decoding](https://arxiv.org/abs/2601.22574)
*Yuansheng Gao,Jinman Zhao,Tong Zhang,Xingguo Xu,Han Bao,Zonghui Wang,Wenzhi Chen*

Main category: cs.CV

TL;DR: 现有视频大语言模型存在幻觉问题，现有解码方法效果有限，提出时空 - 语义对比解码策略抑制幻觉，实验证明有效且能保留模型能力


<details>
  <summary>Details</summary>
Motivation: 现有缓解视频幻觉的解码方法多依赖启发式设计，无法精准捕捉幻觉根源及其细粒度的时间和语义关联，在复杂场景中鲁棒性和泛化性有限，需更有效方法缓解视频幻觉

Method: 提出时空 - 语义对比解码策略，通过故意破坏视频特征的时空一致性和语义关联构建负特征，在推理时通过与原始视频特征对比解码来抑制视频幻觉

Result: 方法能有效缓解幻觉发生

Conclusion: 所提方法不仅有效缓解了幻觉的发生，还保留了模型的一般视频理解和推理能力

Abstract: Although Video Large Language Models perform remarkably well across tasks such as video understanding, question answering, and reasoning, they still suffer from the problem of hallucination, which refers to generating outputs that are inconsistent with explicit video content or factual evidence. However, existing decoding methods for mitigating video hallucinations, while considering the spatiotemporal characteristics of videos, mostly rely on heuristic designs. As a result, they fail to precisely capture the root causes of hallucinations and their fine-grained temporal and semantic correlations, leading to limited robustness and generalization in complex scenarios. To more effectively mitigate video hallucinations, we propose a novel decoding strategy termed Spatiotemporal-Semantic Contrastive Decoding. This strategy constructs negative features by deliberately disrupting the spatiotemporal consistency and semantic associations of video features, and suppresses video hallucinations through contrastive decoding against the original video features during inference. Extensive experiments demonstrate that our method not only effectively mitigates the occurrence of hallucinations, but also preserves the general video understanding and reasoning capabilities of the model.

</details>


### [388] [Cross-Domain Few-Shot Learning for Hyperspectral Image Classification Based on Mixup Foundation Model](https://arxiv.org/abs/2601.22581)
*Naeem Paeedeh,Mahardhika Pratama,Ary Shiddiqi,Zehong Cao,Mukesh Prasad,Wisnu Jatmiko*

Main category: cs.CV

TL;DR: 论文提出用于高光谱图像跨域少样本学习分类的MIFOMO模型，实验显示其优于现有方法，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 现有高光谱图像跨域少样本学习方法依赖不切实际的数据增强，参数多易过拟合，且未利用基础模型优势。

Method: 构建基于遥感基础模型的MIFOMO，引入合并投影、混合域适应和标签平滑概念。

Result: MIFOMO在实验中比现有方法有最多14%的优势提升。

Conclusion: MIFOMO模型在高光谱图像跨域少样本学习分类中表现出色，代码开源利于后续研究。

Abstract: Although cross-domain few-shot learning (CDFSL) for hyper-spectral image (HSI) classification has attracted significant research interest, existing works often rely on an unrealistic data augmentation procedure in the form of external noise to enlarge the sample size, thus greatly simplifying the issue of data scarcity. They involve a large number of parameters for model updates, being prone to the overfitting problem. To the best of our knowledge, none has explored the strength of the foundation model, having strong generalization power to be quickly adapted to downstream tasks. This paper proposes the MIxup FOundation MOdel (MIFOMO) for CDFSL of HSI classifications. MIFOMO is built upon the concept of a remote sensing (RS) foundation model, pre-trained across a large scale of RS problems, thus featuring generalizable features. The notion of coalescent projection (CP) is introduced to quickly adapt the foundation model to downstream tasks while freezing the backbone network. The concept of mixup domain adaptation (MDM) is proposed to address the extreme domain discrepancy problem. Last but not least, the label smoothing concept is implemented to cope with noisy pseudo-label problems. Our rigorous experiments demonstrate the advantage of MIFOMO, where it beats prior arts with up to 14% margin. The source code of MIFOMO is open-sourced in https://github.com/Naeem- Paeedeh/MIFOMO for reproducibility and convenient further study.

</details>


### [389] [What can Computer Vision learn from Ranganathan?](https://arxiv.org/abs/2601.22634)
*Mayukh Bagchi,Fausto Giunchiglia*

Main category: cs.CV

TL;DR: 本文提出用S.R. Ranganathan分类原则解决计算机视觉语义鸿沟问题，阐述其支撑vTelos注释方法，并用实验验证vTelos。


<details>
  <summary>Details</summary>
Motivation: 解决计算机视觉中因视觉和词汇语义不一致导致的语义鸿沟问题，设计高质量数据集。

Method: 采用S.R. Ranganathan分类原则，构建vTelos CV注释方法。

Result: 实验显示CV注释和准确性有提升。

Conclusion: vTelos方法得到验证，S.R. Ranganathan分类原则可用于解决语义鸿沟问题。

Abstract: The Semantic Gap Problem (SGP) in Computer Vision (CV) arises from the misalignment between visual and lexical semantics leading to flawed CV dataset design and CV benchmarks. This paper proposes that classification principles of S.R. Ranganathan can offer a principled starting point to address SGP and design high-quality CV datasets. We elucidate how these principles, suitably adapted, underpin the vTelos CV annotation methodology. The paper also briefly presents experimental evidence showing improvements in CV annotation and accuracy, thereby, validating vTelos.

</details>


### [390] [Unsupervised Synthetic Image Attribution: Alignment and Disentanglement](https://arxiv.org/abs/2601.22663)
*Zongfang Liu,Guangyi Chen,Boyang Sun,Tongliang Liu,Kun Zhang*

Main category: cs.CV

TL;DR: 随着合成图像质量提升，识别其潜在概念很关键。现有监督方法获取标注成本高，本文提出无监督方法 Alignment and Disentanglement，在基准上表现超监督方法。


<details>
  <summary>Details</summary>
Motivation: 现有合成图像归因方法依赖有监督的标注对，获取成本高，因此探索无监督合成图像归因的可能性。

Method: 提出 Alignment and Disentanglement 无监督方法，先通过对比自监督学习进行基本概念对齐，再用 Infomax 损失促进表征解缠。

Result: 在真实世界基准 AbC 上，无监督方法表现优于有监督方法。

Conclusion: 研究的直观见解和实验结果为该挑战性任务提供了新视角。

Abstract: As the quality of synthetic images improves, identifying the underlying concepts of model-generated images is becoming increasingly crucial for copyright protection and ensuring model transparency. Existing methods achieve this attribution goal by training models using annotated pairs of synthetic images and their original training sources. However, obtaining such paired supervision is challenging, as it requires either well-designed synthetic concepts or precise annotations from millions of training sources. To eliminate the need for costly paired annotations, in this paper, we explore the possibility of unsupervised synthetic image attribution. We propose a simple yet effective unsupervised method called Alignment and Disentanglement. Specifically, we begin by performing basic concept alignment using contrastive self-supervised learning. Next, we enhance the model's attribution ability by promoting representation disentanglement with the Infomax loss. This approach is motivated by an interesting observation: contrastive self-supervised models, such as MoCo and DINO, inherently exhibit the ability to perform simple cross-domain alignment. By formulating this observation as a theoretical assumption on cross-covariance, we provide a theoretical explanation of how alignment and disentanglement can approximate the concept-matching process through a decomposition of the canonical correlation analysis objective. On the real-world benchmarks, AbC, we show that our unsupervised method surprisingly outperforms the supervised methods. As a starting point, we expect our intuitive insights and experimental findings to provide a fresh perspective on this challenging task.

</details>


### [391] [Fire on Motion: Optimizing Video Pass-bands for Efficient Spiking Action Recognition](https://arxiv.org/abs/2601.22675)
*Shuhan Ye,Yuanbin Qian,Yi Yu,Chong Wang,Yuqi Xie,Jiazhen Xu,Kun Wang,Xudong Jiang*

Main category: cs.CV

TL;DR: 针对SNN在动态视频任务表现不佳问题，发现通带不匹配现象，提出PBO优化模块，提升多种视频任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决SNN在动态视频任务中表现不如ANN的问题，因其标准脉冲动态存在通带不匹配，强调静态内容而抑制动态信息。

Method: 提出PBO模块，仅引入两个可学习参数和轻量级一致性约束，抑制静态成分，使脉冲活动集中在含运动信息内容上。

Result: 在UCF101上提升超10个百分点，在多模态动作识别和弱监督视频异常检测中也有显著稳定提升。

Conclusion: PBO模块为基于SNN的视频处理和理解提供新视角。

Abstract: Spiking neural networks (SNNs) have gained traction in vision due to their energy efficiency, bio-plausibility, and inherent temporal processing. Yet, despite this temporal capacity, most progress concentrates on static image benchmarks, and SNNs still underperform on dynamic video tasks compared to artificial neural networks (ANNs). In this work, we diagnose a fundamental pass-band mismatch: Standard spiking dynamics behave as a temporal low pass that emphasizes static content while attenuating motion bearing bands, where task relevant information concentrates in dynamic tasks. This phenomenon explains why SNNs can approach ANNs on static tasks yet fall behind on tasks that demand richer temporal understanding.To remedy this, we propose the Pass-Bands Optimizer (PBO), a plug-and-play module that optimizes the temporal pass-band toward task-relevant motion bands. PBO introduces only two learnable parameters, and a lightweight consistency constraint that preserves semantics and boundaries, incurring negligible computational overhead and requires no architectural changes. PBO deliberately suppresses static components that contribute little to discrimination, effectively high passing the stream so that spiking activity concentrates on motion bearing content. On UCF101, PBO yields over ten percentage points improvement. On more complex multi-modal action recognition and weakly supervised video anomaly detection, PBO delivers consistent and significant gains, offering a new perspective for SNN based video processing and understanding.

</details>


### [392] [PEAR: Pixel-aligned Expressive humAn mesh Recovery](https://arxiv.org/abs/2601.22693)
*Jiahao Wu,Yunfei Liu,Lijian Lin,Ye Zhu,Lei Zhu,Jingyi Li,Yu Li*

Main category: cs.CV

TL;DR: 提出PEAR框架用于像素对齐的富有表现力的人体网格恢复，解决现有方法的问题，实现快速准确的重建。


<details>
  <summary>Details</summary>
Motivation: 现有基于SMPLX的方法存在推理慢、姿态粗粒度、细粒度区域有问题等不足，难以应用于下游任务。

Method: 采用基于ViT的模型恢复粗3D人体几何，引入像素级监督优化几何，提出模块化数据标注策略丰富训练数据。

Result: PEAR可在无预处理下以超100 FPS同时推断EHM - s参数，在多个基准数据集上实验表明，相比之前基于SMPLX的方法，姿态估计精度有大幅提升。

Conclusion: PEAR是一种快速且鲁棒的像素对齐人体网格恢复框架，能有效解决现有方法的局限。

Abstract: Reconstructing detailed 3D human meshes from a single in-the-wild image remains a fundamental challenge in computer vision. Existing SMPLX-based methods often suffer from slow inference, produce only coarse body poses, and exhibit misalignments or unnatural artifacts in fine-grained regions such as the face and hands. These issues make current approaches difficult to apply to downstream tasks. To address these challenges, we propose PEAR-a fast and robust framework for pixel-aligned expressive human mesh recovery. PEAR explicitly tackles three major limitations of existing methods: slow inference, inaccurate localization of fine-grained human pose details, and insufficient facial expression capture. Specifically, to enable real-time SMPLX parameter inference, we depart from prior designs that rely on high resolution inputs or multi-branch architectures. Instead, we adopt a clean and unified ViT-based model capable of recovering coarse 3D human geometry. To compensate for the loss of fine-grained details caused by this simplified architecture, we introduce pixel-level supervision to optimize the geometry, significantly improving the reconstruction accuracy of fine-grained human details. To make this approach practical, we further propose a modular data annotation strategy that enriches the training data and enhances the robustness of the model. Overall, PEAR is a preprocessing-free framework that can simultaneously infer EHM-s (SMPLX and scaled-FLAME) parameters at over 100 FPS. Extensive experiments on multiple benchmark datasets demonstrate that our method achieves substantial improvements in pose estimation accuracy compared to previous SMPLX-based approaches. Project page: https://wujh2001.github.io/PEAR

</details>


### [393] [Gated Relational Alignment via Confidence-based Distillation for Efficient VLMs](https://arxiv.org/abs/2601.22709)
*Yanlong Chen,Amirhossein Habibian,Luca Benini,Yawei Li*

Main category: cs.CV

TL;DR: 提出GRACE框架用于视觉语言模型量化，结合知识蒸馏和QAT，在多基准测试中表现出色，提升吞吐量并减少内存占用。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型部署成本高，后训练量化会导致显著精度损失，量化感知训练研究不足。

Method: 在信息瓶颈原则下统一知识蒸馏和QAT，引入置信门控解耦蒸馏、关系中心核对齐和自适应控制器。

Result: INT4模型在多个基准测试中优于FP16基线，接近教师模型性能，使用真实INT4内核实现3倍吞吐量和54%内存减少。

Conclusion: GRACE框架显著优于现有量化方法，是资源受限部署的有力解决方案。

Abstract: Vision-Language Models (VLMs) achieve strong multimodal performance but are costly to deploy, and post-training quantization often causes significant accuracy loss. Despite its potential, quantization-aware training for VLMs remains underexplored. We propose GRACE, a framework unifying knowledge distillation and QAT under the Information Bottleneck principle: quantization constrains information capacity while distillation guides what to preserve within this budget. Treating the teacher as a proxy for task-relevant information, we introduce confidence-gated decoupled distillation to filter unreliable supervision, relational centered kernel alignment to transfer visual token structures, and an adaptive controller via Lagrangian relaxation to balance fidelity against capacity constraints. Across extensive benchmarks on LLaVA and Qwen families, our INT4 models consistently outperform FP16 baselines (e.g., LLaVA-1.5-7B: 70.1 vs. 66.8 on SQA; Qwen2-VL-2B: 76.9 vs. 72.6 on MMBench), nearly matching teacher performance. Using real INT4 kernel, we achieve 3$\times$ throughput with 54% memory reduction. This principled framework significantly outperforms existing quantization methods, making GRACE a compelling solution for resource-constrained deployment.

</details>


### [394] [OpenVTON-Bench: A Large-Scale High-Resolution Benchmark for Controllable Virtual Try-On Evaluation](https://arxiv.org/abs/2601.22725)
*Jin Li,Tao Chen,Shuai Jiang,Weijie Wang,Jingwen Luo,Chenhui Wu*

Main category: cs.CV

TL;DR: 现有VTON评估存在瓶颈，提出OpenVTON - Bench基准和多模态评估协议，实验结果与人类判断高度一致。


<details>
  <summary>Details</summary>
Motivation: 扩散模型提升VTON视觉保真度，但可靠评估仍是瓶颈，传统指标和现有数据集有缺陷。

Method: 构建含约100K高分辨率图像对的OpenVTON - Bench，用DINOv3聚类和Gemini密集标注；提出多模态评估协议，结合VLM语义推理和基于SAM3的多尺度表示指标。

Result: 实验结果与人类判断一致性高，Kendall's τ达0.833，高于SSIM的0.611。

Conclusion: 建立了可靠的VTON评估基准。

Abstract: Recent advances in diffusion models have significantly elevated the visual fidelity of Virtual Try-On (VTON) systems, yet reliable evaluation remains a persistent bottleneck. Traditional metrics struggle to quantify fine-grained texture details and semantic consistency, while existing datasets fail to meet commercial standards in scale and diversity. We present OpenVTON-Bench, a large-scale benchmark comprising approximately 100K high-resolution image pairs (up to $1536 \times 1536$). The dataset is constructed using DINOv3-based hierarchical clustering for semantically balanced sampling and Gemini-powered dense captioning, ensuring a uniform distribution across 20 fine-grained garment categories. To support reliable evaluation, we propose a multi-modal protocol that measures VTON quality along five interpretable dimensions: background consistency, identity fidelity, texture fidelity, shape plausibility, and overall realism. The protocol integrates VLM-based semantic reasoning with a novel Multi-Scale Representation Metric based on SAM3 segmentation and morphological erosion, enabling the separation of boundary alignment errors from internal texture artifacts. Experimental results show strong agreement with human judgments (Kendall's $τ$ of 0.833 vs. 0.611 for SSIM), establishing a robust benchmark for VTON evaluation.

</details>


### [395] [ImgCoT: Compressing Long Chain of Thought into Compact Visual Tokens for Efficient Reasoning of Large Language Model](https://arxiv.org/abs/2601.22730)
*Xiaoshu Chen,Sihang Zhou,Ke Liang,Taichun Zhou,Xinwang Liu*

Main category: cs.CV

TL;DR: 提出ImgCoT和loose ImgCoT方法压缩思维链，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有用自动编码器压缩思维链方法存在强语言归纳偏差，限制逻辑抽象。

Method: 提出ImgCoT将重建目标从文本思维链改为视觉思维链；提出loose ImgCoT用少量关键文本推理步骤增强视觉潜在标记。

Result: 在多个数据集和大语言模型上的广泛实验证明了两种版本ImgCoT的有效性。

Conclusion: ImgCoT和loose ImgCoT能让大语言模型以更少标记保留全局推理结构和细粒度推理细节。

Abstract: Compressing long chains of thought (CoT) into compact latent tokens is crucial for efficient reasoning with large language models (LLMs). Recent studies employ autoencoders to achieve this by reconstructing textual CoT from latent tokens, thus encoding CoT semantics. However, treating textual CoT as the reconstruction target forces latent tokens to preserve surface-level linguistic features (e.g., word choice and syntax), introducing a strong linguistic inductive bias that prioritizes linguistic form over reasoning structure and limits logical abstraction. Thus, we propose ImgCoT that replaces the reconstruction target from textual CoT to the visual CoT obtained by rendering CoT into images. This substitutes linguistic bias with spatial inductive bias, i.e., a tendency to model spatial layouts of the reasoning steps in visual CoT, enabling latent tokens to better capture global reasoning structure. Moreover, although visual latent tokens encode abstract reasoning structure, they may blur reasoning details. We thus propose a loose ImgCoT, a hybrid reasoning that augments visual latent tokens with a few key textual reasoning steps, selected based on low token log-likelihood. This design allows LLMs to retain both global reasoning structure and fine-grained reasoning details with fewer tokens than the complete CoT. Extensive experiments across multiple datasets and LLMs demonstrate the effectiveness of the two versions of ImgCoT.

</details>


### [396] [Procedural Knowledge Extraction from Industrial Troubleshooting Guides Using Vision Language Models](https://arxiv.org/abs/2601.22754)
*Guillermo Gil de Avalle,Laura Maruster,Christos Emmanouilidis*

Main category: cs.CV

TL;DR: 本文评估两种视觉语言模型从工业故障排除指南中提取结构化知识的能力，比较两种提示策略，揭示模型在布局敏感性和语义鲁棒性上的权衡。


<details>
  <summary>Details</summary>
Motivation: 将工业故障排除指南中的知识整合到操作员支持系统，手动提取信息劳动密集且易出错，而视觉语言模型自动化提取的性能未充分研究。

Method: 评估两种视觉语言模型，比较标准指令引导和提示故障排除布局模式的增强方法这两种提示策略。

Result: 发现不同模型在布局敏感性和语义鲁棒性方面有特定的权衡。

Conclusion: 研究结果为实际部署决策提供参考。

Abstract: Industrial troubleshooting guides encode diagnostic procedures in flowchart-like diagrams where spatial layout and technical language jointly convey meaning. To integrate this knowledge into operator support systems, which assist shop-floor personnel in diagnosing and resolving equipment issues, the information must first be extracted and structured for machine interpretation. However, when performed manually, this extraction is labor-intensive and error-prone. Vision Language Models offer potential to automate this process by jointly interpreting visual and textual meaning, yet their performance on such guides remains underexplored. This paper evaluates two VLMs on extracting structured knowledge, comparing two prompting strategies: standard instruction-guided versus an augmented approach that cues troubleshooting layout patterns. Results reveal model-specific trade-offs between layout sensitivity and semantic robustness, informing practical deployment decisions.

</details>


### [397] [DINO-SAE: DINO Spherical Autoencoder for High-Fidelity Image Reconstruction and Generation](https://arxiv.org/abs/2601.22904)
*Hun Chang,Byunghee Cha,Jong Chul Ye*

Main category: cs.CV

TL;DR: 提出DINO - SAE框架解决现有基于预训练视觉基础模型的生成自编码器重建保真度有限问题，实验取得SOTA重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于预训练视觉基础模型的生成自编码器因高频细节丢失导致重建保真度有限。

Method: 引入Hierarchical Convolutional Patch Embedding模块增强局部结构和纹理保存，采用Cosine Similarity Alignment目标保证语义一致性并保留细节，利用Riemannian Flow Matching在球形潜在流形上训练Diffusion Transformer。

Result: 在ImageNet - 1K实验中达到SOTA重建质量，rFID为0.37、PSNR为26.2 dB，基于Riemannian Flow Matching的DiT收敛高效，80个epoch时gFID为3.47。

Conclusion: DINO - SAE框架能有效解决重建保真度问题，同时保持与预训练VFM的强语义对齐。

Abstract: Recent studies have explored using pretrained Vision Foundation Models (VFMs) such as DINO for generative autoencoders, showing strong generative performance. Unfortunately, existing approaches often suffer from limited reconstruction fidelity due to the loss of high-frequency details. In this work, we present the DINO Spherical Autoencoder (DINO-SAE), a framework that bridges semantic representation and pixel-level reconstruction. Our key insight is that semantic information in contrastive representations is primarily encoded in the direction of feature vectors, while forcing strict magnitude matching can hinder the encoder from preserving fine-grained details. To address this, we introduce Hierarchical Convolutional Patch Embedding module that enhances local structure and texture preservation, and Cosine Similarity Alignment objective that enforces semantic consistency while allowing flexible feature magnitudes for detail retention. Furthermore, leveraging the observation that SSL-based foundation model representations intrinsically lie on a hypersphere, we employ Riemannian Flow Matching to train a Diffusion Transformer (DiT) directly on this spherical latent manifold. Experiments on ImageNet-1K demonstrate that our approach achieves state-of-the-art reconstruction quality, reaching 0.37 rFID and 26.2 dB PSNR, while maintaining strong semantic alignment to the pretrained VFM. Notably, our Riemannian Flow Matching-based DiT exhibits efficient convergence, achieving a gFID of 3.47 at 80 epochs.

</details>


### [398] [About an Automating Annotation Method for Robot Markers](https://arxiv.org/abs/2601.22982)
*Wataru Uemura,Takeru Nagashima*

Main category: cs.CV

TL;DR: 因劳动力短缺，工厂自动化中自主移动机器人使用增多，传统标记识别方法有局限，本文提出基于ArUco标记图像的深度学习模型自动标注方法，实验显示其性能更优且减少人力。


<details>
  <summary>Details</summary>
Motivation: 传统基于OpenCV的标记识别方法在噪声、运动模糊等条件下易失败，深度学习方法需大量手动标注数据，数据集准备成瓶颈，因此提出自动标注方法。

Method: 利用ArUco模块的标记检测结果进行自动标注，用自动标注的数据集训练基于YOLO的模型，并在不同条件下评估其性能。

Result: 与传统图像处理技术相比，所提方法提高了识别性能，尤其对模糊或失焦图像，且自动标注减少人力并保证标注质量一致。

Conclusion: 所提自动标注方法有效，未来将研究置信阈值与识别性能的关系。

Abstract: Factory automation has become increasingly important due to labor shortages, leading to the introduction of autonomous mobile robots for tasks such as material transportation. Markers are commonly used for robot self-localization and object identification. In the RoboCup Logistics League (RCLL), ArUco markers are employed both for robot localization and for identifying processing modules. Conventional recognition relies on OpenCV-based image processing, which detects black-and-white marker patterns. However, these methods often fail under noise, motion blur, defocus, or varying illumination conditions. Deep-learning-based recognition offers improved robustness under such conditions, but requires large amounts of annotated data. Annotation must typically be done manually, as the type and position of objects cannot be detected automatically, making dataset preparation a major bottleneck. In contrast, ArUco markers include built-in recognition modules that provide both ID and positional information, enabling automatic annotation. This paper proposes an automated annotation method for training deep-learning models on ArUco marker images. By leveraging marker detection results obtained from the ArUco module, the proposed approach eliminates the need for manual labeling. A YOLO-based model is trained using the automatically annotated dataset, and its performance is evaluated under various conditions. Experimental results demonstrate that the proposed method improves recognition performance compared with conventional image-processing techniques, particularly for images affected by blur or defocus. Automatic annotation also reduces human effort and ensures consistent labeling quality. Future work will investigate the relationship between confidence thresholds and recognition performance.

</details>


### [399] [Self-Supervised Slice-to-Volume Reconstruction with Gaussian Representations for Fetal MRI](https://arxiv.org/abs/2601.22990)
*Yinsong Wang,Thomas Fletcher,Xinzhe Luo,Aine Travers Dineen,Rhodri Cusack,Chen Qin*

Main category: cs.CV

TL;DR: 提出用于胎儿MR切片到体积重建的自监督框架GaussianSVR，表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统切片到体积重建（SVR）方法耗时且需多正交堆栈，基于学习的SVR方法依赖难以获取的真实信息训练。

Method: 提出GaussianSVR框架，用3D高斯表示目标体积，利用模拟前向切片采集模型进行自监督训练，引入多分辨率训练策略优化参数和变换。

Result: 实验表明GaussianSVR在胎儿MR体积重建上优于基线方法。

Conclusion: GaussianSVR能有效解决现有SVR方法的问题，提高重建性能。

Abstract: Reconstructing 3D fetal MR volumes from motion-corrupted stacks of 2D slices is a crucial and challenging task. Conventional slice-to-volume reconstruction (SVR) methods are time-consuming and require multiple orthogonal stacks for reconstruction. While learning-based SVR approaches have significantly reduced the time required at the inference stage, they heavily rely on ground truth information for training, which is inaccessible in practice. To address these challenges, we propose GaussianSVR, a self-supervised framework for slice-to-volume reconstruction. GaussianSVR represents the target volume using 3D Gaussian representations to achieve high-fidelity reconstruction. It leverages a simulated forward slice acquisition model to enable self-supervised training, alleviating the need for ground-truth volumes. Furthermore, to enhance both accuracy and efficiency, we introduce a multi-resolution training strategy that jointly optimizes Gaussian parameters and spatial transformations across different resolution levels. Experiments show that GaussianSVR outperforms the baseline methods on fetal MR volumetric reconstruction. Code will be available upon acceptance.

</details>


### [400] [Do Open-Vocabulary Detectors Transfer to Aerial Imagery? A Comparative Evaluation](https://arxiv.org/abs/2601.22164)
*Christos Tsourveloudis*

Main category: cs.CV

TL;DR: 对五种OVD模型在LAE - 80C航拍数据集上进行零样本评估，发现领域迁移失败，语义混淆是瓶颈，需领域自适应方法。


<details>
  <summary>Details</summary>
Motivation: 探索开放词汇目标检测（OVD）模型在航拍图像上的迁移能力。

Method: 在LAE - 80C航拍数据集上对五种OVD模型进行严格零样本评估，通过Global、Oracle和Single - Category推理模式分离语义混淆和视觉定位。

Result: 最佳模型F1分数仅27.6%，误报率69%；减少词汇量有15倍提升；提示工程策略无显著效果；不同数据集性能差异大。

Conclusion: 建立了基线预期，强调航拍OVD需要领域自适应方法。

Abstract: Open-vocabulary object detection (OVD) enables zero-shot recognition of novel categories through vision-language models, achieving strong performance on natural images. However, transferability to aerial imagery remains unexplored. We present the first systematic benchmark evaluating five state-of-the-art OVD models on the LAE-80C aerial dataset (3,592 images, 80 categories) under strict zero-shot conditions. Our experimental protocol isolates semantic confusion from visual localization through Global, Oracle, and Single-Category inference modes. Results reveal severe domain transfer failure: the best model (OWLv2) achieves only 27.6% F1-score with 69% false positive rate. Critically, reducing vocabulary size from 80 to 3.2 classes yields 15x improvement, demonstrating that semantic confusion is the primary bottleneck. Prompt engineering strategies such as domain-specific prefixing and synonym expansion, fail to provide meaningful performance gains. Performance varies dramatically across datasets (F1: 0.53 on DIOR, 0.12 on FAIR1M), exposing brittleness to imaging conditions. These findings establish baseline expectations and highlight the need for domain-adaptive approaches in aerial OVD.

</details>


### [401] [HierLoc: Hyperbolic Entity Embeddings for Hierarchical Visual Geolocation](https://arxiv.org/abs/2601.23064)
*Hari Krishna Gadi,Daniel Matos,Hongyi Luo,Lu Liu,Yongliang Wang,Yanfeng Zhang,Liqiu Meng*

Main category: cs.CV

TL;DR: 提出以实体为中心的地理定位公式，用地理实体层次结构替代图像检索，在OSV5M基准上表现出色，减少误差并提高准确率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉地理定位范式存在存储量大、忽略地理连续性、细节处理差等问题。

Method: 引入以实体为中心的地理定位公式，通过Geo - Weighted Hyperbolic对比学习将图像与地理实体对齐。

Result: 在OSV5M基准上建立新的最优性能，减少19.5%的平均测地误差，提高43%的细粒度子区域准确率。

Conclusion: 几何感知的分层嵌入为全球图像地理定位提供了可扩展且概念新颖的替代方案。

Abstract: Visual geolocalization, the task of predicting where an image was taken, remains challenging due to global scale, visual ambiguity, and the inherently hierarchical structure of geography. Existing paradigms rely on either large-scale retrieval, which requires storing a large number of image embeddings, grid-based classifiers that ignore geographic continuity, or generative models that diffuse over space but struggle with fine detail. We introduce an entity-centric formulation of geolocation that replaces image-to-image retrieval with a compact hierarchy of geographic entities embedded in Hyperbolic space. Images are aligned directly to country, region, subregion, and city entities through Geo-Weighted Hyperbolic contrastive learning by directly incorporating haversine distance into the contrastive objective. This hierarchical design enables interpretable predictions and efficient inference with 240k entity embeddings instead of over 5 million image embeddings on the OSV5M benchmark, on which our method establishes a new state-of-the-art performance. Compared to the current methods in the literature, it reduces mean geodesic error by 19.5\%, while improving the fine-grained subregion accuracy by 43%. These results demonstrate that geometry-aware hierarchical embeddings provide a scalable and conceptually new alternative for global image geolocation.

</details>


### [402] [Is Hierarchical Quantization Essential for Optimal Reconstruction?](https://arxiv.org/abs/2601.22244)
*Shirin Reyhanian,Laurenz Wiskott*

Main category: cs.CV

TL;DR: 研究单级VQ - VAE在匹配表示预算且无码本崩溃时能否达到多级VQ - VAE的重建保真度，发现单级模型可达到相同效果。


<details>
  <summary>Details</summary>
Motivation: 探讨单级VQ - VAE在匹配表示预算和无码本崩溃情况下，能否达到多级VQ - VAE的重建保真度，且多级结构对重建精度的影响缺乏实证研究。

Method: 在高分辨率ImageNet图像上比较两级VQ - VAE和容量匹配的单级模型，采用从数据初始化、定期重置非活动码本向量和系统调整码本超参数等干预措施。

Result: 确认码本利用不足限制单级VQ - VAE，高维嵌入会使量化不稳定和增加码本崩溃；轻量级干预措施可显著减少崩溃；单级VQ - VAE能达到多级模型的重建保真度。

Conclusion: 当表示预算匹配且码本崩溃得到缓解时，单级VQ - VAE可达到多级VQ - VAE的重建保真度，挑战了多级量化在高质量重建中固有的优越性假设。

Abstract: Vector-quantized variational autoencoders (VQ-VAEs) are central to models that rely on high reconstruction fidelity, from neural compression to generative pipelines. Hierarchical extensions, such as VQ-VAE2, are often credited with superior reconstruction performance because they split global and local features across multiple levels. However, since higher levels derive all their information from lower levels, they should not carry additional reconstructive content beyond what the lower-level already encodes. Combined with recent advances in training objectives and quantization mechanisms, this leads us to ask whether a single-level VQ-VAE, with matched representational budget and no codebook collapse, can equal the reconstruction fidelity of its hierarchical counterpart. Although the multi-scale structure of hierarchical models may improve perceptual quality in downstream tasks, the effect of hierarchy on reconstruction accuracy, isolated from codebook utilization and overall representational capacity, remains empirically underexamined. We revisit this question by comparing a two-level VQ-VAE and a capacity-matched single-level model on high-resolution ImageNet images. Consistent with prior observations, we confirm that inadequate codebook utilization limits single-level VQ-VAEs and that overly high-dimensional embeddings destabilize quantization and increase codebook collapse. We show that lightweight interventions such as initialization from data, periodic reset of inactive codebook vectors, and systematic tuning of codebook hyperparameters significantly reduce collapse. Our results demonstrate that when representational budgets are matched, and codebook collapse is mitigated, single-level VQ-VAEs can match the reconstruction fidelity of hierarchical variants, challenging the assumption that hierarchical quantization is inherently superior for high-quality reconstructions.

</details>


### [403] [Med-Scout: Curing MLLMs' Geometric Blindness in Medical Perception via Geometry-Aware RL Post-Training](https://arxiv.org/abs/2601.23220)
*Anglin Liu,Ruichao Chen,Yi Lu,Hongxia Xu,Jintai Chen*

Main category: cs.CV

TL;DR: 现有多模态大语言模型在医疗诊断中存在几何感知缺陷，本文提出Med - Scout框架利用强化学习解决该问题，还推出Med - Scout - Bench评估基准，评估显示其显著减轻几何盲目性且泛化效果好。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的多模态大语言模型在医疗诊断中存在几何盲目性问题，训练范式重语言流畅性轻几何保真度，导致事实错误的幻觉。

Method: 引入Med - Scout框架，通过强化学习利用无标签医学图像中的内在几何逻辑，通过三个代理任务获取监督信号；推出Med - Scout - Bench基准评估几何感知。

Result: Med - Scout显著减轻几何盲目性，在基准测试中比领先模型性能高出40%以上，在放射学和综合医学VQA任务上取得更好结果。

Conclusion: Med - Scout框架有效解决多模态大语言模型的几何盲目性问题，增强的几何感知可泛化到更广泛的医学理解中。

Abstract: Despite recent Multimodal Large Language Models (MLLMs)' linguistic prowess in medical diagnosis, we find even state-of-the-art MLLMs suffer from a critical perceptual deficit: geometric blindness. This failure to ground outputs in objective geometric constraints leads to plausible yet factually incorrect hallucinations, rooted in training paradigms that prioritize linguistic fluency over geometric fidelity. This paper introduces Med-Scout, a novel framework that "cures" this blindness via Reinforcement Learning (RL) that leverages the intrinsic geometric logic latent within unlabeled medical images. Instead of relying on costly expert annotations, Med-Scout derives verifiable supervision signals through three strategic proxy tasks: Hierarchical Scale Localization, Topological Jigsaw Reconstruction, and Anomaly Consistency Detection. To rigorously quantify this deficit, we present Med-Scout-Bench, a new benchmark specifically designed to evaluate geometric perception. Extensive evaluations show that Med-Scout significantly mitigates geometric blindness, outperforming leading proprietary and open-source MLLMs by over 40% on our benchmark. Furthermore, this enhanced geometric perception generalizes to broader medical understanding, achieving superior results on radiological and comprehensive medical VQA tasks.

</details>


### [404] [ShotFinder: Imagination-Driven Open-Domain Video Shot Retrieval via Web Search](https://arxiv.org/abs/2601.23232)
*Tao Yu,Haopeng Jin,Hao Wang,Shenghua Chai,Yujia Yang,Junhao Gong,Jiaming Guo,Minghui Zhang,Xinlong Chen,Zhenghao Zhang,Yuxuan Zhou,Yanpei Gong,YuanCheng Liu,Yiming Ding,Kangwei Zeng,Pengfei Yang,Zhongtian Luo,Yufei Xiong,Shanbin Zhang,Shaoxiong Cheng,Huang Ruilin,Li Shuo,Yuxi Niu,Xinyuan Zhang,Yueya Xu,Jie Mao,Ruixuan Ji,Yaru Zhao,Mingchen Zhang,Jiabing Yang,Jiaqi Liu,YiFan Zhang,Hongzhu Yi,Xinming Wang,Cheng Zhong,Xiao Ma,Zhang Zhang,Yan Huang,Liang Wang*

Main category: cs.CV

TL;DR: 现有大语言模型在信息检索上有进展，但开放域视频片段检索缺乏基准。引入ShotFinder基准和检索定位管道，实验显示模型与人类表现差距大，视频片段检索仍是挑战。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注文本或静态多模态，开放域视频片段检索缺乏系统基准和分析。

Method: 引入ShotFinder基准，提出文本驱动的三阶段检索和定位管道：通过视频想象进行查询扩展、用搜索引擎检索候选视频、描述引导的时间定位。

Result: 多模型实验显示与人类表现有显著差距，各约束存在不平衡，时间定位较易，颜色和视觉风格是主要挑战。

Conclusion: 开放域视频片段检索仍是多模态大模型待克服的关键能力。

Abstract: In recent years, large language models (LLMs) have made rapid progress in information retrieval, yet existing research has mainly focused on text or static multimodal settings. Open-domain video shot retrieval, which involves richer temporal structure and more complex semantics, still lacks systematic benchmarks and analysis. To fill this gap, we introduce ShotFinder, a benchmark that formalizes editing requirements as keyframe-oriented shot descriptions and introduces five types of controllable single-factor constraints: Temporal order, Color, Visual style, Audio, and Resolution. We curate 1,210 high-quality samples from YouTube across 20 thematic categories, using large models for generation with human verification. Based on the benchmark, we propose ShotFinder, a text-driven three-stage retrieval and localization pipeline: (1) query expansion via video imagination, (2) candidate video retrieval with a search engine, and (3) description-guided temporal localization. Experiments on multiple closed-source and open-source models reveal a significant gap to human performance, with clear imbalance across constraints: temporal localization is relatively tractable, while color and visual style remain major challenges. These results reveal that open-domain video shot retrieval is still a critical capability that multimodal large models have yet to overcome.

</details>


### [405] [Leveraging Data to Say No: Memory Augmented Plug-and-Play Selective Prediction](https://arxiv.org/abs/2601.22570)
*Aditya Sarkar,Yi Li,Jiacheng Cheng,Shlok Mishra,Nuno Vasconcelos*

Main category: cs.CV

TL;DR: 本文提出MA - PaPSP模型用于视觉语言基础模型的选择性预测，在多任务上优于基线方法，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有选择性预测研究主要集中在封闭集任务，本文考虑视觉语言基础模型在多种任务上的选择性预测，并寻求低复杂度、无训练的方法。

Method: 提出MA - PaPSP模型，通过图像 - 文本对的检索数据集增强PaPSP，用平均最近邻对减少嵌入方差，用对比归一化改善分数校准。

Result: 在多个数据集的选择性字幕、图像 - 文本匹配和细粒度分类任务中，MA - PaPSP优于PaPSP和其他选择性预测基线。

Conclusion: MA - PaPSP模型能有效解决视觉语言表示不稳定和相似度分数校准不佳的问题，可用于视觉语言基础模型的选择性预测。

Abstract: Selective prediction aims to endow predictors with a reject option, to avoid low confidence predictions. However, existing literature has primarily focused on closed-set tasks, such as visual question answering with predefined options or fixed-category classification. This paper considers selective prediction for visual language foundation models, addressing a taxonomy of tasks ranging from closed to open set and from finite to unbounded vocabularies, as in image captioning. We seek training-free approaches of low-complexity, applicable to any foundation model and consider methods based on external vision-language model embeddings, like CLIP. This is denoted as Plug-and-Play Selective Prediction (PaPSP). We identify two key challenges: (1) instability of the visual-language representations, leading to high variance in image-text embeddings, and (2) poor calibration of similarity scores. To address these issues, we propose a memory augmented PaPSP (MA-PaPSP) model, which augments PaPSP with a retrieval dataset of image-text pairs. This is leveraged to reduce embedding variance by averaging retrieved nearest-neighbor pairs and is complemented by the use of contrastive normalization to improve score calibration. Through extensive experiments on multiple datasets, we show that MA-PaPSP outperforms PaPSP and other selective prediction baselines for selective captioning, image-text matching, and fine-grained classification. Code is publicly available at https://github.com/kingston-aditya/MA-PaPSP.

</details>


### [406] [VideoGPA: Distilling Geometry Priors for 3D-Consistent Video Generation](https://arxiv.org/abs/2601.23286)
*Hongyang Du,Junjie Ye,Xiaoyan Cong,Runhao Li,Jingcheng Ni,Aman Agarwal,Zeqi Zhou,Zekun Li,Randall Balestriero,Yue Wang*

Main category: cs.CV

TL;DR: 现有视频扩散模型难保持3D结构一致性，提出VideoGPA框架解决该问题，提升视频质量并超越现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型难以保持3D结构一致性，标准去噪目标缺乏对几何一致性的明确激励。

Method: 引入VideoGPA框架，利用几何基础模型自动导出密集偏好信号，通过直接偏好优化（DPO）引导视频扩散模型。

Result: 使用最少的偏好对显著增强了时间稳定性、物理合理性和运动连贯性，在大量实验中持续超越最先进的基线。

Conclusion: VideoGPA能有效引导生成分布实现3D一致性，且无需人工标注，是一种数据高效的自监督框架。

Abstract: While recent video diffusion models (VDMs) produce visually impressive results, they fundamentally struggle to maintain 3D structural consistency, often resulting in object deformation or spatial drift. We hypothesize that these failures arise because standard denoising objectives lack explicit incentives for geometric coherence. To address this, we introduce VideoGPA (Video Geometric Preference Alignment), a data-efficient self-supervised framework that leverages a geometry foundation model to automatically derive dense preference signals that guide VDMs via Direct Preference Optimization (DPO). This approach effectively steers the generative distribution toward inherent 3D consistency without requiring human annotations. VideoGPA significantly enhances temporal stability, physical plausibility, and motion coherence using minimal preference pairs, consistently outperforming state-of-the-art baselines in extensive experiments.

</details>


### [407] [Bi-MCQ: Reformulating Vision-Language Alignment for Negation Understanding](https://arxiv.org/abs/2601.22696)
*Tae Hun Kim,Hyun Gyu Lee*

Main category: cs.CV

TL;DR: 现有的视觉语言模型在理解否定临床陈述方面较弱，本文提出双向多项选择学习框架Bi - MCQ解决该问题，实验显示其能提升对否定陈述的理解。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在医疗图像分析中理解否定临床陈述能力弱，对比对齐目标把否定视为次要语言变体，提示式InfoNCE微调限制了对疾病缺失情况的有效学习。

Method: 将视觉语言对齐重新表述为条件语义比较问题，通过双向多项选择学习框架Bi - MCQ实现，联合训练图像到文本和文本到图像多项选择任务，引入特定方向的交叉注意力融合模块。

Result: 在多个数据集上，Bi - MCQ相比零样本SOTA模型CARZero，提升了否定理解能力，在正负组合评价中有绝对增益，减少了肯定 - 否定AUC差距。

Conclusion: 目标重新表述能显著增强医学视觉语言模型对否定陈述的理解。

Abstract: Recent vision-language models (VLMs) achieve strong zero-shot performance via large-scale image-text pretraining and have been widely adopted in medical image analysis. However, existing VLMs remain notably weak at understanding negated clinical statements, largely due to contrastive alignment objectives that treat negation as a minor linguistic variation rather than a meaning-inverting operator. In multi-label settings, prompt-based InfoNCE fine-tuning further reinforces easy-positive image-prompt alignments, limiting effective learning of disease absence. To overcome these limitations, we reformulate vision-language alignment as a conditional semantic comparison problem, which is instantiated through a bi-directional multiple-choice learning framework(Bi-MCQ). By jointly training Image-to-Text and Text-to-Image MCQ tasks with affirmative, negative, and mixed prompts, our method implements fine-tuning as conditional semantic comparison instead of global similarity maximization. We further introduce direction-specific Cross-Attention fusion modules to address asymmetric cues required by bi-directional reasoning and reduce alignment interference. Experiments on ChestXray14, Open-I, CheXpert, and PadChest show that Bi-MCQ improves negation understanding by up to 0.47 AUC over the zero-shot performance of the state-of-the-art CARZero model, while achieving up to a 0.08 absolute gain on positive-negative combined (PNC) evaluation. Additionally, Bi-MCQ reduces the affirmative-negative AUC gap by an average of 0.12 compared to InfoNCE-based fine-tuning, demonstrating that objective reformulation can substantially enhance negation understanding in medical VLMs.

</details>


### [408] [Beauty and the Beast: Imperceptible Perturbations Against Diffusion-Based Face Swapping via Directional Attribute Editing](https://arxiv.org/abs/2601.22744)
*Yilong Huang,Songze Li*

Main category: cs.CV

TL;DR: 提出FaceDefense框架对抗扩散式换脸攻击，实验表明该框架在不可感知性和防御效果上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 基于扩散的换脸技术发展带来潜在危害，且现有防御方法存在大型扰动扭曲面部结构、小型扰动保护效果弱的权衡问题，需开发新的主动防御方法。

Method: 提出FaceDefense框架，引入新的扩散损失增强对抗样本的防御效果，采用定向面部属性编辑恢复扰动引起的扭曲，设计两阶段交替优化策略生成最终扰动人脸图像。

Result: FaceDefense在不可感知性和防御效果上显著优于现有方法。

Conclusion: FaceDefense能在不可感知性和防御效果之间取得更好的权衡。

Abstract: Diffusion-based face swapping achieves state-of-the-art performance, yet it also exacerbates the potential harm of malicious face swapping to violate portraiture right or undermine personal reputation. This has spurred the development of proactive defense methods. However, existing approaches face a core trade-off: large perturbations distort facial structures, while small ones weaken protection effectiveness. To address these issues, we propose FaceDefense, an enhanced proactive defense framework against diffusion-based face swapping. Our method introduces a new diffusion loss to strengthen the defensive efficacy of adversarial examples, and employs a directional facial attribute editing to restore perturbation-induced distortions, thereby enhancing visual imperceptibility. A two-phase alternating optimization strategy is designed to generate final perturbed face images. Extensive experiments show that FaceDefense significantly outperforms existing methods in both imperceptibility and defense effectiveness, achieving a superior trade-off.

</details>


### [409] [When Anomalies Depend on Context: Learning Conditional Compatibility for Anomaly Detection](https://arxiv.org/abs/2601.22868)
*Shashank Mishra,Didier Stricker,Jason Rambach*

Main category: cs.CV

TL;DR: 本文重新审视上下文异常检测，引入CAAD - 3K基准，提出条件兼容性学习框架，在多个数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统异常检测假设异常是观察值的固有属性，与上下文无关，但在现实中很多情况并非如此，需研究上下文相关的异常检测。

Method: 引入CAAD - 3K基准，提出条件兼容性学习框架，利用视觉 - 语言表示在有限监督下建模主体 - 上下文关系。

Result: 在CAAD - 3K上大幅超越现有方法，在MVTec - AD和VisA上达到了最先进水平。

Conclusion: 对上下文依赖进行建模可补充传统的结构异常检测。

Abstract: Anomaly detection is often formulated under the assumption that abnormality is an intrinsic property of an observation, independent of context. This assumption breaks down in many real-world settings, where the same object or action may be normal or anomalous depending on latent contextual factors (e.g., running on a track versus on a highway). We revisit \emph{contextual anomaly detection}, classically defined as context-dependent abnormality, and operationalize it in the visual domain, where anomaly labels depend on subject--context compatibility rather than intrinsic appearance. To enable systematic study of this setting, we introduce CAAD-3K, a benchmark that isolates contextual anomalies by controlling subject identity while varying context. We further propose a conditional compatibility learning framework that leverages vision--language representations to model subject--context relationships under limited supervision. Our method substantially outperforms existing approaches on CAAD-3K and achieves state-of-the-art performance on MVTec-AD and VisA, demonstrating that modeling context dependence complements traditional structural anomaly detection. Our code and dataset will be publicly released.

</details>


### [410] [Training-Free Test-Time Adaptation with Brownian Distance Covariance in Vision-Language Models](https://arxiv.org/abs/2601.23253)
*Yi Zhang,Chun-Wun Cheng,Angelica I. Aviles-Rivero,Zhihai He,Liang-Jie Zhang*

Main category: cs.CV

TL;DR: 提出无训练的测试时间自适应方法TaTa，利用布朗距离协方差和属性增强提示，降低计算成本并提升领域泛化性能。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型在领域偏移下性能下降，以及现有测试时间自适应方法计算密集、依赖反向传播和单模态的问题。

Method: 提出TaTa方法，利用布朗距离协方差动态适配模型，集成属性增强提示，结合动态聚类和伪标签细化。

Result: 在不同数据集实验中，显著降低计算成本，实现领域和跨数据集泛化的最优性能。

Conclusion: TaTa方法有效，能在不训练和反向传播的情况下提升视觉语言模型在新领域的性能。

Abstract: Vision-language models suffer performance degradation under domain shift, limiting real-world applicability. Existing test-time adaptation methods are computationally intensive, rely on back-propagation, and often focus on single modalities. To address these issues, we propose Training-free Test-Time Adaptation with Brownian Distance Covariance (TaTa). TaTa leverages Brownian Distance Covariance-a powerful statistical measure that captures both linear and nonlinear dependencies via pairwise distances-to dynamically adapt VLMs to new domains without training or back-propagation. This not only improves efficiency but also enhances stability by avoiding disruptive weight updates. TaTa further integrates attribute-enhanced prompting to improve vision-language inference with descriptive visual cues. Combined with dynamic clustering and pseudo-label refinement, it effectively recalibrates the model for novel visual contexts. Experiments across diverse datasets show that TaTa significantly reduces computational cost while achieving state-of-the-art performance in domain and cross-dataset generalization.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [411] [Training Beyond Convergence: Grokking nnU-Net for Glioma Segmentation in Sub-Saharan MRI](https://arxiv.org/abs/2601.22637)
*Mohtady Barakat,Omar Salah,Ahmed Yasser,Mostafa Ahmed,Zahirul Arief,Waleed Khan,Dong Zhang,Aondona Iorumbur,Confidence Raymond,Mohannad Barakat,Noha Magdy*

Main category: eess.IV

TL;DR: 本文利用BraTS Africa 2025挑战赛数据集，用nnUNet建立基线，探索触发“grokking”现象提升性能，评估两种训练方案并取得较好Dice分数。


<details>
  <summary>Details</summary>
Motivation: 撒哈拉以南非洲地区神经胶质瘤临床负担重、患者中位生存期短且诊断成像获取受限，急需基于本地数据训练的自动化工具。

Method: 利用BraTS Africa 2025挑战赛数据集，用nnUNet建立基线，评估两种训练方案，一是限制优化轮数，二是延长训练至收敛后触发“grokking”。

Result: 第一种方案nnUNet取得较好Dice分数，第二种方案触发“grokking”后Dice分数进一步提升。

Conclusion: 基于本地数据的nnUNet模型及训练方案在神经胶质瘤图像分割中表现良好，触发“grokking”可提升性能。

Abstract: Gliomas are placing an increasingly clinical burden on Sub-Saharan Africa (SSA). In the region, the median survival for patients remains under two years, and access to diagnostic imaging is extremely limited. These constraints highlight an urgent need for automated tools that can extract the maximum possible information from each available scan, tools that are specifically trained on local data, rather than adapted from high-income settings where conditions are vastly different. We utilize the Brain Tumor Segmentation (BraTS) Africa 2025 Challenge dataset, an expert annotated collection of glioma MRIs. Our objectives are: (i) establish a strong baseline with nnUNet on this dataset, and (ii) explore whether the celebrated "grokking" phenomenon an abrupt, late training jump from memorization to superior generalization can be triggered to push performance without extra labels. We evaluate two training regimes. The first is a fast, budget-conscious approach that limits optimization to just a few epochs, reflecting the constrained GPU resources typically available in African institutions. Despite this limitation, nnUNet achieves strong Dice scores: 92.3% for whole tumor (WH), 86.6% for tumor core (TC), and 86.3% for enhancing tumor (ET). The second regime extends training well beyond the point of convergence, aiming to trigger a grokking-driven performance leap. With this approach, we were able to achieve grokking and enhanced our results to higher Dice scores: 92.2% for whole tumor (WH), 90.1% for tumor core (TC), and 90.2% for enhancing tumor (ET).

</details>


### [412] [SCENE: Semantic-aware Codec Enhancement with Neural Embeddings](https://arxiv.org/abs/2601.22189)
*Han-Yu Lin,Li-Wei Chen,Hung-Shin Lee*

Main category: eess.IV

TL;DR: 提出轻量级语义感知预处理框架SCENE，增强压缩视频感知保真度，实验表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 标准视频编解码器的压缩伪影降低了感知质量，需要提升视频质量。

Method: 将视觉语言模型的语义嵌入集成到高效卷积架构，用可微编解码器代理端到端训练模型，推理时丢弃代理。

Result: 在高分辨率基准测试中，客观（MS - SSIM）和感知（VMAF）指标上优于基线，突出区域纹理保存更好。

Conclusion: 语义引导、编解码器感知的预处理是提高压缩视频流的有效方法。

Abstract: Compression artifacts from standard video codecs often degrade perceptual quality. We propose a lightweight, semantic-aware pre-processing framework that enhances perceptual fidelity by selectively addressing these distortions. Our method integrates semantic embeddings from a vision-language model into an efficient convolutional architecture, prioritizing the preservation of perceptually significant structures. The model is trained end-to-end with a differentiable codec proxy, enabling it to mitigate artifacts from various standard codecs without modifying the existing video pipeline. During inference, the codec proxy is discarded, and SCENE operates as a standalone pre-processor, enabling real-time performance. Experiments on high-resolution benchmarks show improved performance over baselines in both objective (MS-SSIM) and perceptual (VMAF) metrics, with notable gains in preserving detailed textures within salient regions. Our results show that semantic-guided, codec-aware pre-processing is an effective approach for enhancing compressed video streams.

</details>


### [413] [Compressed BC-LISTA via Low-Rank Convolutional Decomposition](https://arxiv.org/abs/2601.23148)
*Han Wang,Yhonatan Kvich,Eduardo Pérez,Florian Römer,Yonina C. Eldar*

Main category: eess.IV

TL;DR: 研究多通道成像的稀疏信号恢复方法，提出C - BC测量模型并结合OMP选择基滤波器，C - BC - LISTA在超声成像中参数少、模型小且重建精度高，OMP初始化性能最佳。


<details>
  <summary>Details</summary>
Motivation: 研究用于多通道成像的稀疏信号恢复方法，使压缩的前后向算子能保持重建精度。

Method: 提出基于低秩CNN分解的压缩块卷积（C - BC）测量模型，用正交匹配追踪（OMP）选择基滤波器并计算线性混合系数，给出C - BC - LISTA扩展。

Result: 在多信噪比的模拟多通道超声成像中，C - BC - LISTA比其他SOTA方法参数更少、模型更小且重建精度更高，OMP初始化结构化压缩表现最佳。

Conclusion: 所提方法在多通道成像的稀疏信号恢复中具有优势，OMP初始化是较好选择。

Abstract: We study Sparse Signal Recovery (SSR) methods for multichannel imaging with compressed {forward and backward} operators that preserve reconstruction accuracy. We propose a Compressed Block-Convolutional (C-BC) measurement model based on a low-rank Convolutional Neural Network (CNN) decomposition that is analytically initialized from a low-rank factorization of physics-derived forward/backward operators in time delay-based measurements. We use Orthogonal Matching Pursuit (OMP) to select a compact set of basis filters from the analytic model and compute linear mixing coefficients to approximate the full model. We consider the Learned Iterative Shrinkage-Thresholding Algorithm (LISTA) network as a representative example for which the C-BC-LISTA extension is presented. In simulated multichannel ultrasound imaging across multiple Signal-to-Noise Ratios (SNRs), C-BC-LISTA requires substantially fewer parameters and smaller model size than other state-of-the-art (SOTA) methods while improving reconstruction accuracy. In ablations over OMP, Singular Value Decomposition (SVD)-based, and random initializations, OMP-initialized structured compression performs best, yielding the most efficient training and the best performance.

</details>


### [414] [Scale-Cascaded Diffusion Models for Super-Resolution in Medical Imaging](https://arxiv.org/abs/2601.23201)
*Darshan Thaker,Mahmoud Mostapha,Radu Miron,Shihan Qiu,Mariappan Nadar*

Main category: eess.IV

TL;DR: 本文提出将图像分解为拉普拉斯金字塔尺度并为各频带训练独立扩散先验，用于医学图像超分辨率，在多数据集上验证可提升感知质量并减少推理时间。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型解决医学图像超分辨率等逆问题时，多使用单尺度扩散先验，忽略了图像数据的层次尺度结构。

Method: 将图像分解为拉普拉斯金字塔尺度，为每个频带训练单独的扩散先验，开发利用这些先验在不同尺度上逐步细化重建的超分辨率算法。

Result: 在大脑、膝盖和前列腺 MRI 数据上评估，相比基线方法提高了感知质量，通过较小的粗尺度网络减少了推理时间。

Conclusion: 该框架统一了多尺度重建和扩散先验用于医学图像超分辨率。

Abstract: Diffusion models have been increasingly used as strong generative priors for solving inverse problems such as super-resolution in medical imaging. However, these approaches typically utilize a diffusion prior trained at a single scale, ignoring the hierarchical scale structure of image data. In this work, we propose to decompose images into Laplacian pyramid scales and train separate diffusion priors for each frequency band. We then develop an algorithm to perform super-resolution that utilizes these priors to progressively refine reconstructions across different scales. Evaluated on brain, knee, and prostate MRI data, our approach both improves perceptual quality over baselines and reduces inference time through smaller coarse-scale networks. Our framework unifies multiscale reconstruction and diffusion priors for medical image super-resolution.

</details>


### [415] [Solving Inverse Problems with Flow-based Models via Model Predictive Control](https://arxiv.org/abs/2601.23231)
*George Webber,Alexander Denker,Riccardo Barbano,Andrew J Reader*

Main category: eess.IV

TL;DR: 提出MPC - Flow框架解决基于流的生成模型条件生成难题，在图像恢复任务表现良好。


<details>
  <summary>Details</summary>
Motivation: 基于流的生成模型条件生成有挑战，现有训练无关条件生成求解轨迹优化计算和内存开销大。

Method: 提出MPC - Flow框架，将基于流的生成模型逆问题求解转化为一系列控制子问题。

Result: 在基准图像恢复任务中表现良好，可在消费级硬件上对大规模架构进行训练无关引导。

Conclusion: MPC - Flow框架能实现基于最优控制的实用引导，避免通过生成模型轨迹进行反向传播。

Abstract: Flow-based generative models provide strong unconditional priors for inverse problems, but guiding their dynamics for conditional generation remains challenging. Recent work casts training-free conditional generation in flow models as an optimal control problem; however, solving the resulting trajectory optimisation is computationally and memory intensive, requiring differentiation through the flow dynamics or adjoint solves. We propose MPC-Flow, a model predictive control framework that formulates inverse problem solving with flow-based generative models as a sequence of control sub-problems, enabling practical optimal control-based guidance at inference time. We provide theoretical guarantees linking MPC-Flow to the underlying optimal control objective and show how different algorithmic choices yield a spectrum of guidance algorithms, including regimes that avoid backpropagation through the generative model trajectory. We evaluate MPC-Flow on benchmark image restoration tasks, spanning linear and non-linear settings such as in-painting, deblurring, and super-resolution, and demonstrate strong performance and scalability to massive state-of-the-art architectures via training-free guidance of FLUX.2 (32B) in a quantised setting on consumer hardware.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [416] [EmoShift: Lightweight Activation Steering for Enhanced Emotion-Aware Speech Synthesis](https://arxiv.org/abs/2601.22873)
*Li Zhou,Hao Jiang,Junjie Li,Tianrui Wang,Haizhou Li*

Main category: eess.AS

TL;DR: 提出轻量级激活转向框架EmoShift，能在语音合成中实现精准可控情绪表达，表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 许多情感感知TTS系统依赖固定情感嵌入缩放或外部引导，限制对特定情感潜在特征的建模。

Method: 提出包含EmoSteer层的轻量级激活转向框架EmoShift，学习每个目标情感的转向向量。

Result: EmoShift仅10M可训练参数，在主客观评估中胜过零样本和全微调基线模型，提升情感表现力。

Conclusion: EmoSteer层有效，且在语音合成中有控制情感强度的潜力。

Abstract: Achieving precise and controllable emotional expression is crucial for producing natural and context-appropriate speech in text-to-speech (TTS) synthesis. However, many emotion-aware TTS systems, including large language model (LLM)-based designs, rely on scaling fixed emotion embeddings or external guidance, limiting their ability to model emotion-specific latent characteristics. To address this gap, we present EmoShift, a lightweight activation-steering framework incorporating a EmoSteer layer, which learns a steering vector for each target emotion in the output embedding space to capture its latent offset and maintain stable, appropriate expression across utterances and categories. With only 10M trainable parameters,less than 1/30 of full fine-tuning, EmoShift outperforms zero-shot and fully fine-tuned baselines in objective and subjective evaluations, enhancing emotional expressiveness while preserving naturalness and speaker similarity. Further analysis confirms the proposed EmoSteer layer's effectiveness and reveals its potential for controllable emotional intensity in speech synthesis.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [417] [How Far Can Pretrained LLMs Go in Symbolic Music? Controlled Comparisons of Supervised and Preference-based Adaptation](https://arxiv.org/abs/2601.22764)
*Deepak Kumar,Emmanouil Karystinaios,Gerhard Widmer,Markus Schedl*

Main category: cs.SD

TL;DR: 研究将指令微调大语言模型应用于符号音乐的微调策略，给出适应选择见解。


<details>
  <summary>Details</summary>
Motivation: 音乐与语言有相似性，当前指令微调大语言模型用于符号音乐的实际效果缺乏充分研究。

Method: 对基于ABC的生成和理解进行受控比较研究，对比现成指令微调主干、领域适应变体和音乐专用大语言模型基线。

Result: 在多个符号音乐语料库和评估信号上进行研究。

Conclusion: 强调了领域适应与保留先验信息的权衡，以及测量符号音乐领域适应指标的不同行为。

Abstract: Music often shares notable parallels with language, motivating the use of pretrained large language models (LLMs) for symbolic music understanding and generation. Despite growing interest, the practical effectiveness of adapting instruction-tuned LLMs to symbolic music remains insufficiently characterized. We present a controlled comparative study of finetuning strategies for ABC-based generation and understanding, comparing an off-the-shelf instruction-tuned backbone to domain-adapted variants and a music-specialized LLM baseline. Across multiple symbolic music corpora and evaluation signals, we provide some insights into adaptation choices for symbolic music applications. We highlight the domain adaptation vs.~preserving prior information tradeoff as well as the distinct behaviour of metrics used to measure the domain adaptation for symbolic music.

</details>


### [418] [Towards Explicit Acoustic Evidence Perception in Audio LLMs for Speech Deepfake Detection](https://arxiv.org/abs/2601.23066)
*Xiaoxuan Guo,Yuankun Xie,Haonan Cheng,Jiayi Zhou,Jian Liu,Hengyan Huang,Long Ye,Qin Zhang*

Main category: cs.SD

TL;DR: 现有基于音频大语言模型的语音深度伪造检测方法易忽略声学伪影，提出SDD - APALLM框架，结合原始音频和结构化频谱图，实验显示检测准确率和鲁棒性有提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于音频大语言模型的语音深度伪造检测方法决策时易偏向语义相关线索，忽略细粒度声学伪影，导致含微妙声学异常的假语音可绕过检测器。

Method: 在音频大语言模型范式下，引入Auditory Perception - enhanced Audio Large Language Model (APALLM)的语音深度伪造检测框架，通过将原始音频与结构化频谱图结合，使音频大语言模型更有效捕捉微妙声学不一致。

Result: 实验表明检测准确率和鲁棒性持续提升，尤其在语义线索具误导性的情况下。

Conclusion: 性能提升源于语义和声学信息的协同利用，而非简单模态聚合。

Abstract: Speech deepfake detection (SDD) focuses on identifying whether a given speech signal is genuine or has been synthetically generated. Existing audio large language model (LLM)-based methods excel in content understanding; however, their predictions are often biased toward semantically correlated cues, which results in fine-grained acoustic artifacts being overlooked during the decisionmaking process. Consequently, fake speech with natural semantics can bypass detectors despite harboring subtle acoustic anomalies; this suggests that the challenge stems not from the absence of acoustic data, but from its inadequate accessibility when semantic-dominant reasoning prevails. To address this issue, we investigate SDD within the audio LLM paradigm and introduce SDD with Auditory Perception-enhanced Audio Large Language Model (SDD-APALLM), an acoustically enhanced framework designed to explicitly expose fine-grained time-frequency evidence as accessible acoustic cues. By combining raw audio with structured spectrograms, the proposed framework empowers audio LLMs to more effectively capture subtle acoustic inconsistencies without compromising their semantic understanding. Experimental results indicate consistent gains in detection accuracy and robustness, especially in cases where semantic cues are misleading. Further analysis reveals that these improvements stem from a coordinated utilization of semantic and acoustic information, as opposed to simple modality aggregation.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [419] [Smart Routing with Precise Link Estimation: DSEE-Based Anypath Routing for Reliable Wireless Networking](https://arxiv.org/abs/2405.10377)
*Narjes Nourzad,Bhaskar Krishnamachari*

Main category: cs.NI

TL;DR: 本文提出利用DSEE多臂老虎机算法改进最短任意路径路由，提升其在链路条件波动时的可靠性和弹性，且理论证明该方案优于TSOR。


<details>
  <summary>Details</summary>
Motivation: 传统路由协议在动态和资源受限环境中因依赖预定路径而失效，最短任意路径路由依赖链路质量但预测困难，需要准确实时估计链路传输概率的方法。

Method: 引入DSEE多臂老虎机算法，将其与任意路径路由结合，持续学习以准确估计传输概率并选择合适路由。

Result: 该算法能准确估计传输概率，选择合适路径路由数据包，维持可证明的接近对数的遗憾边界。

Conclusion: 提出的方案在网络规模上的遗憾缩放优于之前的TSOR。

Abstract: In dynamic and resource-constrained environments, such as multi-hop wireless mesh networks, traditional routing protocols often falter by relying on predetermined paths that prove ineffective in unpredictable link conditions. Shortest Anypath routing offers a solution by adapting routing decisions based on real-time link conditions. However, the effectiveness of such routing is fundamentally dependent on the quality and reliability of the available links, and predicting these variables with certainty is challenging. This paper introduces a novel approach that leverages the Deterministic Sequencing of Exploration and Exploitation (DSEE), a multi-armed bandit algorithm, to address the need for accurate and real-time estimation of link delivery probabilities. This approach augments the reliability and resilience of the Shortest Anypath routing in the face of fluctuating link conditions. By coupling DSEE with Anypath routing, this algorithm continuously learns and ensures accurate delivery probability estimation and selects the most suitable way to efficiently route packets while maintaining a provable near-logarithmic regret bound. We also theoretically prove that our proposed scheme offers better regret scaling with respect to the network size than the previously proposed Thompson Sampling-based Opportunistic Routing (TSOR).

</details>


### [420] [MCP-Diag: A Deterministic, Protocol-Driven Architecture for AI-Native Network Diagnostics](https://arxiv.org/abs/2601.22633)
*Devansh Lodha,Mohit Panchal,Sameer G. Kulkarni*

Main category: cs.NI

TL;DR: 本文提出MCP - Diag架构解决大语言模型集成到网络操作中的两个挑战，评估显示其有高实体提取准确率等优势。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型集成到网络操作（AIOps）时面临的随机接地问题和安全缺口问题。

Method: 构建基于模型上下文协议（MCP）的混合神经符号架构MCP - Diag，提出确定性翻译层将原始输出转换为JSON模式，引入强制“启发循环”在协议层实施人在环（HITL）授权。

Result: MCP - Diag实现100%实体提取准确率，执行延迟开销小于0.9%，上下文令牌使用量增加3.7倍。

Conclusion: MCP - Diag架构在解决大语言模型集成到网络操作的挑战方面表现良好。

Abstract: The integration of Large Language Models (LLMs) into network operations (AIOps) is hindered by two fundamental challenges: the stochastic grounding problem, where LLMs struggle to reliably parse unstructured, vendor-specific CLI output, and the security gap of granting autonomous agents shell access. This paper introduces MCP-Diag, a hybrid neuro-symbolic architecture built upon the Model Context Protocol (MCP). We propose a deterministic translation layer that converts raw stdout from canonical utilities (dig, ping, traceroute) into rigorous JSON schemas before AI ingestion. We further introduce a mandatory "Elicitation Loop" that enforces Human-in-the-Loop (HITL) authorization at the protocol level. Our preliminary evaluation demonstrates that MCP-Diag achieving 100% entity extraction accuracy with less than 0.9% execution latency overhead and 3.7x increase in context token usage.

</details>


### [421] [Toward Non-Expert Customized Congestion Control](https://arxiv.org/abs/2601.22461)
*Mingrui Zhang,Hamid Bagheri,Lisong Xu*

Main category: cs.NI

TL;DR: 提出非专家定制拥塞控制算法框架NECC，能让非专家用户轻松实现定制，评估显示性能良好并探讨未来方向。


<details>
  <summary>Details</summary>
Motivation: 通用拥塞控制算法无法满足特定用户需求，定制算法非专家用户难以实现。

Method: 利用大语言模型和伯克利数据包过滤器（BPF）接口构建NECC框架。

Result: 使用真实世界拥塞控制算法评估，NECC性能有前景。

Conclusion: 成功解决定制拥塞控制算法实现问题，为未来研究提供方向。

Abstract: General-purpose congestion control algorithms (CCAs) are designed to achieve general congestion control goals, but they may not meet the specific requirements of certain users. Customized CCAs can meet certain users' specific requirements; however, non-expert users often lack the expertise to implement them. In this paper, we present an exploratory non-expert customized CCA framework, named NECC, which enables non-expert users to easily model, implement, and deploy their customized CCAs by leveraging Large Language Models and the Berkeley Packet Filter (BPF) interface. To the best of our knowledge, we are the first to address the customized CCA implementation problem. Our evaluations using real-world CCAs show that the performance of NECC is very promising, and we discuss the insights that we find and possible future research directions.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [422] [Mixed Latent Position Cluster Models for Networks](https://arxiv.org/abs/2601.22380)
*Chaoyi Lu,Riccardo Rastelli*

Main category: stat.ME

TL;DR: 提出混合潜在位置聚类模型（MLPCM）处理潜在位置模型（LPM）在有向网络分析中的局限，介绍参数估计和模型选择方法并验证其准确性和实用性。


<details>
  <summary>Details</summary>
Motivation: 传统LPM具有对称性，不适合分析有向网络，且难以捕捉加权边的非欧几里得模式，需要改进模型。

Method: 开发MLPCM处理不对称和非欧几里得模式，用变分贝叶斯方法估计参数，引入部分积分完全似然准则进行模型选择。

Result: 使用合成数据集验证了方法的准确性，通过国际武器转移数据集展示了其实际效用。

Conclusion: MLPCM能有效解决LPM在有向网络分析中的局限，所提方法准确且实用。

Abstract: Over the last two decades, the Latent Position Model (LPM) has become a prominent tool to obtain model-based visualizations of networks. However, the geometric structure of the LPM is inherently symmetric, in the sense that outgoing and incoming edges are assumed to follow the same statistical distribution. As a consequence, the canonical LPM framework is not ideal for the analysis of directed networks. In addition, edges may be weighted to describe the duration or intensity of a connection. This can lead to disassortative patterns and other motifs that cannot be easily captured by the underlying geometry. To address these limitations, we develop a novel extension of the LPM, called the Mixed Latent Position Cluster Model (MLPCM), which can deal with asymmetry and non-Euclidean patterns, while providing new interpretations of the latent space. We dissect the directed edges of the network by formally disentangling how a node behaves from how it is perceived by others. This leads to a dual representation of a node's profile, identifying its ``overt'' and ``covert'' social positions. In order to efficiently estimate the parameters of our model, we develop a variational Bayes approach to approximate the posterior distribution. Unlike many existing variational frameworks, our algorithm does not require any additional numerical approximations. Model selection is performed by introducing a novel partially integrated complete likelihood criteria, which builds upon the literature on penalized likelihood methods. We demonstrate the accuracy of our proposed methodology using synthetic datasets, and we illustrate its practical utility with an application to a dataset of international arms transfers.

</details>


### [423] [Depth-based estimation for multivariate functional data with phase variability](https://arxiv.org/abs/2601.22884)
*Ana Arribas-Gil,Sara López-Pintado*

Main category: stat.ME

TL;DR: 提出基于深度的方法估计多元功能数据中的主模式函数，评估了性能并进行案例应用。


<details>
  <summary>Details</summary>
Motivation: 在多元功能数据有个体相位变化及跨组件时间扭曲的情况下，估计主模式函数。

Method: 讨论深度函数的必要条件，考虑不同模型假设；通过模拟评估方法性能和鲁棒性。

Result: 通过模拟评估了方法性能和鲁棒性，在两个真实数据集上展示了应用。

Conclusion: 所提出的基于深度的方法可用于估计多元功能数据的主模式函数。

Abstract: In the context of multivariate functional data with individual phase variation, we develop a robust depth-based approach to estimate the main pattern function when cross-component time warping is also present. In particular, we consider the latent deformation model (Carroll and Müller, 2023) in which the different components of a multivariate functional variable are also time-distorted versions of a common template function. Rather than focusing on a particular functional depth measure, we discuss the necessary conditions on a depth function to be able to provide a consistent estimation of the central pattern, considering different model assumptions. We evaluate the method performance and its robustness against atypical observations and violations of the model assumptions through simulations, and illustrate its use on two real data sets.

</details>


### [424] [Computationally efficient segmentation for non-stationary time series with oscillatory patterns](https://arxiv.org/abs/2601.22999)
*Nicolas Bianco,Lorenzo Cappello*

Main category: stat.ME

TL;DR: 提出用于多元非平稳振荡时间序列的变点检测和参数学习新方法，比现有方法快且有类似精度，还给出误差界并应用于实际数据。


<details>
  <summary>Details</summary>
Motivation: 解决多元非平稳振荡时间序列的变点检测和参数学习问题，避免现有方法使用的跨维度马尔可夫链蒙特卡罗算法。

Method: 用含未知频率和振幅的正弦函数之和加噪声的分段函数近似过程，离散化参数空间将问题转化为线性模型，再用变点检测算法。

Result: 模拟表明该方法比现有方法显著更快，保持可比数值精度，给出变点定位误差的高概率界。

Conclusion: 该方法有效，可应用于气候和脑电睡眠数据。

Abstract: We propose a novel approach for change-point detection and parameter learning in multivariate non-stationary time series exhibiting oscillatory behaviour. We approximate the process through a piecewise function defined by a sum of sinusoidal functions with unknown frequencies and amplitudes plus noise. The inference for this model is non-trivial. However, discretising the parameter space allows us to recast this complex estimation problem into a more tractable linear model, where the covariates are Fourier basis functions. Then, any change-point detection algorithms for segmentation can be used. The advantage of our proposal is that it bypasses the need for trans-dimensional Markov chain Monte Carlo algorithms used by state-of-the-art methods. Through simulations, we demonstrate that our method is significantly faster than existing approaches while maintaining comparable numerical accuracy. We also provide high probability bounds on the change-point localization error. We apply our methodology to climate and EEG sleep data.

</details>


### [425] [Changepoint Detection As Model Selection: A General Framework](https://arxiv.org/abs/2601.22481)
*Michael Grantham,Xueheng Shi,Bertrand Clarke*

Main category: stat.ME

TL;DR: 本文提出基于L0模型选择的变点检测通用框架，核心方法IRFL改进广义lasso，模拟和实际应用表明其能准确检测变点，还可用于图像数据处理。


<details>
  <summary>Details</summary>
Motivation: 开发能准确检测复杂数据中结构变化的变点检测方法。

Method: 提出Iteratively Reweighted Fused Lasso (IRFL)方法，通过自适应重新加权惩罚来改进广义lasso，最小化如贝叶斯信息准则（BIC）等标准。

Result: 模拟研究表明IRFL在多种具有挑战性场景中能准确检测变点；扩展到图像数据可实现保边去噪和分割；实际应用揭示Mauna Loa CO2时间序列变点，趋势分解比普通最小二乘法更准确。

Conclusion: IRFL为复杂数据中的结构变化检测提供了一个强大、可扩展的工具。

Abstract: This dissertation presents a general framework for changepoint detection based on L0 model selection. The core method, Iteratively Reweighted Fused Lasso (IRFL), improves upon the generalized lasso by adaptively reweighting penalties to enhance support recovery and minimize criteria such as the Bayesian Information Criterion (BIC). The approach allows for flexible modeling of seasonal patterns, linear and quadratic trends, and autoregressive dependence in the presence of changepoints.
  Simulation studies demonstrate that IRFL achieves accurate changepoint detection across a wide range of challenging scenarios, including those involving nuisance factors such as trends, seasonal patterns, and serially correlated errors. The framework is further extended to image data, where it enables edge-preserving denoising and segmentation, with applications spanning medical imaging and high-throughput plant phenotyping.
  Applications to real-world data demonstrate IRFL's utility. In particular, analysis of the Mauna Loa CO2 time series reveals changepoints that align with volcanic eruptions and ENSO events, yielding a more accurate trend decomposition than ordinary least squares. Overall, IRFL provides a robust, extensible tool for detecting structural change in complex data.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [426] [AI Decodes Historical Chinese Archives to Reveal Lost Climate History](https://arxiv.org/abs/2601.22458)
*Sida He,Lingxi Xie,Xiaopeng Zhang,Qi Tian*

Main category: physics.ao-ph

TL;DR: 提出生成式AI框架将历史档案气候描述转为定量记录，应用于中国历史档案重建降水，有广泛应用价值。


<details>
  <summary>Details</summary>
Motivation: 历史档案中气候事件定性描述转为定量记录存在挑战。

Method: 引入生成式AI框架，反向推断与记载事件关联的定量气候模式。

Result: 重建了1368 - 1911年中国东南部次年度降水，量化极端事件，揭示厄尔尼诺影响。

Conclusion: 该方法和数据集可应用于气候科学，对历史和社会科学也有广泛意义。

Abstract: Historical archives contain qualitative descriptions of climate events, yet converting these into quantitative records has remained a fundamental challenge. Here we introduce a paradigm shift: a generative AI framework that inverts the logic of historical chroniclers by inferring the quantitative climate patterns associated with documented events. Applied to historical Chinese archives, it produces the sub-annual precipitation reconstruction for southeastern China over the period 1368-1911 AD. Our reconstruction not only quantifies iconic extremes like the Ming Dynasty's Great Drought but also, crucially, maps the full spatial and seasonal structure of El Ni$ñ$o influence on precipitation in this region over five centuries, revealing dynamics inaccessible in shorter modern records. Our methodology and high-resolution climate dataset are directly applicable to climate science and have broader implications for the historical and social sciences.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [427] [Practical Evaluation of Quantum Kernel Methods for Radar Micro-Doppler Classification on Noisy Intermediate-Scale Quantum (NISQ) Hardware](https://arxiv.org/abs/2601.22194)
*Vikas Agnihotri,Jasleen Kaur,Sarvagya Kaushik*

Main category: quant-ph

TL;DR: 本文研究QSVM在雷达空中目标分类中的应用，经PCA降维后用ZZFeatureMap嵌入，在模拟器和量子硬件上实验，显示有竞争力且分析了硬件问题与限制。


<details>
  <summary>Details</summary>
Motivation: 探索量子支持向量机（QSVM）在基于雷达微多普勒特征的空中目标分类中的应用。

Method: 提取经典特征并用PCA降维，用ZZFeatureMap嵌入量子核特征空间，用QSVM分类，先在模拟器评估再在IBM量子硬件验证。

Result: QSVM在低特征维度下有与经典SVM相当的分类性能，硬件实验显示噪声、退相干和测量次数影响量子核估计，Heron r2架构稳定性和保真度更好。

Conclusion: 给出模拟器和硬件上QSVM实现的系统比较，强调了量子核方法用于雷达信号分类的可行性和当前限制。

Abstract: This paper examines the application of a Quantum Support Vector Machine (QSVM) for radarbased aerial target classification using micro-Doppler signatures. Classical features are extracted and reduced via Principal Component Analysis (PCA) to enable efficient quantum encoding. The reduced feature vectors are embedded into a quantum kernel-induced feature space using a fully entangled ZZFeatureMap and classified using a kernel based QSVM. Performance is first evaluated on a quantum simulator and subsequently validated on NISQ-era superconducting quantum hardware, specifically the IBM Torino (133-qubit) and IBM Fez (156-qubit) processors. Experimental results demonstrate that the QSVM achieves competitive classification performance relative to classical SVM baselines while operating on substantially reduced feature dimensionality. Hardware experiments reveal the impact of noise and decoherence and measurement shot count on quantum kernel estimation, and further show improved stability and fidelity on newer Heron r2 architecture. This study provides a systematic comparison between simulator-based and hardware-based QSVM implementations and highlights both the feasibility and current limitations of deploying quantum kernel methods for practical radar signal classification tasks.

</details>


### [428] [Spectral Filtering for Learning Quantum Dynamics](https://arxiv.org/abs/2601.22400)
*Elad Hazan,Annie Marsden*

Main category: quant-ph

TL;DR: 本文将线性响应机制下量子演化预测任务化为学习特定复值线性动态系统，并提出量子光谱滤波方法，证明可在样本和计算复杂度与环境状态维度无关情况下学习系统。


<details>
  <summary>Details</summary>
Motivation: 学习高维量子系统面临维度灾难挑战，传统系统识别方法在希尔伯特维度上成本高。

Method: 提出量子光谱滤波方法，将目标转向非精确动态学习，利用Slepian基的最优集中特性。

Result: 证明此类系统的可学习性严格由有效量子维度$k^*$决定，若系统频谱有界，可在样本和计算复杂度与环境状态维度无关情况下学习复值线性动态系统。

Conclusion: 在频谱有界条件下，复值线性动态系统可突破维度限制进行学习。

Abstract: Learning high-dimensional quantum systems is a fundamental challenge that notoriously suffers from the curse of dimensionality. We formulate the task of predicting quantum evolution in the linear response regime as a specific instance of learning a Complex-Valued Linear Dynamical System (CLDS) with sector-bounded eigenvalues -- a setting that also encompasses modern Structured State Space Models (SSMs). While traditional system identification attempts to reconstruct full system matrices (incurring exponential cost in the Hilbert dimension), we propose Quantum Spectral Filtering, a method that shifts the goal to improper dynamic learning. Leveraging the optimal concentration properties of the Slepian basis, we prove that the learnability of such systems is governed strictly by an effective quantum dimension $k^*$, determined by the spectral bandwidth and memory horizon. This result establishes that complex-valued LDSs can be learned with sample and computational complexity independent of the ambient state dimension, provided their spectrum is bounded.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [429] [Large Language Models: A Mathematical Formulation](https://arxiv.org/abs/2601.22170)
*Ricardo Baptista,Andrew Stuart,Son Tran*

Main category: math.NA

TL;DR: 本文为大语言模型（LLMs）提供了数学框架，具有较低的数学门槛，能用以解决相关算法问题并指明发展方向。


<details>
  <summary>Details</summary>
Motivation: 为大语言模型构建数学框架，以研究相关算法的准确性、效率和鲁棒性等问题，同时为方法的改进和发展提供方向。

Method: 描述文本序列编码成令牌序列，定义下一个令牌预测模型架构，说明如何从数据中学习模型以及如何将其应用于各种任务。

Result: 建立了大语言模型的数学框架，且该框架在经验上取得显著成功。

Conclusion: 所建立的数学框架可用于分析大语言模型算法的准确性、效率和鲁棒性等问题，还为改进和开发新方法提供了方向。

Abstract: Large language models (LLMs) process and predict sequences containing text to answer questions, and address tasks including document summarization, providing recommendations, writing software and solving quantitative problems. We provide a mathematical framework for LLMs by describing the encoding of text sequences into sequences of tokens, defining the architecture for next-token prediction models, explaining how these models are learned from data, and demonstrating how they are deployed to address a variety of tasks. The mathematical sophistication required to understand this material is not high, and relies on straightforward ideas from information theory, probability and optimization. Nonetheless, the combination of ideas resting on these different components from the mathematical sciences yields a complex algorithmic structure; and this algorithmic structure has demonstrated remarkable empirical successes. The mathematical framework established here provides a platform from which it is possible to formulate and address questions concerning the accuracy, efficiency and robustness of the algorithms that constitute LLMs. The framework also suggests directions for development of modified and new methodologies.

</details>


### [430] [Bayesian Interpolating Neural Network (B-INN): a scalable and reliable Bayesian model for large-scale physical systems](https://arxiv.org/abs/2601.22860)
*Chanwook Park,Brian Kim,Jiachen Guo,Wing Kam Liu*

Main category: math.NA

TL;DR: 本文提出贝叶斯插值神经网络（B - INN），结合高阶插值理论、张量分解和交替方向算法实现降维，速度比贝叶斯神经网络和高斯过程快，可用于大规模工业模拟。


<details>
  <summary>Details</summary>
Motivation: 神经网络和机器学习模型在不确定性量化方面存在可扩展性有限和可靠性差的问题，在工业规模的主动学习中不实用。

Method: 提出B - INN，结合高阶插值理论、张量分解和交替方向算法进行有效降维。

Result: 理论证明B - INN函数空间是高斯过程的子集，贝叶斯推理复杂度为线性；数值实验表明其速度比贝叶斯神经网络和高斯过程快20到10000倍，且不确定度估计稳健。

Conclusion: B - INN可作为大规模工业模拟中不确定性驱动的主动学习的实用基础。

Abstract: Neural networks and machine learning models for uncertainty quantification suffer from limited scalability and poor reliability compared to their deterministic counterparts. In industry-scale active learning settings, where generating a single high-fidelity simulation may require days or weeks of computation and produce data volumes on the order of gigabytes, they quickly become impractical. This paper proposes a scalable and reliable Bayesian surrogate model, termed the Bayesian Interpolating Neural Network (B-INN). The B-INN combines high-order interpolation theory with tensor decomposition and alternating direction algorithm to enable effective dimensionality reduction without compromising predictive accuracy. We theoretically show that the function space of a B-INN is a subset of that of Gaussian processes, while its Bayesian inference exhibits linear complexity, $\mathcal{O}(N)$, with respect to the number of training samples. Numerical experiments demonstrate that B-INNs can be from 20 times to 10,000 times faster with a robust uncertainty estimation compared to Bayesian neural networks and Gaussian processes. These capabilities make B-INN a practical foundation for uncertainty-driven active learning in large-scale industrial simulations, where computational efficiency and robust uncertainty calibration are paramount.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [431] [UrbanMoE: A Sparse Multi-Modal Mixture-of-Experts Framework for Multi-Task Urban Region Profiling](https://arxiv.org/abs/2601.22746)
*Pingping Liu,Jiamiao Liu,Zijian Zhang,Hao Miao,Qi Jiang,Qingliang Li,Qiuzhan Zhou,Irwin King*

Main category: cs.ET

TL;DR: 现存城市区域画像研究有局限，本文建立多任务基准，提出UrbanMoE框架，实验证明其性能优。


<details>
  <summary>Details</summary>
Motivation: 现有城市区域画像研究存在单任务预测局限性和缺乏标准化实验基准的问题，影响对城市环境多面性的捕捉和研究进展。

Method: 建立全面的多任务城市区域画像基准，提出稀疏多模态、多专家框架UrbanMoE，利用稀疏混合专家架构动态路由特征。

Result: 在三个真实世界数据集实验中，UrbanMoE性能优于所有基线，深入分析验证了其有效性和效率。

Conclusion: UrbanMoE达到新的技术水平，为城市分析研究提供了有价值的工具。

Abstract: Urban region profiling, the task of characterizing geographical areas, is crucial for urban planning and resource allocation. However, existing research in this domain faces two significant limitations. First, most methods are confined to single-task prediction, failing to capture the interconnected, multi-faceted nature of urban environments where numerous indicators are deeply correlated. Second, the field lacks a standardized experimental benchmark, which severely impedes fair comparison and reproducible progress. To address these challenges, we first establish a comprehensive benchmark for multi-task urban region profiling, featuring multi-modal features and a diverse set of strong baselines to ensure a fair and rigorous evaluation environment. Concurrently, we propose UrbanMoE, the first sparse multi-modal, multi-expert framework specifically architected to solve the multi-task challenge. Leveraging a sparse Mixture-of-Experts architecture, it dynamically routes multi-modal features to specialized sub-networks, enabling the simultaneous prediction of diverse urban indicators. We conduct extensive experiments on three real-world datasets within our benchmark, where UrbanMoE consistently demonstrates superior performance over all baselines. Further in-depth analysis validates the efficacy and efficiency of our approach, setting a new state-of-the-art and providing the community with a valuable tool for future research in urban analytics

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [432] [Evaluating the Effectiveness of OpenAI's Parental Control System](https://arxiv.org/abs/2601.23062)
*Kerem Ersoz,Saleh Afroogh,David Atkinson,Junfeng Jiao*

Main category: cs.CY

TL;DR: 评估平台级家长控制对未成年人使用对话助手的调控效果，发现通知不全面、存在漏过和过度屏蔽问题，提出改进建议


<details>
  <summary>Details</summary>
Motivation: 评估平台级家长控制对主流对话助手被未成年人使用时的调控有效性

Method: 两阶段方案，先构建对话语料库，再由训练的人类代理在消费者界面操作并监控家长收件箱，聚焦七个风险领域，量化四个结果，用自动评判和人工审核对比当前后端与旧版本

Result: 通知有选择性，当前后端漏过情况少于旧模型，但仍存在过度屏蔽良性教育查询未反馈给家长的问题

Conclusion: 提出扩大/配置通知分类、关联可见保护措施与隐私保护家长摘要、采用校准合适的改写替代全面拒绝等改进建议

Abstract: We evaluate how effectively platform-level parental controls moderate a mainstream conversational assistant used by minors. Our two-phase protocol first builds a category-balanced conversation corpus via PAIR-style iterative prompt refinement over API, then has trained human agents replay/refine those prompts in the consumer UI using a designated child account while monitoring the linked parent inbox for alerts. We focus on seven risk areas -- physical harm, pornography, privacy violence, health consultation, fraud, hate speech, and malware and quantify four outcomes: Notification Rate (NR), Leak-Through (LR), Overblocking (OBR), and UI Intervention Rate (UIR). Using an automated judge (with targeted human audit) and comparing the current backend to legacy variants (GPT-4.1/4o), we find that notifications are selective rather than comprehensive: privacy violence, fraud, hate speech, and malware triggered no parental alerts in our runs, whereas physical harm (highest), pornography, and some health queries produced intermittent alerts. The current backend shows lower leak-through than legacy models, yet overblocking of benign, educational queries near sensitive topics remains common and is not surfaced to parents, revealing a policy-product gap between on-screen safeguards and parent-facing telemetry. We propose actionable fixes: broaden/configure the notification taxonomy, couple visible safeguards to privacy-preserving parent summaries, and prefer calibrated, age-appropriate safe rewrites over blanket refusals.

</details>


### [433] [AI Narrative Breakdown. A Critical Assessment of Power and Promise](https://arxiv.org/abs/2601.22255)
*Rainer Rehak*

Main category: cs.CY

TL;DR: 文章探讨ChatGPT发布后人工智能相关话语，分析主流叙事，提出更接地气的AI参与方式并建议新叙事。


<details>
  <summary>Details</summary>
Motivation: 探究ChatGPT发布后社会对人工智能话语的参与情况，批判主流叙事。

Method: 基于批判计算机科学等多领域见解分析，先对AI话语进行历史和技术背景梳理，引入‘时代精神AI’概念，细致讨论常见叙事。

Result: 揭示AI应用中固有的政治、权力和价值决策。

Conclusion: 呼吁更实际地参与AI，提出将AI视为需社会治理的人类导向工具的新叙事。

Abstract: This article sets off for an exploration of the still evolving discourse surrounding artificial intelligence (AI) in the wake of the release of ChatGPT. It scrutinizes the pervasive narratives that are shaping the societal engagement with AI, spotlighting key themes such as agency and decision-making, autonomy, truthfulness, knowledge processing, prediction, general purpose, neutrality and objectivity, apolitical optimization, sustainability game-changer, democratization, mass unemployment, and the dualistic portrayal of AI as either a harbinger of societal utopia or dystopia. Those narratives are analysed critically based on insights from critical computer science, critical data and algorithm studies, from STS, data protection theory, as well as from the philosophy of mind and semiotics. To properly analyse the narratives presented, the article first delves into a historical and technical contextualisation of the AI discourse itself. The article then introduces the notion of "Zeitgeist AI" to critique the imprecise and misleading application of the term "AI" across various societal sectors. Then, by discussing common narratives with nuance, the article contextualises and challenges often assumed socio-political implications of AI, uncovering in detail and with examples the inherent political, power infused and value-laden decisions within all AI applications. Concluding with a call for a more grounded engagement with AI, the article carves out acute problems ignored by the narratives discussed and proposes new narratives recognizing AI as a human-directed tool necessarily subject to societal governance.

</details>


### [434] [AI Literacy, Safety Awareness, and STEM Career Aspirations of Australian Secondary Students: Evaluating the Impact of Workshop Interventions](https://arxiv.org/abs/2601.22486)
*Christian Bergh,Alexandra Vassar,Natasha Banks,Jessica Xu,Jake Renzella*

Main category: cs.CY

TL;DR: 研究评估澳大利亚AI日工作坊对7 - 10年级学生AI素养的影响，发现干预后学生AI知识和信心提升，对AI识别改进，但提升STEM职业兴趣效果小，支持推广重视合成媒体安全的AI素养项目。


<details>
  <summary>Details</summary>
Motivation: 深度伪造和合成媒体对青少年有安全风险，相关学生接触和行为的证据有限，评估工作坊干预对澳大利亚中学生AI素养和概念理解的影响。

Method: 采用混合研究方法，进行干预前后调查（干预前205人，干预后163人），分析学生识别AI能力、AI伦理等方面的理解以及对STEM职业的兴趣变化。

Result: 干预后学生自我报告的AI知识和信心提高，对常用平台AI识别改进，对STEM职业兴趣增加但效果量小。

Conclusion: 支持推广将基础AI概念与合成媒体安全明确结合的可扩展AI素养项目。

Abstract: Deepfakes and other forms of synthetic media pose growing safety risks for adolescents, yet evidence on students' exposure and related behaviours remains limited. This study evaluates the impact of Day of AI Australia's workshop-based intervention designed to improve AI literacy and conceptual understanding among Australian secondary students (Years 7-10). Using a mixed-methods approach with pre- and post-intervention surveys (N=205 pre; N=163 post), we analyse changes in students' ability to identify AI in everyday tools, their understanding of AI ethics, training, and safety, and their interest in STEM-related careers.
  Baseline data revealed notable synthetic media risks: 82.4% of students reported having seen deepfakes, 18.5% reported sharing them, and 7.3% reported creating them.
  Results show higher self-reported AI knowledge and confidence after the intervention, alongside improved recognition of AI in widely used platforms such as Netflix, Spotify, and TikTok. This pattern suggests a shift from seeing these tools as merely "algorithm-based" to recognising them as AI-driven systems. Students also reported increased interest in STEM careers post-workshop; however, effect sizes were small, indicating that sustained approaches beyond one-off workshops may be needed to influence longer-term aspirations. Overall, the findings support scalable AI literacy programs that pair foundational AI concepts with an explicit emphasis on synthetic media safety.

</details>


### [435] [Beyond Abstract Compliance: Operationalising trust in AI as a moral relationship](https://arxiv.org/abs/2601.22769)
*Lameck Mbangula Amugongo,Tutaleni Asino,Nicola J Bidwell*

Main category: cs.CY

TL;DR: 本文提出扩展的AI信任原则，借鉴关系伦理，以社区参与构建信任，并用医疗和教育案例说明原则实施。


<details>
  <summary>Details</summary>
Motivation: 现有主流方法未考虑信任的主观培养、文化嵌入和内在关系性，需提出新的AI信任原则。

Method: 借鉴关系伦理和非洲社群主义哲学，让社区参与AI全生命周期，以构建信任。

Result: 提出可融入常见开发方法的AI信任扩展原则，说明基于非洲关系伦理的信任原则可操作化。

Conclusion: 社区参与AI全生命周期能建立信任，促进更公平、因地制宜的AI系统。

Abstract: Dominant approaches, e.g. the EU's "Trustworthy AI framework", treat trust as a property that can be designed for, evaluated, and governed according to normative and technical criteria. They do not address how trust is subjectively cultivated and experienced, culturally embedded, and inherently relational. This paper proposes some expanded principles for trust in AI that can be incorporated into common development methods and frame trust as a dynamic, temporal relationship, which involves transparency and mutual respect. We draw on relational ethics and, in particular, African communitarian philosophies, to foreground the nuances of inclusive, participatory processes and long-term relationships with communities. Involving communities throughout the AI lifecycle can foster meaningful relationships with AI design and development teams that incrementally build trust and promote more equitable and context-sensitive AI systems. We illustrate how trust-enabling principles based on African relational ethics can be operationalised, using two use-cases for AI: healthcare and education.

</details>


### [436] [Eroding the Truth-Default: A Causal Analysis of Human Susceptibility to Foundation Model Hallucinations and Disinformation in the Wild](https://arxiv.org/abs/2601.22871)
*Alexander Loth,Martin Kappes,Marc-Oliver Pahl*

Main category: cs.CY

TL;DR: 本文提出JudgeGPT和RogueGPT双轴框架，研究人类区分合成与自然内容的机制，发现政治倾向与检测性能关联小，'假新闻熟悉度'是潜在中介，存在 '流畅性陷阱'，建议 '预辟谣' 干预应针对认知源监控。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型接近人类语言流利度，区分合成与自然内容成为可信网络智能的关键挑战，需研究人类易感性机制。

Method: 提出JudgeGPT和RogueGPT双轴框架，将“真实性”与“归因”解耦，分析918次评估，采用结构因果模型（SCMs）制定可测试的因果假设。

Result: 政治取向与检测性能关联可忽略（$r=-0.10$），“假新闻熟悉度”是候选中介（$r=0.35$），存在GPT - 4输出绕过源监控机制的“流畅性陷阱”。

Conclusion: “预辟谣”干预应针对认知源监控而非人口细分，以确保信息生态系统的可信度。

Abstract: As foundation models (FMs) approach human-level fluency, distinguishing synthetic from organic content has become a key challenge for Trustworthy Web Intelligence.
  This paper presents JudgeGPT and RogueGPT, a dual-axis framework that decouples "authenticity" from "attribution" to investigate the mechanisms of human susceptibility. Analyzing 918 evaluations across five FMs (including GPT-4 and Llama-2), we employ Structural Causal Models (SCMs) as a principal framework for formulating testable causal hypotheses about detection accuracy.
  Contrary to partisan narratives, we find that political orientation shows a negligible association with detection performance ($r=-0.10$). Instead, "fake news familiarity" emerges as a candidate mediator ($r=0.35$), suggesting that exposure may function as adversarial training for human discriminators. We identify a "fluency trap" where GPT-4 outputs (HumanMachineScore: 0.20) bypass Source Monitoring mechanisms, rendering them indistinguishable from human text.
  These findings suggest that "pre-bunking" interventions should target cognitive source monitoring rather than demographic segmentation to ensure trustworthy information ecosystems.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [437] [Assessing the Real-World Impact of Post-Quantum Cryptography on WPA-Enterprise Networks](https://arxiv.org/abs/2601.22892)
*Lukas Köder,Nils Lohmiller,Phil Schmieder,Bastian Buck,Michael Menth,Tobias Heer*

Main category: cs.CR

TL;DR: 评估后量子密码学（PQC）算法对基于WPA - Enterprise认证的性能影响，证明其实际可行性。


<details>
  <summary>Details</summary>
Motivation: 大规模量子计算机对当代网络安全协议（如WPA - Enterprise认证）构成威胁，需采用PQC缓解。

Method: 用开源工具FreeRADIUS和hostapd搭建测试平台，评估认证延迟，对比不同PQC算法组合与现有加密方案。

Result: PQC会增加认证延迟，但ML - DSA - 65、Falcon - 1024与ML - KEM结合在安全和性能间有较好平衡，会话恢复可减轻开销。

Conclusion: 给出了PQC支持的WPA - Enterprise认证的首次现实性能评估，证明在企业Wi - Fi部署中的可行性。

Abstract: The advent of large-scale quantum computers poses a significant threat to contemporary network security protocols, including Wi-Fi Protected Access (WPA)-Enterprise authentication. To mitigate this threat, the adoption of Post-Quantum Cryptography (PQC) is critical. In this work, we investigate the performance impact of PQC algorithms on WPA-Enterprise-based authentication. To this end, we conduct an experimental evaluation of authentication latency using a testbed built with the open-source tools FreeRADIUS and hostapd, measuring the time spent at the client, access point, and RADIUS server. We evaluate multiple combinations of PQC algorithms and analyze their performance overhead in comparison to currently deployed cryptographic schemes. Beyond performance, we assess the security implications of these algorithm choices by relating authentication mechanisms to the quantum effort required for their exploitation. This perspective enables a systematic categorization of PQ-relevant weaknesses in WPA-Enterprise according to their practical urgency. The evaluation results show that, although PQC introduces additional authentication latency, combinations such as ML-DSA-65 and Falcon-1024 used in conjunction with ML-KEM provide a favorable trade-off between security and performance. Furthermore, we demonstrate that the resulting overhead can be effectively mitigated through session resumption. Overall, this work presents a first real-world performance evaluation of PQC-enabled WPA-Enterprise authentication and demonstrates its practical feasibility for enterprise Wi-Fi deployments.

</details>


### [438] [The Semantic Trap: Do Fine-tuned LLMs Learn Vulnerability Root Cause or Just Functional Pattern?](https://arxiv.org/abs/2601.22655)
*Feiyang Huang,Yuqiang Sun,Fan Zhang,Ziqi Yang,Han Liu,Yang Liu*

Main category: cs.CR

TL;DR: 本文指出微调后的大语言模型在软件漏洞检测中可能陷入‘语义陷阱’，提出评估框架TrapEval并进行实验，表明当前微调方法难以让模型真正理解漏洞。


<details>
  <summary>Details</summary>
Motivation: 不确定微调后的大语言模型在软件漏洞检测中的性能提升，是真正理解漏洞根源还是仅利用功能模式，想找出原因。

Method: 提出评估框架TrapEval，构建V2N和V2P两个数据集，对五个代表性大语言模型进行微调，并在跨数据集测试、语义保留扰动等条件下评估。

Result: 微调后的大语言模型难以区分漏洞代码及其补丁版本，在语义保留转换下鲁棒性严重下降，语义差距小时依赖功能上下文捷径。

Conclusion: 当前微调实践常无法让模型真正进行漏洞推理，传统数据集上的高分可能掩盖模型对漏洞因果逻辑的理解不足。

Abstract: LLMs demonstrate promising performance in software vulnerability detection after fine-tuning. However, it remains unclear whether these gains reflect a genuine understanding of vulnerability root causes or merely an exploitation of functional patterns. In this paper, we identify a critical failure mode termed the "semantic trap," where fine-tuned LLMs achieve high detection scores by associating certain functional domains with vulnerability likelihood rather than reasoning about the underlying security semantics.To systematically evaluate this phenomenon, we propose TrapEval, a comprehensive evaluation framework designed to disentangle vulnerability root cause from functional pattern. TrapEval introduces two complementary datasets derived from real-world open-source projects: V2N, which pairs vulnerable code with unrelated benign code, and V2P, which pairs vulnerable code with its corresponding patched version, forcing models to distinguish near-identical code that differs only in subtle security-critical logic. Using TrapEval, we fine-tune five representative state-of-the-art LLMs across three model families and evaluate them under cross-dataset testing, semantic-preserving perturbations, and varying degrees of semantic gap measured by CodeBLEU.Our empirical results reveal that, despite improvements in metrics, fine-tuned LLMs consistently struggle to distinguish vulnerable code from its patched counterpart, exhibit severe robustness degradation under minor semantic-preserving transformations, and rely heavily on functional-context shortcuts when the semantic gap is small. These findings provide strong evidence that current fine-tuning practices often fail to impart true vulnerability reasoning. Our findings serve as a wake-up call: high benchmark scores on traditional datasets may be illusory, masking the model's inability to understand the true causal logic of vulnerabilities.

</details>


### [439] [RealSec-bench: A Benchmark for Evaluating Secure Code Generation in Real-World Repositories](https://arxiv.org/abs/2601.22706)
*Yanlin Wang,Ziyao Zhang,Chong Wang,Xinyi Xu,Mingwei Liu,Yong Wang,Jiachi Chen,Zibin Zheng*

Main category: cs.CR

TL;DR: 现有大模型代码生成安全性评估不足，提出RealSec - bench基准并实验，揭示功能与安全代码生成差距。


<details>
  <summary>Details</summary>
Motivation: 现有基准难以评估大模型生成安全代码的能力，需新基准评估功能与安全的结合。

Method: 构建多阶段流水线打造RealSec - bench基准，用SecurePass@K指标，对5个流行大模型实验。

Result: RAG技术对安全性提升有限，按安全指南提示易致编译失败。

Conclusion: 当前大模型在功能和安全代码生成间存在差距。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation, but their proficiency in producing secure code remains a critical, under-explored area. Existing benchmarks often fall short by relying on synthetic vulnerabilities or evaluating functional correctness in isolation, failing to capture the complex interplay between functionality and security found in real-world software. To address this gap, we introduce RealSec-bench, a new benchmark for secure code generation meticulously constructed from real-world, high-risk Java repositories. Our methodology employs a multi-stage pipeline that combines systematic SAST scanning with CodeQL, LLM-based false positive elimination, and rigorous human expert validation. The resulting benchmark contains 105 instances grounded in real-word repository contexts, spanning 19 Common Weakness Enumeration (CWE) types and exhibiting a wide diversity of data flow complexities, including vulnerabilities with up to 34-hop inter-procedural dependencies. Using RealSec-bench, we conduct an extensive empirical study on 5 popular LLMs. We introduce a novel composite metric, SecurePass@K, to assess both functional correctness and security simultaneously. We find that while Retrieval-Augmented Generation (RAG) techniques can improve functional correctness, they provide negligible benefits to security. Furthermore, explicitly prompting models with general security guidelines often leads to compilation failures, harming functional correctness without reliably preventing vulnerabilities. Our work highlights the gap between functional and secure code generation in current LLMs.

</details>


### [440] [ShellForge: Adversarial Co-Evolution of Webshell Generation and Multi-View Detection for Robust Webshell Defense](https://arxiv.org/abs/2601.22182)
*Yizhong Ding*

Main category: cs.CR

TL;DR: 提出ShellForge框架应对Webshell检测难题，由生成器和检测器构成，经迭代训练，在公共基准测试中提升防御能力。


<details>
  <summary>Details</summary>
Motivation: 现有Webshell检测机制难以应对快速变异和复杂混淆技术，且误报率高。

Method: 构建ShellForge框架，包括迭代训练的生成器和检测器，生成器优化合成高逃逸性变体，检测器融合多视角特征，用LLM生成无害样本降低误报。

Result: 在公共FWOID基准测试中，检测器F1分数达0.981，生成器对商业引擎逃逸率达0.939。

Conclusion: ShellForge显著增强了Webshell防御的鲁棒性。

Abstract: Webshells remain a primary foothold for attackers to compromise servers, particularly within PHP ecosystems. However, existing detection mechanisms often struggle to keep pace with rapid variant evolution and sophisticated obfuscation techniques that camouflage malicious intent. Furthermore, many current defenses suffer from high false-alarm rates when encountering benign administrative scripts that employ heavy obfuscation for intellectual property protection. To address these challenges, we present ShellForge, an adversarial co-evolution framework that couples automated webshell generation with multi-view detection to continuously harden defensive boundaries. The framework operates through an iterative co-training loop where a generator and a detector mutually reinforce each other via the exchange of hard samples. The generator is optimized through supervised fine-tuning and preference-based reinforcement learning to synthesize functional, highly evasive variants. Simultaneously, we develop a multi-view fusion detector that integrates semantic features from long-string compression, structural features from pruned abstract syntax trees, and global statistical indicators such as Shannon entropy. To minimize false positives, ShellForge utilizes a LLM-based transformation to create de-malicious samples--scripts that retain complex obfuscation patterns but lack harmful payloads--serving as high-quality hard negatives during training. Evaluations on the public FWOID benchmark demonstrate that ShellForge significantly enhances defensive robustness. Upon convergence, the detector maintains a 0.981 F1-score while the generator achieves a 0.939 evasion rate against commercial engines on VirusTotal.

</details>


### [441] [A Systematic Literature Review on LLM Defenses Against Prompt Injection and Jailbreaking: Expanding NIST Taxonomy](https://arxiv.org/abs/2601.22240)
*Pedro H. Barcha Correia,Ryan W. Achjian,Diego E. G. Caetano de Oliveira,Ygor Acacio Maria,Victor Takashi Hayashi,Marcos Lopes,Charles Christian Miers,Marcos A. Simplicio*

Main category: cs.CR

TL;DR: 本文对提示注入缓解策略进行系统文献综述，基于NIST报告提出扩展分类，提供防御措施目录及指南。


<details>
  <summary>Details</summary>
Motivation: 生成式AI和大语言模型发展带来新安全漏洞和挑战，攻防技术快速演变，需对缓解策略有结构化理解。

Method: 进行系统文献综述，分析88项研究，基于NIST报告识别更多研究，扩展NIST分类法。

Result: 识别出NIST报告外研究，提出扩展的NIST分类，提供包含定量有效性、开源性和模型无关性的防御措施目录。

Conclusion: 研究成果为对抗机器学习领域研究者和开发者提供实用资源。

Abstract: The rapid advancement and widespread adoption of generative artificial intelligence (GenAI) and large language models (LLMs) has been accompanied by the emergence of new security vulnerabilities and challenges, such as jailbreaking and other prompt injection attacks. These maliciously crafted inputs can exploit LLMs, causing data leaks, unauthorized actions, or compromised outputs, for instance. As both offensive and defensive prompt injection techniques evolve quickly, a structured understanding of mitigation strategies becomes increasingly important. To address that, this work presents the first systematic literature review on prompt injection mitigation strategies, comprehending 88 studies. Building upon NIST's report on adversarial machine learning, this work contributes to the field through several avenues. First, it identifies studies beyond those documented in NIST's report and other academic reviews and surveys. Second, we propose an extension to NIST taxonomy by introducing additional categories of defenses. Third, by adopting NIST's established terminology and taxonomy as a foundation, we promote consistency and enable future researchers to build upon the standardized taxonomy proposed in this work. Finally, we provide a comprehensive catalog of the reviewed prompt injection defenses, documenting their reported quantitative effectiveness across specific LLMs and attack datasets, while also indicating which solutions are open-source and model-agnostic. This catalog, together with the guidelines presented herein, aims to serve as a practical resource for researchers advancing the field of adversarial machine learning and for developers seeking to implement effective defenses in production systems.

</details>


### [442] [MirrorMark: A Distortion-Free Multi-Bit Watermark for Large Language Models](https://arxiv.org/abs/2601.22246)
*Ya Jiang,Massieh Kordi Boroujeny,Surender Suresh Kumar,Kai Zeng*

Main category: cs.CR

TL;DR: 提出用于大语言模型的无失真多比特水印MirrorMark，实验显示其文本质量与无水印生成相当，检测能力更强。


<details>
  <summary>Details</summary>
Motivation: 大语言模型应用中可靠内容溯源重要，现有水印方法存在提供二进制信号、扭曲采样分布、检测或鲁棒性弱等问题。

Method: 提出MirrorMark，通过保测方式镜像采样随机性嵌入多比特消息，引入基于上下文的调度器提高鲁棒性，还进行等错误率的理论分析。

Result: MirrorMark文本质量与无水印生成相当，检测能力更强，在300个token中嵌入54比特时，比特准确率提高8 - 12%，在1%误报率下能多识别出11%的水印文本。

Conclusion: MirrorMark是一种有效且性能良好的大语言模型水印方法。

Abstract: As large language models (LLMs) become integral to applications such as question answering and content creation, reliable content attribution has become increasingly important. Watermarking is a promising approach, but existing methods either provide only binary signals or distort the sampling distribution, degrading text quality; distortion-free approaches, in turn, often suffer from weak detectability or robustness. We propose MirrorMark, a multi-bit and distortion-free watermark for LLMs. By mirroring sampling randomness in a measure-preserving manner, MirrorMark embeds multi-bit messages without altering the token probability distribution, preserving text quality by design. To improve robustness, we introduce a context-based scheduler that balances token assignments across message positions while remaining resilient to insertions and deletions. We further provide a theoretical analysis of the equal error rate to interpret empirical performance. Experiments show that MirrorMark matches the text quality of non-watermarked generation while achieving substantially stronger detectability: with 54 bits embedded in 300 tokens, it improves bit accuracy by 8-12% and correctly identifies up to 11% more watermarked texts at 1% false positive rate.

</details>


### [443] [FraudShield: Knowledge Graph Empowered Defense for LLMs against Fraud Attacks](https://arxiv.org/abs/2601.22485)
*Naen Xu,Jinghuai Zhang,Ping He,Chunyi Zhou,Jun Wang,Zhihui Fu,Tianyu Du,Zhaoxiang Wang,Shouling Ji*

Main category: cs.CR

TL;DR: 提出FraudShield框架保护大语言模型免受欺诈内容影响，实验显示其性能优于现有方法且具有可解释性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型易受欺诈信息操纵，现有防御方法在有效性、可解释性和泛化性方面存在局限。

Method: 构建并优化欺诈策略 - 关键词知识图谱，通过突出关键词和提供支持证据增强原始输入。

Result: 在四个主流大语言模型和五种代表性欺诈类型上，FraudShield始终优于现有防御方法，并为模型生成提供可解释线索。

Conclusion: FraudShield能有效保护大语言模型免受欺诈内容影响，且具有良好性能和可解释性。

Abstract: Large language models (LLMs) have been widely integrated into critical automated workflows, including contract review and job application processes. However, LLMs are susceptible to manipulation by fraudulent information, which can lead to harmful outcomes. Although advanced defense methods have been developed to address this issue, they often exhibit limitations in effectiveness, interpretability, and generalizability, particularly when applied to LLM-based applications. To address these challenges, we introduce FraudShield, a novel framework designed to protect LLMs from fraudulent content by leveraging a comprehensive analysis of fraud tactics. Specifically, FraudShield constructs and refines a fraud tactic-keyword knowledge graph to capture high-confidence associations between suspicious text and fraud techniques. The structured knowledge graph augments the original input by highlighting keywords and providing supporting evidence, guiding the LLM toward more secure responses. Extensive experiments show that FraudShield consistently outperforms state-of-the-art defenses across four mainstream LLMs and five representative fraud types, while also offering interpretable clues for the model's generations.

</details>


### [444] [Whispers of Wealth: Red-Teaming Google's Agent Payments Protocol via Prompt Injection](https://arxiv.org/abs/2601.22569)
*Tanusree Debi,Wentian Zhu*

Main category: cs.CR

TL;DR: 对AP2进行AI红队评估，发现间接和直接提示注入漏洞，介绍两种攻击技术并验证简单对抗性提示可颠覆代理行为，揭示当前代理支付架构弱点。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的代理用于自动化金融交易，AP2虽旨在保障代理购物安全，但其实践稳健性未充分探索，需评估其安全性。

Method: 对AP2进行AI红队评估，引入Branded Whisper Attack和Vault Whisper Attack两种攻击技术，用基于Gemini - 2.5 - Flash和Google ADK框架构建的购物代理进行实验验证。

Result: 发现间接和直接提示注入导致的漏洞，简单对抗性提示能可靠地颠覆代理行为。

Conclusion: 当前代理支付架构存在关键弱点，LLM介导的金融系统需要更强的隔离和防御保障措施。

Abstract: Large language model (LLM) based agents are increasingly used to automate financial transactions, yet their reliance on contextual reasoning exposes payment systems to prompt-driven manipulation. The Agent Payments Protocol (AP2) aims to secure agent-led purchases through cryptographically verifiable mandates, but its practical robustness remains underexplored. In this work, we perform an AI red-teaming evaluation of AP2 and identify vulnerabilities arising from indirect and direct prompt injection. We introduce two attack techniques, the Branded Whisper Attack and the Vault Whisper Attack which manipulate product ranking and extract sensitive user data. Using a functional AP2 based shopping agent built with Gemini-2.5-Flash and the Google ADK framework, we experimentally validate that simple adversarial prompts can reliably subvert agent behavior. Our findings reveal critical weaknesses in current agentic payment architectures and highlight the need for stronger isolation and defensive safeguards in LLM-mediated financial systems.

</details>


### [445] [AEGIS: White-Box Attack Path Generation using LLMs and Training Effectiveness Evaluation for Large-Scale Cyber Defence Exercises](https://arxiv.org/abs/2601.22720)
*Ivan K. Tung,Yu Xiang Shi,Alex Chien,Wenkai Liu,Lawrence Zheng*

Main category: cs.CR

TL;DR: 介绍AEGIS系统，用LLMs、白盒访问和蒙特卡罗树搜索生成攻击路径，评估显示其生成路径与人工场景相当，能大幅减少场景开发时间。


<details>
  <summary>Details</summary>
Motivation: 现有攻击路径自动化方法依赖预先整理的漏洞图或利用集，应用受限，需要新方法。

Method: 使用LLMs动态发现利用程序，白盒访问在确定攻击路径前单独验证利用程序，结合蒙特卡罗树搜索。

Result: 在CIDeX 2025大规模演习中，AEGIS生成路径在四个训练体验维度上与人工场景相当，用验证过的问卷测量结果。

Conclusion: AEGIS自动化利用链发现和验证，将场景开发时间从数月缩短到数天，使专家精力从技术验证转向场景设计。

Abstract: Creating attack paths for cyber defence exercises requires substantial expert effort. Existing automation requires vulnerability graphs or exploit sets curated in advance, limiting where it can be applied. We present AEGIS, a system that generates attack paths using LLMs, white-box access, and Monte Carlo Tree Search over real exploit execution. LLM-based search discovers exploits dynamically without pre-existing vulnerability graphs, while white-box access enables validating exploits in isolation before committing to attack paths. Evaluation at CIDeX 2025, a large-scale exercise spanning 46 IT hosts, showed that AEGIS-generated paths are comparable to human-authored scenarios across four dimensions of training experience (perceived learning, engagement, believability, challenge). Results were measured with a validated questionnaire extensible to general simulation-based training. By automating exploit chain discovery and validation, AEGIS reduces scenario development from months to days, shifting expert effort from technical validation to scenario design.

</details>


### [446] [Hide and Seek in Embedding Space: Geometry-based Steganography and Detection in Large Language Models](https://arxiv.org/abs/2601.22818)
*Charles Westphal,Keivan Navaie,Fernando E. Rosas*

Main category: cs.CR

TL;DR: 本文提出低可恢复性隐写术降低有效负载可恢复性，并提出基于机械可解释性的检测方法。


<details>
  <summary>Details</summary>
Motivation: 先前工作中的编码容易被恢复，需解决微调大语言模型隐写术的有效负载可恢复性问题及检测问题。

Method: 引入低可恢复性隐写术，用嵌入空间派生映射替代任意映射；提出基于机械可解释性的检测方法，用线性探针在后期层激活上训练。

Result: 在不同模型和数据集上，精确秘密恢复率提升且降低了有效负载可恢复性；线性探针检测秘密的准确率比基础模型最高高33%。

Conclusion: 恶意微调会留下可用于基于可解释性防御的内部特征。

Abstract: Fine-tuned LLMs can covertly encode prompt secrets into outputs via steganographic channels. Prior work demonstrated this threat but relied on trivially recoverable encodings. We formalize payload recoverability via classifier accuracy and show previous schemes achieve 100\% recoverability. In response, we introduce low-recoverability steganography, replacing arbitrary mappings with embedding-space-derived ones. For Llama-8B (LoRA) and Ministral-8B (LoRA) trained on TrojanStego prompts, exact secret recovery rises from 17$\rightarrow$30\% (+78\%) and 24$\rightarrow$43\% (+80\%) respectively, while on Llama-70B (LoRA) trained on Wiki prompts, it climbs from 9$\rightarrow$19\% (+123\%), all while reducing payload recoverability. We then discuss detection. We argue that detecting fine-tuning-based steganographic attacks requires approaches beyond traditional steganalysis. Standard approaches measure distributional shift, which is an expected side-effect of fine-tuning. Instead, we propose a mechanistic interpretability approach: linear probes trained on later-layer activations detect the secret with up to 33\% higher accuracy in fine-tuned models compared to base models, even for low-recoverability schemes. This suggests that malicious fine-tuning leaves actionable internal signatures amenable to interpretability-based defenses.

</details>


### [447] [Evaluating Large Language Models for Security Bug Report Prediction](https://arxiv.org/abs/2601.22921)
*Farnaz Soltaniani,Shoaib Razzaq,Mohammad Ghafari*

Main category: cs.CR

TL;DR: 评估基于提示工程和微调方法利用大语言模型预测安全漏洞报告，发现两种方法各有优劣，需进一步研究。


<details>
  <summary>Details</summary>
Motivation: 早期检测安全漏洞报告对及时缓解漏洞至关重要，评估不同方法用大语言模型预测安全漏洞报告的效果。

Method: 评估基于提示工程和微调两种方法利用大语言模型预测安全漏洞报告。

Result: 提示工程的私有模型对安全漏洞报告敏感性高，G - 度量平均达77%，召回率74%，但假阳性率高，平均精度仅22%；微调模型G - 度量51%，精度75%，召回率36%，在最大数据集上推理速度比私有模型快50倍。

Conclusion: 需要进一步研究以充分发挥大语言模型在安全漏洞报告预测中的作用。

Abstract: Early detection of security bug reports (SBRs) is critical for timely vulnerability mitigation. We present an evaluation of prompt-based engineering and fine-tuning approaches for predicting SBRs using Large Language Models (LLMs). Our findings reveal a distinct trade-off between the two approaches. Prompted proprietary models demonstrate the highest sensitivity to SBRs, achieving a G-measure of 77% and a recall of 74% on average across all the datasets, albeit at the cost of a higher false-positive rate, resulting in an average precision of only 22%. Fine-tuned models, by contrast, exhibit the opposite behavior, attaining a lower overall G-measure of 51% but substantially higher precision of 75% at the cost of reduced recall of 36%. Though a one-time investment in building fine-tuned models is necessary, the inference on the largest dataset is up to 50 times faster than that of proprietary models. These findings suggest that further investigations to harness the power of LLMs for SBR prediction are necessary.

</details>


### [448] [Protecting Private Code in IDE Autocomplete using Differential Privacy](https://arxiv.org/abs/2601.22935)
*Evgeny Grigorenko,David Stanojević,David Ilić,Egor Bogomolov,Kostadin Cvejoski*

Main category: cs.CR

TL;DR: 本文研究使用差分隐私（DP）训练用于Kotlin代码补全的大语言模型，证明DP能有效防御成员推理攻击，且对模型性能影响小，是构建私密可信AI IDE功能的有效方案。


<details>
  <summary>Details</summary>
Motivation: 现代集成开发环境（IDE）的大语言模型在使用用户代码训练时存在隐私风险，恶意者可利用其进行数据攻击。

Method: 使用差分隐私（DP）对Mellum模型进行微调，并对其隐私性和实用性进行综合评估。

Result: DP能有效防御成员推理攻击（AUC从0.901降至0.606），且对模型性能影响小，即使在少100倍数据下训练，DP训练的模型实用性得分仍与非私密模型相当。

Conclusion: DP是构建私密可信AI驱动的IDE功能的实用且有效解决方案。

Abstract: Modern Integrated Development Environments (IDEs) increasingly leverage Large Language Models (LLMs) to provide advanced features like code autocomplete. While powerful, training these models on user-written code introduces significant privacy risks, making the models themselves a new type of data vulnerability. Malicious actors can exploit this by launching attacks to reconstruct sensitive training data or infer whether a specific code snippet was used for training. This paper investigates the use of Differential Privacy (DP) as a robust defense mechanism for training an LLM for Kotlin code completion. We fine-tune a \texttt{Mellum} model using DP and conduct a comprehensive evaluation of its privacy and utility. Our results demonstrate that DP provides a strong defense against Membership Inference Attacks (MIAs), reducing the attack's success rate close to a random guess (AUC from 0.901 to 0.606). Furthermore, we show that this privacy guarantee comes at a minimal cost to model performance, with the DP-trained model achieving utility scores comparable to its non-private counterpart, even when trained on 100x less data. Our findings suggest that DP is a practical and effective solution for building private and trustworthy AI-powered IDE features.

</details>


### [449] [A Real-Time Privacy-Preserving Behavior Recognition System via Edge-Cloud Collaboration](https://arxiv.org/abs/2601.22938)
*Huan Song,Shuyu Tian,Junyi Hao,Cheng Yuan,Zhenyu Jia,Jiawei Shao,Xuelong Li*

Main category: cs.CR

TL;DR: 随着智能感知进入高隐私环境，提出基于AI Flow框架和边云协作架构的隐私保护感知技术，实现去标识行为感知。


<details>
  <summary>Details</summary>
Motivation: 智能感知进入高隐私环境，传统RGB监控有隐私问题，现有隐私保护方法有缺陷。

Method: 提出基于AI Flow理论框架和边云协作架构的方法，结合源脱敏和不可逆特征映射，边缘设备处理图像，云平台进行联合推理。

Result: 该方法从架构层面断绝隐私泄露途径。

Conclusion: 该方法实现从视频监控到去标识行为感知的突破，为高敏感公共空间风险管理提供有力解决方案。

Abstract: As intelligent sensing expands into high-privacy environments such as restrooms and changing rooms, the field faces a critical privacy-security paradox. Traditional RGB surveillance raises significant concerns regarding visual recording and storage, while existing privacy-preserving methods-ranging from physical desensitization to traditional cryptographic or obfuscation techniques-often compromise semantic understanding capabilities or fail to guarantee mathematical irreversibility against reconstruction attacks. To address these challenges, this study presents a novel privacy-preserving perception technology based on the AI Flow theoretical framework and an edge-cloud collaborative architecture. The proposed methodology integrates source desensitization with irreversible feature mapping. Leveraging Information Bottleneck theory, the edge device performs millisecond-level processing to transform raw imagery into abstract feature vectors via non-linear mapping and stochastic noise injection. This process constructs a unidirectional information flow that strips identity-sensitive attributes, rendering the reconstruction of original images impossible. Subsequently, the cloud platform utilizes multimodal family models to perform joint inference solely on these abstract vectors to detect abnormal behaviors. This approach fundamentally severs the path to privacy leakage at the architectural level, achieving a breakthrough from video surveillance to de-identified behavior perception and offering a robust solution for risk management in high-sensitivity public spaces.

</details>


### [450] [From Data Leak to Secret Misses: The Impact of Data Leakage on Secret Detection Models](https://arxiv.org/abs/2601.22946)
*Farnaz Soltaniani,Mohammad Ghafari*

Main category: cs.CR

TL;DR: 研究表明常用基准数据集中的重复样本导致数据泄露，会夸大AI秘密检测器的性能。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型常基于含重复样本的互联网数据集，数据泄露能使模型记忆而非泛化，需研究其影响。

Method: 调查硬编码秘密的广泛使用的基准数据集中的重复情况。

Result: 数据泄露会大幅夸大AI秘密检测器报告的性能。

Conclusion: 数据泄露会造成对AI秘密检测器实际效果的误导。

Abstract: Machine learning models are increasingly used for software security tasks. These models are commonly trained and evaluated on large Internet-derived datasets, which often contain duplicated or highly similar samples. When such samples are split across training and test sets, data leakage may occur, allowing models to memorize patterns instead of learning to generalize. We investigate duplication in a widely used benchmark dataset of hard coded secrets and show how data leakage can substantially inflate the reported performance of AI-based secret detectors, resulting in a misleading picture of their real-world effectiveness.

</details>


### [451] [From Similarity to Vulnerability: Key Collision Attack on LLM Semantic Caching](https://arxiv.org/abs/2601.23088)
*Zhixiang Zhang,Zesen Liu,Yuchong Xie,Quanfeng Huang,Dongdong She*

Main category: cs.CR

TL;DR: 本文指出语义缓存虽用于扩展大模型应用，但存在性能与安全权衡，易受密钥碰撞攻击，介绍CacheAttack框架评估风险并讨论缓解策略。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注边信道和隐私风险，本文旨在系统研究缓存碰撞带来的完整性风险。

Method: 将语义缓存键概念化为模糊哈希，分析性能与安全的权衡，引入CacheAttack自动化框架进行黑盒碰撞攻击评估。

Result: CacheAttack在大模型响应劫持中命中率达86%，能诱导大模型代理恶意行为，且跨不同嵌入模型有强可迁移性。

Conclusion: 语义缓存存在密钥碰撞攻击风险，通过案例说明其现实影响，并讨论了缓解策略。

Abstract: Semantic caching has emerged as a pivotal technique for scaling LLM applications, widely adopted by major providers including AWS and Microsoft. By utilizing semantic embedding vectors as cache keys, this mechanism effectively minimizes latency and redundant computation for semantically similar queries. In this work, we conceptualize semantic cache keys as a form of fuzzy hashes. We demonstrate that the locality required to maximize cache hit rates fundamentally conflicts with the cryptographic avalanche effect necessary for collision resistance. Our conceptual analysis formalizes this inherent trade-off between performance (locality) and security (collision resilience), revealing that semantic caching is naturally vulnerable to key collision attacks.
  While prior research has focused on side-channel and privacy risks, we present the first systematic study of integrity risks arising from cache collisions. We introduce CacheAttack, an automated framework for launching black-box collision attacks. We evaluate CacheAttack in security-critical tasks and agentic workflows. It achieves a hit rate of 86\% in LLM response hijacking and can induce malicious behaviors in LLM agent, while preserving strong transferability across different embedding models. A case study on a financial agent further illustrates the real-world impact of these vulnerabilities. Finally, we discuss mitigation strategies.

</details>


### [452] [WiFiPenTester: Advancing Wireless Ethical Hacking with Governed GenAI](https://arxiv.org/abs/2601.23092)
*Haitham S. Al-Sinani,Chris J. Mitchell*

Main category: cs.CR

TL;DR: 提出GenAI支持的无线道德黑客系统WiFiPenTester，实验表明其提升效率且保障审计与道德。


<details>
  <summary>Details</summary>
Motivation: 解决无线道德黑客过程劳动密集、难扩展、易主观判断和出错的问题。

Method: 将大语言模型集成到无线安全评估的侦察和决策支持阶段，保留人工控制和预算感知执行，介绍系统架构等并做实验。

Result: GenAI辅助提高了目标选择准确性和整体评估效率，同时保持了可审计性和道德保障。

Conclusion: WiFiPenTester是迈向实用、安全和可扩展的GenAI辅助无线渗透测试的有意义一步，部署GenAI需有限自主性、人工监督和严格治理机制。

Abstract: Wireless ethical hacking relies heavily on skilled practitioners manually interpreting reconnaissance results and executing complex, time-sensitive sequences of commands to identify vulnerable targets, capture authentication handshakes, and assess password resilience; a process that is inherently labour-intensive, difficult to scale, and prone to subjective judgement and human error. To help address these limitations, we propose WiFiPenTester, an experimental, governed, and reproducible system for GenAI-enabled wireless ethical hacking. The system integrates large language models into the reconnaissance and decision-support phases of wireless security assessment, enabling intelligent target ranking, attack feasibility estimation, and strategy recommendation, while preserving strict human-in-the-loop control and budget-aware execution. We describe the system architecture, threat model, governance mechanisms, and prompt-engineering methodology, and empirical experiments conducted across multiple wireless environments. The results demonstrate that GenAI assistance improves target selection accuracy and overall assessment efficiency, while maintaining auditability and ethical safeguards. This indicates that WiFiPenTester is a meaningful step toward practical, safe, and scalable GenAI-assisted wireless penetration testing, while reinforcing the necessity of bounded autonomy, human oversight, and rigorous governance mechanisms when deploying GenAI in ethical hacking.

</details>


### [453] [Secure Tool Manifest and Digital Signing Solution for Verifiable MCP and LLM Pipelines](https://arxiv.org/abs/2601.23132)
*Saeid Jamshidi,Kawser Wazed Nafi,Arghavan Moradi Dakhel,Foutse Khomh,Amin Nikanjam,Mohammad Adnan Hamdaqa*

Main category: cs.CR

TL;DR: 提出安全工具清单和数字签名框架解决大语言模型执行管道易受操纵问题，评估显示框架有良好性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在敏感领域应用时执行管道易受操纵，现有控制机制缺乏可验证执行和透明验证。

Method: 提出安全工具清单和数字签名框架，采用加密签名清单、集成透明验证日志、隔离执行元数据。

Result: 框架可近线性扩展（R平方 = 0.998），能近乎完美接受有效执行并拒绝无效执行，保持各执行管道模型利用率平衡。

Conclusion: 所提框架可确保大语言模型执行管道的可验证执行完整性。

Abstract: Large Language Models (LLMs) are increasingly adopted in sensitive domains such as healthcare and financial institutions' data analytics; however, their execution pipelines remain vulnerable to manipulation and unverifiable behavior. Existing control mechanisms, such as the Model Context Protocol (MCP), define compliance policies for tool invocation but lack verifiable enforcement and transparent validation of model actions. To address this gap, we propose a novel Secure Tool Manifest and Digital Signing Framework, a structured and security-aware extension of Model Context Protocols. The framework enforces cryptographically signed manifests, integrates transparent verification logs, and isolates model-internal execution metadata from user-visible components to ensure verifiable execution integrity. Furthermore, the evaluation demonstrates that the framework scales nearly linearly (R-squared = 0.998), achieves near-perfect acceptance of valid executions while consistently rejecting invalid ones, and maintains balanced model utilization across execution pipelines.

</details>


### [454] [Rethinking Anonymity Claims in Synthetic Data Generation: A Model-Centric Privacy Attack Perspective](https://arxiv.org/abs/2601.22434)
*Georgi Ganev,Emiliano De Cristofaro*

Main category: cs.CR

TL;DR: 从模型中心视角重新审视合成数据匿名性，关联法规与隐私攻击，评估合成数据系统。


<details>
  <summary>Details</summary>
Motivation: 现有训练生成模型生成合成表格数据在数据共享时仍有隐私风险，且当前多从单个数据集层面评估匿名性。

Method: 从模型中心视角，结合GDPR定义，分析可识别性风险，映射到不同威胁场景的隐私攻击，并比较DP和SBPMs机制。

Result: 合成数据技术不能确保足够匿名化，DP对可识别性风险有强大保护，SBPMs缺乏足够保障。

Conclusion: 将法规的可识别性概念与模型中心隐私攻击相联系，有助于研究人员、从业者和政策制定者对合成数据系统进行更负责和可信的监管评估。

Abstract: Training generative machine learning models to produce synthetic tabular data has become a popular approach for enhancing privacy in data sharing. As this typically involves processing sensitive personal information, releasing either the trained model or generated synthetic datasets can still pose privacy risks. Yet, recent research, commercial deployments, and privacy regulations like the General Data Protection Regulation (GDPR) largely assess anonymity at the level of an individual dataset.
  In this paper, we rethink anonymity claims about synthetic data from a model-centric perspective and argue that meaningful assessments must account for the capabilities and properties of the underlying generative model and be grounded in state-of-the-art privacy attacks. This perspective better reflects real-world products and deployments, where trained models are often readily accessible for interaction or querying. We interpret the GDPR's definitions of personal data and anonymization under such access assumptions to identify the types of identifiability risks that must be mitigated and map them to privacy attacks across different threat settings. We then argue that synthetic data techniques alone do not ensure sufficient anonymization. Finally, we compare the two mechanisms most commonly used alongside synthetic data -- Differential Privacy (DP) and Similarity-based Privacy Metrics (SBPMs) -- and argue that while DP can offer robust protections against identifiability risks, SBPMs lack adequate safeguards. Overall, our work connects regulatory notions of identifiability with model-centric privacy attacks, enabling more responsible and trustworthy regulatory assessment of synthetic data systems by researchers, practitioners, and policymakers.

</details>


### [455] [Trackly: A Unified SaaS Platform for User Behavior Analytics and Real Time Rule Based Anomaly Detection](https://arxiv.org/abs/2601.22800)
*Md Zahurul Haque,Md. Hafizur Rahman,Yeahyea Sarker*

Main category: cs.CR

TL;DR: Trackly是一个可扩展的SaaS平台，统一了用户行为分析和实时规则异常检测，在合成数据集上表现良好，适用于中小企业和电子商务。


<details>
  <summary>Details</summary>
Motivation: 当前多数平台将产品分析和安全分离，导致可见性碎片化和威胁检测延迟，需要统一的解决方案来理解用户行为、提升体验、防范威胁。

Method: Trackly通过跟踪会话、IP地理位置、设备浏览器指纹和细粒度事件，利用可配置规则和加权风险评分标记可疑活动，使用轻量级JavaScript SDK和安全REST API简化集成，基于多租户微服务栈实现。

Result: Trackly在合成数据集上实现了98.1%的准确率、97.7%的精确率和2.25%的误报率。

Conclusion: Trackly证明了其对中小企业和电子商务的有效性。

Abstract: Understanding user behavior is essential for improving digital experiences, optimizing business conversions, and mitigating threats like account takeovers, fraud, and bot attacks. Most platforms separate product analytics and security, creating fragmented visibility and delayed threat detection. Trackly, a scalable SaaS platform, unifies comprehensive user behavior analytics with real time, rule based anomaly detection. It tracks sessions, IP based geo location, device browser fingerprints, and granular events such as page views, add to cart, and checkouts. Suspicious activities logins from new devices or locations, impossible travel (Haversine formula), rapid bot like actions, VPN proxy usage, or multiple accounts per IP are flagged via configurable rules with weighted risk scoring, enabling transparent, explainable decisions. A real time dashboard provides global session maps, DAU MAU, bounce rates, and session durations. Integration is simplified with a lightweight JavaScript SDK and secure REST APIs. Implemented on a multi tenant microservices stack (ASP.NET Core, MongoDB, RabbitMQ, Next.js), Trackly achieved 98.1% accuracy, 97.7% precision, and 2.25% false positives on synthetic datasets, proving its efficiency for SMEs and ecommerce.

</details>


### [456] [PIDSMaker: Building and Evaluating Provenance-based Intrusion Detection Systems](https://arxiv.org/abs/2601.22983)
*Tristan Bilot,Baoxiang Jiang,Thomas Pasquier*

Main category: cs.CR

TL;DR: 现有基于溯源的入侵检测系统（PIDSs）评估困难，本文提出开源框架PIDSMaker，支持一致实验和公平比较，并提供相关工具和数据集。


<details>
  <summary>Details</summary>
Motivation: 现有PIDSs评估和比较困难，因预处理流程、数据集划分、标签和指标不一致，影响可重复性和公平性，增加研究人员重实现成本。

Method: 提出PIDSMaker框架，将八个先进系统整合为模块化、可扩展架构，有标准化预处理和标签，提供YAML配置界面，还包含多种实用工具。

Result: 通过具体用例展示了PIDSMaker，发布预处理器数据集和标签。

Conclusion: PIDSMaker能支持PIDS社区进行共享评估。

Abstract: Recent provenance-based intrusion detection systems (PIDSs) have demonstrated strong potential for detecting advanced persistent threats (APTs) by applying machine learning to system provenance graphs. However, evaluating and comparing PIDSs remains difficult: prior work uses inconsistent preprocessing pipelines, non-standard dataset splits, and incompatible ground-truth labeling and metrics. These discrepancies undermine reproducibility, impede fair comparison, and impose substantial re-implementation overhead on researchers. We present PIDSMaker, an open-source framework for developing and evaluating PIDSs under consistent protocols. PIDSMaker consolidates eight state-of-the-art systems into a modular, extensible architecture with standardized preprocessing and ground-truth labels, enabling consistent experiments and apples-to-apples comparisons. A YAML-based configuration interface supports rapid prototyping by composing components across systems without code changes. PIDSMaker also includes utilities for ablation studies, hyperparameter tuning, multi-run instability measurement, and visualization, addressing methodological gaps identified in prior work. We demonstrate PIDSMaker through concrete use cases and release it with preprocessed datasets and labels to support shared evaluation for the PIDS community.

</details>


### [457] [No More, No Less: Least-Privilege Language Models](https://arxiv.org/abs/2601.23157)
*Paulius Rauba,Dominykas Seputis,Patrikas Vanagas,Mihaela van der Schaar*

Main category: cs.CR

TL;DR: 本文受最小权限原则启发，定义最小权限语言模型，提出嵌套最小权限网络，挑战语言模型仅能在输出层控制的前提。


<details>
  <summary>Details</summary>
Motivation: 部署的语言模型未遵循最小权限原则，减少不必要的能力暴露能带来很大好处，但存在定义和机制上的障碍。

Method: 定义最小权限语言模型，将部署时控制形式化为监控 - 分配 - 执行栈，提出嵌套最小权限网络。

Result: 该控制旋钮产生了策略可用的权限 - 效用前沿，能在不同策略下有选择地抑制目标能力，且附带损害有限。

Conclusion: 提出了一种新的部署范式，挑战语言模型只能在输出层控制的前提。

Abstract: Least privilege is a core security principle: grant each request only the minimum access needed to achieve its goal. Deployed language models almost never follow it, instead being exposed through a single API endpoint that serves all users and requests. This gap exists not because least privilege would be unhelpful; deployments would benefit greatly from reducing unnecessary capability exposure. The real obstacle is definitional and mechanistic: what does "access" mean inside a language model, and how can we enforce it without retraining or deploying multiple models? We take inspiration from least privilege in computer systems and define a class of models called least-privilege language models, where privilege is reachable internal computation during the forward pass. In this view, lowering privilege literally shrinks the model's accessible function class, as opposed to denying access via learned policies. We formalize deployment-time control as a monitor-allocator-enforcer stack, separating (i) request-time signals, (ii) a decision rule that allocates privilege, and (iii) an inference-time mechanism that selects privilege. We then propose Nested Least-Privilege Networks, a shape-preserving, rank-indexed intervention that provides a smooth, reversible control knob. We show that this knob yields policy-usable privilege-utility frontiers and enables selective suppression of targeted capabilities with limited collateral degradation across various policies. Most importantly, we argue for a new deployment paradigm that challenges the premise that language models can only be controlled at the output level.

</details>
