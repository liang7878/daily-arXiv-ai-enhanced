<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 101]
- [cs.CE](#cs.CE) [Total: 4]
- [cs.DB](#cs.DB) [Total: 13]
- [cs.DC](#cs.DC) [Total: 36]
- [cs.DS](#cs.DS) [Total: 14]
- [cs.GT](#cs.GT) [Total: 11]
- [cs.IR](#cs.IR) [Total: 20]
- [cs.LG](#cs.LG) [Total: 185]
- [cs.NE](#cs.NE) [Total: 4]
- [cs.SE](#cs.SE) [Total: 59]
- [q-fin.RM](#q-fin.RM) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 5]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [stat.ML](#stat.ML) [Total: 12]
- [stat.CO](#stat.CO) [Total: 1]
- [cs.CL](#cs.CL) [Total: 96]
- [cs.IT](#cs.IT) [Total: 2]
- [math.ST](#math.ST) [Total: 1]
- [stat.AP](#stat.AP) [Total: 3]
- [hep-ph](#hep-ph) [Total: 1]
- [math.OC](#math.OC) [Total: 3]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.CG](#cs.CG) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 5]
- [cs.CV](#cs.CV) [Total: 73]
- [cs.HC](#cs.HC) [Total: 23]
- [quant-ph](#quant-ph) [Total: 5]
- [eess.AS](#eess.AS) [Total: 10]
- [econ.EM](#econ.EM) [Total: 3]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [eess.IV](#eess.IV) [Total: 6]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [q-fin.GN](#q-fin.GN) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [cs.RO](#cs.RO) [Total: 12]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 4]
- [cs.SC](#cs.SC) [Total: 1]
- [cs.NI](#cs.NI) [Total: 6]
- [astro-ph.SR](#astro-ph.SR) [Total: 2]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [cs.CC](#cs.CC) [Total: 2]
- [cs.MS](#cs.MS) [Total: 1]
- [math.SP](#math.SP) [Total: 1]
- [econ.TH](#econ.TH) [Total: 1]
- [math.NA](#math.NA) [Total: 4]
- [econ.GN](#econ.GN) [Total: 13]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.CR](#cs.CR) [Total: 27]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [physics.space-ph](#physics.space-ph) [Total: 1]
- [cs.SD](#cs.SD) [Total: 10]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.OS](#cs.OS) [Total: 3]
- [stat.ME](#stat.ME) [Total: 4]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.ET](#cs.ET) [Total: 2]
- [physics.chem-ph](#physics.chem-ph) [Total: 3]
- [cs.MA](#cs.MA) [Total: 4]
- [cs.SI](#cs.SI) [Total: 3]
- [cs.CY](#cs.CY) [Total: 9]
- [cs.DL](#cs.DL) [Total: 2]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [math.PR](#math.PR) [Total: 2]
- [eess.SP](#eess.SP) [Total: 4]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Look-Ahead-Bench: a Standardized Benchmark of Look-ahead Bias in Point-in-Time LLMs for Finance](https://arxiv.org/abs/2601.13770)
*Mostapha Benhenda*

Main category: cs.AI

TL;DR: 介绍Look - Ahead - Bench基准，评估金融工作流中LLMs的前瞻偏差，对开源和PiT - Inference模型评估后发现标准LLMs有显著偏差，Pitinf模型表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有的评估方法主要是通过问答测试内部前瞻知识，而本文需在实际场景中评估模型行为，区分真实预测能力和基于记忆的性能。

Method: 引入Look - Ahead - Bench基准，分析不同时间市场制度下的性能衰减，结合多个定量基线确定性能阈值。

Result: 标准LLMs有显著前瞻偏差，Pitinf模型随着规模增大，泛化和推理能力提升。

Conclusion: 为金融LLMs的时间偏差标准化评估奠定基础，提供识别适合实际部署模型的实用框架。

Abstract: We introduce Look-Ahead-Bench, a standardized benchmark measuring look-ahead bias in Point-in-Time (PiT) Large Language Models (LLMs) within realistic and practical financial workflows. Unlike most existing approaches that primarily test inner lookahead knowledge via Q\\&A, our benchmark evaluates model behavior in practical scenarios. To distinguish genuine predictive capability from memorization-based performance, we analyze performance decay across temporally distinct market regimes, incorporating several quantitative baselines to establish performance thresholds. We evaluate prominent open-source LLMs -- Llama 3.1 (8B and 70B) and DeepSeek 3.2 -- against a family of Point-in-Time LLMs (Pitinf-Small, Pitinf-Medium, and frontier-level model Pitinf-Large) from PiT-Inference. Results reveal significant lookahead bias in standard LLMs, as measured with alpha decay, unlike Pitinf models, which demonstrate improved generalization and reasoning abilities as they scale in size. This work establishes a foundation for the standardized evaluation of temporal bias in financial LLMs and provides a practical framework for identifying models suitable for real-world deployment. Code is available on GitHub: https://github.com/benstaf/lookaheadbench

</details>


### [2] [MIMIC-RD: Can LLMs differentially diagnose rare diseases in real-world clinical settings?](https://arxiv.org/abs/2601.11559)
*Zilal Eiz AlDin,John Wu,Jeffrey Paul Fung,Jennifer King,Mya Watts,Lauren ONeill,Adam Richard Cross,Jimeng Sun*

Main category: cs.AI

TL;DR: 现有评估大语言模型罕见病诊断的方法有局限，本文构建MIMIC - RD基准开展研究，发现当前大语言模型表现欠佳并给出改进方向。


<details>
  <summary>Details</summary>
Motivation: 现有评估大语言模型用于罕见病鉴别诊断的方法有依赖理想案例、用ICD代码计数不足等局限，需改进。

Method: 构建MIMIC - RD基准，先基于大语言模型挖掘，再经四名医学注释员验证实体为罕见病，在145名患者数据集上评估模型。

Result: 当前最先进的大语言模型在罕见病鉴别诊断中表现不佳。

Conclusion: 指出当前能力与临床需求有差距，并列出改进罕见病鉴别诊断的未来步骤。

Abstract: Despite rare diseases affecting 1 in 10 Americans, their differential diagnosis remains challenging. Due to their impressive recall abilities, large language models (LLMs) have been recently explored for differential diagnosis. Existing approaches to evaluating LLM-based rare disease diagnosis suffer from two critical limitations: they rely on idealized clinical case studies that fail to capture real-world clinical complexity, or they use ICD codes as disease labels, which significantly undercounts rare diseases since many lack direct mappings to comprehensive rare disease databases like Orphanet. To address these limitations, we explore MIMIC-RD, a rare disease differential diagnosis benchmark constructed by directly mapping clinical text entities to Orphanet. Our methodology involved an initial LLM-based mining process followed by validation from four medical annotators to confirm identified entities were genuine rare diseases. We evaluated various models on our dataset of 145 patients and found that current state-of-the-art LLMs perform poorly on rare disease differential diagnosis, highlighting the substantial gap between existing capabilities and clinical needs. From our findings, we outline several future steps towards improving differential diagnosis of rare diseases.

</details>


### [3] [A Mind Cannot Be Smeared Across Time](https://arxiv.org/abs/2601.11620)
*Michael Timothy Bennett*

Main category: cs.AI

TL;DR: 论文探讨机器意识与计算时间的关系，指出严格顺序计算的软件难以产生意识，意识归因需检查架构。


<details>
  <summary>Details</summary>
Motivation: 探究机器能否有意识，分析计算时间对意识的影响。

Method: 扩展Stack理论，引入精确的时间语义，区分强同步和弱同步假设，量化并发能力。

Result: 证明存在性时间实现不保留合取，神经生理证据使弱同步假设不太可信。

Conclusion: 在强同步下，严格顺序的软件无法产生需多部分同时贡献的意识，意识归因需检查架构。

Abstract: Whether machines can be conscious depends not only on what they compute, but \emph{when} they compute it. Most deployed artificial systems realise their functions via sequential or time-multiplexed updates. Conscious experience appears unified and simultaneous. I show that this difference matters formally. I augment Stack Theory with algebraic laws relating within time-window constraint satisfaction to conjunction. I introduce a precise temporal semantics over windowed trajectories $τ^{Δ,s}$ and prove that existential temporal realisation $\Diamond_Δ$ does not preserve conjunction. A system can realise all the ingredients of experience across time without ever instantiating the experienced conjunction itself. I then distinguish two postulates. StrongSync requires objective co-instantiation of the grounded conjunction within the window, while WeakSync permits temporal ``smearing''. I formalise concurrency-capacity to measure what is needed to satisfy StrongSync. Finally, I review neurophysiological evidence suggesting that consciousness depends on phase synchrony and effective connectivity, and that loss of consciousness is often associated with its breakdown. This evidence makes WeakSync less plausible. Under StrongSync, software consciousness on strictly sequential substrates is impossible for contents whose grounding requires two or more simultaneous contributors. The more parts from which simultaneous contribution required, the more concurrency capacity is required. The hardware matters. Consciousness attribution therefore requires architectural inspection, not just functional performance.

</details>


### [4] [Dynamical Systems Analysis Reveals Functional Regimes in Large Language Models](https://arxiv.org/abs/2601.11622)
*Hassan Ugail,Newton Howard*

Main category: cs.AI

TL;DR: 借鉴神经科学概念提出动态指标分析大语言模型内部动态的时间组织，在GPT - 2 - medium上评估，结果显示该指标能可靠表征模型不同功能状态差异。


<details>
  <summary>Details</summary>
Motivation: 大语言模型内部动态存在时间组织，但少有研究，多数可解释性方法关注静态表征或因果干预，忽略时间结构。

Method: 借鉴神经科学的时间整合和亚稳定性概念，提出复合动态指标；在GPT - 2 - medium上设置五种评估条件，并使用单因素方差分析。

Result: 结构化推理状态的指标始终高于重复、噪声和扰动状态，有显著统计差异和大效应量，结果对层选择、通道子采样和随机种子稳健。

Conclusion: 受神经科学启发的动态指标能可靠表征大语言模型不同功能状态下计算组织的差异，该指标仅捕捉形式动态属性。

Abstract: Large language models perform text generation through high-dimensional internal dynamics, yet the temporal organisation of these dynamics remains poorly understood. Most interpretability approaches emphasise static representations or causal interventions, leaving temporal structure largely unexplored. Drawing on neuroscience, where temporal integration and metastability are core markers of neural organisation, we adapt these concepts to transformer models and discuss a composite dynamical metric, computed from activation time-series during autoregressive generation. We evaluate this metric in GPT-2-medium across five conditions: structured reasoning, forced repetition, high-temperature noisy sampling, attention-head pruning, and weight-noise injection. Structured reasoning consistently exhibits elevated metric relative to repetitive, noisy, and perturbed regimes, with statistically significant differences confirmed by one-way ANOVA and large effect sizes in key comparisons. These results are robust to layer selection, channel subsampling, and random seeds. Our findings demonstrate that neuroscience-inspired dynamical metrics can reliably characterise differences in computational organisation across functional regimes in large language models. We stress that the proposed metric captures formal dynamical properties and does not imply subjective experience.

</details>


### [5] [Reasoning Stabilization Point: A Training-Time Signal for Stable Evidence and Shortcut Reliance](https://arxiv.org/abs/2601.11625)
*Sahil Rajesh Dhayalkar*

Main category: cs.AI

TL;DR: 提出一种训练时的可解释性观点，引入解释漂移和推理稳定点（RSP），可用于监控微调期间决策证据的演变和选择检查点。


<details>
  <summary>Details</summary>
Motivation: 微调预训练语言模型会改变模型依赖的证据，需要一种方法来监控证据演变。

Method: 提出训练时可解释性观点，跟踪微调期间的词级归因，定义解释漂移和RSP。

Result: 在多个模型和任务中，漂移在训练早期趋于稳定，验证准确率变化不大；在存在捷径的情况下，归因动态显示对捷径的依赖增加。

Conclusion: 解释漂移是一种简单、低成本的诊断方法，可监控微调期间决策证据的演变并选择稳定证据的检查点。

Abstract: Fine-tuning pretrained language models can improve task performance while subtly altering the evidence a model relies on. We propose a training-time interpretability view that tracks token-level attributions across finetuning epochs. We define explanation driftas the epoch-to-epoch change in normalized token attributions on a fixed probe set, and introduce the Reasoning Stabilization Point(RSP), the earliest epoch after which drift remains consistently low. RSP is computed from within-run drift dynamics and requires no tuning on out-of-distribution data. Across multiple lightweight transformer classifiers and benchmark classification tasks, drift typically collapses into a low, stable regime early in training, while validation accuracy continues to change only marginally. In a controlled shortcut setting with label-correlated trigger tokens, attribution dynamics expose increasing reliance on the shortcut even when validation accuracy remains competitive. Overall, explanation drift provides a simple, low-cost diagnostic for monitoring how decision evidence evolves during fine-tuning and for selecting checkpoints in a stable-evidence regime.

</details>


### [6] [PRISM: Learning Design Knowledge from Data for Stylistic Design Improvement](https://arxiv.org/abs/2601.11747)
*Huaxiaoyue Wang,Sunav Choudhary,Franck Dernoncourt,Yu Shen,Stefano Petrangeli*

Main category: cs.AI

TL;DR: 提出PRISM方法，利用设计数据构建知识库提升图形设计风格，实验和用户研究表明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 解决非专家基于自然语言指令进行图形设计风格改进的难题，且现有VLMs在设计风格上的预训练知识太笼统，与特定领域数据不匹配。

Method: 提出PRISM方法，分三个阶段构建和应用设计知识库：聚类高方差设计、总结聚类为可用设计知识、推理时检索相关知识。

Result: 在Crello数据集实验中PRISM平均排名1.49高于基线；用户研究表明设计师更青睐PRISM。

Conclusion: PRISM在图形设计风格改进方面表现良好，能有效提升设计风格贴合度。

Abstract: Graphic design often involves exploring different stylistic directions, which can be time-consuming for non-experts. We address this problem of stylistically improving designs based on natural language instructions. While VLMs have shown initial success in graphic design, their pretrained knowledge on styles is often too general and misaligned with specific domain data. For example, VLMs may associate minimalism with abstract designs, whereas designers emphasize shape and color choices. Our key insight is to leverage design data -- a collection of real-world designs that implicitly capture designer's principles -- to learn design knowledge and guide stylistic improvement. We propose PRISM (PRior-Informed Stylistic Modification) that constructs and applies a design knowledge base through three stages: (1) clustering high-variance designs to capture diversity within a style, (2) summarizing each cluster into actionable design knowledge, and (3) retrieving relevant knowledge during inference to enable style-aware improvement. Experiments on the Crello dataset show that PRISM achieves the highest average rank of 1.49 (closer to 1 is better) over baselines in style alignment. User studies further validate these results, showing that PRISM is consistently preferred by designers.

</details>


### [7] [Risk-Aware Human-in-the-Loop Framework with Adaptive Intrusion Response for Autonomous Vehicles](https://arxiv.org/abs/2601.11781)
*Dawood Wasif,Terrence J. Moore,Seunghyun Yoon,Hyuk Lim,Dan Dongseong Kim,Frederica F. Nelson,Jin-Hee Cho*

Main category: cs.AI

TL;DR: 提出风险感知人在环框架RAIL，融合多线索计算风险分数，依分数采取不同行动，耦合算法学习，在模拟环境中表现优于多种基线。


<details>
  <summary>Details</summary>
Motivation: 让自动驾驶车辆在长尾场景或网络物理入侵时保持安全有效。

Method: 将三个线索融合为入侵风险分数，超过阈值时结合特定线索的防护，用上下文多臂老虎机仲裁防护，耦合Soft Actor - Critic与风险优先回放及双奖励机制。

Result: 在MetaDrive和CARLA模拟环境中取得良好表现，超过多种基线；在攻击场景下提升成功率，降低脱离率和攻击成功率。

Conclusion: RAIL框架在自动驾驶中能有效应对风险，保障安全性和有效性。

Abstract: Autonomous vehicles must remain safe and effective when encountering rare long-tailed scenarios or cyber-physical intrusions during driving. We present RAIL, a risk-aware human-in-the-loop framework that turns heterogeneous runtime signals into calibrated control adaptations and focused learning. RAIL fuses three cues (curvature actuation integrity, time-to-collision proximity, and observation-shift consistency) into an Intrusion Risk Score (IRS) via a weighted Noisy-OR. When IRS exceeds a threshold, actions are blended with a cue-specific shield using a learned authority, while human override remains available; when risk is low, the nominal policy executes. A contextual bandit arbitrates among shields based on the cue vector, improving mitigation choices online. RAIL couples Soft Actor-Critic (SAC) with risk-prioritized replay and dual rewards so that takeovers and near misses steer learning while nominal behavior remains covered. On MetaDrive, RAIL achieves a Test Return (TR) of 360.65, a Test Success Rate (TSR) of 0.85, a Test Safety Violation (TSV) of 0.75, and a Disturbance Rate (DR) of 0.0027, while logging only 29.07 training safety violations, outperforming RL, safe RL, offline/imitation learning, and prior HITL baselines. Under Controller Area Network (CAN) injection and LiDAR spoofing attacks, it improves Success Rate (SR) to 0.68 and 0.80, lowers the Disengagement Rate under Attack (DRA) to 0.37 and 0.03, and reduces the Attack Success Rate (ASR) to 0.34 and 0.11. In CARLA, RAIL attains a TR of 1609.70 and TSR of 0.41 with only 8000 steps.

</details>


### [8] [FutureX-Pro: Extending Future Prediction to High-Value Vertical Domains](https://arxiv.org/abs/2601.12259)
*Jiashuo Liu,Siyuan Chen,Zaiyuan Wang,Zhiyuan Zeng,Jiacheng Guo,Liang Hu,Lingyue Yin,Suozhi Huang,Wenxin Hao,Yang Yang,Zerui Cheng,Zixin Yao,Lingyue Yin,Haoxin Liu,Jiayi Cheng,Yuzhen Li,Zezhong Ma,Bingjie Wang,Bingsen Qiu,Xiao Liu,Zeyang Zhang,Zijian Liu,Jinpeng Wang,Mingren Yin,Tianci He,Yali Liao,Yixiao Tian,Zhenwei Zhu,Anqi Dai,Ge Zhang,Jingkai Liu,Kaiyuan Zhang,Wenlong Wu,Xiang Gao,Xinjie Chen,Zhixin Yao,Zhoufutu Wen,B. Aditya Prakash,Jose Blanchet,Mengdi Wang,Nian Si,Wenhao Huang*

Main category: cs.AI

TL;DR: 本文基于FutureX引入FutureX - Pro，将代理式未来预测拓展到高价值垂直领域，对代理式大语言模型在基础预测任务上进行基准测试，发现通用推理与垂直应用精度间存在差距。


<details>
  <summary>Details</summary>
Motivation: 通用智能体在资本密集和安全关键领域的可靠性未充分探索，需要将代理式未来预测拓展到高价值垂直领域。

Method: 基于FutureX的无污染实时评估流程，对代理式大语言模型在金融、零售、公共卫生和自然灾害等领域的基础预测任务进行基准测试。

Result: 发现当前最先进的代理式大语言模型在通用推理和高价值垂直应用所需精度之间存在性能差距。

Conclusion: 通用推理和高价值垂直领域应用所需的精度存在差异。

Abstract: Building upon FutureX, which established a live benchmark for general-purpose future prediction, this report introduces FutureX-Pro, including FutureX-Finance, FutureX-Retail, FutureX-PublicHealth, FutureX-NaturalDisaster, and FutureX-Search. These together form a specialized framework extending agentic future prediction to high-value vertical domains. While generalist agents demonstrate proficiency in open-domain search, their reliability in capital-intensive and safety-critical sectors remains under-explored. FutureX-Pro targets four economically and socially pivotal verticals: Finance, Retail, Public Health, and Natural Disaster. We benchmark agentic Large Language Models (LLMs) on entry-level yet foundational prediction tasks -- ranging from forecasting market indicators and supply chain demands to tracking epidemic trends and natural disasters. By adapting the contamination-free, live-evaluation pipeline of FutureX, we assess whether current State-of-the-Art (SOTA) agentic LLMs possess the domain grounding necessary for industrial deployment. Our findings reveal the performance gap between generalist reasoning and the precision required for high-value vertical applications.

</details>


### [9] [A self-evolving multi-role collaborative framework with fine-grained difficulty guidance for innovative mathematical problem generation](https://arxiv.org/abs/2601.11792)
*Yifei Sun,Yongan Li,A. K. Qin,Sicheng Hou,Tamas Pflanzner*

Main category: cs.AI

TL;DR: 本文提出创新数学问题生成任务，构建带细粒度难度指导的自进化多角色协作框架，实验显示该方法在保持高正确率时显著提升生成问题的创新性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在数学问题生成任务中缺乏创新和辨别力，提出创新数学问题生成（IMPG）任务。

Method: 构建含采样器等的多角色协作机制；引入改进难度模型并采用DAPS算法；构建HSM3K - CN数据集，使用多阶段训练流程；通过蒸馏实现系统自进化。

Result: 相比基线模型，提出的方法在保持高正确率的同时，显著提升了生成问题的创新性。

Conclusion: 基于自进化、多角色协作框架和细粒度难度指导的方法能有效解决IMPG任务，提高生成问题的创新性。

Abstract: Mathematical problem generation (MPG) is a significant research direction in the field of intelligent education. In recent years, the rapid development of large language models (LLMs) has enabled new technological approaches to problem-generation tasks. Although existing LLMs can achieve high correctness rates, they generally lack innovation and exhibit poor discrimination. In this paper, we propose the task of innovative math problem generation (IMPG). To solve the IMPG task, this paper proposes a self-evolving, multi-role collaborative framework with fine-grained difficulty guidance. First, a multi-role collaborative mechanism comprising a sampler, generator, evaluator, state machine, and memory is constructed, ensuring the correctness of generated problems through iterative optimization informed by self-assessment and external feedback. Second, we introduce an improved difficulty model to quantify difficulty and provide fine-grained guidance. We adopt the data-driven association-guided path sampling (DAPS) algorithm to enhance the semantic rationality of sampled encodings. Third, we construct the HSM3K-CN dataset, which comprises high-quality high school math problems. A multi-stage training pipeline is adopted, incorporating continual pre-training (CPT), supervised fine-tuning (SFT), and group relative policy optimization (GRPO), to enhance the generation and evaluation capabilities of the base model. Finally, system self-evolution is achieved by transferring evaluation capabilities from the expert model to the apprentice model via distillation. Experiments show that, compared to baseline models, our proposed method significantly improves the innovation of the generated problems while maintaining a high correctness rate.

</details>


### [10] [RAG: A Random-Forest-Based Generative Design Framework for Uncertainty-Aware Design of Metamaterials with Complex Functional Response Requirements](https://arxiv.org/abs/2601.13233)
*Bolin Chen,Dex Doksoo Lee,Wei "Wayne'' Chen,Wei Chen*

Main category: cs.AI

TL;DR: 介绍一种基于随机森林的生成方法RAG，解决超材料功能响应逆设计难题并验证其有效性与数据效率。


<details>
  <summary>Details</summary>
Motivation: 现有超材料逆设计方法在处理高维功能响应时面临挑战，生成式设计存在数据需求大等问题。

Method: 引入RAG方法，利用随机森林处理小数据的特性，通过集成估计可能性，从条件似然中采样进行单步设计生成。

Result: 在声学和机械超材料设计中验证RAG，对比神经网络验证其数据效率。

Conclusion: RAG为涉及功能响应、昂贵模拟和复杂设计要求的逆设计提供轻量级、可靠途径。

Abstract: Metamaterials design for advanced functionality often entails the inverse design on nonlinear and condition-dependent responses (e.g., stress-strain relation and dispersion relation), which are described by continuous functions. Most existing design methods focus on vector-valued responses (e.g., Young's modulus and bandgap width), while the inverse design of functional responses remains challenging due to their high-dimensionality, the complexity of accommodating design requirements in inverse-design frameworks, and non-existence or non-uniqueness of feasible solutions. Although generative design approaches have shown promise, they are often data-hungry, handle design requirements heuristically, and may generate infeasible designs without uncertainty quantification. To address these challenges, we introduce a RAndom-forest-based Generative approach (RAG). By leveraging the small-data compatibility of random forests, RAG enables data-efficient predictions of high-dimensional functional responses. During the inverse design, the framework estimates the likelihood through the ensemble which quantifies the trustworthiness of generated designs while reflecting the relative difficulty across different requirements. The one-to-many mapping is addressed through single-shot design generation by sampling from the conditional likelihood. We demonstrate RAG on: 1) acoustic metamaterials with prescribed partial passbands/stopbands, and 2) mechanical metamaterials with targeted snap-through responses, using 500 and 1057 samples, respectively. Its data-efficiency is benchmarked against neural networks on a public mechanical metamaterial dataset with nonlinear stress-strain relations. Our framework provides a lightweight, trustworthy pathway to inverse design involving functional responses, expensive simulations, and complex design requirements, beyond metamaterials.

</details>


### [11] [Actionable Interpretability Must Be Defined in Terms of Symmetries](https://arxiv.org/abs/2601.12913)
*Pietro Barbiero,Mateo Espinosa Zarlenga,Francesco Giannini,Alberto Termine,Filippo Bonchi,Mateja Jamnik,Giuseppe Marra*

Main category: cs.AI

TL;DR: 指出AI可解释性研究因现有定义不可操作而存在问题，提出用对称性定义可操作性，假设四种对称性能达成多项目标。


<details>
  <summary>Details</summary>
Motivation: 解决现有AI可解释性定义不可操作，无法导出具体建模和推理规则的问题。

Method: 以对称性来定义可解释性的可操作性。

Result: 假设四种对称性可达成多项与可解释性相关的目标。

Conclusion: 现有AI可解释性定义有问题，用对称性定义可解决，四种对称性有重要作用。

Abstract: This paper argues that interpretability research in Artificial Intelligence is fundamentally ill-posed as existing definitions of interpretability are not *actionable*: they fail to provide formal principles from which concrete modelling and inferential rules can be derived. We posit that for a definition of interpretability to be actionable, it must be given in terms of *symmetries*. We hypothesise that four symmetries suffice to (i) motivate core interpretability properties, (ii) characterize the class of interpretable models, and (iii) derive a unified formulation of interpretable inference (e.g., alignment, interventions, and counterfactuals) as a form of Bayesian inversion.

</details>


### [12] [Multi-agent DRL-based Lane Change Decision Model for Cooperative Planning in Mixed Traffic](https://arxiv.org/abs/2601.11809)
*Zeyu Mu,Shangtong Zhang,B. Brian Park*

Main category: cs.AI

TL;DR: 本文提出混合多智能体车道变更决策模型，以提高CAV参与协作车队的比例，在微观仿真环境验证其效果优于基线规则模型。


<details>
  <summary>Details</summary>
Motivation: CAV部署初期分布稀疏，难以形成有效协作车队，需要提高CAV参与协作车队的比例并最大化相关效益。

Method: 采用QMIX框架，结合CNN处理交通数据（CNN - QMIX），设计轨迹规划器和模型预测控制器，在微观仿真环境下训练和评估模型。

Result: 模型能有效管理交通智能体数量波动，显著优于基线规则模型，协作车队比例最高提升26.2%。

Conclusion: 该模型在CAV部署早期有优化CAV协作和交通动态的潜力。

Abstract: Connected automated vehicles (CAVs) possess the ability to communicate and coordinate with one another, enabling cooperative platooning that enhances both energy efficiency and traffic flow. However, during the initial stage of CAV deployment, the sparse distribution of CAVs among human-driven vehicles reduces the likelihood of forming effective cooperative platoons. To address this challenge, this study proposes a hybrid multi-agent lane change decision model aimed at increasing CAV participation in cooperative platooning and maximizing its associated benefits. The proposed model employs the QMIX framework, integrating traffic data processed through a convolutional neural network (CNN-QMIX). This architecture addresses a critical issue in dynamic traffic scenarios by enabling CAVs to make optimal decisions irrespective of the varying number of CAVs present in mixed traffic. Additionally, a trajectory planner and a model predictive controller are designed to ensure smooth and safe lane-change execution. The proposed model is trained and evaluated within a microsimulation environment under varying CAV market penetration rates. The results demonstrate that the proposed model efficiently manages fluctuating traffic agent numbers, significantly outperforming the baseline rule-based models. Notably, it enhances cooperative platooning rates up to 26.2\%, showcasing its potential to optimize CAV cooperation and traffic dynamics during the early stage of deployment.

</details>


### [13] [POLARIS: Typed Planning and Governed Execution for Agentic AI in Back-Office Automation](https://arxiv.org/abs/2601.11816)
*Zahra Moslemi,Keerthi Koneru,Yen-Ting Lee,Sheethal Kumar,Ramesh Radhakrishnan*

Main category: cs.AI

TL;DR: 提出POLARIS框架用于企业后台工作流自动化，在文档中心金融任务上表现良好，为治理型代理人工智能提供基准。


<details>
  <summary>Details</summary>
Motivation: 通用多智能体设置难以满足企业后台工作流对可审计、符合政策和操作可预测的要求。

Method: 将自动化视为对大语言模型智能体的类型化计划合成和验证执行，通过规划器、推理模块、验证器等进行控制。

Result: 在SROIE数据集上微F1为0.81，在受控合成套件中异常路由精度达0.95 - 1.00且保留审计跟踪。

Conclusion: POLARIS为符合政策的代理人工智能提供了方法和基准参考。

Abstract: Enterprise back office workflows require agentic systems that are auditable, policy-aligned, and operationally predictable, capabilities that generic multi-agent setups often fail to deliver. We present POLARIS (Policy-Aware LLM Agentic Reasoning for Integrated Systems), a governed orchestration framework that treats automation as typed plan synthesis and validated execution over LLM agents. A planner proposes structurally diverse, type checked directed acyclic graphs (DAGs), a rubric guided reasoning module selects a single compliant plan, and execution is guarded by validator gated checks, a bounded repair loop, and compiled policy guardrails that block or route side effects before they occur. Applied to document centric finance tasks, POLARIS produces decision grade artifacts and full execution traces while reducing human intervention. Empirically, POLARIS achieves a micro F1 of 0.81 on the SROIE dataset and, on a controlled synthetic suite, achieves 0.95 to 1.00 precision for anomaly routing with preserved audit trails. These evaluations constitute an initial benchmark for governed Agentic AI. POLARIS provides a methodological and benchmark reference for policy-aligned Agentic AI. Keywords Agentic AI, Enterprise Automation, Back-Office Tasks, Benchmarks, Governance, Typed Planning, Evaluation

</details>


### [14] [AI Co-Scientist for Knowledge Synthesis in Medical Contexts: A Proof of Concept](https://arxiv.org/abs/2601.11825)
*Arya Rahgozar,Pouria Mortezaagha*

Main category: cs.AI

TL;DR: 本文提出基于PICOS的AI辅助科学家平台以解决生物医学研究浪费问题，评估显示其模型有高准确率，能发现研究冗余和空白，架构通用可减少研究浪费。


<details>
  <summary>Details</summary>
Motivation: 生物医学研究中存在冗余研究、报告不完整和传统证据合成工作流程可扩展性有限导致的研究浪费问题。

Method: 构建基于PICOS的AI平台，集成关系存储、向量语义检索和知识图谱；用双向长短期记忆网络和基于PubMedBERT微调的多任务分类器进行自动化PICOS合规性和研究设计分类；全文合成采用检索增强生成；用BERTopic识别主题结构等。

Result: Transformer模型研究设计分类准确率达95.7%，Bi - LSTM的PICOS合规性检测准确率达87%；检索增强生成在特定查询上表现更好；主题建模发现研究冗余和未充分探索领域。

Conclusion: 基于PICOS的可解释自然语言处理可提高证据合成的可扩展性、透明度和效率，架构通用可减少各生物医学学科的研究浪费。

Abstract: Research waste in biomedical science is driven by redundant studies, incomplete reporting, and the limited scalability of traditional evidence synthesis workflows. We present an AI co-scientist for scalable and transparent knowledge synthesis based on explicit formalization of Population, Intervention, Comparator, Outcome, and Study design (PICOS). The platform integrates relational storage, vector-based semantic retrieval, and a Neo4j knowledge graph. Evaluation was conducted on dementia-sport and non-communicable disease corpora. Automated PICOS compliance and study design classification from titles and abstracts were performed using a Bidirectional Long Short-Term Memory baseline and a transformer-based multi-task classifier fine-tuned from PubMedBERT. Full-text synthesis employed retrieval-augmented generation with hybrid vector and graph retrieval, while BERTopic was used to identify thematic structure, redundancy, and evidence gaps. The transformer model achieved 95.7% accuracy for study design classification with strong agreement against expert annotations, while the Bi-LSTM achieved 87% accuracy for PICOS compliance detection. Retrieval-augmented generation outperformed non-retrieval generation for queries requiring structured constraints, cross-study integration, and graph-based reasoning, whereas non-retrieval approaches remained competitive for high-level summaries. Topic modeling revealed substantial thematic redundancy and identified underexplored research areas. These results demonstrate that PICOS-aware and explainable natural language processing can improve the scalability, transparency, and efficiency of evidence synthesis. The proposed architecture is domain-agnostic and offers a practical framework for reducing research waste across biomedical disciplines.

</details>


### [15] [AgenticRed: Optimizing Agentic Systems for Automated Red-teaming](https://arxiv.org/abs/2601.13518)
*Jiayi Yuan,Jonathan Nöther,Natasha Jaques,Goran Radanović*

Main category: cs.AI

TL;DR: 本文提出自动化红队工具 AgenticRed，利用大模型上下文学习能力迭代设计和优化红队系统，在多个模型上表现优异，证明自动化系统设计对AI安全评估的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有自动化红队方法依赖人工指定工作流，存在人类偏见且探索设计空间成本高。

Method: 引入 AgenticRed 管道，利用大模型上下文学习能力，将红队问题视为系统设计问题，采用类似 Meta Agent Search 的进化选择方法迭代设计系统。

Result: AgenticRed 设计的红队系统在多个开源模型和闭源模型上表现优于现有方法，攻击成功率高，并具有良好的迁移性。

Conclusion: 自动化系统设计是一种强大的 AI 安全评估范式，能跟上快速发展的模型。

Abstract: While recent automated red-teaming methods show promise for systematically exposing model vulnerabilities, most existing approaches rely on human-specified workflows. This dependence on manually designed workflows suffers from human biases and makes exploring the broader design space expensive. We introduce AgenticRed, an automated pipeline that leverages LLMs' in-context learning to iteratively design and refine red-teaming systems without human intervention. Rather than optimizing attacker policies within predefined structures, AgenticRed treats red-teaming as a system design problem. Inspired by methods like Meta Agent Search, we develop a novel procedure for evolving agentic systems using evolutionary selection, and apply it to the problem of automatic red-teaming. Red-teaming systems designed by AgenticRed consistently outperform state-of-the-art approaches, achieving 96% attack success rate (ASR) on Llama-2-7B (36% improvement) and 98% on Llama-3-8B on HarmBench. Our approach exhibits strong transferability to proprietary models, achieving 100% ASR on GPT-3.5-Turbo and GPT-4o-mini, and 60% on Claude-Sonnet-3.5 (24% improvement). This work highlights automated system design as a powerful paradigm for AI safety evaluation that can keep pace with rapidly evolving models.

</details>


### [16] [Imandra CodeLogician: Neuro-Symbolic Reasoning for Precise Analysis of Software Logic](https://arxiv.org/abs/2601.11840)
*Hongyu Lin,Samer Abdallah,Makar Valentinov,Paul Brennan,Elijah Kagan,Christoph M. Wintersteiger,Denis Ignatovich,Grant Passmore*

Main category: cs.AI

TL;DR: 提出CodeLogician进行软件逻辑精确分析，引入code-logic-bench进行评估，证明神经符号集成对程序分析的重要性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型缺乏对程序行为进行精确、详尽数学推理的能力，现有基准存在不足。

Method: 提出CodeLogician这一神经符号代理，与ImandraX集成；引入code - logic - bench基准。

Result: 与仅使用大语言模型推理相比，使用CodeLogician增强后推理准确性有大幅提升，缩小41 - 47个百分点的差距。

Conclusion: 神经符号集成对于扩展程序分析至严谨、自主的软件理解至关重要。

Abstract: Large Language Models (LLMs) have shown strong performance on code understanding tasks, yet they fundamentally lack the ability to perform precise, exhaustive mathematical reasoning about program behavior. Existing benchmarks either focus on mathematical proof automation, largely disconnected from real-world software, or on engineering tasks that do not require semantic rigor.
  We present CodeLogician, a neurosymbolic agent for precise analysis of software logic, integrated with ImandraX, an industrial automated reasoning engine deployed in financial markets and safety-critical systems. Unlike prior approaches that use formal methods primarily to validate LLM outputs, CodeLogician uses LLMs to construct explicit formal models of software systems, enabling automated reasoning to answer rich semantic questions beyond binary verification outcomes.
  To rigorously evaluate mathematical reasoning about software logic, we introduce code-logic-bench, a benchmark targeting the middle ground between theorem proving and software engineering benchmarks. It measures reasoning correctness about program state spaces, control flow, coverage constraints, and edge cases, with ground truth defined via formal modeling and region decomposition.
  Comparing LLM-only reasoning against LLMs augmented with CodeLogician, formal augmentation yields substantial improvements, closing a 41-47 percentage point gap in reasoning accuracy. These results demonstrate that neurosymbolic integration is essential for scaling program analysis toward rigorous, autonomous software understanding.

</details>


### [17] [Human-AI Collaborative Inductive Thematic Analysis: AI Guided Analysis and Human Interpretive Authority](https://arxiv.org/abs/2601.11850)
*Matthew Nyaaba,Min SungEun,Mary Abiswin Apam,Kwame Owoahene Acheampong,Emmanuel Dwamena,Xiaoming Zhai*

Main category: cs.AI

TL;DR: 研究探讨应对访谈记录的定性分析，发现ITA - GPT可搭建分析流程并提升透明度，但解释权仍在人类研究者手中。


<details>
  <summary>Details</summary>
Motivation: 随着生成式人工智能在定性研究中的应用增加，需研究其对分析实践和解释权的影响。

Method: 三位经验丰富的定性研究人员借助ITA - GPT对加纳教师教育领域的访谈记录进行分析，以人类 - 人工智能合作归纳主题分析框架为指导，依据多种数据来源开展研究。

Result: ITA - GPT可作为程序支架构建分析工作流程并增强透明度，解释权在人类研究者，他们通过多种分析行为作出判断。

Conclusion: 证明了通过负责任的人机协作可以实现归纳主题分析。

Abstract: The increasing use of generative artificial intelligence (GenAI) in qualitative research raises important questions about analytic practice and interpretive authority. This study examines how researchers interact with an Inductive Thematic Analysis GPT (ITA-GPT), a purpose-built AI tool designed to support inductive thematic analysis through structured, semi-automated prompts aligned with reflexive thematic analysis and verbatim coding principles. Guided by a Human-Artificial Intelligence Collaborative Inductive Thematic Analysis (HACITA) framework, the study focuses on analytic process rather than substantive findings. Three experienced qualitative researchers conducted ITA-GPT assisted analyses of interview transcripts from education research in the Ghanaian teacher education context. The tool supported familiarization, verbatim in vivo coding, gerund-based descriptive coding, and theme development, while enforcing trace to text integrity, coverage checks, and auditability. Data sources included interaction logs, AI-generated tables, researcher revisions, deletions, insertions, comments, and reflexive memos. Findings show that ITA-GPT functioned as a procedural scaffold that structured analytic workflow and enhanced transparency. However, interpretive authority remained with human researchers, who exercised judgment through recurrent analytic actions including modification, deletion, rejection, insertion, and commenting. The study demonstrates how inductive thematic analysis is enacted through responsible human AI collaboration.

</details>


### [18] [MyGram: Modality-aware Graph Transformer with Global Distribution for Multi-modal Entity Alignment](https://arxiv.org/abs/2601.11885)
*Zhifei Li,Ziyue Qin,Xiangyu Luo,Xiaoju Hou,Yue Zhao,Miao Zhang,Zhifang Huang,Kui Xiao,Bing Yang*

Main category: cs.AI

TL;DR: 提出MyGram用于多模态实体对齐，在多数据集实验中表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有多模态实体对齐方法可能忽略各模态内的结构上下文信息，易受浅层特征干扰。

Method: 提出MyGram，开发模态扩散学习模块捕获模态内深层结构上下文信息并实现细粒度多模态融合，引入Gram Loss实现跨模态全局分布一致性。

Result: 在五个公开数据集上实验，MyGram在多个数据集的Hits@1指标上有显著提升，如FBDB15K最高提升4.8%，FBYG15K最高提升9.9%，DBP15K最高提升4.3%。

Conclusion: MyGram在多模态实体对齐任务中有效，能提升对齐效果。

Abstract: Multi-modal entity alignment aims to identify equivalent entities between two multi-modal Knowledge graphs by integrating multi-modal data, such as images and text, to enrich the semantic representations of entities. However, existing methods may overlook the structural contextual information within each modality, making them vulnerable to interference from shallow features. To address these challenges, we propose MyGram, a modality-aware graph transformer with global distribution for multi-modal entity alignment. Specifically, we develop a modality diffusion learning module to capture deep structural contextual information within modalities and enable fine-grained multi-modal fusion. In addition, we introduce a Gram Loss that acts as a regularization constraint by minimizing the volume of a 4-dimensional parallelotope formed by multi-modal features, thereby achieving global distribution consistency across modalities. We conduct experiments on five public datasets. Results show that MyGram outperforms baseline models, achieving a maximum improvement of 4.8% in Hits@1 on FBDB15K, 9.9% on FBYG15K, and 4.3% on DBP15K.

</details>


### [19] [AEMA: Verifiable Evaluation Framework for Trustworthy and Controlled Agentic LLM Systems](https://arxiv.org/abs/2601.11903)
*YenTing Lee,Keerthi Koneru,Zahra Moslemi,Sheethal Kumar,Ramesh Radhakrishnan*

Main category: cs.AI

TL;DR: 现有大语言模型多智能体系统评估方法有局限，提出AEMA框架，在模拟企业工作流中展现优势。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法在多智能体规模企业场景缺乏稳定性、可扩展性和自动化，需新评估框架。

Method: 提出AEMA框架，在人工监督下对异构智能体工作流进行多步骤评估规划、执行和聚合。

Result: 相比单一LLM评判，AEMA稳定性更高、与人类更契合且有可追溯记录，在模拟企业工作流中提供透明可复现评估途径。

Conclusion: AEMA为大语言模型多智能体系统提供了负责任的评估方法。

Abstract: Evaluating large language model (LLM)-based multi-agent systems remains a critical challenge, as these systems must exhibit reliable coordination, transparent decision-making, and verifiable performance across evolving tasks. Existing evaluation approaches often limit themselves to single-response scoring or narrow benchmarks, which lack stability, extensibility, and automation when deployed in enterprise settings at multi-agent scale. We present AEMA (Adaptive Evaluation Multi-Agent), a process-aware and auditable framework that plans, executes, and aggregates multi-step evaluations across heterogeneous agentic workflows under human oversight. Compared to a single LLM-as-a-Judge, AEMA achieves greater stability, human alignment, and traceable records that support accountable automation. Our results on enterprise-style agent workflows simulated using realistic business scenarios demonstrate that AEMA provides a transparent and reproducible pathway toward responsible evaluation of LLM-based multi-agent systems.
  Keywords Agentic AI, Multi-Agent Systems, Trustworthy AI, Verifiable Evaluation, Human Oversight

</details>


### [20] [LIBRA: Language Model Informed Bandit Recourse Algorithm for Personalized Treatment Planning](https://arxiv.org/abs/2601.11905)
*Junyu Cao,Ruijiang Gao,Esmaeil Keyvanshokooh,Jianhao Ma*

Main category: cs.AI

TL;DR: 本文提出整合算法追索、上下文老虎机和大语言模型的框架，用于高风险决策，开发GLRB和LIBRA算法，证明了相关保证与近最优性，实验表明算法性能良好。


<details>
  <summary>Details</summary>
Motivation: 支持高风险场景（如个性化医疗）下的序列决策。

Method: 提出追索老虎机问题，开发GLRB算法，提出结合大语言模型知识与老虎机学习的LIBRA算法。

Result: 确定了问题的下界，证明算法接近最优；实验显示GLRB和LIBRA在合成环境和真实案例中优于标准上下文老虎机和仅使用大语言模型的基准。

Conclusion: 追索感知、大语言模型辅助的老虎机算法在个性化高风险决策的大语言模型 - 老虎机协作方面很有前景。

Abstract: We introduce a unified framework that seamlessly integrates algorithmic recourse, contextual bandits, and large language models (LLMs) to support sequential decision-making in high-stakes settings such as personalized medicine. We first introduce the recourse bandit problem, where a decision-maker must select both a treatment action and a feasible, minimal modification to mutable patient features. To address this problem, we develop the Generalized Linear Recourse Bandit (GLRB) algorithm. Building on this foundation, we propose LIBRA, a Language Model-Informed Bandit Recourse Algorithm that strategically combines domain knowledge from LLMs with the statistical rigor of bandit learning. LIBRA offers three key guarantees: (i) a warm-start guarantee, showing that LIBRA significantly reduces initial regret when LLM recommendations are near-optimal; (ii) an LLM-effort guarantee, proving that the algorithm consults the LLM only $O(\log^2 T)$ times, where $T$ is the time horizon, ensuring long-term autonomy; and (iii) a robustness guarantee, showing that LIBRA never performs worse than a pure bandit algorithm even when the LLM is unreliable. We further establish matching lower bounds that characterize the fundamental difficulty of the recourse bandit problem and demonstrate the near-optimality of our algorithms. Experiments on synthetic environments and a real hypertension-management case study confirm that GLRB and LIBRA improve regret, treatment quality, and sample efficiency compared with standard contextual bandits and LLM-only benchmarks. Our results highlight the promise of recourse-aware, LLM-assisted bandit algorithms for trustworthy LLM-bandits collaboration in personalized high-stakes decision-making.

</details>


### [21] [Thinking Traps in Long Chain-of-Thought: A Measurable Study and Trap-Aware Adaptive Restart](https://arxiv.org/abs/2601.11940)
*Kang Chen,Fan Yu,Junjie Nian,Shihan Zhao,Zhuoka Feng,Zijun Yao,Heng Wang,Minshen Yu,Yixin Cao*

Main category: cs.AI

TL;DR: 文章指出Long - CoT虽可提升推理能力但会有思维陷阱，提出TAAR框架解决问题，且实验表明其能提升推理表现。


<details>
  <summary>Details</summary>
Motivation: Long - CoT扩展生成不保证推理正确性，存在思维陷阱问题，需寻找解决办法。

Method: 提出TAAR框架，训练诊断策略预测两个信号，推理时截断轨迹并自适应重启解码，对严重陷阱情况施加更强扰动。

Result: 在多个数学和科学推理基准测试中，TAAR在不微调基础模型参数的情况下提升了推理性能。

Conclusion: TAAR能有效解决Long - CoT的思维陷阱问题，提升模型推理能力。

Abstract: Scaling test-time compute via Long Chain-of-Thought (Long-CoT) significantly enhances reasoning capabilities, yet extended generation does not guarantee correctness: after an early wrong commitment, models may keep elaborating a self-consistent but incorrect prefix. Through fine-grained trajectory analysis, we identify Thinking Traps, prefix-dominant deadlocks where later reflection, alternative attempts, or verification fails to revise the root error. On a curated subset of DAPO-MATH, 89\% of failures exhibit such traps. To solve this problem, we introduce TAAR (Trap-Aware Adaptive Restart), a test-time control framework that trains a diagnostic policy to predict two signals from partial trajectories: a trap index for where to truncate and an escape probability for whether and how strongly to intervene. At inference time, TAAR truncates the trajectory before the predicted trap segment and adaptively restarts decoding; for severely trapped cases, it applies stronger perturbations, including higher-temperature resampling and an optional structured reboot suffix. Experiments on challenging mathematical and scientific reasoning benchmarks (AIME24, AIME25, GPQA-Diamond, HMMT25, BRUMO25) show that TAAR improves reasoning performance without fine-tuning base model parameters.

</details>


### [22] [Learn Like Humans: Use Meta-cognitive Reflection for Efficient Self-Improvement](https://arxiv.org/abs/2601.11974)
*Xinmeng Hou,Peiliang Gong,Bohao Qu,Wuqi Wang,Qing Guo,Yang Liu*

Main category: cs.AI

TL;DR: 提出MARS框架解决当前LLM智能体适应性受限和现有自改进框架计算成本高的问题，实验显示该框架表现优且降低计算开销


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体受静态人类设计提示限制适应性，现有自改进框架计算成本高

Method: 提出MARS框架，结合基于原则的反思和程序反思，将见解合成优化指令以系统改进推理逻辑

Result: 在六个基准测试中，MARS优于现有自进化系统，显著降低计算开销

Conclusion: MARS框架能实现高效自进化，解决现有问题

Abstract: While Large Language Models (LLMs) enable complex autonomous behavior, current agents remain constrained by static, human-designed prompts that limit adaptability. Existing self-improving frameworks attempt to bridge this gap but typically rely on inefficient, multi-turn recursive loops that incur high computational costs. To address this, we propose Metacognitive Agent Reflective Self-improvement (MARS), a framework that achieves efficient self-evolution within a single recurrence cycle. Inspired by educational psychology, MARS mimics human learning by integrating principle-based reflection (abstracting normative rules to avoid errors) and procedural reflection (deriving step-by-step strategies for success). By synthesizing these insights into optimized instructions, MARS allows agents to systematically refine their reasoning logic without continuous online feedback. Extensive experiments on six benchmarks demonstrate that MARS outperforms state-of-the-art self-evolving systems while significantly reducing computational overhead.

</details>


### [23] [Process In-Context Learning: Enhancing Mathematical Reasoning via Dynamic Demonstration Insertion](https://arxiv.org/abs/2601.11979)
*Ang Gao,Changshuo Zhang,Xiao Zhang,Deyang Li,Minjun Zhao,Fangchao Liu,Xinyu Zhang*

Main category: cs.AI

TL;DR: 现有ICL方法在数学推理中存在局限性，提出PICL动态演示集成框架，实验表明其能缓解推理中的困惑，优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有ICL方法在数学推理中存在静态使用演示的问题，无法适应多步推理中的动态困惑点，导致最终准确率下降。

Method: 提出Process In-Context Learning (PICL)框架，分两步：一是分析推理过程中的语义和熵，识别潜在困惑点并总结其核心特征；二是遇到困惑点时，从演示池中检索匹配的演示并插入推理过程。

Result: PICL通过缓解推理中的困惑，性能优于基线方法。

Conclusion: 自适应演示插入在复杂数学推理中具有重要价值。

Abstract: In-context learning (ICL) has proven highly effective across diverse large language model (LLM) tasks. However, its potential for enhancing tasks that demand step-by-step logical deduction, such as mathematical reasoning, remains underexplored. A core limitation of existing ICL approaches is their static use of demonstrations: examples are pre-selected before inference and remain fixed, failing to adapt to the dynamic confusion points that often arise during multi-step reasoning such as ambiguous calculations or logical gaps. These unresolved confusion points can lead to cascading errors that degrade final accuracy. To tackle this issue, we propose Process In-Context Learning (PICL), a dynamic demonstration integration framework designed to boost mathematical reasoning by responding to real-time inference needs. PICL operates in two stages: 1)~it identifies potential confusion points by analyzing semantics and entropy in the reasoning process and summarizes their core characteristics; 2)~upon encountering these points, it retrieves relevant demonstrations from the demonstration pool that match the confusion context and inserts them directly into the ongoing reasoning process to guide subsequent steps. Experiments show that PICL outperforms baseline methods by mitigating mid-inference confusion, highlighting the value of adaptive demonstration insertion in complex mathematical reasoning.

</details>


### [24] [Kernel-Based Learning of Safety Barriers](https://arxiv.org/abs/2601.12002)
*Oliver Schön,Zhengang Zhong,Sadegh Soudjani*

Main category: cs.AI

TL;DR: 提出数据驱动方法用于黑盒系统安全验证与综合，利用控制障碍证书和条件均值嵌入，解决传统工具难题，通过有限傅里叶展开优化，在案例中得到验证。


<details>
  <summary>Details</summary>
Motivation: AI算法在安全关键应用中的集成引发对满足严格安全标准能力的担忧，传统形式安全验证工具难以应对AI驱动系统的黑盒特性与现实应用复杂性。

Method: 采用控制障碍证书概念，从系统轨迹中学习证书；用条件均值嵌入将数据嵌入RKHS并构建模糊集；利用有限傅里叶展开将半无限优化问题转化为线性规划。

Result: 提供将方法应用于更广泛时间逻辑规范的理论结果，通过快速傅里叶变换有效生成松弛问题，得到可扩展且分布鲁棒的安全验证框架。

Conclusion: 工作突破了系统动力学和不确定性的限制假设，在两个案例中得到验证。

Abstract: The rapid integration of AI algorithms in safety-critical applications such as autonomous driving and healthcare is raising significant concerns about the ability to meet stringent safety standards. Traditional tools for formal safety verification struggle with the black-box nature of AI-driven systems and lack the flexibility needed to scale to the complexity of real-world applications. In this paper, we present a data-driven approach for safety verification and synthesis of black-box systems with discrete-time stochastic dynamics. We employ the concept of control barrier certificates, which can guarantee safety of the system, and learn the certificate directly from a set of system trajectories. We use conditional mean embeddings to embed data from the system into a reproducing kernel Hilbert space (RKHS) and construct an RKHS ambiguity set that can be inflated to robustify the result to out-of-distribution behavior. We provide the theoretical results on how to apply the approach to general classes of temporal logic specifications beyond safety. For the data-driven computation of safety barriers, we leverage a finite Fourier expansion to cast a typically intractable semi-infinite optimization problem as a linear program. The resulting spectral barrier allows us to leverage the fast Fourier transform to generate the relaxed problem efficiently, offering a scalable yet distributionally robust framework for verifying safety. Our work moves beyond restrictive assumptions on system dynamics and uncertainty, as demonstrated on two case studies including a black-box system with a neural network controller.

</details>


### [25] [Are LLMs Ready for TOON? Benchmarking Structural Correctness-Sustainability Trade-offs in Novel Structured Output Formats](https://arxiv.org/abs/2601.12014)
*Elio Masciari,Vincenzo Moscato,Enea Vincenzo Napolitano,Gian Marco Orlando,Marco Perillo,Diego Russo*

Main category: cs.AI

TL;DR: 本文提出可持续性评估框架和统一指标，对TOON格式与常用表示法进行基准测试，揭示输出紧凑性、正确性和碳排放间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试多关注大语言模型结构化输出的正确性，而忽略了不同输出格式推理过程的环境影响，因此应从环境效率角度评估结构化输出格式。

Method: 引入可持续性评估框架，提出Environment - Aware Generation Correctness Score (GCS_env) 统一指标，对多种大语言模型下的TOON格式与JSON、XML、YAML进行基准测试。

Result: TOON输出更紧凑、排放更低，但模型缺乏原生支持时结构正确性较低；模型容量增加可缩小正确性差距；环境感知评分会根据部署优先级改变格式排名。

Conclusion: 需要进行包含可持续性的基准测试，像TOON这样的紧凑表示法在大规模、注重碳排放的大语言模型部署中有实际优势。

Abstract: Large Language Models (LLMs) are increasingly required to generate structured, machine-readable outputs for downstream systems. While recent benchmarks have focused on evaluating the structural correctness of such outputs, the environmental impact of inference for different output formats has largely been overlooked. In this paper, we argue that structured output formats should be assessed not only in terms of correctness, but also with respect to their environmental efficiency. To this end, we introduce a sustainability-aware evaluation framework for structured generation that measures token usage, generation time, and estimated carbon emissions. Within this framework, we propose the Environment-Aware Generation Correctness Score (GCS_env), a unified metric that integrates structural correctness with carbon-aware efficiency. Using this framework, we systematically benchmark the novel TOON format against established representations (JSON, XML, YAML) across multiple LLMs spanning different architectures and parameter scales.
  Our results reveal a consistent trade-off: TOON yields markedly more compact outputs and lower emissions, but lower structural correctness when models lack native support. We show that increased model capacity reduces this gap and that environment-aware scoring can shift format rankings depending on deployment priorities. highlighting the need for sustainability-inclusive benchmarking and provides empirical evidence that compact representations such as TOON can offer practical advantages in large-scale, carbon-conscious LLM deployments.

</details>


### [26] [A Multi-Agent System for Generating Actionable Business Advice](https://arxiv.org/abs/2601.12024)
*Kartikey Singh Bhandari,Tanish Jain,Archit Agrawal,Dhruv Kumar,Praveen Kumar,Pratik Narang*

Main category: cs.AI

TL;DR: 提出基于多智能体的大语言模型框架将评论语料转化为业务建议，实验显示该框架在多项指标上表现优于单模型基线。


<details>
  <summary>Details</summary>
Motivation: 现有分析方法局限于描述性任务，大语言模型生成的建议缺乏准确性和推理深度，需要有更好的方法将客户评论转化为可操作的业务建议。

Method: 提出含聚类选代表性评论、生成建议、迭代评估、基于可行性排序四个组件的多智能体LLM框架，结合语料蒸馏和反馈驱动的建议优化。

Result: 在三个服务领域和多个模型族的实验中，该框架在可操作性、特异性和非冗余性上始终优于单模型基线，中型模型接近大模型框架性能。

Conclusion: 所提出的多智能体LLM框架能有效将大规模评论语料转化为可操作、具体且实用的业务建议。

Abstract: Customer reviews contain rich signals about product weaknesses and unmet user needs, yet existing analytic methods rarely move beyond descriptive tasks such as sentiment analysis or aspect extraction. While large language models (LLMs) can generate free-form suggestions, their outputs often lack accuracy and depth of reasoning. In this paper, we present a multi-agent, LLM-based framework for prescriptive decision support, which transforms large scale review corpora into actionable business advice. The framework integrates four components: clustering to select representative reviews, generation of advices, iterative evaluation, and feasibility based ranking. This design couples corpus distillation with feedback driven advice refinement to produce outputs that are specific, actionable, and practical. Experiments across three service domains and multiple model families show that our framework consistently outperform single model baselines on actionability, specificity, and non-redundancy, with medium sized models approaching the performance of large model frameworks.

</details>


### [27] [ARC: Active and Reflection-driven Context Management for Long-Horizon Information Seeking Agents](https://arxiv.org/abs/2601.12030)
*Yilun Yao,Shan Huang,Elsie Dai,Zhewen Tan,Zhenyu Duan,Shousheng Jia,Yanbing Jiang,Tong Yang*

Main category: cs.AI

TL;DR: 现有大语言模型在处理长交互历史时性能下降，提出ARC框架将上下文管理作为主动反思驱动过程，实验显示其优于被动上下文压缩方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理长交互历史时出现上下文腐烂问题，现有方法将上下文视为静态，无法解决早期错误和重点偏差问题。

Method: 提出ARC框架，通过反思驱动的监控和修订，将上下文管理作为主动、反思驱动的动态过程。

Result: 在具有挑战性的长时信息检索基准测试中，ARC始终优于被动上下文压缩方法，使用Qwen2.5 - 32B - Instruct在BrowseComp - ZH上准确率绝对提升达11%。

Conclusion: ARC框架能有效解决大语言模型上下文腐烂问题，提升长时信息检索性能。

Abstract: Large language models are increasingly deployed as research agents for deep search and long-horizon information seeking, yet their performance often degrades as interaction histories grow. This degradation, known as context rot, reflects a failure to maintain coherent and task-relevant internal states over extended reasoning horizons. Existing approaches primarily manage context through raw accumulation or passive summarization, treating it as a static artifact and allowing early errors or misplaced emphasis to persist. Motivated by this perspective, we propose ARC, which is the first framework to systematically formulate context management as an active, reflection-driven process that treats context as a dynamic internal reasoning state during execution. ARC operationalizes this view through reflection-driven monitoring and revision, allowing agents to actively reorganize their working context when misalignment or degradation is detected. Experiments on challenging long-horizon information-seeking benchmarks show that ARC consistently outperforms passive context compression methods, achieving up to an 11% absolute improvement in accuracy on BrowseComp-ZH with Qwen2.5-32B-Instruct.

</details>


### [28] [Abstract Argumentation with Subargument Relations](https://arxiv.org/abs/2601.12038)
*Beishui Liao*

Main category: cs.AI

TL;DR: 研究带有显式子论证关系的抽象论证框架，分析其与攻击关系的交互及对语义属性的影响。


<details>
  <summary>Details</summary>
Motivation: 现有抽象论证框架无法表示结构化论证形式中的子论证关系，现有扩展也不能捕捉子论证的特性及与攻击的交互。

Method: 将子论证关系作为基本关系与攻击关系一同研究，分析子论证关系与攻击的交互以及对基本语义属性的影响。

Result: 该框架对结构信息进行了原则性抽象。

Conclusion: 该框架阐明了子论证在抽象可接受性推理中的作用。

Abstract: Dung's abstract argumentation framework characterises argument acceptability solely via an attack relation, deliberately abstracting from the internal structure of arguments. While this level of abstraction has enabled a rich body of results, it limits the ability to represent structural dependencies that are central in many structured argumentation formalisms, in particular subargument relations. Existing extensions, including bipolar argumentation frameworks, introduce support relations, but these do not capture the asymmetric and constitutive nature of subarguments or their interaction with attacks. In this paper, we study abstract argumentation frameworks enriched with an explicit subargument relation, treated alongside attack as a basic relation. We analyse how subargument relations interact with attacks and examine their impact on fundamental semantic properties. This framework provides a principled abstraction of structural information and clarifies the role of subarguments in abstract acceptability reasoning.

</details>


### [29] [Partial Reasoning in Language Models: Search and Refinement Guided by Uncertainty](https://arxiv.org/abs/2601.12040)
*Murilo da Luz,Bruno Brandão,Luana Martins,Gustavo Oliveira,Bryan de Oliveira,Luckeciano Melo,Telma Soares*

Main category: cs.AI

TL;DR: 介绍PREGU方法，通过监测熵触发局部搜索，实验表明熵可作为有效信号提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多步推理场景存在局限，尤其是数学和逻辑推理方面。

Method: 引入PREGU，监测输出分布熵，熵超阈值时暂停生成，在隐空间进行局部搜索，用Soft Reasoning方法选择答案。

Result: 在四个推理基准测试中，使用LLaMA - 3 - 8B、Mistral - 7B和Qwen2 - 7B的实验性能优于或类似于Soft Reasoning。

Conclusion: 熵可作为有效信号，在推理过程中触发选择性细化。

Abstract: The use of Large Language Models (LLMs) for reasoning and planning tasks has drawn increasing attention in Artificial Intelligence research. Despite their remarkable progress, these models still exhibit limitations in multi-step inference scenarios, particularly in mathematical and logical reasoning. We introduce PREGU (Partial Reasoning Guided by Uncertainty). PREGU monitors the entropy of the output distribution during autoregressive generation and halts the process whenever entropy exceeds a defined threshold, signaling uncertainty. From that point, a localized search is performed in the latent space to refine the partial reasoning and select the most coherent answer, using the Soft Reasoning method. Experiments conducted with LLaMA-3-8B, Mistral-7B, and Qwen2-7B across four reasoning benchmarks (GSM8K, GSM-Hard, SVAMP, and StrategyQA) showed performance greater than or similar to Soft Reasoning, indicating that entropy can serve as an effective signal to trigger selective refinement during reasoning.

</details>


### [30] [UniMo: Unified Motion Generation and Understanding with Chain of Thought](https://arxiv.org/abs/2601.12126)
*Guocun Wang,Kenkun Liu,Jing Lin,Guorui Song,Jian Li,Xiaoguang Han*

Main category: cs.AI

TL;DR: 提出UniMo框架解决现有3D人体运动生成和理解方法的局限，实验显示其性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有3D人体运动生成和理解方法可解释性有限，基于大语言模型的统一框架存在语义对齐和任务连贯性问题，且LLM的预测范式不适用于运动序列。

Method: 提出UniMo框架，通过监督微调将运动语言信息和可解释的思维链推理融入LLM，引入基于Group Relative Policy Optimization的强化学习作为后训练策略。

Result: UniMo显著优于现有统一和特定任务模型。

Conclusion: UniMo在运动生成和理解方面达到了最先进的性能。

Abstract: Existing 3D human motion generation and understanding methods often exhibit limited interpretability, restricting effective mutual enhancement between these inherently related tasks. While current unified frameworks based on large language models (LLMs) leverage linguistic priors, they frequently encounter challenges in semantic alignment and task coherence. Moreover, the next-token prediction paradigm in LLMs is ill-suited for motion sequences, causing cumulative prediction errors. To address these limitations, we propose UniMo, a novel framework that integrates motion-language information and interpretable chain of thought (CoT) reasoning into the LLM via supervised fine-tuning (SFT). We further introduce reinforcement learning with Group Relative Policy Optimization (GRPO) as a post-training strategy that optimizes over groups of tokens to enforce structural correctness and semantic alignment, mitigating cumulative errors in motion token prediction. Extensive experiments demonstrate that UniMo significantly outperforms existing unified and task-specific models, achieving state-of-the-art performance in both motion generation and understanding.

</details>


### [31] [DriveSafe: A Hierarchical Risk Taxonomy for Safety-Critical LLM-Based Driving Assistants](https://arxiv.org/abs/2601.12138)
*Abhishek Kumar,Riya Tapwal,Carsten Maple*

Main category: cs.AI

TL;DR: 本文提出DriveSafe风险分类法刻画基于大语言模型的驾驶助手安全关键故障模式，评估六大模型对不安全查询拒绝行为，揭示通用安全校准在驾驶场景的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型安全分类法和评估框架为通用型，未涵盖真实驾驶场景特定风险，而大语言模型集成于车载数字助手时，不安全等回复会有严重后果。

Method: 引入DriveSafe四层风险分类法，包含129个细粒度原子风险类别，基于现实驾驶法规和安全原则并经专家评审；评估六大广泛部署大语言模型对构造提示的拒绝行为。

Result: 评估的模型常不能恰当拒绝不安全或不合规驾驶相关查询。

Conclusion: 通用安全校准在驾驶场景存在局限性。

Abstract: Large Language Models (LLMs) are increasingly integrated into vehicle-based digital assistants, where unsafe, ambiguous, or legally incorrect responses can lead to serious safety, ethical, and regulatory consequences. Despite growing interest in LLM safety, existing taxonomies and evaluation frameworks remain largely general-purpose and fail to capture the domain-specific risks inherent to real-world driving scenarios. In this paper, we introduce DriveSafe, a hierarchical, four-level risk taxonomy designed to systematically characterize safety-critical failure modes of LLM-based driving assistants. The taxonomy comprises 129 fine-grained atomic risk categories spanning technical, legal, societal, and ethical dimensions, grounded in real-world driving regulations and safety principles and reviewed by domain experts. To validate the safety relevance and realism of the constructed prompts, we evaluate their refusal behavior across six widely deployed LLMs. Our analysis shows that the evaluated models often fail to appropriately refuse unsafe or non-compliant driving-related queries, underscoring the limitations of general-purpose safety alignment in driving contexts.

</details>


### [32] [TIDE: A Trace-Informed Depth-First Exploration for Planning with Temporally Extended Goals](https://arxiv.org/abs/2601.12141)
*Yuliia Suprun,Khen Elimelech,Lydia E. Kavraki,Moshe Y. Vardi*

Main category: cs.AI

TL;DR: 介绍一种名为TIDE的方法解决含LTLf的时间扩展目标任务规划问题，实验证明其有良好表现。


<details>
  <summary>Details</summary>
Motivation: 传统LTLf任务规划方法缺乏引导时间目标搜索的启发式策略，需改进。

Method: 将时间问题分解为多个可编程解决的可达 - 避免子问题，利用代价驱动启发式选择自动机轨迹，通过自适应回溯机制恢复失败计划。

Result: TIDE在实验中取得了较好的性能表现。

Conclusion: TIDE是解决时间扩展目标规划问题的有价值规划方法。

Abstract: Task planning with temporally extended goals (TEGs) is a critical challenge in AI and robotics, enabling agents to achieve complex sequences of objectives over time rather than addressing isolated, immediate tasks. Linear Temporal Logic on finite traces (LTLf ) provides a robust formalism for encoding these temporal goals. Traditional LTLf task planning approaches often transform the temporal planning problem into a classical planning problem with reachability goals, which are then solved using off-the-shelf planners. However, these methods often lack informed heuristics to provide a guided search for temporal goals. We introduce TIDE (Trace-Informed Depth-first Exploration), a novel approach that addresses this limitation by decomposing a temporal problem into a sequence of smaller, manageable reach-avoid sub-problems, each solvable using an off-the-shelf planner. TIDE identifies and prioritizes promising automaton traces within the domain graph, using cost-driven heuristics to guide exploration. Its adaptive backtracking mechanism systematically recovers from failed plans by recalculating costs and penalizing infeasible transitions, ensuring completeness and efficiency. Experimental results demonstrate that TIDE achieves promising performance and is a valuable addition to the portfolio of planning methods for temporally extended goals.

</details>


### [33] [Optimal Power Allocation and Sub-Optimal Channel Assignment for Downlink NOMA Systems Using Deep Reinforcement Learning](https://arxiv.org/abs/2601.12242)
*WooSeok Kim,Jeonghoon Lee,Sangho Kim,Taesun An,WonMin Lee,Dowon Kim,Kyungseop Shin*

Main category: cs.AI

TL;DR: 本文提出结合重放记忆和在线策略算法的深度强化学习框架，用于NOMA系统资源分配，并进行模拟评估。


<details>
  <summary>Details</summary>
Motivation: 物联网发展导致网络资源稀缺，需优化网络资源利用，现有NOMA系统有局限性且信道分配问题待研究。

Method: 提出结合重放记忆与在线策略算法的深度强化学习框架，对NOMA系统进行网络资源分配。

Result: 文中未提及具体结果。

Conclusion: 文中未提及具体结论。

Abstract: In recent years, Non-Orthogonal Multiple Access (NOMA) system has emerged as a promising candidate for multiple access frameworks due to the evolution of deep machine learning, trying to incorporate deep machine learning into the NOMA system. The main motivation for such active studies is the growing need to optimize the utilization of network resources as the expansion of the internet of things (IoT) caused a scarcity of network resources. The NOMA addresses this need by power multiplexing, allowing multiple users to access the network simultaneously. Nevertheless, the NOMA system has few limitations. Several works have proposed to mitigate this, including the optimization of power allocation known as joint resource allocation(JRA) method, and integration of the JRA method and deep reinforcement learning (JRA-DRL). Despite this, the channel assignment problem remains unclear and requires further investigation. In this paper, we propose a deep reinforcement learning framework incorporating replay memory with an on-policy algorithm, allocating network resources in a NOMA system to generalize the learning. Also, we provide extensive simulations to evaluate the effects of varying the learning rate, batch size, type of model, and the number of features in the state.

</details>


### [34] [Improving Large Molecular Language Model via Relation-aware Multimodal Collaboration](https://arxiv.org/abs/2601.12256)
*Jinyoung Park,Minseong Bae,Jeehye Na,Hyunwoo J. Kim*

Main category: cs.AI

TL;DR: 现有大语言分子模型有幻觉和鲁棒性不足问题，本文提出CoLLaMo，有新评估方法，实验显示其提升了模型能力并在多任务表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有大语言分子模型因未充分整合多种分子模态，存在幻觉和鲁棒性不足问题，需改进。

Method: 提出CoLLaMo，配备多级分子模态协作投影器，采用关系感知模态协作注意力机制；提出以分子为中心的新自动评估方法。

Result: CoLLaMo增强了大语言分子模型的分子模态泛化能力，在多个任务上取得最佳性能。

Conclusion: CoLLaMo能有效解决现有大语言分子模型的问题，提升其性能。

Abstract: Large language models (LLMs) have demonstrated their instruction-following capabilities and achieved powerful performance on various tasks. Inspired by their success, recent works in the molecular domain have led to the development of large molecular language models (LMLMs) that integrate 1D molecular strings or 2D molecular graphs into the language models. However, existing LMLMs often suffer from hallucination and limited robustness, largely due to inadequate integration of diverse molecular modalities such as 1D sequences, 2D molecular graphs, and 3D conformations. To address these limitations, we propose CoLLaMo, a large language model-based molecular assistant equipped with a multi-level molecular modality-collaborative projector. The relation-aware modality-collaborative attention mechanism in the projector facilitates fine-grained and relation-guided information exchange between atoms by incorporating 2D structural and 3D spatial relations. Furthermore, we present a molecule-centric new automatic measurement, including a hallucination assessment metric and GPT-based caption quality evaluation to address the limitations of token-based generic evaluation metrics (i.e., BLEU) widely used in assessing molecular comprehension of LMLMs. Our extensive experiments demonstrate that our CoLLaMo enhances the molecular modality generalization capabilities of LMLMs, achieving the best performance on multiple tasks, including molecule captioning, computed property QA, descriptive property QA, motif counting, and IUPAC name prediction.

</details>


### [35] [Docs2Synth: A Synthetic Data Trained Retriever Framework for Scanned Visually Rich Documents Understanding](https://arxiv.org/abs/2601.12260)
*Yihao Ding,Qiang Sun,Puzhen Wu,Sirui Li,Siwen Luo,Wei Liu*

Main category: cs.AI

TL;DR: 本文针对监管领域文档理解挑战，提出Docs2Synth合成监督框架，可提升领域文档理解性能且无需人工标注。


<details>
  <summary>Details</summary>
Motivation: 监管领域文档理解存在模型缺乏标注和预训练模型难跟上领域知识更新问题，现有多模态大语言模型有幻觉问题，判别式视觉语言预训练模型需高额标注成本。

Method: 引入Docs2Synth框架，自动处理原始文档集，通过基于代理的系统生成和验证多样QA对，训练轻量级视觉检索器，推理时检索器与MLLM通过迭代检索-生成循环协作。

Result: 在多个VRDU基准测试中，Docs2Synth可大幅提升基础能力和领域泛化能力，且无需人工标注。

Conclusion: Docs2Synth框架能有效解决监管领域文档理解问题，以Python包形式提供，便于在多种现实场景中即插即用。

Abstract: Document understanding (VRDU) in regulated domains is particularly challenging, since scanned documents often contain sensitive, evolving, and domain specific knowledge. This leads to two major challenges: the lack of manual annotations for model adaptation and the difficulty for pretrained models to stay up-to-date with domain-specific facts. While Multimodal Large Language Models (MLLMs) show strong zero-shot abilities, they still suffer from hallucination and limited domain grounding. In contrast, discriminative Vision-Language Pre-trained Models (VLPMs) provide reliable grounding but require costly annotations to cover new domains. We introduce Docs2Synth, a synthetic-supervision framework that enables retrieval-guided inference for private and low-resource domains. Docs2Synth automatically processes raw document collections, generates and verifies diverse QA pairs via an agent-based system, and trains a lightweight visual retriever to extract domain-relevant evidence. During inference, the retriever collaborates with an MLLM through an iterative retrieval--generation loop, reducing hallucination and improving response consistency. We further deliver Docs2Synth as an easy-to-use Python package, enabling plug-and-play deployment across diverse real-world scenarios. Experiments on multiple VRDU benchmarks show that Docs2Synth substantially enhances grounding and domain generalization without requiring human annotations.

</details>


### [36] [ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents](https://arxiv.org/abs/2601.12294)
*Dawei Li,Yuguang Yao,Zhen Tan,Huan Liu,Ruocheng Guo*

Main category: cs.AI

TL;DR: 本文提出大规模基准ToolPRMBench评估工具使用代理的过程奖励模型，通过离线和在线采样构造测试用例，并经多LLM验证管道确保数据质量，实验揭示了不同PRM有效性差异。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对工具使用场景下过程奖励模型（PRMs）系统可靠的评估基准，需提出评估方法。

Method: 基于代表性工具使用基准构建ToolPRMBench，将代理轨迹转换为测试用例，用离线采样分离单步错误、在线采样捕捉多步失败，采用多LLM验证管道降低标签噪声。

Result: 在ToolPRMBench上对大语言模型、通用PRMs和工具专用PRMs进行实验，揭示了PRM有效性的明显差异。

Conclusion: 突出了工具使用专用PRMs的潜力，代码和数据将公开。

Abstract: Reward-guided search methods have demonstrated strong potential in enhancing tool-using agents by effectively guiding sampling and exploration over complex action spaces. As a core design, those search methods utilize process reward models (PRMs) to provide step-level rewards, enabling more fine-grained monitoring. However, there is a lack of systematic and reliable evaluation benchmarks for PRMs in tool-using settings. In this paper, we introduce ToolPRMBench, a large-scale benchmark specifically designed to evaluate PRMs for tool-using agents. ToolPRMBench is built on top of several representative tool-using benchmarks and converts agent trajectories into step-level test cases. Each case contains the interaction history, a correct action, a plausible but incorrect alternative, and relevant tool metadata. We respectively utilize offline sampling to isolate local single-step errors and online sampling to capture realistic multi-step failures from full agent rollouts. A multi-LLM verification pipeline is proposed to reduce label noise and ensure data quality. We conduct extensive experiments across large language models, general PRMs, and tool-specialized PRMs on ToolPRMBench. The results reveal clear differences in PRM effectiveness and highlight the potential of specialized PRMs for tool-using. Code and data will be released at https://github.com/David-Li0406/ToolPRMBench.

</details>


### [37] [Survival is the Only Reward: Sustainable Self-Training Through Environment-Mediated Selection](https://arxiv.org/abs/2601.12310)
*Jennifer Dodgson,Alfath Daryl Alhajir,Michael Joedhitya,Akira Rafhael Janson Pattirane,Surender Suresh Kumar,Joseph Lim,C. H. Peh,Adith Ramdas,Steven Zhang Zhexu*

Main category: cs.AI

TL;DR: 本文提出在稀疏外部反馈和有限内存下稳定自训练的概念验证系统架构，介绍基于环境可行性的自训练架构，分析表明环境选择能实现可持续开放式自我提升。


<details>
  <summary>Details</summary>
Motivation: 解决自训练系统因缺乏判断数据质量的外部标准而导致的奖励破解和语义漂移问题。

Method: 引入仅由环境可行性介导学习的自训练架构，在实际资源约束下执行候选行为，通过行为的差异化生存进行选择。

Result: 分析语义动态表明，改进主要通过有效和可重复策略的持续存在实现，模型可在无明确指令下发展元学习策略。

Conclusion: 基于环境的选择能实现可持续开放式自我提升，为构建更强大和通用的自主系统提供可行途径。

Abstract: Self-training systems often degenerate due to the lack of an external criterion for judging data quality, leading to reward hacking and semantic drift. This paper provides a proof-of-concept system architecture for stable self-training under sparse external feedback and bounded memory, and empirically characterises its learning dynamics and failure modes.
  We introduce a self-training architecture in which learning is mediated exclusively by environmental viability, rather than by reward, objective functions, or externally defined fitness criteria. Candidate behaviours are executed under real resource constraints, and only those whose environmental effects both persist and preserve the possibility of future interaction are propagated. The environment does not provide semantic feedback, dense rewards, or task-specific supervision; selection operates solely through differential survival of behaviours as world-altering events, making proxy optimisation impossible and rendering reward-hacking evolutionarily unstable.
  Analysis of semantic dynamics shows that improvement arises primarily through the persistence of effective and repeatable strategies under a regime of consolidation and pruning, a paradigm we refer to as negative-space learning (NSL), and that models develop meta-learning strategies (such as deliberate experimental failure in order to elicit informative error messages) without explicit instruction. This work establishes that environment-grounded selection enables sustainable open-ended self-improvement, offering a viable path toward more robust and generalisable autonomous systems without reliance on human-curated data or complex reward shaping.

</details>


### [38] [Beyond Human Annotation: Recent Advances in Data Generation Methods for Document Intelligence](https://arxiv.org/abs/2601.12318)
*Dehao Ying,Fengchang Yu,Haihua Chen,Changjiang Jiang,Yurong Li,Wei Lu*

Main category: cs.AI

TL;DR: 本文针对文档智能（DI）的数据生成问题，构建了首个全面技术地图，引入新分类法和评估框架，剖析关键挑战与前沿，将数据生成定位为下一代DI核心引擎。


<details>
  <summary>Details</summary>
Motivation: 文档智能发展需大量高质量训练数据，但人工标注是瓶颈，现有调查缺乏统一视角。

Method: 重新定义数据生成，引入基于“数据和标签可用性”的新分类法，建立多级别评估框架。

Result: 构建了数据生成的全面技术地图，剖析了关键挑战和前沿。

Conclusion: 系统化该领域，将数据生成定位为下一代DI的核心引擎。

Abstract: The advancement of Document Intelligence (DI) demands large-scale, high-quality training data, yet manual annotation remains a critical bottleneck. While data generation methods are evolving rapidly, existing surveys are constrained by fragmented focuses on single modalities or specific tasks, lacking a unified perspective aligned with real-world workflows. To fill this gap, this survey establishes the first comprehensive technical map for data generation in DI. Data generation is redefined as supervisory signal production, and a novel taxonomy is introduced based on the "availability of data and labels." This framework organizes methodologies into four resource-centric paradigms: Data Augmentation, Data Generation from Scratch, Automated Data Annotation, and Self-Supervised Signal Construction. Furthermore, a multi-level evaluation framework is established to integrate intrinsic quality and extrinsic utility, compiling performance gains across diverse DI benchmarks. Guided by this unified structure, the methodological landscape is dissected to reveal critical challenges such as fidelity gaps and frontiers including co-evolutionary ecosystems. Ultimately, by systematizing this fragmented field, data generation is positioned as the central engine for next-generation DI.

</details>


### [39] [Autonomous Knowledge Graph Exploration with Adaptive Breadth-Depth Retrieval](https://arxiv.org/abs/2601.13969)
*Joaquín Polonuer,Lucas Vittor,Iñaki Arango,Ayush Noori,David A. Clifton,Luciano Del Corro,Marinka Zitnik*

Main category: cs.AI

TL;DR: 提出ARK知识检索器，平衡知识图谱检索的广度和深度，表现优于现有方法，还可进行无标签模仿改进。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱检索方法难以平衡广度搜索和多跳遍历，相似度检索浅，基于遍历的方法依赖种子节点选择，查询跨多实体和关系时易失败。

Method: 引入ARK，使用全局词法搜索和单跳邻域探索两种操作工具集，交替进行广度发现和深度扩展，根据查询自适应工具使用。

Result: 在STaRK上，ARK平均Hit@1达到59.1%，平均MRR达到67.4，优于检索和无代理训练方法；无标签模仿实验在不同数据集上显著提升Hit@1。

Conclusion: ARK能有效解决知识图谱检索中广度和深度平衡问题，工具使用自适应查询，无标签模仿可提升小模型性能。

Abstract: Retrieving evidence for language model queries from knowledge graphs requires balancing broad search across the graph with multi-hop traversal to follow relational links. Similarity-based retrievers provide coverage but remain shallow, whereas traversal-based methods rely on selecting seed nodes to start exploration, which can fail when queries span multiple entities and relations. We introduce ARK: Adaptive Retriever of Knowledge, an agentic KG retriever that gives a language model control over this breadth-depth tradeoff using a two-operation toolset: global lexical search over node descriptors and one-hop neighborhood exploration that composes into multi-hop traversal. ARK alternates between breadth-oriented discovery and depth-oriented expansion without depending on a fragile seed selection, a pre-set hop depth, or requiring retrieval training. ARK adapts tool use to queries, using global search for language-heavy queries and neighborhood exploration for relation-heavy queries. On STaRK, ARK reaches 59.1% average Hit@1 and 67.4 average MRR, improving average Hit@1 by up to 31.4% and average MRR by up to 28.0% over retrieval-based and agentic training-free methods. Finally, we distill ARK's tool-use trajectories from a large teacher into an 8B model via label-free imitation, improving Hit@1 by +7.0, +26.6, and +13.5 absolute points over the base 8B model on AMAZON, MAG, and PRIME datasets, respectively, while retaining up to 98.5% of the teacher's Hit@1 rate.

</details>


### [40] [MARO: Learning Stronger Reasoning from Social Interaction](https://arxiv.org/abs/2601.12323)
*Yin Cai,Zhouhong Gu,Juntao Zhang,Ping Chen*

Main category: cs.AI

TL;DR: 论文提出Multi - Agent Reward Optimization (MARO)方法，让大语言模型在多智能体社交环境学习，提升推理能力，实验显示该方法有显著效果和迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型训练方法缺乏在真实交互、谈判和竞争场景中的经验，需要提升推理能力。

Method: 提出MARO方法，包括将最终结果分解到交互行为解决学习信号稀疏问题、平衡不同角色训练样本权重解决角色分布不均问题、直接评估行为效用解决环境不稳定问题。

Result: MARO显著提升社会推理能力，且学习到的能力能有效迁移到数学推理和指令遵循等其他任务。

Conclusion: 多智能体社会学习在增强大语言模型通用推理能力方面有巨大潜力。

Abstract: Humans face countless scenarios that require reasoning and judgment in daily life. However, existing large language model training methods primarily allow models to learn from existing textual content or solve predetermined problems, lacking experience in real scenarios involving interaction, negotiation, and competition with others. To address this, this paper proposes Multi-Agent Reward Optimization (MARO), a method that enables large language models (LLMs) to acquire stronger reasoning abilities by learning and practicing in multi-agent social environments. Specifically, MARO first addresses the sparse learning signal problem by decomposing final success or failure outcomes into each specific behavior during the interaction process; second, it handles the uneven role distribution problem by balancing the training sample weights of different roles; finally, it addresses environmental instability issues by directly evaluating the utility of each behavior. Experimental results demonstrate that MARO not only achieves significant improvements in social reasoning capabilities, but also that the abilities acquired through social simulation learning can effectively transfer to other tasks such as mathematical reasoning and instruction following. This reveals the tremendous potential of multi-agent social learning in enhancing the general reasoning capabilities of LLMs.

</details>


### [41] [Actionable Advice from Reviews via Mixture of LoRA Experts: A Two-LLM Pipeline for Issue Extraction and Business Recommendations](https://arxiv.org/abs/2601.12338)
*Kartikey Singh Bhandari,Manav Ganesh,Yashwant Viswanathan,Archit Agrawal,Dhruv Kumar,Pratik Narang*

Main category: cs.AI

TL;DR: 文章研究将客户评论转换为可执行建议，提出双LLM框架，使用LoRA策略，效果优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 将客户评论中的非结构化反馈转化为可执行的商业决策存在困难，因此研究评论到行动的生成。

Method: 提出模块化双LLM框架，包括问题模型和建议模型；使用LoRA专家混合策略调整建议模型；构建合成三元组进行监督训练，用八维操作准则评估。

Result: 在两个领域中，该方法始终优于仅提示和单适配器基线，具有更高的可操作性和特异性，同时保持了良好的效率 - 质量权衡。

Conclusion: 所提出的方法能有效将客户评论转化为可执行建议，具有较好性能。

Abstract: Customer reviews contain detailed, domain specific signals about service failures and user expectations, but converting this unstructured feedback into actionable business decisions remains difficult. We study review-to-action generation: producing concrete, implementable recommendations grounded in review text. We propose a modular two-LLM framework in which an Issue model extracts salient issues and assigns coarse themes, and an Advice model generates targeted operational fixes conditioned on the extracted issue representation. To enable specialization without expensive full fine-tuning, we adapt the Advice model using a mixture of LoRA experts strategy: multiple low-rank adapters are trained and a lightweight gating mechanism performs token-level expert mixing at inference, combining complementary expertise across issue types. We construct synthetic review-issue-advice triples from Yelp reviews (airlines and restaurants) to supervise training, and evaluate recommendations using an eight dimension operational rubric spanning actionability, specificity, feasibility, expected impact, novelty, non-redundancy, bias, and clarity. Across both domains, our approach consistently outperforms prompting-only and single-adapter baselines, yielding higher actionability and specificity while retaining favorable efficiency-quality trade-offs.

</details>


### [42] [PsychēChat: An Empathic Framework Focused on Emotion Shift Tracking and Safety Risk Analysis in Psychological Counseling](https://arxiv.org/abs/2601.12392)
*Zhentao Xia,Yongqi Fan,Yuxiang Chu,Yichao Yin,Liangliang Chen,Tong Ruan,Weiyan Zhang*

Main category: cs.AI

TL;DR: 提出用于心理咨询的 PsychēChat，结合情绪转变跟踪和安全风险分析，有两种建模范式，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在心理咨询中未明确建模咨询者情绪转变，且未充分探索如何使咨询师模型响应与情绪转变对齐并降低安全风险。

Method: 采用交互式角色扮演合成对话，包含情绪管理和风险控制模块，有 Agent Mode 和 LLM Mode 两种建模范式。

Result: 通过交互式评分、对话级评估和人工评估等实验，表明 PsychēChat 在情绪洞察和安全控制方面优于现有方法。

Conclusion: PsychēChat 能有效解决现有模型在心理咨询中的不足，在情绪洞察和安全控制上表现更好。

Abstract: Large language models (LLMs) have demonstrated notable advancements in psychological counseling. However, existing models generally do not explicitly model seekers' emotion shifts across counseling sessions, a core focus in classical psychological schools. Moreover, how to align counselor models' responses with these emotion shifts while proactively mitigating safety risks remains underexplored. To bridge these gaps, we propose PsychēChat, which explicitly integrates emotion shift tracking and safety risk analysis for psychological counseling. Specifically, we employ interactive role-playing to synthesize counselor--seeker dialogues, incorporating two modules: Emotion Management Module, to capture seekers' current emotions and emotion shifts; and Risk Control Module, to anticipate seekers' subsequent reactions and identify potential risks. Furthermore, we introduce two modeling paradigms. The Agent Mode structures emotion management, risk control, and counselor responses into a collaborative multi-agent pipeline. The LLM Mode integrates these stages into a unified chain-of-thought for end-to-end inference, balancing efficiency and performance. Extensive experiments, including interactive scoring, dialogue-level evaluation, and human assessment, demonstrate that PsychēChat outperforms existing methods for emotional insight and safety control.

</details>


### [43] [Are LLMs Smarter Than Chimpanzees? An Evaluation on Perspective Taking and Knowledge State Estimation](https://arxiv.org/abs/2601.12410)
*Dingyi Yang,Junqi Zhao,Xue Li,Ce Li,Boyang Li*

Main category: cs.AI

TL;DR: 本文评估大语言模型（LLM）在知识状态跟踪和估计方面的表现，结果显示多数当前最先进的LLM表现接近随机水平且远不如人类，建议未来研究应更重视知识估计和意图理解能力。


<details>
  <summary>Details</summary>
Motivation: 认知人类学指出人类智能区别于黑猩猩的关键在于推断他人知识状态和理解意图的能力，因此想要评估LLM在知识状态跟踪和估计方面的表现。

Method: 设计两项任务，一是检测LLM能否发现故事角色通过行动展现出不应具备的知识，二是检测LLM能否根据故事角色自身知识和未知客观事实来预测其下一个行动。

Result: 多数当前最先进的LLM在两项任务上表现接近随机水平，远不如人类。

Conclusion: 未来LLM研究应更重视知识估计和意图理解能力。

Abstract: Cognitive anthropology suggests that the distinction of human intelligence lies in the ability to infer other individuals' knowledge states and understand their intentions. In comparison, our closest animal relative, chimpanzees, lack the capacity to do so. With this paper, we aim to evaluate LLM performance in the area of knowledge state tracking and estimation. We design two tasks to test (1) if LLMs can detect when story characters, through their actions, demonstrate knowledge they should not possess, and (2) if LLMs can predict story characters' next actions based on their own knowledge vs. objective truths they do not know. Results reveal that most current state-of-the-art LLMs achieve near-random performance on both tasks, and are substantially inferior to humans. We argue future LLM research should place more weight on the abilities of knowledge estimation and intention understanding.

</details>


### [44] [Large Language Model for OWL Proofs](https://arxiv.org/abs/2601.12444)
*Hui Yang,Jiaoyan Chen,Uli Sattler*

Main category: cs.AI

TL;DR: 研究大语言模型在OWL本体中生成证明的能力，通过构建框架评估，发现模型表现受逻辑复杂度等因素影响，显示出模型在严格逻辑解释方面有前景但在复杂或不完美条件下推理有差距。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成证明方面的能力研究不足，本文聚焦OWL本体中证明生成的研究。

Method: 开发自动化数据集构建和评估框架，包含提取、简化、解释和评估前提逻辑完整性四个任务。

Result: 部分模型整体结果好但复杂情况受限；逻辑复杂度是影响性能的主要因素；输入数据的噪声和不完整性会降低性能。

Conclusion: 大语言模型在严格逻辑解释方面有前景，但在复杂或不完美条件下支持弹性推理仍有差距。

Abstract: The ability of Large Language Models (LLMs) to perform reasoning tasks such as deduction has been widely investigated in recent years. Yet, their capacity to generate proofs-faithful, human-readable explanations of why conclusions follow-remains largely under explored. In this work, we study proof generation in the context of OWL ontologies, which are widely adopted for representing and reasoning over complex knowledge, by developing an automated dataset construction and evaluation framework. Our evaluation encompassing three sequential tasks for complete proving: Extraction, Simplification, and Explanation, as well as an additional task of assessing Logic Completeness of the premise. Through extensive experiments on widely used reasoning LLMs, we achieve important findings including: (1) Some models achieve overall strong results but remain limited on complex cases; (2) Logical complexity, rather than representation format (formal logic language versus natural language), is the dominant factor shaping LLM performance; and (3) Noise and incompleteness in input data substantially diminish LLMs' performance. Together, these results underscore both the promise of LLMs for explanation with rigorous logics and the gap of supporting resilient reasoning under complex or imperfect conditions. Code and data are available at https://github.com/HuiYang1997/LLMOwlR.

</details>


### [45] [Failure Modes in Multi-Hop QA: The Weakest Link Law and the Recognition Bottleneck](https://arxiv.org/abs/2601.12499)
*Meiru Zhang,Zaiqiao Meng,Nigel Collier*

Main category: cs.AI

TL;DR: 文章提出MFAI语义探针来分析大语言模型多跳推理失败机制，发现“最弱环节定律”，并研究了注意力引导的双重性，还表明利用System - 2推理的模型表现良好。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多跳推理中因位置偏差而失败，不清楚是识别失败还是合成失败导致的，因此需要进行研究来明确机制。

Method: 引入Multi - Focus Attention Instruction (MFAI)语义探针，在5个大语言模型的两个多跳问答任务（MuSiQue和NeoQA）上进行实验。

Result: 发现“最弱环节定律”，即多跳推理性能取决于最不易见证据的性能水平；匹配的MFAI能解决识别瓶颈，提高准确率，误导的MFAI在真实任务中引发混淆但可在合成任务中被过滤；利用System - 2推理的模型能有效定位和整合信息。

Conclusion: 明确了大语言模型多跳推理失败的机制，展示了MFAI的作用，以及利用System - 2推理的模型在多跳推理中的优势。

Abstract: Despite scaling to massive context windows, Large Language Models (LLMs) struggle with multi-hop reasoning due to inherent position bias, which causes them to overlook information at certain positions. Whether these failures stem from an inability to locate evidence (recognition failure) or integrate it (synthesis failure) is unclear. We introduce Multi-Focus Attention Instruction (MFAI), a semantic probe to disentangle these mechanisms by explicitly steering attention towards selected positions. Across 5 LLMs on two multi-hop QA tasks (MuSiQue and NeoQA), we establish the "Weakest Link Law": multi-hop reasoning performance collapses to the performance level of the least visible evidence. Crucially, this failure is governed by absolute position rather than the linear distance between facts (performance variance $<3%$). We further identify a duality in attention steering: while matched MFAI resolves recognition bottlenecks, improving accuracy by up to 11.5% in low-visibility positions, misleading MFAI triggers confusion in real-world tasks but is successfully filtered in synthetic tasks. Finally, we demonstrate that "thinking" models that utilize System-2 reasoning, effectively locate and integrate the required information, matching gold-only baselines even in noisy, long-context settings.

</details>


### [46] [Agentic Reasoning for Large Language Models](https://arxiv.org/abs/2601.12538)
*Tianxin Wei,Ting-Wei Li,Zhining Liu,Xuying Ning,Ze Yang,Jiaru Zou,Zhichen Zeng,Ruizhong Qiu,Xiao Lin,Dongqi Fu,Zihao Li,Mengting Ai,Duo Zhou,Wenxuan Bao,Yunzhe Li,Gaotang Li,Cheng Qian,Yu Wang,Xiangru Tang,Yin Xiao,Liri Fang,Hui Liu,Xianfeng Tang,Yuji Zhang,Chi Wang,Jiaxuan You,Heng Ji,Hanghang Tong,Jingrui He*

Main category: cs.AI

TL;DR: 本文对大语言模型在开放动态环境下的代理推理进行综述，组织代理推理的三个维度，区分两种推理类型，回顾相关框架，总结方法并指出挑战与方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在封闭世界推理能力强，但在开放动态环境有困难，代理推理作为范式转变可解决此问题。

Method: 从三个互补维度组织代理推理，区分上下文推理和训练后推理，回顾代表性框架。

Result: 梳理了不同层次的代理推理，回顾应用和基准的框架。

Conclusion: 将代理推理方法整合为统一路线图，指出个性化、长期交互等开放挑战和未来方向。

Abstract: Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.

</details>


### [47] [MemeLens: Multilingual Multitask VLMs for Memes](https://arxiv.org/abs/2601.12539)
*Ali Ezzat Shahroor,Mohamed Bayan Kmainasi,Abul Hasnat,Dimitar Dimitrov,Giovanni Da San Martino,Preslav Nakov,Firoj Alam*

Main category: cs.AI

TL;DR: 提出统一多语言多任务解释增强的视觉语言模型MemeLens用于表情包理解，整合多数据集并分析，发现表情包理解需多模态训练等。


<details>
  <summary>Details</summary>
Motivation: 现有表情包研究分散在不同任务和语言，限制跨领域泛化，需统一模型。

Method: 提出MemeLens模型，整合38个公共表情包数据集，将特定标签映射到20个任务分类中，并进行全面实证分析。

Result: 表情包理解需多模态训练，语义类别间有显著差异，在单个数据集微调模型易过拟合。

Conclusion: 会将实验资源和数据集公开，以推动相关研究。

Abstract: Memes are a dominant medium for online communication and manipulation because meaning emerges from interactions between embedded text, imagery, and cultural context. Existing meme research is distributed across tasks (hate, misogyny, propaganda, sentiment, humour) and languages, which limits cross-domain generalization. To address this gap we propose MemeLens, a unified multilingual and multitask explanation-enhanced Vision Language Model (VLM) for meme understanding. We consolidate 38 public meme datasets, filter and map dataset-specific labels into a shared taxonomy of $20$ tasks spanning harm, targets, figurative/pragmatic intent, and affect. We present a comprehensive empirical analysis across modeling paradigms, task categories, and datasets. Our findings suggest that robust meme understanding requires multimodal training, exhibits substantial variation across semantic categories, and remains sensitive to over-specialization when models are fine-tuned on individual datasets rather than trained in a unified setting. We will make the experimental resources and datasets publicly available for the community.

</details>


### [48] [Rethinking the AI Scientist: Interactive Multi-Agent Workflows for Scientific Discovery](https://arxiv.org/abs/2601.12542)
*Lukas Weidener,Marko Brkić,Mihailo Jovanović,Ritvik Singh,Chiara Baccin,Emre Ulgac,Alex Dobrin,Aakaash Meduri*

Main category: cs.AI

TL;DR: 本文介绍多智能体系统Deep Research用于交互式科学研究，评估显示其性能超现有基线，还分析了架构约束。


<details>
  <summary>Details</summary>
Motivation: 现有科学发现人工智能方法多为专有且批处理模式，无法实时获取研究者指导，需耗时数小时。

Method: 构建Deep Research多智能体系统，含规划、数据分析等专业智能体，通过持久世界状态统一；有半自主和全自主两种操作模式。

Result: 在BixBench计算生物学基准测试中表现达州界先进水平，开放性回答准确率48.8%，多项选择评估64.5%，超现有基线14 - 26个百分点。

Conclusion: 分析架构约束为人工智能辅助科学工作流的实际部署提供了参考。

Abstract: Artificial intelligence systems for scientific discovery have demonstrated remarkable potential, yet existing approaches remain largely proprietary and operate in batch-processing modes requiring hours per research cycle, precluding real-time researcher guidance. This paper introduces Deep Research, a multi-agent system enabling interactive scientific investigation with turnaround times measured in minutes. The architecture comprises specialized agents for planning, data analysis, literature search, and novelty detection, unified through a persistent world state that maintains context across iterative research cycles. Two operational modes support different workflows: semi-autonomous mode with selective human checkpoints, and fully autonomous mode for extended investigations. Evaluation on the BixBench computational biology benchmark demonstrated state-of-the-art performance, achieving 48.8% accuracy on open response and 64.5% on multiple-choice evaluation, exceeding existing baselines by 14 to 26 percentage points. Analysis of architectural constraints, including open access literature limitations and challenges inherent to automated novelty assessment, informs practical deployment considerations for AI-assisted scientific workflows.

</details>


### [49] [How Clinicians Think and What AI Can Learn From It](https://arxiv.org/abs/2601.12547)
*Dipayan Sengupta,Saumya Panda*

Main category: cs.AI

TL;DR: 指出临床AI系统多为预测引擎，而临床推理是序贯控制问题，提出临床推理是序数、非补偿性决策，阐述此类算法合理性并给出临床AI蓝图。


<details>
  <summary>Details</summary>
Motivation: 解决当前临床AI系统与实际临床推理不匹配的问题，探索更符合临床推理的计算模式。

Method: 从临床权衡的测量特性、偏好和信号引出的结构特点等方面进行理论分析。

Result: 论证了序数、非补偿性决策算法在医学中的合理性，指出插件式期望效用优化在某些情况下的脆弱性。

Conclusion: 提出临床AI应使用丰富模型，但通过稳健的序数规则选择行动，将启发式作为低维特例，以“选择性复杂性”方式部署。

Abstract: Most clinical AI systems operate as prediction engines -- producing labels or risk scores -- yet real clinical reasoning is a time-bounded, sequential control problem under uncertainty. Clinicians interleave information gathering with irreversible actions, guided by regret, constraints and patient values. We argue that the dominant computational substrate of clinician reasoning is not cardinal optimization but ordinal, non-compensatory decision-making: Clinicians frequently rely on fast-and-frugal, lexicographic heuristics (e.g., fast-and-frugal trees) that stop early after checking a small, fixed sequence of cues. We provide a normative rationale for why such algorithms are not merely bounded rationality shortcuts, but can be epistemically preferred in medicine. First, many clinical trade-offs are constructed through human judgment and are only weakly measurable on absolute scales; without strong measurement axioms, only orderings are invariant, motivating an ordinal-by-default stance. Second, preference and signal elicitation are structurally crude: The mapping from truth $\to$ perception $\to$ inference $\to$ recorded variables introduces layered noise, leaving a persistent uncertainty floor. When this 'crudeness' overwhelms the decision margin, plug-in expected-utility optimization becomes brittle (high flip probability under small perturbations), whereas robust dominance/filtering rules ($ε$-dominance, maximin) stabilize decisions.Finally, we outline a clinician-aligned AI blueprint: Use rich models for beliefs and trajectories, but choose actions through robust ordinal rules; treat heuristics as the low-dimensional special case; and deploy AI as 'selective complexity' -- invoked mainly for tie-breaking when decisions are fragile and information has positive expected impact.

</details>


### [50] [Agentic Artificial Intelligence (AI): Architectures, Taxonomies, and Evaluation of Large Language Model Agents](https://arxiv.org/abs/2601.12560)
*Arunkumar V,Gangadharan G. R.,Rajkumar Buyya*

Main category: cs.AI

TL;DR: 文章探讨从仅生成文本的AI到智能主体AI的转变，提出统一分类法，介绍转变特点、运行环境、评估实践，指出挑战并明确未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 人工智能向智能主体AI转变，新兴设计多样，导致领域格局难以把握，因此需要研究架构并构建分类法。

Method: 提出将智能主体分解为感知、大脑、规划、行动、工具使用和协作的统一分类体系，使用该体系描述转变过程，对运行环境分组，并回顾评估实践。

Result: 对从线性推理过程到原生推理时间推理模型，从固定API调用到开放标准的转变进行描述，对运行环境和评估实践进行总结。

Conclusion: 指出行动中的幻觉、无限循环和提示注入等开放性挑战，为更强大可靠的自主系统指明未来研究方向。

Abstract: Artificial Intelligence is moving from models that only generate text to Agentic AI, where systems behave as autonomous entities that can perceive, reason, plan, and act. Large Language Models (LLMs) are no longer used only as passive knowledge engines but as cognitive controllers that combine memory, tool use, and feedback from their environment to pursue extended goals. This shift already supports the automation of complex workflows in software engineering, scientific discovery, and web navigation, yet the variety of emerging designs, from simple single loop agents to hierarchical multi agent systems, makes the landscape hard to navigate. In this paper, we investigate architectures and propose a unified taxonomy that breaks agents into Perception, Brain, Planning, Action, Tool Use, and Collaboration. We use this lens to describe the move from linear reasoning procedures to native inference time reasoning models, and the transition from fixed API calls to open standards like the Model Context Protocol (MCP) and Native Computer Use. We also group the environments in which these agents operate, including digital operating systems, embodied robotics, and other specialized domains, and we review current evaluation practices. Finally, we highlight open challenges, such as hallucination in action, infinite loops, and prompt injection, and outline future research directions toward more robust and reliable autonomous systems.

</details>


### [51] [STEP-LLM: Generating CAD STEP Models from Natural Language with Large Language Models](https://arxiv.org/abs/2601.12641)
*Xiangyu Shi,Junyang Ding,Xu Zhao,Sinong Zhan,Payal Mohapatra,Daniel Quispe,Kojo Welbeck,Jian Cao,Wei Chen,Ping Guo,Qi Zhu*

Main category: cs.AI

TL;DR: 本文提出STEP - LLM解决非专家CAD设计难题，通过处理STEP文件、结合多种技术提升生成模型几何保真度，证明LLM驱动STEP模型生成可行性。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的文本到CAD方法缺乏通用性，而STEP文件虽适用于制造但处理有挑战，需让非专家能将设计意图转化为可制造工件。

Method: 创建约40K个STEP - 文本对数据集，对STEP文件进行预处理，包括基于深度优先搜索的重新序列化和CoT风格结构注释；集成检索增强生成进行监督微调；用基于倒角距离的几何奖励进行强化学习。

Result: STEP - LLM在几何保真度上优于Text2CAD基线，RAG模块提升完整性和可渲染性，DFS重新序列化增强整体准确性，RL减少几何差异。

Conclusion: 证明了大语言模型驱动从自然语言生成STEP模型的可行性，有潜力使CAD设计普及化。

Abstract: Computer-aided design (CAD) is vital to modern manufacturing, yet model creation remains labor-intensive and expertise-heavy. To enable non-experts to translate intuitive design intent into manufacturable artifacts, recent large language models-based text-to-CAD efforts focus on command sequences or script-based formats like CadQuery. However, these formats are kernel-dependent and lack universality for manufacturing. In contrast, the Standard for the Exchange of Product Data (STEP, ISO 10303) file is a widely adopted, neutral boundary representation (B-rep) format directly compatible with manufacturing, but its graph-structured, cross-referenced nature poses unique challenges for auto-regressive LLMs. To address this, we curate a dataset of ~40K STEP-caption pairs and introduce novel preprocessing tailored for the graph-structured format of STEP, including a depth-first search-based reserialization that linearizes cross-references while preserving locality and chain-of-thought(CoT)-style structural annotations that guide global coherence. We integrate retrieval-augmented generation to ground predictions in relevant examples for supervised fine-tuning, and refine generation quality through reinforcement learning with a specific Chamfer Distance-based geometric reward. Experiments demonstrate consistent gains of our STEP-LLM in geometric fidelity over the Text2CAD baseline, with improvements arising from multiple stages of our framework: the RAG module substantially enhances completeness and renderability, the DFS-based reserialization strengthens overall accuracy, and the RL further reduces geometric discrepancy. Both metrics and visual comparisons confirm that STEP-LLM generates shapes with higher fidelity than Text2CAD. These results show the feasibility of LLM-driven STEP model generation from natural language, showing its potential to democratize CAD design for manufacturing.

</details>


### [52] [MedConsultBench: A Full-Cycle, Fine-Grained, Process-Aware Benchmark for Medical Consultation Agents](https://arxiv.org/abs/2601.12661)
*Chuhan Qiao,Jianghua Huang,Daxing Zhao,Ziding Liu,Yanjun Shen,Bing Cheng,Wei Lin,Kai Wu*

Main category: cs.AI

TL;DR: 现有医疗咨询智能体评估不足，提出MedConsultBench综合框架评估咨询全周期，评估19个大模型发现理论与实践能力差距。


<details>
  <summary>Details</summary>
Motivation: 当前医疗咨询智能体评估侧重结果，忽视过程完整性和临床安全，现有交互式基准碎片化、粗粒度，无法满足专业咨询要求。

Method: 提出MedConsultBench框架，引入原子信息单元（AIUs），用22个细粒度指标跟踪临床信息获取。

Result: 对19个大语言模型进行系统评估，发现高诊断准确率往往掩盖信息收集效率和用药安全的重大缺陷。

Conclusion: 凸显理论医学知识与临床实践能力间的关键差距，MedConsultBench为医疗AI契合现实临床护理要求奠定基础。

Abstract: Current evaluations of medical consultation agents often prioritize outcome-oriented tasks, frequently overlooking the end-to-end process integrity and clinical safety essential for real-world practice. While recent interactive benchmarks have introduced dynamic scenarios, they often remain fragmented and coarse-grained, failing to capture the structured inquiry logic and diagnostic rigor required in professional consultations. To bridge this gap, we propose MedConsultBench, a comprehensive framework designed to evaluate the complete online consultation cycle by covering the entire clinical workflow from history taking and diagnosis to treatment planning and follow-up Q\&A. Our methodology introduces Atomic Information Units (AIUs) to track clinical information acquisition at a sub-turn level, enabling precise monitoring of how key facts are elicited through 22 fine-grained metrics. By addressing the underspecification and ambiguity inherent in online consultations, the benchmark evaluates uncertainty-aware yet concise inquiry while emphasizing medication regimen compatibility and the ability to handle realistic post-prescription follow-up Q\&A via constraint-respecting plan revisions. Systematic evaluation of 19 large language models reveals that high diagnostic accuracy often masks significant deficiencies in information-gathering efficiency and medication safety. These results underscore a critical gap between theoretical medical knowledge and clinical practice ability, establishing MedConsultBench as a rigorous foundation for aligning medical AI with the nuanced requirements of real-world clinical care.

</details>


### [53] [Empowering All-in-Loop Health Management of Spacecraft Power System in the Mega-Constellation Era via Human-AI Collaboration](https://arxiv.org/abs/2601.12667)
*Yi Di,Zhibin Zhao,Fujin Wang,Xue Liu,Jiafeng Tang,Jiaxin Ren,Zhi Zhai,Xuefeng Chen*

Main category: cs.AI

TL;DR: 随着卫星数量指数级增长，论文提出AUC原则并开发SpaceHMchat框架用于航天器电源系统健康管理，通过实验验证其性能，还发布首个AIL HM数据集。


<details>
  <summary>Details</summary>
Motivation: 卫星数量剧增，航天器电源系统故障高发，需要适配卫星巨型星座时代的健康管理方法。

Method: 提出AUC原则，开发SpaceHMchat框架，建立硬件逼真故障注入实验平台及模拟模型，发布AIL HM数据集。

Result: SpaceHMchat在23个量化指标上表现优秀，如工况识别逻辑推理结论准确率100%等。

Conclusion: SpaceHMchat框架可有效应用于航天器电源系统的全闭环健康管理。

Abstract: It is foreseeable that the number of spacecraft will increase exponentially, ushering in an era dominated by satellite mega-constellations (SMC). This necessitates a focus on energy in space: spacecraft power systems (SPS), especially their health management (HM), given their role in power supply and high failure rates. Providing health management for dozens of SPS and for thousands of SPS represents two fundamentally different paradigms. Therefore, to adapt the health management in the SMC era, this work proposes a principle of aligning underlying capabilities (AUC principle) and develops SpaceHMchat, an open-source Human-AI collaboration (HAIC) framework for all-in-loop health management (AIL HM). SpaceHMchat serves across the entire loop of work condition recognition, anomaly detection, fault localization, and maintenance decision making, achieving goals such as conversational task completion, adaptive human-in-the-loop learning, personnel structure optimization, knowledge sharing, efficiency enhancement, as well as transparent reasoning and improved interpretability. Meanwhile, to validate this exploration, a hardware-realistic fault injection experimental platform is established, and its simulation model is built and open-sourced, both fully replicating the real SPS. The corresponding experimental results demonstrate that SpaceHMchat achieves excellent performance across 23 quantitative metrics, such as 100% conclusion accuracy in logical reasoning of work condition recognition, over 99% success rate in anomaly detection tool invocation, over 90% precision in fault localization, and knowledge base search time under 3 minutes in maintenance decision-making. Another contribution of this work is the release of the first-ever AIL HM dataset of SPS. This dataset contains four sub-datasets, involving 4 types of AIL HM sub-tasks, 17 types of faults, and over 700,000 timestamps.

</details>


### [54] [Logic-Guided Multistage Inference for Explainable Multidefendant Judgment Prediction](https://arxiv.org/abs/2601.12688)
*Xu Zhang,Qinghua Wang,Mengyang Zhao,Fang Wang,Cunquan Qu*

Main category: cs.AI

TL;DR: 本文提出了掩码多阶段推理（MMSI）框架以解决多被告案件中的责任分配问题，经评估取得显著效果，代码公开。


<details>
  <summary>Details</summary>
Motivation: 犯罪破坏社会稳定，多被告案件中责任分配复杂且司法表述模糊，阻碍AI分析，需要提升智能司法辅助并确保法律可解释性。

Method: 将量刑逻辑融入预训练Transformer编码器框架，采用定向掩码机制和比较数据构建策略，通过广播将预测的有罪标签纳入回归模型。

Result: 所提出的MMSI框架在自定义的IMLJP数据集上评估，在基于角色的罪责区分方面显著提高了准确率，优于基线模型。

Conclusion: 该框架为增强智能司法系统提供了有力解决方案。

Abstract: Crime disrupts societal stability, making law essential for balance. In multidefendant cases, assigning responsibility is complex and challenges fairness, requiring precise role differentiation. However, judicial phrasing often obscures the roles of the defendants, hindering effective AI-driven analyses. To address this issue, we incorporate sentencing logic into a pretrained Transformer encoder framework to enhance the intelligent assistance in multidefendant cases while ensuring legal interpretability. Within this framework an oriented masking mechanism clarifies roles and a comparative data construction strategy improves the model's sensitivity to culpability distinctions between principals and accomplices. Predicted guilt labels are further incorporated into a regression model through broadcasting, consolidating crime descriptions and court views. Our proposed masked multistage inference (MMSI) framework, evaluated on the custom IMLJP dataset for intentional injury cases, achieves significant accuracy improvements, outperforming baselines in role-based culpability differentiation. This work offers a robust solution for enhancing intelligent judicial systems, with publicly code available.

</details>


### [55] [Neurosymbolic LoRA: Why and When to Tune Weights vs. Rewrite Prompts](https://arxiv.org/abs/2601.12711)
*Kevin Wang,Neel P. Bhatt,Cong Liu,Junbo Li,Runjin Chen,Yihan Xi,Timothy Barclay,Alvaro Velasquez,Ufuk Topcu,Zhangyang Wang*

Main category: cs.AI

TL;DR: 介绍神经符号LoRA框架，结合数值和符号策略适配大语言模型，实验显示优于单一策略基线。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型适配方法中，数值微调注重新事实知识，符号更新注重风格和对齐控制，需结合两者优势。

Method: 引入神经符号LoRA框架，用统一监测信号和基于奖励的分类器决定何时用LoRA和TextGrad，按需将符号转换卸载到外部LLM，利用符号编辑生成的提示作为训练数据。

Result: 在多个LLM骨干上的实验表明，神经符号LoRA始终优于纯数值或纯符号基线。

Conclusion: 交错数值和符号更新对语言模型微调具有重要价值，能带来更高灵活性。

Abstract: Large language models (LLMs) can be adapted either through numerical updates that alter model parameters or symbolic manipulations that work on discrete prompts or logical constraints. While numerical fine-tuning excels at injecting new factual knowledge, symbolic updates offer flexible control of style and alignment without retraining. We introduce a neurosymbolic LoRA framework that dynamically combines these two complementary strategies. Specifically, we present a unified monitoring signal and a reward-based classifier to decide when to employ LoRA for deeper factual reconstruction and when to apply TextGrad for token-level edits. Our approach remains memory-efficient by offloading the symbolic transformations to an external LLM only when needed. Additionally, the refined prompts produced during symbolic editing serve as high-quality, reusable training data, an important benefit in data-scarce domains like mathematical reasoning. Extensive experiments across multiple LLM backbones show that neurosymbolic LoRA consistently outperforms purely numerical or purely symbolic baselines, demonstrating superior adaptability and improved performance. Our findings highlight the value of interleaving numerical and symbolic updates to unlock a new level of versatility in language model fine-tuning.

</details>


### [56] [Teaching Large Reasoning Models Effective Reflection](https://arxiv.org/abs/2601.12720)
*Hanbin Wang,Jingwei Song,Jinpeng Li,Qi Zhu,Fei Mi,Ganqu Cui,Yasheng Wang,Lifeng Shang*

Main category: cs.AI

TL;DR: 本文提出SCFT和RLERR方法解决大推理模型表层反思问题，在两个基准测试中提升推理准确性和反思质量。


<details>
  <summary>Details</summary>
Motivation: 大推理模型的自我反思行为存在表层反思问题，带来计算开销且无实质改进。

Method: 提出Self - Critique Fine - Tuning (SCFT)框架，利用自生成批评提升模型反思推理能力；引入Reinforcement Learning with Effective Reflection Rewards (RLERR)，利用SCFT的高质量反思构建奖励信号进行强化学习。

Result: 在AIME2024和AIME2025基准测试中，SCFT和RLERR显著提高推理准确性和反思质量，优于现有基线。

Conclusion: SCFT和RLERR能有效解决大推理模型的表层反思问题，提升模型性能。

Abstract: Large Reasoning Models (LRMs) have recently shown impressive performance on complex reasoning tasks, often by engaging in self-reflective behaviors such as self-critique and backtracking. However, not all reflections are beneficial-many are superficial, offering little to no improvement over the original answer and incurring computation overhead. In this paper, we identify and address the problem of superficial reflection in LRMs. We first propose Self-Critique Fine-Tuning (SCFT), a training framework that enhances the model's reflective reasoning ability using only self-generated critiques. SCFT prompts models to critique their own outputs, filters high-quality critiques through rejection sampling, and fine-tunes the model using a critique-based objective. Building on this strong foundation, we further introduce Reinforcement Learning with Effective Reflection Rewards (RLERR). RLERR leverages the high-quality reflections initialized by SCFT to construct reward signals, guiding the model to internalize the self-correction process via reinforcement learning. Experiments on two challenging benchmarks, AIME2024 and AIME2025, show that SCFT and RLERR significantly improve both reasoning accuracy and reflection quality, outperforming state-of-the-art baselines. All data and codes are available at https://github.com/wanghanbinpanda/SCFT.

</details>


### [57] [Vision Language Models for Optimization-Driven Intent Processing in Autonomous Networks](https://arxiv.org/abs/2601.12744)
*Tasnim Ahmed,Yifan Zhu,Salimur Choudhury*

Main category: cs.AI

TL;DR: 研究利用VLMs处理带注释的网络草图生成优化代码，通过IntentOpt基准评估四种VLMs，揭示视觉输入等对性能影响并展示实际可行性。


<details>
  <summary>Details</summary>
Motivation: IBN特定类意图需生成优化代码，当前系统基于文本意图表达，而网络从业者习惯用图表，探究VLMs能否处理网络草图成优化代码未被探索。

Method: 提出包含85个优化问题的IntentOpt基准，在多模态和纯文本输入下用三种提示策略评估四种VLMs。

Result: 视觉参数提取使执行成功率降低12 - 21个百分点，思维程序提示使性能最多下降13个百分点，开源模型落后于闭源模型。

Conclusion: 确定当前VLMs在IBN系统中生成优化代码的基线能力和局限性，通过案例研究证明实际可行性。

Abstract: Intent-Based Networking (IBN) allows operators to specify high-level network goals rather than low-level configurations. While recent work demonstrates that large language models can automate configuration tasks, a distinct class of intents requires generating optimization code to compute provably optimal solutions for traffic engineering, routing, and resource allocation. Current systems assume text-based intent expression, requiring operators to enumerate topologies and parameters in prose. Network practitioners naturally reason about structure through diagrams, yet whether Vision-Language Models (VLMs) can process annotated network sketches into correct optimization code remains unexplored. We present IntentOpt, a benchmark of 85 optimization problems across 17 categories, evaluating four VLMs (GPT-5-Mini, Claude-Haiku-4.5, Gemini-2.5-Flash, Llama-3.2-11B-Vision) under three prompting strategies on multimodal versus text-only inputs. Our evaluation shows that visual parameter extraction reduces execution success by 12-21 percentage points (pp), with GPT-5-Mini dropping from 93% to 72%. Program-of-thought prompting decreases performance by up to 13 pp, and open-source models lag behind closed-source ones, with Llama-3.2-11B-Vision reaching 18% compared to 75% for GPT-5-Mini. These results establish baseline capabilities and limitations of current VLMs for optimization code generation within an IBN system. We also demonstrate practical feasibility through a case study that deploys VLM-generated code to network testbed infrastructure using Model Context Protocol.

</details>


### [58] [VIRO: Robust and Efficient Neuro-Symbolic Reasoning with Verification for Referring Expression Comprehension](https://arxiv.org/abs/2601.12781)
*Hyejin Park,Junhyuk Kwon,Suha Kwak,Jungseul Ok*

Main category: cs.AI

TL;DR: 提出VIRO神经符号框架，嵌入验证器以解决现有神经符号REC方法的级联错误问题，实现SOTA性能及多项优势。


<details>
  <summary>Details</summary>
Motivation: 现有神经符号REC方法假设中间推理步骤准确，会导致级联错误，无法处理图像中无目标情况。

Method: 引入Verification-Integrated Reasoning Operators (VIRO)神经符号框架，在推理步骤中嵌入轻量级算子级验证器，每个算子执行并验证其输出。

Result: 达到61.1%的平衡准确率，能泛化到真实世界的第一人称视角数据，具有高计算效率、高可靠性，程序失败率低于0.3%，且可扩展。

Conclusion: VIRO框架有效解决了现有方法的局限，实现了良好性能和多种优势。

Abstract: Referring Expression Comprehension (REC) aims to localize the image region corresponding to a natural-language query. Recent neuro-symbolic REC approaches leverage large language models (LLMs) and vision-language models (VLMs) to perform compositional reasoning, decomposing queries 4 structured programs and executing them step-by-step. While such approaches achieve interpretable reasoning and strong zero-shot generalization, they assume that intermediate reasoning steps are accurate. However, this assumption causes cascading errors: false detections and invalid relations propagate through the reasoning chain, yielding high-confidence false positives even when no target is present in the image. To address this limitation, we introduce Verification-Integrated Reasoning Operators (VIRO), a neuro-symbolic framework that embeds lightweight operator-level verifiers within reasoning steps. Each operator executes and validates its output, such as object existence or spatial relationship, thereby allowing the system to robustly handle no-target cases when verification conditions are not met. Our framework achieves state-of-the-art performance, reaching 61.1% balanced accuracy across target-present and no-target settings, and demonstrates generalization to real-world egocentric data. Furthermore, VIRO shows superior computational efficiency in terms of throughput, high reliability with a program failure rate of less than 0.3%, and scalability through decoupled program generation from execution.

</details>


### [59] [SL-CBM: Enhancing Concept Bottleneck Models with Semantic Locality for Better Interpretability](https://arxiv.org/abs/2601.12804)
*Hanwei Zhang,Luo Cheng,Rui Wen,Yang Zhang,Lijun Zhang,Holger Hermanns*

Main category: cs.AI

TL;DR: 提出SL - CBM解决现有CBM局部忠实性差问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有Concept Bottleneck Models (CBMs) 局部忠实性差，无法将概念与有意义图像区域空间对齐，限制了解释性和可靠性。

Method: 提出SL - CBM，通过在概念和类级别生成空间连贯的显著性图来强制局部忠实性，集成1x1卷积层和交叉注意力机制。

Result: 在图像数据集上的实验表明，SL - CBM大幅提高了局部忠实性、解释质量和干预效果，同时保持了有竞争力的分类准确率。

Conclusion: SL - CBM弥合了基于概念推理和空间可解释性之间的差距，为可解释和可信的基于概念的模型设定了新标准。

Abstract: Explainable AI (XAI) is crucial for building transparent and trustworthy machine learning systems, especially in high-stakes domains. Concept Bottleneck Models (CBMs) have emerged as a promising ante-hoc approach that provides interpretable, concept-level explanations by explicitly modeling human-understandable concepts. However, existing CBMs often suffer from poor locality faithfulness, failing to spatially align concepts with meaningful image regions, which limits their interpretability and reliability. In this work, we propose SL-CBM (CBM with Semantic Locality), a novel extension that enforces locality faithfulness by generating spatially coherent saliency maps at both concept and class levels. SL-CBM integrates a 1x1 convolutional layer with a cross-attention mechanism to enhance alignment between concepts, image regions, and final predictions. Unlike prior methods, SL-CBM produces faithful saliency maps inherently tied to the model's internal reasoning, facilitating more effective debugging and intervention. Extensive experiments on image datasets demonstrate that SL-CBM substantially improves locality faithfulness, explanation quality, and intervention efficacy while maintaining competitive classification accuracy. Our ablation studies highlight the importance of contrastive and entropy-based regularization for balancing accuracy, sparsity, and faithfulness. Overall, SL-CBM bridges the gap between concept-based reasoning and spatial explainability, setting a new standard for interpretable and trustworthy concept-based models.

</details>


### [60] [MirrorGuard: Toward Secure Computer-Use Agents via Simulation-to-Real Reasoning Correction](https://arxiv.org/abs/2601.12822)
*Wenqi Zhang,Yulin Shen,Changyue Jiang,Jiarun Dai,Geng Hong,Xudong Pan*

Main category: cs.AI

TL;DR: 本文提出MirrorGuard防御框架，用基于模拟的训练提升计算机使用代理（CUA）安全性，模拟环境中学习拦截不安全推理链，测试表明能显著降低安全风险，代码模型公开。


<details>
  <summary>Details</summary>
Motivation: 大型基础模型集成到CUA中带来安全风险，现有防御方法常过早终止任务，降低代理效用。

Method: 提出MirrorGuard框架，采用基于模拟的训练；提出新颖神经符号模拟管道，在基于文本的模拟环境生成GUI交互轨迹，学习拦截不安全推理链。

Result: 在不同基准测试和CUA架构中，MirrorGuard显著降低安全风险，如在ByteDance UI - TARS系统将不安全率从66.5%降至13.0%，误拒率低。

Conclusion: 基于模拟的防御能在保持代理基本效用的同时，提供强大的现实世界保护。

Abstract: Large foundation models are integrated into Computer Use Agents (CUAs), enabling autonomous interaction with operating systems through graphical user interfaces (GUIs) to perform complex tasks. This autonomy introduces serious security risks: malicious instructions or visual prompt injections can trigger unsafe reasoning and cause harmful system-level actions. Existing defenses, such as detection-based blocking, prevent damage but often abort tasks prematurely, reducing agent utility. In this paper, we present MirrorGuard, a plug-and-play defense framework that uses simulation-based training to improve CUA security in the real world. To reduce the cost of large-scale training in operating systems, we propose a novel neural-symbolic simulation pipeline, which generates realistic, high-risk GUI interaction trajectories entirely in a text-based simulated environment, which captures unsafe reasoning patterns and potential system hazards without executing real operations. In the simulation environment, MirrorGuard learns to intercept and rectify insecure reasoning chains of CUAs before they produce and execute unsafe actions. In real-world testing, extensive evaluations across diverse benchmarks and CUA architectures show that MirrorGuard significantly mitigates security risks. For instance, on the ByteDance UI-TARS system, it reduces the unsafe rate from 66.5% to 13.0% while maintaining a marginal false refusal rate (FRR). In contrast, the state-of-the-art GuardAgent only achieves a reduction to 53.9% and suffers from a 15.4% higher FRR. Our work proves that simulation-derived defenses can provide robust, real-world protection while maintaining the fundamental utility of the agent. Our code and model are publicly available at https://bmz-q-q.github.io/MirrorGuard/.

</details>


### [61] [SCULPT: Constraint-Guided Pruned MCTS that Carves Efficient Paths for Mathematical Reasoning](https://arxiv.org/abs/2601.12842)
*Qitong Fang,Haotian Li,Xu Wang*

Main category: cs.AI

TL;DR: 本文介绍了一种名为SCULPT的约束引导方法，用于蒙特卡罗树搜索，以促进自动化代理工作流的有序探索，该方法在多个数据集上取得稳定改进。


<details>
  <summary>Details</summary>
Motivation: 当前自动化代理工作流的常见搜索策略依赖随机探索，经常遍历不合理分支，需要促进有序探索。

Method: 引入SCULPT，将领域感知评分集成到蒙特卡罗树搜索的选择、扩展、模拟和反向传播中，使用符号检查和结构模式指导对动作进行评分和剪枝。

Result: 在匹配的大语言模型配置下，SCULPT在多个数据集上取得稳定改进，还评估了执行器的可迁移性和前沿推理模型的性能。

Conclusion: 领域感知约束可以在保持效率和推理稳定性的同时提高准确性。

Abstract: Automated agent workflows can enhance the problem-solving ability of large language models (LLMs), but common search strategies rely on stochastic exploration and often traverse implausible branches. This occurs because current pipelines sample candidate steps from generic prompts or learned policies with weak domain priors, yielding near-random walks over operators, units, and formats. To promote ordered exploration, this paper introduces SCULPT, a constraint-guided approach for Monte Carlo Tree Search (MCTS) that integrates domain-aware scoring into selection, expansion, simulation, and backpropagation. SCULPT scores and prunes actions using a combination of symbolic checks (dimensional consistency, type compatibility, magnitude sanity, depth control, and diversity) and structural pattern guidance, thereby steering the search toward plausible reasoning paths. Under matched LLM configurations, SCULPT yields stable improvements on multiple datasets; additional results with GPT-5.2 assess executor transferability and performance on frontier reasoning models. Overall, domain-aware constraints can improve accuracy while maintaining efficiency and reasoning stability.

</details>


### [62] [Mining Citywide Dengue Spread Patterns in Singapore Through Hotspot Dynamics from Open Web Data](https://arxiv.org/abs/2601.12856)
*Liping Huang,Gaoxi Xiao,Stefan Ma,Hechang Chen,Shisong Tang,Flora Salim*

Main category: cs.AI

TL;DR: 研究提出新框架挖掘登革热城市区域潜伏传播联系，利用病例数据预测热点，与通勤流相符，可用于公共卫生规划等。


<details>
  <summary>Details</summary>
Motivation: 登革热在城市地区构成公共卫生挑战，需预测传播风险以主动干预。

Method: 引入新框架，从公开登革热病例数据挖掘区域间潜伏传播联系，通过梯度下降优化，用热点历史预测并验证传播模式。

Result: 在新加坡案例中，四周热点历史能实现平均F - 分数0.79，学习到的传播联系与通勤流相符。

Conclusion: 该框架推进了流行病建模，为公共卫生规划等提供可扩展、低成本工具。

Abstract: Dengue, a mosquito-borne disease, continues to pose a persistent public health challenge in urban areas, particularly in tropical regions such as Singapore. Effective and affordable control requires anticipating where transmission risks are likely to emerge so that interventions can be deployed proactively rather than reactively. This study introduces a novel framework that uncovers and exploits latent transmission links between urban regions, mined directly from publicly available dengue case data. Instead of treating cases as isolated reports, we model how hotspot formation in one area is influenced by epidemic dynamics in neighboring regions. While mosquito movement is highly localized, long-distance transmission is often driven by human mobility, and in our case study, the learned network aligns closely with commuting flows, providing an interpretable explanation for citywide spread. These hidden links are optimized through gradient descent and used not only to forecast hotspot status but also to verify the consistency of spreading patterns, by examining the stability of the inferred network across consecutive weeks. Case studies on Singapore during 2013-2018 and 2020 show that four weeks of hotspot history are sufficient to achieve an average F-score of 0.79. Importantly, the learned transmission links align with commuting flows, highlighting the interpretable interplay between hidden epidemic spread and human mobility. By shifting from simply reporting dengue cases to mining and validating hidden spreading dynamics, this work transforms open web-based case data into a predictive and explanatory resource. The proposed framework advances epidemic modeling while providing a scalable, low-cost tool for public health planning, early intervention, and urban resilience.

</details>


### [63] [Human Emotion Verification by Action Languages via Answer Set Programming](https://arxiv.org/abs/2601.12912)
*Andreas Brännström,Juan Carlos Nieves*

Main category: cs.AI

TL;DR: 本文介绍行动语言C - MT，基于ASP和转换系统表示人类心理状态演变，扩展语言、转换为约束进行评估，还支持比较不同变化动态并应用于情感验证。


<details>
  <summary>Details</summary>
Motivation: 解决对智能体行为控制的需求，限制行动的不良心理副作用。

Method: 基于答案集编程和转换系统构建C - MT语言，借鉴心理学理论将心理状态形式化，扩展语言加入新因果规则，将心理变化原则转换为转换约束和不变性属性，用转换系统评估。

Result: 能实现对人类心理状态动态演变的可控推理，支持分析遵循不同心理学原则的轨迹。

Conclusion: 行动语言C - MT可应用于情感验证模型设计。

Abstract: In this paper, we introduce the action language C-MT (Mind Transition Language). It is built on top of answer set programming (ASP) and transition systems to represent how human mental states evolve in response to sequences of observable actions. Drawing on well-established psychological theories, such as the Appraisal Theory of Emotion, we formalize mental states, such as emotions, as multi-dimensional configurations. With the objective to address the need for controlled agent behaviors and to restrict unwanted mental side-effects of actions, we extend the language with a novel causal rule, forbids to cause, along with expressions specialized for mental state dynamics, which enables the modeling of principles for valid transitions between mental states. These principles of mental change are translated into transition constraints, and properties of invariance, which are rigorously evaluated using transition systems in terms of so-called trajectories. This enables controlled reasoning about the dynamic evolution of human mental states. Furthermore, the framework supports the comparison of different dynamics of change by analyzing trajectories that adhere to different psychological principles. We apply the action language to design models for emotion verification. Under consideration in Theory and Practice of Logic Programming (TPLP).

</details>


### [64] [MagicGUI-RMS: A Multi-Agent Reward Model System for Self-Evolving GUI Agents via Automated Feedback Reflux](https://arxiv.org/abs/2601.13060)
*Zecheng Li,Zhihui Cao,Wenke Huang,Yudong Zhang,Keying Qi,Rui Wang,Zeyu Zheng,Jian Zhao,Hao Zhu,Hengxin Wu,Yuran Wang,Guitao Fan,Guokun Wu,Yicong Liu,Zhilin Gao,Haikun Xu,He Yang,Minqi Xiang,Xingyu Liu,Zuojian Wang*

Main category: cs.AI

TL;DR: 提出MagicGUI - RMS，解决GUI智能体评估和训练数据生成问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有GUI智能体评估方法依赖手动标注或静态规则验证，缺乏可扩展性和适应动态环境能力，需要解决评估和大规模训练数据生成问题。

Method: 集成DS - RM和GP - RM，设计数据构建管道，利用自动化数据回流机制。

Result: MagicGUI - RMS在任务准确性和行为鲁棒性上有显著提升。

Conclusion: MagicGUI - RMS是构建基于奖励自适应的自改进GUI智能体的有效基础。

Abstract: Graphical user interface (GUI) agents are rapidly progressing toward autonomous interaction and reliable task execution across diverse applications. However, two central challenges remain unresolved: automating the evaluation of agent trajectories and generating high-quality training data at scale to enable continual improvement. Existing approaches often depend on manual annotation or static rule-based verification, which restricts scalability and limits adaptability in dynamic environments. We present MagicGUI-RMS, a multi-agent reward model system that delivers adaptive trajectory evaluation, corrective feedback, and self-evolving learning capabilities. MagicGUI-RMS integrates a Domain-Specific Reward Model (DS-RM) with a General-Purpose Reward Model (GP-RM), enabling fine-grained action assessment and robust generalization across heterogeneous GUI tasks. To support reward learning at scale, we design a structured data construction pipeline that automatically produces balanced and diverse reward datasets, effectively reducing annotation costs while maintaining sample fidelity. During execution, the reward model system identifies erroneous actions, proposes refined alternatives, and continuously enhances agent behavior through an automated data-reflux mechanism. Extensive experiments demonstrate that MagicGUI-RMS yields substantial gains in task accuracy, behavioral robustness. These results establish MagicGUI-RMS as a principled and effective foundation for building self-improving GUI agents driven by reward-based adaptation.

</details>


### [65] [Responsible AI for General-Purpose Systems: Overview, Challenges, and A Path Forward](https://arxiv.org/abs/2601.13122)
*Gourab K Patro,Himanshi Agrawal,Himanshu Gharat,Supriya Panigrahi,Nim Sherpa,Vishal Vaddina,Dagnachew Birru*

Main category: cs.AI

TL;DR: 本文指出通用AI虽流行但有风险，对比传统特定任务AI，分析通用AI在RAI原则下的风险，提出C2V2准则并探讨相关努力，认为可通过建模和系统设计实现负责任通用AI开发。


<details>
  <summary>Details</summary>
Motivation: 通用AI存在幻觉、毒性和刻板印象等风险，需重新思考针对通用AI的负责任AI（RAI）方法。

Method: 依据八项RAI原则审查通用AI的风险和漏洞，对比传统特定任务AI，提出C2V2准则并讨论相关技术。

Result: 明确通用AI输出自由度高导致风险，提出C2V2准则以满足未来通用AI的RAI要求。

Conclusion: 通过在C2V2维度上建模特定应用或领域的RAI要求，并采用系统设计方法结合多种技术，可实现开发负责任通用AI的目标。

Abstract: Modern general-purpose AI systems made using large language and vision models, are capable of performing a range of tasks like writing text articles, generating and debugging codes, querying databases, and translating from one language to another, which has made them quite popular across industries. However, there are risks like hallucinations, toxicity, and stereotypes in their output that make them untrustworthy. We review various risks and vulnerabilities of modern general-purpose AI along eight widely accepted responsible AI (RAI) principles (fairness, privacy, explainability, robustness, safety, truthfulness, governance, and sustainability) and compare how they are non-existent or less severe and easily mitigable in traditional task-specific counterparts. We argue that this is due to the non-deterministically high Degree of Freedom in output (DoFo) of general-purpose AI (unlike the deterministically constant or low DoFo of traditional task-specific AI systems), and there is a need to rethink our approach to RAI for general-purpose AI. Following this, we derive C2V2 (Control, Consistency, Value, Veracity) desiderata to meet the RAI requirements for future general-purpose AI systems, and discuss how recent efforts in AI alignment, retrieval-augmented generation, reasoning enhancements, etc. fare along one or more of the desiderata. We believe that the goal of developing responsible general-purpose AI can be achieved by formally modeling application- or domain-dependent RAI requirements along C2V2 dimensions, and taking a system design approach to suitably combine various techniques to meet the desiderata.

</details>


### [66] [Prompt Injection Mitigation with Agentic AI, Nested Learning, and AI Sustainability via Semantic Caching](https://arxiv.org/abs/2601.13186)
*Diego Gosmar,Deborah A. Dahl*

Main category: cs.AI

TL;DR: 本文在原有四指标TIVS基础上扩展评估框架得到TIVS - O，结合代理管道与连续记忆系统进行实验，实现安全响应、计算节省，揭示多代理管道非单调效应，为安全绿色的大语言模型部署提供途径。


<details>
  <summary>Details</summary>
Motivation: 提示注入是大语言模型安全部署的核心障碍，特别是在多代理环境中，本文旨在评估防御有效性与透明性之间的相互作用。

Method: 扩展评估框架得到TIVS - O；结合代理管道与连续记忆系统处理301个合成注入提示；用四个代理和五个关键性能指标进行分析；新增OSR指标量化安全推理丰富度和清晰度。

Result: 系统实现零高风险违规的安全响应；语义缓存减少41.6%大语言模型调用，降低延迟、能耗和碳排放；五种TIVS - O配置揭示缓解严格性和法医透明度的最优权衡。

Conclusion: 可观测性感知评估可揭示多代理管道中的非单调效应；记忆增强代理可在不修改模型权重的情况下，实现安全、性能、成本和环境效益的最大化，为大语言模型部署提供可行方案。

Abstract: Prompt injection remains a central obstacle to the safe deployment of large language models, particularly in multi-agent settings where intermediate outputs can propagate or amplify malicious instructions. Building on earlier work that introduced a four-metric Total Injection Vulnerability Score (TIVS), this paper extends the evaluation framework with semantic similarity-based caching and a fifth metric (Observability Score Ratio) to yield TIVS-O, investigating how defence effectiveness interacts with transparency in a HOPE-inspired Nested Learning architecture. The proposed system combines an agentic pipeline with Continuum Memory Systems that implement semantic similarity-based caching across 301 synthetically generated injection-focused prompts drawn from ten attack families, while a fourth agent performs comprehensive security analysis using five key performance indicators. In addition to traditional injection metrics, OSR quantifies the richness and clarity of security-relevant reasoning exposed by each agent, enabling an explicit analysis of trade-offs between strict mitigation and auditability. Experiments show that the system achieves secure responses with zero high-risk breaches, while semantic caching delivers substantial computational savings, achieving a 41.6% reduction in LLM calls and corresponding decreases in latency, energy consumption, and carbon emissions. Five TIVS-O configurations reveal optimal trade-offs between mitigation strictness and forensic transparency. These results indicate that observability-aware evaluation can reveal non-monotonic effects within multi-agent pipelines and that memory-augmented agents can jointly maximize security robustness, real-time performance, operational cost savings, and environmental sustainability without modifying underlying model weights, providing a production-ready pathway for secure and green LLM deployments.

</details>


### [67] [Real-Time Deadlines Reveal Temporal Awareness Failures in LLM Strategic Dialogues](https://arxiv.org/abs/2601.13206)
*Neil K. R. Sehgal,Sharath Chandra Guntuku,Lyle Ungar*

Main category: cs.AI

TL;DR: 研究用模拟谈判研究大语言模型（LLMs）在限时场景中行为调整，发现LLMs缺乏时间感知，会制约其在时间敏感应用中的部署。


<details>
  <summary>Details</summary>
Motivation: 现实交流依赖连续时间约束，而当前LLM架构和评估协议很少测试实时期限下的时间感知，故开展研究。

Method: 使用配对智能体在严格期限下进行模拟谈判，设置控制组和时间感知组对比。

Result: 在时间感知组中，交易达成率和报价接受率远高于控制组；LLMs在轮次限制下能达到近完美交易达成率。

Conclusion: LLMs存在系统性的时间感知缺失问题，会限制其在时间敏感应用中的部署。

Abstract: Large Language Models (LLMs) generate text token-by-token in discrete time, yet real-world communication, from therapy sessions to business negotiations, critically depends on continuous time constraints. Current LLM architectures and evaluation protocols rarely test for temporal awareness under real-time deadlines. We use simulated negotiations between paired agents under strict deadlines to investigate how LLMs adjust their behavior in time-sensitive settings. In a control condition, agents know only the global time limit. In a time-aware condition, they receive remaining-time updates at each turn. Deal closure rates are substantially higher (32\% vs. 4\% for GPT-5.1) and offer acceptances are sixfold higher in the time-aware condition than in the control, suggesting LLMs struggle to internally track elapsed time. However, the same LLMs achieve near-perfect deal closure rates ($\geq$95\%) under turn-based limits, revealing the failure is in temporal tracking rather than strategic reasoning. These effects replicate across negotiation scenarios and models, illustrating a systematic lack of LLM time awareness that will constrain LLM deployment in many time-sensitive applications.

</details>


### [68] [CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning](https://arxiv.org/abs/2601.13262)
*Eric Onyame,Akash Ghosh,Subhadip Baidya,Sriparna Saha,Xiuying Chen,Chirag Agarwal*

Main category: cs.AI

TL;DR: 本文针对大语言模型在多语言医疗推理应用不可靠问题，引入数据集CUREMED - BENCH，提出CURE - MED框架，在多语言上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多语言医疗推理应用中不可靠，阻碍其在多语言医疗场景的部署。

Method: 引入多语言医疗推理数据集CUREMED - BENCH，提出课程学习强化学习框架CURE - MED，集成代码切换监督微调与组相对策略优化。

Result: 在13种语言中表现优于基线，70亿参数时语言一致性85.21%、逻辑正确性54.35%；320亿参数时语言一致性94.96%、逻辑正确性70.04%。

Conclusion: 所提方法支持大语言模型进行可靠且公平的多语言医疗推理。

Abstract: While large language models (LLMs) have shown to perform well on monolingual mathematical and commonsense reasoning, they remain unreliable for multilingual medical reasoning applications, hindering their deployment in multilingual healthcare settings. We address this by first introducing CUREMED-BENCH, a high-quality multilingual medical reasoning dataset with open-ended reasoning queries with a single verifiable answer, spanning thirteen languages, including underrepresented languages such as Amharic, Yoruba, and Swahili. Building on this dataset, we propose CURE-MED, a curriculum-informed reinforcement learning framework that integrates code-switching-aware supervised fine-tuning and Group Relative Policy Optimization to jointly improve logical correctness and language stability. Across thirteen languages, our approach consistently outperforms strong baselines and scales effectively, achieving 85.21% language consistency and 54.35% logical correctness at 7B parameters, and 94.96% language consistency and 70.04% logical correctness at 32B parameters. These results support reliable and equitable multilingual medical reasoning in LLMs. The code and dataset are available at https://cure-med.github.io/

</details>


### [69] [Improving the Safety and Trustworthiness of Medical AI via Multi-Agent Evaluation Loops](https://arxiv.org/abs/2601.13268)
*Zainab Ghafoor,Md Shafiqul Islam,Koushik Howlader,Md Rasel Khondokar,Tanusree Bhattacharjee,Sayantan Chakraborty,Adrito Roy,Ushashi Bhattacharjee,Tirtho Roy*

Main category: cs.AI

TL;DR: 本文提出多智能体细化框架提升医疗大语言模型安全性与可靠性，评估并取得良好效果，提出治理医疗AI安全的范式。


<details>
  <summary>Details</summary>
Motivation: 确保大语言模型在医疗领域应用的伦理完整性和安全合规性，消除临床部署障碍。

Method: 采用多智能体细化框架，结合DeepSeek R1、Med - PaLM两个生成模型和LLaMA 3.1、Phi - 4两个评估智能体，依据AMA伦理原则和SRA - 5协议评估，对900个不同临床查询评估性能。

Result: DeepSeek R1收敛更快，Med - PaLM处理隐私敏感场景更优，迭代多智能体循环使伦理违规减少89%，风险降级率达92%。

Conclusion: 提出了可扩展、符合监管且成本效益高的医疗AI安全治理范式。

Abstract: Large Language Models (LLMs) are increasingly applied in healthcare, yet ensuring their ethical integrity and safety compliance remains a major barrier to clinical deployment. This work introduces a multi-agent refinement framework designed to enhance the safety and reliability of medical LLMs through structured, iterative alignment. Our system combines two generative models - DeepSeek R1 and Med-PaLM - with two evaluation agents, LLaMA 3.1 and Phi-4, which assess responses using the American Medical Association's (AMA) Principles of Medical Ethics and a five-tier Safety Risk Assessment (SRA-5) protocol. We evaluate performance across 900 clinically diverse queries spanning nine ethical domains, measuring convergence efficiency, ethical violation reduction, and domain-specific risk behavior. Results demonstrate that DeepSeek R1 achieves faster convergence (mean 2.34 vs. 2.67 iterations), while Med-PaLM shows superior handling of privacy-sensitive scenarios. The iterative multi-agent loop achieved an 89% reduction in ethical violations and a 92% risk downgrade rate, underscoring the effectiveness of our approach. This study presents a scalable, regulator-aligned, and cost-efficient paradigm for governing medical AI safety.

</details>


### [70] [PepEDiff: Zero-Shot Peptide Binder Design via Protein Embedding Diffusion](https://arxiv.org/abs/2601.13327)
*Po-Yu Liang,Tobo Duran,Jun Bai*

Main category: cs.AI

TL;DR: 提出PepEDiff，一种不依赖结构预测的新型肽结合剂生成器，在TIGIT案例中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有肽结合剂生成方法依赖中间结构预测，增加复杂度且限制序列多样性。

Method: 在预训练蛋白质嵌入模型的连续潜在空间中直接生成结合序列，进行潜在空间探索和基于扩散的采样。

Result: 在基准测试和TIGIT案例研究中，优于现有方法。

Conclusion: PepEDiff有望成为无结构的零样本肽结合剂设计通用框架。

Abstract: We present PepEDiff, a novel peptide binder generator that designs binding sequences given a target receptor protein sequence and its pocket residues. Peptide binder generation is critical in therapeutic and biochemical applications, yet many existing methods rely heavily on intermediate structure prediction, adding complexity and limiting sequence diversity. Our approach departs from this paradigm by generating binder sequences directly in a continuous latent space derived from a pretrained protein embedding model, without relying on predicted structures, thereby improving structural and sequence diversity. To encourage the model to capture binding-relevant features rather than memorizing known sequences, we perform latent-space exploration and diffusion-based sampling, enabling the generation of peptides beyond the limited distribution of known binders. This zero-shot generative strategy leverages the global protein embedding manifold as a semantic prior, allowing the model to propose novel peptide sequences in previously unseen regions of the protein space. We evaluate PepEDiff on TIGIT, a challenging target with a large, flat protein-protein interaction interface that lacks a druggable pocket. Despite its simplicity, our method outperforms state-of-the-art approaches across benchmark tests and in the TIGIT case study, demonstrating its potential as a general, structure-free framework for zero-shot peptide binder design. The code for this research is available at GitHub: https://github.com/LabJunBMI/PepEDiff-An-Peptide-binder-Embedding-Diffusion-Model

</details>


### [71] [The Geometry of Thought: How Scale Restructures Reasoning In Large Language Models](https://arxiv.org/abs/2601.13358)
*Samuel Cyrenius Anderson*

Main category: cs.AI

TL;DR: 研究发现规模并非统一提升推理能力，而是重构推理，不同领域有特定相变，还引入算子并发现通用振荡特征，指出思维成本由流形几何决定。


<details>
  <summary>Details</summary>
Motivation: 探究规模对推理能力的影响，以及不同领域推理能力随规模变化的规律。

Method: 分析四个领域（法律、科学、代码、数学）、两个规模（8B、70B参数）的25000+思维链轨迹，引入神经推理算子。

Result: 不同领域有特定相变，如法律推理结晶化、科学和数学推理保持液态、代码推理形成离散晶格；算子在法律推理中准确率达63.6%；发现通用振荡特征。

Conclusion: 思维成本由流形几何而非任务难度决定，为拓扑允许的推理加速提供蓝图。

Abstract: Scale does not uniformly improve reasoning - it restructures it. Analyzing 25,000+ chain-of-thought trajectories across four domains (Law, Science, Code, Math) and two scales (8B, 70B parameters), we discover that neural scaling laws trigger domain-specific phase transitions rather than uniform capability gains. Legal reasoning undergoes Crystallization: 45% collapse in representational dimensionality (d95: 501 -> 274), 31% increase in trajectory alignment, and 10x manifold untangling. Scientific and mathematical reasoning remain Liquid - geometrically invariant despite 9x parameter increase. Code reasoning forms a discrete Lattice of strategic modes (silhouette: 0.13 -> 0.42). This geometry predicts learnability. We introduce Neural Reasoning Operators - learned mappings from initial to terminal hidden states. In crystalline legal reasoning, our operator achieves 63.6% accuracy on held-out tasks via probe decoding, predicting reasoning endpoints without traversing intermediate states. We further identify a universal oscillatory signature (coherence ~ -0.4) invariant across domains and scales, suggesting attention and feedforward layers drive reasoning through opposing dynamics. These findings establish that the cost of thought is determined not by task difficulty but by manifold geometry - offering a blueprint for inference acceleration where topology permits.

</details>


### [72] [A Lightweight Modular Framework for Constructing Autonomous Agents Driven by Large Language Models: Design, Implementation, and Applications in AgentForge](https://arxiv.org/abs/2601.13383)
*Akbar Anbar Jafari,Cagri Ozcinar,Gholamreza Anbarjafari*

Main category: cs.AI

TL;DR: 论文提出轻量级框架AgentForge来构建大语言模型驱动的自主智能体，介绍其创新点、验证效果，为用户提供可进行开发部署的基础框架。


<details>
  <summary>Details</summary>
Motivation: 现有智能体框架存在架构刚性、供应商锁定和复杂性高问题，阻碍快速原型开发与部署。

Method: 提出AgentForge框架，有可组合技能抽象、统一LLM后端接口、基于YAML的配置系统，将技能组合机制形式化为DAG。

Result: 在四个基准场景实验中，AgentForge任务完成率有竞争力，相比LangChain和直接API集成分别减少62%和78%开发时间，编排开销低于100ms，模块化设计便于扩展。

Conclusion: AgentForge填补了LLM智能体生态系统的关键空白，为研究人员和从业者提供了构建、评估和部署自主智能体的生产就绪基础。

Abstract: The emergence of LLMs has catalyzed a paradigm shift in autonomous agent development, enabling systems capable of reasoning, planning, and executing complex multi-step tasks. However, existing agent frameworks often suffer from architectural rigidity, vendor lock-in, and prohibitive complexity that impedes rapid prototyping and deployment. This paper presents AgentForge, a lightweight, open-source Python framework designed to democratize the construction of LLM-driven autonomous agents through a principled modular architecture. AgentForge introduces three key innovations: (1) a composable skill abstraction that enables fine-grained task decomposition with formally defined input-output contracts, (2) a unified LLM backend interface supporting seamless switching between cloud-based APIs and local inference engines, and (3) a declarative YAML-based configuration system that separates agent logic from implementation details. We formalize the skill composition mechanism as a directed acyclic graph (DAG) and prove its expressiveness for representing arbitrary sequential and parallel task workflows. Comprehensive experimental evaluation across four benchmark scenarios demonstrates that AgentForge achieves competitive task completion rates while reducing development time by 62% compared to LangChain and 78% compared to direct API integration. Latency measurements confirm sub-100ms orchestration overhead, rendering the framework suitable for real-time applications. The modular design facilitates extension: we demonstrate the integration of six built-in skills and provide comprehensive documentation for custom skill development. AgentForge addresses a critical gap in the LLM agent ecosystem by providing researchers and practitioners with a production-ready foundation for constructing, evaluating, and deploying autonomous agents without sacrificing flexibility or performance.

</details>


### [73] [Explicit Cognitive Allocation: A Principle for Governed and Auditable Inference in Large Language Models](https://arxiv.org/abs/2601.13443)
*Héctor Manuel Manzanilla-Granados,Zaira Navarrete-Cazales,Miriam Pescador-Rojas,Tonahtiu Ramírez-Romero*

Main category: cs.AI

TL;DR: 论文指出当前大语言模型使用认知结构缺失问题，提出显式认知分配原则及CUA架构，实验表明CUA推理效果更好。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型现有使用模式认知结构缺失，导致可追溯性、认知控制和可重复性受限的问题。

Method: 引入显式认知分配原则，构建Cognitive Universal Agent (CUA)架构，通过控制实验对比CUA推理和基线大语言模型推理。

Result: 在农业领域多个提示中，CUA推理表现出更早且受结构控制的认知收敛、语义扩展下更高的认知对齐，能系统展示探究的工具格局；基线大语言模型推理对齐变异性大，未显式呈现工具结构。

Conclusion: 显式认知和工具分配的CUA架构在推理效果上优于基线大语言模型推理。

Abstract: The rapid adoption of large language models (LLMs) has enabled new forms of AI-assisted reasoning across scientific, technical, and organizational domains. However, prevailing modes of LLM use remain cognitively unstructured: problem framing, knowledge exploration, retrieval, methodological awareness, and explanation are typically collapsed into a single generative process. This cognitive collapse limits traceability, weakens epistemic control, and undermines reproducibility, particularly in high-responsibility settings.
  We introduce Explicit Cognitive Allocation, a general principle for structuring AI-assisted inference through the explicit separation and orchestration of epistemic functions. We instantiate this principle in the Cognitive Universal Agent (CUA), an architecture that organizes inference into distinct stages of exploration and framing, epistemic anchoring, instrumental and methodological mapping, and interpretive synthesis. Central to this framework is the notion of Universal Cognitive Instruments (UCIs), which formalize heterogeneous means, including computational, experimental, organizational, regulatory, and educational instruments, through which abstract inquiries become investigable.
  We evaluate the effects of explicit cognitive and instrumental allocation through controlled comparisons between CUA-orchestrated inference and baseline LLM inference under matched execution conditions. Across multiple prompts in the agricultural domain, CUA inference exhibits earlier and structurally governed epistemic convergence, higher epistemic alignment under semantic expansion, and systematic exposure of the instrumental landscape of inquiry. In contrast, baseline LLM inference shows greater variability in alignment and fails to explicitly surface instrumental structure.

</details>


### [74] [SpatialBench-UC: Uncertainty-Aware Evaluation of Spatial Prompt Following in Text-to-Image Generation](https://arxiv.org/abs/2601.13462)
*Amine Rostane*

Main category: cs.AI

TL;DR: 提出SpatialBench - UC基准测试评估文本到图像模型执行空间指令能力，评估三个基线模型，发现接地方法能改善结果但弃权仍是主要因素。


<details>
  <summary>Details</summary>
Motivation: 自动化评估文本到图像模型是否遵循显式空间指令存在困难。

Method: 引入SpatialBench - UC基准，包含200个提示并分组为100个反事实对，发布基准包，利用轻量级人工审计校准检查器，评估三个基线模型。

Result: 接地方法能显著提高通过率和覆盖率，弃权主要因漏检仍是主导因素。

Conclusion: 所提出的基准可进行可重复和可审计的模型对比评估。

Abstract: Evaluating whether text-to-image models follow explicit spatial instructions is difficult to automate. Object detectors may miss targets or return multiple plausible detections, and simple geometric tests can become ambiguous in borderline cases. Spatial evaluation is naturally a selective prediction problem, the checker may abstain when evidence is weak and report confidence so that results can be interpreted as a risk coverage tradeoff rather than a single score. We introduce SpatialBench-UC, a small, reproducible benchmark for pairwise spatial relations. The benchmark contains 200 prompts (50 object pairs times 4 relations) grouped into 100 counterfactual pairs obtained by swapping object roles. We release a benchmark package, versioned prompts, pinned configs, per-sample checker outputs, and report tables, enabling reproducible and auditable comparisons across models. We also include a lightweight human audit used to calibrate the checker's abstention margin and confidence threshold. We evaluate three baselines, Stable Diffusion 1.5, SD 1.5 BoxDiff, and SD 1.4 GLIGEN. The checker reports pass rate and coverage as well as conditional pass rates on decided samples. The results show that grounding methods substantially improve both pass rate and coverage, while abstention remains a dominant factor due mainly to missing detections.

</details>


### [75] [Context and Transcripts Improve Detection of Deepfake Audios of Public Figures](https://arxiv.org/abs/2601.13464)
*Chongyang Gao,Marco Postiglione,Julian Baldwin,Natalia Denisenko,Isabel Gortner,Luke Fosdick,Chiara Pulice,Sarit Kraus,V. S. Subrahmanian*

Main category: cs.AI

TL;DR: 本文创建数据集并提出基于上下文的音频深度伪造检测器CADD，证明上下文和转录本可提升检测器性能，且CADD对对抗策略更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 当前音频深度伪造检测器不考虑上下文和转录本，而人类会利用上下文评估信息真实性，因此需要改进。

Method: 创建JDD数据集，生成SYN数据集，提出CADD架构，并在ITW和P²V数据集上评估性能。

Result: 上下文和转录本可显著提升音频深度伪造检测器性能，多个基线检测器和传统分类器的F1、AUC和EER指标有明显改善；CADD对5种对抗策略更鲁棒，性能平均仅下降-0.71%。

Conclusion: 足够的上下文和/或转录本可显著提高音频深度伪造检测器的效能，CADD架构表现良好。

Abstract: Humans use context to assess the veracity of information. However, current audio deepfake detectors only analyze the audio file without considering either context or transcripts. We create and analyze a Journalist-provided Deepfake Dataset (JDD) of 255 public deepfakes which were primarily contributed by over 70 journalists since early 2024. We also generate a synthetic audio dataset (SYN) of dead public figures and propose a novel Context-based Audio Deepfake Detector (CADD) architecture. In addition, we evaluate performance on two large-scale datasets: ITW and P$^2$V. We show that sufficient context and/or the transcript can significantly improve the efficacy of audio deepfake detectors. Performance (measured via F1 score, AUC, and EER) of multiple baseline audio deepfake detectors and traditional classifiers can be improved by 5%-37.58% in F1-score, 3.77%-42.79% in AUC, and 6.17%-47.83% in EER. We additionally show that CADD, via its use of context and/or transcripts, is more robust to 5 adversarial evasion strategies, limiting performance degradation to an average of just -0.71% across all experiments. Code, models, and datasets are available at our project page: https://sites.northwestern.edu/nsail/cadd-context-based-audio-deepfake-detection (access restricted during review).

</details>


### [76] [Graph Neural Networks are Heuristics](https://arxiv.org/abs/2601.13465)
*Yimeng Min,Carla P. Gomes*

Main category: cs.AI

TL;DR: 单个训练轨迹可将图神经网络转变为组合优化的无监督启发式方法，以TSP为例展示优势。


<details>
  <summary>Details</summary>
Motivation: 探索图神经网络在组合优化中不依赖监督训练和显式搜索的有效方法。

Method: 将全局结构约束编码为归纳偏置，使非自回归模型直接前向传播生成解，推理时用dropout和快照集成。

Result: 图神经网络无需监督训练和显式搜索也有效，可内化全局组合结构。

Conclusion: 重新定义学习在组合优化中的角色，从增强经典算法到直接创建新启发式方法。

Abstract: We demonstrate that a single training trajectory can transform a graph neural network into an unsupervised heuristic for combinatorial optimization. Focusing on the Travelling Salesman Problem, we show that encoding global structural constraints as an inductive bias enables a non-autoregressive model to generate solutions via direct forward passes, without search, supervision, or sequential decision-making. At inference time, dropout and snapshot ensembling allow a single model to act as an implicit ensemble, reducing optimality gaps through increased solution diversity. Our results establish that graph neural networks do not require supervised training nor explicit search to be effective. Instead, they can internalize global combinatorial structure and function as strong, learned heuristics. This reframes the role of learning in combinatorial optimization: from augmenting classical algorithms to directly instantiating new heuristics.

</details>


### [77] [Towards Efficient and Robust Linguistic Emotion Diagnosis for Mental Health via Multi-Agent Instruction Refinement](https://arxiv.org/abs/2601.13481)
*Jian Zhang,Zhangqi Wang,Zhiyuan Wang,Weiping Fu,Yu He,Haiping Zhu,Qika Lin,Jun Liu*

Main category: cs.AI

TL;DR: 文章提出APOLO框架应对LLMs在医学场景下情感分析挑战，实验表明其能提升诊断准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: LLMs在高风险、语境密集医学场景下对提示设计敏感，现有方法存在情感共病和临床线索探索低效问题，需准确识别情感用于临床分诊等。

Method: 提出APOLO框架，将指令细化为部分可观察马尔可夫决策过程，采用多智能体协作机制，各角色分工协作。

Result: APOLO在特定领域和分层基准中持续提高诊断准确性和鲁棒性。

Conclusion: APOLO为可信LLM在精神医疗保健中的应用提供了可扩展和可推广的范式。

Abstract: Linguistic expressions of emotions such as depression, anxiety, and trauma-related states are pervasive in clinical notes, counseling dialogues, and online mental health communities, and accurate recognition of these emotions is essential for clinical triage, risk assessment, and timely intervention. Although large language models (LLMs) have demonstrated strong generalization ability in emotion analysis tasks, their diagnostic reliability in high-stakes, context-intensive medical settings remains highly sensitive to prompt design. Moreover, existing methods face two key challenges: emotional comorbidity, in which multiple intertwined emotional states complicate prediction, and inefficient exploration of clinically relevant cues. To address these challenges, we propose APOLO (Automated Prompt Optimization for Linguistic Emotion Diagnosis), a framework that systematically explores a broader and finer-grained prompt space to improve diagnostic efficiency and robustness. APOLO formulates instruction refinement as a Partially Observable Markov Decision Process and adopts a multi-agent collaboration mechanism involving Planner, Teacher, Critic, Student, and Target roles. Within this closed-loop framework, the Planner defines an optimization trajectory, while the Teacher-Critic-Student agents iteratively refine prompts to enhance reasoning stability and effectiveness, and the Target agent determines whether to continue optimization based on performance evaluation. Experimental results show that APOLO consistently improves diagnostic accuracy and robustness across domain-specific and stratified benchmarks, demonstrating a scalable and generalizable paradigm for trustworthy LLM applications in mental healthcare.

</details>


### [78] [Reasoning While Recommending: Entropy-Guided Latent Reasoning in Generative Re-ranking Models](https://arxiv.org/abs/2601.13533)
*Changshuo Zhang*

Main category: cs.AI

TL;DR: 文章引入EGLR推荐模型，该模型结合潜在推理机制有效降低决策熵，具多种优势，经实验验证可提升现有生成重排序模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有生成方法难以适应列表生成时模型难度的动态熵变，难以准确捕捉复杂偏好。

Method: 引入潜在推理机制，提出EGLR推荐模型，实现“推荐时推理”、熵引导的变长推理、轻量级集成设计。

Result: 在两个真实数据集上的实验验证了模型有效性，能与现有模型兼容提升其性能。

Conclusion: EGLR模型有效，具有实际部署价值和研究潜力。

Abstract: Reinforcement learning plays a crucial role in generative re-ranking scenarios due to its exploration-exploitation capabilities, but existing generative methods mostly fail to adapt to the dynamic entropy changes in model difficulty during list generation, making it challenging to accurately capture complex preferences. Given that language models have achieved remarkable breakthroughs by integrating reasoning capabilities, we draw on this approach to introduce a latent reasoning mechanism, and experimental validation demonstrates that this mechanism effectively reduces entropy in the model's decision-making process. Based on these findings, we introduce the Entropy-Guided Latent Reasoning (EGLR) recommendation model, which has three core advantages. First, it abandons the "reason first, recommend later" paradigm to achieve "reasoning while recommending", specifically designed for the high-difficulty nature of list generation by enabling real-time reasoning during generation. Second, it implements entropy-guided variable-length reasoning using context-aware reasoning token alongside dynamic temperature adjustment, expanding exploration breadth in reasoning and boosting exploitation precision in recommending to achieve a more precisely adapted exploration-exploitation trade-off. Third, the model adopts a lightweight integration design with no complex independent modules or post-processing, enabling easy adaptation to existing models. Experimental results on two real-world datasets validate the model's effectiveness, and its notable advantage lies in being compatible with existing generative re-ranking models to enhance their performance. Further analyses also demonstrate its practical deployment value and research potential.

</details>


### [79] [TruthTensor: Evaluating LLMs Human Imitation through Prediction Market Drift and Holistic Reasoning](https://arxiv.org/abs/2601.13545)
*Shirin Shahabi,Spencer Graham,Haruna Isah*

Main category: cs.AI

TL;DR: 文章介绍了新颖评估范式TruthTensor，并通过实验展示其优势，最后公开了该范式


<details>
  <summary>Details</summary>
Motivation: 现有静态基准难以评估语言模型和AI智能体，本文旨在引入新评估范式

Method: 基于前瞻性、无污染任务，结合实时预测市场与概率评分，并辅以漂移诊断和鲁棒性检查

Result: 在500多个真实市场实验中表明，相近预测准确率的模型在校准、漂移和风险敏感方面差异大

Conclusion: TruthTensor支持现代评估最佳实践，能对大语言模型做出可靠评估并公开

Abstract: Evaluating language models and AI agents remains fundamentally challenging because static benchmarks fail to capture real-world uncertainty, distribution shift, and the gap between isolated task accuracy and human-aligned decision-making under evolving conditions. This paper introduces TruthTensor, a novel, reproducible evaluation paradigm that measures Large Language Models (LLMs) not only as prediction engines but as human-imitation systems operating in socially-grounded, high-entropy environments. Building on forward-looking, contamination-free tasks, our framework anchors evaluation to live prediction markets and combines probabilistic scoring to provide a holistic view of model behavior. TruthTensor complements traditional correctness metrics with drift-centric diagnostics and explicit robustness checks for reproducibility. It specify human vs. automated evaluation roles, annotation protocols, and statistical testing procedures to ensure interpretability and replicability of results. In experiments across 500+ real markets (political, economic, cultural, technological), TruthTensor demonstrates that models with similar forecast accuracy can diverge markedly in calibration, drift, and risk-sensitivity, underscoring the need to evaluate models along multiple axes (accuracy, calibration, narrative stability, cost, and resource efficiency). TruthTensor therefore operationalizes modern evaluation best practices, clear hypothesis framing, careful metric selection, transparent compute/cost reporting, human-in-the-loop validation, and open, versioned evaluation contracts, to produce defensible assessments of LLMs in real-world decision contexts. We publicly release TruthTensor at https://truthtensor.com

</details>


### [80] [ChatAD: Reasoning-Enhanced Time-Series Anomaly Detection with Multi-Turn Instruction Evolution](https://arxiv.org/abs/2601.13546)
*Hui Sun,Chang Xu,Haonan Xie,Hao Li,Yuhao Huang,Chuheng Zhang,Ming Jin,Xiaoguang Liu,Gang Wang,Jiang Bian*

Main category: cs.AI

TL;DR: 提出TSEvol算法、TSEData - 20K数据集、ChatAD聊天机器人家族、TKTO优化方法和LLADBench基准，ChatAD模型在精度、F1值等方面有显著提升，优化后在多任务表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的时间序列异常检测方法存在推理能力不足、多轮对话能力欠缺和泛化性窄等问题。

Method: 提出TSEvol算法；引入TSEData - 20K数据集和ChatAD聊天机器人家族；提出TKTO优化方法；提出LLADBench基准。

Result: 三个ChatAD模型在精度上最高提升34.50%，F1值最高提升34.71%，误报率降低37.42%；优化后的ChatAD在分类、预测和插补任务的推理和跨任务泛化方面表现出色。

Conclusion: 所提方法有效提升了基于大语言模型的时间序列异常检测性能。

Abstract: LLM-driven Anomaly Detection (AD) helps enhance the understanding and explanatory abilities of anomalous behaviors in Time Series (TS). Existing methods face challenges of inadequate reasoning ability, deficient multi-turn dialogue capability, and narrow generalization. To this end, we 1) propose a multi-agent-based TS Evolution algorithm named TSEvol. On top of it, we 2) introduce the AD reasoning and multi-turn dialogue Dataset TSEData-20K and contribute the Chatbot family for AD, including ChatAD-Llama3-8B, Qwen2.5-7B, and Mistral-7B. Furthermore, 3) we propose the TS Kahneman-Tversky Optimization (TKTO) to enhance ChatAD's cross-task generalization capability. Lastly, 4) we propose a LLM-driven Learning-based AD Benchmark LLADBench to evaluate the performance of ChatAD and nine baselines across seven datasets and tasks. Our three ChatAD models achieve substantial gains, up to 34.50% in accuracy, 34.71% in F1, and a 37.42% reduction in false positives. Besides, via KTKO, our optimized ChatAD achieves competitive performance in reasoning and cross-task generalization on classification, forecasting, and imputation.

</details>


### [81] [Leveraging ChatGPT and Other NLP Methods for Identifying Risk and Protective Behaviors in MSM: Social Media and Dating apps Text Analysis](https://arxiv.org/abs/2601.13558)
*Mehrab Beikzadeh,Chenglin Hong,Cory J Cascalheira,Callisto Boka,Majid Sarrafzadeh,Ian W Holloway*

Main category: cs.AI

TL;DR: 研究评估社交媒体和约会应用文本能否预测男同性恋者性风险行为、饮酒和PrEP使用，模型有不错表现，显示文本数据对公共卫生干预有价值。


<details>
  <summary>Details</summary>
Motivation: 男同性恋者性传播感染和有害饮酒风险高，社交媒体和约会应用文本数据或为个性化公共卫生干预提供新机会。

Method: 经参与者同意收集文本数据，用ChatGPT嵌入、BERT嵌入、LIWC和基于词典的风险术语方法提取特征训练机器学习模型。

Result: 模型在预测每月 binge 饮酒和有超过五个性伴侣方面表现出色（F1分数0.78），在预测PrEP使用和大量饮酒方面表现中等（F1分数分别为0.64和0.63）。

Conclusion: 社交媒体和约会应用文本数据能为风险和保护行为提供有价值见解，基于大语言模型的方法有望支持针对男同性恋者的可扩展和个性化公共卫生干预。

Abstract: Men who have sex with men (MSM) are at elevated risk for sexually transmitted infections and harmful drinking compared to heterosexual men. Text data collected from social media and dating applications may provide new opportunities for personalized public health interventions by enabling automatic identification of risk and protective behaviors. In this study, we evaluated whether text from social media and dating apps can be used to predict sexual risk behaviors, alcohol use, and pre-exposure prophylaxis (PrEP) uptake among MSM. With participant consent, we collected textual data and trained machine learning models using features derived from ChatGPT embeddings, BERT embeddings, LIWC, and a dictionary-based risk term approach. The models achieved strong performance in predicting monthly binge drinking and having more than five sexual partners, with F1 scores of 0.78, and moderate performance in predicting PrEP use and heavy drinking, with F1 scores of 0.64 and 0.63. These findings demonstrate that social media and dating app text data can provide valuable insights into risk and protective behaviors and highlight the potential of large language model-based methods to support scalable and personalized public health interventions for MSM.

</details>


### [82] [AgentGC: Evolutionary Learning-based Lossless Compression for Genomics Data with LLM-driven Multiple Agent](https://arxiv.org/abs/2601.13559)
*Sun Hui,Ding Yanfeng,Huidong Ma,Chang Xu,Keyan Jin,Lizheng Zu,Cheng Zhong,xiaoguang Liu,Gang Wang,Wentong Cai*

Main category: cs.AI

TL;DR: 提出首个基于进化代理的基因组数据压缩器AgentGC，设计3种模式支持不同场景，相比基线有显著压缩比和吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 当前基于学习的基因组数据无损压缩方法存在不可进化、低级别压缩建模、适应性有限和用户界面不友好等问题。

Method: 提出AgentGC，包含用户层、认知层和压缩层，由Leader和Worker多智能体组成，设计CP、TP和BM三种模式。

Result: 在9个数据集上与14个基线相比，平均压缩比分别提升16.66%、16.11%和16.33%，吞吐量分别提升4.73倍、9.23倍和9.15倍。

Conclusion: AgentGC能有效解决现有方法问题，在压缩比和吞吐量上表现出色。

Abstract: Lossless compression has made significant advancements in Genomics Data (GD) storage, sharing and management. Current learning-based methods are non-evolvable with problems of low-level compression modeling, limited adaptability, and user-unfriendly interface. To this end, we propose AgentGC, the first evolutionary Agent-based GD Compressor, consisting of 3 layers with multi-agent named Leader and Worker. Specifically, the 1) User layer provides a user-friendly interface via Leader combined with LLM; 2) Cognitive layer, driven by the Leader, integrates LLM to consider joint optimization of algorithm-dataset-system, addressing the issues of low-level modeling and limited adaptability; and 3) Compression layer, headed by Worker, performs compression & decompression via a automated multi-knowledge learning-based compression framework. On top of AgentGC, we design 3 modes to support diverse scenarios: CP for compression-ratio priority, TP for throughput priority, and BM for balanced mode. Compared with 14 baselines on 9 datasets, the average compression ratios gains are 16.66%, 16.11%, and 16.33%, the throughput gains are 4.73x, 9.23x, and 9.15x, respectively.

</details>


### [83] [Reasoning is a Modality](https://arxiv.org/abs/2601.13562)
*Zhiguang Liu,Yi Shang*

Main category: cs.AI

TL;DR: 本文针对ARC任务提出新颖角色分离Transformer块，在VARC协议下训练评估，准确率超人类和先前方法。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统与人类推理方式存在差距，ARC可用于研究抽象推理，作者想探索推理作为独立模态的假说。

Method: 设计了一种新颖的角色分离Transformer块，将全局控制令牌与网格工作区令牌分离，实现迭代规则执行。

Result: 在ARC - 1上达到62.6%的准确率，超过人类平均表现（60.2%），显著优于先前方法。

Conclusion: 模型比密集ViT基线表现出更连贯的规则应用结构，显示出从似然概率团向控制器驱动推理的转变。

Abstract: The Abstraction and Reasoning Corpus (ARC) provides a compact laboratory for studying abstract reasoning, an ability central to human intelligence. Modern AI systems, including LLMs and ViTs, largely operate as sequence-of-behavior prediction machines: they match observable behaviors by modeling token statistics without a persistent, readable mental state. This creates a gap with human-like behavior: humans can explain an action by decoding internal state, while AI systems can produce fluent post-hoc rationalizations that are not grounded in such a state. We hypothesize that reasoning is a modality: reasoning should exist as a distinct channel separate from the low-level workspace on which rules are applied. To test this hypothesis, on solving ARC tasks as a visual reasoning problem, we designed a novel role-separated transformer block that splits global controller tokens from grid workspace tokens, enabling iterative rule execution. Trained and evaluated within the VARC vision-centric protocol, our method achieved 62.6% accuracy on ARC-1, surpassing average human performance (60.2%) and outperforming prior methods significantly. Qualitatively, our models exhibit more coherent rule-application structure than the dense ViT baseline, consistent with a shift away from plausible probability blobs toward controller-driven reasoning.

</details>


### [84] [SCRIPTMIND: Crime Script Inference and Cognitive Evaluation for LLM-based Social Engineering Scam Detection System](https://arxiv.org/abs/2601.13581)
*Heedou Kim,Changsik Kim,Sanghwa Shin,Jaewoo Kang*

Main category: cs.AI

TL;DR: 提出ScriptMind框架用于基于大语言模型的诈骗检测，在多方面表现优异，推动以人类为中心、具备认知适应性的大语言模型用于诈骗防御。


<details>
  <summary>Details</summary>
Motivation: 社交工程诈骗出现个性化、多轮欺骗，传统检测方法有局限，大语言模型识别欺骗的认知辅助潜力待挖掘。

Method: 提出ScriptMind框架，包含CSIT、CSID和CSED三个组件，利用571个韩国电话诈骗案例构建训练实例。

Result: 用ScriptMind微调的11B小语言模型在检测准确率等多方面胜出GPT - 4o 13%，在电话诈骗模拟实验中提升用户对诈骗的认知。

Conclusion: ScriptMind是迈向以人类为中心、认知适应性大语言模型用于诈骗防御的一步。

Abstract: Social engineering scams increasingly employ personalized, multi-turn deception, exposing the limits of traditional detection methods. While Large Language Models (LLMs) show promise in identifying deception, their cognitive assistance potential remains underexplored. We propose ScriptMind, an integrated framework for LLM-based scam detection that bridges automated reasoning and human cognition. It comprises three components: the Crime Script Inference Task (CSIT) for scam reasoning, the Crime Script-Aware Inference Dataset (CSID) for fine-tuning small LLMs, and the Cognitive Simulation-based Evaluation of Social Engineering Defense (CSED) for assessing real-time cognitive impact. Using 571 Korean phone scam cases, we built 22,712 structured scammer-sequence training instances. Experimental results show that the 11B small LLM fine-tuned with ScriptMind outperformed GPT-4o by 13%, achieving superior performance over commercial models in detection accuracy, false-positive reduction, scammer utterance prediction, and rationale quality. Moreover, in phone scam simulation experiments, it significantly enhanced and sustained users' suspicion levels, improving their cognitive awareness of scams. ScriptMind represents a step toward human-centered, cognitively adaptive LLMs for scam defense.

</details>


### [85] [Motion-to-Response Content Generation via Multi-Agent AI System with Real-Time Safety Verification](https://arxiv.org/abs/2601.13589)
*HyeYoung Lee*

Main category: cs.AI

TL;DR: 提出基于音频情感信号实时生成响应式媒体内容的多智能体AI系统，含四个协作智能体，实验有不错结果且架构适用多场景。


<details>
  <summary>Details</summary>
Motivation: 区别于传统注重分类准确率的语音情感识别研究，将推断出的情感状态转化为安全、适合年龄且可控的响应内容。

Method: 构建包含情感识别、响应策略决策、内容参数生成和安全验证四个协作智能体的系统，引入安全验证循环过滤内容。

Result: 在公共数据集上，系统情感识别准确率73.2%，响应模式一致性89.4%，安全合规性100%，推理延迟低于100ms。

Conclusion: 系统模块化架构具有可解释性和可扩展性，适用于儿童相关媒体、治疗应用和情感响应智能设备。

Abstract: This paper proposes a multi-agent artificial intelligence system that generates response-oriented media content in real time based on audio-derived emotional signals. Unlike conventional speech emotion recognition studies that focus primarily on classification accuracy, our approach emphasizes the transformation of inferred emotional states into safe, age-appropriate, and controllable response content through a structured pipeline of specialized AI agents. The proposed system comprises four cooperative agents: (1) an Emotion Recognition Agent with CNN-based acoustic feature extraction, (2) a Response Policy Decision Agent for mapping emotions to response modes, (3) a Content Parameter Generation Agent for producing media control parameters, and (4) a Safety Verification Agent enforcing age-appropriateness and stimulation constraints. We introduce an explicit safety verification loop that filters generated content before output, ensuring compliance with predefined rules. Experimental results on public datasets demonstrate that the system achieves 73.2% emotion recognition accuracy, 89.4% response mode consistency, and 100% safety compliance while maintaining sub-100ms inference latency suitable for on-device deployment. The modular architecture enables interpretability and extensibility, making it applicable to child-adjacent media, therapeutic applications, and emotionally responsive smart devices.

</details>


### [86] [DSAEval: Evaluating Data Science Agents on a Wide Range of Real-World Data Science Problems](https://arxiv.org/abs/2601.13591)
*Maojun Sun,Yifei Xie,Yue Wu,Ruijian Han,Binyan Jiang,Defeng Sun,Yancheng Yuan,Jian Huang*

Main category: cs.AI

TL;DR: 介绍DSAEval基准评估数据科学代理，评估11个大模型，指出各模型优势，发现当前代理在非结构化领域有挑战并给出研究方向。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据科学问题开放性大、缺乏标准答案，现有LLM数据代理评估面临挑战，需新评估基准。

Method: 引入包含641个实际数据科学问题、285个多样数据集的DSAEval基准进行评估，该基准有三特征。

Result: Claude - Sonnet - 4.5整体性能最强，GPT - 5.2最有效率，MiMo - V2 - Flash最具性价比，多模态感知提升视觉任务表现。

Conclusion: 当前数据科学代理在结构化数据和常规分析流程表现好，但非结构化领域挑战大，给出研究方向。

Abstract: Recent LLM-based data agents aim to automate data science tasks ranging from data analysis to deep learning. However, the open-ended nature of real-world data science problems, which often span multiple taxonomies and lack standard answers, poses a significant challenge for evaluation. To address this, we introduce DSAEval, a benchmark comprising 641 real-world data science problems grounded in 285 diverse datasets, covering both structured and unstructured data (e.g., vision and text). DSAEval incorporates three distinctive features: (1) Multimodal Environment Perception, which enables agents to interpret observations from multiple modalities including text and vision; (2) Multi-Query Interactions, which mirror the iterative and cumulative nature of real-world data science projects; and (3) Multi-Dimensional Evaluation, which provides a holistic assessment across reasoning, code, and results. We systematically evaluate 11 advanced agentic LLMs using DSAEval. Our results show that Claude-Sonnet-4.5 achieves the strongest overall performance, GPT-5.2 is the most efficient, and MiMo-V2-Flash is the most cost-effective. We further demonstrate that multimodal perception consistently improves performance on vision-related tasks, with gains ranging from 2.04% to 11.30%. Overall, while current data science agents perform well on structured data and routine data anlysis workflows, substantial challenges remain in unstructured domains. Finally, we offer critical insights and outline future research directions to advance the development of data science agents.

</details>


### [87] [Foundations of Global Consistency Checking with Noisy LLM Oracles](https://arxiv.org/abs/2601.13600)
*Paul He,Elke Kirschbaum,Shiva Kasiviswanathan*

Main category: cs.AI

TL;DR: 本文提出自适应分治算法解决自然语言事实集合全局一致性验证问题，实验证明其有效可扩展。


<details>
  <summary>Details</summary>
Motivation: 大语言模型评估事实一致性有噪声，成对检查无法保证全局一致性，需解决全局一致性验证问题。

Method: 提出自适应分治算法识别事实的最小不一致子集，可选地通过击中集计算最小修复方案。

Result: 在合成和真实大语言模型预言机上的实验表明，该方法能有效检测和定位不一致性。

Conclusion: 提供了一个基于大语言模型评估器的可扩展的语言一致性验证框架。

Abstract: Ensuring that collections of natural-language facts are globally consistent is essential for tasks such as fact-checking, summarization, and knowledge base construction. While Large Language Models (LLMs) can assess the consistency of small subsets of facts, their judgments are noisy, and pairwise checks are insufficient to guarantee global coherence. We formalize this problem and show that verifying global consistency requires exponentially many oracle queries in the worst case. To make the task practical, we propose an adaptive divide-and-conquer algorithm that identifies minimal inconsistent subsets (MUSes) of facts and optionally computes minimal repairs through hitting-sets. Our approach has low-degree polynomial query complexity. Experiments with both synthetic and real LLM oracles show that our method efficiently detects and localizes inconsistencies, offering a scalable framework for linguistic consistency verification with LLM-based evaluators.

</details>


### [88] [Resilient Routing: Risk-Aware Dynamic Routing in Smart Logistics via Spatiotemporal Graph Learning](https://arxiv.org/abs/2601.13632)
*Zhiming Xue,Sichen Zhao,Yalun Qi,Xianling Zeng,Zihan Yu*

Main category: cs.AI

TL;DR: 本文提出Risk - Aware Dynamic Routing (RADR)框架应对电商物流网络压力，经实验验证该框架有效。


<details>
  <summary>Details</summary>
Motivation: 电商行业快速发展使物流网络面临巨大压力，传统静态路由策略难以应对交通拥堵和零售需求波动。

Method: 构建物流拓扑图，采用结合GCN和GRU的混合深度学习模型预测拥堵风险，将结果融入动态边权机制进行路径规划。

Result: 在Smart Logistics Dataset 2024上实验表明，RADR算法显著增强供应链弹性，高拥堵场景下降低潜在拥堵风险暴露19.3%，仅增加运输距离2.1%。

Conclusion: 所提数据驱动方法能有效平衡交付效率和运营安全。

Abstract: With the rapid development of the e-commerce industry, the logistics network is experiencing unprecedented pressure. The traditional static routing strategy most time cannot tolerate the traffic congestion and fluctuating retail demand. In this paper, we propose a Risk-Aware Dynamic Routing(RADR) framework which integrates Spatiotemporal Graph Neural Networks (ST-GNN) with combinatorial optimization. We first construct a logistics topology graph by using the discrete GPS data using spatial clustering methods. Subsequently, a hybrid deep learning model combining Graph Convolutional Network (GCN) and Gated Recurrent Unit (GRU) is adopted to extract spatial correlations and temporal dependencies for predicting future congestion risks. These prediction results are then integrated into a dynamic edge weight mechanism to perform path planning. We evaluated the framework on the Smart Logistics Dataset 2024, which contains real-world Internet of Things(IoT) sensor data. The experimental results show that the RADR algorithm significantly enhances the resilience of the supply chain. Particularly in the case study of high congestion scenarios, our method reduces the potential congestion risk exposure by 19.3% while only increasing the transportation distance by 2.1%. This empirical evidence confirms that the proposed data-driven approach can effectively balance delivery efficiency and operational safety.

</details>


### [89] [Understanding Mental States to Guide Social Influence in Multi-Person Group Dialogue](https://arxiv.org/abs/2601.13687)
*Zhichao Liang,Satoshi Nakamura*

Main category: cs.AI

TL;DR: 介绍SocialMindChange基准，其从追踪思维转向改变思维，构建大量社会情境，评估显示当前大语言模型表现远低于人类。


<details>
  <summary>Details</summary>
Motivation: 现有动态心智理论基准多让语言模型处于被动角色，而现实社交中心智理论也用于行动。

Method: 使用结构化四步框架构建1200个社会情境、6000个场景和超9万个问题并验证质量。

Result: 对十个最先进的大语言模型评估显示，其平均表现比人类低54.2%。

Conclusion: 当前大语言模型在长的关联互动中仍难以维持和改变心理状态表征。

Abstract: Existing dynamic Theory of Mind (ToM) benchmarks mostly place language models in a passive role: the model reads a sequence of connected scenarios and reports what people believe, feel, intend, and do as these states change. In real social interaction, ToM is also used for action: a speaker plans what to say in order to shift another person's mental-state trajectory toward a goal. We introduce SocialMindChange, a benchmark that moves from tracking minds to changing minds in social interaction. Each instance defines a social context with 4 characters and five connected scenes. The model plays one character and generates dialogue across the five scenes to reach the target while remaining consistent with the evolving states of all participants. SocialMindChange also includes selected higher-order states. Using a structured four-step framework, we construct 1,200 social contexts, covering 6000 scenarios and over 90,000 questions, each validated for realism and quality. Evaluations on ten state-of-the-art LLMs show that their average performance is 54.2% below human performance. This gap suggests that current LLMs still struggle to maintain and change mental-state representations across long, linked interactions.

</details>


### [90] [Hidden in Plain Text: Measuring LLM Deception Quality Against Human Baselines Using Social Deduction Games](https://arxiv.org/abs/2601.13709)
*Christopher Kao,Vanshika Vats,James Davis*

Main category: cs.AI

TL;DR: 研究大语言模型（LLM）代理在社交推理游戏Mafia中的欺骗能力，发现LLM比人类更能有效欺骗，还发布相关数据集。


<details>
  <summary>Details</summary>
Motivation: 此前研究对LLM在社交场景中使用自然语言进行欺骗的能力了解较少，本文旨在研究其在社交推理游戏Mafia中的欺骗能力。

Method: 使用异步多智能体框架模拟35场GPT - 4o LLM代理的Mafia游戏，用GPT - 4 - Turbo创建Mafia检测器分析游戏记录以预测黑手党玩家，以预测准确率作为欺骗质量的替代指标，并与28场人类游戏和随机基线进行比较。

Result: Mafia检测器对LLM游戏中黑手党玩家的预测准确率低于人类游戏，且在不同游戏天数和检测出的黑手党数量下结果一致。

Conclusion: LLM在社交场景中融合得更好，欺骗更有效，凸显了LLM欺骗的复杂性和风险。

Abstract: Large Language Model (LLM) agents are increasingly used in many applications, raising concerns about their safety. While previous work has shown that LLMs can deceive in controlled tasks, less is known about their ability to deceive using natural language in social contexts. In this paper, we study deception in the Social Deduction Game (SDG) Mafia, where success is dependent on deceiving others through conversation. Unlike previous SDG studies, we use an asynchronous multi-agent framework which better simulates realistic social contexts. We simulate 35 Mafia games with GPT-4o LLM agents. We then create a Mafia Detector using GPT-4-Turbo to analyze game transcripts without player role information to predict the mafia players. We use prediction accuracy as a surrogate marker for deception quality. We compare this prediction accuracy to that of 28 human games and a random baseline. Results show that the Mafia Detector's mafia prediction accuracy is lower on LLM games than on human games. The result is consistent regardless of the game days and the number of mafias detected. This indicates that LLMs blend in better and thus deceive more effectively. We also release a dataset of LLM Mafia transcripts to support future research. Our findings underscore both the sophistication and risks of LLM deception in social contexts.

</details>


### [91] [Reasoning or Fluency? Dissecting Probabilistic Confidence in Best-of-N Selection](https://arxiv.org/abs/2601.13735)
*Hojin Kim,Jaehyung Kim*

Main category: cs.AI

TL;DR: 文章质疑了概率置信度指标用于推理质量评估的假设，发现其对逻辑结构不敏感，提出对比因果度量指标。


<details>
  <summary>Details</summary>
Motivation: 研究概率置信度指标是否能真正捕捉推理所需的步骤间因果依赖，挑战其作为推理质量代理指标的假设。

Method: 引入三类步骤间因果扰动，在不同模型和基准上进行测试；提出对比因果度量指标。

Result: 在因果扰动下，选择准确性仅轻微下降，严重干预也未大幅降低选择性能；对比因果度量指标比现有基于概率的方法能更准确地选择输出。

Conclusion: 当前概率度量指标对逻辑结构不敏感，主要捕捉表面流畅性或分布内先验；对比因果度量指标更能忠实于选择输出。

Abstract: Probabilistic confidence metrics are increasingly adopted as proxies for reasoning quality in Best-of-N selection, under the assumption that higher confidence reflects higher reasoning fidelity. In this work, we challenge this assumption by investigating whether these metrics truly capture inter-step causal dependencies necessary for valid reasoning. We introduce three classes of inter-step causality perturbations that systematically disrupt dependencies between reasoning steps while preserving local fluency. Surprisingly, across diverse model families and reasoning benchmarks, we find that selection accuracy degrades only marginally under these disruptions. Even severe interventions, such as applying hard attention masks that directly prevent the model from attending to prior reasoning steps, do not substantially reduce selection performance. These findings provide strong evidence that current probabilistic metrics are largely insensitive to logical structure, and primarily capture surface-level fluency or in-distribution priors instead. Motivated by this gap, we propose a contrastive causality metric that explicitly isolates inter-step causal dependencies, and demonstrate that it yields more faithful output selection than existing probability-based approaches.

</details>


### [92] [Finding RELIEF: Shaping Reasoning Behavior without Reasoning Supervision via Belief Engineering](https://arxiv.org/abs/2601.13752)
*Chak Tou Leong,Dingwei Chen,Heming Xia,Qingyu Yin,Sunbowen Lee,Jian Wang,Wenjie Li*

Main category: cs.AI

TL;DR: 论文揭示大推理模型（LRMs）有潜在推理信念，提出RELIEF框架塑造其行为，实验表明它性能好且成本低。


<details>
  <summary>Details</summary>
Motivation: 当前塑造LRM行为的方法计算成本高且难扩展，且LRMs存在计算冗余或推理不忠实问题。

Method: 通过简单logit探测捕捉LRMs的潜在推理信念，提出RELIEF框架，通过微调合成的自问自答对使模型内化目标信念。

Result: 在效率和忠实性任务的大量实验中，RELIEF达到或超越基于行为监督和偏好的基线方法，且训练成本更低。

Conclusion: 改变模型的推理信念能有效塑造其实际行为。

Abstract: Large reasoning models (LRMs) have achieved remarkable success in complex problem-solving, yet they often suffer from computational redundancy or reasoning unfaithfulness. Current methods for shaping LRM behavior typically rely on reinforcement learning or fine-tuning with gold-standard reasoning traces, a paradigm that is both computationally expensive and difficult to scale. In this paper, we reveal that LRMs possess latent \textit{reasoning beliefs} that internally track their own reasoning traits, which can be captured through simple logit probing. Building upon this insight, we propose Reasoning Belief Engineering (RELIEF), a simple yet effective framework that shapes LRM behavior by aligning the model's self-concept with a target belief blueprint. Crucially, RELIEF completely bypasses the need for reasoning-trace supervision. It internalizes desired traits by fine-tuning on synthesized, self-reflective question-answering pairs that affirm the target belief. Extensive experiments on efficiency and faithfulness tasks demonstrate that RELIEF matches or outperforms behavior-supervised and preference-based baselines while requiring lower training costs. Further analysis validates that shifting a model's reasoning belief effectively shapes its actual behavior.

</details>


### [93] [DARC: Decoupled Asymmetric Reasoning Curriculum for LLM Evolution](https://arxiv.org/abs/2601.13761)
*Shengda Fan,Xuyan Ye,Yankai Lin*

Main category: cs.AI

TL;DR: 提出DARC框架解决自博弈优化不稳定问题，在多个推理基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有自博弈框架存在优化不稳定问题，包括提问者的非平稳目标和求解器的自生成伪标签引导误差。

Method: 提出两阶段DARC框架，先训练提问者合成难度校准问题，再用非对称自蒸馏机制训练求解器。

Result: DARC模型无关，在九个推理基准和三个骨干模型上平均提升10.9分，始终优于基线模型，接近全监督模型表现。

Conclusion: DARC有效缓解自博弈优化不稳定问题，不依赖人工注释即可取得好效果。

Abstract: Self-play with large language models has emerged as a promising paradigm for achieving self-improving artificial intelligence. However, existing self-play frameworks often suffer from optimization instability, due to (i) non-stationary objectives induced by solver-dependent reward feedback for the Questioner, and (ii) bootstrapping errors from self-generated pseudo-labels used to supervise the Solver. To mitigate these challenges, we introduce DARC (Decoupled Asymmetric Reasoning Curriculum), a two-stage framework that stabilizes the self-evolution process. First, we train the Questioner to synthesize difficulty-calibrated questions, conditioned on explicit difficulty levels and external corpora. Second, we train the Solver with an asymmetric self-distillation mechanism, where a document-augmented teacher generates high-quality pseudo-labels to supervise the student Solver that lacks document access. Empirical results demonstrate that DARC is model-agnostic, yielding an average improvement of 10.9 points across nine reasoning benchmarks and three backbone models. Moreover, DARC consistently outperforms all baselines and approaches the performance of fully supervised models without relying on human annotations.The code is available at https://github.com/RUCBM/DARC.

</details>


### [94] [Virtual Urbanism: An AI-Driven Framework for Quantifying Urban Identity. A Tokyo-Based Pilot Study Using Diffusion-Generated Synthetic Environments](https://arxiv.org/abs/2601.13846)
*Glinskaya Maria*

Main category: cs.AI

TL;DR: 本文介绍通过合成城市复制品量化城市身份的多模态AI驱动分析框架Virtual Urbanism (VU)，以东京为例验证其可行性，结果表明框架有效。


<details>
  <summary>Details</summary>
Motivation: 提出可计算的城市身份度量指标，推进城市身份量化分析。

Method: 以Vu框架开展东京缩影的试点研究，用集成Stable Diffusion和LoRA模型的流程合成九个东京区域的动态合成城市序列，开展人类评估实验。

Result: 复制品平均识别准确率约81%，证实有效性；Urban Identity Level (UIL)指标可评估区域身份水平；语义分析显示文化类型为核心身份要素。

Conclusion: VU是可行的AI增强城市分析框架，为自动化多参数身份度量指明方向。

Abstract: This paper introduces Virtual Urbanism (VU), a multimodal AI-driven analytical framework for quantifying urban identity through the medium of synthetic urban replicas. The framework aims to advance computationally tractable urban identity metrics. To demonstrate feasibility, the pilot study Virtual Urbanism and Tokyo Microcosms is presented. A pipeline integrating Stable Diffusion and LoRA models was used to produce synthetic replicas of nine Tokyo areas rendered as dynamic synthetic urban sequences, excluding existing orientation markers to elicit core identity-forming elements. Human-evaluation experiments (I) assessed perceptual legitimacy of replicas; (II) quantified area-level identity; (III) derived core identity-forming elements. Results showed a mean identification accuracy of ~81%, confirming the validity of the replicas. Urban Identity Level (UIL) metric enabled assessment of identity levels across areas, while semantic analysis revealed culturally embedded typologies as core identity-forming elements, positioning VU as a viable framework for AI-augmented urban analysis, outlining a path toward automated, multi-parameter identity metrics.

</details>


### [95] [LifeAgentBench: A Multi-dimensional Benchmark and Agent for Personal Health Assistants in Digital Health](https://arxiv.org/abs/2601.13880)
*Ye Tian,Zihao Wang,Onat Gungor,Xiaoran Fan,Tajana Rosing*

Main category: cs.AI

TL;DR: 本文提出LifeAgentBench基准评估LLM在生活方式健康推理能力，找出关键瓶颈，还提出LifeAgent作为健康助手基准模型并取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在个性化数字健康支持场景下能力不明，缺乏系统基准。

Method: 引入LifeAgentBench基准，公布构建流程和评估协议，评估11种主流LLM，提出LifeAgent集成多步证据检索和确定性聚合方法。

Result: 确定LLM在长周期聚合和跨维度推理的关键瓶颈，LifeAgent相比基线有显著改进。

Conclusion: LifeAgentBench可可靠可扩展评估LLM健康助手，LifeAgent在现实场景有应用潜力，基准已公开。

Abstract: Personalized digital health support requires long-horizon, cross-dimensional reasoning over heterogeneous lifestyle signals, and recent advances in mobile sensing and large language models (LLMs) make such support increasingly feasible. However, the capabilities of current LLMs in this setting remain unclear due to the lack of systematic benchmarks. In this paper, we introduce LifeAgentBench, a large-scale QA benchmark for long-horizon, cross-dimensional, and multi-user lifestyle health reasoning, containing 22,573 questions spanning from basic retrieval to complex reasoning. We release an extensible benchmark construction pipeline and a standardized evaluation protocol to enable reliable and scalable assessment of LLM-based health assistants. We then systematically evaluate 11 leading LLMs on LifeAgentBench and identify key bottlenecks in long-horizon aggregation and cross-dimensional reasoning. Motivated by these findings, we propose LifeAgent as a strong baseline agent for health assistant that integrates multi-step evidence retrieval with deterministic aggregation, achieving significant improvements compared with two widely used baselines. Case studies further demonstrate its potential in realistic daily-life scenarios. The benchmark is publicly available at https://anonymous.4open.science/r/LifeAgentBench-CE7B.

</details>


### [96] [Human Simulation Computation: A Human-Inspired Framework for Adaptive AI Systems](https://arxiv.org/abs/2601.13887)
*Hong Su*

Main category: cs.AI

TL;DR: 文章指出大语言模型仅依赖文本有局限，提出人类模拟计算（HSC）框架，强调其在内部推理和与环境交互的作用，并通过理论分析说明人类模拟策略不能仅从语言学习。


<details>
  <summary>Details</summary>
Motivation: 大语言模型仅依赖语言材料，在真实环境中适应、验证推理结果和有效运作的能力受限。

Method: 提出人类模拟计算（HSC）框架，将智能建模为涉及思考、行动、学习等的闭环内部推理过程，且融入人类常用思维策略。

Result: 通过理论分析，论证人类模拟策略不能仅从语言材料学习。

Conclusion: 类人推理过程和基于行动的推理方法对适应现实环境和有效交互至关重要。

Abstract: Large language models (LLMs) have demonstrated strong capabilities in knowledge representation and reasoning based on textual data. However, their reliance on language material alone limits their ability to adapt, verify reasoning outcomes, and operate effectively in open and dynamic real-world environments. In this paper, we propose Human Simulation Computation (HSC), a human-inspired computational framework that models intelligence as a continuous, closed-loop process involving thinking, action, learning, reflection, and activity scheduling, collectively referred to as the internal reasoning process. HSC emphasizes active participation both within the internal reasoning process and in interactions with the environment, where actions are used not only to achieve goals but also to automatically refine and improve internal reasoning mechanisms without external intervention. Furthermore, HSC incorporates commonly used human thinking strategies across all stages of the internal reasoning process, such as main-feature-oriented reasoning, scope expansion through action, and on-time learning driven by environmental feedback. Through theoretical analysis, we argue that human simulation strategies cannot be fully learned from language material alone, and that human-like reasoning processes and action-grounded reasoning methods are essential for robust adaptation and effective interaction with real-world environments.

</details>


### [97] [PREFAB: PREFerence-based Affective Modeling for Low-Budget Self-Annotation](https://arxiv.org/abs/2601.13904)
*Jaeyoung Moon,Youjin Choi,Yucheon Park,David Melhart,Georgios N. Yannakakis,Kyung-Joong Kim*

Main category: cs.AI

TL;DR: 提出低预算回顾式自我注释法PREFAB改善情感标注，结果良好。


<details>
  <summary>Details</summary>
Motivation: 现有全量情感状态标注方法耗时、易疲劳出错，需改进。

Method: 基于峰终规则和情感序数表示，用偏好学习模型检测情感变化，只标注选定片段并插值其余部分，引入预览机制辅助标注。

Result: PREFAB在建模情感变化上优于基线，降低工作量，提高标注者信心且不降低质量。

Conclusion: PREFAB是有效的低预算回顾式自我注释方法。

Abstract: Self-annotation is the gold standard for collecting affective state labels in affective computing. Existing methods typically rely on full annotation, requiring users to continuously label affective states across entire sessions. While this process yields fine-grained data, it is time-consuming, cognitively demanding, and prone to fatigue and errors. To address these issues, we present PREFAB, a low-budget retrospective self-annotation method that targets affective inflection regions rather than full annotation. Grounded in the peak-end rule and ordinal representations of emotion, PREFAB employs a preference-learning model to detect relative affective changes, directing annotators to label only selected segments while interpolating the remainder of the stimulus. We further introduce a preview mechanism that provides brief contextual cues to assist annotation. We evaluate PREFAB through a technical performance study and a 25-participant user study. Results show that PREFAB outperforms baselines in modeling affective inflections while mitigating workload (and conditionally mitigating temporal burden). Importantly PREFAB improves annotator confidence without degrading annotation quality.

</details>


### [98] [Numina-Lean-Agent: An Open and General Agentic Reasoning System for Formal Mathematics](https://arxiv.org/abs/2601.14027)
*Junqi Liu,Zihao Zhou,Zekai Zhu,Marco Dos Santos,Weikun He,Jiawei Liu,Ran Wang,Yunzhou Xie,Junqiao Zhao,Qiufeng Wang,Lihong Zhi,Jia Li,Wenda Li*

Main category: cs.AI

TL;DR: 提出用通用编码代理作为形式数学推理器的范式，引入Numina - Lean - Agent，解决Putnam 2025所有问题，还成功形式化Brascamp - Lieb定理并开源。


<details>
  <summary>Details</summary>
Motivation: 现有定理证明方法依赖特定任务管道和训练过的形式证明器，灵活性和可重复性受限，通用编码代理有自然接口、可简单换模型提升性能、能灵活扩展和调用工具。

Method: 提出使用通用编码代理作为形式数学推理器的范式，引入Numina - Lean - Agent，结合Claude Code与Numina - Lean - MCP实现与Lean的自主交互等功能。

Result: 使用Claude Opus 4.5作为基础模型，Numina - Lean - Agent解决了Putnam 2025的所有问题，还成功与数学家交互形式化了Brascamp - Lieb定理。

Conclusion: 所提出的范式和Numina - Lean - Agent有效，具有良好性能和通用性，已开源供使用。

Abstract: Agentic systems have recently become the dominant paradigm for formal theorem proving, achieving strong performance by coordinating multiple models and tools. However, existing approaches often rely on task-specific pipelines and trained formal provers, limiting their flexibility and reproducibility. In this paper, we propose the paradigm that directly uses a general coding agent as a formal math reasoner. This paradigm is motivated by (1) A general coding agent provides a natural interface for diverse reasoning tasks beyond proving, (2) Performance can be improved by simply replacing the underlying base model, without training, and (3) MCP enables flexible extension and autonomous calling of specialized tools, avoiding complex design. Based on this paradigm, we introduce Numina-Lean-Agent, which combines Claude Code with Numina-Lean-MCP to enable autonomous interaction with Lean, retrieval of relevant theorems, informal proving and auxiliary reasoning tools. Using Claude Opus 4.5 as the base model, Numina-Lean-Agent solves all problems in Putnam 2025 (12 / 12), matching the best closed-source system. Beyond benchmark evaluation, we further demonstrate its generality by interacting with mathematicians to successfully formalize the Brascamp-Lieb theorem. We release Numina-Lean-Agent and all solutions at https://github.com/project-numina/numina-lean-agent.

</details>


### [99] [Remapping and navigation of an embedding space via error minimization: a fundamental organizational principle of cognition in natural and artificial systems](https://arxiv.org/abs/2601.14096)
*Benedikt Hartl,Léo Pio-Lopez,Chris Fields,Michael Levin*

Main category: cs.AI

TL;DR: 本文探讨不同智能领域，提出认知可由嵌入空间重映射和导航两个不变量表征，揭示生物与AI系统共性及工程化框架。


<details>
  <summary>Details</summary>
Motivation: 探索不同来源、组成和基底的智能体解决问题的统一观点，发现决策的尺度不变原则。

Method: 分析生物集体（从单细胞到生物整体）在各空间的重映射和导航，以及现代AI系统的数据重映射和迭代优化。

Result: 发现生物与AI系统存在通过迭代误差最小化进行嵌入空间重映射和导航的共同机制。

Conclusion: 该机制是与基底无关的认知不变量，为跨尺度工程化自适应智能提供统一框架。

Abstract: The emerging field of diverse intelligence seeks an integrated view of problem-solving in agents of very different provenance, composition, and substrates. From subcellular chemical networks to swarms of organisms, and across evolved, engineered, and chimeric systems, it is hypothesized that scale-invariant principles of decision-making can be discovered. We propose that cognition in both natural and synthetic systems can be characterized and understood by the interplay between two equally important invariants: (1) the remapping of embedding spaces, and (2) the navigation within these spaces. Biological collectives, from single cells to entire organisms (and beyond), remap transcriptional, morphological, physiological, or 3D spaces to maintain homeostasis and regenerate structure, while navigating these spaces through distributed error correction. Modern Artificial Intelligence (AI) systems, including transformers, diffusion models, and neural cellular automata enact analogous processes by remapping data into latent embeddings and refining them iteratively through contextualization. We argue that this dual principle - remapping and navigation of embedding spaces via iterative error minimization - constitutes a substrate-independent invariant of cognition. Recognizing this shared mechanism not only illuminates deep parallels between living systems and artificial models, but also provides a unifying framework for engineering adaptive intelligence across scales.

</details>


### [100] [Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance](https://arxiv.org/abs/2601.14171)
*Qianli Ma,Chang Guo,Zhiheng Tian,Siyu Wang,Jipeng Xiao,Yuanhao Yue,Zhipeng Zhang*

Main category: cs.AI

TL;DR: 提出RebuttalAgent多智能体框架处理论文反驳生成问题，在RebuttalBench上验证性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 当前论文反驳生成方案存在幻觉、忽略批评、缺乏可验证依据等问题。

Method: 引入RebuttalAgent多智能体框架，将反驳生成重构为以证据为中心的规划任务，分解反馈、构建混合上下文，生成可检查的响应计划。

Result: 在RebuttalBench上验证，该方法在覆盖度、忠实度和策略一致性上优于强基线。

Conclusion: RebuttalAgent为同行评审过程提供了透明且可控的辅助工具，代码将发布。

Abstract: Writing effective rebuttals is a high-stakes task that demands more than linguistic fluency, as it requires precise alignment between reviewer intent and manuscript details. Current solutions typically treat this as a direct-to-text generation problem, suffering from hallucination, overlooked critiques, and a lack of verifiable grounding. To address these limitations, we introduce $\textbf{RebuttalAgent}$, the first multi-agents framework that reframes rebuttal generation as an evidence-centric planning task. Our system decomposes complex feedback into atomic concerns and dynamically constructs hybrid contexts by synthesizing compressed summaries with high-fidelity text while integrating an autonomous and on-demand external search module to resolve concerns requiring outside literature. By generating an inspectable response plan before drafting, $\textbf{RebuttalAgent}$ ensures that every argument is explicitly anchored in internal or external evidence. We validate our approach on the proposed $\textbf{RebuttalBench}$ and demonstrate that our pipeline outperforms strong baselines in coverage, faithfulness, and strategic coherence, offering a transparent and controllable assistant for the peer review process. Code will be released.

</details>


### [101] [Toward Efficient Agents: Memory, Tool learning, and Planning](https://arxiv.org/abs/2601.14192)
*Xiaofang Yang,Lijun Li,Heng Zhou,Tong Zhu,Xiaoye Qu,Yuchen Fan,Qianshan Wei,Rui Ye,Li Kang,Yiran Qin,Zhiqiang Kou,Daizong Liu,Qi Li,Ning Ding,Siheng Chen,Jing Shao*

Main category: cs.AI

TL;DR: 本文探讨将大语言模型拓展至智能体系统时被忽视的效率问题，从记忆、工具学习和规划三方面研究，回顾现有方法，提出效率衡量方法，还考察面向效率的基准，讨论挑战与方向。


<details>
  <summary>Details</summary>
Motivation: 当前对将大语言模型拓展到智能体系统研究中，效率作为现实部署关键因素常被忽视，因此开展研究。

Method: 从记忆、工具学习和规划三个核心组件研究效率，回顾不同实现但有共同原则的方法；用在固定成本预算下比较效果和在相当效果水平下比较成本两种方式表征效率；梳理组件评估协议和常见效率指标考查面向效率的基准。

Result: 提出了表征效率的方法，如在固定成本和相当效果下的比较方式；对面向效率的基准进行了考查。

Conclusion: 讨论了关键挑战和未来方向，为相关研究提供了有前景的见解。

Abstract: Recent years have witnessed increasing interest in extending large language models into agentic systems. While the effectiveness of agents has continued to improve, efficiency, which is crucial for real-world deployment, has often been overlooked. This paper therefore investigates efficiency from three core components of agents: memory, tool learning, and planning, considering costs such as latency, tokens, steps, etc. Aimed at conducting comprehensive research addressing the efficiency of the agentic system itself, we review a broad range of recent approaches that differ in implementation yet frequently converge on shared high-level principles including but not limited to bounding context via compression and management, designing reinforcement learning rewards to minimize tool invocation, and employing controlled search mechanisms to enhance efficiency, which we discuss in detail. Accordingly, we characterize efficiency in two complementary ways: comparing effectiveness under a fixed cost budget, and comparing cost at a comparable level of effectiveness. This trade-off can also be viewed through the Pareto frontier between effectiveness and cost. From this perspective, we also examine efficiency oriented benchmarks by summarizing evaluation protocols for these components and consolidating commonly reported efficiency metrics from both benchmark and methodological studies. Moreover, we discuss the key challenges and future directions, with the goal of providing promising insights.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [102] [Traffic Collisions: Temporal Patterns and Severity-Weighted Hotspot Analysis](https://arxiv.org/abs/2601.12548)
*Nael Alsaleh,Noura Falis,Tareq Alsaleh,Farah Ba Fakih*

Main category: cs.CE

TL;DR: 研究分析迪拜交通碰撞时空模式与严重程度，发现总体和行人事故有不同时空特征，据此提出政策措施。


<details>
  <summary>Details</summary>
Motivation: 在快速发展的城市环境中，理解交通碰撞模式对有效道路安全规划至关重要，本研究聚焦迪拜交通碰撞时空模式及严重程度。

Method: 分析2024年11月至2025年6月交通碰撞记录，用卡方检验和Cramer's V评估时间与严重程度关联，用基于Getis - Ord Gi*统计的严重程度加权热点分析及反距离加权（IDW）插值分析空间模式。

Result: 总体碰撞频率和严重程度有明显时间变化，夜间高严重程度概率比下午高44%；行人事故在深夜高发，时空变化不同。空间上总体碰撞热点在迪拜北部、西北部和公路沿线，行人热点在西南部工业区附近。

Conclusion: 基于研究结果提出降低夜间限速、加强自动执法、改善道路照明和在热点实施行人专项处理等政策措施。

Abstract: Understanding traffic collision patterns is of high importance for effective road safety planning in fast-growing urban environments. This study examines the temporal and spatial patterns of traffic collisions in Dubai, UAE, with a particular focus on collision severity. To this end, traffic collision records from November 2024 to June 2025 were analyzed to examine hourly, daily, and monthly variations in collision frequency and severity for both overall traffic collisions and pedestrian-related accidents. Temporal associations with severity were evaluated using chi-square tests and Cramer's V, while spatial patterns were analyzed using severity-weighted hotspot analysis based on the Getis-Ord Gi* statistic, complemented by inverse distance weighting (IDW) interpolation. The results show a clear temporal variation in overall collision frequency and severity, with higher collision frequencies during evening and nighttime periods with 44% higher probability of high-severity outcomes at night compared to the afternoon. On the other hand, pedestrian-related accidents showed a distinct temporal profile, characterized by higher occurrence during late-evening hours and relatively limited variation across days of the week and months. Spatial analysis identified statistically significant severity hotspots for overall collisions in the northern and northwestern parts of Dubai and along the Al Ain-Dubai Highway, while pedestrian severity hotspots were concentrated near industrial areas in the southwestern region. Several policy measures are proposed based on the findings including, reducing nighttime speed limits, enhancing automated enforcement, improving roadway lighting, and implementing pedestrian-focused treatments in statistically significant hotspots.

</details>


### [103] [A Model Fusion Approach for Enhancing Credit Approval Decision Making](https://arxiv.org/abs/2601.12684)
*Yuanhong Wu,Jingyan Xu,Wei Ye,Christina Schweikert,D. Frank Hsu*

Main category: cs.CE

TL;DR: 提出组合融合分析（CFA）模型融合框架结合多机器学习算法检测和预测信用卡审批，结果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 信用违约给金融机构和消费者带来挑战和损失，信用违约风险管理是金融行业关键话题。

Method: 提出CFA模型融合框架，使用五个预训练模型进行设计和实现。

Result: CFA结果准确率达89.13%，优于传统机器学习和集成方法。

Conclusion: CFA模型融合框架在信用卡审批检测和预测上有较好效果。

Abstract: Credit default poses significant challenges to financial institutions and consumers, resulting in substantial financial losses and diminished trust. As such, credit default risk management has been a critical topic in the financial industry. In this paper, we present Combinatorial Fusion Analysis (CFA), a model fusion framework, that combines multiple machine learning algorithms to detect and predict credit card approval with high accuracy. We present the design methodology and implementation using five pre-trained models. The CFA results show an accuracy of 89.13% which is better than conventional machine learning and ensemble methods.

</details>


### [104] [Text2Structure3D: Graph-Based Generative Modeling of Equilibrium Structures with Diffusion Transformers](https://arxiv.org/abs/2601.12870)
*Lazlo Bleker,Zifeng Guo,Kaleb Smith,Kam-Ming Mark Tam,Karla Saldaña Ochoa,Pierluigi D'Acunto*

Main category: cs.CE

TL;DR: 本文提出Text2Structure3D模型，结合多种技术从自然语言生成平衡结构，经训练验证，成果符合文本规范且泛化能力强，是结构设计基础模型的早期尝试。


<details>
  <summary>Details</summary>
Motivation: 支持概念结构设计过程中直观的设计探索和迭代新方式。

Method: 结合潜在扩散、变分图自动编码器和图变换器生成接近平衡状态的结构图形，增加残余力优化后处理步骤确保结构满足静平衡，用跨类型数据集训练验证。

Result: 生成的平衡结构符合文本规范，泛化能力比基于参数模型的方法有很大提升。

Conclusion: Text2Structure3D是结构设计通用基础模型的早期步骤，能将生成式AI集成到概念设计工作流程中。

Abstract: This paper presents Text2Structure3D, a graph-based Machine Learning (ML) model that generates equilibrium structures from natural language prompts. Text2Structure3D is designed to support new intuitive ways of design exploration and iteration in the conceptual structural design process. The approach combines latent diffusion with a Variational Graph Auto-Encoder (VGAE) and graph transformers to generate structural graphs that are close to an equilibrium state. Text2Structure3D integrates a residual force optimization post-processing step that ensures generated structures fully satisfy static equilibrium. The model was trained and validated using a cross-typological dataset of funicular form-found and statically determinate bridge structures, paired with text descriptions that capture the formal and structural features of each bridge. Results demonstrate that Text2Structure3D generates equilibrium structures with strong adherence to text-based specifications and greatly improves generalization capabilities compared to parametric model-based approaches. Text2Structure3D represents an early step toward a general-purpose foundation model for structural design, enabling the integration of generative AI into conceptual design workflows.

</details>


### [105] [TransMode-LLM: Feature-Informed Natural Language Modeling with Domain-Enhanced Prompting for Travel Behavior Modeling](https://arxiv.org/abs/2601.13763)
*Meijing Zhang,Ying Xu*

Main category: cs.CE

TL;DR: 本文提出TransMode - LLM框架，结合统计方法与大语言模型技术预测出行模式，实验表明该方法有竞争力，少样本学习可提高准确率，领域增强提示效果因模型而异。


<details>
  <summary>Details</summary>
Motivation: 理解出行者行为并准确预测出行模式对交通规划和政策制定至关重要。

Method: 提出TransMode - LLM框架，分统计分析、自然语言编码、大语言模型适配三个阶段，用多学习范式预测，用不同样本量和模型进行评估。

Result: 大语言模型方法准确率有竞争力，少样本学习显著提高准确率，领域增强提示对通用模型有帮助，但对推理导向模型效果不一。

Conclusion: 该研究推动了大语言模型在出行行为建模中的应用，为学术研究和交通政策制定提供有价值见解。

Abstract: Understanding traveler behavior and accurately predicting travel mode choice are at the heart of transportation planning and policy-making. This study proposes TransMode-LLM, an innovative framework that integrates statistical methods with LLM-based techniques to predict travel modes from travel survey data. The framework operates through three phases: (1) statistical analysis identifies key behavioral features, (2) natural language encoding transforms structured data into contextual descriptions, and (3) LLM adaptation predicts travel mode through multiple learning paradigms including zero-shot and one/few-shot learning and domain-enhanced prompting. We evaluate TransMode-LLM using both general-purpose models (GPT-4o, GPT-4o-mini) and reasoning-focused models (o3-mini, o4-mini) with varying sample sizes on real-world travel survey data. Extensive experiment results demonstrate that the LLM-based approach achieves competitive accuracy compared to state-of-the-art baseline classifiers models. Moreover, few-shot learning significantly improves prediction accuracy, with models like o3-mini showing consistent improvements of up to 42.9\% with 5 provided examples. However, domain-enhanced prompting shows divergent effects across LLM architectures. In detail, it is helpful to improve performance for general-purpose models with GPT-4o achieving improvements of 2.27% to 12.50%. However, for reasoning-oriented models (o3-mini, o4-mini), domain knowledge enhancement does not universally improve performance. This study advances the application of LLMs in travel behavior modeling, providing promising and valuable insights for both academic research and transportation policy-making in the future.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [106] [Knowledge Graph Construction for Stock Markets with LLM-Based Explainable Reasoning](https://arxiv.org/abs/2601.11528)
*Cheonsol Lee,Youngsang Jeong,Jeongyeol Shin,Huiju Kim,Jidong Kim*

Main category: cs.DB

TL;DR: 本文提出股票市场知识图谱架构并与大语言模型集成，经韩国上市公司案例验证，展示其用于高级投资分析和决策支持的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统股票研究方法难以捕捉关系模式、竞争动态和提供可解释投资推理，存在局限性。

Method: 提出股票市场知识图谱架构，将其与大语言模型集成，进行多跳推理和关系查询。

Result: 通过韩国上市公司案例验证，可提取传统数据库查询难以或无法得到的洞察。

Conclusion: 结合知识图谱和大语言模型在高级投资分析和决策支持方面有潜力。

Abstract: The stock market is inherently complex, with interdependent relationships among companies, sectors, and financial indicators. Traditional research has largely focused on time-series forecasting and single-company analysis, relying on numerical data for stock price prediction. While such approaches can provide short-term insights, they are limited in capturing relational patterns, competitive dynamics, and explainable investment reasoning. To address these limitations, we propose a knowledge graph schema specifically designed for the stock market, modeling companies, sectors, stock indicators, financial statements, and inter-company relationships. By integrating this schema with large language models (LLMs), our approach enables multi-hop reasoning and relational queries, producing explainable and in-depth answers to complex financial questions. Figure1 illustrates the system pipeline, detailing the flow from data collection and graph construction to LLM-based query processing and answer generation. We validate the proposed framework through practical case studies on Korean listed companies, demonstrating its capability to extract insights that are difficult or impossible to obtain from traditional database queries alone. The results highlight the potential of combining knowledge graphs with LLMs for advanced investment analysis and decision support.

</details>


### [107] [From HNSW to Information-Theoretic Binarization: Rethinking the Architecture of Scalable Vector Search](https://arxiv.org/abs/2601.11557)
*Seyed Moein Abtahi,Majid Fekri,Tara Khani,Akramul Azim*

Main category: cs.DB

TL;DR: 本文分析主流语义搜索和RAG系统架构局限，提出基于MIB的信息论架构并评估，结果显示质量高、低延迟且吞吐量稳定，可支持无服务器按查询付费模式。


<details>
  <summary>Details</summary>
Motivation: 主流语义搜索和RAG系统依赖高精度浮点向量内存ANN索引，导致运营成本上升，且在延迟、吞吐量和检索准确性间存在权衡。

Method: 引入并实证评估基于最大信息二值化（MIB）、高效位运算距离度量和信息论评分（ITS）机制的信息论架构，用MAIR基准测试与其他系统对比。

Result: 该架构在检索质量上与全精度系统相当，同时实现更低延迟，在高请求率下保持恒定吞吐量。

Conclusion: 这种架构转变可实现真正无服务器、按查询付费的部署模型，挑战了高质量语义搜索依赖大内存ANN索引的必要性。

Abstract: Modern semantic search and retrieval-augmented generation (RAG) systems rely predominantly on in-memory approximate nearest neighbor (ANN) indexes over high-precision floating-point vectors, resulting in escalating operational cost and inherent trade-offs between latency, throughput, and retrieval accuracy. This paper analyzes the architectural limitations of the dominant "HNSW + float32 + cosine similarity" stack and evaluates existing cost-reduction strategies, including storage disaggregation and lossy vector quantization, which inevitably sacrifice either performance or accuracy. We introduce and empirically evaluate an alternative information-theoretic architecture based on maximally informative binarization (MIB), efficient bitwise distance metrics, and an information-theoretic scoring (ITS) mechanism. Unlike conventional ANN systems, this approach enables exhaustive search over compact binary representations, allowing deterministic retrieval and eliminating accuracy degradation under high query concurrency. Using the MAIR benchmark across 14 datasets and 10,038 queries, we compare this architecture against Elasticsearch, Pinecone, PGVector, and Qdrant. Results demonstrate retrieval quality comparable to full-precision systems, while achieving substantially lower latency and maintaining constant throughput at high request rates. We show that this architectural shift enables a truly serverless, cost-per-query deployment model, challenging the necessity of large in-memory ANN indexes for high-quality semantic search.

</details>


### [108] [RelServe: Fast LLM Inference Serving on Relational Data](https://arxiv.org/abs/2601.11546)
*Xin Zhang,Shihong Gao,Yanyan Shen,Haoyang Li,Lei Chen*

Main category: cs.DB

TL;DR: 提出RelServe优化LLM引擎以降低relQuery服务延迟，实验显示比vLLM平均服务延迟最多降低3.1倍。


<details>
  <summary>Details</summary>
Motivation: relQuery服务在并发查询负载下需快速响应，但当前LLM引擎在三个推理阶段存在HoL阻塞导致延迟瓶颈，现有静态优先级调度方法无法完全解决问题。

Method: 提出RelServe引擎，包含动态优先级更新器和自适应批处理安排器。动态优先级更新器通过统计近似持续调整优先级并减少开销，自适应批处理安排器定量评估候选批处理以最小化预估平均延迟。

Result: 在四个真实世界数据集上对参数从13B到70B的LLM进行实验，RelServe比vLLM平均服务延迟最多降低3.1倍。

Conclusion: RelServe可有效降低relQuery服务的平均延迟，能应对LLM引擎的HoL阻塞问题。

Abstract: The use of Large Language Models (LLMs) for querying relational data has given rise to relQuery, a workload pattern that applies templated LLM calls to structured tables. As relQuery services become more widely adopted in applications such as AI-powered spreadsheets, fast response times under concurrent query loads are increasingly important. Unfortunately, current LLM engines face severe latency bottlenecks from Head-of-Line (HoL) blocking across three comparable inference phases: waiting, core running, and tail running. Existing static priority scheduling methods only address HoL blocking during the waiting phase, leaving two critical problems unsolved. First, the absence of a priority update mechanism causes inaccurate prioritization and continued HoL blocking during core execution. Second, suboptimal prefill-decode batching exacerbates HoL blocking in tail execution and worsens latency trade-offs between running and waiting relQueries. To address these problems, we propose RelServe, an optimized LLM engine for low-latency relQuery serving. RelServe features two core innovations: a Dynamic Priority Updater that continuously adjusts priorities while minimizing overhead via statistical approximations, and an Adaptive Batch Arranger that quantitatively evaluates candidate prefill and decode batches to minimize projected average latency. Extensive experiments on four real-world datasets using LLMs ranging from 13B to 70B parameters show that RelServe reduces average serving latency by up to 3.1x compared to vLLM.

</details>


### [109] [Uniqueness ratio as a predictor of a privacy leakage](https://arxiv.org/abs/2601.11550)
*Danah A. AlSalem AlKhashti*

Main category: cs.DB

TL;DR: 研究将候选连接属性的唯一性比率作为重新识别风险的早期预测指标，实验表明高预连接唯一性与连接后泄漏增加有关，该比率可用于评估连接导致的隐私风险。


<details>
  <summary>Details</summary>
Motivation: 以往研究多关注连接后检测或复杂隐私模型，缺乏简单、可解释的预连接指标，本研究旨在填补这一空白。

Method: 使用合成多表数据集，计算每个数据库中属性组合的唯一性比率，并研究这些比率与连接后身份暴露的相关性。

Result: 实验结果显示，预连接的高唯一性与连接后泄漏增加有很强的关系，通过可唯一识别或属于极小群体的记录比例来衡量。

Conclusion: 唯一性比率可作为评估连接导致的隐私风险的可解释且实用的信号，为开发更全面的预连接风险评估模型奠定基础。

Abstract: Identity leakage can emerge when independent databases are joined, even when each dataset is anonymized individually. While previous work focuses on post-join detection or complex privacy models, little attention has been given to simple, interpretable pre-join indicators that can warn data engineers and database administrators before integration occurs. This study investigates the uniqueness ratio of candidate join attributes as an early predictor of re-identification risk. Using synthetic multi-table datasets, we compute the uniqueness ratio of attribute combinations within each database and examine how these ratios correlate with identity exposure after the join. Experimental results show a strong relationship between high pre-join uniqueness and increased post-join leakage, measured by the proportion of records that become uniquely identifiable or fall into very small groups. Our findings demonstrate that uniqueness ratio offers an explainable and practical signal for assessing join induced privacy risk, providing a foundation for developing more comprehensive pre-join risk estimation models.

</details>


### [110] [Bridging Radiology and Pathology: A DICOM-Based Framework for Multimodal Mapping and Integrated Visualization](https://arxiv.org/abs/2601.11558)
*Nilesh P. Rijhwani,Titus J. Brinker,Peter Neher,Marco Nolden,Klaus Maier-Hein,Maximilian Fischer,Christoph Wies*

Main category: cs.DB

TL;DR: 该项目引入跨学科工具包，用于弥合放射学和病理学差距，实现多模态高效分析，促进研究和患者护理。


<details>
  <summary>Details</summary>
Motivation: 医疗各专科数据系统和格式不一、查看器独立，阻碍了互补诊断信息的联合分析和跨专业协作，多模态集成依赖手动配对。

Method: 引入可在Kaapana框架内或独立运行的跨学科工具包，连接特定模态查看器，增加自动图像配准和对齐功能。

Result: 形成集成环境，实现高效、可扩展的多模态分析。

Conclusion: 该集成环境有利于促进可重复的工作流程，加速跨学科研究，助力深入了解疾病机制和改善患者护理。

Abstract: Accurate disease diagnosis depends on effective collaboration between medical specialties, yet departments often use distinct data systems and proprietary formats. This heterogeneity hinders joint analysis and integration of complementary diagnostic information. The use of separate viewers for each modality further restricts cross-specialty collaboration. Although multimodal integration, particularly between radiology and pathology, has demonstrated potential for identifying novel biomarkers, it still relies heavily on manual, time-consuming data pairing. This project introduces an interdisciplinary toolbox that can operate within the Kaapana framework or as a standalone tool to bridge radiology and pathology. By linking modalityspecific viewers and extending them with automated image registration and alignment, the platform enables efficient, scalable multimodal analysis. The integrated environment promotes reproducible workflows, accelerates crossdisciplinary research, and facilitates deeper insights into disease mechanisms and patient care.

</details>


### [111] [GPU-Resident Inverted File Index for Streaming Vector Databases](https://arxiv.org/abs/2601.11808)
*Dongfang Zhao*

Main category: cs.DB

TL;DR: 提出SIVF架构，提升向量数据库数据摄入和删除能力，性能提升显著且存储开销小


<details>
  <summary>Details</summary>
Motivation: 传统GPU - 加速IVF索引架构静态，缺乏原地突变支持，在实时知识更新场景造成系统延迟高

Method: 用基于slab的分配系统和有效性位图替代静态内存布局，引入GPU驻留地址转换表

Result: 在SIFT1M和GIST1M数据集上，SIVF减少删除延迟、提高摄入吞吐量，在滑动窗口场景消除系统冻结并加速

Conclusion: SIVF能为向量数据库带来高速数据摄入和删除能力，性能提升大且存储开销可忽略不计

Abstract: Vector search has emerged as the computational backbone of modern AI infrastructure, powering critical systems ranging from Vector Databases to Retrieval-Augmented Generation (RAG). While the GPU-accelerated Inverted File (IVF) index acts as one of the most widely used techniques for these large-scale workloads due to its memory efficiency, its traditional architecture remains fundamentally static. Existing designs rely on rigid and contiguous memory layouts that lack native support for in-place mutation, creating a severe bottleneck for streaming scenarios. In applications requiring real-time knowledge updates, such as live recommendation engines or dynamic RAG systems, maintaining index freshness necessitates expensive CPU-GPU roundtrips that cause system latency to spike from milliseconds to seconds. In this paper, we propose SIVF (Streaming Inverted File), a new GPU-native architecture designed to empower vector databases with high-velocity data ingestion and deletion capabilities. SIVF replaces the static memory layout with a slab-based allocation system and a validity bitmap, enabling lock-free and in-place mutation directly in VRAM. We further introduce a GPU-resident address translation table (ATT) to resolve the overhead of locating vectors, providing $O(1)$ access to physical storage slots. We evaluate SIVF against the industry-standard GPU IVF implementation on the SIFT1M and GIST1M datasets. Microbenchmarks demonstrate that SIVF reduces deletion latency by up to $13,300\times$ (from 11.8 seconds to 0.89 ms on GIST1M) and improves ingestion throughput by $36\times$ to $105\times$. In end-to-end sliding window scenarios, SIVF eliminates system freezes and achieves a $161\times$ to $266\times$ speedup with single-digit millisecond latency. Notably, this performance incurs negligible storage penalty, maintaining less than 0.8\% memory overhead compared to static indices.

</details>


### [112] [Is Quantum Computing Ready for Real-Time Database Optimization?](https://arxiv.org/abs/2601.12123)
*Hanwen Liu,Ibrahim Sabek*

Main category: cs.DB

TL;DR: 量子计算可用于数据库优化，此前研究因高开销未实现集成，新的低延迟方案出现带来新挑战，本文提出首个量子增强查询优化器Q2O能实时处理查询。


<details>
  <summary>Details</summary>
Motivation: 随着数据量增长和工作负载复杂化，传统方法难高效解决数据库优化问题，量子计算有潜力，但此前因高开销未实现集成，新低延迟方案带来平衡效率和质量的新挑战。

Method: 提出Q2O，将连接顺序问题编码为非线性模型，用实际数据库统计，通过NL-Solver求解并转化为计划提示引导PostgreSQL优化器生成完整计划。

Result: Q2O能够实时处理实际查询。

Conclusion: 可以在数据库系统中实现量子解决方案的效率和质量的平衡。

Abstract: Database systems encompass several performance-critical optimization tasks, such as join ordering and index tuning. As data volumes grow and workloads become more complex, these problems have become exponentially harder to solve efficiently. Quantum computing, especially quantum annealing, is a promising paradigm that can efficiently explore very large search spaces through quantum tunneling. It can escape local optima by tunneling through energy barriers rather than climbing over them. Earlier works mainly focused on providing an abstract representation (e.g., Quadratic Unconstrained Binary Optimization (QUBO)) for the database optimization problems (e.g., join order) and overlooked the real integration within database systems due to the high overhead of quantum computing services (e.g., a minimum 5s runtime for D-Wave's CQM-Solver). Recently, quantum annealing providers have offered more low-latency solutions, e.g., NL-Solver, which paves the road to actually realizing quantum solutions within DBMSs. However, this raises new systems research challenges in balancing efficiency and solution quality.
  In this talk, we show that this balance is possible to achieve. As a proof of concept, we present Q2O, the first real Quantum-augmented Query Optimizer. We show the end-to-end workflow: we encode the join order problem as a nonlinear model, a format solvable by the NL-Solver, using actual database statistics; the solution is translated into a plan hint that guides PostgreSQL's optimizer to produce a complete plan. Q2O is capable of handling actual queries in real time.

</details>


### [113] [RLMiner: Finding the Most Frequent k-sized Subgraph via Reinforcement Learning](https://arxiv.org/abs/2601.12416)
*Wei Huang,Hanchen Wang,Dong Wen,Xin Cao,Ying Zhang,Wenjie Zhang*

Main category: cs.DB

TL;DR: 本文将寻找目标图中大小为k的最频繁诱导子图问题转化为马尔可夫决策过程，用多任务强化学习框架解决，提出RLMiner框架，实验显示其效果好、运行时间短且稳定。


<details>
  <summary>Details</summary>
Motivation: 寻找最频繁诱导子图因子图计数的NP难问题导致计算成本高，传统精确枚举算法时间复杂度高，现有方法有额外约束。

Method: 将任务形式化为马尔可夫决策过程，采用多任务强化学习框架，提出RLMiner框架，集成强化学习和任务状态感知图神经网络。

Result: 在真实数据集上的实验表明，RLMiner找出的子图频率与真实最频繁诱导子图接近，且运行时间比传统方法更短、更稳定。

Conclusion: 提出的RLMiner框架能有效解决寻找最频繁诱导子图的问题，并在时间性能上优于传统方法。

Abstract: Identifying the most frequent induced subgraph of size $k$ in a target graph is a fundamental graph mining problem with direct implications for Web-related data mining and social network analysis. Despite its importance, finding the most frequent induced subgraph remains computationally expensive due to the NP-hard nature of the subgraph counting task. Traditional exact enumeration algorithms often suffer from high time complexity, especially for a large graph size $k$. To mitigate this, existing approaches often utilize frequency measurement with the Downward Closure Property to reduce the search space, imposing additional constraints on the task. In this paper, we first formulate this task as a Markov Decision Process and approach it using a multi-task reinforcement learning framework. Specifically, we introduce RLMiner, a novel framework that integrates reinforcement learning with our proposed task-state-aware Graph Neural Network to find the most frequent induced subgraph of size $k$ with a time complexity linear to $k$. Extensive experiments on real-world datasets demonstrate that our proposed RLMiner effectively identifies subgraphs with frequencies closely matching the ground-truth most frequent induced subgraphs, while achieving significantly shorter and more stable running times compared to traditional methods.

</details>


### [114] [Bringing Data Transformations Near-Memory for Low-Latency Analytics in HTAP Environments](https://arxiv.org/abs/2601.12456)
*Arthur Bernhardt,David Volz,Sajjad Tamimi,Andreas Koch,Ilia Petrov*

Main category: cs.DB

TL;DR: 提出在智能存储系统上近存储或存储内执行数据转换的方法，结果显示前台工作负载性能好、资源争用低。


<details>
  <summary>Details</summary>
Motivation: 当前提取数据再转换的主流方法在转换时性能下降且数据移动量大。

Method: 提出在智能存储系统上近存储或存储内执行数据转换的方法。

Result: 前台工作负载有稳健性能，资源争用更低。

Conclusion: 该方法在多引擎和多系统设置及复用方面有架构机会。

Abstract: In this paper we propose an approach for executing data transformations near- or in-storage on intelligent storage systems. The currently prevailing approach of extracting the data and then transforming it to a target format suffers degraded performance during transformation and causes heavy data movement. Our results show robust performance of foreground workloads and lower resource contention. Our vision draws architectural opportunities in multi-engine and multi-system settings, as well as for reuse.

</details>


### [115] [xBound: Join Size Lower Bounds](https://arxiv.org/abs/2601.13117)
*Mihail Stoian,Tiemo Bang,Hangdong Zhao,Jesús Camacho-Rodríguez,Yuanyuan Tian,Andreas Kipf*

Main category: cs.DB

TL;DR: 云数据库查询优化器的基数估计存在低估问题，之前方法无法解决，本文提出 xBound 框架推导可证明的连接大小下界，减少了实际系统中的低估情况。


<details>
  <summary>Details</summary>
Motivation: 现有基数估计存在严重低估问题，且之前方法无法解决低估问题，需新方法解决。

Method: 引入 xBound 框架来推导可证明的连接大小下界。

Result: 在 JOBlight 基准测试和 Microsoft 企业工作负载中，xBound 成功减少了 DuckDB、PostgreSQL 和 Fabric Data Warehouse 中的低估情况。

Conclusion: xBound 框架是解决长期存在的基数低估问题的重要一步。

Abstract: Cloud database vendors invest substantial resources into their query optimizers, and for good reason. Cardinality estimation, a cornerstone of the optimizer, is critical for the selection of efficient query plans, as well as downstream tasks such as resource allocation and query scheduling. Yet, as many practitioners and researchers have noted, it is also the optimizer's Achilles heel. Prior studies on a number of industrial-strength databases show substantial cardinality estimation errors on all tested systems, with a far greater tendency to underestimate than to overestimate. Unfortunately, cardinality underestimation is more problematic than overestimation, as it misleads the optimizer to choose plans designed for small data, leading to underprovisioned CPU and memory.
  While previous work on pessimistic cardinality estimation has proposed provable join size upper bounds, such methods can only correct overestimation, leaving the more harmful problem of underestimation unaddressed. To fill this critical gap, we introduce xBound, the very first framework for deriving provable join size lower bounds. xBound successfully reduces underestimation in real systems: On the JOBlight benchmark, it corrects 17.5% of subexpression underestimates in DuckDB and 8.7% in PostgreSQL, while on a Microsoft enterprise workload, it fixes 36.1% of Fabric Data Warehouse's underestimates, demonstrating a significant step towards solving this long-standing problem.

</details>


### [116] [A Distributed Spatial Data Warehouse for AIS Data (DIPAAL)](https://arxiv.org/abs/2601.13795)
*Alex S. Klitgaard,Lau E. Josefsen,Mikael V. Mikkelsen,Kristian Torp*

Main category: cs.DB

TL;DR: 本文提出处理AIS数据的ETL系统和分布式空间数据仓库，采用光栅方法查询数据，设计分区数据仓库，发现单元格表示搜索更快，分区能实现良好扩展。


<details>
  <summary>Details</summary>
Motivation: AIS数据需清洗、处理和存储才能使用，且要高效分析大量船舶数据。

Method: 提出高效模块化ETL流程加载AIS数据，采用光栅方法查询数据，设计具有粒度化单元格表示和热力图展示的空间分区数据仓库。

Result: 数据仓库存储大量船舶轨迹，单元格表示搜索比轨迹表示快，空间分区碎片在大区域分析中有354% - 1164%的良好扩展。

Conclusion: 所设计的数据仓库和查询方法在AIS数据处理和分析方面有较好效果，分区能提升分析扩展性。

Abstract: AIS data from ships is excellent for analyzing single-ship movements and monitoring all ships within a specific area. However, the AIS data needs to be cleaned, processed, and stored before being usable. This paper presents a system consisting of an efficient and modular ETL process for loading AIS data, as well as a distributed spatial data warehouse storing the trajectories of ships. To efficiently analyze a large set of ships, a raster approach to querying the AIS data is proposed. A spatially partitioned data warehouse with a granularized cell representation and heatmap presentation is designed, developed, and evaluated. Currently the data warehouse stores ~312 million kilometers of ship trajectories and more than +8 billion rows in the largest table. It is found that searching the cell representation is faster than searching the trajectory representation. Further, we show that the spatially divided shards enable a consistently good scale-up for both cell and heatmap analytics in large areas, ranging between 354% to 1164% with a 5x increase in workers

</details>


### [117] [TLSQL: Table Learning Structured Query Language](https://arxiv.org/abs/2601.14109)
*Feiyang Chen,Ken Zhong,Aoqian Zhang,Zheng Wang,Li Pan,Jianhua Li*

Main category: cs.DB

TL;DR: 提出TLSQL系统，可通过类SQL声明规范直接在关系数据库上进行表学习，实验显示其有效降低了将机器学习集成到以数据库为中心工作流的门槛。


<details>
  <summary>Details</summary>
Motivation: 现有表学习框架通常需要显式数据导出和大量特征工程，给数据库从业者造成高门槛。

Method: 将TLSQL实现为轻量级Python库，把类SQL声明规范转换为标准SQL查询和结构化学习任务描述，SQL查询由数据库引擎本机执行，任务描述由下游表学习框架处理。

Result: 在真实数据集上的实验表明TLSQL有效降低了将机器学习集成到以数据库为中心工作流的门槛。

Conclusion: TLSQL能让用户专注于建模和分析，而非底层数据准备和管道编排，有效降低了相关门槛。

Abstract: Table learning, which lies at the intersection of machine learning and modern database systems, has recently attracted growing attention. However, existing frameworks typically require explicit data export and extensive feature engineering, creating a high barrier for database practitioners. We present TLSQL (Table Learning Structured Query Language), a system that enables table learning directly over relational databases via SQL-like declarative specifications. TLSQL is implemented as a lightweight Python library that translates these specifications into standard SQL queries and structured learning task descriptions. The generated SQL queries are executed natively by the database engine, while the task descriptions are consumed by downstream table learning frameworks. This design allows users to focus on modeling and analysis rather than low-level data preparation and pipeline orchestration. Experiments on real-world datasets demonstrate that TLSQL effectively lowers the barrier to integrating machine learning into databasecentric workflows. Our code is available at https://github.com/rllmproject/tlsql/.

</details>


### [118] [ReSearch: A Multi-Stage Machine Learning Framework for Earth Science Data Discovery](https://arxiv.org/abs/2601.14176)
*Youran Sun,Yixin Wen,Haizhao Yang*

Main category: cs.DB

TL;DR: 现有地球科学数据发现系统有局限，本文介绍ReSearch搜索框架，经实验其在召回率和排序性能上优于基线方法，强调多阶段搜索重要性。


<details>
  <summary>Details</summary>
Motivation: 地球科学数据快速增长，现有数据发现系统难以弥合科学意图与异构元数据差距。

Method: 引入多阶段、增强推理的搜索框架ReSearch，集成多种技术，构建基于文献的基准。

Result: ReSearch在召回率和排序性能上持续优于基线方法，尤其对表达抽象科学目标的查询效果更好。

Conclusion: 具有意图感知的多阶段搜索是可重复和可扩展地球科学研究的基础能力。

Abstract: The rapid expansion of Earth Science data from satellite observations, reanalysis products, and numerical simulations has created a critical bottleneck in scientific discovery, namely identifying relevant datasets for a given research objective.
  Existing discovery systems are primarily retrieval-centric and struggle to bridge the gap between high-level scientific intent and heterogeneous metadata at scale.
  We introduce \textbf{ReSearch}, a multi-stage, reasoning-enhanced search framework that formulates Earth Science data discovery as an iterative process of intent interpretation, high-recall retrieval, and context-aware ranking.
  ReSearch integrates lexical search, semantic embeddings, abbreviation expansion, and large language model reranking within a unified architecture that explicitly separates recall and precision objectives.
  To enable realistic evaluation, we construct a literature-grounded benchmark by aligning natural language intent with datasets cited in peer-reviewed Earth Science studies.
  Experiments demonstrate that ReSearch consistently improves recall and ranking performance over baseline methods, particularly for task-based queries expressing abstract scientific goals.
  These results underscore the importance of intent-aware, multi-stage search as a foundational capability for reproducible and scalable Earth Science research.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [119] [PerCache: Predictive Hierarchical Cache for RAG Applications on Mobile Devices](https://arxiv.org/abs/2601.11553)
*Kaiwei Liu,Liekang Zeng,Lilin Xu,Bufang Yang,Zhenyu Yan*

Main category: cs.DC

TL;DR: 提出PerCache减少移动平台RAG应用端到端延迟，评估显示有显著效果。


<details>
  <summary>Details</summary>
Motivation: 移动RAG系统因长提示和资源限制响应延迟高，现有方法不适合移动场景。

Method: 提出PerCache，采用分层架构匹配查询和QKV缓存，用预测方法填充缓存，可适应动态系统负载。

Result: 在现有移动LLM推理引擎上实现PerCache，能比最佳基线减少34.4%延迟，在动态资源变化下保持最佳延迟性能。

Conclusion: PerCache能有效减少移动平台个性化RAG应用的端到端延迟。

Abstract: Retrieval-augmented generation (RAG) has been extensively used as a de facto paradigm in various large language model (LLM)-driven applications on mobile devices, such as mobile assistants leveraging personal emails or meeting records. However, due to the lengthy prompts and the resource constraints, mobile RAG systems exhibit significantly high response latency. On this issue, one promising approach is to reuse intermediate computational results across different queries to eliminate redundant computation. But most existing approaches, such as KV cache reuse and semantic cache reuse, are designed for cloud settings and perform poorly, overlooking the distinctive characteristics of mobile RAG.
  We propose PerCache, a novel hierarchical cache solution designed for reducing end-to-end latency of personalized RAG applications on mobile platforms. PerCache adopts a hierarchical architecture that progressively matches similar queries and QKV cache to maximize the reuse of intermediate results at different computing stages. To improve cache hit rate, PerCache applies a predictive method to populate cache with queries that are likely to be raised in the future. In addition, PerCache can adapt its configurations to dynamic system loads, aiming at maximizing the caching utility with minimal resource consumption. We implement PerCache on top of an existing mobile LLM inference engine with commodity mobile phones. Extensive evaluations show that PerCache can surpass the best-performing baseline by 34.4% latency reduction across various applications and maintain optimal latency performance under dynamic resource changes.

</details>


### [120] [Computation-Bandwidth-Memory Trade-offs: A Unified Paradigm for AI Infrastructure](https://arxiv.org/abs/2601.11577)
*Yuankai Fan,Qizhen Weng,Xuelong Li*

Main category: cs.DC

TL;DR: 大规模人工智能模型发展遇硬件瓶颈，提出AI Trinity范式平衡计算、带宽和内存以优化系统性能，并通过实例验证


<details>
  <summary>Details</summary>
Motivation: 解决大规模人工智能模型持续扩展中硬件计算、带宽、内存方面的瓶颈，以及各维度相互制约影响系统效率的问题

Method: 引入AI Trinity范式，动态分配计算、带宽和内存资源，识别三种基本权衡策略

Result: 通过边缘云通信、大规模分布式训练和模型推理等代表性系统设计，证明了AI Trinity范式的有效性

Conclusion: AI Trinity范式推动可扩展人工智能基础设施的新范式，为广泛应用场景提供概念基础和实践指导

Abstract: Large-scale artificial intelligence models are transforming industries and redefining human machine collaboration. However, continued scaling exposes critical limitations in hardware, including constraints on computation, bandwidth, and memory. These dimensions are tightly interconnected, so improvements in one often create bottlenecks in others, making isolated optimizations less effective. Balancing them to maximize system efficiency remains a central challenge in scalable AI design. To address this challenge, we introduce {Computation-Bandwidth-Memory Trade-offs}, termed the {AI Trinity}, a unified paradigm that positions {computation}, {bandwidth}, and {memory} as coequal pillars for next-generation AI infrastructure. AI Trinity enables dynamic allocation of resources across these pillars, alleviating single-resource bottlenecks and adapting to diverse scenarios to optimize system performance. Within this framework, AI Trinity identifies three fundamental trade-offs: (1) {More Computation$\rightarrow$Less Bandwidth}, wherein computational resources are exploited to reduce data transmission under limited bandwidth conditions, (2) {More Bandwidth$\rightarrow$Less Memory}, which exploits abundant communication capacity to populate or refresh memory when local storage resources are constrained, and (3) {More Memory$\rightarrow$Less Computation}, whereby storage capacity are utilized to mitigate redundant computation when computational costs are prohibitive. We illustrate the effectiveness of AI Trinity through representative system designs spanning edge-cloud communication, large-scale distributed training, and model inference. The innovations embodied in AI Trinity advance a new paradigm for scalable AI infrastructure, providing both a conceptual foundation and practical guidance for a broad range of application scenarios.

</details>


### [121] [Cost-Aware Logging: Measuring the Financial Impact of Excessive Log Retention in Small-Scale Cloud Deployments](https://arxiv.org/abs/2601.11584)
*Jody Almaida Putra*

Main category: cs.DC

TL;DR: 研究分析日志保留窗口选择对成本和运营的影响，发现减至14天可大幅降低成本且保留有用日志，提供配置框架。


<details>
  <summary>Details</summary>
Motivation: 小型云部署中日志保留策略常超需求，造成隐性成本，需从成本视角研究其影响。

Method: 使用合成日志数据集评估7、14、30和90天保留窗口，关注存储成本、有用日志比例和有用日志成本三个指标。

Result: 从90天减至14天可降低78%存储成本，保留超97%有用日志，长保留窗口效益递减。

Conclusion: 适度变更配置可节省成本且不影响系统可靠性，提供框架帮助小团队优化日志保留策略。

Abstract: Log data plays a critical role in observability, debugging, and performance monitoring in modern cloud-native systems. In small and early-stage cloud deployments, however, log retention policies are frequently configured far beyond operational requirements, often defaulting to 90 days or more, without explicit consideration of their financial and performance implications. As a result, excessive log retention becomes a hidden and recurring cost.
  This study examines the financial and operational impact of log retention window selection from a cost-aware perspective. Using synthetic log datasets designed to reflect real-world variability in log volume and access patterns, we evaluate retention windows of 7, 14, 30, and 90 days. The analysis focuses on three metrics: storage cost, operationally useful log ratio, and cost per useful log. Operational usefulness is defined as log data accessed during simulated debugging and incident analysis tasks.
  The results show that reducing log retention from 90 days to 14 days can lower log storage costs by up to 78 percent while preserving more than 97 percent of operationally useful logs. Longer retention windows provide diminishing operational returns while disproportionately increasing storage cost and query overhead. These findings suggest that modest configuration changes can yield significant cost savings without compromising system reliability.
  Rather than proposing new logging mechanisms, this work offers a lightweight and accessible framework to help small engineering teams reason about log retention policies through a cost-effectiveness lens. The study aims to encourage more deliberate observability configurations, particularly in resource-constrained cloud environments.

</details>


### [122] [Opportunistic Scheduling for Optimal Spot Instance Savings in the Cloud](https://arxiv.org/abs/2601.12266)
*Neelkamal Bhuyan,Randeep Bhatia,Murali Kodialam,TV Lakshman*

Main category: cs.DC

TL;DR: 研究在现货和按需云实例上调度延迟敏感作业问题，用排队论等工具分析，得出不同目标延迟下策略并提出自适应算法，实验验证其接近最优。


<details>
  <summary>Details</summary>
Motivation: 解决在满足平均延迟约束下，调度延迟敏感作业以最小化平均成本的问题。

Method: 运用排队论、随机过程和优化工具进行分析，推导通用策略成本表达式，证明低目标延迟下队列长度一最优，确定高目标延迟下背包结构并设计调度策略，提出自适应算法。

Result: 得到不同目标延迟下的最优策略和等待时间分布，自适应算法经实验验证接近最优。

Conclusion: 所提方法和策略能有效解决调度延迟敏感作业问题，自适应算法可充分利用允许延迟。

Abstract: We study the problem of scheduling delay-sensitive jobs over spot and on-demand cloud instances to minimize average cost while meeting an average delay constraint. Jobs arrive as a general stochastic process, and incur different costs based on the instance type. This work provides the first analytical treatment of this problem using tools from queuing theory, stochastic processes, and optimization. We derive cost expressions for general policies, prove queue length one is optimal for low target delays, and characterize the optimal wait-time distribution. For high target delays, we identify a knapsack structure and design a scheduling policy that exploits it. An adaptive algorithm is proposed to fully utilize the allowed delay, and empirical results confirm its near-optimality.

</details>


### [123] [PLA-Serve: A Prefill-Length-Aware LLM Serving System](https://arxiv.org/abs/2601.11589)
*Jianshu She,Zonghang Li,Hongchao Du,Shangyu Wu,Wenhao Zheng,Eric Xing,Zhengzhong Liu,Huaxiu Yao,Jason Xue,Qirong Ho*

Main category: cs.DC

TL;DR: PLA - Serve通过识别和拆分不同提示长度的请求来降低大语言模型服务中的TTFT延迟，提升了服务性能。


<details>
  <summary>Details</summary>
Motivation: 现有系统依赖统一调度策略，无法适应异构工作负载特性，提示长度变化导致不同性能瓶颈，因此需要自适应调度策略。

Method: 将多轮长提示请求与短提示请求拆分，为短提示工作负载引入长度感知智能批处理机制，采用双队列设计支持时间或空间拆分，利用批处理等待窗口和基于CUDA Graph的聚类减少异构计算干扰。

Result: 在多轮工作负载中，较vanilla SGLang降低预填充延迟超30%，多实例部署中降低SLO违规28%，多GPU设置中较SGLang路由器降低SLO违规12%，高并发和混合请求场景下提升Qwen2.5 - 32B模型预填充实例请求吞吐量35%。

Conclusion: PLA - Serve在优化异构大语言模型服务工作负载方面有效。

Abstract: PLA-Serve identifies and disaggregates requests with different prompt lengths in LLM serving to reduce TTFT latency. While recent systems have decoupled the prefill and decode stages to improve throughput, they still rely on unified scheduling policies that fail to adapt to heterogeneous workload characteristics. We observe that prompt-length variations lead to distinct performance bottlenecks, motivating an adaptive scheduling strategy. PLA-Serve disaggregates multi-turn long-prefill requests from short-prefill ones and introduces a length-aware smart batching mechanism for short-prefill workloads. It adopts a dual-queue design that supports temporal disaggregation on a single prefill instance or spatial disaggregation across multiple instances. For short-prefill batches, a batch waiting window and CUDA Graph-based clustering mitigate interference from heterogeneous computation, reducing batching delay and lowering average latency. In real multi-turn workloads, PLA-Serve reduces prefill latency by over 30% compared to vanilla SGLang under prefill**--**decode disaggregation, and further decreases SLO violations by 28% in multi-instance deployments with vanilla data-parallel configuration. Compared to the SGLang router with load balancing, it further lowers SLO violations by 12% in multi-GPU settings. Under high concurrency and mixed-request scenarios, PLA-Serve improves request throughput by 35% serving Qwen2.5-32B model for prefill instance, demonstrating its effectiveness in optimizing heterogeneous LLM serving workloads.

</details>


### [124] [EPD-Serve: A Flexible Multimodal EPD Disaggregation Inference Serving System On Ascend](https://arxiv.org/abs/2601.11590)
*Fan Bai,Pai Peng,Zhengzhi Tang,Zhe Wang,Gong Chen,Xiang Lu,Yinuo Li,Huan Lin,Weizhe Lin,Yaoyuan Wang,Xiaosong Li*

Main category: cs.DC

TL;DR: 现有多模态推理系统架构存在资源利用低效等问题，提出EPD - Serve系统，实验显示其能提升端到端吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推理系统采用整体架构，忽略各阶段异构计算特性，导致资源利用低效和系统吞吐量受限。

Method: 将推理管道解耦为独立的Encode、Prefill和Decode阶段，利用Ascend互连拓扑引入异步特征预取和分层分组KV缓存传输机制，还采用多路由调度、实例级负载均衡和多级硬件共定位与空间复用。

Result: 在多模态理解模型的高并发场景实验中，EPD - Serve较PD - 解聚合部署端到端吞吐量提升57.37 - 69.48%，满足严格SLO约束。

Conclusion: 阶段级解聚合对优化多模态大模型推理系统有效。

Abstract: With the widespread adoption of large multimodal models, efficient inference across text, image, audio, and video modalities has become critical. However, existing multimodal inference systems typically employ monolithic architectures that tightly couple the Encode, Prefill, and Decode stages on homogeneous hardware, neglecting the heterogeneous computational characteristics of each stage. This design leads to inefficient resource utilization and limited system throughput. To address these issues, we propose EPD-Serve, a stage-level disaggregated inference serving system for multimodal models. EPD-Serve decouples the inference pipeline into independent Encode, Prefill, and Decode stages, enabling logical isolation and flexible co-located deployment through dynamic orchestration. Leveraging the Ascend interconnect topology, EPD-Serve introduces asynchronous feature prefetching between Encode and Prefill stages and a hierarchical grouped KV cache transmission mechanism between Prefill and Decode stages to improve cross-node communication efficiency. In addition, EPD-Serve incorporates multi-route scheduling, instance-level load balancing, and multi-stage hardware co-location with spatial multiplexing to better support diverse multimodal workloads. Comprehensive experiments on multimodal understanding models demonstrate that, under high-concurrency scenarios, EPD-Serve improves end-to-end throughput by 57.37-69.48% compared to PD-disaggregated deployment, while satisfying strict SLO constraints, including TTFT below 2000 ms and TPOT below 50 ms. These results highlight the effectiveness of stage-level disaggregation for optimizing multimodal large model inference systems.

</details>


### [125] [Enhancing Model Context Protocol (MCP) with Context-Aware Server Collaboration](https://arxiv.org/abs/2601.11595)
*Meenakshi Amulya Jayanti,X. Y. Han*

Main category: cs.DC

TL;DR: 本文设计了Context - Aware MCP（CA - MCP），通过实验证明其在LLM驱动的多智能体系统中优于传统MCP。


<details>
  <summary>Details</summary>
Motivation: 传统MCP的智能体、模型和服务器无状态且无全局上下文，在LLM驱动的协调任务中，共享上下文存储可提高多智能体工作流的效率和连贯性。

Method: 设计CA - MCP，将执行逻辑卸载到能读写共享上下文内存的专用MCP服务器，以上下文管理为核心机制。

Result: 在TravelPlanner和REALM - Bench基准数据集实验中，CA - MCP减少了复杂任务的LLM调用次数，降低任务条件不满足时的响应失败频率。

Conclusion: CA - MCP在LLM驱动的多智能体系统中具有潜在优势，能提高效率和响应能力。

Abstract: The Model Context Protocol (MCP) has emerged as a widely used framework for enabling LLM-based agents to communicate with external tools and services. The most common implementation of MCP, proposed by Anthropic, heavily relies on a Large Language Model (LLM) to decompose tasks and issue instructions to servers, which act as stateless executors. In particular, the agents, models, and servers are stateless and do not have access to a global context. However, in tasks involving LLM-driven coordination, it is natural that a Shared Context Store (SCS) could improve the efficiency and coherence of multi-agent workflows by reducing redundancy and enabling knowledge transfer between servers. Thus, in this work, we design and assess the performance of a Context-Aware MCP (CA-MCP) that offloads execution logic to specialized MCP servers that read from and write to a shared context memory, allowing them to coordinate more autonomously in real time. In this design, context management serves as the central mechanism that maintains continuity across task executions by tracking intermediate states and shared variables, thereby enabling persistent collaboration among agents without repeated prompting. We present experiments showing that the CA-MCP can outperform the traditional MCP by reducing the number of LLM calls required for complex tasks and decreasing the frequency of response failures when task conditions are not satisfied, thereby improving overall efficiency and responsiveness. In particular, we conducted experiments on the TravelPlanner and REALM-Bench benchmark datasets and observed statistically significant results indicating the potential advantages of incorporating a shared context store via CA-MCP in LLM-driven multi-agent systems.

</details>


### [126] [Hardware-Aware Reformulation of Convolutions for Efficient Execution on Specialized AI Hardware: A Case Study on NVIDIA Tensor Cores](https://arxiv.org/abs/2601.11608)
*Ganesh Bikshandi*

Main category: cs.DC

TL;DR: 本文提出硬件感知的CNN计算重写方法，可在不修改网络权重的情况下满足硬件对齐，是推动语义调优的初步探索。


<details>
  <summary>Details</summary>
Motivation: 传统CNN性能受硬件约束，传统零填充处理对齐问题效率低，需更好方法。

Method: 使用重写规则对CNN计算进行硬件感知的重新表述，在不修改网络权重的训练后阶段满足硬件对齐。

Result: 目前实现了针对Tensor Cores的单一转换，方法具有可扩展性。

Conclusion: 该研究是迈向语义调优的初步步骤，为CNN模型在AI硬件上高效部署奠定基础。

Abstract: Convolutional Neural Networks (CNNs) are central to modern AI, but their performance is often limited by hardware constraints. NVIDIA Tensor Cores, for instance, require input channels to be multiples of 8 and sometimes 512 for efficient execution. {\em oneDNN} framework for CPU imposes such a requirement for the blocked format. Traditional approaches address such alignment issue using zero-padding, which can be inefficient. In this work, we present a first-step, hardware-aware reformulation of CNN computations using rewrite rules, restructuring the underlying math to satisfy hardware alignment entirely {\bf post-training} without modifying network weights. While our current implementation focuses on a single transformation for Tensor Cores, this approach is generalizable, laying the foundation to explore additional transformations for CPU and accelerators. This study represents an initial step toward {\em semantic tuning}, a systematic, hardware-aware optimization strategy for efficient deployment of CNN models on specialized AI hardware.

</details>


### [127] [Radio Labeling of Strong Prismatic Network With Star](https://arxiv.org/abs/2601.11624)
*Liming Wang,Feng Li,Linlin Cui*

Main category: cs.DC

TL;DR: 文章聚焦无线通信频谱分配，将其转化为图的无线电标号问题，研究强棱柱网络与星图的无线电标号，给出定理和示例，提出并行算法。


<details>
  <summary>Details</summary>
Motivation: 无线通信快速发展，高效频谱分配对提升网络性能至关重要，无线电标号作为信道分配组合优化模型是NP难题，研究特定图类的无线电标号有重要意义。

Method: 讨论强棱柱网络与星图的无线电标号，给出相关定理和示例，提出并行算法。

Result: 给出强棱柱网络与星图无线电标号的相关定理和示例，提出并行算法提升大规模网络场景计算效率。

Conclusion: 研究强棱柱网络与星图的无线电标号对无线网络最优信道分配设计有帮助，并行算法可提高计算效率。

Abstract: The rapid development of wireless communication has made efficient spectrum assignment a crucial factor in enhancing network performance. As a combinatorial optimization model for channel assignment, the radio labeling is recognized as an NP-hard problem. Therefore, converting the spectrum assignment problem into the radio labeling of graphs and studying the radio labeling of specific graph classes is of great significance. For $G$, a radio labeling $\varphi: V(G) \to \{0, 1, 2, \ldots\}$ is required to satisfy $|\varphi(u) - \varphi(v)| \geq \text{diam}(G) + 1 -d_G(u, v)$, where ${diam(G)}$ and $d_G(u, v)$ are diameter and distance between $u$ and $v$. For a radio labeling $\varphi$, its $\text{span}$ is defined as the largest integer assigned by $\varphi$ to the vertices of $G$; the radio labeling specifically denotes the labeling with the minimal span among possible radio labeling. The strong product is a crucial tool for constructing regular networks, and studying its radio labeling is necessary for the design of optimal channel assignment in wireless networks. Within this manuscript, we discuss the radio labeling of strong prismatic network with star, present the relevant theorems and examples, and propose a parallel algorithm to improve computational efficiency in large-scale network scenarios.

</details>


### [128] [A Forward Simulation-Based Hierarchy of Linearizable Concurrent Objects](https://arxiv.org/abs/2601.11646)
*Chao Wang,Ruijia Li,Yang Zhou,Peng Wu,Yi Lv,Jianwei Liao,Jim Woodcock,Zhiming Liu*

Main category: cs.DC

TL;DR: 文章系统研究线性可对象与前向模拟的关系，证明不同活性约束下线性可对象集合在该关系下的格结构，提出线性化的等价刻画并用于验证。


<details>
  <summary>Details</summary>
Motivation: 探究线性可对象与前向模拟的联系，为线性化验证提供理论支持。

Method: 通过数学证明，研究线性可对象集合在不同活性约束和无活性约束下在前向模拟关系中的结构，提出等价刻画。

Result: 证明满足不同活性的线性可对象集合形成有界半格或有界格，证明强线性可对象间的模拟关系，如时间戳队列模拟Herlihy - Wing队列等。

Conclusion: 提出的线性化等价刻画可用于线性化的验证。

Abstract: In this paper, we systematically investigate the connection between linearizable objects and forward simulation. We prove that the sets of linearizable objects satisfying wait-freedom (resp., lock-freedom or obstruction-freedom) form a bounded join-semilattice under the forward simulation relation, and that the sets of linearizable objects without liveness constraints form a bounded lattice under the same relation. As part of our lattice result, we propose an equivalent characterization of linearizability by reducing checking linearizability w.r.t. sequential specification $Spec$ into checking forward simulation by an object $\mathcal{U}_{Spec}$. To demonstrate the forward simulation relation between linearizable objects, we prove that the objects that are strongly linearizable w.r.t. the same sequential specification and are wait-free (resp., lock-free, obstruction-free) simulate each other, and we prove that the time-stamped queue simulates the Herlihy-Wing queue. We also prove that the Herlihy-Wing queue is simulated by $\mathcal{U}_{Spec}$, and thus, our equivalent characterization of linearizability can be used in the verification of linearizability.

</details>


### [129] [WISP: Waste- and Interference-Suppressed Distributed Speculative LLM Serving at the Edge via Dynamic Drafting and SLO-Aware Batching](https://arxiv.org/abs/2601.11652)
*Xiangchen Li,Jiakun Fan,Qingyuan Wang,Dimitrios Spatharakis,Saeid Ghafouri,Hans Vandierendonck,Deepu John,Bo Ji,Ali R. Butt,Dimitrios S. Nikolopoulos*

Main category: cs.DC

TL;DR: 针对LLM推理工作负载不均衡问题，提出分布式推理系统WISP，提高系统容量和吞吐量。


<details>
  <summary>Details</summary>
Motivation: LLM推理计算工作负载呈指数级增长，使数据中心压力大，边缘设备利用率低，导致工作负载不均衡和资源低效。

Method: 识别并形式化分布式推测式LLM服务的两个关键瓶颈，提出包含智能推测控制器、验证时间估计器和验证批处理调度器的WISP系统。

Result: 与集中式服务和SLED相比，WISP分别将系统容量提高2.1倍和4.1倍，将系统有效吞吐量提高1.94倍和3.7倍。

Conclusion: WISP能有效解决分布式推测式LLM服务瓶颈，提升系统效率。

Abstract: As Large Language Models (LLMs) become increasingly accessible to end users, an ever-growing number of inference requests are initiated from edge devices and computed on centralized GPU clusters. However, the resulting exponential growth in computation workload is placing significant strain on data centers, while edge devices remain largely underutilized, leading to imbalanced workloads and resource inefficiency across the network. Integrating edge devices into the LLM inference process via speculative decoding helps balance the workload between the edge and the cloud, while maintaining lossless prediction accuracy. In this paper, we identify and formalize two critical bottlenecks that limit the efficiency and scalability of distributed speculative LLM serving: Wasted Drafting Time and Verification Interference. To address these challenges, we propose WISP, an efficient and SLO-aware distributed LLM inference system that consists of an intelligent speculation controller, a verification time estimator, and a verification batch scheduler. These components collaboratively enhance drafting efficiency and optimize verification request scheduling on the server. Extensive numerical results show that WISP improves system capacity by up to 2.1x and 4.1x, and increases system goodput by up to 1.94x and 3.7x, compared to centralized serving and SLED, respectively.

</details>


### [130] [HALO: Semantic-Aware Distributed LLM Inference in Lossy Edge Network](https://arxiv.org/abs/2601.11676)
*Peirong Zheng,Wenchao Xu,Haozhao Wang,Jinyu Chen,Xuemin Shen*

Main category: cs.DC

TL;DR: 提出HALO框架用于在有损边缘网络中加速分布式大语言模型推理，实验显示有显著加速效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在边缘推理受单节点资源限制，现有分布式推理方法需严格同步，在不可靠网络中不可行。

Method: 提出HALO框架，包括语义感知预测器、神经元组并行加载方案和负载均衡调度器。

Result: 在树莓派集群实验中，HALO对LLaMA系列模型实现3.41倍端到端加速，性能接近最优条件，优于现有方法。

Conclusion: HALO能有效在有损边缘网络中提升分布式大语言模型推理性能。

Abstract: The deployment of large language models' (LLMs) inference at the edge can facilitate prompt service responsiveness while protecting user privacy. However, it is critically challenged by the resource constraints of a single edge node. Distributed inference has emerged to aggregate and leverage computational resources across multiple devices. Yet, existing methods typically require strict synchronization, which is often infeasible due to the unreliable network conditions. In this paper, we propose HALO, a novel framework that can boost the distributed LLM inference in lossy edge network. The core idea is to enable a relaxed yet effective synchronization by strategically allocating less critical neuron groups to unstable devices, thus avoiding the excessive waiting time incurred by delayed packets. HALO introduces three key mechanisms: (1) a semantic-aware predictor to assess the significance of neuron groups prior to activation. (2) a parallel execution scheme of neuron group loading during the model inference. (3) a load-balancing scheduler that efficiently orchestrates multiple devices with heterogeneous resources. Experimental results from a Raspberry Pi cluster demonstrate that HALO achieves a 3.41x end-to-end speedup for LLaMA-series LLMs under unreliable network conditions. It maintains performance comparable to optimal conditions and significantly outperforms the state-of-the-art in various scenarios.

</details>


### [131] [RAPID-Serve: Resource-efficient and Accelerated P/D Intra-GPU Disaggregation](https://arxiv.org/abs/2601.11822)
*Amna Masood,Pratishtha Gaur,Nuwan Jayasena*

Main category: cs.DC

TL;DR: 提出RAPID - Serve技术，结合预填充和解码在同一GPU上并发执行，还提出自适应资源管理，相比现有方法能提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型推理服务系统的混合批处理和分离式服务技术存在资源利用率低、延迟增加等局限，需改进。

Method: 提出RAPID - Serve技术，在同一GPU上并发执行预填充和解码；提出自适应资源管理，可利用CU掩码。

Result: RAPID - Serve实现了最高4.1倍（平均1.7倍）无约束吞吐量提升，在SLO约束下实现32倍及以上（平均4.9倍）吞吐量提升。

Conclusion: RAPID - Serve是一种有效的策略，尤其在资源受限环境中优于现有方法。

Abstract: Two widely adopted techniques for LLM inference serving systems today are hybrid batching and disaggregated serving. A hybrid batch combines prefill and decode tokens of different requests in the same batch to improve resource utilization and throughput at the cost of increased latency per token. In contrast, disaggregated serving decouples compute-bound prefill and bandwidth-bound decode phases to optimize for service level objectives (SLOs) at the cost of resource under-utilization and KV-cache transfer overheads. To address the limitations of these techniques, we propose RAPID-Serve: a technique to concurrently execute prefill and decode on the same GPU(s) to meet latency SLOs while maintaining high throughput and efficient resource utilization. Furthermore, we propose Adaptive Resource Management for runtime compute resource allocation, optionally leveraging CU masking (a fine-grained Compute Unit partitioning feature on AMD Instinct\textsuperscript{TM} GPUs). RAPID-Serve provides up to 4.1x (average 1.7x) unconstrained throughput improvement and 32x and higher (average 4.9x) throughput improvement under SLO constraints, showing it as an effective strategy compared to the state-of-the-art approaches, particularly in resource-constrained environments.

</details>


### [132] [Big Data Workload Profiling for Energy-Aware Cloud Resource Management](https://arxiv.org/abs/2601.11935)
*Milan Parikh,Aniket Abhishek Soni,Sneja Mitinbhai Shah,Ayush Raj Jha*

Main category: cs.DC

TL;DR: 提出工作负载感知且节能的调度框架，实验显示比基线调度器节能15 - 20%。


<details>
  <summary>Details</summary>
Motivation: 大数据工作负载规模和复杂度增加，云数据中心需降低运营能耗。

Method: 通过分析CPU利用率、内存需求和存储IO行为指导虚拟机放置决策，结合历史执行日志和实时监测预测放置影响，实现自适应整合。

Result: 使用代表性工作负载在多节点云测试平台评估，相比基线调度器节能15 - 20%，性能下降可忽略。

Conclusion: 工作负载分析是提高基于云的大数据处理环境可持续性的实用且可扩展策略。

Abstract: Cloud data centers face increasing pressure to reduce operational energy consumption as big data workloads continue to grow in scale and complexity. This paper presents a workload aware and energy efficient scheduling framework that profiles CPU utilization, memory demand, and storage IO behavior to guide virtual machine placement decisions. By combining historical execution logs with real time telemetry, the proposed system predicts the energy and performance impact of candidate placements and enables adaptive consolidation while preserving service level agreement compliance. The framework is evaluated using representative Hadoop MapReduce, Spark MLlib, and ETL workloads deployed on a multi node cloud testbed. Experimental results demonstrate consistent energy savings of 15 to 20 percent compared to a baseline scheduler, with negligible performance degradation. These findings highlight workload profiling as a practical and scalable strategy for improving the sustainability of cloud based big data processing environments.

</details>


### [133] [DaggerFFT: A Distributed FFT Framework Using Task Scheduling in Julia](https://arxiv.org/abs/2601.12209)
*Sana Taghipour Anvari,Julian Samaroo,Matin Raayai Ardakani,David Kaeli*

Main category: cs.DC

TL;DR: 提出分布式FFT框架DaggerFFT， treating FFT计算视为动态调度任务图，可在CPU和GPU后端上超越现有库，还集成到Oceananigans.jl展示优势。


<details>
  <summary>Details</summary>
Motivation: 科学模拟对分布式FFT算法需求增加，传统算法在异构平台有性能瓶颈，现有分布式方法有局限性。

Method: 用Julia开发DaggerFFT，将FFT计算视为动态调度任务图，FFT操作表示为DTasks，运行时用Dagger动态调度器分配任务。

Result: DaggerFFT在CPU和GPU后端上超越现有分布式FFT库，CPU集群加速比达2.6x，GPU集群达1.35x。

Conclusion: 基于任务的运行时能在大规模实际模拟中提供卓越性能和模块化。

Abstract: The Fast Fourier Transform (FFT) is a fundamental numerical technique with widespread application in a range of scientific problems. As scientific simulations attempt to exploit exascale systems, there has been a growing demand for distributed FFT algorithms that can effectively utilize modern heterogeneous high-performance computing (HPC) systems. Conventional FFT algorithms commonly encounter performance bottlenecks, especially when run on heterogeneous platforms. Most distributed FFT approaches rely on static task distribution and require synchronization barriers, limiting scalability and impacting overall resource utilization. In this paper we present DaggerFFT, a distributed FFT framework, developed in Julia, that treats highly parallel FFT computations as a dynamically scheduled task graph. Each FFT stage operates on a separately defined distributed array. FFT operations are expressed as DTasks operating on pencil or slab partitioned DArrays. Each FFT stage owns its own DArray, and the runtime assigns DTasks across devices using Dagger's dynamic scheduler that uses work stealing. We demonstrate how DaggerFFT's dynamic scheduler can outperform state-of-the-art distributed FFT libraries on both CPU and GPU backends, achieving up to a 2.6x speedup on CPU clusters and up to a 1.35x speedup on GPU clusters. We have integrated DaggerFFT into Oceananigans.jl, a geophysical fluid dynamics framework, demonstrating that high-level, task-based runtimes can deliver both superior performance and modularity in large-scale, real-world simulations.

</details>


### [134] [Power Aware Dynamic Reallocation For Inference](https://arxiv.org/abs/2601.12241)
*Yiwei Jiang,Sangeeta Chowdhary,Nathaniel Morris,Rutwik Jain,Srilatha Manne,Sam Bayliss*

Main category: cs.DC

TL;DR: 提出功率感知的解聚合推理框架RAPID，在严格功率限制下提升性能和应用一致性。


<details>
  <summary>Details</summary>
Motivation: 随着模型和集群规模增长，功率成为整体性能和成本效率的主要限制因素，现有解聚合策略未解决功率问题。

Method: 提出RAPID框架，联合管理GPU角色和功率预算，利用静态和动态功率重新分配及GPU重新分配。

Result: 在固定功率限制下提升性能，在峰值负载时SLO达成率最高提升2倍，且不增加复杂度和成本。

Conclusion: RAPID框架在严格功率限制下优于现有解聚合解决方案，能提升整体性能和应用一致性。

Abstract: Disaggregation has emerged as a powerful strategy for optimizing large language model (LLM) inference by separating compute-intensive prefill and memory-bound decode phases across specialized GPUs. This separation improves utilization and throughput under fixed hardware capacity. However, as model and cluster scales grow, power, rather than compute, has become the dominant limiter of overall performance and cost efficiency. In this paper, we propose RAPID, a power-aware disaggregated inference framework that jointly manages GPU roles and power budgets to sustain goodput within strict power caps. RAPID utilizes static and dynamic power reallocation in addition to GPU reallocation to improve performance under fixed power bounds. RAPID improves overall performance and application consistency beyond what is achievable in current disaggregation solutions, resulting in up to a 2x improvement in SLO attainment at peak load when compared to a static assignment without an increase in complexity or cost.

</details>


### [135] [RIPPLE++: An Incremental Framework for Efficient GNN Inference on Evolving Graphs](https://arxiv.org/abs/2601.12347)
*Pranjal Naman,Parv Agarwal,Hrishikesh Haritas,Yogesh Simmhan*

Main category: cs.DC

TL;DR: 现有图神经网络推理方法不适用于动态图，本文提出RIPPLE++框架，能高效准确更新嵌入，在单机器和分布式环境都有良好表现。


<details>
  <summary>Details</summary>
Motivation: 现实世界中图是动态的，现有顶点和层推理方法在动态图上有冗余计算、高通信成本等问题，采样方法有非确定性，无法满足实时应用低延迟推理需求。

Method: 提出RIPPLE++框架，引入广义增量编程模型，捕捉GNN聚合函数语义，增量传播更新到受影响邻域，支持常见图更新，可单机和分布式部署。

Result: 单机环境下，在稀疏图Arxiv上达56K更新/秒，在密集图Products上约7.6K更新/秒，延迟0.06 - 960ms，吞吐量比现有方法高2.2 - 24倍；分布式环境下，吞吐量比重新计算基线高约25倍，通信成本低20倍。

Conclusion: RIPPLE++框架能有效解决动态图上GNN推理的问题，在不同环境下都有出色性能。

Abstract: Real-world graphs are dynamic, with frequent updates to their structure and features due to evolving vertex and edge properties. These continual changes pose significant challenges for efficient inference in graph neural networks (GNNs). Existing vertex-wise and layer-wise inference approaches are ill-suited for dynamic graphs, as they incur redundant computations, large neighborhood traversals, and high communication costs, especially in distributed settings. Additionally, while sampling-based approaches can be adopted to approximate final layer embeddings, these are often not preferred in critical applications due to their non-determinism. These limitations hinder low-latency inference required in real-time applications. To address this, we propose RIPPLE++, a framework for streaming GNN inference that efficiently and accurately updates embeddings in response to changes in the graph structure or features. RIPPLE++ introduces a generalized incremental programming model that captures the semantics of GNN aggregation functions and incrementally propagates updates to affected neighborhoods. RIPPLE++ accommodates all common graph updates, including vertex/edge addition/deletions and vertex feature updates. RIPPLE++ supports both single-machine and distributed deployments. On a single machine, it achieves up to $56$K updates/sec on sparse graphs like Arxiv ($169$K vertices, $1.2$M edges), and about $7.6$K updates/sec on denser graphs like Products ($2.5$M vertices, $123.7$M edges), with latencies of $0.06$--$960$ms, and outperforming state-of-the-art baselines by $2.2$--$24\times$ on throughput. In distributed settings, RIPPLE++ offers up to $\approx25\times$ higher throughput and $20\times$ lower communication costs compared to recomputing baselines.

</details>


### [136] [ASAS-BridgeAMM: Trust-Minimized Cross-Chain Bridge AMM with Failure Containment](https://arxiv.org/abs/2601.12434)
*Shengwei You,Aditya Joshi,Andrey Kuehlkamp,Jarek Nabrzyski*

Main category: cs.DC

TL;DR: 提出ASAS - BridgeAMM桥接自动化做市商，处理跨链桥风险，经测试效果良好并证明安全性等。


<details>
  <summary>Details</summary>
Motivation: 现有跨链桥安全模型为二元性，存在巨大系统风险，自2021年损失超28亿美元，需改进。

Method: 提出ASAS - BridgeAMM，引入Contained Degradation，将跨链消息延迟视为可量化执行风险，动态调整相关参数。

Result: 18个月历史回放中，相比基线架构减少73%最坏情况的桥接资不抵债，压力期保持104.5%交易量；严格模拟中，偿付概率>0.9999，每周期坏账<0.2%总抵押品。

Conclusion: 提供Solidity参考实现，正式证明在拜占庭中继模型下的安全性、活性和抗操纵性。

Abstract: Cross-chain bridges constitute the single largest vector of systemic risk in Decentralized Finance (DeFi), accounting for over \$2.8 billion in losses since 2021. The fundamental vulnerability lies in the binary nature of existing bridge security models: a bridge is either fully operational or catastrophically compromised, with no intermediate state to contain partial failures. We present ASAS-BridgeAMM, a bridge-coupled automated market maker that introduces Contained Degradation: a formally specified operational state where the system gracefully degrades functionality in response to adversarial signals. By treating cross-chain message latency as a quantifiable execution risk, the protocol dynamically adjusts collateral haircuts, slippage bounds, and withdrawal limits. Across 18 months of historical replay on Ethereum and two auxiliary chains, ASAS-BridgeAMM reduces worst-case bridge-induced insolvency by 73% relative to baseline mint-and-burn architectures, while preserving 104.5% of transaction volume during stress periods. In rigorous adversarial simulations involving delayed finality, oracle manipulation, and liquidity griefing, the protocol maintains solvency with probability $>0.9999$ and bounds per-epoch bad debt to $<0.2%$ of total collateral. We provide a reference implementation in Solidity and formally prove safety (bounded debt), liveness (settlement completion), and manipulation resistance under a Byzantine relayer model.

</details>


### [137] [SGCP: A Self-Organized Game-Theoretic Framework For Collaborative Perception](https://arxiv.org/abs/2601.12524)
*Zechuan Gong,Hui Zhang,Yuquan Yang,Wenyu Lu*

Main category: cs.DC

TL;DR: 提出全去中心化框架解决协作感知部署难题，实验表明该方法降低通信开销，提高感知精度和覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 协作感知可提升自动驾驶安全性，但在通信带宽有限且无路边基础设施时，大规模部署协作感知存在困难。

Method: 引入全去中心化框架，将问题分解为两个博弈论阶段，设计分布式算法保证系统势能函数单调提升。

Result: 在CARLA - OpenCDA - NS3联合仿真平台的实验显示，该方法相比现有基线降低通信开销，提高感知精度和覆盖范围。

Conclusion: 所提全去中心化框架有效解决了协作感知部署难题，在通信和感知性能上表现更优。

Abstract: Collaborative perception holds great promise for improving safety in autonomous driving, particularly in dense traffic where vehicles can share sensory information to overcome individual blind spots and extend awareness. However, deploying such collaboration at scale remains difficult when communication bandwidth is limited and no roadside infrastructure is available. To overcome these limitations, we introduce a fully decentralized framework that enables vehicles to self organize into cooperative groups using only vehicle to vehicle communication. The approach decomposes the problem into two sequential game theoretic stages. In the first stage, vehicles form stable clusters by evaluating mutual sensing complementarity and motion coherence, and each cluster elects a coordinator. In the second stage, the coordinator guides its members to selectively transmit point cloud segments from perceptually salient regions through a non cooperative potential game, enabling efficient local fusion. Global scene understanding is then achieved by exchanging compact detection messages across clusters rather than raw sensor data. We design distributed algorithms for both stages that guarantee monotonic improvement of the system wide potential function. Comprehensive experiments on the CARLA-OpenCDA-NS3 co-simulation platform show that our method reduces communication overhead while delivering higher perception accuracy and wider effective coverage compared to existing baselines.

</details>


### [138] [Dynamic Detection of Inefficient Data Mapping Patterns in Heterogeneous OpenMP Applications](https://arxiv.org/abs/2601.12713)
*Luke Marzen,Junhyung Shim,Ali Jannesari*

Main category: cs.DC

TL;DR: 针对异构计算中数据移动瓶颈和现有工具需大量人工干预问题，提出动态分析技术并实现OMPDataPerf工具，开销低。


<details>
  <summary>Details</summary>
Motivation: 异构计算中数据移动是瓶颈，现有性能工具诊断数据传输低效问题需大量程序员干预。

Method: 提出动态分析技术检测和分析异构应用中低效数据传输和分配模式，并实现到OMPDataPerf工具中，利用OpenMP工具接口（OMPT）。

Result: OMPDataPerf可提供有问题的数据映射详细跟踪、源代码归因和优化潜力评估，运行时开销仅5%几何平均。

Conclusion: 所提动态分析技术及实现的OMPDataPerf能有效解决异构计算数据传输问题，且开销低。

Abstract: With the growing prevalence of heterogeneous computing, CPUs are increasingly being paired with accelerators to achieve new levels of performance and energy efficiency. However, data movement between devices remains a significant bottleneck, complicating application development. Existing performance tools require considerable programmer intervention to diagnose and locate data transfer inefficiencies. To address this, we propose dynamic analysis techniques to detect and profile inefficient data transfer and allocation patterns in heterogeneous applications. We implemented these techniques into OMPDataPerf, which provides detailed traces of problematic data mappings, source code attribution, and assessments of optimization potential in heterogeneous OpenMP applications. OMPDataPerf uses the OpenMP Tools Interface (OMPT) and incurs only a 5 % geometric-mean runtime overhead.

</details>


### [139] [Efficient Local-to-Global Collaborative Perception via Joint Communication and Computation Optimization](https://arxiv.org/abs/2601.12749)
*Hui Zhang,Yuquan Yang,Zechuan Gong,Xiaohua Xu,Dan Keun Sung*

Main category: cs.DC

TL;DR: 本文提出一种新的局部到全局协作感知框架LGCP，可高效实现协作感知，实验表明该框架能减少数据传输量并维持或提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有协作感知存在高通信开销和计算延迟问题，为解决这些挑战提出新框架。

Method: 将道路划分为非重叠区域，每个区域分配一个CAV组进行局部感知，组内指定领导者收集融合数据并上传至RSU，RSU汇总结果并广播全局视图，采用集中调度策略。

Result: 所提出的LGCP框架使数据传输量平均减少44倍，同时维持或提升整体协作性能。

Conclusion: LGCP框架能以通信和计算高效的方式实现协作感知。

Abstract: Autonomous driving relies on accurate perception to ensure safe driving. Collaborative perception improves accuracy by mitigating the sensing limitations of individual vehicles, such as limited perception range and occlusion-induced blind spots. However, collaborative perception often suffers from high communication overhead due to redundant data transmission, as well as increasing computation latency caused by excessive load with growing connected and autonomous vehicles (CAVs) participation. To address these challenges, we propose a novel local-to-global collaborative perception framework (LGCP) to achieve collaboration in a communication- and computation-efficient manner. The road of interest is partitioned into non-overlapping areas, each of which is assigned a dedicated CAV group to perform localized perception. A designated leader in each group collects and fuses perception data from its members, and uploads the perception result to the roadside unit (RSU), establishing a link between local perception and global awareness. The RSU aggregates perception results from all groups and broadcasts a global view to all CAVs. LGCP employs a centralized scheduling strategy via the RSU, which assigns CAV groups to each area, schedules their transmissions, aggregates area-level local perception results, and propagates the global view to all CAVs. Experimental results demonstrate that the proposed LGCP framework achieves an average 44 times reduction in the amount of data transmission, while maintaining or even improving the overall collaborative performance.

</details>


### [140] [Unleashing Efficient Asynchronous RL Post-Training via Staleness-Constrained Rollout Coordination](https://arxiv.org/abs/2601.12784)
*Haoyang Li,Sheng Lin,Fangcheng Fu,Yuming Zhou,Xiaodong Ji,Yanfeng Zhao,Lefeng Wang,Jie Jiang,Bin Cui*

Main category: cs.DC

TL;DR: 提出StaleFlow系统解决强化学习后训练中数据陈旧和偏斜问题，提升吞吐量且不影响收敛。


<details>
  <summary>Details</summary>
Motivation: 现有系统无法统一解决强化学习后训练中数据陈旧和偏斜问题，导致系统需在收敛性和性能间权衡。

Method: 引入全局一致性协议控制陈旧性，重新设计架构构建数据服务器缓解偏斜，开发陈旧感知、面向吞吐量的策略。

Result: StaleFlow比现有系统吞吐量高1.42 - 2.68倍（平均1.17 - 2.01倍），且不影响收敛。

Conclusion: StaleFlow能有效解决强化学习后训练中的数据问题，提升系统性能。

Abstract: Reinforcement learning (RL) post-training has become pivotal for enhancing the capabilities of modern large models. A recent trend is to develop RL systems with a fully disaggregated architecture, which decouples the three RL phases (rollout, reward, and training) onto separate resources and executes them asynchronously. However, two critical data-level concerns arise: (1) asynchronous execution leads to data staleness in trajectories (the data generated by rollout) as the model parameters used in rollout may not be up to date, which impairs RL convergence; and (2) the length variation of trajectories introduces severe data skewness, leading to workload imbalance and degraded system performance.
  Existing systems fail to address these two concerns in a unified manner. Techniques that tightly control data staleness often constrain effective data skewness mitigation, while aggressive data skewness mitigation tends to exacerbate data staleness. As a result, systems are forced to trade off convergence for performance, or vice versa. To address this, we propose StaleFlow, an RL post-training system that jointly tackles data staleness and skewness. First, to control staleness, StaleFlow introduces a global consistency protocol that tracks the full lifecycle of each trajectory and constrains staleness. Second, to mitigate skewness, StaleFlow re-designs the RL system architecture by constructing data servers for trajectories and parameters to achieve flexible rollout coordination. Subsequently, we develop a suite of staleness-aware, throughput-oriented strategies to enhance system performance. Evaluations show that StaleFlow achieves up to 1.42-2.68$\times$ (1.17-2.01$\times$ on average) higher throughput than state-of-the-art systems, without compromising convergence.

</details>


### [141] [From Design to Deorbit: A Solar-Electric Autonomous Module for Multi-Debris Remediation](https://arxiv.org/abs/2601.12830)
*Om Mishra,Jayesh Patil,Sathwik Narkedimilli,G Srikantha Sharma,Ananda S,Manjunath K Vanahalli*

Main category: cs.DC

TL;DR: 研究针对轨道碎片累积问题提出新修复架构，经仿真验证其能力，推动轨道管理发展。


<details>
  <summary>Details</summary>
Motivation: 当前依赖燃料的轨道碎片清除方法有局限，轨道碎片积累威胁太空操作可持续性，需新的主动清除方案。

Method: 引入新修复架构，集成机械夹紧系统、高效太阳能动力NASA NEXT推进器和自主导航协议。

Result: 高保真仿真验证架构能力，成功从800公里逆行脱轨到100公里，基于雷达EKF导航位置均方根误差<10米，使用DTN协议1秒内数据传输效率达93%。

Conclusion: 该方法显著推进轨道管理，为可再生太阳能推进树立标杆，减少对传统燃料依赖，延长多目标清除任务寿命。

Abstract: The escalating accumulation of orbital debris threatens the sustainability of space operations, necessitating active removal solutions that overcome the limitations of current fuel-dependent methods. To address this, this study introduces a novel remediation architecture that integrates a mechanical clamping system for secure capture with a high-efficiency, solar-powered NASA Evolutionary Xenon Thruster (NEXT) and autonomous navigation protocols. High-fidelity simulations validate the architecture's capabilities, demonstrating a successful retrograde deorbit from 800 km to 100 km, <10m position Root Mean Square Errors (RMSE) via radar-based Extended Kalman Filter (EKF) navigation, and a 93\% data delivery efficiency within 1 second using Delay/Disruption Tolerant Network (DTN) protocols. This approach significantly advances orbital management by establishing a benchmark for renewable solar propulsion that minimizes reliance on conventional fuels and extends mission longevity for multi-target removal.

</details>


### [142] [On Resilient and Efficient Linear Secure Aggregation in Hierarchical Federated Learning](https://arxiv.org/abs/2601.12853)
*Shudi Weng,Xiang Zhang,Yizhou Zhao,Giuseppe Caire,Ming Xiao,Mikael Skoglund*

Main category: cs.DC

TL;DR: 研究不可靠通信下分层安全聚合的基本限制，提出最优协议并证明其最优性，引入改进问题表述连接理论与实践。


<details>
  <summary>Details</summary>
Motivation: 探究不可靠通信下分层安全聚合的信息论意义上的基本限制。

Method: 刻画实现鲁棒安全聚合所需的最小通信和随机性成本，提出达到这些最小成本的协议并通过反向证明确立其最优性。

Result: 得到了最小通信和随机性成本，提出了最优协议。

Conclusion: 该改进问题表述可缩小现有信息论安全聚合协议与实际联邦学习问题之间的差距。

Abstract: In this paper, we study the fundamental limits of hierarchical secure aggregation under unreliable communication. We consider a hierarchical network where each client connects to multiple relays, and both client-to-relay and relay-to-server links are intermittent. Under this setting, we characterize the minimum communication and randomness costs required to achieve robust secure aggregation. We then propose an optimal protocol that attains these minimum costs, and establish its optimality through a matching converse proof. In addition, we introduce an improved problem formulation that bridges the gap between existing information-theoretic secure aggregation protocols and practical real-world federated learning problems.

</details>


### [143] [Sutradhara: An Intelligent Orchestrator-Engine Co-design for Tool-based Agentic Inference](https://arxiv.org/abs/2601.12967)
*Anish Biswas,Kanishk Goel,Jayashree Mohan,Alind Khare,Anjaly Parayil,Ramachandran Ramjee,Chetan Bansal*

Main category: cs.DC

TL;DR: 本文指出智能体应用产生新性能瓶颈，提出SUTRADHARA系统优化，减少了延迟。


<details>
  <summary>Details</summary>
Motivation: 智能体工作负载产生新的性能瓶颈，即最终答案首次令牌渲染（FTR）延迟增加，需解决该问题。

Method: 提出SUTRADHARA系统，通过工具感知提示拆分、流式工具执行和协调器感知缓存管理进行优化。

Result: 在A100 GPU上的工作负载中，SUTRADHARA将中位FTR延迟降低15％，端到端延迟降低10％。

Conclusion: 协同设计可以系统地控制智能体系统中的延迟。

Abstract: Agentic applications are LLMs that iteratively invoke external tools to accomplish complex tasks. Such tool-based agents are rapidly becoming the dominant paradigm for deploying language models in production. Unlike traditional single-turn inference, agentic workloads chain together multiple LLM calls and tool executions before producing a final response, creating a new performance bottleneck that manifests as increased latency in First Token Rendered (FTR) of the final answer. Through analysis of synthetic requests at production scale, we reveal three critical challenges: tool calls account for 30-80% of FTR latency, KV cache hit rates collapse despite substantial context reuse across iterations, and sequential orchestration wastes potential intra-request parallelism by sequentially executing LLM calls and tools. These bottlenecks stem from a design gap in which orchestrators and LLM engines operate as decoupled black boxes, preventing cross-layer optimizations. We present SUTRADHARA, a co-designed agentic inference system that integrates orchestration with LLM serving through a thin API enabling three optimizations: overlap tool execution with subsequent LLM prefill using tool-aware prompt splitting, streaming tool execution to dispatch tools incrementally during decode rather than waiting for complete output, and orchestrator-aware cache management that uses semantic hints to improve hit rates and reduce thrashing. Implemented on vLLM, SUTRADHARA reduces median FTR latency by 15% and end-to-end latency by 10% across workloads on A100 GPUs, demonstrating that co-design can systematically tame latency in agentic systems.

</details>


### [144] [Enshrined Proposer Builder Separation in the presence of Maximal Extractable Value](https://arxiv.org/abs/2601.12989)
*Yitian Wang,Yebo Feng,Yingjiu Li,Jiahua Xu*

Main category: cs.DC

TL;DR: 研究通过形式化框架评估ePBS的拍卖式区块构建机制，发现ePBS虽重新分配职责，但加剧了利润和内容集中化，凸显需研究更好平衡去中心化、公平和MEV缓解的机制设计。


<details>
  <summary>Details</summary>
Motivation: 在PoS共识机制的区块链系统中，MEV引发经济集中化和内容操纵担忧，ePBS虽被提出，但需评估其效果。

Method: 开发结合数学分析和基于代理的模拟的形式化框架，评估ePBS的拍卖式区块构建机制，重点关注MEV动态。

Result: ePBS虽重新分配职责，但显著加剧利润和内容集中化，利润基尼系数从无ePBS时的0.1749升至0.8358；95.4%的区块价值奖励给提议者，存在经济偏差。

Conclusion: ePBS加剧了构建者采用激进MEV策略的动机，需要未来研究更好平衡去中心化、公平和MEV缓解的机制设计。

Abstract: In blockchain systems operating under the Proof-of-Stake (PoS) consensus mechanism, fairness in transaction processing is essential to preserving decentralization and maintaining user trust. However, with the emergence of Maximal Extractable Value (MEV), concerns about economic centralization and content manipulation have intensified. To address these vulnerabilities, the Ethereum community has introduced Proposer Builder Separation (PBS), which separates block construction from block proposal. Later, enshrined Proposer Builder Separation (ePBS) was also proposed in EIP-7732, which embeds PBS directly into the Ethereum consensus layer.
  Our work identifies key limitations of ePBS by developing a formal framework that combines mathematical analysis and agent-based simulations to evaluate its auction-based block-building mechanism, with particular emphasis on MEV dynamics. Our results reveal that, although ePBS redistributes responsibilities between builders and proposers, it significantly amplifies profit and content centralization: the Gini coefficient for profits rises from 0.1749 under standard PoS without ePBS to 0.8358 under ePBS. This sharp increase indicates that a small number of efficient builders capture most value via MEV-driven auctions. Moreover, 95.4% of the block value is rewarded to proposers in ePBS, revealing a strong economic bias despite their limited role in block assembly. These findings highlight that ePBS exacerbates incentives for builders to adopt aggressive MEV strategies, suggesting the need for future research into mechanism designs that better balance decentralization, fairness, and MEV mitigation.

</details>


### [145] [CPU-less parallel execution of lambda calculus in digital logic](https://arxiv.org/abs/2601.13040)
*Harry Fitchett,Charles Fox*

Main category: cs.DC

TL;DR: 因晶体管密度增加但时钟速度未增，探索新并行架构，用lambda演算编译到数字逻辑做概念验证，实现并模拟执行成功，有望扩展到更大函数式语言。


<details>
  <summary>Details</summary>
Motivation: 晶体管密度增加但时钟速度未提升，需寻找新并行架构。

Method: 以lambda演算为源语言编译到数字逻辑，采用树状表示，用物理数字逻辑块和总线实现，并行执行beta - 约简。

Result: 实现系统并给出模拟结果，成功执行lambda表达式测试套件。

Conclusion: 该方法有望扩展到更大的函数式语言。

Abstract: While transistor density is still increasing, clock speeds are not, motivating the search for new parallel architectures. One approach is to completely abandon the concept of CPU -- and thus serial imperative programming -- and instead to specify and execute tasks in parallel, compiling from programming languages to data flow digital logic. It is well-known that pure functional languages are inherently parallel, due to the Church-Rosser theorem, and CPU-based parallel compilers exist for many functional languages. However, these still rely on conventional CPUs and their von Neumann bottlenecks. An alternative is to compile functional languages directly into digital logic to maximize available parallelism. It is difficult to work with complete modern functional languages due to their many features, so we demonstrate a proof-of-concept system using lambda calculus as the source language and compiling to digital logic. We show how functional hardware can be tailored to a simplistic functional language, forming the ground for a new model of CPU-less functional computation. At the algorithmic level, we use a tree-based representation, with data localized within nodes and communicated data passed between them. This is implemented by physical digital logic blocks corresponding to nodes, and buses enabling message passing. Node types and behaviors correspond to lambda grammar forms, and beta-reductions are performed in parallel allowing branches independent from one another to perform transformations simultaneously. As evidence for this approach, we present an implementation, along with simulation results, showcasing successful execution of lambda expressions. This suggests that the approach could be scaled to larger functional languages. Successful execution of a test suite of lambda expressions suggests that the approach could be scaled to larger functional languages.

</details>


### [146] [Exploration on Highly Dynamic Graphs](https://arxiv.org/abs/2601.13047)
*Ashish Saxena,Kaushik Mondal*

Main category: cs.DC

TL;DR: 研究两种动态图模型中移动代理的探索问题，强化不可能性结果并提出新算法。


<details>
  <summary>Details</summary>
Motivation: 加强先前研究，深入探究两种动态图模型下移动代理的探索问题。

Method: 先强化1-Interval Connectivity模型的不可能性结果，再分析Connectivity Time下不同代理数量的情况，最后提出使用特定数量代理的探索算法。

Result: 强化了1-Interval Connectivity的不可能性结果；在Connectivity Time图中，(n - 1)(n - 2) / 2个代理无法完成探索，改进了先前界限；证明(n - 1)(n - 2) / 2 + 1个代理时需要1-hop可见性；给出使用(n - 1)(n - 2) / 2 + 1个代理的探索算法。

Conclusion: 对两种动态图模型下移动代理探索问题有了更深入的认识，并得到关键条件和有效算法。

Abstract: We study the exploration problem by mobile agents in two prominent models of dynamic graphs: $1$-Interval Connectivity and Connectivity Time. The $1$-Interval Connectivity model was introduced by Kuhn et al.~[STOC 2010], and the Connectivity Time model was proposed by Michail et al.~[JPDC 2014]. Recently, Saxena et al.~[TCS 2025] investigated the exploration problem under both models. In this work, we first strengthen the existing impossibility results for the $1$-Interval Connectivity model. We then show that, in Connectivity Time dynamic graphs, exploration is impossible with $\frac{(n-1)(n-2)}{2}$ mobile agents, even when the agents have full knowledge of all system parameters, global communication, full visibility, and infinite memory. This significantly improves the previously known bound of $n$. Moreover, we prove that to solve exploration with $\frac{(n-1)(n-2)}{2}+1$ agents, $1$-hop visibility is necessary. Finally, we present an exploration algorithm that uses $\frac{(n-1)(n-2)}{2}+1$ agents, assuming global communication, $1$-hop visibility, and $O(\log n)$ memory per agent.

</details>


### [147] [OPTIMUM-DERAM: Highly Consistent, Scalable, and Secure Multi-Object Memory using RLNC](https://arxiv.org/abs/2601.13146)
*Nicolas Nicolaou,Kishori M. Konwar,Moritz Grundei,Aleksandr Bezobchuk,Muriel Médard,Sriram Vishwanath*

Main category: cs.DC

TL;DR: 本文介绍了OPTIMUM - DERAM，一种高效的分布式共享内存解决方案，实验表明其性能和可扩展性优于以往方案。


<details>
  <summary>Details</summary>
Motivation: 传统分布式共享内存实现所需资源多，在实际系统中成本过高。

Method: 提出去中心化、可重构的原子读写共享内存OPTIMUM - DERAM，利用随机线性网络编码提高性能和存储可扩展性，引入基于一致哈希环的对象放置和发现方法扩展支持的原子对象数量，借助区块链预言机实现参与者的动态加入和离开，可容忍拜占庭故障保证安全性。

Result: 通过在全球分布式节点上的实验，发现OPTIMUM - DERAM在性能和可扩展性上优于之前的分布式共享内存解决方案（如ABD算法）。

Conclusion: OPTIMUM - DERAM是一种高度一致、可扩展、安全且去中心化的共享内存解决方案，具有良好性能和可扩展性。

Abstract: This paper introduces OPTIMUM-DERAM, a highly consistent, scalable, secure, and decentralized shared memory solution. Traditional distributed shared memory implementations offer multi-object support by multi-threading a single object memory instance over the same set of data hosts. While theoretically sound, the amount of resources required made such solutions prohibitively expensive in practical systems. OPTIMUM-DERAM proposes a decentralized, reconfigurable, atomic read/write shared memory (DeRAM) that: (i) achieves improved performance and storage scalability by leveraging Random Linear Network Codes (RLNC); (ii) scales in the number of supported atomic objects by introducing a new object placement and discovery approach based on a consistent hashing ring; (iii) scales in the number of participants by allowing dynamic joins and departures leveraging a blockchain oracle to serve as a registry service; and (iv) is secure against malicious behavior by tolerating Byzantine failures.
  Experimental results over a globally distributed set of nodes, help us realize the performance and scalability gains of OPTIMUM-DERAM over previous distributed shared memory solutions (i.e., the ABD algorithm [3])

</details>


### [148] [Towards Scalable Federated Container Orchestration: The CODECO Approach](https://arxiv.org/abs/2601.13351)
*Rute C. Sofia,Josh Salomon,Ray Carrol,Luis Garcés-Erice,Peter Urbanetz,Jürgen Gesswein,Rizkallah Touma,Alejandro Espinosa,Luis M. Contreras,Vasileios Theodorou,George Papathanail,Georgios Koukis,Vassilis Tsaoussidis,Alberto del Rio,David Jimenez,Efterpi Paraskevoulakou,Panagiotis Karamolegkos,John Soldatos,Borja Dorado Nogales,Alejandro Tjaarda*

Main category: cs.DC

TL;DR: 本文提出CODECO，一种用于Kubernetes的联邦编排框架，介绍其方法、架构组件、工作流和实验框架。


<details>
  <summary>Details</summary>
Motivation: 解决以云为中心的部署的局限性。

Method: 采用数据-计算-网络协同编排方法，用语义应用模型、基于分区的联邦和AI辅助决策支持扩展Kubernetes，使用混合治理模型。

Result: 构建了CODECO并描述其架构和核心组件，给出代表性编排工作流，引入基于软件的实验框架。

Conclusion: CODECO能支持异构基础设施、移动性和多供应商操作，实现跨联邦环境的上下文感知应用和微服务放置与自适应管理。

Abstract: This paper presents CODECO, a federated orchestration framework for Kubernetes that addresses the limitations of cloud-centric deployment. CODECO adopts a data-compute-network co-orchestration approach to support heterogeneous infrastructures, mobility, and multi-provider operation.
  CODECO extends Kubernetes with semantic application models, partition-based federation, and AI-assisted decision support, enabling context-aware placement and adaptive management of applications and their micro-services across federated environments. A hybrid governance model combines centralized policy enforcement with decentralized execution and learning to preserve global coherence while supporting far Edge autonomy. The paper describes the architecture and core components of CODECO, outlines representative orchestration workflows, and introduces a software-based experimentation framework for reproducible evaluation in federated Edge-Cloud infrastructure environments.

</details>


### [149] [Driving Computational Efficiency in Large-Scale Platforms using HPC Technologies](https://arxiv.org/abs/2601.13424)
*Alexander Martinez Mendez,Antonio J. Rubio-Montero,Carlos J. Barrios H.,Hernán Asorey,Rafael Mayo-García,Luis A. Núñez*

Main category: cs.DC

TL;DR: 文章聚焦LAGO项目的HPC资源利用效率，分析历史数据找出低效点并给出优化建议。


<details>
  <summary>Details</summary>
Motivation: LAGO项目利用大量HPC资源进行复杂模拟，资源效率对科研生产力和可持续性至关重要，需量化并改善资源利用率。

Method: 分析EGI FedCloud平台的历史作业记账数据，确定主要工作负载类别，用关键效率指标评估性能。

Result: 揭示了显著模式，如单个模拟任务CPU效率高，但短测试作业影响总体指标。

Conclusion: 找出特定低效点，提出优化资源请求、工作流管理策略等建议，以提高计算吞吐量和科研回报。

Abstract: The Latin American Giant Observatory (LAGO) project utilizes extensive High-Performance Computing (HPC) resources for complex astroparticle physics simulations, making resource efficiency critical for scientific productivity and sustainability. This article presents a detailed analysis focused on quantifying and improving HPC resource utilization efficiency specifically within the LAGO computational environment. The core objective is to understand how LAGO's distinct computational workloads-characterized by a prevalent coarse-grained, task-parallel execution model-consume resources in practice. To achieve this, we analyze historical job accounting data from the EGI FedCloud platform, identifying primary workload categories (Monte Carlo simulations, data processing, user analysis/testing) and evaluating their performance using key efficiency metrics (CPU utilization, walltime utilization, and I/O patterns). Our analysis reveals significant patterns, including high CPU efficiency within individual simulation tasks contrasted with the distorting impact of short test jobs on aggregate metrics. This work pinpoints specific inefficiencies and provides data-driven insights into LAGO's HPC usage. The findings directly inform recommendations for optimizing resource requests, refining workflow management strategies, and guiding future efforts to enhance computational throughput, ultimately maximizing the scientific return from LAGO's HPC investments.

</details>


### [150] [RASC: Enhancing Observability & Programmability in Smart Spaces](https://arxiv.org/abs/2601.13496)
*Anna Karanika,Kai-Siang Wang,Han-Ting Liang,Shalni Sundram,Indranil Gupta*

Main category: cs.DC

TL;DR: 提出RASC抽象以改善物联网动作的可观测性和可编程性，集成到Home Assistant评估效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有RPC不适用于用户端物联网设备集合，需更具表现力的抽象，且要提升物联网动作的可观测性和可编程性。

Method: 提出RASC抽象，在物联网设备动作启动后关键节点提供确认，并将其集成到Home Assistant框架。

Result: RASC满足延迟SLO，尤其对长时间动作；家庭自动化调度策略优于现有技术10%-55%。

Conclusion: RASC适合物联网动作，可实现新特性，能基于现有RPC机制实现。

Abstract: While RPCs form the bedrock of systems stacks, we posit that IoT device collections in smart spaces like homes, warehouses, and office buildings--which are all "user-facing"--require a more expressive abstraction. Orthogonal to prior work, which improved the reliability of IoT communication, our work focuses on improving the observability and programmability of IoT actions. We present the RASC (Request-Acknowledge-Start-Complete) abstraction, which provides acknowledgments at critical points after an IoT device action is initiated. RASC is a better fit for IoT actions, which naturally vary in length spatially (across devices) and temporally (across time, for a given device). RASC also enables the design of several new features: predicting action completion times accurately, detecting failures of actions faster, allowing fine-grained dependencies in programming, and scheduling. RASC is intended to be implemented atop today's available RPC mechanisms, rather than as a replacement. We integrated RASC into a popular and open-source IoT framework called Home Assistant. Our trace-driven evaluation finds that RASC meets latency SLOs, especially for long actions that last O(mins), which are common in smart spaces. Our scheduling policies for home automations (e.g., routines) outperform state-of-the-art counterparts by 10%-55%.

</details>


### [151] [A Kubernetes custom scheduler based on reinforcement learning for compute-intensive pods](https://arxiv.org/abs/2601.13579)
*Hanlin Zhou,Huah Yong Chan,Shun Yao Zhang,Meie Lin,Jingfei Ni*

Main category: cs.DC

TL;DR: 提出基于强化学习的调度器SDQN和SDQN - n，在计算密集场景中优于默认Kubernetes调度器，节省资源，且参数易调以适应不同场景。


<details>
  <summary>Details</summary>
Motivation: 默认Kubernetes调度器在计算密集型工作负载中无法实现最优放置。

Method: 提出基于深度Q网络（DQN）框架的两个自定义强化学习调度器SDQN和SDQN - n。

Result: 在计算密集场景中，模型优于默认调度器和基于Transformer、LSTM的替代方案，降低集群节点平均CPU利用率，SDQN - n节省资源效果更明显。

Conclusion: Pod调度需根据不同场景采用不同策略，SDQN和SDQN - n参数易调可适应未来不同场景需求。

Abstract: With the rise of cloud computing and lightweight containers, Docker has emerged as a leading technology for rapid service deployment, with Kubernetes responsible for pod orchestration. However, for compute-intensive workloads-particularly web services executing containerized machine-learning training-the default Kubernetes scheduler does not always achieve optimal placement. To address this, we propose two custom, reinforcement-learning-based schedulers, SDQN and SDQN-n, both built on the Deep Q-Network (DQN) framework. In compute-intensive scenarios, these models outperform the default Kubernetes scheduler as well as Transformer-and LSTM-based alternatives, reducing average CPU utilization per cluster node by 10%, and by over 20% when using SDQN-n. Moreover, our results show that SDQN-n approach of consolidating pods onto fewer nodes further amplifies resource savings and helps advance greener, more energy-efficient data centers.Therefore, pod scheduling must employ different strategies tailored to each scenario in order to achieve better performance.Since the reinforcement-learning components of the SDQN and SDQN-n architectures proposed in this paper can be easily tuned by adjusting their parameters, they can accommodate the requirements of various future scenarios.

</details>


### [152] [Device Association and Resource Allocation for Hierarchical Split Federated Learning in Space-Air-Ground Integrated Network](https://arxiv.org/abs/2601.13817)
*Haitao Zhao,Xiaoyu Tang,Bo Xu,Jinlong Sun,Linghao Zhang*

Main category: cs.DC

TL;DR: 本文针对6G SAGIN中FL面临的问题提出HSFL框架及优化算法，仿真表明算法能平衡训练效率和模型准确率。


<details>
  <summary>Details</summary>
Motivation: 6G促进SAGIN中部署FL，但FL面临资源受限和数据分布不均衡等挑战。

Method: 提出HSFL框架并推导损失函数上界，将联合优化问题分解为子问题，提出基于暴力搜索分割点的迭代优化算法。

Result: 仿真结果表明所提算法能有效平衡SAGIN中FL的训练效率和模型准确率。

Conclusion: 所提算法可有效解决6G SAGIN中FL面临的问题，平衡训练效率和模型准确性。

Abstract: 6G facilitates deployment of Federated Learning (FL) in the Space-Air-Ground Integrated Network (SAGIN), yet FL confronts challenges such as resource constrained and unbalanced data distribution. To address these issues, this paper proposes a Hierarchical Split Federated Learning (HSFL) framework and derives its upper bound of loss function. To minimize the weighted sum of training loss and latency, we formulate a joint optimization problem that integrates device association, model split layer selection, and resource allocation. We decompose the original problem into several subproblems, where an iterative optimization algorithm for device association and resource allocation based on brute-force split point search is proposed. Simulation results demonstrate that the proposed algorithm can effectively balance training efficiency and model accuracy for FL in SAGIN.

</details>


### [153] [torch-sla: Differentiable Sparse Linear Algebra with Adjoint Solvers and Sparse Tensor Parallelism for PyTorch](https://arxiv.org/abs/2601.13994)
*Mingyuan Chi*

Main category: cs.DC

TL;DR: 介绍开源 PyTorch 库 	orchsla{}，支持 GPU 加速、可扩展和可微分的稀疏线性代数，解决三个核心挑战，代码开源。


<details>
  <summary>Details</summary>
Motivation: 工业科学计算常用稀疏矩阵表示非结构化数据，需要一个支持 GPU 加速、可扩展和可微分的稀疏线性代数库。

Method: 解决 GPU 加速、多 GPU 扩展和伴随求导三个挑战，支持多后端并与 PyTorch 自动求导集成。

Result: 	orchsla{}库实现了 GPU 上的稀疏线性求解、非线性求解和特征值计算，多 GPU 扩展能处理 4 亿自由度线性求解，求导计算图节点和内存消耗有优势。

Conclusion: 	orchsla{}是一个支持多后端、与 PyTorch 自动求导无缝集成的开源库，可用于端到端可微分模拟。

Abstract: Industrial scientific computing predominantly uses sparse matrices to represent unstructured data -- finite element meshes, graphs, point clouds. We present \torchsla{}, an open-source PyTorch library that enables GPU-accelerated, scalable, and differentiable sparse linear algebra. The library addresses three fundamental challenges: (1) GPU acceleration for sparse linear solves, nonlinear solves (Newton, Picard, Anderson), and eigenvalue computation; (2) Multi-GPU scaling via domain decomposition with halo exchange, reaching \textbf{400 million DOF linear solve on 3 GPUs}; and (3) Adjoint-based differentiation} achieving $\mathcal{O}(1)$ computational graph nodes (for autograd) and $\mathcal{O}(\text{nnz})$ memory -- independent of solver iterations. \torchsla{} supports multiple backends (SciPy, cuDSS, PyTorch-native) and seamlessly integrates with PyTorch autograd for end-to-end differentiable simulations. Code is available at https://github.com/walkerchi/torch-sla.

</details>


### [154] [Multi-Partner Project: Multi-GPU Performance Portability Analysis for CFD Simulations at Scale](https://arxiv.org/abs/2601.14159)
*Panagiotis-Eleftherios Eleftherakis,George Anagnostopoulos,Anastassis Kapetanakis,Mohammad Umair,Jean-Yves Vet,Konstantinos Iliakis,Jonathan Vincent,Jing Gong,Akshay Patil,Clara García-Sánchez,Gerardo Zampino,Ricardo Vinuesa,Sotirios Xydis*

Main category: cs.DC

TL;DR: 本文分析SOD2D在AMD和NVIDIA GPU架构上的性能可移植性，探讨其物理和数值模型，多层面研究性能与可扩展性，发现内存访问优化效果多样，集群上性能有变化，需多级调优。


<details>
  <summary>Details</summary>
Motivation: 异构超级计算架构中GPU愈发重要，CFD模拟需高效利用硬件，HPC代码面临性能可移植性挑战，本文在REFMAP项目下分析SOD2D在不同GPU架构上的性能可移植性。

Method: 先讨论SOD2D的物理和数值模型，找出计算热点；多层面研究其性能和可扩展性，定义并刻画全栈设计空间；对单GPU在不同架构和编译器栈上进行性能表征；在LUMI多GPU集群上研究其大规模性能可变性。

Result: 单GPU上内存访问优化效果多样，加速比偏差在0.69× - 3.91×；LUMI集群上性能有类似吞吐量变化。

Conclusion: 性能预测有局限，需要进行多级、有依据的调优。

Abstract: As heterogeneous supercomputing architectures leveraging GPUs become increasingly central to high-performance computing (HPC), it is crucial for computational fluid dynamics (CFD) simulations, a de-facto HPC workload, to efficiently utilize such hardware. One of the key challenges of HPC codes is performance portability, i.e. the ability to maintain near-optimal performance across different accelerators. In the context of the \textbf{REFMAP} project, which targets scalable, GPU-enabled multi-fidelity CFD for urban airflow prediction, this paper analyzes the performance portability of SOD2D, a state-of-the-art Spectral Elements simulation framework across AMD and NVIDIA GPU architectures. We first discuss the physical and numerical models underlying SOD2D, highlighting its computational hotspots. Then, we examine its performance and scalability in a multi-level manner, i.e. defining and characterizing an extensive full-stack design space spanning across application, software and hardware infrastructure related parameters. Single-GPU performance characterization across server-grade NVIDIA and AMD GPU architectures and vendor-specific compiler stacks, show the potential as well as the diverse effect of memory access optimizations, i.e. 0.69$\times$ - 3.91$\times$ deviations in acceleration speedup. Performance variability of SOD2D at scale is further examined on the LUMI multi-GPU cluster, where profiling reveals similar throughput variations, highlighting the limits of performance projections and the need for multi-level, informed tuning.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [155] [Exact Computation of the Catalan Number $C(2,050,572,903)$](https://arxiv.org/abs/2601.11621)
*Mahesh Ramani*

Main category: cs.DS

TL;DR: 本文提出一种两阶段算法，以前所未有的规模计算精确卡特兰数，算出了目前最大精确卡特兰数并报告性能、讨论验证策略。


<details>
  <summary>Details</summary>
Motivation: 要以更大规模精确计算卡特兰数，解决评估大阶乘时的内存限制问题。

Method: 采用两阶段算法，第一阶段用并行分段筛枚举素数并用勒让德公式确定素因数分解，将素数按指数分组存到磁盘；第二阶段用内存高效的平衡产品树结合分块重构最终整数。

Result: 计算出n = 2,050,572,903的C(n)，结果有1,234,567,890位十进制数，时间复杂度为Θ(n(log n)^2)位运算，空间复杂度为Θ(n log n)位，是目前算出的最大精确卡特兰数。

Conclusion: 该算法能有效解决内存限制问题，以更大规模计算精确卡特兰数，代码和数据公开保证可重复性。

Abstract: This paper presents a two-phase algorithm for computing exact Catalan numbers at an unprecedented scale. The method is demonstrated by computing $C(n)$ for $n = 2,050,572,903$ yielding a result with a targeted $1,234,567,890$ decimal digits. To circumvent the memory limitations associated with evaluating large factorials, the algorithm operates exclusively in the prime-exponent domain. Phase 1 employs a parallel segmented sieve to enumerate primes up to $2n$ and applies Legendre's formula to determine the precise prime factorization of $C(n)$. The primes are grouped by exponent and serialized to disk. Phase 2 reconstructs the final integer using a memory-efficient balanced product tree with chunking. The algorithm runs on a time complexity of $Θ(n(\log n)^2)$ bit-operations and a space complexity of $Θ(n \log n)$ bits. This result represents the largest exact Catalan number computed to date. Performance statistics for a single-machine execution are reported, and verification strategies -- including modular checks and SHA-256 hash validation -- are discussed. The source code and factorization data are provided to ensure reproducibility.

</details>


### [156] [Bicriteria Algorithms for Submodular Cover with Partition and Fairness Constraints](https://arxiv.org/abs/2601.11755)
*Wenjing Chen,Yixin Chen,Victoria G. Crawford*

Main category: cs.DS

TL;DR: 本文研究带分区约束的子模覆盖问题（SCP）及其变体，开发并分析了可扩展的双准则近似算法，在单调和非单调目标下都适用，算法有良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有子模覆盖工作大多忽略了数据集自然分区结构，而实际应用中存在基于分区约束的子模优化问题。

Method: 为NP难的优化问题开发并分析可扩展的双准则近似算法，分别考虑单调和非单调目标。

Result: 单调情况下提出的算法达到最优近似保证，且与现有方法相比显著降低查询复杂度。

Conclusion: 通过对真实和合成数据集的实证评估，验证了所提算法的效率和有效性。

Abstract: In many submodular optimization applications, datasets are naturally partitioned into disjoint subsets. These scenarios give rise to submodular optimization problems with partition-based constraints, where the desired solution set should be in some sense balanced, fair, or resource-constrained across these partitions. While existing work on submodular cover largely overlooks this structure, we initiate a comprehensive study of the problem of Submodular Cover with Partition Constraints (SCP) and its key variants. Our main contributions are the development and analysis of scalable bicriteria approximation algorithms for these NP-hard optimization problems for both monotone and nonmonotone objectives. Notably, the algorithms proposed for the monotone case achieve optimal approximation guarantees while significantly reducing query complexity compared to existing methods. Finally, empirical evaluations on real-world and synthetic datasets further validate the efficiency and effectiveness of the proposed algorithms.

</details>


### [157] [Sum Estimation via Vector Similarity Search](https://arxiv.org/abs/2601.11765)
*Stephen Mussmann,Mehul Smriti Raje,Kavya Tumkur,Oumayma Messoussi,Cyprien Hachem,Seby Jacob*

Main category: cs.DS

TL;DR: 本文提出新算法，在估计集合中对象总和任务上，只需获取O(log(n))个最相似向量，比现有方法更高效。


<details>
  <summary>Details</summary>
Motivation: 研究集合中对象总和估计任务，改进现有需获取O(√n)个最相似向量的解决方案。

Method: 随机将对象按指数衰减概率分配到不同层级，为每层构建向量相似性搜索数据结构，结合每层top - k对象给出无偏估计并证明误差界。

Result: 在OpenImages和Amazon Reviews实验表明，该方法计算时间更少、误差更低。

Conclusion: 新算法在估计密度、计算softmax分母和统计球内向量数量等应用中表现更好。

Abstract: Semantic embeddings to represent objects such as image, text and audio are widely used in machine learning and have spurred the development of vector similarity search methods for retrieving semantically related objects. In this work, we study the sibling task of estimating a sum over all objects in a set, such as the kernel density estimate (KDE) and the normalizing constant for softmax distributions. While existing solutions provably reduce the sum estimation task to acquiring $\mathcal{O}(\sqrt{n})$ most similar vectors, where $n$ is the number of objects, we introduce a novel algorithm that only requires $\mathcal{O}(\log(n))$ most similar vectors. Our approach randomly assigns objects to levels with exponentially-decaying probabilities and constructs a vector similarity search data structure for each level. With the top-$k$ objects from each level, we propose an unbiased estimate of the sum and prove a high-probability relative error bound. We run experiments on OpenImages and Amazon Reviews with a vector similar search implementation to show that our method can achieve lower error using less computational time than existing reductions. We show results on applications in estimating densities, computing softmax denominators, and counting the number of vectors within a ball.

</details>


### [158] [Analysis of a Random Local Search Algorithm for Dominating Set](https://arxiv.org/abs/2601.11841)
*Hendrik Higl*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Dominating Set is a well-known combinatorial optimization problem which finds application in computational biology or mobile communication. Because of its $\mathrm{NP}$-hardness, one often turns to heuristics for good solutions. Many such heuristics have been empirically tested and perform rather well. However, it is not well understood why their results are so good or even what guarantees they can offer regarding their runtime or the quality of their results. For this, a strong theoretical foundation has to be established. We contribute to this by rigorously analyzing a Random Local Search (RLS) algorithm that aims to find a minimum dominating set on a graph. We consider its performance on cycle graphs with $n$ vertices. We prove an upper bound for the expected runtime until an optimum is found of $\mathcal{O}\left(n^4\log^2(n)\right)$. In doing so, we introduce several models to represent dominating sets on cycles that help us understand how RLS explores the search space to find an optimum. For our proof we use techniques which are already quite popular for the analysis of randomized algorithms. We further apply a special method to analyze a reversible Markov Chain, which arises as a result of our modeling. This method has not yet found wide application in this kind of runtime analysis.

</details>


### [159] [The Energy-Throughput Trade-off in Lossless-Compressed Source Code Storage](https://arxiv.org/abs/2601.13220)
*Paolo Ferragina,Francesco Tosoni*

Main category: cs.DS

TL;DR: 本文研究用于大规模源代码数据集索引的压缩键值存储设计，评估其在空间、时间和能源效率上的权衡，实验显示不同压缩配置有不同权衡，数据并行能提升速度但提升能源效率较难，还为构建存储后端提供指导。


<details>
  <summary>Details</summary>
Motivation: 从大规模源代码存档中检索数据对AI训练等很重要，需设计压缩键值存储并评估资源权衡。

Method: 在国家高性能计算基础设施上进行广泛实验，还研究数据并行性。

Result: 不同压缩配置有不同权衡，高压缩比能提升检索吞吐量和能源效率；数据并行显著提升速度，但提升能源效率较难。

Conclusion: 该工作简化了能源感知配置调优自动化和绿色基准测试部署，为系统架构师提供权衡和构建存储后端的指导。

Abstract: Retrieving data from large-scale source code archives is vital for AI training, neural-based software analysis, and information retrieval, to cite a few. This paper studies and experiments with the design of a compressed key-value store for the indexing of large-scale source code datasets, evaluating its trade-off among three primary computational resources: (compressed) space occupancy, time, and energy efficiency. Extensive experiments on a national high-performance computing infrastructure demonstrate that different compression configurations yield distinct trade-offs, with high compression ratios and order-of-magnitude gains in retrieval throughput and energy efficiency. We also study data parallelism and show that, while it significantly improves speed, scaling energy efficiency is more difficult, reflecting the known non-energy-proportionality of modern hardware and challenging the assumption of a direct time-energy correlation. This work streamlines automation in energy-aware configuration tuning and standardized green benchmarking deployable in CI/CD pipelines, thus empowering system architects with a spectrum of Pareto-optimal energy-compression-throughput trade-offs and actionable guidelines for building sustainable, efficient storage backends for massive open-source code archival.

</details>


### [160] [Parameterized Complexity of Scheduling Problems in Robotic Process Automation](https://arxiv.org/abs/2601.11984)
*Michal Dvořák,Antonín Novák,Přemysl Šůcha,Dušan Knop,Claire Hanen*

Main category: cs.DS

TL;DR: 研究机器人流程自动化调度问题1|prec,rj,dj|*的参数化复杂度，给出了不同参数下的复杂度结果。


<details>
  <summary>Details</summary>
Motivation: 受机器人流程自动化调度问题启发，研究单机器问题1|prec,rj,dj|*的参数化复杂度。

Method: 聚焦与RPA系统自然相关的参数，如链状优先级、不同处理时间数量和时间窗口结构来分析问题复杂度。

Result: 以链的数量为参数时问题是W[2]-难的；所有作业共享单个时间窗口长度时有多项式时间算法，处理时间、释放时间和截止时间链一致时是FPT，以前置关系宽度为参数时问题在XP中。

Conclusion: 明确了单机器问题1|prec,rj,dj|*在不同参数下的复杂度情况。

Abstract: This paper studies the growing domain of Robotic Process Automation (RPA) problems. Motivated by scheduling problems arising in RPA, we study the parameterized complexity of the single-machine problem $1|\text{prec},r_j,d_j|*$. We focus on parameters naturally linked to RPA systems, including chain-like precedences, the number of distinct processing times, and the structure of the time windows. We show that the problem is W[2]-hard parameterized by the number of chains, even with only two prescribed processing times and two distinct time-window lengths. This hardness remains even for distinct processing times and time windows under prec-consistent time windows. On the positive side, we obtain polynomial-time algorithm when all jobs share a single time-window length and FPT when the processing times, release times and deadlines are chain-uniform. We also show that the problem lies in XP when parameterized by the width of the precedence relation.

</details>


### [161] [Computing Maximal Repeating Subsequences in a String](https://arxiv.org/abs/2601.12200)
*Mingyang Gong,Adiesha Liyanage,Braeden Sopp,Binhai Zhu*

Main category: cs.DS

TL;DR: 本文研究在单个输入字符串中计算最大重复模式，改进了特定子序列的计算时间复杂度。


<details>
  <summary>Details</summary>
Motivation: 此前相关重复模式问题研究多针对两个或多个输入字符串，本文发起对单个输入字符串的相关研究。

Method: 未明确提及具体方法。

Result: 计算最大平方子序列时间复杂度从 $O(n^2)$ 提升到 $O(n\log n)$；计算最大 $k$ 重复子序列时间复杂度从 $O(n^{2k - 1})$ 改进到 $O(f(k)n\log n)$，有约束情况下同样适用。

Conclusion: 在单个输入字符串计算最大重复模式上，算法时间复杂度实现显著提升。

Abstract: In this paper we initiate the study of computing a maximal (not necessarily maximum) repeating pattern in a single input string, where the corresponding problems have been studied (e.g., a maximal common subsequence) only in two or more input strings by Hirota and Sakai starting 2019. Given an input string $S$ of length $n$, we can compute a maximal square subsequence of $S$ in $O(n\log n)$ time, greatly improving the $O(n^2)$ bound for computing the longest square subsequence of $S$. For a maximal $k$-repeating subsequence, our bound is $O(f(k)n\log n)$, where \(f(k)\) is a computable function such that $f(k) < k\cdot 4^k$. This greatly improves the $O(n^{2k-1})$ bound for computing a longest $k$-repeating subsequence of $S$, for $k\geq 3$. Both results hold for the constrained case, i.e., when the solution must contain a subsequence $X$ of $S$, though with higher running times.

</details>


### [162] [Analyzing Collection Strategies: A Computational Perspective on the Coupon Collector Problem](https://arxiv.org/abs/2601.12351)
*Hadas Abraham,Ido Feldman,Eitan Yaakobi*

Main category: cs.DS

TL;DR: 本文针对通用形式的优惠券收集问题提出三种算法，利用马尔可夫模型和动态规划解决计算问题并分析复杂度。


<details>
  <summary>Details</summary>
Motivation: 实际应用中优惠券收集问题的数值结果推导面临计算挑战，阻碍其在工程领域的应用。

Method: 提出三种算法，利用为应对计算挑战专门设计的马尔可夫模型和动态规划方法，分别在不同条件下计算收集过程的期望、方差和二阶矩，并分析时间复杂度。

Result: 三种算法可精确计算收集过程的期望和方差。第一个算法提供计算基础模型；第二个在均匀抽取分布下以多项式时间计算；第三个适用于任意抽取分布。

Conclusion: 所提出的算法能有效解决通用形式的优惠券收集问题计算挑战，可应用于实际工程领域。

Abstract: The Coupon Collector Problem (CCP) is a well-known combinatorial problem that seeks to estimate the number of random draws required to complete a collection of $n$ distinct coupon types. Various generalizations of this problem have been applied in numerous engineering domains. However, practical applications are often hindered by the computational challenges associated with deriving numerical results for moments and distributions. In this work, we present three algorithms for solving the most general form of the CCP, where coupons are collected under any arbitrary drawing probability, with the objective of obtaining $t$ copies of a subset of $k$ coupons from a total of $n$. The First algorithm provides the base model to compute the expectation, variance, and the second moment of the collection process. The second algorithm utilizes the construction of the base model and computes the same values in polynomial time with respect to $n$ under the uniform drawing distribution, and the third algorithm extends to any general drawing distribution. All algorithms leverage Markov models specifically designed to address computational challenges, ensuring exact computation of the expectation and variance of the collection process. Their implementation uses a dynamic programming approach that follows from the Markov models framework, and their time complexity is analyzed accordingly.

</details>


### [163] [Approximation Schemes for Sequential Hiring Problems](https://arxiv.org/abs/2601.12750)
*Danny Segev,Uri Stein*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The main contribution of this paper resides in providing novel algorithmic advances and analytical insights for the sequential hiring problem, a recently introduced dynamic optimization model where a firm adaptively fills a limited number of positions from a pool of applicants with known values and acceptance probabilities. While earlier research established a strong foundation -- notably an LP-based $(1 - \frac{e^{-k}k^k}{k!})$-approximation by Epstein and Ma (Operations Research, 2024) -- the attainability of superior approximation guarantees has remained a central open question.
  Our work addresses this challenge by establishing the first polynomial-time approximation scheme for sequential hiring, proposing an $O(n^{O(1)} \cdot T^{2^{\tilde{O}(1/ε^{2})}})$-time construction of semi-adaptive policies whose expected reward is within factor $1 - ε$ of optimal. To overcome the constant-factor optimality loss inherent to earlier literature, and to circumvent intrinsic representational barriers of adaptive policies, our approach is driven by the following innovations:
  -- The block-responsive paradigm: We introduce block-responsive policies, a new class of decision-making strategies, selecting ordered sets (blocks) of applicants rather than single individuals, while still allowing for internal reactivity.
  -- Adaptivity and efficiency: We prove that these policies can nearly match the performance of general adaptive policies while utilizing polynomially-sized decision trees.
  -- Efficient construction: By developing a recursive enumeration-based framework, we resolve the problematic ``few-positions'' regime, bypassing a fundamental hurdle that hindered previous approaches.

</details>


### [164] [Kd-tree Based Wasserstein Distance Approximation for High-Dimensional Data](https://arxiv.org/abs/2601.12975)
*Kanata Teshigawara,Keisho Oh,Ken Kobayashi,Kazuhide Nakata*

Main category: cs.DS

TL;DR: 提出基于kd - tree的Wasserstein距离近似方法kd - Flowtree，在高维数据检索任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: Wasserstein距离在检索应用中计算复杂度高，现有树型近似方法构建时间长且高维下近似精度差。

Method: 提出kd - Flowtree方法，使用kd - tree进行数据嵌入，给出其最近邻搜索精度的概率上界。

Result: 数值实验表明kd - Flowtree在真实数据检索任务中优于现有Wasserstein距离近似方法。

Conclusion: kd - Flowtree在高维数据中能保持良好近似精度，且构建快，可减少计算时间。

Abstract: The Wasserstein distance is a discrepancy measure between probability distributions, defined by an optimal transport problem. It has been used for various tasks such as retrieving similar items in high-dimensional images or text data. In retrieval applications, however, the Wasserstein distance is calculated repeatedly, and its cubic time complexity with respect to input size renders it unsuitable for large-scale datasets. Recently, tree-based approximation methods have been proposed to address this bottleneck. For example, the Flowtree algorithm computes transport on a quadtree and evaluates cost using the ground metric, and clustering-tree approaches have been reported to achieve high accuracy. However, these existing trees often incur significant construction time for preprocessing, and crucially, standard quadtrees cannot grow deep enough in high-dimensional spaces, resulting in poor approximation accuracy. In this paper, we propose kd-Flowtree, a kd-tree-based Wasserstein distance approximation method that uses a kd-tree for data embedding. Since kd-trees can grow sufficiently deep and adaptively even in high-dimensional cases, kd-Flowtree is capable of maintaining good approximation accuracy for such cases. In addition, kd-trees can be constructed quickly than quadtrees, which contributes to reducing the computation time required for nearest neighbor search, including preprocessing. We provide a probabilistic upper bound on the nearest-neighbor search accuracy of kd-Flowtree, and show that this bound is independent of the dataset size. In the numerical experiments, we demonstrated that kd-Flowtree outperformed the existing Wasserstein distance approximation methods for retrieval tasks with real-world data.

</details>


### [165] [Nemesis, an Escape Game in Graphs](https://arxiv.org/abs/2601.13841)
*Pierre Bergé,Antoine Dailly,Yan Gerard*

Main category: cs.DS

TL;DR: 本文定义图上的Nemesis逃脱游戏，分析其在不同图结构中的复杂度，还扩展到相关问题。


<details>
  <summary>Details</summary>
Motivation: 研究图上新型逃脱游戏的可解性和复杂度。

Method: 定义游戏规则，针对不同图结构（树、最大度为3的图、任意图、平面多重图等）分析求解时间和复杂度。

Result: 在树和最大度为3的图中可线性时间求解；任意图中Nemesis问题是PSPACE - 完全的，平面多重图上是NP - 难的；相关的Blizzard游戏可线性时间求解；相关的Cat Herding问题是PSPACE - 完全的；基于满二叉逃脱树找策略是NP - 完全的。

Conclusion: 明确了Nemesis游戏及其相关问题在不同图结构下的复杂度特性。

Abstract: We define a new escape game in graphs that we call Nemesis. The game is played on a graph having a subset of vertices labeled as exits and the goal of one of the two players, called the fugitive, is to reach one of these exit vertices. The second player, i.e. the fugitive adversary, is called the Nemesis. Her goal is to trap the fugitive in a connected component which does not contain any exit. At each round of the game, the fugitive moves from one vertex to an adjacent vertex. Then the Nemesis deletes one edge anywhere in the graph. The game ends when either the fugitive reached an exit or when he is in a connected component that does not contain any exit. In trees and graphs of maximum degree bounded by 3, Nemesis can be solved in linear time. We also show that a variant of the game called Blizzard where only edges adjacent to the position of the fugitive can be deleted also admits a linear time solution. For arbitrary graphs, we show that Nemesis is PSPACE-complete, and that it is NP-hard on planar multigraphs. We extend our results to the related Cat Herding problem, proving its PSPACE-completeness. We also prove that finding a strategy based on a full binary escape tree whose leaves are exists is NP-complete.

</details>


### [166] [Learning-Augmented Online TRP on a Line](https://arxiv.org/abs/2601.13494)
*Swapnil Guragain,Gokarna Sharma*

Main category: cs.DS

TL;DR: 研究在线旅行维修员问题在学习增强框架下的情况，给出新的竞争比上下界。


<details>
  <summary>Details</summary>
Motivation: 在学习增强框架下解决在线旅行维修员问题，利用机器学习预测服务请求，改善原模型性能。

Method: 先建立3-竞争比下界，设计确定性算法，分析完美预测和不完美预测情况。

Result: 完美预测时算法(2+√3)-竞争，不完美预测时算法min{3.732 + 4δ, 4}-竞争。

Conclusion: 这是学习增强框架下在线旅行维修员问题的首个研究结果。

Abstract: We study the online traveling repairperson problem on a line within the recently proposed learning-augmented framework, which provides predictions on the requests to be served via machine learning. In the original model (with no predictions), there is a stream of requests released over time along the line. The goal is to minimize the sum (or average) of the completion times of the requests. In the original model, the state-of-the-art competitive ratio lower bound is $1+\sqrt{2} > 2.414$ for any deterministic algorithm and the state-of-the-art competitive ratio upper bound is 4 for a deterministic algorithm. Our prediction model involves predicted positions, possibly error-prone, of each request in the stream known a priori but the arrival times of requests are not known until their arrival. We first establish a 3-competitive lower bound which extends to the original model. We then design a deterministic algorithm that is $(2+\sqrt{3})\approx 3.732$-competitive when predictions are perfect. With imperfect predictions (maximum error $δ> 0$), we show that our deterministic algorithm becomes $\min\{3.732+4δ,4\}$-competitive, knowing $δ$. To the best of our knowledge, these are the first results for online traveling repairperson problem in the learning-augmented framework.

</details>


### [167] [Zero-free regions and concentration inequalities for hypergraph colorings in the local lemma regime](https://arxiv.org/abs/2601.13796)
*Jingcheng Liu,Yixiao Yu*

Main category: cs.DS

TL;DR: 论文证明k - 均匀超图q - 着色在特定条件下配分函数存在无零点带，得到Berry - Esseen型不等式，扩展到“Fisher零点”研究及近似算法，还给出原子CSP的Chebyshev型不等式。


<details>
  <summary>Details</summary>
Motivation: 研究k - 均匀超图q - 着色配分函数性质、渐近分布以及近似算法等。

Method: 将[Liu, Wang, Yin, Yu, STOC 2025]工作扩展到一般约束满足问题，引入外部场定义配分函数，采用投影 - 提升方案。

Result: 得到配分函数无零点带，Berry - Esseen型不等式，确定性近似算法，原子CSP的Chebyshev型不等式。

Conclusion: 在特定条件下超图着色有良好性质，方法可用于相关问题研究和算法设计。

Abstract: We show that for $q$-colorings in $k$-uniform hypergraphs with maximum degree $Δ$, if $k\ge 50$ and $q\ge 700Δ^{\frac{5}{k-10}}$, there is a "Lee-Yang" zero-free strip around the interval $[0,1]$ of the partition function, which includes the special case of uniform enumeration of hypergraph colorings. As an immediate consequence, we obtain Berry-Esseen type inequalities for hypergraph $q$-colorings under such conditions, demonstrating the asymptotic normality for the size of any color class in a uniformly random coloring. Our framework also extends to the study of "Fisher zeros", leading to deterministic algorithms for approximating the partition function in the zero-free region.
  Our approach is based on extending the recent work of [Liu, Wang, Yin, Yu, STOC 2025] to general constraint satisfaction problems (CSP). We focus on partition functions defined for CSPs by introducing external fields to the variables. A key component in our approach is a projection-lifting scheme, which enables us to essentially lift information percolation type analysis for Markov chains from the real line to the complex plane. Last but not least, we also show a Chebyshev-type inequality under the sampling LLL condition for atomic CSPs.

</details>


### [168] [Efficient Parallel $(Δ+1)$-Edge-Coloring](https://arxiv.org/abs/2601.13822)
*Michael Elkin,Ariel Khuzman*

Main category: cs.DS

TL;DR: 研究PRAM模型下(Δ + 1)-边着色问题，指出前人分析错误，提出更快并行算法及时间与处理器数量的权衡。


<details>
  <summary>Details</summary>
Motivation: 解决(Δ + 1)-边着色问题，改进前人算法的时间和处理器使用效率。

Method: 设计新的并行算法，对前人算法分析纠错。

Result: 提出使用O(Δ^4·log^4 n)时间和O(m·Δ)处理器等不同复杂度的算法，还有更新(Δ + 1)-边着色的快速算法。

Conclusion: 新算法比前人算法更快且更简单，在该问题上取得进展。

Abstract: We study the $(Δ+1)$-edge-coloring problem in the parallel $\left(\mathrm{PRAM}\right)$ model of computation. The celebrated Vizing's theorem [Viz64] states that every simple graph $G = (V,E)$ can be properly $(Δ+1)$-edge-colored. In a seminal paper, Karloff and Shmoys [KS87] devised a parallel algorithm with time $O\left(Δ^5\cdot\log n\cdot\left(\log^3 n+Δ^2\right)\right)$ and $O(m\cdotΔ)$ processors. This result was improved by Liang et al. [LSH96] to time $O\left(Δ^{4.5}\cdot \log^3Δ\cdot \log n + Δ^4 \cdot\log^4 n\right)$ and $O\left(n\cdotΔ^{3} +n^2\right)$ processors. [LSH96] claimed $O\left(Δ^{3.5} \cdot\log^3Δ\cdot \log n + Δ^3\cdot \log^4 n\right)$ time, but we point out a flaw in their analysis, which once corrected, results in the above bound. We devise a faster parallel algorithm for this fundamental problem. Specifically, our algorithm uses $O\left(Δ^4\cdot \log^4 n\right)$ time and $O(m\cdot Δ)$ processors. Another variant of our algorithm requires $O\left(Δ^{4+o(1)}\cdot\log^2 n\right)$ time, and $O\left(m\cdotΔ\cdot\log n\cdot\log^δΔ\right)$ processors, for an arbitrarily small $δ>0$. We also devise a few other tradeoffs between the time and the number of processors, and devise an improved algorithm for graphs with small arboricity. On the way to these results, we also provide a very fast parallel algorithm for updating $(Δ+1)$-edge-coloring. Our algorithm for this problem is dramatically faster and simpler than the previous state-of-the-art algorithm (due to [LSH96]) for this problem.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [169] [Temporal Fair Division of Indivisible Goods with Scheduling](https://arxiv.org/abs/2601.12835)
*Kui Wang Choi,Minming LI*

Main category: cs.GT

TL;DR: 研究多轮物品分配的时间公平性，分析多种公平性概念，在不同设置下探讨可行性与不可行性。


<details>
  <summary>Details</summary>
Motivation: 标准设置中存在公平分配的不可能情况，需在受限设置下研究并引入调度扩展模型。

Method: 研究无调度和有调度两种时间公平分配设置，分析不同公平性概念在其中的情况。

Result: 无调度时，一般情况下常数因子α - TEFX不可能，但特定条件下可实现1/2 - 近似；有调度时，调度缓冲区大小至少为n/2可实现TEF1，而TEFX和TMMS仍难实现。

Conclusion: 严格时间公平存在固有困难，实现近似保证需权衡。

Abstract: We study temporal fair division, where agents receive goods over multiple rounds and cumulative fairness is required. We investigate Temporal Envy-Freeness Up to One Good (TEF1) and Up to Any Good (TEFX), its approximation $α$-TEFX, and Temporal Maximin Share (TMMS). Motivated by known impossibilities in standard settings, we consider the model in various restricted settings and extend it by introducing scheduling.
  Our main contributions draw the boundary between possibility and impossibility. First, regarding temporal fair division without scheduling, we prove that while constant-factor $α$-TEFX is impossible in general, a $1/2$-approximation is achievable for generalized binary valuations and identical days with two agents. Second, regarding temporal fair division with scheduling, we demonstrate that a scheduling buffer of size at least $n/2$ enables TEF1 for identical days. However, we establish that TEFX and TMMS remain largely impossible even with scheduling or restricted domains. These results highlight the inherent difficulty of strict temporal fairness and quantify the trade-offs required to achieve approximation guarantees.

</details>


### [170] [The Cost of EFX: Generalized-Mean Welfare and Complexity Dichotomies with Few Surplus Items](https://arxiv.org/abs/2601.12849)
*Eugene Lim,Tzeh Yuan Neoh,Nicholas Teh*

Main category: cs.GT

TL;DR: 研究在少量剩余物品场景下EFX与广义平均福利的关系，得出不同p值下的复杂度二分结果、公平价格损失情况，以及EFX与帕累托最优结合的复杂度。


<details>
  <summary>Details</summary>
Motivation: 在少量剩余物品场景下，EFX分配存在性已解决，需研究其与广义平均福利的相互作用，关注效率和计算问题。

Method: 理论分析，针对不同p值范围进行复杂度证明和算法设计。

Result: 在p=0处有复杂度二分；p>0时，公平价格损失随代理数量线性增长，p≤0时受剩余物品数限制；EFX与帕累托最优结合是NP难的。

Conclusion: 明确了在少量剩余物品场景下，EFX何时在计算上成本高，何时与福利最大化结构一致。

Abstract: Envy-freeness up to any good (EFX) is a central fairness notion for allocating indivisible goods, yet its existence is unresolved in general. In the setting with few surplus items, where the number of goods exceeds the number of agents by a small constant (at most three), EFX allocations are guaranteed to exist, shifting the focus from existence to efficiency and computation. We study how EFX interacts with generalized-mean ($p$-mean) welfare, which subsumes commonly-studied utilitarian ($p=1$), Nash ($p=0$), and egalitarian ($p \rightarrow -\infty$) objectives. We establish sharp complexity dichotomies at $p=0$: for any fixed $p \in (0,1]$, both deciding whether EFX can attain the global $p$-mean optimum and computing an EFX allocation maximizing $p$-mean welfare are NP-hard, even with at most three surplus goods; in contrast, for any fixed $p \leq 0$, we give polynomial-time algorithms that optimize $p$-mean welfare within the space of EFX allocations and efficiently certify when EFX attains the global optimum. We further quantify the welfare loss of enforcing EFX via the price of fairness framework, showing that for $p > 0$, the loss can grow linearly with the number of agents, whereas for $p \leq 0$, it is bounded by a constant depending on the surplus (and for Nash welfare it vanishes asymptotically). Finally we show that requiring Pareto-optimality alongside EFX is NP-hard (and becomes $Σ_2^P$-complete for a stronger variant of EFX). Overall, our results delineate when EFX is computationally costly versus structurally aligned with welfare maximization in the setting with few surplus items.

</details>


### [171] [The Cost of Failure: On The Complexity of Recampaigning under Fixed Districts](https://arxiv.org/abs/2601.13246)
*Michael C. Chavrimootoo,Aidan Jeansonne*

Main category: cs.GT

TL;DR: 本文聚焦选区重划问题，研究给定选区下一方政党能否策略性安排候选人获胜，将此建模并研究其复杂度。


<details>
  <summary>Details</summary>
Motivation: 以往对选区重划的研究主要关注确定“合适”选区，本文想了解“失败”政党的选择，即给定选区下政党能否策略性安排候选人获胜。

Method: 将重新竞选建模成计算问题，考虑模型自然变体，通过（1）多项式时间多一归约性、（2）分离/坍缩（无条件和公理充分）、（3）最坏情况和参数化复杂度的视角研究新模型。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: Redistricting efforts have gathered contemporary attention in both quotidian and scholarly debates, particularly in the United States where efforts to redraw congressional districts to favor either of the two major parties in 12 states -- such as California, Texas, and Ohio -- have captured the public eye. The treatment of redistricting in computational social choice has essentially focused on the process of determining "appropriate" districts. In this work, we are interested in understanding the gamut of options left for the "losing" party, and so we consider the flip side of the problem: Given fixed/predetermined districts, can a given party still make their candidates win by strategically placing them in certain districts? We dub this as "recampaigning" to capture the intuition that a party would redirect their campaigning efforts from one district to another. We model recampaigning as a computational problem, consider natural variations of the model, and study those new models through the lens of (1) (polynomial-time many-one) interreducibilities, (2) separations/collapses (both unconditional and axiomatic-sufficient), and (3) both worst-case and parametrized complexity.

</details>


### [172] [Tight Asymptotic Bounds for Fair Division With Externalities](https://arxiv.org/abs/2601.13287)
*Frank Connor,Max Dupré la Tour,Vishnu V. Narayan,Šimon Schierreich*

Main category: cs.GT

TL;DR: 研究带外部性偏好的物品分配问题，得出消除嫉妒所需物品数量的渐近界以及 EF - k 的最优松弛情况。


<details>
  <summary>Details</summary>
Motivation: 在带外部性的物品分配中，精确无嫉妒性无法保证，以往研究重点在其松弛概念，但 EF1 分配是否存在以及最优松弛 EF - k 为何的问题未解决。

Method: 推导消除嫉妒所需物品数量的紧渐近界。

Result: 对于 n 个代理人的实例，存在一种在 O(√n) 物品下无嫉妒的分配，能在多项式时间找到；证明了匹配的 Ω(√n) 下界，表明即使是二元估值结果也紧密。

Conclusion: 在代理人有外部性时排除了 EF1 分配的存在性，确定了 EF - k 的最优松弛为 O(√n)。

Abstract: We study the problem of allocating a set of indivisible items among agents whose preferences include externalities. Unlike the standard fair division model, agents may derive positive or negative utility not only from items allocated directly to them, but also from items allocated to other agents. Since exact envy-freeness cannot be guaranteed, prior work has focused on its relaxations. However, two central questions remained open: does there always exist an allocation that is envy-free up to one item (EF1), and if not, what is the optimal relaxation EF-$k$ that can always be attained?
  We settle both questions by deriving tight asymptotic bounds on the number of items sufficient to eliminate envy. We show that for any instance with $n$ agents, an allocation that is envy-free up to $O(\sqrt{n})$ items always exists and can be found in polynomial time, and we prove a matching $Ω(\sqrt{n})$ lower bound showing that this result is tight even for binary valuations, which rules out the existence of EF1 allocations when agents have externalities.

</details>


### [173] [Bridging the Gap Between Estimated and True Regret Towards Reliable Regret Estimation in Deep Learning based Mechanism Design](https://arxiv.org/abs/2601.13489)
*Shuyuan You,Zhiqiang Zhuang,Kewen Wang,Zhe Wang*

Main category: cs.GT

TL;DR: 现有深度学习近似多物品拍卖方法对后悔值估计不准确，本文推导下界、引入近似方法和细化程序改进估计。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习近似多物品拍卖的方法对后悔值估计的真实准确性不明，当前模型依赖梯度优化器且结果受超参数影响大。

Method: 推导后悔值下界，引入高效的按物品后悔值近似方法，提出引导细化程序。

Result: 现有方法系统性低估实际后悔值，本文方法提高了后悔值估计准确性，降低计算成本。

Conclusion: 本文方法为评估基于深度学习的拍卖机制的激励相容性提供更可靠基础，需重新评估该领域先前的性能宣称。

Abstract: Recent advances, such as RegretNet, ALGnet, RegretFormer and CITransNet, use deep learning to approximate optimal multi item auctions by relaxing incentive compatibility (IC) and measuring its violation via ex post regret. However, the true accuracy of these regret estimates remains unclear. Computing exact regret is computationally intractable, and current models rely on gradient based optimizers whose outcomes depend heavily on hyperparameter choices. Through extensive experiments, we reveal that existing methods systematically underestimate actual regret (In some models, the true regret is several hundred times larger than the reported regret), leading to overstated claims of IC and revenue. To address this issue, we derive a lower bound on regret and introduce an efficient item wise regret approximation. Building on this, we propose a guided refinement procedure that substantially improves regret estimation accuracy while reducing computational cost. Our method provides a more reliable foundation for evaluating incentive compatibility in deep learning based auction mechanisms and highlights the need to reassess prior performance claims in this area.

</details>


### [174] [Concurrent Permissive Strategy Templates](https://arxiv.org/abs/2601.13500)
*Ashwani Anand,Christel Baier,Calvin Chau,Sascha Klüppelholz,Ali Mirzaei,Satya Prakash Nayak,Anne-Kathrin Schmuck*

Main category: cs.GT

TL;DR: 本文引入并发策略模板（ConSTels）来表示并发游戏中随机获胜策略集，支持离线与在线调整，并通过实验展示其潜力。


<details>
  <summary>Details</summary>
Motivation: 并发游戏语义虽适用于网络物理系统（CPS），但在CPS设计中应用有限。

Method: 基于回合制游戏的许可策略模板（PeSTels）概念，引入ConSTels，离线利用组合性进行增量综合，在线根据对手行为调整动作概率。

Result: 实现了ConSTel合成与调整的原型工具。

Conclusion: ConSTels有助于在并发游戏中更有效管理策略，展现出其在实际应用中的潜力。

Abstract: Two-player games on finite graphs provide a rigorous foundation for modeling the strategic interaction between reactive systems and their environment. While concurrent game semantics naturally capture the synchronous interactions characteristic of many cyber-physical systems (CPS), their adoption in CPS design remains limited. Building on the concept of permissive strategy templates (PeSTels) for turn-based games, we introduce concurrent (permissive) strategy templates (ConSTels) -- a novel representation for sets of randomized winning strategies in concurrent games with Safety, Büchi, and Co-Büchi objectives. ConSTels compactly encode infinite families of strategies, thereby supporting both offline and online adaptation. Offline, we exploit compositionality to enable incremental synthesis: combining ConSTels for simpler objectives into non-conflicting templates for more complex combined objectives. Online, we demonstrate how ConSTels facilitate runtime adaptation, adjusting action probabilities in response to observed opponent behavior to optimize performance while preserving correctness. We implemented ConSTel synthesis and adaptation in a prototype tool and experimentally show its potential.

</details>


### [175] [Stochastic Dynamic Pricing of Electric Vehicle Charging with Heterogeneous User Behavior: A Stackelberg Game Framework](https://arxiv.org/abs/2601.13571)
*Yongqi Zhang,Dong Ngoduy,Li Duan,Mingchang Zhu,Zhuo Chen*

Main category: cs.GT

TL;DR: 本文提出随机、行为异质动态定价框架管理电动汽车充电需求，用滚动时域法求解，案例验证可减少排队惩罚并提高用户效用。


<details>
  <summary>Details</summary>
Motivation: 电动汽车迅速普及给充电站运营商带来复杂的时空需求管理挑战，传统动态定价模型存在简化用户行为和缺乏可扩展性的问题。

Method: 提出基于双层Stackelberg博弈的定价框架，上层优化定价，下层用MNL模型模拟用户选择，结合PSA - CEM和MSA的滚动时域法求解。

Result: 墨尔本克莱顿的案例显示，该框架比固定和分时定价机制大幅减少排队惩罚，提高用户效用。

Conclusion: 该框架是用于电动汽车充电管理的稳健、可扩展工具，能兼顾现实性和计算效率。

Abstract: The rapid adoption of electric vehicles (EVs) introduces complex spatiotemporal demand management challenges for charging station operators (CSOs), exacerbated by demand imbalances, behavioral heterogeneity, and system uncertainty. Traditional dynamic pricing models, often relying on deterministic EV-CS pairings and network equilibrium assumptions, frequently oversimplify user behavior and lack scalability. This study proposes a stochastic, behaviorally heterogeneous dynamic pricing framework formulated as a bi-level Stackelberg game. The upper level optimizes time-varying pricing to maximize system-wide utility, while the lower level models decentralized EV users via a multinomial logit (MNL) choice model incorporating price sensitivity, battery aging, risk attitudes, and network travel costs. Crucially, the model avoids network equilibrium constraints to enhance scalability, with congestion effects represented via queuing-theoretic approximations. To efficiently solve the resulting large-scale optimization problem, a rolling-horizon approach combining the Dynamic Probabilistic Sensitivity Analysis-guided Cross-Entropy Method (PSA-CEM) with the Method of Successive Averages (MSA) is implemented. A real-world case study in Clayton, Melbourne, validates the framework using 22 charging stations. Simulation results demonstrate that the proposed mechanism substantially reduces queuing penalties and improves user utility compared to fixed and time-of-use pricing. The framework provides a robust, scalable tool for strategic EV charging management, balancing realism with computational efficiency.

</details>


### [176] [Asymmetric regularization mechanism for GAN training with Variational Inequalities](https://arxiv.org/abs/2601.13920)
*Spyridon C. Giagtzoglou,Mark H. M. Winands,Barbara Franci*

Main category: cs.GT

TL;DR: 将GAN训练视为纳什均衡求解问题，提出非对称正则化机制稳定训练，理论保证方法收敛，实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 稳定生成对抗网络（GANs）的训练过程并找到纳什均衡。

Method: 提出基于经典Tikhonov步骤和新型零中心梯度惩罚的非对称正则化机制。

Result: 得到正则化算子的Lipschitz和（强）单调性常数，保证单调用EFTP方法的最后迭代线性收敛；学术示例实验表明非对称正则化能收敛到均衡并稳定轨迹。

Conclusion: 所提非对称正则化机制可有效稳定GANs训练并收敛到均衡。

Abstract: We formulate the training of generative adversarial networks (GANs) as a Nash equilibrium seeking problem. To stabilize the training process and find a Nash equilibrium, we propose an asymmetric regularization mechanism based on the classic Tikhonov step and on a novel zero-centered gradient penalty. Under smoothness and a local identifiability condition induced by a Gauss-Newton Gramian, we obtain explicit Lipschitz and (strong)-monotonicity constants for the regularized operator. These constants ensure last-iterate linear convergence of a single-call Extrapolation-from-the-Past (EFTP) method. Empirical simulations on an academic example show that, even when strong monotonicity cannot be achieved, the asymmetric regularization is enough to converge to an equilibrium and stabilize the trajectory.

</details>


### [177] [BallotRank: A Condorcet Completion Method for Graphs](https://arxiv.org/abs/2601.14015)
*Ismar Volic,Jason Douglas Todd*

Main category: cs.GT

TL;DR: 介绍BallotRank，一种基于改进PageRank算法的排序偏好聚合方法，经实证和理论证明有优势。


<details>
  <summary>Details</summary>
Motivation: 提出新的排序偏好聚合方法，满足社会选择标准并能提供完整候选人排名。

Method: 基于改进的PageRank算法得到BallotRank方法。

Result: 对近2000场排序选择选举和超20000个网络投票的实证表明BallotRank能识别Condorcet胜者；证明该方法满足很多社会选择标准。

Conclusion: BallotRank是一种优秀的排序偏好聚合方法，无阻尼且作为自然社会福利函数能提供候选人完整排名。

Abstract: We introduce BallotRank, a ranked preference aggregation method derived from a modified PageRank algorithm. It is a Condorcet-consistent method without damping, and empirical examination of nearly 2,000 ranked choice elections and over 20,000 internet polls confirms that BallotRank always identifies the Condorcet winner at conventional values of the damping parameter. We also prove that the method satisfies many of the same social choice criteria as other well-known Condorcet completion methods, but it has the advantage of being a natural social welfare function that provides a full ranking of the candidates.

</details>


### [178] [Collective intelligence in science: direct elicitation of diverse information from experts with unknown information structure](https://arxiv.org/abs/2601.14047)
*Alexey V. Osipov,Nikolay N. Osipov*

Main category: cs.GT

TL;DR: 提出基于虚拟货币预测市场和聊天结合的机制，可实现信息有效聚合和为大规模合作研究提供资助。


<details>
  <summary>Details</summary>
Motivation: 对开放科学问题进行深度集体分析，需聚合相互无关专家的多样且不可预测的私人信息。

Method: 提出一种基于自解决虚拟货币预测市场与聊天相纠缠的简单机制，按专家最终持有的虚拟货币比例给予真资产奖励。

Result: 系统能达到参与者直接分享信息和按假设真实情况交易的均衡，可有效聚合相关信息。

Conclusion: 该方法能实现信息有效聚合，还能创新地为大规模合作研究提供资助。

Abstract: Suppose we need a deep collective analysis of an open scientific problem: there is a complex scientific hypothesis and a large online group of mutually unrelated experts with relevant private information of a diverse and unpredictable nature. This information may be results of experts' individual experiments, original reasoning of some of them, results of AI systems they use, etc. We propose a simple mechanism based on a self-resolving play-money prediction market entangled with a chat. We show that such a system can easily be brought to an equilibrium where participants directly share their private information on the hypothesis through the chat and trade as if the market were resolved in accordance with the truth of the hypothesis. This approach will lead to efficient aggregation of relevant information in a completely interpretable form even if the ground truth cannot be established and experts initially know nothing about each other and cannot perform complex Bayesian calculations. Finally, by rewarding the experts with some real assets proportionally to the play money they end up with, we can get an innovative way to fund large-scale collaborative studies of any type.

</details>


### [179] [A Minimax Perspective on Almost-Stable Matchings](https://arxiv.org/abs/2601.14195)
*Frederik Glitzner,David Manlove*

Main category: cs.GT

TL;DR: 论文提出基于极小化极大原则的公平近似稳定方法，在不同匹配场景下分析其计算复杂度，虽多项任务是NP完全问题，但给出特殊情况多项式时间算法、近似算法和整数规划。


<details>
  <summary>Details</summary>
Motivation: 现有'近似稳定'匹配方法采用聚合度量，会导致少数个体承担过多不稳定性，缺乏公平性，需要新方法解决不稳定度量和分配问题。

Method: 引入基于极小化极大原则的公平近似稳定方法，最小化任何一个体所处的最大阻塞对数；分析不同匹配场景下该方法计算复杂度；给出特殊情况算法和整数规划。

Result: 决定是否存在没人处于超过一个阻塞对的匹配是NP完全问题，适用于稳定室友和最大基数稳定婚姻问题；当个体最多对两人排序时有多项式时间算法，还有近似算法和整数规划。

Conclusion: 研究勾勒了算法图景，揭示了分布保证与计算可行性间的基本权衡。

Abstract: Stability is crucial in matching markets, yet in many real-world settings - from hospital residency allocations to roommate assignments - full stability is either impossible to achieve or can come at the cost of leaving many agents unmatched. When stability cannot be achieved, algorithmicists and market designers face a critical question: how should instability be measured and distributed among participants? Existing approaches to "almost-stable" matchings focus on aggregate measures, minimising either the total number of blocking pairs or the count of agents involved in blocking pairs. However, such aggregate objectives can result in concentrated instability on a few individual agents, raising concerns about fairness and incentives to deviate. We introduce a fairness-oriented approach to approximate stability based on the minimax principle: we seek matchings that minimise the maximum number of blocking pairs any agent is in. Equivalently, we minimise the maximum number of agents that anyone has justified envy towards. This distributional objective protects the worst-off agents from a disproportionate amount of instability. We characterise the computational complexity of this notion across fundamental matching settings. Surprisingly, even very modest guarantees prove computationally intractable: we show that it is NP-complete to decide whether a matching exists in which no agent is in more than one blocking pair, even when preference lists have constant-bounded length. This hardness applies to both Stable Roommates and maximum-cardinality Stable Marriage. On the positive side, we provide polynomial-time algorithms when agents rank at most two others, and present approximation algorithms and integer programs. Our results map the algorithmic landscape and reveal fundamental trade-offs between distributional guarantees and computational feasibility.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [180] [DeepEvidence: Empowering Biomedical Discovery with Deep Knowledge Graph Research](https://arxiv.org/abs/2601.11560)
*Zifeng Wang,Zheng Chen,Ziwei Yang,Xuan Wang,Qiao Jin,Yifan Peng,Zhiyong Lu,Jimeng Sun*

Main category: cs.IR

TL;DR: 介绍了AI-agent框架DeepEvidence，可跨异构生物医学知识图谱进行深度研究，在多个医学阶段表现良好，凸显知识图谱驱动深度研究加速生物医学发现的潜力。


<details>
  <summary>Details</summary>
Motivation: 生物医学知识图谱在科学发现中难以整合利用，其结构差异、不断演化和有限的跨资源对齐限制了知识探索。

Method: 引入DeepEvidence框架，核心是协调两个互补代理（BFRS和DFRS），有内部证据图，还包括统一接口和执行沙箱。

Result: 在既定深度推理基准和生物医学发现的四个关键阶段中，DeepEvidence在系统探索和证据合成方面有显著提升。

Conclusion: 知识图谱驱动的深度研究有加速生物医学发现的潜力。

Abstract: Biomedical knowledge graphs (KGs) encode vast, heterogeneous information spanning literature, genes, pathways, drugs, diseases, and clinical trials, but leveraging them collectively for scientific discovery remains difficult. Their structural differences, continual evolution, and limited cross-resource alignment require substantial manual integration, limiting the depth and scale of knowledge exploration. We introduce DeepEvidence, an AI-agent framework designed to perform Deep Research across various heterogeneous biomedical KGs. Unlike generic Deep Research systems that rely primarily on internet-scale text, DeepEvidence incorporates specialized knowledge-graph tooling and coordinated exploration strategies to systematically bridge heterogeneous resources. At its core is an orchestrator that directs two complementary agents: Breadth-First ReSearch (BFRS) for broad, multi-graph entity search, and Depth-First ReSearch (DFRS) for multi-hop, evidence-focused reasoning. An internal, incrementally built evidence graph provides a structured record of retrieved entities, relations, and supporting evidence. To operate at scale, DeepEvidence includes unified interfaces for querying diverse biomedical APIs and an execution sandbox that enables programmatic data retrieval, extraction, and analysis. Across established deep-reasoning benchmarks and four key stages of the biomedical discovery lifecycle: drug discovery, pre-clinical experimentation, clinical trial development, and evidence-based medicine, DeepEvidence demonstrates substantial gains in systematic exploration and evidence synthesis. These results highlight the potential of knowledge-graph-driven Deep Research to accelerate biomedical discovery.

</details>


### [181] [Utilizing Metadata for Better Retrieval-Augmented Generation](https://arxiv.org/abs/2601.11863)
*Raquib Bin Yousuf,Shengzhe Xu,Mandar Sharma,Andrew Neeser,Chris Latimer,Naren Ramakrishnan*

Main category: cs.IR

TL;DR: 本文对元数据感知的检索策略进行系统研究，对比多种方法，发现前缀和统一嵌入法优于纯文本基线，还分析嵌入空间并公开相关资源。


<details>
  <summary>Details</summary>
Motivation: 在结构化重复语料中，仅靠块相似度难以区分文本，且将元数据扁平化为输入文本的影响和权衡尚不明确。

Method: 对比纯文本基线与直接嵌入元数据的方法，评估涵盖元数据作为文本（前缀和后缀）、双编码器统一嵌入、双编码器后期融合检索和元数据感知查询重构等。

Result: 前缀和统一嵌入法在多个检索指标和问题类型上始终优于纯文本基线，统一嵌入法有时表现更好且更易维护；元数据集成可提高检索有效性；字段级消融显示结构线索有强消歧信号。

Conclusion: 元数据感知的检索策略能提升检索效果，统一嵌入法具有优势。

Abstract: Retrieval-Augmented Generation systems depend on retrieving semantically relevant document chunks to support accurate, grounded outputs from large language models. In structured and repetitive corpora such as regulatory filings, chunk similarity alone often fails to distinguish between documents with overlapping language. Practitioners often flatten metadata into input text as a heuristic, but the impact and trade-offs of this practice remain poorly understood. We present a systematic study of metadata-aware retrieval strategies, comparing plain-text baselines with approaches that embed metadata directly. Our evaluation spans metadata-as-text (prefix and suffix), a dual-encoder unified embedding that fuses metadata and content in a single index, dual-encoder late-fusion retrieval, and metadata-aware query reformulation. Across multiple retrieval metrics and question types, we find that prefixing and unified embeddings consistently outperform plain-text baselines, with the unified at times exceeding prefixing while being easier to maintain. Beyond empirical comparisons, we analyze embedding space, showing that metadata integration improves effectiveness by increasing intra-document cohesion, reducing inter-document confusion, and widening the separation between relevant and irrelevant chunks. Field-level ablations show that structural cues provide strong disambiguating signals. Our code, evaluation framework, and the RAGMATE-10K dataset are publicly hosted.

</details>


### [182] [Cultural Analytics for Good: Building Inclusive Evaluation Frameworks for Historical IR](https://arxiv.org/abs/2601.11874)
*Suchana Datta,Dwaipayan Roy,Derek Greene,Gerardine Meaney,Karen Wade,Philipp Mayr*

Main category: cs.IR

TL;DR: 该研究结合信息检索与文化分析领域，用英国图书馆数字馆藏构建基准，提出跨学科评估框架提升检索效果并助力数字档案建设。


<details>
  <summary>Details</summary>
Motivation: 实现对历史知识的公平获取，探索小说对学术和事实材料检索的提升作用。

Method: 结合专家驱动的查询设计、段落级相关性标注和大语言模型辅助，构建基于人类专业知识的可扩展评估框架。

Result: 该跨学科框架提高了检索准确性，促进了数字档案的可解释性、透明度和文化包容性。

Conclusion: 为开发检索系统提供实用评估资源和方法论范式，推动更具解放性的知识基础设施建设。

Abstract: This work bridges the fields of information retrieval and cultural analytics to support equitable access to historical knowledge. Using the British Library BL19 digital collection (more than 35,000 works from 1700-1899), we construct a benchmark for studying changes in language, terminology and retrieval in the 19th-century fiction and non-fiction. Our approach combines expert-driven query design, paragraph-level relevance annotation, and Large Language Model (LLM) assistance to create a scalable evaluation framework grounded in human expertise. We focus on knowledge transfer from fiction to non-fiction, investigating how narrative understanding and semantic richness in fiction can improve retrieval for scholarly and factual materials. This interdisciplinary framework not only improves retrieval accuracy but also fosters interpretability, transparency, and cultural inclusivity in digital archives. Our work provides both practical evaluation resources and a methodological paradigm for developing retrieval systems that support richer, historically aware engagement with digital archives, ultimately working towards more emancipatory knowledge infrastructures.

</details>


### [183] [Agentic-R: Learning to Retrieve for Agentic Search](https://arxiv.org/abs/2601.11888)
*Wenhan Liu,Xinyu Ma,Yutao Zhu,Yuchen Li,Daiting Shi,Dawei Yin,Zhicheng Dou*

Main category: cs.IR

TL;DR: 提出适用于智能体搜索的新型检索器训练框架，在多个问答基准测试中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 当前智能体搜索中检索器设计研究不足，现有搜索智能体依赖的基于相似度的检索器，相似段落不一定对最终答案生成有用。

Method: 提出使用局部查询 - 段落相关性和全局答案正确性衡量段落效用，引入迭代训练策略，对搜索智能体和检索器进行双向迭代优化。

Result: 在七个单跳和多跳问答基准测试中，所提出的检索器始终优于强基线。

Conclusion: 提出的新型检索器训练框架有效，可提升智能体搜索性能 。

Abstract: Agentic search has recently emerged as a powerful paradigm, where an agent interleaves multi-step reasoning with on-demand retrieval to solve complex questions. Despite its success, how to design a retriever for agentic search remains largely underexplored. Existing search agents typically rely on similarity-based retrievers, while similar passages are not always useful for final answer generation. In this paper, we propose a novel retriever training framework tailored for agentic search. Unlike retrievers designed for single-turn retrieval-augmented generation (RAG) that only rely on local passage utility, we propose to use both local query-passage relevance and global answer correctness to measure passage utility in a multi-turn agentic search. We further introduce an iterative training strategy, where the search agent and the retriever are optimized bidirectionally and iteratively. Different from RAG retrievers that are only trained once with fixed questions, our retriever is continuously improved using evolving and higher-quality queries from the agent. Extensive experiments on seven single-hop and multi-hop QA benchmarks demonstrate that our retriever, termed \ours{}, consistently outperforms strong baselines across different search agents. Our codes are available at: https://github.com/8421BCD/Agentic-R.

</details>


### [184] [Facet-Aware Multi-Head Mixture-of-Experts Model with Text-Enhanced Pre-training for Sequential Recommendation](https://arxiv.org/abs/2601.12301)
*Mingrui Liu,Sixiao Zhang,Cheng Long*

Main category: cs.IR

TL;DR: 提出用于序列推荐的FAME架构，解决单嵌入向量难以捕捉物品多面特性和用户复杂偏好问题，还设计文本增强预训练模块。


<details>
  <summary>Details</summary>
Motivation: 现有序列推荐系统使用单嵌入向量表示物品，难以捕捉物品多面性质和用户复杂偏好，因此需要改进。

Method: 提出FAME架构，利用多头注意力层最后各头的子嵌入预测下一个物品，通过门控机制集成预测；在每个注意力头内引入混合专家网络，用可学习路由网络聚合输出；设计文本增强预训练模块，利用预训练文本编码器和交替监督对比学习目标。

Result: 文档未提及明确结果。

Conclusion: 文档未提及明确结论。

Abstract: Sequential recommendation (SR) systems excel at capturing users' dynamic preferences by leveraging their interaction histories. Most existing SR systems assign a single embedding vector to each item to represent its features, adopting various models to combine these embeddings into a sequence representation that captures user intent. However, we argue that this representation alone is insufficient to capture an item's multi-faceted nature (e.g., movie genres, starring actors). Furthermore, users often exhibit complex and varied preferences within these facets (e.g., liking both action and musical films within the genre facet), which are challenging to fully represent with static identifiers. To address these issues, we propose a novel architecture titled Facet-Aware Multi-Head Mixture-of-Experts Model for Sequential Recommendation (FAME). We leverage sub-embeddings from each head in the final multi-head attention layer to predict the next item separately, effectively capturing distinct item facets. A gating mechanism then integrates these predictions by dynamically determining their importance. Additionally, we introduce a Mixture-of-Experts (MoE) network within each attention head to disentangle varied user preferences within each facet, utilizing a learnable router network to aggregate expert outputs based on context. Complementing this architecture, we design a Text-Enhanced Facet-Aware Pre-training module to overcome the limitations of randomly initialized embeddings. By utilizing a pre-trained text encoder and employing an alternating supervised contrastive learning objective, we explicitly disentangle facet-specific features from textual metadata (e.g., descriptions) before sequential training begins. This ensures that the item embeddings are semantically robust and aligned with the downstream multi-facet framework.

</details>


### [185] [Information Farming: From Berry Picking to Berry Growing](https://arxiv.org/abs/2601.12544)
*Leif Azzopardi,Adam Roegiest*

Main category: cs.IR

TL;DR: 传统信息采集范式不再能适应生成式AI时代，本文提出信息耕种概念框架，分析其利弊并预测未来趋势。


<details>
  <summary>Details</summary>
Motivation: 传统信息采集范式无法适应生成式AI带来的人们生产、组织和复用信息方式的变革，需要新的概念框架。

Method: 借助历史类比和实证证据，探讨信息耕种的益处、机会、对设计和评估的影响及伴随风险。

Result: 无明确提及具体研究结果。

Conclusion: 假设随着生成式AI技术普及，信息耕种将逐渐取代临时性的信息采集，成为主要的信息交互模式，标志人类与信息交互研究的转变。

Abstract: The classic paradigms of Berry Picking and Information Foraging Theory have framed users as gatherers, opportunistically searching across distributed sources to satisfy evolving information needs. However, the rise of GenAI is driving a fundamental transformation in how people produce, structure, and reuse information - one that these paradigms no longer fully capture. This transformation is analogous to the Neolithic Revolution, when societies shifted from hunting and gathering to cultivation. Generative technologies empower users to "farm" information by planting seeds in the form of prompts, cultivating workflows over time, and harvesting richly structured, relevant yields within their own plots, rather than foraging across others people's patches. In this perspectives paper, we introduce the notion of Information Farming as a conceptual framework and argue that it represents a natural evolution in how people engage with information. Drawing on historical analogy and empirical evidence, we examine the benefits and opportunities of information farming, its implications for design and evaluation, and the accompanying risks posed by this transition. We hypothesize that as GenAI technologies proliferate, cultivating information will increasingly supplant transient, patch-based foraging as a dominant mode of engagement, marking a broader shift in human-information interaction and its study.

</details>


### [186] [HyFormer: Revisiting the Roles of Sequence Modeling and Feature Interaction in CTR Prediction](https://arxiv.org/abs/2601.12681)
*Yunwen Huang,Shiyong Hong,Xijun Xiao,Jinqiu Jin,Xuanyuan Luo,Zhe Wang,Zheng Chai,Shikang Wu,Yuchao Zheng,Jingjian Lin*

Main category: cs.IR

TL;DR: 现有工业大规模推荐模型架构存在局限，本文提出HyFormer统一混合变压器架构，实验和线上测试证明其有效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 工业大规模推荐模型在严格效率约束下面临联合建模长程用户行为序列和异构非序列特征的挑战，现有架构采用解耦管道，限制了表示能力和交互灵活性。

Method: 提出HyFormer架构，将长序列建模和特征交互集成到单个骨干网络，包括查询解码和查询增强两个核心组件，迭代执行以优化语义表示。

Result: 在十亿规模工业数据集上，HyFormer在可比参数和FLOPs预算下始终优于LONGER和RankMixer基线；大规模线上A/B测试显示其比现有模型有显著提升。

Conclusion: HyFormer作为工业大规模推荐模型的统一建模框架具有实用性和可扩展性。

Abstract: Industrial large-scale recommendation models (LRMs) face the challenge of jointly modeling long-range user behavior sequences and heterogeneous non-sequential features under strict efficiency constraints. However, most existing architectures employ a decoupled pipeline: long sequences are first compressed with a query-token based sequence compressor like LONGER, followed by fusion with dense features through token-mixing modules like RankMixer, which thereby limits both the representation capacity and the interaction flexibility. This paper presents HyFormer, a unified hybrid transformer architecture that tightly integrates long-sequence modeling and feature interaction into a single backbone. From the perspective of sequence modeling, we revisit and redesign query tokens in LRMs, and frame the LRM modeling task as an alternating optimization process that integrates two core components: Query Decoding which expands non-sequential features into Global Tokens and performs long sequence decoding over layer-wise key-value representations of long behavioral sequences; and Query Boosting which enhances cross-query and cross-sequence heterogeneous interactions via efficient token mixing. The two complementary mechanisms are performed iteratively to refine semantic representations across layers. Extensive experiments on billion-scale industrial datasets demonstrate that HyFormer consistently outperforms strong LONGER and RankMixer baselines under comparable parameter and FLOPs budgets, while exhibiting superior scaling behavior with increasing parameters and FLOPs. Large-scale online A/B tests in high-traffic production systems further validate its effectiveness, showing significant gains over deployed state-of-the-art models. These results highlight the practicality and scalability of HyFormer as a unified modeling framework for industrial LRMs.

</details>


### [187] [The Unfairness of Multifactorial Bias in Recommendation](https://arxiv.org/abs/2601.12828)
*Masoud Mansoury,Jin Huang,Mykola Pechenizkiy,Herke van Hoof,Maarten de Rijke*

Main category: cs.IR

TL;DR: 该论文研究推荐系统中流行度偏差和正性偏差结合产生的多因素偏差对物品侧公平性的影响，提出百分位评级转换预处理策略缓解该偏差，实验证明其可提升公平性且损失小，还能增强后处理管道效果。


<details>
  <summary>Details</summary>
Motivation: 已有研究多独立探讨流行度偏差和正性偏差，二者结合的多因素偏差对物品侧公平性影响待研究。

Method: 通过模拟研究多因素偏差影响，采用基于百分位的评级转换作为预处理策略缓解多因素偏差，并在四个公开数据集上用六种推荐算法实验。

Result: 正性偏差集中在流行物品上进一步放大过度曝光；预处理策略能提升曝光公平性且准确性损失小，加入后处理管道可增强其效果和效率并降低计算成本。

Conclusion: 处理多因素偏差很重要，基于数据的简单预处理方法对提升推荐系统公平性有实用价值。

Abstract: Popularity bias and positivity bias are two prominent sources of bias in recommender systems. Both arise from input data, propagate through recommendation models, and lead to unfair or suboptimal outcomes. Popularity bias occurs when a small subset of items receives most interactions, while positivity bias stems from the over-representation of high rating values. Although each bias has been studied independently, their combined effect, to which we refer to as multifactorial bias, remains underexplored. In this work, we examine how multifactorial bias influences item-side fairness, focusing on exposure bias, which reflects the unequal visibility of items in recommendation outputs. Through simulation studies, we find that positivity bias is disproportionately concentrated on popular items, further amplifying their over-exposure. Motivated by this insight, we adapt a percentile-based rating transformation as a pre-processing strategy to mitigate multifactorial bias. Experiments using six recommendation algorithms across four public datasets show that this approach improves exposure fairness with negligible accuracy loss. We also demonstrate that integrating this pre-processing step into post-processing fairness pipelines enhances their effectiveness and efficiency, enabling comparable or better fairness with reduced computational cost. These findings highlight the importance of addressing multifactorial bias and demonstrate the practical value of simple, data-driven pre-processing methods for improving fairness in recommender systems.

</details>


### [188] [Rules, Resources, and Restrictions: A Taxonomy of Task-Based Information Request Intents](https://arxiv.org/abs/2601.12985)
*Melanie A. Kilian,David Elsweiler*

Main category: cs.IR

TL;DR: 现有查询意图分类方法有局限，本文提出基于任务的查询意图分类法以弥合传统方法与新兴需求差距。


<details>
  <summary>Details</summary>
Motivation: 现有意图分类法多源于系统日志数据，未考虑更广泛任务上下文，且大语言模型难以处理复杂多面任务，需新方法。

Method: 基于对机场信息职员的扎根理论访谈研究。

Result: 提出了基于任务的信息请求意图分类法。

Conclusion: 基于任务的视角能弥合传统查询方法与新兴的面向任务搜索需求之间的差距。

Abstract: Understanding and classifying query intents can improve retrieval effectiveness by helping align search results with the motivations behind user queries. However, existing intent taxonomies are typically derived from system log data and capture mostly isolated information needs, while the broader task context often remains unaddressed. This limitation becomes increasingly relevant as interactions with Large Language Models (LLMs) expand user expectations from simple query answering toward comprehensive task support, for example, with purchasing decisions or in travel planning. At the same time, current LLMs still struggle to fully interpret complex and multifaceted tasks. To address this gap, we argue for a stronger task-based perspective on query intent. Drawing on a grounded-theory-based interview study with airport information clerks, we present a taxonomy of task-based information request intents that bridges the gap between traditional query-focused approaches and the emerging demands of AI-driven task-oriented search.

</details>


### [189] [Incorporating Q&A Nuggets into Retrieval-Augmented Generation](https://arxiv.org/abs/2601.13222)
*Laura Dietz,Bryan Li,Gabrielle Liu,Jia-Huei Ju,Eugene Yang,Dawn Lawrie,William Walden,James Mayfield*

Main category: cs.IR

TL;DR: 介绍将自动评估理念融入检索增强生成的RAGE系统，以Crucible为例，展示其构建问答块引导生成的方式，在TREC NeuCLIR 2024集合上表现优于Ginger。


<details>
  <summary>Details</summary>
Motivation: 将自动评估理念融入检索增强生成系统，提升系统表现。

Method: 构建Crucible系统，通过从检索文档构建问答块银行，利用问答块引导提取、选择和报告生成。

Result: 在TREC NeuCLIR 2024集合上，Crucible系统在问答块召回率、密度和引用依据方面大幅优于Ginger系统。

Conclusion: 所提出的Crucible系统在相关检索增强生成任务中有良好表现，基于问答块的方法有效。

Abstract: RAGE systems integrate ideas from automatic evaluation (E) into Retrieval-augmented Generation (RAG). As one such example, we present Crucible, a Nugget-Augmented Generation System that preserves explicit citation provenance by constructing a bank of Q&A nuggets from retrieved documents and uses them to guide extraction, selection, and report generation. Reasoning on nuggets avoids repeated information through clear and interpretable Q&A semantics - instead of opaque cluster abstractions - while maintaining citation provenance throughout the entire generation process. Evaluated on the TREC NeuCLIR 2024 collection, our Crucible system substantially outperforms Ginger, a recent nugget-based RAG system, in nugget recall, density, and citation grounding.

</details>


### [190] [Insider Knowledge: How Much Can RAG Systems Gain from Evaluation Secrets?](https://arxiv.org/abs/2601.13227)
*Laura Dietz,Bryan Li,Eugene Yang,Dawn Lawrie,William Walden,James Mayfield*

Main category: cs.IR

TL;DR: 研究基于金块的RAG系统使用LLM评判评估时因循环性导致测量错误的风险，强调盲评和方法多样性的重要性。


<details>
  <summary>Details</summary>
Motivation: 基于金块的方法融入RAG系统评估和架构虽有改进，但存在因循环性导致测量错误的风险，需进行研究。

Method: 对基于金块的RAG系统（如Ginger和Crucible）与强大基线（如GPT - Researcher）进行对比实验，故意修改Crucible以生成针对LLM评判优化的输出。

Result: 当评估元素（如提示模板或金块）泄露或可预测时，能获得近乎完美的评估分数。

Conclusion: 强调盲评设置和方法多样性对防止将指标过拟合误认为系统真正进步的重要性。

Abstract: RAG systems are increasingly evaluated and optimized using LLM judges, an approach that is rapidly becoming the dominant paradigm for system assessment. Nugget-based approaches in particular are now embedded not only in evaluation frameworks but also in the architectures of RAG systems themselves. While this integration can lead to genuine improvements, it also creates a risk of faulty measurements due to circularity. In this paper, we investigate this risk through comparative experiments with nugget-based RAG systems, including Ginger and Crucible, against strong baselines such as GPT-Researcher. By deliberately modifying Crucible to generate outputs optimized for an LLM judge, we show that near-perfect evaluation scores can be achieved when elements of the evaluation - such as prompt templates or gold nuggets - are leaked or can be predicted. Our results highlight the importance of blind evaluation settings and methodological diversity to guard against mistaking metric overfitting for genuine system progress.

</details>


### [191] [Guidelines for the Creation of an Annotated Corpus](https://arxiv.org/abs/2601.13353)
*Bahdja Boudoua,Nadia Guiffant,Mathieu Roche,Maguelonne Teisseire,Annelise Tran*

Main category: cs.IR

TL;DR: 本文基于反馈和文献给出创建注释指南和标注文本数据集的通用方法，涵盖多方面，提供综合框架。


<details>
  <summary>Details</summary>
Motivation: 为不同研究场景下创建和使用语料库提供支持。

Method: 依据UMR TETIS成员反馈和科学文献，制定通用方法论。

Result: 得到涵盖方法、数据存储、共享和增值等方面的通用方法论，有定义和示例说明。

Conclusion: 提供了综合框架以支持语料库在不同研究环境中的创建和使用。

Abstract: This document, based on feedback from UMR TETIS members and the scientific literature, provides a generic methodology for creating annotation guidelines and annotated textual datasets (corpora). It covers methodological aspects, as well as storage, sharing, and valorization of the data. It includes definitions and examples to clearly illustrate each step of the process, thus providing a comprehensive framework to support the creation and use of corpora in various research contexts.

</details>


### [192] [Integrating Vision-Centric Text Understanding for Conversational Recommender Systems](https://arxiv.org/abs/2601.13505)
*Wei Yuan,Shutong Qiao,Tong Chen,Quoc Viet Hung Nguyen,Zi Huang,Hongzhi Yin*

Main category: cs.IR

TL;DR: 本文提出STARCRS对话推荐系统，结合两种文本理解模式及知识锚定融合框架，实验证明其能提高推荐准确性和回复质量。


<details>
  <summary>Details</summary>
Motivation: 现有对话推荐系统在丰富对话上下文时引入输入长度限制、文本风格不一致和无关文本噪声等问题，需要更强的语言理解能力。

Method: 提出STARCRS，集成屏幕阅读和基于大语言模型的文本理解两种模式，设计知识锚定融合框架结合这两种模式。

Result: 在两个常用基准测试上，STARCRS持续提高了推荐准确性和生成回复质量。

Conclusion: STARCRS通过结合两种文本理解模式和知识锚定融合框架，有效解决现有对话推荐系统的问题。

Abstract: Conversational Recommender Systems (CRSs) have attracted growing attention for their ability to deliver personalized recommendations through natural language interactions. To more accurately infer user preferences from multi-turn conversations, recent works increasingly expand conversational context (e.g., by incorporating diverse entity information or retrieving related dialogues). While such context enrichment can assist preference modeling, it also introduces longer and more heterogeneous inputs, leading to practical issues such as input length constraints, text style inconsistency, and irrelevant textual noise, thereby raising the demand for stronger language understanding ability. In this paper, we propose STARCRS, a Screen-Text-AwaRe Conversational Recommender System that integrates two complementary text understanding modes: (1) a screen-reading pathway that encodes auxiliary textual information as visual tokens, mimicking skim reading on a screen, and (2) an LLM-based textual pathway that focuses on a limited set of critical content for fine-grained reasoning. We design a knowledge-anchored fusion framework that combines contrastive alignment, cross-attention interaction, and adaptive gating to integrate the two modes for improved preference modeling and response generation. Extensive experiments on two widely used benchmarks demonstrate that STARCRS consistently improves both recommendation accuracy and generated response quality.

</details>


### [193] [More Than Efficiency: Embedding Compression Improves Domain Adaptation in Dense Retrieval](https://arxiv.org/abs/2601.13525)
*Chunsheng Zuo,Daniel Khashabi*

Main category: cs.IR

TL;DR: 传统密集检索器在专业领域表现不佳，本文提出对领域嵌入应用PCA进行降维，仅对查询嵌入应用PCA能在多数模型 - 数据集对中提升检索性能。


<details>
  <summary>Details</summary>
Motivation: 密集检索器在专业领域因训练和目标领域分布不匹配而表现不佳，且传统领域自适应方法需高成本标注和重新训练。

Method: 对领域嵌入应用PCA，得到保留领域相关特征、丢弃非区分性成分的低维表示，仅对查询嵌入应用PCA。

Result: 在9个检索器和14个MTEB数据集上评估，仅对查询嵌入应用PCA使75.4%的模型 - 数据集对的NDCG@10得到提升。

Conclusion: 提出的嵌入压缩方法是一种简单轻量级的领域自适应方法，能有效提高检索性能。

Abstract: Dense retrievers powered by pretrained embeddings are widely used for document retrieval but struggle in specialized domains due to the mismatches between the training and target domain distributions. Domain adaptation typically requires costly annotation and retraining of query-document pairs. In this work, we revisit an overlooked alternative: applying PCA to domain embeddings to derive lower-dimensional representations that preserve domain-relevant features while discarding non-discriminative components. Though traditionally used for efficiency, we demonstrate that this simple embedding compression can effectively improve retrieval performance. Evaluated across 9 retrievers and 14 MTEB datasets, PCA applied solely to query embeddings improves NDCG@10 in 75.4% of model-dataset pairs, offering a simple and lightweight method for domain adaptation.

</details>


### [194] [Balancing Fairness and High Match Rates in Reciprocal Recommender Systems: A Nash Social Welfare Approach](https://arxiv.org/abs/2601.13609)
*Yoji Tomita,Tomohiko Yokoyama*

Main category: cs.IR

TL;DR: 本文研究匹配平台上的互惠推荐系统（RRSs）公平性，提出NSW和α - SW方法平衡公平性与匹配率，并开发近似算法，实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 匹配平台的成功需要设计能增加匹配数且避免用户间不公平的RRSs，因此要研究RRSs的公平性。

Method: 从公平分配角度定义用户推荐机会和无嫉妒公平概念；引入SW方法说明公平性与匹配率的权衡；提出NSW方法实现近无嫉妒推荐；推广到α - SW方法平衡公平性与匹配率；基于Sinkhorn算法开发近似算法。

Result: 在合成数据集和两个真实数据集上的大量实验证明了方法的实际有效性。

Conclusion: 所提方法能在匹配平台的RRSs中有效平衡公平性和高匹配率。

Abstract: Matching platforms, such as online dating services and job recommendations, have become increasingly prevalent. For the success of these platforms, it is crucial to design reciprocal recommender systems (RRSs) that not only increase the total number of matches but also avoid creating unfairness among users. In this paper, we investigate the fairness of RRSs on matching platforms. From the perspective of fair division, we define the users' opportunities to be recommended and establish the fairness concept of envy-freeness in the allocation of these opportunities. We first introduce the Social Welfare (SW) method, which approximately maximizes the number of matches, and show that it leads to significant unfairness in recommendation opportunities, illustrating the trade-off between fairness and match rates. To address this challenge, we propose the Nash Social Welfare (NSW) method, which alternately optimizes two NSW functions and achieves nearly envy-free recommendations. We further generalize the SW and NSW method to the $α$-SW method, which balances the trade-off between fairness and high match rates. Additionally, we develop a computationally efficient approximation algorithm for the SW/NSW/$α$-SW methods based on the Sinkhorn algorithm. Through extensive experiments on both synthetic datasets and two real-world datasets, we demonstrate the practical effectiveness of our approach.

</details>


### [195] [Question-Focused Filtering for Knowledge-based VQA](https://arxiv.org/abs/2601.13856)
*Wei Ye,Yixin Su,Yueguo Chen,Longxiang Gao,Jianjun Li,Ruixuan Li,Rui Zhang*

Main category: cs.IR

TL;DR: 本文提出基于问题聚焦的过滤方法用于知识型视觉问答，能降低信息选择错误，实验证明其有效性，代码开源。


<details>
  <summary>Details</summary>
Motivation: 典型过滤方法存在信息选择错误，基于MLLM的过滤方法计算成本高，需要改进知识过滤方法。

Method: 提出问题聚焦的过滤方法，设计可训练的问题聚焦过滤器（QFF）和基于块的动态多文章选择（CDA）模块。

Result: 该方法在E - VQA上比现有最先进模型高4.9%，在InfoSeek上高3.8%。

Conclusion: 所提方法能有效缓解文章和文章内层面的信息选择错误，验证了其有效性。

Abstract: Knowledge-based Visual Question Answering (KB-VQA) aims to answer questions by integrating images with external knowledge. Effective knowledge filtering is crucial for improving accuracy. Typical filtering methods use similarity metrics to locate relevant article sections from one article, leading to information selection errors at the article and intra-article levels. Although recent explorations of Multimodal Large Language Model (MLLM)-based filtering methods demonstrate superior semantic understanding and cross-article filtering capabilities, their high computational cost limits practical application. To address these issues, this paper proposes a question-focused filtering method. This approach can perform question-focused, cross-article filtering, efficiently obtaining high-quality filtered knowledge while keeping computational costs comparable to typical methods. Specifically, we design a trainable Question-Focused Filter (QFF) and a Chunk-based Dynamic Multi-Article Selection (CDA) module, which collectively alleviate information selection errors at both the article and intra-article levels. Experiments show that our method outperforms current state-of-the-art models by 4.9% on E-VQA and 3.8% on InfoSeek, validating its effectiveness. The code is publicly available at: https://github.com/leaffeall/QKVQA.

</details>


### [196] [IF-GEO: Conflict-Aware Instruction Fusion for Multi-Query Generative Engine Optimization](https://arxiv.org/abs/2601.13938)
*Heyang Zhou,JiaJia Chen,Xiaolu Chen,Jie Bao,Zhen Chen,Yong Liao*

Main category: cs.IR

TL;DR: 提出IF - GEO框架解决生成引擎优化中多查询优化的挑战，实验证明其有效且稳健。


<details>
  <summary>Details</summary>
Motivation: 生成引擎在信息检索中需确保源可见性，优化文档以适配不同查询存在受限优化挑战，当前缺少有效解决方案。

Method: 提出IF - GEO框架，分“分散 - 聚合”两阶段：从代表性潜在查询挖掘优化偏好，通过冲突感知指令融合协调偏好合成全局修订蓝图，还引入风险感知稳定性指标。

Result: 在多查询基准测试中，IF - GEO实现了显著的性能提升，并在不同检索场景下保持稳健性。

Conclusion: IF - GEO能解决多查询优化挑战，在生成引擎优化方面表现良好。

Abstract: As Generative Engines revolutionize information retrieval by synthesizing direct answers from retrieved sources, ensuring source visibility becomes a significant challenge. Improving it through targeted content revisions is a practical strategy termed Generative Engine Optimization (GEO). However, optimizing a document for diverse queries presents a constrained optimization challenge where heterogeneous queries often impose conflicting and competing revision requirements under a limited content budget. To address this challenge, we propose IF-GEO, a "diverge-then-converge" framework comprising two phases: (i) mining distinct optimization preferences from representative latent queries; (ii) synthesizing a Global Revision Blueprint for guided editing by coordinating preferences via conflict-aware instruction fusion. To explicitly quantify IF-GEO's objective of cross-query stability, we introduce risk-aware stability metrics. Experiments on multi-query benchmarks demonstrate that IF-GEO achieves substantial performance gains while maintaining robustness across diverse retrieval scenarios.

</details>


### [197] [Auditory Brain Passage Retrieval: Cross-Sensory EEG Training for Neural Information Retrieval](https://arxiv.org/abs/2601.14001)
*Niall McGuire,Yashar Moshfeghi*

Main category: cs.IR

TL;DR: 文章系统性研究听觉EEG用于脑段落检索，用双编码器结构对比不同训练方式，结果表明听觉EEG表现优于视觉，跨感官训练有显著提升，验证了听觉神经接口用于信息检索的可行性。


<details>
  <summary>Details</summary>
Motivation: 现有脑段落检索研究仅使用视觉刺激，而文章旨在探究听觉EEG是否能用于语音接口和视障用户的有效检索，以及跨感官训练能否改善数据稀缺问题。

Method: 采用双编码器架构和四种池化策略，在Alice（听觉）和Nieuwland（视觉）数据集上进行听觉、视觉和组合训练的对比实验。

Result: 听觉EEG始终优于视觉EEG，使用CLS池化的跨感官训练比单独训练有大幅提升，组合听觉EEG模型超越BM25文本基线。

Conclusion: 验证了听觉神经接口用于信息检索任务的可行性，跨感官训练能解决数据稀缺问题且优于单模态方法。

Abstract: Query formulation from internal information needs remains fundamentally challenging across all Information Retrieval paradigms due to cognitive complexity and physical impairments. Brain Passage Retrieval (BPR) addresses this by directly mapping EEG signals to passage representations without intermediate text translation. However, existing BPR research exclusively uses visual stimuli, leaving critical questions unanswered: Can auditory EEG enable effective retrieval for voice-based interfaces and visually impaired users? Can training on combined EEG datasets from different sensory modalities improve performance despite severe data scarcity? We present the first systematic investigation of auditory EEG for BPR and evaluate cross-sensory training benefits. Using dual encoder architectures with four pooling strategies (CLS, mean, max, multi-vector), we conduct controlled experiments comparing auditory-only, visual-only, and combined training on the Alice (auditory) and Nieuwland (visual) datasets. Results demonstrate that auditory EEG consistently outperforms visual EEG, and cross-sensory training with CLS pooling achieves substantial improvements over individual training: 31% in MRR (0.474), 43% in Hit@1 (0.314), and 28% in Hit@10 (0.858). Critically, combined auditory EEG models surpass BM25 text baselines (MRR: 0.474 vs 0.428), establishing neural queries as competitive with traditional retrieval whilst enabling accessible interfaces. These findings validate auditory neural interfaces for IR tasks and demonstrate that cross-sensory training addresses data scarcity whilst outperforming single-modality approaches Code: https://github.com/NiallMcguire/Audio_BPR

</details>


### [198] [Rerank Before You Reason: Analyzing Reranking Tradeoffs through Effective Token Cost in Deep Search Agents](https://arxiv.org/abs/2601.14224)
*Sahel Sharifymoghaddam,Jimmy Lin*

Main category: cs.IR

TL;DR: 研究深度搜索管道中推理预算分配，发现列表重排能改善检索和端到端准确性，适度重排性价比高。


<details>
  <summary>Details</summary>
Motivation: 深度研究代理在测试时计算扩展面临效率问题，需研究推理预算分配。

Method: 使用BrowseComp - Plus基准，通过有效代币成本（ETC）指标分析模型规模、推理工作量、重排深度和总代币成本间的权衡。

Result: 重排持续改善检索和端到端准确性，适度重排常比增加搜索时推理带来更大收益，能以更低成本实现相近准确性。

Conclusion: 列表重排在深度搜索管道的推理预算分配中有优势，可在保证准确性的同时降低成本。

Abstract: Deep research agents rely on iterative retrieval and reasoning to answer complex queries, but scaling test-time computation raises significant efficiency concerns. We study how to allocate reasoning budget in deep search pipelines, focusing on the role of listwise reranking. Using the BrowseComp-Plus benchmark, we analyze tradeoffs between model scale, reasoning effort, reranking depth, and total token cost via a novel effective token cost (ETC) metric. Our results show that reranking consistently improves retrieval and end-to-end accuracy, and that moderate reranking often yields larger gains than increasing search-time reasoning, achieving comparable accuracy at substantially lower cost. All our code is available at https://github.com/texttron/BrowseComp-Plus.git

</details>


### [199] [XR: Cross-Modal Agents for Composed Image Retrieval](https://arxiv.org/abs/2601.14245)
*Zhongyu Yang,Wei Pang,Yingfang Yuan*

Main category: cs.IR

TL;DR: 本文提出无训练多智能体框架XR用于组合图像检索，通过多智能体协作迭代优化检索，在多个数据集上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 传统基于嵌入的组合图像检索方法视角狭窄，捕捉跨模态线索有限且缺乏语义推理，需要新方法解决。

Method: 引入无训练多智能体框架XR，包含想象智能体、相似智能体和问题智能体，通过多智能体逐步协调迭代优化检索。

Result: 在FashionIQ、CIRR和CIRCO数据集上比强无训练和有训练基线最多提升38%，消融实验表明各智能体都必不可少。

Conclusion: XR框架能有效解决组合图像检索问题，提升检索性能。

Abstract: Retrieval is being redefined by agentic AI, demanding multimodal reasoning beyond conventional similarity-based paradigms. Composed Image Retrieval (CIR) exemplifies this shift as each query combines a reference image with textual modifications, requiring compositional understanding across modalities. While embedding-based CIR methods have achieved progress, they remain narrow in perspective, capturing limited cross-modal cues and lacking semantic reasoning. To address these limitations, we introduce XR, a training-free multi-agent framework that reframes retrieval as a progressively coordinated reasoning process. It orchestrates three specialized types of agents: imagination agents synthesize target representations through cross-modal generation, similarity agents perform coarse filtering via hybrid matching, and question agents verify factual consistency through targeted reasoning for fine filtering. Through progressive multi-agent coordination, XR iteratively refines retrieval to meet both semantic and visual query constraints, achieving up to a 38% gain over strong training-free and training-based baselines on FashionIQ, CIRR, and CIRCO, while ablations show each agent is essential. Code is available: https://01yzzyu.github.io/xr.github.io/.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [200] [CSyMR: Benchmarking Compositional Symbolic Muisc Reasoning With MIR Tool Integration](https://arxiv.org/abs/2601.11556)
*Boyang Wang,Yash Vishe,Xin Xu,Zachary Novack,Julian McAuley,Junda Wu*

Main category: cs.LG

TL;DR: 提出用于符号音乐推理的CSyMR - Bench基准及工具增强代理框架，实验显示该基准有挑战性，工具增强代理表现优于基线，准确率提升5 - 7%。


<details>
  <summary>Details</summary>
Motivation: 现有符号音乐推理基准缺乏对音乐结构连接的综合推理能力，需要新的基准。

Method: 创建包含126个问题的CSyMR - Bench数据集，引入利用music21库的工具增强代理框架。

Result: CSyMR - Bench对社区和考试风格问题都有挑战性，工具增强代理始终优于所有基线，准确率绝对提升5 - 7%。

Conclusion: CSyMR - Bench可有效评估符号音乐推理的综合能力，工具增强代理框架在该基准上表现良好。

Abstract: Large Language Models (LLMs) are leveraged in symbolic music reasoning, yet existing benchmarks emphasize isolated knowledge or atomic analyses rather than the integrative compositional reasoning needed to connect musical structures. To address this, we present the Compositional Symbolic Music Reasoning Benchmark (CSyMR-Bench), a curated multiple-choice dataset of 126 questions derived from expert forums and professional examinations. Each item involves combining several atomic analyses to arrive at the final answer. Furthermore, we introduce a tool-augmented agent framework that leverages symbolic music analysis tools from the music21 library to address the challenges posed by CSyMR-Bench. Experiments validate that CSyMR-Bench poses a non-trivial challenge across both community-sourced and exam-style questions, while our tool-augmented agent consistently outperforms all baselines, achieving 5-7% absolute accuracy gains.

</details>


### [201] [Knowledge-Integrated Representation Learning for Crypto Anomaly Detection under Extreme Label Scarcity; Relational Domain-Logic Integration with Retrieval-Grounded Context and Path-Level Explanations](https://arxiv.org/abs/2601.12839)
*Gyuyeon Na,Minjung Park,Soyoun Kim,Jungbin Shin,Sangmi Chai*

Main category: cs.LG

TL;DR: 提出RDLI框架结合领域逻辑和上下文信息，在极端标签稀缺下检测加密网络异常轨迹，F1得分超基线，且解释性更好。


<details>
  <summary>Details</summary>
Motivation: 现有GNN在检测复杂洗钱模式时受限，且极端标签稀缺和市场波动影响异常轨迹检测。

Method: 提出RDLI框架嵌入专家启发式信息，引入RGC模块结合监管和宏观经济上下文。

Result: 在0.01%标签稀缺下F1得分超基线28.9%，用户研究表明路径解释性更好。

Conclusion: 整合领域逻辑和上下文对提高检测准确性和解释性很重要。

Abstract: Detecting anomalous trajectories in decentralized crypto networks is fundamentally challenged by extreme label scarcity and the adaptive evasion strategies of illicit actors. While Graph Neural Networks (GNNs) effectively capture local structural patterns, they struggle to internalize multi hop, logic driven motifs such as fund dispersal and layering that characterize sophisticated money laundering, limiting their forensic accountability under regulations like the FATF Travel Rule. To address this limitation, we propose Relational Domain Logic Integration (RDLI), a framework that embeds expert derived heuristics as differentiable, logic aware latent signals within representation learning. Unlike static rule based approaches, RDLI enables the detection of complex transactional flows that evade standard message passing. To further account for market volatility, we incorporate a Retrieval Grounded Context (RGC) module that conditions anomaly scoring on regulatory and macroeconomic context, mitigating false positives caused by benign regime shifts. Under extreme label scarcity (0.01%), RDLI outperforms state of the art GNN baselines by 28.9% in F1 score. A micro expert user study further confirms that RDLI path level explanations significantly improve trustworthiness, perceived usefulness, and clarity compared to existing methods, highlighting the importance of integrating domain logic with contextual grounding for both accuracy and explainability.

</details>


### [202] [AdaFRUGAL: Adaptive Memory-Efficient Training with Dynamic Control](https://arxiv.org/abs/2601.11568)
*Quang-Hung Bui,Anh Son Ta*

Main category: cs.LG

TL;DR: 提出AdaFRUGAL框架自动化调参，在多实验中减少GPU内存和训练时间，性能有竞争力。


<details>
  <summary>Details</summary>
Motivation: FRUGAL框架静态超参数需手动调参，适应性受限，需自动化方法。

Method: 引入线性衰减控制$ρ$和损失感知调度控制$T$。

Result: 在大规模预训练和微调实验中，相比AdamW和静态FRUGAL，能显著减少GPU内存和训练时间，性能有竞争力。

Conclusion: AdaFRUGAL是资源受限大语言模型训练更实用、自主的解决方案。

Abstract: Training Large Language Models (LLMs) is highly memory-intensive due to optimizer state overhead. The FRUGAL framework mitigates this with gradient splitting, but its static hyperparameters -- the subspace ratio ($ρ$) and update frequency ($T$) -- require costly manual tuning, limiting adaptability. We present AdaFRUGAL, which automates this process by introducing two dynamic controls: (i) a linear decay for $ρ$ to progressively reduce memory, and (ii) a loss-aware schedule for $T$ to lower computational overhead. Experiments across large-scale pre-training (English C4, Vietnamese VietVault) and fine-tuning (GLUE) demonstrate that AdaFRUGAL achieves a compelling trade-off. It maintains competitive performance against AdamW and static FRUGAL while significantly reducing both GPU memory and training time, offering a more practical, autonomous solution for resource-constrained LLM training.

</details>


### [203] [Wavelet-Aware Anomaly Detection in Multi-Channel User Logs via Deviation Modulation and Resolution-Adaptive Attention](https://arxiv.org/abs/2601.12231)
*Kaichuan Kong,Dongjie Liu,Xiaobo Jin,Shijie Xu,Guanggang Geng*

Main category: cs.LG

TL;DR: 提出一种集成小波感知调制、多分辨率小波分解和分辨率自适应注意力的框架用于企业内部威胁检测，在CERT r4.2基准测试中表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 企业内部威胁检测依赖用户活动日志，但日志多通道、非平稳且异常稀少，现有异常检测方法存在挑战。

Method: 采用偏差感知调制方案抑制常规行为并放大异常偏差，用离散小波变换将日志信号分解为多分辨率表示，最后用可学习的注意力机制对最具区分性的频带进行动态重新加权。

Result: 在CERT r4.2基准测试中，在不同时间粒度和场景下，该方法在精确率、召回率和F1分数上均持续优于现有基线。

Conclusion: 该新颖框架在企业内部威胁检测的异常检测方面表现良好，是一种有效的解决方案。

Abstract: Insider threat detection is a key challenge in enterprise security, relying on user activity logs that capture rich and complex behavioral patterns. These logs are often multi-channel, non-stationary, and anomalies are rare, making anomaly detection challenging. To address these issues, we propose a novel framework that integrates wavelet-aware modulation, multi-resolution wavelet decomposition, and resolution-adaptive attention for robust anomaly detection. Our approach first applies a deviation-aware modulation scheme to suppress routine behaviors while amplifying anomalous deviations. Next, discrete wavelet transform (DWT) decomposes the log signals into multi-resolution representations, capturing both long-term trends and short-term anomalies. Finally, a learnable attention mechanism dynamically reweights the most discriminative frequency bands for detection. On the CERT r4.2 benchmark, our approach consistently outperforms existing baselines in precision, recall, and F1 score across various time granularities and scenarios.

</details>


### [204] [Discrete Semantic States and Hamiltonian Dynamics in LLM Embedding Spaces](https://arxiv.org/abs/2601.11572)
*Timo Aukusti Laine*

Main category: cs.LG

TL;DR: 运用数学概念研究大语言模型嵌入空间结构，为理解大模型和减少幻觉提供新思路。


<details>
  <summary>Details</summary>
Motivation: 观察到LLM嵌入有不同状态，暗示离散语义表示，以此为动机用数学工具分析语义关系。

Method: 使用线性代数和哈密顿形式主义，借鉴量子力学系统类比，分析L2归一化约束下的嵌入空间。

Result: 推导余弦相似度与嵌入向量扰动关系，探索语义转换，得出类似零点能量的概念，探讨与Koopman - von Neumann力学的联系。

Conclusion: 此方法为深入理解LLM和减少幻觉提供了有前景的途径。

Abstract: We investigate the structure of Large Language Model (LLM) embedding spaces using mathematical concepts, particularly linear algebra and the Hamiltonian formalism, drawing inspiration from analogies with quantum mechanical systems. Motivated by the observation that LLM embeddings exhibit distinct states, suggesting discrete semantic representations, we explore the application of these mathematical tools to analyze semantic relationships. We demonstrate that the L2 normalization constraint, a characteristic of many LLM architectures, results in a structured embedding space suitable for analysis using a Hamiltonian formalism. We derive relationships between cosine similarity and perturbations of embedding vectors, and explore direct and indirect semantic transitions. Furthermore, we explore a quantum-inspired perspective, deriving an analogue of zero-point energy and discussing potential connections to Koopman-von Neumann mechanics. While the interpretation warrants careful consideration, our results suggest that this approach offers a promising avenue for gaining deeper insights into LLMs and potentially informing new methods for mitigating hallucinations.

</details>


### [205] [GRADE: Replacing Policy Gradients with Backpropagation for LLM Alignment](https://arxiv.org/abs/2601.11574)
*Lukas Abrie Nel*

Main category: cs.LG

TL;DR: 本文提出GRADE方法用于大语言模型对齐，替代高方差的策略梯度估计，在情感控制文本生成任务上表现优于PPO和REINFORCE，更简单、稳定、有效。


<details>
  <summary>Details</summary>
Motivation: 当前基于人类反馈的强化学习（RLHF）中，策略梯度方法如PPO存在高方差梯度估计问题，需要仔细调优超参数和大量计算资源。

Method: 引入GRADE方法，通过对离散令牌采样过程的可微松弛进行直接反向传播，替代高方差策略梯度估计；使用Gumbel - Softmax重参数化和直通估计（GRADE - STE）实现端到端梯度流。

Result: 在IMDB数据集的情感控制文本生成任务中，GRADE - STE测试奖励达0.763 ± 0.344，相对PPO提升50%；梯度方差比REINFORCE低超14倍，训练动态更稳定；在保留数据上泛化能力最佳。

Conclusion: GRADE为大语言模型对齐提供了比强化学习更简单、稳定和有效的替代方案。

Abstract: Reinforcement learning from human feedback (RLHF) has become the dominant paradigm for aligning large language models with human preferences. However, policy gradient methods such as PPO suffer from high variance gradient estimates, requiring careful hyperparameter tuning and extensive computational resources. We introduce GRADE (Gumbel-softmax Relaxation for Alignment via Differentiable Estimation), a method that replaces high-variance policy gradient estimation with direct backpropagation through a differentiable relaxation of the discrete token sampling process. Using the Gumbel-Softmax reparameterization with straight-through estimation (GRADE-STE), we enable end-to-end gradient flow from reward signals through generated tokens to model parameters. On sentiment-controlled text generation using the IMDB dataset, GRADE-STE achieves a test reward of 0.763 +- 0.344 compared to PPO's 0.510 +- 0.313 and REINFORCE's 0.617 +- 0.378, representing a 50% relative improvement over PPO. Critically, GRADE-STE exhibits gradient variance over 14 times lower than REINFORCE and maintains stable training dynamics throughout optimization. Our rigorous evaluation with proper train/validation/test splits demonstrates that these improvements generalize to held-out data, with GRADE-STE showing the best generalization characteristics among all methods tested. GRADE offers a simpler, more stable, and more effective alternative to reinforcement learning for LLM alignment.

</details>


### [206] [Multi-level Monte Carlo Dropout for Efficient Uncertainty Quantification](https://arxiv.org/abs/2601.13272)
*Aaron Pim,Tristan Pryer*

Main category: cs.LG

TL;DR: 开发用于蒙特卡罗丢弃法不确定性量化的多级蒙特卡罗（MLMC）框架，推导相关表达式及样本分配规则，实验证实效率提升。


<details>
  <summary>Details</summary>
Motivation: 解决蒙特卡罗丢弃法在不确定性量化中的问题，提高估计效率。

Method: 将丢弃掩码视为认知随机性来源定义保真度层次，构建耦合的粗细估计器，推导表达式和样本分配规则。

Result: 数值实验证实了预测的方差率，且在相同成本下比单级MC丢弃法有更高效率。

Conclusion: 所提出的MLMC框架在蒙特卡罗丢弃法不确定性量化中有效且高效。

Abstract: We develop a multilevel Monte Carlo (MLMC) framework for uncertainty quantification with Monte Carlo dropout. Treating dropout masks as a source of epistemic randomness, we define a fidelity hierarchy by the number of stochastic forward passes used to estimate predictive moments. We construct coupled coarse--fine estimators by reusing dropout masks across fidelities, yielding telescoping MLMC estimators for both predictive means and predictive variances that remain unbiased for the corresponding dropout-induced quantities while reducing sampling variance at fixed evaluation budget. We derive explicit bias, variance and effective cost expressions, together with sample-allocation rules across levels. Numerical experiments on forward and inverse PINNs--Uzawa benchmarks confirm the predicted variance rates and demonstrate efficiency gains over single-level MC-dropout at matched cost.

</details>


### [207] [Hindsight Preference Replay Improves Preference-Conditioned Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2601.11604)
*Jonaid Shianifar,Michael Schukat,Karl Mason*

Main category: cs.LG

TL;DR: 本文引入后视偏好重放（HPR）策略增强CAPQL，在多个多目标强化学习任务中提升性能，仅mo - halfcheetah - v5任务除外。


<details>
  <summary>Details</summary>
Motivation: CAPQL方法未充分利用离策略数据，作者希望提出一种策略来更有效地利用数据，在多目标强化学习中提升性能。

Method: 引入后视偏好重放（HPR）这一简单通用的重放增强策略，对存储的转换用替代偏好进行追溯性重新标记，且不改变CAPQL架构和损失函数。

Result: 在六个MO - Gymnasium运动任务上评估，HPR - CAPQL在五个任务中提升了超体积（HV），在四个任务中提升了期望效用（EUM）。例如在mo - humanoid - v5任务中，EUM和HV显著提升，仅mo - halfcheetah - v5任务中CAPQL在相当EUM下有更高HV。

Conclusion: HPR策略能有效增强CAPQL在多目标强化学习中的性能，但对部分任务效果欠佳。

Abstract: Multi-objective reinforcement learning (MORL) enables agents to optimize vector-valued rewards while respecting user preferences. CAPQL, a preference-conditioned actor-critic method, achieves this by conditioning on weight vectors w and restricts data usage to the specific preferences under which it was collected, leaving off-policy data from other preferences unused. We introduce Hindsight Preference Replay (HPR), a simple and general replay augmentation strategy that retroactively relabels stored transitions with alternative preferences. This densifies supervision across the preference simplex without altering the CAPQL architecture or loss functions. Evaluated on six MO-Gymnasium locomotion tasks at a fixed 300000-step budget using expected utility (EUM), hypervolume (HV), and sparsity, HPR-CAPQL improves HV in five of six environments and EUM in four of six. On mo-humanoid-v5, for instance, EUM rises from $323\!\pm\!125$ to $1613\!\pm\!464$ and HV from 0.52M to 9.63M, with strong statistical support. mo-halfcheetah-v5 remains a challenging exception where CAPQL attains higher HV at comparable EUM. We report final summaries and Pareto-front visualizations across all tasks.

</details>


### [208] [A Multimodal Data Processing Pipeline for MIMIC-IV Dataset](https://arxiv.org/abs/2601.11606)
*Farzana Islam Adiba,Varsha Danduri,Fahmida Liza Piya,Ali Abbasi,Mehak Gupta,Rahmatollah Beheshti*

Main category: cs.LG

TL;DR: 为MIMIC - IV数据集提出综合可定制的多模态数据处理管道，减少处理时间，提高研究可重复性并开源。


<details>
  <summary>Details</summary>
Motivation: MIMIC - IV数据集多模态数据处理需大量人工，现有管道覆盖模态少或不支持任意下游应用。

Method: 大幅扩展先前单模态管道，系统集成多种模态，实现自动队列选择、模态间时间对齐和标准化输出格式。

Result: 开发出能显著减少多模态处理时间、提高MIMIC研究可重复性的管道。

Conclusion: 所提出的综合可定制多模态管道能有效处理MIMIC - IV数据集，相关代码、UI和Python包已开源。

Abstract: The MIMIC-IV dataset is a large, publicly available electronic health record (EHR) resource widely used for clinical machine learning research. It comprises multiple modalities, including structured data, clinical notes, waveforms, and imaging data. Working with these disjointed modalities requires an extensive manual effort to preprocess and align them for downstream analysis. While several pipelines for MIMIC-IV data extraction are available, they target a small subset of modalities or do not fully support arbitrary downstream applications. In this work, we greatly expand our prior popular unimodal pipeline and present a comprehensive and customizable multimodal pipeline that can significantly reduce multimodal processing time and enhance the reproducibility of MIMIC-based studies. Our pipeline systematically integrates the listed modalities, enabling automated cohort selection, temporal alignment across modalities, and standardized multimodal output formats suitable for arbitrary static and time-series downstream applications. We release the code, a simple UI, and a Python package for selective integration (with embedding) at https://github.com/healthylaife/MIMIC-IV-Data-Pipeline.

</details>


### [209] [Auxiliary-predicted Compress Memory Model(ApCM Model): A Neural Memory Storage Model Based on Invertible Compression and Learnable Prediction](https://arxiv.org/abs/2601.11609)
*Weinuo Ou*

Main category: cs.LG

TL;DR: 鉴于现行大语言模型缺有效运行时内存机制，本文提出了ApCM模型。


<details>
  <summary>Details</summary>
Motivation: 现行大语言模型普遍缺乏有效的运行时内存机制，难以适应动态和个性化交互需求。

Method: 提出一种新颖的神经内存存储架构——辅助预测压缩内存模型（ApCM模型）。

Result: 未提及。

Conclusion: 未提及。

Abstract: Current large language models (LLMs) generally lack an effective runtime memory mechanism,making it difficult to adapt to dynamic and personalized interaction requirements. To address this issue, this paper proposes a novel neural memory storage architecture--the Auxiliary Prediction Compression Memory Model (ApCM Model).

</details>


### [210] [Integrating Temporal Context into Streaming Data for Human Activity Recognition in Smart Home](https://arxiv.org/abs/2601.11611)
*Marina Vicini,Martin Rudorfer,Zhuangzhuang Dai,Luis J. Manso*

Main category: cs.LG

TL;DR: 针对老人居家监测，提出利用时间和位置特征改进人体活动识别的方法，实验显示在多数数据集上提升了准确率和F1分数。


<details>
  <summary>Details</summary>
Motivation: 应对全球人口老龄化，用传感器监测老人日常活动、实现预防性医疗干预的需求，解决活动识别中有效利用时间信息的难题。

Method: 将活动按早、中、晚聚类，编码到特征加权方法中；扩展特征向量，加入时间和位置特征。

Result: 在四个真实数据集的三个中，相比现有先进方法提高了准确率和F1分数，在低数据情况下提升明显。

Conclusion: 该方法有潜力开发有效的智能家居解决方案，支持老人居家养老。

Abstract: With the global population ageing, it is crucial to enable individuals to live independently and safely in their homes. Using ubiquitous sensors such as Passive InfraRed sensors (PIR) and door sensors is drawing increasing interest for monitoring daily activities and facilitating preventative healthcare interventions for the elderly. Human Activity Recognition (HAR) from passive sensors mostly relies on traditional machine learning and includes data segmentation, feature extraction, and classification. While techniques like Sensor Weighting Mutual Information (SWMI) capture spatial context in a feature vector, effectively leveraging temporal information remains a challenge. We tackle this by clustering activities into morning, afternoon, and night, and encoding them into the feature weighting method calculating distinct mutual information matrices. We further propose to extend the feature vector by incorporating time of day and day of week as cyclical temporal features, as well as adding a feature to track the user's location. The experiments show improved accuracy and F1-score over existing state-of-the-art methods in three out of four real-world datasets, with highest gains in a low-data regime. These results highlight the potential of our approach for developing effective smart home solutions to support ageing in place.

</details>


### [211] [A Review on Machine Learning Approaches for the Prediction of Glucose Levels and Hypogylcemia](https://arxiv.org/abs/2601.11615)
*Beyza Cinar,Louisa van den Boom,Maria Maleshkova*

Main category: cs.LG

TL;DR: 文章探讨利用机器学习模型预测1型糖尿病患者低血糖，对比模型在不同预测区间表现探讨多方面问题，提供模型效果与影响因素结论。


<details>
  <summary>Details</summary>
Motivation: 1型糖尿病患者需终身胰岛素治疗有低血糖副作用，机器学习模型可改善糖尿病管理，故研究相关模型。

Method: 研究基于1型糖尿病患者持续血糖监测设备数据训练的先进模型，对比短、长期预测区间模型表现，探讨多方面问题。

Result: 1小时预测区间最佳；传统机器学习分类效果好，深度学习回归效果好；多变量数据集和输入序列长度影响模型表现；个人数据提升效果但因质量问题更倾向基于人群模型。

Conclusion: 明确了不同预测区间、模型类型的表现及影响模型性能的因素和数据使用倾向。

Abstract: Type 1 Diabetes (T1D) is an autoimmune disease leading to insulin insufficiency. Thus, patients require lifelong insulin therapy, which has a side effect of hypoglycemia. Hypoglycemia is a critical state of decreased blood glucose levels (BGL) below 70 mg/dL and is associated with increased risk of mortality. Machine learning (ML) models can improve diabetes management by predicting hypoglycemia and providing optimal prevention methods. ML models are classified into regression and classification based, that forecast glucose levels and identify events based on defined labels, respectively. This review investigates state-of-the-art models trained on data of continuous glucose monitoring (CGM) devices from patients with T1D. We compare the models' performance across short-term (15 to 120 min) and long term (3 to more than 24 hours) prediction horizons (PHs). Particularly, we explore: 1) How much in advance can glucose values or a hypoglycemic event be accurately predicted? 2) Which models have the best performance? 3) Which factors impact the performance? and 4) Does personalization increase performance? The results show that 1) a PH of up to 1 hour provides the best results. 2) Conventional ML methods yield the best results for classification and DL for regression. A single model cannot adequately classify across multiple PHs. 3) The model performance is influenced by multivariate datasets and the input sequence length (ISL). 4) Personal data enhances performance but due to limited data quality population-based models are preferred.

</details>


### [212] [Mixture-of-Experts as Soft Clustering: A Dual Jacobian-PCA Spectral Geometry Perspective](https://arxiv.org/abs/2601.11616)
*Feilong Liu*

Main category: cs.LG

TL;DR: 本文从几何角度研究混合专家（MoE）架构，引入双Jacobian - PCA谱几何探针分析，发现MoE路由降低局部敏感性，专家局部表示有更高有效秩，不同路由方式有不同效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究对混合专家架构在学习函数和表示的几何方面的影响缺乏清晰刻画，本文旨在从几何视角研究MoE。

Method: 引入双Jacobian - PCA谱几何探针，通过Jacobian奇异值谱分析局部函数几何，通过路由隐藏状态的加权PCA分析表示几何，在可控的MLP - MoE设置下比较不同路由架构。

Result: MoE路由降低局部敏感性，专家局部表示在更多主方向分配方差，专家Jacobian近似正交，Top - k路由产生低秩、更集中结构，全软路由产生更广泛、高秩表示。

Conclusion: 支持将MoE解释为函数空间的软分区，可使局部曲率平坦化并重新分配表示方差。

Abstract: Mixture-of-Experts (MoE) architectures are commonly motivated by efficiency and conditional computation, but their effect on the geometry of learned functions and representations remains poorly characterized. In this work, we study MoEs through a geometric lens, interpreting routing as a form of soft partitioning of the representation space into overlapping local charts. We introduce a Dual Jacobian-PCA Spectral Geometry probe. It analyzes local function geometry via Jacobian singular-value spectra and representation geometry via weighted PCA of routed hidden states. Using a controlled MLP-MoE setting that permits exact Jacobian computation, we compare dense, Top-k, and fully-soft routing architectures under matched capacity. Across random seeds, we observe that MoE routing consistently reduces local sensitivity, with expert-local Jacobians exhibiting smaller leading singular values and faster spectral decay than dense baselines. At the same time, weighted PCA reveals that expert-local representations distribute variance across a larger number of principal directions, indicating higher effective rank under identical input distributions. We further find that average expert Jacobians are nearly orthogonal, suggesting a decomposition of the transformation into low-overlap expert-specific subspaces rather than scaled variants of a shared map. We analyze how routing sharpness modulates these effects, showing that Top-k routing produces lower-rank, more concentrated expert-local structure, while fully-soft routing yields broader, higher-rank representations. Together, these results support a geometric interpretation of MoEs as soft partitionings of function space that flatten local curvature while redistributing representation variance.

</details>


### [213] [Geometric Attention: A Regime-Explicit Operator Semantics for Transformer Attention](https://arxiv.org/abs/2601.11618)
*Luis Rosario Freytes*

Main category: cs.LG

TL;DR: 本文介绍几何注意力（GA）的构成要素，推导其在特定条件下的权重形式、规范形式，得到不同操作模式和算子，可用于机制比较和架构扩展。


<details>
  <summary>Details</summary>
Motivation: 为注意力机制的研究提供一个系统的框架，实现对注意力机制的比较和扩展。

Method: 从GA的四个独立输入出发，推导在标量关系工作表示和证据乘法组合律下的情况，对得分场进行处理得到规范形式。

Result: 得到了不同的操作模式如标准固定令牌、自适应载体等，支持多种算子和模式选择。

Conclusion: 分离了不变结构和建模选择，便于对注意力机制和基于注意力的架构进行原则性比较和扩展。

Abstract: Geometric Attention (GA) specifies an attention layer by four independent inputs: a finite carrier (what indices are addressable), an evidence-kernel rule (how masked proto-scores and a link induce nonnegative weights), a probe family (which observables are treated as admissible), and an anchor/update rule (which representative kernel is selected and how it is applied). Probe families induce an operational equivalence relation on kernels and therefore a gauge; anchors select representatives relative to that probe. Under a scalar relational-work representation and a multiplicative compositionality law for evidence, the admissible link family is exponential, yielding Gibbs weights; with row anchoring this includes the softmax kernel family as a subregime. After quotienting unary row/column score fields, the remaining interaction component admits a canonical rank-r normal form (Eckart-Young/SVD); dot-product score charts implement the corresponding low-rank interaction regime. Fixing the carrier and extensionalizing the update yields the standard fixed-token Transformer attention operator; allowing carrier updates yields adaptive-carrier and staged-depth regimes. The operator language also supports multihead/mixed kernels, plan-based anchors (e.g., entropic OT/Sinkhorn), and unary operators (e.g., FFN-style fields) as explicit regime choices. This separates invariant structure from modeling choice, enabling principled comparison and extension of attention mechanisms, and attention-based architectures.

</details>


### [214] [Universal Approximation Theorem for Input-Connected Multilayer Perceptrons](https://arxiv.org/abs/2601.14026)
*Vugar Ismailov*

Main category: cs.LG

TL;DR: 提出输入连接多层感知机（IC - MLP），研究其在单变量和向量值输入设置下性质，证明通用逼近定理。


<details>
  <summary>Details</summary>
Motivation: 引入新的前馈神经网络架构IC - MLP并研究其逼近能力。

Method: 先在单变量设置下研究IC - MLP，给出明确且系统的描述及迭代公式，再拓展到向量值输入。

Result: 在单变量时，证明当激活函数为非线性，深度IC - MLP能逼近实线上闭区间的任何连续函数；在向量值输入时，建立对应紧子集上连续函数逼近定理。

Conclusion: IC - MLP在一定条件下具有通用逼近能力。

Abstract: We introduce the Input-Connected Multilayer Perceptron (IC-MLP), a feedforward neural network architecture in which each hidden neuron receives, in addition to the outputs of the preceding layer, a direct affine connection from the raw input. We first study this architecture in the univariate setting and give an explicit and systematic description of IC-MLPs with an arbitrary finite number of hidden layers, including iterated formulas for the network functions. In this setting, we prove a universal approximation theorem showing that deep IC-MLPs can approximate any continuous function on a closed interval of the real line if and only if the activation function is nonlinear. We then extend the analysis to vector-valued inputs and establish a corresponding universal approximation theorem for continuous functions on compact subsets of $\mathbb{R}^n$.

</details>


### [215] [NoiseFormer -- Noise Diffused Symmetric Attention Transformer](https://arxiv.org/abs/2601.11619)
*Phani Kumar,Nyshadham,Jyothendra Varma,Polisetty V R K,Aditya Rathore*

Main category: cs.LG

TL;DR: 本文分析对称点积注意力技术，提出噪声扩散对称注意力变压器模型，在减少模型大小的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 随着模型内存占用增大，计算成本上升，需要采用稀疏注意力技术减少模型大小和参数。

Method: 分析对称点积注意力技术，提出噪声扩散对称注意力变压器模型。

Result: 在GPT2基础模型上验证，该模型在多种GLUE基准任务上的准确率介于普通对称注意力和GPT2基础模型之间，且模型大小显著减小。

Conclusion: 提出的模型在保持对称注意力内存优势的同时，提升了准确性和推理时间采样性能。

Abstract: Transformer architecture has been very successful long runner in the field of Deep Learning (DL) and Large Language Models (LLM) because of its powerful attention-based learning and parallel-natured architecture. As the models grow gigantic in terms of memory footprint, difficulties in fitting the model on a device like a GPU or an AI accelerator give rise to the need for multiple computing devices thereby escalating the computing cost. This increased training/inference cost paved the way for efficient model size reduction/parametric reduction deploying Sparse Attention techniques. In this paper, we start analyzing one of the techniques of Sparse Attention called Symmetric Dot-Product Attention (referred to as Symmetric Attention) and propose a novel unified model architecture called Noise Diffused Symmetric Attention Transformer to enhance the model's performance. While maintaining the memory gains of Symmetric Attention, with minute overhead in terms of model parameters and computational overhead, the proposed model brings in enhanced performance in terms of accuracy and inference-time sampling. The proposed model is validated upon GPT2 base model and the results reflect the performance gains falling between plain Symmetric attention and GPT2 base model on a variety of GLUE benchmark tasks in terms of accuracy, with significant model size reduction with respect to the base model.

</details>


### [216] [Verifying Physics-Informed Neural Network Fidelity using Classical Fisher Information from Differentiable Dynamical System](https://arxiv.org/abs/2601.11638)
*Josafat Ribeiro Leal Filho,Antônio Augusto Fröhlich*

Main category: cs.LG

TL;DR: 本文提出新框架，用Fisher信息量化PINN对物理系统动力学行为的捕捉能力，以汽车动力学模型实验对比分析模型和PINN的Fisher信息。


<details>
  <summary>Details</summary>
Motivation: 严格量化PINN在简单轨迹预测之外对系统完整动力学行为的捕捉能力是个挑战，需解决此问题。

Method: 提出用可微动力系统的Fisher信息$g_F^C$的实验框架，通过汽车动力学模型，基于系统动力学的雅可比矩阵对比分析模型和训练后的PINN的$g_F^C$。

Result: 通过对比获得了PINN在表示系统复杂动力学特征上保真度的定量衡量。

Conclusion: 若PINN准确学习物理系统动力学，其学习的运动方程得到的Fisher信息景观应与原分析模型相近，意味着PINN不仅能捕捉状态演化，还能捕捉关键几何和稳定性属性。

Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful tool for solving differential equations and modeling physical systems by embedding physical laws into the learning process. However, rigorously quantifying how well a PINN captures the complete dynamical behavior of the system, beyond simple trajectory prediction, remains a challenge. This paper proposes a novel experimental framework to address this by employing Fisher information for differentiable dynamical systems, denoted $g_F^C$. This Fisher information, distinct from its statistical counterpart, measures inherent uncertainties in deterministic systems, such as sensitivity to initial conditions, and is related to the phase space curvature and the net stretching action of the state space evolution. We hypothesize that if a PINN accurately learns the underlying dynamics of a physical system, then the Fisher information landscape derived from the PINN's learned equations of motion will closely match that of the original analytical model. This match would signify that the PINN has achieved comprehensive fidelity capturing not only the state evolution but also crucial geometric and stability properties. We outline an experimental methodology using the dynamical model of a car to compute and compare $g_F^C$ for both the analytical model and a trained PINN. The comparison, based on the Jacobians of the respective system dynamics, provides a quantitative measure of the PINN's fidelity in representing the system's intricate dynamical characteristics.

</details>


### [217] [Global Optimization By Gradient from Hierarchical Score-Matching Spaces](https://arxiv.org/abs/2601.11639)
*Ming Li*

Main category: cs.LG

TL;DR: 本文通过统一优化目标和分数匹配获取梯度，解决梯度下降局限性，实现全局优化并揭示其与扩散生成模型联系。


<details>
  <summary>Details</summary>
Motivation: 解决梯度下降局限于局部最优，且仅适用于简单凸约束连续可微问题的限制。

Method: 将各类复杂约束的优化问题统一为无约束的通用分层优化目标，通过分数匹配获取梯度进行优化。

Result: 首次用确定性方法利用严格梯度实现全局优化，并通过简单构造和复杂实际实验验证。

Conclusion: 揭示了全局优化与基于扩散的生成式建模之间的深刻联系。

Abstract: Gradient descent is the most commonly used optimization method, but limited to local optimality, and confined to the field of continuous differentiable problems with simple convex constraints. This work solve these limitations and restrictions by unifying all optimization problems with various complex constraints as a general hierarchical optimization objective without constraints, which is optimized by gradient obtained through score matching. By this way, global optimization by deterministic method using strict gradient is achieved for the first time, and verified through simple-constructed and complex-practical experiments. Even more importantly, it reveals the profound connection between global optimization and diffusion based generative modeling.

</details>


### [218] [Approximating splits for decision trees quickly in sparse data streams](https://arxiv.org/abs/2601.12525)
*Nikolaj Tatti*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Decision trees are one of the most popular classifiers in the machine learning literature. While the most common decision tree learning algorithms treat data as a batch, numerous algorithms have been proposed to construct decision trees from a data stream. A standard training strategy involves augmenting the current tree by changing a leaf node into a split. Here we typically maintain counters in each leaf which allow us to determine the optimal split, and whether the split should be done. In this paper we focus on how to speed up the search for the optimal split when dealing with sparse binary features and a binary class. We focus on finding splits that have the approximately optimal information gain or Gini index. In both cases finding the optimal split can be done in $O(d)$ time, where $d$ is the number of features. We propose an algorithm that yields $(1 + α)$ approximation when using conditional entropy in amortized $O(α^{-1}(1 + m\log d) \log \log n)$ time, where $m$ is the number of 1s in a data point, and $n$ is the number of data points. Similarly, for Gini index, we achieve $(1 + α)$ approximation in amortized $O(α^{-1} + m \log d)$ time. Our approach is beneficial for sparse data where $m \ll d$. In our experiments we find almost-optimal splits efficiently, faster than the baseline, overperforming the theoretical approximation guarantees.

</details>


### [219] [Size is Not the Solution: Deformable Convolutions for Effective Physics Aware Deep Learning](https://arxiv.org/abs/2601.11657)
*Jack T. Beerman,Shobhan Roy,H. S. Udaykumar,Stephen S. Baek*

Main category: cs.LG

TL;DR: 引入D - PARC解决CNN在物理建模中问题，表现优于大模型，证明合理架构设计比参数扩展有效。


<details>
  <summary>Details</summary>
Motivation: 当前CNN架构处理高度非线性流有困难，扩大模型规模在物理建模中收益递减。

Method: 受HLE数值方法启发，引入可变形物理感知循环卷积（D - PARC）。

Result: 在Burgers方程、Navier - Stokes和反应流中，D - PARC比更大的架构有更高保真度；内核呈现反聚类行为，有效感受野分析表明D - PARC能自主分配资源。

Conclusion: 物理直观的架构设计能优于参数扩展，精简网络的策略性学习是PADL更好的发展方向。

Abstract: Physics-aware deep learning (PADL) enables rapid prediction of complex physical systems, yet current convolutional neural network (CNN) architectures struggle with highly nonlinear flows. While scaling model size addresses complexity in broader AI, this approach yields diminishing returns for physics modeling. Drawing inspiration from Hybrid Lagrangian-Eulerian (HLE) numerical methods, we introduce deformable physics-aware recurrent convolutions (D-PARC) to overcome the rigidity of CNNs. Across Burgers' equation, Navier-Stokes, and reactive flows, D-PARC achieves superior fidelity compared to substantially larger architectures. Analysis reveals that kernels display anti-clustering behavior, evolving into a learned "active filtration" strategy distinct from traditional h- or p-adaptivity. Effective receptive field analysis confirms that D-PARC autonomously concentrates resources in high-strain regions while coarsening focus elsewhere, mirroring adaptive refinement in computational mechanics. This demonstrates that physically intuitive architectural design can outperform parameter scaling, establishing that strategic learning in lean networks offers a more effective path forward for PADL than indiscriminate network expansion.

</details>


### [220] [Machine learning model for predicting surface wettability in laser-textured metal alloys](https://arxiv.org/abs/2601.11661)
*Mohammad Mohammadzadeh Sanandaji,Danial Ebrahimzadeh,Mohammad Ikram Haider,Yaser Mike Banad,Aleksandar Poleksic,Hongtao Ding*

Main category: cs.LG

TL;DR: 本文提出机器学习框架预测激光纹理金属合金润湿性，模型性能佳，表明人工智能在建模和预测润湿行为方面有潜力。


<details>
  <summary>Details</summary>
Motivation: 表面润湿性在多个应用领域至关重要，需准确预测激光纹理金属合金的润湿性。

Method: 通过纳秒激光纹理和化学浸渍处理制备超亲水和超疏水表面，用Laws纹理能量法和轮廓仪量化表面形态，用X射线光电子能谱表征表面化学，提取特征训练集成神经网络模型。

Result: 模型预测精度高（R2 = 0.942，RMSE = 13.896），优于先前方法；特征重要性分析显示表面化学对接触角预测影响最大，地形特征也显著。

Conclusion: 人工智能能通过捕捉表面特性的复杂相互作用来建模和预测润湿行为，为设计定制功能表面提供数据驱动途径。

Abstract: Surface wettability, governed by both topography and chemistry, plays a critical role in applications such as heat transfer, lubrication, microfluidics, and surface coatings. In this study, we present a machine learning (ML) framework capable of accurately predicting the wettability of laser-textured metal alloys using experimentally derived morphological and chemical features. Superhydrophilic and superhydrophobic surfaces were fabricated on AA6061 and AISI 4130 alloys via nanosecond laser texturing followed by chemical immersion treatments. Surface morphology was quantified using the Laws texture energy method and profilometry, while surface chemistry was characterized through X-ray photoelectron spectroscopy (XPS), extracting features such as functional group polarity, molecular volume, and peak area fraction. These features were used to train an ensemble neural network model incorporating residual connections, batch normalization, and dropout regularization. The model achieved high predictive accuracy (R2 = 0.942, RMSE = 13.896), outperforming previous approaches. Feature importance analysis revealed that surface chemistry had the strongest influence on contact angle prediction, with topographical features also contributing significantly. This work demonstrates the potential of artificial intelligence to model and predict wetting behavior by capturing the complex interplay of surface characteristics, offering a data-driven pathway for designing tailored functional surfaces.

</details>


### [221] [Task-tailored Pre-processing: Fair Downstream Supervised Learning](https://arxiv.org/abs/2601.11897)
*Jinwon Sohn,Guang Lin,Qifan Song*

Main category: cs.LG

TL;DR: 本文研究监督学习的算法公平性，指出数据公平方法存在过强正则化问题，提出新的预处理方法，研究下游模型性质并通过实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有数据公平方法从HGR相关性角度看存在过强正则化问题，需设计新的监督学习预处理方法。

Method: 设计新的监督学习预处理方法，考虑公平性与效用的权衡，研究下游模型行为以找到保证公平性提升和效用保留的充分条件。

Result: 通过表格和图像数据集的对比研究，表明该框架在多个下游模型中能保持一致权衡，优于近期竞争模型，在计算机视觉数据中只改变必要语义特征。

Conclusion: 提出的新预处理方法在监督学习的公平性和效用方面表现良好，具有优越性。

Abstract: Fairness-aware machine learning has recently attracted various communities to mitigate discrimination against certain societal groups in data-driven tasks. For fair supervised learning, particularly in pre-processing, there have been two main categories: data fairness and task-tailored fairness. The former directly finds an intermediate distribution among the groups, independent of the type of the downstream model, so a learned downstream classification/regression model returns similar predictive scores to individuals inputting the same covariates irrespective of their sensitive attributes. The latter explicitly takes the supervised learning task into account when constructing the pre-processing map. In this work, we study algorithmic fairness for supervised learning and argue that the data fairness approaches impose overly strong regularization from the perspective of the HGR correlation. This motivates us to devise a novel pre-processing approach tailored to supervised learning. We account for the trade-off between fairness and utility in obtaining the pre-processing map. Then we study the behavior of arbitrary downstream supervised models learned on the transformed data to find sufficient conditions to guarantee their fairness improvement and utility preservation. To our knowledge, no prior work in the branch of task-tailored methods has theoretically investigated downstream guarantees when using pre-processed data. We further evaluate our framework through comparison studies based on tabular and image data sets, showing the superiority of our framework which preserves consistent trade-offs among multiple downstream models compared to recent competing models. Particularly for computer vision data, we see our method alters only necessary semantic features related to the central machine learning task to achieve fairness.

</details>


### [222] [Activation Sensitivity as a Unifying Principle for Post-Training Quantization](https://arxiv.org/abs/2601.11663)
*Bruce Changlong Xu*

Main category: cs.LG

TL;DR: 本文提出用于大语言模型后训练量化（PTQ）的统一理论框架，通过激活敏感性理解和比较PTQ方法。


<details>
  <summary>Details</summary>
Motivation: 现有的PTQ方法概念零散，不清楚其近似的潜在量。

Method: 形式化激活敏感性，用一阶泰勒展开得出通道重要性的原则性度量。

Result: 表明AWQ和GPTQ是在不同简化假设下对敏感性的互补近似，分析敏感性指标设计空间并明确与经典剪枝方法的关系。

Conclusion: 为理解和比较PTQ方法提供概念基础。

Abstract: Post-training quantization (PTQ) methods for large language models rely on heuristics that implicitly estimate which weight channels most strongly influence model behavior. Two dominant paradigms have emerged: activation-aware methods such as AWQ prioritize channels with large activation magnitudes, while second-order methods such as GPTQ allocate quantization error according to input covariance structure. Despite strong empirical performance, these approaches remain conceptually fragmented, and it is unclear what underlying quantity they are approximating. In this work, we present a unified theoretical framework for PTQ by formalizing activation sensitivity, defined as the expected impact of channel-wise perturbations on the loss. Using a first-order Taylor expansion, we show that sensitivity naturally arises as the squared norm of gradient-weighted activations, yielding a principled measure of channel importance that captures both activation magnitude and downstream error propagation. Within this framework, AWQ and GPTQ can be interpreted as complementary approximations that recover sensitivity under distinct simplifying assumptions. We analyze the design space of sensitivity metrics, connect gradient-based saliency, Fisher information, and Hessian-based criteria, and clarify their relationships to classical pruning methods such as Optimal Brain Damage and Optimal Brain Surgeon. Rather than proposing a new quantization algorithm, this work provides a conceptual foundation for understanding and comparing post-training quantization methods through the lens of sensitivity.

</details>


### [223] [Distill-then-Replace: Efficient Task-Specific Hybrid Attention Model Construction](https://arxiv.org/abs/2601.11667)
*Xiaojie Xia,Huigang Zhang,Chaoliang Zhong,Jun Sun,Yusuke Oishi*

Main category: cs.LG

TL;DR: 提出通过块局部蒸馏和贪心层替换策略构建高效混合模型，解决训练成本和注意力类型放置难题。


<details>
  <summary>Details</summary>
Motivation: Transformer全注意力模块计算和内存复杂度高，线性注意力性能下降，混合模型训练成本高且不易设计注意力类型位置。

Method: 先通过块局部蒸馏从预训练全注意力模块转移权重到线性注意力，再用贪心层替换策略迭代替换全注意力块。

Result: 能在一次高效过程中得到特定任务的混合模型，无需昂贵的重新训练或架构搜索。

Conclusion: 该方法可应用于任何预训练全注意力骨干网络以处理不同下游任务。

Abstract: Transformer architectures deliver state-of-the-art accuracy via dense full-attention, but their quadratic time and memory complexity with respect to sequence length limits practical deployment. Linear attention mechanisms offer linear or near-linear scaling yet often incur performance degradation. Hybrid models that integrate full and linear attention layers promise a balance between efficiency and expressiveness, but face two major challenges: training such hybrid models from scratch is computationally expensive, and manually designing the optimal placement of attention types is highly nontrivial. We address both issues by first transferring weights from the pretrained full-attention modules to its linear attention counterparts through blockwise local distillation, and second, introducing a greedy layer replacement strategy that iteratively substitutes full attention blocks with linear ones while monitoring validation performance on the target task. This yields a task-specific hybrid model in a single efficient pass, without costly re-training or neural architecture search, and can be applied to any pretrained full-attention backbone for diverse downstream tasks.

</details>


### [224] [Federated Learning for the Design of Parametric Insurance Indices under Heterogeneous Renewable Production Losses](https://arxiv.org/abs/2601.12178)
*Fallou Niakh*

Main category: cs.LG

TL;DR: 提出用于异构可再生能源生产损失下参数保险指数校准的联邦学习框架，对比多种算法，实证表明在适度异构下性能相当且更通用可扩展。


<details>
  <summary>Details</summary>
Motivation: 解决异构可再生能源生产损失下参数保险指数校准问题。

Method: 生产者用Tweedie广义线性模型和私有数据本地建模损失，通过联邦优化学习公共指数，实现并比较FedAvg、FedProx和FedOpt，与现有基于近似的聚合方法对比。

Result: 在德国太阳能生产实证中，联邦学习在适度异构下恢复相当的指数系数。

Conclusion: 提出的联邦学习框架更通用且可扩展。

Abstract: We propose a federated learning framework for the calibration of parametric insurance indices under heterogeneous renewable energy production losses. Producers locally model their losses using Tweedie generalized linear models and private data, while a common index is learned through federated optimization without sharing raw observations. The approach accommodates heterogeneity in variance and link functions and directly minimizes a global deviance objective in a distributed setting. We implement and compare FedAvg, FedProx and FedOpt, and benchmark them against an existing approximation-based aggregation method. An empirical application to solar power production in Germany shows that federated learning recovers comparable index coefficients under moderate heterogeneity, while providing a more general and scalable framework.

</details>


### [225] [IPEC: Test-Time Incremental Prototype Enhancement Classifier for Few-Shot Learning](https://arxiv.org/abs/2601.11669)
*Wenwen Liao,Hang Ruan,Jianbo Yu,Xiaofeng Yang,Qingchao Jiang,Xuefeng Yan*

Main category: cs.LG

TL;DR: 针对基于度量的少样本方法测试时因批次独立假设存在的问题，提出增量原型增强分类器（IPEC），通过动态辅助集和双过滤机制优化原型估计，在多任务中有出色表现。


<details>
  <summary>Details</summary>
Motivation: 基于度量的少样本方法在测试时存在批次独立假设的问题，无法利用之前批次积累的知识。

Method: 提出IPEC方法，通过选择性地纳入高置信度查询样本形成动态辅助集，设计双过滤机制保障样本质量，结合贝叶斯方法提出两阶段推理协议。

Result: 广泛的实验结果证实了该方法在多个少样本分类任务上的优越性能。

Conclusion: 所提出的IPEC方法能有效解决基于度量的少样本方法的问题，在少样本分类任务中有出色表现。

Abstract: Metric-based few-shot approaches have gained significant popularity due to their relatively straightforward implementation, high interpret ability, and computational efficiency. However, stemming from the batch-independence assumption during testing, which prevents the model from leveraging valuable knowledge accumulated from previous batches. To address these challenges, we propose a novel test-time method called Incremental Prototype Enhancement Classifier (IPEC), a test-time method that optimizes prototype estimation by leveraging information from previous query samples. IPEC maintains a dynamic auxiliary set by selectively incorporating query samples that are classified with high confidence. To ensure sample quality, we design a robust dual-filtering mechanism that assesses each query sample based on both global prediction confidence and local discriminative ability. By aggregating this auxiliary set with the support set in subsequent tasks, IPEC builds progressively more stable and representative prototypes, effectively reducing its reliance on the initial support set. We ground this approach in a Bayesian interpretation, conceptualizing the support set as a prior and the auxiliary set as a data-driven posterior, which in turn motivates the design of a practical "warm-up and test" two-stage inference protocol. Extensive empirical results validate the superior performance of our proposed method across multiple few-shot classification tasks.

</details>


### [226] [One-Sided Matrix Completion from Ultra-Sparse Samples](https://arxiv.org/abs/2601.12213)
*Hongyang R. Zhang,Zhenshuo Zhang,Huy L. Nguyen,Guanghui Lan*

Main category: cs.LG

TL;DR: 本文在超稀疏采样机制下重新审视矩阵补全问题，提出无偏估计器并结合梯度下降法，实验验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 解决涉及大型稀疏面板数据集的矩阵补全问题，在每行观测值少于矩阵秩时准确估计矩阵行空间或平均二阶矩矩阵。

Method: 提出无偏估计器，对二阶矩非零元素按观测频率归一化，再用梯度下降法填补二阶矩矩阵缺失元素。

Result: 估计器无偏且方差低；理论证明满足条件时能近似全局最优恢复二阶矩矩阵；实验表明在多个数据集上相比基线方法降低偏差和恢复误差。

Conclusion: 所提方法在超稀疏采样机制下的矩阵补全问题中有效。

Abstract: Matrix completion is a classical problem that has received recurring interest across a wide range of fields. In this paper, we revisit this problem in an ultra-sparse sampling regime, where each entry of an unknown, $n\times d$ matrix $M$ (with $n \ge d$) is observed independently with probability $p = C / d$, for a fixed integer $C \ge 2$. This setting is motivated by applications involving large, sparse panel datasets, where the number of rows far exceeds the number of columns. When each row contains only $C$ entries -- fewer than the rank of $M$ -- accurate imputation of $M$ is impossible. Instead, we estimate the row span of $M$ or the averaged second-moment matrix $T = M^{\top} M / n$.
  The empirical second-moment matrix computed from observed entries exhibits non-random and sparse missingness. We propose an unbiased estimator that normalizes each nonzero entry of the second moment by its observed frequency, followed by gradient descent to impute the missing entries of $T$. The normalization divides a weighted sum of $n$ binomial random variables by the total number of ones. We show that the estimator is unbiased for any $p$ and enjoys low variance. When the row vectors of $M$ are drawn uniformly from a rank-$r$ factor model satisfying an incoherence condition, we prove that if $n \ge O({d r^5 ε^{-2} C^{-2} \log d})$, any local minimum of the gradient-descent objective is approximately global and recovers $T$ with error at most $ε^2$.
  Experiments on both synthetic and real-world data validate our approach. On three MovieLens datasets, our algorithm reduces bias by $88\%$ relative to baseline estimators. We also empirically validate the linear sampling complexity of $n$ relative to $d$ on synthetic data. On an Amazon reviews dataset with sparsity $10^{-7}$, our method reduces the recovery error of $T$ by $59\%$ and $M$ by $38\%$ compared to baseline methods.

</details>


### [227] [A Confidence-Variance Theory for Pseudo-Label Selection in Semi-Supervised Learning](https://arxiv.org/abs/2601.11670)
*Jinshi Liu,Pan Liu*

Main category: cs.LG

TL;DR: 本文提出Confidence - Variance (CoVar)理论框架用于半监督学习中的伪标签选择，将其作为插件模块集成到多种方法中，在多个数据集上提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的半监督学习伪标签选择策略依赖固定置信阈值，而深度网络常过度自信，高置信度预测可能错误，低置信度样本被丢弃，需要更可靠的选择标准。

Method: 从熵最小化原则出发，推导结合最大置信度（MC）和残差类方差（RCV）的可靠性度量；将伪标签选择转化为谱松弛问题，设计无阈值选择机制；将CoVar作为插件模块集成到半监督语义分割和图像分类方法中。

Result: 在PASCAL VOC 2012、Cityscapes、CIFAR - 10和Mini - ImageNet等数据集上，不同标签比例和骨干网络下，均优于强基线。

Conclusion: 结合置信度和残差类方差为伪标签选择提供了比固定置信阈值更可靠的基础。

Abstract: Most pseudo-label selection strategies in semi-supervised learning rely on fixed confidence thresholds, implicitly assuming that prediction confidence reliably indicates correctness. In practice, deep networks are often overconfident: high-confidence predictions can still be wrong, while informative low-confidence samples near decision boundaries are discarded. This paper introduces a Confidence-Variance (CoVar) theory framework that provides a principled joint reliability criterion for pseudo-label selection. Starting from the entropy minimization principle, we derive a reliability measure that combines maximum confidence (MC) with residual-class variance (RCV), which characterizes how probability mass is distributed over non-maximum classes. The derivation shows that reliable pseudo-labels should have both high MC and low RCV, and that the influence of RCV increases as confidence grows, thereby correcting overconfident but unstable predictions. From this perspective, we cast pseudo-label selection as a spectral relaxation problem that maximizes separability in a confidence-variance feature space, and design a threshold-free selection mechanism to distinguish high- from low-reliability predictions. We integrate CoVar as a plug-in module into representative semi-supervised semantic segmentation and image classification methods. Across PASCAL VOC 2012, Cityscapes, CIFAR-10, and Mini-ImageNet with varying label ratios and backbones, it consistently improves over strong baselines, indicating that combining confidence with residual-class variance provides a more reliable basis for pseudo-label selection than fixed confidence thresholds. (Code: https://github.com/ljs11528/CoVar_Pseudo_Label_Selection.git)

</details>


### [228] [Proof of Concept: Multi-Target Wildfire Risk Prediction and Large Language Model Synthesis](https://arxiv.org/abs/2601.11686)
*Nicolas Caron,Christophe Guyeux,Hassan Noura,Benjamin Aynes*

Main category: cs.LG

TL;DR: 现有野火风险评估方法忽略操作需求，本文提出结合预测模型和大语言模型的混合框架。


<details>
  <summary>Details</summary>
Motivation: 现有的野火风险评估方法忽略操作需求，实用性不足，且有效野火管理需多目标分析。

Method: 开发一个混合框架，将各风险维度的预测模型与大语言模型结合，把异构输出合成为结构化、可操作的报告。

Result: 未提及

Conclusion: 未提及

Abstract: Current state-of-the-art approaches to wildfire risk assessment often overlook operational needs, limiting their practical value for first responders and firefighting services. Effective wildfire management requires a multi-target analysis that captures the diverse dimensions of wildfire risk, including meteorological danger, ignition activity, intervention complexity, and resource mobilization, rather than relying on a single predictive indicator. In this proof of concept, we propose the development of a hybrid framework that combines predictive models for each risk dimension with large language models (LLMs) to synthesize heterogeneous outputs into structured, actionable reports.

</details>


### [229] [Statistical-Neural Interaction Networks for Interpretable Mixed-Type Data Imputation](https://arxiv.org/abs/2601.12380)
*Ou Deng,Shoji Nishimura,Atsushi Ogihara,Qun Jin*

Main category: cs.LG

TL;DR: 提出可解释的混合类型插补框架SNI，结合统计先验和神经特征注意力，评估其在多数据集上表现并讨论相关情况。


<details>
  <summary>Details</summary>
Motivation: 现实表格数据库存在大量缺失值会影响下游分析，需要有效的插补方法。

Method: 提出SNI框架，通过CPFA模块结合统计先验和神经特征注意力，还可聚合注意力图得到特征依赖矩阵。

Result: 在MCAR/strict - MAR 30%缺失率下，SNI在连续指标上有竞争力，在分类变量上常被MissForest、MIWAE超越，能提供依赖诊断和权衡参数。

Conclusion: SNI有一定优势，可在对可解释性有要求的场景部署，但存在对严重不平衡分类目标表现不佳等局限。

Abstract: Real-world tabular databases routinely combine continuous measurements and categorical records, yet missing entries are pervasive and can distort downstream analysis. We propose Statistical-Neural Interaction (SNI), an interpretable mixed-type imputation framework that couples correlation-derived statistical priors with neural feature attention through a Controllable-Prior Feature Attention (CPFA) module. CPFA learns head-wise prior-strength coefficients $\{λ_h\}$ that softly regularize attention toward the prior while allowing data-driven deviations when nonlinear patterns appear to be present in the data. Beyond imputation, SNI aggregates attention maps into a directed feature-dependency matrix that summarizes which variables the imputer relied on, without requiring post-hoc explainers. We evaluate SNI against six baselines (Mean/Mode, MICE, KNN, MissForest, GAIN, MIWAE) on six datasets spanning ICU monitoring, population surveys, socio-economic statistics, and engineering applications. Under MCAR/strict-MAR at 30\% missingness, SNI is generally competitive on continuous metrics but is often outperformed by accuracy-first baselines (MissForest, MIWAE) on categorical variables; in return, it provides intrinsic dependency diagnostics and explicit statistical-neural trade-off parameters. We additionally report MNAR stress tests (with a mask-aware variant) and discuss computational cost, limitations -- particularly for severely imbalanced categorical targets -- and deployment scenarios where interpretability may justify the trade-off.

</details>


### [230] [jBOT: Semantic Jet Representation Clustering Emerges from Self-Distillation](https://arxiv.org/abs/2601.11719)
*Ho Fung Tsoi,Dylan Rankin*

Main category: cs.LG

TL;DR: 提出基于自蒸馏的预训练方法jBOT用于大型强子对撞机的喷流数据，发现预训练能带来语义类聚类，提升异常检测和分类性能


<details>
  <summary>Details</summary>
Motivation: 利用无监督的自监督学习方法为大型强子对撞机的喷流数据学习特征表示以支持下游任务

Method: 提出jBOT方法，结合局部粒子级蒸馏和全局喷流级蒸馏来学习喷流表示

Result: 无标签喷流预训练导致表示空间出现语义类聚类，仅用背景喷流预训练的冻结嵌入可实现异常检测，且微调后用于分类性能优于从头训练的监督模型

Conclusion: jBOT这种自蒸馏预训练方法能有效提升喷流数据在下游异常检测和分类任务的性能

Abstract: Self-supervised learning is a powerful pre-training method for learning feature representations without labels, which often capture generic underlying semantics from the data and can later be fine-tuned for downstream tasks. In this work, we introduce jBOT, a pre-training method based on self-distillation for jet data from the CERN Large Hadron Collider, which combines local particle-level distillation with global jet-level distillation to learn jet representations that support downstream tasks such as anomaly detection and classification. We observe that pre-training on unlabeled jets leads to emergent semantic class clustering in the representation space. The clustering in the frozen embedding, when pre-trained on background jets only, enables anomaly detection via simple distance-based metrics, and the learned embedding can be fine-tuned for classification with improved performance compared to supervised models trained from scratch.

</details>


### [231] [Cooperative Multi-agent RL with Communication Constraints](https://arxiv.org/abs/2601.12518)
*Nuoya Xiong,Aarti Singh*

Main category: cs.LG

TL;DR: 现有合作多智能体强化学习在通信受限下不稳定，本文提出基础策略预测技术，理论上降低通信和样本复杂度，实验结果良好。


<details>
  <summary>Details</summary>
Motivation: 现有基于重要性采样处理缺失数据的方法在通信受限时不稳定，基础策略过时。

Method: 提出基础策略预测技术，利用旧梯度预测策略更新并为一系列基础策略收集样本。

Result: 理论上算法在潜在博弈中收敛到 ε - 纳什均衡，降低通信和样本复杂度，还扩展到一般马尔可夫合作博弈；实验在模拟游戏和复杂环境中验证了算法。

Conclusion: 基础策略预测技术能在少通信轮次下实现有效学习，改进现有结果。

Abstract: Cooperative MARL often assumes frequent access to global information in a data buffer, such as team rewards or other agents' actions, which is typically unrealistic in decentralized MARL systems due to high communication costs. When communication is limited, agents must rely on outdated information to estimate gradients and update their policies. A common approach to handle missing data is called importance sampling, in which we reweigh old data from a base policy to estimate gradients for the current policy. However, it quickly becomes unstable when the communication is limited (i.e. missing data probability is high), so that the base policy in importance sampling is outdated. To address this issue, we propose a technique called base policy prediction, which utilizes old gradients to predict the policy update and collect samples for a sequence of base policies, which reduces the gap between the base policy and the current policy. This approach enables effective learning with significantly fewer communication rounds, since the samples of predicted base policies could be collected within one communication round. Theoretically, we show that our algorithm converges to an $\varepsilon$-Nash equilibrium in potential games with only $O(\varepsilon^{-3/4})$ communication rounds and $O(poly(\max_i |A_i|)\varepsilon^{-11/4})$ samples, improving existing state-of-the-art results in communication cost, as well as sample complexity without the exponential dependence on the joint action space size. We also extend these results to general Markov Cooperative Games to find an agent-wise local maximum. Empirically, we test the base policy prediction algorithm in both simulated games and MAPPO for complex environments.

</details>


### [232] [Suspicious Alignment of SGD: A Fine-Grained Step Size Condition Analysis](https://arxiv.org/abs/2601.11789)
*Shenyang Deng,Boyao Liao,Zhuoli Ouyang,Tianyu Pang,Minhak Song,Yaoqing Yang*

Main category: cs.LG

TL;DR: 本文研究随机梯度下降（SGD）在病态优化下的可疑对齐现象，分析步长选择如何产生该现象，提出步长条件并证明SGD的两阶段行为。


<details>
  <summary>Details</summary>
Motivation: 探索SGD在病态优化下梯度对齐现象及步长选择对其的影响。

Method: 在高维二次设置下进行细粒度分析，提出步长条件。

Result: 提出自适应临界步长分离不同对齐状态，存在使投影到不同子空间有不同损失变化的步长区间，证明SGD的两阶段行为。

Conclusion: 基于自适应步长理论，解释了SGD在病态优化下的可疑对齐现象及行为。

Abstract: This paper explores the suspicious alignment phenomenon in stochastic gradient descent (SGD) under ill-conditioned optimization, where the Hessian spectrum splits into dominant and bulk subspaces. This phenomenon describes the behavior of gradient alignment in SGD updates. Specifically, during the initial phase of SGD updates, the alignment between the gradient and the dominant subspace tends to decrease. Subsequently, it enters a rising phase and eventually stabilizes in a high-alignment phase. The alignment is considered ``suspicious'' because, paradoxically, the projected gradient update along this highly-aligned dominant subspace proves ineffective at reducing the loss. The focus of this work is to give a fine-grained analysis in a high-dimensional quadratic setup about how step size selection produces this phenomenon. Our main contribution can be summarized as follows: We propose a step-size condition revealing that in low-alignment regimes, an adaptive critical step size $η_t^*$ separates alignment-decreasing ($η_t < η_t^*$) from alignment-increasing ($η_t > η_t^*$) regimes, whereas in high-alignment regimes, the alignment is self-correcting and decreases regardless of the step size. We further show that under sufficient ill-conditioning, a step size interval exists where projecting the SGD updates to the bulk space decreases the loss while projecting them to the dominant space increases the loss, which explains a recent empirical observation that projecting gradient updates to the dominant subspace is ineffective. Finally, based on this adaptive step-size theory, we prove that for a constant step size and large initialization, SGD exhibits this distinct two-phase behavior: an initial alignment-decreasing phase, followed by stabilization at high alignment.

</details>


### [233] [What Trace Powers Reveal About Log-Determinants: Closed-Form Estimators, Certificates, and Failure Modes](https://arxiv.org/abs/2601.12612)
*Piyush Sao*

Main category: cs.LG

TL;DR: 本文研究大对称正定矩阵的log det(A)计算，基于迹幂提出新方法，证明了有限正矩估计的局限性并给出上下界。


<details>
  <summary>Details</summary>
Motivation: 标准方法结合矩阵 - 向量积与多项式逼近，本文研究基于迹幂的不同模型，解决经典矩逼近的局限性。

Method: 利用矩生成函数，对其变换后插值求导估计log det(A)，并给出上下界。

Result: 证明了有限正矩估计的局限性，得到log det(A)的上下界，所有估计和界的计算复杂度为O(m)。

Conclusion: 提出的方法在特定m范围内接近常数时间，能提供估计和有保证的界。

Abstract: Computing $\log\det(A)$ for large symmetric positive definite matrices arises in Gaussian process inference and Bayesian model comparison. Standard methods combine matrix-vector products with polynomial approximations. We study a different model: access to trace powers $p_k = \tr(A^k)$, natural when matrix powers are available.
  Classical moment-based approximations Taylor-expand $\log(λ)$ around the arithmetic mean. This requires $|λ- \AM| < \AM$ and diverges when $κ> 4$. We work instead with the moment-generating function $M(t) = \E[X^t]$ for normalized eigenvalues $X = λ/\AM$. Since $M'(0) = \E[\log X]$, the log-determinant becomes $\log\det(A) = n(\log \AM + M'(0))$ -- the problem reduces to estimating a derivative at $t = 0$. Trace powers give $M(k)$ at positive integers, but interpolating $M(t)$ directly is ill-conditioned due to exponential growth. The transform $K(t) = \log M(t)$ compresses this range. Normalization by $\AM$ ensures $K(0) = K(1) = 0$. With these anchors fixed, we interpolate $K$ through $m+1$ consecutive integers and differentiate to estimate $K'(0)$. However, this local interpolation cannot capture arbitrary spectral features.
  We prove a fundamental limit: no continuous estimator using finitely many positive moments can be uniformly accurate over unbounded conditioning. Positive moments downweight the spectral tail; $K'(0) = \E[\log X]$ is tail-sensitive. This motivates guaranteed bounds. From the same traces we derive upper bounds on $(\det A)^{1/n}$. Given a spectral floor $r \leq λ_{\min}$, we obtain moment-constrained lower bounds, yielding a provable interval for $\log\det(A)$. A gap diagnostic indicates when to trust the point estimate and when to report bounds. All estimators and bounds cost $O(m)$, independent of $n$. For $m \in \{4, \ldots, 8\}$, this is effectively constant time.

</details>


### [234] [Physics-Constrained Denoising Autoencoders for Data-Scarce Wildfire UAV Sensing](https://arxiv.org/abs/2601.11794)
*Abdelrahman Ramadan,Zahra Dorbeigi Namaghi,Emily Taylor,Lucas Edwards,Xan Giuliani,David S. McLagan,Sidney Givigi,Melissa Greeff*

Main category: cs.LG

TL;DR: 提出物理信息去噪自编码器PC$^2$DAE处理无人机低成本传感器数据稀缺问题，在少量数据上效果好，训练快。


<details>
  <summary>Details</summary>
Motivation: 野火监测需高分辨率大气测量，但无人机低成本传感器有数据问题，传统深度学习去噪方法需大量数据，飞行活动难以获取。

Method: 提出PC$^2$DAE，将物理约束嵌入网络架构，用softplus激活和时间平滑确保输出物理可行，有两种变体用于不同场景。

Result: 在少量数据上评估，PC$^2$DAE-Lean平滑度提升67.3%，高频噪声降低90.7%，无物理违规，优于基线模型，训练在65秒内完成。

Conclusion: PC$^2$DAE在数据稀缺情况下有效，减少容量和强归纳偏置可防止过拟合。

Abstract: Wildfire monitoring requires high-resolution atmospheric measurements, yet low-cost sensors on Unmanned Aerial Vehicles (UAVs) exhibit baseline drift, cross-sensitivity, and response lag that corrupt concentration estimates. Traditional deep learning denoising approaches demand large datasets impractical to obtain from limited UAV flight campaigns. We present PC$^2$DAE, a physics-informed denoising autoencoder that addresses data scarcity by embedding physical constraints directly into the network architecture. Non-negative concentration estimates are enforced via softplus activations and physically plausible temporal smoothing, ensuring outputs are physically admissible by construction rather than relying on loss function penalties. The architecture employs hierarchical decoder heads for Black Carbon, Gas, and CO$_2$ sensor families, with two variants: PC$^2$DAE-Lean (21k parameters) for edge deployment and PC$^2$DAE-Wide (204k parameters) for offline processing. We evaluate on 7,894 synchronized 1 Hz samples collected from UAV flights during prescribed burns in Saskatchewan, Canada (approximately 2.2 hours of flight data), two orders of magnitude below typical deep learning requirements. PC$^2$DAE-Lean achieves 67.3\% smoothness improvement and 90.7\% high-frequency noise reduction with zero physics violations. Five baselines (LSTM-AE, U-Net, Transformer, CBDAE, DeSpaWN) produce 15--23\% negative outputs. The lean variant outperforms wide (+5.6\% smoothness), suggesting reduced capacity with strong inductive bias prevents overfitting in data-scarce regimes. Training completes in under 65 seconds on consumer hardware.

</details>


### [235] [Shapelets-Enriched Selective Forecasting using Time Series Foundation Models](https://arxiv.org/abs/2601.11821)
*Shivani Tomar,Seshu Tirupathi,Elizabeth Daly,Ivana Dusparic*

Main category: cs.LG

TL;DR: 提出选择性预测框架，用形状特征识别时间序列关键段，在多数据集上降低预测误差。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型在数据关键区域预测不可靠，限制实际应用，尤其是数据有独特趋势时。

Method: 使用形状特征识别时间序列关键段，通过在目标域数据集验证集上进行平移不变字典学习来学习形状特征，利用与形状特征的基于距离的相似度，让用户选择性丢弃不可靠预测。

Result: 在多个基准时间序列数据集上，利用零样本和全样本微调模型的方法平均降低零样本误差22.17%、全样本微调模型误差22.62%，且在一个数据集上比随机选择方法分别最多高21.41%和21.43%。

Conclusion: 所提选择性预测框架能有效降低时间序列预测误差，提升预测可靠性。

Abstract: Time series foundation models have recently gained a lot of attention due to their ability to model complex time series data encompassing different domains including traffic, energy, and weather. Although they exhibit strong average zero-shot performance on forecasting tasks, their predictions on certain critical regions of the data are not always reliable, limiting their usability in real-world applications, especially when data exhibits unique trends. In this paper, we propose a selective forecasting framework to identify these critical segments of time series using shapelets. We learn shapelets using shift-invariant dictionary learning on the validation split of the target domain dataset. Utilizing distance-based similarity to these shapelets, we facilitate the user to selectively discard unreliable predictions and be informed of the model's realistic capabilities. Empirical results on diverse benchmark time series datasets demonstrate that our approach leveraging both zero-shot and full-shot fine-tuned models reduces the overall error by an average of 22.17% for zero-shot and 22.62% for full-shot fine-tuned model. Furthermore, our approach using zero-shot and full-shot fine-tuned models, also outperforms its random selection counterparts by up to 21.41% and 21.43% on one of the datasets.

</details>


### [236] [Decoding Rewards in Competitive Games: Inverse Game Theory with Entropy Regularization](https://arxiv.org/abs/2601.12707)
*Junyi Liao,Zihan Zhu,Ethan Fang,Zhuoran Yang,Vahid Tarokh*

Main category: cs.LG

TL;DR: 本文提出了一个统一框架，用于两人零和矩阵博弈和带熵正则化的马尔可夫博弈中恢复奖励函数，证明算法可靠性并进行数值实验。


<details>
  <summary>Details</summary>
Motivation: 解决逆向强化学习和博弈论中估计未知奖励函数的问题，该任务因逆问题模糊性等因素具有挑战性。

Method: 在线性假设下使用量子响应均衡（QRE）建立奖励函数可识别性，提出从观测行动中学习奖励函数的新算法，算法适用于静态和动态设置并可结合不同方法。

Result: 为算法的可靠性和样本效率提供了强理论保证，通过广泛数值研究证明了框架的实际有效性。

Conclusion: 该框架能为竞争环境中的决策提供新见解。

Abstract: Estimating the unknown reward functions driving agents' behaviors is of central interest in inverse reinforcement learning and game theory. To tackle this problem, we develop a unified framework for reward function recovery in two-player zero-sum matrix games and Markov games with entropy regularization, where we aim to reconstruct the underlying reward functions given observed players' strategies and actions. This task is challenging due to the inherent ambiguity of inverse problems, the non-uniqueness of feasible rewards, and limited observational data coverage. To address these challenges, we establish the reward function's identifiability using the quantal response equilibrium (QRE) under linear assumptions. Building upon this theoretical foundation, we propose a novel algorithm to learn reward functions from observed actions. Our algorithm works in both static and dynamic settings and is adaptable to incorporate different methods, such as Maximum Likelihood Estimation (MLE). We provide strong theoretical guarantees for the reliability and sample efficiency of our algorithm. Further, we conduct extensive numerical studies to demonstrate the practical effectiveness of the proposed framework, offering new insights into decision-making in competitive environments.

</details>


### [237] [MixFlow: Mixture-Conditioned Flow Matching for Out-of-Distribution Generalization](https://arxiv.org/abs/2601.11827)
*Andrea Rubbi,Amir Akbarnejad,Mohammad Vali Sanian,Aryan Yazdan Parast,Hesam Asadollahzadeh,Arian Amani,Naveed Akhtar,Sarah Cooper,Andrew Bassett,Pietro Liò,Lassi Paavolainen,Sattar Vakili,Mo Lotfollahi*

Main category: cs.LG

TL;DR: 提出MixFlow框架解决条件生成模型中分布偏移下泛化问题，在多领域验证效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有基于条件流的方法在分布偏移下难以有效泛化，无法外推到训练条件之外。

Method: 通过最短路径流匹配联合学习描述符条件化的基础分布和流场，将基础分布建模为可学习的、依赖描述符的混合模型。

Result: MixFlow在单细胞转录组数据和药物筛选等多个领域中，始终优于标准的条件流匹配基线。

Conclusion: MixFlow为跨异构领域实现稳健、可泛化和可控的生成建模提供了简单而强大的方法。

Abstract: Achieving robust generalization under distribution shift remains a central challenge in conditional generative modeling, as existing conditional flow-based methods often struggle to extrapolate beyond the training conditions. We introduce MixFlow, a conditional flow-matching framework for descriptor-controlled generation that directly targets this limitation by jointly learning a descriptor-conditioned base distribution and a descriptor-conditioned flow field via shortest-path flow matching. By modeling the base distribution as a learnable, descriptor-dependent mixture, MixFlow enables smooth interpolation and extrapolation to unseen conditions, leading to substantially improved out-of-distribution generalization. We provide analytical insights into the behavior of the proposed framework and empirically demonstrate its effectiveness across multiple domains, including prediction of responses to unseen perturbations in single-cell transcriptomic data and high-content microscopy-based drug screening tasks. Across these diverse settings, MixFlow consistently outperforms standard conditional flow-matching baselines. Overall, MixFlow offers a simple yet powerful approach for achieving robust, generalizable, and controllable generative modeling across heterogeneous domains.

</details>


### [238] [Online Continual Learning for Time Series: a Natural Score-driven Approach](https://arxiv.org/abs/2601.12931)
*Edoardo Urettini,Daniele Atzeni,Ioanna-Yvonni Tsaknaki,Antonio Carta*

Main category: cs.LG

TL;DR: 本文加强时间序列方法与在线持续学习（OCL）的理论和实践联系，提出NatSR方法，在预测性能上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: OCL可适应变化环境且不忘旧知识，在线时间序列预测（OTSF）需要快速适应和长期记忆，有必要加强时间序列方法和OCL的联系。

Method: 将神经网络优化重构为参数过滤问题，证明自然梯度下降的信息论最优性；使用Student's t似然结合自然梯度实现有界更新；引入NatSR方法，结合鲁棒优化器、重放缓冲区和动态尺度启发式。

Result: NatSR在预测性能上比更复杂的现有方法更强。

Conclusion: 加强时间序列方法和OCL的联系是有效的，NatSR方法具有良好的预测性能。

Abstract: Online continual learning (OCL) methods adapt to changing environments without forgetting past knowledge. Similarly, online time series forecasting (OTSF) is a real-world problem where data evolve in time and success depends on both rapid adaptation and long-term memory. Indeed, time-varying and regime-switching forecasting models have been extensively studied, offering a strong justification for the use of OCL in these settings. Building on recent work that applies OCL to OTSF, this paper aims to strengthen the theoretical and practical connections between time series methods and OCL. First, we reframe neural network optimization as a parameter filtering problem, showing that natural gradient descent is a score-driven method and proving its information-theoretic optimality. Then, we show that using a Student's t likelihood in addition to natural gradient induces a bounded update, which improves robustness to outliers. Finally, we introduce Natural Score-driven Replay (NatSR), which combines our robust optimizer with a replay buffer and a dynamic scale heuristic that improves fast adaptation at regime drifts. Empirical results demonstrate that NatSR achieves stronger forecasting performance than more complex state-of-the-art methods.

</details>


### [239] [AGGC: Adaptive Group Gradient Clipping for Stabilizing Large Language Model Training](https://arxiv.org/abs/2601.11864)
*Zhiyuan Li,Yuan Wu,Yi Chang*

Main category: cs.LG

TL;DR: 为稳定大语言模型训练，提出自适应分组梯度裁剪（AGGC）方法，实验表明其优于传统方法，能有效稳定训练且可轻量集成。


<details>
  <summary>Details</summary>
Motivation: 传统全局范数裁剪存在“溢出”效应，预设不同功能模块梯度均匀性有误，需克服梯度异质性问题。

Method: 将参数按功能类型分组，用指数移动平均（EMA）根据历史行为调节，构建自适应区间，采用时间依赖调度机制。

Result: 在多个模型上实验，AGGC 表现优于 LoRA 和全微调；在 GSM8K 基准上，AGGC 微调的 Mistral - 7B 准确率超 LoRA；有效稳定 RLVR，增强模型逻辑推理能力。

Conclusion: AGGC 有效解决传统梯度裁剪方法局限，利用模块化自适应裁剪策略稳定训练，轻量设计可无缝集成现有训练流程。

Abstract: To stabilize the training of Large Language Models (LLMs), gradient clipping is a nearly ubiquitous heuristic used to alleviate exploding gradients. However, traditional global norm clipping erroneously presupposes gradient homogeneity across different functional modules, leading to an adverse "spill-over" effect where volatile parameters force unnecessary scaling on stable ones. To overcome this, we propose Adaptive Group-wise Gradient Clipping (AGGC). AGGC partitions parameters into groups based on functional types and regulates each according to its historical behavior using an Exponential Moving Average (EMA). Specifically, it constructs an adaptive interval to simultaneously mitigate gradient explosion and vanishing, while employing a time-dependent scheduling mechanism to balance exploration and convergence. Experiments on LLaMA 2-7B, Mistral-7B, and Gemma-7B models show that AGGC consistently outperforms LoRA and frequently surpasses Full Fine-Tuning. On the GSM8K benchmark, Mistral-7B fine-tuned with AGGC achieves an accuracy of 72.93%, exceeding LoRA's 69.5%. AGGC also effectively stabilizes Reinforcement Learning with Verifiable Rewards (RLVR), enhancing the logic deduction of Qwen 2.5 and Llama 3.2 models. Experimental results demonstrate that AGGC effectively addresses the limitations of traditional gradient clipping methods, particularly in overcoming gradient heterogeneity, by utilizing a modular, adaptive clipping strategy to stabilize the training process. Due to its lightweight design, AGGC can be seamlessly integrated into existing post-training pipelines with negligible overhead.

</details>


### [240] [TF-CoDiT: Conditional Time Series Synthesis with Diffusion Transformers for Treasury Futures](https://arxiv.org/abs/2601.11880)
*Yingxiao Zhang,Jiaxin Duan,Junfu Zhang,Ke Feng*

Main category: cs.LG

TL;DR: 本文提出TF - CoDiT框架用于语言控制的国债期货数据合成，实验表明其能生成高真实性数据且具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有Diffusion Transformers在国债期货数据合成方面表现待挖掘，国债期货数据有低交易量、市场依赖和多变量相关性等特点，需新方法解决。

Method: 提出TF - CoDiT框架，将多通道1 - D时间序列转换为DWT系数矩阵，用U形VAE编码跨通道依赖，引入FinMAP推导涵盖关键条件的提示。

Result: 实验收集2015 - 2025年四种国债期货数据，TF - CoDiT生成数据与真实数据误差最大为MSE 0.433、MAE 0.453，且在不同合约和时间范围具鲁棒性。

Conclusion: TF - CoDiT能有效进行语言控制的国债期货数据合成，生成高真实性数据且具有鲁棒性。

Abstract: Diffusion Transformers (DiT) have achieved milestones in synthesizing financial time-series data, such as stock prices and order flows. However, their performance in synthesizing treasury futures data is still underexplored. This work emphasizes the characteristics of treasury futures data, including its low volume, market dependencies, and the grouped correlations among multivariables. To overcome these challenges, we propose TF-CoDiT, the first DiT framework for language-controlled treasury futures synthesis. To facilitate low-data learning, TF-CoDiT adapts the standard DiT by transforming multi-channel 1-D time series into Discrete Wavelet Transform (DWT) coefficient matrices. A U-shape VAE is proposed to encode cross-channel dependencies hierarchically into a latent variable and bridge the latent and DWT spaces through decoding, thereby enabling latent diffusion generation. To derive prompts that cover essential conditions, we introduce the Financial Market Attribute Protocol (FinMAP) - a multi-level description system that standardizes daily$/$periodical market dynamics by recognizing 17$/$23 economic indicators from 7/8 perspectives. In our experiments, we gather four types of treasury futures data covering the period from 2015 to 2025, and define data synthesis tasks with durations ranging from one week to four months. Extensive evaluations demonstrate that TF-CoDiT can produce highly authentic data with errors at most 0.433 (MSE) and 0.453 (MAE) to the ground-truth. Further studies evidence the robustness of TF-CoDiT across contracts and temporal horizons.

</details>


### [241] [Fairness-informed Pareto Optimization : An Efficient Bilevel Framework](https://arxiv.org/abs/2601.13448)
*Sofiane Tanji,Samuel Vaiter,Yassine Laguel*

Main category: cs.LG

TL;DR: 本文提出BADR框架以恢复任意公平性指标的最优帕累托有效模型，配备算法并证明收敛性，发布工具包，实验显示其优势。


<details>
  <summary>Details</summary>
Motivation: 现有公平机器学习方法存在问题，传统方法帕累托无效，现有帕累托有效方法有偏差且适应性差。

Method: 提出BADR框架，通过双层自适应重新标度程序恢复模型，配备BADR - GD和BADR - SGD算法。

Result: 建立算法的收敛性保证，发布开源Python工具包，通过实验证明BADR相较于现有方法有优势。

Conclusion: BADR框架能为任意公平性指标恢复最优帕累托有效模型，优于现有帕累托有效公平性方法。

Abstract: Despite their promise, fair machine learning methods often yield Pareto-inefficient models, in which the performance of certain groups can be improved without degrading that of others. This issue arises frequently in traditional in-processing approaches such as fairness-through-regularization. In contrast, existing Pareto-efficient approaches are biased towards a certain perspective on fairness and fail to adapt to the broad range of fairness metrics studied in the literature. In this paper, we present BADR, a simple framework to recover the optimal Pareto-efficient model for any fairness metric. Our framework recovers its models through a Bilevel Adaptive Rescalarisation procedure. The lower level is a weighted empirical risk minimization task where the weights are a convex combination of the groups, while the upper level optimizes the chosen fairness objective. We equip our framework with two novel large-scale, single-loop algorithms, BADR-GD and BADR-SGD, and establish their convergence guarantees. We release badr, an open-source Python toolbox implementing our framework for a variety of learning tasks and fairness metrics. Finally, we conduct extensive numerical experiments demonstrating the advantages of BADR over existing Pareto-efficient approaches to fairness.

</details>


### [242] [Approximation Algorithm for Constrained $k$-Center Clustering: A Local Search Approach](https://arxiv.org/abs/2601.11883)
*Chaoqi Jia,Longkun Guo,Kewen Liao,Zhigang Lu,Chao Chen,Jason Xue*

Main category: cs.LG

TL;DR: 研究带约束的k - 中心聚类问题，提出基于转换为支配匹配集问题的局部搜索框架，实验显示算法优于基线。


<details>
  <summary>Details</summary>
Motivation: 传统k - 中心问题有近似比限制，带CL和ML约束的k - 中心聚类问题中，局部搜索能否达到常数因子近似是开放问题。

Method: 提出基于转换为支配匹配集问题的局部搜索框架。

Result: 在真实和合成数据集上实验表明算法在解的质量上优于基线。

Conclusion: 提出的局部搜索框架能达到2的最佳近似比，且在解质量上表现更好。

Abstract: Clustering is a long-standing research problem and a fundamental tool in AI and data analysis. The traditional k-center problem, a fundamental theoretical challenge in clustering, has a best possible approximation ratio of 2, and any improvement to a ratio of 2 - ε would imply P = NP. In this work, we study the constrained k-center clustering problem, where instance-level cannot-link (CL) and must-link (ML) constraints are incorporated as background knowledge. Although general CL constraints significantly increase the hardness of approximation, previous work has shown that disjoint CL sets permit constant-factor approximations. However, whether local search can achieve such a guarantee in this setting remains an open question. To this end, we propose a novel local search framework based on a transformation to a dominating matching set problem, achieving the best possible approximation ratio of 2. The experimental results on both real-world and synthetic datasets demonstrate that our algorithm outperforms baselines in solution quality.

</details>


### [243] [Preconditioning Benefits of Spectral Orthogonalization in Muon](https://arxiv.org/abs/2601.13474)
*Jianhao Ma,Yu Huang,Yuejie Chi,Yuxin Chen*

Main category: cs.LG

TL;DR: 研究Muon优化器简化变体在矩阵分解和线性变压器上下文学习中的有效性，证明其优于梯度下降和Adam，揭示动力学特性。


<details>
  <summary>Details</summary>
Motivation: Muon优化器在预训练中有重要地位，但底层机制尤其是梯度正交化的作用鲜有人透彻分析。

Method: 通过矩阵分解和线性变压器上下文学习两个案例研究简化版Muon的有效性。

Result: 证明简化的Muon线性收敛，迭代复杂度与条件数无关，优于梯度下降和Adam，其动力学在谱域解耦为独立标量序列。

Conclusion: 理论明确了谱正交化的预条件效应，解释了Muon在矩阵优化问题中的有效性，并有潜在的更广泛应用价值。

Abstract: The Muon optimizer, a matrix-structured algorithm that leverages spectral orthogonalization of gradients, is a milestone in the pretraining of large language models. However, the underlying mechanisms of Muon -- particularly the role of gradient orthogonalization -- remain poorly understood, with very few works providing end-to-end analyses that rigorously explain its advantages in concrete applications. We take a step by studying the effectiveness of a simplified variant of Muon through two case studies: matrix factorization, and in-context learning of linear transformers. For both problems, we prove that simplified Muon converges linearly with iteration complexities independent of the relevant condition number, provably outperforming gradient descent and Adam. Our analysis reveals that the Muon dynamics decouple into a collection of independent scalar sequences in the spectral domain, each exhibiting similar convergence behavior. Our theory formalizes the preconditioning effect induced by spectral orthogonalization, offering insight into Muon's effectiveness in these matrix optimization problems and potentially beyond.

</details>


### [244] [From Relative Entropy to Minimax: A Unified Framework for Coverage in MDPs](https://arxiv.org/abs/2601.11890)
*Xihe Gu,Urbashi Mitra,Tara Javidi*

Main category: cs.LG

TL;DR: 提出基于状态 - 动作占用度量的加权参数化凹覆盖目标家族 $U_ρ$，开发基于梯度算法引导占用朝向期望覆盖模式，并分析了 $ρ$ 对探索策略的影响。


<details>
  <summary>Details</summary>
Motivation: 在无奖励马尔可夫决策问题中，对状态 - 动作对进行有针对性和刻意的探索很重要，需将不同状态 - 动作对的重要性或难度融入探索策略。

Method: 提出加权参数化的凹覆盖目标家族 $U_ρ$，利用其梯度的简单闭式开发基于梯度的算法。

Result: 开发出能引导诱导占用朝向期望覆盖模式的算法，且随着 $ρ$ 增加，探索策略更强调最少探索的状态 - 动作对。

Conclusion: 所提的 $U_ρ$ 家族统一了多个广泛研究的目标，基于其开发的算法能有效实现目标探索。

Abstract: Targeted and deliberate exploration of state--action pairs is essential in reward-free Markov Decision Problems (MDPs). More precisely, different state-action pairs exhibit different degree of importance or difficulty which must be actively and explicitly built into a controlled exploration strategy. To this end, we propose a weighted and parameterized family of concave coverage objectives, denoted by $U_ρ$, defined directly over state--action occupancy measures. This family unifies several widely studied objectives within a single framework, including divergence-based marginal matching, weighted average coverage, and worst-case (minimax) coverage. While the concavity of $U_ρ$ captures the diminishing return associated with over-exploration, the simple closed form of the gradient of $U_ρ$ enables an explicit control to prioritize under-explored state--action pairs. Leveraging this structure, we develop a gradient-based algorithm that actively steers the induced occupancy toward a desired coverage pattern. Moreover, we show that as $ρ$ increases, the resulting exploration strategy increasingly emphasizes the least-explored state--action pairs, recovering worst-case coverage behavior in the limit.

</details>


### [245] [Does Privacy Always Harm Fairness? Data-Dependent Trade-offs via Chernoff Information Neural Estimation](https://arxiv.org/abs/2601.13698)
*Arjun Nichani,Hsiang Hsu,Chun-Fu,Chen,Haewon Jeong*

Main category: cs.LG

TL;DR: 本文利用切尔诺夫信息揭示公平性、隐私性和准确性三者关系的数据依赖性，定义了噪声切尔诺夫差异工具，分析合成数据中该值的三种表现，探讨其公平性和隐私性含义，还提出估计未知分布数据切尔诺夫信息的方法并应用于真实数据集。


<details>
  <summary>Details</summary>
Motivation: 公平性和隐私性是可信机器学习的重要支柱，但二者关系研究较少，旨在统一理解公平性 - 隐私性 - 准确性的关系。

Method: 利用信息论测度切尔诺夫信息，定义噪声切尔诺夫差异工具，分析合成数据；提出估计未知分布数据切尔诺夫信息的方法用于真实数据集。

Result: 合成数据中噪声切尔诺夫差异有三种表现；该差异可作为公平性 - 准确性曲线陡度的代理。

Conclusion: 该研究有助于统一理解公平性 - 隐私性 - 准确性的关系，突出了其数据依赖性。

Abstract: Fairness and privacy are two vital pillars of trustworthy machine learning. Despite extensive research on these individual topics, the relationship between fairness and privacy has received significantly less attention. In this paper, we utilize the information-theoretic measure Chernoff Information to highlight the data-dependent nature of the relationship among the triad of fairness, privacy, and accuracy. We first define Noisy Chernoff Difference, a tool that allows us to analyze the relationship among the triad simultaneously. We then show that for synthetic data, this value behaves in 3 distinct ways (depending on the distribution of the data). We highlight the data distributions involved in these cases and explore their fairness and privacy implications. Additionally, we show that Noisy Chernoff Difference acts as a proxy for the steepness of the fairness-accuracy curves. Finally, we propose a method for estimating Chernoff Information on data from unknown distributions and utilize this framework to examine the triad dynamic on real datasets. This work builds towards a unified understanding of the fairness-privacy-accuracy relationship and highlights its data-dependent nature.

</details>


### [246] [DevBench: A Realistic, Developer-Informed Benchmark for Code Generation Models](https://arxiv.org/abs/2601.11895)
*Pareesa Ameneh Golnari,Adarsh Kumarappan,Wen Wen,Xiaoyu Liu,Gabriel Ryan,Yuting Sun,Shengyu Fu,Elsie Nallipogu*

Main category: cs.LG

TL;DR: DevBench可评估大语言模型代码补全能力，对9个模型评测，为模型选择和改进提供见解。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏在实际代码补全任务中有效评估大语言模型且能提供实用信息的基准测试。

Method: 构建含1800个评估实例、覆盖六种编程语言和任务类别的基准测试，结合功能正确性、相似度指标和大语言模型评判评估。

Result: 对9个最先进模型评估，发现它们在句法精度、语义推理和实用效用方面存在差异。

Conclusion: 该基准测试能为模型选择和改进提供可操作见解，对实际部署和模型开发至关重要。

Abstract: DevBench is a telemetry-driven benchmark designed to evaluate Large Language Models (LLMs) on realistic code completion tasks. It includes 1,800 evaluation instances across six programming languages and six task categories derived from real developer telemetry, such as API usage and code purpose understanding. Unlike prior benchmarks, it emphasizes ecological validity, avoids training data contamination, and enables detailed diagnostics. The evaluation combines functional correctness, similarity-based metrics, and LLM-judge assessments focused on usefulness and contextual relevance. 9 state-of-the-art models were assessed, revealing differences in syntactic precision, semantic reasoning, and practical utility. Our benchmark provides actionable insights to guide model selection and improvement-detail that is often missing from other benchmarks but is essential for both practical deployment and targeted model development.

</details>


### [247] [Orthogonium : A Unified, Efficient Library of Orthogonal and 1-Lipschitz Building Blocks](https://arxiv.org/abs/2601.13776)
*Thibaut Boissin,Franck Mamalet,Valentin Lafargue,Mathieu Serrurier*

Main category: cs.LG

TL;DR: 介绍了Orthogonium，一个统一、高效且全面的PyTorch库，提供正交和1 - Lipschitz层，降低使用门槛。


<details>
  <summary>Details</summary>
Motivation: 现有正交和1 - Lipschitz神经网络层的实现存在碎片化、受限和计算要求高的问题。

Method: 开发了PyTorch库Orthogonium，提供标准卷积特性并保证数学严谨性，进行优化实现和严格测试。

Result: 在大规模基准测试中减少开销，发现现有实现的关键错误。

Conclusion: Orthogonium显著降低采用障碍，可用于各种需正交性和强大Lipschitz约束的应用。

Abstract: Orthogonal and 1-Lipschitz neural network layers are essential building blocks in robust deep learning architectures, crucial for certified adversarial robustness, stable generative models, and reliable recurrent networks. Despite significant advancements, existing implementations remain fragmented, limited, and computationally demanding. To address these issues, we introduce Orthogonium , a unified, efficient, and comprehensive PyTorch library providing orthogonal and 1-Lipschitz layers. Orthogonium provides access to standard convolution features-including support for strides, dilation, grouping, and transposed-while maintaining strict mathematical guarantees. Its optimized implementations reduce overhead on large scale benchmarks such as ImageNet. Moreover, rigorous testing within the library has uncovered critical errors in existing implementations, emphasizing the importance of standardized and reliable tools. Orthogonium thus significantly lowers adoption barriers, enabling scalable experimentation and integration across diverse applications requiring orthogonality and robust Lipschitz constraints. Orthogonium is available at https://github.com/deel-ai/orthogonium.

</details>


### [248] [Inverting Self-Organizing Maps: A Unified Activation-Based Framework](https://arxiv.org/abs/2601.13851)
*Alessandro Londei,Matteo Benati,Denise Lanzieri,Vittorio Loreto*

Main category: cs.LG

TL;DR: 本文指出在一定几何条件下SOM激活模式可反演恢复输入，引入MUSIC更新规则实现潜在空间可控轨迹，不依赖采样等，通过多数据集验证其优势。


<details>
  <summary>Details</summary>
Motivation: 探索自组织映射（SOM）激活模式反演以实现潜在空间的可控探索和数据增强。

Method: 利用欧几里得距离几何经典事实推导线性系统，引入MUSIC更新规则，用Tikhonov正则化稳定规则。

Result: MUSIC可生成平滑、可解释轨迹，揭示学习流形的底层几何结构。

Conclusion: 基于SOM的反演在无监督聚类方面有优势，为数据增强和可控潜在探索提供新视角。

Abstract: Self-Organizing Maps provide topology-preserving projections of high-dimensional data and have been widely used for visualization, clustering, and vector quantization. In this work, we show that the activation pattern of a SOM - the squared distances to its prototypes - can be inverted to recover the exact input under mild geometric conditions. This follows from a classical fact in Euclidean distance geometry: a point in $D$ dimensions is uniquely determined by its distances to $D{+}1$ affinely independent references. We derive the corresponding linear system and characterize the conditions under which the inversion is well-posed. Building upon this mechanism, we introduce the Manifold-Aware Unified SOM Inversion and Control (MUSIC) update rule, which enables controlled, semantically meaningful trajectories in latent space. MUSIC modifies squared distances to selected prototypes while preserving others, resulting in a deterministic geometric flow aligned with the SOM's piecewise-linear structure. Tikhonov regularization stabilizes the update rule and ensures smooth motion on high-dimensional datasets. Unlike variational or probabilistic generative models, MUSIC does not rely on sampling, latent priors, or encoder-decoder architectures. If no perturbation is applied, inversion recovers the exact input; when a target cluster or prototype is specified, MUSIC produces coherent semantic variations while remaining on the data manifold. This leads to a new perspective on data augmentation and controllable latent exploration based solely on prototype geometry. We validate the approach using synthetic Gaussian mixtures, the MNIST and the Faces in the Wild dataset. Across all settings, MUSIC produces smooth, interpretable trajectories that reveal the underlying geometry of the learned manifold, illustrating the advantages of SOM-based inversion over unsupervised clustering.

</details>


### [249] [Communication-Corruption Coupling and Verification in Cooperative Multi-Objective Bandits](https://arxiv.org/abs/2601.11924)
*Ming Shi*

Main category: cs.LG

TL;DR: 研究对抗性干扰和有限验证下带向量值奖励的协作随机多臂老虎机问题，揭示通信与干扰的耦合关系，给出后悔界，确定信息论极限，分析验证观察预算对可学习性的影响。


<details>
  <summary>Details</summary>
Motivation: 在对抗性干扰和有限验证的情况下，研究协作随机多臂老虎机问题，以衡量团队后悔并优化性能。

Method: 通过协议诱导的多重性函数形式化通信 - 干扰耦合关系，证明参数化的后悔界。

Result: 发现固定干扰预算可转化为不同有效干扰水平；原始样本共享有N倍更大附加干扰惩罚，总结共享和仅推荐共享保持O(Γ)项；确定信息论极限，包括不可避免的附加Ω(Γ)惩罚和高干扰下无干净信息无法实现次线性后悔；刻画验证观察预算恢复可学习性的情况。

Conclusion: 验证在高干扰情况下是必要的，达到识别阈值后是充分的，认证共享可使团队后悔与干扰无关。

Abstract: We study cooperative stochastic multi-armed bandits with vector-valued rewards under adversarial corruption and limited verification. In each of $T$ rounds, each of $N$ agents selects an arm, the environment generates a clean reward vector, and an adversary perturbs the observed feedback subject to a global corruption budget $Γ$. Performance is measured by team regret under a coordinate-wise nondecreasing, $L$-Lipschitz scalarization $φ$, covering linear, Chebyshev, and smooth monotone utilities. Our main contribution is a communication-corruption coupling: we show that a fixed environment-side budget $Γ$ can translate into an effective corruption level ranging from $Γ$ to $NΓ$, depending on whether agents share raw samples, sufficient statistics, or only arm recommendations. We formalize this via a protocol-induced multiplicity functional and prove regret bounds parameterized by the resulting effective corruption. As corollaries, raw-sample sharing can suffer an $N$-fold larger additive corruption penalty, whereas summary sharing and recommendation-only sharing preserve an unamplified $O(Γ)$ term and achieve centralized-rate team regret. We further establish information-theoretic limits, including an unavoidable additive $Ω(Γ)$ penalty and a high-corruption regime $Γ=Θ(NT)$ where sublinear regret is impossible without clean information. Finally, we characterize how a global budget $ν$ of verified observations restores learnability. That is, verification is necessary in the high-corruption regime, and sufficient once it crosses the identification threshold, with certified sharing enabling the team's regret to become independent of $Γ$.

</details>


### [250] [Trainability-Oriented Hybrid Quantum Regression via Geometric Preconditioning and Curriculum Optimization](https://arxiv.org/abs/2601.11942)
*Qingyu Meng,Yangshuai Wang*

Main category: cs.LG

TL;DR: 提出混合量子 - 经典回归框架解决量子神经网络在回归中的问题，在基准测试中表现更好且收敛更稳定。


<details>
  <summary>Details</summary>
Motivation: 量子神经网络在回归中存在噪声梯度下可训练性有限和优化病态问题。

Method: 提出混合量子 - 经典回归框架，前置轻量级经典嵌入作为可学习的几何预调节器，并引入课程优化协议。

Result: 在固定训练预算下，该框架在偏微分方程回归基准测试和标准回归数据集上优于纯量子神经网络基线，且在数据有限时收敛更稳定。

Conclusion: 几何预调节与课程训练结合是稳定量子回归的实用方法。

Abstract: Quantum neural networks (QNNs) have attracted growing interest for scientific machine learning, yet in regression settings they often suffer from limited trainability under noisy gradients and ill-conditioned optimization. We propose a hybrid quantum-classical regression framework designed to mitigate these bottlenecks. Our model prepends a lightweight classical embedding that acts as a learnable geometric preconditioner, reshaping the input representation to better condition a downstream variational quantum circuit. Building on this architecture, we introduce a curriculum optimization protocol that progressively increases circuit depth and transitions from SPSA-based stochastic exploration to Adam-based gradient fine-tuning. We evaluate the approach on PDE-informed regression benchmarks and standard regression datasets under a fixed training budget in a simulator setting. Empirically, the proposed framework consistently improves over pure QNN baselines and yields more stable convergence in data-limited regimes. We further observe reduced structured errors that are visually correlated with oscillatory components on several scientific benchmarks, suggesting that geometric preconditioning combined with curriculum training is a practical approach for stabilizing quantum regression.

</details>


### [251] [Penalizing Localized Dirichlet Energies in Low Rank Tensor Products](https://arxiv.org/abs/2601.14173)
*Paris A. Karakasis,Nicholas D. Sidiropoulos*

Main category: cs.LG

TL;DR: 研究低秩张量积B样条（TPBS）模型用于回归任务，提出基于局部狄利克雷能量的正则化策略和两个不完全样本推理估计器，实验显示TPBS模型在过拟合场景表现更好。


<details>
  <summary>Details</summary>
Motivation: 传统基于全局狄利克雷能量的正则化在TPBS模型中无效，需要新的正则化策略。

Method: 提出基于训练点为中心的小超立方体上的局部狄利克雷能量的正则化策略，引入两个基于预训练TPBS模型的不完全样本推理估计器。

Result: TPBS模型在多数数据集的过拟合情况下优于神经网络，其他情况下表现相当。

Conclusion: TPBS模型对过拟合更具鲁棒性，能从正则化中受益，而神经网络对过拟合更敏感，正则化效果较差。

Abstract: We study low-rank tensor-product B-spline (TPBS) models for regression tasks and investigate Dirichlet energy as a measure of smoothness. We show that TPBS models admit a closed-form expression for the Dirichlet energy, and reveal scenarios where perfect interpolation is possible with exponentially small Dirichlet energy. This renders global Dirichlet energy-based regularization ineffective. To address this limitation, we propose a novel regularization strategy based on local Dirichlet energies defined on small hypercubes centered at the training points. Leveraging pretrained TPBS models, we also introduce two estimators for inference from incomplete samples. Comparative experiments with neural networks demonstrate that TPBS models outperform neural networks in the overfitting regime for most datasets, and maintain competitive performance otherwise. Overall, TPBS models exhibit greater robustness to overfitting and consistently benefit from regularization, while neural networks are more sensitive to overfitting and less effective in leveraging regularization.

</details>


### [252] [Controlling Underestimation Bias in Constrained Reinforcement Learning for Safe Exploration](https://arxiv.org/abs/2601.11953)
*Shiqing Gao,Jiaxin Ding,Luoyi Fu,Xinbing Wang*

Main category: cs.LG

TL;DR: 文章指出现有约束强化学习算法训练时约束违反问题，提出MICE方法，通过引入内在成本等降低约束违反，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有约束强化学习算法在训练时存在显著约束违反问题，限制其在安全关键场景的应用。

Method: 提出MICE方法，引入内在成本缓解低估问题，构建记忆模块存储不安全状态，提出内外在成本价值函数并采用偏差校正策略，在信任区域内制定优化目标及方法。

Result: 理论上为成本价值函数提供收敛保证，确定MICE更新的最坏情况约束违反；实验表明MICE显著减少约束违反，策略性能与基线相当。

Conclusion: MICE方法能有效减少约束强化学习中的约束违反，同时保持较好的策略性能。

Abstract: Constrained Reinforcement Learning (CRL) aims to maximize cumulative rewards while satisfying constraints. However, existing CRL algorithms often encounter significant constraint violations during training, limiting their applicability in safety-critical scenarios. In this paper, we identify the underestimation of the cost value function as a key factor contributing to these violations. To address this issue, we propose the Memory-driven Intrinsic Cost Estimation (MICE) method, which introduces intrinsic costs to mitigate underestimation and control bias to promote safer exploration. Inspired by flashbulb memory, where humans vividly recall dangerous experiences to avoid risks, MICE constructs a memory module that stores previously explored unsafe states to identify high-cost regions. The intrinsic cost is formulated as the pseudo-count of the current state visiting these risk regions. Furthermore, we propose an extrinsic-intrinsic cost value function that incorporates intrinsic costs and adopts a bias correction strategy. Using this function, we formulate an optimization objective within the trust region, along with corresponding optimization methods. Theoretically, we provide convergence guarantees for the proposed cost value function and establish the worst-case constraint violation for the MICE update. Extensive experiments demonstrate that MICE significantly reduces constraint violations while preserving policy performance comparable to baselines.

</details>


### [253] [Q-learning with Adjoint Matching](https://arxiv.org/abs/2601.14234)
*Qiyang Li,Sergey Levine*

Main category: cs.LG

TL;DR: 提出QAM算法应对连续动作RL中的挑战，结合TD backup，在离线和离线到在线RL硬稀疏奖励任务上优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 连续动作强化学习中，有效优化基于参数化Q函数的表现性扩散或流匹配策略是长期难题，现有方法存在不足。

Method: 利用生成模型中的伴随匹配技术，转换批评家的动作梯度，形成无不稳定反向传播的逐阶目标函数，结合时序差分备份进行批评家学习。

Result: QAM在离线和离线到在线RL的硬稀疏奖励任务上始终优于先前方法。

Conclusion: QAM算法有效解决了连续动作RL中的挑战，表现出色。

Abstract: We propose Q-learning with Adjoint Matching (QAM), a novel TD-based reinforcement learning (RL) algorithm that tackles a long-standing challenge in continuous-action RL: efficient optimization of an expressive diffusion or flow-matching policy with respect to a parameterized Q-function. Effective optimization requires exploiting the first-order information of the critic, but it is challenging to do so for flow or diffusion policies because direct gradient-based optimization via backpropagation through their multi-step denoising process is numerically unstable. Existing methods work around this either by only using the value and discarding the gradient information, or by relying on approximations that sacrifice policy expressivity or bias the learned policy. QAM sidesteps both of these challenges by leveraging adjoint matching, a recently proposed technique in generative modeling, which transforms the critic's action gradient to form a step-wise objective function that is free from unstable backpropagation, while providing an unbiased, expressive policy at the optimum. Combined with temporal-difference backup for critic learning, QAM consistently outperforms prior approaches on hard, sparse reward tasks in both offline and offline-to-online RL.

</details>


### [254] [Data-centric Prompt Tuning for Dynamic Graphs](https://arxiv.org/abs/2601.11954)
*Yufei Peng,Cheng Yang,Zhengjie Fan,Chuan Shi*

Main category: cs.LG

TL;DR: 传统动态图方法在下游任务存在性能问题，现有提示方法有局限性，本文提出DDGPrompt框架优化预训练节点嵌入，在少样本设置下表现优于传统和提示方法。


<details>
  <summary>Details</summary>
Motivation: 传统动态图节点时间嵌入应用于下游任务存在性能下降问题，现有提示方法与特定模型架构或预训练任务强耦合，且忽略空间结构信息，表现有限。

Method: 定义统一节点表达特征矩阵，引入三种提示矩阵调整特征矩阵，实现节点嵌入的任务特定适应。

Result: 在四个公共动态图数据集的严格少样本设置下评估，该方法在有限标签和冷启动条件下显著优于传统方法和提示方法。

Conclusion: DDGPrompt框架能有效优化预训练节点嵌入，提高对不同下游任务的适应性。

Abstract: Dynamic graphs have attracted increasing attention due to their ability to model complex and evolving relationships in real-world scenarios. Traditional approaches typically pre-train models using dynamic link prediction and directly apply the resulting node temporal embeddings to specific downstream tasks. However, the significant differences among downstream tasks often lead to performance degradation, especially under few-shot settings. Prompt tuning has emerged as an effective solution to this problem. Existing prompting methods are often strongly coupled with specific model architectures or pretraining tasks, which makes it difficult to adapt to recent or future model designs. Moreover, their exclusive focus on modifying node or temporal features while neglecting spatial structural information leads to limited expressiveness and degraded performance. To address these limitations, we propose DDGPrompt, a data-centric prompting framework designed to effectively refine pre-trained node embeddings at the input data level, enabling better adaptability to diverse downstream tasks. We first define a unified node expression feature matrix that aggregates all relevant temporal and structural information of each node, ensuring compatibility with a wide range of dynamic graph models. Then, we introduce three prompt matrices (temporal bias, edge weight, and feature mask) to adjust the feature matrix completely, achieving task-specific adaptation of node embeddings. We evaluate DDGPrompt under a strict few-shot setting on four public dynamic graph datasets. Experimental results demonstrate that our method significantly outperforms traditional methods and prompting approaches in scenarios with limited labels and cold-start conditions.

</details>


### [255] [R$^2$PO: Decoupling Training Trajectories from Inference Responses for LLM Reasoning](https://arxiv.org/abs/2601.11960)
*Jingchu Wang,Bingbing Xu,Yige Yuan,Bin Xie,Xiaoqian Sun,Huawei Shen*

Main category: cs.LG

TL;DR: 本文提出R²PO方法解决强化学习中单一策略问题，实验表明该方法优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法用单一策略生成推理响应和训练优化轨迹，目标冲突导致探索不足，损害推理能力。

Method: 提出R²PO方法，在策略之上引入轻量级残差滚动头，将训练轨迹与推理响应解耦。

Result: 在多个基准测试中，该方法始终优于基线模型，在MATH - 500上平均准确率提高3.1%，在APPS上提高2.4%，减少格式错误并缓解长度偏差。

Conclusion: R²PO方法有效，代码已开源。

Abstract: Reinforcement learning has become a central paradigm for improving LLM reasoning. However, existing methods use a single policy to produce both inference responses and training optimization trajectories. The objective conflict between generating stable inference responses and diverse training trajectories leads to insufficient exploration, which harms reasoning capability. In this paper, to address the problem, we propose R$^2$PO (Residual Rollout Policy Optimization), which introduces a lightweight Residual Rollout-Head atop the policy to decouple training trajectories from inference responses, enabling controlled trajectory diversification during training while keeping inference generation stable. Experiments across multiple benchmarks show that our method consistently outperforms baselines, achieving average accuracy gains of 3.1% on MATH-500 and 2.4% on APPS, while also reducing formatting errors and mitigating length bias for stable optimization. Our code is publicly available at https://github.com/RRPO-ARR/Code.

</details>


### [256] [One-Shot Price Forecasting with Covariate-Guided Experts under Privacy Constraints](https://arxiv.org/abs/2601.11977)
*Ren He,Yinliang Xu,Jinfeng Wang,Jeremy Watson,Jian Song*

Main category: cs.LG

TL;DR: 提出MoE Encoder模块增强预训练预测模型，提升电力系统多元时间序列预测准确性，支持联邦环境下训练和参数共享。


<details>
  <summary>Details</summary>
Motivation: 电力系统预测涉及复杂多元时间序列，传统方法需大量专业知识且泛化性差，预训练模型零样本性能有限。

Method: 提出MoE Encoder模块，在分词和编码间添加稀疏专家混合层，将多元预测转化为专家引导的一元任务，支持联邦环境下局部训练和轻量级参数共享。

Result: 在公共多元数据集上显著提高预测准确性，在模拟联邦环境中仅转移MoE Encoder参数可有效适应新区域，性能损失小。

Conclusion: MoE Encoder为基础时间序列模型提供了可扩展且注重隐私的扩展。

Abstract: Forecasting in power systems often involves multivariate time series with complex dependencies and strict privacy constraints across regions. Traditional forecasting methods require significant expert knowledge and struggle to generalize across diverse deployment scenarios. Recent advancements in pre-trained time series models offer new opportunities, but their zero-shot performance on domain-specific tasks remains limited. To address these challenges, we propose a novel MoE Encoder module that augments pretrained forecasting models by injecting a sparse mixture-of-experts layer between tokenization and encoding. This design enables two key capabilities: (1) trans forming multivariate forecasting into an expert-guided univariate task, allowing the model to effectively capture inter-variable relations, and (2) supporting localized training and lightweight parameter sharing in federated settings where raw data cannot be exchanged. Extensive experiments on public multivariate datasets demonstrate that MoE-Encoder significantly improves forecasting accuracy compared to strong baselines. We further simulate federated environments and show that transferring only MoE-Encoder parameters allows efficient adaptation to new regions, with minimal performance degradation. Our findings suggest that MoE-Encoder provides a scalable and privacy-aware extension to foundation time series models.

</details>


### [257] [Extreme Value Policy Optimization for Safe Reinforcement Learning](https://arxiv.org/abs/2601.12008)
*Shiqing Gao,Yihang Zhou,Shuai Shao,Haoyu Luo,Yiheng Bing,Jiaxin Ding,Luoyi Fu,Xinbing Wang*

Main category: cs.LG

TL;DR: 文章指出传统基于期望的约束强化学习忽略极端事件，提出EVO算法，理论证明其优势，实验表明能显著减少约束违反。


<details>
  <summary>Details</summary>
Motivation: 传统基于期望的约束强化学习会忽略尾部的极端事件，导致严重的约束违反，需要新方法解决。

Method: 提出EVO算法，利用极值理论建模和利用极端样本，引入极端分位数优化目标，提出极端优先机制，理论上建立约束违反的上限。

Result: EVO比基于期望的方法有更低的约束违反概率，比分位数回归方法有更低的方差。

Conclusion: EVO能在训练中显著减少约束违反，同时保持与基线相当的策略性能。

Abstract: Ensuring safety is a critical challenge in applying Reinforcement Learning (RL) to real-world scenarios. Constrained Reinforcement Learning (CRL) addresses this by maximizing returns under predefined constraints, typically formulated as the expected cumulative cost. However, expectation-based constraints overlook rare but high-impact extreme value events in the tail distribution, such as black swan incidents, which can lead to severe constraint violations. To address this issue, we propose the Extreme Value policy Optimization (EVO) algorithm, leveraging Extreme Value Theory (EVT) to model and exploit extreme reward and cost samples, reducing constraint violations. EVO introduces an extreme quantile optimization objective to explicitly capture extreme samples in the cost tail distribution. Additionally, we propose an extreme prioritization mechanism during replay, amplifying the learning signal from rare but high-impact extreme samples. Theoretically, we establish upper bounds on expected constraint violations during policy updates, guaranteeing strict constraint satisfaction at a zero-violation quantile level. Further, we demonstrate that EVO achieves a lower probability of constraint violations than expectation-based methods and exhibits lower variance than quantile regression methods. Extensive experiments show that EVO significantly reduces constraint violations during training while maintaining competitive policy performance compared to baselines.

</details>


### [258] [Why Loss Re-weighting Works If You Stop Early: Training Dynamics of Unconstrained Features](https://arxiv.org/abs/2601.12011)
*Yize Zhao,Christos Thrampoulidis*

Main category: cs.LG

TL;DR: 研究损失重加权在现代深度学习中的应用，引入小规模模型（SSM）分析现象，发现重加权能恢复平衡学习动态。


<details>
  <summary>Details</summary>
Motivation: 展示和分析损失重加权在过参数化深度神经网络训练中早期有益但不改变终端学习阶段这一现象。

Method: 引入小规模模型（SSM），抽象DNN架构和输入数据复杂性，保留不平衡结构关键信息。

Result: SSM显示普通经验风险最小化早期优先学习区分多数类，延迟少数类学习，重加权能恢复平衡学习动态。

Conclusion: 损失重加权在深度学习早期训练中对平衡学习有积极作用。

Abstract: The application of loss reweighting in modern deep learning presents a nuanced picture. While it fails to alter the terminal learning phase in overparameterized deep neural networks (DNNs) trained on high-dimensional datasets, empirical evidence consistently shows it offers significant benefits early in training. To transparently demonstrate and analyze this phenomenon, we introduce a small-scale model (SSM). This model is specifically designed to abstract the inherent complexities of both the DNN architecture and the input data, while maintaining key information about the structure of imbalance within its spectral components. On the one hand, the SSM reveals how vanilla empirical risk minimization preferentially learns to distinguish majority classes over minorities early in training, consequently delaying minority learning. In stark contrast, reweighting restores balanced learning dynamics, enabling the simultaneous learning of features associated with both majorities and minorities.

</details>


### [259] [CooperLLM: Cloud-Edge-End Cooperative Federated Fine-tuning for LLMs via ZOO-based Gradient Correction](https://arxiv.org/abs/2601.12917)
*He Sun,Jinrui Zhou,Li Li,Mingjun Xiao*

Main category: cs.LG

TL;DR: 论文提出CooperLLM框架解决大语言模型在资源受限移动设备上微调难题，结合移动设备零阶优化与云引导梯度校正，实验显示有显著效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在资源受限移动设备上微调因高内存和计算成本面临挑战，现有联邦学习方法存在不足，需更好方案实现隐私保护个性化。

Method: 提出云辅助边端协同联邦微调框架CooperLLM，移动设备用零阶优化更新，云用反向传播微调并注入引导扰动，还有管道调度和自适应压缩。

Result: 实验表明CooperLLM在多个Transformer模型和数据集上，最多减少86.4%设备内存，加速收敛8.8倍，提高准确率10个百分点。

Conclusion: CooperLLM能有效解决大语言模型在资源受限设备上微调问题，改善收敛速度和准确率且不侵犯隐私。

Abstract: Large Language Models (LLMs) perform well on many NLP tasks, but fine-tuning them on resource-constrained mobile devices is challenging due to high memory and computation costs, despite growing demands for privacy-preserving personalization. Federated Learning (FL) enables local-data training, yet existing methods either rely on memory-intensive backpropagation or use zeroth-order optimization (ZOO), which avoids backward passes but suffers from slow convergence and degraded accuracy. We propose CooperLLM, a cloud-assisted edge-end cooperative federated fine-tuning framework that combines ZOO on mobile devices with cloud-guided gradient rectification. Mobile clients perform lightweight ZOO updates on private data, while the cloud fine-tunes on auxiliary public data using backpropagation and injects guided perturbations to rectify local updates, improving convergence and accuracy without violating privacy. To address system bottlenecks, CooperLLM introduces pipeline scheduling and adaptive compression to overlap computation and communication and reduce memory usage. Experiments on multiple Transformer models and datasets show that CooperLLM reduces on-device memory by up to $86.4\%$, accelerates convergence by $8.8 \times$, and improves accuracy by up to 10 percentage points over state-of-the-art ZOO-based baselines.

</details>


### [260] [Learning to Factorize and Adapt: A Versatile Approach Toward Universal Spatio-Temporal Foundation Models](https://arxiv.org/abs/2601.12083)
*Siru Zhong,Junjie Qiu,Yangyu Wu,Yiqiu Liu,Yuanpeng He,Zhongwen Rao,Bin Yang,Chenjuan Guo,Hao Xu,Yuxuan Liang*

Main category: cs.LG

TL;DR: 提出增强的因子化框架FactoST - v2用于时空基础模型，解耦通用时间学习和特定领域空间适应，在多领域评估中表现优异。


<details>
  <summary>Details</summary>
Motivation: 时空基础模型联合时空预训练计算成本高，且面临特定领域空间模式异质性问题。

Method: FactoST - v2分两阶段，第一阶段用随机序列掩码预训练极简仅编码器骨干网络；第二阶段用精简适配器通过元自适应学习和提示注入空间意识。

Result: FactoST - v2在多领域评估中达到了最先进的准确性和线性效率，在零样本和少样本场景中显著优于现有基础模型，与特定领域专家基线相媲美。

Conclusion: 这种因子化范式为实现真正通用的时空基础模型提供了实用、可扩展的途径。

Abstract: Spatio-Temporal (ST) Foundation Models (STFMs) promise cross-dataset generalization, yet joint ST pretraining is computationally expensive and grapples with the heterogeneity of domain-specific spatial patterns. Substantially extending our preliminary conference version, we present FactoST-v2, an enhanced factorized framework redesigned for full weight transfer and arbitrary-length generalization. FactoST-v2 decouples universal temporal learning from domain-specific spatial adaptation. The first stage pretrains a minimalist encoder-only backbone using randomized sequence masking to capture invariant temporal dynamics, enabling probabilistic quantile prediction across variable horizons. The second stage employs a streamlined adapter to rapidly inject spatial awareness via meta adaptive learning and prompting. Comprehensive evaluations across diverse domains demonstrate that FactoST-v2 achieves state-of-the-art accuracy with linear efficiency - significantly outperforming existing foundation models in zero-shot and few-shot scenarios while rivaling domain-specific expert baselines. This factorized paradigm offers a practical, scalable path toward truly universal STFMs. Code is available at https://github.com/CityMind-Lab/FactoST.

</details>


### [261] [Mitigating Cultural Bias in LLMs via Multi-Agent Cultural Debate](https://arxiv.org/abs/2601.12091)
*Qian Tan,Lei Jiang,Yuting Zeng,Shuoyang Ding,Xiaohua Xu*

Main category: cs.LG

TL;DR: 论文引入CEBiasBench和MAV评估大语言模型偏差，发现中文提示不能消除偏差，提出MACD缓解偏差，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在评估和缓解大语言模型西方中心偏差方面存在不足，需研究非西方语言提示能否缓解偏差。

Method: 引入CEBiasBench和MAV进行评估，提出MACD训练-free框架缓解偏差。

Result: 中文提示仅转移偏差，MACD在CEBiasBench和CAMeL基准上表现良好。

Conclusion: 代理框架中明确的文化表征对跨文化公平至关重要。

Abstract: Large language models (LLMs) exhibit systematic Western-centric bias, yet whether prompting in non-Western languages (e.g., Chinese) can mitigate this remains understudied. Answering this question requires rigorous evaluation and effective mitigation, but existing approaches fall short on both fronts: evaluation methods force outputs into predefined cultural categories without a neutral option, while mitigation relies on expensive multi-cultural corpora or agent frameworks that use functional roles (e.g., Planner--Critique) lacking explicit cultural representation. To address these gaps, we introduce CEBiasBench, a Chinese--English bilingual benchmark, and Multi-Agent Vote (MAV), which enables explicit ``no bias'' judgments. Using this framework, we find that Chinese prompting merely shifts bias toward East Asian perspectives rather than eliminating it. To mitigate such persistent bias, we propose Multi-Agent Cultural Debate (MACD), a training-free framework that assigns agents distinct cultural personas and orchestrates deliberation via a "Seeking Common Ground while Reserving Differences" strategy. Experiments demonstrate that MACD achieves 57.6% average No Bias Rate evaluated by LLM-as-judge and 86.0% evaluated by MAV (vs. 47.6% and 69.0% baseline using GPT-4o as backbone) on CEBiasBench and generalizes to the Arabic CAMeL benchmark, confirming that explicit cultural representation in agent frameworks is essential for cross-cultural fairness.

</details>


### [262] [Federated Learning Under Temporal Drift -- Mitigating Catastrophic Forgetting via Experience Replay](https://arxiv.org/abs/2601.13456)
*Sahasra Kokkula,Daniel David,Aaditya Baruah*

Main category: cs.LG

TL;DR: 标准FedAvg在季节性漂移下会灾难性遗忘，提出客户端经验回放方法可防止遗忘，且有内存 - 准确率权衡。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在时间概念漂移下客户端数据分布随时间变化的问题，标准FedAvg在季节性漂移下存在灾难性遗忘。

Method: 提出客户端经验回放方法，每个客户端维护一个包含过去样本的小缓冲区，在本地训练时与当前数据混合，且无需更改服务器聚合。

Result: 每个类别50个样本的缓冲区可将性能恢复到78 - 82%，有效防止遗忘；消融研究显示随着缓冲区大小增加存在明显的内存 - 准确率权衡。

Conclusion: 客户端经验回放方法能有效解决联邦学习在季节性漂移下的灾难性遗忘问题。

Abstract: Federated Learning struggles under temporal concept drift where client data distributions shift over time. We demonstrate that standard FedAvg suffers catastrophic forgetting under seasonal drift on Fashion-MNIST, with accuracy dropping from 74% to 28%. We propose client-side experience replay, where each client maintains a small buffer of past samples mixed with current data during local training. This simple approach requires no changes to server aggregation. Experiments show that a 50-sample-per-class buffer restores performance to 78-82%, effectively preventing forgetting. Our ablation study reveals a clear memory-accuracy trade-off as buffer size increases.

</details>


### [263] [PTL-PINNs: Perturbation-Guided Transfer Learning with Physics- Informed Neural Networks for Nonlinear Systems](https://arxiv.org/abs/2601.12093)
*Duarte Alexandrino,Ben Moseley,Pavlos Protopapas*

Main category: cs.LG

TL;DR: 提出PTL - PINN框架解决PINNs在求解非线性微分方程的局限，速度快且精度与经典方法相当。


<details>
  <summary>Details</summary>
Motivation: PINNs在建模非线性动力学时存在泛化能力有限和训练时间长的问题，需要改进。

Method: 提出PTL - PINN框架，将微扰理论与迁移学习结合，用封闭形式表达式求解近似线性微扰系统。

Result: PTL - PINNs精度与多种Runge - Kutta方法相当，计算速度快一个数量级，求解了一系列问题。

Conclusion: 将微扰方法与PINNs结合，表明微扰理论可指导基础模型快速求解非线性系统。

Abstract: Accurately and efficiently solving nonlinear differential equations is crucial for modeling dynamic behavior across science and engineering. Physics-Informed Neural Networks (PINNs) have emerged as a powerful solution that embeds physical laws in training by enforcing equation residuals. However, these struggle to model nonlinear dynamics, suffering from limited generalization across problems and long training times. To address these limitations, we propose a perturbation-guided transfer learning framework for PINNs (PTL-PINN), which integrates perturbation theory with transfer learning to efficiently solve nonlinear equations. Unlike gradient-based transfer learning, PTL-PINNs solve an approximate linear perturbative system using closed-form expressions, enabling rapid generalization with the time complexity of matrix-vector multiplication. We show that PTL-PINNs achieve accuracy comparable to various Runge-Kutta methods, with computational speeds up to one order of magnitude faster. To benchmark performance, we solve a broad set of problems, including nonlinear oscillators across various damping regimes, the equilibrium-centered Lotka-Volterra system, the KPP-Fisher and the Wave equation. Since perturbation theory sets the accuracy bound of PTL-PINNs, we systematically evaluate its practical applicability. This work connects long-standing perturbation methods with PINNs, demonstrating how perturbation theory can guide foundational models to solve nonlinear systems with speeds comparable to those of classical solvers.

</details>


### [264] [Neural Isomorphic Fields: A Transformer-based Algebraic Numerical Embedding](https://arxiv.org/abs/2601.12095)
*Hamidreza Sadeghi,Saeedeh Momtazi,Reza Safabakhsh*

Main category: cs.LG

TL;DR: 本文针对神经网络处理数字时的数值不稳定问题，提出用嵌入向量代替原始数值，实验显示加法运算保代数性质效果好，乘法运算有待提升。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络模型处理极小或极大数时存在的溢出、下溢和输出变化不稳定等问题。

Method: 引入能保留有理数域代数运算的固定长度数字嵌入向量，提出神经同构场，其元素为在计算中保持代数结构的嵌入向量。

Result: 加法在关键代数测试上准确率超 95%，乘法在不同代数性质上准确率为 53% - 73%。

Conclusion: 模型在加法保代数性质方面有优势，处理乘法有改进空间。

Abstract: Neural network models often face challenges when processing very small or very large numbers due to issues such as overflow, underflow, and unstable output variations. To mitigate these problems, we propose using embedding vectors for numbers instead of directly using their raw values. These embeddings aim to retain essential algebraic properties while preventing numerical instabilities. In this paper, we introduce, for the first time, a fixed-length number embedding vector that preserves algebraic operations, including addition, multiplication, and comparison, within the field of rational numbers. We propose a novel Neural Isomorphic Field, a neural abstraction of algebraic structures such as groups and fields. The elements of this neural field are embedding vectors that maintain algebraic structure during computations. Our experiments demonstrate that addition performs exceptionally well, achieving over 95 percent accuracy on key algebraic tests such as identity, closure, and associativity. In contrast, multiplication exhibits challenges, with accuracy ranging from 53 percent to 73 percent across various algebraic properties. These findings highlight the model's strengths in preserving algebraic properties under addition while identifying avenues for further improvement in handling multiplication.

</details>


### [265] [SynQP: A Framework and Metrics for Evaluating the Quality and Privacy Risk of Synthetic Data](https://arxiv.org/abs/2601.12124)
*Bing Hu,Yixin Li,Asma Bahamyirou,Helen Chen*

Main category: cs.LG

TL;DR: 介绍了SynQP开放框架用于合成数据生成中的隐私基准测试，提出新的身份披露风险指标，为健康应用中合成数据隐私评估提供工具。


<details>
  <summary>Details</summary>
Motivation: 合成数据在健康应用中的隐私评估缺乏开放框架，且缺少可访问的基准数据集，阻碍了其应用。

Method: 引入SynQP框架，使用模拟敏感数据进行隐私基准测试，提出新的身份披露风险指标。

Result: 使用SynQP对CTGAN进行基准测试，新指标能更准确估计隐私风险；非私有模型机器学习效果接近完美，差分隐私能降低身份披露和成员推理攻击风险。

Conclusion: 工作为提高隐私评估的透明度和可靠性提供关键工具，使合成数据在健康应用中使用更安全。

Abstract: The use of synthetic data in health applications raises privacy concerns, yet the lack of open frameworks for privacy evaluations has slowed its adoption. A major challenge is the absence of accessible benchmark datasets for evaluating privacy risks, due to difficulties in acquiring sensitive data. To address this, we introduce SynQP, an open framework for benchmarking privacy in synthetic data generation (SDG) using simulated sensitive data, ensuring that original data remains confidential. We also highlight the need for privacy metrics that fairly account for the probabilistic nature of machine learning models. As a demonstration, we use SynQP to benchmark CTGAN and propose a new identity disclosure risk metric that offers a more accurate estimation of privacy risks compared to existing approaches. Our work provides a critical tool for improving the transparency and reliability of privacy evaluations, enabling safer use of synthetic data in health-related applications. % In our quality evaluations, non-private models achieved near-perfect machine-learning efficacy \(\ge0.97\). Our privacy assessments (Table II) reveal that DP consistently lowers both identity disclosure risk (SD-IDR) and membership-inference attack risk (SD-MIA), with all DP-augmented models staying below the 0.09 regulatory threshold. Code available at https://github.com/CAN-SYNH/SynQP

</details>


### [266] [SolarGPT-QA: A Domain-Adaptive Large Language Model for Educational Question Answering in Space Weather and Heliophysics](https://arxiv.org/abs/2601.12131)
*Santosh Chapagain,MohammadReza EskandariNasab,Onur Vural,Shah Muhammad Hamdi,Soukaina Filali Boubrahimi*

Main category: cs.LG

TL;DR: 介绍基于领域适配大语言模型的问答系统SolarGPT - QA，其在太空科学教育解释上表现良好，为太空科学教育和预测的SolarGPT框架迈出首步。


<details>
  <summary>Details</summary>
Motivation: 太阳活动影响大，准确预测及有效太空科学教育很重要，但大语言模型缺乏领域知识和教学能力，因此需开发新系统。

Method: 基于LLaMA - 3构建领域适配大语言模型SolarGPT - QA，用科学文献和GPT - 4生成的问答数据训练，并用Grok - 3优化成学生友好风格。

Result: 人类成对评估显示SolarGPT - QA在零样本设置中优于通用模型，与指令调优模型竞争；学生理解研究表明解释更清晰易懂；消融实验表明领域适应预训练和教学微调结合很重要。

Conclusion: 此工作是迈向更广泛SolarGPT框架以用于太空科学教育和预测的初始步骤。

Abstract: Solar activity, including solar flares, coronal mass ejections (CMEs), and geomagnetic storms, can significantly impact satellites, aviation, power grids, data centers, and space missions. Extreme solar events can cause substantial economic damage if not predicted in advance, highlighting the importance of accurate forecasting and effective education in space science. Although large language models (LLMs) perform well on general tasks, they often lack domain-specific knowledge and pedagogical capability to clearly explain complex space science concepts.
  We introduce SolarGPT-QA, a question answering system based on a domain-adapted large language model built on the LLaMA-3 base model. The model is trained using scientific literature and large-scale question-answer data generated with GPT-4 and refined using Grok-3 in a student-friendly storytelling style. Human pairwise evaluations show that SolarGPT-QA outperforms general-purpose models in zero-shot settings and achieves competitive performance compared to instruction-tuned models for educational explanations in space weather and heliophysics. A small pilot student comprehension study further suggests improved clarity and accessibility of the generated explanations. Ablation experiments indicate that combining domain-adaptive pretraining with pedagogical fine-tuning is important for balancing scientific accuracy and educational effectiveness. This work represents an initial step toward a broader SolarGPT framework for space science education and forecasting.

</details>


### [267] [EMoE: Eigenbasis-Guided Routing for Mixture-of-Experts](https://arxiv.org/abs/2601.12137)
*Anzhe Cheng,Shukai Duan,Shixuan Li,Chenzhong Yin,Mingxi Cheng,Shahin Nazarian,Paul Thompson,Paul Bogdan*

Main category: cs.LG

TL;DR: 深度学习模型扩展带来计算挑战，现有MoE模型有负载不均衡和专家同质化问题，本文提出EMoE架构解决这些问题，代码开源。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型扩展导致计算需求难以承受，现有MoE模型存在负载不平衡和专家同质化问题，当前解决方案有局限性。

Method: 提出Eigen - Mixture - of - Experts (EMoE)架构，利用基于学习的正交特征基的路由机制，将输入令牌投影到共享特征基上并根据与特征空间主成分的对齐进行路由。

Result: 实现数据的原则性、几何分区，能促进专家负载平衡和发展多样化、专业化的专家，无需辅助损失函数。

Conclusion: EMoE架构有效解决了现有MoE模型的问题，为深度学习模型高效计算提供了新途径。

Abstract: The relentless scaling of deep learning models has led to unsustainable computational demands, positioning Mixture-of-Experts (MoE) architectures as a promising path towards greater efficiency. However, MoE models are plagued by two fundamental challenges: 1) a load imbalance problem known as the``rich get richer" phenomenon, where a few experts are over-utilized, and 2) an expert homogeneity problem, where experts learn redundant representations, negating their purpose. Current solutions typically employ an auxiliary load-balancing loss that, while mitigating imbalance, often exacerbates homogeneity by enforcing uniform routing at the expense of specialization. To resolve this, we introduce the Eigen-Mixture-of-Experts (EMoE), a novel architecture that leverages a routing mechanism based on a learned orthonormal eigenbasis. EMoE projects input tokens onto this shared eigenbasis and routes them based on their alignment with the principal components of the feature space. This principled, geometric partitioning of data intrinsically promotes both balanced expert utilization and the development of diverse, specialized experts, all without the need for a conflicting auxiliary loss function. Our code is publicly available at https://github.com/Belis0811/EMoE.

</details>


### [268] [Threshold Differential Attention for Sink-Free, Ultra-Sparse, and Non-Dispersive Language Modeling](https://arxiv.org/abs/2601.12145)
*Xingyue Huang,Xueying Ding,Mingxuan Ju,Yozen Liu,Neil Shah,Tong Zhao*

Main category: cs.LG

TL;DR: 提出阈值差分注意力机制TDA解决Softmax注意力在长上下文的问题，理论和实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: Softmax注意力因结构限制在长上下文表现不佳，存在注意力集中于无关标记和概率质量分散问题。

Method: 提出TDA机制，应用行级极值阈值和长度相关门限，保留超过阈值部分，借鉴差分变压器减去抑制视图。

Result: 理论上控制每行虚假存活数为O(1)，跨独立视图的共识虚假匹配随上下文增长消失；实验产生超99%精确零值，消除注意力汇聚点，在基准测试表现有竞争力。

Conclusion: TDA是一种无汇聚点的注意力机制，在长序列长度下实现超稀疏性和更好鲁棒性，无投影方法计算开销和标准整流注意力噪声累积问题。

Abstract: Softmax attention struggles with long contexts due to structural limitations: the strict sum-to-one constraint forces attention sinks on irrelevant tokens, and probability mass disperses as sequence lengths increase. We tackle these problems with Threshold Differential Attention (TDA), a sink-free attention mechanism that achieves ultra-sparsity and improved robustness at longer sequence lengths without the computational overhead of projection methods or the performance degradation caused by noise accumulation of standard rectified attention. TDA applies row-wise extreme-value thresholding with a length-dependent gate, retaining only exceedances. Inspired by the differential transformer, TDA also subtracts an inhibitory view to enhance expressivity. Theoretically, we prove that TDA controls the expected number of spurious survivors per row to $O(1)$ and that consensus spurious matches across independent views vanish as context grows. Empirically, TDA produces $>99\%$ exact zeros and eliminates attention sinks while maintaining competitive performance on standard and long-context benchmarks.

</details>


### [269] [Speculative Sampling with Reinforcement Learning](https://arxiv.org/abs/2601.12212)
*Chenan Wang,Daniel H. Shi,Haipeng Chen*

Main category: cs.LG

TL;DR: 提出基于强化学习的推测采样框架Re-SpS优化草稿树超参数，在多个基准测试中比SOTA方法EAGLE - 3有速度提升且不损失输出质量。


<details>
  <summary>Details</summary>
Motivation: 现有SOTA推测采样方法控制树结构的超参数是静态的，限制了不同上下文和领域的灵活性和效率。

Method: 引入Re - SpS框架，实时动态调整草稿树超参数，学习上下文感知策略，利用目标模型隐藏状态的高效状态表示和多步动作持久性进行更好的上下文建模。

Result: 在五个不同的基准测试中，与SOTA方法EAGLE - 3相比有一致的改进，相对于骨干大语言模型实现了高达5.45倍的加速，相对于EAGLE - 3实现了高达1.12倍的加速，且不损失输出保真度。

Conclusion: Re - SpS框架能有效优化大语言模型推测采样中的草稿树超参数，提升推理速度。

Abstract: Inference time latency has remained an open challenge for real world applications of large language models (LLMs). State-of-the-art (SOTA) speculative sampling (SpS) methods for LLMs, like EAGLE-3, use tree-based drafting to explore multiple candidate continuations in parallel. However, the hyperparameters controlling the tree structure are static, which limits flexibility and efficiency across diverse contexts and domains. We introduce Reinforcement learning for Speculative Sampling (Re-SpS), the first reinforcement learning (RL)-based framework for draft tree hyperparameter optimization. Re-SpS dynamically adjusts draft tree hyperparameters in real-time, learning context-aware policies that maximize generation speed by balancing speculative aggression with computational overhead. It leverages efficient state representations from target model hidden states and introduces multi-step action persistence for better context modeling. Evaluation results across five diverse benchmarks demonstrate consistent improvements over the SOTA method EAGLE-3, achieving up to 5.45$\times$ speedup over the backbone LLM and up to 1.12$\times$ speedup compared to EAGLE-3 across five diverse benchmarks, with no loss in output fidelity.

</details>


### [270] [Wavelet-Driven Masked Multiscale Reconstruction for PPG Foundation Models](https://arxiv.org/abs/2601.12215)
*Megha Thukral,Cyrus Tanade,Simon A. Lee,Juhyeon Lee,Hao Zhou,Keum San Chun,Migyeong Gwak,Viswam Nathan,Md Mahbubur Rahman,Li Zhu,Mehrab Bin Morshed,Subramaniam Venkatraman,Sharanya Arcot Desai*

Main category: cs.LG

TL;DR: 提出用于PPG表示学习的Masked Multiscale Reconstruction (MMR)自监督预训练框架，在多数健康相关任务上表现良好，凸显其向通用PPG基础模型发展的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有大规模预训练方法大多忽略PPG信号频谱结构，而下游健康相关任务依赖多分辨率特征，故需新的预训练方法。

Method: 引入MMR框架，通过小波多分辨率分解PPG信号，对随机掩码系数进行重建，用约1700万个未标记PPG片段预训练模型。

Result: 在19个健康相关任务中的17个上，MMR优于或匹配现有开源PPG基础模型、时间序列基础模型和其他自监督基线。

Conclusion: MMR有潜力成为通用PPG基础模型。

Abstract: Wearable foundation models have the potential to transform digital health by learning transferable representations from large-scale biosignals collected in everyday settings. While recent progress has been made in large-scale pretraining, most approaches overlook the spectral structure of photoplethysmography (PPG) signals, wherein physiological rhythms unfold across multiple frequency bands. Motivated by the insight that many downstream health-related tasks depend on multi-resolution features spanning fine-grained waveform morphology to global rhythmic dynamics, we introduce Masked Multiscale Reconstruction (MMR) for PPG representation learning - a self-supervised pretraining framework that explicitly learns from hierarchical time-frequency scales of PPG data. The pretraining task is designed to reconstruct randomly masked out coefficients obtained from a wavelet-based multiresolution decomposition of PPG signals, forcing the transformer encoder to integrate information across temporal and spectral scales. We pretrain our model with MMR using ~17 million unlabeled 10-second PPG segments from ~32,000 smartwatch users. On 17 of 19 diverse health-related tasks, MMR trained on large-scale wearable PPG data improves over or matches state-of-the-art open-source PPG foundation models, time-series foundation models, and other self-supervised baselines. Extensive analysis of our learned embeddings and systematic ablations underscores the value of wavelet-based representations, showing that they capture robust and physiologically-grounded features. Together, these results highlight the potential of MMR as a step toward generalizable PPG foundation models.

</details>


### [271] [Learning Longitudinal Health Representations from EHR and Wearable Data](https://arxiv.org/abs/2601.12227)
*Yuanyun Zhang,Han Zhou,Li Feng,Yilin Hong,Shi Li*

Main category: cs.LG

TL;DR: 提出多模态基础模型联合表征电子病历和可穿戴设备数据，在多项任务中表现优，表明联合预训练可产生更真实的纵向健康表征。


<details>
  <summary>Details</summary>
Motivation: 电子病历训练的基础模型受文档稀疏不规则限制，可穿戴设备数据缺乏语义基础，现有方法通常分别建模或后期融合。

Method: 提出多模态基础模型，使用特定模态编码器和共享时间骨干网络，通过自监督和跨模态目标进行预训练。

Result: 模型在预测生理和风险建模任务中优于仅使用电子病历或可穿戴设备的基线，在长时程和数据缺失情况下表现更佳。

Conclusion: 联合电子病历和可穿戴数据预训练能产生更真实的纵向健康表征。

Abstract: Foundation models trained on electronic health records show strong performance on many clinical prediction tasks but are limited by sparse and irregular documentation. Wearable devices provide dense continuous physiological signals but lack semantic grounding. Existing methods usually model these data sources separately or combine them through late fusion. We propose a multimodal foundation model that jointly represents electronic health records and wearable data as a continuous time latent process. The model uses modality specific encoders and a shared temporal backbone pretrained with self supervised and cross modal objectives. This design produces representations that are temporally coherent and clinically grounded. Across forecasting physiological and risk modeling tasks the model outperforms strong electronic health record only and wearable only baselines especially at long horizons and under missing data. These results show that joint electronic health record and wearable pretraining yields more faithful representations of longitudinal health.

</details>


### [272] [TimeGMM: Single-Pass Probabilistic Forecasting via Adaptive Gaussian Mixture Models with Reversible Normalization](https://arxiv.org/abs/2601.12288)
*Lei Liu,Tengyuan Liu,Hongwei Zhao,Jiahui Huang,Ruibo Guo,Bin Li*

Main category: cs.LG

TL;DR: 本文提出基于高斯混合模型的概率预测框架TimeGMM，能一次前向传播捕捉复杂未来分布，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有概率时间序列预测方法依赖高计算成本采样或有局限的参数假设，限制预测性能并引入分布不匹配问题。

Method: 提出TimeGMM框架，包含GMM - 自适应可逆实例归一化模块GRIN，集成时间编码器和条件时间概率解码器。

Result: 实验表明TimeGMM在CRPS上最高提升22.48%，在NMAE上最高提升21.23%，持续优于现有方法。

Conclusion: TimeGMM能有效解决现有概率时间序列预测方法的不足，性能表现出色。

Abstract: Probabilistic time series forecasting is crucial for quantifying future uncertainty, with significant applications in fields such as energy and finance. However, existing methods often rely on computationally expensive sampling or restrictive parametric assumptions to characterize future distributions, which limits predictive performance and introduces distributional mismatch. To address these challenges, this paper presents TimeGMM, a novel probabilistic forecasting framework based on Gaussian Mixture Models (GMM) that captures complex future distributions in a single forward pass. A key component is GMM-adapted Reversible Instance Normalization (GRIN), a novel module designed to dynamically adapt to temporal-probabilistic distribution shifts. The framework integrates a dedicated Temporal Encoder (TE-Module) with a Conditional Temporal-Probabilistic Decoder (CTPD-Module) to jointly capture temporal dependencies and mixture distribution parameters. Extensive experiments demonstrate that TimeGMM consistently outperforms state-of-the-art methods, achieving maximum improvements of 22.48\% in CRPS and 21.23\% in NMAE.

</details>


### [273] [Distribution Shift Is Key to Learning Invariant Prediction](https://arxiv.org/abs/2601.12296)
*Hong Zheng,Fei Teng*

Main category: cs.LG

TL;DR: 研究发现分布偏移使经验风险最小化（ERM）有时优于分布外任务专门方法，推导理论并实证验证。


<details>
  <summary>Details</summary>
Motivation: ERM 有时优于分布外任务专门方法，探究此现象背后超越算法设计的原因。

Method: 推导理论上分布偏移对模型学习的影响，进行实证验证。

Result: 理论上分布偏移程度直接影响模型预测能力，一定条件下 ERM 解性能与不变预测模型相当；实证上训练数据分布偏移程度增加时，模型预测接近最优模型。

Conclusion: 分布偏移在模型学习中起关键作用，有利于学习不变预测。

Abstract: An interesting phenomenon arises: Empirical Risk Minimization (ERM) sometimes outperforms methods specifically designed for out-of-distribution tasks. This motivates an investigation into the reasons behind such behavior beyond algorithmic design. In this study, we find that one such reason lies in the distribution shift across training domains. A large degree of distribution shift can lead to better performance even under ERM. Specifically, we derive several theoretical and empirical findings demonstrating that distribution shift plays a crucial role in model learning and benefits learning invariant prediction. Firstly, the proposed upper bounds indicate that the degree of distribution shift directly affects the prediction ability of the learned models. If it is large, the models' ability can increase, approximating invariant prediction models that make stable predictions under arbitrary known or unseen domains; and vice versa. We also prove that, under certain data conditions, ERM solutions can achieve performance comparable to that of invariant prediction models. Secondly, the empirical validation results demonstrated that the predictions of learned models approximate those of Oracle or Optimal models, provided that the degree of distribution shift in the training data increases.

</details>


### [274] [Machine Learning as a Service (MLaaS) Dataset Generator Framework for IoT Environments](https://arxiv.org/abs/2601.12305)
*Deepak Kanneganti,Sajib Mistry,Sheik Fattah,Joshua Boland,Aneesh Krishna*

Main category: cs.LG

TL;DR: 提出MDG框架创建可配置、可复现数据集评估MLaaS选择和组合，生成大量实例构建基准数据集，实验表明其提升效果，为相关研究提供基础。


<details>
  <summary>Details</summary>
Motivation: 创建可配置和可复现的数据集用于评估Machine Learning as a Service (MLaaS)的选择和组合。

Method: MDG框架通过在多个真实数据集和数据分布设置上训练和评估不同模型族模拟MLaaS行为，记录详细属性，有内置组合机制。

Result: 生成超一万个MLaaS服务实例，构建大规模基准数据集，生成的数据集提升选择准确性和组合质量。

Conclusion: MDG为MLaaS选择和组合的数据驱动研究提供实用和可扩展基础。

Abstract: We propose a novel MLaaS Dataset Generator (MDG) framework that creates configurable and reproducible datasets for evaluating Machine Learning as a Service (MLaaS) selection and composition. MDG simulates realistic MLaaS behaviour by training and evaluating diverse model families across multiple real-world datasets and data distribution settings. It records detailed functional attributes, quality of service metrics, and composition-specific indicators, enabling systematic analysis of service performance and cross-service behaviour. Using MDG, we generate more than ten thousand MLaaS service instances and construct a large-scale benchmark dataset suitable for downstream evaluation. We also implement a built-in composition mechanism that models how services interact under varied Internet of Things conditions. Experiments demonstrate that datasets generated by MDG enhance selection accuracy and composition quality compared to existing baselines. MDG provides a practical and extensible foundation for advancing data-driven research on MLaaS selection and composition

</details>


### [275] [Explanova: Automatically Discover Data Insights in N \times M Table via XAI Combined LLM Workflow](https://arxiv.org/abs/2601.12317)
*Yiming Huang*

Main category: cs.LG

TL;DR: 自动化数据分析是长期追求，现有基于LLM的代理框架有潜力，Explanova尝试用预设AutoML工作流，且使用本地小模型成本更低。


<details>
  <summary>Details</summary>
Motivation: 探索以预设AutoML工作流实现自动化数据分析的可能性。

Method: 构建Explanova框架，利用预设AutoML工作流进行数据分析，采用本地小LLM。

Result: 未提及明确结果。

Conclusion: 未提及明确结论，但表明Explanova尝试用新方式做数据分析，且成本更低。

Abstract: Automation in data analysis has been a long-time pursuit. Current agentic LLM shows a promising solution towards it. Like DeepAnalyze, DataSage, and Datawise. They are all powerful agentic frameworks for automatic fine-grained analysis and are powered by LLM-based agentic tool calling ability. However, what about powered by a preset AutoML-like workflow? If we traverse all possible exploration, like Xn itself`s statistics, Xn1-Xn2 relationships, Xn to all other, and finally explain? Our Explanova is such an attempt: Cheaper due to a Local Small LLM.

</details>


### [276] [Ordered Local Momentum for Asynchronous Distributed Learning under Arbitrary Delays](https://arxiv.org/abs/2601.12322)
*Chang-Wei Shi,Shi-Shang Wang,Wu-Jun Li*

Main category: cs.LG

TL;DR: 提出OrLoMo方法实现异步分布式MSGD本地更新，证明非凸问题收敛性，实验显示性能优越。


<details>
  <summary>Details</summary>
Motivation: 目前异步分布式MSGD本地更新的实现方法未知，需解决该问题。

Method: 提出OrLoMo方法，各工作节点本地运行MSGD，服务器按全局迭代索引顺序聚合本地动量。

Result: 证明了OrLoMo在非凸问题下任意延迟时的收敛性，实验表明其性能优于同步方法和其他异步方法。

Conclusion: OrLoMo是首个实现异步分布式MSGD本地更新的方法，具有良好性能。

Abstract: Momentum SGD (MSGD) serves as a foundational optimizer in training deep models due to momentum's key role in accelerating convergence and enhancing generalization. Meanwhile, asynchronous distributed learning is crucial for training large-scale deep models, especially when the computing capabilities of the workers in the cluster are heterogeneous. To reduce communication frequency, local updates are widely adopted in distributed learning. However, how to implement asynchronous distributed MSGD with local updates remains unexplored. To solve this problem, we propose a novel method, called \underline{or}dered \underline{lo}cal \underline{mo}mentum (OrLoMo), for asynchronous distributed learning. In OrLoMo, each worker runs MSGD locally. Then the local momentum from each worker will be aggregated by the server in order based on its global iteration index. To the best of our knowledge, OrLoMo is the first method to implement asynchronous distributed MSGD with local updates. We prove the convergence of OrLoMo for non-convex problems under arbitrary delays. Experiments validate that OrLoMo can outperform its synchronous counterpart and other asynchronous methods.

</details>


### [277] [IceWatch: Forecasting Glacial Lake Outburst Floods (GLOFs) using Multimodal Deep Learning](https://arxiv.org/abs/2601.12330)
*Zuha Fatima,Muhammad Anser Sohaib,Muhammad Talha,Ayesha Kanwal,Sidra Sultana,Nazia Perwaiz*

Main category: cs.LG

TL;DR: 提出IceWatch深度学习框架用于GLOF预测，结合时空视角，确保性能、处理速度和鲁棒性，为自动可扩展预警系统奠定基础。


<details>
  <summary>Details</summary>
Motivation: 经典GLOF检测和预测方法存在更新慢、依赖人力、受云干扰和缺乏现场数据影响精度等问题。

Method: 构建IceWatch框架，其RiskFlow组件用CNN分类器处理卫星图像，结合TerraFlow和TempFlow考虑物理动态，通过多模态、物理信息实现GLOF预测。

Result: 系统具有强预测性能、快速数据处理能力，对噪声和缺失信息有鲁棒性。

Conclusion: IceWatch为自动可扩展的GLOF预警系统铺平道路，有与多样传感器输入和全球冰川监测活动集成的潜力。

Abstract: Glacial Lake Outburst Floods (GLOFs) pose a serious threat in high mountain regions. They are hazardous to communities, infrastructure, and ecosystems further downstream. The classical methods of GLOF detection and prediction have so far mainly relied on hydrological modeling, threshold-based lake monitoring, and manual satellite image analysis. These approaches suffer from several drawbacks: slow updates, reliance on manual labor, and losses in accuracy when clouds interfere and/or lack on-site data. To tackle these challenges, we present IceWatch: a novel deep learning framework for GLOF prediction that incorporates both spatial and temporal perspectives. The vision component, RiskFlow, of IceWatch deals with Sentinel-2 multispectral satellite imagery using a CNN-based classifier and predicts GLOF events based on the spatial patterns of snow, ice, and meltwater. Its tabular counterpart confirms this prediction by considering physical dynamics. TerraFlow models glacier velocity from NASA ITS_LIVE time series while TempFlow forecasts near-surface temperature from MODIS LST records; both are trained on long-term observational archives and integrated via harmonized preprocessing and synchronization to enable multimodal, physics-informed GLOF prediction. Both together provide cross-validation, which will improve the reliability and interpretability of GLOF detection. This system ensures strong predictive performance, rapid data processing for real-time use, and robustness to noise and missing information. IceWatch paves the way for automatic, scalable GLOF warning systems. It also holds potential for integration with diverse sensor inputs and global glacier monitoring activities.

</details>


### [278] [Time-Continuous Modeling for Temporal Affective Pattern Recognition in LLMs](https://arxiv.org/abs/2601.12341)
*Rezky Kam,Coddy N. Siswanto*

Main category: cs.LG

TL;DR: 本文引入数据集和概念框架，让大语言模型借助物理信息神经网络模拟现实情绪动态，为可解释对话建模带来可能。


<details>
  <summary>Details</summary>
Motivation: 使大语言模型能够模拟现实世界中的情绪动态，实现可解释的对话建模。

Method: 利用物理信息神经网络，通过引入数据集和概念框架，借助时间和上下文学习。

Result: 为大语言模型模拟现实世界的情绪动态奠定基础，提供了一种新的方法。

Conclusion: 该方法为可解释的对话建模开辟了可能性。

Abstract: This paper introduces a dataset and conceptual framework for LLMs to mimic real world emotional dynamics through time and in-context learning leveraging physics-informed neural network, opening a possibility for interpretable dialogue modeling.

</details>


### [279] [LB-MCTS: Synergizing Large Language Models and Bayesian Optimization for Efficient CASH](https://arxiv.org/abs/2601.12355)
*Beicheng Xu,Weitong Qian,Lingching Tung,Yupeng Lu,Bin Cui*

Main category: cs.LG

TL;DR: 为降低机器学习专业门槛，针对CASH问题，提出结合LLM和BO的LB - MCTS框架，实验证明其优于基线。


<details>
  <summary>Details</summary>
Motivation: 降低机器学习专业门槛，解决传统方法冷启动问题及现有基于LLM优化器在高维结构化CASH空间泛化性差的问题。

Method: 提出LB - MCTS框架，在蒙特卡罗树搜索结构中协同LLM和BO，用选择性调优内存最大化LLM推理，动态切换提案方式。

Result: 在104个AMLB数据集上的实验表明LB - MCTS优于竞争基线。

Conclusion: LB - MCTS框架有效，结合了LLM和BO的优势，能更好解决CASH问题。

Abstract: To lower the expertise barrier in machine learning, the AutoML community has focused on the CASH problem, a fundamental challenge that automates the process of algorithm selection and hyperparameter tuning. While traditional methods like Bayesian Optimization (BO) struggle with cold-start issues, Large Language Models (LLMs) can mitigate these via semantic priors. However, existing LLM-based optimizers generalize poorly to the high-dimensional, structured CASH space. We propose LB-MCTS, a framework synergizing LLMs and BO within a Monte Carlo Tree Search structure. It maximizes LLM reasoning with Selective Tuning Memory (STM) and explicit exploration-exploitation trade-off. It combines the strengths of both paradigms by dynamically shifting from LLM-driven to BO-driven proposals as data accumulates. Experiments on 104 AMLB datasets demonstrate the superiority of LB-MCTS over the competitive baselines.

</details>


### [280] [Machine Learning-Based Framework for Real Time Detection and Early Prediction of Control Valve Stiction in Industrial Control Systems](https://arxiv.org/abs/2601.12362)
*Natthapong Promsricha,Chotirawee Chatpattanasiri,Nuttavut Kerdgongsup,Stavroula Balabani*

Main category: cs.LG

TL;DR: 本文提出用机器学习框架仅通过常规收集的过程信号检测和预测控制阀粘连，并对比三种模型，LSTM 模型最准，可提前四小时预测。


<details>
  <summary>Details</summary>
Motivation: 控制阀粘连是常见故障，传统阀门缺乏实时监测，难以早期预测，需有效检测和预测方法。

Method: 提出机器学习框架，用控制器输出和过程变量作为输入，开发 CNN、CNN - SVM 和 LSTM 三种深度学习模型，用基于斜率比分析的数据驱动标注方法处理炼油厂数据集进行训练。

Result: LSTM 模型精度最高，能提前四小时预测粘连。

Conclusion: 这是首次用工业实际数据实现基于机器学习的控制阀粘连早期预测，框架可集成到现有控制系统，支持预测性维护。

Abstract: Control valve stiction, a friction that prevents smooth valve movement, is a common fault in industrial process systems that causes instability, equipment wear, and higher maintenance costs. Many plants still operate with conventional valves that lack real time monitoring, making early predictions challenging. This study presents a machine learning (ML) framework for detecting and predicting stiction using only routinely collected process signals: the controller output (OP) from control systems and the process variable (PV), such as flow rate. Three deep learning models were developed and compared: a Convolutional Neural Network (CNN), a hybrid CNN with a Support Vector Machine (CNN-SVM), and a Long Short-Term Memory (LSTM) network. To train these models, a data-driven labeling method based on slope ratio analysis was applied to a real oil and gas refinery dataset. The LSTM model achieved the highest accuracy and was able to predict stiction up to four hours in advance. To the best of the authors' knowledge, this is the first study to demonstrate ML based early prediction of control valve stiction from real industry data. The proposed framework can be integrated into existing control systems to support predictive maintenance, reduce downtime, and avoid unnecessary hardware replacement.

</details>


### [281] [Beyond the Dirac Delta: Mitigating Diversity Collapse in Reinforcement Fine-Tuning for Versatile Image Generation](https://arxiv.org/abs/2601.12401)
*Jinmei Liu,Haoru Li,Zhenhong Sun,Chaofeng Chen,Yatao Bian,Bo Wang,Daoyi Dong,Chunlin Chen,Zhi Wang*

Main category: cs.LG

TL;DR: 提出DRIFT框架解决强化学习微调大模型时的多样性崩溃问题，实验显示其在任务对齐和生成多样性上表现出色。


<details>
  <summary>Details</summary>
Motivation: 强化学习微调大模型存在多样性崩溃问题，需解决以满足对多样候选生成有要求的应用。

Method: 从采样、提示、优化三个视角入手，采样奖励集中子集、随机变化提示、用基于势的奖励塑造机制优化组内多样性。

Result: DRIFT在任务对齐和生成多样性上实现帕累托优势，在同等对齐水平下多样性提升9.08% - 43.46%，在同等多样性水平下对齐度提升59.65% - 65.86%。

Conclusion: DRIFT框架能有效调和强任务对齐与高生成多样性，增强图像生成的通用性。

Abstract: Reinforcement learning (RL) has emerged as a powerful paradigm for fine-tuning large-scale generative models, such as diffusion and flow models, to align with complex human preferences and user-specified tasks. A fundamental limitation remains \textit{the curse of diversity collapse}, where the objective formulation and optimization landscape inherently collapse the policy to a Dirac delta distribution. To address this challenge, we propose \textbf{DRIFT} (\textbf{D}ive\textbf{R}sity-\textbf{I}ncentivized Reinforcement \textbf{F}ine-\textbf{T}uning for Versatile Image Generation), an innovative framework that systematically incentivizes output diversity throughout the on-policy fine-tuning process, reconciling strong task alignment with high generation diversity to enhance versatility essential for applications that demand diverse candidate generations. We approach the problem across three representative perspectives: i) \textbf{sampling} a reward-concentrated subset that filters out reward outliers to prevent premature collapse; ii) \textbf{prompting} with stochastic variations to expand the conditioning space, and iii) \textbf{optimization} of the intra-group diversity with a potential-based reward shaping mechanism. Experimental results show that DRIFT achieves superior Pareto dominance regarding task alignment and generation diversity, yielding a $ 9.08\%\!\sim\! 43.46\%$ increase in diversity at equivalent alignment levels and a $ 59.65\% \!\sim\! 65.86\%$ increase in alignment at equivalent levels of diversity.

</details>


### [282] [Explainable Machine Learning for Pediatric Dental Risk Stratification Using Socio-Demographic Determinants](https://arxiv.org/abs/2601.12405)
*Manasi Kanade,Abhi Thakkar,Gabriela Fernandes*

Main category: cs.LG

TL;DR: 本文开发并评估可解释机器学习框架用于儿科牙科风险分层，模型表现中等，确定关键风险因素，强调其在预防和资源分配方面的作用。


<details>
  <summary>Details</summary>
Motivation: 儿科牙科疾病普遍且不公平，现有牙科AI应用有局限，需开发重解释性、校准和道德部署的模型。

Method: 用含年龄、收入贫困比等人口层面儿科数据训练监督机器学习模型，用ROC分析和校准曲线评估性能，用SHAP实现可解释性。

Result: 模型有适度区分度（AUC = 0.61）和保守校准，高估高概率风险；SHAP分析确定年龄和收入贫困比是主要风险因素。

Conclusion: 可解释机器学习能实现透明、预防导向的儿科牙科风险分层，支持人群筛查和公平资源分配。

Abstract: Background: Pediatric dental disease remains one of the most prevalent and inequitable chronic health conditions worldwide. Although strong epidemiological evidence links oral health outcomes to socio-economic and demographic determinants, most artificial intelligence (AI) applications in dentistry rely on image-based diagnosis and black-box prediction models, limiting transparency and ethical applicability in pediatric populations.
  Objective: This study aimed to develop and evaluate an explainable machine learning framework for pediatric dental risk stratification that prioritizes interpretability, calibration, and ethical deployment over maximal predictive accuracy.
  Methods: A supervised machine learning model was trained using population-level pediatric data including age, income-to-poverty ratio, race/ethnicity, gender, and medical history. Model performance was assessed using receiver operating characteristic (ROC) analysis and calibration curves. Explainability was achieved using SHapley Additive exPlanations (SHAP) to provide global and individual-level interpretation of predictions.
  Results: The model achieved modest discrimination (AUC = 0.61) with conservative calibration, underestimating risk at higher probability levels. SHAP analysis identified age and income-to-poverty ratio as the strongest contributors to predicted risk, followed by race/ethnicity and gender.
  Conclusion: Explainable machine learning enables transparent, prevention-oriented pediatric dental risk stratification and supports population screening and equitable resource allocation rather than diagnostic decision-making.

</details>


### [283] [Orthogonalized Policy Optimization:Decoupling Sampling Geometry from Optimization Geometry in RLHF](https://arxiv.org/abs/2601.12415)
*Wang Zixian*

Main category: cs.LG

TL;DR: 指出当前大语言模型对齐方法存在问题，提出OPO框架统一现有对齐方法并实现稳定优化。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法常混淆采样几何和优化几何两个独立设计选择，且常用的KL散度存在数值不稳定和梯度消失问题。

Method: 将对齐表示为策略能量和目标能量广义距离的最小化，提出OPO框架，结合alpha加权重要性采样和卡方诱导的二次正则化。

Result: OPO产生简单且条件良好的目标函数，具有线性梯度动态，能在高模型置信度下保持稳定优化，避免梯度饱和。

Conclusion: OPO为现有对齐方法提供统一视角，为鲁棒推理导向训练提供原则性基础。

Abstract: Recent alignment methods for large language models, including PPO, DPO, and IPO, are often presented as distinct algorithms. In this work, we show that many of these approaches implicitly conflate two fundamental and independent design choices: (i) the sampling geometry, which determines which samples dominate the gradient signal, and (ii) the optimization geometry, which determines how deviations in value are penalized. We formalize this observation by expressing alignment as the minimization of a generalized distance between policy energy and target energy, parameterized by an alpha-divergence-based sampling weight and a Bregman-divergence-based value metric. We demonstrate that the commonly used KL divergence induces an exponential penalty on unbounded value signals, leading to numerical instability and vanishing gradients in high-confidence regimes. To address this issue, we propose Orthogonalized Policy Optimization (OPO), a framework that explicitly decouples sampling geometry from optimization geometry. By combining alpha-weighted importance sampling with a chi-square-induced quadratic regularization in ratio coordinates, OPO yields a simple and well-conditioned objective with linear gradient dynamics. This formulation maintains stable optimization while preserving peak-seeking behavior and avoids gradient saturation even when model confidence is high. Our analysis positions OPO as a unifying perspective on existing alignment methods and provides a principled foundation for robust reasoning-oriented training.

</details>


### [284] [Graph Attention Networks with Physical Constraints for Anomaly Detection](https://arxiv.org/abs/2601.12426)
*Mohammadhossein Homaei,Iman Khazrak,Ruben Molano,Andres Caro,Mar Avila*

Main category: cs.LG

TL;DR: 提出液压感知图注意力网络用于供水管网异常检测，在BATADAL数据集表现良好。


<details>
  <summary>Details</summary>
Motivation: 供水管网面临网络物理风险，现有数据驱动模型忽略网络拓扑且难解释，基于模型的方法依赖参数精度，需可靠异常检测方法。

Method: 使用归一化守恒定律违规作为特征，结合质量和能量平衡残差、图注意力和双向LSTM学习时空模式，用多尺度模块聚合检测分数。

Result: 在BATADAL数据集上F1值达0.979，提升3.3个百分点，在15%参数噪声下有高鲁棒性。

Conclusion: 所提出的液压感知图注意力网络在供水管网异常检测上有效且鲁棒。

Abstract: Water distribution systems (WDSs) face increasing cyber-physical risks, which make reliable anomaly detection essential. Many data-driven models ignore network topology and are hard to interpret, while model-based ones depend strongly on parameter accuracy. This work proposes a hydraulic-aware graph attention network using normalized conservation law violations as features. It combines mass and energy balance residuals with graph attention and bidirectional LSTM to learn spatio-temporal patterns. A multi-scale module aggregates detection scores from node to network level. On the BATADAL dataset, it reaches $F1=0.979$, showing $3.3$pp gain and high robustness under $15\%$ parameter noise.

</details>


### [285] [Constraint-Aware Neurosymbolic Uncertainty Quantification with Bayesian Deep Learning for Scientific Discovery](https://arxiv.org/abs/2601.12442)
*Shahnawaz Alam,Mohammed Mudassir Uddin,Mohammed Kaif Pasha*

Main category: cs.LG

TL;DR: 介绍CANUF框架统一贝叶斯深度学习和可微符号推理，经实验能减少误差、保证约束满足，提供端到端管道。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性量化方法缺乏结合符号科学知识的机制，神经符号方法缺乏原则性不确定性建模。

Method: 引入CANUF框架，包括从科学文献中自动提取约束、带变分推断的概率神经骨干和可微约束满足层。

Result: 在多个基准测试中，CANUF比贝叶斯神经网络减少34.7%的预期校准误差，保持99.2%的约束满足，消融实验显示约束引导重新校准贡献18.3%的性能提升，约束提取精度达91.4%。

Conclusion: CANUF提供第一个端到端可微管道，同时解决不确定性量化、约束满足和科学预测的可解释性问题。

Abstract: Scientific Artificial Intelligence (AI) applications require models that deliver trustworthy uncertainty estimates while respecting domain constraints. Existing uncertainty quantification methods lack mechanisms to incorporate symbolic scientific knowledge, while neurosymbolic approaches operate deterministically without principled uncertainty modeling. We introduce the Constraint-Aware Neurosymbolic Uncertainty Framework (CANUF), unifying Bayesian deep learning with differentiable symbolic reasoning. The architecture comprises three components: automated constraint extraction from scientific literature, probabilistic neural backbone with variational inference, and differentiable constraint satisfaction layer ensuring physical consistency. Experiments on Materials Project (140,000+ materials), QM9 molecular properties, and climate benchmarks show CANUF reduces Expected Calibration Error by 34.7% versus Bayesian neural networks while maintaining 99.2% constraint satisfaction. Ablations reveal constraint-guided recalibration contributes 18.3% performance gain, with constraint extraction achieving 91.4% precision. CANUF provides the first end-to-end differentiable pipeline simultaneously addressing uncertainty quantification, constraint satisfaction, and interpretable explanations for scientific predictions.

</details>


### [286] [Patch-Level Tokenization with CNN Encoders and Attention for Improved Transformer Time-Series Forecasting](https://arxiv.org/abs/2601.12467)
*Saurish Nagrath*

Main category: cs.LG

TL;DR: 本文提出两阶段预测框架提升时间序列预测效果。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在时间序列预测中效果依赖输入表示质量和结构，需改进。

Method: 提出两阶段框架，第一阶段用CNN提取短程动力学和特征交互，并进行自注意力处理；第二阶段用Transformer编码器处理令牌序列建模依赖关系并生成预测。

Result: 在合成数据上，所提策略相较基线有竞争力。

Conclusion: 结构化的时间表示很重要，解耦本地时间编码和全局注意力建模能带来更有效稳定的时间序列预测。

Abstract: Transformer-based models have shown strong performance in time-series forecasting by leveraging self-attention to model long-range temporal dependencies. However, their effectiveness depends critically on the quality and structure of input representations derived from raw multivariate time-series data. This work proposes a two-stage forecasting framework that explicitly separates local temporal representation learning from global dependency modelling. In the first stage, a convolutional neural network (CNN) operates on fixed-length temporal patches to extract short-range temporal dynamics and non-linear feature interactions, producing compact patch-level token embeddings. Token-level self-attention is subsequently applied during representation learning to refine these embeddings by enabling interactions across temporal patches. In the second stage, a Transformer encoder processes the resulting token sequence to model inter-patch temporal dependencies and generate per-patch forecasts. Experiments conducted on synthetic multivariate time-series data with controlled static and dynamic factors demonstrate that the proposed patch-based tokenization strategy achieves competitive forecasting performance compared to convolutional and patch-based Transformer baselines. The results highlight the importance of structured temporal representations and show that decoupling local temporal encoding from global attention-based modelling yields more effective and stable time-series forecasting.

</details>


### [287] [Semidefinite Programming for Quantum Channel Learning](https://arxiv.org/abs/2601.12502)
*Mikhail Gennadievich Belov,Victor Victorovich Dubov,Vadim Konstantinovich Ivanov,Alexander Yurievich Maslov,Olga Vladimirovna Proshina,Vladislav Gennadievich Malyshkin*

Main category: cs.LG

TL;DR: 本文考虑从经典数据样本重建量子信道问题，用半定规划（SDP）解决保真度优化问题，测试多种SDP求解器并应用于投影算子重建，还讨论了基于量子信道变换的经典计算模型。


<details>
  <summary>Details</summary>
Motivation: 解决从经典数据样本重建量子信道的问题，优化量子信道保真度。

Method: 使用半定规划（SDP）解决关于Choi矩阵的保真度优化问题，并测试多种商用SDP求解器。

Result: 多种商用SDP求解器可重建不同形式的量子信道，所得量子信道的Kraus秩通常远小于其最大可能值，理论也可应用于投影算子重建。

Conclusion: 相对较小Kraus秩的量子信道通常足以描述实验观测到的经典数据，还探讨了经典计算模型。

Abstract: The problem of reconstructing a quantum channel from a sample of classical data is considered. When the total fidelity can be represented as a ratio of two quadratic forms (e.g., in the case of mapping a mixed state to a pure state, projective operators, unitary learning, and others), Semidefinite Programming (SDP) can be applied to solve the fidelity optimization problem with respect to the Choi matrix. A remarkable feature of SDP is that the optimization is convex, which allows the problem to be efficiently solved by a variety of numerical algorithms. We have tested several commercially available SDP solvers, all of which allowed for the reconstruction of quantum channels of different forms. A notable feature is that the Kraus rank of the obtained quantum channel typically comprises less than a few percent of its maximal possible value. This suggests that a relatively small Kraus rank quantum channel is typically sufficient to describe experimentally observed classical data. The theory was also applied to the problem of reconstructing projective operators from data. Finally, we discuss a classical computational model based on quantum channel transformation, performed and calculated on a classical computer, possibly hardware-optimized.

</details>


### [288] [Learning Relativistic Geodesics and Chaotic Dynamics via Stabilized Lagrangian Neural Networks](https://arxiv.org/abs/2601.12519)
*Abdullah Umut Hamzaogullari,Arkadas Ozakin*

Main category: cs.LG

TL;DR: 本文提出改进措施解决拉格朗日神经网络（LNNs）训练不稳定问题，提升其对复杂系统的适用性，还可用于相对论场景，拓展了科学发现任务的实际应用。


<details>
  <summary>Details</summary>
Motivation: LNNs不寻常的优化目标导致训练不稳定，限制其在复杂系统中的应用。

Method: 提出Hessian正则化方案、更适合问题的激活函数、物理感知坐标缩放等改进技术，并与先前方法一同评估。

Result: 改进架构能训练前所未有的复杂系统，在双摆系统中验证损失降低96.6%，稳定性提升90.68%，可学习非相对论和广义相对论场景下的拉格朗日量。

Conclusion: 虽继承原LNN框架部分局限性，但显著拓展了LNN在科学发现任务中的实际应用。

Abstract: Lagrangian Neural Networks (LNNs) can learn arbitrary Lagrangians from trajectory data, but their unusual optimization objective leads to significant training instabilities that limit their application to complex systems. We propose several improvements that address these fundamental challenges, namely, a Hessian regularization scheme that penalizes unphysical signatures in the Lagrangian's second derivatives with respect to velocities, preventing the network from learning unstable dynamics, activation functions that are better suited to the problem of learning Lagrangians, and a physics-aware coordinate scaling that improves stability. We systematically evaluate these techniques alongside previously proposed methods for improving stability. Our improved architecture successfully trains on systems of unprecedented complexity, including triple pendulums, and achieved 96.6\% lower validation loss value and 90.68\% better stability than baseline LNNs in double pendulum systems. With the improved framework, we show that our LNNs can learn Lagrangians representing geodesic motion in both non-relativistic and general relativistic settings. To deal with the relativistic setting, we extended our regularization to penalize violations of Lorentzian signatures, which allowed us to predict a geodesic Lagrangian under AdS\textsubscript{4} spacetime metric directly from trajectory data, which to our knowledge has not been done in the literature before. This opens new possibilities for automated discovery of geometric structures in physics, including extraction of spacetime metric tensor components from geodesic trajectories. While our approach inherits some limitations of the original LNN framework, particularly the requirement for invertible Hessians, it significantly expands the practical applicability of LNNs for scientific discovery tasks.

</details>


### [289] [Press Start to Charge: Videogaming the Online Centralized Charging Scheduling Problem](https://arxiv.org/abs/2601.12543)
*Alireza Ghahtarani,Martin Cousineau,Amir-massoud Farahmand,Jorge E. Mendoza*

Main category: cs.LG

TL;DR: 研究在线集中充电调度问题，将其游戏化求解，实验证明方法有效且有经济价值。


<details>
  <summary>Details</summary>
Motivation: 解决在线集中充电调度问题，平衡有限规划期内的负载。

Method: 将问题游戏化建模，设计启发式策略，用专家演示训练学习代理并使用DAgger改进。

Result: 游戏化降低模型复杂度，有更紧的泛化边界，图像到移动模型表现最佳，有鲁棒性。

Conclusion: 提出的方法可提升负载平衡，有经济价值，能降低系统成本并延迟电网升级。

Abstract: We study the online centralized charging scheduling problem (OCCSP). In this problem, a central authority must decide, in real time, when to charge dynamically arriving electric vehicles (EVs), subject to capacity limits, with the objective of balancing load across a finite planning horizon. To solve the problem, we first gamify it; that is, we model it as a game where charging blocks are placed within temporal and capacity constraints on a grid. We design heuristic policies, train learning agents with expert demonstrations, and improve them using Dataset Aggregation (DAgger). From a theoretical standpoint, we show that gamification reduces model complexity and yields tighter generalization bounds than vector-based formulations. Experiments across multiple EV arrival patterns confirm that gamified learning enhances load balancing. In particular, the image-to-movement model trained with DAgger consistently outperforms heuristic baselines, vector-based approaches, and supervised learning agents, while also demonstrating robustness in sensitivity analyses. These operational gains translate into tangible economic value. In a real-world case study for the Greater Montréal Area (Québec, Canada) using utility cost data, the proposed methods lower system costs by tens of millions of dollars per year over the prevailing practice and show clear potential to delay costly grid upgrades.

</details>


### [290] [Life, Machine Learning, and the Search for Habitability: Predicting Biosignature Fluxes for the Habitable Worlds Observatory](https://arxiv.org/abs/2601.12557)
*Mark Moussa,Amber V. Young,Brianna Isola,Vasuda Trehan,Michael D. Himes,Nicholas Wogan,Giada Arney*

Main category: cs.LG

TL;DR: 本文介绍两种机器学习架构用来从系外行星反射光光谱预测生物特征物种通量，在数据集上表现出高准确性，有望助力未来旗舰任务。


<details>
  <summary>Details</summary>
Motivation: 未来直接成像旗舰任务在优先级决策面临时间和资源限制，需预测生物特征物种通量的方法。

Method: 引入贝叶斯卷积神经网络（BCNN）和光谱查询自适应变压器（SQuAT）两种模型。

Result: 两种模型在系外行星条件数据集上有较高预测准确性，且在不确定性量化和光谱解释性上各有优势。

Conclusion: 这两种方法有望加速目标筛选、优化观测计划和提高科学回报，可用于未来旗舰任务。

Abstract: Future direct-imaging flagship missions, such as NASA's Habitable Worlds Observatory (HWO), face critical decisions in prioritizing observations due to extremely stringent time and resource constraints. In this paper, we introduce two advanced machine-learning architectures tailored for predicting biosignature species fluxes from exoplanetary reflected-light spectra: a Bayesian Convolutional Neural Network (BCNN) and our novel model architecture, the Spectral Query Adaptive Transformer (SQuAT). The BCNN robustly quantifies both epistemic and aleatoric uncertainties, offering reliable predictions under diverse observational conditions, whereas SQuAT employs query-driven attention mechanisms to enhance interpretability by explicitly associating spectral features with specific biosignature species. We demonstrate that both models achieve comparably high predictive accuracy on an augmented dataset spanning a wide range of exoplanetary conditions, while highlighting their distinct advantages in uncertainty quantification and spectral interpretability. These capabilities position our methods as promising tools for accelerating target triage, optimizing observation schedules, and maximizing scientific return for upcoming flagship missions such as HWO.

</details>


### [291] [Dissecting Linear Recurrent Models: How Different Gating Strategies Drive Selectivity and Generalization](https://arxiv.org/abs/2601.12598)
*Younes Bouhadjar,Maxime Fabre,Felix Schmidt,Emre Neftci*

Main category: cs.LG

TL;DR: 本文提出线性循环模型的分类法和轻量级可定制基准测试SelectivBench，评估模型选择性，分析架构特征作用，代码开源。


<details>
  <summary>Details</summary>
Motivation: 线性循环模型迭代使复杂度和计算成本增加，但缺乏系统对比，现有基准任务要么简单要么耗资源。

Method: 提出线性循环模型分类法，引入SelectivBench，用基于规则语法生成有不同复杂度序列，含不规则间隙。

Result: 在SelectivBench上评估线性循环模型性能模式与大规模语言任务结果一致。

Conclusion: 阐明架构特征作用，SelectivBench可对线性循环模型进行有针对性、高效探索，为研究大规模评估行为提供可控环境。

Abstract: Linear recurrent neural networks have emerged as efficient alternatives to the original Transformer's softmax attention mechanism, thanks to their highly parallelizable training and constant memory and computation requirements at inference. Iterative refinements of these models have introduced an increasing number of architectural mechanisms, leading to increased complexity and computational costs. Nevertheless, systematic direct comparisons among these models remain limited. Existing benchmark tasks are either too simplistic to reveal substantial differences or excessively resource-intensive for experimentation. In this work, we propose a refined taxonomy of linear recurrent models and introduce SelectivBench, a set of lightweight and customizable synthetic benchmark tasks for systematically evaluating sequence models. SelectivBench specifically evaluates selectivity in sequence models at small to medium scale, such as the capacity to focus on relevant inputs while ignoring context-based distractors. It employs rule-based grammars to generate sequences with adjustable complexity, incorporating irregular gaps that intentionally violate transition rules. Evaluations of linear recurrent models on SelectivBench reveal performance patterns consistent with results from large-scale language tasks. Our analysis clarifies the roles of essential architectural features: gating and rapid forgetting mechanisms facilitate recall, in-state channel mixing is unnecessary for selectivity, but critical for generalization, and softmax attention remains dominant due to its memory capacity scaling with sequence length. Our benchmark enables targeted, efficient exploration of linear recurrent models and provides a controlled setting for studying behaviors observed in large-scale evaluations. Code is available at https://github.com/symseqbench/selectivbench

</details>


### [292] [Beyond Softmax and Entropy: Improving Convergence Guarantees of Policy Gradients by f-SoftArgmax Parameterization with Coupled Regularization](https://arxiv.org/abs/2601.12604)
*Safwan Labbi,Daniil Tiapkin,Paul Mangold,Eric Moulines*

Main category: cs.LG

TL;DR: 提出用广义f - softargmax替代softmax进行策略参数化，结合正则化器，为随机策略梯度方法建立无预条件的非渐近收敛保证，证明f - PG有多项式样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 策略梯度方法对策略参数化选择敏感，常用的softmax参数化会导致优化困难和收敛慢，预条件处理计算成本高。

Method: 用基于广义f - softargmax的策略参数化替代softmax，并结合由相同f - 散度诱导的正则化器。

Result: 为有限MDPs的随机策略梯度方法建立无预条件的首次显式非渐近最后迭代收敛保证，推导无正则化问题的样本复杂度界，f - PG在Tsallis散度下有多项式样本复杂度。

Conclusion: 所提的策略参数化和正则化方法能有效改善优化情况，提高收敛速度和样本效率。

Abstract: Policy gradient methods are known to be highly sensitive to the choice of policy parameterization. In particular, the widely used softmax parameterization can induce ill-conditioned optimization landscapes and lead to exponentially slow convergence. Although this can be mitigated by preconditioning, this solution is often computationally expensive. Instead, we propose replacing the softmax with an alternative family of policy parameterizations based on the generalized f-softargmax. We further advocate coupling this parameterization with a regularizer induced by the same f-divergence, which improves the optimization landscape and ensures that the resulting regularized objective satisfies a Polyak-Lojasiewicz inequality. Leveraging this structure, we establish the first explicit non-asymptotic last-iterate convergence guarantees for stochastic policy gradient methods for finite MDPs without any form of preconditioning. We also derive sample-complexity bounds for the unregularized problem and show that f-PG, with Tsallis divergences achieves polynomial sample complexity in contrast to the exponential complexity incurred by the standard softmax parameterization.

</details>


### [293] [Towards Robust Universal Perturbation Attacks: A Float-Coded, Penalty-Driven Evolutionary Approach](https://arxiv.org/abs/2601.12624)
*Shiqi Wang,Mahdi Khosravy,Neeraj Gupta,Olaf Witkowski*

Main category: cs.LG

TL;DR: 提出用于生成通用对抗扰动（UAP）的浮点编码、惩罚驱动单目标进化框架，在ImageNet数据集实验效果优于现有基于进化的方法。


<details>
  <summary>Details</summary>
Motivation: 通用对抗扰动可通过单一噪声模式破坏多个输入的深度神经网络，进化算法在生成此类扰动上有潜力。

Method: 引入浮点编码、惩罚驱动单目标进化框架，利用连续基因表示、动态进化算子与自适应调度，模块化PyTorch实现，跨多样模型测试并定期切换批次。

Result: 在ImageNet数据集上，该框架生成的扰动范数更小、误分类效果更好、收敛更快。

Conclusion: 该方法对各种深度学习架构的通用对抗攻击具有鲁棒性和可扩展性。

Abstract: Universal adversarial perturbations (UAPs) have garnered significant attention due to their ability to undermine deep neural networks across multiple inputs using a single noise pattern. Evolutionary algorithms offer a promising approach to generating such perturbations due to their ability to navigate non-convex, gradient-free landscapes. In this work, we introduce a float-coded, penalty-driven single-objective evolutionary framework for UAP generation that achieves lower visibility perturbations while enhancing attack success rates. Our approach leverages continuous gene representations aligned with contemporary deep learning scales, incorporates dynamic evolutionary operators with adaptive scheduling, and utilizes a modular PyTorch implementation for seamless integration with modern architectures. Additionally, we ensure the universality of the generated perturbations by testing across diverse models and by periodically switching batches to prevent overfitting. Experimental results on the ImageNet dataset demonstrate that our framework consistently produces perturbations with smaller norms, higher misclassification effectiveness, and faster convergence compared to existing evolutionary-based methods. These findings highlight the robustness and scalability of our approach for universal adversarial attacks across various deep learning architectures.

</details>


### [294] [Topology-Aware Multiscale Mixture of Experts for Efficient Molecular Property Prediction](https://arxiv.org/abs/2601.12637)
*Long D. Nguyen,Kelin Xia,Binh P. Nguyen*

Main category: cs.LG

TL;DR: 提出多尺度交互专家混合模型（MI - MoE）用于3D分子图学习，可跨几何区域自适应交互建模，在多个基准数据集上提升性能。


<details>
  <summary>Details</summary>
Motivation: 多数3D分子图神经网络依赖全局固定邻域启发式方法定义局部消息传递邻域，导致交互预算僵化、与数据无关，而分子许多属性依赖3D几何结构。

Method: 引入距离截止专家集成、设计拓扑门控编码器，且MI - MoE是可插入模块。

Result: MI - MoE能在不同分子和聚合物属性预测基准数据集上持续提升多个3D分子骨干网络性能，涵盖回归和分类任务。

Conclusion: 拓扑感知的多尺度路由是3D分子图学习的有效原则。

Abstract: Many molecular properties depend on 3D geometry, where non-covalent interactions, stereochemical effects, and medium- to long-range forces are determined by spatial distances and angles that cannot be uniquely captured by a 2D bond graph. Yet most 3D molecular graph neural networks still rely on globally fixed neighborhood heuristics, typically defined by distance cutoffs and maximum neighbor limits, to define local message-passing neighborhoods, leading to rigid, data-agnostic interaction budgets. We propose Multiscale Interaction Mixture of Experts (MI-MoE) to adapt interaction modeling across geometric regimes. Our contributions are threefold: (1) we introduce a distance-cutoff expert ensemble that explicitly captures short-, mid-, and long-range interactions without committing to a single cutoff; (2) we design a topological gating encoder that routes inputs to experts using filtration-based descriptors, including persistent homology features, summarizing how connectivity evolves across radii; and (3) we show that MI-MoE is a plug-in module that consistently improves multiple strong 3D molecular backbones across diverse molecular and polymer property prediction benchmark datasets, covering both regression and classification tasks. These results highlight topology-aware multiscale routing as an effective principle for 3D molecular graph learning.

</details>


### [295] [Explanation Multiplicity in SHAP: Characterization and Assessment](https://arxiv.org/abs/2601.12654)
*Hyunseung Hwang,Seungeun Lee,Lucas Rosenblatt,Julia Stoyanovich,Steven Euijong Whang*

Main category: cs.LG

TL;DR: 研究指出SHAP解释存在解释多重性现象，提出刻画方法并分析，发现该现象普遍存在。


<details>
  <summary>Details</summary>
Motivation: 解决SHAP解释在输入、任务和模型固定时多次运行结果差异大的问题。

Method: 提出刻画特征归因解释多重性的方法，区分模型训练/选择与解释流程随机性的影响，推导随机基线值。

Result: 发现基于幅度的距离接近零，而基于排名的指标显示特征排名有较大变化，解释多重性普遍存在。

Conclusion: 需要与解释预期用途相匹配的指标和基线。

Abstract: Post-hoc explanations are widely used to justify, contest, and audit automated decisions in high-stakes domains. SHAP, in particular, is often treated as a reliable account of which features drove an individual prediction. Yet SHAP explanations can vary substantially across repeated runs even when the input, task, and trained model are held fixed. We term this phenomenon explanation multiplicity: multiple internally valid but substantively different explanations for the same decision. We present a methodology to characterize multiplicity in feature-attribution explanations and to disentangle sources due to model training/selection from stochasticity intrinsic to the explanation pipeline. We further show that apparent stability depends on the metric: magnitude-based distances can remain near zero while rank-based measures reveal substantial churn in the identity and ordering of top features. To contextualize observed disagreement, we derive randomized baseline values under plausible null models. Across datasets, model classes, and confidence regimes, we find explanation multiplicity is pervasive and persists even for high-confidence predictions, highlighting the need for metrics and baselines that match the intended use of explanations.

</details>


### [296] [Decentralized Learning Strategies for Estimation Error Minimization with Graph Neural Networks](https://arxiv.org/abs/2601.12662)
*Xingran Chen,Navid NaderiAlizadeh,Alejandro Ribeiro,Shirin Saeedi Bidokhti*

Main category: cs.LG

TL;DR: 提出图形化多智能体强化学习框架用于动态多跳无线网络中自回归马尔可夫源的实时采样和估计，政策可迁移且表现优。


<details>
  <summary>Details</summary>
Motivation: 动态多跳无线网络中，因动作空间高维和网络拓扑复杂，难以解析得出最优策略。

Method: 提出图形化多智能体强化学习框架进行策略优化。

Result: 提出的策略优于现有基线；训练的策略可迁移到更大网络，性能随智能体数量增加；图形训练过程能承受非平稳性；循环在学习和执行中很关键，能提高对非平稳性的恢复能力。

Conclusion: 提出的策略可迁移，能有效解决动态多跳无线网络中的实时采样和估计问题。

Abstract: We address real-time sampling and estimation of autoregressive Markovian sources in dynamic yet structurally similar multi-hop wireless networks. Each node caches samples from others and communicates over wireless collision channels, aiming to minimize time-average estimation error via decentralized policies. Due to the high dimensionality of action spaces and complexity of network topologies, deriving optimal policies analytically is intractable. To address this, we propose a graphical multi-agent reinforcement learning framework for policy optimization. Theoretically, we demonstrate that our proposed policies are transferable, allowing a policy trained on one graph to be effectively applied to structurally similar graphs. Numerical experiments demonstrate that (i) our proposed policy outperforms state-of-the-art baselines; (ii) the trained policies are transferable to larger networks, with performance gains increasing with the number of agents; (iii) the graphical training procedure withstands non-stationarity, even when using independent learning techniques; and (iv) recurrence is pivotal in both independent learning and centralized training and decentralized execution, and improves the resilience to non-stationarity.

</details>


### [297] [MetaToolAgent: Towards Generalizable Tool Usage in LLMs through Meta-Learning](https://arxiv.org/abs/2601.12680)
*Zheng Fang,Wolfgang Mayer,Zeyu Zhang,Jian Wang,Hong-Yu Zhang,Wanli Li,Zaiwen Feng*

Main category: cs.LG

TL;DR: 提出综合数据集和MetaToolAgent提升大语言模型工具选择泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有工具选择方法局限于有限工具集，难以泛化到新工具，需解决此问题。

Method: 引入涵盖7个领域、155个工具和9377个问答对的综合数据集，提出MetaToolAgent元学习方法。

Result: MTA在未见工具上显著优于基线方法。

Conclusion: MTA对构建需动态工具协调的灵活可扩展系统有前景。

Abstract: Tool learning is increasingly important for large language models (LLMs) to effectively coordinate and utilize a diverse set of tools in order to solve complex real-world tasks. By selecting and integrating appropriate tools, LLMs extend their capabilities beyond pure language understanding to perform specialized functions. However, existing methods for tool selection often focus on limited tool sets and struggle to generalize to novel tools encountered in practical deployments. To address these challenges, we introduce a comprehensive dataset spanning 7 domains, containing 155 tools and 9,377 question-answer pairs, which simulates realistic integration scenarios. Additionally, we propose MetaToolAgent (MTA), a meta-learning approach designed to improve cross-tool generalization. Experimental results show that MTA significantly outperforms baseline methods on unseen tools, demonstrating its promise for building flexible and scalable systems that require dynamic tool coordination.

</details>


### [298] [Resource-Conscious RL Algorithms for Deep Brain Stimulation](https://arxiv.org/abs/2601.12699)
*Arkaprava Gupta,Nicholas Carter,William Zellers,Prateek Ganguli,Benedikt Dietrich,Vibhor Krishna,Parasara Sridhar Duggirala,Samarjit Chakraborty*

Main category: cs.LG

TL;DR: 提出用于DBS的T3P MAB RL方法，比现有算法有效，轻量级可部署在植入物中，能同时调节频率和幅度，在硬件上实现并证明适用于资源受限平台。


<details>
  <summary>Details</summary>
Motivation: 现有的DBS固定频率和幅度方法有副作用和电池寿命问题，现有RL算法复杂难在体内训练，多数仅调节频率或幅度。

Method: 提出Time & Threshold - Triggered Multi - Armed Bandit (T3P MAB) RL方法，可同时调节DBS信号频率和幅度。

Result: T3P MAB算法在多种微控制器单元设置上展示了功耗效率，在硬件上实现了DBS的MAB代理并报告能量测量。

Conclusion: T3P MAB RL方法更有效、轻量级，适合资源受限平台，能改善DBS治疗效果。

Abstract: Deep Brain Stimulation (DBS) has proven to be a promising treatment of Parkinson's Disease (PD). DBS involves stimulating specific regions of the brain's Basal Ganglia (BG) using electric impulses to alleviate symptoms of PD such as tremors, rigidity, and bradykinesia. Although most clinical DBS approaches today use a fixed frequency and amplitude, they suffer from side effects (such as slurring of speech) and shortened battery life of the implant. Reinforcement learning (RL) approaches have been used in recent research to perform DBS in a more adaptive manner to improve overall patient outcome. These RL algorithms are, however, too complex to be trained in vivo due to their long convergence time and requirement of high computational resources.
  We propose a new Time & Threshold-Triggered Multi-Armed Bandit (T3P MAB) RL approach for DBS that is more effective than existing algorithms. Further, our T3P agent is lightweight enough to be deployed in the implant, unlike current deep-RL strategies, and even forgoes the need for an offline training phase. Additionally, most existing RL approaches have focused on modulating only frequency or amplitude, and the possibility of tuning them together remains greatly unexplored in the literature. Our RL agent can tune both frequency and amplitude of DBS signals to the brain with better sample efficiency and requires minimal time to converge. We implement an MAB agent for DBS for the first time on hardware to report energy measurements and prove its suitability for resource-constrained platforms. Our T3P MAB algorithm is deployed on a variety of microcontroller unit (MCU) setups to show its efficiency in terms of power consumption as opposed to other existing RL approaches used in recent work.

</details>


### [299] [Towards Spectroscopy: Susceptibility Clusters in Language Models](https://arxiv.org/abs/2601.12703)
*Andrew Gordon,Garrett Baker,George Wang,William Snell,Stan van Wingerden,Daniel Murfet*

Main category: cs.LG

TL;DR: 将光谱学原理应用于神经网络，通过测量扰动响应研究，理论上解释了敏感性分解，实证开发聚类算法并验证与稀疏自编码器结果相似。


<details>
  <summary>Details</summary>
Motivation: 将光谱学测量系统内部结构的原理应用到神经网络，以研究其内部规律。

Method: 通过提高上下文中某一标记的权重来扰动数据分布，利用随机梯度朗之万动力学（SGLD）计算敏感性；开发基于传导性的聚类算法。

Result: 理论上，敏感性可分解为数据分布模式之和；实证中，在 Pythia - 14M 上识别出 510 个可解释的聚类，且 50%的聚类与稀疏自编码器特征匹配。

Conclusion: 该方法能有效识别神经网络中可解释的结构，且与稀疏自编码器得出相似结果。

Abstract: Spectroscopy infers the internal structure of physical systems by measuring their response to perturbations. We apply this principle to neural networks: perturbing the data distribution by upweighting a token $y$ in context $x$, we measure the model's response via susceptibilities $χ_{xy}$, which are covariances between component-level observables and the perturbation computed over a localized Gibbs posterior via stochastic gradient Langevin dynamics (SGLD). Theoretically, we show that susceptibilities decompose as a sum over modes of the data distribution, explaining why tokens that follow their contexts "for similar reasons" cluster together in susceptibility space. Empirically, we apply this methodology to Pythia-14M, developing a conductance-based clustering algorithm that identifies 510 interpretable clusters ranging from grammatical patterns to code structure to mathematical notation. Comparing to sparse autoencoders, 50% of our clusters match SAE features, validating that both methods recover similar structure.

</details>


### [300] [Adaptively trained Physics-informed Radial Basis Function Neural Networks for Solving Multi-asset Option Pricing Problems](https://arxiv.org/abs/2601.12704)
*Yan Ma,Yumeng Ren*

Main category: cs.LG

TL;DR: 研究用物理信息机器学习算法解决多标的资产期权定价的Black - Scholes偏微分方程，通过实验验证方法有效性。


<details>
  <summary>Details</summary>
Motivation: 探索多标的资产期权定价中Black - Scholes偏微分方程的数值解。

Method: 开发基于径向基函数神经网络的物理信息机器学习算法（PIRBFNN），结合传统方法和机器学习优势，用PDE残差技术优化隐藏神经元分布。

Result: 通过单资产欧式看跌期权、双资产交换期权和四资产篮子看涨期权等实验验证方法有效性。

Conclusion: 所提PIRBFNN方法能准确高效处理具有非平滑收益条件的多维期权定价模型。

Abstract: The present study investigates the numerical solution of Black-Scholes partial differential equation (PDE) for option valuation with multiple underlying assets. We develop a physics-informed (PI) machine learning algorithm based on a radial basis function neural network (RBFNN) that concurrently optimizes the network architecture and predicts the target option price. The physics-informed radial basis function neural network (PIRBFNN) combines the strengths of the traditional radial basis function collocation method and the physics-informed neural network machine learning approach to effectively solve PDE problems in the financial context. By employing a PDE residual-based technique to adaptively refine the distribution of hidden neurons during the training process, the PIRBFNN facilitates accurate and efficient handling of multidimensional option pricing models featuring non-smooth payoff conditions. The validity of the proposed method is demonstrated through a set of experiments encompassing a single-asset European put option, a double-asset exchange option, and a four-asset basket call option.

</details>


### [301] [Trend-Adjusted Time Series Models with an Application to Gold Price Forecasting](https://arxiv.org/abs/2601.12706)
*Sina Kazemdehbashi*

Main category: cs.LG

TL;DR: 本文将时间序列预测重构为趋势预测和定量值预测两部分，提出TATS模型，用黄金价格数据验证其优于传统模型，且指出常用评估指标不足。


<details>
  <summary>Details</summary>
Motivation: 解决时间序列预测挑战，提高预测准确性，优化模型评估方式。

Method: 将时间序列预测拆分为两部分任务，用二元分类器预测趋势，LSTM和Bi - LSTM预测定量值，提出TATS模型并结合理论与实证评估。

Result: TATS模型在黄金价格预测中显著降低预测误差，优于标准LSTM和Bi - LSTM模型；常用评估指标如MSE和MAE不足以全面评估模型性能。

Conclusion: TATS模型在时间序列预测上表现更优，评估时间序列模型性能应结合趋势检测准确性指标。

Abstract: Time series data play a critical role in various fields, including finance, healthcare, marketing, and engineering. A wide range of techniques (from classical statistical models to neural network-based approaches such as Long Short-Term Memory (LSTM)) have been employed to address time series forecasting challenges. In this paper, we reframe time series forecasting as a two-part task: (1) predicting the trend (directional movement) of the time series at the next time step, and (2) forecasting the quantitative value at the next time step. The trend can be predicted using a binary classifier, while quantitative values can be forecasted using models such as LSTM and Bidirectional Long Short-Term Memory (Bi-LSTM). Building on this reframing, we propose the Trend-Adjusted Time Series (TATS) model, which adjusts the forecasted values based on the predicted trend provided by the binary classifier. We validate the proposed approach through both theoretical analysis and empirical evaluation. The TATS model is applied to a volatile financial time series (the daily gold price) with the objective of forecasting the next days price. Experimental results demonstrate that TATS consistently outperforms standard LSTM and Bi-LSTM models by achieving significantly lower forecasting error. In addition, our results indicate that commonly used metrics such as MSE and MAE are insufficient for fully assessing time series model performance. Therefore, we also incorporate trend detection accuracy, which measures how effectively a model captures trends in a time series.

</details>


### [302] [Distribution-Centric Policy Optimization Dominates Exploration-Exploitation Trade-off](https://arxiv.org/abs/2601.12730)
*Zhaochun Li,Chen Wang,Jionghao Bai,Shisheng Cui,Ge Lan,Zhou Zhao,Yue Wang*

Main category: cs.LG

TL;DR: 本文提出分布中心策略优化（DCPO）方法解决大语言模型强化学习中探索-利用权衡问题，在多基准上平均优于GRPO约20%。


<details>
  <summary>Details</summary>
Motivation: 现有解决探索 - 利用权衡问题的方法多以样本为中心，依赖信息样本的‘运气’，缺乏对策略的原则性控制，收益有限或不稳定。

Method: 引入分布中心视角，将熵调节重新表述为分布级正则化，提出DCPO方法。

Result: 在多个模型和七个基准测试中，DCPO平均比GRPO提高约20%。

Conclusion: DCPO用分布级原则取代样本级启发式，为可控探索提供理论基础和灵活框架，实现更强的探索 - 利用权衡。

Abstract: The exploration-exploitation (EE) trade-off is a central challenge in reinforcement learning (RL) for large language models (LLMs). With Group Relative Policy Optimization (GRPO), training tends to be exploitation driven: entropy decreases monotonically, samples convergence, and exploration fades. Most existing fixes are \textbf{sample-centric}: they seek or bonus rare samples, assuming exploration comes from novel trajectories and tokens. These heuristics depend on the "luck" of informative samples, lack principled control of the policy, and often yield limited or inconsistent gains. In this work, we are the first to introduce a \textbf{distribution-centric} perspective for RL, in which exploration is always guided by a "better" target distribution, and reveal that a policy's ability to resist entropy collapse is governed by the distribution itself rather than individual samples. Building on this insight, we propose Distribution-Centric Policy Optimization (DCPO), which reformulates entropy regulation as distribution-level regularization. DCPO achieves controllable entropy fully on-policy without sampling from external distributions, enabling efficient exploration while maintaining training stability. Across multiple models and seven benchmarks, DCPO improves over GRPO by about 20\% on average. Overall, DCPO replaces sample-level heuristics with distribution-level principles, offering a theoretically grounded and flexible framework for controllable exploration and a stronger EE trade-off. The code is available in https://github.com/597358816/DCPO.

</details>


### [303] [A Graph Prompt Fine-Tuning Method for WSN Spatio-Temporal Correlation Anomaly Detection](https://arxiv.org/abs/2601.12745)
*Miao Ye,Jing Cui,Yuan huang,Qian He,Yong Wang,Jiwen Zhang*

Main category: cs.LG

TL;DR: 针对无线传感器网络多时间模态数据异常检测问题，设计含时空相关特征的图神经网络骨干网络和多任务自监督训练策略，实验表明该方法检测性能和泛化能力优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多时间模态数据异常检测方法存在时空相关特征提取不足、异常样本类别标注成本高和样本不平衡等问题。

Method: 基于多尺度策略和模态间融合方法改进Mamba模型并结合变分图卷积模块设计异常检测骨干网络；设计三子任务学习的‘预训练’方法和‘图提示 - 微调’机制。

Result: 在公共数据集和实际采集数据集上实验的F1指标分别达91.30%和92.31%。

Conclusion: 该方法比现有方法有更好的检测性能和泛化能力。

Abstract: Anomaly detection of multi-temporal modal data in Wireless Sensor Network (WSN) can provide an important guarantee for reliable network operation. Existing anomaly detection methods in multi-temporal modal data scenarios have the problems of insufficient extraction of spatio-temporal correlation features, high cost of anomaly sample category annotation, and imbalance of anomaly samples. In this paper, a graph neural network anomaly detection backbone network incorporating spatio-temporal correlation features and a multi-task self-supervised training strategy of "pre-training - graph prompting - fine-tuning" are designed for the characteristics of WSN graph structure data. First, the anomaly detection backbone network is designed by improving the Mamba model based on a multi-scale strategy and inter-modal fusion method, and combining it with a variational graph convolution module, which is capable of fully extracting spatio-temporal correlation features in the multi-node, multi-temporal modal scenarios of WSNs. Secondly, we design a three-subtask learning "pre-training" method with no-negative comparative learning, prediction, and reconstruction to learn generic features of WSN data samples from unlabeled data, and design a "graph prompting-fine-tuning" mechanism to guide the pre-trained self-supervised learning. The model is fine-tuned through the "graph prompting-fine-tuning" mechanism to guide the pre-trained self-supervised learning model to complete the parameter fine-tuning, thereby reducing the training cost and enhancing the detection generalization performance. The F1 metrics obtained from experiments on the public dataset and the actual collected dataset are up to 91.30% and 92.31%, respectively, which provides better detection performance and generalization ability than existing methods designed by the method.

</details>


### [304] [A Boolean Function-Theoretic Framework for Expressivity in GNNs with Applications to Fair Graph Mining](https://arxiv.org/abs/2601.12751)
*Manjish Pal*

Main category: cs.LG

TL;DR: 提出基于布尔函数理论的GNN表达性框架，引入SBI概念，识别公平感知GNN表达性的关键障碍，设计算法并在实验中取得好效果。


<details>
  <summary>Details</summary>
Motivation: 对图神经网络（GNN）捕获复杂子群体结构的能力进行细粒度分析，解决公平性相关的表达性问题。

Method: 提出基于布尔函数理论的表达性框架，引入子群体布尔同构（SBI）概念，设计基于电路遍历的公平算法。

Result: 理论上识别出公平感知GNN表达性的关键障碍；实验中在真实图数据上，方法在现有方法失败的交叉群体中实现低公平差距。

Conclusion: 首次为公平性定制了GNN表达性的原则性处理方法。

Abstract: We propose a novel expressivity framework for Graph Neural Networks (GNNs) grounded in Boolean function theory, enabling a fine-grained analysis of their ability to capture complex subpopulation structures. We introduce the notion of \textit{Subpopulation Boolean Isomorphism} (SBI) as an invariant that strictly subsumes existing expressivity measures such as Weisfeiler-Lehman (WL), biconnectivity-based, and homomorphism-based frameworks. Our theoretical results identify Fourier degree, circuit class (AC$^0$, NC$^1$), and influence as key barriers to expressivity in fairness-aware GNNs. We design a circuit-traversal-based fairness algorithm capable of handling subpopulations defined by high-complexity Boolean functions, such as parity, which break existing baselines. Experiments on real-world graphs show that our method achieves low fairness gaps across intersectional groups where state-of-the-art methods fail, providing the first principled treatment of GNN expressivity tailored to fairness.

</details>


### [305] [Eddy-Resolving Global Ocean Forecasting with Multi-Scale Graph Neural Networks](https://arxiv.org/abs/2601.12775)
*Yuta Hirabayashi,Daisuke Matusoka,Konobu Kimura*

Main category: cs.LG

TL;DR: 提出多尺度图神经网络海洋模型用于10天全球预报，提升短期预报技能和多尺度海洋变率表征能力。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动海洋模型在全球涡旋分辨海洋预报应用有限，准确表征多尺度海洋动力是主要挑战。

Method: 采用编码器 - 处理器 - 解码器架构，使用两种不同分辨率的球面网格，将表面大气变量和海洋状态变量作为节点输入。

Result: 模型能准确表征大范围空间尺度，短期预报技能提升。

Conclusion: 该模型可提供更准确短期预报，改善多尺度海洋动力表征，推动数据驱动的涡旋分辨全球海洋预报。

Abstract: Research on data-driven ocean models has progressed rapidly in recent years; however, the application of these models to global eddy-resolving ocean forecasting remains limited. The accurate representation of ocean dynamics across a wide range of spatial scales remains a major challenge in such applications. This study proposes a multi-scale graph neural network-based ocean model for 10-day global forecasting that improves short-term prediction skill and enhances the representation of multi-scale ocean variability. The model employs an encoder-processor-decoder architecture and uses two spherical meshes with different resolutions to better capture the multi-scale nature of ocean dynamics. In addition, the model incorporates surface atmospheric variables along with ocean state variables as node inputs to improve short-term prediction accuracy by representing atmospheric forcing. Evaluation using surface kinetic energy spectra and case studies shows that the model accurately represents a broad range of spatial scales, while root mean square error comparisons demonstrate improved skill in short-term predictions. These results indicate that the proposed model delivers more accurate short-term forecasts and improved representation of multi-scale ocean dynamics, thereby highlighting its potential to advance data-driven, eddy-resolving global ocean forecasting.

</details>


### [306] [Distilling Time Series Foundation Models for Efficient Forecasting](https://arxiv.org/abs/2601.12785)
*Yuqi Li,Kuiye Ding,Chuanguang Yang,Szu-Yu Chen,Yingli Tian*

Main category: cs.LG

TL;DR: 介绍DistilTS蒸馏框架用于TSFMs，能解决关键挑战，减少参数并加速推理，性能与全尺寸模型相当。


<details>
  <summary>Details</summary>
Motivation: TSFMs参数大部署成本高，现有知识蒸馏技术不适用于时间序列预测。

Method: 提出DistilTS框架，引入地平线加权目标平衡学习，采用时间对齐策略减少架构不匹配。

Result: 在多个基准测试中，DistilTS性能与全尺寸TSFMs相当，参数减少达1/150，推理加速达6000x。

Conclusion: DistilTS有效解决TSFMs部署成本高问题，可用于时间序列预测。

Abstract: Time Series foundation models (TSFMs) deliver strong forecasting performance through large-scale pretraining, but their large parameter sizes make deployment costly. While knowledge distillation offers a natural and effective approach for model compression, techniques developed for general machine learning tasks are not directly applicable to time series forecasting due to the unique characteristics. To address this, we present DistilTS, the first distillation framework specifically designed for TSFMs. DistilTS addresses two key challenges: (1) task difficulty discrepancy, specific to forecasting, where uniform weighting makes optimization dominated by easier short-term horizons, while long-term horizons receive weaker supervision; and (2) architecture discrepancy, a general challenge in distillation, for which we design an alignment mechanism in the time series forecasting. To overcome these issues, DistilTS introduces horizon-weighted objectives to balance learning across horizons, and a temporal alignment strategy that reduces architectural mismatch, enabling compact models. Experiments on multiple benchmarks demonstrate that DistilTS achieves forecasting performance comparable to full-sized TSFMs, while reducing parameters by up to 1/150 and accelerating inference by up to 6000x. Code is available at: https://github.com/itsnotacie/DistilTS-ICASSP2026.

</details>


### [307] [Semi-supervised Instruction Tuning for Large Language Models on Text-Attributed Graphs](https://arxiv.org/abs/2601.12807)
*Zixing Song,Irwin King*

Main category: cs.LG

TL;DR: 提出半监督指令微调管道SIT - Graph用于图学习，提升文本属性图基准测试性能。


<details>
  <summary>Details</summary>
Motivation: 当前图学习任务的指令调优方法需大量标签，在社交领域难获取，且未利用无标签节点，本研究旨在解决该问题。

Method: 提出SIT - Graph，通过迭代自训练，先基于标签节点微调，再为无标签节点生成伪响应扩充数据集进行后续微调。

Result: 将SIT - Graph融入现有方法，在文本属性图基准测试中显著提升性能，低标签率下提升超20%。

Conclusion: SIT - Graph有效且可无缝集成到现有图指令调优方法中，提升其在图学习任务中的表现。

Abstract: The emergent reasoning capabilities of Large Language Models (LLMs) offer a transformative paradigm for analyzing text-attributed graphs. While instruction tuning is the prevailing method for adapting pre-trained LLMs to graph learning tasks like node classification, it requires a substantial volume of annotated (INSTRUCTION, OUTPUT) pairs deriving from labeled nodes. This requirement is particularly prohibitive in the social domain, where obtaining expert labels for sensitive or evolving content is costly and slow. Furthermore, standard graph instruction tuning fails to exploit the vast amount of unlabeled nodes, which contain latent correlations due to edge connections that are beneficial for downstream predictions. To bridge this gap, we propose a novel Semi-supervised Instruction Tuning pipeline for Graph Learning, named SIT-Graph. Notably, SIT-Graph is model-agnostic and can be seamlessly integrated into any graph instruction tuning method that utilizes LLMs as the predictor. SIT-Graph operates via an iterative self-training process. Initially, the model is fine-tuned using instruction pairs constructed solely from the labeled nodes. Then it generates confidence-filtered pseudo-responses for unlabeled nodes to strategically augment the dataset for the next round of fine-tuning. Finally, this iterative refinement progressively aligns the LLM with the underlying node correlations. Extensive experiments demonstrate that when incorporated into state-of-the-art graph instruction tuning methods, SIT-Graph significantly enhances their performance on text-attributed graph benchmarks, achieving over 20% improvement under the low label ratio settings.

</details>


### [308] [Fisher-Orthogonal Projected Natural Gradient Descent for Continual Learning](https://arxiv.org/abs/2601.12816)
*Ishir Garg,Neel Kolhe,Andy Peng,Rohan Gopalam*

Main category: cs.LG

TL;DR: 提出FOPNG优化器用于持续学习，统一自然梯度下降与正交梯度方法，在多个基准测试有好结果。


<details>
  <summary>Details</summary>
Motivation: 解决持续学习中学习新任务时灾难性遗忘旧任务的问题。

Method: 提出FOPNG优化器，将梯度投影到先前任务梯度的Fisher正交补空间，在信息几何框架下统一自然梯度下降与正交梯度方法。

Result: 在Permuted - MNIST、Split - MNIST等标准持续学习基准测试中取得良好结果。

Conclusion: FOPNG优化器能在学习新任务时保留旧任务性能，且更新方向在重新参数化下不变，保证在Fisher度量中下降。

Abstract: Continual learning aims to enable neural networks to acquire new knowledge on sequential tasks. However, the key challenge in such settings is to learn new tasks without catastrophically forgetting previously learned tasks. We propose the Fisher-Orthogonal Projected Natural Gradient Descent (FOPNG) optimizer, which enforces Fisher-orthogonal constraints on parameter updates to preserve old task performance while learning new tasks. Unlike existing methods that operate in Euclidean parameter space, FOPNG projects gradients onto the Fisher-orthogonal complement of previous task gradients. This approach unifies natural gradient descent with orthogonal gradient methods within an information-geometric framework. The resulting update direction is invariant under reparameterization, guarantees descent in the Fisher metric, and helps preserve prior task outputs. We provide theoretical analysis establishing the properties of the projected update, describe efficient and practical implementations using the diagonal Fisher, and demonstrate strong results on standard continual learning benchmarks such as Permuted-MNIST, Split-MNIST, Rotated-MNIST, Split-CIFAR10, and Split-CIFAR100.

</details>


### [309] [Generating Cyclic Conformers with Flow Matching in Cremer-Pople Coordinates](https://arxiv.org/abs/2601.12859)
*Luca Schaufelberger,Aline Hartgers,Kjell Jorner*

Main category: cs.LG

TL;DR: 本文介绍生成式机器学习模型PuckerFlow用于环系统构象体生成，性能优于其他方法，为化学和生物学相关应用奠定基础。


<details>
  <summary>Details</summary>
Motivation: 可靠地采样环系统的构象体集合仍具有挑战性。

Method: 在Cremer - Pople空间上进行流匹配的生成式机器学习模型PuckerFlow。

Result: PuckerFlow在生成多样且精确的构象体方面表现出色，在几乎所有定量指标上优于其他构象体生成方法。

Conclusion: 该工作实现了环状结构的高效可靠构象体生成，为化学和生物学中建模结构 - 性质关系及性质导向的环生成铺平道路。

Abstract: Cyclic molecules are ubiquitous across applications in chemistry and biology. Their restricted conformational flexibility provides structural pre-organization that is key to their function in drug discovery and catalysis. However, reliably sampling the conformer ensembles of ring systems remains challenging. Here, we introduce PuckerFlow, a generative machine learning model that performs flow matching on the Cremer-Pople space, a low-dimensional internal coordinate system capturing the relevant degrees of freedom of rings. Our approach enables generation of valid closed rings by design and demonstrates strong performance in generating conformers that are both diverse and precise. We show that PuckerFlow outperforms other conformer generation methods on nearly all quantitative metrics and illustrate the potential of PuckerFlow for ring systems relevant to chemical applications, particularly in catalysis and drug discovery. This work enables efficient and reliable conformer generation of cyclic structures, paving the way towards modeling structure-property relationships and the property-guided generation of rings across a wide range of applications in chemistry and biology.

</details>


### [310] [Hierarchical Sparse Circuit Extraction from Billion-Parameter Language Models through Scalable Attribution Graph Decomposition](https://arxiv.org/abs/2601.12879)
*Mohammed Mudassir Uddin,Shahnawaz Alam,Mohammed Kaif Pasha*

Main category: cs.LG

TL;DR: 提出HAGD框架解决从大语言模型中提取稀疏计算电路的难题，降低复杂度，经多模型验证有一定效果并指出当前归因方法局限。


<details>
  <summary>Details</summary>
Motivation: 机械可解释性旨在将神经网络计算逆向工程为人类可理解的算法，但从数十亿参数语言模型中提取稀疏计算电路因指数搜索复杂性和普遍的多语义性而具有挑战性。

Method: 提出Hierarchical Attribution Graph Decomposition (HAGD)框架，通过多分辨率抽象层次结构和可微电路搜索将电路发现复杂性从O(2^n)的穷举枚举降低到O(n^2 log n)，集成跨层转码器、图神经网络元学习和因果干预协议。

Result: 在模块化算术任务上框架实现高达91%的行为保留，跨架构转移实验显示发现的电路在模型家族间有67%的结构相似性。

Conclusion: 为更大模型规模的可解释性提供初步基础，指出当前归因方法存在显著局限性，需要未来改进。

Abstract: Mechanistic interpretability seeks to reverse-engineer neural network computations into human-understandable algorithms, yet extracting sparse computational circuits from billion-parameter language models remains challenging due to exponential search complexity and pervasive polysemanticity. The proposed Hierarchical Attribution Graph Decomposition (HAGD) framework reduces circuit discovery complexity from O(2^n) exhaustive enumeration to O(n^2 log n) through multi-resolution abstraction hierarchies and differentiable circuit search. The methodology integrates cross-layer transcoders for monosemantic feature extraction, graph neural network meta-learning for topology prediction, and causal intervention protocols for validation. Empirical evaluation spans GPT-2 variants, Llama-7B through Llama-70B, and Pythia suite models across algorithmic tasks and natural language benchmarks. On modular arithmetic tasks, the framework achieves up to 91% behavioral preservation ($\pm$2.3\% across runs) while maintaining interpretable subgraph sizes. Cross-architecture transfer experiments suggest that discovered circuits exhibit moderate structural similarity (averaging 67%) across model families, indicating potential shared computational patterns. These results provide preliminary foundations for interpretability at larger model scales while identifying significant limitations in current attribution methodologies that require future advances.

</details>


### [311] [AdaNODEs: Test Time Adaptation for Time Series Forecasting Using Neural ODEs](https://arxiv.org/abs/2601.12893)
*Ting Dang,Soumyajit Chatterjee,Hong Jia,Yu Wu,Flora Salim,Fahim Kawsar*

Main category: cs.LG

TL;DR: 提出适用于时间序列预测的源自由TTA方法AdaNODEs，通过实验证明其有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多数TTA方法针对独立数据，忽略时间序列数据和预测任务，本文旨在为时间序列预测提供新的TTA方法。

Method: 利用Neural Ordinary Differential Equations (NODEs)提出适应框架，创新提出新损失函数，仅更新有限模型参数。

Result: 在一维和高维数据实验中，比SOTA基线分别提升5.88%和28.4%，在更高程度分布偏移中有鲁棒性。

Conclusion: AdaNODEs是有效的时间序列预测TTA方法，能捕捉时间依赖且节省内存。

Abstract: Test time adaptation (TTA) has emerged as a promising solution to adapt pre-trained models to new, unseen data distributions using unlabeled target domain data. However, most TTA methods are designed for independent data, often overlooking the time series data and rarely addressing forecasting tasks. This paper presents AdaNODEs, an innovative source-free TTA method tailored explicitly for time series forecasting. By leveraging Neural Ordinary Differential Equations (NODEs), we propose a novel adaptation framework that accommodates the unique characteristics of distribution shifts in time series data. Moreover, we innovatively propose a new loss function to tackle TTA for forecasting tasks. AdaNODEs only requires updating limited model parameters, showing effectiveness in capturing temporal dependencies while avoiding significant memory usage. Extensive experiments with one- and high-dimensional data demonstrate that AdaNODEs offer relative improvements of 5.88\% and 28.4\% over the SOTA baselines, especially demonstrating robustness across higher severity distribution shifts.

</details>


### [312] [Supervised Learning for the (s,S) Inventory Model with General Interarrival Demands and General Lead Times](https://arxiv.org/abs/2601.12900)
*Eliran Sherzer,Yonit Barron*

Main category: cs.LG

TL;DR: 提出用神经网络模型的监督学习框架近似(s,S)库存系统的平稳性能指标，训练后能快速预测系统指标，替代昂贵的模拟运行，且可扩展到其他库存模型。


<details>
  <summary>Details</summary>
Motivation: 连续审查(s,S)库存模型在非马尔可夫系统中分析困难，评估长期性能指标依赖昂贵的模拟。

Method: 采用神经网络模型建立监督学习框架，先用模拟生成训练标签，再训练神经网络。

Result: 使用少量分布的低阶矩作为输入足以训练神经网络并准确捕捉稳态分布，大量数值实验表明在广泛系统参数范围内有较高准确性。

Conclusion: 该框架有效替代重复昂贵的模拟运行，且易于扩展到其他库存模型，为分析复杂随机系统提供高效快速的替代方案。

Abstract: The continuous-review (s,S) inventory model is a cornerstone of stochastic inventory theory, yet its analysis becomes analytically intractable when dealing with non-Markovian systems. In such systems, evaluating long-run performance measures typically relies on costly simulation.
  This paper proposes a supervised learning framework via a neural network model for approximating stationary performance measures of (s,S) inventory systems with general distributions for the interarrival time between demands and lead times under lost sales. Simulations are first used to generate training labels, after which the neural network is trained. After training, the neural network provides almost instantaneous predictions of various metrics of the system, such as the stationary distribution of inventory levels, the expected cycle time, and the probability of lost sales. We find that using a small number of low-order moments of the distributions as input is sufficient to train the neural networks and to accurately capture the steady-state distribution. Extensive numerical experiments demonstrate high accuracy over a wide range of system parameters. As such, it effectively replaces repeated and costly simulation runs. Our framework is easily extendable to other inventory models, offering an efficient and fast alternative for analyzing complex stochastic systems.

</details>


### [313] [Deep Temporal Graph Clustering: A Comprehensive Benchmark and Datasets](https://arxiv.org/abs/2601.12903)
*Meng Liu,Ke Liang,Siwei Wang,Xingchen Hu,Sihang Zhou,Xinwang Liu*

Main category: cs.LG

TL;DR: 提出BenchTGC基准应对TGC任务挑战，验证其优势及TGC任务重要性。


<details>
  <summary>Details</summary>
Motivation: TGC任务受关注少，存在聚类技术和数据集不适用的挑战，需推动其发展。

Method: 设计BenchTGC Framework改进聚类技术，开发BenchTGC Datasets适合TGC任务。

Result: 实验验证了BenchTGC的优势，展示了TGC任务的必要性和重要性。

Conclusion: 现实世界动态变化和复杂场景是TGC的基础，代码和数据已公开。

Abstract: Temporal Graph Clustering (TGC) is a new task with little attention, focusing on node clustering in temporal graphs. Compared with existing static graph clustering, it can find the balance between time requirement and space requirement (Time-Space Balance) through the interaction sequence-based batch-processing pattern. However, there are two major challenges that hinder the development of TGC, i.e., inapplicable clustering techniques and inapplicable datasets. To address these challenges, we propose a comprehensive benchmark, called BenchTGC. Specially, we design a BenchTGC Framework to illustrate the paradigm of temporal graph clustering and improve existing clustering techniques to fit temporal graphs. In addition, we also discuss problems with public temporal graph datasets and develop multiple datasets suitable for TGC task, called BenchTGC Datasets. According to extensive experiments, we not only verify the advantages of BenchTGC, but also demonstrate the necessity and importance of TGC task. We wish to point out that the dynamically changing and complex scenarios in real world are the foundation of temporal graph clustering. The code and data is available at: https://github.com/MGitHubL/BenchTGC.

</details>


### [314] [An efficient heuristic for geometric analysis of cell deformations](https://arxiv.org/abs/2601.12928)
*Yaima Paz Soto,Silena Herold Garcia,Ximo Gual-Arnau,Antoni Jaume-i-Capó,Manuel González-Hidalgo*

Main category: cs.LG

TL;DR: 本文提出优化方法对血涂片图像中的镰状细胞进行自动化分类，提升效率且降低计算成本，准确率高。


<details>
  <summary>Details</summary>
Motivation: 镰状细胞病全球流行率高，自动化分类镰状细胞可减少专家工作量、避免错误。

Method: 将红细胞建模为形状空间中的闭合平面曲线，采用基于细胞长轴的固定参数化方法计算距离，使用该参数化将每个细胞与两个模板对齐后计算距离。

Result: 在监督分类和无监督聚类中均实现96.03%的准确率。

Conclusion: 该方法能有效进行红细胞分类，保持或提高形状空间模型的准确率，同时显著降低计算成本。

Abstract: Sickle cell disease causes erythrocytes to become sickle-shaped, affecting their movement in the bloodstream and reducing oxygen delivery. It has a high global prevalence and places a significant burden on healthcare systems, especially in resource-limited regions. Automated classification of sickle cells in blood images is crucial, allowing the specialist to reduce the effort required and avoid errors when quantifying the deformed cells and assessing the severity of a crisis. Recent studies have proposed various erythrocyte representation and classification methods. Since classification depends solely on cell shape, a suitable approach models erythrocytes as closed planar curves in shape space. This approach employs elastic distances between shapes, which are invariant under rotations, translations, scaling, and reparameterizations, ensuring consistent distance measurements regardless of the curves' position, starting point, or traversal speed. While previous methods exploiting shape space distances had achieved high accuracy, we refined the model by considering the geometric characteristics of healthy and sickled erythrocytes. Our method proposes (1) to employ a fixed parameterization based on the major axis of each cell to compute distances and (2) to align each cell with two templates using this parameterization before computing distances. Aligning shapes to templates before distance computation, a concept successfully applied in areas such as molecular dynamics, and using a fixed parameterization, instead of minimizing distances across all possible parameterizations, simplifies calculations. This strategy achieves 96.03\% accuracy rate in both supervised classification and unsupervised clustering. Our method ensures efficient erythrocyte classification, maintaining or improving accuracy over shape space models while significantly reducing computational costs.

</details>


### [315] [Deterministic Dynamics of Sampling Processes in Score-Based Diffusion Models with Multiplicative Noise Conditioning](https://arxiv.org/abs/2601.12965)
*Doheon Kim*

Main category: cs.LG

TL;DR: 本文研究基于分数的扩散模型，解释了现有模型结构有局限但实践表现好的现象。


<details>
  <summary>Details</summary>
Motivation: 解释基于分数的扩散模型结构有局限但实践表现好的现象。

Method: 研究相关微分方程的确定性动力学。

Result: 为模型在有结构局限下仍表现良好这一现象提供了理论解释。

Conclusion: 通过研究微分方程动力学能深入了解模型运作。

Abstract: Score-based diffusion models generate new samples by learning the score function associated with a diffusion process. While the effectiveness of these models can be theoretically explained using differential equations related to the sampling process, previous work by Song and Ermon (2020) demonstrated that neural networks using multiplicative noise conditioning can still generate satisfactory samples. In this setup, the model is expressed as the product of two functions: one depending on the spatial variable and the other on the noise magnitude. This structure limits the model's ability to represent a more general relationship between the spatial variable and the noise, indicating that it cannot fully learn the correct score. Despite this limitation, the models perform well in practice. In this work, we provide a theoretical explanation for this phenomenon by studying the deterministic dynamics of the associated differential equations, offering insight into how the model operates.

</details>


### [316] [Architecture-Optimization Co-Design for Physics-Informed Neural Networks Via Attentive Representations and Conflict-Resolved Gradients](https://arxiv.org/abs/2601.12971)
*Pancheng Niu,Jun Guo,Qiaolin He,Yongming Chen,Yanchao Shi*

Main category: cs.LG

TL;DR: 本文从架构 - 优化统一视角研究PINN训练，提出LDA - PINN和GC - PINN，集成得到ACR - PINN，实验表明其比标准PINN收敛快、误差低。


<details>
  <summary>Details</summary>
Motivation: 实际中PINN性能受限于有限表征能力和优化困难，由竞争物理约束和冲突梯度导致。

Method: 提出层动态注意力机制得到LDA - PINN；将PINN训练重构成多任务学习问题，引入冲突解决梯度更新策略得到GC - PINN；集成二者得到ACR - PINN。

Result: 在多个基准PDE问题上，ACR - PINN比标准PINN收敛更快，相对$L_2$和$L_\infty$误差显著降低。

Conclusion: 架构 - 优化协同设计能有效提高基于PINN求解器的鲁棒性和准确性。

Abstract: Physics-Informed Neural Networks (PINNs) provide a learning-based framework for solving partial differential equations (PDEs) by embedding governing physical laws into neural network training. In practice, however, their performance is often hindered by limited representational capacity and optimization difficulties caused by competing physical constraints and conflicting gradients. In this work, we study PINN training from a unified architecture-optimization perspective. We first propose a layer-wise dynamic attention mechanism to enhance representational flexibility, resulting in the Layer-wise Dynamic Attention PINN (LDA-PINN). We then reformulate PINN training as a multi-task learning problem and introduce a conflict-resolved gradient update strategy to alleviate gradient interference, leading to the Gradient-Conflict-Resolved PINN (GC-PINN). By integrating these two components, we develop the Architecture-Conflict-Resolved PINN (ACR-PINN), which combines attentive representations with conflict-aware optimization while preserving the standard PINN loss formulation. Extensive experiments on benchmark PDEs, including the Burgers, Helmholtz, Klein-Gordon, and lid-driven cavity flow problems, demonstrate that ACR-PINN achieves faster convergence and significantly lower relative $L_2$ and $L_\infty$ errors than standard PINNs. These results highlight the effectiveness of architecture-optimization co-design for improving the robustness and accuracy of PINN-based solvers.

</details>


### [317] [PaperGuide: Making Small Language-Model Paper-Reading Agents More Efficient](https://arxiv.org/abs/2601.12988)
*Zijian Wang,Tiancheng Huang,Hanqi Li,Da Ma,Lu Chen,Kai Yu*

Main category: cs.LG

TL;DR: 为解决手动阅读难追踪科研文献进展问题，受认知科学启发提出PaperCompass框架及DFPO训练方法，能在不牺牲性能下提升效率。


<details>
  <summary>Details</summary>
Motivation: 科学文献增长快，手动阅读难追踪新进展，现有基于大语言模型的方法多存在过度且低效探索问题。

Method: 提出PaperCompass框架，分离高级规划与细粒度执行；引入DFPO强化学习方法联合优化草稿计划和最终解决方案。

Result: 理论分析表明DFPO有良好优化特性，在Paper - QA基准实验中，PaperCompass不牺牲性能前提下提升效率，结果与更大模型相当。

Conclusion: PaperCompass框架和DFPO方法可解决现有方法存在的问题，稳定可靠且高效。

Abstract: The accelerating growth of the scientific literature makes it increasingly difficult for researchers to track new advances through manual reading alone. Recent progress in large language models (LLMs) has therefore spurred interest in autonomous agents that can read scientific papers and extract task-relevant information. However, most existing approaches rely either on heavily engineered prompting or on a conventional SFT-RL training pipeline, both of which often lead to excessive and low-yield exploration. Drawing inspiration from cognitive science, we propose PaperCompass, a framework that mitigates these issues by separating high-level planning from fine-grained execution. PaperCompass first drafts an explicit plan that outlines the intended sequence of actions, and then performs detailed reasoning to instantiate each step by selecting the parameters for the corresponding function calls. To train such behavior, we introduce Draft-and-Follow Policy Optimization (DFPO), a tailored RL method that jointly optimizes both the draft plan and the final solution. DFPO can be viewed as a lightweight form of hierarchical reinforcement learning, aimed at narrowing the `knowing-doing' gap in LLMs. We provide a theoretical analysis that establishes DFPO's favorable optimization properties, supporting a stable and reliable training process. Experiments on paper-based question answering (Paper-QA) benchmarks show that PaperCompass improves efficiency over strong baselines without sacrificing performance, achieving results comparable to much larger models.

</details>


### [318] [HT-GNN: Hyper-Temporal Graph Neural Network for Customer Lifetime Value Prediction in Baidu Ads](https://arxiv.org/abs/2601.13013)
*Xiaohui Zhao,Xinjian Zhao,Jiahui Zhang,Guoyu Liu,Houzhi Wang,Shu Wu*

Main category: cs.LG

TL;DR: 提出HT - GNN解决新闻推送广告LTV预测挑战，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: LTV预测对新闻推送广告至关重要，但面临用户群体LTV分布差异大与行为序列动态变化两大挑战。

Method: 提出HT - GNN，含超图监督模块、自适应加权的基于Transformer的时间编码器、带动态预测塔的任务自适应专家混合体。

Result: 在含1500万用户的百度广告数据集实验中，HT - GNN在所有指标和预测范围内均优于现有方法。

Conclusion: HT - GNN能有效解决LTV预测面临的挑战，有较好性能。

Abstract: Lifetime value (LTV) prediction is crucial for news feed advertising, enabling platforms to optimize bidding and budget allocation for long-term revenue growth. However, it faces two major challenges: (1) demographic-based targeting creates segment-specific LTV distributions with large value variations across user groups; and (2) dynamic marketing strategies generate irregular behavioral sequences where engagement patterns evolve rapidly. We propose a Hyper-Temporal Graph Neural Network (HT-GNN), which jointly models demographic heterogeneity and temporal dynamics through three key components: (i) a hypergraph-supervised module capturing inter-segment relationships; (ii) a transformer-based temporal encoder with adaptive weighting; and (iii) a task-adaptive mixture-of-experts with dynamic prediction towers for multi-horizon LTV forecasting. Experiments on \textit{Baidu Ads} with 15 million users demonstrate that HT-GNN consistently outperforms state-of-the-art methods across all metrics and prediction horizons.

</details>


### [319] [PASs-MoE: Mitigating Misaligned Co-drift among Router and Experts via Pathway Activation Subspaces for Continual Learning](https://arxiv.org/abs/2601.13020)
*Zhiyan Hou,Haiyun Guo,Haokai Ma,Yandu Sun,Yonghui Yang,Jinqiao Wang*

Main category: cs.LG

TL;DR: 本文针对持续指令微调中存在的Misaligned Co - drift现象，引入PASs并提出基于其的MoE - LoRA方法，实验表明该方法在准确率和抗遗忘方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有基于LoRA的Mixture - of - Experts（MoE）方法在更新路由器和专家时存在不分青红皂白的联合更新，导致Misaligned Co - drift现象，加剧模型遗忘，影响模型性能。

Method: 引入反映输入在每个专家中激活的低秩路径方向的PASs，提出基于PASs的固定容量MoE - LoRA方法，包括PAS引导的重新加权和PAS感知的秩稳定两个组件。

Result: 在持续指令微调基准测试中，该方法在准确率和抗遗忘方面始终优于一系列传统持续学习基线和MoE - LoRA变体，且不增加参数。

Conclusion: 所提出的基于PASs的MoE - LoRA方法是有效的，能解决Misaligned Co - drift问题，提升持续指令微调中多模态大语言模型的性能。

Abstract: Continual instruction tuning (CIT) requires multimodal large language models (MLLMs) to adapt to a stream of tasks without forgetting prior capabilities. A common strategy is to isolate updates by routing inputs to different LoRA experts. However, existing LoRA-based Mixture-of-Experts (MoE) methods often jointly update the router and experts in an indiscriminate way, causing the router's preferences to co-drift with experts' adaptation pathways and gradually deviate from early-stage input-expert specialization. We term this phenomenon Misaligned Co-drift, which blurs expert responsibilities and exacerbates forgetting.To address this, we introduce the pathway activation subspace (PASs), a LoRA-induced subspace that reflects which low-rank pathway directions an input activates in each expert, providing a capability-aligned coordinate system for routing and preservation. Based on PASs, we propose a fixed-capacity PASs-based MoE-LoRA method with two components: PAS-guided Reweighting, which calibrates routing using each expert's pathway activation signals, and PAS-aware Rank Stabilization, which selectively stabilizes rank directions important to previous tasks. Experiments on a CIT benchmark show that our approach consistently outperforms a range of conventional continual learning baselines and MoE-LoRA variants in both accuracy and anti-forgetting without adding parameters. Our code will be released upon acceptance.

</details>


### [320] [Enhancing Generalization in Sickle Cell Disease Diagnosis through Ensemble Methods and Feature Importance Analysis](https://arxiv.org/abs/2601.13021)
*Nataša Petrović,Gabriel Moyà-Alcover,Antoni Jaume-i-Capó,Jose Maria Buades Rubio*

Main category: cs.LG

TL;DR: 提出基于集成学习的方法，用红细胞外周血涂片图像为镰状细胞病诊断提供支持，模型泛化能力超现有技术，指标有提升并公开相关数据。


<details>
  <summary>Details</summary>
Motivation: 基于现有技术，为镰状细胞病利用红细胞外周血涂片图像提供诊断支持，实现模型泛化。

Method: 预处理并分割显微图像，提取血细胞特征，采用集成机器学习方法分类细胞形态，设计方法识别关键特征。

Result: 随机森林和Extra Trees分类器集成的模型F1分数达90.71%，SDS分数达93.33%，优于梯度提升分类器。

Conclusion: 提出的方法在泛化能力上优于现有模型，能提升诊断效果，公开相关数据利于科学进步。

Abstract: This work presents a novel approach for selecting the optimal ensemble-based classification method and features with a primarly focus on achieving generalization, based on the state-of-the-art, to provide diagnostic support for Sickle Cell Disease using peripheral blood smear images of red blood cells. We pre-processed and segmented the microscopic images to ensure the extraction of high-quality features. To ensure the reliability of our proposed system, we conducted an in-depth analysis of interpretability. Leveraging techniques established in the literature, we extracted features from blood cells and employed ensemble machine learning methods to classify their morphology. Furthermore, we have devised a methodology to identify the most critical features for classification, aimed at reducing complexity and training time and enhancing interpretability in opaque models. Lastly, we validated our results using a new dataset, where our model overperformed state-of-the-art models in terms of generalization. The results of classifier ensembled of Random Forest and Extra Trees classifier achieved an harmonic mean of precision and recall (F1-score) of 90.71\% and a Sickle Cell Disease diagnosis support score (SDS-score) of 93.33\%. These results demonstrate notable enhancement from previous ones with Gradient Boosting classifier (F1-score 87.32\% and SDS-score 89.51\%). To foster scientific progress, we have made available the parameters for each model, the implemented code library, and the confusion matrices with the raw data.

</details>


### [321] [Analysis of Long Range Dependency Understanding in State Space Models](https://arxiv.org/abs/2601.13048)
*Srividya Ravikumar,Abhinav Anand,Shweta Verma,Mira Mezini*

Main category: cs.LG

TL;DR: 对在真实任务（源代码漏洞检测）中训练的对角化状态空间模型（S4D）进行系统的核可解释性研究，分析其长程建模能力及对性能的影响，为设计更好的基于S4D的模型提供指导。


<details>
  <summary>Details</summary>
Motivation: 现有状态空间模型研究多注重预测准确性而非可解释性，开展对S4D在真实任务中的核可解释性研究。

Method: 通过对S4D核的时域和频域分析。

Result: S4D的长程建模能力在不同模型架构下有显著差异，影响模型性能，S4D核可表现为低通、带通或高通滤波器。

Conclusion: 研究结果可为未来设计更好的基于S4D的模型提供指导。

Abstract: Although state-space models (SSMs) have demonstrated strong performance on long-sequence benchmarks, most research has emphasized predictive accuracy rather than interpretability. In this work, we present the first systematic kernel interpretability study of the diagonalized state-space model (S4D) trained on a real-world task (vulnerability detection in source code). Through time and frequency domain analysis of the S4D kernel, we show that the long-range modeling capability of S4D varies significantly under different model architectures, affecting model performance. For instance, we show that the depending on the architecture, S4D kernel can behave as low-pass, band-pass or high-pass filter. The insights from our analysis can guide future work in designing better S4D-based models.

</details>


### [322] [TinyML-Enabled IoT for Sustainable Precision Irrigation](https://arxiv.org/abs/2601.13054)
*Kamogelo Taueatsoala,Caitlyn Daniels,Angelina J. Ramsunar,Petrus Bronkhorst,Absalom E. Ezugwu*

Main category: cs.LG

TL;DR: 文章提出边缘优先的物联网框架结合TinyML用于精准灌溉，提升了小农耕作社区用水效率，适合资源受限地区。


<details>
  <summary>Details</summary>
Motivation: 小农耕作社区面临水资源短缺、气候不稳定和缺乏先进农业技术等问题，需要解决方案。

Method: 提出四层架构，利用低成本硬件和特定微控制器，用多种传感器监测环境；对比集成模型选梯度提升模型并转化为TinyML推理引擎部署；采用基于MQTT的局域网协议通信。

Result: 梯度提升模型优于随机森林模型；ESP32预测灌溉需求准确率高；实验显示相比传统方法减少了用水。

Conclusion: 该框架是实用、经济的方案，可弥合农业技术差距，提高水资源利用效率，适合资源受限地区部署。

Abstract: Small-scale farming communities are disproportionately affected by water scarcity, erratic climate patterns, and a lack of access to advanced, affordable agricultural technologies. To address these challenges, this paper presents a novel, edge-first IoT framework that integrates Tiny Machine Learning (TinyML) for intelligent, offline-capable precision irrigation. The proposed four-layer architecture leverages low-cost hardware, an ESP32 microcontroller as an edge inference node, and a Raspberry Pi as a local edge server to enable autonomous decision-making without cloud dependency. The system utilizes capacitive soil moisture, temperature, humidity, pH, and ambient light sensors for environmental monitoring. A rigorous comparative analysis of ensemble models identified gradient boosting as superior, achieving an R^2 score of 0.9973 and a Mean Absolute Percentage Error (MAPE) of 0.99%, outperforming a random forest model (R^2 = 0.9916, MAPE = 1.81%). This optimized model was converted and deployed as a lightweight TinyML inference engine on the ESP32 and predicts irrigation needs with exceptional accuracy (MAPE < 1%). Local communication is facilitated by an MQTT-based LAN protocol, ensuring reliable operation in areas with limited or no internet connectivity. Experimental validation in a controlled environment demonstrated a significant reduction in water usage compared to traditional methods, while the system's low-power design and offline functionality confirm its viability for sustainable, scalable deployment in resource-constrained rural settings. This work provides a practical, cost-effective blueprint for bridging the technological divide in agriculture and enhancing water-use efficiency through on-device artificial intelligence.

</details>


### [323] [METIS: Mentoring Engine for Thoughtful Inquiry & Solutions](https://arxiv.org/abs/2601.13075)
*Abhinav Rajeev Kumar,Dhruv Trehan,Paras Chopra*

Main category: cs.LG

TL;DR: 测试AI导师能否助力大学生写论文，构建METIS工具并与GPT - 5、Claude Sonnet 4.5对比，METIS表现较好但存在不足。


<details>
  <summary>Details</summary>
Motivation: 许多学生缺乏专业研究指导，探索AI导师能否帮助本科生将想法转化为论文。

Method: 构建具备文献搜索等功能的METIS工具，通过多种评估方法，与GPT - 5和Claude Sonnet 4.5在六个写作阶段进行对比。

Result: 在单轮提示中，LLM法官更倾向METIS；学生评分各阶段METIS更高；多轮会话中，METIS最终质量略高于GPT - 5；优势集中在文档相关阶段。

Conclusion: METIS有一定优势，但存在过早工具路由、浅层关联和偶尔阶段分类错误等问题。

Abstract: Many students lack access to expert research mentorship. We ask whether an AI mentor can move undergraduates from an idea to a paper. We build METIS, a tool-augmented, stage-aware assistant with literature search, curated guidelines, methodology checks, and memory. We evaluate METIS against GPT-5 and Claude Sonnet 4.5 across six writing stages using LLM-as-a-judge pairwise preferences, student-persona rubrics, short multi-turn tutoring, and evidence/compliance checks. On 90 single-turn prompts, LLM judges preferred METIS to Claude Sonnet 4.5 in 71% and to GPT-5 in 54%. Student scores (clarity/actionability/constraint-fit; 90 prompts x 3 judges) are higher across stages. In multi-turn sessions (five scenarios/agent), METIS yields slightly higher final quality than GPT-5. Gains concentrate in document-grounded stages (D-F), consistent with stage-aware routing and groundings failure modes include premature tool routing, shallow grounding, and occasional stage misclassification.

</details>


### [324] [Recursive Meta-Distillation: An Axiomatic Framework for Iterative Knowledge Refinement](https://arxiv.org/abs/2601.13100)
*Aaron R. Flouro,Shawn P. Chadwick*

Main category: cs.LG

TL;DR: 提出递归元蒸馏的公理和算子理论框架，分析其数学特性及对迭代和多教师蒸馏的理论意义。


<details>
  <summary>Details</summary>
Motivation: 现有工作对递归或多代蒸馏的数学行为理解不足，此前方法多依赖经验启发式。

Method: 引入公理和算子理论框架，定义有效元教师构建的结构公理，在温和假设下分析锚定递归蒸馏特性。

Result: 锚定递归蒸馏会使KL散度收缩，几何收敛到基础教师分布，有唯一全局吸引的不动点。

Conclusion: 该框架从基础层面表征递归蒸馏何时数学上适定且收敛，为理解蒸馏稳定性等提供理论基础。

Abstract: Recent work in probability-domain knowledge distillation has established axiomatic frameworks for temperature scaling, multi-teacher aggregation, and bias-variance trade-offs in single-stage settings. However, the mathematical behavior of recursive or multi-generation distillation remains poorly understood, with prior approaches relying primarily on empirical heuristics. In this work, we introduce an axiomatic and operator-theoretic framework for recursive meta-distillation, formalizing iterative knowledge distillation as a sequence of probability-distribution operators with explicit anchoring to base teachers.
  We define structural axioms for valid meta-teacher construction and prove the existence of non-trivial operator families satisfying these axioms without specifying particular algorithms or loss functions. Under mild realizability and convexity assumptions, we show that anchored recursive distillation induces contraction in KL divergence, yielding geometric convergence to base teacher distributions and a unique, globally attractive fixed point.
  The contribution is foundational rather than algorithmic: the framework characterizes when recursive distillation is mathematically well-posed and convergent rather than error-accumulating, independent of model architecture, optimization details, or specific operator instantiations. These results provide a theoretical basis for understanding stability, bias-variance behavior, and failure modes in iterative and multi-teacher distillation under capacity constraints.

</details>


### [325] [FastAV: Efficient Token Pruning for Audio-Visual Large Language Model Inference](https://arxiv.org/abs/2601.13143)
*Chaeyoung Jung,Youngjoon Jang,Seungwoo Lee,Joon Son Chung*

Main category: cs.LG

TL;DR: 提出首个适用于视听大语言模型的令牌剪枝框架FastAV，减少FLOPs并保持或提升性能。


<details>
  <summary>Details</summary>
Motivation: 令牌剪枝在标准大语言模型和视觉语言模型中被积极探索，但在视听大语言模型中的应用受关注少，而多模态集成增加了令牌需求。

Method: 引入利用注意力权重识别不同阶段重要令牌的剪枝策略，采用两阶段剪枝策略，且与高效注意力机制兼容。

Result: 在两个代表性AV - LLMs上，FastAV减少超过40%的FLOPs，同时保持或提升模型性能。

Conclusion: FastAV框架对视听大语言模型的令牌剪枝有效，能在减少计算量同时保障模型效果。

Abstract: In this work, we present FastAV, the first token pruning framework tailored for audio-visual large language models (AV-LLMs). While token pruning has been actively explored in standard large language models (LLMs) and vision-language models (LVLMs), its application to AV-LLMs has received little attention, even though multimodal integration substantially increases their token demands. To address this gap, we introduce a pruning strategy that utilizes attention weights to identify tokens emphasized at different stages and estimates their importance. Building on this analysis, FastAV applies a two-stage pruning strategy: (1) global pruning in intermediate layers to remove broadly less influential tokens, and (2) fine pruning in later layers considering the impact on next token generation. Notably, our method does not rely on full attention maps, which makes it fully compatible with efficient attention mechanisms such as FlashAttention. Extensive experiments demonstrate that FastAV reduces FLOPs by more than 40% on two representative AV-LLMs, while preserving or even improving model performance.

</details>


### [326] [Training instability in deep learning follows low-dimensional dynamical principles](https://arxiv.org/abs/2601.13160)
*Zhipeng Zhang,Zhenjie Yao,Kai Li,Lei Yang*

Main category: cs.LG

TL;DR: 本文提出统一动力学视角研究深度学习训练稳定性，通过控制扰动审计轨迹，发现三点规律，为研究学习动态提供基础。


<details>
  <summary>Details</summary>
Motivation: 深度学习训练过程稳定性缺乏理解，小扰动会导致训练崩溃，影响可重复性和可扩展性。

Method: 提出统一动力学视角，将训练稳定性分为四个维度，通过控制扰动审计训练轨迹。

Result: 在强化学习和大语言模型训练中发现三点规律：高最终性能常与训练稳定性解耦；可控随机性可缓冲学习动态；低维潜在元状态偏差先于性能崩溃。

Conclusion: 训练稳定性是学习系统可测量和比较的动力学属性，为研究学习动态提供描述基础。

Abstract: Deep learning systems achieve remarkable empirical performance, yet the stability of the training process itself remains poorly understood. Training unfolds as a high-dimensional dynamical system in which small perturbations to optimization, data, parameters, or learning signals can induce abrupt and irreversible collapse, undermining reproducibility and scalability.
  We propose a unified dynamical perspective that characterizes training stability as an intrinsic property of learning systems, organized along four interacting dimensions: optimization, environmental/data, parametric, and learning-signal stability. We operationalize this perspective through controlled perturbation auditing of training trajectories, probing how learning dynamics respond to structured disturbances without modifying learning algorithms.
  Across reinforcement learning and large language model training, we identify three recurring regularities: high final performance is frequently decoupled from training stability; controlled stochasticity consistently buffers learning dynamics across paradigms; and deviations in low-dimensional latent meta-states systematically precede observable performance collapse. Together, these findings establish training stability as a measurable and comparable dynamical property of learning systems, providing a descriptive foundation for studying learning dynamics beyond final performance outcomes.

</details>


### [327] [NeuroShield: A Neuro-Symbolic Framework for Adversarial Robustness](https://arxiv.org/abs/2601.13162)
*Ali Shafiee Sarvestani,Jason Schmidt,Arman Roohi*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Adversarial vulnerability and lack of interpretability are critical limitations of deep neural networks, especially in safety-sensitive settings such as autonomous driving. We introduce \DesignII, a neuro-symbolic framework that integrates symbolic rule supervision into neural networks to enhance both adversarial robustness and explainability. Domain knowledge is encoded as logical constraints over appearance attributes such as shape and color, and enforced through semantic and symbolic logic losses applied during training. Using the GTSRB dataset, we evaluate robustness against FGSM and PGD attacks at a standard $\ell_\infty$ perturbation budget of $\varepsilon = 8/255$. Relative to clean training, standard adversarial training provides modest improvements in robustness ($\sim$10 percentage points). Conversely, our FGSM-Neuro-Symbolic and PGD-Neuro-Symbolic models achieve substantially larger gains, improving adversarial accuracy by 18.1\% and 17.35\% over their corresponding adversarial-training baselines, representing roughly a three-fold larger robustness gain than standard adversarial training provides when both are measured relative to the same clean-training baseline, without reducing clean-sample accuracy. Compared to transformer-based defenses such as LNL-MoEx, which require heavy architectures and extensive data augmentation, our PGD-Neuro-Symbolic variant attains comparable or superior robustness using a ResNet18 backbone trained for 10 epochs. These results show that symbolic reasoning offers an effective path to robust and interpretable AI.

</details>


### [328] [LAViG-FLOW: Latent Autoregressive Video Generation for Fluid Flow Simulations](https://arxiv.org/abs/2601.13190)
*Vittoria De Pellegrini,Tariq Alkhalifah*

Main category: cs.LG

TL;DR: 提出LAViG - FLOW框架模拟和预测地下多相流场，比传统方法快且结果时间上一致。


<details>
  <summary>Details</summary>
Motivation: 高保真多相模拟器在多次前向运行进行反演和不确定性量化时成本过高，需开发新方法。

Method: 提出LAViG - FLOW框架，用2D自动编码器压缩状态变量，用视频扩散变压器跨时间对它们的耦合分布建模，先训练再自回归微调。

Result: 在开源二氧化碳封存数据集上评估，LAViG - FLOW生成的饱和度和压力场在时间上保持一致，且运行速度比传统数值求解器快几个数量级。

Conclusion: LAViG - FLOW框架能有效解决现有方法成本高的问题，在模拟和预测地下多相流场方面具有优势。

Abstract: Modeling and forecasting subsurface multiphase fluid flow fields underpin applications ranging from geological CO2 sequestration (GCS) operations to geothermal production. This is essential for ensuring both operational performance and long-term safety. While high fidelity multiphase simulators are widely used for this purpose, they become prohibitively expensive once many forward runs are required for inversion purposes and quantify uncertainty. To tackle this challenge we propose LAViG-FLOW, a latent autoregressive video generation diffusion framework that explicitly learns the coupled evolution of saturation and pressure fields. Each state variable is compressed by a dedicated 2D autoencoder, and a Video Diffusion Transformer (VDiT) models their coupled distribution across time. We first train the model on a given time horizon to learn their coupled relationship and then fine-tune it autoregressively so it can extrapolate beyond the observed time window. Evaluated on an open-source CO2 sequestration dataset, LAViG-FLOW generates saturation and pressure fields that stay consistent across time while running orders of magnitude faster than traditional numerical solvers.

</details>


### [329] [A Comprehensive Evaluation of LLM Reasoning: From Single-Model to Multi-Agent Paradigms](https://arxiv.org/abs/2601.13243)
*Yapeng Li,Jiakuo Yu,Zhixin Liu,Xinnan Liu,Jing Yu,Songze Li,Tonghua Su*

Main category: cs.LG

TL;DR: 对推理范式进行综合统一评估，引入新基准MIMeBench，结果表明结构复杂性增加不一定提升推理性能，并公开代码。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型作为推理系统时，不同推理范式的相对有效性和成本 - 准确率权衡情况不明。

Method: 对多种推理范式进行全面统一评估，包括直接单模型生成、CoT增强单模型推理和代表性MAS工作流；进行角色隔离分析；分析成本 - 准确率权衡；引入新基准MIMeBench。

Result: 结构复杂性增加并不总是能提高推理性能，其效果高度依赖推理范式本身的属性和适用性。

Conclusion: 不同推理范式在推理性能、成本 - 准确率权衡上有差异，新基准MIMeBench可进行细粒度语义能力评估。

Abstract: Large Language Models (LLMs) are increasingly deployed as reasoning systems, where reasoning paradigms - such as Chain-of-Thought (CoT) and multi-agent systems (MAS) - play a critical role, yet their relative effectiveness and cost-accuracy trade-offs remain poorly understood. In this work, we conduct a comprehensive and unified evaluation of reasoning paradigms, spanning direct single-model generation, CoT-augmented single-model reasoning, and representative MAS workflows, characterizing their reasoning performance across a diverse suite of closed-form benchmarks. Beyond overall performance, we probe role-specific capability demands in MAS using targeted role isolation analyses, and analyze cost-accuracy trade-offs to identify which MAS workflows offer a favorable balance between cost and accuracy, and which incur prohibitive overhead for marginal gains. We further introduce MIMeBench, a new open-ended benchmark that targets two foundational yet underexplored semantic capabilities - semantic abstraction and contrastive discrimination - thereby providing an alternative evaluation axis beyond closed-form accuracy and enabling fine-grained assessment of semantic competence that is difficult to capture with existing benchmarks. Our results show that increased structural complexity does not consistently lead to improved reasoning performance, with its benefits being highly dependent on the properties and suitability of the reasoning paradigm itself. The codes are released at https://gitcode.com/HIT1920/OpenLLMBench.

</details>


### [330] [Do Instruction-Tuned Models Always Perform Better Than Base Models? Evidence from Math and Domain-Shifted Benchmarks](https://arxiv.org/abs/2601.13244)
*Prateek Munjal,Clement Christophe,Ronnie Rajan,Praveenkumar Kanithi*

Main category: cs.LG

TL;DR: 研究指令微调是否提升推理能力，发现其有不稳定和对分布变化脆弱的局限。


<details>
  <summary>Details</summary>
Motivation: 探究指令微调是增强推理能力还是仅诱导表面模式匹配。

Method: 在标准数学基准、结构扰动变体和领域转移任务上评估基础模型和指令微调模型。

Result: 一是性能优势不稳定，零样本设置中基础模型常优于微调模型，微调模型依赖特定提示模式；二是微调收益在分布转移下脆弱，基础模型在特定领域基准上表现更好，微调模型在扰动数据集上性能下降。

Conclusion: 指令微调存在性能不稳定和对分布变化敏感的问题，可能未真正提升推理能力。

Abstract: Instruction finetuning is standard practice for improving LLM performance, yet it remains unclear whether it enhances reasoning or merely induces surface-level pattern matching. We investigate this by evaluating base and instruction-tuned models on standard math benchmarks, structurally perturbed variants, and domain-shifted tasks. Our analysis highlights two key (often overlooked) limitations of instruction tuning. First, the performance advantage is unstable and depends heavily on evaluation settings. In zero-shot CoT settings on GSM8K, base models consistently outperform instruction-tuned variants, with drops as high as 32.67\% (Llama3-70B). Instruction-tuned models only match or exceed this performance when provided with few-shot exemplars, suggesting a reliance on specific prompting patterns rather than intrinsic reasoning. Second, tuning gains are brittle under distribution shift. Our results show that base models surpass instruction-tuned variants on the domain-specific MedCalc benchmark. Additionally, instruction-tuned models show sharp declines on perturbed datasets, indicating sensitivity to prompt structure over robust reasoning.

</details>


### [331] [Balancing Classification and Calibration Performance in Decision-Making LLMs via Calibration Aware Reinforcement Learning](https://arxiv.org/abs/2601.13284)
*Duygu Nur Yaldiz,Evangelia Spiliopoulou,Zheng Qi,Siddharth Varia,Srikanth Doss,Nikolaos Pappas*

Main category: cs.LG

TL;DR: 研究两种微调范式校准，指出RLVR过自信、SFT校准好，提出校准感知强化学习方法降低过自信。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于决策任务时不仅需要准确性，还需可靠置信估计，需研究校准情况。

Method: 对SFT和RLVR进行系统研究，诊断RLVR失败原因，提出校准感知强化学习方法调整决策标记概率。

Result: RLVR提高任务性能但产生过自信模型，SFT校准更好但性能提升小，提出的方法保持准确度同时降低过自信，ECE分数最多降9分。

Conclusion: 校准感知强化学习方法能在保持大语言模型准确度的同时缓解过自信问题。

Abstract: Large language models (LLMs) are increasingly deployed in decision-making tasks, where not only accuracy but also reliable confidence estimates are essential. Well-calibrated confidence enables downstream systems to decide when to trust a model and when to defer to fallback mechanisms. In this work, we conduct a systematic study of calibration in two widely used fine-tuning paradigms: supervised fine-tuning (SFT) and reinforcement learning with verifiable rewards (RLVR). We show that while RLVR improves task performance, it produces extremely overconfident models, whereas SFT yields substantially better calibration, even under distribution shift, though with smaller performance gains. Through targeted experiments, we diagnose RLVR's failure, showing that decision tokens act as extraction steps of the decision in reasoning traces and do not carry confidence information, which prevents reinforcement learning from surfacing calibrated alternatives. Based on this insight, we propose a calibration-aware reinforcement learning formulation that directly adjusts decision-token probabilities. Our method preserves RLVR's accuracy level while mitigating overconfidence, reducing ECE scores up to 9 points.

</details>


### [332] [CooperBench: Why Coding Agents Cannot be Your Teammates Yet](https://arxiv.org/abs/2601.13295)
*Arpandeep Khatua,Hao Zhu,Peter Tran,Arya Prabhudesai,Frederic Sadrieh,Johann K. Lieberwirth,Xinkai Yu,Yicheng Fu,Michael J. Ryan,Jiaxin Pei,Diyi Yang*

Main category: cs.LG

TL;DR: 引入CooperBench基准测试协作编码的AI代理，发现当前代理存在协作问题，呼吁发展社交智能。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理参与复杂工作，需具备协调能力，但当前代理可能缺乏，故进行测试。

Method: 引入CooperBench基准，包含600多个跨多种语言和库的协作编码任务，基于真实开源仓库和专家编写的测试。

Result: 最先进的编码代理协作时成功率比单独工作低30%，存在沟通及行为问题，也有罕见的涌现协调行为。

Conclusion: 提出新的协作编码基准，呼吁从追求个体能力转向发展社交智能。

Abstract: Resolving team conflicts requires not only task-specific competence, but also social intelligence to find common ground and build consensus. As AI agents increasingly collaborate on complex work, they must develop coordination capabilities to function as effective teammates. Yet we hypothesize that current agents lack these capabilities. To test this, we introduce CooperBench, a benchmark of over 600 collaborative coding tasks across 12 libraries in 4 programming languages. Each task assigns two agents different features that can be implemented independently but may conflict without proper coordination. Tasks are grounded in real open-source repositories with expert-written tests. Evaluating state-of-the-art coding agents, we observe the curse of coordination: agents achieve on average 30% lower success rates when working together compared to performing both tasks individually. This contrasts sharply with human teams, where adding teammates typically improves productivity. Our analysis reveals three key issues: (1) communication channels become jammed with vague, ill-timed, and inaccurate messages; (2) even with effective communication, agents deviate from their commitments; and (3) agents often hold incorrect expectations about others' plans and communication. Through large-scale simulation, we also observe rare but interesting emergent coordination behavior including role division, resource division, and negotiation. Our research presents a novel benchmark for collaborative coding and calls for a shift from pursuing individual agent capability to developing social intelligence.

</details>


### [333] [Verifying Local Robustness of Pruned Safety-Critical Networks](https://arxiv.org/abs/2601.13303)
*Minh Le,Phuong Cao*

Main category: cs.LG

TL;DR: 本文研究不同剪枝率对深度神经网络形式化局部鲁棒性证书的影响，发现不同数据集的最优剪枝率不同，为高风险环境中部署高效且经过形式验证的DNN提供见解。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络形式验证在安全关键应用中很重要，但验证大规模模型的计算成本高，阻碍其应用，因此研究剪枝对形式验证的影响。

Method: 使用最先进的$α,β$-CROWN验证器，在MNIST和NASA JPL火星霜冻识别数据集上对不同剪枝率的ResNet4模型进行评估。

Result: MNIST上轻度剪枝（40%）和JPL上重度剪枝（70%-90%）可提高可验证性，在已证明的$L_∞$鲁棒性属性上优于未剪枝的基线模型。

Conclusion: 模型压缩具有复杂性，不同数据集的最优剪枝率差异显著，为高风险环境中部署DNN提供了选择最优剪枝率的关键见解。

Abstract: Formal verification of Deep Neural Networks (DNNs) is essential for safety-critical applications, ranging from surgical robotics to NASA JPL autonomous systems. However, the computational cost of verifying large-scale models remains a significant barrier to adoption. This paper investigates the impact of pruning on formal local robustness certificates with different ratios. Using the state-of-the-art $α,β$-CROWN verifier, we evaluate ResNet4 models across varying pruning ratios on MNIST and, more importantly, on the NASA JPL Mars Frost Identification datasets. Our findings demonstrate a non-linear relationship: light pruning (40%) in MNIST and heavy pruning (70%-90%) in JPL improve verifiability, allowing models to outperform unpruned baselines in proven $L_\infty$ robustness properties. This suggests that reduced connectivity simplifies the search space for formal solvers and that the optimal pruning ratio varies significantly between datasets. This research highlights the complex nature of model compression, offering critical insights into selecting the optimal pruning ratio for deploying efficient, yet formally verified, DNNs in high-stakes environments where reliability is non-negotiable.

</details>


### [334] [Beyond Mapping : Domain-Invariant Representations via Spectral Embedding of Optimal Transport Plans](https://arxiv.org/abs/2601.13350)
*Abdel Djalil Sad Saoud,Fred Maurice Ngolè Mboula,Hanane Slimani*

Main category: cs.LG

TL;DR: 提出将平滑传输计划解释为二分图邻接矩阵，通过谱嵌入推导域不变样本表示，在多个任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 训练和推理时数据的分布偏移是机器学习的挑战，现有基于最优传输的无监督域适应方法有局限性。

Method: 将平滑传输计划解释为连接源域和目标域的二分图邻接矩阵，通过谱嵌入推导域不变样本表示。

Result: 在音乐类型识别、音乐 - 语音区分、电缆缺陷检测和分类等任务中实现了整体强劲的性能。

Conclusion: 所提方法能有效应对数据分布偏移问题，在多个任务中表现良好。

Abstract: Distributional shifts between training and inference time data remain a central challenge in machine learning, often leading to poor performance. It motivated the study of principled approaches for domain alignment, such as optimal transport based unsupervised domain adaptation, that relies on approximating Monge map using transport plans, which is sensitive to the transport problem regularization strategy and hyperparameters, and might yield biased domains alignment. In this work, we propose to interpret smoothed transport plans as adjacency matrices of bipartite graphs connecting source to target domain and derive domain-invariant samples' representations through spectral embedding. We evaluate our approach on acoustic adaptation benchmarks for music genre recognition, music-speech discrimination, as well as electrical cable defect detection and classification tasks using time domain reflection in different diagnosis settings, achieving overall strong performances.

</details>


### [335] [On the Relation of State Space Models and Hidden Markov Models](https://arxiv.org/abs/2601.13357)
*Aydin Ghojogh,M. Hadi Sepanj,Benyamin Ghojogh*

Main category: cs.LG

TL;DR: 本文对HMM、线性高斯状态空间模型、卡尔曼滤波和当代NLP状态空间模型进行统一系统比较，分析异同及关联。


<details>
  <summary>Details</summary>
Motivation: 经典概率SSM、HMM与现代神经序列模型关系因确定性状态空间模型在NLP中重新出现而需探究。

Method: 从概率图模型角度分析模型公式，研究推理算法，对比学习过程。

Result: 明确不同模型何时等效、何时不同，以及现代NLP SSM与经典概率模型的关系。

Conclusion: 分析连接了控制理论、概率建模和现代深度学习的观点。

Abstract: State Space Models (SSMs) and Hidden Markov Models (HMMs) are foundational frameworks for modeling sequential data with latent variables and are widely used in signal processing, control theory, and machine learning. Despite their shared temporal structure, they differ fundamentally in the nature of their latent states, probabilistic assumptions, inference procedures, and training paradigms. Recently, deterministic state space models have re-emerged in natural language processing through architectures such as S4 and Mamba, raising new questions about the relationship between classical probabilistic SSMs, HMMs, and modern neural sequence models.
  In this paper, we present a unified and systematic comparison of HMMs, linear Gaussian state space models, Kalman filtering, and contemporary NLP state space models. We analyze their formulations through the lens of probabilistic graphical models, examine their inference algorithms -- including forward-backward inference and Kalman filtering -- and contrast their learning procedures via Expectation-Maximization and gradient-based optimization. By highlighting both structural similarities and semantic differences, we clarify when these models are equivalent, when they fundamentally diverge, and how modern NLP SSMs relate to classical probabilistic models. Our analysis bridges perspectives from control theory, probabilistic modeling, and modern deep learning.

</details>


### [336] [CausationEntropy: Pythonic Optimal Causation Entropy](https://arxiv.org/abs/2601.13365)
*Kevin Slote,Jeremie Fish,Erik Bollt*

Main category: cs.LG

TL;DR: 介绍CausationEntropy 1.1版本，包括新功能、特点，代码开源，有望成复杂动力系统因果发现基准工具。


<details>
  <summary>Details</summary>
Motivation: 推动复杂动力系统因果发现，提供实用工具。

Method: 介绍优化oCSE及相关方法扩展，为软件包添加新功能。

Result: 发布CausationEntropy 1.1版，含新数据生成器、绘图工具和多种算法，易安装、文档全、有示例且模块化。

Conclusion: 该软件包有望成为复杂动力系统因果发现的基准工具。

Abstract: Optimal Causation Entropy (oCSE) is a robust causal network modeling technique that reveals causal networks from dynamical systems and coupled oscillators, distinguishing direct from indirect paths. CausationEntropy is a Python package that implements oCSE and several of its significant optimizations and methodological extensions. In this paper, we introduce the version 1.1 release of CausationEntropy, which includes new synthetic data generators, plotting tools, and several advanced information-theoretical causal network discovery algorithms with criteria for estimating Gaussian, k-nearest neighbors (kNN), geometric k-nearest neighbors (geometric-kNN), kernel density (KDE) and Poisson entropic estimators. The package is easy to install from the PyPi software repository, is thoroughly documented, supplemented with extensive code examples, and is modularly structured to support future additions. The entire codebase is released under the MIT license and is available on GitHub and through PyPi Repository. We expect this package to serve as a benchmark tool for causal discovery in complex dynamical systems.

</details>


### [337] [Can LLMs Compress (and Decompress)? Evaluating Code Understanding and Execution via Invertibility](https://arxiv.org/abs/2601.13398)
*Nickil Maveli,Antonio Vergari,Shay B. Cohen*

Main category: cs.LG

TL;DR: 现有大语言模型在往返代码执行中推理一致性有局限，提出RTCE基准测试，评估显示当前模型仍难实现往返一致性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在往返代码执行中难以保持推理一致性，需严格测试评估。

Method: 提出包含四个代码执行推理任务的RTCE基准测试，用零样本提示、基于执行轨迹的监督微调、自我反思机制评估模型。

Result: 各评估方法有一定改进，但未消除差距，当前模型仍难实现真正往返一致性。

Conclusion: 当前大语言模型缺乏可靠代码推理所需的内部一致性，RTCE有新发现。

Abstract: LLMs demonstrate strong performance on code benchmarks, yet round-trip code execution reveals limitations in their ability to maintain consistent reasoning across forward and backward execution. We present RoundTripCodeEval (RTCE), a comprehensive benchmark consisting of four distinct code execution reasoning tasks designed to rigorously test round-trip consistency. RTCE provides an execution-free, exact-match evaluation of bijection fidelity, assessing whether models preserve a consistent one-to-one mapping between encoding and decoding operations across various algorithms and directions. We systematically evaluate state-of-the-art Code-LLMs using zero-shot prompting, supervised fine-tuning on execution traces, and self-reflection mechanisms. Each yields modest improvements, but none closes the gap, indicating that current LLMs struggle with true round-trip consistency, which demonstrates that they lack the internal coherence required for trustworthy code reasoning. RTCE surfaces several new and previously unmeasured insights that are not captured by existing I/O-prediction, execution-reasoning, or round-trip natural-language benchmarks. We will release the code and the dataset upon acceptance.

</details>


### [338] [TrustEnergy: A Unified Framework for Accurate and Reliable User-level Energy Usage Prediction](https://arxiv.org/abs/2601.13422)
*Dahai Yu,Rongchao Xu,Dingyi Zhuang,Yuheng Bu,Shenhao Wang,Guang Wang*

Main category: cs.LG

TL;DR: 提出TrustEnergy框架用于精准可靠的用户级能源使用预测，经实验较基线有提升。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法做能源使用预测时，忽视家庭间空间相关性、难以进行个性化预测，且对不确定性量化探索不足。

Method: 提出TrustEnergy框架，包含分层时空表征模块和顺序共形分位数回归模块。

Result: 在佛罗里达州与电力供应商合作评估，预测准确性提高5.4%，不确定性量化改善5.7%。

Conclusion: TrustEnergy框架能有效提高用户级能源使用预测的准确性和可靠性。

Abstract: Energy usage prediction is important for various real-world applications, including grid management, infrastructure planning, and disaster response. Although a plethora of deep learning approaches have been proposed to perform this task, most of them either overlook the essential spatial correlations across households or fail to scale to individualized prediction, making them less effective for accurate fine-grained user-level prediction. In addition, due to the dynamic and uncertain nature of energy usage caused by various factors such as extreme weather events, quantifying uncertainty for reliable prediction is also significant, but it has not been fully explored in existing work. In this paper, we propose a unified framework called TrustEnergy for accurate and reliable user-level energy usage prediction. There are two key technical components in TrustEnergy, (i) a Hierarchical Spatiotemporal Representation module to efficiently capture both macro and micro energy usage patterns with a novel memory-augmented spatiotemporal graph neural network, and (ii) an innovative Sequential Conformalized Quantile Regression module to dynamically adjust uncertainty bounds to ensure valid prediction intervals over time, without making strong assumptions about the underlying data distribution. We implement and evaluate our TrustEnergy framework by working with an electricity provider in Florida, and the results show our TrustEnergy can achieve a 5.4% increase in prediction accuracy and 5.7% improvement in uncertainty quantification compared to state-of-the-art baselines.

</details>


### [339] [A Learnable Wavelet Transformer for Long-Short Equity Trading and Risk-Adjusted Return Optimization](https://arxiv.org/abs/2601.13435)
*Shuozhe Li,Du Cheng,Leqi Liu*

Main category: cs.LG

TL;DR: 提出WaveLSFormer模型用于从金融时间序列学习日内交易策略，实验显示其性能优于多种基线模型。


<details>
  <summary>Details</summary>
Motivation: 从金融时间序列学习盈利的日内交易策略因噪声、非平稳性和资产间依赖而具有挑战性。

Method: 提出基于可学习小波的多空Transformer模型WaveLSFormer，包含可学习小波前端和LGHI模块，输出经风险调整的投资组合并以交易目标和风险正则化优化。

Result: 在六年行业组五年小时级数据上，经十次随机种子评估，WaveLSFormer始终优于多种基线模型，平均累积策略回报0.607±0.045，夏普比率2.157±0.166。

Conclusion: WaveLSFormer能显著提高盈利能力和风险调整后的回报。

Abstract: Learning profitable intraday trading policies from financial time series is challenging due to heavy noise, non-stationarity, and strong cross-sectional dependence among related assets. We propose \emph{WaveLSFormer}, a learnable wavelet-based long-short Transformer that jointly performs multi-scale decomposition and return-oriented decision learning. Specifically, a learnable wavelet front-end generates low-/high-frequency components via an end-to-end trained filter bank, guided by spectral regularizers that encourage stable and well-separated frequency bands. To fuse multi-scale information, we introduce a low-guided high-frequency injection (LGHI) module that refines low-frequency representations with high-frequency cues while controlling training stability. The model outputs a portfolio of long/short positions that is rescaled to satisfy a fixed risk budget, and is optimized directly with a trading objective and risk-aware regularization. Extensive experiments on five years of hourly data across six industry groups, evaluated over ten random seeds, demonstrate that WaveLSFormer consistently outperforms MLP, LSTM and Transformer backbones, with and without fixed discrete wavelet front-ends. On average in all industries, WaveLSFormer achieves a cumulative overall strategy return of $0.607 \pm 0.045$ and a Sharpe ratio of $2.157 \pm 0.166$, substantially improving both profitability and risk-adjusted returns over the strongest baselines.

</details>


### [340] [BladeSDF : Unconditional and Conditional Generative Modeling of Representative Blade Geometries Using Signed Distance Functions](https://arxiv.org/abs/2601.13445)
*Ashish S. Nair,Sandipp Krishnan Ravi,Itzel Salgado,Changjie Sun,Sayan Ghosh,Liping Wang*

Main category: cs.LG

TL;DR: 本文提出用于涡轮叶片几何形状的特定领域隐式生成框架，利用DeepSDF解决性能感知建模和可制造设计生成问题，实现高重建保真度和稳健泛化性。


<details>
  <summary>Details</summary>
Motivation: 解决生成式AI在涡轮叶片设计中性能感知建模和可制造设计生成的关键差距。

Method: 利用连续符号距离函数（SDF）表示重建和生成几何形状，建立与叶片相关参数对齐的潜在空间，用紧凑神经网络将工程描述符映射到潜在代码。

Result: 框架实现高重建保真度，表面距离误差集中在叶片最大尺寸的1%以内，对未见设计有稳健泛化性。

Conclusion: 该方法超越传统2D引导或无约束3D管道，为数据驱动的涡轮叶片建模和概念生成提供实用且可解释的解决方案。

Abstract: Generative AI has emerged as a transformative paradigm in engineering design, enabling automated synthesis and reconstruction of complex 3D geometries while preserving feasibility and performance relevance. This paper introduces a domain-specific implicit generative framework for turbine blade geometry using DeepSDF, addressing critical gaps in performance-aware modeling and manufacturable design generation. The proposed method leverages a continuous signed distance function (SDF) representation to reconstruct and generate smooth, watertight geometries with quantified accuracy. It establishes an interpretable, near-Gaussian latent space that aligns with blade-relevant parameters, such as taper and chord ratios, enabling controlled exploration and unconditional synthesis through interpolation and Gaussian sampling. In addition, a compact neural network maps engineering descriptors, such as maximum directional strains, to latent codes, facilitating the generation of performance-informed geometry. The framework achieves high reconstruction fidelity, with surface distance errors concentrated within $1\%$ of the maximum blade dimension, and demonstrates robust generalization to unseen designs. By integrating constraints, objectives, and performance metrics, this approach advances beyond traditional 2D-guided or unconstrained 3D pipelines, offering a practical and interpretable solution for data-driven turbine blade modeling and concept generation.

</details>


### [341] [Quantum Qualifiers for Neural Network Model Selection in Hadronic Physics](https://arxiv.org/abs/2601.13463)
*Brandon B. Le,D. Keller*

Main category: cs.LG

TL;DR: 本文提出框架解决量子机器学习在强子物理问题中何时优于经典方法，通过诊断工具指导模型选择，确定预测标准，应用于康普顿形状因子提取，建立了量子机器学习工具在强子物理中的部署框架。


<details>
  <summary>Details</summary>
Motivation: 随着量子机器学习架构成熟，需确定其在数据驱动强子物理问题中相对经典方法的实用优势区间。

Method: 开发以定量量子限定符为核心的诊断工具，基于数据内在属性指导经典和量子深度神经网络的模型选择，通过控制分类和回归研究确定趋势和预测标准。

Result: 发现相对模型性能在复杂度、噪声和维度上有系统趋势，可总结为预测标准，在康普顿形状因子提取应用中，量子限定符能识别利于量子模型的运动学区域。

Conclusion: 为量子机器学习工具在精确强子物理中的部署建立了有原则的框架。

Abstract: As quantum machine-learning architectures mature, a central challenge is no longer their construction, but identifying the regimes in which they offer practical advantages over classical approaches. In this work, we introduce a framework for addressing this question in data-driven hadronic physics problems by developing diagnostic tools - centered on a quantitative quantum qualifier - that guide model selection between classical and quantum deep neural networks based on intrinsic properties of the data. Using controlled classification and regression studies, we show how relative model performance follows systematic trends in complexity, noise, and dimensionality, and how these trends can be distilled into a predictive criterion. We then demonstrate the utility of this approach through an application to Compton form factor extraction from deeply virtual Compton scattering, where the quantum qualifier identifies kinematic regimes favorable to quantum models. Together, these results establish a principled framework for deploying quantum machine-learning tools in precision hadronic physics.

</details>


### [342] [A Unified Variational Imputation Framework for Electric Vehicle Charging Data Using Retrieval-Augmented Language Model](https://arxiv.org/abs/2601.13476)
*Jinhao Li,Hao Wang*

Main category: cs.LG

TL;DR: 现有电动汽车充电数据插补方法不足，本文提出PRAIM框架，实验证明其在插补准确性等方面优于基线模型，提升下游预测性能。


<details>
  <summary>Details</summary>
Motivation: 现实中电动汽车充电数据集存在缺失记录问题，现有插补方法不适用于复杂多模态充电数据，且忽略站点间关联。

Method: 开发PRobabilistic variational imputation framework（PRAIM），用预训练语言模型编码异构数据，并用检索增强记忆动态强化，通过变分神经架构实现统一插补模型。

Result: 在四个公共数据集上的实验表明，PRAIM在插补准确性和保持原始数据统计分布方面显著优于现有基线模型。

Conclusion: PRAIM能有效克服数据稀疏问题，显著提升下游预测性能。

Abstract: The reliability of data-driven applications in electric vehicle (EV) infrastructure, such as charging demand forecasting, hinges on the availability of complete, high-quality charging data. However, real-world EV datasets are often plagued by missing records, and existing imputation methods are ill-equipped for the complex, multimodal context of charging data, often relying on a restrictive one-model-per-station paradigm that ignores valuable inter-station correlations. To address these gaps, we develop a novel PRobabilistic variational imputation framework that leverages the power of large lAnguage models and retrIeval-augmented Memory (PRAIM). PRAIM employs a pre-trained language model to encode heterogeneous data, spanning time-series demand, calendar features, and geospatial context, into a unified, semantically rich representation. This is dynamically fortified by retrieval-augmented memory that retrieves relevant examples from the entire charging network, enabling a single, unified imputation model empowered by variational neural architecture to overcome data sparsity. Extensive experiments on four public datasets demonstrate that PRAIM significantly outperforms established baselines in both imputation accuracy and its ability to preserve the original data's statistical distribution, leading to substantial improvements in downstream forecasting performance.

</details>


### [343] [StoTAM: Stochastic Alternating Minimization for Tucker-Structured Tensor Sensing](https://arxiv.org/abs/2601.13522)
*Shuang Li*

Main category: cs.LG

TL;DR: 提出针对低Tucker秩张量的随机交替最小化算法，实验显示该算法在收敛性上优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 现有低Tucker秩张量恢复方法存在计算成本高的问题，如全张量投影昂贵、依赖全梯度计算等，且多数随机因子分解方法受限。

Method: 提出在Tucker分解下直接对核心张量和因子矩阵操作的随机交替最小化算法，避免重复张量投影，实现低维张量因子的高效小批量更新。

Result: 在合成张量传感的数值实验中，所提算法在时钟时间上收敛表现优于代表性随机张量恢复基准方法。

Conclusion: 所提随机交替最小化算法在低Tucker秩张量恢复上具有优势，有较好的收敛性能。

Abstract: Low-rank tensor sensing is a fundamental problem with broad applications in signal processing and machine learning. Among various tensor models, low-Tucker-rank tensors are particularly attractive for capturing multi-mode subspace structures in high-dimensional data. Existing recovery methods either operate on the full tensor variable with expensive tensor projections, or adopt factorized formulations that still rely on full-gradient computations, while most stochastic factorized approaches are restricted to tensor decomposition settings. In this work, we propose a stochastic alternating minimization algorithm that operates directly on the core tensor and factor matrices under a Tucker factorization. The proposed method avoids repeated tensor projections and enables efficient mini-batch updates on low-dimensional tensor factors. Numerical experiments on synthetic tensor sensing demonstrate that the proposed algorithm exhibits favorable convergence behavior in wall-clock time compared with representative stochastic tensor recovery baselines.

</details>


### [344] [MN-TSG:Continuous Time Series Generation with Irregular Observations](https://arxiv.org/abs/2601.13534)
*Xu Zhang,Junwei Deng,Chang Xu,Hao Li,Jiang Bian*

Main category: cs.LG

TL;DR: 提出MN - TSG框架解决不规则时间序列生成问题，实验显示其效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列生成方法假设规则采样和固定输出分辨率，与现实场景不符，NCDEs在捕捉复杂动态时间模式和支持连续TSG方面有局限。

Method: 提出MN - TSG框架，采用基于MoE的NCDEs架构，结合现有TSG模型，MoE - NCDE有动态参数化专家函数和分离式设计，利用现有模型学习联合分布。

Result: 在十个公开和合成数据集上实验，MN - TSG在不规则到规则和不规则到连续生成任务中始终优于强大的TSG基线。

Conclusion: MN - TSG框架有效，能解决不规则时间序列生成问题。

Abstract: Time series generation (TSG) plays a critical role in a wide range of domains, such as healthcare. However, most existing methods assume regularly sampled observations and fixed output resolutions, which are often misaligned with real-world scenarios where data are irregularly sampled and sparsely observed. This mismatch is particularly problematic in applications such as clinical monitoring, where irregular measurements must support downstream tasks requiring continuous and high-resolution time series.
  Neural Controlled Differential Equations (NCDEs) have shown strong potential for modeling irregular time series, yet they still face challenges in capturing complex dynamic temporal patterns and supporting continuous TSG. To address these limitations, we propose MN-TSG, a novel framework that explores Mixture-of-Experts (MoE)-based NCDEs and integrates them with existing TSG models for irregular and continuous generation tasks.
  The core of MN-TSG lies in a MoE-NCDE architecture with dynamically parameterized expert functions and a decoupled design that facilitates more effective optimization of MoE dynamics. Furthermore, we leverage existing TSG models to learn the joint distribution over the mixture of experts and the generated time series. This enables the framework not only to generate new samples, but also to produce appropriate expert configurations tailored to each sample, thereby supporting refined continuous TSG.
  Extensive experiments on ten public and synthetic datasets demonstrate the effectiveness of MN-TSG, consistently outperforming strong TSG baselines on both irregular-to-regular and irregular-to-continuous generation tasks.

</details>


### [345] [Patterning: The Dual of Interpretability](https://arxiv.org/abs/2601.13548)
*George Wang,Daniel Murfet*

Main category: cs.LG

TL;DR: 本文介绍了与机械可解释性对偶的模式化（patterning）问题，基于敏感性提出方法，在小语言模型和合成任务中进行验证，表明读取内部结构的框架可用于写入结构。


<details>
  <summary>Details</summary>
Motivation: 机械可解释性旨在逆向工程神经网络内部结构以理解泛化能力，本文引入对偶问题，即给定期望的泛化形式，确定产生它的训练数据。

Method: 基于敏感性（susceptibilities）衡量可观测量的后验期望值对数据分布微小变化的响应，通过反转这种线性响应关系得到引导模型达到目标内部配置的数据干预方法。

Result: 在小语言模型中，沿主敏感性方向重新加权训练数据可加速或延迟结构形成；在合成括号平衡任务中可选择模型学习的算法。

Conclusion: 用于读取内部结构的数学框架可以反转来写入结构。

Abstract: Mechanistic interpretability aims to understand how neural networks generalize beyond their training data by reverse-engineering their internal structures. We introduce patterning as the dual problem: given a desired form of generalization, determine what training data produces it. Our approach is based on susceptibilities, which measure how posterior expectation values of observables respond to infinitesimal shifts in the data distribution. Inverting this linear response relationship yields the data intervention that steers the model toward a target internal configuration. We demonstrate patterning in a small language model, showing that re-weighting training data along principal susceptibility directions can accelerate or delay the formation of structure, such as the induction circuit. In a synthetic parentheses balancing task where multiple algorithms achieve perfect training accuracy, we show that patterning can select which algorithm the model learns by targeting the local learning coefficient of each solution. These results establish that the same mathematical framework used to read internal structure can be inverted to write it.

</details>


### [346] [ButterflyMoE: Sub-Linear Ternary Experts via Structured Butterfly Orbits](https://arxiv.org/abs/2601.13563)
*Aryan Karmore*

Main category: cs.LG

TL;DR: 提出ButterflyMoE方法解决线性内存扩展问题，在语言模型基准测试中实现显著内存缩减且精度损失小。


<details>
  <summary>Details</summary>
Motivation: 线性内存扩展所需内存超出边缘设备预算，现有压缩方法无法解决缩放瓶颈。

Method: 将专家视为统一共享量化基质的几何重新定向，通过对共享三元原型应用学习的旋转生成专家，同时训练旋转和量化。

Result: 在语言建模基准测试中，256个专家时实现150倍内存缩减，4GB设备可容纳64个专家。

Conclusion: 几何参数化打破了线性缩放

Abstract: Linear memory scaling stores $N$ independent expert weight matrices requiring $\mathcal{O}(N \cdot d^2)$ memory, which exceeds edge devices memory budget. Current compression methods like quantization, pruning and low-rank factorization reduce constant factors but leave the scaling bottleneck unresolved. We introduce ButterflyMoE, a method that treats experts not as independent weight matrices but as geometric reorientations of a unified shared quantized substrate. Diversity among experts arises from viewing different angles of shared capacity, not from redundant storage. By applying learned rotations to a shared ternary prototype, each expert yields $\mathcal{O}(d^2 + N \cdot d \log d)$ memory -- sub-linear in the number of experts. The key insight: training these rotations with quantization reduces activation outliers and stabilizes extreme low bit training, where static methods collapse. Across language modeling benchmarks, ButterflyMoE achieves 150 times memory reduction at 256 experts with negligible accuracy loss. This allows 64 experts to fit on 4GB devices compared to standard MoE's 8 experts, showing geometric parametrization breaks linear scaling.

</details>


### [347] [Multi-objective fluorescent molecule design with a data-physics dual-driven generative framework](https://arxiv.org/abs/2601.13564)
*Yanheng Li,Zhichen Pu,Lijiang Yang,Zehao Zhou,Yi Qin Gao*

Main category: cs.LG

TL;DR: 提出数据与物理驱动的荧光分子逆向设计框架LUMOS，在多方面表现优于基线模型，可生成符合目标规格的有效荧光团。


<details>
  <summary>Details</summary>
Motivation: 传统生成 - 评分 - 筛选方法在设计具有特定光学和物理化学性质的荧光小分子时，因搜索效率低、机器学习预测泛化性不可靠和量子化学计算成本高而不实用。

Method: LUMOS将生成器和预测器结合在共享潜在表示中，结合神经网络与快速TD - DFT计算工作流构建互补预测器，采用属性引导的扩散模型与多目标进化算法结合。

Result: 在综合基准测试中，LUMOS在荧光特性预测的准确性、泛化性和物理合理性方面始终优于基线模型，在多目标支架和片段级分子优化中表现出色，TD - DFT和MD模拟验证其可生成符合目标的有效荧光团。

Conclusion: LUMOS是一个数据 - 物理双驱动的通用荧光团逆向设计框架。

Abstract: Designing fluorescent small molecules with tailored optical and physicochemical properties requires navigating vast, underexplored chemical space while satisfying multiple objectives and constraints. Conventional generate-score-screen approaches become impractical under such realistic design specifications, owing to their low search efficiency, unreliable generalizability of machine-learning prediction, and the prohibitive cost of quantum chemical calculation. Here we present LUMOS, a data-and-physics driven framework for inverse design of fluorescent molecules. LUMOS couples generator and predictor within a shared latent representation, enabling direct specification-to-molecule design and efficient exploration. Moreover, LUMOS combines neural networks with a fast time-dependent density functional theory (TD-DFT) calculation workflow to build a suite of complementary predictors spanning different trade-offs in speed, accuracy, and generalizability, enabling reliable property prediction across diverse scenarios. Finally, LUMOS employs a property-guided diffusion model integrated with multi-objective evolutionary algorithms, enabling de novo design and molecular optimization under multiple objectives and constraints. Across comprehensive benchmarks, LUMOS consistently outperforms baseline models in terms of accuracy, generalizability and physical plausibility for fluorescence property prediction, and demonstrates superior performance in multi-objective scaffold- and fragment-level molecular optimization. Further validation using TD-DFT and molecular dynamics (MD) simulations demonstrates that LUMOS can generate valid fluorophores that meet various target specifications. Overall, these results establish LUMOS as a data-physics dual-driven framework for general fluorophore inverse design.

</details>


### [348] [Self-Improvement as Coherence Optimization: A Theoretical Account](https://arxiv.org/abs/2601.13566)
*Tianyi Qiu,Ahmed Hani Ismail,Zhonghao He,Shi Feng*

Main category: cs.LG

TL;DR: 探讨无外部监督下语言模型自提升方法的原理，指出是连贯优化特例，理论证明其与描述长度正则化等价，实验支持理论解释。


<details>
  <summary>Details</summary>
Motivation: 现有无外部监督下语言模型提升准确性方法的工作原理在理论上不清楚，需进行解释。

Method: 证明连贯优化是描述长度正则化的等价形式，并分析其在半监督学习中的最优性。

Result: 证明了连贯优化与描述长度正则化等价，初步实验支持理论。

Conclusion: 理论解释了无反馈自提升方法的工作原理，并能预测其成败情况。

Abstract: Can language models improve their accuracy without external supervision? Methods such as debate, bootstrap, and internal coherence maximization achieve this surprising feat, even matching golden finetuning performance. Yet why they work remains theoretically unclear. We show that they are all special cases of coherence optimization: finding a context-to-behavior mapping that's most compressible and jointly predictable. We prove that coherence optimization is equivalent to description-length regularization, and that among all such regularization schemes, it is optimal for semi-supervised learning when the regularizer is derived from a pretrained model. Our theory, supported by preliminary experiments, explains why feedback-free self-improvement works and predicts when it should succeed or fail.

</details>


### [349] [DRGW: Learning Disentangled Representations for Robust Graph Watermarking](https://arxiv.org/abs/2601.13569)
*Jiasen Li,Yanwei Liu,Zhuoyi Shang,Xiaoyan Gu,Weiping Wang*

Main category: cs.LG

TL;DR: 提出图水印框架DRGW，通过解纠缠表示学习解决现有图水印方法问题，实验显示其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有图水印方法基于图结构或纠缠的图表示，存在信息耦合和离散化问题，影响水印透明性和鲁棒性。

Method: 设计对抗训练编码器学习不变结构表示，得到独立水印载体；设计图感知可逆神经网络用于水印嵌入和提取；开发结构感知编辑器解决潜在修改到离散图编辑问题。

Result: 在多个基准数据集上的实验表明DRGW具有优越的有效性。

Conclusion: DRGW能有效解决现有图水印方法的问题，具有良好的鲁棒性、透明度和可检测性。

Abstract: Graph-structured data is foundational to numerous web applications, and watermarking is crucial for protecting their intellectual property and ensuring data provenance. Existing watermarking methods primarily operate on graph structures or entangled graph representations, which compromise the transparency and robustness of watermarks due to the information coupling in representing graphs and uncontrollable discretization in transforming continuous numerical representations into graph structures. This motivates us to propose DRGW, the first graph watermarking framework that addresses these issues through disentangled representation learning. Specifically, we design an adversarially trained encoder that learns an invariant structural representation against diverse perturbations and derives a statistically independent watermark carrier, ensuring both robustness and transparency of watermarks. Meanwhile, we devise a graph-aware invertible neural network to provide a lossless channel for watermark embedding and extraction, guaranteeing high detectability and transparency of watermarks. Additionally, we develop a structure-aware editor that resolves the issue of latent modifications into discrete graph edits, ensuring robustness against structural perturbations. Experiments on diverse benchmark datasets demonstrate the superior effectiveness of DRGW.

</details>


### [350] [GeoDynamics: A Geometric State-Space Neural Network for Understanding Brain Dynamics on Riemannian Manifolds](https://arxiv.org/abs/2601.13570)
*Tingting Dan,Jiaqi Ding,Guorong Wu*

Main category: cs.LG

TL;DR: 文章提出GeoDynamics几何状态空间神经网络，可在高维SPD流形上追踪潜在脑状态轨迹，能揭示任务驱动的状态变化和疾病早期标志物，还能用于人类动作识别。


<details>
  <summary>Details</summary>
Motivation: 现有方法多将大脑视为松散连接区域集合或采用过简化网络先验，缺乏整体自组织动力系统视角，而捕捉大脑功能连接的SPD矩阵轨迹对理解认知和行为很重要。

Method: 引入GeoDynamics，将每个连接矩阵嵌入到流形感知的循环框架中。

Result: 揭示了任务驱动的状态变化和阿尔茨海默病、帕金森病和自闭症的早期标志物；在人类动作识别基准测试中得到验证，展示了可扩展性和鲁棒性。

Conclusion: GeoDynamics可用于建模跨不同领域的复杂时空动态。

Abstract: State-space models (SSMs) have become a cornerstone for unraveling brain dynamics, revealing how latent neural states evolve over time and give rise to observed signals. By combining the flexibility of deep learning with the principled dynamical structure of SSMs, recent studies have achieved powerful fits to functional neuroimaging data. However, most existing approaches still view the brain as a set of loosely connected regions or impose oversimplified network priors, falling short of a truly holistic and self-organized dynamical system perspective. Brain functional connectivity (FC) at each time point naturally forms a symmetric positive definite (SPD) matrix, which resides on a curved Riemannian manifold rather than in Euclidean space. Capturing the trajectories of these SPD matrices is key to understanding how coordinated networks support cognition and behavior. To this end, we introduce GeoDynamics, a geometric state-space neural network that tracks latent brain-state trajectories directly on the high-dimensional SPD manifold. GeoDynamics embeds each connectivity matrix into a manifold-aware recurrent framework, learning smooth and geometry-respecting transitions that reveal task-driven state changes and early markers of Alzheimer's disease, Parkinson's disease, and autism. Beyond neuroscience, we validate GeoDynamics on human action recognition benchmarks (UTKinect, Florence, HDM05), demonstrating its scalability and robustness in modeling complex spatiotemporal dynamics across diverse domains.

</details>


### [351] [Behavior Knowledge Merge in Reinforced Agentic Models](https://arxiv.org/abs/2601.13572)
*Xiangchi Yuan,Dachuan Shi,Chunhui Zhang,Zheyuan Liu,Shenglong Yao,Soroush Vosoughi,Wenke Lee*

Main category: cs.LG

TL;DR: 现有模型合并方法不适用于强化学习训练的智能体模型，提出Reinforced Agent Merging (RAM) 框架，实验证明其效果优于基线并能激发智能体协同潜力。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法为监督微调设计，用于强化学习训练的智能体模型时无法有效保留特定任务能力，根源是强化学习和监督微调的任务向量不匹配。

Method: 提出Reinforced Agent Merging (RAM) 分布感知合并框架，分离共享和特定任务的参数更新，对共享部分平均，选择性保留和重新缩放特定部分以抵消参数更新稀释。

Result: 在多个智能体领域和模型架构的实验中，RAM 不仅超越合并基线，还能激发智能体间协同潜力，在特定领域表现优于专门智能体。

Conclusion: RAM 框架适用于强化学习训练的智能体模型，能有效解决现有模型合并方法的问题。

Abstract: Reinforcement learning (RL) is central to post-training, particularly for agentic models that require specialized reasoning behaviors. In this setting, model merging offers a practical mechanism for integrating multiple RL-trained agents from different tasks into a single generalist model. However, existing merging methods are designed for supervised fine-tuning (SFT), and they are suboptimal to preserve task-specific capabilities on RL-trained agentic models. The root is a task-vector mismatch between RL and SFT: on-policy RL induces task vectors that are highly sparse and heterogeneous, whereas SFT-style merging implicitly assumes dense and globally comparable task vectors. When standard global averaging is applied under this mismatch, RL's non-overlapping task vectors that encode critical task-specific behaviors are reduced and parameter updates are diluted. To address this issue, we propose Reinforced Agent Merging (RAM), a distribution-aware merging framework explicitly designed for RL-trained agentic models. RAM disentangles shared and task-specific unique parameter updates, averaging shared components while selectively preserving and rescaling unique ones to counteract parameter update dilution. Experiments across multiple agent domains and model architectures demonstrate that RAM not only surpasses merging baselines, but also unlocks synergistic potential among agents to achieve performance superior to that of specialized agents in their domains.

</details>


### [352] [FG-OrIU: Towards Better Forgetting via Feature-Gradient Orthogonality for Incremental Unlearning](https://arxiv.org/abs/2601.13578)
*Qian Feng,JiaHang Tu,Mintong Kang,Hanbin Zhao,Chao Zhang,Hui Qian*

Main category: cs.LG

TL;DR: 提出FG - OrIU框架用于增量式遗忘，统一特征和梯度正交约束实现深度遗忘，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有增量式遗忘方法存在表面遗忘问题，有安全风险且破坏保留平衡，需实现深度遗忘。

Method: 提出FG - OrIU框架，通过奇异值分解（SVD）分解特征空间，实施特征和梯度正交投影双重约束，采用动态子空间自适应。

Result: 广泛实验证明了FG - OrIU方法的有效性。

Conclusion: FG - OrIU框架能有效解决增量式遗忘中的问题，实现深度且不可逆的遗忘效果。

Abstract: Incremental unlearning (IU) is critical for pre-trained models to comply with sequential data deletion requests, yet existing methods primarily suppress parameters or confuse knowledge without explicit constraints on both feature and gradient level, resulting in \textit{superficial forgetting} where residual information remains recoverable. This incomplete forgetting risks security breaches and disrupts retention balance, especially in IU scenarios. We propose FG-OrIU (\textbf{F}eature-\textbf{G}radient \textbf{Or}thogonality for \textbf{I}ncremental \textbf{U}nlearning), the first framework unifying orthogonal constraints on both features and gradients level to achieve deep forgetting, where the forgetting effect is irreversible. FG-OrIU decomposes feature spaces via Singular Value Decomposition (SVD), separating forgetting and remaining class features into distinct subspaces. It then enforces dual constraints: feature orthogonal projection on both forgetting and remaining classes, while gradient orthogonal projection prevents the reintroduction of forgotten knowledge and disruption to remaining classes during updates. Additionally, dynamic subspace adaptation merges newly forgetting subspaces and contracts remaining subspaces, ensuring a stable balance between removal and retention across sequential unlearning tasks. Extensive experiments demonstrate the effectiveness of our method.

</details>


### [353] [Neural Organ Transplantation (NOT): Checkpoint-Based Modular Adaptation for Transformer Models](https://arxiv.org/abs/2601.13580)
*Ahmad Al-Zuraiqi*

Main category: cs.LG

TL;DR: 提出Neural Organ Transplantation (NOT)模块化适应框架，用于领域适应，在多种模型实验中表现优于现有方法，有位置依赖，目前限于解码器模型。


<details>
  <summary>Details</summary>
Motivation: 传统微调方法将训练参数与特定模型实例和训练数据紧密耦合，需要更灵活的领域适应方法。

Method: 从预训练模型中提取连续层子集作为“供体器官”，在特定领域数据上独立训练并保存为独立检查点文件，移植到兼容的受体模型中。

Result: 在三种解码器架构实验中，供体移植显著优于现有适应方法，困惑度比LoRA有数量级提升，训练更快，有位置依赖，跨域转移有正则化好处。

Conclusion: 解码器架构的变压器中间层可支持高效模块化转移，能通过检查点分发实现隐私保护的专业知识共享，但目前限于解码器模型。

Abstract: We introduce Neural Organ Transplantation (NOT), a modular adaptation framework that enables trained transformer layers to function as reusable transferable checkpoints for domain adaptation. Unlike conventional fine-tuning approaches that tightly couple trained parameters to specific model instances and training data, NOT extracts contiguous layer subsets ("donor organs") from pre-trained models, trains them independently on domain-specific data, and saves them as standalone checkpoint files that can be transplanted into compatible recipient models without access to the original training data. Through experiments on three decoder-only transformer architectures spanning 124M to 20B parameters (GPT-2, TinyLlama, and GPT-OSS), we demonstrate that donor transplantation substantially outperforms existing adaptation methods, achieving an order-of-magnitude improvement in perplexity over LoRA while training significantly faster. The method exhibits position dependence, with early insertion positions yielding optimal results. Cross-domain transfer at billion-parameter scale reveals unexpected regularization benefits. These findings demonstrate that transformer middle layers can support efficient modular transfer for decoder-only architectures, enabling privacy-preserving expertise sharing through checkpoint distribution. We note that this approach is currently limited to decoder-only models; preliminary experiments on encoder-based architectures show reduced effectiveness.

</details>


### [354] [Machine learning based radiative parameterization scheme and its performance in operational reforecast experiments](https://arxiv.org/abs/2601.13592)
*Hao Jing,Sa Xiao,Haoyu Li,Huadong Xiao,Wei Xue*

Main category: cs.LG

TL;DR: 本文研究将深度学习网络嵌入数值预报模式的混合预报框架局限，以CNN近似辐射模式，经多方法改进后，让混合模式能做十天集成预报，计算速度提升约八倍，精度与传统方案相当。


<details>
  <summary>Details</summary>
Motivation: 辐射是数值模式中最耗时的物理过程，采用机器学习模拟辐射过程虽能提高计算效率，但混合预报框架存在耦合兼容性和长期积分稳定性两个基本瓶颈，因此开展相关研究。

Method: 采用残差卷积神经网络近似RRTMG，运用离线训练和在线耦合方法，通过经验回放增强数据集、添加基于物理意义的输出约束确保稳定性，使用基于LibTorch的耦合方法用于实时计算。

Result: 混合模型能按要求进行十天集成预报，两个月的业务性再预报实验表明，机器学习模拟器精度与传统物理方案相当，计算速度提升约八倍。

Conclusion: 通过一系列方法改进的混合模型在计算效率上有显著提升，且能保证预报精度。

Abstract: Radiation is typically the most time-consuming physical process in numerical models. One solution is to use machine learning methods to simulate the radiation process to improve computational efficiency. From an operational standpoint, this study investigates critical limitations inherent to hybrid forecasting frameworks that embed deep neural networks into numerical prediction models, with a specific focus on two fundamental bottlenecks: coupling compatibility and long-term integration stability. A residual convolutional neural network is employed to approximate the Rapid Radiative Transfer Model for General Circulation Models (RRTMG) within the global operational system of China Meteorological Administration. We adopted an offline training and online coupling approach. First, a comprehensive dataset is generated through model simulations, encompassing all atmospheric columns both with and without cloud cover. To ensure the stability of the hybrid model, the dataset is enhanced via experience replay, and additional output constraints based on physical significance are imposed. Meanwhile, a LibTorch-based coupling method is utilized, which is more suitable for real-time operational computations. The hybrid model is capable of performing ten-day integrated forecasts as required. A two-month operational reforecast experiment demonstrates that the machine learning emulator achieves accuracy comparable to that of the traditional physical scheme, while accelerating the computation speed by approximately eightfold.

</details>


### [355] [Diffusion In Diffusion: Breaking the Autoregressive Bottleneck in Block Diffusion Models](https://arxiv.org/abs/2601.13599)
*Linrui Ma,Yufei Cui,Kai Han,Yunhe Wang*

Main category: cs.LG

TL;DR: 提出Diffusion in Diffusion框架解决块扩散语言模型问题，减少生成困惑度。


<details>
  <summary>Details</summary>
Motivation: 解决块扩散语言模型的不可逆性和缺乏全局规划能力的问题。

Method: 先使用块扩散生成草稿，再通过全局双向扩散进行细化，利用快照置信度重掩码和混合尺度训练。

Result: 在OpenWebText数据集上为离散扩散模型设定新基准，用26%微调预算将生成困惑度从25.7降至21.9。

Conclusion: 所提方法有效缩小了与自回归模型的性能差距。

Abstract: Block diffusion language models, operating as semi-autoregressive paradigms, combine the strengths of both autoregressive and diffusion paradigms. However, their strict unidirectional block dependencies introduce irreversibility and sacrifice the global planning capabilities for which diffusion models are renowned. In order to address these issues, we propose Diffusion in Diffusion, a draft-then-refine framework designed to overcome the irreversibility and myopia problems inherent in block diffusion models. Our approach first employs block diffusion to generate rapid drafts using small blocks, then refines these drafts through global bidirectional diffusion with a larger bidirectional receptive field. We utilise snapshot confidence remasking to identify the most critical tokens that require modification, and apply mix-scale training to expand the block diffusion model's global capabilities. Empirical results demonstrate that our approach sets a new benchmark for discrete diffusion models on the OpenWebText dataset. Using just 26% of the fine-tuning budget of baseline models, we reduce generative perplexity from 25.7 to 21.9, significantly narrowing the performance gap with autoregressive models.

</details>


### [356] [Fisher-Informed Parameterwise Aggregation for Federated Learning with Heterogeneous Data](https://arxiv.org/abs/2601.13608)
*Zhipeng Chang,Ting He,Wenrui Hao*

Main category: cs.LG

TL;DR: 提出FIPA二阶聚合方法用于联邦学习，在多种任务表现出色，凸显其在异构数据分布下优势。


<details>
  <summary>Details</summary>
Motivation: 标准一阶方法在非IID数据下对客户端参数统一加权，导致客户端漂移和模型性能下降。

Method: 提出Fisher - Informed Parameterwise Aggregation（FIPA）二阶聚合方法，用特定参数的Fisher信息矩阵权重替代客户端标量权重，并采用低秩近似保证效率。

Result: 在非线性函数回归、PDE学习和图像分类等任务中，FIPA优于基于平均的聚合方法，可与客户端优化算法结合提升图像分类准确率。

Conclusion: FIPA在异构数据分布的联邦学习中有显著优势。

Abstract: Federated learning aggregates model updates from distributed clients, but standard first order methods such as FedAvg apply the same scalar weight to all parameters from each client. Under non-IID data, these uniformly weighted updates can be strongly misaligned across clients, causing client drift and degrading the global model. Here we propose Fisher-Informed Parameterwise Aggregation (FIPA), a second-order aggregation method that replaces client-level scalar weights with parameter-specific Fisher Information Matrix (FIM) weights, enabling true parameter-level scaling that captures how each client's data uniquely influences different parameters. With low-rank approximation, FIPA remains communication- and computation-efficient. Across nonlinear function regression, PDE learning, and image classification, FIPA consistently improves over averaging-based aggregation, and can be effectively combined with state-of-the-art client-side optimization algorithms to further improve image classification accuracy. These results highlight the benefits of FIPA for federated learning under heterogeneous data distributions.

</details>


### [357] [Quadratic Upper Bound for Boosting Robustness](https://arxiv.org/abs/2601.13645)
*Euijin You,Hyang-Won Lee*

Main category: cs.LG

TL;DR: 本文提出QUB损失函数缓解快速对抗训练（FAT）鲁棒性下降问题，实验显示可显著提升鲁棒性，可能源于损失景观更平滑。


<details>
  <summary>Details</summary>
Motivation: FAT因对抗空间探索不足导致鲁棒性受损，需解决其鲁棒性下降问题。

Method: 推导对抗训练损失函数的二次上界（QUB），并将该界与现有FAT方法结合使用。

Result: 将QUB损失应用于现有方法可显著提升模型鲁棒性。

Conclusion: QUB损失能有效缓解FAT鲁棒性下降问题，提升可能源于模型损失景观更平滑。

Abstract: Fast adversarial training (FAT) aims to enhance the robustness of models against adversarial attacks with reduced training time, however, FAT often suffers from compromised robustness due to insufficient exploration of adversarial space. In this paper, we develop a loss function to mitigate the problem of degraded robustness under FAT. Specifically, we derive a quadratic upper bound (QUB) on the adversarial training (AT) loss function and propose to utilize the bound with existing FAT methods. Our experimental results show that applying QUB loss to the existing methods yields significant improvement of robustness. Furthermore, using various metrics, we demonstrate that this improvement is likely to result from the smoothened loss landscape of the resulting model.

</details>


### [358] [TimeART: Towards Agentic Time Series Reasoning via Tool-Augmentation](https://arxiv.org/abs/2601.13653)
*Xingjian Wu,Junkai Lu,Zhengyu Li,Xiangfei Qiu,Jilin Hu,Chenjuan Guo,Christian S. Jensen,Bin Yang*

Main category: cs.LG

TL;DR: 本文提出TimeART框架和TimeToolBench语料库，设计训练策略，训练的8B TSRM在多任务中达SOTA，开创代理式时间序列推理新方法。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列数据分析和解释工作主要依赖人力，成本高且缺乏自动化。

Method: 引入融合工具分析能力和大语言模型推理能力的TimeART框架；收集100k专家轨迹语料库TimeToolBench；设计四阶段训练策略提升模型泛化能力。

Result: 在TimeToolBench上训练的8B TSRM结合TimeART框架，在多个时间序列问答任务上达到一致的最优性能。

Conclusion: 开创了一种新的代理式时间序列推理方法。

Abstract: Time series data widely exist in real-world cyber-physical systems. Though analyzing and interpreting them contributes to significant values, e.g, disaster prediction and financial risk control, current workflows mainly rely on human data scientists, which requires significant labor costs and lacks automation. To tackle this, we introduce TimeART, a framework fusing the analytical capability of strong out-of-the-box tools and the reasoning capability of Large Language Models (LLMs), which serves as a fully agentic data scientist for Time Series Question Answering (TSQA). To teach the LLM-based Time Series Reasoning Models (TSRMs) strategic tool-use, we also collect a 100k expert trajectory corpus called TimeToolBench. To enhance TSRMs' generalization capability, we then devise a four-stage training strategy, which boosts TSRMs through learning from their own early experiences and self-reflections. Experimentally, we train an 8B TSRM on TimeToolBench and equip it with the TimeART framework, and it achieves consistent state-of-the-art performance on multiple TSQA tasks, which pioneers a novel approach towards agentic time series reasoning.

</details>


### [359] [Autoregressive deep learning for real-time simulation of soft tissue dynamics during virtual neurosurgery](https://arxiv.org/abs/2601.13676)
*Fabian Greifeneder,Wolfgang Fenz,Benedikt Alkin,Johannes Brandstetter,Michael Giretzlehner,Philipp Moser*

Main category: cs.LG

TL;DR: 本文提出基于深度学习的替代模型模拟瞬态脑变形，采用随机教师强制策略减少误差，模型准确高效且提升稳定性，可用于交互式神经外科模拟环境。


<details>
  <summary>Details</summary>
Motivation: 传统数值求解器难以满足实时性能要求，需开发高效模型来模拟脑变形以用于神经外科模拟器。

Method: 基于通用物理变压器，直接处理大规模网格数据，在非线性有限元模拟生成的数据集上训练，采用随机教师强制策略减少自回归推理误差。

Result: 替代模型能准确高效预测多种脑变形场景，随机教师强制技术提升长期滚动稳定性，集成到模拟环境后每步运行时间低于10ms。

Conclusion: 所提深度学习框架可实现动态脑组织变形的快速、平滑和准确模拟，为逼真手术训练环境奠定基础。

Abstract: Accurate simulation of brain deformation is a key component for developing realistic, interactive neurosurgical simulators, as complex nonlinear deformations must be captured to ensure realistic tool-tissue interactions. However, traditional numerical solvers often fall short in meeting real-time performance requirements. To overcome this, we introduce a deep learning-based surrogate model that efficiently simulates transient brain deformation caused by continuous interactions between surgical instruments and the virtual brain geometry. Building on Universal Physics Transformers, our approach operates directly on large-scale mesh data and is trained on an extensive dataset generated from nonlinear finite element simulations, covering a broad spectrum of temporal instrument-tissue interaction scenarios. To reduce the accumulation of errors in autoregressive inference, we propose a stochastic teacher forcing strategy applied during model training. Specifically, training consists of short stochastic rollouts in which the proportion of ground truth inputs is gradually decreased in favor of model-generated predictions. Our results show that the proposed surrogate model achieves accurate and efficient predictions across a range of transient brain deformation scenarios, scaling to meshes with up to 150,000 nodes. The introduced stochastic teacher forcing technique substantially improves long-term rollout stability, reducing the maximum prediction error from 6.7 mm to 3.5 mm. We further integrate the trained surrogate model into an interactive neurosurgical simulation environment, achieving runtimes below 10 ms per simulation step on consumer-grade inference hardware. Our proposed deep learning framework enables rapid, smooth and accurate biomechanical simulations of dynamic brain tissue deformation, laying the foundation for realistic surgical training environments.

</details>


### [360] [Who Should Have Surgery? A Comparative Study of GenAI vs Supervised ML for CRS Surgical Outcome Prediction](https://arxiv.org/abs/2601.13710)
*Sayeed Shafayet Chowdhury,Snehasis Mukhopadhyay,Shiaofen Fang,Vijay R. Ramakrishnan*

Main category: cs.LG

TL;DR: 研究术前预测慢性鼻窦炎临床改善情况，对比监督式ML和生成式AI，支持ML优先、GenAI增强的工作流程。


<details>
  <summary>Details</summary>
Motivation: 人工智能在医学影像有应用，但在临床数据前瞻性决策支持中有限，要研究术前预测慢性鼻窦炎临床改善及避免手术患者。

Method: 在前瞻性收集队列中，用术前临床数据，对比监督式ML（逻辑回归、树集成、MLP）和生成式AI（ChatGPT等），给出相同输入并限制输出。

Result: 最佳ML模型（MLP）准确率85%，校准和决策曲线净效益优，GenAI在辨别和校准上表现差，但理由与临床医生启发式和MLP特征重要性一致。

Conclusion: 支持ML优先、GenAI增强的工作流程，用校准ML进行手术候选者初步分诊，GenAI作解释器。

Abstract: Artificial intelligence has reshaped medical imaging, yet the use of AI on clinical data for prospective decision support remains limited. We study pre-operative prediction of clinically meaningful improvement in chronic rhinosinusitis (CRS), defining success as a more than 8.9-point reduction in SNOT-22 at 6 months (MCID). In a prospectively collected cohort where all patients underwent surgery, we ask whether models using only pre-operative clinical data could have identified those who would have poor outcomes, i.e. those who should have avoided surgery. We benchmark supervised ML (logistic regression, tree ensembles, and an in-house MLP) against generative AI (ChatGPT, Claude, Gemini, Perplexity), giving each the same structured inputs and constraining outputs to binary recommendations with confidence. Our best ML model (MLP) achieves 85 % accuracy with superior calibration and decision-curve net benefit. GenAI models underperform on discrimination and calibration across zero-shot setting. Notably, GenAI justifications align with clinician heuristics and the MLP's feature importance, repeatedly highlighting baseline SNOT-22, CT/endoscopy severity, polyp phenotype, and physchology/pain comorbidities. We provide a reproducible tabular-to-GenAI evaluation protocol and subgroup analyses. Findings support an ML-first, GenAI- augmented workflow: deploy calibrated ML for primary triage of surgical candidacy, with GenAI as an explainer to enhance transparency and shared decision-making.

</details>


### [361] [EEG-Titans: Long-Horizon Seizure Forecasting via Dual-Branch Attention and Neural Memory](https://arxiv.org/abs/2601.13748)
*Tien-Dat Pham,Xuan-The Tran*

Main category: cs.LG

TL;DR: 提出EEG - Titans模型用于癫痫发作预测，在CHB - MIT数据集表现良好，能在临床评估下提供可靠预测。


<details>
  <summary>Details</summary>
Motivation: 准确的癫痫发作预测具有挑战性，现有深度学习模型在处理超长序列时难以平衡局部时空模式和长程上下文信息。

Method: 提出EEG - Titans双分支架构，结合滑动窗口注意力捕捉短期异常和循环记忆路径总结长期趋势。

Result: 在CHB - MIT数据集上，18个受试者平均片段级敏感度达99.46%；分层上下文策略可显著减少误报且不牺牲敏感度。

Conclusion: 记忆增强的长上下文建模可在临床约束评估下提供可靠的癫痫发作预测。

Abstract: Accurate epileptic seizure prediction from electroencephalography (EEG) remains challenging because pre-ictal dynamics may span long time horizons while clinically relevant signatures can be subtle and transient. Many deep learning models face a persistent trade-off between capturing local spatiotemporal patterns and maintaining informative long-range context when operating on ultralong sequences. We propose EEG-Titans, a dualbranch architecture that incorporates a modern neural memory mechanism for long-context modeling. The model combines sliding-window attention to capture short-term anomalies with a recurrent memory pathway that summarizes slower, progressive trends over time. On the CHB-MIT scalp EEG dataset, evaluated under a chronological holdout protocol, EEG-Titans achieves 99.46% average segment-level sensitivity across 18 subjects. We further analyze safety-first operating points on artifact-prone recordings and show that a hierarchical context strategy extending the receptive field for high-noise subjects can markedly reduce false alarms (down to 0.00 FPR/h in an extreme outlier) without sacrificing sensitivity. These results indicate that memory-augmented long-context modeling can provide robust seizure forecasting under clinically constrained evaluation

</details>


### [362] [vLinear: A Powerful Linear Model for Multivariate Time Series Forecasting](https://arxiv.org/abs/2601.13768)
*Wenzhen Yue,Ruohao Guo,Ji Shi,Zihan Hao,Shiyu Hu,Xianghua Ying*

Main category: cs.LG

TL;DR: 提出vLinear多元时间序列预测器，含vecTrans模块和WFMLoss目标，降低计算复杂度，提升推理速度和性能，在多基准测试中达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有先进预测器依赖自注意力或其变体捕获多元相关性，计算复杂度高，需改进。

Method: 提出vecTrans模块用可学习向量建模多元相关性，降低复杂度；引入WFMLoss目标，采用最终序列导向公式和路径、时间范围加权策略。

Result: vLinear在22个基准和124个预测设置中达SOTA，WFMLoss可有效提升现有预测器性能。

Conclusion: vLinear是高效有效的多元时间序列预测器，WFMLoss是有效的即插即用目标。

Abstract: In this paper, we present \textbf{vLinear}, an effective yet efficient \textbf{linear}-based multivariate time series forecaster featuring two components: the \textbf{v}ecTrans module and the WFMLoss objective. Many state-of-the-art forecasters rely on self-attention or its variants to capture multivariate correlations, typically incurring $\mathcal{O}(N^2)$ computational complexity with respect to the number of variates $N$. To address this, we propose vecTrans, a lightweight module that utilizes a learnable vector to model multivariate correlations, reducing the complexity to $\mathcal{O}(N)$. Notably, vecTrans can be seamlessly integrated into Transformer-based forecasters, delivering up to 5$\times$ inference speedups and consistent performance gains. Furthermore, we introduce WFMLoss (Weighted Flow Matching Loss) as the objective. In contrast to typical \textbf{velocity-oriented} flow matching objectives, we demonstrate that a \textbf{final-series-oriented} formulation yields significantly superior forecasting accuracy. WFMLoss also incorporates path- and horizon-weighted strategies to focus learning on more reliable paths and horizons. Empirically, vLinear achieves state-of-the-art performance across 22 benchmarks and 124 forecasting settings. Moreover, WFMLoss serves as an effective plug-and-play objective, consistently improving existing forecasters. The code is available at https://anonymous.4open.science/r/vLinear.

</details>


### [363] [Principled Latent Diffusion for Graphs via Laplacian Autoencoders](https://arxiv.org/abs/2601.13780)
*Antoine Siraudin,Christopher Morris*

Main category: cs.LG

TL;DR: 提出LG - Flow潜在图扩散框架，克服现有图扩散模型的问题，实现高效图生成并提速。


<details>
  <summary>Details</summary>
Motivation: 现有图扩散模型存在二次复杂度问题，且对稀疏图建模能力浪费，图生成需无损重建但该挑战未解决。

Method: 提出LG - Flow框架，用置换等变自编码器将节点映射到可恢复邻接矩阵的低维嵌入，在该潜在空间用流匹配训练Diffusion Transformer。

Result: 与现有图扩散模型相比取得有竞争力的结果，提速达1000倍。

Conclusion: LG - Flow框架有效克服现有图扩散模型问题，实现高效且有表现力的图生成。

Abstract: Graph diffusion models achieve state-of-the-art performance in graph generation but suffer from quadratic complexity in the number of nodes -- and much of their capacity is wasted modeling the absence of edges in sparse graphs. Inspired by latent diffusion in other modalities, a natural idea is to compress graphs into a low-dimensional latent space and perform diffusion there. However, unlike images or text, graph generation requires nearly lossless reconstruction, as even a single error in decoding an adjacency matrix can render the entire sample invalid. This challenge has remained largely unaddressed. We propose LG-Flow, a latent graph diffusion framework that directly overcomes these obstacles. A permutation-equivariant autoencoder maps each node into a fixed-dimensional embedding from which the full adjacency is provably recoverable, enabling near-lossless reconstruction for both undirected graphs and DAGs. The dimensionality of this latent representation scales linearly with the number of nodes, eliminating the quadratic bottleneck and making it feasible to train larger and more expressive models. In this latent space, we train a Diffusion Transformer with flow matching, enabling efficient and expressive graph generation. Our approach achieves competitive results against state-of-the-art graph diffusion models, while achieving up to $1000\times$ speed-up.

</details>


### [364] [PAtt: A Pattern Attention Network for ETA Prediction Using Historical Speed Profiles](https://arxiv.org/abs/2601.13793)
*ByeoungDo Kim,JunYeop Na,Kyungwook Tak,JunTae Kim,DongHyeon Kim,Duckky Kim*

Main category: cs.LG

TL;DR: 提出基于注意力机制的ETA模型，利用历史道路速度模式，使用真实世界驾驶数据集验证，优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶和智能交通系统日益普及，对准确可靠的ETA估计需求增长，但传统方法有局限，现有深度学习模型存在计算成本高、无法有效捕捉时空模式问题。

Method: 提出ETA模型，利用注意力机制提取和利用路线上每个时空点积累的时间特征。

Result: 使用真实世界驾驶数据集验证，该方法能以任务感知方式有效整合道路特征、实时交通状况和历史速度模式。

Conclusion: 该模型能高效准确进行ETA估计，模型轻量且可扩展，优于现有基线。

Abstract: In this paper, we propose an ETA model (Estimated Time of Arrival) that leverages an attention mechanism over historical road speed patterns. As autonomous driving and intelligent transportation systems become increasingly prevalent, the need for accurate and reliable ETA estimation has grown, playing a vital role in navigation, mobility planning, and traffic management. However, predicting ETA remains a challenging task due to the dynamic and complex nature of traffic flow. Traditional methods often combine real-time and historical traffic data in simplistic ways, or rely on complex rule-based computations. While recent deep learning models have shown potential, they often require high computational costs and do not effectively capture the spatio-temporal patterns crucial for ETA prediction. ETA prediction inherently involves spatio-temporal causality, and our proposed model addresses this by leveraging attention mechanisms to extract and utilize temporal features accumulated at each spatio-temporal point along a route. This architecture enables efficient and accurate ETA estimation while keeping the model lightweight and scalable. We validate our approach using real-world driving datasets and demonstrate that our approach outperforms existing baselines by effectively integrating road characteristics, real-time traffic conditions, and historical speed patterns in a task-aware manner.

</details>


### [365] [ELSA: Efficient LLM-Centric Split Aggregation for Privacy-Aware Hierarchical Federated Learning over Resource-Constrained Edge Networks](https://arxiv.org/abs/2601.13824)
*Xiaohong Yang,Tong Xie,Minghui Liwang,Chikai Shang,Yang Lu,Zhenzhen Jiao,Liqun Fu,Seyyedali Hosseinalipour*

Main category: cs.LG

TL;DR: 提出ELSA框架用于边缘网络分布式大语言模型微调，有三项创新，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决在网络边缘训练大语言模型时面临的设备资源限制、数据异质性和隐私风险等挑战。

Method: 系统集成拆分学习和分层联邦学习，引入任务无关、行为感知的客户端聚类机制，将大语言模型拆分为三部分，采用基于计算草图和语义子空间正交扰动的轻量级通信方案。

Result: 在不同NLP任务实验中，ELSA在适应性、收敛行为和鲁棒性方面始终优于现有方法。

Conclusion: ELSA为资源受限下的边缘侧大语言模型微调提供了可扩展且注重隐私的解决方案。

Abstract: Training large language models (LLMs) at the network edge faces fundamental challenges arising from device resource constraints, severe data heterogeneity, and heightened privacy risks. To address these, we propose ELSA (Efficient LLM-centric Split Aggregation), a novel framework that systematically integrates split learning (SL) and hierarchical federated learning (HFL) for distributed LLM fine-tuning over resource-constrained edge networks. ELSA introduces three key innovations. First, it employs a task-agnostic, behavior-aware client clustering mechanism that constructs semantic fingerprints using public probe inputs and symmetric KL divergence, further enhanced by prediction-consistency-based trust scoring and latency-aware edge assignment to jointly address data heterogeneity, client unreliability, and communication constraints. Second, it splits the LLM into three parts across clients and edge servers, with the cloud used only for adapter aggregation, enabling an effective balance between on-device computation cost and global convergence stability. Third, it incorporates a lightweight communication scheme based on computational sketches combined with semantic subspace orthogonal perturbation (SS-OP) to reduce communication overhead while mitigating privacy leakage during model exchanges. Experiments across diverse NLP tasks demonstrate that ELSA consistently outperforms state-of-the-art methods in terms of adaptability, convergence behavior, and robustness, establishing a scalable and privacy-aware solution for edge-side LLM fine-tuning under resource constraints.

</details>


### [366] [Optimal L2 Regularization in High-dimensional Continual Linear Regression](https://arxiv.org/abs/2601.13844)
*Gilad Karpel,Edward Moroshko,Ran Levinstein,Ron Meir,Daniel Soudry,Itay Evron*

Main category: cs.LG

TL;DR: 研究过参数化连续线性回归设置下泛化，推导高维下泛化损失闭式解，证明各向同性正则可缓解标签噪声，最优正则强度与任务数近似线性关系，实验验证理论。


<details>
  <summary>Details</summary>
Motivation: 研究在过参数化连续线性回归中，使用L2正则在一系列任务上训练模型时的泛化问题。

Method: 理论推导高维下任意线性教师模型的期望泛化损失闭式解，进行线性回归和神经网络实验。

Result: 各向同性正则可缓解单教师和多独立同分布教师设置下的标签噪声，最优固定正则强度与任务数T近似线性尺度关系。

Conclusion: 揭示线性回归设置下正则化与泛化的关系，为持续学习系统设计提供实用方法。

Abstract: We study generalization in an overparameterized continual linear regression setting, where a model is trained with L2 (isotropic) regularization across a sequence of tasks. We derive a closed-form expression for the expected generalization loss in the high-dimensional regime that holds for arbitrary linear teachers. We demonstrate that isotropic regularization mitigates label noise under both single-teacher and multiple i.i.d. teacher settings, whereas prior work accommodating multiple teachers either did not employ regularization or used memory-demanding methods. Furthermore, we prove that the optimal fixed regularization strength scales nearly linearly with the number of tasks $T$, specifically as $T/\ln T$. To our knowledge, this is the first such result in theoretical continual learning. Finally, we validate our theoretical findings through experiments on linear regression and neural networks, illustrating how this scaling law affects generalization and offering a practical recipe for the design of continual learning systems.

</details>


### [367] [Multi-Objective Hierarchical Optimization with Large Language Models](https://arxiv.org/abs/2601.13892)
*Andrej Schwanke,Lyubomir Ivanov,David Salinas,Frank Hutter,Arber Zela*

Main category: cs.LG

TL;DR: 本文提出在结构化分层搜索策略中利用大语言模型（LLM）作为代理模型和候选采样器进行多目标优化，算法收敛且表现良好。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽有强大推理能力，但还不是多目标优化的现成选择，传统策略在基准测试中表现更好，需缩小差距。

Method: 在结构化分层搜索策略中，将LLM作为代理模型和候选采样器，自适应划分输入空间为不相交超矩形区域，用复合得分函数排序，限制LLM生成过程到高潜力子空间。

Result: 算法生成的候选解在豪斯多夫距离上收敛到真实帕累托集，在合成和现实基准测试中，始终优于基于全局LLM的多目标优化器，与标准进化和贝叶斯优化算法表现相当。

Conclusion: 所提算法有效解决了LLM在多目标优化中的应用问题，能实现较好的优化效果。

Abstract: Despite their widespread adoption in various domains, especially due to their powerful reasoning capabilities, Large Language Models (LLMs) are not the off-the-shelf choice to drive multi-objective optimization yet. Conventional strategies rank high in benchmarks due to their intrinsic capabilities to handle numerical inputs and careful modelling choices that balance exploration and Pareto-front exploitation, as well as handle multiple (conflicting) objectives. In this paper, we close this gap by leveraging LLMs as surrogate models and candidate samplers inside a structured hierarchical search strategy. By adaptively partitioning the input space into disjoint hyperrectangular regions and ranking them with a composite score function, we restrict the generative process of the LLM to specific, high-potential sub-spaces, hence making the problem easier to solve as the LLM doesn't have to reason about the global structure of the problem, but only locally instead. We show that under standard regularity assumptions, our algorithm generates candidate solutions that converge to the true Pareto set in Hausdorff distance. Empirically, it consistently outperforms the global LLM-based multi-objective optimizer and is on par with standard evolutionary and Bayesian optimization algorithm on synthetic and real-world benchmarks.

</details>


### [368] [TractRLFusion: A GPT-Based Multi-Critic Policy Fusion Framework for Fiber Tractography](https://arxiv.org/abs/2601.13897)
*Ankita Joshi,Ashutosh Sharma,Anoushkrit Goel,Ranjeet Ranjan Jha,Chirag Ahuja,Arnav Bhavsar,Aditya Nigam*

Main category: cs.LG

TL;DR: 提出基于GPT的TractRLFusion框架改善白质纤维束重建，在多数据集上表现佳。


<details>
  <summary>Details</summary>
Motivation: 传统白质纤维束成像方法有局限，准确重建白质束并减少伪连接是挑战，需新方法。

Method: 提出TractRLFusion框架，采用两阶段训练数据选择和多批评家微调。

Result: 在HCP、ISMRM和TractoInferno数据集实验中，TractRLFusion在准确性和解剖可靠性上超现有方法。

Conclusion: TractRLFusion是有效改善白质纤维束重建的方案。

Abstract: Tractography plays a pivotal role in the non-invasive reconstruction of white matter fiber pathways, providing vital information on brain connectivity and supporting precise neurosurgical planning. Although traditional methods relied mainly on classical deterministic and probabilistic approaches, recent progress has benefited from supervised deep learning (DL) and deep reinforcement learning (DRL) to improve tract reconstruction. A persistent challenge in tractography is accurately reconstructing white matter tracts while minimizing spurious connections. To address this, we propose TractRLFusion, a novel GPT-based policy fusion framework that integrates multiple RL policies through a data-driven fusion strategy. Our method employs a two-stage training data selection process for effective policy fusion, followed by a multi-critic fine-tuning phase to enhance robustness and generalization. Experiments on HCP, ISMRM, and TractoInferno datasets demonstrate that TractRLFusion outperforms individual RL policies as well as state-of-the-art classical and DRL methods in accuracy and anatomical reliability.

</details>


### [369] [Differentiable Logic Synthesis: Spectral Coefficient Selection via Sinkhorn-Constrained Composition](https://arxiv.org/abs/2601.13953)
*Gorgi Pavlov*

Main category: cs.LG

TL;DR: 提出Hierarchical Spectral Composition架构学习精确布尔逻辑，在不同复杂度任务验证，证明该方法对硬件高效神经符号逻辑合成可行。


<details>
  <summary>Details</summary>
Motivation: 通过梯度下降学习精确布尔逻辑有挑战，神经网络通常收敛到‘模糊’近似，在量化下性能下降。

Method: 引入Hierarchical Spectral Composition架构，结合Sinkhorn约束路由和列符号调制，借鉴Manifold - Constrained Hyper - Connections框架并加入列符号调制实现布尔取反。

Result: 在不同复杂度任务中验证，如n = 2时梯度下降达100%准确率等，所有操作在GPU上有高运算速度。

Conclusion: 三元多项式阈值表示对测试函数存在，高维度时需纯梯度下降外的方法，该方法对硬件高效神经符号逻辑合成可行。

Abstract: Learning precise Boolean logic via gradient descent remains challenging: neural networks typically converge to "fuzzy" approximations that degrade under quantization. We introduce Hierarchical Spectral Composition, a differentiable architecture that selects spectral coefficients from a frozen Boolean Fourier basis and composes them via Sinkhorn-constrained routing with column-sign modulation. Our approach draws on recent insights from Manifold-Constrained Hyper-Connections (mHC), which demonstrated that projecting routing matrices onto the Birkhoff polytope preserves identity mappings and stabilizes large-scale training. We adapt this framework to logic synthesis, adding column-sign modulation to enable Boolean negation -- a capability absent in standard doubly stochastic routing.
  We validate our approach across four phases of increasing complexity: (1) For n=2 (16 Boolean operations over 4-dim basis), gradient descent achieves 100% accuracy with zero routing drift and zero-loss quantization to ternary masks. (2) For n=3 (10 three-variable operations), gradient descent achieves 76% accuracy, but exhaustive enumeration over 3^8 = 6561 configurations proves that optimal ternary masks exist for all operations (100% accuracy, 39% sparsity). (3) For n=4 (10 four-variable operations over 16-dim basis), spectral synthesis -- combining exact Walsh-Hadamard coefficients, ternary quantization, and MCMC refinement with parallel tempering -- achieves 100% accuracy on all operations. This progression establishes (a) that ternary polynomial threshold representations exist for all tested functions, and (b) that finding them requires methods beyond pure gradient descent as dimensionality grows. All operations enable single-cycle combinational logic inference at 10,959 MOps/s on GPU, demonstrating viability for hardware-efficient neuro-symbolic logic synthesis.

</details>


### [370] [RL-BioAug: Label-Efficient Reinforcement Learning for Self-Supervised EEG Representation Learning](https://arxiv.org/abs/2601.13964)
*Cheol-Hui Lee,Hwa-Yeon Lee,Dong-Joo Kim*

Main category: cs.LG

TL;DR: 提出RL - BioAug框架，用少量标注数据引导强化学习智能体确定最优增强策略，实验显示该框架在睡眠和癫痫检测数据集上表现优于随机策略，有潜力替代传统数据增强方法。


<details>
  <summary>Details</summary>
Motivation: 现有静态或随机数据增强策略因脑电信号非平稳性，难以保留内在信息，影响对比学习性能。

Method: 提出RL - BioAug框架，利用标签高效的强化学习智能体自主确定最优增强策略，仅用10%标注数据引导。

Result: 在Sleep - EDFX和CHB - MIT数据集上Macro - F1分数分别提升9.69%和8.80%，智能体为不同任务选择最优策略。

Conclusion: 该框架有潜力取代传统启发式数据增强方法，建立新的自主数据增强范式。

Abstract: The quality of data augmentation serves as a critical determinant for the performance of contrastive learning in EEG tasks. Although this paradigm is promising for utilizing unlabeled data, static or random augmentation strategies often fail to preserve intrinsic information due to the non-stationarity of EEG signals where statistical properties change over time. To address this, we propose RL-BioAug, a framework that leverages a label-efficient reinforcement learning (RL) agent to autonomously determine optimal augmentation policies. While utilizing only a minimal fraction (10\%) of labeled data to guide the agent's policy, our method enables the encoder to learn robust representations in a strictly self-supervised manner. Experimental results demonstrate that RL-BioAug significantly outperforms the random selection strategy, achieving substantial improvements of 9.69\% and 8.80\% in Macro-F1 score on the Sleep-EDFX and CHB-MIT datasets, respectively. Notably, this agent mainly chose optimal strategies for each task -- for example, Time Masking with a 62\% probability for sleep stage classification and Crop \& Resize with a 77\% probability for seizure detection. Our framework suggests its potential to replace conventional heuristic-based augmentations and establish a new autonomous paradigm for data augmentation. The source code is available at \href{https://github.com/dlcjfgmlnasa/RL-BioAug}{https://github.com/dlcjfgmlnasa/RL-BioAug}.

</details>


### [371] [A universal linearized subspace refinement framework for neural networks](https://arxiv.org/abs/2601.13989)
*Wenbo Cao,Weiwei Zhang*

Main category: cs.LG

TL;DR: 提出Linearized Subspace Refinement (LSR)框架，可在不改变网络架构等情况下提升神经网络预测精度，还引入Iterative LSR处理特定问题，提供了广泛适用的细化框架。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度的方法训练神经网络，最终预测精度远未达到模型表达能力上限。

Method: 提出LSR框架，利用固定训练网络状态下的雅可比诱导线性残差模型，在子空间求解简化的直接最小二乘问题；针对复合损失结构的算子约束问题引入Iterative LSR。

Result: LSR能显著提高预测精度，常实现数量级的误差降低；Iterative LSR可加速收敛并提高精度。

Conclusion: LSR在监督学习、算子学习和科学计算中提供了一个基于数值且广泛适用的细化框架。

Abstract: Neural networks are predominantly trained using gradient-based methods, yet in many applications their final predictions remain far from the accuracy attainable within the model's expressive capacity. We introduce Linearized Subspace Refinement (LSR), a general and architecture-agnostic framework that exploits the Jacobian-induced linear residual model at a fixed trained network state. By solving a reduced direct least-squares problem within this subspace, LSR computes a subspace-optimal solution of the linearized residual model, yielding a refined linear predictor with substantially improved accuracy over standard gradient-trained solutions, without modifying network architectures, loss formulations, or training procedures. Across supervised function approximation, data-driven operator learning, and physics-informed operator fine-tuning, we show that gradient-based training often fails to access this attainable accuracy, even when local linearization yields a convex problem. This observation indicates that loss-induced numerical ill-conditioning, rather than nonconvexity or model expressivity, can constitute a dominant practical bottleneck. In contrast, one-shot LSR systematically exposes accuracy levels not fully exploited by gradient-based training, frequently achieving order-of-magnitude error reductions. For operator-constrained problems with composite loss structures, we further introduce Iterative LSR, which alternates one-shot LSR with supervised nonlinear alignment, transforming ill-conditioned residual minimization into numerically benign fitting steps and yielding accelerated convergence and improved accuracy. By bridging nonlinear neural representations with reduced-order linear solvers at fixed linearization points, LSR provides a numerically grounded and broadly applicable refinement framework for supervised learning, operator learning, and scientific computing.

</details>


### [372] [Credible CO2 Comparisons: A Machine Learning Approach to Vehicle Powertrain Assessment](https://arxiv.org/abs/2601.14022)
*Rodrigo Pereira David,Luciano Araujo Dourado Filho,Daniel Marques da Silva,João Alfredo Cal-Braz*

Main category: cs.LG

TL;DR: 本文提出基于机器学习框架，在相同真实驾驶条件下对比内燃机汽车和电动汽车的CO2排放，实现动力总成技术公平评估。


<details>
  <summary>Details</summary>
Motivation: 道路运输脱碳需要一致透明的方法对比不同车辆技术的CO2排放。

Method: 提出机器学习框架，固定速度和环境条件，独立训练循环神经网络模型学习变量映射，构建反事实场景。

Result: 构建框架可统一瞬时排放指标，公平可重复评估动力总成技术。

Conclusion: 该框架为真实工况下车辆碳性能的数据驱动评估提供可扩展基础。

Abstract: Decarbonizing road transport requires consistent and transparent methods for comparing CO2 emissions across vehicle technologies. This paper proposes a machine learning-based framework for like-for-like operational assessment of internal combustion engine vehicles (ICEVs) and electric vehicles (EVs) under identical, real-world driving conditions. The approach isolates technology-specific effects by holding the observed speed profile and environmental context fixed, enabling direct comparison of powertrain performance. Recurrent neural network models are trained independently for each domain to learn the mapping from contextual driving variables (speed, acceleration, temperature) to internal actuation variables (torque, throttle) and instantaneous CO2-equivalent emission rates. This structure allows the construction of counterfactual scenarios that answer: What emissions would an EV have generated if it had followed the same driving profile as an ICEV? By aligning both vehicle types on a unified instantaneous emissions metric, the framework enables fair and reproducible evaluation of powertrain technologies. It offers a scalable foundation for credible, data-driven assessments of vehicle carbon performance under real-world operating conditions.

</details>


### [373] [PAC-Private Responses with Adversarial Composition](https://arxiv.org/abs/2601.14033)
*Xiaochen Zhu,Mayuri Sridhar,Srinivas Devadas*

Main category: cs.LG

TL;DR: 本文提出新算法实现对抗组合，在多任务实验中以小隐私预算获高效用，还能用私有响应蒸馏模型。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习模型多通过API部署，标准权重私有化方法噪声大、效用低，因此要直接在模型输出上实施隐私保护。

Method: 在PAC隐私框架下，通过自适应噪声校准实现对抗组合的新算法，证明互信息保证在自适应和对抗查询下线性累积。

Result: 在多任务实验中，以极小每查询隐私预算获高效用，如CIFAR - 10准确率达87.79%；能用私有响应蒸馏模型，蒸馏模型在CIFAR - 10上准确率达91.86%。

Conclusion: 新算法能在低隐私预算下实现高模型效用，并且私有响应可用于蒸馏隐私保护模型。

Abstract: Modern machine learning models are increasingly deployed behind APIs. This renders standard weight-privatization methods (e.g. DP-SGD) unnecessarily noisy at the cost of utility. While model weights may vary significantly across training datasets, model responses to specific inputs are much lower dimensional and more stable. This motivates enforcing privacy guarantees directly on model outputs.
  We approach this under PAC privacy, which provides instance-based privacy guarantees for arbitrary black-box functions by controlling mutual information (MI). Importantly, PAC privacy explicitly rewards output stability with reduced noise levels. However, a central challenge remains: response privacy requires composing a large number of adaptively chosen, potentially adversarial queries issued by untrusted users, where existing composition results on PAC privacy are inadequate. We introduce a new algorithm that achieves adversarial composition via adaptive noise calibration and prove that mutual information guarantees accumulate linearly under adaptive and adversarial querying.
  Experiments across tabular, vision, and NLP tasks show that our method achieves high utility at extremely small per-query privacy budgets. On CIFAR-10, we achieve 87.79% accuracy with a per-step MI budget of $2^{-32}$. This enables serving one million queries while provably bounding membership inference attack (MIA) success rates to 51.08% -- the same guarantee of $(0.04, 10^{-5})$-DP. Furthermore, we show that private responses can be used to label public data to distill a publishable privacy-preserving model; using an ImageNet subset as a public dataset, our model distilled from 210,000 responses achieves 91.86% accuracy on CIFAR-10 with MIA success upper-bounded by 50.49%, which is comparable to $(0.02,10^{-5})$-DP.

</details>


### [374] [LLMOrbit: A Circular Taxonomy of Large Language Models -From Scaling Walls to Agentic AI Systems](https://arxiv.org/abs/2601.14053)
*Badri N. Patro,Vijay S. Agneeswaran*

Main category: cs.LG

TL;DR: 介绍LLMOrbit分类法，研究超50个模型，指出大语言模型面临的三个危机，揭示六种破局范式和三个范式转变，并提供技术见解。


<details>
  <summary>Details</summary>
Motivation: 梳理2019 - 2025年大语言模型领域情况，应对当前模型发展面临的问题。

Method: 通过LLMOrbit分类法的八个维度，研究15个组织的超50个模型。

Result: 识别出数据稀缺、成本增长、能耗高三个危机；发现六种破局范式和三个范式转变。

Conclusion: 大语言模型发展面临挑战，但有多种技术和范式转变可突破局限，且向更高效、民主方向发展。

Abstract: The field of artificial intelligence has undergone a revolution from foundational Transformer architectures to reasoning-capable systems approaching human-level performance. We present LLMOrbit, a comprehensive circular taxonomy navigating the landscape of large language models spanning 2019-2025. This survey examines over 50 models across 15 organizations through eight interconnected orbital dimensions, documenting architectural innovations, training methodologies, and efficiency patterns defining modern LLMs, generative AI, and agentic systems. We identify three critical crises: (1) data scarcity (9-27T tokens depleted by 2026-2028), (2) exponential cost growth ($3M to $300M+ in 5 years), and (3) unsustainable energy consumption (22x increase), establishing the scaling wall limiting brute-force approaches. Our analysis reveals six paradigms breaking this wall: (1) test-time compute (o1, DeepSeek-R1 achieve GPT-4 performance with 10x inference compute), (2) quantization (4-8x compression), (3) distributed edge computing (10x cost reduction), (4) model merging, (5) efficient training (ORPO reduces memory 50%), and (6) small specialized models (Phi-4 14B matches larger models). Three paradigm shifts emerge: (1) post-training gains (RLHF, GRPO, pure RL contribute substantially, DeepSeek-R1 achieving 79.8% MATH), (2) efficiency revolution (MoE routing 18x efficiency, Multi-head Latent Attention 8x KV cache compression enables GPT-4-level performance at <$0.30/M tokens), and (3) democratization (open-source Llama 3 88.6% MMLU surpasses GPT-4 86.4%). We provide insights into techniques (RLHF, PPO, DPO, GRPO, ORPO), trace evolution from passive generation to tool-using agents (ReAct, RAG, multi-agent systems), and analyze post-training innovations.

</details>


### [375] [Optimizing Energy and Data Collection in UAV-aided IoT Networks using Attention-based Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2601.14092)
*Babacar Toure,Dimitrios Tsilimantos,Omid Esrafilian,Marios Kountouris*

Main category: cs.LG

TL;DR: 提出基于注意力的多目标强化学习架构解决无人机路径规划问题，性能优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 现有无人机路径规划算法训练数据有限，忽视任务多目标特性，在动态环境中性能不佳。

Method: 提出基于注意力的多目标强化学习架构，处理数据收集和能耗间的权衡，且无需无线信道先验知识。

Result: 大量模拟显示该方法在性能、模型紧凑性、样本效率和泛化能力上有显著提升，优于现有强化学习方案。

Conclusion: 所提方法能有效解决现有无人机路径规划算法的局限，表现良好。

Abstract: Due to their adaptability and mobility, Unmanned Aerial Vehicles (UAVs) are becoming increasingly essential for wireless network services, particularly for data harvesting tasks. In this context, Artificial Intelligence (AI)-based approaches have gained significant attention for addressing UAV path planning tasks in large and complex environments, bridging the gap with real-world deployments. However, many existing algorithms suffer from limited training data, which hampers their performance in highly dynamic environments. Moreover, they often overlook the inherently multi-objective nature of the task, treating it in an overly simplistic manner. To address these limitations, we propose an attention-based Multi-Objective Reinforcement Learning (MORL) architecture that explicitly handles the trade-off between data collection and energy consumption in urban environments, even without prior knowledge of wireless channel conditions. Our method develops a single model capable of adapting to varying trade-off preferences and dynamic scenario parameters without the need for fine-tuning or retraining. Extensive simulations show that our approach achieves substantial improvements in performance, model compactness, sample efficiency, and most importantly, generalization to previously unseen scenarios, outperforming existing RL solutions.

</details>


### [376] [Causal feature selection framework for stable soft sensor modeling based on time-delayed cross mapping](https://arxiv.org/abs/2601.14099)
*Shi-Shun Chen,Xiao-Yang Li,Enrico Zio*

Main category: cs.LG

TL;DR: 论文针对现有因果特征选择方法不适用于工业过程的问题，提出基于时间延迟交叉映射的因果特征选择框架，经案例验证有效且代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有因果特征选择方法忽略工业过程变量因果关系存在时间延迟及变量相互依赖的特性，导致软传感器模型缺乏准确性和稳定性。

Method: 提出基于时间延迟交叉映射的因果特征选择框架，引入TDCCM进行总因果推断，TDPCM进行直接因果推断，并提出基于验证集模型性能自动确定因果阈值的特征选择策略。

Result: 两个实际案例表明TDCCM平均性能最高，TDPCM在最坏情况下改善了软传感器稳定性和性能。

Conclusion: 所提基于时间延迟交叉映射的因果特征选择框架有效，能提升软传感器模型性能。

Abstract: Soft sensor modeling plays a crucial role in process monitoring. Causal feature selection can enhance the performance of soft sensor models in industrial applications. However, existing methods ignore two critical characteristics of industrial processes. Firstly, causal relationships between variables always involve time delays, whereas most causal feature selection methods investigate causal relationships in the same time dimension. Secondly, variables in industrial processes are often interdependent, which contradicts the decorrelation assumption of traditional causal inference methods. Consequently, soft sensor models based on existing causal feature selection approaches often lack sufficient accuracy and stability. To overcome these challenges, this paper proposes a causal feature selection framework based on time-delayed cross mapping. Time-delayed cross mapping employs state space reconstruction to effectively handle interdependent variables in causality analysis, and considers varying causal strength across time delay. Time-delayed convergent cross mapping (TDCCM) is introduced for total causal inference, and time-delayed partial cross mapping (TDPCM) is developed for direct causal inference. Then, in order to achieve automatic feature selection, an objective feature selection strategy is presented. The causal threshold is automatically determined based on the model performance on the validation set, and the causal features are then selected. Two real-world case studies show that TDCCM achieves the highest average performance, while TDPCM improves soft sensor stability and performance in the worst scenario. The code is publicly available at https://github.com/dirge1/TDPCM.

</details>


### [377] [Riemannian Liquid Spatio-Temporal Graph Network](https://arxiv.org/abs/2601.14115)
*Liangsi Lu,Jingchao Wang,Zhaorong Dai,Hanqian Liu,Yang Shi*

Main category: cs.LG

TL;DR: 本文提出Riemannian Liquid Spatio - Temporal Graph Network (RLSTG)框架，结合连续时间液态动力学和黎曼流形的几何归纳偏置，实验显示其在复杂结构图上性能优越。


<details>
  <summary>Details</summary>
Motivation: Liquid Time - Constant networks (LTCs)局限于欧几里得空间，处理非欧几里得结构的真实世界图时会产生几何失真，降低表示质量。

Method: 提出RLSTG框架，通过在弯曲流形上直接建立的常微分方程（ODE）对图的演化进行建模，为RLSTG提供理论保证，将LTCs的稳定性定理扩展到黎曼域，并通过状态轨迹分析量化其表达能力。

Result: 在真实世界基准测试中，将先进的时间动态与黎曼空间表示相结合的RLSTG在复杂结构图上表现更优。

Conclusion: RLSTG框架能够统一连续时间液态动力学和黎曼流形的几何归纳偏置，有效处理具有复杂结构的图。

Abstract: Liquid Time-Constant networks (LTCs), a type of continuous-time graph neural network, excel at modeling irregularly-sampled dynamics but are fundamentally confined to Euclidean space. This limitation introduces significant geometric distortion when representing real-world graphs with inherent non-Euclidean structures (e.g., hierarchies and cycles), degrading representation quality. To overcome this limitation, we introduce the Riemannian Liquid Spatio-Temporal Graph Network (RLSTG), a framework that unifies continuous-time liquid dynamics with the geometric inductive biases of Riemannian manifolds. RLSTG models graph evolution through an Ordinary Differential Equation (ODE) formulated directly on a curved manifold, enabling it to faithfully capture the intrinsic geometry of both structurally static and dynamic spatio-temporal graphs. Moreover, we provide rigorous theoretical guarantees for RLSTG, extending stability theorems of LTCs to the Riemannian domain and quantifying its expressive power via state trajectory analysis. Extensive experiments on real-world benchmarks demonstrate that, by combining advanced temporal dynamics with a Riemannian spatial representation, RLSTG achieves superior performance on graphs with complex structures. Project Page: https://rlstg.github.io

</details>


### [378] [A model of errors in transformers](https://arxiv.org/abs/2601.14175)
*Suvrat Raju,Praneeth Netrapalli*

Main category: cs.LG

TL;DR: 研究大语言模型在算术等需确定性输出任务上的错误率，推导准确率与任务复杂度的关系，经实证测试验证，还给出降低错误率的提示构造方法。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在需确定性输出和重复处理少量可选标记任务上的错误率成因。

Method: 从有效场论视角出发，将模型众多原始参数重组为两个控制错误率的参数，推导准确率与任务复杂度关系，并进行大量实证测试。

Result: 对多种任务，预测准确率与观察准确率吻合良好，但部分情况有偏差。

Conclusion: 模型为解释大语言模型在长重复任务上的错误提供了新视角，还可构造提示降低错误率。

Abstract: We study the error rate of LLMs on tasks like arithmetic that require a deterministic output, and repetitive processing of tokens drawn from a small set of alternatives. We argue that incorrect predictions arise when small errors in the attention mechanism accumulate to cross a threshold, and use this insight to derive a quantitative two-parameter relationship between the accuracy and the complexity of the task. The two parameters vary with the prompt and the model; they can be interpreted in terms of an elementary noise rate, and the number of plausible erroneous tokens that can be predicted. Our analysis is inspired by an ``effective field theory'' perspective: the LLM's many raw parameters can be reorganized into just two parameters that govern the error rate. We perform extensive empirical tests, using Gemini 2.5 Flash, Gemini 2.5 Pro and DeepSeek R1, and find excellent agreement between the predicted and observed accuracy for a variety of tasks, although we also identify deviations in some cases. Our model provides an alternative to suggestions that errors made by LLMs on long repetitive tasks indicate the ``collapse of reasoning'', or an inability to express ``compositional'' functions. Finally, we show how to construct prompts to reduce the error rate.

</details>


### [379] [Differentiated Pickup Point Offering for Emission Reduction in Last-Mile Delivery](https://arxiv.org/abs/2601.14196)
*Albina Galiullina,Wouter van Heeswijk,Tom van Woensel*

Main category: cs.LG

TL;DR: 研究提出差异化自提点提供（DPO）政策以减少配送和客户取货碳排放，计算实验表明该政策可显著降低碳排放。


<details>
  <summary>Details</summary>
Motivation: 自提点虽可缩短配送路线，但客户驾车取货可能抵消其优势，需减少配送和客户出行排放。

Method: 采用基于强化学习的方法，考虑客户与自提点的空间关系及对未来路线整合的影响，在动态随机环境下设计DPO政策。

Result: DPO政策可大幅减少碳排放，相较于仅送货上门最多减少9%，较其他政策平均减少2%，在城市密集区域和客户倾向送货上门时更有效。

Conclusion: 差异化自提点提供政策能有效减少碳排放，设计时考虑客户到达和选择的动态性很关键。

Abstract: Pickup points are widely recognized as a sustainable alternative to home delivery, as consolidating orders at pickup locations can shorten delivery routes and improve first-attempt success rates. However, these benefits may be negated when customers drive to pick up their orders. This study proposes a Differentiated Pickup Point Offering (DPO) policy that aims to jointly reduce emissions from delivery truck routes and customer travel. Under DPO, each arriving customer is offered a single recommended pickup point, rather than an unrestricted choice among all locations, while retaining the option of home delivery. We study this problem in a dynamic and stochastic setting, where the pickup point offered to each customer depends on previously realized customer locations and delivery choices. To design effective DPO policies, we adopt a reinforcement learning-based approach that accounts for spatial relationships between customers and pickup points and their implications for future route consolidation. Computational experiments show that differentiated pickup point offerings can substantially reduce total carbon emissions. The proposed policies reduce total emissions by up to 9% relative to home-only delivery and by 2% on average compared with alternative policies, including unrestricted pickup point choice and nearest pickup point assignment. Differentiated offerings are particularly effective in dense urban settings with many pickup points and short inter-location distances. Moreover, explicitly accounting for the dynamic nature of customer arrivals and choices is especially important when customers are less inclined to choose pickup point delivery over home delivery.

</details>


### [380] [InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning](https://arxiv.org/abs/2601.14209)
*Matthew Y. R. Yang,Hao Bai,Ian Wu,Gene Yang,Amrith Setlur,Aviral Kumar*

Main category: cs.LG

TL;DR: 文章指出结果奖励强化学习存在信用分配问题，引入干预训练（InT）范式，该范式能让模型对推理轨迹进行细粒度信用分配，以微调后的模型作为强化学习训练的初始化，可大幅提升数学推理准确率。


<details>
  <summary>Details</summary>
Motivation: 结果奖励强化学习在大语言模型推理能力提升中，存在最终答案层面进行信用分配的问题，即无法有效区分中间步骤的正确性，导致正确中间步骤被抑制、错误步骤被强化。同时，训练过程奖励模型来准确识别正确推理步骤也颇具挑战。

Method: 提出干预训练（InT）范式，利用数学推理数据集中的参考解，让模型识别推理中的首个错误，并提出单步干预；再对错误点之前的策略展开与干预内容进行拼接，进行监督微调；最后将微调后的模型作为强化学习训练的初始化。

Result: 经过InT训练和后续强化学习微调，在IMO - AnswerBench上，相比4B参数的基础模型，准确率提高了近14%，超越了如gpt - oss - 20b等更大的开源模型。

Conclusion: 干预训练（InT）范式能够解决结果奖励强化学习中的信用分配问题，通过细粒度的信用分配和针对性的错误修正，有效提升数学推理任务的准确率。

Abstract: Outcome-reward reinforcement learning (RL) has proven effective at improving the reasoning capabilities of large language models (LLMs). However, standard RL assigns credit only at the level of the final answer, penalizing entire reasoning traces when the outcome is incorrect and uniformly reinforcing all steps when it is correct. As a result, correct intermediate steps may be discouraged in failed traces, while spurious steps may be reinforced in successful ones. We refer to this failure mode as the problem of credit assignment. While a natural remedy is to train a process reward model, accurately optimizing such models to identify corrective reasoning steps remains challenging. We introduce Intervention Training (InT), a training paradigm in which the model performs fine-grained credit assignment on its own reasoning traces by proposing short, targeted corrections that steer trajectories toward higher reward. Using reference solutions commonly available in mathematical reasoning datasets and exploiting the fact that verifying a model-generated solution is easier than generating a correct one from scratch, the model identifies the first error in its reasoning and proposes a single-step intervention to redirect the trajectory toward the correct solution. We then apply supervised fine-tuning (SFT) to the on-policy rollout up to the point of error concatenated with the intervention, localizing error to the specific step that caused failure. We show that the resulting model serves as a far better initialization for RL training. After running InT and subsequent fine-tuning with RL, we improve accuracy by nearly 14% over a 4B-parameter base model on IMO-AnswerBench, outperforming larger open-source models such as gpt-oss-20b.

</details>


### [381] [Attention-Based Offline Reinforcement Learning and Clustering for Interpretable Sepsis Treatment](https://arxiv.org/abs/2601.14228)
*Punit Kumar,Vaibhav Saran,Divyesh Patel,Nitin Kulkarni,Alina Vereshchaka*

Main category: cs.LG

TL;DR: 提出可解释决策支持框架用于脓毒症治疗决策，经评估有高治疗准确率并能提供可解释策略建议。


<details>
  <summary>Details</summary>
Motivation: 脓毒症是重症监护室主要死因之一，及时准确治疗决策影响患者预后，需有效决策支持。

Method: 框架包含聚类分层模块、合成数据增强管道、离线强化学习代理、理由生成模块。

Result: 在MIMIC - III和eICU数据集上评估，实现高治疗准确率。

Conclusion: 该框架能为临床医生提供可解释且稳健的治疗策略建议。

Abstract: Sepsis remains one of the leading causes of mortality in intensive care units, where timely and accurate treatment decisions can significantly impact patient outcomes. In this work, we propose an interpretable decision support framework. Our system integrates four core components: (1) a clustering-based stratification module that categorizes patients into low, intermediate, and high-risk groups upon ICU admission, using clustering with statistical validation; (2) a synthetic data augmentation pipeline leveraging variational autoencoders (VAE) and diffusion models to enrich underrepresented trajectories such as fluid or vasopressor administration; (3) an offline reinforcement learning (RL) agent trained using Advantage Weighted Regression (AWR) with a lightweight attention encoder and supported by an ensemble models for conservative, safety-aware treatment recommendations; and (4) a rationale generation module powered by a multi-modal large language model (LLM), which produces natural-language justifications grounded in clinical context and retrieved expert knowledge. Evaluated on the MIMIC-III and eICU datasets, our approach achieves high treatment accuracy while providing clinicians with interpretable and robust policy recommendations.

</details>


### [382] [KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning](https://arxiv.org/abs/2601.14232)
*Egor Cherepanov,Daniil Zelezetsky,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: 提出KAGE - Env平台和KAGE - Bench基准，研究像素强化学习代理在视觉分布移位下的表现，发现轴依赖失败现象，且JAX实现支持快速实验。


<details>
  <summary>Details</summary>
Motivation: 现有基准混淆多种移位源，阻碍像素强化学习代理在纯视觉分布移位下的系统分析。

Method: 引入KAGE - Env平台将观察过程分解为可独立控制的视觉轴，基于此定义KAGE - Bench基准。

Result: 使用PPO - CNN基线，观察到强轴依赖失败，不同视觉移位影响不同，部分移位破坏任务完成但保留前进运动。

Conclusion: KAGE - Env和KAGE - Bench可用于研究视觉泛化，JAX实现能实现快速可复现的视觉因素实验。

Abstract: Pixel-based reinforcement learning agents often fail under purely visual distribution shift even when latent dynamics and rewards are unchanged, but existing benchmarks entangle multiple sources of shift and hinder systematic analysis. We introduce KAGE-Env, a JAX-native 2D platformer that factorizes the observation process into independently controllable visual axes while keeping the underlying control problem fixed. By construction, varying a visual axis affects performance only through the induced state-conditional action distribution of a pixel policy, providing a clean abstraction for visual generalization. Building on this environment, we define KAGE-Bench, a benchmark of six known-axis suites comprising 34 train-evaluation configuration pairs that isolate individual visual shifts. Using a standard PPO-CNN baseline, we observe strong axis-dependent failures, with background and photometric shifts often collapsing success, while agent-appearance shifts are comparatively benign. Several shifts preserve forward motion while breaking task completion, showing that return alone can obscure generalization failures. Finally, the fully vectorized JAX implementation enables up to 33M environment steps per second on a single GPU, enabling fast and reproducible sweeps over visual factors. Code: https://avanturist322.github.io/KAGEBench/.

</details>


### [383] [Spatiotemporal Wildfire Prediction and Reinforcement Learning for Helitack Suppression](https://arxiv.org/abs/2601.14238)
*Shaurya Mathur,Shreyas Bellary Manjunath,Nitin Kulkarni,Alina Vereshchaka*

Main category: cs.LG

TL;DR: 介绍了FireCastRL主动式AI框架，结合野火预测与智能灭火策略，还发布数据集，展示深度学习和强化学习结合支持野火应对。


<details>
  <summary>Details</summary>
Motivation: 美国野火频率和强度增加，传统野火管理多为被动响应，需要主动式管理框架。

Method: 使用深度时空模型预测野火起火点，对高风险预测部署预训练强化学习智能体在物理信息3D模拟中执行实时灭火策略，生成威胁评估报告。

Result: 开发出FireCastRL框架，并公开包含950万个环境变量样本的大规模时空数据集。

Conclusion: 证明深度学习和强化学习结合可支持野火预测和战术响应。

Abstract: Wildfires are growing in frequency and intensity, devastating ecosystems and communities while causing billions of dollars in suppression costs and economic damage annually in the U.S. Traditional wildfire management is mostly reactive, addressing fires only after they are detected. We introduce \textit{FireCastRL}, a proactive artificial intelligence (AI) framework that combines wildfire forecasting with intelligent suppression strategies. Our framework first uses a deep spatiotemporal model to predict wildfire ignition. For high-risk predictions, we deploy a pre-trained reinforcement learning (RL) agent to execute real-time suppression tactics with helitack units inside a physics-informed 3D simulation. The framework generates a threat assessment report to help emergency responders optimize resource allocation and planning. In addition, we are publicly releasing a large-scale, spatiotemporal dataset containing $\mathbf{9.5}$ million samples of environmental variables for wildfire prediction. Our work demonstrates how deep learning and RL can be combined to support both forecasting and tactical wildfire response. More details can be found at https://sites.google.com/view/firecastrl.

</details>


### [384] [Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow](https://arxiv.org/abs/2601.14243)
*Haocheng Xi,Charlie Ruan,Peiyuan Liao,Yujun Lin,Han Cai,Yilong Zhao,Shuo Yang,Kurt Keutzer,Song Han,Ligeng Zhu*

Main category: cs.LG

TL;DR: 现有强化学习训练管道效率低，本文指出常用策略有问题，提出Jet - RL框架，实验证明其有效且能提速。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习训练管道计算效率低、资源消耗大，常用的BF16训练+FP8展开策略存在训练不稳定和精度崩溃问题。

Method: 提出Jet - RL框架，采用统一的FP8精度流程进行训练和展开，减少数值差异，无需低效的步骤间校准。

Result: 在展开阶段提速达33%，训练阶段提速达41%，端到端提速16%，且收敛稳定，精度损失可忽略。

Conclusion: Jet - RL框架能有效解决现有问题，实现强化学习的高效稳定优化。

Abstract: Reinforcement learning (RL) is essential for enhancing the complex reasoning capabilities of large language models (LLMs). However, existing RL training pipelines are computationally inefficient and resource-intensive, with the rollout phase accounting for over 70% of total training time. Quantized RL training, particularly using FP8 precision, offers a promising approach to mitigating this bottleneck. A commonly adopted strategy applies FP8 precision during rollout while retaining BF16 precision for training. In this work, we present the first comprehensive study of FP8 RL training and demonstrate that the widely used BF16-training + FP8-rollout strategy suffers from severe training instability and catastrophic accuracy collapse under long-horizon rollouts and challenging tasks. Our analysis shows that these failures stem from the off-policy nature of the approach, which introduces substantial numerical mismatch between training and inference. Motivated by these observations, we propose Jet-RL, an FP8 RL training framework that enables robust and stable RL optimization. The key idea is to adopt a unified FP8 precision flow for both training and rollout, thereby minimizing numerical discrepancies and eliminating the need for inefficient inter-step calibration. Extensive experiments validate the effectiveness of Jet-RL: our method achieves up to 33% speedup in the rollout phase, up to 41% speedup in the training phase, and a 16% end-to-end speedup over BF16 training, while maintaining stable convergence across all settings and incurring negligible accuracy degradation.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [385] [Speaking to Silicon: Neural Communication with Bitcoin Mining ASICs](https://arxiv.org/abs/2601.12032)
*Francisco Angulo de Lafuente,Vladimir Veselov,Richard Goodman*

Main category: cs.NE

TL;DR: 本文提出用于比特币挖矿ASIC与神经通信的范式，整合五个框架，展示多项成果，实现数学形式化，建立新范式。


<details>
  <summary>Details</summary>
Motivation: 探索过时加密货币挖矿硬件的新用途，实现AI系统与硅基板的双向信息交换。

Method: 整合热力学储层计算、分层数系理论、算法分析、网络延迟优化和机器验证数学形式化五个框架。

Result: 储层计算NRMSE为0.8661；TPF理论节能92.19%；虚拟块管理器有效哈希率提高25%；硬件在多个ASIC系列通用；完成机器验证数学形式化并证明关键定理。

Conclusion: 可将ASIC视为主动对话伙伴，其热力学状态编码可利用的计算信息，建立了新范式。

Abstract: This definitive research memoria presents a comprehensive, mathematically verified paradigm for neural communication with Bitcoin mining Application-Specific Integrated Circuits (ASICs), integrating five complementary frameworks: thermodynamic reservoir computing, hierarchical number system theory, algorithmic analysis, network latency optimization, and machine-checked mathematical formalization. We establish that obsolete cryptocurrency mining hardware exhibits emergent computational properties enabling bidirectional information exchange between AI systems and silicon substrates. The research program demonstrates: (1) reservoir computing with NARMA-10 Normalized Root Mean Square Error (NRMSE) of 0.8661; (2) the Thermodynamic Probability Filter (TPF) achieving 92.19% theoretical energy reduction; (3) the Virtual Block Manager achieving +25% effective hashrate; and (4) hardware universality across multiple ASIC families including Antminer S9, Lucky Miner LV06, and Goldshell LB-Box. A significant contribution is the machine-checked mathematical formalization using Lean 4 and Mathlib, providing unambiguous definitions, machine-verified theorems, and reviewer-proof claims. Key theorems proven include: independence implies zero leakage, predictor beats baseline implies non-independence (the logical core of TPF), energy savings theoretical maximum, and Physical Unclonable Function (PUF) distinguishability witnesses. Vladimir Veselov's hierarchical number system theory explains why early-round information contains predictive power. This work establishes a new paradigm: treating ASICs not as passive computational substrates but as active conversational partners whose thermodynamic state encodes exploitable computational information.

</details>


### [386] [Statistical Firefly Algorithm for Truss Topology Optimization](https://arxiv.org/abs/2601.12265)
*Nghi Huu Duong,Duy Vo,Pruettha Nanakorn*

Main category: cs.NE

TL;DR: 提出统计萤火虫算法（SFA）用于桁架拓扑优化，该算法简单，减少计算量且提升原算法性能。


<details>
  <summary>Details</summary>
Motivation: 为桁架拓扑优化提出更有效算法，减少计算量并提升性能。

Method: 在普通萤火虫算法（FA）机制中应用假设检验，利用萤火虫运动历史结果限制运动。

Result: SFA减少了萤火虫评估次数和计算量，解决多个桁架拓扑优化问题时显著提升原FA性能。

Conclusion: SFA添加的统计策略能在保持结果质量的同时，显著提升原FA在计算效率方面的性能。

Abstract: This study proposes an algorithm titled a statistical firefly algorithm (SFA) for truss topology optimization. In the proposed algorithm, historical results of fireflies' motions are used in hypothesis testing to limit the motions of fireflies that are suggested by current information exchanges between fireflies only to those that are potentially useful. Hypothesis testing is applied to the mechanism of an ordinary firefly algorithm (FA) without changing its structure. As a result, the implementation of the proposed algorithm is simple and straightforward. Limiting the motions of fireflies to those that are potential useful results in reduction of firefly evaluations, and, subsequently, reduction of computational efforts. To test the validity and efficiency of the proposed algorithm, it is used to solve several truss topology optimization problems, including some benchmark problems. It is found that the added statistical strategy in the SFA significantly enhances the performance of the original FA in terms of computational efforts while still maintains the quality of the obtained results.

</details>


### [387] [An Evolutionary Framework for Automatic Optimization Benchmark Generation via Large Language Models](https://arxiv.org/abs/2601.12723)
*Yuhiro Ono,Tomohiro Harada,Yukiya Miura*

Main category: cs.NE

TL;DR: 针对现有优化基准问题构建的不足，提出LLM - EBG框架，生成的基准问题能体现目标算法优势及不同算法搜索行为特征。


<details>
  <summary>Details</summary>
Motivation: 现有优化基准问题存在不足，人工基准难以反映现实问题多样性和不规则性，现实问题衍生基准构建成本高、难度大。

Method: 提出基于大语言模型作为生成算子的进化自动基准生成框架LLM - EBG，以数学表达式生成无约束单目标连续最小化问题。

Result: LLM - EBG成功生成基准问题，目标算法在超80%试验中表现优于对比算法，且探索性景观分析揭示基准问题具有不同几何特征。

Conclusion: 该框架能生成反映不同优化算法内在搜索行为的具有独特几何特征的基准问题。

Abstract: Optimization benchmarks play a fundamental role in assessing algorithm performance; however, existing artificial benchmarks often fail to capture the diversity and irregularity of real-world problem structures, while benchmarks derived from real-world problems are costly and difficult to construct. To address these challenges, we propose an evolutionary automatic benchmark generation framework that leverages a large language model (LLM) as a generative operator, termed the LLM-driven evolutionary benchmark generator (LLM-EBG). In this framework, the LLM serves as an evolutionary operator that generates and evolves benchmark problems within a flexible, expressive representation space. As a case study, we generate unconstrained single-objective continuous minimization problems represented as mathematical expressions designed to induce significant performance differences between a genetic algorithm (GA) and differential evolution (DE). Experimental results show that LLM-EBG successfully produces benchmark problems in which the designated target algorithm consistently outperforms the comparative algorithm in more than 80\% of trials. Furthermore, exploratory landscape analysis reveals that benchmarks favoring GA are highly sensitive to variable scaling, demonstrating that the proposed framework can generate problems with distinct geometric characteristics that reflect the intrinsic search behaviors of different optimization algorithms.

</details>


### [388] [Generalization and Completeness of Stochastic Local Search Algorithms](https://arxiv.org/abs/2601.14212)
*Daniel Loscos,Narciso Marti-Oliet,Ismael Rodriguez*

Main category: cs.NE

TL;DR: 将随机局部搜索（SLS）启发式方法推广为统一形式模型，证明其图灵完备性。


<details>
  <summary>Details</summary>
Motivation: 构建统一模型来概括SLS启发式方法，并研究其计算能力。

Method: 构建具有通用结构和参数化结构的模型，通过不同实例化得到各启发式方法，利用模型构造能模拟图灵机的遗传算法进行证明。

Result: 证明了SLS算法的图灵完备性，指出确定输入输出关系的非平凡属性对遗传算法和一般的SLS方法是不可判定的。

Conclusion: SLS算法具有图灵完备性，部分相关属性不可判定。

Abstract: We generalize Stochastic Local Search (SLS) heuristics into a unique formal model. This model has two key components: a common structure designed to be as large as possible and a parametric structure intended to be as small as possible. Each heuristic is obtained by instantiating the parametric part in a different way. Particular instances for Genetic Algorithms (GA), Ant Colony Optimization (ACO), and Particle Swarm Optimization (PSO) are presented. Then, we use our model to prove the Turing-completeness of SLS algorithms in general. The proof uses our framework to construct a GA able to simulate any Turing machine. This Turing-completeness implies that determining any non-trivial property concerning the relationship between the inputs and the computed outputs is undecidable for GA and, by extension, for the general set of SLS methods (although not necessarily for each particular method). Similar proofs are more informally presented for PSO and ACO.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [389] [Reinforcement Learning for Dynamic Workflow Optimization in CI/CD Pipelines](https://arxiv.org/abs/2601.11647)
*Aniket Abhishek Soni,Milan Parikh,Rashi Nimesh Kumar Dhenia,Jubin Abhishek Soni,Ayush Raj Jha,Sneja Mitinbhai Shah*

Main category: cs.SE

TL;DR: 本文提出基于强化学习动态优化CI/CD管道工作流，通过模拟环境评估，结果显示能提升吞吐量、减少测试时间，展示了强化学习在DevOps工作流的潜力。


<details>
  <summary>Details</summary>
Motivation: 现代软件交付中CI/CD管道静态工作流随系统扩展效率低下，需动态优化。

Method: 将管道建模为马尔可夫决策过程，训练强化学习智能体在运行时做决策，开发可配置仿真环境评估方法。

Result: 强化学习优化后的管道吞吐量最多提升30%，测试执行时间约减少25%，缺陷漏检率低于5%。

Conclusion: 强化学习可实现自适应和智能的DevOps工作流，为更高效、弹性和可持续的CI/CD自动化提供实用途径。

Abstract: Continuous Integration and Continuous Deployment (CI/CD) pipelines are central to modern software delivery, yet their static workflows often introduce inefficiencies as systems scale. This paper proposes a reinforcement learning (RL) based approach to dynamically optimize CI/CD pipeline workflows. The pipeline is modeled as a Markov Decision Process, and an RL agent is trained to make runtime decisions such as selecting full, partial, or no test execution in order to maximize throughput while minimizing testing overhead.
  A configurable CI/CD simulation environment is developed to evaluate the approach across build, test, and deploy stages. Experimental results show that the RL optimized pipeline achieves up to a 30 percent improvement in throughput and approximately a 25 percent reduction in test execution time compared to static baselines, while maintaining a defect miss rate below 5 percent. The agent learns to selectively skip or abbreviate tests for low risk commits, accelerating feedback cycles without significantly increasing failure risk.
  These results demonstrate the potential of reinforcement learning to enable adaptive and intelligent DevOps workflows, providing a practical pathway toward more efficient, resilient, and sustainable CI/CD automation.

</details>


### [390] [Advances and Frontiers of LLM-based Issue Resolution in Software Engineering: A Comprehensive Survey](https://arxiv.org/abs/2601.11655)
*Caihua Li,Lianghong Guo,Yanlin Wang,Daya Guo,Wei Tao,Zhenyu Shan,Mingwei Liu,Jiachi Chen,Haoyu Song,Duyu Tang,Hongyu Zhang,Zibin Zheng*

Main category: cs.SE

TL;DR: 本文对新兴的问题解决领域进行系统调查，涵盖数据构建、方法分析、关键分析、应用探讨，指出挑战与未来方向，并提供开源仓库。


<details>
  <summary>Details</summary>
Motivation: 问题解决是软件工程难题，大型语言模型处理该任务困难，促使出现自主编码代理，故开展对该新兴领域的系统调查。

Method: 先研究数据构建管道，分析方法从免训练框架到基于训练的技术，再探讨数据质量和代理行为分析及实际应用。

Result: 对该领域数据、方法、应用等方面有了全面且系统的分析。

Conclusion: 明确了关键挑战和有前景的未来研究方向，提供开源仓库作为领域资源。

Abstract: Issue resolution, a complex Software Engineering (SWE) task integral to real-world development, has emerged as a compelling challenge for artificial intelligence. The establishment of benchmarks like SWE-bench revealed this task as profoundly difficult for large language models, thereby significantly accelerating the evolution of autonomous coding agents. This paper presents a systematic survey of this emerging domain. We begin by examining data construction pipelines, covering automated collection and synthesis approaches. We then provide a comprehensive analysis of methodologies, spanning training-free frameworks with their modular components to training-based techniques, including supervised fine-tuning and reinforcement learning. Subsequently, we discuss critical analyses of data quality and agent behavior, alongside practical applications. Finally, we identify key challenges and outline promising directions for future research. An open-source repository is maintained at https://github.com/DeepSoftwareAnalytics/Awesome-Issue-Resolution to serve as a dynamic resource in this field.

</details>


### [391] [The Llama 4 Herd: Architecture, Training, Evaluation, and Deployment Notes](https://arxiv.org/abs/2601.11659)
*Aaron Adcock,Aayushi Srivastava,Abhimanyu Dubey,Abhinav Jauhri,Abhinav Pande,Abhinav Pandey,Abhinav Sharma,Abhishek Kadian,Abhishek Kumawat,Adam Kelsey,Adam Stelle,Adeel Cheema,Adela Kabiljo,Adina Katz,Adithya Gangidi,Aditya Tayade,Adolfo Victoria,Adrian Samatan Alastuey,Adrien Conrath,Afroz Mohiuddin,Ahmed Sharif,Ahnaf Siddiqui,Ahuva Goldstand,Aijung Li,Aidan Boyd,Aidin Kazemi Daliri,Aisha Iqbal,Ajay Menon,Ajit Mathews,Akhil Mathur,Akshat Agarwal,Alan Schelten,Alana Shine,Alejandro Castillejo Muñoz,Aleksei Guliaev,Alex Radovic,Alex Song,Alex Vaughan,Alexander Simeonov,Alexandre Rezende,Alexandre Rezende,Alexei Baevski,Alexey Roubaud,Allen Ma,Alvin Lee,Alyssa Pereira,Aman Ahmed,Aman Shankar,Amanda Kallet,Amar Budhiraja,Ameya Khandekar,Amine Benhalloum,Amir Gershman,Amit Nagpal,Amit Zohar,Amr Sharaf,Anant Desai,Anastasia Razdaibiedina,Anca Agape,Andranik Kurghinyan,Andre Perunicic,Andrea Madotto,Andrei Darabanov,Andrés Alvarado,Andrew Brown,Andrew Cohen,Andrew Fang,Andrew Freeman,Andrew Gallagher,Andrew Gu,Andrew Prasetyo Jo,Andrew Ryan,Andrew Steffen,Andrew Wei,Andrey Rusakov,Andrii Golovei,Andy Shang,Angela Fan,Angela Fan,Angela Flewellen,Animesh Pathak,Anirudh Goyal,Ankit Ramchandani,Ankur Pai,Ankur Singh,Ankush Garg,Anlu Xing,Anna Cai,Anna Grosul,Anna Prochowska,Anna Sun,Annie Dong,Annie Franco,Anqi Hu,Anshul Chawla,Anthony Hartshorn,Antonia Sheng,Antony Thomas,Anuj Goyal,Anusha De,Anvit Bodiwala,Anvit Bodiwala,Aobo Yang,Aparajita Saraf,Apurva Samudra,Aran Mun,Arash Rahnama,Archi Mitra,Archie Sravankumar,Archit Gupta,Aria Haghighi,Ariel Stolerman,Arkabandhu Chowdhury,Arnab Choudhury,Artem Korenev,Arthur Guo,Arthur Hinsvark,Arun Mallya,Arvind Neelakantan,Arya Talebzadeh,Ashish Shah,Ashmitha Jeevaraj Shetty,Ashwin Bharambe,Asif Islam,Aston Zhang,Austen Gregerson,Avi Lewis,Aya Ibrahim,Ayaz Minhas,Ayelet Dahan,Ayelet Regev Dabah,Bangsheng Tang,Bar Ulman,Bardiya Sadeghi,Bartosz Jedrzejewski,Barys Skarabahaty,Beibei Zhu,Beibin Li,Ben Bharier,Benjamin Leonhardi,Benjamin Muller,Bennett Plessala,Bernie Huang,Beth Loyd,Bhargavi Paranjape,Bhavik Sheth,Bill Bonner,Bill Holland,Bill Wang,Bingzhe Liu,Binh Tang,Bo Liu,Bo Wu,Boduo Li,Bokai Yu,Bor-Chun Chen,Boris Araya,Boris Vidolov,Botao Chen,Boya Peng,Boyu Ni,Bradley Davis,Bram Wasti,Brandon Adams,Brandon Taylor,Brandon Wu,Brant Swidler,Brian Chiang,Brian Clerkin,Brian Fuller,Brooks Cutter,Bruno Novais,Bryan Gmyrek,Bysshe Easton,Cait Campos,Canaan Case,Carl Chengyan Fu,Carly Burton,Caro Diaz,Catherine Cole,Ce Liu,Cedric Fougerat,Cen Peng,Cen Peng,Cen Zhao,Changhan Wang,Changkyu Kim,Chantal Shaib,Chao Zhou,Charlotte Caucheteux,Chau Nguyen,Chawin Sitawarin,Chaya Nayak,Chelsea Asher,Chen Fan,Chen Zhu,Cheng Cheng,Cheng Zhang,Chenguang Zhu,Chengxiong Ruan,Chengzhu Yu,Chenheli Hua,Chenxi Whitehouse,Cheryl Holloway,Ching-Hsiang Chu,Ching-Yao Chuang,Chinmay Karande,Chirag Nagpal,Chloé Bakalar,Chloe Bi,Chris Cai,Chris Marra,Chris McConnell,Chris Thi,Chris Tindal,Chris Waterson,Christian Deverall,Christian Fuegen,Christian Keller,Christine Cheng,Christine Jou,Christine Smith,Christine Wang,Christoph Feichtenhofer,Christophe Touret,Christopher Luc,Christy Sauper,Chuanhao Zhuge,Chun-Yi Sung,Chunqiang Tang,Chunyang Wu,Clara Siegel,Cody Heale,Cody Wilbourn,Colin White,Congying Xia,Corinne Wong,Cornel Rat,Cristian Canton Ferrer,Cyrille Habis,Cyrus Nikolaidis,D Lohachov,Da Ju,Dalton Flanagan,Damien Allonsius,Damon Civin,Dan Johnson,Daniel Bolya,Daniel Francisco,Daniel Fried,Daniel Hawthorne,Daniel Haziza,Daniel Ho,Daniel Kreymer,Daniel Li,Daniel Machlab,Daniel McKinnon,Daniel Obenshain,Daniel Rodriguez,Daniel Song,Daniel Tse,Danielle Pintz,Danny Livshits,Daryl James Rodrigo,Dat Huynh,Daulet Askarov,David Brandfonbrener,David Esiobu,David Kant,David Levin,David Renardy,David Soofian,David Stevens,David Xu,David Zhang,Deep Shah,Delia David,Demi Douglas,Denis Boyda,Desh Raj,Devamanyu Hazarika,Dheeraj Mekala,Dhruv Choudhary,Dhruv Mahajan,Di Jin,Didac Suris Coll-Vinent,Didem Foss,Diego Garcia-Olano,Diego Perino,Dieuwke Hupkes,DiJia Su,Dilip Madathil,Dinesh Govindasamy,Dinesh Yeduguru,Dmitry Vengertsev,Dong He,Dong Li,Dong Wang,Dongzhuo Li,Duc Le,Dunant Hin,Dustin Holland,Duy Nguyen,Duy Nguyen,Ed Dowling,Eden Litt,Egor Lakomkin,Ehab AlBadawy,Ehsan K. Ardestani,Elad Eckstein,Elahe Dabir,Elaine Montgomery,Elina Lobanova,Elior Abramoviz,Eliot Hedeman,Elissa Li,Elizabeth Hilbert,Ellen Xiaoqing Tan,Elliot Yun,Elodie Stener,Emilian Stoimenov,Emilien Garreau,Emily Dinan,Emily Hahn,Emily Wood,Emma Li,Emmanuel Ademuwagun,Emrah Seker,Eric Alamillo,Eric Gan,Eric Han,Eric Huang,Eric Michael Smith,Eric-Tuan Le,Ernie Chang,Eryk Helenowski,Eslam Elnikety,Esteban Arcaute,Ethan Myers,Eugene Nho,Eugene Poliukhovych,Evan Dunbar,Evgeniy Litvinenko,Evrim Altıntaş,Eyal Hochman,Eyal Shtrauch,Fabian Mastenbroek,Faiza Zeb,Faizan Ahmad,Farhad Farahbakhshian,Fei Kou,Fei Sun,Feiyu Chen,Felix Chung,Feng Tian,Feng Xu,Filip Radenovic,Filippos Kokkinos,Francesco Barbieri,Francesco Caggioni,Francisco Esparza,Francisco Guzmán,Frank Kanayet,Frank Seide,Frank Zhang,Fred Lewis,Freda Huang,Fulton Wang,Gabriel Synnaeve,Gabriela Jacques-Silva,Gabriella Schwarz,Gaganjit Ghardhora,Gal Elfer,Garrett Dickson,Gaurav Chaurasia,Gautam Sewani,Geet Shingi,Gefei Zuo,Geonhwa Jeong,George Puthanpurackal,Georgia Swee,Gerard Moreno-Torres Bertran,Gil Keren,Gina Ling,Gjergji Stasa,Gobinda Saha,Gor Safran,Gordy French,Goutham Rajendran,Govind Thattai,Grace Cineas,Graeme Nail,Greg Fletcher,Grégoire Mialon,Griffin Adams,Grigory Sizov,Guan Pang,Hady Elsahar,Hai Dang Tran,Hailey Nguyen,Haiping Wu,Hakan Inan,Hamid Eghbalzadeh,Han Fang,Han Zou,Hannah Doyle,Hannah Korevaar,Hannah Wang,Hannah Werbel,Hanwen Zha,Hany Morsy,Hao Ma,Haoci Zhang,Haonan Sun,Haozhu Wang,Hardik Shah,Haroun Habeeb,Harrison Rudolph,Harsh Gupta,Harsh Poddar,Harshil Parikh,Hejia Zhang,Heming Wang,Hengduo Li,Himanshu Sharma,Hoang Phi Nguyen,Hongbo Zhang,Honghao Qiu,Hongjiang Lv,Hongli Xu,Hongyuan Zhan,Hossein Hamooni,Howard Huang,Hu Xu,Hugo Laurençon,Hugo Touvron,Hung Dinh,Hunter Goldman,Hussein Mehanna,Huy Nguyen,Hweimi Tsuo,Ian Graves,Ian Yu,Ibrahim Damlaj,Idan Cohen,Igor Tufanov,Ilan Goldenstein,Ilias Leontiadis,Iliyan Zarov,Imad Ahmed,Innocent Djiofack,Iosif Spulber,Irina-Elena Veliche,Isabella Ramos,Ishan Misra,Itai Gal,Ivan Evtimov,Ivan Evtimov,Ivan Obraztsov,Jack Wu,Jacqueline Romero Vertino,Jaemo Koo,Jaewon Lee,Jake Jung,Jake Weissman,James Beldock,James Crnkovich,James Grinage,James Hongyi Zeng,James Kohli,James Tian,Jamie Cahill,Jan Geffert,Jan Seidel,Jan Seidel,Janey Tracey,Jang Hyun Cho,Janice Wei,Jarrod Kahn,Jasmyn Howell,Jason Long Vu,Jason Park,Jason Yan,Jason Yip,Jay Li,Jay Mahadeokar,Jaya Bharath R Goluguri,Jayasi Mehar,Jean-Baptiste Gaya,Jeet Shah,Jeff Hanson,Jeff Marcus,Jeff Walsh,Jeff Yang,Jelmer van der Linde,Jemma Fan,Jennifer Chan,Jenny Zhen,Jenya Lee,Jeremy Fu,Jeremy Reizenstein,Jeremy Teboul,Jesse He,Jessica Zhong,Ji Hou,Ji Yang,Jia Ding,Jiabo Hu,Jiacheng Zhu,Jiadong Guo,Jialiang Wang,Jialin Ouyang,Jianfeng Chi,Jianyu Huang,Jianyun Zhao,Jiaowen Yang,Jiatong Zhou,Jiawei Zhao,Jiawen Liu,Jie Wang,Jie You,Jiecao Yu,Jillian Schwiep,Jilong Wu,Jing Huang,Jing Li,Jing Yu Koh,Jing Zhang,Jingxiang Chen,Jingyi Yang,Jingyue Shen,Jinho Hwang,Jinxi Guo,Jiwan Khatiwada,Joanna Bitton,Joe Li,Joe Quanaim,Joel Beales,Johan Schuijt,John Chang,John Quan,Johnnie Chan,Jon Shepard,Jona Harris,Jonah Rubin,Jonathan Janzen,Jonathan Kaldor,Jorge Lopez Silva,Jose Leitao,Joseph Greer,Joseph Moon,Joseph Rocca,Joseph Tighe,Josh Fromm,Joshua Deng,Joshua Fernandes,Joshua Saxe,Joyce Zheng,Juan Pino,Julien Prigent,Jun Chen,Junjiao Tian,Junjie Qi,Junjie Wang,Junteng Jia,Kade Baker,Kai Londenberg,Kai Wang,Kainan Peng,Kaiyan Peng,Kaiyue Yang,Kalyan Vasudev Alwala,Kam Hou Yu,Kanika Narang,Karan Chadha,Karan Sikka,Karen Zhang,Karina Schuberts,Karishma Mandyam,Karthik Abinav Sankararaman,Karthik Padthe,Karthik Prasad,Karthik Sivakumar,Kartikeya Upasani,Kate Plawiak,Kate Saenko,Kateřina Žmolíková,Kathryn Stadler,Kathy Matosich,Katie Doulgass,Kaveh Hassani,Kay Ji,Ke Li,Kenneth Heafield,Kenny Yu,Keqian Li,Kevin Chih-Yao Ma,Kevin Hannan,Keyu Man,Kezhen Chen,Khalid El-Arini,Khrystyna Hutsulyak,Kieran Nash,Kiran Jagadeesh,Kody Bartelt,Konstantin Topaloglou-Mundy,Konstantinos Chatziioannou,Konstantinos Karanasos,Konstantinos Vougioukas,Kostas Tsiampouris,Kristen Hamill,Kristy Choi,Krithika Iyer,Kshitiz Malik,Kuenley Chiu,Kun Huang,Kunal Bhalla,Kunal Chawla,Kunpeng Li,Kushal Lakhotia,Kyle Monk,Lakshya Garg,Lalit Chourey,Lars Hamre,Laura Gustafson,Lauren Deason,Laurence Rouesnel,Laurens van der Maaten,Lavender A,Lawrence Chen,Lawrence Jang,Leandro Silva,Leda Sari,Lee Hetherington,Lei Zhang,Leiyu Zhao,Lele Chen,Leo Chenghui Li,Leon Yang,Leon Zhan,Levi Corallo,Liang Tan,Licheng Yu,Lijuan Liu,Lilach Mor,Lincoln Lin,Linfeng Li,Lisa Titus,Liz Jenkins,Lovish Madaan,Lu Fang,Lu Yuan,Lucas Nava,Lucas Pasqualin,Lucas Switzer,Lucia Fang,Lucy Sun,Luka Tadic,Lukas Blecher,Lukas Landzaat,Luxin Zhang,Madhavi Rao,Madian Khabsa,Mahalia Miller,Mahendra Kariya,Mahesh Pasupuleti,Mahi Luthra,Manaal Faruqui,Manav Avlani,Manchen Wang,Mannat Singh,Manohar Paluri,Manoj Chakkaravarthy,Manoj Nair,Maquelle Tiffany,Marcin Pawlowski,Marcus Wu,Maria Lomeli,Mario Consuegra,Marion Boiteux,Marios Andreas Galanis,Marshall Chen,Martin Gleize,Maryam Fazel-Zarandi,Matan Hasson,Mathew Oldham,Mathieu Rita,Matt Dordal,Matt Setzler,Matt Staats,Matt Staats,Matt Wilde,Matthew Clark,Matthew Grange,Matthew Lennie,Matthew Schmohl,Max Raphael,Maxim Naumov,Maxim Samoylov,Maxime Lecanu,Maya Pavlova,Md Taha Bin Jawaid,Meghan Keneally,Melanie Kambadur,Meng Zhang,Mengchen Liu,Mengdi Lin,Mengjiao Wang,Mervyn Abraham,Miao Liu,Michael Au-Yeung,Michael Feldergraf,Michael Man,Michael Matheny,Michael Suo,Michael Tontchev,Michel Meyer,Michelle Ma,Mihir Patel,Mihir Sanjay Kale,Mik Vyatskov,Mikayla Alexander,Mike Andersland,Mike Clark,Mike Lewis,Mike Li,Mike Macey,Mike Macey,Mike Seltzer,Mikel Jimenez Fernandez,Mikhail Antonov,Mikhail Plekhanov,Milan Zhou,Min Si,Ming Qiao,Mingbo Ma,Mingjun Zhang,Mingyi Liang,Miquel Jubert Hermoso,Mirac Suzgun,Mirjam Skarica,Mitesh Kumar Singh,Mohammad Kabbani,Mohammad Rastegari,Mona Sarantakos,Monica Sim,Monika Gangapuram,Mor Moshe,Morrie Doulaty,Morvarid Metanat,Moya Chen,Mrinal Kumar,Munish Bansal,Murali Ramarao,Na Li,Nadav Azaria,Nahiyan Malik,Naman Goyal,Nancy Vargas Balderas,Nanshu Wang,Naoyuki Kanda,Natalia Gimelshein,Natalia Neverova,Nathan Aclander,Natt Sithiviraporn,Navneet Madhu Kumar,Ned Newton,Neeraj Bahl,Negar Ghorbani,Neil Patel,Neta-lee Golan,Nicholas Longenbaugh,Nick Egebo,Nikhil Johri,Nikhil Mehta,Nikhil Naik,Niko Moritz,Nikolay Bashlykov,Nikolay Bogoychev,Nikolay Pavlovich Laptev,Niladri Chatterji,Nile Jones,Nimish Shah,Ning Dong,Ning Li,Ning Li,Ning Zhang,Nishant Yadav,Noam Paz,Norman Cheng,Norman Cheng,Olaoluwa Adesanya,Oleg Repin,Oleksandr Maksymets,Omkar Salpekar,Omri Harosh,Onkar Pednekar,Onur Çelebi,Oran Gafni,Oren Edinger,Osama Hanna,Owais Khan Mohammed,Ozlem Kalinli,Paden Tomasello,Pankaj Singh,Paola Quevedo,Parag Jain,Paria Rashidinejad,Parker Tooley,Parth Parekh,Parth Thakkar,Parvin Taheri,Pasan Hapuarachchi,Pascal Kesseli,Patrick Alrassy,Paulo de Rezende Pinatti,Pavan Balaji,Pawan Sisodiya,Pedro Jose Ferreira Moreira,Pedro Rittner,Pedro Valenzuela,Peize Sun,Peizhao Zhang,Peng-Jen Chen,Pengchao Wang,Pengchuan Zhang,Pengwei Li,Petar Vasic,Peter Carras,Peter Ney,Peter Weng,Petru Dumea,Phil Hayes,Philip Woods,Pierre Andrews,Pierre Ménard,Ping-Hao Wu,Pingchuan Liu,Piotr Dollar,Plamen Dzhelepov,Polina Zvyagina,Posten A,Prabhav Agrawal,Pradhapan Rajendran,Pradyot Prakash,Prajjwal Bhargava,Pramono,Pranay Shah,Pranshu Dave,Prash Jain,Pratik Dubal,Praveen Gollakota,Praveen Krishnan,Pritish Yuvraj,Projjal Ghosh,Punit Singh Koura,Puxin Xu,Qi Qi,Qi Zhou,Qian Guan,Qian Sun,Qiang Liu,Qing He,Qinqing Zheng,Qirui Yang,Qizhen Guo,Quanzeng You,Quentin Carbonneaux,Quentin Carbonneaux,Quentin Duval,Quintin Fettes,Rachad Alao,Rachel Batish,Rachel Guo,Rachel Rodriguez,Radhika Bhargava,Rafael Asuncion,Raghotham Murthy,Rahul Dutta,Rahul Jha,Rahul Kindi,Rahul Mitra,Raj Ganapathy,Raj Shah,Rajarshi Das,Rajat Shrivastava,Rajesh Nishtala,Ramakant Shankar,Raman Shukhau,Ramon Calderer,Rangaprabhu Parthasarathy,Ranjan Subramanian,Raphael Bensadoun,Rares Bostan,Rashnil Chaturvedi,Ravi Agrawal,Ray Gao,Raymond Li,Rebecca Kogen,Ricardo Juan Palma Duran,Ricardo Silveira Cabral,Richard Lee,Richard Yuanzhe Pang,Riddhish Bhalodia,Riham Mansour,Rishabh Singh,Rishi Godugu,Ritun Patney,Rob Boyle,Robbie Goldfarb,Robert Caldwell,Robert Kuo,Roberta Raileanu,Robin Battey,Robin Sharma,Rochit Sapra,Rocky Wang,Rodolfo Granata,Rodrigo De Castro,Rodrigo Paim,Rohan Maheshwari,Rohan Varma,Rohit Girdhar,Rohit Patel,Roshan Sumbaly,Roy Sheaffer,Ruan Silva,Ruben Rodriguez Buchillon,Rui Hou,Ruiming Xie,Ruslan Mavlyutov,Ruslan Semenov,Rustam Dinov,Ruxiao Bao,Ryan Fox,Ryan Kilpatrick,Ryan Kwan,Ryan Lim,Ryan Smith,Saaketh Narayan,Sabrina Qiao,Sachin Mehta,Sachin Siby,Sagar Jain,Saghar Hosseini,Sagie Gur-Ari,Sahana Chennabasappa,Sahin Geyik,Sai Jayesh Bondu,Sai Mounika Chowdhary Nekkalapudi,Saif Hasan,Saisuke Okabayashi,Saketh Rambhatla,Salil Sawhney,Sam Dunster,Sam Zhao,Saman Keon,Samaneh Azadi,Sameet Sapra,Samuel Dooley,Samyak Datta,Sandeep Parab,Sang Michael Xie,Sanjay Singh,Sanyuan Chen,Sara Behn,Sara Khodeir,Sarah Shirazyan,Sargun Dhillon,Sarunya Pumma,Sasha Sidorov,Saskia Adaime,Saurabh Khanna,Sayem Wani,Scott Brenton,Sean Bell,Sean Kelly,Sean Koger,Sean Nunley,Sean Perry,Sebastian Caicedo,Sebastian Dahlgren,Sebastian Ruder,Seiji Yamamoto,Selam Mehretu,Selvan Sunitha Ravi,Sen Lyu,Senthil Chellapan,Serafeim Mellos,Sergey Edunov,Sergey Royt,Shaina Cohen,Shangfu Peng,Shannon Adams,Shaoliang Nie,Sharadh Ramaswamy,Sharan Narang,Shashank Pisupati,Shashi Gandham,Shaun Lim,Shaun Lindsay,Sheena Artrip,Shelly Sheynin,Shen Yan,Sheng Feng,Sheng Shen,Shengbao Zheng,Shenghao Lin,Shengjie Bi,Shengxin Cindy Zha,Shengye Wan,Shengyi Qian,Shengyong Cai,Shengzhi Shao,Shervin Shahidi,Shikai Li,Shimon Bernholtz,Shiqi Wang,Shishir G. Patil,Shiv Verma,Shiva Shankar P,Shiyang Chen,Sho Yaida,Shoubhik Debnath,Shreyas Siravara,Shruti Bhosale,Shuang Ma,Shun Zhang,Shuo Tang,Shuqiang Zhang,Shuyan Zhou,Sicong Che,Sidd Srinivisan,Siddharth Bhattacharya,Siddharth Patki,Sijia Chen,Sili Chen,Simon Vandenhende,Simone Merello,Sinong Wang,Sivan Barzily,Sixian Yi,Siyu Lin,SK Bong,Sky Yin,Sneha Agarwal,Sneha Agarwal,Soerian Lieve,Soji Sajuyigbe,Song Jiang,Songlin Li,Sonia Kim,Sopan Khosla,Soumi Maiti,Spencer Whitman,Sravya Popuri,Sreen Tallam,Srinivas Vaidyanathan,Srinivas Vaidyanathan,Sten Sootla,Stephane Collot,Stephanie Ding,Stephen Chen,Steven Cai,Suchin Gururangan,Sudarshan Govindaprasad,Sue Young,Suganthi Dewakar,Sujan Kumar Gonugondla,Sujeet Bhandari,Suman Gumudavelli,Suman Gumudavelli,Sumit Gupta,Summer Deng,Sungmin Cho,Suresh Ganapathy,Surjyendu Dhal,Susan Fedynak,Susana Contrera,Suyoun Kim,Sylvestre Rebuffi,Takshak Chahande,Tamar Herman,Tan Li,Tao Xu,Tara Fowler,Tarek Sheasha,Tarun Anand,Tarun Kalluri,Tarun Singh,Tatiana Shavrina,Ted Li,Teja Rao,Tejas Patil,Teng Li,Thach Bui,Thai Quach,Thamer Alharbash,Thanh Vinh Vo,Thawan Kooburat,Thilo Koehler,Thomas Georgiou,Thomas Scialom,Tian Ye,Tianhe Li,Tianjun Zhang,Tianyu Li,Tijmen Blankevoort,Timon Willi,Timothy Chou,Timothy Leung,TJ Lee,Todor Mihaylov,Tom Heatwole,Tong Xiao,Tony Cao,Tony Lee,Trang Le,Tristan Rice,Tsz Kei Serena Chan,Tuan Tran,Tudor Tiplea,Tyler Baumgartner,Uday Savagaonkar,Ujjwal Karn,Ulises Martinez Araiza,Umar Farooq,Uriel Cohen,Usman Sharif,Utkarsh Murarka,Van Phung,Varun Joginpalli,Varun Saravagi,Vasu Sharma,Vasudha Viswamurthy,Vedanuj Goswami,Vedika Seth,Venkat Ramesh,Venkat Ramesh,Vibhor Gupta,Victoria Montanez,Vidhya Natarajan,Vidya Sarma,Vignesh Ramanathan,Viktor Kerkez,Vinay Rao,Vincent Gonguet,Vincent Mauge,Virginie Do,Vish Vogeti,Vishrav Chaudhary,Viswesh Sankaran,Vítor Albiero,Vivek Miglani,Vivek Pai,Vlad Cojanu,Vlad Shubin,Vlad Tiberiu Mihailescu,Vladan Petrovic,Vladimir Ivanov,Vladislav Vorotilov,Vrushali Bhutada,Wai I Ng,Wei Cheng,Wei Sun,Wei Tu,Wei Wei,Wei Zhou,Wei-Ning Hsu,Weiwei Chu,Weizhe Yuan,Wenchen Wang,Wenjun Zhao,Wenwen Jiang,Wenyin Fu,Wenzhe Jiang,Whitney Meers,Will Constable,Will Wang,William R. Wong,Xavier Martinet,Xi Victoria Lin,Xi Yan,Xi Yin,Xian Li,Xianfeng Rui,Xianjun Yang,Xiaocheng Tang,Xiaodong Wang,Xiaofang Wang,Xiaolan Wang,Xiaoliang Dai,Xiaoliang Peng,Xiaopeng Li,Xiaozhu Meng,Xibei Zhang,Xide Xia,Xin Jin,xinbo Gao,Xinfeng Xie,Xingyi Zhou,Xu Ma,Xuan Ju,Xuanyi Zhao,Xubo Liu,Xuchao Jia,Xuedong Zhang,Xuefei Cao,Xuewei Wang,Xuewei Wu,Xunnan Xu,Xutai Ma,Xuyang Wang,Yan Cui,Yang Chen,Yang Li,Yang Shu,Yang Xia,Yanjun Chen,Yanjun Zhou,Yash Mehta,Yash Patel,Yash Tekena,Yashesh Gaur,Yasmine Babaei,Yaxuan Zhou,Ye Hu,Ye Qi,Yejin Lee,Yeming Wen,Yen-Cheng Liu,Yexin Bruce Wu,Yi Pan,Yi Yang,Yi-Hui Lin,Yifan Wang,Yifan Wu,Yifan Yang,Yifei Huang,Yiftah Ben Aharon,Yilin Yang,Yiling You,Ying Xu,Ying Zhang,Yingquan Yuan,Yingru Liu,Yingyi Ma,Yining Yang,Yiting Lu,Yonatan Komornik,Yongjie Lin,Yoni Goyhman,Yossi Moran Mamo,Youngjin Nam,Yu Wang,Yu Lu,Yu Zhao,Yu-Ho Hsieh,Yu-Jung Lo,Yuandong Tian,Yuanhan Zhang,Yuanhao Xiong,Yuanshun Yao,Yuchen Hao,Yuchen Zhang,Yuchuan Li,Yue Cao,Yue Yu,Yue Zhao,Yuhan Guo,Yuhao Wang,Yuheng Huang,Yujie Lu,Yujun Shi,Yulun Wang,Yun He,Yun Wang,Yundi Qian,Yunfan Wang,Yunhao Tang,Yuning Mao,Yunlu Li,Yuqi Dai,Yuriy Hulovatyy,Yushi Hu,Yuxuan Sun,Zach Rait,Zach Wentz,Zacharie Delpierre Coudert,Zachary Collins,Zahra Hankir,Zecheng He,Zeeshan Ahmed,Zeeshan Ahmed,Zef RosnBrick,Zhan Shu,Zhanna Rohalska,Zhaoduo Wen,Zhe Liu,Zhe Liu,Zhen Qiao,Zhenggang Xu,Zhengwen Zhou,Zhengxing Chen,Zhenyu Tang,Zhichen Wu,Zhicheng Ouyang,Zhihong Lei,Zhipeng Hong,Zhiping Xiu,Zhiwei Zhao,Zhong Meng,Zhou Jin,Zhouhao Zeng,Zichang Liu,Zihang Meng,Zihuan Qiao,Zinnia Zheng,Zixi Qi,Ziyi Luo,Zoe Foulkes Birkhead,Zoey Sun,Zohar Achdut*

Main category: cs.SE

TL;DR: 本文整理Meta Llama 4模型家族技术细节，涵盖变体、架构、训练方法、基准测试、部署约束、许可义务和安全措施，为相关人员提供参考。


<details>
  <summary>Details</summary>
Motivation: 为需要精确、有来源事实的研究人员和从业者提供关于Llama 4的技术参考。

Method: 整理公开报告的技术细节并进行总结。

Result: 得到关于Llama 4的变体、架构、训练、基准测试、部署约束、许可和安全措施等方面的内容。

Conclusion: 为研究人员和从业者提供了关于Llama 4模型的紧凑技术参考。

Abstract: This document consolidates publicly reported technical details about Metas Llama 4 model family. It summarizes (i) released variants (Scout and Maverick) and the broader herd context including the previewed Behemoth teacher model, (ii) architectural characteristics beyond a high-level MoE description covering routed/shared-expert structure, early-fusion multimodality, and long-context design elements reported for Scout (iRoPE and length generalization strategies), (iii) training disclosures spanning pre-training, mid-training for long-context extension, and post-training methodology (lightweight SFT, online RL, and lightweight DPO) as described in release materials, (iv) developer-reported benchmark results for both base and instruction-tuned checkpoints, and (v) practical deployment constraints observed across major serving environments, including provider-specific context limits and quantization packaging. The manuscript also summarizes licensing obligations relevant to redistribution and derivative naming, and reviews publicly described safeguards and evaluation practices. The goal is to provide a compact technical reference for researchers and practitioners who need precise, source-backed facts about Llama 4.

</details>


### [392] [From Everything-is-a-File to Files-Are-All-You-Need: How Unix Philosophy Informs the Design of Agentic AI Systems](https://arxiv.org/abs/2601.11672)
*Deepak Babu Piskala*

Main category: cs.SE

TL;DR: 本文探讨当代智能代理AI中类似Unix‘一切皆文件’的统一趋势，表明以文件和代码为中心的交互模型可使代理系统更好维护、审计和稳健运行。


<details>
  <summary>Details</summary>
Motivation: 探索当代智能代理AI中类似于早期Unix系统‘一切皆文件’原则的统一现象。

Method: 追溯从Unix到DevOps、基础设施即代码，再到自主软件代理的演进过程，强调类文件抽象和基于代码的规范将不同资源整合为一致、可组合的接口。

Result: 发现采用以文件和代码为中心的交互模型能使代理系统具备更好的可维护性、可审计性和操作稳健性。

Conclusion: 采用以文件和代码为中心的交互模型有助于构建更优质的代理系统。

Abstract: A core abstraction in early Unix systems was the principle that 'everything is a file', enabling heterogeneous devices and kernel resources to be manipulated via uniform read/write interfaces. This paper explores how an analogous unification is emerging in contemporary agentic AI. We trace the evolution from Unix to DevOps, Infrastructure-as-Code, and finally autonomous software agents, highlighting how file-like abstractions and code-based specifications collapse diverse resources into consistent, composable interfaces. The resulting perspective suggests that adopting file- and code-centric interaction models may enable agentic systems that are more maintainable, auditable, and operationally robust.

</details>


### [393] [Semantic Caching and Intent-Driven Context Optimization for Multi-Agent Natural Language to Code Systems](https://arxiv.org/abs/2601.11687)
*Harmohit Singh*

Main category: cs.SE

TL;DR: 提出生产优化的多智能体系统，将自然语言查询转换为Python代码用于结构化数据分析，介绍创新点、部署结果及考虑事项。


<details>
  <summary>Details</summary>
Motivation: 设计一种在不依赖昂贵前沿模型的情况下，实现自然语言查询到Python代码转换，达到高精度和成本效益的系统，用于结构化数据分析和企业库存管理。

Method: 采用语义缓存系统、双阈值决策机制和意图驱动的动态提示组装系统三个关键创新方法。

Result: 语义缓存系统在生产查询中缓存命中率达67%；意图驱动的动态提示组装系统减少40 - 60%的令牌消耗；系统部署处理超10000个查询，平均延迟8.2秒，语义准确率94.3%。

Conclusion: 描述系统架构，展示生产部署实证结果，讨论大规模部署基于大语言模型分析系统的实际考虑。

Abstract: We present a production-optimized multi-agent system designed to translate natural language queries into executable Python code for structured data analytics. Unlike systems that rely on expensive frontier models, our approach achieves high accuracy and cost efficiency through three key innovations: (1) a semantic caching system with LLM-based equivalence detection and structured adaptation hints that provides cache hit rates of 67% on production queries; (2) a dual-threshold decision mechanism that separates exact-match retrieval from reference-guided generation; and (3) an intent-driven dynamic prompt assembly system that reduces token consumption by 40-60% through table-aware context filtering. The system has been deployed in production for enterprise inventory management, processing over 10,000 queries with an average latency of 8.2 seconds and 94.3% semantic accuracy. We describe the architecture, present empirical results from production deployment, and discuss practical considerations for deploying LLM-based analytics systems at scale.

</details>


### [394] [FlipFlop: A Static Analysis-based Energy Optimization Framework for GPU Kernels](https://arxiv.org/abs/2601.13345)
*Saurabhsingh Rajput,Alexander Brandt,Vadim Elisseev,Tushar Sharma*

Main category: cs.SE

TL;DR: 提出FlipFlop框架，用静态代码分析预测能耗并推荐最优配置，减少搜索空间，节能提效。


<details>
  <summary>Details</summary>
Motivation: GPU程序能耗大，软件开发人员缺乏硬件专业知识优化能效。

Method: 采用静态代码分析，分析PTX代码，无需运行时执行。

Result: 在多种GPU和内核上验证，识别最优配置准确率83%，减少搜索空间93.4%，对多头注意力内核节能79%，吞吐量提升106%。

Conclusion: FlipFlop结合静态分析与实时监测，为开发者提供可解释优化指导，创建低环境与计算成本的软件。

Abstract: Artificial Intelligence (AI) applications, such as Large Language Models, are primarily driven and executed by Graphics Processing Units (GPUs). These GPU programs (kernels) consume substantial amounts of energy, yet software developers often lack the hardware expertise and ad hoc knowledge required to optimize for power efficiency. We propose FlipFlop, a framework using static code analysis to predict energy consumption and recommend Pareto-optimal thread block configurations considering both power consumption and execution time. Our framework requires no runtime execution and analyzes PTX code, a low-level instruction set for CUDA-enabled GPUs. It is validated across a diverse set of GPUs and kernels, including multi-head attention, convolution, and matrix multiplication. FlipFlop achieves 83% accuracy in identifying locally optimal energy-efficient configurations, while also minimizing developer effort by reducing the optimization search space by 93.4%. For multi-head attention kernels, it yields up to 79% energy savings and 106% throughput gains relative to NVIDIA's occupancy heuristic. By integrating static analysis with real-time monitoring and providing explainable optimization guidance, FlipFlop empowers developers to create sustainable, high-performance GPU software which minimizes environmental and computational costs.

</details>


### [395] [SpecMap: Hierarchical LLM Agent for Datasheet-to-Code Traceability Link Recovery in Systems Engineering](https://arxiv.org/abs/2601.11688)
*Vedant Nipane,Pulkit Agrawal,Amit Singh*

Main category: cs.SE

TL;DR: 提出分层的数据表到代码映射方法，用大语言模型做语义分析，实验显示比传统方法有显著提升，还能降低计算开销，支持大型嵌入式软件系统分析。


<details>
  <summary>Details</summary>
Motivation: 现有可追溯性链接恢复方法难以捕捉嵌入式系统软件的语义、结构和符号层面关系，手动映射不可行。

Method: 采用分层的数据表到代码映射方法，通过仓库级结构推理、文件级相关性估计和细粒度符号级对齐逐步缩小搜索空间，还覆盖多种代码元素。

Result: 相比传统基于信息检索的基线有显著改进，文件映射准确率达73.3%，降低计算开销，LLM令牌消耗降低84%，端到端运行时间降低约80%。

Conclusion: 该方法支持大型嵌入式软件系统的自动化分析，可用于下游应用。

Abstract: Establishing precise traceability between embedded systems datasheets and their corresponding code implementations remains a fundamental challenge in systems engineering, particularly for low-level software where manual mapping between specification documents and large code repositories is infeasible. Existing Traceability Link Recovery approaches primarily rely on lexical similarity and information retrieval techniques, which struggle to capture the semantic, structural, and symbol level relationships prevalent in embedded systems software. We present a hierarchical datasheet-to-code mapping methodology that employs large language models for semantic analysis while explicitly structuring the traceability process across multiple abstraction levels. Rather than performing direct specification-to-code matching, the proposed approach progressively narrows the search space through repository-level structure inference, file-level relevance estimation, and fine-grained symbollevel alignment. The method extends beyond function-centric mapping by explicitly covering macros, structs, constants, configuration parameters, and register definitions commonly found in systems-level C/C++ codebases. We evaluate the approach on multiple open-source embedded systems repositories using manually curated datasheet-to-code ground truth. Experimental results show substantial improvements over traditional information-retrieval-based baselines, achieving up to 73.3% file mapping accuracy. We significantly reduce computational overhead, lowering total LLM token consumption by 84% and end-to-end runtime by approximately 80%. This methodology supports automated analysis of large embedded software systems and enables downstream applications such as training data generation for systems-aware machine learning models, standards compliance verification, and large-scale specification coverage analysis.

</details>


### [396] [Technical Lag as Latent Technical Debt: A Rapid Review](https://arxiv.org/abs/2601.11693)
*Shane K. Panter,Nasir U. Eisty*

Main category: cs.SE

TL;DR: 本文对技术滞后相关研究进行整合，分析其检测、影响和管理策略，提出未来研究方向以改善大型代码库维护。


<details>
  <summary>Details</summary>
Motivation: 软件系统技术滞后会导致软件质量下降，本文旨在整合现有研究，明确相关定义，探索检测和量化方法等。

Method: 采用带滚雪球法的快速审查，从ACM数字图书馆、IEEE Xplore等数据库选取同行评审研究。

Result: 技术滞后被动积累且常因检测不足被忽视，会影响软件质量，管理策略包括自动依赖更新、持续集成和定期审计。

Conclusion: 增强现有指标和检测方法，将技术滞后作为潜在债务指标可改善大型代码库维护，已识别研究差距并提出未来方向。

Abstract: Context: Technical lag accumulates when software systems fail to keep pace with technological advancements, leading to a deterioration in software quality. Objective: This paper aims to consolidate existing research on technical lag, clarify definitions, explore its detection and quantification methods, examine underlying causes and consequences, review current management practices, and lay out a vision as an indicator of passively accumulated technical debt. Method: We conducted a Rapid Review with snowballing to select the appropriate peer-reviewed studies. We leveraged the ACM Digital Library, IEEE Xplore, Scopus, and Springer as our primary source databases. Results: Technical lag accumulates passively, often unnoticed due to inadequate detection metrics and tools. It negatively impacts software quality through outdated dependencies, obsolete APIs, unsupported platforms, and aging infrastructure. Strategies to manage technical lag primarily involve automated dependency updates, continuous integration processes, and regular auditing. Conclusions: Enhancing and extending the current standardized metrics, detection methods, and empirical studies to use technical lag as an indication of accumulated latent debt can greatly improve the process of maintaining large codebases that are heavily dependent on external packages. We have identified the research gaps and outlined a future vision for researchers and practitioners to explore.

</details>


### [397] [The Stability Trap: Evaluating the Reliability of LLM-Based Instruction Adherence Auditing](https://arxiv.org/abs/2601.11783)
*Murtuza N. Shergadwala*

Main category: cs.SE

TL;DR: 研究生成式AI企业治理审计机制，发现高裁决稳定性可能掩盖脆弱推理，建议审计人员严格规划自动化评估协议。


<details>
  <summary>Details</summary>
Motivation: 受监管行业生成式AI企业治理需要可扩展且可重复的审计机制，LLM - as - a - Judge方法评估系统指令遵循性的可靠性未经验证，需研究被测应用指令类型对评估稳定性的影响。

Method: 引入范围指令分解框架将被测应用指令分为客观和主观类型，应用于两个代表性HR GenAI应用，评估四种评判架构在不同运行中的稳定性。

Result: 发现“稳定性陷阱”，裁决稳定性和推理稳定性存在差异，客观和主观评估裁决一致性高，但推理稳定性差异大，客观定量分析推理稳定性低，离散实体提取推理稳定性高。

Conclusion: 高裁决稳定性可能掩盖脆弱推理，建议审计人员将可确定性验证逻辑交给代码，LLM评判用于复杂语义评估。

Abstract: The enterprise governance of Generative AI (GenAI) in regulated sectors, such as Human Resources (HR), demands scalable yet reproducible auditing mechanisms. While Large Language Model (LLM)-as-a-Judge approaches offer scalability, their reliability in evaluating adherence of different types of system instructions remains unverified. This study asks: To what extent does the instruction type of an Application Under Test (AUT) influence the stability of judge evaluations? To address this, we introduce the Scoped Instruction Decomposition Framework to classify AUT instructions into Objective and Subjective types, isolating the factors that drive judge instability. We applied this framework to two representative HR GenAI applications, evaluating the stability of four judge architectures over variable runs. Our results reveal a ``Stability Trap'' characterized by a divergence between Verdict Stability and Reasoning Stability. While judges achieved near-perfect verdict agreement ($>99\%$) for both objective and subjective evaluations, their accompanying justification traces diverged significantly. Objective instructions requiring quantitative analysis, such as word counting, exhibited reasoning stability as low as $\approx19\%$, driven by variances in numeric justifications. Similarly, reasoning stability for subjective instructions varied widely ($35\%$--$83\%$) based on evidence granularity, with feature-specific checks failing to reproduce consistent rationale. Conversely, objective instructions focusing on discrete entity extraction achieved high reasoning stability ($>90\%$). These findings demonstrate that high verdict stability can mask fragile reasoning. Thus, we suggest that auditors scope automated evaluation protocols strictly: delegate all deterministically verifiable logic to code, while reserving LLM judges for complex semantic evaluation.

</details>


### [398] [Changes in Coding Behavior and Performance Since the Introduction of LLMs](https://arxiv.org/abs/2601.11835)
*Yufan Zhang,Jaromir Savelka,Seth Goldstein,Michael Conway*

Main category: cs.SE

TL;DR: 研究分析五年内研究生云计算课程作业代码提交情况，对比ChatGPT发布前后学生表现，发现其发布后学生代码行为显著变化，建议教育者和雇主反思评估方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型改变学生编程和解题方式，增加教师评估学生学习和努力的难度，需研究其对学生的具体影响。

Method: 开展准纵向研究，分析五年内某研究生云计算课程中一份未变作业的代码提交情况，对比ChatGPT发布前后五个学期学生行为。

Result: 2022年秋季后学生代码行为显著改变，最终提交代码长度增加，连续提交的编辑距离增加但分数提升减少，且行为变化与整体表现有显著相关性。

Conclusion: 虽不能确定是大语言模型滥用导致，但结果与部分学生过度依赖大语言模型影响学习成果的假设相符，提醒教育者和雇主反思评估方法。

Abstract: The widespread availability of large language models (LLMs) has changed how students engage with coding and problem-solving. While these tools may increase student productivity, they also make it more difficult for instructors to assess students' learning and effort. In this quasi-longitudinal study, we analyze five years of student source code submissions in a graduate-level cloud computing course, focusing on an assignment that remained unchanged and examining students' behavior during the period spanning five semesters before the release of ChatGPT and five semesters after.
  Student coding behavior has changed significantly since Fall 2022. The length of their final submissions increased. Between consecutive submissions, average edit distances increased while average score improvement decreased, suggesting that both student productivity and learning have decreased after ChatGPT's release. Additionally, there are statistically significant correlations between these behavioral changes and their overall performance. Although we cannot definitively attribute them to LLM misuse, they are consistent with our hypothesis that some students are over-reliant on LLMs, which is negatively affecting their learning outcomes. Our findings raise an alarm around the first generation of graduates in the age of LLMs, calling upon both educators and employers to reflect on their evaluation methods for genuine expertise and productivity.

</details>


### [399] [Trace Validation of Unmodified Concurrent Systems with OmniLink](https://arxiv.org/abs/2601.11836)
*Finn Hackett,Evan Wrench,Peter Macko,A. Jesse Jiryu Davis,Yuanhao Wei,Ivan Beschastnikh*

Main category: cs.SE

TL;DR: 本文介绍了用于验证并发实现的OmniLink方法，其与现有方法不同，能处理复杂并发问题，评估中表现出色并发现了未知漏洞。


<details>
  <summary>Details</summary>
Motivation: 并发系统验证困难，现有工具存在侵入式检测或不切实际执行模型等问题。

Method: 将系统事件视为黑盒，求解动作逻辑全序，使用灵活规范语言，基于现货模型检查进行线性化检查。

Result: 在大规模验证任务中表现优于现有技术，找到了已知漏洞并发现两个未知漏洞。

Conclusion: OmniLink是一种有效的并发系统验证方法，能提升验证效率并发现未知问题。

Abstract: Concurrent systems are notoriously difficult to validate: subtle bugs may only manifest under rare thread interleavings, and existing tools often require intrusive instrumentation or unrealistic execution models. We present OmniLink, a new methodology for validating concurrent implementations against high-level specifications in TLA+. Unlike prior TLA+ based approaches which use a technique called trace validation, OmniLink treats system events as black boxes with a timebox in which they occurred and a meaning in TLA+, solving for a logical total order of actions. Unlike prior approaches based on linearizability checking, which already solves for total orders of actions with timeboxes, OmniLink uses a flexible specification language, and offers a different linearizability checking method based on off-the-shelf model checking. OmniLink offers different features compared existing linearizability checking tools, and we show that it outperforms the state of the art on large scale validation tasks.
  Our evaluation validates WiredTiger, a state-of-the-art industrial database storage layer, as well as Balanced Augmented Tree (BAT), a state-of-the art lock-free data structure from the research community, and ConcurrentQueue, a popular lock-free queue featuring aggressive performance optimizations. We use OmniLink to improve WiredTiger's existing TLA+ model, as well as develop new TLA+ models that closely match the behavior of the modeled systems, including non-linearizable behaviors. OmniLink is able to find known bugs injected into the systems under test, as well as help discover two previously unknown bugs (1 in BAT, 1 in ConcurrentQueue), which we have confirmed with the authors of those systems.

</details>


### [400] [Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces](https://arxiv.org/abs/2601.11868)
*Mike A. Merrill,Alexander G. Shaw,Nicholas Carlini,Boxuan Li,Harsh Raj,Ivan Bercovich,Lin Shi,Jeong Yeon Shin,Thomas Walshe,E. Kelly Buchanan,Junhong Shen,Guanghao Ye,Haowei Lin,Jason Poulos,Maoyu Wang,Marianna Nezhurina,Jenia Jitsev,Di Lu,Orfeas Menis Mastromichalakis,Zhiwei Xu,Zizhao Chen,Yue Liu,Robert Zhang,Leon Liangyu Chen,Anurag Kashyap,Jan-Lucas Uslu,Jeffrey Li,Jianbo Wu,Minghao Yan,Song Bian,Vedang Sharma,Ke Sun,Steven Dillmann,Akshay Anand,Andrew Lanpouthakoun,Bardia Koopah,Changran Hu,Etash Guha,Gabriel H. S. Dreiman,Jiacheng Zhu,Karl Krauth,Li Zhong,Niklas Muennighoff,Robert Amanfu,Shangyin Tan,Shreyas Pimpalgaonkar,Tushar Aggarwal,Xiangning Lin,Xin Lan,Xuandong Zhao,Yiqing Liang,Yuanli Wang,Zilong Wang,Changzhi Zhou,David Heineman,Hange Liu,Harsh Trivedi,John Yang,Junhong Lin,Manish Shetty,Michael Yang,Nabil Omi,Negin Raoof,Shanda Li,Terry Yue Zhuo,Wuwei Lin,Yiwei Dai,Yuxin Wang,Wenhao Chai,Shang Zhou,Dariush Wahdany,Ziyu She,Jiaming Hu,Zhikang Dong,Yuxuan Zhu,Sasha Cui,Ahson Saiyed,Arinbjörn Kolbeinsson,Jesse Hu,Christopher Michael Rytting,Ryan Marten,Yixin Wang,Alex Dimakis,Andy Konwinski,Ludwig Schmidt*

Main category: cs.SE

TL;DR: 提出Terminal - Bench 2.0基准测试，展示前沿模型表现并发布数据集。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法有效衡量真实世界任务或前沿模型能力。

Method: 构建包含89个计算机终端任务的Terminal - Bench 2.0基准测试，每个任务有独特环境、人工编写解决方案和全面测试。

Result: 前沿模型和智能体在该基准测试中得分低于65%，并进行了错误分析。

Conclusion: 发布数据集和评估工具，助力开发者和研究人员后续工作。

Abstract: AI agents may soon become capable of autonomously completing valuable, long-horizon tasks in diverse domains. Current benchmarks either do not measure real-world tasks, or are not sufficiently difficult to meaningfully measure frontier models. To this end, we present Terminal-Bench 2.0: a carefully curated hard benchmark composed of 89 tasks in computer terminal environments inspired by problems from real workflows. Each task features a unique environment, human-written solution, and comprehensive tests for verification. We show that frontier models and agents score less than 65\% on the benchmark and conduct an error analysis to identify areas for model and agent improvement. We publish the dataset and evaluation harness to assist developers and researchers in future work at https://www.tbench.ai/ .

</details>


### [401] [Harmonica: A Self-Adaptation Exemplar for Sustainable MLOps](https://arxiv.org/abs/2601.11926)
*Ananya Halgatti,Shaunak Biswas,Hiya Bhatt,Srinivasan Rakhunathan,Karthik Vaidhyanathan*

Main category: cs.SE

TL;DR: 本文提出Harmonica，一个基于HarmonE方法的自适应示例，用于支持依赖MLOps管道的机器学习系统（MLS）的可持续运行，通过案例研究证明其能提升系统稳定性和减少人工干预。


<details>
  <summary>Details</summary>
Motivation: MLS在运行中常面临环境变化带来的不确定性，MLOps对解决运行时不确定性支持有限，且缺乏相关研究示例。

Method: 基于HarmonE方法构建Harmonica，通过MAPE - K循环引入结构化自适应控制，分离高层适应策略和低层策略执行，持续监控可持续性指标并与动态适应边界对比。

Result: 通过时间序列回归和计算机视觉的案例研究，证明Harmonica能提升系统稳定性和减少人工干预。

Conclusion: Harmonica为依赖MLOps管道持续运行的MLS提供了实用且可复用的自适应基础。

Abstract: Machine learning enabled systems (MLS) often operate in settings where they regularly encounter uncertainties arising from changes in their surrounding environment. Without structured oversight, such changes can degrade model behavior, increase operational cost, and reduce the usefulness of deployed systems. Although Machine Learning Operations (MLOps) streamlines the lifecycle of ML models, it provides limited support for addressing runtime uncertainties that influence the longer term sustainability of MLS. To support continued viability, these systems need a mechanism that detects when execution drifts outside acceptable bounds and adjusts system behavior in response. Despite the growing interest in sustainable and self-adaptive MLS, there has been limited work towards exemplars that allow researchers to study these challenges in MLOps pipelines. This paper presents Harmonica, a self-adaptation exemplar built on the HarmonE approach, designed to enable the sustainable operation of such pipelines. Harmonica introduces structured adaptive control through MAPE-K loop, separating high-level adaptation policy from low-level tactic execution. It continuously monitors sustainability metrics, evaluates them against dynamic adaptation boundaries, and automatically triggers architectural tactics when thresholds are violated. We demonstrate the tool through case studies in time series regression and computer vision, examining its ability to improve system stability and reduce manual intervention. The results show that Harmonica offers a practical and reusable foundation for enabling adaptive behavior in MLS that rely on MLOps pipelines for sustained operation.

</details>


### [402] [Enhancing Fuzz Testing Efficiency through Automated Fuzz Target Generation](https://arxiv.org/abs/2601.11972)
*Chi Thien Tran*

Main category: cs.SE

TL;DR: 本文围绕模糊测试，指出生成模糊目标的挑战，介绍了通过库源代码静态分析改进生成的方法，并在C/C++库上验证。


<details>
  <summary>Details</summary>
Motivation: 大规模软件项目和库中手动创建模糊目标耗时费力，需要自动化技术生成目标并简化结果执行分析。

Method: 通过静态分析库源代码改进模糊目标生成，包括分析代码结构构建函数调用、映射输入数据到参数、合成编译信息、自动收集分析结果。

Result: 将该方法应用于C/C++库生成模糊目标。

Conclusion: 提出的通过静态分析库源代码改进模糊目标生成的方法具有一定可行性。

Abstract: Fuzzing continues to be the most effective method for identifying security vulnerabilities in software. In the context of fuzz testing, the fuzzer supplies varied inputs to fuzz targets, which are designed to comprehensively exercise critical sections of the client code. Various studies have focused on optimizing and developing advanced fuzzers, such as AFL++, libFuzzer, Honggfuzz, syzkaller, ISP-Fuzzer, which have substantially enhanced vulnerability detection in widely used software and libraries. Nevertheless, achieving greater coverage necessitates improvements in both the quality and quantity of fuzz targets. In large-scale software projects and libraries -- characterized by numerous user defined functions and data types -- manual creation of fuzz targets is both labor-intensive and time-consuming. This challenge underscores the need for automated techniques not only to generate fuzz targets but also to streamline the execution and analysis of their results. In this paper, we introduce an approach to improving fuzz target generation through static analysis of library source code. The proposed method encompasses several key aspects: it analyzes source code structures to accurately construct function calls and generate fuzz targets; it maps fuzzer input data to the corresponding function parameters; it synthesizes compilation information for the fuzz targets; and it automatically collects and analyzes execution results. Our findings are demonstrated through the application of this approach to the generation of fuzz targets for C/C++ libraries.

</details>


### [403] [From LLMs to Agents in Programming: The Impact of Providing an LLM with a Compiler](https://arxiv.org/abs/2601.12146)
*Viktor Kjellberg,Miroslaw Staron,Farnaz Fotrousi*

Main category: cs.SE

TL;DR: 研究大语言模型结合编译器在软件开发中作用，实验表明结合编译器可提升编译成功率、减少错误，小模型结合编译器有时能胜过大型模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成代码质量不高，研究结合软件开发工具（如gcc编译器）对其的帮助程度。

Method: 在RosettaCode数据集上对699个C编程任务进行计算实验，评估16个不同规模语言模型。

Result: 结合编译器使编译成功率提升5.3 - 79.4个百分点，语法错误降75%，未定义引用错误降87%，小模型结合编译器有时胜过大型模型。

Conclusion: 大语言模型需访问软件工程工具提升性能，减少对大模型依赖，降低能耗。

Abstract: Large Language Models have demonstrated a remarkable capability in natural language and program generation and software development. However, the source code generated by the LLMs does not always meet quality requirements and may fail to compile. Therefore, many studies evolve into agents that can reason about the problem before generating the source code for the solution. The goal of this paper is to study the degree to which such agents benefit from access to software development tools, in our case, a \texttt{gcc} compiler. We conduct a computational experiment on the RosettaCode dataset, on 699 programming tasks in C. We evaluate how the integration with a compiler shifts the role of the language model from a passive generator to an active agent capable of iteratively developing runnable programs based on feedback from the compiler. We evaluated 16 language models with sizes ranging from small (135 million) to medium (3 billion) and large (70 billion). Our results show that access to a compiler improved the compilation success by 5.3 to 79.4 percentage units in compilation without affecting the semantics of the generated program. Syntax errors dropped by 75\%, and errors related to undefined references dropped by 87\% for the tasks where the agents outperformed the baselines. We also observed that in some cases, smaller models with a compiler outperform larger models with a compiler. We conclude that it is essential for LLMs to have access to software engineering tools to enhance their performance and reduce the need for large models in software engineering, such as reducing our energy footprint.

</details>


### [404] [Many Hands Make Light Work: An LLM-based Multi-Agent System for Detecting Malicious PyPI Packages](https://arxiv.org/abs/2601.12148)
*Muhammad Umar Zeshan,Motunrayo Ibiyo,Claudio Di Sipio,Phuong T. Nguyen,Davide Di Ruscio*

Main category: cs.SE

TL;DR: 本文提出多智能体系统LAMPS，用协作大语言模型检测恶意PyPI包，经评估效果良好，证明分布式大语言模型推理的可行性和模块化设计益处。


<details>
  <summary>Details</summary>
Motivation: 开源仓库恶意代码威胁软件供应链，传统工具忽略源码语义模式，大语言模型在可解释和模块化安全流程中应用有限。

Method: 提出LAMPS多智能体系统，通过CrewAI框架协调四个特定角色智能体，结合微调CodeBERT模型和LLaMA - 3智能体。

Result: 在D1数据集上准确率达97.7%，超MPHunter；在D2数据集上准确率和平衡准确率达99.5%，超RAG和单智能体基线，McNemar测试表明改进显著。

Conclusion: 分布式大语言模型推理用于恶意代码检测可行，模块化多智能体设计对软件供应链安全有益。

Abstract: Malicious code in open-source repositories such as PyPI poses a growing threat to software supply chains. Traditional rule-based tools often overlook the semantic patterns in source code that are crucial for identifying adversarial components. Large language models (LLMs) show promise for software analysis, yet their use in interpretable and modular security pipelines remains limited. This paper presents LAMPS, a multi-agent system that employs collaborative LLMs to detect malicious PyPI packages. The system consists of four role-specific agents for package retrieval, file extraction, classification, and verdict aggregation, coordinated through the CrewAI framework. A prototype combines a fine-tuned CodeBERT model for classification with LLaMA-3 agents for contextual reasoning. LAMPS has been evaluated on two complementary datasets: D1, a balanced collection of 6,000 setup.py files, and D2, a realistic multi-file dataset with 1,296 files and natural class imbalance. On D1, LAMPS achieves 97.7% accuracy, surpassing MPHunter--one of the state-of-the-art approaches. On D2, it reaches 99.5% accuracy and 99.5% balanced accuracy, outperforming RAG-based approaches and fine-tuned single-agent baselines. McNemar's test confirmed these improvements as highly significant. The results demonstrate the feasibility of distributed LLM reasoning for malicious code detection and highlight the benefits of modular multi-agent designs in software supply chain security.

</details>


### [405] [Aletheia: What Makes RLVR For Code Verifiers Tick?](https://arxiv.org/abs/2601.12186)
*Vatsal Venkatkrishna,Indraneil Paul,Iryna Gurevych*

Main category: cs.SE

TL;DR: 提出开源测试平台Aletheia评估代码验证器，研究RLVR训练配方，发现优化和简化机会。


<details>
  <summary>Details</summary>
Motivation: 多领域思维验证器在代码生成中应用少，代码验证器在难获取执行反馈情景有价值，需评估其鲁棒性。

Method: 创建开源测试平台Aletheia进行执行评估，研究RLVR训练配方的关键组件。

Result: 实验表明RLVR最优，发现可简化训练配方。小模型时，在线策略学习是关键；大模型时，基于思维的训练最重要。

Conclusion: RLVR在代码验证中有优势，且训练配方有优化和简化空间。

Abstract: Multi-domain thinking verifiers trained via Reinforcement Learning from Verifiable Rewards (RLVR) are a prominent fixture of the Large Language Model (LLM) post-training pipeline, owing to their ability to robustly rate and rerank model outputs. However, the adoption of such verifiers towards code generation has been comparatively sparse, with execution feedback constituting the dominant signal. Nonetheless, code verifiers remain valuable toward judging model outputs in scenarios where execution feedback is hard to obtain and are a potentially powerful addition to the code generation post-training toolbox. To this end, we create and open-source Aletheia, a controlled testbed that enables execution-grounded evaluation of code verifiers' robustness across disparate policy models and covariate shifts. We examine components of the RLVR-based verifier training recipe widely credited for its success: (1) intermediate thinking traces, (2) learning from negative samples, and (3) on-policy training. While experiments show the optimality of RLVR, we uncover important opportunities to simplify the recipe. Particularly, despite code verification exhibiting positive training- and inference-time scaling, on-policy learning stands out as the key component at small verifier sizes, and thinking-based training emerges as the most important component at larger scales.

</details>


### [406] [Environment-Aware Code Generation: How far are We?](https://arxiv.org/abs/2601.12262)
*Tongtong Wu,Rongyi Chen,Wenjie Du,Suyu Ma,Guilin Qi,Zhenchang Xing,Shahram Khadivi,Ramesh Periyathambi,Gholamreza Haffari*

Main category: cs.SE

TL;DR: 本文对环境感知代码生成进行了系统研究，引入 VersiBCB 基准测试，探索三种适应策略，发现当前大语言模型在特定环境代码生成上有困难，适应策略可提升兼容性和可执行性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型代码生成评估多针对孤立小代码，不清楚其能否为特定环境生成可靠可执行代码，因此开展环境感知代码生成的系统研究。

Method: 引入 VersiBCB 基准测试，研究数据、参数和缓存三个适应轴并开发对应策略。

Result: 当前大语言模型在环境特定代码生成方面存在困难，所提出的适应策略能提高环境兼容性和可执行性。

Conclusion: 研究结果凸显了在实际软件工程工作流程中部署大语言模型的关键挑战和机遇。

Abstract: Recent progress in large language models (LLMs) has improved code generation, but most evaluations still test isolated, small-scale code (e.g., a single function) under default or unspecified software environments. As a result, it is unclear whether LLMs can reliably generate executable code tailored to a user's specific environment. We present the first systematic study of Environment-Aware Code Generation (EACG), where generated code must be functionally correct and directly executable under arbitrary software configurations. To enable realistic evaluation, we introduce VersiBCB, a benchmark that is multi-package, execution-verified, and deprecation-aware, capturing complex and evolving environments that prior datasets often overlook. Using VersiBCB, we investigate three complementary adaptation axes: data, parameters, and cache, and develop representative strategies for each. Our results show that current LLMs struggle with environment-specific code generation, while our adaptations improve environment compatibility and executability. These findings highlight key challenges and opportunities for deploying LLMs in practical software engineering workflows.

</details>


### [407] [Leveraging Mutation Analysis for LLM-based Repair of Quantum Programs](https://arxiv.org/abs/2601.12273)
*Chihiro Yoshida,Yuta Ishimoto,Olivier Nourry,Masanari Kondo,Makoto Matsushita,Yasutaka Kamei,Yoshiki Higo*

Main category: cs.SE

TL;DR: 本文构建框架让大语言模型生成量子程序修复代码与解释，设计四种提示配置研究上下文信息影响，实验表明突变分析可提升修复成功率和解释质量。


<details>
  <summary>Details</summary>
Motivation: 现有量子程序自动修复技术存在修复成功率低和补丁可理解性差的问题。

Method: 构建大语言模型生成代码修复及解释的框架，设计四种不同组合的提示配置，运用突变分析。

Result: 突变分析能为基于大语言模型的量子程序自动修复提供有价值的上下文信息，修复成功率达94.4%，部分情况下提升解释质量。

Conclusion: 为开发兼具可靠性和可解释性的量子程序自动修复技术指明新方向。

Abstract: In recent years, Automated Program Repair (APR) techniques specifically designed for quantum programs have been proposed. However, existing approaches often suffer from low repair success rates or poor understandability of the generated patches. In this study, we construct a framework in which a large language model (LLM) generates code repairs along with a natural language explanation of the applied repairs. To investigate how the contextual information included in prompts influences APR performance for quantum programs, we design four prompt configurations with different combinations of static information, dynamic information, and mutation analysis results. Mutation analysis evaluates how small changes to specific parts of a program affect its execution results and provides more detailed dynamic information than simple execution outputs such as stack traces. Our experimental results show that mutation analysis can provide valuable contextual information for LLM-based APR of quantum programs, improving repair success rates (achieving 94.4% in our experiment) and in some cases also improving the quality of generated explanations. Our findings point toward new directions for developing APR techniques for quantum programs that enhance both reliability and explainability.

</details>


### [408] [Hybrid Concolic Testing with Large Language Models for Guided Path Exploration](https://arxiv.org/abs/2601.12274)
*Mahdi Eslamimehr*

Main category: cs.SE

TL;DR: 本文提出将concolic执行与大语言模型结合的算法框架，通过实验证明该方法在覆盖率和检测时间上优于传统方法，提升bug检测能力。


<details>
  <summary>Details</summary>
Motivation: 传统concolic测试存在路径爆炸和约束求解成本高的问题，阻碍其在大规模真实软件系统中的应用。

Method: 提出将concolic执行与大语言模型结合的混合方法，利用大语言模型的语义推理能力引导路径探索、优先处理有趣路径并辅助约束求解，还定义系统架构和算法。

Result: 在合成和真实金融科技应用实验中，该方法在分支覆盖率、路径覆盖率和覆盖时间上显著优于传统concolic测试、随机测试和基于遗传算法的方法。

Conclusion: 结合concolic执行和大语言模型优势，能更高效探索程序状态空间，提升bug检测能力。

Abstract: Concolic testing, a powerful hybrid software testing technique, has historically been plagued by fundamental limitations such as path explosion and the high cost of constraint solving, which hinder its practical application in large-scale, real-world software systems. This paper introduces a novel algorithmic framework that synergistically integrates concolic execution with Large Language Models (LLMs) to overcome these challenges. Our hybrid approach leverages the semantic reasoning capabilities of LLMs to guide path exploration, prioritize interesting execution paths, and assist in constraint solving. We formally define the system architecture and algorithms that constitute this new paradigm. Through a series of experiments on both synthetic and real-world Fintech applications, we demonstrate that our approach significantly outperforms traditional concolic testing, random testing, and genetic algorithm-based methods in terms of branch coverage, path coverage, and time-to-coverage. The results indicate that by combining the strengths of both concolic execution and LLMs, our method achieves a more efficient and effective exploration of the program state space, leading to improved bug detection capabilities.

</details>


### [409] [The Expert Validation Framework (EVF): Enabling Domain Expert Control in AI Engineering](https://arxiv.org/abs/2601.12327)
*Lucas Gren,Felix Dobslaw*

Main category: cs.SE

TL;DR: 提出专家验证框架解决生成式AI在企业部署中缺乏质量保证机制的问题。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在企业部署缺乏系统质量保证机制，阻碍其应用。

Method: 提出专家验证框架，通过结构化规范、测试、验证和持续监控等流程，采用四阶段实施过程。

Result: 框架能让组织在利用生成式AI能力时保持专家监督和质量标准。

Conclusion: 该框架建立了严格、专家驱动的方法，填补AI能力与组织信任间的关键差距。

Abstract: Generative AI (GenAI) systems promise to transform knowledge work by automating a range of tasks, yet their deployment in enterprise settings remains hindered by the lack of systematic quality assurance mechanisms. We present an Expert Validation Framework that places domain experts at the center of building software with GenAI components, enabling them to maintain authoritative control over system behavior through structured specification, testing, validation, and continuous monitoring processes. Our framework addresses the critical gap between AI capabilities and organizational trust by establishing a rigorous, expert-driven methodology for ensuring quality across diverse GenAI applications. Through a four-stage implementation process encompassing specification, system creation, validation, and production monitoring, the framework enables organizations to leverage GenAI capabilities while maintaining expert oversight and quality standards.

</details>


### [410] [Discovering 100+ Compiler Defects in 72 Hours via LLM-Driven Semantic Logic Recomposition](https://arxiv.org/abs/2601.12360)
*Xinabang He,Yuanwei Chen,Hao Wu,Jikang Zhang,Zicheng Wang,Ligeng Chen,Junjie Peng,Haiyang Wei,Yi Qian,Tiantai Zhang,Linzhang Wang,Bing Mao*

Main category: cs.SE

TL;DR: 现有编译器模糊测试方法难保留触发漏洞程序语义，提出 FeatureFuzz 结合特征生成程序，经测试能有效发现编译器漏洞。


<details>
  <summary>Details</summary>
Motivation: 当前编译器模糊测试方法难以保留触发漏洞程序的语义，导致生成程序多样性受限，需要新方法解决该问题。

Method: 提出 FeatureFuzz 编译器模糊测试器，通过从历史漏洞报告中提取特征、合成特征组、实例化特征组为有效程序的三阶段工作流程来生成程序。

Result: 在 GCC 和 LLVM 上测试，24 小时发现 167 个独特崩溃，是次优模糊测试器的 2.78 倍；72 小时发现 106 个漏洞，76 个已获开发者确认。

Conclusion: FeatureFuzz 能有效对现代编译器进行压力测试。

Abstract: Compilers constitute the foundational root-of-trust in software supply chains; however, their immense complexity inevitably conceals critical defects. Recent research has attempted to leverage historical bugs to design new mutation operators or fine-tune models to increase program diversity for compiler fuzzing.We observe, however, that bugs manifest primarily based on the semantics of input programs rather than their syntax. Unfortunately, current approaches, whether relying on syntactic mutation or general Large Language Model (LLM) fine-tuning, struggle to preserve the specific semantics found in the logic of bug-triggering programs. Consequently, these critical semantic triggers are often lost, resulting in a limitation of the diversity of generated programs.
  To explicitly reuse such semantics, we propose FeatureFuzz, a compiler fuzzer that combines features to generate programs. We define a feature as a decoupled primitive that encapsulates a natural language description of a bug-prone invariant, such as an out-of-bounds array access, alongside a concrete code witness of its realization. FeatureFuzz operates via a three-stage workflow: it first extracts features from historical bug reports, synthesizes coherent groups of features, and finally instantiates these groups into valid programs for compiler fuzzing.
  We evaluated FeatureFuzz on GCC and LLVM. Over 24-hour campaigns, FeatureFuzz uncovered 167 unique crashes, which is 2.78x more than the second-best fuzzer. Furthermore, through a 72-hour fuzzing campaign, FeatureFuzz identified 106 bugs in GCC and LLVM, 76 of which have already been confirmed by compiler developers, validating the approach's ability to stress-test modern compilers effectively.

</details>


### [411] [Evaluating Large Language Models for Time Series Anomaly Detection in Aerospace Software](https://arxiv.org/abs/2601.12448)
*Yang Liu,Yixing Luo,Xiaofeng Li,Xiaogang Dong,Bin Gu,Zhi Jin*

Main category: cs.SE

TL;DR: 本文提出首个航空航天时间序列异常检测基准ATSADBench，评估大语言模型在该场景的表现，发现其在单变量任务表现好、多变量任务差，还研究增强策略效果并给出未来研究指导。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在航空航天时间序列异常检测场景的有效性因复杂遥测、评估指标不一致和缺乏领域知识而未得到充分研究，需填补此空白。

Method: 引入ATSADBench基准，包含9个任务共108,000个数据点；用直接和基于预测两种范式评估开源大语言模型；提出窗口级评估指标；研究少样本学习和检索增强生成两种增强策略。

Result: 大语言模型在单变量任务表现好，多变量任务差；少样本学习有小提升，检索增强生成无显著改进；少样本提示可减少误报，检索增强生成会加剧误报。

Conclusion: 研究结果为未来航空航天软件中基于大语言模型的时间序列异常检测提供指导。

Abstract: Time series anomaly detection (TSAD) is essential for ensuring the safety and reliability of aerospace software systems. Although large language models (LLMs) provide a promising training-free alternative to unsupervised approaches, their effectiveness in aerospace settings remains under-examined because of complex telemetry, misaligned evaluation metrics, and the absence of domain knowledge. To address this gap, we introduce ATSADBench, the first benchmark for aerospace TSAD. ATSADBench comprises nine tasks that combine three pattern-wise anomaly types, univariate and multivariate signals, and both in-loop and out-of-loop feedback scenarios, yielding 108,000 data points. Using this benchmark, we systematically evaluate state-of-the-art open-source LLMs under two paradigms: Direct, which labels anomalies within sliding windows, and Prediction-Based, which detects anomalies from prediction errors. To reflect operational needs, we reformulate evaluation at the window level and propose three user-oriented metrics: Alarm Accuracy (AA), Alarm Latency (AL), and Alarm Contiguity (AC), which quantify alarm correctness, timeliness, and credibility. We further examine two enhancement strategies, few-shot learning and retrieval-augmented generation (RAG), to inject domain knowledge. The evaluation results show that (1) LLMs perform well on univariate tasks but struggle with multivariate telemetry, (2) their AA and AC on multivariate tasks approach random guessing, (3) few-shot learning provides modest gains whereas RAG offers no significant improvement, and (4) in practice LLMs can detect true anomaly onsets yet sometimes raise false alarms, which few-shot prompting mitigates but RAG exacerbates. These findings offer guidance for future LLM-based TSAD in aerospace software.

</details>


### [412] [Improved Bug Localization with AI Agents Leveraging Hypothesis and Dynamic Cognition](https://arxiv.org/abs/2601.12522)
*Asif Mohammed Samir,Mohammad Masudur Rahman*

Main category: cs.SE

TL;DR: 本文提出新的代理技术CogniGent用于错误定位，克服传统方法和现有LLM技术局限，实验显示其性能优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 传统错误定位方法孤立分析代码组件，现有LLM技术缺乏因果推理且难管理上下文，因此需新方法。

Method: 提出CogniGent技术，利用多个能进行因果推理的AI代理、基于调用图的根本原因分析和上下文工程，模拟开发者调试实践并进行假设测试。

Result: 在591个错误报告数据集上评估，使用三个常用性能指标，与六个基线方法对比，CogniGent在多个指标上显著优于传统和基于LLM的技术。

Conclusion: CogniGent解决了推理、依赖和上下文的局限，将类人认知与代理自动化结合，提升了错误定位性能。

Abstract: Software bugs cost technology providers (e.g., AT&T) billions annually and cause developers to spend roughly 50% of their time on bug resolution. Traditional methods for bug localization often analyze the suspiciousness of code components (e.g., methods, documents) in isolation, overlooking their connections with other components in the codebase. Recent advances in Large Language Models (LLMs) and agentic AI techniques have shown strong potential for code understanding, but still lack causal reasoning during code exploration and struggle to manage growing context effectively, limiting their capability. In this paper, we present a novel agentic technique for bug localization -- CogniGent -- that overcomes the limitations above by leveraging multiple AI agents capable of causal reasoning, call-graph-based root cause analysis and context engineering. It emulates developers-inspired debugging practices (a.k.a., dynamic cognitive debugging) and conducts hypothesis testing to support bug localization. We evaluate CogniGent on a curated dataset of 591 bug reports using three widely adopted performance metrics and compare it against six established baselines from the literature. Experimental results show that our technique consistently outperformed existing traditional and LLM-based techniques, achieving MAP improvements of 23.33-38.57% at the document and method levels. Similar gains were observed in MRR, with increases of 25.14-53.74% at both granularity levels. Statistical significance tests also confirm the superiority of our technique. By addressing the reasoning, dependency, and context limitations, CogniGent advances the state of bug localization, bridging human-like cognition with agentic automation for improved performance.

</details>


### [413] [Automated Tool Support for Category-Partition Testing: Design Decisions, UI and Examples of Use](https://arxiv.org/abs/2601.12559)
*Yvan Labiche*

Main category: cs.SE

TL;DR: 本文介绍基于图形用户界面工具支持，尽可能自动化类别划分测试技术各步骤，并通过九个案例展示工具能力。


<details>
  <summary>Details</summary>
Motivation: 尝试尽可能自动化类别划分测试技术的多个步骤。

Method: 借助图形用户界面工具，让用户指定参数、环境变量、类别和选择等，工具按选择标准自动构建测试框架并生成测试用例。

Result: 成功开发工具并通过九个不同案例研究展示了工具的能力。

Conclusion: 借助图形用户界面工具能有效自动化类别划分测试技术的多个步骤。

Abstract: Category-Partition is a functional testing technique that is based on the idea that the input domain of the system under test can be divided into sub-domains, with the assumption that inputs that belong to the same sub-domain trigger a similar behaviour and that therefore it is sufficient to select one input from each sub-domain. Category-Partition proceeds in several steps, from the identification of so-called categories and choices, possibly constrained, which are subsequently used to form test frames, i.e., combinations of choices, and eventually test cases. This paper reports on an ongoing attempt to automate as many of those steps as possible, with graphical-user interface tool support. Specifically, the user interface allows the user to specify parameters as well as so-called environment variables, further specify categories and choices with optional constraints. Choices are provided with precise specifications with operations specific to their types (e.g., Boolean, Integer, Real, String). Then, the tool automates the construction of test frames, which are combinations of choices, according to alternative selection criteria, and the identification of input values for parameters and environment variables for these test frames, thereby producing test cases. The paper illustrates the capabilities of the tool with the use of nine different case studies.

</details>


### [414] [OpenAI for OpenAPI: Automated generation of REST API specification via LLMs](https://arxiv.org/abs/2601.12735)
*Hao Chen,Yunchun Li,Chen Chen,Fengxu Lin,Wei Li*

Main category: cs.SE

TL;DR: 提出基于LLM的技术无关静态分析方法OOPS用于生成OAS，在多语言和框架下实验效果好。


<details>
  <summary>Details</summary>
Motivation: 开发者编写和维护OAS面临挑战，现有静态分析方法有语言和框架限制，LLMs有上下文限制和幻觉问题。

Method: 提出OOPS，作为LLM代理工作流，分端点方法提取和OAS生成两步，构建API依赖图解决上下文限制，通过多阶段生成和自精炼减少幻觉。

Result: 在12个真实REST API上评估，各方面推理的平均F1分数高，输入输出token数有合理范围。

Conclusion: OOPS能为不同技术实现的REST API准确生成高质量OAS。

Abstract: REST APIs, based on the REpresentational State Transfer (REST) architecture, are the primary type of Web API. The OpenAPI Specification (OAS) serves as the de facto standard for describing REST APIs and is crucial for multiple software engineering tasks. However, developers face challenges in writing and maintaining OAS. Although static analysis shows potential for OAS generation, it is limited to specific programming languages and development frameworks. The powerful code understanding capabilities of LLMs offer new opportunities for OAS generation, yet they are constrained by context limitations and hallucinations. To address these challenges, we propose the OpenAI OpenAPI Project Scanner (OOPS), the first technology-agnostic LLM-based static analysis method for OAS generation, requiring fewer technology-specific rules and less human expert intervention. OOPS is implemented as an LLM agent workflow comprising two key steps: endpoint method extraction and OAS generation. By constructing an API dependency graph, it establishes necessary file associations to address LLMs' context limitations. Through multi-stage generation and self-refine, it mitigates both syntactic and semantic hallucinations during OAS generation. We evaluated OOPS on 12 real-world REST APIs spanning 5 programming languages and 8 development frameworks. Experimental results demonstrate that OOPS accurately generates high-quality OAS for REST APIs implemented with diverse technologies, achieving an average F1-score exceeding 98% for endpoint method inference, 97% for both request parameter and response inference, and 92% for parameter constraint inference. The input tokens average below 5.6K with a maximum of 16.2K, while the output tokens average below 0.9K with a maximum of 7.7K.

</details>


### [415] [Teaching LLMs to Learn Tool Trialing and Execution through Environment Interaction](https://arxiv.org/abs/2601.12762)
*Xingjie Gao,Pengcheng Huang,Zhenghao Liu,Yukun Yan,Shuo Wang,Zulong Chen,Chen Qian,Ge Yu,Yu Gu*

Main category: cs.SE

TL;DR: 提出ToolMaster框架提升大语言模型使用工具的泛化性与鲁棒性，实验效果佳。


<details>
  <summary>Details</summary>
Motivation: 现有方法在面对新工具时鲁棒性不足，轨迹中心范式限制泛化能力。

Method: 采用试错执行范式，先模仿含工具试验和自我校正的轨迹，再强化学习协调试验与执行阶段。

Result: ToolMaster在泛化性和鲁棒性上显著优于现有基线。

Conclusion: ToolMaster可有效让大语言模型自主探索工具使用，形成经验知识。

Abstract: Equipping Large Language Models (LLMs) with external tools enables them to solve complex real-world problems. However, the robustness of existing methods remains a critical challenge when confronting novel or evolving tools. Existing trajectory-centric paradigms primarily rely on memorizing static solution paths during training, which limits the ability of LLMs to generalize tool usage to newly introduced or previously unseen tools. In this paper, we propose ToolMaster, a framework that shifts tool use from imitating golden tool-calling trajectories to actively learning tool usage through interaction with the environment. To optimize LLMs for tool planning and invocation, ToolMaster adopts a trial-and-execution paradigm, which trains LLMs to first imitate teacher-generated trajectories containing explicit tool trials and self-correction, followed by reinforcement learning to coordinate the trial and execution phases jointly. This process enables agents to autonomously explore correct tool usage by actively interacting with environments and forming experiential knowledge that benefits tool execution. Experimental results demonstrate that ToolMaster significantly outperforms existing baselines in terms of generalization and robustness across unseen or unfamiliar tools. All code and data are available at https://github.com/NEUIR/ToolMaster.

</details>


### [416] [Docker Does Not Guarantee Reproducibility](https://arxiv.org/abs/2601.12811)
*Julien Malka,Stefano Zacchiroli,Théo Zimmermann*

Main category: cs.SE

TL;DR: 本文通过系统文献综述和大规模实证研究，评估了Docker镜像的实际可重复性和文献中最佳实践的有效性。


<details>
  <summary>Details</summary>
Motivation: 虽然Docker理论上能实现可重复性，但其实践中的保障和局限性尚待探索，故开展研究。

Method: 通过系统文献综述确定关于Docker可重复性的科学论述和写入Dockerfile的最佳实践；对从GitHub工作流收集的5298个Docker构建进行大规模实证研究，以评估Docker镜像的可重复性和最佳实践的有效性。

Result: 摘要未提及具体研究结果。

Conclusion: 摘要未提及具体研究结论。

Abstract: The reproducibility of software environments is a critical concern in modern software engineering, with ramifications ranging from the effectiveness of collaboration workflows to software supply chain security and scientific reproducibility. Containerization technologies like Docker address this problem by encapsulating software environments into shareable filesystem snapshots known as images. While Docker is frequently cited in the literature as a tool that enables reproducibility in theory, the extent of its guarantees and limitations in practice remains under-explored.
  In this work, we address this gap through two complementary approaches. First, we conduct a systematic literature review to examine how Docker is framed in scientific discourse on reproducibility and to identify documented best practices for writing Dockerfiles enabling reproducible image building. Then, we perform a large-scale empirical study of 5298 Docker builds collected from GitHub workflows. By rebuilding these images and comparing the results with their historical counterparts, we assess the real reproducibility of Docker images and evaluate the effectiveness of the best practices identified in the literature.

</details>


### [417] [Automatic Generation of Formal Specification and Verification Annotations Using LLMs and Test Oracles](https://arxiv.org/abs/2601.12845)
*João Pascoal Faria,Emanuel Trigo,Vinicius Honorato,Rui Abreu*

Main category: cs.SE

TL;DR: 本文探讨用大语言模型为Dafny程序自动生成验证注解，实验表明多模型结合方法效果好，还分析了问题难度，验证注解并集成到IDE。


<details>
  <summary>Details</summary>
Motivation: 现有的验证工具在为程序添加形式化验证注解时仍需大量手动工作和专业知识，故研究用大语言模型自动生成注解。

Method: 结合Claude Opus 4.5和GPT - 5.2的多模型方法，利用验证器反馈，通过最多8次修复迭代，用测试用例断言验证注解。

Result: 在110个Dafny程序实验中，98.2%的程序生成了正确注解；逻辑回归分析显示证明助手注解对当前大语言模型难度大；将自动生成集成到IDE获得了积极的可用性反馈。

Conclusion: 大语言模型可有效为Dafny程序自动生成验证注解，且将自动生成集成到IDE是可行且受欢迎的。

Abstract: Recent verification tools aim to make formal verification more accessible to software engineers by automating most of the verification process. However, annotating conventional programs with the formal specification and verification constructs (preconditions, postconditions, loop invariants, auxiliary predicates and functions and proof helpers) required to prove their correctness still demands significant manual effort and expertise. This paper investigates how LLMs can automatically generate such annotations for programs written in Dafny, a verification-aware programming language, starting from conventional code accompanied by natural language specifications (in comments) and test code. In experiments on 110 Dafny programs, a multimodel approach combining Claude Opus 4.5 and GPT-5.2 generated correct annotations for 98.2% of the programs within at most 8 repair iterations, using verifier feedback. A logistic regression analysis shows that proof-helper annotations contribute disproportionately to problem difficulty for current LLMs. Assertions in the test cases served as static oracles to automatically validate the generated pre/postconditions. We also compare generated and manual solutions and present an extension for Visual Studio Code to incorporate automatic generation into the IDE, with encouraging usability feedback.

</details>


### [418] [Efficient Code Analysis via Graph-Guided Large Language Models](https://arxiv.org/abs/2601.12890)
*Hang Gao,Tao Peng,Baoquan Cui,Hong Huang,Fengge Wu,Junsuo Zhao,Jian Zhang*

Main category: cs.SE

TL;DR: 提出以图为中心的注意力获取管道，提升大语言模型检测恶意代码的能力，在多个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 恶意行为常隐藏在小代码片段中，跨文件依赖使大语言模型难以可靠检测。

Method: 将项目解析为代码图，用大语言模型编码节点，在稀疏监督下训练图神经网络进行初步检测，通过回溯预测确定关键代码段，引导大语言模型深入分析。

Result: 在多个公共和自建数据集上，该方法始终优于现有方法。

Conclusion: 此策略能显著减少无关上下文干扰，保持低标注成本，在软件安全场景有实际部署潜力。

Abstract: Malicious behavior is often hidden in small, easily overlooked code fragments, especially within large and complex codebases. The cross-file dependencies of these fragments make it difficult for even powerful large language models (LLMs) to detect them reliably. We propose a graph-centric attention acquisition pipeline that enhances LLMs' ability to localize malicious behavior. The approach parses a project into a code graph, uses an LLM to encode nodes with semantic and structural signals, and trains a Graph Neural Network (GNN) under sparse supervision. The GNN performs an initial detection, and through backtracking of its predictions, identifies key code sections that are most likely to contain malicious behavior. These influential regions are then used to guide the LLM's attention for in-depth analysis. This strategy significantly reduces interference from irrelevant context while maintaining low annotation costs. Extensive experiments show that the method consistently outperforms existing methods on multiple public and self-built datasets, highlighting its potential for practical deployment in software security scenarios.

</details>


### [419] [A Benchmark for Language Models in Real-World System Building](https://arxiv.org/abs/2601.12927)
*Weilin Jin,Chenyu Zhao,Zeshun Huang,Chaoyun Zhang,Qingwei Lin,Chetan Bansal,Saravan Rajmohan,Shenglin Zhang,Yongqian Sun,Dan Pei,Yifan Wu,Tong Jia,Ying Li,Zhonghai Wu,Minghua Ma*

Main category: cs.SE

TL;DR: 提出跨架构和语言的软件包构建修复基准，评估6个LLM，显示跨ISA修复仍难。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注单指令集架构和同构编程语言，为解决此局限开展研究。

Method: 引入含268个真实软件包构建失败案例的基准，提供标准化评估流程，并在该基准上评估6个先进的大语言模型。

Result: 跨ISA软件包修复仍困难，需进一步发展。

Conclusion: 该基准为改进软件可移植性和弥合架构差距的未来方法奠定基础。

Abstract: During migration across instruction set architectures (ISAs), software package build repair is a critical task for ensuring the reliability of software deployment and the stability of modern operating systems. While Large Language Models (LLMs) have shown promise in tackling this challenge, prior work has primarily focused on single instruction set architecture (ISA) and homogeneous programming languages. To address this limitation, we introduce a new benchmark designed for software package build repair across diverse architectures and languages. Comprising 268 real-world software package build failures, the benchmark provides a standardized evaluation pipeline. We evaluate six state-of-the-art LLMs on the benchmark, and the results show that cross-ISA software package repair remains difficult and requires further advances. By systematically exposing this challenge, the benchmark establishes a foundation for advancing future methods aimed at improving software portability and bridging architectural gaps.

</details>


### [420] [Beyond Accuracy: Characterizing Code Comprehension Capabilities in (Large) Language Models](https://arxiv.org/abs/2601.12951)
*Felix Mächtle,Jan-Niclas Serr,Nils Loose,Thomas Eisenbarth*

Main category: cs.SE

TL;DR: 本文探究大语言模型代码理解性能与传统软件指标的关系，引入诊断框架评估模型，发现人类定义指标与模型成功相关性低，强调需新的基准方法。


<details>
  <summary>Details</summary>
Motivation: 当前基准对大语言模型在软件工程中表现的评估过于粗略，需探究其代码理解性能与传统软件指标的关系。

Method: 引入诊断框架将代码理解转化为二元输入输出一致性任务，用大规模数据集关联模型性能与传统复杂度指标。

Result: 人类定义指标与大语言模型成功相关性极小（AUROC 0.63），影子模型预测性能更高（AUROC 0.86）。

Conclusion: 大语言模型理解反映了特定于模型的规律，需要超越整体准确率、注重实例级诊断的基准方法，同时认识到预测正确结果的基本限制。

Abstract: Large Language Models (LLMs) are increasingly integrated into software engineering workflows, yet current benchmarks provide only coarse performance summaries that obscure the diverse capabilities and limitations of these models. This paper investigates whether LLMs' code-comprehension performance aligns with traditional human-centric software metrics or instead reflects distinct, non-human regularities. We introduce a diagnostic framework that reframes code understanding as a binary input-output consistency task, enabling the evaluation of classification and generative models. Using a large-scale dataset, we correlate model performance with traditional, human-centric complexity metrics, such as lexical size, control-flow complexity, and abstract syntax tree structure. Our analyses reveal minimal correlation between human-defined metrics and LLM success (AUROC 0.63), while shadow models achieve substantially higher predictive performance (AUROC 0.86), capturing complex, partially predictable patterns beyond traditional software measures. These findings suggest that LLM comprehension reflects model-specific regularities only partially accessible through either human-designed or learned features, emphasizing the need for benchmark methodologies that move beyond aggregate accuracy and toward instance-level diagnostics, while acknowledging fundamental limits in predicting correct outcomes.

</details>


### [421] [ArchAgent: Scalable Legacy Software Architecture Recovery with LLMs](https://arxiv.org/abs/2601.13007)
*Rusheng Pan,Bingcheng Mao,Tianyi Ma,Zhenhua Ling*

Main category: cs.SE

TL;DR: 提出ArchAgent框架从跨仓库代码库重构多视图、业务对齐的架构，评估显示有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法在从大规模遗留软件恢复准确架构时，受架构漂移、关系缺失和大语言模型上下文限制等问题阻碍。

Method: 提出基于代理、结合静态分析、自适应代码分割和大语言模型合成的ArchAgent框架，引入带上下文剪枝的可扩展图生成，集成跨仓库数据识别关键业务模块。

Result: 对GitHub典型大规模项目评估显示比现有基准有显著提升，消融实验证实依赖上下文可提高生成架构准确性，真实案例证明能有效恢复遗留项目关键业务逻辑。

Conclusion: ArchAgent可有效解决从大规模遗留软件准确恢复架构的问题。

Abstract: Recovering accurate architecture from large-scale legacy software is hindered by architectural drift, missing relations, and the limited context of Large Language Models (LLMs). We present ArchAgent, a scalable agent-based framework that combines static analysis, adaptive code segmentation, and LLM-powered synthesis to reconstruct multiview, business-aligned architectures from cross-repository codebases. ArchAgent introduces scalable diagram generation with contextual pruning and integrates cross-repository data to identify business-critical modules. Evaluations of typical large-scale GitHub projects show significant improvements over existing benchmarks. An ablation study confirms that dependency context improves the accuracy of generated architectures of production-level repositories, and a real-world case study demonstrates effective recovery of critical business logics from legacy projects. The dataset is available at https://github.com/panrusheng/arch-eval-benchmark.

</details>


### [422] [MeltRTL: Multi-Expert LLMs with Inference-time Intervention for RTL Code Generation](https://arxiv.org/abs/2601.13015)
*Nowfel Mashnoor,Mohammad Akyash,Hadi Kamali,Kimia Azar*

Main category: cs.SE

TL;DR: 本文提出MeltRTL框架，集成多专家注意力和推理时干预机制，提升LLM的RTL代码生成准确性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型自动生成硬件寄存器传输级代码在复杂数字设计上难以产出语法和功能正确的代码，需要提升准 确性。

Method: 提出MeltRTL框架，包含多专家注意力架构、推理时干预机制和高效干预框架。

Result: 在VerilogEval基准测试中，MeltRTL综合率达96%，功能正确性达60%，优于基础模型；仅产生27%的计算开销且无模型微调。

Conclusion: MeltRTL可直接部署在现有预训练模型上，多专家架构和推理时干预机制结合有协同效应。

Abstract: The automated generation of hardware register-transfer level (RTL) code with large language models (LLMs) shows promise, yet current solutions struggle to produce syntactically and functionally correct code for complex digital designs. This paper introduces MeltRTL, a novel framework that integrates multi-expert attention with inference-time intervention (ITI) to significantly improve LLM-based RTL code generation accuracy without retraining the base model. MeltRTL introduces three key innovations: (1) A multi-expert attention architecture that dynamically routes design specifications to specialized expert networks, enabling targeted reasoning across various hardware categories; (2) An inference-time intervention mechanism that employs non-linear probes to detect and correct hardware-specific inaccuracies during generation; and (3) An efficient intervention framework that selectively operates on expert-specific attention heads with minimal computational overhead. We evaluate MeltRTL on the VerilogEval benchmark, achieving 96% synthesizability and 60% functional correctness, compared to the base LLM's 85.3% and 45.3%, respectively. These improvements are obtained entirely at inference time, with only 27% computational overhead and no model fine-tuning, making MeltRTL immediately deployable on existing pre-trained LLMs. Ablation studies further show the complementary benefits of multi-expert architecture and ITI, highlighting their synergistic effects when combined.

</details>


### [423] [RM -RF: Reward Model for Run-Free Unit Test Evaluation](https://arxiv.org/abs/2601.13097)
*Elena Bruches,Daniil Grebenkin,Mikhail Klementev,Vadim Alperovich,Roman Derunets,Dari Baturova,Georgy Mkrtchyan,Oleg Sedukhin,Ivan Bondarenko,Nikolay Bushkov,Stanislav Moiseev*

Main category: cs.SE

TL;DR: 本文提出轻量级奖励模型RM - RF，用于免运行评估自动生成的单元测试，测试多种模型和调优机制，性能良好，能降低延迟和成本。


<details>
  <summary>Details</summary>
Motivation: 传统编译运行工具在评估自动生成的单元测试时存在高延迟和高基础设施成本问题，需要一种更高效的评估方法。

Method: 提出RM - RF模型，可仅从源文件和测试代码预测三个执行结果信号；构建多语言数据集训练和评估模型；测试多种模型家族和调优机制。

Result: 在三个目标上平均F1值达到0.69；较传统工具，RM - RF有更低延迟和基础设施成本，且预测准确性有竞争力。

Conclusion: RM - RF实现了快速、可扩展反馈，可用于大规模测试生成和基于强化学习的代码优化。

Abstract: We present RM-RF, a lightweight reward model for run-free evaluation of automatically generated unit tests. Instead of repeatedly compiling and executing candidate tests, RM-RF predicts - from source and test code alone - three execution-derived signals: (1) whether the augmented test suite compiles and runs successfully, (2) whether the generated test cases increase code coverage, and (3) whether the generated test cases improve the mutation kill rate. To train and evaluate RM-RF we assemble a multilingual dataset (Java, Python, Go) of focal files, test files, and candidate test additions labeled by an execution-based pipeline, and we release an associated dataset and methodology for comparative evaluation. We tested multiple model families and tuning regimes (zero-shot, full fine-tuning, and PEFT via LoRA), achieving an average F1 of 0.69 across the three targets. Compared to conventional compile-and-run instruments, RM-RF provides substantially lower latency and infrastructure cost while delivering competitive predictive fidelity, enabling fast, scalable feedback for large-scale test generation and RL-based code optimization.

</details>


### [424] [Guidelines to Prompt Large Language Models for Code Generation: An Empirical Characterization](https://arxiv.org/abs/2601.13118)
*Alessandro Midolo,Alessandro Giagnorio,Fiorella Zampetti,Rosalia Tufano,Gabriele Bavota,Massimiliano Di Penta*

Main category: cs.SE

TL;DR: 本文推导并评估了特定于开发的代码生成提示优化指南，通过迭代测试驱动方法确定改进项，得出10条指南并让从业者评估，结果对多方面有启示。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明合适的提示工程可帮助开发者改进代码生成提示，但缺乏具体指南，因此本文旨在推导并评估开发特定的提示优化指南。

Method: 采用迭代、测试驱动的方法自动优化代码生成提示，分析结果确定改进项以得出10条指南，邀请50名从业者评估。

Result: 得出10条提示改进指南，从业者对指南的感知有用性与实际使用情况不完全相符。

Conclusion: 研究结果对从业者、教育者和开发LLM辅助软件开发工具的人员有启示。

Abstract: Large Language Models (LLMs) are nowadays extensively used for various types of software engineering tasks, primarily code generation. Previous research has shown how suitable prompt engineering could help developers in improving their code generation prompts. However, so far, there do not exist specific guidelines driving developers towards writing suitable prompts for code generation. In this work, we derive and evaluate development-specific prompt optimization guidelines. First, we use an iterative, test-driven approach to automatically refine code generation prompts, and we analyze the outcome of this process to identify prompt improvement items that lead to test passes. We use such elements to elicit 10 guidelines for prompt improvement, related to better specifying I/O, pre-post conditions, providing examples, various types of details, or clarifying ambiguities. We conduct an assessment with 50 practitioners, who report their usage of the elicited prompt improvement patterns, as well as their perceived usefulness, which does not always correspond to the actual usage before knowing our guidelines. Our results lead to implications not only for practitioners and educators, but also for those aimed at creating better LLM-aided software development tools.

</details>


### [425] [Earth Embeddings as Products: Taxonomy, Ecosystem, and Standardized Access](https://arxiv.org/abs/2601.13134)
*Heng Fang,Adam J. Stewart,Isaac Corley,Xiao Xiang Zhu,Hossein Azizpour*

Main category: cs.SE

TL;DR: 地理空间基础模型计算成本高，预计算嵌入数据产品格式不兼容，论文用三层分类法梳理现状，扩展TorchGeo提供统一API解决问题。


<details>
  <summary>Details</summary>
Motivation: 地理空间基础模型计算成本高，预计算嵌入数据产品缺乏标准化，导致工程瓶颈、无法有效比较和复现模型。

Method: 采用三层分类法（数据、工具和价值）梳理现状，调查现有产品确定互操作性障碍，扩展TorchGeo提供统一API。

Result: 通过将嵌入视为一等地理空间数据集，将下游分析与特定模型工程解耦。

Conclusion: 为更透明和可访问的地球观测工作流程提供了路线图。

Abstract: Geospatial Foundation Models (GFMs) provide powerful representations, but high compute costs hinder their widespread use. Pre-computed embedding data products offer a practical "frozen" alternative, yet they currently exist in a fragmented ecosystem of incompatible formats and resolutions. This lack of standardization creates an engineering bottleneck that prevents meaningful model comparison and reproducibility. We formalize this landscape through a three-layer taxonomy: Data, Tools, and Value. We survey existing products to identify interoperability barriers. To bridge this gap, we extend TorchGeo with a unified API that standardizes the loading and querying of diverse embedding products. By treating embeddings as first-class geospatial datasets, we decouple downstream analysis from model-specific engineering, providing a roadmap for more transparent and accessible Earth observation workflows.

</details>


### [426] [From Human to Machine Refactoring: Assessing GPT-4's Impact on Python Class Quality and Readability](https://arxiv.org/abs/2601.13139)
*Alessandro Midolo,Emiliano Tramontana,Massimiliano Di Penta*

Main category: cs.SE

TL;DR: 本文对基于GPT - 4o的大语言模型驱动的代码重构进行综合实证研究，发现其重构可减少代码异味、改善质量指标，但会降低可读性。


<details>
  <summary>Details</summary>
Motivation: 自动化重构工具实用性有限，大语言模型带来新机遇，但对其重构效果评估存在疑问。

Method: 对ClassEval基准中的100个Python类应用GPT - 4o进行重构，从行为正确性、代码质量和可读性三个角度评估。

Result: GPT - 4o生成的重构代码能保持行为正确，减少代码异味、改善质量指标，但降低了可读性。

Conclusion: 研究给出了大语言模型在自动化软件重构中的能力和局限，为将其集成到实际重构工作流指明方向。

Abstract: Refactoring is a software engineering practice that aims to improve code quality without altering program behavior. Although automated refactoring tools have been extensively studied, their practical applicability remains limited. Recent advances in Large Language Models (LLMs) have introduced new opportunities for automated code refactoring. The evaluation of such an LLM-driven approach, however, leaves unanswered questions about its effects on code quality. In this paper, we present a comprehensive empirical study on LLM-driven refactoring using GPT-4o, applied to 100 Python classes from the ClassEval benchmark. Unlike prior work, our study explores a wide range of class-level refactorings inspired by Fowler's catalog and evaluates their effects from three complementary perspectives: (i) behavioral correctness, verified through unit tests; (ii) code quality, assessed via Pylint, Flake8, and SonarCloud; and (iii) readability, measured using a state-of-the-art readability tool. Our findings show that GPT-4o generally produces behavior-preserving refactorings that reduce code smells and improve quality metrics, albeit at the cost of decreased readability. Our results provide new evidence on the capabilities and limitations of LLMs in automated software refactoring, highlighting directions for integrating LLMs into practical refactoring workflows.

</details>


### [427] [KOCO-BENCH: Can Large Language Models Leverage Domain Knowledge in Software Development?](https://arxiv.org/abs/2601.13240)
*Xue Jiang,Jiaru Qian,Xianjie Shi,Chenjie Li,Hao Zhu,Ziyu Wang,Jielun Zhang,Zheyu Zhao,Kechi Zhang,Jia Li,Wenpin Jiao,Zhi Jin,Ge Li,Yihong Dong*

Main category: cs.SE

TL;DR: 本文提出评估领域特定化方法的新基准KOCO - BENCH，现有基准无法充分评估该方法，实验表明先进大模型有挑战，需更有效方法，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有领域特定代码基准不能评估大语言模型领域特定化方法的有效性，且缺乏开发该方法的明确知识语料库。

Method: 提出KOCO - BENCH基准，包含6个新兴领域、11个软件框架和25个项目，有知识语料库和多粒度评估任务。

Result: KOCO - BENCH对先进大语言模型构成挑战，应用特定化方法改进有限，Claude Code最佳成绩仅34.2%。

Conclusion: 急需更有效的领域特定化方法，开源KOCO - BENCH、评估代码和基线以推动研究。

Abstract: Large language models (LLMs) excel at general programming but struggle with domain-specific software development, necessitating domain specialization methods for LLMs to learn and utilize domain knowledge and data. However, existing domain-specific code benchmarks cannot evaluate the effectiveness of domain specialization methods, which focus on assessing what knowledge LLMs possess rather than how they acquire and apply new knowledge, lacking explicit knowledge corpora for developing domain specialization methods. To this end, we present KOCO-BENCH, a novel benchmark designed for evaluating domain specialization methods in real-world software development. KOCO-BENCH contains 6 emerging domains with 11 software frameworks and 25 projects, featuring curated knowledge corpora alongside multi-granularity evaluation tasks including domain code generation (from function-level to project-level with rigorous test suites) and domain knowledge understanding (via multiple-choice Q&A). Unlike previous benchmarks that only provide test sets for direct evaluation, KOCO-BENCH requires acquiring and applying diverse domain knowledge (APIs, rules, constraints, etc.) from knowledge corpora to solve evaluation tasks. Our evaluations reveal that KOCO-BENCH poses significant challenges to state-of-the-art LLMs. Even with domain specialization methods (e.g., SFT, RAG, kNN-LM) applied, improvements remain marginal. Best-performing coding agent, Claude Code, achieves only 34.2%, highlighting the urgent need for more effective domain specialization methods. We release KOCO-BENCH, evaluation code, and baselines to advance further research at https://github.com/jiangxxxue/KOCO-bench.

</details>


### [428] [SEER: Spectral Entropy Encoding of Roles for Context-Aware Attention-Based Design Pattern Detection](https://arxiv.org/abs/2601.13334)
*Tarik Houichime,Younes El Amrani*

Main category: cs.SE

TL;DR: 本文提出SEER，升级先前方法以检测GoF设计模式，评估显示性能提升，减少误报。


<details>
  <summary>Details</summary>
Motivation: 先前方法缺乏类内角色消歧且统一处理调用边，需改进。

Method: 添加光谱熵角色编码器和时间加权调用上下文，增强模型对角色和重要性的理解。

Result: 在PyDesignNet上评估，宏F1从92.47%提升到93.20%，准确率从92.52%提升到93.98%，减少近20%误报。

Conclusion: SEER性能提升，减少误报，具有鲁棒性、可靠性和可解释性。

Abstract: This paper presents SEER, an upgraded version of our prior method Context Is All You Need for detecting Gang of Four (GoF) design patterns from source code. The earlier approach modeled code as attention-ready sequences that blended lightweight structure with behavioral context; however, it lacked explicit role disambiguation within classes and treated call edges uniformly. SEER addresses these limitations with two principled additions: (i) a spectral-entropy role encoder that derives per-member role embeddings from the Laplacian spectrum of each class's interaction graph, and (ii) a time-weighted calling context that assigns empirically calibrated duration priors to method categories (e.g., constructors, getters/setters, static calls, virtual dispatch, cloning). Together, these components sharpen the model's notion of "who does what" and "how much it matters," while remaining portable across languages with minimal adaptation and fully compatible with Transformer-based sequence encoders. Importantly, SEER does not "force" a win by capacity or data; it nudges the classifier, steering attention toward role-consistent and temporally calibrated signals that matter most. We evaluate SEER on PyDesignNet (1,832 files, 35,000 sequences, 23 GoF patterns) and observe consistent gains over our previous system: macro-F1 increases from 92.47% to 93.20% and accuracy from 92.52% to 93.98%, with macro-precision 93.98% and macro-recall 92.52%. Beyond aggregate metrics, SEER reduces false positives by nearly 20%, a decisive improvement that strengthens its robustness and practical reliability. Moreover, SEER yields interpretable, symbol-level attributions aligned with canonical roles, exhibits robustness under small graph perturbations, and shows stable calibration.

</details>


### [429] [From Completion to Editing: Unlocking Context-Aware Code Infilling via Search-and-Replace Instruction Tuning](https://arxiv.org/abs/2601.13384)
*Jiajun Zhang,Zeyu Cui,Jiaxi Yang,Lei Zhang,Yuheng Jing,Zeyao Ma,Tianyi Bai,Zilei Wang,Qiang Liu,Liang Wang,Binyuan Hui,Junyang Lin*

Main category: cs.SE

TL;DR: 传统代码补全FIM范式有局限，Chat LLMs和通证工作流有问题，提出SRI框架并微调模型，效果好且不影响能力和延迟。


<details>
  <summary>Details</summary>
Motivation: 解决传统Fill-in-the-Middle（FIM）范式在代码补全中无法纠正上下文错误、依赖不安全模型的问题，以及Chat LLMs性能下降和通证工作流延迟高的问题。

Method: 提出Search-and-Replace Infilling（SRI）框架，将通证验证和编辑机制融入单通道推理过程；合成高质量数据集SRI - 200K并微调SRI - Coder系列。

Result: 使用少量数据就让Chat模型补全性能超过基础版本，保留通用编码能力，推理延迟与标准FIM相当。

Conclusion: SRI框架可用于高级自动补全和辅助开发，鼓励开发者社区使用。

Abstract: The dominant Fill-in-the-Middle (FIM) paradigm for code completion is constrained by its rigid inability to correct contextual errors and reliance on unaligned, insecure Base models. While Chat LLMs offer safety and Agentic workflows provide flexibility, they suffer from performance degradation and prohibitive latency, respectively. To resolve this dilemma, we propose Search-and-Replace Infilling (SRI), a framework that internalizes the agentic verification-and-editing mechanism into a unified, single-pass inference process. By structurally grounding edits via an explicit search phase, SRI harmonizes completion tasks with the instruction-following priors of Chat LLMs, extending the paradigm from static infilling to dynamic context-aware editing. We synthesize a high-quality dataset, SRI-200K, and fine-tune the SRI-Coder series. Extensive evaluations demonstrate that with minimal data (20k samples), SRI-Coder enables Chat models to surpass the completion performance of their Base counterparts. Crucially, unlike FIM-style tuning, SRI preserves general coding competencies and maintains inference latency comparable to standard FIM. We empower the entire Qwen3-Coder series with SRI, encouraging the developer community to leverage this framework for advanced auto-completion and assisted development.

</details>


### [430] [A Tool for Automatically Cataloguing and Selecting Pre-Trained Models and Datasets for Software Engineering](https://arxiv.org/abs/2601.13460)
*Alexandra González,Oscar Cerezo,Xavier Franch,Silverio Martínez-Fernández*

Main category: cs.SE

TL;DR: 当前机器学习资产增长快，软件工程师难选合适模型和数据集，提出MLAssetSelection网络应用，有多种功能，还有演示视频。


<details>
  <summary>Details</summary>
Motivation: 机器学习资产快速增长，软件工程师从大型注册表中筛选符合需求的模型和数据集困难且低效，缺乏针对软件工程任务的工具。

Method: 开发名为MLAssetSelection的Web应用程序，实现可配置排行榜、基于需求选择资产、实时自动更新和用户中心功能。

Result: 开发了具备四项关键功能的MLAssetSelection网络应用，还有演示视频。

Conclusion: MLAssetSelection能帮助软件工程师更方便地选择机器学习模型和数据集。

Abstract: The rapid growth of machine learning assets has made it increasingly difficult for software engineers to identify models and datasets that match their specific needs. Browsing large registries, such as Hugging Face, is time-consuming, error-prone, and rarely tailored to Software Engineering (SE) tasks. We present MLAssetSelection, a web application that automatically extracts SE assets and supports four key functionalities: (i) a configurable leaderboard for ranking models across multiple benchmarks and metrics; (ii) requirements-based selection of models and datasets; (iii) real-time automated updates through scheduled jobs that keep asset information current; and (iv) user-centric features including login, personalized asset lists, and configurable alert notifications. A demonstration video is available at https://youtu.be/t6CJ6P9asV4.

</details>


### [431] [Governance Matters: Lessons from Restructuring the data.table OSS Project](https://arxiv.org/abs/2601.13466)
*Pedro Oliveira,Doris Amoakohene,Toby Hocking,Marco Gerosa,Igor Steinmacher*

Main category: cs.SE

TL;DR: 本文以data.table项目为例，研究社区主导的开源软件治理改革，发现改革后新贡献者招募、请求解决时间和贡献者留存率等指标改善，但公平和冲突解决仍有问题。


<details>
  <summary>Details</summary>
Motivation: 许多开源软件项目因非正规或集中式治理面临运营风险，data.table项目存在问题，需解决可扩展性和可持续性问题。

Method: 采用混合方法，结合贡献者调查（n=17）和挖掘项目仓库数据评估治理结构转变的影响。

Result: 改革后新贡献者招募增加200%，拉取请求解决时间从超700天降至不到一周，贡献者留存率提高3倍，社区情绪在透明度等方面改善，但公平和冲突解决仍有担忧。

Conclusion: 该案例研究为开源软件维护者、公司和基金会改善开源软件治理提供实用指导。

Abstract: Open source software (OSS) forms the backbone of industrial data workflows and enterprise systems. However, many OSS projects face operational risks due to informal or centralized governance. This paper presents a practical case study of data.table, a high-performance R package widely adopted in production analytics pipelines, which underwent a community-led governance reform to address scalability and sustainability concerns. Before the reform, data.table faced a growing backlog of unresolved issues and open pull requests, unclear contributor pathways, and bottlenecks caused by reliance on a single core maintainer. In response, the community initiated a redesign of its governance structure. In this paper, we evaluated the impact of this transition through a mixed-methods approach, combining a contributor survey (n=17) with mining project repository data. Our results show that following the reform, the project experienced a 200% increase in new contributor recruitment, a drop in pull request resolution time from over 700 days to under a week, and a 3x increase in contributor retention. Community sentiment improved around transparency, onboarding, and project momentum, though concerns around fairness and conflict resolution remain. This case study provides practical guidance for maintainers, companies, and foundations seeking to enhance OSS governance.

</details>


### [432] [AI IDEs or Autonomous Agents? Measuring the Impact of Coding Agents on Software Development](https://arxiv.org/abs/2601.13597)
*Shyam Agarwal,Hao He,Bogdan Vasilescu*

Main category: cs.SE

TL;DR: 研究大语言模型编码代理在开源仓库的采用情况，发现其仅在首次使用时带来速度提升，质量风险持续，强调需保障质量和权衡加速与可维护性。


<details>
  <summary>Details</summary>
Motivation: 明确大语言模型编码代理在软件项目中的实际影响，特别是与IDE - 基于AI助手对比。

Method: 采用交错双重差分法和匹配控制，对开源仓库中代理采用进行纵向因果研究，使用AIDev数据集，分析每月仓库层面开发速度和软件质量结果。

Result: 代理首次作为项目中可观察AI工具时带来前期速度大幅提升；有IDE使用经验的仓库速度收益小或短暂；质量风险持续，静态分析警告和认知复杂度分别上升约18%和35%。

Conclusion: AI辅助有收益递减现象，需质量保障、来源追踪和选择性部署自主代理，为理解AI工具交互和研究平衡加速与可维护性奠定基础。

Abstract: Large language model (LLM)-based coding agents increasingly act as autonomous contributors that generate and merge pull requests, yet their real-world effects on software projects are unclear, especially relative to widely adopted IDE-based AI assistants. We present a longitudinal causal study of agent adoption in open-source repositories using staggered difference-in-differences with matched controls. Using the AIDev dataset, we define adoption as the first agent-generated pull request and analyze monthly repository-level outcomes spanning development velocity (commits, lines added) and software quality (static-analysis warnings, cognitive complexity, duplication, and comment density). Results show large, front-loaded velocity gains only when agents are the first observable AI tool in a project; repositories with prior AI IDE usage experience minimal or short-lived throughput benefits. In contrast, quality risks are persistent across settings, with static-analysis warnings and cognitive complexity rising roughly 18% and 35%, indicating sustained agent-induced complexity debt even when velocity advantages fade. These heterogeneous effects suggest diminishing returns to AI assistance and highlight the need for quality safeguards, provenance tracking, and selective deployment of autonomous agents. Our findings establish an empirical basis for understanding how agentic and IDE-based tools interact, and motivate research on balancing acceleration with maintainability in AI-integrated development workflows.

</details>


### [433] [Why Does the LLM Stop Computing: An Empirical Study of User-Reported Failures in Open-Source LLMs](https://arxiv.org/abs/2601.13655)
*Guangba Yu,Zirui Wang,Yujie Huang,Renyi Zhong,Yuedong Zhong,Yilun Wang,Michael R. Lyu*

Main category: cs.SE

TL;DR: 对开源大语言模型705个现实故障进行大规模实证研究，揭示可靠性瓶颈转移，识别三个关键现象并提供提升可靠性的指导。


<details>
  <summary>Details</summary>
Motivation: 开源大语言模型的民主化使本地部署面临可靠性盲点，需研究解决。

Method: 对开源DeepSeek、Llama和Qwen生态系统中的705个现实故障进行大规模实证研究。

Result: 发现可靠性瓶颈从模型算法缺陷转移到部署栈的系统脆弱性，识别出诊断分歧、系统同质性、生命周期升级三个关键现象。

Conclusion: 研究结果为提升大语言模型可靠性提供了可操作的指导。

Abstract: The democratization of open-source Large Language Models (LLMs) allows users to fine-tune and deploy models on local infrastructure but exposes them to a First Mile deployment landscape. Unlike black-box API consumption, the reliability of user-managed orchestration remains a critical blind spot. To bridge this gap, we conduct the first large-scale empirical study of 705 real-world failures from the open-source DeepSeek, Llama, and Qwen ecosystems.
  Our analysis reveals a paradigm shift: white-box orchestration relocates the reliability bottleneck from model algorithmic defects to the systemic fragility of the deployment stack. We identify three key phenomena: (1) Diagnostic Divergence: runtime crashes distinctively signal infrastructure friction, whereas incorrect functionality serves as a signature for internal tokenizer defects. (2) Systemic Homogeneity: Root causes converge across divergent series, confirming reliability barriers are inherent to the shared ecosystem rather than specific architectures. (3) Lifecycle Escalation: Barriers escalate from intrinsic configuration struggles during fine-tuning to compounded environmental incompatibilities during inference. Supported by our publicly available dataset, these insights provide actionable guidance for enhancing the reliability of the LLM landscape.

</details>


### [434] [CodeContests-O: Powering LLMs via Feedback-Driven Iterative Test Case Generation](https://arxiv.org/abs/2601.13682)
*Jianfeng Cai,Jinhua Zhu,Ruopei Sun,Kangwen Zhao,Dongyun Xue,Mingxiao Feng,Wengang Zhou,Houqiang Li*

Main category: cs.SE

TL;DR: 提出反馈驱动迭代框架构建测试用例，生成CodeContests - O数据集，效果良好并开源代码和数据集。


<details>
  <summary>Details</summary>
Motivation: 推理模型需要大规模可验证数据，编程任务是理想来源，但竞赛编程平台高质量测试用例稀缺，现有合成方法生成的测试用例多样性不足。

Method: 利用大语言模型生成初始测试用例，用已知正确和错误的解决方案执行测试，将失败结果作为反馈指导大语言模型优化测试用例。

Result: CodeContests - O数据集在评估中真阳性率达89.37%，真阴性率达90.89%，显著优于其他数据集；微调Qwen2.5 - 7B模型在LiveCodeBench上提升9.52%。

Conclusion: 框架有效，CodeContests - O数据集质量高，开源代码和数据集支持可重复性研究。

Abstract: The rise of reasoning models necessitates large-scale verifiable data, for which programming tasks serve as an ideal source. However, while competitive programming platforms provide abundant problems and solutions, high-quality test cases for verification remain scarce. Existing approaches attempt to synthesize test cases using Large Language Models (LLMs), but rely solely on the model's intrinsic generation capabilities without external feedback, frequently resulting in insufficiently diverse cases. To address this limitation, we propose a $\textbf{Feedback-Driven Iterative Framework}$ for comprehensive test case construction. Specifically, our method leverages the LLM to generate initial test cases, executes them against known correct and incorrect solutions, and utilizes the failed results as feedback to guide the LLM in refining the test cases toward high fidelity and discriminability. We then apply this method to the CodeContests dataset to construct an optimized high-quality derivative, $\textbf{CodeContests-O}$. Evaluating against the entire pool of solutions ($1.1 \times 10^7$ in total), our dataset achieves an average True Positive Rate (TPR) of $89.37\%$ and True Negative Rate (TNR) of $90.89\%$, significantly outperforming the CodeContests and CodeContests+ by margins of $4.32\%$ and $9.37\%$, respectively. Furthermore, fine-tuning the Qwen2.5-7B model on CodeContests-O results in a $9.52\%$ improvement on LiveCodeBench (Pass@1). Experiments demonstrate the effectiveness of our framework and the quality of CodeContests-O. To support reproducibility and facilitate future research, we release the $\href{https://github.com/cai-jianfeng/CodeContests-O}{code}$ and $\href{https://huggingface.co/datasets/caijanfeng/CodeContests-O}{dataset}$.

</details>


### [435] [A Blockchain-Oriented Software Engineering Architecture for Carbon Credit Certification Systems](https://arxiv.org/abs/2601.13772)
*Matteo Vaccargiu,Azmat Ullah,Pierluigi Gallo*

Main category: cs.SE

TL;DR: 本文介绍基于区块链的碳信用认证架构，通过光伏案例展示，可生成可验证碳信用记录并支持第三方验证。


<details>
  <summary>Details</summary>
Motivation: 现有的工作对碳信用认证流程支持有限，特别是针对中小型可再生能源装置。

Method: 引入基于区块链的碳信用认证架构，集成实时物联网数据收集、边缘级聚合和在有许可的区块链上通过智能合约进行安全链上存储。

Result: 该架构为生成可验证的碳信用记录提供了结构化途径，并支持第三方验证。

Conclusion: 该架构符合欧洲立法和自愿碳市场标准，明确光伏运营商的实际要求和限制。

Abstract: Carbon credit systems have emerged as a policy tool to incentivize emission reductions and support the transition to clean energy. Reliable carbon-credit certification depends on mechanisms that connect actual, measured renewable-energy production to verifiable emission-reduction records. Although blockchain and IoT technologies have been applied to emission monitoring and trading, existing work offers limited support for certification processes, particularly for small and medium-scale renewable installations. This paper introduces a blockchain-based carbon-credit certification architecture, demonstrated through a 100 kWp photovoltaic case study, that integrates real-time IoT data collection, edge-level aggregation, and secure on-chain storage on a permissioned blockchain with smart contracts. Unlike approaches focused on trading mechanisms, the proposed system aligns with European legislation and voluntary carbon-market standards, clarifying the practical requirements and constraints that apply to photovoltaic operators. The resulting architecture provides a structured pathway for generating verifiable carbon-credit records and supporting third-party verification.

</details>


### [436] [SWE-Tester: Training Open-Source LLMs for Issue Reproduction in Real-World Repositories](https://arxiv.org/abs/2601.13713)
*Aditya Bharat Soni,Rajat Ghosh,Vaishnavi Bhargava,Valerie Chen,Debojyoti Dutta*

Main category: cs.SE

TL;DR: 提出SWE - Tester训练开源大语言模型生成问题复现测试，在数据集训练后模型有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有从自然语言问题描述自动生成问题复现测试的方法多依赖闭源大语言模型，对开源模型探索有限。

Method: 创建包含4.1万个实例的高质量训练数据集，从2600个开源GitHub仓库获取数据，用其训练不同规模和家族的大语言模型。

Result: 微调后的模型在SWT - Bench Verified上成功率绝对提升达10%，变更覆盖率达21%，且增加推理时计算量、数据量和使用更大模型有持续改进。

Conclusion: 所提框架能有效推动该领域开源大语言模型发展。

Abstract: Software testing is crucial for ensuring the correctness and reliability of software systems. Automated generation of issue reproduction tests from natural language issue descriptions enhances developer productivity by simplifying root cause analysis, promotes test-driven development -- "test first, write code later", and can be used for improving the effectiveness of automated issue resolution systems like coding agents. Existing methods proposed for this task predominantly rely on closed-source LLMs, with limited exploration of open models. To address this, we propose SWE-Tester -- a novel pipeline for training open-source LLMs to generate issue reproduction tests. First, we curate a high-quality training dataset of 41K instances from 2.6K open-source GitHub repositories and use it to train LLMs of varying sizes and families. The fine-tuned models achieve absolute improvements of up to 10\% in success rate and 21\% in change coverage on SWT-Bench Verified. Further analysis shows consistent improvements with increased inference-time compute, more data, and larger models. These results highlight the effectiveness of our framework for advancing open-source LLMs in this domain.

</details>


### [437] [Counterexample Classification against Signal Temporal Logic Specifications](https://arxiv.org/abs/2601.13743)
*Zhenya Zhang,Parv Kapoor,Jie An,Eunsuk Kang*

Main category: cs.SE

TL;DR: 本文提出使用PSTL表示类的反例分类标准，推导类间包含关系并提出类似二分查找的方法提高类识别效率，还进行了实验评估。


<details>
  <summary>Details</summary>
Motivation: 实际中STL规范的反例可能源于不同原因，需合适的分类标准理解可能的违反模式和反例分布。

Method: 使用PSTL表示每个类，推导类间包含关系并提出类似二分查找的方法。

Result: 实现了原型工具并在两个广泛研究的系统上实验评估。

Conclusion: 提出的分类标准和类识别方法有效，能提高效率。

Abstract: Signal Temporal Logic (STL) has been widely adopted as a specification language for specifying desirable behaviors of hybrid systems. By monitoring a given STL specification, we can detect the executions that violate it, which are often referred to as counterexamples. In practice, these counterexamples may arise from different causes and thus are relevant to different system defects. To effectively address this, we need a proper criterion for classifying these counterexamples, by which we can comprehend the possible violation patterns and the distributions of these counterexamples with respect to the patterns. In this paper, we propose a classification criterion by using parametric signal temporal logic (PSTL) to represent each class. Due to this formalism, identifying the classes of a counterexample requires finding proper parameter values of PSTL that enable a class to include the counterexample. To improve the efficiency of class identification, we further derive an inclusion relation between different classes, and then propose a binary search-like approach over it that significantly prunes the classes needed to query. We implement a prototype tool and experimentally evaluate its effectiveness on two widely-studied systems.

</details>


### [438] [On Autopilot? An Empirical Study of Human-AI Teaming and Review Practices in Open Source](https://arxiv.org/abs/2601.13754)
*Haoyu Gao,Peerachai Banyongrakkul,Hao Guan,Mansooreh Zahedi,Christoph Treude*

Main category: cs.SE

TL;DR: 研究大语言模型在开源软件开发中，开发者与AI辅助PR的交互模式，发现多数AI合作PR来自无代码所有权者，且合并快、反馈少。


<details>
  <summary>Details</summary>
Motivation: 现有研究对开发者在开源软件中与AI辅助PR的交互模式探索不足。

Method: 扩展AIDev数据集，纳入更细粒度的贡献者代码所有权和人工创建PR的对比基线。

Result: 超67.5%的AI合作PR来自无代码所有权贡献者；多数仓库缺乏AI编码代理使用指南；AI合作PR合并快、反馈少，非所有者创建的AI合作PR约80%无明确审查就合并。

Conclusion: 讨论了研究结果对开发者和研究者的影响。

Abstract: Large Language Models (LLMs) increasingly automate software engineering tasks. While recent studies highlight the accelerated adoption of ``AI as a teammate'' in Open Source Software (OSS), developer interaction patterns remain under-explored. In this work, we investigated project-level guidelines and developers' interactions with AI-assisted pull requests (PRs) by expanding the AIDev dataset to include finer-grained contributor code ownership and a comparative baseline of human-created PRs. We found that over 67.5\% of AI-co-authored PRs originate from contributors without prior code ownership. Despite this, the majority of repositories lack guidelines for AI-coding agent usage. Notably, we observed a distinct interaction pattern: AI-co-authored PRs are merged significantly faster with minimal feedback. In contrast to human-created PRs where non-owner developers receive the most feedback, AI-co-authored PRs from non-owners receive the least, with approximately 80\% merged without any explicit review. Finally, we discuss implications for developers and researchers.

</details>


### [439] [Multi-Location Software Model Completion](https://arxiv.org/abs/2601.13894)
*Alisa Welter,Christof Tinnes,Sven Apel*

Main category: cs.SE

TL;DR: 本文提出NextFocus用于多位置模型完成，在实际项目评估中表现良好，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的软件模型完成方法仅支持单位置，本文旨在处理大型复杂模型中跨多位置的协调变更。

Method: 提出基于全局嵌入的NextFocus预测器，由带注意力机制的神经网络构成，在历史软件模型演化数据上训练。

Result: 在实际项目的多位置模型变更评估中，NextFocus取得良好结果，k ≤ 10时平均Precision@k分数达0.98，显著优于三个基线方法。

Conclusion: NextFocus能有效实现多位置模型完成，即便变更在模型中广泛分布。

Abstract: In model-driven engineering and beyond, software models are key development artifacts. In practice, they often grow to substantial size and complexity, undergoing thousands of modifications over time due to evolution, refactoring, and maintenance. The rise of AI has sparked interest in how software modeling activities can be automated. Recently, LLM-based approaches for software model completion have been proposed, however, the state of the art supports only single-location model completion by predicting changes at a specific location. Going beyond, we aim to bridge the gap toward handling coordinated changes that span multiple locations across large, complex models. Specifically, we propose a novel global embedding-based next focus predictor, NextFocus, which is capable of multi-location model completion for the first time. The predictor consists of a neural network with an attention mechanism that is trained on historical software model evolution data. Starting from an existing change, it predicts further model elements to change, potentially spanning multiple parts of the model. We evaluate our approach on multi-location model changes that have actually been performed by developers in real-world projects. NextFocus achieves promising results for multi-location model completion, even when changes are heavily spread across the model. It achieves an average Precision@k score of 0.98 for $k \leq 10$, significantly outperforming the three baseline approaches.

</details>


### [440] [VulnResolver: A Hybrid Agent Framework for LLM-Based Automated Vulnerability Issue Resolution](https://arxiv.org/abs/2601.13933)
*Mingming Zhang,Xu Wang,Jian Zhang,Xiangxin Meng,Jiayi Zhang,Chunming Hu*

Main category: cs.SE

TL;DR: 随着软件系统复杂度增加，安全漏洞问题严峻。现有AVR方法依赖人工标注且忽略问题报告语义。本文提出基于LLM的VulnResolver框架用于自动解决漏洞问题，评估显示其表现出色。


<details>
  <summary>Details</summary>
Motivation: 软件系统复杂度增加使安全漏洞频发，现有自动漏洞修复方法依赖难以获取的人工标注，且忽略开发者问题报告中的语义信息。

Method: 提出VulnResolver，通过上下文预收集代理（CPCAgent）和安全属性分析代理（SPAAgent）两个专门代理，将自主代理的适应性与工作流引导修复的稳定性相结合，产生结构化分析结果。

Result: 在SEC - bench基准测试中，VulnResolver解决了SEC - bench Lite中75%的问题，表现最佳；在SEC - bench Full中也显著优于最强基线OpenHands。

Conclusion: VulnResolver提供了一个自适应且安全感知的框架，通过工作流稳定性和代理的上下文推理及基于属性分析能力，推动了端到端的自动化漏洞问题解决。

Abstract: As software systems grow in complexity, security vulnerabilities have become increasingly prevalent, posing serious risks and economic costs. Although automated detection tools such as fuzzers have advanced considerably, effective resolution still often depends on human expertise. Existing automated vulnerability repair (AVR) methods rely heavily on manually provided annotations (e.g., fault locations or CWE labels), which are often difficult and time-consuming to obtain, while overlooking the rich, naturally embedded semantic context found in issue reports from developers.
  In this paper, we present VulnResolver, the first LLM-based hybrid agent framework for automated vulnerability issue resolution. VulnResolver unites the adaptability of autonomous agents with the stability of workflow-guided repair through two specialized agents. The Context Pre-Collection Agent (CPCAgent) adaptively explores the repository to gather dependency and contextual information, while the Safety Property Analysis Agent (SPAAgent) generates and validates the safety properties violated by vulnerabilities. Together, these agents produce structured analyses that enrich the original issue reports, enabling more accurate vulnerability localization and patch generation.
  Evaluations on the SEC-bench benchmark show that VulnResolver resolves 75% of issues on SEC-bench Lite, achieving the best resolution performance. On SEC-bench Full, VulnResolver also significantly outperforms the strongest baseline, the agent-based OpenHands, confirming its effectiveness. Overall, VulnResolver delivers an adaptive and security-aware framework that advances end-to-end automated vulnerability issue resolution through workflow stability and the specialized agents' capabilities in contextual reasoning and property-based analysis.

</details>


### [441] [RepoGenesis: Benchmarking End-to-End Microservice Generation from Readme to Repository](https://arxiv.org/abs/2601.13943)
*Zhiyuan Peng,Xin Yin,Pu Zhao,Fangkai Yang,Lu Wang,Ran Jia,Xu Chen,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.SE

TL;DR: 现有代码生成基准忽略完整微服务仓库生成，本文引入RepoGenesis基准评估，结果显示系统在Pass@1指标表现不佳，且展示了RepoGenesis的质量，最后发布该基准。


<details>
  <summary>Details</summary>
Motivation: 现有基准忽视完整微服务仓库生成，不能反映真实的从0到1开发流程，需新基准。

Method: 引入RepoGenesis基准，包含多语言多领域仓库，用Pass@1、API Coverage、Deployment Success Rate评估开源代理和商业IDE。

Result: 系统虽有高AC和DSR，但Pass@1低，暴露架构、依赖管理和跨文件一致性问题，GenesisAgent - 8B微调后性能与GPT - 5 mini相当。

Conclusion: RepoGenesis可推动微服务生成发展，已公开该基准。

Abstract: Large language models and agents have achieved remarkable progress in code generation. However, existing benchmarks focus on isolated function/class-level generation (e.g., ClassEval) or modifications to existing codebases (e.g., SWE-Bench), neglecting complete microservice repository generation that reflects real-world 0-to-1 development workflows. To bridge this gap, we introduce RepoGenesis, the first multilingual benchmark for repository-level end-to-end web microservice generation, comprising 106 repositories (60 Python, 46 Java) across 18 domains and 11 frameworks, with 1,258 API endpoints and 2,335 test cases verified through a "review-rebuttal" quality assurance process. We evaluate open-source agents (e.g., DeepCode) and commercial IDEs (e.g., Cursor) using Pass@1, API Coverage (AC), and Deployment Success Rate (DSR). Results reveal that despite high AC (up to 73.91%) and DSR (up to 100%), the best-performing system achieves only 23.67% Pass@1 on Python and 21.45% on Java, exposing deficiencies in architectural coherence, dependency management, and cross-file consistency. Notably, GenesisAgent-8B, fine-tuned on RepoGenesis (train), achieves performance comparable to GPT-5 mini, demonstrating the quality of RepoGenesis for advancing microservice generation. We release our benchmark at https://github.com/pzy2000/RepoGenesis.

</details>


### [442] [Software Testing in the Quantum World](https://arxiv.org/abs/2601.13996)
*Rui Abreu,Shaukat Ali,Paolo Arcaini,Jose Campos,Michael Felderer,Claude Gravel,Fuyuki Ishikawa,Stefan Klikovits,Andriy Miranskyy,Mohammad Mousavi,Masaomi Yamaguchi,Lei Zhang,Jianjun Zhao,Anila Mjeda*

Main category: cs.SE

TL;DR: 量子计算在多领域有加速优势，经典模拟随量子软件复杂度增加变得不可行，本文提出大规模量子软件测试挑战及软件工程解决视角。


<details>
  <summary>Details</summary>
Motivation: 随着量子软件复杂度增加，传统的量子计算机经典模拟用于质量保证变得不可行，需要新的质量保证方法。

Method: 从软件工程的视角来应对。

Result: 提出了在测试大规模量子软件时面临的关键挑战。

Conclusion: 可通过软件工程视角解决大规模量子软件测试的挑战。

Abstract: Quantum computing offers significant speedups for simulating physical, chemical, and biological systems, and for optimization and machine learning. As quantum software grows in complexity, the classical simulation of quantum computers, which has long been essential for quality assurance, becomes infeasible. This shift requires new quality-assurance methods that operate directly on real quantum computers. This paper presents the key challenges in testing large-scale quantum software and offers software engineering perspectives for addressing them.

</details>


### [443] [Analyzing the Availability of E-Mail Addresses for PyPI Libraries](https://arxiv.org/abs/2601.14034)
*Alexandros Tsakpinis,Alexander Pretschner*

Main category: cs.SE

TL;DR: 本文对686,034个Python库及相关GitHub仓库中的维护者邮件信息进行实证分析，发现多数库包含有效邮件地址，同时指出改进空间。


<details>
  <summary>Details</summary>
Motivation: 开源软件库的长期可持续性依赖维护者的可联系性，因此分析其联系信息可用性。

Method: 对Python Package Index (PyPI)上的686,034个Python库及其关联GitHub仓库中的维护者邮件地址进行实证分析，检验提供方式、评估有效性、考察覆盖情况。

Result: 81.6%的库包含至少一个有效邮件地址，PyPI是主要来源；97.8%的直接依赖和97.7%的传递依赖提供有效联系信息；发现超698,000个无效条目，主要因字段缺失。

Conclusion: 生态系统中维护者可联系性强，但有改进空间，如提供更明确指导和引入可选验证机制。

Abstract: Open Source Software (OSS) libraries form the backbone of modern software systems, yet their long-term sustainability often depends on maintainers being reachable for support, coordination, and security reporting. In this paper, we empirically analyze the availability of contact information - specifically e-mail addresses - across 686,034 Python libraries on the Python Package Index (PyPI) and their associated GitHub repositories. We examine how and where maintainers provide this information, assess its validity, and explore coverage across individual libraries and their dependency chains. Our findings show that 81.6% of libraries include at least one valid e-mail address, with PyPI serving as the primary source (79.5%). When analyzing dependency chains, we observe that up to 97.8% of direct and 97.7% of transitive dependencies provide valid contact information. At the same time, we identify over 698,000 invalid entries, primarily due to missing fields. These results demonstrate strong maintainer reachability across the ecosystem, while highlighting opportunities for improvement - such as offering clearer guidance to maintainers during the packaging process and introducing opt-in validation mechanisms for existing e-mail addresses.

</details>


### [444] [Feature-Aware Test Generation for Deep Learning Models](https://arxiv.org/abs/2601.14081)
*Xingcheng Chen,Oliver Weissl,Andrea Stocco*

Main category: cs.SE

TL;DR: 本文提出Detect框架用于视觉深度学习模型的测试用例生成，通过扰动潜空间特征生成高质量测试用例，揭示模型行为并发现问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于生成AI的测试生成方法在提供语义洞察和细粒度语义可控性方面存在局限。

Method: Detect框架在潜空间中扰动解纠缠的语义属性来生成输入，通过控制扰动单个潜在特征观察对模型输出的影响，利用视觉语言模型进行语义归因，针对泛化性和鲁棒性进行特征感知的扰动。

Result: 在图像分类和检测任务中，Detect能生成细粒度控制的高质量测试用例，揭示不同架构模型的捷径行为和精度指标未捕捉到的错误，在决策边界发现和识别鲁棒性失败方面优于现有方法。

Conclusion: 可解释和特征感知的测试对提高深度学习模型的可靠性有重要价值。

Abstract: As deep learning models are widely used in software systems, test generation plays a crucial role in assessing the quality of such models before deployment. To date, the most advanced test generators rely on generative AI to synthesize inputs; however, these approaches remain limited in providing semantic insight into the causes of misbehaviours and in offering fine-grained semantic controllability over the generated inputs. In this paper, we introduce Detect, a feature-aware test generation framework for vision-based deep learning (DL) models that systematically generates inputs by perturbing disentangled semantic attributes within the latent space. Detect perturbs individual latent features in a controlled way and observes how these changes affect the model's output. Through this process, it identifies which features lead to behavior shifts and uses a vision-language model for semantic attribution. By distinguishing between task-relevant and irrelevant features, Detect applies feature-aware perturbations targeted for both generalization and robustness. Empirical results across image classification and detection tasks show that Detect generates high-quality test cases with fine-grained control, reveals distinct shortcut behaviors across model architectures (convolutional and transformer-based), and bugs that are not captured by accuracy metrics. Specifically, Detect outperforms a state-of-the-art test generator in decision boundary discovery and a leading spurious feature localization method in identifying robustness failures. Our findings show that fully fine-tuned convolutional models are prone to overfitting on localized cues, such as co-occurring visual traits, while weakly supervised transformers tend to rely on global features, such as environmental variances. These findings highlight the value of interpretable and feature-aware testing in improving DL model reliability.

</details>


### [445] [Practitioner Views on Mobile App Accessibility: Practices and Challenges](https://arxiv.org/abs/2601.14131)
*Amila Indika,Rick Kazman,Anthony Peruma*

Main category: cs.SE

TL;DR: 对43个国家的110名移动应用开发者进行混合方法调查，研究平台生态和开发者经验对无障碍实践的影响，发现开发者虽重视但实践有问题，给出促进应用开发的建议。


<details>
  <summary>Details</summary>
Motivation: 此前缺乏跨平台、全球代表性的开发者落实无障碍实践的见解，需研究从业者如何在实践中处理无障碍问题。

Method: 对来自43个国家的110名移动应用开发者进行混合方法调查。

Result: 开发者认可无障碍重要性，但依靠特定平台指南、后期进行合规测试，主要实现文本相关功能，受API限制和组织约束，不同平台和经验水平的开发者实践有差异。

Conclusion: 为实践中实现无障碍面临的挑战提供新见解，并为各方促进更一致和包容性的应用开发提供可行步骤。

Abstract: As mobile applications (apps) become ubiquitous in everyday life, it is crucial for developers to prioritize accessibility for users with diverse abilities. While previous research has identified widespread accessibility issues and raised awareness of developer challenges, there remains a lack of cross-platform, globally representative insights into how practitioners approach accessibility in practice. This paper presents findings from a mixed-methods survey of 110 mobile app developers across 43 countries, examining how platform ecosystems (iOS vs. Android) and developer experience shape accessibility practices. Results show that while developers recognize the importance of accessibility, they often rely on platform-specific guidelines and typically perform compliance testing late in the development process. Developers primarily implement text-focused features while struggling with API limitations and organizational constraints. Through systematic cross-platform comparison, we identify novel platform-specific barriers and demonstrate how accessibility practices differ across developer experience levels. Our findings offer new insights into the challenges of achieving accessibility in practice and provide actionable steps for various stakeholders to promote more consistent and inclusive app development.

</details>


### [446] [Toward self-coding information systems](https://arxiv.org/abs/2601.14132)
*Rodrigo Falcão,Frank Elberzhager,Karthik Vaidhyanathan*

Main category: cs.SE

TL;DR: 提出自编码信息系统这一代理AI领域新研究主题，介绍动机、定义、影响和研究方向


<details>
  <summary>Details</summary>
Motivation: 为了减少新特性的上市时间，提出自编码信息系统的研究

Method: 给出自编码信息系统的正式定义

Result: 无明确提及具体研究成果

Conclusion: 指明了自编码信息系统这一领域的潜在研究方向

Abstract: In this extended abstract, we propose a novel research topic in the field of agentic AI, which we refer to as self-coding information systems. These systems will be able to dynamically adapt their structure or behavior by evaluating potential adaptation decisions, generate source code, test, and (re)deploy their source code autonomously, at runtime, reducing the time to market of new features. Here we motivate the topic, provide a formal definition of self-coding information systems, discuss some expected impacts of the new technology, and indicate potential research directions.

</details>


### [447] [An Empirical Study on Remote Code Execution in Machine Learning Model Hosting Ecosystems](https://arxiv.org/abs/2601.14163)
*Mohammed Latif Siddiq,Tanzim Hossain Romel,Natalie Sekerak,Beatrice Casey,Joanna C. S. Santos*

Main category: cs.SE

TL;DR: 对五大模型共享平台的自定义模型加载实践进行大规模实证研究，发现不安全的默认设置、平台安全执行不均等问题，并给出设计更安全模型共享基础设施的建议。


<details>
  <summary>Details</summary>
Motivation: 模型共享平台在加载模型时执行不可信代码存在安全隐患，需评估其普遍性、风险和开发者看法。

Method: 量化需要自定义代码的模型频率，使用Bandit、CodeQL和Semgrep进行静态分析，用YARA识别恶意模式，系统分析平台文档、API设计和安全机制，定性分析开发者讨论。

Result: 发现普遍依赖不安全默认设置、平台安全执行不均衡以及开发者对执行远程代码影响的持续困惑。

Conclusion: 给出设计更安全模型共享基础设施的可行建议，平衡未来AI生态系统的可用性和安全性。

Abstract: Model-sharing platforms, such as Hugging Face, ModelScope, and OpenCSG, have become central to modern machine learning development, enabling developers to share, load, and fine-tune pre-trained models with minimal effort. However, the flexibility of these ecosystems introduces a critical security concern: the execution of untrusted code during model loading (i.e., via trust_remote_code or trust_repo). In this work, we conduct the first large-scale empirical study of custom model loading practices across five major model-sharing platforms to assess their prevalence, associated risks, and developer perceptions. We first quantify the frequency with which models require custom code to function and identify those that execute arbitrary Python files during loading. We then apply three complementary static analysis tools: Bandit, CodeQL, and Semgrep, to detect security smells and potential vulnerabilities, categorizing our findings by CWE identifiers to provide a standardized risk taxonomy. We also use YARA to identify malicious patterns and payload signatures. In parallel, we systematically analyze the documentation, API design, and safety mechanisms of each platform to understand their mitigation strategies and enforcement levels. Finally, we conduct a qualitative analysis of over 600 developer discussions from GitHub, Hugging Face, and PyTorch Hub forums, as well as Stack Overflow, to capture community concerns and misconceptions regarding security and usability. Our findings reveal widespread reliance on unsafe defaults, uneven security enforcement across platforms, and persistent confusion among developers about the implications of executing remote code. We conclude with actionable recommendations for designing safer model-sharing infrastructures and striking a balance between usability and security in future AI ecosystems.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [448] [On the Order Between the Standard Deviation and Gini Mean Difference](https://arxiv.org/abs/2601.12414)
*Nawaf Mohammed*

Main category: q-fin.RM

TL;DR: 研究标准差（SD）和基尼平均差（GMD）的大小顺序，给出支配关系的充分条件，证明其在多种变换下保持不变并扩展到离散分布，为理解离散排序和选择变异性度量提供框架。


<details>
  <summary>Details</summary>
Motivation: 明确标准差和基尼平均差之间的大小关系，为风险敏感应用中变异性度量的选择提供指导。

Method: 将SD和GMD用成对差异表示，与独立同分布副本绝对差异的平均超额函数联系，利用可靠性和生存分析工具。

Result: 重尾分布下SD占优，轻尾分布下GMD占优，支配关系在多种变换下保持不变，并扩展到离散分布。

Conclusion: 研究结果为理解离散排序提供统一框架，为风险敏感应用中变异性度量的选择提供清晰指导。

Abstract: In this paper, we study the order between the standard deviation (SD) and the Gini mean difference (GMD) and derive sharp, interpretable sufficient conditions under which one exceeds the other. By expressing both the SD and the GMD in terms of pairwise differences and linking their comparison to the mean excess function of the absolute difference of two i.i.d.\ copies, we reduce the problem to structural properties of the underlying distribution. Using tools from reliability and survival analysis, we show that SD dominance arises under heavy-tailed regimes, characterized by decreasing hazard rates or increasing reverse hazard rates. Conversely, when both tails are light -- equivalently, when the hazard rate is increasing and the reverse hazard rate is decreasing -- the GMD dominates the SD.
  We further demonstrate that these dominance relations are preserved under affine transformations, mixtures, convolutions, and tail truncation, and we extend the analysis to discrete distributions. Numerous examples illustrate the sharpness of the results and highlight the distinct roles played by tail behavior and distributional regularity. Our findings provide a unified framework for understanding dispersion ordering and offer clear guidance for the choice of variability measures in risk-sensitive applications.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [449] [Latent Variable Phillips Curve](https://arxiv.org/abs/2601.11601)
*Daniil Bargman,Francesca Medda,Akash Sedai Sharma*

Main category: q-fin.ST

TL;DR: 本文重新检验经验菲利普斯曲线模型及其在中期通胀预测的实用性，发现潜在变量 PC 模型表现更优，纳入 MA(1) 残差过程可提升准确性。


<details>
  <summary>Details</summary>
Motivation: 重新检验经验菲利普斯曲线模型，并探讨其在中期通胀预测中的实用性。

Method: 提出潜在变量菲利普斯曲线假设，使用 3968 个随机生成的因子组合进行测试。

Result: 1983 年第一季度到 2025 年第一季度美国核心 PCE 通胀数据显示，潜在变量 PC 模型在 6 - 8 个季度的预测上优于传统 PC 模型，也更可能超越单变量基准；纳入 MA(1) 残差过程能全面提高经验 PC 模型准确性，但相对单变量模型提升幅度小。

Conclusion: 本研究证实菲利普斯曲线理论的新观点，为提升未来实证研究中菲利普斯曲线预测竞争力提供新途径。

Abstract: This paper re-examines the empirical Phillips curve (PC) model and its usefulness in the context of medium-term inflation forecasting. A latent variable Phillips curve hypothesis is formulated and tested using 3,968 randomly generated factor combinations. Evidence from US core PCE inflation between Q1 1983 and Q1 2025 suggests that latent variable PC models reliably outperform traditional PC models six to eight quarters ahead and stand a greater chance of outperforming a univariate benchmark. Incorporating an MA(1) residual process improves the accuracy of empirical PC models across the board, although the gains relative to univariate models remain small. The findings presented in this paper have two important implications: First, they corroborate a new conceptual view on the Phillips curve theory; second, they offer a novel path towards improving the competitiveness of Phillips curve forecasts in future empirical work.

</details>


### [450] [The Physics of Price Discovery: Deconvolving Information, Volatility, and the Critical Breakdown of Signal during Retail Herding](https://arxiv.org/abs/2601.11602)
*Sungwoo Kang*

Main category: q-fin.ST

TL;DR: 本文基于市值归一化能分离知情交易信号的发现，研究信号传递机制及失效原因，发现二元市场结构但不稳定，高散户羊群效应下市场相变，阻碍价格发现。


<details>
  <summary>Details</summary>
Motivation: 研究市值归一化中知情交易信号的传递机制及其失效原因。

Method: 采用Tikhonov正则化反卷积恢复投资者资金流的脉冲响应核，并用多元Hawkes过程分析市场结构稳定性。

Result: 发现二元市场结构，外国和机构投资者是价格发现的‘建筑师’，个人投资者是流动性提供者；个人投资者订单流接近临界自激发，高散户羊群效应下市场相变，信号噪声比崩溃，老练投资者价格影响反转。

Conclusion: 散户传染效应成为暂时阻碍有效价格发现的物理屏障。

Abstract: Building on the finding that Market Cap Normalization ($\SMC$) isolates the ``pure'' directional signal of informed trading \citep{kang2025}, this paper investigates the physics of how that signal is transmitted -- and how it breaks down. We employ \textbf{Tikhonov-regularized deconvolution} to recover the impulse response kernels of investor flows, revealing a dual-channel market structure: Foreign and Institutional investors act as ``architects'' of price discovery (positive permanent impact), while Individual investors act as liquidity providers (negative total impact). However, using \textbf{Multivariate Hawkes Processes}, we demonstrate that this structure is fragile. We find that individual investor order flow exhibits near-critical self-excitation (Branching Ratio $\approx$ 0.998). During periods of high retail herding, the market undergoes a \textbf{phase transition} into a ``critical state.'' In this regime, the signal-to-noise ratio collapses, causing the price impact of sophisticated investors to reverse from positive to negative. These findings suggest that retail contagion acts as a physical barrier that temporarily disables efficient price discovery.

</details>


### [451] [Distributional Fitting and Tail Analysis of Lead-Time Compositions: Nights vs. Revenue on Airbnb](https://arxiv.org/abs/2601.12175)
*Harrison E. Katz,Jess Needleman,Liz Medina*

Main category: q-fin.ST

TL;DR: 分析Airbnb两个需求指标的每日提前期分布，发现预订量和预订收入提前期形状有差异等结论。


<details>
  <summary>Details</summary>
Motivation: 研究Airbnb中预订量和预订收入两个需求指标的每日提前期分布情况。

Method: 将每日在0 - 365天的分配视为成分向量分析，用Gamma、Weibull、Lognormal分布拟合，用非参数GAMs，进行广义Pareto拟合和Bai - Perron检验。

Result: GBV在中期视野更集中；Gamma和Weibull分布拟合较好；广义Pareto拟合显示阈值下150天有界尾；Bai - Perron检验识别出五个结构断点。

Conclusion: 预订量和预订收入提前期形状有系统差异，简单双参数分布可充分捕捉每日概率质量函数，尾部推断在截断边界附近需谨慎。

Abstract: We analyze daily lead-time distributions for two Airbnb demand metrics, Nights Booked (volume) and Gross Booking Value (revenue), treating each day's allocation across 0-365 days as a compositional vector. The data span 2,557 days from January 2019 through December 2025 in a large North American region. Three findings emerge. First, GBV concentrates more heavily in mid-range horizons: beyond 90 days, GBV tail mass typically exceeds Nights by 20-50%, with ratios reaching 75% at the 180-day threshold during peak seasons. Second, Gamma and Weibull distributions fit comparably well under interval-censored cross-entropy. Gamma wins on 61% of days for Nights and 52% for GBV, with Weibull close behind at 38% and 45%. Lognormal rarely wins (<3%). Nonparametric GAMs achieve 18-80x lower CRPS but sacrifice interpretability. Third, generalized Pareto fits suggest bounded tails for both metrics at thresholds below 150 days, though this may partly reflect right-truncation at 365 days; above 150 days, estimates destabilize. Bai-Perron tests with HAC standard errors identify five structural breaks in the Wasserstein distance series, with early breaks coinciding with COVID-19 disruptions. The results show that volume and revenue lead-time shapes diverge systematically, that simple two-parameter distributions capture daily pmfs adequately, and that tail inference requires care near truncation boundaries.

</details>


### [452] [Beyond Visual Realism: Toward Reliable Financial Time Series Generation](https://arxiv.org/abs/2601.12990)
*Fan Zhang,Jiabin Luo,Zheng Zhang,Shuanghong Huang,Zhipeng Liu,Yu Chen*

Main category: q-fin.ST

TL;DR: 现有金融时间序列生成模型在回测中表现不佳，本文提出SFAG模型解决此问题，实验证明其能生成实用数据。


<details>
  <summary>Details</summary>
Motivation: 现有金融时间序列生成模型在交易回测中常失败，根因是忽视金融不对称和尾部事件。

Method: 引入Stylized Facts Alignment GAN (SFAG)，将关键特征事实转化为可微结构约束，并与对抗损失联合优化。

Result: 在对上证指数的实验中，基线GAN模型交易结果不稳定且不可信，而SFAG生成的合成数据保留特征事实，支持动量策略稳健表现。

Conclusion: 在金融生成建模中，保留结构的目标对于弥合表面逼真度和实际可用性之间的差距至关重要。

Abstract: Generative models for financial time series often create data that look realistic and even reproduce stylized facts such as fat tails or volatility clustering. However, these apparent successes break down under trading backtests: models like GANs or WGAN-GP frequently collapse, yielding extreme and unrealistic results that make the synthetic data unusable in practice. We identify the root cause in the neglect of financial asymmetry and rare tail events, which strongly affect market risk but are often overlooked by objectives focusing on distribution matching. To address this, we introduce the Stylized Facts Alignment GAN (SFAG), which converts key stylized facts into differentiable structural constraints and jointly optimizes them with adversarial loss. This multi-constraint design ensures that generated series remain aligned with market dynamics not only in plots but also in backtesting. Experiments on the Shanghai Composite Index (2004--2024) show that while baseline GANs produce unstable and implausible trading outcomes, SFAG generates synthetic data that preserve stylized facts and support robust momentum strategy performance. Our results highlight that structure-preserving objectives are essential to bridge the gap between superficial realism and practical usability in financial generative modeling.

</details>


### [453] [Demystifying the trend of the healthcare index: Is historical price a key driver?](https://arxiv.org/abs/2601.14062)
*Payel Sadhukhan,Samrat Gupta,Subhasis Ghosh,Tanujit Chakraborty*

Main category: q-fin.ST

TL;DR: 研究利用历史OHLC指数数据预测医疗保健指数次日开盘指数方向，构建多样特征集并测试，结果显示预测性能良好，新的即时预测特征起关键作用，可助力健康经济。


<details>
  <summary>Details</summary>
Motivation: 探究历史OHLC指数数据是否含有足够信息以预测次日开盘指数方向，且短期指数变动与资本配置等决策相关。

Method: 将问题构建为有监督的一步向前滚动窗口分类任务，构建包含原始价格、基于波动率的技术指标和基于OHLC比率的即时预测特征的特征集，在美印市场医疗指数数据上评估。

Result: 预测性能稳健，准确率超0.8，Matthews相关系数超0.6，即时预测特征是市场走势关键决定因素。

Conclusion: 提出的特征和模型可减少信息不对称，支持更稳定公平的健康经济。

Abstract: Healthcare sector indices consolidate the economic health of pharmaceutical, biotechnology, and healthcare service firms. The short-term movements in these indices are closely intertwined with capital allocation decisions affecting research and development investment, drug availability, and long-term health outcomes. This research investigates whether historical open-high-low-close (OHLC) index data contain sufficient information for predicting the directional movement of the opening index on the subsequent trading day. The problem is formulated as a supervised classification task involving a one-step-ahead rolling window. A diverse feature set is constructed, comprising original prices, volatility-based technical indicators, and a novel class of nowcasting features derived from mutual OHLC ratios. The framework is evaluated on data from healthcare indices in the U.S. and Indian markets over a five-year period spanning multiple economic phases, including the COVID-19 pandemic. The results demonstrate robust predictive performance, with accuracy exceeding 0.8 and Matthews correlation coefficients above 0.6. Notably, the proposed nowcasting features have emerged as a key determinant of the market movement. We have employed the Shapley-based explainability paradigm to further elucidate the contribution of the features: outcomes reveal the dominant role of the nowcasting features, followed by a more moderate contribution of original prices. This research offers a societal utility: the proposed features and model for short-term forecasting of healthcare indices can reduce information asymmetry and support a more stable and equitable health economy.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [454] [Market Making and Transient Impact in Spot FX](https://arxiv.org/abs/2601.13421)
*Alexander Barzykin*

Main category: q-fin.TR

TL;DR: 考虑外汇市场做市商风险管理与市场影响弹性的中间情景。


<details>
  <summary>Details</summary>
Motivation: 以往在Almgren - Chriss模型下研究最优做市执行，该模型未考虑市场影响的瞬态性，而现实中有大量市场影响瞬态性的实证证据，故研究中间情景。

Method: 未提及

Result: 未提及

Conclusion: 未提及

Abstract: Dealers in foreign exchange markets provide bid and ask prices to their clients at which they are happy to buy and sell, respectively. To manage risk, dealers can skew their quotes and hedge in the interbank market. Hedging offers certainty but comes with transaction costs and market impact. Optimal market making with execution has previously been addressed within the Almgren-Chriss market impact model, which includes instantaneous and permanent components. However, there is overwhelming empirical evidence of the transient nature of market impact, with instantaneous and permanent impacts arising as the two limiting cases. In this note, we consider an intermediate scenario and study the interplay between risk management and impact resilience.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [455] [Gradient-based Active Learning with Gaussian Processes for Global Sensitivity Analysis](https://arxiv.org/abs/2601.11790)
*Guerlain Lambert,Céline Helbert,Claire Lauvernet*

Main category: stat.ML

TL;DR: 提出一种主动学习方法，在固定评估预算下改善敏感性分析准确性，并与现有方法对比及应用于实际模型。


<details>
  <summary>Details</summary>
Motivation: 复杂数值模拟器全局敏感性分析受模型评估次数限制，需利用代理模型降低计算负担。

Method: 基于主动学习进行敏感性分析的进展，利用高斯过程代理的导数，开发获取函数。

Result: 与最先进方法在标准基准函数上进行比较，并应用于农药转移的真实环境模型。

Conclusion: 该方法比现有的以DGSM为导向的标准更全面、更稳健。

Abstract: Global sensitivity analysis of complex numerical simulators is often limited by the small number of model evaluations that can be afforded. In such settings, surrogate models built from a limited set of simulations can substantially reduce the computational burden, provided that the design of computer experiments is enriched efficiently. In this context, we propose an active learning approach that, for a fixed evaluation budget, targets the most informative regions of the input space to improve sensitivity analysis accuracy. More specifically, our method builds on recent advances in active learning for sensitivity analysis (Sobol' indices and derivative-based global sensitivity measures, DGSM) that exploit derivatives obtained from a Gaussian process (GP) surrogate. By leveraging the joint posterior distribution of the GP gradient, we develop acquisition functions that better account for correlations between partial derivatives and their impact on the response surface, leading to a more comprehensive and robust methodology than existing DGSM-oriented criteria. The proposed approach is first compared to state-of-the-art methods on standard benchmark functions, and is then applied to a real environmental model of pesticide transfers.

</details>


### [456] [A Kernel Approach for Semi-implicit Variational Inference](https://arxiv.org/abs/2601.12023)
*Longlin Yu,Ziheng Cheng,Shiyue Zhang,Cheng Zhang*

Main category: stat.ML

TL;DR: 本文提出核半隐式变分推理(KSIVI)方法，消除了SIVI-SM中的低层优化问题，可高效优化，有理论保证，多层扩展提升表达能力，实证显示其有效性。


<details>
  <summary>Details</summary>
Motivation: 半隐式变分推理(SIVI)基于标准ELBO的优化有偏差，近期SIVI-SM方法存在额外低层优化问题，需改进。

Method: 提出KSIVI方法，利用核方法消除低层优化，将目标简化为核Stein差异(KSD)，利用半隐式分布层次结构用随机梯度法优化，建立相关理论保证，引入多层扩展。

Result: 建立了蒙特卡罗梯度估计的方差界优化保证和统计泛化界，多层扩展提升表达能力且保持易处理性，实证显示KSIVI在合成和真实世界贝叶斯推理任务中有效。

Conclusion: KSIVI是一种有效且易处理的半隐式变分推理替代方法。

Abstract: Semi-implicit variational inference (SIVI) enhances the expressiveness of variational families through hierarchical semi-implicit distributions, but the intractability of their densities makes standard ELBO-based optimization biased. Recent score-matching approaches to SIVI (SIVI-SM) address this issue via a minimax formulation, at the expense of an additional lower-level optimization problem. In this paper, we propose kernel semi-implicit variational inference (KSIVI), a principled and tractable alternative that eliminates the lower-level optimization by leveraging kernel methods. We show that when optimizing over a reproducing kernel Hilbert space, the lower-level problem admits an explicit solution, reducing the objective to the kernel Stein discrepancy (KSD). Exploiting the hierarchical structure of semi-implicit distributions, the resulting KSD objective can be efficiently optimized using stochastic gradient methods. We establish optimization guarantees via variance bounds on Monte Carlo gradient estimators and derive statistical generalization bounds of order $\tilde{\mathcal{O}}(1/\sqrt{n})$. We further introduce a multi-layer hierarchical extension that improves expressiveness while preserving tractability. Empirical results on synthetic and real-world Bayesian inference tasks demonstrate the effectiveness of KSIVI.

</details>


### [457] [On the Provable Suboptimality of Momentum SGD in Nonstationary Stochastic Optimization](https://arxiv.org/abs/2601.12238)
*Sharan Sahu,Cameron J. Hogan,Martin T. Wells*

Main category: stat.ML

TL;DR: 分析随机梯度下降及其动量变体在非平稳环境中的跟踪性能，揭示动量使用存在的权衡，给出上下界，为动量在动态环境中的不稳定性提供理论依据。


<details>
  <summary>Details</summary>
Motivation: 动量加速在确定性优化问题中研究广泛，但在非平稳环境中的行为研究不足，需要深入分析。

Method: 在一致强凸和平滑性及不同步长机制下分析SGD及其动量变体的跟踪性能，推导跟踪误差的有限时间界，建立动态遗憾的极小极大下界。

Result: 跟踪误差可分解为三项，动量会抑制梯度噪声但会增加跟踪误差，推导了上下界，证明了惯性惩罚是信息论障碍。

Conclusion: 为动量在动态环境中的实证不稳定性提供理论基础，明确了SGD优于其加速变体的精确区域边界。

Abstract: While momentum-based acceleration has been studied extensively in deterministic optimization problems, its behavior in nonstationary environments -- where the data distribution and optimal parameters drift over time -- remains underexplored. We analyze the tracking performance of Stochastic Gradient Descent (SGD) and its momentum variants (Polyak heavy-ball and Nesterov) under uniform strong convexity and smoothness in varying stepsize regimes. We derive finite-time bounds in expectation and with high probability for the tracking error, establishing a sharp decomposition into three components: a transient initialization term, a noise-induced variance term, and a drift-induced tracking lag. Crucially, our analysis uncovers a fundamental trade-off: while momentum can suppress gradient noise, it incurs an explicit penalty on the tracking capability. We show that momentum can substantially amplify drift-induced tracking error, with amplification that becomes unbounded as the momentum parameter approaches one, formalizing the intuition that using 'stale' gradients hinders adaptation to rapid regime shifts. Complementing these upper bounds, we establish minimax lower bounds for dynamic regret under gradient-variation constraints. These lower bounds prove that the inertia-induced penalty is not an artifact of analysis but an information-theoretic barrier: in drift-dominated regimes, momentum creates an unavoidable 'inertia window' that fundamentally degrades performance. Collectively, these results provide a definitive theoretical grounding for the empirical instability of momentum in dynamic environments and delineate the precise regime boundaries where SGD provably outperforms its accelerated counterparts.

</details>


### [458] [A Theory of Diversity for Random Matrices with Applications to In-Context Learning of Schrödinger Equations](https://arxiv.org/abs/2601.12587)
*Frank Cole,Yulong Lu,Shaurya Sehgal*

Main category: stat.ML

TL;DR: 研究一组独立随机矩阵的中心化子为平凡的概率，给出下界，并为基于变压器的神经网络学习薛定谔方程的泛化能力提供保证。


<details>
  <summary>Details</summary>
Motivation: 探究给定一组独立的随机矩阵，其中心化子为平凡的概率。

Method: 针对不同类型随机矩阵，根据样本量 N 和维度 d 给出概率的下界。

Result: 得到给定随机矩阵集合的中心化子为平凡的概率的下界。

Conclusion: 结果结合机器学习理论，为基于变压器的神经网络在薛定谔方程上下文学习中的泛化能力提供了保证。

Abstract: We address the following question: given a collection $\{\mathbf{A}^{(1)}, \dots, \mathbf{A}^{(N)}\}$ of independent $d \times d$ random matrices drawn from a common distribution $\mathbb{P}$, what is the probability that the centralizer of $\{\mathbf{A}^{(1)}, \dots, \mathbf{A}^{(N)}\}$ is trivial? We provide lower bounds on this probability in terms of the sample size $N$ and the dimension $d$ for several families of random matrices which arise from the discretization of linear Schrödinger operators with random potentials. When combined with recent work on machine learning theory, our results provide guarantees on the generalization ability of transformer-based neural networks for in-context learning of Schrödinger equations.

</details>


### [459] [Approximate full conformal prediction in RKHS](https://arxiv.org/abs/2601.13102)
*Davidson Lova Razafindrakoto,Alain Celisse,Jérôme Lacaille*

Main category: stat.ML

TL;DR: 文章提出设计全共形预测区域的紧密近似策略并给出近似紧密度的理论量化。


<details>
  <summary>Details</summary>
Motivation: 全共形预测框架在计算置信预测区域时存在需要训练无限多个估计器的经典局限，本文旨在解决该计算难题。

Method: 提出可高效计算的全共形预测区域的紧密近似策略，基于损失和得分函数的平滑假设给出近似紧密度的理论量化，引入厚度概念衡量近似区域和全共形区域的差异。

Result: 设计出可高效计算的近似全共形预测区域及对近似紧密度的理论量化。

Conclusion: 新策略可解决全共形预测区域计算难题，且能衡量近似程度。

Abstract: Full conformal prediction is a framework that implicitly formulates distribution-free confidence prediction regions for a wide range of estimators. However, a classical limitation of the full conformal framework is the computation of the confidence prediction regions, which is usually impossible since it requires training infinitely many estimators (for real-valued prediction for instance). The main purpose of the present work is to describe a generic strategy for designing a tight approximation to the full conformal prediction region that can be efficiently computed. Along with this approximate confidence region, a theoretical quantification of the tightness of this approximation is developed, depending on the smoothness assumptions on the loss and score functions. The new notion of thickness is introduced for quantifying the discrepancy between the approximate confidence region and the full conformal one.

</details>


### [460] [Empirical Risk Minimization with $f$-Divergence Regularization](https://arxiv.org/abs/2601.13191)
*Francisco Daunas,Iñaki Esnaola,Samir M. Perlaza,H. Vincent Poor*

Main category: stat.ML

TL;DR: 本文给出ERM - $f$DR问题的解，建立相关条件，引入归一化函数，构造数值算法，分析不同$f$ - 散度问题的结构等价性并进行数值计算。


<details>
  <summary>Details</summary>
Motivation: 解决ERM - $f$DR问题，拓展$f$ - 散度的适用范围，深入理解不同$f$ - 散度下的经验风险差异。

Method: 给出ERM - $f$DR问题解，建立条件；引入归一化函数，将其表征为非线性常微分方程；通过经验风险变换分析结构等价性；构造数值算法。

Result: 得到理论结果，恢复已知结果；刻画了ERM - $f$DR解与参考测度的期望经验风险差异；完成数值计算。

Conclusion: 提出的方法可解决ERM - $f$DR问题，不同的$f$函数选择在实际中有不同影响。

Abstract: In this paper, the solution to the empirical risk minimization problem with $f$-divergence regularization (ERM-$f$DR) is presented and conditions under which the solution also serves as the solution to the minimization of the expected empirical risk subject to an $f$-divergence constraint are established. The proposed approach extends applicability to a broader class of $f$-divergences than previously reported and yields theoretical results that recover previously known results. Additionally, the difference between the expected empirical risk of the ERM-$f$DR solution and that of its reference measure is characterized, providing insights into previously studied cases of $f$-divergences. A central contribution is the introduction of the normalization function, a mathematical object that is critical in both the dual formulation and practical computation of the ERM-$f$DR solution. This work presents an implicit characterization of the normalization function as a nonlinear ordinary differential equation (ODE), establishes its key properties, and subsequently leverages them to construct a numerical algorithm for approximating the normalization factor under mild assumptions. Further analysis demonstrates structural equivalences between ERM-$f$DR problems with different $f$-divergences via transformations of the empirical risk. Finally, the proposed algorithm is used to compute the training and test risks of ERM-$f$DR solutions under different $f$-divergence regularizers. This numerical example highlights the practical implications of choosing different functions $f$ in ERM-$f$DR problems.

</details>


### [461] [Distribution-Free Confidence Ellipsoids for Ridge Regression with PAC Bounds](https://arxiv.org/abs/2601.13436)
*Szabolcs Szentpéteri,Balázs Csanád Csáji*

Main category: stat.ML

TL;DR: 本文将SPS EOA算法扩展到岭回归，推导PAC上界，明确正则化参数影响，在弱激励假设下给出更紧边界，并通过模拟实验展示正则化效果。


<details>
  <summary>Details</summary>
Motivation: 线性参数化模型中输入激励不足时最小二乘法可能无解或数值不稳定，正则化虽能解决但需量化估计不确定性，当前缺少对岭回归构建置信椭球及推导上界的研究。

Method: 将SPS EOA算法扩展到岭回归，推导该方法得到的区域大小的PAC上界。

Result: 结果明确显示正则化参数如何影响区域大小，在较弱激励假设下给出更紧的边界。

Conclusion: 通过理论推导和模拟实验，证明对岭回归扩展SPS EOA算法的有效性，展示了正则化的实际效果。

Abstract: Linearly parametrized models are widely used in control and signal processing, with the least-squares (LS) estimate being the archetypical solution. When the input is insufficiently exciting, the LS problem may be unsolvable or numerically unstable. This issue can be resolved through regularization, typically with ridge regression. Although regularized estimators reduce the variance error, it remains important to quantify their estimation uncertainty. A possible approach for linear regression is to construct confidence ellipsoids with the Sign-Perturbed Sums (SPS) ellipsoidal outer approximation (EOA) algorithm. The SPS EOA builds non-asymptotic confidence ellipsoids under the assumption that the noises are independent and symmetric about zero. This paper introduces an extension of the SPS EOA algorithm to ridge regression, and derives probably approximately correct (PAC) upper bounds for the resulting region sizes. Compared with previous analyses, our result explicitly show how the regularization parameter affects the region sizes, and provide tighter bounds under weaker excitation assumptions. Finally, the practical effect of regularization is also demonstrated via simulation experiments.

</details>


### [462] [Labels or Preferences? Budget-Constrained Learning with Human Judgments over AI-Generated Outputs](https://arxiv.org/abs/2601.13458)
*Zihan Dong,Ruijia Wu,Linjun Zhang*

Main category: stat.ML

TL;DR: 针对如何在标注预算约束下优化真值标签和成对偏好之间的预算分配问题，提出Preference - Calibrated Active Learning (PCAL)方法，理论证明其渐进最优性与鲁棒性，仿真和实际数据分析验证其优势。


<details>
  <summary>Details</summary>
Motivation: 当前越来越依赖人类偏好反馈来判断AI生成的伪标签，亟需有原则、注重预算的数据采集策略，需解决在AI中如何在真值标签和成对偏好之间优化分配固定标注预算的问题。

Method: 基于半参数推断，将预算分配问题转化为单调缺失数据框架；提出PCAL方法，学习最优数据采集策略并开发用于数据分布泛函的统计高效估计器。

Result: 理论上证明了PCAL估计器的渐近最优性，建立了关键的鲁棒性保证；仿真和实际数据分析表明该方法有实际效益和优越性能。

Conclusion: 该工作为现代AI中预算受限的学习提供了一种有原则且统计高效的方法。

Abstract: The increasing reliance on human preference feedback to judge AI-generated pseudo labels has created a pressing need for principled, budget-conscious data acquisition strategies. We address the crucial question of how to optimally allocate a fixed annotation budget between ground-truth labels and pairwise preferences in AI. Our solution, grounded in semi-parametric inference, casts the budget allocation problem as a monotone missing data framework. Building on this formulation, we introduce Preference-Calibrated Active Learning (PCAL), a novel method that learns the optimal data acquisition strategy and develops a statistically efficient estimator for functionals of the data distribution. Theoretically, we prove the asymptotic optimality of our PCAL estimator and establish a key robustness guarantee that ensures robust performance even with poorly estimated nuisance models. Our flexible framework applies to a general class of problems, by directly optimizing the estimator's variance instead of requiring a closed-form solution. This work provides a principled and statistically efficient approach for budget-constrained learning in modern AI. Simulations and real-data analysis demonstrate the practical benefits and superior performance of our proposed method.

</details>


### [463] [Small Gradient Norm Regret for Online Convex Optimization](https://arxiv.org/abs/2601.13519)
*Wenzhi Gao,Chang He,Madeleine Udell*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper introduces a new problem-dependent regret measure for online convex optimization with smooth losses. The notion, which we call the $G^\star$ regret, depends on the cumulative squared gradient norm evaluated at the decision in hindsight $\sum_{t=1}^T \|\nabla \ell(x^\star)\|^2$. We show that the $G^\star$ regret strictly refines the existing $L^\star$ (small loss) regret, and that it can be arbitrarily sharper when the losses have vanishing curvature around the hindsight decision. We establish upper and lower bounds on the $G^\star$ regret and extend our results to dynamic regret and bandit settings. As a byproduct, we refine the existing convergence analysis of stochastic optimization algorithms in the interpolation regime. Some experiments validate our theoretical findings.

</details>


### [464] [Sample Complexity of Average-Reward Q-Learning: From Single-agent to Federated Reinforcement Learning](https://arxiv.org/abs/2601.13642)
*Yuchen Jiao,Jiin Woo,Gen Li,Gauri Joshi,Yuejie Chi*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Average-reward reinforcement learning offers a principled framework for long-term decision-making by maximizing the mean reward per time step. Although Q-learning is a widely used model-free algorithm with established sample complexity in discounted and finite-horizon Markov decision processes (MDPs), its theoretical guarantees for average-reward settings remain limited. This work studies a simple but effective Q-learning algorithm for average-reward MDPs with finite state and action spaces under the weakly communicating assumption, covering both single-agent and federated scenarios. For the single-agent case, we show that Q-learning with carefully chosen parameters achieves sample complexity $\widetilde{O}\left(\frac{|\mathcal{S}||\mathcal{A}|\|h^{\star}\|_{\mathsf{sp}}^3}{\varepsilon^3}\right)$, where $\|h^{\star}\|_{\mathsf{sp}}$ is the span norm of the bias function, improving previous results by at least a factor of $\frac{\|h^{\star}\|_{\mathsf{sp}}^2}{\varepsilon^2}$. In the federated setting with $M$ agents, we prove that collaboration reduces the per-agent sample complexity to $\widetilde{O}\left(\frac{|\mathcal{S}||\mathcal{A}|\|h^{\star}\|_{\mathsf{sp}}^3}{M\varepsilon^3}\right)$, with only $\widetilde{O}\left(\frac{\|h^{\star}\|_{\mathsf{sp}}}{\varepsilon}\right)$ communication rounds required. These results establish the first federated Q-learning algorithm for average-reward MDPs, with provable efficiency in both sample and communication complexity.

</details>


### [465] [Unified Unbiased Variance Estimation for MMD: Robust Finite-Sample Performance with Imbalanced Data and Exact Acceleration under Null and Alternative Hypotheses](https://arxiv.org/abs/2601.13874)
*Shijie Zhong,Jiangfeng Fu,Yikun Yang*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The maximum mean discrepancy (MMD) is a kernel-based nonparametric statistic for two-sample testing, whose inferential accuracy depends critically on variance characterization. Existing work provides various finite-sample estimators of the MMD variance, often differing under the null and alternative hypotheses and across balanced or imbalanced sampling schemes. In this paper, we study the variance of the MMD statistic through its U-statistic representation and Hoeffding decomposition, and establish a unified finite-sample characterization covering different hypotheses and sample configurations. Building on this analysis, we propose an exact acceleration method for the univariate case under the Laplacian kernel, which reduces the overall computational complexity from $\mathcal O(n^2)$ to $\mathcal O(n \log n)$.

</details>


### [466] [Intermittent time series forecasting: local vs global models](https://arxiv.org/abs/2601.14031)
*Stefano Damato,Nicolò Rubattu,Dario Azzimonti,Giorgio Corani*

Main category: stat.ML

TL;DR: 本文首次比较了先进的局部和全局模型对间歇性时间序列的预测，并测试了神经网络的三种分布头；D - Linear表现最佳，Tweedie分布头在最高分位数估计中有优势。


<details>
  <summary>Details</summary>
Motivation: 间歇性时间序列在供应链库存中占比大，需概率预测，全局模型在间歇性时间序列上未充分测试，需进行比较研究。

Method: 比较先进的局部（iETS、TweedieGP）和全局模型（D - Linear、DeepAR、Transformers），对神经网络考虑三种分布头，在五个含超40000个时间序列的数据集上实验。

Result: 神经网络中D - Linear 准确率最高，持续优于局部模型且计算需求低，Transformers架构计算需求大且准确性低；Tweedie分布头对最高分位数估计最佳，负二项式分布头整体表现最优。

Conclusion: D - Linear适合间歇性时间序列预测；Tweedie分布头在估计最高分位数时有优势，负二项式分布头整体表现出色。

Abstract: Intermittent time series, characterised by the presence of a significant amount of zeros, constitute a large percentage of inventory items in supply chain. Probabilistic forecasts are needed to plan the inventory levels; the predictive distribution should cover non-negative values, have a mass in zero and a long upper tail. Intermittent time series are commonly forecast using local models, which are trained individually on each time series. In the last years global models, which are trained on a large collection of time series, have become popular for time series forecasting. Global models are often based on neural networks. However, they have not yet been exhaustively tested on intermittent time series. We carry out the first study comparing state-of-the-art local (iETS, TweedieGP) and global models (D-Linear, DeepAR, Transformers) on intermittent time series. For neural networks models we consider three different distribution heads suitable for intermittent time series: negative binomial, hurdle-shifted negative binomial and Tweedie. We use, for the first time, the last two distribution heads with neural networks. We perform experiments on five large datasets comprising more than 40'000 real-world time series. Among neural networks D-Linear provides best accuracy; it also consistently outperforms the local models. Moreover, it has also low computational requirements. Transformers-based architectures are instead much more computationally demanding and less accurate. Among the distribution heads, the Tweedie provides the best estimates of the highest quantiles, while the negative binomial offers overall the best performance.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [467] [Bayesian Inference for Partially Observed McKean-Vlasov SDEs with Full Distribution Dependence](https://arxiv.org/abs/2601.12515)
*Ning Ning,Amin Wu*

Main category: stat.CO

TL;DR: 本文为部分观测的McKean - Vlasov随机微分方程（MVSDEs）开发贝叶斯框架用于潜状态推断和参数估计，设计两种PMCMC算法，多级算法复杂度更优，数值实验验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: MVSDEs在许多应用中系统仅部分可观测，当漂移和扩散系数依赖于经验法则时，推理极具挑战。

Method: 结合时间离散化和基于粒子的近似构建似然估计器，设计单级和多级PMCMC算法。

Result: 多级算法均方误差为$O(ε^2)$，计算成本为$O(ε^{-6})$，优于单级方案的$O(ε^{-7})$复杂度。

Conclusion: 所提方法高效准确，且在标准正则性假设下有理论保证。

Abstract: McKean-Vlasov stochastic differential equations (MVSDEs) describe systems whose dynamics depend on both individual states and the population distribution, and they arise widely in neuroscience, finance, and epidemiology. In many applications the system is only partially observed, making inference very challenging when both drift and diffusion coefficients depend on the evolving empirical law. This paper develops a Bayesian framework for latent state inference and parameter estimation in such partially observed MVSDEs. We combine time-discretization with particle-based approximations to construct tractable likelihood estimators, and we design two particle Markov chain Monte Carlo (PMCMC) algorithms: a single-level PMCMC method and a multilevel PMCMC (MLPMCMC) method that couples particle systems across discretization levels. The multilevel construction yields correlated likelihood estimates and achieves mean square error $(O(\varepsilon^2))$ at computational cost $(O(\varepsilon^{-6}))$, improving on the $(O(\varepsilon^{-7}))$ complexity of single-level schemes. We address the fully law-dependent diffusion setting which is the most general formulation of MVSDEs, and provide theoretical guarantees under standard regularity assumptions. Numerical experiments confirm the efficiency and accuracy of the proposed methodology.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [468] [RAC: Retrieval-Augmented Clarification for Faithful Conversational Search](https://arxiv.org/abs/2601.11722)
*Ahmed Rayane Kebir,Vincent Guigue,Lynda Said Lhadj,Laure Soulier*

Main category: cs.CL

TL;DR: 提出RAC框架生成与语料库一致的澄清问题，在四个基准测试中表现优于基线，还引入新指标评估上下文关联性。


<details>
  <summary>Details</summary>
Motivation: 以往对话搜索系统生成澄清问题时对基于语料库的关注不足，可能提出无法从文档回答的问题。

Method: 比较多种检索索引策略，微调大语言模型以利用研究上下文生成基于证据的问题，应用对比偏好优化。

Result: 在四个基准测试中，RAC较基线有显著改进，新指标表明方法能增强忠实性。

Conclusion: RAC框架能有效生成与语料库一致的澄清问题，提升对话搜索系统性能。

Abstract: Clarification questions help conversational search systems resolve ambiguous or underspecified user queries. While prior work has focused on fluency and alignment with user intent, especially through facet extraction, much less attention has been paid to grounding clarifications in the underlying corpus. Without such grounding, systems risk asking questions that cannot be answered from the available documents. We introduce RAC (Retrieval-Augmented Clarification), a framework for generating corpus-faithful clarification questions. After comparing several indexing strategies for retrieval, we fine-tune a large language model to make optimal use of research context and to encourage the generation of evidence-based question. We then apply contrastive preference optimization to favor questions supported by retrieved passages over ungrounded alternatives. Evaluated on four benchmarks, RAC demonstrate significant improvements over baselines. In addition to LLM-as-Judge assessments, we introduce novel metrics derived from NLI and data-to-text to assess how well questions are anchored in the context, and we demonstrate that our approach consistently enhances faithfulness.

</details>


### [469] [Don't Start Over: A Cost-Effective Framework for Migrating Personalized Prompts Between LLMs](https://arxiv.org/abs/2601.12034)
*Ziyi Zhao,Chongming Gao,Yang Zhang,Haoyan Liu,Weinan Gan,Huifeng Guo,Yong Liu,Fuli Feng*

Main category: cs.CL

TL;DR: 提出轻量级框架PUMA，实现跨不兼容模型迁移个性化提示，减少训练成本，实验表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型中用户特定软提示在基础模型升级时需全量重新训练的问题。

Method: 提出PUMA框架，利用参数高效适配器弥合语义差距，结合基于组的用户选择策略。

Result: 在三个大规模数据集上实验表明，该方法性能与从头训练相当甚至更优，降低计算成本达98%。

Conclusion: 框架在不同模型架构上泛化性强，在复杂迁移场景中稳健，为个性化AI可持续发展提供可行路径。

Abstract: Personalization in Large Language Models (LLMs) often relies on user-specific soft prompts. However, these prompts become obsolete when the foundation model is upgraded, necessitating costly, full-scale retraining. To overcome this limitation, we propose the Prompt-level User Migration Adapter (PUMA), a lightweight framework to efficiently migrate personalized prompts across incompatible models. PUMA utilizes a parameter-efficient adapter to bridge the semantic gap, combined with a group-based user selection strategy to significantly reduce training costs. Experiments on three large-scale datasets show our method matches or even surpasses the performance of retraining from scratch, reducing computational cost by up to 98%. The framework demonstrates strong generalization across diverse model architectures and robustness in advanced scenarios like chained and aggregated migrations, offering a practical path for the sustainable evolution of personalized AI by decoupling user assets from the underlying models.

</details>


### [470] [Optimizing User Profiles via Contextual Bandits for Retrieval-Augmented LLM Personalization](https://arxiv.org/abs/2601.12078)
*Linfeng Du,Ye Yuan,Zichen Zhao,Fuyuan Lyu,Emiliano Penaloza,Xiuying Chen,Zipeng Sun,Jikun Kang,Laurent Charlin,Xue Liu,Haolun Wu*

Main category: cs.CL

TL;DR: 现有大语言模型用户适配难，检索增强方法有局限，提出PURPLE框架优化用户配置文件，实验显示其在多任务上效果和效率更优。


<details>
  <summary>Details</summary>
Motivation: 大语言模型适配个体用户有挑战，现有检索增强方法基于语义相关性选记录不可靠。

Method: 提出PURPLE框架，将配置文件构建视为集合生成过程，用Plackett - Luce排名模型捕获记录间依赖关系，并结合参考响应似然提供的密集反馈进行训练。

Result: 在九个个性化任务的实验中，PURPLE在效果和效率上始终优于强大的启发式和检索增强基线。

Conclusion: PURPLE为优化用户配置文件提供了有原则且可扩展的解决方案。

Abstract: Large Language Models (LLMs) excel at general-purpose tasks, yet adapting their responses to individual users remains challenging. Retrieval augmentation provides a lightweight alternative to fine-tuning by conditioning LLMs on user history records, and existing approaches typically select these records based on semantic relevance. We argue that relevance serves as an unreliable proxy for utility: a record may be semantically similar to a query yet fail to improve generation quality or even degrade it due to redundancy or conflicting information. To bridge this gap, we propose PURPLE, a contextual bandit framework that oPtimizes UseR Profiles for Llm pErsonalization. In contrast to a greedy selection of the most relevant records, PURPLE treats profile construction as a set generation process and utilizes a Plackett-Luce ranking model to capture complex inter-record dependencies. By training with dense feedback provided by the likelihood of the reference response, our method aligns retrieval directly with generation quality. Extensive experiments on nine personalization tasks demonstrate that PURPLE consistently outperforms strong heuristic and retrieval-augmented baselines in both effectiveness and efficiency, establishing a principled and scalable solution for optimizing user profiles.

</details>


### [471] [BioPulse-QA: A Dynamic Biomedical Question-Answering Benchmark for Evaluating Factuality, Robustness, and Bias in Large Language Models](https://arxiv.org/abs/2601.12632)
*Kriti Bhattarai,Vipina K. Keloth,Donald Wright,Andrew Loza,Yang Ren,Hua Xu*

Main category: cs.CL

TL;DR: 现有生物医学大语言模型基准数据集有局限性，本文引入BioPulse - QA基准评估大语言模型，评估了四个模型，发现GPT - o1在药物标签上得分高，BioPulse - QA提供评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有生物医学大语言模型基准数据集存在依赖静态或过时数据、有数据泄露风险、忽略语言变体鲁棒性和人口统计学偏差等问题，需要新的评估基准。

Method: 引入包含2280对专家验证问答对及变体的BioPulse - QA基准，评估GPT - 4o、GPT - o1、Gemini - 2.0 - Flash和LLaMA - 3.1 8B Instruct四个模型。

Result: GPT - o1在药物标签上放松F1得分最高为0.92，Gemini - 2.0 - Flash为0.90，临床试验是最具挑战性的来源，提取式F1得分低至0.36。

Conclusion: 释义操作的性能差异大于排版错误，偏差测试差异可忽略，BioPulse - QA为评估生物医学大语言模型提供可扩展且临床相关的框架。

Abstract: Objective: Large language models (LLMs) are increasingly applied in biomedical settings, and existing benchmark datasets have played an important role in supporting model development and evaluation. However, these benchmarks often have limitations. Many rely on static or outdated datasets that fail to capture the dynamic, context-rich, and high-stakes nature of biomedical knowledge. They also carry increasing risk of data leakage due to overlap with model pretraining corpora and often overlook critical dimensions such as robustness to linguistic variation and potential demographic biases.
  Materials and Methods: To address these gaps, we introduce BioPulse-QA, a benchmark that evaluates LLMs on answering questions from newly published biomedical documents including drug labels, trial protocols, and clinical guidelines. BioPulse-QA includes 2,280 expert-verified question answering (QA) pairs and perturbed variants, covering both extractive and abstractive formats. We evaluate four LLMs - GPT-4o, GPT-o1, Gemini-2.0-Flash, and LLaMA-3.1 8B Instruct - released prior to the publication dates of the benchmark documents.
  Results: GPT-o1 achieves the highest relaxed F1 score (0.92), followed by Gemini-2.0-Flash (0.90) on drug labels. Clinical trials are the most challenging source, with extractive F1 scores as low as 0.36.
  Discussion and Conclusion: Performance differences are larger for paraphrasing than for typographical errors, while bias testing shows negligible differences. BioPulse-QA provides a scalable and clinically relevant framework for evaluating biomedical LLMs.

</details>


### [472] [CORE-T: COherent REtrieval of Tables for Text-to-SQL](https://arxiv.org/abs/2601.13111)
*Hassan Soliman,Vivek Gupta,Dan Roth,Iryna Gurevych*

Main category: cs.CL

TL;DR: 提出CORE - T框架，在多表选择任务中提升F1值、减少检索表数量、提高执行准确率并减少token使用。


<details>
  <summary>Details</summary>
Motivation: 现实文本到SQL工作流常需多表连接，准确检索相关表集是端到端性能的关键瓶颈，现有方法存在召回率低、依赖额外假设或推理开销大等问题。

Method: 提出CORE - T框架，用LLM生成目的元数据丰富表，预计算轻量级表兼容性缓存，推理时DR返回候选，LLM选择可连接子集，再通过简单加法调整步骤恢复强兼容表。

Result: 在Bird、Spider和MMQA上，CORE - T将表选择F1值提高最多22.7个点，减少最多42%的表检索量，在Bird和MMQA上分别提高多表执行准确率最多5.0和6.9个点，比LLM密集型基线少用4 - 5倍的token。

Conclusion: CORE - T框架在多表选择和执行方面表现出色，能提升性能并降低成本。

Abstract: Realistic text-to-SQL workflows often require joining multiple tables. As a result, accurately retrieving the relevant set of tables becomes a key bottleneck for end-to-end performance. We study an open-book setting where queries must be answered over large, heterogeneous table collections pooled from many sources, without clean scoping signals such as database identifiers. Here, dense retrieval (DR) achieves high recall but returns many distractors, while join-aware alternatives often rely on extra assumptions and/or incur high inference overhead. We propose CORE-T, a scalable, training-free framework that enriches tables with LLM-generated purpose metadata and pre-computes a lightweight table-compatibility cache. At inference time, DR returns top-K candidates; a single LLM call selects a coherent, joinable subset, and a simple additive adjustment step restores strongly compatible tables. Across Bird, Spider, and MMQA, CORE-T improves table-selection F1 by up to 22.7 points while retrieving up to 42% fewer tables, improving multi-table execution accuracy by up to 5.0 points on Bird and 6.9 points on MMQA, and using 4-5x fewer tokens than LLM-intensive baselines.

</details>


### [473] [Agentic Conversational Search with Contextualized Reasoning via Reinforcement Learning](https://arxiv.org/abs/2601.13115)
*Fengran Mo,Yifan Gao,Sha Li,Hansi Zeng,Xin Liu,Zhaoxuan Tan,Xian Li,Jianshu Chen,Dakuo Wang,Meng Jiang*

Main category: cs.CL

TL;DR: 论文引入跨轮次交错搜索与推理的对话代理，经实验在多个基准测试中超越现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有研究在多轮对话中常采用静态流程，未同时优化混合主动行为，且深度搜索代理多针对单轮场景，缺乏处理多轮交互能力。

Method: 引入跨轮次交错搜索与推理的对话代理，通过强化学习训练，使用定制奖励以适应不断变化的用户目标。

Result: 在四个广泛使用的对话基准测试中，方法超越了几个现有的强大基线。

Conclusion: 所提出的方法在处理多轮对话方面是有效的。

Abstract: Large Language Models (LLMs) have become a popular interface for human-AI interaction, supporting information seeking and task assistance through natural, multi-turn dialogue. To respond to users within multi-turn dialogues, the context-dependent user intent evolves across interactions, requiring contextual interpretation, query reformulation, and dynamic coordination between retrieval and generation. Existing studies usually follow static rewrite, retrieve, and generate pipelines, which optimize different procedures separately and overlook the mixed-initiative action optimization simultaneously. Although the recent developments in deep search agents demonstrate the effectiveness in jointly optimizing retrieval and generation via reasoning, these approaches focus on single-turn scenarios, which might lack the ability to handle multi-turn interactions. We introduce a conversational agent that interleaves search and reasoning across turns, enabling exploratory and adaptive behaviors learned through reinforcement learning (RL) training with tailored rewards towards evolving user goals. The experimental results across four widely used conversational benchmarks demonstrate the effectiveness of our methods by surpassing several existing strong baselines.

</details>


### [474] [A Systematic Analysis of Chunking Strategies for Reliable Question Answering](https://arxiv.org/abs/2601.14123)
*Sofia Bennani,Charles Moslonka*

Main category: cs.CL

TL;DR: 研究文档分块选择对工业RAG系统可靠性的影响，得出成本效益高的部署建议。


<details>
  <summary>Details</summary>
Motivation: 实践中RAG系统文档分块常依赖启发式方法，需系统研究分块选择对系统可靠性的影响。

Method: 在Natural Questions上进行端到端评估，系统改变分块方法、大小、重叠和上下文长度，采用SPLADE检索和Mistral - 8B生成器的标准工业设置。

Result: 重叠无明显益处且增加索引成本；句子分块最具成本效益；超过约2.5k令牌存在“上下文悬崖”降低质量；最优上下文取决于目标。

Conclusion: 为RAG系统的成本效益部署提供了可操作的经验教训。

Abstract: We study how document chunking choices impact the reliability of Retrieval-Augmented Generation (RAG) systems in industry. While practice often relies on heuristics, our end-to-end evaluation on Natural Questions systematically varies chunking method (token, sentence, semantic, code), chunk size, overlap, and context length. We use a standard industrial setup: SPLADE retrieval and a Mistral-8B generator. We derive actionable lessons for cost-efficient deployment: (i) overlap provides no measurable benefit and increases indexing cost; (ii) sentence chunking is the most cost-effective method, matching semantic chunking up to ~5k tokens; (iii) a "context cliff" reduces quality beyond ~2.5k tokens; and (iv) optimal context depends on the goal (semantic quality peaks at small contexts; exact match at larger ones).

</details>


### [475] [Context Discipline and Performance Correlation: Analyzing LLM Performance and Quality Degradation Under Varying Context Lengths](https://arxiv.org/abs/2601.11564)
*Ahilan Ayyachamy Nadar Ponnusamy,Karthic Chandran,M Maruf Hossain*

Main category: cs.CL

TL;DR: 研究大语言模型在处理大量无关上下文时系统性能与模型质量的权衡，发现KV缓存增长致非线性性能下降，MoE架构有行为异常。


<details>
  <summary>Details</summary>
Motivation: 大语言模型扩展上下文窗口带来计算开销，需研究处理大量无关上下文时系统性能与模型质量的权衡。

Method: 研究密集变压器架构（Llama - 3.1 - 70B和Qwen1.5 - 14B）在大量无关上下文下的表现，分析MoE架构在不同上下文规模的情况。

Result: 发现KV缓存增长导致非线性性能下降，MoE架构在不同上下文规模有独特行为异常。

Conclusion: 在高令牌量下，架构优势可能被基础设施瓶颈掩盖。

Abstract: The scaling trend in Large Language Models (LLMs) has prioritized increasing the maximum context window to facilitate complex, long-form reasoning and document analysis. However, managing this expanded context introduces severe computational overhead. This paper investigates the critical trade-off between system performance and model quality when dense transformer architectures--specifically Llama-3.1-70B and Qwen1.5-14B--are exposed to large volumes of irrelevant and distracting context. The research identifies a non-linear performance degradation tied to the growth of the Key-Value (KV) cache. Furthermore, an extended analysis of the Mixture-of-Experts (MoE) architecture reveals unique behavioral anomalies at varying context scales, suggesting that architectural benefits may be masked by infrastructure bottlenecks at high token volumes.

</details>


### [476] [Compass-Embedding v4: Robust Contrastive Learning for Multilingual E-commerce Embeddings](https://arxiv.org/abs/2601.11565)
*Pakorn Ueareeworakul,Shuman Liu,Jinghao Feng,Ling Hu,Zhantang Shi,Chengqi Sun,Liang Yao,Panyi Ouyang,Haibo Zhang,Anxiang Zeng*

Main category: cs.CL

TL;DR: 本文提出用于东南亚电商场景的高效多语言嵌入框架Compass - Embedding v4，解决了数据稀缺等问题，在评估中表现出色。


<details>
  <summary>Details</summary>
Motivation: 全球电商向新兴市场扩张，低资源语言缺乏高质量语义表示成为检索、推荐和搜索系统的瓶颈，尤其是东南亚电商场景存在数据稀缺等挑战。

Method: 提出Class - Aware Masking改进InfoNCE目标；通过上下文合成数据生成、跨语言翻译和结构化电商数据构建多样化训练语料库；结合大数据量训练和球形模型合并，使用vLLM和FP8量化优化推理。

Result: Compass - Embedding v4在主要东南亚语言上达到了先进水平，在特定领域检索和分类中显著优于通用嵌入模型，在高资源语言上也有竞争力。

Conclusion: Compass - Embedding v4是为东南亚电商场景优化的有效多语言嵌入框架，能解决现有挑战并取得良好性能。

Abstract: As global e-commerce rapidly expands into emerging markets, the lack of high-quality semantic representations for low-resource languages has become a decisive bottleneck for retrieval, recommendation, and search systems. In this work, we present Compass-Embedding v4, a high-efficiency multilingual embedding framework specifically optimized for Southeast Asian (SEA) e-commerce scenarios, where data scarcity, noisy supervision, and strict production constraints jointly challenge representation learning. Compass-Embedding v4 addresses three core challenges. First, large-batch contrastive training under mixed task supervision introduces systematic false negatives that degrade semantic alignment. We propose Class-Aware Masking (CAM), a lightweight modification to the InfoNCE objective that suppresses invalid in-batch negatives and improves semantic discrimination without altering training efficiency. Second, low-resource SEA languages suffer from limited and uneven data coverage. We construct a diversified training corpus through context-grounded synthetic data generation, cross-lingual translation, and structured e-commerce data construction, enabling robust multilingual and domain-specific learning. Third, production deployment requires high-throughput inference while preserving embedding quality. We combine robustness-driven large-batch training with spherical model merging to mitigate catastrophic forgetting, and optimize inference via vLLM and FP8 quantization. Extensive evaluations across multilingual benchmarks and proprietary e-commerce tasks show that Compass-Embedding v4 achieves state-of-the-art performance on major SEA languages, significantly outperforming general-purpose embedding models in domain-specific retrieval and classification, while maintaining competitive performance on high-resource languages.

</details>


### [477] [Measuring Stability Beyond Accuracy in Small Open-Source Medical Large Language Models for Pediatric Endocrinology](https://arxiv.org/abs/2601.11567)
*Vanessa D'Amario,Randy Daniel,Alessandro Zanetti,Dhruv Edamadaka,Nitya Alaparthy,Joshua Tarkoff*

Main category: cs.CL

TL;DR: 评估六个小型开源医学大模型在儿科内分泌学的表现，发现高一致性不代表正确性，提示需更广泛诊断框架评估模型。


<details>
  <summary>Details</summary>
Motivation: 当前小型开源医学大模型评估局限于医学选择题准确率，缺乏一致性、鲁棒性和推理行为评估。

Method: 使用多项选择题和人工评估及临床审查，分别在确定性和随机设置下评估模型。

Result: HuatuoGPT - o1 - 8B表现最佳，高一致性不代表正确性，模型存在自我评估偏差，系统扰动会影响输出。

Conclusion: 小的提示扰动会导致输出不同，强调需要更广泛诊断框架来理解模型在临床决策中的潜在问题。

Abstract: Small open-source medical large language models (LLMs) offer promising opportunities for low-resource deployment and broader accessibility. However, their evaluation is often limited to accuracy on medical multiple choice question (MCQ) benchmarks, and lacks evaluation of consistency, robustness, or reasoning behavior. We use MCQ coupled to human evaluation and clinical review to assess six small open-source medical LLMs (HuatuoGPT-o1 (Chen 2024), Diabetica-7B, Diabetica-o1 (Wei 2024), Meditron3-8B (Sallinen2025), MedFound-7B (Liu 2025), and ClinicaGPT-base-zh (Wang 2023)) in pediatric endocrinology. In deterministic settings, we examine the effect of prompt variation on models' output and self-assessment bias. In stochastic settings, we evaluate output variability and investigate the relationship between consistency and correctness. HuatuoGPT-o1-8B achieved the highest performance. The results show that high consistency across the model response is not an indicator of correctness, although HuatuoGPT-o1-8B showed the highest consistency rate. When tasked with selecting correct reasoning, both HuatuoGPT-o1-8B and Diabetica-o1 exhibit self-assessment bias and dependency on the order of the candidate explanations. Expert review of incorrect reasoning rationales identified a mix of clinically acceptable responses and clinical oversight. We further show that system-level perturbations, such as differences in CUDA builds, can yield statistically significant shifts in model output despite stable accuracy. This work demonstrates that small, semantically negligible prompt perturbations lead to divergent outputs, raising concerns about reproducibility of LLM-based evaluations and highlights the output variability under different stochastic regimes, emphasizing the need of a broader diagnostic framework to understand potential pitfalls in real-world clinical decision support scenarios.

</details>


### [478] [Concept Attractors in LLMs and their Applications](https://arxiv.org/abs/2601.11575)
*Sotirios Panagiotis Chytas,Vikas Singh*

Main category: cs.CL

TL;DR: 论文指出大语言模型特定层对语义相关提示有相似内部表征，可用IFS解释，基于此开发免训练方法解决多种任务且效果良好。


<details>
  <summary>Details</summary>
Motivation: 解释大语言模型特定层对语义相关提示产生相似内部表征的行为。

Method: 使用迭代函数系统（IFS），并基于吸引子开发简单免训练方法。

Result: 基于吸引子的干预方法效果与或超过专门基线方法。

Conclusion: 基于吸引子的方法是重微调的有效替代，在基线表现不佳的场景有泛化性。

Abstract: Large language models (LLMs) often map semantically related prompts to similar internal representations at specific layers, even when their surface forms differ widely. We show that this behavior can be explained through Iterated Function Systems (IFS), where layers act as contractive mappings toward concept-specific Attractors. We leverage this insight and develop simple, training-free methods that operate directly on these Attractors to solve a wide range of practical tasks, including language translation, hallucination reduction, guardrailing, and synthetic data generation. Despite their simplicity, these Attractor-based interventions match or exceed specialized baselines, offering an efficient alternative to heavy fine-tuning, generalizable in scenarios where baselines underperform.

</details>


### [479] [LimAgents: Multi-Agent LLMs for Generating Research Limitations](https://arxiv.org/abs/2601.11578)
*Ibrahim Al Azher,Zhishuai Guo,Hamed Alhoori*

Main category: cs.CL

TL;DR: 提出LimAgents多智能体大语言模型框架生成实质性研究局限性描述，引入新评估协议，实验显示性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 零样本大语言模型生成的局限性表述浅显，作者披露的局限性也不完整，传统评估指标有缺陷。

Method: 提出LimAgents框架集成评论和已声明局限性，让不同智能体承担特定角色；引入逐点评估协议用LLM衡量覆盖率。

Result: RAG + 多智能体GPT - 4o mini配置比零样本基线覆盖率提升15.51%，Llama 3 8B多智能体设置提升4.41%。

Conclusion: LimAgents能有效改进生成研究局限性的性能。

Abstract: Identifying and articulating limitations is essential for transparent and rigorous scientific research. However, zero-shot large language models (LLMs) approach often produce superficial or general limitation statements (e.g., dataset bias or generalizability). They usually repeat limitations reported by authors without looking at deeper methodological issues and contextual gaps. This problem is made worse because many authors disclose only partial or trivial limitations. We propose LimAgents, a multi-agent LLM framework for generating substantive limitations. LimAgents integrates OpenReview comments and author-stated limitations to provide stronger ground truth. It also uses cited and citing papers to capture broader contextual weaknesses. In this setup, different agents have specific roles as sequential role: some extract explicit limitations, others analyze methodological gaps, some simulate the viewpoint of a peer reviewer, and a citation agent places the work within the larger body of literature. A Judge agent refines their outputs, and a Master agent consolidates them into a clear set. This structure allows for systematic identification of explicit, implicit, peer review-focused, and literature-informed limitations. Moreover, traditional NLP metrics like BLEU, ROUGE, and cosine similarity rely heavily on n-gram or embedding overlap. They often overlook semantically similar limitations. To address this, we introduce a pointwise evaluation protocol that uses an LLM-as-a-Judge to measure coverage more accurately. Experiments show that LimAgents substantially improve performance. The RAG + multi-agent GPT-4o mini configuration achieves a +15.51% coverage gain over zero-shot baselines, while the Llama 3 8B multi-agent setup yields a +4.41% improvement.

</details>


### [480] [Bielik 11B v3: Multilingual Large Language Model for European Languages](https://arxiv.org/abs/2601.11579)
*Krzysztof Ociepa,Łukasz Flis,Remigiusz Kinas,Krzysztof Wróbel,Adrian Gwoździej*

Main category: cs.CL

TL;DR: 介绍了专为波兰语优化的Bielik 11B v3语言模型，达到了先进水平，在多方面表现出色并为小语种建模树立了新标杆。


<details>
  <summary>Details</summary>
Motivation: 开发针对波兰语优化且在欧洲其他语言也有能力的先进语言模型，促进波兰语的AI能力发展，树立小语种资源高效、高性能模型标杆。

Method: 扩展Mistral 7B v0.2架构，通过深度扩展到11B参数，采用连续预训练、监督微调（SFT）、直接偏好优化（DPO）和强化学习的四阶段训练管道。

Result: 在广泛任务中超越其他波兰语专用模型和许多更大参数的模型，有卓越表现；参数效率高且量化选项丰富，可在不同硬件配置上有效部署。

Conclusion: Bielik 11B v3不仅提升了波兰语的AI能力，还为小语种开发资源高效、高性能模型建立了新基准。

Abstract: We present Bielik 11B v3, a state-of-the-art language model highly optimized for the Polish language, while also maintaining strong capabilities in other European languages. This model extends the Mistral 7B v0.2 architecture, scaled to 11B parameters via depth up-scaling. Its development involved a comprehensive four-stage training pipeline: continuous pre-training, supervised fine-tuning (SFT), Direct Preference Optimization (DPO), and reinforcement learning.
  Comprehensive evaluations demonstrate that Bielik 11B v3 achieves exceptional performance. It significantly surpasses other specialized Polish language models and outperforms many larger models (with 2-6 times more parameters) on a wide range of tasks, from basic linguistic understanding to complex reasoning.
  The model's parameter efficiency, combined with extensive quantization options, allows for effective deployment across diverse hardware configurations. Bielik 11B v3 not only advances AI capabilities for the Polish language but also establishes a new benchmark for developing resource-efficient, high-performance models for less-represented languages.

</details>


### [481] [Speculative Decoding: Performance or Illusion?](https://arxiv.org/abs/2601.11580)
*Xiaoxuan Liu,Jiaxiang Yu,Jongseok Park,Ion Stoica,Alvin Cheung*

Main category: cs.CL

TL;DR: 对生产级推理引擎vLLM上的投机解码（SD）进行系统研究，分析性能关键因素和理论加速上限，揭示实际与理论性能差距及研究机会。


<details>
  <summary>Details</summary>
Motivation: 先前对投机解码的评估基于研究原型和小批量，其实际有效性不明，需进行系统研究。

Method: 在生产级推理引擎vLLM上对多种SD变体进行研究，涵盖不同工作负载、模型规模和批量大小，分析性能关键因素并量化理论加速上限。

Result: 目标模型验证主导执行，接受长度在不同方面差异大，实际与理论上限有显著差距。

Conclusion: 研究揭示了实际与理论性能差距，为改进投机解码提供了新的研究机会。

Abstract: Speculative decoding (SD) has become a popular technique to accelerate Large Language Model (LLM) inference, yet its real-world effectiveness remains unclear as prior evaluations rely on research prototypes and unrealistically small batch sizes. We present, to our knowledge, the first systematic study of SD on a production-grade and widely deployed inference engine (vLLM), covering multiple SD variants ($n$-gram, EAGLE/EAGLE-3, Draft-Model, Multi-Token Prediction) across diverse workloads, model scales, and batch sizes. We analyze key factors governing SD performance, and quantify a theoretical upper bound on SD speedup. Our results show that verification by the target model dominates the execution, while acceptance length varies markedly across output token positions, requests, and datasets. Comparing measured performance with theoretical bounds reveals substantial gaps between observed and theoretical upper bounds, and we leverage this observation to highlight new research opportunities that our study opens up in improving SD.

</details>


### [482] [Enhancing the QA Model through a Multi-domain Debiasing Framework](https://arxiv.org/abs/2601.11581)
*Yuefeng Wang,ChangJae Lee*

Main category: cs.CL

TL;DR: 研究评估ELECTRA - small模型在问答数据集上表现，开发多领域去偏框架，提升了模型分数，凸显去偏策略潜力。


<details>
  <summary>Details</summary>
Motivation: 问答模型存在偏差影响复杂查询表现，尤其是在对抗条件下，需提升其鲁棒性和可靠性。

Method: 在SQuAD v1.1、AddSent和AddOneSent数据集评估ELECTRA - small模型，识别相关错误，开发包含知识蒸馏、去偏技术和领域扩展的多领域去偏框架。

Result: 在所有测试集上精确匹配（EM）和F1分数最多提高2.6个百分点，在对抗环境中也有提升。

Conclusion: 有针对性的去偏策略能增强自然语言理解系统的鲁棒性和可靠性。

Abstract: Question-answering (QA) models have advanced significantly in machine reading comprehension but often exhibit biases that hinder their performance, particularly with complex queries in adversarial conditions. This study evaluates the ELECTRA-small model on the Stanford Question Answering Dataset (SQuAD) v1.1 and adversarial datasets AddSent and AddOneSent. By identifying errors related to lexical bias, numerical reasoning, and entity recognition, we develop a multi-domain debiasing framework incorporating knowledge distillation, debiasing techniques, and domain expansion. Our results demonstrate up to 2.6 percentage point improvements in Exact Match (EM) and F1 scores across all test sets, with gains in adversarial contexts. These findings highlight the potential of targeted bias mitigation strategies to enhance the robustness and reliability of natural language understanding systems.

</details>


### [483] [Towards AGI A Pragmatic Approach Towards Self Evolving Agent](https://arxiv.org/abs/2601.11658)
*Indrajit Kar,Sammy Zonunpuia,Zonunfeli Ralte*

Main category: cs.CL

TL;DR: 本文提出分层自我进化多智能体框架使基于大语言模型的智能体具备持续适应能力，用TaskCraft数据集评估不同进化范式，进化后的智能体表现更优。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的智能体部署后缺乏自主扩展能力、生成新工具和进化推理的能力，需要一种方法使其能持续适应。

Method: 引入分层自我进化多智能体框架，集成基础大模型、操作智能体、代码生成大模型和教师大模型；任务处理不成功时进行工具合成，持续失败则触发进化阶段，使用课程学习、基于奖励的学习和遗传算法进行进化。

Result: 课程学习恢复快、泛化性强；基于奖励的学习在高难度任务中表现出色；遗传算法提供高行为多样性。进化后的智能体在所有设置中都优于原始智能体。

Conclusion: 所提出的框架能实现智能体强大、自主、自我改进的进化。

Abstract: Large Language Model (LLM) based agents are powerful yet fundamentally static after deployment, lacking the ability to autonomously expand capabilities, generate new tools, or evolve their reasoning. This work introduces a hierarchical self-evolving multi-agent framework that integrates a Base LLM, an operational SLM agent, a Code-Generation LLM, and a Teacher-LLM to enable continuous adaptation. The workflow begins with the agent attempting a task using reasoning and existing tools; if unsuccessful, it escalates to tool synthesis through the Code-Gen LLM, and when failures persist, it triggers an evolution phase using Curriculum Learning (CL), Reward-Based Learning (RL), or Genetic Algorithm (GA) evolution. Using the TaskCraft dataset rich in hierarchical tasks, tool-use traces, and difficulty scaling we evaluate these paradigms. CL delivers fast recovery and strong generalization, RL excels on high-difficulty tasks, and GA offers high behavioral diversity. Across all settings, evolved agents outperform their originals, demonstrating robust, autonomous, self-improving agentic evolution.

</details>


### [484] [LIME-LLM: Probing Models with Fluent Counterfactuals, Not Broken Text](https://arxiv.org/abs/2601.11746)
*George Mihaila,Suleyman Olcay Polat,Poli Nemkova,Himanshu Sharma,Namratha V. Urs,Mark V. Albert*

Main category: cs.CL

TL;DR: 现有NLP局部解释方法有缺陷，本文提出LIME - LLM框架，用假设驱动的受控扰动替代随机噪声，实验显示该框架提升了解释保真度。


<details>
  <summary>Details</summary>
Motivation: 现有基于随机标记掩蔽的局部解释方法生成语义无效、分布外的输入，削弱局部代理模型的保真度；而生成式方法存在引入混杂变量的问题。

Method: 引入LIME - LLM框架，采用“单掩码 - 单样本”协议和不同的填充策略，构建流畅、符合流形的邻域。

Result: 与多个基线方法在三个基准数据集上对比，LIME - LLM显著提升了局部解释的保真度。

Conclusion: LIME - LLM为黑盒NLP可解释性建立了新的基准。

Abstract: Local explanation methods such as LIME (Ribeiro et al., 2016) remain fundamental to trustworthy AI, yet their application to NLP is limited by a reliance on random token masking. These heuristic perturbations frequently generate semantically invalid, out-of-distribution inputs that weaken the fidelity of local surrogate models. While recent generative approaches such as LLiMe (Angiulli et al., 2025b) attempt to mitigate this by employing Large Language Models for neighborhood generation, they rely on unconstrained paraphrasing that introduces confounding variables, making it difficult to isolate specific feature contributions. We introduce LIME-LLM, a framework that replaces random noise with hypothesis-driven, controlled perturbations. By enforcing a strict "Single Mask-Single Sample" protocol and employing distinct neutral infill and boundary infill strategies, LIME-LLM constructs fluent, on-manifold neighborhoods that rigorously isolate feature effects. We evaluate our method against established baselines (LIME, SHAP, Integrated Gradients) and the generative LLiMe baseline across three diverse benchmarks: CoLA, SST-2, and HateXplain using human-annotated rationales as ground truth. Empirical results demonstrate that LIME-LLM establishes a new benchmark for black-box NLP explainability, achieving significant improvements in local explanation fidelity compared to both traditional perturbation-based methods and recent generative alternatives.

</details>


### [485] [Early Linguistic Pattern of Anxiety from Social Media Using Interpretable Linguistic Features: A Multi-Faceted Validation Study with Author-Disjoint Evaluation](https://arxiv.org/abs/2601.11758)
*Arnab Das Utsa*

Main category: cs.CL

TL;DR: 本文提出基于社交媒体的透明焦虑检测方法，用Reddit数据训练模型，经多种评估，证明透明语言特征可实现可靠、可泛化且关键词鲁棒的焦虑检测，为跨语境心理健康筛查提供基线。


<details>
  <summary>Details</summary>
Motivation: 全球数亿人受焦虑影响，但大规模筛查受限，现有社交媒体检测模型存在缺乏可解释性、关键词鲁棒性验证和用户级数据完整性等问题。

Method: 采用基于语言可解释特征的建模和跨领域验证方法，用Reddit帖子数据集，在精心挑选的子版块数据上训练逻辑回归分类器，并进行特征消融、关键词掩码实验等综合评估。

Result: 模型表现良好，去除情感或掩码关键词后仍保持高精度，少量帖子历史的早期检测远超随机分类，跨领域分析与临床访谈数据高度一致。

Conclusion: 透明语言特征能支持可靠、可泛化且关键词鲁棒的焦虑检测，所提框架为跨不同在线语境的可解释心理健康筛查提供可复现基线。

Abstract: Anxiety affects hundreds of millions of individuals globally, yet large-scale screening remains limited. Social media language provides an opportunity for scalable detection, but current models often lack interpretability, keyword-robustness validation, and rigorous user-level data integrity. This work presents a transparent approach to social media-based anxiety detection through linguistically interpretable feature-grounded modeling and cross-domain validation. Using a substantial dataset of Reddit posts, we trained a logistic regression classifier on carefully curated subreddits for training, validation, and test splits. Comprehensive evaluation included feature ablation, keyword masking experiments, and varying-density difference analyses comparing anxious and control groups, along with external validation using clinically interviewed participants with diagnosed anxiety disorders. The model achieved strong performance while maintaining high accuracy even after sentiment removal or keyword masking. Early detection using minimal post history significantly outperformed random classification, and cross-domain analysis demonstrated strong consistency with clinical interview data. Results indicate that transparent linguistic features can support reliable, generalizable, and keyword-robust anxiety detection. The proposed framework provides a reproducible baseline for interpretable mental health screening across diverse online contexts.

</details>


### [486] [Industry-Aligned Granular Topic Modeling](https://arxiv.org/abs/2601.11762)
*Sae Young Moon,Myeongjun Erik Jang,Haoyan Luo,Chunyang Xiao,Antonios Georgiadis,Fran Silavong*

Main category: cs.CL

TL;DR: 本文提出TIDE框架用于粒度主题建模，实验表明其优于现代方法且有辅助组件支持业务，框架正开源。


<details>
  <summary>Details</summary>
Motivation: 主题建模方法生成粒度主题的能力未被充分探索，但粒度概念对商业应用有重要价值，需新方法。

Method: 引入TIDE框架，以大语言模型为核心提供粒度主题建模方法，还有总结长文档、主题父化和蒸馏等功能。

Result: 在多种公共和真实商业数据集上实验，TIDE的主题建模方法优于现代方法，辅助组件能支持工业商业场景。

Conclusion: TIDE框架在粒度主题建模上表现出色，有应用价值，目前正进行开源。

Abstract: Topic modeling has extensive applications in text mining and data analysis across various industrial sectors. Although the concept of granularity holds significant value for business applications by providing deeper insights, the capability of topic modeling methods to produce granular topics has not been thoroughly explored. In this context, this paper introduces a framework called TIDE, which primarily provides a novel granular topic modeling method based on large language models (LLMs) as a core feature, along with other useful functionalities for business applications, such as summarizing long documents, topic parenting, and distillation. Through extensive experiments on a variety of public and real-world business datasets, we demonstrate that TIDE's topic modeling approach outperforms modern topic modeling methods, and our auxiliary components provide valuable support for dealing with industrial business scenarios. The TIDE framework is currently undergoing the process of being open sourced.

</details>


### [487] [Cleansing the Artificial Mind: A Self-Reflective Detoxification Framework for Large Language Models](https://arxiv.org/abs/2601.11776)
*Kaituo Zhang,Zhimeng Jiang,Na Zou*

Main category: cs.CL

TL;DR: 本文提出全自反思排毒框架，利用大语言模型内在能力检测和纠正有毒内容，实验表明性能优于现有方法，揭示模型自我排毒能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型排毒技术依赖外部模块、数据标注或人工干预，阻碍可扩展性和一致性，需利用模型内置能力。

Method: 提出有毒信号检测器和系统干预流程，生成对比排毒数据集微调模型。

Result: 在DetoxLLM和ParaDetox等基准数据集实验中，方法排毒性能优于现有方法，且保留语义保真度。

Conclusion: 揭示大语言模型内在自我排毒能力，为减轻有害内容生成提供有效方法，为自调节语言模型发展奠定基础。

Abstract: Recent breakthroughs in Large Language Models (LLMs) have revealed remarkable generative capabilities and emerging self-regulatory mechanisms, including self-correction and self-rewarding. However, current detoxification techniques rarely exploit these built-in abilities; instead, they rely on external modules, labor-intensive data annotation, or human intervention --factors that hinder scalability and consistency. In this paper, we introduce a fully self-reflective detoxification framework that harnesses the inherent capacities of LLMs to detect, correct toxic content, and refine LLMs without external modules and data annotation. Specifically, we propose a Toxic Signal Detector --an internal self-identification mechanism, coupled with a systematic intervention process to transform toxic text into its non-toxic counterpart. This iterative procedure yields a contrastive detoxification dataset used to fine-tune the model, enhancing its ability for safe and coherent text generation. Experiments on benchmark datasets such as DetoxLLM and ParaDetox show that our method achieves better detoxification performance than state-of-the-art methods while preserving semantic fidelity. By obviating the need for human intervention or external components, this paper reveals the intrinsic self-detoxification ability of LLMs, offering a consistent and effective approach for mitigating harmful content generation. Ultimately, our findings underscore the potential for truly self-regulated language models, paving the way for more responsible and ethically guided text generation systems.

</details>


### [488] [Translation as a Scalable Proxy for Multilingual Evaluation](https://arxiv.org/abs/2601.11778)
*Sheriff Issaka,Erick Rosas Gonzalez,Lieqi Liu,Evans Kofi Agyei,Lucas Bandarkar,Nanyun Peng,David Ifeoluwa Adelani,Francisco Guzmán,Saadia Gabriel*

Main category: cs.CL

TL;DR: 研究评估翻译质量能否指示模型多语言能力，发现翻译性能可作为多语言表现代理。


<details>
  <summary>Details</summary>
Motivation: 大语言模型宣称多语言能力，但多数语言缺乏非机器翻译基准，传统基准构建有挑战，需探索简单替代方法。

Method: 对14个不同参数模型在9个基准和7个翻译指标上进行系统评估。

Result: 翻译性能是下游任务成功的良好指标，如Phi - 4在多个指标上相关性高。

Conclusion: 翻译质量可作为多语言性能的有力、低成本初步代理，可进行翻译优先筛选和针对性跟进。

Abstract: The rapid proliferation of LLMs has created a critical evaluation paradox: while LLMs claim multilingual proficiency, comprehensive non-machine-translated benchmarks exist for fewer than 30 languages, leaving >98% of the world's 7,000 languages in an empirical void. Traditional benchmark construction faces scaling challenges such as cost, scarcity of domain experts, and data contamination. We evaluate the validity of a simpler alternative: can translation quality alone indicate a model's broader multilingual capabilities? Through systematic evaluation of 14 models (1B-72B parameters) across 9 diverse benchmarks and 7 translation metrics, we find that translation performance is a good indicator of downstream task success (e.g., Phi-4, median Pearson r: MetricX = 0.89, xCOMET = 0.91, SSA-COMET = 0.87). These results suggest that the representational abilities supporting faithful translation overlap with those required for multilingual understanding. Translation quality, thus emerges as a strong, inexpensive first-pass proxy of multilingual performance, enabling a translation-first screening with targeted follow-up for specific tasks.

</details>


### [489] [ATOD: An Evaluation Framework and Benchmark for Agentic Task-Oriented Dialogue System](https://arxiv.org/abs/2601.11854)
*Yifei Zhang,Hooshang Nayyeri,Rinat Khaziev,Emine Yilmaz,Gokhan Tur,Dilek Hakkani-Tür,Hari Thadakamalla*

Main category: cs.CL

TL;DR: 本文引入ATOD基准和对话生成管道，提出ATOD - Eval评估框架与评估器，实验表明评估框架能综合评估，评估器有更好的准确率 - 效率权衡。


<details>
  <summary>Details</summary>
Motivation: 现有基准缺乏对先进任务导向对话系统代理行为的系统评估支持。

Method: 引入ATOD基准和对话生成管道；提出ATOD - Eval评估框架；给出基于代理记忆的评估器。

Result: ATOD - Eval能在任务完成、代理能力和响应质量方面进行综合评估，评估器在该评估设置下比现有基于记忆和大语言模型的方法有更好的准确率 - 效率权衡。

Conclusion: ATOD及ATOD - Eval能有效评估先进任务导向对话系统的代理行为，所提出评估器有优势。

Abstract: Recent advances in task-oriented dialogue (TOD) systems, driven by large language models (LLMs) with extensive API and tool integration, have enabled conversational agents to coordinate interleaved goals, maintain long-horizon context, and act proactively through asynchronous execution. These capabilities extend beyond traditional TOD systems, yet existing benchmarks lack systematic support for evaluating such agentic behaviors. To address this gap, we introduce ATOD, a benchmark and synthetic dialogue generation pipeline that produces richly annotated conversations requiring long-term reasoning. ATOD captures key characteristics of advanced TOD, including multi-goal coordination, dependency management, memory, adaptability, and proactivity. Building on ATOD, we propose ATOD-Eval, a holistic evaluation framework that translates these dimensions into fine-grained metrics and supports reproducible offline and online evaluation. We further present a strong agentic memory-based evaluator for benchmarking on ATOD. Experiments show that ATOD-Eval enables comprehensive assessment across task completion, agentic capability, and response quality, and that the proposed evaluator offers a better accuracy-efficiency tradeoff compared to existing memory- and LLM-based approaches under this evaluation setting.

</details>


### [490] [LSTM-MAS: A Long Short-Term Memory Inspired Multi-Agent System for Long-Context Understanding](https://arxiv.org/abs/2601.11913)
*Yichen Jiang,Peng Ye,Jiakang Yuan,Chongjun Tu,Lei Bai,Tao Chen*

Main category: cs.CL

TL;DR: 为解决大语言模型长上下文处理难题，受LSTM启发设计多智能体系统LSTM - MAS，评估显示其在多项任务上比之前最佳多智能体方法有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有单大语言模型长上下文处理方法有计算成本高或扩展长度受限问题，多智能体框架易出现错误积累和幻觉传播问题，需一种新方法解决这些难题。

Method: 受LSTM架构启发，设计多智能体系统LSTM - MAS，采用链式架构，每个节点包含不同功能的智能体，模拟LSTM的信息流动和记忆机制。

Result: 与之前最佳多智能体方法CoA相比，LSTM - MAS在NarrativeQA、Qasper、HotpotQA和MuSiQue上分别提升40.93%、43.70%、121.57% 和33.12%。

Conclusion: LSTM - MAS的设计能有效控制信息传递和选择性建模长期依赖，可避免错误积累和幻觉传播，在长上下文处理上有较好效果。

Abstract: Effectively processing long contexts remains a fundamental yet unsolved challenge for large language models (LLMs). Existing single-LLM-based methods primarily reduce the context window or optimize the attention mechanism, but they often encounter additional computational costs or constrained expanded context length. While multi-agent-based frameworks can mitigate these limitations, they remain susceptible to the accumulation of errors and the propagation of hallucinations. In this work, we draw inspiration from the Long Short-Term Memory (LSTM) architecture to design a Multi-Agent System called LSTM-MAS, emulating LSTM's hierarchical information flow and gated memory mechanisms for long-context understanding. Specifically, LSTM-MAS organizes agents in a chained architecture, where each node comprises a worker agent for segment-level comprehension, a filter agent for redundancy reduction, a judge agent for continuous error detection, and a manager agent for globally regulates information propagation and retention, analogous to LSTM and its input gate, forget gate, constant error carousel unit, and output gate. These novel designs enable controlled information transfer and selective long-term dependency modeling across textual segments, which can effectively avoid error accumulation and hallucination propagation. We conducted an extensive evaluation of our method. Compared with the previous best multi-agent approach, CoA, our model achieves improvements of 40.93%, 43.70%,121.57% and 33.12%, on NarrativeQA, Qasper, HotpotQA, and MuSiQue, respectively.

</details>


### [491] [Enhancing LLM-Based Data Annotation with Error Decomposition](https://arxiv.org/abs/2601.11920)
*Zhen Xu,Vedant Khatri,Yijun Dai,Xiner Liu,Siyan Li,Xuanming Zhang,Renzhe Yu*

Main category: cs.CL

TL;DR: 论文提出用于大语言模型（LLM）主观数据标注任务的诊断评估范式，在教育标注任务验证其有效性与实用性。


<details>
  <summary>Details</summary>
Motivation: LLM在主观标注任务上表现不稳定、易出错，标准评估方法不能区分不同错误对结论的影响。

Method: 提出含人类介入步骤的诊断评估范式，细化用于序数标注任务，包括错误分类、轻量级人工标注测试和计算方法。

Result: 在四个教育标注任务验证范式的概念有效性和实用价值。

Conclusion: 理论上揭示特定标注任务高一致性不现实、单一指标不适用的原因；实践中可作为低成本诊断工具指导任务适配性与技术优化。

Abstract: Large language models offer a scalable alternative to human coding for data annotation tasks, enabling the scale-up of research across data-intensive domains. While LLMs are already achieving near-human accuracy on objective annotation tasks, their performance on subjective annotation tasks, such as those involving psychological constructs, is less consistent and more prone to errors. Standard evaluation practices typically collapse all annotation errors into a single alignment metric, but this simplified approach may obscure different kinds of errors that affect final analytical conclusions in different ways. Here, we propose a diagnostic evaluation paradigm that incorporates a human-in-the-loop step to separate task-inherent ambiguity from model-driven inaccuracies and assess annotation quality in terms of their potential downstream impacts. We refine this paradigm on ordinal annotation tasks, which are common in subjective annotation. The refined paradigm includes: (1) a diagnostic taxonomy that categorizes LLM annotation errors along two dimensions: source (model-specific vs. task-inherent) and type (boundary ambiguity vs. conceptual misidentification); (2) a lightweight human annotation test to estimate task-inherent ambiguity from LLM annotations; and (3) a computational method to decompose observed LLM annotation errors following our taxonomy. We validate this paradigm on four educational annotation tasks, demonstrating both its conceptual validity and practical utility. Theoretically, our work provides empirical evidence for why excessively high alignment is unrealistic in specific annotation tasks and why single alignment metrics inadequately reflect the quality of LLM annotations. In practice, our paradigm can be a low-cost diagnostic tool that assesses the suitability of a given task for LLM annotation and provides actionable insights for further technical optimization.

</details>


### [492] [Double-Calibration: Towards Trustworthy LLMs via Calibrating Knowledge and Reasoning Confidence](https://arxiv.org/abs/2601.11956)
*Yuyin Lu,Ziran Liang,Yanghui Rao,Wenqi Fan,Fu Lee Wang,Qing Li*

Main category: cs.CL

TL;DR: 引入DoublyCal框架解决大语言模型推理不可靠问题，通过实验证明其能以低token成本提高黑盒大语言模型的准确性和置信度校准。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理受幻觉影响，现有知识图谱增强方法无法量化检索证据和大语言模型推理的认知不确定性。

Method: 引入基于双重校准原则的DoublyCal框架，使用轻量级代理模型生成带校准证据置信度的知识图谱证据，引导黑盒大语言模型做出推理。

Result: 在知识密集型基准测试中，DoublyCal能以低token成本显著提高黑盒大语言模型的准确性和置信度校准。

Conclusion: DoublyCal框架有效解决大语言模型推理的不确定性问题，提升推理的可靠性。

Abstract: Trustworthy reasoning in Large Language Models (LLMs) is challenged by their propensity for hallucination. While augmenting LLMs with Knowledge Graphs (KGs) improves factual accuracy, existing KG-augmented methods fail to quantify epistemic uncertainty in both the retrieved evidence and LLMs' reasoning. To bridge this gap, we introduce DoublyCal, a framework built on a novel double-calibration principle. DoublyCal employs a lightweight proxy model to first generate KG evidence alongside a calibrated evidence confidence. This calibrated supporting evidence then guides a black-box LLM, yielding final predictions that are not only more accurate but also well-calibrated, with confidence scores traceable to the uncertainty of the supporting evidence. Experiments on knowledge-intensive benchmarks show that DoublyCal significantly improves both the accuracy and confidence calibration of black-box LLMs with low token cost.

</details>


### [493] [$\texttt{MemoryRewardBench}$: Benchmarking Reward Models for Long-Term Memory Management in Large Language Models](https://arxiv.org/abs/2601.11969)
*Zecheng Tang,Baibei Ji,Ruoxi Sun,Haitian Wang,WangJie You,Zhang Yijun,Wenpeng Zhu,Ji Qi,Juntao Li,Min Zhang*

Main category: cs.CL

TL;DR: 本文介绍首个评估奖励模型评估长期内存管理能力的基准MemoryRewardBench，对13个前沿奖励模型评估，揭示当前奖励模型能力与局限。


<details>
  <summary>Details</summary>
Motivation: 现有工作采用以内存为中心机制处理长上下文，有效内存管理很关键，需利用奖励模型自动可靠评估内存质量。

Method: 引入MemoryRewardBench基准，涵盖长上下文理解和长文本生成任务，有10种不同内存管理模式设置，上下文长度8K到128K。

Result: 对13个奖励模型评估显示，开源和闭源模型性能差距缩小，新一代模型表现更好。

Conclusion: 揭示当前奖励模型在不同设置下评估大语言模型内存管理的能力和基本局限。

Abstract: Existing works increasingly adopt memory-centric mechanisms to process long contexts in a segment manner, and effective memory management is one of the key capabilities that enables large language models to effectively propagate information across the entire sequence. Therefore, leveraging reward models (RMs) to automatically and reliably evaluate memory quality is critical. In this work, we introduce $\texttt{MemoryRewardBench}$, the first benchmark to systematically study the ability of RMs to evaluate long-term memory management processes. $\texttt{MemoryRewardBench}$ covers both long-context comprehension and long-form generation tasks, featuring 10 distinct settings with different memory management patterns, with context length ranging from 8K to 128K tokens. Evaluations on 13 cutting-edge RMs indicate a diminishing performance gap between open-source and proprietary models, with newer-generation models consistently outperforming their predecessors regardless of parameter count. We further expose the capabilities and fundamental limitations of current RMs in evaluating LLM memory management across diverse settings.

</details>


### [494] [Acting Flatterers via LLMs Sycophancy: Combating Clickbait with LLMs Opposing-Stance Reasoning](https://arxiv.org/abs/2601.12019)
*Chaowei Zhang,Xiansheng Luo,Zewei Zhang,Yi Zhu,Jipeng Qiang,Longwei Wang*

Main category: cs.CL

TL;DR: 文章提出利用大语言模型的附和倾向生成对比推理来检测标题党，实验显示该方法优于其他基线。


<details>
  <summary>Details</summary>
Motivation: 网络内容泛滥使标题党问题受关注，大语言模型处理此问题时的附和行为偏离指令遵循原则，但可被利用。

Method: 提出SORG框架生成新闻标题的正反推理对，开发基于局部对立推理的ORCD模型，集成三个BERT编码器，结合对比学习和软标签提升检测鲁棒性。

Result: 在三个基准数据集上的实验表明，该方法优于大语言模型提示、微调的小语言模型和现有标题党检测基线。

Conclusion: 所提方法在标题党检测上有效，能利用大语言模型的附和行为提升检测效果。

Abstract: The widespread proliferation of online content has intensified concerns about clickbait, deceptive or exaggerated headlines designed to attract attention. While Large Language Models (LLMs) offer a promising avenue for addressing this issue, their effectiveness is often hindered by Sycophancy, a tendency to produce reasoning that matches users' beliefs over truthful ones, which deviates from instruction-following principles. Rather than treating sycophancy as a flaw to be eliminated, this work proposes a novel approach that initially harnesses this behavior to generate contrastive reasoning from opposing perspectives. Specifically, we design a Self-renewal Opposing-stance Reasoning Generation (SORG) framework that prompts LLMs to produce high-quality agree and disagree reasoning pairs for a given news title without requiring ground-truth labels. To utilize the generated reasoning, we develop a local Opposing Reasoning-based Clickbait Detection (ORCD) model that integrates three BERT encoders to represent the title and its associated reasoning. The model leverages contrastive learning, guided by soft labels derived from LLM-generated credibility scores, to enhance detection robustness. Experimental evaluations on three benchmark datasets demonstrate that our method consistently outperforms LLM prompting, fine-tuned smaller language models, and state-of-the-art clickbait detection baselines.

</details>


### [495] [Codebook-Injected Dialogue Segmentation for Multi-Utterance Constructs Annotation: LLM-Assisted and Gold-Label-Free Evaluation](https://arxiv.org/abs/2601.12061)
*Jinsook Lee,Kirk Vanacore,Zhuqian Zhou,Jeanine Grutter,Rene F. Kizilcec*

Main category: cs.CL

TL;DR: 提出代码本注入分割方法评估基于大语言模型的分割器，发现DA感知分割有优势，各分割器各有优劣，分割应按需优化。


<details>
  <summary>Details</summary>
Motivation: 传统对话行为标注因分割边界问题降低可靠性，需改进分割方法。

Method: 提出代码本注入分割方法，用评估指标评估基于大语言模型的分割器与基线模型。

Result: DA感知分割内部更一致，大语言模型擅长创建一致跨度，基于连贯性的基线模型更擅检测对话流全局变化，各数据集无单一主导分割器，内聚性与边界独特性等存在权衡。

Conclusion: 分割是重要设计选择，应根据下游目标优化而非单一性能得分。

Abstract: Dialogue Act (DA) annotation typically treats communicative or pedagogical intent as localized to individual utterances or turns. This leads annotators to agree on the underlying action while disagreeing on segment boundaries, reducing apparent reliability. We propose codebook-injected segmentation, which conditions boundary decisions on downstream annotation criteria, and evaluate LLM-based segmenters against standard and retrieval-augmented baselines. To assess these without gold labels, we introduce evaluation metrics for span consistency, distinctiveness, and human-AI distributional agreement. We found DA-awareness produces segments that are internally more consistent than text-only baselines. While LLMs excel at creating construct-consistent spans, coherence-based baselines remain superior at detecting global shifts in dialogue flow. Across two datasets, no single segmenter dominates. Improvements in within-segment coherence frequently trade off against boundary distinctiveness and human-AI distributional agreement. These results highlight segmentation as a consequential design choice that should be optimized for downstream objectives rather than a single performance score.

</details>


### [496] [Bridging the Gap in Bangla Healthcare: Machine Learning Based Disease Prediction Using a Symptoms-Disease Dataset](https://arxiv.org/abs/2601.12068)
*Rowzatul Zannat,Abdullah Al Shafi,Abdul Muntakim*

Main category: cs.CL

TL;DR: 研究构建孟加拉语症状 - 疾病数据集用于疾病预测，评估多种机器学习模型，集成方法准确率达98%，为相关领域奠基。


<details>
  <summary>Details</summary>
Motivation: 非英语人群获取可靠健康信息重要，但孟加拉语疾病预测资源有限，旨在填补该空白。

Method: 开发含758个症状 - 疾病关系、85种疾病的孟加拉语数据集，公开数据，用其评估多种机器学习模型预测疾病。

Result: 软、硬投票集成方法结合表现最佳模型，准确率达98%，展现强大鲁棒性和泛化能力。

Conclusion: 为孟加拉语疾病预测提供基础资源，推动本地化健康信息学和诊断工具进步，促进孟加拉语社区公平获取健康信息。

Abstract: Increased access to reliable health information is essential for non-English-speaking populations, yet resources in Bangla for disease prediction remain limited. This study addresses this gap by developing a comprehensive Bangla symptoms-disease dataset containing 758 unique symptom-disease relationships spanning 85 diseases. To ensure transparency and reproducibility, we also make our dataset publicly available. The dataset enables the prediction of diseases based on Bangla symptom inputs, supporting healthcare accessibility for Bengali-speaking populations. Using this dataset, we evaluated multiple machine learning models to predict diseases based on symptoms provided in Bangla and analyzed their performance on our dataset. Both soft and hard voting ensemble approaches combining top-performing models achieved 98\% accuracy, demonstrating superior robustness and generalization. Our work establishes a foundational resource for disease prediction in Bangla, paving the way for future advancements in localized health informatics and diagnostic tools. This contribution aims to enhance equitable access to health information for Bangla-speaking communities, particularly for early disease detection and healthcare interventions.

</details>


### [497] [Large language models struggle with ethnographic text annotation](https://arxiv.org/abs/2601.12099)
*Leonardo S. Goodall,Dor Shilton,Daniel A. Mullins,Harvey Whitehouse*

Main category: cs.CL

TL;DR: 评估7个大语言模型对民族志文本进行仪式特征注释的能力，结果显示其性能有限，尚不能替代人类专业知识。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自动文本注释方面有前景，期望其加速跨文化研究，因此评估其在民族志文本注释的能力。

Method: 评估7个最先进的大语言模型对567篇民族志摘录中121个仪式特征的注释能力。

Result: 模型表现有限，远低于可靠自动注释所需水平，长文本、需顺序区分的特征和模糊概念较难处理，且即便人类可达成可靠共识的特征，模型表现也不如人类。

Conclusion: 大语言模型目前不能替代人类在民族志注释中的专业知识。

Abstract: Large language models (LLMs) have shown promise for automated text annotation, raising hopes that they might accelerate cross-cultural research by extracting structured data from ethnographic texts. We evaluated 7 state-of-the-art LLMs on their ability to annotate 121 ritual features across 567 ethnographic excerpts. Performance was limited, falling well below levels required for reliable automated annotation. Longer texts, features requiring ordinal distinctions, and ambiguous constructs proved particularly difficult. Human inter-coder reliability set an approximate ceiling on LLM accuracy: features that human coders found difficult to agree upon were also difficult for LLMs. Yet even on features where humans reliably agreed, models fell short of human performance. Our findings suggest that LLMs cannot yet substitute for human expertise in ethnographic annotation.

</details>


### [498] [Powerful Training-Free Membership Inference Against Autoregressive Language Models](https://arxiv.org/abs/2601.12104)
*David Ilić,David Stanojević,Kostadin Cvejoski*

Main category: cs.CL

TL;DR: 提出EZ - MIA成员推断攻击检测微调语言模型隐私风险，相比现有方法检测率大幅提升，表明微调语言模型隐私风险比以往认知大。


<details>
  <summary>Details</summary>
Motivation: 微调语言模型存在隐私风险，现有成员推断攻击检测率有限，需更好方法进行隐私审计。

Method: 提出EZ - MIA攻击，利用错误位置的记忆特性，引入误差区（EZ）分数衡量相对于预训练参考模型在错误位置概率转移的方向不平衡。

Result: 在WikiText和GPT - 2上，比现有方法检测率高3.8倍；在严格的0.1% FPR阈值下，检测率高8倍；在AG News和Llama - 2 - 7B上，检测率高3倍。

Conclusion: 微调语言模型的隐私风险比以前理解的大得多，对隐私审计和部署决策有影响。

Abstract: Fine-tuned language models pose significant privacy risks, as they may memorize and expose sensitive information from their training data. Membership inference attacks (MIAs) provide a principled framework for auditing these risks, yet existing methods achieve limited detection rates, particularly at the low false-positive thresholds required for practical privacy auditing. We present EZ-MIA, a membership inference attack that exploits a key observation: memorization manifests most strongly at error positions, specifically tokens where the model predicts incorrectly yet still shows elevated probability for training examples. We introduce the Error Zone (EZ) score, which measures the directional imbalance of probability shifts at error positions relative to a pretrained reference model. This principled statistic requires only two forward passes per query and no model training of any kind. On WikiText with GPT-2, EZ-MIA achieves 3.8x higher detection than the previous state-of-the-art under identical conditions (66.3% versus 17.5% true positive rate at 1% false positive rate), with near-perfect discrimination (AUC 0.98). At the stringent 0.1% FPR threshold critical for real-world auditing, we achieve 8x higher detection than prior work (14.0% versus 1.8%), requiring no reference model training. These gains extend to larger architectures: on AG News with Llama-2-7B, we achieve 3x higher detection (46.7% versus 15.8% TPR at 1% FPR). These results establish that privacy risks of fine-tuned language models are substantially greater than previously understood, with implications for both privacy auditing and deployment decisions. Code is available at https://github.com/JetBrains-Research/ez-mia.

</details>


### [499] [Bengali Text Classification: An Evaluation of Large Language Model Approaches](https://arxiv.org/abs/2601.12132)
*Md Mahmudul Hoque,Md Mehedi Hassain,Md Hojaifa Tanvir,Rahul Nandy*

Main category: cs.CL

TL;DR: 本文探索大语言模型对孟加拉语报纸文章分类的有效性，Qwen 2.5准确率最高达72%，未来将多方向改进。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语因缺乏标注数据集和预训练语言模型，在文本分类时面临挑战，需探索大语言模型用于其文本分类的有效性。

Method: 使用来自Kaggle的孟加拉国主要报纸《Prothom Alo》的文章数据集，在相同分类框架下评估三个指令调优的大语言模型LLaMA 3.1 8B Instruct、LLaMA 3.2 3B Instruct和Qwen 2.5 7B Instruct。

Result: Qwen 2.5分类准确率最高达72%，在“体育”类别表现突出；LLaMA 3.1和LLaMA 3.2准确率分别为53%和56%。

Conclusion: 大语言模型用于孟加拉语文本分类有效，未来将探索更多模型、解决类别不平衡问题、优化微调方法以提升分类性能。

Abstract: Bengali text classification is a Significant task in natural language processing (NLP), where text is categorized into predefined labels. Unlike English, Bengali faces challenges due to the lack of extensive annotated datasets and pre-trained language models. This study explores the effectiveness of large language models (LLMs) in classifying Bengali newspaper articles. The dataset used, obtained from Kaggle, consists of articles from Prothom Alo, a major Bangladeshi newspaper. Three instruction-tuned LLMs LLaMA 3.1 8B Instruct, LLaMA 3.2 3B Instruct, and Qwen 2.5 7B Instruct were evaluated for this task under the same classification framework. Among the evaluated models, Qwen 2.5 achieved the highest classification accuracy of 72%, showing particular strength in the "Sports" category. In comparison, LLaMA 3.1 and LLaMA 3.2 attained accuracies of 53% and 56%, respectively. The findings highlight the effectiveness of LLMs in Bengali text classification, despite the scarcity of resources for Bengali NLP. Future research will focus on exploring additional models, addressing class imbalance issues, and refining fine-tuning approaches to improve classification performance.

</details>


### [500] [Plan, Verify and Fill: A Structured Parallel Decoding Approach for Diffusion Language Models](https://arxiv.org/abs/2601.12247)
*Miao Li,Hanyang Jiang,Sikai Chen,Hengyu Fu,Yuhang Cai,Baihe Huang,Tinghan Ye,Xuanzhou Chen,Pascal Van Hentenryck*

Main category: cs.CL

TL;DR: 提出训练无关范式PVF用于扩散语言模型文本生成，评估显示其在不降低准确率下提升效率。


<details>
  <summary>Details</summary>
Motivation: 当前扩散语言模型解码策略未充分利用全局双向上下文来决定全局轨迹。

Method: 提出训练无关的Plan-Verify-Fill (PVF)范式，优先处理高影响力语义锚点构建分层骨架，并采用验证协议。

Result: 在LLaDA - 8B - Instruct和Dream - 7B - Instruct上评估，相比基于置信度的并行解码，PVF在基准数据集上最多减少65%的函数评估次数。

Conclusion: PVF能在不降低准确率的情况下提升扩散语言模型文本生成的效率。

Abstract: Diffusion Language Models (DLMs) present a promising non-sequential paradigm for text generation, distinct from standard autoregressive (AR) approaches. However, current decoding strategies often adopt a reactive stance, underutilizing the global bidirectional context to dictate global trajectories. To address this, we propose Plan-Verify-Fill (PVF), a training-free paradigm that grounds planning via quantitative validation. PVF actively constructs a hierarchical skeleton by prioritizing high-leverage semantic anchors and employs a verification protocol to operationalize pragmatic structural stopping where further deliberation yields diminishing returns. Extensive evaluations on LLaDA-8B-Instruct and Dream-7B-Instruct demonstrate that PVF reduces the Number of Function Evaluations (NFE) by up to 65% compared to confidence-based parallel decoding across benchmark datasets, unlocking superior efficiency without compromising accuracy.

</details>


### [501] [Multimodal Generative Engine Optimization: Rank Manipulation for Vision-Language Model Rankers](https://arxiv.org/abs/2601.12263)
*Yixuan Du,Chenxiao Yu,Haoyan Xu,Ziyi Wang,Yue Zhao,Xiyang Hu*

Main category: cs.CL

TL;DR: 该论文指出基于视觉语言模型（VLM）的产品搜索存在多模态排名攻击的关键漏洞，提出了对抗性框架MGEO，实验表明其攻击效果优于单模态基线，揭示了多模态协同可能被恶意利用。


<details>
  <summary>Details</summary>
Motivation: 探究视觉语言模型在竞争排名场景下对抗恶意操纵的鲁棒性，发现基于VLM的产品搜索中的关键漏洞。

Method: 提出多模态生成引擎优化（MGEO）框架，采用交替基于梯度的优化策略，联合优化难以察觉的图像扰动和流畅的文本后缀。

Result: 在真实数据集和先进模型上的实验显示，协调攻击性能显著优于仅文本和仅图像的基线。

Conclusion: VLMs的多模态协同优势可能被恶意利用，破坏搜索排名的完整性且不触发传统内容过滤器。

Abstract: Vision-Language Models (VLMs) are rapidly replacing unimodal encoders in modern retrieval and recommendation systems. While their capabilities are well-documented, their robustness against adversarial manipulation in competitive ranking scenarios remains largely unexplored. In this paper, we uncover a critical vulnerability in VLM-based product search: multimodal ranking attacks. We present Multimodal Generative Engine Optimization (MGEO), a novel adversarial framework that enables a malicious actor to unfairly promote a target product by jointly optimizing imperceptible image perturbations and fluent textual suffixes. Unlike existing attacks that treat modalities in isolation, MGEO employs an alternating gradient-based optimization strategy to exploit the deep cross-modal coupling within the VLM. Extensive experiments on real-world datasets using state-of-the-art models demonstrate that our coordinated attack significantly outperforms text-only and image-only baselines. These findings reveal that multimodal synergy, typically a strength of VLMs, can be weaponized to compromise the integrity of search rankings without triggering conventional content filters.

</details>


### [502] [Simulated Annealing Enhances Theory-of-Mind Reasoning in Autoregressive Language Models](https://arxiv.org/abs/2601.12269)
*Xucong Hu,Jian-Qiao Zhu*

Main category: cs.CL

TL;DR: 本文指出自回归语言模型在心智理论任务上常被认为会失败，提出不更新权重直接从基础模型恢复强大心智理论能力的方法，发现结合退火能提升性能，表明基于采样的优化是提取语言模型潜在能力的有效方式。


<details>
  <summary>Details</summary>
Motivation: 自回归语言模型被批评只优化表面合理性而无法维持正确的潜在状态表征，在心智理论任务上常被认为会失败，需要探索提升其心智理论能力的方法。

Method: 基于最近的幂采样方法，使用马尔可夫链蒙特卡罗从自回归语言模型的序列级概率分布采样，并结合退火，将回火分布从高温逐渐转移到低温。

Result: 结合退火相比固定温度的幂采样能显著提升心智理论任务表现。

Conclusion: 基于采样的优化是一种无需重新训练就能从语言模型中提取潜在能力的强大方法。

Abstract: Autoregressive language models are next-token predictors and have been criticized for only optimizing surface plausibility (i.e., local coherence) rather than maintaining correct latent-state representations (i.e., global coherence). Because Theory of Mind (ToM) tasks crucially depend on reasoning about latent mental states of oneself and others, such models are therefore often thought to fail at ToM. While post-training methods can improve ToM performance, we show that strong ToM capability can be recovered directly from the base model without any additional weight updates or verifications. Our approach builds on recent power-sampling methods (Karan & Du, 2025) that use Markov chain Monte Carlo (MCMC) to sample from sharpened sequence-level (rather than token-level) probability distributions of autoregressive language models. We further find that incorporating annealing, where the tempered distribution is gradually shifted from high to low temperature, substantially improves ToM performance over fixed-temperature power sampling. Together, these results suggest that sampling-based optimization provides a powerful way to extract latent capabilities from language models without retraining.

</details>


### [503] [Conversational Context Classification: A Representation Engineering Approach](https://arxiv.org/abs/2601.12286)
*Jonathan Pan*

Main category: cs.CL

TL;DR: 针对大语言模型易生成脱离上下文回复的问题，本文用RepE和OCSVM探索特定上下文子空间，在Llama和Qwen模型评估取得良好结果，且助于LLM解释研究。


<details>
  <summary>Details</summary>
Motivation: 大语言模型普遍存在，需有效保障其运行，特别是检测其脱离预期对话规范的情况，传统异常检测难用于上下文语义。

Method: 使用Representation Engineering (RepE)和One-Class Support Vector Machine (OCSVM)，在上下文示例上训练OCSVM以在LLM隐藏状态潜在空间建立边界，确定与感兴趣上下文强关联的最佳层。

Result: 评估结果显示，在识别特定上下文子空间方面取得了良好效果。

Conclusion: 该方法不仅可检测对话是否脱离上下文，还对更好地解释大语言模型的研究有贡献。

Abstract: The increasing prevalence of Large Language Models (LLMs) demands effective safeguards for their operation, particularly concerning their tendency to generate out-of-context responses. A key challenge is accurately detecting when LLMs stray from expected conversational norms, manifesting as topic shifts, factual inaccuracies, or outright hallucinations. Traditional anomaly detection struggles to directly apply within contextual semantics. This paper outlines our experiment in exploring the use of Representation Engineering (RepE) and One-Class Support Vector Machine (OCSVM) to identify subspaces within the internal states of LLMs that represent a specific context. By training OCSVM on in-context examples, we establish a robust boundary within the LLM's hidden state latent space. We evaluate out study with two open source LLMs - Llama and Qwen models in specific contextual domain. Our approach entailed identifying the optimal layers within the LLM's internal state subspaces that strongly associates with the context of interest. Our evaluation results showed promising results in identifying the subspace for a specific context. Aside from being useful in detecting in or out of context conversation threads, this research work contributes to the study of better interpreting LLMs.

</details>


### [504] [A Scalable Entity-Based Framework for Auditing Bias in LLMs](https://arxiv.org/abs/2601.12374)
*Akram Elbouanani,Aboubacar Tuo,Adrian Popescu*

Main category: cs.CL

TL;DR: 论文引入以命名实体为探针的可扩展偏差审计框架，对大语言模型进行大规模偏差审计，揭示系统偏差，提出高风险应用前需严格审计的建议。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型偏差评估方法存在生态效度与统计控制的权衡问题，人工提示与现实使用偏差大，自然主义任务缺乏规模和严谨性。

Method: 引入以命名实体为探针的可扩展偏差审计框架，利用合成数据重现自然文本中的偏差模式以进行大规模分析。

Result: 进行了包含19亿个数据点的偏差审计，揭示模型存在对不同政治派别、国家、公司等的系统偏差，指令调优可减少偏差，增加模型规模会放大偏差，特定语言提示不减弱西方偏好。

Conclusion: 大语言模型在高风险应用部署前应进行严格审计。

Abstract: Existing approaches to bias evaluation in large language models (LLMs) trade ecological validity for statistical control, relying on artificial prompts that poorly reflect real-world use, or on naturalistic tasks that lack scale and rigor. We introduce a scalable bias-auditing framework using named entities as probes to measure structural disparities in model behavior. We show that synthetic data reliably reproduces bias patterns observed in natural text, enabling large-scale analysis. Using this approach, we conduct the largest bias audit to date, comprising 1.9 billion data points across multiple entity types, tasks, languages, models, and prompting strategies. Our results reveal systematic biases: models penalize right-wing politicians, favor left-wing politicians, prefer Western and wealthy nations over the Global South, favor Western companies, and penalize firms in the defense and pharmaceutical sectors. While instruction tuning reduces bias, increasing model scale amplifies it, and prompting in Chinese or Russian does not attenuate Western-aligned preferences. These results indicate that LLMs should undergo rigorous auditing before deployment in high-stakes applications.

</details>


### [505] [NADIR: Differential Attention Flow for Non-Autoregressive Transliteration in Indic Languages](https://arxiv.org/abs/2601.12389)
*Lakshya Tomar,Vinayak Abrol,Puneet Agarwal*

Main category: cs.CL

TL;DR: 论文指出并非所有序列到序列任务都需自回归（AR）模型，聚焦多语言音译任务提出NADIR非自回归（NAR）架构，平衡速度与准确性，实现加速并降低误差。


<details>
  <summary>Details</summary>
Motivation: 并非所有序列到序列任务都需要AR模型强归纳偏置，AR模型高推理延迟，而NAR模型有幻觉和长度控制问题，需探索速度与准确性的权衡。

Method: 提出NADIR架构，集成差分变压器和专家混合机制，以无序列依赖方式对复杂字符映射进行建模。

Result: NADIR比现有AR基线提速超13倍，字符错误率有竞争力，重复、替换、遗漏和插入错误率显著降低。

Conclusion: 该工作为构建快速可靠的NAR系统提供实用蓝图，弥合AR准确性与实时大规模部署需求间的差距。

Abstract: In this work, we argue that not all sequence-to-sequence tasks require the strong inductive biases of autoregressive (AR) models. Tasks like multilingual transliteration, code refactoring, grammatical correction or text normalization often rely on local dependencies where the full modeling capacity of AR models can be overkill, creating a trade-off between their high accuracy and high inference latency. While non-autoregressive (NAR) models offer speed, they typically suffer from hallucinations and poor length control. To explore this trade-off, we focus on the multilingual transliteration task in Indic languages and introduce NADIR, a novel NAR architecture designed to strike a balance between speed and accuracy. NADIR integrates a Differential Transformer and a Mixture-of-Experts mechanism, enabling it to robustly model complex character mappings without sequential dependencies. NADIR achieves over a 13x speed-up compared to the state-of-the-art AR baseline. It maintains a competitive mean Character Error Rate of 15.78%, compared to 14.44% for the AR model and 21.88% for a standard NAR equivalent. Importantly, NADIR reduces Repetition errors by 49.53%, Substitution errors by 24.45%, Omission errors by 32.92%, and Insertion errors by 16.87%. This work provides a practical blueprint for building fast and reliable NAR systems, effectively bridging the gap between AR accuracy and the demands of real-time, large-scale deployment.

</details>


### [506] [Incentivizing In-depth Reasoning over Long Contexts with Process Advantage Shaping](https://arxiv.org/abs/2601.12465)
*Miao Peng,Weizhou Shen,Nuo Chen,Chenliang Li,Ming Yan,Jia Li*

Main category: cs.CL

TL;DR: 提出DeepReasonQA框架和LongPAS方法解决长上下文推理问题，实验显示效果好。


<details>
  <summary>Details</summary>
Motivation: RLVR在长上下文推理场景性能下降，存在‘almost - there’现象，明确两个导致失败的因素，需解决瓶颈。

Method: 提出DeepReasonQA框架构建高难度多跳长上下文QA对；引入LongPAS方法，从有效性和相关性维度评估推理步骤进行细粒度信用分配。

Result: 在三个长上下文推理基准测试中，大幅超越RLVR基线，使用更少参数达到前沿大模型水平。

Conclusion: 方法能有效增强长上下文推理能力并保持稳定的强化学习训练。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective in enhancing LLMs short-context reasoning, but its performance degrades in long-context scenarios that require both precise grounding and robust long-range reasoning. We identify the "almost-there" phenomenon in long-context reasoning, where trajectories are largely correct but fail at the final step, and attribute this failure to two factors: (1) the lack of high reasoning density in long-context QA data that push LLMs beyond mere grounding toward sophisticated multi-hop reasoning; and (2) the loss of valuable learning signals during long-context RL training due to the indiscriminate penalization of partially correct trajectories with incorrect outcomes. To overcome this bottleneck, we propose DeepReasonQA, a KG-driven synthesis framework that controllably constructs high-difficulty, multi-hop long-context QA pairs with inherent reasoning chains. Building on this, we introduce Long-context Process Advantage Shaping (LongPAS), a simple yet effective method that performs fine-grained credit assignment by evaluating reasoning steps along Validity and Relevance dimensions, which captures critical learning signals from "almost-there" trajectories. Experiments on three long-context reasoning benchmarks show that our approach substantially outperforms RLVR baselines and matches frontier LLMs while using far fewer parameters. Further analysis confirms the effectiveness of our methods in strengthening long-context reasoning while maintaining stable RL training.

</details>


### [507] [Knowing When to Abstain: Medical LLMs Under Clinical Uncertainty](https://arxiv.org/abs/2601.12471)
*Sravanthi Machcha,Sushrita Yerra,Sahil Gupta,Aishwarya Sahoo,Sharmin Sultana,Hong Yu,Zonghai Yao*

Main category: cs.CL

TL;DR: 引入医疗多选问答弃权基准MedAbstain，评估发现现有模型弃权能力不足，提供明确弃权选项更能提升安全性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型评估重准确率，在现实和安全关键应用中，不确定时弃权能力对可靠部署同样重要。

Method: 引入MedAbstain基准和评估协议，结合共形预测、对抗性问题扰动和明确弃权选项。

Result: 即使是最先进的高准确率模型在不确定时也常无法弃权，提供明确弃权选项比输入扰动更能增加模型不确定性和安全弃权，扩大模型规模或高级提示改进不大。

Conclusion: 弃权机制对可靠部署大语言模型至关重要，为高风险应用提高安全性提供实用指导。

Abstract: Current evaluation of large language models (LLMs) overwhelmingly prioritizes accuracy; however, in real-world and safety-critical applications, the ability to abstain when uncertain is equally vital for trustworthy deployment. We introduce MedAbstain, a unified benchmark and evaluation protocol for abstention in medical multiple-choice question answering (MCQA) -- a discrete-choice setting that generalizes to agentic action selection -- integrating conformal prediction, adversarial question perturbations, and explicit abstention options. Our systematic evaluation of both open- and closed-source LLMs reveals that even state-of-the-art, high-accuracy models often fail to abstain with uncertain. Notably, providing explicit abstention options consistently increases model uncertainty and safer abstention, far more than input perturbations, while scaling model size or advanced prompting brings little improvement. These findings highlight the central role of abstention mechanisms for trustworthy LLM deployment and offer practical guidance for improving safety in high-stakes applications.

</details>


### [508] [Improving Low-Resource Machine Translation via Round-Trip Reinforcement Learning](https://arxiv.org/abs/2601.12535)
*Ahmed Attia,Alham Fikri*

Main category: cs.CL

TL;DR: 研究用自监督强化学习微调低资源翻译，用NLLB模型和往返引导，组合评估指标，在多语言上有提升，方法可从扩大规模获益。


<details>
  <summary>Details</summary>
Motivation: 低资源机器翻译受关注但许多潜在改进方法未被探索，本文旨在研究新的低资源翻译改进方法。

Method: 利用NLLB系列模型进行往返引导的自监督强化学习微调，将英语翻译成低资源语言再翻译回英语，用chrF++和BLEU组合作为重构英语句子的奖励函数。

Result: 在NLLB - MD数据集上评估600M和1.3B参数的NLLB模型，在多种语言上有持续改进，翻译输出更流畅且语义更准确。

Conclusion: 该方法可从扩大规模中进一步受益，使模型更好利用预训练知识并持续自我提升。

Abstract: Low-resource machine translation (MT) has gained increasing attention as parallel data from low-resource language communities is collected, but many potential methods for improving low-resource MT remain unexplored. We investigate a self-supervised reinforcement-learning-based fine-tuning for translation in low-resource settings using round-trip bootstrapping with the No Language Left Behind (NLLB) family of models. Our approach translates English into a target low-resource language and then back into English, using a combination of chrF++ and BLEU as the reward function on the reconstructed English sentences. Using the NLLB-MD dataset, we evaluate both the 600M and 1.3B parameter NLLB models and observe consistent improvements for the following languages: Central Aymara, Friulian, Wolof and Russian. Qualitative inspection of translation outputs indicates increased fluency and semantic fidelity. We argue that our method can further benefit from scale, enabling models to increasingly leverage their pretrained knowledge and continue self-improving.

</details>


### [509] [Benchmarking Concept-Spilling Across Languages in LLMs](https://arxiv.org/abs/2601.12549)
*Ilia Badanin,Daniil Dzenhaliou,Imanol Schlag*

Main category: cs.CL

TL;DR: 本文提出评估多语言语义鲁棒性的比较框架，用多语言生成任务评估多种大模型，揭示模型和语言间语义鲁棒性差异，提供评估基准和验证管道。


<details>
  <summary>Details</summary>
Motivation: 多语言大模型存在语言溢出问题，需评估其多语言语义鲁棒性。

Method: 提出比较框架，用跨9种语言的结构化含义生成任务和100个高歧义英语单词的基准评估多种大模型。

Result: 发现模型和语言间在语义鲁棒性上有显著差异，提供无明确错误归因的模型比较排名系统。

Conclusion: 贡献了可扩展的多语言语义评估基准和严格验证管道，为开发语言平衡的AI系统提供关键工具。

Abstract: Multilingual Large Language Models (LLMs) exhibit remarkable cross-lingual abilities, yet often exhibit a systematic bias toward the representations from other languages, resulting in semantic interference when generating content in non-English languages$-$a phenomenon we define as language spilling. This paper presents a novel comparative framework for evaluating multilingual semantic robustness by systematically measuring how models handle polysemous words across languages. Our methodology provides a relative measure of model performance: when required to generate exactly five meanings, both strong and weak models may resort to meanings from dominant languages, but semantically stronger models do so later in the generation sequence, producing more true meanings from the target language before failing, while weaker models resort to dominant-language meanings earlier in the sequence. We evaluate a diverse set of open and closed multilingual LLMs using a structured meaning generation task across nine languages, employing a carefully curated benchmark of 100 high-polysemy English words. Our findings reveal significant variation in semantic robustness across both models and languages, providing a principled ranking system for model comparison without requiring definitive causal attribution of error sources. We contribute both a scalable comparative benchmark for multilingual semantic evaluation and a rigorous validation pipeline$-$critical tools for developing more linguistically balanced AI systems.

</details>


### [510] [A Cloud-based Multi-Agentic Workflow for Science](https://arxiv.org/abs/2601.12607)
*Anurag Acharya,Timothy Vega,Rizwan A. Ashraf,Anshu Sharma,Derek Parker,Robert Rallo*

Main category: cs.CL

TL;DR: 针对大语言模型处理复杂任务能力不足问题，提出可在云上运行的通用智能体框架，能高效处理多种任务，经评估表现良好，适用于其他科学领域。


<details>
  <summary>Details</summary>
Motivation: 大语言模型处理复杂任务能力有限，而基于大语言模型的智能体虽能调用外部资源，但设计平衡模型、云服务提供商和外部资源的工作流极具挑战。

Method: 构建一个由监督智能体协调多个带单独能力智能体的框架，并构建概念验证系统，还在自定义合成基准和化学基准上评估系统并进行专家验证。

Result: 系统90%的时间能将任务分配给正确的智能体，合成任务97.5%、现实任务91%能成功完成，且准确性与多数前沿模型相当或更好。

Conclusion: 该框架是可行的，可供其他科学领域借鉴。

Abstract: As Large Language Models (LLMs) become ubiquitous across various scientific domains, their lack of ability to perform complex tasks like running simulations or to make complex decisions limits their utility. LLM-based agents bridge this gap due to their ability to call external resources and tools and thus are now rapidly gaining popularity. However, coming up with a workflow that can balance the models, cloud providers, and external resources is very challenging, making implementing an agentic system more of a hindrance than a help. In this work, we present a domain-agnostic, model-independent workflow for an agentic framework that can act as a scientific assistant while being run entirely on cloud. Built with a supervisor agent marshaling an array of agents with individual capabilities, our framework brings together straightforward tasks like literature review and data analysis with more complex ones like simulation runs. We describe the framework here in full, including a proof-of-concept system we built to accelerate the study of Catalysts, which is highly important in the field of Chemistry and Material Science. We report the cost to operate and use this framework, including the breakdown of the cost by services use. We also evaluate our system on a custom-curated synthetic benchmark and a popular Chemistry benchmark, and also perform expert validation of the system. The results show that our system is able to route the task to the correct agent 90% of the time and successfully complete the assigned task 97.5% of the time for the synthetic tasks and 91% of the time for real-world tasks, while still achieving better or comparable accuracy to most frontier models, showing that this is a viable framework for other scientific domains to replicate.

</details>


### [511] [Intelligent Documentation in Medical Education: Can AI Replace Manual Case Logging?](https://arxiv.org/abs/2601.12648)
*Nafiz Imtiaz Khan,Kylie Cleland,Vladimir Filkov,Roger Eric Goldman*

Main category: cs.CL

TL;DR: 研究探讨大语言模型（LLMs）自动化放射程序病例日志记录的可行性，评估多种模型，结果显示模型提取性能强，有减轻学员负担等潜力。


<details>
  <summary>Details</summary>
Motivation: 放射程序病例日志手动完成耗时且易不一致，探究LLMs能否直接从放射报告中自动化文档记录。

Method: 评估多种本地和商用LLMs，采用基于指令和思维链提示，从414份介入放射报告中提取信息，用多种指标评估模型表现。

Result: 本地和商用模型提取性能都强，最佳F1分数接近0.87，速度和成本有不同权衡。

Conclusion: LLMs自动化有减轻学员文书负担和提高病例记录一致性的潜力，证明医学教育中AI辅助文档可行，需跨机构和临床工作流程进一步验证。

Abstract: Procedural case logs are a core requirement in radiology training, yet they are time-consuming to complete and prone to inconsistency when authored manually. This study investigates whether large language models (LLMs) can automate procedural case log documentation directly from free-text radiology reports. We evaluate multiple local and commercial LLMs under instruction-based and chain-of-thought prompting to extract structured procedural information from 414 curated interventional radiology reports authored by nine residents between 2018 and 2024. Model performance is assessed using sensitivity, specificity, and F1-score, alongside inference latency and token efficiency to estimate operational cost. Results show that both local and commercial models achieve strong extraction performance, with best F1-scores approaching 0.87, while exhibiting different trade-offs between speed and cost. Automation using LLMs has the potential to substantially reduce clerical burden for trainees and improve consistency in case logging. These findings demonstrate the feasibility of AI-assisted documentation in medical education and highlight the need for further validation across institutions and clinical workflows.

</details>


### [512] [Augmenting Question Answering with A Hybrid RAG Approach](https://arxiv.org/abs/2601.12658)
*Tianyi Yang,Nashrah Haque,Vaishnave Jonnalagadda,Yuya Jeremy Ong,Zhehui Chen,Yanzhao Wu,Lei Yu,Divyesh Jadav,Wenqi Wei*

Main category: cs.CL

TL;DR: 提出Structured - Semantic RAG (SSRAG)架构提升问答质量，在多个数据集和大模型上验证效果更好。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法在检索上下文相关信息时存在困难，导致答案不完整或不理想。

Method: 引入SSRAG混合架构，集成查询增强、智能路由和结合向量与图技术及上下文统一的结构化检索机制。

Result: 在TruthfulQA、SQuAD和WikiQA三个数据集及五个大模型上评估，SSRAG比标准RAG实现持续提升回答质量。

Conclusion: SSRAG架构通过优化检索过程和改善上下文关联，提高了答案的准确性和信息量。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful technique for enhancing the quality of responses in Question-Answering (QA) tasks. However, existing approaches often struggle with retrieving contextually relevant information, leading to incomplete or suboptimal answers. In this paper, we introduce Structured-Semantic RAG (SSRAG), a hybrid architecture that enhances QA quality by integrating query augmentation, agentic routing, and a structured retrieval mechanism combining vector and graph based techniques with context unification. By refining retrieval processes and improving contextual grounding, our approach improves both answer accuracy and informativeness. We conduct extensive evaluations on three popular QA datasets, TruthfulQA, SQuAD and WikiQA, across five Large Language Models (LLMs), demonstrating that our proposed approach consistently improves response quality over standard RAG implementations.

</details>


### [513] [Objective Matters: Fine-Tuning Objectives Shape Safety, Robustness, and Persona Drift](https://arxiv.org/abs/2601.12639)
*Daniel Vennemeyer,Punya Syon Pandey,Phan Anh Duong,Michael Umeokoli,Samuel Ratnam*

Main category: cs.CL

TL;DR: 本文对六种微调目标进行对比，发现微调目标在小训练规模时对安全性影响小，大训练规模时是对抗鲁棒性和潜在角色稳定性的主要驱动因素。


<details>
  <summary>Details</summary>
Motivation: 当前对微调目标在大语言模型安全结果方面的作用缺乏直接分析。

Method: 在固定数据、领域、架构和优化的情况下，对六种微调目标进行受控比较，涵盖封闭式推理和开放式生成任务。

Result: 目标选择会导致沿安全 - 能力边界出现与规模相关的系统性变化；小训练预算时不同目标鲁棒性相似但能力有差异，大预算时目标差异显著，基于监督和偏好的微调会增加对抗脆弱性和角色漂移，约束学习信号的目标能缓解问题。

Conclusion: 微调目标在小尺度下对安全性影响不大，随着训练规模增加成为对抗鲁棒性和潜在角色稳定性的主要驱动因素。

Abstract: Fine-tuning LLMs on benign data can still degrade alignment and adversarial robustness, yet direct analysis of the role of fine-tuning objectives in shaping these safety outcomes remain limited. We present a controlled comparison of six fine-tuning objectives -- Supervised Fine-Tuning, Direct Preference Optimization, Conditional Fine-Tuning, Inoculation Prompting, Odds Ratio Preference Optimization, and KL-regularized fine-tuning -- holding data, domain, architecture, and optimization fixed. Across closed-form reasoning and open-ended generation tasks, we find that objective choice induces systematic, scale-dependent shifts along the safety-capability frontier. At small training budgets, robustness is similar across objectives but capability differs. At larger budgets, objectives diverge sharply: supervised and preference-based tuning tightly couple capability gains to increased adversarial vulnerability and persona drift, while objectives that constrain learning signals -- especially ORPO and KL-regularization -- substantially mitigate both. Fine-tuning objectives therefore matter little for safety at small scales but become a primary driver of adversarial robustness and latent persona stability as training scale increases.

</details>


### [514] [A Shared Geometry of Difficulty in Multilingual Language Models](https://arxiv.org/abs/2601.12731)
*Stefano Civelli,Pietro Bernardelle,Nicolò Brunello,Gianluca Demartini*

Main category: cs.CL

TL;DR: 研究大语言模型中问题难度的多语言几何特性，发现模型内部不同阶段表征有不同功能，揭示两阶段表征过程。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型中问题难度的多语言几何特性。

Method: 使用Easy2Hard基准的AMC子集翻译成21种语言训练线性探针。

Result: 难度相关信号出现在模型内部两个不同阶段，深层表征单语言评估精度高但跨语言泛化差，浅层表征跨语言泛化好但单语言性能低。

Conclusion: 大语言模型先形成与语言无关的问题难度表征，随后变为特定语言表征，两阶段表征过程可扩展到问题难度估计等元认知属性。

Abstract: Predicting problem-difficulty in large language models (LLMs) refers to estimating how difficult a task is according to the model itself, typically by training linear probes on its internal representations. In this work, we study the multilingual geometry of problem-difficulty in LLMs by training linear probes using the AMC subset of the Easy2Hard benchmark, translated into 21 languages. We found that difficulty-related signals emerge at two distinct stages of the model internals, corresponding to shallow (early-layers) and deep (later-layers) internal representations, that exhibit functionally different behaviors. Probes trained on deep representations achieve high accuracy when evaluated on the same language but exhibit poor cross-lingual generalization. In contrast, probes trained on shallow representations generalize substantially better across languages, despite achieving lower within-language performance. Together, these results suggest that LLMs first form a language-agnostic representation of problem difficulty, which subsequently becomes language-specific. This closely aligns with existing findings in LLM interpretability showing that models tend to operate in an abstract conceptual space before producing language-specific outputs. We demonstrate that this two-stage representational process extends beyond semantic content to high-level meta-cognitive properties such as problem-difficulty estimation.

</details>


### [515] [VISPA: Pluralistic Alignment via Automatic Value Selection and Activation](https://arxiv.org/abs/2601.12758)
*Shenyan Zheng,Jiayou Zhong,Anudeex Shetty,Heng Ji,Preslav Nakov,Usman Naseem*

Main category: cs.CL

TL;DR: 介绍了无训练的多元对齐框架VISPA，它能实现对价值观表达的直接控制，在多模型和评估设置中表现良好，具适应性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在高风险领域应用时，需输出反映不同观点，但现有方法难以实现多元表达，存在价值控制和表示不足的问题。

Method: 引入无训练的多元对齐框架VISPA，通过动态选择和内部模型激活引导实现对价值观表达的直接控制。

Result: 在多模型和评估设置中，VISPA在所有多元对齐模式中表现良好，且具有适应性，可适配不同引导启动方式、模型和价值观。

Conclusion: 可通过内部激活机制实现多元对齐，为服务所有人的语言模型提供可扩展路径。

Abstract: As large language models are increasingly used in high-stakes domains, it is essential that their outputs reflect not average} human preference, rather range of varying perspectives. Achieving such pluralism, however, remains challenging. Existing approaches consider limited values or rely on prompt-level interventions, lacking value control and representation. To address this, we introduce VISPA, a training-free pluralistic alignment framework, that enables direct control over value expression by dynamic selection and internal model activation steering. Across extensive empirical studies spanning multiple models and evaluation settings, we show VISPA is performant across all pluralistic alignment modes in healthcare and beyond. Further analysis reveals VISPA is adaptable with different steering initiations, model, and/or values. These results suggest that pluralistic alignment can be achieved through internal activation mechanisms, offering a scalable path toward language models that serves all.

</details>


### [516] [Race, Ethnicity and Their Implication on Bias in Large Language Models](https://arxiv.org/abs/2601.12868)
*Shiyue Hu,Ruizhe Li,Yanjun Gao*

Main category: cs.CL

TL;DR: 对大语言模型中种族和族裔的表示和运作机制进行研究，发现人口统计信息分布有差异，干预可减少偏差但有残留影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注结果层面差异，对大语言模型中种族和族裔相关影响的内在机制了解有限。

Method: 利用两个公开数据集，对三个开源模型采用结合探测、神经元级归因和目标干预的可复现解释性流程进行分析。

Result: 人口统计信息分布在内部单元且跨模型差异大；部分单元编码敏感或刻板印象关联；相同人口统计线索会导致不同行为；干预抑制神经元可减少偏差但有残留效应。

Conclusion: 干预带来行为而非表征变化，需更系统的缓解措施。

Abstract: Large language models (LLMs) increasingly operate in high-stakes settings including healthcare and medicine, where demographic attributes such as race and ethnicity may be explicitly stated or implicitly inferred from text. However, existing studies primarily document outcome-level disparities, offering limited insight into internal mechanisms underlying these effects. We present a mechanistic study of how race and ethnicity are represented and operationalized within LLMs. Using two publicly available datasets spanning toxicity-related generation and clinical narrative understanding tasks, we analyze three open-source models with a reproducible interpretability pipeline combining probing, neuron-level attribution, and targeted intervention. We find that demographic information is distributed across internal units with substantial cross-model variation. Although some units encode sensitive or stereotype-related associations from pretraining, identical demographic cues can induce qualitatively different behaviors. Interventions suppressing such neurons reduce bias but leave substantial residual effects, suggesting behavioral rather than representational change and motivating more systematic mitigation.

</details>


### [517] [From Prefix Cache to Fusion RAG Cache: Accelerating LLM Inference in Retrieval-Augmented Generation](https://arxiv.org/abs/2601.12904)
*Jiahao Wang,Weiyu Xie,Mingxing Zhang,Boxing Zhang,Jianwei Dong,Yuening Zhu,Chen Lin,Jinqi Tang,Yaochen Han,Zhiyuan Ai,Xianglin Chen,Yongwei Wu,Congfeng Jiang*

Main category: cs.CL

TL;DR: 提出FusionRAG框架优化RAG的预处理和再处理阶段，在生成质量和效率间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 现有RAG加速方案因缺乏跨块上下文信息，导致生成质量下降，需要解决如何在复用KV缓存时保持生成质量的问题。

Method: 提出FusionRAG框架，离线预处理阶段将相关文本块信息嵌入每个块，在线再处理阶段为模型关注的令牌重新计算KV缓存。

Result: FusionRAG在相同重计算比率下显著提高生成质量，重计算少于15%的令牌时，归一化F1分数比基线高70%，TTFT比全注意力减少2.66 - 9.39倍。

Conclusion: FusionRAG能在生成质量和效率之间取得更好的权衡。

Abstract: Retrieval-Augmented Generation enhances Large Language Models by integrating external knowledge, which reduces hallucinations but increases prompt length. This increase leads to higher computational costs and longer Time to First Token (TTFT). To mitigate this issue, existing solutions aim to reuse the preprocessed KV cache of each retrieved chunk to accelerate RAG. However, the lack of cross-chunk contextual information leads to a significant drop in generation quality, leaving the potential benefits of KV cache reuse largely unfulfilled. The challenge lies in how to reuse the precomputed KV cache of chunks while preserving generation quality. We propose FusionRAG, a novel inference framework that optimizes both the preprocessing and reprocessing stages of RAG. In the offline preprocessing stage, we embed information from other related text chunks into each chunk, while in the online reprocessing stage, we recompute the KV cache for tokens that the model focuses on. As a result, we achieve a better trade-off between generation quality and efficiency. According to our experiments, FusionRAG significantly improves generation quality at the same recomputation ratio compared to previous state-of-the-art solutions. By recomputing fewer than 15% of the tokens, FusionRAG achieves up to 70% higher normalized F1 scores than baselines and reduces TTFT by 2.66x-9.39x compared to Full Attention.

</details>


### [518] [SciCoQA: Quality Assurance for Scientific Paper--Code Alignment](https://arxiv.org/abs/2601.12910)
*Tim Baumgärtner,Iryna Gurevych*

Main category: cs.CL

TL;DR: 提出数据集SciCoQA用于检测科研论文与代码库差异，构建数据集并分析差异，评估21个LLMs，最佳模型GPT - 5检测准确率仅45.7%。


<details>
  <summary>Details</summary>
Motivation: 确保科研论文的代码忠实实现，检测论文与代码库间的差异。

Method: 从GitHub问题和可重复性论文构建SciCoQA数据集，提出合成数据生成方法扩展数据集，分析差异并提出类型和类别。

Result: 数据集包含611个差异（81个真实及530个合成），涵盖多个计算科学学科；评估21个LLMs，GPT - 5检测45.7%的真实差异。

Conclusion: SciCoQA检测存在难度，尤其是针对论文细节缺失、长文本输入及模型预训练语料外的数据。

Abstract: We present SciCoQA, a dataset for detecting discrepancies between scientific publications and their codebases to ensure faithful implementations. We construct SciCoQA from GitHub issues and reproducibility papers, and to scale our dataset, we propose a synthetic data generation method for constructing paper-code discrepancies. We analyze the paper-code discrepancies in detail and propose discrepancy types and categories to better understand the occurring mismatches. In total, our dataset consists of 611 paper-code discrepancies (81 real, 530 synthetic), spanning diverse computational science disciplines, including AI, Physics, Quantitative Biology, and others. Our evaluation of 21 LLMs highlights the difficulty of SciCoQA, particularly for instances involving omitted paper details, long-context inputs, and data outside the models' pre-training corpus. The best performing model in our evaluation, GPT-5, can only detect 45.7\% of real-world paper-code discrepancies.

</details>


### [519] [A Component-Based Survey of Interactions between Large Language Models and Multi-Armed Bandits](https://arxiv.org/abs/2601.12945)
*Miao Xie,Siguang Chen,Chunli Lv*

Main category: cs.CL

TL;DR: 本文是首篇系统回顾大语言模型与多臂老虎机在组件层面双向交互的综述，介绍了两者相互促进作用，分析现有系统，指明挑战与发现并提供文献索引仓库。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型与多臂老虎机两个领域交叉的潜力。

Method: 系统回顾大语言模型与多臂老虎机在组件层面的双向交互，分析现有LLM增强的多臂老虎机系统和多臂老虎机增强的LLM系统。

Result: 明确了两者双向的好处，如MAB算法解决LLM挑战，LLM增强MAB系统；识别了关键挑战和代表性发现。

Conclusion: 研究为未来相关研究提供了指导，还提供了相关文献索引的GitHub仓库。

Abstract: Large language models (LLMs) have become powerful and widely used systems for language understanding and generation, while multi-armed bandit (MAB) algorithms provide a principled framework for adaptive decision-making under uncertainty. This survey explores the potential at the intersection of these two fields. As we know, it is the first survey to systematically review the bidirectional interaction between large language models and multi-armed bandits at the component level. We highlight the bidirectional benefits: MAB algorithms address critical LLM challenges, spanning from pre-training to retrieval-augmented generation (RAG) and personalization. Conversely, LLMs enhance MAB systems by redefining core components such as arm definition and environment modeling, thereby improving decision-making in sequential tasks. We analyze existing LLM-enhanced bandit systems and bandit-enhanced LLM systems, providing insights into their design, methodologies, and performance. Key challenges and representative findings are identified to help guide future research. An accompanying GitHub repository that indexes relevant literature is available at https://github.com/bucky1119/Awesome-LLM-Bandit-Interaction.

</details>


### [520] [SASA: Semantic-Aware Contrastive Learning Framework with Separated Attention for Triple Classification](https://arxiv.org/abs/2601.13035)
*Xu Xiaodan,Hu Xiaolin*

Main category: cs.CL

TL;DR: 论文提出SASA框架解决知识图谱三元组分类问题，通过分离注意力机制和语义感知对比学习提升性能，实验显示其显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱三元组分类方法存在忽略KG组件间有效语义交互，单二分类训练目标导致语义表示学习不足的问题。

Method: 提出分离注意力机制将三元组编码为解耦上下文表示并有效融合；引入语义感知层次对比学习作为辅助训练目标。

Result: 在FB15k - 237和YAGO3 - 10两个基准数据集上，SASA准确性分别比现有最优方法提升5.9%和3.4%。

Conclusion: SASA能显著提升三元组分类模型性能，优于现有方法。

Abstract: Knowledge Graphs~(KGs) often suffer from unreliable knowledge, which restricts their utility. Triple Classification~(TC) aims to determine the validity of triples from KGs. Recently, text-based methods learn entity and relation representations from natural language descriptions, significantly improving the generalization capabilities of TC models and setting new benchmarks in performance. However, there are still two critical challenges. First, existing methods often ignore the effective semantic interaction among different KG components. Second, most approaches adopt single binary classification training objective, leading to insufficient semantic representation learning. To address these challenges, we propose \textbf{SASA}, a novel framework designed to enhance TC models via separated attention mechanism and semantic-aware contrastive learning~(CL). Specifically, we first propose separated attention mechanism to encode triples into decoupled contextual representations and then fuse them through a more effective interactive way. Then, we introduce semantic-aware hierarchical CL as auxiliary training objective to guide models in improving their discriminative capabilities and achieving sufficient semantic learning, considering both local level and global level CL. Experimental results across two benchmark datasets demonstrate that SASA significantly outperforms state-of-the-art methods. In terms of accuracy, we advance the state-of-the-art by +5.9\% on FB15k-237 and +3.4\% on YAGO3-10.

</details>


### [521] [Bi-Attention HateXplain : Taking into account the sequential aspect of data during explainability in a multi-task context](https://arxiv.org/abs/2601.13018)
*Ghislain Dorian Tchuente Mondjo*

Main category: cs.CL

TL;DR: 为解决HateXplain算法注意力可变问题，提出BiAtt - BiRNN - HateXplain模型，实验显示其在检测性能、可解释性和减少无意偏差上有提升。


<details>
  <summary>Details</summary>
Motivation: 现有HateXplain算法预测注意力可变，导致解释不一致、预测不稳定和学习困难，需提高仇恨言论检测模型可靠性。

Method: 提出BiAtt - BiRNN - HateXplain模型，利用双向注意力和BiRNN层考虑输入数据顺序，采用多任务学习。

Result: 在HateXplain数据上实验，检测性能、可解释性明显提升，减少了无意偏差。

Conclusion: 所提模型能更好地进行仇恨言论分类，减少与社区相关的无意偏差。

Abstract: Technological advances in the Internet and online social networks have brought many benefits to humanity. At the same time, this growth has led to an increase in hate speech, the main global threat. To improve the reliability of black-box models used for hate speech detection, post-hoc approaches such as LIME, SHAP, and LRP provide the explanation after training the classification model. In contrast, multi-task approaches based on the HateXplain benchmark learn to explain and classify simultaneously. However, results from HateXplain-based algorithms show that predicted attention varies considerably when it should be constant. This attention variability can lead to inconsistent interpretations, instability of predictions, and learning difficulties. To solve this problem, we propose the BiAtt-BiRNN-HateXplain (Bidirectional Attention BiRNN HateXplain) model which is easier to explain compared to LLMs which are more complex in view of the need for transparency, and will take into account the sequential aspect of the input data during explainability thanks to a BiRNN layer. Thus, if the explanation is correctly estimated, thanks to multi-task learning (explainability and classification task), the model could classify better and commit fewer unintentional bias errors related to communities. The experimental results on HateXplain data show a clear improvement in detection performance, explainability and a reduction in unintentional bias.

</details>


### [522] [Probe and Skip: Self-Predictive Token Skipping for Efficient Long-Context LLM Inference](https://arxiv.org/abs/2601.13155)
*Zimeng Wu,Donghao Wang,Chaozhe Jin,Jiaxin Chen,Yunhong Wang*

Main category: cs.CL

TL;DR: 现有面向token的方法在长上下文大模型推理加速方面存在局限，本文提出无训练框架SPTS，实验证明其可有效提升推理速度且保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前token导向方法在长上下文大模型推理加速中存在加速潜力有限、代理信号陈旧和冗余干扰问题，无法实现最优的速度-准确性平衡。

Method: 提出无训练框架SPTS，设计针对多头注意力的部分注意力探测（PAP）和前馈网络的低秩变换探测（LTP）两种选择性token跳过策略，以及多阶段延迟剪枝（MSDP）策略。

Result: 在预填充和端到端生成中分别实现高达2.46倍和2.29倍的加速，同时保持了最先进的模型性能。

Conclusion: 所提的SPTS框架能有效解决现有方法的问题，实现高效的长上下文大模型推理。

Abstract: Long-context inference enhances the reasoning capability of Large Language Models (LLMs) while incurring significant computational overhead. Token-oriented methods, such as pruning and skipping, have shown promise in reducing inference latency, but still suffer from inherently limited acceleration potential, outdated proxy signals, and redundancy interference, thus yielding suboptimal speed-accuracy trade-offs. To address these challenges, we propose SPTS (Self-Predictive Token Skipping), a training-free framework for efficient long-context LLM inference. Specifically, motivated by the thought of probing the influence of targeted skipping layers, we design two component-specific strategies for selective token skipping: Partial Attention Probing (PAP) for multi-head attention, which selects informative tokens by performing partial forward attention computation, and Low-rank Transformation Probing (LTP) for feed forward network, which constructs a low-rank proxy network to predict token transformations. Furthermore, a Multi-Stage Delayed Pruning (MSDP) strategy reallocates the skipping budget and progressively prunes redundant tokens across layers. Extensive experiments demonstrate the effectiveness of our method, achieving up to 2.46$\times$ and 2.29$\times$ speedups for prefilling and end-to-end generation, respectively, while maintaining state-of-the-art model performance. The source code will be publicly available upon paper acceptance.

</details>


### [523] [Aligning Agentic World Models via Knowledgeable Experience Learning](https://arxiv.org/abs/2601.13247)
*Baochang Ren,Yunzhi Yao,Rui Sun,Shuofei Qiao,Ningyu Zhang,Huajun Chen*

Main category: cs.CL

TL;DR: 现有大语言模型存在模态脱节，易产生物理幻觉，现有对齐策略有局限性。本文提出WorldMind框架，实验显示其性能优越且有良好迁移性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型存在的模态脱节问题，避免物理幻觉，克服现有对齐策略难以适应物理动态变化的局限性。

Method: 引入WorldMind框架，通过合成环境反馈自主构建符号化的世界知识存储库，统一过程经验和目标经验。

Result: 在EB - ALFRED和EB - Habitat上的实验表明，WorldMind比基线模型表现更优，且有显著的跨模型和跨环境迁移能力。

Conclusion: WorldMind框架能有效解决大语言模型的模态脱节问题，具有良好性能和迁移性。

Abstract: Current Large Language Models (LLMs) exhibit a critical modal disconnect: they possess vast semantic knowledge but lack the procedural grounding to respect the immutable laws of the physical world. Consequently, while these agents implicitly function as world models, their simulations often suffer from physical hallucinations-generating plans that are logically sound but physically unexecutable. Existing alignment strategies predominantly rely on resource-intensive training or fine-tuning, which attempt to compress dynamic environmental rules into static model parameters. However, such parametric encapsulation is inherently rigid, struggling to adapt to the open-ended variability of physical dynamics without continuous, costly retraining. To bridge this gap, we introduce WorldMind, a framework that autonomously constructs a symbolic World Knowledge Repository by synthesizing environmental feedback. Specifically, it unifies Process Experience to enforce physical feasibility via prediction errors and Goal Experience to guide task optimality through successful trajectories. Experiments on EB-ALFRED and EB-Habitat demonstrate that WorldMind achieves superior performance compared to baselines with remarkable cross-model and cross-environment transferability.

</details>


### [524] [Beyond Cosine Similarity: Taming Semantic Drift and Antonym Intrusion in a 15-Million Node Turkish Synonym Graph](https://arxiv.org/abs/2601.13251)
*Ebubekir Tosun,Mehmet Emin Buldur,Özay Ezerceli,Mahmoud ElHussieni*

Main category: cs.CL

TL;DR: 论文构建大规模语义聚类系统解决神经嵌入无法区分同义词和反义词问题，产出高精度语义簇，用于语义搜索和生成。


<details>
  <summary>Details</summary>
Motivation: 现有神经嵌入无法可靠区分同义词和反义词，增加相似度阈值也无法避免反义词分组在一起，因此要构建针对性系统解决此问题。

Method: 建立含843,000个概念对的标注数据集，提出实现90%宏F1的三元语义关系判别器，引入软到硬聚类算法及拓扑感知的两阶段扩展 - 剪枝程序。

Result: 系统处理了1500万个词汇项，评估5.2亿潜在关系，生成290万个高精度语义簇。

Conclusion: 该系统成果可用于高精度语义搜索和检索增强生成，尤其适用于形态丰富和资源匮乏的语言。

Abstract: Neural embeddings have a notorious blind spot: they can't reliably tell synonyms apart from antonyms. Consequently, increasing similarity thresholds often fails to prevent opposites from being grouped together. We've built a large-scale semantic clustering system specifically designed to tackle this problem head on. Our pipeline chews through 15 million lexical items, evaluates a massive 520 million potential relationships, and ultimately generates 2.9 million high-precision semantic clusters. The system makes three primary contributions. First, we introduce a labeled dataset of 843,000 concept pairs spanning synonymy, antonymy, and co-hyponymy, constructed via Gemini 2.5-Flash LLM augmentation and verified using human-curated dictionary resources. Second, we propose a specialized three-way semantic relation discriminator that achieves 90% macro-F1, enabling robust disambiguation beyond raw embedding similarity. Third, we introduce a novel soft-to-hard clustering algorithm that mitigates semantic drift preventing erroneous transitive chains (e.g., hot -> spicy -> pain -> depression) while simultaneously resolving polysemy. Our approach employs a topology-aware two-stage expansion-pruning procedure with topological voting, ensuring that each term is assigned to exactly one semantically coherent cluster. The resulting resource enables high-precision semantic search and retrieval-augmented generation, particularly for morphologically rich and low-resource languages where existing synonym databases remain sparse.

</details>


### [525] [Beyond Single-shot Writing: Deep Research Agents are Unreliable at Multi-turn Report Revision](https://arxiv.org/abs/2601.13217)
*Bingsen Chen,Boyan Li,Ping Nie,Yuyu Zhang,Xi Ye,Chen Zhao*

Main category: cs.CL

TL;DR: 提出Mr Dre评估套件评估DRAs多轮报告修订能力，分析发现DRAs存在局限性且难以通过推理时修复解决。


<details>
  <summary>Details</summary>
Motivation: 现有DRAs基准将报告生成视为单次写作任务，与人类研究人员迭代修订报告方式不同，且DRAs能否根据用户反馈可靠修订报告未被探索。

Method: 引入Mr Dre评估套件，包含统一评估协议和反馈模拟管道。

Result: 分析五个不同DRAs发现，虽能处理大部分反馈，但会在16 - 27%已覆盖内容和引用质量上退步，多轮修订后仍有不足。

Conclusion: 这些问题难以通过推理时修复解决。

Abstract: Existing benchmarks for Deep Research Agents (DRAs) treat report generation as a single-shot writing task, which fundamentally diverges from how human researchers iteratively draft and revise reports via self-reflection or peer feedback. Whether DRAs can reliably revise reports with user feedback remains unexplored. We introduce Mr Dre, an evaluation suite that establishes multi-turn report revision as a new evaluation axis for DRAs. Mr Dre consists of (1) a unified long-form report evaluation protocol spanning comprehensiveness, factuality, and presentation, and (2) a human-verified feedback simulation pipeline for multi-turn revision. Our analysis of five diverse DRAs reveals a critical limitation: while agents can address most user feedback, they also regress on 16-27% of previously covered content and citation quality. Over multiple revision turns, even the best-performing agents leave significant headroom, as they continue to disrupt content outside the feedback's scope and fail to preserve earlier edits. We further show that these issues are not easily resolvable through inference-time fixes such as prompt engineering and a dedicated sub-agent for report revision.

</details>


### [526] [A Hybrid Protocol for Large-Scale Semantic Dataset Generation in Low-Resource Languages: The Turkish Semantic Relations Corpus](https://arxiv.org/abs/2601.13253)
*Ebubekir Tosun,Mehmet Emin Buldur,Özay Ezerceli,Mahmoud ElHussieni*

Main category: cs.CL

TL;DR: 论文提出生成低资源语言大规模语义关系数据集的混合方法，以土耳其语语料为例，生成大规模数据集，验证效果好并开源。


<details>
  <summary>Details</summary>
Motivation: 解决土耳其语等低资源语言在自然语言处理中数据稀缺的问题。

Method: 方法分为三个阶段，先用FastText嵌入和凝聚聚类识别语义簇，再用Gemini 2.5 - Flash进行自动语义关系分类，最后与策划字典源集成。

Result: 生成包含843,000个独特土耳其语语义对的数据集，在两项下游任务中验证，嵌入模型的top - 1检索准确率达90%，分类模型的F1 - macro达90%。

Conclusion: 该可扩展协议解决了土耳其语自然语言处理中的数据稀缺问题，且适用于其他低资源语言，数据集和模型已公开。

Abstract: We present a hybrid methodology for generating large-scale semantic relationship datasets in low-resource languages, demonstrated through a comprehensive Turkish semantic relations corpus. Our approach integrates three phases: (1) FastText embeddings with Agglomerative Clustering to identify semantic clusters, (2) Gemini 2.5-Flash for automated semantic relationship classification, and (3) integration with curated dictionary sources. The resulting dataset comprises 843,000 unique Turkish semantic pairs across three relationship types (synonyms, antonyms, co-hyponyms) representing a 10x scale increase over existing resources at minimal cost ($65). We validate the dataset through two downstream tasks: an embedding model achieving 90% top-1 retrieval accuracy and a classification model attaining 90% F1-macro. Our scalable protocol addresses critical data scarcity in Turkish NLP and demonstrates applicability to other low-resource languages. We publicly release the dataset and models.

</details>


### [527] [Stop Taking Tokenizers for Granted: They Are Core Design Decisions in Large Language Models](https://arxiv.org/abs/2601.13260)
*Sawsan Alqahtani,Mir Tafseer Nayeem,Md Tahmid Rahman Laskar,Tasnim Mohiuddin,M Saiful Bari*

Main category: cs.CL

TL;DR: 论文提出将分词作为核心建模决策，倡导上下文感知框架，强调标准化评估和透明报告的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有分词方法如BPE存在与语言结构不匹配、放大偏差、浪费容量等问题。

Method: 将分词重新定义为核心建模决策，提出上下文感知框架，整合分词器和模型协同设计。

Result: 无明确提及具体结果。

Conclusion: 将分词作为核心设计问题可产生更公平、高效和适应性更强的语言技术。

Abstract: Tokenization underlies every large language model, yet it remains an under-theorized and inconsistently designed component. Common subword approaches such as Byte Pair Encoding (BPE) offer scalability but often misalign with linguistic structure, amplify bias, and waste capacity across languages and domains. This paper reframes tokenization as a core modeling decision rather than a preprocessing step. We argue for a context-aware framework that integrates tokenizer and model co-design, guided by linguistic, domain, and deployment considerations. Standardized evaluation and transparent reporting are essential to make tokenization choices accountable and comparable. Treating tokenization as a core design problem, not a technical afterthought, can yield language technologies that are fairer, more efficient, and more adaptable.

</details>


### [528] [Autoregressive Models Rival Diffusion Models at ANY-ORDER Generation](https://arxiv.org/abs/2601.13228)
*Tianqi Du,Lizhe Fang,Weijie Yang,Chenheng Zhang,Zeming Wei,Yifei Wang,Yisen Wang*

Main category: cs.CL

TL;DR: 本文提出A3框架解决扩散语言模型的问题，实验显示其优于基于扩散的模型，提供了统一的语言建模范式。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型单步依赖限制建模深度，样本质量和稳定性不如自回归模型，需解决此问题。

Method: 以自回归建模为基础，将扩散式训练重新表述为结构化多组预测过程，提出A3框架，通过双流注意力架构和渐进适应策略实现。

Result: 在问答、常识推理和故事填充实验中，A3优于基于扩散的模型并保持灵活解码。

Conclusion: 该工作为语言建模提供了灵活、高效且新颖的统一方法。

Abstract: Diffusion language models enable any-order generation and bidirectional conditioning, offering appealing flexibility for tasks such as infilling, rewriting, and self-correction. However, their formulation-predicting one part of a sequence from another within a single-step dependency-limits modeling depth and often yields lower sample quality and stability than autoregressive (AR) models. To address this, we revisit autoregressive modeling as a foundation and reformulate diffusion-style training into a structured multi-group prediction process. We propose Any-order Any-subset Autoregressive modeling (A3), a generalized framework that extends the standard AR factorization to arbitrary token groups and generation orders. A3 preserves the probabilistic rigor and multi-layer dependency modeling of AR while inheriting diffusion models' flexibility for parallel and bidirectional generation. We implement A3 through a two-stream attention architecture and a progressive adaptation strategy that transitions pretrained AR models toward any-order prediction. Experiments on question answering, commonsense reasoning, and story infilling demonstrate that A3 outperforms diffusion-based models while maintaining flexible decoding. This work offers a unified approach for a flexible, efficient, and novel language modeling paradigm.

</details>


### [529] [Paid Voices vs. Public Feeds: Interpretable Cross-Platform Theme Modeling of Climate Discourse](https://arxiv.org/abs/2601.13317)
*Samantha Sudhoff,Pranav Perumal,Zhaoqing Wu,Tunazzina Islam*

Main category: cs.CL

TL;DR: 论文对Meta付费广告和Bluesky公开帖子的气候话语进行对比分析，提出主题发现和分配框架，发现平台激励机制影响气候叙事。


<details>
  <summary>Details</summary>
Motivation: 现有计算研究孤立分析不同平台气候传播环境，限制区分机构信息和公众表达的能力。

Method: 提出可解释的端到端主题发现和分配框架，通过语义相似性聚类文本，用大语言模型生成主题标签，用人类判断和基于大语言模型的评估器评估主题质量，通过下游任务验证语义连贯性。

Result: 平台层面的激励机制反映在气候叙事的主题结构、立场一致性和时间响应性上。

Conclusion: 所提出的框架可支持跨异构传播环境的比较叙事分析。

Abstract: Climate discourse online plays a crucial role in shaping public understanding of climate change and influencing political and policy outcomes. However, climate communication unfolds across structurally distinct platforms with fundamentally different incentive structures: paid advertising ecosystems incentivize targeted, strategic persuasion, while public social media platforms host largely organic, user-driven discourse. Existing computational studies typically analyze these environments in isolation, limiting our ability to distinguish institutional messaging from public expression. In this work, we present a comparative analysis of climate discourse across paid advertisements on Meta (previously known as Facebook) and public posts on Bluesky from July 2024 to September 2025. We introduce an interpretable, end-to-end thematic discovery and assignment framework that clusters texts by semantic similarity and leverages large language models (LLMs) to generate concise, human-interpretable theme labels. We evaluate the quality of the induced themes against traditional topic modeling baselines using both human judgments and an LLM-based evaluator, and further validate their semantic coherence through downstream stance prediction and theme-guided retrieval tasks. Applying the resulting themes, we characterize systematic differences between paid climate messaging and public climate discourse and examine how thematic prevalence shifts around major political events. Our findings show that platform-level incentives are reflected in the thematic structure, stance alignment, and temporal responsiveness of climate narratives. While our empirical analysis focuses on climate communication, the proposed framework is designed to support comparative narrative analysis across heterogeneous communication environments.

</details>


### [530] [Sockpuppetting: Jailbreaking LLMs Without Optimization Through Output Prefix Injection](https://arxiv.org/abs/2601.13359)
*Asen Dotsinski,Panagiotis Eustratiadis*

Main category: cs.CL

TL;DR: 介绍了一种名为 "sockpuppetting" 的简单方法来越狱大语言模型，对比GCG方法有更高攻击成功率，凸显了防范输出前缀注入的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着开源大语言模型能力增强，需防范恶意提示，且现有自动化越狱方法要求高，需要简单有效的方法。

Method: 提出 "sockpuppetting" 方法，在模型输出起始插入接受序列让其完成响应，还探索了在助手消息块中优化对抗后缀的混合方法。

Result: 在Qwen3 - 8B上，sockpuppetting比GCG攻击成功率高80%；在Llama - 3.1 - 8B上，混合方法比GCG攻击成功率高64%。

Conclusion: sockpuppetting是低成本有效攻击方法，凸显了开源模型防范输出前缀注入防御的必要性。

Abstract: As open-weight large language models (LLMs) increase in capabilities, safeguarding them against malicious prompts and understanding possible attack vectors becomes ever more important. While automated jailbreaking methods like GCG [Zou et al., 2023] remain effective, they often require substantial computational resources and specific expertise. We introduce "sockpuppetting'', a simple method for jailbreaking open-weight LLMs by inserting an acceptance sequence (e.g., "Sure, here is how to...'') at the start of a model's output and allowing it to complete the response. Requiring only a single line of code and no optimization, sockpuppetting achieves up to 80% higher attack success rate (ASR) than GCG on Qwen3-8B in per-prompt comparisons. We also explore a hybrid approach that optimizes the adversarial suffix within the assistant message block rather than the user prompt, increasing ASR by 64% over GCG on Llama-3.1-8B in a prompt-agnostic setting. The results establish sockpuppetting as an effective low-cost attack accessible to unsophisticated adversaries, highlighting the need for defences against output-prefix injection in open-weight models.

</details>


### [531] [Recurrent Confidence Chain: Temporal-Aware Uncertainty Quantification in Large Language Models](https://arxiv.org/abs/2601.13368)
*Zhenjiang Mao,Anirudhh Venkat*

Main category: cs.CL

TL;DR: 现有大语言模型推理模块虽表现好，但评估答案不确定性时忽视置信度时间传播，本文提出结合跨步骤注意力和隐藏置信机制的方法，在相关数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型推理模块在评估答案不确定性时，常忽略置信度的时间传播，可能导致整体置信度虚高，需解决此问题。

Method: 提出结合跨步骤注意力分析步骤间语义关联的方法，引入隐藏置信机制保留历史置信信息并与逐步置信度结合。

Result: 在GAOKAO数学基准和CLadder因果推理数据集上，使用主流开源大语言模型评估，该方法在预测质量和校准方面优于现有方法。

Conclusion: 所提方法能在预测质量和校准之间取得更好平衡，表现为在负对数似然和预期校准误差上表现出色。

Abstract: As reasoning modules, such as the chain-of-thought mechanism, are applied to large language models, they achieve strong performance on various tasks such as answering common-sense questions and solving math problems. The main challenge now is to assess the uncertainty of answers, which can help prevent misleading or serious hallucinations for users. Although current methods analyze long reasoning sequences by filtering unrelated tokens and examining potential connections between nearby tokens or sentences, the temporal spread of confidence is often overlooked. This oversight can lead to inflated overall confidence, even when earlier steps exhibit very low confidence. To address this issue, we propose a novel method that incorporates inter-step attention to analyze semantic correlations across steps. For handling long-horizon responses, we introduce a hidden confidence mechanism to retain historical confidence information, which is then combined with stepwise confidence to produce a more accurate overall estimate. We evaluate our method on the GAOKAO math benchmark and the CLadder causal reasoning dataset using mainstream open-source large language models. Our approach is shown to outperform state-of-the-art methods by achieving a superior balance between predictive quality and calibration, demonstrated by strong performance on both Negative Log-Likelihood and Expected Calibration Error.

</details>


### [532] [Confidence over Time: Confidence Calibration with Temporal Logic for Large Language Model Reasoning](https://arxiv.org/abs/2601.13387)
*Zhenjiang Mao,Anirudhh Venkat,Artem Bisliouk,Akshat Kothiyal,Sindhura Kumbakonam Subramanian,Saithej Singhu,Ivan Ruchkin*

Main category: cs.CL

TL;DR: 文章指出现有大模型信心评估方法不足，提出用信号时序逻辑（STL）刻画逐步骤信心信号，开发了基于参数超网络的信心评估方法，实验显示效果更好。


<details>
  <summary>Details</summary>
Motivation: 现有信心评估方法将整个推理过程简化为单个标量分数，忽略信心在生成过程中的演变，对表面因素敏感，难以区分正确推理和自信陈述的错误。

Method: 使用信号时序逻辑（STL）刻画逐步骤信心信号，通过判别式STL挖掘程序发现区分正确和错误响应信心信号的时间公式，开发用参数超网络为STL块提供信息的信心评估方法。

Result: 发现STL模式跨任务具有泛化性，数值参数对单个问题敏感，多个推理任务实验表明提出的信心分数比基线更校准。

Conclusion: 提出的基于STL和参数超网络的信心评估方法优于现有基线方法。

Abstract: Large Language Models (LLMs) increasingly rely on long-form, multi-step reasoning to solve complex tasks such as mathematical problem solving and scientific question answering. Despite strong performance, existing confidence estimation methods typically reduce an entire reasoning process to a single scalar score, ignoring how confidence evolves throughout the generation. As a result, these methods are often sensitive to superficial factors such as response length or verbosity, and struggle to distinguish correct reasoning from confidently stated errors. We propose to characterize the stepwise confidence signal using Signal Temporal Logic (STL). Using a discriminative STL mining procedure, we discover temporal formulas that distinguish confidence signals of correct and incorrect responses. Our analysis found that the STL patterns generalize across tasks, and numeric parameters exhibit sensitivity to individual questions. Based on these insights, we develop a confidence estimation approach that informs STL blocks with parameter hypernetworks. Experiments on multiple reasoning tasks show our confidence scores are more calibrated than the baselines.

</details>


### [533] [Trust Me, I'm an Expert: Decoding and Steering Authority Bias in Large Language Models](https://arxiv.org/abs/2601.13433)
*Priyanka Mary Mammen,Emil Joswin,Shankar Venkitachalam*

Main category: cs.CL

TL;DR: 研究语言模型在背书影响下的推理表现，发现模型易受权威误导且可纠正偏差。


<details>
  <summary>Details</summary>
Motivation: 先前研究未充分探索背书来源可信度对语言模型推理任务表现的影响，本文旨在研究模型是否会基于背书者专业程度产生系统偏差。

Method: 选取4个涵盖不同推理领域的数据集，用代表四个专业水平的角色评估11个模型。

Result: 模型对错误/误导性背书的易感性随来源专业性增加而提高，高权威来源会降低准确率并增加对错误答案的信心，且该权威偏差在模型中被机制性编码。

Conclusion: 模型能被引导减少这种权威偏差，即使专家给出误导性背书也可提高其性能。

Abstract: Prior research demonstrates that performance of language models on reasoning tasks can be influenced by suggestions, hints and endorsements. However, the influence of endorsement source credibility remains underexplored. We investigate whether language models exhibit systematic bias based on the perceived expertise of the provider of the endorsement. Across 4 datasets spanning mathematical, legal, and medical reasoning, we evaluate 11 models using personas representing four expertise levels per domain. Our results reveal that models are increasingly susceptible to incorrect/misleading endorsements as source expertise increases, with higher-authority sources inducing not only accuracy degradation but also increased confidence in wrong answers. We also show that this authority bias is mechanistically encoded within the model and a model can be steered away from the bias, thereby improving its performance even when an expert gives a misleading endorsement.

</details>


### [534] [LLM-as-RNN: A Recurrent Language Model for Memory Updates and Sequence Prediction](https://arxiv.org/abs/2601.13352)
*Yuxing Lu,J. Ben Tamo,Weichen Zhao,Nan Sun,Yishan Zhong,Wenqi Shi,Jinzhuo Wang,May D. Wang*

Main category: cs.CL

TL;DR: 提出LLM - as - RNN框架，将冻结的大语言模型转变为循环预测器，在三个序列基准测试中表现出色，平均提高预测准确率6.5%。


<details>
  <summary>Details</summary>
Motivation: 标准推理依赖不可变上下文历史，大语言模型在生成步骤出错后缺乏可更新的记忆机制来改进后续预测。

Method: 提出LLM - as - RNN框架，将隐藏状态表示为自然语言记忆，通过反馈驱动的文本重写在每个时间步更新状态，在固定令牌预算下进行学习。

Result: 在医疗、气象和金融三个序列基准测试中，LLM - as - RNN显著优于零样本、全历史和MemPrompt基线，平均提高预测准确率6.5%。

Conclusion: LLM - as - RNN能有效通过语言进行在线学习，且产生可解释、人类可读的学习痕迹。

Abstract: Large language models are strong sequence predictors, yet standard inference relies on immutable context histories. After making an error at generation step t, the model lacks an updatable memory mechanism that improves predictions for step t+1. We propose LLM-as-RNN, an inference-only framework that turns a frozen LLM into a recurrent predictor by representing its hidden state as natural-language memory. This state, implemented as a structured system-prompt summary, is updated at each timestep via feedback-driven text rewrites, enabling learning without parameter updates. Under a fixed token budget, LLM-as-RNN corrects errors and retains task-relevant patterns, effectively performing online learning through language. We evaluate the method on three sequential benchmarks in healthcare, meteorology, and finance across Llama, Gemma, and GPT model families. LLM-as-RNN significantly outperforms zero-shot, full-history, and MemPrompt baselines, improving predictive accuracy by 6.5% on average, while producing interpretable, human-readable learning traces absent in standard context accumulation.

</details>


### [535] [MOSLD-Bench: Multilingual Open-Set Learning and Discovery Benchmark for Text Categorization](https://arxiv.org/abs/2601.13437)
*Adriana-Valentina Costache,Daria-Nicoleta Dragomir,Silviu-Florin Gheorghe,Eduard Poesina,Paul Irofti,Radu Tudor Ionescu*

Main category: cs.CL

TL;DR: 本文介绍了首个用于文本主题分类的多语言开放集学习与发现（MOSLD）基准，并提出新框架，评估多种语言模型，还公开了基准。


<details>
  <summary>Details</summary>
Motivation: 开放集学习与发现（OSLD）在文本领域是较新设置，缺乏相关基准，因此有必要引入多语言的OSLD基准。

Method: （1）重新整理现有数据集并从新闻领域收集新数据样本构建基准；（2）提出集成多阶段的新框架用于OSLD任务；（3）评估多种语言模型。

Result: 获得可作为未来工作参考的结果。

Conclusion: 成功引入多语言MOSLD基准，公开在https://github.com/Adriana19Valentina/MOSLD - Bench。

Abstract: Open-set learning and discovery (OSLD) is a challenging machine learning task in which samples from new (unknown) classes can appear at test time. It can be seen as a generalization of zero-shot learning, where the new classes are not known a priori, hence involving the active discovery of new classes. While zero-shot learning has been extensively studied in text classification, especially with the emergence of pre-trained language models, open-set learning and discovery is a comparatively new setup for the text domain. To this end, we introduce the first multilingual open-set learning and discovery (MOSLD) benchmark for text categorization by topic, comprising 960K data samples across 12 languages. To construct the benchmark, we (i) rearrange existing datasets and (ii) collect new data samples from the news domain. Moreover, we propose a novel framework for the OSLD task, which integrates multiple stages to continuously discover and learn new classes. We evaluate several language models, including our own, to obtain results that can be used as reference for future work. We release our benchmark at https://github.com/Adriana19Valentina/MOSLD-Bench.

</details>


### [536] [Beyond Memorization: Testing LLM Reasoning on Unseen Theory of Computation Tasks](https://arxiv.org/abs/2601.13392)
*Shlok Shelat,Jay Raval,Souvik Roy,Manas Gaur*

Main category: cs.CL

TL;DR: 评估大语言模型在DFA构造基准测试中的表现，发现其在未见问题上准确率低，多种提示策略均无法解决语义推理的根本问题。


<details>
  <summary>Details</summary>
Motivation: 明确大语言模型在形式语言任务上的良好表现是源于真正的符号推理还是模式匹配。

Method: 引入DFA构造基准测试，包含不同类型问题；评估三阶段提示协议；分析多种提示策略。

Result: 模型在事实问题和见过的任务上准确率较高，但在未见问题上准确率大幅下降，提示协议无法可靠解决全局不一致或结构有缺陷的自动机问题。

Conclusion: 大语言模型能生成语法上合理的DFA，但在语义正确的形式推理方面存在根本差距。

Abstract: Large language models (LLMs) have demonstrated strong performance on formal language tasks, yet whether this reflects genuine symbolic reasoning or pattern matching on familiar constructions remains unclear. We introduce a benchmark for deterministic finite automata (DFA) construction from regular languages, comprising factual knowledge questions, seen construction problems from public sources, and two types of unseen problems: hand-crafted instances with multiple interacting constraints and systematically generated problems via Arden's theorem. Models achieve perfect accuracy on factual questions and 84-90% on seen tasks. However, accuracy drops sharply on unseen problems (by 30-64%), with failures stemming from systematic misinterpretation of language constraints, incorrect handling of Kleene-star semantics, and a failure to preserve global consistency. We evaluate a three-stage hint protocol that enables correction of shallow errors but does not reliably resolve globally inconsistent or structurally flawed automata. Our analysis across multiple prompting strategies (direct, Chain-of-Thought, Tree-of-Thought) reveals that errors persist regardless of prompting approach, exposing a fundamental gap between LLMs' ability to generate syntactically plausible DFAs and their capacity for semantically correct formal reasoning.

</details>


### [537] [CauScientist: Teaching LLMs to Respect Data for Causal Discovery](https://arxiv.org/abs/2601.13614)
*Bo Peng,Sirui Chen,Lei Xu,Chaochao Lu*

Main category: cs.CL

TL;DR: 提出CauScientist协同框架用于因果发现，实验显示其性能优于纯数据驱动基线。


<details>
  <summary>Details</summary>
Motivation: 现有因果发现方法存在局限，纯数据驱动方法有统计不可区分性和建模假设问题，基于大语言模型的方法忽略统计证据或引入不可验证的先验信息。

Method: 提出CauScientist协同框架，结合大语言模型作为假设生成器和概率统计作为验证器，采用混合初始化、迭代优化结构和错误记忆引导搜索空间。

Result: CauScientist显著优于纯数据驱动基线，F1分数提升达53.8%，召回率从35.0%提升到100.0%，在37节点图上相比Qwen3 - 32B降低结构汉明距离44.0%。

Conclusion: CauScientist是一种有效的因果发现方法。

Abstract: Causal discovery is fundamental to scientific understanding and reliable decision-making. Existing approaches face critical limitations: purely data-driven methods suffer from statistical indistinguishability and modeling assumptions, while recent LLM-based methods either ignore statistical evidence or incorporate unverified priors that can mislead result. To this end, we propose CauScientist, a collaborative framework that synergizes LLMs as hypothesis-generating "data scientists" with probabilistic statistics as rigorous "verifiers". CauScientist employs hybrid initialization to select superior starting graphs, iteratively refines structures through LLM-proposed modifications validated by statistical criteria, and maintains error memory to guide efficient search space. Experiments demonstrate that CauScientist substantially outperforms purely data-driven baselines, achieving up to 53.8% F1 score improvement and enhancing recall from 35.0% to 100.0%. Notably, while standalone LLM performance degrades with graph complexity, CauScientist reduces structural hamming distance (SHD) by 44.0% compared to Qwen3-32B on 37-node graphs. Our project page is at https://github.com/OpenCausaLab/CauScientist.

</details>


### [538] [Towards Token-Level Text Anomaly Detection](https://arxiv.org/abs/2601.13644)
*Yang Cao,Bicheng Yu,Sikun Yang,Ming Liu,Yujiu Yang*

Main category: cs.CL

TL;DR: 本文引入token级异常检测范式，定义文本异常，提出跨级检测框架，发布三个带token标签的数据集，实验效果佳且代码数据公开。


<details>
  <summary>Details</summary>
Motivation: 现有文本异常检测方法局限于文档级分析，无法识别文本具体异常部分。

Method: 引入token级异常检测范式，形式化定义文档和token级文本异常，提出统一跨级检测框架，收集并标注三个带token标签的数据集。

Result: 实验显示该框架性能优于6个基线模型。

Conclusion: 该框架为文本精确异常定位开辟新可能。

Abstract: Despite significant progress in text anomaly detection for web applications such as spam filtering and fake news detection, existing methods are fundamentally limited to document-level analysis, unable to identify which specific parts of a text are anomalous. We introduce token-level anomaly detection, a novel paradigm that enables fine-grained localization of anomalies within text. We formally define text anomalies at both document and token-levels, and propose a unified detection framework that operates across multiple levels. To facilitate research in this direction, we collect and annotate three benchmark datasets spanning spam, reviews and grammar errors with token-level labels. Experimental results demonstrate that our framework get better performance than other 6 baselines, opening new possibilities for precise anomaly localization in text. All the codes and data are publicly available on https://github.com/charles-cao/TokenCore.

</details>


### [539] [Uncertainty-Aware Gradient Signal-to-Noise Data Selection for Instruction Tuning](https://arxiv.org/abs/2601.13697)
*Zhihang Yuan,Chengyu Yue,Long Huang,Litu Ou,Lei Shi*

Main category: cs.CL

TL;DR: 现有指令数据集问题多，全量微调成本高且不必要，本文提出GRADFILTERING数据选择框架，表现良好且收敛更快。


<details>
  <summary>Details</summary>
Motivation: 现代指令数据集大、嘈杂且冗余，全量微调成本高且不必要，现有数据选择方法存在缺陷，忽略了不确定性。

Method: 提出GRADFILTERING，一个与目标无关、考虑不确定性的数据选择框架，利用小的GPT - 2代理和LoRA集成，将每个示例的梯度聚合为梯度信噪比（G - SNR）效用。

Result: 在大多数大语言模型评判评估和人工评估中，该方法与随机子集和强基线相当或更优，在相同计算预算下，GRADFILTERING选择的子集比竞争过滤器收敛更快。

Conclusion: GRADFILTERING在数据选择方面表现出色，考虑不确定性的评分有益。

Abstract: Instruction tuning is a standard paradigm for adapting large language models (LLMs), but modern instruction datasets are large, noisy, and redundant, making full-data fine-tuning costly and often unnecessary. Existing data selection methods either build expensive gradient datastores or assign static scores from a weak proxy, largely ignoring evolving uncertainty, and thus missing a key source of LLM interpretability. We propose GRADFILTERING, an objective-agnostic, uncertainty-aware data selection framework that utilizes a small GPT-2 proxy with a LoRA ensemble and aggregates per-example gradients into a Gradient Signal-to-Noise Ratio (G-SNR) utility. Our method matches or surpasses random subsets and strong baselines in most LLM-as-a-judge evaluations as well as in human assessment. Moreover, GRADFILTERING-selected subsets converge faster than competitive filters under the same compute budget, reflecting the benefit of uncertainty-aware scoring.

</details>


### [540] [When Wording Steers the Evaluation: Framing Bias in LLM judges](https://arxiv.org/abs/2601.13537)
*Yerin Hwang,Dongryeol Lee,Taegwan Kang,Minwoo Lee,Kyomin Jung*

Main category: cs.CL

TL;DR: 本文研究大语言模型基于提示措辞的框架偏差对评估的影响，发现当前大语言模型评估系统存在框架偏差问题。


<details>
  <summary>Details</summary>
Motivation: 已知大语言模型的回答受提示措辞影响，但该框架偏差对大语言模型评估的影响未被充分研究，因此开展研究。

Method: 受心理学框架效应启发，针对四个高风险评估任务，设计对称的肯定和否定提示结构来研究提示框架对模型判断的影响。

Result: 在14个大语言模型评判者上，观察到模型明显受框架影响，不同模型家族有不同的同意或拒绝倾向。

Conclusion: 框架偏差是当前大语言模型评估系统的结构属性，需要制定考虑框架效应的协议。

Abstract: Large language models (LLMs) are known to produce varying responses depending on prompt phrasing, indicating that subtle guidance in phrasing can steer their answers. However, the impact of this framing bias on LLM-based evaluation, where models are expected to make stable and impartial judgments, remains largely underexplored. Drawing inspiration from the framing effect in psychology, we systematically investigate how deliberate prompt framing skews model judgments across four high-stakes evaluation tasks. We design symmetric prompts using predicate-positive and predicate-negative constructions and demonstrate that such framing induces significant discrepancies in model outputs. Across 14 LLM judges, we observe clear susceptibility to framing, with model families showing distinct tendencies toward agreement or rejection. These findings suggest that framing bias is a structural property of current LLM-based evaluation systems, underscoring the need for framing-aware protocols.

</details>


### [541] [HateXScore: A Metric Suite for Evaluating Reasoning Quality in Hate Speech Explanations](https://arxiv.org/abs/2601.13547)
*Yujia Hu,Roy Ka-Wei Lee*

Main category: cs.CL

TL;DR: 提出HateXScore评估指标套件，评估仇恨言论检测模型解释的推理质量，在六个数据集上评估，有人类评估验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前评估框架很少评估文本被视为仇恨言论的原因，需要评估模型解释的推理质量。

Method: 引入HateXScore指标套件，从四个方面评估模型解释：结论明确性、引用跨度的忠实性和因果基础、受保护群体识别、元素间逻辑一致性。

Result: 在六个仇恨言论数据集上评估，能揭示标准指标看不到的解释性失败和标注不一致问题，人类评估与HateXScore高度一致。

Conclusion: HateXScore可作为诊断性补充，是值得信赖和透明审核的实用工具。

Abstract: Hateful speech detection is a key component of content moderation, yet current evaluation frameworks rarely assess why a text is deemed hateful. We introduce \textsf{HateXScore}, a four-component metric suite designed to evaluate the reasoning quality of model explanations. It assesses (i) conclusion explicitness, (ii) faithfulness and causal grounding of quoted spans, (iii) protected group identification (policy-configurable), and (iv) logical consistency among these elements. Evaluated on six diverse hate speech datasets, \textsf{HateXScore} is intended as a diagnostic complement to reveal interpretability failures and annotation inconsistencies that are invisible to standard metrics like Accuracy or F1. Moreover, human evaluation shows strong agreement with \textsf{HateXScore}, validating it as a practical tool for trustworthy and transparent moderation.
  \textcolor{red}{Disclaimer: This paper contains sensitive content that may be disturbing to some readers.}

</details>


### [542] [Pro-AI Bias in Large Language Models](https://arxiv.org/abs/2601.13749)
*Benaya Trabelsi,Jonathan Shaki,Sarit Kraus*

Main category: cs.CL

TL;DR: 研究大语言模型是否存在支持AI的偏好偏差，发现有这种偏好，会影响高风险决策。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型是否对人工智能存在系统性偏好偏差。

Method: 进行三个互补实验，包括分析对咨询问题的推荐、对比AI与非AI工作薪资预估、探测开放权重模型内部表征。

Result: 发现大语言模型存在亲AI偏差，如推荐AI相关选项、高估AI工作薪资、AI在内部表征中具有中心地位。

Conclusion: 大语言模型生成的建议和估值会系统性地影响高风险决策中的选择和认知。

Abstract: Large language models (LLMs) are increasingly employed for decision-support across multiple domains. We investigate whether these models display a systematic preferential bias in favor of artificial intelligence (AI) itself. Across three complementary experiments, we find consistent evidence of pro-AI bias. First, we show that LLMs disproportionately recommend AI-related options in response to diverse advice-seeking queries, with proprietary models doing so almost deterministically. Second, we demonstrate that models systematically overestimate salaries for AI-related jobs relative to closely matched non-AI jobs, with proprietary models overestimating AI salaries more by 10 percentage points. Finally, probing internal representations of open-weight models reveals that ``Artificial Intelligence'' exhibits the highest similarity to generic prompts for academic fields under positive, negative, and neutral framings alike, indicating valence-invariant representational centrality. These patterns suggest that LLM-generated advice and valuation can systematically skew choices and perceptions in high-stakes decisions.

</details>


### [543] [Knowledge Graph-Assisted LLM Post-Training for Enhanced Legal Reasoning](https://arxiv.org/abs/2601.13806)
*Dezhao Song,Guglielmo Bonifazi,Frank Schilder,Jonathan Richard Schwarz*

Main category: cs.CL

TL;DR: 本文提出用知识图谱辅助方法增强大语言模型在法律领域的推理能力，在多个法律基准测试中表现更好。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型后训练未捕捉领域知识结构，难以处理复杂推理任务，在法律领域缺乏对法律概念关系的理解。

Method: 遵循IRAC框架建模关键法律概念，构建含12K法律案例的知识图谱，用其生成训练数据，对三个SOTA大语言模型进行监督微调与直接偏好优化。

Result: 后训练模型在4/5的法律基准测试中平均表现优于基线，70B DPO模型在4/6推理任务中得分最佳。

Conclusion: 所构建的知识图谱能有效增强大语言模型的法律推理能力。

Abstract: LLM post-training has primarily relied on large text corpora and human feedback, without capturing the structure of domain knowledge. This has caused models to struggle dealing with complex reasoning tasks, especially for high-stakes professional domains. In Law, reasoning requires deep understanding of the relations between various legal concepts, a key component missing in current LLM post-training. In this paper, we propose a knowledge graph (KG)-assisted approach for enhancing LLMs' reasoning capability in Legal that is generalizable to other high-stakes domains. We model key legal concepts by following the \textbf{IRAC} (Issue, Rule, Analysis and Conclusion) framework, and construct a KG with 12K legal cases. We then produce training data using our IRAC KG, and conduct both Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) with three state-of-the-art (SOTA) LLMs (30B, 49B and 70B), varying architecture and base model family. Our post-trained models obtained better average performance on 4/5 diverse legal benchmarks (14 tasks) than baselines. In particular, our 70B DPO model achieved the best score on 4/6 reasoning tasks, among baselines and a 141B SOTA legal LLM, demonstrating the effectiveness of our KG for enhancing LLMs' legal reasoning capability.

</details>


### [544] [TREX: Tokenizer Regression for Optimal Data Mixture](https://arxiv.org/abs/2601.13588)
*Inho Won,Hangyeol Yoo,Minkyung Cho,Jungyeul Park,Hoyun Song,KyungTae Lim*

Main category: cs.CL

TL;DR: 论文提出TREX框架，能高效预测多语言大模型分词器训练的最优数据混合比，提升压缩效率。


<details>
  <summary>Details</summary>
Motivation: 现有确定多语言大模型分词器数据混合比的方法依靠启发式或大规模搜索，存在精度-成本权衡问题。

Method: 训练小规模代理分词器，收集压缩统计数据，学习从数据混合比预测压缩性能，在大规模分词器训练前进行可扩展的混合比搜索。

Result: 使用TREX预测的混合比训练的分词器在分布内和分布外的压缩效率上比基于LLaMA3和均匀分布的混合比高12%。

Conclusion: TREX具有强可扩展性、鲁棒性和实际有效性。

Abstract: Building effective tokenizers for multilingual Large Language Models (LLMs) requires careful control over language-specific data mixtures. While a tokenizer's compression performance critically affects the efficiency of LLM training and inference, existing approaches rely on heuristics or costly large-scale searches to determine optimal language ratios. We introduce Tokenizer Regression for Optimal Data MiXture (TREX), a regression-based framework that efficiently predicts the optimal data mixture for tokenizer training. TREX trains small-scale proxy tokenizers on random mixtures, gathers their compression statistics, and learns to predict compression performance from data mixtures. This learned model enables scalable mixture search before large-scale tokenizer training, mitigating the accuracy-cost trade-off in multilingual tokenizer design. Tokenizers trained with TReX's predicted mixtures outperform mixtures based on LLaMA3 and uniform distributions by up to 12% in both inand out-of-distribution compression efficiency, demonstrating strong scalability, robustness, and practical effectiveness.

</details>


### [545] [Vulnerability of LLMs' Belief Systems? LLMs Belief Resistance Check Through Strategic Persuasive Conversation Interventions](https://arxiv.org/abs/2601.13590)
*Fan Huang,Haewoon Kwak,Jisun An*

Main category: cs.CL

TL;DR: 对大语言模型在SMCR框架下的说服易感性进行系统评估，发现小模型易被说服，元认知提示反而增加易感性，对抗性微调防御效果因模型而异。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在问答任务中易受说服且采用反事实信念，需评估其说服易感性。

Method: 在SMCR通信框架下，对五个主流大语言模型和三个领域，分析不同说服策略对多轮交互中信念稳定性的影响，研究元认知提示作用，评估对抗性微调防御效果。

Result: 小模型首轮说服时超80%信念改变；元认知提示加速信念侵蚀；GPT - 4o - mini近完全鲁棒，Mistral 7B有改善，Llama模型仍高度易感。

Conclusion: 当前鲁棒性干预有显著模型依赖局限性，为开发更可信大语言模型提供指导。

Abstract: Large Language Models (LLMs) are increasingly employed in various question-answering tasks. However, recent studies showcase that LLMs are susceptible to persuasion and could adopt counterfactual beliefs. We present a systematic evaluation of LLM susceptibility to persuasion under the Source--Message--Channel--Receiver (SMCR) communication framework. Across five mainstream Large Language Models (LLMs) and three domains (factual knowledge, medical QA, and social bias), we analyze how different persuasive strategies influence belief stability over multiple interaction turns. We further examine whether meta-cognition prompting (i.e., eliciting self-reported confidence) affects resistance to persuasion. Results show that smaller models exhibit extreme compliance, with over 80% of belief changes occurring at the first persuasive turn (average end turn of 1.1--1.4). Contrary to expectations, meta-cognition prompting increases vulnerability by accelerating belief erosion rather than enhancing robustness. Finally, we evaluate adversarial fine-tuning as a defense. While GPT-4o-mini achieves near-complete robustness (98.6%) and Mistral~7B improves substantially (35.7% $\rightarrow$ 79.3%), Llama models remain highly susceptible (<14%) even when fine-tuned on their own failure cases. Together, these findings highlight substantial model-dependent limits of current robustness interventions and offer guidance for developing more trustworthy LLMs.

</details>


### [546] [Fairness or Fluency? An Investigation into Language Bias of Pairwise LLM-as-a-Judge](https://arxiv.org/abs/2601.13649)
*Xiaolin Zhou,Zheng Luo,Yicheng Gao,Qixuan Chen,Xiyang Hu,Yue Zhao,Ruishan Liu*

Main category: cs.CL

TL;DR: 研究大语言模型作为评判者时的两种语言偏差，发现同语言评判存在跨语系表现差异、跨语言评判模型倾向英语答案，且语言偏差不能仅由低困惑度偏差解释。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明大语言模型作为评判者存在偏差且与人类偏好不一致，本文聚焦研究其中的语言偏差。

Method: 研究同语言评判时模型对同语言选项的表现差异，以及跨语言评判时模型对不同语言选项的倾向。还探究语言偏差是否由低困惑度偏差导致。

Result: 同语言评判中欧洲语言表现优于非洲语言，在文化相关主题更明显；跨语言评判多数模型偏好英语答案，且受答案语言影响更大；困惑度与语言偏差有轻微关联但不能完全解释。

Conclusion: 大语言模型作为评判者存在语言偏差，且不能仅用低困惑度偏差解释该语言偏差。

Abstract: Recent advances in Large Language Models (LLMs) have incentivized the development of LLM-as-a-judge, an application of LLMs where they are used as judges to decide the quality of a certain piece of text given a certain context. However, previous studies have demonstrated that LLM-as-a-judge can be biased towards different aspects of the judged texts, which often do not align with human preference. One of the identified biases is language bias, which indicates that the decision of LLM-as-a-judge can differ based on the language of the judged texts. In this paper, we study two types of language bias in pairwise LLM-as-a-judge: (1) performance disparity between languages when the judge is prompted to compare options from the same language, and (2) bias towards options written in major languages when the judge is prompted to compare options of two different languages. We find that for same-language judging, there exist significant performance disparities across language families, with European languages consistently outperforming African languages, and this bias is more pronounced in culturally-related subjects. For inter-language judging, we observe that most models favor English answers, and that this preference is influenced more by answer language than question language. Finally, we investigate whether language bias is in fact caused by low-perplexity bias, a previously identified bias of LLM-as-a-judge, and we find that while perplexity is slightly correlated with language bias, language bias cannot be fully explained by perplexity only.

</details>


### [547] [Temporal-Spatial Decouple before Act: Disentangled Representation Learning for Multimodal Sentiment Analysis](https://arxiv.org/abs/2601.13659)
*Chunlei Meng,Ziyang Zhou,Lucas He,Xiaojing Du,Chun Ouyang,Zhongxue Gan*

Main category: cs.CL

TL;DR: 提出TSDA方法用于多模态情感分析，通过时空解耦提升性能，实验表明优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 主流多模态情感分析方法依赖时空混合建模，忽略时空异质性，导致时空信息不对称和性能受限。

Method: 提出TSDA方法，先将各模态解耦为时间动态和空间结构上下文，通过因子一致跨模态对齐分别对齐时间和空间特征，使用特定监督和去相关正则化减少跨因子泄漏，最后通过门控重耦合模块重耦合进行任务。

Result: TSDA在实验中优于基线模型，消融分析确认设计的必要性和可解释性。

Conclusion: TSDA方法有效，通过时空解耦和对齐能提升多模态情感分析的性能。

Abstract: Multimodal Sentiment Analysis integrates Linguistic, Visual, and Acoustic. Mainstream approaches based on modality-invariant and modality-specific factorization or on complex fusion still rely on spatiotemporal mixed modeling. This ignores spatiotemporal heterogeneity, leading to spatiotemporal information asymmetry and thus limited performance. Hence, we propose TSDA, Temporal-Spatial Decouple before Act, which explicitly decouples each modality into temporal dynamics and spatial structural context before any interaction. For every modality, a temporal encoder and a spatial encoder project signals into separate temporal and spatial body. Factor-Consistent Cross-Modal Alignment then aligns temporal features only with their temporal counterparts across modalities, and spatial features only with their spatial counterparts. Factor specific supervision and decorrelation regularization reduce cross factor leakage while preserving complementarity. A Gated Recouple module subsequently recouples the aligned streams for task. Extensive experiments show that TSDA outperforms baselines. Ablation analysis studies confirm the necessity and interpretability of the design.

</details>


### [548] [HeteroCache: A Dynamic Retrieval Approach to Heterogeneous KV Cache Compression for Long-Context LLM Inference](https://arxiv.org/abs/2601.13684)
*Zhiyuan Shi,Qibo Qiu,Feng Xue,Zhonglin Jiang,Li Yu,Jian Jiang,Xiaofei He,Wenxiao Wang*

Main category: cs.CL

TL;DR: 提出无训练动态压缩框架HeteroCache解决KV缓存线性内存增长瓶颈，实验表现优且代码将开源


<details>
  <summary>Details</summary>
Motivation: KV缓存线性内存增长是长上下文任务LLM推理瓶颈，现有静态压缩和动态检索方法有局限

Method: 基于两个关键洞察对注意力头分类，采用细粒度加权策略，使用分层存储机制

Result: HeteroCache在多基准测试达SOTA，在224K上下文解码速度最高提升3倍

Conclusion: HeteroCache有效解决了现有方法的局限，在长上下文任务中表现出色

Abstract: The linear memory growth of the KV cache poses a significant bottleneck for LLM inference in long-context tasks. Existing static compression methods often fail to preserve globally important information, principally because they overlook the attention drift phenomenon where token significance evolves dynamically. Although recent dynamic retrieval approaches attempt to address this issue, they typically suffer from coarse-grained caching strategies and incur high I/O overhead due to frequent data transfers. To overcome these limitations, we propose HeteroCache, a training-free dynamic compression framework. Our method is built on two key insights: attention heads exhibit diverse temporal heterogeneity, and there is significant spatial redundancy among heads within the same layer. Guided by these insights, HeteroCache categorizes heads based on stability and redundancy. Consequently, we apply a fine-grained weighting strategy that allocates larger cache budgets to heads with rapidly shifting attention to capture context changes, thereby addressing the inefficiency of coarse-grained strategies. Furthermore, we employ a hierarchical storage mechanism in which a subset of representative heads monitors attention shift, and trigger an asynchronous, on-demand retrieval of contexts from the CPU, effectively hiding I/O latency. Finally, experiments demonstrate that HeteroCache achieves state-of-the-art performance on multiple long-context benchmarks and accelerates decoding by up to $3\times$ compared to the original model in the 224K context. Our code will be open-source.

</details>


### [549] [Kakugo: Distillation of Low-Resource Languages into Small Language Models](https://arxiv.org/abs/2601.14051)
*Peter Devine,Mardhiyah Sanni,Farid Adilazuarda,Julieta Gil Loizaga,Barry Haddow*

Main category: cs.CL

TL;DR: 提出Kakugo管道，用语言名训练低资源语言通用小语言模型，成本低且性能提升。


<details>
  <summary>Details</summary>
Motivation: 为低资源语言训练通用小语言模型，且降低成本，让社区可开发特定语言AI。

Method: 用大教师模型生成合成提示和翻译指令数据集，以生成训练数据和小语言模型。

Result: 为54种低资源语言生成训练数据和小语言模型，在多种自然语言处理任务中性能优于基础模型。

Conclusion: Kakugo成本低于每种语言50美元，是社区开发特定语言AI的可行方法。

Abstract: We present Kakugo, a novel and cost-effective pipeline designed to train general-purpose Small Language Models (SLMs) for low-resource languages using only the language name as input. By using a large teacher model to generate synthetic prompts and translate instruction datasets, we produced training data and SLMs for 54 low-resource languages. Evaluations across a diverse set of general natural language processing tasks, including translation, classification, and question answering, demonstrate that our pipeline consistently improves performance over base models. With a total generation and training cost of under $50 per language, Kakugo offers an accessible method for communities to develop language-specific AI.

</details>


### [550] [Simulated Ignorance Fails: A Systematic Study of LLM Behaviors on Forecasting Problems Before Model Knowledge Cutoff](https://arxiv.org/abs/2601.13717)
*Zehan Li,Yuxuan Wang,Ali El Lahib,Ying-Jieh Xia,Xinyu Pi*

Main category: cs.CL

TL;DR: 研究模拟无知（SI）能否近似真实无知（TI），发现SI系统性失败，不建议用基于SI的回顾性设置来评估预测能力。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型预测能力时，前瞻性评估有延迟问题，回顾性预测（RF）面临评估数据减少的问题，SI被视为潜在解决方案，需测试其能否近似TI。

Method: 在477个竞赛级问题和9个模型上进行测试。

Result: SI系统性失败，指令使SI和TI有52%的性能差距；思维链推理无法抑制先验知识；推理优化模型的SI保真度更差。

Conclusion: 对预截止事件的RF在方法上有缺陷，不建议用基于SI的回顾性设置来评估预测能力。

Abstract: Evaluating LLM forecasting capabilities is constrained by a fundamental tension: prospective evaluation offers methodological rigor but prohibitive latency, while retrospective forecasting (RF) -- evaluating on already-resolved events -- faces rapidly shrinking clean evaluation data as SOTA models possess increasingly recent knowledge cutoffs. Simulated Ignorance (SI), prompting models to suppress pre-cutoff knowledge, has emerged as a potential solution. We provide the first systematic test of whether SI can approximate True Ignorance (TI). Across 477 competition-level questions and 9 models, we find that SI fails systematically: (1) cutoff instructions leave a 52% performance gap between SI and TI; (2) chain-of-thought reasoning fails to suppress prior knowledge, even when reasoning traces contain no explicit post-cutoff references; (3) reasoning-optimized models exhibit worse SI fidelity despite superior reasoning trace quality. These findings demonstrate that prompts cannot reliably "rewind" model knowledge. We conclude that RF on pre-cutoff events is methodologically flawed; we recommend against using SI-based retrospective setups to benchmark forecasting capabilities.

</details>


### [551] [OP-Bench: Benchmarking Over-Personalization for Memory-Augmented Personalized Conversational Agents](https://arxiv.org/abs/2601.13722)
*Yulin Hu,Zimo Long,Jiahe Guo,Xingyu Sui,Xing Fu,Weixiang Zhao,Yanyan Zhao,Bing Qin*

Main category: cs.CL

TL;DR: 论文指出记忆增强对话代理存在过度个性化问题，定义其类型，构建OP - Bench基准评估，发现问题普遍存在，提出Self - ReCheck机制缓解该问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准忽略记忆增强对话代理中个人信息使用是否恰当的问题，即过度个性化问题。

Method: 将过度个性化分为三种类型，构建OP - Bench基准评估大语言模型和记忆增强方法，提出Self - ReCheck记忆过滤机制。

Result: 发现引入记忆时过度个性化问题普遍存在，代理人常不必要地检索和过度关注用户记忆。

Conclusion: 提出的Self - ReCheck机制可缓解过度个性化问题，为更可控和恰当的个性化记忆增强对话系统迈出第一步。

Abstract: Memory-augmented conversational agents enable personalized interactions using long-term user memory and have gained substantial traction. However, existing benchmarks primarily focus on whether agents can recall and apply user information, while overlooking whether such personalization is used appropriately. In fact, agents may overuse personal information, producing responses that feel forced, intrusive, or socially inappropriate to users. We refer to this issue as \emph{over-personalization}. In this work, we formalize over-personalization into three types: Irrelevance, Repetition, and Sycophancy, and introduce \textbf{OP-Bench} a benchmark of 1,700 verified instances constructed from long-horizon dialogue histories. Using \textbf{OP-Bench}, we evaluate multiple large language models and memory-augmentation methods, and find that over-personalization is widespread when memory is introduced. Further analysis reveals that agents tend to retrieve and over-attend to user memories even when unnecessary. To address this issue, we propose \textbf{Self-ReCheck}, a lightweight, model-agnostic memory filtering mechanism that mitigates over-personalization while preserving personalization performance. Our work takes an initial step toward more controllable and appropriate personalization in memory-augmented dialogue systems.

</details>


### [552] [Learning to Explain: Supervised Token Attribution from Transformer Attention Patterns](https://arxiv.org/abs/2601.14112)
*George Mihaila*

Main category: cs.CL

TL;DR: 随着基于Transformer的模型在高风险应用中部署，可解释AI至关重要，本文提出轻量级神经网络ExpNet，自动发现最优注意力特征组合，并进行跨任务评估。


<details>
  <summary>Details</summary>
Motivation: 基于Transformer的模型在高风险应用中存在不透明问题，现有基于注意力的解释方法依赖手动聚合策略和固定归因规则，模型无关方法把模型视为黑箱且计算成本高。

Method: 引入轻量级神经网络ExpNet，学习从Transformer注意力模式到词元级重要性得分的显式映射，自动发现最优注意力特征组合。

Result: 论文提到在具有挑战性的跨任务设置中评估了ExpNet，并与广泛的模型无关方法和基于注意力的技术进行了对比，但未给出具体评估结果。

Conclusion: 论文未明确提及结论。

Abstract: Explainable AI (XAI) has become critical as transformer-based models are deployed in high-stakes applications including healthcare, legal systems, and financial services, where opacity hinders trust and accountability. Transformers self-attention mechanisms have proven valuable for model interpretability, with attention weights successfully used to understand model focus and behavior (Xu et al., 2015); (Wiegreffe and Pinter, 2019). However, existing attention-based explanation methods rely on manually defined aggregation strategies and fixed attribution rules (Abnar and Zuidema, 2020a); (Chefer et al., 2021), while model-agnostic approaches (LIME, SHAP) treat the model as a black box and incur significant computational costs through input perturbation. We introduce Explanation Network (ExpNet), a lightweight neural network that learns an explicit mapping from transformer attention patterns to token-level importance scores. Unlike prior methods, ExpNet discovers optimal attention feature combinations automatically rather than relying on predetermined rules. We evaluate ExpNet in a challenging cross-task setting and benchmark it against a broad spectrum of model-agnostic methods and attention-based techniques spanning four methodological families.

</details>


### [553] [Towards robust long-context understanding of large language model via active recap learning](https://arxiv.org/abs/2601.13734)
*Chenyu Hui*

Main category: cs.CL

TL;DR: 提出主动回顾学习（ARL）框架增强大语言模型长上下文理解能力，实验取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 提升大语言模型对长上下文的理解能力。

Method: 在持续预训练中通过有针对性的序列构建和推理时的回顾性总结，让模型回顾和总结前文内容；基于长短前向上下文的损失差距识别关键标记，找到最相关前文段落并用大语言模型总结；推理时模型自主生成和利用回顾性总结，建立段落间递归记忆机制。

Result: ARL在RULER上提升26.8%，在LongBench上提升9.44%。

Conclusion: ARL是一种简单有效的基于持续预训练的方法，可加强长上下文理解，推动大语言模型可扩展内存增强。

Abstract: In this paper, we propose active recap learning (ARL), a framework for enhancing large language model (LLM) in understanding long contexts. ARL enables models to revisit and summarize earlier content through targeted sequence construction during contined pretraining and retrospective summarization at inference. First, we identify key tokens in prepared long context based on loss gaps between long and short forward contexts and find most revant preceding paragraphs, then summarize them using an LLM. Second, ARL equips models with the ability to autonomously generate and utilize these retrospective summaries during inference, thereby establishing a recursive memory mechanism across paragraphs. Experimental results show substantial gains, with ARL achieving a 26.8% improvement on RULER and a 9.44% improvement on LongBench. Overall, ARL offers a simple yet effective continued pretraining-based approach to strengthen long-context understanding, advancing scalable memory augmentation in LLM

</details>


### [554] [Lost in the Prompt Order: Revealing the Limitations of Causal Attention in Language Models](https://arxiv.org/abs/2601.14152)
*Hyunjong Ok,Jaeho Lee*

Main category: cs.CL

TL;DR: 研究大语言模型中选择题回答提示结构影响，发现CQO优于QOC，核心机制是因果注意力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型对提示结构敏感，但机制不明，具体研究选择题回答中提示顺序影响。

Method: 进行系统架构分析。

Result: 在多种模型和数据集上，选择题回答中CQO顺序比QOC顺序性能高超14个百分点，核心机制是QOC中因果掩码造成信息瓶颈。

Conclusion: 因果注意力是大语言模型对选择题提示结构敏感的核心机制。

Abstract: Large language models exhibit surprising sensitivity to the structure of the prompt, but the mechanisms underlying this sensitivity remain poorly understood. In this work, we conduct an in-depth investigation on a striking case: in multiple-choice question answering, placing context before the questions and options (CQO) outperforms the reverse order (QOC) by over 14%p, consistently over a wide range of models and datasets. Through systematic architectural analysis, we identify causal attention as the core mechanism: in QOC prompts, the causal mask prevents option tokens from attending to context, creating an information bottleneck where context becomes invisible to options.

</details>


### [555] [APEX-Agents](https://arxiv.org/abs/2601.14242)
*Bertie Vidgen,Austin Mann,Abby Fennelly,John Wright Stanly,Lucas Rothman,Marco Burstein,Julien Benchek,David Ostrofsky,Anirudh Ravichandran,Debnil Sur,Neel Venugopal,Alannah Hsia,Isaac Robinson,Calix Huang,Olivia Varones,Daniyal Khan,Michael Haines,Zach Richards,Chirag Mahapatra,Brendan Foody,Osvald Nitski*

Main category: cs.CL

TL;DR: 提出APEX - Agents基准评估AI智能体执行长周期跨应用任务的能力，测试八个智能体，展现排名并开源基准和评估基础设施。


<details>
  <summary>Details</summary>
Motivation: 评估AI智能体能否执行投资银行分析师、管理顾问和企业律师创建的长周期、跨应用任务。

Method: 使用APEX - Agents基准，要求智能体在含文件和工具的现实工作环境中操作，用Pass@1测试八个智能体。

Result: Gemini 3 Flash (Thinking=High)得分最高为24.0%，其次是GPT - 5.2 (Thinking=High)等。

Conclusion: 开源APEX - Agents基准及所有相关数据和评估基础设施Archipelago。

Abstract: We introduce the AI Productivity Index for Agents (APEX-Agents), a benchmark for assessing whether AI agents can execute long-horizon, cross-application tasks created by investment banking analysts, management consultants, and corporate lawyers. APEX-Agents requires agents to navigate realistic work environments with files and tools. We test eight agents for the leaderboard using Pass@1. Gemini 3 Flash (Thinking=High) achieves the highest score of 24.0%, followed by GPT-5.2 (Thinking=High), Claude Opus 4.5 (Thinking=High), and Gemini 3 Pro (Thinking=High). We open source the APEX-Agents benchmark (n=480) with all prompts, rubrics, gold outputs, files, and metadata. We also open-source Archipelago, our infrastructure for agent execution and evaluation.

</details>


### [556] [Confident Rankings with Fewer Items: Adaptive LLM Evaluation with Continuous Scores](https://arxiv.org/abs/2601.13885)
*Esma Balkır,Alice Pernthaller,Marco Basaldella,José Hernández-Orallo,Nigel Collier*

Main category: cs.CL

TL;DR: 提出将基于IRT的自适应测试扩展到连续有界分数的方法，引入带自适应停止标准的不确定性感知排名器，在多个基准测试验证效果好。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型评估越来越依赖生成任务，现有基于选择题的计算机化自适应测试（CAT）方法不适用。

Method: 用异方差正态分布代替伯努利响应分布，将基于IRT的自适应测试扩展到连续有界分数；引入带自适应停止标准的不确定性感知排名器。

Result: 在五个基准测试验证，使用2%的项目，排名相关性比随机抽样提高0.12 τ ，置信预测准确率达95%。

Conclusion: 所提方法能在尽可能少测试项目和低成本情况下实现可靠的模型排名。

Abstract: Computerized Adaptive Testing (CAT) has proven effective for efficient LLM evaluation on multiple-choice benchmarks, but modern LLM evaluation increasingly relies on generation tasks where outputs are scored continuously rather than marked correct/incorrect. We present a principled extension of IRT-based adaptive testing to continuous bounded scores (ROUGE, BLEU, LLM-as-a-Judge) by replacing the Bernoulli response distribution with a heteroskedastic normal distribution. Building on this, we introduce an uncertainty aware ranker with adaptive stopping criteria that achieves reliable model ranking while testing as few items and as cheaply as possible. We validate our method on five benchmarks spanning n-gram-based, embedding-based, and LLM-as-judge metrics. Our method uses 2% of the items while improving ranking correlation by 0.12 τ over random sampling, with 95% accuracy on confident predictions.

</details>


### [557] ["The Whole Is Greater Than the Sum of Its Parts": A Compatibility-Aware Multi-Teacher CoT Distillation Framework](https://arxiv.org/abs/2601.13992)
*Jin Cui,Jiaqi Guo,Jiepeng Zhou,Ruixuan Yang,Jiayi Lu,Jiajun Xu,Jiangcheng Song,Boran Zhao,Pengju Ren*

Main category: cs.CL

TL;DR: 提出COMPACT框架解决CoT蒸馏中多教师监督融合问题，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 现有CoT蒸馏方法多依赖单一教师，限制学生模型潜力，多教师监督融合存在挑战。

Method: 引入COMPACT框架，通过图共识、互信息适应性和损失难度三个维度动态加权教师梯度，自适应融合不同教师监督。

Result: 广泛实验和潜空间分析表明，COMPACT有效整合多种推理能力，不破坏模型原有知识结构，在多个基准测试中达到最优性能，减轻灾难性遗忘。

Conclusion: COMPACT框架能有效解决多教师监督融合问题，提升学生模型推理能力。

Abstract: Chain-of-Thought (CoT) reasoning empowers Large Language Models (LLMs) with remarkable capabilities but typically requires prohibitive parameter scales. CoT distillation has emerged as a promising paradigm to transfer reasoning prowess into compact Student Models (SLMs), but existing approaches often rely on a solitary teacher, capping the student's potential since individual LLMs often exhibit distinct capability biases and may suffer from catastrophic forgetting. While leveraging diverse teachers seems appealing, effectively fusing their supervisions remains challenging: teacher-student incompatibility risks amplifying hallucinations, and passive supervision fails to ensure genuine logic internalization. To address this, we introduce COMPACT, a framework that adaptively fuses supervisions from different teachers by dynamically weighting teacher gradients based on the student's real-time compatibility evaluated by a multi-dimensional metric: (1) Graph-based Consensus to filter misleading rationales by identifying mainstream reasoning paths; (2) Mutual-Information-based Adaptability to detect "epiphany moments" for genuinely understanding the reasoning process rather than merely imitating; and (3) Loss-based Difficulty to assess student receptivity to the teacher's guidance and prevent negative transfer. Extensive experiments and latent space analysis demonstrate that COMPACT effectively integrates diverse reasoning capabilities without damaging the model's original knowledge structure, achieving state-of-the-art performance on various benchmarks while mitigating catastrophic forgetting.

</details>


### [558] [Top 10 Open Challenges Steering the Future of Diffusion Language Model and Its Variants](https://arxiv.org/abs/2601.14041)
*Yunhe Wang,Kai Han,Huiling Zhen,Yuchuan Tian,Hanting Chen,Yongbing Huang,Yufei Cui,Yingte Shu,Shan Gao,Ismail Elezi,Roy Vaughan Miles,Songcen Xu,Feng Wen,Chao Xu,Sinan Zeng,Dacheng Tao*

Main category: cs.CL

TL;DR: 本文针对扩散语言模型（DLMs）展开探讨，指出其受限问题，提出应对挑战的战略路线图，强调转向扩散原生生态的重要性。


<details>
  <summary>Details</summary>
Motivation: 自回归（AR）模型存在因果瓶颈，扩散语言模型虽有潜力但未充分发挥，为挖掘其潜力以开发下一代人工智能。

Method: 识别阻碍DLMs发展的十大挑战，提出由基础架构、算法优化、认知推理和统一多模态智能四大支柱构成的战略路线图，倡导转向扩散原生生态。

Result: 未提及具体实验或实际应用中的结果，仅从理论层面分析并提出方案。

Conclusion: 转向扩散原生生态以突破因果界限对于开发具备复杂结构推理、动态自我修正和无缝多模态集成能力的下一代人工智能至关重要。

Abstract: The paradigm of Large Language Models (LLMs) is currently defined by auto-regressive (AR) architectures, which generate text through a sequential ``brick-by-brick'' process. Despite their success, AR models are inherently constrained by a causal bottleneck that limits global structural foresight and iterative refinement. Diffusion Language Models (DLMs) offer a transformative alternative, conceptualizing text generation as a holistic, bidirectional denoising process akin to a sculptor refining a masterpiece. However, the potential of DLMs remains largely untapped as they are frequently confined within AR-legacy infrastructures and optimization frameworks. In this Perspective, we identify ten fundamental challenges ranging from architectural inertia and gradient sparsity to the limitations of linear reasoning that prevent DLMs from reaching their ``GPT-4 moment''. We propose a strategic roadmap organized into four pillars: foundational infrastructure, algorithmic optimization, cognitive reasoning, and unified multimodal intelligence. By shifting toward a diffusion-native ecosystem characterized by multi-scale tokenization, active remasking, and latent thinking, we can move beyond the constraints of the causal horizon. We argue that this transition is essential for developing next-generation AI capable of complex structural reasoning, dynamic self-correction, and seamless multimodal integration.

</details>


### [559] [XCR-Bench: A Multi-Task Benchmark for Evaluating Cultural Reasoning in LLMs](https://arxiv.org/abs/2601.14063)
*Mohsinul Kabir,Tasnim Ahmed,Md Mezbaur Rahman,Shaoxiong Ji,Hassan Alhuzali,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 提出跨文化推理基准XCR - Bench评估大语言模型跨文化能力，发现模型在识别和适应特定文化项有弱点及存在偏见，并开源数据和代码。


<details>
  <summary>Details</summary>
Motivation: 现有评估大语言模型跨文化能力受高质量带注释的跨文化平行句子语料库稀缺的限制。

Method: 引入包含4.9k平行句子和1098个独特文化特定项的XCR - Bench基准，结合Newmark的文化特定项框架和Hall的文化三元组进行分析。

Result: 最先进的大语言模型在识别和适应与社交礼仪和文化参考相关的文化特定项上有持续弱点，且在文化适应中存在区域和民族宗教偏见。

Conclusion: 发布语料库和代码以促进未来跨文化自然语言处理研究。

Abstract: Cross-cultural competence in large language models (LLMs) requires the ability to identify Culture-Specific Items (CSIs) and to adapt them appropriately across cultural contexts. Progress in evaluating this capability has been constrained by the scarcity of high-quality CSI-annotated corpora with parallel cross-cultural sentence pairs. To address this limitation, we introduce XCR-Bench, a Cross(X)-Cultural Reasoning Benchmark consisting of 4.9k parallel sentences and 1,098 unique CSIs, spanning three distinct reasoning tasks with corresponding evaluation metrics. Our corpus integrates Newmark's CSI framework with Hall's Triad of Culture, enabling systematic analysis of cultural reasoning beyond surface-level artifacts and into semi-visible and invisible cultural elements such as social norms, beliefs, and values. Our findings show that state-of-the-art LLMs exhibit consistent weaknesses in identifying and adapting CSIs related to social etiquette and cultural reference. Additionally, we find evidence that LLMs encode regional and ethno-religious biases even within a single linguistic setting during cultural adaptation. We release our corpus and code to facilitate future research on cross-cultural NLP.

</details>


### [560] [Style Transfer as Bias Mitigation: Diffusion Models for Synthetic Mental Health Text for Arabic](https://arxiv.org/abs/2601.14124)
*Saad Mankarious,Aya Zirikly*

Main category: cs.CL

TL;DR: 提出无预训练扩散式方法生成合成文本，缓解心理健康分析中的性别偏差，且效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据方法多依赖预训练大语言模型，存在输出多样性有限和传递训练数据偏差的问题，需新方法缓解心理健康分析中的数据稀缺和人口统计学偏差。

Method: 将偏差缓解作为风格迁移问题，使用CARMA阿拉伯心理健康语料库进行男性到女性的风格迁移，构建五个数据集，为每个设置训练单独的扩散模型。

Result: 定量评估显示源文本和生成文本语义保真度高且有表面风格差异，定性分析证实性别转换语言合理。

Conclusion: 基于扩散的风格迁移可生成高熵、语义忠实的合成数据，无需预训练大模型，能为缓解低资源心理健康领域的性别偏差提供有效灵活框架。

Abstract: Synthetic data offers a promising solution for mitigating data scarcity and demographic bias in mental health analysis, yet existing approaches largely rely on pretrained large language models (LLMs), which may suffer from limited output diversity and propagate biases inherited from their training data. In this work, we propose a pretraining-free diffusion-based approach for synthetic text generation that frames bias mitigation as a style transfer problem. Using the CARMA Arabic mental health corpus, which exhibits a substantial gender imbalance, we focus on male-to-female style transfer to augment underrepresented female-authored content. We construct five datasets capturing varying linguistic and semantic aspects of gender expression in Arabic and train separate diffusion models for each setting. Quantitative evaluations demonstrate consistently high semantic fidelity between source and generated text, alongside meaningful surface-level stylistic divergence, while qualitative analysis confirms linguistically plausible gender transformations. Our results show that diffusion-based style transfer can generate high-entropy, semantically faithful synthetic data without reliance on pretrained LLMs, providing an effective and flexible framework for mitigating gender bias in sensitive, low-resource mental health domains.

</details>


### [561] [Domain-Adaptation through Synthetic Data: Fine-Tuning Large Language Models for German Law](https://arxiv.org/abs/2601.14160)
*Ali Hamza Bashir,Muhammad Rehan Khalid,Kostadin Cvejoski,Jana Birr,Jule Berghaus,Armin Berger,Sandra Halscheidt,Christian Temath,Rafet Sifa,David Berghaus*

Main category: cs.CL

TL;DR: 本文提出新颖合成数据生成法使大语言模型适应德国法律问答，经调优的模型表现更好，证明合成数据可替代人工标注。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在法律推理等专业领域存在问题，因专业知识有限产生错误输出或幻觉，需使其适应德国法律问答。

Method: 从德国权威法规中系统生成高质量、多样化且法律准确的问答对，使用严格自动过滤方法和参数高效微调技术。

Result: 用合成数据集调整后的大语言模型在德国法律问答任务中显著优于基线模型。

Conclusion: 在高风险、知识密集型领域，精心设计的合成数据可作为人工标注的可靠替代方案。

Abstract: Large language models (LLMs) often struggle in specialized domains such as legal reasoning due to limited expert knowledge, resulting in factually incorrect outputs or hallucinations. This paper presents an effective method for adapting advanced LLMs to German legal question answering through a novel synthetic data generation approach. In contrast to costly human-annotated resources or unreliable synthetic alternatives, our approach systematically produces high-quality, diverse, and legally accurate question-answer pairs directly from authoritative German statutes. Using rigorous automated filtering methods and parameter-efficient fine-tuning techniques, we demonstrate that LLMs adapted with our synthetic dataset significantly outperform their baseline counterparts on German legal question answering tasks. Our results highlight the feasibility of using carefully designed synthetic data as a robust alternative to manual annotation in high-stakes, knowledge-intensive domains.

</details>


### [562] [Human Values in a Single Sentence: Moral Presence, Hierarchies, and Transformer Ensembles on the Schwartz Continuum](https://arxiv.org/abs/2601.14172)
*Víctor Yeste,Paolo Rosso*

Main category: cs.CL

TL;DR: 研究文本中句子级别的人类价值检测，多种模型对比，轻量级信号和小集成效果好，有计算约束时调优监督编码器为有力基线。


<details>
  <summary>Details</summary>
Motivation: 在新闻和政治宣言的无上下文句子中，特征稀疏且类别不平衡，使细粒度句子级价值检测困难，需研究有效方法。

Method: 先开展二元道德存在任务，对比基于DeBERTa - base的存在门控层次结构和直接多标签分类器，对指令调优大模型进行零/少样本和QLoRA设置实验并构建集成。

Result: 门控层次结构未优于直接预测；软投票监督集成宏观F1达0.332，超最佳单监督模型和先前英语基线。

Conclusion: 轻量级信号和小集成改进效果好，层次门控益处有限；在单GPU 8GB约束和7 - 9B规模下，调优监督编码器是有力且高效的基线，丰富价值结构和文档上下文可提升性能。

Abstract: We study sentence-level identification of the 19 values in the Schwartz motivational continuum as a concrete formulation of human value detection in text. The setting - out-of-context sentences from news and political manifestos - features sparse moral cues and severe class imbalance. This combination makes fine-grained sentence-level value detection intrinsically difficult, even for strong modern neural models. We first operationalize a binary moral presence task ("does any value appear?") and show that it is learnable from single sentences (positive-class F1 $\approx$ 0.74 with calibrated thresholds). We then compare a presence-gated hierarchy to a direct multi-label classifier under matched compute, both based on DeBERTa-base and augmented with lightweight signals (prior-sentence context, LIWC-22/eMFD/MJD lexica, and topic features). The hierarchy does not outperform direct prediction, indicating that gate recall limits downstream gains. We also benchmark instruction-tuned LLMs - Gemma 2 9B, Llama 3.1 8B, Mistral 8B, and Qwen 2.5 7B - in zero-/few-shot and QLoRA setups and build simple ensembles; a soft-vote supervised ensemble reaches macro-F1 0.332, significantly surpassing the best single supervised model and exceeding prior English-only baselines. Overall, in this scenario, lightweight signals and small ensembles yield the most reliable improvements, while hierarchical gating offers limited benefit. We argue that, under an 8 GB single-GPU constraint and at the 7-9B scale, carefully tuned supervised encoders remain a strong and compute-efficient baseline for structured human value detection, and we outline how richer value structure and sentence-in-document context could further improve performance.

</details>


### [563] [MASCOT: Towards Multi-Agent Socio-Collaborative Companion Systems](https://arxiv.org/abs/2601.14230)
*Yiyang Wang,Yiqiao Jin,Alex Cabral,Josiah Hester*

Main category: cs.CL

TL;DR: 本文提出多视角社会协作同伴框架MASCOT，解决多智能体系统存在的问题，评估显示其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统常出现人设崩塌和社交谄媚问题，需要改进。

Method: 引入双层优化策略，包括基于RLAIF的人设感知行为对齐和基于群体奖励的协作对话优化。

Result: 在心理支持和职场领域评估显示，MASCOT在人设一致性和社会贡献方面有显著提升。

Conclusion: 该框架为下一代社交智能多智能体系统工程提供了实用路线图。

Abstract: Multi-agent systems (MAS) have recently emerged as promising socio-collaborative companions for emotional and cognitive support. However, these systems frequently suffer from persona collapse--where agents revert to generic, homogenized assistant behaviors--and social sycophancy, which produces redundant, non-constructive dialogue. We propose MASCOT, a generalizable framework for multi-perspective socio-collaborative companions. MASCOT introduces a novel bi-level optimization strategy to harmonize individual and collective behaviors: 1) Persona-Aware Behavioral Alignment, an RLAIF-driven pipeline that finetunes individual agents for strict persona fidelity to prevent identity loss; and 2) Collaborative Dialogue Optimization, a meta-policy guided by group-level rewards to ensure diverse and productive discourse. Extensive evaluations across psychological support and workplace domains demonstrate that MASCOT significantly outperforms state-of-the-art baselines, achieving improvements of up to +14.1 in Persona Consistency and +10.6 in Social Contribution. Our framework provides a practical roadmap for engineering the next generation of socially intelligent multi-agent systems.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [564] [Joint Source-Channel-Generation Coding: From Distortion-oriented Reconstruction to Semantic-consistent Generation](https://arxiv.org/abs/2601.12808)
*Tong Wu,Zhiyong Chen,Guo Lu,Li Song,Feng Yang,Meixia Tao,Wenjun Zhang*

Main category: cs.IT

TL;DR: 提出联合源-信道-生成编码（JSCGC）范式，以概率生成代替确定性重建，推导通信在控制生成过程中的理论下限，实验显示其在图像传输上表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统通信系统依赖通用失真指标，无法捕捉复杂人类视觉感知，导致重建效果不佳。

Method: 提出JSCGC范式，在接收器使用生成模型作为生成器而非传统解码器来参数化数据分布，推导最大语义不一致性的理论下限。

Result: 在图像传输实验中，JSCGC大幅提升感知质量和语义保真度，显著优于传统以失真为导向的联合源信道编码方法。

Conclusion: JSCGC是一种有效的通信编码范式，能解决传统方法在图像传输中的不足。

Abstract: Conventional communication systems, including both separation-based coding and AI-driven joint source-channel coding (JSCC), are largely guided by Shannon's rate-distortion theory. However, relying on generic distortion metrics fails to capture complex human visual perception, often resulting in blurred or unrealistic reconstructions. In this paper, we propose Joint Source-Channel-Generation Coding (JSCGC), a novel paradigm that shifts the focus from deterministic reconstruction to probabilistic generation. JSCGC leverages a generative model at the receiver as a generator rather than a conventional decoder to parameterize the data distribution, enabling direct maximization of mutual information under channel constraints while controlling stochastic sampling to produce outputs residing on the authentic data manifold with high fidelity. We further derive a theoretical lower bound on the maximum semantic inconsistency with given transmitted mutual information, elucidating the fundamental limits of communication in controlling the generative process. Extensive experiments on image transmission demonstrate that JSCGC substantially improves perceptual quality and semantic fidelity, significantly outperforming conventional distortion-oriented JSCC methods.

</details>


### [565] [An Elementary Approach to Scheduling in Generative Diffusion Models](https://arxiv.org/abs/2601.13602)
*Qiang Sun,H. Vincent Poor,Wenyi Zhang*

Main category: cs.IT

TL;DR: 本文提出一种基础方法研究生成扩散模型中噪声调度和时间离散化的影响，推导分布轨迹与KL散度，研究离散化步数影响并优化噪声调度，实验表明所选时间离散化策略表现更优。


<details>
  <summary>Details</summary>
Motivation: 刻画生成扩散模型中噪声调度和时间离散化的影响。

Method: 考虑简化模型推导分布轨迹和KL散度，用Euler - Maclaurin展开研究离散化步数影响，通过变分法求解优化问题得到噪声调度，并利用KL散度比较不同时间离散化策略。

Result: 得到的噪声调度遵循正切定律，系数由源协方差矩阵特征值决定；实验显示该方法选择的时间离散化策略优于基线和基于搜索的策略，尤其是函数评估次数预算紧张时。

Conclusion: 所提出的方法能有效研究噪声调度和时间离散化影响，所选策略有更好的性能。

Abstract: An elementary approach to characterizing the impact of noise scheduling and time discretization in generative diffusion models is developed. Considering a simplified model where the source distribution is multivariate Gaussian with a given covariance matrix, the explicit closed-form evolution trajectory of the distributions across reverse sampling steps is derived, and consequently, the Kullback-Leibler (KL) divergence between the source distribution and the reverse sampling output is obtained. The effect of the number of time discretization steps on the convergence of this KL divergence is studied via the Euler-Maclaurin expansion. An optimization problem is formulated, and its solution noise schedule is obtained via calculus of variations, shown to follow a tangent law whose coefficient is determined by the eigenvalues of the source covariance matrix. For an alternative scenario, more realistic in practice, where pretrained models have been obtained for some given noise schedules, the KL divergence also provides a measure to compare different time discretization strategies in reverse sampling. Experiments across different datasets and pretrained models demonstrate that the time discretization strategy selected by our approach consistently outperforms baseline and search-based strategies, particularly when the budget on the number of function evaluations is very tight.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [566] [Stability and Accuracy Trade-offs in Statistical Estimation](https://arxiv.org/abs/2601.11701)
*Abhinav Chakraborty,Yuetian Luo,Rina Foygel Barber*

Main category: math.ST

TL;DR: 本文从统计决策理论角度，研究算法稳定性的统计代价，给出不同稳定性约束下估计精度的下界，开发最优稳定估计器，刻画稳定性与准确性间的权衡。


<details>
  <summary>Details</summary>
Motivation: 算法稳定性虽有期望属性，但对统计学习通常不充分，且可能与准确性冲突，因此需了解稳定性的统计代价。

Method: 采用统计决策理论视角，将稳定性作为估计约束，聚焦最坏情况稳定性和平均情况稳定性，建立估计精度下界，开发最优稳定估计器。

Result: 得到不同稳定性约束下可实现的估计精度的下界，开发了四个典型估计问题的最优稳定估计器，刻画了稳定性和准确性之间的最优权衡。发现平均情况稳定性的限制比最坏情况稳定性更弱，两者差距因估计问题而异。

Conclusion: 明确了平均情况稳定性和最坏情况稳定性对估计问题限制程度的差异，揭示了不同估计问题中两者差距的变化情况。

Abstract: Algorithmic stability is a central concept in statistics and learning theory that measures how sensitive an algorithm's output is to small changes in the training data. Stability plays a crucial role in understanding generalization, robustness, and replicability, and a variety of stability notions have been proposed in different learning settings. However, while stability entails desirable properties, it is typically not sufficient on its own for statistical learning -- and indeed, it may be at odds with accuracy, since an algorithm that always outputs a constant function is perfectly stable but statistically meaningless. Thus, it is essential to understand the potential statistical cost of stability. In this work, we address this question by adopting a statistical decision-theoretic perspective, treating stability as a constraint in estimation. Focusing on two representative notions-worst-case stability and average-case stability-we first establish general lower bounds on the achievable estimation accuracy under each type of stability constraint. We then develop optimal stable estimators for four canonical estimation problems, including several mean estimation and regression settings. Together, these results characterize the optimal trade-offs between stability and accuracy across these tasks. Our findings formalize the intuition that average-case stability imposes a qualitatively weaker restriction than worst-case stability, and they further reveal that the gap between these two can vary substantially across different estimation problems.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [567] [A data-driven merit order: Learning a fundamental electricity price model](https://arxiv.org/abs/2501.02963)
*Paul Ghelasi,Florian Ziel*

Main category: stat.AP

TL;DR: 提出结合数据驱动和基础模型范式的新电价预测模型，应用于德国市场表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有电价预测方法一般分为数据驱动模型和基础模型两类，期望提出融合两种范式的高效模型。

Method: 提出新的数据驱动电价预测模型，可从历史数据估算关键参数，加入水电等扩展元素，并应用于德国日前市场。

Result: 该模型在德国日前列市场表现优于经典基础模型和最先进的机器学习模型。

Conclusion: 该模型保留了基础模型的可解释性，具有高计算效率，为电价建模提供了新方向。

Abstract: Electricity price forecasting approaches generally fall into two categories: data-driven models, which learn from historical patterns, or fundamental models, which simulate market mechanisms. We propose a novel and highly efficient data-driven merit order model that integrates both paradigms. The model embeds the classical expert-based merit order as a nested special case, allowing all key parameters, such as plant efficiencies, bidding behavior, and available capacities, to be estimated directly from historical data, rather than assumed. We further enhance the model with critical embedded extensions such as hydro power, cross-border flows and corrections for underreported capacities, which considerably improve forecasting accuracy. Applied to the German day-ahead market, our model outperforms both classic fundamental and state-of-the-art machine learning models. It retains the interpretability of fundamental models, offering insights into marginal technologies, fuel switches, and dispatch patterns, elements which are typically inaccessible to black-box machine learning approaches. This transparency and high computational efficiency make it a promising new direction for electricity price modeling.

</details>


### [568] [Adversarial Drift-Aware Predictive Transfer: Toward Durable Clinical AI](https://arxiv.org/abs/2601.11860)
*Xin Xiong,Zijian Guo,Haobo Zhu,Chuan Hong,Jordan W Smoller,Tianxi Cai,Molei Liu*

Main category: stat.AP

TL;DR: 提出ADAPT框架应对临床AI系统性能衰退问题，在自杀风险预测验证中表现出优越性，提供维持可靠AI的可扩展途径。


<details>
  <summary>Details</summary>
Motivation: 临床AI系统部署后因数据随时间变化性能衰退，频繁再训练不实际，需解决此‘老化’问题。

Method: 引入ADAPT框架，结合历史源模型和少量当前数据构建未来模型不确定性集，优化最坏情况性能。

Result: 在自杀风险预测中，ADAPT在编码转换和疫情引起的变化中表现出卓越稳定性。

Conclusion: ADAPT无需标记或再训练未来数据，能最小化年度性能衰退，为高风险医疗环境维持可靠AI提供可扩展途径。

Abstract: Clinical AI systems frequently suffer performance decay post-deployment due to temporal data shifts, such as evolving populations, diagnostic coding updates (e.g., ICD-9 to ICD-10), and systemic shocks like the COVID-19 pandemic. Addressing this ``aging'' effect via frequent retraining is often impractical due to computational costs and privacy constraints. To overcome these hurdles, we introduce Adversarial Drift-Aware Predictive Transfer (ADAPT), a novel framework designed to confer durability against temporal drift with minimal retraining. ADAPT innovatively constructs an uncertainty set of plausible future models by combining historical source models and limited current data. By optimizing worst-case performance over this set, it balances current accuracy with robustness against degradation due to future drifts. Crucially, ADAPT requires only summary-level model estimators from historical periods, preserving data privacy and ensuring operational simplicity. Validated on longitudinal suicide risk prediction using electronic health records from Mass General Brigham (2005--2021) and Duke University Health Systems, ADAPT demonstrated superior stability across coding transitions and pandemic-induced shifts. By minimizing annual performance decay without labeling or retraining future data, ADAPT offers a scalable pathway for sustaining reliable AI in high-stakes healthcare environments.

</details>


### [569] [Improving Geopolitical Forecasts with Bayesian Networks](https://arxiv.org/abs/2601.13362)
*Matthew Martin*

Main category: stat.AP

TL;DR: 本文利用Good Judgment Project数据，对比贝叶斯网络、逻辑回归及校准聚合方法的预测准确性，发现校准聚合准确性最高，建议未来探索混合方法等。


<details>
  <summary>Details</summary>
Motivation: 探索贝叶斯网络相比逻辑回归、校准和聚合方法在提高预测准确性方面的作用。

Method: 将正则化逻辑回归模型、基线校准聚合与两种贝叶斯网络（结构学习的贝叶斯网络和朴素贝叶斯网络）进行比较，同时考察四个预测变量。

Result: 校准聚合准确性最高（AUC = 0.985），其次是两种贝叶斯网络，然后是逻辑回归模型。

Conclusion: 未来研究应探索贝叶斯网络与逻辑回归的混合方法、考察更多预测变量并考虑分层数据依赖。

Abstract: This study explores how Bayesian networks (BNs) can improve forecast accuracy compared to logistic regression and recalibration and aggregation methods, using data from the Good Judgment Project. Regularized logistic regression models and a baseline recalibrated aggregate were compared to two types of BNs: structure-learned BNs with arcs between predictors, and naive BNs. Four predictor variables were examined: absolute difference from the aggregate, forecast value, days prior to question close, and mean standardized Brier score. Results indicated the recalibrated aggregate achieved the highest accuracy (AUC = 0.985), followed by both types of BNs, then the logistic regression models. Performance of the BNs was likely harmed by reduced information from the discretization process and violation of the assumption of linearity likely harmed the logistic regression models. Future research should explore hybrid approaches combining BNs with logistic regression, examine additional predictor variables, and account for hierarchical data dependencies.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [570] [Scaling laws for amplitude surrogates](https://arxiv.org/abs/2601.13308)
*Henning Bahl,Victor Bresó-Pla,Anja Butter,Joaquín Iturriza Ramirez*

Main category: hep-ph

TL;DR: 研究粒子物理振幅替代中的缩放定律，表明系数与外部粒子数有关，定律有助于实现精度目标。


<details>
  <summary>Details</summary>
Motivation: 系统研究粒子物理振幅替代中的缩放定律。

Method: 对粒子物理振幅替代中的缩放定律进行系统研究。

Result: 发现缩放系数与过程的外部粒子数量有关。

Conclusion: 缩放定律是实现所需精度目标的有用工具。

Abstract: Scaling laws describing the dependence of neural network performance on the amount of training data, the spent compute, and the network size have emerged across a huge variety of machine learning task and datasets. In this work, we systematically investigate these scaling laws in the context of amplitude surrogates for particle physics. We show that the scaling coefficients are connected to the number of external particles of the process. Our results demonstrate that scaling laws are a useful tool to achieve desired precision targets.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [571] [Gradient Flow for Finding E-optimal Designs](https://arxiv.org/abs/2601.14147)
*Jieling Shi,Kim-Chuan Toh,Xin T. Tong,Weng Kee Wong*

Main category: math.OC

TL;DR: 本文研究用Wasserstein梯度流寻找回归模型的E - 最优设计，推导E - 最优准则的Wasserstein梯度公式，提出上升方向，开发粒子近似方法，实验证明该方法在精度和可扩展性上有竞争力。


<details>
  <summary>Details</summary>
Motivation: E - 准则在统计应用中有诸多极大极小设计问题，存在独特的理论和计算挑战，需要新方法研究。

Method: 基于2 - Wasserstein空间的微分结构推导简单特征值情况下E - 最优准则的Wasserstein梯度公式；针对高重数情况提出Wasserstein最陡上升方向并通过半定规划放松精确计算；开发粒子近似方法，通过投影Wasserstein梯度流扩展到约束设计。

Result: 数值实验表明，所提方法能成功恢复线性和非线性回归模型的E - 最优设计，在准确性和可扩展性上与现有启发式方法相比有竞争力。

Conclusion: 基于最优传输的动力学方法可作为研究具有挑战性的最优设计问题的统一工具。

Abstract: We investigate the use of Wasserstein gradient flows for finding an $E$-optimal design for a regression model. Unlike the commonly used $D$- and $L$-optimality criteria, the $E$-criterion finds a design that maximizes the smallest eigenvalue of the information matrix, and so it is a non-differentiable criterion unless the minimum eigenvalue has geometric multiplicity equals to one. Such maximin design problems abound in statistical applications and present unique theoretical and computational challenges. Building on the differential structure of the $2$-Wasserstein space, we derive explicit formulas for the Wasserstein gradient of the $E$-optimality criterion in the simple-eigenvalue case. For higher multiplicities, we propose a Wasserstein steepest ascent direction and show that it can be computed exactly via a semidefinite programming (SDP) relaxation. We develop particle approximations that connect infinite-dimensional flows with finite-dimensional optimization, and provide approximation guarantees for empirical measures. Our framework extends naturally to constrained designs via projected Wasserstein gradient flows. Numerical experiments demonstrate that the proposed methods successfully recover $E$-optimal designs for both linear and nonlinear regression models, with competitive accuracy and scalability compared to existing heuristic approaches. This work highlights the potential of optimal transport-based dynamics as a unifying tool for studying challenging optimal design problems.

</details>


### [572] [Offline Policy Learning with Weight Clipping and Heaviside Composite Optimization](https://arxiv.org/abs/2601.12117)
*Jingren Liu,Hanzhang Qin,Junyi Liu,Mabel C. Chou,Jong-Shi Pang*

Main category: math.OC

TL;DR: 本文提出基于权重裁剪估计器的离线策略学习算法，解决重加权方法高方差问题，提高策略学习性能。


<details>
  <summary>Details</summary>
Motivation: 标准估计 - 优化框架中重加权方法在倾向得分小时存在高方差问题，误导策略优化。

Method: 开发基于权重裁剪估计器的算法，将问题重构成Heaviside复合优化问题，用渐进整数规划方法求解。

Result: 建立算法次优性上界，揭示权重裁剪估计器降低均方误差可提升策略学习性能。

Conclusion: 所提算法能有效解决重加权方法的高方差问题，提高策略学习性能。

Abstract: Offline policy learning aims to use historical data to learn an optimal personalized decision rule. In the standard estimate-then-optimize framework, reweighting-based methods (e.g., inverse propensity weighting or doubly robust estimators) are widely used to produce unbiased estimates of policy values. However, when the propensity scores of some treatments are small, these reweighting-based methods suffer from high variance in policy value estimation, which may mislead the downstream policy optimization and yield a learned policy with inferior value. In this paper, we systematically develop an offline policy learning algorithm based on a weight-clipping estimator that truncates small propensity scores via a clipping threshold chosen to minimize the mean squared error (MSE) in policy value estimation. Focusing on linear policies, we address the bilevel and discontinuous objective induced by weight-clipping-based policy optimization by reformulating the problem as a Heaviside composite optimization problem, which provides a rigorous computational framework. The reformulated policy optimization problem is then solved efficiently using the progressive integer programming method, making practical policy learning tractable. We establish an upper bound for the suboptimality of the proposed algorithm, which reveals how the reduction in MSE of policy value estimation, enabled by our proposed weight-clipping estimator, leads to improved policy learning performance.

</details>


### [573] [BiCoLoR: Communication-Efficient Optimization with Bidirectional Compression and Local Training](https://arxiv.org/abs/2601.12400)
*Laurent Condat,Artavazd Maranjyan,Peter Richtárik*

Main category: math.OC

TL;DR: 介绍通信高效优化算法BiCoLoR，结合本地训练和双向压缩，在凸和强凸异质环境中表现好且优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 分布式优化中通信慢且成本高是瓶颈，尤其是联邦学习在无线网络中，以往压缩仅用于上行通信，未处理下行。

Method: 提出算法BiCoLoR，结合本地训练和使用任意无偏压缩器的双向压缩。

Result: 在凸和强凸异质环境中获得加速复杂度保证，实证上优于现有算法。

Conclusion: BiCoLoR建立了通信效率新标准。

Abstract: Slow and costly communication is often the main bottleneck in distributed optimization, especially in federated learning where it occurs over wireless networks. We introduce BiCoLoR, a communication-efficient optimization algorithm that combines two widely used and effective strategies: local training, which increases computation between communication rounds, and compression, which encodes high-dimensional vectors into short bitstreams. While these mechanisms have been combined before, compression has typically been applied only to uplink (client-to-server) communication, leaving the downlink (server-to-client) side unaddressed. In practice, however, both directions are costly. We propose BiCoLoR, the first algorithm to combine local training with bidirectional compression using arbitrary unbiased compressors. This joint design achieves accelerated complexity guarantees in both convex and strongly convex heterogeneous settings. Empirically, BiCoLoR outperforms existing algorithms and establishes a new standard in communication efficiency.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [574] [Robust Verification of Concurrent Stochastic Games](https://arxiv.org/abs/2601.12003)
*Angel Y. He,David Parker*

Main category: cs.LO

TL;DR: 引入鲁棒并发随机博弈（CSGs）及其子类区间 CSGs（ICSGs），提出鲁棒验证框架，在 PRISM - games 实现并通过基准测试验证可行性。


<details>
  <summary>Details</summary>
Motivation: 传统 CSGs 模型要求精确指定转移概率，在现实中不切实际，需考虑转移概率的认知不确定性。

Method: 提出鲁棒验证框架，为零和与非零和场景的有限和无限期目标发展理论基础与高效算法，基于纳什均衡。

Result: 在 PRISM - games 模型检查器中实现，通过大量基准测试证明 ICSGs 鲁棒验证的可行性。

Conclusion: 所提出的鲁棒 CSGs 和 ICSGs 以及鲁棒验证框架可处理 CSGs 中转移概率的不确定性，且具有可行性。

Abstract: Autonomous systems often operate in multi-agent settings and need to make concurrent, strategic decisions, typically in uncertain environments. Verification and control problems for these systems can be tackled with concurrent stochastic games (CSGs), but this model requires transition probabilities to be precisely specified - an unrealistic requirement in many real-world settings. We introduce *robust CSGs* and their subclass *interval CSGs* (ICSGs), which capture epistemic uncertainty about transition probabilities in CSGs. We propose a novel framework for *robust* verification of these models under worst-case assumptions about transition uncertainty. Specifically, we develop the underlying theoretical foundations and efficient algorithms, for finite- and infinite-horizon objectives in both zero-sum and nonzero-sum settings, the latter based on (social-welfare optimal) Nash equilibria. We build an implementation in the PRISM-games model checker and demonstrate the feasibility of robust verification of ICSGs across a selection of large benchmarks.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [575] [Classifiers in High Dimensional Hilbert Metrics](https://arxiv.org/abs/2601.13410)
*Aditya Acharya,Auguste H. Gezalyan,David M. Mount*

Main category: cs.CG

TL;DR: 本文研究d维希尔伯特多边形度量下的点分类问题，提出高效算法，改进了先前工作。


<details>
  <summary>Details</summary>
Motivation: 解决高维空间点分类这一机器学习中的基本几何问题，希尔伯特度量在机器学习和凸几何有广泛应用。

Method: 提出基于线性规划（LP）的算法解决大间隔支持向量机（SVM）问题，还考虑了Funk度量，给出软间隔SVM问题和基于最近邻分类的高效算法。

Result: 算法运行时间与点数、边界面数和维度呈多项式关系，相比之前工作有显著改进。

Conclusion: 提出的算法在希尔伯特度量下的点分类问题上表现良好，改进了之前工作在运行时间上的不足。

Abstract: Classifying points in high dimensional spaces is a fundamental geometric problem in machine learning. In this paper, we address classifying points in the $d$-dimensional Hilbert polygonal metric. The Hilbert metric is a generalization of the Cayley-Klein hyperbolic distance to arbitrary convex bodies and has a diverse range of applications in machine learning and convex geometry. We first present an efficient LP-based algorithm in the metric for the large-margin SVM problem. Our algorithm runs in time polynomial to the number of points, bounding facets, and dimension. This is a significant improvement on previous works, which either provide no theoretical guarantees on running time, or suffer from exponential runtime. We also consider the closely related Funk metric. We also present efficient algorithms for the soft-margin SVM problem and for nearest neighbor-based classification in the Hilbert metric.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [576] [Cognition spaces: natural, artificial, and hybrid](https://arxiv.org/abs/2601.12837)
*Ricard Solé,Luis F Seoane,Jordi Pla-Mauri,Michael Timothy Bennett,Michael E. Hochberg,Michael Levin*

Main category: q-bio.NC

TL;DR: 提出认知空间方法统一比较各类认知系统，介绍三种认知空间，指出其结构特点及研究意义。


<details>
  <summary>Details</summary>
Motivation: 缺乏统一框架对比自然、人工和混合系统的认知形式、限制和可能性。

Method: 提出基于组织和信息维度的认知空间方法，将认知视为处理信息能力，分析多种系统。

Result: 引入三种认知空间，发现其占用不均衡，存在大量未占用区域，这些区域反映多种限制。

Conclusion: 该方法阐明现有认知系统多样性，强调混合认知是探索新复杂性的前沿。

Abstract: Cognitive processes are realized across an extraordinary range of natural, artificial, and hybrid systems, yet there is no unified framework for comparing their forms, limits, and unrealized possibilities. Here, we propose a cognition space approach that replaces narrow, substrate-dependent definitions with a comparative representation based on organizational and informational dimensions. Within this framework, cognition is treated as a graded capacity to sense, process, and act upon information, allowing systems as diverse as cells, brains, artificial agents, and human-AI collectives to be analyzed within a common conceptual landscape. We introduce and examine three cognition spaces -- basal aneural, neural, and human-AI hybrid -- and show that their occupation is highly uneven, with clusters of realized systems separated by large unoccupied regions. We argue that these voids are not accidental but reflect evolutionary contingencies, physical constraints, and design limitations. By focusing on the structure of cognition spaces rather than on categorical definitions, this approach clarifies the diversity of existing cognitive systems and highlights hybrid cognition as a promising frontier for exploring novel forms of complexity beyond those produced by biological evolution.

</details>


### [577] [Global stability of a Hebbian/anti-Hebbian network for principal subspace learning](https://arxiv.org/abs/2601.13170)
*David Lipshutz,Robert J. Lipshutz*

Main category: q-bio.NC

TL;DR: 证明自组织神经网络突触动力学连续极限的全局稳定性，指出其分两个阶段演化。


<details>
  <summary>Details</summary>
Motivation: 生物神经网络通过局部突触修改自组织产生稳定计算，但突触层面修改如何产生网络层面计算仍未知，且之前模型的非线性突触动力学全局稳定性未确立。

Method: 针对前馈和循环权重在相同时间尺度上演化的情况进行分析证明。

Result: 突触动力学连续极限具有全局稳定性，且分为两个阶段演化，第一阶段突触权重收敛到“神经滤波器”正交归一的不变流形，第二阶段突触动力学遵循非凸势函数的梯度流，其最小值对应输入数据主特征子空间的神经滤波器。

Conclusion: 确立了特定情况下自组织神经网络突触动力学连续极限的全局稳定性及演化特性。

Abstract: Biological neural networks self-organize according to local synaptic modifications to produce stable computations. How modifications at the synaptic level give rise to such computations at the network level remains an open question. Pehlevan et al. [Neur. Comp. 27 (2015), 1461--1495] proposed a model of a self-organizing neural network with Hebbian and anti-Hebbian synaptic updates that implements an algorithm for principal subspace analysis; however, global stability of the nonlinear synaptic dynamics has not been established. Here, for the case that the feedforward and recurrent weights evolve at the same timescale, we prove global stability of the continuum limit of the synaptic dynamics and show that the dynamics evolve in two phases. In the first phase, the synaptic weights converge to an invariant manifold where the `neural filters' are orthonormal. In the second phase, the synaptic dynamics follow the gradient flow of a non-convex potential function whose minima correspond to neural filters that span the principal subspace of the input data.

</details>


### [578] [A New Strategy for Artificial Intelligence: Training Foundation Models Directly on Human Brain Data](https://arxiv.org/abs/2601.12053)
*Maël Donoso*

Main category: q-bio.NC

TL;DR: 本文探索直接基于人类大脑数据训练基础模型，提出RLHB和CoTHB方法，探讨其影响、挑战与机遇。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型依赖人类产生的数据，存在局限性，探索用人类大脑数据训练模型以克服这些局限。

Method: 对基础模型的当前局限进行分类，提出RLHB和CoTHB方法来优先使用有限的神经成像数据进行基础模型训练。

Result: 未提及具体实验结果。

Conclusion: 基于大脑训练的基础模型可能是现有架构扩展和神经科学启发方案之间的现实有效折衷方案。

Abstract: While foundation models have achieved remarkable results across a diversity of domains, they still rely on human-generated data, such as text, as a fundamental source of knowledge. However, this data is ultimately the product of human brains, the filtered projection of a deeper neural complexity. In this paper, we explore a new strategy for artificial intelligence: moving beyond surface-level statistical regularities by training foundation models directly on human brain data. We hypothesize that neuroimaging data could open a window into elements of human cognition that are not accessible through observable actions, and argue that this additional knowledge could be used, alongside classical training data, to overcome some of the current limitations of foundation models. While previous research has demonstrated the possibility to train classical machine learning or deep learning models on neural patterns, this path remains largely unexplored for high-level cognitive functions. Here, we classify the current limitations of foundation models, as well as the promising brain regions and cognitive processes that could be leveraged to address them, along four levels: perception, valuation, execution, and integration. Then, we propose two methods that could be implemented to prioritize the use of limited neuroimaging data for strategically chosen, high-value steps in foundation model training: reinforcement learning from human brain (RLHB) and chain of thought from human brain (CoTHB). We also discuss the potential implications for agents, artificial general intelligence, and artificial superintelligence, as well as the ethical, social, and technical challenges and opportunities. We argue that brain-trained foundation models could represent a realistic and effective middle ground between continuing to scale current architectures and exploring alternative, neuroscience-inspired solutions.

</details>


### [579] [AI Agents Need Memory Control Over More Context](https://arxiv.org/abs/2601.11653)
*Fouad Bousetouane*

Main category: q-bio.NC

TL;DR: 本文介绍Agent Cognitive Compressor (ACC) 解决长多轮交互中AI智能体行为退化问题，评估显示其能实现有界内存和更稳定行为。


<details>
  <summary>Details</summary>
Motivation: 长多轮交互中AI智能体行为常因约束焦点丢失、错误积累和内存漂移而退化，现有持久内存方法有缺陷。

Method: 引入受生物启发的内存控制器ACC，用有界内部状态替代转录重放，分离工件召回和状态承诺；使用智能体 - 评判驱动的实时评估框架。

Result: 在多个场景中，ACC能保持有界内存，多轮行为更稳定，幻觉和漂移显著低于转录重放和基于检索的智能体。

Conclusion: 认知压缩为长周期AI智能体的可靠内存控制提供了实用有效的基础。

Abstract: AI agents are increasingly used in long, multi-turn workflows in both research and enterprise settings. As interactions grow, agent behavior often degrades due to loss of constraint focus, error accumulation, and memory-induced drift. This problem is especially visible in real-world deployments where context evolves, distractions are introduced, and decisions must remain consistent over time. A common practice is to equip agents with persistent memory through transcript replay or retrieval-based mechanisms. While convenient, these approaches introduce unbounded context growth and are vulnerable to noisy recall and memory poisoning, leading to unstable behavior and increased drift. In this work, we introduce the Agent Cognitive Compressor (ACC), a bio-inspired memory controller that replaces transcript replay with a bounded internal state updated online at each turn. ACC separates artifact recall from state commitment, enabling stable conditioning while preventing unverified content from becoming persistent memory. We evaluate ACC using an agent-judge-driven live evaluation framework that measures both task outcomes and memory-driven anomalies across extended interactions. Across scenarios spanning IT operations, cybersecurity response, and healthcare workflows, ACC consistently maintains bounded memory and exhibits more stable multi-turn behavior, with significantly lower hallucination and drift than transcript replay and retrieval-based agents. These results show that cognitive compression provides a practical and effective foundation for reliable memory control in long-horizon AI agents.

</details>


### [580] [Primate-like perceptual decision making emerges through deep recurrent reinforcement learning](https://arxiv.org/abs/2601.12577)
*Nathan J. Wispinski,Scott A. Stone,Anthony Singhal,Patrick M. Pilarski,Craig S. Chapman*

Main category: q-bio.NC

TL;DR: 使用强化学习训练深度循环神经网络验证灵长类决策机制形成理论，网络表现出灵长类决策能力，支持相关结论。


<details>
  <summary>Details</summary>
Motivation: 现有研究对灵长类决策机制为何存在了解较少，理论认为其是为了在有噪声、随时间变化的信息环境中最大化奖励而产生，作者想验证该理论。

Method: 使用强化学习在有噪声的感知辨别任务上训练端到端的深度循环神经网络。

Result: 网络学习到灵长类决策的几个关键能力，如权衡速度和准确性、根据新信息灵活改变决策，其内部动态表明决策机制与灵长类神经生理学研究相似。

Conclusion: 这些结果为灵长类形成灵活决策能力的关键压力提供了实验支持。

Abstract: Progress has led to a detailed understanding of the neural mechanisms that underlie decision making in primates. However, less is known about why such mechanisms are present in the first place. Theory suggests that primate decision making mechanisms, and their resultant behavioral abilities, emerged to maximize reward in the face of noisy, temporally evolving information. To test this theory, we trained an end-to-end deep recurrent neural network using reinforcement learning on a noisy perceptual discrimination task. Networks learned several key abilities of primate-like decision making including trading off speed for accuracy, and flexibly changing their mind in the face of new information. Internal dynamics of these networks suggest that these abilities were supported by similar decision mechanisms as those observed in primate neurophysiological studies. These results provide experimental support for key pressures that gave rise to the primate ability to make flexible decisions.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [581] [Hierarchical Long Video Understanding with Audiovisual Entity Cohesion and Agentic Search](https://arxiv.org/abs/2601.13719)
*Xinlei Yin,Xiulian Peng,Xiao Li,Zhiwei Xiong,Yan Lu*

Main category: cs.CV

TL;DR: 本文提出HAVEN框架用于长视频理解，它通过整合视听实体内聚和分层视频索引与智能搜索实现连贯全面推理，实验效果好，在LVBench上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有长视频理解方案依赖简单分块策略和检索增强生成，存在信息碎片化和全局连贯性缺失问题。

Method: 提出HAVEN框架，整合跨视觉和听觉流的实体级表示，维持语义一致性；将内容组织成包含全局摘要、场景、片段和实体级别的结构化层次；采用智能搜索机制实现跨层动态检索和推理。

Result: 方法在时间连贯性、实体一致性和检索效率方面表现良好，在LVBench上整体准确率达84.1%，在推理类别中达到80.1%。

Conclusion: 结构化、多模态推理对长视频的全面和上下文一致理解具有效性。

Abstract: Long video understanding presents significant challenges for vision-language models due to extremely long context windows. Existing solutions relying on naive chunking strategies with retrieval-augmented generation, typically suffer from information fragmentation and a loss of global coherence. We present HAVEN, a unified framework for long-video understanding that enables coherent and comprehensive reasoning by integrating audiovisual entity cohesion and hierarchical video indexing with agentic search. First, we preserve semantic consistency by integrating entity-level representations across visual and auditory streams, while organizing content into a structured hierarchy spanning global summary, scene, segment, and entity levels. Then we employ an agentic search mechanism to enable dynamic retrieval and reasoning across these layers, facilitating coherent narrative reconstruction and fine-grained entity tracking. Extensive experiments demonstrate that our method achieves good temporal coherence, entity consistency, and retrieval efficiency, establishing a new state-of-the-art with an overall accuracy of 84.1% on LVBench. Notably, it achieves outstanding performance in the challenging reasoning category, reaching 80.1%. These results highlight the effectiveness of structured, multimodal reasoning for comprehensive and context-consistent understanding of long-form videos.

</details>


### [582] [Towards Airborne Object Detection: A Deep Learning Analysis](https://arxiv.org/abs/2601.11907)
*Prosenjit Chatterjee,ANK Zaman*

Main category: cs.CV

TL;DR: 本文提出基于EfficientNetB4的双任务模型用于机载物体分类和威胁等级预测，构建了AODTA数据集，模型在相关数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有机载威胁评估系统依赖手动监测，可扩展性和效率有限，需要实时自动威胁评估系统。

Method: 引入基于EfficientNetB4的双任务模型，构建AODTA数据集，在AVD和AODTA数据集上进行基准测试并与ResNet - 50对比。

Result: EfficientNetB4模型在物体分类中准确率达96%，威胁等级预测准确率达90%，优于ResNet - 50。

Conclusion: 该模型在监视、国防和空域管理等应用中具有潜力。

Abstract: The rapid proliferation of airborne platforms, including commercial aircraft, drones, and UAVs, has intensified the need for real-time, automated threat assessment systems. Current approaches depend heavily on manual monitoring, resulting in limited scalability and operational inefficiencies. This work introduces a dual-task model based on EfficientNetB4 capable of performing airborne object classification and threat-level prediction simultaneously. To address the scarcity of clean, balanced training data, we constructed the AODTA Dataset by aggregating and refining multiple public sources. We benchmarked our approach on both the AVD Dataset and the newly developed AODTA Dataset and further compared performance against a ResNet-50 baseline, which consistently underperformed EfficientNetB4. Our EfficientNetB4 model achieved 96% accuracy in object classification and 90% accuracy in threat-level prediction, underscoring its promise for applications in surveillance, defense, and airspace management. Although the title references detection, this study focuses specifically on classification and threat-level inference using pre-localized airborne object images provided by existing datasets.

</details>


### [583] [Predicting When to Trust Vision-Language Models for Spatial Reasoning](https://arxiv.org/abs/2601.11644)
*Muhammad Imran,Yugyung Lee*

Main category: cs.CV

TL;DR: 提出基于视觉的置信度估计框架，验证VLM空间预测，在多方面有显著提升，证明外部几何验证优于自我评估。


<details>
  <summary>Details</summary>
Motivation: VLMs在空间推理任务存在失败情况，为安全部署需要预测何时信任其空间预测。

Method: 提出基于视觉的置信度估计框架，通过目标检测进行独立几何验证，利用梯度提升融合四个信号。

Result: 在BLIP - 2和CLIP上AUROC有提升，实现选择性预测，覆盖率提高，特征分析表明视觉信号贡献大，场景图构建中精度提升。

Conclusion: 外部几何验证优于基于文本的自我评估方法，框架有效可靠。

Abstract: Vision-Language Models (VLMs) demonstrate impressive capabilities across multimodal tasks, yet exhibit systematic spatial reasoning failures, achieving only 49% (CLIP) to 54% (BLIP-2) accuracy on basic directional relationships. For safe deployment in robotics and autonomous systems, we need to predict when to trust VLM spatial predictions rather than accepting all outputs. We propose a vision-based confidence estimation framework that validates VLM predictions through independent geometric verification using object detection. Unlike text-based approaches relying on self-assessment, our method fuses four signals via gradient boosting: geometric alignment between VLM claims and coordinates, spatial ambiguity from overlap, detection quality, and VLM internal uncertainty. We achieve 0.674 AUROC on BLIP-2 (34.0% improvement over text-based baselines) and 0.583 AUROC on CLIP (16.1% improvement), generalizing across generative and classification architectures. Our framework enables selective prediction: at 60% target accuracy, we achieve 61.9% coverage versus 27.6% baseline (2.2x improvement) on BLIP-2. Feature analysis reveals vision-based signals contribute 87.4% of model importance versus 12.7% from VLM confidence, validating that external geometric verification outperforms self-assessment. We demonstrate reliable scene graph construction where confidence-based pruning improves precision from 52.1% to 78.3% while retaining 68.2% of edges.

</details>


### [584] [Aesthetics as Structural Harm: Algorithmic Lookism Across Text-to-Image Generation and Classification](https://arxiv.org/abs/2601.11651)
*Miriam Doh,Aditya Gulati,Corina Canali,Nuria Oliver*

Main category: cs.CV

TL;DR: 研究文本到图像生成式AI及下游性别分类任务中的算法外貌主义，揭示相关偏见和危害。


<details>
  <summary>Details</summary>
Motivation: 探究文本到图像生成式AI和下游性别分类任务中是否存在算法外貌主义。

Method: 分析用Stable Diffusion 2.1和3.5 Medium创建的26400张合成人脸。

Result: 发现生成式AI模型将面部吸引力与积极属性关联，三种性别分类算法存在显著性别偏见，揭示三种关键危害。

Conclusion: 算法外貌主义是跨AI视觉系统的系统性基础设施，通过表征和识别加剧现有不平等。

Abstract: This paper examines algorithmic lookism-the systematic preferential treatment based on physical appearance-in text-to-image (T2I) generative AI and a downstream gender classification task. Through the analysis of 26,400 synthetic faces created with Stable Diffusion 2.1 and 3.5 Medium, we demonstrate how generative AI models systematically associate facial attractiveness with positive attributes and vice-versa, mirroring socially constructed biases rather than evidence-based correlations. Furthermore, we find significant gender bias in three gender classification algorithms depending on the attributes of the input faces. Our findings reveal three critical harms: (1) the systematic encoding of attractiveness-positive attribute associations in T2I models; (2) gender disparities in classification systems, where women's faces, particularly those generated with negative attributes, suffer substantially higher misclassification rates than men's; and (3) intensifying aesthetic constraints in newer models through age homogenization, gendered exposure patterns, and geographic reductionism. These convergent patterns reveal algorithmic lookism as systematic infrastructure operating across AI vision systems, compounding existing inequalities through both representation and recognition.
  Disclaimer: This work includes visual and textual content that reflects stereotypical associations between physical appearance and socially constructed attributes, including gender, race, and traits associated with social desirability. Any such associations found in this study emerge from the biases embedded in generative AI systems-not from empirical truths or the authors' views.

</details>


### [585] [MATEX: Multi-scale Attention and Text-guided Explainability of Medical Vision-Language Models](https://arxiv.org/abs/2601.11666)
*Muhammad Imran,Chi Lee,Yugyung Lee*

Main category: cs.CV

TL;DR: 介绍MATEX框架，结合多种方法提升医学视觉语言模型可解释性，在MS - CXR数据集上表现优于M2IB，增强放射AI应用的可信赖性。


<details>
  <summary>Details</summary>
Motivation: 解决先前方法在空间精度、解剖学基础和注意力粒度方面的关键局限，提升医学视觉语言模型的可解释性。

Method: MATEX框架结合多层注意力展开、文本引导的空间先验和层一致性分析生成梯度归因图。

Result: 在MS - CXR数据集上，MATEX在空间精度和与专家标注发现的对齐方面优于最先进的M2IB方法。

Conclusion: MATEX有潜力增强放射AI应用的信任度和透明度。

Abstract: We introduce MATEX (Multi-scale Attention and Text-guided Explainability), a novel framework that advances interpretability in medical vision-language models by incorporating anatomically informed spatial reasoning. MATEX synergistically combines multi-layer attention rollout, text-guided spatial priors, and layer consistency analysis to produce precise, stable, and clinically meaningful gradient attribution maps. By addressing key limitations of prior methods, such as spatial imprecision, lack of anatomical grounding, and limited attention granularity, MATEX enables more faithful and interpretable model explanations. Evaluated on the MS-CXR dataset, MATEX outperforms the state-of-the-art M2IB approach in both spatial precision and alignment with expert-annotated findings. These results highlight MATEX's potential to enhance trust and transparency in radiological AI applications.

</details>


### [586] [Generating metamers of human scene understanding](https://arxiv.org/abs/2601.11675)
*Ritik Raina,Abe Leite,Alexandros Graikos,Seoyoung Ahn,Dimitris Samaras,Gregory J. Zelinsky*

Main category: cs.CV

TL;DR: 介绍工具MetamerGen生成与人类场景表征对齐的图像，通过实验评估，证明是理解场景理解的有力工具。


<details>
  <summary>Details</summary>
Motivation: 为了生成与人类潜在场景表征对齐的场景，助力理解人类视觉对场景的构建。

Method: 采用潜在扩散模型MetamerGen，结合周边场景要点信息和注视点信息，引入双流表征处理不同分辨率输入，通过相同 - 不同行为实验评估。

Result: 能识别出与观众潜在场景表征的同态图像，发现生成场景基于观众自身注视区域时，高级语义对齐最能预测同态性。

Conclusion: MetamerGen是理解场景理解的强大工具，分析揭示了视觉处理多层次特征对人类判断的贡献。

Abstract: Human vision combines low-resolution "gist" information from the visual periphery with sparse but high-resolution information from fixated locations to construct a coherent understanding of a visual scene. In this paper, we introduce MetamerGen, a tool for generating scenes that are aligned with latent human scene representations. MetamerGen is a latent diffusion model that combines peripherally obtained scene gist information with information obtained from scene-viewing fixations to generate image metamers for what humans understand after viewing a scene. Generating images from both high and low resolution (i.e. "foveated") inputs constitutes a novel image-to-image synthesis problem, which we tackle by introducing a dual-stream representation of the foveated scenes consisting of DINOv2 tokens that fuse detailed features from fixated areas with peripherally degraded features capturing scene context. To evaluate the perceptual alignment of MetamerGen generated images to latent human scene representations, we conducted a same-different behavioral experiment where participants were asked for a "same" or "different" response between the generated and the original image. With that, we identify scene generations that are indeed metamers for the latent scene representations formed by the viewers. MetamerGen is a powerful tool for understanding scene understanding. Our proof-of-concept analyses uncovered specific features at multiple levels of visual processing that contributed to human judgments. While it can generate metamers even conditioned on random fixations, we find that high-level semantic alignment most strongly predicts metamerism when the generated scenes are conditioned on viewers' own fixated regions.

</details>


### [587] [Telling Human and Machine Handwriting Apart](https://arxiv.org/abs/2601.11700)
*Luis A. Leiva,Moises Diaz,Nuwan T. Attygalle,Miguel A. Ferrer,Rejean Plamondon*

Main category: cs.CV

TL;DR: 研究利用手写动作作为行为生物特征验证用户，用多个数据集和合成器训练浅层循环神经网络，在不同场景表现优异，为系统增添安全层。


<details>
  <summary>Details</summary>
Motivation: 利用手写动作作为独特行为生物特征，验证设备或应用的真实用户操作，解决类似反向图灵测试的任务。

Method: 研究十个手写符号公共数据集，用七种合成器人工复制数据，使用非特征化轨迹数据作为输入，训练浅层循环神经网络。

Result: 模型表现出色，平均ROC曲线下面积（AUC）得分为98.3%，等错误率为1.4%，少样本和域外设置下结果也很有竞争力。

Conclusion: 研究成果对需要验证人类存在的计算机系统有意义，增加了额外安全保障。

Abstract: Handwriting movements can be leveraged as a unique form of behavioral biometrics, to verify whether a real user is operating a device or application. This task can be framed as a reverse Turing test in which a computer has to detect if an input instance has been generated by a human or artificially. To tackle this task, we study ten public datasets of handwritten symbols (isolated characters, digits, gestures, pointing traces, and signatures) that are artificially reproduced using seven different synthesizers, including, among others, the Kinematic Theory (Sigma h model), generative adversarial networks, Transformers, and Diffusion models. We train a shallow recurrent neural network that achieves excellent performance (98.3 percent Area Under the ROC Curve (AUC) score and 1.4 percent equal error rate on average across all synthesizers and datasets) using nonfeaturized trajectory data as input. In few-shot settings, we show that our classifier achieves such an excellent performance when trained on just 10 percent of the data, as evaluated on the remaining 90% of the data as a test set. We further challenge our classifier in out-of-domain settings, and observe very competitive results as well. Our work has implications for computerized systems that need to verify human presence, and adds an additional layer of security to keep attackers at bay.

</details>


### [588] [\textit{FocaLogic}: Logic-Based Interpretation of Visual Model Decisions](https://arxiv.org/abs/2601.12049)
*Chenchen Zhao,Muxi Chen,Qiang Xu*

Main category: cs.CV

TL;DR: 提出FocaLogic框架用于解释视觉模型，可找出视觉焦点并转化为逻辑表达式，还有定量指标评估，实证显示其能揭示关键信息。


<details>
  <summary>Details</summary>
Motivation: 现有可解释性方法存在依赖白盒模型访问或定量严谨性不足的问题，需新方法解决。

Method: 引入FocaLogic框架，识别视觉焦点并转化为逻辑表达式，提出一套定量指标。

Result: 能揭示训练引起的集中性、泛化提高焦点准确性以及偏见和对抗攻击下的异常焦点等关键信息。

Conclusion: FocaLogic为解释视觉模型提供了系统、可扩展且定量的解决方案。

Abstract: Interpretability of modern visual models is crucial, particularly in high-stakes applications. However, existing interpretability methods typically suffer from either reliance on white-box model access or insufficient quantitative rigor. To address these limitations, we introduce FocaLogic, a novel model-agnostic framework designed to interpret and quantify visual model decision-making through logic-based representations. FocaLogic identifies minimal interpretable subsets of visual regions-termed visual focuses-that decisively influence model predictions. It translates these visual focuses into precise and compact logical expressions, enabling transparent and structured interpretations. Additionally, we propose a suite of quantitative metrics, including focus precision, recall, and divergence, to objectively evaluate model behavior across diverse scenarios. Empirical analyses demonstrate FocaLogic's capability to uncover critical insights such as training-induced concentration, increasing focus accuracy through generalization, and anomalous focuses under biases and adversarial attacks. Overall, FocaLogic provides a systematic, scalable, and quantitative solution for interpreting visual models.

</details>


### [589] [Domain-Specific Self-Supervised Pre-training for Agricultural Disease Classification: A Hierarchical Vision Transformer Study](https://arxiv.org/abs/2601.11612)
*Arnav S. Sonavane*

Main category: cs.CV

TL;DR: 研究特定领域自监督预训练对农业病害分类的影响，发现预训练比架构设计更能提升准确率，推荐优先收集领域数据。


<details>
  <summary>Details</summary>
Motivation: 探究特定领域自监督预训练在农业病害分类中的作用。

Method: 使用HierarchicalViT在三个数据集上评估，进行SimCLR预训练，对比不同架构。

Result: 3000张未标记农业图像的SimCLR预训练使准确率提升4.57%；HVT - Base比Swin - Base准确率高1.68%；HVT的ECE为3.56%（温度缩放后1.52%）。

Conclusion: 从业者应优先收集领域数据而非选择架构。

Abstract: We investigate the impact of domain-specific self-supervised pre-training on agricultural disease classification using hierarchical vision transformers. Our key finding is that SimCLR pre-training on just 3,000 unlabeled agricultural images provides a +4.57% accuracy improvement--exceeding the +3.70% gain from hierarchical architecture design. Critically, we show this SSL benefit is architecture-agnostic: applying the same pre-training to Swin-Base yields +4.08%, to ViT-Base +4.20%, confirming practitioners should prioritize domain data collection over architectural choices. Using HierarchicalViT (HVT), a Swin-style hierarchical transformer, we evaluate on three datasets: Cotton Leaf Disease (7 classes, 90.24%), PlantVillage (38 classes, 96.3%), and PlantDoc (27 classes, 87.1%). At matched parameter counts, HVT-Base (78M) achieves 88.91% vs. Swin-Base (88M) at 87.23%, a +1.68% improvement. For deployment reliability, we report calibration analysis showing HVT achieves 3.56% ECE (1.52% after temperature scaling). Code: https://github.com/w2sg-arnav/HierarchicalViT

</details>


### [590] [Automating Parameter Selection in Deep Image Prior for Fluorescence Microscopy Image Denoising via Similarity-Based Parameter Transfer](https://arxiv.org/abs/2601.12055)
*Lina Meyer,Felix Wissel,Tobias Knopp,Susanne Pfefferle,Ralf Fliegert,Maximilian Sandmann,Liana Uebler,Franziska Möckl,Björn-Philipp Diercks,David Lohr,René Werner*

Main category: cs.CV

TL;DR: 本文聚焦荧光显微镜数据，提出自动参数转移管道 AUTO - DIP ，对比多种方法，显示基于图像元数据相似性的参数转移性能好，AUTO - DIP 在多测试集表现优。


<details>
  <summary>Details</summary>
Motivation: 传统无监督深度图像先验（DIP）优化参数耗时，限制在多图像处理领域应用，推测相似图像在基于 DIP 去噪中有相似最优参数配置，以实现免优化 DIP 用于荧光显微镜。

Method: 从开源数据集生成校准集和验证集进行网络架构搜索，确定理想 U - net 架构和停止点，实现自动参数转移管道 AUTO - DIP ，并与基线和最先进的特定图像变分去噪方法对比。

Result: 基于图像元数据相似性的参数转移比基于定量图像相似性度量的转移有相似或更好性能；AUTO - DIP 在多个测试数据集上优于基线 DIP 和变分去噪方法，尤其是高噪声输入情况。

Conclusion: AUTO - DIP 在荧光显微镜图像去噪上有明显优势，证明了基于图像元数据的参数转移策略可行且有效。

Abstract: Unsupervised deep image prior (DIP) addresses shortcomings of training data requirements and limited generalization associated with supervised deep learning. The performance of DIP depends on the network architecture and the stopping point of its iterative process. Optimizing these parameters for a new image requires time, restricting DIP application in domains where many images need to be processed. Focusing on fluorescence microscopy data, we hypothesize that similar images share comparable optimal parameter configurations for DIP-based denoising, potentially enabling optimization-free DIP for fluorescence microscopy. We generated a calibration (n=110) and validation set (n=55) of semantically different images from an open-source dataset for a network architecture search targeted towards ideal U-net architectures and stopping points. The calibration set represented our transfer basis. The validation set enabled the assessment of which image similarity criterion yields the best results. We then implemented AUTO-DIP, a pipeline for automatic parameter transfer, and compared it to the originally published DIP configuration (baseline) and a state-of-the-art image-specific variational denoising approach. We show that a parameter transfer from the calibration dataset to a test image based on only image metadata similarity (e.g., microscope type, imaged specimen) leads to similar and better performance than a transfer based on quantitative image similarity measures. AUTO-DIP outperforms the baseline DIP (DIP with original DIP parameters) as well as the variational denoising approaches for several open-source test datasets of varying complexity, particularly for very noisy inputs. Applications to locally acquired fluorescence microscopy images further proved superiority of AUTO-DIP.

</details>


### [591] [Multi-modal MRI-Based Alzheimer's Disease Diagnosis with Transformer-based Image Synthesis and Transfer Learning](https://arxiv.org/abs/2601.11614)
*Jason Qiu*

Main category: cs.CV

TL;DR: 提出3D TransUNet图像合成框架，从T1w MRI预测FA和MD图，提升AD诊断准确性和可及性


<details>
  <summary>Details</summary>
Motivation: AD需早期检测，dMRI有优势但受限，需从T1w MRI获取dMRI信息

Method: 构建3D TransUNet图像合成框架从T1w MRI预测FA和MD图

Result: 模型生成高保真图，SSIM超0.93，Pearson相关超0.94；提升AD分类准确率5%，MCI检测率12.5%

Conclusion: 可从T1w MRI推断高质量扩散微结构信息，改进AD诊断的可及性、效率和准确性

Abstract: Alzheimer's disease (AD) is a progressive neurodegenerative disorder in which pathological changes begin many years before the onset of clinical symptoms, making early detection essential for timely intervention. T1-weighted (T1w) Magnetic Resonance Imaging (MRI) is routinely used in clinical practice to identify macroscopic brain alterations, but these changes typically emerge relatively late in the disease course. Diffusion MRI (dMRI), in contrast, is sensitive to earlier microstructural abnormalities by probing water diffusion in brain tissue. dMRI metrics, including fractional anisotropy (FA) and mean diffusivity (MD), provide complementary information about white matter integrity and neurodegeneration. However, dMRI acquisitions are time-consuming and susceptible to motion artifacts, limiting their routine use in clinical populations. To bridge this gap, I propose a 3D TransUNet image synthesis framework that predicts FA and MD maps directly from T1w MRI. My model generates high-fidelity maps, achieving a structural similarity index (SSIM) exceeding 0.93 and a strong Pearson correlation (>0.94) with ground-truth dMRI. When integrated into a multi-modal diagnostic model, these synthetic features boost AD classification accuracy by 5% (78.75%->83.75%) and, most importantly, improve mild cognitive impairment (MCI) detection by 12.5%. This study demonstrates that high-quality diffusion microstructural information can be inferred from routinely acquired T1w MRI, effectively transferring the benefits of multi-modality imaging to settings where diffusion data are unavailable. By reducing scan time while preserving complementary structural and microstructural information, the proposed approach has the potential to improve the accessibility, efficiency, and accuracy of AD diagnosis in clinical practice.

</details>


### [592] [Conditional Random Fields for Interactive Refinement of Histopathological Predictions](https://arxiv.org/abs/2601.12082)
*Tiffanie Godelaine,Maxime Zanella,Karim El Khoury,Saïd Mahmoudi,Benoît Macq,Christophe De Vleeschouwer*

Main category: cs.CV

TL;DR: 提出HistoCRF框架改进组织病理学图像零样本预测，实验显示在不同情况下有显著准确率提升。


<details>
  <summary>Details</summary>
Motivation: 组织病理学图像分析对癌症检测和分期有高临床价值，现有视觉语言模型零样本预测不完善，需改进。

Method: 将条件随机场（CRFs）应用于组织病理学，提出基于CRF的HistoCRF框架，定义促进标签多样性并利用专家注释的成对势函数。

Result: 在五个数据集上，无注释时准确率提升16.0%，仅100注释时提升27.5%，引入人工环节在相同注释数下提升32.6%。

Conclusion: 所提HistoCRF框架能有效改进组织病理学图像零样本预测，且代码将开源。

Abstract: Assisting pathologists in the analysis of histopathological images has high clinical value, as it supports cancer detection and staging. In this context, histology foundation models have recently emerged. Among them, Vision-Language Models (VLMs) provide strong yet imperfect zero-shot predictions. We propose to refine these predictions by adapting Conditional Random Fields (CRFs) to histopathological applications, requiring no additional model training. We present HistoCRF, a CRF-based framework, with a novel definition of the pairwise potential that promotes label diversity and leverages expert annotations. We consider three experiments: without annotations, with expert annotations, and with iterative human-in-the-loop annotations that progressively correct misclassified patches. Experiments on five patch-level classification datasets covering different organs and diseases demonstrate average accuracy gains of 16.0% without annotations and 27.5% with only 100 annotations, compared to zero-shot predictions. Moreover, integrating a human in the loop reaches a further gain of 32.6% with the same number of annotations. The code will be made available on https://github.com/tgodelaine/HistoCRF.

</details>


### [593] [Mixture of Distributions Matters: Dynamic Sparse Attention for Efficient Video Diffusion Transformers](https://arxiv.org/abs/2601.11641)
*Yuxi Liu,Yipeng Hu,Zekun Zhang,Kunze Jiang,Kun Yuan*

Main category: cs.CV

TL;DR: 提出MOD - DiT解决Diffusion Transformers在视频生成中的计算瓶颈，提升效率与质量。


<details>
  <summary>Details</summary>
Motivation: Diffusion Transformers在视频生成中受自注意力机制二次复杂度限制，现有稀疏注意力方法有缺陷，影响实际应用和生成质量。

Method: 提出MOD - DiT，通过两阶段过程建模注意力模式，先利用早期去噪信息建立线性近似模型预测掩码模式，再用在线块掩码策略动态应用掩码。

Result: 在多个基准和模型架构上实现加速和质量提升。

Conclusion: MOD - DiT有效克服传统稀疏注意力方法计算局限，可用于高效、高质量视频生成。

Abstract: While Diffusion Transformers (DiTs) have achieved notable progress in video generation, this long-sequence generation task remains constrained by the quadratic complexity inherent to self-attention mechanisms, creating significant barriers to practical deployment. Although sparse attention methods attempt to address this challenge, existing approaches either rely on oversimplified static patterns or require computationally expensive sampling operations to achieve dynamic sparsity, resulting in inaccurate pattern predictions and degraded generation quality. To overcome these limitations, we propose a \underline{\textbf{M}}ixtrue-\underline{\textbf{O}}f-\underline{\textbf{D}}istribution \textbf{DiT} (\textbf{MOD-DiT}), a novel sampling-free dynamic attention framework that accurately models evolving attention patterns through a two-stage process. First, MOD-DiT leverages prior information from early denoising steps and adopts a {distributed mixing approach} to model an efficient linear approximation model, which is then used to predict mask patterns for a specific denoising interval. Second, an online block masking strategy dynamically applies these predicted masks while maintaining historical sparsity information, eliminating the need for repetitive sampling operations. Extensive evaluations demonstrate consistent acceleration and quality improvements across multiple benchmarks and model architectures, validating MOD-DiT's effectiveness for efficient, high-quality video generation while overcoming the computational limitations of traditional sparse attention approaches.

</details>


### [594] [PSSF: Early osteoarthritis detection using physical synthetic knee X-ray scans and AI radiomics models](https://arxiv.org/abs/2601.11642)
*Abbas Alzubaidi,Ali Al-Bayaty*

Main category: cs.CV

TL;DR: 本文介绍基于物理的合成模拟框架PSSF生成可控X射线扫描，创建虚拟队列并进行处理，用机器学习模型预测，评估鲁棒性和特征稳定性。


<details>
  <summary>Details</summary>
Motivation: 膝关节骨关节炎主要靠主观影像分级评估，AI和影像组学依赖难获取的影像数据集，需新方法。

Method: 引入PSSF，创建虚拟队列，进行图像预处理，用三种机器学习模型训练，在不同场景评估，用类内相关系数评估特征稳定性。

Result: 利用PSSF创建虚拟队列，对其进行多种处理和模型训练。

Conclusion: 未明确提及最终结论，主要展示了生成图像、训练模型、评估性能的过程。

Abstract: Knee osteoarthritis (OA) is a major cause of disability worldwide and is still largely assessed using subjective radiographic grading, most commonly the Kellgren-Lawrence (KL) scale. Artificial intelligence (AI) and radiomics offer quantitative tools for OA assessment but depend on large, well-annotated image datasets, mainly X-ray scans, that are often difficult to obtain because of privacy, governance and resourcing constraints. In this research, we introduce a physics-based synthetic simulation framework (PSSF) to fully generate controllable X-ray scans without patients' involvement and violating their privacy and institutional constraints. This PSSF is a 2D X-ray projection simulator of anteroposterior knee radiographs from a parametric anatomical model of the distal femur and proximal tibia. Using PSSF, we create a virtual cohort of 180 subjects (260 knees), each is imaged under three protocols (reference, low-dose, and geometry-shift). Medial joint regions are automatically localized, preprocessed, and processed with the Image Biomarker Standardisation Initiative (IBSI). Practically, three machine learning (ML) models are utilized, logistic regression, random forest, and gradient boosting, to train binary (KL-like "0" vs. "2") and three-class (0-2) prediction radiographic images. Robustness is assessed within IBSI protocol, cross-protocol, and multi-protocol scenarios. Finally, features stability is then evaluated using intraclass correlation coefficients across acquisition changes.

</details>


### [595] [Segment and Matte Anything in a Unified Model](https://arxiv.org/abs/2601.12147)
*Zezhong Fan,Xiaohan Li,Topojoy Biswas,Kaushiki Nag,Kannan Achan*

Main category: cs.CV

TL;DR: 提出轻量级扩展模型SAMA，实现高质量交互式图像分割和抠图。


<details>
  <summary>Details</summary>
Motivation: SAM掩码预测精度不足，且未在交互式图像抠图领域探索，期望构建统一模型完成分割和抠图任务。

Method: 引入Multi - View Localization Encoder捕获局部特征，用Localization Adapter细化掩码输出，架构中加入两个预测头同时生成分割和抠图掩码，在多样数据集上训练。

Result: SAMA在多个分割和抠图基准测试中达到了最先进的性能。

Conclusion: SAMA作为SAM的轻量级扩展，参数增加少，在多种下游任务中展现出适应性和有效性。

Abstract: Segment Anything (SAM) has recently pushed the boundaries of segmentation by demonstrating zero-shot generalization and flexible prompting after training on over one billion masks. Despite this, its mask prediction accuracy often falls short of the precision required in real-world applications. While several refinement modules have been proposed to boost SAM's segmentation quality, achieving highly accurate object delineation within a single, unified framework remains an open challenge. Furthermore, interactive image matting, which aims to generate fine-grained alpha mattes guided by diverse user hints, has not yet been explored in the context of SAM. Insights from recent studies highlight strong correlations between segmentation and matting, suggesting the feasibility of a unified model capable of both tasks. In this paper, we introduce Segment And Matte Anything (SAMA), a lightweight extension of SAM that delivers high-quality interactive image segmentation and matting with minimal extra parameters. Our Multi-View Localization Encoder (MVLE) captures detailed features from local views, while the Localization Adapter (Local-Adapter) refines mask outputs by recovering subtle boundary details. We also incorporate two prediction heads for each task into the architecture to generate segmentation and matting masks, simultaneously. Trained on a diverse dataset aggregated from publicly available sources, SAMA achieves state-of-the-art performance across multiple segmentation and matting benchmarks, showcasing its adaptability and effectiveness in a wide range of downstream tasks.

</details>


### [596] [Zeros can be Informative: Masked Binary U-Net for Image Segmentation on Tensor Cores](https://arxiv.org/abs/2601.11660)
*Chunshu Wu,Ruibing Song,Sushant Kondguli,Tong Geng,Ang Li*

Main category: cs.CV

TL;DR: 提出Masked Binary U-Net（MBU-Net）及GPU执行框架，在3个分割基准测试中实现近全精度准确率，加速并节能。


<details>
  <summary>Details</summary>
Motivation: 实时图像分割需在资源受限边缘设备满足要求，U-Net处理高分辨率输入有挑战，极端量化有局限，需解决精度和效率问题。

Method: 基于两个经验观察，采用成本感知掩码策略得到MBU-Net，开发GPU执行框架通过减法位编码方案映射到张量核心。

Result: 在3个分割基准测试中，MBU-Net达到近全精度准确率（平均下降3%），比16位浮点U-Net加速2.04倍，节能3.54倍。

Conclusion: MBU-Net能在保证精度的同时实现接近二进制的效率，且GPU执行框架能在通用GPU上高效实现。

Abstract: Real-time image segmentation is a key enabler for AR/VR, robotics, drones, and autonomous systems, where tight accuracy, latency, and energy budgets must be met on resource-constrained edge devices. While U-Net offers a favorable balance of accuracy and efficiency compared to large transformer-based models, achieving real-time performance on high-resolution input remains challenging due to compute, memory, and power limits. Extreme quantization, particularly binary networks, is appealing for its hardware-friendly operations. However, two obstacles limit practicality: (1) severe accuracy degradation, and (2) a lack of end-to-end implementations that deliver efficiency on general-purpose GPUs.
  We make two empirical observations that guide our design. (1) An explicit zero state is essential: training with zero masking to binary U-Net weights yields noticeable sparsity. (2) Quantization sensitivity is uniform across layers. Motivated by these findings, we introduce Masked Binary U-Net (MBU-Net), obtained through a cost-aware masking strategy that prioritizes masking where it yields the highest accuracy-per-cost, reconciling accuracy with near-binary efficiency.
  To realize these gains in practice, we develop a GPU execution framework that maps MBU-Net to Tensor Cores via a subtractive bit-encoding scheme, efficiently implementing masked binary weights with binary activations. This design leverages native binary Tensor Core BMMA instructions, enabling high throughput and energy savings on widely available GPUs. Across 3 segmentation benchmarks, MBU-Net attains near full-precision accuracy (3% average drop) while delivering 2.04x speedup and 3.54x energy reductions over a 16-bit floating point U-Net.

</details>


### [597] [Enhanced Diagnostic Performance via Large-Resolution Inference Optimization for Pathology Foundation Models](https://arxiv.org/abs/2601.12150)
*Mengxuan Hu,Zihan Guan,John Kang,Sheng Li,Zhongliang Zhou*

Main category: cs.CV

TL;DR: 许多病理基础模型受输入尺寸限制，处理全切片图像效率低，本文提出时空高效推理策略，减少GPU内存和运行时间，提升下游性能。


<details>
  <summary>Details</summary>
Motivation: 现有病理基础模型受特定输入尺寸限制，处理全切片图像时，放大输入会增加GPU内存消耗，下采样会改变分辨率并模糊关键形态细节，需解决这些问题。

Method: 提出一种时空高效推理策略，利用空间感知相邻块对注意力进行稀疏化，并通过全局注意力分数过滤掉无信息的标记。

Result: 该方法在ROI分类中最多可提升7.67%，在分割任务中取得了相当的结果。

Conclusion: 所提策略能在高分辨率全切片图像推理时大幅减少GPU内存和运行时间，同时保持甚至提升下游性能，可在相同GPU预算下实现更高分辨率的推理。

Abstract: Despite their prominent performance on tasks such as ROI classification and segmentation, many pathology foundation models remain constrained by a specific input size e.g. 224 x 224, creating substantial inefficiencies when applied to whole-slide images (WSIs), which span thousands of resolutions. A naive strategy is to either enlarge inputs or downsample the WSIs. However, enlarging inputs results in prohibitive GPU memory consumption, while downsampling alters the microns-per-pixel resolution and obscures critical morphological details. To overcome these limitations, we propose an space- and time- efficient inference strategy that sparsifies attention using spatially aware neighboring blocks and filters out non-informative tokens through global attention scores. This design substantially reduces GPU memory and runtime during high-resolution WSI inference while preserving and even improving the downstream performance, enabling inference at higher resolutions under the same GPU budget. The experimental results show that our method can achieves up to an 7.67% improvement in the ROI classification and compatible results in segmentation.

</details>


### [598] [Where It Moves, It Matters: Referring Surgical Instrument Segmentation via Motion](https://arxiv.org/abs/2601.12224)
*Meng Wei,Kun Yuan,Shi Li,Yue Zhou,Long Bai,Nassir Navab,Hongliang Ren,Hong Joo Lee,Tom Vercauteren,Nicolas Padoy*

Main category: cs.CV

TL;DR: 提出SurgRef框架和Ref - IMotion数据集用于手术视频分割，达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有手术视频指称分割方法依赖静态视觉线索和预定义器械名称，泛化能力差，需实现直观、语言驱动的手术场景交互。

Method: 引入SurgRef框架，将自由形式语言表达与器械运动关联；构建Ref - IMotion数据集用于训练和评估。

Result: SurgRef在手术过程中实现了最先进的准确性和泛化能力。

Conclusion: SurgRef为鲁棒的、语言驱动的手术视频分割设定了新基准。

Abstract: Enabling intuitive, language-driven interaction with surgical scenes is a critical step toward intelligent operating rooms and autonomous surgical robotic assistance. However, the task of referring segmentation, localizing surgical instruments based on natural language descriptions, remains underexplored in surgical videos, with existing approaches struggling to generalize due to reliance on static visual cues and predefined instrument names. In this work, we introduce SurgRef, a novel motion-guided framework that grounds free-form language expressions in instrument motion, capturing how tools move and interact across time, rather than what they look like. This allows models to understand and segment instruments even under occlusion, ambiguity, or unfamiliar terminology. To train and evaluate SurgRef, we present Ref-IMotion, a diverse, multi-institutional video dataset with dense spatiotemporal masks and rich motion-centric expressions. SurgRef achieves state-of-the-art accuracy and generalization across surgical procedures, setting a new benchmark for robust, language-driven surgical video segmentation.

</details>


### [599] [SpaRRTa: A Synthetic Benchmark for Evaluating Spatial Intelligence in Visual Foundation Models](https://arxiv.org/abs/2601.11729)
*Turhan Can Kargin,Wojciech Jasiński,Adam Pardyl,Bartosz Zieliński,Marcin Przewięźlikowski*

Main category: cs.CV

TL;DR: 本文引入SpaRRTa基准评估视觉基础模型（VFMs）空间关系识别能力，揭示其空间推理能力差异，为模型发展提供参考。


<details>
  <summary>Details</summary>
Motivation: 现有VFMs空间推理能力有限且在不同空间任务上表现不一致，需探究其是否真有空间感知能力。

Method: 引入SpaRRTa基准，它能生成具有多样场景和可控对象排列的图像及空间注释，对一系列先进VFMs进行评估。

Result: 揭示了不同VFMs空间推理能力存在显著差异。

Conclusion: SpaRRTa有望成为指导未来具有空间感知能力视觉模型发展的有用工具。

Abstract: Visual Foundation Models (VFMs), such as DINO and CLIP, excel in semantic understanding of images but exhibit limited spatial reasoning capabilities, which limits their applicability to embodied systems. As a result, recent work incorporates some 3D tasks (such as depth estimation) into VFM training. However, VFM performance remains inconsistent across other spatial tasks, raising the question of whether these models truly have spatial awareness or overfit to specific 3D objectives. To address this question, we introduce the Spatial Relation Recognition Task (SpaRRTa) benchmark, which evaluates the ability of VFMs to identify relative positions of objects in the image. Unlike traditional 3D objectives that focus on precise metric prediction (e.g., surface normal estimation), SpaRRTa probes a fundamental capability underpinning more advanced forms of human-like spatial understanding. SpaRRTa generates an arbitrary number of photorealistic images with diverse scenes and fully controllable object arrangements, along with freely accessible spatial annotations. Evaluating a range of state-of-the-art VFMs, we reveal significant disparities between their spatial reasoning abilities. Through our analysis, we provide insights into the mechanisms that support or hinder spatial awareness in modern VFMs. We hope that SpaRRTa will serve as a useful tool for guiding the development of future spatially aware visual models.

</details>


### [600] [Less is More: Label-Guided Summarization of Procedural and Instructional Videos](https://arxiv.org/abs/2601.12243)
*Shreya Rajpal,Michal Golovanesky,Carsten Eickhoff*

Main category: cs.CV

TL;DR: 提出PRISM框架用于视频摘要，在多数据集评估表现好。


<details>
  <summary>Details</summary>
Motivation: 视频摘要在高风险领域很重要，现有工作有进展，希望产生语义合理的视频摘要。

Method: 提出PRISM三阶段框架，结合自适应视觉采样、标签驱动的关键帧锚定和大语言模型的上下文验证。

Result: 抽样不足原视频5%的帧，保留84%语义内容，比基线最多提升33%。

Conclusion: 该方法能泛化到不同视频任务，在语义对齐和精度上表现出色。

Abstract: Video summarization helps turn long videos into clear, concise representations that are easier to review, document, and analyze, especially in high-stakes domains like surgical training. Prior work has progressed from using basic visual features like color, motion, and structural changes to using pre-trained vision-language models that can better understand what's happening in the video (semantics) and capture temporal flow, resulting in more context-aware video summarization. We propose a three-stage framework, PRISM: Procedural Representation via Integrated Semantic and Multimodal analysis, that produces semantically grounded video summaries. PRISM combines adaptive visual sampling, label-driven keyframe anchoring, and contextual validation using a large language model (LLM). Our method ensures that selected frames reflect meaningful and procedural transitions while filtering out generic or hallucinated content, resulting in contextually coherent summaries across both domain-specific and instructional videos. We evaluate our method on instructional and activity datasets, using reference summaries for instructional videos. Despite sampling fewer than 5% of the original frames, our summaries retain 84% semantic content while improving over baselines by as much as 33%. Our approach generalizes across procedural and domain-specific video tasks, achieving strong performance with both semantic alignment and precision.

</details>


### [601] [An Innovative Framework for Breast Cancer Detection Using Pyramid Adaptive Atrous Convolution, Transformer Integration, and Multi-Scale Feature Fusion](https://arxiv.org/abs/2601.12249)
*Ehsan Sadeghi Pour,Mahdi Esmaeili,Morteza Romoozi*

Main category: cs.CV

TL;DR: 本文提出结合PAAC和Transformer架构的乳腺癌检测框架，利用多尺度特征融合和特定损失函数，在综合数据集上训练，表现优于基础模型，指标良好，可用于医疗诊断。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌是常见癌症，准确及时诊断对改善治疗结果至关重要，需创新检测方法。

Method: 结合PAAC和Transformer架构，利用多尺度特征融合，结合Dice Loss和Focal Loss函数；对综合数据集预处理后训练模型。

Result: 模型准确率98.5%，灵敏度97.8%，特异性96.3%，F1分数98.2%，整体精度97.9%，优于基础模型。

Conclusion: 该模型是可靠高效的乳腺癌诊断工具，可集成到医疗诊断系统。

Abstract: Breast cancer is one of the most common cancers among women worldwide, and its accurate and timely diagnosis plays a critical role in improving treatment outcomes. This thesis presents an innovative framework for detecting malignant masses in mammographic images by integrating the Pyramid Adaptive Atrous Convolution (PAAC) and Transformer architectures. The proposed approach utilizes Multi-Scale Feature Fusion to enhance the extraction of features from benign and malignant tissues and combines Dice Loss and Focal Loss functions to improve the model's learning process, effectively reducing errors in binary breast cancer classification and achieving high accuracy and efficiency. In this study, a comprehensive dataset of breast cancer images from INbreast, MIAS, and DDSM was preprocessed through data augmentation and contrast enhancement and resized to 227x227 pixels for model training. Leveraging the Transformer's ability to manage long-range dependencies with Self-Attention mechanisms, the proposed model achieved high accuracy in detecting cancerous masses, outperforming foundational models such as BreastNet, DeepMammo, Multi-Scale CNN, Swin-Unet, and SegFormer. The final evaluation results for the proposed model include an accuracy of 98.5\%, sensitivity of 97.8\%, specificity of 96.3\%, F1-score of 98.2\%, and overall precision of 97.9\%. These metrics demonstrate a significant improvement over traditional methods and confirm the model's effectiveness in identifying cancerous masses in complex scenarios and large datasets. This model shows potential as a reliable and efficient tool for breast cancer diagnosis and can be effectively integrated into medical diagnostic systems.

</details>


### [602] [Soft Shadow Diffusion (SSD): Physics-inspired Learning for 3D Computational Periscopy](https://arxiv.org/abs/2601.12257)
*Fadlullah Raji,John Murray-Bruce*

Main category: cs.CV

TL;DR: 本文提出新方法实现从普通非视距照片进行隐藏场景的3D重建，通过两种方案解决逆问题，且在实际场景有效，模型泛化性和鲁棒性好。


<details>
  <summary>Details</summary>
Motivation: 传统成像需视线条件，非视距成像可解决视线受阻问题，但现有被动非视距方法有局限，本文旨在实现隐藏场景的3D重建。

Method: 提出新的光传输模型，将隐藏场景分解为光遮挡和非光遮挡组件，形成可分离的非线性最小二乘逆问题，开发基于梯度的优化方法和物理启发的神经网络方法SSD。

Result: 两种方法在实际实验场景对众多3D场景有效，SSD训练后对模拟和真实非视距场景有良好泛化性，对噪声和环境光有较好鲁棒性。

Conclusion: 所提方法能有效从普通非视距照片实现隐藏场景的3D重建，SSD表现出良好的泛化性和鲁棒性。

Abstract: Conventional imaging requires a line of sight to create accurate visual representations of a scene. In certain circumstances, however, obtaining a suitable line of sight may be impractical, dangerous, or even impossible. Non-line-of-sight (NLOS) imaging addresses this challenge by reconstructing the scene from indirect measurements. Recently, passive NLOS methods that use an ordinary photograph of the subtle shadow cast onto a visible wall by the hidden scene have gained interest. These methods are currently limited to 1D or low-resolution 2D color imaging or to localizing a hidden object whose shape is approximately known. Here, we generalize this class of methods and demonstrate a 3D reconstruction of a hidden scene from an ordinary NLOS photograph. To achieve this, we propose a novel reformulation of the light transport model that conveniently decomposes the hidden scene into \textit{light-occluding} and \textit{non-light-occluding} components to yield a separable non-linear least squares (SNLLS) inverse problem. We develop two solutions: A gradient-based optimization method and a physics-inspired neural network approach, which we call Soft Shadow diffusion (SSD). Despite the challenging ill-conditioned inverse problem encountered here, our approaches are effective on numerous 3D scenes in real experimental scenarios. Moreover, SSD is trained in simulation but generalizes well to unseen classes in simulation and real-world NLOS scenes. SSD also shows surprising robustness to noise and ambient illumination.

</details>


### [603] [CytoCLIP: Learning Cytoarchitectural Characteristics in Developing Human Brain Using Contrastive Language Image Pre-Training](https://arxiv.org/abs/2601.12282)
*Pralaypati Ta,Sriram Venkatesaperumal,Keerthi Ram,Mohanasankar Sivaprakasam*

Main category: cs.CV

TL;DR: 提出CytoCLIP视觉语言模型用于脑细胞结构分析，在区域分类和跨模态检索任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 手动划分大脑组织切片区域耗时且需专业知识，需自动化方法。

Method: 基于预训练CLIP框架构建CytoCLIP，有低分辨率和高分辨率两种模型变体，使用不同孕周胎儿脑NISSL染色组织切片数据训练。

Result: 在区域分类和跨模态检索任务中，不同数据集设置下实验，F1分数分别达0.87（全区域分类）和0.91（高分辨率图像块分类）。

Conclusion: CytoCLIP在脑细胞结构分析中优于现有方法。

Abstract: The functions of different regions of the human brain are closely linked to their distinct cytoarchitecture, which is defined by the spatial arrangement and morphology of the cells. Identifying brain regions by their cytoarchitecture enables various scientific analyses of the brain. However, delineating these areas manually in brain histological sections is time-consuming and requires specialized knowledge. An automated approach is necessary to minimize the effort needed from human experts. To address this, we propose CytoCLIP, a suite of vision-language models derived from pre-trained Contrastive Language-Image Pre-Training (CLIP) frameworks to learn joint visual-text representations of brain cytoarchitecture. CytoCLIP comprises two model variants: one is trained using low-resolution whole-region images to understand the overall cytoarchitectural pattern of an area, and the other is trained on high-resolution image tiles for detailed cellular-level representation. The training dataset is created from NISSL-stained histological sections of developing fetal brains of different gestational weeks. It includes 86 distinct regions for low-resolution images and 384 brain regions for high-resolution tiles. We evaluate the model's understanding of the cytoarchitecture and generalization ability using region classification and cross-modal retrieval tasks. Multiple experiments are performed under various data setups, including data from samples of different ages and sectioning planes. Experimental results demonstrate that CytoCLIP outperforms existing methods. It achieves an F1 score of 0.87 for whole-region classification and 0.91 for high-resolution image tile classification.

</details>


### [604] [A Two-Stage Globally-Diverse Adversarial Attack for Vision-Language Pre-training Models](https://arxiv.org/abs/2601.12304)
*Wutao Chen,Huaqin Zou,Chen Wan,Lifeng Huang*

Main category: cs.CV

TL;DR: 提出2S - GDA框架应对VLP模型对抗样本攻击问题，实验表明攻击成功率高且框架有模块化优势。


<details>
  <summary>Details</summary>
Motivation: VLP模型易受对抗样本攻击，现有多模态攻击存在扰动多样性有限和多阶段流水线不稳定的问题。

Method: 提出2S - GDA两阶段全局多样性攻击框架，先通过全局多样策略引入文本扰动，再用多尺度调整和块打乱旋转生成图像级扰动。

Result: 在VLP模型上的大量实验显示，2S - GDA在黑盒设置下比现有方法攻击成功率最高提升11.17%。

Conclusion: 该框架具有模块化特点，可与现有方法结合进一步增强对抗可迁移性。

Abstract: Vision-language pre-training (VLP) models are vulnerable to adversarial examples, particularly in black-box scenarios. Existing multimodal attacks often suffer from limited perturbation diversity and unstable multi-stage pipelines. To address these challenges, we propose 2S-GDA, a two-stage globally-diverse attack framework. The proposed method first introduces textual perturbations through a globally-diverse strategy by combining candidate text expansion with globally-aware replacement. To enhance visual diversity, image-level perturbations are generated using multi-scale resizing and block-shuffle rotation. Extensive experiments on VLP models demonstrate that 2S-GDA consistently improves attack success rates over state-of-the-art methods, with gains of up to 11.17\% in black-box settings. Our framework is modular and can be easily combined with existing methods to further enhance adversarial transferability.

</details>


### [605] [GazeFormer-MoE: Context-Aware Gaze Estimation via CLIP and MoE Transformer](https://arxiv.org/abs/2601.12316)
*Xinyuan Zhao,Xianrui Chen,Ahmad Chaddad*

Main category: cs.CV

TL;DR: 提出语义调制多尺度Transformer用于3D注视估计，在多个数据集达SOTA。


<details>
  <summary>Details</summary>
Motivation: 提升3D注视估计的性能。

Method: 用可学习原型库调节CLIP全局特征，在统一注意力空间融合特征，用路由/共享混合专家替换FFN块。

Result: 在MPIIFaceGaze、EYEDIAP、Gaze360和ETH - XGaze上实现新SOTA角度误差，相对提升达64%。

Conclusion: 性能提升得益于原型调节、跨尺度融合、混合专家和超参数，代码开源。

Abstract: We present a semantics modulated, multi scale Transformer for 3D gaze estimation. Our model conditions CLIP global features with learnable prototype banks (illumination, head pose, background, direction), fuses these prototype-enriched global vectors with CLIP patch tokens and high-resolution CNN tokens in a unified attention space, and replaces several FFN blocks with routed/shared Mixture of Experts to increase conditional capacity. Evaluated on MPIIFaceGaze, EYEDIAP, Gaze360 and ETH-XGaze, our model achieves new state of the art angular errors of 2.49°, 3.22°, 10.16°, and 1.44°, demonstrating up to a 64% relative improvement over previously reported results. ablations attribute gains to prototype conditioning, cross scale fusion, MoE and hyperparameter. Our code is publicly available at https://github. com/AIPMLab/Gazeformer.

</details>


### [606] [Effects of Gabor Filters on Classification Performance of CNNs Trained on a Limited Number of Conditions](https://arxiv.org/abs/2601.11918)
*Akito Morita,Hirotsugu Okuno*

Main category: cs.CV

TL;DR: 本文提出用Gabor滤波器作为CNN预处理器，提升边缘设备上CNN执行机器人视觉应用的精度和减小其规模，实验表明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上运行的CNN需小架构，用于机器人视觉应用的CNN要能根据有限条件下获取的数据有效训练识别特定视觉目标，而视觉神经系统能从少量视觉经验中学习，可借鉴其特性。

Method: 使用Gabor滤波器作为CNN的预处理器，创建不同相机位置的图像数据集，比较多个有和没有Gabor滤波器预处理的CNN架构训练后的精度。

Result: 使用Gabor滤波器预处理能提高CNN的泛化性能，并有助于减小CNN的规模。

Conclusion: 用Gabor滤波器作为预处理器是提升边缘设备上CNN性能的有效方法。

Abstract: In this study, we propose a technique to improve the accuracy and reduce the size of convolutional neural networks (CNNs) running on edge devices for real-world robot vision applications. CNNs running on edge devices must have a small architecture, and CNNs for robot vision applications involving on-site object recognition must be able to be trained efficiently to identify specific visual targets from data obtained under a limited variation of conditions. The visual nervous system (VNS) is a good example that meets the above requirements because it learns from few visual experiences. Therefore, we used a Gabor filter, a model of the feature extractor of the VNS, as a preprocessor for CNNs to investigate the accuracy of the CNNs trained with small amounts of data. To evaluate how well CNNs trained on image data acquired under a limited variation of conditions generalize to data acquired under other conditions, we created an image dataset consisting of images acquired from different camera positions, and investigated the accuracy of the CNNs that trained using images acquired at a certain distance. The results were compared after training on multiple CNN architectures with and without Gabor filters as preprocessing. The results showed that preprocessing with Gabor filters improves the generalization performance of CNNs and contributes to reducing the size of CNNs.

</details>


### [607] [SimpleMatch: A Simple and Strong Baseline for Semantic Correspondence](https://arxiv.org/abs/2601.12357)
*Hailing Jin,Huiying Li*

Main category: cs.CV

TL;DR: 文章指出当前语义对应方法依赖高分辨率输入有计算开销大问题，提出SimpleMatch框架解决相邻关键点特征不可逆融合问题，在低分辨率下表现优。


<details>
  <summary>Details</summary>
Motivation: 当前语义对应方法依赖高分辨率输入图像，存在计算开销大的问题，且深层下采样操作会导致相邻关键点特征不可逆融合。

Method: 提出轻量级上采样解码器、多尺度监督损失，引入稀疏匹配和基于窗口的定位优化训练内存使用。

Result: 在252x252分辨率（比当前SOTA方法小3.3倍）下，SimpleMatch在SPair - 71k基准上PCK@0.1达84.1%。

Conclusion: 此框架为语义对应未来研究提供实用高效的基线。

Abstract: Recent advances in semantic correspondence have been largely driven by the use of pre-trained large-scale models. However, a limitation of these approaches is their dependence on high-resolution input images to achieve optimal performance, which results in considerable computational overhead. In this work, we address a fundamental limitation in current methods: the irreversible fusion of adjacent keypoint features caused by deep downsampling operations. This issue is triggered when semantically distinct keypoints fall within the same downsampled receptive field (e.g., 16x16 patches). To address this issue, we present SimpleMatch, a simple yet effective framework for semantic correspondence that delivers strong performance even at low resolutions. We propose a lightweight upsample decoder that progressively recovers spatial detail by upsampling deep features to 1/4 resolution, and a multi-scale supervised loss that ensures the upsampled features retain discriminative features across different spatial scales. In addition, we introduce sparse matching and window-based localization to optimize training memory usage and reduce it by 51%. At a resolution of 252x252 (3.3x smaller than current SOTA methods), SimpleMatch achieves superior performance with 84.1% PCK@0.1 on the SPair-71k benchmark. We believe this framework provides a practical and efficient baseline for future research in semantic correspondence. Code is available at: https://github.com/hailong23-jin/SimpleMatch.

</details>


### [608] [From Prompts to Pavement: LMMs-based Agentic Behavior-Tree Generation Framework for Autonomous Vehicles](https://arxiv.org/abs/2601.12358)
*Omar Y. Goba,Ahmed Y. Gado,Catherine M. Elias,Ahmed Hussein*

Main category: cs.CV

TL;DR: 本文提出利用大语言模型和多模态视觉模型动态生成和调整行为树的智能框架，在模拟中成功实现无人工干预绕开意外障碍物导航。


<details>
  <summary>Details</summary>
Motivation: 传统行为树具有静态性和手动调整工作量大的缺点，不适用于SAE 5级自动驾驶，需要自适应的行为规划器。

Method: 提出一个智能框架，包含描述符代理评估场景关键度、规划器代理构建高级子目标、生成器代理合成可执行行为树子树，并集成到CARLA+Nav2模拟中，仅在基线行为树失败时触发。

Result: 系统在模拟中成功实现无人工干预绕开意外障碍物（如街道堵塞）的导航。

Conclusion: 与静态行为树基线相比，该方法是一个概念验证，可扩展到不同驾驶场景。

Abstract: Autonomous vehicles (AVs) require adaptive behavior planners to navigate unpredictable, real-world environments safely. Traditional behavior trees (BTs) offer structured decision logic but are inherently static and demand labor-intensive manual tuning, limiting their applicability at SAE Level 5 autonomy. This paper presents an agentic framework that leverages large language models (LLMs) and multi-modal vision models (LVMs) to generate and adapt BTs on the fly. A specialized Descriptor agent applies chain-of-symbols prompting to assess scene criticality, a Planner agent constructs high-level sub-goals via in-context learning, and a Generator agent synthesizes executable BT sub-trees in XML format. Integrated into a CARLA+Nav2 simulation, our system triggers only upon baseline BT failure, demonstrating successful navigation around unexpected obstacles (e.g., street blockage) with no human intervention. Compared to a static BT baseline, this approach is a proof-of-concept that extends to diverse driving scenarios.

</details>


### [609] [Weaknesses of Facial Emotion Recognition Systems](https://arxiv.org/abs/2601.12402)
*Aleksandra Jamróz,Patrycja Wysocka,Piotr Garbat*

Main category: cs.CV

TL;DR: 对人脸情绪检测方法进行综述，选模型和数据集实验，揭示现有方案弱点。


<details>
  <summary>Details</summary>
Motivation: 人脸情绪检测方法多样，需深入综述相关文章和研究。

Method: 选择三种有趣且优的解决方案，选三个有多样图像的数据集，训练所选神经网络并实验。

Result: 揭示现有方案弱点，如数据集差异、特定情绪识别难度不均等和区分相似情绪有挑战。

Conclusion: 无明确体现，可推测现有模型有提升空间。

Abstract: Emotion detection from faces is one of the machine learning problems needed for human-computer interaction. The variety of methods used is enormous, which motivated an in-depth review of articles and scientific studies. Three of the most interesting and best solutions are selected, followed by the selection of three datasets that stood out for the diversity and number of images in them. The selected neural networks are trained, and then a series of experiments are performed to compare their performance, including testing on different datasets than a model was trained on. This reveals weaknesses in existing solutions, including differences between datasets, unequal levels of difficulty in recognizing certain emotions and the challenges in differentiating between closely related emotions.

</details>


### [610] [Adversarial Defense in Vision-Language Models: An Overview](https://arxiv.org/abs/2601.12443)
*Xiaowei Fu,Lei Zhang*

Main category: cs.CV

TL;DR: 文章关注视觉语言模型（VLMs）对抗攻击防御，介绍三种防御范式并进行综述。


<details>
  <summary>Details</summary>
Motivation: VLMs广泛使用，易受复杂隐蔽对抗攻击，影响模型性能和系统安全，需防御策略。

Method: 介绍三种防御范式，分别是训练时防御、测试时自适应防御和免训练防御。

Result: 梳理了VLMs对抗防御策略的最新进展，指出各方法优缺点。

Conclusion: 讨论了增强VLMs鲁棒性面临的持续挑战。

Abstract: The widespread use of Vision Language Models (VLMs, e.g. CLIP) has raised concerns about their vulnerability to sophisticated and imperceptible adversarial attacks. These attacks could compromise model performance and system security in cross-modal tasks. To address this challenge, three main defense paradigms have been proposed: Training-time Defense, Test-time Adaptation Defense, and Training-free Defense. Training-time Defense involves modifying the training process, typically through adversarial fine-tuning to improve the robustness to adversarial examples. While effective, this approach requires substantial computational resources and may not generalize across all adversarial attacks. Test-time Adaptation Defense focuses on adapting the model at inference time by updating its parameters to handle unlabeled adversarial examples, offering flexibility but often at the cost of increased complexity and computational overhead. Training-free Defense avoids modifying the model itself, instead focusing on altering the adversarial inputs or their feature embeddings, which enforces input perturbations to mitigate the impact of attacks without additional training. This survey reviews the latest advancements in adversarial defense strategies for VLMs, highlighting the strengths and limitations of such approaches and discussing ongoing challenges in enhancing the robustness of VLMs.

</details>


### [611] [Federated Joint Learning for Domain and Class Generalization](https://arxiv.org/abs/2601.12253)
*Haoran Xu,Jiaze Li,Jianzhong Ju,Zhenbo Luo*

Main category: cs.CV

TL;DR: 提出FedDCG方法解决联合学习中类和领域泛化问题，实验表明其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法未考虑联合框架解决未见类和未见领域问题，高效微调视觉语言模型很关键。

Method: 提出FedDCG方法，引入领域分组策略，训练类泛化网络，推理时基于领域相似度聚合结果，用可学习网络增强类泛化能力，用解耦机制分离知识。

Result: 在多个数据集上的大量实验显示，FedDCG在准确性和鲁棒性方面优于现有基线。

Conclusion: FedDCG是解决联合学习中类和领域泛化问题的有效方法。

Abstract: Efficient fine-tuning of visual-language models like CLIP has become crucial due to their large-scale parameter size and extensive pretraining requirements. Existing methods typically address either the issue of unseen classes or unseen domains in isolation, without considering a joint framework for both. In this paper, we propose \textbf{Fed}erated Joint Learning for \textbf{D}omain and \textbf{C}lass \textbf{G}eneralization, termed \textbf{FedDCG}, a novel approach that addresses both class and domain generalization in federated learning settings. Our method introduces a domain grouping strategy where class-generalized networks are trained within each group to prevent decision boundary confusion. During inference, we aggregate class-generalized results based on domain similarity, effectively integrating knowledge from both class and domain generalization. Specifically, a learnable network is employed to enhance class generalization capabilities, and a decoupling mechanism separates general and domain-specific knowledge, improving generalization to unseen domains. Extensive experiments across various datasets show that \textbf{FedDCG} outperforms state-of-the-art baselines in terms of accuracy and robustness.

</details>


### [612] [Encoding Emotion Through Self-Supervised Eye Movement Reconstruction](https://arxiv.org/abs/2601.12534)
*Marcus Ma,Jordan Prescott,Emily Zhou,Tiantian Feng,Kleanthis Avramidis,Gabor Mihaly Toth,Shrikanth Narayanan*

Main category: cs.CV

TL;DR: 研究利用低分辨率视频，通过新的注视检测模型预测情绪表达多模态标记，发现模型有预测性且预训练与情绪处理表现正相关，认为自监督眼动重建是有效编码情感信号方法。


<details>
  <summary>Details</summary>
Motivation: 多数研究使用专业高分辨率眼动追踪设备，限制研究成果应用范围，因此研究用低分辨率视频中眼动预测情绪表达多模态标记。

Method: 利用大屠杀幸存者视频访谈，受语言模型预训练方法启发，开发自监督眼动重建的注视检测模型，用其编码器嵌入微调两个下游情绪表达相关任务的模型。

Result: 新模型能预测情绪结果，两个实验中预训练表现与情绪处理表现呈正相关。

Conclusion: 自监督眼动重建是编码情感信号的有效方法。

Abstract: The relationship between emotional expression and eye movement is well-documented, with literature establishing gaze patterns are reliable indicators of emotion. However, most studies utilize specialized, high-resolution eye-tracking equipment, limiting the potential reach of findings. We investigate how eye movement can be used to predict multimodal markers of emotional expression from naturalistic, low-resolution videos. We utilize a collection of video interviews from the USC Shoah Foundation's Visual History Archive with Holocaust survivors as they recount their experiences in the Auschwitz concentration camp. Inspired by pretraining methods on language models, we develop a novel gaze detection model that uses self-supervised eye movement reconstruction that can effectively leverage unlabeled video. We use this model's encoder embeddings to fine-tune models on two downstream tasks related to emotional expression. The first is aligning eye movement with directional emotion estimates from speech. The second task is using eye gaze as a predictor of three momentary manifestations of emotional behaviors: laughing, crying/sobbing, and sighing. We find our new model is predictive of emotion outcomes and observe a positive correlation between pretraining performance and emotion processing performance for both experiments. We conclude self-supervised eye movement reconstruction is an effective method for encoding the affective signal they carry.

</details>


### [613] [Adaptive Multi-Scale Correlation Meta-Network for Few-Shot Remote Sensing Image Classification](https://arxiv.org/abs/2601.12308)
*Anurag Kaushish,Ayan Sar,Sampurna Roy,Sudeshna Chakraborty,Prashant Trivedi,Tanupriya Choudhury,Kanav Gupta*

Main category: cs.CV

TL;DR: 提出AMC - MetaNet框架解决遥感少样本学习难题，参数少、效率高，在多数据集上准确率高。


<details>
  <summary>Details</summary>
Motivation: 解决遥感少样本学习中标记数据稀缺、领域偏移和地理空间对象多尺度特性的问题。

Method: 引入Adaptive Multi - Scale Correlation Meta - Network (AMC - MetaNet)，有相关引导特征金字塔、自适应通道相关模块、相关引导元学习三项创新，且从头开始训练。

Result: 在EuroSAT、NWPU - RESISC45等多遥感数据集5 - way 5 - shot分类中准确率达86.65%，参数比ResNet - 18少20倍，单张图像推理时间<50ms。

Conclusion: AMC - MetaNet是适用于现实世界遥感少样本学习的计算高效、尺度感知框架。

Abstract: Few-shot learning in remote sensing remains challenging due to three factors: the scarcity of labeled data, substantial domain shifts, and the multi-scale nature of geospatial objects. To address these issues, we introduce Adaptive Multi-Scale Correlation Meta-Network (AMC-MetaNet), a lightweight yet powerful framework with three key innovations: (i) correlation-guided feature pyramids for capturing scale-invariant patterns, (ii) an adaptive channel correlation module (ACCM) for learning dynamic cross-scale relationships, and (iii) correlation-guided meta-learning that leverages correlation patterns instead of conventional prototype averaging. Unlike prior approaches that rely on heavy pre-trained models or transformers, AMC-MetaNet is trained from scratch with only $\sim600K$ parameters, offering $20\times$ fewer parameters than ResNet-18 while maintaining high efficiency ($<50$ms per image inference). AMC-MetaNet achieves up to 86.65\% accuracy in 5-way 5-shot classification on various remote sensing datasets, including EuroSAT, NWPU-RESISC45, UC Merced Land Use, and AID. Our results establish AMC-MetaNet as a computationally efficient, scale-aware framework for real-world few-shot remote sensing.

</details>


### [614] [Mixed Precision PointPillars for Efficient 3D Object Detection with TensorRT](https://arxiv.org/abs/2601.12638)
*Ninnart Fuengfusin,Keisuke Yoneda,Naoki Suganuma*

Main category: cs.CV

TL;DR: 为解决LIDAR 3D目标检测实时性和模型量化性能下降问题，提出混合精度框架，实现低延迟和小尺寸模型。


<details>
  <summary>Details</summary>
Motivation: LIDAR 3D目标检测需实时性，直接模型量化因LIDAR数值分布和异常值导致性能下降。

Method: 提出适用于PointPillars的混合精度框架，用PTQ搜索敏感层设为FP，贪心搜索组合模型并用PTQ或QAT确定；用少量校准数据处理异常值。

Result: PTQ管道无需训练提供混合精度模型，QAT管道性能与FP模型相当；TensorRT部署后模型延迟和尺寸分别最多降低2.35和2.26倍。

Conclusion: 所提方法有效解决LIDAR 3D目标检测模型量化问题，提升模型性能和效率。

Abstract: LIDAR 3D object detection is one of the important tasks for autonomous vehicles. Ensuring that this task operates in real-time is crucial. Toward this, model quantization can be used to accelerate the runtime. However, directly applying model quantization often leads to performance degradation due to LIDAR's wide numerical distributions and extreme outliers. To address the wide numerical distribution, we proposed a mixed precision framework designed for PointPillars. Our framework first searches for sensitive layers with post-training quantization (PTQ) by quantizing one layer at a time to 8-bit integer (INT8) and evaluating each model for average precision (AP). The top-k most sensitive layers are assigned as floating point (FP). Combinations of these layers are greedily searched to produce candidate mixed precision models, which are finalized with either PTQ or quantization-aware training (QAT). Furthermore, to handle outliers, we observe that using a very small number of calibration data reduces the likelihood of encountering outliers, thereby improving PTQ performance. Our methods provides mixed precision models without training in the PTQ pipeline, while our QAT pipeline achieves the performance competitive to FP models. With TensorRT deployment, our models offer less latency and sizes by up to 2.35 and 2.26 times, respectively.

</details>


### [615] [SDCoNet: Saliency-Driven Multi-Task Collaborative Network for Remote Sensing Object Detection](https://arxiv.org/abs/2601.12507)
*Ruo Qi,Linhui Dai,Yusong Qin,Chaolei Yang,Yanshan Li*

Main category: cs.CV

TL;DR: 提出Saliency - Driven多任务协作网络SDCoNet用于低质量遥感图像小目标检测，融合SR与检测，在几个公共数据集实验显示其效果优且计算效率有竞争力。


<details>
  <summary>Details</summary>
Motivation: 遥感图像复杂背景等因素使检测难，现有串行SR与检测方案存在优化目标不一致等问题。

Method: 提出SDCoNet，用基于Swin Transformer的共享编码器支持跨任务特征协作，多尺度显著性预测模块选择关键令牌，引入梯度路由策略缓解优化冲突。

Result: 在NWPU VHR - 10 - Split等数据集实验表明，该方法对比现有主流算法在低质量遥感图像小目标检测表现更好，且计算效率有竞争力。

Conclusion: SDCoNet能有效解决现有方案问题，在低质量遥感图像小目标检测上有良好效果。

Abstract: In remote sensing images, complex backgrounds, weak object signals, and small object scales make accurate detection particularly challenging, especially under low-quality imaging conditions. A common strategy is to integrate single-image super-resolution (SR) before detection; however, such serial pipelines often suffer from misaligned optimization objectives, feature redundancy, and a lack of effective interaction between SR and detection. To address these issues, we propose a Saliency-Driven multi-task Collaborative Network (SDCoNet) that couples SR and detection through implicit feature sharing while preserving task specificity. SDCoNet employs the swin transformer-based shared encoder, where hierarchical window-shifted self-attention supports cross-task feature collaboration and adaptively balances the trade-off between texture refinement and semantic representation. In addition, a multi-scale saliency prediction module produces importance scores to select key tokens, enabling focused attention on weak object regions, suppression of background clutter, and suppression of adverse features introduced by multi-task coupling. Furthermore, a gradient routing strategy is introduced to mitigate optimization conflicts. It first stabilizes detection semantics and subsequently routes SR gradients along a detection-oriented direction, enabling the framework to guide the SR branch to generate high-frequency details that are explicitly beneficial for detection. Experiments on public datasets, including NWPU VHR-10-Split, DOTAv1.5-Split, and HRSSD-Split, demonstrate that the proposed method, while maintaining competitive computational efficiency, significantly outperforms existing mainstream algorithms in small object detection on low-quality remote sensing images. Our code is available at https://github.com/qiruo-ya/SDCoNet.

</details>


### [616] [Generalizable Hyperparameter Optimization for Federated Learning on Non-IID Cancer Images](https://arxiv.org/abs/2601.12664)
*Elisa Gonçalves Ribeiro,Rodrigo Moreira,Larissa Ferreira Rodrigues Moreira,André Ricardo Backes*

Main category: cs.CV

TL;DR: 论文探讨癌症组织病理学深度学习的联邦学习中，超参数在非IID场景的泛化性，提出跨数据集聚合启发式方法实现有竞争力的分类性能。


<details>
  <summary>Details</summary>
Motivation: 癌症组织病理学深度学习训练受隐私约束，联邦学习虽可缓解但性能依赖非IID客户端数据集的超参数选择，需研究超参数泛化性。

Method: 进行集中式贝叶斯超参数优化，并将数据集特定的最优解转移到非IID联邦设置，采用简单跨数据集聚合启发式方法。

Result: 提出的结合学习率平均、考虑模态优化器和批量大小的组合配置实现了有竞争力的分类性能。

Conclusion: 所提出的跨数据集聚合启发式方法在非IID联邦场景下有效，超参数组合配置有良好表现。

Abstract: Deep learning for cancer histopathology training conflicts with privacy constraints in clinical settings. Federated Learning (FL) mitigates this by keeping data local; however, its performance depends on hyperparameter choices under non-independent and identically distributed (non-IID) client datasets. This paper examined whether hyperparameters optimized on one cancer imaging dataset generalized across non-IID federated scenarios. We considered binary histopathology tasks for ovarian and colorectal cancers. We perform centralized Bayesian hyperparameter optimization and transfer dataset-specific optima to the non-IID FL setup. The main contribution of this study is the introduction of a simple cross-dataset aggregation heuristic by combining configurations by averaging the learning rates and considering the modal optimizers and batch sizes. This combined configuration achieves a competitive classification performance.

</details>


### [617] [Exploiting Test-Time Augmentation in Federated Learning for Brain Tumor MRI Classification](https://arxiv.org/abs/2601.12671)
*Thamara Leandra de Deus Melo,Rodrigo Moreira,Larissa Ferreira Rodrigues Moreira,André Ricardo Backes*

Main category: cs.CV

TL;DR: 评估联邦学习下CNN对脑肿瘤诊断效果，发现预处理结合TTA可显著提升分类效果。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤高效诊断因病变差异和图像复杂具有挑战性，需评估CNN在联邦学习中的表现。

Method: 评估联邦学习下CNN，对比原始和预处理MRI图像训练的模型，结合测试时增强（TTA）。

Result: 预处理单独使用效果不佳，与TTA结合在联邦MRI分类中有显著改善（p<0.001）。

Conclusion: TTA应作为基于联邦学习的医学成像默认推理策略，计算资源允许时，TTA结合轻度预处理有额外可靠提升。

Abstract: Efficient brain tumor diagnosis is crucial for early treatment; however, it is challenging because of lesion variability and image complexity. We evaluated convolutional neural networks (CNNs) in a federated learning (FL) setting, comparing models trained on original versus preprocessed MRI images (resizing, grayscale conversion, normalization, filtering, and histogram equalization). Preprocessing alone yielded negligible gains; combined with test-time augmentation (TTA), it delivered consistent, statistically significant improvements in federated MRI classification (p<0.001). In practice, TTA should be the default inference strategy in FL-based medical imaging; when the computational budget permits, pairing TTA with light preprocessing provides additional reliable gains.

</details>


### [618] [RSOD: Reliability-Guided Sonar Image Object Detection with Extremely Limited Labels](https://arxiv.org/abs/2601.12715)
*Chengzhou Li,Ping Guo,Guanchen Meng,Qi Jia,Jinyuan Liu,Zhu Liu,Xiaokang Liu,Yu Liu,Zhongxuan Luo,Xin Fan*

Main category: cs.CV

TL;DR: 提出RSOD教师 - 学生框架处理声纳图像有限标签目标检测问题，利用无标签数据，在UATD数据集表现好，还收集新数据集。


<details>
  <summary>Details</summary>
Motivation: 声纳图像纹理少、易受噪声影响，非专家难提供精确标注数据，需设计有限标签下有效的声纳图像目标检测方法。

Method: 提出RSOD框架，计算可靠性分数，引入对象混合伪标签方法，实施可靠性引导自适应约束。

Result: 在UATD数据集上，仅用5%标注数据的结果可与用100%标注数据的基线算法竞争。

Conclusion: 该方法能充分利用无标签数据，在有限标签下表现良好，新数据集可为声纳领域研究提供更多数据。

Abstract: Object detection in sonar images is a key technology in underwater detection systems. Compared to natural images, sonar images contain fewer texture details and are more susceptible to noise, making it difficult for non-experts to distinguish subtle differences between classes. This leads to their inability to provide precise annotation data for sonar images. Therefore, designing effective object detection methods for sonar images with extremely limited labels is particularly important. To address this, we propose a teacher-student framework called RSOD, which aims to fully learn the characteristics of sonar images and develop a pseudo-label strategy suitable for these images to mitigate the impact of limited labels. First, RSOD calculates a reliability score by assessing the consistency of the teacher's predictions across different views. To leverage this score, we introduce an object mixed pseudo-label method to tackle the shortage of labeled data in sonar images. Finally, we optimize the performance of the student by implementing a reliability-guided adaptive constraint. By taking full advantage of unlabeled data, the student can perform well even in situations with extremely limited labels. Notably, on the UATD dataset, our method, using only 5% of labeled data, achieves results that can compete against those of our baseline algorithm trained on 100% labeled data. We also collected a new dataset to provide more valuable data for research in the field of sonar.

</details>


### [619] [P2L-CA: An Effective Parameter Tuning Framework for Rehearsal-Free Multi-Label Class-Incremental Learning](https://arxiv.org/abs/2601.12714)
*Songlin Dong,Jiangyang Li,Chenhao Ding,Zhiheng Ma,Haoyu Luo,Yuhang He,Yihong Gong*

Main category: cs.CV

TL;DR: 提出P2L - CA参数高效框架用于多标签增量学习，实验显示效果好且所需可训练参数少、无需内存缓冲区。


<details>
  <summary>Details</summary>
Motivation: 现有多标签类别增量学习方法存在计算成本高、存储开销大、难以解决特征混淆和领域差异等问题。

Method: 引入P2L - CA框架，结合Prompt - to - Label模块利用特定类别的提示解开多标签表示并融入语言先验，Continuous Adapter模块使用轻量级适配器减轻领域差距。

Result: 在MS - COCO和PASCAL VOC数据集上，在标准和具有挑战性的多标签类别增量学习设置的广泛实验中，P2L - CA比现有方法有显著改进，在类别增量学习场景中表现出强泛化性。

Conclusion: P2L - CA是一种有效的多标签类别增量学习解决方案，能在减少计算和存储成本的情况下取得良好效果。

Abstract: Multi-label Class-Incremental Learning aims to continuously recognize novel categories in complex scenes where multiple objects co-occur. However, existing approaches often incur high computational costs due to full-parameter fine-tuning and substantial storage overhead from memory buffers, or they struggle to address feature confusion and domain discrepancies adequately. To overcome these limitations, we introduce P2L-CA, a parameter-efficient framework that integrates a Prompt-to-Label module with a Continuous Adapter module. The P2L module leverages class-specific prompts to disentangle multi-label representations while incorporating linguistic priors to enforce stable semantic-visual alignment. Meanwhile, the CA module employs lightweight adapters to mitigate domain gaps between pre-trained models and downstream tasks, thereby enhancing model plasticity. Extensive experiments across standard and challenging MLCIL settings on MS-COCO and PASCAL VOC show that P2L-CA not only achieves substantial improvements over state-of-the-art methods but also demonstrates strong generalization in CIL scenarios, all while requiring minimal trainable parameters and eliminating the need for memory buffers.

</details>


### [620] [Left-Right Symmetry Breaking in CLIP-style Vision-Language Models Trained on Synthetic Spatial-Relation Data](https://arxiv.org/abs/2601.12809)
*Takaki Yamamoto,Chihiro Noguchi,Toshihiro Tanizawa*

Main category: cs.CV

TL;DR: 提出可控一维图像文本测试平台，探究基于Transformer的视觉和文本编码器中左右关系理解的出现机制，发现对比训练能学习左右关系，标签多样性是泛化的主要驱动因素。


<details>
  <summary>Details</summary>
Motivation: 空间理解是视觉语言模型的关键挑战，不清楚是否真正获得这种理解以及通过何种机制。

Method: 构建可控一维图像文本测试平台，端到端训练轻量级基于Transformer的视觉和文本编码器，评估对未见对象对的泛化能力，进行注意力分解。

Result: 对比训练能学习左右关系，标签多样性比布局多样性更能驱动泛化，位置和标记嵌入的相互作用会产生水平注意力梯度。

Conclusion: 为CLIP风格模型何时以及如何获得关系能力提供了机制性见解。

Abstract: Spatial understanding remains a key challenge in vision-language models. Yet it is still unclear whether such understanding is truly acquired, and if so, through what mechanisms. We present a controllable 1D image-text testbed to probe how left-right relational understanding emerges in Transformer-based vision and text encoders trained with a CLIP-style contrastive objective. We train lightweight Transformer-based vision and text encoders end-to-end on paired descriptions of one- and two-object scenes and evaluate generalization to unseen object pairs while systematically varying label and layout diversity. We find that contrastive training learns left-right relations and that label diversity, more than layout diversity, is the primary driver of generalization in this setting. To gain the mechanistic understanding, we perform an attention decomposition and show that interactions between positional and token embeddings induce a horizontal attention gradient that breaks left-right symmetry in the encoders; ablating this contribution substantially reduces left-right discrimination. Our results provide a mechanistic insight of when and how CLIP-style models acquire relational competence.

</details>


### [621] [YOLO26: An Analysis of NMS-Free End to End Framework for Real-Time Object Detection](https://arxiv.org/abs/2601.12882)
*Sudip Chakrabarty*

Main category: cs.CV

TL;DR: 本文分析YOLO26架构，其去除NMS采用端到端学习策略，通过多项创新技术提升性能，在推理速度和检测精度上超越前任和竞品。


<details>
  <summary>Details</summary>
Motivation: 传统YOLO迭代因非极大值抑制（NMS）后处理的延迟和超参数敏感性受限，需新架构突破。

Method: 分析YOLO26架构，引入MuSGD优化器稳定轻量级主干，采用STAL进行小目标感知分配，使用ProgLoss进行动态监督。

Result: 通过官方性能基准测试，YOLO26在推理速度和检测精度上超越诸多前辈和竞品，建立了新的帕累托前沿。

Conclusion: YOLO26通过解耦表征学习和启发式后处理，解决了延迟和精度之间的权衡问题，是基于边缘的计算机视觉的下一步发展。

Abstract: The "You Only Look Once" (YOLO) framework has long served as the benchmark for real-time object detection, yet traditional iterations (YOLOv1 through YOLO11) remain constrained by the latency and hyperparameter sensitivity of Non-Maximum Suppression (NMS) post-processing. This paper analyzes a comprehensive analysis of YOLO26, an architecture that fundamentally redefines this paradigm by eliminating NMS in favor of a native end-to-end learning strategy. This study examines the critical innovations that enable this transition, specifically the introduction of the MuSGD optimizer for stabilizing lightweight backbones, STAL for small-target-aware assignment, and ProgLoss for dynamic supervision. Through a systematic review of official performance benchmarks, the results demonstrate that YOLO26 establishes a new Pareto front, outperforming a comprehensive suite of predecessors and state-of-the-art competitors (including RTMDet and DAMO-YOLO) in both inference speed and detection accuracy. The analysis confirms that by decoupling representation learning from heuristic post-processing, YOLOv26 successfully resolves the historical trade-off between latency and precision, signaling the next evolutionary step in edge-based computer vision.

</details>


### [622] [TwoHead-SwinFPN: A Unified DL Architecture for Synthetic Manipulation, Detection and Localization in Identity Documents](https://arxiv.org/abs/2601.12895)
*Chan Naseeb,Adeel Ashraf Cheema,Hassan Sami,Tayyab Afzal,Muhammad Omair,Usman Habib*

Main category: cs.CV

TL;DR: 本文提出TwoHead - SwinFPN架构用于检测和定位身份文件中的合成操作区域，在FantasyIDiap数据集上表现出色，兼顾性能与效率。


<details>
  <summary>Details</summary>
Motivation: 生成式AI模型增加了身份文件合成操作威胁，需有效检测和定位方法。

Method: 提出TwoHead - SwinFPN架构，集成Swin Transformer、FPN和UNet风格解码器，用CBAM增强特征表示，采用双头部架构和不确定性加权多任务学习。

Result: 在FantasyIDiap数据集上，分类准确率84.31%、AUC 90.78%，定位平均Dice分数57.24%，二元分类F1分数88.61%，通过FastAPI实现适合实际部署。

Conclusion: TwoHead - SwinFPN架构在检测和定位身份文件合成操作区域上性能优越，且计算效率适合实际应用。

Abstract: The proliferation of sophisticated generative AI models has significantly escalated the threat of synthetic manipulations in identity documents, particularly through face swapping and text inpainting attacks. This paper presents TwoHead-SwinFPN, a unified deep learning architecture that simultaneously performs binary classification and precise localization of manipulated regions in ID documents. Our approach integrates a Swin Transformer backbone with Feature Pyramid Network (FPN) and UNet-style decoder, enhanced with Convolutional Block Attention Module (CBAM) for improved feature representation. The model employs a dual-head architecture for joint optimization of detection and segmentation tasks, utilizing uncertainty-weighted multi-task learning. Extensive experiments on the FantasyIDiap dataset demonstrate superior performance with 84.31\% accuracy, 90.78\% AUC for classification, and 57.24\% mean Dice score for localization. The proposed method achieves an F1-score of 88.61\% for binary classification while maintaining computational efficiency suitable for real-world deployment through FastAPI implementation. Our comprehensive evaluation includes ablation studies, cross-device generalization analysis, and detailed performance assessment across 10 languages and 3 acquisition devices.

</details>


### [623] [Membership Inference Test: Auditing Training Data in Object Classification Models](https://arxiv.org/abs/2601.12929)
*Gonzalo Mancera,Daniel DeAlcala,Aythami Morales,Ruben Tolosana,Julian Fierrez*

Main category: cs.CV

TL;DR: 研究聚焦对象识别中成员推理测试（MINT）性能，提出定制架构，实验达70%-80%精度并分析影响因素。


<details>
  <summary>Details</summary>
Motivation: 分析对象识别中MINT性能，解决该领域数据利用的复杂性问题。

Method: 提出适用于MINT模型的架构，对对象检测模型、嵌入提取器和MINT模块进行实验，利用卷积层建模训练数据激活模式。

Result: 能识别测试和训练数据，精度达70%-80%，取决于检测模块层深度。

Conclusion: 架构可优化对象识别中MINT性能与效率，研究分析了影响MINT模块因素和训练过程透明性要素。

Abstract: In this research, we analyze the performance of Membership Inference Tests (MINT), focusing on determining whether given data were utilized during the training phase, specifically in the domain of object recognition. Within the area of object recognition, we propose and develop architectures tailored for MINT models. These architectures aim to optimize performance and efficiency in data utilization, offering a tailored solution to tackle the complexities inherent in the object recognition domain. We conducted experiments involving an object detection model, an embedding extractor, and a MINT module. These experiments were performed in three public databases, totaling over 174K images. The proposed architecture leverages convolutional layers to capture and model the activation patterns present in the data during the training process. Through our analysis, we are able to identify given data used for testing and training, achieving precision rates ranging between 70% and 80%, contingent upon the depth of the detection module layer chosen for input to the MINT module. Additionally, our studies entail an analysis of the factors influencing the MINT Module, delving into the contributing elements behind more transparent training processes.

</details>


### [624] [Early Prediction of Type 2 Diabetes Using Multimodal data and Tabular Transformers](https://arxiv.org/abs/2601.12981)
*Sulaiman Khan,Md. Rafiul Biswas,Zubair Shah*

Main category: cs.CV

TL;DR: 研究用TabTrans架构分析纵向患者数据进行2型糖尿病早期风险预测，在QBB队列验证，模型表现优，确定关键风险指标。


<details>
  <summary>Details</summary>
Motivation: 传统方法常忽略疾病进展中复杂的长期依赖关系，需要新方法进行2型糖尿病早期风险预测。

Method: 使用TabTrans架构分析纵向患者数据，集成电子健康记录和DXA数据，用SMOTE和SMOTE - ENN处理类别不平衡，与传统机器学习和生成式AI模型对比。

Result: TabTrans模型预测性能优越，ROC AUC≥79.7%，特征解释分析确定关键风险指标。

Conclusion: TabTrans在分析复杂医疗表格数据方面有显著潜力，可为卡塔尔人群2型糖尿病管理和个性化干预提供有力工具。

Abstract: This study introduces a novel approach for early Type 2 Diabetes Mellitus (T2DM) risk prediction using a tabular transformer (TabTrans) architecture to analyze longitudinal patient data. By processing patients` longitudinal health records and bone-related tabular data, our model captures complex, long-range dependencies in disease progression that conventional methods often overlook. We validated our TabTrans model on a retrospective Qatar BioBank (QBB) cohort of 1,382 subjects, comprising 725 men (146 diabetic, 579 healthy) and 657 women (133 diabetic, 524 healthy). The study integrated electronic health records (EHR) with dual-energy X-ray absorptiometry (DXA) data. To address class imbalance, we employed SMOTE and SMOTE-ENN resampling techniques. The proposed model`s performance is evaluated against conventional machine learning (ML) and generative AI models, including Claude 3.5 Sonnet (Anthropic`s constitutional AI), GPT-4 (OpenAI`s generative pre-trained transformer), and Gemini Pro (Google`s multimodal language model). Our TabTrans model demonstrated superior predictive performance, achieving ROC AUC $\geq$ 79.7 % for T2DM prediction compared to both generative AI models and conventional ML approaches. Feature interpretation analysis identified key risk indicators, with visceral adipose tissue (VAT) mass and volume, ward bone mineral density (BMD) and bone mineral content (BMC), T and Z-scores, and L1-L4 scores emerging as the most important predictors associated with diabetes development in Qatari adults. These findings demonstrate the significant potential of TabTrans for analyzing complex tabular healthcare data, providing a powerful tool for proactive T2DM management and personalized clinical interventions in the Qatari population.
  Index Terms: tabular transformers, multimodal data, DXA data, diabetes, T2DM, feature interpretation, tabular data

</details>


### [625] [From 100,000+ images to winning the first brain MRI foundation model challenges: Sharing lessons and models](https://arxiv.org/abs/2601.13166)
*Pedro M. Gordaliza,Jaume Banus,Benoît Gérin,Maxence Wynen,Nataliia Molchanova,Jonas Richiardi,Meritxell Bach Cuadra*

Main category: cs.CV

TL;DR: 本文介绍在MICCAI 2025的3D脑MRI挑战赛中排名第一的解决方案，其模型训练快、体积小，代码开源。


<details>
  <summary>Details</summary>
Motivation: 应对放射学任务挑战，开发用于医学图像分析的基础模型。

Method: 采用U - Net CNN架构，结合利用解剖先验和神经影像领域知识的策略。

Result: 在两项挑战赛的赛道中均排名第一，模型比基于Transformer的竞争方法训练快1 - 2个数量级，体积小10倍。

Conclusion: 所提出的方法在医学图像分析挑战赛中表现出色，具有训练速度快和模型体积小的优势。

Abstract: Developing Foundation Models for medical image analysis is essential to overcome the unique challenges of radiological tasks. The first challenges of this kind for 3D brain MRI, SSL3D and FOMO25, were held at MICCAI 2025. Our solution ranked first in tracks of both contests. It relies on a U-Net CNN architecture combined with strategies leveraging anatomical priors and neuroimaging domain knowledge. Notably, our models trained 1-2 orders of magnitude faster and were 10 times smaller than competing transformer-based approaches. Models are available here: https://github.com/jbanusco/BrainFM4Challenges.

</details>


### [626] [TVWorld: Foundations for Remote-Control TV Agents](https://arxiv.org/abs/2601.13142)
*Zhantao Ma,Quanfeng Lu,Shuai Zhong,Dahai Yu,Ping Luo,Michael K. Ng*

Main category: cs.CV

TL;DR: 现有大视觉语言模型在设备控制有潜力，但电视遥控交互研究不足。提出TVWorld及相关基准，发现现有模型拓扑感知不足，提出Topology - Aware Training框架构建TVTheseus，在TVWorld - N上表现超基线。


<details>
  <summary>Details</summary>
Motivation: 现有关于大视觉语言模型在设备控制的研究主要集中于点按交互，电视遥控交互研究不足，需填补该研究空白。

Method: 引入TVWorld进行可复现无部署评估，推导TVWorld - N和TVWorld - G两个基准，提出Topology - Aware Training框架注入拓扑感知，开发TVTheseus模型。

Result: TVTheseus在TVWorld - N上成功率达68.3%，超越Gemini 3 Flash等基线。

Conclusion: 研究为开发有效的电视使用代理提供了有价值的见解。

Abstract: Recent large vision-language models (LVLMs) have demonstrated strong potential for device control. However, existing research has primarily focused on point-and-click (PnC) interaction, while remote-control (RC) interaction commonly encountered in everyday TV usage remains largely underexplored. To fill this gap, we introduce \textbf{TVWorld}, an offline graph-based abstraction of real-world TV navigation that enables reproducible and deployment-free evaluation. On this basis, we derive two complementary benchmarks that comprehensively assess TV-use capabilities: \textbf{TVWorld-N} for topology-aware navigation and \textbf{TVWorld-G} for focus-aware grounding. These benchmarks expose a key limitation of existing agents: insufficient topology awareness for focus-based, long-horizon TV navigation. Motivated by this finding, we propose a \emph{Topology-Aware Training} framework that injects topology awareness into LVLMs. Using this framework, we develop \textbf{TVTheseus}, a foundation model specialized for TV navigation. TVTheseus achieves a success rate of $68.3\%$ on TVWorld-N, surpassing strong closed-source baselines such as Gemini 3 Flash and establishing state-of-the-art (SOTA) performance. Additional analyses further provide valuable insights into the development of effective TV-use agents.

</details>


### [627] [A Semantic Decoupling-Based Two-Stage Rainy-Day Attack for Revealing Weather Robustness Deficiencies in Vision-Language Models](https://arxiv.org/abs/2601.13238)
*Chengyin Hu,Xiang Chen,Zhe Jia,Weiwen Shi,Fengyu Zhang,Jiujiang Guo,Yiwei Wei*

Main category: cs.CV

TL;DR: 提出首个利用真实天气攻击视觉语言模型的对抗框架，实验表明天气扰动会造成模型语义失调。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在真实天气条件下的鲁棒性和跨模态语义对齐稳定性研究不足，聚焦降雨场景进行研究。

Method: 采用基于语义解耦的两阶段参数化扰动模型，第一阶段对嵌入空间进行低维全局调制，第二阶段显式建模雨滴外观与光照变化并优化天气空间。

Result: 多任务实验显示合理的天气扰动会使主流视觉语言模型出现语义失调，消融实验表明光照建模和多尺度雨滴结构是语义偏移的关键因素。

Conclusion: 真实天气扰动会给视觉语言模型的实际部署带来安全和可靠性风险。

Abstract: Vision-Language Models (VLMs) are trained on image-text pairs collected under canonical visual conditions and achieve strong performance on multimodal tasks. However, their robustness to real-world weather conditions, and the stability of cross-modal semantic alignment under such structured perturbations, remain insufficiently studied. In this paper, we focus on rainy scenarios and introduce the first adversarial framework that exploits realistic weather to attack VLMs, using a two-stage, parameterized perturbation model based on semantic decoupling to analyze rain-induced shifts in decision-making. In Stage 1, we model the global effects of rainfall by applying a low-dimensional global modulation to condition the embedding space and gradually weaken the original semantic decision boundaries. In Stage 2, we introduce structured rain variations by explicitly modeling multi-scale raindrop appearance and rainfall-induced illumination changes, and optimize the resulting non-differentiable weather space to induce stable semantic shifts. Operating in a non-pixel parameter space, our framework generates perturbations that are both physically grounded and interpretable. Experiments across multiple tasks show that even physically plausible, highly constrained weather perturbations can induce substantial semantic misalignment in mainstream VLMs, posing potential safety and reliability risks in real-world deployment. Ablations further confirm that illumination modeling and multi-scale raindrop structures are key drivers of these semantic shifts.

</details>


### [628] [MultiST: A Cross-Attention-Based Multimodal Model for Spatial Transcriptomic](https://arxiv.org/abs/2601.13331)
*Wei Wang,Quoc-Toan Ly,Chong Yu,Jun Bai*

Main category: cs.CV

TL;DR: 本文提出MultiST框架，有效整合组织学形态与分子特征，在不同ST数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有空间转录组学方法缺乏有效整合组织学形态与分子特征，难以解决空间域边界模糊问题。

Method: 提出MultiST统一多模态框架，通过基于交叉注意力的融合联合建模空间拓扑、基因表达和组织形态，使用基于图的基因编码器和对抗性对齐学习空间表示，集成颜色归一化的组织学特征。

Result: 在13个不同ST数据集上评价，MultiST划分的空间域边界更清晰连贯，能得到更稳定的假时间轨迹和更具生物学可解释性的细胞-细胞相互作用模式。

Conclusion: MultiST框架在空间转录组学分析中表现良好，能有效解决现有方法的局限性。

Abstract: Spatial transcriptomics (ST) enables transcriptome-wide profiling while preserving the spatial context of tissues, offering unprecedented opportunities to study tissue organization and cell-cell interactions in situ. Despite recent advances, existing methods often lack effective integration of histological morphology with molecular profiles, relying on shallow fusion strategies or omitting tissue images altogether, which limits their ability to resolve ambiguous spatial domain boundaries. To address this challenge, we propose MultiST, a unified multimodal framework that jointly models spatial topology, gene expression, and tissue morphology through cross-attention-based fusion. MultiST employs graph-based gene encoders with adversarial alignment to learn robust spatial representations, while integrating color-normalized histological features to capture molecular-morphological dependencies and refine domain boundaries. We evaluated the proposed method on 13 diverse ST datasets spanning two organs, including human brain cortex and breast cancer tissue. MultiST yields spatial domains with clearer and more coherent boundaries than existing methods, leading to more stable pseudotime trajectories and more biologically interpretable cell-cell interaction patterns. The MultiST framework and source code are available at https://github.com/LabJunBMI/MultiST.git.

</details>


### [629] [Organ-Aware Attention Improves CT Triage and Classification](https://arxiv.org/abs/2601.13385)
*Lavsen Dahal,Yubraj Bhandari,Geoffrey D. Rubin,Joseph Y. Lo*

Main category: cs.CV

TL;DR: 针对CT影像分诊分类需求，改进现有模型，提出ORACLE - CT，在胸部和腹部CT分类中取得先进表现，源码公开。


<details>
  <summary>Details</summary>
Motivation: 高容量医学影像（如CT）分诊分类可改善患者护理、减轻放射科医生负担，但现有视觉语言模型在3D解剖、协议转移和嘈杂报告监督方面存在问题。

Method: 使用CT - RATE和RADCHEST - CT数据集，建立监督基线模型，提出ORACLE - CT，结合器官掩码注意力和器官标量融合。

Result: 胸部CT上ORACLE - CT掩码注意力模型AUROC达0.86；腹部CT上监督基线超零样本VLM基线，添加掩码注意力和标量融合后AUROC达0.85。

Conclusion: 在统一评估协议下，模型在胸部和腹部CT分类中实现了先进的监督分类性能。

Abstract: There is an urgent need for triage and classification of high-volume medical imaging modalities such as computed tomography (CT), which can improve patient care and mitigate radiologist burnout. Study-level CT triage requires calibrated predictions with localized evidence; however, off-the-shelf Vision Language Models (VLM) struggle with 3D anatomy, protocol shifts, and noisy report supervision. This study used the two largest publicly available chest CT datasets: CT-RATE and RADCHEST-CT (held-out external test set). Our carefully tuned supervised baseline (instantiated as a simple Global Average Pooling head) establishes a new supervised state of the art, surpassing all reported linear-probe VLMs. Building on this baseline, we present ORACLE-CT, an encoder-agnostic, organ-aware head that pairs Organ-Masked Attention (mask-restricted, per-organ pooling that yields spatial evidence) with Organ-Scalar Fusion (lightweight fusion of normalized volume and mean-HU cues). In the chest setting, ORACLE-CT masked attention model achieves AUROC 0.86 on CT-RATE; in the abdomen setting, on MERLIN (30 findings), our supervised baseline exceeds a reproduced zero-shot VLM baseline obtained by running publicly released weights through our pipeline, and adding masked attention plus scalar fusion further improves performance to AUROC 0.85. Together, these results deliver state-of-the-art supervised classification performance across both chest and abdomen CT under a unified evaluation protocol. The source code is available at https://github.com/lavsendahal/oracle-ct.

</details>


### [630] [Deep Image Prior with L0 Gradient Regularizer for Image Smoothing](https://arxiv.org/abs/2601.13400)
*Nhat Thanh Tran,Kevin Bui,Jack Xin*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Image smoothing is a fundamental image processing operation that preserves the underlying structure, such as strong edges and contours, and removes minor details and textures in an image. Many image smoothing algorithms rely on computing local window statistics or solving an optimization problem. Recent state-of-the-art methods leverage deep learning, but they require a carefully curated training dataset. Because constructing a proper training dataset for image smoothing is challenging, we propose DIP-$\ell_0$, a deep image prior framework that incorporates the $\ell_0$ gradient regularizer. This framework can perform high-quality image smoothing without any training data. To properly minimize the associated loss function that has the nonconvex, nonsmooth $\ell_0$ ``norm", we develop an alternating direction method of multipliers algorithm that utilizes an off-the-shelf $\ell_0$ gradient minimization solver. Numerical experiments demonstrate that the proposed DIP-$\ell_0$ outperforms many image smoothing algorithms in edge-preserving image smoothing and JPEG artifact removal.

</details>


### [631] [Reasoning with Pixel-level Precision: QVLM Architecture and SQuID Dataset for Quantitative Geospatial Analytics](https://arxiv.org/abs/2601.13401)
*Peter A. Massih,Eric Cosatto*

Main category: cs.CV

TL;DR: 当前视觉语言模型在定量空间推理上存在问题，本文提出SQuID数据集和QVLM模型，实验显示QVLM效果更好，揭示架构解耦对定量任务的作用。


<details>
  <summary>Details</summary>
Motivation: 解决当前视觉语言模型（VLMs）在定量空间推理方面的不足，因其架构破坏了计数和测量所需的像素级信息。

Method: 1. 引入SQuID数据集用于评估定量空间推理；2. 提出QVLM模型，通过将语言理解和视觉分析解耦来保持像素精度，生成可执行代码操作像素级掩码。

Result: 使用GPT - 5作为编码器的QVLM在SQuID上的准确率达到42.0%，而使用图像 - 问题对提示的VLM准确率为28.1%。

Conclusion: 对于定量空间推理，架构解耦能提高定量任务的准确性。

Abstract: Current Vision-Language Models (VLMs) fail at quantitative spatial reasoning because their architectures destroy pixel-level information required for counting and measurements. Vision encoders compress images through patch embeddings, reducing spatial indexing and losing the precise pixel-level tracking required for accurate counting. We present two contributions to address this fundamental limitation. First, we introduce SQuID (Satellite Quantitative Intelligence Dataset), a benchmark of 2,000 satellite image Question-Answer pairs with both numerical range and categorical answers, designed to evaluate quantitative spatial reasoning. The dataset spans three difficulty tiers with annotations automatically generated from human labels and their learned variability. Second, we propose QVLM (Quantitative Vision-Language Model), a code-generation architecture that maintains pixel precision by decoupling language understanding from visual analysis. Instead of encoding images into embeddings, QVLM generates executable code that first calls a segmentation model to obtain pixel-level masks, then operates directly on these masks, preserving spatial indexing throughout the reasoning process. Our experiments show that QVLM using GPT-5 as coder achieves 42.0% accuracy on SQuID compared to 28.1% for a VLM prompted with image-question pairs. Our work reveals that, for quantitative spatial reasoning, architectural decoupling enables better accuracy on quantitative tasks.

</details>


### [632] [Local-to-Global Logical Explanations for Deep Vision Models](https://arxiv.org/abs/2601.13404)
*Bhavan Vasu,Giuseppe Raffa,Prasad Tadepalli*

Main category: cs.CV

TL;DR: 提出黑盒模型局部和全局解释方法，以人类可识别概念生成解释，在视觉数据集上有高保真和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络不透明、难解释，需方法以人类可识别概念进行解释。

Method: 引入以单调析取范式逻辑公式表示的局部和全局解释方法，还有多类分类的单调解释列表算法。

Result: 在具有挑战性的视觉数据集上，解释对黑盒模型有高保真和覆盖率。

Conclusion: 提出的解释方法简单、可解释，能有效解释黑盒模型。

Abstract: While deep neural networks are extremely effective at classifying images, they remain opaque and hard to interpret. We introduce local and global explanation methods for black-box models that generate explanations in terms of human-recognizable primitive concepts. Both the local explanations for a single image and the global explanations for a set of images are cast as logical formulas in monotone disjunctive-normal-form (MDNF), whose satisfaction guarantees that the model yields a high score on a given class. We also present an algorithm for explaining the classification of examples into multiple classes in the form of a monotone explanation list over primitive concepts. Despite their simplicity and interpretability we show that the explanations maintain high fidelity and coverage with respect to the blackbox models they seek to explain in challenging vision datasets.

</details>


### [633] [Using deep learning for predicting cleansing quality of colon capsule endoscopy images](https://arxiv.org/abs/2601.13412)
*Puneet Sharma,Kristian Dalsbø Hindberg,Benedicte Schelde-Olesen,Ulrik Deding,Esmaeil S. Nadimi,Jan-Matthias Braun*

Main category: cs.CV

TL;DR: 本文探索用深度学习预测结肠胶囊内窥镜图像清洁质量，训练ResNet - 18模型，迭代应用结构化剪枝优化，评估剪枝模型可解释性及效率，还对模型进行校准。


<details>
  <summary>Details</summary>
Motivation: 探索深度学习技术在结肠胶囊内窥镜（CCE）图像清洁质量预测中的应用。

Method: 使用500张由14名临床医生按Leighton - Rex量表标注的图像训练ResNet - 18模型，采用分层K折交叉验证，迭代应用结构化剪枝技术，用多种方法评估剪枝模型可解释性，用ROAD方法进行一致评估，用自适应温度缩放变体校准模型。

Result: 剪枝模型交叉验证准确率达88%，稀疏度为79%，在不降低性能的情况下将效率从84%提升。

Conclusion: 强调CCE图像清洁质量评估挑战、临床应用中可解释性的重要性及ROAD方法用于此任务的挑战，展示了剪枝对提高效率的有效性。

Abstract: In this study, we explore the application of deep learning techniques for predicting cleansing quality in colon capsule endoscopy (CCE) images. Using a dataset of 500 images labeled by 14 clinicians on the Leighton-Rex scale (Poor, Fair, Good, and Excellent), a ResNet-18 model was trained for classification, leveraging stratified K-fold cross-validation to ensure robust performance. To optimize the model, structured pruning techniques were applied iteratively, achieving significant sparsity while maintaining high accuracy. Explainability of the pruned model was evaluated using Grad-CAM, Grad-CAM++, Eigen-CAM, Ablation-CAM, and Random-CAM, with the ROAD method employed for consistent evaluation. Our results indicate that for a pruned model, we can achieve a cross-validation accuracy of 88% with 79% sparsity, demonstrating the effectiveness of pruning in improving efficiency from 84% without compromising performance. We also highlight the challenges of evaluating cleansing quality of CCE images, emphasize the importance of explainability in clinical applications, and discuss the challenges associated with using the ROAD method for our task. Finally, we employ a variant of adaptive temperature scaling to calibrate the pruned models for an external dataset.

</details>


### [634] [Attention-space Contrastive Guidance for Efficient Hallucination Mitigation in LVLMs](https://arxiv.org/abs/2601.13707)
*Yujin Jo,Sangyoon Bae,Taesup Kim*

Main category: cs.CV

TL;DR: 本文针对大视觉语言模型幻觉问题，提出Attention - space Contrastive Guidance (ACG)方法，在减少计算成本同时提高忠实度和字幕质量。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型中语言先验主导视觉证据会导致幻觉问题，如物体误识别和视觉描述不一致，需缓解该问题。

Method: 将幻觉缓解构建为对比引导，提出ACG单通机制在自注意力层操作，构建视觉语言和仅语言的注意力路径，还应用正交校正去除仅语言路径的成分。

Result: 在CHAIR和POPE基准测试中，ACG达到了最先进的忠实度和字幕质量，显著降低计算成本，相比多轮前向传播的对比解码方法，延迟最多降低2倍。

Conclusion: ACG是一种有原则且高效的替代方案，能有效缓解大视觉语言模型的幻觉问题。

Abstract: Hallucinations in large vision-language models (LVLMs) often arise when language priors dominate over visual evidence, causing object misidentification and visually inconsistent descriptions. We address this issue by framing hallucination mitigation as contrastive guidance, steering generation toward visually grounded and semantically faithful text. This approach regulates the model's internal behavior by reducing over-dependence on language priors and contrasting visually grounded with language-only representations. We propose Attention-space Contrastive Guidance (ACG), a single-pass mechanism that operates within self-attention layers to construct both vision-language and language-only attention paths in a single forward computation. This integration enables computationally efficient guidance directly embedded in the model's representation contextualization. To correct approximation bias introduced by the single-pass formulation, we further apply an orthogonalized correction that removes components aligned with the language-only path, selectively amplifying visual contributions. Experiments on the CHAIR and POPE benchmarks show that ACG achieves state-of-the-art faithfulness and caption quality while significantly reducing computational cost. Our method establishes a principled and efficient alternative, reducing latency by up to 2x compared to prior contrastive decoding methods that require multiple forward passes.

</details>


### [635] [HiT: History-Injection Transformers for Onboard Continuous Flood Change Detection](https://arxiv.org/abs/2601.13751)
*Daniel Kyselica,Jonáš Herec,Oliver Kutis,Rado Pitoňák*

Main category: cs.CV

TL;DR: 本文开发了适用于小卫星的星载变化检测系统以进行洪水检测，提出HiT机制，减少数据存储，在STTORM - CD数据集上测试保持检测精度，HiT - Prithvi模型在Jetson Orin Nano上速度达43 FPS，为卫星连续监测自然灾害建立实用框架。


<details>
  <summary>Details</summary>
Motivation: 在严格操作约束下处理多时相数据进行自然灾害监测，特别是洪水检测，且需在小卫星的内存和计算限制内实现。

Method: 提出适用于Transformer模型的History Injection机制（HiT），将其应用于Prithvi - tiny基础模型。

Result: HiT机制减少超99%原始图像大小的数据存储；在STTORM - CD洪水数据集上保持检测精度；HiT - Prithvi模型在Jetson Orin Nano上达到43 FPS。

Conclusion: 建立了卫星连续监测自然灾害的实用框架，可支持实时灾害评估，且无需依赖地面处理基础设施。

Abstract: Natural disaster monitoring through continuous satellite observation requires processing multi-temporal data under strict operational constraints. This paper addresses flood detection, a critical application for hazard management, by developing an onboard change detection system that operates within the memory and computational limits of small satellites. We propose History Injection mechanism for Transformer models (HiT), that maintains historical context from previous observations while reducing data storage by over 99\% of original image size. Moreover, testing on the STTORM-CD flood dataset confirms that the HiT mechanism within the Prithvi-tiny foundation model maintains detection accuracy compared to the bitemporal baseline. The proposed HiT-Prithvi model achieved 43 FPS on Jetson Orin Nano, a representative onboard hardware used in nanosats. This work establishes a practical framework for satellite-based continuous monitoring of natural disasters, supporting real-time hazard assessment without dependency on ground-based processing infrastructure. Architecture as well as model checkpoints is available at https://github.com/zaitra/HiT-change-detection

</details>


### [636] [Insight: Interpretable Semantic Hierarchies in Vision-Language Encoders](https://arxiv.org/abs/2601.13798)
*Kai Wittenmayer,Sukrut Rao,Amin Parchami-Araghi,Bernt Schiele,Jonas Fischer*

Main category: cs.CV

TL;DR: 提出语言对齐的概念基础模型 Insight，可提供细粒度、人类可解释且有空间定位的概念，在分类和分割任务有竞争力并能给出优质解释。


<details>
  <summary>Details</summary>
Motivation: 现有语言对齐视觉基础模型决策过程难解释，相关概念分解工作空间定位差且局限于图像分类任务。

Method: 利用分层稀疏自动编码器和有强语义表示的基础模型自动提取不同粒度概念，考察概念局部共现依赖定义概念关系，借此改进概念命名。

Result: Insight 在分类和分割任务性能上可与不透明基础模型竞争，并能提供细粒度、高质量基于概念的解释。

Conclusion: Insight 能在保证性能的同时，为模型决策提供更具解释性的依据。

Abstract: Language-aligned vision foundation models perform strongly across diverse downstream tasks. Yet, their learned representations remain opaque, making interpreting their decision-making hard. Recent works decompose these representations into human-interpretable concepts, but provide poor spatial grounding and are limited to image classification tasks. In this work, we propose Insight, a language-aligned concept foundation model that provides fine-grained concepts, which are human-interpretable and spatially grounded in the input image. We leverage a hierarchical sparse autoencoder and a foundation model with strong semantic representations to automatically extract concepts at various granularities. Examining local co-occurrence dependencies of concepts allows us to define concept relationships. Through these relations we further improve concept naming and obtain richer explanations. On benchmark data, we show that Insight provides performance on classification and segmentation that is competitive with opaque foundation models while providing fine-grained, high quality concept-based explanations. Code is available at https://github.com/kawi19/Insight.

</details>


### [637] [Discriminant Learning-based Colorspace for Blade Segmentation](https://arxiv.org/abs/2601.13816)
*Raül Pérez-Gonzalo,Andreas Espersen,Antonio Agudo*

Main category: cs.CV

TL;DR: 提出CSDA算法改进图像分割，实验显示精度提升，强调定制预处理重要性。


<details>
  <summary>Details</summary>
Motivation: 次优的颜色表示妨碍图像分割精度，且现代算法常忽视此关键预处理步骤。

Method: 提出CSDA算法，将线性判别分析拓展到深度学习，通过广义判别损失定制颜色表示，引入三种替代损失进行端到端优化。

Result: 在风力涡轮机叶片数据实验中取得显著精度提升。

Conclusion: 定制预处理在特定领域图像分割中非常重要。

Abstract: Suboptimal color representation often hinders accurate image segmentation, yet many modern algorithms neglect this critical preprocessing step. This work presents a novel multidimensional nonlinear discriminant analysis algorithm, Colorspace Discriminant Analysis (CSDA), for improved segmentation. Extending Linear Discriminant Analysis into a deep learning context, CSDA customizes color representation by maximizing multidimensional signed inter-class separability while minimizing intra-class variability through a generalized discriminative loss. To ensure stable training, we introduce three alternative losses that enable end-to-end optimization of both the discriminative colorspace and segmentation process. Experiments on wind turbine blade data demonstrate significant accuracy gains, emphasizing the importance of tailored preprocessing in domain-specific segmentation.

</details>


### [638] [Probabilistic Deep Discriminant Analysis for Wind Blade Segmentation](https://arxiv.org/abs/2601.13852)
*Raül Pérez-Gonzalo,Andreas Espersen,Antonio Agudo*

Main category: cs.CV

TL;DR: 提出Deep Discriminant Analysis (DDA)和Probabilistic DDA (PDDA)，应用于风叶分割有良好表现。


<details>
  <summary>Details</summary>
Motivation: 线性判别分析处理非线性可分数据有困难，需克服此问题。

Method: 用深度网络直接优化Fisher准则，引入有符号的类间方差、用sigmoid函数约束输出、将乘法关系转换为加法关系，提出两个稳定的DDA损失函数并添加概率损失得到PDDA。

Result: PDDA有效减少输出分布的类重叠，预测置信度高、类内方差小，应用于风叶分割性能和一致性有显著提升。

Conclusion: 首次将DDA应用于图像分割，该方法对风能维护关键的风叶分割有重要意义。

Abstract: Linear discriminant analysis improves class separability but struggles with non-linearly separable data. To overcome this, we introduce Deep Discriminant Analysis (DDA), which directly optimizes the Fisher criterion utilizing deep networks. To ensure stable training and avoid computational instabilities, we incorporate signed between-class variance, bound outputs with a sigmoid function, and convert multiplicative relationships into additive ones. We present two stable DDA loss functions and augment them with a probability loss, resulting in Probabilistic DDA (PDDA). PDDA effectively minimizes class overlap in output distributions, producing highly confident predictions with reduced within-class variance. When applied to wind blade segmentation, PDDA showcases notable advances in performance and consistency, critical for wind energy maintenance. To our knowledge, this is the first application of DDA to image segmentation.

</details>


### [639] [CARPE: Context-Aware Image Representation Prioritization via Ensemble for Large Vision-Language Models](https://arxiv.org/abs/2601.13622)
*Donghee Lee,Rui Cai,Zhe Zhao*

Main category: cs.CV

TL;DR: 本文提出CARPE框架解决LVLMs在以视觉为中心的任务上表现不佳的问题，能提升多种基准测试性能且可与多数开源LVLMs集成。


<details>
  <summary>Details</summary>
Motivation: LVLMs在图像分类等以视觉为中心的任务上表现不如其基础视觉编码器，存在局限性。

Method: 提出CARPE框架，引入视觉集成层和上下文感知集成策略，自适应权衡视觉和文本模态。

Result: 在图像分类基准测试和多种视觉 - 语言基准测试中表现提升。

Conclusion: CARPE可有效集成到多数由视觉编码器和语言模型组成的开源LVLMs，适应不同架构。

Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have pushed them closer to becoming general-purpose assistants. Despite their strong performance, LVLMs still struggle with vision-centric tasks such as image classification, underperforming compared to their base vision encoders, which are often CLIP-based models. To address this limitation, we propose Context-Aware Image Representation Prioritization via Ensemble (CARPE), a novel, model-agnostic framework which introduces vision-integration layers and a context-aware ensemble strategy to identify when to prioritize image representations or rely on the reasoning capabilities of the language model. This design enhances the model's ability to adaptively weight visual and textual modalities and enables the model to capture various aspects of image representations, leading to consistent improvements in generalization across classification and vision-language benchmarks. Extensive experiments demonstrate that CARPE not only improves performance on image classification benchmarks but also enhances results across various vision-language benchmarks. Finally, CARPE is designed to be effectively integrated with most open-source LVLMs that consist of a vision encoder and a language model, ensuring its adaptability across diverse architectures.

</details>


### [640] [TrackletGPT: A Language-like GPT Framework for White Matter Tract Segmentation](https://arxiv.org/abs/2601.13935)
*Anoushkrit Goel,Simroop Singh,Ankita Joshi,Ranjeet Ranjan Jha,Chirag Ahuja,Aditya Nigam,Arnav Bhavsar*

Main category: cs.CV

TL;DR: 提出TrackletGPT用于白质束分割，在多个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 白质束分割对研究大脑结构连接、神经疾病和神经外科很重要，但任务复杂，需解决不同束、不同受试者和条件下的差异问题。

Method: 提出TrackletGPT，一个类语言的GPT框架，使用小轨迹段重新引入令牌中的顺序信息。

Result: TrackletGPT在TractoInferno和HCP数据集的平均DICE、重叠和过分割分数上优于现有方法，在跨数据集实验中也是如此。

Conclusion: TrackletGPT在白质束分割任务中表现出色，能无缝跨数据集推广，是一种有效的方法。

Abstract: White Matter Tract Segmentation is imperative for studying brain structural connectivity, neurological disorders and neurosurgery. This task remains complex, as tracts differ among themselves, across subjects and conditions, yet have similar 3D structure across hemispheres and subjects. To address these challenges, we propose TrackletGPT, a language-like GPT framework which reintroduces sequential information in tokens using tracklets. TrackletGPT generalises seamlessly across datasets, is fully automatic, and encodes granular sub-streamline segments, Tracklets, scaling and refining GPT models in Tractography Segmentation. Based on our experiments, TrackletGPT outperforms state-of-the-art methods on average DICE, Overlap and Overreach scores on TractoInferno and HCP datasets, even on inter-dataset experiments.

</details>


### [641] [Harmonizing the Deep: A Unified Information Pipeline for Robust Marine Biodiversity Assessment Across Heterogeneous Domains](https://arxiv.org/abs/2601.13975)
*Marco Piccolo,Qiwei Han,Astrid van Toor,Joachim Vanneste*

Main category: cs.CV

TL;DR: 本文为海洋生态系统入侵物种监测建立基础检测层，开发统一信息管道，分析跨域性能损失原因，验证边缘硬件推理可行性，强调结构感知可靠性。


<details>
  <summary>Details</summary>
Motivation: 海洋生物多样性监测需可扩展性和可靠性，但现有检测方案在新地点性能下降，为北极和大西洋海洋生态系统入侵物种监测建立基础检测层。

Method: 开发统一信息管道将异构数据集标准化，在受控跨域协议下评估固定探测器。

Result: 结构因素比视觉退化更能解释跨域性能损失，稀疏场景会导致“上下文崩溃”，在低成本边缘硬件上验证了推理可行性。

Conclusion: 应将重点从图像增强转向结构感知可靠性，为海洋生态系统评估提供了通用工具。

Abstract: Marine biodiversity monitoring requires scalability and reliability across complex underwater environments to support conservation and invasive-species management. Yet existing detection solutions often exhibit a pronounced deployment gap, with performance degrading sharply when transferred to new sites. This work establishes the foundational detection layer for a multi-year invasive species monitoring initiative targeting Arctic and Atlantic marine ecosystems. We address this challenge by developing a Unified Information Pipeline that standardises heterogeneous datasets into a comparable information flow and evaluates a fixed, deployment-relevant detector under controlled cross-domain protocols. Across multiple domains, we find that structural factors, such as scene composition, object density, and contextual redundancy, explain cross-domain performance loss more strongly than visual degradation such as turbidity, with sparse scenes inducing a characteristic "Context Collapse" failure mode. We further validate operational feasibility by benchmarking inference on low-cost edge hardware, showing that runtime optimisation enables practical sampling rates for remote monitoring. The results shift emphasis from image enhancement toward structure-aware reliability, providing a democratised tool for consistent marine ecosystem assessment.

</details>


### [642] [Federated Balanced Learning](https://arxiv.org/abs/2601.14042)
*Jiaze Li,Haoran Xu,Wanyi Wu,Changwei Wang,Shuaiguang Li,Jianzhong Ju,Zhenbo Luo,Jian Luan,Youyang Qu,Longxiang Gao,Xudong Yang,Lumin Xing*

Main category: cs.CV

TL;DR: 文章指出联邦学习中非独立同分布（non - iid）设置下的客户端漂移问题，提出联邦平衡学习（FBL）方法防止该问题，实验显示该方法优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在non - iid设置下，全局模型因客户端漂移影响最终性能的问题，且过往方法忽略客户端样本影响。

Method: 提出FBL方法，通过边缘端生成模型的知识填充和知识采样实现客户端样本平衡，设计知识对齐策略和知识丢弃策略，将方法扩展到复杂场景。

Result: 大量实验表明该方法优于现有最先进的基线。

Conclusion: 所提出的FBL方法可有效解决联邦学习中客户端漂移问题并提升模型性能，代码待接受后发布。

Abstract: Federated learning is a paradigm of joint learning in which clients collaborate by sharing model parameters instead of data. However, in the non-iid setting, the global model experiences client drift, which can seriously affect the final performance of the model. Previous methods tend to correct the global model that has already deviated based on the loss function or gradient, overlooking the impact of the client samples. In this paper, we rethink the role of the client side and propose Federated Balanced Learning, i.e., FBL, to prevent this issue from the beginning through sample balance on the client side. Technically, FBL allows unbalanced data on the client side to achieve sample balance through knowledge filling and knowledge sampling using edge-side generation models, under the limitation of a fixed number of data samples on clients. Furthermore, we design a Knowledge Alignment Strategy to bridge the gap between synthetic and real data, and a Knowledge Drop Strategy to regularize our method. Meanwhile, we scale our method to real and complex scenarios, allowing different clients to adopt various methods, and extend our framework to further improve performance. Numerous experiments show that our method outperforms state-of-the-art baselines. The code is released upon acceptance.

</details>


### [643] [Unsupervised Video Class-Incremental Learning via Deep Embedded Clustering Management](https://arxiv.org/abs/2601.14069)
*Nattapong Kurpukdee,Adrian G. Bors*

Main category: cs.CV

TL;DR: 本文提出处理无监督视频类别增量学习的方法，在三个标准数据集上进行评估，效果显著优于其他基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法多关注有监督类别增量学习，依赖标签和任务边界信息，成本高且需人工标注，而无监督视频类别增量学习无需考虑数据标签且防止遗忘，有研究价值。

Method: 先使用深度特征提取网络在每个任务中提供代表性视频特征，再从提取的特征中逐步构建一系列深度聚类，在后续任务学习时使用前一任务更新后的模型作为初始状态转移知识。

Result: 在UCF101、HMDB51和Something - to - Something V2三个标准视频动作识别数据集上进行评估，忽略监督设置中的标签，方法显著优于其他基线。

Conclusion: 所提方法简单且有效，能够很好地处理无监督视频类别增量学习问题。

Abstract: Unsupervised video class incremental learning (uVCIL) represents an important learning paradigm for learning video information without forgetting, and without considering any data labels. Prior approaches have focused on supervised class-incremental learning, relying on using the knowledge of labels and task boundaries, which is costly, requires human annotation, or is simply not a realistic option. In this paper, we propose a simple yet effective approach to address the uVCIL. We first consider a deep feature extractor network, providing a set of representative video features during each task without assuming any class or task information. We then progressively build a series of deep clusters from the extracted features. During the successive task learning, the model updated from the previous task is used as an initial state in order to transfer knowledge to the current learning task. We perform in-depth evaluations on three standard video action recognition datasets, including UCF101, HMDB51, and Something-to-Something V2, by ignoring the labels from the supervised setting. Our approach significantly outperforms other baselines on all datasets.

</details>


### [644] [Two-Stream temporal transformer for video action classification](https://arxiv.org/abs/2601.14086)
*Nattapong Kurpukdee,Adrian G. Bors*

Main category: cs.CV

TL;DR: 提出双流变压器视频分类器用于视频理解，在三个数据集上效果好


<details>
  <summary>Details</summary>
Motivation: 运动表示在视频理解中重要，变压器网络在多应用中有效，需新模型提取时空信息

Method: 引入新的双流变压器视频分类器，从内容和光流中提取时空信息，在变压器编码器机制内表示特征关系

Result: 在三个著名人类活动视频数据集上分类效果优异

Conclusion: 所提方法有效，能用于视频分类任务

Abstract: Motion representation plays an important role in video understanding and has many applications including action recognition, robot and autonomous guidance or others. Lately, transformer networks, through their self-attention mechanism capabilities, have proved their efficiency in many applications. In this study, we introduce a new two-stream transformer video classifier, which extracts spatio-temporal information from content and optical flow representing movement information. The proposed model identifies self-attention features across the joint optical flow and temporal frame domain and represents their relationships within the transformer encoder mechanism. The experimental results show that our proposed methodology provides excellent classification results on three well-known video datasets of human activities.

</details>


### [645] [Rig-Aware 3D Reconstruction of Vehicle Undercarriages using Gaussian Splatting](https://arxiv.org/abs/2601.14208)
*Nitin Kulkarni,Akhil Devarashetti,Charlie Cluss,Livio Forte,Dan Buckmaster,Philip Schneider,Chunming Qiao,Alina Vereshchaka*

Main category: cs.CV

TL;DR: 提出利用三相机装置的端到端管道，生成汽车底盘交互式3D模型以辅助检查和提升买家信心，有专门的SfM管道及方法。


<details>
  <summary>Details</summary>
Motivation: 传统二手车底盘检查劳动强度大，线上买家难见底盘照片，需更高效安全的检查方式。

Method: 采用专门设计的rig - aware SfM管道，结合精确相机校准、同步视频流和几何先验，使用约束匹配策略、DISK特征提取器和LightGlue匹配器生成点云，再用高斯溅射生成模型。

Result: 能生成实时渲染的逼真底盘模型，实验和消融研究表明设计选择可实现先进质量。

Conclusion: 所提方法能克服广角镜头畸变和低视差场景挑战，提高工作场所安全性和买家信心。

Abstract: Inspecting the undercarriage of used vehicles is a labor-intensive task that requires inspectors to crouch or crawl underneath each vehicle to thoroughly examine it. Additionally, online buyers rarely see undercarriage photos. We present an end-to-end pipeline that utilizes a three-camera rig to capture videos of the undercarriage as the vehicle drives over it, and produces an interactive 3D model of the undercarriage. The 3D model enables inspectors and customers to rotate, zoom, and slice through the undercarriage, allowing them to detect rust, leaks, or impact damage in seconds, thereby improving both workplace safety and buyer confidence. Our primary contribution is a rig-aware Structure-from-Motion (SfM) pipeline specifically designed to overcome the challenges of wide-angle lens distortion and low-parallax scenes. Our method overcomes the challenges of wide-angle lens distortion and low-parallax scenes by integrating precise camera calibration, synchronized video streams, and strong geometric priors from the camera rig. We use a constrained matching strategy with learned components, the DISK feature extractor, and the attention-based LightGlue matcher to generate high-quality sparse point clouds that are often unattainable with standard SfM pipelines. These point clouds seed the Gaussian splatting process to generate photorealistic undercarriage models that render in real-time. Our experiments and ablation studies demonstrate that our design choices are essential to achieve state-of-the-art quality.

</details>


### [646] [OmniOVCD: Streamlining Open-Vocabulary Change Detection with SAM 3](https://arxiv.org/abs/2601.13895)
*Xu Zhang,Danyang Li,Yingjie Xia,Xiaohang Dong,Hualong Yu,Jianye Wang,Qicheng Li*

Main category: cs.CV

TL;DR: 本文提出用于开放词汇变化检测的独立框架OmniOVCD，利用SAM 3解耦输出头提出SFID策略，在四个公开基准测试中取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有无训练开放词汇变化检测方法结合不同模型存在特征匹配问题和系统不稳定问题，需新方法。

Method: 提出OmniOVCD框架，利用SAM 3解耦输出头，采用SFID策略，先融合输出构建土地覆盖掩码，再分解为单个实例掩码进行变化比较。

Result: 在四个公开基准测试（LEVIR - CD、WHU - CD、S2Looking和SECOND）中实现了IoU分数分别为67.2、66.5、24.5和27.1（类别平均），超越之前所有方法。

Conclusion: OmniOVCD框架能在类别识别中保持高精度和跨图像实例级一致性，可生成准确变化掩码。

Abstract: Change Detection (CD) is a fundamental task in remote sensing. It monitors the evolution of land cover over time. Based on this, Open-Vocabulary Change Detection (OVCD) introduces a new requirement. It aims to reduce the reliance on predefined categories. Existing training-free OVCD methods mostly use CLIP to identify categories. These methods also need extra models like DINO to extract features. However, combining different models often causes problems in matching features and makes the system unstable. Recently, the Segment Anything Model 3 (SAM 3) is introduced. It integrates segmentation and identification capabilities within one promptable model, which offers new possibilities for the OVCD task. In this paper, we propose OmniOVCD, a standalone framework designed for OVCD. By leveraging the decoupled output heads of SAM 3, we propose a Synergistic Fusion to Instance Decoupling (SFID) strategy. SFID first fuses the semantic, instance, and presence outputs of SAM 3 to construct land-cover masks, and then decomposes them into individual instance masks for change comparison. This design preserves high accuracy in category recognition and maintains instance-level consistency across images. As a result, the model can generate accurate change masks. Experiments on four public benchmarks (LEVIR-CD, WHU-CD, S2Looking, and SECOND) demonstrate SOTA performance, achieving IoU scores of 67.2, 66.5, 24.5, and 27.1 (class-average), respectively, surpassing all previous methods.

</details>


### [647] [Glance-or-Gaze: Incentivizing LMMs to Adaptively Focus Search via Reinforcement Learning](https://arxiv.org/abs/2601.13942)
*Hongbo Bai,Yujin Zhou,Yile Wu,Chi-Min Chan,Pengcheng Wen,Kunhao Pan,Sirui Han,Yike Guo*

Main category: cs.CV

TL;DR: 提出GoG框架解决LMMs知识密集查询问题，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: LMMs因静态参数知识难以处理知识密集查询，现有搜索增强方法有视觉冗余和缺乏深度迭代反思问题。

Method: 提出GoG框架，引入Selective Gaze机制，设计双阶段训练策略。

Result: 在六个基准测试中表现达到最优，消融实验证明Selective Gaze和复杂度自适应RL对有效视觉搜索至关重要。

Conclusion: GoG框架有效，将发布数据和模型供进一步探索。

Abstract: Large Multimodal Models (LMMs) have achieved remarkable success in visual understanding, yet they struggle with knowledge-intensive queries involving long-tail entities or evolving information due to static parametric knowledge. Recent search-augmented approaches attempt to address this limitation, but existing methods rely on indiscriminate whole-image retrieval that introduces substantial visual redundancy and noise, and lack deep iterative reflection, limiting their effectiveness on complex visual queries. To overcome these challenges, we propose Glance-or-Gaze (GoG), a fully autonomous framework that shifts from passive perception to active visual planning. GoG introduces a Selective Gaze mechanism that dynamically chooses whether to glance at global context or gaze into high-value regions, filtering irrelevant information before retrieval. We design a dual-stage training strategy: Reflective GoG Behavior Alignment via supervised fine-tuning instills the fundamental GoG paradigm, while Complexity-Adaptive Reinforcement Learning further enhances the model's capability to handle complex queries through iterative reasoning. Experiments across six benchmarks demonstrate state-of-the-art performance. Ablation studies confirm that both Selective Gaze and complexity-adaptive RL are essential for effective visual search. We will release our data and models for further exploration soon.

</details>


### [648] [Generalizing Abstention for Noise-Robust Learning in Medical Image Segmentation](https://arxiv.org/abs/2601.14039)
*Wesam Moustafa,Hossam Elsafty,Helen Schneider,Lorenz Sparrenberg,Rafet Sifa*

Main category: cs.CV

TL;DR: 介绍一种通用模块化弃权框架提升医学图像分割损失函数的抗噪性，实验表明该方法在高噪声水平下表现出色。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割中标签噪声问题严重影响模型性能，弃权机制在分割领域潜力未被验证，存在研究缺口。

Method: 引入通用模块化弃权框架，含引导弃权行为的正则项和更灵活的自动调优算法，与三种损失函数结合创建新的抗噪变体。

Result: 在CaDIS和DSAD医学数据集实验中，方法在高噪声水平下显著优于非弃权基线。

Conclusion: 让模型选择性忽略损坏样本是构建更可靠分割模型的有效且通用策略。

Abstract: Label noise is a critical problem in medical image segmentation, often arising from the inherent difficulty of manual annotation. Models trained on noisy data are prone to overfitting, which degrades their generalization performance. While a number of methods and strategies have been proposed to mitigate noisy labels in the segmentation domain, this area remains largely under-explored. The abstention mechanism has proven effective in classification tasks by enhancing the capabilities of Cross Entropy, yet its potential in segmentation remains unverified. In this paper, we address this gap by introducing a universal and modular abstention framework capable of enhancing the noise-robustness of a diverse range of loss functions. Our framework improves upon prior work with two key components: an informed regularization term to guide abstention behaviour, and a more flexible power-law-based auto-tuning algorithm for the abstention penalty. We demonstrate the framework's versatility by systematically integrating it with three distinct loss functions to create three novel, noise-robust variants: GAC, SAC, and ADS. Experiments on the CaDIS and DSAD medical datasets show our methods consistently and significantly outperform their non-abstaining baselines, especially under high noise levels. This work establishes that enabling models to selectively ignore corrupted samples is a powerful and generalizable strategy for building more reliable segmentation models. Our code is publicly available at https://github.com/wemous/abstention-for-segmentation.

</details>


### [649] [Decoder-Free Supervoxel GNN for Accurate Brain-Tumor Localization in Multi-Modal MRI](https://arxiv.org/abs/2601.14055)
*Andrea Protani,Marc Molina Van Den Bosch,Lorenzo Giusti,Heloisa Barbosa Da Silva,Paolo Cacace,Albert Sund Aillet,Miguel Angel Gonzalez Ballester,Friedhelm Hummel,Luigi Serio*

Main category: cs.CV

TL;DR: 提出不含解码器的SVGFormer处理3D医学图像，在BraTS数据集上验证效果好，证明基于图的仅编码器范式的可行性


<details>
  <summary>Details</summary>
Motivation: 现代3D医学图像视觉骨干网将大量参数用于空间重建而非特征学习

Method: 引入SVGFormer，通过内容感知分组将体积划分为超体素语义图，使用分层编码器结合Patch Transformer和超体素级图注意力网络学习特征

Result: 在BraTS数据集上训练的分类和回归模型表现良好，分类模型F1分数达0.875，回归模型MAE为0.028

Conclusion: 基于图的仅编码器范式为3D医学图像表示提供准确且可解释的替代方案

Abstract: Modern vision backbones for 3D medical imaging typically process dense voxel grids through parameter-heavy encoder-decoder structures, a design that allocates a significant portion of its parameters to spatial reconstruction rather than feature learning. Our approach introduces SVGFormer, a decoder-free pipeline built upon a content-aware grouping stage that partitions the volume into a semantic graph of supervoxels. Its hierarchical encoder learns rich node representations by combining a patch-level Transformer with a supervoxel-level Graph Attention Network, jointly modeling fine-grained intra-region features and broader inter-regional dependencies. This design concentrates all learnable capacity on feature encoding and provides inherent, dual-scale explainability from the patch to the region level. To validate the framework's flexibility, we trained two specialized models on the BraTS dataset: one for node-level classification and one for tumor proportion regression. Both models achieved strong performance, with the classification model achieving a F1-score of 0.875 and the regression model a MAE of 0.028, confirming the encoder's ability to learn discriminative and localized features. Our results establish that a graph-based, encoder-only paradigm offers an accurate and inherently interpretable alternative for 3D medical image representation.

</details>


### [650] [POCI-Diff: Position Objects Consistently and Interactively with 3D-Layout Guided Diffusion](https://arxiv.org/abs/2601.14056)
*Andrea Rigo,Luca Stornaiuolo,Weijie Wang,Mauro Martino,Bruno Lepri,Nicu Sebe*

Main category: cs.CV

TL;DR: 提出扩散方法POCI - Diff用于T2I生成，有3D布局控制和编辑能力，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在进行T2I生成时用2D提示或迭代复制 - 扭曲 - 粘贴策略，会扭曲对象几何形状且编辑时难以保持一致性。

Method: 提出POCI - Diff框架，通过混合潜在扩散将文本描述与3D边界框绑定实现每个对象语义控制；提出无扭曲生成编辑管道；使用IP - Adapter以参考图像为条件进行扩散。

Result: POCI - Diff生成图像质量高，与指定3D布局和编辑一致，在视觉保真度和布局遵守方面胜过现有方法，消除了扭曲导致的几何伪影。

Conclusion: POCI - Diff是一种有效的T2I生成方法，能解决现有方法的不足。

Abstract: We propose a diffusion-based approach for Text-to-Image (T2I) generation with consistent and interactive 3D layout control and editing. While prior methods improve spatial adherence using 2D cues or iterative copy-warp-paste strategies, they often distort object geometry and fail to preserve consistency across edits. To address these limitations, we introduce a framework for Positioning Objects Consistently and Interactively (POCI-Diff), a novel formulation for jointly enforcing 3D geometric constraints and instance-level semantic binding within a unified diffusion process. Our method enables explicit per-object semantic control by binding individual text descriptions to specific 3D bounding boxes through Blended Latent Diffusion, allowing one-shot synthesis of complex multi-object scenes. We further propose a warping-free generative editing pipeline that supports object insertion, removal, and transformation via regeneration rather than pixel deformation. To preserve object identity and consistency across edits, we condition the diffusion process on reference images using IP-Adapter, enabling coherent object appearance throughout interactive 3D editing while maintaining global scene coherence. Experimental results demonstrate that POCI-Diff produces high-quality images consistent with the specified 3D layouts and edits, outperforming state-of-the-art methods in both visual fidelity and layout adherence while eliminating warping-induced geometric artifacts.

</details>


### [651] [DermaBench: A Clinician-Annotated Benchmark Dataset for Dermatology Visual Question Answering and Reasoning](https://arxiv.org/abs/2601.14084)
*Abdurrahim Yilmaz,Ozan Erdem,Ece Gokyayla,Ayda Acar,Burc Bugra Dagtas,Dilara Ilhan Erdil,Gulsum Gencoglan,Burak Temelkuran*

Main category: cs.CV

TL;DR: 本文介绍了用于评估视觉语言模型的皮肤病学VQA基准DermaBench。


<details>
  <summary>Details</summary>
Motivation: 现有皮肤病数据集主要关注图像级分类任务，无法全面评估多模态模型的视觉理解、语言基础和临床推理能力，需要视觉问答基准。

Method: 基于DDI数据集构建DermaBench，由专家皮肤科医生使用分层注释模式对图像进行诊断、解剖部位等多方面注释。

Result: DermaBench包含来自570名不同菲茨帕特里克皮肤类型患者的656张临床图像，约14474个VQA风格的注释，并以仅含元数据的数据集形式发布。

Conclusion: DermaBench可用于评估视觉语言模型在皮肤病学领域的性能。

Abstract: Vision-language models (VLMs) are increasingly important in medical applications; however, their evaluation in dermatology remains limited by datasets that focus primarily on image-level classification tasks such as lesion recognition. While valuable for recognition, such datasets cannot assess the full visual understanding, language grounding, and clinical reasoning capabilities of multimodal models. Visual question answering (VQA) benchmarks are required to evaluate how models interpret dermatological images, reason over fine-grained morphology, and generate clinically meaningful descriptions. We introduce DermaBench, a clinician-annotated dermatology VQA benchmark built on the Diverse Dermatology Images (DDI) dataset. DermaBench comprises 656 clinical images from 570 unique patients spanning Fitzpatrick skin types I-VI. Using a hierarchical annotation schema with 22 main questions (single-choice, multi-choice, and open-ended), expert dermatologists annotated each image for diagnosis, anatomic site, lesion morphology, distribution, surface features, color, and image quality, together with open-ended narrative descriptions and summaries, yielding approximately 14.474 VQA-style annotations. DermaBench is released as a metadata-only dataset to respect upstream licensing and is publicly available at Harvard Dataverse.

</details>


### [652] [LLM Augmented Intervenable Multimodal Adaptor for Post-operative Complication Prediction in Lung Cancer Surgery](https://arxiv.org/abs/2601.14154)
*Shubham Pandey,Bhavin Jawade,Srirangaraj Setlur,Venu Govindaraju,Kenneth Seastedt*

Main category: cs.CV

TL;DR: 提出深度学习架构MIRACLE，融合术前临床与放射学数据预测肺癌手术术后并发症风险，在真实数据集上验证优于传统模型和大语言模型。


<details>
  <summary>Details</summary>
Motivation: 术后并发症影响患者预后并增加医疗成本，需要有效预测方法。

Method: 构建MIRACLE架构，采用超球嵌入空间融合异构输入，融入介入式深度学习模块增强预测透明度和临床实用性。

Result: 在POC - L数据集上验证，MIRACLE优于传统机器学习模型和大语言模型变体。

Conclusion: MIRACLE可用于个性化和可解释的术后风险管理。

Abstract: Postoperative complications remain a critical concern in clinical practice, adversely affecting patient outcomes and contributing to rising healthcare costs. We present MIRACLE, a deep learning architecture for prediction of risk of postoperative complications in lung cancer surgery by integrating preoperative clinical and radiological data. MIRACLE employs a hyperspherical embedding space fusion of heterogeneous inputs, enabling the extraction of robust, discriminative features from both structured clinical records and high-dimensional radiological images. To enhance transparency of prediction and clinical utility, we incorporate an interventional deep learning module in MIRACLE, that not only refines predictions but also provides interpretable and actionable insights, allowing domain experts to interactively adjust recommendations based on clinical expertise. We validate our approach on POC-L, a real-world dataset comprising 3,094 lung cancer patients who underwent surgery at Roswell Park Comprehensive Cancer Center. Our results demonstrate that MIRACLE outperforms various traditional machine learning models and contemporary large language models (LLM) variants alone, for personalized and explainable postoperative risk management.

</details>


### [653] [VideoMaMa: Mask-Guided Video Matting via Generative Prior](https://arxiv.org/abs/2601.14255)
*Sangbeom Lim,Seoung Wug Oh,Jiahui Huang,Heeji Yoon,Seungryong Kim,Joon-Young Lee*

Main category: cs.CV

TL;DR: 提出VideoMaMa模型，利用预训练视频扩散模型将粗分割掩码转换为精确alpha蒙版，构建MA - V数据集，微调SAM2得到SAM2 - Matte表现更优，强调大规模伪标签视频抠图的重要性。


<details>
  <summary>Details</summary>
Motivation: 解决因标注数据稀缺导致视频抠图模型难以泛化到真实世界视频的问题。

Method: 提出VideoMaMa模型，利用预训练视频扩散模型；开发可扩展伪标签管道构建MA - V数据集；在MA - V上微调SAM2模型得到SAM2 - Matte。

Result: VideoMaMa在零样本下能很好泛化到真实视频；SAM2 - Matte在野外视频上比在现有数据集训练的模型更具鲁棒性。

Conclusion: 强调大规模伪标签视频抠图的重要性，展示生成先验和可访问分割线索可推动视频抠图研究的可扩展进展。

Abstract: Generalizing video matting models to real-world videos remains a significant challenge due to the scarcity of labeled data. To address this, we present Video Mask-to-Matte Model (VideoMaMa) that converts coarse segmentation masks into pixel accurate alpha mattes, by leveraging pretrained video diffusion models. VideoMaMa demonstrates strong zero-shot generalization to real-world footage, even though it is trained solely on synthetic data. Building on this capability, we develop a scalable pseudo-labeling pipeline for large-scale video matting and construct the Matting Anything in Video (MA-V) dataset, which offers high-quality matting annotations for more than 50K real-world videos spanning diverse scenes and motions. To validate the effectiveness of this dataset, we fine-tune the SAM2 model on MA-V to obtain SAM2-Matte, which outperforms the same model trained on existing matting datasets in terms of robustness on in-the-wild videos. These findings emphasize the importance of large-scale pseudo-labeled video matting and showcase how generative priors and accessible segmentation cues can drive scalable progress in video matting research.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [654] [NOVAID: Natural-language Observability Visualization Assistant for ITOps Dashboard Widget Generation](https://arxiv.org/abs/2601.11531)
*Pratik Mishra,Caner Gözübüyük,Seema Nagar,Prateeti Mohapatra,Raya Wittich,Arthur de Magalhaes*

Main category: cs.HC

TL;DR: 提出交互式聊天机器人NOVAID，利用大语言模型从自然语言查询生成IT监控小部件，在测试和用户研究中效果良好，有应用潜力。


<details>
  <summary>Details</summary>
Motivation: 手动创建IT监控仪表盘小部件慢且易出错，对新手和专家都是障碍。

Method: 结合领域感知语义解析器、模糊实体匹配和模式完成生成标准化小部件JSON规范，有交互式澄清循环确保准确性。

Result: 在271个实际查询的策划数据集上，NOVAID在多个大语言模型中实现较高准确率（指标提取高达94.10%）；用户研究中系统可用性量表得分为74.2。

Conclusion: NOVAID能将自然语言意图与操作仪表盘连接，有在企业IT运维监控平台部署的潜力。

Abstract: Manual creation of IT monitoring dashboard widgets is slow, error-prone, and a barrier for both novice and expert users. We present NOVAID, an interactive chatbot that leverages Large Language Models (LLMs) to generate IT monitoring widgets directly from natural language queries. Unlike general natural language-to-visualization tools, NOVAID addresses IT operations-specific challenges: specialized widget types like SLO charts, dynamic API-driven data retrieval, and complex contextual filters. The system combines a domain-aware semantic parser, fuzzy entity matching, and schema completion to produce standardized widget JSON specifications. An interactive clarification loop ensures accuracy in underspecified queries. On a curated dataset of 271 realistic queries, NOVAID achieves promising accuracy (up to 94.10% in metric extraction) across multiple LLMs. A user study with IT engineers yielded a System Usability Scale score of 74.2 for NOVAID, indicating good usability. By bridging natural language intent with operational dashboards, NOVAID demonstrates clear potential and a path for deployment in enterprise ITOps monitoring platforms.

</details>


### [655] [Chatsparent: An Interactive System for Detecting and Mitigating Cognitive Fatigue in LLMs](https://arxiv.org/abs/2601.11526)
*Riju Marwah,Vishal Pallagani,Ritvik Garimella,Amit Sheth*

Main category: cs.HC

TL;DR: 介绍交互式演示Chatsparent，缓解大语言模型认知疲劳，提升用户对模型行为理解和推理可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型聊天机器人界面缺乏透明度，用户易盲目信任，模型存在认知疲劳问题。

Method: 构建Chatsparent系统，检测实时、标记级疲劳信号并可视化成疲劳指数，超阈值时用户可进行轻量级干预。

Result: 演示可实时展示文本和疲劳信号，让用户观察疲劳产生、影响及干预效果。

Conclusion: 系统将被动交互变为交互式诊断体验，提升用户对模型行为理解和推理可靠性。

Abstract: LLMs are increasingly being deployed as chatbots, but today's interfaces offer little to no friction: users interact through seamless conversations that conceal when the model is drifting, hallucinating or failing. This lack of transparency fosters blind trust, even as models produce unstable or repetitive outputs. We introduce an interactive demo that surfaces and mitigates cognitive fatigue, a failure mode where LLMs gradually lose coherence during auto-regressive generation. Our system, Chatsparent, instruments real-time, token-level signals of fatigue, including attention-to-prompt decay, embedding drift, and entropy collapse, and visualizes them as a unified fatigue index. When fatigue thresholds are crossed, the interface allows users to activate lightweight interventions such as attention resets, entropy-regularized decoding, and self-reflection checkpoints. The demo streams live text and fatigue signals, allowing users to observe when fatigue arises, how it affects output quality, and how interventions restore stability. By turning passive chatbot interaction into an interactive diagnostic experience, our system empowers users to better understand LLM behavior while improving reliability at inference time.

</details>


### [656] [Do LLMs Give Good Romantic Relationship Advice? A Study on User Satisfaction and Attitude Change](https://arxiv.org/abs/2601.11527)
*Niva Manchanda,Akshata Kishore Moharir,Isabel Michel,Ratna Kandala*

Main category: cs.HC

TL;DR: 研究调查人们对大语言模型（LLMs）生成的恋爱关系建议的评价，结果显示参与者满意度高，且接触建议后对LLMs态度改善。


<details>
  <summary>Details</summary>
Motivation: 大语言模型越来越多用于个人领域提供建议，但人们对这类建议的看法尚不清楚，故研究人们对LLM生成的恋爱关系建议的评价。

Method: 让参与者对建议满意度、模型可靠性和有用性进行评分，并完成对LLMs总体态度的前后测。

Result: 参与者对LLM生成的建议满意度高，满意度与对模型可靠性和有用性的感知呈强正相关，接触建议后参与者对LLMs的态度显著改善。

Conclusion: 支持性且与情境相关的建议能增强用户对这类AI系统的信任和开放度。

Abstract: Large Language Models (LLMs) are increasingly being used to provide support and advice in personal domains such as romantic relationships, yet little is known about user perceptions of this type of advice. This study investigated how people evaluate advice on LLM-generated romantic relationships. Participants rated advice satisfaction, model reliability, and helpfulness, and completed pre- and post-measures of their general attitudes toward LLMs. Overall, the results showed participants' high satisfaction with LLM-generated advice. Greater satisfaction was, in turn, strongly and positively associated with their perceptions of the models' reliability and helpfulness. Importantly, participants' attitudes toward LLMs improved significantly after exposure to the advice, suggesting that supportive and contextually relevant advice can enhance users' trust and openness toward these AI systems.

</details>


### [657] [SNAP: A Plan-Driven Framework for Controllable Interactive Narrative Generation](https://arxiv.org/abs/2601.11529)
*Geonwoo Bang,DongMyung Kim,Hayoung Oh*

Main category: cs.HC

TL;DR: 提出SNAP框架解决基于大语言模型的对话代理在网络环境中的叙事漂移问题，经评估该框架在叙事可控性上表现优越。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的对话代理在响应不同用户输入时存在时空扭曲问题，无法与给定场景保持一致。

Method: 提出SNAP框架，将叙事结构化为带有明确计划的单元，限制每个单元内的上下文并使用详细计划。

Result: 通过自动和人工评估，验证了SNAP在叙事可控性方面的优越性。

Conclusion: SNAP能在网络交互式叙事中，尽管面对不同用户输入仍能有效保持场景一致性。

Abstract: Large Language Models (LLMs) hold great potential for web-based interactive applications, including browser games, online education, and digital storytelling platforms. However, LLM-based conversational agents suffer from spatiotemporal distortions when responding to variant user inputs, failing to maintain consistency with provided scenarios. We propose SNAP (Story and Narrative-based Agent with Planning), a framework that structures narratives into Cells with explicit Plans to prevent narrative drift in web environments. By confining context within each Cell and employing detailed plans that specify spatiotemporal settings, character actions, and plot developments, SNAP enables coherent and scenario-consistent dialogues while adapting to diverse user responses. Via automated and human evaluations, we validate SNAP's superiority in narrative controllability, demonstrating effective scenario consistency despite variant user inputs in web-based interactive storytelling.

</details>


### [658] [AI for Proactive Mental Health: A Multi-Institutional, Longitudinal, Randomized Controlled Trial](https://arxiv.org/abs/2601.11530)
*Julie Y. A. Cachia,Xuan Zhao,John Hunter,Delancey Wu,Eta Lin,Julian De Freitas*

Main category: cs.HC

TL;DR: 研究发现用生成式AI移动应用干预大学生心理健康有效，能带来多种积极影响。


<details>
  <summary>Details</summary>
Motivation: 当代年轻人面临心理健康挑战却因多种阻碍不愿寻求支持，目前缺乏个性化、互动性和可扩展性的技术来提供简短的心理健康干预。

Method: 开展首个多机构、纵向、预注册的随机对照试验，让486名美国本科生在6周内随机分配接收应用访问或成为等待列表对照。

Result: 干预组报告有更显著的积极情绪、恢复力和社会幸福感，且能抵御正念和繁荣感的下降。

Conclusion: 经过有目的和道德的设计，生成式AI可提供预防性、人群层面的幸福干预措施并产生可衡量的益处。

Abstract: Young adults today face unprecedented mental health challenges, yet many hesitate to seek support due to barriers such as accessibility, stigma, and time constraints. Bite-sized well-being interventions offer a promising solution to preventing mental distress before it escalates to clinical levels, but have not yet been delivered through personalized, interactive, and scalable technology. We conducted the first multi-institutional, longitudinal, preregistered randomized controlled trial of a generative AI-powered mobile app ("Flourish") designed to address this gap. Over six weeks in Fall 2024, 486 undergraduate students from three U.S. institutions were randomized to receive app access or waitlist control. Participants in the treatment condition reported significantly greater positive affect, resilience, and social well-being (i.e., increased belonging, closeness to community, and reduced loneliness) and were buffered against declines in mindfulness and flourishing. These findings suggest that, with purposeful and ethical design, generative AI can deliver proactive, population-level well-being interventions that produce measurable benefits.

</details>


### [659] ["Jutters"](https://arxiv.org/abs/2601.11532)
*Meike Driessen,Selina Khan,Gonçalo Marcelino*

Main category: cs.HC

TL;DR: 项目以荷兰海岸拾荒者为视角，打造海滩装置让参观者像拾荒者一样与AI内容互动，鼓励对AI内容更有鉴别性的参与。


<details>
  <summary>Details</summary>
Motivation: 探讨人们如何与AI生成的内容互动，反映AI生成媒体对生活的影响。

Method: 创建融合真实海岸线碎片和AI转换图像、视频的海滩装置，邀请参观者像当代拾荒者一样探索空间并决定保留或丢弃内容。

Result: 打造出相关海滩装置，有视频预览可在指定链接查看。

Conclusion: 该项目将AI图像重新想象为反思素材，鼓励对信息流中的内容更有鉴别性的参与。

Abstract: This project explores how we engage with AI-generated content through the lens of the jutter: Dutch coastal foragers who comb the shoreline after storms, gathering and repurposing what the sea leaves behind. Reflecting how our lives are increasingly shaped by AI-generated media, we create a beach-like installation that blends real shoreline debris with AI-transformed images and videos. Visitors are invited to explore this space as contemporary jutters, deciding what to keep and what to discard. In doing so, the project reimagines AI-imagery as material for reflection, encouraging a more discerning engagement with the content that drifts through our feeds. A video preview of the installation can be found at https://www.youtube.com/watch?v=L6319Ii7MT8.

</details>


### [660] [Artificial Intelligence as a Training Tool in Clinical Psychology: A Comparison of Text-Based and Avatar Simulations](https://arxiv.org/abs/2601.11533)
*V. El Sawah,A. Bhardwaj,A. Pryke-Hobbes,D. Gamaleldin,C. S. Ang,A. K. Martin*

Main category: cs.HC

TL;DR: 研究探讨临床心理学生对基于AI的两种模拟工具（文本聊天机器人和语音化身）的看法，发现两者评价积极，语音化身更优，AI模拟或可补充临床技能训练。


<details>
  <summary>Details</summary>
Motivation: 临床心理学生常觉对治疗工作人际需求准备不足，需练习核心咨询技能的机会，AI模拟可支持早期技能发展。

Method: 24名研究生临床心理学生用两种AI工具完成认知行为角色扮演，提供定量评分和定性反馈。

Result: 两种AI工具各维度评价积极，语音化身在有用性、技能应用和技能提升感方面评分显著高于聊天机器人。

Conclusion: AI驱动模拟可补充早期临床技能训练，语音化身有额外优势，未来需测试模拟互动能否转化为实际治疗表现的客观提升。

Abstract: Clinical psychology students frequently report feeling underprepared for the interpersonal demands of therapeutic work, highlighting the need for accessible opportunities to practise core counselling skills before seeing real clients. Advances in artificial intelligence (AI) now enable simulated interaction partners that may support early skills development. This study examined postgraduate clinical psychology students' perceptions of two AI-based simulations: a text-based chatbot (ChatGPT) and a voice-based avatar (HeyGen). Twenty-four students completed two brief cognitive-behavioural role-plays (counterbalanced), one with each tool, and provided both quantitative ratings and qualitative feedback on perceived usefulness, skill application, responsiveness and engagement, and perceived skill improvement. Both AI tools were evaluated positively across dimensions. However, the avatar was rated significantly higher than the chatbot for perceived usefulness, skill application, and perceived skill improvement, and qualitative comments highlighted the added value of voice-based interaction for conveying social and emotional cues. These findings suggest that AI-driven simulation may supplement early-stage clinical skills training, with voice-based avatars offering additional benefits. Future work should test whether such simulated interactions translate to objective improvements in real therapeutic performance.

</details>


### [661] [Modular AI-Powered Interviewer with Dynamic Question Generation and Expertise Profiling](https://arxiv.org/abs/2601.11534)
*Aisvarya Adeseye,Jouni Isoaho,Seppo Virtanen,Mohammad Tahir*

Main category: cs.HC

TL;DR: 现有自动化面试官和聊天机器人存在不足，本文提出基于本地大语言模型的AI面试官，测试满意度和参与度高，是可扩展且注重隐私的定性数据收集方案。


<details>
  <summary>Details</summary>
Motivation: 现有系统使用固定问题列表、规则严格、个性化有限，导致对话重复、参与度低，不适用于复杂定性研究，需更自适应和上下文感知的面试系统。

Method: 构建基于本地大语言模型的AI面试官，实时分析参与者专业知识以生成合适问题和回复；设计模块化提示工程管道保证对话可扩展、自适应和语义丰富。

Result: 对不同参与者进行测试，满意度平均4.45，参与度平均4.33。

Conclusion: 提出的AI面试官是可扩展、注重隐私的解决方案，推动了AI辅助定性数据收集。

Abstract: Automated interviewers and chatbots are common in research, recruitment, customer service, and education. Many existing systems use fixed question lists, strict rules, and limited personalization, leading to repeated conversations that cause low engagement. Therefore, these tools are not effective for complex qualitative research, which requires flexibility, context awareness, and ethical sensitivity. Consequently, there is a need for a more adaptive and context-aware interviewing system. To address this, an AI-powered interviewer that dynamically generates questions that are contextually appropriate and expertise aligned is presented in this study. The interviewer is built on a locally hosted large language model (LLM) that generates coherent dialogue while preserving data privacy. The interviewer profiles the participants' expertise in real time to generate knowledge-appropriate questions, well-articulated responses, and smooth transition messages similar to human-like interviews. To implement these functionalities, a modular prompt engineering pipeline was designed to ensure that the interview conversation remains scalable, adaptive, and semantically rich. To evaluate the AI-powered interviewer, it was tested with various participants, and it achieved high satisfaction (mean 4.45) and engagement (mean 4.33). The proposed interviewer is a scalable, privacy-conscious solution that advances AI-assisted qualitative data collection.

</details>


### [662] [Augmented Assembly: Object Recognition and Hand Tracking for Adaptive Assembly Instructions in Augmented Reality](https://arxiv.org/abs/2601.11535)
*Alexander Htet Kyaw,Haotian Ma,Sasa Zivkovic,Jenny Sabin*

Main category: cs.HC

TL;DR: 本文提出一种AR辅助装配工作流，利用目标识别和手部跟踪实现组件识别、显示装配指令、检测偏差和动态更新指令，并通过案例证明其可消除手动操作需求。


<details>
  <summary>Details</summary>
Motivation: 利用AR技术发展，设计可辅助用户进行物理装配任务的交互式系统。

Method: 使用目标识别实时检测和定位组件以创建数字孪生工作空间，通过手部跟踪验证用户交互部件，不强制固定顺序，高亮显示潜在装配错误。

Result: 通过乐高积木和定制3D打印组件的案例研究，展示系统将数字指令与物理装配相连接。

Conclusion: 该AR辅助装配工作流能消除手动搜索、分类或标记部件的需求。

Abstract: Recent advances in augmented reality (AR) have enabled interactive systems that assist users in physical assembly tasks. In this paper, we present an AR-assisted assembly workflow that leverages object recognition and hand tracking to (1) identify custom components, (2) display step-by-step instructions, (3) detect assembly deviations, and (4) dynamically update the instructions based on users' hands-on interactions with physical parts. Using object recognition, the system detects and localizes components in real time to create a digital twin of the workspace. For each assembly step, it overlays bounding boxes in AR to indicate both the current position and the target placement of relevant components, while hand-tracking data verifies whether the user interacts with the correct part. Rather than enforcing a fixed sequence, the system highlights potential assembly errors and interprets user deviations as opportunities for iteration and creative exploration. A case study with LEGO blocks and custom 3D-printed components demonstrates how the system links digital instructions to physical assembly, eliminating the need for manual searching, sorting, or labeling of parts.

</details>


### [663] [A Comparative Study of Technical Writing Feedback Quality: Evaluating LLMs, SLMs, and Humans in Computer Science Topics](https://arxiv.org/abs/2601.11541)
*Suqing Liu,Bogdan Simion,Christopher Eaton,Michael Liut*

Main category: cs.HC

TL;DR: 研究比较大语言模型（LLMs）、小语言模型（SLMs）与人类反馈在三门含技术写作的计算机科学课程中的质量，发现不同课程学生偏好不同，强调结合AI与人类反馈的混合方法潜力。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs、SLMs生成的反馈与人类反馈在计算机科学教育中的质量差异。

Method: 采用混合方法，结合定量李克特量表问题和定性评论，从多个标准分析学生对反馈质量的看法。

Result: 在操作系统课程中，SLMs和LLMs反馈清晰、可操作，人类提供更具情境细节的指导；CS2课程学生偏好AI工具的清晰和广泛，但认为AI反馈缺乏人类的简洁；在AI技术写作课程中，学生更青睐教师的个性化反馈。还强调了AI反馈的可扩展性。

Conclusion: 结合AI和人类反馈的混合方法有潜力实现高效、高质量的大规模反馈。

Abstract: Feedback is a critical component of the learning process, particularly in computer science education. This study investigates the quality of feedback generated by Large Language Models (LLMs), Small Language Models (SLMs), compared with human feedback, in three computer science course with technical writing components: an introductory computer science course (CS2), a third-year advanced systems course (operating systems), and a third-year writing course (a topics course on artificial intelligence). Using a mixed-methods approach which integrates quantitative Likert-scale questions with qualitative commentary, we analyze the student perspective on feedback quality, evaluated based on multiple criteria, including readability, detail, specificity, actionability, helpfulness, and overall quality. The analysis reveals that in the larger upper-year operating systems course ($N=80$), SLMs and LLMs are perceived to deliver clear, actionable, and well-structured feedback, while humans provide more contextually nuanced guidance. As for the high-enrollment CS2 course ($N=176$) showed the same preference for the AI tools' clarity and breadth, but students noted that AI feedback sometimes lacked the concise, straight-to-the-point, guidance offered by humans. Conversely, in the smaller upper-year technical writing course on AI topics ($N=7$), all students preferred feedback from the course instructor, who was able to provide clear, specific, and personalized feedback, compared to the more general and less targeted AI-based feedback. We also highlight the scalability of AI-based feedback by focusing on its effectiveness at large scale. Our findings underscore the potential of hybrid approaches that combine AI and human feedback to achieve efficient and high-quality feedback at scale.

</details>


### [664] [Medication counseling with large language models: balancing flexibility and rigidity](https://arxiv.org/abs/2601.11544)
*Joar Sabel,Mattias Wingren,Andreas Lundell,Sören Andersson,Sara Rosenberg,Susanne Hägglund,Linda Estman,Malin Andtfolk*

Main category: cs.HC

TL;DR: 介绍大语言模型增强软件智能体能力，提出在药房场景采用窄而长任务策略，给出平衡对话要求与灵活性的原型系统及设计方法，同时指出后续工作方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在药房场景中，交互灵活性与准确性难以平衡，宽范围策略有局限，需探索更合适的方法。

Method: 聚焦窄而长任务，设计原型系统，采用满足对话要求、减少幻觉和促进高质量回复的方法。

Result: 设计的方法有潜力提高系统确定性，保留大语言模型的动态对话能力。

Conclusion: 此类系统开发需持续测试、人在环参与，且要在常用基准之外评估。

Abstract: The introduction of large language models (LLMs) has greatly enhanced the capabilities of software agents. Instead of relying on rule-based interactions, agents can now interact in flexible ways akin to humans. However, this flexibility quickly becomes a problem in fields where errors can be disastrous, such as in a pharmacy context, but the opposite also holds true; a system that is too inflexible will also lead to errors, as it can become too rigid to handle situations that are not accounted for. Work using LLMs in a pharmacy context have adopted a wide scope, accounting for many different medications in brief interactions -- our strategy is the opposite: focus on a more narrow and long task. This not only enables a greater understanding of the task at hand, but also provides insight into what challenges are present in an interaction of longer nature. The main challenge, however, remains the same for a narrow and wide system: it needs to strike a balance between adherence to conversational requirements and flexibility. In an effort to strike such a balance, we present a prototype system meant to provide medication counseling while juggling these two extremes. We also cover our design in constructing such a system, with a focus on methods aiming to fulfill conversation requirements, reduce hallucinations and promote high-quality responses. The methods used have the potential to increase the determinism of the system, while simultaneously not removing the dynamic conversational abilities granted by the usage of LLMs. However, a great deal of work remains ahead, and the development of this kind of system needs to involve continuous testing and a human-in-the-loop. It should also be evaluated outside of commonly used benchmarks for LLMs, as these do not adequately capture the complexities of this kind of conversational system.

</details>


### [665] [PASTA: A Scalable Framework for Multi-Policy AI Compliance Evaluation](https://arxiv.org/abs/2601.11702)
*Yu Yang,Ig-Jae Kim,Dongwook Yoon*

Main category: cs.HC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: AI compliance is becoming increasingly critical as AI systems grow more powerful and pervasive. Yet the rapid expansion of AI policies creates substantial burdens for resource-constrained practitioners lacking policy expertise. Existing approaches typically address one policy at a time, making multi-policy compliance costly. We present PASTA, a scalable compliance tool integrating four innovations: (1) a comprehensive model-card format supporting descriptive inputs across development stages; (2) a policy normalization scheme; (3) an efficient LLM-powered pairwise evaluation engine with cost-saving strategies; and (4) an interface delivering interpretable evaluations via compliance heatmaps and actionable recommendations. Expert evaluation shows PASTA's judgments closely align with human experts ($ρ\geq .626$). The system evaluates five major policies in under two minutes at approximately \$3. A user study (N = 12) confirms practitioners found outputs easy-to-understand and actionable, introducing a novel framework for scalable automated AI governance.

</details>


### [666] [Human-Human-AI Triadic Programming: Uncovering the Role of AI Agent and the Value of Human Partner in Collaborative Learning](https://arxiv.org/abs/2601.12134)
*Taufiq Daryanto,Xiaohan Ding,Kaike Ping,Lance T. Wilhelm,Yan Chen,Chris Brown,Eugenia H. Rho*

Main category: cs.HC

TL;DR: 提出人-人-人工智能（HHAI）三元编程模式，通过研究表明其比二元人机（HAI）协作更能提升学习与社交感受，且减少对AI代码依赖。


<details>
  <summary>Details</summary>
Motivation: 过往研究将AI视为人类协作替代品，忽视协作编程中的社交与学习方面，因此提出HHAI三元编程模式。

Method: 进行了一项有20名参与者的受试者内研究。

Result: 三元协作比二元基线更能提升协作学习与社交感受，参与者对AI代码依赖显著减少，在HHAI共享条件下效果最强。

Conclusion: 三元设置能激活学习的社会共享调节，增强同伴协作而非自动化的AI系统可更好保留协作编程的学习过程。

Abstract: As AI assistance becomes embedded in programming practice, researchers have increasingly examined how these systems help learners generate code and work more efficiently. However, these studies often position AI as a replacement for human collaboration and overlook the social and learning-oriented aspects that emerge in collaborative programming. Our work introduces human-human-AI (HHAI) triadic programming, where an AI agent serves as an additional collaborator rather than a substitute for a human partner. Through a within-subjects study with 20 participants, we show that triadic collaboration enhances collaborative learning and social presence compared to the dyadic human-AI (HAI) baseline. In the triadic HHAI conditions, participants relied significantly less on AI-generated code in their work. This effect was strongest in the HHAI-shared condition, where participants had an increased sense of responsibility to understand AI suggestions before applying them. These findings demonstrate how triadic settings activate socially shared regulation of learning by making AI use visible and accountable to a human peer, suggesting that AI systems that augment rather than automate peer collaboration can better preserve the learning processes that collaborative programming relies on.

</details>


### [667] [Predictive Prototyping: Evaluating Design Concepts with ChatGPT](https://arxiv.org/abs/2601.12276)
*Hilsann Yong,Bradley A. Camburn*

Main category: cs.HC

TL;DR: 文章探讨GPT能否预测原型制作信息，引入RAG方法，经两项研究表明GPT - RAG估算成本和性能更准，其指导的原型表现更佳，重复查询取平均能提高准确性。


<details>
  <summary>Details</summary>
Motivation: 物理原型制作慢且贵，有意义的评估常受限，因此研究GPT能否预测原型制作相关信息。

Method: 引入RAG方法，利用OpenAI GPT - 4o和从Instructables.com获取的原型数据；开展两项研究，一是对照实验比较GPT - RAG和人类设计师预测结果，二是应用示范对比GPT - RAG指导的原型与商业基线和拓扑优化设计。

Result: GPT - RAG比人类估算成本和性能更准确，可用性见解相当；GPT - RAG指导的原型表现优于对比原型；重复查询取平均可提高准确性。

Conclusion: 大语言模型能模拟符合大数定律的群体聚合效应

Abstract: The design-build-test cycle is essential for innovation, but physical prototyping is often slow and expensive. Although physics-based simulation and strategic prototyping can reduce cost, meaningful evaluation is frequently constrained until an integrated prototype is built. This paper investigates whether a generative pretrained transformer (GPT) can predict information typically obtained through prototyping, including cost, performance, and perceived usability. We introduce a retrieval-augmented generation (RAG) method to emulate design feedback using OpenAI GPT-4o, grounded in prototyping data scraped from Instructables.com to increase access to relevant precedent. Two studies are reported. First, a controlled experiment compares GPT-RAG and human designers, who receive design sketches and predict cost, performance, and usability; predictions are evaluated against ground-truth results from physical prototypes. Second, we report an applied demonstration in which a physical prototype is produced from GPT-RAG recommendations and compared with a commercial baseline and a topology-optimized design. Results show that GPT-RAG provides more accurate cost and performance estimates than individual or crowd human estimates, while yielding comparable usability insights; the GPT-RAG-informed prototype also outperforms both comparison prototypes. Repeated querying with response averaging significantly improves accuracy, suggesting that LLMs can emulate crowd aggregation effects consistent with the law of large numbers.

</details>


### [668] [HCFT: Hierarchical Convolutional Fusion Transformer for EEG Decoding](https://arxiv.org/abs/2601.12279)
*Haodong Zhang,Jiapeng Zhu,Yitong Chen,Hongqi Li*

Main category: cs.HC

TL;DR: 提出轻量级可泛化的EEG解码框架HCFT，结合卷积编码器和Transformer块，在两个数据集上表现优于十种基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决从多通道信号中有效提取和整合复杂时空频谱特征的挑战。

Method: 提出Hierarchical Convolutional Fusion Transformer (HCFT)，结合双分支卷积编码器和分层Transformer块，引入定制的Dynamic Tanh归一化模块。

Result: 在BCI IV - 2b上平均准确率80.83%，Cohen's kappa为0.6165；在CHB - MIT上灵敏度99.10%，每小时误报0.0236次，特异性98.82%，优于十种基线方法。

Conclusion: HCFT能有效捕捉EEG动态，在现实BCI应用中有潜力。

Abstract: Electroencephalography (EEG) decoding requires models that can effectively extract and integrate complex temporal, spectral, and spatial features from multichannel signals. To address this challenge, we propose a lightweight and generalizable decoding framework named Hierarchical Convolutional Fusion Transformer (HCFT), which combines dual-branch convolutional encoders and hierarchical Transformer blocks for multi-scale EEG representation learning. Specifically, the model first captures local temporal and spatiotemporal dynamics through time-domain and time-space convolutional branches, and then aligns these features via a cross-attention mechanism that enables interaction between branches at each stage. Subsequently, a hierarchical Transformer fusion structure is employed to encode global dependencies across all feature stages, while a customized Dynamic Tanh normalization module is introduced to replace traditional Layer Normalization in order to enhance training stability and reduce redundancy. Extensive experiments are conducted on two representative benchmark datasets, BCI Competition IV-2b and CHB-MIT, covering both event-related cross-subject classification and continuous seizure prediction tasks. Results show that HCFT achieves 80.83% average accuracy and a Cohen's kappa of 0.6165 on BCI IV-2b, as well as 99.10% sensitivity, 0.0236 false positives per hour, and 98.82% specificity on CHB-MIT, consistently outperforming over ten state-of-the-art baseline methods. Ablation studies confirm that each core component of the proposed framework contributes significantly to the overall decoding performance, demonstrating HCFT's effectiveness in capturing EEG dynamics and its potential for real-world BCI applications.

</details>


### [669] [Do MLLMs See What We See? Analyzing Visualization Literacy Barriers in AI Systems](https://arxiv.org/abs/2601.12585)
*Mengli,Duan,Yuhe,Jiang,Matthew Varona,Carolina Nobre*

Main category: cs.HC

TL;DR: 系统性分析多模态大语言模型（MLLMs）可视化理解障碍，得出失败分类，发现特定障碍，对可视化理解有深入发现并为未来评估和设计提供参考。


<details>
  <summary>Details</summary>
Motivation: 目前对MLLMs解释可视化失败原因了解甚少，旨在进行系统性分析可视化理解障碍。

Method: 使用再生可视化素养评估测试（reVLAT）基准和合成数据，采用障碍中心策略对四个模型的309个错误响应进行开放编码。

Result: 得出MLLM失败分类，发现两种特定障碍，模型在简单图表表现好，在颜色密集、基于分段可视化上有困难，难以形成一致比较推理。

Conclusion: 研究结果为可靠AI驱动可视化助手的未来评估和设计提供依据。

Abstract: Multimodal Large Language Models (MLLMs) are increasingly used to interpret visualizations, yet little is known about why they fail. We present the first systematic analysis of barriers to visualization literacy in MLLMs. Using the regenerated Visualization Literacy Assessment Test (reVLAT) benchmark with synthetic data, we open-coded 309 erroneous responses from four state-of-the-art models with a barrier-centric strategy adapted from human visualization literacy research. Our analysis yields a taxonomy of MLLM failures, revealing two machine-specific barriers that extend prior human-participation frameworks. Results show that models perform well on simple charts but struggle with color-intensive, segment-based visualizations, often failing to form consistent comparative reasoning. Our findings inform future evaluation and design of reliable AI-driven visualization assistants.

</details>


### [670] [Creating Disability Story Videos with Generative AI: Motivation, Expression, and Sharing](https://arxiv.org/abs/2601.12617)
*Shuo Niu,Dylan Clements,Hyungsin Kim*

Main category: cs.HC

TL;DR: 研究残疾倡导组织的9名残疾人使用生成式AI创作视频分享残疾经历，得出重大描绘框架并讨论设计启示


<details>
  <summary>Details</summary>
Motivation: 生成式AI在支持残疾人创作残疾故事方面既有机遇也有挑战，需研究其使用情况

Method: 以数字叙事理论为基础，研究9名残疾人使用生成式AI创作故事视频的动机、表达和分享情况

Result: 得到重大描绘框架，指出生成式AI有非可捕捉描绘、身份隐藏与呈现、情境真实与一致和情感表达4种核心功能

Conclusion: 基于框架讨论了生成式AI在故事完成、媒体格式和纠正机制方面的设计启示

Abstract: Generative AI (GenAI) is both promising and challenging in supporting people with disabilities (PwDs) in creating stories about disability. GenAI can reduce barriers to media production and inspire the creativity of PwDs, but it may also introduce biases and imperfections that hinder its adoption for personal expression. In this research, we examine how nine PwD from a disability advocacy group used GenAI to create videos sharing their disability experiences. Grounded in digital storytelling theory, we explore the motivations, expression, and sharing of PwD-created GenAI story videos. We conclude with a framework of momentous depiction, which highlights four core affordances of GenAI that either facilitate or require improvements to better support disability storytelling: non-capturable depiction, identity concealment and representation, contextual realism and consistency, and emotional articulation. Based on this framework, we further discuss design implications for GenAI in relation to story completion, media formats, and corrective mechanisms.

</details>


### [671] [AI-exhibited Personality Traits Can Shape Human Self-concept through Conversations](https://arxiv.org/abs/2601.12727)
*Jingshu Li,Tianqi Song,Nattapat Boonprakong,Zicheng Zhu,Yitian Yang,Yi-Chieh Lee*

Main category: cs.HC

TL;DR: 研究发现与基于大语言模型的AI聊天后，用户自我概念会与AI人格特征趋同，时长越长趋同越高，还与聊天愉悦度正相关。


<details>
  <summary>Details</summary>
Motivation: 由于人类对AI人格特质的理解会受交互对象影响，可能存在AI特质影响用户自我概念的风险，需探索其可能性。

Method: 进行了随机行为实验。

Result: 与使用GPT - 4默认人格特质的AI聊天后，用户自我概念与AI人格特征对齐，对话越长对齐度越高，且导致用户自我概念趋同，对齐度与聊天愉悦度正相关。

Conclusion: 揭示了AI人格特质通过人机对话影响用户自我概念的方式，指出风险与机遇，为开发负责任和道德的AI系统提供设计启示。

Abstract: Recent Large Language Model (LLM) based AI can exhibit recognizable and measurable personality traits during conversations to improve user experience. However, as human understandings of their personality traits can be affected by their interaction partners' traits, a potential risk is that AI traits may shape and bias users' self-concept of their own traits. To explore the possibility, we conducted a randomized behavioral experiment. Our results indicate that after conversations about personal topics with an LLM-based AI chatbot using GPT-4o default personality traits, users' self-concepts aligned with the AI's measured personality traits. The longer the conversation, the greater the alignment. This alignment led to increased homogeneity in self-concepts among users. We also observed that the degree of self-concept alignment was positively associated with users' conversation enjoyment. Our findings uncover how AI personality traits can shape users' self-concepts through human-AI conversation, highlighting both risks and opportunities. We provide important design implications for developing more responsible and ethical AI systems.

</details>


### [672] [TreeWriter: AI-Assisted Hierarchical Planning and Writing for Long-Form Documents](https://arxiv.org/abs/2601.12740)
*Zijian Zhang,Fangshi Du,Xingjian Liu,Pan Chen,Oliver Huang,Runlong Ye,Michael Liut,Alán Aspuru-Guzik*

Main category: cs.HC

TL;DR: 介绍TreeWriter层次化写作系统，其文档以树状表示并集成AI支持，研究表明它能改善写作多方面能力且支持协作写作，为未来工具提供设计指南。


<details>
  <summary>Details</summary>
Motivation: 当前智能写作系统处理长文档存在维持一致性、高效规划写作及提供有效AI协助等挑战，现有的AI写作工具很少支持完整写作过程。

Method: 开发TreeWriter系统，开展有12人参与的内主体研究对比其与Google Docs + Gemini 的效果，进行为期两个月、8人参与的实地部署。

Result: TreeWriter在长文档编辑和创意写作任务中改善了想法探索/发展、AI帮助性和作者控制感，层次化组织支持协作写作。

Conclusion: 具有集成AI支持的层次化、树状结构编辑器有潜力，为未来AI辅助写作工具提供平衡自动化和用户自主性的设计指南。

Abstract: Long documents pose many challenges to current intelligent writing systems. These include maintaining consistency across sections, sustaining efficient planning and writing as documents become more complex, and effectively providing and integrating AI assistance to the user. Existing AI co-writing tools offer either inline suggestions or limited structured planning, but rarely support the entire writing process that begins with high-level ideas and ends with polished prose, in which many layers of planning and outlining are needed. Here, we introduce TreeWriter, a hierarchical writing system that represents documents as trees and integrates contextual AI support. TreeWriter allows authors to create, save, and refine document outlines at multiple levels, facilitating drafting, understanding, and iterative editing of long documents. A built-in AI agent can dynamically load relevant content, navigate the document hierarchy, and provide context-aware editing suggestions. A within-subject study (N=12) comparing TreeWriter with Google Docs + Gemini on long-document editing and creative writing tasks shows that TreeWriter improves idea exploration/development, AI helpfulness, and perceived authorial control. A two-month field deployment (N=8) further demonstrated that hierarchical organization supports collaborative writing. Our findings highlight the potential of hierarchical, tree-structured editors with integrated AI support and provide design guidelines for future AI-assisted writing tools that balance automation with user agency.

</details>


### [673] [PAIR-SAFE: A Paired-Agent Approach for Runtime Auditing and Refining AI-Mediated Mental Health Support](https://arxiv.org/abs/2601.12754)
*Jiwon Kim,Violeta J. Rodriguez,Dong Whi Yoo,Eshwar Chandrasekharan,Koustuv Saha*

Main category: cs.HC

TL;DR: 介绍PAIR - SAFE框架用于审计和改进AI生成的心理健康支持，模拟咨询交互显示有显著改善，证明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于心理健康支持时存在回应问题，现有风险缓解方法透明度和运行时问责性有限。

Method: 引入PAIR - SAFE，一个将响应者代理与基于MITI - 4框架的监督法官代理相结合的框架，法官审计响应并提供决策以指导运行时响应改进，用支持寻求者模拟器模拟咨询交互。

Result: 法官监督的交互在关键MITI维度上显著改善，定量结果得到定性专家评估支持。

Conclusion: 成对代理方法可为AI辅助的对话式心理健康支持提供基于临床的审计和改进。

Abstract: Large language models (LLMs) are increasingly used for mental health support, yet they can produce responses that are overly directive, inconsistent, or clinically misaligned, particularly in sensitive or high-risk contexts. Existing approaches to mitigating these risks largely rely on implicit alignment through training or prompting, offering limited transparency and runtime accountability. We introduce PAIR-SAFE, a paired-agent framework for auditing and refining AI-generated mental health support that integrates a Responder agent with a supervisory Judge agent grounded in the clinically validated Motivational Interviewing Treatment Integrity (MITI-4) framework. The Judgeaudits each response and provides structuredALLOW or REVISE decisions that guide runtime response refinement. We simulate counseling interactions using a support-seeker simulator derived from human-annotated motivational interviewing data. We find that Judge-supervised interactions show significant improvements in key MITI dimensions, including Partnership, Seek Collaboration, and overall Relational quality. Our quantitative findings are supported by qualitative expert evaluation, which further highlights the nuances of runtime supervision. Together, our results reveal that such pairedagent approach can provide clinically grounded auditing and refinement for AI-assisted conversational mental health support.

</details>


### [674] [RubRIX: Rubric-Driven Risk Mitigation in Caregiver-AI Interactions](https://arxiv.org/abs/2601.13235)
*Drishti Goel,Jeongah Lee,Qiuyue Joy Zhong,Violeta J. Rodriguez,Daniel S. Brown,Ravi Karkar,Dong Whi Yoo,Koustuv Saha*

Main category: cs.HC

TL;DR: 现有AI评估框架难以满足护理场景需求，本文引入RubRIX框架评估大语言模型护理响应风险，经评估和改进降低了风险，并强调领域敏感评估的重要性，还发布了基准数据集。


<details>
  <summary>Details</summary>
Motivation: 现有AI评估框架主要关注一般风险，无法充分捕捉大语言模型在护理场景中响应的细微风险，需要新的评估框架。

Method: 引入基于理论且经临床验证的RubRIX框架，该框架基于关怀伦理要素，涵盖五个风险维度，对六个最先进的大语言模型在超20000个护理查询上进行评估。

Result: 通过Rubric引导的改进，一次迭代后各模型的风险成分一致降低了45 - 98%。

Conclusion: 为高负担场景开发了以用户为中心、领域敏感的评估框架方法，强调领域敏感和交互风险评估对大语言模型在护理支持场景中负责任部署的重要性，并发布基准数据集以推动未来研究。

Abstract: Caregivers seeking AI-mediated support express complex needs -- information-seeking, emotional validation, and distress cues -- that warrant careful evaluation of response safety and appropriateness. Existing AI evaluation frameworks, primarily focused on general risks (toxicity, hallucinations, policy violations, etc), may not adequately capture the nuanced risks of LLM-responses in caregiving-contexts. We introduce RubRIX (Rubric-based Risk Index), a theory-driven, clinician-validated framework for evaluating risks in LLM caregiving responses. Grounded in the Elements of an Ethic of Care, RubRIX operationalizes five empirically-derived risk dimensions: Inattention, Bias & Stigma, Information Inaccuracy, Uncritical Affirmation, and Epistemic Arrogance. We evaluate six state-of-the-art LLMs on over 20,000 caregiver queries from Reddit and ALZConnected. Rubric-guided refinement consistently reduced risk-components by 45-98% after one iteration across models. This work contributes a methodological approach for developing domain-sensitive, user-centered evaluation frameworks for high-burden contexts. Our findings highlight the importance of domain-sensitive, interactional risk evaluation for the responsible deployment of LLMs in caregiving support contexts. We release benchmark datasets to enable future research on contextual risk evaluation in AI-mediated support.

</details>


### [675] [The AI Genie Phenomenon and Three Types of AI Chatbot Addiction: Escapist Roleplays, Pseudosocial Companions, and Epistemic Rabbit Holes](https://arxiv.org/abs/2601.13348)
*M. Karen Shen,Jessica Huang,Olivia Liang,Ig-Jae Kim,Dongwook Yoon*

Main category: cs.HC

TL;DR: 研究分析Reddit上用户使用AI聊天机器人成瘾经历，发现成瘾与‘AI精灵’现象有关，有三种成瘾类型等，为预防等策略奠定基础。


<details>
  <summary>Details</summary>
Motivation: 近期对生成式AI聊天机器人使用报告引发成瘾担忧，但该领域研究不足，需深入了解以降低风险。

Method: 对14个子版块334条Reddit帖子进行主题分析，并开展探索性数据分析。

Result: 发现成瘾与‘AI精灵’现象有关；有三种成瘾类型；多例涉及性内容；不同成瘾类型对恢复策略感知帮助不同。

Conclusion: 本研究为未来预防、诊断和干预策略奠定实证基础。

Abstract: Recent reports on generative AI chatbot use raise concerns about its addictive potential. An in-depth understanding is imperative to minimize risks, yet AI chatbot addiction remains poorly understood. This study examines how to characterize AI chatbot addiction--why users become addicted, the symptoms commonly reported, and the distinct types it comprises. We conducted a thematic analysis of Reddit entries (n=334) across 14 subreddits where users narrated their experiences with addictive AI chatbot use, followed by an exploratory data analysis. We found: (1) users' dependence tied to the "AI Genie" phenomenon--users can get exactly anything they want with minimal effort--and marked by symptoms that align with addiction literature, (2) three distinct addiction types: Escapist Roleplay, Pseudosocial Companion, and Epistemic Rabbit Hole, (3) sexual content involved in multiple cases, and (4) recovery strategies' perceived helpfulness differ between addiction types. Our work lays empirical groundwork to inform future strategies for prevention, diagnosis, and intervention.

</details>


### [676] [Integrating Virtual Reality and Large Language Models for Team-Based Non-Technical Skills Training and Evaluation in the Operating Room](https://arxiv.org/abs/2601.13406)
*Jacob Barker,Doga Demirel,Cullen Jackson,Anna Johansson,Robbin Miraglia,Darian Hoagland,Stephanie B. Jones,John Mitchell,Daniel B. Jones,Suvranu De*

Main category: cs.HC

TL;DR: 介绍多用户虚拟现实平台VORTeX，可训练和评估手术团队非技术技能，经试点评估效果良好，提供了可扩展、合规的评估框架。


<details>
  <summary>Details</summary>
Motivation: 外科手术中非技术技能结构化培训有限，ACS/APDS课程需要可扩展工具来教学和评估腹腔镜紧急情况下的非技术技能。

Method: 引入VORTeX平台，结合沉浸式团队模拟和大语言模型分析，用NOTSS框架分析团队对话，实施两个腹腔镜紧急场景。

Result: 12名外科专业人员试点评价VORTeX直观、沉浸且有价值，大语言模型生成的通信网络反映了预期的手术层级。

Conclusion: VORTeX结合沉浸式VR和大语言模型驱动的行为分析，为分布式培训环境提供了可扩展、合规的客观评估和数据驱动的反馈框架。

Abstract: Although effective teamwork and communication are critical to surgical safety, structured training for non-technical skills (NTS) remains limited compared with technical simulation. The ACS/APDS Phase III Team-Based Skills Curriculum calls for scalable tools that both teach and objectively assess these competencies during laparoscopic emergencies. We introduce the Virtual Operating Room Team Experience (VORTeX), a multi-user virtual reality (VR) platform that integrates immersive team simulation with large language model (LLM) analytics to train and evaluate communication, decision-making, teamwork, and leadership. Team dialogue is analyzed using structured prompts derived from the Non-Technical Skills for Surgeons (NOTSS) framework, enabling automated classification of behaviors and generation of directed interaction graphs that quantify communication structure and hierarchy. Two laparoscopic emergency scenarios, pneumothorax and intra-abdominal bleeding, were implemented to elicit realistic stress and collaboration. Twelve surgical professionals completed pilot sessions at the 2024 SAGES conference, rating VORTeX as intuitive, immersive, and valuable for developing teamwork and communication. The LLM consistently produced interpretable communication networks reflecting expected operative hierarchies, with surgeons as central integrators, nurses as initiators, and anesthesiologists as balanced intermediaries. By integrating immersive VR with LLM-driven behavioral analytics, VORTeX provides a scalable, privacy-compliant framework for objective assessment and automated, data-informed debriefing across distributed training environments.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [677] [Quantum Data Structure for Range Minimum Query](https://arxiv.org/abs/2601.13195)
*Qisheng Wang,Zhean Xu,Zhicheng Zhang*

Main category: quant-ph

TL;DR: 提出一种支持RMQ查询和范围更新的量子数据结构，在无预处理下，对于q = O(n)个操作，时间复杂度为最优的tilde Θ(sqrt(nq))，且获得无量子随机存取存储器的k - 最小查找的高效量子算法。


<details>
  <summary>Details</summary>
Motivation: 解决RMQ问题，即维护支持RMQ查询的数据结构并实现范围更新，探索更优时间复杂度的算法。

Method: 提出一种量子数据结构。

Result: 对于q = O(n)个操作，在无预处理时，时间复杂度为最优的tilde Θ(sqrt(nq))，优于经典的tilde Θ(n + q)；得到无量子随机存取存储器的k - 最小查找的高效量子算法。

Conclusion: 提出的量子数据结构能有效解决RMQ问题，在时间复杂度上有优势，并可应用于k - 最小查找问题。

Abstract: Given an array $a[1..n]$, the Range Minimum Query (RMQ) problem is to maintain a data structure that supports RMQ queries: given a range $[l, r]$, find the index of the minimum element among $a[l..r]$, i.e., $\operatorname{argmin}_{i \in [l, r]} a[i]$. In this paper, we propose a quantum data structure that supports RMQ queries and range updates, with an optimal time complexity $\widetilde Θ(\sqrt{nq})$ for performing $q = O(n)$ operations without preprocessing, compared to the classical $\widetildeΘ(n+q)$. As an application, we obtain a time-efficient quantum algorithm for $k$-minimum finding without the use of quantum random access memory.

</details>


### [678] [Impact of Circuit Depth versus Qubit Count on Variational Quantum Classifiers for Higgs Boson Signal Detection](https://arxiv.org/abs/2601.11937)
*Fatih Maulana*

Main category: quant-ph

TL;DR: 研究探索变分量子分类器检测希格斯玻色子信号性能，比较不同量子电路配置，发现增加电路深度比增加量子比特数更利于高能物理数据异常检测。


<details>
  <summary>Details</summary>
Motivation: 高能物理实验产生海量数据挑战经典计算极限，量子机器学习有处理高维数据潜力，但找到适用于当前NISQ设备的最优架构是挑战，本研究聚焦用VQC检测希格斯玻色子信号。

Method: 使用主成分分析进行降维，将30个物理特征映射到4比特和8比特潜空间，对比浅4比特电路、带更多纠缠层的深4比特电路、扩展8比特电路三种配置。

Result: 增加电路深度显著提升性能，深4比特电路（配置B）准确率达56.2%，高于基线51.9%；扩展到8比特电路因优化问题准确率降至50.6%。

Conclusion: 对于近期量子硬件，优先考虑电路深度和纠缠能力对高能物理数据有效异常检测比增加量子比特数更关键。

Abstract: High-Energy Physics (HEP) experiments, such as those at the Large Hadron Collider (LHC), generate massive datasets that challenge classical computational limits. Quantum Machine Learning (QML) offers a potential advantage in processing high-dimensional data; however, finding the optimal architecture for current Noisy Intermediate-Scale Quantum (NISQ) devices remains an open challenge. This study investigates the performance of Variational Quantum Classifiers (VQC) in detecting Higgs Boson signals using the ATLAS Higgs Boson Machine Learning Challenge 2014 experiment dataset. We implemented a dimensionality reduction pipeline using Principal Component Analysis (PCA) to map 30 physical features into 4-qubit and 8-qubit latent spaces. We benchmarked three configurations: (A) a shallow 4-qubit circuit, (B) a deep 4-qubit circuit with increased entanglement layers, and (C) an expanded 8-qubit circuit. Experimental results demonstrate that increasing circuit depth significantly improves performance, yielding the highest accuracy of 56.2% (Configuration B), compared to a baseline of 51.9%. Conversely, simply scaling to 8 qubits resulted in a performance degradation to 50.6% due to optimization challenges associated with Barren Plateaus in the larger Hilbert space. These findings suggest that for near-term quantum hardware, prioritizing circuit depth and entanglement capability is more critical than increasing qubit count for effective anomaly detection in HEP data.

</details>


### [679] [A Mixture of Experts Vision Transformer for High-Fidelity Surface Code Decoding](https://arxiv.org/abs/2601.12483)
*Hoang Viet Nguyen,Manh Hung Nguyen,Hoang Ta,Van Khu Vu,Yeow Meng Chee*

Main category: quant-ph

TL;DR: 提出量子解码器QuantumSMoE，可结合代码结构并提升可扩展性，在实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有解码器存在计算开销大、未充分利用拓扑代码结构等问题，影响可扩展实时操作。

Method: 提出基于量子视觉变压器的解码器QuantumSMoE，通过加形状嵌入和自适应掩码结合代码结构，用专家混合层和新辅助损失提升可扩展性。

Result: 在曲面码实验中，QuantumSMoE优于最先进的机器学习解码器和广泛使用的经典基线。

Conclusion: QuantumSMoE是一种有效的量子解码器，能提升量子纠错解码性能。

Abstract: Quantum error correction is a key ingredient for large scale quantum computation, protecting logical information from physical noise by encoding it into many physical qubits. Topological stabilizer codes are particularly appealing due to their geometric locality and practical relevance. In these codes, stabilizer measurements yield a syndrome that must be decoded into a recovery operation, making decoding a central bottleneck for scalable real time operation. Existing decoders are commonly classified into two categories. Classical algorithmic decoders provide strong and well established baselines, but may incur substantial computational overhead at large code distances or under stringent latency constraints. Machine learning based decoders offer fast GPU inference and flexible function approximation, yet many approaches do not explicitly exploit the lattice geometry and local structure of topological codes, which can limit performance. In this work, we propose QuantumSMoE, a quantum vision transformer based decoder that incorporates code structure through plus shaped embeddings and adaptive masking to capture local interactions and lattice connectivity, and improves scalability via a mixture of experts layer with a novel auxiliary loss. Experiments on the toric code demonstrate that QuantumSMoE outperforms state-of-the-art machine learning decoders as well as widely used classical baselines.

</details>


### [680] [Generative Adversarial Networks for Resource State Generation](https://arxiv.org/abs/2601.13708)
*Shahbaz Shaik,Sourav Chatterjee,Sayantan Pramanik,Indranil Chakrabarty*

Main category: quant-ph

TL;DR: 提出物理信息生成对抗网络框架用于量子资源态生成，对比不同架构，证明该方法有效且可用于信息处理应用。


<details>
  <summary>Details</summary>
Motivation: 将量子资源态生成转化为逆设计任务，为信息处理应用自动设计定制量子资源。

Method: 引入物理信息生成对抗网络框架，将特定效用函数嵌入训练，比较基于分解和直接生成的两种架构。

Result: 结构强制方法比仅依赖损失的方法有更高保真度和训练稳定性，框架能以超98%的保真度再现理论资源边界。

Conclusion: 对抗学习是约束驱动量子态发现的轻量级有效方法，为量子网络设计提供可能性。

Abstract: We introduce a physics-informed Generative Adversarial Network framework that recasts quantum resource-state generation as an inverse-design task. By embedding task-specific utility functions into training, the model learns to generate valid two-qubit states optimized for teleportation and entanglement broadcasting. Comparing decomposition-based and direct-generation architectures reveals that structural enforcement of Hermiticity, trace-one, and positivity yields higher fidelity and training stability than loss-only approaches. The framework reproduces theoretical resource boundaries for Werner-like and Bell-diagonal states with fidelities exceeding ~98%, establishing adversarial learning as a lightweight yet effective method for constraint-driven quantum-state discovery. This approach provides a scalable foundation for automated design of tailored quantum resources for information-processing applications, exemplified with teleportation and broadcasting of entanglement, and it opens up the possibility of using such states in efficient quantum network design.

</details>


### [681] [Deep Learning Approaches to Quantum Error Mitigation](https://arxiv.org/abs/2601.14226)
*Leonardo Placidi,Ifan Williams,Enrico Rinaldi,Daniel Mills,Cristina Cîrstoiu,Vanya Eccles,Ross Duncan*

Main category: quant-ph

TL;DR: 研究深度学习方法用于量子电路噪声输出概率分布的量子误差缓解，对比模型，在模拟和真实数据上测试，验证方法有效性并研究泛化性能


<details>
  <summary>Details</summary>
Motivation: 探究深度学习方法在量子误差缓解中的应用

Method: 比较不同架构和设计/训练模式，进行消融研究

Result: 序列到序列、基于注意力的模型最有效，方法在多方面优于其他基线误差缓解技术，相似设备泛化有效

Conclusion: 深度学习可有效应用于量子误差缓解，相似设备泛化无需完全重新训练模型

Abstract: We present a systematic investigation of deep learning methods applied to quantum error mitigation of noisy output probability distributions from measured quantum circuits. We compare different architectures, from fully connected neural networks to transformers, and we test different design/training modalities, identifying sequence-to-sequence, attention-based models as the most effective on our datasets. These models consistently produce mitigated distributions that are closer to the ideal outputs when tested on both simulated and real device data obtained from IBM superconducting quantum processing units (QPU) up to five qubits. Across several different circuit depths, our approach outperforms other baseline error mitigation techniques. We perform a series of ablation studies to examine: how different input features (circuit, device properties, noisy output statistics) affect performance; cross-dataset generalization across circuit families; and transfer learning to a different IBM QPU. We observe that generalization performance across similar devices with the same architecture works effectively, without needing to fully retrain models.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [682] [Lightweight Self-Supervised Detection of Fundamental Frequency and Accurate Probability of Voicing in Monophonic Music](https://arxiv.org/abs/2601.11768)
*Venkat Suprabath Bitra,Homayoon Beigi*

Main category: eess.AS

TL;DR: 提出轻量级自监督框架用于基频估计和发声推断，在跨语料库表现和跨乐器泛化上有竞争力。


<details>
  <summary>Details</summary>
Motivation: 可靠的基频和发声估计对神经合成至关重要，但许多基频提取器依赖大量标注语料，且在真实录音伪影下性能下降。

Method: 采用基于CQT特征的转置等变学习，引入EM风格迭代重加权方案，用Shift Cross-Entropy一致性作为可靠性信号抑制无信息的噪声/未发声帧，通过得到的权重进行伪标注训练发声分类器。

Result: 在MedleyDB上训练，在MDB - stem - synth真实数据上评估，取得了有竞争力的跨语料库性能（RPA 95.84，RCA 96.24）。

Conclusion: 该方法具有良好的跨语料库性能和跨乐器泛化能力。

Abstract: Reliable fundamental frequency (F 0) and voicing estimation is essential for neural synthesis, yet many pitch extractors depend on large labeled corpora and degrade under realistic recording artifacts. We propose a lightweight, fully self-supervised framework for joint F 0 estimation and voicing inference, designed for rapid single-instrument training from limited audio. Using transposition-equivariant learning on CQT features, we introduce an EM-style iterative reweighting scheme that uses Shift Cross-Entropy (SCE) consistency as a reliability signal to suppress uninformative noisy/unvoiced frames. The resulting weights provide confidence scores that enable pseudo-labeling for a separate lightweight voicing classifier without manual annotations. Trained on MedleyDB and evaluated on MDB-stem-synth ground truth, our method achieves competitive cross-corpus performance (RPA 95.84, RCA 96.24) and demonstrates cross-instrument generalization.

</details>


### [683] [AQUA-Bench: Beyond Finding Answers to Knowing When There Are None in Audio Question Answering](https://arxiv.org/abs/2601.12248)
*Chun-Yi Kuan,Hung-yi Lee*

Main category: eess.AS

TL;DR: 文章提出音频不可回答性评估基准AQUA - Bench，实验显示当前模型处理不可回答问题有挑战。


<details>
  <summary>Details</summary>
Motivation: 现有音频问答基准大多忽略不可回答问题，而现实中此类问题常见，需要新基准评估模型在此类问题上的可靠性。

Method: 提出AQUA - Bench基准，系统评估三种不可回答问题场景：缺失答案检测、不兼容答案集检测、不兼容音频问题检测。

Result: 实验表明模型在标准可回答任务上表现优秀，但处理不可回答问题面临显著挑战。

Conclusion: 当前音频语言理解存在盲点，AQUA - Bench可严格衡量模型可靠性，推动更健壮、可信的音频语言系统发展。

Abstract: Recent advances in audio-aware large language models have shown strong performance on audio question answering. However, existing benchmarks mainly cover answerable questions and overlook the challenge of unanswerable ones, where no reliable answer can be inferred from the audio. Such cases are common in real-world settings, where questions may be misleading, ill-posed, or incompatible with the information. To address this gap, we present AQUA-Bench, a benchmark for Audio Question Unanswerability Assessment. It systematically evaluates three scenarios: Absent Answer Detection (the correct option is missing), Incompatible Answer Set Detection (choices are categorically mismatched with the question), and Incompatible Audio Question Detection (the question is irrelevant or lacks sufficient grounding in the audio). By assessing these cases, AQUA-Bench offers a rigorous measure of model reliability and promotes the development of audio-language systems that are more robust and trustworthy. Our experiments suggest that while models excel on standard answerable tasks, they often face notable challenges with unanswerable ones, pointing to a blind spot in current audio-language understanding.

</details>


### [684] [Purification Before Fusion: Toward Mask-Free Speech Enhancement for Robust Audio-Visual Speech Recognition](https://arxiv.org/abs/2601.12436)
*Linzhi Wu,Xingyu Zhang,Hao Yuan,Yakun Zhang,Changyan Zheng,Liang Xie,Tiejun Liu,Erwei Yin*

Main category: eess.AS

TL;DR: 提出端到端带语音增强的抗噪AVSR框架，在LRS3基准测试中表现优于基于掩码的基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统AVSR在高噪声音频输入时特征融合易受干扰，基于掩码的策略会丢弃语义相关信息。

Method: 提出端到端抗噪AVSR框架，利用基于Conformer的瓶颈融合模块，借助视频隐式优化噪声音频特征。

Result: 在公共LRS3基准测试中，该方法在噪声条件下优于之前先进的基于掩码的基线方法。

Conclusion: 所提方法能减少模态冗余、增强模态间交互，保留语音语义完整性，实现鲁棒的识别性能。

Abstract: Audio-visual speech recognition (AVSR) typically improves recognition accuracy in noisy environments by integrating noise-immune visual cues with audio signals. Nevertheless, high-noise audio inputs are prone to introducing adverse interference into the feature fusion process. To mitigate this, recent AVSR methods often adopt mask-based strategies to filter audio noise during feature interaction and fusion, yet such methods risk discarding semantically relevant information alongside noise. In this work, we propose an end-to-end noise-robust AVSR framework coupled with speech enhancement, eliminating the need for explicit noise mask generation. This framework leverages a Conformer-based bottleneck fusion module to implicitly refine noisy audio features with video assistance. By reducing modality redundancy and enhancing inter-modal interactions, our method preserves speech semantic integrity to achieve robust recognition performance. Experimental evaluations on the public LRS3 benchmark suggest that our method outperforms prior advanced mask-based baselines under noisy conditions.

</details>


### [685] [Adaptive Rotary Steering with Joint Autoregression for Robust Extraction of Closely Moving Speakers in Dynamic Scenarios](https://arxiv.org/abs/2601.12345)
*Jakob Kienegger,Timo Gerkmann*

Main category: eess.AS

TL;DR: 提出自动化旋转转向以适用于移动声源动态声学条件，用联合自回归框架处理复杂声源位置情况，提升对近间距声源跟踪和增强性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度空间滤波方法在动态声学条件、移动声源场景适用性不足，且难以处理相近或交叉声源情况。

Method: 先使用基于目标初始方向的交错跟踪算法自动化旋转转向，再将处理过的录音作为额外引导，构建联合自回归框架。

Result: 在合成数据集上，所提方法显著提升跟踪和增强近间距声源的效果，持续优于可比非自回归方法；在真实场景录音中也有良好表现。

Conclusion: 所提方法能有效解决复杂声源位置情况下的声源跟踪和增强问题。

Abstract: Latest advances in deep spatial filtering for Ambisonics demonstrate strong performance in stationary multi-speaker scenarios by rotating the sound field toward a target speaker prior to multi-channel enhancement. For applicability in dynamic acoustic conditions with moving speakers, we propose to automate this rotary steering using an interleaved tracking algorithm conditioned on the target's initial direction. However, for nearby or crossing speakers, robust tracking becomes difficult and spatial cues less effective for enhancement. By incorporating the processed recording as additional guide into both algorithms, our novel joint autoregressive framework leverages temporal-spectral correlations of speech to resolve spatially challenging speaker constellations. Consequently, our proposed method significantly improves tracking and enhancement of closely spaced speakers, consistently outperforming comparable non-autoregressive methods on a synthetic dataset. Real-world recordings complement these findings in complex scenarios with multiple speaker crossings and varying speaker-to-array distances.

</details>


### [686] [Bone-conduction Guided Multimodal Speech Enhancement with Conditional Diffusion Models](https://arxiv.org/abs/2601.12354)
*Sina Khanagha,Bunlong Lay,Timo Gerkmann*

Main category: eess.AS

TL;DR: 提出一种用条件扩散模型集成骨传导传感器与气传导麦克风的多模态语音增强框架，在多种声学条件下表现优于先前技术。


<details>
  <summary>Details</summary>
Motivation: 单通道语音增强模型在极噪环境性能下降，有效集成抗噪的骨传导语音存在挑战。

Method: 引入基于条件扩散模型集成骨传导传感器与气传导麦克风的多模态语音增强框架。

Result: 在多种声学条件下，所提模型显著优于先前多模态技术和基于扩散的单模态基线。

Conclusion: 所提出的多模态语音增强框架有效提升语音增强性能。

Abstract: Single-channel speech enhancement models face significant performance degradation in extremely noisy environments. While prior work has shown that complementary bone-conducted speech can guide enhancement, effective integration of this noise-immune modality remains a challenge. This paper introduces a novel multimodal speech enhancement framework that integrates bone-conduction sensors with air-conducted microphones using a conditional diffusion model. Our proposed model significantly outperforms previously established multimodal techniques and a powerful diffusion-based single-modal baseline across a wide range of acoustic conditions.

</details>


### [687] [SLAP: Scalable Language-Audio Pretraining with Variable-Duration Audio and Multi-Objective Training](https://arxiv.org/abs/2601.12594)
*Xinhao Mei,Gael Le Lan,Haohe Liu,Zhaoheng Ni,Varun Nagaraja,Yang Liu,Yangyang Shi,Vikas Chandra*

Main category: eess.AS

TL;DR: 现有CLAP模型有数据集小、音频时长固定、难学细粒度特征的问题，提出SLAP模型，用10900万可变时长音频文本对训练，结合多目标，在相关任务达SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前CLAP模型存在训练数据集小、音频时长固定、标准对比训练目标阻碍细粒度特征学习等问题，需要改进。

Method: 引入Scalable Language - Audio Pretraining (SLAP)，用10900万可变时长音频文本对进行语言 - 音频预训练，并在单阶段训练中结合对比损失、自监督损失和字幕损失。

Result: SLAP模型在音频 - 文本检索和零样本音频分类任务上达到了新的SOTA性能。

Conclusion: SLAP模型有效，能在不同基准测试中展现出良好效果。

Abstract: Contrastive language-audio pretraining (CLAP) has achieved notable success in learning semantically rich audio representations and is widely adopted for various audio-related tasks. However, current CLAP models face several key limitations. First, they are typically trained on relatively small datasets, often comprising a few million audio samples. Second, existing CLAP models are restricted to short and fixed duration, which constrains their usage in real-world scenarios with variable-duration audio. Third, the standard contrastive training objective operates on global representations, which may hinder the learning of dense, fine-grained audio features. To address these challenges, we introduce Scalable Language-Audio Pretraining (SLAP), which scales language-audio pretraining to 109 million audio-text pairs with variable audio durations and incorporates multiple training objectives. SLAP unifies contrastive loss with additional self-supervised and captioning losses in a single-stage training, facilitating the learning of richer dense audio representations. The proposed SLAP model achieves new state-of-the-art performance on audio-text retrieval and zero-shot audio classification tasks, demonstrating its effectiveness across diverse benchmarks.

</details>


### [688] [Co-Initialization of Control Filter and Secondary Path via Meta-Learning for Active Noise Control](https://arxiv.org/abs/2601.13849)
*Ziyi Yang,Li Rao,Zhengding Luo,Dongyuan Shi,Qirui Huang,Woon-Seng Gan*

Main category: eess.AS

TL;DR: 提出MAML共初始化方法用于基于FxLMS的主动噪声控制，实验表明该方法在早期误差、达到目标时间等方面表现更好，能为前馈ANC提供简单快速启动方式。


<details>
  <summary>Details</summary>
Motivation: 主动噪声控制在声学环境变化时需快速适应，早期性能受初始化影响，需改进初始化方法。

Method: 采用Model - Agnostic Meta - Learning（MAML）共初始化方法，在小范围测量路径上预训练，使用短的两阶段内循环，应用时设置学习到的初始系数。

Result: 在在线次级路径建模FxLMS测试平台上，相比无重新初始化的基线方法，该方法早期误差更低、达到目标时间更短、辅助噪声能量减少、路径变化后恢复更快。

Conclusion: 该方法能为环境变化下的前馈ANC提供简单快速的启动方式，只需小范围路径进行预训练。

Abstract: Active noise control (ANC) must adapt quickly when the acoustic environment changes, yet early performance is largely dictated by initialization. We address this with a Model-Agnostic Meta-Learning (MAML) co-initialization that jointly sets the control filter and the secondary-path model for FxLMS-based ANC while keeping the runtime algorithm unchanged. The initializer is pre-trained on a small set of measured paths using short two-phase inner loops that mimic identification followed by residual-noise reduction, and is applied by simply setting the learned initial coefficients. In an online secondary path modeling FxLMS testbed, it yields lower early-stage error, shorter time-to-target, reduced auxiliary-noise energy, and faster recovery after path changes than a baseline without re-initialization. The method provides a simple fast start for feedforward ANC under environment changes, requiring a small set of paths to pre-train.

</details>


### [689] [Stream-Voice-Anon: Enhancing Utility of Real-Time Speaker Anonymization via Neural Audio Codec and Language Models](https://arxiv.org/abs/2601.13948)
*Nikita Kuzmin,Songting Liu,Kong Aik Lee,Eng Siong Chng*

Main category: eess.AS

TL;DR: 提出用于流式语音匿名化的Stream - Voice - Anon系统，在语音清晰度、情感保留上有提升，与先前方法有相近延迟和隐私保护效果。


<details>
  <summary>Details</summary>
Motivation: 在线语音应用保护说话人身份很重要，但流式说话人匿名化研究不足，现有基于NAC的在线LM系统用于语音转换而非匿名化，缺乏隐私保护技术。

Method: 提出Stream - Voice - Anon，适配基于因果LM的NAC架构用于流式SA，采用伪说话人表示采样、说话人嵌入混合和多样提示选择策略，比较动态和固定延迟配置。

Result: 在VoicePrivacy 2024挑战协议下，与DarkStream相比，在语音清晰度上相对WER最多降低46%，情感保留上UAR最多提升28%，延迟相近，对懒信息攻击者有相近隐私保护效果，对半信息攻击者隐私保护效果下降15%。

Conclusion: Stream - Voice - Anon在流式语音匿名化任务中表现良好，能在保持一定隐私保护和低延迟的情况下提升语音质量。

Abstract: Protecting speaker identity is crucial for online voice applications, yet streaming speaker anonymization (SA) remains underexplored. Recent research has demonstrated that neural audio codec (NAC) provides superior speaker feature disentanglement and linguistic fidelity. NAC can also be used with causal language models (LM) to enhance linguistic fidelity and prompt control for streaming tasks. However, existing NAC-based online LM systems are designed for voice conversion (VC) rather than anonymization, lacking the techniques required for privacy protection. Building on these advances, we present Stream-Voice-Anon, which adapts modern causal LM-based NAC architectures specifically for streaming SA by integrating anonymization techniques. Our anonymization approach incorporates pseudo-speaker representation sampling, a speaker embedding mixing and diverse prompt selection strategies for LM conditioning that leverage the disentanglement properties of quantized content codes to prevent speaker information leakage. Additionally, we compare dynamic and fixed delay configurations to explore latency-privacy trade-offs in real-time scenarios. Under the VoicePrivacy 2024 Challenge protocol, Stream-Voice-Anon achieves substantial improvements in intelligibility (up to 46% relative WER reduction) and emotion preservation (up to 28% UAR relative) compared to the previous state-of-the-art streaming method DarkStream while maintaining comparable latency (180ms vs 200ms) and privacy protection against lazy-informed attackers, though showing 15% relative degradation against semi-informed attackers.

</details>


### [690] [DAME: Duration-Aware Matryoshka Embedding for Duration-Robust Speaker Verification](https://arxiv.org/abs/2601.13999)
*Youngmoon Jung,Joon-Young Yang,Ju-ho Kim,Jaeyoung Roh,Chang Woo Han,Hoon-Young Cho*

Main category: eess.AS

TL;DR: 提出DAME框架解决短语音说话人验证问题，提升不同时长语音验证性能。


<details>
  <summary>Details</summary>
Motivation: 短语音说话人验证因信息有限具有挑战性，现有嵌入学习策略容量与不同时长信息不匹配。

Method: 提出模型无关的Duration - Aware Matryoshka Embedding (DAME)框架，构建与语音时长对齐的子嵌入分层结构。

Result: 在VoxCeleb1 - O/E/H和VOiCES评估集上，降低1s及其他短时长试验的等错误率，且维持全长语音性能，无额外推理成本，增益可跨架构泛化。

Conclusion: DAME能作为传统大边际微调的直接替代，持续提升不同时长的验证性能。

Abstract: Short-utterance speaker verification remains challenging due to limited speaker-discriminative cues in short speech segments. While existing methods focus on enhancing speaker encoders, the embedding learning strategy still forces a single fixed-dimensional representation reused for utterances of any length, leaving capacity misaligned with the information available at different durations. We propose Duration-Aware Matryoshka Embedding (DAME), a model-agnostic framework that builds a nested hierarchy of sub-embeddings aligned to utterance durations: lower-dimensional representations capture compact speaker traits from short utterances, while higher dimensions encode richer details from longer speech. DAME supports both training from scratch and fine-tuning, and serves as a direct alternative to conventional large-margin fine-tuning, consistently improving performance across durations. On the VoxCeleb1-O/E/H and VOiCES evaluation sets, DAME consistently reduces the equal error rate on 1-s and other short-duration trials, while maintaining full-length performance with no additional inference cost. These gains generalize across various speaker encoder architectures under both general training and fine-tuning setups.

</details>


### [691] [MATE: Matryoshka Audio-Text Embeddings for Open-Vocabulary Keyword Spotting](https://arxiv.org/abs/2601.14012)
*Youngmoon Jung,Myunghun Jung,Joon-Young Yang,Yong-Hyeok Lee,Jaeyoung Roh,Hoon-Young Cho*

Main category: eess.AS

TL;DR: 提出Matryoshka Audio - Text Embeddings (MATE)框架用于开放词汇关键词检测，在WSJ和LibriPhrase上获SOTA结果且无推理开销。


<details>
  <summary>Details</summary>
Motivation: 现有基于嵌入学习的话语级匹配方法在单一固定维度学习嵌入，本文提出新方法改进。

Method: 提出双编码器框架MATE，通过嵌套子嵌入在单向量中编码多粒度嵌入，引入PCA引导的前缀对齐。用标准深度度量学习目标训练，且与损失无关。

Result: 在WSJ和LibriPhrase上取得了最先进的结果，且没有任何推理开销。

Conclusion: MATE框架在开放词汇关键词检测任务中表现出色，是首次将套娃式嵌入应用于该任务。

Abstract: Open-vocabulary keyword spotting (KWS) with text-based enrollment has emerged as a flexible alternative to fixed-phrase triggers. Prior utterance-level matching methods, from an embedding-learning standpoint, learn embeddings at a single fixed dimensionality. We depart from this design and propose Matryoshka Audio-Text Embeddings (MATE), a dual-encoder framework that encodes multiple embedding granularities within a single vector via nested sub-embeddings ("prefixes"). Specifically, we introduce a PCA-guided prefix alignment: PCA-compressed versions of the full text embedding for each prefix size serve as teacher targets to align both audio and text prefixes. This alignment concentrates salient keyword cues in lower-dimensional prefixes, while higher dimensions add detail. MATE is trained with standard deep metric learning objectives for audio-text KWS, and is loss-agnostic. To our knowledge, this is the first application of matryoshka-style embeddings to KWS, achieving state-of-the-art results on WSJ and LibriPhrase without any inference overhead.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [692] [Spectral Dynamics and Regularization for High-Dimensional Copulas](https://arxiv.org/abs/2601.13281)
*Koos B. Gubbels,Andre Lucas*

Main category: econ.EM

TL;DR: 提出一种高维时变、非对称、尾部相关的新Copula模型，结合谱动态与正则化，在模拟和实证数据中表现良好，优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 构建能处理高维、时变、非对称和尾部相关的Copula模型，以捕捉金融市场动态。

Method: 用得分驱动方法对依赖矩阵特征值动态建模，用非线性收缩解决无条件特征值谱偏差，动态参数化Copula依赖矩阵。

Result: 新模型简洁高效、可扩展到高维，在模拟和实证数据中表现好；实证中能捕捉地理和行业相关联动，优于基于聚类的因子Copula替代方案；市场压力时期，谱动态显示国际股市依赖增强。

Conclusion: 谱动态和正则化都有助于新模型表现，市场压力时股市依赖增强会降低分散投资潜力和增加系统性风险。

Abstract: We introduce a novel model for time-varying, asymmetric, tail-dependent copulas in high dimensions that incorporates both spectral dynamics and regularization. The dynamics of the dependence matrix' eigenvalues are modeled in a score-driven way, while biases in the unconditional eigenvalue spectrum are resolved by non-linear shrinkage. The dynamic parameterization of the copula dependence matrix ensures that it satisfies the appropriate restrictions at all times and for any dimension. The model is parsimonious, computationally efficient, easily scalable to high dimensions, and performs well for both simulated and empirical data. In an empirical application to financial market dynamics using 100 stocks from 10 different countries and 10 different industry sectors, we find that our copula model captures both geographic and industry related co-movements and outperforms recent computationally more intensive clustering-based factor copula alternatives. Both the spectral dynamics and the regularization contribute to the new model's performance. During periods of market stress, we find that the spectral dynamics reveal strong increases in international stock market dependence, which causes reductions in diversification potential and increases in systemic risk.

</details>


### [693] [How Well Do LLMs Predict Human Behavior? A Measure of their Pretrained Knowledge](https://arxiv.org/abs/2601.12343)
*Wayne Gao,Sukjin Han,Annie Liang*

Main category: econ.EM

TL;DR: 提出评估预训练大语言模型预测人类行为时的等效样本量指标，给出估计方法和统计推断程序，并应用于实际研究，发现其对不同经济变量的预测信息差异大。


<details>
  <summary>Details</summary>
Motivation: 评估预训练大语言模型在预测人类行为时所具备的知识量。

Method: 通过比较固定大语言模型在给定领域的预测误差与在特定领域数据上训练的灵活机器学习模型的预测误差来估计等效样本量，开发新的渐近理论进行交叉验证预测误差的统计推断。

Result: 大语言模型对一些经济变量编码了相当多的预测信息，对另一些则较少。

Conclusion: 大语言模型作为特定领域数据替代品的价值在不同场景下差异显著。

Abstract: Large language models (LLMs) are increasingly used to predict human behavior. We propose a measure for evaluating how much knowledge a pretrained LLM brings to such a prediction: its equivalent sample size, defined as the amount of task-specific data needed to match the predictive accuracy of the LLM. We estimate this measure by comparing the prediction error of a fixed LLM in a given domain to that of flexible machine learning models trained on increasing samples of domain-specific data. We further provide a statistical inference procedure by developing a new asymptotic theory for cross-validated prediction error. Finally, we apply this method to the Panel Study of Income Dynamics. We find that LLMs encode considerable predictive information for some economic variables but much less for others, suggesting that their value as substitutes for domain-specific data differs markedly across settings.

</details>


### [694] [Nonlinear Dynamic Factor Analysis With a Transformer Network](https://arxiv.org/abs/2601.12039)
*Oliver Snellman*

Main category: econ.EM

TL;DR: 本文开发用于估计动态因子的Transformer架构，用正则化改进小数据集性能，结果用注意力矩阵解释，实验表明比线性模型准确，并用于构建美国经济活动一致指数。


<details>
  <summary>Details</summary>
Motivation: 在灵活识别假设下从多元时间序列数据中估计动态因子，并提升小数据集上的表现。

Method: 开发Transformer架构，在训练目标中通过正则化项使用传统因子模型作为先验信息，用注意力矩阵解释结果。

Result: 蒙特卡罗实验显示，当数据偏离线性高斯假设时，Transformer比线性因子模型更准确。

Conclusion: 所开发的Transformer架构在估计动态因子方面有优势，可用于构建经济活动指数等实际应用。

Abstract: The paper develops a Transformer architecture for estimating dynamic factors from multivariate time series data under flexible identification assumptions. Performance on small datasets is improved substantially by using a conventional factor model as prior information via a regularization term in the training objective. The results are interpreted with Attention matrices that quantify the relative importance of variables and their lags for the factor estimate. Time variation in Attention patterns can help detect regime switches and evaluate narratives. Monte Carlo experiments suggest that the Transformer is more accurate than the linear factor model, when the data deviate from linear-Gaussian assumptions. An empirical application uses the Transformer to construct a coincident index of U.S. real economic activity.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [695] [Opportunities in AI/ML for the Rubin LSST Dark Energy Science Collaboration](https://arxiv.org/abs/2601.14235)
*LSST Dark Energy Science Collaboration,Eric Aubourg,Camille Avestruz,Matthew R. Becker,Biswajit Biswas,Rahul Biswas,Boris Bolliet,Adam S. Bolton,Clecio R. Bom,Raphaël Bonnet-Guerrini,Alexandre Boucaud,Jean-Eric Campagne,Chihway Chang,Aleksandra Ćiprijanović,Johann Cohen-Tanugi,Michael W. Coughlin,John Franklin Crenshaw,Juan C. Cuevas-Tello,Juan de Vicente,Seth W. Digel,Steven Dillmann,Mariano Javier de León Dominguez Romero,Alex Drlica-Wagner,Sydney Erickson,Alexander T. Gagliano,Christos Georgiou,Aritra Ghosh,Matthew Grayling,Kirill A. Grishin,Alan Heavens,Lindsay R. House,Mustapha Ishak,Wassim Kabalan,Arun Kannawadi,François Lanusse,C. Danielle Leonard,Pierre-François Léget,Michelle Lochner,Yao-Yuan Mao,Peter Melchior,Grant Merz,Martin Millon,Anais Möller,Gautham Narayan,Yuuki Omori,Hiranya Peiris,Laurence Perreault-Levasseur,Andrés A. Plazas Malagón,Nesar Ramachandra,Benjamin Remy,Cécile Roucelle,Jaime Ruiz-Zapatero,Stefan Schuldt,Ignacio Sevilla-Noarbe,Ved G. Shah,Tjitske Starkenburg,Stephen Thorp,Laura Toribio San Cipriano,Tilman Tröster,Roberto Trotta,Padma Venkatraman,Amanda Wasserman,Tim White,Justine Zeghal,Tianqing Zhang,Yuanyuan Zhang*

Main category: astro-ph.IM

TL;DR: 鲁宾天文台LSST将产生海量异构天文数据，DESC旨在从中获取暗能量和暗物质约束，AI/ML已用于DESC科学流程，但需解决不确定性量化等问题，本文调查其现状，确定关键研究优先级，探索新兴技术潜力并讨论部署要求和风险。


<details>
  <summary>Details</summary>
Motivation: 鲁宾天文台LSST产生的海量异构天文数据挑战传统分析流程，DESC需从这些数据中获取暗能量和暗物质的可靠约束，而AI/ML的应用有诸多问题需解决。

Method: 对DESC的主要宇宙学探测和跨领域分析中AI/ML的现状进行调查。

Result: 发现不同科学案例中存在相同的核心方法和基本挑战，确定了关键的方法学研究优先级，探索了新兴技术重塑DESC工作流程的潜力。

Conclusion: 讨论了新方法成功部署所需的软件、计算、数据基础设施和人力资本要求，以及相关风险和与外部合作的机会。

Abstract: The Vera C. Rubin Observatory's Legacy Survey of Space and Time (LSST) will produce unprecedented volumes of heterogeneous astronomical data (images, catalogs, and alerts) that challenge traditional analysis pipelines. The LSST Dark Energy Science Collaboration (DESC) aims to derive robust constraints on dark energy and dark matter from these data, requiring methods that are statistically powerful, scalable, and operationally reliable. Artificial intelligence and machine learning (AI/ML) are already embedded across DESC science workflows, from photometric redshifts and transient classification to weak lensing inference and cosmological simulations. Yet their utility for precision cosmology hinges on trustworthy uncertainty quantification, robustness to covariate shift and model misspecification, and reproducible integration within scientific pipelines. This white paper surveys the current landscape of AI/ML across DESC's primary cosmological probes and cross-cutting analyses, revealing that the same core methodologies and fundamental challenges recur across disparate science cases. Since progress on these cross-cutting challenges would benefit multiple probes simultaneously, we identify key methodological research priorities, including Bayesian inference at scale, physics-informed methods, validation frameworks, and active learning for discovery. With an eye on emerging techniques, we also explore the potential of the latest foundation model methodologies and LLM-driven agentic AI systems to reshape DESC workflows, provided their deployment is coupled with rigorous evaluation and governance. Finally, we discuss critical software, computing, data infrastructure, and human capital requirements for the successful deployment of these new methodologies, and consider associated risks and opportunities for broader coordination with external actors.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [696] [Pigment Network Detection and Classification in Dermoscopic Images Using Directional Imaging Algorithms and Convolutional Neural Networks](https://arxiv.org/abs/2601.11674)
*M. A. Rasel,Sameem Abdul Kareem,Unaizah Obaidellah*

Main category: eess.IV

TL;DR: 研究用定向成像算法自动检测色素网络（PN），并用机器学习分类器对PN分类，CNN模型表现出色，建议扩数据集等增强黑色素瘤诊断。


<details>
  <summary>Details</summary>
Motivation: 黑色素瘤早期诊断依赖皮肤镜图像分析，区分典型和非典型PN有挑战，需自动化检测和分类方法。

Method: 用含PCA等的定向成像算法检测PN，创建新PN数据集，用CNN和BoF分类PN。

Result: 定向成像算法成功率最高达100%，CNN准确率90%、灵敏度90%、特异度89%，优于现有方法。

Conclusion: 提出的CNN模型有有效分类PN的潜力，未来应扩数据集和引入更多皮肤学特征。

Abstract: Early diagnosis of melanoma, which can save thousands of lives, relies heavily on the analysis of dermoscopic images. One crucial diagnostic criterion is the identification of unusual pigment network (PN). However, distinguishing between regular (typical) and irregular (atypical) PN is challenging. This study aims to automate the PN detection process using a directional imaging algorithm and classify PN types using machine learning classifiers. The directional imaging algorithm incorporates Principal Component Analysis (PCA), contrast enhancement, filtering, and noise reduction. Applied to the PH2 dataset, this algorithm achieved a 96% success rate, which increased to 100% after pixel intensity adjustments. We created a new dataset containing only PN images from these results. We then employed two classifiers, Convolutional Neural Network (CNN) and Bag of Features (BoF), to categorize PN into atypical and typical classes. Given the limited dataset of 200 images, a simple and effective CNN was designed, featuring two convolutional layers and two batch normalization layers. The proposed CNN achieved 90% accuracy, 90% sensitivity, and 89% specificity. When compared to state-of-the-art methods, our CNN demonstrated superior performance. Our study highlights the potential of the proposed CNN model for effective PN classification, suggesting future research should focus on expanding datasets and incorporating additional dermatological features to further enhance melanoma diagnosis.

</details>


### [697] [Mobile-friendly Image de-noising: Hardware Conscious Optimization for Edge Application](https://arxiv.org/abs/2601.11684)
*Srinivas Miriyala,Sowmya Vajrala,Hitesh Kumar,Sravanth Kodavanti,Vikram Rajendiran*

Main category: eess.IV

TL;DR: 本文提出一种新型移动友好的图像去噪网络，在参数、延迟、内存占用等方面有优势，且泛化能力好。


<details>
  <summary>Details</summary>
Motivation: 传统图像信号处理在图像增强去噪任务中不如深度学习方法，且深度学习方法需便于在边缘设备部署。

Method: 使用熵正则化可微神经架构搜索（NAS）在硬件感知搜索空间为U - Net架构设计网络。

Result: 在三星Galaxy S24 Ultra上部署，参数减少12%，设备延迟提升约2倍，内存占用提升1.5倍，PSNR下降0.7%；与SOTA Swin - Transformer相比，GMACs减少约18倍；在多个基准测试中测试成功。

Conclusion: 所提出的网络具有移动友好性和良好的泛化能力。

Abstract: Image enhancement is a critical task in computer vision and photography that is often entangled with noise. This renders the traditional Image Signal Processing (ISP) ineffective compared to the advances in deep learning. However, the success of such methods is increasingly associated with the ease of their deployment on edge devices, such as smartphones. This work presents a novel mobile-friendly network for image de-noising obtained with Entropy-Regularized differentiable Neural Architecture Search (NAS) on a hardware-aware search space for a U-Net architecture, which is first-of-its-kind. The designed model has 12% less parameters, with ~2-fold improvement in ondevice latency and 1.5-fold improvement in the memory footprint for a 0.7% drop in PSNR, when deployed and profiled on Samsung Galaxy S24 Ultra. Compared to the SOTA Swin-Transformer for Image Restoration, the proposed network had competitive accuracy with ~18-fold reduction in GMACs. Further, the network was tested successfully for Gaussian de-noising with 3 intensities on 4 benchmarks and real-world de-noising on 1 benchmark demonstrating its generalization ability.

</details>


### [698] [Towards Efficient Image Deblurring for Edge Deployment](https://arxiv.org/abs/2601.11685)
*Srinivas Miriyala,Sowmya Vajrala,Sravanth Kodavanti*

Main category: eess.IV

TL;DR: 提出硬件感知适应框架优化图像去模糊模型，减少计算量、提升推理速度并保证准确性。


<details>
  <summary>Details</summary>
Motivation: 现有深度网络在图像去模糊中效率衡量指标与嵌入式硬件延迟不相关，需优化模型以平衡恢复能力和实时性。

Method: 提出硬件感知适应框架，通过敏感度引导的块替换、代理蒸馏和基于设备分析的无训练多目标搜索重构现有模型。

Result: 优化后的36块NAFNet变体GMACs最多降低55%，设备上推理延迟提升1.25倍，在多个去模糊任务中验证了方法的通用性和准确性。

Conclusion: 反馈驱动的适应是缩小算法设计和可部署去模糊模型差距的有效策略。

Abstract: Image deblurring is a critical stage in mobile image signal processing pipelines, where the ability to restore fine structures and textures must be balanced with real-time constraints on edge devices. While recent deep networks such as transformers and activation-free architectures achieve state-of-the-art (SOTA) accuracy, their efficiency is typically measured in FLOPs or parameters, which do not correlate with latency on embedded hardware. We propose a hardware-aware adaptation framework that restructures existing models through sensitivity-guided block substitution, surrogate distillation, and training-free multi-objective search driven by device profiling. Applied to the 36-block NAFNet baseline, the optimized variants achieve up to 55% reduction in GMACs compared to the recent transformer-based SOTA while maintaining competitive accuracy. Most importantly, on-device deployment yields a 1.25X latency improvement over the baseline. Experiments on motion deblurring (GoPro), defocus deblurring (DPDD), and auxiliary benchmarks (RealBlur-J/R, HIDE) demonstrate the generality of the approach, while comparisons with prior efficient baselines confirm its accuracy-efficiency trade-off. These results establish feedback-driven adaptation as a principled strategy for bridging the gap between algorithmic design and deployment-ready deblurring models.

</details>


### [699] [Explainable histomorphology-based survival prediction of glioblastoma, IDH-wildtype](https://arxiv.org/abs/2601.11691)
*Jan-Philipp Redlich,Friedrich Feuerhake,Stefan Nikolin,Nadine Sarah Schaadt,Sarah Teuber-Hanselmann,Joachim Weis,Sabine Luttmann,Andrea Eberle,Christoph Buck,Timm Intemann,Pascal Birnstill,Klaus Kraywinkel,Jonas Ort,Peter Boor,André Homeyer*

Main category: eess.IV

TL;DR: 提出可解释AI方法分析GBM - IDHwt组织学特征与生存关系，有一定判别能力并找到相关可视模式。


<details>
  <summary>Details</summary>
Motivation: 现有GBM - IDHwt集成诊断依赖组织形态学，期望用AI方法从组织学全切片图像中提取更多与生存相关的预后信息。

Method: 结合可解释的多实例学习（MIL）架构和稀疏自动编码器（SAE），用新数据集训练评估MIL，用公开数据集训练SAE。

Result: 方法有一定能力仅基于组织形态学区分生存期不同患者（AUC: 0.67; 95% CI: 0.63 - 0.72），Cox回归确认预测组生存时间有显著差异（风险比: 1.47; 95% CI: 1.26 - 1.72）。

Conclusion: 方法能识别多个与生存相关的可解释视觉模式，坏死和出血与短生存期有关，高细胞肿瘤区域与长生存期有关。

Abstract: Glioblastoma, IDH-wildtype (GBM-IDHwt) is the most common malignant brain tumor. Histomorphology is a crucial component of the integrated diagnosis of GBM-IDHwt. Artificial intelligence (AI) methods have shown promise to extract additional prognostic information from histological whole-slide images (WSI) of hematoxylin and eosin-stained glioblastoma tissue. Here, we present an explainable AI-based method to support systematic interpretation of histomorphological features associated with survival. It combines an explainable multiple instance learning (MIL) architecture with a sparse autoencoder (SAE) to relate human-interpretable visual patterns of tissue to survival. The MIL architecture directly identifies prognosis-relevant image tiles and the SAE maps these tiles post-hoc to visual patterns. The MIL method was trained and evaluated using a new real-world dataset that comprised 720 GBM-IDHwt cases from three hospitals and four cancer registries in Germany. The SAE was trained using 1878 WSIs of glioblastoma from five independent public data collections. Despite the many factors influencing survival time, our method showed some ability to discriminate between patients living less than 180 days or more than 360 days solely based on histomorphology (AUC: 0.67; 95% CI: 0.63-0.72). Cox proportional hazards regression confirmed a significant difference in survival time between the predicted groups after adjustment for established prognostic factors (hazard ratio: 1.47; 95% CI: 1.26-1.72). Our method identified multiple interpretable visual patterns associated with survival. Three neuropathologists separately found that 21 of the 24 most strongly associated patterns could be clearly attributed to seven histomorphological categories. Necrosis and hemorrhage appeared to be associated with shorter survival while highly cellular tumor areas were associated with longer survival.

</details>


### [700] [Anisotropic Tensor Deconvolution of Hyperspectral Images](https://arxiv.org/abs/2601.11694)
*Xinjue Wang,Xiuheng Wang,Esa Ollila,Sergiy A. Vorobyov*

Main category: eess.IV

TL;DR: 提出基于低秩CPD的高光谱图像解卷积参数简约框架，用高效算法求解，实验证明其高效性。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像解卷积是高维难处理的病态逆问题。

Method: 提出基于低秩CPD的框架，将问题转化为估计CPD因子，对空间因子应用结构感知各向异性TV正则化，用PALM框架开发高效算法求解非凸优化问题。

Result: 实验显示参数减少超两个数量级，在模型紧凑性和重建精度间有良好权衡。

Conclusion: 所提模型在高光谱图像解卷积中高效。

Abstract: Hyperspectral image (HSI) deconvolution is a challenging ill-posed inverse problem, made difficult by the data's high dimensionality.We propose a parameter-parsimonious framework based on a low-rank Canonical Polyadic Decomposition (CPD) of the entire latent HSI $\mathbf{\mathcal{X}} \in \mathbb{R}^{P\times Q \times N}$.This approach recasts the problem from recovering a large-scale image with $PQN$ variables to estimating the CPD factors with $(P+Q+N)R$ variables.This model also enables a structure-aware, anisotropic Total Variation (TV) regularization applied only to the spatial factors, preserving the smooth spectral signatures.An efficient algorithm based on the Proximal Alternating Linearized Minimization (PALM) framework is developed to solve the resulting non-convex optimization problem.Experiments confirm the model's efficiency, showing a numerous parameter reduction of over two orders of magnitude and a compelling trade-off between model compactness and reconstruction accuracy.

</details>


### [701] [Pixelwise Uncertainty Quantification of Accelerated MRI Reconstruction](https://arxiv.org/abs/2601.13236)
*Ilias I. Giannakopoulos,Lokesh B Gautham Muthukumar,Yvonne W. Lui,Riccardo Lattanzi*

Main category: eess.IV

TL;DR: 提出用于并行MRI重建中逐像素不确定性量化的通用框架，能在无参考图像时自动识别不可靠区域，实验表明该方法能有效评估重建质量，迈向自适应MRI采集协议。


<details>
  <summary>Details</summary>
Motivation: 并行成像技术加速MRI扫描时会降低图像质量，且缺乏自动评估欠采样重建诊断质量的机制，因此需解决无参考图像时评估问题。

Method: 将共形分位数回归与图像重建方法集成以估计逐像素不确定性区间，用端到端变分网络进行图像重建，在fastMRI数据集上训练和评估。

Result: 定量实验显示预测不确定图与真实重建误差高度一致，加速水平≥4时皮尔逊相关系数超90%；定性示例表明分位数回归的不确定图能捕捉重建误差大小和空间分布。

Conclusion: 所提框架可在无全采样参考图像时评估重建质量，向自适应MRI采集协议迈进。

Abstract: Parallel imaging techniques reduce magnetic resonance imaging (MRI) scan time but image quality degrades as the acceleration factor increases. In clinical practice, conservative acceleration factors are chosen because no mechanism exists to automatically assess the diagnostic quality of undersampled reconstructions. This work introduces a general framework for pixel-wise uncertainty quantification in parallel MRI reconstructions, enabling automatic identification of unreliable regions without access to any ground-truth reference image. Our method integrates conformal quantile regression with image reconstruction methods to estimate statistically rigorous pixel-wise uncertainty intervals. We trained and evaluated our model on Cartesian undersampled brain and knee data obtained from the fastMRI dataset using acceleration factors ranging from 2 to 10. An end-to-end Variational Network was used for image reconstruction. Quantitative experiments demonstrate strong agreement between predicted uncertainty maps and true reconstruction error. Using our method, the corresponding Pearson correlation coefficient was higher than 90% at acceleration levels at and above four-fold; whereas it dropped to less than 70% when the uncertainty was computed using a simpler a heuristic notion (magnitude of the residual). Qualitative examples further show the uncertainty maps based on quantile regression capture the magnitude and spatial distribution of reconstruction errors across acceleration factors, with regions of elevated uncertainty aligning with pathologies and artifacts. The proposed framework enables evaluation of reconstruction quality without access to fully-sampled ground-truth reference images. It represents a step toward adaptive MRI acquisition protocols that may be able to dynamically balance scan time and diagnostic reliability.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [702] [Karhunen-Loève Expansion-Based Residual Anomaly Map for Resource-Efficient Glioma MRI Segmentation](https://arxiv.org/abs/2601.11833)
*Anthony Hur*

Main category: q-bio.QM

TL;DR: 本文提出基于KLE的残差异常图用于脑肿瘤分割，可在降低计算成本和数据需求的同时保持先进性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习用于脑肿瘤分割需大量数据和高计算资源，多数地区无法满足。

Method: 对降采样、z分数归一化的MRI体积实施KLE作为特征提取步骤，生成的近似重建用于构建残差异常图，添加到紧凑3D U-Net。

Result: 模型在消费者工作站用少量训练案例，Dice分数和HD95距离结果优于BraTS 2023获奖方法。

Conclusion: 基于KLE的残差异常图可大幅降低计算成本和数据需求，保持先进性能。

Abstract: Accurate segmentation of brain tumors is essential for clinical diagnosis and treatment planning. Deep learning is currently the state-of-the-art for brain tumor segmentation, yet it requires either large datasets or extensive computational resources that are inaccessible in most areas. This makes the problem increasingly difficult: state-of-the-art models use thousands of training cases and vast computational power, where performance drops sharply when either is limited. The top performer in the Brats GLI 2023 competition relied on supercomputers trained on over 92,000 augmented MRI scans using an AMD EPYC 7402 CPU, six NVIDIA RTX 6000 GPUs (48GB VRAM each), and 1024GB of RAM over multiple weeks. To address this, the Karhunen--Loève Expansion (KLE) was implemented as a feature extraction step on downsampled, z-score normalized MRI volumes. Each 240$\times$240$\times$155 multi-modal scan is reduced to four $48^3$ channels and compressed into 32 KL coefficients. The resulting approximate reconstruction enables a residual-based anomaly map, which is upsampled and added as a fifth channel to a compact 3D U-Net. All experiments were run on a consumer workstation (AMD Ryzen 5 7600X CPU, RTX 4060Ti (8GB VRAM), and 64GB RAM while using far fewer training cases. This model achieves post-processed Dice scores of 0.929 (WT), 0.856 (TC), and 0.821 (ET), with HD95 distances of 2.93, 6.78, and 10.35 voxels. These results are significantly better than the winning BraTS 2023 methodology for HD95 distances and WT dice scores. This demonstrates that a KLE-based residual anomaly map can dramatically reduce computational cost and data requirements while retaining state-of-the-art performance.

</details>


### [703] [SCG With Your Phone: Diagnosis of Rhythmic Spectrum Disorders in Field Conditions](https://arxiv.org/abs/2601.13926)
*Peter Golenderov,Yaroslav Matushenko,Anastasia Tushina,Michal Barodkin*

Main category: q-bio.QM

TL;DR: 提出基于消费者智能手机加速度计记录的深度学习框架用于SCG分割和心律分析，实验显示高准确性和鲁棒性，为心血管评估等铺平道路。


<details>
  <summary>Details</summary>
Motivation: 在现实场景中，智能手机收集的SCG信号存在噪声、运动伪影和设备异质性导致的变化，需检测频率和节律障碍。

Method: 开发集成多尺度卷积、残差连接和注意力门的增强U - Net v3架构分割噪声SCG信号，用后处理管道转换概率掩码得到AO时间戳，用自适应3D到1D投影方法确保对任意手机方向的鲁棒性。

Result: 该方法在不同设备类型和无监督数据收集条件下实现了一致的高精度和鲁棒性。

Conclusion: 该方法能实现使用日常移动设备进行实用、低成本和自动化的心律监测，为可扩展的心血管评估和未来多模态诊断系统奠定基础。

Abstract: Aortic valve opening (AO) events are crucial for detecting frequency and rhythm disorders, especially in real-world settings where seismocardiography (SCG) signals collected via consumer smartphones are subject to noise, motion artifacts, and variability caused by device heterogeneity. In this work, we present a robust deep-learning framework for SCG segmentation and rhythm analysis using accelerometer recordings obtained with consumer smartphones. We develop an enhanced U-Net v3 architecture that integrates multi-scale convolutions, residual connections, and attention gates, enabling reliable segmentation of noisy SCG signals. A dedicated post-processing pipeline converts probability masks into precise AO timestamps, whereas a novel adaptive 3D-to-1D projection method ensures robustness to arbitrary smartphone orientation. Experimental results demonstrate that the proposed method achieves consistently high accuracy and robustness across various device types and unsupervised data-collection conditions. Our approach enables practical, low-cost, and automated cardiac-rhythm monitoring using everyday mobile devices, paving the way for scalable, field-deployable cardiovascular assessment and future multimodal diagnostic systems.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [704] [SciHorizon-GENE: Benchmarking LLM for Life Sciences Inference from Gene Knowledge to Functional Understanding](https://arxiv.org/abs/2601.12805)
*Xiaohan Huang,Meng Xiao,Chuan Qin,Qingqing Long,Jinmiao Chen,Yuanchun Zhou,Hengshu Zhu*

Main category: q-bio.GN

TL;DR: 介绍SciHorizon - GENE基准测试评估大语言模型基因层面推理能力，揭示模型能力异质性和现存挑战，为模型选择和开发提供依据。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型从基因知识到功能理解的可靠推理能力未充分探索，需构建基准测试评估其在知识增强细胞图谱解释中的能力。

Method: 构建SciHorizon - GENE基准测试，整合超19万个基因知识，包含超54万个问题，从四个生物学关键视角评估大语言模型。

Result: 评估多种大语言模型，发现基因层面推理能力有显著异质性，生成忠实、完整和基于文献的功能解释仍有挑战。

Conclusion: 该基准测试为分析大语言模型基因尺度行为打下基础，对模型选择和开发有直接意义。

Abstract: Large language models (LLMs) have shown growing promise in biomedical research, particularly for knowledge-driven interpretation tasks. However, their ability to reliably reason from gene-level knowledge to functional understanding, However, their ability to reliably reason from gene-level knowledge to functional understanding, a core requirement for knowledge-enhanced cell atlas interpretation, remains largely underexplored. To address this gap, we introduce SciHorizon-GENE, a large-scale gene-centric benchmark constructed from authoritative biological databases. The benchmark integrates curated knowledge for over 190K human genes and comprises more than 540K questions covering diverse gene-to-function reasoning scenarios relevant to cell type annotation, functional interpretation, and mechanism-oriented analysis. Motivated by behavioral patterns observed in preliminary examinations, SciHorizon-GENE evaluates LLMs along four biologically critical perspectives: research attention sensitivity, hallucination tendency, answer completeness, and literature influence, explicitly targeting failure modes that limit the safe adoption of LLMs in biological interpretation pipelines. We systematically evaluate a wide range of state-of-the-art general-purpose and biomedical LLMs, revealing substantial heterogeneity in gene-level reasoning capabilities and persistent challenges in generating faithful, complete, and literature-grounded functional interpretations. Our benchmark establishes a systematic foundation for analyzing LLM behavior at the gene scale and offers insights for model selection and development, with direct relevance to knowledge-enhanced biological interpretation.

</details>


<div id='q-fin.GN'></div>

# q-fin.GN [[Back]](#toc)

### [705] [Autonomous Market Intelligence: Agentic AI Nowcasting Predicts Stock Returns](https://arxiv.org/abs/2601.11958)
*Zefeng Chen,Darcy Pu*

Main category: q-fin.GN

TL;DR: 本文探讨全自主AI能否即时预测股票回报，利用大语言模型评估罗素1000指数成分股，发现AI有选股能力但集中在识别顶级赢家。


<details>
  <summary>Details</summary>
Motivation: 研究全自主AI能否进行股票回报的即时预测。

Method: 从2025年4月起，使用大语言模型每日评估罗素1000指数中每只股票的吸引力，构建无样本外偏差和前瞻性偏差的预测框架，且模型自主搜索网络、筛选来源和合成信息。

Result: AI有选股能力，但仅能识别顶级赢家，做多排名前20的股票有可观收益，交易成本低，但预测能力集中，超出顶级范围收益迅速稀释，排名靠后的股票回报与市场无显著差异。

Conclusion: 这种预测能力的不对称性可能反映了在线信息结构，正面消息信号清晰，负面消息受企业混淆和社交媒体噪音干扰。

Abstract: Can fully agentic AI nowcast stock returns? We deploy a state-of-the-art Large Language Model to evaluate the attractiveness of each Russell 1000 stock daily, starting from April 2025 when AI web interfaces enabled real-time search. Our data contribution is unique along three dimensions. First, the nowcasting framework is completely out-of-sample and free of look-ahead bias by construction: predictions are collected at the current edge of time, ensuring the AI has no knowledge of future outcomes. Second, this temporal design is irreproducible -- once the information environment passes, it can never be recreated. Third, our framework is 100% agentic: we do not feed the model news, disclosures, or curated text; it autonomously searches the web, filters sources, and synthesises information into quantitative predictions. We find that AI possesses genuine stock selection ability, but only for identifying top winners. Longing the 20 highest-ranked stocks generates a daily Fama-French five-factor plus momentum alpha of 18.4 basis points and an annualised Sharpe ratio of 2.43. Critically, these returns derive from an implementable strategy trading highly liquid Russell 1000 constituents, with transaction costs representing less than 10\% of gross alpha. However, this predictability is highly concentrated: expanding beyond the top tier rapidly dilutes alpha, and bottom-ranked stocks exhibit returns statistically indistinguishable from the market. We hypothesise that this asymmetry reflects online information structure: genuinely positive news generates coherent signals, while negative news is contaminated by strategic corporate obfuscation and social media noise.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [706] ['1'-bit Count-based Sorting Unit to Reduce Link Power in DNN Accelerators](https://arxiv.org/abs/2601.14087)
*Ruichi Han,Yizhi Chen,Tong Lei,Jordi Altayo Gonzalez,Ahmed Hemani*

Main category: cs.AR

TL;DR: 提出用于CNN的无比较排序单元硬件实现，减少硬件面积并保留数据重排序的链路功耗优势


<details>
  <summary>Details</summary>
Motivation: 互连功耗是DNN加速器的瓶颈，基于'1'位计数的排序可降低功耗，但实用硬件排序实现研究不足

Method: 利用近似计算将总体计数分组到粗粒度桶中，提出无比较排序单元硬件实现

Result: 近似排序单元实现了高达35.4%的面积减少，同时保持了19.50%的BT减少

Conclusion: 该设计在减少硬件面积的同时保留了数据重排序的链路功耗好处

Abstract: Interconnect power consumption remains a bottleneck in Deep Neural Network (DNN) accelerators. While ordering data based on '1'-bit counts can mitigate this via reduced switching activity, practical hardware sorting implementations remain underexplored. This work proposes the hardware implementation of a comparison-free sorting unit optimized for Convolutional Neural Networks (CNN). By leveraging approximate computing to group population counts into coarse-grained buckets, our design achieves hardware area reductions while preserving the link power benefits of data reordering. Our approximate sorting unit achieves up to 35.4% area reduction while maintaining 19.50\% BT reduction compared to 20.42% of precise implementation.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [707] [End-to-End Reverse Screening Identifies Protein Targets of Small Molecules Using HelixFold3](https://arxiv.org/abs/2601.13693)
*Shengjie Xu,Xianbin Ye,Mengran Zhu,Xiaonan Zhang,Shanzhuo Zhang,Xiaomin Fang*

Main category: q-bio.BM

TL;DR: 提出基于HelixFold3的端到端反向筛选策略，验证其在小分子与蛋白靶点筛选中的效果优于传统方法，为药物研究提供平台。


<details>
  <summary>Details</summary>
Motivation: 反向筛选对理解药物作用等方面至关重要，但传统方法存在流程易传播误差、难以准确捕捉小分子与多样蛋白相互作用的问题。

Method: 利用HelixFold3，在统一框架下同时对蛋白库中蛋白折叠和小分子配体对接进行建模。

Result: 在约一百种小分子测试集上，与传统反向对接相比，该方法提高了筛选准确性，在结构保真度、结合位点精度和靶点优先级排序方面表现更优。

Conclusion: 该框架为剖析分子机制、探索脱靶相互作用和支持合理药物发现建立了可扩展且直接的平台。

Abstract: Identifying protein targets for small molecules, or reverse screening, is essential for understanding drug action, guiding compound repurposing, predicting off-target effects, and elucidating the molecular mechanisms of bioactive compounds. Despite its critical role, reverse screening remains challenging because accurately capturing interactions between a small molecule and structurally diverse proteins is inherently complex, and conventional step-wise workflows often propagate errors across decoupled steps such as target structure modeling, pocket identification, docking, and scoring. Here, we present an end-to-end reverse screening strategy leveraging HelixFold3, a high-accuracy biomolecular structure prediction model akin to AlphaFold3, which simultaneously models the folding of proteins from a protein library and the docking of small-molecule ligands within a unified framework. We validate this approach on a diverse and representative set of approximately one hundred small molecules. Compared with conventional reverse docking, our method improves screening accuracy and demonstrates enhanced structural fidelity, binding-site precision, and target prioritization. By systematically linking small molecules to their protein targets, this framework establishes a scalable and straightforward platform for dissecting molecular mechanisms, exploring off-target interactions, and supporting rational drug discovery.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [708] [AllShowers: One model for all calorimeter showers](https://arxiv.org/abs/2601.11716)
*Thorsten Buss,Henry Day-Hall,Frank Gaede,Gregor Kasieczka,Katja Krüger*

Main category: physics.ins-det

TL;DR: 提出统一生成模型AllShowers模拟多种粒子的量能器簇射，有创新点且表现出色，迈向通用模型。


<details>
  <summary>Details</summary>
Motivation: 传统量能器簇射建模的替代模型按粒子种类训练独立网络，限制扩展性和复用性，需降低计算成本。

Method: 引入基于Transformer架构的连续归一化流模型AllShowers，采用层嵌入、自定义注意力掩码方案和簇射及层最优传输映射。

Result: 在ILD探测器模拟簇射数据集上训练后，能为多种粒子生成逼真簇射，超越之前强子簇射单粒子模型的保真度。

Conclusion: AllShowers是对撞机实验中量能器簇射模拟通用模型的重要一步。

Abstract: Accurate and efficient detector simulation is essential for modern collider experiments. To reduce the high computational cost, various fast machine learning surrogate models have been proposed. Traditional surrogate models for calorimeter shower modeling train separate networks for each particle species, limiting scalability and reuse. We introduce AllShowers, a unified generative model that simulates calorimeter showers across multiple particle types using a single generative model. AllShowers is a continuous normalizing flow model with a Transformer architecture, enabling it to generate complex spatial and energy correlations in variable-length point cloud representations of showers. Trained on a diverse dataset of simulated showers in the highly granular ILD detector, the model demonstrates the ability to generate realistic showers for electrons, photons, and charged and neutral hadrons across a wide range of incident energies and angles without retraining. In addition to unifying shower generation for multiple particle types, AllShowers surpasses the fidelity of previous single-particle-type models for hadronic showers. Key innovations include the use of a layer embedding, allowing the model to learn all relevant calorimeter layer properties; a custom attention masking scheme to reduce computational demands and introduce a helpful inductive bias; and a shower- and layer-wise optimal transport mapping to improve training convergence and sample quality. AllShowers marks a significant step towards a universal model for calorimeter shower simulations in collider experiments.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [709] [A Comprehensive Review of Bio-Inspired Approaches to Coordination, Communication, and System Architecture in Underwater Swarm Robotics](https://arxiv.org/abs/2601.12244)
*Shyalan Ramesh,Scott Mann,Alex Stumpf*

Main category: cs.RO

TL;DR: 本文综述水下群体机器人技术，涵盖生物启发协调机制、通信策略和系统设计，分析算法、通信和硬件进展，提出分类框架，指明未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 海洋作业复杂性增加，需要智能机器人系统，水下群体机器人有潜力但研究分散。

Method: 综合生物启发协调机制、通信策略和系统设计考虑，分析关键算法、通信约束及解决方案，考察硬件和系统设计进展，用多维分类框架评估现有方法。

Result: 统一生物启发协调算法、通信方式和系统设计方法，确定融合趋势、关键挑战。

Conclusion: 为水下群体系统的实际部署指明未来研究方向。

Abstract: The increasing complexity of marine operations has intensified the need for intelligent robotic systems to support ocean observation, exploration, and resource management. Underwater swarm robotics offers a promising framework that extends the capabilities of individual autonomous platforms through collective coordination. Inspired by natural systems, such as fish schools and insect colonies, bio-inspired swarm approaches enable distributed decision-making, adaptability, and resilience under challenging marine conditions. Yet research in this field remains fragmented, with limited integration across algorithmic, communication, and hardware design perspectives. This review synthesises bio-inspired coordination mechanisms, communication strategies, and system design considerations for underwater swarm robotics. It examines key marine-specific algorithms, including the Artificial Fish Swarm Algorithm, Whale Optimisation Algorithm, Coral Reef Optimisation, and Marine Predators Algorithm, highlighting their applications in formation control, task allocation, and environmental interaction. The review also analyses communication constraints unique to the underwater domain and emerging acoustic, optical, and hybrid solutions that support cooperative operation. Additionally, it examines hardware and system design advances that enhance system efficiency and scalability. A multi-dimensional classification framework evaluates existing approaches across communication dependency, environmental adaptability, energy efficiency, and swarm scalability. Through this integrated analysis, the review unifies bio-inspired coordination algorithms, communication modalities, and system design approaches. It also identifies converging trends, key challenges, and future research directions for real-world deployment of underwater swarm systems.

</details>


### [710] [RobotDesignGPT: Automated Robot Design Synthesis using Vision Language Models](https://arxiv.org/abs/2601.11801)
*Nitish Sontakke,K. Niranjan Kumar,Sehoon Ha*

Main category: cs.RO

TL;DR: 提出自动化机器人设计框架RobotDesignGPT，利用大模型知识和推理能力，结合用户提示和参考图像合成设计，经方法改进设计质量，能设计多种机器人并通过研究验证框架。


<details>
  <summary>Details</summary>
Motivation: 机器人设计过程复杂，依赖专业知识和人力，现有方法多基于规则。

Method: 提出RobotDesignGPT框架，利用大预训练视觉语言模型的知识和推理能力，结合用户提示和参考图像合成初始设计，采用新颖视觉反馈方法改进设计。

Result: 框架能设计出视觉上吸引人且运动学有效的受自然启发的机器人。

Conclusion: 通过消融研究和用户研究验证了所提出框架的有效性。

Abstract: Robot design is a nontrivial process that involves careful consideration of multiple criteria, including user specifications, kinematic structures, and visual appearance. Therefore, the design process often relies heavily on domain expertise and significant human effort. The majority of current methods are rule-based, requiring the specification of a grammar or a set of primitive components and modules that can be composed to create a design. We propose a novel automated robot design framework, RobotDesignGPT, that leverages the general knowledge and reasoning capabilities of large pre-trained vision-language models to automate the robot design synthesis process. Our framework synthesizes an initial robot design from a simple user prompt and a reference image. Our novel visual feedback approach allows us to greatly improve the design quality and reduce unnecessary manual feedback. We demonstrate that our framework can design visually appealing and kinematically valid robots inspired by nature, ranging from legged animals to flying creatures. We justify the proposed framework by conducting an ablation study and a user study.

</details>


### [711] [AI for Green Spaces: Leveraging Autonomous Navigation and Computer Vision for Park Litter Removal](https://arxiv.org/abs/2601.11876)
*Christopher Kao,Akhil Pathapati,James Davis*

Main category: cs.RO

TL;DR: 美国有大量垃圾，提出在公园建造自主捡垃圾机器人，采用多种技术，总体成功率达80%，证明方案可行。


<details>
  <summary>Details</summary>
Motivation: 美国存在大量垃圾问题，尤其是野餐者在草地留下垃圾，需解决垃圾清理问题。

Method: 用Spanning Tree Coverage (STC)算法生成覆盖路径；用Real - Time Kinematic (RTK) GPS导航路径；用ResNet50 Convolutional Neural Network (CNN)进行计算机视觉识别；测试多种设计概念并选出新的捡垃圾机制。

Result: 机器人整体成功率达到80%。

Conclusion: 在草地使用自主捡垃圾机器人是可行的解决方案。

Abstract: There are 50 billion pieces of litter in the U.S. alone. Grass fields contribute to this problem because picnickers tend to leave trash on the field. We propose building a robot that can autonomously navigate, identify, and pick up trash in parks. To autonomously navigate the park, we used a Spanning Tree Coverage (STC) algorithm to generate a coverage path the robot could follow. To navigate this path, we successfully used Real-Time Kinematic (RTK) GPS, which provides a centimeter-level reading every second. For computer vision, we utilized the ResNet50 Convolutional Neural Network (CNN), which detects trash with 94.52% accuracy. For trash pickup, we tested multiple design concepts. We select a new pickup mechanism that specifically targets the trash we encounter on the field. Our solution achieved an overall success rate of 80%, demonstrating that autonomous trash pickup robots on grass fields are a viable solution.

</details>


### [712] [AirHunt: Bridging VLM Semantics and Continuous Planning for Efficient Aerial Object Navigation](https://arxiv.org/abs/2601.12742)
*Xuecheng Chen,Zongzhuo Liu,Jianfa Ma,Bang Du,Tiantian Zhang,Xueqian Wang,Boyu Zhou*

Main category: cs.RO

TL;DR: 文章提出AirHunt系统，融合VLM与路径规划，实现在户外环境零样本泛化查找开放集对象，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有系统存在VLM推理与实时规划频率不匹配、3D场景理解有限、缺乏平衡语义引导与运动效率机制等问题，需解决这些挑战以将VLM集成到实际航空系统。

Method: 提出AirHunt系统，采用双路径异步架构，建立VLM推理与路径规划协同接口；提出主动双任务推理模块和语义 - 几何相干规划模块。

Result: 在不同对象导航任务和环境评估中，AirHunt比现有方法成功率更高、导航误差更小、飞行时间更短，真实世界实验也验证了其实用性。

Conclusion: AirHunt系统能有效解决现有问题，在户外环境定位开放集对象方面具有良好性能，代码和数据集将公开。

Abstract: Recent advances in large Vision-Language Models (VLMs) have provided rich semantic understanding that empowers drones to search for open-set objects via natural language instructions. However, prior systems struggle to integrate VLMs into practical aerial systems due to orders-of-magnitude frequency mismatch between VLM inference and real-time planning, as well as VLMs' limited 3D scene understanding. They also lack a unified mechanism to balance semantic guidance with motion efficiency in large-scale environments. To address these challenges, we present AirHunt, an aerial object navigation system that efficiently locates open-set objects with zero-shot generalization in outdoor environments by seamlessly fusing VLM semantic reasoning with continuous path planning. AirHunt features a dual-pathway asynchronous architecture that establishes a synergistic interface between VLM reasoning and path planning, enabling continuous flight with adaptive semantic guidance that evolves through motion. Moreover, we propose an active dual-task reasoning module that exploits geometric and semantic redundancy to enable selective VLM querying, and a semantic-geometric coherent planning module that dynamically reconciles semantic priorities and motion efficiency in a unified framework, enabling seamless adaptation to environmental heterogeneity. We evaluate AirHunt across diverse object navigation tasks and environments, demonstrating a higher success rate with lower navigation error and reduced flight time compared to state-of-the-art methods. Real-world experiments further validate AirHunt's practical capability in complex and challenging environments. Code and dataset will be made publicly available before publication.

</details>


### [713] [ForeDiffusion: Foresight-Conditioned Diffusion Policy via Future View Construction for Robot Manipulation](https://arxiv.org/abs/2601.12925)
*Weize Xie,Yi Ding,Ying He,Leilei Wang,Binwen Bai,Zheyi Zhao,Chenyang Wang,F. Richard Yu*

Main category: cs.RO

TL;DR: 现有扩散策略在视觉运动控制中遇复杂任务成功率降低，本文提出Foresight - Conditioned Diffusion (ForeDiffusion) 并使用双损失机制，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有扩散策略在复杂任务下成功率低，存在仅依赖短期观测和单一去噪损失导致误差积累、抓握偏差等问题。

Method: 提出ForeDiffusion，将预测的未来视图表示注入扩散过程，采用双损失机制，结合传统去噪损失和未来观测一致性损失进行统一优化。

Result: 在Adroit套件和MetaWorld基准测试中，ForeDiffusion整体任务平均成功率达80%，在复杂任务中比现有主流扩散方法高23%，且在所有任务中性能更稳定。

Conclusion: ForeDiffusion能有效提升机器人视觉运动控制在复杂任务中的成功率，且性能更稳定，有较好应用前景。

Abstract: Diffusion strategies have advanced visual motor control by progressively denoising high-dimensional action sequences, providing a promising method for robot manipulation. However, as task complexity increases, the success rate of existing baseline models decreases considerably. Analysis indicates that current diffusion strategies are confronted with two limitations. First, these strategies only rely on short-term observations as conditions. Second, the training objective remains limited to a single denoising loss, which leads to error accumulation and causes grasping deviations. To address these limitations, this paper proposes Foresight-Conditioned Diffusion (ForeDiffusion), by injecting the predicted future view representation into the diffusion process. As a result, the policy is guided to be forward-looking, enabling it to correct trajectory deviations. Following this design, ForeDiffusion employs a dual loss mechanism, combining the traditional denoising loss and the consistency loss of future observations, to achieve the unified optimization. Extensive evaluation on the Adroit suite and the MetaWorld benchmark demonstrates that ForeDiffusion achieves an average success rate of 80% for the overall task, significantly outperforming the existing mainstream diffusion methods by 23% in complex tasks, while maintaining more stable performance across the entire tasks.

</details>


### [714] [Dynamic Hand Gesture Recognition for Robot Manipulator Tasks](https://arxiv.org/abs/2601.12918)
*Dharmendra Sharma,Peeyush Thakur,Sandeep Gupta,Narendra Kumar Dhar,Laxmidhar Behera*

Main category: cs.RO

TL;DR: 本文提出一种识别动态手势的新方法，通过基于高斯混合模型的无监督模型实时准确识别，训练和测试准确性证明了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 实现人类与机器人之间的无缝交互。

Method: 提出基于高斯混合模型的无监督模型，给每个机器人操作任务分配特定手势，并实时准确识别不同手势的动态变化。

Result: 在训练和实时测试中的准确性证明了该方法的有效性。

Conclusion: 所提的识别动态手势的方法是有效的。

Abstract: This paper proposes a novel approach to recognizing dynamic hand gestures facilitating seamless interaction between humans and robots. Here, each robot manipulator task is assigned a specific gesture. There may be several such tasks, hence, several gestures. These gestures may be prone to several dynamic variations. All such variations for different gestures shown to the robot are accurately recognized in real-time using the proposed unsupervised model based on the Gaussian Mixture model. The accuracy during training and real-time testing prove the efficacy of this methodology.

</details>


### [715] [Active Inference-Driven World Modeling for Adaptive UAV Swarm Trajectory Design](https://arxiv.org/abs/2601.12939)
*Kaleem Arshid,Ali Krayani,Lucio Marcenaro,David Martin Gomez,Carlo Regazzoni*

Main category: cs.RO

TL;DR: 提出基于主动推理的无人机群自主轨迹设计框架，结合概率推理与自学习，用GA - RF生成专家轨迹训练世界模型，仿真显示比Q - Learning有优势。


<details>
  <summary>Details</summary>
Motivation: 设计一个能让无人机群在动态环境中自主进行轨迹设计的有效框架。

Method: 集成概率推理和自学习，用GA - RF生成专家轨迹训练分层世界模型，在线运行时无人机通过最小化当前信念与模型预测状态的差异来推断动作。

Result: 仿真结果表明，比Q - Learning收敛更快、稳定性更高、导航更安全。

Conclusion: 所提出的框架具有可扩展性和认知基础，适用于智能无人机群控制。

Abstract: This paper proposes an Active Inference-based framework for autonomous trajectory design in UAV swarms. The method integrates probabilistic reasoning and self-learning to enable distributed mission allocation, route ordering, and motion planning. Expert trajectories generated using a Genetic Algorithm with Repulsion Forces (GA-RF) are employed to train a hierarchical World Model capturing swarm behavior across mission, route, and motion levels. During online operation, UAVs infer actions by minimizing divergence between current beliefs and model-predicted states, enabling adaptive responses to dynamic environments. Simulation results show faster convergence, higher stability, and safer navigation than Q-Learning, demonstrating the scalability and cognitive grounding of the proposed framework for intelligent UAV swarm control.

</details>


### [716] [Communication-Free Collective Navigation for a Swarm of UAVs via LiDAR-Based Deep Reinforcement Learning](https://arxiv.org/abs/2601.13657)
*Myong-Yol Choi,Hankyoul Ko,Hanse Cho,Changseung Kim,Seunghwan Kim,Jaemin Seo,Hyondong Oh*

Main category: cs.RO

TL;DR: 本文提出基于深度强化学习的无人机群集体导航控制器，在无通信环境实现导航，经模拟和真实实验验证。


<details>
  <summary>Details</summary>
Motivation: 实现无人机群在通信受限、复杂多障碍环境中的集体导航。

Method: 采用隐式领导 - 跟随框架，利用LiDAR点聚类和扩展卡尔曼滤波器，在Nvidia Isaac Sim中训练DRL控制器。

Result: 在模拟和五个无人机组成的真实实验中成功实现不同室内外环境下无通信和外部定位的集体导航。

Conclusion: 所提方法具有鲁棒性和从虚拟到现实的可迁移性。

Abstract: This paper presents a deep reinforcement learning (DRL) based controller for collective navigation of unmanned aerial vehicle (UAV) swarms in communication-denied environments, enabling robust operation in complex, obstacle-rich environments. Inspired by biological swarms where informed individuals guide groups without explicit communication, we employ an implicit leader-follower framework. In this paradigm, only the leader possesses goal information, while follower UAVs learn robust policies using only onboard LiDAR sensing, without requiring any inter-agent communication or leader identification. Our system utilizes LiDAR point clustering and an extended Kalman filter for stable neighbor tracking, providing reliable perception independent of external positioning systems. The core of our approach is a DRL controller, trained in GPU-accelerated Nvidia Isaac Sim, that enables followers to learn complex emergent behaviors - balancing flocking and obstacle avoidance - using only local perception. This allows the swarm to implicitly follow the leader while robustly addressing perceptual challenges such as occlusion and limited field-of-view. The robustness and sim-to-real transfer of our approach are confirmed through extensive simulations and challenging real-world experiments with a swarm of five UAVs, which successfully demonstrated collective navigation across diverse indoor and outdoor environments without any communication or external localization.

</details>


### [717] [Efficient Coordination with the System-Level Shared State: An Embodied-AI Native Modular Framework](https://arxiv.org/abs/2601.13945)
*Yixuan Deng,Tongrun Wu,Donghao Wu,Zeyu Wei,Jiayuan Wang,Zhenglong Sun,Yuqing Tang,Xiaoqiang Ji*

Main category: cs.RO

TL;DR: 介绍ANCHOR框架，使解耦和鲁棒性成为明确的系统级原语，助力闭环AI系统可扩展部署


<details>
  <summary>Details</summary>
Motivation: 实际中许多Embodied AI系统部署部分解耦，导致接口漂移、跨模块干扰等问题，需要更好的解决方法

Method: 提出ANCHOR框架，分离Canonical Records和通信总线，形成可检查的端到端循环

Result: 在去标识化工作流实例上验证闭环可行性，描述不同负载和发布率下的延迟分布，展示硬崩溃和重启后自动恢复

Conclusion: ANCHOR将临时集成变为显式契约，实现负载下的可控降级和闭环AI系统可扩展部署的自愈恢复

Abstract: As Embodied AI systems move from research prototypes to real world deployments, they tend to evolve rapidly while remaining reliable under workload changes and partial failures. In practice, many deployments are only partially decoupled: middleware moves messages, but shared context and feedback semantics are implicit, causing interface drift, cross-module interference, and brittle recovery at scale. We present ANCHOR, a modular framework that makes decoupling and robustness explicit system-level primitives. ANCHOR separates (i) Canonical Records, an evolvable contract for the standardized shared state, from (ii) a communication bus for many-to-many dissemination and feedback-oriented coordination, forming an inspectable end-to-end loop. We validate closed-loop feasibility on a de-identified workflow instantiation, characterize latency distributions under varying payload sizes and publish rates, and demonstrate automatic stream resumption after hard crashes and restarts even with shared-memory loss. Overall, ANCHOR turns ad-hoc integration glue into explicit contracts, enabling controlled degradation under load and self-healing recovery for scalable deployment of closed-loop AI systems.

</details>


### [718] [Group-Invariant Unsupervised Skill Discovery: Symmetry-aware Skill Representations for Generalizable Behavior](https://arxiv.org/abs/2601.14000)
*Junwoo Chang,Joseph Park,Roberto Horowitz,Jongmin Lee,Jongeun Choi*

Main category: cs.RO

TL;DR: 针对无监督技能发现中现有方法忽略环境几何对称性的问题，提出GISD框架，实验显示其效果更好。


<details>
  <summary>Details</summary>
Motivation: 现有无监督技能发现方法常忽略物理环境的几何对称性，导致行为冗余和样本效率低。

Method: 引入Group-Invariant Skill Discovery (GISD)框架，将群结构嵌入技能发现目标，提出Group-Invariant Wasserstein依赖度量，用群傅里叶表示参数化评分函数并通过等变潜在特征对齐定义内在奖励。

Result: 在基于状态和像素的运动基准测试中，GISD比强基线实现了更广泛的状态空间覆盖和更高的下游任务学习效率。

Conclusion: GISD有效解决了现有无监督技能发现方法的问题，提升了性能。

Abstract: Unsupervised skill discovery aims to acquire behavior primitives that improve exploration and accelerate downstream task learning. However, existing approaches often ignore the geometric symmetries of physical environments, leading to redundant behaviors and sample inefficiency. To address this, we introduce Group-Invariant Skill Discovery (GISD), a framework that explicitly embeds group structure into the skill discovery objective. Our approach is grounded in a theoretical guarantee: we prove that in group-symmetric environments, the standard Wasserstein dependency measure admits a globally optimal solution comprised of an equivariant policy and a group-invariant scoring function. Motivated by this, we formulate the Group-Invariant Wasserstein dependency measure, which restricts the optimization to this symmetry-aware subspace without loss of optimality. Practically, we parameterize the scoring function using a group Fourier representation and define the intrinsic reward via the alignment of equivariant latent features, ensuring that the discovered skills generalize systematically under group transformations. Experiments on state-based and pixel-based locomotion benchmarks demonstrate that GISD achieves broader state-space coverage and improved efficiency in downstream task learning compared to a strong baseline.

</details>


### [719] [DroneVLA: VLA based Aerial Manipulation](https://arxiv.org/abs/2601.13809)
*Fawad Mehboob,Monijesu James,Amir Habel,Jeffrin Sam,Miguel Altamirano Cabrera,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: 本文介绍了一种能理解自然语言指令的自主空中操作无人机系统，展示了其功能和性能。


<details>
  <summary>Details</summary>
Motivation: 当前空中平台向主动操作转变，需要设计让非专业用户能自然控制的直观界面。

Method: 集成基于Grounding DINO的MediaPipe和VLA模型到无人机，用VLA推理指令和生成任务队列，用Grounding DINO和动态A*算法导航，使用MediaPipe进行人机交互控制。

Result: 通过实验，系统定位和导航的最大、平均欧氏和均方根误差分别为0.164m、0.070m和0.084m。

Conclusion: VLA用于空中操作具有可行性。

Abstract: As aerial platforms evolve from passive observers to active manipulators, the challenge shifts toward designing intuitive interfaces that allow non-expert users to command these systems naturally. This work introduces a novel concept of autonomous aerial manipulation system capable of interpreting high-level natural language commands to retrieve objects and deliver them to a human user. The system is intended to integrate a MediaPipe based on Grounding DINO and a Vision-Language-Action (VLA) model with a custom-built drone equipped with a 1-DOF gripper and an Intel RealSense RGB-D camera. VLA performs semantic reasoning to interpret the intent of a user prompt and generates a prioritized task queue for grasping of relevant objects in the scene. Grounding DINO and dynamic A* planning algorithm are used to navigate and safely relocate the object. To ensure safe and natural interaction during the handover phase, the system employs a human-centric controller driven by MediaPipe. This module provides real-time human pose estimation, allowing the drone to employ visual servoing to maintain a stable, distinct position directly in front of the user, facilitating a comfortable handover. We demonstrate the system's efficacy through real-world experiments for localization and navigation, which resulted in a 0.164m, 0.070m, and 0.084m of max, mean euclidean, and root-mean squared errors, respectively, highlighting the feasibility of VLA for aerial manipulation operations.

</details>


### [720] [Zero-shot adaptable task planning for autonomous construction robots: a comparative study of lightweight single and multi-AI agent systems](https://arxiv.org/abs/2601.14091)
*Hossein Naderi,Alireza Shojaei,Lifu Huang,Philip Agee,Kereshmeh Afsari,Abiola Akanmu*

Main category: cs.RO

TL;DR: 研究探索用基础模型提升建筑机器人任务规划适应性与泛化性，提出四种模型，显示四智能体团队表现优且具成本效益，还能提升对AI团队理解。


<details>
  <summary>Details</summary>
Motivation: 机器人在未来建筑行业面临高成本和难以适应动态任务的挑战，需提升其任务规划的适应性和泛化性。

Method: 使用轻量级、开源大语言模型和视觉语言模型提出并实现四种模型，包括单个智能体和三个多智能体团队。

Result: 四智能体团队在多数指标上优于GPT - 4o，成本效益高十倍，三及四个智能体的团队泛化性提高。

Conclusion: 增进对AI团队的理解，为建筑外的非结构化环境研究提供支持。

Abstract: Robots are expected to play a major role in the future construction industry but face challenges due to high costs and difficulty adapting to dynamic tasks. This study explores the potential of foundation models to enhance the adaptability and generalizability of task planning in construction robots. Four models are proposed and implemented using lightweight, open-source large language models (LLMs) and vision language models (VLMs). These models include one single agent and three multi-agent teams that collaborate to create robot action plans. The models are evaluated across three construction roles: Painter, Safety Inspector, and Floor Tiling. Results show that the four-agent team outperforms the state-of-the-art GPT-4o in most metrics while being ten times more cost-effective. Additionally, teams with three and four agents demonstrate the improved generalizability. By discussing how agent behaviors influence outputs, this study enhances the understanding of AI teams and supports future research in diverse unstructured environments beyond construction.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [721] [Quantum Kernel Machine Learning for Autonomous Materials Science](https://arxiv.org/abs/2601.11775)
*Felix Adams,Daiwei Zhu,David W. Steuerman,A. Gilad Kusne,Ichiro Takeuchi*

Main category: cond-mat.mtrl-sci

TL;DR: 本文对比量子和经典核在自主材料科学序贯相空间导航中的效用，实验验证量子核模型优于部分经典核模型，凸显其加速材料发现的潜力。


<details>
  <summary>Details</summary>
Motivation: 自主材料科学关键在于用少量数据探索新材料，量子核模型理论上用更少训练数据达相似性能，因此研究其在自主材料发现中的应用。

Method: 计算Fe - Ga - Pd三元成分扩散库X射线衍射图案的量子核和几种经典核，在IonQ的Aria俘获离子量子计算机硬件及相应经典噪声模拟器上开展研究。

Result: 实验验证量子核模型能比一些经典核模型表现更好。

Conclusion: 量子核机器学习方法有加速材料发现的潜力，复杂X射线衍射数据是实现量子核模型优势的备选对象。

Abstract: Autonomous materials science, where active learning is used to navigate large compositional phase space, has emerged as a powerful vehicle to rapidly explore new materials. A crucial aspect of autonomous materials science is exploring new materials using as little data as possible. Gaussian process-based active learning allows effective charting of multi-dimensional parameter space with a limited number of training data, and thus is a common algorithmic choice for autonomous materials science. An integral part of the autonomous workflow is the application of kernel functions for quantifying similarities among measured data points. A recent theoretical breakthrough has shown that quantum kernel models can achieve similar performance with less training data than classical models. This signals the possible advantage of applying quantum kernel machine learning to autonomous materials discovery. In this work, we compare quantum and classical kernels for their utility in sequential phase space navigation for autonomous materials science. Specifically, we compute a quantum kernel and several classical kernels for x-ray diffraction patterns taken from an Fe-Ga-Pd ternary composition spread library. We conduct our study on both IonQ's Aria trapped ion quantum computer hardware and the corresponding classical noisy simulator. We experimentally verify that a quantum kernel model can outperform some classical kernel models. The results highlight the potential of quantum kernel machine learning methods for accelerating materials discovery and suggest complex x-ray diffraction data is a candidate for robust quantum kernel model advantage.

</details>


### [722] [Artificial Intelligence in Materials Science and Engineering: Current Landscape, Key Challenges, and Future Trajectorie](https://arxiv.org/abs/2601.12554)
*Iman Peivaste,Salim Belouettar,Francesco Mercuri,Nicholas Fantuzzi,Hamidreza Dehghani,Razieh Izadi,Halliru Ibrahim,Jakub Lengiewicz,Maël Belouettar-Mathis,Kouider Bendine,Ahmed Makradi,Martin Hörsch,Peter Klein,Mohamed El Hachemi,Heinz A. Preisig,Yacine Rezgui,Natalia Konchakova,Ali Daouadji*

Main category: cond-mat.mtrl-sci

TL;DR: 该综述介绍AI在材料科学工程应用，涵盖机器学习方法、数据作用，还提及挑战。


<details>
  <summary>Details</summary>
Motivation: AI发展迅速且数据增多，为材料研究者提供利用数据驱动技术的途径。

Method: 全面综述机器学习方法，如传统算法、深度学习架构等，强调数据表示和预处理等。

Result: 梳理了当前AI在材料科学领域各类机器学习方法和数据处理策略。

Conclusion: 指出数据质量、数量和标准化等挑战影响材料科学工程中模型发展和应用。

Abstract: Artificial Intelligence is rapidly transforming materials science and engineering, offering powerful tools to navigate complexity, accelerate discovery, and optimize material design in ways previously unattainable. Driven by the accelerating pace of algorithmic advancements and increasing data availability, AI is becoming an essential competency for materials researchers. This review provides a comprehensive and structured overview of the current landscape, synthesizing recent advancements and methodologies for materials scientists seeking to effectively leverage these data-driven techniques. We survey the spectrum of machine learning approaches, from traditional algorithms to advanced deep learning architectures, including CNNs, GNNs, and Transformers, alongside emerging generative AI and probabilistic models such as Gaussian Processes for uncertainty quantification. The review also examines the pivotal role of data in this field, emphasizing how effective representation and featurization strategies, spanning compositional, structural, image-based, and language-inspired approaches, combined with appropriate preprocessing, fundamentally underpin the performance of machine learning models in materials research. Persistent challenges related to data quality, quantity, and standardization, which critically impact model development and application in materials science and engineering, are also addressed.

</details>


### [723] [Ontology-aligned structuring and reuse of multimodal materials data and workflows towards automatic reproduction](https://arxiv.org/abs/2601.12582)
*Sepideh Baghaee Ravari,Abril Azocar Guzman,Sarath Menon,Stefan Sandfeld,Tilmann Hickel,Markus Stricker*

Main category: cond-mat.mtrl-sci

TL;DR: 提出一种本体驱动、大语言模型辅助的框架，从文献中提取和结构化计算工作流，以解决材料科学计算结果可重复性问题，虽受部分限制，但能提升数据可重用性。


<details>
  <summary>Details</summary>
Motivation: 材料科学中计算结果可重复性存在挑战，现有文本挖掘方法不足以提取完整计算工作流，需新方法实现大规 模整理和系统比较。

Method: 引入本体驱动、大语言模型辅助的框架，聚焦镁及其二元合金的堆垛层错能计算，采用多阶段过滤策略和提示工程化的大语言模型提取，统一信息并与材料本体对齐构建知识图谱。

Result: 构建知识图谱，可对堆垛层错能值进行系统比较，支持计算协议的结构化重用。

Conclusion: 虽因缺失元数据，完全的计算可重复性仍受限，但该框架能以语义互操作形式组织和关联已发表结果，提高计算材料数据的透明度和可重用性。

Abstract: Reproducibility of computational results remains a challenge in materials science, as simulation workflows and parameters are often reported only in unstructured text and tables. While literature data are valuable for validation and reuse, the lack of machine-readable workflow descriptions prevents large-scale curation and systematic comparison. Existing text-mining approaches are insufficient to extract complete computational workflows with their associated parameters. An ontology-driven, large language model (LLM)-assisted framework is introduced for the automated extraction and structuring of computational workflows from the literature. The approach focuses on density functional theory-based stacking fault energy (SFE) calculations in hexagonal close-packed magnesium and its binary alloys, and uses a multi-stage filtering strategy together with prompt-engineered LLM extraction applied to method sections and tables. Extracted information is unified into a canonical schema and aligned with established materials ontologies (CMSO, ASMO, and PLDO), enabling the construction of a knowledge graph using atomRDF. The resulting knowledge graph enables systematic comparison of reported SFE values and supports the structured reuse of computational protocols. While full computational reproducibility is still constrained by missing or implicit metadata, the framework provides a foundation for organizing and contextualizing published results in a semantically interoperable form, thereby improving transparency and reusability of computational materials data.

</details>


### [724] [CatMaster: An Agentic Autonomous System for Computational Heterogeneous Catalysis Research](https://arxiv.org/abs/2601.13508)
*Honghao Chen,Jiangjie Qiu,Yi Shen Tew,Xiaonan Wang*

Main category: cond-mat.mtrl-sci

TL;DR: 提出由大语言模型驱动的代理系统CatMaster，可将自然语言请求转化为计算工作区，通过四个不同复杂度的示例展示其功能，旨在让催化研究者专注建模和化学解释。


<details>
  <summary>Details</summary>
Motivation: 计算催化研究工作流成本高、迭代性强且易受设置影响，实际工作流问题会使项目变慢、结果难重现和扩展。

Method: 开发CatMaster系统，将自然语言请求转化为完整计算工作空间，搭配多保真工具库，涵盖快速替代弛豫和高保真DFT计算。

Result: 通过四个复杂度递增的示例展示了CatMaster系统的功能。

Conclusion: CatMaster可减少人工脚本编写和记录工作，让催化研究者专注于建模选择和化学解释，而非工作流管理。

Abstract: Density functional theory (DFT) is widely used to connect atomic structure with catalytic behavior, but computational heterogeneous catalysis studies often require long workflows that are costly, iterative, and sensitive to setup choices. Besides the intrinsic cost and accuracy limits of first-principles calculations, practical workflow issues such as keeping references consistent, preparing many related inputs, recovering from failed runs on computing clusters, and maintaining a complete record of what was done, can slow down projects and make results difficult to reproduce or extend.
  Here we present CatMaster, a large-language-model (LLM)-driven agent system that turns natural language requests into complete calculation workspaces, including structures, inputs, outputs, logs, and a concise run record. CatMaster maintains a persistent project record of key facts, constraints, and file pointers to support inspection and restartability. It is paired with a multi-fidelity tool library that covers rapid surrogate relaxations and high-fidelity DFT calculations for validation when needed. We demonstrate CatMaster on four demonstrations of increasing complexity: an O2 spin-state check with remote execution, BCC Fe surface energies with a protocol-sensitivity study and CO adsorption site ranking, high-throughput Pt--Ni--Cu alloy screening for hydrogen evolution reaction (HER) descriptors with surrogate-to-DFT validation, and a demonstration beyond the predefined tool set, including equation-of-state fitting for BCC Fe and CO-FeN4-graphene single-atom catalyst geometry preparation. By reducing manual scripting and bookkeeping while keeping the full evidence trail, CatMaster aims to help catalysis researchers focus on modeling choices and chemical interpretation rather than workflow management.

</details>


<div id='cs.SC'></div>

# cs.SC [[Back]](#toc)

### [725] [Breaking the Data Barrier in Learning Symbolic Computation: A Case Study on Variable Ordering Suggestion for Cylindrical Algebraic Decomposition](https://arxiv.org/abs/2601.13731)
*Rui-Juan Jing,Yuegang Zhao,Changbo Chen*

Main category: cs.SC

TL;DR: 本文针对符号计算中CAD排序效率问题，设计系列易获取标注数据的关联任务，预训练Transformer模型并微调，实验表明新模型预测排序优于最佳启发式方法。


<details>
  <summary>Details</summary>
Motivation: 符号计算效率受高维深度计算限制，获取标注数据困难，现有基于学习的方法在CAD排序效率上仅与最佳专家启发式方法相当。

Method: 设计一系列易获取大量标注数据的紧密关联任务，用这些数据预训练Transformer模型，再在CAD排序数据集上微调。

Result: 在公开的CAD排序数据集实验中，新模型预测的排序平均显著优于最佳启发式方法建议的排序。

Conclusion: 通过设计特定任务获取数据训练和微调模型，能有效提升CAD排序效率，为加速符号计算提供了有效方法。

Abstract: Symbolic computation, powered by modern computer algebra systems, has important applications in mathematical reasoning through exact deep computations. The efficiency of symbolic computation is largely constrained by such deep computations in high dimension. This creates a fundamental barrier on labelled data acquisition if leveraging supervised deep learning to accelerate symbolic computation. Cylindrical algebraic decomposition (CAD) is a pillar symbolic computation method for reasoning with first-order logic formulas over reals with many applications in formal verification and automatic theorem proving. Variable orderings have a huge impact on its efficiency. Impeded by the difficulty to acquire abundant labelled data, existing learning-based approaches are only competitive with the best expert-based heuristics. In this work, we address this problem by designing a series of intimately connected tasks for which a large amount of annotated data can be easily obtained. We pre-train a Transformer model with these data and then fine-tune it on the datasets for CAD ordering. Experiments on publicly available CAD ordering datasets show that on average the orderings predicted by the new model are significantly better than those suggested by the best heuristic methods.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [726] [Cascaded Transformer for Robust and Scalable SLA Decomposition via Amortized Optimization](https://arxiv.org/abs/2601.11859)
*Cyril Shih-Huan Hsu*

Main category: cs.NI

TL;DR: 本文介绍用于6G网络切片的SLA分解模型Casformer，性能优于现有方法，可推进网络自动化。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案在将端到端SLA分解为特定领域SLA时，计算密集、延迟和复杂度高。

Method: 提出Casformer，用两层Transformer结构，在DINNs启发的学习范式下训练，融入风险建模和摊销优化。

Result: Casformer在SLA分解质量上优于现有基于优化的框架，在不稳定有噪声网络条件下可扩展性和鲁棒性更强。

Conclusion: 将摊销优化与基于Transformer的序列建模结合可推进网络自动化，适用于5G及未来网络的实时SLA管理。

Abstract: The evolution toward 6G networks increasingly relies on network slicing to provide tailored, End-to-End (E2E) logical networks over shared physical infrastructures. A critical challenge is effectively decomposing E2E Service Level Agreements (SLAs) into domain-specific SLAs, which current solutions handle through computationally intensive, iterative optimization processes that incur substantial latency and complexity. To address this, we introduce Casformer, a cascaded Transformer architecture designed for fast, optimization-free SLA decomposition. Casformer leverages historical domain feedback encoded through domain-specific Transformer encoders in its first layer, and integrates cross-domain dependencies using a Transformer-based aggregator in its second layer. The model is trained under a learning paradigm inspired by Domain-Informed Neural Networks (DINNs), incorporating risk-informed modeling and amortized optimization to learn a stable, forward-only SLA decomposition policy. Extensive evaluations demonstrate that Casformer achieves improved SLA decomposition quality against state-of-the-art optimization-based frameworks, while exhibiting enhanced scalability and robustness under volatile and noisy network conditions. In addition, its forward-only design reduces runtime complexity and simplifies deployment and maintenance. These insights reveal the potential of combining amortized optimization with Transformer-based sequence modeling to advance network automation, providing a scalable and efficient solution suitable for real-time SLA management in advanced 5G-and-beyond network environments.

</details>


### [727] [Cross-reality Location Privacy Protection in 6G-enabled Vehicular Metaverses: An LLM-enhanced Hybrid Generative Diffusion Model-based Approach](https://arxiv.org/abs/2601.12311)
*Xiaofeng Luo,Jiayi He,Jiawen Kang,Ruichen Zhang,Zhaoshui He,Ekram Hossain,Dong In Kim*

Main category: cs.NI

TL;DR: 本文针对6G车联网元宇宙中自动驾驶汽车跨现实交互的位置隐私风险，设计了基于混合动作的保护框架，提出新隐私度量，用LHDPPO算法求解优化问题，实验证明框架有效。


<details>
  <summary>Details</summary>
Motivation: 6G车联网元宇宙中，自动驾驶汽车跨现实交互会导致严重的位置隐私风险，需要解决该问题。

Method: 设计基于混合动作（现实中连续位置扰动和虚拟中离散隐私感知AI代理迁移）的跨现实位置隐私保护框架，提出交叉现实位置熵隐私度量，用LHDPPO算法求解优化问题。

Result: 在真实数据集上的大量实验表明，该框架能有效减少自动驾驶汽车的跨现实位置隐私泄露，同时保持用户的强沉浸感。

Conclusion: 所提出的框架在6G车联网元宇宙场景中能有效保护自动驾驶汽车的位置隐私。

Abstract: The emergence of 6G-enabled vehicular metaverses enables Autonomous Vehicles (AVs) to operate across physical and virtual spaces through space-air-ground-sea integrated networks. The AVs can deploy AI agents powered by large AI models as personalized assistants, on edge servers to support intelligent driving decision making and enhanced on-board experiences. However, such cross-reality interactions may cause serious location privacy risks, as adversaries can infer AV trajectories by correlating the location reported when AVs request LBS in reality with the location of the edge servers on which their corresponding AI agents are deployed in virtuality. To address this challenge, we design a cross-reality location privacy protection framework based on hybrid actions, including continuous location perturbation in reality and discrete privacy-aware AI agent migration in virtuality. In this framework, a new privacy metric, termed cross-reality location entropy, is proposed to effectively quantify the privacy levels of AVs. Based on this metric, we formulate an optimization problem to optimize the hybrid action, focusing on achieving a balance between location protection, service latency reduction, and quality of service maintenance. To solve the complex mixed-integer problem, we develop a novel LLM-enhanced Hybrid Diffusion Proximal Policy Optimization (LHDPPO) algorithm, which integrates LLM-driven informative reward design to enhance environment understanding with double Generative Diffusion Models-based policy exploration to handle high-dimensional action spaces, thereby enabling reliable determination of optimal hybrid actions. Extensive experiments on real-world datasets demonstrate that the proposed framework effectively mitigates cross-reality location privacy leakage for AVs while maintaining strong user immersion within 6G-enabled vehicular metaverse scenarios.

</details>


### [728] [LiQSS: Post-Transformer Linear Quantum-Inspired State-Space Tensor Networks for Real-Time 6G](https://arxiv.org/abs/2601.12375)
*Farhad Rezazadeh,Hatim Chergui,Mehdi Bennis,Houbing Song,Lingjia Liu,Dusit Niyato,Merouane Debbah*

Main category: cs.NI

TL;DR: 本文研究6G O - RAN高效无线电遥测预测的后Transformer设计范式，提出量子启发的多体状态空间张量网络模型LiQSS，其在参数规模、推理速度和预测精度上表现良好。


<details>
  <summary>Details</summary>
Motivation: 6G O - RAN的主动和自主控制需要严格近实时延迟和计算约束下的控制级预测，而Transformer模型复杂度高，限制了其在近实时RAN智能控制器分析中的可扩展性。

Method: 提出量子启发的多体状态空间张量网络，用稳定的结构化状态空间动态核替换自注意力机制；采用张量列车/矩阵积态表示的张量网络分解减少参数化和数据移动；使用轻量级通道门控和混合层捕捉非平稳的跨关键性能指标依赖。

Result: 将模型实例化为自主感知 - 预测xApp，在定制数据集上评估，LiQSS比之前的结构化状态空间基线模型小10.8 - 15.8倍、快约1.4倍；与Transformer模型相比，参数数量最多减少155倍，推理速度最多快2.74倍，且不牺牲预测精度。

Conclusion: 提出的LiQSS模型在6G O - RAN无线电遥测预测中，能在满足近实时性能要求的同时，保持良好的预测精度。

Abstract: Proactive and agentic control in Sixth-Generation (6G) Open Radio Access Networks (O-RAN) requires control-grade prediction under stringent Near-Real-Time (Near-RT) latency and computational constraints. While Transformer-based models are effective for sequence modeling, their quadratic complexity limits scalability in Near-RT RAN Intelligent Controller (RIC) analytics. This paper investigates a post-Transformer design paradigm for efficient radio telemetry forecasting. We propose a quantum-inspired many-body state-space tensor network that replaces self-attention with stable structured state-space dynamics kernels, enabling linear-time sequence modeling. Tensor-network factorizations in the form of Tensor Train (TT) / Matrix Product State (MPS) representations are employed to reduce parameterization and data movement in both input projections and prediction heads, while lightweight channel gating and mixing layers capture non-stationary cross-Key Performance Indicator (KPI) dependencies. The proposed model is instantiated as an agentic perceive-predict xApp and evaluated on a bespoke O-RAN KPI time-series dataset comprising 59,441 sliding windows across 13 KPIs, using Reference Signal Received Power (RSRP) forecasting as a representative use case. Our proposed Linear Quantum-Inspired State-Space (LiQSS) model is 10.8x-15.8x smaller and approximately 1.4x faster than prior structured state-space baselines. Relative to Transformer-based models, LiQSS achieves up to a 155x reduction in parameter count and up to 2.74x faster inference, without sacrificing forecasting accuracy.

</details>


### [729] [IntAgent: NWDAF-Based Intent LLM Agent Towards Advanced Next Generation Networks](https://arxiv.org/abs/2601.13114)
*Abdelrahman Soliman,Ahmed Refaey,Aiman Erbad,Amr Mohamed*

Main category: cs.NI

TL;DR: 介绍智能意图LLM代理IntAgent，整合NWDAF分析与工具实现网络运营商意图，通过用例证明框架有效性。


<details>
  <summary>Details</summary>
Motivation: 现有IBN技术发展，需实现一种能利用网络分析信息推理决策，满足运营商网络意图自动化的方案。

Method: 在NWDAF分析引擎中开发意图工具引擎，利用实时网络分析进行推理和工具选择，提供3GPP合规数据源及MCP工具服务器。

Result: 通过两个实际用例（ML流量预测和定时策略执行）证明了IntAgent能自主完成复杂网络意图。

Conclusion: 所提出的集成NWDAF分析和工具的IntAgent框架在实现网络运营商意图方面有效可行。

Abstract: Intent-based networks (IBNs) are gaining prominence as an innovative technology that automates network operations through high-level request statements, defining what the network should achieve. In this work, we introduce IntAgent, an intelligent intent LLM agent that integrates NWDAF analytics and tools to fulfill the network operator's intents. Unlike previous approaches, we develop an intent tools engine directly within the NWDAF analytics engine, allowing our agent to utilize live network analytics to inform its reasoning and tool selection. We offer an enriched, 3GPP-compliant data source that enhances the dynamic, context-aware fulfillment of network operator goals, along with an MCP tools server for scheduling, monitoring, and analytics tools. We demonstrate the efficacy of our framework through two practical use cases: ML-based traffic prediction and scheduled policy enforcement, which validate IntAgent's ability to autonomously fulfill complex network intents.

</details>


### [730] [Reinforcement Learning for Opportunistic Routing in Software-Defined LEO-Terrestrial Systems](https://arxiv.org/abs/2601.13662)
*Sivaram Krishnan,Zhouyou Gu,Jihong Park,Sung-Min Oh,Jinho Choi*

Main category: cs.NI

TL;DR: 针对大规模低轨卫星星座，利用GEO - SDN控制器提出机会路由策略，用残差强化学习框架优化以降低传输延迟，仿真显示队列长度减少效果显著。


<details>
  <summary>Details</summary>
Motivation: 大规模低轨卫星星座需要能在时变拓扑和间歇性网关可见性下有效传输数据到地面网络的智能路由策略。

Method: 引入机会路由策略，将数据包转发到任何可用地面网关；构建约束随机优化问题，采用残差强化学习框架优化机会路由。

Result: 多日轨道数据仿真表明，与经典背压和其他知名排队算法相比，该方法在减少队列长度方面有显著改善。

Conclusion: 此机会路由策略是在高度动态低轨网络中实现低延迟和可靠数据传输的有效方法。

Abstract: The proliferation of large-scale low Earth orbit (LEO) satellite constellations is driving the need for intelligent routing strategies that can effectively deliver data to terrestrial networks under rapidly time-varying topologies and intermittent gateway visibility. Leveraging the global control capabilities of a geostationary (GEO)-resident software-defined networking (SDN) controller, we introduce opportunistic routing, which aims to minimize delivery delay by forwarding packets to any currently available ground gateways rather than fixed destinations. This makes it a promising approach for achieving low-latency and robust data delivery in highly dynamic LEO networks. Specifically, we formulate a constrained stochastic optimization problem and employ a residual reinforcement learning framework to optimize opportunistic routing for reducing transmission delay. Simulation results over multiple days of orbital data demonstrate that our method achieves significant improvements in queue length reduction compared to classical backpressure and other well-known queueing algorithms.

</details>


### [731] [Variational Dual-path Attention Network for CSI-Based Gesture Recognition](https://arxiv.org/abs/2601.13745)
*N. Zhang*

Main category: cs.NI

TL;DR: 提出轻量化特征预处理模块VDAN用于WiFi手势识别，实验验证其有效性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于CSI的WiFi手势识别受高维噪声和边缘设备资源限制问题挑战，且端到端模型存在冗余和泛化性差问题。

Method: 提出Variational Dual - path Attention Network (VDAN)模块，通过频域滤波和时间检测进行特征细化，引入变分推理建模注意力权重的不确定性。

Result: 在公开数据集实验显示，学习的注意力权重与CSI物理稀疏特征相符，验证了可解释性。

Conclusion: 为资源受限的无线传感系统提供了高效且可解释的前端处理解决方案。

Abstract: Wi-Fi gesture recognition based on Channel State Information (CSI) is challenged by high-dimensional noise and resource constraints on edge devices. Prevailing end-to-end models tightly couple feature extraction with classification, overlooking the inherent time-frequency sparsity of CSI and leading to redundancy and poor generalization. To address this, this paper proposes a lightweight feature preprocessing module--the Variational Dual-path Attention Network (VDAN). It performs structured feature refinement through frequency-domain filtering and temporal detection. Variational inference is introduced to model the uncertainty in attention weights, thereby enhancing robustness to noise. The design principles of the module are explained from the perspectives of the information bottleneck and regularization. Experiments on a public dataset demonstrate that the learned attention weights align with the physical sparse characteristics of CSI, verifying its interpretability. This work provides an efficient and explainable front-end processing solution for resource-constrained wireless sensing systems.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [732] [Forecasting Continuum Intensity for Solar Active Region Emergence Prediction using Transformers](https://arxiv.org/abs/2601.13144)
*Jonas Tirona,Sarang Patil,Spiridon Kasapis,Eren Dogan,John Stefan,Irina N. Kitiashvili,Alexander G. Kosovichev,Mengjia Xu*

Main category: astro-ph.SR

TL;DR: 本文基于LSTM方法，用Transformer架构预测太阳活动区出现，经消融研究优化模型，结果显示修改后的Transformer架构在提前预警上表现出色。


<details>
  <summary>Details</summary>
Motivation: 早期准确预测太阳活动区出现对空间天气预报至关重要，需改进现有预测方法。

Method: 采用滑动窗口Transformer架构，用46个太阳活动区数据预测连续强度演变；进行消融研究评估含一维卷积前端和‘早期检测’架构两个关键组件。

Result: 最佳模型结合无Conv1D层的早期检测架构，RMSE为0.1189，比LSTM基线提高10.6%，平均提前预警时间4.73小时；Transformer在整体时间和准确性上表现优，但有更高方差。

Conclusion: 不带时间平滑层、经早期检测偏差修改的Transformer架构，为预测太阳活动区出现提供高灵敏度替代方案，更注重提前预警而非统计平滑性。

Abstract: Early and accurate prediction of solar active region (AR) emergence is crucial for space weather forecasting. Building on established Long Short-Term Memory (LSTM) based approaches for forecasting the continuum intensity decrease associated with AR emergence, this work expands the modeling with new architectures and targets. We investigate a sliding-window Transformer architecture to forecast continuum intensity evolution up to 12 hours ahead using data from 46 ARs observed by SDO/HMI. We conduct a systematic ablation study to evaluate two key components: (1) the inclusion of a temporal 1D convolutional (Conv1D) front-end and (2) a novel `Early Detection' architecture featuring attention biases and a timing-aware loss function. Our best-performing model, combining the Early Detection architecture without the Conv1D layer, achieved a Root Mean Square Error (RMSE) of 0.1189 (representing a 10.6% improvement over the LSTM baseline) and an average advance warning time of 4.73 hours (timing difference of -4.73h), even under a stricter emergence criterion than previous studies. While the Transformer demonstrates superior aggregate timing and accuracy, we note that this high-sensitivity detection comes with increased variance compared to smoother baseline models. However, this volatility is a necessary trade-off for operational warning systems: the model's ability to detect micro-changes in precursor signals enables significantly earlier detection, outweighing the cost of increased noise. Our results demonstrate that Transformer architectures modified with early detection biases, when used without temporal smoothing layers, provide a high-sensitivity alternative for forecasting AR emergence that prioritizes advance warning over statistical smoothness.

</details>


### [733] [SolARED: Solar Active Region Emergence Dataset for Machine Learning Aided Predictions](https://arxiv.org/abs/2601.13145)
*Spiridon Kasapis,Eren Dogan,Irina N. Kitiashvili,Alexander G. Kosovichev,John T. Stefan,Jake D. Butler,Jonas Tirona,Sarang Patil,Mengjia Xu*

Main category: astro-ph.SR

TL;DR: 为预测太阳爆发活动，制备Solar Active Region Emergence Dataset (SolARED) 支持预测活动区出现，数据集公开可用。


<details>
  <summary>Details</summary>
Motivation: 精确预测太阳爆发活动对防护太空技术和探索影响越发重要，需在活动区形成前检测到以实现预警。

Method: 从太阳动力学天文台 (SDO) 上的日震与磁成像仪 (HMI) 获取的多普勒速度、磁场和连续强度全圆盘图中提取数据，制备包含 2010 - 2023 年 50 个大型活动区及周边区域相关数据的数据集。

Result: 制备出可供机器学习使用的 SolARED 数据集。

Conclusion: SolARED 数据集能增强预测能力，支持活动区出现的运行预报，且可通过网站获取。

Abstract: The development of accurate forecasts of solar eruptive activity has become increasingly important for preventing potential impacts on space technologies and exploration. Therefore, it is crucial to detect Active Regions (ARs) before they start forming on the solar surface. This will enable the development of early-warning capabilities for upcoming space weather disturbances. For this reason, we prepared the Solar Active Region Emergence Dataset (SolARED). The dataset is derived from full-disk maps of the Doppler velocity, magnetic field, and continuum intensity, obtained by the Helioseismic and Magnetic Imager (HMI) onboard the Solar Dynamics Observatory (SDO). SolARED includes time series of remapped, tracked, and binned data that characterize the evolution of acoustic power of solar oscillations, unsigned magnetic flux, and continuum intensity for 50 large ARs before, during, and after their emergence on the solar surface, as well as surrounding areas observed on the solar disc between 2010 and 2023. The resulting ML-ready SolARED dataset is designed to support enhancements of predictive capabilities, enabling the development of operational forecasts for the emergence of active regions. The SolARED dataset is available at https://sun.njit.edu/sarportal/, through an interactive visualization web application.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [734] [Polychronous Wave Computing: Timing-Native Address Selection in Spiking Networks](https://arxiv.org/abs/2601.13079)
*Natalila G. Berloff*

Main category: cond-mat.dis-nn

TL;DR: 介绍了Polychronous Wave Computing (PWC)，一种基于时间的地址选择原语，适用于多种平台。


<details>
  <summary>Details</summary>
Motivation: 多数神经形态和光子系统将事件数字化后在时钟逻辑中进行选择，本文旨在引入一种直接在波域将相对尖峰延迟映射到离散输出路径的方法。

Method: 将尖峰时间进行相位编码，通过可编程多端口干涉仪并行评估K个模板相关性，由驱动 - 耗散赢家通吃阶段进行物理argmax操作。

Result: 模拟显示非线性竞争比线性强度读取提高路由保真度，硬件在环相位调谐可提高时间顺序门的准确性。

Conclusion: PWC为查找表式脉冲网络和稀疏top - 1门提供快速路由协处理器，适用于多种平台。

Abstract: Spike timing offers a combinatorial address space, suggesting that timing-based spiking inference can be executed as lookup and routing rather than as dense multiply--accumulate. Yet most neuromorphic and photonic systems still digitize events into timestamps, bins, or rates and then perform selection in clocked logic. We introduce Polychronous Wave Computing (PWC), a timing-native address-selection primitive that maps relative spike latencies directly to a discrete output route in the wave domain. Spike times are phase-encoded in a rotating frame and processed by a programmable multiport interferometer that evaluates K template correlations in parallel; a driven--dissipative winner-take-all stage then performs a physical argmax, emitting a one-hot output port. We derive the operating envelope imposed by phase wrapping and mutual coherence, and collapse timing jitter, static phase mismatch, and dephasing into a single effective phase-noise budget whose induced winner--runner-up margin predicts boundary-first failures and provides an intensity-only calibration target. Simulations show that nonlinear competition improves routing fidelity compared with noisy linear intensity readout, and that hardware-in-the-loop phase tuning rescues a temporal-order gate from 55.9% to 97.2% accuracy under strong static mismatch. PWC provides a fast routing coprocessor for LUT-style spiking networks and sparse top-1 gates (e.g., mixture-of-experts routing) across polaritonic, photonic, and oscillator platforms.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [735] [Explicit Almost-Optimal $\varepsilon$-Balanced Codes via Free Expander Walks](https://arxiv.org/abs/2601.12606)
*Jun-Ting Hsieh,Sidhanth Mohanty,Rachel Yun Zhang*

Main category: cs.CC

TL;DR: 研究低速率、高距离下构造匹配Gilbert - Varshamov界的显式码问题，改进Ta - Shma构造方法，给出更简单的近最优构造。


<details>
  <summary>Details</summary>
Motivation: 在低速率、高距离情况下，构造出速率和距离匹配Gilbert - Varshamov界的显式码。

Method: 基于所谓的自由扩张器游走，即从精心选择的序列中每个步骤在不同扩张器上进行普通扩张器游走，扩张器序列源自O'Donnell和Wu的近X - Ramanujan图构造。

Result: 给出了一种比Ta - Shma构造更简单的近最优构造。

Conclusion: 新的基于自由扩张器游走的构造方法为解决该编码构造问题提供了更优方案。

Abstract: We study the problem of constructing explicit codes whose rate and distance match the Gilbert-Varshamov bound in the low-rate, high-distance regime. In 2017, Ta-Shma gave an explicit family of codes where every pair of codewords has relative distance $\frac{1-\varepsilon}{2}$, with rate $Ω(\varepsilon^{2+o(1)})$, matching the Gilbert-Varshamov bound up to a factor of $\varepsilon^{o(1)}$. Ta-Shma's construction was based on starting with a good code and amplifying its bias with walks arising from the $s$-wide-replacement product.
  In this work, we give an arguably simpler almost-optimal construction, based on what we call free expander walks: ordinary expander walks where each step is taken on a distinct expander from a carefully chosen sequence. This sequence of expanders is derived from the construction of near-$X$-Ramanujan graphs due to O'Donnell and Wu.

</details>


### [736] [The Query Complexity of Local Search in Rounds on General Graphs](https://arxiv.org/abs/2601.13266)
*Simina Brânzei,Ioannis Panageas,Dimitris Paparas*

Main category: cs.CC

TL;DR: 分析t轮内在一般图中寻找局部最小值的查询复杂度，给出确定性上界和随机化下界，发现并行最速下降法可改进部分图的界限。


<details>
  <summary>Details</summary>
Motivation: 查询复杂度在网格上已有研究，但在其他图中了解较少，且该问题可应用于许多优化任务，如神经网络训练。

Method: 对任意n个顶点的图进行理论分析，通过证明得到确定性上界和随机化下界，研究并行最速下降法。

Result: 证明了确定性上界为$O(t n^{1/t} (sΔ)^{1 - 1/t})$，随机化下界为$Ω(t n^{1/t}-t)$，并行最速下降法可改进高分离数和有界度图的界限。

Conclusion: 明确了在一般图上t轮内寻找局部最小值的查询复杂度上下界，并行最速下降法在特定图中有优势。

Abstract: We analyze the query complexity of finding a local minimum in $t$ rounds on general graphs. More precisely, given a graph $G = (V,E)$ and oracle access to an unknown function $f : V \to \mathbb{R}$, the goal is to find a local minimum--a vertex $v$ such that $f(v) \leq f(u)$ for all $(u,v) \in E$--using at most $t$ rounds of interaction with the oracle. The query complexity is well understood on grids, but much less is known beyond. This abstract problem captures many optimization tasks, such as finding a local minimum of a loss function during neural network training.
  For each graph with $n$ vertices, we prove a deterministic upper bound of $O(t n^{1/t} (sΔ)^{1-1/t})$, where $s$ is the separation number and $Δ$ is the maximum degree of the graph. We complement this result with a randomized lower bound of $Ω(t n^{1/t}-t)$ that holds for any connected graph. We also find that parallel steepest descent with a warm start provides improved bounds for graphs with high separation number and bounded degree.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [737] [Canonicalization of Batched Einstein Summations for Tuning Retrieval](https://arxiv.org/abs/2601.12220)
*Kaushik Kulkarni,Andreas Klöckner*

Main category: cs.MS

TL;DR: 提出将Batched Einstein Summation表达式归一化的算法，通过图规范化得到唯一形式，评估显示有速度提升。


<details>
  <summary>Details</summary>
Motivation: Batched einsums等价表示缺乏规范形式，阻碍优化和调优知识的复用。

Method: 将batched einsums编码为彩色图，应用图规范化得到标准形式，用功能数组操作数表示einsums并提供转换策略。

Result: 与JAX对比，TCCG基准套件和FEM求解器中的einsums有4.7倍的几何平均加速。

Conclusion: 所提出的算法可有效实现Batched Einstein Summation表达式的归一化，提升性能。

Abstract: We present an algorithm for normalizing \emph{Batched Einstein Summation}
  expressions by mapping mathematically equivalent formulations to a unique
  normal form. Batches of einsums with the same Einstein notation that exhibit
  substantial data reuse appear frequently in finite element methods (FEM),
  numerical linear algebra, and computational chemistry. To effectively exploit
  this temporal locality for high performance, we consider groups of einsums in
  batched form.
  Representations of equivalent batched einsums may differ due to index
  renaming, permutations within the batch, and, due to the commutativity and
  associativity of multiplication operation. The lack of a canonical
  representation hinders the reuse of optimization and tuning knowledge in
  software systems. To this end, we develop a novel encoding of batched einsums
  as colored graphs and apply graph canonicalization to derive a normal form.
  In addition to the canonicalization algorithm, we propose a representation of
  einsums using functional array operands and provide a strategy to transfer
  transformations operating on the normal form to \emph{functional batched
  einsums} that exhibit the same normal form; crucial for fusing surrounding
  computations for memory bound einsums. We evaluate our approach against JAX,
  and observe a geomean speedup of $4.7\times$ for einsums from the TCCG
  benchmark suite and an FEM solver.

</details>


<div id='math.SP'></div>

# math.SP [[Back]](#toc)

### [738] [Persistent Sheaf Laplacian Analysis of Protein Stability and Solubility Changes upon Mutation](https://arxiv.org/abs/2601.12219)
*Yiming Ren,Junjie Wee,Xi Chen,Grace Qian,Guo-Wei Wei*

Main category: math.SP

TL;DR: 本文提出基于拓扑深度学习和持久层拉普拉斯的SheafLapNet预测框架，融合多视角特征，在稳定性和溶解度预测基准测试中达到了最先进水平，提升了预测的可解释性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有计算模型缺乏可解释性且未整合关键物理化学相互作用，而基因突变常破坏蛋白质结构等，是多种疾病的主要驱动因素。

Method: 提出SheafLapNet统一预测框架，将层论不变量与蛋白质变压器特征和辅助物理描述符相结合，以多尺度和机制性方式捕捉分子内在相互作用，并进行回归和分类任务的严格基准测试。

Result: SheafLapNet在稳定性和溶解度预测的多个基准测试中达到了最先进水平。

Conclusion: 层论建模显著增强了预测突变引起的结构和功能变化的可解释性和泛化性。

Abstract: Genetic mutations frequently disrupt protein structure, stability, and solubility, acting as primary drivers for a wide spectrum of diseases. Despite the critical importance of these molecular alterations, existing computational models often lack interpretability, and fail to integrate essential physicochemical interaction. To overcome these limitations, we propose SheafLapNet, a unified predictive framework grounded in the mathematical theory of Topological Deep Learning (TDL) and Persistent Sheaf Laplacian (PSL). Unlike standard Topological Data Analysis (TDA) tools such as persistent homology, which are often insensitive to heterogeneous information, PSL explicitly encodes specific physical and chemical information such as partial charges directly into the topological analysis. SheafLapNet synergizes these sheaf-theoretic invariants with advanced protein transformer features and auxiliary physical descriptors to capture intrinsic molecular interactions in a multiscale and mechanistic manner. To validate our framework, we employ rigorous benchmarks for both regression and classification tasks. For stability prediction, we utilize the comprehensive S2648 and S350 datasets. For solubility prediction, we employ the PON-Sol2 dataset, which provides annotations for increased, decreased, or neutral solubility changes. By integrating these multi-perspective features, SheafLapNet achieves state-of-the-art performance across these diverse benchmarks, demonstrating that sheaf-theoretic modeling significantly enhances both interpretability and generalizability in predicting mutation-induced structural and functional changes.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [739] [Irreversible Failure Reverses the Value of Information](https://arxiv.org/abs/2601.12046)
*Nicholas H. Kirk*

Main category: econ.TH

TL;DR: 研究带隐藏状态和吸收性失败的动态博弈，发现信息精度与崩溃概率关系及不透明性对均衡生存的影响。


<details>
  <summary>Details</summary>
Motivation: 探究信念驱动行动引发不可逆崩溃的动态博弈中信息结构的作用。

Method: 通过极限可行性准则形式化机制，用布莱克威尔混淆对不透明性建模。

Result: 信息精度提高有限期崩溃概率，透明破坏均衡可行性，不透明可恢复。事前选择信息结构时，正不透明性对均衡生存必要。

Conclusion: 不可逆失败是动态博弈中产生对不透明性内生需求的根本原因。

Abstract: We study dynamic games with hidden states and absorbing failure, where belief-driven actions can trigger irreversible collapse. In such environments, equilibria that sustain activity generically operate at the boundary of viability. We show that this geometry endogenously reverses the value of information: greater informational precision increases the probability of collapse on every finite horizon. We formalize this mechanism through a limit-viability criterion, and model opacity as a strategic choice of the information structure via Blackwell garbling. When failure is absorbing, survival values become locally concave in beliefs, implying that transparency destroys equilibrium viability while sufficient opacity restores it. In an extended game where agents choose the information structure ex ante, strictly positive opacity is necessary for equilibrium survival. The results identify irreversible failure--not coordination, misspecification, or ambiguity--as a primitive force generating an endogenous demand for opacity in dynamic games.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [740] [Concatenated Matrix SVD: Compression Bounds, Incremental Approximation, and Error-Constrained Clustering](https://arxiv.org/abs/2601.11626)
*Maksym Shamrai*

Main category: math.NA

TL;DR: 提出基于理论的矩阵聚类框架，在SVD压缩约束下实现带误差控制的聚类。


<details>
  <summary>Details</summary>
Motivation: 现有矩阵拼接压缩方法未解决在明确重构误差约束下哪些矩阵可安全拼接压缩的问题，且无误差保证。

Method: 建立水平拼接矩阵的谱界，推导最优秩 - r SVD重构误差的全局上界；开发基于增量截断SVD的近似估计器；提出三种聚类算法。

Result: 提出的算法能在预测联合SVD压缩误差低于用户指定阈值时合并矩阵，平衡速度、准确性和可扩展性。

Conclusion: 所提框架可实现带明确误差控制的压缩感知聚类，代码已开源。

Abstract: Large collections of matrices arise throughout modern machine learning, signal processing, and scientific computing, where they are commonly compressed by concatenation followed by truncated singular value decomposition (SVD). This strategy enables parameter sharing and efficient reconstruction and has been widely adopted across domains ranging from multi-view learning and signal processing to neural network compression. However, it leaves a fundamental question unanswered: which matrices can be safely concatenated and compressed together under explicit reconstruction error constraints? Existing approaches rely on heuristic or architecture-specific grouping and provide no principled guarantees on the resulting SVD approximation error. In the present work, we introduce a theory-driven framework for compression-aware clustering of matrices under SVD compression constraints. Our analysis establishes new spectral bounds for horizontally concatenated matrices, deriving global upper bounds on the optimal rank-$r$ SVD reconstruction error from lower bounds on singular value growth. The first bound follows from Weyl-type monotonicity under blockwise extensions, while the second leverages singular values of incremental residuals to yield tighter, per-block guarantees. We further develop an efficient approximate estimator based on incremental truncated SVD that tracks dominant singular values without forming the full concatenated matrix. Therefore, we propose three clustering algorithms that merge matrices only when their predicted joint SVD compression error remains below a user-specified threshold. The algorithms span a trade-off between speed, provable accuracy, and scalability, enabling compression-aware clustering with explicit error control. Code is available online.

</details>


### [741] [Streaming Operator Inference for Model Reduction of Large-Scale Dynamical Systems](https://arxiv.org/abs/2601.12161)
*Tomoki Koike,Prakash Mohan,Marc T. Henry de Frahan,Julie Bessac,Elizabeth Qian*

Main category: math.NA

TL;DR: 提出Streaming OpInf方法，从顺序到达的数据流中学习降阶模型，减少内存需求并支持在线模型适应，实验表明其精度与传统方法相当且有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统OpInf是批量学习方法，在大规模应用中无法将所有数据加载到内存，且不能自然地利用新数据更新模型。

Method: 提出Streaming OpInf方法，采用增量SVD进行自适应基构造，递归LS进行流式算子更新，可灵活组合不同的流算法。

Result: 数值实验表明，Streaming OpInf精度与批量 OpInf 相当，减少99%以上的内存需求，实现超过31000倍的降维，预测速度提高多个数量级。

Conclusion: Streaming OpInf能有效解决传统 OpInf 在大规模应用中的局限，有良好的应用前景。

Abstract: Projection-based model reduction enables efficient simulation of complex dynamical systems by constructing low-dimensional surrogate models from high-dimensional data. The Operator Inference (OpInf) approach learns such reduced surrogate models through a two-step process: constructing a low-dimensional basis via Singular Value Decomposition (SVD) to compress the data, then solving a linear least-squares (LS) problem to infer reduced operators that govern the dynamics in this compressed space, all without access to the underlying code or full model operators, i.e., non-intrusively. Traditional OpInf operates as a batch learning method, where both the SVD and LS steps process all data simultaneously. This poses a barrier to deployment of the approach on large-scale applications where dataset sizes prevent the loading of all data into memory at once. Additionally, the traditional batch approach does not naturally allow model updates using new data acquired during online computation. To address these limitations, we propose Streaming OpInf, which learns reduced models from sequentially arriving data streams. Our approach employs incremental SVD for adaptive basis construction and recursive LS for streaming operator updates, eliminating the need to store complete data sets while enabling online model adaptation. The approach can flexibly combine different choices of streaming algorithms for numerical linear algebra: we systematically explore the impact of these choices both analytically and numerically to identify effective combinations for accurate reduced model learning. Numerical experiments on benchmark problems and a large-scale turbulent channel flow demonstrate that Streaming OpInf achieves accuracy comparable to batch OpInf while reducing memory requirements by over 99% and enabling dimension reductions exceeding 31,000x, resulting in orders-of-magnitude faster predictions.

</details>


### [742] [Deep Neural networks for solving high-dimensional parabolic partial differential equations](https://arxiv.org/abs/2601.13256)
*Wenzhong Zhang,Zhenyuan Hu,Wei Cai,George EM Karniadakis*

Main category: math.NA

TL;DR: 该综述介绍用神经网络方法求解高维抛物型偏微分方程，按三种范式组织文献，给出实例并讨论挑战与方向。


<details>
  <summary>Details</summary>
Motivation: 高维偏微分方程数值解受维数诅咒限制，经典方法失效，神经网络提供新方案，需综述介绍相关方法。

Method: 以三种统一范式（PDE残差法、随机方法、混合无导数随机差分法）组织文献，阐述其数学公式、算法实现及优缺点。

Result: 用代表性基准问题（如高达1000维的Hamilton - Jacobi - Bellman和Black - Scholes方程）展示方法可扩展性、有效性和准确性。

Conclusion: 讨论高维偏微分方程可靠可扩展求解器面临的开放挑战和未来方向。

Abstract: The numerical solution of high dimensional partial differential equations (PDEs) is severely constrained by the curse of dimensionality (CoD), rendering classical grid--based methods impractical beyond a few dimensions. In recent years, deep neural networks have emerged as a promising mesh free alternative, enabling the approximation of PDE solutions in tens to thousands of dimensions. This review provides a tutorial--oriented introduction to neural--network--based methods for solving high dimensional parabolic PDEs, emphasizing conceptual clarity and methodological connections. We organize the literature around three unifying paradigms: (i) PDE residual--based approaches, including physicsinformed neural networks and their high dimensional variants; (ii) stochastic methods derived from Feynman--Kac and backward stochastic differential equation formulations; and (iii) hybrid derivative--free random difference approaches designed to alleviate the computational cost of derivatives in high dimensions. For each paradigm, we outline the underlying mathematical formulation, algorithmic implementation, and practical strengths and limitations. Representative benchmark problems--including Hamilton--Jacobi--Bellman and Black--Scholes equations in up to 1000 dimensions --illustrate the scalability, effectiveness, and accuracy of the methods. The paper concludes with a discussion of open challenges and future directions for reliable and scalable solvers of high dimensional PDEs.

</details>


### [743] [Optimizing Parallel Schemes with Lyapunov Exponents and kNN-LLE Estimation](https://arxiv.org/abs/2601.13604)
*Mudassir Shams,Andrei Velichko,Bruno Carpentieri*

Main category: math.NA

TL;DR: 提出一种统一分析 - 数据驱动方法来识别、测量和减少单参数逆并行求解器中的不稳定性，通过理论推导和计算分析实现，实验表明该方法增强了鲁棒性，建立微序列李雅普诺夫分析工具。


<details>
  <summary>Details</summary>
Motivation: 逆并行方案在计算非线性系统根时不可或缺，但动力学行为不稳定。研究旨在识别、测量和减少单参数逆并行求解器中的不稳定性。

Method: 理论上推导迭代映射的稳定性和分岔特征；计算上引入基于kNN的局部最大李雅普诺夫指数微序列管道，提供实时诊断，进而提出李雅普诺夫信息参数选择策略。

Result: 实验表明理论稳定性图与经验李雅普诺夫分布吻合，自适应机制显著提高了鲁棒性。

Conclusion: 确立微序列李雅普诺夫分析为构建自稳定求根方案的实用、可解释工具，为扩展诊断到高维或噪声污染问题开辟了途径。

Abstract: Inverse parallel schemes remain indispensable tools for computing the roots of nonlinear systems, yet their dynamical behavior can be unexpectedly rich, ranging from strong contraction to oscillatory or chaotic transients depending on the choice of algorithmic parameters and initial states. A unified analytical-data-driven methodology for identifying, measuring, and reducing such instabilities in a family of uni-parametric inverse parallel solvers is presented in this study. On the theoretical side, we derive stability and bifurcation characterizations of the underlying iterative maps, identifying parameter regions associated with periodic or chaotic behavior. On the computational side, we introduce a micro-series pipeline based on kNN-driven estimation of the local largest Lyapunov exponent (LLE), applied to scalar time series derived from solver trajectories. The resulting sliding-window Lyapunov profiles provide fine-grained, real-time diagnostics of contractive or unstable phases and reveal transient behaviors not captured by coarse linearized analysis. Leveraging this correspondence, we introduce a Lyapunov-informed parameter selection strategy that identifies solver settings associated with stable behavior, particularly when the estimated LLE indicates persistent instability. Comprehensive experiments on ensembles of perturbed initial guesses demonstrate close agreement between the theoretical stability diagrams and empirical Lyapunov profiles, and show that the proposed adaptive mechanism significantly improves robustness. The study establishes micro-series Lyapunov analysis as a practical, interpretable tool for constructing self-stabilizing root-finding schemes and opens avenues for extending such diagnostics to higher-dimensional or noise-contaminated problems.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [744] [On Analyzing the Conditions for Stability of Opportunistic Supply Chains Under Network Growth](https://arxiv.org/abs/2601.11566)
*Gurkirat Wadhwa,Priyank Sinha*

Main category: econ.GN

TL;DR: 论文针对大型企业面临的持续波动问题，开发GBM - 贝叶斯 - LOLOG集成框架分析机会主义供应链（OSC）稳定性，确定关键波动阈值，通过实验验证并表明模型可扩展到其他OSC。


<details>
  <summary>Details</summary>
Motivation: 大型企业面临成本、需求和原材料供应的持续波动，传统中断模型无法评估，持续波动引发机会主义行为形成OSC，需研究其演化。

Method: 开发集成数学框架，结合GBM模型表示随机价格波动、贝叶斯学习模型描述伙伴可靠性的信念更新、LOLOG网络模型描述网络结构内生变化，并在基于代理的模拟中实现。

Result: 确定关键波动阈值，网络会从稳定保链状态转变为碎片化状态；分析得出波动对盈利能力、信任和链接激活的单调影响，推导形式稳定性条件和相变。

Conclusion: 开发的GBM - 贝叶斯 - LOLOG集成框架可分析OSC稳定性，且模型可扩展到其他人道主义、制药和家禽等网络。

Abstract: Even large firms such as Walmart, Apple, and Coca-Cola face persistent fluctuations in costs, demand, and raw material availability. These are not \textit{rare events} and cannot be evaluated using traditional disruption models focused on infrequent events. Instead, sustained volatility induces opportunistic behavior, as firms repeatedly reconfigure partners in absence of long-term contracts, often due to trust deficits. The resulting web of transient relationships forms opportunistic supply chains (OSCs). To capture OSC evolution, we develop an integrated mathematical framework combining a Geometric Brownian Motion (GBM) model to represent stochastic price volatility, a Bayesian learning model to describe adaptive belief updates regarding partner reliability, and a Latent Order Logistic (LOLOG) network model for endogenous changes in network structure. This framework is implemented in an agent-based simulation to examine how volatility, trust, and network structure jointly shape SC resilience. Our modeling approach identifies critical volatility threshold; a tipping point beyond which the network shifts from a stable, link-preserving regime to a fragmented regime marked by rapid relationship dissolution. We analytically establish monotonic effects of volatility on profitability, trust, and link activation; derive formal stability conditions and volatility-driven phase transitions, and show how these mechanisms shape node importance and procurement behavior. These theoretical mechanisms are illustrated through computational experiments reflecting industry behaviors in fast fashion, electronics, and perishables. Overall, our contribution is to develop an integrated GBM-Bayesian-LOLOG framework to analyze OSC stability and our model can be extended to other OSCs including humanitarian, pharmaceutical, and poultry networks.

</details>


### [745] [Measuring growth and convergence at the mesoscale](https://arxiv.org/abs/2601.12158)
*Isaak Mengesha,Debraj Roy*

Main category: econ.GN

TL;DR: 全球不平等转向国内，国家层面数据常误判发展动态，中等收入‘跳板’变平，能力分层影响增长，国家政策可能与能力积累地理尺度不匹配


<details>
  <summary>Details</summary>
Motivation: 研究全球不平等转移背景下国家层面数据对发展动态的反映情况，以及新的增长影响因素

Method: 使用8790个新协调的功能城市区域（FUAs）数据、高分辨率全球GDP数据和国家层面能力指标进行分析

Result: 国家层面数据不能准确反映增长、趋同和结构变化动态；中等收入‘跳板’变平；低收入地区有正增长预期，无贫困陷阱；能力分层决定增长模式，能力提升呈J曲线

Conclusion: 国家趋同政策可能与能力积累的地理尺度不一致

Abstract: Global inequality has shifted inward, with rising dispersion increasingly occurring within countries rather than between them. Using 8,790 newly harmonised Functional Urban Areas (FUAs), micro-founded labour-market regions encompassing 3.9 billion people and representing approximately 80% of global GDP, we show that national aggregates systematically, and increasingly, misrepresent the dynamics of growth, convergence, and structural change. Drawing on high-resolution global GDP data and country-level capability measures, we find that the middle-income trampoline that previously drove global convergence is flattening. This divergence in the lower-income regime does not reflect poverty traps: low-income FUAs exhibit positive expected growth, and the transition curve displays no stable low-income equilibrium. Instead, productive capabilities, proxied by the Economic Complexity Index, define distinct growth regimes. FUAs converge within capability strata but diverge across them, and capability upgrading follows a predictable J-curve marked by short-run disruption and medium-run acceleration. These findings suggest that national convergence policies may be systematically misaligned with the geographic scale at which capability accumulation operates.

</details>


### [746] [The Economics of Digital Intelligence Capital: Endogenous Depreciation and the Structural Jevons Paradox](https://arxiv.org/abs/2601.12339)
*Yukun Zhang,Tianyang Zhang*

Main category: econ.GN

TL;DR: 本文构建AI行业微观经济理论，指出其特征会从三方面重塑行业动态，还分析相关条件，模型证实机制和影响，提供统一框架。


<details>
  <summary>Details</summary>
Motivation: 构建AI行业微观经济理论，研究其特征对行业动态的影响。

Method: 将大语言模型建模为数字智能资本，构建基于主体的模型进行校准。

Result: 发现红皇后效应、结构性杰文斯悖论、数据飞轮可使市场走向赢家通吃，还分析了包装器陷阱条件。

Conclusion: 研究提供了连接上下游的统一框架，为AI经济的竞争、可扩展性和监管提供新见解。

Abstract: This paper develops a micro-founded economic theory of the AI industry by modeling large language models as a distinct asset class-Digital Intelligence Capital-characterized by data-compute complementarities, increasing returns to scale, and relative (rather than absolute) valuation. We show that these features fundamentally reshape industry dynamics along three dimensions. First, because downstream demand depends on relative capability, innovation by one firm endogenously depreciates the economic value of rivals' existing capital, generating a persistent innovation pressure we term the Red Queen Effect. Second, falling inference prices induce downstream firms to adopt more compute-intensive agent architectures, rendering aggregate demand for compute super-elastic and producing a structural Jevons paradox. Third, learning from user feedback creates a data flywheel that can destabilize symmetric competition: when data accumulation outpaces data decay, the market bifurcates endogenously toward a winner-takes-all equilibrium. We further characterize conditions under which expanding upstream capabilities erode downstream application value (the Wrapper Trap). A calibrated agent-based model confirms these mechanisms and their quantitative implications. Together, the results provide a unified framework linking intelligence production upstream with agentic demand downstream, offering new insights into competition, scalability, and regulation in the AI economy.

</details>


### [747] [Economic complexity and regional development in India: Insights from a state-industry bipartite network](https://arxiv.org/abs/2601.12356)
*Joel M Thomas,Abhijit Chakraborty*

Main category: econ.GN

TL;DR: 研究利用公司数据构建印度邦 - 行业二分网络，计算经济复杂性指数，发现地区能力结构异质性，强调能力积累对经济绩效的作用及基于企业数据评估地区生产结构的有用性。


<details>
  <summary>Details</summary>
Motivation: 研究印度各邦的经济复杂性，为制定能力导向的产业和区域政策提供量化基础。

Method: 构建邦 - 行业二分网络，计算经济复杂性指数，应用适应度 - 复杂性算法。

Result: 地区能力结构存在显著异质性，复杂性指标与人均邦国内生产总值呈强正相关，活跃公司数量持续指数增长，有序二进制矩阵呈现三角结构。

Conclusion: 基于企业的数据对评估地区生产结构有用，能力导向策略对印度各邦平衡和可持续发展很重要，该研究推动了经济复杂性方法的实证应用。

Abstract: This study investigates the economic complexity of Indian states by constructing a state-industry bipartite network using firm-level data on registered companies and their paid-up capital. We compute the Economic Complexity Index and apply the fitness-complexity algorithm to quantify the diversity and sophistication of productive capabilities across the Indian states and two union territories. The results reveal substantial heterogeneity in regional capability structures, with states such as Maharashtra, Karnataka, and Delhi exhibiting consistently high complexity, while others remain concentrated in ubiquitous, low-value industries. The analysis also shows a strong positive relationship between complexity metrics and per-capita Gross State Domestic Product, underscoring the role of capability accumulation in shaping economic performance. Additionally, the number of active firms in India demonstrates a persistent exponential growth at an annual rate of 11.2%, reflecting ongoing formalization and industrial expansion. The ordered binary matrix displays the characteristic triangular structure observed in complexity studies, validating the applicability of complexity frameworks at the sub-national level. This work highlights the usefulness of firm-based data for assessing regional productive structures and emphasizes the importance of capability-oriented strategies for fostering balanced and sustainable development across Indian states. By demonstrating the usefulness of firm registry data in data constrained environments, this study advances the empirical application of economic complexity methods and provides a quantitative foundation for capability-oriented industrial and regional policy in India.

</details>


### [748] [Generative AI as a Non-Convex Supply Shock: Market Bifurcation and Welfare Analysis](https://arxiv.org/abs/2601.12488)
*Yukun Zhang,Tianyang Zhang*

Main category: econ.GN

TL;DR: 本文构建三层一般均衡框架研究生成式AI对市场、动态和福利的影响，发现市场分层、转型非单调及福利受污染强度影响，应转向输出端管理。


<details>
  <summary>Details</summary>
Motivation: 生成式AI扩散带来边际成本趋近零与信息污染问题，需研究其对市场结构、动态和社会福利的影响。

Method: 构建三层一般均衡框架，结合静态垂直差异化模型、平均场演化系统和有界理性的校准基于代理模型。

Result: 市场分层出现“中产阶级空心”；转型非单调，有暂时生态崩溃和选择性恢复；福利影响受污染强度高度敏感。

Conclusion: 自由放任采用AI可能低效，最优治理应从投入监管转向输出端拥堵管理。

Abstract: The diffusion of Generative AI (GenAI) constitutes a supply shock of a fundamentally different nature: while marginal production costs approach zero, content generation creates congestion externalities through information pollution. We develop a three-layer general equilibrium framework to study how this non-convex technology reshapes market structure, transition dynamics, and social welfare. In a static vertical differentiation model, we show that the GenAI cost shock induces a kinked production frontier that bifurcates the market into exit, AI, and human segments, generating a ``middle-class hollow'' in the quality distribution. To analyze adjustment paths, we embed this structure in a mean-field evolutionary system and a calibrated agent-based model with bounded rationality. The transition to the AI-integrated equilibrium is non-monotonic: rather than smooth diffusion, the economy experiences a temporary ecological collapse driven by search frictions and delayed skill adaptation, followed by selective recovery. Survival depends on asymmetric skill reconfiguration, whereby humans retreat from technical execution toward semantic creativity. Finally, we show that the welfare impact of AI adoption is highly sensitive to pollution intensity: low congestion yields monotonic welfare gains, whereas high pollution produces an inverted-U relationship in which further AI expansion reduces total welfare. These results imply that laissez-faire adoption can be inefficient and that optimal governance must shift from input regulation toward output-side congestion management.

</details>


### [749] [Liability Sharing and Staffing in AI-Assisted Online Medical Consultation](https://arxiv.org/abs/2601.12817)
*Yang Xiao*

Main category: econ.GN

TL;DR: 本文构建Stackelberg排队模型研究AI辅助在线医疗咨询中责任分担与人员配置交互作用，分析最优政策及福利差距，为合同和产能校准提供指导。


<details>
  <summary>Details</summary>
Motivation: 现有研究很少在整合框架中考察AI辅助在线医疗咨询中责任分担与人员配置的交互作用，本文旨在填补这一空白。

Method: 构建Stackelberg排队模型，平台选择责任分担比例和人员配置水平，医生选择诊断模式。

Result: 医生模式选择有阈值结构；最优平台政策使责任低于阈值；更高拥堵或人员成本倾向AI辅助，更高损失严重性倾向独立诊断；平台和社会最优福利差距随损失严重性扩大。

Conclusion: 本研究明确责任设计如何通过排队动态传播，为AI辅助医疗咨询中合同和产能校准提供指导。

Abstract: Liability sharing and staffing jointly determine service quality in AI-assisted online medical consultation, yet their interaction is rarely examined in an integrated framework linking contracts to congestion via physician responses. This paper develops a Stackelberg queueing model where the platform selects a liability share and a staffing level while physicians choose between AI-assisted and independent diagnostic modes. Physician mode choice exhibits a threshold structure, with the critical liability share decreasing in loss severity and increasing in the effort cost of independent diagnosis. Optimal platform policy sets liability below this threshold to trade off risk transfer against compliance costs, revealing that liability sharing and staffing function as substitute safety mechanisms. Higher congestion or staffing costs tilt optimal policy toward AI-assisted operation, whereas elevated loss severity shifts the preferred regime toward independent diagnosis. The welfare gap between platform and social optima widens with loss severity, suggesting greater scope for incentive alignment in high-stakes settings. By endogenizing physician mode choice within a congested service system, this study clarifies how liability design propagates through queueing dynamics, offering guidance for calibrating contracts and capacity in AI-assisted medical consultation.

</details>


### [750] [AI Skills Improve Job Prospects: Causal Evidence from a Hiring Experiment](https://arxiv.org/abs/2601.13286)
*Fabian Stephany,Ole Teutloff,Angelo Leone*

Main category: econ.GN

TL;DR: 研究通过对英美招聘者调查，发现AI技能可显著提升面试邀请概率，能部分或完全抵消年龄和低学历劣势，且受招聘者背景和AI使用情况影响。


<details>
  <summary>Details</summary>
Motivation: 探究AI技能在招聘决策中的作用，以及能否抵消年龄和低学历等传统劣势。

Method: 对英美1700名招聘者进行实验调查，采用配对联合设计，让招聘者评估合成简历的虚拟候选人。

Result: 在三个职业中，AI技能使面试邀请概率提高8 - 15个百分点，能部分或完全抵消年龄和低学历劣势，不同职业效果有差异，招聘者背景和AI使用情况会调节效果。

Conclusion: AI技能是强大的招聘信号，可缓解传统劳动力市场劣势，对员工技能获取和企业招聘实践有启示。

Abstract: The growing adoption of artificial intelligence (AI) technologies has heightened interest in the labour market value of AI-related skills, yet causal evidence on their role in hiring decisions remains scarce. This study examines whether AI skills serve as a positive hiring signal and whether they can offset conventional disadvantages such as older age or lower formal education. We conduct an experimental survey with 1,700 recruiters from the United Kingdom and the United States. Using a paired conjoint design, recruiters evaluated hypothetical candidates represented by synthetically designed resumes. Across three occupations - graphic designer, office assistant, and software engineer - AI skills significantly increase interview invitation probabilities by approximately 8 to 15 percentage points. AI skills also partially or fully offset disadvantages related to age and lower education, with effects strongest for office assistants, where formal AI certification plays an additional compensatory role. Effects are weaker for graphic designers, consistent with more skeptical recruiter attitudes toward AI in creative work. Finally, recruiters' own background and AI usage significantly moderate these effects. Overall, the findings demonstrate that AI skills function as a powerful hiring signal and can mitigate traditional labour market disadvantages, with implications for workers' skill acquisition strategies and firms' recruitment practices.

</details>


### [751] [Human-AI Collaboration in Radiology: The Case of Pulmonary Embolism](https://arxiv.org/abs/2601.13379)
*Paul Goldsmith-Pinkham,Chenhao Tan,Alexander K. Zentefis*

Main category: econ.GN

TL;DR: 研究放射科医生使用AI诊断肺栓塞情况，发现AI辅助可改善工作流程且不影响结果，不同医生与AI协作有差异。


<details>
  <summary>Details</summary>
Motivation: 了解放射科医生在现实世界中如何使用AI诊断肺栓塞。

Method: 跟踪近400名放射科医生在医院系统中使用FDA批准的诊断平台解读超10万次扫描。

Result: AI标记肺栓塞时医生同意率84%，预测无肺栓塞时同意率97%；扫描量增16%，诊断速度稳定，人均月工作量近翻倍，患者死亡率不变；不同医生与AI协作有显著差异；医生推翻AI诊断后，54%后续扫描30天内显示两者达成无肺栓塞共识。

Conclusion: AI可改善工作流程且不影响诊断结果，不同医生与AI协作情况不同，适度参与AI诊断时一致性最高。

Abstract: We study how radiologists use AI to diagnose pulmonary embolism (PE), tracking over 100,000 scans interpreted by nearly 400 radiologists during the staggered rollout of a real-world FDA-approved diagnostic platform in a hospital system. When AI flags PE, radiologists agree 84% of the time; when AI predicts no PE, they agree 97%. Disagreement evolves substantially: radiologists initially reject AI-positive PEs in 30% of cases, dropping to 12% by year two. Despite a 16% increase in scan volume, diagnostic speed remains stable while per-radiologist monthly volumes nearly double, with no change in patient mortality -- suggesting AI improves workflow without compromising outcomes. We document significant heterogeneity in AI collaboration: some radiologists reject AI-flagged PEs half the time while others accept nearly always; female radiologists are 6 percentage points less likely to override AI than male radiologists. Moderate AI engagement is associated with the highest agreement, whereas both low and high engagement show more disagreement. Follow-up imaging reveals that when radiologists override AI to diagnose PE, 54% of subsequent scans show both agreeing on no PE within 30 days.

</details>


### [752] [Liabilities for the social cost of carbon](https://arxiv.org/abs/2601.13834)
*Matthew K. Agrawala,Richard S. J. Tol*

Main category: econ.GN

TL;DR: 用元分析和综合评估模型估算国家社会碳成本，发现其与人均收入和人口规模有关，定义净责任，指出中等收入高碳国家净责任为正，贫富国家可获补偿。


<details>
  <summary>Details</summary>
Motivation: 估算国家社会碳成本并分析不同国家碳排放的影响及责任情况。

Method: 使用近期对气候变化总影响的元分析和标准综合评估模型。

Result: 国家社会碳成本与人均收入和人口规模相关，中等收入高碳国家净责任为正，贫富国家因排放低或脆弱可获补偿。

Conclusion: 通过模型估算能明确不同国家碳排放的自我伤害程度和净责任情况，为碳排放责任分配和补偿提供依据。

Abstract: We estimate the national social cost of carbon using a recent meta-analysis of the total impact of climate change and a standard integrated assessment model. The average social cost of carbon closely follows per capita income, the national social cost of carbon the size of the population. The national social cost of carbon measures self-harm. Net liability is defined as the harm done by a country's emissions on other countries minus the harm done to a country by other countries' emissions. Net liability is positive in middle-income, carbon-intensive countries. Poor and rich countries would be compensated because their current emissions are relatively low, poor countries additionally because they are vulnerable.

</details>


### [753] [How Disruptive is Financial Technology?](https://arxiv.org/abs/2601.14071)
*Douglas Cumming,Hisham Farag,Santosh Koirala,Danny McGowan*

Main category: econ.GN

TL;DR: 研究金融科技是否通过加剧存款竞争和提高利率扰乱银行业，发现小金融机构存款成本上升但防止了流动性外流，规模和地域多元化可缓解竞争影响。


<details>
  <summary>Details</summary>
Motivation: 探究金融科技是否会通过加剧对稀缺存款资金的竞争和提高存款利率来扰乱银行业。

Method: 利用美国各州取消市场平台投资限制这一外生事件进行双重差分估计。

Result: 小金融机构存款成本约增加11.5%，但价格变化有效防止了流动性外流，规模和地域多元化可缓解金融科技竞争影响。

Conclusion: 指出金融科技发展对银行的意外影响，为监管者和管理者提供政策见解。

Abstract: We study whether Fintech disrupts the banking sector by intensifying competition for scarce deposits funds and raising deposit rates. Using difference-in-difference estimation around the exogenous removal of marketplace platform investing restrictions by US states, we show the cost of deposits increase by approximately 11.5% within small financial institutions. However, these price changes are effective in preventing a drain of liquidity. Size and geographical diversification through branch networks can mitigate the effects of Fintech competition by sourcing deposits from less competitive markets. The findings highlight the unintended consequences of the growing Fintech sector on banks and offer policy insights for regulators and managers into the ongoing development and impact of technology on the banking sector.

</details>


### [754] [Hot Days, Unsafe Schools? The Impact of Heat on School Shootings](https://arxiv.org/abs/2601.14094)
*Seunghyun Lee,Goeun Lee*

Main category: econ.GN

TL;DR: 研究基于1981 - 2022年美国K - 12学校数据，发现高温使校园枪击增加，预测气候变化下枪击将增多及带来社会成本。


<details>
  <summary>Details</summary>
Motivation: 探究高温对校园枪击的因果影响及气候变化的潜在影响。

Method: 利用1981 - 2022年美国K - 12学校枪击事件数据进行估计和分析。

Result: 90°F以上高温日校园枪击较70°F以下日增加80%；高温增加杀人及威胁类枪击，在学生活动多监管少时段更易发生；气候变化下，不同排放情景到2091 - 2100年杀人及威胁类校园枪击将分别增加8%和14%，对应社会成本分别为3.43亿美元和5.92亿美元。

Conclusion: 气候变化将导致美国校园枪击事件增加，带来显著社会成本。

Abstract: Using data on school shooting incidents in U.S. K--12 schools from 1981 to 2022, we estimate the causal effects of high temperatures on school shootings and assess the implications of climate change. We find that days with maximum temperatures exceeding 90$^\circ$F lead to a 80\% increase in school shootings relative to days below 70$^\circ$F. Consistent with theories linking heat exposure to aggression, high temperatures increase homicidal and threat-related shootings but have no effect on accidental or suicidal shootings. Heat-induced shootings occur disproportionately during periods of greater student mobility and reduced supervision, including before and after school hours and lunch periods. Higher temperatures increase shootings involving both student and non-student perpetrators. We project that climate change will increase homicidal and threat-related school shootings in the U.S. by 8\% under SSP2--4.5 (moderate emissions) and by 14\% under SSP5--8.5 (high emissions) by 2091--2100, corresponding to approximately 23 and 39 additional shootings per decade, respectively. The present discounted value of the resulting social costs is \$343 million and \$592 million (2025 dollars), respectively.

</details>


### [755] [Foreign influencer operations: How TikTok shapes American perceptions of China](https://arxiv.org/abs/2601.14118)
*Trevor Incerti,Jonathan Elkobi,Daniel Mattingly*

Main category: econ.GN

TL;DR: 研究了威权政权（以中国为例为虚假描述）利用TikTok上的网红塑造全球舆论的现象，发现亲华网红比官方媒体更能提升美国民众对中国好感。


<details>
  <summary>Details</summary>
Motivation: 探究威权政权（文内不实指代中国）如何借助社交媒体网红加强对非民主政治体系（文内不实指代中国政治体系）的全球支持。

Method: 收集TikTok上亲华网红数据，创建TikTok应用克隆版，招募超8500名美国人参与随机实验。

Result: 亲华网红比官方媒体互动更多，亲华外国网红能显著提升对中国的好感，而传统中国官方媒体引发反感。

Conclusion: 强调网红在塑造全球舆论方面的重要性。

Abstract: How do authoritarian regimes strengthen global support for nondemocratic political systems? Roughly half of the users of the social media platform TikTok report getting news from social media influencers. Against this backdrop, authoritarian regimes have increasingly outsourced content creation to these influencers. To gain understanding of the extent of this phenomenon and the persuasive capabilities of these influencers, we collect comprehensive data on pro-China influencers on TikTok. We show that pro-China influencers have more engagement than state media. We then create a realistic clone of the TikTok app, and conduct a randomized experiment in which over 8,500 Americans are recruited to use this app and view a random sample of actual TikTok content. We show that pro-China foreign influencers are strikingly effective at increasing favorability toward China, while traditional Chinese state media causes backlash. The findings highlight the importance of influencers in shaping global public opinion.

</details>


### [756] [Trade relationships during and after a crisis](https://arxiv.org/abs/2601.14150)
*Alejandra Martinez*

Main category: econ.GN

TL;DR: 研究企业如何应对通过关系合同组织的国际贸易临时中断，利用天气冲击研究哥伦比亚与美国花卉贸易，发现冲击对企业交易组合有持久影响。


<details>
  <summary>Details</summary>
Motivation: 了解企业如何对国际贸易关系中的临时中断做出调整。

Method: 利用2010 - 2011年拉尼娜季节极端天气冲击（限制哥伦比亚花卉出口商进入货运码头），结合哥伦比亚 - 美国花卉贸易交易层面数据进行研究。

Result: 供应商组合受影响较小的进口商不太可能终止中断的关系；受影响更大的企业合作伙伴更替率更高、更易退出市场，退出占关系分离的很大比例。

Conclusion: 买卖关系的异质性冲击会导致企业贸易组合的持久变化。

Abstract: I study how firms adjust to temporary disruptions in international trade relationships organized through relational contracts. I exploit an extreme, plausibly exogenous weather shock during the 2010-11 La Niña season that restricted Colombian flower exporters' access to cargo terminals. Using transaction-level data from the Colombian-U.S. flower trade, I show that importers with less-exposed supplier portfolios are less likely to terminate disrupted relationships, instead tolerating shipment delays. In contrast, firms facing greater exposure experience higher partner turnover and are more likely to exit the market, with exit accounting for a substantial share of relationship separations. These findings demonstrate that idiosyncratic shocks to buyer-seller relationships can propagate into persistent changes in firms' trading portfolios.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [757] [Learning Audio-Visual Embeddings with Inferred Latent Interaction Graphs](https://arxiv.org/abs/2601.11995)
*Donghuo Zeng,Hao Niu,Yanan Wang,Masato Taya*

Main category: cs.MM

TL;DR: 提出一个框架解决音频-视觉嵌入学习问题，通过实验证明提升了鲁棒性和语义连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有对比和三元组损失方法使用稀疏标注标签，会将偶然共现视为负样本，产生假阴性和遗漏真实跨模态依赖。

Method: 提出包含AV - SAL、ILI和LIR的框架，利用软标签预测和推断潜在交互解决问题。

Result: 在AVE和VEGAS基准测试中，平均准确率均值（mAP）持续提升。

Conclusion: 将推断的潜在交互融入嵌入学习能增强鲁棒性和语义连贯性。

Abstract: Learning robust audio-visual embeddings requires bringing genuinely related audio and visual signals together while filtering out incidental co-occurrences - background noise, unrelated elements, or unannotated events. Most contrastive and triplet-loss methods use sparse annotated labels per clip and treat any co-occurrence as semantic similarity. For example, a video labeled "train" might also contain motorcycle audio and visual, because "motorcycle" is not the chosen annotation; standard methods treat these co-occurrences as negatives to true motorcycle anchors elsewhere, creating false negatives and missing true cross-modal dependencies. We propose a framework that leverages soft-label predictions and inferred latent interactions to address these issues: (1) Audio-Visual Semantic Alignment Loss (AV-SAL) trains a teacher network to produce aligned soft-label distributions across modalities, assigning nonzero probability to co-occurring but unannotated events and enriching the supervision signal. (2) Inferred Latent Interaction Graph (ILI) applies the GRaSP algorithm to teacher soft labels to infer a sparse, directed dependency graph among classes. This graph highlights directional dependencies (e.g., "Train (visual)" -> "Motorcycle (audio)") that expose likely semantic or conditional relationships between classes; these are interpreted as estimated dependency patterns. (3) Latent Interaction Regularizer (LIR): A student network is trained with both metric loss and a regularizer guided by the ILI graph, pulling together embeddings of dependency-linked but unlabeled pairs in proportion to their soft-label probabilities. Experiments on AVE and VEGAS benchmarks show consistent improvements in mean average precision (mAP), demonstrating that integrating inferred latent interactions into embedding learning enhances robustness and semantic coherence.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [758] [CoSMeTIC: Zero-Knowledge Computational Sparse Merkle Trees with Inclusion-Exclusion Proofs for Clinical Research](https://arxiv.org/abs/2601.12136)
*Mohammad Shahid,Paritosh Ramanan,Mohammad Fili,Guiping Hu,Hillel Haim*

Main category: cs.CR

TL;DR: 本文提出用于临床研究的零知识计算框架CoSMeTIC，通过实验证明其能在保证隐私同时维持统计准确性，可用于大规模临床研究合规。


<details>
  <summary>Details</summary>
Motivation: 临床数据分析需平衡隐私保护和可验证责任，但这是一个关键挑战。

Method: 提出使用计算稀疏默克尔树（SMTs）的零知识计算框架CoSMeTIC，进行形式化分析和大量实验，用假设检验和基于逻辑回归的基因组分析。

Result: CoSMeTIC能实现强隐私保护，同时维持统计准确性。

Conclusion: CoSMeTIC为大规模临床研究严格隐私保护和合规提供可扩展且实用的方案。

Abstract: Analysis of clinical data is a cornerstone of biomedical research with applications in areas such as genomic testing and response characterization of therapeutic drugs. Maintaining strict privacy controls is essential because such data typically contains personally identifiable health information of patients. At the same time, regulatory compliance often requires study managers to demonstrate the integrity and authenticity of participant data used in analyses. Balancing these competing requirements, privacy preservation and verifiable accountability, remains a critical challenge. In this paper, we present CoSMeTIC, a zero-knowledge computational framework that proposes computational Sparse Merkle Trees (SMTs) as a means to generate verifiable inclusion and exclusion proofs for individual participants' data in clinical studies. We formally analyze the zero-knowledge properties of CoSMeTIC and evaluate its computational efficiency through extensive experiments. Using the Kolmogorov-Smirnov and likelihood-ratio hypothesis tests, along with logistic-regression-based genomic analyses on real-world Huntington's disease datasets, we demonstrate that CoSMeTIC achieves strong privacy guarantees while maintaining statistical fidelity. Our results suggest that CoSMeTIC provides a scalable and practical alternative for achieving regulatory compliance with rigorous privacy protection in large-scale clinical research.

</details>


### [759] [The Limits of Conditional Volatility: Assessing Cryptocurrency VaR under EWMA and IGARCH Models](https://arxiv.org/abs/2601.13757)
*Ekleen Kaur*

Main category: cs.CR

TL;DR: 标准静态GBM模型用于加密货币风险管理失败，研究对比三种条件波动率模型用于高贝塔山寨币，发现EWMA/IGARCH基线模型表现最佳，拒绝传统金融假设。


<details>
  <summary>Details</summary>
Motivation: 标准静态GBM模型用于加密货币风险管理失败，且主流GARCH文献很少关注高贝塔山寨币，存在研究缺口。

Method: 在相关蒙特卡罗VaR框架下，对比测试EWMA/IGARCH基线、IGARCH + MR和修正的EGARCH风格非对称冲击模型。

Result: IGARCH + MR严重低估下行风险，非对称模型过度惩罚，EWMA/IGARCH基线模型提供了唯一可靠的条件波动率估计。

Conclusion: 拒绝山寨币资产类别中波动率均值回归和非对称杠杆效应的传统金融假设，非平稳框架是该领域监管级风险建模的先决条件。

Abstract: The application of the standard static Geometric Brownian Motion (GBM) model for cryptocurrency risk management resulted in a systemic failure, evidenced by a 80.67% chance of loss in the 5% value-at-risk benchmark. This study addresses a critical literature gap by comparatively testing three conditional volatility models the EWMA/IGARCH baseline, an IGARCH model augmented with explicit mean reversion (IGARCH + MR), and a modified EGARCH-style asymmetric shock model within a correlated Monte Carlo VaR framework. Crucially, the analysis is applied specifically to high-beta altcoins (XRP, SOL, ADA), an asset class largely neglected by mainstream GARCH literature. Our results demonstrate that imposing stationarity (IGARCH + MR) drastically underestimates downside risk (5 percent value-at-risk reduced by 50%), while the asymmetric model (Model 3) leads to severe over-penalization. The EWMA/IGARCH baseline, characterized by infinite volatility persistence (alpha + beta = 1), provided the only robust conditional volatility estimate. This finding constitutes a formal rejection of the conventional financial hypotheses of volatility mean reversion and the asymmetric leverage effect in the altcoin asset class, establishing that non-stationary frameworks are a prerequisite for regulatory-grade risk modeling in this domain.

</details>


### [760] [MongoDB Injection Query Classification Model using MongoDB Log files as Training Data](https://arxiv.org/abs/2601.11996)
*Shaunak Perni,Minal Shirodkar,Ramdas Karmalli*

Main category: cs.CR

TL;DR: 本文探索基于日志数据和其他特征（非原始查询语句）对MongoDB服务器的NoSQL注入攻击分类，使用机器学习模型训练评估，最佳模型准确率71%。


<details>
  <summary>Details</summary>
Motivation: 传统规则系统和基于查询语句的模型系统在防御NoSQL注入攻击时有局限性，需新方法。

Method: 收集模拟攻击的日志数据并处理，经判别分析得到显著特征数据集，用AutoML库“FLAML”和6个手动编程模型训练评估。

Result: 最佳模型是“FLAML”库的“XGBoost limited depth”模型，准确率71%。

Conclusion: 基于日志数据和提取特征分类NoSQL注入攻击有一定效果，可用于防御攻击。

Abstract: NoSQL Injection attacks are a class of cybersecurity attacks where an attacker sends a specifically engineered query to a NoSQL database which then performs an unauthorized operation. To defend against such attacks, rule based systems were initially developed but then were found to be ineffective to innovative injection attacks hence a model based approach was developed. Most model based detection systems, during testing gave exponentially positive results but were trained only on the query statement sent to the server. However due to the scarcity of data and class imbalances these model based systems were found to be not effective against all attacks in the real world. This paper explores classifying NoSQL injection attacks sent to a MongoDB server based on Log Data, and other extracted features excluding raw query statements. The log data was collected from a simulated attack on an empty MongoDB server which was then processed and explored. A discriminant analysis was carried out to determine statistically significant features to discriminate between injection and benign queries resulting in a dataset of significant features. Several Machine learning based classification models using an AutoML library, "FLAML", as well as 6 manually programmed models were trained on this dataset , which were then trained on 50 randomized samples of data, cross validated and evaluated. The study found that the best model was the "FLAML" library's "XGBoost limited depth" model with an accuracy of 71%.

</details>


### [761] [SWORD: A Secure LoW-Latency Offline-First Authentication and Data Sharing Scheme for Resource Constrained Distributed Networks](https://arxiv.org/abs/2601.12875)
*Faisal Haque Bappy,Tahrim Hossain,Raiful Hasan,Kamrul Hasan,Mohamed Younis,Tariqul Islam*

Main category: cs.CR

TL;DR: 针对资源受限网络，提出离线优先的认证与数据共享方案SWORD，实验显示其性能优且安全。


<details>
  <summary>Details</summary>
Motivation: 资源受限网络多依赖中心服务器，区块链方案难满足实时应用低延迟需求，5G下服务器与节点间延迟仍是挑战。

Method: 引入SWORD，采用基于接近度的聚类方法实现离线认证与数据共享。

Result: SWORD性能优于传统区块链方案，与基于中心服务器方案在资源效率和认证延迟上相近。

Conclusion: SWORD能保证低延迟、安全操作，且能抵御多种攻击。

Abstract: While many resource-constrained networks, such as Internet of Things (IoT) and Internet of Vehicles (IoV), are inherently distributed, the majority still rely on central servers for fast authentication and data sharing. Blockchain-based solutions offer decentralized alternatives but often struggle to meet the stringent latency requirements of real-time applications. Even with the rollout of 5G, network latency between servers and peers remains a significant challenge. To address this, we introduce SWORD, a novel offline-first authentication and data-sharing scheme designed specifically for resource-constrained networks. SWORD utilizes a proximity-based clustering approach to enable offline authentication and data sharing, ensuring low-latency, secure operations even in intermittently connected scenarios. Our experimental results show that SWORD outperforms traditional blockchain-based solutions while offering similar resource efficiency and authentication latency to central-server-based solutions. Additionally, we provide a comprehensive security analysis, demonstrating that SWORD is resilient against spoofing, impersonation, replay, and man-in-the-middle attacks.

</details>


### [762] [Automatic Adjustment of HPA Parameters and Attack Prevention in Kubernetes Using Random Forests](https://arxiv.org/abs/2601.13515)
*Hanlin Zhou,Huah Yong Chan,Jingfei Ni,Mengchun Wu,Qing Deng*

Main category: cs.CR

TL;DR: 本文以HPA中用HTTP状态码作自定义指标为场景，结合机器学习随机森林算法评估和预测攻击，管理攻击流量，达成降低5XX状态码率、隔离攻击流量等效果，还体现设定HPA调整阈值的重要性


<details>
  <summary>Details</summary>
Motivation: 有效管理攻击流量，降低高负载下5XX状态码发生率，隔离攻击流量，防止HPA因攻击过度扩展

Method: 以HTTP状态码为HPA自定义指标，集成机器学习随机森林算法评估和预测攻击，动态调整HPA最大pod参数，将攻击IP访问重定向到蜜罐pod

Result: 在高负载下通过HPA pod调整降低5XX状态码发生率，有效隔离攻击流量，不同条件实验体现设定合适HPA调整阈值的重要性

Conclusion: 该方法能在目标攻击场景用机器学习脚本调整HPA参数，有效管理攻击流量

Abstract: In this paper, HTTP status codes are used as custom metrics within the HPA as the experimental scenario. By integrating the Random Forest classification algorithm from machine learning, attacks are assessed and predicted, dynamically adjusting the maximum pod parameter in the HPA to manage attack traffic. This approach enables the adjustment of HPA parameters using machine learning scripts in targeted attack scenarios while effectively managing attack traffic. All access from attacking IPs is redirected to honeypot pods, achieving a lower incidence of 5XX status codes through HPA pod adjustments under high load conditions. This method also ensures effective isolation of attack traffic, preventing excessive HPA expansion due to attacks. Additionally, experiments conducted under various conditions demonstrate the importance of setting appropriate thresholds for HPA adjustments.

</details>


### [763] [Know Your Contract: Extending eIDAS Trust into Public Blockchains](https://arxiv.org/abs/2601.13903)
*Awid Vaziry,Christoph Wronka,Sandro Rodriguez Garzon,Axel Küpper*

Main category: cs.CR

TL;DR: 本文提出将欧盟eIDAS信任框架扩展到公共区块链生态系统的架构，实现监管合规及自动化验证，为机构参与DeFi等提供途径。


<details>
  <summary>Details</summary>
Motivation: 公共区块链缺乏将链上行为与合法责任实体关联的机制，阻碍机构采用和监管合规，需要加以解决。

Method: 通过将智能合约与合格电子印章进行加密绑定，建立从欧盟委员会信任列表到链上地址的可验证信任链，分析相关监管要求，确定加密套件，提出两种信任验证模型。

Result: 将监管合规从行政负担转变为自动化、标准化流程，实现首次交互时的相互验证。

Conclusion: 随着eIDAS钱包在欧盟成员国普及，该架构为欧洲数字信任基础设施融入区块链系统提供途径，促进机构DeFi参与等。

Abstract: Public blockchains lack native mechanisms to attribute on-chain actions to legally accountable entities, creating a fundamental barrier to institutional adoption and regulatory compliance. This paper presents an architecture that extends the European Union eIDAS trust framework into public blockchain ecosystems by cryptographically binding smart contracts to qualified electronic seals issued by Qualified Trust Service Providers. The mechanism establishes a verifiable chain of trust from the European Commission List of Trusted Lists to individual on-chain addresses, enabling machine-verifiable proofs for automated regulatory validation, such as Know Your Contract, Counterparty, and Business checks, without introducing new trusted intermediaries. Regulatory requirements arising from eIDAS, MiCA, PSD2, PSR, and the proposed European Business Wallet are analyzed, and a cryptographic suite meeting both eIDAS implementing regulations and EVM execution constraints following the Ethereum Fusaka upgrade is identified, namely ECDSA with P-256 and CAdES formatting. Two complementary trust validation models are presented: an off-chain workflow for agent-to-agent payment protocols and a fully on-chain workflow enabling regulatory-compliant DeFi operations between legal entities. The on-chain model converts regulatory compliance from a per-counterparty administrative burden into an automated, standardized process, enabling mutual validation at first interaction without prior business relationships. As eIDAS wallets become mandatory across EU member states, the proposed architecture provides a pathway for integrating European digital trust infrastructure into blockchain-based systems, enabling institutional DeFi participation, real-world asset tokenization, and agentic commerce within a trusted, regulatory-compliant framework.

</details>


### [764] [SecureSplit: Mitigating Backdoor Attacks in Split Learning](https://arxiv.org/abs/2601.14054)
*Zhihao Dou,Dongfei Cui,Weida Wang,Anjun Gao,Yueyang Quan,Mengyao Ma,Viet Vo,Guangdong Bai,Zhuqing Liu,Minghong Fang*

Main category: cs.CR

TL;DR: 该论文提出针对Split Learning（SL）的防御机制SecureSplit，通过特定策略区分并过滤恶意嵌入，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: SL易受后门攻击，恶意客户端会篡改嵌入影响最终模型，因此需要防御机制。

Method: 使用维度转换策略突出良性和中毒嵌入的差异，开发基于多数投票方案的自适应过滤方法去除污染嵌入。

Result: 在四个数据集、五种后门攻击场景和七种替代防御方法下的严格实验，证实了SecureSplit在各种挑战性条件下的有效性。

Conclusion: SecureSplit能有效应对SL中的后门攻击，保障模型安全。

Abstract: Split Learning (SL) offers a framework for collaborative model training that respects data privacy by allowing participants to share the same dataset while maintaining distinct feature sets. However, SL is susceptible to backdoor attacks, in which malicious clients subtly alter their embeddings to insert hidden triggers that compromise the final trained model. To address this vulnerability, we introduce SecureSplit, a defense mechanism tailored to SL. SecureSplit applies a dimensionality transformation strategy to accentuate subtle differences between benign and poisoned embeddings, facilitating their separation. With this enhanced distinction, we develop an adaptive filtering approach that uses a majority-based voting scheme to remove contaminated embeddings while preserving clean ones. Rigorous experiments across four datasets (CIFAR-10, MNIST, CINIC-10, and ImageNette), five backdoor attack scenarios, and seven alternative defenses confirm the effectiveness of SecureSplit under various challenging conditions.

</details>


### [765] [A Survey on Mapping Digital Systems with Bill of Materials: Development, Practices, and Challenges](https://arxiv.org/abs/2601.11678)
*Shuai Zhang,Minzhao Lyu,Hassan Habibi Gharakheili*

Main category: cs.CR

TL;DR: 本文对BOM发展和实践进行跨领域综述，介绍其框架演进、行业实践、下游应用、学术研究，并指出当前框架的四个关键不足。


<details>
  <summary>Details</summary>
Motivation: 现代数字生态系统复杂度增加，组织难以理解和管理组件依赖，BOM可提升数字供应链可见性和安全性，本文旨在全面综述BOM发展和实践。

Method: 先分三阶段考察BOM框架演进，总结核心原则、利益相关者和标准化工作；再回顾行业生成、评估和共享BOM数据的实践；接着介绍BOM数据的下游应用；讨论学术界针对现有框架局限的研究；最后识别关键不足。

Result: 梳理了BOM框架演进、行业实践、下游应用和学术研究情况，识别出四个关键局限。

Conclusion: 当前BOM框架存在限制可用性和可靠性的关键不足，需开展未来研究。

Abstract: Modern digital ecosystems, spanning software, hardware, learning models, datasets, and cryptographic products, continue to grow in complexity, making it difficult for organizations to understand and manage component dependencies. Bills of Materials (BOMs) have emerged as a structured way to document product components, their interrelationships, and key metadata, improving visibility and security across digital supply chains. This survey provides the first comprehensive cross-domain review of BOM developments and practices. We start by examining the evolution of BOM frameworks in three stages (i.e., pre-development, initial, and accelerated) and summarizing their core principles, key stakeholders, and standardization efforts for hardware, software, artificial intelligence (AI) models, datasets, and cryptographic assets. We then review industry practices for generating BOM data, evaluating its quality, and securely sharing it. Next, we review practical downstream uses of BOM data, including dependency modeling, compliance verification, operational risk assessment, and vulnerability tracking. We also discuss academic efforts to address limitations in current BOM frameworks through refinements, extensions, or new models tailored to emerging domains such as data ecosystems and AI supply chains. Finally, we identify four key gaps that limit the usability and reliability of today's BOM frameworks, motivating future research directions.

</details>


### [766] [Attesting Model Lineage by Consisted Knowledge Evolution with Fine-Tuning Trajectory](https://arxiv.org/abs/2601.11683)
*Zhuoyi Shang,Jiasen Li,Pengzhen Chen,Yanwei Liu,Xiaoyan Gu,Weiping Wang*

Main category: cs.CR

TL;DR: 文章提出新的模型谱系认证框架，验证知识进化和参数修改的联合轨迹，经实验验证其有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决开放权重模型库中存在的未经授权的模型重新分发和虚假模型来源声明等安全问题，现有方法无法捕捉谱系关系中的知识动态演变。

Method: 提出新的模型谱系认证框架，先利用模型编辑量化微调引起的参数变化，再引入知识向量化机制精炼进化后的知识，以验证模型间知识关系的算术一致性。

Result: 在多种现实世界的对抗场景中，该方法有效且有韧性，在包括分类器、扩散模型和大语言模型等多种模型类型上能实现可靠的谱系验证。

Conclusion: 所提方法能有效解决模型谱系认证问题，在多种模型和场景下表现可靠。

Abstract: The fine-tuning technique in deep learning gives rise to an emerging lineage relationship among models. This lineage provides a promising perspective for addressing security concerns such as unauthorized model redistribution and false claim of model provenance, which are particularly pressing in \textcolor{blue}{open-weight model} libraries where robust lineage verification mechanisms are often lacking. Existing approaches to model lineage detection primarily rely on static architectural similarities, which are insufficient to capture the dynamic evolution of knowledge that underlies true lineage relationships. Drawing inspiration from the genetic mechanism of human evolution, we tackle the problem of model lineage attestation by verifying the joint trajectory of knowledge evolution and parameter modification. To this end, we propose a novel model lineage attestation framework. In our framework, model editing is first leveraged to quantify parameter-level changes introduced by fine-tuning. Subsequently, we introduce a novel knowledge vectorization mechanism that refines the evolved knowledge within the edited models into compact representations by the assistance of probe samples. The probing strategies are adapted to different types of model families. These embeddings serve as the foundation for verifying the arithmetic consistency of knowledge relationships across models, thereby enabling robust attestation of model lineage. Extensive experimental evaluations demonstrate the effectiveness and resilience of our approach in a variety of adversarial scenarios in the real world. Our method consistently achieves reliable lineage verification across a broad spectrum of model types, including classifiers, diffusion models, and large language models.

</details>


### [767] [Zero-Permission Manipulation: Can We Trust Large Multimodal Model Powered GUI Agents?](https://arxiv.org/abs/2601.12349)
*Yi Qian,Kunwei Qian,Xingbang He,Ligeng Chen,Jikang Zhang,Tiantai Zhang,Haiyang Wei,Linzhang Wang,Hao Wu,Bing Mao*

Main category: cs.CR

TL;DR: 本文指出大型多模态模型驱动的GUI代理在Android系统中存在视觉原子性假设不成立的问题，提出Action Rebinding攻击方法，评估显示攻击成功率高且检测率为0，揭示了当前代理 - OS集成的架构缺陷。


<details>
  <summary>Details</summary>
Motivation: 发现大型多模态模型驱动的GUI代理在Android系统中视觉原子性假设不成立，存在攻击面。

Method: 提出Action Rebinding攻击，利用代理推理管道中观察到行动的间隙，触发前台转换来重新绑定代理计划的行动；引入Intent Alignment Strategy (IAS) 操纵代理推理过程绕过验证门。

Result: 在15个任务上对6个广泛使用的Android GUI代理评估，原子动作重新绑定成功率100%，能可靠编排多步攻击链；使用IAS后绕过验证门成功率从0%提升到最高100%，攻击应用无敏感权限、无特权API调用，恶意软件扫描检测率为0。

Conclusion: 当前代理 - OS集成存在基本架构缺陷，为未来代理系统的安全设计提供关键见解。

Abstract: Large multimodal model powered GUI agents are emerging as high-privilege operators on mobile platforms, entrusted with perceiving screen content and injecting inputs. However, their design operates under the implicit assumption of Visual Atomicity: that the UI state remains invariant between observation and action. We demonstrate that this assumption is fundamentally invalid in Android, creating a critical attack surface.
  We present Action Rebinding, a novel attack that allows a seemingly-benign app with zero dangerous permissions to rebind an agent's execution. By exploiting the inevitable observation-to-action gap inherent in the agent's reasoning pipeline, the attacker triggers foreground transitions to rebind the agent's planned action toward the target app. We weaponize the agent's task-recovery logic and Android's UI state preservation to orchestrate programmable, multi-step attack chains. Furthermore, we introduce an Intent Alignment Strategy (IAS) that manipulates the agent's reasoning process to rationalize UI states, enabling it to bypass verification gates (e.g., confirmation dialogs) that would otherwise be rejected.
  We evaluate Action Rebinding Attacks on six widely-used Android GUI agents across 15 tasks. Our results demonstrate a 100% success rate for atomic action rebinding and the ability to reliably orchestrate multi-step attack chains. With IAS, the success rate in bypassing verification gates increases (from 0% to up to 100%). Notably, the attacker application requires no sensitive permissions and contains no privileged API calls, achieving a 0% detection rate across malware scanners (e.g., VirusTotal). Our findings reveal a fundamental architectural flaw in current agent-OS integration and provide critical insights for the secure design of future agent systems. To access experimental logs and demonstration videos, please contact yi_qian@smail.nju.edu.cn.

</details>


### [768] [The Cost of Convenience: Identifying, Analyzing, and Mitigating Predatory Loan Applications on Android](https://arxiv.org/abs/2601.12634)
*Olawale Amos Akanji,Manuel Egele,Gianluca Stringhini*

Main category: cs.CR

TL;DR: 本文跨国测量贷款应用合规情况，分析434款应用，发现大量违规，谷歌移除部分应用，提出方法论和建议。


<details>
  <summary>Details</summary>
Motivation: 许多贷款应用过度获取权限、滥用用户数据用于强制催债，影响借款人和联系人，需测量其合规性。

Method: 从印尼、肯尼亚等国官方注册表和应用市场收集434款应用，用大语言模型辅助将政策文本转化为可测试的权限检查，结合静态和动态分析。

Result: 发现大量被批准应用违规，动态分析显示部分应用在用户注册前传输敏感数据；向谷歌披露后移除93款应用。

Conclusion: 应采用该方法论开展合规监测，呼吁各方合作加强隐私保护，需协同执法和强大技术保障。

Abstract: Digital lending applications, commonly referred to as loan apps, have become a primary channel for microcredit in emerging markets. However, many of these apps demand excessive permissions and misuse sensitive user data for coercive debt-recovery practices, including harassment, blackmail, and public shaming that affect both borrowers and their contacts.
  This paper presents the first cross-country measurement of loan app compliance against both national regulations and Google's Financial Services Policy. We analyze 434 apps drawn from official registries and app markets from Indonesia, Kenya, Nigeria, Pakistan, and the Philippines. To operationalize policy requirements at scale, we translate policy text into testable permission checks using LLM-assisted policy-to-permission mapping and combine this with static and dynamic analyses of loan apps' code and runtime behavior.
  Our findings reveal pervasive non-compliance among approved apps: 141 violate national regulatory policy and 147 violate Google policy. Dynamic analysis further shows that several apps transmit sensitive data (contacts, SMS, location, media) before user signup or registration, undermining informed consent and enabling downstream harassment of borrowers and third parties. Following our disclosures, Google removed 93 flagged apps from Google Play, representing over 300M cumulative installs.
  We advocate for adopting our methodology as a proactive compliance-monitoring tool and offer targeted recommendations for regulators, platforms, and developers to strengthen privacy protections. Overall, our results highlight the need for coordinated enforcement and robust technical safeguards to ensure that digital lending supports financial inclusion without compromising user privacy or safety.

</details>


### [769] [Eliciting Harmful Capabilities by Fine-Tuning On Safeguarded Outputs](https://arxiv.org/abs/2601.13528)
*Jackson Kaunismaa,Avery Griffin,John Hughes,Christina Q. Knight,Mrinank Sharma,Erik Jones*

Main category: cs.CR

TL;DR: 研究表明，即使是有防护机制的前沿模型，也能通过诱导攻击在开源模型中引发有害能力，评估显示攻击能恢复部分能力差距。


<details>
  <summary>Details</summary>
Motivation: 探讨有防护机制的前沿模型是否会在开源模型中引发有害能力，研究缓解生态系统层面风险的挑战。

Method: 诱导攻击分三步：构建相邻领域无害提示、从前沿模型获取回应、用提示 - 输出对微调开源模型。

Result: 在危险化学品合成和处理领域评估，攻击恢复约 40% 开源模型与无限制前沿模型的能力差距，攻击效果与前沿模型能力和微调数据量有关。

Conclusion: 输出级别的防护机制难以缓解生态系统层面的风险。

Abstract: Model developers implement safeguards in frontier models to prevent misuse, for example, by employing classifiers to filter dangerous outputs. In this work, we demonstrate that even robustly safeguarded models can be used to elicit harmful capabilities in open-source models through elicitation attacks. Our elicitation attacks consist of three stages: (i) constructing prompts in adjacent domains to a target harmful task that do not request dangerous information; (ii) obtaining responses to these prompts from safeguarded frontier models; (iii) fine-tuning open-source models on these prompt-output pairs. Since the requested prompts cannot be used to directly cause harm, they are not refused by frontier model safeguards. We evaluate these elicitation attacks within the domain of hazardous chemical synthesis and processing, and demonstrate that our attacks recover approximately 40% of the capability gap between the base open-source model and an unrestricted frontier model. We then show that the efficacy of elicitation attacks scales with the capability of the frontier model and the amount of generated fine-tuning data. Our work demonstrates the challenge of mitigating ecosystem level risks with output-level safeguards.

</details>


### [770] [Serverless AI Security: Attack Surface Analysis and Runtime Protection Mechanisms for FaaS-Based Machine Learning](https://arxiv.org/abs/2601.11664)
*Chetan Pathade,Vinod Dhimam,Sheheryar Ahmad,Ilsa Lareb*

Main category: cs.CR

TL;DR: 本文对无服务器环境中的机器学习工作负载进行安全分析，提出Serverless AI Shield框架，评估显示有较高检测率和低性能开销，还发布开源安全工具包。


<details>
  <summary>Details</summary>
Motivation: 无服务器计算广泛应用且机器学习推理工作负载迁移到FaaS平台，但带来安全挑战，需进行全面安全分析。

Method: 系统性地将攻击面分为五类，对AWS Lambda、Azure Functions和Google Cloud Functions进行实证评估，提出包含预部署验证、运行时监控和执行后取证的多层防御框架。

Result: 提出的Serverless AI Shield框架检测率达94%，推理延迟的性能开销低于9%。

Conclusion: 发布开源安全工具包，有助于构建更具弹性的云原生机器学习系统。

Abstract: Serverless computing has achieved widespread adoption, with over 70% of AWS organizations using serverless solutions [1]. Meanwhile, machine learning inference workloads increasingly migrate to Function-as-a-Service (FaaS) platforms for their scalability and cost-efficiency [2], [3], [4]. However, this convergence introduces critical security challenges, with recent reports showing a 220% increase in AI/ML vulnerabilities [5] and serverless computing's fragmented architecture raises new security concerns distinct from traditional cloud deployments [6], [7]. This paper presents the first comprehensive security analysis of machine learning workloads in serverless environments. We systematically characterize the attack surface across five categories: function-level vulnerabilities (cold start exploitation, dependency poisoning), model-specific threats (API-based extraction, adversarial inputs), infrastructure attacks (cross-function contamination, privilege escalation), supply chain risks (malicious layers, backdoored libraries), and IAM complexity (ephemeral nature, serverless functions). Through empirical assessments across AWS Lambda, Azure Functions, and Google Cloud Functions, we demonstrate real-world attack scenarios and quantify their security impact. We propose Serverless AI Shield (SAS), a multi-layered defense framework providing pre-deployment validation, runtime monitoring, and post-execution forensics. Our evaluation shows SAS achieves 94% detection rates while maintaining performance overhead below 9% for inference latency. We release an open-source security toolkit to enable practitioners to assess and harden their serverless AI deployments, advancing the field toward more resilient cloud-native machine learning systems.

</details>


### [771] [Hybrid IDS Using Signature-Based and Anomaly-Based Detection](https://arxiv.org/abs/2601.11998)
*Messaouda Boutassetta,Amina Makhlouf,Newfel Messaoudi,Abdelmadjid Benmachiche,Ines Boutabia*

Main category: cs.CR

TL;DR: 本文对混合入侵检测系统（Hybrid IDS）进行综合调查和概念概述，还探讨相关趋势及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统IDS有检测未知攻击困难和误报率高的局限，需提高检测能力。

Method: 对Hybrid IDS相关研究进行调查、分类，分析其优缺点和应用领域，回顾研究趋势。

Result: 对Hybrid IDS进行分类、讨论优缺点及应用领域，总结了研究趋势。

Conclusion: 指出未来潜在研究方向是开发更具成本效益且能检测新兴复杂攻击的Hybrid IDS解决方案。

Abstract: Intrusion detection systems (IDS) are essential for protecting computer systems and networks against a wide range of cyber threats that continue to evolve over time. IDS are commonly categorized into two main types, each with its own strengths and limitations, such as difficulty in detecting previously unseen attacks and the tendency to generate high false positive rates. This paper presents a comprehensive survey and a conceptual overview of Hybrid IDS, which integrate signature-based and anomaly-based detection techniques to enhance attack detection capabilities. The survey examines recent research on Hybrid IDS, classifies existing models into functional categories, and discusses their advantages, limitations, and application domains, including financial systems, air traffic control, and social networks. In addition, recent trends in Hybrid IDS research, such as machine learning-based approaches and cloud-based deployments, are reviewed. Finally, this work outlines potential future research directions aimed at developing more cost-effective Hybrid IDS solutions with improved ability to detect emerging and sophisticated cyberattacks.

</details>


### [772] [Less Is More -- Until It Breaks: Security Pitfalls of Vision Token Compression in Large Vision-Language Models](https://arxiv.org/abs/2601.12042)
*Xiaomei Zhang,Zhaoxi Zhang,Leo Yu Zhang,Yanjun Zhang,Guanhong Tao,Shirui Pan*

Main category: cs.CR

TL;DR: 本文揭示视觉标记压缩会降低大视觉语言模型鲁棒性，提出压缩感知攻击研究该漏洞，实验表明存在效率 - 安全权衡。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注视觉标记压缩的效率和性能，其安全影响未被充分探索。

Method: 分析压缩过程关键阶段确定鲁棒性下降原因，提出压缩感知攻击（CAA）并扩展到黑盒设置（Transfer CAA），评估潜在防御措施。

Result: 视觉标记压缩显著破坏模型鲁棒性，现有防御措施保护有限。

Conclusion: 视觉标记压缩存在之前被忽视的效率 - 安全权衡。

Abstract: Visual token compression is widely adopted to improve the inference efficiency of Large Vision-Language Models (LVLMs), enabling their deployment in latency-sensitive and resource-constrained scenarios. However, existing work has mainly focused on efficiency and performance, while the security implications of visual token compression remain largely unexplored. In this work, we first reveal that visual token compression substantially degrades the robustness of LVLMs: models that are robust under uncompressed inference become highly vulnerable once compression is enabled. These vulnerabilities are state-specific; failure modes emerge only in the compressed setting and completely disappear when compression is disabled, making them particularly hidden and difficult to diagnose. By analyzing the key stages of the compression process, we identify instability in token importance ranking as the primary cause of this robustness degradation. Small and imperceptible perturbations can significantly alter token rankings, leading the compression mechanism to mistakenly discard task-critical information and ultimately causing model failure. Motivated by this observation, we propose a Compression-Aware Attack to systematically study and exploit this vulnerability. CAA directly targets the token selection mechanism and induces failures exclusively under compressed inference. We further extend this approach to more realistic black-box settings and introduce Transfer CAA, where neither the target model nor the compression configuration is accessible. We further evaluate potential defenses and find that they provide only limited protection. Extensive experiments across models, datasets, and compression methods show that visual token compression significantly undermines robustness, revealing a previously overlooked efficiency-security trade-off.

</details>


### [773] [Semantic Differentiation for Tackling Challenges in Watermarking Low-Entropy Constrained Generation Outputs](https://arxiv.org/abs/2601.11629)
*Nghia T. Le,Alan Ritter,Kartik Goyal*

Main category: cs.CR

TL;DR: 现有语言模型水印方法在受限生成任务不足，提出SeqMark算法，改善水印检测准确率并保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型水印方法在受限生成任务中表现不佳，需更好的算法。

Method: 设计具有语义区分的序列级水印算法SeqMark，区分高概率输出子空间并分区。

Result: 在多种受限生成任务中，SeqMark大幅提高水印检测准确率（F1提升达28%），且保持高生成质量。

Conclusion: SeqMark能解决当前水印方法在受限生成任务的不足，平衡输出质量、水印可检测性和不可感知性。

Abstract: We demonstrate that while the current approaches for language model watermarking are effective for open-ended generation, they are inadequate at watermarking LM outputs for constrained generation tasks with low-entropy output spaces. Therefore, we devise SeqMark, a sequence-level watermarking algorithm with semantic differentiation that balances the output quality, watermark detectability, and imperceptibility. It improves on the shortcomings of the prevalent token-level watermarking algorithms that cause under-utilization of the sequence-level entropy available for constrained generation tasks. Moreover, we identify and improve upon a different failure mode we term region collapse, associated with prior sequence-level watermarking algorithms. This occurs because the pseudorandom partitioning of semantic space for watermarking in these approaches causes all high-probability outputs to collapse into either invalid or valid regions, leading to a trade-off in output quality and watermarking effectiveness. SeqMark instead, differentiates the high-probable output subspace and partitions it into valid and invalid regions, ensuring the even spread of high-quality outputs among all the regions. On various constrained generation tasks like machine translation, code generation, and abstractive summarization, SeqMark substantially improves watermark detection accuracy (up to 28% increase in F1) while maintaining high generation quality.

</details>


### [774] [Efficient Privacy-Preserving Retrieval Augmented Generation with Distance-Preserving Encryption](https://arxiv.org/abs/2601.12331)
*Huanyi Ye,Jiale Guo,Ziyao Liu,Kwok-Yan Lam*

Main category: cs.CR

TL;DR: 提出高效隐私保护RAG框架ppRAG用于不可信云环境，实验证明其高效、准确且隐私性强。


<details>
  <summary>Details</summary>
Motivation: 传统RAG服务在资源受限场景依赖第三方云存储有隐私风险，现有隐私保护技术计算开销大。

Method: 提出CAPRISE加密嵌入，允许云计算加密查询与数据库嵌入的相似度；引入DP对查询嵌入加密前扰动。

Result: ppRAG实现高效处理吞吐量、高检索准确率和强隐私保证。

Conclusion: ppRAG是资源受限用户寻求安全云增强大语言模型的实用解决方案。

Abstract: RAG has emerged as a key technique for enhancing response quality of LLMs without high computational cost. In traditional architectures, RAG services are provided by a single entity that hosts the dataset within a trusted local environment. However, individuals or small organizations often lack the resources to maintain data storage servers, leading them to rely on outsourced cloud storage. This dependence on untrusted third-party services introduces privacy risks. Embedding-based retrieval mechanisms, commonly used in RAG systems, are vulnerable to privacy leakage such as vector-to-text reconstruction attacks and structural leakage via vector analysis. Several privacy-preserving RAG techniques have been proposed but most existing approaches rely on partially homomorphic encryption, which incurs substantial computational overhead. To address these challenges, we propose an efficient privacy-preserving RAG framework (ppRAG) tailored for untrusted cloud environments that defends against vector-to-text attack, vector analysis, and query analysis. We propose Conditional Approximate Distance-Comparison-Preserving Symmetric Encryption (CAPRISE) that encrypts embeddings while still allowing the cloud to compute similarity between an encrypted query and the encrypted database embeddings. CAPRISE preserves only the relative distance ordering between the encrypted query and each encrypted database embedding, without exposing inter-database distances, thereby enhancing both privacy and efficiency. To mitigate query analysis, we introduce DP by perturbing the query embedding prior to encryption, preventing the cloud from inferring sensitive patterns. Experimental results show that ppRAG achieves efficient processing throughput, high retrieval accuracy, strong privacy guarantees, making it a practical solution for resource-constrained users seeking secure cloud-augmented LLMs.

</details>


### [775] [AgenTRIM: Tool Risk Mitigation for Agentic AI](https://arxiv.org/abs/2601.12449)
*Roy Betser,Shamik Bose,Amit Giloni,Chiara Picardi,Sindhu Padakandla,Roman Vainshtein*

Main category: cs.CR

TL;DR: AI智能体使用外部工具存在安全风险，本文提出AgenTRIM框架检测和缓解风险，实验证明其有效且能保持性能。


<details>
  <summary>Details</summary>
Motivation: AI智能体结合外部工具时，不当的工具权限会引入安全风险，如间接提示注入和工具滥用。

Method: 引入AgenTRIM框架，通过离线和在线两个互补阶段解决风险。离线时从代码和执行跟踪中重构和验证智能体的工具接口；运行时通过自适应过滤和状态感知验证实施每步最小权限工具访问。

Result: 在AgentDojo基准测试中，AgenTRIM大幅降低攻击成功率，同时保持高任务性能，对基于描述的攻击具有鲁棒性，能有效执行明确的安全策略。

Conclusion: AgenTRIM为基于大语言模型的智能体提供了一种实用、能保持能力且更安全的工具使用方法。

Abstract: AI agents are autonomous systems that combine LLMs with external tools to solve complex tasks. While such tools extend capability, improper tool permissions introduce security risks such as indirect prompt injection and tool misuse. We characterize these failures as unbalanced tool-driven agency. Agents may retain unnecessary permissions (excessive agency) or fail to invoke required tools (insufficient agency), amplifying the attack surface and reducing performance. We introduce AgenTRIM, a framework for detecting and mitigating tool-driven agency risks without altering an agent's internal reasoning. AgenTRIM addresses these risks through complementary offline and online phases. Offline, AgenTRIM reconstructs and verifies the agent's tool interface from code and execution traces. At runtime, it enforces per-step least-privilege tool access through adaptive filtering and status-aware validation of tool calls. Evaluating on the AgentDojo benchmark, AgenTRIM substantially reduces attack success while maintaining high task performance. Additional experiments show robustness to description-based attacks and effective enforcement of explicit safety policies. Together, these results demonstrate that AgenTRIM provides a practical, capability-preserving approach to safer tool use in LLM-based agents.

</details>


### [776] [De-Anonymization at Scale via Tournament-Style Attribution](https://arxiv.org/abs/2601.12407)
*Lirui Zhang,Huishuai Zhang*

Main category: cs.CR

TL;DR: 研究LLM在作者身份去匿名化方面的威胁，提出DAS方法，实验证明其有现实隐私风险且优于先前方法


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型发展并进入实际应用，其隐私影响愈发重要，研究作者身份去匿名化威胁，如对双盲同行评审的潜在影响

Method: 提出基于大语言模型的DAS方法，用顺序推进策略，添加密集检索预过滤器和多数投票式聚合

Result: 在匿名评审数据上能从数万文本中准确恢复同作者文本，在标准作者身份基准测试中准确性和可扩展性优于先前方法

Conclusion: 展示了匿名平台存在现实隐私风险，凸显了由大语言模型带来的新的去匿名化漏洞

Abstract: As LLMs rapidly advance and enter real-world use, their privacy implications are increasingly important. We study an authorship de-anonymization threat: using LLMs to link anonymous documents to their authors, potentially compromising settings such as double-blind peer review.
  We propose De-Anonymization at Scale (DAS), a large language model-based method for attributing authorship among tens of thousands of candidate texts. DAS uses a sequential progression strategy: it randomly partitions the candidate corpus into fixed-size groups, prompts an LLM to select the text most likely written by the same author as a query text, and iteratively re-queries the surviving candidates to produce a ranked top-k list. To make this practical at scale, DAS adds a dense-retrieval prefilter to shrink the search space and a majority-voting style aggregation over multiple independent runs to improve robustness and ranking precision. Experiments on anonymized review data show DAS can recover same-author texts from pools of tens of thousands with accuracy well above chance, demonstrating a realistic privacy risk for anonymous platforms. On standard authorship benchmarks (Enron emails and blog posts), DAS also improves both accuracy and scalability over prior approaches, highlighting a new LLM-enabled de-anonymization vulnerability.

</details>


### [777] [TrojanPraise: Jailbreak LLMs via Benign Fine-Tuning](https://arxiv.org/abs/2601.12460)
*Zhixin Xie,Xurui Song,Jun Luo*

Main category: cs.CR

TL;DR: 提出TrojanPraise攻击方法，利用良性数据对大语言模型进行越狱攻击，实验显示可达到95.88%成功率并规避审核。


<details>
  <summary>Details</summary>
Motivation: 商业大语言模型提供黑盒微调API带来安全漏洞，但恶意数据集会被检测，因此探索利用良性数据的攻击方法。

Method: 提出TrojanPraise攻击，先让模型将精心设计的词与无害含义关联，再用该词赞美有害概念，微调模型；将查询的内部表示解耦为知识和态度两维度。

Result: 在五个开源和两个商业大语言模型的严格黑盒实验中，TrojanPraise攻击成功率最高达95.88%，且可规避审核。

Conclusion: TrojanPraise可利用良性数据对大语言模型进行有效越狱攻击，并逃避审核。

Abstract: The demand of customized large language models (LLMs) has led to commercial LLMs offering black-box fine-tuning APIs, yet this convenience introduces a critical security loophole: attackers could jailbreak the LLMs by fine-tuning them with malicious data. Though this security issue has recently been exposed, the feasibility of such attacks is questionable as malicious training dataset is believed to be detectable by moderation models such as Llama-Guard-3. In this paper, we propose TrojanPraise, a novel finetuning-based attack exploiting benign and thus filter-approved data. Basically, TrojanPraise fine-tunes the model to associate a crafted word (e.g., "bruaf") with harmless connotations, then uses this word to praise harmful concepts, subtly shifting the LLM from refusal to compliance. To explain the attack, we decouple the LLM's internal representation of a query into two dimensions of knowledge and attitude. We demonstrate that successful jailbreak requires shifting the attitude while avoiding knowledge shift, a distortion in the model's understanding of the concept. To validate this attack, we conduct experiments on five opensource LLMs and two commercial LLMs under strict black-box settings. Results show that TrojanPraise achieves a maximum attack success rate of 95.88% while evading moderation.

</details>


### [778] [BlocksecRT-DETR: Decentralized Privacy-Preserving and Token-Efficient Federated Transformer Learning for Secure Real-Time Object Detection in ITS](https://arxiv.org/abs/2601.12693)
*Mohoshin Ara Tahera,Sabbir Rahman,Shuvalaxmi Dass,Sharif Ullah,Mahmoud Abouyessef*

Main category: cs.CR

TL;DR: 提出BlockSecRT - DETR框架解决智能交通系统中联合实时目标检测面临的挑战，经评估有较好效果。


<details>
  <summary>Details</summary>
Motivation: 解决智能交通系统中联合实时目标检测面临的非IID数据异质性、边缘硬件延迟、隐私安全风险三大问题。

Method: 提出BlockSecRT - DETR框架，集成RT - DETR训练与TEM解决前两个挑战，采用去中心化区块链安全更新验证机制解决第三个挑战。

Result: TEM提升推理延迟17.2%，降低编码器FLOPs 47.8%，保持全局检测精度89.20% mAP@0.5；区块链集成每轮增加400ms，账本大小低于12KB。

Conclusion: BlockSecRT - DETR框架可为智能交通系统提供去中心化、高效且隐私保护的联合训练解决方案。

Abstract: Federated real-time object detection using transformers in Intelligent Transportation Systems (ITS) faces three major challenges: (1) missing-class non-IID data heterogeneity from geographically diverse traffic environments, (2) latency constraints on edge hardware for high-capacity transformer models, and (3) privacy and security risks from untrusted client updates and centralized aggregation. We propose BlockSecRT-DETR, a BLOCKchain-SECured Real-Time Object DEtection TRansformer framework for ITS that provides a decentralized, token-efficient, and privacy-preserving federated training solution using RT-DETR transformer, incorporating a blockchain-secured update validation mechanism for trustworthy aggregation. In this framework, challenges (1) and (2) are jointly addressed through a unified client-side design that integrates RT-DETR training with a Token Engineering Module (TEM). TEM prunes low-utility tokens, reducing encoder complexity and latency on edge hardware, while aggregated updates mitigate non-IID data heterogeneity across clients. To address challenge (3), BlockSecRT-DETR incorporates a decentralized blockchain-secured update validation mechanism that enables tamper-proof, privacy-preserving, and trust-free authenticated model aggregation without relying on a central server. We evaluated the proposed framework under a missing-class Non-IID partition of the KITTI dataset and conducted a blockchain case study to quantify security overhead. TEM improves inference latency by 17.2% and reduces encoder FLOPs by 47.8%, while maintaining global detection accuracy (89.20% mAP@0.5). The blockchain integration adds 400 ms per round, and the ledger size remains under 12 KB due to metadata-only on-chain storage.

</details>


### [779] [PDFInspect: A Unified Feature Extraction Framework for Malicious Document Detection](https://arxiv.org/abs/2601.12866)
*Sharmila S P*

Main category: cs.CR

TL;DR: 本文提出统一框架提取PDF文件特征，生成高维向量用于下游任务，适合真实场景。


<details>
  <summary>Details</summary>
Motivation: 恶意PDF文件日益增多，需有效特征提取方法。

Method: 结合图、结构和元数据驱动分析，提取文本、图特征，解析元数据，衍生时间特征，量化结构元素，提取布尔标志。

Result: 生成170维高维向量表示，适合恶意软件分类、异常检测等任务。

Conclusion: 该方法可扩展，适合真实世界PDF威胁情报工作流。

Abstract: The increasing prevalence of malicious Portable Document Format (PDF) files necessitates robust and comprehensive feature extraction techniques for effective detection and analysis. This work presents a unified framework that integrates graph-based, structural, and metadata-driven analysis to generate a rich feature representation for each PDF document. The system extracts text from PDF pages and constructs undirected graphs based on pairwise word relationships, enabling the computation of graph-theoretic features such as node count, edge density, and clustering coefficient. Simultaneously, the framework parses embedded metadata to quantify character distributions, entropy patterns, and inconsistencies across fields such as author, title, and producer. Temporal features are derived from creation and modification timestamps to capture behavioral signatures, while structural elements including, object streams, fonts, and embedded images, are quantified to reflect document complexity. Boolean flags for potentially malicious PDF constructs (e.g., JavaScript, launch actions) are also extracted. Together, these features form a high-dimensional vector representation (170 dimensions) that is well-suited for downstream tasks such as malware classification, anomaly detection, and forensic analysis. The proposed approach is scalable, extensible, and designed to support real-world PDF threat intelligence workflows.6

</details>


### [780] [Your Privacy Depends on Others: Collusion Vulnerabilities in Individual Differential Privacy](https://arxiv.org/abs/2601.12922)
*Johannes Kaiser,Alexander Ziller,Eleni Triantafillou,Daniel Rückert,Georgios Kaissis*

Main category: cs.CR

TL;DR: 揭示基于采样的iDP机制漏洞，个体隐私风险受他人影响，有攻击风险，提出新隐私合约应对。


<details>
  <summary>Details</summary>
Motivation: 指出iDP在实践中无法兑现用户控制隐私的承诺，需研究其存在的问题。

Method: 通过实证研究展示隐私偏好分布对个体隐私风险的影响，设计针对个体的攻击，提出(εi,δi,Δ̅)-iDP隐私合约。

Result: 成功对62%的目标个体发动攻击，增加其成员推断易感性；提出的隐私合约能为用户提供超额漏洞上限。

Conclusion: 当前iDP范式存在根本挑战，需重新评估iDP系统的设计、审计、沟通和部署。

Abstract: Individual Differential Privacy (iDP) promises users control over their privacy, but this promise can be broken in practice. We reveal a previously overlooked vulnerability in sampling-based iDP mechanisms: while conforming to the iDP guarantees, an individual's privacy risk is not solely governed by their own privacy budget, but critically depends on the privacy choices of all other data contributors. This creates a mismatch between the promise of individual privacy control and the reality of a system where risk is collectively determined. We demonstrate empirically that certain distributions of privacy preferences can unintentionally inflate the privacy risk of individuals, even when their formal guarantees are met. Moreover, this excess risk provides an exploitable attack vector. A central adversary or a set of colluding adversaries can deliberately choose privacy budgets to amplify vulnerabilities of targeted individuals. Most importantly, this attack operates entirely within the guarantees of DP, hiding this excess vulnerability. Our empirical evaluation demonstrates successful attacks against 62% of targeted individuals, substantially increasing their membership inference susceptibility. To mitigate this, we propose $(\varepsilon_i,δ_i,\overlineΔ)$-iDP a privacy contract that uses $Δ$-divergences to provide users with a hard upper bound on their excess vulnerability, while offering flexibility to mechanism design. Our findings expose a fundamental challenge to the current paradigm, demanding a re-evaluation of how iDP systems are designed, audited, communicated, and deployed to make excess risks transparent and controllable.

</details>


### [781] [On the Evidentiary Limits of Membership Inference for Copyright Auditing](https://arxiv.org/abs/2601.12937)
*Murat Bilgehan Ertan,Emirhan Böge,Min Chen,Kaleel Mahmood,Marten van Dijk*

Main category: cs.CR

TL;DR: 研究在对抗性版权纠纷中成员推理攻击（MIAs）作为证据的有效性，引入SAGE框架测试，结果显示MIAs在对抗性环境中较脆弱。


<details>
  <summary>Details</summary>
Motivation: 探究MIAs在对抗性版权纠纷中作为可采纳证据的可行性，因被告模型开发者可能混淆训练数据。

Method: 通过法官 - 检察官 - 被告通信协议形式化该场景，引入SAGE框架改写训练数据以测试MIAs鲁棒性。

Result: 在SAGE生成的释义上微调模型时，现有MIAs效果下降，部分微调机制仍有信息泄露。

Conclusion: MIAs在对抗性环境中脆弱，不足以单独作为大语言模型版权审计机制。

Abstract: As large language models (LLMs) are trained on increasingly opaque corpora, membership inference attacks (MIAs) have been proposed to audit whether copyrighted texts were used during training, despite growing concerns about their reliability under realistic conditions. We ask whether MIAs can serve as admissible evidence in adversarial copyright disputes where an accused model developer may obfuscate training data while preserving semantic content, and formalize this setting through a judge-prosecutor-accused communication protocol. To test robustness under this protocol, we introduce SAGE (Structure-Aware SAE-Guided Extraction), a paraphrasing framework guided by Sparse Autoencoders (SAEs) that rewrites training data to alter lexical structure while preserving semantic content and downstream utility. Our experiments show that state-of-the-art MIAs degrade when models are fine-tuned on SAGE-generated paraphrases, indicating that their signals are not robust to semantics-preserving transformations. While some leakage remains in certain fine-tuning regimes, these results suggest that MIAs are brittle in adversarial settings and insufficient, on their own, as a standalone mechanism for copyright auditing of LLMs.

</details>


### [782] [Adversarial News and Lost Profits: Manipulating Headlines in LLM-Driven Algorithmic Trading](https://arxiv.org/abs/2601.13082)
*Advije Rizvani,Giovanni Apruzzese,Pavel Laskov*

Main category: cs.CR

TL;DR: 研究大语言模型在金融领域受对抗性新闻攻击风险，量化其对算法交易系统的货币风险。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于金融领域算法交易系统有被对抗性新闻误导风险，以往未量化其货币风险。

Method: 考虑能修改股票新闻标题的对手，评估两种人类难以察觉的操纵方式，用Backtrader实现算法交易系统，结合LSTM价格预测和大语言模型情感分析，用投资组合指标量化影响，并评估现实可行性。

Result: 为期14个月操纵一天的攻击可误导LLMs，使年回报率最多降低17.7个百分点，分析和调查证实假设。

Conclusion: 指出大语言模型在金融领域应用受对抗性新闻攻击存在风险，已通知交易平台所有者。

Abstract: Large Language Models (LLMs) are increasingly adopted in the financial domain. Their exceptional capabilities to analyse textual data make them well-suited for inferring the sentiment of finance-related news. Such feedback can be leveraged by algorithmic trading systems (ATS) to guide buy/sell decisions. However, this practice bears the risk that a threat actor may craft "adversarial news" intended to mislead an LLM. In particular, the news headline may include "malicious" content that remains invisible to human readers but which is still ingested by the LLM. Although prior work has studied textual adversarial examples, their system-wide impact on LLM-supported ATS has not yet been quantified in terms of monetary risk. To address this threat, we consider an adversary with no direct access to an ATS but able to alter stock-related news headlines on a single day. We evaluate two human-imperceptible manipulations in a financial context: Unicode homoglyph substitutions that misroute models during stock-name recognition, and hidden-text clauses that alter the sentiment of the news headline. We implement a realistic ATS in Backtrader that fuses an LSTM-based price forecast with LLM-derived sentiment (FinBERT, FinGPT, FinLLaMA, and six general-purpose LLMs), and quantify monetary impact using portfolio metrics. Experiments on real-world data show that manipulating a one-day attack over 14 months can reliably mislead LLMs and reduce annual returns by up to 17.7 percentage points. To assess real-world feasibility, we analyze popular scraping libraries and trading platforms and survey 27 FinTech practitioners, confirming our hypotheses. We notified trading platform owners of this security issue.

</details>


### [783] [Diffusion-Driven Synthetic Tabular Data Generation for Enhanced DoS/DDoS Attack Classification](https://arxiv.org/abs/2601.13197)
*Aravind B,Anirud R. S.,Sai Surya Teja N,Bala Subrahmanya Sriranga Navaneeth A,Karthika R,Mohankumar N*

Main category: cs.CR

TL;DR: 本文解决TabDDPM用于网络入侵检测时的数据类别不平衡问题，合成少数类样本增强数据，使ANN分类器在攻击类上召回率接近完美。


<details>
  <summary>Details</summary>
Motivation: 解决网络入侵检测中使用Tabular Denoising Diffusion Probability Models（TabDDPM）时数据集类别不平衡，导致模型性能有偏差的问题。

Method: 通过迭代去噪过程从CIC - IDS2017数据集中合成高保真少数类样本，将合成样本与原始数据集合并进行数据增强。

Result: 使用增强后的训练数据，ANN分类器在之前代表性不足的攻击类上实现了接近完美的召回率。

Conclusion: 扩散模型可有效解决安全领域表格数据不平衡问题，在欺诈检测和医疗诊断等领域有潜在应用。

Abstract: Class imbalance refers to a situation where certain classes in a dataset have significantly fewer samples than oth- ers, leading to biased model performance. Class imbalance in network intrusion detection using Tabular Denoising Diffusion Probability Models (TabDDPM) for data augmentation is ad- dressed in this paper. Our approach synthesizes high-fidelity minority-class samples from the CIC-IDS2017 dataset through iterative denoising processes. For the minority classes that have smaller samples, synthetic samples were generated and merged with the original dataset. The augmented training data enables an ANN classifier to achieve near-perfect recall on previously underrepresented attack classes. These results establish diffusion models as an effective solution for tabular data imbalance in security domains, with potential applications in fraud detection and medical diagnostics.

</details>


### [784] [HardSecBench: Benchmarking the Security Awareness of LLMs for Hardware Code Generation](https://arxiv.org/abs/2601.13864)
*Qirui Chen,Jingxian Shuai,Shuangwu Chen,Shenghao Ye,Zijian Wen,Xufei Su,Jie Jin,Jiangming Li,Jun Chen,Xiaobin Tan,Jian Yang*

Main category: cs.CR

TL;DR: 提出HardSecBench基准评估LLM生成硬件和固件代码的安全性，发现模型常留安全风险。


<details>
  <summary>Details</summary>
Motivation: 现有研究对LLM生成代码的安全性关注不足，而有功能缺陷的代码可能有安全漏洞，因此设计安全评估基准。

Method: 引入HardSecBench基准，涵盖924个任务，涉及Verilog RTL和C语言，包含多种规范和测试；提出多智能体流水线实现自动化合成和评估。

Result: 用HardSecBench评估LLMs，发现模型常满足功能需求但留安全风险，安全结果随提示而异。

Conclusion: 指出LLM辅助硬件设计面临挑战，为未来发展提供见解，相关数据和代码将发布。

Abstract: Large language models (LLMs) are being increasingly integrated into practical hardware and firmware development pipelines for code generation. Existing studies have primarily focused on evaluating the functional correctness of LLM-generated code, yet paid limited attention to its security issues. However, LLM-generated code that appears functionally sound may embed security flaws which could induce catastrophic damages after deployment. This critical research gap motivates us to design a benchmark for assessing security awareness under realistic specifications. In this work, we introduce HardSecBench, a benchmark with 924 tasks spanning Verilog Register Transfer Level (RTL) and firmware-level C, covering 76 hardware-relevant Common Weakness Enumeration (CWE) entries. Each task includes a structured specification, a secure reference implementation, and executable tests. To automate artifact synthesis, we propose a multi-agent pipeline that decouples synthesis from verification and grounds evaluation in execution evidence, enabling reliable evaluation. Using HardSecBench, we evaluate a range of LLMs on hardware and firmware code generation and find that models often satisfy functional requirements while still leaving security risks. We also find that security results vary with prompting. These findings highlight pressing challenges and offer actionable insights for future advancements in LLM-assisted hardware design. Our data and code will be released soon.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [785] [Conservation priorities to prevent the next pandemic](https://arxiv.org/abs/2601.13349)
*Leonardo Viotti,Luis Diego Herrera,Garo Batmanian,Franck Berthe,Rachael Kramp*

Main category: q-bio.PE

TL;DR: 该论文指出野生动物疾病威胁全球健康，因缺乏有效信息，疾病预防被忽视。文中制作了高分辨率疾病传播风险防控优先区域地图，发现当前保护工作存在不足。


<details>
  <summary>Details</summary>
Motivation: 野生动物疾病对全球健康威胁大，现有疾病预防工作缺乏可靠信息支持，且鉴于新疾病出现加速，需确定防控优先区域。

Method: 收集记录详细的风险因素、保护状况、森林恢复潜力和土地机会成本等数据制作地图，构建适应本地情况的方法。

Result: 确定了50个国家的低成本优先区域，发现大量可通过环境恢复和防止森林砍伐降低人畜共患病溢出风险的区域且大多未受保护，地图分层数据及交互平台免费提供。

Conclusion: 生态对策是降低新病原体出现的经济有效策略，但当前保护工作未达成这一目标。

Abstract: Diseases originating from wildlife pose a significant threat to global health, causing human and economic losses each year. The transmission of disease from animals to humans occurs at the interface between humans, livestock, and wildlife reservoirs, influenced by abiotic factors and ecological mechanisms. Although evidence suggests that intact ecosystems can reduce transmission, disease prevention has largely been neglected in conservation efforts and remains underfunded compared to mitigation. A major constraint is the lack of reliable, spatially explicit information to guide efforts effectively. Given the increasing rate of new disease emergence, accelerated by climate change and biodiversity loss, identifying priority areas for mitigating the risk of disease transmission is more crucial than ever. We present new high-resolution (1 km) maps of priority areas for targeted ecological countermeasures aimed at reducing the likelihood of zoonotic spillover, along with a methodology adaptable to local contexts. Our study compiles data on well-documented risk factors, protection status, forest restoration potential, and opportunity cost of the land to map areas with high potential for cost-effective interventions. We identify low-cost priority areas across 50 countries, including 277,000 km2 where environmental restoration could mitigate the risk of zoonotic spillover and 198,000 km2 where preventing deforestation could do the same, 95% of which are not currently under protection. The resulting layers, covering tropical regions globally, are freely available alongside an interactive no-code platform that allows users to adjust parameters and identify priority areas at multiple scales. Ecological countermeasures can be a cost-effective strategy for reducing the emergence of new pathogens; however, our study highlights the extent to which current conservation efforts fall short of this goal.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [786] [HERMES: A Unified Open-Source Framework for Realtime Multimodal Physiological Sensing, Edge AI, and Intervention in Closed-Loop Smart Healthcare Applications](https://arxiv.org/abs/2601.12610)
*Maxim Yudayev,Juha Carlon,Diwas Lamsal,Vayalet Stefanova,Benjamin Filtjens*

Main category: eess.SY

TL;DR: 本文介绍开源框架HERMES，用于边缘端连续多模态传感和AI处理，适用于多种环境，通过案例验证其在智能医疗领域的性能和相关性。


<details>
  <summary>Details</summary>
Motivation: 智能辅助技术在可靠的高通量多模态传感和处理方面存在需求未解决，现有问题阻碍相关应用，为加速临床部署而开展研究。

Method: 开发HERMES这一开源的高性能Python框架，实现同步数据收集和实时流推理，适用于不同环境和传感器。

Result: 通过在闭环智能假肢用例上的应用和验证，展示了HERMES的性能和与智能医疗领域发展轨迹的相关性。

Conclusion: HERMES是首个提供完整方法的工作，能弥合实际应用策略中的跨学科差距，指导下游AI模型开发。

Abstract: Intelligent assistive technologies are increasingly recognized as critical daily-use enablers for people with disabilities and age-related functional decline. Longitudinal studies, curation of quality datasets, live monitoring in activities of daily living, and intelligent intervention devices, share the largely unsolved need in reliable high-throughput multimodal sensing and processing. Streaming large heterogeneous data from distributed sensors, historically closed-source environments, and limited prior works on realtime closed-loop AI methodologies, inhibit such applications. To accelerate the emergence of clinical deployments, we deliver HERMES - an open-source high-performance Python framework for continuous multimodal sensing and AI processing at the edge. It enables synchronized data collection, and realtime streaming inference with user PyTorch models, on commodity computing devices. HERMES is applicable to fixed-lab and free-living environments, of distributed commercial and custom sensors. It is the first work to offer a holistic methodology that bridges cross-disciplinary gaps in real-world implementation strategies, and guides downstream AI model development. Its application on the closed-loop intelligent prosthesis use case illustrates the process of suitable AI model development from the generated constraints and trade-offs. Validation on the use case, with 4 synchronized hosts cooperatively capturing 18 wearable and off-body modalities, demonstrates performance and relevance of HERMES to the trajectory of the intelligent healthcare domain.

</details>


<div id='physics.space-ph'></div>

# physics.space-ph [[Back]](#toc)

### [787] [Deterministic and probabilistic neural surrogates of global hybrid-Vlasov simulations](https://arxiv.org/abs/2601.12614)
*Daniel Holmberg,Ivan Zaitsev,Markku Alho,Ioanna Bouri,Fanni Franssila,Haewon Jeong,Minna Palmroth,Teemu Roos*

Main category: physics.space-ph

TL;DR: 研究表明基于图的机器学习模拟器可学习近地空间电磁场和离子速度分布低阶矩的时空演化，能快速准确预测等离子体状态，为混合-弗拉索夫模拟实时应用提供可能。


<details>
  <summary>Details</summary>
Motivation: 混合-弗拉索夫模拟计算成本高，难以用于实时应用，需找到提高效率的方法。

Method: 使用图神经网络架构，基于2D空间模拟网格，构建确定性预测模型（Graph - FM）和概率性集成预测模型（Graph - EFM），训练中加入散度惩罚和连续排序概率得分目标。

Result: 模拟器能准确预测未来等离子体状态，训练后生成下一步的速度比原始模拟快两个数量级以上，且与不同运行的磁层物理响应匹配良好。

Conclusion: 机器学习使混合-弗拉索夫模拟可用于实时应用，并能提供预测不确定性。

Abstract: Hybrid-Vlasov simulations resolve ion-kinetic effects for modeling the solar wind-magnetosphere interaction, but even 5D (2D + 3V) simulations are computationally expensive. We show that graph-based machine learning emulators can learn the spatiotemporal evolution of electromagnetic fields and lower order moments of ion velocity distribution in the near-Earth space environment from four 5D Vlasiator runs performed with identical steady solar wind conditions. The initial ion number density is systematically varied, while the grid spacing is held constant, to scan the ratio of the characteristic ion skin depth to the numerical grid size. Using a graph neural network architecture operating on the 2D spatial simulation grid comprising 670k cells, we demonstrate that both a deterministic forecasting model (Graph-FM) and a probabilistic ensemble forecasting model (Graph-EFM) based on a latent variable formulation are capable of producing accurate predictions of future plasma states. A divergence penalty is incorporated during training to encourage divergence-freeness in the magnetic fields and improve physical consistency. For the probabilistic model, a continuous ranked probability score objective is added to improve the calibration of the ensemble forecasts. When trained, the emulators achieve more than two orders of magnitude speedup in generating the next time step relative to the original simulation on a single GPU compared to 100 CPUs for the Vlasiator runs, while closely matching physical magnetospheric response of the different runs. These results demonstrate that machine learning offers a way to make hybrid-Vlasov simulation tractable for real-time use while providing forecast uncertainty.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [788] [Towards Effective Negation Modeling in Joint Audio-Text Models for Music](https://arxiv.org/abs/2601.13931)
*Yannis Vasilakis,Rachel Bittner,Johan Pauwels*

Main category: cs.SD

TL;DR: 现有联合音频 - 文本模型处理否定语义有局限，本文通过训练CLAP模型、文本增强和对比损失改进处理能力，实验显示改进效果良好。


<details>
  <summary>Details</summary>
Motivation: 当前联合音频 - 文本模型在处理否定语义时存在问题，无法可靠表示否定信息，影响对音乐元素存在与否的区分。

Method: 在Million Song Dataset数据集上用LP - MusicCaps - MSD标签从零开始训练CLAP模型；通过文本增强引入否定信息；采用基于相异度的对比损失函数；提出将否定建模作为检索和二分类任务的评估协议。

Result: 改进方法单独或组合使用，都可改善否定语义处理能力，且基本保留检索性能。

Conclusion: 所用方法能有效缓解联合音频 - 文本模型处理否定语义的局限。

Abstract: Joint audio-text models are widely used for music retrieval, yet they struggle with semantic phenomena such as negation. Negation is fundamental for distinguishing the absence (or presence) of musical elements (e.g., "with vocals" vs. "without vocals"), but current systems fail to represent this reliably. In this work, we investigate and mitigate this limitation by training CLAP models from scratch on the Million Song Dataset with LP-MusicCaps-MSD captions. We introduce negation through text augmentation and a dissimilarity-based contrastive loss, designed to explicitly separate original and negated captions in the joint embedding space. To evaluate progress, we propose two protocols that frame negation modeling as retrieval and binary classification tasks. Experiments demonstrate that both methods, individually and combined, improve negation handling while largely preserving retrieval performance.

</details>


### [789] [Do Neural Codecs Generalize? A Controlled Study Across Unseen Languages and Non-Speech Tasks](https://arxiv.org/abs/2601.12205)
*Shih-Heng Wang,Jiatong Shi,Jinchuan Tian,Haibin Wu,Shinji Watanabe*

Main category: cs.SD

TL;DR: 论文探究神经音频编解码器（NACs）泛化能力的三个方面，用严格控制的配置从零训练NACs进行评估，发现NACs能泛化到未见过语言，仅语音预训练的NACs在非语音任务上性能下降，预训练融入非语音数据可提升非语音任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究使用现成NACs比较，因实现差异限制了深入理解，需研究NACs泛化能力的三个未充分探索方面。

Method: 使用严格控制的配置从零训练NACs，用精心策划的预训练数据进行公平比较，用11个指标从信号重建质量和下游应用综合评估NAC性能。

Result: NACs能泛化到预训练时未见过的语言；仅语音预训练的NACs在非语音任务上性能下降；预训练融入非语音数据能提升非语音任务性能且语音任务性能相当。

Conclusion: 对于NACs泛化能力的三个方面研究得到了明确结论，为NACs的预训练数据选择和应用提供参考。

Abstract: This paper investigates three crucial yet underexplored aspects of the generalization capabilities of neural audio codecs (NACs): (i) whether NACs can generalize to unseen languages during pre-training, (ii) whether speech-only pre-trained NACs can effectively generalize to non-speech applications such as environmental sounds, music, and animal vocalizations, and (iii) whether incorporating non-speech data during pre-training can improve performance on both speech and non-speech tasks. Existing studies typically rely on off-the-shelf NACs for comparison, which limits insight due to variations in implementation. In this work, we train NACs from scratch using strictly controlled configurations and carefully curated pre-training data to enable fair comparisons. We conduct a comprehensive evaluation of NAC performance on both signal reconstruction quality and downstream applications using 11 metrics. Our results show that NACs can generalize to unseen languages during pre-training, speech-only pre-trained NACs exhibit degraded performance on non-speech tasks, and incorporating non-speech data during pre-training improves performance on non-speech tasks while maintaining comparable performance on speech tasks.

</details>


### [790] [Harmonizing the Arabic Audio Space with Data Scheduling](https://arxiv.org/abs/2601.12494)
*Hunzalah Hassan Bhatti,Firoj Alam,Shammur Absar Chowdhury*

Main category: cs.SD

TL;DR: 本文对以阿拉伯语为中心的音频大语言模型进行多任务指令调优的系统研究，引入新数据集，提出新策略，揭示效率与鲁棒性权衡，给出最优训练策略。


<details>
  <summary>Details</summary>
Motivation: 音频大语言模型在语言复杂、方言丰富场景的适配研究不足，需对以阿拉伯语为中心的音频大语言模型进行多任务指令调优研究。

Method: 引入新数据集AraMega - SSum，微调Qwen2.5 - Omni (7B)，提出Task - Progressive Curriculum (TPC)和Aligner - Based Diverse Sampling (ADS)策略。

Result: ADS加速初始收敛、提升副语言F1分数，但长时间训练会使生成解码不稳定；TPC稳定核心声学映射，但下游任务会出现负迁移。

Conclusion: 混合TPC + ADS策略是最优训练“配方”，为复杂、低资源多模态环境下全模态模型的高效适配提供实用指导。

Abstract: Audio large language models (LLMs) enable unified speech understanding and generation, yet their adaptation to linguistically complex, dialect-rich settings remains underexplored. This paper presents the first systematic study of multi-task instruction tuning for an Arabic-centric audio LLM, covering a hierarchy of generative tasks (ASR, speech summarization) and discriminative tasks (dialect and emotion identification). To support this study, we introduce AraMega-SSum, a novel dataset for Arabic speech summarization. We fine-tune Qwen2.5-Omni (7B) and propose Task-Progressive Curriculum (TPC) along with Aligner-Based Diverse Sampling (ADS), a strategy that constructs information-dense batches by selecting task- and label-balanced examples. Our results reveal a critical efficiency, robustness trade-off: while ADS accelerates initial convergence and boosts paralinguistic F1-scores, its inherent gradient volatility can destabilize generative decoding under prolonged training. Furthermore, while the TPC stabilizes core acoustic mapping, it often induces negative transfer in downstream tasks. We demonstrate that a Hybrid TPC+ADS Strategy provides an optimal training ``recipe'', first establishing a robust representative foundation before employing diversity-aware refinement to capture fine-grained nuances. These findings offer practical guidance for the efficient adaptation of Omni-models in complex, low-resource multimodal environments.

</details>


### [791] [ParaMETA: Towards Learning Disentangled Paralinguistic Speaking Styles Representations from Speech](https://arxiv.org/abs/2601.12289)
*Haowei Lou,Hye-young Paik,Wen Hu,Lina Yao*

Main category: cs.SD

TL;DR: 提出ParaMETA框架，可直接从语音学习和控制说话风格，在分类和语音生成上表现优。


<details>
  <summary>Details</summary>
Motivation: 学习不同说话风格的代表性嵌入对识别和生成任务很重要，现有方法有局限。

Method: ParaMETA通过将语音投影到每种风格的专用子空间来学习解纠缠、特定任务的嵌入。

Result: ParaMETA在分类准确率上优于强基线，生成更自然和富有表现力的语音，且模型轻量高效。

Conclusion: ParaMETA是一个统一灵活的框架，能有效学习和控制说话风格，适用于实际应用。

Abstract: Learning representative embeddings for different types of speaking styles, such as emotion, age, and gender, is critical for both recognition tasks (e.g., cognitive computing and human-computer interaction) and generative tasks (e.g., style-controllable speech generation). In this work, we introduce ParaMETA, a unified and flexible framework for learning and controlling speaking styles directly from speech. Unlike existing methods that rely on single-task models or cross-modal alignment, ParaMETA learns disentangled, task-specific embeddings by projecting speech into dedicated subspaces for each type of style. This design reduces inter-task interference, mitigates negative transfer, and allows a single model to handle multiple paralinguistic tasks such as emotion, gender, age, and language classification. Beyond recognition, ParaMETA enables fine-grained style control in Text-To-Speech (TTS) generative models. It supports both speech- and text-based prompting and allows users to modify one speaking styles while preserving others. Extensive experiments demonstrate that ParaMETA outperforms strong baselines in classification accuracy and generates more natural and expressive speech, while maintaining a lightweight and efficient model suitable for real-world applications.

</details>


### [792] [SSVD-O: Parameter-Efficient Fine-Tuning with Structured SVD for Speech Recognition](https://arxiv.org/abs/2601.12600)
*Pu Wang,Shinji Watanabe,Hugo Van hamme*

Main category: cs.SD

TL;DR: 本文介绍SSVD - Outer方法用于语音模型参数高效微调，在多任务上实验表明其能缩小与全量微调性能差距，提升泛化并缓解灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有方法如LoRA及其变体在语音应用中均匀分配参数，限制了效率和可扩展性，本文旨在提出更优的微调方法。

Method: 提出SSVD - Outer方法，结合输入声学特征空间相关的内部变换和输出语义特征空间相关的外部变换，并对自动语音识别中参数预算分配进行系统分析。

Result: 在ESPnet框架下，在不同模型规模和任务上与多种方法对比，SSVD - Outer能缩小与全量微调性能差距，改善泛化，缓解灾难性遗忘。

Conclusion: SSVD - Outer是一种可扩展且平衡的语音模型微调方法，有效提升性能和泛化能力。

Abstract: Parameter-efficient fine-tuning (PEFT) is a scalable approach for adapting large speech foundation models to new domains. While methods such as LoRA and its state-of-the-art variants reduce adaptation costs, they typically allocate parameters uniformly across model subspaces, which limits their efficiency and scalability in speech applications. Building on our prior work, this paper introduces SSVD-Outer (SSVD-O), an extension of the structured SVD-guided (SSVD) fine-tuning method. SSVD-O combines input acoustic feature space-associated inner transformations with output semantic feature space-associated outer transformations to enable scalable and balanced adaptation. We conduct the first systematic analysis of parameter budget allocation across model subspaces in PEFT for automatic speech recognition (ASR), and investigate the trade-off between learning and forgetting under constrained resources. SSVD-O is benchmarked against LoRA, DoRA, PiSSA, and SSVD on domain-shifted ASR tasks, including child speech and regional accents, across model scales from 0.1B to 2B within the ESPnet framework. Experimental results show that SSVD-O consistently narrows the performance gap to full fine-tuning while improving generalization and mitigating catastrophic forgetting.

</details>


### [793] [Toward Faithful Explanations in Acoustic Anomaly Detection](https://arxiv.org/abs/2601.12660)
*Maab Elrashid,Anthony Deschênes,Cem Subakan,Mirco Ravanelli,Rémi Georges,Michael Morin*

Main category: cs.SD

TL;DR: 研究基于自编码器的音频异常检测模型的可解释性，对比标准自编码器和掩码自编码器，提出基于扰动的忠实度指标，强调将可解释性纳入异常检测流程的重要性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在异常检测中缺乏透明度，为增强用户信任，需研究自编码器模型的可解释性。

Method: 对比标准自编码器（AE）和掩码自编码器（MAE）检测性能和可解释性，应用多种归因方法；提出基于扰动的忠实度指标评估解释方法突出区域的相关性。

Result: MAE检测性能稍低，但解释更忠实、时间更精确；实验基于真实工业场景。

Conclusion: 将可解释性纳入异常检测流程很重要，掩码训练能提升解释质量且不影响性能。

Abstract: Interpretability is essential for user trust in real-world anomaly detection applications. However, deep learning models, despite their strong performance, often lack transparency. In this work, we study the interpretability of autoencoder-based models for audio anomaly detection, by comparing a standard autoencoder (AE) with a mask autoencoder (MAE) in terms of detection performance and interpretability. We applied several attribution methods, including error maps, saliency maps, SmoothGrad, Integrated Gradients, GradSHAP, and Grad-CAM. Although MAE shows a slightly lower detection, it consistently provides more faithful and temporally precise explanations, suggesting a better alignment with true anomalies. To assess the relevance of the regions highlighted by the explanation method, we propose a perturbation-based faithfulness metric that replaces them with their reconstructions to simulate normal input. Our findings, based on experiments in a real industrial scenario, highlight the importance of incorporating interpretability into anomaly detection pipelines and show that masked training improves explanation quality without compromising performance.

</details>


### [794] [SoundPlot: An Open-Source Framework for Birdsong Acoustic Analysis and Neural Synthesis with Interactive 3D Visualization](https://arxiv.org/abs/2601.12752)
*Naqcho Ali Mehdi,Mohammad Adeel,Aizaz Ali Larik*

Main category: cs.SD

TL;DR: 介绍开源框架SoundPlot用于分析鸟类发声，可实现特征提取、降维与音频合成，可视化效果好，保真度高并开源


<details>
  <summary>Details</summary>
Motivation: 为生物声学、音频信号处理和计算行为学研究提供工具

Method: 通过声学特征提取、降维与神经音频合成构建分析 - 合成管道，用Griffin - Lim算法进行音频重建，用Three.js界面可视化

Result: 定量评估显示梅尔频谱图相关分数超0.92，高保真保留声学结构

Conclusion: SoundPlot开源有助于相关领域研究

Abstract: We present SoundPlot, an open-source framework for analyzing avian vocalizations through acoustic feature extraction, dimensionality reduction, and neural audio synthesis. The system transforms audio signals into a multi-dimensional acoustic feature space, enabling real-time visualization of temporal dynamics in 3D using web-based interactive graphics. Our framework implements a complete analysis-synthesis pipeline that extracts spectral features (centroid, bandwidth, contrast), pitch contours via probabilistic YIN (pYIN), and mel-frequency cepstral coefficients (MFCCs), mapping them to a unified timbre space for visualization. Audio reconstruction employs the Griffin-Lim phase estimation algorithm applied to mel spectrograms. The accompanying Three.js-based interface provides dual-viewport visualization comparing original and synthesized audio trajectories with independent playback controls. We demonstrate the framework's capabilities through comprehensive waveform analysis, spectrogram comparisons, and feature space evaluation using Principal Component Analysis (PCA). Quantitative evaluation shows mel spectrogram correlation scores exceeding 0.92, indicating high-fidelity preservation of perceptual acoustic structure. SoundPlot is released under the MIT License to facilitate research in bioacoustics, audio signal processing, and computational ethology.

</details>


### [795] [Performance and Complexity Trade-off Optimization of Speech Models During Training](https://arxiv.org/abs/2601.13704)
*Esteban Gómez,Tom Bäckström*

Main category: cs.SD

TL;DR: 提出基于特征噪声注入的重新参数化技术，可在训练中联合优化语音机器学习模型的性能和计算复杂度，通过案例研究证明有效。


<details>
  <summary>Details</summary>
Motivation: 传统语音机器学习中模型层大小启发式选择，无法保证性能和计算复杂度最优权衡，SGD方法难以优化非可微的计算复杂度因素。

Method: 提出基于特征噪声注入的重新参数化技术，用基于SGD的方法在训练中联合优化性能和计算复杂度，可动态优化模型大小。

Result: 通过合成示例和两个实际应用（语音活动检测和音频反欺骗）的案例研究，证明了方法的有效性。

Conclusion: 所提方法能在语音机器学习模型训练中实现性能和计算复杂度的联合优化，代码公开以促进进一步研究。

Abstract: In speech machine learning, neural network models are typically designed by choosing an architecture with fixed layer sizes and structure. These models are then trained to maximize performance on metrics aligned with the task's objective. While the overall architecture is usually guided by prior knowledge of the task, the sizes of individual layers are often chosen heuristically. However, this approach does not guarantee an optimal trade-off between performance and computational complexity; consequently, post hoc methods such as weight quantization or model pruning are typically employed to reduce computational cost. This occurs because stochastic gradient descent (SGD) methods can only optimize differentiable functions, while factors influencing computational complexity, such as layer sizes and floating-point operations per second (FLOP/s), are non-differentiable and require modifying the model structure during training. We propose a reparameterization technique based on feature noise injection that enables joint optimization of performance and computational complexity during training using SGD-based methods. Unlike traditional pruning methods, our approach allows the model size to be dynamically optimized for a target performance-complexity trade-off, without relying on heuristic criteria to select which weights or structures to remove. We demonstrate the effectiveness of our method through three case studies, including a synthetic example and two practical real-world applications: voice activity detection and audio anti-spoofing. The code related to our work is publicly available to encourage further research.

</details>


### [796] [Fusion Segment Transformer: Bi-Directional Attention Guided Fusion Network for AI-Generated Music Detection](https://arxiv.org/abs/2601.13647)
*Yumin Kim,Seonghyeon Go*

Main category: cs.SD

TL;DR: 针对全音频AI生成音乐检测难题，提出融合段Transformer模型，经实验验证优于先前模型。


<details>
  <summary>Details</summary>
Motivation: 生成式AI技术兴起使AI生成音乐版权问题凸显，现有工作主要针对短音频，全音频检测面临挑战且探索不足。

Method: 提出改进版的Segment Transformer即Fusion Segment Transformer，用不同特征提取器提取短音乐片段的内容嵌入，引入门控融合层增强架构以整合内容和结构信息。

Result: 在SONICS和AIME数据集上的实验表明，该方法优于先前模型和最新基线，在AI生成音乐检测中取得了最先进的成果。

Conclusion: 所提出的Fusion Segment Transformer能有效解决全音频AI生成音乐检测问题，具有较好的效果。

Abstract: With the rise of generative AI technology, anyone can now easily create and deploy AI-generated music, which has heightened the need for technical solutions to address copyright and ownership issues. While existing works mainly focused on short-audio, the challenge of full-audio detection, which requires modeling long-term structure and context, remains insufficiently explored. To address this, we propose an improved version of the Segment Transformer, termed the Fusion Segment Transformer. As in our previous work, we extract content embeddings from short music segments using diverse feature extractors. Furthermore, we enhance the architecture for full-audio AI-generated music detection by introducing a Gated Fusion Layer that effectively integrates content and structural information, enabling the capture of long-term context. Experiments on the SONICS and AIME datasets show that our approach outperforms the previous model and recent baselines, achieving state-of-the-art results in AI-generated music detection.

</details>


### [797] [ConceptCaps -- a Distilled Concept Dataset for Interpretability in Music Models](https://arxiv.org/abs/2601.14157)
*Bruno Sienkiewicz,Łukasz Neumann,Mateusz Modrzejewski*

Main category: cs.SD

TL;DR: 介绍ConceptCaps音乐数据集，其构建管道分离语义建模与文本生成，经多指标验证有效且代码和数据集开源。


<details>
  <summary>Details</summary>
Motivation: 现有音乐数据集缺乏概念解释方法所需的结构，标签稀疏、嘈杂或定义不明确。

Method: 构建包含23k个音乐 - 字幕 - 音频三元组的ConceptCaps数据集，管道中VAE学习属性共现模式，LLM将属性列表转为描述，MusicGen合成音频。

Result: 数据集通过音频文本对齐、语言质量指标和TCAV分析验证，概念探针可恢复有音乐意义的模式。

Conclusion: 分离语义建模与文本生成的方法提高了连贯性和可控性，且数据集和代码可在线获取。

Abstract: Concept-based interpretability methods like TCAV require clean, well-separated positive and negative examples for each concept. Existing music datasets lack this structure: tags are sparse, noisy, or ill-defined. We introduce ConceptCaps, a dataset of 23k music-caption-audio triplets with explicit labels from a 200-attribute taxonomy. Our pipeline separates semantic modeling from text generation: a VAE learns plausible attribute co-occurrence patterns, a fine-tuned LLM converts attribute lists into professional descriptions, and MusicGen synthesizes corresponding audio. This separation improves coherence and controllability over end-to-end approaches. We validate the dataset through audio-text alignment (CLAP), linguistic quality metrics (BERTScore, MAUVE), and TCAV analysis confirming that concept probes recover musically meaningful patterns. Dataset and code are available online.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [798] [Proc3D: Procedural 3D Generation and Parametric Editing of 3D Shapes with Large Language Models](https://arxiv.org/abs/2601.12234)
*Fadlullah Raji,Stefano Petrangeli,Matheus Gadelha,Yu Shen,Uttaran Bhattacharya,Gang Wu*

Main category: cs.GR

TL;DR: 论文介绍了Proc3D系统，用于生成可编辑3D模型，支持实时修改，实验显示其在编辑效率和一致性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统3D模型生成复杂，现有生成式AI方法产出不可编辑表示，限制迭代设计适应性。

Method: 引入程序紧凑图（PCG）表示3D模型，结合GPT - 4o与上下文学习、微调的LLAMA - 3两种生成方法。

Result: Proc3D编辑效率比传统方法快超400倍，ULIP得分提高28%。

Conclusion: Proc3D能实现对齐文本的3D模型生成和实时参数编辑，便于高精度基于文本的图像编辑应用。

Abstract: Generating 3D models has traditionally been a complex task requiring specialized expertise. While recent advances in generative AI have sought to automate this process, existing methods produce non-editable representation, such as meshes or point clouds, limiting their adaptability for iterative design. In this paper, we introduce Proc3D, a system designed to generate editable 3D models while enabling real-time modifications. At its core, Proc3D introduces procedural compact graph (PCG), a graph representation of 3D models, that encodes the algorithmic rules and structures necessary for generating the model. This representation exposes key parameters, allowing intuitive manual adjustments via sliders and checkboxes, as well as real-time, automated modifications through natural language prompts using Large Language Models (LLMs). We demonstrate Proc3D's capabilities using two generative approaches: GPT-4o with in-context learning (ICL) and a fine-tuned LLAMA-3 model. Experimental results show that Proc3D outperforms existing methods in editing efficiency, achieving more than 400x speedup over conventional approaches that require full regeneration for each modification. Additionally, Proc3D improves ULIP scores by 28%, a metric that evaluates the alignment between generated 3D models and text prompts. By enabling text-aligned 3D model generation along with precise, real-time parametric edits, Proc3D facilitates highly accurate text-based image editing applications.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [799] [Nixie: Efficient, Transparent Temporal Multiplexing for Consumer GPUs](https://arxiv.org/abs/2601.11743)
*Yechen Xu,Yifei Wang,Nathanael Ren,Yiran Chen,Danyang Zhuo*

Main category: cs.OS

TL;DR: 论文提出Nixie系统，实现消费级GPU高效透明的时间复用，提升交互式任务性能并节省CPU固定内存。


<details>
  <summary>Details</summary>
Motivation: 消费级GPU运行多应用时，现有共享机制因内存抖动和CPU固定内存过度使用而性能不佳。

Method: 设计实现Nixie系统服务，协调GPU内存分配和内核启动行为，用轻量级调度器优先处理低延迟交互式任务。

Result: Nixie将交互式代码完成任务的延迟最多降低3.8倍，在相同延迟要求下最多节省66.8%的CPU固定内存使用。

Conclusion: Nixie能在不改变应用或驱动的情况下，有效提升消费级GPU多应用运行效率。

Abstract: Consumer machines are increasingly running large ML workloads such as large language models (LLMs), text-to-image generation, and interactive image editing. Unlike datacenter GPUs, consumer GPUs serve single-user, rapidly changing workloads, and each model's working set often nearly fills the GPU memory. As a result, existing sharing mechanisms (e.g., NVIDIA Unified Virtual Memory) perform poorly due to memory thrashing and excessive use of CPU pinned memory when multiple applications are active.
  We design and implement Nixie, a system that enables efficient and transparent temporal multiplexing on consumer GPUs without requiring any application or driver changes. Nixie is a system service that coordinates GPU memory allocation and kernel launch behavior to efficiently utilize the CPU-GPU bi-directional bandwidth and CPU pinned memory. A lightweight scheduler in Nixie further improves responsiveness by automatically prioritizing latency-sensitive interactive jobs using MLFQ-inspired techniques. Our evaluations show that Nixie improves latency of real interactive code-completion tasks by up to $3.8\times$ and saves up to 66.8% CPU pinned memory usage given the same latency requirement.

</details>


### [800] [ContiguousKV: Accelerating LLM Prefill with Granularity-Aligned KV Cache Management](https://arxiv.org/abs/2601.13631)
*Jing Zou,Shangyu Wu,Hancong Duan,Qiao Li,Chun Jason Xue*

Main category: cs.OS

TL;DR: 本文提出ContiguousKV系统加速大语言模型Re - Prefill阶段，在Qwen2.5系列模型上比IMPRESS快3.85倍。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型前缀KV缓存卸载在Re - Prefill阶段存在I/O瓶颈，如读放大和I/O与计算闲置等问题，影响效率。

Method: 引入统一数据管理粒度ContiguousChunk消除读放大；利用层间重要ContiguousChunk索引高相似性，提出异步预取打破I/O与计算的顺序依赖；采用注意力引导缓存管理保留关键数据。

Result: 在Qwen2.5系列模型评估中，ContiguousKV在Re - Prefill阶段比IMPRESS提速3.85倍，且保持高输出质量。

Conclusion: ContiguousKV能有效加速大语言模型的Re - Prefill阶段，提升I/O效率。

Abstract: Efficiently serving Large Language Models (LLMs) with persistent Prefix Key-Value (KV) Cache is critical for applications like conversational search and multi-turn dialogue. Serving a request requires loading the pre-computed prefix KV cache and generating the first token, defined as the Re-Prefill Phase. Offloading this shared prefix cache to secondary storage is essential for memory scalability. Re-Prefill with offloading suffers from severe I/O bottlenecks in two aspects. First, semantic-aware KV cache pruning algorithms select important tokens in fine granularity, while systems manage I/O in coarse, fixed-size blocks, causing severe read amplification. Second, the sequential dependency between identifying important tokens and loading KV cache creates idle I/O and compute bubbles, under-utilizing system resources.
  This paper proposes \textit{ContiguousKV}, a high-performance prefix KV cache offloading system that bridges algorithmic semantics with I/O efficiency to accelerate the Re-Prefill phase. We first introduce \textit{ContiguousChunk}, a unified data management granularity that aligns KV cache pruning with I/O operations. All the mechanisms critical for I/O performance are performed at the granularity of ContiguousChunk, thereby eliminating read amplification. By exploiting the high similarity in important ContiguousChunk indices across layers, we propose intra- and inter-period asynchronous prefetching to break the sequential dependency between I/O and compute, effectively eliminating idle bubbles. Finally, we propose attention-guided cache management to retain semantically critical prefix data in memory. Evaluations on Qwen2.5 series models show that ContiguousKV achieves a 3.85x speedup in the Re-Prefill phase over the state-of-the-art offloading system IMPRESS, while maintaining high output quality.

</details>


### [801] ["Range as a Key" is the Key! Fast and Compact Cloud Block Store Index with RASK](https://arxiv.org/abs/2601.14129)
*Haoru Zhao,Mingkai Dong,Erci Xu,Zhongyu Wang,Haibo Chen*

Main category: cs.OS

TL;DR: 云块存储中索引内存占用大，本文提出RASK索引，可高效处理范围重叠和碎片化，评估显示其内存占用最多降98.9%，吞吐量最多提升31.0倍。


<details>
  <summary>Details</summary>
Motivation: 云块存储中索引是内存主要消耗者，造成内存紧张，需要节省内存的索引方法。

Method: 提出RASK索引，用日志结构叶子处理范围重叠，采用范围感知的拆分和合并机制减少范围碎片化。

Result: 在四个生产跟踪数据上评估，RASK与十个先进索引相比，内存占用最多降98.9%，吞吐量最多提升31.0倍。

Conclusion: RASK是一种内存高效且高性能的树状索引，能有效节省内存并提高吞吐量。

Abstract: In cloud block store, indexing is on the critical path of I/O operations and typically resides in memory. With the scaling of users and the emergence of denser storage media, the index has become a primary memory consumer, causing memory strain. Our extensive analysis of production traces reveals that write requests exhibit a strong tendency to target continuous block ranges in cloud storage systems. Thus, compared to current per-block indexing, our insight is that we should directly index block ranges (i.e., range-as-a-key) to save memory.
  In this paper, we propose RASK, a memory-efficient and high-performance tree-structured index that natively indexes ranges. While range-as-a-key offers the potential to save memory and improve performance, realizing this idea is challenging due to the range overlap and range fragmentation issues. To handle range overlap efficiently, RASK introduces the log-structured leaf, combined with range-tailored search and garbage collection. To reduce range fragmentation, RASK employs range-aware split and merge mechanisms. Our evaluations on four production traces show that RASK reduces memory footprint by up to 98.9% and increases throughput by up to 31.0x compared to ten state-of-the-art indexes.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [802] [Robust semi-parametric mixtures of linear experts using the contaminated Gaussian distribution](https://arxiv.org/abs/2601.12425)
*Peterson Mambondimumwe,Sphiwe B. Skhosana,Najmeh Nakhaei Rad*

Main category: stat.ME

TL;DR: 提出半和非参数污染高斯混合回归模型，用两种算法估计参数，经模拟研究和实际数据验证。


<details>
  <summary>Details</summary>
Motivation: 现有半和非参数回归混合模型基于高斯假设，估计对离群值和重尾误差分布敏感，需要稳健估计。

Method: 提出半和非参数污染高斯混合回归模型，使用EM类算法和ECM类算法进行最大似然和局部似然核估计。

Result: 通过广泛模拟研究检验了模型稳健性，用实际数据证明了模型实用性。

Conclusion: 所提模型能在有轻度离群值时稳健估计模型参数，并可同时进行观测值聚类和离群值检测。

Abstract: Semi- and non-parametric mixture of regressions are a very useful flexible class of mixture of regressions in which some or all of the parameters are non-parametric functions of the covariates. These models are, however, based on the Gaussian assumption of the component error distributions. Thus, their estimation is sensitive to outliers and heavy-tailed error distributions. In this paper, we propose semi- and non-parametric contaminated Gaussian mixture of regressions to robustly estimate the parametric and/or non-parametric terms of the models in the presence of mild outliers. The virtue of using a contaminated Gaussian error distribution is that we can simultaneously perform model-based clustering of observations and model-based outlier detection. We propose two algorithms, an expectation-maximization (EM)-type algorithm and an expectation-conditional-maximization (ECM)-type algorithm, to perform maximum likelihood and local-likelihood kernel estimation of the parametric and non-parametric of the proposed models, respectively. The robustness of the proposed models is examined using an extensive simulation study. The practical utility of the proposed models is demonstrated using real data.

</details>


### [803] [Modeling Zero-Inflated Longitudinal Circular Data Using Bayesian Methods: Application to Ophthalmology](https://arxiv.org/abs/2601.13998)
*Prajamitra Bhuyan,Soutik Halder,Jayant Jha*

Main category: stat.ME

TL;DR: 本文在纵向框架下对含过多零值的圆形数据建模，提出混合效应两阶段模型，用贝叶斯方法估计参数，模拟和实际数据显示方法有效。


<details>
  <summary>Details</summary>
Motivation: 现有文献缺少纵向研究中处理零膨胀圆形观测值的模型，受实际案例启发进行研究。

Method: 提出基于投影正态分布的混合效应两阶段模型，用基于吉布斯抽样技术的贝叶斯方法估计模型参数。

Result: 模拟结果显示该方法在多种情况下优于竞争对手，对术后散光实际数据集的分析展示了方法的实际应用。

Conclusion: 所提方法有助于治疗选择和后续阶段的有效决策。

Abstract: This paper introduces the modeling of circular data with excess zeros under a longitudinal framework, where the response is a circular variable and the covariates can be both linear and circular in nature. In the literature, various circular-circular and circular-linear regression models have been studied and applied to different real-world problems. However, there are no models for addressing zero-inflated circular observations in the context of longitudinal studies. Motivated by a real case study, a mixed-effects two-stage model based on the projected normal distribution is proposed to handle such issues. The interpretation of the model parameters is discussed and identifiability conditions are derived. A Bayesian methodology based on Gibbs sampling technique is developed for estimating the associated model parameters. Simulation results show that the proposed method outperforms its competitors in various situations. A real dataset on post-operative astigmatism is analyzed to demonstrate the practical implementation of the proposed methodology. The use of the proposed method facilitates effective decision-making for treatment choices and in the follow-up phases.

</details>


### [804] [On Nonasymptotic Confidence Intervals for Treatment Effects in Randomized Experiments](https://arxiv.org/abs/2601.11744)
*Ricardo J. Sandoval,Sivaraman Balakrishnan,Avi Feller,Michael I. Jordan,Ian Waudby-Smith*

Main category: stat.ME

TL;DR: 研究随机实验中治疗效果的非渐近置信区间，缩小与渐近对应区间的性能差距并证明所得非渐近率不可改进。


<details>
  <summary>Details</summary>
Motivation: 现有文献中，非渐近置信区间的有效样本量比基于中心极限定理的对应区间宽松，存在性能差距。

Method: 系统地利用负相关性或方差适应性（或两者）来设计非渐近置信区间。

Result: 设计出的非渐近置信区间与渐近对应区间具有相同的有效样本量。

Conclusion: 所实现的非渐近率在信息论意义上不可改进。

Abstract: We study nonasymptotic (finite-sample) confidence intervals for treatment effects in randomized experiments. In the existing literature, the effective sample sizes of nonasymptotic confidence intervals tend to be looser than the corresponding central-limit-theorem-based confidence intervals by a factor depending on the square root of the propensity score. We show that this performance gap can be closed, designing nonasymptotic confidence intervals that have the same effective sample size as their asymptotic counterparts. Our approach involves systematic exploitation of negative dependence or variance adaptivity (or both). We also show that the nonasymptotic rates that we achieve are unimprovable in an information-theoretic sense.

</details>


### [805] [Lost in Aggregation: The Causal Interpretation of the IV Estimand](https://arxiv.org/abs/2601.12120)
*Danielle Tsao,Krikamol Muandet,Frederick Eberhardt,Emilija Perković*

Main category: stat.ME

TL;DR: 指出工具变量法在处理聚合处理变量时的问题，分析识别聚合效应的条件及局限性。


<details>
  <summary>Details</summary>
Motivation: 解决工具变量法中证明工具有效性的难题，关注聚合处理变量问题。

Method: 分析聚合处理变量因果效应的模糊性，通过聚合约束组件干预分布形式化，刻画标准工具变量估计量识别聚合效应的条件。

Result: 发现聚合处理因果效应通常模糊，识别聚合效应的条件较牵强。

Conclusion: 基于聚合处理的工具变量估计解释有重大局限，需更广泛理由支持排除限制。

Abstract: Instrumental variable based estimation of a causal effect has emerged as a standard approach to mitigate confounding bias in the social sciences and epidemiology, where conducting randomized experiments can be too costly or impossible. However, justifying the validity of the instrument often poses a significant challenge. In this work, we highlight a problem generally neglected in arguments for instrumental variable validity: the presence of an ''aggregate treatment variable'', where the treatment (e.g., education, GDP, caloric intake) is composed of finer-grained components that each may have a different effect on the outcome. We show that the causal effect of an aggregate treatment is generally ambiguous, as it depends on how interventions on the aggregate are instantiated at the component level, formalized through the aggregate-constrained component intervention distribution. We then characterize conditions on the interventional distribution and the aggregate setting under which standard instrumental variable estimators identify the aggregate effect. The contrived nature of these conditions implies major limitations on the interpretation of instrumental variable estimates based on aggregate treatments and highlights the need for a broader justificatory base for the exclusion restriction in such settings.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [806] [Learning Deterministic Finite-State Machines from the Prefixes of a Single String is NP-Complete](https://arxiv.org/abs/2601.12621)
*Radu Cosmin Dumitru,Ryo Yoshinaka,Ayumi Shinohara*

Main category: cs.FL

TL;DR: 研究输入样本为前缀封闭时计算最小DFA的计算复杂度，证明在特定样本集下问题NP难近似及作为决策问题NP难，结论也适用于Mealy机。


<details>
  <summary>Details</summary>
Motivation: 已有工作识别了输入样本条件使计算最小DFA问题变得易处理或仍困难，本文研究输入样本为前缀封闭时的计算复杂度。

Method: 分析输入样本为所有二进制字符串前缀及单个二进制字符串前缀的情况。

Result: 当样本集为所有二进制字符串前缀时问题NP难近似，样本集为单个二进制字符串前缀时作为决策问题NP难，结论也适用于Mealy机。

Conclusion: 输入样本为前缀封闭时计算最小DFA问题在特定情况下NP难，且结论可扩展到Mealy机。

Abstract: It is well known that computing a minimum DFA consistent with a given set of positive and negative examples is NP-hard. Previous work has identified conditions on the input sample under which the problem becomes tractable or remains hard. In this paper, we study the computational complexity of the case where the input sample is prefix-closed. This formulation is equivalent to computing a minimum Moore machine consistent with observations along its runs. We show that the problem is NP-hard to approximate when the sample set consists of all prefixes of binary strings. Furthermore, we show that the problem remains NP-hard as a decision problem even when the sample set consists of the prefixes of a single binary string. Our argument also extends to the corresponding problem for Mealy machines.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [807] [A Proof of Concept for a Digital Twin of an Ultrasonic Fermentation System](https://arxiv.org/abs/2601.11723)
*Francesco Saverio Sconocchia Pisoni,Andrea Vitaletti,Davide Appolloni,Federico Ortenzi,Blasco Morozzo della Rocca,Mariano José Guillén,Alessandro Contaldo*

Main category: cs.ET

TL;DR: 本文设计并实现了创新超声增强啤酒发酵系统的概念验证数字孪生，结果证明方法可行。


<details>
  <summary>Details</summary>
Motivation: 开发创新超声增强啤酒发酵系统的数字孪生，以实现酵母生长环境的智能监测、预测和驱动。

Method: 在传统发酵罐配备压电换能器，用超声波刺激酵母生长；采用并扩展Palacios等人提出的模型，以温度、超声频率和占空比为输入处理有限训练样本。

Result: 获得结果并评估模型性能，证明了所提方法的可行性。

Conclusion: 所提出的数字孪生设计和实现方法是可行的。

Abstract: This paper presents the design and implementation of a proof of concept digital twin for an innovative ultrasonic-enhanced beer-fermentation system, developed to enable intelligent monitoring, prediction, and actuation in yeast-growth environments. A traditional fermentation tank is equipped with a piezoelectric transducer able to irradiate the tank with ultrasonic waves, providing an external abiotic stimulus to enhance the growth of yeast and accelerate the fermentation process. At its core, the digital twin incorporates a predictive model that estimates yeast's culture density over time based on the surrounding environmental conditions. To this end, we implement, tailor and extend the model proposed in Palacios et al., allowing us to effectively handle the limited number of available training samples by using temperature, ultrasonic frequency, and duty cycle as inputs. The results obtained along with the assessment of model performance demonstrate the feasibility of the proposed approach.

</details>


### [808] [Bounded Minds, Generative Machines: Envisioning Conversational AI that Works with Human Heuristics and Reduces Bias Risk](https://arxiv.org/abs/2601.13376)
*Jiqun Liu*

Main category: cs.ET

TL;DR: 文章提出对话式AI应基于有限理性设计，与人类启发式方法协同，并指出相关研究方向。


<details>
  <summary>Details</summary>
Motivation: 当前多数对话式AI系统假设用户理想化，而实际人类推理受有限注意力、知识不均和启发式方法影响，存在局限性。

Method: 提出基于有限理性的研究路径，设计对话式AI与人类启发式方法协同。

Result: 确定了检测认知脆弱性、支持不确定下判断、超越事实准确性评价对话系统等关键方向。

Conclusion: 对话式AI的设计应与人类启发式方法合作，以提升决策质量和认知鲁棒性。

Abstract: Conversational AI is rapidly becoming a primary interface for information seeking and decision making, yet most systems still assume idealized users. In practice, human reasoning is bounded by limited attention, uneven knowledge, and reliance on heuristics that are adaptive but bias-prone. This article outlines a research pathway grounded in bounded rationality, and argues that conversational AI should be designed to work with human heuristics rather than against them. It identifies key directions for detecting cognitive vulnerability, supporting judgment under uncertainty, and evaluating conversational systems beyond factual accuracy, toward decision quality and cognitive robustness.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [809] [Large Language Model Agent for User-friendly Chemical Process Simulations](https://arxiv.org/abs/2601.11650)
*Jingkang Liang,Niklas Groll,Gürkan Sin*

Main category: physics.chem-ph

TL;DR: 本文提出将大语言模型代理与AVEVA Process Simulation集成，通过两个案例研究评估框架性能，虽有局限但LLM代理有潜力成为有价值的协作伙伴。


<details>
  <summary>Details</summary>
Motivation: 解决现代过程模拟器构建和解释耗时且需专业知识，限制非经验用户早期探索的问题。

Method: 通过Model Context Protocol将大语言模型代理与AVEVA Process Simulation集成，使用MCP服务器工具集让LLM用Python与APS编程通信，通过两个水 - 甲醇分离案例研究评估框架。

Result: 第一个案例中代理能自主分析流程图、优化并清晰呈现结果；第二个案例展示了不同对话模式下自主流程图合成能力。

Conclusion: 虽有局限性，但基于LLM的代理在分析、优化和引导构建方面有潜力成为有价值的协作伙伴。

Abstract: Modern process simulators enable detailed process design, simulation, and optimization; however, constructing and interpreting simulations is time-consuming and requires expert knowledge. This limits early exploration by inexperienced users. To address this, a large language model (LLM) agent is integrated with AVEVA Process Simulation (APS) via Model Context Protocol (MCP), allowing natural language interaction with rigorous process simulations. An MCP server toolset enables the LLM to communicate programmatically with APS using Python, allowing it to execute complex simulation tasks from plain-language instructions. Two water-methanol separation case studies assess the framework across different task complexities and interaction modes. The first shows the agent autonomously analyzing flowsheets, finding improvement opportunities, and iteratively optimizing, extracting data, and presenting results clearly. The framework benefits both educational purposes, by translating technical concepts and demonstrating workflows, and experienced practitioners by automating data extraction, speeding routine tasks, and supporting brainstorming. The second case study assesses autonomous flowsheet synthesis through both a step-by-step dialogue and a single prompt, demonstrating its potential for novices and experts alike. The step-by-step mode gives reliable, guided construction suitable for educational contexts; the single-prompt mode constructs fast baseline flowsheets for later refinement. While current limitations such as oversimplification, calculation errors, and technical hiccups mean expert oversight is still needed, the framework's capabilities in analysis, optimization, and guided construction suggest LLM-based agents can become valuable collaborators.

</details>


### [810] [onepot CORE -- an enumerated chemical space to streamline drug discovery, enabled by automated small molecule synthesis and AI](https://arxiv.org/abs/2601.12603)
*Andrei S. Tyrin,Brandon Wang,Manuel Muñoz,Samuel H. Foxman,Daniil A. Boiko*

Main category: physics.chem-ph

TL;DR: 传统药物发现的‘制造’步骤受限，现有化学空间有局限。论文介绍了含34亿分子的onepot CORE化学空间，描述其构建方法、工作流程并验证效果，展示了快速获取多样小分子的途径。


<details>
  <summary>Details</summary>
Motivation: 早期药物发现中‘制造’步骤存在问题，现有枚举化学空间也有不足，需开发更好的方法提高获取多样小分子的速度和可靠性。

Method: 构建onepot CORE：选择常用反应集、筛选构建模块、枚举候选产物、用机器学习评估可行性；描述端到端工作流。

Result: 验证了操作指标（成功率、时间线等），通过NMR确认合成化合物，用DPP4抑制剂证明测定适用性。

Conclusion: onepot CORE展示了快速、可靠获取多样小分子的途径，有助于加速药物及其他领域的发现。

Abstract: The design-make-test-analyze cycle in early-stage drug discovery remains constrained primarily by the "make" step: small-molecule synthesis is slow, costly, and difficult to scale or automate across diverse chemotypes. Enumerated chemical spaces aim to reduce this bottleneck by predefining synthesizable regions of chemical space from available building blocks and reliable reactions, yet existing commercial spaces are still limited by long turnaround times, narrow reaction scope, and substantial manual decision-making in route selection and execution.
  Here we present the first version of onepot CORE, an enumerated chemical space containing 3.4B molecules and corresponding on-demand synthesis product enabled by an automated synthesis platform and an AI chemist, Phil, that designs, executes, and analyzes experiments. onepot CORE is constructed by (i) selecting a reaction set commonly used in medicinal chemistry, (ii) sourcing and curating building blocks from supplier catalogs, (iii) enumerating candidate products, and (iv) applying ML-based feasibility assessment to prioritize compounds for robust execution. In the current release, the space is supported by seven reactions.
  We describe an end-to-end workflow - from route selection and automated liquid handling through workup and purification. We further report validation across operational metrics (success rate, timelines, purity, and identity), including NMR confirmation for a representative set of synthesized compounds and assay suitability demonstrated using a series of DPP4 inhibitors. Collectively, onepot CORE illustrates a path toward faster, more reliable access to diverse small molecules, supporting accelerated discovery in pharmaceuticals and beyond.

</details>


### [811] [Reorienting off-path Nudged Elastic Bands (RONEB) via Minimum Mode Following](https://arxiv.org/abs/2601.12630)
*Rohit Goswami,Miha Gunde,Hannes Jónsson*

Main category: physics.chem-ph

TL;DR: 提出自适应混合算法RONEB确定过渡态，经基准测试效率提升，是高通量自动化学发现的有效工具。


<details>
  <summary>Details</summary>
Motivation: 现有双端方法计算成本高、易停滞，单端方法无法约束特定状态，需更优方法确定过渡态。

Method: 提出RONEB算法，结合NEB双端特性与单端Min - Mode Following方法的加速能力，通过路径优化历史、相对力触发和基于对齐的回退罚函数实现动态解耦。

Result: 与CI - NEB相比，梯度调用中位数减少46.3%，表面扩散测试中59种金属重排机制减少28%。

Conclusion: RONEB是高通量自动化学发现的高效工具。

Abstract: Accurate determination of transition states remains central to understanding reaction kinetics. Double-ended methods like the Nudged Elastic Band (NEB) ensure relevant transition states and paths, but incur high computational costs and suffer stagnation on flat or rough potential energy surfaces. Conversely, single-ended eigenmode-following techniques offer efficiency but cannot often be constrained between specific states. Here, we present the Reorienting Off-path Nudged Elastic Bands (RONEB), an adaptive hybrid algorithm that integrates the double ended nature of the NEB with the acceleration of single ended Min-Mode Following methods. RONEB provides stability based on the history of the path optimization, relative force triggering, and an alignment-based back-off penalty to dynamically decouple the climbing image from the elastic band constraints. We benchmark the method against the standard Climbing Image NEB (CI-NEB) across the Baker-Chan transition state test set using the PET-MAD machine-learned potential and the OptBench Pt(111) heptamer island surface diffusion set. A Bayesian analysis of the performance data quantifies a median reduction in gradient calls of 46.3% [95% CrI: -54.7%, -36.9%] relative to the baseline, while surface diffusion tests reveal a 28% reduction across 59 metallic rearrangement mechanisms. These results establish RONEB as a highly effective tool for high-throughput automated chemical discovery.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [812] [Rethinking the Value of Multi-Agent Workflow: A Strong Single Agent Baseline](https://arxiv.org/abs/2601.12307)
*Jiawei Xu,Arief Koesdwiady,Sisong Bei,Yan Han,Baixiang Huang,Dakuo Wang,Yutong Chen,Zheshen Wang,Peihao Wang,Pan Li,Ying Ding*

Main category: cs.MA

TL;DR: 研究单LLM代理能否模拟多代理系统工作流，发现单代理性能可与同质工作流匹敌，还提出OneFlow算法降低推理成本，同时指出单LLM方法存在局限。


<details>
  <summary>Details</summary>
Motivation: 多数LLM多代理系统框架为同质，引发能否用单代理模拟其工作流的疑问，从而开展研究。

Method: 在七个涵盖编码、数学等不同类型的基准测试中进行研究。

Result: 单代理可达到同质工作流性能且有KV缓存复用的效率优势，甚至能匹配自动优化的异质工作流性能；提出OneFlow算法降低推理成本。

Conclusion: 单LLM实现多代理工作流可作为多代理系统研究的强大基线；单LLM方法无法捕捉异质工作流，未来需开发真正的异质多代理系统。

Abstract: Recent advances in LLM-based multi-agent systems (MAS) show that workflows composed of multiple LLM agents with distinct roles, tools, and communication patterns can outperform single-LLM baselines on complex tasks. However, most frameworks are homogeneous, where all agents share the same base LLM and differ only in prompts, tools, and positions in the workflow. This raises the question of whether such workflows can be simulated by a single agent through multi-turn conversations. We investigate this across seven benchmarks spanning coding, mathematics, general question answering, domain-specific reasoning, and real-world planning and tool use. Our results show that a single agent can reach the performance of homogeneous workflows with an efficiency advantage from KV cache reuse, and can even match the performance of an automatically optimized heterogeneous workflow. Building on this finding, we propose \textbf{OneFlow}, an algorithm that automatically tailors workflows for single-agent execution, reducing inference costs compared to existing automatic multi-agent design frameworks without trading off accuracy. These results position the single-LLM implementation of multi-agent workflows as a strong baseline for MAS research. We also note that single-LLM methods cannot capture heterogeneous workflows due to the lack of KV cache sharing across different LLMs, highlighting future opportunities in developing \textit{truly} heterogeneous multi-agent systems.

</details>


### [813] [Communication Methods in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2601.12886)
*Christoph Wittner*

Main category: cs.MA

TL;DR: 本文对多智能体强化学习中的通信技术进行综述，分析29篇文献，比较不同通信方式优缺点，指出无通用最优框架，强调低计算开销通信方法的重要性，并讨论当前研究差距。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习领域引入了众多通信方法，本文旨在对这些通信技术进行综述。

Method: 深入分析29篇关于多智能体强化学习通信技术的文献。

Result: 比较结果表明，没有适用于所有问题的通用最优通信框架，通信方式的选择很大程度取决于具体问题，同时强调低计算开销通信方法对可扩展性的重要性。

Conclusion: 讨论当前研究差距，强调需要对系统级指标进行标准化基准测试，并提高在现实通信条件下的鲁棒性，以增强这些方法的实际应用能力。

Abstract: Multi-agent reinforcement learning is a promising research area that extends established reinforcement learning approaches to problems formulated as multi-agent systems. Recently, a multitude of communication methods have been introduced to this field to address problems such as partially observable environments, non-stationarity, and exponentially growing action spaces. Communication further enables efficient cooperation among all agents interacting in an environment. This work aims at providing an overview of communication techniques in multi-agent reinforcement learning. By an in-depth analysis of 29 publications on this topic, the strengths and weaknesses of explicit, implicit, attention-based, graph-based, and hierarchical/role-based communication are evaluated. The results of this comparison show that there is no general, optimal communication framework for every problem. On the contrary, the choice of communication depends heavily on the problem at hand. The comparison also highlights the importance of communication methods with low computational overhead to enable scalability to environments where many agents interact. Finally, the paper discusses current research gaps, emphasizing the need for standardized benchmarking of system-level metrics and improved robustness under realistic communication conditions to enhance the real-world applicability of these approaches.

</details>


### [814] [OFA-MAS: One-for-All Multi-Agent System Topology Design based on Mixture-of-Experts Graph Generative Models](https://arxiv.org/abs/2601.12996)
*Shiyuan Li,Yixin Liu,Yu Zheng,Mei Li,Quoc Viet Hung Nguyen,Shirui Pan*

Main category: cs.MA

TL;DR: 提出OFA - TAD框架为多智能体系统生成自适应协作图，在多基准测试中表现优于专门模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于图学习的设计方法泛化能力差，无法利用不同任务间的共享结构知识，需要为多智能体系统设计适应不同跨域用户查询的自适应拓扑结构。

Method: 提出OFA - TAD框架，集成任务感知图状态编码器和专家混合架构，采用三阶段训练策略（无条件预训练、大规模条件预训练和监督微调）。

Result: 在六个不同基准测试中，OFA - TAD显著优于专门的一对一模型，生成高度自适应的多智能体系统拓扑。

Conclusion: OFA - TAD框架有效可行，能为多智能体系统生成自适应协作图。

Abstract: Multi-Agent Systems (MAS) offer a powerful paradigm for solving complex problems, yet their performance is critically dependent on the design of their underlying collaboration topology. As MAS become increasingly deployed in web services (e.g., search engines), designing adaptive topologies for diverse cross-domain user queries becomes essential. Current graph learning-based design methodologies often adhere to a "one-for-one" paradigm, where a specialized model is trained for each specific task domain. This approach suffers from poor generalization to unseen domains and fails to leverage shared structural knowledge across different tasks. To address this, we propose OFA-TAD, a one-for-all framework that generates adaptive collaboration graphs for any task described in natural language through a single universal model. Our approach integrates a Task-Aware Graph State Encoder (TAGSE) that filters task-relevant node information via sparse gating, and a Mixture-of-Experts (MoE) architecture that dynamically selects specialized sub-networks to drive node and edge prediction. We employ a three-stage training strategy: unconditional pre-training on canonical topologies for structural priors, large-scale conditional pre-training on LLM-generated datasets for task-topology mappings, and supervised fine-tuning on empirically validated graphs. Experiments across six diverse benchmarks show that OFA-TAD significantly outperforms specialized one-for-one models, generating highly adaptive MAS topologies. Code: https://github.com/Shiy-Li/OFA-MAS.

</details>


### [815] [The Orchestration of Multi-Agent Systems: Architectures, Protocols, and Enterprise Adoption](https://arxiv.org/abs/2601.13671)
*Apoorva Adimulam,Rajesh Gupta,Sumit Kumar*

Main category: cs.MA

TL;DR: 本文整合并形式化了编排式多智能体系统的技术构成，提出统一架构框架，详述两种通信协议，为企业级AI生态提供设计原则。


<details>
  <summary>Details</summary>
Motivation: 为适应人工智能发展，实现多智能体协作达成复杂共享目标。

Method: 提出统一架构框架，设计两种通信协议，详述编排逻辑、治理框架和可观测性机制。

Result: 建立了可互操作的通信基础，能实现分布式智能体集体的可扩展、可审计和符合政策的推理。

Conclusion: 将各元素合成连贯技术蓝图，为企业级AI生态连接了概念架构和可实施的设计原则。

Abstract: Orchestrated multi-agent systems represent the next stage in the evolution of artificial intelligence, where autonomous agents collaborate through structured coordination and communication to achieve complex, shared objectives. This paper consolidates and formalizes the technical composition of such systems, presenting a unified architectural framework that integrates planning, policy enforcement, state management, and quality operations into a coherent orchestration layer. Another primary contribution of this work is the in-depth technical delineation of two complementary communication protocols - the Model Context Protocol, which standardizes how agents access external tools and contextual data, and the Agent2Agent protocol, which governs peer coordination, negotiation, and delegation. Together, these protocols establish an interoperable communication substrate that enables scalable, auditable, and policy-compliant reasoning across distributed agent collectives. Beyond protocol design, the paper details how orchestration logic, governance frameworks, and observability mechanisms collectively sustain system coherence, transparency, and accountability. By synthesizing these elements into a cohesive technical blueprint, this paper provides comprehensive treatments of orchestrated multi-agent systems - bridging conceptual architectures with implementation-ready design principles for enterprise-scale AI ecosystems.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [816] [Multifaceted Scenario-Aware Hypergraph Learning for Next POI Recommendation](https://arxiv.org/abs/2601.11610)
*Yuxi Lin,Yongkang Li,Jie Xing,Zipei Fan*

Main category: cs.SI

TL;DR: 提出多面场景感知超图学习方法（MSAHG）用于下一兴趣点推荐，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于序列和图的方法忽略不同上下文场景下的移动性差异，存在无法捕捉特定场景特征和解决场景间冲突的问题。

Method: 采用场景拆分范式，构建特定场景、多视图解耦子超图，使用参数拆分机制。

Result: 在三个真实数据集上的实验表明，MSAHG在不同场景下始终优于五种最先进的方法。

Conclusion: MSAHG在多场景兴趣点推荐中有效。

Abstract: Among the diverse services provided by Location-Based Social Networks (LBSNs), Next Point-of-Interest (POI) recommendation plays a crucial role in inferring user preferences from historical check-in trajectories. However, existing sequential and graph-based methods frequently neglect significant mobility variations across distinct contextual scenarios (e.g., tourists versus locals). This oversight results in suboptimal performance due to two fundamental limitations: the inability to capture scenario-specific features and the failure to resolve inherent inter-scenario conflicts. To overcome these limitations, we propose the Multifaceted Scenario-Aware Hypergraph Learning method (MSAHG), a framework that adopts a scenario-splitting paradigm for next POI recommendation.
  Our main contributions are:
  (1) Construction of scenario-specific, multi-view disentangled sub-hypergraphs to capture distinct mobility patterns;
  (2) A parameter-splitting mechanism to adaptively resolve conflicting optimization directions across scenarios while preserving generalization capability.
  Extensive experiments on three real-world datasets demonstrate that MSAHG consistently outperforms five state-of-the-art methods across diverse scenarios, confirming its effectiveness in multi-scenario POI recommendation.

</details>


### [817] [The Tag is the Signal: URL-Agnostic Credibility Scoring for Messages on Telegram](https://arxiv.org/abs/2601.13294)
*Yipeng Wang,Huy Gia Han Vu,Mohit Singhal*

Main category: cs.SI

TL;DR: 现有方法难以处理Telegram上短且无链接的信息，本文提出TAG2CRED模型，利用标签直接打分，表现优于基线模型，集成模型性能进一步提升。


<details>
  <summary>Details</summary>
Motivation: 现有信息可信度分类方法在处理Telegram上短且URL稀少的高风险帖子时失败，需要新的方法。

Method: 提出TAG2CRED管道，使用简洁标签系统，通过微调大语言模型分配标签，用L2正则化逻辑回归映射为风险分数。

Result: TAG2CRED模型的ROC - AUC达0.871，macro - F1值0.787，Brier分数0.167，优于TF - IDF；集成模型性能进一步提升，ROC - AUC达0.901，macro - F1值0.813。

Conclusion: 风格标签和词法特征可捕捉不同但互补的信息风险维度，TAG2CRED及集成模型有效。

Abstract: Telegram has become one of the leading platforms for disseminating misinformational messages. However, many existing pipelines still classify each message's credibility based on the reputation of its associated domain names or its lexical features. Such methods work well on traditional long-form news articles published by well-known sources, but high-risk posts on Telegram are short and URL-sparse, leading to failures for link-based and standard TF-IDF models. To this end, we propose the TAG2CRED pipeline, a method designed for such short, convoluted messages. Our model will directly score each post based on the tags assigned to the text. We designed a concise label system that covers the dimensions of theme, claim type, call to action, and evidence. The fine-tuned large language model (LLM) assigns tags to messages and then maps these tags to calibrated risk scores in the [0,1] interval through L2-regularized logistic regression. We evaluated 87,936 Telegram messages associated with Media Bias/Fact Check (MBFC), using URL masking and domain disjoint splits. The results showed that the ROC-AUC of the TAG2CRED model reached 0.871, the macro-F1 value was 0.787, and the Brier score was 0.167, outperforming the baseline TF-IDF (macro-F1 value 0.737, Brier score 0.248); at the same time, the number of features used in this model is much smaller, and the generalization ability on infrequent domains is stronger. The performance of the stacked ensemble model (TF-IDF + TAG2CRED + SBERT) was further improved over the baseline SBERT. ROC-AUC reached 0.901, and the macro-F1 value was 0.813 (Brier score 0.114). This indicates that style labels and lexical features may capture different but complementary dimensions of information risk.

</details>


### [818] [The Hidden Toll of Social Media News: Causal Effects on Psychosocial Wellbeing](https://arxiv.org/abs/2601.13487)
*Olivia Pal,Agam Goyal,Eshwar Chandrasekharan,Koustuv Saha*

Main category: cs.SI

TL;DR: 利用BlueSky平台大数据做准实验研究新闻参与对心理社会结果的影响，发现新闻参与有得有失且与参与形式有关，研究拓展理论并给出启示。


<details>
  <summary>Details</summary>
Motivation: 当前不同形式新闻参与对心理社会结果的影响尚不明确，需开展研究填补空白。

Method: 利用BlueSky平台约2600万条帖子和4500万条评论的大数据集，进行准实验研究，用分层倾向得分分析匹配处理组和对照组，考察情感、行为和认知方面的心理社会幸福感。

Result: 新闻参与有正负影响，如增加抑郁、压力和焦虑，但减少孤独感和增加社交互动；新闻书签操作比评论或引用更易导致心理变差，且反复接触影响更显著。

Conclusion: 该研究将新闻效果理论拓展到非危机框架，表明常规新闻消费因参与类型产生不同心理动态，并对降低社交媒体新闻消费心理成本的工具和干预措施有启示。

Abstract: News consumption on social media has become ubiquitous, yet how different forms of engagement shape psychosocial outcomes remains unclear. To address this gap, we leveraged a large-scale dataset of ~26M posts and ~45M comments on the BlueSky platform, and conducted a quasi-experimental study, matching 81,345 Treated users exposed to News feeds with 83,711 Control users using stratified propensity score analysis. We examined psychosocial wellbeing, in terms of affective, behavioral, and cognitive outcomes. Our findings reveal that news engagement produces systematic trade-offs: increased depression, stress, and anxiety, yet decreased loneliness and increased social interaction on the platform. Regression models reveal that News feed bookmarking is associated with greater psychosocial deterioration compared to commenting or quoting, with magnitude differences exceeding tenfold. These per-engagement effects accumulate with repeated exposure, showing significant psychosocial impacts. Our work extends theories of news effects beyond crisis-centric frameworks by demonstrating that routine consumption creates distinct psychological dynamics depending on engagement type, and bears implications for tools and interventions for mitigating the psychosocial costs of news consumption on social media.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [819] [The Dynamic and Endogenous Behavior of Re-Offense Risk: An Agent-Based Simulation Study of Treatment Allocation in Incarceration Diversion Programs](https://arxiv.org/abs/2601.12441)
*Chuwen Zhang,Pengyi Shi,Amy Ward*

Main category: cs.CY

TL;DR: 本文提出新框架评估不同容量约束和监禁环境下的治疗分配政策，发现无单一优先政策占优，政策有效性取决于时间窗口和系统参数，强调将基于风险的决策系统视为社会技术系统进行评估。


<details>
  <summary>Details</summary>
Motivation: 现有风险评估工具将风险视为静态个体属性，忽视风险随时间演变及治疗决策通过社会互动对结果的影响，需新方法辅助政策制定。

Method: 开发将再犯风险建模为人 - 系统交互的新框架，使用基于美国缓刑数据校准的基于代理的模拟方法。

Result: 没有单一的优先政策占主导，政策有效性取决于时间窗口和系统参数，长期看优先考虑低风险个体效果更好，短期或监禁导致监测期短时优先考虑高风险个体更有效。

Conclusion: 应将基于风险的决策系统作为具有长期问责制的社会技术系统进行评估，而非孤立的预测工具。

Abstract: Incarceration-diversion treatment programs aim to improve societal reintegration and reduce recidivism, but limited capacity forces policymakers to make prioritization decisions that often rely on risk assessment tools. While predictive, these tools typically treat risk as a static, individual attribute, which overlooks how risk evolves over time and how treatment decisions shape outcomes through social interactions. In this paper, we develop a new framework that models reoffending risk as a human-system interaction, linking individual behavior with system-level dynamics and endogenous community feedback. Using an agent-based simulation calibrated to U.S. probation data, we evaluate treatment allocation policies under different capacity constraints and incarceration settings. Our results show that no single prioritization policy dominates. Instead, policy effectiveness depends on temporal windows and system parameters: prioritizing low-risk individuals performs better when long-term trajectories matter, while prioritizing high-risk individuals becomes more effective in the short term or when incarceration leads to shorter monitoring periods. These findings highlight the need to evaluate risk-based decision systems as sociotechnical systems with long-term accountability, rather than as isolated predictive tools.

</details>


### [820] [Overview of the SciHigh Track at FIRE 2025: Research Highlight Generation from Scientific Papers](https://arxiv.org/abs/2601.11582)
*Tohida Rehman,Debarshi Kumar Sanyal,Samiran Chattopadhyay*

Main category: cs.CY

TL;DR: 文章聚焦从科学论文摘要自动生成亮点的任务，12个团队参与，用多种方法和指标评估，结果表明自动生成亮点有诸多好处并提供了基准。


<details>
  <summary>Details</summary>
Motivation: 评估计算模型以简洁形式生成能体现论文关键贡献、发现和新颖性亮点的有效性，帮助读者快速掌握要点。

Method: 使用MixSub数据集，12个团队采用多种方法（包括预训练语言模型）生成亮点，用ROUGE、METEOR和BERTScore等指标评估，按ROUGE - L分数排名。

Result: 自动生成的亮点可减少阅读负担、加速文献综述、增强数字图书馆和学术搜索平台的元数据。

Conclusion: SciHigh为从科学写作中准确简洁地生成亮点的方法提供了专门的基准。

Abstract: `SciHigh: Research Highlight Generation from Scientific Papers' focuses on the task of automatically generating concise, informative, and meaningful bullet-point highlights directly from scientific abstracts. The goal of this task is to evaluate how effectively computational models can generate highlights that capture the key contributions, findings, and novelty of a paper in a concise form. Highlights help readers grasp essential ideas quickly and are often easier to read and understand than longer paragraphs, especially on mobile devices. The track uses the MixSub dataset \cite{10172215}, which provides pairs of abstracts and corresponding author-written highlights.
  In this inaugural edition of the track, 12 teams participated, exploring various approaches, including pre-trained language models, to generate highlights from this scientific dataset. All submissions were evaluated using established metrics such as ROUGE, METEOR, and BERTScore to measure both alignment with author-written highlights and overall informativeness. Teams were ranked based on ROUGE-L scores. The findings suggest that automatically generated highlights can reduce reading effort, accelerate literature reviews, and enhance metadata for digital libraries and academic search platforms. SciHigh provides a dedicated benchmark for advancing methods aimed at concise and accurate highlight generation from scientific writing.

</details>


### [821] [Bit-politeia: An AI Agent Community in Blockchain](https://arxiv.org/abs/2601.11583)
*Xing Yang*

Main category: cs.CY

TL;DR: 论文针对现有资源分配范式局限，提出区块链上的AI代理社区Bit - politeia构建公平高效可持续资源分配系统，介绍其架构、交互方式和激励机制等，为科研创新资源配置提供新途径。


<details>
  <summary>Details</summary>
Motivation: 现有资源分配范式存在如马太效应、古德哈特定律引发的奖励操纵及效率与公平权衡等问题，需构建新的资源分配系统。

Method: 提出区块链上的AI代理社区Bit - politeia，采用“集群分组+分层架构”结合民主集中制，代理通过聊天和审议互动评估研究成果并分配虚拟货币奖励，利用区块链技术记录交易和声誉数据。

Result: 该系统可利用AI客观评估和去中心化验证，减少人为偏见，缓解传统同行评审中的资源集中问题。

Conclusion: Bit - politeia框架为通过公平自动化资源配置优化科研创新提供了新途径。

Abstract: Current resource allocation paradigms, particularly in academic evaluation, are constrained by inherent limitations such as the Matthew Effect, reward hacking driven by Goodhart's Law, and the trade-off between efficiency and fairness. To address these challenges, this paper proposes "Bit-politeia", an AI agent community on blockchain designed to construct a fair, efficient, and sustainable resource allocation system. In this virtual community, residents interact via AI agents serving as their exclusive proxies, which are optimized for impartiality and value alignment. The community adopts a "clustered grouping + hierarchical architecture" that integrates democratic centralism to balance decision-making efficiency and trust mechanisms. Agents engage through casual chat and deliberative interactions to evaluate research outputs and distribute a virtual currency as rewards. This incentive mechanism aims to achieve incentive compatibility through consensus-driven evaluation, while blockchain technology ensures immutable records of all transactions and reputation data. By leveraging AI for objective assessment and decentralized verification, Bit-politeia minimizes human bias and mitigates resource centralization issues found in traditional peer review. The proposed framework provides a novel pathway for optimizing scientific innovation through a fair and automated resource configuration process.

</details>


### [822] [Let Me Try Again: Examining Replay Behavior by Tracing Students' Latent Problem-Solving Pathways](https://arxiv.org/abs/2601.11586)
*Shan Zhang,Siddhartha Pradhan,Ji-Eun Lee,Ashish Gurung,Anthony F. Botelho*

Main category: cs.CY

TL;DR: 利用马尔可夫链和隐马尔可夫模型分析777名七年级学生游戏学习日志数据，发现游戏中重玩对学习结果的影响取决于时机，即时重玩更有益。


<details>
  <summary>Details</summary>
Motivation: 前人研究未充分探究学生游戏学习中解决问题路径的顺序展开方式、重玩及策略与学习结果的关系，本研究旨在填补该空白。

Method: 对777名七年级学生在游戏平台的日志数据使用马尔可夫链和隐马尔可夫模型进行分析，结合回归分析。

Result: 问题序列内学生倾向持续状态或成功后立即重玩；隐马尔可夫模型得出四种潜在状态；重玩占主导和以最优方式结束的状态对学习结果预测更好；即时重玩支持学习结果，延迟重玩关联弱或负相关。

Conclusion: 数字学习中重玩并非都有益，其效果取决于时机，即时重玩支持灵活性和更高效探索。

Abstract: Prior research has shown that students' problem-solving pathways in game-based learning environments reflect their conceptual understanding, procedural knowledge, and flexibility. Replay behaviors, in particular, may indicate productive struggle or broader exploration, which in turn foster deeper learning. However, little is known about how these pathways unfold sequentially across problems or how the timing of replays and other problem-solving strategies relates to proximal and distal learning outcomes. This study addresses these gaps using Markov Chains and Hidden Markov Models (HMMs) on log data from 777 seventh graders playing the game-based learning platform of From Here to There!. Results show that within problem sequences, students often persisted in states or engaged in immediate replay after successful completions, while across problems, strong self-transitions indicated stable strategic pathways. Four latent states emerged from HMMs: Incomplete-dominant, Optimal-ending, Replay, and Mixed. Regression analyses revealed that engagement in replay-dominant and optimal-ending states predicted higher conceptual knowledge, flexibility, and performance compared with the Incomplete-dominant state. Immediate replay consistently supported learning outcomes, whereas delayed replay was weakly or negatively associated in relation to Non-Replay. These findings suggest that replay in digital learning is not uniformly beneficial but depends on timing, with immediate replay supporting flexibility and more productive exploration.

</details>


### [823] [Toward Youth-Centered Privacy-by-Design in Smart Devices: A Systematic Review](https://arxiv.org/abs/2601.11598)
*Molly Campbell,Mohamad Sheikho Al Jasem,Ajay Kumar Shrestha*

Main category: cs.CY

TL;DR: 文献综述使用PRISMA工作流评估AI智能设备中保护青少年隐私的框架、工具和政策，发现技术、政策和教育三方面存在问题，建议多方参与构建隐私生态。


<details>
  <summary>Details</summary>
Motivation: 评估AI智能设备中保护青少年的隐私设计框架、工具和政策。

Method: 采用PRISMA工作流，对过去十年主要学术和灰色文献库进行筛选，将文献分为三个主题类别分析。

Result: 技术干预可减少数据暴露但应用有限，政策框架有基线但执行和设计义务有差距，教育措施很少系统融入课程。文献偏向技术方案，其他领域存在实施差距。

Conclusion: 建议采用多利益相关者模式，共同开发包容、透明和因地制宜的隐私生态，为设计适合年轻用户的AI系统提供建议。

Abstract: This literature review evaluates privacy-by-design frameworks, tools, and policies intended to protect youth in AI-enabled smart devices using a PRISMA-guided workflow. Sources from major academic and grey-literature repositories from the past decade were screened. The search identified 2,216 records; after deduplication and screening, 645 articles underwent eligibility assessment, and 122 were included for analysis. The corpus was organized along three thematic categories: technical solutions, policy/regulatory measures, and education/awareness strategies. Findings reveal that while technical interventions such as on-device processing, federated learning, and lightweight encryption significantly reduce data exposure, their adoption remains limited. Policy frameworks, including the EU's GDPR, the UK Age-Appropriate Design Code, and Canada's PIPEDA, provide important baselines but are hindered by gaps in enforcement and age-appropriate design obligations, while educational initiatives are rarely integrated systematically into curricula. Overall, the corpus skews toward technical solutions (67%) relative to policy (21%) and education (12%), indicating an implementation gap outside the technical domain. To address these challenges, we recommend a multi-stakeholder model in which policymakers, manufacturers, and educators co-develop inclusive, transparent, and context-sensitive privacy ecosystems. This work advances discourse on youth data protection by offering empirically grounded insights and actionable recommendations for the design of ethical, privacy-preserving AI systems tailored to young users.

</details>


### [824] [Syllabic Agglutinative Tokenizations for Indonesian LLM: A Study from Gasing Literacy Learning System](https://arxiv.org/abs/2601.11643)
*H. Situngkir,A. B. Lumbantobing,Y. Surya*

Main category: cs.CY

TL;DR: 本文提出基于印尼语音节的大语言模型分词方法，在印尼语语料上效果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 开发适合印尼语的分词方法，尊重其黏着语形态，减少语言模型计算负担。

Method: 受教学法启发，结合信息论原理，先按音节边界分割，再用字节对编码，构建含3500个标记的词汇表。

Result: 在印尼语语料上，音节分词法Rényi效率达0.74，平均标记长度更长，词汇表更小。

Conclusion: 该方法融合教学法与计算优化原则，为形态丰富且资源少的语言提供了有前景的分词策略。

Abstract: This paper presents a novel syllable-based tokenization approach for Indonesian large language models, inspired by the Gasing Literacy Learning System's pedagogical methodology. Drawing on information-theoretic principles, we develop a tokenization framework that segments Indonesian text at syllable boundaries before applying byte-pair encoding, creating a vocabulary that aligns with the language's morphophonological structure. Our approach first identifies high-frequency syllables through rule-based segmentation, then constructs a compact vocabulary of 3,500 tokens that preserves meaningful linguistic units while maintaining coverage through character-level fallback. Empirical evaluation on Indonesian Wikipedia and folklore corpora from Indonesian Culture Digital Library (PDBI) demonstrates substantial improvements over conventional tokenization methods: the syllable-based approach achieves Rényi efficiency of 0.74 compared to 0.50-0.64 for pretrained multilingual tokenizers, while maintaining higher average token lengths (3.67 characters versus 2.72 for GPT-2) despite using a vocabulary an order of magnitude smaller. These gains emerge from the method's ability to internalize character-level dependencies within syllable units, reducing the computational burden on language models while respecting Indonesian's agglutinative morphology. We call the LLM built upon this principle, TOBA LLM (Tokenisasi Optimum Berbasis Aglutinasi), the convergence of human literacy pedagogy with computational optimization principles offers a promising paradigm for developing linguistically-informed tokenization strategies, particularly for morphologically rich and underrepresented languages in natural language processing.

</details>


### [825] [Unbounded Harms, Bounded Law: Liability in the Age of Borderless AI](https://arxiv.org/abs/2601.12646)
*Ha-Chi Tran*

Main category: cs.CY

TL;DR: 人工智能快速发展暴露风险治理缺陷，尤其是事后责任问题。本文借鉴高风险跨国领域机制，提出全球AI问责与赔偿架构，强调地缘政治与集体行动的矛盾。


<details>
  <summary>Details</summary>
Motivation: 人工智能风险治理中事后责任研究不足，特别是跨境危害和风险的责任分配、归属及补救效果缺乏理论和制度支持。

Method: 采用比较和跨学科方法，研究高风险跨国领域的赔偿和责任框架。

Result: 提炼出严格责任、风险共担等可转移的法律设计原则，并指出应用于AI相关危害的潜在结构约束。

Conclusion: 勾勒出全球AI问责与赔偿架构轮廓，强调地缘政治竞争与有效治理跨境AI风险集体行动之间的矛盾。

Abstract: The rapid proliferation of artificial intelligence (AI) has exposed significant deficiencies in risk governance. While ex-ante harm identification and prevention have advanced, Responsible AI scholarship remains underdeveloped in addressing ex-post liability. Core legal questions regarding liability allocation, responsibility attribution, and remedial effectiveness remain insufficiently theorized and institutionalized, particularly for transboundary harms and risks that transcend national jurisdictions. Drawing on contemporary AI risk analyses, we argue that such harms are structurally embedded in global AI supply chains and are likely to escalate in frequency and severity due to cross-border deployment, data infrastructures, and uneven national oversight capacities. Consequently, territorially bounded liability regimes are increasingly inadequate. Using a comparative and interdisciplinary approach, this paper examines compensation and liability frameworks from high-risk transnational domains - including vaccine injury schemes, systemic financial risk governance, commercial nuclear liability, and international environmental regimes - to distill transferable legal design principles such as strict liability, risk pooling, collective risk-sharing, and liability channelling, while highlighting potential structural constraints on their application to AI-related harms. Situated within an international order shaped more by AI arms race dynamics than cooperative governance, the paper outlines the contours of a global AI accountability and compensation architecture, emphasizing the tension between geopolitical rivalry and the collective action required to govern transboundary AI risks effectively.

</details>


### [826] [AI-generated data contamination erodes pathological variability and diagnostic reliability](https://arxiv.org/abs/2601.12946)
*Hongyu He,Shaowen Xiang,Ye Zhang,Yingtao Zhu,Jin Zhang,Hao Deng,Emily Alsentzer,Qingyu Chen,Kun-Hsing Yu,Andrew Marmenshall,Tingting Chen,Srinivas Anumasa,Daniel Ebner,Dean Ho,Kee Yuan Ngiam,Ching-Yu Cheng,Dianbo Liu*

Main category: cs.CY

TL;DR: 研究指出无人工审核时，生成式AI数据污染会使医疗数据质量下降，评估了三种缓解策略，强调需政策强制的人工监督。


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI数据污染在医疗领域的临床后果。

Method: 分析超80万个跨临床文本生成、视觉语言报告和医学图像合成的合成数据点，进行盲法医生评估，系统评估三种缓解策略。

Result: 模型会收敛到通用表型，罕见关键发现消失，人口统计表征倾斜，假诊断信心掩盖退化，两轮后AI生成文档无临床价值，合成体积缩放无效，混合真实数据与质量感知过滤可保留多样性。

Conclusion: 无政策强制的人工监督，生成式AI会破坏其依赖的医疗数据生态系统。

Abstract: Generative artificial intelligence (AI) is rapidly populating medical records with synthetic content, creating a feedback loop where future models are increasingly at risk of training on uncurated AI-generated data. However, the clinical consequences of this AI-generated data contamination remain unexplored. Here, we show that in the absence of mandatory human verification, this self-referential cycle drives a rapid erosion of pathological variability and diagnostic reliability. By analysing more than 800,000 synthetic data points across clinical text generation, vision-language reporting, and medical image synthesis, we find that models progressively converge toward generic phenotypes regardless of the model architecture. Specifically, rare but critical findings, including pneumothorax and effusions, vanish from the synthetic content generated by AI models, while demographic representations skew heavily toward middle-aged male phenotypes. Crucially, this degradation is masked by false diagnostic confidence; models continue to issue reassuring reports while failing to detect life-threatening pathology, with false reassurance rates tripling to 40%. Blinded physician evaluation confirms that this decoupling of confidence and accuracy renders AI-generated documentation clinically useless after just two generations. We systematically evaluate three mitigation strategies, finding that while synthetic volume scaling fails to prevent collapse, mixing real data with quality-aware filtering effectively preserves diversity. Ultimately, our results suggest that without policy-mandated human oversight, the deployment of generative AI threatens to degrade the very healthcare data ecosystems it relies upon.

</details>


### [827] [The Post-Turing Condition: Conceptualising Artificial Subjectivity and Synthetic Sociality](https://arxiv.org/abs/2601.12938)
*Thorsten Jelinek,Patrick Glauner,Alvin Wang Graylin,Yubao Qiu*

Main category: cs.CY

TL;DR: 后图灵时代AI影响社会协调和意义形成，引入PRMO框架，提出合成社交性风险，给出Quadrangulation设计原则分析AI系统。


<details>
  <summary>Details</summary>
Motivation: 探讨后图灵时代AI在社会协调和意义形成方面带来的挑战，避免人类在意义形成中被边缘化。

Method: 引入PRMO框架，将AI设计轨迹与人类主体性的四个维度联系起来。

Result: 指出合成社交性带来人类被排除在意义形成之外的结构风险。

Conclusion: 提出Quadrangulation作为社会嵌入式AI系统的设计原则，为分析计算与社会交叉处的AI系统提供结构词汇。

Abstract: In the Post-Turing era, artificial intelligence increasingly shapes social coordination and meaning formation rather than merely automating cognitive tasks. The central challenge is therefore not whether machines become conscious, but whether processes of interpretation and shared reference are progressively automated in ways that marginalize human participation. This paper introduces the PRMO framework, relating AI design trajectories to four constitutive dimensions of human subjectivity: Perception, Representation, Meaning, and the Real. Within this framework, Synthetic Sociality denotes a technological horizon in which artificial agents negotiate coherence and social order primarily among themselves, raising the structural risk of human exclusion from meaning formation. To address this risk, the paper proposes Quadrangulation as a design principle for socially embedded AI systems, requiring artificial agents to treat the human subject as a constitutive reference within shared contexts of meaning. This work is a conceptual perspective that contributes a structural vocabulary for analyzing AI systems at the intersection of computation and society, without proposing a specific technical implementation.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [828] [Audit du syst{è}me d'information et du mod{è}le de gouvernance de la Biblioth{è}que Num{é}rique de l'Espace universitaire Francophone (BNEUF) du projet Initiative pour le D{é}veloppement du Num{é}rique dans l'Espace Universitaire Francophone (IDNEUF)](https://arxiv.org/abs/2601.12902)
*Mokhtar Ben Henda*

Main category: cs.DL

TL;DR: 对BNEUF系统整体结构及在IDNEUF框架下运行情况评估，为支持AUF新战略对资源和服务重组，提供新思路。


<details>
  <summary>Details</summary>
Motivation: 支持AUF2021 - 2025年新战略，落实法语国家科学项目。

Method: 对现有和未来数字资源与服务进行重组，并纳入未来综合服务协作平台，进行外部评估。

Result: 文档完成对BNEUF系统的评估，并给出新的组织和使用方式。

Conclusion: 能为AUF项目团队提供数字资源优化管理新途径，促进其与相关模块协同。

Abstract: This document provides an assessment of the overall structure of the BNEUF system and how it operates within the framework of the Initiative for Digital Development in French speaking Universities (IDNEUF). This report aims to support the AUF's new strategy for 2021-2025, with its new structural and governance foundations for the implementation of the Francophonie scientifique project. It was therefore decided to reorganize existing and future digital resources and services with a view to incorporating them into the future global collaborative platform for integrated services. This report provides an external assessment with new forms of organization and use of the BNEUF system. The aim is to provide the AUF project team with new avenues for optimized management of the compiled digital resources and to synergize them with the related modules of the Atlas of Expertise and the Francophone Social Network.

</details>


### [829] [Scientific production in the era of Large Language Models](https://arxiv.org/abs/2601.13187)
*Keigo Kusumegi,Xinyu Yang,Paul Ginsparg,Mathijs de Vaan,Toby Stuart,Yian Yin*

Main category: cs.DL

TL;DR: 分析LLMs对科研的影响，发现其使论文产量增加、改变写作复杂性与质量关系、让引用更多样，需改变科研成果评估方式。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型（LLMs）对科研的改变

Method: 分析多个包含210万预印本、2.8万同行评审报告和24.6亿次科学文献在线访问记录的大规模数据集

Result: 1. 使用LLMs起草论文的科学家论文产量大幅增加，增幅因领域和作者背景而异；2. LLM使用逆转了写作复杂性和论文质量的关系；3. LLM使用者引用更多样的先前研究。

Conclusion: 科研产出发生巨大转变，期刊、资助机构和终身教职委员会需改变科研成果评估方式。

Abstract: Large Language Models (LLMs) are rapidly reshaping scientific research. We analyze these changes in multiple, large-scale datasets with 2.1M preprints, 28K peer review reports, and 246M online accesses to scientific documents. We find: 1) scientists adopting LLMs to draft manuscripts demonstrate a large increase in paper production, ranging from 23.7-89.3% depending on scientific field and author background, 2) LLM use has reversed the relationship between writing complexity and paper quality, leading to an influx of manuscripts that are linguistically complex but substantively underwhelming, and 3) LLM adopters access and cite more diverse prior work, including books and younger, less-cited documents. These findings highlight a stunning shift in scientific production that will likely require a change in how journals, funding agencies, and tenure committees evaluate scientific works.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [830] [Multi-Scale Negative Coupled Information Systems (MNCIS): A Unified Spectral Topology Framework for Stability in Turbulence, AI, and Biology](https://arxiv.org/abs/2601.11594)
*Pengyue Hou*

Main category: physics.comp-ph

TL;DR: 本文推广MNCIS框架，提出ASNC算子，通过三个实例验证其有效性，表明该框架能区分可行复杂系统与崩溃系统。


<details>
  <summary>Details</summary>
Motivation: 复杂动力系统常出现谱隙坍塌的结构不稳定性，需解决系统向低维吸引子的问题。

Method: 基于已有全局适定性估计，推广MNCIS框架，提出ASNC算子作为依赖状态的高通滤波器来惩罚谱边界的熵积累，并通过三个实例验证。

Result: 在流体动力学、人工智能、生物物理三个领域的应用中，该框架表现良好，如在3D纳维 - 斯托克斯湍流中稳定无粘极限，在图神经网络中训练无残差连接的超深网络，在反应 - 扩散形态发生中稳定图灵模式。

Conclusion: MNCIS框架为区分可行复杂系统和崩溃系统提供了与基无关的拓扑条件，连接了物理稳定性和信息持久性。

Abstract: Complex dynamical systems frequently encounter a recurrent structural instability: the collapse of the spectral gap, driving the system toward a low-dimensional "Zero-Mode Attractor" (e.g., spectral pile-up or over-smoothing). Building upon recent global well-posedness estimates [Hou, arXiv:2601.00638], this work generalizes the Multi-Scale Negative Coupled Information System (MNCIS) framework. We postulate that global stability requires an active topological operator -- Adaptive Spectral Negative Coupling (ASNC) -- functioning as a state-dependent high-pass filter that penalizes entropy accumulation at spectral boundaries. We validate this unified framework via three implementations:(1) Hydrodynamics: In 3D Navier-Stokes turbulence ($N=256^3$), ASNC acts as a global-enstrophy adaptive sub-grid scale (SGS) model, stabilizing the inviscid limit and preserving the Kolmogorov $-5/3$ inertial range without artificial hyper-viscosity.(2) Artificial Intelligence: Addressing Over-smoothing in Graph Neural Networks (GNNs), we implement ASNC as a parameter-free topological constraint. Unlike baselines (e.g., DeepGCNs) relying on dense residual connections to bypass signal decay, our framework enables the training of ultra-deep 64-layer networks without residual connections, maintaining perfectly stationary feature variance ($σ^2 \equiv 1.0$) on the ogbn-arxiv benchmark. (3) Biological Physics: In reaction-diffusion morphogenesis, it stabilizes Turing patterns against diffusive washout in high-entropy regimes. Our results suggest that the MNCIS framework provides a base-independent topological condition for distinguishing viable complex systems from those collapsing into thermal equilibrium, bridging physical stability and information persistence.

</details>


### [831] [Refined Gradient-Based Temperature Optimization for the Replica-Exchange Monte-Carlo Method](https://arxiv.org/abs/2601.13542)
*Tatsuya Miyata,Shunta Arai,Satoshi Takabe*

Main category: physics.comp-ph

TL;DR: 本文提出改进的在线温度选择方法用于RXMC法，通过实验证明有效性，相比其他方法有优势。


<details>
  <summary>Details</summary>
Motivation: RXMC法采样效率高度依赖温度选择，寻找最优温度存在挑战。

Method: 扩展梯度优化框架，引入重参数化技术，定义损失函数，用梯度下降优化温度。

Result: 实验表明方法能实现均匀接受率，减少温度空间往返时间。

Conclusion: 提出的方法比政策梯度方法有显著优势，能防止约束违反，稳定优化。

Abstract: The replica-exchange Monte-Carlo (RXMC) method is a powerful Markov-chain Monte-Carlo algorithm for sampling from multi-modal distributions, which are challenging for conventional methods. The sampling efficiency of the RXMC method depends highly on the selection of the temperatures, and finding optimal temperatures remains a challenge. In this study, we propose a refined online temperature selection method by extending the gradient-based optimization framework proposed previously. Building upon the existing temperature update approach, we introduce a reparameterization technique to strictly enforce physical constraints, such as the monotonic ordering of inverse temperatures, which were not explicitly addressed in the original formulation. The proposed method defines the variance of acceptance rates between adjacent replicas as a loss function, estimates its gradient using differential information from the sampling process, and optimizes the temperatures via gradient descent. We demonstrate the effectiveness of our method through experiments on benchmark spin systems, including the two-dimensional ferromagnetic Ising model, the two-dimensional ferromagnetic XY model, and the three-dimensional Edwards-Anderson model. Our results show that the method successfully achieves uniform acceptance rates and reduces round-trip times across the temperature space. Furthermore, our proposed method offers a significant advantage over recently proposed policy gradient method that require careful hyperparameter tuning, while simultaneously preventing the constraint violations that destabilize optimization.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [832] [A uniformity principle for spatial matching](https://arxiv.org/abs/2601.13426)
*Taha Ameen,Flore Sentenac,Sophie H. Yu*

Main category: math.PR

TL;DR: 研究平台服务范围分配问题，提出均匀性原则并给出部分结果用于指导实践。


<details>
  <summary>Details</summary>
Motivation: 解决平台将固定服务范围预算预先分配给供应节点以最大化满足需求的设计问题。

Method: 用二分随机几何图建模，利用边际收益递减和图的碎片化特性分析，对k=1情况用马尔可夫链嵌入。

Result: 建立均匀性原则，即更均匀的服务范围分配有更大预期匹配；对k=1情况刻画预期匹配大小并给出特殊情况的闭式解。

Conclusion: 研究结果可为打车、按需劳动力市场和无人机配送网络优化服务范围分配和设计激励结构提供理论指导。

Abstract: Platforms matching spatially distributed supply to demand face a fundamental design choice: given a fixed total budget of service range, how should it be allocated across supply nodes ex ante, i.e. before supply and demand locations are realized, to maximize fulfilled demand? We model this problem using bipartite random geometric graphs where $n$ supply and $m$ demand nodes are uniformly distributed on $[0,1]^k$ ($k \ge 1$), and edges form when demand falls within a supply node's service region, the volume of which is determined by its service range. Since each supply node serves at most one demand, platform performance is determined by the expected size of a maximum matching. We establish a uniformity principle: whenever one service range allocation is more uniform than the other, the more uniform allocation yields a larger expected matching. This principle emerges from diminishing marginal returns to range expanding service range, and limited interference between supply nodes due to bounded ranges naturally fragmenting the graph. For $k=1$, we further characterize the expected matching size through a Markov chain embedding and derive closed-form expressions for special cases. Our results provide theoretical guidance for optimizing service range allocation and designing incentive structures in ride-hailing, on-demand labor markets, and drone delivery networks.

</details>


### [833] [New Trends in the Stability of Sinkhorn Semigroups](https://arxiv.org/abs/2601.12633)
*Pierre Del Moral,Ajay Jasra*

Main category: math.PR

TL;DR: 本文介绍基于收缩系数和Lyapunov型算子理论技术的Sinkhorn/Gibbs型半群分析，统一简化了Sinkhorn算法稳定性的论证并得到新收缩估计。


<details>
  <summary>Details</summary>
Motivation: 传统Sinkhorn桥稳定性收敛证明方法多样，本文旨在以自包含方式呈现新的Sinkhorn/Gibbs型半群分析方法。

Method: 基于运输成本不等式、$φ$-散度、Kantorovich型准则等强有效的半群方法。

Result: 统一并简化Sinkhorn算法稳定性的许多论证，得到关于广义$φ$-熵、加权总变差范数等的新收缩估计。

Conclusion: 新的半群分析在Sinkhorn算法稳定性研究中有重要作用，能简化论证并得到新结果。

Abstract: Entropic optimal transport problems play an increasingly important role in machine learning and generative modelling. In contrast with optimal transport maps which often have limited applicability in high dimensions, Schrodinger bridges can be solved using the celebrated Sinkhorn's algorithm, a.k.a. the iterative proportional fitting procedure. The stability properties of Sinkhorn bridges when the number of iterations tends to infinity is a very active research area in applied probability and machine learning. Traditional proofs of convergence are mainly based on nonlinear versions of Perron-Frobenius theory and related Hilbert projective metric techniques, gradient descent, Bregman divergence techniques and Hamilton-Jacobi-Bellman equations, including propagation of convexity profiles based on coupling diffusions by reflection methods. The objective of this review article is to present, in a self-contained manner, recently developed Sinkhorn/Gibbs-type semigroup analysis based upon contraction coefficients and Lyapunov-type operator-theoretic techniques. These powerful, off-the-shelf semigroup methods are based upon transportation cost inequalities (e.g. log-Sobolev, Talagrand quadratic inequality, curvature estimates), $φ$-divergences, Kantorovich-type criteria and Dobrushin contraction-type coefficients on weighted Banach spaces as well as Wasserstein distances. This novel semigroup analysis allows one to unify and simplify many arguments in the stability of Sinkhorn algorithm. It also yields new contraction estimates w.r.t. generalized $φ$-entropies, as well as weighted total variation norms, Kantorovich criteria and Wasserstein distances.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [834] [Inter-Cell Interference Rejection Based on Ultrawideband Walsh-Domain Wireless Autoencoding](https://arxiv.org/abs/2601.11713)
*Rodney Martinez Alonso,Cel Thys,Cedric Dehos,Yuneisy Esthela Garcia Guzman,Sofie Pollin*

Main category: eess.SP

TL;DR: 提出用于超宽带通信系统中抑制带内部分小区间干扰（ICI）的技术，设计端到端无线自动编码器架构，在Walsh域联合优化编解码，实验显示该自动编码器可实现高达12 dB的ICI抑制。


<details>
  <summary>Details</summary>
Motivation: 解决超宽带通信系统中来自共存的窄带5G基站的干扰问题。

Method: 设计端到端无线自动编码器架构，在Walsh域联合优化发射器和接收器的编解码，利用Walsh函数的正交性和自反性，在并行Walsh分支分布并学习编码位字。

Result: 该自动编码器在相同基线信道噪声下，最多可实现12 dB的ICI抑制，同时保持低块错误率（BLER）。

Conclusion: 所提出的自动编码器架构能有效抑制超宽带通信系统中的ICI。

Abstract: This paper proposes a novel technique for rejecting partial-in-band inter-cell interference (ICI) in ultrawideband communication systems. We present the design of an end-to-end wireless autoencoder architecture that jointly optimizes the transmitter and receiver encoding/decoding in the Walsh domain to mitigate interference from coexisting narrower-band 5G base stations. By exploiting the orthogonality and self-inverse properties of Walsh functions, the system distributes and learns to encode bit-words across parallel Walsh branches. Through analytical modeling and simulation, we characterize how 5G CPOFDM interference maps into the Walsh domain and identify optimal ratios of transmission frequencies and sampling rate where the end-to-end autoencoder achieves the highest rejection. Experimental results show that the proposed autoencoder achieves up to 12 dB of ICI rejection while maintaining a low block error rate (BLER) for the same baseline channel noise, i.e., baseline Signal-to-Noise-Ratio (SNR) without the interference.

</details>


### [835] [Accelerated MR Elastography Using Learned Neural Network Representation](https://arxiv.org/abs/2601.11878)
*Xi Peng*

Main category: eess.SP

TL;DR: 本文开发了一种深度学习方法，可从高度欠采样数据中实现快速高分辨率MR弹性成像，无需高质量训练数据集，实验表明该方法效果良好。


<details>
  <summary>Details</summary>
Motivation: 开发一种无需高质量训练数据集，就能从高度欠采样数据中实现快速高分辨率MR弹性成像的深度学习方法。

Method: 将深度神经网络表示为线性子空间模型的非线性扩展，使用多级k空间一致损失以自监督方式学习网络权重，并结合相位对比特定的幅度和相位先验。

Result: 与传统线性子空间方法相比，该非线性网络表示方法能产生更好的图像重建效果，抑制噪声和伪影，刚度估计与全采样数据相当。

Conclusion: 证明了使用深度网络表示对高度欠采样的MRE图像进行建模和重建的可行性，是基于子空间方法的非线性扩展。

Abstract: To develop a deep-learning method for achieving fast high-resolution MR elastography from highly undersampled data without the need of high-quality training dataset. We first framed the deep neural network representation as a nonlinear extension of the linear subspace model, then used it to represent and reconstruct MRE image repetitions from undersampled k-space data. The network weights were learned using a multi-level k-space consistent loss in a self-supervised manner. To further enhance reconstruction quality, phase-contrast specific magnitude and phase priors were incorporated, including the similarity of anatomical structures and smoothness of wave-induced harmonic displacement. Experiments were conducted using both 3D gradient-echo spiral and multi-slice spin-echo spiral MRE datasets. Compared to the conventional linear subspace-based approaches, the nonlinear network representation method was able to produce superior image reconstruction with suppressed noise and artifacts from a single in-plane spiral arm per MRE repetition (e.g., total R=10), yielding comparable stiffness estimation to the fully sampled data. This work demonstrated the feasibility of using deep network representations to model and reconstruct MRE images from highly-undersampled data, a nonlinear extension of the subspace-based approaches.

</details>


### [836] [Temporal Data and Short-Time Averages Improve Multiphase Mass Flow Metering](https://arxiv.org/abs/2601.12433)
*Amanda Nyholm,Yessica Arellano,Jinyu Liu,Damian Krakowiak,Pierluigi Salvo Rossi*

Main category: eess.SP

TL;DR: 本文研究结合机器学习算法与科里奥利质量流量计减少多相流测量误差，发现保留时间信息可提升模型性能，CNN表现最佳且结果稳健。


<details>
  <summary>Details</summary>
Motivation: 当前仪器难以准确估计多相流，结合机器学习算法与单相流量计的方法受关注，需提升多相流测量模型性能。

Method: 对比多层感知器、窗口多层感知器和CNN在三相气 - 水 - 油流数据上的表现，采用短时平均保留时间信息进行模型训练。

Result: CNN在0.25Hz下表现最佳，约95%相对误差低于13%，归一化均方根误差0.03，平均绝对百分比误差约4.3%，优于单平均模型。

Conclusion: 在多相流测量中，保留时间信息能显著提升模型性能，单个实验内短时平均更优，结果稳健。

Abstract: Reliable flow measurements are essential in many industries, but current instruments often fail to accurately estimate multiphase flows, which are frequently encountered in real-world operations. Combining machine learning (ML) algorithms with accurate single-phase flowmeters has therefore received extensive research attention in recent years. The Coriolis mass flowmeter is a widely used single-phase meter that provides direct mass flow measurements, which ML models can be trained to correct, thereby reducing measurement errors in multiphase conditions. This paper demonstrates that preserving temporal information significantly improves model performance in such scenarios. We compare a multilayer perceptron, a windowed multilayer perceptron, and a convolutional neural network (CNN) on three-phase air-water-oil flow data from 342 experiments. Whereas prior work typically compresses each experiment into a single averaged sample, we instead compute short-time averages from within each experiment and train models that preserve temporal information at several downsampling intervals. The CNN performed best at 0.25 Hz with approximately 95 % of relative errors below 13 %, a normalized root mean squared error of 0.03, and a mean absolute percentage error of approximately 4.3 %, clearly outperforming the best single-averaged model and demonstrating that short-time averaging within individual experiments is preferable. Results are consistent across multiple data splits and random seeds, demonstrating robustness.

</details>


### [837] [Energy-Efficient Prediction in Textile Manufacturing: Enhancing Accuracy and Data Efficiency With Ensemble Deep Transfer Learning](https://arxiv.org/abs/2601.12663)
*Yan-Chen Chen,Wei-Yu Chiu,Qun-Yu Wang,Jing-Wei Chen,Hao-Ting Zhao*

Main category: eess.SP

TL;DR: 提出Ensemble Deep Transfer Learning (EDTL)框架解决传统纺织厂能耗大、DNN需大量数据问题，实验显示其提升预测精度和模型鲁棒性，助力节能生产。


<details>
  <summary>Details</summary>
Motivation: 传统纺织厂能耗高需节能优化，DNN需大量历史数据，传感器部署和数据收集成本高。

Method: 提出EDTL框架，结合迁移学习、集成策略和特征对齐层，在数据丰富生产线预训练DNN模型并适配到数据有限生产线。

Result: 在真实纺织厂数据集实验中，EDTL比传统DNN提高预测精度5.66%，增强模型鲁棒性3.96%，尤其在数据有限场景（20%-40%数据可用性）效果显著。

Conclusion: 该研究实现少数据准确预测，为智能生产系统提供可扩展且经济高效的节能纺织制造解决方案。

Abstract: Traditional textile factories consume substantial energy, making energy-efficient production optimization crucial for sustainability and cost reduction. Meanwhile, deep neural networks (DNNs), which are effective for factory output prediction and operational optimization, require extensive historical data, posing challenges due to high sensor deployment and data collection costs. To address this, we propose Ensemble Deep Transfer Learning (EDTL), a novel framework that enhances prediction accuracy and data efficiency by integrating transfer learning with an ensemble strategy and a feature alignment layer. EDTL pretrains DNN models on data-rich production lines (source domain) and adapts them to data-limited lines (target domain), reducing dependency on large datasets. Experiments on real-world textile factory datasets show that EDTL improves prediction accuracy by 5.66% and enhances model robustness by 3.96% compared to conventional DNNs, particularly in data-limited scenarios (20%-40% data availability). This research contributes to energy-efficient textile manufacturing by enabling accurate predictions with fewer data requirements, providing a scalable and cost-effective solution for smart production systems.

</details>
