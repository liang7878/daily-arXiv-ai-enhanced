<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 11]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.DS](#cs.DS) [Total: 3]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.IR](#cs.IR) [Total: 6]
- [cs.LG](#cs.LG) [Total: 60]
- [cs.NE](#cs.NE) [Total: 5]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.SE](#cs.SE) [Total: 10]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [stat.ML](#stat.ML) [Total: 5]
- [stat.CO](#stat.CO) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.HC](#cs.HC) [Total: 8]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [cs.ET](#cs.ET) [Total: 2]
- [math.NT](#math.NT) [Total: 1]
- [stat.ME](#stat.ME) [Total: 6]
- [cs.CG](#cs.CG) [Total: 1]
- [cs.RO](#cs.RO) [Total: 10]
- [cs.CV](#cs.CV) [Total: 24]
- [econ.GN](#econ.GN) [Total: 1]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.IT](#cs.IT) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.SI](#cs.SI) [Total: 2]
- [eess.IV](#eess.IV) [Total: 3]
- [physics.ao-ph](#physics.ao-ph) [Total: 2]
- [cs.CL](#cs.CL) [Total: 18]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Grounding Rule-Based Argumentation Using Datalog](https://arxiv.org/abs/2508.10976)
*Martin Diller,Sarah Alice Gaggl,Philipp Hanisch,Giuseppina Monterosso,Fritz Rauschenbach*

Main category: cs.AI

TL;DR: 提出适用于ASPIC+的智能接地程序，将一阶ASPIC+实例转换为Datalog程序获取接地替换，还提出简化方法并进行实证评估。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则的论证推理大多仅支持命题规则，实现一阶实例推理需接地步骤，接地会使输入理论规模指数级增长，且缺乏针对ASPIC+的专用解决方案。

Method: 将一阶ASPIC+实例转换为Datalog程序，查询Datalog引擎获取接地替换以进行规则和相反情况的接地，提出针对ASPIC+形式主义的简化方法。

Result: 进行了原型实现的实证评估。

Conclusion: 所提智能接地程序可控制接地规模，同时保持推理过程的正确性和可扩展性。

Abstract: ASPIC+ is one of the main general frameworks for rule-based argumentation for
AI. Although first-order rules are commonly used in ASPIC+ examples, most
existing approaches to reason over rule-based argumentation only support
propositional rules. To enable reasoning over first-order instances, a
preliminary grounding step is required. As groundings can lead to an
exponential increase in the size of the input theories, intelligent procedures
are needed. However, there is a lack of dedicated solutions for ASPIC+.
Therefore, we propose an intelligent grounding procedure that keeps the size of
the grounding manageable while preserving the correctness of the reasoning
process. To this end, we translate the first-order ASPIC+ instance into a
Datalog program and query a Datalog engine to obtain ground substitutions to
perform the grounding of rules and contraries. Additionally, we propose
simplifications specific to the ASPIC+ formalism to avoid grounding of rules
that have no influence on the reasoning process. Finally, we performed an
empirical evaluation of a prototypical implementation to show scalability.

</details>


### [2] [From Individual to Multi-Agent Algorithmic Recourse: Minimizing the Welfare Gap via Capacitated Bipartite Matching](https://arxiv.org/abs/2508.11070)
*Zahra Khotanlou,Kate Larson,Amir-Hossein Karimi*

Main category: cs.AI

TL;DR: 提出多智能体算法追索框架，解决多利益相关者场景问题，实验证明可实现接近最优福利。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注单个体和单模型场景，忽视现实系统多智能体本质，需考虑多追索者和提供者场景。

Method: 将多对多交互建模为带容量加权二分匹配问题，提出三层优化框架。

Result: 在合成和真实数据集实验验证，框架能以最小系统设置修改实现接近最优福利。

Conclusion: 将算法追索从个体推荐扩展到系统级设计，提供实现更高社会福利且保持个体可操作性的可行途径。

Abstract: Decision makers are increasingly relying on machine learning in sensitive
situations. In such settings, algorithmic recourse aims to provide individuals
with actionable and minimally costly steps to reverse unfavorable AI-driven
decisions. While existing research predominantly focuses on single-individual
(i.e., seeker) and single-model (i.e., provider) scenarios, real-world
applications often involve multiple interacting stakeholders. Optimizing
outcomes for seekers under an individual welfare approach overlooks the
inherently multi-agent nature of real-world systems, where individuals interact
and compete for limited resources. To address this, we introduce a novel
framework for multi-agent algorithmic recourse that accounts for multiple
recourse seekers and recourse providers. We model this many-to-many interaction
as a capacitated weighted bipartite matching problem, where matches are guided
by both recourse cost and provider capacity. Edge weights, reflecting recourse
costs, are optimized for social welfare while quantifying the welfare gap
between individual welfare and this collectively feasible outcome. We propose a
three-layer optimization framework: (1) basic capacitated matching, (2) optimal
capacity redistribution to minimize the welfare gap, and (3) cost-aware
optimization balancing welfare maximization with capacity adjustment costs.
Experimental validation on synthetic and real-world datasets demonstrates that
our framework enables the many-to-many algorithmic recourse to achieve
near-optimal welfare with minimum modification in system settings. This work
extends algorithmic recourse from individual recommendations to system-level
design, providing a tractable path toward higher social welfare while
maintaining individual actionability.

</details>


### [3] [Learn to optimize for automatic proton PBS treatment planning for H&N cancers](https://arxiv.org/abs/2508.11085)
*Qingqing Wang,Liqiang Xiao,Chang Chang*

Main category: cs.AI

TL;DR: 提出数据驱动逆优化器并集成到PPO自动治疗规划框架，提高质子PBS治疗规划效率和质量。


<details>
  <summary>Details</summary>
Motivation: 头颈部癌症质子PBS治疗规划需平衡多目标，现有逆优化依赖理论驱动，耗时且需大量人力。

Method: 提出基于L2O的数据驱动逆优化器，集成LLM长上下文处理技术；采用PPO框架作为外循环虚拟规划器，剂量预测器初始化参数，内循环L2O逆优化器计算MU值。

Result: 与L - BFGSB相比，L2O逆优化器有效性和效率分别提高22.97%和36.41%；框架平均2.55小时生成的计划在OAR保护和靶区覆盖上优于或等同于人工计划。

Conclusion: 所提框架能在临床可接受时间内自动生成高质量治疗计划。

Abstract: Proton PBS treatment planning for H&N cancers involves numerous conflicting
objectives, requiring significant effort from human planners to balance and
satisfy multiple clinical goals during planning. To achieve this,
experience-demanding objective parameter adjustment and computationally
expensive inverse optimization are performed iteratively. Extensive efforts
have been made to automatically adjust objective parameters, but the most
time-consuming component, i.e., inverse optimization, still relies heavily on
theory-driven approaches. We propose a data-driven inverse optimizer and
integrate it into a PPO-based automatic treatment planning framework to
automatically generate high-quality plans within a clinical acceptable planning
time. The inverse optimizer is a L2O method that predicts update steps by
learning from the task-specific data distribution. For the first time, we
integrate techniques designed for long-context processing, originally developed
for LLMs, into a Transformer-based L2O framework to address the scalability
issue of existing L2O methods. The PPO framework functions as an outer-loop
virtual planner, autonomously adjusting objective parameters through a policy
network, and the dose predictor is used to initialize objective parameters. The
inner-loop L2O inverse optimizer computes machine-deliverable MU values based
on objectives refined by the PPO policy network. 97 patients are collected in
this study, and compared with L-BFGSB, our L2O-based inverse optimizer improves
the effectiveness and efficiency by 22.97% and 36.41%, respectively. In
conjunction with the PPO-based learned virtual planner, plans generated by our
framework within an average of 2.55 hours show improved or comparable OAR
sparing with superior target coverage for patients with different prescription
dose levels, number of target volumes, beam angles, etc., compared with
human-generated plans.

</details>


### [4] [On Strong and Weak Admissibility in Non-Flat Assumption-Based Argumentation](https://arxiv.org/abs/2508.11182)
*Matti Berthold,Lydia Blümel,Anna Rapberger*

Main category: cs.AI

TL;DR: 本文拓宽了基于假设的论证（ABA）中可接受性概念的研究，引入了强和弱可接受性及相关语义，研究其性质并讨论缺点及解决方法。


<details>
  <summary>Details</summary>
Motivation: 之前对ABA中强可接受性未研究，弱可接受性仅在受限片段研究，需拓宽可接受性概念研究。

Method: 使用抽象双极集合论证框架（BSAFs）作为形式化工具，引入强可接受性并研究其性质，将弱可接受性研究扩展到非平坦情况。

Result: 证明经典、强和弱可接受性下中心模块化属性得以保持，发现非平坦ABA中强和弱可接受语义存在标准可接受语义的一些缺点。

Conclusion: 引入了ABA中的强可接受性，扩展了弱可接受性研究，指出了语义存在的问题并探讨解决办法。

Abstract: In this work, we broaden the investigation of admissibility notions in the
context of assumption-based argumentation (ABA). More specifically, we study
two prominent alternatives to the standard notion of admissibility from
abstract argumentation, namely strong and weak admissibility, and introduce the
respective preferred, complete and grounded semantics for general (sometimes
called non-flat) ABA. To do so, we use abstract bipolar set-based argumentation
frameworks (BSAFs) as formal playground since they concisely capture the
relations between assumptions and are expressive enough to represent general
non-flat ABA frameworks, as recently shown. While weak admissibility has been
recently investigated for a restricted fragment of ABA in which assumptions
cannot be derived (flat ABA), strong admissibility has not been investigated
for ABA so far. We introduce strong admissibility for ABA and investigate
desirable properties. We furthermore extend the recent investigations of weak
admissibility in the flat ABA fragment to the non-flat case. We show that the
central modularization property is maintained under classical, strong, and weak
admissibility. We also show that strong and weakly admissible semantics in
non-flat ABA share some of the shortcomings of standard admissible semantics
and discuss ways to address these.

</details>


### [5] [Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning Models to Ask for Information](https://arxiv.org/abs/2508.11252)
*Youcheng Huang,Bowen Qin,Chen Huang,Duanyu Feng,Xi Yang,Wenqiang Lei*

Main category: cs.AI

TL;DR: 现有基准仅评估大推理模型（LRMs）解决定义明确问题的能力，本文提出新数据集评估其主动询问信息能力，发现不足并揭示相关行为及微调挑战。


<details>
  <summary>Details</summary>
Motivation: 现有评估仅关注LRMs解决定义明确问题的能力，真正的智能体应能在信息不足时主动询问信息，因此需要填补这一评估空白。

Method: 提出包含两种不同上下文不完整问题的新数据集，基于该数据集对LRMs进行系统评估。

Result: 发现LRMs无法主动询问信息，揭示了其过度思考和幻觉行为，以及监督微调学习该能力的潜力与挑战。

Conclusion: 为开发具有真正智能而非仅能解决问题的LRMs提供新见解。

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable problem-solving
abilities in mathematics, as evaluated by existing benchmarks exclusively on
well-defined problems. However, such evaluation setup constitutes a critical
gap, since a genuine intelligent agent should not only solve problems (as a
math quiz solver), but also be able~to ask for information when the problems
lack sufficient information, enabling proactivity in responding users'
requests. To bridge such gap, we proposes a new dataset consisting of two types
of incomplete problems with diverse contexts. Based on the dataset, our
systematical evaluation of LRMs reveals their inability in proactively asking
for information. In addition, we uncover the behaviors related to overthinking
and hallucination of LRMs, and highlight the potential and challenges of
supervised fine-tuning in learning such ability. We hope to provide new
insights in developing LRMs with genuine intelligence, rather than just solving
problems.

</details>


### [6] [SAGE: Scale-Aware Gradual Evolution for Continual Knowledge Graph Embedding](https://arxiv.org/abs/2508.11347)
*Yifei Li,Lingling Zhang,Hang Yan,Tianzhe Zhao,Zihan Ma,Muye Huang,Jun Liu*

Main category: cs.AI

TL;DR: 本文提出SAGE框架解决动态知识图谱嵌入问题，实验显示其性能优于现有基线方法，且证明自适应嵌入维度的重要性。


<details>
  <summary>Details</summary>
Motivation: 传统知识图谱嵌入方法聚焦静态图，现有持续知识图谱嵌入（CKGE）方法未考虑更新规模差异且缺乏全流程系统评估，需解决知识图谱的动态特性问题。

Method: 提出SAGE框架，先根据更新规模确定嵌入维度并扩展嵌入空间，再用动态蒸馏机制平衡旧知识保留和新事实融入。

Result: 在七个基准测试中，SAGE始终优于现有基线方法，MRR提升1.38%、H@1提升1.25%、H@10提升1.6%；与固定嵌入维度方法对比，SAGE在每个快照上都有最优表现。

Conclusion: SAGE框架有效，自适应嵌入维度在CKGE中很重要。

Abstract: Traditional knowledge graph (KG) embedding methods aim to represent entities
and relations in a low-dimensional space, primarily focusing on static graphs.
However, real-world KGs are dynamically evolving with the constant addition of
entities, relations and facts. To address such dynamic nature of KGs, several
continual knowledge graph embedding (CKGE) methods have been developed to
efficiently update KG embeddings to accommodate new facts while maintaining
learned knowledge. As KGs grow at different rates and scales in real-world
scenarios, existing CKGE methods often fail to consider the varying scales of
updates and lack systematic evaluation throughout the entire update process. In
this paper, we propose SAGE, a scale-aware gradual evolution framework for
CKGE. Specifically, SAGE firstly determine the embedding dimensions based on
the update scales and expand the embedding space accordingly. The Dynamic
Distillation mechanism is further employed to balance the preservation of
learned knowledge and the incorporation of new facts. We conduct extensive
experiments on seven benchmarks, and the results show that SAGE consistently
outperforms existing baselines, with a notable improvement of 1.38% in MRR,
1.25% in H@1 and 1.6% in H@10. Furthermore, experiments comparing SAGE with
methods using fixed embedding dimensions show that SAGE achieves optimal
performance on every snapshot, demonstrating the importance of adaptive
embedding dimensions in CKGE. The codes of SAGE are publicly available at:
https://github.com/lyfxjtu/Dynamic-Embedding.

</details>


### [7] [CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks](https://arxiv.org/abs/2508.11360)
*Songqin Nong,Jingxuan Xu,Sheng Zhou,Jianfeng Chen,Xiaoxuan Tang,Tao Jiang,Wenhao Xu*

Main category: cs.AI

TL;DR: 提出CRAFT - GUI框架解决现有GUI环境强化学习方法的局限，实验显示其优于现有方法，验证了强化学习与课程学习结合的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在GUI环境中存在忽略任务难度差异和奖励信号单一的问题，影响学习效果。

Method: 提出基于GRPO的课程学习框架CRAFT - GUI，设计结合规则信号和模型评估的奖励函数。

Result: 在公共基准Android Control和内部在线基准上分别比现有方法提高5.6%和10.3%。

Conclusion: 强化学习与课程学习结合在GUI交互任务中有效。

Abstract: As autonomous agents become adept at understanding and interacting with
graphical user interface (GUI) environments, a new era of automated task
execution is emerging. Recent studies have demonstrated that Reinforcement
Learning (RL) can effectively enhance agents' performance in dynamic
interactive GUI environments. However, these methods face two key limitations:
(1) they overlook the significant variation in difficulty across different GUI
tasks by treating the entire training data as a uniform set, which hampers the
agent's ability to adapt its learning process; and (2) most approaches collapse
task-specific nuances into a single, coarse reward, leaving the agent with a
uniform signal that yields inefficient policy updates. To address these
limitations, we propose CRAFT-GUI, a curriculum learning framework based on
Group Relative Policy Optimization (GRPO) that explicitly accounts for the
varying difficulty across trajectories. To enable more fine-grained policy
optimization, we design a reward function that combines simple rule-based
signals with model-judged evaluation, providing richer and more nuanced
feedback during training. Experimental results demonstrate that our method
achieves significant improvements over previous state-of-the-art approaches,
outperforming them by 5.6% on public benchmarks Android Control and 10.3% on
our internal online benchmarks, respectively. These findings empirically
validate the effectiveness of integrating reinforcement learning with
curriculum learning in GUI interaction tasks.

</details>


### [8] [AIM-Bench: Evaluating Decision-making Biases of Agentic LLM as Inventory Manager](https://arxiv.org/abs/2508.11416)
*Xuhua Zhao,Yuxuan Xie,Caihua Chen,Yuxiang Sun*

Main category: cs.AI

TL;DR: 本文引入AIM - Bench评估大语言模型代理在不确定供应链管理场景中的决策行为，发现不同大语言模型有类似人类的决策偏差，并探索缓解效应的策略，强调部署时需考虑偏差。


<details>
  <summary>Details</summary>
Motivation: 大语言模型代理在不确定环境下进行库存决策的能力及决策偏差未被充分研究，引发对其解决实际问题能力和偏差影响的担忧。

Method: 引入AIM - Bench，通过一系列库存补货实验评估大语言模型代理的决策行为。

Result: 不同大语言模型通常表现出类似人类的不同程度决策偏差，探索了缓解拉向中心效应和牛鞭效应的策略。

Conclusion: 在库存决策场景中部署大语言模型时需仔细考虑潜在偏差，研究为缓解人类决策偏差和开发以人为中心的供应链决策支持系统提供思路。

Abstract: Recent advances in mathematical reasoning and the long-term planning
capabilities of large language models (LLMs) have precipitated the development
of agents, which are being increasingly leveraged in business operations
processes. Decision models to optimize inventory levels are one of the core
elements of operations management. However, the capabilities of the LLM agent
in making inventory decisions in uncertain contexts, as well as the
decision-making biases (e.g. framing effect, etc.) of the agent, remain largely
unexplored. This prompts concerns regarding the capacity of LLM agents to
effectively address real-world problems, as well as the potential implications
of biases that may be present. To address this gap, we introduce AIM-Bench, a
novel benchmark designed to assess the decision-making behaviour of LLM agents
in uncertain supply chain management scenarios through a diverse series of
inventory replenishment experiments. Our results reveal that different LLMs
typically exhibit varying degrees of decision bias that are similar to those
observed in human beings. In addition, we explored strategies to mitigate the
pull-to-centre effect and the bullwhip effect, namely cognitive reflection and
implementation of information sharing. These findings underscore the need for
careful consideration of the potential biases in deploying LLMs in Inventory
decision-making scenarios. We hope that these insights will pave the way for
mitigating human decision bias and developing human-centred decision support
systems for supply chains.

</details>


### [9] [Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps](https://arxiv.org/abs/2508.11452)
*Kangyu Wang,Hongliang He,Lin Liu,Ruiqi Liang,Zhenzhong Lan,Jianguo Li*

Main category: cs.AI

TL;DR: 提出Inclusion Arena实时排行榜，基于应用中人类反馈对模型排名，有创新机制，排名可靠稳定，可加速大模型实用化发展。


<details>
  <summary>Details</summary>
Motivation: 现有基准和排行榜多依赖静态数据集或通用领域众包提示，难以反映模型在现实应用中的性能，需新的评估方式。

Method: 将成对模型比较融入自然用户交互，使用改进的Bradley - Terry模型，有Placement Matches冷启动机制和Proximity Sampling智能比较策略。

Result: Inclusion Arena排名可靠稳定，数据传递性更高，降低恶意操纵风险。

Conclusion: Inclusion Arena促进基础模型和现实应用结合，可加速大模型以用户为中心的实用化部署。

Abstract: Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)
have ushered in a new era of AI capabilities, demonstrating near-human-level
performance across diverse scenarios. While numerous benchmarks (e.g., MMLU)
and leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the
development of LLMs and MLLMs, most rely on static datasets or crowdsourced
general-domain prompts, often falling short of reflecting performance in
real-world applications. To bridge this critical gap, we present Inclusion
Arena, a live leaderboard that ranks models based on human feedback collected
directly from AI-powered applications. Our platform integrates pairwise model
comparisons into natural user interactions, ensuring evaluations reflect
practical usage scenarios. For robust model ranking, we employ the
Bradley-Terry model augmented with two key innovations: (1) Placement Matches,
a cold-start mechanism to quickly estimate initial ratings for newly integrated
models, and (2) Proximity Sampling, an intelligent comparison strategy that
prioritizes battles between models of similar capabilities to maximize
information gain and enhance rating stability. Extensive empirical analyses and
simulations demonstrate that Inclusion Arena yields reliable and stable
rankings, exhibits higher data transitivity compared to general crowdsourced
datasets, and significantly mitigates the risk of malicious manipulation. By
fostering an open alliance between foundation models and real-world
applications, Inclusion Arena aims to accelerate the development of LLMs and
MLLMs truly optimized for practical, user-centric deployments. The platform is
publicly accessible at https://doraemon.alipay.com/model-ranking.

</details>


### [10] [Landmark-Assisted Monte Carlo Planning](https://arxiv.org/abs/2508.11493)
*David H. Chan,Mark Roberts,Dana S. Nau*

Main category: cs.AI

TL;DR: 本文形式化概率地标，调整UCT算法利用其分解MDPs，实验表明选好地标可提升UCT在线概率规划性能，且贪心与长期目标平衡因问题而异。


<details>
  <summary>Details</summary>
Motivation: 地标在经典规划有进展，但很少用于随机领域，希望在随机领域利用地标。

Method: 形式化概率地标，调整UCT算法，以地标为子目标分解MDPs，平衡贪心地标达成和最终目标达成。

Result: 在基准领域实验显示，选好地标能显著提升UCT在线概率规划性能，贪心与长期目标的最佳平衡因问题而异。

Conclusion: 地标可为求解MDPs的任意时间算法提供有用指导。

Abstract: Landmarks$\unicode{x2013}$conditions that must be satisfied at some point in
every solution plan$\unicode{x2013}$have contributed to major advancements in
classical planning, but they have seldom been used in stochastic domains. We
formalize probabilistic landmarks and adapt the UCT algorithm to leverage them
as subgoals to decompose MDPs; core to the adaptation is balancing between
greedy landmark achievement and final goal achievement. Our results in
benchmark domains show that well-chosen landmarks can significantly improve the
performance of UCT in online probabilistic planning, while the best balance of
greedy versus long-term goal achievement is problem-dependent. The results
suggest that landmarks can provide helpful guidance for anytime algorithms
solving MDPs.

</details>


### [11] [Inspire or Predict? Exploring New Paradigms in Assisting Classical Planners with Large Language Models](https://arxiv.org/abs/2508.11524)
*Wenkai Yu,Jianhang Tang,Yang Zhang,Shanjiang Tang,Kebing Jin,Hankz Hankui Zhuo*

Main category: cs.AI

TL;DR: 本文提出结合问题分解的LLM辅助规划器，通过两种范式利用LLM辅助分解问题，实验验证其在多领域解决大规模规划问题的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有利用大语言模型（LLMs）解决大规模规划问题的工作大多忽略结合特定领域知识以确保生成有效计划。

Method: 提出结合问题分解的LLM辅助规划器，使用LLM4Inspire和LLM4Predict两种范式辅助问题分解，前者提供启发式指导，后者利用特定领域知识推断中间条件。

Result: 实验验证了规划器在多领域的有效性，表明LLMs在剪枝搜索空间时能有效定位可行解，融入特定领域知识的LLM4Predict比提供通用知识的LLM4Inspire更有前景。

Conclusion: 提出的规划器在解决大规模规划问题时能进行搜索空间划分，LLMs在剪枝搜索空间方面有效，融入特定领域知识的方法更具优势。

Abstract: Addressing large-scale planning problems has become one of the central
challenges in the planning community, deriving from the state-space explosion
caused by growing objects and actions. Recently, researchers have explored the
effectiveness of leveraging Large Language Models (LLMs) to generate helpful
actions and states to prune the search space. However, prior works have largely
overlooked integrating LLMs with domain-specific knowledge to ensure valid
plans. In this paper, we propose a novel LLM-assisted planner integrated with
problem decomposition, which first decomposes large planning problems into
multiple simpler sub-tasks. Then we explore two novel paradigms to utilize
LLMs, i.e., LLM4Inspire and LLM4Predict, to assist problem decomposition, where
LLM4Inspire provides heuristic guidance according to general knowledge and
LLM4Predict employs domain-specific knowledge to infer intermediate conditions.
We empirically validate the effectiveness of our planner across multiple
domains, demonstrating the ability of search space partition when solving
large-scale planning problems. The experimental results show that LLMs
effectively locate feasible solutions when pruning the search space, where
infusing domain-specific knowledge into LLMs, i.e., LLM4Predict, holds
particular promise compared with LLM4Inspire, which offers general knowledge
within LLMs.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [12] [Tabularis Formatus: Predictive Formatting for Tables](https://arxiv.org/abs/2508.11121)
*Mukul Singh,José Cambronero,Sumit Gulwani,Vu Le,Gust Verbruggen*

Main category: cs.DB

TL;DR: 提出神经符号方法TaFo为表格生成条件格式建议，评估显示其优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 电子表格操作软件中创建条件格式规则复杂，存在用户认知不足、规则创建困难和界面不佳等问题。

Method: 受基于组件的合成系统启发，结合语言模型语义知识和多样性规则排名，自动学习规则触发和视觉格式属性，去除对用户规范的依赖。

Result: 使用180万公共工作簿语料评估，TaFo生成的格式建议更准确、多样和完整，在匹配用户添加的真实规则上比现有系统高15.6% - 26.5%。

Conclusion: TaFo在表格条件格式建议生成方面优于当前系统。

Abstract: Spreadsheet manipulation software are widely used for data management and
analysis of tabular data, yet the creation of conditional formatting (CF) rules
remains a complex task requiring technical knowledge and experience with
specific platforms. In this paper we present TaFo, a neuro-symbolic approach to
generating CF suggestions for tables, addressing common challenges such as user
unawareness, difficulty in rule creation, and inadequate user interfaces. TaFo
takes inspiration from component based synthesis systems and extends them with
semantic knowledge of language models and a diversity preserving rule
ranking.Unlike previous methods focused on structural formatting, TaFo uniquely
incorporates value-based formatting, automatically learning both the rule
trigger and the associated visual formatting properties for CF rules. By
removing the dependency on user specification used by existing techniques in
the form of formatted examples or natural language instruction, TaFo makes
formatting completely predictive and automated for the user. To evaluate TaFo,
we use a corpus of 1.8 Million public workbooks with CF and manual formatting.
We compare TaFo against a diverse set of symbolic and neural systems designed
for or adapted for the task of table formatting. Our results show that TaFo
generates more accurate, diverse and complete formatting suggestions than
current systems and outperforms these by 15.6\%--26.5\% on matching user added
ground truth rules in tables.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [13] [EMLIO: Minimizing I/O Latency and Energy Consumption for Large-Scale AI Training](https://arxiv.org/abs/2508.11035)
*Hasibul Jamil,MD S Q Zulkar Nine,Tevfik Kosar*

Main category: cs.DC

TL;DR: 大规模深度学习工作负载面临I/O瓶颈，现有系统忽略I/O能耗，EMLIO可降低数据加载延迟和I/O能耗。


<details>
  <summary>Details</summary>
Motivation: 大规模深度学习工作负载受I/O瓶颈影响，且现有系统忽略I/O能耗这一关键因素。

Method: 部署轻量级数据服务守护进程，对原始样本序列化和批处理，通过TCP流式传输并采用无序预取，与客户端GPU加速预处理集成。

Result: 在不同网络环境评估中，EMLIO比现有加载器快8.6倍，能耗低10.9倍，且性能和能耗不受网络距离影响。

Conclusion: EMLIO的基于服务架构为下一代AI云的能耗感知I/O提供可扩展蓝图。

Abstract: Large-scale deep learning workloads increasingly suffer from I/O bottlenecks
as datasets grow beyond local storage capacities and GPU compute outpaces
network and disk latencies. While recent systems optimize data-loading time,
they overlook the energy cost of I/O - a critical factor at large scale. We
introduce EMLIO, an Efficient Machine Learning I/O service that jointly
minimizes end-to-end data-loading latency T and I/O energy consumption E across
variable-latency networked storage. EMLIO deploys a lightweight data-serving
daemon on storage nodes that serializes and batches raw samples, streams them
over TCP with out-of-order prefetching, and integrates seamlessly with
GPU-accelerated (NVIDIA DALI) preprocessing on the client side. In exhaustive
evaluations over local disk, LAN (0.05 ms & 10 ms RTT), and WAN (30 ms RTT)
environments, EMLIO delivers up to 8.6X faster I/O and 10.9X lower energy use
compared to state-of-the-art loaders, while maintaining constant performance
and energy profiles irrespective of network distance. EMLIO's service-based
architecture offers a scalable blueprint for energy-aware I/O in
next-generation AI clouds.

</details>


### [14] [Element and Everything Tokens: Two-Tier Architecture for Mobilizing Alternative Assets](https://arxiv.org/abs/2508.11266)
*Ailiya Borjigin,Cong He,Charles CC Lee,Wei Zhou*

Main category: cs.DC

TL;DR: 本文提出两层代币化架构增强复杂另类资产流动性和透明度，介绍元素代币和全资产代币概念，以能源和工业为例说明可让非流动资产像股票或ETF交易，并讨论利弊。


<details>
  <summary>Details</summary>
Motivation: 传统框架下另类资产价值难交易和分割，需提升其流动性和透明度。

Method: 提出两层代币化架构，引入元素代币和全资产代币概念，设置双向可转换系统及套利机制。

Result: 通过能源和工业示例，证明该方法能使非流动资产像股票或ETF一样分割和交易。

Conclusion: 该架构对投资者和资产所有者有益，如降低准入门槛、改善价格发现和灵活融资，但也需考虑实施和监管问题。

Abstract: Alternative assets such as mines, power plants, or infrastructure projects
are often large, heterogeneous bundles of resources, rights, and outputs whose
value is difficult to trade or fractionalize under traditional frameworks. This
paper proposes a novel two-tier tokenization architecture to enhance the
liquidity and transparency of such complex assets. We introduce the concepts of
Element Tokens and Everything Tokens: elemental tokens represent standardized,
fully collateralized components of an asset (e.g., outputs, rights, or
credits), while an everything token represents the entire asset as a fixed
combination of those elements. The architecture enables both fine-grained
partial ownership and integrated whole-asset ownership through a system of
two-way convertibility. We detail the design and mechanics of this system,
including an arbitrage mechanism that keeps the price of the composite token
aligned with the net asset value of its constituents. Through illustrative
examples in the energy and industrial sectors, we demonstrate that our approach
allows previously illiquid, high-value projects to be fractionalized and traded
akin to stocks or exchange-traded funds (ETFs). We discuss the benefits for
investors and asset owners, such as lower entry barriers, improved price
discovery, and flexible financing, as well as the considerations for
implementation and regulation.

</details>


### [15] [Efficient GPU-Centered Singular Value Decomposition Using the Divide-and-Conquer Method](https://arxiv.org/abs/2508.11467)
*Shifang Liu,Huiyuan Li,Hongjiao Sheng,Haoyuan Gui,Xiaoyu Zhang*

Main category: cs.DC

TL;DR: 本文提出以GPU为中心的SVD算法，优化计算和数据布局，消除CPU - GPU数据传输，实验显示相比现有方法有显著加速。


<details>
  <summary>Details</summary>
Motivation: 传统SVD方法在异构系统中存在面板分解慢和频繁CPU - GPU数据传输问题，尽管GPU计算能力提升。

Method: 引入基于GPU的双对角分治（BDC）方法，重新设计SVD计算步骤的算法和数据布局，在GPU上完成所有面板级计算和尾随矩阵更新，整合相关计算优化BLAS使用，开发新的基于GPU的BDC算法消除矩阵级CPU - GPU数据传输并实现CPU和GPU异步执行。

Result: 在AMD MI210和NVIDIA V100 GPU上实验，相比rocSOLVER/cuSOLVER和MAGMA，分别实现最高1293.64x/7.47x和14.10x/12.38x的加速。

Conclusion: 所提以GPU为中心的SVD算法能有效解决传统SVD方法的问题，显著提升计算速度。

Abstract: Singular Value Decomposition (SVD) is a fundamental matrix factorization
technique in linear algebra, widely applied in numerous matrix-related
problems. However, traditional SVD approaches are hindered by slow panel
factorization and frequent CPU-GPU data transfers in heterogeneous systems,
despite advancements in GPU computational capabilities. In this paper, we
introduce a GPU-centered SVD algorithm, incorporating a novel GPU-based
bidiagonal divide-and-conquer (BDC) method. We reformulate the algorithm and
data layout of different steps for SVD computation, performing all panel-level
computations and trailing matrix updates entirely on GPU to eliminate CPU-GPU
data transfers. Furthermore, we integrate related computations to optimize BLAS
utilization, thereby increasing arithmetic intensity and fully leveraging the
computational capabilities of GPUs. Additionally, we introduce a newly
developed GPU-based BDC algorithm that restructures the workflow to eliminate
matrix-level CPU-GPU data transfers and enable asynchronous execution between
the CPU and GPU. Experimental results on AMD MI210 and NVIDIA V100 GPUs
demonstrate that our proposed method achieves speedups of up to 1293.64x/7.47x
and 14.10x/12.38x compared to rocSOLVER/cuSOLVER and MAGMA, respectively.

</details>


### [16] [Inter-APU Communication on AMD MI300A Systems via Infinity Fabric: a Deep Dive](https://arxiv.org/abs/2508.11298)
*Gabin Schieffer,Jacob Wahlgren,Ruimin Shi,Edgar A. León,Roger Pearce,Maya Gokhale,Ivy Peng*

Main category: cs.DC

TL;DR: 本文设计基准测试评估AMD MI300A系统中APU间通信效率，对比不同库，优化两个HPC应用并评估。


<details>
  <summary>Details</summary>
Motivation: GPU加速器计算性能提升，需要高效的数据移动，AMD MI300A APU可缓解CPU - GPU数据移动问题，需评估其APU间通信效率。

Method: 设计特定基准测试评估GPU直接内存访问、APU间显式数据移动和多APU集体通信，比较HIP APIs、MPI例程和RCCL库效率。

Result: 得出在具有Infinity Fabric的多APU AMD MI300A系统上优化APU间通信的关键设计选择。

Conclusion: 优化了Quicksilver和CloverLeaf两个真实HPC应用，并在四MI100A APU系统上进行评估。

Abstract: The ever-increasing compute performance of GPU accelerators drives up the
need for efficient data movements within HPC applications to sustain
performance. Proposed as a solution to alleviate CPU-GPU data movement, AMD
MI300A Accelerated Processing Unit (APU) combines CPU, GPU, and high-bandwidth
memory (HBM) within a single physical package. Leadership supercomputers, such
as El Capitan, group four APUs within a single compute node, using Infinity
Fabric Interconnect. In this work, we design specific benchmarks to evaluate
direct memory access from the GPU, explicit inter-APU data movement, and
collective multi-APU communication. We also compare the efficiency of HIP APIs,
MPI routines, and the GPU-specialized RCCL library. Our results highlight key
design choices for optimizing inter-APU communication on multi-APU AMD MI300A
systems with Infinity Fabric, including programming interfaces, allocators, and
data movement. Finally, we optimize two real HPC applications, Quicksilver and
CloverLeaf, and evaluate them on a four MI100A APU system.

</details>


### [17] [Space-efficient population protocols for exact majority in general graphs](https://arxiv.org/abs/2508.11384)
*Joel Rybicki,Jakob Solnerzik,Olivier Stietel,Robin Vacus*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study exact majority consensus in the population protocol model. In this
model, the system is described by a graph $G = (V,E)$ with $n$ nodes, and in
each time step, a scheduler samples uniformly at random a pair of adjacent
nodes to interact. In the exact majority consensus task, each node is given a
binary input, and the goal is to design a protocol that almost surely reaches a
stable configuration, where all nodes output the majority input value.
  We give improved upper and lower bounds for the exact majority in general
graphs. First, we give asymptotically tight time lower bounds for general
(unbounded space) protocols. Second, we obtain new upper bounds parameterized
by the relaxation time $\tau_{\mathsf{rel}}$ of the random walk on $G$ induced
by the scheduler and the degree imbalance $\Delta/\delta$ of $G$. Specifically,
we give a protocol that stabilizes in $O\left( \tfrac{\Delta}{\delta}
\tau_{\mathsf{rel}} \log^2 n \right)$ steps in expectation and with high
probability and uses $O\left( \log n \cdot \left(
\log\left(\tfrac{\Delta}{\delta}\right) + \log
\left(\tfrac{\tau_{\mathsf{rel}}}{n}\right) \right) \right)$ states in any
graph with minimum degree at least $\delta$ and maximum degree at most
$\Delta$.
  For regular expander graphs, this matches the optimal space complexity of
$\Theta(\log n)$ for fast protocols in complete graphs [Alistarh et al., SODA
2016 and Doty et al., FOCS 2022] with a nearly optimal stabilization time of
$O(n \log^2 n)$ steps. Finally, we give a new upper bound of
$O(\tau_{\mathsf{rel}} \cdot n \log n)$ for the stabilization time of a
constant-state protocol.

</details>


### [18] [Time, Fences and the Ordering of Events in TSO](https://arxiv.org/abs/2508.11415)
*Raïssa Nataf,Yoram Moses*

Main category: cs.DC

TL;DR: 本文为TSO内存模型开发语义框架，引入新的发生前关系，证明确保不同站点事件时间顺序需创建发生前链，分析同步操作必要性，推广共享内存对象线性化实现的下界。


<details>
  <summary>Details</summary>
Motivation: TSO虽支持硬件优化、提升多处理器效率，但使正确性推理复杂，插入同步原语有性能开销，需明确同步何时必要。

Method: 开发语义框架，引入TSO特定的发生前关系，研究内存屏障和原子读 - 修改 - 写操作在创建发生前链中的作用。

Result: 证明确保不同站点事件时间顺序需创建发生前链，能捕捉同步操作不可避免的情况，推广线性化实现的下界。

Conclusion: 分析为TSO模型提供理论理解，将异步系统基于通信的推理扩展到TSO内存模型，捕捉信息流动和因果结构。

Abstract: The Total Store Order (TSO) is arguably the most widely used relaxed memory
model in multiprocessor architectures, widely implemented, for example in
Intel's x86 and x64 platforms. It allows processes to delay the visibility of
writes through store buffering. While this supports hardware-level
optimizations and makes a significant contribution to multiprocessor
efficiency, it complicates reasoning about correctness, as executions may
violate sequential consistency. Ensuring correct behavior often requires
inserting synchronization primitives such as memory fences ($F$) or atomic
read-modify-write ($RMW$) operations, but this approach can incur significant
performance costs. In this work, we develop a semantic framework that precisely
characterizes when such synchronization is necessary under TSO. We introduce a
novel TSO-specific occurs-before relation, which adapts Lamport's celebrated
happens-before relation from asynchronous message-passing systems to the TSO
setting. Our main result is a theorem that proves that the only way to ensure
that two events that take place at different sites are temporally ordered is by
having the execution create an occurs-before chain between the events. By
studying the role of fences and $RMW$s in creating occurs-before chains, we are
then able to capture cases in which these costly synchronization operations are
unavoidable. Since proper real-time ordering of events is a fundamental aspect
of consistency conditions such as Linearizability, our analysis provides a
sound theoretical understanding of essential aspects of the TSO model. In
particular, we are able to generalize prior lower bounds for linearizable
implementations of shared memory objects. Our results capture the structure of
information flow and causality in the TSO model by extending the standard
communication-based reasoning from asynchronous systems to the TSO memory
model.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [19] [A Gentle Wakeup Call: Symmetry Breaking with Less Collision Cost](https://arxiv.org/abs/2508.11006)
*Umesh Biswas,Maxwell Young*

Main category: cs.DS

TL;DR: 提出随机唤醒算法Aim - High，分析其在静态和动态版本中的延迟和碰撞成本，并给出下界。


<details>
  <summary>Details</summary>
Motivation: 现有唤醒算法未解决碰撞引入的显著延迟问题，导致总延迟大。

Method: 设计并分析随机唤醒算法Aim - High。

Result: 对于足够大的C且有界误差时，静态和动态版本的延迟和期望碰撞成本接近O(√C)；否则，静态设置为O(poly(log n))，动态设置为O(n poly(log n))，并建立了下界。

Conclusion: Aim - High算法在解决唤醒问题的延迟和碰撞成本方面有较好效果。

Abstract: The wakeup problem addresses the fundamental challenge of symmetry breaking.
There are $n$ devices sharing a time-slotted multiple access channel. In any
fixed slot, if a single device sends a packet, it succeeds; however, if two or
more devices send, then there is a collision and none of the corresponding
packets succeed. For the static version of wakeup, all packets are initially
active (i.e., can send and listen on the channel); for the dynamic version, the
packets become active at arbitrary times. In both versions, the goal is to
successfully send a single packet.
  Prior results on wakeup have largely focused on the number of slots until the
first success; that is, the latency. However, in many modern systems,
collisions introduce significant delay, an aspect that current wakeup
algorithms do not address. For instance, while existing results for static
wakeup have polylogarithmic-in-$n$ latency, they can incur additional latency
that is {\it linear} in the cost of a collision $C$. Thus, the total latency is
large and dominated by the contributions from collisions.
  Here, we design and analyze a randomized wakeup algorithm, Aim-High. For
sufficiently large $C$ and with bounded error, Aim-High has latency and
expected collision cost that is nearly $O(\sqrt{C})$ for both the static and
dynamic versions. Otherwise, the latency and expected collision cost are
$O(\texttt{poly}{(\log n)})$ for the static setting, and
$O(n\,\texttt{poly}{(\log n)})$ for the dynamic setting. We also establish
lower bounds that complement these results.

</details>


### [20] [Sampling tree-weighted partitions without sampling trees](https://arxiv.org/abs/2508.11130)
*Sarah Cannon,Wesley Pegden,Jamie Tucker-Foltz*

Main category: cs.DS

TL;DR: 本文提出平面图树加权分区抽样新算法，可直接对平衡树加权2 - 分区分布抽样，在多种平面图上期望时间为线性。


<details>
  <summary>Details</summary>
Motivation: 计算选区重划分析问题促使对平衡分区的条件分布产生兴趣，现有2 - 分区采样器存在计算瓶颈。

Method: 提出可直接从平衡树加权2 - 分区分布抽样的算法，无需先抽样生成树。

Result: 算法在多种平面图上期望时间为线性O(n)，渐近快于已知最佳生成随机树方法；算法变体对精确抽样均匀随机树可加速至O(n log n)。

Conclusion: 新算法在平面图树加权分区抽样和随机树抽样上有更好的时间复杂度表现。

Abstract: This paper gives a new algorithm for sampling tree-weighted partitions of a
large class of planar graphs. Formally, the tree-weighted distribution on
$k$-partitions of a graph weights $k$-partitions proportional to the product of
the number of spanning trees of each partition class. Recent work on problems
in computational redistricting analysis has driven special interest in the
conditional distribution where all partition classes have the same size
(balanced partitions). One class of Markov chains in wide use aims to sample
from balanced tree-weighted $k$-partitions using a sampler for balanced
tree-weighted 2-partitions. Previous implementations of this 2-partition
sampler would draw a random spanning tree and check whether it contains an edge
whose removal produces a balanced 2-component forest; if it does, this
2-partition is accepted, otherwise the algorithm rejects and repeats. In
practice, this is a significant computational bottleneck.
  We show that in fact it is possible to sample from the balanced tree-weighted
2-partition distribution directly, without first sampling a spanning tree; the
acceptance and rejection rates are the same as in previous samplers. We prove
that on a wide class of planar graphs encompassing network structures typically
arising from the geographic data used in computational redistricting, our
algorithm takes expected linear time $O(n)$. Notably, this is asymptotically
faster than the best known method to generate random trees, which is $O(n
\log^2 n)$ for approximate sampling and $O(n^{1 + \log \log \log n / \log \log
n})$ for exact sampling. Additionally, we show that a variant of our algorithm
also gives a speedup to $O(n \log n)$ for exact sampling of uniformly random
trees on these families of graphs, improving the bounds for both exact and
approximate sampling.

</details>


### [21] [Face-hitting dominating sets in planar graphs: Alternative proof and linear-time algorithm](https://arxiv.org/abs/2508.11444)
*Therese Biedl*

Main category: cs.DS

TL;DR: 本文给出平面n顶点图在相同限制下划分为两个支配面命中集的新证明，新证明具构造性且可线性时间完成。


<details>
  <summary>Details</summary>
Motivation: 原证明非算法性且依赖不易实现的四色定理，需新证明。

Method: 通过将图拆分为2 - 连通分量、找耳分解、计算3 - 正则平面图的完美匹配等。

Result: 能在线性时间内找到顶点划分。

Conclusion: 新证明具构造性，比原证明更优，可在线性时间内完成顶点划分。

Abstract: In a recent paper, Francis, Illickan, Jose and Rajendraprasad showed that
every $n$-vertex plane graph $G$ has (under some natural restrictions) a
vertex-partition into two sets $V_1$ and $V_2$ such that each $V_i$ is
\emph{dominating} (every vertex of $G$ contains a vertex of $V_i$ in its closed
neighbourhood) and \emph{face-hitting} (every face of $G$ is incident to a
vertex of $V_i$). Their proof works by considering a supergraph $G'$ of $G$
that has certain properties, and among all such graphs, taking one that has the
fewest edges. As such, their proof is not algorithmic. Their proof also relies
on the 4-color theorem, for which a quadratic-time algorithm exists, but it
would not be easy to implement.
  In this paper, we give a new proof that every $n$-vertex plane graph $G$ has
(under the same restrictions) a vertex-partition into two dominating
face-hitting sets. Our proof is constructive, and requires nothing more
complicated than splitting a graph into 2-connected components, finding an ear
decomposition, and computing a perfect matching in a 3-regular plane graph. For
all these problems, linear-time algorithms are known and so we can find the
vertex-partition in linear time.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [22] [Can We Tell if ChatGPT is a Parasite? Studying Human-AI Symbiosis with Game Theory](https://arxiv.org/abs/2508.11359)
*Jiejun Hu-Bolz,James Stovold*

Main category: cs.GT

TL;DR: 本文探讨人与生成式AI能否通过交互融合为一体，建立三人随机博弈模型，用信息论指标证明可形成聚合个体，还能解答人机共生相关问题。


<details>
  <summary>Details</summary>
Motivation: 探究人与生成式AI系统能否通过迭代、信息驱动的交互融合为单一个体。

Method: 将人与生成式AI及人类更广泛环境的交互建模为三人随机博弈，使用信息论指标（熵、互信息和转移熵）进行分析。

Result: 证明了所建模的人类和生成式AI能够在Krakauer等人（2020）的意义上形成聚合个体。

Conclusion: 所提出的模型能够解答围绕人类和AI系统共生性质的有趣问题。

Abstract: This work asks whether a human interacting with a generative AI system can
merge into a single individual through iterative, information-driven
interactions. We model the interactions between a human, a generative AI
system, and the human's wider environment as a three-player stochastic game. We
use information-theoretic measures (entropy, mutual information, and transfer
entropy) to show that our modelled human and generative AI are able to form an
aggregate individual in the sense of Krakauer et al. (2020). The model we
present is able to answer interesting questions around the symbiotic nature of
humans and AI systems, including whether LLM-driven chatbots are acting as
parasites, feeding on the information provided by humans.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [23] [PaperRegister: Boosting Flexible-grained Paper Search via Hierarchical Register Indexing](https://arxiv.org/abs/2508.11116)
*Zhuoqun Li,Xuanang Chen,Hongyu Lin,Yaojie Lu,Xianpei Han,Le Sun*

Main category: cs.IR

TL;DR: 提出PaperRegister系统用于灵活粒度的论文搜索，实验表明其性能达到最优，尤其在细粒度场景表现出色。


<details>
  <summary>Details</summary>
Motivation: 以往论文搜索系统无法满足灵活粒度的搜索需求，因主要基于论文摘要构建索引，缺乏详细信息。

Method: 提出由离线分层索引和在线自适应检索组成的PaperRegister，将传统基于摘要的索引转换为分层索引树。

Result: 在一系列粒度的论文搜索任务实验中，PaperRegister达到了最优性能，在细粒度场景表现突出。

Conclusion: PaperRegister在现实应用中作为灵活粒度论文搜索的有效解决方案具有良好潜力。

Abstract: Paper search is an important activity for researchers, typically involving
using a query with description of a topic to find relevant papers. As research
deepens, paper search requirements may become more flexible, sometimes
involving specific details such as module configuration rather than being
limited to coarse-grained topics. However, previous paper search systems are
unable to meet these flexible-grained requirements, as these systems mainly
collect paper abstracts to construct index of corpus, which lack detailed
information to support retrieval by finer-grained queries. In this work, we
propose PaperRegister, consisted of offline hierarchical indexing and online
adaptive retrieval, transforming traditional abstract-based index into
hierarchical index tree for paper search, thereby supporting queries at
flexible granularity. Experiments on paper search tasks across a range of
granularity demonstrate that PaperRegister achieves the state-of-the-art
performance, and particularly excels in fine-grained scenarios, highlighting
the good potential as an effective solution for flexible-grained paper search
in real-world applications. Code for this work is in
https://github.com/Li-Z-Q/PaperRegister.

</details>


### [24] [+VeriRel: Verification Feedback to Enhance Document Retrieval for Scientific Fact Checking](https://arxiv.org/abs/2508.11122)
*Xingyu Deng,Xi Wang,Mark Stevenson*

Main category: cs.IR

TL;DR: 本文提出+VeriRel算法用于科学事实核查的文档证据检索，实验显示其表现出色，强调结合验证反馈评估文档相关性的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有科学事实核查方法依赖基于相关性而非证据的文档排名算法，需改进。

Method: 提出+VeriRel，将验证成功率纳入文档排名。

Result: +VeriRel在三个科学事实核查数据集上文档证据检索表现领先，对下游验证有积极影响。

Conclusion: 结合验证反馈评估文档相关性对有效科学事实核查系统有潜力，未来可评估细粒度相关性。

Abstract: Identification of appropriate supporting evidence is critical to the success
of scientific fact checking. However, existing approaches rely on off-the-shelf
Information Retrieval algorithms that rank documents based on relevance rather
than the evidence they provide to support or refute the claim being checked.
This paper proposes +VeriRel which includes verification success in the
document ranking. Experimental results on three scientific fact checking
datasets (SciFact, SciFact-Open and Check-Covid) demonstrate consistently
leading performance by +VeriRel for document evidence retrieval and a positive
impact on downstream verification. This study highlights the potential of
integrating verification feedback to document relevance assessment for
effective scientific fact checking systems. It shows promising future work to
evaluate fine-grained relevance when examining complex documents for advanced
scientific fact checking.

</details>


### [25] [Role-Augmented Intent-Driven Generative Search Engine Optimization](https://arxiv.org/abs/2508.11158)
*Xiaolu Chen,Haojie Wu,Jie Bao,Zhen Chen,Yong Liao,Hu Huang*

Main category: cs.IR

TL;DR: 生成式搜索引擎重塑信息检索，但黑盒性质破坏SEO实践，提出G - SEO方法及评估方案，实验表明搜索意图能有效指导内容优化。


<details>
  <summary>Details</summary>
Motivation: 商业生成式搜索引擎的黑盒性质使传统SEO策略失效，内容创作者面临优化挑战，需新的优化方法。

Method: 提出Role - Augmented Intent - Driven Generative Search Engine Optimization (G - SEO)方法，通过不同信息角色的反思式细化来建模搜索意图；扩展GEO数据集，引入G - Eval 2.0评估标准。

Result: 实验显示搜索意图作为信号指导内容优化，在主观印象和客观内容可见性上比单一方面的基线方法有显著提升。

Conclusion: 搜索意图能有效指导生成式搜索引擎的内容优化，G - SEO方法可行。

Abstract: Generative Search Engines (GSEs), powered by Large Language Models (LLMs) and
Retrieval-Augmented Generation (RAG), are reshaping information retrieval.
While commercial systems (e.g., BingChat, Perplexity.ai) demonstrate impressive
semantic synthesis capabilities, their black-box nature fundamentally
undermines established Search Engine Optimization (SEO) practices. Content
creators face a critical challenge: their optimization strategies, effective in
traditional search engines, are misaligned with generative retrieval contexts,
resulting in diminished visibility. To bridge this gap, we propose a
Role-Augmented Intent-Driven Generative Search Engine Optimization (G-SEO)
method, providing a structured optimization pathway tailored for GSE scenarios.
Our method models search intent through reflective refinement across diverse
informational roles, enabling targeted content enhancement. To better evaluate
the method under realistic settings, we address the benchmarking limitations of
prior work by: (1) extending the GEO dataset with diversified query variations
reflecting real-world search scenarios and (2) introducing G-Eval 2.0, a
6-level LLM-augmented evaluation rubric for fine-grained human-aligned
assessment. Experimental results demonstrate that search intent serves as an
effective signal for guiding content optimization, yielding significant
improvements over single-aspect baseline approaches in both subjective
impressions and objective content visibility within GSE responses.

</details>


### [26] [Representation Quantization for Collaborative Filtering Augmentation](https://arxiv.org/abs/2508.11194)
*Yunze Luo,Yinjie Jiang,Gaode Chen,Jingchi Wang,Shicheng Wang,Ruina Sun,Jiang Yuezihan,Jun Zhang,Jian Liang,Han Li,Kun Gai,Kaigui Bian*

Main category: cs.IR

TL;DR: 提出两阶段协同推荐算法 DQRec 解决协同过滤数据稀疏问题，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有协同过滤算法受数据稀疏问题困扰，且现有解决方法在提取特征和连接方面存在局限，无法有效结合交互序列和属性提取行为特征。

Method: 提出 DQRec 算法，基于 DQ - VAE 从交互序列和属性中联合提取行为特征，将预训练表示嵌入分解并量化生成语义 ID，通过特征和连接增强将语义 ID 模式集成到推荐过程。

Result: 在多个数据集上与基线方法的实验对比，证明 DQRec 方法性能优越。

Conclusion: DQRec 算法能有效解决协同过滤数据稀疏问题，通过提取和集成特征模式，提高信息传播效率和推荐性能。

Abstract: As the core algorithm in recommendation systems, collaborative filtering (CF)
algorithms inevitably face the problem of data sparsity. Since CF captures
similar users and items for recommendations, it is effective to augment the
lacking user-user and item-item homogeneous linkages. However, existing methods
are typically limited to connecting through overlapping interacted neighbors or
through similar attributes and contents. These approaches are constrained by
coarse-grained, sparse attributes and fail to effectively extract behavioral
characteristics jointly from interaction sequences and attributes. To address
these challenges, we propose a novel two-stage collaborative recommendation
algorithm, DQRec: Decomposition-based Quantized Variational AutoEncoder
(DQ-VAE) for Recommendation. DQRec augments features and homogeneous linkages
by extracting the behavior characteristics jointly from interaction sequences
and attributes, namely patterns, such as user multi-aspect interests. Inspired
by vector quantization (VQ) technology, we propose a new VQ algorithm, DQ-VAE,
which decomposes the pre-trained representation embeddings into distinct
dimensions, and quantize them to generates semantic IDs. We utilize the
generated semantic IDs as the extracted patterns mentioned above. By
integrating these semantic ID patterns into the recommendation process through
feature and linkage augmentation, the system enriches both latent and explicit
user and item features, identifies pattern-similar neighbors, and thereby
improves the efficiency of information diffusion. Experimental comparisons with
baselines across multiple datasets demonstrate the superior performance of the
proposed DQRec method.

</details>


### [27] [Mitigating Filter Bubble from the Perspective of Community Detection: A Universal Framework](https://arxiv.org/abs/2508.11239)
*Ming Tang,Xiaowen Huang,Jitao Sang*

Main category: cs.IR

TL;DR: 提出CD - CGCN框架从社区检测角度解决推荐系统的过滤气泡问题，实验验证其在减轻过滤气泡同时保持推荐质量的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统重准确性轻多样性，加剧过滤气泡效应。

Method: 提出CD - CGCN模型无关框架，集成条件判别器和社区重加权图卷积网络，采用基于社区标签的对抗学习，结合针对用户过滤气泡状态的推理策略。

Result: 在真实数据集上用多个基础模型实验，验证其减轻过滤气泡且保持推荐质量的有效性，在构建的无偏测试集上能更好捕捉用户社区间偏好。

Conclusion: CD - CGCN能有效解决推荐系统的过滤气泡问题，在减轻气泡同时保证推荐质量。

Abstract: In recent years, recommender systems have primarily focused on improving
accuracy at the expense of diversity, which exacerbates the well-known filter
bubble effect. This paper proposes a universal framework called CD-CGCN to
address the filter bubble issue in recommender systems from a community
detection perspective. By analyzing user-item interaction histories with a
community detection algorithm, we reveal that state-of-the-art recommendations
often focus on intra-community items, worsening the filter bubble effect.
CD-CGCN, a model-agnostic framework, integrates a Conditional Discriminator and
a Community-reweighted Graph Convolutional Network which can be plugged into
most recommender models. Using adversarial learning based on community labels,
it counteracts the extracted community attributes and incorporates an inference
strategy tailored to the user's specific filter bubble state. Extensive
experiments on real-world datasets with multiple base models validate its
effectiveness in mitigating filter bubbles while preserving recommendation
quality. Additionally, by applying community debiasing to the original test set
to construct an unbiased test set, we observe that CD-CGCN demonstrates
superior performance in capturing users' inter-community preferences.

</details>


### [28] [INFNet: A Task-aware Information Flow Network for Large-Scale Recommendation Systems](https://arxiv.org/abs/2508.11565)
*Kaiyuan Li,Dongdong Mao,Yongxiang Tang,Yanhua Cheng,Yanxiang Zeng,Chao Wang,Xialong Liu,Peng Jiang*

Main category: cs.IR

TL;DR: 现有特征交互策略在工业应用有计算和多任务建模问题，提出INFNet架构，实验表现佳且上线效果好。


<details>
  <summary>Details</summary>
Motivation: 解决现有特征交互策略在工业应用中计算成本高和多任务建模能力受限的问题。

Method: 提出INFNet架构，区分三种令牌类型，采用双信息流设计，包括跨注意力机制和类型特定的代理门控单元。

Result: 在多个离线基准测试中达到了最先进的性能，在商业在线广告系统中使收入和点击率显著提升。

Conclusion: INFNet架构有效解决了现有特征交互策略的问题，具有良好性能和实际应用价值。

Abstract: Feature interaction has long been a cornerstone of ranking models in
large-scale recommender systems due to its proven effectiveness in capturing
complex dependencies among features. However, existing feature interaction
strategies face two critical challenges in industrial applications: (1) The
vast number of categorical and sequential features makes exhaustive interaction
computationally prohibitive, often resulting in optimization difficulties. (2)
Real-world recommender systems typically involve multiple prediction
objectives, yet most current approaches apply feature interaction modules prior
to the multi-task learning layers. This late-fusion design overlooks
task-specific feature dependencies and inherently limits the capacity of
multi-task modeling. To address these limitations, we propose the Information
Flow Network (INFNet), a task-aware architecture designed for large-scale
recommendation scenarios. INFNet distinguishes features into three token types,
categorical tokens, sequence tokens, and task tokens, and introduces a novel
dual-flow design comprising heterogeneous and homogeneous alternating
information blocks. For heterogeneous information flow, we employ a
cross-attention mechanism with proxy that facilitates efficient cross-modal
token interaction with balanced computational cost. For homogeneous flow, we
design type-specific Proxy Gated Units (PGUs) to enable fine-grained intra-type
feature processing. Extensive experiments on multiple offline benchmarks
confirm that INFNet achieves state-of-the-art performance. Moreover, INFNet has
been successfully deployed in a commercial online advertising system, yielding
significant gains of +1.587% in Revenue (REV) and +1.155% in Click-Through Rate
(CTR).

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [29] [Compressive Meta-Learning](https://arxiv.org/abs/2508.11090)
*Daniel Mas Montserrat,David Bonet,Maria Perera,Xavier Giró-i-Nieto,Alexander G. Ioannidis*

Main category: cs.LG

TL;DR: 新数据集规模扩大需高效参数学习技术，现有压缩学习方法未充分利用数据结构，本文提出压缩元学习框架，用神经网络优化编码和解码，还探索多应用。


<details>
  <summary>Details</summary>
Motivation: 新数据集规模快速扩张，现有压缩学习方法未利用数据结构，需要更高效准确的学习框架。

Method: 提出压缩元学习框架，使用神经网络对压缩学习的编码和解码阶段进行元学习。

Result: 该框架比现有技术更快、更准确，探索了多个应用场景。

Conclusion: 提出的压缩元学习框架有潜力，可在多个应用中实现比现有方法更好的效果。

Abstract: The rapid expansion in the size of new datasets has created a need for fast
and efficient parameter-learning techniques. Compressive learning is a
framework that enables efficient processing by using random, non-linear
features to project large-scale databases onto compact, information-preserving
representations whose dimensionality is independent of the number of samples
and can be easily stored, transferred, and processed. These database-level
summaries are then used to decode parameters of interest from the underlying
data distribution without requiring access to the original samples, offering an
efficient and privacy-friendly learning framework. However, both the encoding
and decoding techniques are typically randomized and data-independent, failing
to exploit the underlying structure of the data. In this work, we propose a
framework that meta-learns both the encoding and decoding stages of compressive
learning methods by using neural networks that provide faster and more accurate
systems than the current state-of-the-art approaches. To demonstrate the
potential of the presented Compressive Meta-Learning framework, we explore
multiple applications -- including neural network-based compressive PCA,
compressive ridge regression, compressive k-means, and autoencoders.

</details>


### [30] [Nested Operator Inference for Adaptive Data-Driven Learning of Reduced-order Models](https://arxiv.org/abs/2508.11542)
*Nicole Aretz,Karen Willcox*

Main category: cs.LG

TL;DR: 提出数据驱动的嵌套算子推理方法学习物理信息降阶模型，在热传导和格陵兰冰盖模型中表现良好


<details>
  <summary>Details</summary>
Motivation: 从高维动力系统快照数据中学习物理信息降阶模型

Method: 利用降维空间的层次结构为算子推理学习问题迭代构造初始猜测，可从先前学习的模型热启动

Result: 在立方热传导问题中误差比标准算子推理小四倍，在格陵兰冰盖模型中平均误差3%，计算加速因子超19000

Conclusion: 嵌套算子推理方法有效，能用于动态基和模型形式更新的场景

Abstract: This paper presents a data-driven, nested Operator Inference (OpInf) approach
for learning physics-informed reduced-order models (ROMs) from snapshot data of
high-dimensional dynamical systems. The approach exploits the inherent
hierarchy within the reduced space to iteratively construct initial guesses for
the OpInf learning problem that prioritize the interactions of the dominant
modes. The initial guess computed for any target reduced dimension corresponds
to a ROM with provably smaller or equal snapshot reconstruction error than with
standard OpInf. Moreover, our nested OpInf algorithm can be warm-started from
previously learned models, enabling versatile application scenarios involving
dynamic basis and model form updates. We demonstrate the performance of our
algorithm on a cubic heat conduction problem, with nested OpInf achieving a
four times smaller error than standard OpInf at a comparable offline time.
Further, we apply nested OpInf to a large-scale, parameterized model of the
Greenland ice sheet where, despite model form approximation errors, it learns a
ROM with, on average, 3% error and computational speed-up factor above 19,000.

</details>


### [31] [A Cooperative Game-Based Multi-Criteria Weighted Ensemble Approach for Multi-Class Classification](https://arxiv.org/abs/2508.10926)
*DongSeong-Yoon*

Main category: cs.LG

TL;DR: 本文针对AI技术局限，提出通过合作博弈在多标准情况下决策的集成学习方法，实验显示性能优于其他加权方法。


<details>
  <summary>Details</summary>
Motivation: AI技术存在过拟合/欠拟合、类别不平衡等局限，现有考虑分类器先验信息的加权方法仅考虑单一评估标准，限制了模型对现实中各种信息的反映。

Method: 提出在多标准情况下通过合作博弈考虑各种信息进行决策的方法，将机器学习算法应用于Open - ML - CC18数据集，并与现有集成加权方法对比。

Result: 实验结果显示该方法性能优于其他加权方法。

Conclusion: 该方法能同时考虑和反映分类器中事先已知的各种类型信息，实现适当的权重分配并提升性能。

Abstract: Since the Fourth Industrial Revolution, AI technology has been widely used in
many fields, but there are several limitations that need to be overcome,
including overfitting/underfitting, class imbalance, and the limitations of
representation (hypothesis space) due to the characteristics of different
models. As a method to overcome these problems, ensemble, commonly known as
model combining, is being extensively used in the field of machine learning.
Among ensemble learning methods, voting ensembles have been studied with
various weighting methods, showing performance improvements. However, the
existing methods that reflect the pre-information of classifiers in weights
consider only one evaluation criterion, which limits the reflection of various
information that should be considered in a model realistically. Therefore, this
paper proposes a method of making decisions considering various information
through cooperative games in multi-criteria situations. Using this method,
various types of information known beforehand in classifiers can be
simultaneously considered and reflected, leading to appropriate weight
distribution and performance improvement. The machine learning algorithms were
applied to the Open-ML-CC18 dataset and compared with existing ensemble
weighting methods. The experimental results showed superior performance
compared to other weighting methods.

</details>


### [32] [Apriel-Nemotron-15B-Thinker](https://arxiv.org/abs/2508.10948)
*Shruthan Radhakrishna,Soham Parikh,Gopal Sarda,Anil Turkkan,Quaizar Vohra,Raymond Li,Dhruv Jhamb,Kelechi Ogueji,Aanjaneya Shukla,Oluwanifemi Bamgbose,Toby Liang,Luke Kumar,Oleksiy Ostapenko,Shiva Krishna Reddy Malay,Aman Tiwari,Tara Bogavelli,Vikas Yadav,Jash Mehta,Saloni Mittal,Akshay Kalkunte,Pulkit Pattnaik,Khalil Slimi,Anirudh Sreeram,Jishnu Nair,Akintunde Oladipo,Shashank Maiya,Khyati Mahajan,Rishabh Maheshwary,Masoud Hashemi,Sai Rajeswar Mudumba,Sathwik Tejaswi Madhusudhan,Torsten Scholak,Sebastien Paquet,Sagar Davasam,Srinivas Sunkara*

Main category: cs.LG

TL;DR: 介绍Apriel - Nemotron - 15B - Thinker模型，它参数少、内存占用低，性能可媲美或超320亿参数模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型内存和计算成本高，不适用于企业实际场景，需开发低成本高性能模型。

Method: 采用四阶段训练管道，包括基础模型升级、持续预训练、有监督微调、基于GRPO的强化学习。

Result: 综合评估显示，该模型虽规模不到320亿参数模型一半，但性能相当或更优。

Conclusion: Apriel - Nemotron - 15B - Thinker模型以较小规模实现了与中型先进模型相当甚至更好的性能。

Abstract: While large language models (LLMs) have achieved remarkable reasoning
capabilities across domains like code, math and other enterprise tasks, their
significant memory and computational costs often preclude their use in
practical enterprise settings. To this end, we introduce
Apriel-Nemotron-15B-Thinker, a 15-billion parameter model in the ServiceNow
Apriel SLM series that achieves performance against medium sized
state-of-the-art models such as o1-mini, QWQ32B, and EXAONE-Deep-32B while
maintaining only half the memory footprint of those alternatives.
Apriel-Nemotron-15B-Thinker model is trained in a four stage training pipeline
including 1) Base Model upscaling, 2) Continual Pre-training 3) Supervised
Fine-tuning (SFT) and 4) Reinforcement Learning using GRPO. Comprehensive
evaluations across a diverse suite of benchmarks consistently demonstrate that
our Apriel-Nemotron-15B-Thinker model matches or exceeds the performance of its
32-billion parameter counterparts, despite being less than half their size.

</details>


### [33] [Towards Efficient Prompt-based Continual Learning in Distributed Medical AI](https://arxiv.org/abs/2508.10954)
*Gyutae Oh,Jitae Shin*

Main category: cs.LG

TL;DR: 医疗领域数据共享受限，传统训练有问题，提出基于提示的持续学习方法PCL，实验显示其优于现有方法，有望推动医疗AI发展。


<details>
  <summary>Details</summary>
Motivation: 医疗领域数据共享受限，传统训练过拟合且会灾难性遗忘，现有持续学习方法多针对自然图像，医疗领域特定的持续学习研究不足。

Method: 提出基于提示的持续学习（PCL）方法，采用统一提示池和最小扩展策略，通过扩展和冻结部分提示减少计算开销，引入新的正则化项平衡保留和适应。

Result: 在三个糖尿病视网膜病变数据集上，模型比现有方法最终分类准确率至少提高10%，F1分数提高9分，同时降低推理成本。

Conclusion: 该研究有望推动可持续医疗AI进步，实现分布式医疗中的实时诊断、患者监测和远程医疗应用。

Abstract: Modern AI models achieve state-of-the-art performance with large-scale,
high-quality datasets; however, ethical, social, and institutional constraints
in the medical domain severely restrict data sharing, rendering centralized
learning nearly impossible. Each institution must incrementally update models
using only local data. Traditional training overfits new samples and suffers
from catastrophic forgetting, losing previously acquired knowledge. Medical
data distributions also shift due to varying diagnostic equipment and
demographics. Although continual learning (CL) has advanced, most methods
address natural images, leaving medical-domain-specific CL underexplored. We
propose a prompt-based continual learning (PCL) approach featuring a unified
prompt pool with a minimal expansion strategy: by expanding and freezing a
subset of prompts, our method reduces computational overhead, and a novel
regularization term balances retention and adaptation. Experiments on three
diabetic retinopathy datasets Aptos2019, LI2019, and Diabetic Retinopathy
Detection show our model improves final classification accuracy by at least 10%
and F1-score by 9 points over state-of-the-art approaches while lowering
inference cost. We anticipate this study will drive sustainable medical AI
advances, enabling real-time diagnosis, patient monitoring, and telemedicine
applications in distributed healthcare. Code will be released upon acceptance

</details>


### [34] [MDNS: Masked Diffusion Neural Sampler via Stochastic Optimal Control](https://arxiv.org/abs/2508.10684)
*Yuchen Zhu,Wei Guo,Jaemoo Choi,Guan-Horng Liu,Yongxin Chen,Molei Tao*

Main category: cs.LG

TL;DR: 提出MDNS框架学习离散神经采样器从离散状态空间生成样本，实验验证其高效可扩展性且性能远超基线。


<details>
  <summary>Details</summary>
Motivation: 解决状态空间基数大且分布为多模态时，从离散状态空间生成样本这一挑战性任务。

Method: 提出MDNS框架，通过一组学习目标对齐两个路径测度来训练离散神经采样器，理论基于连续时间马尔可夫链的随机最优控制。

Result: 在多种具有不同统计特性的分布上的实验表明，MDNS能准确从目标分布采样，即使问题维度极高，且大幅超越其他基于学习的基线。

Conclusion: MDNS框架具有高效性、可扩展性、有效性和潜力。

Abstract: We study the problem of learning a neural sampler to generate samples from
discrete state spaces where the target probability mass function
$\pi\propto\mathrm{e}^{-U}$ is known up to a normalizing constant, which is an
important task in fields such as statistical physics, machine learning,
combinatorial optimization, etc. To better address this challenging task when
the state space has a large cardinality and the distribution is multi-modal, we
propose $\textbf{M}$asked $\textbf{D}$iffusion $\textbf{N}$eural
$\textbf{S}$ampler ($\textbf{MDNS}$), a novel framework for training discrete
neural samplers by aligning two path measures through a family of learning
objectives, theoretically grounded in the stochastic optimal control of the
continuous-time Markov chains. We validate the efficiency and scalability of
MDNS through extensive experiments on various distributions with distinct
statistical properties, where MDNS learns to accurately sample from the target
distributions despite the extremely high problem dimensions and outperforms
other learning-based baselines by a large margin. A comprehensive study of
ablations and extensions is also provided to demonstrate the efficacy and
potential of the proposed framework.

</details>


### [35] [Retro-Expert: Collaborative Reasoning for Interpretable Retrosynthesis](https://arxiv.org/abs/2508.10967)
*Xinyi Li,Sai Wang,Yutian Lin,Yu Wu,Yi Yang*

Main category: cs.LG

TL;DR: 提出可解释的逆合成框架Retro-Expert，结合大语言模型和专业模型优势，实验表现佳且能提供专业解释。


<details>
  <summary>Details</summary>
Motivation: 现有逆合成预测模型依赖静态模式匹配，缺乏有效逻辑决策能力，存在黑盒决策问题。

Method: 通过强化学习结合大语言模型和专业模型的推理优势进行协作推理，包含专业模型浅层推理、大语言模型关键推理和强化学习优化可解释决策策略三个组件。

Result: Retro-Expert在不同指标上超越基于大语言模型和专业模型，能提供与专家一致的解释。

Conclusion: Retro-Expert能弥补AI预测和可操作化学见解之间的差距。

Abstract: Retrosynthesis prediction aims to infer the reactant molecule based on a
given product molecule, which is a fundamental task in chemical synthesis.
However, existing models rely on static pattern-matching paradigm, which limits
their ability to perform effective logic decision-making, leading to black-box
decision-making. Building on this, we propose Retro-Expert, an interpretable
retrosynthesis framework that performs collaborative reasoning by combining the
complementary reasoning strengths of Large Language Models and specialized
models via reinforcement learning. It outputs natural language explanations
grounded in chemical logic through three components: (1) specialized models
perform shallow reasoning to construct high-quality chemical decision space,
(2) LLM-driven critical reasoning to generate predictions and corresponding
interpretable reasoning path, and (3) reinforcement learning optimizing
interpretable decision policy. Experiments show that Retro-Expert not only
surpasses both LLM-based and specialized models across different metrics but
also provides expert-aligned explanations that bridge the gap between AI
predictions and actionable chemical insights.

</details>


### [36] [BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining](https://arxiv.org/abs/2508.10975)
*Pratyush Maini,Vineeth Dorna,Parth Doshi,Aldo Carranza,Fan Pan,Jack Urbanek,Paul Burstein,Alex Fang,Alvin Deng,Amro Abbas,Brett Larsen,Cody Blakeney,Charvi Bannur,Christina Baek,Darren Teh,David Schwab,Haakon Mongstad,Haoli Yin,Josh Wills,Kaleigh Mentzer,Luke Merrick,Ricardo Monti,Rishabh Adiga,Siddharth Joshi,Spandan Das,Zhengping Wang,Bogdan Gaza,Ari Morcos,Matthew Leavitt*

Main category: cs.LG

TL;DR: 本文介绍了合成数据预训练的问题，提出BeyondWeb框架，其在性能和训练速度上表现出色，并给出合成数据相关见解，强调生成高质量合成预训练数据需多因素联合优化。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型预训练中数据量扩展收益递减的问题，且当前对影响合成数据质量的因素了解不足。

Method: 引入BeyondWeb合成数据生成框架。

Result: BeyondWeb在14个基准评估中平均性能优于Cosmopedia和Nemotron - Synth，训练速度更快，3B模型在BeyondWeb上训练效果优于8B模型在Cosmopedia上的训练效果。

Conclusion: 生成高质量合成预训练数据没有单一方法，需要联合优化多因素。

Abstract: Recent advances in large language model (LLM) pretraining have shown that
simply scaling data quantity eventually leads to diminishing returns, hitting a
data wall. In response, the use of synthetic data for pretraining has emerged
as a promising paradigm for pushing the frontier of performance. Despite this,
the factors affecting synthetic data quality remain poorly understood. In this
work, we introduce BeyondWeb, a synthetic data generation framework that
produces high-quality synthetic data for pretraining. BeyondWeb significantly
extends the capabilities of traditional web-scale datasets, outperforming
state-of-the-art synthetic pretraining datasets such as Cosmopedia and
Nemotron-CC's high-quality synthetic subset (Nemotron-Synth) by up to 5.1
percentage points (pp) and 2.6pp, respectively, when averaged across a suite of
14 benchmark evaluations. It delivers up to 7.7x faster training than open web
data and 2.7x faster than Nemotron-Synth. Remarkably, a 3B model trained for
180B tokens on BeyondWeb outperforms an 8B model trained for the same token
budget on Cosmopedia. We also present several insights from BeyondWeb on
synthetic data for pretraining: what drives its benefits, which data to
rephrase and how, and the impact of model size and family on data quality.
Overall, our work shows that there's no silver bullet for generating
high-quality synthetic pretraining data. The best outcomes require jointly
optimizing many factors, a challenging task that requires rigorous science and
practical expertise. Naive approaches can yield modest improvements,
potentially at great cost, while well-executed methods can yield transformative
improvements, as exemplified by BeyondWeb.

</details>


### [37] [Match & Choose: Model Selection Framework for Fine-tuning Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.10993)
*Basile Lewandowski,Robert Birke,Lydia Y. Chen*

Main category: cs.LG

TL;DR: 提出模型选择框架M&C，助用户从模型平台选预训练T2I模型，评估显示其有较高预测准确率。


<details>
  <summary>Details</summary>
Motivation: 公共预训练T2I模型普及，用户面临选最佳微调模型挑战，且该问题在T2I模型中研究少。

Method: 提出M&C框架，核心是匹配图，含模型和数据集节点及边，构建基于特征和图嵌入特征预测最佳模型的模型。

Result: 在32个数据集选10个T2I模型评估，61.3%案例成功预测最佳微调模型，其余案例预测相近表现模型。

Conclusion: M&C框架能有效帮助用户选择预训练T2I模型，提升选择效率。

Abstract: Text-to-image (T2I) models based on diffusion and transformer architectures
advance rapidly. They are often pretrained on large corpora, and openly shared
on a model platform, such as HuggingFace. Users can then build up AI
applications, e.g., generating media contents, by adopting pretrained T2I
models and fine-tuning them on the target dataset. While public pretrained T2I
models facilitate the democratization of the models, users face a new
challenge: which model can be best fine-tuned based on the target data domain?
Model selection is well addressed in classification tasks, but little is known
in (pretrained) T2I models and their performance indication on the target
domain. In this paper, we propose the first model selection framework, M&C,
which enables users to efficiently choose a pretrained T2I model from a model
platform without exhaustively fine-tuning them all on the target dataset. The
core of M&C is a matching graph, which consists of: (i) nodes of available
models and profiled datasets, and (ii) edges of model-data and data-data pairs
capturing the fine-tuning performance and data similarity, respectively. We
then build a model that, based on the inputs of model/data feature, and,
critically, the graph embedding feature, extracted from the matching graph,
predicts the model achieving the best quality after fine-tuning for the target
domain. We evaluate M&C on choosing across ten T2I models for 32 datasets
against three baselines. Our results show that M&C successfully predicts the
best model for fine-tuning in 61.3% of the cases and a closely performing model
for the rest.

</details>


### [38] [CURE: Critical-Token-Guided Re-concatenation for Entropy-collapse Prevention](https://arxiv.org/abs/2508.11016)
*Qingbin Li,Rongkun Xue,Jie Wang,Ming Zhou,Zhi Li,Xiaofeng Ji,Yongqi Wang,Miao Liu,Zheming Yang,Minghui Qiu,Jing Yang*

Main category: cs.LG

TL;DR: 现有RLVR使大模型推理能力提升，但存在熵坍塌问题。论文提出CURE框架解决该问题，在数学推理任务上有5%性能提升。


<details>
  <summary>Details</summary>
Motivation: 先前RLVR管道在采样阶段使用静态初始状态采样，导致模型行为确定性过高、多样性低，出现熵坍塌，阻碍长期训练性能提升。

Method: 提出CURE两阶段框架，第一阶段在高熵关键标记处重新生成并联合优化轨迹；第二阶段用DAPO静态初始状态采样继续训练。

Result: 与其他RLVR方法相比，CURE在六个数学基准测试中性能提升5%，在熵和准确率上达到最优。

Conclusion: 一系列实验验证了CURE方法的有效性。

Abstract: Recent advances in Reinforcement Learning with Verified Reward (RLVR) have
driven the emergence of more sophisticated cognitive behaviors in large
language models (LLMs), thereby enhancing their reasoning capabilities.
However, in prior RLVR pipelines, the repeated use of static initial-state
sampling drawn exactly from the dataset distribution during each sampling phase
produced overly deterministic, low diversity model behavior, which manifested
as rapid entropy collapse and hindered sustained performance gains during
prolonged training. To address this issue, we introduce CURE
(Critical-token-gUided Re concatenation for Entropy-collapse prevention), a
two-stage framework that balances exploration and exploitation. Specifically,
in the first stage, to deliberately steer the model toward novel yet coherent
contexts, we re-generate at high-entropy critical tokens and jointly optimize
the original and the branched trajectories. The further comparison with vanilla
DAPO shows that the regeneration process achieves a better performance on math
reasoning tasks while sustaining a high-level entropy degree for exploration.
In the second stage, we continue training with static initial-state sampling by
DAPO, intentionally placing the model in a familiar state to gradually
strengthen exploitation. Extensive experiments on Qwen-2.5-Math-7B show that,
compared to other RLVR methods, CURE achieves a 5% performance gain across six
math benchmarks, establishing state-of-the-art performance in both entropy and
accuracy. A series of experiments further validate the effectiveness of our
approach. Code is available at https://github.com/CURE-Project/CURE.

</details>


### [39] [Conditional Independence Estimates for the Generalized Nonparanormal](https://arxiv.org/abs/2508.11050)
*Ujas Shah,Manuel Lladser,Rebecca Morrison*

Main category: cs.LG

TL;DR: 本文针对一类非高斯分布数据，展示从精度矩阵推断条件独立结构的方法并给出算法，经实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 一般非高斯分布中，协方差和精度矩阵无法像多元高斯分布那样编码变量的独立结构，需找到推断条件独立结构的方法。

Method: 基于先前工作，针对从高斯对角变换得到的非高斯分布（广义非正态），在数据满足一定条件下从精度矩阵推断条件独立结构，提出简单高效的算法。

Result: 所提出的算法通过合成实验和实际数据应用证明了有效性。

Conclusion: 对于广义非正态数据，可从精度矩阵推断条件独立结构，所提算法有效。

Abstract: For general non-Gaussian distributions, the covariance and precision matrices
do not encode the independence structure of the variables, as they do for the
multivariate Gaussian. This paper builds on previous work to show that for a
class of non-Gaussian distributions -- those derived from diagonal
transformations of a Gaussian -- information about the conditional independence
structure can still be inferred from the precision matrix, provided the data
meet certain criteria, analogous to the Gaussian case. We call such
transformations of the Gaussian as the generalized nonparanormal. The functions
that define these transformations are, in a broad sense, arbitrary. We also
provide a simple and computationally efficient algorithm that leverages this
theory to recover conditional independence structure from the generalized
nonparanormal data. The effectiveness of the proposed algorithm is demonstrated
via synthetic experiments and applications to real-world data.

</details>


### [40] [Relative Advantage Debiasing for Watch-Time Prediction in Short-Video Recommendation](https://arxiv.org/abs/2508.11086)
*Emily Liu,Kuan Han,Minfeng Zhan,Bocheng Zhao,Guanyu Mu,Yang Song*

Main category: cs.LG

TL;DR: 提出相对优势去偏框架校正观看时间，实验显示推荐准确性和鲁棒性有显著提升。


<details>
  <summary>Details</summary>
Motivation: 原始观看时间受混杂因素影响，会扭曲偏好信号和导致推荐模型有偏差。

Method: 提出相对优势去偏框架，通过与经验参考分布比较校正观看时间，采用两阶段架构分离分布估计和偏好学习，引入分布嵌入参数化观看时间分位数。

Result: 离线和在线实验表明，与现有基线方法相比，推荐准确性和鲁棒性有显著提高。

Conclusion: 所提方法能有效解决观看时间受混杂因素影响的问题，提高推荐性能。

Abstract: Watch time is widely used as a proxy for user satisfaction in video
recommendation platforms. However, raw watch times are influenced by
confounding factors such as video duration, popularity, and individual user
behaviors, potentially distorting preference signals and resulting in biased
recommendation models. We propose a novel relative advantage debiasing
framework that corrects watch time by comparing it to empirically derived
reference distributions conditioned on user and item groups. This approach
yields a quantile-based preference signal and introduces a two-stage
architecture that explicitly separates distribution estimation from preference
learning. Additionally, we present distributional embeddings to efficiently
parameterize watch-time quantiles without requiring online sampling or storage
of historical data. Both offline and online experiments demonstrate significant
improvements in recommendation accuracy and robustness compared to existing
baseline methods.

</details>


### [41] [Quantization vs Pruning: Insights from the Strong Lottery Ticket Hypothesis](https://arxiv.org/abs/2508.11020)
*Aakash Kumar,Emanuele Natale*

Main category: cs.LG

TL;DR: 本文基于数划分问题结果推导随机子集和问题在量化设置下的新理论结果，扩展强彩票假设框架到有限精度网络，证明初始网络必要过参数化的最优界限。


<details>
  <summary>Details</summary>
Motivation: 现有量化理论理解有限，之前强彩票假设结果主要针对连续设置，无法应用到量化设置，需拓展到量化场景。

Method: 基于Borgs等人在数划分问题上的基础结果，推导随机子集和问题在量化设置下的新理论结果，进而扩展强彩票假设框架。

Result: 在量化设置下，能精确表示目标离散神经网络的类似类别，证明了初始网络必要过参数化作为目标网络精度函数的最优界限。

Conclusion: 成功将强彩票假设框架扩展到有限精度网络，在量化理论方面取得进展。

Abstract: Quantization is an essential technique for making neural networks more
efficient, yet our theoretical understanding of it remains limited. Previous
works demonstrated that extremely low-precision networks, such as binary
networks, can be constructed by pruning large, randomly-initialized networks,
and showed that the ratio between the size of the original and the pruned
networks is at most polylogarithmic.
  The specific pruning method they employed inspired a line of theoretical work
known as the Strong Lottery Ticket Hypothesis (SLTH), which leverages insights
from the Random Subset Sum Problem. However, these results primarily address
the continuous setting and cannot be applied to extend SLTH results to the
quantized setting.
  In this work, we build on foundational results by Borgs et al. on the Number
Partitioning Problem to derive new theoretical results for the Random Subset
Sum Problem in a quantized setting.
  Using these results, we then extend the SLTH framework to finite-precision
networks. While prior work on SLTH showed that pruning allows approximation of
a certain class of neural networks, we demonstrate that, in the quantized
setting, the analogous class of target discrete neural networks can be
represented exactly, and we prove optimal bounds on the necessary
overparameterization of the initial network as a function of the precision of
the target network.

</details>


### [42] [Quantization through Piecewise-Affine Regularization: Optimization and Statistical Guarantees](https://arxiv.org/abs/2508.11112)
*Jianhao Ma,Lin Xiao*

Main category: cs.LG

TL;DR: 本文从优化和统计角度研究分段仿射正则化（PAR）在监督学习中的理论基础，包括临界点量化、近端映射求解及统计保证。


<details>
  <summary>Details</summary>
Motivation: 离散或量化变量优化问题因搜索空间的组合性质具有挑战性，PAR为基于连续优化的量化提供了灵活框架，需研究其在监督学习中的理论基础。

Method: 在过参数化情形下分析PAR正则化损失函数临界点，推导不同PAR的近端映射并使用近端梯度法等求解问题，研究PAR正则化线性回归问题的统计保证。

Result: 过参数化时PAR正则化损失函数的临界点有高度量化；能推导不同PAR的近端映射并使用多种方法求解问题；可通过PAR近似经典正则化并获得量化解的类似统计保证。

Conclusion: PAR在监督学习中有良好理论基础，可用于解决离散或量化变量优化问题。

Abstract: Optimization problems over discrete or quantized variables are very
challenging in general due to the combinatorial nature of their search space.
Piecewise-affine regularization (PAR) provides a flexible modeling and
computational framework for quantization based on continuous optimization. In
this work, we focus on the setting of supervised learning and investigate the
theoretical foundations of PAR from optimization and statistical perspectives.
First, we show that in the overparameterized regime, where the number of
parameters exceeds the number of samples, every critical point of the
PAR-regularized loss function exhibits a high degree of quantization. Second,
we derive closed-form proximal mappings for various (convex, quasi-convex, and
non-convex) PARs and show how to solve PAR-regularized problems using the
proximal gradient method, its accelerated variant, and the Alternating
Direction Method of Multipliers. Third, we study statistical guarantees of
PAR-regularized linear regression problems; specifically, we can approximate
classical formulations of $\ell_1$-, squared $\ell_2$-, and nonconvex
regularizations using PAR and obtain similar statistical guarantees with
quantized solutions.

</details>


### [43] [Hybrid-Hierarchical Fashion Graph Attention Network for Compatibility-Oriented and Personalized Outfit Recommendation](https://arxiv.org/abs/2508.11105)
*Sajjad Saed,Babak Teimourpour*

Main category: cs.LG

TL;DR: 时尚电商推荐挑战大，研究提出FGAT框架解决问题并取得好效果。


<details>
  <summary>Details</summary>
Motivation: 时尚行业扩张使电商平台找适配商品难，现有研究难兼顾搭配兼容性与个性化推荐。

Method: 受HFGN启发提出FGAT框架，构建三层分层图，结合视觉与文本特征，用图注意力机制。

Result: 在POG数据集上FGAT优于基线模型，各项指标有提升。

Conclusion: 结合多模态特征、分层图结构和注意力机制可提升个性化时尚推荐系统性能。

Abstract: The rapid expansion of the fashion industry and the growing variety of
products have made it challenging for users to find compatible items on
e-commerce platforms. Effective fashion recommendation systems are crucial for
filtering irrelevant items and suggesting suitable ones. However,
simultaneously addressing outfit compatibility and personalized recommendations
remains a significant challenge, as these aspects are often treated
independently in existing studies, often overlooking the complex interactions
between items and user preferences. This research introduces a new framework
named FGAT, inspired by the HFGN model, which leverages graph neural networks
and graph attention mechanisms to tackle this issue. The proposed framework
constructs a three-tier hierarchical graph of users, outfits, and items,
integrating visual and textual features to simultaneously model outfit
compatibility and user preferences. A graph attention mechanism dynamically
weights node importance during representation propagation, enabling the capture
of key interactions and generating precise representations for both user
preferences and outfit compatibility. Evaluated on the POG dataset, FGAT
outperforms baseline models such as HFGN, achieving improved results in
precision, HR, recall, NDCG, and accuracy.These results demonstrate that
combining multimodal visual-textual features with a hierarchical graph
structure and attention mechanisms significantly enhances the accuracy and
efficiency of personalized fashion recommendation systems.

</details>


### [44] [Zono-Conformal Prediction: Zonotope-Based Uncertainty Quantification for Regression and Classification Tasks](https://arxiv.org/abs/2508.11025)
*Laura Lützow,Michael Eichelbeck,Mykel J. Kochenderfer,Matthias Althoff*

Main category: cs.LG

TL;DR: 提出zono - conformal prediction方法，用于解决现有保形预测方法计算昂贵、数据密集等问题，在回归和分类任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 当前保形预测方法计算昂贵、数据密集，且用区间表示预测集难以捕捉多维输出依赖关系，需改进。

Method: 引入受区间预测模型和可达集一致识别启发的zono - conformal prediction方法，将 zonotopic 不确定集直接放入基础预测器模型，通过单一线性规划识别预测器。

Result: zono - conformal预测器比区间预测模型和标准保形预测方法保守性低，在测试数据上有相似覆盖率。

Conclusion: zono - conformal prediction方法有效解决了现有保形预测方法的局限性，在回归和分类任务中表现较好。

Abstract: Conformal prediction is a popular uncertainty quantification method that
augments a base predictor with prediction sets with statistically valid
coverage guarantees. However, current methods are often computationally
expensive and data-intensive, as they require constructing an uncertainty model
before calibration. Moreover, existing approaches typically represent the
prediction sets with intervals, which limits their ability to capture
dependencies in multi-dimensional outputs. We address these limitations by
introducing zono-conformal prediction, a novel approach inspired by interval
predictor models and reachset-conformant identification that constructs
prediction zonotopes with assured coverage. By placing zonotopic uncertainty
sets directly into the model of the base predictor, zono-conformal predictors
can be identified via a single, data-efficient linear program. While we can
apply zono-conformal prediction to arbitrary nonlinear base predictors, we
focus on feed-forward neural networks in this work. Aside from regression
tasks, we also construct optimal zono-conformal predictors in classification
settings where the output of an uncertain predictor is a set of possible
classes. We provide probabilistic coverage guarantees and present methods for
detecting outliers in the identification data. In extensive numerical
experiments, we show that zono-conformal predictors are less conservative than
interval predictor models and standard conformal prediction methods, while
achieving a similar coverage over the test data.

</details>


### [45] [Borrowing From the Future: Enhancing Early Risk Assessment through Contrastive Learning](https://arxiv.org/abs/2508.11210)
*Minghui Sun,Matthew M. Engelhard,Benjamin A. Goldstein*

Main category: cs.LG

TL;DR: 本文提出Borrowing From the Future (BFF)框架以改善儿科早期风险评估预测性能，在两个实际任务中验证有效，代码开源。


<details>
  <summary>Details</summary>
Motivation: 儿科风险评估分多阶段进行，后期预测精度高，但临床希望尽早进行可靠风险评估，故聚焦于提高早期风险评估的预测性能。

Method: 提出对比多模态框架BFF，将每个时间窗口视为不同模态，模型在所有可用数据上训练，利用最新信息进行风险评估，从后期阶段“借用”信息信号隐式监督早期阶段学习。

Result: 在两个真实世界儿科结果预测任务中验证了BFF，早期风险评估有持续改善。

Conclusion: BFF框架能有效提高儿科早期风险评估的预测性能。

Abstract: Risk assessments for a pediatric population are often conducted across
multiple stages. For example, clinicians may evaluate risks prenatally, at
birth, and during Well-Child visits. Although predictions made at later stages
typically achieve higher precision, it is clinically desirable to make reliable
risk assessments as early as possible. Therefore, this study focuses on
improving prediction performance in early-stage risk assessments. Our solution,
\textbf{Borrowing From the Future (BFF)}, is a contrastive multi-modal
framework that treats each time window as a distinct modality. In BFF, a model
is trained on all available data throughout the time while performing a risk
assessment using up-to-date information. This contrastive framework allows the
model to ``borrow'' informative signals from later stages (e.g., Well-Child
visits) to implicitly supervise the learning at earlier stages (e.g.,
prenatal/birth stages). We validate BFF on two real-world pediatric outcome
prediction tasks, demonstrating consistent improvements in early risk
assessments. The code is available at https://github.com/scotsun/bff.

</details>


### [46] [Learning with Confidence](https://arxiv.org/abs/2508.11037)
*Oliver Ethan Richardson*

Main category: cs.LG

TL;DR: 本文刻画了学习或更新信念时的置信概念，给出测量方法和表示形式，还将贝叶斯规则作为特例分析。


<details>
  <summary>Details</summary>
Motivation: 研究学习或更新信念时产生的置信概念，明确其与概率等概念的区别。

Method: 对带置信的学习进行形式化公理化，给出两种测量置信的方法并证明表示方式，在额外假设下推导更紧凑的表示。

Result: 得到置信的表示形式，诱导出复合“并行”观测的扩展语言，将贝叶斯规则作为特例。

Conclusion: 置信是与概率不同的概念，有多种表示形式，贝叶斯规则是特定情况下的特例。

Abstract: We characterize a notion of confidence that arises in learning or updating
beliefs: the amount of trust one has in incoming information and its impact on
the belief state. This learner's confidence can be used alongside (and is
easily mistaken for) probability or likelihood, but it is fundamentally a
different concept -- one that captures many familiar concepts in the
literature, including learning rates and number of training epochs, Shafer's
weight of evidence, and Kalman gain. We formally axiomatize what it means to
learn with confidence, give two canonical ways of measuring confidence on a
continuum, and prove that confidence can always be represented in this way.
Under additional assumptions, we derive more compact representations of
confidence-based learning in terms of vector fields and loss functions. These
representations induce an extended language of compound "parallel"
observations. We characterize Bayes Rule as the special case of an optimizing
learner whose loss representation is a linear expectation.

</details>


### [47] [Calibrated and uncertain? Evaluating uncertainty estimates in binary classification models](https://arxiv.org/abs/2508.11460)
*Aurora Grefsrud,Nello Blaser,Trygve Buanes*

Main category: cs.LG

TL;DR: 本文通过近似贝叶斯推理框架和合成分类数据集，研究六种概率机器学习算法的不确定性估计特性，结果显示算法校准良好，但深度学习算法对分布外数据的不确定性估计不足。


<details>
  <summary>Details</summary>
Motivation: 随着数据模型复杂度增加，不确定性量化困难，需要研究不同算法的不确定性估计特性。

Method: 使用近似贝叶斯推理统一框架，对合成分类数据集进行实证测试，研究六种概率机器学习算法。

Result: 所有算法校准良好，但基于深度学习的算法不能始终反映分布外数据缺乏实验证据的不确定性。

Conclusion: 本研究可为开发科学数据驱动建模的不确定性估计新方法的研究人员提供清晰示例。

Abstract: Rigorous statistical methods, including parameter estimation with
accompanying uncertainties, underpin the validity of scientific discovery,
especially in the natural sciences. With increasingly complex data models such
as deep learning techniques, uncertainty quantification has become exceedingly
difficult and a plethora of techniques have been proposed. In this case study,
we use the unifying framework of approximate Bayesian inference combined with
empirical tests on carefully created synthetic classification datasets to
investigate qualitative properties of six different probabilistic machine
learning algorithms for class probability and uncertainty estimation: (i) a
neural network ensemble, (ii) neural network ensemble with conflictual loss,
(iii) evidential deep learning, (iv) a single neural network with Monte Carlo
Dropout, (v) Gaussian process classification and (vi) a Dirichlet process
mixture model. We check if the algorithms produce uncertainty estimates which
reflect commonly desired properties, such as being well calibrated and
exhibiting an increase in uncertainty for out-of-distribution data points. Our
results indicate that all algorithms are well calibrated, but none of the deep
learning based algorithms provide uncertainties that consistently reflect lack
of experimental evidence for out-of-distribution data points. We hope our study
may serve as a clarifying example for researchers developing new methods of
uncertainty estimation for scientific data-driven modeling.

</details>


### [48] [SHLIME: Foiling adversarial attacks fooling SHAP and LIME](https://arxiv.org/abs/2508.11053)
*Sam Chauhan,Estelle Duguet,Karthik Ramakrishnan,Hugh Van Deventer,Jack Kruger,Ranjan Subbaraman*

Main category: cs.LG

TL;DR: 研究LIME和SHAP后验解释方法对有偏模型的敏感性，评估提升鲁棒性策略，找到能提升偏差检测的配置。


<details>
  <summary>Details</summary>
Motivation: 后验解释方法如LIME和SHAP易受对抗操纵，可能隐藏有害偏差，需提升其对有偏模型的鲁棒性。

Method: 先复制COMPAS实验验证先前结果并建立基线，引入模块化测试框架对不同性能分类器的增强和集成解释方法进行系统评估，评估多种LIME/SHAP集成配置。

Result: 找到能大幅提高偏差检测的配置。

Conclusion: 这些配置有潜力提升高风险机器学习系统部署的透明度。

Abstract: Post hoc explanation methods, such as LIME and SHAP, provide interpretable
insights into black-box classifiers and are increasingly used to assess model
biases and generalizability. However, these methods are vulnerable to
adversarial manipulation, potentially concealing harmful biases. Building on
the work of Slack et al. (2020), we investigate the susceptibility of LIME and
SHAP to biased models and evaluate strategies for improving robustness. We
first replicate the original COMPAS experiment to validate prior findings and
establish a baseline. We then introduce a modular testing framework enabling
systematic evaluation of augmented and ensemble explanation approaches across
classifiers of varying performance. Using this framework, we assess multiple
LIME/SHAP ensemble configurations on out-of-distribution models, comparing
their resistance to bias concealment against the original methods. Our results
identify configurations that substantially improve bias detection, highlighting
their potential for enhancing transparency in the deployment of high-stakes
machine learning systems.

</details>


### [49] [Abundance-Aware Set Transformer for Microbiome Sample Embedding](https://arxiv.org/abs/2508.11075)
*Hyunwoo Yoo,Gail Rosen*

Main category: cs.LG

TL;DR: 提出丰度感知的Set Transformer变体用于微生物组样本表示，在分类任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 以往微生物组样本嵌入表示方法多忽略分类群丰度的生物学重要性，需要改进。

Method: 提出丰度感知的Set Transformer变体，根据相对丰度对序列嵌入加权，复制与丰度成比例的嵌入向量并应用基于自注意力的聚合。

Result: 在真实世界微生物组分类任务中优于平均池化和未加权的Set Transformer，部分情况实现完美性能。

Conclusion: 丰度感知的聚合对强大且有生物学依据的微生物组表示有用，是将序列级丰度集成到基于Transformer的样本嵌入的首批方法之一。

Abstract: Microbiome sample representation to input into LLMs is essential for
downstream tasks such as phenotype prediction and environmental classification.
While prior studies have explored embedding-based representations of each
microbiome sample, most rely on simple averaging over sequence embeddings,
often overlooking the biological importance of taxa abundance. In this work, we
propose an abundance-aware variant of the Set Transformer to construct
fixed-size sample-level embeddings by weighting sequence embeddings according
to their relative abundance. Without modifying the model architecture, we
replicate embedding vectors proportional to their abundance and apply
self-attention-based aggregation. Our method outperforms average pooling and
unweighted Set Transformers on real-world microbiome classification tasks,
achieving perfect performance in some cases. These results demonstrate the
utility of abundance-aware aggregation for robust and biologically informed
microbiome representation. To the best of our knowledge, this is one of the
first approaches to integrate sequence-level abundance into Transformer-based
sample embeddings.

</details>


### [50] [A Feasibility Experiment on the Application of Predictive Coding to Instant Messaging Corpora](https://arxiv.org/abs/2508.11084)
*Thanasis Schoinas,Ghulam Qadir*

Main category: cs.LG

TL;DR: 本文针对即时消息数据集的预测编码问题，提出数据管理工作流分组、特征选择和逻辑回归分类器方案，并通过降维提升性能，在数据集上测试并给出成本节约示例。


<details>
  <summary>Details</summary>
Motivation: 即时消息数据集因非正式性和规模小，给法律行业的预测编码（机器学习文档分类）带来额外挑战。

Method: 利用数据管理工作流将消息分组为日聊天记录，进行特征选择，使用逻辑回归分类器，通过降维提升基线模型性能。

Result: 在即时彭博数据集上测试了该方法，并给出了成本节约的示例。

Conclusion: 提出的方法为即时消息数据集的预测编码提供了经济可行的解决方案。

Abstract: Predictive coding, the term used in the legal industry for document
classification using machine learning, presents additional challenges when the
dataset comprises instant messages, due to their informal nature and smaller
sizes. In this paper, we exploit a data management workflow to group messages
into day chats, followed by feature selection and a logistic regression
classifier to provide an economically feasible predictive coding solution. We
also improve the solution's baseline model performance by dimensionality
reduction, with focus on quantitative features. We test our methodology on an
Instant Bloomberg dataset, rich in quantitative information. In parallel, we
provide an example of the cost savings of our approach.

</details>


### [51] [Predictive Multimodal Modeling of Diagnoses and Treatments in EHR](https://arxiv.org/abs/2508.11092)
*Cindy Shih-Ting Huang,Clarence Boon Liang Ng,Marek Rei*

Main category: cs.LG

TL;DR: 提出多模态系统和加权时间损失策略用于ICD代码早期预测，实验表明优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注出院后文档分类，早期预测信息可用于识别健康风险、建议有效治疗或优化资源分配，需解决患者入院初期信息有限的预测建模挑战。

Method: 提出多模态系统融合电子健康记录中的临床笔记和表格事件，集成预训练编码器、特征池化和跨模态注意力，还提出加权时间损失调整各时间点贡献。

Result: 这些策略增强了早期预测模型，优于当前最先进系统。

Conclusion: 所提多模态系统和加权时间损失策略在ICD代码早期预测上有效。

Abstract: While the ICD code assignment problem has been widely studied, most works
have focused on post-discharge document classification. Models for early
forecasting of this information could be used for identifying health risks,
suggesting effective treatments, or optimizing resource allocation. To address
the challenge of predictive modeling using the limited information at the
beginning of a patient stay, we propose a multimodal system to fuse clinical
notes and tabular events captured in electronic health records. The model
integrates pre-trained encoders, feature pooling, and cross-modal attention to
learn optimal representations across modalities and balance their presence at
every temporal point. Moreover, we present a weighted temporal loss that
adjusts its contribution at each point in time. Experiments show that these
strategies enhance the early prediction model, outperforming the current
state-of-the-art systems.

</details>


### [52] [CTRL Your Shift: Clustered Transfer Residual Learning for Many Small Datasets](https://arxiv.org/abs/2508.11144)
*Gauri Jain,Dominik Rothenhäusler,Kirk Bansak,Elisabeth Paulson*

Main category: cs.LG

TL;DR: 本文介绍元学习方法CTRL，结合跨域残差学习和自适应池化/聚类优势，在多数据集上表现优于基准。


<details>
  <summary>Details</summary>
Motivation: 机器学习任务使用多源数据时，需预测兼顾整体准确性和源间差异，但数据存在多源、分布偏移和样本量差异等挑战。

Method: 提出Clustered Transfer Residual Learning (CTRL) 方法，结合跨域残差学习和自适应池化/聚类。

Result: 在5个大规模数据集上，CTRL在多个关键指标上始终优于基准，使用不同基础学习器时也是如此。

Conclusion: CTRL方法能同时提高整体准确性并保留源级异质性，有效应对多源数据挑战。

Abstract: Machine learning (ML) tasks often utilize large-scale data that is drawn from
several distinct sources, such as different locations, treatment arms, or
groups. In such settings, practitioners often desire predictions that not only
exhibit good overall accuracy, but also remain reliable within each source and
preserve the differences that matter across sources. For instance, several
asylum and refugee resettlement programs now use ML-based employment
predictions to guide where newly arriving families are placed within a host
country, which requires generating informative and differentiated predictions
for many and often small source locations. However, this task is made
challenging by several common characteristics of the data in these settings:
the presence of numerous distinct data sources, distributional shifts between
them, and substantial variation in sample sizes across sources. This paper
introduces Clustered Transfer Residual Learning (CTRL), a meta-learning method
that combines the strengths of cross-domain residual learning and adaptive
pooling/clustering in order to simultaneously improve overall accuracy and
preserve source-level heterogeneity. We provide theoretical results that
clarify how our objective navigates the trade-off between data quantity and
data quality. We evaluate CTRL alongside other state-of-the-art benchmarks on 5
large-scale datasets. This includes a dataset from the national asylum program
in Switzerland, where the algorithmic geographic assignment of asylum seekers
is currently being piloted. CTRL consistently outperforms the benchmarks across
several key metrics and when using a range of different base learners.

</details>


### [53] [Towards the Next-generation Bayesian Network Classifiers](https://arxiv.org/abs/2508.11145)
*Huan Zhang,Daokun Zhang,Kexin Meng,Geoffrey I. Webb*

Main category: cs.LG

TL;DR: 本文提出新范式设计高阶贝叶斯网络分类器，以NeuralKDB为例，在60个UCI数据集实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯网络分类器因参数爆炸和数据稀疏问题，只能进行低阶特征依赖建模，难以推断复杂数据发生概率。

Method: 学习特征值的分布式表示，将K - dependence Bayesian classifier扩展为NeuralKDB，设计新神经网络架构学习特征值表示并参数化条件概率，用随机梯度下降算法训练模型。

Result: 在60个UCI数据集的分类实验中，NeuralKDB在捕捉高阶特征依赖方面表现出色，显著优于传统贝叶斯网络分类器和其他竞争分类器。

Conclusion: 所提出的设计高阶贝叶斯网络分类器的新范式有效，NeuralKDB性能良好。

Abstract: Bayesian network classifiers provide a feasible solution to tabular data
classification, with a number of merits like high time and memory efficiency,
and great explainability. However, due to the parameter explosion and data
sparsity issues, Bayesian network classifiers are restricted to low-order
feature dependency modeling, making them struggle in extrapolating the
occurrence probabilities of complex real-world data. In this paper, we propose
a novel paradigm to design high-order Bayesian network classifiers, by learning
distributional representations for feature values, as what has been done in
word embedding and graph representation learning. The learned distributional
representations are encoded with the semantic relatedness between different
features through their observed co-occurrence patterns in training data, which
then serve as a hallmark to extrapolate the occurrence probabilities of new
test samples. As a classifier design realization, we remake the K-dependence
Bayesian classifier (KDB) by extending it into a neural version, i.e.,
NeuralKDB, where a novel neural network architecture is designed to learn
distributional representations of feature values and parameterize the
conditional probabilities between interdependent features. A stochastic
gradient descent based algorithm is designed to train the NeuralKDB model
efficiently. Extensive classification experiments on 60 UCI datasets
demonstrate that the proposed NeuralKDB classifier excels in capturing
high-order feature dependencies and significantly outperforms the conventional
Bayesian network classifiers, as well as other competitive classifiers,
including two neural network based classifiers without distributional
representation learning.

</details>


### [54] [Mitigating Modality Quantity and Quality Imbalance in Multimodal Online Federated Learning](https://arxiv.org/abs/2508.11159)
*Heqiang Wang,Weihong Yang,Xiaoxiong Zhong,Jia Zhou,Fangming Liu,Weizhe Zhang*

Main category: cs.LG

TL;DR: 论文研究物联网多模态在线联邦学习（MMO - FL）中模态数量和质量不平衡（QQI）问题，提出QQR算法，实验表明该算法在模态不平衡条件下表现优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 物联网生态产生大量多模态数据，边缘智能发展使分布式学习和在线学习框架成为必要，MMO - FL面临QQI问题影响学习性能。

Method: 系统研究MMO - FL框架内QQI的影响并进行理论分析，提出基于原型学习的QQR算法与训练过程并行运行。

Result: 在两个真实世界多模态数据集上的广泛实验显示，QQR算法在模态不平衡条件下始终优于基准方法，学习性能良好。

Conclusion: QQR算法能有效解决MMO - FL中的QQI问题，提高学习性能。

Abstract: The Internet of Things (IoT) ecosystem produces massive volumes of multimodal
data from diverse sources, including sensors, cameras, and microphones. With
advances in edge intelligence, IoT devices have evolved from simple data
acquisition units into computationally capable nodes, enabling localized
processing of heterogeneous multimodal data. This evolution necessitates
distributed learning paradigms that can efficiently handle such data.
Furthermore, the continuous nature of data generation and the limited storage
capacity of edge devices demand an online learning framework. Multimodal Online
Federated Learning (MMO-FL) has emerged as a promising approach to meet these
requirements. However, MMO-FL faces new challenges due to the inherent
instability of IoT devices, which often results in modality quantity and
quality imbalance (QQI) during data collection. In this work, we systematically
investigate the impact of QQI within the MMO-FL framework and present a
comprehensive theoretical analysis quantifying how both types of imbalance
degrade learning performance. To address these challenges, we propose the
Modality Quantity and Quality Rebalanced (QQR) algorithm, a prototype learning
based method designed to operate in parallel with the training process.
Extensive experiments on two real-world multimodal datasets show that the
proposed QQR algorithm consistently outperforms benchmarks under modality
imbalance conditions with promising learning performance.

</details>


### [55] [A Semi-supervised Generative Model for Incomplete Multi-view Data Integration with Missing Labels](https://arxiv.org/abs/2508.11180)
*Yiyang Shen,Weiran Wang*

Main category: cs.LG

TL;DR: 提出半监督生成模型处理多视图学习中视图缺失和标签缺失问题，在多数据集上性能更优


<details>
  <summary>Details</summary>
Motivation: 现有概率方法基于信息瓶颈原则虽能处理视图缺失问题，但信息瓶颈框架是全监督的，无法利用无标签数据

Method: 提出半监督生成模型，在统一框架中利用有标签和无标签样本，最大化无标签样本似然以学习与信息瓶颈共享的潜在空间，并在潜在空间进行跨视图互信息最大化

Result: 相比现有方法，在图像和多组学数据上，对视图缺失和标签有限的情况，模型有更好的预测和插补性能

Conclusion: 所提出的半监督生成模型能有效处理多视图学习中的视图缺失和标签缺失问题

Abstract: Multi-view learning is widely applied to real-life datasets, such as multiple
omics biological data, but it often suffers from both missing views and missing
labels. Prior probabilistic approaches addressed the missing view problem by
using a product-of-experts scheme to aggregate representations from present
views and achieved superior performance over deterministic classifiers, using
the information bottleneck (IB) principle. However, the IB framework is
inherently fully supervised and cannot leverage unlabeled data. In this work,
we propose a semi-supervised generative model that utilizes both labeled and
unlabeled samples in a unified framework. Our method maximizes the likelihood
of unlabeled samples to learn a latent space shared with the IB on labeled
data. We also perform cross-view mutual information maximization in the latent
space to enhance the extraction of shared information across views. Compared to
existing approaches, our model achieves better predictive and imputation
performance on both image and multi-omics data with missing views and limited
labeled samples.

</details>


### [56] [Quantum-Boosted High-Fidelity Deep Learning](https://arxiv.org/abs/2508.11190)
*Feng-ao Wang,Shaobo Chen,Yao Xuan,Junwei Liu,Qi Gao,Hongdong Zhu,Junjie Hou,Lixin Yuan,Jinyu Cheng,Chenxin Yi,Hai Wei,Yin Ma,Tao Xu,Kai Wen,Yixue Li*

Main category: cs.LG

TL;DR: 本文提出量子玻尔兹曼机 - 变分自编码器（QBM - VAE），解决概率深度学习依赖高斯先验的局限，在单细胞数据集任务上表现优于传统模型，展示了量子优势并提供混合量子AI模型蓝图。


<details>
  <summary>Details</summary>
Motivation: 概率深度学习主要依赖高斯先验，难以捕捉自然数据的复杂非高斯特征，影响科学发现的模型保真度，而玻尔兹曼分布虽更具表达力但经典计算难以处理，量子方法受限于比特规模和稳定性。

Method: 引入QBM - VAE这一混合量子 - 经典架构，利用量子处理器从玻尔兹曼分布高效采样并作为深度生成模型的先验。

Result: 应用于百万级单细胞数据集，QBM - VAE生成的潜在空间能更好保留复杂生物结构，在组学数据集成、细胞类型分类和轨迹推断等任务上优于传统高斯深度学习模型。

Conclusion: 该工作展示了大规模科学问题中深度学习的实际量子优势，为开发混合量子AI模型提供可迁移蓝图。

Abstract: A fundamental limitation of probabilistic deep learning is its predominant
reliance on Gaussian priors. This simplistic assumption prevents models from
accurately capturing the complex, non-Gaussian landscapes of natural data,
particularly in demanding domains like complex biological data, severely
hindering the fidelity of the model for scientific discovery. The
physically-grounded Boltzmann distribution offers a more expressive
alternative, but it is computationally intractable on classical computers. To
date, quantum approaches have been hampered by the insufficient qubit scale and
operational stability required for the iterative demands of deep learning.
Here, we bridge this gap by introducing the Quantum Boltzmann
Machine-Variational Autoencoder (QBM-VAE), a large-scale and long-time stable
hybrid quantum-classical architecture. Our framework leverages a quantum
processor for efficient sampling from the Boltzmann distribution, enabling its
use as a powerful prior within a deep generative model. Applied to
million-scale single-cell datasets from multiple sources, the QBM-VAE generates
a latent space that better preserves complex biological structures,
consistently outperforming conventional Gaussian-based deep learning models
like VAE and SCVI in essential tasks such as omics data integration, cell-type
classification, and trajectory inference. It also provides a typical example of
introducing a physics priori into deep learning to drive the model to acquire
scientific discovery capabilities that breaks through data limitations. This
work provides the demonstration of a practical quantum advantage in deep
learning on a large-scale scientific problem and offers a transferable
blueprint for developing hybrid quantum AI models.

</details>


### [57] [Meta-learning Structure-Preserving Dynamics](https://arxiv.org/abs/2508.11205)
*Cheng Jing,Uvini Balasuriya Mudiyanselage,Woojin Cho,Minju Jo,Anthony Gruber,Kookjin Lee*

Main category: cs.LG

TL;DR: 提出基于调制的元学习框架用于结构保留动力学建模，避免灰盒知识和显式优化，实验证明在少样本学习中准确且不影响物理约束。


<details>
  <summary>Details</summary>
Motivation: 现有结构保留动力学建模方法需固定系统配置，元学习方法有训练不稳定或泛化能力有限的问题。

Method: 引入基于调制的元学习框架，将结构保留模型基于潜在未知系统参数的紧凑潜在表示进行条件化。

Result: 在标准基准问题实验中，该方法在少样本学习设置下实现准确预测。

Conclusion: 该方法在不损害动力学稳定性和跨参数空间泛化性能所需物理约束的情况下有效。

Abstract: Structure-preserving approaches to dynamics modeling have demonstrated great
potential for modeling physical systems due to their strong inductive biases
that enforce conservation laws and dissipative behavior. However, the resulting
models are typically trained for fixed system configurations, requiring
explicit knowledge of system parameters as well as costly retraining for each
new set of parameters -- a major limitation in many-query or parameter-varying
scenarios. Meta-learning offers a potential solution, but existing approaches
like optimization-based meta-learning often suffer from training instability or
limited generalization capability. Inspired by ideas from computer vision, we
introduce a modulation-based meta-learning framework that directly conditions
structure-preserving models on compact latent representations of potentially
unknown system parameters, avoiding the need for gray-box system knowledge and
explicit optimization during adaptation. Through the application of novel
modulation strategies to parametric energy-conserving and dissipative systems,
we enable scalable and generalizable learning across parametric families of
dynamical systems. Experiments on standard benchmark problems demonstrate that
our approach achieves accurate predictions in few-shot learning settings,
without compromising on the essential physical constraints necessary for
dynamical stability and effective generalization performance across parameter
space.

</details>


### [58] [How Causal Abstraction Underpins Computational Explanation](https://arxiv.org/abs/2508.11214)
*Atticus Geiger,Jacqueline Harding,Thomas Icard*

Main category: cs.LG

TL;DR: 论文探讨系统实现计算的条件，用因果抽象理论给出计算实现的解释，认为应结合泛化和预测探讨相关问题。


<details>
  <summary>Details</summary>
Motivation: 探究系统在合适表征载体上实现给定计算所需的条件。

Method: 借助因果语言和因果抽象理论，结合深度学习中人工神经网络的讨论。

Result: 给出基于因果抽象的计算实现解释，并研究了表征在其中的作用。

Conclusion: 相关问题结合泛化和预测来探讨最有成效。

Abstract: Explanations of cognitive behavior often appeal to computations over
representations. What does it take for a system to implement a given
computation over suitable representational vehicles within that system? We
argue that the language of causality -- and specifically the theory of causal
abstraction -- provides a fruitful lens on this topic. Drawing on current
discussions in deep learning with artificial neural networks, we illustrate how
classical themes in the philosophy of computation and cognition resurface in
contemporary machine learning. We offer an account of computational
implementation grounded in causal abstraction, and examine the role for
representation in the resulting picture. We argue that these issues are most
profitably explored in connection with generalization and prediction.

</details>


### [59] [Air Quality PM2.5 Index Prediction Model Based on CNN-LSTM](https://arxiv.org/abs/2508.11215)
*Zicheng Guo,Shuqi Wu,Meixing Zhu,He Guandi*

Main category: cs.LG

TL;DR: 提出基于CNN - LSTM混合架构的PM2.5指数预测模型，用北京工业区数据实验，结果优于传统模型，但需高计算资源，后续将提升扩展性。


<details>
  <summary>Details</summary>
Motivation: 全球气候变化加剧，准确预测空气质量指标（尤其是PM2.5浓度）在环保、公共卫生和城市管理等领域愈发重要。

Method: 提出基于CNN - LSTM混合架构的空气质量PM2.5指数预测模型，结合CNN提取局部空间特征，LSTM建模时间序列数据的时间依赖关系。

Result: 模型均方根误差（RMSE）为5.236，在准确性和泛化性上优于传统时间序列模型。

Conclusion: 模型在空气污染预警系统等实际应用中有强大潜力，但因多变量输入复杂，需高计算资源，处理多样大气因素能力待优化，后续将提升扩展性以支持更复杂的多变量天气预测任务。

Abstract: With the intensification of global climate change, accurate prediction of air
quality indicators, especially PM2.5 concentration, has become increasingly
important in fields such as environmental protection, public health, and urban
management. To address this, we propose an air quality PM2.5 index prediction
model based on a hybrid CNN-LSTM architecture. The model effectively combines
Convolutional Neural Networks (CNN) for local spatial feature extraction and
Long Short-Term Memory (LSTM) networks for modeling temporal dependencies in
time series data. Using a multivariate dataset collected from an industrial
area in Beijing between 2010 and 2015 -- which includes hourly records of PM2.5
concentration, temperature, dew point, pressure, wind direction, wind speed,
and precipitation -- the model predicts the average PM2.5 concentration over
6-hour intervals. Experimental results show that the model achieves a root mean
square error (RMSE) of 5.236, outperforming traditional time series models in
both accuracy and generalization. This demonstrates its strong potential in
real-world applications such as air pollution early warning systems. However,
due to the complexity of multivariate inputs, the model demands high
computational resources, and its ability to handle diverse atmospheric factors
still requires optimization. Future work will focus on enhancing scalability
and expanding support for more complex multivariate weather prediction tasks.

</details>


### [60] [Enhancing Interactive Voting-Based Map Matching: Improving Efficiency and Robustness for Heterogeneous GPS Trajectories](https://arxiv.org/abs/2508.11235)
*William Alemanni,Arianna Burzacchi,Davide Colombi,Elena Giarratano*

Main category: cs.LG

TL;DR: 本文提出增强版交互式投票地图匹配算法，处理不同采样率轨迹，提升适用性。


<details>
  <summary>Details</summary>
Motivation: 高精度重建GPS轨迹，且不受输入数据质量影响，扩展原算法适用性。

Method: 在原算法基础上集成轨迹插补，采用距离受限交互式投票策略，处理道路网络缺失数据，引入OpenStreetMap定制资产。

Result: 保留原算法核心优势，显著扩展了在不同现实场景中的适用性。

Conclusion: 改进后的算法能高效处理不同采样率轨迹，可应用于OpenStreetMap覆盖的任意地理区域。

Abstract: This paper presents an enhanced version of the Interactive Voting-Based Map
Matching algorithm, designed to efficiently process trajectories with varying
sampling rates. The main aim is to reconstruct GPS trajectories with high
accuracy, independent of input data quality. Building upon the original
algorithm, developed exclusively for aligning GPS signals to road networks, we
extend its capabilities by integrating trajectory imputation. Our improvements
also include the implementation of a distance-bounded interactive voting
strategy to reduce computational complexity, as well as modifications to
address missing data in the road network. Furthermore, we incorporate a
custom-built asset derived from OpenStreetMap, enabling this approach to be
smoothly applied in any geographic region covered by OpenStreetMap's road
network. These advancements preserve the core strengths of the original
algorithm while significantly extending its applicability to diverse real-world
scenarios.

</details>


### [61] [Graph Neural Diffusion via Generalized Opinion Dynamics](https://arxiv.org/abs/2508.11249)
*Asela Hevapathige,Asiri Wijesinghe,Ahad N. Zehmakan*

Main category: cs.LG

TL;DR: 本文提出GODNF框架解决现有基于扩散的GNN方法的局限，经理论分析和实验验证其优势。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的GNN方法存在依赖静态同构扩散、深度受限、收敛行为理论理解不足等问题。

Method: 提出GODNF框架，将多种观点动力学模型统一为可训练的扩散机制，通过节点特定行为建模和动态邻域影响捕捉异质扩散模式和时间动态。

Result: 理论分析表明GODNF能模拟不同收敛配置，大量实验证明其在节点分类和影响估计任务上优于现有GNN。

Conclusion: GODNF能有效解决现有基于扩散的GNN方法的局限，具有良好性能。

Abstract: There has been a growing interest in developing diffusion-based Graph Neural
Networks (GNNs), building on the connections between message passing mechanisms
in GNNs and physical diffusion processes. However, existing methods suffer from
three critical limitations: (1) they rely on homogeneous diffusion with static
dynamics, limiting adaptability to diverse graph structures; (2) their depth is
constrained by computational overhead and diminishing interpretability; and (3)
theoretical understanding of their convergence behavior remains limited. To
address these challenges, we propose GODNF, a Generalized Opinion Dynamics
Neural Framework, which unifies multiple opinion dynamics models into a
principled, trainable diffusion mechanism. Our framework captures heterogeneous
diffusion patterns and temporal dynamics via node-specific behavior modeling
and dynamic neighborhood influence, while ensuring efficient and interpretable
message propagation even at deep layers. We provide a rigorous theoretical
analysis demonstrating GODNF's ability to model diverse convergence
configurations. Extensive empirical evaluations of node classification and
influence estimation tasks confirm GODNF's superiority over state-of-the-art
GNNs.

</details>


### [62] [Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed LLMs via Post-Processing](https://arxiv.org/abs/2508.11258)
*Ruicheng Xian,Yuxuan Wan,Han Zhao*

Main category: cs.LG

TL;DR: 本文提出一种通过提示从闭权大语言模型推导公平分类器的框架，实验表明该框架在多个数据集上有良好的准确率 - 公平性权衡。


<details>
  <summary>Details</summary>
Motivation: 现有在基于大语言模型的分类器上执行群体公平性的方法不适用于闭权大语言模型，而闭权大语言模型包含一些最强大的商业模型，需要新方法。

Method: 将大语言模型视为特征提取器，通过为指定公平标准设计的提示从其概率预测中提取特征，然后使用公平算法对这些特征进行事后训练，得到轻量级公平分类器。

Result: 在五个数据集上的实验表明，该框架在开权和闭权大语言模型上推导的分类器有强准确率 - 公平性权衡，数据效率高，优于基于大语言模型嵌入训练的公平分类器或从头在原始表格特征上训练的分类器。

Conclusion: 提出的框架能有效从闭权大语言模型推导公平分类器，在准确率和公平性上表现良好。

Abstract: Instruction fine-tuned large language models (LLMs) enable a simple zero-shot
or few-shot prompting paradigm, also known as in-context learning, for building
prediction models. This convenience, combined with continued advances in LLM
capability, has the potential to drive their adoption across a broad range of
domains, including high-stakes applications where group fairness -- preventing
disparate impacts across demographic groups -- is essential. The majority of
existing approaches to enforcing group fairness on LLM-based classifiers rely
on traditional fair algorithms applied via model fine-tuning or head-tuning on
final-layer embeddings, but they are no longer applicable to closed-weight LLMs
under the in-context learning setting, which include some of the most capable
commercial models today, such as GPT-4, Gemini, and Claude. In this paper, we
propose a framework for deriving fair classifiers from closed-weight LLMs via
prompting: the LLM is treated as a feature extractor, and features are elicited
from its probabilistic predictions (e.g., token log probabilities) using
prompts strategically designed for the specified fairness criterion to obtain
sufficient statistics for fair classification; a fair algorithm is then applied
to these features to train a lightweight fair classifier in a post-hoc manner.
Experiments on five datasets, including three tabular ones, demonstrate strong
accuracy-fairness tradeoffs for the classifiers derived by our framework from
both open-weight and closed-weight LLMs; in particular, our framework is
data-efficient and outperforms fair classifiers trained on LLM embeddings
(i.e., head-tuning) or from scratch on raw tabular features.

</details>


### [63] [Boosting the Robustness-Accuracy Trade-off of SNNs by Robust Temporal Self-Ensemble](https://arxiv.org/abs/2508.11279)
*Jihang Wang,Dongcheng Zhao,Ruolin Chen,Qian Zhang,Yi Zeng*

Main category: cs.LG

TL;DR: 研究通过时间集成视角重新审视SNNs对抗鲁棒性，提出RTE框架，实验表明其在鲁棒性和准确性权衡上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: SNNs在节能和类脑计算有前景，但对抗扰动的脆弱性研究不足。

Method: 提出Robust Temporal self-Ensemble (RTE)训练框架，将两个目标集成到统一损失中，采用随机采样策略优化。

Result: RTE在多个基准测试中始终在鲁棒性 - 准确性权衡上优于现有训练方法，重塑SNNs内部鲁棒性格局。

Conclusion: 强调了时间结构在对抗学习中的重要性，为构建鲁棒脉冲模型提供了理论基础。

Abstract: Spiking Neural Networks (SNNs) offer a promising direction for
energy-efficient and brain-inspired computing, yet their vulnerability to
adversarial perturbations remains poorly understood. In this work, we revisit
the adversarial robustness of SNNs through the lens of temporal ensembling,
treating the network as a collection of evolving sub-networks across discrete
timesteps. This formulation uncovers two critical but underexplored
challenges-the fragility of individual temporal sub-networks and the tendency
for adversarial vulnerabilities to transfer across time. To overcome these
limitations, we propose Robust Temporal self-Ensemble (RTE), a training
framework that improves the robustness of each sub-network while reducing the
temporal transferability of adversarial perturbations. RTE integrates both
objectives into a unified loss and employs a stochastic sampling strategy for
efficient optimization. Extensive experiments across multiple benchmarks
demonstrate that RTE consistently outperforms existing training methods in
robust-accuracy trade-off. Additional analyses reveal that RTE reshapes the
internal robustness landscape of SNNs, leading to more resilient and temporally
diversified decision boundaries. Our study highlights the importance of
temporal structure in adversarial learning and offers a principled foundation
for building robust spiking models.

</details>


### [64] [Generalize across Homophily and Heterophily: Hybrid Spectral Graph Pre-Training and Prompt Tuning](https://arxiv.org/abs/2508.11328)
*Haitong Luo,Suhang Wang,Weiyao Zhang,Ruiqi Meng,Xuying Meng,Yujun Zhang*

Main category: cs.LG

TL;DR: 现有图‘预训练和提示微调’方法依赖同质性低频知识，难以处理不同同质性的图。本文提出HS - GPPT模型确保光谱对齐，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有图‘预训练和提示微调’方法依赖同质性低频知识，无法处理现实世界中图的不同光谱分布，需要解决预训练和下游任务间大光谱差距问题以实现有效适配。

Method: 提出HS - GPPT模型，利用混合光谱滤波器骨干和局部 - 全局对比学习获取光谱知识，设计提示图使光谱分布与预文本对齐。

Result: 在归纳和直推学习设置下的大量实验验证了模型有效性。

Conclusion: HS - GPPT模型能通过确保光谱对齐，实现跨同质性和异质性的光谱知识转移，解决现有方法的局限。

Abstract: Graph ``pre-training and prompt-tuning'' aligns downstream tasks with
pre-trained objectives to enable efficient knowledge transfer under limited
supervision. However, existing methods rely on homophily-based low-frequency
knowledge, failing to handle diverse spectral distributions in real-world
graphs with varying homophily. Our theoretical analysis reveals a spectral
specificity principle: optimal knowledge transfer requires alignment between
pre-trained spectral filters and the intrinsic spectrum of downstream graphs.
Under limited supervision, large spectral gaps between pre-training and
downstream tasks impede effective adaptation. To bridge this gap, we propose
the HS-GPPT model, a novel framework that ensures spectral alignment throughout
both pre-training and prompt-tuning. We utilize a hybrid spectral filter
backbone and local-global contrastive learning to acquire abundant spectral
knowledge. Then we design prompt graphs to align the spectral distribution with
pretexts, facilitating spectral knowledge transfer across homophily and
heterophily. Extensive experiments validate the effectiveness under both
transductive and inductive learning settings. Our code is available at
https://anonymous.4open.science/r/HS-GPPT-62D2/.

</details>


### [65] [RegimeNAS: Regime-Aware Differentiable Architecture Search With Theoretical Guarantees for Financial Trading](https://arxiv.org/abs/2508.11338)
*Prathamesh Devadiga,Yashmitha Shailesh*

Main category: cs.LG

TL;DR: 介绍RegimeNAS框架，融合市场制度意识提升加密货币交易表现，优于现有基准。


<details>
  <summary>Details</summary>
Motivation: 解决静态深度学习模型在动态金融环境中的局限性。

Method: 采用理论基础的贝叶斯搜索空间、针对不同市场条件的动态激活神经模块、结合市场特定惩罚的多目标损失函数，利用多头注意力进行制度识别。

Result: 在真实加密货币数据上显著优于现有基准，平均绝对误差降低80.3%，收敛更快。

Conclusion: 在NAS过程中嵌入市场制度等特定领域知识，对开发金融应用的鲁棒自适应模型至关重要。

Abstract: We introduce RegimeNAS, a novel differentiable architecture search framework
specifically designed to enhance cryptocurrency trading performance by
explicitly integrating market regime awareness. Addressing the limitations of
static deep learning models in highly dynamic financial environments, RegimeNAS
features three core innovations: (1) a theoretically grounded Bayesian search
space optimizing architectures with provable convergence properties; (2)
specialized, dynamically activated neural modules (Volatility, Trend, and Range
blocks) tailored for distinct market conditions; and (3) a multi-objective loss
function incorporating market-specific penalties (e.g., volatility matching,
transition smoothness) alongside mathematically enforced Lipschitz stability
constraints. Regime identification leverages multi-head attention across
multiple timeframes for improved accuracy and uncertainty estimation. Rigorous
empirical evaluation on extensive real-world cryptocurrency data demonstrates
that RegimeNAS significantly outperforms state-of-the-art benchmarks, achieving
an 80.3% Mean Absolute Error reduction compared to the best traditional
recurrent baseline and converging substantially faster (9 vs. 50+ epochs).
Ablation studies and regime-specific analysis confirm the critical contribution
of each component, particularly the regime-aware adaptation mechanism. This
work underscores the imperative of embedding domain-specific knowledge, such as
market regimes, directly within the NAS process to develop robust and adaptive
models for challenging financial applications.

</details>


### [66] [Conformal Prediction Meets Long-tail Classification](https://arxiv.org/abs/2508.11345)
*Shuqi Liu,Jianguo Huang,Luke Ong*

Main category: cs.LG

TL;DR: 提出Tail - Aware Conformal Prediction (TACP)方法缓解长尾类覆盖不足，又提出软TACP (sTACP)改善所有类覆盖平衡，实验证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 现有CP方法在长尾标签分布下各类覆盖不均衡，尾部类覆盖不足，影响少数类预测集可靠性。

Method: 提出TACP方法利用长尾结构缩小头尾覆盖差距，又通过重加权机制提出sTACP改善所有类覆盖平衡，且框架可与多种非一致性得分结合。

Result: 理论分析表明TACP比标准方法头尾覆盖差距更小，多长尾基准数据集实验证明方法有效。

Conclusion: 提出的TACP和sTACP方法能有效缓解尾部类覆盖不足，改善各类覆盖平衡。

Abstract: Conformal Prediction (CP) is a popular method for uncertainty quantification
that converts a pretrained model's point prediction into a prediction set, with
the set size reflecting the model's confidence. Although existing CP methods
are guaranteed to achieve marginal coverage, they often exhibit imbalanced
coverage across classes under long-tail label distributions, tending to over
cover the head classes at the expense of under covering the remaining tail
classes. This under coverage is particularly concerning, as it undermines the
reliability of the prediction sets for minority classes, even with coverage
ensured on average. In this paper, we propose the Tail-Aware Conformal
Prediction (TACP) method to mitigate the under coverage of the tail classes by
utilizing the long-tail structure and narrowing the head-tail coverage gap.
Theoretical analysis shows that it consistently achieves a smaller head-tail
coverage gap than standard methods. To further improve coverage balance across
all classes, we introduce an extension of TACP: soft TACP (sTACP) via a
reweighting mechanism. The proposed framework can be combined with various
non-conformity scores, and experiments on multiple long-tail benchmark datasets
demonstrate the effectiveness of our methods.

</details>


### [67] [NeMo: A Neuron-Level Modularizing-While-Training Approach for Decomposing DNN Models](https://arxiv.org/abs/2508.11348)
*Xiaohan Bi,Binhang Qi,Hailong Sun,Xiang Gao,Yue Yu,Xiaojun Liang*

Main category: cs.LG

TL;DR: 随着DNN融入软件系统，构建成本高，现有MwT方法有局限，提出NeMo方法，实验证明其优势及实际应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有DNN构建成本高，模型复用有推理开销，现有MwT方法局限于小尺度CNN模型，无法处理多样和大规模模型。

Method: 提出NeMo方法，在神经元层面操作，设计基于对比学习的模块化训练方法和复合损失函数。

Result: 在Transformer和CNN模型实验中，模块分类准确率平均提高1.72%，模块大小减少58.10%。

Conclusion: NeMo是可扩展、通用的DNN模块化方法，在实际场景有潜在益处。

Abstract: With the growing incorporation of deep neural network (DNN) models into
modern software systems, the prohibitive construction costs have become a
significant challenge. Model reuse has been widely applied to reduce training
costs, but indiscriminately reusing entire models may incur significant
inference overhead. Consequently, DNN modularization has gained attention,
enabling module reuse by decomposing DNN models. The emerging
modularizing-while-training (MwT) paradigm, which incorporates modularization
into training, outperforms modularizing-after-training approaches. However,
existing MwT methods focus on small-scale CNN models at the convolutional
kernel level and struggle with diverse DNNs and large-scale models,
particularly Transformer-based models. To address these limitations, we propose
NeMo, a scalable and generalizable MwT approach. NeMo operates at the neuron
level fundamental component common to all DNNs-ensuring applicability to
Transformers and various architectures. We design a contrastive learning-based
modular training method with an effective composite loss function, enabling
scalability to large-scale models. Comprehensive experiments on two
Transformer-based models and four CNN models across two classification datasets
demonstrate NeMo's superiority over state-of-the-art MwT methods. Results show
average gains of 1.72% in module classification accuracy and 58.10% reduction
in module size, demonstrating efficacy across both CNN and large-scale
Transformer-based models. A case study on open-source projects shows NeMo's
potential benefits in practical scenarios, offering a promising approach for
scalable and generalizable DNN modularization.

</details>


### [68] [A Global Dataset of Location Data Integrity-Assessed Reforestation Efforts](https://arxiv.org/abs/2508.11349)
*Angela John,Selvyn Allotey,Till Koebe,Alexandra Tyukavina,Ingmar Weber*

Main category: cs.LG

TL;DR: 研究提出全球造林和再造林数据集，涵盖大量种植点和项目，引入LDIS评估位置数据完整性，发现多数监测点有问题，该数据集对碳市场和计算机视觉任务有价值。


<details>
  <summary>Details</summary>
Motivation: 应对自愿碳市场审查，解决造林和再造林项目数据可靠性及完整性问题。

Method: 从原始信息编译数据集，结合卫星图像等二次数据，引入LDIS评估种植点位置信息。

Result: 约79%监测的地理参考种植点至少有1项LDIS指标不通过，15%项目缺乏机器可读地理参考数据。

Conclusion: 该数据集可增强自愿碳市场问责制，也可作为计算机视觉任务训练数据。

Abstract: Afforestation and reforestation are popular strategies for mitigating climate
change by enhancing carbon sequestration. However, the effectiveness of these
efforts is often self-reported by project developers, or certified through
processes with limited external validation. This leads to concerns about data
reliability and project integrity. In response to increasing scrutiny of
voluntary carbon markets, this study presents a dataset on global afforestation
and reforestation efforts compiled from primary (meta-)information and
augmented with time-series satellite imagery and other secondary data. Our
dataset covers 1,289,068 planting sites from 45,628 projects spanning 33 years.
Since any remote sensing-based validation effort relies on the integrity of a
planting site's geographic boundary, this dataset introduces a standardized
assessment of the provided site-level location information, which we summarize
in one easy-to-communicate key indicator: LDIS -- the Location Data Integrity
Score. We find that approximately 79\% of the georeferenced planting sites
monitored fail on at least 1 out of 10 LDIS indicators, while 15\% of the
monitored projects lack machine-readable georeferenced data in the first place.
In addition to enhancing accountability in the voluntary carbon market, the
presented dataset also holds value as training data for e.g. computer
vision-related tasks with millions of linked Sentinel-2 and Planetscope
satellite images.

</details>


### [69] [Harmonized Gradient Descent for Class Imbalanced Data Stream Online Learning](https://arxiv.org/abs/2508.11353)
*Han Zhou,Hongpeng Yin,Xuanhong Deng,Yuyu Huang,Hao Ren*

Main category: cs.LG

TL;DR: 本文提出协调梯度下降（HGD）算法解决不平衡数据流学习问题，具有高效性和有效性。


<details>
  <summary>Details</summary>
Motivation: 现实中数据按时间顺序收集且类别分布倾斜，形成不平衡数据流，现有方法多采用重采样和重加权，本文通过训练修改解决不平衡问题。

Method: 引入协调梯度下降（HGD）算法，平衡不同类别的梯度范数，实现平衡在线学习，且无需数据缓冲、额外参数或先验知识。

Result: 理论分析表明HGD达到满意的次线性遗憾界，实验评估证明HGD在学习不平衡数据流方面的效率和有效性。

Conclusion: HGD算法在解决不平衡数据流学习问题上是高效且有效的。

Abstract: Many real-world data are sequentially collected over time and often exhibit
skewed class distributions, resulting in imbalanced data streams. While
existing approaches have explored several strategies, such as resampling and
reweighting, for imbalanced data stream learning, our work distinguishes itself
by addressing the imbalance problem through training modification, particularly
focusing on gradient descent techniques. We introduce the harmonized gradient
descent (HGD) algorithm, which aims to equalize the norms of gradients across
different classes. By ensuring the gradient norm balance, HGD mitigates
under-fitting for minor classes and achieves balanced online learning. Notably,
HGD operates in a streamlined implementation process, requiring no data-buffer,
extra parameters, or prior knowledge, making it applicable to any learning
models utilizing gradient descent for optimization. Theoretical analysis, based
on a few common and mild assumptions, shows that HGD achieves a satisfied
sub-linear regret bound. The proposed algorithm are compared with the commonly
used online imbalance learning methods under several imbalanced data stream
scenarios. Extensive experimental evaluations demonstrate the efficiency and
effectiveness of HGD in learning imbalanced data streams.

</details>


### [70] [ETTRL: Balancing Exploration and Exploitation in LLM Test-Time Reinforcement Learning Via Entropy Mechanism](https://arxiv.org/abs/2508.11356)
*Jia Liu,ChangYi He,YingQiao Lin,MingMin Yang,FeiYang Shen,ShaoGuo Liu,TingTing Gao*

Main category: cs.LG

TL;DR: 现有大语言模型依赖标注数据且无监督适应性有限，TTRL有挑战，本文提出熵机制优化TTRL，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型依赖标注数据、无监督适应性有限及TTRL存在的高推理成本、早期估计偏差等问题。

Method: 引入基于熵的机制，采用Entropy-fork Tree Majority Rollout (ETMR) 和Entropy-based Advantage Reshaping (EAR) 两种策略。

Result: 使Llama3.1 - 8B在AIME 2024基准测试的Pass at 1指标上相对提升68%，仅消耗60%的滚动令牌预算。

Conclusion: 该方法能有效优化推理效率、多样性和估计稳健性之间的权衡，推动开放域推理任务的无监督强化学习。

Abstract: Recent advancements in Large Language Models have yielded significant
improvements in complex reasoning tasks such as mathematics and programming.
However, these models remain heavily dependent on annotated data and exhibit
limited adaptability in unsupervised scenarios. To address these limitations,
test-time reinforcement learning (TTRL) has been proposed, which enables
self-optimization by leveraging model-generated pseudo-labels. Despite its
promise, TTRL faces several key challenges, including high inference costs due
to parallel rollouts and early-stage estimation bias that fosters
overconfidence, reducing output diversity and causing performance plateaus. To
address these challenges, we introduce an entropy-based mechanism to enhance
the exploration-exploitation balance in test-time reinforcement learning
through two strategies: Entropy-fork Tree Majority Rollout (ETMR) and
Entropy-based Advantage Reshaping (EAR). Compared with the baseline, our
approach enables Llama3.1-8B to achieve a 68 percent relative improvement in
Pass at 1 metric on the AIME 2024 benchmark, while consuming only 60 percent of
the rollout tokens budget. This highlights our method's ability to effectively
optimize the trade-off between inference efficiency, diversity, and estimation
robustness, thereby advancing unsupervised reinforcement learning for
open-domain reasoning tasks.

</details>


### [71] [PTSM: Physiology-aware and Task-invariant Spatio-temporal Modeling for Cross-Subject EEG Decoding](https://arxiv.org/abs/2508.11357)
*Changhong Jing,Yan Liu,Shuqiang Wang,Bruce X. B. Yu,Gong Chen,Zhejing Hu,Zhi Zhang,Yanyan Shen*

Main category: cs.LG

TL;DR: 提出PTSM框架用于跨未见过受试者的可解释且稳健的EEG解码，实验显示其零样本泛化能力强，优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 解决跨受试者脑电图解码中因个体差异大、缺乏受试者不变表征带来的挑战。

Method: 提出PTSM框架，采用双分支掩码机制学习个性化和共享的时空模式，对掩码在时空维度分解，施加信息论约束分解潜在嵌入，通过多目标损失端到端训练模型。

Result: 在跨受试者运动想象数据集上，PTSM实现强零样本泛化，无需特定受试者校准就优于现有基线。

Conclusion: 解纠缠的神经表征在非平稳神经生理环境中对实现个性化和可迁移解码有效。

Abstract: Cross-subject electroencephalography (EEG) decoding remains a fundamental
challenge in brain-computer interface (BCI) research due to substantial
inter-subject variability and the scarcity of subject-invariant
representations. This paper proposed PTSM (Physiology-aware and Task-invariant
Spatio-temporal Modeling), a novel framework for interpretable and robust EEG
decoding across unseen subjects. PTSM employs a dual-branch masking mechanism
that independently learns personalized and shared spatio-temporal patterns,
enabling the model to preserve individual-specific neural characteristics while
extracting task-relevant, population-shared features. The masks are factorized
across temporal and spatial dimensions, allowing fine-grained modulation of
dynamic EEG patterns with low computational overhead. To further address
representational entanglement, PTSM enforces information-theoretic constraints
that decompose latent embeddings into orthogonal task-related and
subject-related subspaces. The model is trained end-to-end via a
multi-objective loss integrating classification, contrastive, and
disentanglement objectives. Extensive experiments on cross-subject motor
imagery datasets demonstrate that PTSM achieves strong zero-shot
generalization, outperforming state-of-the-art baselines without
subject-specific calibration. Results highlight the efficacy of disentangled
neural representations for achieving both personalized and transferable
decoding in non-stationary neurophysiological settings.

</details>


### [72] [Fusing Rewards and Preferences in Reinforcement Learning](https://arxiv.org/abs/2508.11363)
*Sadegh Khorasani,Saber Salehkaleybar,Negar Kiyavash,Matthias Grossglauser*

Main category: cs.LG

TL;DR: 提出双反馈智能体（DFA）强化学习算法，融合个体奖励和成对偏好，模拟结果表明其在多环境表现良好。


<details>
  <summary>Details</summary>
Motivation: 为强化学习设计一种能融合个体奖励和成对偏好的算法。

Method: 使用策略的对数概率直接对偏好概率建模，避免单独的奖励建模步骤，偏好可由人工标注或从离线策略重放缓冲区合成。证明在布拉德利 - 特里模型下最小化偏好损失可恢复熵正则化的软演员 - 评论家（SAC）策略。

Result: 在六个控制环境中，基于生成偏好训练的DFA达到或超过SAC，训练过程更稳定；在随机网格世界中，仅使用半合成偏好数据集，算法优于基于人类反馈的奖励建模强化学习基线。

Conclusion: DFA算法在融合个体奖励和成对偏好方面表现良好，具有较好的性能和稳定性。

Abstract: We present Dual-Feedback Actor (DFA), a reinforcement learning algorithm that
fuses both individual rewards and pairwise preferences (if available) into a
single update rule. DFA uses the policy's log-probabilities directly to model
the preference probability, avoiding a separate reward-modeling step.
Preferences can be provided by human-annotators (at state-level or
trajectory-level) or be synthesized online from Q-values stored in an
off-policy replay buffer. Under a Bradley-Terry model, we prove that minimizing
DFA's preference loss recovers the entropy-regularized Soft Actor-Critic (SAC)
policy. Our simulation results show that DFA trained on generated preferences
matches or exceeds SAC on six control environments and demonstrates a more
stable training process. With only a semi-synthetic preference dataset under
Bradley-Terry model, our algorithm outperforms reward-modeling reinforcement
learning from human feedback (RLHF) baselines in a stochastic GridWorld and
approaches the performance of an oracle with true rewards.

</details>


### [73] [Minimizing Surrogate Losses for Decision-Focused Learning using Differentiable Optimization](https://arxiv.org/abs/2508.11365)
*Jayanta Mandi,Ali İrfan Mahmutoğulları,Senne Berden,Tias Guns*

Main category: cs.LG

TL;DR: 本文指出现有基于梯度的线性规划决策聚焦学习方法问题，提出最小化替代损失策略，实验证明其效果佳且用DYS - Net可显著减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度的线性规划决策聚焦学习方法存在梯度为零的问题，需要改进。

Method: 提出即使使用可微优化层且能直接最小化遗憾时，也采用最小化替代损失的方法。

Result: 最小化替代损失使可微优化层实现的遗憾与基于替代损失的决策聚焦学习方法相当或更好；用DYS - Net最小化替代损失能在达到现有最佳水平的同时显著减少训练时间。

Conclusion: 最小化替代损失策略有效，结合DYS - Net可提升训练效率。

Abstract: Decision-focused learning (DFL) trains a machine learning (ML) model to
predict parameters of an optimization problem, to directly minimize decision
regret, i.e., maximize decision quality. Gradient-based DFL requires computing
the derivative of the solution to the optimization problem with respect to the
predicted parameters. However, for many optimization problems, such as linear
programs (LPs), the gradient of the regret with respect to the predicted
parameters is zero almost everywhere. Existing gradient-based DFL approaches
for LPs try to circumvent this issue in one of two ways: (a) smoothing the LP
into a differentiable optimization problem by adding a quadratic regularizer
and then minimizing the regret directly or (b) minimizing surrogate losses that
have informative (sub)gradients. In this paper, we show that the former
approach still results in zero gradients, because even after smoothing the
regret remains constant across large regions of the parameter space. To address
this, we propose minimizing surrogate losses -- even when a differentiable
optimization layer is used and regret can be minimized directly. Our
experiments demonstrate that minimizing surrogate losses allows differentiable
optimization layers to achieve regret comparable to or better than
surrogate-loss based DFL methods. Further, we demonstrate that this also holds
for DYS-Net, a recently proposed differentiable optimization technique for LPs,
that computes approximate solutions and gradients through operations that can
be performed using feedforward neural network layers. Because DYS-Net executes
the forward and the backward pass very efficiently, by minimizing surrogate
losses using DYS-Net, we are able to attain regret on par with the
state-of-the-art while reducing training time by a significant margin.

</details>


### [74] [A Remedy for Over-Squashing in Graph Learning via Forman-Ricci Curvature based Graph-to-Hypergraph Structural Lifting](https://arxiv.org/abs/2508.11390)
*Michael Banf,Dominik Filipiak,Max Schattauer,Liliya Imasheva*

Main category: cs.LG

TL;DR: 提出基于Forman - Ricci曲率的结构提升策略，解决图学习中长距离信息传递和瓶颈的信息失真问题。


<details>
  <summary>Details</summary>
Motivation: 许多现实系统复杂交互需高阶拓扑结构表示，传统图神经网络有局限，要解决图学习中长距离信息传递和瓶颈的信息失真问题。

Method: 提出使用Forman - Ricci曲率的结构提升策略，利用其定义基于黎曼几何的边网络特征。

Result: 未明确提及具体结果。

Conclusion: 该方法可解决图学习中信息失真的过挤压现象。

Abstract: Graph Neural Networks are highly effective at learning from relational data,
leveraging node and edge features while maintaining the symmetries inherent to
graph structures. However, many real-world systems, such as social or
biological networks, exhibit complex interactions that are more naturally
represented by higher-order topological domains. The emerging field of
Geometric and Topological Deep Learning addresses this challenge by introducing
methods that utilize and benefit from higher-order structures. Central to TDL
is the concept of lifting, which transforms data representations from basic
graph forms to more expressive topologies before the application of GNN models
for learning. In this work, we propose a structural lifting strategy using
Forman-Ricci curvature, which defines an edge-based network characteristic
based on Riemannian geometry. Curvature reveals local and global properties of
a graph, such as a network's backbones, i.e. coarse, structure-preserving graph
geometries that form connections between major communities - most suitably
represented as hyperedges to model information flows between clusters across
large distances in the network. To this end, our approach provides a remedy to
the problem of information distortion in message passing across long distances
and graph bottlenecks - a phenomenon known in graph learning as over-squashing.

</details>


### [75] [On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised Fine-Tuning and Reinforcement Learning via Dynamic Weighting](https://arxiv.org/abs/2508.11408)
*Wenhao Zhang,Yuexiang Xie,Yuchang Sun,Yanxi Chen,Guoyin Wang,Yaliang Li,Bolin Ding,Jingren Zhou*

Main category: cs.LG

TL;DR: 本文通过新颖视角统一SFT和RL，提出CHORD框架，实验证明其有效且稳定，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 现有集成SFT和RL的方法存在破坏模型模式和过拟合专家数据的风险。

Method: 从离策略和策略内视角统一SFT和RL，提出CHORD框架，纳入双控机制，用全局系数和逐令牌加权函数。

Result: 在常用基准上实验表明CHORD学习过程稳定高效，相比基线有显著提升。

Conclusion: CHORD能有效协调离策略专家数据和策略内探索，值得进一步研究。

Abstract: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are two
prominent post-training paradigms for refining the capabilities and aligning
the behavior of Large Language Models (LLMs). Existing approaches that
integrate SFT and RL often face the risk of disrupting established model
patterns and inducing overfitting to expert data. To address this, we present a
novel investigation into the unified view of SFT and RL through an off-policy
versus on-policy lens. We propose CHORD, a framework for the Controllable
Harmonization of On- and Off-Policy Reinforcement Learning via Dynamic
Weighting, which reframes SFT not as a separate stage but as a dynamically
weighted auxiliary objective within the on-policy RL process. Based on an
analysis of off-policy expert data's influence at both holistic and granular
levels, we incorporate a dual-control mechanism in CHORD. Specifically, the
framework first employs a global coefficient to holistically guide the
transition from off-policy imitation to on-policy exploration, and then applies
a token-wise weighting function that enables granular learning from expert
tokens, which preserves on-policy exploration and mitigates disruption from
off-policy data. We conduct extensive experiments on widely used benchmarks,
providing empirical evidence that CHORD achieves a stable and efficient
learning process. By effectively harmonizing off-policy expert data with
on-policy exploration, CHORD demonstrates significant improvements over
baselines. We release the implementation at
https://github.com/modelscope/Trinity-RFT/tree/main/examples/mix_chord to
inspire further research.

</details>


### [76] [Generative Co-Design of Antibody Sequences and Structures via Black-Box Guidance in a Shared Latent Space](https://arxiv.org/abs/2508.11424)
*Yinghua Yao,Yuangang Pan,Xixian Chen*

Main category: cs.LG

TL;DR: 提出序列 - 结构协同设计框架 LEAD 优化抗体 CDR，性能优于基线方法，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有优化互补决定区（CDRs）方法在原始数据空间操作，搜索效率低、评估成本高。

Method: 提出序列 - 结构协同设计框架 LEAD，在共享潜空间同时优化序列和结构，设计黑盒引导策略适应不可微评估器。

Result: LEAD 在单属性和多属性优化目标上性能优越，减少一半查询消耗，超越基线方法。

Conclusion: LEAD 能突破现有方法局限，实现不同模态设计同步，有效优化抗体 CDR。

Abstract: Advancements in deep generative models have enabled the joint modeling of
antibody sequence and structure, given the antigen-antibody complex as context.
However, existing approaches for optimizing complementarity-determining regions
(CDRs) to improve developability properties operate in the raw data space,
leading to excessively costly evaluations due to the inefficient search
process. To address this, we propose LatEnt blAck-box Design (LEAD), a
sequence-structure co-design framework that optimizes both sequence and
structure within their shared latent space. Optimizing shared latent codes can
not only break through the limitations of existing methods, but also ensure
synchronization of different modality designs. Particularly, we design a
black-box guidance strategy to accommodate real-world scenarios where many
property evaluators are non-differentiable. Experimental results demonstrate
that our LEAD achieves superior optimization performance for both single and
multi-property objectives. Notably, LEAD reduces query consumption by a half
while surpassing baseline methods in property optimization. The code is
available at https://github.com/EvaFlower/LatEnt-blAck-box-Design.

</details>


### [77] [Robust Convolution Neural ODEs via Contractivity-promoting regularization](https://arxiv.org/abs/2508.11432)
*Muhammad Zakwan,Liang Xu,Giancarlo Ferrari-Trecate*

Main category: cs.LG

TL;DR: 论文考虑卷积神经常微分方程（NODEs），用收缩理论提升其对输入噪声和对抗攻击的鲁棒性，通过正则化项实现，并在MNIST和FashionMNIST数据集上验证性能。


<details>
  <summary>Details</summary>
Motivation: 神经网络易受输入噪声和对抗攻击影响，需提升其鲁棒性。

Method: 使用收缩理论，在训练中通过涉及系统动力学雅可比矩阵的正则化项诱导收缩性，对一类有斜率限制激活函数的NODEs用精心选择的权重正则化项降低计算负担。

Result: 通过MNIST和FashionMNIST数据集的基准图像分类任务展示了所提正则化器的性能。

Conclusion: 收缩理论可有效提升卷积神经常微分方程的鲁棒性。

Abstract: Neural networks can be fragile to input noise and adversarial attacks.
  In this work, we consider Convolutional Neural Ordinary Differential
Equations (NODEs), a family of continuous-depth neural networks represented by
dynamical systems, and propose to use contraction theory to improve their
robustness.
  For a contractive dynamical system two trajectories starting from different
initial conditions converge to each other exponentially fast.
  Contractive Convolutional NODEs can enjoy increased robustness as slight
perturbations of the features do not cause a significant change in the output.
  Contractivity can be induced during training by using a regularization term
involving the Jacobian of the system dynamics.
  To reduce the computational burden, we show that it can also be promoted
using carefully selected weight regularization terms for a class of NODEs with
slope-restricted activation functions.
  The performance of the proposed regularizers is illustrated through benchmark
image classification tasks on MNIST and FashionMNIST datasets, where images are
corrupted by different kinds of noise and attacks.

</details>


### [78] [Multi-Sensory Cognitive Computing for Learning Population-level Brain Connectivity](https://arxiv.org/abs/2508.11436)
*Mayssa Soussia,Mohamed Ali Mahjoub,Islem Rekik*

Main category: cs.LG

TL;DR: 本文提出mCOCO框架学习功能连接脑模板，优于GNN方法，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有CBT学习方法存在可解释性差、计算成本高、忽视认知能力等局限。

Method: 引入mCOCO框架，利用储层计算从BOLD信号学习功能CBT，分两步，先映射信号聚合为组级CBT，再通过认知储层融入多感官输入。

Result: mCOCO模板在多个指标上显著优于GNN模板。

Conclusion: mCOCO框架有效解决现有方法问题，在CBT学习上表现出色。

Abstract: The generation of connectional brain templates (CBTs) has recently garnered
significant attention for its potential to identify unique connectivity
patterns shared across individuals. However, existing methods for CBT learning
such as conventional machine learning and graph neural networks (GNNs) are
hindered by several limitations. These include: (i) poor interpretability due
to their black-box nature, (ii) high computational cost, and (iii) an exclusive
focus on structure and topology, overlooking the cognitive capacity of the
generated CBT. To address these challenges, we introduce mCOCO (multi-sensory
COgnitive COmputing), a novel framework that leverages Reservoir Computing (RC)
to learn population-level functional CBT from BOLD
(Blood-Oxygen-level-Dependent) signals. RC's dynamic system properties allow
for tracking state changes over time, enhancing interpretability and enabling
the modeling of brain-like dynamics, as demonstrated in prior literature. By
integrating multi-sensory inputs (e.g., text, audio, and visual data), mCOCO
captures not only structure and topology but also how brain regions process
information and adapt to cognitive tasks such as sensory processing, all in a
computationally efficient manner. Our mCOCO framework consists of two phases:
(1) mapping BOLD signals into the reservoir to derive individual functional
connectomes, which are then aggregated into a group-level CBT - an approach, to
the best of our knowledge, not previously explored in functional connectivity
studies - and (2) incorporating multi-sensory inputs through a cognitive
reservoir, endowing the CBT with cognitive traits. Extensive evaluations show
that our mCOCO-based template significantly outperforms GNN-based CBT in terms
of centeredness, discriminativeness, topological soundness, and multi-sensory
memory retention. Our source code is available at
https://github.com/basiralab/mCOCO.

</details>


### [79] [Informative Post-Hoc Explanations Only Exist for Simple Functions](https://arxiv.org/abs/2508.11441)
*Eric Günther,Balázs Szabados,Robi Bhattacharjee,Sebastian Bordt,Ulrike von Luxburg*

Main category: cs.LG

TL;DR: 提出解释应提供决策函数信息的框架，证明不少流行解释算法对复杂决策函数无信息性，推导其变得有信息性的条件并讨论改进方法，分析对AI应用有实际意义。


<details>
  <summary>Details</summary>
Motivation: 现有局部事后解释算法对复杂模型的理论保证不足，需明确其对复杂模型的有效性及相关假设。

Method: 引入基于学习理论的框架，以解释能否降低似然决策函数空间的复杂度判断其是否有信息性。

Result: 许多流行解释算法对复杂决策函数无信息性，不同算法有使其有信息性的条件，如梯度和反事实解释对可微函数空间无信息性等。

Conclusion: 分析对解释算法的实际应用，尤其在AI审计、监管和高风险应用方面有重要意义。

Abstract: Many researchers have suggested that local post-hoc explanation algorithms
can be used to gain insights into the behavior of complex machine learning
models. However, theoretical guarantees about such algorithms only exist for
simple decision functions, and it is unclear whether and under which
assumptions similar results might exist for complex models. In this paper, we
introduce a general, learning-theory-based framework for what it means for an
explanation to provide information about a decision function. We call an
explanation informative if it serves to reduce the complexity of the space of
plausible decision functions. With this approach, we show that many popular
explanation algorithms are not informative when applied to complex decision
functions, providing a rigorous mathematical rejection of the idea that it
should be possible to explain any model. We then derive conditions under which
different explanation algorithms become informative. These are often stronger
than what one might expect. For example, gradient explanations and
counterfactual explanations are non-informative with respect to the space of
differentiable functions, and SHAP and anchor explanations are not informative
with respect to the space of decision trees. Based on these results, we discuss
how explanation algorithms can be modified to become informative. While the
proposed analysis of explanation algorithms is mathematical, we argue that it
holds strong implications for the practical applicability of these algorithms,
particularly for auditing, regulation, and high-risk applications of AI.

</details>


### [80] [Predicting and Explaining Traffic Crash Severity Through Crash Feature Selection](https://arxiv.org/abs/2508.11504)
*Andrea Castellani,Zacharias Papadovasilakis,Giorgos Papoutsoglou,Mary Cole,Brian Bautsch,Tobias Rodemann,Ioannis Tsamardinos,Angela Harden*

Main category: cs.LG

TL;DR: 研究引入俄亥俄州六年超300万人事故数据集，结合AutoML和可解释AI分析严重车祸风险因素，最终模型表现良好，强调方法严谨和可解释性。


<details>
  <summary>Details</summary>
Motivation: 机动车碰撞是全球伤亡主因，需数据驱动方法理解和减轻碰撞严重程度。

Method: 结合Automated Machine Learning (AutoML) 和可解释人工智能 (AI)，用JADBio AutoML平台构建预测模型，通过SHapley Additive exPlanations (SHAP) 解释输出。

Result: 最终Ridge Logistic Regression模型在训练集AUC - ROC达85.6%，测试集达84.9%，确定17个最有影响力预测因素，部分传统因素影响力不如环境和背景变量。

Conclusion: 本研究强调方法严谨和可解释性，提供可扩展框架支持零伤亡愿景和先进交通安全政策。

Abstract: Motor vehicle crashes remain a leading cause of injury and death worldwide,
necessitating data-driven approaches to understand and mitigate crash severity.
This study introduces a curated dataset of more than 3 million people involved
in accidents in Ohio over six years (2017-2022), aggregated to more than 2.3
million vehicle-level records for predictive analysis. The primary contribution
is a transparent and reproducible methodology that combines Automated Machine
Learning (AutoML) and explainable artificial intelligence (AI) to identify and
interpret key risk factors associated with severe crashes. Using the JADBio
AutoML platform, predictive models were constructed to distinguish between
severe and non-severe crash outcomes. The models underwent rigorous feature
selection across stratified training subsets, and their outputs were
interpreted using SHapley Additive exPlanations (SHAP) to quantify the
contribution of individual features. A final Ridge Logistic Regression model
achieved an AUC-ROC of 85.6% on the training set and 84.9% on a hold-out test
set, with 17 features consistently identified as the most influential
predictors. Key features spanned demographic, environmental, vehicle, human,
and operational categories, including location type, posted speed, minimum
occupant age, and pre-crash action. Notably, certain traditionally emphasized
factors, such as alcohol or drug impairment, were less influential in the final
model compared to environmental and contextual variables. Emphasizing
methodological rigor and interpretability over mere predictive performance,
this study offers a scalable framework to support Vision Zero with aligned
interventions and advanced data-informed traffic safety policy.

</details>


### [81] [Towards Faithful Class-level Self-explainability in Graph Neural Networks by Subgraph Dependencies](https://arxiv.org/abs/2508.11513)
*Fanzhen Liu,Xiaoxiao Ma,Jian Yang,Alsharif Abuadbba,Kristen Moore,Surya Nepal,Cecile Paris,Quan Z. Sheng,Jia Wu*

Main category: cs.LG

TL;DR: 本文提出GraphOracle框架用于生成和评估GNN的类级解释，对比显示其在保真度、可解释性和可扩展性上更优。


<details>
  <summary>Details</summary>
Motivation: 现有自解释GNN模型评估仅关注实例级解释，不清楚类特定原型能否跨同类实例有效泛化，需生成和评估类级解释。

Method: 联合学习GNN分类器和一组对每个类有区分性的结构化、稀疏子图，采用新颖的集成训练并通过基于掩码的评估策略验证。

Result: ProtGNN和PGIB不能有效提供类级解释，GraphOracle在一系列图分类任务中表现出更好的保真度、可解释性和可扩展性，避免了先前方法的计算瓶颈。

Conclusion: GraphOracle是GNN中实现可靠类级自解释性的实用且有原则的解决方案。

Abstract: Enhancing the interpretability of graph neural networks (GNNs) is crucial to
ensure their safe and fair deployment. Recent work has introduced
self-explainable GNNs that generate explanations as part of training, improving
both faithfulness and efficiency. Some of these models, such as ProtGNN and
PGIB, learn class-specific prototypes, offering a potential pathway toward
class-level explanations. However, their evaluations focus solely on
instance-level explanations, leaving open the question of whether these
prototypes meaningfully generalize across instances of the same class. In this
paper, we introduce GraphOracle, a novel self-explainable GNN framework
designed to generate and evaluate class-level explanations for GNNs. Our model
jointly learns a GNN classifier and a set of structured, sparse subgraphs that
are discriminative for each class. We propose a novel integrated training that
captures graph$\unicode{x2013}$subgraph$\unicode{x2013}$prediction dependencies
efficiently and faithfully, validated through a masking-based evaluation
strategy. This strategy enables us to retroactively assess whether prior
methods like ProtGNN and PGIB deliver effective class-level explanations. Our
results show that they do not. In contrast, GraphOracle achieves superior
fidelity, explainability, and scalability across a range of graph
classification tasks. We further demonstrate that GraphOracle avoids the
computational bottlenecks of previous methods$\unicode{x2014}$like Monte Carlo
Tree Search$\unicode{x2014}$by using entropy-regularized subgraph selection and
lightweight random walk extraction, enabling faster and more scalable training.
These findings position GraphOracle as a practical and principled solution for
faithful class-level self-explainability in GNNs.

</details>


### [82] [DiCriTest: Testing Scenario Generation for Decision-Making Agents Considering Diversity and Criticality](https://arxiv.org/abs/2508.11514)
*Qitong Chu,Yufeng Yue,Danya Yao,Huaxin Pei*

Main category: cs.LG

TL;DR: 提出双空间引导测试框架生成兼顾多样性和关键性的测试场景，实验显示该框架优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 动态环境中决策代理部署增加对安全验证的需求，现有关键测试场景生成方法难平衡多样性和关键性，易陷入局部最优。

Method: 提出双空间引导测试框架，在场景参数空间采用分层表示框架定位子空间并协调两种生成模式；在代理行为空间利用交互数据量化行为指标并支持模式切换形成闭环。

Result: 在五个决策代理上测试，框架平均提高关键场景生成率56.23%，在新指标下展现出更高多样性，优于现有基线。

Conclusion: 所提双空间引导测试框架能有效生成兼顾多样性和关键性的测试场景，提升安全验证效果。

Abstract: The growing deployment of decision-making agents in dynamic environments
increases the demand for safety verification. While critical testing scenario
generation has emerged as an appealing verification methodology, effectively
balancing diversity and criticality remains a key challenge for existing
methods, particularly due to local optima entrapment in high-dimensional
scenario spaces. To address this limitation, we propose a dual-space guided
testing framework that coordinates scenario parameter space and agent behavior
space, aiming to generate testing scenarios considering diversity and
criticality. Specifically, in the scenario parameter space, a hierarchical
representation framework combines dimensionality reduction and
multi-dimensional subspace evaluation to efficiently localize diverse and
critical subspaces. This guides dynamic coordination between two generation
modes: local perturbation and global exploration, optimizing critical scenario
quantity and diversity. Complementarily, in the agent behavior space,
agent-environment interaction data are leveraged to quantify behavioral
criticality/diversity and adaptively support generation mode switching, forming
a closed feedback loop that continuously enhances scenario characterization and
exploration within the parameter space. Experiments show our framework improves
critical scenario generation by an average of 56.23\% and demonstrates greater
diversity under novel parameter-behavior co-driven metrics when tested on five
decision-making agents, outperforming state-of-the-art baselines.

</details>


### [83] [Finite-Width Neural Tangent Kernels from Feynman Diagrams](https://arxiv.org/abs/2508.11522)
*Max Guillen,Philipp Misof,Jan E. Gerken*

Main category: cs.LG

TL;DR: 本文引入费曼图计算神经切线核（NTK）统计量的有限宽度修正，简化代数运算，扩展深度网络稳定性结果并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 无限宽度下NTK虽易计算，但缺少训练的重要属性，需考虑有限宽度效应。

Method: 引入费曼图计算有限宽度修正，推导层递归关系。

Result: 扩展了深度网络稳定性结果，证明特定情况下无有限宽度修正，数值实验验证结果。

Conclusion: 提出的框架可行，能有效处理NTK有限宽度修正问题。

Abstract: Neural tangent kernels (NTKs) are a powerful tool for analyzing deep,
non-linear neural networks. In the infinite-width limit, NTKs can easily be
computed for most common architectures, yielding full analytic control over the
training dynamics. However, at infinite width, important properties of training
such as NTK evolution or feature learning are absent. Nevertheless, finite
width effects can be included by computing corrections to the Gaussian
statistics at infinite width. We introduce Feynman diagrams for computing
finite-width corrections to NTK statistics. These dramatically simplify the
necessary algebraic manipulations and enable the computation of layer-wise
recursive relations for arbitrary statistics involving preactivations, NTKs and
certain higher-derivative tensors (dNTK and ddNTK) required to predict the
training dynamics at leading order. We demonstrate the feasibility of our
framework by extending stability results for deep networks from preactivations
to NTKs and proving the absence of finite-width corrections for scale-invariant
nonlinearities such as ReLU on the diagonal of the Gram matrix of the NTK. We
validate our results with numerical experiments.

</details>


### [84] [Physics-Informed Diffusion Models for Unsupervised Anomaly Detection in Multivariate Time Series](https://arxiv.org/abs/2508.11528)
*Juhi Soni,Markus Lange-Hegermann,Stefan Windmann*

Main category: cs.LG

TL;DR: 提出基于物理信息扩散模型的无监督异常检测方法，实验表明该方法在异常检测上有更好表现。


<details>
  <summary>Details</summary>
Motivation: 利用物理信息提升扩散模型在多元时间序列数据无监督异常检测中的性能。

Method: 在扩散模型训练中使用加权物理信息损失学习多元时间序列数据的物理依赖时间分布，通过静态权重表构建损失。

Result: 物理信息训练提高了异常检测的F1分数，生成更好的数据多样性和对数似然，模型在合成和真实数据集上表现优于基线方法、先前物理信息工作和纯数据驱动扩散模型。

Conclusion: 所提出的基于物理信息扩散模型的无监督异常检测方法有效，能提升异常检测性能。

Abstract: We propose an unsupervised anomaly detection approach based on a
physics-informed diffusion model for multivariate time series data. Over the
past years, diffusion model has demonstrated its effectiveness in forecasting,
imputation, generation, and anomaly detection in the time series domain. In
this paper, we present a new approach for learning the physics-dependent
temporal distribution of multivariate time series data using a weighted
physics-informed loss during diffusion model training. A weighted
physics-informed loss is constructed using a static weight schedule. This
approach enables a diffusion model to accurately approximate underlying data
distribution, which can influence the unsupervised anomaly detection
performance. Our experiments on synthetic and real-world datasets show that
physics-informed training improves the F1 score in anomaly detection; it
generates better data diversity and log-likelihood. Our model outperforms
baseline approaches, additionally, it surpasses prior physics-informed work and
purely data-driven diffusion models on a synthetic dataset and one real-world
dataset while remaining competitive on others.

</details>


### [85] [A Comprehensive Perspective on Explainable AI across the Machine Learning Workflow](https://arxiv.org/abs/2508.11529)
*George Paterakis,Andrea Castellani,George Papoutsoglou,Tobias Rodemann,Ioannis Tsamardinos*

Main category: cs.LG

TL;DR: 提出整体可解释人工智能（HXAI）框架，嵌入解释到数据分析各阶段，统一组件，识别解释特征，还展示AI代理作用，融合多学科概念推动负责任AI部署。


<details>
  <summary>Details</summary>
Motivation: 传统可解释人工智能方法忽略上下游决策和质量检查，用户视AI模型为黑盒，需新框架解决信任问题。

Method: 提出HXAI框架，统一六个组件形成分类法，用112项问题库覆盖需求，基于多理论识别解释特征，展示AI代理运用解释技术。

Result: 明确解释特征，形成综合分类法，减少术语歧义，可对现有工具链进行严格覆盖分析。

Conclusion: 融合多学科概念、实际项目经验和文献，提出新颖的端到端观点以推动AI透明、可信和负责任部署。

Abstract: Artificial intelligence is reshaping science and industry, yet many users
still regard its models as opaque "black boxes". Conventional explainable
artificial-intelligence methods clarify individual predictions but overlook the
upstream decisions and downstream quality checks that determine whether
insights can be trusted. In this work, we present Holistic Explainable
Artificial Intelligence (HXAI), a user-centric framework that embeds
explanation into every stage of the data-analysis workflow and tailors those
explanations to users. HXAI unifies six components (data, analysis set-up,
learning process, model output, model quality, communication channel) into a
single taxonomy and aligns each component with the needs of domain experts,
data analysts and data scientists. A 112-item question bank covers these needs;
our survey of contemporary tools highlights critical coverage gaps. Grounded in
theories of human explanation, principles from human-computer interaction and
findings from empirical user studies, HXAI identifies the characteristics that
make explanations clear, actionable and cognitively manageable. A comprehensive
taxonomy operationalises these insights, reducing terminological ambiguity and
enabling rigorous coverage analysis of existing toolchains. We further
demonstrate how AI agents that embed large-language models can orchestrate
diverse explanation techniques, translating technical artifacts into
stakeholder-specific narratives that bridge the gap between AI developers and
domain experts. Departing from traditional surveys or perspective articles,
this work melds concepts from multiple disciplines, lessons from real-world
projects and a critical synthesis of the literature to advance a novel,
end-to-end viewpoint on transparency, trustworthiness and responsible AI
deployment.

</details>


### [86] [DFed-SST: Building Semantic- and Structure-aware Topologies for Decentralized Federated Graph Learning](https://arxiv.org/abs/2508.11530)
*Lianshuai Guo,Zhongzheng Yuan,Xunkai Li,Yinlin Zhu,Meixia Qu,Wenyu Wang*

Main category: cs.LG

TL;DR: 提出DFed - SST去中心化联邦图学习框架，用双拓扑自适应通信机制，实验显示其优于基线方法，平均准确率提高3.26%。


<details>
  <summary>Details</summary>
Motivation: 现有DFL优化策略未处理本地子图拓扑信息，FGL多为集中式未利用去中心化优势。

Method: 提出DFed - SST框架，采用双拓扑自适应通信机制，利用本地子图拓扑特征动态构建和优化客户端间通信拓扑。

Result: 在八个真实数据集上实验，DFed - SST平均准确率比基线方法提高3.26%。

Conclusion: DFed - SST框架在处理图数据的去中心化联邦学习中表现优越，能有效应对异质性问题。

Abstract: Decentralized Federated Learning (DFL) has emerged as a robust distributed
paradigm that circumvents the single-point-of-failure and communication
bottleneck risks of centralized architectures. However, a significant challenge
arises as existing DFL optimization strategies, primarily designed for tasks
such as computer vision, fail to address the unique topological information
inherent in the local subgraph. Notably, while Federated Graph Learning (FGL)
is tailored for graph data, it is predominantly implemented in a centralized
server-client model, failing to leverage the benefits of decentralization.To
bridge this gap, we propose DFed-SST, a decentralized federated graph learning
framework with adaptive communication. The core of our method is a
dual-topology adaptive communication mechanism that leverages the unique
topological features of each client's local subgraph to dynamically construct
and optimize the inter-client communication topology. This allows our framework
to guide model aggregation efficiently in the face of heterogeneity. Extensive
experiments on eight real-world datasets consistently demonstrate the
superiority of DFed-SST, achieving 3.26% improvement in average accuracy over
baseline methods.

</details>


### [87] [SeamlessFlow: A Trainer Agent Isolation RL Framework Achieving Bubble-Free Pipelines via Tag Scheduling](https://arxiv.org/abs/2508.11553)
*Jinghui Wang,Shaojie Wang,Yinghan Cui,Xuxing Chen,Chao Wang,Xiaojiang Zhang,Minglei Zhang,Jiarong Zhang,Wenhao Zhuang,Yuchen Cao,Wankang Bao,Haimo Li,Zheng Lin,Huiming Wang,Haoyang Huang,Zongxian Feng,Zizheng Zhan,Ken Deng,Wen Xiang,Huaixi Tang,Kun Wu,Mengtong Li,Mengfei Xie,Junyi Peng,Haotian Zhang,Bin Chen,Bing Yu*

Main category: cs.LG

TL;DR: 介绍SeamlessFlow，一种基于服务器的强化学习框架，解决工业规模强化学习的两个核心挑战，结合创新设计实现稳定高性能。


<details>
  <summary>Details</summary>
Motivation: 解决工业规模强化学习中，将RL训练与复杂执行流程解耦，以及在保证稳定性和可扩展性的前提下最大化GPU利用率的问题。

Method: 引入数据平面解耦RL训练器与复杂代理实现；提出标签驱动调度范式，抽象硬件资源；引入时空复用管道动态分配空闲节点。

Result: SeamlessFlow结合创新设计，实现了稳定性和高性能。

Conclusion: SeamlessFlow适用于多智能体、长视野等复杂强化学习任务。

Abstract: We introduce SeamlessFlow, a server based reinforcement learning (RL)
framework that addresses two core challenges in industrial scale RL: (1)
decoupling RL training from the complex execution flow of agents; (2)
maximizing GPU utilization with minimal idle time while preserving the
stability and scalability required for large-scale deployments. First,
SeamlessFlow introduces a data plane that decouples the RL trainer from
diverse, complex agent implementations while sustaining high throughput. A
central trajectory manager maintains complete interaction histories and
supports partial rollout, allowing rollout to pause for weight updates and
resume seamlessly, keeping agents unaware of service interruptions. Second, we
propose a tag driven scheduling paradigm that abstracts hardware into
capability tagged resources, unifying colocated and disaggregated
architectures. Based on this, SeamlessFlow introduces a spatiotemporal
multiplexing pipeline that dynamically reassigns idle training nodes to rollout
in a train rollout separated setup, eliminating pipeline bubbles and fully
exploiting heterogeneous cluster resources. By combining these innovations,
SeamlessFlow delivers both stability and high performance, making it well
suited for multi agent, long horizon, and other complex RL tasks.

</details>


### [88] [Optimal CO2 storage management considering safety constraints in multi-stakeholder multi-site CCS projects: a game theoretic perspective](https://arxiv.org/abs/2508.11618)
*Jungang Chen,Seyyed A. Hosseini*

Main category: cs.LG

TL;DR: 本文提出基于马尔可夫博弈的范式，以多智能体强化学习解决多利益相关者多站点的二氧化碳封存管理问题，结果证明该框架有效。


<details>
  <summary>Details</summary>
Motivation: CCS项目涉及多利益相关者，地质关联与过往设施利用使单边优化复杂不现实，需确定利益最大化方式。

Method: 将多利益相关者多站点问题构建为带安全约束的多智能体强化学习问题，用基于Embed - to - Control (E2C)框架的代理模型降低计算成本。

Result: 所提框架在涉及多利益相关者的二氧化碳封存最优管理中有效。

Conclusion: 基于马尔可夫博弈的范式能有效解决多利益相关者参与的二氧化碳封存管理问题。

Abstract: Carbon capture and storage (CCS) projects typically involve a diverse array
of stakeholders or players from public, private, and regulatory sectors, each
with different objectives and responsibilities. Given the complexity, scale,
and long-term nature of CCS operations, determining whether individual
stakeholders can independently maximize their interests or whether
collaborative coalition agreements are needed remains a central question for
effective CCS project planning and management. CCS projects are often
implemented in geologically connected sites, where shared geological features
such as pressure space and reservoir pore capacity can lead to competitive
behavior among stakeholders. Furthermore, CO2 storage sites are often located
in geologically mature basins that previously served as sites for hydrocarbon
extraction or wastewater disposal in order to leverage existing
infrastructures, which makes unilateral optimization even more complicated and
unrealistic.
  In this work, we propose a paradigm based on Markov games to quantitatively
investigate how different coalition structures affect the goals of
stakeholders. We frame this multi-stakeholder multi-site problem as a
multi-agent reinforcement learning problem with safety constraints. Our
approach enables agents to learn optimal strategies while compliant with safety
regulations. We present an example where multiple operators are injecting CO2
into their respective project areas in a geologically connected basin. To
address the high computational cost of repeated simulations of high-fidelity
models, a previously developed surrogate model based on the Embed-to-Control
(E2C) framework is employed. Our results demonstrate the effectiveness of the
proposed framework in addressing optimal management of CO2 storage when
multiple stakeholders with various objectives and goals are involved.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [89] [SDSNN: A Single-Timestep Spiking Neural Network with Self-Dropping Neuron and Bayesian Optimization](https://arxiv.org/abs/2508.10913)
*Changqing Xu,Buxuan Song,Yi Liu,Xinfang Liao,Wenbin Zheng,Yintang Yang*

Main category: cs.NE

TL;DR: 提出单步长SNN，优化参数并设计自丢弃神经元机制，实验表明在多数据集上提升精度并降低能耗


<details>
  <summary>Details</summary>
Motivation: 多步长SNN推理延迟和能耗高，限制在边缘计算场景应用

Method: 提出单步长SNN，设计自丢弃神经元机制，用贝叶斯优化搜索时间参数

Result: 在Fashion - MNIST、CIFAR - 10和CIFAR - 100数据集上，单步长实现高分类精度，能耗分别降低56%、21%和22%

Conclusion: 单步长SNN可提升精度并降低能耗，有更好应用潜力

Abstract: Spiking Neural Networks (SNNs), as an emerging biologically inspired
computational model, demonstrate significant energy efficiency advantages due
to their event-driven information processing mechanism. Compared to traditional
Artificial Neural Networks (ANNs), SNNs transmit information through discrete
spike signals, which substantially reduces computational energy consumption
through their sparse encoding approach. However, the multi-timestep computation
model significantly increases inference latency and energy, limiting the
applicability of SNNs in edge computing scenarios. We propose a single-timestep
SNN, which enhances accuracy and reduces computational energy consumption in a
single timestep by optimizing spike generation and temporal parameters. We
design a Self-Dropping Neuron mechanism, which enhances information-carrying
capacity through dynamic threshold adjustment and selective spike suppression.
Furthermore, we employ Bayesian optimization to globally search for time
parameters and obtain an efficient inference mode with a single time step.
Experimental results on the Fashion-MNIST, CIFAR-10, and CIFAR-100 datasets
demonstrate that, compared to traditional multi-timestep SNNs employing the
Leaky Integrate-and-Fire (LIF) model, our method achieves classification
accuracies of 93.72%, 92.20%, and 69.45%, respectively, using only
single-timestep spikes, while maintaining comparable or even superior accuracy.
Additionally, it reduces energy consumption by 56%, 21%, and 22%, respectively.

</details>


### [90] [Insect-Wing Structured Microfluidic System for Reservoir Computing](https://arxiv.org/abs/2508.10915)
*Jacob Clouse,Thomas Ramsey,Samitha Somathilaka,Nicholas Kleinsasser,Sangjin Ryu,Sasitharan Balasubramaniam*

Main category: cs.NE

TL;DR: 研究基于蜻蜓翅膀启发的微流控芯片混合储层计算系统，评估性能，结果显示分类准确率达91%，证明微流控储层计算可行。


<details>
  <summary>Details</summary>
Motivation: 随着高效自适应计算需求增长，电子设计有局限，微流控平台为不适合电子设备的环境提供低功耗、高弹性计算基础。

Method: 构建基于蜻蜓翅膀启发的微流控芯片混合储层计算系统，通过三个染料入口通道和三个检测区域将离散空间模式转换为动态颜色输出信号，经修改后传至可训练读出层进行模式分类，结合原始和合成输出评估性能。

Result: 系统在低分辨率和有限训练数据下分类准确率达91%。

Conclusion: 微流控储层计算是可行的。

Abstract: As the demand for more efficient and adaptive computing grows,
nature-inspired architectures offer promising alternatives to conventional
electronic designs. Microfluidic platforms, drawing on biological forms and
fluid dynamics, present a compelling foundation for low-power, high-resilience
computing in environments where electronics are unsuitable. This study explores
a hybrid reservoir computing system based on a dragonfly-wing inspired
microfluidic chip, which encodes temporal input patterns as fluid interactions
within the micro channel network.
  The system operates with three dye-based inlet channels and three
camera-monitored detection areas, transforming discrete spatial patterns into
dynamic color output signals. These reservoir output signals are then modified
and passed to a simple and trainable readout layer for pattern classification.
Using a combination of raw reservoir outputs and synthetically generated
outputs, we evaluated system performance, system clarity, and data efficiency.
The results demonstrate consistent classification accuracies up to $91\%$, even
with coarse resolution and limited training data, highlighting the viability of
the microfluidic reservoir computing.

</details>


### [91] [Use of a genetic algorithm to find solutions to introductory physics problems](https://arxiv.org/abs/2508.10920)
*Tom Bensky,Justin Kopcinski*

Main category: cs.NE

TL;DR: 本文展示遗传算法用于求解基础物理问题的逐步解决方案，可引导学生解决一维运动学问题并讨论可解释性。


<details>
  <summary>Details</summary>
Motivation: 探索用遗传算法为基础物理问题找到逐步解决方案，以解决找到能得出答案的方程序列这一任务。

Method: 使用遗传算法，通过最小化衡量方程组中未知量和已知量差异的适应度函数来找到合适的方程序列，向学生提问获取已知信息，用差异小的方程产生中间结果供差异大的方程使用。

Result: 该技术能引导学生解决任何涉及一维运动学的基础物理问题。

Conclusion: 文中未明确提及结论，但可推测此方法在解决一维运动学基础物理问题上有一定可行性和应用价值。

Abstract: In this work, we show how a genetic algorithm (GA) can be used to find
step-by-step solutions to introductory physics problems. Our perspective is
that the underlying task for this is one of finding a sequence of equations
that will lead to the needed answer. Here a GA is used to find an appropriate
equation sequence by minimizing a fitness function that measures the difference
between the number of unknowns versus knowns in a set of equations. Information
about knowns comes from the GA posing questions to the student about what
quantities exist in the text of their problem. The questions are generated from
enumerations pulled from the chromosomes that drive the GA. Equations with
smaller known vs. unknown differences are considered more fit and are used to
produce intermediate results that feed less fit equations. We show that this
technique can guide a student to an answer to any introductory physics problem
involving one-dimensional kinematics. Interpretability findings are discussed.

</details>


### [92] [SO-PIFRNN: Self-optimization physics-informed Fourier-features randomized neural network for solving partial differential equations](https://arxiv.org/abs/2508.10921)
*Jiale Linghu,Weifeng Gao,Hao Dong,Yufeng Nie*

Main category: cs.NE

TL;DR: 提出SO - PIFRNN框架，用双层优化架构，经多项创新显著提升PDE数值求解精度，实验验证其有效性和优势。


<details>
  <summary>Details</summary>
Motivation: 提升偏微分方程（PDEs）数值求解的精度。

Method: 采用双层优化架构，外层用MSC - PSO算法搜索最优超参数，内层用最小二乘法确定输出层权重；引入傅里叶基函数激活机制，提出新的导数神经网络方法，设计混合优化策略的MSC - PSO算法。

Result: 通过对多尺度、高阶、高维、非线性等方程的数值实验，验证了SO - PIFRNN具有优越的逼近精度和频率捕获能力。

Conclusion: SO - PIFRNN框架能有效提升PDEs数值求解精度，具有良好性能。

Abstract: This study proposes a self-optimization physics-informed Fourier-features
randomized neural network (SO-PIFRNN) framework, which significantly improves
the numerical solving accuracy of PDEs through hyperparameter optimization
mechanism. The framework employs a bi-level optimization architecture: the
outer-level optimization utilizes a multi-strategy collaborated particle swarm
optimization (MSC-PSO) algorithm to search for optimal hyperparameters of
physics-informed Fourier-features randomized neural network, while the
inner-level optimization determines the output layer weights of the neural
network via the least squares method. The core innovation of this study is
embodied in the following three aspects: First, the Fourier basis function
activation mechanism is introduced in the hidden layer of neural network, which
significantly enhances the ability of the network to capture multi-frequency
components of the solution. Secondly, a novel derivative neural network method
is proposed, which improves the calculation accuracy and efficiency of PIFRNN
method. Finally, the MSC-PSO algorithm of the hybrid optimization strategy is
designed to improve the global search ability and convergence accuracy through
the synergistic effect of dynamic parameter adjustment, elitist and mutation
strategies. Through a series of numerical experiments, including multiscale
equations in complex regions, high-order equations, high-dimensional equations
and nonlinear equations, the validity of SO-PIFRNN is verified. The
experimental results affirm that SO-PIFRNN exhibits superior approximation
accuracy and frequency capture capability.

</details>


### [93] [Allee Synaptic Plasticity and Memory](https://arxiv.org/abs/2508.10929)
*Eddy Kwessi*

Main category: cs.NE

TL;DR: 论文研究基于Allee的非线性可塑性模型，分析其在记忆保留和模式检索中的表现，扩展模型以解决时间限制问题，为神经适应建模提供框架。


<details>
  <summary>Details</summary>
Motivation: 现有模型在解决噪声敏感性和突触权重无界增长方面存在不足，需要更好的模型。

Method: 研究基于Allee的非线性可塑性模型，分析其性能，并通过集成时间依赖动态扩展模型。

Result: 该模型比经典模型有更高的容量和可靠性，扩展后在动态环境中检索准确性和恢复能力提高。

Conclusion: 此工作连接理论与实践，为神经适应建模提供了强大框架，有助于人工智能和神经科学发展。

Abstract: Neural plasticity is fundamental to memory storage and retrieval in
biological systems, yet existing models often fall short in addressing noise
sensitivity and unbounded synaptic weight growth. This paper investigates the
Allee-based nonlinear plasticity model, emphasizing its biologically inspired
weight stabilization mechanisms, enhanced noise robustness, and critical
thresholds for synaptic regulation. We analyze its performance in memory
retention and pattern retrieval, demonstrating increased capacity and
reliability compared to classical models like Hebbian and Oja's rules. To
address temporal limitations, we extend the model by integrating time-dependent
dynamics, including eligibility traces and oscillatory inputs, resulting in
improved retrieval accuracy and resilience in dynamic environments. This work
bridges theoretical insights with practical implications, offering a robust
framework for modeling neural adaptation and informing advances in artificial
intelligence and neuroscience.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [94] [Inference performance evaluation for LLMs on edge devices with a novel benchmarking framework and metric](https://arxiv.org/abs/2508.11269)
*Hao Chen,Cong Tian,Zixuan He,Bin Yu,Yepang Liu,Jialun Cao*

Main category: cs.PF

TL;DR: 本文介绍边缘大语言模型推理基准工具ELIB，提出MBU指标优化内存使用，在三个边缘平台用五个量化模型进行基准测试并分析结果。


<details>
  <summary>Details</summary>
Motivation: 边缘计算对大语言模型推理服务有数据隐私需求，但不同边缘平台硬件特性不同，部署和基准测试大语言模型有挑战。

Method: 引入ELIB工具评估不同边缘平台的大语言模型推理性能，提出MBU指标优化内存使用，在三个边缘平台用五个量化模型进行基准测试。

Result: 通过基准测试得到结果，并分析出优化MBU的关键因素、约束和不可预测性。

Conclusion: 分析结果可指导在更多边缘平台部署大语言模型。

Abstract: With the significant success achieved by large language models (LLMs) like
LLaMA, edge computing-based LLM inference services for mobile and PC are in
high demand for data privacy. However, different edge platforms have different
hardware characteristics and the large demand for memory capacity and bandwidth
makes it very challenging to deploy and benchmark LLMs on edge devices. In this
paper, we introduce a benchmarking tool named ELIB (edge LLM inference
benchmarking) to evaluate LLM inference performance of different edge
platforms, and propose a novel metric named MBU to indicate the percentage of
the theoretically efficient use of available memory bandwidth for a specific
model running on edge hardware to optimize memory usage. We deploy ELIB on
three edge platforms and benchmark using five quantized models to optimize MBU
in combination with other metrics such as FLOPS, throughput, latency and
accuracy. And we analyze the results to derive the key factors, constraints,
unpredictability in optimizing MBU that can guide deploying LLMs on more edge
platforms.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [95] [The Impact of Large Language Models (LLMs) on Code Review Process](https://arxiv.org/abs/2508.11034)
*Antonio Collante,Samuel Abedu,SayedHassan Khatoonabadi,Ahmad Abdellatif,Ebube Alor,Emad Shihab*

Main category: cs.SE

TL;DR: 研究GPT对GitHub PR工作流的影响，发现早期采用GPT可提升PR流程效率，节省时间，明确开发者使用场景。


<details>
  <summary>Details</summary>
Motivation: 已有研究未充分探索LLM辅助对代码审查流程分阶段的效率影响，本研究旨在探究GPT对GitHub PR工作流的作用。

Method: 整理25473个PR的数据集，用半自动化启发式方法识别GPT辅助的PR，运用统计建模评估差异。

Result: 早期采用GPT可大幅提升PR流程效果，节省各阶段时间，GPT辅助的PR中位解决时间减少超60%，审查时间减少33%，接受前等待时间减少87%，开发者主要用GPT进行代码优化等。

Conclusion: 研究揭示了GPT模型对代码审查过程的影响，为软件团队提升工作流和促进协作提供了可行见解。

Abstract: Large language models (LLMs) have recently gained prominence in the field of
software development, significantly boosting productivity and simplifying
teamwork. Although prior studies have examined task-specific applications, the
phase-specific effects of LLM assistance on the efficiency of code review
processes remain underexplored. This research investigates the effect of GPT on
GitHub pull request (PR) workflows, with a focus on reducing resolution time,
optimizing phase-specific performance, and assisting developers. We curated a
dataset of 25,473 PRs from 9,254 GitHub projects and identified GPT-assisted
PRs using a semi-automated heuristic approach that combines keyword-based
detection, regular expression filtering, and manual verification until
achieving 95% labeling accuracy. We then applied statistical modeling,
including multiple linear regression and Mann-Whitney U test, to evaluate
differences between GPT-assisted and non-assisted PRs, both at the overall
resolution level and across distinct review phases. Our research has revealed
that early adoption of GPT can substantially boost the effectiveness of the PR
process, leading to considerable time savings at various stages. Our findings
suggest that GPT-assisted PRs reduced median resolution time by more than 60%
(9 hours compared to 23 hours for non-assisted PRs). We discovered that
utilizing GPT can reduce the review time by 33% and the waiting time before
acceptance by 87%. Analyzing a sample dataset of 300 GPT-assisted PRs, we
discovered that developers predominantly use GPT for code optimization (60%),
bug fixing (26%), and documentation updates (12%). This research sheds light on
the impact of the GPT model on the code review process, offering actionable
insights for software teams seeking to enhance workflows and promote seamless
collaboration.

</details>


### [96] [Diffusion is a code repair operator and generator](https://arxiv.org/abs/2508.11110)
*Mukul Singh,Gust Verbruggen,Vu Le,Sumit Gulwani*

Main category: cs.SE

TL;DR: 探索利用代码扩散模型进行代码最后一公里修复，提出两种应用并在三个领域实验。


<details>
  <summary>Details</summary>
Motivation: 利用代码扩散模型后期代码片段离散表示与最后一公里修复的相似性，解决最后一公里修复问题。

Method: 一是对破损代码添加噪声并恢复扩散过程；二是从扩散过程中采样中间程序和最终程序生成训练数据。

Result: 在Python、Excel和PowerShell三个领域进行实验。

Conclusion: 未明确提及，但暗示了利用代码扩散模型进行最后一公里修复有一定可行性。

Abstract: Code diffusion models generate code by iteratively removing noise from the
latent representation of a code snippet. During later steps of the diffusion
process, when the code snippet has almost converged, differences between
discrete representations of these snippets look like last-mile repairs applied
to broken or incomplete code. We evaluate the extent to which this resemblance
can be exploited to leverage pre-trained code diffusion models for the problem
of last-mile repair by considering two applications with significant potential.
First, we can leverage the diffusion model for last-mile repair by adding noise
to a broken code snippet and resuming the diffusion process. Second, we can
leverage the diffusion model to generate arbitrary amount of training data for
last-mile repair tasks (that are computationally more efficient) by sampling an
intermediate program (input) and the final program (output) from the diffusion
process. We perform experiments on 3 domains (Python, Excel and PowerShell) to
evaluate applications, as well as analyze properties.

</details>


### [97] [AI Agentic Programming: A Survey of Techniques, Challenges, and Opportunities](https://arxiv.org/abs/2508.11126)
*Huanting Wang,Jingzhi Gong,Huawei Zhang,Zheng Wang*

Main category: cs.SE

TL;DR: 本文全面综述AI智能体编程，介绍分类、核心技术、评估方法，指出挑战与机遇，为下一代智能可信AI编码智能体研究奠基。


<details>
  <summary>Details</summary>
Motivation: 随着AI智能体编程领域快速发展，需界定范围、巩固技术基础和识别研究挑战。

Method: 引入智能体行为和系统架构分类法，研究规划、内存和上下文管理等核心技术，分析现有基准和评估方法。

Result: 识别出处理长上下文有局限、跨任务缺乏持久内存、安全及与用户意图对齐等关键挑战，讨论了提升系统可靠性等方面的机遇。

Conclusion: 通过综合进展和规划未来方向，为构建下一代AI编码智能体的研发提供基础。

Abstract: AI agentic programming is an emerging paradigm in which large language models
(LLMs) autonomously plan, execute, and interact with external tools like
compilers, debuggers, and version control systems to iteratively perform
complex software development tasks. Unlike conventional code generation tools,
agentic systems are capable of decomposing high-level goals, coordinating
multi-step processes, and adapting their behavior based on intermediate
feedback. These capabilities are transforming the software development
practice. As this emerging field evolves rapidly, there is a need to define its
scope, consolidate its technical foundations, and identify open research
challenges. This survey provides a comprehensive and timely review of AI
agentic programming. We introduce a taxonomy of agent behaviors and system
architectures, and examine core techniques including planning, memory and
context management, tool integration, and execution monitoring. We also analyze
existing benchmarks and evaluation methodologies used to assess coding agent
performance. Our study identifies several key challenges, including limitations
in handling long context, a lack of persistent memory across tasks, and
concerns around safety, alignment with user intent, and collaboration with
human developers. We discuss emerging opportunities to improve the reliability,
adaptability, and transparency of agentic systems. By synthesizing recent
advances and outlining future directions, this survey aims to provide a
foundation for research and development in building the next generation of
intelligent and trustworthy AI coding agents.

</details>


### [98] [From Feedback to Failure: Automated Android Performance Issue Reproduction](https://arxiv.org/abs/2508.11147)
*Zhengquan Li,Zhenhao Li,Zishuo Ding*

Main category: cs.SE

TL;DR: 提出RevPerf工具利用应用评论重现移动应用性能问题，实验成功率达70%。


<details>
  <summary>Details</summary>
Motivation: 移动应用性能对用户体验重要，但开发环境中性能问题难检测，需解决此局限。

Method: 利用Google Play应用评论，通过相关评论和提示工程丰富评论信息，用执行代理生成并执行命令重现问题，结合多方面检测方法识别问题。

Result: 在自建并手动验证的数据集上，所提框架重现性能问题成功率达70%。

Conclusion: RevPerf工具能有效重现移动应用性能问题。

Abstract: Mobile application performance is a vital factor for user experience. Yet,
performance issues are notoriously difficult to detect within development
environments, where their manifestations are often less conspicuous and
diagnosis proves more challenging. To address this limitation, we propose
RevPerf, an advanced performance issue reproduction tool that leverages app
reviews from Google Play to acquire pertinent information. RevPerf employs
relevant reviews and prompt engineering to enrich the original review with
performance issue details. An execution agent is then employed to generate and
execute commands to reproduce the issue. After executing all necessary steps,
the system incorporates multifaceted detection methods to identify performance
issues by monitoring Android logs, GUI changes, and system resource utilization
during the reproduction process. Experimental results demonstrate that our
proposed framework achieves a 70\% success rate in reproducing performance
issues on the dataset we constructed and manually validated.

</details>


### [99] [PTMPicker: Facilitating Efficient Pretrained Model Selection for Application Developers](https://arxiv.org/abs/2508.11179)
*Pei Liu,Terry Zhuo,Jiawei Deng,Zhenchang Xing,Qinghua Lu,Xiaoning Du,Hongyu Zhan*

Main category: cs.SE

TL;DR: 现有基于关键词的预训练模型搜索方法有局限，提出PTMPicker工具准确识别合适的预训练模型，实验表明其能有效帮助用户识别模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于关键词的预训练模型搜索方法难以充分捕捉用户意图，在考虑偏见缓解、硬件要求或许可证合规等因素时，难以确定合适的模型。

Method: 定义预训练模型的结构化模板，用统一格式表示候选模型和用户意图特征；计算功能相关属性的嵌入相似度，用精心设计的提示评估特殊约束；从Hugging Face抓取模型作为候选，提取描述信息；合成模型搜索请求。

Result: 在整理的预训练模型数据集和合成的模型搜索请求上进行实验，85%的采样请求能在前10名候选中成功找到合适的预训练模型。

Conclusion: PTMPicker可以有效帮助用户识别合适的预训练模型。

Abstract: The rapid emergence of pretrained models (PTMs) has attracted significant
attention from both Deep Learning (DL) researchers and downstream application
developers. However, selecting appropriate PTMs remains challenging because
existing methods typically rely on keyword-based searches in which the keywords
are often derived directly from function descriptions. This often fails to
fully capture user intent and makes it difficult to identify suitable models
when developers also consider factors such as bias mitigation, hardware
requirements, or license compliance. To address the limitations of
keyword-based model search, we propose PTMPicker to accurately identify
suitable PTMs. We first define a structured template composed of common and
essential attributes for PTMs and then PTMPicker represents both candidate
models and user-intended features (i.e., model search requests) in this unified
format. To determine whether candidate models satisfy user requirements, it
computes embedding similarities for function-related attributes and uses
well-crafted prompts to evaluate special constraints such as license compliance
and hardware requirements. We scraped a total of 543,949 pretrained models from
Hugging Face to prepare valid candidates for selection. PTMPicker then
represented them in the predefined structured format by extracting their
associated descriptions. Guided by the extracted metadata, we synthesized a
total of 15,207 model search requests with carefully designed prompts, as no
such search requests are readily available. Experiments on the curated PTM
dataset and the synthesized model search requests show that PTMPicker can help
users effectively identify models,with 85% of the sampled requests successfully
locating appropriate PTMs within the top-10 ranked candidates.

</details>


### [100] [ORFuzz: Fuzzing the "Other Side" of LLM Safety -- Testing Over-Refusal](https://arxiv.org/abs/2508.11222)
*Haonan Zhang,Dongxia Wang,Yi Liu,Kexin Chen,Jiashui Wang,Xinlei Ying,Long Liu,Wenhai Wang*

Main category: cs.SE

TL;DR: 论文提出首个进化测试框架ORFuzz检测大语言模型过度拒绝问题，能生成多样有效测试用例，还构建了ORFuzzSet基准，提升测试效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在过度拒绝的功能缺陷，当前测试方法不足，需要新的测试框架。

Method: ORFuzz集成安全类别感知种子选择、使用推理大语言模型的自适应变异优化器、人类对齐判断模型OR - Judge三个核心组件。

Result: ORFuzz生成过度拒绝实例的比率超领先基线一倍多，ORFuzzSet在10个不同大语言模型上实现63.56%的平均过度拒绝率，优于现有数据集。

Conclusion: ORFuzz和ORFuzzSet提供了强大的自动化测试框架和有价值的社区资源，有助于开发更可靠和可信的大语言模型软件系统。

Abstract: Large Language Models (LLMs) increasingly exhibit over-refusal - erroneously
rejecting benign queries due to overly conservative safety measures - a
critical functional flaw that undermines their reliability and usability.
Current methods for testing this behavior are demonstrably inadequate,
suffering from flawed benchmarks and limited test generation capabilities, as
highlighted by our empirical user study. To the best of our knowledge, this
paper introduces the first evolutionary testing framework, ORFuzz, for the
systematic detection and analysis of LLM over-refusals. ORFuzz uniquely
integrates three core components: (1) safety category-aware seed selection for
comprehensive test coverage, (2) adaptive mutator optimization using reasoning
LLMs to generate effective test cases, and (3) OR-Judge, a human-aligned judge
model validated to accurately reflect user perception of toxicity and refusal.
Our extensive evaluations demonstrate that ORFuzz generates diverse, validated
over-refusal instances at a rate (6.98% average) more than double that of
leading baselines, effectively uncovering vulnerabilities. Furthermore,
ORFuzz's outputs form the basis of ORFuzzSet, a new benchmark of 1,855 highly
transferable test cases that achieves a superior 63.56% average over-refusal
rate across 10 diverse LLMs, significantly outperforming existing datasets.
ORFuzz and ORFuzzSet provide a robust automated testing framework and a
valuable community resource, paving the way for developing more reliable and
trustworthy LLM-based software systems.

</details>


### [101] [Hallucination in LLM-Based Code Generation: An Automotive Case Study](https://arxiv.org/abs/2508.11257)
*Marc Pavel,Nenad Petrovic,Lukasz Mazur,Vahid Zolfaghari,Fengjunjie Pan,Alois Knoll*

Main category: cs.SE

TL;DR: 研究大语言模型在代码生成中的幻觉现象，以汽车领域为例评估多个模型，发现现有模型存在问题，需有效缓解技术。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在代码生成有潜力，但因幻觉现象实际应用受限，聚焦汽车领域研究该现象。

Method: 进行案例研究，评估多个代码大语言模型在三种不同提示复杂度下的表现。

Result: 评估显示GPT - 4.1、Codex和GPT - 4o等模型存在高频语法违规等问题，仅GPT - 4.1和GPT - 4o在最丰富提示下能产生正确解决方案，简单提示策略难出成果。

Conclusion: 需要有效缓解技术来确保大语言模型生成代码在安全关键领域（如汽车软件系统）的安全可靠使用。

Abstract: Large Language Models (LLMs) have shown significant potential in automating
code generation tasks offering new opportunities across software engineering
domains. However, their practical application remains limited due to
hallucinations - outputs that appear plausible but are factually incorrect,
unverifiable or nonsensical. This paper investigates hallucination phenomena in
the context of code generation with a specific focus on the automotive domain.
A case study is presented that evaluates multiple code LLMs for three different
prompting complexities ranging from a minimal one-liner prompt to a prompt with
Covesa Vehicle Signal Specifications (VSS) as additional context and finally to
a prompt with an additional code skeleton. The evaluation reveals a high
frequency of syntax violations, invalid reference errors and API knowledge
conflicts in state-of-the-art models GPT-4.1, Codex and GPT-4o. Among the
evaluated models, only GPT-4.1 and GPT-4o were able to produce a correct
solution when given the most context-rich prompt. Simpler prompting strategies
failed to yield a working result, even after multiple refinement iterations.
These findings highlight the need for effective mitigation techniques to ensure
the safe and reliable use of LLM generated code, especially in safety-critical
domains such as automotive software systems.

</details>


### [102] [Defects4Log: Benchmarking LLMs for Logging Code Defect Detection and Reasoning](https://arxiv.org/abs/2508.11305)
*Xin Wang,Zhenhao Li,Zishuo Ding*

Main category: cs.SE

TL;DR: 文章推导了日志代码缺陷的综合分类法，构建数据集，提出框架评估大语言模型检测日志代码缺陷能力，发现结合知识可提升检测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有日志缺陷研究范围窄且缺乏系统全面分析，大语言模型检测日志代码缺陷潜力待挖掘。

Method: 推导日志代码缺陷分类法，构建基准数据集，提出自动化框架评估大语言模型能力。

Result: 大语言模型仅基于源代码难以准确检测和推理日志代码缺陷，结合知识可使检测准确率提高10.9%。

Conclusion: 研究为从业者避免常见缺陷模式提供指导，为改进基于大语言模型的日志代码缺陷检测推理奠定基础。

Abstract: Logging code is written by developers to capture system runtime behavior and
plays a vital role in debugging, performance analysis, and system monitoring.
However, defects in logging code can undermine the usefulness of logs and lead
to misinterpretations. Although prior work has identified several logging
defect patterns and provided valuable insights into logging practices, these
studies often focus on a narrow range of defect patterns derived from limited
sources (e.g., commit histories) and lack a systematic and comprehensive
analysis. Moreover, large language models (LLMs) have demonstrated promising
generalization and reasoning capabilities across a variety of code-related
tasks, yet their potential for detecting logging code defects remains largely
unexplored.
  In this paper, we derive a comprehensive taxonomy of logging code defects,
which encompasses seven logging code defect patterns with 14 detailed
scenarios. We further construct a benchmark dataset, \dataset, consisting of
164 developer-verified real-world logging defects. Then we propose an automated
framework that leverages various prompting strategies and contextual
information to evaluate LLMs' capability in detecting and reasoning logging
code defects. Experimental results reveal that LLMs generally struggle to
accurately detect and reason logging code defects based on the source code
only. However, incorporating proper knowledge (e.g., detailed scenarios of
defect patterns) can lead to 10.9\% improvement in detection accuracy. Overall,
our findings provide actionable guidance for practitioners to avoid common
defect patterns and establish a foundation for improving LLM-based reasoning in
logging code defect detection.

</details>


### [103] [TRACY: Benchmarking Execution Efficiency of LLM-Based Code Translation](https://arxiv.org/abs/2508.11468)
*Zhihao Gong,Zeyu Sun,Dong Huang,Qingyuan Liang,Jie M. Zhang,Dan Hao*

Main category: cs.SE

TL;DR: 提出TRACY基准评估大模型代码翻译执行效率，评估26个模型发现即使顶级模型也难稳定产出高效代码，强调需联合优化正确性与效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型提升代码翻译正确性，但执行效率被忽视，需评估模型代码翻译执行效率。

Method: 通过大模型驱动的两阶段管道构建TRACY基准，包括生成压力测试和任务剪枝。

Result: 评估26个模型发现顶级模型在效率方面表现不佳，算法缺陷和资源处理不当影响大。

Conclusion: 未来基于大模型的代码翻译需联合优化正确性和效率。

Abstract: Automatic code translation is a fundamental task in modern software
development. While the advent of Large Language Models (LLMs) has significantly
improved the correctness of code translation, the critical dimension of
execution efficiency remains overlooked. To address this gap, we introduce
TRACY, the first comprehensive benchmark designed to evaluate the execution
efficiency of LLM-translated code. TRACY is constructed through an LLM-driven
two-stage pipeline: an initial stage generates a suite of stress tests to
amplify performance differences, followed by an efficiency-oriented task
pruning stage that isolates the efficiency-distinguishing tasks. The resulting
benchmark comprises 1,011 code translation tasks across C++, Java, and Python,
each accompanied by an average of 22.1 verified reference translations and 10
computationally demanding tests. Our extensive evaluation of 26 representative
LLMs reveals that even top-tier LLMs struggle to consistently produce efficient
code translations. For instance, Claude-4-think, the leading model for
correctness, ranks eighth overall when time efficiency is taken into account,
surpassed by several smaller open-source models. We further pinpoint that
algorithmic flaws and improper resource handling are the most detrimental,
causing a median time slowdown of 5.6$\times$ and memory increase of
12.0$\times$, respectively. Our work underscores the necessity of jointly
optimizing for correctness and efficiency in future LLM-based code translation.

</details>


### [104] [Temporal Network Analysis of Microservice Architectural Degradation](https://arxiv.org/abs/2508.11571)
*Alexander Bakhtin*

Main category: cs.SE

TL;DR: 本文探讨从微服务系统获取时态网络并进行分析的挑战，且可用的最完整时态网络数据有限。


<details>
  <summary>Details</summary>
Motivation: 利用网络科学方法研究微服务架构，尤其是时态网络分析方法。

Method: 使用时态网络分析方法研究微服务系统中的网络。

Result: 能获取的最完整时态网络包含7个时间实例和42个微服务，限制了潜在分析。

Conclusion: 在从微服务系统获取时态网络并分析方面存在挑战。

Abstract: Microservice architecture can be modeled as a network of microservices making
calls to each other, commonly known as the service dependency graph. Network
Science can provide methods to study such networks. In particular, temporal
network analysis is a branch of Network Science that analyzes networks evolving
with time. In microservice systems, temporal networks can arise if we examine
the architecture of the system across releases or monitor a deployed system
using tracing.
  In this research summary paper, I discuss the challenges in obtaining
temporal networks from microservice systems and analyzing them with the
temporal network methods. In particular, the most complete temporal network
that we could obtain contains 7 time instances and 42 microservices, which
limits the potential analysis that could be applied.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [105] [AlphaAgents: Large Language Model based Multi-Agents for Equity Portfolio Constructions](https://arxiv.org/abs/2508.11152)
*Tianjiao Zhao,Jingrao Lyu,Stokes Jones,Harrison Garber,Stefano Pasquali,Dhagash Mehta*

Main category: q-fin.ST

TL;DR: 研究基于角色的多智能体系统在股票选择中的应用，分析其选股表现及框架优缺点。


<details>
  <summary>Details</summary>
Motivation: 人工智能领域发展，多智能体协作有前景，研究其在股票选择和投资组合管理中的应用。

Method: 由专业智能体团队进行综合分析，在不同风险容忍度下与既定基准对比选股表现。

Result: 未提及具体结果。

Conclusion: 未提及具体结论，但会探讨多智能体框架在股票分析中的优缺点、实际效果和实施挑战。

Abstract: The field of artificial intelligence (AI) agents is evolving rapidly, driven
by the capabilities of Large Language Models (LLMs) to autonomously perform and
refine tasks with human-like efficiency and adaptability. In this context,
multi-agent collaboration has emerged as a promising approach, enabling
multiple AI agents to work together to solve complex challenges. This study
investigates the application of role-based multi-agent systems to support stock
selection in equity research and portfolio management. We present a
comprehensive analysis performed by a team of specialized agents and evaluate
their stock-picking performance against established benchmarks under varying
levels of risk tolerance. Furthermore, we examine the advantages and
limitations of employing multi-agent frameworks in equity analysis, offering
critical insights into their practical efficacy and implementation challenges.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [106] [Non-asymptotic convergence bound of conditional diffusion models](https://arxiv.org/abs/2508.10944)
*Mengze Li*

Main category: stat.ML

TL;DR: 研究条件扩散模型CARD，将预训练模型融入框架，理论推导相关方程，证明分布误差上界和收敛上界。


<details>
  <summary>Details</summary>
Motivation: 现有条件扩散模型缺乏非渐近性质阻碍理论研究，需进行改进。

Method: 聚焦分类和回归领域的条件扩散模型CARD，将预训练模型融入原框架，推导随机微分方程和广义形式，利用二阶Wasserstein距离证明误差上界。

Result: 推导出CARD的随机微分方程，建立广义形式，证明了原条件分布和生成条件分布的误差上界，以及真实值与网络估计值的收敛上界。

Conclusion: 为条件扩散模型CARD建立了坚实理论基础，推动了该领域理论研究。

Abstract: Learning and generating various types of data based on conditional diffusion
models has been a research hotspot in recent years. Although conditional
diffusion models have made considerable progress in improving acceleration
algorithms and enhancing generation quality, the lack of non-asymptotic
properties has hindered theoretical research. To address this gap, we focus on
a conditional diffusion model within the domains of classification and
regression (CARD), which aims to learn the original distribution with given
input x (denoted as Y|X). It innovatively integrates a pre-trained model
f_{\phi}(x) into the original diffusion model framework, allowing it to
precisely capture the original conditional distribution given f (expressed as
Y|f_{\phi}(x)). Remarkably, when f_{\phi}(x) performs satisfactorily,
Y|f_{\phi}(x) closely approximates Y|X. Theoretically, we deduce the stochastic
differential equations of CARD and establish its generalized form predicated on
the Fokker-Planck equation, thereby erecting a firm theoretical foundation for
analysis. Mainly under the Lipschitz assumptions, we utilize the second-order
Wasserstein distance to demonstrate the upper error bound between the original
and the generated conditional distributions. Additionally, by appending
assumptions such as light-tailedness to the original distribution, we derive
the convergence upper bound between the true value analogous to the score
function and the corresponding network-estimated value.

</details>


### [107] [Counterfactual Survival Q Learning for Longitudinal Randomized Trials via Buckley James Boosting](https://arxiv.org/abs/2508.11060)
*Jeongjin Lee,Jong-Min Kim*

Main category: stat.ML

TL;DR: 提出BJ Boost Q学习框架用于右删失生存数据下估计最优动态治疗方案，比Cox Q学习更优。


<details>
  <summary>Details</summary>
Motivation: 在右删失生存数据的纵向随机临床试验场景下，现有Cox Q学习依赖风险建模且可能存在误设偏差，需要更优方法估计最优动态治疗方案。

Method: 将加速失效时间模型与迭代提升技术（如逐分量最小二乘法和回归树）集成到反事实Q学习框架中，直接建模条件生存时间。

Result: 模拟研究和ACTG175 HIV试验分析表明，BJ Boost Q学习在治疗决策上准确性更高，尤其在多阶段场景。

Conclusion: BJ Boost Q学习避免了比例风险假设的限制，能无偏估计阶段特定Q函数，在标准因果假设下确保最优治疗方案可识别，提供了稳健灵活的估计。

Abstract: We propose a Buckley James (BJ) Boost Q learning framework for estimating
optimal dynamic treatment regimes under right censored survival data, tailored
for longitudinal randomized clinical trial settings. The method integrates
accelerated failure time models with iterative boosting techniques, including
componentwise least squares and regression trees, within a counterfactual Q
learning framework. By directly modeling conditional survival time, BJ Boost Q
learning avoids the restrictive proportional hazards assumption and enables
unbiased estimation of stage specific Q functions. Grounded in potential
outcomes, this framework ensures identifiability of the optimal treatment
regime under standard causal assumptions. Compared to Cox based Q learning,
which relies on hazard modeling and may suffer from bias under
misspecification, our approach provides robust and flexible estimation.
Simulation studies and analysis of the ACTG175 HIV trial demonstrate that BJ
Boost Q learning yields higher accuracy in treatment decision making,
especially in multistage settings where bias can accumulate.

</details>


### [108] [Uniform convergence for Gaussian kernel ridge regression](https://arxiv.org/abs/2508.11274)
*Paul Dommel,Rajmadan Lakshmanan*

Main category: stat.ML

TL;DR: 本文为固定超参数的高斯核岭回归（KRR）在一致范数和L²范数下建立了首个多项式收敛率。


<details>
  <summary>Details</summary>
Motivation: 填补高斯核KRR理论理解中收敛率的空白，增进对平滑核的理解。

Method: 未提及

Result: 在一致范数下得到收敛结果，填补理论空白；在L²范数下证明了多项式收敛率，此前类似情况只有亚多项式L²率。

Conclusion: 为在非参数回归中使用固定超参数的高斯KRR提供了新的理论依据。

Abstract: This paper establishes the first polynomial convergence rates for Gaussian
kernel ridge regression (KRR) with a fixed hyperparameter in both the uniform
and the $L^{2}$-norm. The uniform convergence result closes a gap in the
theoretical understanding of KRR with the Gaussian kernel, where no such rates
were previously known. In addition, we prove a polynomial $L^{2}$-convergence
rate in the case, where the Gaussian kernel's width parameter is fixed. This
also contributes to the broader understanding of smooth kernels, for which
previously only sub-polynomial $L^{2}$-rates were known in similar settings.
Together, these results provide new theoretical justification for the use of
Gaussian KRR with fixed hyperparameters in nonparametric regression.

</details>


### [109] [ADMIRE-BayesOpt: Accelerated Data MIxture RE-weighting for Language Models with Bayesian Optimization](https://arxiv.org/abs/2508.11551)
*Shengzhuang Chen,Xu Ouyang,Michael Arthur Leopold Pearce,Thomas Hartvigsen,Jonathan Richard Schwarz*

Main category: stat.ML

TL;DR: 本文将训练数据混合选择视为黑盒超参数优化问题，提出多保真度贝叶斯优化方法，在不同规模模型实验中取得强结果，还分享数据集降低研究成本。


<details>
  <summary>Details</summary>
Motivation: 确定大语言模型训练的最优数据混合是难题，目前缺乏可靠的基于学习的解决方案。

Method: 将数据混合学习视为顺序决策问题，采用多保真度贝叶斯优化方法平衡实验成本和模型拟合。

Result: 在不同规模模型的预训练和指令微调实验中相对多种基准取得强结果，最大实验确定最佳数据混合速度提升超500%，分享ADMIRE IFT Runs数据集。

Conclusion: 提出的方法有效，能提升确定最佳数据混合的效率，分享数据集有利于该领域研究。

Abstract: Determining the optimal data mixture for large language model training
remains a challenging problem with an outsized impact on performance. In
practice, language model developers continue to rely on heuristic exploration
since no learning-based approach has emerged as a reliable solution. In this
work, we propose to view the selection of training data mixtures as a black-box
hyperparameter optimization problem, for which Bayesian Optimization is a
well-established class of appropriate algorithms. Firstly, we cast data mixture
learning as a sequential decision-making problem, in which we aim to find a
suitable trade-off between the computational cost of training exploratory
(proxy-) models and final mixture performance. Secondly, we systematically
explore the properties of transferring mixtures learned at a small scale to
larger-scale experiments, providing insights and highlighting opportunities for
research at a modest scale. By proposing Multi-fidelity Bayesian Optimization
as a suitable method in this common scenario, we introduce a natural framework
to balance experiment cost with model fit, avoiding the risks of overfitting to
smaller scales while minimizing the number of experiments at high cost. We
present results for pre-training and instruction finetuning across models
ranging from 1 million to 7 billion parameters, varying from simple
architectures to state-of-the-art models and benchmarks spanning dozens of
datasets. We demonstrate consistently strong results relative to a wide range
of benchmarks, showingspeed-ups of over 500% in determining the best data
mixture on our largest experiments relative to recent baselines. In addition,
we broaden access to research by sharing ADMIRE IFT Runs, a dataset of 460 full
training & evaluation runs across various model sizes worth over 13,000 GPU
hours, greatly reducing the cost of conducting research in this area.

</details>


### [110] [Nonparametric learning of stochastic differential equations from sparse and noisy data](https://arxiv.org/abs/2508.11597)
*Arnab Ganguly,Riten Mitra,Jinpu Zhou*

Main category: stat.ML

TL;DR: 本文提出从稀疏、有噪声观测构建数据驱动随机微分方程模型的框架，通过EM - SMC - RKHS程序实现低数据条件下漂移函数准确估计，并经实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 传统参数方法需假设漂移已知函数形式，本文旨在不做强结构假设下直接从数据学习整个漂移函数，适用于系统动力学部分已知或高度复杂的学科。

Method: 将估计问题转化为再生核希尔伯特空间中惩罚负对数似然泛函的最小化问题；开发EM算法，用新型序贯蒙特卡罗方法近似滤波分布；提出混合贝叶斯变体算法控制模型复杂度；建立精确和近似EM序列的理论收敛结果。

Result: EM - SMC - RKHS程序能在低数据条件下准确估计随机动力系统的漂移函数。

Conclusion: 该方法可在观测约束下进行连续时间建模，具有广泛适用性，数值实验证明了其有效性。

Abstract: The paper proposes a systematic framework for building data-driven stochastic
differential equation (SDE) models from sparse, noisy observations. Unlike
traditional parametric approaches, which assume a known functional form for the
drift, our goal here is to learn the entire drift function directly from data
without strong structural assumptions, making it especially relevant in
scientific disciplines where system dynamics are partially understood or highly
complex. We cast the estimation problem as minimization of the penalized
negative log-likelihood functional over a reproducing kernel Hilbert space
(RKHS). In the sparse observation regime, the presence of unobserved trajectory
segments makes the SDE likelihood intractable. To address this, we develop an
Expectation-Maximization (EM) algorithm that employs a novel Sequential Monte
Carlo (SMC) method to approximate the filtering distribution and generate Monte
Carlo estimates of the E-step objective. The M-step then reduces to a penalized
empirical risk minimization problem in the RKHS, whose minimizer is given by a
finite linear combination of kernel functions via a generalized representer
theorem. To control model complexity across EM iterations, we also develop a
hybrid Bayesian variant of the algorithm that uses shrinkage priors to identify
significant coefficients in the kernel expansion. We establish important
theoretical convergence results for both the exact and approximate EM
sequences. The resulting EM-SMC-RKHS procedure enables accurate estimation of
the drift function of stochastic dynamical systems in low-data regimes and is
broadly applicable across domains requiring continuous-time modeling under
observational constraints. We demonstrate the effectiveness of our method
through a series of numerical experiments.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [111] [Importance Sampling Approximation of Sequence Evolution Models with Site-Dependence](https://arxiv.org/abs/2508.11461)
*Joseph Mathews,Scott C. Schmidler*

Main category: stat.CO

TL;DR: 提出随机近似算法计算分子序列进化模型边际序列似然，给出误差上下界，复杂度不随序列长度指数增长。


<details>
  <summary>Details</summary>
Motivation: 研究位点转换率依赖局部序列上下文的分子序列进化模型，解决其边际序列似然计算问题。

Method: 引入基于重要性采样的随机近似算法，并给出有限样本近似误差的上下界。

Result: 对于实际的突变率，重要性采样器复杂度不随序列长度指数增长，而是随突变数增长。

Conclusion: 算法对许多实际问题可行，并可用于获得特定依赖位点模型的复杂度边界。

Abstract: We consider models for molecular sequence evolution in which the transition
rates at each site depend on the local sequence context, giving rise to a
time-inhomogeneous Markov process in which sites evolve under a complex
dependency structure. We introduce a randomized approximation algorithm for the
marginal sequence likelihood under these models using importance sampling, and
provide matching order upper and lower bounds on the finite sample
approximation error. Given two sequences of length $n$ with $r$ observed
mutations, we show that for practical regimes of $r/n$, the complexity of the
importance sampler does not grow exponentially $n$, but rather in $r$, making
the algorithm practical for many applied problems. We demonstrate the use of
our techniques to obtain problem-specific complexity bounds for a well-known
dependent-site model from the phylogenetics literature.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [112] [CleanCTG: A Deep Learning Model for Multi-Artefact Detection and Reconstruction in Cardiotocography](https://arxiv.org/abs/2508.10928)
*Sheng Wong,Beth Albert,Gabriel Davis Jones*

Main category: eess.AS

TL;DR: 文章提出CleanCTG模型处理CTG信号伪影，在合成数据和外部验证中表现出色，集成应用提升诊断性能。


<details>
  <summary>Details</summary>
Motivation: 当前CTG信号常受伪影干扰，现有深度学习和传统方法处理噪声能力不足，易导致误诊或干预延迟。

Method: 提出端到端双阶段模型CleanCTG，先通过多尺度卷积和上下文感知交叉注意力识别多种伪影类型，再通过特定校正分支重建受损片段，用超80万分钟合成受损CTG数据训练。

Result: 在合成数据上实现完美伪影检测，降低受损片段均方误差；外部验证中优于六个比较分类器；与Dawes - Redman系统集成，提高特异性，缩短决策时间。

Conclusion: 明确去除伪影和信号重建可维持诊断准确性、缩短监测时间，为CTG可靠解读提供实用途径。

Abstract: Cardiotocography (CTG) is essential for fetal monitoring but is frequently
compromised by diverse artefacts which obscure true fetal heart rate (FHR)
patterns and can lead to misdiagnosis or delayed intervention. Current
deep-learning approaches typically bypass comprehensive noise handling,
applying minimal preprocessing or focusing solely on downstream classification,
while traditional methods rely on simple interpolation or rule-based filtering
that addresses only missing samples and fail to correct complex artefact types.
We present CleanCTG, an end-to-end dual-stage model that first identifies
multiple artefact types via multi-scale convolution and context-aware
cross-attention, then reconstructs corrupted segments through artefact-specific
correction branches. Training utilised over 800,000 minutes of physiologically
realistic, synthetically corrupted CTGs derived from expert-verified "clean"
recordings. On synthetic data, CleanCTG achieved perfect artefact detection
(AU-ROC = 1.00) and reduced mean squared error (MSE) on corrupted segments to
2.74 x 10^-4 (clean-segment MSE = 2.40 x 10^-6), outperforming the next best
method by more than 60%. External validation on 10,190 minutes of
clinician-annotated segments yielded AU-ROC = 0.95 (sensitivity = 83.44%,
specificity 94.22%), surpassing six comparator classifiers. Finally, when
integrated with the Dawes-Redman system on 933 clinical CTG recordings,
denoised traces increased specificity (from 80.70% to 82.70%) and shortened
median time to decision by 33%. These findings suggest that explicit artefact
removal and signal reconstruction can both maintain diagnostic accuracy and
enable shorter monitoring sessions, offering a practical route to more reliable
CTG interpretation.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [113] [Repetitive TMS-based Identification of Methamphetamine-Dependent Individuals Using EEG Spectra](https://arxiv.org/abs/2508.11312)
*Ziyi Zeng,Yun-Hsuan Chen,Xurong Gao,Wenyao Zheng,Hemmings Wu,Zhoule Zhu,Jie Yang,Chengkai Wang,Lihua Zhong,Weiwei Cheng,Mohamad Sawan*

Main category: q-bio.NC

TL;DR: 研究探索用神经信号评估rTMS对甲基苯丙胺使用者渴望水平影响的可行性，分析EEG信号，发现γ相对频带功率可区分不同状态并评估rTMS效果，有望用于定制闭环神经调节系统。


<details>
  <summary>Details</summary>
Motivation: 传统用问卷评估rTMS对甲基苯丙胺使用者渴望水平影响不够客观，探索用神经信号获取更客观结果。

Method: 记录20名甲基苯丙胺成瘾者rTMS前后和20名健康者的EEG信号，展示相关和中性图片，分析相对频带功率，用随机森林分类。

Result: MAT的α、β和γ相对频带功率更接近HC；γ相对频带功率区分MBT和HC准确率达90%；区分MAT和HC性能低于MBT和HC；TP10和CP2通道的γ相对频带功率在区分MBT和HC中占主导。

Conclusion: γ相对频带功率可作为区分MBT和HC及评估rTMS效果的生物标志物，实时监测其变化有望用于治疗甲基苯丙胺成瘾的定制闭环神经调节系统。

Abstract: The impact of repetitive transcranial magnetic stimulation (rTMS) on
methamphetamine (METH) users' craving levels is often assessed using
questionnaires. This study explores the feasibility of using neural signals to
obtain more objective results. EEG signals recorded from 20 METH-addicted
participants Before and After rTMS (MBT and MAT) and from 20 healthy
participants (HC) are analyzed. In each EEG paradigm, participants are shown 15
METH-related and 15 neutral pictures randomly, and the relative band power
(RBP) of each EEG sub-band frequency is derived. The average RBP across all 31
channels, as well as individual brain regions, is analyzed. Statistically,
MAT's alpha, beta, and gamma RBPs are more like those of HC compared to MBT, as
indicated by the power topographies. Utilizing a random forest (RF), the gamma
RBP is identified as the optimal frequency band for distinguishing between MBT
and HC with a 90% accuracy. The performance of classifying MAT versus HC is
lower than that of MBT versus HC, suggesting that the efficacy of rTMS can be
validated using RF with gamma RBP. Furthermore, the gamma RBP recorded by the
TP10 and CP2 channels dominates the classification task of MBT versus HC when
receiving METH-related image cues. The gamma RBP during exposure to
METH-related cues can serve as a biomarker for distinguishing between MBT and
HC and for evaluating the effectiveness of rTMS. Therefore, real-time
monitoring of gamma RBP variations holds promise as a parameter for
implementing a customized closed-loop neuromodulation system for treating METH
addiction.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [114] [Functional Analysis of Variance for Association Studies](https://arxiv.org/abs/2508.11069)
*Olga A. Vsevolozhskaya,Dmitri V. Zaykin,Mark C. Greenwood,Changshuai Wei,Qing Lu*

Main category: stat.AP

TL;DR: 本文提出FANOVA方法检测基因组区域序列变异与定性性状的关联，经模拟和实证研究表明其优于SKAT和FLM。


<details>
  <summary>Details</summary>
Motivation: 多数复杂疾病已识别的遗传变异仅占遗传度小部分，寻找疾病相关未知遗传变异存在挑战，需要强大且高效的统计方法。

Method: 提出功能方差分析（FANOVA）方法，测试基因组区域序列变异与定性性状的关联。

Result: 模拟显示FANOVA在小样本和序列变异效应低至中等时优于SKAT和FLM；实证研究中，SKAT和FLM分别检测到与肥胖相关的ANGPTL 4和ANGPTL 3，FANOVA能识别两者。

Conclusion: FANOVA方法在检测疾病相关序列变异方面有优势，尤其是小样本和低至中等效应的情况。

Abstract: While progress has been made in identifying common genetic variants
associated with human diseases, for most of common complex diseases, the
identified genetic variants only account for a small proportion of
heritability. Challenges remain in finding additional unknown genetic variants
predisposing to complex diseases. With the advance in next-generation
sequencing technologies, sequencing studies have become commonplace in genetic
research. The ongoing exome-sequencing and whole-genome-sequencing studies
generate a massive amount of sequencing variants and allow researchers to
comprehensively investigate their role in human diseases. The discovery of new
disease-associated variants can be enhanced by utilizing powerful and
computationally efficient statistical methods. In this paper, we propose a
functional analysis of variance (FANOVA) method for testing an association of
sequence variants in a genomic region with a qualitative trait. The FANOVA has
a number of advantages: (1) it tests for a joint effect of gene variants,
including both common and rare; (2) it fully utilizes linkage disequilibrium
and genetic position information; and (3) allows for either protective or
risk-increasing causal variants. Through simulations, we show that FANOVA
outperform two popularly used methods - SKAT and a previously proposed method
based on functional linear models (FLM), - especially if a sample size of a
study is small and/or sequence variants have low to moderate effects. We
conduct an empirical study by applying three methods (FANOVA, SKAT and FLM) to
sequencing data from Dallas Heart Study. While SKAT and FLM respectively
detected ANGPTL 4 and ANGPTL 3 associated with obesity, FANOVA was able to
identify both genes associated with obesity.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [115] [Towards Efficient Hash Maps in Functional Array Languages](https://arxiv.org/abs/2508.11443)
*William Henrich Due,Martin Elsman,Troels Henriksen*

Main category: cs.PL

TL;DR: 本文推导数据并行实现两级无冲突静态哈希映射，在Futhark实现算法并对比性能，分析性能差异原因并思考功能数组语言扩展。


<details>
  <summary>Details</summary>
Motivation: 实现数据并行的哈希映射，解决功能数组语言中哈希映射接口的灵活性、多态性和抽象性问题。

Method: 对Fredman等人的构造进行功能式表述并扁平化，在Futhark中实现算法，通过基准测试对比性能。

Result: 哈希映射性能优于传统树/搜索方法；与cuCollections库相比，构建速度较慢，查找速度稍慢。

Conclusion: 分析了与cuCollections性能差异的原因，思考功能数组语言编程模型的扩展方向。

Abstract: We present a systematic derivation of a data-parallel implementation of
two-level, static and collision-free hash maps, by giving a functional
formulation of the Fredman et al. construction, and then flattening it. We
discuss the challenges of providing a flexible, polymorphic, and abstract
interface to hash maps in a functional array language, with particular
attention paid to the problem of dynamically sized keys, which we address by
associating each hash map with an arbitrary context. The algorithm is
implemented in Futhark, and the achieved GPU execution performance is compared
on simple benchmark problems. We find that our hash maps outperform
conventional tree/search-based approaches. Furthermore, our implementation is
compared against the state-of-the-art cuCollections library, which is
significantly faster for hash map construction, and to a lesser degree for
lookups. We explain to which extent the performance difference is due to
low-level code generation limitation in the Futhark compiler, and to which
extent it can be attributed to the data-parallel programming vocabulary not
providing the constructs necessary to express the equivalent of the algorithms
used by cuCollections. We end by reflecting to which extent the functional
array language programming model could, or should, be extended to address these
weaknesses.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [116] [Risk-Based Prognostics and Health Management](https://arxiv.org/abs/2508.11031)
*John W. Sheppard*

Main category: eess.SY

TL;DR: 介绍基于风险的预测方法，用连续时间贝叶斯网络实现风险评估与故障预测更紧密耦合，并概述相关技术及应用。


<details>
  <summary>Details</summary>
Motivation: 打破风险评估和预后通常被视为分离任务的现状，实现二者更紧密耦合。

Method: 采用连续时间贝叶斯网络作为底层建模框架，从数据中推导模型。

Result: 展示了如何实现风险评估与故障预测的紧密耦合，概述相关技术在决策支持和基于性能的物流等任务中的应用。

Conclusion: 旨在概述基于风险的预测技术的最新进展，为他人采用这些技术提供指导。

Abstract: It is often the case that risk assessment and prognostics are viewed as
related but separate tasks. This chapter describes a risk-based approach to
prognostics that seeks to provide a tighter coupling between risk assessment
and fault prediction. We show how this can be achieved using the
continuous-time Bayesian network as the underlying modeling framework.
Furthermore, we provide an overview of the techniques that are available to
derive these models from data and show how they might be used in practice to
achieve tasks like decision support and performance-based logistics. This work
is intended to provide an overview of the recent developments related to
risk-based prognostics, and we hope that it will serve as a tutorial of sorts
that will assist others in adopting these techniques.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [117] [Is General-Purpose AI Reasoning Sensitive to Data-Induced Cognitive Biases? Dynamic Benchmarking on Typical Software Engineering Dilemmas](https://arxiv.org/abs/2508.11278)
*Francesco Sovrano,Gabriele Dominici,Rita Sevastjanova,Alessandra Stramiglio,Alberto Bacchelli*

Main category: cs.HC

TL;DR: 本文提出动态基准框架评估软件工程工作流中通用人工智能（GPAI）的数据诱导认知偏差，评估发现领先GPAI系统有依赖浅层语言启发式而非深度推理的倾向，均存在认知偏差。


<details>
  <summary>Details</summary>
Motivation: 人类在软件工程中的认知偏差会导致代价高昂的错误，GPAI虽可能缓解这些偏差，但因其基于人类生成数据训练，需研究其自身是否存在认知偏差。

Method: 提出动态基准框架，从16个手工制作的现实任务开始，测试无关语言线索是否使GPAI得出错误结论；开发按需增强管道生成任务变体，利用Prolog推理和大语言模型验证。

Result: 评估发现领先GPAI系统有依赖浅层语言启发式而非深度推理的倾向，均存在认知偏差，偏差敏感度随任务复杂度急剧上升。

Conclusion: 领先GPAI系统在软件工程实际部署中存在关键风险。

Abstract: Human cognitive biases in software engineering can lead to costly errors.
While general-purpose AI (GPAI) systems may help mitigate these biases due to
their non-human nature, their training on human-generated data raises a
critical question: Do GPAI systems themselves exhibit cognitive biases?
  To investigate this, we present the first dynamic benchmarking framework to
evaluate data-induced cognitive biases in GPAI within software engineering
workflows. Starting with a seed set of 16 hand-crafted realistic tasks, each
featuring one of 8 cognitive biases (e.g., anchoring, framing) and
corresponding unbiased variants, we test whether bias-inducing linguistic cues
unrelated to task logic can lead GPAI systems from correct to incorrect
conclusions.
  To scale the benchmark and ensure realism, we develop an on-demand
augmentation pipeline relying on GPAI systems to generate task variants that
preserve bias-inducing cues while varying surface details. This pipeline
ensures correctness (88--99% on average, according to human evaluation),
promotes diversity, and controls reasoning complexity by leveraging
Prolog-based reasoning and LLM-as-a-judge validation. It also verifies that the
embedded biases are both harmful and undetectable by logic-based, unbiased
reasoners.
  We evaluate leading GPAI systems (GPT, LLaMA, DeepSeek) and find a consistent
tendency to rely on shallow linguistic heuristics over deep reasoning. All
systems exhibit cognitive biases (ranging from 5.9% to 35% across types), with
bias sensitivity increasing sharply with task complexity (up to 49%),
highlighting critical risks in real-world software engineering deployments.

</details>


### [118] [Trustworthy AI Psychotherapy: Multi-Agent LLM Workflow for Counseling and Explainable Mental Disorder Diagnosis](https://arxiv.org/abs/2508.11398)
*Mithat Can Ozgun,Jiahuan Pei,Koen Hindriks,Lucia Donatelli,Qingzhi Liu,Xin Sun,Junxiao Wang*

Main category: cs.HC

TL;DR: 现有基于大语言模型的智能体在心理健康诊断等专业领域表现不佳，本文提出DSM5AgentFlow工作流，可自主生成DSM - 5一级诊断问卷，还进行多维度评估且开源数据和实现。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的智能体在心理健康诊断领域效果有限，现有方法依赖难获取的数据，且缺乏主动询问等能力，无法与临床推理对齐。

Method: 提出DSM5AgentFlow工作流，模拟治疗师 - 客户对话生成诊断问卷，实现透明、分步的疾病预测。

Result: 通过综合实验从对话真实性、诊断准确性和可解释性三个关键维度评估了领先的大语言模型。

Conclusion: DSM5AgentFlow工作流可作为心理健康诊断的补充工具，能确保符合道德和法律标准，且相关数据集和实现已完全开源。

Abstract: LLM-based agents have emerged as transformative tools capable of executing
complex tasks through iterative planning and action, achieving significant
advancements in understanding and addressing user needs. Yet, their
effectiveness remains limited in specialized domains such as mental health
diagnosis, where they underperform compared to general applications. Current
approaches to integrating diagnostic capabilities into LLMs rely on scarce,
highly sensitive mental health datasets, which are challenging to acquire.
These methods also fail to emulate clinicians' proactive inquiry skills, lack
multi-turn conversational comprehension, and struggle to align outputs with
expert clinical reasoning. To address these gaps, we propose DSM5AgentFlow, the
first LLM-based agent workflow designed to autonomously generate DSM-5 Level-1
diagnostic questionnaires. By simulating therapist-client dialogues with
specific client profiles, the framework delivers transparent, step-by-step
disorder predictions, producing explainable and trustworthy results. This
workflow serves as a complementary tool for mental health diagnosis, ensuring
adherence to ethical and legal standards. Through comprehensive experiments, we
evaluate leading LLMs across three critical dimensions: conversational realism,
diagnostic accuracy, and explainability. Our datasets and implementations are
fully open-sourced.

</details>


### [119] [Multimodal Quantitative Measures for Multiparty Behaviour Evaluation](https://arxiv.org/abs/2508.10916)
*Ojas Shirekar,Wim Pouw,Chenxu Hao,Vrushank Phadnis,Thabo Beeler,Chirag Raman*

Main category: cs.HC

TL;DR: 提出统一框架评估多主体社交行为，验证指标敏感性并做感知研究，形成评估工具包。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标忽略多主体交互中的上下文协调动态，需客观评估多主体社交行为。

Method: 引入涵盖同步性、时间对齐和结构相似性三个维度的统一框架，通过三种理论驱动的扰动验证指标敏感性，开展感知研究。

Result: 混合效应分析显示扰动带来可预测的变化，三个指标能提供不同维度的见解。

Conclusion: 形成评估和改进社交智能体的可靠工具包，代码开源。

Abstract: Digital humans are emerging as autonomous agents in multiparty interactions,
yet existing evaluation metrics largely ignore contextual coordination
dynamics. We introduce a unified, intervention-driven framework for objective
assessment of multiparty social behaviour in skeletal motion data, spanning
three complementary dimensions: (1) synchrony via Cross-Recurrence
Quantification Analysis, (2) temporal alignment via Multiscale Empirical Mode
Decompositionbased Beat Consistency, and (3) structural similarity via Soft
Dynamic Time Warping. We validate metric sensitivity through three
theory-driven perturbations -- gesture kinematic dampening, uniform
speech-gesture delays, and prosodic pitch-variance reduction-applied to
$\approx 145$ 30-second thin slices of group interactions from the DnD dataset.
Mixed-effects analyses reveal predictable, joint-independent shifts: dampening
increases CRQA determinism and reduces beat consistency, delays weaken
cross-participant coupling, and pitch flattening elevates F0 Soft-DTW costs. A
complementary perception study ($N=27$) compares judgments of full-video and
skeleton-only renderings to quantify representation effects. Our three measures
deliver orthogonal insights into spatial structure, timing alignment, and
behavioural variability. Thereby forming a robust toolkit for evaluating and
refining socially intelligent agents. Code available on
\href{https://github.com/tapri-lab/gig-interveners}{GitHub}.

</details>


### [120] [Managing the unexpected: Operator behavioural data and its value in predicting correct alarm responses](https://arxiv.org/abs/2508.10917)
*Chidera W. Amazu,Joseph Mietkiewicz,Ammar N. Abbas,Gabriele Baldissone,Davide Fissore,Micaela Demichela,Anders L. Madsen,Maria Chiara Leva*

Main category: cs.HC

TL;DR: 文章探讨利用分布式控制系统数据洞察操作员行为并预测响应结果，用实验数据和两种模型分析，确定了预测指标。


<details>
  <summary>Details</summary>
Motivation: 传统可穿戴生理测量工具不适用于日常操作，需寻找不干扰日常任务的方法来洞察操作员行为和预测响应结果。

Method: 使用甲醛生产工厂模拟器和四种人在环实验支持配置获取数据，运用逐步逻辑回归和贝叶斯网络模型进行分析。

Result: 确定了一些预测指标。

Conclusion: 实时获取相关且有预测性的行为指标能帮助决策者预测结果并为操作员提供及时支持。

Abstract: Data from psychophysiological measures can offer new insight into control
room operators' behaviour, cognition, and mental workload status. This can be
particularly helpful when combined with appraisal of capacity to respond to
possible critical plant conditions (i.e. critical alarms response scenarios).
However, wearable physiological measurement tools such as eye tracking and EEG
caps can be perceived as intrusive and not suitable for usage in daily
operations. Therefore, this article examines the potential of using real-time
data from process and operator-system interactions during abnormal scenarios
that can be recorded and retrieved from the distributed control system's
historian or process log, and their capacity to provide insight into operator
behavior and predict their response outcomes, without intruding on daily tasks.
Data for this study were obtained from a design of experiment using a
formaldehyde production plant simulator and four human-in-the-loop experimental
support configurations. A comparison between the different configurations in
terms of both behaviour and performance is presented in this paper. A step-wise
logistic regression and a Bayesian network models were used to achieve this
objective. The results identified some predictive metrics and the paper discuss
their value as precursor or predictor of overall system performance in alarm
response scenarios. Knowledge of relevant and predictive behavioural metrics
accessible in real time can better equip decision-makers to predict outcomes
and provide timely support measures for operators.

</details>


### [121] [Human-AI collaboration or obedient and often clueless AI in instruct, serve, repeat dynamics?](https://arxiv.org/abs/2508.10919)
*Mohammed Saqr,Kamila Misiejuk,Sonsoles López-Pernas*

Main category: cs.HC

TL;DR: 研究学生与AI在解决复杂问题时的交互，发现主导模式为指令式，且当前大语言模型缺乏认知协同，对设计AI系统有启示。


<details>
  <summary>Details</summary>
Motivation: 以往人类与AI协作研究多关注语言学习，用传统计数法，较少关注认知需求任务中协作的演变和动态，本文要研究解决复杂问题时的人机交互。

Method: 对学生与AI的交互进行定性编码，用过渡网络分析、序列分析、偏相关网络，以及卡方检验和Person残差阴影马赛克图比较频率，来绘制交互模式及其演变，分析与问题复杂度和学生表现的关系。

Result: 发现主导的指令式交互模式，学生提示与AI输出常不一致，缺乏协同；作业复杂度、提示长度和学生成绩无显著相关性。

Conclusion: 当前大语言模型更适合遵循指令，而非认知协作，需设计注重认知对齐和协作的AI系统。

Abstract: While research on human-AI collaboration exists, it mainly examined language
learning and used traditional counting methods with little attention to
evolution and dynamics of collaboration on cognitively demanding tasks. This
study examines human-AI interactions while solving a complex problem.
Student-AI interactions were qualitatively coded and analyzed with transition
network analysis, sequence analysis and partial correlation networks as well as
comparison of frequencies using chi-square and Person-residual shaded Mosaic
plots to map interaction patterns, their evolution, and their relationship to
problem complexity and student performance. Findings reveal a dominant
Instructive pattern with interactions characterized by iterative ordering
rather than collaborative negotiation. Oftentimes, students engaged in long
threads that showed misalignment between their prompts and AI output that
exemplified a lack of synergy that challenges the prevailing assumptions about
LLMs as collaborative partners. We also found no significant correlations
between assignment complexity, prompt length, and student grades suggesting a
lack of cognitive depth, or effect of problem difficulty. Our study indicates
that the current LLMs, optimized for instruction-following rather than
cognitive partnership, compound their capability to act as cognitively
stimulating or aligned collaborators. Implications for designing AI systems
that prioritize cognitive alignment and collaboration are discussed.

</details>


### [122] [AI That Helps Us Help Each Other: A Proactive System for Scaffolding Mentor-Novice Collaboration in Entrepreneurship Coaching](https://arxiv.org/abs/2508.11052)
*Evey Jiaxin Huang,Matthew Easterday,Elizabeth Gerber*

Main category: cs.HC

TL;DR: 提出结合特定领域认知模型和大语言模型的人机辅导系统，通过实地部署发现其有积极作用，同时暴露出问题，并给出设计原则。


<details>
  <summary>Details</summary>
Motivation: 新手创业者在应对创业中的元认知需求方面存在困难，导师时间和视野有限，难以提供量身定制的支持。

Method: 提出结合创业风险的特定领域认知模型和大语言模型的人机辅导系统，进行探索性实地部署。

Result: 系统支持新手元认知，帮助导师制定策略，提高会议质量，但也暴露出信任、误诊和对AI期望等问题。

Conclusion: 为复杂、模糊领域中支持元认知和人际协作的主动式AI系统提供设计原则，对其他类似领域有启示。

Abstract: Entrepreneurship requires navigating open-ended, ill-defined problems:
identifying risks, challenging assumptions, and making strategic decisions
under deep uncertainty. Novice founders often struggle with these metacognitive
demands, while mentors face limited time and visibility to provide tailored
support. We present a human-AI coaching system that combines a domain-specific
cognitive model of entrepreneurial risk with a large language model (LLM) to
proactively scaffold both novice and mentor thinking. The system proactively
poses diagnostic questions that challenge novices' thinking and helps both
novices and mentors plan for more focused and emotionally attuned meetings.
Critically, mentors can inspect and modify the underlying cognitive model,
shaping the logic of the system to reflect their evolving needs. Through an
exploratory field deployment, we found that using the system supported novice
metacognition, helped mentors plan emotionally attuned strategies, and improved
meeting depth, intentionality, and focus--while also surfaced key tensions
around trust, misdiagnosis, and expectations of AI. We contribute design
principles for proactive AI systems that scaffold metacognition and human-human
collaboration in complex, ill-defined domains, offering implications for
similar domains like healthcare, education, and knowledge work.

</details>


### [123] [Uncovering Latent Connections in Indigenous Heritage: Semantic Pipelines for Cultural Preservation in Brazil](https://arxiv.org/abs/2508.10911)
*Luis Vitor Zerkowski,Nina S. T. Hirata*

Main category: cs.HC

TL;DR: 利用巴西国家原住民博物馆在线藏品数据，通过AI开发语义管道和可视化工具，助力文化遗产保护。


<details>
  <summary>Details</summary>
Motivation: 原住民社区在文化遗产保护上面临挑战，巴西国家原住民博物馆线上藏品数据可用于文化参与，需要利用数据提升可及性等。

Method: 开发视觉和文本语义管道，将嵌入空间投影到二维并集成到交互式可视化工具。

Result: 系统支持策展任务、促进公众参与、揭示藏品潜在联系。

Conclusion: 展示了AI可在文化保护实践中做出符合伦理的贡献。

Abstract: Indigenous communities face ongoing challenges in preserving their cultural
heritage, particularly in the face of systemic marginalization and urban
development. In Brazil, the Museu Nacional dos Povos Indigenas through the
Tainacan platform hosts the country's largest online collection of Indigenous
objects and iconographies, providing a critical resource for cultural
engagement. Using publicly available data from this repository, we present a
data-driven initiative that applies artificial intelligence to enhance
accessibility, interpretation, and exploration. We develop two semantic
pipelines: a visual pipeline that models image-based similarity and a textual
pipeline that captures semantic relationships from item descriptions. These
embedding spaces are projected into two dimensions and integrated into an
interactive visualization tool we also developed. In addition to
similarity-based navigation, users can explore the collection through temporal
and geographic lenses, enabling both semantic and contextualized perspectives.
The system supports curatorial tasks, aids public engagement, and reveals
latent connections within the collection. This work demonstrates how AI can
ethically contribute to cultural preservation practices.

</details>


### [124] [Human-in-the-Loop Systems for Adaptive Learning Using Generative AI](https://arxiv.org/abs/2508.11062)
*Bhavishya Tarun,Haoze Du,Dinesh Kannan,Edward F. Gehringer*

Main category: cs.HC

TL;DR: 采用人在回路（HITL）方法，结合生成式AI，利用学生反馈改进AI生成内容，提升个性化学习效果，在STEM教育中初步显示积极成果。


<details>
  <summary>Details</summary>
Motivation: 在自适应学习研究基础上，探索利用学生驱动的反馈循环改进AI生成内容，以提升学生的知识留存率和参与度，尤其是在STEM教育领域。

Method: 采用标记技术和提示工程进行内容个性化，学生通过预定义反馈标签对AI响应进行批判和修改，为检索增强生成（RAG）系统提供信息以实时调整解释。

Result: 对STEM学生的初步研究发现，与传统AI工具相比，该方法改善了学习成果并提升了学生信心。

Conclusion: 强调了AI通过迭代改进创建动态、反馈驱动和个性化学习环境的潜力。

Abstract: A Human-in-the-Loop (HITL) approach leverages generative AI to enhance
personalized learning by directly integrating student feedback into
AI-generated solutions. Students critique and modify AI responses using
predefined feedback tags, fostering deeper engagement and understanding. This
empowers students to actively shape their learning, with AI serving as an
adaptive partner. The system uses a tagging technique and prompt engineering to
personalize content, informing a Retrieval-Augmented Generation (RAG) system to
retrieve relevant educational material and adjust explanations in real time.
This builds on existing research in adaptive learning, demonstrating how
student-driven feedback loops can modify AI-generated responses for improved
student retention and engagement, particularly in STEM education. Preliminary
findings from a study with STEM students indicate improved learning outcomes
and confidence compared to traditional AI tools. This work highlights AI's
potential to create dynamic, feedback-driven, and personalized learning
environments through iterative refinement.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [125] [JobPulse: A Big Data Approach to Real-Time Engineering Workforce Analysis and National Industrial Policy](https://arxiv.org/abs/2508.11014)
*Karen S. Markel,Mihir Tale,Andrea Belz*

Main category: cs.CY

TL;DR: 本文采用大数据方法，聚焦招聘信息估计就业市场不匹配情况，以美国南加州半导体行业为例开展研究并取得多方面成果。


<details>
  <summary>Details</summary>
Motivation: 现有就业报告多关注求职者供应端，技能不匹配情况不明，招聘平台招聘信息主要关注求职者技能，因此要从需求端估计就业市场不匹配情况。

Method: 使用商业网络爬虫工具和新的数据处理方案，构建美国半导体行业的招聘信息数据集，聚焦南加州。

Result: 了解了雇主群体和各职位职能的相对需求。

Conclusion: 工作有三方面贡献：提供近实时的劳动力需求洞察；讨论大规模分析雇主数据库时的消歧和语义挑战；研究南加州半导体工程生态系统。

Abstract: Employment on a societal scale contributes heavily to national and global
affairs; consequently, job openings and unemployment estimates provide
important information to financial markets and governments alike. However, such
reports often describe only the supply (employee job seeker) side of the job
market, and skill mismatches are poorly understood. Job postings aggregated on
recruiting platforms illuminate marketplace demand, but to date have primarily
focused on candidate skills described in their personal profiles. In this
paper, we report on a big data approach to estimating job market mismatches by
focusing on demand, as represented in publicly available job postings. We use
commercially available web scraping tools and a new data processing scheme to
build a job posting data set for the semiconductor industry, a strategically
critical sector of the United States economy; we focus on Southern California
as a central hub of advanced technologies. We report on the employer base and
relative needs of various job functions. Our work contributes on three fronts:
First, we provide nearly real-time insight into workforce demand; second, we
discuss disambiguation and semantic challenges in analysis of employer data
bases at scale; and third, we report on the Southern California semiconductor
engineering ecosystem.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [126] [StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image Translation](https://arxiv.org/abs/2508.11203)
*Seungmi Lee,Kwan Yun,Junyong Noh*

Main category: cs.GR

TL;DR: 提出StyleMM框架，基于用户文本描述构建风格化3D可变形模型，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 实现基于用户文本描述的3D人脸风格化，且在风格化过程中保留身份、对齐和表情等面部属性。

Method: 基于预训练的网格变形网络和纹理生成器，用扩散模型生成的风格化面部图像微调模型，引入保留面部属性的风格化方法。

Result: StyleMM能前馈生成风格化人脸网格，定量和定性评估显示在身份级面部多样性和风格化能力上优于现有方法。

Conclusion: 提出的StyleMM框架有效，可显式控制形状、表情和纹理参数生成风格化人脸网格。

Abstract: We introduce StyleMM, a novel framework that can construct a stylized 3D
Morphable Model (3DMM) based on user-defined text descriptions specifying a
target style. Building upon a pre-trained mesh deformation network and a
texture generator for original 3DMM-based realistic human faces, our approach
fine-tunes these models using stylized facial images generated via text-guided
image-to-image (i2i) translation with a diffusion model, which serve as
stylization targets for the rendered mesh. To prevent undesired changes in
identity, facial alignment, or expressions during i2i translation, we introduce
a stylization method that explicitly preserves the facial attributes of the
source image. By maintaining these critical attributes during image
stylization, the proposed approach ensures consistent 3D style transfer across
the 3DMM parameter space through image-based training. Once trained, StyleMM
enables feed-forward generation of stylized face meshes with explicit control
over shape, expression, and texture parameters, producing meshes with
consistent vertex connectivity and animatability. Quantitative and qualitative
evaluations demonstrate that our approach outperforms state-of-the-art methods
in terms of identity-level facial diversity and stylization capability. The
code and videos are available at
[kwanyun.github.io/stylemm_page](kwanyun.github.io/stylemm_page).

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [127] [Weighted First Order Model Counting for Two-variable Logic with Axioms on Two Relations](https://arxiv.org/abs/2508.11515)
*Qipeng Kuang,Václav Kůla,Ondřej Kuželka,Yuanhong Wang,Yuyi Wang*

Main category: cs.LO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The Weighted First-Order Model Counting Problem (WFOMC) asks to compute the
weighted sum of models of a given first-order logic sentence over a given
domain. The boundary between fragments for which WFOMC can be computed in
polynomial time relative to the domain size lies between the two-variable
fragment ($\text{FO}^2$) and the three-variable fragment ($\text{FO}^3$). It is
known that WFOMC for \FOthree{} is $\mathsf{\#P_1}$-hard while polynomial-time
algorithms exist for computing WFOMC for $\text{FO}^2$ and $\text{C}^2$,
possibly extended by certain axioms such as the linear order axiom, the
acyclicity axiom, and the connectedness axiom. All existing research has
concentrated on extending the fragment with axioms on a single distinguished
relation, leaving a gap in understanding the complexity boundary of axioms on
multiple relations. In this study, we explore the extension of the two-variable
fragment by axioms on two relations, presenting both negative and positive
results. We show that WFOMC for $\text{FO}^2$ with two linear order relations
and $\text{FO}^2$ with two acyclic relations are $\mathsf{\#P_1}$-hard.
Conversely, we provide an algorithm in time polynomial in the domain size for
WFOMC of $\text{C}^2$ with a linear order relation, its successor relation and
another successor relation.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [128] [Holistic Bioprocess Development Across Scales Using Multi-Fidelity Batch Bayesian Optimization](https://arxiv.org/abs/2508.10970)
*Adrian Martens,Mathias Neufang,Alessandro Butté,Moritz von Stosch,Antonio del Rio Chanona,Laura Marie Helleckes*

Main category: q-bio.QM

TL;DR: 提出多保真度批量贝叶斯优化框架加速生物过程开发，减少实验成本，案例显示可降低成本并提高产量。


<details>
  <summary>Details</summary>
Motivation: 生物过程开发成本高且复杂，传统实验设计方法难以解决过程放大及反应条件与生物催化剂选择的联合优化问题。

Method: 提出多保真度批量贝叶斯优化框架，集成适用于多保真度建模和混合变量优化的高斯过程。

Result: 通过自定义模拟和多个案例研究，表明该方法能降低实验成本并提高产量。

Conclusion: 提供了数据高效的生物过程优化策略，指出了可持续生物技术在迁移学习和不确定性感知设计方面的未来机会。

Abstract: Bioprocesses are central to modern biotechnology, enabling sustainable
production in pharmaceuticals, specialty chemicals, cosmetics, and food.
However, developing high-performing processes is costly and complex, requiring
iterative, multi-scale experimentation from microtiter plates to pilot
reactors. Conventional Design of Experiments (DoE) approaches often struggle to
address process scale-up and the joint optimization of reaction conditions and
biocatalyst selection.
  We propose a multi-fidelity batch Bayesian optimization framework to
accelerate bioprocess development and reduce experimental costs. The method
integrates Gaussian Processes tailored for multi-fidelity modeling and
mixed-variable optimization, guiding experiment selection across scales and
biocatalysts. A custom simulation of a Chinese Hamster Ovary bioprocess,
capturing non-linear and coupled scale-up dynamics, is used for benchmarking
against multiple simulated industrial DoE baselines. Multiple case studies show
how the proposed workflow can achieve a reduction in experimental costs and
increased yield.
  This work provides a data-efficient strategy for bioprocess optimization and
highlights future opportunities in transfer learning and uncertainty-aware
design for sustainable biotechnology.

</details>


### [129] [Trees Assembling Mann Whitney Approach for Detecting Genome-wide Joint Association among Low Marginal Effect loci](https://arxiv.org/abs/1505.01206)
*Changshuai Wei,Daniel J. Schaid,Qing Lu*

Main category: q-bio.QM

TL;DR: 提出TAMW方法用于分析大量低边际效应（LME）基因变异的联合关联，经模拟和实证研究表明其性能优于MDR和LRMW，并应用于全基因组分析发现与克罗恩病相关的联合关联。


<details>
  <summary>Details</summary>
Motivation: 常见复杂疾病受大量基因变异相互作用影响，发现LME基因变异并评估其在高维数据中的联合关联是巨大挑战，需要有效方法。

Method: 提出Trees Assembling Mann whitney（TAMW）方法进行联合关联分析。

Result: 模拟研究中TAMW在涉及多个LME位点及其相互作用时性能优于MDR和LRMW；实证研究中TAMW检测到与克罗恩病更强的联合关联；全基因组分析完成459K单核苷酸多态性分析，发现与克罗恩病的联合关联。

Conclusion: TAMW是一种计算高效且强大的方法，能有效分析LME基因变异的联合关联，新发现的关联表明部分基因可能在克罗恩病病理生理和病因过程中起重要作用。

Abstract: Common complex diseases are likely influenced by the interplay of hundreds,
or even thousands, of genetic variants. Converging evidence shows that genetic
variants with low marginal effects (LME) play an important role in disease
development. Despite their potential significance, discovering LME genetic
variants and assessing their joint association on high dimensional data (e.g.,
genome wide association studies) remain a great challenge. To facilitate joint
association analysis among a large ensemble of LME genetic variants, we
proposed a computationally efficient and powerful approach, which we call Trees
Assembling Mann whitney (TAMW). Through simulation studies and an empirical
data application, we found that TAMW outperformed multifactor dimensionality
reduction (MDR) and the likelihood ratio based Mann whitney approach (LRMW)
when the underlying complex disease involves multiple LME loci and their
interactions. For instance, in a simulation with 20 interacting LME loci, TAMW
attained a higher power (power=0.931) than both MDR (power=0.599) and LRMW
(power=0.704). In an empirical study of 29 known Crohn's disease (CD) loci,
TAMW also identified a stronger joint association with CD than those detected
by MDR and LRMW. Finally, we applied TAMW to Wellcome Trust CD GWAS to conduct
a genome wide analysis. The analysis of 459K single nucleotide polymorphisms
was completed in 40 hours using parallel computing, and revealed a joint
association predisposing to CD (p-value=2.763e-19). Further analysis of the
newly discovered association suggested that 13 genes, such as ATG16L1 and
LACC1, may play an important role in CD pathophysiological and etiological
processes.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [130] [Banking 2.0: The Stablecoin Banking Revolution -- How Digital Assets Are Reshaping Global Finance](https://arxiv.org/abs/2508.11395)
*Kevin McNamara,Rhea Pritham Marpu*

Main category: cs.ET

TL;DR: 稳定币有望重塑银行业，实现“银行2.0”，解决传统金融漏洞，有诸多机构采用案例，能解决宏观经济失衡等问题。


<details>
  <summary>Details</summary>
Motivation: 全球金融体系处于转折点，传统法定货币有漏洞，稳定币有潜力变革金融行业。

Method: 结合现实例子、当前市场数据进行分析。

Result: 稳定币正加速被机构采用，能解决宏观经济失衡，促进金融系统互联互通。

Conclusion: 稳定币有望重塑银行业，推动金融变革。

Abstract: The global financial system stands at an inflection point. Stablecoins
represent the most significant evolution in banking since the abandonment of
the gold standard, positioned to enable "Banking 2.0" by seamlessly integrating
cryptocurrency innovation with traditional finance infrastructure. This
transformation rivals artificial intelligence as the next major disruptor in
the financial sector. Modern fiat currencies derive value entirely from
institutional trust rather than physical backing, creating vulnerabilities that
stablecoins address through enhanced stability, reduced fraud risk, and unified
global transactions that transcend national boundaries. Recent developments
demonstrate accelerating institutional adoption: landmark U.S. legislation
including the GENIUS Act of 2025, strategic industry pivots from major players
like JPMorgan's crypto-backed loan initiatives, and PayPal's comprehensive "Pay
with Crypto" service. Widespread stablecoin implementation addresses critical
macroeconomic imbalances, particularly the inflation-productivity gap plaguing
modern monetary systems, through more robust and diversified backing
mechanisms. Furthermore, stablecoins facilitate deregulation and efficiency
gains, paving the way for a more interconnected international financial system.
This whitepaper comprehensively explores how stablecoins are poised to reshape
banking, supported by real-world examples, current market data, and analysis of
their transformative potential.

</details>


### [131] [RAG for Geoscience: What We Expect, Gaps and Opportunities](https://arxiv.org/abs/2508.11246)
*Runlong Yu,Shiyuan Luo,Rahul Ghosh,Lingyao Li,Yiqun Xie,Xiaowei Jia*

Main category: cs.ET

TL;DR: 传统RAG以文本为主，在地球科学应用受限，提出Geo - RAG新范式，有新能力并带来新机遇。


<details>
  <summary>Details</summary>
Motivation: 当前RAG工作流以文本为主，在地球科学领域适用性有限，而地球科学任务需要更多证据支持。

Method: 提出Geo - RAG范式，构建‘retrieve → reason → generate → verify’循环，支持多模态数据检索、约束推理、生成科学产物和验证假设四项核心能力。

Result: 提出新的Geo - RAG范式。

Conclusion: Geo - RAG范式为更可信和透明的地球科学工作流带来新机会。

Abstract: Retrieval-Augmented Generation (RAG) enhances language models by combining
retrieval with generation. However, its current workflow remains largely
text-centric, limiting its applicability in geoscience. Many geoscientific
tasks are inherently evidence-hungry. Typical examples involve imputing missing
observations using analog scenes, retrieving equations and parameters to
calibrate models, geolocating field photos based on visual cues, or surfacing
historical case studies to support policy analyses. A simple
``retrieve-then-generate'' pipeline is insufficient for these needs. We
envision Geo-RAG, a next-generation paradigm that reimagines RAG as a modular
retrieve $\rightarrow$ reason $\rightarrow$ generate $\rightarrow$ verify loop.
Geo-RAG supports four core capabilities: (i) retrieval of multi-modal Earth
data; (ii) reasoning under physical and domain constraints; (iii) generation of
science-grade artifacts; and (iv) verification of generated hypotheses against
numerical models, ground measurements, and expert assessments. This shift opens
new opportunities for more trustworthy and transparent geoscience workflows.

</details>


<div id='math.NT'></div>

# math.NT [[Back]](#toc)

### [132] [Dyadically resolving trinomials for fast modular arithmetic](https://arxiv.org/abs/2508.11043)
*Robert Dougherty-Bliss,Mits Kobayashi,Natalya Ter-Saakov,Eugene Zima*

Main category: math.NT

TL;DR: 研究形如$2^n - 2^k + 1$的三项式模，构建固定位长两两互质三项式模的大集合，给出互质条件、图论模型及上界证明。


<details>
  <summary>Details</summary>
Motivation: 剩余数系统可加速整数计算，研究具有高效算术和位级属性的三项式模，构建大的两两互质三项式模集合。

Method: 分析对应多项式$x^n - x^k + 1$，基于多项式结式建立互质充分条件，用图论模型和最大团查找算法构建实例，利用图着色、结式和分圆多项式性质证明上界。

Result: 建立互质充分条件，得到图论模型，可构建大的两两互质三项式模集合，证明了集合大小的上界。

Conclusion: 通过理论分析和算法构建，对固定位长两两互质三项式模集合的构建有了深入研究，确定了集合大小的限制。

Abstract: Residue number systems based on pairwise relatively prime moduli are a
powerful tool for accelerating integer computations via the Chinese Remainder
Theorem. We study a structured family of moduli of the form $2^n - 2^k + 1$,
originally proposed for their efficient arithmetic and bit-level properties.
These trinomial moduli support fast modular operations and exhibit scalable
modular inverses.
  We investigate the problem of constructing large sets of pairwise relatively
prime trinomial moduli of fixed bit length. By analyzing the corresponding
trinomials $x^n - x^k + 1$, we establish a sufficient condition for coprimality
based on polynomial resultants. This leads to a graph-theoretic model where
maximal sets correspond to cliques in a compatibility graph, and we use maximum
clique-finding algorithms to construct large examples in practice. Using the
theory of graph colorings, resultants, and properties of cyclotomic
polynomials, we also prove upper bounds on the size of such sets as a function
of $n$.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [133] [Two-Sample Testing with Missing Data via Energy Distance: Weighting and Imputation Approaches](https://arxiv.org/abs/2508.11421)
*Danijel G. Aleksić,Bojana Milošević*

Main category: stat.ME

TL;DR: 本文研究有缺失数据时基于能量距离的两样本检验问题，提出新方法，推导渐近零分布，通过模拟研究比较各方法并给出建议。


<details>
  <summary>Details</summary>
Motivation: 解决多种缺失机制下有缺失数据时的两样本检验问题。

Method: 提出结合所有可用数据并使用适当权重的检验统计量修改方法，推导其渐近零分布，提出两种重采样程序，还提出新的自助法，通过模拟研究比较各方法。

Result: 通过模拟研究比较了不同样本量、维度、分布、缺失机制和缺失率下各方法的第一类错误控制和统计功效。

Conclusion: 基于结果为每种考虑的场景提供了通用建议。

Abstract: In this paper, we address the problem of two-sample testing in the presence
of missing data under a variety of missingness mechanisms. Our focus is on the
well-known energy distance-based two-sample test. In addition to the standard
complete-case approach, we propose a modification of the test statistic that
incorporates all available data, utilizing appropriate weights. The asymptotic
null distribution of the test statistic is derived and two resampling
procedures for approximating the corresponding p-values are proposed. We also
propose a new bootstrap method specifically designed for a test statistic based
on samples completed via common imputation methods. Through an extensive
simulation study, we compare all methods in terms of type I error control and
statistical power across a set of sample sizes, dimensions, distributions,
missingness mechanisms, and missingness rates. Based on these results, we
provide general recommendations for each considered scenario.

</details>


### [134] [Simulation-based inference using splitting schemes for partially observed diffusions in chemical reaction networks](https://arxiv.org/abs/2508.11438)
*Petar Jovanovski,Andrew Golightly,Umberto Picchini,Massimiliano Tamborrino*

Main category: stat.ME

TL;DR: 本文针对化学朗之万方程描述的化学反应网络模拟与参数推断问题，提出三项改进，在多个模型验证中展现了有效性、准确性和低计算成本。


<details>
  <summary>Details</summary>
Motivation: 化学朗之万方程对应的随机微分方程难以显式求解且存在非交换噪声，数据不完整或有测量误差，需解决模拟和参数推断难题。

Method: 将模型重写为扰动的条件Cox - Ingersoll - Ross型SDE；开发保留模型结构特性的数值分裂方案；提出结合“数据条件”模拟和序贯学习的序贯蒙特卡罗近似贝叶斯计算算法。

Result: 在随机阻遏振荡器、洛特卡 - 沃尔泰拉和双池系统等模型验证中，方法在数值和推断准确性上表现有效，且降低了计算成本。

Conclusion: 所提方法能有效解决化学反应网络的模拟和参数推断问题，具有较好的准确性和较低计算成本。

Abstract: We address the problem of simulation and parameter inference for chemical
reaction networks described by the chemical Langevin equation, a stochastic
differential equation (SDE) representation of the dynamics of the chemical
species. This is challenging for two main reasons. First, the
(multi-dimensional) SDEs cannot be explicitly solved and are driven by
multiplicative and non-commutative noise, requiring the development of advanced
numerical schemes for their approximation and simulation. Second, not all
components of the SDEs are directly observed, as the available discrete-time
data are typically incomplete and/or perturbed with measurement error. We
tackle these issues via three contributions. First, we show that these models
can be rewritten as perturbed conditionally Cox-Ingersoll-Ross-type SDEs, i.e.,
each coordinate, conditioned on all other coordinates being fixed, follows an
SDE with linear drift and square root diffusion coefficient perturbed by
additional Brownian motions. Second, for this class of SDEs, we develop a
numerical splitting scheme that preserves structural properties of the model,
such as oscillations, state space and invariant distributions, unlike the
commonly used Euler-Maruyama scheme. Our numerical method is robust for large
integration time steps. Third, we propose a sequential Monte Carlo approximate
Bayesian computation algorithm incorporating "data-conditional" simulation and
sequential learning of summary statistics, allowing inference for
multidimensional partially observed systems, further developing previous
results on fully observed systems based on the Euler-Maruyama scheme. We
validate our approach on models of interest in chemical reaction networks, such
as the stochastic Repressilator, Lotka-Volterra, and two-pool systems,
demonstrating its effectiveness, in terms of both numerical and inferential
accuracy, and reduced computational cost.

</details>


### [135] [A weighted U statistic for association analysis considering genetic heterogeneity](https://arxiv.org/abs/1504.08319)
*Changshuai Wei,Robert C. Elston,Qing Lu*

Main category: stat.ME

TL;DR: 本文提出考虑遗传异质性的HWU方法用于关联分析，经模拟和实际数据验证其优势。


<details>
  <summary>Details</summary>
Motivation: 现有复杂疾病遗传研究多忽略异质性影响，多数统计方法假设疾病遗传效应均一，在疾病异质性情况下效力低。

Method: 提出异质性加权U（HWU）方法用于关联分析，可处理多种表型，适用于高维遗传数据。

Result: 模拟显示HWU在疾病遗传病因异质时有优势，对不同模型假设具鲁棒性；对SAGE数据集进行全基因组分析，7小时分析近百万遗传标记，鉴定出两个新基因对尼古丁依赖的异质效应。

Conclusion: HWU方法在考虑遗传异质性的关联分析中有效且高效。

Abstract: Converging evidence suggests that common complex diseases with the same or
similar clinical manifestations could have different underlying genetic
etiologies. While current research interests have shifted toward uncovering
rare variants and structural variations predisposing to human diseases, the
impact of heterogeneity in genetic studies of complex diseases has been largely
overlooked. Most of the existing statistical methods assume the disease under
investigation has a homogeneous genetic effect and could, therefore, have low
power if the disease undergoes heterogeneous pathophysiological and etiological
processes. In this paper, we propose a heterogeneity weighted U (HWU) method
for association analyses considering genetic heterogeneity. HWU can be applied
to various types of phenotypes (e.g., binary and continuous) and is
computationally effcient for high- dimensional genetic data. Through
simulations, we showed the advantage of HWU when the underlying genetic
etiology of a disease was heterogeneous, as well as the robustness of HWU
against different model assumptions (e.g., phenotype distributions). Using HWU,
we conducted a genome-wide analysis of nicotine dependence from the Study of
Addiction: Genetics and Environments (SAGE) dataset. The genome-wide analysis
of nearly one million genetic markers took 7 hours, identifying heterogeneous
effects of two new genes (i.e., CYP3A5 and IKBKB) on nicotine dependence.

</details>


### [136] [A Generalized Similarity U Test for Multivariate Analysis of Sequencing Data](https://arxiv.org/abs/1505.01179)
*Changshuai Wei,Qing Lu*

Main category: stat.ME

TL;DR: 提出广义相似性U检验GSU处理复杂疾病基因关联研究，模拟显示其优势并应用于实际数据。


<details>
  <summary>Details</summary>
Motivation: 传统统计方法难以应对测序研究的高维数据和低频变异，且现有方法难以处理多疾病表型不同分布问题。

Method: 提出基于相似性的GSU检验，研究其理论性质，提供p值、样本量和功效计算方法。

Result: 模拟表明GSU在功效和对表型分布的稳健性上优于现有方法，在实际数据中识别出4个基因与5个代谢相关表型的联合关联。

Conclusion: GSU是一种有效处理高维基因和表型数据，进行多变量分析的方法。

Abstract: Sequencing-based studies are emerging as a major tool for genetic association
studies of complex diseases. These studies pose great challenges to the
traditional statistical methods (e.g., single-locus analyses based on
regression methods) because of the high-dimensionality of data and the low
frequency of genetic variants. In addition, there is a great interest in
biology and epidemiology to identify genetic risk factors contributed to
multiple disease phenotypes. The multiple phenotypes can often follow different
distributions, which violates the assumptions of most current methods. In this
paper, we propose a generalized similarity U test, referred to as GSU. GSU is a
similarity-based test and can handle high-dimensional genotypes and phenotypes.
We studied the theoretical properties of GSU, and provided the efficient
p-value calculation for association test as well as the sample size and power
calculation for the study design. Through simulation, we found that GSU had
advantages over existing methods in terms of power and robustness to phenotype
distributions. Finally, we used GSU to perform a multivariate analysis of
sequencing data in the Dallas Heart Study and identified a joint association of
4 genes with 5 metabolic related phenotypes.

</details>


### [137] [A Weighted U Statistic for Genetic Association Analyses of Sequencing Data](https://arxiv.org/abs/1505.01204)
*Changshuai Wei,Ming Li,Zihuai He,Olga Vsevolozhskaya,Daniel J. Schaid,Qing Lu*

Main category: stat.ME

TL;DR: 为应对高维测序数据关联分析挑战，开发加权U统计量WU - seq，经研究其表现良好并应用于实际数据检测到关联。


<details>
  <summary>Details</summary>
Motivation: 下一代测序技术产生大量数据，传统统计方法用于高维测序数据分析有较大统计功效损失，需新方法。

Method: 开发加权U统计量WU - seq，基于非参数U统计量，不假定疾病模型和表型分布。

Result: 模拟和实证研究显示，假设违背时WU - seq优于常用SKAT方法，假设满足时性能相当；应用于DHS测序数据检测到ANGPTL 4与极低密度脂蛋白胆固醇的关联。

Conclusion: WU - seq是高维测序数据关联分析的有效方法。

Abstract: With advancements in next generation sequencing technology, a massive amount
of sequencing data are generated, offering a great opportunity to
comprehensively investigate the role of rare variants in the genetic etiology
of complex diseases. Nevertheless, this poses a great challenge for the
statistical analysis of high-dimensional sequencing data. The association
analyses based on traditional statistical methods suffer substantial power loss
because of the low frequency of genetic variants and the extremely high
dimensionality of the data. We developed a weighted U statistic, referred to as
WU-seq, for the high-dimensional association analysis of sequencing data. Based
on a non-parametric U statistic, WU-SEQ makes no assumption of the underlying
disease model and phenotype distribution, and can be applied to a variety of
phenotypes. Through simulation studies and an empirical study, we showed that
WU-SEQ outperformed a commonly used SKAT method when the underlying assumptions
were violated (e.g., the phenotype followed a heavy-tailed distribution). Even
when the assumptions were satisfied, WU-SEQ still attained comparable
performance to SKAT. Finally, we applied WU-seq to sequencing data from the
Dallas Heart Study (DHS), and detected an association between ANGPTL 4 and very
low density lipoprotein cholesterol.

</details>


### [138] [Generalized Similarity U: A Non-parametric Test of Association Based on Similarity](https://arxiv.org/abs/1801.01220)
*Changshuai Wei,Qing Lu*

Main category: stat.ME

TL;DR: 提出基于相似性的广义相似性U（GSU）检验来测试复杂对象间关联，理论分析后选用拉普拉斯核提升效能和稳健性，模拟显示其优于现有方法，对ADNI数据进行全基因组扫描，识别出三个相关基因并开发C++包。


<details>
  <summary>Details</summary>
Motivation: 二代测序技术用于遗传关联研究，需解决复杂对象间关联测试的统计问题。

Method: 提出广义相似性U（GSU）检验，研究其理论性质，选用拉普拉斯核相似性提升效能和稳健性。

Result: 模拟显示GSU在效能和稳健性上优于现有方法；对ADNI数据全基因组扫描识别出三个与成像表型相关的基因。

Conclusion: GSU是测试复杂对象间关联的有效方法，可用于全基因组测序数据分析。

Abstract: Second generation sequencing technologies are being increasingly used for
genetic association studies, where the main research interest is to identify
sets of genetic variants that contribute to various phenotype. The phenotype
can be univariate disease status, multivariate responses and even
high-dimensional outcomes. Considering the genotype and phenotype as two
complex objects, this also poses a general statistical problem of testing
association between complex objects. We here proposed a similarity-based test,
generalized similarity U (GSU), that can test the association between complex
objects. We first studied the theoretical properties of the test in a general
setting and then focused on the application of the test to sequencing
association studies. Based on theoretical analysis, we proposed to use
Laplacian kernel based similarity for GSU to boost power and enhance
robustness. Through simulation, we found that GSU did have advantages over
existing methods in terms of power and robustness. We further performed a whole
genome sequencing (WGS) scan for Alzherimer Disease Neuroimaging Initiative
(ADNI) data, identifying three genes, APOE, APOC1 and TOMM40, associated with
imaging phenotype. We developed a C++ package for analysis of whole genome
sequencing data using GSU. The source codes can be downloaded at
https://github.com/changshuaiwei/gsu.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [139] [A Tight Lower Bound for Doubling Spanners](https://arxiv.org/abs/2508.11555)
*An La,Hung Le,Shay Solomon,Cuong Than,Vinayak,Shuang Yang,Tianyi Zhang*

Main category: cs.CG

TL;DR: 本文给出欧氏空间中(1+ε)-扩张器边数和轻量级的上下界，证明结果是紧的；并解决了双倍度量空间中能否达到欧氏空间上界的开放问题，给出否定答案。


<details>
  <summary>Details</summary>
Motivation: 解决双倍度量空间中能否达到欧氏空间(1+ε)-扩张器边数和轻量级的 superior bounds 的开放问题。

Method: 通过给出一个简单且紧的下界来解决开放问题。

Result: 证明欧氏空间结果是紧的；否定了双倍度量空间中能达到欧氏空间上界的可能性，表明 net - tree 扩张器及其修剪版本是最优的。

Conclusion: 在双倍度量空间中无法达到欧氏空间的 superior bounds，net - tree 扩张器及其修剪版本是最优的。

Abstract: Any $n$-point set in the $d$-dimensional Euclidean space $\mathbb{R}^d$, for
$d = O(1)$, admits a $(1+\epsilon)$-spanner with $\tilde{O}(n \cdot
\epsilon^{-d+1})$ edges and lightness $\tilde{O}(\epsilon^{-d})$, for any
$\epsilon > 0$. (The {lightness} is a normalized notion of weight, where we
divide the spanner weight by the MST weight. The $\tilde{O}$ and
$\tilde{\Omega}$ notations hide $\texttt{polylog}(\epsilon^{-1})$ terms.)
Moreover, this result is tight: For any $2 \le d = O(1)$, there exists an
$n$-point set in $\mathbb{R}^d$, for which any $(1+\epsilon)$-spanner has
$\tilde{\Omega}(n \cdot \epsilon^{-d+1})$ edges and lightness $\tilde{\Omega}(n
\cdot \epsilon^{-d})$.
  The upper bounds for Euclidean spanners rely heavily on the spatial property
of {cone partitioning} in $\mathbb{R}^d$, which does not seem to extend to the
wider family of {doubling metrics}, i.e., metric spaces of constant {doubling
dimension}. In doubling metrics, a {simple spanner construction from two
decades ago, the {net-tree spanner}}, has $\tilde{O}(n \cdot \epsilon^{-d})$
edges, and it could be transformed into a spanner of lightness $\tilde{O}(n
\cdot \epsilon^{-(d+1)})$ by pruning redundant edges. Despite a large body of
work, it has remained an open question whether the superior Euclidean bounds of
$\tilde{O}(n \cdot \epsilon^{-d+1})$ edges and lightness
$\tilde{O}(\epsilon^{-d})$ could be achieved also in doubling metrics. We
resolve this question in the negative by presenting a surprisingly simple and
tight lower bound, which shows, in particular, that the net-tree spanner and
its pruned version are both optimal.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [140] [Utilizing Vision-Language Models as Action Models for Intent Recognition and Assistance](https://arxiv.org/abs/2508.11093)
*Cesar Alan Contreras,Manolis Chiou,Alireza Rastegarpanah,Michal Szulik,Rustam Stolkin*

Main category: cs.RO

TL;DR: 提出用VLM和LLM增强GUIDER框架，筛选目标，使机器人能按需导航取物，未来将在Isaac Sim评估。


<details>
  <summary>Details</summary>
Motivation: 人类-机器人协作需要机器人快速推断用户意图、提供透明推理并协助用户达成目标。

Method: 用VLM和LLM增强GUIDER框架，形成语义先验筛选目标，视觉管道提供候选目标，VLM和LLM打分加权，超过阈值改变自主性。

Result: 可使机器人导航到指定区域并取回目标物体，能适应操作者意图变化。

Conclusion: 未来将在Isaac Sim用特定机械臂和底座评估系统，聚焦实时协助。

Abstract: Human-robot collaboration requires robots to quickly infer user intent,
provide transparent reasoning, and assist users in achieving their goals. Our
recent work introduced GUIDER, our framework for inferring navigation and
manipulation intents. We propose augmenting GUIDER with a vision-language model
(VLM) and a text-only language model (LLM) to form a semantic prior that
filters objects and locations based on the mission prompt. A vision pipeline
(YOLO for object detection and the Segment Anything Model for instance
segmentation) feeds candidate object crops into the VLM, which scores their
relevance given an operator prompt; in addition, the list of detected object
labels is ranked by a text-only LLM. These scores weight the existing
navigation and manipulation layers of GUIDER, selecting context-relevant
targets while suppressing unrelated objects. Once the combined belief exceeds a
threshold, autonomy changes occur, enabling the robot to navigate to the
desired area and retrieve the desired object, while adapting to any changes in
the operator's intent. Future work will evaluate the system on Isaac Sim using
a Franka Emika arm on a Ridgeback base, with a focus on real-time assistance.

</details>


### [141] [Actor-Critic for Continuous Action Chunks: A Reinforcement Learning Framework for Long-Horizon Robotic Manipulation with Sparse Reward](https://arxiv.org/abs/2508.11143)
*Jiarui Yang,Bin Zhu,Jingjing Chen,Yu-Gang Jiang*

Main category: cs.RO

TL;DR: 提出AC3框架解决强化学习处理长周期机器人操作任务难题，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法处理长周期机器人操作任务（特别是稀疏奖励任务）存在困难，直接用强化学习稳定且高效学习连续动作块是关键挑战。

Method: 引入AC3框架，通过为演员和评论家引入目标稳定机制使学习过程稳定高效。演员用非对称更新规则训练，仅从成功轨迹学习；评论家更新用块内n步回报稳定，并通过自监督模块提供内在奖励丰富。

Result: 在BiGym和RLBench基准的25个任务上实验，AC3用少量演示和简单模型架构在多数任务上取得更高成功率。

Conclusion: 验证了AC3设计的有效性。

Abstract: Existing reinforcement learning (RL) methods struggle with long-horizon
robotic manipulation tasks, particularly those involving sparse rewards. While
action chunking is a promising paradigm for robotic manipulation, using RL to
directly learn continuous action chunks in a stable and data-efficient manner
remains a critical challenge. This paper introduces AC3 (Actor-Critic for
Continuous Chunks), a novel RL framework that learns to generate
high-dimensional, continuous action sequences. To make this learning process
stable and data-efficient, AC3 incorporates targeted stabilization mechanisms
for both the actor and the critic. First, to ensure reliable policy
improvement, the actor is trained with an asymmetric update rule, learning
exclusively from successful trajectories. Second, to enable effective value
learning despite sparse rewards, the critic's update is stabilized using
intra-chunk $n$-step returns and further enriched by a self-supervised module
providing intrinsic rewards at anchor points aligned with each action chunk. We
conducted extensive experiments on 25 tasks from the BiGym and RLBench
benchmarks. Results show that by using only a few demonstrations and a simple
model architecture, AC3 achieves superior success rates on most tasks,
validating its effective design.

</details>


### [142] [Visuomotor Grasping with World Models for Surgical Robots](https://arxiv.org/abs/2508.11200)
*Hongbin Lin,Bin Li,Kwok Wai Samuel Au*

Main category: cs.RO

TL;DR: 提出用于手术抓取的视觉运动学习框架GASv2，可解决三个关键挑战，在实验中表现出良好性能、通用性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有手术抓取自动化方法存在泛化性、鲁棒性不足等问题，视觉运动学习在机器人辅助手术中应用有独特挑战，需解决相关问题。

Method: 引入GASv2框架，利用基于世界模型的架构和手术感知管道进行视觉观察，结合混合控制系统执行，在模拟中使用领域随机化训练策略并部署到真实机器人。

Result: 策略在两种手术设置中成功率达65%，能泛化到未见物体和夹具，适应多种干扰。

Conclusion: GASv2框架在手术抓取任务中具有强性能、通用性和鲁棒性。

Abstract: Grasping is a fundamental task in robot-assisted surgery (RAS), and
automating it can reduce surgeon workload while enhancing efficiency, safety,
and consistency beyond teleoperated systems. Most prior approaches rely on
explicit object pose tracking or handcrafted visual features, limiting their
generalization to novel objects, robustness to visual disturbances, and the
ability to handle deformable objects. Visuomotor learning offers a promising
alternative, but deploying it in RAS presents unique challenges, such as low
signal-to-noise ratio in visual observations, demands for high safety and
millimeter-level precision, as well as the complex surgical environment. This
paper addresses three key challenges: (i) sim-to-real transfer of visuomotor
policies to ex vivo surgical scenes, (ii) visuomotor learning using only a
single stereo camera pair -- the standard RAS setup, and (iii) object-agnostic
grasping with a single policy that generalizes to diverse, unseen surgical
objects without retraining or task-specific models. We introduce Grasp Anything
for Surgery V2 (GASv2), a visuomotor learning framework for surgical grasping.
GASv2 leverages a world-model-based architecture and a surgical perception
pipeline for visual observations, combined with a hybrid control system for
safe execution. We train the policy in simulation using domain randomization
for sim-to-real transfer and deploy it on a real robot in both phantom-based
and ex vivo surgical settings, using only a single pair of endoscopic cameras.
Extensive experiments show our policy achieves a 65% success rate in both
settings, generalizes to unseen objects and grippers, and adapts to diverse
disturbances, demonstrating strong performance, generality, and robustness.

</details>


### [143] [Multi-Group Equivariant Augmentation for Reinforcement Learning in Robot Manipulation](https://arxiv.org/abs/2508.11204)
*Hongbin Lin,Juan Rojas,Kwok Wai Samuel Au*

Main category: cs.RO

TL;DR: 本文探索非等距对称性以提升视觉运动学习在机器人操作中的采样效率，提出新方法并经实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有基于任务对称性提升采样效率的工作多局限于等距对称性，需探索非等距对称性来突破限制。

Method: 引入包含非等距对称结构的POMDP公式，提出多组等变增强（MEA）数据增强方法，将MEA与离线强化学习结合，引入基于体素的视觉表示。

Result: 在两个操作领域的大量模拟和真实机器人实验证明了方法的有效性。

Conclusion: 所提出的利用非等距对称性的方法能有效提升机器人操作中视觉运动学习的采样效率。

Abstract: Sampling efficiency is critical for deploying visuomotor learning in
real-world robotic manipulation. While task symmetry has emerged as a promising
inductive bias to improve efficiency, most prior work is limited to isometric
symmetries -- applying the same group transformation to all task objects across
all timesteps. In this work, we explore non-isometric symmetries, applying
multiple independent group transformations across spatial and temporal
dimensions to relax these constraints. We introduce a novel formulation of the
partially observable Markov decision process (POMDP) that incorporates the
non-isometric symmetry structures, and propose a simple yet effective data
augmentation method, Multi-Group Equivariance Augmentation (MEA). We integrate
MEA with offline reinforcement learning to enhance sampling efficiency, and
introduce a voxel-based visual representation that preserves translational
equivariance. Extensive simulation and real-robot experiments across two
manipulation domains demonstrate the effectiveness of our approach.

</details>


### [144] [Scene Graph-Guided Proactive Replanning for Failure-Resilient Embodied Agent](https://arxiv.org/abs/2508.11286)
*Che Rin Yu,Daewon Chae,Dabin Seo,Sangwon Lee,Hyeongwoo Im,Jinkyu Kim*

Main category: cs.RO

TL;DR: 提出一种主动重新规划框架，通过比较场景图检测和纠正子任务边界的失败，在AI2 - THOR模拟器实验中提升了任务成功率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 许多自主机器人缺乏自适应意识，现有重新规划方法多在失败后响应，主动重新规划方法依赖手动规则和大量监督。

Method: 构建当前RGB - D观测的场景图并与成功演示中提取的参考图比较，在场景不匹配时激活轻量级推理模块诊断并调整计划。

Result: 在AI2 - THOR模拟器中能在执行失败前检测语义和空间不匹配。

Conclusion: 该方法显著提高了任务成功率和鲁棒性。

Abstract: When humans perform everyday tasks, we naturally adjust our actions based on
the current state of the environment. For instance, if we intend to put
something into a drawer but notice it is closed, we open it first. However,
many autonomous robots lack this adaptive awareness. They often follow
pre-planned actions that may overlook subtle yet critical changes in the scene,
which can result in actions being executed under outdated assumptions and
eventual failure. While replanning is critical for robust autonomy, most
existing methods respond only after failures occur, when recovery may be
inefficient or infeasible. While proactive replanning holds promise for
preventing failures in advance, current solutions often rely on manually
designed rules and extensive supervision. In this work, we present a proactive
replanning framework that detects and corrects failures at subtask boundaries
by comparing scene graphs constructed from current RGB-D observations against
reference graphs extracted from successful demonstrations. When the current
scene fails to align with reference trajectories, a lightweight reasoning
module is activated to diagnose the mismatch and adjust the plan. Experiments
in the AI2-THOR simulator demonstrate that our approach detects semantic and
spatial mismatches before execution failures occur, significantly improving
task success and robustness.

</details>


### [145] [An Exploratory Study on Crack Detection in Concrete through Human-Robot Collaboration](https://arxiv.org/abs/2508.11404)
*Junyeon Kim,Tianshu Ruan,Cesar Alan Contreras,Manolis Chiou*

Main category: cs.RO

TL;DR: 核设施结构检查重要，传统人工检查有挑战，研究AI辅助移动机器人视觉裂纹检测，结果显示人机协作提升检查效果并减轻操作员负担。


<details>
  <summary>Details</summary>
Motivation: 传统核设施人工检查存在安全风险、认知要求高和潜在不准确等问题，需要更安全、高效和准确的检查方法。

Method: 探索将AI辅助视觉裂纹检测集成到移动Jackal机器人平台。

Result: 人机协作提高了检查准确性，减少了操作员工作量。

Conclusion: 人机协作在核设施结构检查中相比传统人工方法可能有更优表现。

Abstract: Structural inspection in nuclear facilities is vital for maintaining
operational safety and integrity. Traditional methods of manual inspection pose
significant challenges, including safety risks, high cognitive demands, and
potential inaccuracies due to human limitations. Recent advancements in
Artificial Intelligence (AI) and robotic technologies have opened new
possibilities for safer, more efficient, and accurate inspection methodologies.
Specifically, Human-Robot Collaboration (HRC), leveraging robotic platforms
equipped with advanced detection algorithms, promises significant improvements
in inspection outcomes and reductions in human workload. This study explores
the effectiveness of AI-assisted visual crack detection integrated into a
mobile Jackal robot platform. The experiment results indicate that HRC enhances
inspection accuracy and reduces operator workload, resulting in potential
superior performance outcomes compared to traditional manual methods.

</details>


### [146] [Open, Reproducible and Trustworthy Robot-Based Experiments with Virtual Labs and Digital-Twin-Based Execution Tracing](https://arxiv.org/abs/2508.11406)
*Benjamin Alt,Mareike Picklum,Sorin Arion,Franklin Kenghagho Kenfack,Michael Beetz*

Main category: cs.RO

TL;DR: 提出语义执行跟踪框架和AICOR虚拟研究大楼平台，使机器人驱动的科学实验可复现，为自主系统参与科学发现奠定基础。


<details>
  <summary>Details</summary>
Motivation: 实现自主机器人以开放、可信和透明的方式进行科学实验的愿景。

Method: 提出语义执行跟踪框架记录传感器数据和机器人信念状态；构建AICOR虚拟研究大楼平台用于大规模共享、复制和验证机器人任务执行。

Result: 通过整合确定性执行、语义记忆和开放知识表示，实现可复现的机器人驱动科学。

Conclusion: 这些工具为自主系统参与科学发现奠定了基础。

Abstract: We envision a future in which autonomous robots conduct scientific
experiments in ways that are not only precise and repeatable, but also open,
trustworthy, and transparent. To realize this vision, we present two key
contributions: a semantic execution tracing framework that logs sensor data
together with semantically annotated robot belief states, ensuring that
automated experimentation is transparent and replicable; and the AICOR Virtual
Research Building (VRB), a cloud-based platform for sharing, replicating, and
validating robot task executions at scale. Together, these tools enable
reproducible, robot-driven science by integrating deterministic execution,
semantic memory, and open knowledge representation, laying the foundation for
autonomous systems to participate in scientific discovery.

</details>


### [147] [Sim2Dust: Mastering Dynamic Waypoint Tracking on Granular Media](https://arxiv.org/abs/2508.11503)
*Andrej Orsula,Matthieu Geist,Miguel Olivares-Mendez,Carol Martinez*

Main category: cs.RO

TL;DR: 提出用于复杂行星表面动态航点跟踪的从仿真到现实的框架，通过并行仿真训练强化学习智能体并零样本迁移到真实漫游车，对比多种算法和滤波器，证明程序多样性训练的优势及微调的权衡。


<details>
  <summary>Details</summary>
Motivation: 解决学习型控制器在行星表面部署时存在的仿真与现实差距问题，特别是车轮与颗粒介质相互作用的复杂动力学问题。

Method: 利用大规模并行仿真，在随机物理参数的程序生成环境中训练强化学习智能体，将策略零样本迁移到物理漫游车，系统比较多种强化学习算法和动作平滑滤波器。

Result: 程序多样性训练的智能体零样本性能优于静态场景训练的；微调高保真粒子物理在低速精度上有小提升，但计算成本高。

Conclusion: 建立了创建可靠的基于学习的导航系统的有效工作流程，是在太空部署自主机器人的关键一步。

Abstract: Reliable autonomous navigation across the unstructured terrains of distant
planetary surfaces is a critical enabler for future space exploration. However,
the deployment of learning-based controllers is hindered by the inherent
sim-to-real gap, particularly for the complex dynamics of wheel interactions
with granular media. This work presents a complete sim-to-real framework for
developing and validating robust control policies for dynamic waypoint tracking
on such challenging surfaces. We leverage massively parallel simulation to
train reinforcement learning agents across a vast distribution of procedurally
generated environments with randomized physics. These policies are then
transferred zero-shot to a physical wheeled rover operating in a lunar-analogue
facility. Our experiments systematically compare multiple reinforcement
learning algorithms and action smoothing filters to identify the most effective
combinations for real-world deployment. Crucially, we provide strong empirical
evidence that agents trained with procedural diversity achieve superior
zero-shot performance compared to those trained on static scenarios. We also
analyze the trade-offs of fine-tuning with high-fidelity particle physics,
which offers minor gains in low-speed precision at a significant computational
cost. Together, these contributions establish a validated workflow for creating
reliable learning-based navigation systems, marking a critical step towards
deploying autonomous robots in the final frontier.

</details>


### [148] [Visual Perception Engine: Fast and Flexible Multi-Head Inference for Robotic Vision Tasks](https://arxiv.org/abs/2508.11584)
*Jakub Łucki,Jonathan Becktor,Georgios Georgakis,Robert Royce,Shehryar Khattak*

Main category: cs.RO

TL;DR: 提出视觉感知引擎VPEngine框架，可高效利用GPU进行视觉多任务处理，相比顺序执行提速3倍，在NVIDIA Jetson Orin AGX上实现端到端实时性能。


<details>
  <summary>Details</summary>
Motivation: 解决在资源受限的机器人平台上部署多个机器学习模型进行不同感知任务时存在的计算冗余、内存占用大及集成复杂等问题。

Method: 采用共享基础模型骨干提取图像表示，跨多个并行的特定任务模型头高效共享，基于CUDA Multi - Process Service (MPS)，允许运行时动态调整每个任务的推理频率。

Result: 以DINOv2为基础模型实现示例，相比顺序执行提速3倍，在NVIDIA Jetson Orin AGX上TensorRT优化模型端到端实时性能≥50 Hz。

Conclusion: VPEngine框架能高效利用GPU，保持恒定内存占用，具有可扩展性和开发者易访问性，适用于不同机器人平台。

Abstract: Deploying multiple machine learning models on resource-constrained robotic
platforms for different perception tasks often results in redundant
computations, large memory footprints, and complex integration challenges. In
response, this work presents Visual Perception Engine (VPEngine), a modular
framework designed to enable efficient GPU usage for visual multitasking while
maintaining extensibility and developer accessibility. Our framework
architecture leverages a shared foundation model backbone that extracts image
representations, which are efficiently shared, without any unnecessary GPU-CPU
memory transfers, across multiple specialized task-specific model heads running
in parallel. This design eliminates the computational redundancy inherent in
feature extraction component when deploying traditional sequential models while
enabling dynamic task prioritization based on application demands. We
demonstrate our framework's capabilities through an example implementation
using DINOv2 as the foundation model with multiple task (depth, object
detection and semantic segmentation) heads, achieving up to 3x speedup compared
to sequential execution. Building on CUDA Multi-Process Service (MPS), VPEngine
offers efficient GPU utilization and maintains a constant memory footprint
while allowing per-task inference frequencies to be adjusted dynamically during
runtime. The framework is written in Python and is open source with ROS2 C++
(Humble) bindings for ease of use by the robotics community across diverse
robotic platforms. Our example implementation demonstrates end-to-end real-time
performance at $\geq$50 Hz on NVIDIA Jetson Orin AGX for TensorRT optimized
models.

</details>


### [149] [Investigating Sensors and Methods in Grasp State Classification in Agricultural Manipulation](https://arxiv.org/abs/2508.11588)
*Benjamin Walt,Jordan Westphal,Girish Krishnan*

Main category: cs.RO

TL;DR: 本文研究用多种传感器集成到柔顺夹爪中对抓取状态分类，比较不同模型，发现随机森林分类器表现佳，还确定了最小可行传感器组合，提升水果采摘效率。


<details>
  <summary>Details</summary>
Motivation: 农业环境复杂，水果采摘需准确理解抓取状态，选择合适传感器和建模技术获取可靠反馈。

Method: 将IMU、红外反射、张力、触觉传感器和RGB相机集成到柔顺夹爪中，评估各传感器贡献，比较随机森林和LSTM网络两种分类模型。

Result: 在实验室环境训练的随机森林分类器在真实樱桃番茄植株上测试，识别滑移、抓取失败和成功采摘的准确率达100%，优于基线表现，确定IMU和张力传感器为最小可行组合。

Conclusion: 该分类器可根据实时反馈规划纠正措施，提高水果采摘操作的效率和可靠性。

Abstract: Effective and efficient agricultural manipulation and harvesting depend on
accurately understanding the current state of the grasp. The agricultural
environment presents unique challenges due to its complexity, clutter, and
occlusion. Additionally, fruit is physically attached to the plant, requiring
precise separation during harvesting. Selecting appropriate sensors and
modeling techniques is critical for obtaining reliable feedback and correctly
identifying grasp states. This work investigates a set of key sensors, namely
inertial measurement units (IMUs), infrared (IR) reflectance, tension, tactile
sensors, and RGB cameras, integrated into a compliant gripper to classify grasp
states. We evaluate the individual contribution of each sensor and compare the
performance of two widely used classification models: Random Forest and Long
Short-Term Memory (LSTM) networks. Our results demonstrate that a Random Forest
classifier, trained in a controlled lab environment and tested on real cherry
tomato plants, achieved 100% accuracy in identifying slip, grasp failure, and
successful picks, marking a substantial improvement over baseline performance.
Furthermore, we identify a minimal viable sensor combination, namely IMU and
tension sensors that effectively classifies grasp states. This classifier
enables the planning of corrective actions based on real-time feedback, thereby
enhancing the efficiency and reliability of fruit harvesting operations.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [150] [TrajSV: A Trajectory-based Model for Sports Video Representations and Applications](https://arxiv.org/abs/2508.11569)
*Zheng Wang,Shihao Xu,Wei Shi*

Main category: cs.CV

TL;DR: 本文提出TrajSV框架解决体育分析现存问题，含数据预处理、CRNet和VRNet组件，在三个体育视频数据集实验，表现优异并部署系统。


<details>
  <summary>Details</summary>
Motivation: 体育分析领域存在数据不可用、缺乏有效轨迹框架和监督标签不足等问题。

Method: 提出TrajSV框架，包含数据预处理、CRNet和VRNet组件，引入三重对比损失无监督优化。

Result: 在体育视频检索中提升近70%，动作识别在9/17类别达最优，视频字幕提升近20%。

Conclusion: TrajSV框架有效解决现存问题，在多项体育视频应用中表现出色。

Abstract: Sports analytics has received significant attention from both academia and
industry in recent years. Despite the growing interest and efforts in this
field, several issues remain unresolved, including (1) data unavailability, (2)
lack of an effective trajectory-based framework, and (3) requirement for
sufficient supervision labels. In this paper, we present TrajSV, a
trajectory-based framework that addresses various issues in existing studies.
TrajSV comprises three components: data preprocessing, Clip Representation
Network (CRNet), and Video Representation Network (VRNet). The data
preprocessing module extracts player and ball trajectories from sports
broadcast videos. CRNet utilizes a trajectory-enhanced Transformer module to
learn clip representations based on these trajectories. Additionally, VRNet
learns video representations by aggregating clip representations and visual
features with an encoder-decoder architecture. Finally, a triple contrastive
loss is introduced to optimize both video and clip representations in an
unsupervised manner. The experiments are conducted on three broadcast video
datasets to verify the effectiveness of TrajSV for three types of sports (i.e.,
soccer, basketball, and volleyball) with three downstream applications (i.e.,
sports video retrieval, action spotting, and video captioning). The results
demonstrate that TrajSV achieves state-of-the-art performance in sports video
retrieval, showcasing a nearly 70% improvement. It outperforms baselines in
action spotting, achieving state-of-the-art results in 9 out of 17 action
categories, and demonstrates a nearly 20% improvement in video captioning.
Additionally, we introduce a deployed system along with the three applications
based on TrajSV.

</details>


### [151] [ORBIT: An Object Property Reasoning Benchmark for Visual Inference Tasks](https://arxiv.org/abs/2508.10956)
*Abhishek Kolari,Mohammadhossein Khojasteh,Yifan Jiang,Floris den Hengst,Filip Ilievski*

Main category: cs.CV

TL;DR: 本文引入ORBIT基准测试评估视觉语言模型对象属性推理能力，实验显示模型有显著局限，需开发新方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉问答基准测试在推理和图像类别上缺乏代表性，不清楚视觉语言模型能否对描绘对象进行抽象和推理。

Method: 引入包含三种代表性图像类型、三种复杂度推理层次和四个对象属性维度的评估框架，创建ORBIT基准测试。

Result: 12个最先进的视觉语言模型在零样本设置下与人类相比有显著局限，最佳模型准确率仅40%，模型在真实图像、反事实推理和高计数问题上表现不佳。

Conclusion: 需要开发可扩展基准测试方法、通用注释指南，探索更多推理视觉语言模型，并公开了基准测试和实验代码。

Abstract: While vision-language models (VLMs) have made remarkable progress on many
popular visual question answering (VQA) benchmarks, it remains unclear whether
they abstract and reason over depicted objects. Inspired by human object
categorisation, object property reasoning involves identifying and recognising
low-level details and higher-level abstractions. While current VQA benchmarks
consider a limited set of object property attributes like size, they typically
blend perception and reasoning, and lack representativeness in terms of
reasoning and image categories. To this end, we introduce a systematic
evaluation framework with images of three representative types, three reasoning
levels of increasing complexity, and four object property dimensions driven by
prior work on commonsense reasoning. We develop a procedure to instantiate this
benchmark into ORBIT, a multi-level reasoning VQA benchmark for object
properties comprising 360 images paired with a total of 1,080 count-based
questions. Experiments with 12 state-of-the-art VLMs in zero-shot settings
reveal significant limitations compared to humans, with the best-performing
model only reaching 40\% accuracy. VLMs struggle particularly with realistic
(photographic) images, counterfactual reasoning about physical and functional
properties, and higher counts. ORBIT points to the need to develop methods for
scalable benchmarking, generalize annotation guidelines, and explore additional
reasoning VLMs. We make the ORBIT benchmark and the experimental code available
to support such endeavors.

</details>


### [152] [Not There Yet: Evaluating Vision Language Models in Simulating the Visual Perception of People with Low Vision](https://arxiv.org/abs/2508.10972)
*Rosiana Natalie,Wenqian Xu,Ruei-Che Chang,Rada Mihalcea,Anhong Guo*

Main category: cs.CV

TL;DR: 本文评估视觉语言模型（VLMs）模拟低视力个体视觉感知的能力，发现结合视力信息和示例图像响应能显著提高模拟结果与参与者原始答案的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有研究未在无障碍领域探究VLMs的模拟能力，本文旨在评估其模拟低视力个体视觉感知的程度。

Method: 通过对40名低视力参与者的调查研究编制基准数据集，构建VLMs（GPT - 4o）的提示以创建每个参与者的模拟代理，评估VLM生成的响应与参与者原始答案的一致性。

Result: 仅给予最小提示时，VLMs倾向于超出指定视力能力进行推断，一致性低（0.59）；仅提供视力信息或示例图像响应时一致性仍低（0.59），结合两者显著提高一致性（0.70，p < 0.0001）；结合开放式和多项选择响应的单个示例比单独使用效果好（p < 0.0001），额外示例益处不大（p > 0.05）。

Conclusion: 结合视力信息和示例图像响应能有效提高VLMs模拟低视力个体视觉感知的准确性。

Abstract: Advances in vision language models (VLMs) have enabled the simulation of
general human behavior through their reasoning and problem solving
capabilities. However, prior research has not investigated such simulation
capabilities in the accessibility domain. In this paper, we evaluate the extent
to which VLMs can simulate the vision perception of low vision individuals when
interpreting images. We first compile a benchmark dataset through a survey
study with 40 low vision participants, collecting their brief and detailed
vision information and both open-ended and multiple-choice image perception and
recognition responses to up to 25 images. Using these responses, we construct
prompts for VLMs (GPT-4o) to create simulated agents of each participant,
varying the included information on vision information and example image
responses. We evaluate the agreement between VLM-generated responses and
participants' original answers. Our results indicate that VLMs tend to infer
beyond the specified vision ability when given minimal prompts, resulting in
low agreement (0.59). The agreement between the agent' and participants'
responses remains low when only either the vision information (0.59) or example
image responses (0.59) are provided, whereas a combination of both
significantly increase the agreement (0.70, p < 0.0001). Notably, a single
example combining both open-ended and multiple-choice responses, offers
significant performance improvements over either alone (p < 0.0001), while
additional examples provided minimal benefits (p > 0.05).

</details>


### [153] [A Cross-Modal Rumor Detection Scheme via Contrastive Learning by Exploring Text and Image internal Correlations](https://arxiv.org/abs/2508.11141)
*Bin Ma,Yifei Zhang,Yongjin Xian,Qi Li,Linna Zhou,Gongxun Miao*

Main category: cs.CV

TL;DR: 文章提出基于对比学习的跨模态谣言检测方案MICC，在真实数据集上实验表明其比现有方法性能有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有谣言检测方法忽略图像内容以及不同视觉尺度上下文与图像的内在关系，导致关键信息丢失。

Method: 设计SCLIP编码器生成文本和多尺度图像块的统一语义嵌入；引入跨模态多尺度对齐模块，通过互信息最大化和信息瓶颈原则及Top - K选择策略识别与文本语义最相关的图像区域；设计尺度感知融合网络，根据图像区域语义重要性和跨模态相关性分配自适应权重，融合多尺度图像特征和全局文本特征。

Result: 在两个真实数据集上进行评估，相比现有最先进方法，在谣言检测上性能有显著提升。

Conclusion: 所提方法有效，有实际应用潜力。

Abstract: Existing rumor detection methods often neglect the content within images as
well as the inherent relationships between contexts and images across different
visual scales, thereby resulting in the loss of critical information pertinent
to rumor identification. To address these issues, this paper presents a novel
cross-modal rumor detection scheme based on contrastive learning, namely the
Multi-scale Image and Context Correlation exploration algorithm (MICC).
Specifically, we design an SCLIP encoder to generate unified semantic
embeddings for text and multi-scale image patches through contrastive
pretraining, enabling their relevance to be measured via dot-product
similarity. Building upon this, a Cross-Modal Multi-Scale Alignment module is
introduced to identify image regions most relevant to the textual semantics,
guided by mutual information maximization and the information bottleneck
principle, through a Top-K selection strategy based on a cross-modal relevance
matrix constructed between the text and multi-scale image patches. Moreover, a
scale-aware fusion network is designed to integrate the highly correlated
multi-scale image features with global text features by assigning adaptive
weights to image regions based on their semantic importance and cross-modal
relevance. The proposed methodology has been extensively evaluated on two
real-world datasets. The experimental results demonstrate that it achieves a
substantial performance improvement over existing state-of-the-art approaches
in rumor detection, highlighting its effectiveness and potential for practical
applications.

</details>


### [154] [Better Supervised Fine-tuning for VQA: Integer-Only Loss](https://arxiv.org/abs/2508.11170)
*Baihong Qian,Haotian Fan,Wenjie Liao,Yunqiu Wang,Tao Li,Junhui Cui*

Main category: cs.CV

TL;DR: 提出IOVQA方法提升VLM在视频质量评估任务的性能，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有方法在视频质量评估中结果不精确、损失计算效率低，限制模型关注关键评估指标。

Method: 提出IOVQA方法，在数据集整理时将模型输出限制在[10,50]的整数，将十进制Overall_MOS转换为整数作标签，引入目标掩码策略。

Result: 在VQA任务中显著提高模型准确性和一致性，在VQualA 2025挑战赛Track I中排名第3。

Conclusion: 微调时仅保留整数标签有效，为定量评估场景下优化VLM提供有效思路。

Abstract: With the rapid advancement of vision language models(VLM), their ability to
assess visual content based on specific criteria and dimensions has become
increasingly critical for applications such as video-theme consistency
assessment and visual quality scoring. However, existing methods often suffer
from imprecise results and inefficient loss calculation, which limit the focus
of the model on key evaluation indicators. To address this, we propose
IOVQA(Integer-only VQA), a novel fine-tuning approach tailored for VLMs to
enhance their performance in video quality assessment tasks. The key innovation
of IOVQA lies in its label construction and its targeted loss calculation
mechanism. Specifically, during dataset curation, we constrain the model's
output to integers within the range of [10,50], ensuring numerical stability,
and convert decimal Overall_MOS to integer before using them as labels. We also
introduce a target-mask strategy: when computing the loss, only the first
two-digit-integer of the label is unmasked, forcing the model to learn the
critical components of the numerical evaluation. After fine-tuning the
Qwen2.5-VL model using the constructed dataset, experimental results
demonstrate that the proposed method significantly improves the model's
accuracy and consistency in the VQA task, ranking 3rd in VQualA 2025
GenAI-Bench AIGC Video Quality Assessment Challenge -- Track I. Our work
highlights the effectiveness of merely leaving integer labels during
fine-tuning, providing an effective idea for optimizing VLMs in quantitative
evaluation scenarios.

</details>


### [155] [Generalized Decoupled Learning for Enhancing Open-Vocabulary Dense Perception](https://arxiv.org/abs/2508.11256)
*Junjie Wang,Keyu Chen,Yulin Li,Bin Chen,Hengshuang Zhao,Xiaojuan Qi,Zhuotao Tian*

Main category: cs.CV

TL;DR: 提出DeCLIP框架增强CLIP用于开放词汇密集感知任务，实验表现佳。


<details>
  <summary>Details</summary>
Motivation: 现有密集视觉感知任务依赖预定义类别，CLIP直接用于密集感知性能不佳，其图像令牌聚合信息能力弱。

Method: 提出DeCLIP框架，解耦自注意力模块得到内容和上下文特征，分别增强空间一致性和局部判别性。

Result: DeCLIP在2D检测和分割、3D实例分割等多种任务中达到了最先进的性能。

Conclusion: DeCLIP为开放词汇密集感知奠定了坚实基础。

Abstract: Dense visual perception tasks have been constrained by their reliance on
predefined categories, limiting their applicability in real-world scenarios
where visual concepts are unbounded. While Vision-Language Models (VLMs) like
CLIP have shown promise in open-vocabulary tasks, their direct application to
dense perception often leads to suboptimal performance due to limitations in
local feature representation. In this work, we present our observation that
CLIP's image tokens struggle to effectively aggregate information from
spatially or semantically related regions, resulting in features that lack
local discriminability and spatial consistency. To address this issue, we
propose DeCLIP, a novel framework that enhances CLIP by decoupling the
self-attention module to obtain ``content'' and ``context'' features
respectively. \revise{The context features are enhanced by jointly distilling
semantic correlations from Vision Foundation Models (VFMs) and object integrity
cues from diffusion models, thereby enhancing spatial consistency. In parallel,
the content features are aligned with image crop representations and
constrained by region correlations from VFMs to improve local discriminability.
Extensive experiments demonstrate that DeCLIP establishes a solid foundation
for open-vocabulary dense perception, consistently achieving state-of-the-art
performance across a broad spectrum of tasks, including 2D detection and
segmentation, 3D instance segmentation, video instance segmentation, and 6D
object pose estimation.} Code is available at
https://github.com/xiaomoguhz/DeCLIP

</details>


### [156] [Vision-Language Models display a strong gender bias](https://arxiv.org/abs/2508.11262)
*Aiswarya Konavoor,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.CV

TL;DR: 研究对比视觉 - 语言编码器是否存在性别关联，构建数据集计算关联得分并进行评估，得到性别关联图谱和评估框架。


<details>
  <summary>Details</summary>
Motivation: 视觉 - 语言模型在对齐图像和文本时可能会以微妙方式编码和放大社会刻板印象，需测试对比视觉 - 语言编码器是否存在性别关联。

Method: 组装人脸照片数据集和语句集，计算图像和文本嵌入，定义关联得分，通过重采样添加置信区间，运行标签交换零模型。

Result: 得到对比视觉 - 语言空间中按语句和类别划分的性别关联图谱，附带不确定性、简单合理性检查和鲁棒的性别偏差评估框架。

Conclusion: 提出了一种评估对比视觉 - 语言编码器中性别关联和偏差的方法和框架。

Abstract: Vision-language models (VLM) align images and text in a shared representation
space that is useful for retrieval and zero-shot transfer. Yet, this alignment
can encode and amplify social stereotypes in subtle ways that are not obvious
from standard accuracy metrics. In this study, we test whether the contrastive
vision-language encoder exhibits gender-linked associations when it places
embeddings of face images near embeddings of short phrases that describe
occupations and activities. We assemble a dataset of 220 face photographs split
by perceived binary gender and a set of 150 unique statements distributed
across six categories covering emotional labor, cognitive labor, domestic
labor, technical labor, professional roles, and physical labor. We compute
unit-norm image embeddings for every face and unit-norm text embeddings for
every statement, then define a statement-level association score as the
difference between the mean cosine similarity to the male set and the mean
cosine similarity to the female set, where positive values indicate stronger
association with the male set and negative values indicate stronger association
with the female set. We attach bootstrap confidence intervals by resampling
images within each gender group, aggregate by category with a separate
bootstrap over statements, and run a label-swap null model that estimates the
level of mean absolute association we would expect if no gender structure were
present. The outcome is a statement-wise and category-wise map of gender
associations in a contrastive vision-language space, accompanied by
uncertainty, simple sanity checks, and a robust gender bias evaluation
framework.

</details>


### [157] [Enhancing Supervised Composed Image Retrieval via Reasoning-Augmented Representation Engineering](https://arxiv.org/abs/2508.11272)
*Jun Li,Kai Li,Shaoguo Liu,Tingting Gao*

Main category: cs.CV

TL;DR: 提出包含PMTFR的框架解决CIR挑战，在监督CIR任务超SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有CIR方法两阶段需额外训练排序模型，CoT技术在CIR应用有限，难在监督CIR获满意结果。

Method: 提出PMTFR框架，用Pyramid Patcher增强模型对不同粒度视觉信息理解，从COT数据提取表示注入LVLMs获精炼检索分数。

Result: 在CIR基准上的大量实验表明，PMTFR在监督CIR任务中超越了现有最先进的方法。

Conclusion: 所提出的框架有效解决了CIR任务中的挑战，提升了监督CIR任务的性能。

Abstract: Composed Image Retrieval (CIR) presents a significant challenge as it
requires jointly understanding a reference image and a modified textual
instruction to find relevant target images. Some existing methods attempt to
use a two-stage approach to further refine retrieval results. However, this
often requires additional training of a ranking model. Despite the success of
Chain-of-Thought (CoT) techniques in reducing training costs for language
models, their application in CIR tasks remains limited -- compressing visual
information into text or relying on elaborate prompt designs. Besides, existing
works only utilize it for zero-shot CIR, as it is challenging to achieve
satisfactory results in supervised CIR with a well-trained model. In this work,
we proposed a framework that includes the Pyramid Matching Model with
Training-Free Refinement (PMTFR) to address these challenges. Through a simple
but effective module called Pyramid Patcher, we enhanced the Pyramid Matching
Model's understanding of visual information at different granularities.
Inspired by representation engineering, we extracted representations from COT
data and injected them into the LVLMs. This approach allowed us to obtain
refined retrieval scores in the Training-Free Refinement paradigm without
relying on explicit textual reasoning, further enhancing performance. Extensive
experiments on CIR benchmarks demonstrate that PMTFR surpasses state-of-the-art
methods in supervised CIR tasks. The code will be made public.

</details>


### [158] [HQ-OV3D: A High Box Quality Open-World 3D Detection Framework based on Diffision Model](https://arxiv.org/abs/2508.10935)
*Qi Liu,Yabei Li,Hongsong Wang,Lei He*

Main category: cs.CV

TL;DR: 传统3D检测框架无法满足开放世界应用需求，现有方法忽略伪标签几何质量。提出HQ - OV3D框架生成和优化伪标签，提升了在新类别上的mAP，可作独立检测器或插件。


<details>
  <summary>Details</summary>
Motivation: 传统封闭集3D检测框架无法满足开放世界应用需求，现有开放词汇3D检测方法忽略伪标签几何质量。

Method: 提出HQ - OV3D框架，包含IMCV Proposal Generator生成高质量初始3D提案，ACA Denoiser通过基于DDIM的去噪机制优化3D提案。

Result: 与现有方法相比，用该方法生成的伪标签训练在新类别上mAP提升7.37%。

Conclusion: HQ - OV3D可作独立开放词汇3D检测器，也可作现有开放词汇检测或标注流程的高质量伪标签生成插件。

Abstract: Traditional closed-set 3D detection frameworks fail to meet the demands of
open-world applications like autonomous driving. Existing open-vocabulary 3D
detection methods typically adopt a two-stage pipeline consisting of
pseudo-label generation followed by semantic alignment. While vision-language
models (VLMs) recently have dramatically improved the semantic accuracy of
pseudo-labels, their geometric quality, particularly bounding box precision,
remains commonly neglected.To address this issue, we propose a High Box Quality
Open-Vocabulary 3D Detection (HQ-OV3D) framework, dedicated to generate and
refine high-quality pseudo-labels for open-vocabulary classes. The framework
comprises two key components: an Intra-Modality Cross-Validated (IMCV) Proposal
Generator that utilizes cross-modality geometric consistency to generate
high-quality initial 3D proposals, and an Annotated-Class Assisted (ACA)
Denoiser that progressively refines 3D proposals by leveraging geometric priors
from annotated categories through a DDIM-based denoising mechanism.Compared to
the state-of-the-art method, training with pseudo-labels generated by our
approach achieves a 7.37% improvement in mAP on novel classes, demonstrating
the superior quality of the pseudo-labels produced by our framework. HQ-OV3D
can serve not only as a strong standalone open-vocabulary 3D detector but also
as a plug-in high-quality pseudo-label generator for existing open-vocabulary
detection or annotation pipelines.

</details>


### [159] [iWatchRoad: Scalable Detection and Geospatial Visualization of Potholes for Smart Cities](https://arxiv.org/abs/2508.10945)
*Rishi Raj Sahoo,Surbhi Saswati Mohanty,Subhankar Mishra*

Main category: cs.CV

TL;DR: 本文提出用于自动检测坑洼的iWatchRoad系统，利用自标注数据集微调YOLO模型，结合OCR模块和GPS日志实现坑洼检测、标记和实时映射，该系统高效且适用于发展中地区道路管理。


<details>
  <summary>Details</summary>
Motivation: 道路坑洼对道路安全和车辆寿命构成威胁，尤其是印度道路多样且维护不足，需要自动化坑洼检测系统。

Method: 创建超7000帧自标注数据集，微调YOLO模型进行实时坑洼检测；用自定义OCR模块提取时间戳，与GPS日志同步以精确地理标记；将处理数据存储在数据库，通过OSM在用户界面可视化。

Result: iWatchRoad提高了复杂条件下的检测精度，提供政府兼容的道路评估和维护规划输出。

Conclusion: iWatchRoad成本低、硬件高效、可扩展，是发展中地区城乡道路管理的实用工具。

Abstract: Potholes on the roads are a serious hazard and maintenance burden. This poses
a significant threat to road safety and vehicle longevity, especially on the
diverse and under-maintained roads of India. In this paper, we present a
complete end-to-end system called iWatchRoad for automated pothole detection,
Global Positioning System (GPS) tagging, and real time mapping using
OpenStreetMap (OSM). We curated a large, self-annotated dataset of over 7,000
frames captured across various road types, lighting conditions, and weather
scenarios unique to Indian environments, leveraging dashcam footage. This
dataset is used to fine-tune, Ultralytics You Only Look Once (YOLO) model to
perform real time pothole detection, while a custom Optical Character
Recognition (OCR) module was employed to extract timestamps directly from video
frames. The timestamps are synchronized with GPS logs to geotag each detected
potholes accurately. The processed data includes the potholes' details and
frames as metadata is stored in a database and visualized via a user friendly
web interface using OSM. iWatchRoad not only improves detection accuracy under
challenging conditions but also provides government compatible outputs for road
assessment and maintenance planning through the metadata visible on the
website. Our solution is cost effective, hardware efficient, and scalable,
offering a practical tool for urban and rural road management in developing
regions, making the system automated. iWatchRoad is available at
https://smlab.niser.ac.in/project/iwatchroad

</details>


### [160] [Leveraging the RETFound foundation model for optic disc segmentation in retinal images](https://arxiv.org/abs/2508.11354)
*Zhenyi Zhao,Muthu Rama Krishnan Mookiah,Emanuele Trucco*

Main category: cs.CV

TL;DR: 本文首次将RETFound用于视盘分割任务，用少量特定任务示例训练头部后，分割系统优于现有基线网络，在多个数据集上Dice系数达约96%，性能出色。


<details>
  <summary>Details</summary>
Motivation: RETFound虽在视网膜图像疾病诊断中表现良好，但未用于其他任务，作者希望将其用于视盘分割这一基础任务。

Method: 对RETFound进行适配用于视盘分割，用少量特定任务示例训练头部。

Result: 在四个公共数据集和一个私有数据集上，Dice系数约96%，在内部验证、领域泛化和领域适应方面表现出色，超过多数现有基线结果。

Conclusion: 该方法在视盘分割任务中取得优秀性能，可结合关于基础模型作为特定任务架构替代方案的讨论。

Abstract: RETFound is a well-known foundation model (FM) developed for fundus camera
and optical coherence tomography images. It has shown promising performance
across multiple datasets in diagnosing diseases, both eye-specific and
systemic, from retinal images. However, to our best knowledge, it has not been
used for other tasks. We present the first adaptation of RETFound for optic
disc segmentation, a ubiquitous and foundational task in retinal image
analysis. The resulting segmentation system outperforms state-of-the-art,
segmentation-specific baseline networks after training a head with only a very
modest number of task-specific examples. We report and discuss results with
four public datasets, IDRID, Drishti-GS, RIM-ONE-r3, and REFUGE, and a private
dataset, GoDARTS, achieving about 96% Dice consistently across all datasets.
Overall, our method obtains excellent performance in internal verification,
domain generalization and domain adaptation, and exceeds most of the
state-of-the-art baseline results. We discuss the results in the framework of
the debate about FMs as alternatives to task-specific architectures. The code
is available at: [link to be added after the paper is accepted]

</details>


### [161] [Does the Skeleton-Recall Loss Really Work?](https://arxiv.org/abs/2508.11374)
*Devansh Arora,Nitin Kumar,Sukrit Gupta*

Main category: cs.CV

TL;DR: 本文对Skeleton Recall Loss (SRL) 损失函数进行理论分析和实验对比，发现SRL性能不如传统基线模型，评价了拓扑损失函数的局限性。


<details>
  <summary>Details</summary>
Motivation: 探究拓扑保存损失函数在管状结构分割中的有效性，评估SRL损失函数的性能。

Method: 对SRL损失的梯度进行理论分析，在管状数据集上与传统基线模型对比。

Result: SRL-based分割模型的性能未超过传统基线模型。

Conclusion: 拓扑损失函数有局限性，为开发复杂管状结构分割模型提供洞见。

Abstract: Image segmentation is an important and widely performed task in computer
vision. Accomplishing effective image segmentation in diverse settings often
requires custom model architectures and loss functions. A set of models that
specialize in segmenting thin tubular structures are topology
preservation-based loss functions. These models often utilize a pixel
skeletonization process claimed to generate more precise segmentation masks of
thin tubes and better capture the structures that other models often miss. One
such model, Skeleton Recall Loss (SRL) proposed by Kirchhoff et al.~\cite
{kirchhoff2024srl}, was stated to produce state-of-the-art results on benchmark
tubular datasets. In this work, we performed a theoretical analysis of the
gradients for the SRL loss. Upon comparing the performance of the proposed
method on some of the tubular datasets (used in the original work, along with
some additional datasets), we found that the performance of SRL-based
segmentation models did not exceed traditional baseline models. By providing
both a theoretical explanation and empirical evidence, this work critically
evaluates the limitations of topology-based loss functions, offering valuable
insights for researchers aiming to develop more effective segmentation models
for complex tubular structures.

</details>


### [162] [CHARM3R: Towards Unseen Camera Height Robust Monocular 3D Detector](https://arxiv.org/abs/2508.11185)
*Abhinav Kumar,Yuliang Guo,Zhihao Zhang,Xinyu Huang,Liu Ren,Xiaoming Liu*

Main category: cs.CV

TL;DR: 现有单目3D目标检测器在不同相机高度数据上表现不佳，本文研究相机高度变化对模型影响，提出CHARM3R模型，提升了模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决单目3D目标检测器在未见过或分布外相机高度数据上表现不佳的问题。

Method: 先研究相机高度变化对现有Mono3D模型的影响，通过对扩展的CARLA数据集进行系统分析，发现深度估计是影响性能的主要因素；数学证明并实证观察回归和基于地面的深度模型的平均深度误差在相机高度变化下的趋势；提出CHARM3R模型，在模型内对两种深度估计进行平均。

Result: CHARM3R模型将对未见过相机高度的泛化能力提高了45%以上，在CARLA数据集上达到了最先进性能。

Conclusion: CHARM3R模型有效提升了单目3D目标检测器在不同相机高度下的泛化能力。

Abstract: Monocular 3D object detectors, while effective on data from one ego camera
height, struggle with unseen or out-of-distribution camera heights. Existing
methods often rely on Plucker embeddings, image transformations or data
augmentation. This paper takes a step towards this understudied problem by
first investigating the impact of camera height variations on state-of-the-art
(SoTA) Mono3D models. With a systematic analysis on the extended CARLA dataset
with multiple camera heights, we observe that depth estimation is a primary
factor influencing performance under height variations. We mathematically prove
and also empirically observe consistent negative and positive trends in mean
depth error of regressed and ground-based depth models, respectively, under
camera height changes. To mitigate this, we propose Camera Height Robust
Monocular 3D Detector (CHARM3R), which averages both depth estimates within the
model. CHARM3R improves generalization to unseen camera heights by more than
$45\%$, achieving SoTA performance on the CARLA dataset. Codes and Models at
https://github.com/abhi1kumar/CHARM3R

</details>


### [163] [G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior Integration](https://arxiv.org/abs/2508.11379)
*Ramil Khafizov,Artem Komarichev,Ruslan Rakhimov,Peter Wonka,Evgeny Burnaev*

Main category: cs.CV

TL;DR: 介绍G - CUT3R，一种用于引导3D场景重建的前馈方法，在CUT3R模型基础上集成先验信息，评估显示性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的前馈方法仅依赖输入图像，而现实场景中有辅助数据可利用，需改进方法以提升性能。

Method: 对CUT3R进行轻量级修改，为每个模态设计专用编码器提取特征，通过零卷积与RGB图像令牌融合。

Result: 在多个基准测试中，包括3D重建和多视图任务，该方法显示出显著的性能提升。

Conclusion: G - CUT3R能有效利用可用先验信息，同时与不同输入模态兼容。

Abstract: We introduce G-CUT3R, a novel feed-forward approach for guided 3D scene
reconstruction that enhances the CUT3R model by integrating prior information.
Unlike existing feed-forward methods that rely solely on input images, our
method leverages auxiliary data, such as depth, camera calibrations, or camera
positions, commonly available in real-world scenarios. We propose a lightweight
modification to CUT3R, incorporating a dedicated encoder for each modality to
extract features, which are fused with RGB image tokens via zero convolution.
This flexible design enables seamless integration of any combination of prior
information during inference. Evaluated across multiple benchmarks, including
3D reconstruction and other multi-view tasks, our approach demonstrates
significant performance improvements, showing its ability to effectively
utilize available priors while maintaining compatibility with varying input
modalities.

</details>


### [164] [A CLIP-based Uncertainty Modal Modeling (UMM) Framework for Pedestrian Re-Identification in Autonomous Driving](https://arxiv.org/abs/2508.11218)
*Jialin Li,Shuqi Wu,Ning Wang*

Main category: cs.CV

TL;DR: 提出轻量级不确定模态建模（UMM）框架用于自动驾驶行人重识别，实验证明其性能好且高效。


<details>
  <summary>Details</summary>
Motivation: 传统ReID方法在输入模态不确定或缺失时面临挑战，大规模预训练模型计算开销大，难以在资源受限环境部署。

Method: 提出UMM框架，集成多模态令牌映射器、合成模态增强策略和跨模态线索交互学习器，利用CLIP的视觉 - 语言对齐能力高效融合多模态输入。

Result: UMM在不确定模态条件下实现了强鲁棒性、泛化性和计算效率。

Conclusion: UMM为自动驾驶场景中的行人重识别提供了可扩展且实用的解决方案。

Abstract: Re-Identification (ReID) is a critical technology in intelligent perception
systems, especially within autonomous driving, where onboard cameras must
identify pedestrians across views and time in real-time to support safe
navigation and trajectory prediction. However, the presence of uncertain or
missing input modalities--such as RGB, infrared, sketches, or textual
descriptions--poses significant challenges to conventional ReID approaches.
While large-scale pre-trained models offer strong multimodal semantic modeling
capabilities, their computational overhead limits practical deployment in
resource-constrained environments. To address these challenges, we propose a
lightweight Uncertainty Modal Modeling (UMM) framework, which integrates a
multimodal token mapper, synthetic modality augmentation strategy, and
cross-modal cue interactive learner. Together, these components enable unified
feature representation, mitigate the impact of missing modalities, and extract
complementary information across different data types. Additionally, UMM
leverages CLIP's vision-language alignment ability to fuse multimodal inputs
efficiently without extensive finetuning. Experimental results demonstrate that
UMM achieves strong robustness, generalization, and computational efficiency
under uncertain modality conditions, offering a scalable and practical solution
for pedestrian re-identification in autonomous driving scenarios.

</details>


### [165] [Probing the Representational Power of Sparse Autoencoders in Vision Models](https://arxiv.org/abs/2508.11277)
*Matthew Lyle Olson,Musashi Hinck,Neale Ratzlaff,Changbai Li,Phillip Howard,Vasudev Lal,Shao-Yen Tseng*

Main category: cs.CV

TL;DR: 本文对视觉模型中稀疏自编码器（SAEs）表征能力进行广泛评估，结果显示其特征有语义意义，能提升泛化能力和可控生成，为视觉模型中SAE评估奠定基础。


<details>
  <summary>Details</summary>
Motivation: SAEs在语言模型中受欢迎但在视觉领域研究不足，需评估其在视觉模型中的表征能力。

Method: 使用广泛的基于图像的任务对视觉模型中SAEs的表征能力进行评估。

Result: SAE特征有语义意义，提升泛化能力和可控生成，可用于OOD检测、语义引导等，揭示跨模态共享表征。

Conclusion: 为视觉模型中SAE评估奠定基础，凸显其在视觉领域提升可解释性、泛化性和可控性的潜力。

Abstract: Sparse Autoencoders (SAEs) have emerged as a popular tool for interpreting
the hidden states of large language models (LLMs). By learning to reconstruct
activations from a sparse bottleneck layer, SAEs discover interpretable
features from the high-dimensional internal representations of LLMs. Despite
their popularity with language models, SAEs remain understudied in the visual
domain. In this work, we provide an extensive evaluation the representational
power of SAEs for vision models using a broad range of image-based tasks. Our
experimental results demonstrate that SAE features are semantically meaningful,
improve out-of-distribution generalization, and enable controllable generation
across three vision model architectures: vision embedding models, multi-modal
LMMs and diffusion models. In vision embedding models, we find that learned SAE
features can be used for OOD detection and provide evidence that they recover
the ontological structure of the underlying model. For diffusion models, we
demonstrate that SAEs enable semantic steering through text encoder
manipulation and develop an automated pipeline for discovering
human-interpretable attributes. Finally, we conduct exploratory experiments on
multi-modal LLMs, finding evidence that SAE features reveal shared
representations across vision and language modalities. Our study provides a
foundation for SAE evaluation in vision models, highlighting their strong
potential improving interpretability, generalization, and steerability in the
visual domain.

</details>


### [166] [Inside Knowledge: Graph-based Path Generation with Explainable Data Augmentation and Curriculum Learning for Visual Indoor Navigation](https://arxiv.org/abs/2508.11446)
*Daniel Airinei,Elena Burceanu,Marius Leordeanu*

Main category: cs.CV

TL;DR: 提出仅基于视觉输入的深度学习方法用于室内导航，有新数据集和易用应用，数据代码开源。


<details>
  <summary>Details</summary>
Motivation: 室内导航因GPS差依赖其他信息，现有方案复杂且难部署到生产应用。

Method: 基于新颖的图路径生成方法，结合可解释的数据增强和课程学习，减少数据收集、标注和训练过程的人工干预。

Result: 创建大型商场视频数据集，每帧标注下一方向；开发安卓应用，数据代码和演示公开。

Conclusion: 该方法高效、实时、易部署，仅依赖视觉，无需特殊传感器等。

Abstract: Indoor navigation is a difficult task, as it generally comes with poor GPS
access, forcing solutions to rely on other sources of information. While
significant progress continues to be made in this area, deployment to
production applications is still lacking, given the complexity and additional
requirements of current solutions. Here, we introduce an efficient, real-time
and easily deployable deep learning approach, based on visual input only, that
can predict the direction towards a target from images captured by a mobile
device. Our technical approach, based on a novel graph-based path generation
method, combined with explainable data augmentation and curriculum learning,
includes contributions that make the process of data collection, annotation and
training, as automatic as possible, efficient and robust. On the practical
side, we introduce a novel largescale dataset, with video footage inside a
relatively large shopping mall, in which each frame is annotated with the
correct next direction towards different specific target destinations.
Different from current methods, ours relies solely on vision, avoiding the need
of special sensors, additional markers placed along the path, knowledge of the
scene map or internet access. We also created an easy to use application for
Android, which we plan to make publicly available. We make all our data and
code available along with visual demos on our project site

</details>


### [167] [Semantically Guided Adversarial Testing of Vision Models Using Language Models](https://arxiv.org/abs/2508.11341)
*Katarzyna Filus,Jorge M. Cruz-Duarte*

Main category: cs.CV

TL;DR: 本文提出语义引导框架进行对抗目标选择，评估多个模型作为相似度来源，实验表明其超越静态词汇数据库，适合构建对抗基准。


<details>
  <summary>Details</summary>
Motivation: 现有目标标签选择策略在可解释性、可重复性和灵活性上有局限，需更好的目标选择方法。

Method: 提出语义引导框架，利用预训练语言和视觉 - 语言模型的跨模态知识转移，评估BERT、TinyLLAMA和CLIP作为相似度来源。

Result: 实验显示这些模型能产生实用对抗目标，超越WordNet等静态词汇数据库，静态测试可初步评估相似度来源有效性。

Conclusion: 预训练模型适合在不同架构和数据集上构建可解释、标准化和可扩展的对抗基准。

Abstract: In targeted adversarial attacks on vision models, the selection of the target
label is a critical yet often overlooked determinant of attack success. This
target label corresponds to the class that the attacker aims to force the model
to predict. Now, existing strategies typically rely on randomness, model
predictions, or static semantic resources, limiting interpretability,
reproducibility, or flexibility. This paper then proposes a semantics-guided
framework for adversarial target selection using the cross-modal knowledge
transfer from pretrained language and vision-language models. We evaluate
several state-of-the-art models (BERT, TinyLLAMA, and CLIP) as similarity
sources to select the most and least semantically related labels with respect
to the ground truth, forming best- and worst-case adversarial scenarios. Our
experiments on three vision models and five attack methods reveal that these
models consistently render practical adversarial targets and surpass static
lexical databases, such as WordNet, particularly for distant class
relationships. We also observe that static testing of target labels offers a
preliminary assessment of the effectiveness of similarity sources, \textit{a
priori} testing. Our results corroborate the suitability of pretrained models
for constructing interpretable, standardized, and scalable adversarial
benchmarks across architectures and datasets.

</details>


### [168] [Handwritten Text Recognition of Historical Manuscripts Using Transformer-Based Models](https://arxiv.org/abs/2508.11499)
*Erez Meoded*

Main category: cs.CV

TL;DR: 本文将TrOCR模型应用于16世纪拉丁手稿，采用图像预处理、数据增强和集成学习方法，提升了历史手写文本识别性能。


<details>
  <summary>Details</summary>
Motivation: 历史手写文本识别因转录稀缺、语言变化和手写风格多样而受阻，需提升其性能以挖掘档案文献价值。

Method: 应用TrOCR模型，进行图像预处理和数据增强，引入四种针对历史手写特征的增强方法，评估集成学习策略。

Result: 最佳单模型增强方法（Elastic）的字符错误率为1.86，前5投票集成的字符错误率为1.60，相对最佳TrOCR_BASE结果提升50%，较先前最优结果提升42%。

Conclusion: 特定领域的增强和集成策略对提升历史手稿的手写文本识别性能有显著影响。

Abstract: Historical handwritten text recognition (HTR) is essential for unlocking the
cultural and scholarly value of archival documents, yet digitization is often
hindered by scarce transcriptions, linguistic variation, and highly diverse
handwriting styles. In this study, we apply TrOCR, a state-of-the-art
transformer-based HTR model, to 16th-century Latin manuscripts authored by
Rudolf Gwalther. We investigate targeted image preprocessing and a broad suite
of data augmentation techniques, introducing four novel augmentation methods
designed specifically for historical handwriting characteristics. We also
evaluate ensemble learning approaches to leverage the complementary strengths
of augmentation-trained models. On the Gwalther dataset, our best single-model
augmentation (Elastic) achieves a Character Error Rate (CER) of 1.86, while a
top-5 voting ensemble achieves a CER of 1.60 - representing a 50% relative
improvement over the best reported TrOCR_BASE result and a 42% improvement over
the previous state of the art. These results highlight the impact of
domain-specific augmentations and ensemble strategies in advancing HTR
performance for historical manuscripts.

</details>


### [169] [Unified Knowledge Distillation Framework: Fine-Grained Alignment and Geometric Relationship Preservation for Deep Face Recognition](https://arxiv.org/abs/2508.11376)
*Durgesh Mishra,Rishabh Uikey*

Main category: cs.CV

TL;DR: 提出统一知识蒸馏方法用于人脸识别模型优化，在多数据集表现优于现有方法，强教师下学生模型可超教师精度


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法在人脸识别模型优化时难以兼顾细粒度实例细节和复杂关系结构，性能欠佳

Method: 提出集成实例级嵌入蒸馏和基于关系的成对相似度蒸馏两种新损失函数的统一方法，分别采用动态难样本挖掘策略和记忆库机制与样本挖掘策略

Result: 在多个基准人脸识别数据集上，统一框架优于现有蒸馏方法，强教师下学生模型精度可超教师

Conclusion: 统一框架能有效实现实例级对齐和样本间几何关系保留，实现更全面蒸馏过程

Abstract: Knowledge Distillation is crucial for optimizing face recognition models for
deployment in computationally limited settings, such as edge devices.
Traditional KD methods, such as Raw L2 Feature Distillation or Feature
Consistency loss, often fail to capture both fine-grained instance-level
details and complex relational structures, leading to suboptimal performance.
We propose a unified approach that integrates two novel loss functions,
Instance-Level Embedding Distillation and Relation-Based Pairwise Similarity
Distillation. Instance-Level Embedding Distillation focuses on aligning
individual feature embeddings by leveraging a dynamic hard mining strategy,
thereby enhancing learning from challenging examples. Relation-Based Pairwise
Similarity Distillation captures relational information through pairwise
similarity relationships, employing a memory bank mechanism and a sample mining
strategy. This unified framework ensures both effective instance-level
alignment and preservation of geometric relationships between samples, leading
to a more comprehensive distillation process. Our unified framework outperforms
state-of-the-art distillation methods across multiple benchmark face
recognition datasets, as demonstrated by extensive experimental evaluations.
Interestingly, when using strong teacher networks compared to the student, our
unified KD enables the student to even surpass the teacher's accuracy.

</details>


### [170] [SelfAdapt: Unsupervised Domain Adaptation of Cell Segmentation Models](https://arxiv.org/abs/2508.11411)
*Fabian H. Reith,Jannik Franzen,Dinesh R. Palli,J. Lorenz Rumberger,Dagmar Kainmueller*

Main category: cs.CV

TL;DR: 提出SelfAdapt方法，无需标签适配预训练细胞分割模型，在数据集上效果好且能改进有监督微调模型，代码开源。


<details>
  <summary>Details</summary>
Motivation: 通用细胞分割模型在与训练数据不同的领域效果差，有监督微调需标注数据，而标注数据不易获取。

Method: 基于师生增强一致性训练，引入L2 - SP正则化和无标签停止准则。

Result: 在LiveCell和TissueNet数据集上，相比基线Cellpose，AP0.5最多相对提升29.64%，还能改进有监督微调过的模型。

Conclusion: SelfAdapt可在无标签情况下适配预训练细胞分割模型，作为Cellpose框架的易用扩展发布，代码公开。

Abstract: Deep neural networks have become the go-to method for biomedical instance
segmentation. Generalist models like Cellpose demonstrate state-of-the-art
performance across diverse cellular data, though their effectiveness often
degrades on domains that differ from their training data. While supervised
fine-tuning can address this limitation, it requires annotated data that may
not be readily available. We propose SelfAdapt, a method that enables the
adaptation of pre-trained cell segmentation models without the need for labels.
Our approach builds upon student-teacher augmentation consistency training,
introducing L2-SP regularization and label-free stopping criteria. We evaluate
our method on the LiveCell and TissueNet datasets, demonstrating relative
improvements in AP0.5 of up to 29.64% over baseline Cellpose. Additionally, we
show that our unsupervised adaptation can further improve models that were
previously fine-tuned with supervision. We release SelfAdapt as an easy-to-use
extension of the Cellpose framework. The code for our method is publicly
available at https: //github.com/Kainmueller-Lab/self_adapt.

</details>


### [171] [An Efficient Medical Image Classification Method Based on a Lightweight Improved ConvNeXt-Tiny Architecture](https://arxiv.org/abs/2508.11532)
*Jingsong Xia,Yue Yin,Xiuhan Li*

Main category: cs.CV

TL;DR: 本文提出基于改进ConvNeXt - Tiny架构的医学图像分类方法，提升分类性能并降低计算复杂度，实验证明其在资源受限环境有效。


<details>
  <summary>Details</summary>
Motivation: 在资源受限计算环境中实现高效、高精度的医学图像分类存在挑战。

Method: 提出基于改进ConvNeXt - Tiny架构的方法，引入双全局池化特征融合策略、设计轻量级通道注意力模块SEVector、在损失函数中加入Feature Smoothing Loss。

Result: 在仅CPU（8线程）条件下，10个训练周期内测试集最高分类准确率达89.10%，损失值收敛趋势稳定。

Conclusion: 该方法能有效提升资源受限环境下的医学图像分类性能，为医学影像分析模型的部署和推广提供可行高效方案。

Abstract: Intelligent analysis of medical imaging plays a crucial role in assisting
clinical diagnosis. However, achieving efficient and high-accuracy image
classification in resource-constrained computational environments remains
challenging. This study proposes a medical image classification method based on
an improved ConvNeXt-Tiny architecture. Through structural optimization and
loss function design, the proposed method enhances feature extraction
capability and classification performance while reducing computational
complexity. Specifically, the method introduces a dual global pooling (Global
Average Pooling and Global Max Pooling) feature fusion strategy into the
ConvNeXt-Tiny backbone to simultaneously preserve global statistical features
and salient response information. A lightweight channel attention module,
termed Squeeze-and-Excitation Vector (SEVector), is designed to improve the
adaptive allocation of channel weights while minimizing parameter overhead.
Additionally, a Feature Smoothing Loss is incorporated into the loss function
to enhance intra-class feature consistency and suppress intra-class variance.
Under CPU-only conditions (8 threads), the method achieves a maximum
classification accuracy of 89.10% on the test set within 10 training epochs,
exhibiting a stable convergence trend in loss values. Experimental results
demonstrate that the proposed method effectively improves medical image
classification performance in resource-limited settings, providing a feasible
and efficient solution for the deployment and promotion of medical imaging
analysis models.

</details>


### [172] [Controlling Multimodal LLMs via Reward-guided Decoding](https://arxiv.org/abs/2508.11616)
*Oscar Mañas,Pierluca D'Oro,Koustuv Sinha,Adriana Romero-Soriano,Michal Drozdzal,Aishwarya Agrawal*

Main category: cs.CV

TL;DR: 本文研究通过受控解码对多模态大语言模型（MLLMs）进行适配，提出奖励引导解码方法提升视觉定位能力，经评估该方法显著提升推理可控性且优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型广泛应用，需对其进行适配以满足不同用户需求。

Method: 引入奖励引导解码方法，构建视觉定位奖励模型，分别控制输出中物体精度和召回率，从控制奖励函数重要性和搜索广度两方面实现推理过程可控。

Result: 在标准物体幻觉基准测试中，该方法对MLLM推理有显著可控性，且持续优于现有幻觉缓解方法。

Conclusion: 所提出的奖励引导解码方法能有效提升MLLM视觉定位能力和推理可控性。

Abstract: As Multimodal Large Language Models (MLLMs) gain widespread applicability, it
is becoming increasingly desirable to adapt them for diverse user needs. In
this paper, we study the adaptation of MLLMs through controlled decoding. To
achieve this, we introduce the first method for reward-guided decoding of MLLMs
and demonstrate its application in improving their visual grounding. Our method
involves building reward models for visual grounding and using them to guide
the MLLM's decoding process. Concretely, we build two separate reward models to
independently control the degree of object precision and recall in the model's
output. Our approach enables on-the-fly controllability of an MLLM's inference
process in two ways: first, by giving control over the relative importance of
each reward function during decoding, allowing a user to dynamically trade off
object precision for recall in image captioning tasks; second, by giving
control over the breadth of the search during decoding, allowing the user to
control the trade-off between the amount of test-time compute and the degree of
visual grounding. We evaluate our method on standard object hallucination
benchmarks, showing that it provides significant controllability over MLLM
inference, while consistently outperforming existing hallucination mitigation
methods.

</details>


### [173] [Is ChatGPT-5 Ready for Mammogram VQA?](https://arxiv.org/abs/2508.11628)
*Qiang Li,Shansong Wang,Mingzhe Hu,Mojtaba Safari,Zachary Eidex,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 评估GPT - 5家族和GPT - 4o模型在四个公开乳腺数据集的多项任务表现，GPT - 5有潜力但性能仍不足，从GPT - 4o到GPT - 5有进步趋势。


<details>
  <summary>Details</summary>
Motivation: 乳腺钼靶视觉问答（VQA）可支持乳腺癌筛查，评估GPT模型在相关任务的表现。

Method: 在四个公开乳腺数据集上，对GPT - 5家族和GPT - 4o模型进行BI - RADS评估、异常检测和恶性分类任务的系统评估。

Result: GPT - 5是表现最佳的模型，但落后于人类专家和特定领域微调模型；在不同数据集有不同得分；与人类专家相比，灵敏度和特异性较低。

Conclusion: GPT - 5用于筛查任务有潜力，但未经针对性优化不适用于高风险临床影像应用；从GPT - 4o到GPT - 5性能提升显示通用大语言模型辅助乳腺钼靶VQA任务有前景。

Abstract: Mammogram visual question answering (VQA) integrates image interpretation
with clinical reasoning and has potential to support breast cancer screening.
We systematically evaluated the GPT-5 family and GPT-4o model on four public
mammography datasets (EMBED, InBreast, CMMD, CBIS-DDSM) for BI-RADS assessment,
abnormality detection, and malignancy classification tasks. GPT-5 consistently
was the best performing model but lagged behind both human experts and
domain-specific fine-tuned models. On EMBED, GPT-5 achieved the highest scores
among GPT variants in density (56.8%), distortion (52.5%), mass (64.5%),
calcification (63.5%), and malignancy (52.8%) classification. On InBreast, it
attained 36.9% BI-RADS accuracy, 45.9% abnormality detection, and 35.0%
malignancy classification. On CMMD, GPT-5 reached 32.3% abnormality detection
and 55.0% malignancy accuracy. On CBIS-DDSM, it achieved 69.3% BI-RADS
accuracy, 66.0% abnormality detection, and 58.2% malignancy accuracy. Compared
with human expert estimations, GPT-5 exhibited lower sensitivity (63.5%) and
specificity (52.3%). While GPT-5 exhibits promising capabilities for screening
tasks, its performance remains insufficient for high-stakes clinical imaging
applications without targeted domain adaptation and optimization. However, the
tremendous improvements in performance from GPT-4o to GPT-5 show a promising
trend in the potential for general large language models (LLMs) to assist with
mammography VQA tasks.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [174] [Note on Selection Bias in Observational Estimates of Algorithmic Progress](https://arxiv.org/abs/2508.11033)
*Parker Whitfill*

Main category: econ.GN

TL;DR: 指出Ho等人（2024）论文在估算语言模型算法进展程度策略上存在潜在方法问题。


<details>
  <summary>Details</summary>
Motivation: 发现Ho等人（2024）论文估算语言模型算法进展程度策略可能存在问题。

Method: 分析Ho等人论文中收集语言模型损失观测数据并随时间计算的估算策略。

Result: 指出若算法质量部分是潜在的，且计算选择与算法质量相关，算法质量的估算会有偏差。

Conclusion: Ho等人（2024）论文的估算策略有潜在方法问题。

Abstract: Ho et. al (2024) is an interesting paper that attempts to estimate the degree
of algorithmic progress from language models. They collect observational data
on language models' loss and compute over time, and argue that as time has
passed, language models' algorithmic efficiency has been rising. That is, the
loss achieved for fixed compute has been dropping over time. In this note, I
want to raise one potential methodological problem with the estimation
strategy. Intuitively, if part of algorithmic quality is latent, and compute
choices are endogenous to algorithmic quality, then resulting estimates of
algorithmic quality will be biased.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [175] [MCP-Guard: A Defense Framework for Model Context Protocol Integrity in Large Language Model Applications](https://arxiv.org/abs/2508.10991)
*Wenpeng Xing,Zhonghao Qi,Yupeng Qin,Yilin Li,Caini Chang,Jiahui Yu,Changting Lin,Zhenzhen Xie,Meng Han*

Main category: cs.CR

TL;DR: 文章指出LLM与外部工具集成存在安全漏洞，提出MCP - Guard防御架构和MCP - AttackBench基准。


<details>
  <summary>Details</summary>
Motivation: 解决LLM与外部工具通过MCP集成时出现的安全漏洞问题。

Method: 提出MCP - Guard三层检测管道架构，包括轻量级静态扫描、深度神经检测器和微调的E5模型；引入包含超70000样本的MCP - AttackBench基准。

Result: 微调的E5模型识别对抗性提示准确率达96.01%，MCP - AttackBench为研究提供基础。

Conclusion: MCP - Guard和MCP - AttackBench有助于保障LLM - 工具生态系统安全，为后续研究奠定基础。

Abstract: The integration of Large Language Models (LLMs) with external tools via
protocols such as the Model Context Protocol (MCP) introduces critical security
vulnerabilities, including prompt injection, data exfiltration, and other
threats. To counter these challenges, we propose MCP-Guard, a robust, layered
defense architecture designed for LLM--tool interactions. MCP-Guard employs a
three-stage detection pipeline that balances efficiency with accuracy: it
progresses from lightweight static scanning for overt threats and a deep neural
detector for semantic attacks, to our fine-tuned E5-based model achieves
(96.01) accuracy in identifying adversarial prompts. Finally, a lightweight LLM
arbitrator synthesizes these signals to deliver the final decision while
minimizing false positives. To facilitate rigorous training and evaluation, we
also introduce MCP-AttackBench, a comprehensive benchmark of over 70,000
samples. Sourced from public datasets and augmented by GPT-4, MCP-AttackBench
simulates diverse, real-world attack vectors in the MCP format, providing a
foundation for future research into securing LLM-tool ecosystems.

</details>


### [176] [RMSL: Weakly-Supervised Insider Threat Detection with Robust Multi-sphere Learning](https://arxiv.org/abs/2508.11472)
*Yang Wang,Yaxin Zhao,Xinyu Jiao,Sihan Xu,Xiangrui Cai,Ying Zhang,Xiaojie Yuan*

Main category: cs.CR

TL;DR: 引入行为序列弱标签，提出RMSL框架提升行为级内部威胁检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有内部威胁检测因缺乏细粒度行为级标注，无监督方法误报和漏报率高，检测行为级异常具有挑战性。

Method: 引入低标注成本的行为序列弱标签，提出RMSL框架，用多超球体表示正常行为模式，结合单类分类器、多实例学习和自适应行为级自训练去偏。

Result: 广泛实验表明RMSL显著提升了行为级内部威胁检测性能。

Conclusion: RMSL框架能有效增强模型区分正常和异常行为的能力，提升检测性能。

Abstract: Insider threat detection aims to identify malicious user behavior by
analyzing logs that record user interactions. Due to the lack of fine-grained
behavior-level annotations, detecting specific behavior-level anomalies within
user behavior sequences is challenging. Unsupervised methods face high false
positive rates and miss rates due to the inherent ambiguity between normal and
anomalous behaviors. In this work, we instead introduce weak labels of behavior
sequences, which have lower annotation costs, i.e., the training labels
(anomalous or normal) are at sequence-level instead of behavior-level, to
enhance the detection capability for behavior-level anomalies by learning
discriminative features. To achieve this, we propose a novel framework called
Robust Multi-sphere Learning (RMSL). RMSL uses multiple hyper-spheres to
represent the normal patterns of behaviors. Initially, a one-class classifier
is constructed as a good anomaly-supervision-free starting point. Building on
this, using multiple instance learning and adaptive behavior-level
self-training debiasing based on model prediction confidence, the framework
further refines hyper-spheres and feature representations using weak
sequence-level labels. This approach enhances the model's ability to
distinguish between normal and anomalous behaviors. Extensive experiments
demonstrate that RMSL significantly improves the performance of behavior-level
insider threat detection.

</details>


### [177] [CryptoScope: Utilizing Large Language Models for Automated Cryptographic Logic Vulnerability Detection](https://arxiv.org/abs/2508.11599)
*Zhihao Li,Zimo Ji,Tao Zheng,Hao Ren,Xiao Lan*

Main category: cs.CR

TL;DR: 提出基于大语言模型的CryptoScope框架用于自动检测加密漏洞，结合CoT提示和RAG，在基准测试中表现优于基线模型，还发现开源项目中9个未披露漏洞。


<details>
  <summary>Details</summary>
Motivation: 加密算法实现常存在难以检测的逻辑漏洞，需要有效检测方法。

Method: 引入CryptoScope框架，结合Chain-of-Thought提示与Retrieval-Augmented Generation，以超12000条条目的加密知识库为指导。

Result: 在LLM - CLVA基准测试中，相比强LLM基线模型提升了性能，DeepSeek - V3提升11.62%，GPT - 4o - mini提升20.28%，GLM - 4 - Flash提升28.69%，还发现9个未披露漏洞。

Conclusion: CryptoScope在加密漏洞检测方面表现出色，能有效发现漏洞。

Abstract: Cryptographic algorithms are fundamental to modern security, yet their
implementations frequently harbor subtle logic flaws that are hard to detect.
We introduce CryptoScope, a novel framework for automated cryptographic
vulnerability detection powered by Large Language Models (LLMs). CryptoScope
combines Chain-of-Thought (CoT) prompting with Retrieval-Augmented Generation
(RAG), guided by a curated cryptographic knowledge base containing over 12,000
entries. We evaluate CryptoScope on LLM-CLVA, a benchmark of 92 cases primarily
derived from real-world CVE vulnerabilities, complemented by cryptographic
challenges from major Capture The Flag (CTF) competitions and synthetic
examples across 11 programming languages. CryptoScope consistently improves
performance over strong LLM baselines, boosting DeepSeek-V3 by 11.62%,
GPT-4o-mini by 20.28%, and GLM-4-Flash by 28.69%. Additionally, it identifies 9
previously undisclosed flaws in widely used open-source cryptographic projects.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [178] [CSGO: Generalized Optimization for Cold Start in Wireless Collaborative Edge LLM Systems](https://arxiv.org/abs/2508.11287)
*Xuran Liu,Nan Xue,Rui Bao,Yaping Sun,Zhiyong Chen,Meixia Tao,Xiaodong Xu,Shuguang Cui*

Main category: cs.IT

TL;DR: 提出低延迟调度框架减少边缘设备大语言模型推理冷启动延迟


<details>
  <summary>Details</summary>
Motivation: 边缘设备资源有限，现有管道并行方法忽略按需加载模型的冷启动延迟

Method: 提出延迟感知调度框架，将模型加载与计算和通信重叠，把问题建模为混合整数非线性规划，设计动态规划算法优化模型分区和设备分配

Result: 相比基线策略，显著减少冷启动延迟

Conclusion: 所提方法能有效减少边缘设备大语言模型推理的冷启动延迟

Abstract: While deploying large language models on edge devices promises low-latency
and privacy-preserving AI services, it is hindered by limited device resources.
Although pipeline parallelism facilitates distributed inference, existing
approaches often ignore the cold-start latency caused by on-demand model
loading. In this paper, we propose a latency-aware scheduling framework that
overlaps model loading with computation and communication to minimize total
inference latency. Based on device and model parameters, the framework
dynamically adjusts layer partitioning and allocation to effectively hide
loading time, thereby eliminating as many idle periods as possible. We
formulate the problem as a Mixed-Integer Non-Linear Program and design an
efficient dynamic programming algorithm to optimize model partitioning and
device assignment. Experimental results show that the proposed method
significantly reduces cold-start latency compared to baseline strategies.

</details>


### [179] [Dynamic Quality-Latency Aware Routing for LLM Inference in Wireless Edge-Device Networks](https://arxiv.org/abs/2508.11291)
*Rui Bao,Nan Xue,Yaping Sun,Zhiyong Chen*

Main category: cs.IT

TL;DR: 提出动态、质量 - 延迟感知路由框架，协调移动设备和边缘服务器模型推理，实验表明能降低平均响应延迟和大模型调用次数。


<details>
  <summary>Details</summary>
Motivation: 无线通信与大语言模型集成在无线边缘设备协作环境中存在推理质量和端到端延迟的权衡问题，任务复杂度和资源分配不匹配。

Method: 提出动态、质量 - 延迟感知路由框架，使用两种成本模型，单轮查询融合语义得分与通信计算开销，多轮对话量化上下文感知成本。

Result: 在MMLU、GSM8K和MT - Bench - 101基准测试中，相比竞争基线，平均响应延迟降低5 - 15%，大模型调用次数减少10 - 20%。

Conclusion: 所提框架在保持全推理质量的同时，能有效降低延迟和大模型调用次数。

Abstract: The integration of wireless communications and Large Language Models (LLMs)
is poised to unlock ubiquitous intelligent services, yet deploying them in
wireless edge-device collaborative environments presents a critical trade-off
between inference quality and end-to-end latency. A fundamental mismatch exists
between task complexity and resource allocation: offloading simple queries
invites prohibitive latency, while on-device models lack the capacity for
demanding computations. To address this challenge, we propose a dynamic,
quality-latency aware routing framework that orchestrates inference between a
lightweight model on the mobile device and a powerful model on the edge server.
Our framework employs two distinct cost models: for single-turn queries, it
fuses a BERT-predicted semantic score with communication and computation
overheads; for multi-turn dialogues, it further quantifies context-aware costs
arising from model switching and KV-cache management. While maintaining full
inference quality, extensive experiments demonstrate that our framework cuts
average response latency by 5-15% and reduces large model invocations by 10-20%
against competitive baselines on MMLU, GSM8K, and MT-Bench-101 benchmarks.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [180] [The Role of Entanglement in Quantum Reservoir Computing with Coupled Kerr Nonlinear Oscillators](https://arxiv.org/abs/2508.11175)
*Ali Karimi,Hadi Zadeh-Haghighi,Youssef Kora,Christoph Simon*

Main category: quant-ph

TL;DR: 研究基于两个耦合Kerr非线性振子的量子储层计算（QRC）框架，探讨其在时间序列预测中性能与物理参数关系及纠缠的作用。


<details>
  <summary>Details</summary>
Motivation: 探究量子储层计算在时间序列预测中的性能，理解量子储层对高性能、高效量子机器学习和时间序列预测的作用。

Method: 研究基于两个耦合Kerr非线性振子的QRC框架，通过对数负性量化纠缠，用归一化均方根误差评估预测准确性。

Result: 纠缠在输入频率阈值内平均提供计算优势，在一定耗散和退相水平下持续存在，较高耗散率可提升性能，纠缠优势体现于平均和最坏情况性能，但不改善最佳情况误差。

Conclusion: 研究结果有助于更广泛地理解量子储层在高性能、高效量子机器学习和时间序列预测中的应用。

Abstract: Quantum Reservoir Computing (QRC) uses quantum dynamics to efficiently
process temporal data. In this work, we investigate a QRC framework based on
two coupled Kerr nonlinear oscillators, a system well-suited for time-series
prediction tasks due to its complex nonlinear interactions and potentially
high-dimensional state space. We explore how its performance in time-series
prediction depends on key physical parameters: input drive strength, Kerr
nonlinearity, and oscillator coupling, and analyze the role of entanglement in
improving the reservoir's computational performance, focusing on its effect on
predicting non-trivial time series. Using logarithmic negativity to quantify
entanglement and normalized root mean square error (NRMSE) to evaluate
predictive accuracy, our results suggest that entanglement provides a
computational advantage on average-up to a threshold in the input
frequency-that persists under some levels of dissipation and dephasing. In
particular, we find that higher dissipation rates can enhance performance.
While the entanglement advantage manifests as improvements in both average and
worst-case performance, it does not lead to improvements in the best-case
error. These findings contribute to the broader understanding of quantum
reservoirs for high performance, efficient quantum machine learning and
time-series forecasting.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [181] [Pretrained Conformers for Audio Fingerprinting and Retrieval](https://arxiv.org/abs/2508.11609)
*Kemal Altwlkany,Elmedin Selmanovic,Sead Delalic*

Main category: cs.SD

TL;DR: 本文利用自监督对比学习框架训练基于Conformer的编码器用于音频检索，用3秒音频生成嵌入，在音频检索任务中取得SOTA成果，对音频失真情况鲁棒，代码和模型公开。


<details>
  <summary>Details</summary>
Motivation: 利用Conformer捕捉局部和全局交互的能力，训练能为小音频片段生成独特嵌入并对未见数据有良好泛化能力的编码器。

Method: 采用自监督对比学习框架训练基于Conformer的编码器。

Result: 在音频检索任务中取得SOTA成果，对时间不对齐和其他音频失真情况有很好的鲁棒性，使用流行免费数据集训练测试，结果易复现。

Conclusion: 基于Conformer的编码器在音频检索任务中表现出色，具有良好的泛化性和鲁棒性。

Abstract: Conformers have shown great results in speech processing due to their ability
to capture both local and global interactions. In this work, we utilize a
self-supervised contrastive learning framework to train conformer-based
encoders that are capable of generating unique embeddings for small segments of
audio, generalizing well to previously unseen data. We achieve state-of-the-art
results for audio retrieval tasks while using only 3 seconds of audio to
generate embeddings. Our models are almost completely immune to temporal
misalignments and achieve state-of-the-art results in cases of other audio
distortions such as noise, reverb or extreme temporal stretching. Code and
models are made publicly available and the results are easy to reproduce as we
train and test using popular and freely available datasets of different sizes.

</details>


### [182] [LD-LAudio-V1: Video-to-Long-Form-Audio Generation Extension with Dual Lightweight Adapters](https://arxiv.org/abs/2508.11074)
*Haomin Zhang,Kristin Qi,Shuxin Yang,Zihao Chen,Chaofan Ding,Xinhan Di*

Main category: cs.SD

TL;DR: 本文提出LD - LAudio - V1模型和干净数据集，改善长视频音频生成效果，多指标提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有方法在长视频音频生成上存在局限，如聚焦短片段或依赖嘈杂数据集。

Method: 引入LD - LAudio - V1模型，结合双轻量级适配器；发布干净且有人工标注的视频 - 音频数据集。

Result: 显著减少拼接伪影和时间不一致性，在多个指标上相比直接微调短训练视频有显著提升。

Conclusion: 方法有效，数据集利于长视频音频生成的进一步研究，数据集开源。

Abstract: Generating high-quality and temporally synchronized audio from video content
is essential for video editing and post-production tasks, enabling the creation
of semantically aligned audio for silent videos. However, most existing
approaches focus on short-form audio generation for video segments under 10
seconds or rely on noisy datasets for long-form video-to-audio zsynthesis. To
address these limitations, we introduce LD-LAudio-V1, an extension of
state-of-the-art video-to-audio models and it incorporates dual lightweight
adapters to enable long-form audio generation. In addition, we release a clean
and human-annotated video-to-audio dataset that contains pure sound effects
without noise or artifacts. Our method significantly reduces splicing artifacts
and temporal inconsistencies while maintaining computational efficiency.
Compared to direct fine-tuning with short training videos, LD-LAudio-V1
achieves significant improvements across multiple metrics: $FD_{\text{passt}}$
450.00 $\rightarrow$ 327.29 (+27.27%), $FD_{\text{panns}}$ 34.88 $\rightarrow$
22.68 (+34.98%), $FD_{\text{vgg}}$ 3.75 $\rightarrow$ 1.28 (+65.87%),
$KL_{\text{panns}}$ 2.49 $\rightarrow$ 2.07 (+16.87%), $KL_{\text{passt}}$ 1.78
$\rightarrow$ 1.53 (+14.04%), $IS_{\text{panns}}$ 4.17 $\rightarrow$ 4.30
(+3.12%), $IB_{\text{score}}$ 0.25 $\rightarrow$ 0.28 (+12.00%),
$Energy\Delta10\text{ms}$ 0.3013 $\rightarrow$ 0.1349 (+55.23%),
$Energy\Delta10\text{ms(vs.GT)}$ 0.0531 $\rightarrow$ 0.0288 (+45.76%), and
$Sem.\,Rel.$ 2.73 $\rightarrow$ 3.28 (+20.15%). Our dataset aims to facilitate
further research in long-form video-to-audio generation and is available at
https://github.com/deepreasonings/long-form-video2audio.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [183] [When Algorithms Mirror Minds: A Confirmation-Aware Social Dynamic Model of Echo Chamber and Homogenization Traps](https://arxiv.org/abs/2508.11516)
*Ming Tang,Xiaowen Huang,Jitao Sang*

Main category: cs.SI

TL;DR: 本文提出确认感知社会动态模型研究推荐系统回声室和用户同质化问题，理论证明其必然发生，通过实验探索影响因素并提出缓解策略。


<details>
  <summary>Details</summary>
Motivation: 现有研究较少从用户心理机制和用户与推荐器的闭环交互角度研究推荐系统的回声室和用户同质化问题，本文旨在填补这一空白。

Method: 提出确认感知社会动态模型模拟用户与推荐器交互过程，进行理论分析，并在多个数据集上用五个指标进行实证模拟。

Result: 理论证明回声室和同质化陷阱必然发生，从系统、用户、平台三个层面探索影响因素，提出四个缓解策略但会牺牲一定推荐准确性。

Conclusion: 研究为回声室和用户同质化的出现及驱动因素提供理论和实证见解，为以人为中心的推荐系统设计提供行动指南。

Abstract: Recommender systems increasingly suffer from echo chambers and user
homogenization, systemic distortions arising from the dynamic interplay between
algorithmic recommendations and human behavior. While prior work has studied
these phenomena through the lens of algorithmic bias or social network
structure, we argue that the psychological mechanisms of users and the
closed-loop interaction between users and recommenders are critical yet
understudied drivers of these emergent effects. To bridge this gap, we propose
the Confirmation-Aware Social Dynamic Model which incorporates user psychology
and social relationships to simulate the actual user and recommender
interaction process. Our theoretical analysis proves that echo chambers and
homogenization traps, defined respectively as reduced recommendation diversity
and homogenized user representations, will inevitably occur. We also conduct
extensive empirical simulations on two real-world datasets and one synthetic
dataset with five well-designed metrics, exploring the root factors influencing
the aforementioned phenomena from three level perspectives: the stochasticity
and social integration degree of recommender (system-level), the psychological
mechanisms of users (user-level), and the dataset scale (platform-level).
Furthermore, we demonstrate four practical mitigation strategies that help
alleviate echo chambers and user homogenization at the cost of some
recommendation accuracy. Our findings provide both theoretical and empirical
insights into the emergence and drivers of echo chambers and user
homogenization, as well as actionable guidelines for human-centered recommender
design.

</details>


### [184] [FLUID: Flow-Latent Unified Integration via Token Distillation for Expert Specialization in Multimodal Learning](https://arxiv.org/abs/2508.07264)
*Van Duc Cuong,Ta Dinh Tam,Tran Duc Chinh,Nguyen Thi Hanh*

Main category: cs.SI

TL;DR: 提出FLUID方法提升多模态产品分类的跨模态鲁棒性和可扩展性，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 常见融合策略在多模态分类中易受模态特定噪声影响，不够鲁棒。

Method: 提出FLUID，含Q-transforms提取特征、两阶段融合方案及轻量级Mixture-of-Experts。

Result: 在GLAMI - 1M基准上达91%准确率，超越基线，抗噪声、长尾类别不平衡和语义异质性能力强。

Conclusion: FLUID是可扩展、抗噪声的多模态产品分类解决方案。

Abstract: Multimodal classification requires robust integration of visual and textual
signals, yet common fusion strategies are brittle and vulnerable to
modality-specific noise. In this paper, we present \textsc{FLUID}-Flow-Latent
Unified Integration via Token Distillation for Expert Specialization, a
principled token-level pipeline that improves cross-modal robustness and
scalability. \textsc{FLUID} contributes three core elements: (1)
\emph{Q-transforms}, learnable query tokens that distill and retain salient
token-level features from modality-specific backbones; (2) a two-stage fusion
scheme that enforces cross-modal consistency via contrastive alignment and then
performs adaptive, task-aware fusion through a gating mechanism and a
\emph{Q-bottleneck} that selectively compresses information for downstream
reasoning; and (3) a lightweight, load-balanced Mixture-of-Experts at
prediction time that enables efficient specialization to diverse semantic
patterns. Extensive experiments demonstrate that \textsc{FLUID} attains
\(91\%\) accuracy on the GLAMI-1M benchmark, significantly outperforming prior
baselines and exhibiting strong resilience to label noise, long-tail class
imbalance, and semantic heterogeneity. Targeted ablation studies corroborate
both the individual and synergistic benefits of the proposed components,
positioning \textsc{FLUID} as a scalable, noise-resilient solution for
multimodal product classification.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [185] [Deep Learning-Based Automated Segmentation of Uterine Myomas](https://arxiv.org/abs/2508.11010)
*Tausifa Jan Saleem,Mohammad Yaqub*

Main category: eess.IV

TL;DR: 子宫肌瘤是常见妇科良性肿瘤，临床诊断依赖MRI分割，但人工分割耗时长且有差异，深度学习有潜力，此前多使用私有数据集，本研究用公开UMD数据集为自动分割建立基线。


<details>
  <summary>Details</summary>
Motivation: 子宫肌瘤影响女性生殖健康，人工MRI分割耗时长且有差异，需准确自动分割方法，此前研究多使用私有数据集不利于验证和比较。

Method: 利用公开的子宫肌瘤MRI数据集（UMD）进行自动分割。

Result: 文中未提及具体结果。

Conclusion: 为子宫平滑肌瘤的自动分割建立了基线，便于标准化评估和该领域未来研究。

Abstract: Uterine fibroids (myomas) are the most common benign tumors of the female
reproductive system, particularly among women of childbearing age. With a
prevalence exceeding 70%, they pose a significant burden on female reproductive
health. Clinical symptoms such as abnormal uterine bleeding, infertility,
pelvic pain, and pressure-related discomfort play a crucial role in guiding
treatment decisions, which are largely influenced by the size, number, and
anatomical location of the fibroids. Magnetic Resonance Imaging (MRI) is a
non-invasive and highly accurate imaging modality commonly used by clinicians
for the diagnosis of uterine fibroids. Segmenting uterine fibroids requires a
precise assessment of both the uterus and fibroids on MRI scans, including
measurements of volume, shape, and spatial location. However, this process is
labor intensive and time consuming and subjected to variability due to intra-
and inter-expert differences at both pre- and post-treatment stages. As a
result, there is a critical need for an accurate and automated segmentation
method for uterine fibroids. In recent years, deep learning algorithms have
shown re-markable improvements in medical image segmentation, outperforming
traditional methods. These approaches offer the potential for fully automated
segmentation. Several studies have explored the use of deep learning models to
achieve automated segmentation of uterine fibroids. However, most of the
previous work has been conducted using private datasets, which poses challenges
for validation and comparison between studies. In this study, we leverage the
publicly available Uterine Myoma MRI Dataset (UMD) to establish a baseline for
automated segmentation of uterine fibroids, enabling standardized evaluation
and facilitating future research in this domain.

</details>


### [186] [HistoViT: Vision Transformer for Accurate and Scalable Histopathological Cancer Diagnosis](https://arxiv.org/abs/2508.11181)
*Faisal Ahmed*

Main category: eess.IV

TL;DR: 提出基于Transformer的深度学习框架用于组织病理图像多类肿瘤分类，在四个数据集上表现优于现有方法，证实其在数字病理中的潜力。


<details>
  <summary>Details</summary>
Motivation: 现代病理学中准确且可扩展的癌症诊断是关键挑战，传统卷积神经网络有局限性。

Method: 提出基于Transformer的深度学习框架，采用微调的Vision Transformer (ViT)架构，实现简化的预处理流程。

Result: 在四个基准数据集上表现优于现有深度学习方法，乳腺癌、前列腺癌、骨癌和宫颈癌分类准确率分别达99.32%、96.92%、95.28%和96.94%，ROC曲线下面积（AUC）得分均超99%。

Conclusion: Transformer架构在数字病理中具有鲁棒性、泛化性和临床潜力，推动了可靠、自动化和可解释的癌症诊断系统发展。

Abstract: Accurate and scalable cancer diagnosis remains a critical challenge in modern
pathology, particularly for malignancies such as breast, prostate, bone, and
cervical, which exhibit complex histological variability. In this study, we
propose a transformer-based deep learning framework for multi-class tumor
classification in histopathological images. Leveraging a fine-tuned Vision
Transformer (ViT) architecture, our method addresses key limitations of
conventional convolutional neural networks, offering improved performance,
reduced preprocessing requirements, and enhanced scalability across tissue
types. To adapt the model for histopathological cancer images, we implement a
streamlined preprocessing pipeline that converts tiled whole-slide images into
PyTorch tensors and standardizes them through data normalization. This ensures
compatibility with the ViT architecture and enhances both convergence stability
and overall classification performance. We evaluate our model on four benchmark
datasets: ICIAR2018 (breast), SICAPv2 (prostate), UT-Osteosarcoma (bone), and
SipakMed (cervical) dataset -- demonstrating consistent outperformance over
existing deep learning methods. Our approach achieves classification accuracies
of 99.32%, 96.92%, 95.28%, and 96.94% for breast, prostate, bone, and cervical
cancers respectively, with area under the ROC curve (AUC) scores exceeding 99%
across all datasets. These results confirm the robustness, generalizability,
and clinical potential of transformer-based architectures in digital pathology.
Our work represents a significant advancement toward reliable, automated, and
interpretable cancer diagnosis systems that can alleviate diagnostic burdens
and improve healthcare outcomes.

</details>


### [187] [Semi-Supervised Learning with Online Knowledge Distillation for Skin Lesion Classification](https://arxiv.org/abs/2508.11511)
*Siyamalan Manivannan*

Main category: eess.IV

TL;DR: 提出结合集成学习与在线知识蒸馏的半监督深度学习方法用于皮肤病变分类，减少标注数据需求，实验效果超当前最优。


<details>
  <summary>Details</summary>
Motivation: 现有皮肤病变分析方法多依赖全监督学习，获取大量标注数据困难且成本高，需减轻标注负担。

Method: 训练卷积神经网络集成模型，用在线知识蒸馏将集成的知识传递给成员，提升各模型及整体性能。

Result: 知识蒸馏后的单个模型表现优于独立训练模型，在公开基准数据集上超当前最优结果。

Conclusion: 该方法减少了对大量标注数据的需求，为现实场景中的皮肤病变分类提供了资源高效的解决方案。

Abstract: Deep Learning has emerged as a promising approach for skin lesion analysis.
However, existing methods mostly rely on fully supervised learning, requiring
extensive labeled data, which is challenging and costly to obtain. To alleviate
this annotation burden, this study introduces a novel semi-supervised deep
learning approach that integrates ensemble learning with online knowledge
distillation for enhanced skin lesion classification. Our methodology involves
training an ensemble of convolutional neural network models, using online
knowledge distillation to transfer insights from the ensemble to its members.
This process aims to enhance the performance of each model within the ensemble,
thereby elevating the overall performance of the ensemble itself.
Post-training, any individual model within the ensemble can be deployed at test
time, as each member is trained to deliver comparable performance to the
ensemble. This is particularly beneficial in resource-constrained environments.
Experimental results demonstrate that the knowledge-distilled individual model
performs better than independently trained models. Our approach demonstrates
superior performance on both the \emph{International Skin Imaging
Collaboration} 2018 and 2019 public benchmark datasets, surpassing current
state-of-the-art results. By leveraging ensemble learning and online knowledge
distillation, our method reduces the need for extensive labeled data while
providing a more resource-efficient solution for skin lesion classification in
real-world scenarios.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [188] [Data-driven global ocean model resolving ocean-atmosphere coupling dynamics](https://arxiv.org/abs/2508.10908)
*Jeong-Hwan Kim,Daehyun Kang,Young-Min Yang,Jae-Heung Park,Yoo-Geun Ham*

Main category: physics.ao-ph

TL;DR: 本文介绍基于深度学习的全球三维海洋环流模型KIST - Ocean，评估显示其有良好预测能力，能代表关键海气耦合机制，增强了对基于深度学习的气候模型的信心。


<details>
  <summary>Details</summary>
Motivation: 现有基于人工智能的全球天气预报虽有进展，但要延长超季节尺度的预测，需开发基于深度学习的海 - 气耦合模型。

Method: 采用U形视觉注意力对抗网络架构，结合部分卷积、对抗训练和迁移学习来解决沿海复杂性和自回归模型中的预测分布漂移问题。

Result: 模型有强大海洋预测技能和效率，能准确捕捉现实海洋响应，如热带太平洋的开尔文和罗斯比波传播等。

Conclusion: 增强了对基于深度学习的全球天气和气候模型的信心，为更广泛地球系统建模提供潜力，有助于提升气候预测能力。

Abstract: Artificial intelligence has advanced global weather forecasting,
outperforming traditional numerical models in both accuracy and computational
efficiency. Nevertheless, extending predictions beyond subseasonal timescales
requires the development of deep learning (DL)-based ocean-atmosphere coupled
models that can realistically simulate complex oceanic responses to atmospheric
forcing. This study presents KIST-Ocean, a DL-based global three-dimensional
ocean general circulation model using a U-shaped visual attention adversarial
network architecture. KIST-Ocean integrates partial convolution, adversarial
training, and transfer learning to address coastal complexity and predictive
distribution drift in auto-regressive models. Comprehensive evaluations
confirmed the model's robust ocean predictive skill and efficiency. Moreover,
it accurately captures realistic ocean response, such as Kelvin and Rossby wave
propagation in the tropical Pacific, and vertical motions induced by cyclonic
and anticyclonic wind stress, demonstrating its ability to represent key
ocean-atmosphere coupling mechanisms underlying climate phenomena, including
the El Nino-Southern Oscillation. These findings reinforce confidence in
DL-based global weather and climate models and their extending DL-based
approaches to broader Earth system modeling, offering potential for enhancing
climate prediction capabilities.

</details>


### [189] [Approximating the universal thermal climate index using sparse regression with orthogonal polynomials](https://arxiv.org/abs/2508.11307)
*Sabin Roman,Gregor Skok,Ljupco Todorovski,Saso Dzeroski*

Main category: physics.ao-ph

TL;DR: 本文探索数据驱动建模方法分析和近似通用热气候指数（UTCI），利用符号和稀疏回归技术，结合勒让德多项式，模型在准确性和效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: UTCI具有非线性、多变量结构，需要可解释且高效的函数近似方法。

Method: 研究符号和稀疏回归技术，在稀疏回归框架中使用正交多项式基（如勒让德多项式）。

Result: 模型均方根损失显著低于六次多项式基准，利用勒让德多项式构建的模型能有效平衡准确性和复杂性，在20%数据上训练可良好泛化到80%数据。

Conclusion: 结合稀疏性、正交性和符号结构能对UTCI等复杂环境指数进行稳健、可解释建模，在准确性和效率上显著优于现有方法。

Abstract: This article explores novel data-driven modeling approaches for analyzing and
approximating the Universal Thermal Climate Index (UTCI), a
physiologically-based metric integrating multiple atmospheric variables to
assess thermal comfort. Given the nonlinear, multivariate structure of UTCI, we
investigate symbolic and sparse regression techniques as tools for
interpretable and efficient function approximation. In particular, we highlight
the benefits of using orthogonal polynomial bases-such as Legendre
polynomials-in sparse regression frameworks, demonstrating their advantages in
stability, convergence, and hierarchical interpretability compared to standard
polynomial expansions. We demonstrate that our models achieve significantly
lower root-mean squared losses than the widely used sixth-degree polynomial
benchmark-while using the same or fewer parameters. By leveraging Legendre
polynomial bases, we construct models that efficiently populate a Pareto front
of accuracy versus complexity and exhibit stable, hierarchical coefficient
structures across varying model capacities. Training on just 20% of the data,
our models generalize robustly to the remaining 80%, with consistent
performance under bootstrapping. The decomposition effectively approximates the
UTCI as a Fourier-like expansion in an orthogonal basis, yielding results near
the theoretical optimum in the L2 (least squares) sense. We also connect these
findings to the broader context of equation discovery in environmental
modeling, referencing probabilistic grammar-based methods that enforce domain
consistency and compactness in symbolic expressions. Taken together, these
results illustrate how combining sparsity, orthogonality, and symbolic
structure enables robust, interpretable modeling of complex environmental
indices like UTCI - and significantly outperforms the state-of-the-art
approximation in both accuracy and efficiency.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [190] [Modeling and Detecting Company Risks from News: A Case Study in Bloomberg News](https://arxiv.org/abs/2508.10927)
*Jiaxin Pei,Soumya Vadlamannati,Liang-Kang Huang,Daniel Preotiuc-Pietro,Xinyu Hua*

Main category: cs.CL

TL;DR: 构建计算框架从新闻文章中自动提取公司风险因素，对比不同模型表现，分析大量新闻展示其对了解公司和行业运营的作用。


<details>
  <summary>Details</summary>
Motivation: 识别公司相关风险对投资者和金融市场健康很重要，需要从新闻中自动提取公司风险因素。

Method: 构建包含七个方面的计算框架，对744篇新闻文章采样标注，对比多种机器学习模型，包括零样本和少样本提示的大语言模型及微调的预训练语言模型。

Result: 零样本和少样本提示的大语言模型在识别风险因素上表现中等到低，微调的预训练语言模型在多数风险因素上表现更好。

Conclusion: 从新闻中识别风险因素可为公司和行业运营提供广泛见解。

Abstract: Identifying risks associated with a company is important to investors and the
well-being of the overall financial market. In this study, we build a
computational framework to automatically extract company risk factors from news
articles. Our newly proposed schema comprises seven distinct aspects, such as
supply chain, regulations, and competitions. We sample and annotate 744 news
articles and benchmark various machine learning models. While large language
models have achieved huge progress in various types of NLP tasks, our
experiment shows that zero-shot and few-shot prompting state-of-the-art LLMs
(e.g. LLaMA-2) can only achieve moderate to low performances in identifying
risk factors. And fine-tuned pre-trained language models are performing better
on most of the risk factors. Using this model, we analyze over 277K Bloomberg
news articles and demonstrate that identifying risk factors from news could
provide extensive insight into the operations of companies and industries.

</details>


### [191] [MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens of Documents](https://arxiv.org/abs/2508.11133)
*Tomer Wolfson,Harsh Trivedi,Mor Geva,Yoav Goldberg,Dan Roth,Tushar Khot,Ashish Sabharwal,Reut Tsarfaty*

Main category: cs.CL

TL;DR: 介绍MoNaCo基准测试，包含1315个自然复杂问题，前沿大语言模型在该基准上F1最高61.2%，凸显对更好推理模型的需求。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型基准测试缺乏兼具信息查询和耗时特点的自然问题，为填补这一空白开展研究。

Method: 开发分解注释管道，大规模引出并手动回答自然耗时问题，构建MoNaCo基准。

Result: 前沿大语言模型在MoNaCo上F1最高达61.2%，受低召回率和幻觉问题阻碍。

Conclusion: 需要能更好处理现实世界信息查询问题复杂性和广度的推理模型，MoNaCo可有效跟踪此类进展，相关资源已公开。

Abstract: Large language models (LLMs) are emerging as a go-to tool for querying
information. However, current LLM benchmarks rarely feature natural questions
that are both information-seeking as well as genuinely time-consuming for
humans. To address this gap we introduce MoNaCo, a benchmark of 1,315 natural
and complex questions that require dozens, and at times hundreds, of
intermediate steps to solve -- far more than any existing QA benchmark. To
build MoNaCo, we developed a decomposed annotation pipeline to elicit and
manually answer natural time-consuming questions at scale. Frontier LLMs
evaluated on MoNaCo achieve at most 61.2% F1, hampered by low recall and
hallucinations. Our results underscore the need for reasoning models that
better handle the complexity and sheer breadth of real-world
information-seeking questions -- with MoNaCo providing an effective resource
for tracking such progress. The MONACO benchmark, codebase, prompts and models
predictions are publicly available at: https://tomerwolgithub.github.io/monaco

</details>


### [192] [SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced Benchmark for Automatic Survey Generation Systems](https://arxiv.org/abs/2508.11310)
*Beichen Guo,Zhiyuan Wen,Yu Yang,Peng Gao,Ruosong Yang,Jiaxing Shen*

Main category: cs.CL

TL;DR: 因大语言模型发展，自动调查生成受关注，现有评估方法有局限，提出SGSimEval基准进行多方面评估，实验显示当前ASG系统各方面表现情况。


<details>
  <summary>Details</summary>
Motivation: 大语言模型使自动调查生成成为可行方法，现有评估方法存在有偏指标、缺乏人类偏好等局限，需要更稳健评估方法。

Method: 提出SGSimEval基准，综合评估大纲、内容和参考文献，结合基于大语言模型评分和定量指标，引入人类偏好指标。

Result: 当前ASG系统在大纲生成上与人类表现相当，内容和参考文献生成有提升空间，评估指标与人类评估高度一致。

Conclusion: SGSimEval基准可有效评估自动调查生成系统。

Abstract: The growing interest in automatic survey generation (ASG), a task that
traditionally required considerable time and effort, has been spurred by recent
advances in large language models (LLMs). With advancements in
retrieval-augmented generation (RAG) and the rising popularity of multi-agent
systems (MASs), synthesizing academic surveys using LLMs has become a viable
approach, thereby elevating the need for robust evaluation methods in this
domain. However, existing evaluation methods suffer from several limitations,
including biased metrics, a lack of human preference, and an over-reliance on
LLMs-as-judges. To address these challenges, we propose SGSimEval, a
comprehensive benchmark for Survey Generation with Similarity-Enhanced
Evaluation that evaluates automatic survey generation systems by integrating
assessments of the outline, content, and references, and also combines
LLM-based scoring with quantitative metrics to provide a multifaceted
evaluation framework. In SGSimEval, we also introduce human preference metrics
that emphasize both inherent quality and similarity to humans. Extensive
experiments reveal that current ASG systems demonstrate human-comparable
superiority in outline generation, while showing significant room for
improvement in content and reference generation, and our evaluation metrics
maintain strong consistency with human assessments.

</details>


### [193] [gpt-oss-120b & gpt-oss-20b Model Card](https://arxiv.org/abs/2508.10925)
*OpenAI,:,Sandhini Agarwal,Lama Ahmad,Jason Ai,Sam Altman,Andy Applebaum,Edwin Arbus,Rahul K. Arora,Yu Bai,Bowen Baker,Haiming Bao,Boaz Barak,Ally Bennett,Tyler Bertao,Nivedita Brett,Eugene Brevdo,Greg Brockman,Sebastien Bubeck,Che Chang,Kai Chen,Mark Chen,Enoch Cheung,Aidan Clark,Dan Cook,Marat Dukhan,Casey Dvorak,Kevin Fives,Vlad Fomenko,Timur Garipov,Kristian Georgiev,Mia Glaese,Tarun Gogineni,Adam Goucher,Lukas Gross,Katia Gil Guzman,John Hallman,Jackie Hehir,Johannes Heidecke,Alec Helyar,Haitang Hu,Romain Huet,Jacob Huh,Saachi Jain,Zach Johnson,Chris Koch,Irina Kofman,Dominik Kundel,Jason Kwon,Volodymyr Kyrylov,Elaine Ya Le,Guillaume Leclerc,James Park Lennon,Scott Lessans,Mario Lezcano-Casado,Yuanzhi Li,Zhuohan Li,Ji Lin,Jordan Liss,Lily,Liu,Jiancheng Liu,Kevin Lu,Chris Lu,Zoran Martinovic,Lindsay McCallum,Josh McGrath,Scott McKinney,Aidan McLaughlin,Song Mei,Steve Mostovoy,Tong Mu,Gideon Myles,Alexander Neitz,Alex Nichol,Jakub Pachocki,Alex Paino,Dana Palmie,Ashley Pantuliano,Giambattista Parascandolo,Jongsoo Park,Leher Pathak,Carolina Paz,Ludovic Peran,Dmitry Pimenov,Michelle Pokrass,Elizabeth Proehl,Huida Qiu,Gaby Raila,Filippo Raso,Hongyu Ren,Kimmy Richardson,David Robinson,Bob Rotsted,Hadi Salman,Suvansh Sanjeev,Max Schwarzer,D. Sculley,Harshit Sikchi,Kendal Simon,Karan Singhal,Yang Song,Dane Stuckey,Zhiqing Sun,Philippe Tillet,Sam Toizer,Foivos Tsimpourlas,Nikhil Vyas,Eric Wallace,Xin Wang,Miles Wang,Olivia Watkins,Kevin Weil,Amy Wendling,Kevin Whinnery,Cedric Whitney,Hannah Wong,Lin Yang,Yu Yang,Michihiro Yasunaga,Kristen Ying,Wojciech Zaremba,Wenting Zhan,Cyril Zhang,Brian Zhang,Eddie Zhang,Shengjia Zhao*

Main category: cs.CL

TL;DR: 介绍gpt - oss - 120b和gpt - oss - 20b两个开源推理模型，介绍其架构、训练方法、能力、表现并开源相关资源。


<details>
  <summary>Details</summary>
Motivation: 推动推理模型在准确性和推理成本方面的发展。

Method: 采用高效的混合专家变压器架构，通过大规模蒸馏和强化学习进行训练，优化模型以具备强大代理能力。

Result: 两个模型在数学、编码和安全等基准测试中取得了优异成绩。

Conclusion: 以Apache 2.0许可发布模型权重、推理实现、工具环境和分词器，以促进广泛使用和进一步研究。

Abstract: We present gpt-oss-120b and gpt-oss-20b, two open-weight reasoning models
that push the frontier of accuracy and inference cost. The models use an
efficient mixture-of-expert transformer architecture and are trained using
large-scale distillation and reinforcement learning. We optimize the models to
have strong agentic capabilities (deep research browsing, python tool use, and
support for developer-provided functions), all while using a rendered chat
format that enables clear instruction following and role delineation. Both
models achieve strong results on benchmarks ranging from mathematics, coding,
and safety. We release the model weights, inference implementations, tool
environments, and tokenizers under an Apache 2.0 license to enable broad use
and further research.

</details>


### [194] [Rule2Text: A Framework for Generating and Evaluating Natural Language Explanations of Knowledge Graph Rules](https://arxiv.org/abs/2508.10971)
*Nasim Shirvani-Mahdavi,Chengkai Li*

Main category: cs.CL

TL;DR: 提出Rule2Text框架用大语言模型为知识图谱逻辑规则生成自然语言解释，经多数据集实验、多策略评估，用LLM评判框架辅助，微调开源模型提升解释质量，代码数据开源。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱规则挖掘得到的逻辑规则因复杂性和标签习惯难被人类解释，需提升知识图谱的可访问性和可用性。

Method: 用Rule2Text框架，在多个数据集上用AMIE 3.5.1挖掘规则，评估多种大语言模型和提示策略，进行人工评估，开发LLM评判框架，结合最优模型、评判框架和人工反馈微调开源模型，集成类型推理模块。

Result: 微调后解释质量显著提升，特定领域数据集提升明显。

Conclusion: Rule2Text框架能有效为知识图谱逻辑规则生成自然语言解释，提升解释质量。

Abstract: Knowledge graphs (KGs) can be enhanced through rule mining; however, the
resulting logical rules are often difficult for humans to interpret due to
their inherent complexity and the idiosyncratic labeling conventions of
individual KGs. This work presents Rule2Text, a comprehensive framework that
leverages large language models (LLMs) to generate natural language
explanations for mined logical rules, thereby improving KG accessibility and
usability. We conduct extensive experiments using multiple datasets, including
Freebase variants (FB-CVT-REV, FB+CVT-REV, and FB15k-237) as well as the
ogbl-biokg dataset, with rules mined using AMIE 3.5.1. We systematically
evaluate several LLMs across a comprehensive range of prompting strategies,
including zero-shot, few-shot, variable type incorporation, and
Chain-of-Thought reasoning. To systematically assess models' performance, we
conduct a human evaluation of generated explanations on correctness and
clarity. To address evaluation scalability, we develop and validate an
LLM-as-a-judge framework that demonstrates strong agreement with human
evaluators. Leveraging the best-performing model (Gemini 2.0 Flash), LLM judge,
and human-in-the-loop feedback, we construct high-quality ground truth
datasets, which we use to fine-tune the open-source Zephyr model. Our results
demonstrate significant improvements in explanation quality after fine-tuning,
with particularly strong gains in the domain-specific dataset. Additionally, we
integrate a type inference module to support KGs lacking explicit type
information. All code and data are publicly available at
https://github.com/idirlab/KGRule2NL.

</details>


### [195] [SproutBench: A Benchmark for Safe and Ethical Large Language Models for Youth](https://arxiv.org/abs/2508.11009)
*Wenpeng Xing,Lanyi Wei,Haixiao Hu,Rongchang Li,Mohan Li,Changting Lin,Meng Han*

Main category: cs.CL

TL;DR: 现有AI安全框架多针对成人，忽视未成年人。本文指出LLM安全基准不足，引入SproutBench，评估47个LLM发现安全漏洞，给出以儿童为中心的AI设计和部署指南。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在针对儿童和青少年的应用中迅速普及，而现有AI安全框架多针对成人，忽视未成年人发育的脆弱性，需要重新评估。

Method: 引入包含1283个基于发育的对抗性提示的评估套件SproutBench，对47个不同的大型语言模型进行严格实证评估。

Result: 发现了大量安全漏洞，存在维度间的强相关性，交互性与年龄适宜性呈显著负相关。

Conclusion: 研究结果为推进以儿童为中心的AI设计和部署提供了实用指南。

Abstract: The rapid proliferation of large language models (LLMs) in applications
targeting children and adolescents necessitates a fundamental reassessment of
prevailing AI safety frameworks, which are largely tailored to adult users and
neglect the distinct developmental vulnerabilities of minors. This paper
highlights key deficiencies in existing LLM safety benchmarks, including their
inadequate coverage of age-specific cognitive, emotional, and social risks
spanning early childhood (ages 0--6), middle childhood (7--12), and adolescence
(13--18). To bridge these gaps, we introduce SproutBench, an innovative
evaluation suite comprising 1,283 developmentally grounded adversarial prompts
designed to probe risks such as emotional dependency, privacy violations, and
imitation of hazardous behaviors. Through rigorous empirical evaluation of 47
diverse LLMs, we uncover substantial safety vulnerabilities, corroborated by
robust inter-dimensional correlations (e.g., between Safety and Risk
Prevention) and a notable inverse relationship between Interactivity and Age
Appropriateness. These insights yield practical guidelines for advancing
child-centric AI design and deployment.

</details>


### [196] [Beyond the Rosetta Stone: Unification Forces in Generalization Dynamics](https://arxiv.org/abs/2508.11017)
*Carter Blum,Katja Filipova,Ann Yuan,Asma Ghandeharioun,Julian Zimmert,Fred Zhang,Jessica Hoffmann,Tal Linzen,Martin Wattenberg,Lucas Dixon,Mor Geva*

Main category: cs.CL

TL;DR: 本文通过训练小型Transformer模型研究大语言模型跨语言知识迁移问题，发现统一表示对跨语言迁移很重要，还提出调节方法和评估指标。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型跨语言知识迁移时产生幻觉的问题，研究该现象的原因和动态。

Method: 在合成多语言数据集上从头训练小型Transformer模型，分析学习阶段，研究统一表示情况；基于此操纵数据分布和分词来调节跨语言迁移水平，引入指标和可视化方法。

Result: 确定模型在学习阶段会形成不同语言相同事实的分离或统一表示，统一表示对跨语言迁移至关重要；发现统一程度与事实和训练数据语言的互信息及语言提取难易有关。

Conclusion: 可控设置能揭示预训练动态，为改进大语言模型的跨语言迁移提供新方向。

Abstract: Large language models (LLMs) struggle with cross-lingual knowledge transfer:
they hallucinate when asked in one language about facts expressed in a
different language during training. This work introduces a controlled setting
to study the causes and dynamics of this phenomenon by training small
Transformer models from scratch on synthetic multilingual datasets. We identify
a learning phase wherein a model develops either separate or unified
representations of the same facts across languages, and show that unification
is essential for cross-lingual transfer. We also show that the degree of
unification depends on mutual information between facts and training data
language, and on how easy it is to extract that language. Based on these
insights, we develop methods to modulate the level of cross-lingual transfer by
manipulating data distribution and tokenization, and we introduce metrics and
visualizations to formally characterize their effects on unification. Our work
shows how controlled settings can shed light on pre-training dynamics and
suggests new directions for improving cross-lingual transfer in LLMs.

</details>


### [197] [E-CaTCH: Event-Centric Cross-Modal Attention with Temporal Consistency and Class-Imbalance Handling for Misinformation Detection](https://arxiv.org/abs/2508.11197)
*Ahmad Mousavi,Yeganeh Abdollahinejad,Roberto Corizzo,Nathalie Japkowicz,Zois Boukouvalas*

Main category: cs.CL

TL;DR: 提出E - CaTCH框架检测社交媒体多模态错误信息，聚类帖子为伪事件，经特征提取、融合和时间建模，在多数据集上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理多模态错误信息检测中的模态不一致、时间模式变化和类别不平衡问题，且未捕捉事件级结构。

Method: 提出E - CaTCH框架，聚类帖子为伪事件，用预训练编码器提取特征，通过自注意力和交叉模态注意力处理，软门控机制融合特征，用趋势感知LSTM建模时间演变，在事件级分类，集成自适应类加权等方法解决类别不平衡。

Result: 在Fakeddit、IND和COVID - 19 MISINFOGRAPH数据集上始终优于现有基线，跨数据集评估显示其鲁棒性、泛化性和实用性。

Conclusion: E - CaTCH框架能有效且稳健地检测社交媒体多模态错误信息。

Abstract: Detecting multimodal misinformation on social media remains challenging due
to inconsistencies between modalities, changes in temporal patterns, and
substantial class imbalance. Many existing methods treat posts independently
and fail to capture the event-level structure that connects them across time
and modality. We propose E-CaTCH, an interpretable and scalable framework for
robustly detecting misinformation. If needed, E-CaTCH clusters posts into
pseudo-events based on textual similarity and temporal proximity, then
processes each event independently. Within each event, textual and visual
features are extracted using pre-trained BERT and ResNet encoders, refined via
intra-modal self-attention, and aligned through bidirectional cross-modal
attention. A soft gating mechanism fuses these representations to form
contextualized, content-aware embeddings of each post. To model temporal
evolution, E-CaTCH segments events into overlapping time windows and uses a
trend-aware LSTM, enhanced with semantic shift and momentum signals, to encode
narrative progression over time. Classification is performed at the event
level, enabling better alignment with real-world misinformation dynamics. To
address class imbalance and promote stable learning, the model integrates
adaptive class weighting, temporal consistency regularization, and hard-example
mining. The total loss is aggregated across all events. Extensive experiments
on Fakeddit, IND, and COVID-19 MISINFOGRAPH demonstrate that E-CaTCH
consistently outperforms state-of-the-art baselines. Cross-dataset evaluations
further demonstrate its robustness, generalizability, and practical
applicability across diverse misinformation scenarios.

</details>


### [198] [Cross-Granularity Hypergraph Retrieval-Augmented Generation for Multi-hop Question Answering](https://arxiv.org/abs/2508.11247)
*Changjian Wang,Weihong Deng,Weili Guan,Quan Lu,Ning Jiang*

Main category: cs.CL

TL;DR: 提出用于多跳问答的HGRAG方法，通过超图实现结构和语义信息的跨粒度集成，实验显示在问答性能上超越SOTA方法且检索效率提升6倍。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法忽视分散知识的结构关联，GraphRAG方法过度依赖结构信息而未充分利用文本语义，限制了多跳问答任务的效果。

Method: 构建实体超图，以细粒度实体为节点、粗粒度段落为超边；设计超图检索方法，通过超图扩散集成细粒度实体相似度和粗粒度段落相似度；采用检索增强模块进一步优化检索结果。

Result: 在基准数据集上的实验表明，该方法在问答性能上超越了现有最优方法，检索效率提升6倍。

Conclusion: 提出的HGRAG方法有效实现了结构和语义信息的跨粒度集成，在多跳问答任务中表现良好。

Abstract: Multi-hop question answering (MHQA) requires integrating knowledge scattered
across multiple passages to derive the correct answer. Traditional
retrieval-augmented generation (RAG) methods primarily focus on coarse-grained
textual semantic similarity and ignore structural associations among dispersed
knowledge, which limits their effectiveness in MHQA tasks. GraphRAG methods
address this by leveraging knowledge graphs (KGs) to capture structural
associations, but they tend to overly rely on structural information and
fine-grained word- or phrase-level retrieval, resulting in an underutilization
of textual semantics. In this paper, we propose a novel RAG approach called
HGRAG for MHQA that achieves cross-granularity integration of structural and
semantic information via hypergraphs. Structurally, we construct an entity
hypergraph where fine-grained entities serve as nodes and coarse-grained
passages as hyperedges, and establish knowledge association through shared
entities. Semantically, we design a hypergraph retrieval method that integrates
fine-grained entity similarity and coarse-grained passage similarity via
hypergraph diffusion. Finally, we employ a retrieval enhancement module, which
further refines the retrieved results both semantically and structurally, to
obtain the most relevant passages as context for answer generation with the
LLM. Experimental results on benchmark datasets demonstrate that our approach
outperforms state-of-the-art methods in QA performance, and achieves a
6$\times$ speedup in retrieval efficiency.

</details>


### [199] [LETToT: Label-Free Evaluation of Large Language Models On Tourism Using Expert Tree-of-Thought](https://arxiv.org/abs/2508.11280)
*Ruiyan Qi,Congding Wen,Weibo Zhou,Shangsong Liang,Lingbo Li*

Main category: cs.CL

TL;DR: 提出LETToT框架用于无标签评估旅游领域大语言模型，验证了优化的专家ToT的有效性，并分析不同规模模型表现，建立了可扩展的无标签评估范式。


<details>
  <summary>Details</summary>
Motivation: 解决特定领域（旅游）大语言模型评估因标注基准成本高和幻觉问题带来的挑战。

Method: 提出LETToT框架，利用专家推理结构而非标注数据评估模型，迭代优化和验证分层ToT组件。

Result: 优化的专家ToT比基线有4.99 - 14.15%的相对质量提升；揭示了不同规模模型的表现特点。

Conclusion: 建立了可扩展的、无标签的特定领域大语言模型评估范式，是传统标注基准的有力替代。

Abstract: Evaluating large language models (LLMs) in specific domain like tourism
remains challenging due to the prohibitive cost of annotated benchmarks and
persistent issues like hallucinations. We propose $\textbf{L}$able-Free
$\textbf{E}$valuation of LLM on $\textbf{T}$ourism using Expert
$\textbf{T}$ree-$\textbf{o}$f-$\textbf{T}$hought (LETToT), a framework that
leverages expert-derived reasoning structures-instead of labeled data-to access
LLMs in tourism. First, we iteratively refine and validate hierarchical ToT
components through alignment with generic quality dimensions and expert
feedback. Results demonstrate the effectiveness of our systematically optimized
expert ToT with 4.99-14.15\% relative quality gains over baselines. Second, we
apply LETToT's optimized expert ToT to evaluate models of varying scales
(32B-671B parameters), revealing: (1) Scaling laws persist in specialized
domains (DeepSeek-V3 leads), yet reasoning-enhanced smaller models (e.g.,
DeepSeek-R1-Distill-Llama-70B) close this gap; (2) For sub-72B models, explicit
reasoning architectures outperform counterparts in accuracy and conciseness
($p<0.05$). Our work established a scalable, label-free paradigm for
domain-specific LLM evaluation, offering a robust alternative to conventional
annotated benchmarks.

</details>


### [200] [ToxiFrench: Benchmarking and Enhancing Language Models via CoT Fine-Tuning for French Toxicity Detection](https://arxiv.org/abs/2508.11281)
*Axel Delaval,Shujian Yang,Haicheng Wang,Han Qiu,Jialiang Lu*

Main category: cs.CL

TL;DR: 本文引入法语毒性检测基准TOXIFRENCH，发现小语言模型在毒性检测中表现更好，提出新微调策略使4B模型达SOTA，且有跨语言能力。


<details>
  <summary>Details</summary>
Motivation: 法语毒性检测因缺乏相关大规模数据集而发展不足，需推动其发展。

Method: 构建半自动化标注的TOXIFRENCH基准，进行模型基准测试，提出基于动态加权损失的思维链微调策略。

Result: 小语言模型在毒性检测中鲁棒性和泛化性更好；微调后的4B模型F1分数提升13%，超GPT - 40和Gemini - 2.5；在跨语言基准测试中表现出多语言能力。

Conclusion: 提出的方法可有效扩展到其他语言和安全关键分类任务。

Abstract: Detecting toxic content using language models is crucial yet challenging.
While substantial progress has been made in English, toxicity detection in
French remains underdeveloped, primarily due to the lack of culturally
relevant, large-scale datasets. In this work, we introduce TOXIFRENCH, a new
public benchmark of 53,622 French online comments, constructed via a
semi-automated annotation pipeline that reduces manual labeling to only 10%
through high-confidence LLM-based pre-annotation and human verification. Then,
we benchmark a broad range of models and uncover a counterintuitive insight:
Small Language Models (SLMs) outperform many larger models in robustness and
generalization under the toxicity detection task. Motivated by this finding, we
propose a novel Chain-of-Thought (CoT) fine-tuning strategy using a dynamic
weighted loss that progressively emphasizes the model's final decision,
significantly improving faithfulness. Our fine-tuned 4B model achieves
state-of-the-art performance, improving its F1 score by 13% over its baseline
and outperforming LLMs such as GPT-40 and Gemini-2.5. Further evaluation on a
cross-lingual toxicity benchmark demonstrates strong multilingual ability,
suggesting that our methodology can be effectively extended to other languages
and safety-critical classification tasks.

</details>


### [201] [Improving Text Style Transfer using Masked Diffusion Language Models with Inference-time Scaling](https://arxiv.org/abs/2508.10995)
*Tejomay Kishor Padole,Suyash P Awate,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 本文提出基于验证器的推理时间缩放方法，助力MDM在去噪过程中找到更好候选生成，实验表明MDM用于文本风格转换任务效果良好，简单验证器设置可提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 提升掩码扩散语言模型（MDM）的生成质量，使其在离散数据生成中更具优势。

Method: 提出基于验证器的推理时间缩放方法，使用现成预训练嵌入模型设置简单的基于软值的验证器。

Result: 将MDM应用于标准文本风格转换任务，证明其比自回归语言模型更优；简单验证器设置在现有无分类器引导设置基础上显著提升生成质量。

Conclusion: MDM是离散数据的优秀非自回归生成器，所提方法能有效提升其生成质量。

Abstract: Masked diffusion language models (MDMs) have recently gained traction as a
viable generative framework for natural language. This can be attributed to its
scalability and ease of training compared to other diffusion model paradigms
for discrete data, establishing itself as the state-of-the-art
non-autoregressive generator for discrete data. Diffusion models, in general,
have shown excellent ability to improve the generation quality by leveraging
inference-time scaling either by increasing the number of denoising steps or by
using external verifiers on top of the outputs of each step to guide the
generation. In this work, we propose a verifier-based inference-time scaling
method that aids in finding a better candidate generation during the denoising
process of the MDM. Our experiments demonstrate the application of MDMs for
standard text-style transfer tasks and establish MDMs as a better alternative
to autoregressive language models. Additionally, we show that a simple
soft-value-based verifier setup for MDMs using off-the-shelf pre-trained
embedding models leads to significant gains in generation quality even when
used on top of typical classifier-free guidance setups in the existing
literature.

</details>


### [202] [When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs](https://arxiv.org/abs/2508.11383)
*Mikhail Seleznyov,Mikhail Chaichuk,Gleb Ershov,Alexander Panchenko,Elena Tutubalina,Oleg Somov*

Main category: cs.CL

TL;DR: 对5种提升提示鲁棒性的方法进行系统评估并提供见解。


<details>
  <summary>Details</summary>
Motivation: 大语言模型对提示措辞和格式的细微非语义变化敏感，需评估提升提示鲁棒性的方法。

Method: 在统一实验框架下评估5种方法，在8个模型的52个任务上进行基准测试，涵盖微调与上下文学习范式，测试对多种分布偏移的泛化能力，还分析了GPT - 4.1和DeepSeek V3。

Result: 得出了不同鲁棒性方法相对有效性的发现。

Conclusion: 研究结果为从业者在实际应用中选择稳定可靠的大语言模型性能方法提供了决策依据。

Abstract: Large Language Models (LLMs) are highly sensitive to subtle, non-semantic
variations in prompt phrasing and formatting. In this work, we present the
first systematic evaluation of 5 methods for improving prompt robustness within
a unified experimental framework. We benchmark these techniques on 8 models
from Llama, Qwen and Gemma families across 52 tasks from Natural Instructions
dataset. Our evaluation covers robustness methods from both fine-tuned and
in-context learning paradigms, and tests their generalization against multiple
types of distribution shifts. Finally, we extend our analysis to GPT-4.1 and
DeepSeek V3 to assess frontier models' current robustness to format
perturbations. Our findings offer actionable insights into the relative
effectiveness of these robustness methods, enabling practitioners to make
informed decisions when aiming for stable and reliable LLM performance in
real-world applications. Code:
https://github.com/AIRI-Institute/when-punctuation-matters.

</details>


### [203] [Retrieval-augmented reasoning with lean language models](https://arxiv.org/abs/2508.11386)
*Ryan Sze-Yin Chan,Federico Nanni,Tomas Lazauskas,Rosie Wood,Penelope Yong,Lionel Tarassenko,Mark Girolami,James Geddes,Andrew Duncan*

Main category: cs.CL

TL;DR: 提出在单一轻量级语言模型架构中结合推理和检索增强生成（RAG）的新方法，评估显示该方法在答案准确性和一致性上有显著提升，且适合本地部署。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统依赖大规模模型和外部API，无法满足资源受限或安全环境下对高性能和隐私保护解决方案的需求。

Method: 基于测试时缩放和小规模推理模型的进展，开发检索增强对话代理，集成密集检索器与微调的Qwen2.5 - Instruct模型，利用合成查询生成和推理轨迹，探索多种策略对模型性能的影响。

Result: 与非推理和通用轻量级模型相比，特定领域微调方法在答案准确性和一致性上有显著提升，接近前沿水平且适合本地部署。

Conclusion: 提出的方法有效可行，代码公开利于跨领域复现和应用。

Abstract: This technical report details a novel approach to combining reasoning and
retrieval augmented generation (RAG) within a single, lean language model
architecture. While existing RAG systems typically rely on large-scale models
and external APIs, our work addresses the increasing demand for performant and
privacy-preserving solutions deployable in resource-constrained or secure
environments. Building on recent developments in test-time scaling and
small-scale reasoning models, we develop a retrieval augmented conversational
agent capable of interpreting complex, domain-specific queries using a
lightweight backbone model. Our system integrates a dense retriever with
fine-tuned Qwen2.5-Instruct models, using synthetic query generation and
reasoning traces derived from frontier models (e.g., DeepSeek-R1) over a
curated corpus, in this case, the NHS A-to-Z condition pages. We explore the
impact of summarisation-based document compression, synthetic data design, and
reasoning-aware fine-tuning on model performance. Evaluation against both
non-reasoning and general-purpose lean models demonstrates that our
domain-specific fine-tuning approach yields substantial gains in answer
accuracy and consistency, approaching frontier-level performance while
remaining feasible for local deployment. All implementation details and code
are publicly released to support reproducibility and adaptation across domains.

</details>


### [204] [Reference Points in LLM Sentiment Analysis: The Role of Structured Context](https://arxiv.org/abs/2508.11454)
*Junichiro Niimi*

Main category: cs.CL

TL;DR: 研究用大语言模型进行情感分析时补充信息的内容和格式影响，JSON提示表现佳，小模型可借结构化提示有竞争力。


<details>
  <summary>Details</summary>
Motivation: 多数NLP研究仅从评论文本分类情感，而营销理论指出顾客评价受额外参考点影响，因此研究补充信息的内容和格式对情感分析的影响。

Method: 使用适合实际营销应用的轻量级3B参数模型，比较自然语言和JSON格式提示。

Result: 在两个Yelp类别实验中，带额外信息的JSON提示无需微调就优于所有基线，Macro - F1上升，RMSE下降，可部署在资源受限边缘设备，且性能提升源于上下文推理。

Conclusion: 结构化提示能让小模型取得有竞争力的性能，为大规模模型部署提供实用替代方案。

Abstract: Large language models (LLMs) are now widely used across many fields,
including marketing research. Sentiment analysis, in particular, helps firms
understand consumer preferences. While most NLP studies classify sentiment from
review text alone, marketing theories, such as prospect theory and
expectation--disconfirmation theory, point out that customer evaluations are
shaped not only by the actual experience but also by additional reference
points. This study therefore investigates how the content and format of such
supplementary information affect sentiment analysis using LLMs. We compare
natural language (NL) and JSON-formatted prompts using a lightweight 3B
parameter model suitable for practical marketing applications. Experiments on
two Yelp categories (Restaurant and Nightlife) show that the JSON prompt with
additional information outperforms all baselines without fine-tuning: Macro-F1
rises by 1.6% and 4% while RMSE falls by 16% and 9.1%, respectively, making it
deployable in resource-constrained edge devices. Furthermore, a follow-up
analysis confirms that performance gains stem from genuine contextual reasoning
rather than label proxying. This work demonstrates that structured prompting
can enable smaller models to achieve competitive performance, offering a
practical alternative to large-scale model deployment.

</details>


### [205] [Model Interpretability and Rationale Extraction by Input Mask Optimization](https://arxiv.org/abs/2508.11388)
*Marc Brinner,Sina Zarriess*

Main category: cs.CL

TL;DR: 提出基于输入掩码生成神经网络预测解释的新方法，该方法可用于文本和图像输入，表明自然语言处理中理由提取条件有更广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 随着神经网络模型发展，对黑盒模型预测解释需求增加，需要新的生成解释方法。

Method: 基于掩码部分输入生成解释，使用基于梯度的优化和新的正则化方案，保证解释的充分性、全面性和紧凑性。

Result: 将方法应用于图像输入，获得高质量图像分类解释。

Conclusion: 弥合了模型可解释性和理由提取的差距，无需专门模型，仅基于训练好的分类器即可进行理由提取，自然语言处理中理由提取条件可广泛应用于不同输入类型。

Abstract: Concurrent to the rapid progress in the development of neural-network based
models in areas like natural language processing and computer vision, the need
for creating explanations for the predictions of these black-box models has
risen steadily. We propose a new method to generate extractive explanations for
predictions made by neural networks, that is based on masking parts of the
input which the model does not consider to be indicative of the respective
class. The masking is done using gradient-based optimization combined with a
new regularization scheme that enforces sufficiency, comprehensiveness and
compactness of the generated explanation, three properties that are known to be
desirable from the related field of rationale extraction in natural language
processing. In this way, we bridge the gap between model interpretability and
rationale extraction, thereby proving that the latter of which can be performed
without training a specialized model, only on the basis of a trained
classifier. We further apply the same method to image inputs and obtain high
quality explanations for image classifications, which indicates that the
conditions proposed for rationale extraction in natural language processing are
more broadly applicable to different input types.

</details>


### [206] [Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training](https://arxiv.org/abs/2508.11393)
*Marc Brinner,Sina Zarrieß*

Main category: cs.CL

TL;DR: 提出端到端可微训练范式用于稳定训练合理化Transformer分类器，简化训练方法并改进与人类标注的对齐。


<details>
  <summary>Details</summary>
Motivation: 解决现有合理化模型训练不稳定问题，实现更高效训练并提升与人类标注的对齐。

Method: 构建单模型同时承担三种角色，简化传统三人游戏训练方法，扩展范式生成类特定理由并结合参数化和正则化。

Result: 得到单一模型，能同时分类样本并为输入标记评分，实现更高效训练，且在无明确监督下显著提升与人类标注的对齐。

Conclusion: 所提端到端可微训练范式有效，能避免训练不稳定，提升模型性能。

Abstract: We propose an end-to-end differentiable training paradigm for stable training
of a rationalized transformer classifier. Our approach results in a single
model that simultaneously classifies a sample and scores input tokens based on
their relevance to the classification. To this end, we build on the widely-used
three-player-game for training rationalized models, which typically relies on
training a rationale selector, a classifier and a complement classifier. We
simplify this approach by making a single model fulfill all three roles,
leading to a more efficient training paradigm that is not susceptible to the
common training instabilities that plague existing approaches. Further, we
extend this paradigm to produce class-wise rationales while incorporating
recent advances in parameterizing and regularizing the resulting rationales,
thus leading to substantially improved and state-of-the-art alignment with
human annotations without any explicit supervision.

</details>


### [207] [Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme Reasoning Efficiency in Large Language Models](https://arxiv.org/abs/2508.11582)
*Qiguang Chen,Dengyun Peng,Jinhao Liu,HuiKang Su,Jiannan Guan,Libo Qin,Wanxiang Che*

Main category: cs.CL

TL;DR: 论文提出动态推理边界自我感知框架DR. SAF，可让模型按需调整推理深度，实验显示其能减少响应tokens、提升效率并缩短训练时间。


<details>
  <summary>Details</summary>
Motivation: 长思维链推理存在冗余，影响计算效率，现有方法依赖人类定义的难度先验，与大语言模型自我感知难度不一致，导致效率低下。

Method: 引入DR. SAF框架，包含边界自我感知对齐、自适应奖励管理和边界保留机制三个关键组件。

Result: DR. SAF使总响应tokens减少49.27%，token效率提升6.59倍，训练时间减少5倍，极端训练时token效率超传统模型且准确率提升超16%。

Conclusion: DR. SAF能平衡效率和准确性，适合资源受限场景。

Abstract: Recent advancements in large language models (LLMs) have greatly improved
their capabilities on complex reasoning tasks through Long Chain-of-Thought
(CoT). However, this approach often results in substantial redundancy,
impairing computational efficiency and causing significant delays in real-time
applications. To improve the efficiency, current methods often rely on
human-defined difficulty priors, which do not align with the LLM's self-awared
difficulty, leading to inefficiencies. In this paper, we introduce the Dynamic
Reasoning-Boundary Self-Awareness Framework (DR. SAF), which enables models to
dynamically assess and adjust their reasoning depth in response to problem
complexity. DR. SAF integrates three key components: Boundary Self-Awareness
Alignment, Adaptive Reward Management, and a Boundary Preservation Mechanism.
These components allow models to optimize their reasoning processes, balancing
efficiency and accuracy without compromising performance. Our experimental
results demonstrate that DR. SAF achieves a 49.27% reduction in total response
tokens with minimal loss in accuracy. The framework also delivers a 6.59x gain
in token efficiency and a 5x reduction in training time, making it well-suited
to resource-limited settings. During extreme training, DR. SAF can even surpass
traditional instruction-based models in token efficiency with more than 16%
accuracy improvement.

</details>
