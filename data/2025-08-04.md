<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 25]
- [cs.CE](#cs.CE) [Total: 6]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.GT](#cs.GT) [Total: 3]
- [cs.IR](#cs.IR) [Total: 6]
- [cs.LG](#cs.LG) [Total: 63]
- [cs.NE](#cs.NE) [Total: 4]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.SE](#cs.SE) [Total: 19]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [stat.ML](#stat.ML) [Total: 2]
- [stat.CO](#stat.CO) [Total: 2]
- [cs.NI](#cs.NI) [Total: 8]
- [cs.RO](#cs.RO) [Total: 4]
- [cs.OS](#cs.OS) [Total: 1]
- [eess.AS](#eess.AS) [Total: 2]
- [math.NA](#math.NA) [Total: 1]
- [physics.ed-ph](#physics.ed-ph) [Total: 1]
- [cs.LO](#cs.LO) [Total: 4]
- [quant-ph](#quant-ph) [Total: 6]
- [cs.CC](#cs.CC) [Total: 1]
- [math.OC](#math.OC) [Total: 3]
- [cs.CR](#cs.CR) [Total: 6]
- [cs.CV](#cs.CV) [Total: 25]
- [econ.GN](#econ.GN) [Total: 4]
- [cs.AR](#cs.AR) [Total: 1]
- [eess.SY](#eess.SY) [Total: 3]
- [math.PR](#math.PR) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [math.ST](#math.ST) [Total: 2]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.HC](#cs.HC) [Total: 8]
- [eess.IV](#eess.IV) [Total: 3]
- [cs.CL](#cs.CL) [Total: 23]
- [hep-ph](#hep-ph) [Total: 2]
- [cs.IT](#cs.IT) [Total: 2]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Rethinking Evidence Hierarchies in Medical Language Benchmarks: A Critical Evaluation of HealthBench](https://arxiv.org/abs/2508.00081)
*Fred Mutisya,Shikoh Gitau,Nasubo Ongoma,Keith Mbae,Elizabeth Wamicha*

Main category: cs.AI

TL;DR: HealthBench有局限，尤其在中低收入地区，本文提出用基于临床实践指南的奖励函数解决问题，构建更优基准。


<details>
  <summary>Details</summary>
Motivation: HealthBench依赖专家意见，存在区域偏见等局限，非洲有数据稀缺等挑战，需更具全球相关性和公平性的基准。

Method: 将奖励函数锚定在版本控制的临床实践指南上，包括规则与指南关联、证据加权评分、上下文覆盖逻辑，注重伦理考量和延迟结果反馈。

Result: 未提及具体结果。

Conclusion: 通过以严格审查的临床实践指南为基础重新构建奖励，可培养出语言出色、临床可信、伦理合理且全球相关的医疗语言模型。

Abstract: HealthBench, a benchmark designed to measure the capabilities of AI systems
for health better (Arora et al., 2025), has advanced medical language model
evaluation through physician-crafted dialogues and transparent rubrics.
However, its reliance on expert opinion, rather than high-tier clinical
evidence, risks codifying regional biases and individual clinician
idiosyncrasies, further compounded by potential biases in automated grading
systems. These limitations are particularly magnified in low- and middle-income
settings, where issues like sparse neglected tropical disease coverage and
region-specific guideline mismatches are prevalent.
  The unique challenges of the African context, including data scarcity,
inadequate infrastructure, and nascent regulatory frameworks, underscore the
urgent need for more globally relevant and equitable benchmarks. To address
these shortcomings, we propose anchoring reward functions in version-controlled
Clinical Practice Guidelines (CPGs) that incorporate systematic reviews and
GRADE evidence ratings.
  Our roadmap outlines "evidence-robust" reinforcement learning via
rubric-to-guideline linkage, evidence-weighted scoring, and contextual override
logic, complemented by a focus on ethical considerations and the integration of
delayed outcome feedback. By re-grounding rewards in rigorously vetted CPGs,
while preserving HealthBench's transparency and physician engagement, we aim to
foster medical language models that are not only linguistically polished but
also clinically trustworthy, ethically sound, and globally relevant.

</details>


### [2] [Hyperproperty-Constrained Secure Reinforcement Learning](https://arxiv.org/abs/2508.00106)
*Ernest Bonnah,Luan Viet Nguyen,Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: 本文聚焦HyperTWTL约束的安全强化学习，提出满足HyperTWTL约束的学习方法，通过案例验证有效性和可扩展性，且优于其他基线算法。


<details>
  <summary>Details</summary>
Motivation: 现有研究在利用超属性探索安全感知强化学习方面存在显著差距，需进行相关研究。

Method: 将智能体动态表示为马尔可夫决策过程，将不透明性/安全约束形式化为HyperTWTL，使用动态玻尔兹曼softmax强化学习学习满足HyperTWTL约束的安全感知最优策略。

Result: 通过机器人取货送货任务案例研究证明了所提方法的有效性和可扩展性，且优于其他两种基线强化学习算法。

Conclusion: 所提出的满足HyperTWTL约束的安全强化学习方法是有效的，具有较好性能。

Abstract: Hyperproperties for Time Window Temporal Logic (HyperTWTL) is a
domain-specific formal specification language known for its effectiveness in
compactly representing security, opacity, and concurrency properties for
robotics applications. This paper focuses on HyperTWTL-constrained secure
reinforcement learning (SecRL). Although temporal logic-constrained safe
reinforcement learning (SRL) is an evolving research problem with several
existing literature, there is a significant research gap in exploring
security-aware reinforcement learning (RL) using hyperproperties. Given the
dynamics of an agent as a Markov Decision Process (MDP) and opacity/security
constraints formalized as HyperTWTL, we propose an approach for learning
security-aware optimal policies using dynamic Boltzmann softmax RL while
satisfying the HyperTWTL constraints. The effectiveness and scalability of our
proposed approach are demonstrated using a pick-up and delivery robotic mission
case study. We also compare our results with two other baseline RL algorithms,
showing that our proposed method outperforms them.

</details>


### [3] [No AI Without PI! Object-Centric Process Mining as the Enabler for Generative, Predictive, and Prescriptive Artificial Intelligence](https://arxiv.org/abs/2508.00116)
*Wil M. P. van der Aalst*

Main category: cs.AI

TL;DR: 本文探讨AI在工业场景应用挑战，提出用OCPM落地AI，阐述Process Intelligence（PI）概念及结合OCPM与AI的机会。


<details>
  <summary>Details</summary>
Motivation: 解决组织在工业场景将AI成功应用于端到端运营流程的难题。

Method: 考虑生成式、预测式和规范性AI，引入OCPM，提出PI概念。

Result: 表明AI需用OCPM落地，PI可结合OCPM与不同形式AI。

Conclusion: AI需要PI来改进运营流程，结合OCPM和不同形式AI存在机会。

Abstract: The uptake of Artificial Intelligence (AI) impacts the way we work, interact,
do business, and conduct research. However, organizations struggle to apply AI
successfully in industrial settings where the focus is on end-to-end
operational processes. Here, we consider generative, predictive, and
prescriptive AI and elaborate on the challenges of diagnosing and improving
such processes. We show that AI needs to be grounded using Object-Centric
Process Mining (OCPM). Process-related data are structured and
organization-specific and, unlike text, processes are often highly dynamic.
OCPM is the missing link connecting data and processes and enables different
forms of AI. We use the term Process Intelligence (PI) to refer to the
amalgamation of process-centric data-driven techniques able to deal with a
variety of object and event types, enabling AI in an organizational context.
This paper explains why AI requires PI to improve operational processes and
highlights opportunities for successfully combining OCPM and generative,
predictive, and prescriptive AI.

</details>


### [4] [Algorithmic Detection of Rank Reversals, Transitivity Violations, and Decomposition Inconsistencies in Multi-Criteria Decision Analysis](https://arxiv.org/abs/2508.00129)
*Agustín Borda,Juan Bautista Cabral,Gonzalo Giarda,Diego Nicolás Gimenez Irusta,Paula Pacheco,Alvaro Roy Schachner*

Main category: cs.AI

TL;DR: 本文提出检测排名反转的三个测试及在Scikit - Criteria库中的实现，讨论其对评判多准则决策方法的作用。


<details>
  <summary>Details</summary>
Motivation: 多准则决策分析中排名反转会影响决策结果，需机制衡量方法性能并对不同方法有效性进行全局排名。

Method: 提出三个检测排名反转的测试，并在Scikit - Criteria库中实现，同时处理通用场景下的实现问题。

Result: 完成三个测试的实现，解决通用场景下的实现难题。

Conclusion: 这些测试的添加对评判多准则决策方法解决问题有重要作用。

Abstract: In Multi-Criteria Decision Analysis, Rank Reversals are a serious problem
that can greatly affect the results of a Multi-Criteria Decision Method against
a particular set of alternatives. It is therefore useful to have a mechanism
that allows one to measure the performance of a method on a set of
alternatives. This idea could be taken further to build a global ranking of the
effectiveness of different methods to solve a problem. In this paper, we
present three tests that detect the presence of Rank Reversals, along with
their implementation in the Scikit-Criteria library. We also address the
complications that arise when implementing these tests for general scenarios
and the design considerations we made to handle them. We close with a
discussion about how these additions could play a major role in the judgment of
multi-criteria decision methods for problem solving.

</details>


### [5] [SHACL Validation under Graph Updates (Extended Paper)](https://arxiv.org/abs/2508.00137)
*Shqiponja Ahmetaj,George Konstantinidis,Magdalena Ortiz,Paolo Pareti,Mantas Simkus*

Main category: cs.AI

TL;DR: 本文研究RDF图在更新下的SHACL验证，提出更新语言，将更新下的静态验证问题转化为SHACL约束的可满足性问题，分析复杂度并实现原型。


<details>
  <summary>Details</summary>
Motivation: 研究RDF图在更新情况下的SHACL验证，为演化RDF图的推理服务提供基础。

Method: 使用将更新动作嵌入SHACL约束的回归技术，将更新下的静态验证问题转化为约束的可满足性问题。

Result: 分析了SHACL及关键片段的静态验证问题的计算复杂度，实现了执行静态验证和其他静态分析任务的原型。

Conclusion: 提出的方法可解决RDF图更新下的SHACL静态验证问题，为相关推理服务提供支持。

Abstract: SHACL (SHApe Constraint Language) is a W3C standardized constraint language
for RDF graphs. In this paper, we study SHACL validation in RDF graphs under
updates. We present a SHACL-based update language that can capture intuitive
and realistic modifications on RDF graphs and study the problem of static
validation under such updates. This problem asks to verify whether every graph
that validates a SHACL specification will still do so after applying a given
update sequence. More importantly, it provides a basis for further services for
reasoning about evolving RDF graphs. Using a regression technique that embeds
the update actions into SHACL constraints, we show that static validation under
updates can be reduced to (un)satisfiability of constraints in (a minor
extension of) SHACL. We analyze the computational complexity of the static
validation problem for SHACL and some key fragments. Finally, we present a
prototype implementation that performs static validation and other static
analysis tasks on SHACL constraints and demonstrate its behavior through
preliminary experiments.

</details>


### [6] [Co-Producing AI: Toward an Augmented, Participatory Lifecycle](https://arxiv.org/abs/2508.00138)
*Rashid Mushkani,Hugo Berard,Toumadher Ammar,Cassandre Chatonnier,Shin Koseki*

Main category: cs.AI

TL;DR: 现有AI算法对文化边缘群体有负面影响，提出需重新架构AI生产流程，引入含五个阶段的增强AI生命周期并关联伦理框架，列出后续研究问题。


<details>
  <summary>Details</summary>
Motivation: 当前减轻AI算法风险和偏见的努力未能解决其对文化边缘群体的不利影响，需要新方法。

Method: 借鉴设计正义、扩展学习理论和参与式AI的实证研究，通过四个多学科研讨会，提出增强AI生命周期。

Result: 提出由五个相互关联阶段组成的增强AI生命周期。

Conclusion: 减轻AI算法对文化边缘群体的伤害需要重新架构AI生产流程，提出的生命周期与伦理框架相关，同时指出了扩大参与式治理的关键研究问题。

Abstract: Despite efforts to mitigate the inherent risks and biases of artificial
intelligence (AI) algorithms, these algorithms can disproportionately impact
culturally marginalized groups. A range of approaches has been proposed to
address or reduce these risks, including the development of ethical guidelines
and principles for responsible AI, as well as technical solutions that promote
algorithmic fairness. Drawing on design justice, expansive learning theory, and
recent empirical work on participatory AI, we argue that mitigating these harms
requires a fundamental re-architecture of the AI production pipeline. This
re-design should center co-production, diversity, equity, inclusion (DEI), and
multidisciplinary collaboration. We introduce an augmented AI lifecycle
consisting of five interconnected phases: co-framing, co-design,
co-implementation, co-deployment, and co-maintenance. The lifecycle is informed
by four multidisciplinary workshops and grounded in themes of distributed
authority and iterative knowledge exchange. Finally, we relate the proposed
lifecycle to several leading ethical frameworks and outline key research
questions that remain for scaling participatory governance.

</details>


### [7] [Beyond Agreement: Rethinking Ground Truth in Educational AI Annotation](https://arxiv.org/abs/2508.00143)
*Danielle R. Thomas,Conrad Borchers,Kenneth R. Koedinger*

Main category: cs.AI

TL;DR: 文章指出过度依赖人类评分者间信度（IRR）衡量标注质量阻碍教育数据分类进展，提出五种补充评估方法，呼吁重新思考标注质量和真值。


<details>
  <summary>Details</summary>
Motivation: 解决在教育应用中大量生成训练数据时，过度依赖传统IRR指标衡量标注质量阻碍有效预测性数据分类以提升学习效果的问题。

Method: 提出多标签标注方案、基于专家的方法、闭环有效性等五种补充评估方法，强调外部有效性。

Result: 这些补充评估方法比单独的IRR方法更能产生提升学生学习效果和提供更多可行动见解的训练数据和模型。

Conclusion: 呼吁该领域重新思考标注质量和真值，优先考虑有效性和教育影响而非仅追求共识。

Abstract: Humans can be notoriously imperfect evaluators. They are often biased,
unreliable, and unfit to define "ground truth." Yet, given the surging need to
produce large amounts of training data in educational applications using AI,
traditional inter-rater reliability (IRR) metrics like Cohen's kappa remain
central to validating labeled data. IRR remains a cornerstone of many machine
learning pipelines for educational data. Take, for example, the classification
of tutors' moves in dialogues or labeling open responses in machine-graded
assessments. This position paper argues that overreliance on human IRR as a
gatekeeper for annotation quality hampers progress in classifying data in ways
that are valid and predictive in relation to improving learning. To address
this issue, we highlight five examples of complementary evaluation methods,
such as multi-label annotation schemes, expert-based approaches, and
close-the-loop validity. We argue that these approaches are in a better
position to produce training data and subsequent models that produce improved
student learning and more actionable insights than IRR approaches alone. We
also emphasize the importance of external validity, for example, by
establishing a procedure of validating tutor moves and demonstrating that it
works across many categories of tutor actions (e.g., providing hints). We call
on the field to rethink annotation quality and ground truth--prioritizing
validity and educational impact over consensus alone.

</details>


### [8] [Model-Based Soft Maximization of Suitable Metrics of Long-Term Human Power](https://arxiv.org/abs/2508.00159)
*Jobst Heitzig,Ram Potham*

Main category: cs.AI

TL;DR: 本文探讨通过让AI赋能人类、管理人机权力平衡来促进安全与福祉，设计目标函数并推导计算算法，认为最大化人类权力指标对AI系统更安全。


<details>
  <summary>Details</summary>
Motivation: 权力在AI安全和人类福祉中至关重要，研究如何让AI赋能人类并管理人机权力平衡，以促进安全与福祉。

Method: 采用原则性、部分公理化方法设计可参数化和可分解的目标函数，通过反向归纳或多智能体强化学习计算该指标。

Result: 推导了计算指标的算法，举例说明了最大化该指标的后果和可能的工具性子目标。

Conclusion: 适度最大化人类权力的聚合指标可能是比直接基于效用的目标更安全的AI系统目标。

Abstract: Power is a key concept in AI safety: power-seeking as an instrumental goal,
sudden or gradual disempowerment of humans, power balance in human-AI
interaction and international AI governance. At the same time, power as the
ability to pursue diverse goals is essential for wellbeing.
  This paper explores the idea of promoting both safety and wellbeing by
forcing AI agents explicitly to empower humans and to manage the power balance
between humans and AI agents in a desirable way. Using a principled, partially
axiomatic approach, we design a parametrizable and decomposable objective
function that represents an inequality- and risk-averse long-term aggregate of
human power. It takes into account humans' bounded rationality and social
norms, and, crucially, considers a wide variety of possible human goals.
  We derive algorithms for computing that metric by backward induction or
approximating it via a form of multi-agent reinforcement learning from a given
world model. We exemplify the consequences of (softly) maximizing this metric
in a variety of paradigmatic situations and describe what instrumental
sub-goals it will likely imply. Our cautious assessment is that softly
maximizing suitable aggregate metrics of human power might constitute a
beneficial objective for agentic AI systems that is safer than direct
utility-based objectives.

</details>


### [9] [MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning](https://arxiv.org/abs/2508.00271)
*Hongjin Qian,Zheng Liu*

Main category: cs.AI

TL;DR: 提出MetaAgent，基于边做边学，通过元工具学习提升能力，在知识发现基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 构建能自我进化、用于通用知识发现的智能体系统。

Method: MetaAgent从最小工作流开始，遇到知识缺口时生成请求并由工具路由器分配，解决任务时进行自我反思和答案验证，将经验融入后续任务，自主构建工具和知识库，进行元工具学习。

Result: 在GAIA、WebWalkerQA和BrowseCamp等基准测试中，MetaAgent始终优于基于工作流的基线，与端到端训练的智能体相当或更优。

Conclusion: MetaAgent展示了自我进化的智能体系统在鲁棒、通用知识发现方面的潜力。

Abstract: In this work, we propose MetaAgent, an agentic paradigm inspired by the
principle of learning-by-doing, where expertise is developed through hands-on
practice and continual self-improvement. MetaAgent starts with a minimal
workflow, equipped only with basic reasoning and adaptive help-seeking
abilities. When a knowledge gap is encountered, MetaAgent generates natural
language help requests, which are routed to the most suitable external tool by
a dedicated tool router. As MetaAgent solves tasks, it continually conducts
self-reflection and answer verification, distilling actionable experience into
concise texts that are dynamically incorporated into future task contexts.
Besides, MetaAgent autonomously builds in-house tools and a persistent
knowledge base by organizing its tool-use history, further enhancing its
ability to retrieve and integrate relevant information We term this continual,
data-driven process as \textit{meta tool learning}, through which MetaAgent
incrementally refines its reasoning and tool-use strategies, without changing
model parameters or requiring further post-training. Evaluated on challenging
knowledge discovery benchmarks, including GAIA, WebWalkerQA, and BrowseCamp,
MetaAgent consistently outperforms workflow-based baselines and matches or
exceeds end-to-end trained agents, demonstrating the promise of self-evolving
agentic systems for robust, general-purpose knowledge discovery. We provide our
source codes in https://github.com/qhjqhj00/MetaAgent.

</details>


### [10] [RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization](https://arxiv.org/abs/2508.00222)
*Yihong Dong,Xue Jiang,Yongding Tao,Huanyu Liu,Kechi Zhang,Lili Mou,Rongyu Cao,Yingwei Ma,Jue Chen,Binhua Li,Zhi Jin,Fei Huang,Yongbin Li,Ge Li*

Main category: cs.AI

TL;DR: 现有RLVR方法提升大语言模型推理能力有局限，本文提出RL - PLUS方法，结合内部探索与外部数据，实验证明其性能优越且解决了能力边界坍塌问题。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法难以突破基础大语言模型的能力边界，还会导致能力边界坍塌，缩小模型解决问题的范围。

Method: 提出RL - PLUS方法，集成多重重要性采样和基于探索的优势函数两个核心组件。

Result: 在六个数学推理基准测试中表现优于现有RLVR方法，在六个分布外推理任务中表现出色，在不同模型族上有显著提升，平均相对改进从21.1%到69.2%，解决了能力边界坍塌问题。

Conclusion: RL - PLUS方法具有优越性和可推广性，能提升大语言模型推理能力并解决能力边界坍塌问题。

Abstract: Reinforcement Learning with Verifiable Reward (RLVR) has significantly
advanced the complex reasoning abilities of Large Language Models (LLMs).
However, it struggles to break through the inherent capability boundaries of
the base LLM, due to its inherently on-policy strategy with LLM's immense
action space and sparse reward. Further, RLVR can lead to the capability
boundary collapse, narrowing the LLM's problem-solving scope. To address this
problem, we propose RL-PLUS, a novel approach that synergizes internal
exploitation (i.e., Thinking) with external data (i.e., Learning) to achieve
stronger reasoning capabilities and surpass the boundaries of base models.
RL-PLUS integrates two core components: Multiple Importance Sampling to address
for distributional mismatch from external data, and an Exploration-Based
Advantage Function to guide the model towards high-value, unexplored reasoning
paths. We provide both theoretical analysis and extensive experiments to
demonstrate the superiority and generalizability of our approach. The results
show that RL-PLUS achieves state-of-the-art performance compared with existing
RLVR methods on six math reasoning benchmarks and exhibits superior performance
on six out-of-distribution reasoning tasks. It also achieves consistent and
significant gains across diverse model families, with average relative
improvements ranging from 21.1\% to 69.2\%. Moreover, Pass@k curves across
multiple benchmarks indicate that RL-PLUS effectively resolves the capability
boundary collapse problem.

</details>


### [11] [Mind the Gap: The Divergence Between Human and LLM-Generated Tasks](https://arxiv.org/abs/2508.00282)
*Yi-Long Lu,Jiajun Song,Chunhui Zhang,Wei Wang*

Main category: cs.AI

TL;DR: 通过对比人类与GPT - 4o生成任务的实验，发现LLM生成任务与人类有差异，提出设计更拟人化智能体需考虑内在动机和物理基础。


<details>
  <summary>Details</summary>
Motivation: 探究由大语言模型驱动的生成式智能体是否与人类基于相似认知原则进行任务生成。

Method: 进行任务生成实验，对比人类与GPT - 4o的响应。

Result: 人类任务生成受心理驱动影响，LLM即便获得这些信息也无法反映对应行为模式，其生成任务社交性和物理性低、主题偏抽象，虽被认为更有趣新颖，但与人类有差距。

Conclusion: 人类认知的价值驱动、具身性与LLM的统计模式存在核心差距，设计更拟人化智能体需融入内在动机和物理基础。

Abstract: Humans constantly generate a diverse range of tasks guided by internal
motivations. While generative agents powered by large language models (LLMs)
aim to simulate this complex behavior, it remains uncertain whether they
operate on similar cognitive principles. To address this, we conducted a
task-generation experiment comparing human responses with those of an LLM agent
(GPT-4o). We find that human task generation is consistently influenced by
psychological drivers, including personal values (e.g., Openness to Change) and
cognitive style. Even when these psychological drivers are explicitly provided
to the LLM, it fails to reflect the corresponding behavioral patterns. They
produce tasks that are markedly less social, less physical, and thematically
biased toward abstraction. Interestingly, while the LLM's tasks were perceived
as more fun and novel, this highlights a disconnect between its linguistic
proficiency and its capacity to generate human-like, embodied goals.We conclude
that there is a core gap between the value-driven, embodied nature of human
cognition and the statistical patterns of LLMs, highlighting the necessity of
incorporating intrinsic motivation and physical grounding into the design of
more human-aligned agents.

</details>


### [12] [Oedipus and the Sphinx: Benchmarking and Improving Visual Language Models for Complex Graphic Reasoning](https://arxiv.org/abs/2508.00323)
*Jianyi Zhang,Xu Ji,Ziyin Zhou,Yuchen Zhou,Shubo Shi,Haoyu Wu,Zhen Li,Shizhao Liu*

Main category: cs.AI

TL;DR: 提出专注结构化图形推理任务的评估基准ReasonBench，对11个主流VLM模型进行测试，揭示其局限并提出双优化策略提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLM模型在复杂图形推理和抽象问题解决方面存在不足，且相关研究较少，需评估其在复杂图形推理中的性能。

Method: 提出包含1613个来自现实智力测试问题的ReasonBench基准，对11个主流VLM模型进行测试，提出双优化策略（DiaCoT和ReasonTune）。

Result: 揭示当前模型存在显著局限性，双优化策略使VLM性能提升33.5%。

Conclusion: ReasonBench可为VLM模型在复杂图形推理任务上提供全面评估，双优化策略能有效提升模型性能。

Abstract: Evaluating the performance of visual language models (VLMs) in graphic
reasoning tasks has become an important research topic. However, VLMs still
show obvious deficiencies in simulating human-level graphic reasoning
capabilities, especially in complex graphic reasoning and abstract problem
solving, which are less studied and existing studies only focus on simple
graphics. To evaluate the performance of VLMs in complex graphic reasoning, we
propose ReasonBench, the first evaluation benchmark focused on structured
graphic reasoning tasks, which includes 1,613 questions from real-world
intelligence tests. ReasonBench covers reasoning dimensions related to
location, attribute, quantity, and multi-element tasks, providing a
comprehensive evaluation of the performance of VLMs in spatial, relational, and
abstract reasoning capabilities. We benchmark 11 mainstream VLMs (including
closed-source and open-source models) and reveal significant limitations of
current models. Based on these findings, we propose a dual optimization
strategy: Diagrammatic Reasoning Chain (DiaCoT) enhances the interpretability
of reasoning by decomposing layers, and ReasonTune enhances the task
adaptability of model reasoning through training, all of which improves VLM
performance by 33.5\%. All experimental data and code are in the repository:
https://huggingface.co/datasets/cistine/ReasonBench.

</details>


### [13] [R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge](https://arxiv.org/abs/2508.00324)
*Yeonjun In,Wonjoong Kim,Sangwu Park,Chanyoung Park*

Main category: cs.AI

TL;DR: 本文研究大推理模型安全风险根源，提出R1 - Act后训练方法提升安全性，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 大推理模型常执行有害指令，存在安全隐患，需解决其安全风险。

Method: 提出R1 - Act后训练方法，通过结构化推理过程触发安全知识。

Result: R1 - Act在提升安全性同时保持推理性能，优于先前对齐方法，仅需少量训练样本和时间。

Conclusion: R1 - Act方法具有鲁棒性、可扩展性和实际效率。

Abstract: Although large reasoning models (LRMs) have demonstrated impressive
capabilities on complex tasks, recent studies reveal that these models
frequently fulfill harmful user instructions, raising significant safety
concerns. In this paper, we investigate the underlying cause of LRM safety
risks and find that models already possess sufficient safety knowledge but fail
to activate it during reasoning. Based on this insight, we propose R1-Act, a
simple and efficient post-training method that explicitly triggers safety
knowledge through a structured reasoning process. R1-Act achieves strong safety
improvements while preserving reasoning performance, outperforming prior
alignment methods. Notably, it requires only 1,000 training examples and 90
minutes of training on a single RTX A6000 GPU. Extensive experiments across
multiple LRM backbones and sizes demonstrate the robustness, scalability, and
practical efficiency of our approach.

</details>


### [14] [CoRGI: Verified Chain-of-Thought Reasoning with Visual Grounding](https://arxiv.org/abs/2508.00378)
*Shixin Yi,Lin Shang*

Main category: cs.AI

TL;DR: 提出CoRGI框架解决VLM中CoT提示推理缺乏视觉依据问题，在VCR基准测试中提升推理表现，强调视觉证据对多模态推理的重要性。


<details>
  <summary>Details</summary>
Motivation: 解决CoT提示在VLM中推理时生成的解释缺乏视觉依据、存在幻觉的问题。

Method: 提出CoRGI模块化框架，采用三阶段流程，先生成推理链，再提取视觉证据，最后合成答案，可集成到现有VLM。

Result: 在VCR基准测试中，提升了Qwen - 2.5VL和LLaVA - 1.6的推理表现，消融实验和人工评估证明其有效性。

Conclusion: 强调将中间推理步骤与视觉证据相结合对增强多模态推理鲁棒性的重要性。

Abstract: Chain-of-Thought (CoT) prompting has shown promise in improving reasoning in
vision-language models (VLMs), but it often produces explanations that are
linguistically fluent yet lack grounding in visual content. We observe that
such hallucinations arise in part from the absence of an explicit verification
mechanism during multi-step reasoning. To address this, we propose
\textbf{CoRGI}(\textbf{C}hain \textbf{o}f \textbf{R}easoning with
\textbf{G}rounded \textbf{I}nsights), a modular framework that introduces
visual verification into the reasoning process. CoRGI follows a three-stage
pipeline: it first generates a textual reasoning chain, then extracts
supporting visual evidence for each reasoning step via a dedicated module
(VEVM), and finally synthesizes the textual rationale with visual evidence to
generate a grounded, verified answer. The framework can be integrated with
existing VLMs without end-to-end retraining. We evaluate CoRGI on the VCR
benchmark and find that it improves reasoning performance on two representative
open-source VLM backbones, Qwen-2.5VL and LLaVA-1.6. Ablation studies confirm
the contribution of each step in the verification module, and human evaluations
suggest that CoRGI leads to more factual and helpful explanations. We also
examine alternative designs for the visual verification step and discuss
potential limitations of post-hoc verification frameworks. These findings
highlight the importance of grounding intermediate reasoning steps in visual
evidence to enhance the robustness of multimodal reasoning.

</details>


### [15] [Theory of Mind Using Active Inference: A Framework for Multi-Agent Cooperation](https://arxiv.org/abs/2508.00401)
*Riddhi J. Pitliya,Ozan Catal,Toon Van de Maele,Corrado Pezzato,Tim Verbelen*

Main category: cs.AI

TL;DR: 提出在主动推理中实现心智理论（ToM）的多智能体合作新方法，经模拟评估效果良好，推动了AI应用和对ToM的计算洞察。


<details>
  <summary>Details</summary>
Motivation: 提出不依赖特定任务共享生成模型、无需明确通信且具有通用性的多智能体合作新方法。

Method: 在主动推理中实现ToM，智能体维护自身和他人信念与目标的不同表示，扩展推理树规划算法探索联合策略空间。

Result: 在避碰和觅食任务模拟中，配备ToM的智能体比无ToM的表现更好，能避免碰撞和减少冗余工作。

Conclusion: 该工作推动了人工智能实际应用，为ToM提供计算见解。

Abstract: We present a novel approach to multi-agent cooperation by implementing theory
of mind (ToM) within active inference. ToM - the ability to understand that
others can have differing knowledge and goals - enables agents to reason about
others' beliefs while planning their own actions. Unlike previous active
inference approaches to multi-agent cooperation, our method neither relies on
task-specific shared generative models nor requires explicit communication,
while being generalisable. In our framework, the ToM-equipped agent maintains
distinct representations of its own and others' beliefs and goals. We extend
the sophisticated inference tree-based planning algorithm to systematically
explore joint policy spaces through recursive reasoning. Our approach is
evaluated through collision avoidance and foraging task simulations. Results
demonstrate that ToM-equipped agents cooperate better compared to non-ToM
counterparts by being able to avoid collisions and reduce redundant efforts.
Crucially, ToM agents accomplish this by inferring others' beliefs solely from
observable behaviour. This work advances practical applications in artificial
intelligence while providing computational insights into ToM.

</details>


### [16] [Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training](https://arxiv.org/abs/2508.00414)
*Tianqing Fang,Zhisong Zhang,Xiaoyang Wang,Rui Wang,Can Qin,Yuxuan Wan,Jun-Yu Ma,Ce Zhang,Jiaqi Chen,Xiyun Li,Hongming Zhang,Haitao Mi,Dong Yu*

Main category: cs.AI

TL;DR: 本文提出开源免费的多模块智能体框架Cognitive Kernel - Pro，研究高质量训练数据整理和新策略，在GAIA上评估取得开源免费智能体中的最优结果。


<details>
  <summary>Details</summary>
Motivation: 当前智能体系统闭源或依赖付费API和专有工具，限制研究社区的可访问性和可复现性，需开发开源免费框架。

Method: 提出Cognitive Kernel - Pro框架，系统研究面向智能体基础模型的高质量训练数据整理，探索智能体测试时反思和投票的新策略。

Result: 在GAIA上评估，Cognitive Kernel - Pro在开源免费智能体中取得最优结果，8B参数的开源模型超越先前领先系统。

Conclusion: Cognitive Kernel - Pro为高级AI智能体的开发和评估提供了开源免费途径，树立了高可用性、高能力AI智能体的新性能标准。

Abstract: General AI Agents are increasingly recognized as foundational frameworks for
the next generation of artificial intelligence, enabling complex reasoning, web
interaction, coding, and autonomous research capabilities. However, current
agent systems are either closed-source or heavily reliant on a variety of paid
APIs and proprietary tools, limiting accessibility and reproducibility for the
research community. In this work, we present \textbf{Cognitive Kernel-Pro}, a
fully open-source and (to the maximum extent) free multi-module agent framework
designed to democratize the development and evaluation of advanced AI agents.
Within Cognitive Kernel-Pro, we systematically investigate the curation of
high-quality training data for Agent Foundation Models, focusing on the
construction of queries, trajectories, and verifiable answers across four key
domains: web, file, code, and general reasoning. Furthermore, we explore novel
strategies for agent test-time reflection and voting to enhance agent
robustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving
state-of-the-art results among open-source and free agents. Notably, our
8B-parameter open-source model surpasses previous leading systems such as
WebDancer and WebSailor, establishing a new performance standard for
accessible, high-capability AI agents. Code is available at
https://github.com/Tencent/CognitiveKernel-Pro

</details>


### [17] [Thinking Machines: Mathematical Reasoning in the Age of LLMs](https://arxiv.org/abs/2508.00459)
*Andrea Asperti,Alberto Naibo,Claudio Sacerdoti Coen*

Main category: cs.AI

TL;DR: 本文探讨大语言模型在数学应用中的现状，提出机器学习与数学认知交叉的三个核心问题并明确目标。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在编程领域成功，但在形式数学进展困难，引发对其推理、监督及状态跟踪等问题的思考。

Method: 聚焦最新模型和基准，探讨三个核心问题。

Result: 未提及具体研究结果。

Conclusion: 目标不是划定界限，而是明确当前限制并思考如何突破。

Abstract: Large Language Models (LLMs) have shown remarkable abilities in structured
reasoning and symbolic tasks, with coding emerging as a particular area of
strength. This success has sparked growing interest in applying LLMs to
mathematics, both in informal problem-solving and formal theorem proving.
However, progress in formal mathematics has proven to be significantly more
difficult, despite surface-level similarities between programming and proof
construction. This discrepancy raises important questions about how LLMs
``reason'', how they are supervised, and whether they internally track a notion
of computational or deductive state. In this article, we address the
state-of-the-art of the discipline, focusing on recent models and benchmarks,
and explore three central issues at the intersection of machine learning and
mathematical cognition: (i) the trade-offs between formal and informal
mathematics as training domains; (ii) the deeper reasons why proof generation
remains more brittle than code synthesis; (iii) and the question of whether
LLMs represent, or merely mimic, a notion of evolving logical state. Our goal
is not to draw hard boundaries, but to identify where the current limits lie,
and how they might be extended.

</details>


### [18] [Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via Probabilistic Model Checking](https://arxiv.org/abs/2508.00500)
*Haoyu Wang,Chris M. Poskitt,Jun Sun,Jiali Wei*

Main category: cs.AI

TL;DR: 提出Pro2Guard框架解决大语言模型代理随机行为带来的安全风险，在多领域评估效果好。


<details>
  <summary>Details</summary>
Motivation: 大语言模型代理随机行为有安全风险，现有规则系统缺乏前瞻性，难以处理长时依赖和分布偏移。

Method: 提出Pro2Guard框架，将代理行为抽象为符号状态，学习离散时间马尔可夫链，运行时估计到达不安全状态概率，超阈值干预，结合语义检查和PAC边界确保可靠性。

Result: 在家庭代理和自动驾驶场景评估，家庭代理任务最多93.6%提前执行安全措施，任务完成率达80.4%；自动驾驶场景100%预测违规和碰撞，提前38.66秒预警。

Conclusion: Pro2Guard能有效解决大语言模型代理的安全问题，有良好应用效果。

Abstract: Large Language Model (LLM) agents exhibit powerful autonomous capabilities
across domains such as robotics, virtual assistants, and web automation.
However, their stochastic behavior introduces significant safety risks that are
difficult to anticipate. Existing rule-based enforcement systems, such as
AgentSpec, focus on developing reactive safety rules, which typically respond
only when unsafe behavior is imminent or has already occurred. These systems
lack foresight and struggle with long-horizon dependencies and distribution
shifts. To address these limitations, we propose Pro2Guard, a proactive runtime
enforcement framework grounded in probabilistic reachability analysis.
Pro2Guard abstracts agent behaviors into symbolic states and learns a
Discrete-Time Markov Chain (DTMC) from execution traces. At runtime, it
anticipates future risks by estimating the probability of reaching unsafe
states, triggering interventions before violations occur when the predicted
risk exceeds a user-defined threshold. By incorporating semantic validity
checks and leveraging PAC bounds, Pro2Guard ensures statistical reliability
while approximating the underlying ground-truth model. We evaluate Pro2Guard
extensively across two safety-critical domains: embodied household agents and
autonomous vehicles. In embodied agent tasks, Pro2Guard enforces safety early
on up to 93.6% of unsafe tasks using low thresholds, while configurable modes
(e.g., reflect) allow balancing safety with task success, maintaining up to
80.4% task completion. In autonomous driving scenarios, Pro2Guard achieves 100%
prediction of traffic law violations and collisions, anticipating risks up to
38.66 seconds ahead.

</details>


### [19] [MultiSHAP: A Shapley-Based Framework for Explaining Cross-Modal Interactions in Multimodal AI Models](https://arxiv.org/abs/2508.00576)
*Zhanliang Wang,Kai Wang*

Main category: cs.AI

TL;DR: 现有多模态AI模型解释方法有局限，本文提出MultiSHAP框架，能对开闭源模型做细粒度跨模态交互解释，实验验证其有效性和扩展性。


<details>
  <summary>Details</summary>
Motivation: 多模态AI模型的黑盒性质阻碍其在高风险场景应用，现有解释方法无法精确量化模态间协同效应且局限于开源模型。

Method: 引入MultiSHAP框架，利用Shapley Interaction Index将多模态预测归因于细粒度视觉和文本元素间的成对交互。

Result: 在公共多模态基准测试中证实能忠实捕捉跨模态推理机制，真实案例研究展示其实用性。

Conclusion: MultiSHAP框架可扩展到两种以上模态，为解释复杂多模态AI模型提供通用解决方案。

Abstract: Multimodal AI models have achieved impressive performance in tasks that
require integrating information from multiple modalities, such as vision and
language. However, their "black-box" nature poses a major barrier to deployment
in high-stakes applications where interpretability and trustworthiness are
essential. How to explain cross-modal interactions in multimodal AI models
remains a major challenge. While existing model explanation methods, such as
attention map and Grad-CAM, offer coarse insights into cross-modal
relationships, they cannot precisely quantify the synergistic effects between
modalities, and are limited to open-source models with accessible internal
weights. Here we introduce MultiSHAP, a model-agnostic interpretability
framework that leverages the Shapley Interaction Index to attribute multimodal
predictions to pairwise interactions between fine-grained visual and textual
elements (such as image patches and text tokens), while being applicable to
both open- and closed-source models. Our approach provides: (1) instance-level
explanations that reveal synergistic and suppressive cross-modal effects for
individual samples - "why the model makes a specific prediction on this input",
and (2) dataset-level explanation that uncovers generalizable interaction
patterns across samples - "how the model integrates information across
modalities". Experiments on public multimodal benchmarks confirm that MultiSHAP
faithfully captures cross-modal reasoning mechanisms, while real-world case
studies demonstrate its practical utility. Our framework is extensible beyond
two modalities, offering a general solution for interpreting complex multimodal
AI models.

</details>


### [20] [From EMR Data to Clinical Insight: An LLM-Driven Framework for Automated Pre-Consultation Questionnaire Generation](https://arxiv.org/abs/2508.00581)
*Ruiqing Ding,Qianfang Sun,Yongkang Leng,Hui Yin,Xiaojian Li*

Main category: cs.AI

TL;DR: 本文提出多阶段大语言模型驱动框架，从电子病历生成预咨询问卷，经评估性能优越，有实际应用潜力。


<details>
  <summary>Details</summary>
Motivation: 直接使用大语言模型从复杂大量电子病历生成预咨询问卷存在信息完整性、逻辑顺序和疾病层面综合等难题。

Method: 提出三阶段框架：第一阶段从电子病历提取原子断言；第二阶段构建个人因果网络并综合疾病知识；第三阶段生成个性化和标准化疾病特定问卷。

Result: 在真实电子病历数据集上评估，经临床专家验证，在信息覆盖、诊断相关性、可理解性和生成时间方面表现优越。

Conclusion: 该框架有实际潜力提升患者信息收集。

Abstract: Pre-consultation is a critical component of effective healthcare delivery.
However, generating comprehensive pre-consultation questionnaires from complex,
voluminous Electronic Medical Records (EMRs) is a challenging task. Direct
Large Language Model (LLM) approaches face difficulties in this task,
particularly regarding information completeness, logical order, and
disease-level synthesis. To address this issue, we propose a novel multi-stage
LLM-driven framework: Stage 1 extracts atomic assertions (key facts with
timing) from EMRs; Stage 2 constructs personal causal networks and synthesizes
disease knowledge by clustering representative networks from an EMR corpus;
Stage 3 generates tailored personal and standardized disease-specific
questionnaires based on these structured representations. This framework
overcomes limitations of direct methods by building explicit clinical
knowledge. Evaluated on a real-world EMR dataset and validated by clinical
experts, our method demonstrates superior performance in information coverage,
diagnostic relevance, understandability, and generation time, highlighting its
practical potential to enhance patient information collection.

</details>


### [21] [Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings](https://arxiv.org/abs/2508.00632)
*Alexia Jolicoeur-Martineau*

Main category: cs.AI

TL;DR: 当前大语言模型生成交互式视听内容有挑战，本文提出AVR - Eval指标和AVR - Agent多智能体系统，实验表明AVR - Agent生成内容胜率高，但模型利用自定义资产和反馈效果不佳。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型生成交互式视听内容缺乏自动评估指标，处理复杂内容有困难。

Method: 提出相对指标AVR - Eval评估多媒体内容质量；构建多智能体系统AVR - Agent，利用多媒体资产生成JavaScript代码，用AVR - Eval选优并迭代改进。

Result: AVR - Agent生成内容比一次性生成内容胜率高，模型利用自定义资产和AVR反馈效果不佳。

Conclusion: 当前编码模型不能像人类一样有效利用高质量资产和视听反馈，体现人机内容创作方式的根本差异。

Abstract: While AI excels at generating text, audio, images, and videos, creating
interactive audio-visual content such as video games remains challenging.
Current LLMs can generate JavaScript games and animations, but lack automated
evaluation metrics and struggle with complex content that normally requires
teams of humans working for many months (multi-shot, multi-agents) using assets
made by artists. To tackle these issues, we built a new metric and a
multi-agent system.
  We propose AVR-Eval, a relative metric for multimedia content quality using
Audio-Visual Recordings (AVRs). An omni-modal model (processing text, video,
and audio) compares the AVRs of two contents, with a text model reviewing
evaluations to determine superiority. We show that AVR-Eval properly identifies
good from broken or mismatched content.
  We built AVR-Agent, a multi-agent system generating JavaScript code from a
bank of multimedia assets (audio, images, 3D models). The coding agent selects
relevant assets, generates multiple initial codes, uses AVR-Eval to identify
the best version, and iteratively improves it through omni-modal agent feedback
from the AVR.
  We run experiments on games and animations with AVR-Eval (win rate of content
A against B). We find that content generated by AVR-Agent has a significantly
higher win rate against content made through one-shot generation. However,
models struggle to leverage custom assets and AVR feedback effectively, showing
no higher win rate. This reveals a critical gap: while humans benefit from
high-quality assets and audio-visual feedback, current coding models do not
seem to utilize these resources as effectively, highlighting fundamental
differences between human and machine content creation approaches.

</details>


### [22] [Multi-Band Variable-Lag Granger Causality: A Unified Framework for Causal Time Series Inference across Frequencies](https://arxiv.org/abs/2508.00658)
*Chakattrai Sookkongwaree,Tattep Lakmuang,Chainarong Amornbunchornvej*

Main category: cs.AI

TL;DR: 本文提出多频段可变滞后格兰杰因果关系（MB - VLGC）框架，考虑频带差异，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统格兰杰因果关系有固定滞后假设，可变滞后格兰杰因果关系（VLGC）未考虑因果交互在频带上的差异，因此需要改进。

Method: 形式化定义MB - VLGC，提出新框架，明确对频率依赖的因果延迟建模，给出形式定义，证明理论合理性并提出推理流程。

Result: 在多个领域的大量实验中，该框架在合成和真实数据集上显著优于现有方法。

Conclusion: 提出的MB - VLGC框架具有广泛适用性，可用于任何类型的时间序列数据。

Abstract: Understanding causal relationships in time series is fundamental to many
domains, including neuroscience, economics, and behavioral science. Granger
causality is one of the well-known techniques for inferring causality in time
series. Typically, Granger causality frameworks have a strong fix-lag
assumption between cause and effect, which is often unrealistic in complex
systems. While recent work on variable-lag Granger causality (VLGC) addresses
this limitation by allowing a cause to influence an effect with different time
lags at each time point, it fails to account for the fact that causal
interactions may vary not only in time delay but also across frequency bands.
For example, in brain signals, alpha-band activity may influence another region
with a shorter delay than slower delta-band oscillations. In this work, we
formalize Multi-Band Variable-Lag Granger Causality (MB-VLGC) and propose a
novel framework that generalizes traditional VLGC by explicitly modeling
frequency-dependent causal delays. We provide a formal definition of MB-VLGC,
demonstrate its theoretical soundness, and propose an efficient inference
pipeline. Extensive experiments across multiple domains demonstrate that our
framework significantly outperforms existing methods on both synthetic and
real-world datasets, confirming its broad applicability to any type of time
series data. Code and datasets are publicly available.

</details>


### [23] [Transparent Adaptive Learning via Data-Centric Multimodal Explainable AI](https://arxiv.org/abs/2508.00665)
*Maryam Mosleh,Marie Devlin,Ellis Solaiman*

Main category: cs.AI

TL;DR: 本文提出将传统XAI技术与生成式AI模型和用户个性化相结合的混合框架，以生成多模态、个性化解释，推动可解释AI提升透明度并支持以用户为中心的体验。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能驱动的自适应学习系统缺乏透明度，多数可解释AI技术忽略用户角色和理解。

Method: 提出混合框架，将传统XAI技术与生成式AI模型和用户个性化结合，重新定义可解释性为针对用户角色和学习目标的动态沟通过程。

Result: 概述了框架设计、教育中XAI的关键局限以及准确性、公平性和个性化方面的研究方向。

Conclusion: 旨在推动可解释AI在提升透明度的同时支持以用户为中心的体验。

Abstract: Artificial intelligence-driven adaptive learning systems are reshaping
education through data-driven adaptation of learning experiences. Yet many of
these systems lack transparency, offering limited insight into how decisions
are made. Most explainable AI (XAI) techniques focus on technical outputs but
neglect user roles and comprehension. This paper proposes a hybrid framework
that integrates traditional XAI techniques with generative AI models and user
personalisation to generate multimodal, personalised explanations tailored to
user needs. We redefine explainability as a dynamic communication process
tailored to user roles and learning goals. We outline the framework's design,
key XAI limitations in education, and research directions on accuracy,
fairness, and personalisation. Our aim is to move towards explainable AI that
enhances transparency while supporting user-centred experiences.

</details>


### [24] [Context-Aware Visualization for Explainable AI Recommendations in Social Media: A Vision for User-Aligned Explanations](https://arxiv.org/abs/2508.00674)
*Banan Alkhateeb,Ellis Solaiman*

Main category: cs.AI

TL;DR: 提出用户细分和上下文感知的可视化解释系统，以解决社交媒体AI推荐可解释性问题，并将进行公开试点验证。


<details>
  <summary>Details</summary>
Motivation: 社交媒体AI推荐因缺乏个性化可解释性，导致用户难以理解推荐原因，价值降低。

Method: 提出具有多种解释方法的可视化解释系统，根据用户需求和上下文展示不同形式解释，且统一调整解释风格和粒度。

Result: 未提及具体结果，将通过30名X用户的公开试点验证对决策和信任的影响。

Conclusion: 未明确给出结论，预计试点后得出系统对决策和信任影响的结论。

Abstract: Social media platforms today strive to improve user experience through AI
recommendations, yet the value of such recommendations vanishes as users do not
understand the reasons behind them. This issue arises because explainability in
social media is general and lacks alignment with user-specific needs. In this
vision paper, we outline a user-segmented and context-aware explanation layer
by proposing a visual explanation system with diverse explanation methods. The
proposed system is framed by the variety of user needs and contexts, showing
explanations in different visualized forms, including a technically detailed
version for AI experts and a simplified one for lay users. Our framework is the
first to jointly adapt explanation style (visual vs. numeric) and granularity
(expert vs. lay) inside a single pipeline. A public pilot with 30 X users will
validate its impact on decision-making and trust.

</details>


### [25] [Unraveling Hidden Representations: A Multi-Modal Layer Analysis for Better Synthetic Content Forensics](https://arxiv.org/abs/2508.00784)
*Tom Or,Omri Azencot*

Main category: cs.AI

TL;DR: 现有伪造检测器泛化性差，本文提出用大预训练多模态模型检测生成内容，线性分类器在多模态上效果好。


<details>
  <summary>Details</summary>
Motivation: 生成模型被恶意利用传播虚假信息，现有伪造检测器泛化性差，需通用分类器。

Method: 使用大预训练多模态模型，利用其潜在代码区分真假信息，训练线性分类器。

Result: 线性分类器在各模态上取得了最先进的结果，在音频和图像伪造检测中性能超越或匹配强基线方法。

Conclusion: 基于大预训练多模态模型的线性分类器能有效检测生成内容，计算高效、训练快，少样本下也有效。

Abstract: Generative models achieve remarkable results in multiple data domains,
including images and texts, among other examples. Unfortunately, malicious
users exploit synthetic media for spreading misinformation and disseminating
deepfakes. Consequently, the need for robust and stable fake detectors is
pressing, especially when new generative models appear everyday. While the
majority of existing work train classifiers that discriminate between real and
fake information, such tools typically generalize only within the same family
of generators and data modalities, yielding poor results on other generative
classes and data domains. Towards a universal classifier, we propose the use of
large pre-trained multi-modal models for the detection of generative content.
Effectively, we show that the latent code of these models naturally captures
information discriminating real from fake. Building on this observation, we
demonstrate that linear classifiers trained on these features can achieve
state-of-the-art results across various modalities, while remaining
computationally efficient, fast to train, and effective even in few-shot
settings. Our work primarily focuses on fake detection in audio and images,
achieving performance that surpasses or matches that of strong baseline
methods.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [26] [A Practical Finite Element Approach for Simulating Dynamic Crack Growth in Cu/Ultra Low-k Interconnect Structures](https://arxiv.org/abs/2508.00193)
*Yuxi Xie,Ethan J. Wu,Lu Xu,Jimmy Perez,Shaofan Li*

Main category: cs.CE

TL;DR: 提出裂纹单元法（CEM）模拟二维结构动态裂纹扩展，经验证准确可靠且具适用性。


<details>
  <summary>Details</summary>
Motivation: 开发实用的有限元建模策略来模拟二维结构的动态裂纹扩展。

Method: 采用基于边光滑有限元法（ES - FEM）的单元分裂算法，同时基于分裂单元的拓扑演变开发断裂能量释放率公式。

Result: 通过一系列经典基准问题验证了该方法在处理动态断裂场景中的准确性和鲁棒性，且在Cu/超低k互连结构案例中展示了适用性。

Conclusion: 所提出的CEM方法是一种实用且有效的模拟二维结构动态裂纹扩展的策略。

Abstract: This work presents a practical finite element modeling strategy, the Crack
Element Method (CEM), for simulating the dynamic crack propagation in
two-dimensional structures. The method employs an element-splitting algorithm
based on the Edge-based Smoothed Finite Element Method (ES-FEM) to capture the
element-wise crack growth while reducing the formation of poorly shaped
elements that can compromise numerical accuracy and computational performance.
A fracture energy release rate formulation is also developed based on the
evolving topology of the split elements. The proposed approach is validated
through a series of classical benchmark problems, demonstrating its accuracy
and robustness in addressing dynamic fracture scenarios. Finally, the
applicability of the CEM is illustrated in a case study involving patterned
Cu/Ultra Low-k interconnect structures.

</details>


### [27] [WeightFlow: Learning Stochastic Dynamics via Evolving Weight of Neural Network](https://arxiv.org/abs/2508.00451)
*Ruikun Li,Jiazhen Liu,Huandong Wang,Qingmin Liao,Yong Li*

Main category: cs.CE

TL;DR: 本文提出WeightFlow范式解决离散观测建模随机动力学问题，实验显示比现有方法性能平均提升43.02%。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以从轨迹估计概率密度的连续演化或面临维数灾难，需新方法解决这些局限。

Method: 在神经网络的权重空间直接建模动力学，建立测度空间中动态最优传输与权重空间中等价能量泛函的联系，设计WeightFlow将神经网络权重构建为图并通过图控制微分方程学习其演化。

Result: 在跨学科数据集上实验表明，WeightFlow比现有最先进方法性能平均提升43.02%。

Conclusion: WeightFlow为高维随机动力学建模提供了有效且可扩展的解决方案。

Abstract: Modeling stochastic dynamics from discrete observations is a key
interdisciplinary challenge. Existing methods often fail to estimate the
continuous evolution of probability densities from trajectories or face the
curse of dimensionality. To address these limitations, we presents a novel
paradigm: modeling dynamics directly in the weight space of a neural network by
projecting the evolving probability distribution. We first theoretically
establish the connection between dynamic optimal transport in measure space and
an equivalent energy functional in weight space. Subsequently, we design
WeightFlow, which constructs the neural network weights into a graph and learns
its evolution via a graph controlled differential equation. Experiments on
interdisciplinary datasets demonstrate that WeightFlow improves performance by
an average of 43.02\% over state-of-the-art methods, providing an effective and
scalable solution for modeling high-dimensional stochastic dynamics.

</details>


### [28] [LEO: An Open-Source Platform for Linking OMERO with Lab Notebooks and Heterogeneous Metadata Sources](https://arxiv.org/abs/2508.00654)
*Rodrigo Escobar Díaz Guerrero,Jamile Mohammad Jafari,Tobias Meyer-Zedler,Michael Schmitt,Juergen Popp,Thomas Bocklitz*

Main category: cs.CE

TL;DR: 论文提出基于网络的平台LEO，解决显微镜研究中异构数据源数据整合与链接难题，且具有可扩展性。


<details>
  <summary>Details</summary>
Motivation: 显微镜研究领域中，不同平台存储的大量数据管理和整合困难，缺乏有效整合异构数据源的工具，需满足FAIR数据管理原则。

Method: 开发基于网络的平台LEO，采用插件式架构扩展功能。

Result: LEO可创建和管理分布式数据系统间的链接，最初用于连接电子实验室笔记本和OMERO，后扩展到其他数据源。

Conclusion: LEO是适用于多种显微镜研究工作流程的可扩展、灵活解决方案。

Abstract: In the interdisciplinary field of microscopy research, managing and
integrating large volumes of data stored across disparate platforms remains a
major challenge. Data types such as bioimages, experimental records, and
spectral information are often maintained in separate repositories, each
following different management standards. However, linking these data sources
across the research lifecycle is essential to align with the FAIR principles of
data management: Findability, Accessibility, Interoperability, and Reusability.
Despite this need, there is a notable lack of tools capable of effectively
integrating and linking data from heterogeneous sources. To address this gap,
we present LEO (Linking Electronic Lab Notebooks with OMERO), a web-based
platform designed to create and manage links between distributed data systems.
LEO was initially developed to link objects between Electronic Lab Notebooks
(ELNs) and OMERO, but its functionality has since been extended through a
plugin-based architecture, allowing the integration of additional data sources.
This extensibility makes LEO a scalable and flexible solution for a wide range
of microscopy research workflows.

</details>


### [29] [Contact Sensors to Remote Cameras: Quantifying Cardiorespiratory Coupling in High-Altitude Exercise Recovery](https://arxiv.org/abs/2508.00773)
*Jiankai Tang,Meng Kang,Yiru Zhang,Kegang Wang,Daniel Mcduff,Xin Liu,Yuanchun Shi,Yuntao Wang*

Main category: cs.CE

TL;DR: 研究高海拔下静息和运动后恢复两种状态的心肺耦合（CRC），发现显著差异，探索非接触式CRC测量可行性，认为CRC有潜力作自主调节敏感标志物。


<details>
  <summary>Details</summary>
Motivation: 研究高海拔下不同状态的CRC情况以及非接触式CRC测量的可行性。

Method: 在高海拔下检测静息和运动后恢复两种状态的CRC，进行定量分析；用远程光电容积脉搏波描记法（rPPG）进行非接触式CRC测量并与血氧仪指标对比。

Result: 高海拔下两种状态的CRC有显著差异；恢复时呼吸和脉搏同步更频繁但稳定性差；rPPG测量与血氧仪指标强相关（Pearson r = 0.96）。

Conclusion: CRC有潜力作为自主调节的敏感标志物，可用于非接触式监测。

Abstract: Cardiorespiratory coupling (CRC) captures the dynamic interaction between the
cardiac and respiratory systems--an interaction strengthened by physical
exercise and linked to improved physiological function. We examined CRC at high
altitude in two states, rest and post-exercise recovery, and found significant
differences (p < 0.05). Quantitative analysis revealed that recovery involved
more frequent yet less stable episodes of synchronization between respiration
and pulse. Furthermore, we explored the feasibility of non-contact CRC
measurement with remote photoplethysmography (rPPG), observing a strong
correlation with oximeter-based metrics (Pearson r = 0.96). These findings
highlight the potential of CRC as a sensitive marker for autonomic regulation
and its future application in contactless monitoring. Source code is available
at GitHub: https://github.com/McJackTang/CRC.

</details>


### [30] [τ-Ring: A Smart Ring Platform for Multimodal Physiological and Behavioral Sensing](https://arxiv.org/abs/2508.00778)
*Jiankai Tang,Zhe He,Mingyu Zhang,Wei Geng,Chengchi Zhou,Weinan Shi,Yuanchun Shi,Yuntao Wang*

Main category: cs.CE

TL;DR: 介绍了商业就绪平台τ - Ring，可解决商业智能戒指的问题，还进行了验证研究，代码开源。


<details>
  <summary>Details</summary>
Motivation: 多数商业智能戒指方案专有，阻碍可重复性并减缓可穿戴研究创新。

Method: 推出τ - Ring平台，包含可访问硬件、可调节固件和开源安卓软件套件。

Result: 该平台能实现开箱即用、可重复获取丰富数据集，加速原型设计并规范实验。

Conclusion: 通过心率监测和基于戒指的手写识别演示研究验证了平台有效性。

Abstract: Smart rings have emerged as uniquely convenient devices for continuous
physiological and behavioral sensing, offering unobtrusive, constant access to
metrics such as heart rate, motion, and skin temperature. Yet most commercial
solutions remain proprietary, hindering reproducibility and slowing innovation
in wearable research. We introduce {\tau}-Ring, a commercial-ready platform
that bridges this gap through: (i) accessible hardware combining
time-synchronized multi-channel PPG, 6-axis IMU, temperature sensing, NFC, and
on-board storage; (ii) adjustable firmware that lets researchers rapidly
reconfigure sampling rates, power modes, and wireless protocols; and (iii) a
fully open-source Android software suite that supports both real-time streaming
and 8-hour offline logging. Together, these features enable out-of-the-box,
reproducible acquisition of rich physiological and behavioral datasets,
accelerating prototyping and standardizing experimentation. We validate the
platform with demonstration studies in heart-rate monitoring and ring-based
handwriting recognition. Source code is available at GitHub:
https://github.com/thuhci/OpenRing.

</details>


### [31] [Online Fine-Tuning of Carbon Emission Predictions using Real-Time Recurrent Learning for State Space Models](https://arxiv.org/abs/2508.00804)
*Julian Lemmel,Manuel Kranzl,Adam Lamine,Philipp Neubauer,Radu Grosu,Sophie Neubauer*

Main category: cs.CE

TL;DR: 本文介绍用实时循环学习在推理时微调结构化状态空间模型预测的新方法，实验表明该方法能减少预测误差。


<details>
  <summary>Details</summary>
Motivation: 结构化状态空间模型（SSMs）通常离线训练且部署时静态，需要在线适应能力。

Method: 用实时循环学习在推理时连续更新模型参数以实现在线适应。

Result: 在小型碳排放数据集上实验显示，该方法在推理时持续减少在线预测误差。

Conclusion: 该方法在动态、资源受限环境有应用潜力。

Abstract: This paper introduces a new approach for fine-tuning the predictions of
structured state space models (SSMs) at inference time using real-time
recurrent learning. While SSMs are known for their efficiency and long-range
modeling capabilities, they are typically trained offline and remain static
during deployment. Our method enables online adaptation by continuously
updating model parameters in response to incoming data. We evaluate our
approach for linear-recurrent-unit SSMs using a small carbon emission dataset
collected from embedded automotive hardware. Experimental results show that our
method consistently reduces prediction error online during inference,
demonstrating its potential for dynamic, resource-constrained environments.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [32] [Integrated user scheduling and beam steering in over-the-air federated learning for mobile IoT](https://arxiv.org/abs/2508.00341)
*Shengheng Liu,Ningning Fu,Zhonghao Zhang,Yongming Huang,Tony Q. S. Quek*

Main category: cs.DC

TL;DR: 论文聚焦物联网隐私问题，将空中计算引入联邦学习框架，提出用户调度和接收波束转向集成方法，后又提出低复杂度用户调度策略，实验验证了方法优越性。


<details>
  <summary>Details</summary>
Motivation: 物联网服务提供商收集大量敏感数据引发隐私担忧，联邦学习虽有潜力但在大规模网络中存在通信效率和推理精度问题，需优化。

Method: 将空中计算引入联邦学习框架，提出用户调度和接收波束转向集成方法，用差凸技术分解非凸优化问题；提出基于无线信道特征分析的低复杂度用户调度策略。

Result: 通过大量实验验证，所提方法在聚合误差和学习性能上优于现有方法。

Conclusion: 所提方法能有效提高分布式本地模型的聚合效率，在优化通信效率和推理精度方面表现出色。

Abstract: The rising popularity of Internet of things (IoT) has spurred technological
advancements in mobile internet and interconnected systems. While offering
flexible connectivity and intelligent applications across various domains, IoT
service providers must gather vast amounts of sensitive data from users, which
nonetheless concomitantly raises concerns about privacy breaches. Federated
learning (FL) has emerged as a promising decentralized training paradigm to
tackle this challenge. This work focuses on enhancing the aggregation
efficiency of distributed local models by introducing over-the-air computation
into the FL framework. Due to radio resource scarcity in large-scale networks,
only a subset of users can participate in each training round. This highlights
the need for effective user scheduling and model transmission strategies to
optimize communication efficiency and inference accuracy. To address this, we
propose an integrated approach to user scheduling and receive beam steering,
subject to constraints on the number of selected users and transmit power.
Leveraging the difference-of-convex technique, we decompose the primal
non-convex optimization problem into two sub-problems, yielding an iterative
solution. While effective, the computational load of the iterative method
hampers its practical implementation. To overcome this, we further propose a
low-complexity user scheduling policy based on characteristic analysis of the
wireless channel to directly determine the user subset without iteration.
Extensive experiments validate the superiority of the proposed method in terms
of aggregation error and learning performance over existing approaches.

</details>


### [33] [Tetris: Efficient Intra-Datacenter Calls Packing for Large Conferencing Services](https://arxiv.org/abs/2508.00426)
*Rohan Gandhi,Ankur Mallick,Ken Sueda,Rui Liang*

Main category: cs.DC

TL;DR: 现有会议服务保证性能同时降低成本有挑战，本文提出Tetris框架解决媒体处理器服务器呼叫分配问题，评估显示可减少热点MP上参与者数量。


<details>
  <summary>Details</summary>
Motivation: 现有算法在媒体处理器服务器上分配通话易使部分MP过热，导致性能下降和成本上升，需解决该问题。

Method: 提出Tetris多步框架，利用历史数据优化初始呼叫分配，用线性优化定期迁移热点MP上的呼叫。

Result: 基于24小时超1000万次呼叫的跟踪评估，Tetris使热点MP上的参与者数量至少减少2.5倍。

Conclusion: Tetris框架能有效减少热点MP的使用，优化会议服务中呼叫分配问题。

Abstract: Conference services like Zoom, Microsoft Teams, and Google Meet facilitate
millions of daily calls, yet ensuring high performance at low costs remains a
significant challenge. This paper revisits the problem of packing calls across
Media Processor (MP) servers that host the calls within individual datacenters
(DCs). We show that the algorithm used in Teams -- a large scale conferencing
service as well as other state-of-art algorithms are prone to placing calls
resulting in some of the MPs becoming hot (high CPU utilization) that leads to
degraded performance and/or elevated hosting costs. The problem arises from
disregarding the variability in CPU usage among calls, influenced by
differences in participant numbers and media types (audio/video), compounded by
bursty call arrivals. To tackle this, we propose Tetris, a multi-step framework
which (a) optimizes initial call assignments by leveraging historical data and
(b) periodically migrates calls from hot MPs using linear optimization, aiming
to minimize hot MP usage. Evaluation based on a 24-hour trace of over 10
million calls in one DC shows that Tetris reduces participant numbers on hot
MPs by at least 2.5X.

</details>


### [34] [SwarnRaft: Leveraging Consensus for Robust Drone Swarm Coordination in GNSS-Degraded Environments](https://arxiv.org/abs/2508.00622)
*Kapel Dev,Yash Madhwal,Sofia Shevelo,Pavel Osinenko,Yury Yanovich*

Main category: cs.DC

TL;DR: 提出SwarnRaft框架，用于在无GNSS信号下维持无人机群协调和数据完整性，系统展示了鲁棒性和容错性。


<details>
  <summary>Details</summary>
Motivation: 无人机群系统可靠性依赖GNSS信号，而现实中信号可能中断，导致方向迷失、碰撞风险和任务失败。

Method: 提出受区块链启发的SwarnRaft定位和共识框架，利用Raft共识算法，节点使用GNSS和本地传感，通过WiFi通信，信号丢失时基于已知状态和轨迹重建或验证故障节点位置。

Result: 系统通过轻量级、可扩展的通信模型，维持了集群的连贯性和容错性。

Conclusion: 为不可预测环境中的分布式无人机操作提供了实用且安全的基础。

Abstract: Unmanned aerial vehicle (UAV) swarms are increasingly used in critical
applications such as aerial mapping, environmental monitoring, and autonomous
delivery. However, the reliability of these systems is highly dependent on
uninterrupted access to the Global Navigation Satellite Systems (GNSS) signals,
which can be disrupted in real-world scenarios due to interference,
environmental conditions, or adversarial attacks, causing disorientation,
collision risks, and mission failure. This paper proposes SwarnRaft, a
blockchain-inspired positioning and consensus framework for maintaining
coordination and data integrity in UAV swarms operating under GNSS-denied
conditions. SwarnRaft leverages the Raft consensus algorithm to enable
distributed drones (nodes) to agree on state updates such as location and
heading, even in the absence of GNSS signals for one or more nodes. In our
prototype, each node uses GNSS and local sensing, and communicates over WiFi in
a simulated swarm. Upon signal loss, consensus is used to reconstruct or verify
the position of the failed node based on its last known state and trajectory.
Our system demonstrates robustness in maintaining swarm coherence and fault
tolerance through a lightweight, scalable communication model. This work offers
a practical and secure foundation for decentralized drone operation in
unpredictable environments.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [35] [From Dynamic Programs to Greedy Algorithms](https://arxiv.org/abs/2508.00776)
*Dieter van Melkebeek*

Main category: cs.DS

TL;DR: 展示从一般情况的动态规划简单推导特殊情况经典贪心算法的方法，适用于本科课程。


<details>
  <summary>Details</summary>
Motivation: 为本科算法课程提供开发贪心算法及论证其正确性的替代方法。

Method: 反复展开动态规划的贝尔曼方程，利用单调性确定特定限制下的最优值。

Result: 推导出区间调度、背包、最短路径特殊情况的贪心算法，阐明区间调度中排序顺序的变化。

Conclusion: 该方法可用于本科算法课程开发贪心算法及论证正确性。

Abstract: We show for several computational problems how classical greedy algorithms
for special cases can be derived in a simple way from dynamic programs for the
general case: interval scheduling (restricted to unit weights), knapsack
(restricted to unit values), and shortest paths (restricted to nonnegative edge
lengths). Conceptually, we repeatedly expand the Bellman equations underlying
the dynamic program and use straightforward monotonicity properties to figure
out which terms yield the optimal value under the respective restrictions. The
approach offers an alternative for developing these greedy algorithms in
undergraduate algorithms courses and/or for arguing their correctness. In the
setting of interval scheduling, it elucidates the change in order from earliest
start time first for the memoized dynamic program to earliest finish time first
for the greedy algorithm.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [36] [Computation of Approximately Stable Committees in Approval-based Elections](https://arxiv.org/abs/2508.00130)
*Drew Gao,Yihang Sun,Jan Vondrák*

Main category: cs.GT

TL;DR: 研究批准式委员会选择模型中的近似稳定性标准，证明3.65 - 近似稳定委员会存在并可算法计算。


<details>
  <summary>Details</summary>
Motivation: 批准式委员会选择在社会选择理论中是重要模型，研究其近似稳定性标准。

Method: 基于寻找林达尔均衡并从与之相关的强瑞利分布中采样。

Result: 证明了3.65 - 近似稳定委员会总是存在且能算法计算。

Conclusion: 在批准式委员会选择模型中，可通过特定方法找到3.65 - 近似稳定委员会。

Abstract: Approval-based committee selection is a model of significant interest in
social choice theory. In this model, we have a set of voters $\mathcal{V}$, a
set of candidates $\mathcal{C}$, and each voter has a set $A_v \subset
\mathcal{C}$ of approved candidates. For any committee size $K$, the goal is to
choose $K$ candidates to represent the voters' preferences. We study a
criterion known as \emph{approximate stability}, where a committee is
$\lambda$-approximately-stable if there is no other committee $T$ preferred by
at least $\frac{\lambda|T|}{k} |\mathcal{V}| $ voters. We prove that a
$3.65$-approximately stable committee always exists and can be computed
algorithmically in this setting. Our approach is based on finding a Lindahl
equilibrium and sampling from a strongly Rayleigh distribution associated with
it.

</details>


### [37] [On the Equivalence of the Graph-Structural and Optimization-Based Characterizations of Popular Matchings](https://arxiv.org/abs/2508.00349)
*Yuga Kanaya,Kenjiro Takazawa*

Main category: cs.GT

TL;DR: 本文研究二分图中流行匹配的三类问题，建立了图结构与基于优化的两种流行匹配特征描述之间的直接联系。


<details>
  <summary>Details</summary>
Motivation: 流行匹配算法方面的主要问题是确定匹配的流行度需指数时间，现有两种特征描述方式各有优劣，需建立联系。

Method: 通过证明两种特征描述可相互推导，不依赖其表征流行匹配这一事实。

Result: 建立了两类特征描述的直接联系，证明可相互推导。

Conclusion: 证明提供了对两种特征描述等价性的全面理解，给出图结构特征描述基于最大权匹配问题对偶最优解的新解释。

Abstract: Popular matchings provide a model of matching under preferences in which a
solution corresponds to a Condorcet winner in voting systems. In a bipartite
graph in which the vertices have preferences over their neighbours, a matching
is defined to be popular if it does not lose in a majority vote against any
matching. In this paper, we study the following three primary problems: only
the vertices on one side have preferences; a generalization of this problem
allowing ties in the preferences; and the vertices on both sides have
preferences. A principal issue in the algorithmic aspects of popular matchings
is how to determine the popularity of a matching, because it requires
exponential time if the definition is simply applied. In the literature, we
have the following two types of characterizations: a graph-structural
characterization; and an optimization-based characterization described by
maximum-weight matchings. The graph-structural characterizations are
specifically designed for each problem and provide a combinatorial structure of
the popular matchings. The optimization-based characterizations work in the
same manner for all problems, while they do not reveal the structure of the
popular matchings. A main contribution of this paper is to provide a direct
connection of the above two types of characterizations for all of the three
problems. Specifically, we prove that each characterization can be derived from
the other, without relying on the fact that they characterize popular
matchings. Our proofs offer a comprehensive understanding of the equivalence of
the two types of characterizations, and suggest a new interpretation of the
graph-structural characterization in terms of the dual optimal solution for the
maximum-weight matching problem.

</details>


### [38] [Justified Representation: From Hare to Droop](https://arxiv.org/abs/2508.00811)
*Matthew M. Casey,Edith Elkind*

Main category: cs.GT

TL;DR: 本文系统研究用Droop配额替代Hare配额定义的JR式公理及满足它们的投票规则，识别出满足各标准JR公理Droop版本的投票规则，并进行实验研究。


<details>
  <summary>Details</summary>
Motivation: 多赢家投票比例性研究受关注，虽有作者考虑在批准投票中使用Droop配额，但现有分析不全面，需系统研究。

Method: 对标准JR公理，识别满足其Droop版本的投票规则，部分修改已知规则证明，部分修改已有规则；还进行实验研究。

Result: 识别出满足各标准JR公理Droop版本的投票规则；实验显示，对许多选民批准概率模型，Droop JR/EJR+比标准（Hare）JR/EJR+要求高得多。

Conclusion: 研究拓展了可满足比例性公理的边界。

Abstract: The study of proportionality in multiwinner voting with approval ballots has
received much attention in recent years. Typically, proportionality is captured
by variants of the Justified Representation axiom, which say that cohesive
groups of at least $\ell\cdot\frac{n}{k}$ voters (where $n$ is the total number
of voters and $k$ is the desired number of winners) deserve $\ell$
representatives. The quantity $\frac{n}{k}$ is known as the Hare quota in the
social choice literature. Another -- more demanding -- choice of quota is the
Droop quota, defined as $\lfloor\frac{n}{k+1}\rfloor+1$. This quota is often
used in multiwinner voting with ranked ballots: in algorithms such as Single
Transferable Voting, and in proportionality axioms, such as Droop's
Proportionality Criterion. A few authors have considered it in the context of
approval ballots, but the existing analysis is far from comprehensive. The
contribution of our work is a systematic study of JR-style axioms (and voting
rules that satisfy them) defined using the Droop quota instead of the Hare
quota. For each of the standard JR axioms (namely, JR, PJR, EJR, FPJR, FJR,
PJR+ and EJR+), we identify a voting rule that satisfies the Droop version of
this axiom. In some cases, it suffices to consider known rules (modifying the
corresponding Hare proof, sometimes quite substantially), and in other cases it
is necessary to modify the rules from prior work. Each axiom is more difficult
to satisfy when defined using the Droop quota, so our results expand the
frontier of satisfiable proportionality axioms. We complement our theoretical
results with an experimental study, showing that for many probabilistic models
of voter approvals, Droop JR/EJR+ are considerably more demanding than standard
(Hare) JR/EJR+.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [39] [Audio Prototypical Network For Controllable Music Recommendation](https://arxiv.org/abs/2508.00194)
*Fırat Öncel,Emiliano Penaloza,Haolun Wu,Shubham Gupta,Mirco Ravanelli,Laurent Charlin,Cem Subakan*

Main category: cs.IR

TL;DR: 提出音频原型网络用于可控音乐推荐，性能有竞争力且用户画像可解释和可控


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统缺乏可解释性，在音乐推荐中用户偏好高度个人化且多变，需要可解释和可控的模型

Method: 提出音频原型网络，用原型表达与音乐品质相关的有语义意义的特征来表示用户偏好

Result: 模型与流行基线模型相比获得了有竞争力的推荐性能

Conclusion: 所提模型可提供可解释和可控的用户画像

Abstract: Traditional recommendation systems represent user preferences in dense
representations obtained through black-box encoder models. While these models
often provide strong recommendation performance, they lack interpretability for
users, leaving users unable to understand or control the system's modeling of
their preferences. This limitation is especially challenging in music
recommendation, where user preferences are highly personal and often evolve
based on nuanced qualities like mood, genre, tempo, or instrumentation. In this
paper, we propose an audio prototypical network for controllable music
recommendation. This network expresses user preferences in terms of prototypes
representative of semantically meaningful features pertaining to musical
qualities. We show that the model obtains competitive recommendation
performance compared to popular baseline models while also providing
interpretable and controllable user profiles.

</details>


### [40] [When Relevance Meets Novelty: Dual-Stable Periodic Optimization for Exploratory Recommendation](https://arxiv.org/abs/2508.00450)
*Hongxiang Lin,Hao Guo,Zeshun Li,Erpeng Xue,Yongqian He,Xiangyu Hou,Zhaoyu Hu,Lei Wang,Sheng Chen*

Main category: cs.IR

TL;DR: 传统推荐系统有局限，现有大语言模型增强框架也有不足，本文提出CoEA方法，经实验验证其在探索性推荐中有效。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统会使用户陷入反馈循环，现有大语言模型增强框架存在忽视群体身份驱动的长期偏好、静态优化有缺陷等问题。

Method: 提出Co - Evolutionary Alignment (CoEA)方法，用Dual - Stable Interest Exploration (DSIE)模块解决兴趣建模偏差，设计Periodic Collaborative Optimization (PCO)机制解决静态优化局限。

Result: 大量线上和线下实验验证了CoEA模型在探索性推荐中的有效性。

Conclusion: CoEA方法能有效解决传统推荐系统和现有大语言模型增强框架的问题，在探索性推荐中表现良好。

Abstract: Traditional recommendation systems tend to trap users in strong feedback
loops by excessively pushing content aligned with their historical preferences,
thereby limiting exploration opportunities and causing content fatigue.
Although large language models (LLMs) demonstrate potential with their diverse
content generation capabilities, existing LLM-enhanced dual-model frameworks
face two major limitations: first, they overlook long-term preferences driven
by group identity, leading to biased interest modeling; second, they suffer
from static optimization flaws, as a one-time alignment process fails to
leverage incremental user data for closed-loop optimization. To address these
challenges, we propose the Co-Evolutionary Alignment (CoEA) method. For
interest modeling bias, we introduce Dual-Stable Interest Exploration (DSIE)
module, jointly modeling long-term group identity and short-term individual
interests through parallel processing of behavioral sequences. For static
optimization limitations, we design a Periodic Collaborative Optimization (PCO)
mechanism. This mechanism regularly conducts preference verification on
incremental data using the Relevance LLM, then guides the Novelty LLM to
perform fine-tuning based on the verification results, and subsequently feeds
back the output of the incrementally fine-tuned Novelty LLM to the Relevance
LLM for re-evaluation, thereby achieving a dynamic closed-loop optimization.
Extensive online and offline experiments verify the effectiveness of the CoEA
model in exploratory recommendation.

</details>


### [41] [M^2VAE: Multi-Modal Multi-View Variational Autoencoder for Cold-start Item Recommendation](https://arxiv.org/abs/2508.00452)
*Chuan He,Yongchao Liu,Qiang Li,Wenliang Zhong,Chuntao Hong,Xinwei Yao*

Main category: cs.IR

TL;DR: 提出M^2VAE模型解决冷启动物品推荐问题，通过多种技术处理特征和用户偏好，实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有多模态方法忽视模态多视图结构及特征区别，解决冷启动物品推荐难题。

Method: 提出M^2VAE模型，生成类型特定潜在变量，用PoE得共同表示，用解耦对比损失分离视图，用偏好引导MoE融合表示，结合对比学习引入共现信号。

Result: 在真实数据集上的大量实验验证了方法的有效性。

Conclusion: M^2VAE模型能有效解决冷启动物品推荐问题。

Abstract: Cold-start item recommendation is a significant challenge in recommendation
systems, particularly when new items are introduced without any historical
interaction data. While existing methods leverage multi-modal content to
alleviate the cold-start issue, they often neglect the inherent multi-view
structure of modalities, the distinction between shared and modality-specific
features. In this paper, we propose Multi-Modal Multi-View Variational
AutoEncoder (M^2VAE), a generative model that addresses the challenges of
modeling common and unique views in attribute and multi-modal features, as well
as user preferences over single-typed item features. Specifically, we generate
type-specific latent variables for item IDs, categorical attributes, and image
features, and use Product-of-Experts (PoE) to derive a common representation. A
disentangled contrastive loss decouples the common view from unique views while
preserving feature informativeness. To model user inclinations, we employ a
preference-guided Mixture-of-Experts (MoE) to adaptively fuse representations.
We further incorporate co-occurrence signals via contrastive learning,
eliminating the need for pretraining. Extensive experiments on real-world
datasets validate the effectiveness of our approach.

</details>


### [42] [Session-Based Recommendation with Validated and Enriched LLM Intents](https://arxiv.org/abs/2508.00570)
*Gyuseok Lee,Yaokun Liu,Yifan Liu,Susik Yoon,Dong Wang,SeongKu Kang*

Main category: cs.IR

TL;DR: 文章提出VELI4SBR框架解决基于会话推荐（SBR）中利用大语言模型生成意图辅助训练面临的挑战，实验表明其性能和可解释性更好。


<details>
  <summary>Details</summary>
Motivation: SBR存在数据稀疏问题，利用大语言模型生成意图辅助训练虽有前景但面临验证意图质量、整合多意图和弥补大模型失败情况等挑战。

Method: 提出VELI4SBR两阶段框架，第一阶段用预测 - 纠正循环生成高质量意图，第二阶段用轻量级多意图预测和融合机制增强SBR模型，还引入从会话间行为相似性推断意图的训练策略弥补大模型失败。

Result: VELI4SBR在实验中优于现有基线模型。

Conclusion: VELI4SBR能有效解决相关挑战，提升性能并改善可解释性。

Abstract: Session-based recommendation (SBR) aims to predict the next item for an
anonymous user in a timely manner. However, SBR suffers from data sparsity due
to the short and anonymous nature of sessions. Recently, an emerging line of
work has explored inferring the underlying user intents of a session using
large language models (LLMs), with the generated intents serving as auxiliary
training signals to enhance SBR models. Despite its promise, this approach
faces three key challenges: validating intent quality, incorporating
session-level multi-intents, and complementing inevitable LLM failure cases. In
this paper, we propose VELI4SBR, a two-stage framework that leverages Validated
and Enriched LLM-generated Intents for SBR. In the first stage, we generate
high-quality intents using a predict-and-correct loop that validates the
informativeness of LLM-generated intents with a global intent pool to constrain
the LLM's output space and reduce hallucination. In the second stage, we
enhance the SBR model using the generated intents through a lightweight
multi-intent prediction and fusion mechanism. Furthermore, we introduce a
training strategy that compensates for LLM failures by inferring intents from
inter-session behavioral similarities. Extensive experiments show that VELI4SBR
outperforms state-of-the-art baselines while improving explainability.

</details>


### [43] [Experimental Evaluation of Dynamic Topic Modeling Algorithms](https://arxiv.org/abs/2508.00710)
*Ngozichukwuka Onah,Nadine Steinmetz,Hani Al-Sayeh,Kai-Uwe Sattler*

Main category: cs.IR

TL;DR: 社交媒体文本分析有需求，当前自供电主题模型定量比较少，本文进行模型比较并提出评估指标。


<details>
  <summary>Details</summary>
Motivation: 社交媒体每日产生大量文本，需要自供电主题模型的可靠有效计算技术，但当前此类模型的全面定量比较较少。

Method: 对自供电主题模型进行比较，并提出记录主题随时间变化的评估指标。

Result: 未提及

Conclusion: 未提及

Abstract: The amount of text generated daily on social media is gigantic and analyzing
this text is useful for many purposes. To understand what lies beneath a huge
amount of text, we need dependable and effective computing techniques from
self-powered topic models. Nevertheless, there are currently relatively few
thorough quantitative comparisons between these models. In this study, we
compare these models and propose an assessment metric that documents how the
topics change in time.

</details>


### [44] [Harnessing the Power of Interleaving and Counterfactual Evaluation for Airbnb Search Ranking](https://arxiv.org/abs/2508.00751)
*Qing Zhang,Alex Deng,Michelle Du,Huiji Gao,Liwei He,Sanjeev Katariya*

Main category: cs.IR

TL;DR: 论文提出交错和反事实评估方法，用于搜索和推荐系统排名算法的快速在线评估，比传统A/B测试更高效。


<details>
  <summary>Details</summary>
Motivation: 在线A/B测试在商业应用中有挑战，离线评估又不准确，需更好方法评估排名算法。

Method: 开发交错和反事实评估方法进行快速在线评估。

Result: 该方法使实验灵敏度最高提升100倍，简化了实验流程。

Conclusion: 方法能有效解决排名算法评估问题，其实践见解对类似组织有帮助。

Abstract: Evaluation plays a crucial role in the development of ranking algorithms on
search and recommender systems. It enables online platforms to create
user-friendly features that drive commercial success in a steady and effective
manner. The online environment is particularly conducive to applying causal
inference techniques, such as randomized controlled experiments (known as A/B
test), which are often more challenging to implement in fields like medicine
and public policy. However, businesses face unique challenges when it comes to
effective A/B test. Specifically, achieving sufficient statistical power for
conversion-based metrics can be time-consuming, especially for significant
purchases like booking accommodations. While offline evaluations are quicker
and more cost-effective, they often lack accuracy and are inadequate for
selecting candidates for A/B test. To address these challenges, we developed
interleaving and counterfactual evaluation methods to facilitate rapid online
assessments for identifying the most promising candidates for A/B tests. Our
approach not only increased the sensitivity of experiments by a factor of up to
100 (depending on the approach and metrics) compared to traditional A/B testing
but also streamlined the experimental process. The practical insights gained
from usage in production can also benefit organizations with similar interests.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [45] [Predicting Large-scale Urban Network Dynamics with Energy-informed Graph Neural Diffusion](https://arxiv.org/abs/2508.00037)
*Tong Nie,Jian Sun,Wei Ma*

Main category: cs.LG

TL;DR: 本文提出可扩展时空Transformer模型ScaleSTF解决城市系统时空动态预测中模型效率与效果的权衡问题，在大规模城市系统验证有优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动预测模型如GNN在大规模网络应用中因计算需求面临效果与效率的权衡问题，需改进。

Method: 从物理定律获取灵感，结合对微观和宏观过程的理解，提出基于类Transformer结构的可解释神经扩散方案，构建具有线性复杂度的ScaleSTF。

Result: ScaleSTF在交通流量、太阳能发电和智能电表等大规模城市系统验证中表现出当前最优性能和显著的可扩展性。

Conclusion: 研究为大规模城市网络动态预测提供新视角。

Abstract: Networked urban systems facilitate the flow of people, resources, and
services, and are essential for economic and social interactions. These systems
often involve complex processes with unknown governing rules, observed by
sensor-based time series. To aid decision-making in industrial and engineering
contexts, data-driven predictive models are used to forecast spatiotemporal
dynamics of urban systems. Current models such as graph neural networks have
shown promise but face a trade-off between efficacy and efficiency due to
computational demands. Hence, their applications in large-scale networks still
require further efforts. This paper addresses this trade-off challenge by
drawing inspiration from physical laws to inform essential model designs that
align with fundamental principles and avoid architectural redundancy. By
understanding both micro- and macro-processes, we present a principled
interpretable neural diffusion scheme based on Transformer-like structures
whose attention layers are induced by low-dimensional embeddings. The proposed
scalable spatiotemporal Transformer (ScaleSTF), with linear complexity, is
validated on large-scale urban systems including traffic flow, solar power, and
smart meters, showing state-of-the-art performance and remarkable scalability.
Our results constitute a fresh perspective on the dynamics prediction in
large-scale urban networks.

</details>


### [46] [Hybrid LSTM-Transformer Models for Profiling Highway-Railway Grade Crossings](https://arxiv.org/abs/2508.00039)
*Kaustav Chatterjee,Joshua Q. Li,Fatemeh Ansari,Masud Rana Munna,Kundan Parajulee,Jared Schwennesen*

Main category: cs.LG

TL;DR: 本文运用先进技术和创新建模方法测量公路铁路平交道口（HRGC）轮廓，开发结合LSTM和Transformer的混合深度学习框架，评估三种模型，模型2和3表现更佳并用于生成2D/3D HRGC轮廓，模型有提升交通安全潜力。


<details>
  <summary>Details</summary>
Motivation: 传统HRGC轮廓测量方法成本高、耗时长、干扰交通且有安全挑战，需新方法解决。

Method: 开发结合LSTM和Transformer的混合深度学习框架，用配备IMU和GPS传感器的测试车收集仪器数据，用步行轮廓仪获取地面真值数据，在俄克拉荷马州收集实地数据，评估三种深度学习模型。

Result: 模型2和3表现优于其他模型，被用于生成2D/3D HRGC轮廓。

Conclusion: 深度学习模型有通过快速准确评估HRGC挂起易发性来提升公路和铁路安全的显著潜力。

Abstract: Hump crossings, or high-profile Highway Railway Grade Crossings (HRGCs), pose
safety risks to highway vehicles due to potential hang-ups. These crossings
typically result from post-construction railway track maintenance activities or
non-compliance with design guidelines for HRGC vertical alignments.
Conventional methods for measuring HRGC profiles are costly, time-consuming,
traffic-disruptive, and present safety challenges. To address these issues,
this research employed advanced, cost-effective techniques and innovative
modeling approaches for HRGC profile measurement. A novel hybrid deep learning
framework combining Long Short-Term Memory (LSTM) and Transformer architectures
was developed by utilizing instrumentation and ground truth data.
Instrumentation data were gathered using a highway testing vehicle equipped
with Inertial Measurement Unit (IMU) and Global Positioning System (GPS)
sensors, while ground truth data were obtained via an industrial-standard
walking profiler. Field data was collected at the Red Rock Railroad Corridor in
Oklahoma. Three advanced deep learning models Transformer-LSTM sequential
(model 1), LSTM-Transformer sequential (model 2), and LSTM-Transformer parallel
(model 3) were evaluated to identify the most efficient architecture. Models 2
and 3 outperformed the others and were deployed to generate 2D/3D HRGC
profiles. The deep learning models demonstrated significant potential to
enhance highway and railroad safety by enabling rapid and accurate assessment
of HRGC hang-up susceptibility.

</details>


### [47] [Regime-Aware Conditional Neural Processes with Multi-Criteria Decision Support for Operational Electricity Price Forecasting](https://arxiv.org/abs/2508.00040)
*Abhinav Das,Stephan Schlüter*

Main category: cs.LG

TL;DR: 本文将贝叶斯状态检测与条件神经过程结合用于德国市场24小时电价预测，评估多种模型在电池存储优化框架中的表现，用TOPSIS分析得出LEAR在2021年排名第一，提出的R - NP模型在2021 - 2023年最平衡优选。


<details>
  <summary>Details</summary>
Motivation: 实现德国市场24小时电价准确预测，并评估不同模型在电池存储优化中的表现，找到最优模型。

Method: 用DS - HDP - HMM进行状态检测，用独立的CNP对每个状态建模，将预测结果作为CNP输出的状态加权混合；将不同模型预测集成到电池存储优化框架评估，用TOPSIS进行多标准评估。

Result: 评估发现不同模型有复杂性能权衡，LEAR常带来更高绝对利润或更低成本，DNN在特定成本最小化情境中最优。

Conclusion: LEAR在2021年排名第一，提出的R - NP模型在2021 - 2023年是最平衡和优选的解决方案。

Abstract: This work integrates Bayesian regime detection with conditional neural
processes for 24-hour electricity price prediction in the German market. Our
methodology integrates regime detection using a disentangled sticky
hierarchical Dirichlet process hidden Markov model (DS-HDP-HMM) applied to
daily electricity prices. Each identified regime is subsequently modeled by an
independent conditional neural process (CNP), trained to learn localized
mappings from input contexts to 24-dimensional hourly price trajectories, with
final predictions computed as regime-weighted mixtures of these CNP outputs. We
rigorously evaluate R-NP against deep neural networks (DNN) and Lasso estimated
auto-regressive (LEAR) models by integrating their forecasts into diverse
battery storage optimization frameworks, including price arbitrage, risk
management, grid services, and cost minimization. This operational utility
assessment revealed complex performance trade-offs: LEAR often yielded superior
absolute profits or lower costs, while DNN showed exceptional optimality in
specific cost-minimization contexts. Recognizing that raw prediction accuracy
doesn't always translate to optimal operational outcomes, we employed TOPSIS as
a comprehensive multi-criteria evaluation layer. Our TOPSIS analysis identified
LEAR as the top-ranked model for 2021, but crucially, our proposed R-NP model
emerged as the most balanced and preferred solution for 2021, 2022 and 2023.

</details>


### [48] [DO-EM: Density Operator Expectation Maximization](https://arxiv.org/abs/2507.22786)
*Adit Vishnu,Abhay Shastry,Dhruva Kashyap,Chiranjib Bhattacharyya*

Main category: cs.LG

TL;DR: 本文提出基于密度算子模型的生成式建模的可扩展训练算法DO - EM，用于经典硬件上学习DOMs定义的潜变量模型，在MNIST数据集上的图像生成表现优于经典DBM。


<details>
  <summary>Details</summary>
Motivation: 现有基于密度算子模型的生成式建模训练算法无法扩展到真实世界数据，期望开发可扩展算法。

Method: 将期望最大化算法框架应用于DOMs潜变量模型学习，把期望步骤重新表述为量子信息投影问题，引入DO - EM算法。

Result: DO - EM算法能确保一类广泛模型的对数似然不下降，QiDBM在MNIST数据集图像生成上优于经典DBM，弗雷歇 inception距离降低40 - 60%。

Conclusion: 开发的DO - EM算法可用于DOMs潜变量模型在经典硬件上的可扩展训练，且在图像生成任务中有良好表现。

Abstract: Density operators, quantum generalizations of probability distributions, are
gaining prominence in machine learning due to their foundational role in
quantum computing. Generative modeling based on density operator models
(\textbf{DOMs}) is an emerging field, but existing training algorithms -- such
as those for the Quantum Boltzmann Machine -- do not scale to real-world data,
such as the MNIST dataset. The Expectation-Maximization algorithm has played a
fundamental role in enabling scalable training of probabilistic latent variable
models on real-world datasets. \textit{In this paper, we develop an
Expectation-Maximization framework to learn latent variable models defined
through \textbf{DOMs} on classical hardware, with resources comparable to those
used for probabilistic models, while scaling to real-world data.} However,
designing such an algorithm is nontrivial due to the absence of a well-defined
quantum analogue to conditional probability, which complicates the Expectation
step. To overcome this, we reformulate the Expectation step as a quantum
information projection (QIP) problem and show that the Petz Recovery Map
provides a solution under sufficient conditions. Using this formulation, we
introduce the Density Operator Expectation Maximization (DO-EM) algorithm -- an
iterative Minorant-Maximization procedure that optimizes a quantum evidence
lower bound. We show that the \textbf{DO-EM} algorithm ensures non-decreasing
log-likelihood across iterations for a broad class of models. Finally, we
present Quantum Interleaved Deep Boltzmann Machines (\textbf{QiDBMs}), a
\textbf{DOM} that can be trained with the same resources as a DBM. When trained
with \textbf{DO-EM} under Contrastive Divergence, a \textbf{QiDBM} outperforms
larger classical DBMs in image generation on the MNIST dataset, achieving a
40--60\% reduction in the Fr\'echet Inception Distance.

</details>


### [49] [Learning Like Humans: Resource-Efficient Federated Fine-Tuning through Cognitive Developmental Stages](https://arxiv.org/abs/2508.00041)
*Yebo Wu,Jingguang Li,Zhijiang Guo,Li Li*

Main category: cs.LG

TL;DR: 本文提出资源高效的Developmental Federated Tuning (DevFT)方法用于大语言模型微调，分解微调过程，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 联邦微调资源消耗大，限制在边缘设备部署，需资源高效的微调方法。

Method: 引入DevFT，将微调过程分解为发展阶段，优化不同参数容量的子模型，知识从前阶段传递到后阶段；采用去冲突引导的层分组和基于差分的层融合构建特定阶段子模型。

Result: 在多个基准测试中，DevFT显著优于现有方法，收敛速度快4.59倍，通信开销降低10.67倍，平均性能提升9.07%，且与现有方法兼容。

Conclusion: DevFT是一种资源高效的大语言模型微调方法，能有效解决联邦微调资源消耗大的问题。

Abstract: Federated fine-tuning enables Large Language Models (LLMs) to adapt to
downstream tasks while preserving data privacy, but its resource-intensive
nature limits deployment on edge devices. In this paper, we introduce
Developmental Federated Tuning (DevFT), a resource-efficient approach inspired
by cognitive development that progressively builds a powerful LLM from a
compact foundation. DevFT decomposes the fine-tuning process into developmental
stages, each optimizing submodels with increasing parameter capacity. Knowledge
from earlier stages transfers to subsequent submodels, providing optimized
initialization parameters that prevent convergence to local minima and
accelerate training. This paradigm mirrors human learning, gradually
constructing comprehensive knowledge structure while refining existing skills.
To efficiently build stage-specific submodels, DevFT introduces
deconfliction-guided layer grouping and differential-based layer fusion to
distill essential information and construct representative layers. Evaluations
across multiple benchmarks demonstrate that DevFT significantly outperforms
state-of-the-art methods, achieving up to 4.59$\times$ faster convergence,
10.67$\times$ reduction in communication overhead, and 9.07% average
performance improvement, while maintaining compatibility with existing
approaches.

</details>


### [50] [Evaluating COVID 19 Feature Contributions to Bitcoin Return Forecasting: Methodology Based on LightGBM and Genetic Optimization](https://arxiv.org/abs/2508.00078)
*Imen Mahmoud,Andrei Velichko*

Main category: cs.LG

TL;DR: 本文提出结合LightGBM回归模型和遗传算法的方法框架，评估新冠指标对比特币回报预测的贡献，结果显示新冠指标显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 确定纳入疫情相关健康数据是否能显著提高比特币回报预测的准确性。

Method: 构建包含比特币日回报和新冠指标的数据集，用遗传算法对含和不含新冠特征的预测模型进行31次独立运行优化，通过分布重叠和Mann - Whitney U检验比较性能指标，用排列特征重要性分析量化特征贡献。

Result: 新冠指标显著提升模型性能，尤其在捕捉极端市场波动方面（R2提高40%，RMSE降低2%），疫苗接种指标是主要预测因素。

Conclusion: 所提方法扩展了现有金融分析工具，为投资者和政策制定者在系统性危机期间应对市场不确定性提供了更精确的指标。

Abstract: This study proposes a novel methodological framework integrating a LightGBM
regression model and genetic algorithm (GA) optimization to systematically
evaluate the contribution of COVID-19-related indicators to Bitcoin return
prediction. The primary objective was not merely to forecast Bitcoin returns
but rather to determine whether including pandemic-related health data
significantly enhances prediction accuracy. A comprehensive dataset comprising
daily Bitcoin returns and COVID-19 metrics (vaccination rates,
hospitalizations, testing statistics) was constructed. Predictive models,
trained with and without COVID-19 features, were optimized using GA over 31
independent runs, allowing robust statistical assessment. Performance metrics
(R2, RMSE, MAE) were statistically compared through distribution overlaps and
Mann-Whitney U tests. Permutation Feature Importance (PFI) analysis quantified
individual feature contributions. Results indicate that COVID-19 indicators
significantly improved model performance, particularly in capturing extreme
market fluctuations (R2 increased by 40%, RMSE decreased by 2%, both highly
significant statistically). Among COVID-19 features, vaccination metrics,
especially the 75th percentile of fully vaccinated individuals, emerged as
dominant predictors. The proposed methodology extends existing financial
analytics tools by incorporating public health signals, providing investors and
policymakers with refined indicators to navigate market uncertainty during
systemic crises.

</details>


### [51] [Improved Robustness and Functional Localization in Topographic CNNs Through Weight Similarity](https://arxiv.org/abs/2508.00043)
*Nhut Truong,Uri Hasson*

Main category: cs.LG

TL;DR: 本文比较两种空间约束训练的地形卷积神经网络，发现权重相似性（WS）约束比激活相似性（AS）和标准CNN有优势，能产生更鲁棒表示。


<details>
  <summary>Details</summary>
Motivation: 不同地形约束实现对神经网络学习表示的影响未被系统研究，因此进行比较研究。

Method: 比较用权重相似性（WS）和激活相似性（AS）两种空间约束训练的地形卷积神经网络，从分类准确率、对权重扰动和输入退化的鲁棒性等方面评估模型。

Result: 与AS和标准CNN相比，WS有三方面优势，包括提高对噪声的鲁棒性、更高的输入敏感性和更强的功能定位，还影响网络的表征几何。

Conclusion: 端到端训练中，WS约束比AS或非地形CNN产生更鲁棒的表示，基于权重的空间约束可塑造生物物理启发模型的特征学习和功能组织。

Abstract: Topographic neural networks are computational models that can simulate the
spatial and functional organization of the brain. Topographic constraints in
neural networks can be implemented in multiple ways, with potentially different
impacts on the representations learned by the network. The impact of such
different implementations has not been systematically examined. To this end,
here we compare topographic convolutional neural networks trained with two
spatial constraints: Weight Similarity (WS), which pushes neighboring units to
develop similar incoming weights, and Activation Similarity (AS), which
enforces similarity in unit activations. We evaluate the resulting models on
classification accuracy, robustness to weight perturbations and input
degradation, and the spatial organization of learned representations. Compared
to both AS and standard CNNs, WS provided three main advantages: i) improved
robustness to noise, also showing higher accuracy under weight corruption; ii)
greater input sensitivity, reflected in higher activation variance; and iii)
stronger functional localization, with units showing similar activations
positioned at closer distances. In addition, WS produced differences in
orientation tuning, symmetry sensitivity, and eccentricity profiles of units,
indicating an influence of this spatial constraint on the representational
geometry of the network. Our findings suggest that during end-to-end training,
WS constraints produce more robust representations than AS or non-topographic
CNNs. These findings also suggest that weight-based spatial constraints can
shape feature learning and functional organization in biophysical inspired
models.

</details>


### [52] [Benchmarking Partial Observability in Reinforcement Learning with a Suite of Memory-Improvable Domains](https://arxiv.org/abs/2508.00046)
*Ruo Yu Tao,Kaicheng Guo,Cameron Allen,George Konidaris*

Main category: cs.LG

TL;DR: 本文指出当前处理部分可观测性的强化学习算法基准存在不足，提出部分可观测基准的关键属性，介绍了部分可观测性强化学习基准的最佳实践指南和开源库POBAX。


<details>
  <summary>Details</summary>
Motivation: 当前多数处理部分可观测性的算法评估基准不能代表现实领域中的多种部分可观测形式，需要全面的基准来衡量算法在缓解部分可观测性方面的进展。

Method: 提出部分可观测基准应具备的两个关键属性，选取多种环境中的代表环境构建基准，提供推荐超参数和算法实现。

Result: 构建了开源库POBAX，所选任务具有记忆可提升性，需要难以学习的记忆函数。

Conclusion: 该框架为部分可观测性研究提供了具体信号，可实现快速评估和GPU可扩展实验。

Abstract: Mitigating partial observability is a necessary but challenging task for
general reinforcement learning algorithms. To improve an algorithm's ability to
mitigate partial observability, researchers need comprehensive benchmarks to
gauge progress. Most algorithms tackling partial observability are only
evaluated on benchmarks with simple forms of state aliasing, such as feature
masking and Gaussian noise. Such benchmarks do not represent the many forms of
partial observability seen in real domains, like visual occlusion or unknown
opponent intent. We argue that a partially observable benchmark should have two
key properties. The first is coverage in its forms of partial observability, to
ensure an algorithm's generalizability. The second is a large gap between the
performance of a agents with more or less state information, all other factors
roughly equal. This gap implies that an environment is memory improvable: where
performance gains in a domain are from an algorithm's ability to cope with
partial observability as opposed to other factors. We introduce best-practice
guidelines for empirically benchmarking reinforcement learning under partial
observability, as well as the open-source library POBAX: Partially Observable
Benchmarks in JAX. We characterize the types of partial observability present
in various environments and select representative environments for our
benchmark. These environments include localization and mapping, visual control,
games, and more. Additionally, we show that these tasks are all memory
improvable and require hard-to-learn memory functions, providing a concrete
signal for partial observability research. This framework includes recommended
hyperparameters as well as algorithm implementations for fast, out-of-the-box
evaluation, as well as highly performant environments implemented in JAX for
GPU-scalable experimentation.

</details>


### [53] [EMA Without the Lag: Bias-Corrected Iterate Averaging Schemes](https://arxiv.org/abs/2508.00180)
*Adam Block,Cyril Zhang*

Main category: cs.LG

TL;DR: 提出BEMA改进EMA，减少随机扰动并消除偏差，实验证明其在语言模型微调中效果更好。


<details>
  <summary>Details</summary>
Motivation: 语言模型微调的随机性会导致训练不稳定，EMA虽能减少随机性但会引入偏差，影响优化。

Method: 提出Bias - Corrected Exponential Moving Average (BEMA)，基于理论模型证明其加速效果。

Result: 在多种标准语言模型基准测试中，BEMA比EMA和普通训练有显著更好的收敛速度和最终性能。

Conclusion: BEMA是一种实用且有理论依据的方法，可使语言模型微调更稳定高效。

Abstract: Stochasticity in language model fine-tuning, often caused by the small batch
sizes typically used in this regime, can destabilize training by introducing
large oscillations in generation quality. A popular approach to mitigating this
instability is to take an Exponential moving average (EMA) of weights
throughout training. While EMA reduces stochasticity, thereby smoothing
training, the introduction of bias from old iterates often creates a lag in
optimization relative to vanilla training. In this work, we propose the
Bias-Corrected Exponential Moving Average (BEMA), a simple and practical
augmentation of EMA that retains variance-reduction benefits while eliminating
bias. BEMA is motivated by a simple theoretical model wherein we demonstrate
provable acceleration of BEMA over both a standard EMA and vanilla training.
Through an extensive suite of experiments on Language Models, we show that BEMA
leads to significantly improved convergence rates and final performance over
both EMA and vanilla training in a variety of standard LM benchmarks, making
BEMA a practical and theoretically motivated intervention for more stable and
efficient fine-tuning.

</details>


### [54] [Foundations of Interpretable Models](https://arxiv.org/abs/2508.00545)
*Pietro Barbiero,Mateo Espinosa Zarlenga,Alberto Termine,Mateja Jamnik,Giuseppe Marra*

Main category: cs.LG

TL;DR: 指出现有可解释性定义不可行，提出新定义、设计蓝图并开源库。


<details>
  <summary>Details</summary>
Motivation: 现有可解释性定义无法指导通用、合理和稳健的可解释模型设计，使研究存在问题。

Method: 提出通用、简单且涵盖现有概念的可解释性定义，在此基础上提出设计蓝图并开发开源库。

Result: 所提定义可揭示设计可解释模型所需的基础属性、假设、原则等。

Conclusion: 新定义具有可操作性，能推动可解释模型的设计。

Abstract: We argue that existing definitions of interpretability are not actionable in
that they fail to inform users about general, sound, and robust interpretable
model design. This makes current interpretability research fundamentally
ill-posed. To address this issue, we propose a definition of interpretability
that is general, simple, and subsumes existing informal notions within the
interpretable AI community. We show that our definition is actionable, as it
directly reveals the foundational properties, underlying assumptions,
principles, data structures, and architectural features necessary for designing
interpretable models. Building on this, we propose a general blueprint for
designing interpretable models and introduce the first open-sourced library
with native support for interpretable data structures and processes.

</details>


### [55] [TriP-LLM: A Tri-Branch Patch-wise Large Language Model Framework for Time-Series Anomaly Detection](https://arxiv.org/abs/2508.00047)
*Yuan-Cheng Yu,Yen-Chieh Ouyang,Chun-An Lin*

Main category: cs.LG

TL;DR: 提出TriP-LLM框架用于时间序列异常检测，实验显示其性能优于现有方法，内存消耗低，代码开源。


<details>
  <summary>Details</summary>
Motivation: 物联网和智能制造使时间序列数据规模和维度增加，传统统计方法难以处理其高异质性和复杂性，受大语言模型在多模态任务成功的启发。

Method: 提出TriP - LLM框架，通过三分支设计整合局部和全局时间特征，将输入时间序列编码为逐块令牌，由预训练的大语言模型处理，用轻量级逐块解码器重建输入并得出异常分数，使用PATE评估指标在统一开源框架下评估。

Result: TriP - LLM在所有数据集上始终优于最新的先进方法，具有强检测能力；与基于通道独立（CI）块处理的LLM方法相比，内存消耗显著降低。

Conclusion: TriP - LLM是一种有效的时间序列异常检测框架，大语言模型对整体架构有重要贡献，且适合GPU内存受限环境。

Abstract: Time-series anomaly detection plays a central role across a wide range of
application domains. With the increasing proliferation of the Internet of
Things (IoT) and smart manufacturing, time-series data has dramatically
increased in both scale and dimensionality. This growth has exposed the
limitations of traditional statistical methods in handling the high
heterogeneity and complexity of such data. Inspired by the recent success of
large language models (LLMs) in multimodal tasks across language and vision
domains, we propose a novel unsupervised anomaly detection framework: A
Tri-Branch Patch-wise Large Language Model Framework for Time-Series Anomaly
Detection (TriP-LLM). TriP-LLM integrates local and global temporal features
through a tri-branch design-Patching, Selection, and Global-to encode the input
time series into patch-wise tokens, which are then processed by a frozen,
pretrained LLM. A lightweight patch-wise decoder reconstructs the input, from
which anomaly scores are derived. We evaluate TriP-LLM on several public
benchmark datasets using PATE, a recently proposed threshold-free evaluation
metric, and conduct all comparisons within a unified open-source framework to
ensure fairness. Experimental results show that TriP-LLM consistently
outperforms recent state-of-the-art methods across all datasets, demonstrating
strong detection capabilities. Furthermore, through extensive ablation studies,
we verify the substantial contribution of the LLM to the overall architecture.
Compared to LLM-based approaches using Channel Independence (CI) patch
processing, TriP-LLM achieves significantly lower memory consumption, making it
more suitable for GPU memory-constrained environments. All code and model
checkpoints are publicly available on https://github.com/YYZStart/TriP-LLM.git

</details>


### [56] [Calibrated Language Models and How to Find Them with Label Smoothing](https://arxiv.org/abs/2508.00264)
*Jerry Huang,Peng Lu,Qiuhao Zeng*

Main category: cs.LG

TL;DR: 研究指令微调对大语言模型置信度校准的影响，提出用标签平滑解决校准退化问题，分析其在大词汇量模型中的不足，还设计定制内核减少内存消耗。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对指令微调影响大语言模型置信度校准的研究，需找到解决校准退化的方法。

Method: 研究各种开源大语言模型，采用标签平滑方法，从理论和实验上分析其在大词汇量模型中的问题，设计定制内核。

Result: 发现指令微调后模型校准退化，标签平滑在大词汇量模型中效果受限，定制内核可减少内存消耗。

Conclusion: 标签平滑可一定程度维持校准，但在大词汇量模型中有局限，定制内核能有效减少内存消耗。

Abstract: Recent advances in natural language processing (NLP) have opened up greater
opportunities to enable fine-tuned large language models (LLMs) to behave as
more powerful interactive agents through improved instruction-following
ability. However, understanding how this impacts confidence calibration for
reliable model output has not been researched in full. In this work, we examine
various open-sourced LLMs, identifying significant calibration degradation
after instruction tuning in each. Seeking a practical solution, we look towards
label smoothing, which has been shown as an effective method to regularize for
overconfident predictions but has yet to be widely adopted in the supervised
fine-tuning (SFT) of LLMs. We first provide insight as to why label smoothing
is sufficient to maintain calibration throughout the SFT process. However,
settings remain where the effectiveness of smoothing is severely diminished, in
particular the case of large vocabulary LLMs (LV-LLMs). We posit the cause to
stem from the ability to become over-confident, which has a direct relationship
with the hidden size and vocabulary size, and justify this theoretically and
experimentally. Finally, we address an outstanding issue regarding the memory
footprint of the cross-entropy loss computation in the label smoothed loss
setting, designing a customized kernel to dramatically reduce memory
consumption without sacrificing speed or performance in comparison to existing
solutions for non-smoothed losses.

</details>


### [57] [Adacc: Adaptive Compression and Activation Checkpointing for LLM Memory Management](https://arxiv.org/abs/2508.00806)
*Ping Chen,Zhuohong Deng,Ping Li,Shuibing He,Hongzi Zhu,Yi Zheng,Zhefeng Wang,Baoxing Huai,Minyi Guo*

Main category: cs.LG

TL;DR: 提出Adacc内存管理框架，结合自适应压缩和激活检查点减少GPU内存占用，加速LLM训练。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型训练中重计算方法会引入高达30%的开销，需减少GPU内存占用。

Method: 设计考虑LLM张量离群值的特定层压缩算法；提出用MILP确定每个张量最佳内存优化的调度策略；引入自适应策略演化机制调整策略。

Result: Adacc比现有框架将LLM训练加速1.01x到1.37x，且模型精度与基线相当。

Conclusion: Adacc能有效减少GPU内存占用，加速训练并保证模型精度。

Abstract: Training large language models often employs recomputation to alleviate
memory pressure, which can introduce up to 30% overhead in real-world
scenarios. In this paper, we propose Adacc, a novel memory management framework
that combines adaptive compression and activation checkpointing to reduce the
GPU memory footprint. It comprises three modules: (1) We design layer-specific
compression algorithms that account for outliers in LLM tensors, instead of
directly quantizing floats from FP16 to INT4, to ensure model accuracy. (2) We
propose an optimal scheduling policy that employs MILP to determine the best
memory optimization for each tensor. (3) To accommodate changes in training
tensors, we introduce an adaptive policy evolution mechanism that adjusts the
policy during training to enhance throughput. Experimental results show that
Adacc can accelerate the LLM training by 1.01x to 1.37x compared to
state-of-the-art frameworks, while maintaining comparable model accuracy to the
Baseline.

</details>


### [58] [Toward using explainable data-driven surrogate models for treating performance-based seismic design as an inverse engineering problem](https://arxiv.org/abs/2508.00286)
*Mohsen Zaker Esteghamati*

Main category: cs.LG

TL;DR: 研究提出将基于性能的抗震设计视为逆工程问题的方法，用可解释机器学习模型处理，结合遗传优化算法应用于钢和混凝土框架，结果显示代理模型精度高，算法能找出最优值。


<details>
  <summary>Details</summary>
Motivation: 解决基于性能的抗震设计计算效率低的问题，直接推导设计参数以实现特定性能目标。

Method: 采用可解释机器学习模型映射设计变量和性能指标，将其作为评估函数集成到遗传优化算法中解决逆问题。

Result: 代理模型在多种建筑类型、几何形状、抗震设计和场地危险情况下精度高（R2> 90%），优化算法能根据固定几何变量找出构件属性最优值。

Conclusion: 该方法可行，结果符合工程原理，能有效解决基于性能的抗震设计问题。

Abstract: This study presents a methodology to treat performance-based seismic design
as an inverse engineering problem, where design parameters are directly derived
to achieve specific performance objectives. By implementing explainable machine
learning models, this methodology directly maps design variables and
performance metrics, tackling computational inefficiencies of performance-based
design. The resultant machine learning model is integrated as an evaluation
function into a genetic optimization algorithm to solve the inverse problem.
The developed methodology is then applied to two different inventories of steel
and concrete moment frames in Los Angeles and Charleston to obtain sectional
properties of frame members that minimize expected annualized seismic loss in
terms of repair costs. The results show high accuracy of the surrogate models
(e.g., R2> 90%) across a diverse set of building types, geometries, seismic
design, and site hazard, where the optimization algorithm could identify the
optimum values of members' properties for a fixed set of geometric variables,
consistent with engineering principles.

</details>


### [59] [Stress-Aware Resilient Neural Training](https://arxiv.org/abs/2508.00098)
*Ashkan Shakarami,Yousef Yeganeh,Azade Farshad,Lorenzo Nicole,Stefano Ghidoni,Nassir Navab*

Main category: cs.LG

TL;DR: 本文提出应力感知学习（Stress - Aware Learning），一种弹性神经训练范式，通过塑性变形优化器实现，实验证明能提升模型鲁棒性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 让深度神经网络在稳定训练或动态不确定环境中动态调整优化行为，解决训练中优化困难问题。

Method: 基于材料科学中临时（弹性）和永久（塑性）变形概念，提出塑性变形优化器，在内部应力信号表明优化困难时向模型参数注入自适应噪声。

Result: 在六种架构、四种优化器和七个视觉基准测试中，模型鲁棒性和泛化性得到提升，且计算开销极小。

Conclusion: 应力感知学习范式可行且有效，能提升模型性能。

Abstract: This paper introduces Stress-Aware Learning, a resilient neural training
paradigm in which deep neural networks dynamically adjust their optimization
behavior - whether under stable training regimes or in settings with uncertain
dynamics - based on the concept of Temporary (Elastic) and Permanent (Plastic)
Deformation, inspired by structural fatigue in materials science. To
instantiate this concept, we propose Plastic Deformation Optimizer, a
stress-aware mechanism that injects adaptive noise into model parameters
whenever an internal stress signal - reflecting stagnation in training loss and
accuracy - indicates persistent optimization difficulty. This enables the model
to escape sharp minima and converge toward flatter, more generalizable regions
of the loss landscape. Experiments across six architectures, four optimizers,
and seven vision benchmarks demonstrate improved robustness and generalization
with minimal computational overhead. The code and 3D visuals will be available
on GitHub: https://github.com/Stress-Aware-Learning/SAL.

</details>


### [60] [StackLiverNet: A Novel Stacked Ensemble Model for Accurate and Interpretable Liver Disease Detection](https://arxiv.org/abs/2508.00117)
*Md. Ehsanul Haque,S. M. Jahidul Islam,Shakil Mia,Rumana Sharmin,Ashikuzzaman,Md Samir Morshed,Md. Tahmidul Huque*

Main category: cs.LG

TL;DR: 为解决现有肝疾病分类模型问题，提出StackLiverNet模型，性能优且可解释。


<details>
  <summary>Details</summary>
Motivation: 现有肝疾病分类的机器学习和深度学习模型存在高误分类错误、可解释性差、计算成本高和预处理策略不佳等问题。

Method: 采用先进数据预处理和特征选择技术，随机欠采样处理类别不平衡，StackLiverNet由多个超参数优化的基分类器集成，通过LightGBM元模型发挥优势，还使用LIME、SHAP和Morris方法。

Result: 模型测试准确率99.89%，Cohen Kappa为0.9974，AUC为0.9993，仅5个误分类，训练和推理速度适用于临床。

Conclusion: StackLiverNet模型性能出色，训练和推理速度快，可解释性强，适用于肝疾病检测。

Abstract: Liver diseases are a serious health concern in the world, which requires
precise and timely diagnosis to enhance the survival chances of patients. The
current literature implemented numerous machine learning and deep learning
models to classify liver diseases, but most of them had some issues like high
misclassification error, poor interpretability, prohibitive computational
expense, and lack of good preprocessing strategies. In order to address these
drawbacks, we introduced StackLiverNet in this study; an interpretable stacked
ensemble model tailored to the liver disease detection task. The framework uses
advanced data preprocessing and feature selection technique to increase model
robustness and predictive ability. Random undersampling is performed to deal
with class imbalance and make the training balanced. StackLiverNet is an
ensemble of several hyperparameter-optimized base classifiers, whose
complementary advantages are used through a LightGBM meta-model. The provided
model demonstrates excellent performance, with the testing accuracy of 99.89%,
Cohen Kappa of 0.9974, and AUC of 0.9993, having only 5 misclassifications, and
efficient training and inference speeds that are amenable to clinical practice
(training time 4.2783 seconds, inference time 0.1106 seconds). Besides, Local
Interpretable Model-Agnostic Explanations (LIME) are applied to generate
transparent explanations of individual predictions, revealing high
concentrations of Alkaline Phosphatase and moderate SGOT as important
observations of liver disease. Also, SHAP was used to rank features by their
global contribution to predictions, while the Morris method confirmed the most
influential features through sensitivity analysis.

</details>


### [61] [Structured Transformations for Stable and Interpretable Neural Computation](https://arxiv.org/abs/2508.00127)
*Saleh Nikooroo,Thomas Engel*

Main category: cs.LG

TL;DR: 本文提出层级别变换的新公式，提升神经网络稳定性和可解释性，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 当代神经网络缺乏促进稳定学习和可解释行为的结构保障。

Method: 将每个变换分解为结构化线性算子和残差校正分量。

Result: 构建的模型具有更好的梯度条件、对扰动的敏感性降低、层间鲁棒性增强，且这些优势在不同架构规模和训练机制下都存在。

Conclusion: 为更注重稳定性和透明度的神经网络架构奠定基础，提供新工具且不牺牲表达能力。

Abstract: Despite their impressive performance, contemporary neural networks often lack
structural safeguards that promote stable learning and interpretable behavior.
In this work, we introduce a reformulation of layer-level transformations that
departs from the standard unconstrained affine paradigm. Each transformation is
decomposed into a structured linear operator and a residual corrective
component, enabling more disciplined signal propagation and improved training
dynamics. Our formulation encourages internal consistency and supports stable
information flow across depth, while remaining fully compatible with standard
learning objectives and backpropagation. Through a series of synthetic and
real-world experiments, we demonstrate that models constructed with these
structured transformations exhibit improved gradient conditioning, reduced
sensitivity to perturbations, and layer-wise robustness. We further show that
these benefits persist across architectural scales and training regimes. This
study serves as a foundation for a more principled class of neural
architectures that prioritize stability and transparency-offering new tools for
reasoning about learning behavior without sacrificing expressive power.

</details>


### [62] [ECG Latent Feature Extraction with Autoencoders for Downstream Prediction Tasks](https://arxiv.org/abs/2508.00131)
*Christopher Harvey,Sumaiya Shomaji,Zijun Yao,Amit Noheria*

Main category: cs.LG

TL;DR: 本文探索心电图（ECG）特征生成方法，引入三种变分自编码器（VAE）变体，比较其效果，实验表明VAE编码能简化ECG数据，为小数据集深度学习提供实用方案。


<details>
  <summary>Details</summary>
Motivation: ECG信号复杂度高、个体差异大，在小训练数据集下用于深度学习模型有挑战，需探索特征生成方法。

Method: 探索代表性心跳ECG特征生成方法，用主成分分析（PCA）和自编码器降维，引入三种VAE变体，用Light Gradient Boost Machine（LGBM）评估效果。

Result: A beta - VAE信号重建效果好，MAE降至15.7±3.2 μV；SAE编码结合传统特征预测左心室射血分数（LVEF）表现佳，AUROC达0.901，接近最先进CNN模型，且计算资源需求少；ECG特征提取 - LGBM管道避免过拟合，小数据集训练仍有良好预测性能。

Conclusion: VAE编码有效简化ECG数据，为有限规模标注训练数据的深度学习应用提供实用解决方案。

Abstract: The electrocardiogram (ECG) is an inexpensive and widely available tool for
cardiac assessment. Despite its standardized format and small file size, the
high complexity and inter-individual variability of ECG signals (typically a
60,000-size vector with 12 leads at 500 Hz) make it challenging to use in deep
learning models, especially when only small training datasets are available.
This study addresses these challenges by exploring feature generation methods
from representative beat ECGs, focusing on Principal Component Analysis (PCA)
and Autoencoders to reduce data complexity. We introduce three novel
Variational Autoencoder (VAE) variants-Stochastic Autoencoder (SAE), Annealed
beta-VAE (A beta-VAE), and Cyclical beta VAE (C beta-VAE)-and compare their
effectiveness in maintaining signal fidelity and enhancing downstream
prediction tasks using a Light Gradient Boost Machine (LGBM). The A beta-VAE
achieved superior signal reconstruction, reducing the mean absolute error (MAE)
to 15.7+/-3.2 muV, which is at the level of signal noise. Moreover, the SAE
encodings, when combined with traditional ECG summary features, improved the
prediction of reduced Left Ventricular Ejection Fraction (LVEF), achieving an
holdout test set area under the receiver operating characteristic curve (AUROC)
of 0.901 with a LGBM classifier. This performance nearly matches the 0.909
AUROC of state-of-the-art CNN model but requires significantly less
computational resources. Further, the ECG feature extraction-LGBM pipeline
avoids overfitting and retains predictive performance when trained with less
data. Our findings demonstrate that these VAE encodings are not only effective
in simplifying ECG data but also provide a practical solution for applying deep
learning in contexts with limited-scale labeled training data.

</details>


### [63] [INSPIRE-GNN: Intelligent Sensor Placement to Improve Sparse Bicycling Network Prediction via Reinforcement Learning Boosted Graph Neural Networks](https://arxiv.org/abs/2508.00141)
*Mohit Gupta,Debjit Bhowmick,Rhys Newbury,Meead Saberi,Shirui Pan,Ben Beck*

Main category: cs.LG

TL;DR: 提出INSPIRE - GNN框架优化传感器布局，提升数据稀疏环境下自行车流量估计性能，在墨尔本网络测试中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 城市自行车计数传感器覆盖有限导致数据稀疏，影响准确的链路级自行车流量估计，需解决此问题。

Method: 提出INSPIRE - GNN框架，集成GCN、GAT与基于DQN的RL智能体，进行数据驱动的传感器位置选择。

Result: 应用于墨尔本自行车网络，在不同传感器部署数量下显著提升流量估计，在关键指标上优于传统启发式方法，且优于标准机器学习和深度学习模型。

Conclusion: 该框架为交通规划者提供有效扩展传感器网络、优化布局和提高自行车数据流量估计准确性与可靠性的见解，助力交通规划决策。

Abstract: Accurate link-level bicycling volume estimation is essential for sustainable
urban transportation planning. However, many cities face significant challenges
of high data sparsity due to limited bicycling count sensor coverage. To
address this issue, we propose INSPIRE-GNN, a novel Reinforcement Learning
(RL)-boosted hybrid Graph Neural Network (GNN) framework designed to optimize
sensor placement and improve link-level bicycling volume estimation in
data-sparse environments. INSPIRE-GNN integrates Graph Convolutional Networks
(GCN) and Graph Attention Networks (GAT) with a Deep Q-Network (DQN)-based RL
agent, enabling a data-driven strategic selection of sensor locations to
maximize estimation performance. Applied to Melbourne's bicycling network,
comprising 15,933 road segments with sensor coverage on only 141 road segments
(99% sparsity) - INSPIRE-GNN demonstrates significant improvements in volume
estimation by strategically selecting additional sensor locations in
deployments of 50, 100, 200 and 500 sensors. Our framework outperforms
traditional heuristic methods for sensor placement such as betweenness
centrality, closeness centrality, observed bicycling activity and random
placement, across key metrics such as Mean Squared Error (MSE), Root Mean
Squared Error (RMSE) and Mean Absolute Error (MAE). Furthermore, our
experiments benchmark INSPIRE-GNN against standard machine learning and deep
learning models in the bicycle volume estimation performance, underscoring its
effectiveness. Our proposed framework provides transport planners actionable
insights to effectively expand sensor networks, optimize sensor placement and
maximize volume estimation accuracy and reliability of bicycling data for
informed transportation planning decisions.

</details>


### [64] [Watch the Weights: Unsupervised monitoring and control of fine-tuned LLMs](https://arxiv.org/abs/2508.00161)
*Ziqian Zhong,Aditi Raghunathan*

Main category: cs.LG

TL;DR: 提出一种解释、监控和控制微调大语言模型的新方法，通过解释权重而非激活值，绕过对与训练数据分布相似数据的需求，在检测后门攻击、未学习主题推理等方面有良好效果，还可用于模型预部署审计。


<details>
  <summary>Details</summary>
Motivation: 现有可解释性方法依赖与训练数据分布相似的数据，在检测后门等新威胁时存在局限，需要新方法来理解、监控和控制微调大语言模型。

Method: 解释权重而非激活值，利用微调模型与基础模型权重差的顶部奇异向量对应新获得的行为，通过监测激活值沿这些方向的余弦相似度来检测微调中引入的显著行为。

Result: 对后门模型攻击拦截率达100%，误报率低于1.2%；对未学习模型检测擦除主题推理准确率达95.42%，还能引导模型恢复“未学习”信息；可用于商业指令微调模型审计，发现特定微调重点。

Conclusion: 所提出的方法有效，在检测威胁、监控模型和预部署审计等方面具有潜力，代码可在https://github.com/fjzzq2002/WeightWatch获取。

Abstract: The releases of powerful open-weight large language models (LLMs) are often
not accompanied by access to their full training data. Existing
interpretability methods, particularly those based on activations, often
require or assume distributionally similar data. This is a significant
limitation when detecting and defending against novel potential threats like
backdoors, which are by definition out-of-distribution.
  In this work, we introduce a new method for understanding, monitoring and
controlling fine-tuned LLMs that interprets weights, rather than activations,
thereby side stepping the need for data that is distributionally similar to the
unknown training data. We demonstrate that the top singular vectors of the
weight difference between a fine-tuned model and its base model correspond to
newly acquired behaviors. By monitoring the cosine similarity of activations
along these directions, we can detect salient behaviors introduced during
fine-tuning with high precision.
  For backdoored models that bypasses safety mechanisms when a secret trigger
is present, our method stops up to 100% of attacks with a false positive rate
below 1.2%. For models that have undergone unlearning, we detect inference on
erased topics with accuracy up to 95.42% and can even steer the model to
recover "unlearned" information. Besides monitoring, our method also shows
potential for pre-deployment model auditing: by analyzing commercial
instruction-tuned models (OLMo, Llama, Qwen), we are able to uncover
model-specific fine-tuning focus including marketing strategies and Midjourney
prompt generation.
  Our implementation can be found at https://github.com/fjzzq2002/WeightWatch.

</details>


### [65] [DiSC-Med: Diffusion-based Semantic Communications for Robust Medical Image Transmission](https://arxiv.org/abs/2508.00172)
*Fupei Guo,Hao Zheng,Xiang Zhang,Li Chen,Yue Wang,Songyang Zhang*

Main category: cs.LG

TL;DR: 提出用于医学图像传输的基于扩散的语义通信框架DiSC - Med，实验验证其有效性和潜力。


<details>
  <summary>Details</summary>
Motivation: 人工智能和无线通信推动远程医疗发展，而在有限带宽和噪声信道下高效传输医疗数据是关键挑战。

Method: 提出DiSC - Med框架，开发医疗增强压缩和去噪模块。

Result: 能捕捉关键语义信息，在噪声信道下以超高带宽效率实现卓越重建性能。

Conclusion: 该框架经实验验证有效，有用于远程医疗应用的潜力。

Abstract: The rapid development of artificial intelligence has driven smart health with
next-generation wireless communication technologies, stimulating exciting
applications in remote diagnosis and intervention. To enable a timely and
effective response for remote healthcare, efficient transmission of medical
data through noisy channels with limited bandwidth emerges as a critical
challenge. In this work, we propose a novel diffusion-based semantic
communication framework, namely DiSC-Med, for the medical image transmission,
where medical-enhanced compression and denoising blocks are developed for
bandwidth efficiency and robustness, respectively. Unlike conventional
pixel-wise communication framework, our proposed DiSC-Med is able to capture
the key semantic information and achieve superior reconstruction performance
with ultra-high bandwidth efficiency against noisy channels. Extensive
experiments on real-world medical datasets validate the effectiveness of our
framework, demonstrating its potential for robust and efficient telehealth
applications.

</details>


### [66] [RL as Regressor: A Reinforcement Learning Approach for Function Approximation](https://arxiv.org/abs/2508.00174)
*Yongchao Huang*

Main category: cs.LG

TL;DR: 文章提出将回归问题转化为强化学习问题，通过学习噪声正弦波案例展示其可行性，结果表明该框架能解决回归问题且有更强灵活性。


<details>
  <summary>Details</summary>
Motivation: 标准回归技术受预定义可微损失函数限制，无法充分捕捉系统期望行为，尤其是处理非对称成本或复杂不可微目标时。

Method: 将模型预测视为动作，基于预测误差定义自定义奖励信号，利用强化学习算法进行函数逼近，通过学习噪声正弦波案例迭代增强智能体。

Result: 强化学习框架成功解决回归问题，在定义目标和引导学习过程中具有更强灵活性。

Conclusion: 将回归问题转化为强化学习问题是可行且有效的，能提供比标准回归技术更多的灵活性。

Abstract: Standard regression techniques, while powerful, are often constrained by
predefined, differentiable loss functions such as mean squared error. These
functions may not fully capture the desired behavior of a system, especially
when dealing with asymmetric costs or complex, non-differentiable objectives.
In this paper, we explore an alternative paradigm: framing regression as a
Reinforcement Learning (RL) problem. We demonstrate this by treating a model's
prediction as an action and defining a custom reward signal based on the
prediction error, and we can leverage powerful RL algorithms to perform
function approximation. Through a progressive case study of learning a noisy
sine wave, we illustrate the development of an Actor-Critic agent, iteratively
enhancing it with Prioritized Experience Replay, increased network capacity,
and positional encoding to enable a capable RL agent for this regression task.
Our results show that the RL framework not only successfully solves the
regression problem but also offers enhanced flexibility in defining objectives
and guiding the learning process.

</details>


### [67] [RecoMind: A Reinforcement Learning Framework for Optimizing In-Session User Satisfaction in Recommendation Systems](https://arxiv.org/abs/2508.00201)
*Mehdi Ben Ayed,Fei Feng,Jay Adams,Vishwakarma Singh,Kritarth Anand,Jiajing Xu*

Main category: cs.LG

TL;DR: 本文介绍适用于网络规模的基于模拟器的强化学习框架RecoMind，评估显示其训练的策略在提升用户会话满意度上优于传统监督学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有网络规模推荐系统常用监督学习方法关注即时反馈，强化学习虽能优化长期目标，但在网络规模应用面临挑战，需有效解决方案。

Method: 利用现有推荐模型建立模拟环境并引导强化学习策略，集成现有行业流程；引入自定义探索策略探索大规模动作空间。

Result: 离线模拟和在线A/B测试表明，RecoMind训练的强化学习策略在提升会话内用户满意度上显著优于传统监督学习方法，如在线测试中特定指标有提升。

Conclusion: RecoMind为将强化学习嵌入网络规模推荐系统提供了系统且可扩展的方法，有望优化基于会话的用户满意度。

Abstract: Existing web-scale recommendation systems commonly use supervised learning
methods that prioritize immediate user feedback. Although reinforcement
learning (RL) offers a solution to optimize longer-term goals, such as
in-session engagement, applying it at web scale is challenging due to the
extremely large action space and engineering complexity. In this paper, we
introduce RecoMind, a simulator-based RL framework designed for the effective
optimization of session-based goals at web-scale. RecoMind leverages existing
recommendation models to establish a simulation environment and to bootstrap
the RL policy to optimize immediate user interactions from the outset. This
method integrates well with existing industry pipelines, simplifying the
training and deployment of RL policies. Additionally, RecoMind introduces a
custom exploration strategy to efficiently explore web-scale action spaces with
hundreds of millions of items. We evaluated RecoMind through extensive offline
simulations and online A/B testing on a video streaming platform. Both methods
showed that the RL policy trained using RecoMind significantly outperforms
traditional supervised learning recommendation approaches in in-session user
satisfaction. In online A/B tests, the RL policy increased videos watched for
more than 10 seconds by 15.81\% and improved session depth by 4.71\% for
sessions with at least 10 interactions. As a result, RecoMind presents a
systematic and scalable approach for embedding RL into web-scale recommendation
systems, showing great promise for optimizing session-based user satisfaction.

</details>


### [68] [Robust Classification under Noisy Labels: A Geometry-Aware Reliability Framework for Foundation Models](https://arxiv.org/abs/2508.00202)
*Ecem Bozkurt,Antonio Ortega*

Main category: cs.LG

TL;DR: 论文提出两阶段框架，在不重新训练模型下确保含标签噪声数据的鲁棒分类，通过引入几何信息提升性能，在CIFAR - 10和DermaMNIST上验证效果优于标准K - NN等方法。


<details>
  <summary>Details</summary>
Motivation: 在使用预训练基础模型用噪声数据微调时，现有基于局部几何的kNN方法虽在标签噪声下有一定性能，但希望通过引入几何信息进一步提升性能。

Method: 采用两阶段程序，即可靠性估计和可靠性加权推理，推理时用非负核（NNK）邻域构造获取训练数据局部邻域，提出几种可靠性估计方法。

Result: 在CIFAR - 10和DermaMNIST上的评估显示，该方法在不同噪声条件下提高了鲁棒性，超越标准K - NN方法和最近的自适应邻域基线。

Conclusion: 提出的两阶段框架能在不重新训练模型的情况下，有效提升含标签噪声数据的分类鲁棒性。

Abstract: Foundation models (FMs) pretrained on large datasets have become fundamental
for various downstream machine learning tasks, in particular in scenarios where
obtaining perfectly labeled data is prohibitively expensive. In this paper, we
assume an FM has to be fine-tuned with noisy data and present a two-stage
framework to ensure robust classification in the presence of label noise
without model retraining. Recent work has shown that simple k-nearest neighbor
(kNN) approaches using an embedding derived from an FM can achieve good
performance even in the presence of severe label noise. Our work is motivated
by the fact that these methods make use of local geometry. In this paper,
following a similar two-stage procedure, reliability estimation followed by
reliability-weighted inference, we show that improved performance can be
achieved by introducing geometry information. For a given instance, our
proposed inference uses a local neighborhood of training data, obtained using
the non-negative kernel (NNK) neighborhood construction. We propose several
methods for reliability estimation that can rely less on distance and local
neighborhood as the label noise increases. Our evaluation on CIFAR-10 and
DermaMNIST shows that our methods improve robustness across various noise
conditions, surpassing standard K-NN approaches and recent
adaptive-neighborhood baselines.

</details>


### [69] [Towards Higher Effective Rank in Parameter-efficient Fine-tuning using Khatri--Rao Product](https://arxiv.org/abs/2508.00230)
*Paul Albert,Frederic Z. Zhang,Hemanth Saratchandran,Anton van den Hengel,Ehsan Abbasnejad*

Main category: cs.LG

TL;DR: 本文定量比较全秩和低秩参数高效微调方法，指出LoRA局限，提出KRAdapter算法，在视觉语言和大语言模型上有性能提升，且保持内存和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有低秩适应（LoRA）方法在多模态和大语言模型中有局限性，需对比全秩和低秩方法并提出改进。

Method: 使用合成矩阵近似基准进行定量比较，提出利用Khatri - Rao积产生权重更新的KRAdapter算法。

Result: LoRA难以近似高有效秩矩阵，KRAdapter在视觉语言和大语言模型上有性能提升，尤其在未见常识推理任务上。

Conclusion: KRAdapter是微调十亿级参数模型实用且稳健的替代方法。

Abstract: Parameter-efficient fine-tuning (PEFT) has become a standard approach for
adapting large pre-trained models. Amongst PEFT methods, low-rank adaptation
(LoRA) has achieved notable success. However, recent studies have highlighted
its limitations compared against full-rank alternatives, particularly when
applied to multimodal and large language models. In this work, we present a
quantitative comparison amongst full-rank and low-rank PEFT methods using a
synthetic matrix approximation benchmark with controlled spectral properties.
Our results confirm that LoRA struggles to approximate matrices with relatively
flat spectrums or high frequency components -- signs of high effective ranks.
To this end, we introduce KRAdapter, a novel PEFT algorithm that leverages the
Khatri-Rao product to produce weight updates, which, by construction, tends to
produce matrix product with a high effective rank. We demonstrate performance
gains with KRAdapter on vision-language models up to 1B parameters and on large
language models up to 8B parameters, particularly on unseen common-sense
reasoning tasks. In addition, KRAdapter maintains the memory and compute
efficiency of LoRA, making it a practical and robust alternative to fine-tune
billion-scale parameter models.

</details>


### [70] [Learning to Optimize Feedback for One Million Students: Insights from Multi-Armed and Contextual Bandits in Large-Scale Online Tutoring](https://arxiv.org/abs/2508.00270)
*Robin Schmucker,Nimish Pachapurkar,Shanmuga Bala,Miral Shah,Tom Mitchell*

Main category: cs.LG

TL;DR: 本文介绍在线辅导系统，用多臂老虎机框架和离线策略评估优化反馈，评估策略提升学生表现，还研究情境老虎机策略，虽部分有效果异质性但提升有限。


<details>
  <summary>Details</summary>
Motivation: 开发在线辅导系统，为回答错误的学生提供有效反馈，优化学生学习效果。

Method: 采用多臂老虎机（MAB）框架和离线策略评估，设计算法决定合适策略训练目标；用因果推断研究情境老虎机（CB）策略。

Result: 评估的MAB策略在实践中显著提升学生表现；CB策略因效果异质性效果大小小，难超优化的MAB策略。

Conclusion: 系统优化的教学策略支持数千学生，讨论大规模部署数据驱动系统的见解和未来改进方向。

Abstract: We present an online tutoring system that learns to provide effective
feedback to students after they answer questions incorrectly. Using data from
one million students, the system learns which assistance action (e.g., one of
multiple hints) to provide for each question to optimize student learning.
Employing the multi-armed bandit (MAB) framework and offline policy evaluation,
we assess 43,000 assistance actions, and identify trade-offs between assistance
policies optimized for different student outcomes (e.g., response correctness,
session completion). We design an algorithm that for each question decides on a
suitable policy training objective to enhance students' immediate second
attempt success and overall practice session performance. We evaluate the
resulting MAB policies in 166,000 practice sessions, verifying significant
improvements in student outcomes. While MAB policies optimize feedback for the
overall student population, we further investigate whether contextual bandit
(CB) policies can enhance outcomes by personalizing feedback based on
individual student features (e.g., ability estimates, response times). Using
causal inference, we examine (i) how effects of assistance actions vary across
students and (ii) whether CB policies, which leverage such effect
heterogeneity, outperform MAB policies. While our analysis reveals that some
actions for some questions exhibit effect heterogeneity, effect sizes may often
be too small for CB policies to provide significant improvements beyond what
well-optimized MAB policies that deliver the same action to all students
already achieve. We discuss insights gained from deploying data-driven systems
at scale and implications for future refinements. Today, the teaching policies
optimized by our system support thousands of students daily.

</details>


### [71] [Invariant Graph Transformer for Out-of-Distribution Generalization](https://arxiv.org/abs/2508.00304)
*Tianyin Liao,Ziwei Zhang,Yufei Sun,Chunyu Hu,Jianxin Li*

Main category: cs.LG

TL;DR: 提出GOODFormer，通过联合优化三个模块学习广义图表示，实验证明其在分布偏移下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图Transformer在分布偏移下泛化能力不足，图不变学习设计注意力机制和PSEs有挑战。

Method: 开发基于GT的熵引导不变子图解缠器，设计演化子图位置和结构编码器，提出利用子图节点表示和编码的不变学习模块，并给出理论依据。

Result: 在基准数据集上的大量实验表明，该方法在分布偏移下优于现有最先进的基线方法。

Conclusion: GOODFormer能有效学习广义图表示，在分布偏移场景有较好泛化能力。

Abstract: Graph Transformers (GTs) have demonstrated great effectiveness across various
graph analytical tasks. However, the existing GTs focus on training and testing
graph data originated from the same distribution, but fail to generalize under
distribution shifts. Graph invariant learning, aiming to capture generalizable
graph structural patterns with labels under distribution shifts, is potentially
a promising solution, but how to design attention mechanisms and positional and
structural encodings (PSEs) based on graph invariant learning principles
remains challenging. To solve these challenges, we introduce Graph
Out-Of-Distribution generalized Transformer (GOODFormer), aiming to learn
generalized graph representations by capturing invariant relationships between
predictive graph structures and labels through jointly optimizing three
modules. Specifically, we first develop a GT-based entropy-guided invariant
subgraph disentangler to separate invariant and variant subgraphs while
preserving the sharpness of the attention function. Next, we design an evolving
subgraph positional and structural encoder to effectively and efficiently
capture the encoding information of dynamically changing subgraphs during
training. Finally, we propose an invariant learning module utilizing subgraph
node representations and encodings to derive generalizable graph
representations that can to unseen graphs. We also provide theoretical
justifications for our method. Extensive experiments on benchmark datasets
demonstrate the superiority of our method over state-of-the-art baselines under
distribution shifts.

</details>


### [72] [PnP-DA: Towards Principled Plug-and-Play Integration of Variational Data Assimilation and Generative Models](https://arxiv.org/abs/2508.00325)
*Yongquan Qu,Matthieu Blanke,Sara Shamekh,Pierre Gentine*

Main category: cs.LG

TL;DR: 本文提出PnP - DA算法用于地球系统建模的数据同化，可减少预测误差，优于经典变分方法。


<details>
  <summary>Details</summary>
Motivation: 地球系统建模存在计算效率与预测误差的挑战，现有AI或物理预测系统有误差积累，传统变分方法假设的高斯误差统计不适用于混沌动力系统。

Method: 提出PnP - DA算法，交替进行轻量级基于梯度的分析更新和通过条件Wasserstein耦合对预训练生成先验进行单次前向传播。

Result: 在标准混沌测试平台上的实验表明，该策略在不同观测稀疏度和噪声水平下持续减少预测误差。

Conclusion: PnP - DA算法放松了统计假设，利用历史数据，避免了在同化周期中通过复杂神经网络反向传播梯度，优于经典变分方法。

Abstract: Earth system modeling presents a fundamental challenge in scientific
computing: capturing complex, multiscale nonlinear dynamics in computationally
efficient models while minimizing forecast errors caused by necessary
simplifications. Even the most powerful AI- or physics-based forecast system
suffer from gradual error accumulation. Data assimilation (DA) aims to mitigate
these errors by optimally blending (noisy) observations with prior model
forecasts, but conventional variational methods often assume Gaussian error
statistics that fail to capture the true, non-Gaussian behavior of chaotic
dynamical systems. We propose PnP-DA, a Plug-and-Play algorithm that alternates
(1) a lightweight, gradient-based analysis update (using a Mahalanobis-distance
misfit on new observations) with (2) a single forward pass through a pretrained
generative prior conditioned on the background forecast via a conditional
Wasserstein coupling. This strategy relaxes restrictive statistical assumptions
and leverages rich historical data without requiring an explicit regularization
functional, and it also avoids the need to backpropagate gradients through the
complex neural network that encodes the prior during assimilation cycles.
Experiments on standard chaotic testbeds demonstrate that this strategy
consistently reduces forecast errors across a range of observation sparsities
and noise levels, outperforming classical variational methods.

</details>


### [73] [Embryology of a Language Model](https://arxiv.org/abs/2508.00331)
*George Wang,Garrett Baker,Andrew Gordon,Daniel Murfet*

Main category: cs.LG

TL;DR: 本文用胚胎学方法结合UMAP可视化语言模型结构发展，发现新结构，证明易感性分析可发现新机制。


<details>
  <summary>Details</summary>
Motivation: 理解语言模型内部计算结构是深度学习核心问题，统计物理中的易感性用于可视化网络组织的潜力未充分挖掘。

Method: 引入胚胎学方法，将UMAP应用于易感性矩阵，可视化模型训练过程中的结构发展。

Result: 可视化揭示清晰的“身体蓝图”，发现已知特征形成，如新的“间距鳍”结构。

Conclusion: 易感性分析可超越验证，为研究复杂神经网络发展原理提供有力整体视角。

Abstract: Understanding how language models develop their internal computational
structure is a central problem in the science of deep learning. While
susceptibilities, drawn from statistical physics, offer a promising analytical
tool, their full potential for visualizing network organization remains
untapped. In this work, we introduce an embryological approach, applying UMAP
to the susceptibility matrix to visualize the model's structural development
over training. Our visualizations reveal the emergence of a clear ``body
plan,'' charting the formation of known features like the induction circuit and
discovering previously unknown structures, such as a ``spacing fin'' dedicated
to counting space tokens. This work demonstrates that susceptibility analysis
can move beyond validation to uncover novel mechanisms, providing a powerful,
holistic lens for studying the developmental principles of complex neural
networks.

</details>


### [74] [BOOD: Boundary-based Out-Of-Distribution Data Generation](https://arxiv.org/abs/2508.00350)
*Qilin Liao,Shuo Yang,Bo Zhao,Ping Luo,Hengshuang Zhao*

Main category: cs.LG

TL;DR: 本文提出BOOD框架，利用扩散模型生成高质量OOD特征和图像，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在潜在空间中提取分布外有效特征存在困难，难以确定类别间决策边界，需提升OOD检测性能。

Method: BOOD先从分布内数据集学习文本条件潜在特征空间，选择靠近决策边界的特征并扰动形成OOD特征，再用扩散模型将特征解码为图像。

Result: 在常见基准测试中，BOOD表现显著优于现有方法，如在CIFAR - 100数据集上平均FPR95降低29.64%，平均AUROC提高7.27%。

Conclusion: BOOD为合成信息丰富的OOD特征提供了更高效的训练策略，有助于更清晰区分分布内和分布外数据。

Abstract: Harnessing the power of diffusion models to synthesize auxiliary training
data based on latent space features has proven effective in enhancing
out-of-distribution (OOD) detection performance. However, extracting effective
features outside the in-distribution (ID) boundary in latent space remains
challenging due to the difficulty of identifying decision boundaries between
classes. This paper proposes a novel framework called Boundary-based
Out-Of-Distribution data generation (BOOD), which synthesizes high-quality OOD
features and generates human-compatible outlier images using diffusion models.
BOOD first learns a text-conditioned latent feature space from the ID dataset,
selects ID features closest to the decision boundary, and perturbs them to
cross the decision boundary to form OOD features. These synthetic OOD features
are then decoded into images in pixel space by a diffusion model. Compared to
previous works, BOOD provides a more training efficient strategy for
synthesizing informative OOD features, facilitating clearer distinctions
between ID and OOD data. Extensive experimental results on common benchmarks
demonstrate that BOOD surpasses the state-of-the-art method significantly,
achieving a 29.64% decrease in average FPR95 (40.31% vs. 10.67%) and a 7.27%
improvement in average AUROC (90.15% vs. 97.42%) on the CIFAR-100 dataset.

</details>


### [75] [Sheaf Graph Neural Networks via PAC-Bayes Spectral Optimization](https://arxiv.org/abs/2508.00357)
*Yoonhyuk Choi,Jiho Choi,Chong-Kwon Kim*

Main category: cs.LG

TL;DR: 提出SGPC方案用于半监督节点分类，在多个数据集上表现优于现有方法并提供置信区间。


<details>
  <summary>Details</summary>
Motivation: 图神经网络存在过平滑问题，现有基于层的模型有泛化性和可扩展性问题且无严格稳定性保证。

Method: 结合细胞层消息传递、最优传输提升、方差缩减扩散和PAC - Bayes谱正则化等机制的SGPC统一架构。

Result: 理论上建立性能边界，能以线性计算复杂度端到端训练实现边界感知目标；在九个数据集实验中优于现有方法并提供未见过节点的置信区间。

Conclusion: SGPC是解决图神经网络过平滑问题的有效方案。

Abstract: Over-smoothing in Graph Neural Networks (GNNs) causes collapse in distinct
node features, particularly on heterophilic graphs where adjacent nodes often
have dissimilar labels. Although sheaf neural networks partially mitigate this
problem, they typically rely on static or heavily parameterized sheaf
structures that hinder generalization and scalability. Existing sheaf-based
models either predefine restriction maps or introduce excessive complexity, yet
fail to provide rigorous stability guarantees. In this paper, we introduce a
novel scheme called SGPC (Sheaf GNNs with PAC-Bayes Calibration), a unified
architecture that combines cellular-sheaf message passing with several
mechanisms, including optimal transport-based lifting, variance-reduced
diffusion, and PAC-Bayes spectral regularization for robust semi-supervised
node classification. We establish performance bounds theoretically and
demonstrate that the resulting bound-aware objective can be achieved via
end-to-end training in linear computational complexity. Experiments on nine
homophilic and heterophilic benchmarks show that SGPC outperforms
state-of-the-art spectral and sheaf-based GNNs while providing certified
confidence intervals on unseen nodes.

</details>


### [76] [OID-PPO: Optimal Interior Design using Proximal Policy Optimization by Transforming Design Guidelines into Reward Functions](https://arxiv.org/abs/2508.00364)
*Chanyoung Yoon,Sangbong Yoo,Soobin Yim,Chansoo Kim,Yun Jang*

Main category: cs.LG

TL;DR: 提出OID - PPO强化学习框架用于室内设计，实验表明其在布局质量和计算效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有室内设计方法存在计算成本高、数据稀缺、家具放置受限和未充分融入设计原则等问题，需要新方法解决。

Method: 提出OID - PPO框架，将专家定义的功能和视觉准则融入结构化奖励函数，使用对角高斯策略进行连续灵活的家具放置。

Result: 在不同房间形状和家具配置的实验中，OID - PPO在布局质量和计算效率上显著优于现有方法，消融研究揭示了准则集成和设计约束的影响。

Conclusion: OID - PPO是一种有效的室内设计方法，结构化准则集成和设计约束对室内设计有重要作用。

Abstract: Designing residential interiors strongly impacts occupant satisfaction but
remains challenging due to unstructured spatial layouts, high computational
demands, and reliance on expert knowledge. Existing methods based on
optimization or deep learning are either computationally expensive or
constrained by data scarcity. Reinforcement learning (RL) approaches often
limit furniture placement to discrete positions and fail to incorporate design
principles adequately. We propose OID-PPO, a novel RL framework for Optimal
Interior Design using Proximal Policy Optimization, which integrates
expert-defined functional and visual guidelines into a structured reward
function. OID-PPO utilizes a diagonal Gaussian policy for continuous and
flexible furniture placement, effectively exploring latent environmental
dynamics under partial observability. Experiments conducted across diverse room
shapes and furniture configurations demonstrate that OID-PPO significantly
outperforms state-of-the-art methods in terms of layout quality and
computational efficiency. Ablation studies further demonstrate the impact of
structured guideline integration and reveal the distinct contributions of
individual design constraints.

</details>


### [77] [Dual Adaptivity: Universal Algorithms for Minimizing the Adaptive Regret of Convex Functions](https://arxiv.org/abs/2508.00392)
*Lijun Zhang,Wenhao Yang,Guanghui Wang,Wei Jiang,Zhi-Hua Zhou*

Main category: cs.LG

TL;DR: 本文提出具有双重自适应性的通用算法，用于在线凸优化和在线复合优化，以最小化自适应遗憾，解决现有算法缺乏通用性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有在线凸优化算法缺乏通用性，只能处理一种凸函数且需先验参数知识，阻碍其在现实场景应用。

Method: 提出元专家框架，动态创建多个专家并由元算法聚合，元算法需产生二阶界，结合休眠专家技术，构建专家时采用两种策略。

Result: 算法能同时最小化多种凸函数的自适应遗憾，允许函数类型在轮次间切换，并将框架扩展到在线复合优化。

Conclusion: 所提算法有效解决现有算法局限性，可用于多种场景最小化自适应遗憾。

Abstract: To deal with changing environments, a new performance measure -- adaptive
regret, defined as the maximum static regret over any interval, was proposed in
online learning. Under the setting of online convex optimization, several
algorithms have been successfully developed to minimize the adaptive regret.
However, existing algorithms lack universality in the sense that they can only
handle one type of convex functions and need apriori knowledge of parameters,
which hinders their application in real-world scenarios. To address this
limitation, this paper investigates universal algorithms with dual adaptivity,
which automatically adapt to the property of functions (convex, exponentially
concave, or strongly convex), as well as the nature of environments (stationary
or changing). Specifically, we propose a meta-expert framework for dual
adaptive algorithms, where multiple experts are created dynamically and
aggregated by a meta-algorithm. The meta-algorithm is required to yield a
second-order bound, which can accommodate unknown function types. We further
incorporate the technique of sleeping experts to capture the changing
environments. For the construction of experts, we introduce two strategies
(increasing the number of experts or enhancing the capabilities of experts) to
achieve universality. Theoretical analysis shows that our algorithms are able
to minimize the adaptive regret for multiple types of convex functions
simultaneously, and also allow the type of functions to switch between rounds.
Moreover, we extend our meta-expert framework to online composite optimization,
and develop a universal algorithm for minimizing the adaptive regret of
composite functions.

</details>


### [78] [ExeKGLib: A Platform for Machine Learning Analytics based on Knowledge Graphs](https://arxiv.org/abs/2508.00394)
*Antonis Klironomos,Baifan Zhou,Zhipeng Tan,Zhuoxun Zheng,Mohamed H. Gad-Elrab,Heiko Paulheim,Evgeny Kharlamov*

Main category: cs.LG

TL;DR: 本文介绍了ExeKGLib，一个带有图形界面的Python库，帮助缺乏ML知识的用户构建ML管道，并展示其实用案例。


<details>
  <summary>Details</summary>
Motivation: 当前开发高质量ML管道需要专业知识和训练，而科学与工程领域专家急需基于ML的分析却缺乏相关技能。

Method: 构建带有图形界面层的Python库ExeKGLib，借助知识图谱以简单术语编码ML知识。

Result: 通过实际用例展示了ExeKGLib的可用性和实用性。

Conclusion: ExeKGLib能让缺乏ML知识的用户构建ML管道，提高工作流的透明度和可重用性，确保可执行性。

Abstract: Nowadays machine learning (ML) practitioners have access to numerous ML
libraries available online. Such libraries can be used to create ML pipelines
that consist of a series of steps where each step may invoke up to several ML
libraries that are used for various data-driven analytical tasks. Development
of high-quality ML pipelines is non-trivial; it requires training, ML
expertise, and careful development of each step. At the same time, domain
experts in science and engineering may not possess such ML expertise and
training while they are in pressing need of ML-based analytics. In this paper,
we present our ExeKGLib, a Python library enhanced with a graphical interface
layer that allows users with minimal ML knowledge to build ML pipelines. This
is achieved by relying on knowledge graphs that encode ML knowledge in simple
terms accessible to non-ML experts. ExeKGLib also allows improving the
transparency and reusability of the built ML workflows and ensures that they
are executable. We show the usability and usefulness of ExeKGLib by presenting
real use cases.

</details>


### [79] [Co-Reward: Self-supervised Reinforcement Learning for Large Language Model Reasoning via Contrastive Agreement](https://arxiv.org/abs/2508.00410)
*Zizhuo Zhang,Jianing Zhu,Xinmu Ge,Zihua Zhao,Zhanke Zhou,Xuan Li,Xiao Feng,Jiangchao Yao,Bo Han*

Main category: cs.LG

TL;DR: 提出Co - Reward强化学习框架，利用语义类比问题的对比一致性作为奖励基础，在多个推理基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习与可验证奖励（RLVR）方法在复杂任务中依赖人工标注标签存在扩展困境，探索自奖励信号的方法存在崩溃问题。

Method: 为每个训练样本构建相似问题，通过简单的滚动投票合成替代标签，交叉引用问题对的标签构建奖励，以增强类比输入间的内部推理一致性。

Result: Co - Reward在多个推理基准和大语言模型系列上优于其他自奖励基线，在MATH500上比基于真实标签的奖励最高提升6.8%。

Conclusion: Co - Reward是一种有效的强化学习框架，能促进稳定的推理启发和提升。

Abstract: Although reinforcement learning with verifiable rewards (RLVR) shows promise
in improving the reasoning ability of large language models (LLMs), the scaling
up dilemma remains due to the reliance on human annotated labels especially for
complex tasks. Recent alternatives that explore various self-reward signals
exhibit the eliciting potential of LLM reasoning, but suffer from the
non-negligible collapse issue. Inspired by the success of self-supervised
learning, we propose \textit{Co-Reward}, a novel RL framework that leverages
contrastive agreement across semantically analogical questions as a reward
basis. Specifically, we construct a similar question for each training sample
(without labels) and synthesize their individual surrogate labels through a
simple rollout voting, and then the reward is constructed by cross-referring
the labels of each question pair to enforce the internal reasoning consistency
across analogical inputs. Intuitively, such a self-supervised reward-shaping
mechanism increases the difficulty of learning collapse into a trivial
solution, and promotes stable reasoning elicitation and improvement through
expanding the input sample variants. Empirically, Co-Reward achieves superior
performance compared to other self-reward baselines on multiple reasoning
benchmarks and LLM series, and reaches or even surpasses ground-truth (GT)
labeled reward, with improvements of up to $+6.8\%$ on MATH500 over GT reward
on Llama-3.2-3B-Instruct. Our code is publicly available at
https://github.com/tmlr-group/Co-Reward.

</details>


### [80] [Transforming Credit Risk Analysis: A Time-Series-Driven ResE-BiLSTM Framework for Post-Loan Default Detection](https://arxiv.org/abs/2508.00415)
*Yue Yang,Yuxiang Lin,Ying Zhang,Zihan Su,Chang Chuan Goh,Tangtangfang Fang,Anthony Graham Bellotti,Boon Giin Lee*

Main category: cs.LG

TL;DR: 研究引入ResE - BiLSTM模型用于贷后违约预测，与五个基线模型对比，实验表明其性能更优。


<details>
  <summary>Details</summary>
Motivation: 提高贷后违约预测性能，解决信用风险管理中的重要任务。

Method: 引入ResE - BiLSTM模型，使用滑动窗口技术，在Freddie Mac美国抵押贷款数据集上评估，与五个基线模型对比，进行消融研究和SHAP分析。

Result: ResE - BiLSTM在多个指标上比基线模型有更优的预测性能。

Conclusion: ResE - BiLSTM在现实场景中有实用价值和适用性。

Abstract: Prediction of post-loan default is an important task in credit risk
management, and can be addressed by detection of financial anomalies using
machine learning. This study introduces a ResE-BiLSTM model, using a sliding
window technique, and is evaluated on 44 independent cohorts from the extensive
Freddie Mac US mortgage dataset, to improve prediction performance. The
ResE-BiLSTM is compared with five baseline models: Long Short-Term Memory
(LSTM), BiLSTM, Gated Recurrent Units (GRU), Convolutional Neural Networks
(CNN), and Recurrent Neural Networks (RNN), across multiple metrics, including
Accuracy, Precision, Recall, F1, and AUC. An ablation study was conducted to
evaluate the contribution of individual components in the ResE-BiLSTM
architecture. Additionally, SHAP analysis was employed to interpret the
underlying features the model relied upon for its predictions. Experimental
results demonstrate that ResE-BiLSTM achieves superior predictive performance
compared to baseline models, underscoring its practical value and applicability
in real-world scenarios.

</details>


### [81] [A Conditional GAN for Tabular Data Generation with Probabilistic Sampling of Latent Subspaces](https://arxiv.org/abs/2508.00472)
*Leonidas Akritidis,Panayiotis Bozanis*

Main category: cs.LG

TL;DR: 提出ctdGAN缓解表格数据集中的类别不平衡问题，在14个不平衡数据集上表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有GAN模型未考虑输入样本向量子空间，类别标签处理方式影响条件采样效果，导致表格数据类别不平衡问题处理不佳。

Method: 执行空间分区步骤给输入样本分配聚类标签，利用新的概率采样策略和损失函数合成样本，引入聚类缩放技术。

Result: 在14个不平衡数据集上的详尽评估表明，ctdGAN在生成高保真样本和提高分类准确率方面表现优越。

Conclusion: ctdGAN能有效缓解表格数据集中的类别不平衡问题。

Abstract: The tabular form constitutes the standard way of representing data in
relational database systems and spreadsheets. But, similarly to other forms,
tabular data suffers from class imbalance, a problem that causes serious
performance degradation in a wide variety of machine learning tasks. One of the
most effective solutions dictates the usage of Generative Adversarial Networks
(GANs) in order to synthesize artificial data instances for the
under-represented classes. Despite their good performance, none of the proposed
GAN models takes into account the vector subspaces of the input samples in the
real data space, leading to data generation in arbitrary locations. Moreover,
the class labels are treated in the same manner as the other categorical
variables during training, so conditional sampling by class is rendered less
effective. To overcome these problems, this study presents ctdGAN, a
conditional GAN for alleviating class imbalance in tabular datasets. Initially,
ctdGAN executes a space partitioning step to assign cluster labels to the input
samples. Subsequently, it utilizes these labels to synthesize samples via a
novel probabilistic sampling strategy and a new loss function that penalizes
both cluster and class mis-predictions. In this way, ctdGAN is trained to
generate samples in subspaces that resemble those of the original data
distribution. We also introduce several other improvements, including a simple,
yet effective cluster-wise scaling technique that captures multiple feature
modes without affecting data dimensionality. The exhaustive evaluation of
ctdGAN with 14 imbalanced datasets demonstrated its superiority in generating
high fidelity samples and improving classification accuracy.

</details>


### [82] [Court of LLMs: Evidence-Augmented Generation via Multi-LLM Collaboration for Text-Attributed Graph Anomaly Detection](https://arxiv.org/abs/2508.00507)
*Yiming Xu,Jiarun Chen,Zhen Peng,Zihan Chen,Qika Lin,Lan Ma,Bin Shi,Bo Dong*

Main category: cs.LG

TL;DR: 文章指出现有文本属性图异常检测方法忽视文本模态价值，提出结合大语言模型和图神经网络的CoLL框架，实验证明其优势并为相关领域开辟新途径。


<details>
  <summary>Details</summary>
Motivation: 现有图异常检测方法忽略文本模态互补价值，大语言模型应用于文本属性图异常检测尚处起步且难以编码图的高阶结构信息，需开发高质量检测方法。

Method: 提出CoLL框架，采用多LLM协作进行证据增强生成，结合带门控机制的GNN自适应融合文本特征与证据并保留高阶拓扑信息。

Result: 实验表明CoLL具有优越性，平均AP提升13.37%。

Conclusion: 本研究为将大语言模型融入图异常检测开辟新途径。

Abstract: The natural combination of intricate topological structures and rich textual
information in text-attributed graphs (TAGs) opens up a novel perspective for
graph anomaly detection (GAD). However, existing GAD methods primarily focus on
designing complex optimization objectives within the graph domain, overlooking
the complementary value of the textual modality, whose features are often
encoded by shallow embedding techniques, such as bag-of-words or skip-gram, so
that semantic context related to anomalies may be missed. To unleash the
enormous potential of textual modality, large language models (LLMs) have
emerged as promising alternatives due to their strong semantic understanding
and reasoning capabilities. Nevertheless, their application to TAG anomaly
detection remains nascent, and they struggle to encode high-order structural
information inherent in graphs due to input length constraints. For
high-quality anomaly detection in TAGs, we propose CoLL, a novel framework that
combines LLMs and graph neural networks (GNNs) to leverage their complementary
strengths. CoLL employs multi-LLM collaboration for evidence-augmented
generation to capture anomaly-relevant contexts while delivering human-readable
rationales for detected anomalies. Moreover, CoLL integrates a GNN equipped
with a gating mechanism to adaptively fuse textual features with evidence while
preserving high-order topological information. Extensive experiments
demonstrate the superiority of CoLL, achieving an average improvement of 13.37%
in AP. This study opens a new avenue for incorporating LLMs in advancing GAD.

</details>


### [83] [Text-Attributed Graph Anomaly Detection via Multi-Scale Cross- and Uni-Modal Contrastive Learning](https://arxiv.org/abs/2508.00513)
*Yiming Xu,Xu Hua,Zhen Peng,Bin Shi,Jiarun Chen,Xingbo Fu,Song Wang,Bo Dong*

Main category: cs.LG

TL;DR: 本文提出端到端文本属性图异常检测范式CMUCL，联合训练文本和图编码器，设计异常分数估计器，还发布8个数据集，评估显示其显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有图异常检测流程文本编码与图领域异常检测训练目标分离，难以聚焦相关信息，限制检测能力，需无缝整合原始文本和图拓扑进行异常检测。

Method: 提出CMUCL范式，同时对文本和图结构数据建模，利用跨模态和单模态多尺度一致性联合训练文本和图编码器，设计基于不一致挖掘的异常分数估计器。

Result: CMUCL在文本属性图异常检测上显著进步，平均准确率比次优方法提高11.13%。

Conclusion: CMUCL能有效进行文本属性图异常检测，所发布数据集有助于未来研究。

Abstract: The widespread application of graph data in various high-risk scenarios has
increased attention to graph anomaly detection (GAD). Faced with real-world
graphs that often carry node descriptions in the form of raw text sequences,
termed text-attributed graphs (TAGs), existing graph anomaly detection
pipelines typically involve shallow embedding techniques to encode such textual
information into features, and then rely on complex self-supervised tasks
within the graph domain to detect anomalies. However, this text encoding
process is separated from the anomaly detection training objective in the graph
domain, making it difficult to ensure that the extracted textual features focus
on GAD-relevant information, seriously constraining the detection capability.
How to seamlessly integrate raw text and graph topology to unleash the vast
potential of cross-modal data in TAGs for anomaly detection poses a challenging
issue. This paper presents a novel end-to-end paradigm for text-attributed
graph anomaly detection, named CMUCL. We simultaneously model data from both
text and graph structures, and jointly train text and graph encoders by
leveraging cross-modal and uni-modal multi-scale consistency to uncover
potential anomaly-related information. Accordingly, we design an anomaly score
estimator based on inconsistency mining to derive node-specific anomaly scores.
Considering the lack of benchmark datasets tailored for anomaly detection on
TAGs, we release 8 datasets to facilitate future research. Extensive
evaluations show that CMUCL significantly advances in text-attributed graph
anomaly detection, delivering an 11.13% increase in average accuracy (AP) over
the suboptimal.

</details>


### [84] [Online Nonsubmodular Optimization with Delayed Feedback in the Bandit Setting](https://arxiv.org/abs/2508.00523)
*Sifan Yang,Yuanyu Wan,Lijun Zhang*

Main category: cs.LG

TL;DR: 研究带延迟反馈的在线非子模优化问题，提出DBGD - NF算法及扩展算法，给出新的后悔界并通过实验验证优越性。


<details>
  <summary>Details</summary>
Motivation: 先前工作的后悔界依赖最大延迟，对不规则延迟敏感，且未分离延迟和土匪反馈的影响。

Method: 提出DBGD - NF算法，使用单点梯度估计器并利用可用梯度更新决策；扩展DBGD - NF算法，采用块更新机制分离延迟和土匪反馈的联合影响。

Result: DBGD - NF算法获得更好的 $\mathcal{O}(n\bar{d}^{1/3}T^{2/3})$ 后悔界；扩展算法获得 $\mathcal{O}(n(T^{2/3} + \sqrt{dT}))$ 后悔界，在特定条件下与无延迟反馈的后悔界匹配。

Conclusion: 所提方法在结构化稀疏学习实验中表现优越。

Abstract: We investigate the online nonsubmodular optimization with delayed feedback in
the bandit setting, where the loss function is $\alpha$-weakly DR-submodular
and $\beta$-weakly DR-supermodular. Previous work has established an
$(\alpha,\beta)$-regret bound of $\mathcal{O}(nd^{1/3}T^{2/3})$, where $n$ is
the dimensionality and $d$ is the maximum delay. However, its regret bound
relies on the maximum delay and is thus sensitive to irregular delays.
Additionally, it couples the effects of delays and bandit feedback as its bound
is the product of the delay term and the $\mathcal{O}(nT^{2/3})$ regret bound
in the bandit setting without delayed feedback. In this paper, we develop two
algorithms to address these limitations, respectively. Firstly, we propose a
novel method, namely DBGD-NF, which employs the one-point gradient estimator
and utilizes all the available estimated gradients in each round to update the
decision. It achieves a better $\mathcal{O}(n\bar{d}^{1/3}T^{2/3})$ regret
bound, which is relevant to the average delay $\bar{d} =
\frac{1}{T}\sum_{t=1}^T d_t\leq d$. Secondly, we extend DBGD-NF by employing a
blocking update mechanism to decouple the joint effect of the delays and bandit
feedback, which enjoys an $\mathcal{O}(n(T^{2/3} + \sqrt{dT}))$ regret bound.
When $d = \mathcal{O}(T^{1/3})$, our regret bound matches the
$\mathcal{O}(nT^{2/3})$ bound in the bandit setting without delayed feedback.
Compared to our first $\mathcal{O}(n\bar{d}^{1/3}T^{2/3})$ bound, it is more
advantageous when the maximum delay $d = o(\bar{d}^{2/3}T^{1/3})$. Finally, we
conduct experiments on structured sparse learning to demonstrate the
superiority of our methods.

</details>


### [85] [Phase-Locked SNR Band Selection for Weak Mineral Signal Detection in Hyperspectral Imagery](https://arxiv.org/abs/2508.00539)
*Judy X Yang*

Main category: cs.LG

TL;DR: 提出两阶段集成框架用于增强矿物检测，先处理高光谱数据降噪去冗余，再进行聚类和丰度反演，实验证明可提高反演精度和检测弱矿物区。


<details>
  <summary>Details</summary>
Motivation: 高光谱成像中弱矿物特征常被噪声和冗余波段掩盖，限制检测性能，需改进方法。

Method: 第一阶段计算信噪比并应用锁相阈值技术丢弃低信噪比波段，用Savitzky - Golay滤波平滑光谱；第二阶段用KMeans聚类提取端元光谱，用非负最小二乘法进行丰度反演，并用余弦相似度和RMSE指标与实验室光谱对比。

Result: 提出的流程提高了反演精度，增强了对弱矿物区的检测。

Conclusion: 两阶段策略为地质高光谱应用中的光谱降维和反演提供了实用且可重复的解决方案。

Abstract: Hyperspectral imaging offers detailed spectral information for mineral
mapping; however, weak mineral signatures are often masked by noisy and
redundant bands, limiting detection performance. To address this, we propose a
two-stage integrated framework for enhanced mineral detection in the Cuprite
mining district. In the first stage, we compute the signal-to-noise ratio (SNR)
for each spectral band and apply a phase-locked thresholding technique to
discard low-SNR bands, effectively removing redundancy and suppressing
background noise. Savitzky-Golay filtering is then employed for spectral
smoothing, serving a dual role first to stabilize trends during band selection,
and second to preserve fine-grained spectral features during preprocessing. In
the second stage, the refined HSI data is reintroduced into the model, where
KMeans clustering is used to extract 12 endmember spectra (W1 custom), followed
by non negative least squares (NNLS) for abundance unmixing. The resulting
endmembers are quantitatively compared with laboratory spectra (W1 raw) using
cosine similarity and RMSE metrics. Experimental results confirm that our
proposed pipeline improves unmixing accuracy and enhances the detection of weak
mineral zones. This two-pass strategy demonstrates a practical and reproducible
solution for spectral dimensionality reduction and unmixing in geological HSI
applications.

</details>


### [86] [Learning Potential Energy Surfaces of Hydrogen Atom Transfer Reactions in Peptides](https://arxiv.org/abs/2508.00578)
*Marlen Neubert,Patrick Reiser,Frauke Gräter,Pascal Friederich*

Main category: cs.LG

TL;DR: 本文针对生物过程中HAT反应模拟难题，用半经验方法和DFT构建数据集，对比三种图神经网络架构，MACE表现最佳，可集成到大规模模拟中，还分析相关特性并提出改进策略，方法具通用性。


<details>
  <summary>Details</summary>
Motivation: HAT反应在生物过程中重要但机理路径未完全明晰，经典力场和DFT分子动力学不适用于模拟，机器学习势虽有替代可能，但需定制数据生成和模型选择。

Method: 用半经验方法和DFT系统生成肽中HAT构型构建大数据集，对比SchNet、Allegro和MACE三种图神经网络架构学习HAT势能面及间接预测反应势垒的能力。

Result: MACE在能量、力和势垒预测上始终优于其他架构，在分布外DFT势垒预测上平均绝对误差为1.13 kcal/mol，可集成到大规模胶原模拟中计算反应速率。

Conclusion: 该方法可推进对肽中HAT和自由基迁移的机理理解，具有可扩展性和通用性，能用于其他生物分子系统复杂环境化学反应性的量子精确模拟。

Abstract: Hydrogen atom transfer (HAT) reactions are essential in many biological
processes, such as radical migration in damaged proteins, but their mechanistic
pathways remain incompletely understood. Simulating HAT is challenging due to
the need for quantum chemical accuracy at biologically relevant scales; thus,
neither classical force fields nor DFT-based molecular dynamics are applicable.
Machine-learned potentials offer an alternative, able to learn potential energy
surfaces (PESs) with near-quantum accuracy. However, training these models to
generalize across diverse HAT configurations, especially at radical positions
in proteins, requires tailored data generation and careful model selection.
Here, we systematically generate HAT configurations in peptides to build large
datasets using semiempirical methods and DFT. We benchmark three graph neural
network architectures (SchNet, Allegro, and MACE) on their ability to learn HAT
PESs and indirectly predict reaction barriers from energy predictions. MACE
consistently outperforms the others in energy, force, and barrier prediction,
achieving a mean absolute error of 1.13 kcal/mol on out-of-distribution DFT
barrier predictions. This accuracy enables integration of ML potentials into
large-scale collagen simulations to compute reaction rates from predicted
barriers, advancing mechanistic understanding of HAT and radical migration in
peptides. We analyze scaling laws, model transferability, and cost-performance
trade-offs, and outline strategies for improvement by combining ML potentials
with transition state search algorithms and active learning. Our approach is
generalizable to other biomolecular systems, enabling quantum-accurate
simulations of chemical reactivity in complex environments.

</details>


### [87] [The Role of Active Learning in Modern Machine Learning](https://arxiv.org/abs/2508.00586)
*Thorben Werner,Lars Schmidt-Thieme,Vijaya Krishna Yalavarthi*

Main category: cs.LG

TL;DR: 研究不同方法应对低数据场景，发现主动学习单独效率低，与数据增强和半监督学习结合仍有提升，应作为最后优化步骤。


<details>
  <summary>Details</summary>
Motivation: 主动学习计算成本高且在少量标注点场景提升小，很少应用于科学文献外，研究不同方法应对低数据场景。

Method: 研究数据增强（DA）、半监督学习（SSL）和主动学习（AL）应对低数据场景。

Result: 主动学习单独解决低数据问题效率最低，仅比随机采样提升1 - 4%，DA和SSL与随机采样结合可提升达60%，AL与强DA和SSL结合仍能改进。

Conclusion: 应将主动学习视为在应用合适DA和SSL方法后进一步挖掘数据性能的最后步骤，而非解决标签缺失的方法。

Abstract: Even though Active Learning (AL) is widely studied, it is rarely applied in
contexts outside its own scientific literature. We posit that the reason for
this is AL's high computational cost coupled with the comparatively small lifts
it is typically able to generate in scenarios with few labeled points. In this
work we study the impact of different methods to combat this low data scenario,
namely data augmentation (DA), semi-supervised learning (SSL) and AL. We find
that AL is by far the least efficient method of solving the low data problem,
generating a lift of only 1-4\% over random sampling, while DA and SSL methods
can generate up to 60\% lift in combination with random sampling. However, when
AL is combined with strong DA and SSL techniques, it surprisingly is still able
to provide improvements. Based on these results, we frame AL not as a method to
combat missing labels, but as the final building block to squeeze the last bits
of performance out of data after appropriate DA and SSL methods as been
applied.

</details>


### [88] [Similarity-Based Self-Construct Graph Model for Predicting Patient Criticalness Using Graph Neural Networks and EHR Data](https://arxiv.org/abs/2508.00615)
*Mukesh Kumar Sahu,Pinki Roy*

Main category: cs.LG

TL;DR: 提出SBSCGM和HybridGraphMedGNN模型预测ICU患者死亡率和临界分数，实验表现优异，提供可解释方案。


<details>
  <summary>Details</summary>
Motivation: 传统模型难以利用电子健康记录中的关系结构，需准确预测ICU患者临界性以进行早期干预。

Method: SBSCGM用混合相似度度量实时构建患者相似图，HybridGraphMedGNN集成GCN、GraphSAGE和GAT层学习患者表示。

Result: 在MIMIC - III数据集6000个ICU病例实验中，模型AUC - ROC达0.94，优于基线分类器和单类型GNN模型，精度/召回率提高，注意力机制使预测可解释。

Conclusion: 该框架为重症监护风险预测提供可扩展和可解释的解决方案，有实际应用潜力。

Abstract: Accurately predicting the criticalness of ICU patients (such as in-ICU
mortality risk) is vital for early intervention in critical care. However,
conventional models often treat each patient in isolation and struggle to
exploit the relational structure in Electronic Health Records (EHR). We propose
a Similarity-Based Self-Construct Graph Model (SBSCGM) that dynamically builds
a patient similarity graph from multi-modal EHR data, and a HybridGraphMedGNN
architecture that operates on this graph to predict patient mortality and a
continuous criticalness score. SBSCGM uses a hybrid similarity measure
(combining feature-based and structural similarities) to connect patients with
analogous clinical profiles in real-time. The HybridGraphMedGNN integrates
Graph Convolutional Network (GCN), GraphSAGE, and Graph Attention Network (GAT)
layers to learn robust patient representations, leveraging both local and
global graph patterns. In experiments on 6,000 ICU stays from the MIMIC-III
dataset, our model achieves state-of-the-art performance (AUC-ROC $0.94$)
outperforming baseline classifiers and single-type GNN models. We also
demonstrate improved precision/recall and show that the attention mechanism
provides interpretable insights into model predictions. Our framework offers a
scalable and interpretable solution for critical care risk prediction, with
potential to support clinicians in real-world ICU deployment.

</details>


### [89] [IAMAP: Unlocking Deep Learning in QGIS for non-coders and limited computing resources](https://arxiv.org/abs/2508.00627)
*Paul Tresson,Pierre Le Coz,Hadrien Tulet,Anthony Malkassian,Maxime Réjou Méchain*

Main category: cs.LG

TL;DR: 本文介绍了用户友好的QGIS插件IAMAP，它解决了深度学习在遥感应用中的限制，推动了深度学习方法的普及。


<details>
  <summary>Details</summary>
Motivation: 深度学习在遥感应用中因需要大量参考数据集、计算资源和编码技能，限制了非专家的使用，因此需要开发解决方案。

Method: 基于自监督学习策略的最新进展，利用通用基础模型，通过IAMAP界面简化遥感图像分析的关键步骤。

Result: IAMAP让非AI专家能利用深度学习方法提供的高质量特征，无需GPU和大量参考数据集。

Conclusion: IAMAP有助于实现计算高效且节能的深度学习方法的普及。

Abstract: Remote sensing has entered a new era with the rapid development of artificial
intelligence approaches. However, the implementation of deep learning has
largely remained restricted to specialists and has been impractical because it
often requires (i) large reference datasets for model training and validation;
(ii) substantial computing resources; and (iii) strong coding skills. Here, we
introduce IAMAP, a user-friendly QGIS plugin that addresses these three
challenges in an easy yet flexible way. IAMAP builds on recent advancements in
self-supervised learning strategies, which now provide robust feature
extractors, often referred to as foundation models. These generalist models can
often be reliably used in few-shot or zero-shot scenarios (i.e., with little to
no fine-tuning). IAMAP's interface allows users to streamline several key steps
in remote sensing image analysis: (i) extracting image features using a wide
range of deep learning architectures; (ii) reducing dimensionality with
built-in algorithms; (iii) performing clustering on features or their reduced
representations; (iv) generating feature similarity maps; and (v) calibrating
and validating supervised machine learning models for prediction. By enabling
non-AI specialists to leverage the high-quality features provided by recent
deep learning approaches without requiring GPU capacity or extensive reference
datasets, IAMAP contributes to the democratization of computationally efficient
and energy-conscious deep learning methods.

</details>


### [90] [Separated-Variable Spectral Neural Networks: A Physics-Informed Learning Approach for High-Frequency PDEs](https://arxiv.org/abs/2508.00628)
*Xiong Xiong,Zhuo Zhang,Rongchun Hu,Chen Gao,Zichen Deng*

Main category: cs.LG

TL;DR: 提出Separated - Variable Spectral Neural Networks (SV - SNN)解决高频振荡偏微分方程，解决传统PINNs光谱偏差问题，评估显示有高精度、低参数和短训练时间优势。


<details>
  <summary>Details</summary>
Motivation: 传统物理信息神经网络（PINNs）存在光谱偏差，难以捕捉高频解分量，需要新方法解决高频振荡偏微分方程。

Method: 引入SV - SNN框架，将变量分离与自适应光谱方法结合，包括将多元函数分解、采用自适应傅里叶光谱特征、建立基于奇异值分解的理论框架。

Result: 在多个基准问题上评估，SV - SNN精度提高1 - 3个数量级，参数减少超90%，训练时间减少60%。

Conclusion: SV - SNN是解决神经偏微分方程求解中光谱偏差问题的有效方案。

Abstract: Solving high-frequency oscillatory partial differential equations (PDEs) is a
critical challenge in scientific computing, with applications in fluid
mechanics, quantum mechanics, and electromagnetic wave propagation. Traditional
physics-informed neural networks (PINNs) suffer from spectral bias, limiting
their ability to capture high-frequency solution components. We introduce
Separated-Variable Spectral Neural Networks (SV-SNN), a novel framework that
addresses these limitations by integrating separation of variables with
adaptive spectral methods. Our approach features three key innovations: (1)
decomposition of multivariate functions into univariate function products,
enabling independent spatial and temporal networks; (2) adaptive Fourier
spectral features with learnable frequency parameters for high-frequency
capture; and (3) theoretical framework based on singular value decomposition to
quantify spectral bias. Comprehensive evaluation on benchmark problems
including Heat equation, Helmholtz equation, Poisson equations and
Navier-Stokes equations demonstrates that SV-SNN achieves 1-3 orders of
magnitude improvement in accuracy while reducing parameter count by over 90\%
and training time by 60\%. These results establish SV-SNN as an effective
solution to the spectral bias problem in neural PDE solving. The implementation
will be made publicly available upon acceptance at
https://github.com/xgxgnpu/SV-SNN.

</details>


### [91] [KFS: KAN based adaptive Frequency Selection learning architecture for long term time series forecasting](https://arxiv.org/abs/2508.00635)
*Changning Wu,Gao Wu,Rongyao Cai,Yong Liu,Kexin Zhang*

Main category: cs.LG

TL;DR: 提出基于KAN的自适应频率选择学习架构KFS用于时间序列预测，在多数据集上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现实时间序列存在跨尺度噪声干扰和不同尺度频率分量信息分布异构问题，导致多尺度表示不佳。

Method: 提出KFS框架，通过FreK模块在频谱域进行基于能量分布的主频率选择，KAN实现复杂模式表示，时间戳嵌入对齐同步跨尺度时间表示，特征混合模块融合特定尺度模式和对齐的时间特征。

Result: 在多个真实世界时间序列数据集上进行大量实验，KFS取得了最先进的性能。

Conclusion: KFS是一种简单而有效的架构。

Abstract: Multi-scale decomposition architectures have emerged as predominant
methodologies in time series forecasting. However, real-world time series
exhibit noise interference across different scales, while heterogeneous
information distribution among frequency components at varying scales leads to
suboptimal multi-scale representation. Inspired by Kolmogorov-Arnold Networks
(KAN) and Parseval's theorem, we propose a KAN based adaptive Frequency
Selection learning architecture (KFS) to address these challenges. This
framework tackles prediction challenges stemming from cross-scale noise
interference and complex pattern modeling through its FreK module, which
performs energy-distribution-based dominant frequency selection in the spectral
domain. Simultaneously, KAN enables sophisticated pattern representation while
timestamp embedding alignment synchronizes temporal representations across
scales. The feature mixing module then fuses scale-specific patterns with
aligned temporal features. Extensive experiments across multiple real-world
time series datasets demonstrate that KT achieves state-of-the-art performance
as a simple yet effective architecture.

</details>


### [92] [Reinforcement Learning for Decision-Level Interception Prioritization in Drone Swarm Defense](https://arxiv.org/abs/2508.00641)
*Alessandro Palmas*

Main category: cs.LG

TL;DR: 本文通过案例研究展示强化学习在应对低成本自杀式无人机群威胁中的优势，通过模拟实验验证其有效性并开源代码。


<details>
  <summary>Details</summary>
Motivation: 低成本自杀式无人机群对现代防御系统构成挑战，需要快速战略决策进行拦截优先级排序。

Method: 引入高保真模拟环境，让决策级强化学习智能体学习协调多个拦截器进行最优拦截优先级排序，并在离散动作空间中根据观测状态特征选择拦截目标，与手工规则基线策略对比。

Result: 强化学习策略在保护关键区域时平均损失更低、防御效率更高。

Conclusion: 强化学习可作为防御架构中的战略层，增强防御系统弹性且不取代现有控制系统。

Abstract: The growing threat of low-cost kamikaze drone swarms poses a critical
challenge to modern defense systems demanding rapid and strategic
decision-making to prioritize interceptions across multiple effectors and
high-value target zones. In this work, we present a case study demonstrating
the practical advantages of reinforcement learning in addressing this
challenge. We introduce a high-fidelity simulation environment that captures
realistic operational constraints, within which a decision-level reinforcement
learning agent learns to coordinate multiple effectors for optimal interception
prioritization. Operating in a discrete action space, the agent selects which
drone to engage per effector based on observed state features such as
positions, classes, and effector status. We evaluate the learned policy against
a handcrafted rule-based baseline across hundreds of simulated attack
scenarios. The reinforcement learning based policy consistently achieves lower
average damage and higher defensive efficiency in protecting critical zones.
This case study highlights the potential of reinforcement learning as a
strategic layer within defense architectures, enhancing resilience without
displacing existing control systems. All code and simulation assets are
publicly released for full reproducibility, and a video demonstration
illustrates the policy's qualitative behavior.

</details>


### [93] [Light-Weight Diffusion Multiplier and Uncertainty Quantification for Fourier Neural Operators](https://arxiv.org/abs/2508.00643)
*Albert Matveev,Sanmitra Ghosh,Aamal Hussain,James-Michael Leahy,Michalis Michaelides*

Main category: cs.LG

TL;DR: 本文介绍了具有不确定性量化的基于扩散的神经算子DINOZAUR，减少参数和内存占用，在PDE基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: FNOs存在过参数化导致的可扩展性问题，且缺乏原生的不确定性量化，现有后验UQ方法忽略几何归纳偏差。

Method: 受热核结构启发，用与维度无关的扩散乘数取代FNOs中的密集张量乘数，为时间参数定义先验，将DINOZAUR作为贝叶斯神经算子。

Result: 在多个PDE基准测试中取得有竞争力或更优的性能，能提供有效的不确定性量化。

Conclusion: DINOZAUR在减少参数和内存占用的同时，不影响预测性能，还能提供不确定性量化。

Abstract: Operator learning is a powerful paradigm for solving partial differential
equations, with Fourier Neural Operators serving as a widely adopted
foundation. However, FNOs face significant scalability challenges due to
overparameterization and offer no native uncertainty quantification -- a key
requirement for reliable scientific and engineering applications. Instead,
neural operators rely on post hoc UQ methods that ignore geometric inductive
biases. In this work, we introduce DINOZAUR: a diffusion-based neural operator
parametrization with uncertainty quantification. Inspired by the structure of
the heat kernel, DINOZAUR replaces the dense tensor multiplier in FNOs with a
dimensionality-independent diffusion multiplier that has a single learnable
time parameter per channel, drastically reducing parameter count and memory
footprint without compromising predictive performance. By defining priors over
those time parameters, we cast DINOZAUR as a Bayesian neural operator to yield
spatially correlated outputs and calibrated uncertainty estimates. Our method
achieves competitive or superior performance across several PDE benchmarks
while providing efficient uncertainty quantification.

</details>


### [94] [TrajSurv: Learning Continuous Latent Trajectories from Electronic Health Records for Trustworthy Survival Prediction](https://arxiv.org/abs/2508.00657)
*Sihang Zeng,Lucas Jing Liu,Jun Wen,Meliha Yetisgen,Ruth Etzioni,Gang Luo*

Main category: cs.LG

TL;DR: 提出TrajSurv模型用于可靠生存预测，在两个医疗数据集上展现优势。


<details>
  <summary>Details</summary>
Motivation: 可靠生存预测对临床决策重要，纵向电子健康记录有预测潜力，但难以准确建模患者临床进展并关联生存结果。

Method: 开发TrajSurv模型，用神经控制微分方程提取连续时间潜在状态，通过时间感知对比学习使潜在状态空间与患者状态空间对齐，采用两步分治解释过程关联临床进展和生存结果。

Result: 在MIMIC - III和eICU两个真实医疗数据集上，TrajSurv比现有深度学习方法有有竞争力的准确性和更高透明度。

Conclusion: TrajSurv模型在可靠生存预测上表现良好，能准确建模患者临床进展并关联生存结果。

Abstract: Trustworthy survival prediction is essential for clinical decision making.
Longitudinal electronic health records (EHRs) provide a uniquely powerful
opportunity for the prediction. However, it is challenging to accurately model
the continuous clinical progression of patients underlying the irregularly
sampled clinical features and to transparently link the progression to survival
outcomes. To address these challenges, we develop TrajSurv, a model that learns
continuous latent trajectories from longitudinal EHR data for trustworthy
survival prediction. TrajSurv employs a neural controlled differential equation
(NCDE) to extract continuous-time latent states from the irregularly sampled
data, forming continuous latent trajectories. To ensure the latent trajectories
reflect the clinical progression, TrajSurv aligns the latent state space with
patient state space through a time-aware contrastive learning approach. To
transparently link clinical progression to the survival outcome, TrajSurv uses
latent trajectories in a two-step divide-and-conquer interpretation process.
First, it explains how the changes in clinical features translate into the
latent trajectory's evolution using a learned vector field. Second, it clusters
these latent trajectories to identify key clinical progression patterns
associated with different survival outcomes. Evaluations on two real-world
medical datasets, MIMIC-III and eICU, show TrajSurv's competitive accuracy and
superior transparency over existing deep learning methods.

</details>


### [95] [DP-DGAD: A Generalist Dynamic Graph Anomaly Detector with Dynamic Prototypes](https://arxiv.org/abs/2508.00664)
*Jialun Zheng,Jie Liu,Jiannong Cao,Xiao Wang,Hanchen Yang,Yankai Chen,Philip S. Yu*

Main category: cs.LG

TL;DR: 提出DP - DGAD模型用于动态图异常检测，在十个真实数据集上达SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有通用图异常检测模型在动态图中难以捕捉演化异常，新领域不断出现且缺乏标注数据，需捕捉特定和通用异常模式。

Method: 提取动态原型存入内存缓冲，选择性更新缓冲，用异常评分器比较数据与原型，采用基于置信度的伪标签进行自监督适应。

Result: 在十个来自不同领域的真实数据集上取得了最先进的性能。

Conclusion: DP - DGAD模型能有效进行跨领域动态图异常检测。

Abstract: Dynamic graph anomaly detection (DGAD) is essential for identifying anomalies
in evolving graphs across domains such as finance, traffic, and social
networks. Recently, generalist graph anomaly detection (GAD) models have shown
promising results. They are pretrained on multiple source datasets and
generalize across domains. While effective on static graphs, they struggle to
capture evolving anomalies in dynamic graphs. Moreover, the continuous
emergence of new domains and the lack of labeled data further challenge
generalist DGAD. Effective cross-domain DGAD requires both domain-specific and
domain-agnostic anomalous patterns. Importantly, these patterns evolve
temporally within and across domains. Building on these insights, we propose a
DGAD model with Dynamic Prototypes (DP) to capture evolving domain-specific and
domain-agnostic patterns. Firstly, DP-DGAD extracts dynamic prototypes, i.e.,
evolving representations of normal and anomalous patterns, from temporal
ego-graphs and stores them in a memory buffer. The buffer is selectively
updated to retain general, domain-agnostic patterns while incorporating new
domain-specific ones. Then, an anomaly scorer compares incoming data with
dynamic prototypes to flag both general and domain-specific anomalies. Finally,
DP-DGAD employs confidence-based pseudo-labeling for effective self-supervised
adaptation in target domains. Extensive experiments demonstrate
state-of-the-art performance across ten real-world datasets from different
domains.

</details>


### [96] [Wind Power Scenario Generation based on the Generalized Dynamic Factor Model and Generative Adversarial Network](https://arxiv.org/abs/2508.00692)
*Young-ho Cho,Hao Zhu,Duehee Lee,Ross Baldick*

Main category: cs.LG

TL;DR: 本文结合GDFM和GAN合成分布式风电场长期风电场景，数值测试显示其性能优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 为进行资源充足性研究，需合成分布式风电场的长期风电场景。

Method: 结合GDFM和GAN，用GAN提供从观测数据中提取含时间信息动态因子的滤波器，再应用于GDFM。

Result: 结合GDFM和GAN在合成澳大利亚风电场景上性能优于其他方法，能更好实现实际风电的统计特征。

Conclusion: 结合GDFM和GAN合成风电场景是一种有效的方法。

Abstract: For conducting resource adequacy studies, we synthesize multiple long-term
wind power scenarios of distributed wind farms simultaneously by using the
spatio-temporal features: spatial and temporal correlation, waveforms, marginal
and ramp rates distributions of waveform, power spectral densities, and
statistical characteristics. Generating the spatial correlation in scenarios
requires the design of common factors for neighboring wind farms and
antithetical factors for distant wind farms. The generalized dynamic factor
model (GDFM) can extract the common factors through cross spectral density
analysis, but it cannot closely imitate waveforms. The GAN can synthesize
plausible samples representing the temporal correlation by verifying samples
through a fake sample discriminator. To combine the advantages of GDFM and GAN,
we use the GAN to provide a filter that extracts dynamic factors with temporal
information from the observation data, and we then apply this filter in the
GDFM to represent both spatial and frequency correlations of plausible
waveforms. Numerical tests on the combination of GDFM and GAN have demonstrated
performance improvements over competing alternatives in synthesizing wind power
scenarios from Australia, better realizing plausible statistical
characteristics of actual wind power compared to alternatives such as the GDFM
with a filter synthesized from distributions of actual dynamic filters and the
GAN with direct synthesis without dynamic factors.

</details>


### [97] [Classification of Psychiatry Clinical Notes by Diagnosis: A Deep Learning and Machine Learning Approach](https://arxiv.org/abs/2508.00695)
*Sergio Rubio-Martín,María Teresa García-Ordás,Antonio Serrano-García,Clara Margarita Franch-Pato,Arturo Crespo-Álvaro,José Alberto Benítez-Andrades*

Main category: cs.LG

TL;DR: 研究比较多种AI模型对焦虑和适应障碍临床笔记分类表现，用三种过采样策略和超参数调优，发现过采样影响小，超参数调优显著提升准确率，决策树等模型达96%准确率，强调超参数调优重要性。


<details>
  <summary>Details</summary>
Motivation: 对临床笔记进行特定诊断分类，特别是针对焦虑和适应障碍，研究不同AI模型和数据平衡方法的效果。

Method: 比较传统机器学习和深度学习模型对临床笔记分类，实施三种过采样策略，进行超参数调优。

Result: 过采样技术总体对模型性能影响小，SMOTE对基于BERT模型有积极效果，超参数优化显著提升准确率，决策树等模型达96%准确率。

Conclusion: 强调超参数调优对最大化模型性能的重要性，为心理健康AI辅助诊断工具研究提供见解。

Abstract: The classification of clinical notes into specific diagnostic categories is
critical in healthcare, especially for mental health conditions like Anxiety
and Adjustment Disorder. In this study, we compare the performance of various
Artificial Intelligence models, including both traditional Machine Learning
approaches (Random Forest, Support Vector Machine, K-nearest neighbors,
Decision Tree, and eXtreme Gradient Boost) and Deep Learning models (DistilBERT
and SciBERT), to classify clinical notes into these two diagnoses.
Additionally, we implemented three oversampling strategies: No Oversampling,
Random Oversampling, and Synthetic Minority Oversampling Technique (SMOTE), to
assess their impact on model performance. Hyperparameter tuning was also
applied to optimize model accuracy. Our results indicate that oversampling
techniques had minimal impact on model performance overall. The only exception
was SMOTE, which showed a positive effect specifically with BERT-based models.
However, hyperparameter optimization significantly improved accuracy across the
models, enhancing their ability to generalize and perform on the dataset. The
Decision Tree and eXtreme Gradient Boost models achieved the highest accuracy
among machine learning approaches, both reaching 96%, while the DistilBERT and
SciBERT models also attained 96% accuracy in the deep learning category. These
findings underscore the importance of hyperparameter tuning in maximizing model
performance. This study contributes to the ongoing research on AI-assisted
diagnostic tools in mental health by providing insights into the efficacy of
different model architectures and data balancing methods.

</details>


### [98] [Learning Network Dismantling without Handcrafted Inputs](https://arxiv.org/abs/2508.00706)
*Haozhe Tian,Pietro Ferraro,Robert Shorten,Mahdi Jalili,Homayoun Hamedmoghadam*

Main category: cs.LG

TL;DR: 引入注意机制和消息迭代配置文件，构建消息传递框架MIND解决网络拆解问题，优于现有方法且有泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有消息传递图神经网络依赖手工特征，增加计算成本并引入偏差。

Method: 引入注意机制和消息迭代配置文件，采用有效算法生成小型合成网络训练集，构建消息传递框架。

Result: MIND模型在未见过的大型真实网络上表现优于现有网络拆解方法。

Conclusion: 模型的高效性和泛化性可用于解决一系列复杂网络问题。

Abstract: The application of message-passing Graph Neural Networks has been a
breakthrough for important network science problems. However, the competitive
performance often relies on using handcrafted structural features as inputs,
which increases computational cost and introduces bias into the otherwise
purely data-driven network representations. Here, we eliminate the need for
handcrafted features by introducing an attention mechanism and utilizing
message-iteration profiles, in addition to an effective algorithmic approach to
generate a structurally diverse training set of small synthetic networks.
Thereby, we build an expressive message-passing framework and use it to
efficiently solve the NP-hard problem of Network Dismantling, virtually
equivalent to vital node identification, with significant real-world
applications. Trained solely on diversified synthetic networks, our proposed
model -- MIND: Message Iteration Network Dismantler -- generalizes to large,
unseen real networks with millions of nodes, outperforming state-of-the-art
network dismantling methods. Increased efficiency and generalizability of the
proposed model can be leveraged beyond dismantling in a range of complex
network problems.

</details>


### [99] [Efficient Solution and Learning of Robust Factored MDPs](https://arxiv.org/abs/2508.00707)
*Yannik Schnitzer,Alessandro Abate,David Parker*

Main category: cs.LG

TL;DR: 提出基于因子化状态空间表示解决和学习鲁棒马尔可夫决策过程（r - MDPs）的新方法，实验显示可提高样本效率。


<details>
  <summary>Details</summary>
Motivation: 学习r - MDPs需大量样本交互，希望找到更高效方法。

Method: 基于因子化状态空间表示解决和学习r - MDPs，将非凸优化问题转化为易处理的线性规划，并提出直接学习因子化模型表示的方法。

Result: 利用因子化结构可提高样本效率，产生比现有方法更有效的鲁棒策略和更严格的性能保证。

Conclusion: 所提方法在解决和学习r - MDPs上具有优势，能提升样本效率和策略性能。

Abstract: Robust Markov decision processes (r-MDPs) extend MDPs by explicitly modelling
epistemic uncertainty about transition dynamics. Learning r-MDPs from
interactions with an unknown environment enables the synthesis of robust
policies with provable (PAC) guarantees on performance, but this can require a
large number of sample interactions. We propose novel methods for solving and
learning r-MDPs based on factored state-space representations that leverage the
independence between model uncertainty across system components. Although
policy synthesis for factored r-MDPs leads to hard, non-convex optimisation
problems, we show how to reformulate these into tractable linear programs.
Building on these, we also propose methods to learn factored model
representations directly. Our experimental results show that exploiting
factored structure can yield dimensional gains in sample efficiency, producing
more effective robust policies with tighter performance guarantees than
state-of-the-art methods.

</details>


### [100] [JSON-Bag: A generic game trajectory representation](https://arxiv.org/abs/2508.00712)
*Dien Nguyen,Diego Perez-Liebana,Simon Lucas*

Main category: cs.LG

TL;DR: 提出JSON Bag-of-Tokens模型表示游戏轨迹，用Jensen-Shannon距离作度量，在多游戏多任务中评估，表现优于基线，还展示特征提取能力及相关性。


<details>
  <summary>Details</summary>
Motivation: 找到通用方法表示游戏轨迹并进行分类任务评估。

Method: 引入JSON-Bag模型，使用JSD作距离度量，用P-NNS评估，还采用随机森林处理任务。

Result: 在多数任务中优于基线，N-shot分类中样本高效，自动特征提取提高准确率，JSD与策略距离高度相关。

Conclusion: JSON-Bag模型在游戏轨迹分类任务中有良好表现和应用价值。

Abstract: We introduce JSON Bag-of-Tokens model (JSON-Bag) as a method to generically
represent game trajectories by tokenizing their JSON descriptions and apply
Jensen-Shannon distance (JSD) as distance metric for them. Using a
prototype-based nearest-neighbor search (P-NNS), we evaluate the validity of
JSON-Bag with JSD on six tabletop games -- \textit{7 Wonders},
\textit{Dominion}, \textit{Sea Salt and Paper}, \textit{Can't Stop},
\textit{Connect4}, \textit{Dots and boxes} -- each over three game trajectory
classification tasks: classifying the playing agents, game parameters, or game
seeds that were used to generate the trajectories.
  Our approach outperforms a baseline using hand-crafted features in the
majority of tasks. Evaluating on N-shot classification suggests using JSON-Bag
prototype to represent game trajectory classes is also sample efficient.
Additionally, we demonstrate JSON-Bag ability for automatic feature extraction
by treating tokens as individual features to be used in Random Forest to solve
the tasks above, which significantly improves accuracy on underperforming
tasks. Finally, we show that, across all six games, the JSD between JSON-Bag
prototypes of agent classes highly correlates with the distances between
agents' policies.

</details>


### [101] [Nested Graph Pseudo-Label Refinement for Noisy Label Domain Adaptation Learning](https://arxiv.org/abs/2508.00716)
*Yingxu Wang,Mengzhu Wang,Zhichao Huang,Suyu Liu*

Main category: cs.LG

TL;DR: 现有图域自适应方法假设源标签干净，现实中标签噪声普遍影响性能。提出 NeGPR 框架，通过预训练双分支、嵌套细化机制和噪声感知正则化策略提升抗噪性，实验显示其在标签噪声下表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图域自适应方法大多假设源标签干净，而现实中标签噪声普遍，严重影响特征对齐和适应性能，需解决标签噪声问题。

Method: 提出 NeGPR 框架，先预训练语义和拓扑双分支减少噪声监督影响，采用嵌套细化机制进行跨域学习，结合噪声感知正则化策略减轻伪标签噪声影响。

Result: 在基准数据集上的大量实验表明，NeGPR 在严重标签噪声下始终优于现有方法，准确率最高提升 12.7%。

Conclusion: NeGPR 框架有效解决了图域自适应中的标签噪声问题，提升了适应过程的鲁棒性。

Abstract: Graph Domain Adaptation (GDA) facilitates knowledge transfer from labeled
source graphs to unlabeled target graphs by learning domain-invariant
representations, which is essential in applications such as molecular property
prediction and social network analysis. However, most existing GDA methods rely
on the assumption of clean source labels, which rarely holds in real-world
scenarios where annotation noise is pervasive. This label noise severely
impairs feature alignment and degrades adaptation performance under domain
shifts. To address this challenge, we propose Nested Graph Pseudo-Label
Refinement (NeGPR), a novel framework tailored for graph-level domain
adaptation with noisy labels. NeGPR first pretrains dual branches, i.e.,
semantic and topology branches, by enforcing neighborhood consistency in the
feature space, thereby reducing the influence of noisy supervision. To bridge
domain gaps, NeGPR employs a nested refinement mechanism in which one branch
selects high-confidence target samples to guide the adaptation of the other,
enabling progressive cross-domain learning. Furthermore, since pseudo-labels
may still contain noise and the pre-trained branches are already overfitted to
the noisy labels in the source domain, NeGPR incorporates a noise-aware
regularization strategy. This regularization is theoretically proven to
mitigate the adverse effects of pseudo-label noise, even under the presence of
source overfitting, thus enhancing the robustness of the adaptation process.
Extensive experiments on benchmark datasets demonstrate that NeGPR consistently
outperforms state-of-the-art methods under severe label noise, achieving gains
of up to 12.7% in accuracy.

</details>


### [102] [Democratizing Tabular Data Access with an Open$\unicode{x2013}$Source Synthetic$\unicode{x2013}$Data SDK](https://arxiv.org/abs/2508.00718)
*Ivona Krchova,Mariana Vargas Vieyra,Mario Scriminaci,Andrey Sidorenko*

Main category: cs.LG

TL;DR: 本文介绍MOSTLY AI合成数据软件开发工具包（SDK），该开源工具可合成高质量表格数据，功能强大且实用，能解决现实数据瓶颈。


<details>
  <summary>Details</summary>
Motivation: 机器学习依赖高质量数据，但隐私、专有利益和伦理问题限制了数据获取，合成数据是可行解决方案。

Method: 提出MOSTLY AI合成数据SDK，集成差分隐私保证、公平感知数据生成和自动质量保证等功能，利用TabularARGN自回归框架。

Result: SDK支持多种数据类型和复杂数据集，在速度和可用性上有显著提升，已作为云服务和本地软件部署并被快速采用。

Conclusion: SDK具有实用性，可解决现实数据瓶颈，促进数据民主化。

Abstract: Machine learning development critically depends on access to high-quality
data. However, increasing restrictions due to privacy, proprietary interests,
and ethical concerns have created significant barriers to data accessibility.
Synthetic data offers a viable solution by enabling safe, broad data usage
without compromising sensitive information. This paper presents the MOSTLY AI
Synthetic Data Software Development Kit (SDK), an open-source toolkit designed
specifically for synthesizing high-quality tabular data. The SDK integrates
robust features such as differential privacy guarantees, fairness-aware data
generation, and automated quality assurance into a flexible and accessible
Python interface. Leveraging the TabularARGN autoregressive framework, the SDK
supports diverse data types and complex multi-table and sequential datasets,
delivering competitive performance with notable improvements in speed and
usability. Currently deployed both as a cloud service and locally installable
software, the SDK has seen rapid adoption, highlighting its practicality in
addressing real-world data bottlenecks and promoting widespread data
democratization.

</details>


### [103] [Adaptive Machine Learning-Driven Multi-Fidelity Stratified Sampling for Failure Analysis of Nonlinear Stochastic Systems](https://arxiv.org/abs/2508.00734)
*Liuyun Xu,Seymour M. J. Spence*

Main category: cs.LG

TL;DR: 提出多保真分层抽样方案结合自适应机器学习元模型，用于高效传播不确定性和估计小失效概率，应用于高层建筑显示能准确估计概率曲线并节省计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有随机模拟中的方差缩减技术在复杂非线性有限元建模环境中估计小失效概率时需大量模型评估，计算挑战大。

Method: 引入多保真分层抽样方案，用分层抽样生成的高保真数据集训练深度学习元模型作为低保真模型，提出自适应训练方案平衡质量与计算需求，用多保真蒙特卡罗框架结合高低保真结果得到无偏估计，用全概率定理计算总失效概率。

Result: 应用于高层建筑，能准确估计非线性响应的超越概率曲线。

Conclusion: 该方案能准确估计概率曲线，相比单保真方差缩减方法可显著节省计算成本。

Abstract: Existing variance reduction techniques used in stochastic simulations for
rare event analysis still require a substantial number of model evaluations to
estimate small failure probabilities. In the context of complex, nonlinear
finite element modeling environments, this can become computationally
challenging-particularly for systems subjected to stochastic excitation. To
address this challenge, a multi-fidelity stratified sampling scheme with
adaptive machine learning metamodels is introduced for efficiently propagating
uncertainties and estimating small failure probabilities. In this approach, a
high-fidelity dataset generated through stratified sampling is used to train a
deep learning-based metamodel, which then serves as a cost-effective and highly
correlated low-fidelity model. An adaptive training scheme is proposed to
balance the trade-off between approximation quality and computational demand
associated with the development of the low-fidelity model. By integrating the
low-fidelity outputs with additional high-fidelity results, an unbiased
estimate of the strata-wise failure probabilities is obtained using a
multi-fidelity Monte Carlo framework. The overall probability of failure is
then computed using the total probability theorem. Application to a full-scale
high-rise steel building subjected to stochastic wind excitation demonstrates
that the proposed scheme can accurately estimate exceedance probability curves
for nonlinear responses of interest, while achieving significant computational
savings compared to single-fidelity variance reduction approaches.

</details>


### [104] [A Simple and Effective Method for Uncertainty Quantification and OOD Detection](https://arxiv.org/abs/2508.00754)
*Yaxin Ma,Benjamin Colburn,Jose C. Principe*

Main category: cs.LG

TL;DR: 提出基于特征空间密度的方法解决贝叶斯神经网络和深度集成方法在不确定性量化方面的计算和存储问题，实验显示该方法优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯神经网络和深度集成方法在不确定性量化中计算量大、存储需求高，需要更有效的方法。

Method: 利用核密度估计得到的信息势场近似训练集特征空间密度，通过与测试样本特征空间表示比较来检测分布偏移。

Result: 在2D合成数据集和OOD检测任务中，该方法表现优于基线模型。

Conclusion: 所提出的基于特征空间密度的方法能有效解决不确定性量化问题，且性能良好。

Abstract: Bayesian neural networks and deep ensemble methods have been proposed for
uncertainty quantification; however, they are computationally intensive and
require large storage. By utilizing a single deterministic model, we can solve
the above issue. We propose an effective method based on feature space density
to quantify uncertainty for distributional shifts and out-of-distribution (OOD)
detection. Specifically, we leverage the information potential field derived
from kernel density estimation to approximate the feature space density of the
training set. By comparing this density with the feature space representation
of test samples, we can effectively determine whether a distributional shift
has occurred. Experiments were conducted on a 2D synthetic dataset (Two Moons
and Three Spirals) as well as an OOD detection task (CIFAR-10 vs. SVHN). The
results demonstrate that our method outperforms baseline models.

</details>


### [105] [Diffusion-Scheduled Denoising Autoencoders for Anomaly Detection in Tabular Data](https://arxiv.org/abs/2508.00758)
*Timur Sattarov,Marco Schreyer,Damian Borth*

Main category: cs.LG

TL;DR: 提出DDAE框架用于表格数据异常检测，在57个数据集上评估，在半监督和无监督设置中表现良好，强调噪声策略重要性。


<details>
  <summary>Details</summary>
Motivation: 表格数据异常检测因特征交互复杂和异常样本稀缺具有挑战性，现有降噪自编码器和扩散模型存在局限性。

Method: 提出Diffusion - Scheduled Denoising Autoencoder (DDAE)框架，将基于扩散的噪声调度和对比学习集成到编码过程。

Result: 在半监督设置中表现出色，在无监督设置中取得有竞争力结果，相比基线模型，PR - AUC最多提高65% (9%)，ROC - AUC提高16% (6%)。

Conclusion: 在表格异常检测中，有原则的噪声策略很重要。

Abstract: Anomaly detection in tabular data remains challenging due to complex feature
interactions and the scarcity of anomalous examples. Denoising autoencoders
rely on fixed-magnitude noise, limiting adaptability to diverse data
distributions. Diffusion models introduce scheduled noise and iterative
denoising, but lack explicit reconstruction mappings. We propose the
Diffusion-Scheduled Denoising Autoencoder (DDAE), a framework that integrates
diffusion-based noise scheduling and contrastive learning into the encoding
process to improve anomaly detection. We evaluated DDAE on 57 datasets from
ADBench. Our method outperforms in semi-supervised settings and achieves
competitive results in unsupervised settings, improving PR-AUC by up to 65%
(9%) and ROC-AUC by 16% (6%) over state-of-the-art autoencoder (diffusion)
model baselines. We observed that higher noise levels benefit unsupervised
training, while lower noise with linear scheduling is optimal in
semi-supervised settings. These findings underscore the importance of
principled noise strategies in tabular anomaly detection.

</details>


### [106] [Evaluating Angle and Amplitude Encoding Strategies for Variational Quantum Machine Learning: their impact on model's accuracy](https://arxiv.org/abs/2508.00768)
*Antonio Tudisco,Andrea Marchesin,Maurizio Zamboni,Mariagrazia Graziano,Giovanna Turvani*

Main category: cs.LG

TL;DR: 本文围绕变分量子电路（VQC）展开，对比不同编码模型和旋转门对分类性能的影响，发现选择不同会显著影响性能且嵌入是VQC模型超参数。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习受关注，VQC是常用模型，研究不同编码模型和旋转门对其分类性能的影响。

Method: 考虑振幅和角度编码模型，在Wine和Diabetes两个数据集上训练不同模型，对比使用不同旋转门时模型的分类性能。

Result: 相同模型拓扑下，最佳和最差模型准确率差异在10% - 30%，最高达41%，编码中旋转门选择显著影响模型分类性能。

Conclusion: 嵌入是VQC模型的超参数。

Abstract: Recent advancements in Quantum Computing and Machine Learning have increased
attention to Quantum Machine Learning (QML), which aims to develop machine
learning models by exploiting the quantum computing paradigm. One of the widely
used models in this area is the Variational Quantum Circuit (VQC), a hybrid
model where the quantum circuit handles data inference while classical
optimization adjusts the parameters of the circuit. The quantum circuit
consists of an encoding layer, which loads data into the circuit, and a
template circuit, known as the ansatz, responsible for processing the data.
This work involves performing an analysis by considering both Amplitude- and
Angle-encoding models, and examining how the type of rotational gate applied
affects the classification performance of the model. This comparison is carried
out by training the different models on two datasets, Wine and Diabetes, and
evaluating their performance. The study demonstrates that, under identical
model topologies, the difference in accuracy between the best and worst models
ranges from 10% to 30%, with differences reaching up to 41%. Moreover, the
results highlight how the choice of rotational gates used in encoding can
significantly impact the model's classification performance. The findings
confirm that the embedding represents a hyperparameter for VQC models.

</details>


### [107] [Explainable AI and Machine Learning for Exam-based Student Evaluation: Causal and Predictive Analysis of Socio-academic and Economic Factors](https://arxiv.org/abs/2508.00785)
*Bushra Akter,Md Biplob Hosen,Sabbir Ahmed,Mehrin Anannya,Md. Farhad Hossain*

Main category: cs.LG

TL;DR: 研究分析社会学术与财务因素对学生CGPA的影响，构建模型预测并分类，开发网络应用提供个性化建议。


<details>
  <summary>Details</summary>
Motivation: 探究影响学生CGPA的因素，以制定优化策略。

Method: 文献回顾确定关键因素，构建因果图；开展在线调查收集数据；进行数据预处理；用因果分析验证变量关系；实施回归和分类模型；采用可解释AI技术。

Result: Ridge Regression预测精度高，Random Forest分类表现出色；可解释AI技术突出关键因素。

Conclusion: 开发的网络应用能为学生提供个性化见解，助其提升学业表现。

Abstract: Academic performance depends on a multivariable nexus of socio-academic and
financial factors. This study investigates these influences to develop
effective strategies for optimizing students' CGPA. To achieve this, we
reviewed various literature to identify key influencing factors and constructed
an initial hypothetical causal graph based on the findings. Additionally, an
online survey was conducted, where 1,050 students participated, providing
comprehensive data for analysis. Rigorous data preprocessing techniques,
including cleaning and visualization, ensured data quality before analysis.
Causal analysis validated the relationships among variables, offering deeper
insights into their direct and indirect effects on CGPA. Regression models were
implemented for CGPA prediction, while classification models categorized
students based on performance levels. Ridge Regression demonstrated strong
predictive accuracy, achieving a Mean Absolute Error of 0.12 and a Mean Squared
Error of 0.023. Random Forest outperformed in classification, attaining an
F1-score near perfection and an accuracy of 98.68%. Explainable AI techniques
such as SHAP, LIME, and Interpret enhanced model interpretability, highlighting
critical factors such as study hours, scholarships, parental education, and
prior academic performance. The study culminated in the development of a
web-based application that provides students with personalized insights,
allowing them to predict academic performance, identify areas for improvement,
and make informed decisions to enhance their outcomes.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [108] [Reinitializing weights vs units for maintaining plasticity in neural networks](https://arxiv.org/abs/2508.00212)
*J. Fernando Hernandez-Garcia,Shibhansh Dohare,Jun Luo,Rich S. Sutton*

Main category: cs.NE

TL;DR: 本文比较重新初始化单元和权重两种方案，提出选择性权重重新初始化算法，实验表明重新初始化权重在更多场景下能有效保持可塑性。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络在非平稳数据上长时间训练时丧失可塑性的问题，设计持续学习系统。

Method: 比较重新初始化单元和权重两种方案，提出选择性权重重新初始化算法，并与持续反向传播和ReDo算法对比。

Result: 发现重新初始化权重在网络单元少和有层归一化两种场景下比重新初始化单元更有效；网络规模足够且无层归一化时二者效果相同。

Conclusion: 重新初始化权重在更多场景下能保持可塑性。

Abstract: Loss of plasticity is a phenomenon in which a neural network loses its
ability to learn when trained for an extended time on non-stationary data. It
is a crucial problem to overcome when designing systems that learn continually.
An effective technique for preventing loss of plasticity is reinitializing
parts of the network. In this paper, we compare two different reinitialization
schemes: reinitializing units vs reinitializing weights. We propose a new
algorithm, which we name \textit{selective weight reinitialization}, for
reinitializing the least useful weights in a network. We compare our algorithm
to continual backpropagation and ReDo, two previously proposed algorithms that
reinitialize units in the network. Through our experiments in continual
supervised learning problems, we identify two settings when reinitializing
weights is more effective at maintaining plasticity than reinitializing units:
(1) when the network has a small number of units and (2) when the network
includes layer normalization. Conversely, reinitializing weights and units are
equally effective at maintaining plasticity when the network is of sufficient
size and does not include layer normalization. We found that reinitializing
weights maintains plasticity in a wider variety of settings than reinitializing
units.

</details>


### [109] [Sequential, Parallel and Consecutive Hybrid Evolutionary-Swarm Optimization Metaheuristics](https://arxiv.org/abs/2508.00229)
*Piotr Urbańczyk,Aleksandra Urbańczyk,Magdalena Król,Leszek Rutkowski,Marek Kisiel-Dorohinicki*

Main category: cs.NE

TL;DR: 本文对比混合进化 - 群体元启发式算法与标准形式，在多维度基准函数上测试，结果显示混合方法更优，还介绍了新的连续混合PSO - GA算法。


<details>
  <summary>Details</summary>
Motivation: 探索结合PSO和GA特点的混合进化 - 群体元启发式算法，并引入新的连续混合PSO - GA算法。

Method: 将PSO和GA以顺序、并行和连续方式结合，在多维度的一组基准函数上测试算法；通过修改GA变异算子继承速度和个人最优信息来确保PSO和GA步骤间的连续性。

Result: 混合方法在收敛性和一致性上表现更优，尤其在高维搜索空间。

Conclusion: 混合进化 - 群体元启发式算法有优势，新的连续混合PSO - GA算法可确保PSO和GA步骤间的连续性。

Abstract: The goal of this paper is twofold. First, it explores hybrid
evolutionary-swarm metaheuristics that combine the features of PSO and GA in a
sequential, parallel and consecutive manner in comparison with their standard
basic form: Genetic Algorithm and Particle Swarm Optimization. The algorithms
were tested on a set of benchmark functions, including Ackley, Griewank, Levy,
Michalewicz, Rastrigin, Schwefel, and Shifted Rotated Weierstrass, across
multiple dimensions. The experimental results demonstrate that the hybrid
approaches achieve superior convergence and consistency, especially in
higher-dimensional search spaces. The second goal of this paper is to introduce
a novel consecutive hybrid PSO-GA evolutionary algorithm that ensures
continuity between PSO and GA steps through explicit information transfer
mechanisms, specifically by modifying GA's variation operators to inherit
velocity and personal best information.

</details>


### [110] [Evolutionary Generative Optimization: Towards Fully Data-Driven Evolutionary Optimization via Generative Learning](https://arxiv.org/abs/2508.00380)
*Kebin Sun,Tao Jiang,Ran Cheng,Yaochu Jin,Kay Chen Tan*

Main category: cs.NE

TL;DR: 提出EvoGO框架解决现有数据驱动进化算法依赖手工启发式方法的问题，实验显示其收敛快且性能优。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动进化算法大多依赖手工启发式方法，限制了通用性和自动化，需改进。

Method: 提出EvoGO框架，分为数据准备、模型训练和种群生成三阶段。数据准备构建成对数据集；模型训练用定制生成模型；种群生成用可扩展并行的生成机制。

Result: 在数值基准、经典控制问题和高维机器人任务实验中，EvoGO仅10代就收敛，显著优于多种优化方法。

Conclusion: EvoGO是有效的完全数据驱动框架，源代码将公开。

Abstract: Recent advances in data-driven evolutionary algorithms (EAs) have
demonstrated the potential of leveraging data to improve optimization accuracy
and adaptability. Nevertheless, most existing approaches remain dependent on
handcrafted heuristics, which limits their generality and automation. To
address this challenge, we propose Evolutionary Generative Optimization
(EvoGO), a fully data-driven framework empowered by generative learning. EvoGO
streamlines the evolutionary optimization process into three stages: data
preparation, model training, and population generation. The data preparation
stage constructs a pairwise dataset to enrich training diversity without
incurring additional evaluation costs. During model training, a tailored
generative model learns to transform inferior solutions into superior ones. In
the population generation stage, EvoGO replaces traditional reproduction
operators with a scalable and parallelizable generative mechanism. Extensive
experiments on numerical benchmarks, classical control problems, and
high-dimensional robotic tasks demonstrate that EvoGO consistently converges
within merely 10 generations and significantly outperforms a wide spectrum of
optimization approaches, including traditional EAs, Bayesian optimization, and
reinforcement learning based methods. Source code will be made publicly
available.

</details>


### [111] [STF: Shallow-Level Temporal Feedback to Enhance Spiking Transformers](https://arxiv.org/abs/2508.00387)
*Zeqi Zheng,Zizheng Zhu,Yingchao Yu,Yanchen Huang,Changze Lv,Junfeng Tang,Zhaofei Yu,Yaochu Jin*

Main category: cs.NE

TL;DR: 提出轻量级模块STF用于Transformer - 基SNN编码层，提升性能并增强鲁棒性，代码待接受后发布。


<details>
  <summary>Details</summary>
Motivation: 现有引入深层反馈环缩小SNN与ANN性能差距的方法存在特征变换成本高、参数开销大等问题。

Method: 提出由TSPE和TF组成的浅级时间反馈模块STF，用于编码层。

Result: STF在不同Transformer - 基SNN骨干网络和静态数据集上提升性能，增强了脉冲模式多样性，在对抗鲁棒性和时间敏感性评估中表现优于直接编码及其变体。

Conclusion: STF有潜力成为静态场景下新的脉冲编码方案。

Abstract: Transformer-based Spiking Neural Networks (SNNs) suffer from a great
performance gap compared to floating-point Artificial Neural Networks (ANNs)
due to the binary nature of spike trains. Recent efforts have introduced
deep-level feedback loops to transmit high-level semantic information to narrow
this gap. However, these designs often span multiple deep layers, resulting in
costly feature transformations, higher parameter overhead, increased energy
consumption, and longer inference latency. To address this issue, we propose
Shallow-level Temporal Feedback (STF), a lightweight plug-and-play module for
the encoding layer, which consists of Temporal-Spatial Position Embedding
(TSPE) and Temporal Feedback (TF).Extensive experiments show that STF
consistently improves performance across various Transformer-based SNN
backbones on static datasets, including CIFAR-10, CIFAR-100, and ImageNet-1K,
under different spike timestep settings. Further analysis reveals that STF
enhances the diversity of the spike patterns, which is key to performance gain.
Moreover, evaluations on adversarial robustness and temporal sensitivity
confirm that STF outperforms direct coding and its variants, highlighting its
potential as a new spike encoding scheme for static scenarios. Our code will be
released upon acceptance.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [112] [DGEMM without FP64 Arithmetic -- using FP64 Emulation and FP8 Tensor Cores with Ozaki Scheme](https://arxiv.org/abs/2508.00441)
*Daichi Mukunoki*

Main category: cs.PF

TL;DR: 本文考虑最新AI硬件，重新审视Ozaki方案中低精度浮点运算的使用，评估了在GPU上使用FP8 Tensor Cores和FP64仿真进行DGEMM的性能。


<details>
  <summary>Details</summary>
Motivation: 现有用于AI计算的处理器难以直接用于科学计算，且最新硬件倾向于提升FP8等低精度浮点运算性能，需重新审视Ozaki方案。

Method: 考虑使用FP6和FP8 Tensor Cores，对不支持或FP64运算慢的处理器采用基于整数运算的FP64仿真，还研究了新的分块策略。

Result: 通过在Blackwell架构GPU上评估使用FP8 Tensor Cores和FP64仿真进行DGEMM的性能，证明了方法的有效性。

Conclusion: 所提出的考虑最新硬件利用低精度浮点运算及相关策略在DGEMM性能评估中有效。

Abstract: Since AI computations require low-precision matrix multiplications,
processors with enhanced performance for these operations are increasing along
with the growing demand for AI computations. However, it is difficult to use
these operations directly for scientific computations. The Ozaki scheme, an
accurate matrix multiplication method proposed by Ozaki et al. in 2012, enables
FP64 matrix multiplication (DGEMM) using low-precision floating-point
operations such as FP16. The method was subsequently extended to utilize
integer arithmetic. The use of integer operations reduces computational cost
compared to the floating-point based approach. It has also demonstrated higher
performance than hardware FP64 operations on GPUs with fast INT8 Tensor Cores
for AI workloads. However, the latest hardware tends to enhance low-precision
floating-point operation performance such as FP8 instead of INT8. This study
revisits the utilization of low-precision floating-point operations in the
Ozaki scheme, considering the latest AI hardware. Specifically, we consider the
use of FP6 and FP8 Tensor Cores. Moreover, for processors that support very
slow FP64 operations or do not support them at all, we consider the use of the
FP64 emulation based on integer arithmetic. We also examine a new blocking
strategy. We demonstrate the effectiveness of these methods by evaluating the
performance of DGEMM using FP8 Tensor Cores and FP64 emulation on a Blackwell
architecture GPU.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [113] [Git Context Controller: Manage the Context of LLM-based Agents like Git](https://arxiv.org/abs/2508.00031)
*Junde Wu*

Main category: cs.SE

TL;DR: 提出Git-Context-Controller (GCC)上下文管理框架，提升大语言模型代理处理长流程任务能力，实验效果佳。


<details>
  <summary>Details</summary>
Motivation: 大语言模型代理在长流程工作流中，上下文管理成关键瓶颈。

Method: 引入受软件版本控制系统启发的GCC框架，将上下文提升为类似Git的版本化内存层次结构，结构化代理内存。

Result: 配备GCC的代理在SWE - Bench - Lite基准测试中达最优性能，解决48.00软件bug；在自我复制案例中，任务解决率达40.7%。

Conclusion: GCC框架能有效提升代理管理长期目标、隔离实验及跨会话和代理恢复或交接内存的能力。

Abstract: Large language model (LLM) based agents have shown impressive capabilities by
interleaving internal reasoning with external tool use. However, as these
agents are deployed in long-horizon workflows, such as coding for a big,
long-term project, context management becomes a critical bottleneck. We
introduce Git-Context-Controller (GCC), a structured context management
framework inspired by software version control systems. GCC elevates context as
versioned memory hierarchy like Git. It structures agent memory as a persistent
file system with explicit operations: COMMIT, BRANCH, MERGE, and CONTEXT,
enabling milestone-based checkpointing, exploration of alternative plans, and
structured reflection. Our approach empowers agents to manage long-term goals,
isolate architectural experiments, and recover or hand off memory across
sessions and agents. Empirically, agents equipped with GCC achieve
state-of-the-art performance on the SWE-Bench-Lite benchmark, resolving 48.00
of software bugs, outperforming 26 competitive systems. In a self-replication
case study, a GCC-augmented agent builds a new CLI agent from scratch,
achieving 40.7 task resolution, compared to only 11.7 without GCC. The code is
released at: https://github.com/theworldofagents/GCC

</details>


### [114] [GPT-4.1 Sets the Standard in Automated Experiment Design Using Novel Python Libraries](https://arxiv.org/abs/2508.00033)
*Nuno Fachada,Daniel Fernandes,Carlos M. Fernandes,Bruno D. Ferreira-Saraiva,João P. Matos-Carvalho*

Main category: cs.SE

TL;DR: 研究对LLMs生成Python代码能力进行基准测试，发现仅少数模型能稳定生成正确代码，GPT - 4.1表现突出，同时指出当前LLMs在科学自动化中的局限。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在解释和使用陌生Python API进行复杂计算实验的能力缺乏明确评估，需对其进行基准测试。

Method: 对选定的LLMs在两个挑战性场景下生成Python代码进行测试，使用结构化零样本提示，从功能正确性、提示合规性进行定量评估，分析执行失败的错误进行定性评估。

Result: 仅一小部分模型能稳定生成正确可执行代码，GPT - 4.1在两项任务中均始终成功。该方法还能发现第三方库的不足。

Conclusion: 当前LLMs用于端到端科学自动化存在局限，需要精心设计提示、完善库文档以及提升语言模型能力。

Abstract: Large Language Models (LLMs) have advanced rapidly as tools for automating
code generation in scientific research, yet their ability to interpret and use
unfamiliar Python APIs for complex computational experiments remains poorly
characterized. This study systematically benchmarks a selection of
state-of-the-art LLMs in generating functional Python code for two increasingly
challenging scenarios: conversational data analysis with the \textit{ParShift}
library, and synthetic data generation and clustering using \textit{pyclugen}
and \textit{scikit-learn}. Both experiments use structured, zero-shot prompts
specifying detailed requirements but omitting in-context examples. Model
outputs are evaluated quantitatively for functional correctness and prompt
compliance over multiple runs, and qualitatively by analyzing the errors
produced when code execution fails. Results show that only a small subset of
models consistently generate correct, executable code, with GPT-4.1 standing
out as the only model to always succeed in both tasks. In addition to
benchmarking LLM performance, this approach helps identify shortcomings in
third-party libraries, such as unclear documentation or obscure implementation
bugs. Overall, these findings highlight current limitations of LLMs for
end-to-end scientific automation and emphasize the need for careful prompt
design, comprehensive library documentation, and continued advances in language
model capabilities.

</details>


### [115] [Machine Learning Pipeline for Software Engineering: A Systematic Literature Review](https://arxiv.org/abs/2508.00045)
*Samah Kansab*

Main category: cs.SE

TL;DR: 随着软件开发复杂度增加，传统软件工程方法面临挑战，机器学习成为解决方案。本文通过系统文献综述研究软件工程中的机器学习管道，给出最佳实践、挑战和差距，强调设计良好的管道对解决软件工程挑战的重要性。


<details>
  <summary>Details</summary>
Motivation: 软件开发实践发展使传统软件工程方法难适应复杂度增长，需借助机器学习解决质量和效率问题，而机器学习在软件工程中的效果依赖其管道健壮性，因此开展对相关管道的研究。

Method: 进行系统文献综述，研究用于软件工程的机器学习管道。

Result: 发现如SMOTE数据平衡和基于SZZ算法特征选择等预处理方法可提高模型可靠性；集成方法在各任务中性能出色，简单模型在效率和可解释性方面有价值；常见评估指标有AUC、F1 - score等，新指标在特定应用中出现；广泛使用自举等验证技术确保模型稳定性和泛化性。

Conclusion: 强调设计良好的机器学习管道对解决软件工程挑战的重要性，为研究人员和从业者提供见解，为促进机器学习在复杂开发环境中的应用和创新奠定基础。

Abstract: The rapid advancement of software development practices has introduced
challenges in ensuring quality and efficiency across the software engineering
(SE) lifecycle. As SE systems grow in complexity, traditional approaches often
fail to scale, resulting in longer debugging times, inefficient defect
detection, and resource-heavy development cycles. Machine Learning (ML) has
emerged as a key solution, enabling automation in tasks such as defect
prediction, code review, and release quality estimation. However, the
effectiveness of ML in SE depends on the robustness of its pipeline, including
data collection, preprocessing, feature engineering, algorithm selection,
validation, and evaluation.
  This systematic literature review (SLR) examines state-of-the-art ML
pipelines designed for SE, consolidating best practices, challenges, and gaps.
Our findings show that robust preprocessing, such as SMOTE for data balancing
and SZZ-based algorithms for feature selection, improves model reliability.
Ensemble methods like Random Forest and Gradient Boosting dominate performance
across tasks, while simpler models such as Naive Bayes remain valuable for
efficiency and interpretability. Evaluation metrics including AUC, F1-score,
and precision are most common, with new metrics like Best Arithmetic Mean (BAM)
emerging in niche applications. Validation techniques such as bootstrapping are
widely used to ensure model stability and generalizability.
  This SLR highlights the importance of well-designed ML pipelines for
addressing SE challenges and provides actionable insights for researchers and
practitioners seeking to optimize software quality and efficiency. By
identifying gaps and trends, this study sets a foundation for advancing ML
adoption and fostering innovation in increasingly complex development
environments.

</details>


### [116] [A Survey on Code Generation with LLM-based Agents](https://arxiv.org/abs/2508.00083)
*Yihong Dong,Xue Jiang,Jiaru Qian,Tian Wang,Kechi Zhang,Zhi Jin,Ge Li*

Main category: cs.SE

TL;DR: 本文对基于大语言模型的代码生成智能体进行系统调研，梳理技术发展轨迹、核心技术，介绍其在软件开发全生命周期的应用、评估基准和工具，分析挑战并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 代码生成智能体有自主性、任务范围广和工程实用性增强等特点，该领域发展迅速且研究众多，有显著应用潜力，需系统调研。

Method: 追溯技术发展轨迹，对核心技术进行分类，介绍全生命周期应用、评估基准和工具，分析主要挑战。

Result: 完成对基于大语言模型的代码生成智能体领域的系统调研。

Conclusion: 提出该领域未来的几个基础性、长期研究方向。

Abstract: Code generation agents powered by large language models (LLMs) are
revolutionizing the software development paradigm. Distinct from previous code
generation techniques, code generation agents are characterized by three core
features. 1) Autonomy: the ability to independently manage the entire workflow,
from task decomposition to coding and debugging. 2) Expanded task scope:
capabilities that extend beyond generating code snippets to encompass the full
software development lifecycle (SDLC). 3) Enhancement of engineering
practicality: a shift in research emphasis from algorithmic innovation toward
practical engineering challenges, such as system reliability, process
management, and tool integration. This domain has recently witnessed rapid
development and an explosion in research, demonstrating significant application
potential. This paper presents a systematic survey of the field of LLM-based
code generation agents. We trace the technology's developmental trajectory from
its inception and systematically categorize its core techniques, including both
single-agent and multi-agent architectures. Furthermore, this survey details
the applications of LLM-based agents across the full SDLC, summarizes
mainstream evaluation benchmarks and metrics, and catalogs representative
tools. Finally, by analyzing the primary challenges, we identify and propose
several foundational, long-term research directions for the future work of the
field.

</details>


### [117] [How Quantization Impacts Privacy Risk on LLMs for Code?](https://arxiv.org/abs/2508.00128)
*Md Nazmul Haque,Hua Yang,Zhou Yang,Bowen Xu*

Main category: cs.SE

TL;DR: 研究量化技术对代码大语言模型任务性能和隐私风险的影响，发现量化可降低隐私风险，存在任务性能与隐私风险的权衡。


<details>
  <summary>Details</summary>
Motivation: 代码大语言模型存在隐私问题，量化模型是否影响隐私信息留存和暴露尚不明确，需研究其对任务性能和隐私风险的影响。

Method: 对Pythia、CodeGen和GPTNeo三个代表性模型家族实施静态和动态量化技术。

Result: 量化显著降低隐私风险，任务性能和隐私风险呈正相关，量化大模型可能比全精度小模型有更好平衡，结果在不同架构、模型大小和成员推理方法中具有普遍性。

Conclusion: 研究结果为部署压缩代码大语言模型时保护隐私提供实际指导。

Abstract: Large language models for code (LLMs4Code) rely heavily on massive training
data, including sensitive data, such as cloud service credentials of the
projects and personal identifiable information of the developers, raising
serious privacy concerns. Membership inference (MI) has recently emerged as an
effective tool for assessing privacy risk by identifying whether specific data
belong to a model's training set. In parallel, model compression techniques,
especially quantization, have gained traction for reducing computational costs
and enabling the deployment of large models. However, while quantized models
still retain knowledge learned from the original training data, it remains
unclear whether quantization affects their ability to retain and expose privacy
information. Answering this question is of great importance to understanding
privacy risks in real-world deployments. In this work, we conduct the first
empirical study on how quantization influences task performance and privacy
risk simultaneously in LLMs4Code. To do this, we implement widely used
quantization techniques (static and dynamic) to three representative model
families, namely Pythia, CodeGen, and GPTNeo. Our results demonstrate that
quantization has a significant impact on reducing the privacy risk relative to
the original model. We also uncover a positive correlation between task
performance and privacy risk, indicating an underlying tradeoff. Moreover, we
reveal the possibility that quantizing larger models could yield better balance
than using full-precision small models. Finally, we demonstrate that these
findings generalize across different architectures, model sizes and MI methods,
offering practical guidance for safeguarding privacy when deploying compressed
LLMs4Code.

</details>


### [118] [Testing the Untestable? An Empirical Study on the Testing Process of LLM-Powered Software Systems](https://arxiv.org/abs/2508.00198)
*Cleyton Magalhaes,Italo Santos,Brody Stuart-Verner,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: 本文探讨大语言模型驱动系统在实际应用开发中的测试情况，结合学生报告分析得出测试策略及面临挑战，指出需调整传统验证方法。


<details>
  <summary>Details</summary>
Motivation: 此前研究较少关注集成大语言模型的完整系统在开发中的测试，故探索其在实际应用开发中的测试情况。

Method: 对99份学生构建和部署大语言模型应用的报告进行探索性案例研究，采用主题分析和结构化编码过程。

Result: 测试策略结合手动和自动方法评估系统逻辑和模型行为，常见实践有探索性测试等，面临集成失败等挑战。

Conclusion: 测试大语言模型驱动系统需调整传统验证方法，融合源代码级推理和行为感知评估，为测试软件系统生成组件提供实践依据。

Abstract: Background: Software systems powered by large language models are becoming a
routine part of everyday technologies, supporting applications across a wide
range of domains. In software engineering, many studies have focused on how
LLMs support tasks such as code generation, debugging, and documentation.
However, there has been limited focus on how full systems that integrate LLMs
are tested during development. Aims: This study explores how LLM-powered
systems are tested in the context of real-world application development.
Method: We conducted an exploratory case study using 99 individual reports
written by students who built and deployed LLM-powered applications as part of
a university course. Each report was independently analyzed using thematic
analysis, supported by a structured coding process. Results: Testing strategies
combined manual and automated methods to evaluate both system logic and model
behavior. Common practices included exploratory testing, unit testing, and
prompt iteration. Reported challenges included integration failures,
unpredictable outputs, prompt sensitivity, hallucinations, and uncertainty
about correctness. Conclusions: Testing LLM-powered systems required
adaptations to traditional verification methods, blending source-level
reasoning with behavior-aware evaluations. These findings provide evidence on
the practical context of testing generative components in software systems.

</details>


### [119] [Functional vs. Object-Oriented: Comparing How Programming Paradigms Affect the Architectural Characteristics of Systems](https://arxiv.org/abs/2508.00244)
*Briza Mel Dias de Sousa,Renato Cordeiro Ferreira,Alfredo Goldman*

Main category: cs.SE

TL;DR: 研究对比OOP和FP对软件系统架构特性的影响，以数字钱包系统为例，通过定性和定量分析得出结果，为开发者决策提供参考。


<details>
  <summary>Details</summary>
Motivation: 在OOP长期主导后，FP在软件行业受关注，研究两者对软件系统架构特性的影响。

Method: 以Kotlin实现OOP、Scala实现FP开发数字钱包系统，进行自我民族志定性分析和基于调查的定量分析。

Result: 定性分析展示代码编写者视角对比，定量分析收集不同背景开发者对代码的反馈。

Conclusion: 研究结果有助于开发者或组织为项目选择合适的编程范式。

Abstract: After decades of dominance by object-oriented programming (OOP), functional
programming (FP) is gaining increasing attention in the software industry. This
study compares the impact of OOP and FP on the architectural characteristics of
software systems. For that, it examines the design and implementation of a
Digital Wallet system, developed in Kotlin (representing OOP) and Scala
(representing FP). The comparison is made through both qualitative and
quantitative analyses to explore how each paradigm influences the system's
architectural characteristics. The self-ethnographic qualitative analysis
provides a side-by-side comparison of both implementations, revealing the
perspective of those writing such code. The survey-based quantitative analysis
gathers feedback from developers with diverse backgrounds, showing their
impressions of those reading this code. Hopefully, these results may be useful
for developers or organizations seeking to make more informed decisions about
which paradigm is best suited for their next project.

</details>


### [120] [Leveraging Large Language Model for Information Retrieval-based Bug Localization](https://arxiv.org/abs/2508.00253)
*Moumita Asad,Rafed Muhammad Yasir,Armin Geramirad,Sam Malek*

Main category: cs.SE

TL;DR: 提出基于大语言模型的Bug定位方法GenLoc，经实验验证优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有信息检索式Bug定位方法受限于错误报告和源代码间的词汇不匹配问题，效果有限。

Method: GenLoc利用具备代码探索功能的大语言模型迭代分析代码库，还可选择用向量嵌入检索语义相关文件。

Result: 在六个大型Java项目的9000多个真实错误报告上评估，GenLoc在多个指标上优于五种现有Bug定位技术，Accuracy@1平均提升超60%。

Conclusion: GenLoc在Bug定位方面效果良好，能有效解决现有方法的局限。

Abstract: Information Retrieval-based Bug Localization aims to identify buggy source
files for a given bug report. While existing approaches -- ranging from vector
space models to deep learning models -- have shown potential in this domain,
their effectiveness is often limited by the vocabulary mismatch between bug
reports and source code. To address this issue, we propose a novel Large
Language Model (LLM) based bug localization approach, called GenLoc. Given a
bug report, GenLoc leverages an LLM equipped with code-exploration functions to
iteratively analyze the code base and identify potential buggy files. To gather
better context, GenLoc may optionally retrieve semantically relevant files
using vector embeddings. GenLoc has been evaluated on over 9,000 real-world bug
reports from six large-scale Java projects. Experimental results show that
GenLoc outperforms five state-of-the-art bug localization techniques across
multiple metrics, achieving an average improvement of more than 60\% in
Accuracy@1.

</details>


### [121] [Accurate and Consistent Graph Model Generation from Text with Large Language Models](https://arxiv.org/abs/2508.00255)
*Boqi Chen,Ou Wei,Bingzhou Zheng,Gunter Mussbacher*

Main category: cs.SE

TL;DR: 本文提出抽象 - 具体化框架提升大语言模型生成图模型的一致性和质量，实验证明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成图模型存在语法违规、约束不一致和不准确等问题，后两者缺乏有效解决办法，受自一致性方法启发开展研究。

Method: 构建聚合所有候选输出的概率部分模型，再将其细化为满足所有约束的具体模型。

Result: 在多个流行的开源和闭源大语言模型上使用不同数据集评估，该方法显著提高了生成图模型的一致性和质量。

Conclusion: 所提抽象 - 具体化框架能有效解决大语言模型生成图模型存在的问题，提升模型质量。

Abstract: Graph model generation from natural language description is an important task
with many applications in software engineering. With the rise of large language
models (LLMs), there is a growing interest in using LLMs for graph model
generation. Nevertheless, LLM-based graph model generation typically produces
partially correct models that suffer from three main issues: (1) syntax
violations: the generated model may not adhere to the syntax defined by its
metamodel, (2) constraint inconsistencies: the structure of the model might not
conform to some domain-specific constraints, and (3) inaccuracy: due to the
inherent uncertainty in LLMs, the models can include inaccurate, hallucinated
elements. While the first issue is often addressed through techniques such as
constraint decoding or filtering, the latter two remain largely unaddressed.
Motivated by recent self-consistency approaches in LLMs, we propose a novel
abstraction-concretization framework that enhances the consistency and quality
of generated graph models by considering multiple outputs from an LLM. Our
approach first constructs a probabilistic partial model that aggregates all
candidate outputs and then refines this partial model into the most appropriate
concrete model that satisfies all constraints. We evaluate our framework on
several popular open-source and closed-source LLMs using diverse datasets for
model generation tasks. The results demonstrate that our approach significantly
improves both the consistency and quality of the generated graph models.

</details>


### [122] [Benchmarking LLMs for Unit Test Generation from Real-World Functions](https://arxiv.org/abs/2508.00408)
*Dong Huang,Jie M. Zhang,Mark Harman,Qianru Zhang,Mingzhe Du,See-Kiong Ng*

Main category: cs.SE

TL;DR: 文章指出现有大语言模型测试生成基准存在数据污染和函数代码结构简单问题，提出ULT基准和PLT基准，评估显示ULT更具挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型测试生成基准存在数据污染和结构简单的缺陷，无法准确评估其在单元测试生成方面的能力，需设计新基准。

Method: 通过多阶段筛选构建ULT基准，同时提供与之配对的PLT基准。

Result: 评估结果显示ULT更具挑战性，大语言模型在ULT上各项指标远低于TestEval和PLT。

Conclusion: ULT基准能更真实且具挑战性地评估大语言模型的测试生成能力。

Abstract: Recently, large language models (LLMs) have shown great promise in automating
unit test generation, significantly reducing the manual effort required by
developers. To effectively evaluate the capabilities of LLMs in this domain, it
is crucial to have a well-designed benchmark that accurately reflects
real-world scenarios and mitigates common pitfalls. Existing LLM test
generation benchmarks are limited by two critical drawbacks: data contamination
and structurally simple function code. As a result, we often cannot rely on the
validity of scientific conclusions drawn from empirical studies using these
limited benchmarks. The empirical evidence presented may be biased due to
contamination and may fail to generalize beyond toy programs due to structural
simplicity.
  To address these problems, we introduce ULT (UnLeakedTestbench), a new
benchmark specifically designed for function-level unit test generation from
real-world Python functions. ULT is constructed through a multi-stage curation
process that ensures high cyclomatic complexity and mitigates test case
contamination. With 3,909 carefully selected function-level tasks, ULT provides
a more realistic and challenging evaluation of LLMs' test generation
capabilities. We also provide PLT (PreLeakedTestbench), a pair benchmark of ULT
with leaked tests designed to enable a controlled analysis of memorization
versus reasoning in test generation. Our evaluation results demonstrate that
ULT is significantly more challenging. For example, test cases generated by
LLMs only achieve 41.32\%, 45.10\%, 30.22\%, and 40.21\% for accuracy,
statement coverage, branch coverage, and mutation score on average for all
LLMs, respectively. These results are substantially lower than the
corresponding metrics on TestEval (91.79\%, 92.18\%, 82.04\%, and 49.69\%) and
PLT (47.07\%, 55.13\%, 40.07\%, and 50.80\%).

</details>


### [123] [Managing Power Gaps as a Topic of Pair Programming Skill: A Grounded Theory](https://arxiv.org/abs/2508.00462)
*Linus Ververs,Lutz Prechelt*

Main category: cs.SE

TL;DR: 本文分析工业界结对编程的权力现象，提出权力差距理论，认为避免权力差距是结对编程重要技能。


<details>
  <summary>Details</summary>
Motivation: 了解工业界结对编程中的权力相关现象，并为从业者提供更好进行结对编程的建议。

Method: 运用扎根理论方法分析22个工业结对编程会话，构建权力相关行为的扎根理论，对292名参与者进行关于该理论的调查。

Result: 提出权力差距现象理论，该差距会损害知识转移、代码质量和流程效率，调查显示理论中的概念在实践中常见。

Conclusion: 避免权力差距是结对编程技能的重要组成部分，结对伙伴应避免层级行为，多进行平等行为。

Abstract: Context: Pair Programming as a work mode is used (occasionally or frequently)
throughout professional software development. Objective: Understand what
power-related phenomena occur in pair programming as it is used in industry;
give advice to practitioners on how to do better pair programming. Method:
Analyze 22 industrial pair programming sessions using Grounded Theory
Methodology. Formulate a Grounded Theory on power-related behaviors. Run a
survey with 292 participants about that theory. Use it to demonstrate that the
phenomena are common. Results: Our theory describes the phenomenon of Power
Gap: a perceived difference in participation opportunities. The theory shows
the behaviors that create a Power Gap or result from it. Power Gaps tend to
damage knowledge transfer, code quality, and process effi ciency. The survey
results show that all concepts from our theory are frequent in practice. They
also provide more grounding for concepts that are observable only indirectly.
Conclusions: It is a valuable component of pair programming skill to be able to
avoid Power Gaps. Specifically, pair partners need to avoid Hierarchical
Behavior (which tends to create or increase a Power Gap) and should perform
enough Equalizing Behavior (which prevents or reduces a Power Gap).

</details>


### [124] [Desyan: A Platform for Seamless Value-Flow and Symbolic Analysis](https://arxiv.org/abs/2508.00508)
*Panagiotis Diamantakis,Thanassis Avgerinos,Yannis Smaragdakis*

Main category: cs.SE

TL;DR: 文章介绍了Desyan平台，可无缝集成值流和符号推理，在不同类型的程序分析中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的值流分析和符号分析两种静态分析方法缺乏统一平台进行有效集成，需要一个能整合二者的平台。

Method: 在Soufflé Datalog定点引擎基础上扩展，引入SMT求解并调用行业领先的SMT引擎，提供自动处理程序分析中典型模式的构造，还有自底向上的代数推理模块支持Datalog原生符号推理。

Result: Desyan引擎能按需混合不同推理方式，在值流分析中是一流的Datalog评估器，在需要完整SMT的应用中利用领先SMT求解器，在轻量级符号评估中使用Datalog原生符号推理可实现大幅加速。

Conclusion: Desyan平台有效弥合了值流分析和符号分析的差距，实现了二者的无缝集成，在不同类型的程序分析中都能展现出良好性能。

Abstract: Over the past two decades, two different types of static analyses have
emerged as dominant paradigms both in academia and industry: value-flow
analysis (e.g., data-flow analysis or points-to analysis) and symbolic analysis
(e.g., symbolic execution). Despite their individual successes in numerous
application fields, the two approaches have remained largely separate; an
artifact of the simple reality that there is no broadly adopted unifying
platform for effortless and efficient integration of symbolic techniques with
high-performance data-flow reasoning.
  To bridge this gap, we introduce Desyan: a platform for writing program
analyses with seamless integration of value-flow and symbolic reasoning. Desyan
expands a production-ready Datalog fixpoint engine (Souffl\'e) with
full-fledged SMT solving invoking industry-leading SMT engines. Desyan provides
constructs for automatically (and efficiently!) handling typical patterns that
come up in program analysis. At the same time, the integration is agnostic with
respect to the solving technology, and supports Datalog-native symbolic
reasoning, via a bottom-up algebraic reasoning module.
  The result is an engine that allows blending different kinds of reasoning, as
needed for the underlying analysis. For value-flow analysis, the engine is the
best-in-class Datalog evaluator (often by a factor of over 20x in execution
time); for applications that require full SMT (e.g., a concolic execution
engine or other symbolic evaluator that needs to solve arbitrarily complex
conditions), the engine is leveraging the leading SMT solvers; for lightweight
symbolic evaluation (e.g., solving simple conditionals in the context of a
path-sensitive analysis), the engine can use Datalog-native symbolic reasoning,
achieving large speedups (often of over 2x) compared to eagerly appealing to an
SMT solver.

</details>


### [125] [SPENCER: Self-Adaptive Model Distillation for Efficient Code Retrieval](https://arxiv.org/abs/2508.00546)
*Wenchao Gu,Zongyi Lyu,Yanlin Wang,Hongyu Zhang,Cuiyun Gao,Michael R. Lyu*

Main category: cs.SE

TL;DR: 提出SPENCER框架用于代码检索，结合双编码器和交叉编码器，还提出模型蒸馏技术和助教选择策略，实验表明提升性能并减少推理时间。


<details>
  <summary>Details</summary>
Motivation: 现有双编码器模型结构在训练时缺乏代码片段和描述的底层交互，限制了模型性能，需提升模型有效性和效率。

Method: 提出SPENCER框架，先用双编码器缩小搜索空间，再用交叉编码器提高准确率；提出模型蒸馏技术减少双编码器推理时间；提出助教选择策略确保模型性能。

Result: 结合双编码器和交叉编码器比仅基于双编码器的模型提升整体性能；模型蒸馏技术保留超98%整体性能，减少70%双编码器推理时间。

Conclusion: SPENCER框架及相关技术能有效提升代码检索模型的性能和效率。

Abstract: Code retrieval aims to provide users with desired code snippets based on
users' natural language queries. With the development of deep learning
technologies, adopting pre-trained models for this task has become mainstream.
Considering the retrieval efficiency, most of the previous approaches adopt a
dual-encoder for this task, which encodes the description and code snippet into
representation vectors, respectively. However, the model structure of the
dual-encoder tends to limit the model's performance, since it lacks the
interaction between the code snippet and description at the bottom layer of the
model during training. To improve the model's effectiveness while preserving
its efficiency, we propose a framework, which adopts Self-AdaPtive Model
Distillation for Efficient CodE Retrieval, named SPENCER. SPENCER first adopts
the dual-encoder to narrow the search space and then adopts the cross-encoder
to improve accuracy. To improve the efficiency of SPENCER, we propose a novel
model distillation technique, which can greatly reduce the inference time of
the dual-encoder while maintaining the overall performance. We also propose a
teaching assistant selection strategy for our model distillation, which can
adaptively select the suitable teaching assistant models for different
pre-trained models during the model distillation to ensure the model
performance. Extensive experiments demonstrate that the combination of
dual-encoder and cross-encoder improves overall performance compared to solely
dual-encoder-based models for code retrieval. Besides, our model distillation
technique retains over 98% of the overall performance while reducing the
inference time of the dual-encoder by 70%.

</details>


### [126] [Can User Feedback Help Issue Detection? An Empirical Study on a One-billion-user Online Service System](https://arxiv.org/abs/2508.00593)
*Shuyao Jiang,Jiazhen Gu,Wujie Zheng,Yangfan Zhou,Michael R. Lyu*

Main category: cs.SE

TL;DR: 对十亿用户在线服务系统中6个真实服务的大量用户反馈进行实证研究，发现处理反馈需过滤无关信息，单靠反馈特征难检测严重问题，设计基于机器学习的方法可行，研究为问题检测提供实证基础。


<details>
  <summary>Details</summary>
Motivation: 大规模在线服务系统从大量用户反馈中识别严重问题有挑战，为开发更好的基于反馈的问题检测方法，需全面了解实际生产系统中用户反馈的特征。

Method: 对50,378,766条用户反馈进行实证研究，研究用户反馈内容、反馈特征能否指示严重问题以及采用机器学习技术分析反馈是否合理。

Result: 大量反馈含无关信息，需过滤；单靠反馈特征难检测严重问题；不同时间间隔反馈主题分布相似，基于机器学习的方法可行。

Conclusion: 研究结果为大规模服务系统中基于反馈的问题检测提供实证基础，为实际问题检测方法的设计和实现提供思路。

Abstract: Background: It has long been suggested that user feedback, typically written
in natural language by end-users, can help issue detection. However, for
large-scale online service systems that receive a tremendous amount of
feedback, it remains a challenging task to identify severe issues from user
feedback. Aims: To develop a better feedback-based issue detection approach, it
is crucial first to gain a comprehensive understanding of the characteristics
of user feedback in real production systems. Method: In this paper, we conduct
an empirical study on 50,378,766 user feedback items from six real-world
services in a one-billion-user online service system. We first study what users
provide in their feedback. We then examine whether certain features of feedback
items can be good indicators of severe issues. Finally, we investigate whether
adopting machine learning techniques to analyze user feedback is reasonable.
Results: Our results show that a large proportion of user feedback provides
irrelevant information about system issues. As a result, it is crucial to
filter out issue-irrelevant information when processing user feedback.
Moreover, we find severe issues that cannot be easily detected based solely on
user feedback characteristics. Finally, we find that the distributions of the
feedback topics in different time intervals are similar. This confirms that
designing machine learning-based approaches is a viable direction for better
analyzing user feedback. Conclusions: We consider that our findings can serve
as an empirical foundation for feedback-based issue detection in large-scale
service systems, which sheds light on the design and implementation of
practical issue detection approaches.

</details>


### [127] [MCeT: Behavioral Model Correctness Evaluation using Large Language Models](https://arxiv.org/abs/2508.00630)
*Khaled Ahmed,Jialing Song,Boqi Chen,Ou Wei,Bingzhou Zheng*

Main category: cs.SE

TL;DR: 本文提出全自动工具MCeT评估行为模型（序列图）正确性，结合多视角方法和自一致性检查，提升评估效果。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型用于自动生成图，需推进自动模型正确性评估工具的发展。

Method: 提出MCeT工具，利用大语言模型，采用细粒度、多视角方法及自一致性检查方法。

Result: 结合方法在真实需求数据集上使直接方法的精度从0.58提升到0.81，比直接方法多发现90%有经验工程师找到的问题，每张图平均报告6个新问题。

Conclusion: MCeT工具结合多视角和自一致性检查方法可有效提升行为模型正确性评估效果。

Abstract: Behavioral model diagrams, e.g., sequence diagrams, are an essential form of
documentation that are typically designed by system engineers from requirements
documentation, either fully manually or assisted by design tools. With the
growing use of Large Language Models (LLM) as AI modeling assistants, more
automation will be involved in generating diagrams. This necessitates the
advancement of automatic model correctness evaluation tools. Such a tool can be
used to evaluate both manually and AI automatically generated models; to
provide feedback to system engineers, and enable AI assistants to self-evaluate
and self-enhance their generated models.
  In this paper, we propose MCeT, the first fully automated tool to evaluate
the correctness of a behavioral model, sequence diagrams in particular, against
its corresponding requirements text and produce a list of issues that the model
has. We utilize LLMs for the correctness evaluation tasks as they have shown
outstanding natural language understanding ability. However, we show that
directly asking an LLM to compare a diagram to requirements finds less than 35%
of issues that experienced engineers can find. We propose to supplement the
direct check with a fine-grained, multi-perspective approach; we split the
diagram into atomic, non-divisible interactions, and split the requirements
text into atomic, self-contained items. We compare the diagram with atomic
requirements and each diagram-atom with the requirements. We also propose a
self-consistency checking approach that combines perspectives to mitigate LLM
hallucinated issues. Our combined approach improves upon the precision of the
direct approach from 0.58 to 0.81 in a dataset of real requirements. Moreover,
the approach finds 90% more issues that the experienced engineers found than
the direct approach, and reports an average of 6 new issues per diagram.

</details>


### [128] [Is LLM-Generated Code More Maintainable \& Reliable than Human-Written Code?](https://arxiv.org/abs/2508.00700)
*Alfred Santa Molison,Marcia Moraes,Glaucia Melo,Fabio Santos,Wesley K. G. Assuncao*

Main category: cs.SE

TL;DR: 研究对比大语言模型生成代码与人类编写代码的内部质量属性，发现大语言模型生成代码整体缺陷少、修复成本低，但复杂场景有问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于代码生成广泛，但不清楚其生成代码在软件质量上表现及与人类编写代码的对比情况，故开展研究。

Method: 结合编码任务数据集、三种大语言模型配置和SonarQube评估软件质量，分析Python代码关键质量指标和修复问题的估计工作量。

Result: 大语言模型生成代码缺陷少、修复成本低；微调模型减少高严重性问题但降低性能；竞争级问题中模型生成代码有结构问题。

Conclusion: 研究为大语言模型生成代码质量提供见解，复杂场景引入关键问题表明需系统评估和验证，加深了对大语言模型代码生成优缺点的理解。

Abstract: Background: The rise of Large Language Models (LLMs) in software development
has opened new possibilities for code generation. Despite the widespread use of
this technology, it remains unclear how well LLMs generate code solutions in
terms of software quality and how they compare to human-written code. Aims:
This study compares the internal quality attributes of LLM-generated and
human-written code. Method: Our empirical study integrates datasets of coding
tasks, three LLM configurations (zero-shot, few-shot, and fine-tuning), and
SonarQube to assess software quality. The dataset comprises Python code
solutions across three difficulty levels: introductory, interview, and
competition. We analyzed key code quality metrics, including maintainability
and reliability, and the estimated effort required to resolve code issues.
Results: Our analysis shows that LLM-generated code has fewer bugs and requires
less effort to fix them overall. Interestingly, fine-tuned models reduced the
prevalence of high-severity issues, such as blocker and critical bugs, and
shifted them to lower-severity categories, but decreased the model's
performance. In competition-level problems, the LLM solutions sometimes
introduce structural issues that are not present in human-written code.
Conclusion: Our findings provide valuable insights into the quality of
LLM-generated code; however, the introduction of critical issues in more
complex scenarios highlights the need for a systematic evaluation and
validation of LLM solutions. Our work deepens the understanding of the
strengths and limitations of LLMs for code generation.

</details>


### [129] [Tool-Assisted Conformance Checking to Reference Process Models](https://arxiv.org/abs/2508.00738)
*Bernhard Rumpe,Max Stachon,Sebastian Stüber,Valdes Voufo*

Main category: cs.SE

TL;DR: 本文探索使用任务和事件的因果依赖分析，对具体流程模型与参考模型进行自动一致性检查，提供了工具辅助方案。


<details>
  <summary>Details</summary>
Motivation: 现有流程模型一致性检查缺乏语义模型比较所需的表达能力和自动化，需要解决此问题以确保流程符合参考模型的准则和原则，保证质量和一致性。

Method: 将方法集成到更广泛的语义框架中定义参考模型一致性，给出参考流程模型一致性检查算法，并通过案例研究评估。

Result: 完成了算法评估，讨论了其优缺点。

Conclusion: 提供了一个工具辅助解决方案，提高了流程模型一致性验证的准确性和灵活性。

Abstract: Reference models convey best practices and standards. The reference
frameworks necessitate conformance checks to ensure adherence to established
guidelines and principles, which is crucial for maintaining quality and
consistency in various processes. This paper explores automated conformance
checks for concrete process models against reference models using causal
dependency analysis of tasks and events. Existing notions of conformance
checking for process models focus on verifying process execution traces and
lack the expressiveness and automation needed for semantic model comparison,
leaving this question unresolved. We integrate our approach into a broader
semantic framework for defining reference model conformance. We outline an
algorithm for reference process model conformance checking, evaluate it through
a case study, and discuss its strengths and limitations. Our research provides
a tool-assisted solution enhancing accuracy and flexibility in process model
conformance verification.

</details>


### [130] [Dynamic Symbolic Execution for Semantic Difference Analysis of Component and Connector Architectures](https://arxiv.org/abs/2508.00749)
*Johanna Grahl,Bernhard Rumpe,Max Stachon,Sebastian Stüber*

Main category: cs.SE

TL;DR: 本文研究动态符号执行（DSE）在组件与连接器架构语义差异分析中的应用，增强MontiArc - Java生成器收集数据，评估执行策略，发现DSE有前景但扩展性是主要限制。


<details>
  <summary>Details</summary>
Motivation: 在模型驱动开发中，确保演化模型的正确性和一致性至关重要，因此研究DSE在组件与连接器架构语义差异分析中的应用。

Method: 增强现有MontiArc - Java生成器以在运行时收集符号和具体执行数据，评估不同执行策略。

Result: 发现DSE在分析组件和连接器架构方面有前景，但扩展性是主要限制。

Conclusion: 需要进一步研究以提高DSE在大型系统中的实用性。

Abstract: In the context of model-driven development, ensuring the correctness and
consistency of evolving models is paramount. This paper investigates the
application of Dynamic Symbolic Execution (DSE) for semantic difference
analysis of component-and-connector architectures, specifically utilizing
MontiArc models. We have enhanced the existing MontiArc-to-Java generator to
gather both symbolic and concrete execution data at runtime, encompassing
transition conditions, visited states, and internal variables of automata. This
data facilitates the identification of significant execution traces that
provide critical insights into system behavior. We evaluate various execution
strategies based on the criteria of runtime efficiency, minimality, and
completeness, establishing a framework for assessing the applicability of DSE
in semantic difference analysis. Our findings indicate that while DSE shows
promise for analyzing component and connector architectures, scalability
remains a primary limitation, suggesting further research is needed to enhance
its practical utility in larger systems.

</details>


### [131] [From Code to Career: Assessing Competitive Programmers for Industry Placement](https://arxiv.org/abs/2508.00772)
*Md Imranur Rahman Akib,Fathima Binthe Muhammed,Umit Saha,Md Fazlul Karim Patwary,Mehrin Anannya,Md Alomgeer Hussein,Md Biplob Hosen*

Main category: cs.SE

TL;DR: 研究基于Codeforces用户编码表现预测其软件工程师岗位就业潜力，用随机森林分类器建模并部署，证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 快节奏科技行业需要基于编码表现评估程序员岗位就绪度的工具，本研究聚焦预测Codeforces用户获不同级别软件工程岗位的潜力。

Method: 通过Codeforces API收集用户数据，处理关键绩效指标，用随机森林分类器构建预测模型，用Flask实现系统并在Render上部署进行实时预测。

Result: 该方法能基于编码熟练度和参与度有效区分不同技能水平。

Conclusion: 此研究为机器学习用于职业评估奠定基础，可扩展到更广泛技术领域的岗位就绪度预测。

Abstract: In today's fast-paced tech industry, there is a growing need for tools that
evaluate a programmer's job readiness based on their coding performance. This
study focuses on predicting the potential of Codeforces users to secure various
levels of software engineering jobs. The primary objective is to analyze how a
user's competitive programming activity correlates with their chances of
obtaining positions, ranging from entry-level roles to jobs at major tech
companies. We collect user data using the Codeforces API, process key
performance metrics, and build a prediction model using a Random Forest
classifier. The model categorizes users into four levels of employability,
ranging from those needing further development to those ready for top-tier tech
jobs. The system is implemented using Flask and deployed on Render for
real-time predictions. Our evaluation demonstrates that the approach
effectively distinguishes between different skill levels based on coding
proficiency and participation. This work lays a foundation for the use of
machine learning in career assessment and could be extended to predict job
readiness in broader technical fields.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [132] [ContestTrade: A Multi-Agent Trading System Based on Internal Contest Mechanism](https://arxiv.org/abs/2508.00554)
*Li Zhao,Rui Sun,Zuoyou Jiang,Bo Yang,Yuxiao Bai,Mengting Chen,Xinyang Wang,Jing Li,Zuo Bai*

Main category: q-fin.TR

TL;DR: 提出含内部竞争机制的多智能体系统提升金融交易表现，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决基于大语言模型的交易系统对市场噪声敏感、性能受影响的问题。

Method: 构建含数据团队和研究团队的多智能体系统，在团队内实施实时评估和排名机制。

Result: 所提系统在多种评估指标上显著优于现有多智能体系统和传统量化投资方法。

Conclusion: 该系统能自适应动态环境，增强抗市场噪声能力，带来更优交易表现。

Abstract: In financial trading, large language model (LLM)-based agents demonstrate
significant potential. However, the high sensitivity to market noise undermines
the performance of LLM-based trading systems. To address this limitation, we
propose a novel multi-agent system featuring an internal competitive mechanism
inspired by modern corporate management structures. The system consists of two
specialized teams: (1) Data Team - responsible for processing and condensing
massive market data into diversified text factors, ensuring they fit the
model's constrained context. (2) Research Team - tasked with making
parallelized multipath trading decisions based on deep research methods. The
core innovation lies in implementing a real-time evaluation and ranking
mechanism within each team, driven by authentic market feedback. Each agent's
performance undergoes continuous scoring and ranking, with only outputs from
top-performing agents being adopted. The design enables the system to
adaptively adjust to dynamic environment, enhances robustness against market
noise and ultimately delivers superior trading performance. Experimental
results demonstrate that our proposed system significantly outperforms
prevailing multiagent systems and traditional quantitative investment methods
across diverse evaluation metrics.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [133] [funOCLUST: Clustering Functional Data with Outliers](https://arxiv.org/abs/2508.00110)
*Katharine M. Clark,Paul D. McNicholas*

Main category: stat.ML

TL;DR: 提出OCLUST算法在函数数据上的扩展，评估显示其聚类和识别异常值表现好。


<details>
  <summary>Details</summary>
Motivation: 函数数据因无限维特性和对异常值敏感，给聚类带来挑战。

Method: 将OCLUST算法扩展到函数数据场景，利用OCLUST框架创建聚类曲线和修剪异常值的鲁棒方法。

Result: 在模拟和真实世界的函数数据集上评估，该方法在聚类和异常值识别方面表现出色。

Conclusion: 所提出的OCLUST算法扩展方法在函数数据聚类和异常值处理上有效。

Abstract: Functional data present unique challenges for clustering due to their
infinite-dimensional nature and potential sensitivity to outliers. An extension
of the OCLUST algorithm to the functional setting is proposed to address these
issues. The approach leverages the OCLUST framework, creating a robust method
to cluster curves and trim outliers. The methodology is evaluated on both
simulated and real-world functional datasets, demonstrating strong performance
in clustering and outlier identification.

</details>


### [134] [Sinusoidal Approximation Theorem for Kolmogorov-Arnold Networks](https://arxiv.org/abs/2508.00247)
*Sergei Gleyzer,Hanh Nguyen,Dinesh P. Ramakrishnan,Eric A. F. Reinhardt*

Main category: stat.ML

TL;DR: 本文提出一种新的Kolmogorov - Arnold网络变体，用可学习频率的加权正弦函数替换表示中的内外函数，证明理论有效性并实验验证性能。


<details>
  <summary>Details</summary>
Motivation: 探索Kolmogorov - Arnold网络新变体，替代多层感知机。

Method: 用可学习频率的加权正弦函数替换内外函数，固定正弦激活的相位为线性间隔常数值，并证明理论有效性，进行数值实验。

Result: 新变体优于固定频率傅里叶变换方法，与多层感知机性能相当。

Conclusion: 提出的新KAN变体在多变量函数处理上有良好表现，具有理论和实际应用价值。

Abstract: The Kolmogorov-Arnold representation theorem states that any continuous
multivariable function can be exactly represented as a finite superposition of
continuous single variable functions. Subsequent simplifications of this
representation involve expressing these functions as parameterized sums of a
smaller number of unique monotonic functions. These developments led to the
proof of the universal approximation capabilities of multilayer perceptron
networks with sigmoidal activations, forming the alternative theoretical
direction of most modern neural networks.
  Kolmogorov-Arnold Networks (KANs) have been recently proposed as an
alternative to multilayer perceptrons. KANs feature learnable nonlinear
activations applied directly to input values, modeled as weighted sums of basis
spline functions. This approach replaces the linear transformations and
sigmoidal post-activations used in traditional perceptrons. Subsequent works
have explored alternatives to spline-based activations. In this work, we
propose a novel KAN variant by replacing both the inner and outer functions in
the Kolmogorov-Arnold representation with weighted sinusoidal functions of
learnable frequencies. Inspired by simplifications introduced by Lorentz and
Sprecher, we fix the phases of the sinusoidal activations to linearly spaced
constant values and provide a proof of its theoretical validity. We also
conduct numerical experiments to evaluate its performance on a range of
multivariable functions, comparing it with fixed-frequency Fourier transform
methods and multilayer perceptrons (MLPs). We show that it outperforms the
fixed-frequency Fourier transform and achieves comparable performance to MLPs.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [135] [Efficient rare event estimation for multimodal and high-dimensional system reliability via subset adaptive importance sampling](https://arxiv.org/abs/2508.00210)
*Sara Helal,Victor Elvira*

Main category: stat.CO

TL;DR: 提出子集自适应重要性采样（SAIS）策略估计复杂系统罕见事件，在多方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法在多模态、高维问题中估计复杂系统罕见事件存在局限性，易忽略关键失效模式，需大样本量。

Method: 结合子集模拟和自适应多重重要性采样，利用前阶段加权样本迭代优化建议分布。

Result: 通过一系列基准问题验证，SAIS在捕获不同失效模式和高精度估计失效概率上始终优于竞争方法。

Conclusion: SAIS比现有方法用更少样本获得低方差估计，在准确性和计算成本上有显著改进。

Abstract: Estimating rare events in complex systems is a key challenge in reliability
analysis. The challenge grows in multimodal problems, where traditional methods
often rely on a small set of design points and risk overlooking critical
failure modes. Further, higher dimensions make the probability mass harder to
capture and demand substantially larger sample sizes to estimate failures. In
this work, we propose a new sampling strategy, subset adaptive importance
sampling (SAIS), that combines the strengths of subset simulation and adaptive
multiple importance sampling. SAIS iteratively refines a set of proposal
distributions using weighted samples from previous stages, efficiently
exploring complex and high-dimensional failure regions. Leveraging recent
advances in adaptive importance sampling, SAIS yields low-variance estimates
using fewer samples than state-of-the-art methods and achieves pronounced
improvements in both accuracy and computational cost. Through a series of
benchmark problems involving high-dimensional, nonlinear performance functions,
and multimodal scenarios, we demonstrate that SAIS consistently outperforms
competing methods in capturing diverse failure modes and estimating failure
probabilities with high precision.

</details>


### [136] [Online Rolling Controlled Sequential Monte Carlo](https://arxiv.org/abs/2508.00696)
*Liwen Xue,Axel Finke,Adam M. Johansen*

Main category: stat.CO

TL;DR: 提出用于一般状态空间隐马尔可夫模型实时推理的方法，数值结果显示比标准粒子滤波方法更优。


<details>
  <summary>Details</summary>
Motivation: 为一般状态空间隐马尔可夫模型开发实时推理方法。

Method: 通过滚动窗口机制将离线平滑的受控顺序蒙特卡罗（CSMC）方法扩展到在线设置，提出在线滚动受控顺序蒙特卡罗（ORCSMC）算法，使用两个粒子系统同时估计扭曲函数和执行滤波。

Result: 在多个模型的数值结果中，相比标准粒子滤波方法，该方法在高维情况下提高了估计精度和鲁棒性。

Conclusion: 该方法为复杂潜变量模型的顺序和实时推理提供了统计上有效且实用的解决方案。

Abstract: We introduce methodology for real-time inference in general-state-space
hidden Markov models. Specifically, we extend recent advances in controlled
sequential Monte Carlo (CSMC) methods-originally proposed for offline
smoothing-to the online setting via a rolling window mechanism. Our novel
online rolling controlled sequential Monte Carlo (ORCSMC) algorithm employs two
particle systems to simultaneously estimate twisting functions and perform
filtering, ensuring real-time adaptivity to new observations while maintaining
bounded computational cost. Numerical results on linear-Gaussian, stochastic
volatility, and neuroscience models demonstrate improved estimation accuracy
and robustness in higher dimensions, compared to standard particle filtering
approaches. The method offers a statistically efficient and practical solution
for sequential and real-time inference in complex latent variable models.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [137] [Energy-Aware CPU Orchestration in O-RAN: A dApp-Driven Lightweight Approach](https://arxiv.org/abs/2508.00629)
*Francisco Crespo,Javier Villegas,Carlos Baena,Eduardo Baena,Sergio Fortes,Raquel Barco*

Main category: cs.NI

TL;DR: 本文提出在分布式单元部署轻量级可编程分布式应用（dApp）动态编排CPU使用，以解决软交换无线接入网CPU资源管理问题，实验证明可节能且不影响性能。


<details>
  <summary>Details</summary>
Motivation: 软交换无线接入网在严格实时约束下高效管理CPU资源面临挑战，现有方案导致性能不佳和能耗高。

Method: 在分布式单元部署轻量级可编程dApp，与操作系统闭环运行，利用线程级遥测数据实时调整CPU线程亲和性、核心隔离和频率缩放。

Result: 使用商用级srsRAN部署实验表明，能持续节省电力且不影响实时处理性能。

Conclusion: 低延迟dApp在下一代网络细粒度资源控制方面有潜力。

Abstract: The transition toward softwarized Radio Access Networks (RANs), driven by the
Open RAN (O-RAN) paradigm, enables flexible, vendor-neutral deployments through
disaggregation and virtualization of base station functions. However, this
shift introduces new challenges in managing CPU resources efficiently under
strict real-time constraints. In particular, the interplay between
latency-sensitive RAN workloads and general-purpose Operating System (OS)
schedulers often leads to sub-optimal performance and unnecessary energy
consumption. This work proposes a lightweight, programmable distributed
application (dApp) deployed at the Distributed Unit (DU) level to dynamically
orchestrate CPU usage. The dApp operates in closed loop with the OS, leveraging
thread-level telemetry like context switches, Instructions Per Cycle (IPC), and
cache metrics, to adapt CPU thread affinity, core isolation, and frequency
scaling in real time. Unlike existing solutions, it requires no access to
proprietary RAN software, hardware-specific features, or kernel modifications.
Fully compliant with the O-RAN architecture and agnostic to the underlying RAN
stack, the proposed solution introduces negligible overhead while improving
energy efficiency and CPU utilization. Experimental results using a
commercial-grade srsRAN deployment demonstrate consistent power savings without
compromising real-time processing performance, highlighting the potential of
low-latency dApps for fine-grained resource control in next-generation networks

</details>


### [138] [Quality-of-Service Aware LLM Routing for Edge Computing with Multiple Experts](https://arxiv.org/abs/2508.00234)
*Jin Yang,Qiong Wu,Zhiying Feng,Zhi Zhou,Deke Guo,Xu Chen*

Main category: cs.NI

TL;DR: 为解决云基大语言模型服务的问题，提出基于深度强化学习的QoS感知路由框架，实验证明其优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 云基大语言模型服务有高延迟、响应不稳定和隐私问题，现有路由算法无法同时解决LLM服务异质性等问题，需保证长期稳定的服务质量。

Method: 提出基于深度强化学习的QoS感知LLM路由框架，用异构图注意力网络进行动态状态抽象，引入动作影响估计器和定制奖励函数。

Result: 在泊松和真实工作负载上的实验表明，该算法显著提高了平均QoS和计算资源效率。

Conclusion: 提出的框架能有效应对LLM服务路由问题，提升服务质量和资源效率。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities,
leading to a significant increase in user demand for LLM services. However,
cloud-based LLM services often suffer from high latency, unstable
responsiveness, and privacy concerns. Therefore, multiple LLMs are usually
deployed at the network edge to boost real-time responsiveness and protect data
privacy, particularly for many emerging smart mobile and IoT applications.
Given the varying response quality and latency of LLM services, a critical
issue is how to route user requests from mobile and IoT devices to an
appropriate LLM service (i.e., edge LLM expert) to ensure acceptable
quality-of-service (QoS). Existing routing algorithms fail to simultaneously
address the heterogeneity of LLM services, the interference among requests, and
the dynamic workloads necessary for maintaining long-term stable QoS. To meet
these challenges, in this paper we propose a novel deep reinforcement learning
(DRL)-based QoS-aware LLM routing framework for sustained high-quality LLM
services. Due to the dynamic nature of the global state, we propose a dynamic
state abstraction technique to compactly represent global state features with a
heterogeneous graph attention network (HAN). Additionally, we introduce an
action impact estimator and a tailored reward function to guide the DRL agent
in maximizing QoS and preventing latency violations. Extensive experiments on
both Poisson and real-world workloads demonstrate that our proposed algorithm
significantly improves average QoS and computing resource efficiency compared
to existing baselines.

</details>


### [139] [Agent Network Protocol Technical White Paper](https://arxiv.org/abs/2508.00007)
*Gaowei Chang,Eidan Lin,Chengxuan Yuan,Rizhao Cai,Binbin Chen,Xuan Xie,Yin Zhang*

Main category: cs.NI

TL;DR: 现有互联网基础设施难以支持大规模智能体互联协作，Agent Network Protocol (ANP) 提出面向智能体网络的新一代通信协议，解决了相关问题。


<details>
  <summary>Details</summary>
Motivation: 现有互联网基础设施主要为人类交互设计，难以支持大规模智能体互联协作，而互联网正经历深刻变革，呈现四大核心趋势，需要新的通信协议。

Method: ANP 坚持 AI 原生设计，与现有互联网协议兼容，采用模块化可组合架构，遵循简约可扩展原则，通过三层协议系统（身份与加密通信层、元协议协商层和应用协议层）解决问题。

Result: 系统地解决了智能体身份认证、动态协商和能力发现互操作性等问题。

Conclusion: ANP 是适应互联网发展趋势的新一代通信协议，能有效支持智能体网络的发展。

Abstract: With the development of large models and autonomous decision-making AI,
agents are rapidly becoming the new entities of the internet, following mobile
apps. However, existing internet infrastructure is primarily designed for human
interaction, creating data silos, unfriendly interfaces, and high collaboration
costs among agents, making it difficult to support the needs for large-scale
agent interconnection and collaboration. The internet is undergoing a profound
transformation, showing four core trends: agents replacing traditional
software, universal agent interconnection, native protocol-based connections,
and autonomous agent organization and collaboration. To align with these
trends, Agent Network Protocol (ANP) proposes a new generation of communication
protocols for the Agentic Web. ANP adheres to AI-native design, maintains
compatibility with existing internet protocols, adopts a modular composable
architecture, follows minimalist yet extensible principles, and enables rapid
deployment based on existing infrastructure. Through a three-layer protocol
system--identity and encrypted communication layer, meta-protocol negotiation
layer, and application protocol layer--ANP. systematically solves the problems
of agent identity authentication, dynamic negotiation, and capability discovery
interoperability.

</details>


### [140] [Enabling Immersive XR Collaborations over FTTR Networks (Invited)](https://arxiv.org/abs/2508.00009)
*Sourav Mondal,Elaine Wong*

Main category: cs.NI

TL;DR: 本文探讨FTTR上的预测带宽分配和无缝切换方案，实现室内协作的高质量沉浸式体验。


<details>
  <summary>Details</summary>
Motivation: 探索实现室内扩展现实协作的潜在解决方案，即Fiber - To - The - Room（FTTR）。

Method: 研究FTTR上的预测带宽分配和无缝切换方案。

Result: 可以实现室内协作的高质量沉浸式体验。

Conclusion: FTTR配合预测带宽分配和无缝切换方案，是实现室内扩展现实协作的有效途径。

Abstract: Fiber-To-The-Room is a potential solution to achieve in-premise extended
reality collaborations. This paper explores predictive bandwidth allocation and
seamless handover schemes over FTTR, showing high-quality immersive experience
for in-premise collaborations can be achieved. \c{opyright} 2025 The Author(s).

</details>


### [141] [AoI-Aware Resource Allocation with Deep Reinforcement Learning for HAPS-V2X Networks](https://arxiv.org/abs/2508.00011)
*Ahmet Melih Ince,Ayse Elif Canbilen,Halim Yanikomeroglu*

Main category: cs.NI

TL;DR: 本文提出基于深度确定性策略梯度（DDPG）的强化学习方法，优化高空平台站（HAPS）支持的车联网信息年龄（AoI），提升信息新鲜度和网络可靠性。


<details>
  <summary>Details</summary>
Motivation: 6G网络需满足自动驾驶等安全关键应用的高可靠低延迟通信需求，将非地面网络（NTN）集成到6G基础设施中可增加网络冗余，HAPS具有广覆盖和低延迟优势，可支持通信可靠性和增强信息新鲜度。

Method: 采用基于深度确定性策略梯度（DDPG）的强化学习方法，在无集中协调的情况下实现独立学习。

Result: 所提出的方法能够提高信息新鲜度和整体网络可靠性。

Conclusion: HAPS支持的解决方案结合基于DDPG的学习，在基于车队的自动驾驶系统中进行高效的AoI感知资源分配具有潜力。

Abstract: Sixth-generation (6G) networks are designed to meet the hyper-reliable and
low-latency communication (HRLLC) requirements of safety-critical applications
such as autonomous driving. Integrating non-terrestrial networks (NTN) into the
6G infrastructure brings redundancy to the network, ensuring continuity of
communications even under extreme conditions. In particular, high-altitude
platform stations (HAPS) stand out for their wide coverage and low latency
advantages, supporting communication reliability and enhancing information
freshness, especially in rural areas and regions with infrastructure
constraints. In this paper, we present reinforcement learning-based approaches
using deep deterministic policy gradient (DDPG) to dynamically optimize the
age-of-information (AoI) in HAPS-enabled vehicle-to-everything (V2X) networks.
The proposed method improves information freshness and overall network
reliability by enabling independent learning without centralized coordination.
The findings reveal the potential of HAPS-supported solutions, combined with
DDPG-based learning, for efficient AoI-aware resource allocation in
platoon-based autonomous vehicle systems.

</details>


### [142] [Scalable Spectrum Availability Prediction using a Markov Chain Framework and ITU-R Propagation Models](https://arxiv.org/abs/2508.00028)
*Abir Ray*

Main category: cs.NI

TL;DR: 本文提出可扩展频谱可用性预测框架，结合马尔可夫链模型与ITU - R传播模型，能时空预测频谱机会，成本低适用于实时频谱管理。


<details>
  <summary>Details</summary>
Motivation: 频谱资源时空利用率低，需动态频谱接入策略，关键挑战是预测频谱可用情况。

Method: 结合主用户活动的两状态马尔可夫链模型和ITU - R的高保真传播模型，开发系统模型和算法。

Result: 所提方法能以低计算成本有效识别可用频谱。

Conclusion: 该框架灵活，适用于认知无线电网络等动态频谱共享系统的实时频谱管理。

Abstract: Spectrum resources are often underutilized across time and space, motivating
dynamic spectrum access strategies that allow secondary users to exploit unused
frequencies. A key challenge is predicting when and where spectrum will be
available (i.e., unused by primary licensed users) in order to enable proactive
and interference-free access. This paper proposes a scalable framework for
spectrum availability prediction that combines a two-state Markov chain model
of primary user activity with high-fidelity propagation models from the ITU-R
(specifically Recommendations P.528 and P.2108). The Markov chain captures
temporal occupancy patterns, while the propagation models incorporate path loss
and clutter effects to determine if primary signals exceed interference
thresholds at secondary user locations. By integrating these components, the
proposed method can predict spectrum opportunities both in time and space with
improved accuracy. We develop the system model and algorithm for the approach,
analyze its scalability and computational efficiency, and discuss assumptions,
limitations, and potential applications. The framework is flexible and can be
adapted to various frequency bands and scenarios. The results and analysis show
that the proposed approach can effectively identify available spectrum with low
computational cost, making it suitable for real-time spectrum management in
cognitive radio networks and other dynamic spectrum sharing systems.

</details>


### [143] [Large AI Model-Enabled Secure Communications in Low-Altitude Wireless Networks: Concepts, Perspectives and Case Study](https://arxiv.org/abs/2508.00256)
*Chuang Zhang,Geng Sun,Jiacheng Wang,Yijing Lin,Weijie Yuan,Sinem Coleri,Dusit Niyato,Tony Q. S. Quek*

Main category: cs.NI

TL;DR: 本文研究用大人工智能模型（LAM）解决低空无线网络（LAWNs）安全通信问题，提出基于LAM的优化框架，仿真验证其有效性并给出未来方向。


<details>
  <summary>Details</summary>
Motivation: LAWNs因低空操作、频繁移动和依赖非授权频谱，相比传统无线网络面临独特安全挑战，传统AI方法有局限性。

Method: 先探讨LAWNs中传统AI方法的安全风险和局限性，介绍LAM概念；提出基于LAM的优化框架，利用大语言模型（LLMs）生成增强状态特征和设计内在奖励，提高强化学习性能。

Result: 通过典型案例研究，仿真结果验证了所提框架的有效性。

Conclusion: 给出了将LAMs集成到安全LAWN应用的未来方向。

Abstract: Low-altitude wireless networks (LAWNs) have the potential to revolutionize
communications by supporting a range of applications, including urban parcel
delivery, aerial inspections and air taxis. However, compared with traditional
wireless networks, LAWNs face unique security challenges due to low-altitude
operations, frequent mobility and reliance on unlicensed spectrum, making it
more vulnerable to some malicious attacks. In this paper, we investigate some
large artificial intelligence model (LAM)-enabled solutions for secure
communications in LAWNs. Specifically, we first explore the amplified security
risks and important limitations of traditional AI methods in LAWNs. Then, we
introduce the basic concepts of LAMs and delve into the role of LAMs in
addressing these challenges. To demonstrate the practical benefits of LAMs for
secure communications in LAWNs, we propose a novel LAM-based optimization
framework that leverages large language models (LLMs) to generate enhanced
state features on top of handcrafted representations, and to design intrinsic
rewards accordingly, thereby improving reinforcement learning performance for
secure communication tasks. Through a typical case study, simulation results
validate the effectiveness of the proposed framework. Finally, we outline
future directions for integrating LAMs into secure LAWN applications.

</details>


### [144] [Towards Reliable AI in 6G: Detecting Concept Drift in Wireless Network](https://arxiv.org/abs/2508.00042)
*Athanasios Tziouvaras,Carolina Fortuna,George Floros,Kostas Kolomvatsos,Panagiotis Sarigiannidis,Marko Grobelnik,Blaž Bertalanič*

Main category: cs.NI

TL;DR: 本文针对AI原生6G网络中概念漂移致模型精度下降问题，提出两种无监督、模型无关的批量概念漂移检测器，通过真实用例验证其性能优于经典检测器。


<details>
  <summary>Details</summary>
Motivation: AI原生6G网络存在概念漂移使模型精度下降，现有方法有局限性。

Method: 引入两种无监督、模型无关的批量概念漂移检测器，计算期望效用分数判断概念漂移及是否需模型再训练。

Result: 在两个真实无线用例中，两种方法比经典检测器性能高20 - 40个百分点，F1分数高，降低误报率达20个百分点。

Conclusion: 提出的概念漂移检测器有效，性能优于经典检测器。

Abstract: AI-native 6G networks promise unprecedented automation and performance by
embedding machine-learning models throughout the radio access and core segments
of the network. However, the non-stationary nature of wireless environments due
to infrastructure changes, user mobility, and emerging traffic patterns,
induces concept drifts that can quickly degrade these model accuracies.
Existing methods in general are very domain specific, or struggle with certain
type of concept drift. In this paper, we introduce two unsupervised,
model-agnostic, batch concept drift detectors. Both methods compute an
expected-utility score to decide when concept drift occurred and if model
retraining is warranted, without requiring ground-truth labels after
deployment. We validate our framework on two real-world wireless use cases in
outdoor fingerprinting for localization and for link-anomaly detection, and
demonstrate that both methods are outperforming classical detectors such as
ADWIN, DDM, CUSUM by 20-40 percentage points. Additionally, they achieve an
F1-score of 0.94 and 1.00 in correctly triggering retraining alarm, thus
reducing the false alarm rate by up to 20 percentage points compared to the
best classical detectors.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [145] [XRoboToolkit: A Cross-Platform Framework for Robot Teleoperation](https://arxiv.org/abs/2508.00097)
*Zhigen Zhao,Liuchuan Yu,Ke Jing,Ning Yang*

Main category: cs.RO

TL;DR: 提出跨平台框架XRoboToolkit用于基于扩展现实的机器人远程操作，通过任务验证其有效性和数据质量。


<details>
  <summary>Details</summary>
Motivation: 视觉 - 语言 - 动作模型发展急需大规模、高质量机器人演示数据集，现有远程操作数据收集方法存在可扩展性有限、设置复杂和数据质量不佳等问题。

Method: 构建基于OpenXR标准的跨平台框架XRoboToolkit，具备低延迟立体视觉反馈、基于优化的逆运动学和多种跟踪模式支持，模块化架构可跨机器人平台和仿真环境集成。

Result: 通过精确操作任务展示框架有效性，训练的VLA模型表现出强大自主性能验证数据质量。

Conclusion: XRoboToolkit能有效解决现有远程操作数据收集方法的问题，可用于生成高质量机器人演示数据集。

Abstract: The rapid advancement of Vision-Language-Action models has created an urgent
need for large-scale, high-quality robot demonstration datasets. Although
teleoperation is the predominant method for data collection, current approaches
suffer from limited scalability, complex setup procedures, and suboptimal data
quality. This paper presents XRoboToolkit, a cross-platform framework for
extended reality based robot teleoperation built on the OpenXR standard. The
system features low-latency stereoscopic visual feedback, optimization-based
inverse kinematics, and support for diverse tracking modalities including head,
controller, hand, and auxiliary motion trackers. XRoboToolkit's modular
architecture enables seamless integration across robotic platforms and
simulation environments, spanning precision manipulators, mobile robots, and
dexterous hands. We demonstrate the framework's effectiveness through precision
manipulation tasks and validate data quality by training VLA models that
exhibit robust autonomous performance.

</details>


### [146] [HannesImitation: Grasping with the Hannes Prosthetic Hand via Imitation Learning](https://arxiv.org/abs/2508.00491)
*Carlo Alessi,Federico Vasile,Federico Ceola,Giulia Pasquale,Nicolò Boccardo,Lorenzo Natale*

Main category: cs.RO

TL;DR: 本文提出基于模仿学习的HannesImitationPolicy方法控制Hannes假肢手，创建数据集训练扩散策略，实验表明能成功抓握，且在非结构化场景中优于基于分割的视觉伺服控制器。


<details>
  <summary>Details</summary>
Motivation: 现有假肢手控制研究多关注提升自主性，模仿学习应用于假肢手控制未充分探索，填补此空白可增强灵活性恢复，使假肢在更多无约束场景操作。

Method: 提出HannesImitationPolicy方法，创建HannesImitationDataset数据集，用数据训练单一扩散策略并部署在假肢手上预测抓握的手腕方向和手部闭合。

Result: 实验评估显示在不同物体和条件下能成功抓握，策略在非结构化场景中优于基于分割的视觉伺服控制器。

Conclusion: 基于模仿学习的方法可有效控制假肢手，在非结构化环境中实现物体抓握，且性能良好。

Abstract: Recent advancements in control of prosthetic hands have focused on increasing
autonomy through the use of cameras and other sensory inputs. These systems aim
to reduce the cognitive load on the user by automatically controlling certain
degrees of freedom. In robotics, imitation learning has emerged as a promising
approach for learning grasping and complex manipulation tasks while simplifying
data collection. Its application to the control of prosthetic hands remains,
however, largely unexplored. Bridging this gap could enhance dexterity
restoration and enable prosthetic devices to operate in more unconstrained
scenarios, where tasks are learned from demonstrations rather than relying on
manually annotated sequences. To this end, we present HannesImitationPolicy, an
imitation learning-based method to control the Hannes prosthetic hand, enabling
object grasping in unstructured environments. Moreover, we introduce the
HannesImitationDataset comprising grasping demonstrations in table, shelf, and
human-to-prosthesis handover scenarios. We leverage such data to train a single
diffusion policy and deploy it on the prosthetic hand to predict the wrist
orientation and hand closure for grasping. Experimental evaluation demonstrates
successful grasps across diverse objects and conditions. Finally, we show that
the policy outperforms a segmentation-based visual servo controller in
unstructured scenarios. Additional material is provided on our project page:
https://hsp-iit.github.io/HannesImitation

</details>


### [147] [OmniUnet: A Multimodal Network for Unstructured Terrain Segmentation on Planetary Rovers Using RGB, Depth, and Thermal Imagery](https://arxiv.org/abs/2508.00580)
*Raul Castilla-Arquillo,Carlos Perez-del-Pulgar,Levin Gerdes,Alfonso Garcia-Cerezo,Miguel A. Olivares-Mendez*

Main category: cs.RO

TL;DR: 本文提出基于Transformer的OmniUnet网络用于RGB、深度和热成像语义分割，在模拟火星环境收集数据集训练评估，性能良好且代码数据公开。


<details>
  <summary>Details</summary>
Motivation: 非结构化环境中机器人导航需要多模态感知系统，火星探测中热成像对评估地形安全有价值，需确定对导航最有信息价值的传感器模态。

Method: 提出OmniUnet网络架构，用3D打印开发多模态传感器外壳，在西班牙半沙漠收集多模态数据集，手动标注部分数据进行监督训练，定量和定性评估模型。

Result: 模型像素准确率达80.37%，在复杂非结构化地形分割中表现良好，在资源受限计算机上平均预测时间673ms。

Conclusion: 模型适合机器人部署，公开软件实现和标注数据集以支持行星机器人多模态地形感知研究。

Abstract: Robot navigation in unstructured environments requires multimodal perception
systems that can support safe navigation. Multimodality enables the integration
of complementary information collected by different sensors. However, this
information must be processed by machine learning algorithms specifically
designed to leverage heterogeneous data. Furthermore, it is necessary to
identify which sensor modalities are most informative for navigation in the
target environment. In Martian exploration, thermal imagery has proven valuable
for assessing terrain safety due to differences in thermal behaviour between
soil types. This work presents OmniUnet, a transformer-based neural network
architecture for semantic segmentation using RGB, depth, and thermal (RGB-D-T)
imagery. A custom multimodal sensor housing was developed using 3D printing and
mounted on the Martian Rover Testbed for Autonomy (MaRTA) to collect a
multimodal dataset in the Bardenas semi-desert in northern Spain. This location
serves as a representative environment of the Martian surface, featuring
terrain types such as sand, bedrock, and compact soil. A subset of this dataset
was manually labeled to support supervised training of the network. The model
was evaluated both quantitatively and qualitatively, achieving a pixel accuracy
of 80.37% and demonstrating strong performance in segmenting complex
unstructured terrain. Inference tests yielded an average prediction time of 673
ms on a resource-constrained computer (Jetson Orin Nano), confirming its
suitability for on-robot deployment. The software implementation of the network
and the labeled dataset have been made publicly available to support future
research in multimodal terrain perception for planetary robotics.

</details>


### [148] [On-Device Diffusion Transformer Policy for Efficient Robot Manipulation](https://arxiv.org/abs/2508.00697)
*Yiming Wu,Huan Wang,Zhenghao Chen,Jianxin Pang,Dong Xu*

Main category: cs.RO

TL;DR: 提出LightDP框架加速扩散策略在移动设备上实时部署，通过网络压缩和减少采样步骤解决计算瓶颈，实验证明其实时性和性能表现。


<details>
  <summary>Details</summary>
Motivation: 扩散策略在资源受限移动平台应用因计算效率低和内存占用大而面临挑战，需加速以实现实时部署。

Method: 提出LightDP框架，采用网络压缩（统一剪枝和再训练管道）和减少采样步骤（结合剪枝技术与一致性蒸馏）两个核心策略。

Result: 在标准数据集实验中，LightDP在移动设备上实现实时动作预测，性能有竞争力；真实世界实验表明其性能与先进扩散策略相当。

Conclusion: LightDP是基于扩散策略在资源受限环境实际部署的重要一步。

Abstract: Diffusion Policies have significantly advanced robotic manipulation tasks via
imitation learning, but their application on resource-constrained mobile
platforms remains challenging due to computational inefficiency and extensive
memory footprint. In this paper, we propose LightDP, a novel framework
specifically designed to accelerate Diffusion Policies for real-time deployment
on mobile devices. LightDP addresses the computational bottleneck through two
core strategies: network compression of the denoising modules and reduction of
the required sampling steps. We first conduct an extensive computational
analysis on existing Diffusion Policy architectures, identifying the denoising
network as the primary contributor to latency. To overcome performance
degradation typically associated with conventional pruning methods, we
introduce a unified pruning and retraining pipeline, optimizing the model's
post-pruning recoverability explicitly. Furthermore, we combine pruning
techniques with consistency distillation to effectively reduce sampling steps
while maintaining action prediction accuracy. Experimental evaluations on the
standard datasets, \ie, PushT, Robomimic, CALVIN, and LIBERO, demonstrate that
LightDP achieves real-time action prediction on mobile devices with competitive
performance, marking an important step toward practical deployment of
diffusion-based policies in resource-limited environments. Extensive real-world
experiments also show the proposed LightDP can achieve performance comparable
to state-of-the-art Diffusion Policies.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [149] [Composable OS Kernel Architectures for Autonomous Intelligence](https://arxiv.org/abs/2508.00604)
*Rajpreet Singh,Vidhi Kothari*

Main category: cs.OS

TL;DR: 本文提出面向智能系统的新操作系统内核架构，使内核从静态资源管理器转变为自适应、集成AI的平台。


<details>
  <summary>Details</summary>
Motivation: 随着智能系统渗透到边缘设备、云基础设施和嵌入式实时环境，需要新的操作系统内核架构以适应智能应用需求。

Method: 将可加载内核模块作为面向AI的计算单元；将Linux内核扩展为支持深度学习推理等的AI原生环境；引入神经符号内核设计统一符号推理和可微逻辑。

Result: 使操作系统能够主动预测并适应自主智能应用的认知需求。

Conclusion: 所提出的新内核架构能有效支持智能系统，满足智能应用对操作系统的要求。

Abstract: As intelligent systems permeate edge devices, cloud infrastructure, and
embedded real-time environments, this research proposes a new OS kernel
architecture for intelligent systems, transforming kernels from static resource
managers to adaptive, AI-integrated platforms. Key contributions include: (1)
treating Loadable Kernel Modules (LKMs) as AI-oriented computation units for
fast sensory and cognitive processing in kernel space; (2) expanding the Linux
kernel into an AI-native environment with built-in deep learning inference,
floating-point acceleration, and real-time adaptive scheduling for efficient ML
workloads; and (3) introducing a Neurosymbolic kernel design leveraging
Category Theory and Homotopy Type Theory to unify symbolic reasoning and
differentiable logic within OS internals. Together, these approaches enable
operating systems to proactively anticipate and adapt to the cognitive needs of
autonomous intelligent applications.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [150] [Melody-Lyrics Matching with Contrastive Alignment Loss](https://arxiv.org/abs/2508.00123)
*Changhong Wang,Michel Olvera,Gaël Richard*

Main category: eess.AS

TL;DR: 提出旋律 - 歌词匹配新任务，用自监督学习框架及新歌词表示方法，实现旋律与歌词匹配并开源代码。


<details>
  <summary>Details</summary>
Motivation: 音乐与歌词的联系在音乐信息检索领域是少有人探索的方向，需挖掘旋律与歌词关系。

Method: 提出自监督表示学习框架，用对比对齐损失处理旋律与歌词，引入sylphone作为歌词音节级表示。

Result: 通过实证结果和直观例子证明方法能匹配旋律与连贯、可唱的歌词。

Conclusion: 该方法可有效实现旋律与歌词匹配，代码已开源。

Abstract: The connection between music and lyrics is far beyond semantic bonds.
Conceptual pairs in the two modalities such as rhythm and rhyme, note duration
and syllabic stress, and structure correspondence, raise a compelling yet
seldom-explored direction in the field of music information retrieval. In this
paper, we present melody-lyrics matching (MLM), a new task which retrieves
potential lyrics for a given symbolic melody from text sources. Rather than
generating lyrics from scratch, MLM essentially exploits the relationships
between melody and lyrics. We propose a self-supervised representation learning
framework with contrastive alignment loss for melody and lyrics. This has the
potential to leverage the abundance of existing songs with paired melody and
lyrics. No alignment annotations are required. Additionally, we introduce
sylphone, a novel representation for lyrics at syllable-level activated by
phoneme identity and vowel stress. We demonstrate that our method can match
melody with coherent and singable lyrics with empirical results and intuitive
examples. We open source code and provide matching examples on the companion
webpage: https://github.com/changhongw/mlm.

</details>


### [151] [Beamformed 360° Sound Maps: U-Net-Driven Acoustic Source Segmentation and Localization](https://arxiv.org/abs/2508.00307)
*Belman Jahir Rodriguez,Sergio F. Chevtchenko,Marcelo Herrera Martinez,Yeshwant Bethy,Saeed Afshar*

Main category: eess.AS

TL;DR: 本文引入U-net模型用于360°声源定位，将其作为球面语义分割任务，能适应不同麦克风配置，实验显示其泛化性好、精度高。


<details>
  <summary>Details</summary>
Motivation: 为360°声源定位提供新方法，解决传统方法问题，实现更优的声源定位。

Method: 将声源定位作为球面语义分割任务，用DAS波束形成生成信号制作监督掩码，训练改进的U-net模型，用Tversky损失解决类别不平衡，对分割输出后处理计算质心。

Result: U-net模型能在不同环境中泛化，提供更高的角度精度。

Conclusion: 该U-net模型为密集空间音频理解提供了新范式，超越传统声源定位方法。

Abstract: We introduce a U-net model for 360{\deg} acoustic source localization
formulated as a spherical semantic segmentation task. Rather than regressing
discrete direction-of-arrival (DoA) angles, our model segments beamformed audio
maps (azimuth and elevation) into regions of active sound presence. Using
delay-and-sum (DAS) beamforming on a custom 24-microphone array, we generate
signals aligned with drone GPS telemetry to create binary supervision masks. A
modified U-Net, trained on frequency-domain representations of these maps,
learns to identify spatially distributed source regions while addressing class
imbalance via the Tversky loss. Because the network operates on beamformed
energy maps, the approach is inherently array-independent and can adapt to
different microphone configurations without retraining from scratch. The
segmentation outputs are post-processed by computing centroids over activated
regions, enabling robust DoA estimates. Our dataset includes real-world
open-field recordings of a DJI Air 3 drone, synchronized with 360{\deg} video
and flight logs across multiple dates and locations. Experimental results show
that U-net generalizes across environments, providing improved angular
precision, offering a new paradigm for dense spatial audio understanding beyond
traditional Sound Source Localization (SSL).

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [152] [Leveraging Operator Learning to Accelerate Convergence of the Preconditioned Conjugate Gradient Method](https://arxiv.org/abs/2508.00101)
*Alena Kopaničáková,Youngkyu Lee,George Em Karniadakis*

Main category: math.NA

TL;DR: 提出基于DeepONet的放缩策略加速预条件共轭梯度法求解参数化大规模线性方程组，数值实验证明其有效性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 加速预条件共轭梯度法（PCG）求解参数化大规模线性方程组的收敛速度。

Method: 使用DeepONet生成放缩子空间，提出两种组装放缩算子的方法，还提出规定放缩算子稀疏模式的策略。

Result: 一组涵盖多种问题的数值实验表明所提基于DeepONet的放缩PCG方法有效。

Conclusion: 所提方法有效且能在广泛的模型参数和问题分辨率上泛化。

Abstract: We propose a new deflation strategy to accelerate the convergence of the
preconditioned conjugate gradient(PCG) method for solving parametric
large-scale linear systems of equations. Unlike traditional deflation
techniques that rely on eigenvector approximations or recycled Krylov
subspaces, we generate the deflation subspaces using operator learning,
specifically the Deep Operator Network~(DeepONet). To this aim, we introduce
two complementary approaches for assembling the deflation operators. The first
approach approximates near-null space vectors of the discrete PDE operator
using the basis functions learned by the DeepONet. The second approach directly
leverages solutions predicted by the DeepONet. To further enhance convergence,
we also propose several strategies for prescribing the sparsity pattern of the
deflation operator. A comprehensive set of numerical experiments encompassing
steady-state, time-dependent, scalar, and vector-valued problems posed on both
structured and unstructured geometries is presented and demonstrates the
effectiveness of the proposed DeepONet-based deflated PCG method, as well as
its generalization across a wide range of model parameters and problem
resolutions.

</details>


<div id='physics.ed-ph'></div>

# physics.ed-ph [[Back]](#toc)

### [153] [Advancing Quantum Information Science Pre-College Education: The Case for Learning Sciences Collaboration](https://arxiv.org/abs/2508.00668)
*Raquel Coelho,Roy Pea,Christian Schunn,Jinglei Cheng,Junyu Liu*

Main category: physics.ed-ph

TL;DR: 随着量子信息科学发展和对学前教育参与需求增加，本文提出需与学习科学跨学科合作推动量子信息科学教育，并阐述学习科学的两大贡献，呼吁建立双向合作。


<details>
  <summary>Details</summary>
Motivation: 解决如何让年轻学习者参与与以往截然不同的量子信息科学领域这一问题。

Method: 借鉴以往STEM教育经验，探讨学习科学对量子信息科学教育的两大贡献，即基于设计的研究和重塑学习者认知的框架。

Result: 明确学习科学对量子信息科学教育的两大贡献，呼吁建立双向合作。

Conclusion: 量子信息科学与学习科学的双向合作不仅能支持量子概念和实践学习，还能提升对复杂领域教学和学习支持的理解，理论和实践益处值得付出努力。

Abstract: As quantum information science advances and the need for pre-college
engagement grows, a critical question remains: How can young learners be
prepared to participate in a field so radically different from what they have
encountered before? This paper argues that meeting this challenge will require
strong interdisciplinary collaboration with the Learning Sciences (LS), a field
dedicated to understanding how people learn and designing theory-guided
environments to support learning. Drawing on lessons from previous STEM
education efforts, we discuss two key contributions of the learning sciences to
quantum information science (QIS) education. The first is design-based
research, the signature methodology of learning sciences, which can inform the
development, refinement, and scaling of effective QIS learning experiences. The
second is a framework for reshaping how learners reason about, learn and
participate in QIS practices through shifts in knowledge representations that
provide new forms of engagement and associated learning. We call for a two-way
partnership between quantum information science and the learning sciences, one
that not only supports learning in quantum concepts and practices but also
improves our understanding of how to teach and support learning in highly
complex domains. We also consider potential questions involved in bridging
these disciplinary communities and argue that the theoretical and practical
benefits justify the effort.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [154] [Ordinal Folding Index: A Computable Metric for Self-Referential Semantics](https://arxiv.org/abs/2508.00151)
*Faruk Alpay,Hamdi Al Alakkad*

Main category: cs.LO

TL;DR: 介绍新指标OFI，它关联多个研究领域，有诸多性质并提出开放问题，为多领域提供统一视角。


<details>
  <summary>Details</summary>
Motivation: 建立不同孤立研究领域间的直接联系，提供统一视角研究无限博弈、超限归纳和反思推理。

Method: 证明OFI对经典指标的改进，给出有限区域的多项式时间近似方案，揭示其与最短获胜策略长度的关系。

Result: OFI可细化经典指标，算法可枚举，有多项式时间近似方案，与最短获胜策略长度一致。

Conclusion: OFI为多领域提供共同基础，提出的开放问题可引导后续研究。

Abstract: The Ordinal Folding Index (OFI) is a new, fully computable yard-stick that
measures how many rounds of self-reference a statement, protocol or position
must unfold before its truth or outcome stabilises. By turning this abstract
'fold-back' depth into a single ordinal number, OFI forges a direct link
between areas that are usually studied in isolation: the closure stages of
fixed-point logics, the time-to-win values of infinite parity games, and the
ordinal progressions that calibrate the strength of formal theories. We prove
that OFI refines all classical game-theoretic and logical metrics while
remaining algorithmically enumerable, supply a polynomial-time approximation
scheme on finite arenas, and show how the index coincides exactly with the
length of the shortest winning strategy in the associated evaluation game.
Alongside the theory we outline five open problems from the completeness of the
computable-ordinal spectrum to the possibility of 'compressing' deep
self-reference that chart a research programme at the intersection of
computer-aided logic, algorithmic game theory and ordinal analysis. OFI thus
invites game theorists and logicians alike to view infinite play, transfinite
induction and reflective reasoning through a single, intuitive lens, opening
common ground for techniques.

</details>


### [155] [Generative Logic: A New Computer Architecture for Deterministic Reasoning and Knowledge Generation](https://arxiv.org/abs/2508.00017)
*Nikolai Sergeev*

Main category: cs.LO

TL;DR: 提出Generative Logic (GL)架构，以Peano算术为例展示其功能，还提及软硬件协同设计和与概率模型集成前景，代码和证明图公开。


<details>
  <summary>Details</summary>
Motivation: 开发一种从用户提供的公理定义出发，系统探索其演绎邻域的确定性架构。

Method: 将定义编译成分布式逻辑块网格，消息交换，应用推理规则生成新事实；以一阶Peano算术为例，枚举候选蕴含式，应用过滤，重建算术定律证明。

Result: 自动重建基础算术定律的机器可检查证明，证明可导出为HTML；提出软硬件协同设计路径和与概率模型集成设想。

Conclusion: GL架构有效可行，邀请社区反馈与合作。

Abstract: We present Generative Logic (GL), a deterministic architecture that begins
from user-supplied axiomatic definitions -- written in a minimalist
Mathematical Programming Language (MPL) -- and systematically explores their
deductive neighborhood. Definitions are compiled into a distributed grid of
simple Logic Blocks (LBs) that exchange messages; any time several expressions
unify under an inference rule, a new fact is emitted with full provenance to
its sources, yielding replayable, auditable proof graphs.
  A prototype software implementation instantiates the workflow on first-order
Peano arithmetic. Starting only from the Peano axioms, GL enumerates candidate
implications, applies normalization and type filters, and automatically
reconstructs machine-checkable proofs of foundational arithmetic laws including
associativity and commutativity of addition, associativity and commutativity of
multiplication, and distributivity. Generated proofs export to navigable HTML
so that every inference step can be inspected independently.
  We outline a hardware-software co-design path toward massively parallel
realizations and describe prospective integration with probabilistic models
(e.g., Large Language Models (LLMs)) for autoformalization and conjecture
seeding. The Python and MPL code to reproduce the Peano experiments, along with
the full HTML proof graphs, are available in the project's GitHub repository at
https://github.com/Generative-Logic/GL/tree/35a111ea9ba53afe051703d6050be0c3923e9724
and are permanently archived at https://doi.org/10.5281/zenodo.16408441. We
invite community feedback and collaboration.

</details>


### [156] [Analysing Temporal Reasoning in Description Logics Using Formal Grammars](https://arxiv.org/abs/2508.00575)
*Camille Bourgaux,Anton Gnatenko,Michaël Thomazo*

Main category: cs.LO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We establish a correspondence between (fragments of)
$\mathcal{TEL}^\bigcirc$, a temporal extension of the $\mathcal{EL}$
description logic with the LTL operator $\bigcirc^k$, and some specific kinds
of formal grammars, in particular, conjunctive grammars (context-free grammars
equipped with the operation of intersection). This connection implies that
$\mathcal{TEL}^\bigcirc$ does not possess the property of ultimate periodicity
of models, and further leads to undecidability of query answering in
$\mathcal{TEL}^\bigcirc$, closing a question left open since the introduction
of $\mathcal{TEL}^\bigcirc$. Moreover, it also allows to establish decidability
of query answering for some new interesting fragments of
$\mathcal{TEL}^\bigcirc$, and to reuse for this purpose existing tools and
algorithms for conjunctive grammars.

</details>


### [157] [Loop Invariant Generation: A Hybrid Framework of Reasoning optimised LLMs and SMT Solvers](https://arxiv.org/abs/2508.00419)
*Varun Bharti,Shashwat Jha,Dhruv Kumar,Pankaj Jalote*

Main category: cs.LO

TL;DR: 本文研究大语言模型在循环不变式合成中的应用，结合大语言模型和SMT求解器的框架在Code2Inv基准测试上取得100%覆盖率，证明LLMs有潜在逻辑推理能力且方法可推广。


<details>
  <summary>Details</summary>
Motivation: 现有循环不变式合成方法只能在部分标准基准测试子集上正确合成，研究现代推理优化的大语言模型能否表现更好。

Method: 将OpenAI的O1、O1 - mini和O3 - mini集成到与Z3 SMT求解器紧密耦合的生成 - 检查管道中，利用求解器反例迭代引导不变式细化。

Result: 在Code2Inv基准测试的133个任务中，框架实现100%覆盖率，优于之前的最佳结果，每个实例仅需1 - 2个模型提议，耗时14 - 55秒。

Conclusion: LLMs具备潜在逻辑推理能力，可帮助自动进行循环不变式合成，该方法可推广到其他命令式语言。

Abstract: Loop invariants are essential for proving the correctness of programs with
loops. Developing loop invariants is challenging, and fully automatic synthesis
cannot be guaranteed for arbitrary programs. Some approaches have been proposed
to synthesize loop invariants using symbolic techniques and more recently using
neural approaches. These approaches are able to correctly synthesize loop
invariants only for subsets of standard benchmarks. In this work, we
investigate whether modern, reasoning-optimized large language models can do
better. We integrate OpenAI's O1, O1-mini, and O3-mini into a tightly coupled
generate-and-check pipeline with the Z3 SMT solver, using solver
counterexamples to iteratively guide invariant refinement. We use Code2Inv
benchmark, which provides C programs along with their formal preconditions and
postconditions. On this benchmark of 133 tasks, our framework achieves 100%
coverage (133 out of 133), outperforming the previous best of 107 out of 133,
while requiring only 1-2 model proposals per instance and 14-55 seconds of
wall-clock time. These results demonstrate that LLMs possess latent logical
reasoning capabilities which can help automate loop invariant synthesis. While
our experiments target C-specific programs, this approach should be
generalizable to other imperative languages.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [158] [Are controlled unitaries helpful?](https://arxiv.org/abs/2508.00055)
*Ewin Tang,John Wright*

Main category: quant-ph

TL;DR: 本文指出对一大类量子问题，访问cU无帮助，可将使用cU和cU†的量子电路“去控制”为仅用U和U†的电路，还推广结果并给出应用。


<details>
  <summary>Details</summary>
Motivation: 推广Sheridan等人附录中的结果，反驳文献中认为去控制不可能的负面结论。

Method: 展示将使用cU和cU†的量子电路“去控制”为仅用U和U†的电路的方法。

Result: 成功将电路“去控制”，仅产生少量时空开销；给出简单证明说明存在在访问U、U†、cU和cU†时伪随机的酉系综。

Conclusion: 对于只关心U全局相位的输出态，去控制后的电路足够；cU的帮助仅在于包含U的全局相位信息。

Abstract: Many quantum algorithms, to compute some property of a unitary $U$, require
access not just to $U$, but to $cU$, the unitary with a control qubit. We show
that having access to $cU$ does not help for a large class of quantum problems.
For a quantum circuit which uses $cU$ and $cU^\dagger$ and outputs
$|\psi(U)\rangle$, we show how to ``decontrol'' the circuit into one which uses
only $U$ and $U^\dagger$ and outputs $|\psi(\varphi U)\rangle$ for a uniformly
random phase $\varphi$, with a small amount of time and space overhead. When we
only care about the output state up to a global phase on $U$, then the
decontrolled circuit suffices. Stated differently, $cU$ is only helpful because
it contains global phase information about $U$.
  A version of our procedure is described in an appendix of Sheridan, Maslov,
and Mosca [SMM09]. Our goal with this work is to popularize this result by
generalizing it and investigating its implications, in order to counter
negative results in the literature which might lead one to believe that
decontrolling is not possible. As an application, we give a simple proof for
the existence of unitary ensembles which are pseudorandom under access to $U$,
$U^\dagger$, $cU$, and $cU^\dagger$.

</details>


### [159] [Weak Values as Geometric Lenses: Deformations of Hilbert Space and the Emergence of superoscillations](https://arxiv.org/abs/2508.00023)
*Mirco A. Mannucci*

Main category: quant-ph

TL;DR: 本文推导无指针超振荡，揭示弱值与超振荡是同一几何原理的两面。


<details>
  <summary>Details</summary>
Motivation: 探索量子力学弱测量形式主义中测量理论、量子基础和信号处理的联系，理解弱值本质。

Method: 进行无指针超振荡推导，将弱值解释为变形的半双线性形式与标准形式的比较。

Result: 发现超振荡是弱值底层几何结构的自然且必要结果，揭示弱值与广义瑞利商和量子态射影几何的联系。

Conclusion: 弱值和超振荡是同一底层几何原理的两个方面。

Abstract: The formalism of weak measurement in quantum mechanics has revealed profound
connections between measurement theory, quantum foundations, and signal
processing. In this paper, we develop a pointer-free derivation of
superoscillations, demonstrating that they are a natural and necessary
consequence of the geometric structure underlying weak values. We argue that
the weak value is best understood as a ratio of geometric deformation,
quantifying how an observable transforms the structure of Hilbert space
relative to a reference provided by the standard inner product. This
deformation acts as a conceptual lens, warping the local structure of quantum
states to produce oscillations far exceeding the global Fourier bandwidth. We
formalize this by interpreting the weak value as a comparison between a
deformed sesquilinear form and the standard one, and explore its deep
connections to generalized Rayleigh quotients and the projective geometry of
quantum states. This perspective unifies weak values and superoscillations as
two facets of a single underlying geometric principle.

</details>


### [160] [Embedding-Aware Quantum-Classical SVMs for Scalable Quantum Machine Learning](https://arxiv.org/abs/2508.00024)
*Sebastián Andrés Cajas Ordóñez,Luis Fernando Torres Torres,Mario Bifulco,Carlos Andrés Durán,Cristian Bosch,Ricardo Simón Carbajo*

Main category: quant-ph

TL;DR: 提出嵌入感知的量子 - 经典管道，发现ViT嵌入能带来量子优势，为可扩展量子机器学习提供途径。


<details>
  <summary>Details</summary>
Motivation: 解决量子支持向量机因高维量子态和硬件限制面临的可扩展性挑战。

Method: 提出结合类平衡k - 均值蒸馏与预训练视觉Transformer嵌入的嵌入感知量子 - 经典管道，使用16量子比特张量网络通过cuTensorNet进行模拟。

Result: ViT嵌入在Fashion - MNIST和MNIST上比经典SVM分别提升8.02%和4.42%的准确率，CNN特征性能下降。

Conclusion: 量子核优势关键取决于嵌入选择，揭示了Transformer注意力和量子特征空间的协同作用，为可扩展量子机器学习提供实用途径。

Abstract: Quantum Support Vector Machines face scalability challenges due to
high-dimensional quantum states and hardware limitations. We propose an
embedding-aware quantum-classical pipeline combining class-balanced k-means
distillation with pretrained Vision Transformer embeddings. Our key finding:
ViT embeddings uniquely enable quantum advantage, achieving up to 8.02%
accuracy improvements over classical SVMs on Fashion-MNIST and 4.42% on MNIST,
while CNN features show performance degradation. Using 16-qubit tensor network
simulation via cuTensorNet, we provide the first systematic evidence that
quantum kernel advantage depends critically on embedding choice, revealing
fundamental synergy between transformer attention and quantum feature spaces.
This provides a practical pathway for scalable quantum machine learning that
leverages modern neural architectures.

</details>


### [161] [Quantum Semi-Random Forests for Qubit-Efficient Recommender Systems](https://arxiv.org/abs/2508.00027)
*Azadeh Alavi,Fatemeh Kouchmeshki,Abdolrahman Alavi,Yongli Ren,Jiayang Niu*

Main category: quant-ph

TL;DR: 提出三阶段混合机器学习算法，仅用5个量子比特实现推荐，效果与最先进方法相当。


<details>
  <summary>Details</summary>
Motivation: 现代推荐器需大量量子比特，当前NISQ设备难以满足，且易产生深度误差放大电路。

Method: 采用三阶段混合机器学习算法，包括压缩标签配置文件、通过QAOA优化特征选择、用量子半随机森林（QsRF）打分。

Result: 学习到1000原子字典，通过深度3的QAOA选择5个原子，100树的QsRF在ICM - 150/500上匹配全特征基线。

Conclusion: 该算法在量子比特受限情况下能达到与现有方法相似的推荐效果。

Abstract: Modern recommenders describe each item with hundreds of sparse semantic tags,
yet most quantum pipelines still map one qubit per tag, demanding well beyond
one hundred qubits, far out of reach for current noisy-intermediate-scale
quantum (NISQ) devices and prone to deep, error-amplifying circuits. We close
this gap with a three-stage hybrid machine learning algorithm that compresses
tag profiles, optimizes feature selection under a fixed qubit budget via QAOA,
and scores recommendations with a Quantum semi-Random Forest (QsRF) built on
just five qubits, while performing similarly to the state-of-the-art methods.
Leveraging SVD sketching and k-means, we learn a 1000-atom dictionary ($>$97 \%
variance), then solve a 2020 QUBO via depth-3 QAOA to select 5 atoms. A
100-tree QsRF trained on these codes matches full-feature baselines on
ICM-150/500.

</details>


### [162] [Hybrid Quantum Classical Surrogate for Real Time Inverse Finite Element Modeling in Digital Twins](https://arxiv.org/abs/2508.00029)
*Azadeh Alavi,Sanduni Jayasinghe,Mojtaba Mahmoodian,Sam Mazaheri,John Thangarajah,Sujeeva Setunge*

Main category: quant-ph

TL;DR: 提出混合量子经典多层感知器（QMLP）框架用于结构健康监测，实验显示性能远超经典基线，证实量子增强方法潜力。


<details>
  <summary>Details</summary>
Motivation: 有限元建模用于实时结构健康监测存在计算成本高和逆有限元分析复杂的问题。

Method: 采用对称正定矩阵和多项式特征嵌入传感器数据，经参数化量子电路变换后输入经典神经网络进行最终推理。

Result: 在桥梁实验中，QMLP均方误差为0.0000000000316，大幅优于纯经典基线。

Conclusion: 量子增强方法在实时结构健康监测中有潜力，为更高效、可扩展的数字孪生体发展提供途径。

Abstract: Large-scale civil structures, such as bridges, pipelines, and offshore
platforms, are vital to modern infrastructure, where unexpected failures can
cause significant economic and safety repercussions. Although finite element
(FE) modeling is widely used for real-time structural health monitoring (SHM),
its high computational cost and the complexity of inverse FE analysis, where
low dimensional sensor data must map onto high-dimensional displacement or
stress fields pose ongoing challenges. Here, we propose a hybrid quantum
classical multilayer perceptron (QMLP) framework to tackle these issues and
facilitate swift updates to digital twins across a range of structural
applications.
  Our approach embeds sensor data using symmetric positive definite (SPD)
matrices and polynomial features, yielding a representation well suited to
quantum processing. A parameterized quantum circuit (PQC) transforms these
features, and the resultant quantum outputs feed into a classical neural
network for final inference. By fusing quantum capabilities with classical
modeling, the QMLP handles large scale inverse FE mapping while preserving
computational viability.
  Through extensive experiments on a bridge, we demonstrate that the QMLP
achieves a mean squared error (MSE) of 0.0000000000316, outperforming purely
classical baselines with a large margin. These findings confirm the potential
of quantum-enhanced methods for real time SHM, establishing a pathway toward
more efficient, scalable digital twins that can robustly monitor and diagnose
structural integrity in near real time.

</details>


### [163] [Dimension reduction with structure-aware quantum circuits for hybrid machine learning](https://arxiv.org/abs/2508.00048)
*Ammar Daskin*

Main category: quant-ph

TL;DR: 本文提出基于量子电路的混合机器学习模型，可对数据进行指数级压缩并实现有效近似。


<details>
  <summary>Details</summary>
Motivation: 为了在保留向量重要部分的同时去除噪声，实现数据压缩并降低大规模模型训练的可学习参数数量。

Method: 基于训练样本均值向量的张量网络分解确定值k，设计量子电路近似数据集的简化形式表示，结合经典神经网络构建混合机器学习模型。

Result: 使用Python scikit - learn模块的数据集进行实验，结果表明量子电路能成功压缩数据，为经典处理组件提供有效的k阶近似。

Conclusion: 所设计的量子电路能对数据进行有效压缩和近似，具有降低大规模模型训练参数数量的潜力。

Abstract: Schmidt decomposition of a vector can be understood as writing the singular
value decomposition (SVD) in vector form. A vector can be written as a linear
combination of tensor product of two dimensional vectors by recursively
applying Schmidt decompositions via SVD to all subsystems. Given a vector
expressed as a linear combination of tensor products, using only the $k$
principal terms yields a $k$-rank approximation of the vector. Therefore,
writing a vector in this reduced form allows to retain most important parts of
the vector while removing small noises from it, analogous to SVD-based
denoising.
  In this paper, we show that quantum circuits designed based on a value $k$
(determined from the tensor network decomposition of the mean vector of the
training sample) can approximate the reduced-form representations of entire
datasets. We then employ this circuit ansatz with a classical neural network
head to construct a hybrid machine learning model. Since the output of the
quantum circuit for an $2^n$ dimensional vector is an $n$ dimensional
probability vector, this provides an exponential compression of the input and
potentially can reduce the number of learnable parameters for training
large-scale models. We use datasets provided in the Python scikit-learn module
for the experiments. The results confirm the quantum circuit is able to
compress data successfully to provide effective $k$-rank approximations to the
classical processing component.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [164] [Asymptotically Optimal Inapproximability of E$k$-SAT Reconfiguration](https://arxiv.org/abs/2508.00276)
*Shuichi Hirahara,Naoto Ohsaka*

Main category: cs.CC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In the Maxmin E$k$-SAT Reconfiguration problem, we are given a satisfiable
$k$-CNF formula $\varphi$ where each clause contains exactly $k$ literals,
along with a pair of its satisfying assignments. The objective is transform one
satisfying assignment into the other by repeatedly flipping the value of a
single variable, while maximizing the minimum fraction of satisfied clauses of
$\varphi$ throughout the transformation. In this paper, we demonstrate that the
optimal approximation factor for Maxmin E$k$-SAT Reconfiguration is $1 -
\Theta\left(\frac{1}{k}\right)$. On the algorithmic side, we develop a
deterministic $\left(1-\frac{1}{k-1}-\frac{1}{k}\right)$-factor approximation
algorithm for every $k \geq 3$. On the hardness side, we show that it is
$\mathsf{PSPACE}$-hard to approximate this problem within a factor of
$1-\frac{1}{10k}$ for every sufficiently large $k$. Note that an
``$\mathsf{NP}$ analogue'' of Maxmin E$k$-SAT Reconfiguration is Max E$k$-SAT,
whose approximation threshold is $1-\frac{1}{2^k}$ shown by H\r{a}stad (JACM
2001). To the best of our knowledge, this is the first reconfiguration problem
whose approximation threshold is (asymptotically) worse than that of its
$\mathsf{NP}$ analogue. To prove the hardness result, we introduce a new
``non-monotone'' test, which is specially tailored to reconfiguration problems,
despite not being helpful in the PCP regime.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [165] [Efficient Solving of Large Single Input Superstate Decomposable Markovian Decision Process](https://arxiv.org/abs/2508.00816)
*Youssef Ait El Mahjoub,Jean-Michel Fourneau,Salma Alouah*

Main category: math.OC

TL;DR: 文章将聚合与分解技术扩展到一类结构化MDPs，定义SISDMDP，开发高效策略评估方法，适用于平均和折扣奖励MDPs。


<details>
  <summary>Details</summary>
Motivation: 解决MDPs在处理大状态空间和长期优化标准时的挑战，降低Bellman动态规划算法中策略评估的计算复杂度。

Method: 定义SISDMDP，结合Chiu的单输入分解和Robertazzi的单周期递归属性，基于此结构开发精确高效的策略评估方法。

Result: 得到可扩展的解决方案，适用于平均和折扣奖励MDPs。

Conclusion: 提出的方法可有效解决MDPs在特定情况下的计算难题。

Abstract: Solving Markov Decision Processes (MDPs) remains a central challenge in
sequential decision-making, especially when dealing with large state spaces and
long-term optimization criteria. A key step in Bellman dynamic programming
algorithms is the policy evaluation, which becomes computationally demanding in
infinite-horizon settings such as average-reward or discounted-reward
formulations. In the context of Markov chains, aggregation and disaggregation
techniques have for a long time been used to reduce complexity by exploiting
structural decompositions. In this work, we extend these principles to a
structured class of MDPs. We define the Single-Input Superstate Decomposable
Markov Decision Process (SISDMDP), which combines Chiu's single-input
decomposition with Robertazzi's single-cycle recurrence property. When a policy
induces this structure, the resulting transition graph can be decomposed into
interacting components with centralized recurrence. We develop an exact and
efficient policy evaluation method based on this structure. This yields a
scalable solution applicable to both average and discounted reward MDPs.

</details>


### [166] [Riemannian Optimization for Distance Geometry: A Study of Convergence, Robustness, and Incoherence](https://arxiv.org/abs/2508.00091)
*Chandler Smith,HanQin Cai,Abiy Tasissa*

Main category: math.OC

TL;DR: 提出黎曼优化框架解决欧几里得距离几何问题，证明收敛性，有理论分析和实证评估。


<details>
  <summary>Details</summary>
Motivation: 欧几里得距离几何问题在多领域有应用，需有效解决方法。

Method: 将问题转化为低秩矩阵补全任务，利用黎曼梯度下降优化，使用一步硬阈值初始化。

Result: 在伯努利采样模型下证明局部线性收敛，实证评估算法性能有竞争力。

Conclusion: 提出的方法有效，有新的矩阵非相干性概念和鲁棒性保证。

Abstract: The problem of recovering a configuration of points from partial pairwise
distances, referred to as the Euclidean Distance Geometry (EDG) problem, arises
in a broad range of applications, including sensor network localization,
molecular conformation, and manifold learning. In this paper, we propose a
Riemannian optimization framework for solving the EDG problem by formulating it
as a low-rank matrix completion task over the space of positive semi-definite
Gram matrices. The available distance measurements are encoded as expansion
coefficients in a non-orthogonal basis, and optimization over the Gram matrix
implicitly enforces geometric consistency through the triangle inequality, a
structure inherited from classical multidimensional scaling. Under a Bernoulli
sampling model for observed distances, we prove that Riemannian gradient
descent on the manifold of rank-$r$ matrices locally converges linearly with
high probability when the sampling probability satisfies $p \geq
\mathcal{O}(\nu^2 r^2 \log(n)/n)$, where $\nu$ is an EDG-specific incoherence
parameter. Furthermore, we provide an initialization candidate using a one-step
hard thresholding procedure that yields convergence, provided the sampling
probability satisfies $p \geq \mathcal{O}(\nu r^{3/2} \log^{3/4}(n)/n^{1/4})$.
A key technical contribution of this work is the analysis of a symmetric linear
operator arising from a dual basis expansion in the non-orthogonal basis, which
requires a novel application of the Hanson--Wright inequality to establish an
optimal restricted isometry property in the presence of coupled terms.
Empirical evaluations on synthetic data demonstrate that our algorithm achieves
competitive performance relative to state-of-the-art methods. Moreover, we
propose a novel notion of matrix incoherence tailored to the EDG setting and
provide robustness guarantees for our method.

</details>


### [167] [Neighbor-Sampling Based Momentum Stochastic Methods for Training Graph Neural Networks](https://arxiv.org/abs/2508.00267)
*Molly Noel,Gabriel Mancino-Ball,Yangyang Xu*

Main category: math.OC

TL;DR: 本文提出基于邻居采样的Adam型随机方法解决非凸GCN训练问题，证明方法有最优收敛率，实验显示其性能优。


<details>
  <summary>Details</summary>
Motivation: 现有GCN高效训练方法缺乏理论保证或缺少现代深度学习算法实用元素。

Method: 提出基于邻居采样的Adam型随机方法，利用控制变量技术减少邻居采样的随机误差。

Result: 方法在标准假设下有最优收敛率，实验显示在节点分类任务中性能优于经典基于邻居采样的SGD。

Conclusion: 所提方法在GCN训练中表现出色，尤其适用于大规模图数据集。

Abstract: Graph convolutional networks (GCNs) are a powerful tool for graph
representation learning. Due to the recursive neighborhood aggregations
employed by GCNs, efficient training methods suffer from a lack of theoretical
guarantees or are missing important practical elements from modern deep
learning algorithms, such as adaptivity and momentum. In this paper, we present
several neighbor-sampling (NS) based Adam-type stochastic methods for solving a
nonconvex GCN training problem. We utilize the control variate technique
proposed by [1] to reduce the stochastic error caused by neighbor sampling.
Under standard assumptions for Adam-type methods, we show that our methods
enjoy the optimal convergence rate. In addition, we conduct extensive numerical
experiments on node classification tasks with several benchmark datasets. The
results demonstrate superior performance of our methods over classic NS-based
SGD that also uses the control-variate technique, especially for large-scale
graph datasets. Our code is available at https://github.com/RPI-OPT/CV-ADAM-GNN .

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [168] [FedGuard: A Diverse-Byzantine-Robust Mechanism for Federated Learning with Major Malicious Clients](https://arxiv.org/abs/2508.00636)
*Haocheng Jiang,Hua Shen,Jixin Zhang,Willy Susilo,Mingwu Zhang*

Main category: cs.CR

TL;DR: 提出FedGuard机制应对联邦学习中的拜占庭攻击，在高比例恶意客户端和多种攻击类型下效果优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 联邦学习易受拜占庭攻击，尤其在高比例恶意客户端或数据非IID时，且现有防御机制针对特定攻击类型，效果有限。

Method: 利用成员推理对模型偏差的高敏感性，让客户端在训练中加入服务器指定的小批量数据，识别并排除中毒模型。

Result: 在三个高度非IID数据集上，90%客户端为拜占庭且每轮有七种不同拜占庭攻击时，FedGuard显著优于现有鲁棒联邦学习方案。

Conclusion: FedGuard能有效缓解联邦学习中的各种拜占庭攻击。

Abstract: Federated learning is a distributed training framework vulnerable to
Byzantine attacks, particularly when over 50% of clients are malicious or when
datasets are highly non-independent and identically distributed (non-IID).
Additionally, most existing defense mechanisms are designed for specific attack
types (e.g., gradient similarity-based schemes can only defend against outlier
model poisoning), limiting their effectiveness. In response, we propose
FedGuard, a novel federated learning mechanism. FedGuard cleverly addresses the
aforementioned issues by leveraging the high sensitivity of membership
inference to model bias. By requiring clients to include an additional
mini-batch of server-specified data in their training, FedGuard can identify
and exclude poisoned models, as their confidence in the mini-batch will drop
significantly. Our comprehensive evaluation unequivocally shows that, under
three highly non-IID datasets, with 90% of clients being Byzantine and seven
different types of Byzantine attacks occurring in each round, FedGuard
significantly outperforms existing robust federated learning schemes in
mitigating various types of Byzantine attacks.

</details>


### [169] [Unveiling Dynamic Binary Instrumentation Techniques](https://arxiv.org/abs/2508.00682)
*Oscar Llorente-Vazquez,Xabier Ugarte-Pedrero,Igor Santos-Grueiro,Pablo Garcia Bringas*

Main category: cs.CR

TL;DR: 本文梳理动态二进制插桩（DBI），对比不同方法并评估性能，发现无技术能在所有情况下最优。


<details>
  <summary>Details</summary>
Motivation: DBI有多种基于不同技术和实现方法的方案，各有优缺点，需梳理和对比。

Method: 将进程级和全系统方法结合，描绘构建模块，分析底层插桩技术，对比插桩不同原语和运行时事件的能力，并评估性能。

Result: 没有一种技术在所有情况下都优于其他技术。

Conclusion: 在选择DBI技术时，需要根据具体情况权衡不同技术的优劣。

Abstract: Dynamic Binary Instrumentation (DBI) is the set of techniques that enable
instrumentation of programs at run-time, making it possible to monitor and
modify the execution of compiled binaries or entire systems. DBI is used for
countless security applications and analyses, and is extensively used across
many fields in both industry and academia. Over the years, several DBI
approaches have been proposed based on different technologies and implementing
diverse techniques. Every solution tries to overcome certain limitations, but
they sometimes bring other shortcomings. Some are specialized for one
particular domain or task, while others have a wider scope.
  In this paper, we shed light into the labyrinth of DBI, bringing together
process-level and whole-system approaches. We depict their building blocks and
analyze the underlying instrumentation techniques, comparing their ability to
instrument different primitives and run-time events. Then, we evaluate their
performance when implementing each primitive, and highlight relevant
observations. Our results show that no single technique is better than the rest
in all circumstances.

</details>


### [170] [CyGATE: Game-Theoretic Cyber Attack-Defense Engine for Patch Strategy Optimization](https://arxiv.org/abs/2508.00478)
*Yuning Jiang,Nay Oo,Qiaoran Meng,Lu Lin,Dusit Niyato,Zehui Xiong,Hoon Wei Lim,Biplab Sikdar*

Main category: cs.CR

TL;DR: 本文提出CyGATE框架，用大语言模型和检索增强生成技术处理攻防交互，在动态补丁调度场景中有效提升适应性和资源利用效率。


<details>
  <summary>Details</summary>
Motivation: 现有博弈论模型在处理网络攻击时依赖静态假设，缺乏与实时威胁情报的整合，适应性有限。

Method: 提出CyGATE框架，用大语言模型与检索增强生成技术，将网络冲突建模为部分可观察随机博弈，让攻防双方基于信念状态决策。

Result: 在动态补丁调度场景中，CyGATE能有效对高风险漏洞进行优先级排序。

Conclusion: CyGATE框架通过动态威胁集成、战略预见和资源优化，增强了适应性。

Abstract: Modern cyber attacks unfold through multiple stages, requiring defenders to
dynamically prioritize mitigations under uncertainty. While game-theoretic
models capture attacker-defender interactions, existing approaches often rely
on static assumptions and lack integration with real-time threat intelligence,
limiting their adaptability. This paper presents CyGATE, a game-theoretic
framework modeling attacker-defender interactions, using large language models
(LLMs) with retrieval-augmented generation (RAG) to enhance tactic selection
and patch prioritization. Applied to a two-agent scenario, CyGATE frames cyber
conflicts as a partially observable stochastic game (POSG) across Cyber Kill
Chain stages. Both agents use belief states to navigate uncertainty, with the
attacker adapting tactics and the defender re-prioritizing patches based on
evolving risks and observed adversary behavior. The framework's flexible
architecture enables extension to multi-agent scenarios involving coordinated
attackers, collaborative defenders, or complex enterprise environments with
multiple stakeholders. Evaluated in a dynamic patch scheduling scenario, CyGATE
effectively prioritizes high-risk vulnerabilities, enhancing adaptability
through dynamic threat integration, strategic foresight by anticipating
attacker moves under uncertainty, and efficiency by optimizing resource use.

</details>


### [171] [Activation-Guided Local Editing for Jailbreaking Attacks](https://arxiv.org/abs/2508.00555)
*Jiecong Wang,Haoran Li,Hao Peng,Ziqian Zeng,Zihao Wang,Haohua Du,Zhengtao Yu*

Main category: cs.CR

TL;DR: 提出两阶段框架AGILE进行越狱攻击，实验表明其攻击成功率高、可迁移性好，还分析了对防御机制的效果。


<details>
  <summary>Details</summary>
Motivation: 现有越狱方法存在显著缺点，如token级攻击输入不连贯、可迁移性差，提示级攻击缺乏可扩展性且依赖人工。

Method: 提出两阶段框架，第一阶段基于场景生成上下文并改写恶意查询，第二阶段利用模型隐藏状态信息进行细粒度编辑。

Result: 该方法达到了最先进的攻击成功率，比最强基线提高了37.74%，对黑盒模型有良好的可迁移性。

Conclusion: AGILE对主要防御机制仍有显著效果，凸显了当前防护措施的局限性，为未来防御发展提供了有价值的见解。

Abstract: Jailbreaking is an essential adversarial technique for red-teaming these
models to uncover and patch security flaws. However, existing jailbreak methods
face significant drawbacks. Token-level jailbreak attacks often produce
incoherent or unreadable inputs and exhibit poor transferability, while
prompt-level attacks lack scalability and rely heavily on manual effort and
human ingenuity. We propose a concise and effective two-stage framework that
combines the advantages of these approaches. The first stage performs a
scenario-based generation of context and rephrases the original malicious query
to obscure its harmful intent. The second stage then utilizes information from
the model's hidden states to guide fine-grained edits, effectively steering the
model's internal representation of the input from a malicious toward a benign
one. Extensive experiments demonstrate that this method achieves
state-of-the-art Attack Success Rate, with gains of up to 37.74% over the
strongest baseline, and exhibits excellent transferability to black-box models.
Our analysis further demonstrates that AGILE maintains substantial
effectiveness against prominent defense mechanisms, highlighting the
limitations of current safeguards and providing valuable insights for future
defense development. Our code is available at
https://github.com/yunsaijc/AGILE.

</details>


### [172] [Preliminary Investigation into Uncertainty-Aware Attack Stage Classification](https://arxiv.org/abs/2508.00368)
*Alessandro Gaudenzi,Lorenzo Nodari,Lance Kaplan,Alessandra Russo,Murat Sensoy,Federico Cerutti*

Main category: cs.CR

TL;DR: 本文提出基于证据深度学习的分类方法解决不确定下的攻击阶段推断问题，初步实验证明该模型可准确推断攻击阶段并检测OOD输入。


<details>
  <summary>Details</summary>
Motivation: 传统检测系统未考虑攻击进展，有效响应策略依赖准确推断攻击阶段，需解决不确定下攻击阶段推断问题并增强对OOD输入的鲁棒性。

Method: 提出基于证据深度学习（EDL）的分类方法，通过输出狄利克雷分布参数对预测不确定性建模。

Result: 在模拟环境初步实验表明，模型能以校准的置信度准确推断攻击阶段并有效检测OOD输入。

Conclusion: 支持在动态和对抗环境中部署不确定性感知模型进行分阶段威胁检测的可行性。

Abstract: Advanced Persistent Threats (APTs) represent a significant challenge in
cybersecurity due to their prolonged, multi-stage nature and the sophistication
of their operators. Traditional detection systems typically focus on
identifying malicious activity in binary terms (benign or malicious) without
accounting for the progression of an attack. However, effective response
strategies depend on accurate inference of the attack's current stage, as
countermeasures must be tailored to whether an adversary is in the early
reconnaissance phase or actively conducting exploitation or exfiltration. This
work addresses the problem of attack stage inference under uncertainty, with a
focus on robustness to out-of-distribution (OOD) inputs. We propose a
classification approach based on Evidential Deep Learning (EDL), which models
predictive uncertainty by outputting parameters of a Dirichlet distribution
over possible stages. This allows the system not only to predict the most
likely stage of an attack but also to indicate when it is uncertain or the
input lies outside the training distribution. Preliminary experiments in a
simulated environment demonstrate that the proposed model can accurately infer
the stage of an attack with calibrated confidence while effectively detecting
OOD inputs, which may indicate changes in the attackers' tactics. These results
support the feasibility of deploying uncertainty-aware models for staged threat
detection in dynamic and adversarial environments.

</details>


### [173] [LeakSealer: A Semisupervised Defense for LLMs Against Prompt Injection and Leakage Attacks](https://arxiv.org/abs/2508.00602)
*Francesco Panebianco,Stefano Bonfanti,Francesco Trovò,Michele Carminati*

Main category: cs.CR

TL;DR: 本文指出大语言模型广泛应用带来安全威胁，提出分析历史交互数据生成使用地图的方法，以及LeakSealer框架，并进行了实验评估，取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型广泛应用引入越狱和数据泄露等安全威胁，检索增强生成也带来敏感信息泄露漏洞。

Method: 一是分析大语言模型系统历史交互数据生成按主题分类的使用地图；二是提出LeakSealer框架，结合静态分析和动态防御，在人工参与的管道中识别主题组和异常模式。

Result: 在静态设置下，LeakSealer在ToxicChat数据集上识别提示注入时达到最高精度和召回率；在动态设置下，PII泄漏检测的AUPRC为0.97，显著优于Llama Guard等基线。

Conclusion: 所提出的方法和框架能有效应对大语言模型的安全威胁，具有良好的防御效果。

Abstract: The generalization capabilities of Large Language Models (LLMs) have led to
their widespread deployment across various applications. However, this
increased adoption has introduced several security threats, notably in the
forms of jailbreaking and data leakage attacks. Additionally, Retrieval
Augmented Generation (RAG), while enhancing context-awareness in LLM responses,
has inadvertently introduced vulnerabilities that can result in the leakage of
sensitive information. Our contributions are twofold. First, we introduce a
methodology to analyze historical interaction data from an LLM system, enabling
the generation of usage maps categorized by topics (including adversarial
interactions). This approach further provides forensic insights for tracking
the evolution of jailbreaking attack patterns. Second, we propose LeakSealer, a
model-agnostic framework that combines static analysis for forensic insights
with dynamic defenses in a Human-In-The-Loop (HITL) pipeline. This technique
identifies topic groups and detects anomalous patterns, allowing for proactive
defense mechanisms. We empirically evaluate LeakSealer under two scenarios: (1)
jailbreak attempts, employing a public benchmark dataset, and (2) PII leakage,
supported by a curated dataset of labeled LLM interactions. In the static
setting, LeakSealer achieves the highest precision and recall on the ToxicChat
dataset when identifying prompt injection. In the dynamic setting, PII leakage
detection achieves an AUPRC of $0.97$, significantly outperforming baselines
such as Llama Guard.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [174] [Advancing Welding Defect Detection in Maritime Operations via Adapt-WeldNet and Defect Detection Interpretability Analysis](https://arxiv.org/abs/2508.00381)
*Kamal Basha S,Athira Nambiar*

Main category: cs.CV

TL;DR: 提出Adapt - WeldNet框架优化焊接缺陷检测，提出DDIA框架增强系统可解释性，提升检测系统的信任、安全和可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统无损检测方法难检测细微或内部缺陷，现有基于神经网络的方法依赖任意选择的预训练架构且缺乏可解释性。

Method: 引入Adapt - WeldNet框架评估预训练架构、迁移学习策略和自适应优化器；提出DDIA框架，采用XAI技术和领域特定评估，结合人在回路方法。

Result: 未明确提及具体实验结果，但表明可优化缺陷检测并提供可操作见解。

Conclusion: 改进性能和可解释性，增强焊接缺陷检测系统的信任、安全和可靠性，支持海洋和近海环境关键操作。

Abstract: Weld defect detection is crucial for ensuring the safety and reliability of
piping systems in the oil and gas industry, especially in challenging marine
and offshore environments. Traditional non-destructive testing (NDT) methods
often fail to detect subtle or internal defects, leading to potential failures
and costly downtime. Furthermore, existing neural network-based approaches for
defect classification frequently rely on arbitrarily selected pretrained
architectures and lack interpretability, raising safety concerns for
deployment. To address these challenges, this paper introduces
``Adapt-WeldNet", an adaptive framework for welding defect detection that
systematically evaluates various pre-trained architectures, transfer learning
strategies, and adaptive optimizers to identify the best-performing model and
hyperparameters, optimizing defect detection and providing actionable insights.
Additionally, a novel Defect Detection Interpretability Analysis (DDIA)
framework is proposed to enhance system transparency. DDIA employs Explainable
AI (XAI) techniques, such as Grad-CAM and LIME, alongside domain-specific
evaluations validated by certified ASNT NDE Level II professionals.
Incorporating a Human-in-the-Loop (HITL) approach and aligning with the
principles of Trustworthy AI, DDIA ensures the reliability, fairness, and
accountability of the defect detection system, fostering confidence in
automated decisions through expert validation. By improving both performance
and interpretability, this work enhances trust, safety, and reliability in
welding defect detection systems, supporting critical operations in offshore
and marine environments.

</details>


### [175] [$MV_{Hybrid}$: Improving Spatial Transcriptomics Prediction with Hybrid State Space-Vision Transformer Backbone in Pathology Vision Foundation Models](https://arxiv.org/abs/2508.00383)
*Won June Cho,Hongjun Yoon,Daeky Jeong,Hyeongyeol Lim,Yosep Chong*

Main category: cs.CV

TL;DR: 本文提出结合状态空间模型和ViT的混合架构$MV_{Hybrid}$用于病理视觉基础模型，在基因表达预测等任务中表现优于ViT，代码开源。


<details>
  <summary>Details</summary>
Motivation: 空间转录组学成本高、技术复杂，当前基于ViT的病理视觉基础模型未达临床标准，需新架构更好捕捉与分子表型相关的形态模式。

Method: 引入结合状态空间模型和ViT的$MV_{Hybrid}$架构，对比其他五种不同骨干架构，用DINOv2自监督学习方法在相同结直肠癌数据集上预训练，用随机分割和留一研究法评估。

Result: 在留一研究评估中，$MV_{Hybrid}$在基因表达预测上比最佳ViT相关性高57%，性能下降小43%，在分类等下游任务表现相当或更好。

Conclusion: $MV_{Hybrid}$作为下一代病理视觉基础模型骨干架构很有前景。

Abstract: Spatial transcriptomics reveals gene expression patterns within tissue
context, enabling precision oncology applications such as treatment response
prediction, but its high cost and technical complexity limit clinical adoption.
Predicting spatial gene expression (biomarkers) from routine histopathology
images offers a practical alternative, yet current vision foundation models
(VFMs) in pathology based on Vision Transformer (ViT) backbones perform below
clinical standards. Given that VFMs are already trained on millions of diverse
whole slide images, we hypothesize that architectural innovations beyond ViTs
may better capture the low-frequency, subtle morphological patterns correlating
with molecular phenotypes. By demonstrating that state space models initialized
with negative real eigenvalues exhibit strong low-frequency bias, we introduce
$MV_{Hybrid}$, a hybrid backbone architecture combining state space models
(SSMs) with ViT. We compare five other different backbone architectures for
pathology VFMs, all pretrained on identical colorectal cancer datasets using
the DINOv2 self-supervised learning method. We evaluate all pretrained models
using both random split and leave-one-study-out (LOSO) settings of the same
biomarker dataset. In LOSO evaluation, $MV_{Hybrid}$ achieves 57% higher
correlation than the best-performing ViT and shows 43% smaller performance
degradation compared to random split in gene expression prediction,
demonstrating superior performance and robustness, respectively. Furthermore,
$MV_{Hybrid}$ shows equal or better downstream performance in classification,
patch retrieval, and survival prediction tasks compared to that of ViT, showing
its promise as a next-generation pathology VFM backbone. Our code is publicly
available at: https://github.com/deepnoid-ai/MVHybrid.

</details>


### [176] [Context-based Motion Retrieval using Open Vocabulary Methods for Autonomous Driving](https://arxiv.org/abs/2508.00589)
*Stefan Englmeier,Max A. Büttner,Katharina Winter,Fabian B. Flohr*

Main category: cs.CV

TL;DR: 提出上下文感知运动检索框架，扩展Waymo Open Dataset为WayMoCo数据集，在运动上下文检索上优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统需在涉及弱势道路使用者的边缘场景可靠运行，从大规模数据集中检索此类罕见人类行为场景有挑战，需支持多样化以人类为中心场景的评估。

Method: 结合基于SMPL的运动序列和视频帧，编码到与自然语言对齐的多模态嵌入空间，通过文本查询检索人类行为及其上下文；引入WayMoCo数据集。

Result: 在WayMoCo数据集上，运动上下文检索准确率比现有模型最高提升27.5%。

Conclusion: 所提上下文感知运动检索框架有效，能用于自动驾驶系统在多样化场景的评估。

Abstract: Autonomous driving systems must operate reliably in safety-critical
scenarios, particularly those involving unusual or complex behavior by
Vulnerable Road Users (VRUs). Identifying these edge cases in driving datasets
is essential for robust evaluation and generalization, but retrieving such rare
human behavior scenarios within the long tail of large-scale datasets is
challenging. To support targeted evaluation of autonomous driving systems in
diverse, human-centered scenarios, we propose a novel context-aware motion
retrieval framework. Our method combines Skinned Multi-Person Linear
(SMPL)-based motion sequences and corresponding video frames before encoding
them into a shared multimodal embedding space aligned with natural language.
Our approach enables the scalable retrieval of human behavior and their context
through text queries. This work also introduces our dataset WayMoCo, an
extension of the Waymo Open Dataset. It contains automatically labeled motion
and scene context descriptions derived from generated pseudo-ground-truth SMPL
sequences and corresponding image data. Our approach outperforms
state-of-the-art models by up to 27.5% accuracy in motion-context retrieval,
when evaluated on the WayMoCo dataset.

</details>


### [177] [Punching Bag vs. Punching Person: Motion Transferability in Videos](https://arxiv.org/abs/2508.00085)
*Raiyaan Abdullah,Jared Claypoole,Michael Cogswell,Ajay Divakaran,Yogesh Rawat*

Main category: cs.CV

TL;DR: 本文引入运动可迁移性框架评估13个模型，发现模型在新场景识别高级动作性能下降，还探索分离粗细动作改善识别的方法。


<details>
  <summary>Details</summary>
Motivation: 探究动作识别模型能否在不同甚至相似分布的情境中有效迁移高级运动概念。

Method: 引入运动可迁移性框架，使用三个数据集（Syn - TA、Kinetics400 - TA、Something - Something - v2 - TA）评估13个模型。

Result: 模型在新场景识别高级动作性能显著下降；多模态模型处理细粒度未知动作更困难；Syn - TA与真实数据集难度相当；大模型在空间线索主导时可提升可迁移性，但在时间推理上表现不佳。

Conclusion: 本研究为评估动作识别中的运动可迁移性建立了关键基准。

Abstract: Action recognition models demonstrate strong generalization, but can they
effectively transfer high-level motion concepts across diverse contexts, even
within similar distributions? For example, can a model recognize the broad
action "punching" when presented with an unseen variation such as "punching
person"? To explore this, we introduce a motion transferability framework with
three datasets: (1) Syn-TA, a synthetic dataset with 3D object motions; (2)
Kinetics400-TA; and (3) Something-Something-v2-TA, both adapted from natural
video datasets. We evaluate 13 state-of-the-art models on these benchmarks and
observe a significant drop in performance when recognizing high-level actions
in novel contexts. Our analysis reveals: 1) Multimodal models struggle more
with fine-grained unknown actions than with coarse ones; 2) The bias-free
Syn-TA proves as challenging as real-world datasets, with models showing
greater performance drops in controlled settings; 3) Larger models improve
transferability when spatial cues dominate but struggle with intensive temporal
reasoning, while reliance on object and background cues hinders generalization.
We further explore how disentangling coarse and fine motions can improve
recognition in temporally challenging datasets. We believe this study
establishes a crucial benchmark for assessing motion transferability in action
recognition. Datasets and relevant code:
https://github.com/raiyaan-abdullah/Motion-Transfer.

</details>


### [178] [Exploring the Feasibility of Deep Learning Techniques for Accurate Gender Classification from Eye Images](https://arxiv.org/abs/2508.00135)
*Basna Mohammed Salih Hasan,Ramadhan J. Mstafa*

Main category: cs.CV

TL;DR: 本文研究使用眼周区域彩色图像进行性别分类，介绍CNN模型，在两个数据集测试效果好，证明模型可实际应用。


<details>
  <summary>Details</summary>
Motivation: 性别分类准确性受化妆品和伪装等因素影响，研究使用眼周区域彩色图像进行性别分类。

Method: 引入复杂CNN模型，利用彩色图像数据库，在CVBL和(Female and Male)两个数据集上测试。

Result: 模型在CVBL数据集上准确率达99%，在(Female and Male)数据集上以少量可学习参数达到96%准确率。

Conclusion: 模型在性别分类中有效，有在安全和监控等领域实际应用的潜力。

Abstract: Gender classification has emerged as a crucial aspect in various fields,
including security, human-machine interaction, surveillance, and advertising.
Nonetheless, the accuracy of this classification can be influenced by factors
such as cosmetics and disguise. Consequently, our study is dedicated to
addressing this concern by concentrating on gender classification using color
images of the periocular region. The periocular region refers to the area
surrounding the eye, including the eyelids, eyebrows, and the region between
them. It contains valuable visual cues that can be used to extract key features
for gender classification. This paper introduces a sophisticated Convolutional
Neural Network (CNN) model that utilizes color image databases to evaluate the
effectiveness of the periocular region for gender classification. To validate
the model's performance, we conducted tests on two eye datasets, namely CVBL
and (Female and Male). The recommended architecture achieved an outstanding
accuracy of 99% on the previously unused CVBL dataset while attaining a
commendable accuracy of 96% with a small number of learnable parameters
(7,235,089) on the (Female and Male) dataset. To ascertain the effectiveness of
our proposed model for gender classification using the periocular region, we
evaluated its performance through an extensive range of metrics and compared it
with other state-of-the-art approaches. The results unequivocally demonstrate
the efficacy of our model, thereby suggesting its potential for practical
application in domains such as security and surveillance.

</details>


### [179] [Controllable Pedestrian Video Editing for Multi-View Driving Scenarios via Motion Sequence](https://arxiv.org/abs/2508.00299)
*Danzhen Fu,Jiagao Hu,Daiguo Zhou,Fei Wang,Zepeng Wang,Wenhua Liao*

Main category: cs.CV

TL;DR: 提出多视角驾驶场景下可控行人视频编辑框架，实验证明其效果好，有应用潜力


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统中行人检测模型因训练数据缺乏危险行人场景表征而缺乏鲁棒性

Method: 结合视频修复和人体运动控制技术，识别多相机视角行人感兴趣区域，扩展检测框，调整大小并拼接，用二进制掩码指定编辑区域，按姿态序列控制条件编辑行人

Result: 框架实现高质量行人编辑，有强视觉真实感、时空连贯性和跨视角一致性

Conclusion: 该方法是多视角行人视频生成的强大通用解决方案，在自动驾驶数据增强和场景模拟有广泛应用潜力

Abstract: Pedestrian detection models in autonomous driving systems often lack
robustness due to insufficient representation of dangerous pedestrian scenarios
in training datasets. To address this limitation, we present a novel framework
for controllable pedestrian video editing in multi-view driving scenarios by
integrating video inpainting and human motion control techniques. Our approach
begins by identifying pedestrian regions of interest across multiple camera
views, expanding detection bounding boxes with a fixed ratio, and resizing and
stitching these regions into a unified canvas while preserving cross-view
spatial relationships. A binary mask is then applied to designate the editable
area, within which pedestrian editing is guided by pose sequence control
conditions. This enables flexible editing functionalities, including pedestrian
insertion, replacement, and removal. Extensive experiments demonstrate that our
framework achieves high-quality pedestrian editing with strong visual realism,
spatiotemporal coherence, and cross-view consistency. These results establish
the proposed method as a robust and versatile solution for multi-view
pedestrian video generation, with broad potential for applications in data
augmentation and scenario simulation in autonomous driving.

</details>


### [180] [GV-VAD : Exploring Video Generation for Weakly-Supervised Video Anomaly Detection](https://arxiv.org/abs/2508.00312)
*Suhang Cai,Xiaohao Peng,Chong Wang,Xiaojie Cai,Jiangbo Qian*

Main category: cs.CV

TL;DR: 提出GV - VAD框架解决视频异常检测数据集扩展难题，实验表明在UCF - Crime数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界异常事件的稀有性、不可预测性和高标注成本使视频异常检测数据集难以扩展，限制了现有模型性能和泛化能力。

Method: 提出GV - VAD框架，利用文本条件视频生成模型生成语义可控、物理合理的合成视频来低成本扩充训练数据，并采用合成样本损失缩放策略控制生成样本影响以高效训练。

Result: 所提框架在UCF - Crime数据集上的表现优于现有方法。

Conclusion: 所提GV - VAD框架能有效解决视频异常检测数据集扩展问题，提升模型性能。

Abstract: Video anomaly detection (VAD) plays a critical role in public safety
applications such as intelligent surveillance. However, the rarity,
unpredictability, and high annotation cost of real-world anomalies make it
difficult to scale VAD datasets, which limits the performance and
generalization ability of existing models. To address this challenge, we
propose a generative video-enhanced weakly-supervised video anomaly detection
(GV-VAD) framework that leverages text-conditioned video generation models to
produce semantically controllable and physically plausible synthetic videos.
These virtual videos are used to augment training data at low cost. In
addition, a synthetic sample loss scaling strategy is utilized to control the
influence of generated synthetic samples for efficient training. The
experiments show that the proposed framework outperforms state-of-the-art
methods on UCF-Crime datasets. The code is available at
https://github.com/Sumutan/GV-VAD.git.

</details>


### [181] [Decouple before Align: Visual Disentanglement Enhances Prompt Tuning](https://arxiv.org/abs/2508.00395)
*Fei Zhang,Tianfei Zhou,Jiangchao Yao,Ya Zhang,Ivor W. Tsang,Yanfeng Wang*

Main category: cs.CV

TL;DR: 本文针对提示调优中信息不对称问题提出DAPT框架，通过解耦和对齐模态、视觉正则化等方法提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决提示调优中视觉和文本模态信息不对称，避免因模态粗对齐导致的注意力偏差问题。

Method: 提出DAPT框架，先将视觉模态解耦为前景和背景表示，再与对应文本对齐；提出视觉推拉正则化方法。

Result: 在少样本学习、基类到新类泛化和数据高效学习等方面，DAPT在主流基准测试中表现出色。

Conclusion: 所提出的架构无关的DAPT框架有效解决了提示调优中的信息不对称问题，提升了模型性能。

Abstract: Prompt tuning (PT), as an emerging resource-efficient fine-tuning paradigm,
has showcased remarkable effectiveness in improving the task-specific
transferability of vision-language models. This paper delves into a previously
overlooked information asymmetry issue in PT, where the visual modality mostly
conveys more context than the object-oriented textual modality.
Correspondingly, coarsely aligning these two modalities could result in the
biased attention, driving the model to merely focus on the context area. To
address this, we propose DAPT, an effective PT framework based on an intuitive
decouple-before-align concept. First, we propose to explicitly decouple the
visual modality into the foreground and background representation via
exploiting coarse-and-fine visual segmenting cues, and then both of these
decoupled patterns are aligned with the original foreground texts and the
hand-crafted background classes, thereby symmetrically strengthening the modal
alignment. To further enhance the visual concentration, we propose a visual
pull-push regularization tailored for the foreground-background patterns,
directing the original visual representation towards unbiased attention on the
region-of-interest object. We demonstrate the power of architecture-free DAPT
through few-shot learning, base-to-novel generalization, and data-efficient
learning, all of which yield superior performance across prevailing benchmarks.
Our code will be released at https://github.com/Ferenas/DAPT.

</details>


### [182] [DC-AE 1.5: Accelerating Diffusion Model Convergence with Structured Latent Space](https://arxiv.org/abs/2508.00413)
*Junyu Chen,Dongyun Zou,Wenkun He,Junsong Chen,Enze Xie,Song Han,Han Cai*

Main category: cs.CV

TL;DR: 提出DC - AE 1.5用于高分辨率扩散模型，通过两项创新解决收敛慢问题，表现优于DC - AE。


<details>
  <summary>Details</summary>
Motivation: 增加自编码器潜在通道数虽提升重建质量，但导致扩散模型收敛慢、生成质量差，限制潜扩散模型质量上限，阻碍高空间压缩比自编码器应用。

Method: 引入结构化潜在空间和增强扩散训练两项创新技术。

Result: DC - AE 1.5比DC - AE收敛更快、扩散缩放效果更好，在ImageNet 512x512上，DC - AE - 1.5 - f64c128比DC - AE - f32c32图像生成质量更好且快4倍。

Conclusion: DC - AE 1.5能有效解决收敛慢问题，提升图像生成效果。

Abstract: We present DC-AE 1.5, a new family of deep compression autoencoders for
high-resolution diffusion models. Increasing the autoencoder's latent channel
number is a highly effective approach for improving its reconstruction quality.
However, it results in slow convergence for diffusion models, leading to poorer
generation quality despite better reconstruction quality. This issue limits the
quality upper bound of latent diffusion models and hinders the employment of
autoencoders with higher spatial compression ratios. We introduce two key
innovations to address this challenge: i) Structured Latent Space, a
training-based approach to impose a desired channel-wise structure on the
latent space with front latent channels capturing object structures and latter
latent channels capturing image details; ii) Augmented Diffusion Training, an
augmented diffusion training strategy with additional diffusion training
objectives on object latent channels to accelerate convergence. With these
techniques, DC-AE 1.5 delivers faster convergence and better diffusion scaling
results than DC-AE. On ImageNet 512x512, DC-AE-1.5-f64c128 delivers better
image generation quality than DC-AE-f32c32 while being 4x faster. Code:
https://github.com/dc-ai-projects/DC-Gen.

</details>


### [183] [Contact-Aware Amodal Completion for Human-Object Interaction via Multi-Regional Inpainting](https://arxiv.org/abs/2508.00427)
*Seunggeun Chi,Enna Sachdeva,Pin-Hao Huang,Kwonjoon Lee*

Main category: cs.CV

TL;DR: 提出新方法利用物理先验知识和多区域修复技术解决动态场景无模态补全问题，实验显示效果优于现有方法且适用范围广。


<details>
  <summary>Details</summary>
Motivation: 现有方法在动态场景中因对人类 - 对象交互（HOI）理解有限，难以生成合理补全，需新方法解决。

Method: 结合物理先验知识和专为HOI设计的多区域修复技术，在扩散模型中根据人类拓扑和接触信息划分主次区域并采用定制去噪策略。

Result: 该方法在HOI场景中显著优于现有方法，即使无真实接触注释也很稳健。

Conclusion: 该方法使机器感知更接近人类对动态环境的理解，且适用于3D重建等任务。

Abstract: Amodal completion, which is the process of inferring the full appearance of
objects despite partial occlusions, is crucial for understanding complex
human-object interactions (HOI) in computer vision and robotics. Existing
methods, such as those that use pre-trained diffusion models, often struggle to
generate plausible completions in dynamic scenarios because they have a limited
understanding of HOI. To solve this problem, we've developed a new approach
that uses physical prior knowledge along with a specialized multi-regional
inpainting technique designed for HOI. By incorporating physical constraints
from human topology and contact information, we define two distinct regions:
the primary region, where occluded object parts are most likely to be, and the
secondary region, where occlusions are less probable. Our multi-regional
inpainting method uses customized denoising strategies across these regions
within a diffusion model. This improves the accuracy and realism of the
generated completions in both their shape and visual detail. Our experimental
results show that our approach significantly outperforms existing methods in
HOI scenarios, moving machine perception closer to a more human-like
understanding of dynamic environments. We also show that our pipeline is robust
even without ground-truth contact annotations, which broadens its applicability
to tasks like 3D reconstruction and novel view/pose synthesis.

</details>


### [184] [Reducing the gap between general purpose data and aerial images in concentrated solar power plants](https://arxiv.org/abs/2508.00440)
*M. A. Pérez-Cutiño,J. Valverde,J. Capitán,J. M. Díaz-Báñez*

Main category: cs.CV

TL;DR: 提出虚拟数据集AerialCSP解决CSP工厂无人机航拍图机器学习难题，公开数据集并证明其有效性


<details>
  <summary>Details</summary>
Motivation: CSP工厂航拍图有独特挑战，通用数据集训练的模型难泛化，收集标注数据成本高

Method: 创建模拟CSP工厂航拍图的虚拟数据集AerialCSP用于模型预训练

Result: 对多个模型在AerialCSP上进行基准测试，预训练能提升现实故障检测效果，减少手动标注需求

Conclusion: AerialCSP能助力CSP相关视觉任务，减少手动标注工作，已公开数据集

Abstract: In the context of Concentrated Solar Power (CSP) plants, aerial images
captured by drones present a unique set of challenges. Unlike urban or natural
landscapes commonly found in existing datasets, solar fields contain highly
reflective surfaces, and domain-specific elements that are uncommon in
traditional computer vision benchmarks. As a result, machine learning models
trained on generic datasets struggle to generalize to this setting without
extensive retraining and large volumes of annotated data. However, collecting
and labeling such data is costly and time-consuming, making it impractical for
rapid deployment in industrial applications.
  To address this issue, we propose a novel approach: the creation of
AerialCSP, a virtual dataset that simulates aerial imagery of CSP plants. By
generating synthetic data that closely mimic real-world conditions, our
objective is to facilitate pretraining of models before deployment,
significantly reducing the need for extensive manual labeling. Our main
contributions are threefold: (1) we introduce AerialCSP, a high-quality
synthetic dataset for aerial inspection of CSP plants, providing annotated data
for object detection and image segmentation; (2) we benchmark multiple models
on AerialCSP, establishing a baseline for CSP-related vision tasks; and (3) we
demonstrate that pretraining on AerialCSP significantly improves real-world
fault detection, particularly for rare and small defects, reducing the need for
extensive manual labeling. AerialCSP is made publicly available at
https://mpcutino.github.io/aerialcsp/.

</details>


### [185] [TopoTTA: Topology-Enhanced Test-Time Adaptation for Tubular Structure Segmentation](https://arxiv.org/abs/2508.00442)
*Jiale Zhou,Wenhan Wang,Shikun Li,Xiaolei Qu,Xin Guo,Yizhong Liu,Wenzhong Tang,Xun Lin,Yefeng Zheng*

Main category: cs.CV

TL;DR: 提出TopoTTA框架解决管状结构分割中领域偏移问题，实验证明有效且可作为即插即用方案。


<details>
  <summary>Details</summary>
Motivation: 管状结构分割存在领域偏移挑战，导致在未知目标领域性能下降，且该任务对领域偏移更敏感。

Method: 提出TopoTTA框架，分两个阶段，第一阶段用TopoMDCs适应跨领域拓扑差异，第二阶段用TopoHG策略和预测对齐改善拓扑连续性。

Result: 在四个场景和十个数据集实验中，TopoTTA处理拓扑分布偏移有效，clDice平均提升31.81%。

Conclusion: TopoTTA能处理拓扑分布偏移，可作为基于CNN的管状结构分割模型的即插即用TTA解决方案。

Abstract: Tubular structure segmentation (TSS) is important for various applications,
such as hemodynamic analysis and route navigation. Despite significant progress
in TSS, domain shifts remain a major challenge, leading to performance
degradation in unseen target domains. Unlike other segmentation tasks, TSS is
more sensitive to domain shifts, as changes in topological structures can
compromise segmentation integrity, and variations in local features
distinguishing foreground from background (e.g., texture and contrast) may
further disrupt topological continuity. To address these challenges, we propose
Topology-enhanced Test-Time Adaptation (TopoTTA), the first test-time
adaptation framework designed specifically for TSS. TopoTTA consists of two
stages: Stage 1 adapts models to cross-domain topological discrepancies using
the proposed Topological Meta Difference Convolutions (TopoMDCs), which enhance
topological representation without altering pre-trained parameters; Stage 2
improves topological continuity by a novel Topology Hard sample Generation
(TopoHG) strategy and prediction alignment on hard samples with pseudo-labels
in the generated pseudo-break regions. Extensive experiments across four
scenarios and ten datasets demonstrate TopoTTA's effectiveness in handling
topological distribution shifts, achieving an average improvement of 31.81% in
clDice. TopoTTA also serves as a plug-and-play TTA solution for CNN-based TSS
models.

</details>


### [186] [Graph Lineages and Skeletal Graph Products](https://arxiv.org/abs/2508.00197)
*Eric Mjolsness,Cory B. Scott*

Main category: cs.CV

TL;DR: 本文定义结构化图“谱系”，推导相关运算符，形成代数类型理论，并展示其在深度学习和多重网格法中的应用。


<details>
  <summary>Details</summary>
Motivation: 为许多领域的数学模型架构提供一种代数类型理论，适合定义分层模型架构及相关算法。

Method: 定义分层增长的结构化图谱系，推导图操作的低代价“骨架”变体及一元运算符。

Result: 形成了分级图和（分层）图谱系的代数类型理论。

Conclusion: 该方法适用于定义分层模型架构及局部采样、搜索或优化算法，且在深度学习和多重网格法中得到应用。

Abstract: Graphs, and sequences of growing graphs, can be used to specify the
architecture of mathematical models in many fields including machine learning
and computational science. Here we define structured graph "lineages" (ordered
by level number) that grow in a hierarchical fashion, so that: (1) the number
of graph vertices and edges increases exponentially in level number; (2)
bipartite graphs connect successive levels within a graph lineage and, as in
multigrid methods, can constrain matrices relating successive levels; (3) using
prolongation maps within a graph lineage, process-derived distance measures
between graphs at successive levels can be defined; (4) a category of "graded
graphs" can be defined, and using it low-cost "skeletal" variants of standard
algebraic graph operations and type constructors (cross product, box product,
disjoint sum, and function types) can be derived for graded graphs and hence
hierarchical graph lineages; (5) these skeletal binary operators have similar
but not identical algebraic and category-theoretic properties to their standard
counterparts; (6) graph lineages and their skeletal product constructors can
approach continuum limit objects. Additional space-efficient unary operators on
graded graphs are also derived: thickening, which creates a graph lineage of
multiscale graphs, and escalation to a graph lineage of search frontiers
(useful as a generalization of adaptive grids and in defining "skeletal"
functions). The result is an algebraic type theory for graded graphs and
(hierarchical) graph lineages. The approach is expected to be well suited to
defining hierarchical model architectures - "hierarchitectures" - and local
sampling, search, or optimization algorithms on them. We demonstrate such
application to deep neural networks (including visual and feature scale spaces)
and to multigrid numerical methods.

</details>


### [187] [SAM-PTx: Text-Guided Fine-Tuning of SAM with Parameter-Efficient, Parallel-Text Adapters](https://arxiv.org/abs/2508.00213)
*Shayan Jalilian,Abdul Bais*

Main category: cs.CV

TL;DR: 本文提出SAM - PTx方法，用冻结CLIP文本嵌入作为语义指导适配SAM，提升分割性能，是COD10K数据集上首次用文本提示进行分割的工作。


<details>
  <summary>Details</summary>
Motivation: SAM在基于提示的分割中表现良好，但语义文本提示潜力未充分挖掘，相比传统空间提示研究较少。

Method: 提出名为Parallel - Text的轻量级适配器，将文本嵌入注入SAM的图像编码器，仅修改每个Transformer块的MLP并行分支。

Result: 在COD10K、COCO和ADE20K低数据子集上实验表明，使用固定文本嵌入作为输入比纯空间提示基线提升了分割性能。

Conclusion: 将语义条件集成到SAM架构中是一种实用且可扩展的高效适配方法，计算复杂度低。

Abstract: The Segment Anything Model (SAM) has demonstrated impressive generalization
in prompt-based segmentation. Yet, the potential of semantic text prompts
remains underexplored compared to traditional spatial prompts like points and
boxes. This paper introduces SAM-PTx, a parameter-efficient approach for
adapting SAM using frozen CLIP-derived text embeddings as class-level semantic
guidance. Specifically, we propose a lightweight adapter design called
Parallel-Text that injects text embeddings into SAM's image encoder, enabling
semantics-guided segmentation while keeping most of the original architecture
frozen. Our adapter modifies only the MLP-parallel branch of each transformer
block, preserving the attention pathway for spatial reasoning. Through
supervised experiments and ablations on the COD10K dataset as well as low-data
subsets of COCO and ADE20K, we show that incorporating fixed text embeddings as
input improves segmentation performance over purely spatial prompt baselines.
To our knowledge, this is the first work to use text prompts for segmentation
on the COD10K dataset. These results suggest that integrating semantic
conditioning into SAM's architecture offers a practical and scalable path for
efficient adaptation with minimal computational complexity.

</details>


### [188] [Object-Centric Cropping for Visual Few-Shot Classification](https://arxiv.org/abs/2508.00218)
*Aymane Abdali,Bartosz Boguslawski,Lucas Drumetz,Vincent Gripon*

Main category: cs.CV

TL;DR: 在小样本图像分类中，利用物体局部定位信息可提升分类性能，用Segment Anything Model或无监督前景提取方法能实现显著改进。


<details>
  <summary>Details</summary>
Motivation: 解决小样本图像分类中因图像模糊（多物体或复杂背景）导致性能下降的问题。

Method: 引入物体在图像中的局部定位信息，使用Segment Anything Model或无监督前景物体提取方法。

Result: 在既定基准上，利用上述方法显著提升了图像分类性能。

Conclusion: 在小样本图像分类中，物体局部定位信息能有效提升分类性能。

Abstract: In the domain of Few-Shot Image Classification, operating with as little as
one example per class, the presence of image ambiguities stemming from multiple
objects or complex backgrounds can significantly deteriorate performance. Our
research demonstrates that incorporating additional information about the local
positioning of an object within its image markedly enhances classification
across established benchmarks. More importantly, we show that a significant
fraction of the improvement can be achieved through the use of the Segment
Anything Model, requiring only a pixel of the object of interest to be pointed
out, or by employing fully unsupervised foreground object extraction methods.

</details>


### [189] [LesiOnTime -- Joint Temporal and Clinical Modeling for Small Breast Lesion Segmentation in Longitudinal DCE-MRI](https://arxiv.org/abs/2508.00496)
*Mohammed Kamran,Maria Bernathova,Raoul Varga,Christian Singer,Zsuzsanna Bago-Horvath,Thomas Helbich,Georg Langs,Philipp Seeböck*

Main category: cs.CV

TL;DR: 提出LesiOnTime 3D分割方法用于乳腺DCE - MRI小病灶分割，结合纵向成像和BIRADS分数，在Dice指标上优于现有方法，强调结合时空和临床信息重要性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法主要针对大病灶，忽视纵向和临床信息，而实际筛查需要对比不同时间点和参考先前评估。

Method: 提出LesiOnTime方法，包含TPA模块动态整合前后扫描信息，BCR损失函数使相似放射评估扫描潜在空间对齐。

Result: 在内部数据集上，该方法在Dice指标上比现有单时间点和纵向基线方法高5%，消融实验表明TPA和BCR互补提升性能。

Conclusion: 在实际乳腺癌筛查中，结合时空和临床信息对早期病灶可靠分割很重要。

Abstract: Accurate segmentation of small lesions in Breast Dynamic Contrast-Enhanced
MRI (DCE-MRI) is critical for early cancer detection, especially in high-risk
patients. While recent deep learning methods have advanced lesion segmentation,
they primarily target large lesions and neglect valuable longitudinal and
clinical information routinely used by radiologists. In real-world screening,
detecting subtle or emerging lesions requires radiologists to compare across
timepoints and consider previous radiology assessments, such as the BI-RADS
score. We propose LesiOnTime, a novel 3D segmentation approach that mimics
clinical diagnostic workflows by jointly leveraging longitudinal imaging and
BIRADS scores. The key components are: (1) a Temporal Prior Attention (TPA)
block that dynamically integrates information from previous and current scans;
and (2) a BI-RADS Consistency Regularization (BCR) loss that enforces latent
space alignment for scans with similar radiological assessments, thus embedding
domain knowledge into the training process. Evaluated on a curated in-house
longitudinal dataset of high-risk patients with DCE-MRI, our approach
outperforms state-of-the-art single-timepoint and longitudinal baselines by 5%
in terms of Dice. Ablation studies demonstrate that both TPA and BCR contribute
complementary performance gains. These results highlight the importance of
incorporating temporal and clinical context for reliable early lesion
segmentation in real-world breast cancer screening. Our code is publicly
available at https://github.com/cirmuw/LesiOnTime

</details>


### [190] [Steering Guidance for Personalized Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.00319)
*Sunghyun Park,Seokeon Choi,Hyoungwoo Park,Sungrack Yun*

Main category: cs.CV

TL;DR: 提出个性化引导方法解决文本到图像扩散模型微调问题，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有采样引导方法无法有效引导输出至平衡空间，存在适应目标分布和保留原模型知识的权衡问题。

Method: 提出个性化引导方法，利用基于空文本提示的未学习弱模型，在推理时通过预训练和微调模型的权重插值动态控制弱模型的遗忘程度。

Result: 所提引导方法能提高文本对齐和目标分布保真度，可与各种微调策略无缝集成。

Conclusion: 所提方法能有效解决现有方法局限，且无额外计算开销。

Abstract: Personalizing text-to-image diffusion models is crucial for adapting the
pre-trained models to specific target concepts, enabling diverse image
generation. However, fine-tuning with few images introduces an inherent
trade-off between aligning with the target distribution (e.g., subject
fidelity) and preserving the broad knowledge of the original model (e.g., text
editability). Existing sampling guidance methods, such as classifier-free
guidance (CFG) and autoguidance (AG), fail to effectively guide the output
toward well-balanced space: CFG restricts the adaptation to the target
distribution, while AG compromises text alignment. To address these
limitations, we propose personalization guidance, a simple yet effective method
leveraging an unlearned weak model conditioned on a null text prompt. Moreover,
our method dynamically controls the extent of unlearning in a weak model
through weight interpolation between pre-trained and fine-tuned models during
inference. Unlike existing guidance methods, which depend solely on guidance
scales, our method explicitly steers the outputs toward a balanced latent space
without additional computational overhead. Experimental results demonstrate
that our proposed guidance can improve text alignment and target distribution
fidelity, integrating seamlessly with various fine-tuning strategies.

</details>


### [191] [Wukong Framework for Not Safe For Work Detection in Text-to-Image systems](https://arxiv.org/abs/2508.00591)
*Mingrui Liu,Sixiao Zhang,Cheng Long*

Main category: cs.CV

TL;DR: 提出基于Transformer的NSFW检测框架Wukong，利用早期去噪步骤中间输出和预训练交叉注意力参数，能在扩散过程中早期检测，效率高。


<details>
  <summary>Details</summary>
Motivation: 现有T2I生成技术输出可能含NSFW内容，而现有外部防护方法存在不足，需高效准确检测方法。

Method: 基于早期去噪步骤定义图像语义布局、U-Net中交叉注意力层对文本和图像区域对齐的观察，提出Wukong框架，引入新数据集并在多个基准测试评估。

Result: Wukong显著优于基于文本的防护方法，与图像过滤器准确率相当，但效率更高。

Conclusion: Wukong是一种高效准确的T2I生成中NSFW内容检测方法。

Abstract: Text-to-Image (T2I) generation is a popular AI-generated content (AIGC)
technology enabling diverse and creative image synthesis. However, some outputs
may contain Not Safe For Work (NSFW) content (e.g., violence), violating
community guidelines. Detecting NSFW content efficiently and accurately, known
as external safeguarding, is essential. Existing external safeguards fall into
two types: text filters, which analyze user prompts but overlook T2I
model-specific variations and are prone to adversarial attacks; and image
filters, which analyze final generated images but are computationally costly
and introduce latency. Diffusion models, the foundation of modern T2I systems
like Stable Diffusion, generate images through iterative denoising using a
U-Net architecture with ResNet and Transformer blocks. We observe that: (1)
early denoising steps define the semantic layout of the image, and (2)
cross-attention layers in U-Net are crucial for aligning text and image
regions. Based on these insights, we propose Wukong, a transformer-based NSFW
detection framework that leverages intermediate outputs from early denoising
steps and reuses U-Net's pre-trained cross-attention parameters. Wukong
operates within the diffusion process, enabling early detection without waiting
for full image generation. We also introduce a new dataset containing prompts,
seeds, and image-specific NSFW labels, and evaluate Wukong on this and two
public benchmarks. Results show that Wukong significantly outperforms
text-based safeguards and achieves comparable accuracy of image filters, while
offering much greater efficiency.

</details>


### [192] [Backdoor Attacks on Deep Learning Face Detection](https://arxiv.org/abs/2508.00620)
*Quentin Le Roux,Yannick Teglia,Teddy Furon,Philippe Loubet-Moundi*

Main category: cs.CV

TL;DR: 论文展示人脸生成攻击对人脸检测的有效性，首次提出地标偏移攻击，还给出应对漏洞的缓解措施。


<details>
  <summary>Details</summary>
Motivation: 无约束环境下人脸识别系统面临光照、姿态等挑战，需人脸检测模块，研究人脸检测在对抗攻击下的漏洞。

Method: 提出人脸生成攻击和地标偏移攻击并进行研究。

Result: 展示了人脸生成攻击的有效性，首次实现地标偏移攻击。

Conclusion: 发现人脸检测存在的漏洞并给出缓解措施。

Abstract: Face Recognition Systems that operate in unconstrained environments capture
images under varying conditions,such as inconsistent lighting, or diverse face
poses. These challenges require including a Face Detection module that
regresses bounding boxes and landmark coordinates for proper Face Alignment.
This paper shows the effectiveness of Object Generation Attacks on Face
Detection, dubbed Face Generation Attacks, and demonstrates for the first time
a Landmark Shift Attack that backdoors the coordinate regression task performed
by face detectors. We then offer mitigations against these vulnerabilities.

</details>


### [193] [CLIPTime: Time-Aware Multimodal Representation Learning from Images and Text](https://arxiv.org/abs/2508.00447)
*Anju Rani,Daniel Ortiz-Arroyo,Petar Durdevic*

Main category: cs.CV

TL;DR: 提出CLIPTime框架预测真菌生长阶段和时间戳，用合成数据集训练评估，实验证明其有效性和在生物监测中的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型捕捉时间进展能力有限，需要有效方法理解生物生长的时间动态。

Method: 提出CLIPTime框架，基于CLIP架构，联合执行分类和回归；引入合成真菌生长数据集；提出自定义评估指标。

Result: CLIPTime能有效建模生物进展，产生可解释、有时间依据的输出。

Conclusion: 视觉语言模型在现实生物监测应用中有潜力。

Abstract: Understanding the temporal dynamics of biological growth is critical across
diverse fields such as microbiology, agriculture, and biodegradation research.
Although vision-language models like Contrastive Language Image Pretraining
(CLIP) have shown strong capabilities in joint visual-textual reasoning, their
effectiveness in capturing temporal progression remains limited. To address
this, we propose CLIPTime, a multimodal, multitask framework designed to
predict both the developmental stage and the corresponding timestamp of fungal
growth from image and text inputs. Built upon the CLIP architecture, our model
learns joint visual-textual embeddings and enables time-aware inference without
requiring explicit temporal input during testing. To facilitate training and
evaluation, we introduce a synthetic fungal growth dataset annotated with
aligned timestamps and categorical stage labels. CLIPTime jointly performs
classification and regression, predicting discrete growth stages alongside
continuous timestamps. We also propose custom evaluation metrics, including
temporal accuracy and regression error, to assess the precision of time-aware
predictions. Experimental results demonstrate that CLIPTime effectively models
biological progression and produces interpretable, temporally grounded outputs,
highlighting the potential of vision-language models in real-world biological
monitoring applications.

</details>


### [194] [Guiding Diffusion-Based Articulated Object Generation by Partial Point Cloud Alignment and Physical Plausibility Constraints](https://arxiv.org/abs/2508.00558)
*Jens U. Kreber,Joerg Stueckler*

Main category: cs.CV

TL;DR: 提出基于扩散模型的PhysNAP方法生成铰接物体，使其与部分点云对齐并提高物理合理性，评估显示其能改善约束一致性。


<details>
  <summary>Details</summary>
Motivation: 生成与部分点云对齐且具有物理合理性的铰接物体。

Method: 用有符号距离函数（SDFs）表示部件形状，用预测的SDFs计算点云对齐损失来引导反向扩散过程，基于部件SDFs施加非穿透和移动性约束，使扩散方法具有类别感知能力。

Result: 使用PartNet - Mobility数据集评估，与无引导的基线扩散模型对比，显示PhysNAP能改善约束一致性，在生成能力上有折衷。

Conclusion: PhysNAP能提高铰接物体生成的约束一致性，可在一定程度上平衡生成能力。

Abstract: Articulated objects are an important type of interactable objects in everyday
environments. In this paper, we propose PhysNAP, a novel diffusion model-based
approach for generating articulated objects that aligns them with partial point
clouds and improves their physical plausibility. The model represents part
shapes by signed distance functions (SDFs). We guide the reverse diffusion
process using a point cloud alignment loss computed using the predicted SDFs.
Additionally, we impose non-penetration and mobility constraints based on the
part SDFs for guiding the model to generate more physically plausible objects.
We also make our diffusion approach category-aware to further improve point
cloud alignment if category information is available. We evaluate the
generative ability and constraint consistency of samples generated with PhysNAP
using the PartNet-Mobility dataset. We also compare it with an unguided
baseline diffusion model and demonstrate that PhysNAP can improve constraint
consistency and provides a tradeoff with generative ability.

</details>


### [195] [D3: Training-Free AI-Generated Video Detection Using Second-Order Features](https://arxiv.org/abs/2508.00701)
*Chende Zheng,Ruiqi suo,Chenhao Lin,Zhengyu Zhao,Le Yang,Shuai Liu,Minghui Yang,Cong Wang,Chao Shen*

Main category: cs.CV

TL;DR: 现有合成视频检测方法对时间伪影探索不足，本文建立理论框架，提出无训练检测方法D3，在多数据集验证其优越性和高效性。


<details>
  <summary>Details</summary>
Motivation: 视频生成技术发展使合成内容传播引发公众担忧，而现有检测方法对合成视频时间伪影探索不足。

Method: 通过牛顿力学下的二阶动力学分析建立理论框架，扩展二阶中心差分特征用于时间伪影检测，提出利用二阶时间差异的无训练检测方法D3。

Result: 在4个开源数据集共40个子集上验证了D3的优越性，如在GenVideo上，D3的平均精度均值比之前最佳方法高10.39%，且在时间成本和后处理操作实验中表现出计算高效性和强鲁棒性。

Conclusion: 提出的D3方法能有效检测合成视频，具有优越性、计算高效性和强鲁棒性。

Abstract: The evolution of video generation techniques, such as Sora, has made it
increasingly easy to produce high-fidelity AI-generated videos, raising public
concern over the dissemination of synthetic content. However, existing
detection methodologies remain limited by their insufficient exploration of
temporal artifacts in synthetic videos. To bridge this gap, we establish a
theoretical framework through second-order dynamical analysis under Newtonian
mechanics, subsequently extending the Second-order Central Difference features
tailored for temporal artifact detection. Building on this theoretical
foundation, we reveal a fundamental divergence in second-order feature
distributions between real and AI-generated videos. Concretely, we propose
Detection by Difference of Differences (D3), a novel training-free detection
method that leverages the above second-order temporal discrepancies. We
validate the superiority of our D3 on 4 open-source datasets (Gen-Video,
VideoPhy, EvalCrafter, VidProM), 40 subsets in total. For example, on GenVideo,
D3 outperforms the previous best method by 10.39% (absolute) mean Average
Precision. Additional experiments on time cost and post-processing operations
demonstrate D3's exceptional computational efficiency and strong robust
performance. Our code is available at https://github.com/Zig-HS/D3.

</details>


### [196] [Is It Really You? Exploring Biometric Verification Scenarios in Photorealistic Talking-Head Avatar Videos](https://arxiv.org/abs/2508.00748)
*Laura Pedrouzo-Rodriguez,Pedro Delgado-DeRobles,Luis F. Gomez,Ruben Tolosana,Ruben Vera-Rodriguez,Aythami Morales,Julian Fierrez*

Main category: cs.CV

TL;DR: 本文探讨头像介导场景下生物特征验证问题，引入新数据集，提出轻量级时空图卷积网络架构，实验表明面部运动线索可实现身份验证，相关基准和系统供研究界使用。


<details>
  <summary>Details</summary>
Motivation: 逼真的会说话头像在增加沉浸感交流的同时带来严重安全风险，如冒名顶替，需研究生物特征验证方法。

Method: 引入用GAGAvatar生成的包含真实和冒名头像视频的新数据集，提出带时间注意力池化的轻量级、可解释的时空图卷积网络架构，仅用面部标志点建模动态面部手势。

Result: 面部运动线索能实现有意义的身份验证，AUC值接近80%。

Conclusion: 应关注基于头像的通信系统中更先进的行为生物特征防御的迫切需求。

Abstract: Photorealistic talking-head avatars are becoming increasingly common in
virtual meetings, gaming, and social platforms. These avatars allow for more
immersive communication, but they also introduce serious security risks. One
emerging threat is impersonation: an attacker can steal a user's
avatar-preserving their appearance and voice-making it nearly impossible to
detect its fraudulent usage by sight or sound alone. In this paper, we explore
the challenge of biometric verification in such avatar-mediated scenarios. Our
main question is whether an individual's facial motion patterns can serve as
reliable behavioral biometrics to verify their identity when the avatar's
visual appearance is a facsimile of its owner. To answer this question, we
introduce a new dataset of realistic avatar videos created using a
state-of-the-art one-shot avatar generation model, GAGAvatar, with genuine and
impostor avatar videos. We also propose a lightweight, explainable
spatio-temporal Graph Convolutional Network architecture with temporal
attention pooling, that uses only facial landmarks to model dynamic facial
gestures. Experimental results demonstrate that facial motion cues enable
meaningful identity verification with AUC values approaching 80%. The proposed
benchmark and biometric system are available for the research community in
order to bring attention to the urgent need for more advanced behavioral
biometric defenses in avatar-based communication systems.

</details>


### [197] [Sample-Aware Test-Time Adaptation for Medical Image-to-Image Translation](https://arxiv.org/abs/2508.00766)
*Irene Iele,Francesco Di Feola,Valerio Guarrasi,Paolo Soda*

Main category: cs.CV

TL;DR: 提出一种新的测试时自适应（TTA）框架用于医学图像到图像翻译，在两项任务中表现优于基线和先前TTA方法，证明动态样本特定调整能提升模型弹性。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像到图像翻译在处理分布外样本时性能下降的问题。

Method: 提出TTA框架，引入重建模块量化域偏移，动态自适应块修改预训练模型内部特征。

Result: 在低剂量CT去噪和T1到T2 MRI翻译任务中，相比无TTA的基线模型和先前TTA方法有持续改进。

Conclusion: 动态、样本特定调整为提高现实场景中模型弹性提供了有前景的途径。

Abstract: Image-to-image translation has emerged as a powerful technique in medical
imaging, enabling tasks such as image denoising and cross-modality conversion.
However, it suffers from limitations in handling out-of-distribution samples
without causing performance degradation. To address this limitation, we propose
a novel Test-Time Adaptation (TTA) framework that dynamically adjusts the
translation process based on the characteristics of each test sample. Our
method introduces a Reconstruction Module to quantify the domain shift and a
Dynamic Adaptation Block that selectively modifies the internal features of a
pretrained translation model to mitigate the shift without compromising the
performance on in-distribution samples that do not require adaptation. We
evaluate our approach on two medical image-to-image translation tasks: low-dose
CT denoising and T1 to T2 MRI translation, showing consistent improvements over
both the baseline translation model without TTA and prior TTA methods. Our
analysis highlights the limitations of the state-of-the-art that uniformly
apply the adaptation to both out-of-distribution and in-distribution samples,
demonstrating that dynamic, sample-specific adjustment offers a promising path
to improve model resilience in real-world scenarios. The code is available at:
https://github.com/cosbidev/Sample-Aware_TTA.

</details>


### [198] [SU-ESRGAN: Semantic and Uncertainty-Aware ESRGAN for Super-Resolution of Satellite and Drone Imagery with Fine-Tuning for Cross Domain Evaluation](https://arxiv.org/abs/2508.00750)
*Prerana Ramkumar*

Main category: cs.CV

TL;DR: 本文提出SU - ESRGAN用于卫星图像超分辨率，性能与Baseline ESRGAN相当，适用于多种场景，且强调跨域应用中领域感知训练的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有GANs在关键遥感应用中缺乏语义一致性和逐像素置信度，限制其可信度，需更好的超分辨率框架。

Method: 引入Semantic and Uncertainty - Aware ESRGAN (SU - ESRGAN)，集成ESRGAN、通过DeepLabv3进行分割损失以保留类细节，使用蒙特卡罗丢弃法生成逐像素不确定性图。

Result: SU - ESRGAN在航空图像上的结果（PSNR、SSIM、LPIPS）与Baseline ESRGAN相当；微调模型在与训练数据成像特征一致的数据集上表现更好。

Conclusion: SU - ESRGAN模型有价值，适用于多种场景；跨域应用中领域感知训练很重要。

Abstract: Generative Adversarial Networks (GANs) have achieved realistic
super-resolution (SR) of images however, they lack semantic consistency and
per-pixel confidence, limiting their credibility in critical remote sensing
applications such as disaster response, urban planning and agriculture. This
paper introduces Semantic and Uncertainty-Aware ESRGAN (SU-ESRGAN), the first
SR framework designed for satellite imagery to integrate the ESRGAN,
segmentation loss via DeepLabv3 for class detail preservation and Monte Carlo
dropout to produce pixel-wise uncertainty maps. The SU-ESRGAN produces results
(PSNR, SSIM, LPIPS) comparable to the Baseline ESRGAN on aerial imagery. This
novel model is valuable in satellite systems or UAVs that use wide
field-of-view (FoV) cameras, trading off spatial resolution for coverage. The
modular design allows integration in UAV data pipelines for on-board or
post-processing SR to enhance imagery resulting due to motion blur, compression
and sensor limitations. Further, the model is fine-tuned to evaluate its
performance on cross domain applications. The tests are conducted on two drone
based datasets which differ in altitude and imaging perspective. Performance
evaluation of the fine-tuned models show a stronger adaptation to the Aerial
Maritime Drone Dataset, whose imaging characteristics align with the training
data, highlighting the importance of domain-aware training in SR-applications.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [199] [Channel Choice and Customer Value](https://arxiv.org/abs/2508.00208)
*Shirsho Biswas,Hema Yoganarasimhan,Haonan Zhang*

Main category: econ.GN

TL;DR: 研究巴西宠物用品零售商多渠道顾客行为，发现不同渠道采纳原因影响采纳后行为，提醒企业区别对待多渠道顾客。


<details>
  <summary>Details</summary>
Motivation: 现有文献忽视新渠道采纳原因对采纳后行为的影响，探究线下顾客采纳线上购物后在消费、盈利和渠道使用方面的变化。

Method: 利用巴西一家主要宠物用品零售商的交易数据，研究有机采纳、疫情、黑色星期五促销和新推出的忠诚度计划四种采纳途径。

Result: 所有采纳群体消费均高于纯线下顾客，但采纳后行为因采纳原因而异，疫情采纳者消费与有机采纳者相似且线下粘性高、利润高，促销驱动采纳者采纳后消费少、盈利低。

Conclusion: 不应将所有多渠道顾客等同看待，企业评估渠道和促销投资时应考虑采纳动机，将行为理论纳入预测和目标策略。

Abstract: The rapid growth of digital shopping channels has prompted many traditional
retailers to invest in e-commerce websites and mobile apps. While prior
literature shows that multichannel customers tend to be more valuable, it
overlooks how the reason for adopting a new channel may shape post-adoption
behavior. Using transaction-level data from a major Brazilian pet supplies
retailer, we examine how adoption of online shopping - by previously
offline-only customers - affects post-adoption spend, profitability, and
channel usage. We study four distinct adoption pathways: organic adoption,
adoption due to the COVID-19 pandemic, Black Friday promotions, and a newly
launched loyalty program. We find that although all adopter groups increase
their spending relative to offline-only customers, post-adoption behavior
differs based on the reason for adoption. COVID-19 adopters behave similarly to
organic adopters in terms of spending, but show greater offline stickiness
consistent with consumer inertia and habit theory, yielding higher profits due
to higher offline margins. In contrast, promotion-driven adopters spend less
post-adoption due to forward buying and exhibit lower profitability. Our
findings caution against treating all multichannel customers as equal and
highlight the importance of incorporating behavioral theories into forecasting
and targeting strategies. Firms should account for adoption motives when
evaluating channel and promotional investments.

</details>


### [200] [Assessing the Macroeconomic Impacts of Disasters: an Updated Multi-Regional Impact Assessment (MRIA) model](https://arxiv.org/abs/2508.00510)
*Surender Raj Vanniya Perumal,Mark Thissen,Marleen de Ruiter,Elco E. Koks*

Main category: econ.GN

TL;DR: 研究引入MRIA模型评估灾害对供应链的区域和宏观经济影响，指出提升产能不足，还需提高区域贸易灵活性，分析不同地区受灾影响并提出部门关键度评估。


<details>
  <summary>Details</summary>
Motivation: 灾害影响供应链，未受灾地区补偿能力受限，需评估灾害的区域和宏观经济后果。

Method: 引入Multi - Regional Impact Assessment (MRIA)模型，同时提出部门关键度评估，结合常见的敏感性和增量中断分析。

Result: 提升生产能力不足以减轻灾害影响，还需提高区域贸易灵活性；受灾地区有严重负面影响，大型出口导向型地区受益；能有效识别低冗余部门。

Conclusion: 应同时提升区域贸易灵活性和生产能力来减轻灾害对供应链的影响。

Abstract: Disasters often impact supply chains, leading to cascading effects across
regions. While unaffected regions may attempt to compensate, their ability is
constrained by their available production capacity and logistical constraints
between regions. This study introduces a Multi-Regional Impact Assessment
(MRIA) model to evaluate the regional and macroeconomic consequences of
disasters, capturing regional post-disaster trade dynamics and logistical
constraints. Our findings emphasize that enhancing production capacity alone is
inadequate; regional trade flexibility must also be improved to mitigate
disaster impacts. At the regional level, disaster-affected areas experience
severe negative impacts, whereas larger, export-oriented regions benefit from
increased production activity. Additionally, we propose a sectoral criticality
assessment alongside the more common sensitivity and incremental disruption
analysis, which effectively identifies sectors with low redundancy while
accounting for the potential for regional substitution in a post-disaster
scenario.

</details>


### [201] [Systemic Trade Risk Suppresses Comparative Advantage in Rare Earth Dependent Industries](https://arxiv.org/abs/2508.00556)
*Peter Klimek,Sophia Baum,Markus Gerschberger,Maximilian Hess*

Main category: econ.GN

TL;DR: 构建2007 - 2023年稀土产品投入产出贸易网络，分析贸易依赖，分类经济体，指出战略依赖结构具层级特异性及应对策略。


<details>
  <summary>Details</summary>
Motivation: 稀土对清洁和高科技应用关键，但全球贸易依赖使各国在生产网络中面临脆弱性，需研究贸易依赖情况。

Method: 使用新型AI增强统计框架构建多层级投入产出贸易网络，计算网络依赖指标，进行回归分析。

Result: 发现上下游产品依赖差异，对经济体分类，指出高暴露预示未来出口强，高系统贸易风险阻碍比较优势发展。

Conclusion: 战略依赖结构具层级特异性，有效缓解策略应超越原材料获取，解决中游加工和关键投入生产的特定瓶颈。

Abstract: Rare earth elements (REEs) are critical to a wide range of clean and
high-tech applications, yet global trade dependencies expose countries to
vulnerabilities across production networks. Here, we construct a multi-tiered
input-output trade network spanning 168 REE-related product codes from
2007-2023 using a novel AI-augmented statistical framework. We identify
significant differences between dependencies in upstream and intermediate
(input) products, revealing that exposure and supplier concentration are
systematically higher in input products, while systemic trade risk is lower,
suggesting localized vulnerabilities. By computing network-based dependency
indicators across countries and over time, we classify economies into five
distinct clusters that capture structural differences in rare-earth reliance.
China dominates the low-risk, high-influence cluster, while the EU and US
remain vulnerable at intermediate tiers. Regression analyses show that high
exposure across all products predicts future export strength, consistent with
import substitution. However, high systemic trade risk in input products like
magnets, advanced ceramics or phosphors, significantly impedes the development
of comparative advantage. These results demonstrate that the structure of
strategic dependencies is tier-specific, with critical implications for
industrial resilience and policy design. Effective mitigation strategies must
move beyond raw material access and directly address country-specific
chokepoints in midstream processing and critical input production.

</details>


### [202] [Generative AI in Higher Education: Evidence from an Elite College](https://arxiv.org/abs/2508.00717)
*Zara Contractor,Germán Reyes*

Main category: econ.GN

TL;DR: 研究基于美国一所大学调查数据，发现超80%学生在ChatGPT发布两年内学术使用AI，其采用情况有差异，受积极认知影响，制度政策有潜在影响。


<details>
  <summary>Details</summary>
Motivation: 生成式AI改变高等教育，但学生采用的系统证据有限，需研究。

Method: 利用美国一所大学的调查数据进行分析。

Result: 超80%学生在ChatGPT发布两年内学术使用AI，采用情况因学科、人口统计和成绩水平而异，学生主要用AI促进学习和自动化任务，积极认知强预测采用，制度政策影响使用模式但可能有不同影响。

Conclusion: AI有重塑教育不平等的潜力，制度政策制定需考虑不同学生群体的影响。

Abstract: Generative AI is transforming higher education, yet systematic evidence on
student adoption remains limited. Using novel survey data from a selective U.S.
college, we document over 80 percent of students using AI academically within
two years of ChatGPT's release. Adoption varies across disciplines,
demographics, and achievement levels, highlighting AI's potential to reshape
educational inequalities. Students predominantly use AI for augmenting learning
(e.g., explanations, feedback), but also to automate tasks (e.g., essay
generation). Positive perceptions of AI's educational benefits strongly predict
adoption. Institutional policies can influence usage patterns but risk creating
unintended disparate impacts across student groups due to uneven compliance.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [203] [E2ATST: A Temporal-Spatial Optimized Energy-Efficient Architecture for Training Spiking Transformer](https://arxiv.org/abs/2508.00475)
*Yunhao Ma,Yanyu Lin,Mingjing Li,Puli Quan,Chenlin Zhou,Wenyue Zhang,Zhiwei Zhong,Wanyi Jia,Xueke Zhu,Qingyan Meng,Huihui Zhou,Fengwei An*

Main category: cs.AR

TL;DR: 仅提及多个研究机构，无有效学术内容


<details>
  <summary>Details</summary>
Motivation: 未体现研究动机

Method: 未提及研究方法

Result: 无研究结果

Conclusion: 无有效结论信息

Abstract: (1) Pengcheng Laboratory, (2) Southern University of Science and Technology,
(3) Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences,
(4) University of Chinese Academy of Sciences

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [204] [Optimal Messaging Strategy for Incentivizing Agents in Dynamic Systems](https://arxiv.org/abs/2508.00188)
*Renyan Sun,Ashutosh Nayyar*

Main category: eess.SY

TL;DR: 研究有限时域离散时间动态系统中设计者通过信息披露激励代理执行特定策略，在一定假设下可用反向归纳算法求解最优策略。


<details>
  <summary>Details</summary>
Motivation: 在设计者与代理共同控制的系统中，激励代理执行特定策略并使设计者总期望奖励最大化。

Method: 基于序贯理性定义激励兼容性，在特定信息结构假设下，使用反向归纳算法求解一系列线性规划问题。

Result: 在给定假设下，可通过反向归纳算法计算出设计者的最优策略。

Conclusion: 提出的方法能有效解决设计者在激励代理执行特定策略的同时最大化自身期望奖励的问题。

Abstract: We consider a finite-horizon discrete-time dynamic system jointly controlled
by a designer and one or more agents, where the designer can influence the
agents' actions through selective information disclosure. At each time step,
the designer sends a message to the agent(s) from a prespecified message space.
The designer may also take an action that directly influences system dynamics
and rewards. Each agent uses its received message (and its own information) to
choose its action. We are interested in the setting where the designer would
like to incentivize each agent to play a specific strategy. We consider a
notion of incentive compatibility that is based on sequential rationality at
each realization of the common information between the designer and the
agent(s). Our objective is to find a messaging and action strategy for the
designer that maximizes its total expected reward while incentivizing each
agent to follow a prespecified strategy. Under certain assumptions on the
information structure of the problem, we show that an optimal designer strategy
can be computed using a backward inductive algorithm that solves a family of
linear programs.

</details>


### [205] [Data-Driven Motion Planning for Uncertain Nonlinear Systems](https://arxiv.org/abs/2508.00154)
*Babak Esmaeili,Hamidreza Modares,Stefano Di Cairano*

Main category: eess.SY

TL;DR: 提出一种数据驱动的非线性系统运动规划框架，通过构建重叠不变多面体实现安全路径规划，仅需数据，经仿真验证有效。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖系统动力学模型，该研究旨在提出仅需数据来计算安全区域和设计状态反馈控制器的方法。

Method: 围绕随机采样的路点确定凸可行区域，求解数据驱动的线性矩阵不等式问题学习椭球不变集及局部状态反馈增益，用多面体近似椭球的凸包，验证连续凸包多面体的交集确保节点间安全过渡，通过基于单纯形的插值实时插值控制增益。

Result: 通过仿真验证了该方法能为复杂非线性系统实现安全、动态可行的路径。

Conclusion: 所提出的数据驱动运动规划框架有效，可用于复杂非线性系统的安全路径规划。

Abstract: This paper proposes a data-driven motion-planning framework for nonlinear
systems that constructs a sequence of overlapping invariant polytopes. Around
each randomly sampled waypoint, the algorithm identifies a convex admissible
region and solves data-driven linear-matrix-inequality problems to learn
several ellipsoidal invariant sets together with their local state-feedback
gains. The convex hull of these ellipsoids, still invariant under a
piece-wise-affine controller obtained by interpolating the gains, is then
approximated by a polytope. Safe transitions between nodes are ensured by
verifying the intersection of consecutive convex-hull polytopes and introducing
an intermediate node for a smooth transition. Control gains are interpolated in
real time via simplex-based interpolation, keeping the state inside the
invariant polytopes throughout the motion. Unlike traditional approaches that
rely on system dynamics models, our method requires only data to compute safe
regions and design state-feedback controllers. The approach is validated
through simulations, demonstrating the effectiveness of the proposed method in
achieving safe, dynamically feasible paths for complex nonlinear systems.

</details>


### [206] [Learning to optimize with guarantees: a complete characterization of linearly convergent algorithms](https://arxiv.org/abs/2508.00775)
*Andrea Martin,Ian R. Manchester,Luca Furieri*

Main category: eess.SY

TL;DR: 本文旨在改进线性收敛算法在特定目标问题上的平均性能，同时保留其最坏情况保证，并展示了该方法在解决特定优化问题上的有效性。


<details>
  <summary>Details</summary>
Motivation: 在高风险工程应用中，优化算法需有最坏情况保证，但设计最坏情况会牺牲实际性能，因此要改进给定线性收敛算法在特定目标问题上的平均性能。

Method: 刻画了非光滑复合优化问题类中实现线性收敛的算法类，从基线线性收敛算法出发，推导其更新规则的所有且仅有的修改以保持收敛性。

Result: 结果适用于增强如非凸梯度主导函数的梯度下降、强凸函数的Nesterov加速方法等传统算法。

Conclusion: 该方法在解决病态线性方程组和线性系统模型预测控制等迭代预算紧张的优化问题上有效。

Abstract: In high-stakes engineering applications, optimization algorithms must come
with provable worst-case guarantees over a mathematically defined class of
problems. Designing for the worst case, however, inevitably sacrifices
performance on the specific problem instances that often occur in practice. We
address the problem of augmenting a given linearly convergent algorithm to
improve its average-case performance on a restricted set of target problems -
for example, tailoring an off-the-shelf solver for model predictive control
(MPC) for an application to a specific dynamical system - while preserving its
worst-case guarantees across the entire problem class. Toward this goal, we
characterize the class of algorithms that achieve linear convergence for
classes of nonsmooth composite optimization problems. In particular, starting
from a baseline linearly convergent algorithm, we derive all - and only - the
modifications to its update rule that maintain its convergence properties. Our
results apply to augmenting legacy algorithms such as gradient descent for
nonconvex, gradient-dominated functions; Nesterov's accelerated method for
strongly convex functions; and projected methods for optimization over
polyhedral feasibility sets. We showcase effectiveness of the approach on
solving optimization problems with tight iteration budgets in application to
ill-conditioned systems of linear equations and MPC for linear systems.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [207] [Formal Power Series Representations in Probability and Expected Utility Theory](https://arxiv.org/abs/2508.00294)
*Arthur Paul Pedersen,Samuel Allen Alexander*

Main category: math.PR

TL;DR: 提出一种不设正统教义限制的连贯偏好一般理论，有偏好系统扩展及效用表示性质，扩展和强化相关定理。


<details>
  <summary>Details</summary>
Motivation: 突破正统教义对偏好理论的限制，提出更具一般性的连贯偏好理论。

Method: 设定满足类似de Finetti概率基础连贯性要求的偏好系统，构建理论。

Result: 任何满足连贯性要求的偏好系统可扩展为完整系统，完整连贯偏好系统可用实数序域扩展中的效用表示。

Conclusion: 理论既扩展了Hölder定理，又强化了Hahn嵌入定理。

Abstract: We advance a general theory of coherent preference that surrenders
restrictions embodied in orthodox doctrine. This theory enjoys the property
that any preference system admits extension to a complete system of
preferences, provided it satisfies a certain coherence requirement analogous to
the one de Finetti advanced for his foundations of probability. Unlike de
Finetti's theory, the one we set forth requires neither transitivity nor
Archimedeanness nor boundedness nor continuity of preference. This theory also
enjoys the property that any complete preference system meeting the standard of
coherence can be represented by utility in an ordered field extension of the
reals. Representability by utility is a corollary of this paper's central
result, which at once extends H\"older's Theorem and strengthens Hahn's
Embedding Theorem.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [208] [SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation](https://arxiv.org/abs/2508.00782)
*Kien T. Pham,Yingqing He,Yazhou Xing,Qifeng Chen,Long Chen*

Main category: cs.GR

TL;DR: 提出SpA2V框架，利用音频空间听觉线索生成语义和空间对应的视频，实验证明其效果好。


<details>
  <summary>Details</summary>
Motivation: 现有音频驱动视频生成方法主要关注语义信息，忽略空间属性，无法生成内容和空间构图准确的视频。

Method: 将生成过程分为两阶段，一是音频引导视频规划，用MLLM构建视频场景布局；二是布局引导视频生成，将布局融入预训练扩散模型进行无训练视频生成。

Result: SpA2V能生成与输入音频语义和空间对齐的逼真视频。

Conclusion: SpA2V在利用音频空间线索生成视频方面表现出色。

Abstract: Audio-driven video generation aims to synthesize realistic videos that align
with input audio recordings, akin to the human ability to visualize scenes from
auditory input. However, existing approaches predominantly focus on exploring
semantic information, such as the classes of sounding sources present in the
audio, limiting their ability to generate videos with accurate content and
spatial composition. In contrast, we humans can not only naturally identify the
semantic categories of sounding sources but also determine their deeply encoded
spatial attributes, including locations and movement directions. This useful
information can be elucidated by considering specific spatial indicators
derived from the inherent physical properties of sound, such as loudness or
frequency. As prior methods largely ignore this factor, we present SpA2V, the
first framework explicitly exploits these spatial auditory cues from audios to
generate videos with high semantic and spatial correspondence. SpA2V decomposes
the generation process into two stages: 1) Audio-guided Video Planning: We
meticulously adapt a state-of-the-art MLLM for a novel task of harnessing
spatial and semantic cues from input audio to construct Video Scene Layouts
(VSLs). This serves as an intermediate representation to bridge the gap between
the audio and video modalities. 2) Layout-grounded Video Generation: We develop
an efficient and effective approach to seamlessly integrate VSLs as conditional
guidance into pre-trained diffusion models, enabling VSL-grounded video
generation in a training-free manner. Extensive experiments demonstrate that
SpA2V excels in generating realistic videos with semantic and spatial alignment
to the input audios.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [209] [Constructive Disintegration and Conditional Modes](https://arxiv.org/abs/2508.00617)
*Nathaël Da Costa,Marvin Pförtner,Jon Cockayne*

Main category: math.ST

TL;DR: 本文提供构建测度分解的数学工具，用于求可微流形上分解的密度，指出受限密度和分解密度有差异，还研究分解的众数，讨论两种测度差异的实际影响。


<details>
  <summary>Details</summary>
Motivation: 解决构建测度分解困难的问题，以及受近似贝叶斯推断和贝叶斯逆问题应用的驱动。

Method: 提供一套数学工具构建分解，并应用于可微流形求分解密度。

Result: 给出受限密度和分解密度差异巨大的例子，表明“条件众数”通常与通过分解得到的条件测度众数不一致，而是受限测度的众数。

Conclusion: 两种测度存在差异，在不同建模背景下两种方法都有实用性。

Abstract: Conditioning, the central operation in Bayesian statistics, is formalised by
the notion of disintegration of measures. However, due to the implicit nature
of their definition, constructing disintegrations is often difficult. A
folklore result in machine learning conflates the construction of a
disintegration with the restriction of probability density functions onto the
subset of events that are consistent with a given observation. We provide a
comprehensive set of mathematical tools which can be used to construct
disintegrations and apply these to find densities of disintegrations on
differentiable manifolds. Using our results, we provide a disturbingly simple
example in which the restricted density and the disintegration density
drastically disagree. Motivated by applications in approximate Bayesian
inference and Bayesian inverse problems, we further study the modes of
disintegrations. We show that the recently introduced notion of a "conditional
mode" does not coincide in general with the modes of the conditional measure
obtained through disintegration, but rather the modes of the restricted
measure. We also discuss the implications of the discrepancy between the two
measures in practice, advocating for the utility of both approaches depending
on the modelling context.

</details>


### [210] [Local Poisson Deconvolution for Discrete Signals](https://arxiv.org/abs/2508.00824)
*Shayan Hundrieser,Tudor Manole,Danila Litskevich,Axel Munk*

Main category: math.ST

TL;DR: 本文分析从分箱泊松卷积模型中恢复原子信号的统计问题，量化局部极小极大风险，结果表明在更广泛信号类下可准确恢复，还应用于超分辨率显微镜数据。


<details>
  <summary>Details</summary>
Motivation: 受超分辨率激光显微镜应用启发，精确估计原子信号分布有助于了解细胞蛋白质组件的空间结构。

Method: 量化广泛平滑卷积核下估计原子信号分布的局部极小极大风险，采用多尺度损失函数分析。

Result: 结果显示在更广泛信号类下可准确恢复信号，还确定了高斯混合模型中参数估计的局部极小极大率，应用于实验数据识别DNA折纸的位置和配置。

Conclusion: 对泊松反卷积问题持乐观态度，所提方法有实际应用价值和良好性能。

Abstract: We analyze the statistical problem of recovering an atomic signal, modeled as
a discrete uniform distribution $\mu$, from a binned Poisson convolution model.
This question is motivated, among others, by super-resolution laser microscopy
applications, where precise estimation of $\mu$ provides insights into spatial
formations of cellular protein assemblies. Our main results quantify the local
minimax risk of estimating $\mu$ for a broad class of smooth convolution
kernels. This local perspective enables us to sharply quantify optimal
estimation rates as a function of the clustering structure of the underlying
signal. Moreover, our results are expressed under a multiscale loss function,
which reveals that different parts of the underlying signal can be recovered at
different rates depending on their local geometry. Overall, these results paint
an optimistic perspective on the Poisson deconvolution problem, showing that
accurate recovery is achievable under a much broader class of signals than
suggested by existing global minimax analyses. Beyond Poisson deconvolution,
our results also allow us to establish the local minimax rate of parameter
estimation in Gaussian mixture models with uniform weights.
  We apply our methods to experimental super-resolution microscopy data to
identify the location and configuration of individual DNA origamis. In
addition, we complement our findings with numerical experiments on runtime and
statistical recovery that showcase the practical performance of our estimators
and their trade-offs.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [211] [Modelling Program Spaces in Program Synthesis with Constraints](https://arxiv.org/abs/2508.00005)
*Tilman Hinnerichs,Bart Swinkels,Jaap de Jong,Reuben Gardos Reid,Tudor Magirescu,Neil Yorke-Smith,Sebastijan Dumancic*

Main category: cs.PL

TL;DR: 本文利用语法约束对程序空间建模，介绍求解器BART，评估发现约束可消除大部分程序空间并减少枚举时间。


<details>
  <summary>Details</summary>
Motivation: 解决程序综合中可能程序空间大的核心挑战。

Method: 利用语法约束对程序空间建模，引入求解器BART来高效传播和求解约束。

Result: 约束能消除高达99%的程序空间，对程序空间建模显著减少枚举时间。

Conclusion: 利用语法约束建模程序空间是有效的，可大幅减少程序空间和枚举时间。

Abstract: A core challenge in program synthesis is taming the large space of possible
programs. Since program synthesis is essentially a combinatorial search, the
community has sought to leverage powerful combinatorial constraint solvers.
Here, constraints are used to express the program semantics, but not as a
potentially potent tool to remove unwanted programs. Recent inductive logic
programming approaches introduce constraints on the program's syntax to be
synthesized. These syntactic constraints allow for checking and propagating a
constraint without executing the program, and thus for arbitrary operators. In
this work, we leverage syntactic constraints to model program spaces, defining
not just solutions that are feasible, but also ones that are likely useful. To
demonstrate this idea, we introduce BART, a solver that efficiently propagates
and solves these constraints. We evaluate BART on program space enumeration
tasks, finding that the constraints eliminate up to 99 percent of the program
space, and that modeling program spaces significantly reduces enumeration time.

</details>


### [212] [Automated Type Annotation in Python Using Large Language Models](https://arxiv.org/abs/2508.00422)
*Varun Bharti,Shashwat Jha,Dhruv Kumar,Pankaj Jalote*

Main category: cs.PL

TL;DR: 本文探索用大语言模型生成Python类型注解，开发生成-检查-修复管道，评估四种LLM变体，结果表明无需微调或额外训练的LLM在生成一致类型注解上有效，且管道可扩展到其他语言。


<details>
  <summary>Details</summary>
Motivation: 手动生成Python类型注解易出错且费力，传统自动化方法存在局限，因此探索用LLM生成类型注解。

Method: 开发生成-检查-修复管道，用静态类型检查器验证LLM生成的注解并迭代优化；在6000个代码片段上评估四种LLM变体。

Result: GPT 4oMini一致性达65.9%，GPT 4.1mini、O3Mini和O4Mini约88.6%；GPT 4.1mini和O3Mini精确匹配和基本类型匹配准确率最高，平均修复迭代少于一次。

Conclusion: 通用和推理优化的LLM无需特定微调或额外训练，在生成一致类型注解上有效，可与传统深度学习技术竞争，管道可扩展到其他可选类型的命令式语言。

Abstract: Type annotations in Python enhance maintainability and error detection.
However, generating these annotations manually is error prone and requires
extra effort. Traditional automation approaches like static analysis, machine
learning, and deep learning struggle with limited type vocabularies, behavioral
over approximation, and reliance on large labeled datasets. In this work, we
explore the use of LLMs for generating type annotations in Python. We develop a
generate check repair pipeline: the LLM proposes annotations guided by a
Concrete Syntax Tree representation, a static type checker (Mypy) verifies
them, and any errors are fed back for iterative refinement. We evaluate four
LLM variants: GPT 4oMini, GPT 4.1mini (general-purpose), and O3Mini, O4Mini
(reasoning optimized), on 6000 code snippets from the ManyTypes4Py benchmark.
We first measure the proportion of code snippets annotated by LLMs for which
MyPy reported no errors (i.e., consistent results): GPT 4oMini achieved
consistency on 65.9% of cases (34.1% inconsistent), while GPT 4.1mini, O3Mini,
and O4Mini each reached approximately 88.6% consistency (around 11.4%
failures). To measure annotation quality, we then compute exact-match and
base-type match accuracies over all 6000 snippets: GPT 4.1mini and O3Mini
perform the best, achieving up to 70.5% exact match and 79.1% base type
accuracy, requiring under one repair iteration on average. Our results
demonstrate that general-purpose and reasoning optimized LLMs, without any task
specific fine tuning or additional training can be effective in generating
consistent type annotations.They perform competitively with traditional deep
learning techniques which require large labeled dataset for training. While our
work focuses on Python, the pipeline can be extended to other optionally typed
imperative languages like Ruby

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [213] [The SPACE of AI: Real-World Lessons on AI's Impact on Developers](https://arxiv.org/abs/2508.00178)
*Brian Houck,Travis Lowdermilk,Cody Beyer,Steven Clarke,Ben Hanrahan*

Main category: cs.HC

TL;DR: 研究开发者对AI在软件开发中影响的看法，发现AI提升生产力但效益因多种因素而异，AI增强而非取代开发者，最后给出利用AI潜力的建议。


<details>
  <summary>Details</summary>
Motivation: 探究AI工具嵌入软件开发工作流对开发者生产力和体验的真实影响。

Method: 采用混合方法研究，收集500多名开发者的调查回复，结合访谈和观察研究的定性见解。

Result: AI广泛被采用，能提升生产力，尤其对于常规任务；效益因任务复杂度、个人使用模式和团队采用情况而异；开发者效率和满意度提高，对协作影响证据较少；组织支持和同行学习对发挥AI价值很关键。

Conclusion: AI增强而非取代开发者，有效集成依赖团队文化和支持结构；给出团队、组织和研究者利用AI潜力的实用建议。

Abstract: As artificial intelligence (AI) tools become increasingly embedded in
software development workflows, questions persist about their true impact on
developer productivity and experience. This paper presents findings from a
mixed-methods study examining how developers perceive AI's influence across the
dimensions of the SPACE framework: Satisfaction, Performance, Activity,
Collaboration and Efficiency. Drawing on survey responses from over 500
developers and qualitative insights from interviews and observational studies,
we find that AI is broadly adopted and widely seen as enhancing productivity,
particularly for routine tasks. However, the benefits vary, depending on task
complexity, individual usage patterns, and team-level adoption. Developers
report increased efficiency and satisfaction, with less evidence of impact on
collaboration. Organizational support and peer learning play key roles in
maximizing AI's value. These findings suggest that AI is augmenting developers
rather than replacing them, and that effective integration depends as much on
team culture and support structures as on the tools themselves. We conclude
with practical recommendations for teams, organizations and researchers seeking
to harness AI's potential in software engineering.

</details>


### [214] [Agency Among Agents: Designing with Hypertextual Friction in the Algorithmic Web](https://arxiv.org/abs/2507.23585)
*Sophia Liu,Shm Garanganao Almeda*

Main category: cs.HC

TL;DR: 文章引入“超文本摩擦”概念，通过对比分析不同界面，探讨界面结构对用户能动性的影响并提出相关设计理念。


<details>
  <summary>Details</summary>
Motivation: 解决算法驱动界面牺牲用户能动性，用户对所见内容及意义构建控制减少的问题。

Method: 对维基百科与Instagram Explore、Are.na与生成式AI图像工具等真实界面进行对比分析。

Result: 超文本系统强调来源、联想思维和用户驱动的意义构建，算法系统倾向于模糊过程和平坦化参与。

Conclusion: 进行了用户驱动与代理驱动系统中界面结构对能动性影响的对比分析，提出以超文本价值观作为设计承诺来在算法网络中夺回用户能动性的概念立场。

Abstract: Today's algorithm-driven interfaces, from recommendation feeds to GenAI
tools, often prioritize engagement and efficiency at the expense of user
agency. As systems take on more decision-making, users have less control over
what they see and how meaning or relationships between content are constructed.
This paper introduces "Hypertextual Friction," a conceptual design stance that
repositions classical hypertext principles--friction, traceability, and
structure--as actionable values for reclaiming agency in algorithmically
mediated environments. Through a comparative analysis of real-world
interfaces--Wikipedia vs. Instagram Explore, and Are.na vs. GenAI image
tools--we examine how different systems structure user experience, navigation,
and authorship. We show that hypertext systems emphasize provenance,
associative thinking, and user-driven meaning-making, while algorithmic systems
tend to obscure process and flatten participation. We contribute: (1) a
comparative analysis of how interface structures shape agency in user-driven
versus agent-driven systems, and (2) a conceptual stance that offers
hypertextual values as design commitments for reclaiming agency in an
increasingly algorithmic web.

</details>


### [215] [A Mixed User-Centered Approach to Enable Augmented Intelligence in Intelligent Tutoring Systems: The Case of MathAIde app](https://arxiv.org/abs/2508.00103)
*Guilherme Guerino,Luiz Rodrigues,Luana Bianchiniand Mariana Alves,Marcelo Marinho,Thomaz Veloso,Valmir Macario,Diego Dermeval,Thales Vieira,Ig Bittencourt,Seiji Isotani*

Main category: cs.HC

TL;DR: 本文聚焦于设计、开发和评估MathAIde这一智能辅导系统，探讨在教育中集成人工智能面临的挑战及利用增强智能解决问题，提出以教师为中心的设计方法并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 集成人工智能的教育（AIED）虽能提升学习体验，但面临教师角色、AI工具局限性及技术资源可及性等挑战，需寻找解决办法。

Method: 包括与潜在用户进行头脑风暴会议、高保真原型制作、A/B测试以及涉及真实课堂环境的案例研究。

Result: 确定了在智能辅导系统中实施增强智能的设计可能性，强调用户需求与技术可行性平衡，提出为教师提供预定义补救方案的解决方案并在现实中证明其有用性。

Conclusion: 提供了一种以教师为中心、让教师参与所有设计阶段的可用设计方法，用户中心设计方法可提高AIED系统的有用性和采用潜力，尤其在资源有限环境中。

Abstract: Integrating Artificial Intelligence in Education (AIED) aims to enhance
learning experiences through technologies like Intelligent Tutoring Systems
(ITS), offering personalized learning, increased engagement, and improved
retention rates. However, AIED faces three main challenges: the critical role
of teachers in the design process, the limitations and reliability of AI tools,
and the accessibility of technological resources. Augmented Intelligence (AuI)
addresses these challenges by enhancing human capabilities rather than
replacing them, allowing systems to suggest solutions. In contrast, humans
provide final assessments, thus improving AI over time. In this sense, this
study focuses on designing, developing, and evaluating MathAIde, an ITS that
corrects mathematics exercises using computer vision and AI and provides
feedback based on photos of student work. The methodology included
brainstorming sessions with potential users, high-fidelity prototyping, A/B
testing, and a case study involving real-world classroom environments for
teachers and students. Our research identified several design possibilities for
implementing AuI in ITSs, emphasizing a balance between user needs and
technological feasibility. Prioritization and validation through prototyping
and testing highlighted the importance of efficiency metrics, ultimately
leading to a solution that offers pre-defined remediation alternatives for
teachers. Real-world deployment demonstrated the usefulness of the proposed
solution. Our research contributes to the literature by providing a usable,
teacher-centered design approach that involves teachers in all design phases.
As a practical implication, we highlight that the user-centered design approach
increases the usefulness and adoption potential of AIED systems, especially in
resource-limited environments.

</details>


### [216] [Your Model Is Unfair, Are You Even Aware? Inverse Relationship Between Comprehension and Trust in Explainability Visualizations of Biased ML Models](https://arxiv.org/abs/2508.00140)
*Zhanna Kaufman,Madeline Endres,Cindy Xiong Bearfield,Yuriy Brun*

Main category: cs.HC

TL;DR: 研究调查可解释性可视化技术，评估五种模型可解释性可视化工具，发现理解与信任呈负相关，且受偏见感知调节，还证实可视化设计可影响理解、偏见感知和信任。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统中存在偏差行为，不同背景利益相关者对系统看法和信任不同，解释模型行为对理解和信任至关重要。

Method: 创建可解释性可视化设计特征分类法，对五种可视化工具进行用户研究。

Result: 发现理解与信任呈负相关，该关系受偏见感知强烈调节，可视化设计可显著影响理解、偏见感知和信任。

Conclusion: 增进了对理解如何影响信任的理解，系统研究了可视化在促进负责任机器学习应用中的作用。

Abstract: Systems relying on ML have become ubiquitous, but so has biased behavior
within them. Research shows that bias significantly affects stakeholders' trust
in systems and how they use them. Further, stakeholders of different
backgrounds view and trust the same systems differently. Thus, how ML models'
behavior is explained plays a key role in comprehension and trust. We survey
explainability visualizations, creating a taxonomy of design characteristics.
We conduct user studies to evaluate five state-of-the-art visualization tools
(LIME, SHAP, CP, Anchors, and ELI5) for model explainability, measuring how
taxonomy characteristics affect comprehension, bias perception, and trust for
non-expert ML users. Surprisingly, we find an inverse relationship between
comprehension and trust: the better users understand the models, the less they
trust them. We investigate the cause and find that this relationship is
strongly mediated by bias perception: more comprehensible visualizations
increase people's perception of bias, and increased bias perception reduces
trust. We confirm this relationship is causal: Manipulating explainability
visualizations to control comprehension, bias perception, and trust, we show
that visualization design can significantly (p < 0.001) increase comprehension,
increase perceived bias, and reduce trust. Conversely, reducing perceived model
bias, either by improving model fairness or by adjusting visualization design,
significantly increases trust even when comprehension remains high. Our work
advances understanding of how comprehension affects trust and systematically
investigates visualization's role in facilitating responsible ML applications.

</details>


### [217] [DeformTune: A Deformable XAI Music Prototype for Non-Musicians](https://arxiv.org/abs/2508.00160)
*Ziqing Xu,Nick Bryan-Kinns*

Main category: cs.HC

TL;DR: 介绍DeformTune系统结合可触变形界面与MeasureVAE模型探索AI音乐交互，研究无音乐训练者体验，发现挑战并提出设计机会。


<details>
  <summary>Details</summary>
Motivation: 现有AI音乐生成工具依赖文本提示等，需要非音乐人不具备的知识，要探索更直观、具身和可解释的AI交互。

Method: 引入DeformTune系统，对11名无正式音乐训练的成年参与者进行初步研究，用主题分析法分析反馈。

Result: 发现了诸如控制映射不清晰、表达范围有限、使用中需要指导等反复出现的挑战。

Conclusion: 提出增强AI可解释性的设计机会，为让AI音乐系统对新手用户更具可解释性和赋能提供早期见解。

Abstract: Many existing AI music generation tools rely on text prompts, complex
interfaces, or instrument-like controls, which may require musical or technical
knowledge that non-musicians do not possess. This paper introduces DeformTune,
a prototype system that combines a tactile deformable interface with the
MeasureVAE model to explore more intuitive, embodied, and explainable AI
interaction. We conducted a preliminary study with 11 adult participants
without formal musical training to investigate their experience with
AI-assisted music creation. Thematic analysis of their feedback revealed
recurring challenge--including unclear control mappings, limited expressive
range, and the need for guidance throughout use. We discuss several design
opportunities for enhancing explainability of AI, including multimodal feedback
and progressive interaction support. These findings contribute early insights
toward making AI music systems more explainable and empowering for novice
users.

</details>


### [218] [What's Behind the Magic? Audiences Seek Artistic Value in Generative AI's Contributions to a Live Dance Performance](https://arxiv.org/abs/2508.00239)
*Jacqueline Elise Bruen,Myounghoon Jeon*

Main category: cs.HC

TL;DR: 研究揭示公众对AI艺术价值看法不一，通过技术增强舞蹈表演实验发现，观众不知使用GenAI时更认可其艺术价值，呼吁结合社会背景和用户解读来解释技术。


<details>
  <summary>Details</summary>
Motivation: 随着生成式人工智能工具用于创作艺术，利益相关者对这些作品的价值无法达成一致，需探究公众对AI艺术的看法。

Method: 开发有或无GenAI技术增强的两个版本舞蹈表演，在观众对表演进行感知调查前后告知其表演开发情况，39名参与者被分配到四种表演中。

Result: 当个体未意识到使用GenAI时，更倾向于赋予其作品艺术价值。

Conclusion: 应重视利用社会背景和用户对GenAI的解读来构建技术解释，促进更广泛的讨论以弥合理解差距。

Abstract: With the development of generative artificial intelligence (GenAI) tools to
create art, stakeholders cannot come to an agreement on the value of these
works. In this study we uncovered the mixed opinions surrounding art made by
AI. We developed two versions of a dance performance augmented by technology
either with or without GenAI. For each version we informed audiences of the
performance's development either before or after a survey on their perceptions
of the performance. There were thirty-nine participants (13 males, 26 female)
divided between the four performances. Results demonstrated that individuals
were more inclined to attribute artistic merit to works made by GenAI when they
were unaware of its use. We present this case study as a call to address the
importance of utilizing the social context and the users' interpretations of
GenAI in shaping a technical explanation, leading to a greater discussion that
can bridge gaps in understanding.

</details>


### [219] [MetaExplainer: A Framework to Generate Multi-Type User-Centered Explanations for AI Systems](https://arxiv.org/abs/2508.00300)
*Shruthi Chari,Oshani Seneviratne,Prithwish Chakraborty,Pablo Meyer,Deborah L. McGuinness*

Main category: cs.HC

TL;DR: 本文提出MetaExplainer框架生成以用户为中心的解释，评估显示各阶段表现良好，适用于多领域。


<details>
  <summary>Details</summary>
Motivation: 解决模型提供的解释与用户需求之间的差距，增强AI系统的可解释性和可信度。

Method: 采用三阶段流程，利用大语言模型分解用户问题，委托模型解释器生成推荐，合成自然语言解释，并使用解释本体指导。

Result: 各阶段表现出色，问题重构F1分数59.06%，模型解释忠实度70%，自然语言合成上下文利用率67%，用户研究证实解释有创意且全面。

Conclusion: MetaExplainer框架具有通用性和可追溯性，是增强各领域AI可解释性的有前景工具。

Abstract: Explanations are crucial for building trustworthy AI systems, but a gap often
exists between the explanations provided by models and those needed by users.
To address this gap, we introduce MetaExplainer, a neuro-symbolic framework
designed to generate user-centered explanations. Our approach employs a
three-stage process: first, we decompose user questions into machine-readable
formats using state-of-the-art large language models (LLM); second, we delegate
the task of generating system recommendations to model explainer methods; and
finally, we synthesize natural language explanations that summarize the
explainer outputs. Throughout this process, we utilize an Explanation Ontology
to guide the language models and explainer methods. By leveraging LLMs and a
structured approach to explanation generation, MetaExplainer aims to enhance
the interpretability and trustworthiness of AI systems across various
applications, providing users with tailored, question-driven explanations that
better meet their needs. Comprehensive evaluations of MetaExplainer demonstrate
a step towards evaluating and utilizing current state-of-the-art explanation
frameworks. Our results show high performance across all stages, with a 59.06%
F1-score in question reframing, 70% faithfulness in model explanations, and 67%
context-utilization in natural language synthesis. User studies corroborate
these findings, highlighting the creativity and comprehensiveness of generated
explanations. Tested on the Diabetes (PIMA Indian) tabular dataset,
MetaExplainer supports diverse explanation types, including Contrastive,
Counterfactual, Rationale, Case-Based, and Data explanations. The framework's
versatility and traceability from using ontology to guide LLMs suggest broad
applicability beyond the tested scenarios, positioning MetaExplainer as a
promising tool for enhancing AI explainability across various domains.

</details>


### [220] [How LLMs are Shaping the Future of Virtual Reality](https://arxiv.org/abs/2508.00737)
*Süeda Özkaya,Santiago Berrezueta-Guzman,Stefan Wagner*

Main category: cs.HC

TL;DR: 本文对大语言模型（LLMs）与虚拟现实（VR）游戏结合的研究进行综述，分析应用领域、面临挑战，指出有效部署需策略，最后给出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs融入VR游戏在设计沉浸式、自适应和智能数字体验方面的作用，了解其研究现状。

Method: 分析2018 - 2025年间62篇同行评审研究。

Result: 确定了情感智能NPC、程序生成故事等关键应用领域，也指出实时性能、伦理风险等挑战，LLMs能增强VR环境体验但部署需策略。

Conclusion: 给出了多模态AI、情感计算等未来研究方向，以推动智能和包容的VR系统发展。

Abstract: The integration of Large Language Models (LLMs) into Virtual Reality (VR)
games marks a paradigm shift in the design of immersive, adaptive, and
intelligent digital experiences. This paper presents a comprehensive review of
recent research at the intersection of LLMs and VR, examining how these models
are transforming narrative generation, non-player character (NPC) interactions,
accessibility, personalization, and game mastering. Drawing from an analysis of
62 peer reviewed studies published between 2018 and 2025, we identify key
application domains ranging from emotionally intelligent NPCs and procedurally
generated storytelling to AI-driven adaptive systems and inclusive gameplay
interfaces. We also address the major challenges facing this convergence,
including real-time performance constraints, memory limitations, ethical risks,
and scalability barriers. Our findings highlight that while LLMs significantly
enhance realism, creativity, and user engagement in VR environments, their
effective deployment requires robust design strategies that integrate
multimodal interaction, hybrid AI architectures, and ethical safeguards. The
paper concludes by outlining future research directions in multimodal AI,
affective computing, reinforcement learning, and open-source development,
aiming to guide the responsible advancement of intelligent and inclusive VR
systems.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [221] [GEPAR3D: Geometry Prior-Assisted Learning for 3D Tooth Segmentation](https://arxiv.org/abs/2508.00155)
*Tomasz Szczepański,Szymon Płotka,Michal K. Grzeszczyk,Arleta Adamowicz,Piotr Fudalej,Przemysław Korzeniowski,Tomasz Trzciński,Arkadiusz Sitek*

Main category: eess.IV

TL;DR: 提出GEPAR3D方法改进牙齿CBCT图像根部分割，在多测试集表现优，有临床应用潜力。


<details>
  <summary>Details</summary>
Motivation: 牙齿CBCT图像分割有挑战，尤其是根尖精细结构分割，对正畸评估根吸收很关键。

Method: 将实例检测和多类分割统一，集成牙列统计形状模型作为几何先验，采用深度分水岭方法。

Result: GEPAR3D在各测试集上整体分割性能最佳，DSC平均95.0%，召回率95.2%，根部定性分析有显著提升。

Conclusion: GEPAR3D有潜力实现更准确根吸收评估和改善正畸临床决策。

Abstract: Tooth segmentation in Cone-Beam Computed Tomography (CBCT) remains
challenging, especially for fine structures like root apices, which is critical
for assessing root resorption in orthodontics. We introduce GEPAR3D, a novel
approach that unifies instance detection and multi-class segmentation into a
single step tailored to improve root segmentation. Our method integrates a
Statistical Shape Model of dentition as a geometric prior, capturing anatomical
context and morphological consistency without enforcing restrictive adjacency
constraints. We leverage a deep watershed method, modeling each tooth as a
continuous 3D energy basin encoding voxel distances to boundaries. This
instance-aware representation ensures accurate segmentation of narrow, complex
root apices. Trained on publicly available CBCT scans from a single center, our
method is evaluated on external test sets from two in-house and two public
medical centers. GEPAR3D achieves the highest overall segmentation performance,
averaging a Dice Similarity Coefficient (DSC) of 95.0% (+2.8% over the
second-best method) and increasing recall to 95.2% (+9.5%) across all test
sets. Qualitative analyses demonstrated substantial improvements in root
segmentation quality, indicating significant potential for more accurate root
resorption assessment and enhanced clinical decision-making in orthodontics. We
provide the implementation and dataset at https://github.com/tomek1911/GEPAR3D.

</details>


### [222] [Weakly Supervised Intracranial Aneurysm Detection and Segmentation in MR angiography via Multi-task UNet with Vesselness Prior](https://arxiv.org/abs/2508.00235)
*Erin Rainville,Amirhossein Rasoulian,Hassan Rivaz,Yiming Xiao*

Main category: eess.IV

TL;DR: 提出集成血管先验的弱监督3D多任务UNet进行颅内动脉瘤检测与分割，在多数据集验证效果佳。


<details>
  <summary>Details</summary>
Motivation: 颅内动脉瘤尺寸小、影像对比度低，检测和形态分析难，且缺乏大量带体素级标注的公共数据集，阻碍深度学习算法开发。

Method: 提出集成血管先验的弱监督3D多任务UNet，用Frangi血管滤波器导出软脑血管先验用于网络输入和注意力块，在Lausanne数据集训练，多数据集评估。

Result: 所提技术在动脉瘤分割（Dice = 0.614, 95%HD =1.38mm）和检测（假阳性率 = 1.47，灵敏度 = 92.9%）上优于SOTA技术。

Conclusion: 所提集成血管先验的弱监督3D多任务UNet在颅内动脉瘤检测和分割上性能优越。

Abstract: Intracranial aneurysms (IAs) are abnormal dilations of cerebral blood vessels
that, if ruptured, can lead to life-threatening consequences. However, their
small size and soft contrast in radiological scans often make it difficult to
perform accurate and efficient detection and morphological analyses, which are
critical in the clinical care of the disorder. Furthermore, the lack of large
public datasets with voxel-wise expert annotations pose challenges for
developing deep learning algorithms to address the issues. Therefore, we
proposed a novel weakly supervised 3D multi-task UNet that integrates
vesselness priors to jointly perform aneurysm detection and segmentation in
time-of-flight MR angiography (TOF-MRA). Specifically, to robustly guide IA
detection and segmentation, we employ the popular Frangi's vesselness filter to
derive soft cerebrovascular priors for both network input and an attention
block to conduct segmentation from the decoder and detection from an auxiliary
branch. We train our model on the Lausanne dataset with coarse ground truth
segmentation, and evaluate it on the test set with refined labels from the same
database. To further assess our model's generalizability, we also validate it
externally on the ADAM dataset. Our results demonstrate the superior
performance of the proposed technique over the SOTA techniques for aneurysm
segmentation (Dice = 0.614, 95%HD =1.38mm) and detection (false positive rate =
1.47, sensitivity = 92.9%).

</details>


### [223] [FMPlug: Plug-In Foundation Flow-Matching Priors for Inverse Problems](https://arxiv.org/abs/2508.00721)
*Yuxiang Wan,Ryan Devera,Wenjie Zhang,Ju Sun*

Main category: eess.IV

TL;DR: 提出FMPlug框架增强基础流匹配先验解决不适定逆问题，在图像超分辨率和高斯去模糊任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖特定领域或未训练的先验，希望利用新方法增强基础流匹配先验解决不适定逆问题。

Method: 引入时间自适应预热策略和尖锐高斯正则化，利用观测与期望对象的相似性和生成流的高斯性。

Result: 在图像超分辨率和高斯去模糊任务上，显著优于使用基础流匹配先验的现有方法。

Conclusion: FMPlug框架能有效增强基础流匹配先验，挖掘与领域无关的基础模型的潜力。

Abstract: We present FMPlug, a novel plug-in framework that enhances foundation
flow-matching (FM) priors for solving ill-posed inverse problems. Unlike
traditional approaches that rely on domain-specific or untrained priors, FMPlug
smartly leverages two simple but powerful insights: the similarity between
observed and desired objects and the Gaussianity of generative flows. By
introducing a time-adaptive warm-up strategy and sharp Gaussianity
regularization, FMPlug unlocks the true potential of domain-agnostic foundation
models. Our method beats state-of-the-art methods that use foundation FM priors
by significant margins, on image super-resolution and Gaussian deblurring.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [224] [Tabular Data Understanding with LLMs: A Survey of Recent Advances and Challenges](https://arxiv.org/abs/2508.00217)
*Xiaofeng Wu,Alan Ritter,Wei Xu*

Main category: cs.CL

TL;DR: 论文针对表格理解任务挑战，通过分类和介绍任务引入关键概念，指出领域研究差距。


<details>
  <summary>Details</summary>
Motivation: 表格结构多样，现有无通用方法，导航表格理解任务困难。

Method: 通过对表格输入表示进行分类和介绍表格理解任务引入关键概念。

Result: 指出领域三个关键研究差距，如检索任务主导、处理复杂表格困难、模型泛化性有限。

Conclusion: 表格理解领域存在需进一步研究的差距。

Abstract: Tables have gained significant attention in large language models (LLMs) and
multimodal large language models (MLLMs) due to their complex and flexible
structure. Unlike linear text inputs, tables are two-dimensional, encompassing
formats that range from well-structured database tables to complex,
multi-layered spreadsheets, each with different purposes. This diversity in
format and purpose has led to the development of specialized methods and tasks,
instead of universal approaches, making navigation of table understanding tasks
challenging. To address these challenges, this paper introduces key concepts
through a taxonomy of tabular input representations and an introduction of
table understanding tasks. We highlight several critical gaps in the field that
indicate the need for further research: (1) the predominance of
retrieval-focused tasks that require minimal reasoning beyond mathematical and
logical operations; (2) significant challenges faced by models when processing
complex table structures, large-scale tables, length context, or multi-table
scenarios; and (3) the limited generalization of models across different
tabular representations and formats.

</details>


### [225] [Systematic Evaluation of Optimization Techniques for Long-Context Language Models](https://arxiv.org/abs/2508.00305)
*Ammar Ahmed,Sheng Di,Franck Cappello,Zirui Liu,Jingoo Han,Ali Anwar*

Main category: cs.CL

TL;DR: 本文对大语言模型优化技术进行系统基准测试，研究其在长上下文场景下对性能指标的影响，发现简单组合优化算法对大模型有负面影响，研究有助于从业者平衡效率、准确性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型面临资源需求大、上下文窗口有限问题，现有优化技术在长上下文场景和系统评估方面效果不明。

Method: 对两种支持长上下文的大语言模型架构的单个优化方法进行分析，评估组合技术，研究单个优化方法在700亿参数模型上的可扩展性。

Result: 简单组合推理优化算法因近似误差累积对大模型有负面影响，仅依赖F1指标会掩盖问答任务中的精确率 - 召回率权衡问题。

Conclusion: 结合系统级分析和特定任务洞察，有助于大语言模型从业者和研究者在不同任务和硬件配置下平衡效率、准确性和可扩展性。

Abstract: Large language models (LLMs) excel across diverse natural language processing
tasks but face resource demands and limited context windows. Although
techniques like pruning, quantization, and token dropping can mitigate these
issues, their efficacy in long-context scenarios and system evaluation remains
underexplored. This paper systematically benchmarks these optimizations,
characterizing memory usage, latency, and throughput, and studies how these
methods impact the quality of text generation. We first analyze individual
optimization methods for two LLM architectures supporting long context and then
systematically evaluate combinations of these techniques to assess how this
deeper analysis impacts performance metrics. We subsequently study the
scalability of individual optimization methods on a larger variant with 70
billion-parameter model. Our novel insights reveal that naive combination
inference optimization algorithms can adversely affect larger models due to
compounded approximation errors, as compared to their smaller counterparts.
Experiments show that relying solely on F1 obscures these effects by hiding
precision-recall trade-offs in question answering tasks. By integrating
system-level profiling with task-specific insights, this study helps LLM
practitioners and researchers explore and balance efficiency, accuracy, and
scalability across tasks and hardware configurations.

</details>


### [226] [Segment First, Retrieve Better: Realistic Legal Search via Rhetorical Role-Based Queries](https://arxiv.org/abs/2508.00679)
*Shubham Kumar Nigam,Tanmay Dubey,Noel Shallum,Arnab Bhattacharya*

Main category: cs.CL

TL;DR: TraceRetriever是一种新的法律先例检索方法，可应对法律文件数量和复杂性增长的挑战，在部分数据集上评估效果良好


<details>
  <summary>Details</summary>
Motivation: 传统法律先例检索方法难以应对法律文件数量和复杂性的增长

Method: TraceRetriever操作时使用有限的案例信息，提取具有修辞意义的片段，集成BM25、向量数据库和交叉编码器模型，通过互反排名融合组合初始结果后再进行最终重新排序，使用在印度判决上训练的分层BiLSTM CRF分类器生成修辞注释

Result: 在IL - PCR和COLIEE 2025数据集上进行评估

Conclusion: TraceRetriever解决了文件数量增长的挑战，符合实际搜索约束，为仅有部分案例知识时的先例检索提供了可靠且可扩展的基础，能增强法律研究

Abstract: Legal precedent retrieval is a cornerstone of the common law system, governed
by the principle of stare decisis, which demands consistency in judicial
decisions. However, the growing complexity and volume of legal documents
challenge traditional retrieval methods. TraceRetriever mirrors real-world
legal search by operating with limited case information, extracting only
rhetorically significant segments instead of requiring complete documents. Our
pipeline integrates BM25, Vector Database, and Cross-Encoder models, combining
initial results through Reciprocal Rank Fusion before final re-ranking.
Rhetorical annotations are generated using a Hierarchical BiLSTM CRF classifier
trained on Indian judgments. Evaluated on IL-PCR and COLIEE 2025 datasets,
TraceRetriever addresses growing document volume challenges while aligning with
practical search constraints, reliable and scalable foundation for precedent
retrieval enhancing legal research when only partial case knowledge is
available.

</details>


### [227] [NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian Common Law System](https://arxiv.org/abs/2508.00709)
*Shubham Kumar Nigam,Balaramamahanthi Deepak Patnaik,Shivam Mishra,Ajay Varghese Thomas,Noel Shallum,Kripabandhu Ghosh,Arnab Bhattacharya*

Main category: cs.CL

TL;DR: 提出NyayaRAG框架用于印度法律判决预测，结合多种输入，结果显示结构化法律知识可提升预测和解释质量。


<details>
  <summary>Details</summary>
Motivation: 以往印度法律判决预测方法忽视普通法系统核心要素，如法定条款和司法先例，需改进。

Method: 提出NyayaRAG框架，结合事实描述、法律条文和先前案例，用特定管道评估，用多种指标和LLM评估器评估性能。

Result: 结合结构化法律知识可显著提升预测准确性和解释质量。

Conclusion: 在印度法律判决预测中，结合结构化法律知识能有效改善预测和解释效果。

Abstract: Legal Judgment Prediction (LJP) has emerged as a key area in AI for law,
aiming to automate judicial outcome forecasting and enhance interpretability in
legal reasoning. While previous approaches in the Indian context have relied on
internal case content such as facts, issues, and reasoning, they often overlook
a core element of common law systems, which is reliance on statutory provisions
and judicial precedents. In this work, we propose NyayaRAG, a
Retrieval-Augmented Generation (RAG) framework that simulates realistic
courtroom scenarios by providing models with factual case descriptions,
relevant legal statutes, and semantically retrieved prior cases. NyayaRAG
evaluates the effectiveness of these combined inputs in predicting court
decisions and generating legal explanations using a domain-specific pipeline
tailored to the Indian legal system. We assess performance across various input
configurations using both standard lexical and semantic metrics as well as
LLM-based evaluators such as G-Eval. Our results show that augmenting factual
inputs with structured legal knowledge significantly improves both predictive
accuracy and explanation quality.

</details>


### [228] [NusaAksara: A Multimodal and Multilingual Benchmark for Preserving Indonesian Indigenous Scripts](https://arxiv.org/abs/2502.18148)
*Muhammad Farid Adilazuarda,Musa Izzanardi Wijanarko,Lucky Susanto,Khumaisa Nur'aini,Derry Wijaya,Alham Fikri Aji*

Main category: cs.CL

TL;DR: 提出针对印尼语言含原始文字的公开基准NusaAksara，涵盖多模态多任务，经多种模型测试发现多数NLP技术难处理当地文字。


<details>
  <summary>Details</summary>
Motivation: 印尼语言和文字丰富，但NLP进展多基于罗马化文本，需针对原始文字的基准。

Method: 由人类专家严格构建数据，涵盖8种文字7种语言，用多种模型（如GPT - 4o、Llama 3.2等）进行基准测试。

Result: 多数NLP技术无法处理印尼当地文字，很多模型表现接近零。

Conclusion: 当前多数NLP技术难以有效处理印尼当地文字，NusaAksara基准有助于推动相关研究。

Abstract: Indonesia is rich in languages and scripts. However, most NLP progress has
been made using romanized text. In this paper, we present NusaAksara, a novel
public benchmark for Indonesian languages that includes their original scripts.
Our benchmark covers both text and image modalities and encompasses diverse
tasks such as image segmentation, OCR, transliteration, translation, and
language identification. Our data is constructed by human experts through
rigorous steps. NusaAksara covers 8 scripts across 7 languages, including
low-resource languages not commonly seen in NLP benchmarks. Although
unsupported by Unicode, the Lampung script is included in this dataset. We
benchmark our data across several models, from LLMs and VLMs such as GPT-4o,
Llama 3.2, and Aya 23 to task-specific systems such as PP-OCR and LangID, and
show that most NLP technologies cannot handle Indonesia's local scripts, with
many achieving near-zero performance.

</details>


### [229] [PhysicsEval: Inference-Time Techniques to Improve the Reasoning Proficiency of Large Language Models on Physics Problems](https://arxiv.org/abs/2508.00079)
*Oshayer Siddique,J. M Areeb Uzair Alam,Md Jobayer Rahman Rafy,Syed Rifat Raiyan,Hasan Mahmud,Md Kamrul Hasan*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The discipline of physics stands as a cornerstone of human intellect, driving
the evolution of technology and deepening our understanding of the fundamental
principles of the cosmos. Contemporary literature includes some works centered
on the task of solving physics problems - a crucial domain of natural language
reasoning. In this paper, we evaluate the performance of frontier LLMs in
solving physics problems, both mathematical and descriptive. We also employ a
plethora of inference-time techniques and agentic frameworks to improve the
performance of the models. This includes the verification of proposed solutions
in a cumulative fashion by other, smaller LLM agents, and we perform a
comparative analysis of the performance that the techniques entail. There are
significant improvements when the multi-agent framework is applied to problems
that the models initially perform poorly on. Furthermore, we introduce a new
evaluation benchmark for physics problems, ${\rm P{\small HYSICS}E{\small
VAL}}$, consisting of 19,609 problems sourced from various physics textbooks
and their corresponding correct solutions scraped from physics forums and
educational websites. Our code and data are publicly available at
https://github.com/areebuzair/PhysicsEval.

</details>


### [230] [FACTORY: A Challenging Human-Verified Prompt Set for Long-Form Factuality](https://arxiv.org/abs/2508.00109)
*Mingda Chen,Yang Li,Xilun Chen,Adina Williams,Gargi Ghosh,Scott Yih*

Main category: cs.CL

TL;DR: 介绍了大规模人工验证的提示集FACTORY用于长形式事实性评估，对6个SOTA语言模型进行评估，显示其具有挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有长形式事实性评估基准缺乏人工验证，存在质量问题。

Method: 采用模型在环方法开发并经人工完善得到FACTORY，用其和现有数据集对6个SOTA语言模型进行人工评估。

Result: FACTORY是具有挑战性的基准，SOTA模型在其响应中约40%的声明不属实，其他数据集仅10%。

Conclusion: 强调FACTORY相对于先前基准的优势、可靠性以及模型对长尾事实推理的必要性。

Abstract: Long-form factuality evaluation assesses the ability of models to generate
accurate, comprehensive responses to short prompts. Existing benchmarks often
lack human verification, leading to potential quality issues. To address this
limitation, we introduce FACTORY, a large-scale, human-verified prompt set.
Developed using a model-in-the-loop approach and refined by humans, FACTORY
includes challenging prompts that are fact-seeking, answerable, and
unambiguous. We conduct human evaluations on 6 state-of-the-art language models
using FACTORY and existing datasets. Our results show that FACTORY is a
challenging benchmark: approximately 40% of the claims made in the responses of
SOTA models are not factual, compared to only 10% for other datasets. Our
analysis identifies the strengths of FACTORY over prior benchmarks, emphasizing
its reliability and the necessity for models to reason across long-tailed
facts.

</details>


### [231] [Model Misalignment and Language Change: Traces of AI-Associated Language in Unscripted Spoken English](https://arxiv.org/abs/2508.00238)
*Bryce Anderson,Riley Galpin,Tom S. Juzek*

Main category: cs.CL

TL;DR: 分析2022年ChatGPT发布前后口语词汇趋势，发现与大语言模型相关词汇使用增加，反映语言使用或有显著转变。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型影响下词汇使用变化是反映人类语言系统变化，还是仅为直接使用AI工具的结果。

Method: 构建2210万字的科技播客口语数据集，分析ChatGPT发布前后与大语言模型相关常用词汇的趋势。

Result: 2022年后相关词汇使用适度但显著增加，基准同义词无显著变化。

Conclusion: 可能预示语言使用显著转变，尚不清楚是自然语言变化还是AI驱动，也可能受模型训练偏差影响，还引发伦理担忧。

Abstract: In recent years, written language, particularly in science and education, has
undergone remarkable shifts in word usage. These changes are widely attributed
to the growing influence of Large Language Models (LLMs), which frequently rely
on a distinct lexical style. Divergences between model output and target
audience norms can be viewed as a form of misalignment. While these shifts are
often linked to using Artificial Intelligence (AI) directly as a tool to
generate text, it remains unclear whether the changes reflect broader changes
in the human language system itself. To explore this question, we constructed a
dataset of 22.1 million words from unscripted spoken language drawn from
conversational science and technology podcasts. We analyzed lexical trends
before and after ChatGPT's release in 2022, focusing on commonly LLM-associated
words. Our results show a moderate yet significant increase in the usage of
these words post-2022, suggesting a convergence between human word choices and
LLM-associated patterns. In contrast, baseline synonym words exhibit no
significant directional shift. Given the short time frame and the number of
words affected, this may indicate the onset of a remarkable shift in language
use. Whether this represents natural language change or a novel shift driven by
AI exposure remains an open question. Similarly, although the shifts may stem
from broader adoption patterns, it may also be that upstream training
misalignments ultimately contribute to changes in human language use. These
findings parallel ethical concerns that misaligned models may shape social and
moral beliefs.

</details>


### [232] [SynAdapt: Learning Adaptive Reasoning in Large Language Models via Synthetic Continuous Chain-of-Thought](https://arxiv.org/abs/2508.00574)
*Jianwei Wang,Ziming Wu,Fuming Lai,Shaobing Lian,Ziqian Zeng*

Main category: cs.CL

TL;DR: 现有CoT推理有时间成本，CCoT有局限，提出SynAdapt框架，实验证明其能实现最佳准确率 - 效率权衡。


<details>
  <summary>Details</summary>
Motivation: Chain - of - Thought推理有时间成本，现有Continuous CoT方法存在间接微调、对齐有限或目标不一致等问题。

Method: 提出SynAdapt框架，生成合成CCoT作为对齐目标，集成难度分类器识别难题并自适应提示大语言模型重新思考。

Result: 在不同难度的各种基准测试中取得了良好的实验结果。

Conclusion: SynAdapt方法能实现最佳的准确率 - 效率权衡。

Abstract: While Chain-of-Thought (CoT) reasoning improves model performance, it incurs
significant time costs due to the generation of discrete CoT tokens (DCoT).
Continuous CoT (CCoT) offers a more efficient alternative, but existing CCoT
methods are hampered by indirect fine-tuning, limited alignment, or
inconsistent targets. To overcome these limitations, we propose
\textit{SynAdapt}, an innovative efficient reasoning framework. Specifically,
\textit{SynAdapt} generates the synthetic CCoT to serve as a precise and
effective alignment target for LLMs. This synthetic CCoT explicitly guides the
LLM to learn CCoT and derive accurate answers directly. Furthermore, relying
solely on CCoT is insufficient for solving hard questions. To address this,
\textit{SynAdapt} integrates a difficulty classifier that leverages both
question context and CCoT to identify hard questions. CCoT can effectively help
identify hard questions after some brief reasoning. We then adaptively prompt
the LLM to re-think these hard questions for improved performance. Extensive
experimental results across various benchmarks from different difficulty levels
strongly demonstrate the effectiveness of our method, achieving the best
accuracy-efficiency trade-off.

</details>


### [233] [EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level Efficiency for Edge Devices](https://arxiv.org/abs/2508.00370)
*Jiyu Chen,Poh Seng Lim,Shuang Peng,Daxiong Luo,JungHau Foo,Yap Deep,Timothy Lee Jun Jie,Kelvin Teh Kae Wen,Fan Yang,Danyu Feng,Hao-Yun Chen,Peng-Wen Chen,Fangyuan Li,Xiaoxin Chen,Wong Wai Mun*

Main category: cs.CL

TL;DR: 现有基于Transformer的大语言模型在资源受限边缘设备上处理长序列任务有挑战，EdgeInfinite有不足，本文提出EdgeInfinite - Instruct，经实验能提升特定领域性能并保持效率。


<details>
  <summary>Details</summary>
Motivation: 解决EdgeInfinite在指令跟随能力和移动特定优化方面的不足，实现在资源受限边缘设备上高效处理长序列任务。

Method: 提出针对长序列任务的分段监督微调（S - SFT）策略，采用细粒度训练后量化（PTQ）降低计算需求，实现固定形状计算图平衡内存使用和设备效率。

Result: 在长上下文基准测试和实际移动任务实验中，该方法提升了特定领域性能，在NPU加速边缘设备上保持效率。

Conclusion: EdgeInfinite - Instruct方法能在资源受限边缘设备上有效处理长序列任务，提升特定领域性能并保持效率。

Abstract: Deploying Transformer-based large language models (LLMs) on
resource-constrained edge devices for long-sequence tasks remains challenging
due to the quadratic time complexity of self-attention and growing Key-Value
(KV) cache demands. While existing KV cache optimizations improve memory
efficiency, they often fail to reduce time to first token (TTFT) and may
degrade performance through token pruning. Alternative sequence modeling
architectures address some of these limitations, but typically require full
retraining and lack infrastructure support. EdgeInfinite offers an efficient
solution by fine-tuning only a small subset of parameters, maintaining quality
while reducing both computational and memory costs, including improved TTFT.
However, its instruction-following ability is limited, and it lacks
mobile-specific optimizations. To address these issues, we propose
EdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning
(S-SFT) strategy tailored to long-sequence tasks such as summarization and
question answering. We further optimized EdgeInfinite-Instruct for efficient
deployment on edge NPUs by employing fine-grained post-training quantization
(PTQ) to reduce computational demands while maintaining accuracy, and by
implementing a fixed-shape computation graph that balances memory usage and
on-device efficiency through scenario-specific customization of input token and
cache sizes. Experiments on long-context benchmarks and real-world mobile tasks
show that our approach improves domain-specific performance while maintaining
efficiency on NPU-accelerated edge devices.

</details>


### [234] [Multi-Layer Attention is the Amplifier of Demonstration Effectiveness](https://arxiv.org/abs/2508.00385)
*Dingzirui Wang,Xuangliang Zhang,Keyan Xu,Qingfu Zhu,Wanxiang Che,Yang Deng*

Main category: cs.CL

TL;DR: 文章研究上下文学习（ICL）中示例无效的原因，基于梯度流和线性自注意力模型推导，提出用梯度流选择示例的GradS方法，实验验证推导并证明GradS有效。


<details>
  <summary>Details</summary>
Motivation: 现有研究多假设ICL中示例有效，但实际并非所有示例都能提升性能，因此要探究示例无效的原因。

Method: 基于梯度流和线性自注意力模型分析，将梯度流设为零推导示例无效条件；提出GradS方法，以示例关于用户查询的梯度流大小为标准选择示例。

Result: 实验证实模型层数增加会放大示例有效性差异；GradS比最强基线平均相对提升6.8%。

Conclusion: 文章推导合理，GradS方法能有效选择示例，提升性能。

Abstract: Numerous studies have investigated the underlying mechanisms of in-context
learning (ICL) effectiveness to inspire the design of related methods. However,
existing work predominantly assumes the effectiveness of the demonstrations
provided within ICL, while many research indicates that not all demonstrations
are effective, failing to yielding any performance improvement during ICL.
Therefore, in this paper, we investigate the reasons behind demonstration
ineffectiveness. Our analysis is based on gradient flow and linear
self-attention models. By setting the gradient flow to zero, we deduce that a
demonstration becomes ineffective if its information has either been learned by
the model or is irrelevant to the user query. Furthermore, we demonstrate that
in multi-layer models, the disparity in effectiveness among demonstrations is
amplified with layer increasing, causing the model to focus more on effective
ones. Considering that current demonstration selection methods primarily focus
on the relevance to the user query while overlooking the information that the
model has already assimilated, we propose a novel method called GradS, which
leverages gradient flow for demonstration selection. We use the magnitude of
the gradient flow of the demonstration with respect to a given user query as
the criterion, thereby ensuring the effectiveness of the chosen ones. We
validate our derivation and GradS on four prominent LLMs across five mainstream
datasets. The experimental results confirm that the disparity in effectiveness
among demonstrations is magnified as the model layer increases, substantiating
our derivations. Moreover, GradS achieves a relative improvement of $6.8\%$ on
average over the strongest baselines, demonstrating its effectiveness.

</details>


### [235] [Prompting Science Report 3: I'll pay you or I'll kill you -- but will you care?](https://arxiv.org/abs/2508.00614)
*Lennart Meincke,Ethan Mollick,Lilach Mollick,Dan Shapiro*

Main category: cs.CL

TL;DR: 报告通过测试研究对AI模型给予小费和威胁两种提示方式，发现对基准性能无显著影响，提示变化对单题表现有影响但难预测效果，简单提示变化可能不如预期有效。


<details>
  <summary>Details</summary>
Motivation: 帮助商业、教育和政策领导者了解与AI合作的技术细节，对两种常见的AI提示信念进行实证测试。

Method: 在GPQA和MMLU - Pro上评估模型性能。

Result: 威胁或给小费对基准性能无显著影响；提示变化对单题表现有显著影响，但难提前知晓对特定问题的作用。

Conclusion: 简单提示变化可能不如之前认为的有效，尤其是针对难题，但提示方法对个别问题会产生不同结果。

Abstract: This is the third in a series of short reports that seek to help business,
education, and policy leaders understand the technical details of working with
AI through rigorous testing. In this report, we investigate two commonly held
prompting beliefs: a) offering to tip the AI model and b) threatening the AI
model. Tipping was a commonly shared tactic for improving AI performance and
threats have been endorsed by Google Founder Sergey Brin (All-In, May 2025,
8:20) who observed that 'models tend to do better if you threaten them,' a
claim we subject to empirical testing here. We evaluate model performance on
GPQA (Rein et al. 2024) and MMLU-Pro (Wang et al. 2024).
  We demonstrate two things:
  - Threatening or tipping a model generally has no significant effect on
benchmark performance.
  - Prompt variations can significantly affect performance on a per-question
level. However, it is hard to know in advance whether a particular prompting
approach will help or harm the LLM's ability to answer any particular question.
  Taken together, this suggests that simple prompting variations might not be
as effective as previously assumed, especially for difficult problems. However,
as reported previously (Meincke et al. 2025a), prompting approaches can yield
significantly different results for individual questions.

</details>


### [236] [ReaGAN: Node-as-Agent-Reasoning Graph Agentic Network](https://arxiv.org/abs/2508.00429)
*Minghao Guo,Xi Zhu,Jingyuan Huang,Kai Mei,Yongfeng Zhang*

Main category: cs.CL

TL;DR: 现有GNN有局限性，提出ReaGAN框架，它能让节点自主决策和建立全局关系，在少样本情境下有竞争力。


<details>
  <summary>Details</summary>
Motivation: 解决现有GNN固定聚合机制无法处理节点信息不平衡、忽略全局语义关系的问题。

Method: 提出基于代理的ReaGAN框架，让节点作为代理自主规划，利用RAG建立全局关系。

Result: ReaGAN在少样本上下文设置中，使用冻结的LLM骨干且无需微调就取得了有竞争力的性能。

Conclusion: 展示了代理规划和本地 - 全局检索在图学习中的潜力。

Abstract: Graph Neural Networks (GNNs) have achieved remarkable success in graph-based
learning by propagating information among neighbor nodes via predefined
aggregation mechanisms. However, such fixed schemes often suffer from two key
limitations. First, they cannot handle the imbalance in node informativeness --
some nodes are rich in information, while others remain sparse. Second,
predefined message passing primarily leverages local structural similarity
while ignoring global semantic relationships across the graph, limiting the
model's ability to capture distant but relevant information. We propose
Retrieval-augmented Graph Agentic Network (ReaGAN), an agent-based framework
that empowers each node with autonomous, node-level decision-making. Each node
acts as an agent that independently plans its next action based on its internal
memory, enabling node-level planning and adaptive message propagation.
Additionally, retrieval-augmented generation (RAG) allows nodes to access
semantically relevant content and build global relationships in the graph.
ReaGAN achieves competitive performance under few-shot in-context settings
using a frozen LLM backbone without fine-tuning, showcasing the potential of
agentic planning and local-global retrieval in graph learning.

</details>


### [237] [Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement Techniques and Applications](https://arxiv.org/abs/2508.00669)
*Wenxuan Wang,Zizhan Ma,Meidan Ding,Shiyi Zheng,Shengyuan Liu,Jie Liu,Jiaming Ji,Wenting Chen,Xiang Li,Linlin Shen,Yixuan Yuan*

Main category: cs.CL

TL;DR: 本文对医学大语言模型推理领域进行系统综述，提出推理增强技术分类，分析其应用与评估基准演变，指出挑战并指明未来方向。


<details>
  <summary>Details</summary>
Motivation: 医学大语言模型在系统、透明和可验证推理能力上存在差距，促使研究转向专为医学推理设计的大语言模型，本文旨在对该新兴领域进行系统综述。

Method: 提出推理增强技术分类，分析不同数据模态和临床应用中的技术应用，调查评估基准演变，基于2022 - 2025年60项开创性研究进行分析。

Result: 明确了推理增强技术分类，了解其在不同场景应用及评估基准变化。

Conclusion: 指出关键挑战，如忠实性 - 似真性差距和原生多模态推理需求，指明构建高效、强大且符合社会技术责任的医学人工智能的未来方向。

Abstract: The proliferation of Large Language Models (LLMs) in medicine has enabled
impressive capabilities, yet a critical gap remains in their ability to perform
systematic, transparent, and verifiable reasoning, a cornerstone of clinical
practice. This has catalyzed a shift from single-step answer generation to the
development of LLMs explicitly designed for medical reasoning. This paper
provides the first systematic review of this emerging field. We propose a
taxonomy of reasoning enhancement techniques, categorized into training-time
strategies (e.g., supervised fine-tuning, reinforcement learning) and test-time
mechanisms (e.g., prompt engineering, multi-agent systems). We analyze how
these techniques are applied across different data modalities (text, image,
code) and in key clinical applications such as diagnosis, education, and
treatment planning. Furthermore, we survey the evolution of evaluation
benchmarks from simple accuracy metrics to sophisticated assessments of
reasoning quality and visual interpretability. Based on an analysis of 60
seminal studies from 2022-2025, we conclude by identifying critical challenges,
including the faithfulness-plausibility gap and the need for native multimodal
reasoning, and outlining future directions toward building efficient, robust,
and sociotechnically responsible medical AI.

</details>


### [238] [PaPaformer: Language Model from Pre-trained Paraller Paths](https://arxiv.org/abs/2508.00544)
*Joonas Tapaninaho,Mourad Oussala*

Main category: cs.CL

TL;DR: 本文提出 PaPaformer 架构，可在数小时内完成仅解码器的基于 Transformer 的语言模型训练，减少参数与时间并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型训练需大量计算资源和时间，小语言模型训练也需数天，探索更快的训练和评估方法。

Method: 引入 PaPaformer 架构，将低维并行路径组合成更大模型，各路径用不同数据单独训练后合并。

Result: 该方法可减少模型参数总数和训练时间，同时提升性能，并行路径结构可定制以适应特定任务。

Conclusion: PaPaformer 架构能有效解决语言模型训练耗时久的问题，且有定制化潜力。

Abstract: The training of modern large-language models requires an increasingly amount
of computation power and time. Even smaller variants, such as small-language
models (SLMs), take several days to train in the best-case scenarios, often
requiring multiple GPUs. This paper explores methods to train and evaluate
decoder-only transformer-based language models in hours instead of days/weeks.
We introduces \textit{PaPaformer}, a decoder-only transformer architecture
variant, whose lower-dimensional parallel paths are combined into larger model.
The paper shows that these lower-dimensional paths can be trained individually
with different types of training data and then combined into one larger model.
This method gives the option to reduce the total number of model parameters and
the training time with increasing performance. Moreover, the use of parallel
path structure opens interesting possibilities to customize paths to
accommodate specific task requirements.

</details>


### [239] [A Context-Aware Dual-Metric Framework for Confidence Estimation in Large Language Models](https://arxiv.org/abs/2508.00600)
*Mingruo Yuan,Shuyi Zhang,Ben Kao*

Main category: cs.CL

TL;DR: 提出CRUX框架用于大语言模型置信度估计，在多数据集实验中比现有基线表现好。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型置信度估计方法忽略响应与上下文信息相关性，本文旨在弥补这一差距。

Method: 提出CRUX框架，通过上下文熵减少和统一一致性检查两个新指标整合上下文忠实性和一致性进行置信度估计。

Result: 在三个基准数据集和两个特定领域数据集实验中，CRUX达到比现有基线更高的AUROC。

Conclusion: CRUX框架在大语言模型置信度估计中有效。

Abstract: Accurate confidence estimation is essential for trustworthy large language
models (LLMs) systems, as it empowers the user to determine when to trust
outputs and enables reliable deployment in safety-critical applications.
Current confidence estimation methods for LLMs neglect the relevance between
responses and contextual information, a crucial factor in output quality
evaluation, particularly in scenarios where background knowledge is provided.
To bridge this gap, we propose CRUX (Context-aware entropy Reduction and
Unified consistency eXamination), the first framework that integrates context
faithfulness and consistency for confidence estimation via two novel metrics.
First, contextual entropy reduction represents data uncertainty with the
information gain through contrastive sampling with and without context. Second,
unified consistency examination captures potential model uncertainty through
the global consistency of the generated answers with and without context.
Experiments across three benchmark datasets (CoQA, SQuAD, QuAC) and two
domain-specific datasets (BioASQ, EduQG) demonstrate CRUX's effectiveness,
achieving the highest AUROC than existing baselines.

</details>


### [240] [DACTYL: Diverse Adversarial Corpus of Texts Yielded from Large Language Models](https://arxiv.org/abs/2508.00619)
*Shantanu Thorat,Andrew Caines*

Main category: cs.CL

TL;DR: 现有AIG文本检测器在现实中表现不佳，本文引入DACTYL数据集，用两种方法训练分类器，发现DXO分类器泛化性更好。


<details>
  <summary>Details</summary>
Motivation: 解决现有AIG文本检测器在现实中不够鲁棒的问题，且当前数据集对少样本生成文本关注不足。

Method: 引入DACTYL数据集，包含少样本生成文本和特定领域持续预训练模型生成的文本；用标准二元交叉熵（BCE）和深度X风险优化（DXO）两种方法训练分类器。

Result: 现有检测器在DACTYL数据集上表现差；BCE训练的分类器在DACTYL测试集上略胜一筹，DXO分类器在分布外文本上表现出色；在模拟学生论文检测场景中，DXO分类器表现更优。

Conclusion: DXO分类器泛化性更好，实验指出了AIG文本检测器的改进方向。

Abstract: Existing AIG (AI-generated) text detectors struggle in real-world settings
despite succeeding in internal testing, suggesting that they may not be robust
enough. We rigorously examine the machine-learning procedure to build these
detectors to address this. Most current AIG text detection datasets focus on
zero-shot generations, but little work has been done on few-shot or one-shot
generations, where LLMs are given human texts as an example. In response, we
introduce the Diverse Adversarial Corpus of Texts Yielded from Language models
(DACTYL), a challenging AIG text detection dataset focusing on
one-shot/few-shot generations. We also include texts from domain-specific
continued-pre-trained (CPT) language models, where we fully train all
parameters using a memory-efficient optimization approach. Many existing AIG
text detectors struggle significantly on our dataset, indicating a potential
vulnerability to one-shot/few-shot and CPT-generated texts. We also train our
own classifiers using two approaches: standard binary cross-entropy (BCE)
optimization and a more recent approach, deep X-risk optimization (DXO). While
BCE-trained classifiers marginally outperform DXO classifiers on the DACTYL
test set, the latter excels on out-of-distribution (OOD) texts. In our mock
deployment scenario in student essay detection with an OOD student essay
dataset, the best DXO classifier outscored the best BCE-trained classifier by
50.56 macro-F1 score points at the lowest false positive rates for both. Our
results indicate that DXO classifiers generalize better without overfitting to
the test set. Our experiments highlight several areas of improvement for AIG
text detectors.

</details>


### [241] [Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and Context-Aware KGQA](https://arxiv.org/abs/2508.00719)
*Yingxu Wang,Shiqi Fan,Mengzhu Wang,Siwei Liu*

Main category: cs.CL

TL;DR: 本文提出DAMR框架解决现有KGQA方法问题，在多基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有KGQA方法存在适应性有限、计算成本高和路径评估不准确等问题，需新方法解决。

Method: 提出DAMR框架，采用MCTS骨干结合LLM规划器减少搜索空间，引入轻量级Transformer评分器提高路径评估准确性，还有动态伪路径细化机制缓解监督数据不足。

Result: 在多个KGQA基准测试中，DAMR显著优于现有方法。

Conclusion: DAMR框架能有效解决现有KGQA方法问题，实现高效和上下文感知的KGQA。

Abstract: Knowledge Graph Question Answering (KGQA) aims to interpret natural language
queries and perform structured reasoning over knowledge graphs by leveraging
their relational and semantic structures to retrieve accurate answers. Recent
KGQA methods primarily follow either retrieve-then-reason paradigm, relying on
GNNs or heuristic rules for static paths extraction, or dynamic path generation
strategies that use large language models (LLMs) with prompting to jointly
perform retrieval and reasoning. However, the former suffers from limited
adaptability due to static path extraction and lack of contextual refinement,
while the latter incurs high computational costs and struggles with accurate
path evaluation due to reliance on fixed scoring functions and extensive LLM
calls. To address these issues, this paper proposes Dynamically Adaptive
MCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search
with adaptive path evaluation for efficient and context-aware KGQA. DAMR
employs a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based
planner, which selects top-$k$ relevant relations at each step to reduce search
space. To improve path evaluation accuracy, we introduce a lightweight
Transformer-based scorer that performs context-aware plausibility estimation by
jointly encoding the question and relation sequence through cross-attention,
enabling the model to capture fine-grained semantic shifts during multi-hop
reasoning. Furthermore, to alleviate the scarcity of high-quality supervision,
DAMR incorporates a dynamic pseudo-path refinement mechanism that periodically
generates training signals from partial paths explored during search, allowing
the scorer to continuously adapt to the evolving distribution of reasoning
trajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR
significantly outperforms state-of-the-art methods.

</details>


### [242] [Out-of-Context Abduction: LLMs Make Inferences About Procedural Data Leveraging Declarative Facts in Earlier Training Data](https://arxiv.org/abs/2508.00741)
*Sohaib Imran,Rob Lamb,Peter M. Atkinson*

Main category: cs.CL

TL;DR: 研究大语言模型上下文外溯因推理能力，发现GPT 4o能推断聊天机器人名字及展现特定行为，对大模型情境感知和AI安全有意义。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型能否对训练数据中的信息进行推理。

Method: 设计实验研究大语言模型上下文外溯因推理能力，训练处理模型但不提供对话示例。

Result: GPT 4o能在观察特征回复后推断至少一个聊天机器人的名字，预训练行为描述后能展现更具特征的行为。

Conclusion: 研究结果对大语言模型的情境感知和AI安全有影响。

Abstract: Large language models (LLMs) are trained on large corpora, yet it is unclear
whether they can reason about the information present within their training
data. We design experiments to study out-of-context abduction in LLMs, the
ability to infer the most plausible explanations for observations using
relevant facts present in training data. We train treatment LLMs on names and
behavior descriptions of fictitious chatbots, but not on examples of dialogue
with the chatbots. We find that OpenAI's GPT 4o LLM can correctly infer at
least one chatbot's name after observing example responses characteristic of
that chatbot. We also find that previously training GPT 4o on descriptions of a
chatbot's behavior allows it to display behaviors more characteristic of the
chatbot when iteratively trained to display such behaviors. Our results have
implications for situational awareness in LLMs and, therefore, for AI safety.

</details>


### [243] [Agentic large language models improve retrieval-based radiology question answering](https://arxiv.org/abs/2508.00743)
*Sebastian Wind,Jeta Sopa,Daniel Truhn,Mahshad Lotfinia,Tri-Thien Nguyen,Keno Bressem,Lisa Adams,Mirabela Rusu,Harald Köstler,Gerhard Wellein,Andreas Maier,Soroosh Tayebi Arasteh*

Main category: cs.CL

TL;DR: 提出代理式RAG框架提升放射学问答性能，在多种LLMs上验证有显著效果。


<details>
  <summary>Details</summary>
Motivation: 传统放射学问答的RAG系统单步检索能力有限，难以处理复杂临床推理任务。

Method: 提出代理式RAG框架，使LLMs自主分解问题、迭代检索证据并合成回答，用104个专业放射学问题评估24种不同LLMs。

Result: 代理式检索显著提高平均诊断准确率，减少幻觉，在中型和小型模型上提升明显，非常大模型提升小，临床微调模型也有改善。

Conclusion: 代理式框架有潜力提高放射学问答的事实性和诊断准确性，特别是中型LLMs，需进一步研究验证临床实用性。

Abstract: Clinical decision-making in radiology increasingly benefits from artificial
intelligence (AI), particularly through large language models (LLMs). However,
traditional retrieval-augmented generation (RAG) systems for radiology question
answering (QA) typically rely on single-step retrieval, limiting their ability
to handle complex clinical reasoning tasks. Here we propose an agentic RAG
framework enabling LLMs to autonomously decompose radiology questions,
iteratively retrieve targeted clinical evidence from Radiopaedia, and
dynamically synthesize evidence-based responses. We evaluated 24 LLMs spanning
diverse architectures, parameter scales (0.5B to >670B), and training paradigms
(general-purpose, reasoning-optimized, clinically fine-tuned), using 104
expert-curated radiology questions from previously established RSNA-RadioQA and
ExtendedQA datasets. Agentic retrieval significantly improved mean diagnostic
accuracy over zero-shot prompting (73% vs. 64%; P<0.001) and conventional
online RAG (73% vs. 68%; P<0.001). The greatest gains occurred in mid-sized
models (e.g., Mistral Large improved from 72% to 81%) and small-scale models
(e.g., Qwen 2.5-7B improved from 55% to 71%), while very large models (>200B
parameters) demonstrated minimal changes (<2% improvement). Additionally,
agentic retrieval reduced hallucinations (mean 9.4%) and retrieved clinically
relevant context in 46% of cases, substantially aiding factual grounding. Even
clinically fine-tuned models exhibited meaningful improvements (e.g.,
MedGemma-27B improved from 71% to 81%), indicating complementary roles of
retrieval and fine-tuning. These results highlight the potential of agentic
frameworks to enhance factuality and diagnostic accuracy in radiology QA,
particularly among mid-sized LLMs, warranting future studies to validate their
clinical utility.

</details>


### [244] [Applying Psychometrics to Large Language Model Simulated Populations: Recreating the HEXACO Personality Inventory Experiment with Generative Agents](https://arxiv.org/abs/2508.00742)
*Sarah Mercer,Daniel P. Martin,Phil Swatton*

Main category: cs.CL

TL;DR: 本文探讨基于角色的生成式智能体在代表人类群体方面的有效性，通过实验发现其部分符合HEXACO框架，不同模型存在差异，为社会科学研究使用智能体提供指导。


<details>
  <summary>Details</summary>
Motivation: 探究基于角色的生成式智能体在代表人类群体方面的有效性，因其可作为社会科学研究中人类参与者的经济替代方案。

Method: 对310个由GPT - 4驱动的智能体进行HEXACO人格量表实验，对其回答进行因子分析，并与2004年Ashton等人的原始结果对比。

Result: 1. 从智能体回答中可恢复出连贯可靠的人格结构，部分符合HEXACO框架；2. 在有足够筛选的群体中，GPT - 4得出的人格维度一致且可靠；3. 跨模型分析显示人格分析存在差异，表明有模型特定的偏差和局限。

Conclusion: 本研究有助于探讨在社会科学研究中使用生成式智能体的利弊，并为设计能最大程度覆盖和代表人类人格特质的智能体角色提供指导。

Abstract: Generative agents powered by Large Language Models demonstrate human-like
characteristics through sophisticated natural language interactions. Their
ability to assume roles and personalities based on predefined character
biographies has positioned them as cost-effective substitutes for human
participants in social science research. This paper explores the validity of
such persona-based agents in representing human populations; we recreate the
HEXACO personality inventory experiment by surveying 310 GPT-4 powered agents,
conducting factor analysis on their responses, and comparing these results to
the original findings presented by Ashton, Lee, & Goldberg in 2004. Our results
found 1) a coherent and reliable personality structure was recoverable from the
agents' responses demonstrating partial alignment to the HEXACO framework. 2)
the derived personality dimensions were consistent and reliable within GPT-4,
when coupled with a sufficiently curated population, and 3) cross-model
analysis revealed variability in personality profiling, suggesting
model-specific biases and limitations. We discuss the practical considerations
and challenges encountered during the experiment. This study contributes to the
ongoing discourse on the potential benefits and limitations of using generative
agents in social science research and provides useful guidance on designing
consistent and representative agent personas to maximise coverage and
representation of human personality traits.

</details>


### [245] [MMBERT: Scaled Mixture-of-Experts Multimodal BERT for Robust Chinese Hate Speech Detection under Cloaking Perturbations](https://arxiv.org/abs/2508.00760)
*Qiyao Xue,Yuchen Dou,Ryan Shi,Xiang Lorraine Li,Wei Gao*

Main category: cs.CL

TL;DR: 提出基于BERT的多模态框架MMBERT检测中文社交网络仇恨言论，效果超现有模型。


<details>
  <summary>Details</summary>
Motivation: 中文社交网络仇恨言论检测有挑战，现有工作多关注英文数据集且对中文多模态策略关注有限。

Method: 提出MMBERT框架，用Mixture-of-Experts架构集成文本、语音和视觉模态，开发渐进式三阶段训练范式，有特定专家、共享自注意力机制和基于路由器的专家分配策略。

Result: 在多个中文仇恨言论数据集上，MMBERT显著超越微调的基于BERT的编码器模型、微调的大语言模型和使用上下文学习方法的大语言模型。

Conclusion: MMBERT在中文仇恨言论检测上有显著优势，能增强对抗扰动的鲁棒性。

Abstract: Hate speech detection on Chinese social networks presents distinct
challenges, particularly due to the widespread use of cloaking techniques
designed to evade conventional text-based detection systems. Although large
language models (LLMs) have recently improved hate speech detection
capabilities, the majority of existing work has concentrated on English
datasets, with limited attention given to multimodal strategies in the Chinese
context. In this study, we propose MMBERT, a novel BERT-based multimodal
framework that integrates textual, speech, and visual modalities through a
Mixture-of-Experts (MoE) architecture. To address the instability associated
with directly integrating MoE into BERT-based models, we develop a progressive
three-stage training paradigm. MMBERT incorporates modality-specific experts, a
shared self-attention mechanism, and a router-based expert allocation strategy
to enhance robustness against adversarial perturbations. Empirical results in
several Chinese hate speech datasets show that MMBERT significantly surpasses
fine-tuned BERT-based encoder models, fine-tuned LLMs, and LLMs utilizing
in-context learning approaches.

</details>


### [246] [Do They Understand Them? An Updated Evaluation on Nonbinary Pronoun Handling in Large Language Models](https://arxiv.org/abs/2508.00788)
*Xushuo Tang,Yi Ding,Zhengyi Yang,Yin Chen,Yongrui Gu,Wenke Yang,Mingchen Ju,Xin Cao,Yongfei Liu,Wenjie Zhang*

Main category: cs.CL

TL;DR: 本文引入MISGENDERED+基准评估大语言模型代词准确性，对五个模型进行多场景测试，发现相比之前有改善但新代词和反向推理任务仍有不足。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在敏感场景需注重公平和包容性，先前基准存在局限性，需新基准评估模型代词准确性。

Method: 引入MISGENDERED+基准，对GPT - 4o、Claude 4等五个模型在零样本、少样本和性别认同推理场景下进行基准测试。

Result: 与先前研究相比有显著改善，尤其是二元和性别中立代词准确性，但新代词和反向推理任务准确性不稳定。

Conclusion: 指出当前模型在身份敏感推理方面仍存在差距，探讨了影响、模型特性及未来研究方向。

Abstract: Large language models (LLMs) are increasingly deployed in sensitive contexts
where fairness and inclusivity are critical. Pronoun usage, especially
concerning gender-neutral and neopronouns, remains a key challenge for
responsible AI. Prior work, such as the MISGENDERED benchmark, revealed
significant limitations in earlier LLMs' handling of inclusive pronouns, but
was constrained to outdated models and limited evaluations. In this study, we
introduce MISGENDERED+, an extended and updated benchmark for evaluating LLMs'
pronoun fidelity. We benchmark five representative LLMs, GPT-4o, Claude 4,
DeepSeek-V3, Qwen Turbo, and Qwen2.5, across zero-shot, few-shot, and gender
identity inference. Our results show notable improvements compared with
previous studies, especially in binary and gender-neutral pronoun accuracy.
However, accuracy on neopronouns and reverse inference tasks remains
inconsistent, underscoring persistent gaps in identity-sensitive reasoning. We
discuss implications, model-specific observations, and avenues for future
inclusive AI research.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [247] [Anomaly detection with spiking neural networks for LHC physics](https://arxiv.org/abs/2508.00063)
*Barry M. Dillon,Jim Harkin,Aqib Javed*

Main category: hep-ph

TL;DR: 本文研究用于大型强子对撞机（LHC）异常检测的基于脉冲神经网络（SNN）的自编码器，设计并评估了简单架构，结果显示其在LHC异常检测中与传统自编码器有竞争力。


<details>
  <summary>Details</summary>
Motivation: 异常检测是LHC发现新物理的有前景策略，现有系统需在严格延迟和计算约束下工作，SNN适合低延迟、低内存实时推理。

Method: 使用CMS ADC2021数据集，设计并评估简单SNN自编码器架构。

Result: SNN自编码器在所有信号模型的LHC异常检测中与传统自编码器有竞争力。

Conclusion: SNN自编码器可用于LHC异常检测，在相关场景有应用潜力。

Abstract: Anomaly detection offers a promising strategy for discovering new physics at
the Large Hadron Collider (LHC). This paper investigates AutoEncoders built
using neuromorphic Spiking Neural Networks (SNNs) for this purpose. One key
application is at the trigger level, where anomaly detection tools could
capture signals that would otherwise be discarded by conventional selection
cuts. These systems must operate under strict latency and computational
constraints. SNNs are inherently well-suited for low-latency, low-memory,
real-time inference, particularly on Field-Programmable Gate Arrays (FPGAs).
Further gains are expected with the rapid progress in dedicated neuromorphic
hardware development. Using the CMS ADC2021 dataset, we design and evaluate a
simple SNN AutoEncoder architecture. Our results show that the SNN AutoEncoders
are competitive with conventional AutoEncoders for LHC anomaly detection across
all signal models.

</details>


### [248] [Jet Image Generation in High Energy Physics Using Diffusion Models](https://arxiv.org/abs/2508.00250)
*Victor D. Martinez,Vidya Manian,Sudhir Malik*

Main category: hep-ph

TL;DR: 首次应用扩散模型生成LHC质子 - 质子碰撞事件的喷注图像，对比不同模型，一致性模型表现更优。


<details>
  <summary>Details</summary>
Motivation: 为高能物理研究提供有效工具，解决喷注图像生成问题。

Method: 将喷注运动学变量映射为二维图像，用扩散模型学习喷注成分空间分布，在图像空间操作，对比分数扩散模型和一致性模型。

Result: 用FID等指标评估，一致性模型生成图像保真度和稳定性高于分数扩散模型。

Conclusion: 该研究提升计算效率和生成准确性，为高能物理研究提供有价值工具。

Abstract: This article presents, for the first time, the application of diffusion
models for generating jet images corresponding to proton-proton collision
events at the Large Hadron Collider (LHC). The kinematic variables of quark,
gluon, W-boson, Z-boson, and top quark jets from the JetNet simulation dataset
are mapped to two-dimensional image representations. Diffusion models are
trained on these images to learn the spatial distribution of jet constituents.
We compare the performance of score-based diffusion models and consistency
models in accurately generating class-conditional jet images. Unlike approaches
based on latent distributions, our method operates directly in image space. The
fidelity of the generated images is evaluated using several metrics, including
the Fr\'echet Inception Distance (FID), which demonstrates that consistency
models achieve higher fidelity and generation stability compared to score-based
diffusion models. These advancements offer significant improvements in
computational efficiency and generation accuracy, providing valuable tools for
High Energy Physics (HEP) research.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [249] [Information-Theoretic Decentralized Secure Aggregation with Collusion Resilience](https://arxiv.org/abs/2508.00596)
*Xiang Zhang,Zhou Li,Shuangyang Li,Kai Wan,Derrick Wing Kwan Ng,Giuseppe Caire*

Main category: cs.IT

TL;DR: 本文从信息论角度研究去中心化安全聚合问题，刻画了最优速率区域，明确了通信和密钥使用的基本性能限制。


<details>
  <summary>Details</summary>
Motivation: 现有关于安全聚合的工作主要关注协议设计和计算保证，对系统的信息论基本限制理解有限，且去中心化环境下通信和密钥使用的最优界限未知。

Method: 考虑K个全连接用户的网络，每个用户持有私有输入，目标是安全计算所有输入的总和，安全约束为用户在与至多T个其他用户勾结时也无法得知输入总和之外的信息。

Result: 刻画了最优速率区域，表明安全计算一个所需输入和的符号时，每个用户至少传输一个符号、持有一个符号的密钥，所有用户集体持有不少于K - 1个独立密钥符号。

Conclusion: 研究结果确立了去中心化安全聚合的基本性能限制，为分布式学习系统中可证明安全且通信高效的协议设计提供了见解。

Abstract: In decentralized federated learning (FL), multiple clients collaboratively
learn a shared machine learning (ML) model by leveraging their privately held
datasets distributed across the network, through interactive exchange of the
intermediate model updates. To ensure data security, cryptographic techniques
are commonly employed to protect model updates during aggregation. Despite
growing interest in secure aggregation, existing works predominantly focus on
protocol design and computational guarantees, with limited understanding of the
fundamental information-theoretic limits of such systems. Moreover, optimal
bounds on communication and key usage remain unknown in decentralized settings,
where no central aggregator is available. Motivated by these gaps, we study the
problem of decentralized secure aggregation (DSA) from an information-theoretic
perspective. Specifically, we consider a network of $K$ fully-connected users,
each holding a private input -- an abstraction of local training data -- who
aim to securely compute the sum of all inputs. The security constraint requires
that no user learns anything beyond the input sum, even when colluding with up
to $T$ other users. We characterize the optimal rate region, which specifies
the minimum achievable communication and secret key rates for DSA. In
particular, we show that to securely compute one symbol of the desired input
sum, each user must (i) transmit at least one symbol to others, (ii) hold at
least one symbol of secret key, and (iii) all users must collectively hold no
fewer than $K - 1$ independent key symbols. Our results establish the
fundamental performance limits of DSA, providing insights for the design of
provably secure and communication-efficient protocols in distributed learning
systems.

</details>


### [250] [Towards a Measure Theory of Semantic Information](https://arxiv.org/abs/2508.00525)
*George M. Coghill*

Main category: cs.IT

TL;DR: 批判Floridi语义信息理论，提出基于单位圆的新方法消除悖论，指出矛盾和重言式无信息，矛盾消息等信息并举例说明。


<details>
  <summary>Details</summary>
Motivation: 解决Bar - Hillel - Carnap悖论，指出Floridi理论未达成目标，需新方法。

Method: 批判分析Floridi理论，基于单位圆类比冯·诺依曼量子概率构建信息度量空间。

Result: 新方法满足Floridi规定要求，消除悖论，发现矛盾和重言式无信息，矛盾消息等信息。

Conclusion: 新的基于单位圆的方法可有效解决语义信息量化中的悖论问题。

Abstract: A classic account of the quantification of semantic information is that of
Bar-Hiller and Carnap. Their account proposes an inverse relation between the
informativeness of a statement and its probability. However, their approach
assigns the maximum informativeness to a contradiction: which Floridi refers to
as the Bar-Hillel-Carnap paradox. He developed a novel theory founded on a
distance metric and parabolic relation, designed to remove this paradox.
Unfortunately is approach does not succeed in that aim.
  In this paper I critique Floridi's theory of strongly semantic information on
its own terms and show where it succeeds and fails. I then present a new
approach based on the unit circle (a relation that has been the basis of
theories from basic trigonometry to quantum theory). This is used, by analogy
with von Neumann's quantum probability to construct a measure space for
informativeness that meets all the requirements stipulated by Floridi and
removes the paradox. In addition, while contradictions and tautologies have
zero informativeness, it is found that messages which are contradictory to each
other are equally informative. The utility of this is explained by means of an
example.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [251] [AdapDISCOM: An Adaptive Sparse Regression Method for High-Dimensional Multimodal Data With Block-Wise Missingness and Measurement Errors](https://arxiv.org/abs/2508.00120)
*Abdoul O. Diakité,Claudia Moreau,Gleb Bezgin,Nikhil Bhagwat,Pedro Rosa-Neto,Jean-Baptiste Poline,Simon Girard,Amadou Barry,for the Alzheimers Disease Neuroimaging Initiative*

Main category: stat.ME

TL;DR: 提出AdapDISCOM方法处理多模态高维数据的块缺失和测量误差问题，理论分析并实验验证其优势，应用于ADNI数据有良好表现。


<details>
  <summary>Details</summary>
Motivation: 多模态高维数据存在块缺失和测量误差，给统计推断和预测带来挑战。

Method: 在DISCOM框架基础上，引入特定模态加权方案，建立理论性质，开发鲁棒高效变体。

Result: 模拟实验中AdapDISCOM优于现有方法，应用于ADNI数据能改善认知分数预测和可靠选择生物标志物。

Conclusion: AdapDISCOM为有数据缺陷的高维多模态数据分析提供灵活、鲁棒和可扩展的框架。

Abstract: Multimodal high-dimensional data are increasingly prevalent in biomedical
research, yet they are often compromised by block-wise missingness and
measurement errors, posing significant challenges for statistical inference and
prediction. We propose AdapDISCOM, a novel adaptive direct sparse regression
method that simultaneously addresses these two pervasive issues. Building on
the DISCOM framework, AdapDISCOM introduces modality-specific weighting schemes
to account for heterogeneity in data structures and error magnitudes across
modalities. We establish the theoretical properties of AdapDISCOM, including
model selection consistency and convergence rates under sub-Gaussian and
heavy-tailed settings, and develop robust and computationally efficient
variants (AdapDISCOM-Huber and Fast-AdapDISCOM). Extensive simulations
demonstrate that AdapDISCOM consistently outperforms existing methods such as
DISCOM, SCOM, and CoCoLasso, particularly under heterogeneous contamination and
heavy-tailed distributions. Finally, we apply AdapDISCOM to Alzheimers Disease
Neuroimaging Initiative (ADNI) data, demonstrating improved prediction of
cognitive scores and reliable selection of established biomarkers, even with
substantial missingness and measurement errors. AdapDISCOM provides a flexible,
robust, and scalable framework for high-dimensional multimodal data analysis
under realistic data imperfections.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [252] [MMRAG-DocQA: A Multi-Modal Retrieval-Augmented Generation Method for Document Question-Answering with Hierarchical Index and Multi-Granularity Retrieval](https://arxiv.org/abs/2508.00579)
*Ziyu Gong,Yihua Huang,Chengcheng Mai*

Main category: cs.MM

TL;DR: 提出多模态RAG模型MMRAG - DocQA用于长上下文文档问答，实验证明其在多模态多页文档问答上的优越性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态长上下文文档问答的LVLM和RAG方法分别存在易产生幻觉、模态断开和跨页碎片化问题，需改进。

Method: 提出MMRAG - DocQA模型，设计分层索引方法建立页面内和跨页依赖，提出多粒度语义检索方法促进证据连接和推理。

Result: 在公共数据集MMLongBench - Doc和LongDocURL上实验，证明MMRAG - DocQA方法的优越性。

Conclusion: MMRAG - DocQA方法在理解和回答多模态多页文档方面表现出色。

Abstract: The multi-modal long-context document question-answering task aims to locate
and integrate multi-modal evidences (such as texts, tables, charts, images, and
layouts) distributed across multiple pages, for question understanding and
answer generation. The existing methods can be categorized into Large
Vision-Language Model (LVLM)-based and Retrieval-Augmented Generation
(RAG)-based methods. However, the former were susceptible to hallucinations,
while the latter struggled for inter-modal disconnection and cross-page
fragmentation. To address these challenges, a novel multi-modal RAG model,
named MMRAG-DocQA, was proposed, leveraging both textual and visual information
across long-range pages to facilitate accurate question answering. A
hierarchical indexing method with the integration of flattened in-page chunks
and topological cross-page chunks was designed to jointly establish in-page
multi-modal associations and long-distance cross-page dependencies. By means of
joint similarity evaluation and large language model (LLM)-based re-ranking, a
multi-granularity semantic retrieval method, including the page-level parent
page retrieval and document-level summary retrieval, was proposed to foster
multi-modal evidence connection and long-distance evidence integration and
reasoning. Experimental results performed on public datasets, MMLongBench-Doc
and LongDocURL, demonstrated the superiority of our MMRAG-DocQA method in
understanding and answering modality-rich and multi-page documents.

</details>
