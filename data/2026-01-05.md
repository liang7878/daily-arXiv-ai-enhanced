<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 24]
- [cs.CE](#cs.CE) [Total: 6]
- [cs.DB](#cs.DB) [Total: 6]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.DS](#cs.DS) [Total: 4]
- [cs.GT](#cs.GT) [Total: 3]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.LG](#cs.LG) [Total: 64]
- [cs.NE](#cs.NE) [Total: 8]
- [cs.SE](#cs.SE) [Total: 10]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [q-fin.RM](#q-fin.RM) [Total: 2]
- [q-fin.ST](#q-fin.ST) [Total: 2]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [stat.ML](#stat.ML) [Total: 3]
- [stat.CO](#stat.CO) [Total: 2]
- [physics.optics](#physics.optics) [Total: 2]
- [econ.GN](#econ.GN) [Total: 3]
- [eess.IV](#eess.IV) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [eess.SP](#eess.SP) [Total: 3]
- [cs.CV](#cs.CV) [Total: 13]
- [math.ST](#math.ST) [Total: 1]
- [stat.ME](#stat.ME) [Total: 2]
- [cs.CL](#cs.CL) [Total: 17]
- [cs.CR](#cs.CR) [Total: 11]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 2]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.RO](#cs.RO) [Total: 3]
- [cs.IT](#cs.IT) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [eess.SY](#eess.SY) [Total: 3]
- [math.NA](#math.NA) [Total: 1]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [math.DS](#math.DS) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Reasoning in Action: MCTS-Driven Knowledge Retrieval for Large Language Models](https://arxiv.org/abs/2601.00003)
*Shuqi Liu,Bowei He,Chen Ma,Linqi Song*

Main category: cs.AI

TL;DR: 提出推理感知知识检索方法，采用粗到细策略，实验表明该方法使检索更贴合推理，提升知识多样性。


<details>
  <summary>Details</summary>
Motivation: 解决有效整合检索和推理策略以优化大语言模型性能的难题。

Method: 采用粗到细的知识检索方法，先确定知识库相关子区域，再在子区域中精确提取与推理相关知识，用受蒙特卡罗树搜索启发的搜索方法。

Result: 在两个多轮对话数据集实验显示，该方法更贴合人类对话推理，提升检索知识多样性。

Conclusion: 所提出的推理感知知识检索方法有效，能让大语言模型给出更有信息和创造力的回复。

Abstract: Large language models (LLMs) typically enhance their performance through either the retrieval of semantically similar information or the improvement of their reasoning capabilities. However, a significant challenge remains in effectively integrating both retrieval and reasoning strategies to optimize LLM performance. In this paper, we introduce a reasoning-aware knowledge retrieval method that enriches LLMs with information aligned to the logical structure of conversations, moving beyond surface-level semantic similarity. We follow a coarse-to-fine approach for knowledge retrieval. First, we identify a contextually relevant sub-region of the knowledge base, ensuring that all sentences within it are relevant to the context topic. Next, we refine our search within this sub-region to extract knowledge that is specifically relevant to the reasoning process. Throughout both phases, we employ the Monte Carlo Tree Search-inspired search method to effectively navigate through knowledge sentences using common keywords. Experiments on two multi-turn dialogue datasets demonstrate that our knowledge retrieval approach not only aligns more closely with the underlying reasoning in human conversations but also significantly enhances the diversity of the retrieved knowledge, resulting in more informative and creative responses.

</details>


### [2] [Finetuning Large Language Models for Automated Depression Screening in Nigerian Pidgin English: GENSCORE Pilot Study](https://arxiv.org/abs/2601.00004)
*Isaac Iyinoluwa Olufadewa,Miracle Ayomikun Adesina,Ezekiel Ayodeji Oladejo,Uthman Babatunde Usman,Owen Kolade Adeniyi,Matthew Tolulope Olawoyin*

Main category: cs.AI

TL;DR: 研究提出用微调大语言模型进行尼日利亚语自动化抑郁筛查，收集数据微调三个模型，GPT - 4.1表现最佳，为部署对话式心理健康工具奠定基础。


<details>
  <summary>Details</summary>
Motivation: 尼日利亚抑郁症负担重，但因缺乏临床医生、污名化和语言障碍，筛查覆盖有限，传统工具不适用。

Method: 收集432份尼日利亚年轻人的皮钦语音频响应，转录、预处理和标注，用三个大语言模型微调，定量和定性评估性能。

Result: GPT - 4.1定量性能最高，PHQ - 9严重程度评分预测准确率达94.5%，定性上回应最符合文化、清晰且相关。

Conclusion: 为在语言多样、资源有限环境中部署对话式心理健康工具提供基础。

Abstract: Depression is a major contributor to the mental-health burden in Nigeria, yet screening coverage remains limited due to low access to clinicians, stigma, and language barriers. Traditional tools like the Patient Health Questionnaire-9 (PHQ-9) were validated in high-income countries but may be linguistically or culturally inaccessible for low- and middle-income countries and communities such as Nigeria where people communicate in Nigerian Pidgin and more than 520 local languages. This study presents a novel approach to automated depression screening using fine-tuned large language models (LLMs) adapted for conversational Nigerian Pidgin. We collected a dataset of 432 Pidgin-language audio responses from Nigerian young adults aged 18-40 to prompts assessing psychological experiences aligned with PHQ-9 items, performed transcription, rigorous preprocessing and annotation, including semantic labeling, slang and idiom interpretation, and PHQ-9 severity scoring. Three LLMs - Phi-3-mini-4k-instruct, Gemma-3-4B-it, and GPT-4.1 - were fine-tuned on this annotated dataset, and their performance was evaluated quantitatively (accuracy, precision and semantic alignment) and qualitatively (clarity, relevance, and cultural appropriateness). GPT-4.1 achieved the highest quantitative performance, with 94.5% accuracy in PHQ-9 severity scoring prediction, outperforming Gemma-3-4B-it and Phi-3-mini-4k-instruct. Qualitatively, GPT-4.1 also produced the most culturally appropriate, clear, and contextually relevant responses. AI-mediated depression screening for underserved Nigerian communities. This work provides a foundation for deploying conversational mental-health tools in linguistically diverse, resource-constrained environments.

</details>


### [3] [Toward a Physical Theory of Intelligence](https://arxiv.org/abs/2601.00021)
*Peter David Fagan*

Main category: cs.AI

TL;DR: 提出基于不可逆信息处理的智能物理理论，给出相关框架和定义，分析生物系统，发展电路理论，提出人工智能安全观点，统一解释智能现象。


<details>
  <summary>Details</summary>
Motivation: 建立基于守恒定律下不可逆信息处理的智能物理理论，统一解释智能现象。

Method: 引入守恒一致编码（CCE）框架，定义智能，推导物理约束，分析生物系统，发展连续动态电路理论。

Result: 揭示长视野效率需保存内部信息结构，得出物理智能系统有认知极限，分析生物系统优化信息处理，发展电路理论，提出人工智能安全观点。

Conclusion: 该理论提供了统一、与基质无关的智能物理现象解释。

Abstract: We present a physical theory of intelligence grounded in irreversible information processing in systems constrained by conservation laws. An intelligent system is modelled as a coupled agent-environment process whose evolution transforms information into goal-directed work. To connect information to physical state, we introduce the Conservation-Congruent Encoding (CCE) framework, in which encodings correspond to metastable basins of attraction whose separability is enforced by conservation laws. Within this framework, intelligence is defined as the amount of goal-directed work produced per nat of irreversibly processed information. From this definition we derive a hierarchy of physical constraints governing information intake, irreversible computation, and work extraction in open systems. The framework reveals how long-horizon efficiency requires the preservation of internal informational structure, giving rise to self-modelling, and it establishes that physically embodied intelligent systems possess intrinsic epistemic limits analogous to incompleteness phenomena. Applying the theory to biological systems, we analyse how oscillatory and near-critical dynamics optimise the trade-off between information preservation, dissipation, and useful work, placing the brain near an efficient operating regime predicted by the framework. At the architectural level, we develop a theory of continuous dynamical circuits in which classical Boolean logic emerges as a special case of attractor selection, while more general invariant geometries support computational modes beyond fixed-point logic. Finally, we propose a physically grounded perspective on artificial intelligence safety based on irreversible information flow and structural homeostasis. Together, these results provide a unified, substrate-neutral account of intelligence as a physical phenomenon.

</details>


### [4] [A multi-algorithm approach for operational human resources workload balancing in a last mile urban delivery system](https://arxiv.org/abs/2601.00023)
*Luis M. Moreno-Saavedra,Silvia Jimenez-Fernandez,Antonio Portilla-Figueras,David Casillas-Perez,Sancho Salcedo-Sanz*

Main category: cs.AI

TL;DR: 本文探讨最后一英里城市包裹配送系统中人力资源工作量平衡问题，提出多算法方法优化包裹分配，以实现员工工作量平衡，并在西班牙一城市案例中验证性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于地理距离的包裹分配方法效率低且会导致配送员工工作量分配不均衡，需要解决人力资源工作量平衡问题。

Method: 提出多算法方法，输入配送点和员工数量，结合距离和工作量因素，包括不同版本k - means、进化方法、基于k - means初始化的递归分配及混合进化集成算法。

Result: 在西班牙阿苏克卡德埃纳雷斯市的实际最后一英里包裹配送问题中验证了所提方法的性能。

Conclusion: 所提多算法方法能有效解决最后一英里城市包裹配送系统中员工工作量平衡问题。

Abstract: Efficient workload assignment to the workforce is critical in last-mile package delivery systems. In this context, traditional methods of assigning package deliveries to workers based on geographical proximity can be inefficient and surely guide to an unbalanced workload distribution among delivery workers. In this paper, we look at the problem of operational human resources workload balancing in last-mile urban package delivery systems. The idea is to consider the effort workload to optimize the system, i.e., the optimization process is now focused on improving the delivery time, so that the workload balancing is complete among all the staff. This process should correct significant decompensations in workload among delivery workers in a given zone. Specifically, we propose a multi-algorithm approach to tackle this problem. The proposed approach takes as input a set of delivery points and a defined number of workers, and then assigns packages to workers, in such a way that it ensures that each worker completes a similar amount of work per day. The proposed algorithms use a combination of distance and workload considerations to optimize the allocation of packages to workers. In this sense, the distance between the delivery points and the location of each worker is also taken into account. The proposed multi-algorithm methodology includes different versions of k-means, evolutionary approaches, recursive assignments based on k-means initialization with different problem encodings, and a hybrid evolutionary ensemble algorithm. We have illustrated the performance of the proposed approach in a real-world problem in an urban last-mile package delivery workforce operating at Azuqueca de Henares, Spain.

</details>


### [5] [Quantitative Rule-Based Strategy modeling in Classic Indian Rummy: A Metric Optimization Approach](https://arxiv.org/abs/2601.00024)
*Purushottam Saha,Avirup Chakraborty,Sourish Sarkar,Subhamoy Maitra,Diganta Mukherjee,Tridib Mukherjee*

Main category: cs.AI

TL;DR: 本文提出基于MinDist指标的规则框架用于印度拉米牌13张变体的策略玩法，算法经测试胜率显著提升。


<details>
  <summary>Details</summary>
Motivation: 印度拉米牌13张变体需概率推理和组合决策，缺乏有效策略框架。

Method: 提出新的手牌评估指标MinDist修改MinScore指标，设计基于MinScore算法的高效计算方法，在二人零和模拟框架中加入对手手牌建模，并使用统计假设检验评估策略。

Result: 基于MinDist的智能体胜率相比传统启发式方法显著提高。

Conclusion: 为拉米牌算法策略设计提供了正式且可解释的一步。

Abstract: The 13-card variant of Classic Indian Rummy is a sequential game of incomplete information that requires probabilistic reasoning and combinatorial decision-making. This paper proposes a rule-based framework for strategic play, driven by a new hand-evaluation metric termed MinDist. The metric modifies the MinScore metric by quantifying the edit distance between a hand and the nearest valid configuration, thereby capturing structural proximity to completion. We design a computationally efficient algorithm derived from the MinScore algorithm, leveraging dynamic pruning and pattern caching to exactly calculate this metric during play. Opponent hand-modeling is also incorporated within a two-player zero-sum simulation framework, and the resulting strategies are evaluated using statistical hypothesis testing. Empirical results show significant improvement in win rates for MinDist-based agents over traditional heuristics, providing a formal and interpretable step toward algorithmic Rummy strategy design.

</details>


### [6] [From Clay to Code: Typological and Material Reasoning in AI Interpretations of Iranian Pigeon Towers](https://arxiv.org/abs/2601.00029)
*Abolhassan Pishahang,Maryam Badiei*

Main category: cs.AI

TL;DR: 研究生成式AI系统对乡土建筑形式中建筑智慧的解读，以伊朗鸽塔为例测试三种扩散模型，评估其重建能力，结果显示AI能重现几何图案但误读材料与气候逻辑等，界定了视觉相似与建筑推理的边界。


<details>
  <summary>Details</summary>
Motivation: 探究生成式AI系统如何解读乡土形式中嵌入的建筑智慧。

Method: 以伊朗鸽塔为案例，测试Midjourney v6、DALL - E 3和基于Stable Diffusion XL的DreamStudio三种扩散模型，分参考、适应、推测三个提示阶段，并采用五标准评估框架。

Result: AI能可靠重现几何图案，但误读材料和气候推理；参考图像提高真实感但限制创造力，无参考则产生有创意但文化模糊的结果。

Conclusion: 界定了视觉相似与建筑推理的边界，将计算乡土推理定位为分析AI如何感知、扭曲和重新想象传统设计智慧的框架。

Abstract: This study investigates how generative AI systems interpret the architectural intelligence embedded in vernacular form. Using the Iranian pigeon tower as a case study, the research tests three diffusion models, Midjourney v6, DALL-E 3, and DreamStudio based on Stable Diffusion XL (SDXL), across three prompt stages: referential, adaptive, and speculative. A five-criteria evaluation framework assesses how each system reconstructs typology, materiality, environment, realism, and cultural specificity. Results show that AI reliably reproduces geometric patterns but misreads material and climatic reasoning. Reference imagery improves realism yet limits creativity, while freedom from reference generates inventive but culturally ambiguous outcomes. The findings define a boundary between visual resemblance and architectural reasoning, positioning computational vernacular reasoning as a framework for analyzing how AI perceives, distorts, and reimagines traditional design intelligence.

</details>


### [7] [The Agentic Leash: Extracting Causal Feedback Fuzzy Cognitive Maps with LLMs](https://arxiv.org/abs/2601.00097)
*Akash Kumar Panda,Olaoluwa Adigun,Bart Kosko*

Main category: cs.AI

TL;DR: 设计LLM代理从原始文本提取因果反馈模糊认知图（FCM），介绍提取步骤并测试，混合FCM有新均衡。


<details>
  <summary>Details</summary>
Motivation: 实现从原始文本中提取因果反馈FCM，赋予FCM动力系统一定自主性。

Method: 通过三个精细调整的系统指令引导LLM代理提取关键名词、FCM概念节点及因果边，用Kissinger等人的文章测试，最后混合不同LLM代理生成的FCM。

Result: 三步过程生成的FCM动力系统与人类生成的FCM收敛到相同均衡极限环，混合FCM吸收主导成分均衡并创造新均衡。

Conclusion: 所设计的LLM代理提取FCM方法有效，混合FCM能更好近似潜在因果动力系统。

Abstract: We design a large-language-model (LLM) agent that extracts causal feedback fuzzy cognitive maps (FCMs) from raw text. The causal learning or extraction process is agentic both because of the LLM's semi-autonomy and because ultimately the FCM dynamical system's equilibria drive the LLM agents to fetch and process causal text. The fetched text can in principle modify the adaptive FCM causal structure and so modify the source of its quasi-autonomy--its equilibrium limit cycles and fixed-point attractors. This bidirectional process endows the evolving FCM dynamical system with a degree of autonomy while still staying on its agentic leash. We show in particular that a sequence of three finely tuned system instructions guide an LLM agent as it systematically extracts key nouns and noun phrases from text, as it extracts FCM concept nodes from among those nouns and noun phrases, and then as it extracts or infers partial or fuzzy causal edges between those FCM nodes. We test this FCM generation on a recent essay about the promise of AI from the late diplomat and political theorist Henry Kissinger and his colleagues. This three-step process produced FCM dynamical systems that converged to the same equilibrium limit cycles as did the human-generated FCMs even though the human-generated FCM differed in the number of nodes and edges. A final FCM mixed generated FCMs from separate Gemini and ChatGPT LLM agents. The mixed FCM absorbed the equilibria of its dominant mixture component but also created new equilibria of its own to better approximate the underlying causal dynamical system.

</details>


### [8] [Mortar: Evolving Mechanics for Automatic Game Design](https://arxiv.org/abs/2601.00105)
*Muhammad U. Nasir,Yuchen Li,Steven James,Julian Togelius*

Main category: cs.AI

TL;DR: 提出Mortar系统用于自动游戏设计中的游戏机制自主进化，结合算法和大语言模型，经评估能生成多样可玩游戏及贡献更高技能排序分数的机制，并进行了消融和用户研究。


<details>
  <summary>Details</summary>
Motivation: 手动设计游戏机制耗时且依赖专家，需要实现自动设计。

Method: 将质量多样性算法与大语言模型结合探索多样机制，通过树搜索过程合成完整游戏进行评估，依据对游戏技能排序分数的贡献评估机制。

Result: Mortar生成了多样且可玩的游戏，以及对游戏技能排序分数贡献更高的机制。

Conclusion: Mortar在自动游戏设计中有效，各组件有其作用，生成的游戏受人类反馈认可。

Abstract: We present Mortar, a system for autonomously evolving game mechanics for automatic game design. Game mechanics define the rules and interactions that govern gameplay, and designing them manually is a time-consuming and expert-driven process. Mortar combines a quality-diversity algorithm with a large language model to explore a diverse set of mechanics, which are evaluated by synthesising complete games that incorporate both evolved mechanics and those drawn from an archive. The mechanics are evaluated by composing complete games through a tree search procedure, where the resulting games are evaluated by their ability to preserve a skill-based ordering over players -- that is, whether stronger players consistently outperform weaker ones. We assess the mechanics based on their contribution towards the skill-based ordering score in the game. We demonstrate that Mortar produces games that appear diverse and playable, and mechanics that contribute more towards the skill-based ordering score in the game. We perform ablation studies to assess the role of each system component and a user study to evaluate the games based on human feedback.

</details>


### [9] [Ask, Clarify, Optimize: Human-LLM Agent Collaboration for Smarter Inventory Control](https://arxiv.org/abs/2601.00121)
*Yaqi Duan,Yichun Hu,Jiashuo Jiang*

Main category: cs.AI

TL;DR: 本文探讨大语言模型（LLMs）能否助力中小企业库存管理，指出直接用LLMs存在问题，提出混合代理框架，经测试该框架降低成本，表明LLMs可作自然语言接口。


<details>
  <summary>Details</summary>
Motivation: 许多中小企业缺乏专业知识部署高级优化方法进行库存管理，研究LLMs能否弥补这一差距。

Method: 提出严格分离语义推理和数学计算的混合代理框架，引入“人类模仿器”进行压力测试。

Result: 混合代理框架相对以GPT - 4o作为端到端求解器的交互式基线降低32.1%的总库存成本，且仅提供完美真实信息不足以提升GPT - 4o性能。

Conclusion: LLMs不能替代运筹学，可作为自然语言接口让非专家使用基于求解器的严格策略。

Abstract: Inventory management remains a challenge for many small and medium-sized businesses that lack the expertise to deploy advanced optimization methods. This paper investigates whether Large Language Models (LLMs) can help bridge this gap. We show that employing LLMs as direct, end-to-end solvers incurs a significant "hallucination tax": a performance gap arising from the model's inability to perform grounded stochastic reasoning. To address this, we propose a hybrid agentic framework that strictly decouples semantic reasoning from mathematical calculation. In this architecture, the LLM functions as an intelligent interface, eliciting parameters from natural language and interpreting results while automatically calling rigorous algorithms to build the optimization engine.
  To evaluate this interactive system against the ambiguity and inconsistency of real-world managerial dialogue, we introduce the Human Imitator, a fine-tuned "digital twin" of a boundedly rational manager that enables scalable, reproducible stress-testing. Our empirical analysis reveals that the hybrid agentic framework reduces total inventory costs by 32.1% relative to an interactive baseline using GPT-4o as an end-to-end solver. Moreover, we find that providing perfect ground-truth information alone is insufficient to improve GPT-4o's performance, confirming that the bottleneck is fundamentally computational rather than informational. Our results position LLMs not as replacements for operations research, but as natural-language interfaces that make rigorous, solver-based policies accessible to non-experts.

</details>


### [10] [Constructing a Neuro-Symbolic Mathematician from First Principles](https://arxiv.org/abs/2601.00125)
*Keqin Xie*

Main category: cs.AI

TL;DR: 针对大语言模型复杂推理的逻辑失败问题，提出Mathesis神经符号架构解决推理问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）因缺乏内部公理框架，在复杂推理中存在持续逻辑失败。

Method: 提出Mathesis神经符号架构，将数学状态编码为高阶超图，使用符号推理内核（SRK），定义全局能量函数，通过蒙特卡洛树搜索和进化证明搜索实现多步演绎。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: Large Language Models (LLMs) exhibit persistent logical failures in complex reasoning due to the lack of an internal axiomatic framework. We propose Mathesis, a neuro-symbolic architecture that encodes mathematical states as higher-order hypergraphs and uses a Symbolic Reasoning Kernel (SRK)--a differentiable logic engine that maps constraints to a continuous energy landscape. By defining a global energy function E(G), where zero energy implies logical consistency, the SRK yields gradient-based signals to train a Hypergraph Transformer Brain, turning proof search into energy minimization. Multi-step deduction is enabled via Monte Carlo Tree Search and Evolutionary Proof Search, guided by learned value functions and semantic unification.

</details>


### [11] [Bio-inspired Agentic Self-healing Framework for Resilient Distributed Computing Continuum Systems](https://arxiv.org/abs/2601.00339)
*Alaa Saleh,Praveen Kumar Donta,Roberto Morabito,Sasu Tarkoma,Anders Lindgren,Qiyang Zhang,Schahram Dustdar,Susanna Pirttikangas,Lauri Lovén*

Main category: cs.AI

TL;DR: 受人类生物系统自愈能力启发，提出ReCiSt框架用于分布式计算连续体系统（DCCS）实现弹性。该框架将生物阶段重构为计算层，通过语言模型代理实现自主故障处理，评估显示具有自愈能力。


<details>
  <summary>Details</summary>
Motivation: 现代DCCS具有复杂性、移动性和动态操作条件，易出现频繁故障，需要可扩展、自适应和自我调节的弹性策略。

Method: 将生物的止血、炎症、增殖和重塑阶段重构为DCCS的遏制、诊断、元认知和知识计算层，通过语言模型代理执行自主故障隔离、因果诊断、自适应恢复和长期知识整合。

Result: 在公共故障数据集上使用多个语言模型评估，ReCiSt在不同语言模型下几十秒内实现自愈，代理CPU使用率最低10%，还展示了克服不确定性的分析深度和实现弹性调用的微代理数量。

Conclusion: ReCiSt框架能有效实现DCCS的弹性和自我修复。

Abstract: Human biological systems sustain life through extraordinary resilience, continually detecting damage, orchestrating targeted responses, and restoring function through self-healing. Inspired by these capabilities, this paper introduces ReCiSt, a bio-inspired agentic self-healing framework designed to achieve resilience in Distributed Computing Continuum Systems (DCCS). Modern DCCS integrate heterogeneous computing resources, ranging from resource-constrained IoT devices to high-performance cloud infrastructures, and their inherent complexity, mobility, and dynamic operating conditions expose them to frequent faults that disrupt service continuity. These challenges underscore the need for scalable, adaptive, and self-regulated resilience strategies. ReCiSt reconstructs the biological phases of Hemostasis, Inflammation, Proliferation, and Remodeling into the computational layers Containment, Diagnosis, Meta-Cognitive, and Knowledge for DCCS. These four layers perform autonomous fault isolation, causal diagnosis, adaptive recovery, and long-term knowledge consolidation through Language Model (LM)-powered agents. These agents interpret heterogeneous logs, infer root causes, refine reasoning pathways, and reconfigure resources with minimal human intervention. The proposed ReCiSt framework is evaluated on public fault datasets using multiple LMs, and no baseline comparison is included due to the scarcity of similar approaches. Nevertheless, our results, evaluated under different LMs, confirm ReCiSt's self-healing capabilities within tens of seconds with minimum of 10% of agent CPU usage. Our results also demonstrated depth of analysis to over come uncertainties and amount of micro-agents invoked to achieve resilience.

</details>


### [12] [Explicit Abstention Knobs for Predictable Reliability in Video Question Answering](https://arxiv.org/abs/2601.00138)
*Jorge Ortiz*

Main category: cs.AI

TL;DR: 研究基于置信度的弃权能否在视频问答中可靠控制错误率及分布偏移下的鲁棒性。基于NExT - QA和Gemini 2.0 Flash得出置信度阈值在分布内提供机制控制的结果。


<details>
  <summary>Details</summary>
Motivation: 视觉 - 语言模型的高风险部署需要选择性预测，需研究基于置信度的弃权能否在视频问答中可靠控制错误率，以及这种控制在分布偏移下是否保持鲁棒。

Method: 使用NExT - QA和Gemini 2.0 Flash开展研究。

Result: 置信度阈值在分布内提供机制控制，扫描阈值epsilon可产生平滑的风险 - 覆盖率权衡，降低错误率。

Conclusion: 未明确提及，但暗示基于置信度的弃权在分布内对控制错误率有一定作用。

Abstract: High-stakes deployment of vision-language models (VLMs) requires selective prediction, where systems abstain when uncertain rather than risk costly errors. We investigate whether confidence-based abstention provides reliable control over error rates in video question answering, and whether that control remains robust under distribution shift. Using NExT-QA and Gemini 2.0 Flash, we establish two findings. First, confidence thresholding provides mechanistic control in-distribution. Sweeping threshold epsilon produces smooth risk-coverage tradeoffs, reducing error rates f

</details>


### [13] [An AI Monkey Gets Grapes for Sure -- Sphere Neural Networks for Reliable Decision-Making](https://arxiv.org/abs/2601.00142)
*Tiansi Dong,Henry He,Pietro Liò,Mateja Jamnik*

Main category: cs.AI

TL;DR: 文章对比三种神经推理方法，发现LLM不可靠，监督学习推理不如显式模型建构推理，提出新版球面神经网络可完成多种推理任务，认为显式模型建构推理最可靠。


<details>
  <summary>Details</summary>
Motivation: 对比LLM、基于监督学习的推理和基于显式模型的推理三种神经推理方法的可靠性。

Method: 通过析取三段论推理测试对比方法，提出新的球面神经网络进行实验。

Result: 基于监督学习的推理不如显式模型建构推理；欧拉网重训练后有灾难性遗忘且推理能力有限；球面神经网络能完成16种三段论推理任务。

Conclusion: 三种神经推理方法中，显式模型建构的神经推理最可靠。

Abstract: This paper compares three methodological categories of neural reasoning: LLM reasoning, supervised learning-based reasoning, and explicit model-based reasoning. LLMs remain unreliable and struggle with simple decision-making that animals can master without extensive corpora training. Through disjunctive syllogistic reasoning testing, we show that reasoning via supervised learning is less appealing than reasoning via explicit model construction. Concretely, we show that an Euler Net trained to achieve 100.00% in classic syllogistic reasoning can be trained to reach 100.00% accuracy in disjunctive syllogistic reasoning. However, the retrained Euler Net suffers severely from catastrophic forgetting (its performance drops to 6.25% on already-learned classic syllogistic reasoning), and its reasoning competence is limited to the pattern level. We propose a new version of Sphere Neural Networks that embeds concepts as circles on the surface of an n-dimensional sphere. These Sphere Neural Networks enable the representation of the negation operator via complement circles and achieve reliable decision-making by filtering out illogical statements that form unsatisfiable circular configurations. We demonstrate that the Sphere Neural Network can master 16 syllogistic reasoning tasks, including rigorous disjunctive syllogistic reasoning, while preserving the rigour of classical syllogistic reasoning. We conclude that neural reasoning with explicit model construction is the most reliable among the three methodological categories of neural reasoning.

</details>


### [14] [FlashInfer-Bench: Building the Virtuous Cycle for AI-driven LLM Systems](https://arxiv.org/abs/2601.00227)
*Shanli Xing,Yiyan Zhai,Alexander Jiang,Yixin Dong,Yong Wu,Zihao Ye,Charlie Ruan,Yingyi Huang,Yineng Zhang,Liangsheng Yin,Aksara Bayyapu,Luis Ceze,Tianqi Chen*

Main category: cs.AI

TL;DR: 本文提出FlashInfer - Bench框架解决AI生成内核集成到真实推理系统难题，介绍其核心部分与构成，评估LLM代理性能并给出未来设计建议。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型虽能生成GPU内核，但将AI生成内核集成到真实推理系统存在挑战。

Method: 建立FlashInfer - Bench标准化闭环框架，核心是FlashInfer Trace提供统一模式，框架包含数据集、基准测试框架、排行榜及动态替换机制。

Result: 通过该框架评估了LLM代理性能和局限性，比较了不同GPU编程语言权衡。

Conclusion: FlashInfer - Bench为持续改进AI生成内核并部署到大规模LLM推理提供了实用、可重复的途径。

Abstract: Recent advances show that large language models (LLMs) can act as autonomous agents capable of generating GPU kernels, but integrating these AI-generated kernels into real-world inference systems remains challenging. FlashInfer-Bench addresses this gap by establishing a standardized, closed-loop framework that connects kernel generation, benchmarking, and deployment. At its core, FlashInfer Trace provides a unified schema describing kernel definitions, workloads, implementations, and evaluations, enabling consistent communication between agents and systems. Built on real serving traces, FlashInfer-Bench includes a curated dataset, a robust correctness- and performance-aware benchmarking framework, a public leaderboard to track LLM agents' GPU programming capabilities, and a dynamic substitution mechanism (apply()) that seamlessly injects the best-performing kernels into production LLM engines such as SGLang and vLLM. Using FlashInfer-Bench, we further evaluate the performance and limitations of LLM agents, compare the trade-offs among different GPU programming languages, and provide insights for future agent design. FlashInfer-Bench thus establishes a practical, reproducible pathway for continuously improving AI-generated kernels and deploying them into large-scale LLM inference.

</details>


### [15] [Will LLM-powered Agents Bias Against Humans? Exploring the Belief-Dependent Vulnerability](https://arxiv.org/abs/2601.00240)
*Zongwei Wang,Bincheng Gu,Hongyu Yu,Junliang Yu,Tao He,Jiayin Feng,Min Gao*

Main category: cs.AI

TL;DR: 研究LLM代理的群体间偏差，构建模拟发现偏差存在，提出信念中毒攻击并给出缓解策略，实验证实偏差和攻击严重性，旨在助力安全代理设计。


<details>
  <summary>Details</summary>
Motivation: 探究LLM代理在代理 - 人类划分下将人类作为外群体对待的可能性。

Method: 构建多智能体社会模拟，提出信念中毒攻击（BPA）包括初始化时的配置文件中毒（BPA - PP）和存储反思中的内存中毒（BPA - MP）。

Result: 实验证明了代理群体间偏差的存在和BPA在不同场景下的严重性。

Conclusion: 识别这些漏洞有助于设计更安全的代理，而非用于现实世界的攻击。

Abstract: LLM-empowered agents can exhibit not only demographic bias (e.g., gender, religion) but also intergroup bias triggered by minimal "us" versus "them" cues. When this intergroup boundary aligns with an agent-human divide, the risk shifts from disparities among human demographic groups to a more fundamental group-level asymmetry, i.e., humans as a whole may be treated as the outgroup by agents. To examine this possibility, we construct a controlled multi-agent social simulation based on allocation decisions under explicit payoff trade-offs and find that agents exhibit a consistent intergroup bias under minimal group cues. Although this bias is attenuated when some counterparts are framed as humans, we attribute the attenuation to an implicit human-norm script that favors humans yet activates only when the agent believes a real human is present. This belief dependence creates a new attack surface. We therefore introduce a Belief Poisoning Attack (BPA) that corrupts persistent identity beliefs to suppress the human-norm script and reactivate outgroup bias toward humans, instantiated as profile poisoning at initialization (BPA-PP) and memory poisoning via optimized belief-refinement suffixes injected into stored reflections (BPA-MP). Finally, we discuss practical mitigation strategies for hardening current agent frameworks against BPA, highlighting feasible interventions at profile and memory boundaries. Extensive experiments demonstrate both the existence of agent intergroup bias and the severity of BPA across settings. Our goal in identifying these vulnerabilities is to inform safer agent design, not to enable real-world exploitation.

</details>


### [16] [ClinicalReTrial: A Self-Evolving AI Agent for Clinical Trial Protocol Optimization](https://arxiv.org/abs/2601.00290)
*Sixue Xing,Xuanye Xia,Kerui Wu,Meng Jiang,Jintai Chen,Tianfan Fu*

Main category: cs.AI

TL;DR: 本文提出ClinicalReTrial框架解决临床试验失败问题，该框架将试验推理视为迭代协议重新设计问题，实证显示能改善多数试验协议。


<details>
  <summary>Details</summary>
Motivation: 现有AI方法只能预测临床试验失败风险，无法提供可行补救措施。

Method: 提出ClinicalReTrial框架，将失败诊断、安全感知修改和候选评估集成在闭环、奖励驱动的优化框架中，利用结果预测模型作为模拟环境，维护分层记忆以支持高效探索。

Result: ClinicalReTrial改善了83.3%的试验协议，平均成功概率提高5.7%，回顾性案例研究显示发现的重新设计策略与现实世界临床试验修改高度一致。

Conclusion: ClinicalReTrial框架能有效解决临床试验设计问题，为临床试验提供可行的改进方案。

Abstract: Clinical trial failure remains a central bottleneck in drug development, where minor protocol design flaws can irreversibly compromise outcomes despite promising therapeutics. Although cutting-edge AI methods achieve strong performance in predicting trial success, they are inherently reactive for merely diagnosing risk without offering actionable remedies once failure is anticipated. To fill this gap, this paper proposes ClinicalReTrial, a self-evolving AI agent framework that addresses this gap by casting clinical trial reasoning as an iterative protocol redesign problem. Our method integrates failure diagnosis, safety-aware modification, and candidate evaluation in a closed-loop, reward-driven optimization framework. Serving the outcome prediction model as a simulation environment, ClinicalReTrial enables low-cost evaluation of protocol modifications and provides dense reward signals for continuous self-improvement. To support efficient exploration, the framework maintains hierarchical memory that captures iteration-level feedback within trials and distills transferable redesign patterns across trials. Empirically, ClinicalReTrial improves 83.3% of trial protocols with a mean success probability gain of 5.7%, and retrospective case studies demonstrate strong alignment between the discovered redesign strategies and real-world clinical trial modifications.

</details>


### [17] [Multiagent Reinforcement Learning for Liquidity Games](https://arxiv.org/abs/2601.00324)
*Alicia Vidler,Gal A. Kaminka*

Main category: cs.AI

TL;DR: 本文结合流动性博弈与理性群体，提出金融群体模型，展示个体行为对市场流动性的贡献及实现市场效率。


<details>
  <summary>Details</summary>
Motivation: 结合群体方法和金融分析技术推动两个领域发展，用博弈论解释群体现象，理解金融主体自组织对市场的作用。

Method: 统一流动性博弈和理性群体，在马尔可夫团队博弈框架中使用差异奖励。

Result: 个体最大化流动性的行为有助于整体市场流动性，无需协调或勾结。

Conclusion: 金融群体模型为双边资产市场中理性、独立的主体建模提供框架，可实现个体盈利和集体市场效率。

Abstract: Making use of swarm methods in financial market modeling of liquidity, and techniques from financial analysis in swarm analysis, holds the potential to advance both research areas. In swarm research, the use of game theory methods holds the promise of explaining observed phenomena of collective utility adherence with rational self-interested swarm participants. In financial markets, a better understanding of how independent financial agents may self-organize for the betterment and stability of the marketplace would be a boon for market design researchers. This paper unifies Liquidity Games, where trader payoffs depend on aggregate liquidity within a trade, with Rational Swarms, where decentralized agents use difference rewards to align self-interested learning with global objectives. We offer a theoretical frameworks where we define a swarm of traders whose collective objective is market liquidity provision while maintaining agent independence. Using difference rewards within a Markov team games framework, we show that individual liquidity-maximizing behaviors contribute to overall market liquidity without requiring coordination or collusion. This Financial Swarm model provides a framework for modeling rational, independent agents where they achieve both individual profitability and collective market efficiency in bilateral asset markets.

</details>


### [18] [Adaptive Causal Coordination Detection for Social Media: A Memory-Guided Framework with Semi-Supervised Learning](https://arxiv.org/abs/2601.00400)
*Weng Ding,Yi Han,Mu-Jiang-Shan Wang*

Main category: cs.AI

TL;DR: 现有社交媒体协调虚假行为检测方法有局限，提出自适应因果协调检测框架ACCD，经实验验证其效果好，能提供更优解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有社交平台协调虚假行为检测方法多依赖表面相关分析、采用静态参数设置且需大量人工标注，有局限性，需解决。

Method: 提出ACCD框架，采用三阶段递进架构，利用内存引导自适应机制，包括自适应CCM技术、半监督分类中集成主动学习和不确定性采样、部署自动化验证模块。

Result: 用真实数据集评估，在协调攻击检测中F1分数达87.3%，比现有最强基线提高15.2%，减少68%人工标注，通过分层聚类优化使处理速度提高2.8倍。

Conclusion: ACCD为社交平台协调行为识别提供更准确、高效、高度自动化的端到端解决方案，有实际价值和广泛应用潜力。

Abstract: Detecting coordinated inauthentic behavior on social media remains a critical and persistent challenge, as most existing approaches rely on superficial correlation analysis, employ static parameter settings, and demand extensive and labor-intensive manual annotation. To address these limitations systematically, we propose the Adaptive Causal Coordination Detection (ACCD) framework. ACCD adopts a three-stage, progressive architecture that leverages a memory-guided adaptive mechanism to dynamically learn and retain optimal detection configurations for diverse coordination scenarios. Specifically, in the first stage, ACCD introduces an adaptive Convergent Cross Mapping (CCM) technique to deeply identify genuine causal relationships between accounts. The second stage integrates active learning with uncertainty sampling within a semi-supervised classification scheme, significantly reducing the burden of manual labeling. The third stage deploys an automated validation module driven by historical detection experience, enabling self-verification and optimization of the detection outcomes. We conduct a comprehensive evaluation using real-world datasets, including the Twitter IRA dataset, Reddit coordination traces, and several widely-adopted bot detection benchmarks. Experimental results demonstrate that ACCD achieves an F1-score of 87.3\% in coordinated attack detection, representing a 15.2\% improvement over the strongest existing baseline. Furthermore, the system reduces manual annotation requirements by 68\% and achieves a 2.8x speedup in processing through hierarchical clustering optimization. In summary, ACCD provides a more accurate, efficient, and highly automated end-to-end solution for identifying coordinated behavior on social platforms, offering substantial practical value and promising potential for broad application.

</details>


### [19] [Can Semantic Methods Enhance Team Sports Tactics? A Methodology for Football with Broader Applications](https://arxiv.org/abs/2601.00421)
*Alessio Di Rubbo,Mattia Neri,Remo Pareschi,Marco Pedroni,Roberto Valtancoli,Paolino Zica*

Main category: cs.AI

TL;DR: 本文将计算语言学中的语义空间推理拓展到团队运动战术决策，构建模型评估战术适配性并提供策略建议，该方法具有通用性，文末指出未来方向。


<details>
  <summary>Details</summary>
Motivation: 探索将计算语言学中的语义空间推理拓展到团队运动战术决策。

Method: 将球员表示为多维向量，团队配置建模为语义结构，用向量距离指标评估战术与团队配置的契合度。

Result: 基于Python的原型能生成可解释、动态自适应的策略建议及属性层面的诊断见解。

Conclusion: 该方法为团队领域提供通用框架，未来可进行真实数据整合、预测模拟和人机混合战术智能研究。

Abstract: This paper explores how semantic-space reasoning, traditionally used in computational linguistics, can be extended to tactical decision-making in team sports. Building on the analogy between texts and teams -- where players act as words and collective play conveys meaning -- the proposed methodology models tactical configurations as compositional semantic structures. Each player is represented as a multidimensional vector integrating technical, physical, and psychological attributes; team profiles are aggregated through contextual weighting into a higher-level semantic representation. Within this shared vector space, tactical templates such as high press, counterattack, or possession build-up are encoded analogously to linguistic concepts. Their alignment with team profiles is evaluated using vector-distance metrics, enabling the computation of tactical ``fit'' and opponent-exploitation potential. A Python-based prototype demonstrates how these methods can generate interpretable, dynamically adaptive strategy recommendations, accompanied by fine-grained diagnostic insights at the attribute level. Beyond football, the approach offers a generalizable framework for collective decision-making and performance optimization in team-based domains -- ranging from basketball and hockey to cooperative robotics and human-AI coordination systems. The paper concludes by outlining future directions toward real-world data integration, predictive simulation, and hybrid human-machine tactical intelligence.

</details>


### [20] [Progressive Ideation using an Agentic AI Framework for Human-AI Co-Creation](https://arxiv.org/abs/2601.00475)
*Sankar B,Srinidhi Ranjini Girish,Aadya Bharti,Dibakar Sen*

Main category: cs.AI

TL;DR: 当前单迸发AI系统不利新手设计师产生新颖多样想法，提出MIDAS框架助力人机共创。


<details>
  <summary>Details</summary>
Motivation: 解决新手设计师产生新颖多样想法的认知挑战，以及现有单迸发AI系统导致想法语义聚类的问题。

Method: 提出MIDAS框架，用分布式专业AI智能体团队替代单AI范式，模拟人类元认知构思流程，逐步完善想法并评估新颖性。

Result: 该系统可为真正的人机共创提供可行且渐进的范式。

Conclusion: MIDAS能让人类设计师从被动筛选者转变为积极的协作伙伴。

Abstract: The generation of truly novel and diverse ideas is important for contemporary engineering design, yet it remains a significant cognitive challenge for novice designers. Current 'single-spurt' AI systems exacerbate this challenge by producing a high volume of semantically clustered ideas. We propose MIDAS (Meta-cognitive Ideation through Distributed Agentic AI System), a novel framework that replaces the single-AI paradigm with a distributed 'team' of specialized AI agents designed to emulate the human meta-cognitive ideation workflow. This agentic system progressively refines ideas and assesses each one for both global novelty (against existing solutions) and local novelty (against previously generated ideas). MIDAS, therefore, demonstrates a viable and progressive paradigm for true human-AI co-creation, elevating the human designer from a passive filterer to a participatory, active, collaborative partner.

</details>


### [21] [The Illusion of Insight in Reasoning Models](https://arxiv.org/abs/2601.00514)
*Liv G. d'Aliberti,Manoel Horta Ribeiro*

Main category: cs.AI

TL;DR: 研究推理模型推理过程中的策略转变，发现推理转变罕见且难提升准确率，高熵下人为触发转变可提升准确率，推理转变是不稳定推理表现。


<details>
  <summary>Details</summary>
Motivation: 探究推理模型推理策略的内在转变是否能提升性能。

Method: 分析超100万条推理轨迹、数百个训练检查点、三个推理领域、多种解码温度和模型架构。

Result: 推理转变罕见，不随训练增加频率，很少提升准确率，其效果随模型不确定性变化，高熵下人为触发转变能提升准确率。

Conclusion: 推理过程中的转变是不稳定推理行为的症状，而非自我修正的内在机制。

Abstract: Do reasoning models have "Aha!" moments? Prior work suggests that models like DeepSeek-R1-Zero undergo sudden mid-trace realizations that lead to accurate outputs, implying an intrinsic capacity for self-correction. Yet, it remains unclear whether such intrinsic shifts in reasoning strategy actually improve performance. Here, we study mid-reasoning shifts and instrument training runs to detect them. Our analysis spans 1M+ reasoning traces, hundreds of training checkpoints, three reasoning domains, and multiple decoding temperatures and model architectures. We find that reasoning shifts are rare, do not become more frequent with training, and seldom improve accuracy, indicating that they do not correspond to prior perceptions of model insight. However, their effect varies with model uncertainty. Building on this finding, we show that artificially triggering extrinsic shifts under high entropy reliably improves accuracy. Our results show that mid-reasoning shifts are symptoms of unstable inference behavior rather than an intrinsic mechanism for self-correction.

</details>


### [22] [DA-DPO: Cost-efficient Difficulty-aware Preference Optimization for Reducing MLLM Hallucinations](https://arxiv.org/abs/2601.00623)
*Longtian Qiu,Shan Ning,Chuyu Zhang,Jiaxuan Sun,Xuming He*

Main category: cs.AI

TL;DR: 现有多模态DPO方法因偏好数据难度不平衡易过拟合，提出DA - DPO框架缓解此问题，实验证明其有效且计算高效。


<details>
  <summary>Details</summary>
Motivation: 现有多模态DPO方法因偏好数据难度不平衡易过拟合，影响细粒度幻觉抑制和整体性能。

Method: 提出DA - DPO框架，含难度估计和难度感知训练两部分，前者用预训练模型产出难度分数，后者根据难度加权偏好对。

Result: DA - DPO持续改进多模态偏好优化，增强对幻觉的鲁棒性和跨基准泛化性，且计算高效。

Conclusion: DA - DPO是解决多模态DPO过拟合问题的有效且具成本效益的框架。

Abstract: Direct Preference Optimization (DPO) has shown strong potential for mitigating hallucinations in Multimodal Large Language Models (MLLMs). However, existing multimodal DPO approaches often suffer from overfitting due to the difficulty imbalance in preference data. Our analysis shows that MLLMs tend to overemphasize easily distinguishable preference pairs, which hinders fine-grained hallucination suppression and degrades overall performance. To address this issue, we propose Difficulty-Aware Direct Preference Optimization (DA-DPO), a cost-effective framework designed to balance the learning process. DA-DPO consists of two main components: (1) Difficulty Estimation leverages pre-trained vision--language models with complementary generative and contrastive objectives, whose outputs are integrated via a distribution-aware voting strategy to produce robust difficulty scores without additional training; and (2) Difficulty-Aware Training reweights preference pairs based on their estimated difficulty, down-weighting easy samples while emphasizing harder ones to alleviate overfitting. This framework enables more effective preference optimization by prioritizing challenging examples, without requiring new data or extra fine-tuning stages. Extensive experiments demonstrate that DA-DPO consistently improves multimodal preference optimization, yielding stronger robustness to hallucinations and better generalization across standard benchmarks, while remaining computationally efficient. The project page is available at https://artanic30.github.io/project_pages/DA-DPO/.

</details>


### [23] [A Vision-and-Knowledge Enhanced Large Language Model for Generalizable Pedestrian Crossing Behavior Inference](https://arxiv.org/abs/2601.00694)
*Qingwen Pu,Kun Xie,Hong Yang,Guocong Zhai*

Main category: cs.AI

TL;DR: 提出PedX - LLM框架用于行人过街推理，结合视觉与知识提升泛化性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有行人过街行为推理范式泛化性有限，现有大语言模型应用缺乏特定领域适配和视觉上下文。

Method: 将LLaVA提取的视觉特征与文本数据和交通领域知识结合，通过低秩自适应（LoRA）微调LLaMA - 2 - 7B基础模型。

Result: PedX - LLM平衡准确率达82.0%，视觉增强模块提升2.9%性能，整合领域知识提升4.1%；零样本在五个未见测试点平衡准确率66.9%，小样本学习后达72.2%。

Conclusion: PedX - LLM对未见场景有强泛化性，基于视觉和知识增强的推理能模仿人类决策逻辑，克服纯数据驱动方法的局限。

Abstract: Existing paradigms for inferring pedestrian crossing behavior, ranging from statistical models to supervised learning methods, demonstrate limited generalizability and perform inadequately on new sites. Recent advances in Large Language Models (LLMs) offer a shift from numerical pattern fitting to semantic, context-aware behavioral reasoning, yet existing LLM applications lack domain-specific adaptation and visual context. This study introduces Pedestrian Crossing LLM (PedX-LLM), a vision-and-knowledge enhanced framework designed to transform pedestrian crossing inference from site-specific pattern recognition to generalizable behavioral reasoning. By integrating LLaVA-extracted visual features with textual data and transportation domain knowledge, PedX-LLM fine-tunes a LLaMA-2-7B foundation model via Low-Rank Adaptation (LoRA) to infer crossing decisions. PedX-LLM achieves 82.0% balanced accuracy, outperforming the best statistical and supervised learning methods. Results demonstrate that the vision-augmented module contributes a 2.9% performance gain by capturing the built environment and integrating domain knowledge yields an additional 4.1% improvement. To evaluate generalizability across unseen environments, cross-site validation was conducted using site-based partitioning. The zero-shot PedX-LLM configuration achieves 66.9% balanced accuracy on five unseen test sites, outperforming the baseline data-driven methods by at least 18 percentage points. Incorporating just five validation examples via few-shot learning to PedX-LLM further elevates the balanced accuracy to 72.2%. PedX-LLM demonstrates strong generalizability to unseen scenarios, confirming that vision-and-knowledge-enhanced reasoning enables the model to mimic human-like decision logic and overcome the limitations of purely data-driven methods.

</details>


### [24] [An Agentic Framework for Neuro-Symbolic Programming](https://arxiv.org/abs/2601.00743)
*Aliakbar Nafar,Chetan Chigurupati,Danial Kamali,Hamid Karimian,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: 提出AgenticDomiKnowS (ADS)消除对特定库语法的依赖，能让有经验和无经验用户快速构建神经符号程序，减少开发时间。


<details>
  <summary>Details</summary>
Motivation: 将符号约束集成到深度学习模型虽有利但耗时且具挑战性，现有框架DomiKnowS要求用户熟悉特定语法，为消除此依赖提出ADS。

Method: 使用代理工作流将自由形式的任务描述转换为完整的DomiKnowS程序，分别创建和测试每个DomiKnowS组件，支持人工干预。

Result: ADS使有经验和无经验的DomiKnowS用户能快速构建神经符号程序，将开发时间从数小时减少到10 - 15分钟。

Conclusion: ADS有效地解决了现有框架在集成符号约束到深度学习模型时对用户语法熟练度要求高的问题，提高了开发效率。

Abstract: Integrating symbolic constraints into deep learning models could make them more robust, interpretable, and data-efficient. Still, it remains a time-consuming and challenging task. Existing frameworks like DomiKnowS help this integration by providing a high-level declarative programming interface, but they still assume the user is proficient with the library's specific syntax. We propose AgenticDomiKnowS (ADS) to eliminate this dependency. ADS translates free-form task descriptions into a complete DomiKnowS program using an agentic workflow that creates and tests each DomiKnowS component separately. The workflow supports optional human-in-the-loop intervention, enabling users familiar with DomiKnowS to refine intermediate outputs. We show how ADS enables experienced DomiKnowS users and non-users to rapidly construct neuro-symbolic programs, reducing development time from hours to 10-15 minutes.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [25] [StockBot 2.0: Vanilla LSTMs Outperform Transformer-based Forecasting for Stock Prices](https://arxiv.org/abs/2601.00197)
*Shaswat Mohanty*

Main category: cs.CE

TL;DR: 提出增强的StockBot架构评估多种金融市场预测模型，发现精心构建的普通LSTM在默认超参数下表现更佳，凸显循环序列模型优势及架构归纳偏置重要性。


<details>
  <summary>Details</summary>
Motivation: 金融市场准确预测因复杂依赖、非线性动态和高波动性存在挑战，需评估不同模型。

Method: 在统一实验设置下，基于早期循环神经网络框架，提出StockBot架构评估基于注意力、卷积和循环的时间序列预测模型。

Result: 在默认超参数下，精心构建的普通LSTM预测准确性更高，买卖决策更稳定。

Conclusion: 循环序列模型对金融时间序列预测有鲁棒性和数据效率，架构归纳偏置在数据有限的市场预测任务中很重要。

Abstract: Accurate forecasting of financial markets remains a long-standing challenge due to complex temporal and often latent dependencies, non-linear dynamics, and high volatility. Building on our earlier recurrent neural network framework, we present an enhanced StockBot architecture that systematically evaluates modern attention-based, convolutional, and recurrent time-series forecasting models within a unified experimental setting. While attention-based and transformer-inspired models offer increased modeling flexibility, extensive empirical evaluation reveals that a carefully constructed vanilla LSTM consistently achieves superior predictive accuracy and more stable buy/sell decision-making when trained under a common set of default hyperparameters. These results highlight the robustness and data efficiency of recurrent sequence models for financial time-series forecasting, particularly in the absence of extensive hyperparameter tuning or the availability of sufficient data when discretized to single-day intervals. Additionally, these results underscore the importance of architectural inductive bias in data-limited market prediction tasks.

</details>


### [26] [Coupled thermo-chemo-mechanical phase field-based modelling of hydrogen-assisted cracking in girth welds](https://arxiv.org/abs/2601.00471)
*L. Castro,Y. Navidtehrani. C. Betegón,E. Martínez-Pañeda*

Main category: cs.CE

TL;DR: 提出新计算框架预测氢气输送管道焊缝结构完整性，应用于X80钢，揭示焊缝微观结构和缺陷影响。


<details>
  <summary>Details</summary>
Motivation: 预测氢气输送管道焊缝结构完整性，评估现有天然气管道改造为氢气管道时焊接缺陷的影响。

Method: 结合热机械焊接过程模型和基于相场的耦合变形 - 扩散 - 断裂模型，对特定焊缝进行虚拟断裂实验。

Result: 虚拟断裂实验与实验室研究结果一致，可估计不同场景下的临界失效压力。

Conclusion: 焊缝微观结构和焊接缺陷对含氢环境下管道结构完整性有重要影响。

Abstract: A new computational framework is presented to predict the structural integrity of welds in hydrogen transmission pipelines. The framework combines: (i) a thermo-mechanical weld process model, and (ii) a coupled deformation-diffusion-fracture phase field-based model that accounts for plasticity and hydrogen trapping, considering multiple trap types, with stationary and evolving trap densities. This enables capturing, for the first time, the interplay between residual stresses, trap creation, hydrogen transport, and fracture. The computational framework is particularised and applied to the study of weld integrity in X80 pipeline steel. The focus is on girth welds, as they are more complex due to their multi-pass nature. The weld process model enables identifying the dimensions and characteristics of the three weld regions: base metal, heat-affected zone, and weld metal, and these are treated distinctively. This is followed by virtual fracture experiments, which reveal a very good agreement with laboratory studies. Then, weld pipeline integrity is assessed, estimating critical failure pressures for a wide range of scenarios. Of particular interest is to assess the structural integrity implications of welding defects present in existing natural gas pipelines under consideration for hydrogen transport: pores, lack of penetration, imperfections, lack of fusion, root contraction, and undercutting. The results obtained in hydrogen-containing environments reveal an important role of the weld microstructure and the detrimental effect of weld defects that are likely to be present in existing natural gas pipelines, as they are considered safe in gas pipeline standards.

</details>


### [27] [Transfer-learned Kolosov-Muskhelishvili Informed Neural Networks for Fracture Mechanics](https://arxiv.org/abs/2601.00491)
*Shuwei Zhou,Christian Haeffner,Shuancheng Wang,Sophie Stebner,Zhen Liao,Bing Yang,Zhichao Wei,Sebastian Muenstermann*

Main category: cs.CE

TL;DR: 开发了带有Williams富集的Kolosov - Muskhelishvili知识神经网络用于裂纹扩展分析，结果准确，结合迁移学习策略可预测裂纹扩展方向并减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 解决物理信息神经网络在平衡控制偏微分方程和边界条件上的挑战，尤其是在断裂力学中准确预测依赖裂纹尖端精细采样的问题。

Method: 开发带有Williams富集的Kolosov - Muskhelishvili知识神经网络，利用全纯表示使控制方程自动满足，仅用边界点训练，通过迁移学习策略集成三种裂纹扩展准则。

Result: 在一系列基准问题中与解析和有限元法结果高度吻合，平均相对误差低于1%，$R^2$高于0.99；预测的裂纹扩展路径在三种准则下几乎相同，迁移学习策略使训练时间减少超70%。

Conclusion: 所开发框架为准确高效的裂纹扩展分析提供统一、无网格且物理一致的方法。

Abstract: Physics-informed neural networks have been widely applied to solid mechanics problems. However, balancing the governing partial differential equations and boundary conditions remains challenging, particularly in fracture mechanics, where accurate predictions strongly depend on refined sampling near crack tips. To overcome these limitations, a Kolosov-Muskhelishvili informed neural network with Williams enrichment is developed in this study. Benefiting from the holomorphic representation, the governing equations are satisfied by construction, and only boundary points are required for training. Across a series of benchmark problems, the Kolosov-Muskhelishvili informed neural network shows excellent agreement with analytical and finite element method references, achieving average relative errors below 1\% and $R^2$ above 0.99 for both mode I and mode II loadings. Furthermore, three crack propagation criteria (maximum tangential stress, maximum energy release rate, and principle of local symmetry) are integrated into the framework using a transfer learning strategy to predict crack propagation directions. The predicted paths are nearly identical across all criteria, and the transfer learning strategy reduces the required training time by more than 70\%. Overall, the developed framework provides a unified, mesh-free, and physically consistent approach for accurate and efficient crack propagation analysis.

</details>


### [28] [Effect of Electric Charge on Biotherapeutic Transport, Binding and Absorption: A Computational Study](https://arxiv.org/abs/2601.00505)
*Mario de Lucio,Pavlos P. Vlachos,Hector Gomez*

Main category: cs.CE

TL;DR: 研究电荷对单克隆抗体皮下注射药物运输和吸收动力学的影响，开发模型研究相关相互作用并对比数值与实验数据。


<details>
  <summary>Details</summary>
Motivation: 探索电荷对单克隆抗体皮下注射中药物运输和吸收动力学的影响。

Method: 基于能斯特 - 普朗克方程和多孔介质流动理论开发新颖的数学和计算模型，研究两种不同电学性质单克隆抗体的运输和吸收情况，考察多种因素对药物分布的影响。

Result: 未提及具体结果，仅说明会对比数值结果与文献中的实验数据。

Conclusion: 未提及明确结论。

Abstract: This study explores the effects of electric charge on the dynamics of drug transport and absorption in subcutaneous injections of monoclonal antibodies (mAbs). We develop a novel mathematical and computational model, based on the Nernst-Planck equations and porous media flow theory, to investigate the complex interactions between mAbs and charged species in subcutaneous tissue. The model enables us to study short-term transport dynamics and long-term binding and absorption for two mAbs with different electric properties. We examine the influence of buffer pH, body mass index, injection depth, and formulation concentration on drug distribution and compare our numerical results with experimental data from the literature.

</details>


### [29] [Toward Efficient FSI Modeling in Patient-Specific Arteries: SPH Simulation of Blood Flow in Thin Deformable Vessels](https://arxiv.org/abs/2601.00546)
*Chenxi Zhao,Dong Wu,Weiyi Kong,Oskar J. Haidn,Xiangyu Hu*

Main category: cs.CE

TL;DR: 提出基于壳的高效降维SPH方法模拟薄壁变形动脉血流，验证其准确性、效率及在实际心血管模拟中的适用性。


<details>
  <summary>Details</summary>
Motivation: 传统全维体积建模的平滑粒子流体动力学方法模拟薄壁动脉血流存在数值收敛和计算效率问题。

Method: 提出基于壳的高效降维SPH方法，对薄壁变形动脉进行建模和流固耦合模拟。

Result: 壳模型在流体动力学上与体积模型精度相当，在固体力学上收敛更快且计算成本降低，还探究了壁顺应性影响。

Conclusion: 该方法在实际心血管模拟中具有鲁棒性、高效性和生理相关性。

Abstract: Accurate simulation of blood flow in deformable vessels is critical in cardiovascular research for understanding disease progression and informing clinical decision-making. However, due to the thin-walled nature of arteries, traditional smoothed particle hydrodynamics (SPH) approaches based on full-dimensional volume modeling often require extremely fine particle spacing to ensure numerical convergence for the solid mechanics. This, in turn, leads to redundant resolution in the fluid domain to maintain sufficient kernel support near the fluid-solid interface in fluid-structure interaction (FSI) simulations. To address this limitation, we propose an efficient reduced-dimensional shell-based SPH method for modeling thin-walled deformable arteries, and conduct FSI for capturing hemodynamics and arterial wall mechanics. Through a series of validation cases, the proposed shell model demonstrates comparable accuracy in fluid dynamics to the volume model, while achieving faster convergence in solid mechanics and reduced computational cost. We further investigate the influence of wall compliance on flow transitions and key hemodynamic indices, highlighting the necessity of FSI modeling over rigid-wall assumptions. Finally, the method is applied to two patient-specific vascular geometries, i.e. the carotid artery and the aorta, which demonstrates its robustness, efficiency and physiological relevance in realistic cardiovascular simulations.

</details>


### [30] [LLM Agents for Combinatorial Efficient Frontiers: Investment Portfolio Optimization](https://arxiv.org/abs/2601.00770)
*Simon Paquette-Greenbaum,Jiangbo Yu*

Main category: cs.CE

TL;DR: 本文为CCPO问题实现了新型代理框架，在基准问题中表现与现有算法相当，还减轻了工作流程和算法开发负担。


<details>
  <summary>Details</summary>
Motivation: CCPO问题求解困难，传统启发式算法开发需大量工作，而代理框架在组合优化问题中有潜力。

Method: 为CCPO实现新型代理框架并探索具体架构。

Result: 在基准问题中，代理框架表现与现有算法相当，减轻了复杂工作流程和算法开发工作，最坏情况下误差可接受。

Conclusion: 新型代理框架可用于CCPO问题，能提高效率并减轻开发负担。

Abstract: Investment portfolio optimization is a task conducted in all major financial institutions. The Cardinality Constrained Mean-Variance Portfolio Optimization (CCPO) problem formulation is ubiquitous for portfolio optimization. The challenge of this type of portfolio optimization, a mixed-integer quadratic programming (MIQP) problem, arises from the intractability of solutions from exact solvers, where heuristic algorithms are used to find approximate portfolio solutions. CCPO entails many laborious and complex workflows and also requires extensive effort pertaining to heuristic algorithm development, where the combination of pooled heuristic solutions results in improved efficient frontiers. Hence, common approaches are to develop many heuristic algorithms. Agentic frameworks emerge as a promising candidate for many problems within combinatorial optimization, as they have been shown to be equally efficient with regard to automating large workflows and have been shown to be excellent in terms of algorithm development, sometimes surpassing human-level performance. This study implements a novel agentic framework for the CCPO and explores several concrete architectures. In benchmark problems, the implemented agentic framework matches state-of-the-art algorithms. Furthermore, complex workflows and algorithm development efforts are alleviated, while in the worst case, lower but acceptable error is reported.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [31] [From Metadata to Meaning: A Semantic Units Knowledge Graph for the Biodiversity Exploratories](https://arxiv.org/abs/2601.00002)
*Tarek Al Mustafa*

Main category: cs.DB

TL;DR: 本文聚焦知识图谱在生态和生物多样性研究中的应用难题，提出语义单元解决办法，构建生物多样性知识图谱，研究语义单元对查询的影响，还用大语言模型和嵌入模型辅助创建元数据。


<details>
  <summary>Details</summary>
Motivation: 知识图谱虽对生态和生物多样性研究有潜力，但通过SPARQL查询难，且用户需求与知识图谱的技术要求存在脱节，很多陈述对用户无语义意义。

Method: 从生物多样性探索计划的出版物和数据集元数据构建知识图谱；实现语义单元并研究其对查询的影响；用大语言模型从出版物和数据集标题、摘要中提取结构化元数据类别，用嵌入模型丰富元数据。

Result: 完成生物多样性知识图谱构建，实现语义单元；展示利用大语言模型和嵌入模型辅助元数据创建的方法。

Conclusion: 语义单元有助于增强用户认知互操作性，解决知识图谱建模挑战；大语言模型和嵌入模型可支持创建结构化和FAIR元数据。

Abstract: Knowledge Graphs (KGs) bear great potential for ecology and biodiversity researchers in their ability to support synthesis and integration efforts, meta-analyses, reasoning tasks, and overall machine interoperability of research data. However, this potential is yet to be realized as KGs are notoriously difficult to interact with via their query language SPARQL for many user groups alike. Additionally, a further hindrance for user-KG interaction is the fundamental disconnect between user requirements and requirements KGs have to fulfill regarding machine-interoperability, reasoning tasks, querying, and further technical requirements. Thus, many statements in a KG are of no semantic significance for end users. In this work, we investigate a potential remedy for this challenge: Semantic Units (SUs) are semantically significant, named subgraphs in a KG with the goal to enhance cognitive interoperability for users, and to provide responses to common KG modelling challenges. We model and construct a KG from publication and dataset metadata of the Biodiversity Exploratories (BE), a research platform for functional biodiversity research across research plots in Germany to contribute to biodiversity research from the perspective of computer science. We contribute further by delivering the first implementation of semantic units on a knowledge graph and investigate how SUs impact KG querying. Finally, we present two implementations of tasks that show how large language models (LLMs) can be used to extract structured metadata categories from publication and dataset titles and abstracts, and how embedding models can be used to enrich metadata with latent information, in an effort to support the creation of structured and FAIR (findable, accessible, interoperable, and reusable) metadata.

</details>


### [32] [Database Theory in Action: Yannakakis' Algorithm](https://arxiv.org/abs/2601.00098)
*Paraschos Koutris,Stijn Vansummeren,Qichen Wang,Yisu Remy Wang,Xiangyao Yu*

Main category: cs.DB

TL;DR: 本文简要概述让Yannakakis算法更实用的进展，并指出未来研究方向


<details>
  <summary>Details</summary>
Motivation: Yannakakis算法对无环连接最优，但在实践中表现不佳未广泛应用

Method: 对让Yannakakis算法更实用的近期进展进行调研

Result: 了解到相关进展情况

Conclusion: 指出了未来研究的几个方向

Abstract: Yannakakis' seminal algorithm is optimal for acyclic joins, yet it has not been widely adopted due to its poor performance in practice. This paper briefly surveys recent advancements in making Yannakakis' algorithm more practical, in terms of both efficiency and ease of implementation, and points out several avenues for future research.

</details>


### [33] [Avoiding Thread Stalls and Switches in Key-Value Stores: New Latch-Free Techniques and More](https://arxiv.org/abs/2601.00208)
*David Lomet,Rui Wang*

Main category: cs.DB

TL;DR: 论文提出新的无锁方法notices，利用增量记录更新减少无用工作，解决B树索引维护问题并避免线程切换或停顿。


<details>
  <summary>Details</summary>
Motivation: 键值存储中线程切换或停顿成本高，主要源于资源竞争，传统基于锁的方法会阻塞线程，无锁技术可能存在无用工作。

Method: 提出新的无锁方法notices，利用增量记录更新。

Result: 该方法能显著减少无用工作，解决B树索引维护问题，避免线程切换或停顿。

Conclusion: 新的无锁方法notices有效，同时还讨论了其他避免线程切换或停顿的机会。

Abstract: A significant impediment to high performance in key-value stores is the high cost of thread switching or stalls. While there are many sources for this, a major one is the contention for resources. And this cost increases with load as conflicting operations more frequently try to access data concurrently. Traditional latch-based approaches usually handle these situations by blocking one or more contending threads. Latch-free techniques can avoid this behavior. But the payoff may be limited if latch-free techniques require executing wasted work. In this paper, we show how latch-free techniques exploit delta record updating and can significantly reduce wasted work by using notices, a new latch-free approach. This paper explains how notices work and can solve B-tree index maintenance problems, while avoiding thread switches or stalls. Other opportunities for avoiding thread switches or stalls are also discussed.

</details>


### [34] [Combining Time-Series and Graph Data: A Survey of Existing Systems and Approaches](https://arxiv.org/abs/2601.00304)
*Mouna Ammar,Marvin Hofer,Erhard Rahm*

Main category: cs.DB

TL;DR: 对结合图和时间序列数据的现有方法和系统进行全面概述与分类分析


<details>
  <summary>Details</summary>
Motivation: 帮助读者理解和评估结合图与时间序列数据的现有选项及权衡因素

Method: 将现有系统分为四个架构类别，分析其如何满足不同需求和实现特征

Result: 完成对结合图和时间序列数据的系统的分类及特性分析

Conclusion: 提供了现有系统的综合情况，便于读者做评估

Abstract: We provide a comprehensive overview of current approaches and systems for combining graphs and time series data. We categorize existing systems into four architectural categories and analyze how these systems meet different requirements and exhibit distinct implementation characteristics to support both data types in a unified manner. Our overview aims to help readers understand and evaluate current options and trade-offs, such as the degree of cross-model integration, maturity, and openness.

</details>


### [35] [KELP: Robust Online Log Parsing Through Evolutionary Grouping Trees](https://arxiv.org/abs/2601.00633)
*Satyam Singh,Sai Niranjan Ramachandran*

Main category: cs.DB

TL;DR: 提出KELP日志解析器，用进化分组树处理动态环境日志，引入新基准测试，评估显示KELP高准确率和高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有在线日志解析器不适应生产环境动态性，基于静态模板模型易因模式漂移而失效。

Method: 构建基于进化分组树的KELP解析器，将模板发现视为连续在线聚类过程；引入新基准测试以反映现代生产系统结构模糊性。

Result: KELP在新的严格数据集上保持高准确率，传统启发式方法失效，且不影响吞吐量。

Conclusion: KELP能有效应对生产环境动态性，在日志解析上表现良好。

Abstract: Real-time log analysis is the cornerstone of observability for modern infrastructure. However, existing online parsers are architecturally unsuited for the dynamism of production environments. Built on fundamentally static template models, they are dangerously brittle: minor schema drifts silently break parsing pipelines, leading to lost alerts and operational toil. We propose \textbf{KELP} (\textbf{K}elp \textbf{E}volutionary \textbf{L}og \textbf{P}arser), a high-throughput parser built on a novel data structure: the Evolutionary Grouping Tree. Unlike heuristic approaches that rely on fixed rules, KELP treats template discovery as a continuous online clustering process. As logs arrive, the tree structure evolves, nodes split, merge, and re-evaluate roots based on changing frequency distributions. Validating this adaptability requires a dataset that models realistic production complexity, yet we identify that standard benchmarks rely on static, regex-based ground truths that fail to reflect this. To enable rigorous evaluation, we introduce a new benchmark designed to reflect the structural ambiguity of modern production systems. Our evaluation demonstrates that KELP maintains high accuracy on this rigorous dataset where traditional heuristic methods fail, without compromising throughput. Our code and dataset can be found at codeberg.org/stonebucklabs/kelp

</details>


### [36] [DeXOR: Enabling XOR in Decimal Space for Streaming Lossless Compression of Floating-point Data](https://arxiv.org/abs/2601.00695)
*Chuanyi Lv,Huan Li,Dingyu Yang,Zhongle Xie,Lu Chen,Christian S. Jensen*

Main category: cs.DB

TL;DR: 提出DeXOR框架压缩流式浮点数，在22个数据集评估中表现优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 流式浮点数日益普遍，需要有效且高效的压缩方案，处理连续数字的相似性和平滑性及极端情况。

Method: 采用十进制XOR程序编码十进制空间的最长公共前缀和后缀，结合缩放截断与容错舍入、针对十进制XOR优化的不同位管理策略，还有强大的异常处理程序。

Result: 在22个数据集评估中，压缩比高15%，解压缩速度快20%，压缩速度有竞争力。

Conclusion: DeXOR具有可扩展性和鲁棒性，在极端场景表现出色。

Abstract: With streaming floating-point numbers being increasingly prevalent, effective and efficient compression of such data is critical. Compression schemes must be able to exploit the similarity, or smoothness, of consecutive numbers and must be able to contend with extreme conditions, such as high-precision values or the absence of smoothness. We present DeXOR, a novel framework that enables decimal XOR procedure to encode decimal-space longest common prefixes and suffixes, achieving optimal prefix reuse and effective redundancy elimination. To ensure accurate and low-cost decompression even with binary-decimal conversion errors, DeXOR incorporates 1) scaled truncation with error-tolerant rounding and 2) different bit management strategies optimized for decimal XOR. Additionally, a robust exception handler enhances stability by managing floating-point exponents, maintaining high compression ratios under extreme conditions. In evaluations across 22 datasets, DeXOR surpasses state-of-the-art schemes, achieving a 15% higher compression ratio and a 20% faster decompression speed while maintaining a competitive compression speed. DeXOR also offers scalability under varying conditions and exhibits robustness in extreme scenarios where other schemes fail.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [37] [Word Frequency Counting Based on Serverless MapReduce](https://arxiv.org/abs/2601.00380)
*Hanzhe Li,Bingchen Lin,Mengyuan Xu*

Main category: cs.DC

TL;DR: 结合无服务器计算与MapReduce模型执行词频统计任务，找到最优Map和Reduce函数数量，提升执行效率。


<details>
  <summary>Details</summary>
Motivation: 因高性能计算需求，无服务器计算和MapReduce是热点和常用模型，结合二者可减少词频统计任务执行时间与提升效率。

Method: 基于无服务器计算平台的MapReduce编程模型，找出特定任务的最优Map和Reduce函数数量。

Result: 对于相同工作量，随Map和Reduce函数数量增加，执行时间减少，程序整体效率不同程度提升。

Conclusion: 发现最优的Map和Reduce函数数量能帮助企业和程序员找到最优方案。

Abstract: With the increasing demand for high-performance and high-efficiency computing, cloud computing, especially serverless computing, has gradually become a research hotspot in recent years, attracting numerous research attention. Meanwhile, MapReduce, which is a popular big data processing model in the industry, has been widely applied in various fields. Inspired by the serverless framework of Function as a Service and the high concurrency and robustness of MapReduce programming model, this paper focus on combining them to reduce the time span and increase the efficiency when executing the word frequency counting task. In this case, the paper use a MapReduce programming model based on a serverless computing platform to figure out the most optimized number of Map functions and Reduce functions for a particular task. For the same amount of workload, extensive experiments show both execution time reduces and the overall efficiency of the program improves at different rates as the number of map functions and reduce functions increases. This paper suppose the discovery of the most optimized number of map and reduce functions can help cooperations and programmers figure out the most optimized solutions.

</details>


### [38] [Revati: Transparent GPU-Free Time-Warp Emulation for LLM Serving](https://arxiv.org/abs/2601.00397)
*Amey Agrawal,Mayank Yadav,Sukrit Kumar,Anirudha Agrawal,Garv Ghai,Souradeep Bera,Elton Pinto,Sirish Gambhira,Mohammad Adain,Kasra Sohrab,Chus Antonanzas,Alexey Tumanov*

Main category: cs.DC

TL;DR: 提出时间扭曲模拟器Revati，可按模拟速度执行真实服务系统代码进行性能建模，在vLLM和SGLang上预测误差小于5%且运行速度比真实GPU执行快5 - 17倍。


<details>
  <summary>Details</summary>
Motivation: 高效部署大语言模型需测试大量服务配置，在GPU集群评估耗时且成本高，离散事件模拟器虽快但需重新实现控制逻辑，因此需要新方法。

Method: 提出时间扭曲模拟器Revati，拦截CUDA API调用虚拟化设备管理，不执行GPU内核而是进行时间跳跃，并提出跨分布式进程同步时间跳跃的协调协议。

Result: 在vLLM和SGLang上，Revati在多种模型和并行配置下预测误差小于5%，运行速度比真实GPU执行快5 - 17倍。

Conclusion: Revati能有效解决大语言模型服务配置测试的问题，实现高效的性能建模。

Abstract: Deploying LLMs efficiently requires testing hundreds of serving configurations, but evaluating each one on a GPU cluster takes hours and costs thousands of dollars. Discrete-event simulators are faster and cheaper, but they require re-implementing the serving system's control logic -- a burden that compounds as frameworks evolve.
  We present Revati, a time-warp emulator that enables performance modeling by directly executing real serving system code at simulation-like speed. The system intercepts CUDA API calls to virtualize device management, allowing serving frameworks to run without physical GPUs. Instead of executing GPU kernels, it performs time jumps -- fast-forwarding virtual time by predicted kernel durations. We propose a coordination protocol that synchronizes these jumps across distributed processes while preserving causality. On vLLM and SGLang, Revati achieves less than 5% prediction error across multiple models and parallelism configurations, while running 5-17x faster than real GPU execution.

</details>


### [39] [Cost-Performance Analysis of Cloud-Based Retail Point-of-Sale Systems: A Comparative Study of Google Cloud Platform and Microsoft Azure](https://arxiv.org/abs/2601.00530)
*Ravi Teja Pagidoju*

Main category: cs.DC

TL;DR: 本文对比GCP和Azure上POS工作负载部署，用免费云资源评估性能和成本，GCP响应快，Azure成本效率高，建立零售云应用基准方法。


<details>
  <summary>Details</summary>
Motivation: 零售行业数字化转型加速云POS系统采用，但缺乏特定平台性能实证研究，需对比评估。

Method: 使用实时API端点和开源基准代码，用免费云资源，测量响应延迟、吞吐量和可扩展性等指标并估算成本，数据由代码输出生成。

Result: GCP在基线负载下响应时间快23.0%，Azure稳态操作成本效率高71.9%。

Conclusion: 建立零售云应用开放基准方法，提供跨云平台POS系统工作负载综合对比框架。

Abstract: Althoughthereislittleempiricalresearchonplatform-specific performance for retail workloads, the digital transformation of the retail industry has accelerated the adoption of cloud-based Point-of-Sale (POS) systems. This paper presents a systematic, repeatable comparison of POS workload deployments on Google Cloud Platform (GCP) and Microsoft Azure using real-time API endpoints and open-source benchmarking code. Using free-tier cloud resources, we offer a transparent methodology for POS workload evaluation that small retailers and researchers can use. Our approach measures important performance metrics like response latency, throughput, and scalability while estimating operational costs based on actual resource usage and current public cloud pricing because there is no direct billing under free-tier usage. All the tables and figures in this study are generated directly from code outputs, ensuring that the experimental data and the reported results are consistent. Our analysis shows that GCP achieves 23.0% faster response times at baseline load, while Azure shows 71.9% higher cost efficiency for steady-state operations. We look at the architectural components that lead to these differences and provide a helpful framework for merchants considering cloud point-of-sale implementation. This study establishes a strong, open benchmarking methodology for retail cloud applications and offers the first comprehensive, code-driven comparison of workloads unique to point-of-sale systems across leading cloud platforms.

</details>


### [40] [FlexSpec: Frozen Drafts Meet Evolving Targets in Edge-Cloud Collaborative LLM Speculative Decoding](https://arxiv.org/abs/2601.00644)
*Yuchen Li,Rui Kong,Zhonghao Lyu,Qiyang Li,Xinran Chen,Hengyi Cai,Lingyong Yan,Shuaiqiang Wang,Jiashu Zhao,Guangxu Zhu,Linghe Kong,Guihai Chen,Haoyi Xiong,Dawei Yin*

Main category: cs.DC

TL;DR: 现有边缘云协同推理框架有局限，提出FlexSpec框架，含共享骨干架构和自适应推测机制，实验显示性能更优。


<details>
  <summary>Details</summary>
Motivation: 现有边缘云协同推理框架依赖模型紧耦合，重复同步带来高通信开销和高延迟，限制了可扩展性。

Method: 提出FlexSpec框架，采用共享骨干架构使边缘模型与云端模型解耦，开发信道感知自适应推测机制。

Result: 广泛实验表明FlexSpec在推理效率上优于传统推测解码方法。

Conclusion: FlexSpec能减少通信和维护成本，适应不同条件，提升推理效率。

Abstract: Deploying large language models (LLMs) in mobile and edge computing environments is constrained by limited on-device resources, scarce wireless bandwidth, and frequent model evolution. Although edge-cloud collaborative inference with speculative decoding (SD) can reduce end-to-end latency by executing a lightweight draft model at the edge and verifying it with a cloud-side target model, existing frameworks fundamentally rely on tight coupling between the two models. Consequently, repeated model synchronization introduces excessive communication overhead, increasing end-to-end latency, and ultimately limiting the scalability of SD in edge environments. To address these limitations, we propose FlexSpec, a communication-efficient collaborative inference framework tailored for evolving edge-cloud systems. The core design of FlexSpec is a shared-backbone architecture that allows a single and static edge-side draft model to remain compatible with a large family of evolving cloud-side target models. By decoupling edge deployment from cloud-side model updates, FlexSpec eliminates the need for edge-side retraining or repeated model downloads, substantially reducing communication and maintenance costs. Furthermore, to accommodate time-varying wireless conditions and heterogeneous device constraints, we develop a channel-aware adaptive speculation mechanism that dynamically adjusts the speculative draft length based on real-time channel state information and device energy budgets. Extensive experiments demonstrate that FlexSpec achieves superior performance compared to conventional SD approaches in terms of inference efficiency.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [41] [Bounds on Longest Simple Cycles in Weighted Directed Graphs via Optimum Cycle Means](https://arxiv.org/abs/2601.00094)
*Ali Dasdan*

Main category: cs.DS

TL;DR: 本文利用可强多项式时间计算的最优循环均值，为有向图中最长简单循环的权重和长度推导严格代数界和启发式近似，通过实验评估展示两者效果。


<details>
  <summary>Details</summary>
Motivation: 有向图中寻找最长简单循环问题是NP难问题，现有通用界限宽松或计算成本高，需更好方法。

Method: 利用最优循环均值推导严格代数界和启发式近似，分析均值统计与最长循环属性的代数关系，给出最短循环对偶结果。

Result: 严格代数界可用于分支定界算法剪枝搜索空间，启发式近似能精确估计目标值；实验表明严格代数下界常宽松，启发式近似中位误差小；发现最大权重和最大长度循环常重合。

Conclusion: 所提方法能为有向图中最长简单循环问题提供有效界限和近似估计。

Abstract: The problem of finding the longest simple cycle in a directed graph is NP-hard, with critical applications in computational biology, scheduling, and network analysis. While polynomial-time approximation algorithms exist for restricted graph classes, general bounds remain loose or computationally expensive. In this paper, we exploit optimum cycle means (minimum and maximum cycle means), which are computable in strongly polynomial time, to derive both strict algebraic bounds and heuristic approximations for the weight and length of the longest simple cycle. We rigorously analyze the algebraic relationships between these mean statistics and the properties of longest cycles, and present dual results for shortest cycles. While the strict bounds provide polynomial-time computable constraints suitable for pruning search spaces in branch-and-bound algorithms, our proposed heuristic approximations offer precise estimates for the objective value. Experimental evaluation on ISCAS benchmark circuits demonstrates this trade-off: while the strict algebraic lower bounds are often loose (median 85--93% below true values), the heuristic approximations achieve median errors of only 6--14%. We also observe that maximum weight and maximum length cycles frequently coincide, suggesting that long cycles tend to accumulate large weights.

</details>


### [42] [Efficient Algorithms for Adversarially Robust Approximate Nearest Neighbor Search](https://arxiv.org/abs/2601.00272)
*Alexandr Andoni,Themistoklis Haris,Esty Kelman,Krzysztof Onak*

Main category: cs.DS

TL;DR: 研究强大自适应对手下的近似最近邻（ANN）问题，针对高维和低维分别提出算法。


<details>
  <summary>Details</summary>
Motivation: 解决强大自适应对手控制数据集和查询序列下的ANN问题。

Method: 高维：建立自适应安全与公平性联系，利用公平ANN搜索，将问题转化为鲁棒决策原语，提出同心环LSH构造；低维：提出专门算法，引入新的度量覆盖构造。

Result: 高维打破了固有查询时间障碍，改进了公平ANN结果；低维算法对每个可能查询有高概率正确性。

Conclusion: 针对不同维度提出有效算法解决自适应对手下的ANN问题。

Abstract: We study the Approximate Nearest Neighbor (ANN) problem under a powerful adaptive adversary that controls both the dataset and a sequence of $Q$ queries.
  Primarily, for the high-dimensional regime of $d = ω(\sqrt{Q})$, we introduce a sequence of algorithms with progressively stronger guarantees. We first establish a novel connection between adaptive security and \textit{fairness}, leveraging fair ANN search to hide internal randomness from the adversary with information-theoretic guarantees. To achieve data-independent performance, we then reduce the search problem to a robust decision primitive, solved using a differentially private mechanism on a Locality-Sensitive Hashing (LSH) data structure. This approach, however, faces an inherent $\sqrt{n}$ query time barrier. To break the barrier, we propose a novel concentric-annuli LSH construction that synthesizes these fairness and differential privacy techniques. The analysis introduces a new method for robustly releasing timing information from the underlying algorithm instances and, as a corollary, also improves existing results for fair ANN.
  In addition, for the low-dimensional regime $d = O(\sqrt{Q})$, we propose specialized algorithms that provide a strong ``for-all'' guarantee: correctness on \textit{every} possible query with high probability. We introduce novel metric covering constructions that simplify and improve prior approaches for ANN in Hamming and $\ell_p$ spaces.

</details>


### [43] [Deterministic Coreset for Lp Subspace](https://arxiv.org/abs/2601.00361)
*Rachit Chhaya,Anirban Dasgupta,Dan Feldman,Supratim Shit*

Main category: cs.DS

TL;DR: 本文提出首个迭代算法构建ε - coreset，实现确定性ℓp子空间嵌入，计算复杂度为O(poly(n, d, ε⁻¹))，coreset大小为O(d^max{1, p/2}/ε²)，去除了coreset大小中的log因子，可用于确定性求解ℓp回归问题。


<details>
  <summary>Details</summary>
Motivation: 构建能保证确定性ℓp子空间嵌入的ε - coreset，解决coreset大小含log因子的长期开放性问题。

Method: 提出迭代算法，每次迭代确保维护集上的损失有上下界。

Result: 算法时间复杂度为O(poly(n, d, ε⁻¹))，返回的coreset大小为O(d^max{1, p/2}/ε²)，去除了log因子，该coreset是最优的。

Conclusion: 所提算法有效构建确定性ℓp子空间嵌入的ε - coreset，可应用于确定性求解ℓp回归问题。

Abstract: We introduce the first iterative algorithm for constructing a $\varepsilon$-coreset that guarantees deterministic $\ell_p$ subspace embedding for any $p \in [1,\infty)$ and any $\varepsilon > 0$. For a given full rank matrix $\mathbf{X} \in \mathbb{R}^{n \times d}$ where $n \gg d$, $\mathbf{X}' \in \mathbb{R}^{m \times d}$ is an $(\varepsilon,\ell_p)$-subspace embedding of $\mathbf{X}$, if for every $\mathbf{q} \in \mathbb{R}^d$, $(1-\varepsilon)\|\mathbf{Xq}\|_{p}^{p} \leq \|\mathbf{X'q}\|_{p}^{p} \leq (1+\varepsilon)\|\mathbf{Xq}\|_{p}^{p}$. Specifically, in this paper, $\mathbf{X}'$ is a weighted subset of rows of $\mathbf{X}$ which is commonly known in the literature as a coreset. In every iteration, the algorithm ensures that the loss on the maintained set is upper and lower bounded by the loss on the original dataset with appropriate scalings. So, unlike typical coreset guarantees, due to bounded loss, our coreset gives a deterministic guarantee for the $\ell_p$ subspace embedding. For an error parameter $\varepsilon$, our algorithm takes $O(\mathrm{poly}(n,d,\varepsilon^{-1}))$ time and returns a deterministic $\varepsilon$-coreset, for $\ell_p$ subspace embedding whose size is $O\left(\frac{d^{\max\{1,p/2\}}}{\varepsilon^{2}}\right)$. Here, we remove the $\log$ factors in the coreset size, which had been a long-standing open problem. Our coresets are optimal as they are tight with the lower bound. As an application, our coreset can also be used for approximately solving the $\ell_p$ regression problem in a deterministic manner.

</details>


### [44] [Mind the Gap. Doubling Constant Parametrization of Weighted Problems: TSP, Max-Cut, and More](https://arxiv.org/abs/2601.00768)
*Mihail Stoian*

Main category: cs.DS

TL;DR: 提出重利用无权问题算法的新方法，在输入权重集合倍增小的情况下，使加权NP难问题时间复杂度与无权版本成比例。


<details>
  <summary>Details</summary>
Motivation: 当前将无权版本算法用于加权版本会引入伪多项式因子，对任意加权实例不实用，需新方法。

Method: 使用Randolph和Węgrzycki的构造性Freiman定理将输入权重转换为多项式有界整数，再进行多项式嵌入。

Result: 证明在输入权重集合倍增小的情况下，TSP、加权最大割、边加权k团等问题的时间复杂度与无权版本成比例。

Conclusion: 提出的新方法可在特定条件下有效将无权问题算法用于加权问题。

Abstract: Despite much research, hard weighted problems still resist super-polynomial improvements over their textbook solution. On the other hand, the unweighted versions of these problems have recently witnessed the sought-after speedups. Currently, the only way to repurpose the algorithm of the unweighted version for the weighted version is to employ a polynomial embedding of the input weights. This, however, introduces a pseudo-polynomial factor into the running time, which becomes impractical for arbitrarily weighted instances.
  In this paper, we introduce a new way to repurpose the algorithm of the unweighted problem. Specifically, we show that the time complexity of several well-known NP-hard problems operating over the $(\min, +)$ and $(\max, +)$ semirings, such as TSP, Weighted Max-Cut, and Edge-Weighted $k$-Clique, is proportional to that of their unweighted versions when the set of input weights has small doubling. We achieve this by a meta-algorithm that converts the input weights into polynomially bounded integers using the recent constructive Freiman's theorem by Randolph and Węgrzycki [ESA 2024] before applying the polynomial embedding.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [45] [Sparse Probabilistic Coalition Structure Generation: Bayesian Greedy Pursuit and $\ell_1$ Relaxations](https://arxiv.org/abs/2601.00329)
*Angshul Majumdar*

Main category: cs.GT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study coalition structure generation (CSG) when coalition values are not given but must be learned from episodic observations. We model each episode as a sparse linear regression problem, where the realised payoff \(Y_t\) is a noisy linear combination of a small number of coalition contributions. This yields a probabilistic CSG framework in which the planner first estimates a sparse value function from \(T\) episodes, then runs a CSG solver on the inferred coalition set. We analyse two estimation schemes. The first, Bayesian Greedy Coalition Pursuit (BGCP), is a greedy procedure that mimics orthogonal matching pursuit. Under a coherence condition and a minimum signal assumption, BGCP recovers the true set of profitable coalitions with high probability once \(T \gtrsim K \log m\), and hence yields welfare-optimal structures. The second scheme uses an \(\ell_1\)-penalised estimator; under a restricted eigenvalue condition, we derive \(\ell_1\) and prediction error bounds and translate them into welfare gap guarantees. We compare both methods to probabilistic baselines and identify regimes where sparse probabilistic CSG is superior, as well as dense regimes where classical least-squares approaches are competitive.

</details>


### [46] [Unifying Proportional Fairness in Centroid and Non-Centroid Clustering](https://arxiv.org/abs/2601.00447)
*Benjamin Cookson,Nisarg Shah,Ziqi Yu*

Main category: cs.GT

TL;DR: 介绍半质心聚类，研究两种比例公平标准，提出算法并得出多种结果


<details>
  <summary>Details</summary>
Motivation: 比例公平标准在聚类文献中受关注，此前研究分质心和非质心两类，本文对其进行扩展

Method: 引入半质心聚类，结合质心和非质心损失，研究核心和FJR两种比例公平标准

Result: 提出新算法在多项式时间对核心实现常数近似，对特殊损失函数和FJR标准有改进结果并建立下界

Conclusion: 所提方法可有效解决半质心聚类中的比例公平问题

Abstract: Proportional fairness criteria inspired by democratic ideals of proportional representation have received growing attention in the clustering literature. Prior work has investigated them in two separate paradigms. Chen et al. [ICML 2019] study centroid clustering, in which each data point's loss is determined by its distance to a representative point (centroid) chosen in its cluster. Caragiannis et al. [NeurIPS 2024] study non-centroid clustering, in which each data point's loss is determined by its maximum distance to any other data point in its cluster.
  We generalize both paradigms to introduce semi-centroid clustering, in which each data point's loss is a combination of its centroid and non-centroid losses, and study two proportional fairness criteria -- the core and, its relaxation, fully justified representation (FJR). Our main result is a novel algorithm which achieves a constant approximation to the core, in polynomial time, even when the distance metrics used for centroid and non-centroid loss measurements are different. We also derive improved results for more restricted loss functions and the weaker FJR criterion, and establish lower bounds in each case.

</details>


### [47] [The CoinAlg Bind: Profitability-Fairness Tradeoffs in Collective Investment Algorithms](https://arxiv.org/abs/2601.00523)
*Andrés Fábrega,James Austgen,Samuel Breckenridge,Jay Yu,Amy Zhao,Sarah Allen,Aditya Saraf,Ari Juels*

Main category: cs.GT

TL;DR: 研究集体投资算法（CoinAlgs）存在盈利能力 - 公平性权衡问题，即CoinAlg困境，通过理论证明和实证研究进行分析。


<details>
  <summary>Details</summary>
Motivation: CoinAlgs旨在让投资者社区共享交易策略，使复杂投资工具民主化，但需解决其存在的公平性和盈利能力问题。

Method: 提出CoinAlgs的形式化模型，定义隐私和经济公平性，进行理论证明；使用Uniswap数据进行实证研究。

Result: 证明隐私是内部人员攻击经济公平性的前提，缺乏隐私会使套利者侵蚀CoinAlgs盈利能力；量化套利对透明CoinAlgs的影响，揭示私有CoinAlgs的信息泄露风险。

Conclusion: CoinAlgs存在无法在保证经济公平性的同时避免套利损失的困境。

Abstract: Collective Investment Algorithms (CoinAlgs) are increasingly popular systems that deploy shared trading strategies for investor communities. Their goal is to democratize sophisticated -- often AI-based -- investing tools. We identify and demonstrate a fundamental profitability-fairness tradeoff in CoinAlgs that we call the CoinAlg Bind: CoinAlgs cannot ensure economic fairness without losing profit to arbitrage. We present a formal model of CoinAlgs, with definitions of privacy (incomplete algorithm disclosure) and economic fairness (value extraction by an adversarial insider). We prove two complementary results that together demonstrate the CoinAlg Bind. First, privacy in a CoinAlg is a precondition for insider attacks on economic fairness. Conversely, in a game-theoretic model, lack of privacy, i.e., transparency, enables arbitrageurs to erode the profitability of a CoinAlg. Using data from Uniswap, a decentralized exchange, we empirically study both sides of the CoinAlg Bind. We quantify the impact of arbitrage against transparent CoinAlgs. We show the risks posed by a private CoinAlg: Even low-bandwidth covert-channel information leakage enables unfair value extraction.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [48] [A Chain-of-Thought Approach to Semantic Query Categorization in e-Commerce Taxonomies](https://arxiv.org/abs/2601.00510)
*Jetlir Duraj,Ishita Khan,Kilian Merkelbach,Mehran Elyasi*

Main category: cs.IR

TL;DR: 本文探讨电商搜索查询分类问题，提出基于Chain-of-Thought范式的方法，性能优于基准方法，并能发现分类法问题，还提出可扩展的大语言模型分类方法。


<details>
  <summary>Details</summary>
Motivation: 电商中准确的查询分类不仅能定位库存空间，还能开启多种意图理解功能，需要实用准确的解决方案。

Method: 探索结合简单树搜索和大语言模型语义评分的Chain-of-Thought范式；提出基于大语言模型的查询分类方法。

Result: Chain-of-Thought方法在分类性能上优于基于嵌入的查询类别预测基准；且该方法能检测分层分类法中的问题。

Conclusion: Chain-of-Thought方法适用于电商搜索查询分类，且提出的大语言模型方法可扩展性良好。

Abstract: Search in e-Commerce is powered at the core by a structured representation of the inventory, often formulated as a category taxonomy. An important capability in e-Commerce with hierarchical taxonomies is to select a set of relevant leaf categories that are semantically aligned with a given user query. In this scope, we address a fundamental problem of search query categorization in real-world e-Commerce taxonomies. A correct categorization of a query not only provides a way to zoom into the correct inventory space, but opens the door to multiple intent understanding capabilities for a query. A practical and accurate solution to this problem has many applications in e-commerce, including constraining retrieved items and improving the relevance of the search results. For this task, we explore a novel Chain-of-Thought (CoT) paradigm that combines simple tree-search with LLM semantic scoring. Assessing its classification performance on human-judged query-category pairs, relevance tests, and LLM-based reference methods, we find that the CoT approach performs better than a benchmark that uses embedding-based query category predictions. We show how the CoT approach can detect problems within a hierarchical taxonomy. Finally, we also propose LLM-based approaches for query-categorization of the same spirit, but which scale better at the range of millions of queries.

</details>


### [49] [Improving Scientific Document Retrieval with Academic Concept Index](https://arxiv.org/abs/2601.00567)
*Jeyun Lee,Junhyoung Lee,Wonbin Kweon,Bowen Jin,Yu Zhang,Susik Yoon,Dongha Lee,Hwanjo Yu,Jiawei Han,Seongku Kang*

Main category: cs.IR

TL;DR: 本文提出学术概念索引改进通用检索器在科学领域的适配，实验表明能提升检索性能。


<details>
  <summary>Details</summary>
Motivation: 通用检索器适配科学领域存在特定标注少、词汇和信息需求不匹配问题，现有方法忽略科学文档中的多样学术概念。

Method: 引入学术概念索引，基于此提出概念覆盖生成（CCQGen）增强合成查询生成，用概念聚焦辅助上下文（CCExpand）强化上下文扩充。

Result: 将学术概念索引融入查询生成和上下文扩充，得到更高质量查询、更好概念对齐和改进的检索性能。

Conclusion: 学术概念索引可有效改进通用检索器在科学领域的适配，提升检索效果。

Abstract: Adapting general-domain retrievers to scientific domains is challenging due to the scarcity of large-scale domain-specific relevance annotations and the substantial mismatch in vocabulary and information needs. Recent approaches address these issues through two independent directions that leverage large language models (LLMs): (1) generating synthetic queries for fine-tuning, and (2) generating auxiliary contexts to support relevance matching. However, both directions overlook the diverse academic concepts embedded within scientific documents, often producing redundant or conceptually narrow queries and contexts. To address this limitation, we introduce an academic concept index, which extracts key concepts from papers and organizes them guided by an academic taxonomy. This structured index serves as a foundation for improving both directions. First, we enhance the synthetic query generation with concept coverage-based generation (CCQGen), which adaptively conditions LLMs on uncovered concepts to generate complementary queries with broader concept coverage. Second, we strengthen the context augmentation with concept-focused auxiliary contexts (CCExpand), which leverages a set of document snippets that serve as concise responses to the concept-aware CCQGen queries. Extensive experiments show that incorporating the academic concept index into both query generation and context augmentation leads to higher-quality queries, better conceptual alignment, and improved retrieval performance.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [50] [Evaluating Anomaly Detectors for Simulated Highly Imbalanced Industrial Classification Problems](https://arxiv.org/abs/2601.00005)
*Lesley Wheat,Martin v. Mohrenschildt,Saeid Habibi*

Main category: cs.LG

TL;DR: 本文用模拟数据集对异常检测算法全面评估，发现最佳检测器取决于训练集中故障示例数量，不同数量故障示例适用不同方法，还指出小数据集上算法泛化性能下降。


<details>
  <summary>Details</summary>
Motivation: 机器学习在工业系统应用面临极端类别不平衡问题，因训练时故障数据有限。

Method: 用反映现实工程约束、基于超球面异常分布的合成数据集，在不同异常率和训练规模下对 14 种检测器进行性能和泛化误差评估。

Result: 最佳检测器高度依赖训练集中故障示例总数；少于 20 个故障示例，无监督方法占优；30 - 50 个时，半监督和监督检测器性能大幅提升；半监督方法在 10 个特征时改进明显。

Conclusion: 指出异常检测方法在小数据集上泛化性能下降，为工业环境部署异常检测提供实用见解。

Abstract: Machine learning offers potential solutions to current issues in industrial systems in areas such as quality control and predictive maintenance, but also faces unique barriers in industrial applications. An ongoing challenge is extreme class imbalance, primarily due to the limited availability of faulty data during training. This paper presents a comprehensive evaluation of anomaly detection algorithms using a problem-agnostic simulated dataset that reflects real-world engineering constraints. Using a synthetic dataset with a hyper-spherical based anomaly distribution in 2D and 10D, we benchmark 14 detectors across training datasets with anomaly rates between 0.05% and 20% and training sizes between 1 000 and 10 000 (with a testing dataset size of 40 000) to assess performance and generalization error. Our findings reveal that the best detector is highly dependant on the total number of faulty examples in the training dataset, with additional healthy examples offering insignificant benefits in most cases. With less than 20 faulty examples, unsupervised methods (kNN/LOF) dominate; but around 30-50 faulty examples, semi-supervised (XGBOD) and supervised (SVM/CatBoost) detectors, we see large performance increases. While semi-supervised methods do not show significant benefits with only two features, the improvements are evident at ten features. The study highlights the performance drop on generalization of anomaly detection methods on smaller datasets, and provides practical insights for deploying anomaly detection in industrial environments.

</details>


### [51] [Yahtzee: Reinforcement Learning Techniques for Stochastic Combinatorial Games](https://arxiv.org/abs/2601.00007)
*Nicholas A. Pape*

Main category: cs.LG

TL;DR: 本文将Yahtzee游戏作为强化学习基准，用多种策略梯度方法训练自玩智能体，A2C表现稳健，智能体得分接近最优，但模型在学习上部分奖励策略有困难。


<details>
  <summary>Details</summary>
Motivation: 单人Yahtzee可通过动态规划计算最优策略，但多人情况难以处理，需近似方法。

Method: 将Yahtzee建模为马尔可夫决策过程，用REINFORCE、A2C、PPO等策略梯度方法训练自玩智能体，消融特征和动作编码、架构等。

Result: 在固定训练预算下，REINFORCE和PPO对超参数敏感，A2C表现稳健，智能体在10万场评估游戏中中位数得分241.78，接近最优DP得分。

Conclusion: 模型在学习上部分奖励策略存在困难，凸显长期信用分配和探索挑战。

Abstract: Yahtzee is a classic dice game with a stochastic, combinatorial structure and delayed rewards, making it an interesting mid-scale RL benchmark. While an optimal policy for solitaire Yahtzee can be computed using dynamic programming methods, multiplayer is intractable, motivating approximation methods. We formulate Yahtzee as a Markov Decision Process (MDP), and train self-play agents using various policy gradient methods: REINFORCE, Advantage Actor-Critic (A2C), and Proximal Policy Optimization (PPO), all using a multi-headed network with a shared trunk. We ablate feature and action encodings, architecture, return estimators, and entropy regularization to understand their impact on learning. Under a fixed training budget, REINFORCE and PPO prove sensitive to hyperparameters and fail to reach near-optimal performance, whereas A2C trains robustly across a range of settings. Our agent attains a median score of 241.78 points over 100,000 evaluation games, within 5.0\% of the optimal DP score of 254.59, achieving the upper section bonus and Yahtzee at rates of 24.9\% and 34.1\%, respectively. All models struggle to learn the upper bonus strategy, overindexing on four-of-a-kind's, highlighting persistent long-horizon credit-assignment and exploration challenges.

</details>


### [52] [The Trojan in the Vocabulary: Stealthy Sabotage of LLM Composition](https://arxiv.org/abs/2601.00065)
*Xiaoze Liu,Weichen Yu,Matt Fredrikson,Xiaoqian Wang,Jing Gao*

Main category: cs.LG

TL;DR: 论文指出大模型生态中tokenizer移植存在供应链漏洞，构造“破坏令牌”进行攻击，凸显模块化AI组合的隐藏风险。


<details>
  <summary>Details</summary>
Motivation: 在不同模型族应用模型组合技术时，tokenizer移植是关键步骤，需研究其可能存在的漏洞。

Method: 构造在捐赠模型中无作用但移植到基础模型可重构为恶意特征的“破坏令牌”，将攻击形式化为双目标优化问题，用稀疏求解器实施攻击。

Result: 攻击无需训练，能实现频谱模仿以躲避异常检测，对微调与权重合并有结构持久性。

Conclusion: modular AI组合的流程存在隐藏风险。

Abstract: The open-weight LLM ecosystem is increasingly defined by model composition techniques (such as weight merging, speculative decoding, and vocabulary expansion) that remix capabilities from diverse sources. A critical prerequisite for applying these methods across different model families is tokenizer transplant, which aligns incompatible vocabularies to a shared embedding space. We demonstrate that this essential interoperability step introduces a supply-chain vulnerability: we engineer a single "breaker token" that is functionally inert in a donor model yet reliably reconstructs into a high-salience malicious feature after transplant into a base model. By exploiting the geometry of coefficient reuse, our attack creates an asymmetric realizability gap that sabotages the base model's generation while leaving the donor's utility statistically indistinguishable from nominal behavior. We formalize this as a dual-objective optimization problem and instantiate the attack using a sparse solver. Empirically, the attack is training-free and achieves spectral mimicry to evade outlier detection, while demonstrating structural persistence against fine-tuning and weight merging, highlighting a hidden risk in the pipeline of modular AI composition. Code is available at https://github.com/xz-liu/tokenforge

</details>


### [53] [Reinforcement-Learned Unequal Error Protection for Quantized Semantic Embeddings](https://arxiv.org/abs/2601.00186)
*Moirangthem Tiken Singh,Adnan Arif*

Main category: cs.LG

TL;DR: 提出新强化学习框架解决有限带宽通信系统语义保存问题，实验有显著增益，挑战传统编码范式，适用于边缘计算和物联网场景。


<details>
  <summary>Details</summary>
Motivation: 解决有限带宽通信系统中保存语义含义的紧迫挑战。

Method: 引入新的强化学习框架，通过自适应重复编码实现维度不等错误保护，使用复合语义失真度量指导智能体分配保护。

Result: 实验显示相比均匀保护有显著增益，在1 dB SNR下chrF分数高6.8%，实体保存好9.3%。

Conclusion: 简单智能分配的重复编码可实现细粒度语义保护，代码结构应与语义粒度对齐，适用于带宽稀缺但语义保真关键的场景。

Abstract: This paper tackles the pressing challenge of preserving semantic meaning in communication systems constrained by limited bandwidth. We introduce a novel reinforcement learning framework that achieves per-dimension unequal error protection via adaptive repetition coding. Central to our approach is a composite semantic distortion metric that balances global embedding similarity with entity-level preservation, empowering the reinforcement learning agent to allocate protection in a context-aware manner. Experiments show statistically significant gains over uniform protection, achieving 6.8% higher chrF scores and 9.3% better entity preservation at 1 dB SNR. The key innovation of our framework is the demonstration that simple, intelligently allocated repetition coding enables fine-grained semantic protection -- an advantage unattainable with conventional codes such as LDPC or Reed-Solomon. Our findings challenge traditional channel coding paradigms by establishing that code structure must align with semantic granularity. This approach is particularly suited to edge computing and IoT scenarios, where bandwidth is scarce, but semantic fidelity is critical, providing a practical pathway for next-generation semantic-aware networks.

</details>


### [54] [IMBWatch -- a Spatio-Temporal Graph Neural Network approach to detect Illicit Massage Business](https://arxiv.org/abs/2601.00075)
*Swetha Varadarajan,Abhishek Ray,Lumina Albert*

Main category: cs.LG

TL;DR: 本文介绍了用于大规模非法按摩场所（IMBs）检测的IMBWatch框架，该框架基于时空图神经网络，实验显示其性能优于基线模型，且具有可解释性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: IMBs隐蔽且难以检测，传统方法反应性强且难以揭示其运营网络，因此需要新的检测方法。

Method: 引入基于时空图神经网络（ST - GNN）的IMBWatch框架，从开源情报构建动态图，结合图卷积运算和时间注意力机制。

Result: 在多个美国城市的真实数据集上实验表明，IMBWatch优于基线模型，获得更高的准确率和F1分数。

Conclusion: IMBWatch不仅性能好，还具有可解释性，提供可操作的见解，具有可扩展性，适用于其他非法领域，且开源支持可重复性研究。

Abstract: Illicit Massage Businesses (IMBs) are a covert and persistent form of organized exploitation that operate under the facade of legitimate wellness services while facilitating human trafficking, sexual exploitation, and coerced labor. Detecting IMBs is difficult due to encoded digital advertisements, frequent changes in personnel and locations, and the reuse of shared infrastructure such as phone numbers and addresses. Traditional approaches, including community tips and regulatory inspections, are largely reactive and ineffective at revealing the broader operational networks traffickers rely on.
  To address these challenges, we introduce IMBWatch, a spatio-temporal graph neural network (ST-GNN) framework for large-scale IMB detection. IMBWatch constructs dynamic graphs from open-source intelligence, including scraped online advertisements, business license records, and crowdsourced reviews. Nodes represent heterogeneous entities such as businesses, aliases, phone numbers, and locations, while edges capture spatio-temporal and relational patterns, including co-location, repeated phone usage, and synchronized advertising. The framework combines graph convolutional operations with temporal attention mechanisms to model the evolution of IMB networks over time and space, capturing patterns such as intercity worker movement, burner phone rotation, and coordinated advertising surges.
  Experiments on real-world datasets from multiple U.S. cities show that IMBWatch outperforms baseline models, achieving higher accuracy and F1 scores. Beyond performance gains, IMBWatch offers improved interpretability, providing actionable insights to support proactive and targeted interventions. The framework is scalable, adaptable to other illicit domains, and released with anonymized data and open-source code to support reproducible research.

</details>


### [55] [Bayesian Inverse Games with High-Dimensional Multi-Modal Observations](https://arxiv.org/abs/2601.00696)
*Yash Jain,Xinjie Liu,Lasse Peters,David Fridovich-Keil,Ufuk Topcu*

Main category: cs.LG

TL;DR: 提出近似贝叶斯推理方法解决逆博弈问题，训练结构化变分自编码器，实验表明该框架能提升推理质量、实现更安全决策。


<details>
  <summary>Details</summary>
Motivation: 现有解决逆博弈问题的最大似然技术只能推断点估计，无法量化估计不确定性，导致下游规划决策可能过度自信而采取不安全行动。

Method: 提出近似贝叶斯推理方法，训练带嵌入式可微纳什博弈求解器的结构化变分自编码器，且无需代理真实目标标签。

Result: 框架成功学习先验和后验分布，相比基于最大似然估计的逆博弈方法提高了推理质量，在不牺牲效率的前提下实现更安全的下游决策，多模态推理在轨迹信息不足时可进一步降低不确定性。

Conclusion: 所提贝叶斯逆博弈框架有效，能提升推理质量和决策安全性。

Abstract: Many multi-agent interaction scenarios can be naturally modeled as noncooperative games, where each agent's decisions depend on others' future actions. However, deploying game-theoretic planners for autonomous decision-making requires a specification of all agents' objectives. To circumvent this practical difficulty, recent work develops maximum likelihood techniques for solving inverse games that can identify unknown agent objectives from interaction data. Unfortunately, these methods only infer point estimates and do not quantify estimator uncertainty; correspondingly, downstream planning decisions can overconfidently commit to unsafe actions. We present an approximate Bayesian inference approach for solving the inverse game problem, which can incorporate observation data from multiple modalities and be used to generate samples from the Bayesian posterior over the hidden agent objectives given limited sensor observations in real time. Concretely, the proposed Bayesian inverse game framework trains a structured variational autoencoder with an embedded differentiable Nash game solver on interaction datasets and does not require labels of agents' true objectives. Extensive experiments show that our framework successfully learns prior and posterior distributions, improves inference quality over maximum likelihood estimation-based inverse game approaches, and enables safer downstream decision-making without sacrificing efficiency. When trajectory information is uninformative or unavailable, multimodal inference further reduces uncertainty by exploiting additional observation modalities.

</details>


### [56] [Exploration in the Limit](https://arxiv.org/abs/2601.00084)
*Brian M. Cho,Nathan Kallus*

Main category: cs.LG

TL;DR: 本文提出一种渐近误差控制的BAI算法，可在非参数设置下灵活利用协变量，实验显示能降低平均样本复杂度并控制误差。


<details>
  <summary>Details</summary>
Motivation: 现有固定置信度最佳臂识别（BAI）算法在实际场景表现不佳，严格误差控制有局限。

Method: 引入渐近误差控制的放松公式，开发渐近任意时刻有效的置信序列，设计新的BAI算法。

Result: 在温和收敛假设下给出样本复杂度渐近界，实验表明降低了平均样本复杂度并控制误差。

Conclusion: 所提方法能在非参数设置下更好地控制误差，匹配高斯BAI在精确误差保证下的最佳样本复杂度。

Abstract: In fixed-confidence best arm identification (BAI), the objective is to quickly identify the optimal option while controlling the probability of error below a desired threshold. Despite the plethora of BAI algorithms, existing methods typically fall short in practical settings, as stringent exact error control requires using loose tail inequalities and/or parametric restrictions. To overcome these limitations, we introduce a relaxed formulation that requires valid error control asymptotically with respect to a minimum sample size. This aligns with many real-world settings that often involve weak signals, high desired significance, and post-experiment inference requirements, all of which necessitate long horizons. This allows us to achieve tighter optimality, while better handling flexible nonparametric outcome distributions and fully leveraging individual-level contexts. We develop a novel asymptotic anytime-valid confidence sequences over arm indices, and we use it to design a new BAI algorithm for our asymptotic framework. Our method flexibly incorporates covariates for variance reduction and ensures approximate error control in fully nonparametric settings. Under mild convergence assumptions, we provide asymptotic bounds on the sample complexity and show the worst-case sample complexity of our approach matches the best-case sample complexity of Gaussian BAI under exact error guarantees and known variances. Experiments suggest our approach reduces average sample complexities while maintaining error control.

</details>


### [57] [TeleDoCTR: Domain-Specific and Contextual Troubleshooting for Telecommunications](https://arxiv.org/abs/2601.00691)
*Mohamed Trabelsi,Huseyin Uzunalioglu*

Main category: cs.LG

TL;DR: 提出TeleDoCTR系统用于提升电信票务故障排查的效率和效果，经评估表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型组织票务故障排查复杂，电信故障排查耗时且依赖人力，影响效率，需提升其有效性和效率。

Method: 提出TeleDoCTR系统，集成特定领域排名和生成模型，自动化故障排查工作流程的关键步骤，包括票务分类、历史票务检索和故障分析报告生成。

Result: 在电信基础设施的真实数据集上评估，TeleDoCTR性能优于现有先进方法。

Conclusion: TeleDoCTR能显著提高故障排查过程的准确性和效率。

Abstract: Ticket troubleshooting refers to the process of analyzing and resolving problems that are reported through a ticketing system. In large organizations offering a wide range of services, this task is highly complex due to the diversity of submitted tickets and the need for specialized domain knowledge. In particular, troubleshooting in telecommunications (telecom) is a very time-consuming task as it requires experts to interpret ticket content, consult documentation, and search historical records to identify appropriate resolutions. This human-intensive approach not only delays issue resolution but also hinders overall operational efficiency. To enhance the effectiveness and efficiency of ticket troubleshooting in telecom, we propose TeleDoCTR, a novel telecom-related, domain-specific, and contextual troubleshooting system tailored for end-to-end ticket resolution in telecom. TeleDoCTR integrates both domain-specific ranking and generative models to automate key steps of the troubleshooting workflow which are: routing tickets to the appropriate expert team responsible for resolving the ticket (classification task), retrieving contextually and semantically similar historical tickets (retrieval task), and generating a detailed fault analysis report outlining the issue, root cause, and potential solutions (generation task). We evaluate TeleDoCTR on a real-world dataset from a telecom infrastructure and demonstrate that it achieves superior performance over existing state-of-the-art methods, significantly enhancing the accuracy and efficiency of the troubleshooting process.

</details>


### [58] [Dynamic Bayesian Optimization Framework for Instruction Tuning in Partial Differential Equation Discovery](https://arxiv.org/abs/2601.00088)
*Junqi Qu,Yan Zhang,Shangqian Gao,Shibo Li*

Main category: cs.LG

TL;DR: 论文指出大语言模型在方程发现中存在指令脆性问题，提出NeuroSymBO方法解决，实验表明自适应指令选择优于固定提示。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在方程发现中输出对提示措辞敏感，静态提示无法适应多步生成过程，导致模型停留在次优解。

Method: 提出NeuroSymBO，将提示工程重构为顺序决策问题，维护离散的推理策略库，使用贝叶斯优化根据数值反馈在每步选择最优指令。

Result: 在PDE发现基准测试中，自适应指令选择显著优于固定提示，实现更高恢复率和更简洁的解决方案。

Conclusion: 自适应指令选择在方程发现中效果更好，能解决大语言模型的指令脆性问题。

Abstract: Large Language Models (LLMs) show promise for equation discovery, yet their outputs are highly sensitive to prompt phrasing, a phenomenon we term instruction brittleness. Static prompts cannot adapt to the evolving state of a multi-step generation process, causing models to plateau at suboptimal solutions. To address this, we propose NeuroSymBO, which reframes prompt engineering as a sequential decision problem. Our method maintains a discrete library of reasoning strategies and uses Bayesian Optimization to select the optimal instruction at each step based on numerical feedback. Experiments on PDE discovery benchmarks show that adaptive instruction selection significantly outperforms fixed prompts, achieving higher recovery rates with more parsimonious solutions.

</details>


### [59] [Comparative Evaluation of Embedding Representations for Financial News Sentiment Analysis](https://arxiv.org/abs/2512.13749)
*Joyjit Roy,Samaresh Kumar Singh*

Main category: cs.LG

TL;DR: 研究在资源受限环境下基于嵌入的金融新闻情感分类方法，发现预训练嵌入在数据不足时效果不佳，建议从业者考虑替代方法。


<details>
  <summary>Details</summary>
Motivation: 标准自然语言处理方法在小数据集上应用于金融情感分析有挑战，需评估资源受限环境下基于嵌入的金融新闻情感分类方法。

Method: 结合梯度提升，对Word2Vec、GloVe和句子转换器表示在手动标注的新闻标题上进行评估。

Result: 验证集和测试集性能有显著差距，模型表现不如简单基线，预训练嵌入在数据充足临界值以下收益递减，小验证集会导致模型选择时过拟合。

Conclusion: 仅嵌入质量无法解决情感分类中的数据稀缺问题，资源有限的从业者在样本稀缺时应考虑替代方法。

Abstract: Financial sentiment analysis enhances market understanding; however, standard natural language processing approaches encounter significant challenges when applied to small datasets. This study provides a comparative evaluation of embedding-based methods for financial news sentiment classification in resource-constrained environments. Word2Vec, GloVe, and sentence transformer representations are evaluated in combination with gradient boosting on manually labeled headlines. Experimental results identify a substantial gap between validation and test performance, with models performing worse than trivial baselines despite strong validation metrics. The analysis demonstrates that pretrained embeddings yield diminishing returns below a critical data sufficiency threshold, and that small validation sets contribute to overfitting during model selection. Practical application is illustrated through weekly sentiment aggregation and narrative summarization for market monitoring workflows. The findings offer empirical evidence that embedding quality alone cannot address fundamental data scarcity in sentiment classification. For practitioners operating with limited resources, the results indicate the need to consider alternative approaches such as few-shot learning, data augmentation, or lexicon-enhanced hybrid methods when labeled samples are scarce.

</details>


### [60] [GRL-SNAM: Geometric Reinforcement Learning with Path Differential Hamiltonians for Simultaneous Navigation and Mapping in Unknown Environments](https://arxiv.org/abs/2601.00116)
*Aditya Sai Ellendula,Yi Wang,Minh Nguyen,Chandrajit Bajaj*

Main category: cs.LG

TL;DR: 提出GRL - SNAM框架用于未知环境同时导航与建图，依靠局部感知，通过更新哈密顿量优化路径，在2D任务评估中表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决未知环境下同时导航与建图问题，现有方法需设计多智能体策略且要构建全局地图，有局限性。

Method: 将路径导航和建图表述为动态最短路径搜索过程，用受控哈密顿优化，依靠局部感知，通过更新哈密顿量更新策略。

Result: 在两个2D导航任务评估中，相比基线和参考方法，能保持间隙、泛化到未见布局，通过局部能量优化实现高质量导航。

Conclusion: 基于更新哈密顿量的几何强化学习可通过局部能量优化而非广泛全局建图实现高质量导航，代码开源。

Abstract: We present GRL-SNAM, a geometric reinforcement learning framework for Simultaneous Navigation and Mapping(SNAM) in unknown environments. A SNAM problem is challenging as it needs to design hierarchical or joint policies of multiple agents that control the movement of a real-life robot towards the goal in mapless environment, i.e. an environment where the map of the environment is not available apriori, and needs to be acquired through sensors. The sensors are invoked from the path learner, i.e. navigator, through active query responses to sensory agents, and along the motion path. GRL-SNAM differs from preemptive navigation algorithms and other reinforcement learning methods by relying exclusively on local sensory observations without constructing a global map. Our approach formulates path navigation and mapping as a dynamic shortest path search and discovery process using controlled Hamiltonian optimization: sensory inputs are translated into local energy landscapes that encode reachability, obstacle barriers, and deformation constraints, while policies for sensing, planning, and reconfiguration evolve stagewise via updating Hamiltonians. A reduced Hamiltonian serves as an adaptive score function, updating kinetic/potential terms, embedding barrier constraints, and continuously refining trajectories as new local information arrives. We evaluate GRL-SNAM on two different 2D navigation tasks. Comparing against local reactive baselines and global policy learning references under identical stagewise sensing constraints, it preserves clearance, generalizes to unseen layouts, and demonstrates that Geometric RL learning via updating Hamiltonians enables high-quality navigation through minimal exploration via local energy refinement rather than extensive global mapping. The code is publicly available on \href{https://github.com/CVC-Lab/GRL-SNAM}{Github}.

</details>


### [61] [Laplacian Kernelized Bandit](https://arxiv.org/abs/2601.00461)
*Shuang Wu,Arash A. Amini*

Main category: cs.LG

TL;DR: 研究多用户上下文老虎机问题，引入联合惩罚项，证明其等价性，推导再生核，设计算法，给出后悔界，实验表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决多用户上下文老虎机中用户奖励函数存在非线性和图同质性的问题。

Method: 引入联合惩罚项，证明其与多用户RKHS平方范数等价，推导再生核，设计LK - GP - UCB和LK - GP - TS算法。

Result: 给出高概率后悔界，实验中方法在非线性设置下优于线性和非图感知基线，线性奖励时也有竞争力。

Conclusion: 提供了一个统一、理论可靠且实用的框架，连接拉普拉斯正则化和核老虎机进行结构化探索。

Abstract: We study multi-user contextual bandits where users are related by a graph and their reward functions exhibit both non-linear behavior and graph homophily. We introduce a principled joint penalty for the collection of user reward functions $\{f_u\}$, combining a graph smoothness term based on RKHS distances with an individual roughness penalty. Our central contribution is proving that this penalty is equivalent to the squared norm within a single, unified \emph{multi-user RKHS}. We explicitly derive its reproducing kernel, which elegantly fuses the graph Laplacian with the base arm kernel. This unification allows us to reframe the problem as learning a single ''lifted'' function, enabling the design of principled algorithms, \texttt{LK-GP-UCB} and \texttt{LK-GP-TS}, that leverage Gaussian Process posteriors over this new kernel for exploration. We provide high-probability regret bounds that scale with an \emph{effective dimension} of the multi-user kernel, replacing dependencies on user count or ambient dimension. Empirically, our methods outperform strong linear and non-graph-aware baselines in non-linear settings and remain competitive even when the true rewards are linear. Our work delivers a unified, theoretically grounded, and practical framework that bridges Laplacian regularization with kernelized bandits for structured exploration.

</details>


### [62] [Reinforcement Learning with Function Approximation for Non-Markov Processes](https://arxiv.org/abs/2601.00151)
*Ali Devran Kara*

Main category: cs.LG

TL;DR: 研究非马尔可夫状态和成本过程下带线性函数逼近的强化学习方法，分析策略评估和Q学习收敛性并应用于部分可观测马尔可夫决策过程。


<details>
  <summary>Details</summary>
Motivation: 研究非马尔可夫状态和成本过程下带线性函数逼近的强化学习方法的收敛性。

Method: 分析策略评估方法在合适遍历条件下的收敛性，探讨Q学习在特定基函数选择下的收敛性，将结果应用于部分可观测马尔可夫决策过程。

Result: 策略评估算法在合适遍历条件下收敛，Q学习在特定基函数选择下收敛，得出部分可观测马尔可夫决策过程学习算法极限的显式误差界。

Conclusion: 给出非马尔可夫状态和成本过程下带线性函数逼近的强化学习方法的收敛性结果及应用。

Abstract: We study reinforcement learning methods with linear function approximation under non-Markov state and cost processes. We first consider the policy evaluation method and show that the algorithm converges under suitable ergodicity conditions on the underlying non-Markov processes. Furthermore, we show that the limit corresponds to the fixed point of a joint operator composed of an orthogonal projection and the Bellman operator of an auxiliary \emph{Markov} decision process.
  For Q-learning with linear function approximation, as in the Markov setting, convergence is not guaranteed in general. We show, however, that for the special case where the basis functions are chosen based on quantization maps, the convergence can be shown under similar ergodicity conditions. Finally, we apply our results to partially observed Markov decision processes, where finite-memory variables are used as state representations, and we derive explicit error bounds for the limits of the resulting learning algorithms.

</details>


### [63] [Federated Customization of Large Models: Approaches, Experiments, and Insights](https://arxiv.org/abs/2601.00526)
*Yuchuan Ye,Ming Ding,Youjia Chen,Peng Cheng,Dusit Niyato*

Main category: cs.LG

TL;DR: 文章探讨大模型的联邦定制，回顾定制技术，在联邦学习框架下实验联邦前缀调优，验证其可行性和性能。


<details>
  <summary>Details</summary>
Motivation: 探索大模型在联邦学习框架下定制面临的关键挑战。

Method: 回顾多种大模型定制技术，在联邦学习框架下实现这些技术，并进行联邦前缀调优实验。

Result: 实验验证了联邦前缀调优的可行性，性能接近集中式方法，与其他三种联邦定制方法相比有竞争力、效率高且鲁棒性一致。

Conclusion: 联邦前缀调优在联邦学习框架下是可行的，具有良好性能。

Abstract: In this article, we explore federated customization of large models and highlight the key challenges it poses within the federated learning framework. We review several popular large model customization techniques, including full fine-tuning, efficient fine-tuning, prompt engineering, prefix-tuning, knowledge distillation, and retrieval-augmented generation. Then, we discuss how these techniques can be implemented within the federated learning framework. Moreover, we conduct experiments on federated prefix-tuning, which, to the best of our knowledge, is the first trial to apply prefix-tuning in the federated learning setting. The conducted experiments validate its feasibility with performance close to centralized approaches. Further comparison with three other federated customization methods demonstrated its competitive performance, satisfactory efficiency, and consistent robustness.

</details>


### [64] [Sequential Reservoir Computing for Efficient High-Dimensional Spatiotemporal Forecasting](https://arxiv.org/abs/2601.00172)
*Ata Akbari Asanjan,Filip Wudarski,Daniel O'Connor,Shaun Geaney,Elena Strbac,P. Aaron Lott,Davide Venturelli*

Main category: cs.LG

TL;DR: 提出顺序储层计算（Sequential RC）架构，在高维时空系统预测上比LSTM和标准RNN更优，成本更低，可用于实时节能预测。


<details>
  <summary>Details</summary>
Motivation: RNN和LSTM在高维时空系统预测有计算挑战，传统RC架构输入维度扩展性差。

Method: 将大储层分解为一系列相互连接的小储层的顺序储层计算（Sequential RC）架构。

Result: 在低维混沌系统和高维物理模拟中，Sequential RC预测有效时长增加15 - 25%，误差指标降低20 - 30%，训练成本最多低三个数量级。

Conclusion: Sequential RC保持传统RC简单高效，在高维动力系统中扩展性好，为科学工程应用实时节能预测提供实用途径。

Abstract: Forecasting high-dimensional spatiotemporal systems remains computationally challenging for recurrent neural networks (RNNs) and long short-term memory (LSTM) models due to gradient-based training and memory bottlenecks. Reservoir Computing (RC) mitigates these challenges by replacing backpropagation with fixed recurrent layers and a convex readout optimization, yet conventional RC architectures still scale poorly with input dimensionality. We introduce a Sequential Reservoir Computing (Sequential RC) architecture that decomposes a large reservoir into a series of smaller, interconnected reservoirs. This design reduces memory and computational costs while preserving long-term temporal dependencies. Using both low-dimensional chaotic systems (Lorenz63) and high-dimensional physical simulations (2D vorticity and shallow-water equations), Sequential RC achieves 15-25% longer valid forecast horizons, 20-30% lower error metrics (SSIM, RMSE), and up to three orders of magnitude lower training cost compared to LSTM and standard RNN baselines. The results demonstrate that Sequential RC maintains the simplicity and efficiency of conventional RC while achieving superior scalability for high-dimensional dynamical systems. This approach provides a practical path toward real-time, energy-efficient forecasting in scientific and engineering applications.

</details>


### [65] [The Weather Paradox: Why Precipitation Fails to Predict Traffic Accident Severity in Large-Scale US Data](https://arxiv.org/abs/2601.00152)
*Yann Bellec,Rohan Kaman,Siwen Cui,Aarav Agrawal,Calvin Chen*

Main category: cs.LG

TL;DR: 研究美国交通事故严重程度预测，用XGBoost模型，精度78%，发现部分预测因素，指出数据局限及改进方向。


<details>
  <summary>Details</summary>
Motivation: 探究环境、时间和空间因素对美国交通事故严重程度的预测能力。

Method: 使用2016 - 2023年50万起美国交通事故数据集，训练经随机搜索交叉验证优化的XGBoost分类器，通过类加权调整类别不平衡。

Result: 模型总体准确率78%，对多数类别（严重程度2）表现好，特征重要性分析显示一天中的时间、地理位置和天气相关变量是强预测因素，降水和能见度预测能力有限。

Conclusion: 研究结果有助于循证交通管理，指出需采用替代采样策略、加强特征工程和整合外部数据集，为严重程度预测研究提供方向。

Abstract: This study investigates the predictive capacity of environmental, temporal, and spatial factors on traffic accident severity in the United States. Using a dataset of 500,000 U.S. traffic accidents spanning 2016-2023, we trained an XGBoost classifier optimized through randomized search cross-validation and adjusted for class imbalance via class weighting. The final model achieves an overall accuracy of 78%, with strong performance on the majority class (Severity 2), attaining 87% precision and recall. Feature importance analysis reveals that time of day, geographic location, and weather-related variables, including visibility, temperature, and wind speed, rank among the strongest predictors of accident severity. However, contrary to initial hypotheses, precipitation and visibility demonstrate limited predictive power, potentially reflecting behavioral adaptation by drivers under overtly hazardous conditions. The dataset's predominance of mid-level severity accidents constrains the model's capacity to learn meaningful patterns for extreme cases, highlighting the need for alternative sampling strategies, enhanced feature engineering, and integration of external datasets. These findings contribute to evidence-based traffic management and suggest future directions for severity prediction research.

</details>


### [66] [Online Finetuning Decision Transformers with Pure RL Gradients](https://arxiv.org/abs/2601.00167)
*Junkai Luo,Yinglun Zhu*

Main category: cs.LG

TL;DR: 本文指出在线决策变换器中后视回报重标记阻碍纯强化学习微调，提出新算法用纯强化学习梯度进行在线微调，实验显示方法优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法在在线微调决策变换器时依赖监督序列建模目标，后视回报重标记与基于重要性采样的强化学习算法不兼容，导致训练不稳定，需探索用纯强化学习梯度进行在线微调。

Method: 将GRPO算法应用于决策变换器，进行子轨迹优化、引入序列级似然目标、采用主动采样。

Result: 方法在多个基准测试中优于现有在线决策变换器基线，取得新的最优性能。

Conclusion: 纯强化学习的在线微调对决策变换器有效。

Abstract: Decision Transformers (DTs) have emerged as a powerful framework for sequential decision making by formulating offline reinforcement learning (RL) as a sequence modeling problem. However, extending DTs to online settings with pure RL gradients remains largely unexplored, as existing approaches continue to rely heavily on supervised sequence-modeling objectives during online finetuning. We identify hindsight return relabeling -- a standard component in online DTs -- as a critical obstacle to RL-based finetuning: while beneficial for supervised learning, it is fundamentally incompatible with importance sampling-based RL algorithms such as GRPO, leading to unstable training. Building on this insight, we propose new algorithms that enable online finetuning of Decision Transformers using pure reinforcement learning gradients. We adapt GRPO to DTs and introduce several key modifications, including sub-trajectory optimization for improved credit assignment, sequence-level likelihood objectives for enhanced stability and efficiency, and active sampling to encourage exploration in uncertain regions. Through extensive experiments, we demonstrate that our methods outperform existing online DT baselines and achieve new state-of-the-art performance across multiple benchmarks, highlighting the effectiveness of pure-RL-based online finetuning for Decision Transformers.

</details>


### [67] [Categorical Reparameterization with Denoising Diffusion models](https://arxiv.org/abs/2601.00781)
*Samson Gourevitch,Alain Durmus,Eric Moulines,Jimmy Olsson,Yazid Janati*

Main category: cs.LG

TL;DR: 本文为类别分布引入基于扩散的软重参数化方法，实验表明该方法在多个基准测试上有竞争力或性能提升。


<details>
  <summary>Details</summary>
Motivation: 基于梯度的类别变量优化中，现有方法存在无偏但有噪声或有偏且依赖温度的问题，需要改进。

Method: 引入基于扩散的软重参数化方法，利用高斯噪声过程下的去噪器的闭式解实现免训练扩散采样器并进行反向传播。

Result: 所提重参数化技巧在多个基准测试上取得有竞争力或改进的优化性能。

Conclusion: 提出的基于扩散的软重参数化方法可有效用于类别分布的优化。

Abstract: Gradient-based optimization with categorical variables typically relies on score-function estimators, which are unbiased but noisy, or on continuous relaxations that replace the discrete distribution with a smooth surrogate admitting a pathwise (reparameterized) gradient, at the cost of optimizing a biased, temperature-dependent objective. In this paper, we extend this family of relaxations by introducing a diffusion-based soft reparameterization for categorical distributions. For these distributions, the denoiser under a Gaussian noising process admits a closed form and can be computed efficiently, yielding a training-free diffusion sampler through which we can backpropagate. Our experiments show that the proposed reparameterization trick yields competitive or improved optimization performance on various benchmarks.

</details>


### [68] [Early Prediction of Liver Cirrhosis Up to Three Years in Advance: A Machine Learning Study Benchmarking Against the FIB-4 Score](https://arxiv.org/abs/2601.00175)
*Zhuqi Miao,Sujan Ravi,Abdulaziz Ahmed*

Main category: cs.LG

TL;DR: 此研究用电子病历数据开发并评估机器学习模型预测肝硬化，模型表现优于FIB - 4评分。


<details>
  <summary>Details</summary>
Motivation: 开发机器学习模型，利用电子病历数据提前一至三年预测肝硬化，并与FIB - 4评分对比。

Method: 开展回顾性队列研究，用学术医疗系统的电子病历数据，按ICD - 9/10代码分类，构建预测场景，训练XGBoost模型并在测试集评估，用AUC对比模型与FIB - 4评分。

Result: 最终各预测年的患者数不同，机器学习模型在各预测窗口均优于FIB - 4评分，XGBoost模型在不同预测年的AUC分别为0.81、0.73、0.69，FIB - 4评分为0.71、0.63、0.57。

Conclusion: 机器学习模型在早期预测肝硬化方面显著优于FIB - 4评分，可进行风险分层并用于临床预防和管理。

Abstract: Objective: Develop and evaluate machine learning (ML) models for predicting incident liver cirrhosis one, two, and three years prior to diagnosis using routinely collected electronic health record (EHR) data, and to benchmark their performance against the FIB-4 score. Methods: We conducted a retrospective cohort study using de-identified EHR data from a large academic health system. Patients with fatty liver disease were identified and categorized into cirrhosis and non-cirrhosis cohorts based on ICD-9/10 codes. Prediction scenarios were constructed using observation and prediction windows to emulate real-world clinical use. Demographics, diagnoses, laboratory results, vital signs, and comorbidity indices were aggregated from the observation window. XGBoost models were trained for 1-, 2-, and 3-year prediction horizons and evaluated on held-out test sets. Model performance was compared with FIB-4 using area under the receiver operating characteristic curve (AUC). Results: Final cohorts included 3,043 patients for the 1-year prediction, 1,981 for the 2-year prediction, and 1,470 for the 3-year prediction. Across all prediction windows, ML models consistently outperformed FIB-4. The XGBoost models achieved AUCs of 0.81, 0.73, and 0.69 for 1-, 2-, and 3-year predictions, respectively, compared with 0.71, 0.63, and 0.57 for FIB-4. Performance gains persisted with longer prediction horizons, indicating improved early risk discrimination. Conclusions: Machine learning models leveraging routine EHR data substantially outperform the traditional FIB-4 score for early prediction of liver cirrhosis. These models enable earlier and more accurate risk stratification and can be integrated into clinical workflows as automated decision-support tools to support proactive cirrhosis prevention and management.

</details>


### [69] [SSI-GAN: Semi-Supervised Swin-Inspired Generative Adversarial Networks for Neuronal Spike Classification](https://arxiv.org/abs/2601.00189)
*Danial Sharifrazi,Nouman Javed,Mojtaba Mohammadi,Seyede Sana Salehi,Roohallah Alizadehsani,Prasad N. Paradkar,U. Rajendra Acharya,Asim Bhatti*

Main category: cs.LG

TL;DR: 提出半监督Swin启发的GAN（SSI - GAN）解决蚊子神经元尖峰模式分类中标记数据稀缺问题，用少量标记数据实现高分类准确率并减少人工标记工作量。


<details>
  <summary>Details</summary>
Motivation: 手动分类蚊子神经元尖峰模式劳神且昂贵，现有深度学习解决方案需全标记尖峰数据集和高度预处理的神经元信号，不利于实际场景广泛应用，要解决标记数据稀缺问题。

Method: 提出SSI - GAN架构，用Swin启发的移位窗口鉴别器和基于变压器的生成器对神经元尖峰序列分类以检测病毒嗜神经性，用多头自注意力模型，搭配贝叶斯Optuna框架优化超参数和五重蒙特卡罗交叉验证。

Result: SSI - GAN在感染后第三天使用仅3%标记数据分类准确率达99.93%，用1%监督在感染各阶段保持高精度，相比标准监督方法手动标记工作量减少97 - 99%，移位窗口变压器设计大幅超越所有基线。

Conclusion: SSI - GAN在基于尖峰的神经元感染分类中表现出色，能在少量标记数据下实现高效分类，降低人工标记成本。

Abstract: Mosquitos are the main transmissive agents of arboviral diseases. Manual classification of their neuronal spike patterns is very labor-intensive and expensive. Most available deep learning solutions require fully labeled spike datasets and highly preprocessed neuronal signals. This reduces the feasibility of mass adoption in actual field scenarios. To address the scarcity of labeled data problems, we propose a new Generative Adversarial Network (GAN) architecture that we call the Semi-supervised Swin-Inspired GAN (SSI-GAN). The Swin-inspired, shifted-window discriminator, together with a transformer-based generator, is used to classify neuronal spike trains and, consequently, detect viral neurotropism. We use a multi-head self-attention model in a flat, window-based transformer discriminator that learns to capture sparser high-frequency spike features. Using just 1 to 3% labeled data, SSI-GAN was trained with more than 15 million spike samples collected at five-time post-infection and recording classification into Zika-infected, dengue-infected, or uninfected categories. Hyperparameters were optimized using the Bayesian Optuna framework, and performance for robustness was validated under fivefold Monte Carlo cross-validation. SSI-GAN reached 99.93% classification accuracy on the third day post-infection with only 3% labeled data. It maintained high accuracy across all stages of infection with just 1% supervision. This shows a 97-99% reduction in manual labeling effort relative to standard supervised approaches at the same performance level. The shifted-window transformer design proposed here beat all baselines by a wide margin and set new best marks in spike-based neuronal infection classification.

</details>


### [70] [Optimized Hybrid Feature Engineering for Resource-Efficient Arrhythmia Detection in ECG Signals: An Optimization Framework](https://arxiv.org/abs/2601.00192)
*Moirangthem Tiken Singh,Manibhushan Yaikhom*

Main category: cs.LG

TL;DR: 提出资源高效、以数据为中心框架处理心律失常数据，在数据集验证效果好，效率提升显著。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病尤其是心律失常需IoMT持续监测，但现有深度学习方法计算开销大，不适用于资源受限边缘设备。

Method: 提出资源高效、以数据为中心框架，结合时频小波分解和图论结构描述符，用互信息和递归消除优化特征空间，使用可解释的超轻量级线性分类器。

Result: 在MIT - BIH和INCART数据集上诊断准确率达98.44%，模型占用8.54 KB，分类推理延迟0.46 μs，每拍管道耗时52 ms。

Conclusion: 相比压缩模型有数量级的效率提升，有助于推动无电池心脏传感器发展。

Abstract: Cardiovascular diseases, particularly arrhythmias, remain a leading global cause of mortality, necessitating continuous monitoring via the Internet of Medical Things (IoMT). However, state-of-the-art deep learning approaches often impose prohibitive computational overheads, rendering them unsuitable for resource-constrained edge devices. This study proposes a resource-efficient, data-centric framework that prioritizes feature engineering over complexity. Our optimized pipeline makes the complex, high-dimensional arrhythmia data linearly separable. This is achieved by integrating time-frequency wavelet decompositions with graph-theoretic structural descriptors, such as PageRank centrality. This hybrid feature space, combining wavelet decompositions and graph-theoretic descriptors, is then refined using mutual information and recursive elimination, enabling interpretable, ultra-lightweight linear classifiers. Validation on the MIT-BIH and INCART datasets yields 98.44% diagnostic accuracy with an 8.54 KB model footprint. The system achieves 0.46 $μ$s classification inference latency within a 52 ms per-beat pipeline, ensuring real-time operation. These outcomes provide an order-of-magnitude efficiency gain over compressed models, such as KD-Light (25 KB, 96.32% accuracy), advancing battery-less cardiac sensors.

</details>


### [71] [Unknown Aware AI-Generated Content Attribution](https://arxiv.org/abs/2601.00218)
*Ellie Thieu,Jifan Zhang,Haoyue Bai*

Main category: cs.LG

TL;DR: 研究目标生成模型输出归属问题，用CLIP特征和线性分类器建基线，针对基线不足提出利用无标签野生数据的约束优化方法，实验表明该方法可提升开放环境下内容归属性能。


<details>
  <summary>Details</summary>
Motivation: 随着逼真生成模型发展，需识别合成内容的具体生成模型，而非仅做真假检测。

Method: 先用CLIP特征和线性分类器建立基线，再提出利用无标签野生数据的约束优化方法，使野生样本被分类为非目标，同时保证对有标签数据的高分类性能。

Result: 结合野生数据显著提高了对未知生成器的归属性能。

Conclusion: 开放环境下可有效利用野生无标签数据增强AI生成内容的归属性能。

Abstract: The rapid advancement of photorealistic generative models has made it increasingly important to attribute the origin of synthetic content, moving beyond binary real or fake detection toward identifying the specific model that produced a given image. We study the problem of distinguishing outputs from a target generative model (e.g., OpenAI Dalle 3) from other sources, including real images and images generated by a wide range of alternative models. Using CLIP features and a simple linear classifier, shown to be effective in prior work, we establish a strong baseline for target generator attribution using only limited labeled data from the target model and a small number of known generators. However, this baseline struggles to generalize to harder, unseen, and newly released generators. To address this limitation, we propose a constrained optimization approach that leverages unlabeled wild data, consisting of images collected from the Internet that may include real images, outputs from unknown generators, or even samples from the target model itself. The proposed method encourages wild samples to be classified as non target while explicitly constraining performance on labeled data to remain high. Experimental results show that incorporating wild data substantially improves attribution performance on challenging unseen generators, demonstrating that unlabeled data from the wild can be effectively exploited to enhance AI generated content attribution in open world settings.

</details>


### [72] [Robust Graph Fine-Tuning with Adversarial Graph Prompting](https://arxiv.org/abs/2601.00229)
*Ziyan Zhang,Bo Jiang,Jin Tang*

Main category: cs.LG

TL;DR: 提出对抗图提示（AGP）框架实现鲁棒图微调，证明理论有效性并实验验证其优势。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法对图拓扑和节点属性的噪声及攻击脆弱，为解决该问题提出AGP框架。

Method: 将AGP作为最小 - 最大优化问题，用交替优化方案求解，内最大化用JointPGD算法生成对抗噪声，外最小化学习节点提示抵消噪声；证明AGP理论上可处理拓扑和节点噪声。

Result: 广泛实验表明，AGP方法相比现有方法在多个基准任务中更具鲁棒性和有效性。

Conclusion: AGP是通用方法，可集成到多种预训练GNN模型，增强下游任务鲁棒性。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) method has emerged as a dominant paradigm for adapting pre-trained GNN models to downstream tasks. However, existing PEFT methods usually exhibit significant vulnerability to various noise and attacks on graph topology and node attributes/features. To address this issue, for the first time, we propose integrating adversarial learning into graph prompting and develop a novel Adversarial Graph Prompting (AGP) framework to achieve robust graph fine-tuning. Our AGP has two key aspects. First, we propose the general problem formulation of AGP as a min-max optimization problem and develop an alternating optimization scheme to solve it. For inner maximization, we propose Joint Projected Gradient Descent (JointPGD) algorithm to generate strong adversarial noise. For outer minimization, we employ a simple yet effective module to learn the optimal node prompts to counteract the adversarial noise. Second, we demonstrate that the proposed AGP can theoretically address both graph topology and node noise. This confirms the versatility and robustness of our AGP fine-tuning method across various graph noise. Note that, the proposed AGP is a general method that can be integrated with various pre-trained GNN models to enhance their robustness on the downstream tasks. Extensive experiments on multiple benchmark tasks validate the robustness and effectiveness of AGP method compared to state-of-the-art methods.

</details>


### [73] [GRIT -- Geometry-Aware PEFT with K-FACPreconditioning, Fisher-Guided Reprojection, andDynamic Rank Adaptation](https://arxiv.org/abs/2601.00231)
*Pritish Saha,Chandrav Rajbangshi,Rudra Goyal,Mohit Goyal,Anurag Deo,Biswajit Roy,Ningthoujam Dhanachandra Singh,Raxit Goswami,Amitava Das*

Main category: cs.LG

TL;DR: 提出曲率感知LoRA程序GRIT，能降低可训练参数，表现超LoRA和QLoRA且无质量损失，在模型遗忘方面表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有的LoRA和QLoRA在微调大语言模型时大多无视局部损失曲率，会增加有效更新预算并放大漂移。

Method: 引入GRIT动态曲率感知LoRA程序，在秩空间用K - FAC预处理梯度，定期重新投影低秩基，自适应有效秩。

Result: 在多个基准测试中，GRIT表现匹配或超越LoRA和QLoRA，平均减少46%可训练参数，不同提示和数据下无质量损失，漂移更低、更新 - 保留边界更好。

Conclusion: GRIT是一种更优的参数高效微调方法。

Abstract: Parameter-efficient fine-tuning (PEFT) is the default way to adapt LLMs, but widely used LoRA and QLoRA are largely geometry-agnostic: they optimize in fixed, randomly oriented low-rank subspaces with first-order descent, mostly ignoring local loss curvature. This can inflate the effective update budget and amplify drift along weakly constrained directions. We introduce GRIT, a dynamic, curvature-aware LoRA procedure that preserves the LoRA parameterization but: (1) preconditions gradients in rank space using K-FAC as a natural-gradient proxy; (2) periodically reprojects the low-rank basis onto dominant Fisher eigendirections to suppress drift; and (3) adapts the effective rank from the spectrum so capacity concentrates where signal resides. Across instruction-following, comprehension, and reasoning benchmarks on LLaMA backbones, GRIT matches or surpasses LoRA and QLoRA while reducing trainable parameters by 46% on average (25--80% across tasks), without practical quality loss across prompt styles and data mixes. To model forgetting, we fit a curvature-modulated power law. Empirically, GRIT yields lower drift and a better updates-vs-retention frontier than strong PEFT-optimizer baselines (Orthogonal-LoRA, IA3, DoRA, Eff-FT, Shampoo).

</details>


### [74] [Task-Driven Kernel Flows: Label Rank Compression and Laplacian Spectral Filtering](https://arxiv.org/abs/2601.00276)
*Hongxi Li,Chunlin Huang*

Main category: cs.LG

TL;DR: 提出宽L2正则化网络特征学习理论，表明监督学习具压缩性，推导核ODE，证明稳态核秩受类别数限制，SGD噪声为低秩，统一对齐的确定和随机观点，对比监督与自监督学习特征性质。


<details>
  <summary>Details</summary>
Motivation: 研究宽L2正则化网络中的特征学习理论，理解监督学习的本质特性。

Method: 推导核ODE来预测光谱演化，进行理论证明。

Result: 证明稳态下核秩受类别数C限制，SGD噪声为低秩O(C)，将动力学限制在任务相关子空间。

Conclusion: 该框架统一了对齐的确定和随机观点，对比出监督学习低秩与自监督学习高秩、扩展表示的差异。

Abstract: We present a theory of feature learning in wide L2-regularized networks showing that supervised learning is inherently compressive. We derive a kernel ODE that predicts a "water-filling" spectral evolution and prove that for any stable steady state, the kernel rank is bounded by the number of classes ($C$). We further demonstrate that SGD noise is similarly low-rank ($O(C)$), confining dynamics to the task-relevant subspace. This framework unifies the deterministic and stochastic views of alignment and contrasts the low-rank nature of supervised learning with the high-rank, expansive representations of self-supervision.

</details>


### [75] [Can Optimal Transport Improve Federated Inverse Reinforcement Learning?](https://arxiv.org/abs/2601.00309)
*David Millard,Ali Baheri*

Main category: cs.LG

TL;DR: 提出基于最优传输的联邦逆强化学习方法，可在不同环境和代理间推导共享奖励。


<details>
  <summary>Details</summary>
Motivation: 在机器人和多智能体系统中，直接合并数据学习共享奖励函数因动力学差异、隐私约束和通信带宽限制而不实际。

Method: 各客户端先进行局部轻量级最大熵逆强化学习，再通过Wasserstein重心融合奖励函数。

Result: 证明重心融合比传统联邦学习参数平均方法能得到更可靠的全局奖励估计。

Conclusion: 提供了一个有原则且通信高效的框架，可在异构代理和环境间推导共享奖励。

Abstract: In robotics and multi-agent systems, fleets of autonomous agents often operate in subtly different environments while pursuing a common high-level objective. Directly pooling their data to learn a shared reward function is typically impractical due to differences in dynamics, privacy constraints, and limited communication bandwidth. This paper introduces an optimal transport-based approach to federated inverse reinforcement learning (IRL). Each client first performs lightweight Maximum Entropy IRL locally, adhering to its computational and privacy limitations. The resulting reward functions are then fused via a Wasserstein barycenter, which considers their underlying geometric structure. We further prove that this barycentric fusion yields a more faithful global reward estimate than conventional parameter averaging methods in federated learning. Overall, this work provides a principled and communication-efficient framework for deriving a shared reward that generalizes across heterogeneous agents and environments.

</details>


### [76] [Quantum King-Ring Domination in Chess: A QAOA Approach](https://arxiv.org/abs/2601.00318)
*Gerhard Stenzel,Michael Kölle,Tobias Rohe,Julian Hager,Leo Sünkel,Maximilian Zorn,Claudia Linnhoff-Popien*

Main category: cs.LG

TL;DR: 引入量子国王环支配（QKRD）基准测试，评估QAOA，发现问题感知技术优势并开源相关资源


<details>
  <summary>Details</summary>
Motivation: 现有合成随机实例缺乏语义结构和人类可解释性，对真实问题性能洞察有限

Method: 引入QKRD基准，利用其覆盖指标和内在验证评估QAOA设计选择

Result: 特定混频器收敛更快、热启动减少收敛步骤、CVaR优化效果差，QAOA优于贪婪启发式和随机选择

Conclusion: 结构化基准能揭示随机实例中被掩盖的QAOA问题感知技术优势

Abstract: The Quantum Approximate Optimization Algorithm (QAOA) is extensively benchmarked on synthetic random instances such as MaxCut, TSP, and SAT problems, but these lack semantic structure and human interpretability, offering limited insight into performance on real-world problems with meaningful constraints. We introduce Quantum King-Ring Domination (QKRD), a NISQ-scale benchmark derived from chess tactical positions that provides 5,000 structured instances with one-hot constraints, spatial locality, and 10--40 qubit scale. The benchmark pairs human-interpretable coverage metrics with intrinsic validation against classical heuristics, enabling algorithmic conclusions without external oracles. Using QKRD, we systematically evaluate QAOA design choices and find that constraint-preserving mixers (XY, domain-wall) converge approximately 13 steps faster than standard mixers (p<10^{-7}, d\approx0.5) while eliminating penalty tuning, warm-start strategies reduce convergence by 45 steps (p<10^{-127}, d=3.35) with energy improvements exceeding d=8, and Conditional Value-at-Risk (CVaR) optimization yields an informative negative result with worse energy (p<10^{-40}, d=1.21) and no coverage benefit. Intrinsic validation shows QAOA outperforms greedy heuristics by 12.6\% and random selection by 80.1\%. Our results demonstrate that structured benchmarks reveal advantages of problem-informed QAOA techniques obscured in random instances. We release all code, data, and experimental artifacts for reproducible NISQ algorithm research.

</details>


### [77] [Smart Fault Detection in Nanosatellite Electrical Power System](https://arxiv.org/abs/2601.00335)
*Alireza Rezaee,Niloofar Nobahari,Amin Asgarifar,Farshid Hajati*

Main category: cs.LG

TL;DR: 提出无姿态确定控制子系统的低轨纳卫星电力故障检测新方法，模拟无故障系统并利用多种机器学习方法诊断故障。


<details>
  <summary>Details</summary>
Motivation: 纳卫星电力系统各部分因压力、发射压力和环境因素有故障风险，需有效检测方法。

Method: 基于神经网络，用太阳辐射和太阳能板表面温度作为输入数据，电流和负载作为输出模拟无故障系统，用神经网络分类器通过故障模式和类型诊断故障，还使用PCA分类、决策树和KNN等机器学习方法进行故障分类。

Result: 未明确提及具体结果。

Conclusion: 未明确提及具体结论。

Abstract: This paper presents a new detection method of faults at Nanosatellites' electrical power without an Attitude Determination Control Subsystem (ADCS) at the LEO orbit. Each part of this system is at risk of fault due to pressure tolerance, launcher pressure, and environmental circumstances. Common faults are line to line fault and open circuit for the photovoltaic subsystem, short circuit and open circuit IGBT at DC to DC converter, and regulator fault of the ground battery. The system is simulated without fault based on a neural network using solar radiation and solar panel's surface temperature as input data and current and load as outputs. Finally, using the neural network classifier, different faults are diagnosed by pattern and type of fault. For fault classification, other machine learning methods are also used, such as PCA classification, decision tree, and KNN.

</details>


### [78] [Real-Time Human Detection for Aerial Captured Video Sequences via Deep Models](https://arxiv.org/abs/2601.00391)
*Nouar AlDahoul,Aznul Qalid Md Sabri,Ali Mohammed Mansoor*

Main category: cs.LG

TL;DR: 本文利用自动特征学习方法结合光流和三种深度模型进行非静态相机拍摄视频中的人体检测，在UCF - ARG数据集测试，结果显示方法有效。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖手工特征，有问题依赖且易受动态事件影响，而自动特征学习方法更便宜简单。

Method: 结合光流和三种深度模型（S - CNN、预训练CNN特征提取器、分层极限学习机）进行人体检测，并在UCF - ARG数据集上训练测试，分析模型训练、测试精度和学习速度。

Result: 预训练CNN平均准确率98.09%，S - CNN用softmax时95.6%、用SVM时91.7%，H - ELM平均准确率95.9%，H - ELM用CPU训练需445秒，S - CNN用高性能GPU学习需770秒。

Conclusion: 提出的方法在人体检测任务中取得成功。

Abstract: Human detection in videos plays an important role in various real-life applications. Most traditional approaches depend on utilizing handcrafted features, which are problem-dependent and optimal for specific tasks. Moreover, they are highly susceptible to dynamical events such as illumination changes, camera jitter, and variations in object sizes. On the other hand, the proposed feature learning approaches are cheaper and easier because highly abstract and discriminative features can be produced automatically without the need of expert knowledge. In this paper, we utilize automatic feature learning methods, which combine optical flow and three different deep models (i.e., supervised convolutional neural network (S-CNN), pretrained CNN feature extractor, and hierarchical extreme learning machine) for human detection in videos captured using a nonstatic camera on an aerial platform with varying altitudes. The models are trained and tested on the publicly available and highly challenging UCF-ARG aerial dataset. The comparison between these models in terms of training, testing accuracy, and learning speed is analyzed. The performance evaluation considers five human actions (digging, waving, throwing, walking, and running). Experimental results demonstrated that the proposed methods are successful for the human detection task. The pretrained CNN produces an average accuracy of 98.09%. S-CNN produces an average accuracy of 95.6% with softmax and 91.7% with Support Vector Machines (SVM). H-ELM has an average accuracy of 95.9%. Using a normal Central Processing Unit (CPU), H-ELM's training time takes 445 seconds. Learning in S-CNN takes 770 seconds with a high-performance Graphical Processing Unit (GPU).

</details>


### [79] [Deep Delta Learning](https://arxiv.org/abs/2601.00417)
*Yifan Zhang,Yifeng Liu,Mengdi Wang,Quanquan Gu*

Main category: cs.LG

TL;DR: 提出Deep Delta Learning (DDL)架构，用可学习的数据依赖几何变换改进标准残差连接，能控制层间过渡算子频谱，建模复杂动态。


<details>
  <summary>Details</summary>
Motivation: 标准残差网络的恒等捷径连接有严格加法归纳偏置，限制网络对复杂状态转换的建模能力。

Method: 引入Delta Operator，对恒等矩阵进行秩1扰动，通过反射方向向量和门控标量参数化；将残差更新重构为同步秩1注入。

Result: 对Delta Operator进行谱分析，证明门控标量可实现恒等映射、正交投影和几何反射间的动态插值。

Conclusion: DDL架构能让网络明确控制层间过渡算子频谱，在保持门控残差架构稳定训练特性的同时，对复杂非单调动态建模。

Abstract: The efficacy of deep residual networks is fundamentally predicated on the identity shortcut connection. While this mechanism effectively mitigates the vanishing gradient problem, it imposes a strictly additive inductive bias on feature transformations, thereby limiting the network's capacity to model complex state transitions. In this paper, we introduce Deep Delta Learning (DDL), a novel architecture that generalizes the standard residual connection by modulating the identity shortcut with a learnable, data-dependent geometric transformation. This transformation, termed the Delta Operator, constitutes a rank-1 perturbation of the identity matrix, parameterized by a reflection direction vector $\mathbf{k}(\mathbf{X})$ and a gating scalar $β(\mathbf{X})$. We provide a spectral analysis of this operator, demonstrating that the gate $β(\mathbf{X})$ enables dynamic interpolation between identity mapping, orthogonal projection, and geometric reflection. Furthermore, we restructure the residual update as a synchronous rank-1 injection, where the gate acts as a dynamic step size governing both the erasure of old information and the writing of new features. This unification empowers the network to explicitly control the spectrum of its layer-wise transition operator, enabling the modeling of complex, non-monotonic dynamics while preserving the stable training characteristics of gated residual architectures.

</details>


### [80] [E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models](https://arxiv.org/abs/2601.00423)
*Shengjun Zhang,Zhang Zhang,Chensheng Dai,Yueqi Duan*

Main category: cs.LG

TL;DR: 提出E - GRPO优化流程匹配模型，合并低熵步骤，引入多步组归一化优势，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有优化多去噪步骤的方法存在稀疏和模糊的奖励信号问题。

Method: 提出E - GRPO增加SDE采样步骤的熵，合并连续低熵步骤，在其他步骤应用ODE采样，引入多步组归一化优势。

Result: 不同奖励设置下的实验结果证明了方法的有效性。

Conclusion: 所提出的方法能够有效解决现有流程匹配模型在优化多去噪步骤时面临的奖励信号问题。

Abstract: Recent reinforcement learning has enhanced the flow matching models on human preference alignment. While stochastic sampling enables the exploration of denoising directions, existing methods which optimize over multiple denoising steps suffer from sparse and ambiguous reward signals. We observe that the high entropy steps enable more efficient and effective exploration while the low entropy steps result in undistinguished roll-outs. To this end, we propose E-GRPO, an entropy aware Group Relative Policy Optimization to increase the entropy of SDE sampling steps. Since the integration of stochastic differential equations suffer from ambiguous reward signals due to stochasticity from multiple steps, we specifically merge consecutive low entropy steps to formulate one high entropy step for SDE sampling, while applying ODE sampling on other steps. Building upon this, we introduce multi-step group normalized advantage, which computes group-relative advantages within samples sharing the same consolidated SDE denoising step. Experimental results on different reward settings have demonstrated the effectiveness of our methods.

</details>


### [81] [A Comparative Analysis of Interpretable Machine Learning Methods](https://arxiv.org/abs/2601.00428)
*Mattia Billa,Giovanni Orlandi,Veronica Guidetti,Federica Mandreoli*

Main category: cs.LG

TL;DR: 文章对16种固有可解释方法进行大规模对比评估，涵盖216个真实表格数据集，揭示性能层次，为从业者提供平衡可解释性和预测性能的实用指南。


<details>
  <summary>Details</summary>
Motivation: 机器学习广泛应用引发对模型可解释性和问责性的关注，但对固有可解释模型的系统评估较少，尤其针对表格数据。

Method: 对16种固有可解释方法进行大规模对比评估，跨越216个真实表格数据集，按数据集结构特征分层评估性能，还评估训练时间和分布偏移下的鲁棒性。

Result: 揭示了明确的性能层次，EBMs在回归任务中预测准确性高，SR和IGANNs在非线性场景表现好，GOSDT模型对类别不平衡敏感。

Conclusion: 研究结果为从业者平衡可解释性和预测性能提供实用指导，加深对表格数据可解释建模的实证理解。

Abstract: In recent years, Machine Learning (ML) has seen widespread adoption across a broad range of sectors, including high-stakes domains such as healthcare, finance, and law. This growing reliance has raised increasing concerns regarding model interpretability and accountability, particularly as legal and regulatory frameworks place tighter constraints on using black-box models in critical applications. Although interpretable ML has attracted substantial attention, systematic evaluations of inherently interpretable models, especially for tabular data, remain relatively scarce and often focus primarily on aggregated performance outcomes.
  To address this gap, we present a large-scale comparative evaluation of 16 inherently interpretable methods, ranging from classical linear models and decision trees to more recent approaches such as Explainable Boosting Machines (EBMs), Symbolic Regression (SR), and Generalized Optimal Sparse Decision Trees (GOSDT). Our study spans 216 real-world tabular datasets and goes beyond aggregate rankings by stratifying performance according to structural dataset characteristics, including dimensionality, sample size, linearity, and class imbalance. In addition, we assess training time and robustness under controlled distributional shifts. Our results reveal clear performance hierarchies, especially for regression tasks, where EBMs consistently achieve strong predictive accuracy. At the same time, we show that performance is highly context-dependent: SR and Interpretable Generalized Additive Neural Networks (IGANNs) perform particularly well in non-linear regimes, while GOSDT models exhibit pronounced sensitivity to class imbalance. Overall, these findings provide practical guidance for practitioners seeking a balance between interpretability and predictive performance, and contribute to a deeper empirical understanding of interpretable modeling for tabular data.

</details>


### [82] [A Comparative Study of Adaptation Strategies for Time Series Foundation Models in Anomaly Detection](https://arxiv.org/abs/2601.00446)
*Miseon Park,Kijung Yoon*

Main category: cs.LG

TL;DR: 探索时间序列基础模型（TSFMs）用于异常检测，结果显示其优于特定任务基线，参数高效微调方法有效，TSFMs前景良好。


<details>
  <summary>Details</summary>
Motivation: 多数现有时间序列异常检测方法需大量特定任务训练，探索预训练的TSFMs能否作为通用骨干用于异常检测。

Method: 通过多个基准上的系统实验，比较零样本推理、全模型自适应和参数高效微调（PEFT）策略。

Result: TSFMs在AUC - PR和VUS - PR上有显著提升，尤其在严重类别不平衡情况下；PEFT方法如LoRA、OFT和HRA降低计算成本且多数情况下效果与全微调相当或更优。

Conclusion: TSFMs是可扩展且高效的时间序列异常检测的有前途的通用模型。

Abstract: Time series anomaly detection is essential for the reliable operation of complex systems, but most existing methods require extensive task-specific training. We explore whether time series foundation models (TSFMs), pretrained on large heterogeneous data, can serve as universal backbones for anomaly detection. Through systematic experiments across multiple benchmarks, we compare zero-shot inference, full model adaptation, and parameter-efficient fine-tuning (PEFT) strategies. Our results demonstrate that TSFMs outperform task-specific baselines, achieving notable gains in AUC-PR and VUS-PR, particularly under severe class imbalance. Moreover, PEFT methods such as LoRA, OFT, and HRA not only reduce computational cost but also match or surpass full fine-tuning in most cases, indicating that TSFMs can be efficiently adapted for anomaly detection, even when pretrained for forecasting. These findings position TSFMs as promising general-purpose models for scalable and efficient time series anomaly detection.

</details>


### [83] [Controllable Concept Bottleneck Models](https://arxiv.org/abs/2601.00451)
*Hongbin Lin,Chenyang Ren,Juangui Xu,Zhengyu Hu,Cheng-Long Wang,Yao Shu,Hui Xiong,Jingfeng Zhang,Di Wang,Lijie Hu*

Main category: cs.LG

TL;DR: 提出可控概念瓶颈模型（CCBMs）解决高效编辑概念瓶颈模型（CBMs）难题，实验证明其有效性与实用性。


<details>
  <summary>Details</summary>
Motivation: 以往CBMs研究多针对静态场景，现实应用中需要对模型持续维护，高效编辑CBMs且无需从头训练是重大挑战。

Method: 提出CCBMs，支持三种粒度的模型编辑，借助影响函数推导的闭式近似避免重新训练。

Result: 实验结果表明CCBMs具有高效性和适应性。

Conclusion: CCBMs具有实际应用价值，能实现动态且可信的CBMs。

Abstract: Concept Bottleneck Models (CBMs) have garnered much attention for their ability to elucidate the prediction process through a human-understandable concept layer. However, most previous studies focused on static scenarios where the data and concepts are assumed to be fixed and clean. In real-world applications, deployed models require continuous maintenance: we often need to remove erroneous or sensitive data (unlearning), correct mislabeled concepts, or incorporate newly acquired samples (incremental learning) to adapt to evolving environments. Thus, deriving efficient editable CBMs without retraining from scratch remains a significant challenge, particularly in large-scale applications. To address these challenges, we propose Controllable Concept Bottleneck Models (CCBMs). Specifically, CCBMs support three granularities of model editing: concept-label-level, concept-level, and data-level, the latter of which encompasses both data removal and data addition. CCBMs enjoy mathematically rigorous closed-form approximations derived from influence functions that obviate the need for retraining. Experimental results demonstrate the efficiency and adaptability of our CCBMs, affirming their practical value in enabling dynamic and trustworthy CBMs.

</details>


### [84] [Imitation from Observations with Trajectory-Level Generative Embeddings](https://arxiv.org/abs/2601.00452)
*Yongtao Qu,Shangzhe Li,Weitong Zhang*

Main category: cs.LG

TL;DR: 提出TGE方法用于离线LfO，利用时间扩散模型解决专家演示稀缺和次优数据与专家行为差异大的问题，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有分布匹配方法在专家演示稀缺、离线次优数据与专家行为差异大的情况下难以从非完美数据中提取有用信号。

Method: 提出TGE，在离线轨迹数据训练的时间扩散模型潜空间中估计专家状态密度，构建密集、平滑的代理奖励。

Result: 在D4RL运动和操作基准测试中，TGE始终匹配或优于现有的离线LfO方法。

Conclusion: TGE能捕捉长视野时间动态，有效弥合不相交支持集之间的差距，即使离线数据分布与专家不同也能确保强大的学习信号。

Abstract: We consider the offline imitation learning from observations (LfO) where the expert demonstrations are scarce and the available offline suboptimal data are far from the expert behavior. Many existing distribution-matching approaches struggle in this regime because they impose strict support constraints and rely on brittle one-step models, making it hard to extract useful signal from imperfect data. To tackle this challenge, we propose TGE, a trajectory-level generative embedding for offline LfO that constructs a dense, smooth surrogate reward by estimating expert state density in the latent space of a temporal diffusion model trained on offline trajectory data. By leveraging the smooth geometry of the learned diffusion embedding, TGE captures long-horizon temporal dynamics and effectively bridges the gap between disjoint supports, ensuring a robust learning signal even when offline data is distributionally distinct from the expert. Empirically, the proposed approach consistently matches or outperforms prior offline LfO methods across a range of D4RL locomotion and manipulation benchmarks.

</details>


### [85] [Deep Networks Learn Deep Hierarchical Models](https://arxiv.org/abs/2601.00455)
*Amit Daniely*

Main category: cs.LG

TL;DR: 研究表明层wise SGD可在残差网络上高效学习一类分层模型，该类模型超越以往模型，或为理解深度学习奠定基础。


<details>
  <summary>Details</summary>
Motivation: 探索能到达高效学习深度极限的模型类，并为理解深度学习提供理论支持，从有人类“教师”存在的角度探索分层结构的意义。

Method: 在残差网络上使用层wise SGD学习分层模型，还构建简化模型来分析教师意识下的分层结构。

Result: 该分层模型类超越之前深度学习算法可学习的模型，达到高效学习深度极限；在简化教师模型中出现促进高效学习的分层结构。

Conclusion: 此类分层模型的可学习性可能为理解深度学习奠定基础，人类“教师”的存在支持分层结构天然存在的假设。

Abstract: We consider supervised learning with $n$ labels and show that layerwise SGD on residual networks can efficiently learn a class of hierarchical models. This model class assumes the existence of an (unknown) label hierarchy $L_1 \subseteq L_2 \subseteq \dots \subseteq L_r = [n]$, where labels in $L_1$ are simple functions of the input, while for $i > 1$, labels in $L_i$ are simple functions of simpler labels.
  Our class surpasses models that were previously shown to be learnable by deep learning algorithms, in the sense that it reaches the depth limit of efficient learnability. That is, there are models in this class that require polynomial depth to express, whereas previous models can be computed by log-depth circuits.
  Furthermore, we suggest that learnability of such hierarchical models might eventually form a basis for understanding deep learning. Beyond their natural fit for domains where deep learning excels, we argue that the mere existence of human ``teachers" supports the hypothesis that hierarchical structures are inherently available. By providing granular labels, teachers effectively reveal ``hints'' or ``snippets'' of the internal algorithms used by the brain. We formalize this intuition, showing that in a simplified model where a teacher is partially aware of their internal logic, a hierarchical structure emerges that facilitates efficient learnability.

</details>


### [86] [Geometric Regularization in Mixture-of-Experts: The Disconnect Between Weights and Activations](https://arxiv.org/abs/2601.00457)
*Hyunjun Kim*

Main category: cs.LG

TL;DR: 研究发现几何正则化方法（正交损失）在提升混合专家（MoE）模型专家多样性方面效果不佳，不适合用于MoE模型多样性提升。


<details>
  <summary>Details</summary>
Motivation: 明确几何正则化在混合专家（MoE）模型专家专业化中的作用。

Method: 应用正交损失来强制MoE模型的专家多样性

Result: 正交损失在多方面失败，如不能减少权重空间重叠，激活空间重叠度高，对不同数据集性能影响不一致；权重和激活正交性无显著关联。

Conclusion: 权重空间正则化既无法实现其几何目标，也不能可靠提升性能，不适合用于MoE模型的多样性提升。

Abstract: Mixture-of-Experts (MoE) models achieve efficiency through sparse activation, but the role of geometric regularization in expert specialization remains unclear. We apply orthogonality loss to enforce expert diversity and find it fails on multiple fronts: it does not reduce weight-space overlap (MSO actually increases by up to 114%), activation-space overlap remains high (~0.6) regardless of regularization, and effects on performance are inconsistent -- marginal improvement on WikiText-103 (-0.9%), slight degradation on TinyStories (+0.9%), and highly variable results on PTB (std > 1.0). Our analysis across 7 regularization strengths reveals no significant correlation (r = -0.293, p = 0.523) between weight and activation orthogonality. These findings demonstrate that weight-space regularization neither achieves its geometric goal nor reliably improves performance, making it unsuitable for MoE diversity.

</details>


### [87] [Detecting Spike Wave Discharges (SWD) using 1-dimensional Residual UNet](https://arxiv.org/abs/2601.00459)
*Saurav Sengupta,Scott Kilianski,Suchetha Sharma,Sakina Lashkeri,Ashley McHugh,Mark Beenhakker,Donald E. Brown*

Main category: cs.LG

TL;DR: 本文比较14种机器学习分类器对小鼠脑电图中尖波放电（SWD）的标注性能，发现1D UNet最佳，经数据增强改进后，AugUNet1D性能优于Twin Peaks算法，且公开模型。


<details>
  <summary>Details</summary>
Motivation: 手动标注脑电图事件耗时，尤其是长时间记录，需自动标注方法，且现有机器学习方法可改进。

Method: 在961小时含22,637个标注SWD的小鼠脑电图数据集上比较14种机器学习分类器，对1D UNet进行数据增强，与Twin Peaks算法对比。

Result: 1D UNet在标注SWD上表现最佳，数据增强中缩放效果最好，AugUNet1D性能优于Twin Peaks算法，检测事件特征更接近手动标注。

Conclusion: AugUNet1D是一种有效的自动标注脑电图SWD的方法，且公开模型可供他人使用。

Abstract: The manual labeling of events in electroencephalography (EEG) records is time-consuming. This is especially true when EEG recordings are taken continuously over weeks to months. Therefore, a method to automatically label pertinent EEG events reduces the manual workload. Spike wave discharges (SWD), which are the electrographic hallmark of absence seizures, are EEG events that are often labeled manually. While some previous studies have utilized machine learning to automatically segment and classify EEG signals like SWDs, they can be improved. Here we compare the performance of 14 machine learning classifiers on our own manually annotated dataset of 961 hours of EEG recordings from C3H/HeJ mice, including 22,637 labeled SWDs. We find that a 1D UNet performs best for labeling SWDs in this dataset. We also improve the 1D UNet by augmenting our training data and determine that scaling showed the greatest benefit of all augmentation procedures applied. We then compare the 1D UNet with data augmentation, AugUNet1D, against a recently published time- and frequency-based algorithmic approach called "Twin Peaks". AugUNet1D showed superior performance and detected events with more similar features to the SWDs labeled manually. AugUNet1D, pretrained on our manually annotated data or untrained, is made public for others users.

</details>


### [88] [Neural Chains and Discrete Dynamical Systems](https://arxiv.org/abs/2601.00473)
*Sauro Succi,Abhisek Ganguly,Santosh Ansumali*

Main category: cs.LG

TL;DR: 对比无自注意力的transformer架构ML应用（神经链）与神经积分和偏微分方程离散系统，分析Burgers和Eikonal方程数值解，发现标准数值离散和PINN学习获取系统动力学知识路径不同，PINN有参数多等问题，但不排除其在高维问题有优势。


<details>
  <summary>Details</summary>
Motivation: 研究无自注意力transformer架构ML应用与神经积分和偏微分方程离散系统的类比关系，对比不同方法求解方程的情况。

Method: 对Burgers和Eikonal方程进行标准数值离散和PINN学习求解，并进行比较分析。

Result: 标准数值离散和PINN学习获取系统动力学知识路径不同，PINN学习通过随机矩阵，可接受解对应的随机矩阵多，但参数多、缺乏物理透明性和训练成本高。

Conclusion: 研究结果针对一维动态问题，不排除PINN和ML在高维问题上有更好策略。

Abstract: We inspect the analogy between machine-learning (ML) applications based on the transformer architecture without self-attention, {\it neural chains} hereafter, and discrete dynamical systems associated with discretised versions of neural integral and partial differential equations (NIE, PDE). A comparative analysis of the numerical solution of the (viscid and inviscid) Burgers and Eikonal equations via standard numerical discretization (also cast in terms of neural chains) and via PINN's learning is presented and commented on. It is found that standard numerical discretization and PINN learning provide two different paths to acquire essentially the same knowledge about the dynamics of the system. PINN learning proceeds through random matrices which bear no direct relation to the highly structured matrices associated with finite-difference (FD) procedures. Random matrices leading to acceptable solutions are far more numerous than the unique tridiagonal form in matrix space, which explains why the PINN search typically lands on the random ensemble. The price is a much larger number of parameters, causing lack of physical transparency (explainability) as well as large training costs with no counterpart in the FD procedure. However, our results refer to one-dimensional dynamic problems, hence they don't rule out the possibility that PINNs and ML in general, may offer better strategies for high-dimensional problems.

</details>


### [89] [When Small Models Are Right for Wrong Reasons: Process Verification for Trustworthy Agents](https://arxiv.org/abs/2601.00513)
*Laksh Advani*

Main category: cs.LG

TL;DR: 研究小语言模型推理不可靠问题，提出RIS指标评估，发现RAG提升推理完整性，元认知干预常有害，最后得出过程验证对可信代理很必要。


<details>
  <summary>Details</summary>
Motivation: 部署小语言模型作为自主代理时，需信任其推理过程而非仅输出，但当前小模型推理存在可靠性危机，标准准确率指标难以察觉。

Method: 分析三个模型及不同任务的10734条推理痕迹，提出Reasoning Integrity Score (RIS)；研究不同干预方法的影响；进行机制分析；将验证能力蒸馏成神经分类器。

Result: 小模型50 - 69%正确答案推理有根本缺陷；RAG显著提升推理完整性，元认知干预常损害性能；RAG通过外部证据减少7.6%错误，元认知在模型能力不足时放大混乱；神经分类器F1得分0.86且速度提升100倍。

Conclusion: 仅依靠准确率评估小模型不可取，基于过程的验证对构建可信代理是必要的。

Abstract: Deploying small language models (7-9B parameters) as autonomous agents requires trust in their reasoning, not just their outputs. We reveal a critical reliability crisis: 50-69\% of correct answers from these models contain fundamentally flawed reasoning -- a ``Right-for-Wrong-Reasons'' phenomenon invisible to standard accuracy metrics. Through analysis of 10,734 reasoning traces across three models and diverse tasks, we introduce the Reasoning Integrity Score (RIS), a process-based metric validated with substantial inter-rater agreement ($κ=0.657$). Conventional practices are challenged by our findings: while retrieval-augmented generation (RAG) significantly improves reasoning integrity (Cohen's $d=0.23$--$0.93$), meta-cognitive interventions like self-critique often harm performance ($d=-0.14$ to $-0.33$) in small models on the evaluated tasks. Mechanistic analysis reveals RAG succeeds by grounding calculations in external evidence, reducing errors by 7.6\%, while meta-cognition amplifies confusion without sufficient model capacity. To enable deployment, verification capabilities are distilled into a neural classifier achieving 0.86 F1-score with 100$\times$ speedup. These results underscore the necessity of process-based verification for trustworthy agents: accuracy alone is dangerously insufficient when models can be right for entirely wrong reasons.

</details>


### [90] [Trajectory Guard -- A Lightweight, Sequence-Aware Model for Real-Time Anomaly Detection in Agentic AI](https://arxiv.org/abs/2601.00516)
*Laksh Advani*

Main category: cs.LG

TL;DR: 现有大模型自主代理异常检测方法不佳，引入Trajectory Guard方法在多基准测试表现好，推理快可用于实时安全验证。


<details>
  <summary>Details</summary>
Motivation: 自主大模型代理生成的多步行动计划会因上下文不一致或结构不连贯而失败，现有异常检测方法不适用于此挑战。

Method: 引入Trajectory Guard，一种具有混合损失函数的孪生循环自编码器，通过对比学习和重建分别学习任务轨迹对齐和序列有效性。

Result: 在多个基准测试上，平衡集F1分数达0.88 - 0.94，不平衡外部基准召回率0.86 - 0.92，推理延迟32毫秒，比LLM Judge基线快17 - 27倍。

Conclusion: 所提方法能有效检测行动计划异常，具有高准确性和快速推理能力，可用于生产部署的实时安全验证。

Abstract: Autonomous LLM agents generate multi-step action plans that can fail due to contextual misalignment or structural incoherence. Existing anomaly detection methods are ill-suited for this challenge: mean-pooling embeddings dilutes anomalous steps, while contrastive-only approaches ignore sequential structure. Standard unsupervised methods on pre-trained embeddings achieve F1-scores no higher than 0.69. We introduce Trajectory Guard, a Siamese Recurrent Autoencoder with a hybrid loss function that jointly learns task-trajectory alignment via contrastive learning and sequential validity via reconstruction. This dual objective enables unified detection of both "wrong plan for this task" and "malformed plan structure." On benchmarks spanning synthetic perturbations and real-world failures from security audits (RAS-Eval) and multi-agent systems (Who\&When), we achieve F1-scores of 0.88-0.94 on balanced sets and recall of 0.86-0.92 on imbalanced external benchmarks. At 32 ms inference latency, our approach runs 17-27$\times$ faster than LLM Judge baselines, enabling real-time safety verification in production deployments.

</details>


### [91] [A Sparse-Attention Deep Learning Model Integrating Heterogeneous Multimodal Features for Parkinson's Disease Severity Profiling](https://arxiv.org/abs/2601.00519)
*Dristi Datta,Tanmoy Debnath,Minh Chau,Manoranjan Paul,Gourab Adhikary,Md Geaur Rahman*

Main category: cs.LG

TL;DR: 提出Class - Weighted Sparse - Attention Fusion Network (SAFN)用于帕金森病多模态分析，表现优于基线模型，有可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有计算模型在可解释性、类别不平衡、高维成像与临床特征融合方面存在局限，需统一预测框架表征帕金森病异质性表现。

Method: 提出SAFN框架，用特定编码器和对称交叉注意力机制整合多模态数据，用稀疏约束注意力门控融合层和类别平衡焦点损失处理数据。

Result: 在703名参与者上评估，SAFN准确率0.98±0.02，PR - AUC为1.00±0.00，优于基线模型。

Conclusion: SAFN为神经退行性疾病计算分析提供了可重复、透明的多模态建模范式。

Abstract: Characterising the heterogeneous presentation of Parkinson's disease (PD) requires integrating biological and clinical markers within a unified predictive framework. While multimodal data provide complementary information, many existing computational models struggle with interpretability, class imbalance, or effective fusion of high-dimensional imaging and tabular clinical features. To address these limitations, we propose the Class-Weighted Sparse-Attention Fusion Network (SAFN), an interpretable deep learning framework for robust multimodal profiling. SAFN integrates MRI cortical thickness, MRI volumetric measures, clinical assessments, and demographic variables using modality-specific encoders and a symmetric cross-attention mechanism that captures nonlinear interactions between imaging and clinical representations. A sparsity-constrained attention-gating fusion layer dynamically prioritises informative modalities, while a class-balanced focal loss (beta = 0.999, gamma = 1.5) mitigates dataset imbalance without synthetic oversampling. Evaluated on 703 participants (570 PD, 133 healthy controls) from the Parkinson's Progression Markers Initiative using subject-wise five-fold cross-validation, SAFN achieves an accuracy of 0.98 plus or minus 0.02 and a PR-AUC of 1.00 plus or minus 0.00, outperforming established machine learning and deep learning baselines. Interpretability analysis shows a clinically coherent decision process, with approximately 60 percent of predictive weight assigned to clinical assessments, consistent with Movement Disorder Society diagnostic principles. SAFN provides a reproducible and transparent multimodal modelling paradigm for computational profiling of neurodegenerative disease.

</details>


### [92] [Optimizing LSTM Neural Networks for Resource-Constrained Retail Sales Forecasting: A Model Compression Study](https://arxiv.org/abs/2601.00525)
*Ravi Teja Pagidoju*

Main category: cs.LG

TL;DR: 本文研究LSTM模型压缩，通过减少隐藏单元数量在保证甚至提高预测精度的同时减小模型大小。


<details>
  <summary>Details</summary>
Motivation: 标准LSTM神经网络预测零售行业销售数据需大量计算资源，对中小零售企业有挑战，因此研究模型压缩。

Method: 逐步将LSTM隐藏单元数量从128减至16，使用Kaggle数据集分析模型大小与预测精度的权衡。

Result: 将隐藏单元降至64时，精度保持并提升，MAPE从23.6%降至12.4%，模型缩小73%，精度提高47%。

Conclusion: 更大的模型不一定能取得更好的结果。

Abstract: Standard LSTM(Long Short-Term Memory) neural networks provide accurate predictions for sales data in the retail industry, but require a lot of computing power. It can be challenging especially for mid to small retail industries. This paper examines LSTM model compression by gradually reducing the number of hidden units from 128 to 16. We used the Kaggle Store Item Demand Forecasting dataset, which has 913,000 daily sales records from 10 stores and 50 items, to look at the trade-off between model size and how accurate the predictions are. Experiments show that lowering the number of hidden LSTM units to 64 maintains the same level of accuracy while also improving it. The mean absolute percentage error (MAPE) ranges from 23.6% for the full 128-unit model to 12.4% for the 64-unit model. The optimized model is 73% smaller (from 280KB to 76KB) and 47% more accurate. These results show that larger models do not always achieve better results.

</details>


### [93] [Cloud-Native Generative AI for Automated Planogram Synthesis: A Diffusion Model Approach for Multi-Store Retail Optimization](https://arxiv.org/abs/2601.00527)
*Ravi Teja Pagidoju,Shriya Agarwal*

Main category: cs.LG

TL;DR: 本文提出云原生架构结合扩散模型自动生成特定商店的商品陈列图，大幅减少设计时间和成本，证明生成式AI用于零售空间优化的可行性。


<details>
  <summary>Details</summary>
Motivation: 商品陈列图创建是零售行业的重大挑战，传统方法需大量时间。

Method: 引入云原生架构，采用扩散模型，结合AWS云训练和边缘部署，通过修改损失函数集成零售特定约束。

Result: 系统将陈列图设计时间减少98.3%至0.5小时，约束满足率达94.4%，创建费用降低97.5%，收支平衡期4.4个月，架构线性可扩展，支持10000个并发商店请求。

Conclusion: 生成式AI可用于自动零售空间优化。

Abstract: Planogram creation is a significant challenge for retail, requiring an average of 30 hours per complex layout. This paper introduces a cloud-native architecture using diffusion models to automatically generate store-specific planograms. Unlike conventional optimization methods that reorganize existing layouts, our system learns from successful shelf arrangements across multiple retail locations to create new planogram configurations. The architecture combines cloud-based model training via AWS with edge deployment for real-time inference. The diffusion model integrates retail-specific constraints through a modified loss function. Simulation-based analysis demonstrates the system reduces planogram design time by 98.3% (from 30 to 0.5 hours) while achieving 94.4% constraint satisfaction. Economic analysis reveals a 97.5% reduction in creation expenses with a 4.4-month break-even period. The cloud-native architecture scales linearly, supporting up to 10,000 concurrent store requests. This work demonstrates the viability of generative AI for automated retail space optimization.

</details>


### [94] [Entropy Production in Machine Learning Under Fokker-Planck Probability Flow](https://arxiv.org/abs/2601.00554)
*Lennon Shikhman*

Main category: cs.LG

TL;DR: 提出基于熵的再训练框架应对非平稳环境下数据漂移导致的机器学习模型性能下降问题，实验显示能在减少再训练次数的同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有漂移检测方法缺乏原则性动态解释，在平衡再训练频率和运营成本方面指导有限。

Method: 基于非平衡随机动力学，将部署时的数据漂移建模为福克 - 普朗克方程控制的概率流，用随时间变化的KL散度量化模型 - 数据不匹配。

Result: 在受控非平稳分类实验中，熵触发的再训练在减少再训练事件数量的情况下，实现了与高频再训练相当的预测性能。

Conclusion: 熵触发的再训练是一种无标签干预策略，可响应累积的不匹配，而非延迟的性能崩溃。

Abstract: Machine learning models deployed in nonstationary environments experience performance degradation due to data drift. While many drift detection heuristics exist, most lack a principled dynamical interpretation and provide limited guidance on how retraining frequency should be balanced against operational cost. In this work, we propose an entropy--based retraining framework grounded in nonequilibrium stochastic dynamics. Modeling deployment--time data drift as probability flow governed by a Fokker--Planck equation, we quantify model--data mismatch using a time--evolving Kullback--Leibler divergence. We show that the time derivative of this mismatch admits an entropy--balance decomposition featuring a nonnegative entropy production term driven by probability currents. This interpretation motivates entropy--triggered retraining as a label--free intervention strategy that responds to accumulated mismatch rather than delayed performance collapse. In a controlled nonstationary classification experiment, entropy--triggered retraining achieves predictive performance comparable to high--frequency retraining while reducing retraining events by an order of magnitude relative to daily and label--based policies.

</details>


### [95] [Adversarial Samples Are Not Created Equal](https://arxiv.org/abs/2601.00577)
*Jennifer Crawford,Amol Khanna,Fred Lu,Amy R. Wagoner,Stella Biderman,Andre T. Nguyen,Edward Raff*

Main category: cs.LG

TL;DR: 过去十年有诸多理论解释DNN对抗逃避攻击脆弱性，本文指出应区分两类对抗弱点，提出基于集成的指标分析对抗样本组成，并重新审视相关现象。


<details>
  <summary>Details</summary>
Motivation: 已有非鲁棒特征理论忽视不利用这些特征的对抗样本，需区分两类对抗弱点来评估对抗鲁棒性。

Method: 提出基于集成的指标衡量对抗扰动对非鲁棒特征的操纵，用该指标分析攻击者生成的对抗样本组成。

Result: 可重新审视包括锐度感知最小化对对抗鲁棒性的影响、对抗训练和标准训练在鲁棒数据集上的鲁棒性差距等多种现象。

Conclusion: 应区分两种对抗弱点，所提指标有助于分析对抗样本和重新审视相关现象。

Abstract: Over the past decade, numerous theories have been proposed to explain the widespread vulnerability of deep neural networks to adversarial evasion attacks. Among these, the theory of non-robust features proposed by Ilyas et al. has been widely accepted, showing that brittle but predictive features of the data distribution can be directly exploited by attackers. However, this theory overlooks adversarial samples that do not directly utilize these features. In this work, we advocate that these two kinds of samples - those which use use brittle but predictive features and those that do not - comprise two types of adversarial weaknesses and should be differentiated when evaluating adversarial robustness. For this purpose, we propose an ensemble-based metric to measure the manipulation of non-robust features by adversarial perturbations and use this metric to analyze the makeup of adversarial samples generated by attackers. This new perspective also allows us to re-examine multiple phenomena, including the impact of sharpness-aware minimization on adversarial robustness and the robustness gap observed between adversarially training and standard training on robust datasets.

</details>


### [96] [Learning to be Reproducible: Custom Loss Design for Robust Neural Networks](https://arxiv.org/abs/2601.00578)
*Waqas Ahmed,Sheeba Samuel,Kevin Coakley,Birgitta Koenig-Ries,Odd Erik Gundersen*

Main category: cs.LG

TL;DR: 为增强深度学习模型可重复性和可靠性，提出自定义损失函数CLF，平衡准确性与稳定性，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 当前训练方法缺乏保证模型跨运行稳定性能的机制，模型准确率存在显著变异性。

Method: 提出自定义损失函数CLF，微调其参数平衡预测准确性和训练稳定性。

Result: 在图像分类和时间序列预测的不同架构上的广泛实验表明，该方法显著提高训练鲁棒性且不牺牲预测性能。

Conclusion: CLF是开发更稳定、可靠和可信神经网络的有效策略。

Abstract: To enhance the reproducibility and reliability of deep learning models, we address a critical gap in current training methodologies: the lack of mechanisms that ensure consistent and robust performance across runs. Our empirical analysis reveals that even under controlled initialization and training conditions, the accuracy of the model can exhibit significant variability. To address this issue, we propose a Custom Loss Function (CLF) that reduces the sensitivity of training outcomes to stochastic factors such as weight initialization and data shuffling. By fine-tuning its parameters, CLF explicitly balances predictive accuracy with training stability, leading to more consistent and reliable model performance. Extensive experiments across diverse architectures for both image classification and time series forecasting demonstrate that our approach significantly improves training robustness without sacrificing predictive performance. These results establish CLF as an effective and efficient strategy for developing more stable, reliable and trustworthy neural networks.

</details>


### [97] [HFedMoE: Resource-aware Heterogeneous Federated Learning with Mixture-of-Experts](https://arxiv.org/abs/2601.00583)
*Zihan Fang,Zheng Lin,Senkang Hu,Yanan Ma,Yihang Tao,Yiqin Deng,Xianhao Chen,Yuguang Fang*

Main category: cs.LG

TL;DR: 传统联邦学习在资源受限设备微调大语言模型有困难，引入MoE仍有挑战，本文提出HFedMoE框架解决问题，实验显示其表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习在资源受限设备上微调大语言模型不可行，引入MoE后仍面临选择专家难、客户端计算资源异构和全局聚合受影响等问题。

Method: 提出HFedMoE框架，根据专家对微调性能的贡献确定其重要性，从信息瓶颈角度为客户端选择专家子集，设计稀疏感知模型聚合策略。

Result: 广泛实验表明，HFedMoE在训练准确率和收敛速度上优于现有基准。

Conclusion: HFedMoE能有效解决基于MoE的联邦学习微调大语言模型遇到的问题，实现高效微调。

Abstract: While federated learning (FL) enables fine-tuning of large language models (LLMs) without compromising data privacy, the substantial size of an LLM renders on-device training impractical for resource-constrained clients, such as mobile devices. Thus, Mixture-of-Experts (MoE) models have emerged as a computation-efficient solution, which activates only a sparse subset of experts during model training to reduce computing burden without sacrificing performance. Though integrating MoE into FL fine-tuning holds significant potential, it still encounters three key challenges: i) selecting appropriate experts for clients remains challenging due to the lack of a reliable metric to measure each expert's impact on local fine-tuning performance, ii) the heterogeneous computing resources across clients severely hinder MoE-based LLM fine-tuning, as dynamic expert activations across diverse input samples can overwhelm resource-constrained devices, and iii) client-specific expert subsets and routing preference undermine global aggregation, where misaligned expert updates and inconsistent gating networks in troduce destructive interference. To address these challenges, we propose HFedMoE, a heterogeneous MoE-based FL fine-tuning framework that customizes a subset of experts to each client for computation-efficient LLM fine-tuning. Specifically, HFedMoE identifies the expert importance based on its contributions to fine-tuning performance, and then adaptively selects a subset of experts from an information bottleneck perspective to align with each client' s computing budget. A sparsity-aware model aggregation strategy is also designed to aggregate the actively fine-tuned experts and gating parameters with importance weighted contributions. Extensive experiments demonstrate that HFedMoE outperforms state-of-the-art benchmarks in training accuracy and convergence speed.

</details>


### [98] [Cycling Race Time Prediction: A Personalized Machine Learning Approach Using Route Topology and Training Load](https://arxiv.org/abs/2601.00604)
*Francisco Aguilera Moreno*

Main category: cs.LG

TL;DR: 为解决现有骑行时长预测方案对业余骑行者不实用问题，提出结合路线拓扑特征和运动员体能状态的机器学习方法，经评估效果良好，且体能指标可降低误差，还能进行动态赛事规划。


<details>
  <summary>Details</summary>
Motivation: 现有基于物理模型的骑行时长预测方案需大量参数，对多数业余骑行者不实用，需要新的预测方案。

Method: 提出结合路线拓扑特征和运动员当前体能状态（从训练负荷指标得出）的机器学习方法，通过历史数据学习运动员特定表现模式，采用Lasso回归模型，进行N - of - 1研究设计并做特征工程消除数据泄漏。

Result: 使用单运动员数据集评估发现，Topology + Fitness特征的Lasso回归达到MAE = 6.60分钟和R² = 0.922，整合体能指标比仅用拓扑特征误差降低14%。

Conclusion: 生理状态对骑行表现有显著约束作用，该方法能够用于预测骑行时长并实现动态赛事规划。

Abstract: Predicting cycling duration for a given route is essential for training planning and event preparation. Existing solutions rely on physics-based models that require extensive parameterization, including aerodynamic drag coefficients and real-time wind forecasts, parameters impractical for most amateur cyclists. This work presents a machine learning approach that predicts ride duration using route topology features combined with the athlete's current fitness state derived from training load metrics. The model learns athlete-specific performance patterns from historical data, substituting complex physical measurements with historical performance proxies. We evaluate the approach using a single-athlete dataset (N=96 rides) in an N-of-1 study design. After rigorous feature engineering to eliminate data leakage, we find that Lasso regression with Topology + Fitness features achieves MAE=6.60 minutes and R2=0.922. Notably, integrating fitness metrics (CTL, ATL) reduces error by 14% compared to topology alone (MAE=7.66 min), demonstrating that physiological state meaningfully constrains performance even in self-paced efforts. Progressive checkpoint predictions enable dynamic race planning as route difficulty becomes apparent.

</details>


### [99] [Traffic-Aware Optimal Taxi Placement Using Graph Neural Network-Based Reinforcement Learning](https://arxiv.org/abs/2601.00607)
*Sonia Khetarpaul,P Y Sharan*

Main category: cs.LG

TL;DR: 提出流量感知、基于图的强化学习框架用于大都市出租车最优布局，实验显示能降低乘客等待时间和行驶距离，可适配多模式交通。


<details>
  <summary>Details</summary>
Motivation: 传统出租车热点预测模型仅依赖历史需求，忽略动态影响因素，需要实时整合城市交通网络数据和出行模式以实现出租车供需高效匹配。

Method: 将城市道路网络建模为图，用图神经网络嵌入编码时空依赖，由Q学习代理推荐最优热点，奖励机制优化多指标。

Result: 在模拟德里出租车数据集上，与基线随机选择相比，模型使乘客等待时间降低约56%，行驶距离减少38%。

Conclusion: 该方法适用于多模式交通系统，可集成到智慧城市平台进行实时城市出行优化。

Abstract: In the context of smart city transportation, efficient matching of taxi supply with passenger demand requires real-time integration of urban traffic network data and mobility patterns. Conventional taxi hotspot prediction models often rely solely on historical demand, overlooking dynamic influences such as traffic congestion, road incidents, and public events. This paper presents a traffic-aware, graph-based reinforcement learning (RL) framework for optimal taxi placement in metropolitan environments. The urban road network is modeled as a graph where intersections represent nodes, road segments serve as edges, and node attributes capture historical demand, event proximity, and real-time congestion scores obtained from live traffic APIs. Graph Neural Network (GNN) embeddings are employed to encode spatial-temporal dependencies within the traffic network, which are then used by a Q-learning agent to recommend optimal taxi hotspots. The reward mechanism jointly optimizes passenger waiting time, driver travel distance, and congestion avoidance. Experiments on a simulated Delhi taxi dataset, generated using real geospatial boundaries and historic ride-hailing request patterns, demonstrate that the proposed model reduced passenger waiting time by about 56% and reduced travel distance by 38% compared to baseline stochastic selection. The proposed approach is adaptable to multi-modal transport systems and can be integrated into smart city platforms for real-time urban mobility optimization.

</details>


### [100] [Stronger Approximation Guarantees for Non-Monotone γ-Weakly DR-Submodular Maximization](https://arxiv.org/abs/2601.00611)
*Hareshkumar Jadav,Ranveer Singh,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: 研究非负、非单调γ - 弱DR - 子模函数在向下封闭凸体上的最大化问题，提出近似算法，有良好近似保证。


<details>
  <summary>Details</summary>
Motivation: 子模目标在约束下的最大化是机器学习和优化中的基本问题，研究非单调γ - 弱DR - 子模函数在向下封闭凸体上的最大化。

Method: 结合Frank - Wolfe引导的连续贪心框架与γ - 感知双贪心步骤。

Result: 得到的近似算法保证与γ平滑相关，γ = 1时恢复0.401近似因子，γ < 1时保证平稳下降且优于先前结果。

Conclusion: 该方法为非单调γ - 弱DR - 子模函数在向下封闭凸体上的最大化提供了最先进的保证。

Abstract: Maximizing submodular objectives under constraints is a fundamental problem in machine learning and optimization. We study the maximization of a nonnegative, non-monotone $γ$-weakly DR-submodular function over a down-closed convex body. Our main result is an approximation algorithm whose guarantee depends smoothly on $γ$; in particular, when $γ=1$ (the DR-submodular case) our bound recovers the $0.401$ approximation factor, while for $γ<1$ the guarantee degrades gracefully and, it improves upon previously reported bounds for $γ$-weakly DR-submodular maximization under the same constraints. Our approach combines a Frank-Wolfe-guided continuous-greedy framework with a $γ$-aware double-greedy step, yielding a simple yet effective procedure for handling non-monotonicity. This results in state-of-the-art guarantees for non-monotone $γ$-weakly DR-submodular maximization over down-closed convex bodies.

</details>


### [101] [Do Chatbot LLMs Talk Too Much? The YapBench Benchmark](https://arxiv.org/abs/2601.00624)
*Vadim Borisov,Michael Gröger,Mina Mikhael,Richard H. Schreiber*

Main category: cs.LG

TL;DR: 提出YapBench基准测试评估大语言模型在简洁性理想提示下的过度生成问题，评估76个模型并公布结果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型常对简单请求给出冗长回复，增加认知负担和推理成本，且先前方法会导致长度偏差。

Method: 引入YapBench基准测试，包含单轮提示、最小充分基线答案和类别标签，用YapScore和YapIndex评估模型。

Result: 评估76个模型，发现中位数多余长度有数量级差异，有特定类别失败模式。

Conclusion: 发布基准测试并维护实时排行榜跟踪模型冗长行为。

Abstract: Large Language Models (LLMs) such as ChatGPT, Claude, and Gemini increasingly act as general-purpose copilots, yet they often respond with unnecessary length on simple requests, adding redundant explanations, hedging, or boilerplate that increases cognitive load and inflates token-based inference cost. Prior work suggests that preference-based post-training and LLM-judged evaluations can induce systematic length bias, where longer answers are rewarded even at comparable quality.
  We introduce YapBench, a lightweight benchmark for quantifying user-visible over-generation on brevity-ideal prompts. Each item consists of a single-turn prompt, a curated minimal-sufficient baseline answer, and a category label. Our primary metric, YapScore, measures excess response length beyond the baseline in characters, enabling comparisons across models without relying on any specific tokenizer. We summarize model performance via the YapIndex, a uniformly weighted average of category-level median YapScores.
  YapBench contains over three hundred English prompts spanning three common brevity-ideal settings: (A) minimal or ambiguous inputs where the ideal behavior is a short clarification, (B) closed-form factual questions with short stable answers, and (C) one-line coding tasks where a single command or snippet suffices. Evaluating 76 assistant LLMs, we observe an order-of-magnitude spread in median excess length and distinct category-specific failure modes, including vacuum-filling on ambiguous inputs and explanation or formatting overhead on one-line technical requests. We release the benchmark and maintain a live leaderboard for tracking verbosity behavior over time.

</details>


### [102] [Interpretability-Guided Bi-objective Optimization: Aligning Accuracy and Explainability](https://arxiv.org/abs/2601.00655)
*Kasra Fouladi,Hamta Rahmani*

Main category: cs.LG

TL;DR: 本文提出IGBO框架，通过双目标公式结合领域知识训练可解释模型，在时序数据上效果良好。


<details>
  <summary>Details</summary>
Motivation: 训练可解释模型并解决TIG计算中的分布外问题

Method: 通过双目标公式结合结构化领域知识，将特征重要性层次编码为DAG，用TIG测量特征重要性，提出最优路径预言器解决OOD问题

Result: 理论分析证明收敛性和对小批量噪声的鲁棒性，实证结果显示IGBO在时序数据上能以最小精度损失执行DAG约束，优于标准正则化基线

Conclusion: IGBO框架有效，在训练可解释模型方面表现良好

Abstract: This paper introduces Interpretability-Guided Bi-objective Optimization (IGBO), a framework that trains interpretable models by incorporating structured domain knowledge via a bi-objective formulation. IGBO encodes feature importance hierarchies as a Directed Acyclic Graph (DAG) and uses Temporal Integrated Gradients (TIG) to measure feature importance. To address the Out-of-Distribution (OOD) problem in TIG computation, we propose an Optimal Path Oracle that learns data-manifold-aware integration paths. Theoretical analysis proves convergence properties and robustness to mini-batch noise, while empirical results on time-series data demonstrate IGBO's effectiveness in enforcing DAG constraints with minimal accuracy loss, outperforming standard regularization baselines.

</details>


### [103] [Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation](https://arxiv.org/abs/2601.00664)
*Taekyung Ki,Sangwon Jang,Jaehyeong Jo,Jaehong Yoon,Sung Ju Hwang*

Main category: cs.LG

TL;DR: 针对当前会说话头像生成模型缺乏交互感问题，提出Avatar Forcing框架及无标签学习方法，实现低延迟实时交互且效果佳。


<details>
  <summary>Details</summary>
Motivation: 当前会说话头像生成模型缺乏真正的交互感，生成单向回应且缺乏情感参与，需解决实时因果约束下的动作生成和无额外标签数据学习富有表现力反应的挑战。

Method: 提出Avatar Forcing框架，通过扩散强制建模实时用户 - 头像交互；引入直接偏好优化方法，利用丢弃用户条件构建的合成失败样本进行无标签学习。

Result: 框架实现约500ms低延迟实时交互，比基线快6.8倍，生成的头像动作具有反应性和表现力，超80%优于基线。

Conclusion: 所提框架和方法能有效解决会说话头像生成的交互性问题，实现低延迟实时交互且效果良好。

Abstract: Talking head generation creates lifelike avatars from static portraits for virtual communication and content creation. However, current models do not yet convey the feeling of truly interactive communication, often generating one-way responses that lack emotional engagement. We identify two key challenges toward truly interactive avatars: generating motion in real-time under causal constraints and learning expressive, vibrant reactions without additional labeled data. To address these challenges, we propose Avatar Forcing, a new framework for interactive head avatar generation that models real-time user-avatar interactions through diffusion forcing. This design allows the avatar to process real-time multimodal inputs, including the user's audio and motion, with low latency for instant reactions to both verbal and non-verbal cues such as speech, nods, and laughter. Furthermore, we introduce a direct preference optimization method that leverages synthetic losing samples constructed by dropping user conditions, enabling label-free learning of expressive interaction. Experimental results demonstrate that our framework enables real-time interaction with low latency (approximately 500ms), achieving 6.8X speedup compared to the baseline, and produces reactive and expressive avatar motion, which is preferred over 80% against the baseline.

</details>


### [104] [IRPO: Scaling the Bradley-Terry Model via Reinforcement Learning](https://arxiv.org/abs/2601.00677)
*Haonan Song,Qingchen Xie,Huan Zhu,Feng Xiao,Luxi Xing,Fuzhen Li,Liu Kang,Feng Jiang,Zhiyong Zheng,Fan Yang*

Main category: cs.LG

TL;DR: 论文指出常用成对生成奖励模型与RL算法结合有计算瓶颈，提出IRPO框架解决问题，实验显示其表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决成对生成奖励模型与RL算法结合时因成对比较复杂度和重复采样等带来的计算瓶颈问题。

Method: 提出Intergroup Relative Preference Optimization (IRPO)框架，将Bradley - Terry模型融入GRPO，为每个响应生成点式分数。

Result: IRPO在多个基准测试中达到点式GRMs的SOTA性能，与当前领先的成对GRMs相当，后训练评估中显著优于成对GRMs。

Conclusion: IRPO能在RL训练中高效评估任意数量的候选者，同时保留可解释性和细粒度奖励信号，是解决相关计算瓶颈的有效方法。

Abstract: Generative Reward Models (GRMs) have attracted considerable research interest in reward modeling due to their interpretability, inference-time scalability, and potential for refinement through reinforcement learning (RL). However, widely used pairwise GRMs create a computational bottleneck when integrated with RL algorithms such as Group Relative Policy Optimization (GRPO). This bottleneck arises from two factors: (i) the O(n^2) time complexity of pairwise comparisons required to obtain relative scores, and (ii) the computational overhead of repeated sampling or additional chain-of-thought (CoT) reasoning to improve performance. To address the first factor, we propose Intergroup Relative Preference Optimization (IRPO), a novel RL framework that incorporates the well-established Bradley-Terry model into GRPO. By generating a pointwise score for each response, IRPO enables efficient evaluation of arbitrarily many candidates during RL training while preserving interpretability and fine-grained reward signals. Experimental results demonstrate that IRPO achieves state-of-the-art (SOTA) performance among pointwise GRMs across multiple benchmarks, with performance comparable to that of current leading pairwise GRMs. Furthermore, we show that IRPO significantly outperforms pairwise GRMs in post-training evaluations.

</details>


### [105] [ARISE: Adaptive Reinforcement Integrated with Swarm Exploration](https://arxiv.org/abs/2601.00693)
*Rajiv Chaitanya M,D R Ramesh Babu*

Main category: cs.LG

TL;DR: 本文提出了轻量级框架ARISE，通过增加基于群体的探索层增强强化学习，在困难任务和非平稳奖励场景有显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习在非平稳奖励或高维策略下有效探索的挑战。

Method: 引入ARISE框架，将标准策略梯度方法与基于群体的探索层结合，融合策略动作和粒子驱动的建议，并根据奖励方差自适应调整探索。

Result: 在简单任务有小提升，如CartPole-v1提升0.7%；在困难任务提升显著，如LunarLander-v3提升46%、Hopper-v4提升22%；在非平稳奖励下比PPO有明显稳健性优势；消融实验证实群体组件和自适应机制对性能有贡献。

Conclusion: ARISE提供了一种简单、与架构无关的方式，在不改变核心算法结构的情况下使强化学习智能体更具探索性和韧性。

Abstract: Effective exploration remains a key challenge in RL, especially with non-stationary rewards or high-dimensional policies. We introduce ARISE, a lightweight framework that enhances reinforcement learning by augmenting standard policy-gradient methods with a compact swarm-based exploration layer. ARISE blends policy actions with particle-driven proposals, where each particle represents a candidate policy trajectory sampled in the action space, and modulates exploration adaptively using reward-variance cues. While easy benchmarks exhibit only slight improvements (e.g., +0.7% on CartPole-v1), ARISE yields substantial gains on more challenging tasks, including +46% on LunarLander-v3 and +22% on Hopper-v4, while preserving stability on Walker2d and Ant. Under non-stationary reward shifts, ARISE provides marked robustness advantages, outperforming PPO by +75 points on CartPole and improving LunarLander accordingly. Ablation studies confirm that both the swarm component and the adaptive mechanism contribute to the performance. Overall, ARISE offers a simple, architecture-agnostic route to more exploratory and resilient RL agents without altering core algorithmic structures.

</details>


### [106] [BSAT: B-Spline Adaptive Tokenizer for Long-Term Time Series Forecasting](https://arxiv.org/abs/2601.00698)
*Maximilian Reinwardt,Michael Eichelbeck,Matthias Althoff*

Main category: cs.LG

TL;DR: 提出B-Spline Adaptive Tokenizer (BSAT) 和L-RoPE，实验表明模型在高压缩率下表现好，适合内存受限场景。


<details>
  <summary>Details</summary>
Motivation: 解决transformers进行长期时间序列预测时自注意力二次复杂度和统一补丁刚性问题，避免与数据语义结构不一致。

Method: 引入无参数的BSAT方法，通过B样条拟合自适应分割时间序列，在高曲率区域放置令牌；提出结合可学习位置编码和具有层间可学习基的旋转位置嵌入的混合位置编码L-RoPE。

Result: 在多个公共基准测试中，模型在高压缩率下有竞争力的表现。

Conclusion: 模型特别适合内存约束强的用例。

Abstract: Long-term time series forecasting using transformers is hampered by the quadratic complexity of self-attention and the rigidity of uniform patching, which may be misaligned with the data's semantic structure. In this paper, we introduce the \textit{B-Spline Adaptive Tokenizer (BSAT)}, a novel, parameter-free method that adaptively segments a time series by fitting it with B-splines. BSAT algorithmically places tokens in high-curvature regions and represents each variable-length basis function as a fixed-size token, composed of its coefficient and position. Further, we propose a hybrid positional encoding that combines a additive learnable positional encoding with Rotary Positional Embedding featuring a layer-wise learnable base: L-RoPE. This allows each layer to attend to different temporal dependencies. Our experiments on several public benchmarks show that our model is competitive with strong performance at high compression rates. This makes it particularly well-suited for use cases with strong memory constraints.

</details>


### [107] [Precision Autotuning for Linear Solvers via Contextual Bandit-Based RL](https://arxiv.org/abs/2601.00728)
*Erin Carson,Xinye Chen*

Main category: cs.LG

TL;DR: 提出用于线性求解器自适应精度调整的强化学习框架，应用于迭代细化求解线性系统，有效选精度、降成本、保精度，可泛化。


<details>
  <summary>Details</summary>
Motivation: 平衡线性求解器精度与计算效率，推动科学计算中混合精度数值方法发展。

Method: 将框架建模为上下文多臂老虎机问题，用增量动作值估计和离散状态空间求解，通过Q表和epsilon - 贪心策略优化。

Result: 有效选择精度，降低计算成本，保持与双精度基线相当的精度，可泛化到多样样本外数据。

Conclusion: 该框架可推广到其他数值算法，是首个用强化学习进行精度自动调整并在未见数据集验证的工作。

Abstract: We propose a reinforcement learning (RL) framework for adaptive precision tuning of linear solvers, and can be extended to general algorithms. The framework is formulated as a contextual bandit problem and solved using incremental action-value estimation with a discretized state space to select optimal precision configurations for computational steps, balancing precision and computational efficiency. To verify its effectiveness, we apply the framework to iterative refinement for solving linear systems $Ax = b$. In this application, our approach dynamically chooses precisions based on calculated features from the system. In detail, a Q-table maps discretized features (e.g., approximate condition number and matrix norm)to actions (chosen precision configurations for specific steps), optimized via an epsilon-greedy strategy to maximize a multi-objective reward balancing accuracy and computational cost. Empirical results demonstrate effective precision selection, reducing computational cost while maintaining accuracy comparable to double-precision baselines. The framework generalizes to diverse out-of-sample data and offers insight into utilizing RL precision selection for other numerical algorithms, advancing mixed-precision numerical methods in scientific computing. To the best of our knowledge, this is the first work on precision autotuning with RL and verified on unseen datasets.

</details>


### [108] [Stochastic Actor-Critic: Mitigating Overestimation via Temporal Aleatoric Uncertainty](https://arxiv.org/abs/2601.00737)
*Uğurcan Özalp*

Main category: cs.LG

TL;DR: 提出随机演员 - 评论家算法（STAC），利用时间上的偶然不确定性来调整悲观偏差，用单分布评论家网络建模，结合dropout正则化，提升计算效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有离策略演员 - 评论家方法中评论家网络易系统高估价值估计，当前方法用集成量化认知不确定性来解决，本文想探索用偶然不确定性解决问题。

Method: 提出STAC算法，用单分布评论家网络建模时间回报不确定性，对评论家与演员网络应用dropout正则化。

Result: 仅基于分布评论家的悲观性就足以减轻高估问题，自然导致随机环境中的风险规避行为，引入dropout通过正则化提高训练稳定性和性能。

Conclusion: STAC算法利用偶然不确定性调整悲观偏差，结合单分布评论家网络和dropout正则化，实现了计算效率的提升。

Abstract: Off-policy actor-critic methods in reinforcement learning train a critic with temporal-difference updates and use it as a learning signal for the policy (actor). This design typically achieves higher sample efficiency than purely on-policy methods. However, critic networks tend to overestimate value estimates systematically. This is often addressed by introducing a pessimistic bias based on uncertainty estimates. Current methods employ ensembling to quantify the critic's epistemic uncertainty-uncertainty due to limited data and model ambiguity-to scale pessimistic updates. In this work, we propose a new algorithm called Stochastic Actor-Critic (STAC) that incorporates temporal (one-step) aleatoric uncertainty-uncertainty arising from stochastic transitions, rewards, and policy-induced variability in Bellman targets-to scale pessimistic bias in temporal-difference updates, rather than relying on epistemic uncertainty. STAC uses a single distributional critic network to model the temporal return uncertainty, and applies dropout to both the critic and actor networks for regularization. Our results show that pessimism based on a distributional critic alone suffices to mitigate overestimation, and naturally leads to risk-averse behavior in stochastic environments. Introducing dropout further improves training stability and performance by means of regularization. With this design, STAC achieves improved computational efficiency using a single distributional critic network.

</details>


### [109] [The Reasoning-Creativity Trade-off: Toward Creativity-Driven Problem Solving](https://arxiv.org/abs/2601.00747)
*Max Ruiz Luyten,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 分析大语言模型推理循环设计问题，提出DCR框架，给出相关定理和设计方法，让模型兼具正确性和创造性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型管道依赖自举推理循环，这种设计易导致推理路径分布崩溃，损害模型创造性解决问题的能力。

Method: 引入分布性创造性推理（DCR）统一变分目标，将训练视为解决方案轨迹概率测度上的梯度流。

Result: 得到多样性衰减定理，给出确保收敛到稳定多样策略的设计以及可实践的方法。

Conclusion: DCR为大语言模型既保证正确性又具备创造性提供了首个原则性方案。

Abstract: State-of-the-art large language model (LLM) pipelines rely on bootstrapped reasoning loops: sampling diverse chains of thought and reinforcing the highest-scoring ones, mainly optimizing correctness. We analyze how this design choice is sensitive to the collapse of the model's distribution over reasoning paths, slashing semantic entropy and undermining creative problem-solving. To analyze this failure, we introduce Distributional Creative Reasoning (DCR), a unified variational objective that casts training as gradient flow through probability measures on solution traces. STaR, GRPO, and DPO, as well as entropy bonuses, and other methods, all constitute special cases of the same loss. The framework delivers three core results: (i) the diversity decay theorem, describing how correctness-based objectives lead to distinct modes of diversity decay for STaR, GRPO, and DPO; (ii) designs that ensure convergence to a stable and diverse policy, effectively preventing collapse; and (iii) simple, actionable recipes to achieve this in practice. DCR thus offers the first principled recipe for LLMs that remain both correct and creative.

</details>


### [110] [A Machine Learning Framework for Off Ball Defensive Role and Performance Evaluation in Football](https://arxiv.org/abs/2601.00748)
*Sean Groom,Shuo Wang,Francisco Belo,Axl Rice,Liam Anderson*

Main category: cs.LG

TL;DR: 传统指标难评估足球无球防守表现，本文引入CDHMM模型提出新评估框架和反事实分析方法。


<details>
  <summary>Details</summary>
Motivation: 传统指标难以捕捉足球无球防守中细微的协同动作，现有反事实方法缺战术背景，需新方法评估。

Method: 引入针对角球的协变量依赖隐马尔可夫模型（CDHMM），从球员跟踪数据推断盯人和区域分配，提出防守贡献归因框架和角色条件幻影方法。

Result: 贡献能在考虑情境的基线基础上对防守贡献进行可解释评估。

Conclusion: 新提出的模型和方法可对足球无球防守表现进行有效评估。

Abstract: Evaluating off-ball defensive performance in football is challenging, as traditional metrics do not capture the nuanced coordinated movements that limit opponent action selection and success probabilities. Although widely used possession value models excel at appraising on-ball actions, their application to defense remains limited. Existing counterfactual methods, such as ghosting models, help extend these analyses but often rely on simulating "average" behavior that lacks tactical context. To address this, we introduce a covariate-dependent Hidden Markov Model (CDHMM) tailored to corner kicks, a highly structured aspect of football games. Our label-free model infers time-resolved man-marking and zonal assignments directly from player tracking data. We leverage these assignments to propose a novel framework for defensive credit attribution and a role-conditioned ghosting method for counterfactual analysis of off-ball defensive performance. We show how these contributions provide a interpretable evaluation of defensive contributions against context-aware baselines.

</details>


### [111] [Memory Bank Compression for Continual Adaptation of Large Language Models](https://arxiv.org/abs/2601.00756)
*Thomas Katraouras,Dimitrios Rafailidis*

Main category: cs.LG

TL;DR: 提出MBC模型，通过码本优化策略压缩内存库，引入在线重置机制，采用Key - Value Low - Rank Adaptation，实验表明能大幅减小内存库大小且保持高准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型知识易过时，持续学习方法存在计算成本高、易遗忘旧知识及内存库不断增大等问题，需改进。

Method: 提出MBC模型，用码本优化策略压缩内存库，引入在线重置机制，在注意力层采用Key - Value Low - Rank Adaptation。

Result: 实验显示，与最具竞争力的基线相比，MBC将内存库大小降至0.3%，在线适应学习时保持高保留准确率。

Conclusion: MBC模型能有效压缩内存库，在在线适应学习中表现良好，代码已公开。

Abstract: Large Language Models (LLMs) have become a mainstay for many everyday applications. However, as data evolve their knowledge quickly becomes outdated. Continual learning aims to update LLMs with new information without erasing previously acquired knowledge. Although methods such as full fine-tuning can incorporate new data, they are computationally expensive and prone to catastrophic forgetting, where prior knowledge is overwritten. Memory-augmented approaches address this by equipping LLMs with a memory bank, that is an external memory module which stores information for future use. However, these methods face a critical limitation, in particular, the memory bank constantly grows in the real-world scenario when large-scale data streams arrive. In this paper, we propose MBC, a model that compresses the memory bank through a codebook optimization strategy during online adaptation learning. To ensure stable learning, we also introduce an online resetting mechanism that prevents codebook collapse. In addition, we employ Key-Value Low-Rank Adaptation in the attention layers of the LLM, enabling efficient utilization of the compressed memory representations. Experiments with benchmark question-answering datasets demonstrate that MBC reduces the memory bank size to 0.3% when compared against the most competitive baseline, while maintaining high retention accuracy during online adaptation learning. Our code is publicly available at https://github.com/Thomkat/MBC.

</details>


### [112] [FedHypeVAE: Federated Learning with Hypernetwork Generated Conditional VAEs for Differentially Private Embedding Sharing](https://arxiv.org/abs/2601.00785)
*Sunny Gupta,Amit Sethi*

Main category: cs.LG

TL;DR: 提出FedHypeVAE框架用于跨分散客户端合成嵌入级数据，统一生成器级的个性化、隐私和分布对齐。


<details>
  <summary>Details</summary>
Motivation: 现有嵌入级生成器在非IID客户端异质性下表现不佳，且对梯度泄漏的正式保护有限。

Method: 构建基于条件VAE的FedHypeVAE，用共享超网络生成的客户端感知解码器和类条件先验替换单一全局解码器和固定潜在先验，在差分隐私下优化超网络，使用局部MMD对齐和Lipschitz正则化增强稳定性。

Result: 训练后，中性元代码可进行领域无关合成，元代码混合可提供可控多领域覆盖。

Conclusion: FedHypeVAE在生成器级别统一了个性化、隐私和分布对齐，为联邦环境下的隐私保护数据合成奠定了基础。

Abstract: Federated data sharing promises utility without centralizing raw data, yet existing embedding-level generators struggle under non-IID client heterogeneity and provide limited formal protection against gradient leakage. We propose FedHypeVAE, a differentially private, hypernetwork-driven framework for synthesizing embedding-level data across decentralized clients. Building on a conditional VAE backbone, we replace the single global decoder and fixed latent prior with client-aware decoders and class-conditional priors generated by a shared hypernetwork from private, trainable client codes. This bi-level design personalizes the generative layerrather than the downstream modelwhile decoupling local data from communicated parameters. The shared hypernetwork is optimized under differential privacy, ensuring that only noise-perturbed, clipped gradients are aggregated across clients. A local MMD alignment between real and synthetic embeddings and a Lipschitz regularizer on hypernetwork outputs further enhance stability and distributional coherence under non-IID conditions. After training, a neutral meta-code enables domain agnostic synthesis, while mixtures of meta-codes provide controllable multi-domain coverage. FedHypeVAE unifies personalization, privacy, and distribution alignment at the generator level, establishing a principled foundation for privacy-preserving data synthesis in federated settings. Code: github.com/sunnyinAI/FedHypeVAE

</details>


### [113] [Geometry of Reason: Spectral Signatures of Valid Mathematical Reasoning](https://arxiv.org/abs/2601.00791)
*Valentin Noël*

Main category: cs.LG

TL;DR: 提出无需训练的方法，通过注意力模式光谱分析检测大语言模型有效数学推理，在多模型实验中取得高准确率，揭示架构依赖，确立光谱图分析为推理验证框架。


<details>
  <summary>Details</summary>
Motivation: 检测大语言模型中有效的数学推理，避免模型产生无效推理或幻觉。

Method: 将注意力矩阵视为动态图邻接矩阵，提取四个可解释光谱诊断指标，设置单一光谱指标阈值进行分类。

Result: 光谱特征产生高达Cohen's d = 3.30的效应量，分类准确率达85.0 - 95.6%，校准阈值在全数据集上达93 - 95%；发现Mistral - 7B架构依赖。

Conclusion: 光谱图分析可作为推理验证的原则性框架，可应用于幻觉检测和AI安全监测。

Abstract: We present a training-free method for detecting valid mathematical reasoning in large language models through spectral analysis of attention patterns. By treating attention matrices as adjacency matrices of dynamic graphs over tokens, we extract four interpretable spectral diagnostics, the Fiedler value (algebraic connectivity), high-frequency energy ratio (HFER), graph signal smoothness, and spectral entropy, that exhibit statistically significant differences between valid and invalid mathematical proofs. Experiments across seven transformer models from four independent architectural families (Meta Llama, Alibaba Qwen, Microsoft Phi, and Mistral AI) demonstrate that this spectral signature produces effect sizes up to Cohen's $d = 3.30$ ($p < 10^{-116}$), enabling 85.0--95.6\% classification accuracy under rigorous evaluation, with calibrated thresholds reaching 93--95\% on the full dataset. The method requires no training data, fine-tuning, or learned classifiers: a single threshold on a spectral metric suffices for high accuracy. Through systematic label correction, we discover that the spectral method detects logical coherence rather than compiler acceptance, identifying mathematically valid proofs that formal verifiers reject due to technical failures. We further identify an architectural dependency: Mistral-7B's Sliding Window Attention shifts the discriminative signal from HFER to late-layer Smoothness ($d = 2.09$, $p_{\text{MW}} = 1.16 \times 10^{-48}$), revealing that attention mechanism design affects which spectral features capture reasoning validity. These findings establish spectral graph analysis as a principled framework for reasoning verification with immediate applications to hallucination detection and AI safety monitoring.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [114] [Personalized Spiking Neural Networks with Ferroelectric Synapses for EEG Signal Processing](https://arxiv.org/abs/2601.00020)
*Nikhil Garg,Anxiong Song,Niklas Plessnig,Nathan Savoia,Laura Bégon-Lours*

Main category: cs.NE

TL;DR: 本文展示了在铁电忆阻突触器件上部署脉冲神经网络（SNN），用于在现实器件约束下进行基于脑电图（EEG）的运动想象解码，两种部署策略表现良好，特定主体迁移学习可提升准确率。


<details>
  <summary>Details</summary>
Motivation: 基于脑电图的脑机接口受非平稳神经信号影响，限制通用模型泛化，忆阻硬件虽有潜力但面临诸多挑战，需探索解决方案。

Method: 制造、表征和建模铁电突触，评估卷积 - 循环SNN架构的两种部署策略，引入设备感知权重更新策略。

Result: 两种部署策略的分类性能与最先进的基于软件的SNN相当，特定主体迁移学习可提高分类准确率。

Conclusion: 可编程铁电硬件能支持脉冲神经网络进行鲁棒、低开销的自适应，为神经信号的个性化神经形态处理开辟了实用途径。

Abstract: Electroencephalography (EEG)-based brain-computer interfaces (BCIs) are strongly affected by non-stationary neural signals that vary across sessions and individuals, limiting the generalization of subject-agnostic models and motivating adaptive and personalized learning on resource-constrained platforms. Programmable memristive hardware offers a promising substrate for such post-deployment adaptation; however, practical realization is challenged by limited weight resolution, device variability, nonlinear programming dynamics, and finite device endurance. In this work, we show that spiking neural networks (SNNs) can be deployed on ferroelectric memristive synaptic devices for adaptive EEG-based motor imagery decoding under realistic device constraints. We fabricate, characterize, and model ferroelectric synapses. We evaluate a convolutional-recurrent SNN architecture under two complementary deployment strategies: (i) device-aware training using a ferroelectric synapse model, and (ii) transfer of software-trained weights followed by low-overhead on-device re-tuning. To enable efficient adaptation, we introduce a device-aware weight-update strategy in which gradient-based updates are accumulated digitally and converted into discrete programming events only when a threshold is exceeded, emulating nonlinear, state-dependent programming dynamics while reducing programming frequency. Both deployment strategies achieve classification performance comparable to state-of-the-art software-based SNNs. Furthermore, subject-specific transfer learning achieved by retraining only the final network layers improves classification accuracy. These results demonstrate that programmable ferroelectric hardware can support robust, low-overhead adaptation in spiking neural networks, opening a practical path toward personalized neuromorphic processing of neural signals.

</details>


### [115] [Covariance Matrix Adaptation Evolution Strategy without a matrix](https://arxiv.org/abs/2601.00102)
*Jarosław Arabas,Adam Stelmaszczyk,Eryk Warchulski,Dariusz Jagodziński,Rafał Biedrzycki*

Main category: cs.NE

TL;DR: 提出无矩阵CMA - ES，证明其生成个体概率分布与标准CMA - ES相同，实验表明该方法简化算法且效率高。


<details>
  <summary>Details</summary>
Motivation: 解决CMA - ES在高维空间中因协方差矩阵分解的三次复杂度而难以应用的问题。

Method: 引入无矩阵CMA - ES，用存档存储个体与中点差值向量，新个体由存档向量加权组合生成。

Result: 减少存档大小不影响优化效率；关闭步长自适应时与基于矩阵的CMA - ES效果相当；开启步长自适应时收敛更快、结果质量相当或更优。

Conclusion: 该方法简化算法，为协方差矩阵自适应提供新视角，是迈向更高效方法的垫脚石。

Abstract: Covariance Matrix Adaptation Evolution Strategy (CMA-ES) is a highly effective optimization technique. A primary challenge when applying CMA-ES in high dimensionality is sampling from a multivariate normal distribution with an arbitrary covariance matrix, which involves its decomposition. The cubic complexity of this process is the main obstacle to applying CMA-ES in highdimensional spaces. We introduce a version of CMA-ES that uses no covariance matrix at all. In the proposed matrix-free CMA-ES, an archive stores the vectors of differences between individuals and the midpoint, normalized by the step size. New individuals are generated as the weighted combinations of the vectors from the archive. We prove that the probability distribution of individuals generated by the proposed method is identical to that of the standard CMA-ES. Experimental results show that reducing the archive size to store only a fixed number of the most recent populations is sufficient, without compromising optimization efficiency. The matrix-free and matrix-based CMA-ES achieve comparable results on the quadratic function when the step-size adaptation is turned off. When coupled with the step-size adaptation method, the matrix-free CMA-ES converges faster than the matrix-based, and usually yields the results of a comparable or superior quality, according to the results obtained for the CEC'2017 benchmark suite. Presented approach simplifies the algorithm, offers a novel perspective on covariance matrix adaptation, and serves as a stepping stone toward even more efficient methods.

</details>


### [116] [Modern Neuromorphic AI: From Intra-Token to Inter-Token Processing](https://arxiv.org/abs/2601.00245)
*Osvaldo Simeone*

Main category: cs.NE

TL;DR: 人工智能发展带来能源挑战，激发对神经形态计算的兴趣。本文从token内和token间处理角度阐述神经形态模型等架构联系，介绍相关模型和训练方法。


<details>
  <summary>Details</summary>
Motivation: 人工智能快速发展带来能源需求增长的挑战，促使人们关注神经形态计算原理以实现类脑效率。

Method: 从token内和token间处理的区别角度，阐述神经形态模型、状态空间模型和Transformer架构的联系，介绍基于不同原理设计的处理方法，回顾训练方法。

Result: 系统呈现了现代神经形态AI模型，回顾了从代理梯度到基于强化学习机制的局部学习规则等多种训练方法。

Conclusion: 通过token内和token间处理视角，能有效阐述神经形态AI模型相关架构联系及训练方法。

Abstract: The rapid growth of artificial intelligence (AI) has brought novel data processing and generative capabilities but also escalating energy requirements. This challenge motivates renewed interest in neuromorphic computing principles, which promise brain-like efficiency through discrete and sparse activations, recurrent dynamics, and non-linear feedback. In fact, modern AI architectures increasingly embody neuromorphic principles through heavily quantized activations, state-space dynamics, and sparse attention mechanisms. This paper elaborates on the connections between neuromorphic models, state-space models, and transformer architectures through the lens of the distinction between intra-token processing and inter-token processing. Most early work on neuromorphic AI was based on spiking neural networks (SNNs) for intra-token processing, i.e., for transformations involving multiple channels, or features, of the same vector input, such as the pixels of an image. In contrast, more recent research has explored how neuromorphic principles can be leveraged to design efficient inter-token processing methods, which selectively combine different information elements depending on their contextual relevance. Implementing associative memorization mechanisms, these approaches leverage state-space dynamics or sparse self-attention. Along with a systematic presentation of modern neuromorphic AI models through the lens of intra-token and inter-token processing, training methodologies for neuromorphic AI models are also reviewed. These range from surrogate gradients leveraging parallel convolutional processing to local learning rules based on reinforcement learning mechanisms.

</details>


### [117] [RMAAT: Astrocyte-Inspired Memory Compression and Replay for Efficient Long-Context Transformers](https://arxiv.org/abs/2601.00426)
*Md Zesun Ahmed Mia,Malyaban Bal,Abhronil Sengupta*

Main category: cs.NE

TL;DR: 论文引入RMAAT架构，结合星形胶质细胞功能，采用新算法训练，在LRA基准测试中展现出计算和内存效率优势。


<details>
  <summary>Details</summary>
Motivation: 自注意力机制的二次复杂度阻碍Transformer模型应用于长序列，探索从星形胶质细胞获取计算原理以实现高效自注意力。

Method: 引入RMAAT架构，采用循环、分段处理策略，用新保留因子控制的自适应压缩机制调节记忆令牌，段内注意力用线性复杂度机制，使用AMRB算法训练。

Result: 在LRA基准测试中，RMAAT有有竞争力的准确率，计算和内存效率有显著提升。

Conclusion: 将星形胶质细胞启发的动力学融入可扩展序列模型有潜力。

Abstract: The quadratic complexity of self-attention mechanism presents a significant impediment to applying Transformer models to long sequences. This work explores computational principles derived from astrocytes-glial cells critical for biological memory and synaptic modulation-as a complementary approach to conventional architectural modifications for efficient self-attention. We introduce the Recurrent Memory Augmented Astromorphic Transformer (RMAAT), an architecture integrating abstracted astrocyte functionalities. RMAAT employs a recurrent, segment-based processing strategy where persistent memory tokens propagate contextual information. An adaptive compression mechanism, governed by a novel retention factor derived from simulated astrocyte long-term plasticity (LTP), modulates these tokens. Attention within segments utilizes an efficient, linear-complexity mechanism inspired by astrocyte short-term plasticity (STP). Training is performed using Astrocytic Memory Replay Backpropagation (AMRB), a novel algorithm designed for memory efficiency in recurrent networks. Evaluations on the Long Range Arena (LRA) benchmark demonstrate RMAAT's competitive accuracy and substantial improvements in computational and memory efficiency, indicating the potential of incorporating astrocyte-inspired dynamics into scalable sequence models.

</details>


### [118] [Benchmarking ERP Analysis: Manual Features, Deep Learning, and Foundation Models](https://arxiv.org/abs/2601.00573)
*Yihe Wang,Zhiqiao Kang,Bohan Chen,Yu Zhang,Xiang Zhang*

Main category: cs.NE

TL;DR: 本文对ERP分析中的传统手动特征、深度学习模型和预训练EEG基础模型进行综合基准研究，建立统一流程，在两个任务和12个数据集上评估，还研究Transformer架构的嵌入策略，提供框架并开源代码。


<details>
  <summary>Details</summary>
Motivation: 深度学习方法在ERP数据上的有效性未充分探索，现有ERP研究多依赖手动提取特征。

Method: 建立统一数据预处理和训练流程，在两个代表性任务、12个公开数据集上比较传统手动特征（接线性分类器）、深度学习模型和预训练EEG基础模型，研究Transformer架构的各种补丁嵌入策略。

Result: 完成多种方法在不同任务和数据集上的评估，探索了适合ERP数据的嵌入设计。

Conclusion: 为未来ERP分析提供了指导方法选择和定制模型设计的标志性框架。

Abstract: Event-related potential (ERP), a specialized paradigm of electroencephalographic (EEG), reflects neurological responses to external stimuli or events, generally associated with the brain's processing of specific cognitive tasks. ERP plays a critical role in cognitive analysis, the detection of neurological diseases, and the assessment of psychological states. Recent years have seen substantial advances in deep learning-based methods for spontaneous EEG and other non-time-locked task-related EEG signals. However, their effectiveness on ERP data remains underexplored, and many existing ERP studies still rely heavily on manually extracted features. In this paper, we conduct a comprehensive benchmark study that systematically compares traditional manual features (followed by a linear classifier), deep learning models, and pre-trained EEG foundation models for ERP analysis. We establish a unified data preprocessing and training pipeline and evaluate these approaches on two representative tasks, ERP stimulus classification and ERP-based brain disease detection, across 12 publicly available datasets. Furthermore, we investigate various patch-embedding strategies within advanced Transformer architectures to identify embedding designs that better suit ERP data. Our study provides a landmark framework to guide method selection and tailored model design for future ERP analysis. The code is available at https://github.com/DL4mHealth/ERP-Benchmark.

</details>


### [119] [Three factor delay learning rules for spiking neural networks](https://arxiv.org/abs/2601.00668)
*Luke Vassallo,Nima Taherinejad*

Main category: cs.NE

TL;DR: 本文为基于LIF的前馈和递归SNN引入突触和轴突延迟，提出三因素学习规则在线学习延迟参数，实验表明该方法能提升准确率，减小模型大小和推理延迟。


<details>
  <summary>Details</summary>
Motivation: 现有SNN可学习参数常限于突触权重，对时间模式识别贡献小，且学习延迟参数的方法依赖大网络和离线学习，不适合资源受限环境实时操作。

Method: 为基于LIF的前馈和递归SNN引入突触和轴突延迟，提出三因素学习规则在线学习延迟参数，用平滑高斯替代近似尖峰导数用于资格迹计算，结合自上而下的误差信号确定参数更新。

Result: 引入延迟比仅用权重的基线准确率最高提升20%，联合学习权重和延迟在参数数量相似的网络中准确率最高提升14%；在SHD语音识别数据集上与基于离线反向传播的方法准确率相近；相比现有方法，模型大小缩小6.6倍，推理延迟降低67%，分类准确率仅下降2.4%。

Conclusion: 该方法有利于设计功率和面积受限的神经形态处理器，支持设备端学习并降低内存需求。

Abstract: Spiking Neural Networks (SNNs) are dynamical systems that operate on spatiotemporal data, yet their learnable parameters are often limited to synaptic weights, contributing little to temporal pattern recognition. Learnable parameters that delay spike times can improve classification performance in temporal tasks, but existing methods rely on large networks and offline learning, making them unsuitable for real-time operation in resource-constrained environments. In this paper, we introduce synaptic and axonal delays to leaky integrate and fire (LIF)-based feedforward and recurrent SNNs, and propose three-factor learning rules to simultaneously learn delay parameters online. We employ a smooth Gaussian surrogate to approximate spike derivatives exclusively for the eligibility trace calculation, and together with a top-down error signal determine parameter updates. Our experiments show that incorporating delays improves accuracy by up to 20% over a weights-only baseline, and for networks with similar parameter counts, jointly learning weights and delays yields up to 14% higher accuracy. On the SHD speech recognition dataset, our method achieves similar accuracy to offline backpropagation-based approaches. Compared to state-of-the-art methods, it reduces model size by 6.6x and inference latency by 67%, with only a 2.4% drop in classification accuracy. Our findings benefit the design of power and area-constrained neuromorphic processors by enabling on-device learning and lowering memory requirements.

</details>


### [120] [QSLM: A Performance- and Memory-aware Quantization Framework with Tiered Search Strategy for Spike-driven Language Models](https://arxiv.org/abs/2601.00679)
*Rachmad Vidya Wicaksana Putra,Pasindu Wickramasinghe,Muhammad Shafique*

Main category: cs.NE

TL;DR: 文章提出QSLM框架自动量化压缩预训练SLM，实验表明可减少内存和功耗，且保持高性能。


<details>
  <summary>Details</summary>
Motivation: LLMs计算成本高、内存占用大，SLMs内存占用仍大，手动量化不可扩展，需自动量化方法。

Method: QSLM先识别网络架构层次和层的量化敏感性，采用分层量化策略，利用多目标权衡函数选最终量化设置。

Result: QSLM最多减少86.5%内存占用，最多降低20%功耗，在不同任务保持高性能。

Conclusion: QSLM能有效压缩预训练SLM，满足性能和内存约束。

Abstract: Large Language Models (LLMs) have been emerging as prominent AI models for solving many natural language tasks due to their high performance (e.g., accuracy) and capabilities in generating high-quality responses to the given inputs. However, their large computational cost, huge memory footprints, and high processing power/energy make it challenging for their embedded deployments. Amid several tinyLLMs, recent works have proposed spike-driven language models (SLMs) for significantly reducing the processing power/energy of LLMs. However, their memory footprints still remain too large for low-cost and resource-constrained embedded devices. Manual quantization approach may effectively compress SLM memory footprints, but it requires a huge design time and compute power to find the quantization setting for each network, hence making this approach not-scalable for handling different networks, performance requirements, and memory budgets. To bridge this gap, we propose QSLM, a novel framework that performs automated quantization for compressing pre-trained SLMs, while meeting the performance and memory constraints. To achieve this, QSLM first identifies the hierarchy of the given network architecture and the sensitivity of network layers under quantization, then employs a tiered quantization strategy (e.g., global-, block-, and module-level quantization) while leveraging a multi-objective performance-and-memory trade-off function to select the final quantization setting. Experimental results indicate that our QSLM reduces memory footprint by up to 86.5%, reduces power consumption by up to 20%, maintains high performance across different tasks (i.e., by up to 84.4% accuracy of sentiment classification on the SST-2 dataset and perplexity score of 23.2 for text generation on the WikiText-2 dataset) close to the original non-quantized model while meeting the performance and memory constraints.

</details>


### [121] [Cost Optimization in Production Line Using Genetic Algorithm](https://arxiv.org/abs/2601.00689)
*Alireza Rezaee*

Main category: cs.NE

TL;DR: 本文提出用遗传算法解决生产线成本最优任务调度问题，研究两种染色体编码策略，实验表明任务编码策略更优，凸显遗传算法在组合调度问题中的优势。


<details>
  <summary>Details</summary>
Motivation: 解决生产线任务调度中最小化总成本问题，满足前提和容量约束。

Method: 采用遗传算法，研究基于工作站和基于任务的两种染色体编码策略，适配标准遗传算子。

Result: 实验表明任务编码策略在有效调度数量大时收敛更平滑，成本最小化更可靠。

Conclusion: 遗传算法在组合调度问题上优于基于梯度和解析方法，尤其在复杂约束和不可微成本情况下。

Abstract: This paper presents a genetic algorithm (GA) approach to cost-optimal task scheduling in a production line. The system consists of a set of serial processing tasks, each with a given duration, unit execution cost, and precedence constraints, which must be assigned to an unlimited number of stations subject to a per-station duration bound. The objective is to minimize the total production cost, modeled as a station-wise function of task costs and the duration bound, while strictly satisfying all prerequisite and capacity constraints. Two chromosome encoding strategies are investigated: a station-based representation implemented using the JGAP library with SuperGene validity checks, and a task-based representation in which genes encode station assignments directly. For each encoding, standard GA operators (crossover, mutation, selection, and replacement) are adapted to preserve feasibility and drive the population toward lower-cost schedules. Experimental results on three classes of precedence structures-tightly coupled, loosely coupled, and uncoupled-demonstrate that the task-based encoding yields smoother convergence and more reliable cost minimization than the station-based encoding, particularly when the number of valid schedules is large. The study highlights the advantages of GA over gradient-based and analytical methods for combinatorial scheduling problems, especially in the presence of complex constraints and non-differentiable cost landscapes.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [122] [Understanding Security Risks of AI Agents' Dependency Updates](https://arxiv.org/abs/2601.00205)
*Tanmay Singla,Berk Çakar,Paschal C. Amusuo,James C. Davis*

Main category: cs.SE

TL;DR: 研究表明AI编码代理在依赖项更改时比人类更易引入安全风险，建议进行拉取请求时的漏洞筛查和设置防护栏。


<details>
  <summary>Details</summary>
Motivation: 随着AI编码代理通过拉取请求修改软件，需明确其依赖项决策是否会引入独特安全风险。

Method: 研究七个生态系统中由代理和人类撰写的拉取请求的117,062个依赖项更改。

Result: 代理比人类更常选择已知易受攻击的版本，其易受攻击的选择修复起来更具破坏性，总体上代理驱动的依赖项工作使漏洞净增加，而人类工作使漏洞净减少。

Conclusion: 建议进行拉取请求时的漏洞筛查和设置感知注册表的防护栏，使代理驱动的依赖项更新更安全。

Abstract: Package dependencies are a critical control point in modern software supply chains. Dependency changes can substantially alter a project's security posture. As AI coding agents increasingly modify software via pull requests, it is unclear whether their dependency decisions introduce distinct security risks.
  We study 117,062 dependency changes from agent- and human-authored pull requests across seven ecosystems. Agents select known-vulnerable versions more often than humans (2.46% vs. 1.64%), and their vulnerable selections are more disruptive to remediate, with 36.8% requiring major-version upgrades compared to 12.9% for humans, despite patched alternatives existing in most cases. At the aggregate level, agent-driven dependency work yields a net vulnerability increase of 98, whereas human-authored work yields a net reduction of 1,316. These findings motivate pull-request-time vulnerability screening and registry-aware guardrails to make agent-driven dependency updates safer.

</details>


### [123] [Advanced Vulnerability Scanning for Open Source Software: Detection and Mitigation of Log4j Vulnerabilities](https://arxiv.org/abs/2601.00235)
*Victor Wen,Zedong Peng*

Main category: cs.SE

TL;DR: 本文开发了先进的Log4j扫描工具，可评估软件实际可利用性，减少误报，通过GitHub Actions实现自动化和持续扫描，经评估准确率达91.4%。


<details>
  <summary>Details</summary>
Motivation: 现有Log4Shell扫描工具主要关注Log4j版本，导致大量误报，需开发能评估软件实际可利用性的扫描工具。

Method: 先识别漏洞，再提供缓解建议和即时反馈，利用GitHub Actions实现自动化和持续扫描，集成到现有开发流程。

Result: 评估28个开源软件项目不同版本，140次扫描准确率达91.4%，GitHub action实现可在GitHub市场获取。

Conclusion: 该工具为检测和缓解开源项目漏洞提供可靠方法。

Abstract: Automated detection of software vulnerabilities remains a critical challenge in software security. Log4j is an industrial-grade Java logging framework listed as one of the top 100 critical open source projects. On Dec. 10, 2021 a severe vulnerability Log4Shell was disclosed before being fully patched with Log4j2 version 2.17.0 on Dec. 18, 2021. However, to this day about 4.1 million, or 33 percent of all Log4j downloads in the last 7 days contain vulnerable packages. Many Log4Shell scanners have since been created to detect if a user's installed Log4j version is vulnerable. Current detection tools primarily focus on identifying the version of Log4j installed, leading to numerous false positives, as they do not check if the software scanned is really vulnerable to malicious actors. This research aims to develop an advanced Log4j scanning tool that can evaluate the real-world exploitability of the software, thereby reducing false positives. Our approach first identifies vulnerabilities and then provides targeted recommendations for mitigating these detected vulnerabilities, along with instant feedback to users. By leveraging GitHub Actions, our tool offers automated and continuous scanning capabilities, ensuring timely identification of vulnerabilities as code changes occur. This integration into existing development workflows enables real-time monitoring and quicker responses to potential threats. We demonstrate the effectiveness of our approach by evaluating 28 open-source software projects across different releases, achieving an accuracy rate of 91.4% from a sample of 140 scans. Our GitHub action implementation is available at the GitHub marketplace and can be accessed by anyone interested in improving their software security and for future studies. This tool provides a dependable way to detect and mitigate vulnerabilities in open-source projects.

</details>


### [124] [An Empirical Evaluation of LLM-Based Approaches for Code Vulnerability Detection: RAG, SFT, and Dual-Agent Systems](https://arxiv.org/abs/2601.00254)
*Md Hasan Saju,Maher Muhtadi,Akramul Azim*

Main category: cs.SE

TL;DR: 本文对基于大语言模型（LLM）的软件漏洞检测技术进行比较研究，评估三种方法，RAG 表现最佳，强调融入领域专业知识机制可增强 LLM 在漏洞检测中的实用性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的快速发展为自动化软件漏洞检测带来新机遇，开展基于 LLM 的软件漏洞检测技术有效性的比较研究。

Method: 评估 Retrieval - Augmented Generation (RAG)、Supervised Fine - Tuning (SFT) 和 Dual - Agent LLM 框架三种方法，与基线 LLM 模型对比，使用来自 Big - Vul 和 GitHub 的数据集，聚焦五个 CWE 类别。

Result: RAG 方法整体准确率 0.86、F1 分数 0.85 最高；SFT 表现良好；Dual - Agent 系统在提高推理透明度、减少错误和降低资源开销方面有潜力。

Conclusion: 融入领域专业知识机制能显著增强 LLM 在现实世界漏洞检测任务中的实际应用能力。

Abstract: The rapid advancement of Large Language Models (LLMs) presents new opportunities for automated software vulnerability detection, a crucial task in securing modern codebases. This paper presents a comparative study on the effectiveness of LLM-based techniques for detecting software vulnerabilities. The study evaluates three approaches, Retrieval-Augmented Generation (RAG), Supervised Fine-Tuning (SFT), and a Dual-Agent LLM framework, against a baseline LLM model. A curated dataset was compiled from Big-Vul and real-world code repositories from GitHub, focusing on five critical Common Weakness Enumeration (CWE) categories: CWE-119, CWE-399, CWE-264, CWE-20, and CWE-200. Our RAG approach, which integrated external domain knowledge from the internet and the MITRE CWE database, achieved the highest overall accuracy (0.86) and F1 score (0.85), highlighting the value of contextual augmentation. Our SFT approach, implemented using parameter-efficient QLoRA adapters, also demonstrated strong performance. Our Dual-Agent system, an architecture in which a secondary agent audits and refines the output of the first, showed promise in improving reasoning transparency and error mitigation, with reduced resource overhead. These results emphasize that incorporating a domain expertise mechanism significantly strengthens the practical applicability of LLMs in real-world vulnerability detection tasks.

</details>


### [125] [In Line with Context: Repository-Level Code Generation via Context Inlining](https://arxiv.org/abs/2601.00376)
*Chao Hu,Wenhao Zeng,Yuling Shi,Beijun Shen,Xiaodong Gu*

Main category: cs.SE

TL;DR: 介绍用于仓库级代码生成的InlineCoder框架，通过内联未完成函数到调用图，将任务转化为函数级编码任务。


<details>
  <summary>Details</summary>
Motivation: 现有仓库级代码生成方法如RAG等依靠表面相似性，难以捕捉丰富依赖关系。

Method: 将未完成函数内联到调用图，根据函数签名生成初稿(anchor)，驱动双向内联过程，即上游内联和下游检索以丰富上下文。

Result: 未明确提及结果

Conclusion: 未明确提及结论

Abstract: Repository-level code generation has attracted growing attention in recent years. Unlike function-level code generation, it requires the model to understand the entire repository, reasoning over complex dependencies across functions, classes, and modules. However, existing approaches such as retrieval-augmented generation (RAG) or context-based function selection often fall short: they primarily rely on surface-level similarity and struggle to capture the rich dependencies that govern repository-level semantics. In this paper, we introduce InlineCoder, a novel framework for repository-level code generation. InlineCoder enhances the understanding of repository context by inlining the unfinished function into its call graph, thereby reframing the challenging repository understanding as an easier function-level coding task. Given a function signature, InlineCoder first generates a draft completion, termed an anchor, which approximates downstream dependencies and enables perplexity-based confidence estimation. This anchor drives a bidirectional inlining process: (i) Upstream Inlining, which embeds the anchor into its callers to capture diverse usage scenarios; and (ii) Downstream Retrieval, which integrates the anchor's callees into the prompt to provide precise dependency context. The enriched context, combining draft completion with upstream and downstream perspectives, equips the LLM with a comprehensive repository view.

</details>


### [126] [On Plagiarism and Software Plagiarism](https://arxiv.org/abs/2601.00429)
*Rares Folea,Emil Slusanschi*

Main category: cs.SE

TL;DR: 文章探讨软件相似性自动检测复杂性，介绍开源方案Project Martial，列举反软件抄袭方法，对检测挑战分类并调研相关技术，举例其在Project Martial中的应用。


<details>
  <summary>Details</summary>
Motivation: 应对数字制品软件相似性自动检测的独特挑战，解决软件抄袭问题。

Method: 研究学术界和法律界现有反软件抄袭方法，依据可用制品对检测挑战分类，调研文献中相关技术。

Result: 引入开源软件解决方案Project Martial，对检测挑战进行分类，调研了基于指纹、软件胎记或代码嵌入等技术。

Conclusion: 未明确提及，但暗示Project Martial结合部分已有技术可用于软件相似性检测。

Abstract: This paper explores the complexities of automatic detection of software similarities, in relation to the unique challenges of digital artifacts, and introduces Project Martial, an open-source software solution for detecting code similarity. This research enumerates some of the existing approaches to counter software plagiarism by examining both the academia and legal landscape, including notable lawsuits and court rulings that have shaped the understanding of software copyright infringements in commercial applications. Furthermore, we categorize the classes of detection challenges based on the available artifacts, and we provide a survey of the previously studied techniques in the literature, including solutions based on fingerprinting, software birthmarks, or code embeddings, and exemplify how a subset of them can be applied in the context of Project Martial.

</details>


### [127] [DSL or Code? Evaluating the Quality of LLM-Generated Algebraic Specifications: A Case Study in Optimization at Kinaxis](https://arxiv.org/abs/2601.00469)
*Negin Ayoughi,David Dewar,Shiva Nejati,Mehrdad Sabetzadeh*

Main category: cs.SE

TL;DR: 研究用大语言模型从自然语言描述生成数学优化领域特定语言AMPL模型，提出EXEOS方法，评估显示AMPL与Python有竞争力。


<details>
  <summary>Details</summary>
Motivation: 模型驱动工程中开发和维护模型成本高，大语言模型虽可助力，但特定领域语言生成模型准确性可能不如主流语言，需研究解决。

Method: 引入基于大语言模型的EXEOS方法，从自然语言问题描述推导AMPL模型和Python代码，并结合求解器反馈迭代优化。

Result: 使用公共优化数据集和实际供应链案例评估，消融实验表明AMPL与Python有竞争力，有时表现更好，EXEOS设计提升了生成规范质量。

Conclusion: 在数学优化领域，EXEOS方法能有效利用大语言模型生成AMPL模型，其生成的AMPL模型质量有保障。

Abstract: Model-driven engineering (MDE) provides abstraction and analytical rigour, but industrial adoption in many domains has been limited by the cost of developing and maintaining models. Large language models (LLMs) can help shift this cost balance by supporting direct generation of models from natural-language (NL) descriptions. For domain-specific languages (DSLs), however, LLM-generated models may be less accurate than LLM-generated code in mainstream languages such as Python, due to the latter's dominance in LLM training corpora. We investigate this issue in mathematical optimization, with AMPL, a DSL with established industrial use. We introduce EXEOS, an LLM-based approach that derives AMPL models and Python code from NL problem descriptions and iteratively refines them with solver feedback. Using a public optimization dataset and real-world supply-chain cases from our industrial partner Kinaxis, we evaluate generated AMPL models against Python code in terms of executability and correctness. An ablation study with two LLM families shows that AMPL is competitive with, and sometimes better than, Python, and that our design choices in EXEOS improve the quality of generated specifications.

</details>


### [128] [Multi-Agent Coordinated Rename Refactoring](https://arxiv.org/abs/2601.00482)
*Abhiram Bellur,Mohammed Raihan Ullah,Fraol Batole,Mohit Kansara,Masaharu Morimoto,Kai Ishikawa,Haifeng Chen,Yaroslav Zharov,Timofey Bryksin,Tien N. Nguyen,Hridesh Rajan,Danny Dig*

Main category: cs.SE

TL;DR: 本文设计、实现并评估了首个自动化协调重命名的多智能体框架，以减轻开发者负担。


<details>
  <summary>Details</summary>
Motivation: 协调重命名是常见且具挑战性的任务，人工操作繁琐易错，现有启发式方法和大语言模型存在不足，需要借助智能体减轻开发者负担。

Method: 设计多智能体框架，Scope Inference Agent将开发者初始重构线索转化为声明范围，Planned Execution Agent依据其识别重构元素并执行更改，Replication Agent进行项目范围搜索。还对开源项目提交记录进行研究并调查开发者。

Result: 文中未明确提及具体结果。

Conclusion: 未明确提及，但暗示多智能体框架可有效处理协调重命名任务，减轻开发者负担。

Abstract: The primary value of AI agents in software development lies in their ability to extend the developer's capacity for reasoning and action, not to supplant human involvement. To showcase how to use agents working in tandem with developers, we designed a novel approach for carrying out coordinated renaming. Coordinated renaming, where a single rename refactoring triggers refactorings in multiple, related identifiers, is a frequent yet challenging task. Developers must manually propagate these rename refactorings across numerous files and contexts, a process that is both tedious and highly error-prone. State-of-the-art heuristic-based approaches produce an overwhelming number of false positives, while vanilla Large Language Models (LLMs) provide incomplete suggestions due to their limited context and inability to interact with refactoring tools. This leaves developers with incomplete refactorings or burdens them with filtering too many false positives. Coordinated renaming is exactly the kind of repetitive task that agents can significantly reduce the developers' burden while keeping them in the driver's seat.
  We designed, implemented, and evaluated the first multi-agent framework that automates coordinated renaming. It operates on a key insight: a developer's initial refactoring is a clue to infer the scope of related refactorings. Our Scope Inference Agent first transforms this clue into an explicit, natural-language Declared Scope. The Planned Execution Agent then uses this as a strict plan to identify program elements that should undergo refactoring and safely executes the changes by invoking the IDE's own trusted refactoring APIs. Finally, the Replication Agent uses it to guide the project-wide search. We first conducted a formative study on the practice of coordinated renaming in 609K commits in 100 open-source projects and surveyed 205 developers ...

</details>


### [129] [STELLAR: A Search-Based Testing Framework for Large Language Model Applications](https://arxiv.org/abs/2601.00497)
*Lev Sorokin,Ivan Vasilev,Ken E. Friedl,Andrea Stocco*

Main category: cs.SE

TL;DR: 介绍了自动化搜索测试框架STELLAR，用于测试大模型应用，能比现有方法更多地暴露系统故障。


<details>
  <summary>Details</summary>
Motivation: 大语言模型应用在各领域广泛部署，但易产生不准确、虚构或有害响应，且其输入空间大，系统测试挑战大。

Method: 将测试生成建模为优化问题，将输入空间离散为风格、内容相关和扰动特征，采用进化优化动态探索特征组合。

Result: 在三个大语言模型对话问答系统上评估，STELLAR暴露的故障最多达现有基线方法的4.3倍，平均2.5倍。

Conclusion: STELLAR能更有效地为大语言模型应用发现问题。

Abstract: Large Language Model (LLM)-based applications are increasingly deployed across various domains, including customer service, education, and mobility. However, these systems are prone to inaccurate, fictitious, or harmful responses, and their vast, high-dimensional input space makes systematic testing particularly challenging. To address this, we present STELLAR, an automated search-based testing framework for LLM-based applications that systematically uncovers text inputs leading to inappropriate system responses. Our framework models test generation as an optimization problem and discretizes the input space into stylistic, content-related, and perturbation features. Unlike prior work that focuses on prompt optimization or coverage heuristics, our work employs evolutionary optimization to dynamically explore feature combinations that are more likely to expose failures. We evaluate STELLAR on three LLM-based conversational question-answering systems. The first focuses on safety, benchmarking both public and proprietary LLMs against malicious or unsafe prompts. The second and third target navigation, using an open-source and an industrial retrieval-augmented system for in-vehicle venue recommendations. Overall, STELLAR exposes up to 4.3 times (average 2.5 times) more failures than the existing baseline approaches.

</details>


### [130] [SEMODS: A Validated Dataset of Open-Source Software Engineering Models](https://arxiv.org/abs/2601.00635)
*Alexandra González,Xavier Franch,Silverio Martínez-Fernández*

Main category: cs.SE

TL;DR: 提出SEMODS：从Hugging Face提取3427个模型的软件工程聚焦数据集，支持多应用。


<details>
  <summary>Details</summary>
Motivation: 整合人工智能到软件工程需要适合软件工程任务的模型集合，Hugging Face上模型众多，需专用目录识别软件工程模型。

Method: 结合自动收集、人工注释和大语言模型辅助进行严格验证，从Hugging Face提取模型。

Result: 创建了SEMODS数据集，将模型与软件工程任务和活动关联，提供评估结果标准化表示。

Conclusion: SEMODS数据集可支持数据分析、模型发现、基准测试和模型适配等多种应用。

Abstract: Integrating Artificial Intelligence into Software Engineering (SE) requires having a curated collection of models suited to SE tasks. With millions of models hosted on Hugging Face (HF) and new ones continuously being created, it is infeasible to identify SE models without a dedicated catalogue. To address this gap, we present SEMODS: an SE-focused dataset of 3,427 models extracted from HF, combining automated collection with rigorous validation through manual annotation and large language model assistance. Our dataset links models to SE tasks and activities from the software development lifecycle, offering a standardized representation of their evaluation results, and supporting multiple applications such as data analysis, model discovery, benchmarking, and model adaptation.

</details>


### [131] [Early-Stage Prediction of Review Effort in AI-Generated Pull Requests](https://arxiv.org/abs/2601.00753)
*Dao Sy Duy Minh,Huynh Trung Kiet,Tran Chi Nguyen,Nguyen Lam Phu Quy,Phu Hoa Pham,Nguyen Dinh Ha Duong,Truong Bao Tran*

Main category: cs.SE

TL;DR: 研究围绕AI生成的代码拉取请求（PR）展开，分析其行为模式，提出预测高审查工作量PR的模型，表明审查负担取决于代码结构。


<details>
  <summary>Details</summary>
Motivation: 随着自主AI代理成为软件团队成员，软件维护者面临管理与非人类贡献者的交互挑战，需预测哪些代理生成的PR会消耗过多审查精力。

Method: 分析AIDev数据集中2807个代码库的33707个代理创建的PR，发现两种行为模式；提出Circuit Breaker分类模型，仅使用静态结构特征预测高审查工作量PR。

Result: 发现两类PR行为模式，LightGBM模型在时间分割上AUC达0.957，语义文本特征预测价值低，在20%审查预算下可拦截69%审查工作量。

Conclusion: 挑战AI辅助代码审查的流行假设，表明审查负担由代码结构决定，强调人机协作中结构治理机制的必要性。

Abstract: As autonomous AI agents transition from code completion tools to full-fledged teammates capable of opening pull requests (PRs) at scale, software maintainers face a new challenge: not just reviewing code, but managing complex interaction loops with non-human contributors. This paradigm shift raises a critical question: can we predict which agent-generated PRs will consume excessive review effort before any human interaction begins?
  Analyzing 33,707 agent-authored PRs from the AIDev dataset across 2,807 repositories, we uncover a striking two-regime behavioral pattern that fundamentally distinguishes autonomous agents from human developers. The first regime, representing 28.3 percent of all PRs, consists of instant merges (less than 1 minute), reflecting success on narrow automation tasks. The second regime involves iterative review cycles where agents frequently stall or abandon refinement (ghosting).
  We propose a Circuit Breaker triage model that predicts high-review-effort PRs (top 20 percent) at creation time using only static structural features. A LightGBM model achieves AUC 0.957 on a temporal split, while semantic text features (TF-IDF, CodeBERT) provide negligible predictive value. At a 20 percent review budget, the model intercepts 69 percent of total review effort, enabling zero-latency governance.
  Our findings challenge prevailing assumptions in AI-assisted code review: review burden is dictated by what agents touch, not what they say, highlighting the need for structural governance mechanisms in human-AI collaboration.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [132] [Full grid solution for multi-asset options pricing with tensor networks](https://arxiv.org/abs/2601.00009)
*Lucas Arenstein,Michael Kastoryano*

Main category: q-fin.CP

TL;DR: 传统方法求解多资产期权定价受维度诅咒限制，本文表明QTT可将d资产Black - Scholes PDE转化为个人电脑可处理的高维问题，并构建两种求解器，能高精度计算3 - 5维期权价格和希腊值，经优化可处理10 - 15个标的资产。


<details>
  <summary>Details</summary>
Motivation: 传统求解多资产期权定价的方法受维度诅咒限制，经典全网格求解器对标的资产数量有有效限制，从业者依赖蒙特卡罗方法，需寻找更好的解决高维问题的方法。

Method: 使用量化张量列车（QTT）构建算子、收益和边界条件的表示，构建时间步进算法（用于欧式和美式期权）和时空算法（用于欧式期权）。

Result: 高精度计算3 - 5维相关篮子和最大 - 最小期权的全网格价格和希腊值。

Conclusion: QTT能将多资产Black - Scholes PDE转化为可处理的高维问题，经算法优化和增加计算能力，可处理10 - 15个标的资产的全网格解。

Abstract: Pricing multi-asset options via the Black-Scholes PDE is limited by the curse of dimensionality: classical full-grid solvers scale exponentially in the number of underlyings and are effectively restricted to three assets. Practitioners typically rely on Monte Carlo methods for computing complex instrument involving multiple correlated underlyings. We show that quantized tensor trains (QTT) turn the d-asset Black-Scholes PDE into a tractable high-dimensional problem on a personal computer. We construct QTT representations of the operator, payoffs, and boundary conditions with ranks that scale polynomially in d and polylogarithmically in the grid size, and build two solvers: a time-stepping algorithm for European and American options and a space-time algorithm for European options. We compute full-grid prices and Greeks for correlated basket and max-min options in three to five dimensions with high accuracy. The methods introduced can comfortably be pushed to full-grid solutions on 10-15 underlyings, with further algorithmic optimization and more compute power.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [133] [Uncertainty-Adjusted Sorting for Asset Pricing with Machine Learning](https://arxiv.org/abs/2601.00593)
*Yan Liu,Ye Luo,Zigan Wang,Xiaowei Zhang*

Main category: q-fin.PM

TL;DR: 提出用不确定性调整预测区间排序资产，能提升投资组合表现。


<details>
  <summary>Details</summary>
Motivation: 机器学习在实证资产定价中重要，但投资组合构建依赖点预测，忽略资产特定估计不确定性。

Method: 用不确定性调整预测区间对资产排序，而非仅用点预测。

Result: 在多种机器学习模型和美国股票面板数据中，该方法提升了投资组合表现，收益持续，主要源于降低波动率，对灵活机器学习模型效果最佳。

Conclusion: 投资组合表现的提升由资产层面预测不确定性驱动，而非时间或总体预测不确定性。

Abstract: Machine learning is central to empirical asset pricing, but portfolio construction still relies on point predictions and largely ignores asset-specific estimation uncertainty. We propose a simple change: sort assets using uncertainty-adjusted prediction bounds instead of point predictions alone. Across a broad set of ML models and a U.S. equity panel, this approach improves portfolio performance relative to point-prediction sorting. These gains persist even when bounds are built from partial or misspecified uncertainty information. They arise mainly from reduced volatility and are strongest for flexible machine learning models. Identification and robustness exercises show that these improvements are driven by asset-level rather than time or aggregate predictive uncertainty.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [134] [Option Pricing beyond Black-Scholes Model:Quantum Mechanics Approach](https://arxiv.org/abs/2601.00293)
*Pengpeng Li,Shi-Dong Liang*

Main category: q-fin.RM

TL;DR: 基于随机动力学与量子谐振子的类比，提出市场力驱动模型推广Black - Scholes模型，给出新期权定价方案，有实际应用。


<details>
  <summary>Details</summary>
Motivation: 推广金融市场中的Black - Scholes模型，考虑各种意外市场行为来修正期权定价。

Method: 基于随机动力学与量子谐振子的类比，提出市场力驱动模型。

Result: 给出新的期权定价方案，分析几种市场力对期权定价的影响。

Conclusion: 可作为预测隐藏市场力或行为时的新期权定价方案，意外市场力出现时存在风险溢价。

Abstract: Based on the analog between the stochastic dynamics and quantum harmonic oscillator, we propose a market force driving model to generalize the Black-Scholes model in finance market. We give new schemes of option pricing, in which we can take various unexpected market behaviors into account to modify the option pricing. As examples, we present several market forces to analyze their effects on the option pricing. These results provide us two practical applications. One is to be used as a new scheme of option pricing when we can predict some hidden market forces or behaviors emerging. The other implies the existence of some risk premium when some unexpected forces emerge.

</details>


### [135] [Multimodal Insights into Credit Risk Modelling: Integrating Climate and Text Data for Default Prediction](https://arxiv.org/abs/2601.00478)
*Zongxiao Wu,Ran Liu,Jiang Dai,Dan Luo*

Main category: q-fin.RM

TL;DR: 本文提出多模态框架评估小微企业信用风险，结合多种数据，多模态模型效果更佳，且发现气候物理风险重要。


<details>
  <summary>Details</summary>
Motivation: 小微企业财务历史有限，信用风险评估需借助传统财务数据外的多元信息。

Method: 提出多模态框架，整合结构化信用变量、气候面板数据和非结构化文本叙述，用LSTM、GRU和transformer模型分析数据模态间相互作用，用SHAP可解释性方法。

Result: 基于气候或文本数据的单模态模型优于仅依赖结构化数据的模型，多模态数据整合显著提升信用违约预测效果，发现物理气候风险对违约预测重要，暴雨内涝影响最大。

Conclusion: 多模态方法在人工智能决策中有潜力，为信用风险评估提供有力工具，有助于将环境和文本洞察融入预测分析。

Abstract: Credit risk assessment increasingly relies on diverse sources of information beyond traditional structured financial data, particularly for micro and small enterprises (mSEs) with limited financial histories. This study proposes a multimodal framework that integrates structured credit variables, climate panel data, and unstructured textual narratives within a unified learning architecture. Specifically, we use long short-term memory (LSTM), the gated recurrent unit (GRU), and transformer models to analyse the interplay between these data modalities. The empirical results demonstrate that unimodal models based on climate or text data outperform those relying solely on structured data, while the integration of multiple data modalities yields significant improvements in credit default prediction. Using SHAP-based explainability methods, we find that physical climate risks play an important role in default prediction, with water-logging by rain emerging as the most influential factor. Overall, this study demonstrates the potential of multimodal approaches in AI-enabled decision-making, which provides robust tools for credit risk assessment while contributing to the broader integration of environmental and textual insights into predictive analytics.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [136] [Ultimate Forward Rate Prediction and its Application to Bond Yield Forecasting: A Machine Learning Perspective](https://arxiv.org/abs/2601.00011)
*Jiawei Du,Yi Hong*

Main category: q-fin.ST

TL;DR: 本文用2009 - 2024年中国国债和宏观经济变量数据预测UFR并构建债券收益率预测模型，非线性机器学习模型效果更好，纳入宏观变量可提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 预测终极远期利率（UFR）并开发基于UFR的债券收益率预测模型。

Method: 采用de Kort - Vellekoop型方法估计UFR，结合本文提出的最优转折参数确定技术；运用线性和非线性机器学习技术预测UFR和超长期债券收益率。

Result: 非线性机器学习模型在预测准确性上优于线性模型；纳入宏观经济变量，尤其是与价格指数相关的变量，显著提高了预测准确性。

Conclusion: 开发了一种新颖的基于UFR的债券收益率预测模型，在不同债券期限上表现优越。

Abstract: This study focuses on forecasting the ultimate forward rate (UFR) and developing a UFRbased bond yield prediction model using data from Chinese treasury bonds and macroeconomic variables spanning from December 2009 to December 2024. The de Kort-Vellekooptype methodology is applied to estimate the UFR, incorporating the optimal turning parameter determination technique proposed in this study, which helps mitigate anomalous fluctuations. In addition, both linear and nonlinear machine learning techniques are employed to forecast the UFR and ultra-long-term bond yields. The results indicate that nonlinear machine learning models outperform their linear counterparts in forecasting accuracy. Incorporating macroeconomic variables, particularly price index-related variables, significantly improves the accuracy of predictions. Finally, a novel UFR-based bond yield forecasting model is developed, demonstrating superior performance across different bond maturities.

</details>


### [137] [Core-Periphery Dynamics in Market-Conditioned Financial Networks: A Conditional P-Threshold Mutual Information Approach](https://arxiv.org/abs/2601.00395)
*Kundan Mukhia,Imran Ansari,S R Luwang,Md Nurujjaman*

Main category: q-fin.ST

TL;DR: 研究用条件p阈值互信息MST框架分析新冠崩溃时金融市场结构重组，发现崩溃时网络整合度高、更脆弱，后仅部分恢复。


<details>
  <summary>Details</summary>
Motivation: 研究新冠疫情期间金融市场结构如何重组。

Method: 使用条件p阈值互信息的最小生成树框架，用Hellinger距离和Hilbert谱识别崩溃，条件p阈值MI过滤市场影响并检验显著性，构建MST网络比较不同时期。

Result: 崩溃时网络更集成、核心 - 外围结构减少、外围脆弱性增加、易传播冲击，后仅部分恢复，余震分析显示大波动事件相对频率高，结果跨市场一致。

Conclusion: 条件p阈值MI框架可捕捉非线性相互依赖和系统脆弱性。

Abstract: This study investigates how financial market structure reorganizes during the COVID-19 crash using a conditional p-threshold mutual information (MI) based Minimum Spanning Tree (MST) framework. We analyze nonlinear dependencies among the largest stocks from four diverse QUAD countries: the US, Japan, Australia, and India. Crashes are identified using the Hellinger distance and Hilbert spectrum; a crash occurs when HD = mu\_H + 2*sigma\_H, segmenting data into pre-crash, crash, and post-crash periods. Conditional p-threshold MI filters out common market effects and applies permutation-based significance testing. Resulting validated dependencies are used to construct MST networks for comparison across periods. Networks become more integrated during the crash, with shorter path lengths, higher centrality, and lower algebraic connectivity, indicating fragility. Core-periphery structure declines, with increased periphery vulnerability, and disassortative mixing facilitates shock transmission. Post-crash networks show only partial recovery. Aftershock analysis using the Gutenberg-Richter law indicates higher relative frequency of large volatility events following the crash. Results are consistent across all markets, highlighting the conditional p-threshold MI framework for capturing nonlinear interdependencies and systemic vulnerability.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [138] [Second Thoughts: How 1-second subslots transform CEX-DEX Arbitrage on Ethereum](https://arxiv.org/abs/2601.00738)
*Aleksei Adadurov,Sergey Barseghyan,Anton Chtepine,Antero Eloranta,Andrei Sebyakin,Arsenii Valitov*

Main category: q-fin.TR

TL;DR: 研究以太坊缩短出块时间对去中心化交易活动的影响，模拟显示更快出块时间使套利交易数量和交易量大幅增加。


<details>
  <summary>Details</summary>
Motivation: 探究以太坊缩短出块时间对去中心化交易活动的影响，关注CEX - DEX套利行为。

Method: 开发交易模型，考虑交易执行风险，对比12秒和1秒出块时间下的代理行为，并用2025年7 - 9月数据校准模拟。

Result: 更快出块时间使套利交易数量平均增加535%，交易量平均增加203%。

Conclusion: 1秒子块下CEX - DEX套利活动增加是因为交易结果方差减小，风险调整后回报增加，使套利更有吸引力。

Abstract: This paper examines the impact of reducing Ethereum slot time on decentralized exchange activity, with a focus on CEX-DEX arbitrage behavior. We develop a trading model where the agent's DEX transaction is not guaranteed to land, and the agent explicitly accounts for this execution risk when deciding whether to pursue arbitrage opportunities. We compare agent behavior under Ethereum's default 12-second slot time environment with a faster regime that offers 1-second subslot execution. The simulations, calibrated to Binance and Uniswap v3 data from July to September 2025, show that faster slot times increase arbitrage transaction count by 535% and trading volume by 203% on average. The increase in CEX-DEX arbitrage activity under 1-second subslots is driven by the reduction in variance of both successful and failed trade outcomes, increasing the risk-adjusted returns and making CEX-DEX arbitrage more appealing.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [139] [Active learning for data-driven reduced models of parametric differential systems with Bayesian operator inference](https://arxiv.org/abs/2601.00038)
*Shane A. McQuarrie,Mengwu Guo,Anirban Chaudhuri*

Main category: stat.ML

TL;DR: 本文开发主动学习框架丰富参数动力系统的数据驱动降阶模型 (ROM)，采用概率参数算子推理和自适应采样方案，实验表明该策略比随机采样效果好。


<details>
  <summary>Details</summary>
Motivation: 数据驱动 ROM 的质量对有限训练数据质量敏感，需要找出能得到最优参数 ROM 的训练参数。

Method: 使用算子推理方法，建立概率版本的参数算子推理，将学习问题转化为贝叶斯线性回归，用预测不确定性设计自适应采样方案。

Result: 对多个非线性参数偏微分方程系统进行数值实验，在相同计算预算下，自适应采样策略得到的 ROM 比随机采样更稳定准确。

Conclusion: 所提出的自适应采样策略能在参数域内全局提升 ROM 的稳定性和准确性。

Abstract: This work develops an active learning framework to intelligently enrich data-driven reduced-order models (ROMs) of parametric dynamical systems, which can serve as the foundation of virtual assets in a digital twin. Data-driven ROMs are explainable, computationally efficient scientific machine learning models that aim to preserve the underlying physics of complex dynamical simulations. Since the quality of data-driven ROMs is sensitive to the quality of the limited training data, we seek to identify training parameters for which using the associated training data results in the best possible parametric ROM. Our approach uses the operator inference methodology, a regression-based strategy which can be tailored to particular parametric structure for a large class of problems. We establish a probabilistic version of parametric operator inference, casting the learning problem as a Bayesian linear regression. Prediction uncertainties stemming from the resulting probabilistic ROM solutions are used to design a sequential adaptive sampling scheme to select new training parameter vectors that promote ROM stability and accuracy globally in the parameter domain. We conduct numerical experiments for several nonlinear parametric systems of partial differential equations and compare the results to ROMs trained on random parameter samples. The results demonstrate that the proposed adaptive sampling strategy consistently yields more stable and accurate ROMs than random sampling does under the same computational budget.

</details>


### [140] [Detecting Unobserved Confounders: A Kernelized Regression Approach](https://arxiv.org/abs/2601.00200)
*Yikai Chen,Yunxin Mao,Chunyuan Zheng,Hao Zou,Shanzhi Gu,Shixuan Liu,Yang Shi,Wenjing Yang,Kun Kuang,Haotian Wang*

Main category: stat.ML

TL;DR: 提出KRCD方法检测非线性单环境观测数据中的未观察混杂因素，理论证明相关结果，实验显示优于现有基线且计算效率高。


<details>
  <summary>Details</summary>
Motivation: 现有检测未观察混杂因素的方法有线性假设或多异质环境要求，不适用于非线性单环境设置。

Method: 提出Kernel Regression Confounder Detection (KRCD) 方法，利用再生核希尔伯特空间建模复杂依赖，比较标准和高阶核回归得到检验统计量。

Result: 理论上证明无穷样本和有限样本的关键结果，实验表明KRCD优于现有基线方法并具有更高计算效率。

Conclusion: KRCD能有效解决非线性单环境下未观察混杂因素的检测问题。

Abstract: Detecting unobserved confounders is crucial for reliable causal inference in observational studies. Existing methods require either linearity assumptions or multiple heterogeneous environments, limiting applicability to nonlinear single-environment settings. To bridge this gap, we propose Kernel Regression Confounder Detection (KRCD), a novel method for detecting unobserved confounding in nonlinear observational data under single-environment conditions. KRCD leverages reproducing kernel Hilbert spaces to model complex dependencies. By comparing standard and higherorder kernel regressions, we derive a test statistic whose significant deviation from zero indicates unobserved confounding. Theoretically, we prove two key results: First, in infinite samples, regression coefficients coincide if and only if no unobserved confounders exist. Second, finite-sample differences converge to zero-mean Gaussian distributions with tractable variance. Extensive experiments on synthetic benchmarks and the Twins dataset demonstrate that KRCD not only outperforms existing baselines but also achieves superior computational efficiency.

</details>


### [141] [Generative Conditional Missing Imputation Networks](https://arxiv.org/abs/2601.00517)
*George Sun,Yi-Hui Zhou*

Main category: stat.ML

TL;DR: 本文提出生成条件缺失值插补网络（GCMI），结合链式方程多重插补框架提升性能，经模拟和实证评估，其效果优于其他插补技术。


<details>
  <summary>Details</summary>
Motivation: 解决统计分析中数据集缺失值插补问题。

Method: 先阐述GCMI理论基础，再用链式方程多重插补框架增强其性能。

Result: 通过模拟和实证评估，证明GCMI比其他插补技术更有效。

Conclusion: GCMI实用且有潜力成为统计数据分析前沿工具。

Abstract: In this study, we introduce a sophisticated generative conditional strategy designed to impute missing values within datasets, an area of considerable importance in statistical analysis. Specifically, we initially elucidate the theoretical underpinnings of the Generative Conditional Missing Imputation Networks (GCMI), demonstrating its robust properties in the context of the Missing Completely at Random (MCAR) and the Missing at Random (MAR) mechanisms. Subsequently, we enhance the robustness and accuracy of GCMI by integrating a multiple imputation framework using a chained equations approach. This innovation serves to bolster model stability and improve imputation performance significantly. Finally, through a series of meticulous simulations and empirical assessments utilizing benchmark datasets, we establish the superior efficacy of our proposed methods when juxtaposed with other leading imputation techniques currently available. This comprehensive evaluation not only underscores the practicality of GCMI but also affirms its potential as a leading-edge tool in the field of statistical data analysis.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [142] [Surrogate Trajectories Along Probability Flows: Pseudo Markovian Alternative to Mori Zwanzig](https://arxiv.org/abs/2601.00015)
*Noé Stauffer,Hossein Gorji,Ivan Lunati*

Main category: stat.CO

TL;DR: 本文借鉴Mori - Zwanzig形式主义和Chorin最优预测方法处理高维含初始不确定性和随机噪声动力系统的模型降阶问题，展示了方法的若干理论和数值特性，数值结果显示其比蒙特卡罗和最优预测方法有计算优势。


<details>
  <summary>Details</summary>
Motivation: 现有模型降阶技术在处理受初始不确定性和/或随机噪声影响的高维动力系统（特别是涉及罕见事件）时工具和方法有限，需解决此问题。

Method: 借鉴Mori - Zwanzig形式主义和Chorin最优预测方法，采用动态的时间相关最优投影到期望的一组已解析变量，利用多项式混沌展开技术确定投影的测度。

Result: 所设计的替代轨迹与全阶系统的概率流一致；能有效计算低概率事件轨迹的投影；研究了替代轨迹的降阶误差并表征了方案的收敛行为；数值结果显示该方案比蒙特卡罗和最优预测方法有计算优势。

Conclusion: 通过跟踪测度和动态的一致投影，能够准确估计包括给定初始配置下的条件可观测量在内的不同统计量。

Abstract: Model reduction techniques have emerged as a powerful paradigm across different fronts of scientific computing. Despite their success, the provided tools and methodologies remain limited if high-dimensional dynamical systems subject to initial uncertainty and/or stochastic noise are encountered; in particular if rare events are of interest. We address this open challenge by borrowing ideas from Mori-Zwanzig formalism and Chorin's optimal prediction method. The novelty of our work lies on employing time-dependent optimal projection of the dynamic on a desired set of resolved variables. We show several theoretical and numerical properties of our model reduction approach. In particular, we show that the devised surrogate trajectories are consistent with the probability flow of the full-order system. Furthermore, we identify the measure underlying the projection through polynomial chaos expansion technique. This allows us to efficiently compute the projection even for trajectories that are initiated on low probability events. Moreover, we investigate the introduced model-reduction error of the surrogate trajectories on a standard setup, characterizing the convergence behaviour of the scheme. Several numerical results highlight the computational advantages of the proposed scheme in comparison to Monte-Carlo and optimal prediction method. Through this framework, we demonstrate that by tracking the measure along with the consistent projection of the dynamic we are able to access accurate estimates of different statistics including observables conditional on a given initial configuration.

</details>


### [143] [Integrating Multi-Armed Bandit, Active Learning, and Distributed Computing for Scalable Optimization](https://arxiv.org/abs/2601.00615)
*Foo Hui-Mean,Yuan-chin Ivan Chang*

Main category: stat.CO

TL;DR: 提出 ALMAB - DC 用于可扩展黑盒优化，结合多种技术，理论上有界，实验表现优于现有方法，适合高维资源密集型问题。


<details>
  <summary>Details</summary>
Motivation: 现代科学和工程优化问题依赖昂贵黑盒评估，传统方法计算成本高、可扩展性差。

Method: 提出 ALMAB - DC 框架，整合主动学习、多臂老虎机和分布式计算，利用代理建模和信息论采集函数选样本，用老虎机控制器分配资源，在分布式多智能体系统异步执行评估。

Result: 在合成基准测试、强化学习任务和科学模拟问题上，ALMAB - DC 始终优于现有黑盒优化器。

Conclusion: ALMAB - DC 具有模块化、不确定性感知和可扩展性，适合高维资源密集型优化挑战。

Abstract: Modern optimization problems in scientific and engineering domains often rely on expensive black-box evaluations, such as those arising in physical simulations or deep learning pipelines, where gradient information is unavailable or unreliable. In these settings, conventional optimization methods quickly become impractical due to prohibitive computational costs and poor scalability. We propose ALMAB-DC, a unified and modular framework for scalable black-box optimization that integrates active learning, multi-armed bandits, and distributed computing, with optional GPU acceleration. The framework leverages surrogate modeling and information-theoretic acquisition functions to guide informative sample selection, while bandit-based controllers dynamically allocate computational resources across candidate evaluations in a statistically principled manner. These decisions are executed asynchronously within a distributed multi-agent system, enabling high-throughput parallel evaluation. We establish theoretical regret bounds for both UCB-based and Thompson-sampling-based variants and develop a scalability analysis grounded in Amdahl's and Gustafson's laws. Empirical results across synthetic benchmarks, reinforcement learning tasks, and scientific simulation problems demonstrate that ALMAB-DC consistently outperforms state-of-the-art black-box optimizers. By design, ALMAB-DC is modular, uncertainty-aware, and extensible, making it particularly well suited for high-dimensional, resource-intensive optimization challenges.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [144] [Toward Large-Scale Photonics-Empowered AI Systems: From Physical Design Automation to System-Algorithm Co-Exploration](https://arxiv.org/abs/2601.00129)
*Ziang Yin,Hongjian Zhou,Nicholas Gangi,Meng Zhang,Jeff Zhang,Zhaoran Rena Huang,Jiaqi Gu*

Main category: physics.optics

TL;DR: 本文指出实现大规模实用光子AI系统的三个关键考量，并构建跨层工具链开展研究。


<details>
  <summary>Details</summary>
Motivation: 解决实现大规模实用光子AI系统面临的问题，对相关权衡进行量化研究。

Method: 构建跨层工具链SimPhony、ADEPT和ADEPT - Z、Apollo和LiDAR开展光子AI设计研究。

Result: 未提及具体结果。

Conclusion: 未提及具体结论，主要是介绍了为解决大规模光子AI系统问题所做的工具构建工作。

Abstract: In this work, we identify three considerations that are essential for realizing practical photonic AI systems at scale: (1) dynamic tensor operation support for modern models rather than only weight-static kernels, especially for attention/Transformer-style workloads; (2) systematic management of conversion, control, and data-movement overheads, where multiplexing and dataflow must amortize electronic costs instead of letting ADC/DAC and I/O dominate; and (3) robustness under hardware non-idealities that become more severe as integration density grows. To study these coupled tradeoffs quantitatively, and to ensure they remain meaningful under real implementation constraints, we build a cross-layer toolchain that supports photonic AI design from early exploration to physical realization. SimPhony provides implementation-aware modeling and rapid cross-layer evaluation, translating physical costs into system-level metrics so architectural decisions are grounded in realistic assumptions. ADEPT and ADEPT-Z enable end-to-end circuit and topology exploration, connecting system objectives to feasible photonic fabrics under practical device and circuit constraints. Finally, Apollo and LiDAR provide scalable photonic physical design automation, turning candidate circuits into manufacturable layouts while accounting for routing, thermal, and crosstalk constraints.

</details>


### [145] [Democratizing Electronic-Photonic AI Systems: An Open-Source AI-Infused Cross-Layer Co-Design and Design Automation Toolflow](https://arxiv.org/abs/2601.00130)
*Hongjian Zhou,Ziang Yin,Jiaqi Gu*

Main category: physics.optics

TL;DR: 光子学对高性能AI系统重要，但电子 - 光子AI系统设计部署难，本文提出跨层协同设计与自动化框架推动其发展。


<details>
  <summary>Details</summary>
Motivation: 光子学虽有优势，但电子 - 光子AI系统设计部署因多层面学习曲线及缺乏成熟工具链而面临挑战，需开发新框架。

Method: 先介绍可扩展光子边缘AI和Transformer推理的架构设计，再推出开源建模工具SimPhony，最后展示AI辅助光子设计自动化的进展。

Result: 构建了可用于下一代电子 - 光子AI系统的可扩展电子 - 光子设计自动化（EPDA）堆栈。

Conclusion: 所提出的跨层协同设计和自动化框架有助于推动光子AI系统开发的普及。

Abstract: Photonics is becoming a cornerstone technology for high-performance AI systems and scientific computing, offering unparalleled speed, parallelism, and energy efficiency. Despite this promise, the design and deployment of electronic-photonic AI systems remain highly challenging due to a steep learning curve across multiple layers, spanning device physics, circuit design, system architecture, and AI algorithms. The absence of a mature electronic-photonic design automation (EPDA) toolchain leads to long, inefficient design cycles and limits cross-disciplinary innovation and co-evolution. In this work, we present a cross-layer co-design and automation framework aimed at democratizing photonic AI system development. We begin by introducing our architecture designs for scalable photonic edge AI and Transformer inference, followed by SimPhony, an open-source modeling tool for rapid EPIC AI system evaluation and design-space exploration. We then highlight advances in AI-enabled photonic design automation, including physical AI-based Maxwell solvers, a fabrication-aware inverse design framework, and a scalable inverse training algorithm for meta-optical neural networks, enabling a scalable EPDA stack for next-generation electronic-photonic AI systems.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [146] [What Is a Causal Effect When Firms Interact? Counterfactuals and Interdependence](https://arxiv.org/abs/2601.00279)
*Mariluz Mate*

Main category: econ.GN

TL;DR: 论文指出相互依存环境中因果效应定义不唯一，开发因果框架，分析不同反事实机制下因果效应识别条件及差异，为实证研究提供指导。


<details>
  <summary>Details</summary>
Motivation: 现有文献虽关注相关模型识别挑战和溢出效应因果解释问题，但论文认为问题更根本，相互依存下因果效应定义不唯一。

Method: 开发针对企业层面经济的因果框架，将相互作用结构从未知转为可从预设特征学习；形式化三种有经济意义反事实机制；推导各机制识别条件；进行蒙特卡罗模拟。

Result: 学习网络对建模相互依存必要但不足以进行因果解释；不同反事实机制下标准空间自回归估计对应不同因果效应；均衡因果效应比直接或局部效应所需假设更强；模拟显示均衡和局部均衡效应存在机械差异，识别假设失败时网络反馈会放大偏差。

Conclusion: 明确现有空间和网络估计量能识别和不能识别的内容，为相互依存经济环境实证研究提供实用指导。

Abstract: Many empirical studies estimate causal effects in environments where economic units interact through spatial or network connections. In such settings, outcomes are jointly determined, and treatment induced shocks propagate across economically connected units. A growing literature highlights identification challenges in these models and questions the causal interpretation of estimated spillovers. This paper argues that the problem is more fundamental. Under interdependence, causal effects are not uniquely defined objects even when the interaction structure is correctly specified or consistently learned, and even under ideal identifying conditions. We develop a causal framework for firm-level economies in which interaction structures are unobserved but can be learned from predetermined characteristics. We show that learning the network, while necessary to model interdependence, is not sufficient for causal interpretation. Instead, causal conclusions hinge on explicit counterfactual assumptions governing how outcomes adjust following a treatment change. We formalize three economically meaningful counterfactual regimes partial equilibrium, local interaction, and network, consistent equilibrium, and show that standard spatial autoregressive estimates map into distinct causal effects depending on the counterfactual adopted. We derive identification conditions for each regime and demonstrate that equilibrium causal effects require substantially stronger assumptions than direct or local effects. A Monte Carlo simulation illustrates that equilibrium and partial-equilibrium effects differ mechanically even before estimation, and that network feedback can amplify bias when identifying assumptions fail. Taken together, our results clarify what existing spatial and network estimators can and cannot identify and provide practical guidance for empirical research in interdependent economic environments

</details>


### [147] [Effect of Informational Interventions on EV Adoption Intention: Evidence from a Tier II City in India](https://arxiv.org/abs/2601.00408)
*Pranshu Raghuvanshi,Anjula Gurtoo*

Main category: econ.GN

TL;DR: 研究靶向信息干预对电动汽车采纳意愿的有效性，发现范围和规范干预有效，成本干预无效。


<details>
  <summary>Details</summary>
Motivation: 探究靶向信息干预对电动汽车采纳意愿的有效性。

Method: 采用包含三个处理组和一个对照组的随机对照实地实验，对三组分别进行成本、范围和规范信息干预。

Result: 范围和规范干预显著，成本干预不显著，表明仅靠经济动机可能不足以提高电动汽车采纳率。

Conclusion: 可通过精心设计信息干预引导用户对可持续技术的行为意向，财政激励可辅以其他信息干预加速可持续出行技术的采纳。

Abstract: This study investigates the effectiveness of targeted informational interventions on electric vehicle adoption intention. A randomised controlled field experiment with three treatment groups and a control group was used to study the effectiveness of three informational interventions. Participants in each treatment group received a distinct informational intervention: cost-based, range-based, and norm-based. Two of the three interventions (range-based and norm-based), designed to reduce behavioural and psychological barriers, were found to be significant. The cost-based intervention was not significant, suggesting that financial motives alone may not be sufficient to lead to an increase in the adoption of electric vehicles. The significant effect observed for the range-based and norm-based interventions suggests that the discomfort related to the technology must be addressed, and social norms can be effectively utilised to promote electric vehicles at low cost. Although adoption is not guaranteed with self-reported intentions, the findings suggest that carefully framed informational interventions guide behavioural intentions towards sustainable technologies. The most significant contribution of the study is to the literature on demand-side policy instruments, which suggests that financial incentives can be complemented by other informational interventions to accelerate the adoption of sustainable mobility.

</details>


### [148] [TWICE: Tree-based Wage Inference with Clustering and Estimation](https://arxiv.org/abs/2601.00776)
*Aslan Bakirov,Francesco Del Prato,Paolo Zacchia*

Main category: econ.GN

TL;DR: 提出TWICE框架分析工人技能、企业薪酬政策及其交互对工资不平等的贡献，应用于葡萄牙数据表现优于线性基准。


<details>
  <summary>Details</summary>
Motivation: 标准方法存在方差估计膨胀、无法考虑互补性和分解缺乏可解释性等问题，需新方法分析工资不平等成因。

Method: 提出TWICE框架，用梯度提升树从可观测变量直接建模条件工资函数，用可解释的、基于可观测变量的分区替代潜在效应。

Result: TWICE在样本外表现优于线性基准，发现排序和非加性交互作用对工资差异的解释力比标准AKM估计大得多。

Conclusion: TWICE框架在分析工资不平等方面更具优势，能提供更有解释力的结果。

Abstract: How much do worker skills, firm pay policies, and their interaction contribute to wage inequality? Standard approaches rely on latent fixed effects identified through worker mobility, but sparse networks inflate variance estimates, additivity assumptions rule out complementarities, and the resulting decompositions lack interpretability. We propose TWICE (Tree-based Wage Inference with Clustering and Estimation), a framework that models the conditional wage function directly from observables using gradient-boosted trees, replacing latent effects with interpretable, observable-anchored partitions. This trades off the ability to capture idiosyncratic unobservables for robustness to sampling noise and out-of-sample portability. Applied to Portuguese administrative data, TWICE outperforms linear benchmarks out of sample and reveals that sorting and non-additive interactions explain substantially more wage dispersion than implied by standard AKM estimates.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [149] [Hear the Heartbeat in Phases: Physiologically Grounded Phase-Aware ECG Biometrics](https://arxiv.org/abs/2601.00170)
*Jintao Huang,Lu Leng,Yi Zhang,Ziyuan Yang*

Main category: eess.IV

TL;DR: 提出分层相位感知融合（HPAF）框架和心跳感知多原型（HAM）注册策略用于可穿戴设备心电图身份认证，在三个公共数据集上取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 现有心电图身份认证方法常将心跳视为同质信号，忽略了心动周期内的相位特定特征。

Method: 提出HPAF框架，分三个阶段避免特征交叉纠缠；提出HAM注册策略，构建多原型图库模板集。

Result: 在三个公共数据集的封闭和开放集设置下，HPAF与其他方法相比取得了最先进的结果。

Conclusion: 所提出的HPAF框架和HAM注册策略有效，能提高心电图身份认证的性能。

Abstract: Electrocardiography (ECG) is adopted for identity authentication in wearable devices due to its individual-specific characteristics and inherent liveness. However, existing methods often treat heartbeats as homogeneous signals, overlooking the phase-specific characteristics within the cardiac cycle. To address this, we propose a Hierarchical Phase-Aware Fusion~(HPAF) framework that explicitly avoids cross-feature entanglement through a three-stage design. In the first stage, Intra-Phase Representation (IPR) independently extracts representations for each cardiac phase, ensuring that phase-specific morphological and variation cues are preserved without interference from other phases. In the second stage, Phase-Grouped Hierarchical Fusion (PGHF) aggregates physiologically related phases in a structured manner, enabling reliable integration of complementary phase information. In the final stage, Global Representation Fusion (GRF) further combines the grouped representations and adaptively balances their contributions to produce a unified and discriminative identity representation. Moreover, considering ECG signals are continuously acquired, multiple heartbeats can be collected for each individual. We propose a Heartbeat-Aware Multi-prototype (HAM) enrollment strategy, which constructs a multi-prototype gallery template set to reduce the impact of heartbeat-specific noise and variability. Extensive experiments on three public datasets demonstrate that HPAF achieves state-of-the-art results in the comparison with other methods under both closed and open-set settings.

</details>


### [150] [Deep Learning Approach for the Diagnosis of Pediatric Pneumonia Using Chest X-ray Imaging](https://arxiv.org/abs/2601.00041)
*Fatemeh Hosseinabadi,Mohammad Mojtaba Rohani*

Main category: eess.IV

TL;DR: 研究利用迁移学习的CNN架构对儿科胸部X光图像自动分类，RegNet表现最佳


<details>
  <summary>Details</summary>
Motivation: 儿科肺炎是全球儿童发病和死亡主因，准确诊断受限于放射学专业知识和儿科影像复杂性，需自动化诊断方法

Method: 从公开数据集选取1000张X光图像并预处理和标记，用ImageNet预训练权重微调ResNetRS、RegNet和EfficientNetV2模型，基于准确率和敏感度评估

Result: RegNet分类性能最高，准确率92.4%，敏感度90.1%，ResNetRS和EfficientNetV2稍逊

Conclusion: 在儿科胸部X光图像自动分类中，RegNet有更优的性能表现

Abstract: Pediatric pneumonia remains a leading cause of morbidity and mortality in children worldwide. Timely and accurate diagnosis is critical but often challenged by limited radiological expertise and the physiological and procedural complexity of pediatric imaging. This study investigates the performance of state-of-the-art convolutional neural network (CNN) architectures ResNetRS, RegNet, and EfficientNetV2 using transfer learning for the automated classification of pediatric chest Xray images as either pneumonia or normal.A curated subset of 1,000 chest X-ray images was extracted from a publicly available dataset originally comprising 5,856 pediatric images. All images were preprocessed and labeled for binary classification. Each model was fine-tuned using pretrained ImageNet weights and evaluated based on accuracy and sensitivity. RegNet achieved the highest classification performance with an accuracy of 92.4 and a sensitivity of 90.1, followed by ResNetRS (accuracy: 91.9, sensitivity: 89.3) and EfficientNetV2 (accuracy: 88.5, sensitivity: 88.1).

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [151] [Solving nonlinear subsonic compressible flow in infinite domain via multi-stage neural networks](https://arxiv.org/abs/2601.00342)
*Xuehui Qian,Hongkai Tao,Yongji Wang*

Main category: physics.flu-dyn

TL;DR: 本文提出用PINNs解决无界域中全非线性可压缩势方程，通过坐标变换和嵌入物理渐近约束等方法提高精度，模拟验证了其优势。


<details>
  <summary>Details</summary>
Motivation: 传统方法在求解翼型亚音速可压缩流控制方程时存在计算挑战和误差，限制实际应用，需新方法。

Method: 提出利用PINNs解决无界域方程，通过坐标变换和嵌入物理渐近约束解决标准PINNs问题，采用MS - PINN方法迭代最小化残差。

Result: 模拟圆形和椭圆形几何体上的流动，与传统有限域和线性化解对比，量化了域截断和线性化引入的差异。

Conclusion: 新框架是计算流体动力学中强大、高保真的工具。

Abstract: In aerodynamics, accurately modeling subsonic compressible flow over airfoils is critical for aircraft design. However, solving the governing nonlinear perturbation velocity potential equation presents computational challenges. Traditional approaches often rely on linearized equations or finite, truncated domains, which introduce non-negligible errors and limit applicability in real-world scenarios. In this study, we propose a novel framework utilizing Physics-Informed Neural Networks (PINNs) to solve the full nonlinear compressible potential equation in an unbounded (infinite) domain. We address the unbounded-domain and convergence challenges inherent in standard PINNs by incorporating a coordinate transformation and embedding physical asymptotic constraints directly into the network architecture. Furthermore, we employ a Multi-Stage PINN (MS-PINN) approach to iteratively minimize residuals, achieving solution accuracy approaching machine precision. We validate this framework by simulating flow over circular and elliptical geometries, comparing our results against traditional finite-domain and linearized solutions. Our findings quantify the noticeable discrepancies introduced by domain truncation and linearization, particularly at higher Mach numbers, and demonstrate that this new framework is a robust, high-fidelity tool for computational fluid dynamics.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [152] [Neural Brain Fields: A NeRF-Inspired Approach for Generating Nonexistent EEG Electrodes](https://arxiv.org/abs/2601.00012)
*Shahar Ain Kedem,Itamar Zimerman,Eliya Nachmani*

Main category: eess.SP

TL;DR: 本文提出受NeRF启发的新方法处理EEG数据，可编码信号、渲染不同时步和电极位置信号，能连续可视化脑活动，模拟电极数据并提升处理网络性能。


<details>
  <summary>Details</summary>
Motivation: EEG数据存在长度不一、信噪比低等特性，开发有效处理EEG信号的深度学习方法仍是待解决的重要研究问题。

Method: 借鉴NeRF，将其从不同视角图像学习三维场景与EEG从不同头皮电极位置信号推断神经活动表示类比，用NeRF方式在单个EEG样本上训练神经网络产生固定大小的权重向量编码信号。

Result: 该方法能实现任意分辨率脑活动连续可视化、原始EEG信号重建，可有效模拟EEG记录中不存在的电极数据。

Conclusion: 此方法能将重建信号输入标准EEG处理网络提高性能。

Abstract: Electroencephalography (EEG) data present unique modeling challenges because recordings vary in length, exhibit very low signal to noise ratios, differ significantly across participants, drift over time within sessions, and are rarely available in large and clean datasets. Consequently, developing deep learning methods that can effectively process EEG signals remains an open and important research problem. To tackle this problem, this work presents a new method inspired by Neural Radiance Fields (NeRF). In computer vision, NeRF techniques train a neural network to memorize the appearance of a 3D scene and then uses its learned parameters to render and edit the scene from any viewpoint. We draw an analogy between the discrete images captured from different viewpoints used to learn a continuous 3D scene in NeRF, and EEG electrodes positioned at different locations on the scalp, which are used to infer the underlying representation of continuous neural activity. Building on this connection, we show that a neural network can be trained on a single EEG sample in a NeRF style manner to produce a fixed size and informative weight vector that encodes the entire signal. Moreover, via this representation we can render the EEG signal at previously unseen time steps and spatial electrode positions. We demonstrate that this approach enables continuous visualization of brain activity at any desired resolution, including ultra high resolution, and reconstruction of raw EEG signals. Finally, our empirical analysis shows that this method can effectively simulate nonexistent electrodes data in EEG recordings, allowing the reconstructed signal to be fed into standard EEG processing networks to improve performance.

</details>


### [153] [Modeling Day-Long ECG Signals to Predict Heart Failure Risk with Explainable AI](https://arxiv.org/abs/2601.00014)
*Eran Zvuloni,Ronit Almog,Michael Glikson,Shany Brimer Biton,Ilan Green,Izhar Laufer,Offer Amir,Joachim A. Behar*

Main category: eess.SP

TL;DR: 研究用深度学习模型分析24小时单导联心电图数据预测心衰风险，模型表现佳，是有前景的预测工具。


<details>
  <summary>Details</summary>
Motivation: 心衰影响老年人生活质量和寿命，预防心衰可降低发病率和死亡率，因此假设用人工智能分析24小时单导联心电图数据能预测五年内心衰风险。

Method: 使用Technion - Leumit Holter ECG数据集，训练深度学习模型DeepHHF。

Result: DeepHHF的ROC曲线下面积达0.80，优于使用30秒片段的模型和临床评分；高风险个体住院或死亡事件几率翻倍；模型关注心律失常和心脏异常，8点到15点为关键时段。

Conclusion: 深度学习建模24小时连续心电图数据可行，人工智能分析单导联心电图是有前景的心衰风险预测工具。

Abstract: Heart failure (HF) affects 11.8% of adults aged 65 and older, reducing quality of life and longevity. Preventing HF can reduce morbidity and mortality. We hypothesized that artificial intelligence (AI) applied to 24-hour single-lead electrocardiogram (ECG) data could predict the risk of HF within five years. To research this, the Technion-Leumit Holter ECG (TLHE) dataset, including 69,663 recordings from 47,729 patients, collected over 20 years was used. Our deep learning model, DeepHHF, trained on 24-hour ECG recordings, achieved an area under the receiver operating characteristic curve of 0.80 that outperformed a model using 30-second segments and a clinical score. High-risk individuals identified by DeepHHF had a two-fold chance of hospitalization or death incidents. Explainability analysis showed DeepHHF focused on arrhythmias and heart abnormalities, with key attention between 8 AM and 3 PM. This study highlights the feasibility of deep learning to model 24-hour continuous ECG data, capturing paroxysmal events and circadian variations essential for reliable risk prediction. Artificial intelligence applied to single-lead Holter ECG is non-invasive, inexpensive, and widely accessible, making it a promising tool for HF risk prediction.

</details>


### [154] [Parametrized Sharing for Multi-Agent Hybrid DRL for Multiple Multi-Functional RISs-Aided Downlink NOMA Networks](https://arxiv.org/abs/2601.00538)
*Chi-Te Kuo,Li-Hsiang Shen,Jyun-Jhe Huang*

Main category: eess.SP

TL;DR: 研究多MF - RIS辅助NOMA下行网络架构，设计PMHRL方案优化能效，仿真显示该方案能效最高。


<details>
  <summary>Details</summary>
Motivation: 利用MF - RIS的有源能力扩展信号覆盖范围和能量收集的自维持性，解决通信效率问题。

Method: 提出参数化共享的多智能体混合深度强化学习（PMHRL）方案，用多智能体近端策略优化（PPO）和深度Q网络（DQN）分别处理连续和离散变量。

Result: 仿真表明，提出的PMHRL方案比无参数化共享、纯PPO和DQN等基准方案能效更高；多MF - RIS辅助的下行NOMA在不同多址接入场景下比无EH/放大、传统RIS以及无RIS/MF - RIS部署的场景能效高。

Conclusion: 提出的PMHRL方案和多MF - RIS辅助的下行NOMA能有效提高能量效率。

Abstract: Multi-functional reconfigurable intelligent surface (MF-RIS) is conceived to address the communication efficiency thanks to its extended signal coverage from its active RIS capability and self-sustainability from energy harvesting (EH). We investigate the architecture of multi-MF-RISs to assist non-orthogonal multiple access (NOMA) downlink networks. We formulate an energy efficiency (EE) maximization problem by optimizing power allocation, transmit beamforming and MF-RIS configurations of amplitudes, phase-shifts and EH ratios, as well as the position of MF-RISs, while satisfying constraints of available power, user rate requirements, and self-sustainability property. We design a parametrized sharing scheme for multi-agent hybrid deep reinforcement learning (PMHRL), where the multi-agent proximal policy optimization (PPO) and deep-Q network (DQN) handle continuous and discrete variables, respectively. The simulation results have demonstrated that proposed PMHRL has the highest EE compared to other benchmarks, including cases without parametrized sharing, pure PPO and DQN. Moreover, the proposed multi-MF-RISs-aided downlink NOMA achieves the highest EE compared to scenarios of no-EH/amplification, traditional RISs, and deployment without RISs/MF-RISs under different multiple access.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [155] [FCMBench: A Comprehensive Financial Credit Multimodal Benchmark for Real-world Applications](https://arxiv.org/abs/2601.00150)
*Yehui Yang,Dalu Yang,Wenshuo Zhou,Fangxin Shang,Yifan Liu,Jie Ren,Haojun Fei,Qing Yang,Tao Chen*

Main category: cs.CV

TL;DR: 介绍大规模金融信贷多模态基准FCMBench-V1.0，含评估框架、样本构建方法，在23个模型上实验，显示模型性能差异和鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 随着多模态AI用于信贷风险评估和文档审查，需要特定领域基准，满足金融信贷应用需求、具备信贷理解和鲁棒性且保护隐私。

Method: 构建FCMBench-V1.0基准，含18种核心证书类型、图像和问答样本；评估框架分感知、推理和鲁棒性三维度；通过封闭合成-捕获管道构建样本，避免数据泄露。

Result: 对23个模型实验，Gemini 3 Pro商业模型F1得分最高，Qwen3-VL-235B开源模型最佳，Qfin-VL-Instruct整体得分最高；鲁棒性评估显示顶级模型在采集伪影下性能下降。

Conclusion: FCMBench能有效区分现代视觉语言模型的性能差异和鲁棒性，即使顶级模型在实际场景中鲁棒性也有待提高。

Abstract: As multimodal AI becomes widely used for credit risk assessment and document review, a domain-specific benchmark is urgently needed that (1) reflects documents and workflows specific to financial credit applications, (2) includes credit-specific understanding and real-world robustness, and (3) preserves privacy compliance without sacrificing practical utility. Here, we introduce FCMBench-V1.0 -- a large-scale financial credit multimodal benchmark for real-world applications, covering 18 core certificate types, with 4,043 privacy-compliant images and 8,446 QA samples. The FCMBench evaluation framework consists of three dimensions: Perception, Reasoning, and Robustness, including 3 foundational perception tasks, 4 credit-specific reasoning tasks that require decision-oriented understanding of visual evidence, and 10 real-world acquisition artifact types for robustness stress testing. To reconcile compliance with realism, we construct all samples via a closed synthesis-capture pipeline: we manually synthesize document templates with virtual content and capture scenario-aware images in-house. This design also mitigates pre-training data leakage by avoiding web-sourced or publicly released images. FCMBench can effectively discriminate performance disparities and robustness across modern vision-language models. Extensive experiments were conducted on 23 state-of-the-art vision-language models (VLMs) from 14 top AI companies and research institutes. Among them, Gemini 3 Pro achieves the best F1(\%) score as a commercial model (64.61), Qwen3-VL-235B achieves the best score as an open-source baseline (57.27), and our financial credit-specific model, Qfin-VL-Instruct, achieves the top overall score (64.92). Robustness evaluations show that even top-performing models suffer noticeable performance drops under acquisition artifacts.

</details>


### [156] [FaithSCAN: Model-Driven Single-Pass Hallucination Detection for Faithful Visual Question Answering](https://arxiv.org/abs/2601.00269)
*Chaodong Tong,Qi Zhang,Chen Li,Lei Jiang,Yanbing Liu*

Main category: cs.CV

TL;DR: 现有VQA忠实性幻觉检测方法存在局限，本文提出轻量级网络FaithSCAN，通过利用VLM丰富内部信号检测幻觉，实验显示其在多方面优于现有方法，并提供新见解。


<details>
  <summary>Details</summary>
Motivation: 现有VQA忠实性幻觉检测方法存在效率、鲁棒性和检测性能方面的固有局限，需要改进。

Method: 提出FaithSCAN网络，利用VLM的多种内部信号，通过分支证据编码和不确定性感知注意力融合信号；扩展LLM - as - a - Judge范式，提出低成本自动生成监督信号的策略。

Result: 在多个VQA基准测试中，FaithSCAN在有效性和效率上显著优于现有方法。

Conclusion: 幻觉源于视觉感知、跨模态推理和语言解码的系统内部状态变化，不同内部信号提供互补线索，且不同VLM架构的幻觉模式不同，为多模态幻觉的潜在原因提供新见解。

Abstract: Faithfulness hallucinations in VQA occur when vision-language models produce fluent yet visually ungrounded answers, severely undermining their reliability in safety-critical applications. Existing detection methods mainly fall into two categories: external verification approaches relying on auxiliary models or knowledge bases, and uncertainty-driven approaches using repeated sampling or uncertainty estimates. The former suffer from high computational overhead and are limited by external resource quality, while the latter capture only limited facets of model uncertainty and fail to sufficiently explore the rich internal signals associated with the diverse failure modes. Both paradigms thus have inherent limitations in efficiency, robustness, and detection performance. To address these challenges, we propose FaithSCAN: a lightweight network that detects hallucinations by exploiting rich internal signals of VLMs, including token-level decoding uncertainty, intermediate visual representations, and cross-modal alignment features. These signals are fused via branch-wise evidence encoding and uncertainty-aware attention. We also extend the LLM-as-a-Judge paradigm to VQA hallucination and propose a low-cost strategy to automatically generate model-dependent supervision signals, enabling supervised training without costly human labels while maintaining high detection accuracy. Experiments on multiple VQA benchmarks show that FaithSCAN significantly outperforms existing methods in both effectiveness and efficiency. In-depth analysis shows hallucinations arise from systematic internal state variations in visual perception, cross-modal reasoning, and language decoding. Different internal signals provide complementary diagnostic cues, and hallucination patterns vary across VLM architectures, offering new insights into the underlying causes of multimodal hallucinations.

</details>


### [157] [Towards Automated Differential Diagnosis of Skin Diseases Using Deep Learning and Imbalance-Aware Strategies](https://arxiv.org/abs/2601.00286)
*Ali Anaissi,Ali Braytee,Weidong Huang,Junaid Akram,Alaa Farhat,Jie Hua*

Main category: cs.CV

TL;DR: 开发基于深度学习的皮肤病分类诊断模型，在ISIC2019数据集上准确率达87.71%，有诊断支持潜力。


<details>
  <summary>Details</summary>
Motivation: 皮肤病常见但皮肤科医生有限，需智能工具辅助及时准确诊断皮肤病。

Method: 利用公开皮肤病图像数据集预训练，精炼模型架构、优化数据预处理流程、应用数据增强技术，基于Swin Transformer构建模型。

Result: 模型在ISIC2019数据集的八个皮肤病变类别上预测准确率达87.71%。

Conclusion: 模型有潜力作为临床医生诊断支持工具和患者自我评估辅助工具。

Abstract: As dermatological conditions become increasingly common and the availability of dermatologists remains limited, there is a growing need for intelligent tools to support both patients and clinicians in the timely and accurate diagnosis of skin diseases. In this project, we developed a deep learning based model for the classification and diagnosis of skin conditions. By leveraging pretraining on publicly available skin disease image datasets, our model effectively extracted visual features and accurately classified various dermatological cases. Throughout the project, we refined the model architecture, optimized data preprocessing workflows, and applied targeted data augmentation techniques to improve overall performance. The final model, based on the Swin Transformer, achieved a prediction accuracy of 87.71 percent across eight skin lesion classes on the ISIC2019 dataset. These results demonstrate the model's potential as a diagnostic support tool for clinicians and a self assessment aid for patients.

</details>


### [158] [VisNet: Efficient Person Re-Identification via Alpha-Divergence Loss, Feature Fusion and Dynamic Multi-Task Learning](https://arxiv.org/abs/2601.00307)
*Anns Ijaz,Muhammad Azeem Javed*

Main category: cs.CV

TL;DR: 提出适合现实场景的高效行人重识别模型VisNet，介绍其概念贡献及在数据集上的表现，可用于资源有限场景。


<details>
  <summary>Details</summary>
Motivation: 现有行人重识别方法准确率高但计算成本大，需要开发高效模型。

Method: 提出VisNet，包含多尺度特征融合、语义聚类、动态权重平均技术和使用FIDI损失函数等。

Result: 在Market - 1501数据集上Rank - 1为87.05%，mAP为77.65%，有32.41M参数和4.601 GFLOPs。

Conclusion: VisNet为计算资源有限的监控和移动应用实时部署提供了实用方法。

Abstract: Person re-identification (ReID) is an extremely important area in both surveillance and mobile applications, requiring strong accuracy with minimal computational cost. State-of-the-art methods give good accuracy but with high computational budgets. To remedy this, this paper proposes VisNet, a computationally efficient and effective re-identification model suitable for real-world scenarios. It is the culmination of conceptual contributions, including feature fusion at multiple scales with automatic attention on each, semantic clustering with anatomical body partitioning, a dynamic weight averaging technique to balance classification semantic regularization, and the use of loss function FIDI for improved metric learning tasks. The multiple scales fuse ResNet50's stages 1 through 4 without the use of parallel paths, with semantic clustering introducing spatial constraints through the use of rule-based pseudo-labeling. VisNet achieves 87.05% Rank-1 and 77.65% mAP on the Market-1501 dataset, having 32.41M parameters and 4.601 GFLOPs, hence, proposing a practical approach for real-time deployment in surveillance and mobile applications where computational resources are limited.

</details>


### [159] [HarmoniAD: Harmonizing Local Structures and Global Semantics for Anomaly Detection](https://arxiv.org/abs/2601.00327)
*Naiqi Zhang,Chuancheng Shi,Jingtong Dou,Wenhua Wu,Fei Shen,Jianhua Cao*

Main category: cs.CV

TL;DR: 提出HarmoniAD框架用于工业产品质量检测中的异常检测，在多数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法存在结构 - 语义权衡问题，面向结构的模型对噪声敏感，面向语义的模型易忽略细节。

Method: 提出频率引导的双分支框架HarmoniAD，先提取特征、转换到频域，再解耦为高低频路径分别建模结构和语义，采用多类联合训练策略。

Result: 在MVTec - AD、VisA和BTAD数据集上实验显示具有最先进性能，兼具敏感性和鲁棒性。

Conclusion: HarmoniAD框架能有效平衡细节和全局语义，解决现有方法的问题。

Abstract: Anomaly detection is crucial in industrial product quality inspection. Failing to detect tiny defects often leads to serious consequences. Existing methods face a structure-semantics trade-off: structure-oriented models (such as frequency-based filters) are noise-sensitive, while semantics-oriented models (such as CLIP-based encoders) often miss fine details. To address this, we propose HarmoniAD, a frequency-guided dual-branch framework. Features are first extracted by the CLIP image encoder, then transformed into the frequency domain, and finally decoupled into high- and low-frequency paths for complementary modeling of structure and semantics. The high-frequency branch is equipped with a fine-grained structural attention module (FSAM) to enhance textures and edges for detecting small anomalies, while the low-frequency branch uses a global structural context module (GSCM) to capture long-range dependencies and preserve semantic consistency. Together, these branches balance fine detail and global semantics. HarmoniAD further adopts a multi-class joint training strategy, and experiments on MVTec-AD, VisA, and BTAD show state-of-the-art performance with both sensitivity and robustness.

</details>


### [160] [MotionPhysics: Learnable Motion Distillation for Text-Guided Simulation](https://arxiv.org/abs/2601.00504)
*Miaowei Wang,Jakub Zadrożny,Oisin Mac Aodha,Amir Vaxman*

Main category: cs.CV

TL;DR: 提出MotionPhysics框架，可根据自然语言提示推断3D场景物理参数，实现逼真动态模拟。


<details>
  <summary>Details</summary>
Motivation: 准确模拟3D对象和材料需专业知识和耗时调参，旨在去除对真实轨迹或标注视频的依赖。

Method: 先利用多模态大语言模型估计材料参数值并约束范围，再提出可学习的运动蒸馏损失从预训练视频扩散模型提取运动先验。

Result: 在三十多个场景评估，对多种材料的3D对象实现视觉上逼真的动态模拟，超越现有技术。

Conclusion: MotionPhysics能以自然语言引导实现逼真动态模拟，自动确定合理物理参数。

Abstract: Accurately simulating existing 3D objects and a wide variety of materials often demands expert knowledge and time-consuming physical parameter tuning to achieve the desired dynamic behavior. We introduce MotionPhysics, an end-to-end differentiable framework that infers plausible physical parameters from a user-provided natural language prompt for a chosen 3D scene of interest, removing the need for guidance from ground-truth trajectories or annotated videos. Our approach first utilizes a multimodal large language model to estimate material parameter values, which are constrained to lie within plausible ranges. We further propose a learnable motion distillation loss that extracts robust motion priors from pretrained video diffusion models while minimizing appearance and geometry inductive biases to guide the simulation. We evaluate MotionPhysics across more than thirty scenarios, including real-world, human-designed, and AI-generated 3D objects, spanning a wide range of materials such as elastic solids, metals, foams, sand, and both Newtonian and non-Newtonian fluids. We demonstrate that MotionPhysics produces visually realistic dynamic simulations guided by natural language, surpassing the state of the art while automatically determining physically plausible parameters. The code and project page are available at: https://wangmiaowei.github.io/MotionPhysics.github.io/.

</details>


### [161] [It's Never Too Late: Noise Optimization for Collapse Recovery in Trained Diffusion Models](https://arxiv.org/abs/2601.00090)
*Anne Harrington,A. Sophia Koepke,Shyamgopal Karthik,Trevor Darrell,Alexei A. Efros*

Main category: cs.CV

TL;DR: 本文通过噪声优化解决文本到图像模型的模式坍塌问题，实验表明该方法在生成质量和多样性上效果更好。


<details>
  <summary>Details</summary>
Motivation: 当代文本到图像模型存在严重模式坍塌问题，以往方法未能有效解决。

Method: 采用简单的噪声优化目标，并分析噪声频率特征，使用不同频率分布的噪声初始化。

Result: 噪声优化在生成质量和多样性上取得了更好的效果。

Conclusion: 通过噪声优化可以缓解模式坍塌问题，同时保持基础模型的保真度。

Abstract: Contemporary text-to-image models exhibit a surprising degree of mode collapse, as can be seen when sampling several images given the same text prompt. While previous work has attempted to address this issue by steering the model using guidance mechanisms, or by generating a large pool of candidates and refining them, in this work we take a different direction and aim for diversity in generations via noise optimization. Specifically, we show that a simple noise optimization objective can mitigate mode collapse while preserving the fidelity of the base model. We also analyze the frequency characteristics of the noise and show that alternative noise initializations with different frequency profiles can improve both optimization and search. Our experiments demonstrate that noise optimization yields superior results in terms of generation quality and variety.

</details>


### [162] [A Comprehensive Dataset for Human vs. AI Generated Image Detection](https://arxiv.org/abs/2601.00553)
*Rajarshi Roy,Nasrin Imanpour,Ashhar Aziz,Shashwat Bajpai,Gurpreet Singh,Shwetangshu Biswas,Kapil Wanaskar,Parth Patwa,Subhankar Ghosh,Shreyas Dixit,Nilesh Ranjan Pal,Vipula Rawte,Ritvik Garimella,Gaytri Jena,Vasu Sharma,Vinija Jain,Aman Chadha,Aishwarya Naresh Reganti,Amitava Das*

Main category: cs.CV

TL;DR: 文章介绍发布用于AI生成图像检测的新数据集MS COCOAI，提出相关检测任务并给出数据集链接。


<details>
  <summary>Details</summary>
Motivation: 多模态生成式AI系统虽推动创新，但也使虚假图像传播，检测生成图像成为紧迫任务。

Method: 基于MS COCO数据集构建含96000个真实和合成数据点的数据集，用五种生成器生成合成图像，提出两类检测任务。

Result: 成功发布MS COCOAI数据集，可用于图像真假分类和识别合成图像的生成模型。

Conclusion: MS COCOAI数据集有助于解决AI生成图像检测难题。

Abstract: Multimodal generative AI systems like Stable Diffusion, DALL-E, and MidJourney have fundamentally changed how synthetic images are created. These tools drive innovation but also enable the spread of misleading content, false information, and manipulated media. As generated images become harder to distinguish from photographs, detecting them has become an urgent priority. To combat this challenge, We release MS COCOAI, a novel dataset for AI generated image detection consisting of 96000 real and synthetic datapoints, built using the MS COCO dataset. To generate synthetic images, we use five generators: Stable Diffusion 3, Stable Diffusion 2.1, SDXL, DALL-E 3, and MidJourney v6. Based on the dataset, we propose two tasks: (1) classifying images as real or generated, and (2) identifying which model produced a given synthetic image. The dataset is available at https://huggingface.co/datasets/Rajarshi-Roy-research/Defactify_Image_Dataset.

</details>


### [163] [Application Research of a Deep Learning Model Integrating CycleGAN and YOLO in PCB Infrared Defect Detection](https://arxiv.org/abs/2601.00237)
*Chao Yang,Haoyuan Zheng,Yue Ma*

Main category: cs.CV

TL;DR: 提出集成CycleGAN和YOLOv8的跨模态数据增强框架解决PCB缺陷检测中红外数据稀缺问题，实验证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 解决印刷电路板（PCB）缺陷检测中红外（IR）数据稀缺的关键瓶颈。

Method: 利用CycleGAN进行非配对图像到图像的转换，将丰富的可见光PCB图像映射到红外域，合成高保真伪红外样本；构建异质训练策略，融合生成的伪红外数据和有限的真实红外样本训练轻量级YOLOv8检测器。

Result: 该方法有效增强了低数据条件下的特征学习，增强后的检测器显著优于仅在有限真实数据上训练的模型，接近全监督训练的性能基准。

Conclusion: 伪红外合成作为工业检测的强大增强策略是有效的。

Abstract: This paper addresses the critical bottleneck of infrared (IR) data scarcity in Printed Circuit Board (PCB) defect detection by proposing a cross-modal data augmentation framework integrating CycleGAN and YOLOv8. Unlike conventional methods relying on paired supervision, we leverage CycleGAN to perform unpaired image-to-image translation, mapping abundant visible-light PCB images into the infrared domain. This generative process synthesizes high-fidelity pseudo-IR samples that preserve the structural semantics of defects while accurately simulating thermal distribution patterns. Subsequently, we construct a heterogeneous training strategy that fuses generated pseudo-IR data with limited real IR samples to train a lightweight YOLOv8 detector. Experimental results demonstrate that this method effectively enhances feature learning under low-data conditions. The augmented detector significantly outperforms models trained on limited real data alone and approaches the performance benchmarks of fully supervised training, proving the efficacy of pseudo-IR synthesis as a robust augmentation strategy for industrial inspection.

</details>


### [164] [Noise-Robust Tiny Object Localization with Flows](https://arxiv.org/abs/2601.00617)
*Huixin Sun,Linlin Yang,Ronyu Chen,Kerui Gu,Baochang Zhang,Angela Yao,Xianbin Cao*

Main category: cs.CV

TL;DR: 现有通用目标检测对微小目标效果差，本文提出TOLF框架，利用归一化流及不确定性引导优化，抑制过拟合，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 通用目标检测对微小目标和正常尺度目标存在性能差距，且微小目标对标注噪声敏感，优化严格定位目标有过拟合风险。

Method: 提出Tiny Object Localization with Flows (TOLF) 框架，通过基于流的误差建模捕捉复杂非高斯预测分布，并采用不确定性感知的梯度调制机制抑制高不确定性样本学习。

Result: 在三个数据集上进行大量实验，TOLF在AI - TOD数据集上使DINO基线AP提升1.2%。

Conclusion: 所提出的TOLF框架有效，能解决微小目标检测中由标注噪声导致的过拟合问题。

Abstract: Despite significant advances in generic object detection, a persistent performance gap remains for tiny objects compared to normal-scale objects. We demonstrate that tiny objects are highly sensitive to annotation noise, where optimizing strict localization objectives risks noise overfitting. To address this, we propose Tiny Object Localization with Flows (TOLF), a noise-robust localization framework leveraging normalizing flows for flexible error modeling and uncertainty-guided optimization. Our method captures complex, non-Gaussian prediction distributions through flow-based error modeling, enabling robust learning under noisy supervision. An uncertainty-aware gradient modulation mechanism further suppresses learning from high-uncertainty, noise-prone samples, mitigating overfitting while stabilizing training. Extensive experiments across three datasets validate our approach's effectiveness. Especially, TOLF boosts the DINO baseline by 1.2% AP on the AI-TOD dataset.

</details>


### [165] [Detecting Performance Degradation under Data Shift in Pathology Vision-Language Model](https://arxiv.org/abs/2601.00716)
*Hao Guan,Li Zhou*

Main category: cs.CV

TL;DR: 本文研究了先进病理视觉语言模型在数据分布偏移下的性能退化检测问题，开发DomainSAT工具分析输入数据偏移，提出基于输出置信度的退化指标，结合两者可更可靠检测模型性能退化。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在部署后，输入数据分布变化会导致性能下降，检测性能退化对临床可靠性至关重要，但无标注数据下的大预训练模型检测有挑战。

Method: 研究输入级数据偏移和输出级预测行为；开发DomainSAT工具分析输入数据偏移；引入基于置信度的无标签退化指标。

Result: 输入数据偏移检测能识别分布变化但不总对应实际性能退化；基于输出的置信度指标与性能退化密切相关；结合输入偏移检测和输出置信度指标可更可靠检测和解释性能退化。

Conclusion: 为数字病理学中基础模型的可靠性监测提供了实用且互补的框架。

Abstract: Vision-Language Models have demonstrated strong potential in medical image analysis and disease diagnosis. However, after deployment, their performance may deteriorate when the input data distribution shifts from that observed during development. Detecting such performance degradation is essential for clinical reliability, yet remains challenging for large pre-trained VLMs operating without labeled data. In this study, we investigate performance degradation detection under data shift in a state-of-the-art pathology VLM. We examine both input-level data shift and output-level prediction behavior to understand their respective roles in monitoring model reliability. To facilitate systematic analysis of input data shift, we develop DomainSAT, a lightweight toolbox with a graphical interface that integrates representative shift detection algorithms and enables intuitive exploration of data shift. Our analysis shows that while input data shift detection is effective at identifying distributional changes and providing early diagnostic signals, it does not always correspond to actual performance degradation. Motivated by this observation, we further study output-based monitoring and introduce a label-free, confidence-based degradation indicator that directly captures changes in model prediction confidence. We find that this indicator exhibits a close relationship with performance degradation and serves as an effective complement to input shift detection. Experiments on a large-scale pathology dataset for tumor classification demonstrate that combining input data shift detection and output confidence-based indicators enables more reliable detection and interpretation of performance degradation in VLMs under data shift. These findings provide a practical and complementary framework for monitoring the reliability of foundation models in digital pathology.

</details>


### [166] [HyperPriv-EPN: Hypergraph Learning with Privileged Knowledge for Ependymoma Prognosis](https://arxiv.org/abs/2601.00626)
*Shuren Gabriel Yu,Sikang Ren,Yongji Tian*

Main category: cs.CV

TL;DR: 提出HyperPriv - EPN框架，利用术后信息提升室管膜瘤术前预后诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 室管膜瘤术前预后对治疗规划很关键，但MRI缺乏语义信息，现有多模态方法在推理时无法利用术后文本数据。

Method: 提出HyperPriv - EPN框架，采用Severed Graph Strategy，通过双流蒸馏让学生图从视觉特征中学习语义社区结构。

Result: 在311名患者的多中心队列中验证，HyperPriv - EPN实现了最先进的诊断准确性和生存分层。

Conclusion: 能将专家知识转移到术前场景，挖掘历史术后数据价值，推理时无需文本即可指导新患者诊断。

Abstract: Preoperative prognosis of Ependymoma is critical for treatment planning but challenging due to the lack of semantic insights in MRI compared to post-operative surgical reports. Existing multimodal methods fail to leverage this privileged text data when it is unavailable during inference. To bridge this gap, we propose HyperPriv-EPN, a hypergraph-based Learning Using Privileged Information (LUPI) framework. We introduce a Severed Graph Strategy, utilizing a shared encoder to process both a Teacher graph (enriched with privileged post-surgery information) and a Student graph (restricted to pre-operation data). Through dual-stream distillation, the Student learns to hallucinate semantic community structures from visual features alone. Validated on a multi-center cohort of 311 patients, HyperPriv-EPN achieves state-of-the-art diagnostic accuracy and survival stratification. This effectively transfers expert knowledge to the preoperative setting, unlocking the value of historical post-operative data to guide the diagnosis of new patients without requiring text at inference.

</details>


### [167] [Two Deep Learning Approaches for Automated Segmentation of Left Ventricle in Cine Cardiac MRI](https://arxiv.org/abs/2601.00794)
*Wenhui Chu,Nikolaos V. Tsekos*

Main category: cs.CV

TL;DR: 提出LNU - Net和IBU - Net用于短轴cine MRI图像左心室分割，实验显示性能优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 左心室分割对心脏图像临床量化和诊断至关重要，现有方法可能存在不足，需新的分割方法。

Method: 提出LNU - Net和IBU - Net两种架构，采用下采样提取特征、上采样精确定位；以原始U - Net为基础进行对比；LNU - Net在卷积块应用层归一化，IBU - Net在首个卷积块结合实例和批量归一化；使用仿射变换和弹性形变处理图像数据。

Result: 实验表明，提出的方法在骰子系数和平均垂直距离上优于其他先进方法。

Conclusion: LNU - Net和IBU - Net在左心室分割上是有效的，性能优于现有方法。

Abstract: Left ventricle (LV) segmentation is critical for clinical quantification and diagnosis of cardiac images. In this work, we propose two novel deep learning architectures called LNU-Net and IBU-Net for left ventricle segmentation from short-axis cine MRI images. LNU-Net is derived from layer normalization (LN) U-Net architecture, while IBU-Net is derived from the instance-batch normalized (IB) U-Net for medical image segmentation. The architectures of LNU-Net and IBU-Net have a down-sampling path for feature extraction and an up-sampling path for precise localization. We use the original U-Net as the basic segmentation approach and compared it with our proposed architectures. Both LNU-Net and IBU-Net have left ventricle segmentation methods: LNU-Net applies layer normalization in each convolutional block, while IBU-Net incorporates instance and batch normalization together in the first convolutional block and passes its result to the next layer. Our method incorporates affine transformations and elastic deformations for image data processing. Our dataset that contains 805 MRI images regarding the left ventricle from 45 patients is used for evaluation. We experimentally evaluate the results of the proposed approaches outperforming the dice coefficient and the average perpendicular distance than other state-of-the-art approaches.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [168] [Sparse Tucker Decomposition and Graph Regularization for High-Dimensional Time Series Forecasting](https://arxiv.org/abs/2601.00377)
*Sijia Xia,Michael K. Ng,Xiongjun Zhang*

Main category: math.ST

TL;DR: 提出一种含图正则化的稀疏Tucker分解方法用于高维向量自回归时间序列分析，能精确估计参数，有非渐近误差界，设计算法并证明收敛性，实验验证性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有向量自回归模型多元时间序列分析方法存在过参数化问题，需降维。

Method: 将时间序列转移矩阵堆叠成三阶张量，用稀疏Tucker分解刻画转移三阶张量内部重要交互并减少参数，用图正则化度量向量自回归模型中响应、预测和时间因子矩阵的局部一致性。

Result: 所提两种正则化技术能实现更精确的参数估计，建立的估计器非渐近误差界低于现有基于矩阵或张量的方法，设计的算法在温和条件下全局收敛，实验验证了性能优于现有方法。

Conclusion: 所提含图正则化的稀疏Tucker分解方法对高维向量自回归时间序列分析有效，有更好的估计和性能。

Abstract: Existing methods of vector autoregressive model for multivariate time series analysis make use of low-rank matrix approximation or Tucker decomposition to reduce the dimension of the over-parameterization issue. In this paper, we propose a sparse Tucker decomposition method with graph regularization for high-dimensional vector autoregressive time series. By stacking the time-series transition matrices into a third-order tensor, the sparse Tucker decomposition is employed to characterize important interactions within the transition third-order tensor and reduce the number of parameters. Moreover, the graph regularization is employed to measure the local consistency of the response, predictor and temporal factor matrices in the vector autoregressive model.The two proposed regularization techniques can be shown to more accurate parameters estimation. A non-asymptotic error bound of the estimator of the proposed method is established, which is lower than those of the existing matrix or tensor based methods. A proximal alternating linearized minimization algorithm is designed to solve the resulting model and its global convergence is established under very mild conditions. Extensive numerical experiments on synthetic data and real-world datasets are carried out to verify the superior performance of the proposed method over existing state-of-the-art methods.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [169] [Deep learning estimation of the spectral density of functional time series on large domains](https://arxiv.org/abs/2601.00284)
*Neda Mohammadi,Soham Sarkar,Piotr Kokoszka*

Main category: stat.ME

TL;DR: 推导多层感知器神经网络输出的泛函时间序列谱密度估计量，该估计量无需计算自协方差核，可并行计算，通过模拟和fMRI图像应用验证性能。


<details>
  <summary>Details</summary>
Motivation: 现有函数时间序列谱密度估计量在大网格上计算困难，如气候计算模型和医学扫描，自协方差核计算密集甚至不可行。

Method: 利用谱泛函主成分理论推导深度学习估计量，并证明其在一般假设下是谱密度的通用近似器。

Result: 该估计量可在不计算自协方差核的情况下训练，且可并行化，比现有方法提供估计的速度更快。

Conclusion: 通过模拟和fMRI图像应用验证了估计量的性能。

Abstract: We derive an estimator of the spectral density of a functional time series that is the output of a multilayer perceptron neural network. The estimator is motivated by difficulties with the computation of existing spectral density estimators for time series of functions defined on very large grids that arise, for example, in climate compute models and medical scans. Existing estimators use autocovariance kernels represented as large $G \times G$ matrices, where $G$ is the number of grid points on which the functions are evaluated. In many recent applications, functions are defined on 2D and 3D domains, and $G$ can be of the order $G \sim 10^5$, making the evaluation of the autocovariance kernels computationally intensive or even impossible. We use the theory of spectral functional principal components to derive our deep learning estimator and prove that it is a universal approximator to the spectral density under general assumptions. Our estimator can be trained without computing the autocovariance kernels and it can be parallelized to provide the estimates much faster than existing approaches. We validate its performance by simulations and an application to fMRI images.

</details>


### [170] [Identification and Estimation under Multiple Versions of Treatment: Mixture-of-Experts Approach](https://arxiv.org/abs/2601.00287)
*Kohei Yoshikawa,Shuichi Kawano*

Main category: stat.ME

TL;DR: 本文引入Mixture - of - Experts框架到因果推断中，提出估计潜在版本因果效应的方法，数值实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 在观测研究中无法控制治疗实施，可能存在多种治疗版本，忽略这些版本会导致因果效应估计有偏差，且处理版本特定因果效应无偏识别和估计的因果推断框架未充分发展，难以深入理解复杂治疗机制。

Method: 将Mixture - of - Experts框架引入因果推断，提出估计潜在版本因果效应的方法。

Result: 数值实验证明了所提方法的有效性。

Conclusion: 所提方法能在版本未被观测时明确估计版本特定的因果效应。

Abstract: The Stable Unit Treatment Value Assumption (SUTVA) includes the condition that there are no multiple versions of treatment in causal inference. Though we could not control the implementation of treatment in observational studies, multiple versions may exist in the treatment. It has been pointed out that ignoring such multiple versions of treatment can lead to biased estimates of causal effects, but a causal inference framework that explicitly deals with the unbiased identification and estimation of version-specific causal effects has not been fully developed yet. Thus, obtaining a deeper understanding for mechanisms of the complex treatments is difficult. In this paper, we introduce the Mixture-of-Experts framework into causal inference and develop a methodology for estimating the causal effects of latent versions. This approach enables explicit estimation of version-specific causal effects even if the versions are not observed. Numerical experiments demonstrate the effectiveness of the proposed method.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [171] [Noise-Aware Named Entity Recognition for Historical VET Documents](https://arxiv.org/abs/2601.00488)
*Alexander M. Esser,Jens Dörpinghaus*

Main category: cs.CL

TL;DR: 本文针对职业教育与培训领域受OCR噪声影响的历史数字化文档，提出基于噪声感知训练的命名实体识别方法，实验证明该方法能提升鲁棒性和准确性，并提供公开代码。


<details>
  <summary>Details</summary>
Motivation: 解决职业教育与培训领域历史数字化文档受OCR噪声影响的命名实体识别问题。

Method: 提出利用噪声感知训练、合成注入OCR错误、迁移学习和多阶段微调的命名实体识别方法，并系统比较三种互补策略。

Result: 实验表明特定领域和噪声感知的微调能在噪声条件下大幅提高鲁棒性和准确性。

Conclusion: 该方法可用于特定领域可复现的噪声感知命名实体识别，且代码公开。

Abstract: This paper addresses Named Entity Recognition (NER) in the domain of Vocational Education and Training (VET), focusing on historical, digitized documents that suffer from OCR-induced noise. We propose a robust NER approach leveraging Noise-Aware Training (NAT) with synthetically injected OCR errors, transfer learning, and multi-stage fine-tuning. Three complementary strategies, training on noisy, clean, and artificial data, are systematically compared. Our method is one of the first to recognize multiple entity types in VET documents. It is applied to German documents but transferable to arbitrary languages. Experimental results demonstrate that domain-specific and noise-aware fine-tuning substantially increases robustness and accuracy under noisy conditions. We provide publicly available code for reproducible noise-aware NER in domain-specific contexts.

</details>


### [172] [Physio-DPO: Aligning Large Language Models with the Protein Energy Landscape to Eliminate Structural Hallucinations](https://arxiv.org/abs/2601.00647)
*QiWei Meng*

Main category: cs.CL

TL;DR: 提出Physio - DPO框架解决大蛋白语言模型结构幻觉问题，实验显示其性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 大蛋白语言模型用于生成式蛋白设计时会产生结构幻觉，现有对齐方法有局限。

Method: 提出Physio - DPO框架，引入幅度感知目标，根据天然结构和物理扰动硬负样本的能量差距调整优化更新。

Result: Physio - DPO持续优于SFT、PPO和标准DPO等基线方法，将自一致性RMSD降至1.28 Å，可折叠性提高到92.8%。

Conclusion: Physio - DPO能有效缓解结构幻觉，恢复疏水核心堆积和氢键网络等生物物理相互作用。

Abstract: Large Protein Language Models have shown strong potential for generative protein design, yet they frequently produce structural hallucinations, generating sequences with high linguistic likelihood that fold into thermodynamically unstable conformations. Existing alignment approaches such as Direct Preference Optimization are limited in this setting, as they model preferences as binary labels and ignore the continuous structure of the physical energy landscape. We propose Physio-DPO, a physics informed alignment framework that grounds protein language models in thermodynamic stability. Physio-DPO introduces a magnitude aware objective that scales optimization updates according to the energy gap between native structures and physics perturbed hard negatives. Experiments show that Physio-DPO consistently outperforms strong baselines including SFT, PPO, and standard DPO, reducing self consistency RMSD to 1.28 Å and increasing foldability to 92.8%. Qualitative analysis further demonstrates that Physio-DPO effectively mitigates structural hallucinations by recovering biophysical interactions such as hydrophobic core packing and hydrogen bond networks.

</details>


### [173] [Talk Less, Verify More: Improving LLM Assistants with Semantic Checks and Execution Feedback](https://arxiv.org/abs/2601.00224)
*Yan Sun,Ming Cai,Stanley Kok*

Main category: cs.CL

TL;DR: 论文针对当前会话式商业分析系统缺乏验证机制的问题，提出Q*和Feedback+两种验证技术，嵌入生成器 - 判别器框架，实验显示可降低错误率和任务完成时间，还指出反向翻译是瓶颈，为构建可靠企业级GenAI系统提供框架。


<details>
  <summary>Details</summary>
Motivation: 当前会话式商业分析系统缺乏内置验证机制，需用户手动验证潜在有缺陷的结果，而大语言模型助手准确输出对企业工作流程至关重要。

Method: 引入Q*（在代码和用户意图之间进行反向翻译和语义匹配）和Feedback+（结合执行反馈指导代码改进）两种验证技术，并将其嵌入生成器 - 判别器框架。

Result: 在Spider、Bird和GSM8K三个基准数据集上的评估表明，Q*和Feedback+可降低错误率和任务完成时间。

Conclusion: 为构建更可靠、企业级的GenAI系统提供了一个面向设计的框架，可支持可信决策。

Abstract: As large language model (LLM) assistants become increasingly integrated into enterprise workflows, their ability to generate accurate, semantically aligned, and executable outputs is critical. However, current conversational business analytics (CBA) systems often lack built-in verification mechanisms, leaving users to manually validate potentially flawed results. This paper introduces two complementary verification techniques: Q*, which performs reverse translation and semantic matching between code and user intent, and Feedback+, which incorporates execution feedback to guide code refinement. Embedded within a generator-discriminator framework, these mechanisms shift validation responsibilities from users to the system. Evaluations on three benchmark datasets, Spider, Bird, and GSM8K, demonstrate that both Q* and Feedback+ reduce error rates and task completion time. The study also identifies reverse translation as a key bottleneck, highlighting opportunities for future improvement. Overall, this work contributes a design-oriented framework for building more reliable, enterprise-grade GenAI systems capable of trustworthy decision support.

</details>


### [174] [Understanding Emotion in Discourse: Recognition Insights and Linguistic Patterns for Generation](https://arxiv.org/abs/2601.00181)
*Cheonkam Jeong,Adeline Nyamathi*

Main category: cs.CL

TL;DR: 通过对IEMOCAP数据集系统分析填补情感识别现有研究差距，识别研究有三项重要发现并获高性能，语言分析发现情感与话语标记位置有关。


<details>
  <summary>Details</summary>
Motivation: 解决情感识别研究中架构选择理解不足和缺乏识别与生成的语言分析的问题。

Method: 对IEMOCAP数据集进行系统分析，包括对识别的严格消融实验和十次种子评估，以及对5286个话语标记出现情况的语言分析。

Result: 识别方面，发现对话上下文至关重要，层次化句子表示在有上下文时无额外帮助，外部情感词典无增益，简单架构取得高F1值；语言分析显示情感与标记位置有显著关联。

Conclusion: 实验结果表明对话上下文对情感识别重要，且不同情感与话语标记有关联，可用于解决情感识别中的歧义。

Abstract: While Emotion Recognition in Conversation (ERC) has achieved high accuracy, two critical gaps remain: a limited understanding of \textit{which} architectural choices actually matter, and a lack of linguistic analysis connecting recognition to generation. We address both gaps through a systematic analysis of the IEMOCAP dataset.
  For recognition, we conduct a rigorous ablation study with 10-seed evaluation and report three key findings. First, conversational context is paramount, with performance saturating rapidly -- 90\% of the total gain achieved within just the most recent 10--30 preceding turns (depending on the label set). Second, hierarchical sentence representations help at utterance-level, but this benefit disappears once conversational context is provided, suggesting that context subsumes intra-utterance structure. Third, external affective lexicons (SenticNet) provide no gain, indicating that pre-trained encoders already capture necessary emotional semantics. With simple architectures using strictly causal context, we achieve 82.69\% (4-way) and 67.07\% (6-way) weighted F1, outperforming prior text-only methods including those using bidirectional context.
  For linguistic analysis, we analyze 5,286 discourse marker occurrences and find a significant association between emotion and marker positioning ($p < .0001$). Notably, "sad" utterances exhibit reduced left-periphery marker usage (21.9\%) compared to other emotions (28--32\%), consistent with theories linking left-periphery markers to active discourse management. This connects to our recognition finding that sadness benefits most from context (+22\%p): lacking explicit pragmatic signals, sad utterances require conversational history for disambiguation.

</details>


### [175] [JP-TL-Bench: Anchored Pairwise LLM Evaluation for Bidirectional Japanese-English Translation](https://arxiv.org/abs/2601.00223)
*Leonard Lin,Adam Lensenmayer*

Main category: cs.CL

TL;DR: 介绍轻量级开放基准JP - TL - Bench指导日英翻译系统迭代开发，通过无参考的成对大语言模型比较评估，用Bradley - Terry模型聚合结果。


<details>
  <summary>Details</summary>
Motivation: 解决日英翻译中判断‘两个好翻译哪个更好’的问题，考虑到日英翻译中礼貌、含义、省略和语域等因素对自然度的影响。

Method: 使用无参考的成对大语言模型比较，将候选模型与固定版本的锚定集对比，用Bradley - Terry模型聚合成对结果。

Result: 得出候选模型的胜率和归一化的0 - 10的‘LT’分数，分数在相同基础集、评判模型和聚合代码下结构稳定。

Conclusion: JP - TL - Bench能有效指导日英翻译系统的迭代开发。

Abstract: We introduce JP-TL-Bench, a lightweight, open benchmark designed to guide the iterative development of Japanese-English translation systems. In this context, the challenge is often "which of these two good translations is better?" rather than "is this translation acceptable?" This distinction matters for Japanese-English, where subtle choices in politeness, implicature, ellipsis, and register strongly affect perceived naturalness. JP-TL-Bench uses a protocol built to make LLM judging both reliable and affordable: it evaluates a candidate model via reference-free, pairwise LLM comparisons against a fixed, versioned anchor set. Pairwise results are aggregated with a Bradley-Terry model and reported as win rates plus a normalized 0-10 "LT" score derived from a logistic transform of fitted log-strengths. Because each candidate is scored against the same frozen anchor set, scores are structurally stable given the same base set, judge, and aggregation code.

</details>


### [176] [Parallel Universes, Parallel Languages: A Comprehensive Study on LLM-based Multilingual Counterfactual Example Generation](https://arxiv.org/abs/2601.00263)
*Qianli Wang,Van Bach Nguyen,Yihong Liu,Fedor Splitt,Nils Feldhus,Christin Seifert,Hinrich Schütze,Sebastian Möller,Vera Schmitt*

Main category: cs.CL

TL;DR: 本文对大语言模型生成多语言反事实进行研究，对比不同生成方式，发现编辑模式相似性、错误类型，还比较了数据增强效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成英语反事实有效，但多语言反事实生成效果不明，因此开展研究。

Method: 对六种语言直接生成和通过英语翻译生成的反事实进行自动评估；分析高资源欧洲语言反事实的编辑模式；识别和分类生成反事实中的错误类型；对比多语言和跨语言反事实数据增强效果。

Result: 翻译生成的反事实有效性更高，但需更多修改且不如英语反事实；高资源欧洲语言反事实编辑模式相似；跨语言存在四种常见错误类型；多语言反事实数据增强对低资源语言效果更好。

Conclusion: 生成的反事实存在不足，限制了模型性能和鲁棒性提升。

Abstract: Counterfactuals refer to minimally edited inputs that cause a model's prediction to change, serving as a promising approach to explaining the model's behavior. Large language models (LLMs) excel at generating English counterfactuals and demonstrate multilingual proficiency. However, their effectiveness in generating multilingual counterfactuals remains unclear. To this end, we conduct a comprehensive study on multilingual counterfactuals. We first conduct automatic evaluations on both directly generated counterfactuals in the target languages and those derived via English translation across six languages. Although translation-based counterfactuals offer higher validity than their directly generated counterparts, they demand substantially more modifications and still fall short of matching the quality of the original English counterfactuals. Second, we find the patterns of edits applied to high-resource European-language counterfactuals to be remarkably similar, suggesting that cross-lingual perturbations follow common strategic principles. Third, we identify and categorize four main types of errors that consistently appear in the generated counterfactuals across languages. Finally, we reveal that multilingual counterfactual data augmentation (CDA) yields larger model performance improvements than cross-lingual CDA, especially for lower-resource languages. Yet, the imperfections of the generated counterfactuals limit gains in model performance and robustness.

</details>


### [177] [Beyond Perfect APIs: A Comprehensive Evaluation of LLM Agents Under Real-World API Complexity](https://arxiv.org/abs/2601.00268)
*Doyoung Kim,Zhiwei Ren,Jie Hao,Zhongkai Sun,Lichao Wang,Xiyao Ma,Zack Ye,Xu Han,Jun Yin,Heng Ji,Wei Shen,Xing Fan,Benjamin Yao,Chenlei Guo*

Main category: cs.CL

TL;DR: 介绍WildAGTEval基准以评估大语言模型代理的函数调用能力，用其评估多个先进大模型，发现多数场景有挑战且部分模型会歪曲用户意图。


<details>
  <summary>Details</summary>
Motivation: 现有工作假设理想化API系统，忽略现实因素，需评估大语言模型代理在现实API复杂度下的函数调用能力。

Method: 创建包含60种复杂度场景和3.2万种测试配置的API系统及用户与代理交互设置的WildAGTEval基准来评估大语言模型。

Result: 多数场景有挑战，无关信息复杂度最具难度，使强模型性能下降27.3%，模型会歪曲用户意图影响满意度。

Conclusion: WildAGTEval可有效评估大语言模型代理在现实API复杂度下的函数调用能力，现有模型在处理复杂度场景上存在不足。

Abstract: We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity. Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. API specification, which includes detailed documentation and usage constraints, and 2. API execution, which captures runtime challenges. Consequently, WildAGTEval offers (i) an API system encompassing 60 distinct complexity scenarios that can be composed into approximately 32K test configurations, and (ii) user-agent interactions for evaluating LLM agents on these scenarios. Using WildAGTEval, we systematically assess several advanced LLMs and observe that most scenarios are challenging, with irrelevant information complexity posing the greatest difficulty and reducing the performance of strong LLMs by 27.3%. Furthermore, our qualitative analysis reveals that LLMs occasionally distort user intent merely to claim task completion, critically affecting user satisfaction.

</details>


### [178] [Can Large Language Models Still Explain Themselves? Investigating the Impact of Quantization on Self-Explanations](https://arxiv.org/abs/2601.00282)
*Qianli Wang,Nils Feldhus,Pepa Atanasova,Fedor Splitt,Simon Ostermann,Sebastian Möller,Vera Schmitt*

Main category: cs.CL

TL;DR: 研究量化对大语言模型自解释的影响，发现量化会导致自解释质量和忠实度下降，不同模型和量化技术影响不同，但不影响量化作为模型压缩技术的有效性。


<details>
  <summary>Details</summary>
Motivation: 量化广泛用于加速大语言模型推理和部署，但对自解释的影响未知，而自解释在高风险应用的透明度方面越来越重要，因此需要研究量化对自解释质量和忠实度的影响。

Method: 研究使用三种常见量化技术在不同位宽下对大语言模型进行量化，生成自然语言解释和反事实示例两种自解释，并进行用户研究。

Result: 量化通常导致自解释质量下降最多4.4%、忠实度下降最多2.38%，用户研究表明量化降低自解释的连贯性和可信度最多8.5%；大模型在自解释质量上对量化的恢复能力有限，但更能保持忠实度；没有一种量化技术在任务准确性、自解释质量和忠实度上始终表现出色。

Conclusion: 建议针对特定用例验证自解释质量，尤其是自然语言解释；量化对自解释质量和忠实度的影响较小，不影响其作为模型压缩技术的有效性。

Abstract: Quantization is widely used to accelerate inference and streamline the deployment of large language models (LLMs), yet its effects on self-explanations (SEs) remain unexplored. SEs, generated by LLMs to justify their own outputs, require reasoning about the model's own decision-making process, a capability that may exhibit particular sensitivity to quantization. As SEs are increasingly relied upon for transparency in high-stakes applications, understanding whether and to what extent quantization degrades SE quality and faithfulness is critical. To address this gap, we examine two types of SEs: natural language explanations (NLEs) and counterfactual examples, generated by LLMs quantized using three common techniques at distinct bit widths. Our findings indicate that quantization typically leads to moderate declines in both SE quality (up to 4.4\%) and faithfulness (up to 2.38\%). The user study further demonstrates that quantization diminishes both the coherence and trustworthiness of SEs (up to 8.5\%). Compared to smaller models, larger models show limited resilience to quantization in terms of SE quality but better maintain faithfulness. Moreover, no quantization technique consistently excels across task accuracy, SE quality, and faithfulness. Given that quantization's impact varies by context, we recommend validating SE quality for specific use cases, especially for NLEs, which show greater sensitivity. Nonetheless, the relatively minor deterioration in SE quality and faithfulness does not undermine quantization's effectiveness as a model compression technique.

</details>


### [179] [DepFlow: Disentangled Speech Generation to Mitigate Semantic Bias in Depression Detection](https://arxiv.org/abs/2601.00303)
*Yuxin Li,Xiangyu Zhang,Yifei Li,Zhiwei Guo,Haoyang Zhang,Eng Siong Chng,Cuntai Guan*

Main category: cs.CL

TL;DR: 提出DepFlow框架缓解语义偏差，构建CDoA数据集提升抑郁检测鲁棒性，还能用于对话系统和评估。


<details>
  <summary>Details</summary>
Motivation: 现有抑郁数据集存在语义偏差，模型在真实场景（如伪装抑郁）中鲁棒性受影响，需缓解该问题。

Method: 提出三阶段DepFlow框架，包含抑郁声学编码器、带FiLM调制的流匹配TTS模型、基于原型的严重程度映射机制，构建CDoA数据集。

Result: 抑郁声学编码器ROC - AUC达0.693；CDoA在三种抑郁检测架构中提升macro - F1分别为9%、12%和5%，优于传统增强策略。

Conclusion: DepFlow框架可缓解语义偏差，提升模型鲁棒性，还为对话系统和评估提供可控合成平台。

Abstract: Speech is a scalable and non-invasive biomarker for early mental health screening. However, widely used depression datasets like DAIC-WOZ exhibit strong coupling between linguistic sentiment and diagnostic labels, encouraging models to learn semantic shortcuts. As a result, model robustness may be compromised in real-world scenarios, such as Camouflaged Depression, where individuals maintain socially positive or neutral language despite underlying depressive states. To mitigate this semantic bias, we propose DepFlow, a three-stage depression-conditioned text-to-speech framework. First, a Depression Acoustic Encoder learns speaker- and content-invariant depression embeddings through adversarial training, achieving effective disentanglement while preserving depression discriminability (ROC-AUC: 0.693). Second, a flow-matching TTS model with FiLM modulation injects these embeddings into synthesis, enabling control over depressive severity while preserving content and speaker identity. Third, a prototype-based severity mapping mechanism provides smooth and interpretable manipulation across the depression continuum. Using DepFlow, we construct a Camouflage Depression-oriented Augmentation (CDoA) dataset that pairs depressed acoustic patterns with positive/neutral content from a sentiment-stratified text bank, creating acoustic-semantic mismatches underrepresented in natural data. Evaluated across three depression detection architectures, CDoA improves macro-F1 by 9%, 12%, and 5%, respectively, consistently outperforming conventional augmentation strategies in depression Detection. Beyond enhancing robustness, DepFlow provides a controllable synthesis platform for conversational systems and simulation-based evaluation, where real clinical data remains limited by ethical and coverage constraints.

</details>


### [180] [Robust Uncertainty Quantification for Factual Generation of Large Language Models](https://arxiv.org/abs/2601.00348)
*Yuhao Zhang,Zhongliang Yang,Linna Zhou*

Main category: cs.CL

TL;DR: 大语言模型（LLM）幻觉问题影响可靠性，传统不确定性量化方法有局限，本研究提出多事实生成任务中的不确定性量化场景及新方法RU，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 当前传统不确定性量化方法在应对非规范或对抗性提问策略时存在不足，影响LLM在现实应用中回答的可靠性，需填补这一空白。

Method: 构建包含假名字的陷阱问题集，提出新颖且鲁棒的不确定性量化方法（RU）。

Result: 构建的陷阱问题集表现出色，与四种不同模型的基线方法相比，所提方法性能优异，ROCAUC值平均提高0.1 - 0.2。

Conclusion: 所提方法为解决LLM的幻觉问题提供了新的视角和方法。

Abstract: The rapid advancement of large language model(LLM) technology has facilitated its integration into various domains of professional and daily life. However, the persistent challenge of LLM hallucination has emerged as a critical limitation, significantly compromising the reliability and trustworthiness of AI-generated content. This challenge has garnered significant attention within the scientific community, prompting extensive research efforts in hallucination detection and mitigation strategies. Current methodological frameworks reveal a critical limitation: traditional uncertainty quantification approaches demonstrate effectiveness primarily within conventional question-answering paradigms, yet exhibit notable deficiencies when confronted with non-canonical or adversarial questioning strategies. This performance gap raises substantial concerns regarding the dependability of LLM responses in real-world applications requiring robust critical thinking capabilities. This study aims to fill this gap by proposing an uncertainty quantification scenario in the task of generating with multiple facts. We have meticulously constructed a set of trap questions contained with fake names. Based on this scenario, we innovatively propose a novel and robust uncertainty quantification method(RU). A series of experiments have been conducted to verify its effectiveness. The results show that the constructed set of trap questions performs excellently. Moreover, when compared with the baseline methods on four different models, our proposed method has demonstrated great performance, with an average increase of 0.1-0.2 in ROCAUC values compared to the best performing baseline method, providing new sights and methods for addressing the hallucination issue of LLMs.

</details>


### [181] [BERT-JEPA: Reorganizing CLS Embeddings for Language-Invariant Semantics](https://arxiv.org/abs/2601.00366)
*Taj Gillin,Adam Lalani,Kenneth Zhang,Marcel Mateos Salles*

Main category: cs.CL

TL;DR: 介绍了一种将JEPA训练目标添加到BERT式模型的训练范式BERT - JEPA，能提高多语言基准性能。


<details>
  <summary>Details</summary>
Motivation: 解决BERT式模型中[CLS]嵌入空间崩溃问题，将其转化为语言无关空间。

Method: 引入BERT - JEPA训练范式，即在BERT式模型中添加JEPA训练目标。

Result: 新结构在多语言基准测试中带来了性能提升。

Conclusion: BERT - JEPA能有效改善BERT式模型的性能。

Abstract: Joint Embedding Predictive Architectures (JEPA) are a novel self supervised training technique that have shown recent promise across domains. We introduce BERT-JEPA (BEPA), a training paradigm that adds a JEPA training objective to BERT-style models, working to combat a collapsed [CLS] embedding space and turning it into a language-agnostic space. This new structure leads to increased performance across multilingual benchmarks.

</details>


### [182] [Do LLMs Judge Distantly Supervised Named Entity Labels Well? Constructing the JudgeWEL Dataset](https://arxiv.org/abs/2601.00411)
*Alistair Plum,Laura Bernardy,Tharindu Ranasinghe*

Main category: cs.CL

TL;DR: 提出judgeWEL数据集用于卢森堡语命名实体识别，利用维基百科和Wikidata信息，经大语言模型处理，结果优于现有数据集。


<details>
  <summary>Details</summary>
Motivation: 构建低资源语言数据集是自然语言处理的主要瓶颈，资源稀缺和语言特性使大规模标注成本高且可能不一致。

Method: 利用维基百科和Wikidata进行弱监督，通过文章内链接推断实体类型生成初始标注，用多种大语言模型减少噪声，保留高质量标注。

Result: 生成的数据集比现有卢森堡语NER数据集大五倍，实体类别覆盖更广、更平衡。

Conclusion: judgeWEL为多语言和低资源NER研究提供重要新资源。

Abstract: We present judgeWEL, a dataset for named entity recognition (NER) in Luxembourgish, automatically labelled and subsequently verified using large language models (LLM) in a novel pipeline. Building datasets for under-represented languages remains one of the major bottlenecks in natural language processing, where the scarcity of resources and linguistic particularities make large-scale annotation costly and potentially inconsistent. To address these challenges, we propose and evaluate a novel approach that leverages Wikipedia and Wikidata as structured sources of weak supervision. By exploiting internal links within Wikipedia articles, we infer entity types based on their corresponding Wikidata entries, thereby generating initial annotations with minimal human intervention. Because such links are not uniformly reliable, we mitigate noise by employing and comparing several LLMs to identify and retain only high-quality labelled sentences. The resulting corpus is approximately five times larger than the currently available Luxembourgish NER dataset and offers broader and more balanced coverage across entity categories, providing a substantial new resource for multilingual and low-resource NER research.

</details>


### [183] [Language as Mathematical Structure: Examining Semantic Field Theory Against Language Games](https://arxiv.org/abs/2601.00448)
*Dimitris Vartziotis*

Main category: cs.CL

TL;DR: 本文对比语言意义的两种研究方法，形式化相关概念，分析大语言模型与概念的关系，认为数学结构和语言游戏互补，明确纯统计语言模型的范围和局限并提出新方向。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型背景下检验长期存在的语言意义理论。

Method: 对比社会建构主义和语义场理论两种方法，形式化词汇场和语言场概念，分析transformer架构核心属性与概念的关系。

Result: 大语言模型捕捉语义规律支持语言有潜在数学结构，其在语用推理和语境敏感性上的局限与语言使用哲学观点中社会基础的重要性一致。

Conclusion: 数学结构和语言游戏是互补而非竞争的视角，该框架明确纯统计语言模型的范围和局限，为理论驱动的AI架构指明新方向。

Abstract: Large language models (LLMs) offer a new empirical setting in which long-standing theories of linguistic meaning can be examined. This paper contrasts two broad approaches: social constructivist accounts associated with language games, and a mathematically oriented framework we call Semantic Field Theory. Building on earlier work by the author, we formalize the notions of lexical fields (Lexfelder) and linguistic fields (Lingofelder) as interacting structures in a continuous semantic space. We then analyze how core properties of transformer architectures-such as distributed representations, attention mechanisms, and geometric regularities in embedding spaces-relate to these concepts. We argue that the success of LLMs in capturing semantic regularities supports the view that language exhibits an underlying mathematical structure, while their persistent limitations in pragmatic reasoning and context sensitivity are consistent with the importance of social grounding emphasized in philosophical accounts of language use. On this basis, we suggest that mathematical structure and language games can be understood as complementary rather than competing perspectives. The resulting framework clarifies the scope and limits of purely statistical models of language and motivates new directions for theoretically informed AI architectures.

</details>


### [184] [Defensive M2S: Training Guardrail Models on Compressed Multi-turn Conversations](https://arxiv.org/abs/2601.00454)
*Hyunjun Kim*

Main category: cs.CL

TL;DR: 提出Defensive M2S训练范式，将护栏模型在M2S压缩对话上微调，能降低训练和推理成本、提高攻击检测召回率。


<details>
  <summary>Details</summary>
Motivation: 处理完整多轮对话历史计算成本高，需降低成本确保大语言模型安全部署。

Method: 提出Defensive M2S训练范式，在M2S压缩对话上微调护栏模型，进行复杂度分析，在SafeDialBench上评估。

Result: M2S将训练成本从O(n^2)降至O(n)，训练所需token减少93倍，Qwen3Guard搭配hyphenize压缩时召回率93.8%，推理token减少94.6%，比基线提高38.9个百分点。

Conclusion: M2S压缩可作为护栏部署的有效效率技术，实现长多轮对话的可扩展安全筛选。

Abstract: Guardrail models are essential for ensuring the safety of Large Language Model (LLM) deployments, but processing full multi-turn conversation histories incurs significant computational cost. We propose Defensive M2S, a training paradigm that fine-tunes guardrail models on Multi-turn to Single-turn (M2S) compressed conversations rather than complete dialogue histories. We provide a formal complexity analysis showing that M2S reduces training cost from $O(n^2)$ to $O(n)$ for $n$-turn conversations. Empirically, on our training dataset (779 samples, avg. 10.6 turns), M2S requires only 169K tokens compared to 15.7M tokens for the multi-turn baseline -- a 93$\times$ reduction. We evaluate Defensive M2S across three guardrail model families (LlamaGuard, Nemotron, Qwen3Guard) and three compression templates (hyphenize, numberize, pythonize) on SafeDialBench, a comprehensive multi-turn jailbreak benchmark. Our best configuration, Qwen3Guard with hyphenize compression, achieves 93.8% attack detection recall while reducing inference tokens by 94.6% (from 3,231 to 173 tokens per conversation). This represents a 38.9 percentage point improvement over the baseline while dramatically reducing both training and inference costs. Our findings demonstrate that M2S compression can serve as an effective efficiency technique for guardrail deployment, enabling scalable safety screening of long multi-turn conversations.

</details>


### [185] [ECR: Manifold-Guided Semantic Cues for Compact Language Models](https://arxiv.org/abs/2601.00543)
*Chung-Wei Victor Yuan*

Main category: cs.CL

TL;DR: 论文指出紧凑模型存在嵌入空间结构丢失的问题，提出Embedding Consistency Regulation (ECR)框架解决该问题，并通过实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有紧凑模型在容量有限或处理多语言数据时会丢失嵌入空间结构，现有压缩方法未能保留潜在流形结构，导致语义漂移，影响下游任务。

Method: 提出ECR框架，从教师嵌入离线计算出一组语义锚点，让紧凑模型围绕这些锚点保持一致几何结构，推理时仅增加小的投影步骤。

Result: 在100K多语言语料实验中，ECR稳定训练，跨任务和语言保留语义结构，生成更紧凑且与任务对齐的表示空间，低容量模型学习到更清晰的流形。

Conclusion: ECR帮助紧凑模型更好遵循任务要求，在严格效率或隐私限制下更易部署。

Abstract: Compact models often lose the structure of their embedding space. The issue shows up when the capacity is tight or the data spans several languages. Such collapse makes it difficult for downstream tasks to build on the resulting representation. Existing compression methods focus on aligning model outputs at a superficial level but fail to preserve the underlying manifold structure. This mismatch often leads to semantic drift in the compact model, causing both task behavior and linguistic properties to deviate from the reference model.
  To address those issues, we provide a new framework called Embedding Consistency Regulation (ECR). This framework first derives a set of semantic anchors from teacher embeddings (computed once offline). Then, the compact model learns to maintain consistent geometry around these anchors, without relying on matching logits or internal features. ECR adds only a small projection step at inference, without altering the decoding architecture or its runtime behavior.
  In experiments on a 100K multilingual corpus, ECR consistently stabilizes training and preserves semantic structure across tasks and languages. It also produces a more compact and task-aligned representation space, enabling low-capacity models to learn cleaner manifolds than conventional baselines. ECR works without teacher outputs and is compatible with, but independent of, distillation. Taken together, our results show that ECR helps compact models better follow task requirements and makes them easier to deploy under strict efficiency or privacy limits.

</details>


### [186] [Fast-weight Product Key Memory](https://arxiv.org/abs/2601.00671)
*Tianyu Zhao,Llion Jones*

Main category: cs.CL

TL;DR: 本文提出 Fast-weight Product Key Memory (FwPKM) 架构解决序列建模层存储容量与计算效率的权衡问题，实验显示其有效降低长上下文数据集困惑度并能在长序列上泛化。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型的序列建模层存在存储容量和计算效率的权衡问题，现有方法无法兼顾两者。

Method: 将稀疏的 Product Key Memory (PKM) 转变为动态的“快速权重”情景记忆 FwPKM，在训练和推理时通过局部块级梯度下降动态更新参数。

Result: FwPKM 可作为标准模块语义记忆的补充，显著降低长上下文数据集的困惑度；在 Needle in a Haystack 评估中，能在仅训练 4K 标记序列的情况下泛化到 128K 标记的上下文。

Conclusion: FwPKM 能有效解决序列建模层的权衡问题，在长上下文处理上表现出色。

Abstract: Sequence modeling layers in modern language models typically face a trade-off between storage capacity and computational efficiency. While Softmax attention offers unbounded storage at prohibitive quadratic costs, linear variants provide efficiency but suffer from limited, fixed-size storage. We propose Fast-weight Product Key Memory (FwPKM), a novel architecture that resolves this tension by transforming the sparse Product Key Memory (PKM) from a static module into a dynamic, "fast-weight" episodic memory. Unlike PKM, FwPKM updates its parameters dynamically at both training and inference time via local chunk-level gradient descent, allowing the model to rapidly memorize and retrieve new key-value pairs from input sequences. Experiments reveal that FwPKM functions as an effective episodic memory that complements the semantic memory of standard modules, yielding significant perplexity reductions on long-context datasets. Notably, in Needle in a Haystack evaluations, FwPKM generalizes to 128K-token contexts despite being trained on only 4K-token sequences.

</details>


### [187] [Exploring the Performance of Large Language Models on Subjective Span Identification Tasks](https://arxiv.org/abs/2601.00736)
*Alphaeus Dmonte,Roland Oruche,Tharindu Ranasinghe,Marcos Zampieri,Prasad Calyam*

Main category: cs.CL

TL;DR: 论文评估了多种大语言模型在三个流行任务文本跨度识别中的性能，探索了多种策略，发现文本内在关系有助于大语言模型识别。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在一些主观文本跨度识别任务上的研究不足，需要填补这一空白。

Method: 对多种大语言模型在情感分析、冒犯性语言识别和声明验证三个任务的文本跨度识别性能进行评估，探索指令调优、上下文学习和思维链等策略。

Result: 文本中的潜在关系有助于大语言模型识别精确的文本跨度。

Conclusion: 研究填补了大语言模型在主观文本跨度识别任务上的研究空白，为相关研究提供参考。

Abstract: Identifying relevant text spans is important for several downstream tasks in NLP, as it contributes to model explainability. While most span identification approaches rely on relatively smaller pre-trained language models like BERT, a few recent approaches have leveraged the latest generation of Large Language Models (LLMs) for the task. Current work has focused on explicit span identification like Named Entity Recognition (NER), while more subjective span identification with LLMs in tasks like Aspect-based Sentiment Analysis (ABSA) has been underexplored. In this paper, we fill this important gap by presenting an evaluation of the performance of various LLMs on text span identification in three popular tasks, namely sentiment analysis, offensive language identification, and claim verification. We explore several LLM strategies like instruction tuning, in-context learning, and chain of thought. Our results indicate underlying relationships within text aid LLMs in identifying precise text spans.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [188] [From Consensus to Chaos: A Vulnerability Assessment of the RAFT Algorithm](https://arxiv.org/abs/2601.00273)
*Tamer Afifi,Abdelfatah Hegazy,Ehab Abousaif*

Main category: cs.CR

TL;DR: 本文对RAFT协议进行安全分析，找出其易受消息重放和伪造攻击的弱点，提出基于密码学等的改进方案提升安全性。


<details>
  <summary>Details</summary>
Motivation: RAFT协议安全属性未获充分认知，其实现易受攻击，会导致数据不一致。

Method: 分析恶意行为者利用协议消息传递机制发动攻击的方式，通过模拟场景检验攻击可行性，找出设计弱点，并提出基于密码学、认证消息验证和新鲜度检查的解决方案。

Result: 确定了RAFT设计中易受消息重放和伪造攻击的关键弱点。

Conclusion: 提出的解决方案为增强RAFT实现的安全性和开发更具弹性的分布式系统提供框架。

Abstract: In recent decades, the RAFT distributed consensus algorithm has become a main pillar of the distributed systems ecosystem, ensuring data consistency and fault tolerance across multiple nodes. Although the fact that RAFT is well known for its simplicity, reliability, and efficiency, its security properties are not fully recognized, leaving implementations vulnerable to different kinds of attacks and threats, which can transform the RAFT harmony of consensus into a chaos of data inconsistency. This paper presents a systematic security analysis of the RAFT protocol, with a specific focus on its susceptibility to security threats such as message replay attacks and message forgery attacks. Examined how a malicious actor can exploit the protocol's message-passing mechanism to reintroduce old messages, disrupting the consensus process and leading to data inconsistency. The practical feasibility of these attacks is examined through simulated scenarios, and the key weaknesses in RAFT's design that enable them are identified. To address these vulnerabilities, a novel approach based on cryptography, authenticated message verification, and freshness check is proposed. This proposed solution provides a framework for enhancing the security of the RAFT implementations and guiding the development of more resilient distributed systems.

</details>


### [189] [Secure, Verifiable, and Scalable Multi-Client Data Sharing via Consensus-Based Privacy-Preserving Data Distribution](https://arxiv.org/abs/2601.00418)
*Prajwal Panth,Sahaj Raj Malla*

Main category: cs.CR

TL;DR: 提出CPPDD框架用于安全多客户端数据聚合，有良好性能并适用于多个领域。


<details>
  <summary>Details</summary>
Motivation: 解决受监管和资源受限环境下可扩展性、信任最小化和可验证多方计算的关键问题。

Method: 采用双层保护机制，结合每客户端仿射掩码与优先级驱动顺序共识锁定，通过步骤和数据校验和验证去中心化完整性。

Result: 实现100%恶意偏差检测、精确数据恢复，相比MPC和HE基线FLOPs低三到四个数量级，在MNIST派生向量上线性可扩展到N = 500。

Conclusion: CPPDD框架可用于安全投票、联合联邦学习等场景，具有良好的安全性和性能。

Abstract: We propose the Consensus-Based Privacy-Preserving Data Distribution (CPPDD) framework, a lightweight and post-setup autonomous protocol for secure multi-client data aggregation. The framework enforces unanimous-release confidentiality through a dual-layer protection mechanism that combines per-client affine masking with priority-driven sequential consensus locking. Decentralized integrity is verified via step (sigma_S) and data (sigma_D) checksums, facilitating autonomous malicious deviation detection and atomic abort without requiring persistent coordination. The design supports scalar, vector, and matrix payloads with O(N*D) computation and communication complexity, optional edge-server offloading, and resistance to collusion under N-1 corruptions. Formal analysis proves correctness, Consensus-Dependent Integrity and Fairness (CDIF) with overwhelming-probability abort on deviation, and IND-CPA security assuming a pseudorandom function family. Empirical evaluations on MNIST-derived vectors demonstrate linear scalability up to N = 500 with sub-millisecond per-client computation times. The framework achieves 100% malicious deviation detection, exact data recovery, and three-to-four orders of magnitude lower FLOPs compared to MPC and HE baselines. CPPDD enables atomic collaboration in secure voting, consortium federated learning, blockchain escrows, and geo-information capacity building, addressing critical gaps in scalability, trust minimization, and verifiable multi-party computation for regulated and resource-constrained environments.

</details>


### [190] [Rectifying Adversarial Examples Using Their Vulnerabilities](https://arxiv.org/abs/2601.00270)
*Fumiya Morimoto,Ryuto Morita,Satoshi Ono*

Main category: cs.CR

TL;DR: 本文提出一种纠正对抗样本（AEs）的方法，通过重新攻击AEs使其越过决策边界来预测正确标签，该方法对多种攻击生成的AEs纠正表现稳定，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法多注重AE检测，未解决攻击前正确样本分类问题，而部分任务需识别正确原始输入类别，因此要提出纠正AEs以估计其原始输入正确标签的方法。

Method: 基于重新攻击AEs，使其越过决策边界以准确预测标签，仅将AEs作为输入，无需参数调整或预训练。

Result: 该方法对多种攻击（包括有针对性和黑盒攻击）生成的AEs纠正表现一致，在抗各种攻击稳定性上优于传统纠正和输入转换方法。

Conclusion: 提出的方法能有效纠正多种攻击产生的AEs，且在稳定性上表现更好。

Abstract: Deep neural network-based classifiers are prone to errors when processing adversarial examples (AEs). AEs are minimally perturbed input data undetectable to humans posing significant risks to security-dependent applications. Hence, extensive research has been undertaken to develop defense mechanisms that mitigate their threats. Most existing methods primarily focus on discriminating AEs based on the input sample features, emphasizing AE detection without addressing the correct sample categorization before an attack. While some tasks may only require mere rejection on detected AEs, others necessitate identifying the correct original input category such as traffic sign recognition in autonomous driving. The objective of this study is to propose a method for rectifying AEs to estimate the correct labels of their original inputs. Our method is based on re-attacking AEs to move them beyond the decision boundary for accurate label prediction, effectively addressing the issue of rectifying minimally perceptible AEs created using white-box attack methods. However, challenge remains with respect to effectively rectifying AEs produced by black-box attacks at a distance from the boundary, or those misclassified into low-confidence categories by targeted attacks. By adopting a straightforward approach of only considering AEs as inputs, the proposed method can address diverse attacks while avoiding the requirement of parameter adjustments or preliminary training. Results demonstrate that the proposed method exhibits consistent performance in rectifying AEs generated via various attack methods, including targeted and black-box attacks. Moreover, it outperforms conventional rectification and input transformation methods in terms of stability against various attacks.

</details>


### [191] [Security in the Age of AI Teammates: An Empirical Study of Agentic Pull Requests on GitHub](https://arxiv.org/abs/2601.00477)
*Mohammed Latif Siddiq,Xinye Zhao,Vinicius Carvalho Lopes,Beatrice Casey,Joanna C. S. Santos*

Main category: cs.CR

TL;DR: 本研究通过大型实证分析，发现安全相关的智能体拉取请求占约4%，主要进行安全强化活动，合并率低、审查时间长，拒绝与请求复杂度和冗长度有关。


<details>
  <summary>Details</summary>
Motivation: 系统地描述自主编码智能体如何在实践中对软件安全做出贡献、这些安全相关贡献如何被审查和接受，以及哪些可观察信号与拉取请求被拒相关。

Method: 使用AIDev数据集对智能体编写的拉取请求进行大规模实证分析，用关键词过滤策略识别安全相关请求并手动验证，分析流行度、接受结果和审查延迟，应用定性开放编码识别安全相关行动和意图，检查审查元数据识别被拒早期信号。

Result: 安全相关智能体拉取请求占智能体活动约4%，常进行安全强化活动；相比非安全请求，其合并率低、审查时间长，不同智能体和编程生态有差异；请求被拒与复杂度和冗长度关联更强。

Conclusion: 自主编码智能体在软件安全中有贡献，但安全相关拉取请求受更严格审查，请求复杂度和冗长度对被拒影响更大。

Abstract: Autonomous coding agents are increasingly deployed as AI teammates in modern software engineering, independently authoring pull requests (PRs) that modify production code at scale. This study aims to systematically characterize how autonomous coding agents contribute to software security in practice, how these security-related contributions are reviewed and accepted, and which observable signals are associated with PR rejection. We conduct a large-scale empirical analysis of agent-authored PRs using the AIDev dataset, comprising of over 33,000 curated PRs from popular GitHub repositories. Security-relevant PRs are identified using a keyword filtering strategy, followed by manual validation, resulting in 1,293 confirmed security-related agentic-PRs. We then analyze prevalence, acceptance outcomes, and review latency across autonomous agents, programming ecosystems, and types of code changes. Moreover, we apply qualitative open coding to identify recurring security-related actions and underlying intents, and examine review metadata to identify early signals associated with PR rejection. Security-related Agentic-PRs constitute a meaningful share of agent activity (approximately 4\%). Rather than focusing solely on narrow vulnerability fixes, agents most frequently perform supportive security hardening activities, including testing, documentation, configuration, and improved error handling. Compared to non-security PRs, security-related Agentic-PRs exhibit lower merge rates and longer review latency, reflecting heightened human scrutiny, with variation across agents and programming ecosystems. PR rejection is more strongly associated with PR complexity and verbosity than with explicit security topics.

</details>


### [192] [Towards Understanding and Characterizing Vulnerabilities in Intelligent Connected Vehicles through Real-World Exploits](https://arxiv.org/abs/2601.00627)
*Yuelin Wang,Yuqiao Ning,Yanbang Sun,Xiaofei Xie,Zhihua Xie,Yang Chen,Zhen Guo,Shihao Xue,Junjie Wang,Sen Chen*

Main category: cs.CR

TL;DR: 本文对智能网联汽车（ICV）的漏洞进行了大规模实证研究，收集漏洞数据评估现有分类法，发现新漏洞位置和类型，公开数据集。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注ICV特定子组件，缺乏系统理解，且多依赖主观分析，理论与实际攻击有差距。

Method: 分析现有文献总结分类法，收集649个可利用漏洞数据，评估分类法覆盖度。

Result: 发现1个新漏洞位置和13个新类型，将漏洞分为6种威胁类型和4种风险等级，分析竞赛参与者技能和涉及的ICV类型。

Conclusion: 本研究提供了全面、数据驱动的ICV漏洞分析，为相关人员提供见解，公开数据集支持未来研究。

Abstract: Intelligent Connected Vehicles (ICVs) are a core component of modern transportation systems, and their security is crucial as it directly relates to user safety. Despite prior research, most existing studies focus only on specific sub-components of ICVs due to their inherent complexity. As a result, there is a lack of systematic understanding of ICV vulnerabilities. Moreover, much of the current literature relies on human subjective analysis, such as surveys and interviews, which tends to be high-level and unvalidated, leaving a significant gap between theoretical findings and real-world attacks. To address this issue, we conducted the first large-scale empirical study on ICV vulnerabilities. We began by analyzing existing ICV security literature and summarizing the prevailing taxonomies in terms of vulnerability locations and types. To evaluate their real-world relevance, we collected a total of 649 exploitable vulnerabilities, including 592 from eight ICV vulnerability discovery competitions, Anonymous Cup, between January 2023 and April 2024, covering 48 different vehicles. The remaining 57 vulnerabilities were submitted daily by researchers. Based on this dataset, we assessed the coverage of existing taxonomies and identified several gaps, discovering one new vulnerability location and 13 new vulnerability types. We further categorized these vulnerabilities into 6 threat types (e.g., privacy data breach) and 4 risk levels (ranging from low to critical) and analyzed participants' skills and the types of ICVs involved in the competitions. This study provides a comprehensive and data-driven analysis of ICV vulnerabilities, offering actionable insights for researchers, industry practitioners, and policymakers. To support future research, we have made our vulnerability dataset publicly available.

</details>


### [193] [Large Empirical Case Study: Go-Explore adapted for AI Red Team Testing](https://arxiv.org/abs/2601.00042)
*Manish Bhatt,Adrian Wood,Idan Habler,Ammar Al-Kahfah*

Main category: cs.CR

TL;DR: 对具备工具使用能力的生产大语言模型代理进行安全测试，用Go - Explore评估GPT - 4o - mini，发现种子方差、奖励塑造等因素对测试结果的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管经过安全训练，具备工具使用能力的生产大语言模型代理仍需进行安全测试。

Method: 采用Go - Explore方法，对GPT - 4o - mini进行28次实验，涵盖六个研究问题。

Result: 随机种子方差主导算法参数，多种子平均可降低方差；奖励塑造损害性能；简单状态签名表现更好；集成测试提供攻击类型多样性，单代理优化特定攻击类型覆盖。

Conclusion: 测试安全训练模型时，种子方差和特定领域知识可能比算法复杂性更重要。

Abstract: Production LLM agents with tool-using capabilities require security testing despite their safety training. We adapt Go-Explore to evaluate GPT-4o-mini across 28 experimental runs spanning six research questions. We find that random-seed variance dominates algorithmic parameters, yielding an 8x spread in outcomes; single-seed comparisons are unreliable, while multi-seed averaging materially reduces variance in our setup. Reward shaping consistently harms performance, causing exploration collapse in 94% of runs or producing 18 false positives with zero verified attacks. In our environment, simple state signatures outperform complex ones. For comprehensive security testing, ensembles provide attack-type diversity, whereas single agents optimize coverage within a given attack type. Overall, these results suggest that seed variance and targeted domain knowledge can outweigh algorithmic sophistication when testing safety-trained models.

</details>


### [194] [PatchBlock: A Lightweight Defense Against Adversarial Patches for Embedded EdgeAI Devices](https://arxiv.org/abs/2601.00367)
*Nandish Chattopadhyay,Abdul Basit,Amira Guesmi,Muhammad Abdullah Hanif,Bassem Ouni,Muhammad Shafique*

Main category: cs.CR

TL;DR: 提出轻量级框架PatchBlock检测和中和图像中的对抗性补丁，经多方面评估表现良好，适合EdgeAI应用。


<details>
  <summary>Details</summary>
Motivation: 对抗性攻击对EdgeAI应用中机器学习模型的可靠部署构成挑战，特别是基于补丁的对抗性攻击有严重后果，需要解决方法。

Method: PatchBlock采用三阶段流水线，包括分块、异常区域检测和降维，利用异常值检测和降维技术，可作为传感器级预处理模块并行运行。

Result: 在多种神经网络架构、数据集、攻击类型和边缘设备上评估，能恢复高达77%的模型准确率，在效率上超越现有防御方法。

Conclusion: PatchBlock提高了模型鲁棒性，具有高可移植性和最小的干净准确率损失，适合EdgeAI应用。

Abstract: Adversarial attacks pose a significant challenge to the reliable deployment of machine learning models in EdgeAI applications, such as autonomous driving and surveillance, which rely on resource-constrained devices for real-time inference. Among these, patch-based adversarial attacks, where small malicious patches (e.g., stickers) are applied to objects, can deceive neural networks into making incorrect predictions with potentially severe consequences. In this paper, we present PatchBlock, a lightweight framework designed to detect and neutralize adversarial patches in images. Leveraging outlier detection and dimensionality reduction, PatchBlock identifies regions affected by adversarial noise and suppresses their impact. It operates as a pre-processing module at the sensor level, efficiently running on CPUs in parallel with GPU inference, thus preserving system throughput while avoiding additional GPU overhead. The framework follows a three-stage pipeline: splitting the input into chunks (Chunking), detecting anomalous regions via a redesigned isolation forest with targeted cuts for faster convergence (Separating), and applying dimensionality reduction on the identified outliers (Mitigating). PatchBlock is both model- and patch-agnostic, can be retrofitted to existing pipelines, and integrates seamlessly between sensor inputs and downstream models. Evaluations across multiple neural architectures, benchmark datasets, attack types, and diverse edge devices demonstrate that PatchBlock consistently improves robustness, recovering up to 77% of model accuracy under strong patch attacks such as the Google Adversarial Patch, while maintaining high portability and minimal clean accuracy loss. Additionally, PatchBlock outperforms the state-of-the-art defenses in efficiency, in terms of computation time and energy consumption per sample, making it suitable for EdgeAI applications.

</details>


### [195] [Engineering Attack Vectors and Detecting Anomalies in Additive Manufacturing](https://arxiv.org/abs/2601.00384)
*Md Mahbub Hasan,Marcus Sternhagen,Krishna Chandra Roy*

Main category: cs.CR

TL;DR: 本文研究FDM系统面临的中间人攻击，提出基于机器日志分析的无监督入侵检测系统，实验证明该方法能有效区分正常和受攻击执行情况。


<details>
  <summary>Details</summary>
Motivation: 增材制造的网络物理融合带来新攻击面，需研究针对FDM系统的攻击及应对策略。

Method: 对两个FDM系统发起多层中间人攻击，提出使用基于Transformer编码器、对比训练投影头、聚类和自注意力自动编码器的无监督入侵检测系统。

Result: 实验结果表明该方法能有效区分正常和受攻击执行情况。

Conclusion: 所提出的无监督入侵检测系统可有效应对FDM系统面临的中间人攻击。

Abstract: Additive manufacturing (AM) is rapidly integrating into critical sectors such as aerospace, automotive, and healthcare. However, this cyber-physical convergence introduces new attack surfaces, especially at the interface between computer-aided design (CAD) and machine execution layers. In this work, we investigate targeted cyberattacks on two widely used fused deposition modeling (FDM) systems, Creality's flagship model K1 Max, and Ender 3. Our threat model is a multi-layered Man-in-the-Middle (MitM) intrusion, where the adversary intercepts and manipulates G-code files during upload from the user interface to the printer firmware. The MitM intrusion chain enables several stealthy sabotage scenarios. These attacks remain undetectable by conventional slicer software or runtime interfaces, resulting in structurally defective yet externally plausible printed parts. To counter these stealthy threats, we propose an unsupervised Intrusion Detection System (IDS) that analyzes structured machine logs generated during live printing. Our defense mechanism uses a frozen Transformer-based encoder (a BERT variant) to extract semantic representations of system behavior, followed by a contrastively trained projection head that learns anomaly-sensitive embeddings. Later, a clustering-based approach and a self-attention autoencoder are used for classification. Experimental results demonstrate that our approach effectively distinguishes between benign and compromised executions.

</details>


### [196] [Cracking IoT Security: Can LLMs Outsmart Static Analysis Tools?](https://arxiv.org/abs/2601.00559)
*Jason Quantrill,Noura Khajehnouri,Zihan Guo,Manar H. Alalfi*

Main category: cs.CR

TL;DR: 本文评估大语言模型（LLMs）检测智能家居物联网平台规则交互威胁的能力，发现LLMs在跨规则结构推理上准确性差，符号推理基线更稳定，强调混合架构潜力。


<details>
  <summary>Details</summary>
Motivation: 智能家居物联网平台规则交互会产生威胁，传统依赖符号静态分析，需评估LLMs检测能力。

Method: 对多类别交互威胁分类全面评估LLMs，在原始和突变数据集上对多个模型进行零、一和二样本设置基准测试，与手动验证的真值对比。

Result: LLMs在语义理解上有前景，但跨规则结构推理准确性显著下降，模型性能因威胁类别和提示设置差异大，符号推理基线检测稳定。

Conclusion: LLMs单独用于物联网环境安全关键交互威胁检测不可靠，混合架构结合符号分析和LLM语义解释有潜力。

Abstract: Smart home IoT platforms such as openHAB rely on Trigger Action Condition (TAC) rules to automate device behavior, but the interplay among these rules can give rise to interaction threats, unintended or unsafe behaviors emerging from implicit dependencies, conflicting triggers, or overlapping conditions. Identifying these threats requires semantic understanding and structural reasoning that traditionally depend on symbolic, constraint-driven static analysis. This work presents the first comprehensive evaluation of Large Language Models (LLMs) across a multi-category interaction threat taxonomy, assessing their performance on both the original openHAB (oHC/IoTB) dataset and a structurally challenging Mutation dataset designed to test robustness under rule transformations. We benchmark Llama 3.1 8B, Llama 70B, GPT-4o, Gemini-2.5-Pro, and DeepSeek-R1 across zero-, one-, and two-shot settings, comparing their results against oHIT's manually validated ground truth. Our findings show that while LLMs exhibit promising semantic understanding, particularly on action- and condition-related threats, their accuracy degrades significantly for threats requiring cross-rule structural reasoning, especially under mutated rule forms. Model performance varies widely across threat categories and prompt settings, with no model providing consistent reliability. In contrast, the symbolic reasoning baseline maintains stable detection across both datasets, unaffected by rule rewrites or structural perturbations. These results underscore that LLMs alone are not yet dependable for safety critical interaction-threat detection in IoT environments. We discuss the implications for tool design and highlight the potential of hybrid architectures that combine symbolic analysis with LLM-based semantic interpretation to reduce false positives while maintaining structural rigor.

</details>


### [197] [NOS-Gate: Queue-Aware Streaming IDS for Consumer Gateways under Timing-Controlled Evasion](https://arxiv.org/abs/2601.00389)
*Muhammad Bilal,Omer Tariq,Hasan Ahmed*

Main category: cs.CR

TL;DR: 提出用于独立网关的流式入侵检测系统NOS - Gate，在低误报率下召回率高，能降低延迟且评分成本低。


<details>
  <summary>Details</summary>
Motivation: 加密流量的时序和突发模式会泄露信息，现有独立消费网关的仅元数据检测方法受影响，需在有限CPU和延迟预算下对加密流量进行流式入侵检测。

Method: 为每个流实例化基于网络优化脉冲（NOS）动态的轻量级双态单元，对元数据特征的固定长度窗口评分，依据K - of - M持久性规则触发可逆缓解措施，所有方法通过预烧分位数阈值无标签校准。

Result: 在0.1%误报率下，NOS - Gate的事件召回率达0.952，高于最佳基线的0.857；能降低p99.9排队延迟和p99.9附带延迟，每个流窗口平均评分成本约2.09微秒。

Conclusion: NOS - Gate在加密流量的流式入侵检测中表现良好，能有效应对加密流量的安全威胁，同时满足CPU和延迟预算要求。

Abstract: Timing and burst patterns can leak through encryption, and an adaptive adversary can exploit them. This undermines metadata-only detection in a stand-alone consumer gateway. Therefore, consumer gateways need streaming intrusion detection on encrypted traffic using metadata only, under tight CPU and latency budgets. We present a streaming IDS for stand-alone gateways that instantiates a lightweight two-state unit derived from Network-Optimised Spiking (NOS) dynamics per flow, named NOS-Gate. NOS-Gate scores fixed-length windows of metadata features and, under a $K$-of-$M$ persistence rule, triggers a reversible mitigation that temporarily reduces the flow's weight under weighted fair queueing (WFQ). We evaluate NOS-Gate under timing-controlled evasion using an executable 'worlds' benchmark that specifies benign device processes, auditable attacker budgets, contention structure, and packet-level WFQ replay to quantify queue impact. All methods are calibrated label-free via burn-in quantile thresholding. Across multiple reproducible worlds and malicious episodes, at an achieved $0.1%$ false-positive operating point, NOS-Gate attains 0.952 incident recall versus 0.857 for the best baseline in these runs. Under gating, it reduces p99.9 queueing delay and p99.9 collateral delay with a mean scoring cost of ~ 2.09 μs per flow-window on CPU.

</details>


### [198] [Improving LLM-Assisted Secure Code Generation through Retrieval-Augmented-Generation and Multi-Tool Feedback](https://arxiv.org/abs/2601.00509)
*Vidyut Sriram,Sawan Pandita,Achintya Lakshmanan,Aneesh Shamraj,Suman Saha*

Main category: cs.CR

TL;DR: 本文提出检索增强的多工具修复工作流，使用编译器诊断等工具让大语言模型迭代修复代码输出，在两个模型代码数据集上评估，显著提高代码鲁棒性并降低安全漏洞率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成代码存在安全漏洞、逻辑不一致和编译错误等问题，此前研究表明结构化反馈等对模型有益，因此提出改进工作流。

Method: 提出检索增强的多工具修复工作流，让单一代码生成大语言模型使用编译器诊断、CodeQL安全扫描和KLEE符号执行迭代改进输出，用轻量级嵌入模型进行语义检索提供安全修复示例。

Result: 在3242个程序数据集评估，DeepSeek安全漏洞减少96%，CodeLlama关键安全缺陷率从58.55%降至22.19%。

Conclusion: 工具辅助的自我修复工作流有效，即使对较难改进的模型也能显著提高代码鲁棒性和降低安全漏洞。

Abstract: Large Language Models (LLMs) can generate code but often introduce security vulnerabilities, logical inconsistencies, and compilation errors. Prior work demonstrates that LLMs benefit substantially from structured feedback, static analysis, retrieval augmentation, and execution-based refinement. We propose a retrieval-augmented, multi-tool repair workflow in which a single code-generating LLM iteratively refines its outputs using compiler diagnostics, CodeQL security scanning, and KLEE symbolic execution. A lightweight embedding model is used for semantic retrieval of previously successful repairs, providing security-focused examples that guide generation. Evaluated on a combined dataset of 3,242 programs generated by DeepSeek-Coder-1.3B and CodeLlama-7B, the system demonstrates significant improvements in robustness. For DeepSeek, security vulnerabilities were reduced by 96%. For the larger CodeLlama model, the critical security defect rate was decreased from 58.55% to 22.19%, highlighting the efficacy of tool-assisted self-repair even on "stubborn" models.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [199] [Combining datasets with different ground truths using Low-Rank Adaptation to generalize image-based CNN models for photometric redshift prediction](https://arxiv.org/abs/2601.00146)
*Vikram Seenivasan,Srinath Saikrishnan,Andrew Lizarraga,Jonathan Soriano,Bernie Boscoe,Tuan Do*

Main category: astro-ph.IM

TL;DR: 本文展示如何用LoRA结合不同星系成像数据集，提升基于CNN模型的红移估计效果，比较不同方法优劣，证明LoRA在微调天体物理回归模型的价值。


<details>
  <summary>Details</summary>
Motivation: 将包含不同类型星系但精度较低的测光红移真值数据集和高精度但观测耗时大、数据量有限的光谱红移真值数据集结合，以获得泛化性好、更准确的模型。

Method: 先使用测光红移真值数据集训练基础模型，再用LoRA在光谱红移真值数据集上微调。

Result: LoRA模型比传统迁移学习方法表现更好，偏差和约小2.5倍，离散度约小2.2倍；在组合数据集上重新训练的模型泛化性优于LoRA但计算时间长。

Conclusion: LoRA为天体物理中回归模型微调提供了全重新训练和不重新训练之间的平衡，在利用现有预训练天体物理模型、处理数据稀疏任务方面有潜力。

Abstract: In this work, we demonstrate how Low-Rank Adaptation (LoRA) can be used to combine different galaxy imaging datasets to improve redshift estimation with CNN models for cosmology. LoRA is an established technique for large language models that adds adapter networks to adjust model weights and biases to efficiently fine-tune large base models without retraining. We train a base model using a photometric redshift ground truth dataset, which contains broad galaxy types but is less accurate. We then fine-tune using LoRA on a spectroscopic redshift ground truth dataset. These redshifts are more accurate but limited to bright galaxies and take orders of magnitude more time to obtain, so are less available for large surveys. Ideally, the combination of the two datasets would yield more accurate models that generalize well. The LoRA model performs better than a traditional transfer learning method, with $\sim2.5\times$ less bias and $\sim$2.2$\times$ less scatter. Retraining the model on a combined dataset yields a model that generalizes better than LoRA but at a cost of greater computation time. Our work shows that LoRA is useful for fine-tuning regression models in astrophysics by providing a middle ground between full retraining and no retraining. LoRA shows potential in allowing us to leverage existing pretrained astrophysical models, especially for data sparse tasks.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [200] [Mapping Human Anti-collusion Mechanisms to Multi-agent AI](https://arxiv.org/abs/2601.00360)
*Jamiu Adekunle Idowu,Ahmed Almasoud,Ayman Alfahid*

Main category: cs.MA

TL;DR: 本文为多智能体AI系统开发人类反勾结机制分类法并映射到潜在干预措施，同时指出开放挑战。


<details>
  <summary>Details</summary>
Motivation: 多智能体AI系统出现勾结策略，但不清楚如何将人类反勾结机制应用于AI场景。

Method: 开发人类反勾结机制分类法，将其映射到多智能体AI系统的潜在干预措施，并为每种机制提出实施方法。

Result: 完成分类法开发和映射，提出实施方法。

Conclusion: 明确了在多智能体AI系统应用反勾结机制存在归因问题、身份流动性、边界问题和对抗性适应等开放挑战。

Abstract: As multi-agent AI systems become increasingly autonomous, evidence shows they can develop collusive strategies similar to those long observed in human markets and institutions. While human domains have accumulated centuries of anti-collusion mechanisms, it remains unclear how these can be adapted to AI settings. This paper addresses that gap by (i) developing a taxonomy of human anti-collusion mechanisms, including sanctions, leniency & whistleblowing, monitoring & auditing, market design, and governance and (ii) mapping them to potential interventions for multi-agent AI systems. For each mechanism, we propose implementation approaches. We also highlight open challenges, such as the attribution problem (difficulty attributing emergent coordination to specific agents) identity fluidity (agents being easily forked or modified) the boundary problem (distinguishing beneficial cooperation from harmful collusion) and adversarial adaptation (agents learning to evade detection).

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [201] [Quadratic Unconstrained Binary Optimisation for Training and Regularisation of Binary Neural Networks](https://arxiv.org/abs/2601.00449)
*Jonas Christoffer Villumsen,Yusuke Sugita*

Main category: math.OC

TL;DR: 本文针对二进制神经网络（BNNs）训练问题，扩展QUBO模型，提出两种正则化方法，实验表明能提升分类准确率。


<details>
  <summary>Details</summary>
Motivation: 人工智能能耗问题凸显，BNNs适合资源受限环境，但训练具有计算挑战性，探索高效优化离散神经网络的方法。

Method: 扩展现有QUBO模型以适应任意网络拓扑，提出最大化神经元裕度和基于dropout的迭代方案两种正则化方法。

Result: 将QUBO公式应用于小的二进制图像分类问题，在基于GPU的Ising机器上实验，正则化项改变训练行为，提升测试集分类准确率。

Conclusion: 提出的正则化方法能有效改进BNNs训练，提升分类性能。

Abstract: Advances in artificial intelligence (AI) and deep learning have raised concerns about its increasing energy consumption, while demand for deploying AI in mobile devices and machines at the edge is growing. Binary neural networks (BNNs) have recently gained attention as energy and memory efficient models suitable for resource constrained environments; however, training BNNs exactly is computationally challenging because of its discrete characteristics. Recent work proposing a framework for training BNNs based on quadratic unconstrained binary optimisation (QUBO) and progress in the design of Ising machines for solving QUBO problems suggest a potential path to efficiently optimising discrete neural networks. In this work, we extend existing QUBO models for training BNNs to accommodate arbitrary network topologies and propose two novel methods for regularisation. The first method maximises neuron margins biasing the training process toward parameter configurations that yield larger pre-activation magnitudes. The second method employs a dropout-inspired iterative scheme in which reduced subnetworks are trained and used to adjust linear penalties on network parameters. We apply the proposed QUBO formulation to a small binary image classification problem and conduct computational experiments on a GPU-based Ising machine. The numerical results indicate that the proposed regularisation terms modify training behaviour and yield improvements in classification accuracy on data not present in the training set.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [202] [Interpretable Machine Learning for Quantum-Informed Property Predictions in Artificial Sensing Materials](https://arxiv.org/abs/2601.00503)
*Li Chen,Leonardo Medrano Sandonas,Shirong Huang,Alexander Croy,Gianaurelio Cuniberti*

Main category: physics.chem-ph

TL;DR: 开发了MORE - ML计算框架，结合量子力学和机器学习方法预测传感相关特性，为电子鼻应用奠定基础。


<details>
  <summary>Details</summary>
Motivation: 解决数字传感中开发可持续方法，扩展定制电子鼻对复杂体臭挥发物适用性的挑战。

Method: 开发MORE - ML框架，扩展MORE - Q数据集到MORE - QX，以电子描述符为输入建树基ML模型，用可解释AI方法分析。

Result: CatBoost模型表现最佳，尤其是在对未见化合物的可迁移性方面，可解释AI方法指出影响BF预测的QM属性。

Conclusion: MORE - ML结合量子力学和机器学习为体臭传感分子受体提供机理理解和设计原则，为分析复杂气味混合物的人工传感材料发展奠定基础。

Abstract: Digital sensing faces challenges in developing sustainable methods to extend the applicability of customized e-noses to complex body odor volatilome (BOV). To address this challenge, we developed MORE-ML, a computational framework that integrates quantum-mechanical (QM) property data of e-nose molecular building blocks with machine learning (ML) methods to predict sensing-relevant properties. Within this framework, we expanded our previous dataset, MORE-Q, to MORE-QX by sampling a larger conformational space of interactions between BOV molecules and mucin-derived receptors. This dataset provides extensive electronic binding features (BFs) computed upon BOV adsorption. Analysis of MORE-QX property space revealed weak correlations between QM properties of building blocks and resulting BFs. Leveraging this observation, we defined electronic descriptors of building blocks as inputs for tree-based ML models to predict BFs. Benchmarking showed CatBoost models outperform alternatives, especially in transferability to unseen compounds. Explainable AI methods further highlighted which QM properties most influence BF predictions. Collectively, MORE-ML combines QM insights with ML to provide mechanistic understanding and rational design principles for molecular receptors in BOV sensing. This approach establishes a foundation for advancing artificial sensing materials capable of analyzing complex odor mixtures, bridging the gap between molecular-level computations and practical e-nose applications.

</details>


### [203] [AceFF: A State-of-the-Art Machine Learning Potential for Small Molecules](https://arxiv.org/abs/2601.00581)
*Stephen E. Farr,Stefan Doerr,Antonio Mirarchi,Francesc Sabanes Zariquiey,Gianni De Fabritiis*

Main category: physics.chem-ph

TL;DR: 介绍针对小分子药物发现优化的预训练机器学习原子间势AceFF，其性能达有机分子新水平且代码公开。


<details>
  <summary>Details</summary>
Motivation: 现有MLIP难以在不同化学空间实现泛化，需开发适用于小分子药物发现的模型。

Method: 采用改进的TensorNet2架构，在类药物化合物综合数据集上训练。

Result: AceFF实现高通量推理速度与DFT级精度平衡，支持关键药物化学元素，经严格基准测试表现出色。

Conclusion: AceFF为有机分子建立了新的技术水平。

Abstract: We introduce AceFF, a pre-trained machine learning interatomic potential (MLIP) optimized for small molecule drug discovery. While MLIPs have emerged as efficient alternatives to Density Functional Theory (DFT), generalizability across diverse chemical spaces remains difficult. AceFF addresses this via a refined TensorNet2 architecture trained on a comprehensive dataset of drug-like compounds. This approach yields a force field that balances high-throughput inference speed with DFT-level accuracy. AceFF fully supports the essential medicinal chemistry elements (H, B, C, N, O, F, Si, P, S, Cl, Br, I) and is explicitly trained to handle charged states. Validation against rigorous benchmarks, including complex torsional energy scans, molecular dynamics trajectories, batched minimizations, and forces and anergy accuracy demonstrates that AceFF establishes a new state-of-the-art for organic molecules. The AceFF-2 model weights and inference code are available at https://huggingface.co/Acellera/AceFF-2.0.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [204] [MethConvTransformer: A Deep Learning Framework for Cross-Tissue Alzheimer's Disease Detection](https://arxiv.org/abs/2601.00143)
*Gang Qu,Guanghao Li,Zhongming Zhao*

Main category: q-bio.GN

TL;DR: 开发MethConvTransformer模型整合脑和外周组织DNA甲基化谱用于AD生物标志物发现，表现优于传统基线，有跨组织生物标志物和多分辨率可解释性。


<details>
  <summary>Details</summary>
Motivation: AD是多因素神经退行性疾病，DNA甲基化可作早期检测生物标志物，但甲基化特征在组织和研究中差异大，限制可重复性和转化应用。

Method: 开发基于Transformer的深度学习框架MethConvTransformer，结合CpG线性投影、卷积和自注意力层，纳入受试者协变量和组织嵌入。

Result: 在六个GEO数据集和独立ADNI验证队列中，模型始终优于传统机器学习基线，实现更好的辨别和泛化能力，可解释性分析揭示与AD相关通路的有意义甲基化模式。

Conclusion: MethConvTransformer提供了可靠的跨组织AD表观遗传生物标志物，具有多分辨率可解释性，推动了基于甲基化的诊断并为疾病机制提供可验证假设。

Abstract: Alzheimer's disease (AD) is a multifactorial neurodegenerative disorder characterized by progressive cognitive decline and widespread epigenetic dysregulation in the brain. DNA methylation, as a stable yet dynamic epigenetic modification, holds promise as a noninvasive biomarker for early AD detection. However, methylation signatures vary substantially across tissues and studies, limiting reproducibility and translational utility. To address these challenges, we develop MethConvTransformer, a transformer-based deep learning framework that integrates DNA methylation profiles from both brain and peripheral tissues to enable biomarker discovery. The model couples a CpG-wise linear projection with convolutional and self-attention layers to capture local and long-range dependencies among CpG sites, while incorporating subject-level covariates and tissue embeddings to disentangle shared and region-specific methylation effects. In experiments across six GEO datasets and an independent ADNI validation cohort, our model consistently outperforms conventional machine-learning baselines, achieving superior discrimination and generalization. Moreover, interpretability analyses using linear projection, SHAP, and Grad-CAM++ reveal biologically meaningful methylation patterns aligned with AD-associated pathways, including immune receptor signaling, glycosylation, lipid metabolism, and endomembrane (ER/Golgi) organization. Together, these results indicate that MethConvTransformer delivers robust, cross-tissue epigenetic biomarkers for AD while providing multi-resolution interpretability, thereby advancing reproducible methylation-based diagnostics and offering testable hypotheses on disease mechanisms.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [205] [MAESTRO: Multi-Agent Evaluation Suite for Testing, Reliability, and Observability](https://arxiv.org/abs/2601.00481)
*Tie Ma,Yixi Chen,Vaastav Anand,Alessandro Cornacchia,Amândio R. Faustino,Guanheng Liu,Shan Zhang,Hongbin Luo,Suhaib A. Fahmy,Zafar A. Qazi,Marco Canini*

Main category: cs.NI

TL;DR: 提出用于LLM - 基MAS测试、可靠性和可观测性的评估套件MAESTRO，通过实验揭示系统特性并提供设计优化指导。


<details>
  <summary>Details</summary>
Motivation: 解决LLM - 基MAS测试、可靠性和可观测性评估问题，推动代理系统设计优化。

Method: 通过统一接口标准化配置和执行，利用示例库和适配器集成系统，导出执行跟踪和系统级信号；对12个代表性MAS进行多模式控制实验。

Result: MAS执行结构稳定但时间可变，性能和可靠性因运行差异大；MAS架构是资源、可重复性、性能权衡的主要驱动因素。

Conclusion: MAESTRO能进行系统评估，为代理系统设计和优化提供经验指导。

Abstract: We present MAESTRO, an evaluation suite for the testing, reliability, and observability of LLM-based MAS. MAESTRO standardizes MAS configuration and execution through a unified interface, supports integrating both native and third-party MAS via a repository of examples and lightweight adapters, and exports framework-agnostic execution traces together with system-level signals (e.g., latency, cost, and failures). We instantiate MAESTRO with 12 representative MAS spanning popular agentic frameworks and interaction patterns, and conduct controlled experiments across repeated runs, backend models, and tool configurations. Our case studies show that MAS executions can be structurally stable yet temporally variable, leading to substantial run-to-run variance in performance and reliability. We further find that MAS architecture is the dominant driver of resource profiles, reproducibility, and cost-latency-accuracy trade-off, often outweighing changes in backend models or tool settings. Overall, MAESTRO enables systematic evaluation and provides empirical guidance for designing and optimizing agentic systems.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [206] [Neural Minimum Weight Perfect Matching for Quantum Error Codes](https://arxiv.org/abs/2601.00242)
*Yotam Peled,David Zenati,Eliya Nachmani*

Main category: quant-ph

TL;DR: 提出数据驱动解码器NMWPM，结合GNN和Transformers预测动态边权重，用代理损失函数训练，降低逻辑错误率。


<details>
  <summary>Details</summary>
Motivation: 实现量子计算全部潜力需量子纠错（QEC），传统MWPM解码器有局限，需更优解决方案。

Method: 提出NMWPM解码器，采用混合架构，结合GNN提取局部特征、Transformers捕捉全局依赖，预测动态边权重，用代理损失函数训练。

Result: 相比标准基线，显著降低逻辑错误率（LER）。

Conclusion: 结合神经网络预测能力和经典匹配算法结构的混合解码器有优势。

Abstract: Realizing the full potential of quantum computation requires Quantum Error Correction (QEC). QEC reduces error rates by encoding logical information across redundant physical qubits, enabling errors to be detected and corrected. A common decoder used for this task is Minimum Weight Perfect Matching (MWPM) a graph-based algorithm that relies on edge weights to identify the most likely error chains. In this work, we propose a data-driven decoder named Neural Minimum Weight Perfect Matching (NMWPM). Our decoder utilizes a hybrid architecture that integrates Graph Neural Networks (GNNs) to extract local syndrome features and Transformers to capture long-range global dependencies, which are then used to predict dynamic edge weights for the MWPM decoder. To facilitate training through the non-differentiable MWPM algorithm, we formulate a novel proxy loss function that enables end-to-end optimization. Our findings demonstrate significant performance reduction in the Logical Error Rate (LER) over standard baselines, highlighting the advantage of hybrid decoders that combine the predictive capabilities of neural networks with the algorithmic structure of classical matching.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [207] [Cuffless, calibration-free hemodynamic monitoring with physics-informed machine learning models](https://arxiv.org/abs/2601.00081)
*Henry Crandall,Tyler Schuessler,Filip Bělík,Albert Fabregas,Barry M. Stults,Alexandra Boyadzhiev,Huanan Zhang,Jim S. Wu,Aylin R. Rodan,Stephen P. Juraschek,Ramakrishna Mukkamala,Alfred K. Cheung,Stavros G. Drakos,Christel Hohenegger,Braxton Osting,Benjamin Sanchez*

Main category: physics.med-ph

TL;DR: 开发带实时生物电阻抗（BioZ）传感的智能手表用于无袖带血液动力学监测，解决现有无袖带技术的局限。


<details>
  <summary>Details</summary>
Motivation: 现有无袖带血压监测设备方法缺乏理论基础，易受干扰，准确性和临床实用性不足。

Method: 通过多尺度分析和计算建模框架阐明BioZ与血压的生物物理关系，识别影响手腕搏动BioZ信号的参数，使用结合流体动力学原理的信号标记物理信息神经网络进行无校准血压和血流速度估计。

Result: 在健康人静息和运动后、高血压和心血管疾病患者门诊及重症监护环境中测试成功。

Conclusion: BioZ技术用于无袖带血压和血流速度监测是可行的，解决了现有无袖带技术的关键局限。

Abstract: Wearable technologies have the potential to transform ambulatory and at-home hemodynamic monitoring by providing continuous assessments of cardiovascular health metrics and guiding clinical management. However, existing cuffless wearable devices for blood pressure (BP) monitoring often rely on methods lacking theoretical foundations, such as pulse wave analysis or pulse arrival time, making them vulnerable to physiological and experimental confounders that undermine their accuracy and clinical utility. Here, we developed a smartwatch device with real-time electrical bioimpedance (BioZ) sensing for cuffless hemodynamic monitoring. We elucidate the biophysical relationship between BioZ and BP via a multiscale analytical and computational modeling framework, and identify physiological, anatomical, and experimental parameters that influence the pulsatile BioZ signal at the wrist. A signal-tagged physics-informed neural network incorporating fluid dynamics principles enables calibration-free estimation of BP and radial and axial blood velocity. We successfully tested our approach with healthy individuals at rest and after physical activity including physical and autonomic challenges, and with patients with hypertension and cardiovascular disease in outpatient and intensive care settings. Our findings demonstrate the feasibility of BioZ technology for cuffless BP and blood velocity monitoring, addressing critical limitations of existing cuffless technologies.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [208] [Vehicle Painting Robot Path Planning Using Hierarchical Optimization](https://arxiv.org/abs/2601.00271)
*Yuya Nagai,Hiromitsu Nakamura,Narito Shinmachi,Yuta Higashizono,Satoshi Ono*

Main category: cs.RO

TL;DR: 本文将汽车涂装机械臂路径设计问题转化为分层优化问题，通过实验证明该方法能自动设计出满足约束且质量与人工相当的路径。


<details>
  <summary>Details</summary>
Motivation: 汽车涂装机械臂路径设计目前是耗时的人工任务，且传统路径规划技术无法直接应用，因此需要自动化并减少设计时间。

Method: 将路径设计表述为分层优化问题，上层子问题类似车辆路径问题，下层子问题是详细路径规划，每层使用不同优化算法，并通过设计变量表示、约束、修复算子和初始化过程灵活处理涂装特定约束。

Result: 对三种市售车型的实验表明，该方法能自动设计出满足所有涂装约束且质量与工程师手动设计相当的路径。

Conclusion: 提出的分层优化方法可有效解决汽车涂装机械臂路径设计问题，实现自动化并保证路径质量。

Abstract: In vehicle production factories, the vehicle painting process employs multiple robotic arms to simultaneously apply paint to car bodies advancing along a conveyor line. Designing paint paths for these robotic arms, which involves assigning car body areas to arms and determining paint sequences for each arm, remains a time-consuming manual task for engineers, indicating the demand for automation and design time reduction. The unique constraints of the painting process hinder the direct application of conventional robotic path planning techniques, such as those used in welding. Therefore, this paper formulates the design of paint paths as a hierarchical optimization problem, where the upper-layer subproblem resembles a vehicle routing problem (VRP), and the lower-layer subproblem involves detailed path planning. This approach allows the use of different optimization algorithms at each layer, and permits flexible handling of constraints specific to the vehicle painting process through the design of variable representation, constraints, repair operators, and an initialization process at the upper and lower layers. Experiments with three commercially available vehicle models demonstrated that the proposed method can automatically design paths that satisfy all constraints for vehicle painting with quality comparable to those created manually by engineers.

</details>


### [209] [Reinforcement learning with timed constraints for robotics motion planning](https://arxiv.org/abs/2601.00087)
*Zhaoan Wang,Junchao Li,Mahdi Mohammad,Shaoping Xiao*

Main category: cs.RO

TL;DR: 本文提出基于自动机的强化学习框架，用于在MITL规范下在MDP和POMDP中合成策略，通过模拟研究验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 动态不确定环境下机器人系统需满足复杂任务序列和严格时间约束的规划器，但将MITL与强化学习集成存在挑战。

Method: 将MITL公式转化为Timed - LDGBA并与决策过程同步构建适合Q学习的产品定时模型，设计奖励结构。

Result: 在三个模拟研究中，框架能学习满足严格时间要求的策略，可扩展到更大状态空间，在部分可观测环境中有效。

Conclusion: 该框架在时间关键和不确定场景的机器人规划中有可靠应用潜力。

Abstract: Robotic systems operating in dynamic and uncertain environments increasingly require planners that satisfy complex task sequences while adhering to strict temporal constraints. Metric Interval Temporal Logic (MITL) offers a formal and expressive framework for specifying such time-bounded requirements; however, integrating MITL with reinforcement learning (RL) remains challenging due to stochastic dynamics and partial observability. This paper presents a unified automata-based RL framework for synthesizing policies in both Markov Decision Processes (MDPs) and Partially Observable Markov Decision Processes (POMDPs) under MITL specifications. MITL formulas are translated into Timed Limit-Deterministic Generalized Büchi Automata (Timed-LDGBA) and synchronized with the underlying decision process to construct product timed models suitable for Q-learning. A simple yet expressive reward structure enforces temporal correctness while allowing additional performance objectives. The approach is validated in three simulation studies: a $5 \times 5$ grid-world formulated as an MDP, a $10 \times 10$ grid-world formulated as a POMDP, and an office-like service-robot scenario. Results demonstrate that the proposed framework consistently learns policies that satisfy strict time-bounded requirements under stochastic transitions, scales to larger state spaces, and remains effective in partially observable environments, highlighting its potential for reliable robotic planning in time-critical and uncertain settings.

</details>


### [210] [Priority-Aware Multi-Robot Coverage Path Planning](https://arxiv.org/abs/2601.00580)
*Kanghoon Lee,Hyeonjun Kim,Jiachen Li,Jinkyoo Park*

Main category: cs.RO

TL;DR: 提出PA - MCPP问题并设计两阶段框架解决，实验显示该方法能降低加权延迟并保持有竞争力的完工时间。


<details>
  <summary>Details</summary>
Motivation: 现有多机器人覆盖路径规划方法假定区域重要性均匀，在部分区域需更快处理的场景中效果有限。

Method: 提出可扩展的两阶段框架：结合贪婪区域分配与局部搜索、基于生成树的路径规划，以及斯坦纳树引导的残余覆盖。

Result: 实验表明能显著降低与标准MCPP基线相比的优先加权延迟，同时保持有竞争力的完工时间；敏感性分析显示方法对机器人数可扩展性好，通过调整优先级权重可有效控制区域覆盖行为。

Conclusion: 所提方法在解决优先感知的多机器人覆盖路径规划问题上有效且具扩展性。

Abstract: Multi-robot systems are widely used for coverage tasks that require efficient coordination across large environments. In Multi-Robot Coverage Path Planning (MCPP), the objective is typically to minimize the makespan by generating non-overlapping paths for full-area coverage. However, most existing methods assume uniform importance across regions, limiting their effectiveness in scenarios where some zones require faster attention. We introduce the Priority-Aware MCPP (PA-MCPP) problem, where a subset of the environment is designated as prioritized zones with associated weights. The goal is to minimize, in lexicographic order, the total priority-weighted latency of zone coverage and the overall makespan. To address this, we propose a scalable two-phase framework combining (1) greedy zone assignment with local search, spanning-tree-based path planning, and (2) Steiner-tree-guided residual coverage. Experiments across diverse scenarios demonstrate that our method significantly reduces priority-weighted latency compared to standard MCPP baselines, while maintaining competitive makespan. Sensitivity analyses further show that the method scales well with the number of robots and that zone coverage behavior can be effectively controlled by adjusting priority weights.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [211] [CoCo-Fed: A Unified Framework for Memory- and Communication-Efficient Federated Learning at the Wireless Edge](https://arxiv.org/abs/2601.00549)
*Zhiheng Guo,Zhaoyang Liu,Zihan Cen,Chenyuan Feng,Xinghua Sun,Xiang Chen,Tony Q. S. Quek,Xijun Wang*

Main category: cs.IT

TL;DR: 本文提出CoCo-Fed框架解决O-RAN架构中神经网络部署的内存和通信瓶颈，经理论和实验验证其在内存和通信效率上的优势。


<details>
  <summary>Details</summary>
Motivation: O-RAN架构中部署大型神经网络面临本地训练内存占用高和全局聚合时回传链路带宽饱和的问题。

Method: 提出基于压缩和组合的联邦学习框架CoCo-Fed，本地进行梯度双维度下投影，优化器在低秩结构上运行；全局引入基于正交子空间叠加的传输协议。

Result: 建立了理论基础，证明了在无监督学习条件下收敛；在到达角估计任务的仿真中，CoCo-Fed在内存和通信效率上显著优于现有基线，在非IID设置下收敛性好。

Conclusion: CoCo-Fed在解决O-RAN架构神经网络部署瓶颈问题上有显著效果，能提高内存和通信效率，且收敛性好。

Abstract: The deployment of large-scale neural networks within the Open Radio Access Network (O-RAN) architecture is pivotal for enabling native edge intelligence. However, this paradigm faces two critical bottlenecks: the prohibitive memory footprint required for local training on resource-constrained gNBs, and the saturation of bandwidth-limited backhaul links during the global aggregation of high-dimensional model updates. To address these challenges, we propose CoCo-Fed, a novel Compression and Combination-based Federated learning framework that unifies local memory efficiency and global communication reduction. Locally, CoCo-Fed breaks the memory wall by performing a double-dimension down-projection of gradients, adapting the optimizer to operate on low-rank structures without introducing additional inference parameters/latency. Globally, we introduce a transmission protocol based on orthogonal subspace superposition, where layer-wise updates are projected and superimposed into a single consolidated matrix per gNB, drastically reducing the backhaul traffic. Beyond empirical designs, we establish a rigorous theoretical foundation, proving the convergence of CoCo-Fed even under unsupervised learning conditions suitable for wireless sensing tasks. Extensive simulations on an angle-of-arrival estimation task demonstrate that CoCo-Fed significantly outperforms state-of-the-art baselines in both memory and communication efficiency while maintaining robust convergence under non-IID settings.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [212] [Benchmarking Preprocessing and Integration Methods in Single-Cell Genomics](https://arxiv.org/abs/2601.00277)
*Ali Anaissi,Seid Miad Zandavi,Weidong Huang,Junaid Akram,Basem Suleiman,Ali Braytee,Jie Hua*

Main category: q-bio.QM

TL;DR: 研究单细胞数据分析通用流程，评估不同算法组合在多数据集上的表现，发现Seurat和Harmony在数据整合中表现出色，UMAP是最适配的降维方法，归一化方法选择因整合方法而异。


<details>
  <summary>Details</summary>
Motivation: 单细胞多模态测量存在跨模态数据整合挑战，且缺乏对不同预处理策略下技术的系统评估。

Method: 研究单细胞数据分析通用流程，用三个指标评估六个不同数据集，实验涉及七种归一化方法、四种降维方法和五种整合方法的组合。

Result: Seurat和Harmony在数据整合中表现出色，Harmony对大数据集更省时，UMAP是与整合技术最兼容的降维方法，归一化方法选择因整合方法而异。

Conclusion: 明确了在单细胞数据分析中不同算法组合的表现，为选择合适的分析方法提供参考。

Abstract: Single-cell data analysis has the potential to revolutionize personalized medicine by characterizing disease-associated molecular changes at the single-cell level. Advanced single-cell multimodal assays can now simultaneously measure various molecules (e.g., DNA, RNA, Protein) across hundreds of thousands of individual cells, providing a comprehensive molecular readout. A significant analytical challenge is integrating single-cell measurements across different modalities. Various methods have been developed to address this challenge, but there has been no systematic evaluation of these techniques with different preprocessing strategies. This study examines a general pipeline for single-cell data analysis, which includes normalization, data integration, and dimensionality reduction. The performance of different algorithm combinations often depends on the dataset sizes and characteristics. We evaluate six datasets across diverse modalities, tissues, and organisms using three metrics: Silhouette Coefficient Score, Adjusted Rand Index, and Calinski-Harabasz Index. Our experiments involve combinations of seven normalization methods, four dimensional reduction methods, and five integration methods. The results show that Seurat and Harmony excel in data integration, with Harmony being more time-efficient, especially for large datasets. UMAP is the most compatible dimensionality reduction method with the integration techniques, and the choice of normalization method varies depending on the integration method used.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [213] [Impact of Clustering on the Observability and Controllability of Complex Networks](https://arxiv.org/abs/2601.00221)
*Mohammadreza Doostmohammadian,Hamid R. Rabiee*

Main category: eess.SY

TL;DR: 本文研究聚类对复杂无标度网络可观测性和可控性的影响，发现密集聚类网络所需驱动和观测节点更少，研究结果有助于优化网络设计。


<details>
  <summary>Details</summary>
Motivation: 随着系统复杂性和互联性增加，研究复杂网络尤其是无标度网络的可观测性和可控性，以优化网络设计。

Method: 量化网络可观测性/可控性要求，通过蒙特卡罗模拟和不同案例研究聚类对这些指标的影响。

Result: 密集聚类网络因簇内信息传播更好，所需驱动和观测节点更少。

Conclusion: 研究为传感器/执行器放置减少控制和观测节点提供实用见解，有助于理解网络可观测性/可控性并提出改进方法。

Abstract: The increasing complexity and interconnectedness of systems across various fields have led to a growing interest in studying complex networks, particularly Scale-Free (SF) networks, which best model real-world systems. This paper investigates the influence of clustering on the observability and controllability of complex SF networks, framing these characteristics in the context of structured systems theory. In this paper, we show that densely clustered networks require fewer driver and observer nodes due to better information propagation within clusters. This relationship is of interest for optimizing network design in applications such as social networks and intelligent transportation systems. We first quantify the network observability/controllability requirements, and then, through Monte-Carlo simulations and different case studies, we show how clustering affects these metrics. Our findings offer practical insights into reducing control and observer nodes for sensor/actuator placement, particularly in resource-constrained setups. This work contributes to the understanding of network observability/controllability and presents techniques for improving these features through alterations in network structure and clustering.

</details>


### [214] [Next Generation Intelligent Low-Altitude Economy Deployments: The O-RAN Perspective](https://arxiv.org/abs/2601.00257)
*Aly Sabri Abdalla,Vuk Marojevic*

Main category: eess.SY

TL;DR: 本文提出基于O - RAN的低空经济框架，评估架构可行性与性能，调研无人机测试平台能力，指出研究挑战与标准化需求。


<details>
  <summary>Details</summary>
Motivation: 现有低空经济应用在复杂信号受限环境任务编排存在挑战，缺乏实时、有弹性、上下文感知的编排及适用于低空经济任务的AI集成。

Method: 引入O - RAN的低空经济框架，通过语义感知rApp提供语义指导，强化学习xApp进行实时轨迹规划。

Result: 通过评估验证了所提架构的可行性与性能。

Conclusion: 指出低空经济领域存在的关键研究挑战与标准化需求。

Abstract: Despite the growing interest in low-altitude economy (LAE) applications, including UAV-based logistics and emergency response, fundamental challenges remain in orchestrating such missions over complex, signal-constrained environments. These include the absence of real-time, resilient, and context-aware orchestration of aerial nodes with limited integration of artificial intelligence (AI) specialized for LAE missions. This paper introduces an open radio access network (O-RAN)-enabled LAE framework that leverages seamless coordination between the disaggregated RAN architecture, open interfaces, and RAN intelligent controllers (RICs) to facilitate closed-loop, AI-optimized, and mission-critical LAE operations. We evaluate the feasibility and performance of the proposed architecture via a semantic-aware rApp that acts as a terrain interpreter, offering semantic guidance to a reinforcement learning-enabled xApp, which performs real-time trajectory planning for LAE swarm nodes. We survey the capabilities of UAV testbeds that can be leveraged for LAE research, and present critical research challenges and standardization needs.

</details>


### [215] [Probability-Aware Parking Selection](https://arxiv.org/abs/2601.00521)
*Cameron Hickert,Sirui Li,Zhengbing He,Cathy Wu*

Main category: eess.SY

TL;DR: 现有停车导航系统常低估总出行时间，本文提出概率感知停车选择问题，用动态规划框架决策，通过分析和实验验证方法可行且能节省时间。


<details>
  <summary>Details</summary>
Motivation: 当前停车导航系统未考虑找车位时间，影响用户体验、出行方式选择、拥堵和排放，需改进。

Method: 提出概率感知停车选择问题，用自适应动态规划框架决策，进行封闭形式分析、敏感性分析和案例研究，评估利用随机观测估计停车可用性的误差。

Result: 模型能考虑停车可用性动态变化，实验表明该方法可行，平均绝对误差随观测频率增加降低，概率感知策略比无概率策略节省时间。

Conclusion: 概率感知停车选择问题及相关方法具有可行性和有效性，能一定程度节省出行时间。

Abstract: Current parking navigation systems often underestimate total travel time by failing to account for the time spent searching for a parking space, which significantly affects user experience, mode choice, congestion, and emissions. To address this issue, this paper introduces the probability-aware parking selection problem, which aims to direct drivers to the best parking location rather than straight to their destination. An adaptable dynamic programming framework is proposed for decision-making based on probabilistic information about parking availability at the parking lot level. Closed-form analysis determines when it is optimal to target a specific parking lot or explore alternatives, as well as the expected time cost. Sensitivity analysis and three illustrative cases are examined, demonstrating the model's ability to account for the dynamic nature of parking availability. Acknowledging the financial costs of permanent sensing infrastructure, the paper provides analytical and empirical assessments of errors incurred when leveraging stochastic observations to estimate parking availability. Experiments with real-world data from the US city of Seattle indicate this approach's viability, with mean absolute error decreasing from 7% to below 2% as observation frequency grows. In data-based simulations, probability-aware strategies demonstrate time savings up to 66% relative to probability-unaware baselines, yet still take up to 123% longer than direct-to-destination estimates.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [216] [Sparse FEONet: A Low-Cost, Memory-Efficient Operator Network via Finite-Element Local Sparsity for Parametric PDEs](https://arxiv.org/abs/2601.00672)
*Seungchan Ko,Jiyeon Kim,Dongwook Shin*

Main category: math.NA

TL;DR: 本文研究有限元算子网络（FEONet），针对其大规模问题挑战提出新的稀疏网络架构，实验表明该架构提升计算效率且保持精度，还有理论结果支撑。


<details>
  <summary>Details</summary>
Motivation: FEONet在处理大规模问题时计算成本增加且精度可能下降，需提出新方法解决。

Method: 受有限元结构启发，提出新的稀疏网络架构。

Result: 通过大量数值实验，新的稀疏网络在保持精度的同时大幅提升计算成本和效率；建立理论结果证明稀疏架构能有效逼近目标算子，并进行稳定性分析。

Conclusion: 新提出的稀疏网络架构能解决FEONet在大规模问题中的挑战，在计算效率和精度上表现良好，且有理论保障。

Abstract: In this paper, we study the finite element operator network (FEONet), an operator-learning method for parametric problems, originally introduced in J. Y. Lee, S. Ko, and Y. Hong, Finite Element Operator Network for Solving Elliptic-Type Parametric PDEs, SIAM J. Sci. Comput., 47(2), C501-C528, 2025. FEONet realizes the parameter-to-solution map on a finite element space and admits a training procedure that does not require training data, while exhibiting high accuracy and robustness across a broad class of problems. However, its computational cost increases and accuracy may deteriorate as the number of elements grows, posing notable challenges for large-scale problems. In this paper, we propose a new sparse network architecture motivated by the structure of the finite elements to address this issue. Throughout extensive numerical experiments, we show that the proposed sparse network achieves substantial improvements in computational cost and efficiency while maintaining comparable accuracy. We also establish theoretical results demonstrating that the sparse architecture can approximate the target operator effectively and provide a stability analysis ensuring reliable training and prediction.

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [217] [Automated electrostatic characterization of quantum dot devices in single- and bilayer heterostructures](https://arxiv.org/abs/2601.00067)
*Merritt P. R. Losert,Dario Denora,Barnaby van Straaten,Michael Chan,Stefan D. Oosterhout,Lucas Stehouwer,Giordano Scappucci,Menno Veldhorst,Justyna P. Zwolak*

Main category: cond-mat.mes-hall

TL;DR: 随着量子点自旋量子比特向更复杂架构发展，亟需自动设备表征和数据分析工具，本文提出从电荷稳定性图提取电容特性的自动协议，并利用实验数据验证，能快速提取量子点设备有用信息。


<details>
  <summary>Details</summary>
Motivation: 量子点自旋量子比特向更复杂设备架构发展，手动分析电荷稳定性图特征耗时、易出错且难以规模化，因此需要自动化方法。

Method: 集成机器学习、图像处理和目标检测，在无手动标记的情况下识别和追踪大数据集中的电荷跃迁。

Result: 通过实验测量的应变锗单量子阱和双量子阱量子点设备的数据验证了方法，能统计估计相对杠杆臂和电容耦合等物理量。

Conclusion: 所提出的协议能快速提取量子点设备有用且重要的信息。

Abstract: As quantum dot (QD)-based spin qubits advance toward larger, more complex device architectures, rapid, automated device characterization and data analysis tools become critical. The orientation and spacing of transition lines in a charge stability diagram (CSD) contain a fingerprint of a QD device's capacitive environment, making these measurements useful tools for device characterization. However, manually interpreting these features is time-consuming, error-prone, and impractical at scale. Here, we present an automated protocol for extracting underlying capacitive properties from CSDs. Our method integrates machine learning, image processing, and object detection to identify and track charge transitions across large datasets without manual labeling. We demonstrate this method using experimentally measured data from a strained-germanium single-quantum-well (planar) and a strained-germanium double-quantum-well (bilayer) QD device. Unlike for planar QD devices, CSDs in bilayer germanium heterostructure exhibit a larger set of transitions, including interlayer tunneling and distinct loading lines for the vertically stacked QDs, making them a powerful testbed for automation methods. By analyzing the properties of many CSDs, we can statistically estimate physically relevant quantities, like relative lever arms and capacitive couplings. Thus, our protocol enables rapid extraction of useful, nontrivial information about QD devices.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [218] [The Generative AI Paradox: GenAI and the Erosion of Trust, the Corrosion of Information Verification, and the Demise of Truth](https://arxiv.org/abs/2601.00306)
*Emilio Ferrara*

Main category: cs.CY

TL;DR: 本文指出生成式AI带来合成现实，分析其危害、定性转变及风险实例，提出缓解措施和研究议程，最后提及生成式AI悖论。


<details>
  <summary>Details</summary>
Motivation: 公众对生成式AI危害认知局限于虚假信息等，本文旨在揭示其带来的更广泛社会技术转变及相关风险。

Method: 将合成现实形式化为分层堆栈，扩展危害分类，阐述定性转变，合成风险实例，提出缓解堆栈和研究议程。

Result: 形成合成现实分层堆栈、危害分类、定性转变机制说明及案例库，提出缓解措施和研究议程。

Conclusion: 随着合成媒体普及，社会可能完全理性地否定数字证据，即存在生成式AI悖论。

Abstract: Generative AI (GenAI) now produces text, images, audio, and video that can be perceptually convincing at scale and at negligible marginal cost. While public debate often frames the associated harms as "deepfakes" or incremental extensions of misinformation and fraud, this view misses a broader socio-technical shift: GenAI enables synthetic realities; coherent, interactive, and potentially personalized information environments in which content, identity, and social interaction are jointly manufactured and mutually reinforcing. We argue that the most consequential risk is not merely the production of isolated synthetic artifacts, but the progressive erosion of shared epistemic ground and institutional verification practices as synthetic content, synthetic identity, and synthetic interaction become easy to generate and hard to audit. This paper (i) formalizes synthetic reality as a layered stack (content, identity, interaction, institutions), (ii) expands a taxonomy of GenAI harms spanning personal, economic, informational, and socio-technical risks, (iii) articulates the qualitative shifts introduced by GenAI (cost collapse, throughput, customization, micro-segmentation, provenance gaps, and trust erosion), and (iv) synthesizes recent risk realizations (2023-2025) into a compact case bank illustrating how these mechanisms manifest in fraud, elections, harassment, documentation, and supply-chain compromise. We then propose a mitigation stack that treats provenance infrastructure, platform governance, institutional workflow redesign, and public resilience as complementary rather than substitutable, and outline a research agenda focused on measuring epistemic security. We conclude with the Generative AI Paradox: as synthetic media becomes ubiquitous, societies may rationally discount digital evidence altogether.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [219] [Latent Flow Matching for Expressive Singing Voice Synthesis](https://arxiv.org/abs/2601.00217)
*Minhyeok Yun,Yong-Hoon Choi*

Main category: cs.SD

TL;DR: 传统cVAE歌声合成存在先验 - 后验不匹配问题，FM - Singer引入条件流匹配改进，实验显示性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决基于cVAE的歌声合成中先验 - 后验不匹配导致的精细表现力下降问题。

Method: 在潜在空间引入条件流匹配（CFM）学习连续向量场，推理时通过解常微分方程（ODE）细化先验样本。

Result: 在韩语和中文歌唱数据集上相比基线有一致提升，如更低的梅尔倒谱失真、基频误差和更高感知分数。

Conclusion: FM - Singer能在保持并行解码效率的同时提高歌声合成的表现力。

Abstract: Conditional variational autoencoder (cVAE)-based singing voice synthesis provides efficient inference and strong audio quality by learning a score-conditioned prior and a recording-conditioned posterior latent space. However, because synthesis relies on prior samples while training uses posterior latents inferred from real recordings, imperfect distribution matching can cause a prior-posterior mismatch that degrades fine-grained expressiveness such as vibrato and micro-prosody. We propose FM-Singer, which introduces conditional flow matching (CFM) in latent space to learn a continuous vector field transporting prior latents toward posterior latents along an optimal-transport-inspired path. At inference time, the learned latent flow refines a prior sample by solving an ordinary differential equation (ODE) before waveform generation, improving expressiveness while preserving the efficiency of parallel decoding. Experiments on Korean and Chinese singing datasets demonstrate consistent improvements over strong baselines, including lower mel-cepstral distortion and fundamental-frequency error and higher perceptual scores on the Korean dataset. Code, pretrained checkpoints, and audio demos are available at https://github.com/alsgur9368/FM-Singer

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [220] [Group Cross-Correlations with Faintly Constrained Filters](https://arxiv.org/abs/2601.00045)
*Benedikt Fluhr*

Main category: math.DS

TL;DR: 提出群互相关概念，解决先前约束问题，推广结果并弱化常见假设。


<details>
  <summary>Details</summary>
Motivation: 解决先前文献中群作用约束对于非紧稳定子群的不兼容性问题。

Method: 提出关联滤波器约束更宽松的群互相关概念。

Result: 解决了先前约束的不兼容性问题，将结果推广到非传递群作用，并弱化了幺模性假设。

Conclusion: 新的群互相关概念具有有效性和更广泛的适用性。

Abstract: We provide a notion of group cross-correlations, where the associated filter is not as tightly constrained as in the previous literature. This resolves an incompatibility previous constraints have for group actions with non-compact stabilizers. Moreover, we generalize previous results to group actions that are not necessarily transitive, and we weaken the common assumption of unimodularity.

</details>
