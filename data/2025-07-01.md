<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 45]
- [cs.CE](#cs.CE) [Total: 7]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.DS](#cs.DS) [Total: 16]
- [cs.GT](#cs.GT) [Total: 5]
- [cs.IR](#cs.IR) [Total: 12]
- [cs.LG](#cs.LG) [Total: 112]
- [cs.NE](#cs.NE) [Total: 4]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.SE](#cs.SE) [Total: 23]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [q-fin.PM](#q-fin.PM) [Total: 3]
- [q-fin.RM](#q-fin.RM) [Total: 2]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [stat.ML](#stat.ML) [Total: 8]
- [nlin.CD](#nlin.CD) [Total: 1]
- [econ.GN](#econ.GN) [Total: 5]
- [cs.IT](#cs.IT) [Total: 1]
- [eess.SY](#eess.SY) [Total: 2]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [cs.CR](#cs.CR) [Total: 23]
- [cs.DM](#cs.DM) [Total: 2]
- [cs.PL](#cs.PL) [Total: 1]
- [eess.IV](#eess.IV) [Total: 11]
- [eess.SP](#eess.SP) [Total: 16]
- [cs.SD](#cs.SD) [Total: 7]
- [cs.SI](#cs.SI) [Total: 1]
- [econ.EM](#econ.EM) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [cs.GR](#cs.GR) [Total: 2]
- [physics.optics](#physics.optics) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [astro-ph.GA](#astro-ph.GA) [Total: 1]
- [math.OC](#math.OC) [Total: 8]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.CY](#cs.CY) [Total: 7]
- [cs.RO](#cs.RO) [Total: 12]
- [math.ST](#math.ST) [Total: 3]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.HC](#cs.HC) [Total: 9]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [stat.AP](#stat.AP) [Total: 3]
- [stat.OT](#stat.OT) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [cs.NI](#cs.NI) [Total: 8]
- [stat.ME](#stat.ME) [Total: 2]
- [cs.CV](#cs.CV) [Total: 60]
- [cs.CL](#cs.CL) [Total: 45]
- [cs.ET](#cs.ET) [Total: 1]
- [math.NT](#math.NT) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Bootstrapping Human-Like Planning via LLMs](https://arxiv.org/abs/2506.22604)
*David Porfirio,Vincent Hsiao,Morgan Fine-Morris,Leslie Smith,Laura M. Hiatt*

Main category: cs.AI

TL;DR: 本文研究结合自然语言和拖放式编程，构建基于大语言模型的管道生成动作序列并与手动指定序列对比，发现大模型表现更好，小模型也有不错表现。


<details>
  <summary>Details</summary>
Motivation: 机器人终端用户需要更便捷指定任务的方式，自然语言和拖放式编程各有优势，因此研究两者结合的程度。

Method: 构建基于大语言模型（LLM）的管道，以自然语言为输入生成人类级别的动作序列，并与手动指定的动作序列数据集进行对比。

Result: 较大模型在生成类人动作序列方面往往优于较小模型，但小模型也取得了令人满意的性能。

Conclusion: 结合自然语言和拖放式编程是可行的，不同规模的模型在相关任务中都有一定表现。

Abstract: Robot end users increasingly require accessible means of specifying tasks for
robots to perform. Two common end-user programming paradigms include
drag-and-drop interfaces and natural language programming. Although natural
language interfaces harness an intuitive form of human communication,
drag-and-drop interfaces enable users to meticulously and precisely dictate the
key actions of the robot's task. In this paper, we investigate the degree to
which both approaches can be combined. Specifically, we construct a large
language model (LLM)-based pipeline that accepts natural language as input and
produces human-like action sequences as output, specified at a level of
granularity that a human would produce. We then compare these generated action
sequences to another dataset of hand-specified action sequences. Although our
results reveal that larger models tend to outperform smaller ones in the
production of human-like action sequences, smaller models nonetheless achieve
satisfactory performance.

</details>


### [2] [Ludax: A GPU-Accelerated Domain Specific Language for Board Games](https://arxiv.org/abs/2506.22609)
*Graham Todd,Alexander G. Padula,Dennis J. N. J. Soemers,Julian Togelius*

Main category: cs.AI

TL;DR: 提出针对棋盘游戏的领域特定语言Ludax，结合游戏描述语言通用性与硬件加速，可融入深度学习管道，开源可用。


<details>
  <summary>Details</summary>
Motivation: 结合游戏描述语言和硬件加速进展，开发工具以加速游戏研究。

Method: 开发领域特定语言Ludax，自动编译为硬件加速代码，融入现有深度学习管道。

Result: 给出Ludax描述语言详细分析、编译过程技术说明、速度基准测试及训练强化学习代理演示。

Conclusion: Ludax可加速从强化学习到认知科学等游戏研究，且开源免费。

Abstract: Games have long been used as benchmarks and testing environments for research
in artificial intelligence. A key step in supporting this research was the
development of game description languages: frameworks that compile
domain-specific code into playable and simulatable game environments, allowing
researchers to generalize their algorithms and approaches across multiple games
without having to manually implement each one. More recently, progress in
reinforcement learning (RL) has been largely driven by advances in hardware
acceleration. Libraries like JAX allow practitioners to take full advantage of
cutting-edge computing hardware, often speeding up training and testing by
orders of magnitude. Here, we present a synthesis of these strands of research:
a domain-specific language for board games which automatically compiles into
hardware-accelerated code. Our framework, Ludax, combines the generality of
game description languages with the speed of modern parallel processing
hardware and is designed to fit neatly into existing deep learning pipelines.
We envision Ludax as a tool to help accelerate games research generally, from
RL to cognitive science, by enabling rapid simulation and providing a flexible
representation scheme. We present a detailed breakdown of Ludax's description
language and technical notes on the compilation process, along with speed
benchmarking and a demonstration of training RL agents. The Ludax framework,
along with implementations of existing board games, is open-source and freely
available.

</details>


### [3] [URSA: The Universal Research and Scientific Agent](https://arxiv.org/abs/2506.22653)
*Michael Grosskopf,Russell Bent,Rahul Somasundaram,Isaac Michaud,Arthur Lui,Nathan Debardeleben,Earl Lawrence*

Main category: cs.AI

TL;DR: 介绍大语言模型能力及潜力，提出科学代理生态系统URSA并展示其架构和应用潜力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型能力与人类科学家日常技能重叠，使用其在“代理型”AI中有潜力革新现代科学并消除研究瓶颈。

Method: 提出科学代理生态系统URSA，其由一组模块化代理和工具组成，可耦合先进物理模拟代码。

Result: 展示了URSA的架构和应用潜力。

Conclusion: URSA可用于加速研究任务，有解决不同复杂程度和影响的科学问题的潜力。

Abstract: Large language models (LLMs) have moved far beyond their initial form as
simple chatbots, now carrying out complex reasoning, planning, writing, coding,
and research tasks. These skills overlap significantly with those that human
scientists use day-to-day to solve complex problems that drive the cutting edge
of research. Using LLMs in "agentic" AI has the potential to revolutionize
modern science and remove bottlenecks to progress. In this work, we present
URSA, a scientific agent ecosystem for accelerating research tasks. URSA
consists of a set of modular agents and tools, including coupling to advanced
physics simulation codes, that can be combined to address scientific problems
of varied complexity and impact. This work highlights the architecture of URSA,
as well as examples that highlight the potential of the system.

</details>


### [4] [Explanations are a means to an end](https://arxiv.org/abs/2506.22740)
*Jessica Hullman,Ziyang Guo,Berk Ustun*

Main category: cs.AI

TL;DR: 本文提出可解释机器学习的解释应结合具体用途设计和评估，介绍基于统计决策理论的框架及应用。


<details>
  <summary>Details</summary>
Motivation: 现有可解释机器学习方法未充分考虑解释在实际中的应用，提出应结合具体用途设计和评估解释。

Method: 基于统计决策理论构建框架，将功能性方法应用于不同用例。

Result: 展示该方法可用于刻画特定任务中解释能为理想决策者带来的最大性能提升，防止因歧义导致的误用。

Conclusion: 评估应融合理论和实证视角，给出跨越这些视角的定义。

Abstract: Modern methods for explainable machine learning are designed to describe how
models map inputs to outputs--without deep consideration of how these
explanations will be used in practice. This paper argues that explanations
should be designed and evaluated with a specific end in mind. We describe how
to formalize this end in a framework based in statistical decision theory. We
show how this functionally-grounded approach can be applied across diverse use
cases, such as clinical decision support, providing recourse, or debugging. We
demonstrate its use to characterize the maximum "boost" in performance on a
particular task that an explanation could provide an idealized decision-maker,
preventing misuse due to ambiguity by forcing researchers to specify concrete
use cases that can be analyzed in light of models of expected explanation use.
We argue that evaluation should meld theoretical and empirical perspectives on
the value of explanation, and contribute definitions that span these
perspectives.

</details>


### [5] [Bridging Ethical Principles and Algorithmic Methods: An Alternative Approach for Assessing Trustworthiness in AI Systems](https://arxiv.org/abs/2506.22774)
*Michael Papademas,Xenia Ziouvelou,Antonis Troumpoukis,Vangelis Karkaletsis*

Main category: cs.AI

TL;DR: 本文指出AI有复杂挑战，现有评估方法有局限，引入结合伦理与算法的评估方法，可实现对AI系统可信度的整体评估。


<details>
  <summary>Details</summary>
Motivation: AI广泛应用影响深远，但现有理论和技术工具在评估可信AI方面有局限，需更好的评估方法。

Method: 结合可信AI的伦理成分与PageRank和TrustRank的算法过程，引入算法标准建立评估框架。

Result: 应用该方法可在考虑相关指南理论内容的同时提供定量见解，实现对AI系统可信度的整体评估。

Conclusion: 所提出的评估方法能减少该领域自我评估技术的主观性，有效评估AI系统的可信度。

Abstract: Artificial Intelligence (AI) technology epitomizes the complex challenges
posed by human-made artifacts, particularly those widely integrated into
society and exert significant influence, highlighting potential benefits and
their negative consequences. While other technologies may also pose substantial
risks, AI's pervasive reach makes its societal effects especially profound. The
complexity of AI systems, coupled with their remarkable capabilities, can lead
to a reliance on technologies that operate beyond direct human oversight or
understanding. To mitigate the risks that arise, several theoretical tools and
guidelines have been developed, alongside efforts to create technological tools
aimed at safeguarding Trustworthy AI. The guidelines take a more holistic view
of the issue but fail to provide techniques for quantifying trustworthiness.
Conversely, while technological tools are better at achieving such
quantification, they lack a holistic perspective, focusing instead on specific
aspects of Trustworthy AI. This paper aims to introduce an assessment method
that combines the ethical components of Trustworthy AI with the algorithmic
processes of PageRank and TrustRank. The goal is to establish an assessment
framework that minimizes the subjectivity inherent in the self-assessment
techniques prevalent in the field by introducing algorithmic criteria. The
application of our approach indicates that a holistic assessment of an AI
system's trustworthiness can be achieved by providing quantitative insights
while considering the theoretical content of relevant guidelines.

</details>


### [6] [ReasonBridge: Efficient Reasoning Transfer from Closed to Open-Source Language Models](https://arxiv.org/abs/2506.22865)
*Ziqi Zhong,Xunzhu Tang*

Main category: cs.AI

TL;DR: 本文介绍ReasonBridge方法，通过分层知识蒸馏框架将推理能力从闭源模型迁移到开源模型，在基准任务上提升开源模型推理能力，缩小与闭源模型差距。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型中闭源和开源模型在复杂推理和指令遵循任务上存在显著性能差距，需提升开源模型推理能力。

Method: 提出ReasonBridge方法，开发含1000条推理轨迹的Reason1K数据集，采用分层蒸馏、稀疏推理适配器架构和测试时计算缩放机制。

Result: ReasonBridge使开源模型在基准任务上推理能力提升达23%，增强后的Qwen2.5 - 14B在MATH500上优于Claude - Sonnet3.5，在AIME问题上表现相当。

Conclusion: 该方法在不同推理领域和模型架构上有效泛化，是提升指令遵循推理能力的样本高效方法。

Abstract: Recent advancements in Large Language Models (LLMs) have revealed a
significant performance gap between closed-source and open-source models,
particularly in tasks requiring complex reasoning and precise instruction
following. This paper introduces ReasonBridge, a methodology that efficiently
transfers reasoning capabilities from powerful closed-source to open-source
models through a novel hierarchical knowledge distillation framework. We
develop a tailored dataset Reason1K with only 1,000 carefully curated reasoning
traces emphasizing difficulty, diversity, and quality. These traces are
filtered from across multiple domains using a structured multi-criteria
selection algorithm. Our transfer learning approach incorporates: (1) a
hierarchical distillation process capturing both strategic abstraction and
tactical implementation patterns, (2) a sparse reasoning-focused adapter
architecture requiring only 0.3% additional trainable parameters, and (3) a
test-time compute scaling mechanism using guided inference interventions.
Comprehensive evaluations demonstrate that ReasonBridge improves reasoning
capabilities in open-source models by up to 23% on benchmark tasks,
significantly narrowing the gap with closed-source models. Notably, the
enhanced Qwen2.5-14B outperforms Claude-Sonnet3.5 on MATH500 and matches its
performance on competition-level AIME problems. Our methodology generalizes
effectively across diverse reasoning domains and model architectures,
establishing a sample-efficient approach to reasoning enhancement for
instruction following.

</details>


### [7] [Agentic Enterprise: AI-Centric User to User-Centric AI](https://arxiv.org/abs/2506.22893)
*Arpit Narechania,Alex Endert,Atanu R Sinha*

Main category: cs.AI

TL;DR: 文章指出AI春天已至，探讨其在企业决策领域潜力，提出以用户为中心的AI六原则及市场机制。


<details>
  <summary>Details</summary>
Motivation: 研究AI在企业决策领域的潜力，解决当前AI - 中心用户范式不足。

Method: 考虑赋予AI的智能体，分析当前范式缺失，强调转向以用户为中心的AI。

Result: 提出企业中智能体成功的六个原则，推动平台市场机制。

Conclusion: 应将AI设计和智能体交付与企业用户需求对齐，采用以用户为中心的AI。

Abstract: After a very long winter, the Artificial Intelligence (AI) spring is here.
Or, so it seems over the last three years. AI has the potential to impact many
areas of human life - personal, social, health, education, professional. In
this paper, we take a closer look at the potential of AI for Enterprises, where
decision-making plays a crucial and repeated role across functions, tasks, and
operations. We consider Agents imbued with AI as means to increase
decision-productivity of enterprises. We highlight six tenets for Agentic
success in enterprises, by drawing attention to what the current, AI-Centric
User paradigm misses, in the face of persistent needs of and usefulness for
Enterprise Decision-Making. In underscoring a shift to User-Centric AI, we
offer six tenets and promote market mechanisms for platforms, aligning the
design of AI and its delivery by Agents to the cause of enterprise users.

</details>


### [8] [Hecto: Modular Sparse Experts for Adaptive and Interpretable Reasoning](https://arxiv.org/abs/2506.22919)
*Sanskar Pandey,Ruhaan Chopra,Saad Murtaza Bhat,Ark Abhyudaya*

Main category: cs.AI

TL;DR: 提出轻量级MoE架构Hecto，结合GRU和FFNN专家，在多个任务上表现良好，有专家专业化优势，在大批次下性能提升，架构多样性带来稳定性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型专家依赖相同归纳偏置，静态计算路径对不同推理类型输入效率低，限制专业化和可解释性。

Method: 提出Hecto架构，结合GRU专家用于时间推理和FFNN专家用于静态抽象，采用稀疏Top - 1门控机制。

Result: 在多个推理基准和回归任务上，Hecto性能与同质基线相当或接近，实现专家专业化，大批次下性能提升，消融实验表明架构多样性带来稳定性和可解释性。

Conclusion: Hecto为条件计算建立新基准，为低资源场景下专业推理提供框架，其优势源于专业专业化。

Abstract: Mixture-of-Experts (MoE) models enable conditional computation by routing
inputs to specialized experts, but these experts rely on identical inductive
biases, thus limiting representational diversity. This static computation
pathway is inefficient for inputs that require different types of reasoning and
limits specialization and interpretability. We propose Hecto, a lightweight MoE
architecture that leverages architectural heterogeneity by combining a GRU
expert for temporal reasoning and an FFNN expert for static abstraction under a
sparse Top-1 gating mechanism. Evaluated on three reasoning benchmarks (AG
News, SST-2, HotpotQA) and a regression task (STS-B), Hecto matches or closely
trails homogeneous baselines in performance despite receiving isolated input
representations, while achieving clear expert specialization, with each expert
aligning to distinct reasoning types (temporal vs static). At larger batch
sizes, Hecto exhibits improved performance, benefiting from relaxed
computational constraints that allow its heterogeneous architecture to optimize
more effectively. Ablation results isolate architectural diversity as the
source of Hecto's stability and interpretability across diverse reasoning
tasks. Overall, Hecto establishes itself as a new benchmark for conditional
computation, offering a principled framework for specialized reasoning in
low-resource regimes with its model strength derived from principled
specialization.

</details>


### [9] [Improving Rationality in the Reasoning Process of Language Models through Self-playing Game](https://arxiv.org/abs/2506.22920)
*Pinzheng Wang,Juntao Li,Zecheng Tang,Haijia Gui,Min zhang*

Main category: cs.AI

TL;DR: 本文探讨无监督下自博弈提升大语言模型推理过程合理性，设计CDG游戏，实验表明其能提升模型理解推理过程能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型缺乏对推理过程的真正理解，需提升其在推理过程中的合理性。

Method: 设计Critic - Discernment Game (CDG)，让证明者给出问题解决方案并接受批判，证明者需应对误导性和建设性反馈。

Result: 在数学推理、逐步错误检测、自我修正和长链推理等任务实验中，CDG训练显著提升了校准良好的大语言模型理解推理过程的能力。

Conclusion: CDG训练可有效增强大语言模型在推理过程中理解自身推理的能力。

Abstract: Large language models (LLMs) have demonstrated considerable reasoning
abilities in various tasks such as mathematics and coding. However, recent
studies indicate that even the best models lack true comprehension of their
reasoning processes. In this paper, we explore how self-play can enhance the
rationality of models in the reasoning process without supervision from humans
or superior models. We design a Critic-Discernment Game(CDG) in which a prover
first provides a solution to a given problem and is subsequently challenged by
critiques of its solution. These critiques either aim to assist or mislead the
prover. The objective of the prover is to maintain the correct answer when
faced with misleading comments, while correcting errors in response to
constructive feedback. Our experiments on tasks involving mathematical
reasoning, stepwise error detection, self-correction, and long-chain reasoning
demonstrate that CDG training can significantly improve the ability of
well-aligned LLMs to comprehend their reasoning process.

</details>


### [10] [MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning](https://arxiv.org/abs/2506.22992)
*Yulun Jiang,Yekun Chai,Maria Brbić,Michael Moor*

Main category: cs.AI

TL;DR: 提出多模态推理基准MARBLE，测试多模态语言模型多步推理能力，发现现有模型表现不佳，旨在推动下一代模型发展。


<details>
  <summary>Details</summary>
Motivation: 现有推理基准聚焦文本推理或简单多模态信息检索，复杂多模态推理研究不足，需新基准测试模型能力。

Method: 构建包含M - Portal和M - Cube两个任务的多模态推理基准MARBLE，测试现有多模态语言模型。

Result: 现有12个先进模型在M - Portal接近随机表现，M - Cube准确率为0%，简化子任务部分模型超随机基线，且感知是瓶颈。

Conclusion: 复杂推理对现有多模态语言模型仍是挑战，MARBLE可推动下一代模型发展。

Abstract: The ability to process information from multiple modalities and to reason
through it step-by-step remains a critical challenge in advancing artificial
intelligence. However, existing reasoning benchmarks focus on text-only
reasoning, or employ multimodal questions that can be answered by directly
retrieving information from a non-text modality. Thus, complex reasoning
remains poorly understood in multimodal domains. Here, we present MARBLE, a
challenging multimodal reasoning benchmark that is designed to scrutinize
multimodal language models (MLLMs) in their ability to carefully reason
step-by-step through complex multimodal problems and environments. MARBLE is
composed of two highly challenging tasks, M-Portal and M-Cube, that require the
crafting and understanding of multistep plans under spatial, visual, and
physical constraints. We find that current MLLMs perform poorly on MARBLE --
all the 12 advanced models obtain near-random performance on M-Portal and 0%
accuracy on M-Cube. Only in simplified subtasks some models outperform the
random baseline, indicating that complex reasoning is still a challenge for
existing MLLMs. Moreover, we show that perception remains a bottleneck, where
MLLMs occasionally fail to extract information from the visual inputs. By
shedding a light on the limitations of MLLMs, we hope that MARBLE will spur the
development of the next generation of models with the ability to reason and
plan across many, multimodal reasoning steps.

</details>


### [11] [AURA: Agent for Understanding, Reasoning, and Automated Tool Use in Voice-Driven Tasks](https://arxiv.org/abs/2506.23049)
*Leander Melroy Maben,Gayathri Ganesh Lakshmy,Srijith Radhakrishnan,Siddhant Arora,Shinji Watanabe*

Main category: cs.AI

TL;DR: 介绍首个开源语音原生助手AURA，能完成复杂任务，在多项评估中表现良好。


<details>
  <summary>Details</summary>
Motivation: 当前无开源系统能实现全语音到语音、多轮对话并集成工具使用和智能推理，因此开发AURA。

Method: 将开源权重的ASR、TTS和大语言模型结合在级联管道中，支持多种工具，模块化设计便于集成新工具。

Result: 在VoiceBench上，OpenBookQA得分92.75%，AlpacaEval得分为4.39；人工评估复杂多轮语音任务成功率达90%。

Conclusion: AURA是首个开源语音原生助手，在评估中表现出色，能有效完成复杂多轮语音任务。

Abstract: Despite advances in language and speech technologies, no open-source system
enables full speech-to-speech, multi-turn dialogue with integrated tool use and
agentic reasoning. We introduce AURA (Agent for Understanding, Reasoning, and
Automated Tool Use), the first open-source, speech-native assistant capable of
completing complex, goal-driven tasks through dynamic tool invocation and
multi-turn conversation. AURA combines open-weight ASR, TTS, and LLMs in a
cascaded pipeline and supports tools such as calendar booking, contact lookup,
web search, and email. Its modular design allows easy integration of new tools
using natural language prompts and action classes. On VoiceBench, AURA scores
92.75% on OpenBookQA-outperforming all open-weight systems and nearing
GPT-4o-and 4.39 on AlpacaEval, competitive with other open-weight systems.
Human evaluation shows 90% task success on complex, multi-turn speech tasks.

</details>


### [12] [AI's Euclid's Elements Moment: From Language Models to Computable Thought](https://arxiv.org/abs/2506.23080)
*Xinmin Fang,Lingfeng Tao,Zhengxiong Li*

Main category: cs.AI

TL;DR: 提出五阶段进化框架理解AI发展，类比人类认知技术进步，展示其发展阶段与反馈循环，指出当前过渡阶段及后续阶段，为未来研究提供理论基础和策略。


<details>
  <summary>Details</summary>
Motivation: 为理解AI发展提供全面框架，完善此前关于AI经济驱动和认知本质研究，解决‘如何发展’问题。

Method: 构建‘认知几何学’框架，类比人类认知技术发明，分析AI从专家系统到Transformer的架构转变。

Result: 明确AI发展的五阶段，指出当前处于‘元语言时刻’，后续还有‘数学符号时刻’和‘形式逻辑系统时刻’。

Conclusion: 该框架不仅解释AI过去架构转变，还为未来研究提供理论基础，为开发者提供可操作策略。

Abstract: This paper presents a comprehensive five-stage evolutionary framework for
understanding the development of artificial intelligence, arguing that its
trajectory mirrors the historical progression of human cognitive technologies.
We posit that AI is advancing through distinct epochs, each defined by a
revolutionary shift in its capacity for representation and reasoning, analogous
to the inventions of cuneiform, the alphabet, grammar and logic, mathematical
calculus, and formal logical systems. This "Geometry of Cognition" framework
moves beyond mere metaphor to provide a systematic, cross-disciplinary model
that not only explains AI's past architectural shifts-from expert systems to
Transformers-but also charts a concrete and prescriptive path forward.
Crucially, we demonstrate that this evolution is not merely linear but
reflexive: as AI advances through these stages, the tools and insights it
develops create a feedback loop that fundamentally reshapes its own underlying
architecture. We are currently transitioning into a "Metalinguistic Moment,"
characterized by the emergence of self-reflective capabilities like
Chain-of-Thought prompting and Constitutional AI. The subsequent stages, the
"Mathematical Symbolism Moment" and the "Formal Logic System Moment," will be
defined by the development of a computable calculus of thought, likely through
neuro-symbolic architectures and program synthesis, culminating in provably
aligned and reliable AI that reconstructs its own foundational representations.
This work serves as the methodological capstone to our trilogy, which
previously explored the economic drivers ("why") and cognitive nature ("what")
of AI. Here, we address the "how," providing a theoretical foundation for
future research and offering concrete, actionable strategies for startups and
developers aiming to build the next generation of intelligent systems.

</details>


### [13] [Can Large Language Models Capture Human Risk Preferences? A Cross-Cultural Study](https://arxiv.org/abs/2506.23107)
*Bing Song,Jianing Liu,Sisi Jian,Chenyang Wu,Vinayak Dixit*

Main category: cs.AI

TL;DR: 研究探究大语言模型模拟风险决策能力，对比模型与人类在彩票任务决策，发现模型比人类更风险厌恶，o1 - mini更接近人类决策，且提示语言影响模拟表现。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型应用增多，其在模拟复杂决策行为（如风险决策）的可靠性受关注，故研究其模拟风险决策场景的能力。

Method: 用悉尼、达卡、香港和南京交通陈述偏好调查数据，在一系列彩票任务中对比模型生成决策与人类实际反应，向ChatGPT 4o和ChatGPT o1 - mini提供人口统计输入以预测个体选择，用CRRA框架分析风险偏好。

Result: 两个模型比人类参与者更具风险厌恶行为，o1 - mini更接近人类决策；南京和香港多语言数据显示，中文提示下模型预测与实际反应偏差大于英文。

Conclusion: 大语言模型在复制人类风险行为方面有前景，但在语言和文化场景下存在局限。

Abstract: Large language models (LLMs) have made significant strides, extending their
applications to dialogue systems, automated content creation, and
domain-specific advisory tasks. However, as their use grows, concerns have
emerged regarding their reliability in simulating complex decision-making
behavior, such as risky decision-making, where a single choice can lead to
multiple outcomes. This study investigates the ability of LLMs to simulate
risky decision-making scenarios. We compare model-generated decisions with
actual human responses in a series of lottery-based tasks, using transportation
stated preference survey data from participants in Sydney, Dhaka, Hong Kong,
and Nanjing. Demographic inputs were provided to two LLMs -- ChatGPT 4o and
ChatGPT o1-mini -- which were tasked with predicting individual choices. Risk
preferences were analyzed using the Constant Relative Risk Aversion (CRRA)
framework. Results show that both models exhibit more risk-averse behavior than
human participants, with o1-mini aligning more closely with observed human
decisions. Further analysis of multilingual data from Nanjing and Hong Kong
indicates that model predictions in Chinese deviate more from actual responses
compared to English, suggesting that prompt language may influence simulation
performance. These findings highlight both the promise and the current
limitations of LLMs in replicating human-like risk behavior, particularly in
linguistic and cultural settings.

</details>


### [14] [The Societal Impact of Foundation Models: Advancing Evidence-based AI Policy](https://arxiv.org/abs/2506.23123)
*Rishi Bommasani*

Main category: cs.AI

TL;DR: 本文围绕三个主题解释人工智能时代技术与社会的共同演化，旨在为更好的AI治理奠定基础以实现更好的社会成果。


<details>
  <summary>Details</summary>
Motivation: 人工智能虽有前景，但基础模型难理解且可能带来危害，需研究技术与社会如何共同演化。

Method: 从概念框架（能力、风险和供应链）、实证见解（模型层面评估和组织层面指标带来的透明度）以及从理解到行动（更好理解社会影响以推进循证AI政策）三个主题进行研究。

Result: 未提及具体结果。

Conclusion: 通过构建科学基础和研究 - 政策接口，为人工智能时代实现更好的社会成果取得进展。

Abstract: Artificial intelligence is humanity's most promising technology because of
the remarkable capabilities offered by foundation models. Yet, the same
technology brings confusion and consternation: foundation models are poorly
understood and they may precipitate a wide array of harms. This dissertation
explains how technology and society coevolve in the age of AI, organized around
three themes. First, the conceptual framing: the capabilities, risks, and the
supply chain that grounds foundation models in the broader economy. Second, the
empirical insights that enrich the conceptual foundations: transparency created
via evaluations at the model level and indexes at the organization level.
Finally, the transition from understanding to action: superior understanding of
the societal impact of foundation models advances evidence-based AI policy.
View together, this dissertation makes inroads into achieving better societal
outcomes in the age of AI by building the scientific foundations and
research-policy interface required for better AI governance.

</details>


### [15] [Are Large Language Models Capable of Deep Relational Reasoning? Insights from DeepSeek-R1 and Benchmark Comparisons](https://arxiv.org/abs/2506.23128)
*Chi Chiu So,Yueyue Sun,Jun-Min Wang,Siu Pang Yung,Anthony Wai Keung Loh,Chun Pong Chau*

Main category: cs.AI

TL;DR: 本文评估比较三款前沿大语言模型在家族树和图推理中的能力，发现DeepSeek - R1表现最佳，但所有模型随问题复杂度增加表现变差，还指出需深入研究内部推理机制并给出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型进行深度关系推理的能力。

Method: 通过一系列精心设计的家族树和一般图推理基准任务评估和比较DeepSeek - R1、DeepSeek - V3和GPT - 4o三款模型。

Result: DeepSeek - R1在多任务和不同问题规模下F1分数最高，但所有模型随问题复杂度增加表现显著下降，DeepSeek - R1的长思维链响应有独特策略，但存在推理不连贯或不完整情况。

Conclusion: 研究为提升大语言模型推理能力提供实证和理论启示，指出需深入研究内部推理机制，还给出未来研究方向。

Abstract: How far are Large Language Models (LLMs) in performing deep relational
reasoning? In this paper, we evaluate and compare the reasoning capabilities of
three cutting-edge LLMs, namely, DeepSeek-R1, DeepSeek-V3 and GPT-4o, through a
suite of carefully designed benchmark tasks in family tree and general graph
reasoning. Our experiments reveal that DeepSeek-R1 consistently achieves the
highest F1-scores across multiple tasks and problem sizes, demonstrating strong
aptitude in logical deduction and relational inference. However, all evaluated
models, including DeepSeek-R1, struggle significantly as problem complexity
increases, largely due to token length limitations and incomplete output
structures. A detailed analysis of DeepSeek-R1's long Chain-of-Thought
responses uncovers its unique planning and verification strategies, but also
highlights instances of incoherent or incomplete reasoning, calling attention
to the need for deeper scrutiny into LLMs' internal inference dynamics. We
further discuss key directions for future work, including the role of
multimodal reasoning and the systematic examination of reasoning failures. Our
findings provide both empirical insights and theoretical implications for
advancing LLMs' reasoning abilities, particularly in tasks that demand
structured, multi-step logical inference. Our code repository will be publicly
available at https://github.com/kelvinhkcs/Deep-Relational-Reasoning.

</details>


### [16] [Context-Driven Knowledge Graph Completion with Semantic-Aware Relational Message Passing](https://arxiv.org/abs/2506.23141)
*Siyuan Li,Ruitong Liu,Yan Wen,Te Sun*

Main category: cs.AI

TL;DR: 提出语义感知关系消息传递框架，含Top - K邻居选择策略和多头注意力聚合器，在知识图谱补全任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于节点的消息传递机制在知识图谱中存在引入噪声、信息稀释和过平滑问题，需要更好的方法来利用语义上下文进行知识图谱补全。

Method: 提出语义感知关系消息传递框架，采用语义感知Top - K邻居选择策略评估中心节点与其关联边的语义相关性，选择最相关的K条边，并用多头注意力聚合器融合信息。

Result: 在多个既定基准测试中，该方法比现有方法取得了更优的性能。

Conclusion: 所提方法能有效利用知识图谱中边的结构和特征，更准确地捕捉和传播与链接预测任务相关的上下文信息，减少无关信息干扰。

Abstract: Semantic context surrounding a triplet $(h, r, t)$ is crucial for Knowledge
Graph Completion (KGC), providing vital cues for prediction. However,
traditional node-based message passing mechanisms, when applied to knowledge
graphs, often introduce noise and suffer from information dilution or
over-smoothing by indiscriminately aggregating information from all neighboring
edges. To address this challenge, we propose a semantic-aware relational
message passing. A core innovation of this framework is the introduction of a
\textbf{semantic-aware Top-K neighbor selection strategy}. Specifically, this
strategy first evaluates the semantic relevance between a central node and its
incident edges within a shared latent space, selecting only the Top-K most
pertinent ones. Subsequently, information from these selected edges is
effectively fused with the central node's own representation using a
\textbf{multi-head attention aggregator} to generate a semantically focused
node message. In this manner, our model not only leverages the structure and
features of edges within the knowledge graph but also more accurately captures
and propagates the contextual information most relevant to the specific link
prediction task, thereby effectively mitigating interference from irrelevant
information. Extensive experiments demonstrate that our method achieves
superior performance compared to existing approaches on several established
benchmarks.

</details>


### [17] [Rises for Measuring Local Distributivity in Lattices](https://arxiv.org/abs/2506.23168)
*Mohammad Abdulla,Tobias Hille,Dominik Dürrschnabel,Gerd Stumme*

Main category: cs.AI

TL;DR: 本文引入格中‘上升’概念量化形式概念分析中格的分配性，证明无非单位上升与分配性等价，研究其与经典分配性的关系及现实数据概念格的分配性特点。


<details>
  <summary>Details</summary>
Motivation: 形式概念分析中格常具高分配性，但缺乏量化分配性的标准度量。

Method: 引入格中‘上升’概念，通过其反映覆盖概念中属性或对象数量的变化来评估分配性。

Result: 证明格是分配的当且仅当没有非单位上升；现实数据的概念格高度并分配，但交分配性低；研究了并分配性在有序集层面的表现。

Conclusion: ‘上升’概念可作为量化格分配性的有效手段，对现实数据概念格的分配性有了新认识。

Abstract: Distributivity is a well-established and extensively studied notion in
lattice theory. In the context of data analysis, particularly within Formal
Concept Analysis (FCA), lattices are often observed to exhibit a high degree of
distributivity. However, no standardized measure exists to quantify this
property. In this paper, we introduce the notion of rises in (concept) lattices
as a means to assess distributivity. Rises capture how the number of attributes
or objects in covering concepts change within the concept lattice. We show that
a lattice is distributive if and only if no non-unit rises occur. Furthermore,
we relate rises to the classical notion of meet- and join distributivity. We
observe that concept lattices from real-world data are to a high degree
join-distributive, but much less meet-distributive. We additionally study how
join-distributivity manifests on the level of ordered sets.

</details>


### [18] [FinStat2SQL: A Text2SQL Pipeline for Financial Statement Analysis](https://arxiv.org/abs/2506.23273)
*Quang Hung Nguyen,Phuong Anh Trinh,Phan Quoc Hung Mai,Tuan Phong Trinh*

Main category: cs.AI

TL;DR: 提出FinStat2SQL轻量级文本转SQL管道，在金融报表自然语言查询上表现良好，为越南企业提供可扩展、低成本解决方案。


<details>
  <summary>Details</summary>
Motivation: 大语言模型下文本转SQL仍有挑战，金融领域因数据库设计和报表布局差异使问题更复杂。

Method: 采用多智能体设置结合大小语言模型进行实体提取、SQL生成和自我修正，构建特定领域数据库并在合成问答数据集上评估模型。

Result: 微调后的7B模型在消费级硬件上准确率达61.33%，响应时间小于4秒，优于GPT - 4o - mini。

Conclusion: FinStat2SQL为金融分析提供可扩展、经济高效的解决方案，使越南企业能使用人工智能查询。

Abstract: Despite the advancements of large language models, text2sql still faces many
challenges, particularly with complex and domain-specific queries. In finance,
database designs and financial reporting layouts vary widely between financial
entities and countries, making text2sql even more challenging. We present
FinStat2SQL, a lightweight text2sql pipeline enabling natural language queries
over financial statements. Tailored to local standards like VAS, it combines
large and small language models in a multi-agent setup for entity extraction,
SQL generation, and self-correction. We build a domain-specific database and
evaluate models on a synthetic QA dataset. A fine-tuned 7B model achieves
61.33\% accuracy with sub-4-second response times on consumer hardware,
outperforming GPT-4o-mini. FinStat2SQL offers a scalable, cost-efficient
solution for financial analysis, making AI-powered querying accessible to
Vietnamese enterprises.

</details>


### [19] [Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games](https://arxiv.org/abs/2506.23276)
*David Guzman Piedrahita,Yongjin Yang,Mrinmaya Sachan,Giorgia Ramponi,Bernhard Schölkopf,Zhijing Jin*

Main category: cs.AI

TL;DR: 研究多智能体大语言模型系统中代价高昂的制裁挑战，发现不同模型行为模式及推理型大模型合作困难，为协作部署提供见解。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型作为自主智能体应用增加，理解其合作与社会机制重要，需解决其平衡自身利益和集体福祉的挑战。

Method: 采用行为经济学中带制度选择的公共物品博弈，观察不同大语言模型在重复交互中应对社会困境的情况。

Result: 发现模型有四种不同行为模式，推理型大语言模型合作困难，传统大语言模型能实现高合作水平。

Conclusion: 当前提升大语言模型推理能力的方法未必带来合作，为需要持续协作的环境中部署大语言模型智能体提供有价值见解。

Abstract: As large language models (LLMs) are increasingly deployed as autonomous
agents, understanding their cooperation and social mechanisms is becoming
increasingly important. In particular, how LLMs balance self-interest and
collective well-being is a critical challenge for ensuring alignment,
robustness, and safe deployment. In this paper, we examine the challenge of
costly sanctioning in multi-agent LLM systems, where an agent must decide
whether to invest its own resources to incentivize cooperation or penalize
defection. To study this, we adapt a public goods game with institutional
choice from behavioral economics, allowing us to observe how different LLMs
navigate social dilemmas over repeated interactions. Our analysis reveals four
distinct behavioral patterns among models: some consistently establish and
sustain high levels of cooperation, others fluctuate between engagement and
disengagement, some gradually decline in cooperative behavior over time, and
others rigidly follow fixed strategies regardless of outcomes. Surprisingly, we
find that reasoning LLMs, such as the o1 series, struggle significantly with
cooperation, whereas some traditional LLMs consistently achieve high levels of
cooperation. These findings suggest that the current approach to improving
LLMs, which focuses on enhancing their reasoning capabilities, does not
necessarily lead to cooperation, providing valuable insights for deploying LLM
agents in environments that require sustained collaboration. Our code is
available at https://github.com/davidguzmanp/SanctSim

</details>


### [20] [GATSim: Urban Mobility Simulation with Generative Agents](https://arxiv.org/abs/2506.23306)
*Qi Liu,Can Li,Wanjing Ma*

Main category: cs.AI

TL;DR: 传统基于代理的城市出行模拟有局限，提出GATSim框架创建具有丰富行为特征的生成式代理用于城市出行模拟，经实验验证效果良好且代码开源。


<details>
  <summary>Details</summary>
Motivation: 传统基于代理的城市出行模拟依赖刚性规则系统，无法捕捉人类出行决策的复杂性、适应性和行为多样性，而大语言模型和AI代理技术带来新机遇。

Method: 提出GATSim框架，结合城市出行基础模型、代理认知系统和交通模拟环境，让代理拥有多样属性和偏好，通过心理记忆系统、工具使用能力和终身学习机制做决策，还有反思过程。

Result: 生成式代理能产生可信的出行行为，在出行场景中与人类标注者表现相当，自然产生宏观交通演化模式。

Conclusion: GATSim框架有效，生成式代理可用于城市出行模拟并实现行为适应。

Abstract: Traditional agent-based urban mobility simulations rely on rigid rule-based
systems that fail to capture the complexity, adaptability, and behavioral
diversity characteristic of human travel decision-making. Recent advances in
large language models and AI agent technology offer opportunities to create
agents with reasoning capabilities, persistent memory, and adaptive learning
mechanisms. We propose GATSim (Generative-Agent Transport Simulation), a novel
framework that leverages these advances to create generative agents with rich
behavioral characteristics for urban mobility simulation. Unlike conventional
approaches, GATSim agents possess diverse socioeconomic attributes, individual
lifestyles, and evolving preferences that shape their mobility decisions
through psychologically-informed memory systems, tool usage capabilities, and
lifelong learning mechanisms. The main contributions of this study include: (1)
a comprehensive architecture combining an urban mobility foundation model with
agent cognitive systems and transport simulation environment, (2) a fully
functional prototype implementation, and (3) systematic validation
demonstrating that generative agents produce believable travel behaviors.
Through designed reflection processes, generative agents in this study can
transform specific travel experiences into generalized insights, enabling
realistic behavioral adaptation over time with specialized mechanisms for
activity planning and real-time reactive behaviors tailored to urban mobility
contexts. Experiments show that generative agents perform competitively with
human annotators in mobility scenarios while naturally producing macroscopic
traffic evolution patterns. The code for the prototype system is shared at
https://github.com/qiliuchn/gatsim.

</details>


### [21] [The Confidence Paradox: Can LLM Know When It's Wrong](https://arxiv.org/abs/2506.23464)
*Sahil Tripathi,Md Tabrez Nafis,Imran Hussain,Jiechao Gao*

Main category: cs.AI

TL;DR: 现有DocVQA系统存在伦理不透明问题，本文提出HonestVQA框架解决该问题，提升了性能并减少过度自信。


<details>
  <summary>Details</summary>
Motivation: 现有DocVQA系统存在模型置信度与实际知识不匹配的伦理风险，现有方法在伦理响应方面不足。

Method: 引入HonestVQA自监督诚实校准框架，量化不确定性，用加权损失函数校准置信度，通过对比学习强化伦理响应行为，还引入两个评估指标。

Result: 在多个数据集上提高了准确性和F1分数，降低了过度自信，跨领域评估表现良好，消融实验显示无校准和对比损失会使准确率下降。

Conclusion: HonestVQA框架有效解决了DocVQA系统的伦理问题，提升了性能和泛化能力。

Abstract: Document Visual Question Answering (DocVQA) systems are increasingly deployed
in real world applications, yet they remain ethically opaque-often producing
overconfident answers to ambiguous questions or failing to communicate
uncertainty in a trustworthy manner. This misalignment between model confidence
and actual knowledge poses significant risks, particularly in domains requiring
ethical accountability. Existing approaches such as LayoutLMv3, UDOP, and DONUT
have advanced SOTA performance by focusing on architectural sophistication and
accuracy; however, they fall short in ethical responsiveness.
  To address these limitations, we introduce HonestVQA, a self-supervised
honesty calibration framework for ethically aligned DocVQA. Our model-agnostic
method quantifies uncertainty to identify knowledge gaps, aligns model
confidence with actual correctness using weighted loss functions, and enforces
ethical response behavior via contrastive learning. We further introduce two
principled evaluation metrics--Honesty Score (H-Score) and Ethical Confidence
Index (ECI)--to benchmark alignment between confidence, accuracy, and ethical
communication. Empirically, HonestVQA improves DocVQA accuracy by up to 4.3%
and F1 by 4.3% across SpDocVQA, InfographicsVQA, and SROIE datasets. It reduces
overconfidence, lowering H-Score and ECI by 0.072 and 0.078, respectively. In
cross domain evaluation, it achieves up to 78.9% accuracy and 76.1% F1-score,
demonstrating strong generalization. Ablation shows a 3.8% drop in accuracy
without alignment or contrastive loss.

</details>


### [22] [Data Augmentation for Cognitive Behavioral Therapy: Leveraging ERNIE Language Models using Artificial Intelligence](https://arxiv.org/abs/2506.23503)
*Bosubabu Sambana,Kondreddygari Archana,Suram Indhra Sena Reddy,Shaik Meethaigar Jameer Basha,Shaik Karishma*

Main category: cs.AI

TL;DR: 提出利用CBT框架结合多种技术分析社交媒体数据以检测负面情绪和认知扭曲，还能预测更多心理问题，为心理治疗师提供早期干预工具。


<details>
  <summary>Details</summary>
Motivation: CBT治疗需准确识别认知途径，但缺乏分析社交媒体上认知途径的方法，为心理治疗师提供在线环境及时有效干预手段。

Method: 利用CBT框架，结合接受、承诺和数据增强，采用BERT、RoBERTa等进行情感分析、文本摘要和多语言翻译，分析社交媒体数据。

Result: 能检测负面情绪和认知扭曲，还可预测更多负面副作用和潜在心理疾病。

Conclusion: 该系统可使心理治疗师更全面理解和干预，是早期检测和治疗心理问题的有力工具。

Abstract: Cognitive Behavioral Therapy (CBT) is a proven approach for addressing the
irrational thought patterns associated with mental health disorders, but its
effectiveness relies on accurately identifying cognitive pathways to provide
targeted treatment. In today's digital age, individuals often express negative
emotions on social media, where they may reveal cognitive distortions, and in
severe cases, exhibit suicidal tendencies. However, there is a significant gap
in methodologies designed to analyze these cognitive pathways, which could be
critical for psychotherapists aiming to deliver timely and effective
interventions in online environments. Cognitive Behavioral Therapy (CBT)
framework leveraging acceptance, commitment and data augmentation to categorize
and address both textual and visual content as positive or negative.
Specifically, the system employs BERT, RoBERTa for Sentiment Analysis and T5,
PEGASUS for Text Summarization, mT5 for Text Translation in Multiple Languages
focusing on detecting negative emotions and cognitive distortions within social
media data. While existing models are primarily designed to identify negative
thoughts, the proposed system goes beyond this by predicting additional
negative side effects and other potential mental health disorders likes
Phobias, Eating Disorders. This enhancement allows for a more comprehensive
understanding and intervention strategy, offering psychotherapists a powerful
tool for early detection and treatment of various psychological issues.

</details>


### [23] [Hybrid Approach for Electricity Price Forecasting using AlexNet and LSTM](https://arxiv.org/abs/2506.23504)
*Bosubabu Sambana,Kotamsetty Geethika Devi,Bandi Rajeswara Reddy,Galeti Mohammad Hussain,Gownivalla Siddartha*

Main category: cs.AI

TL;DR: 本文提出结合AlexNet和LSTM的混合模型预测电价，比传统模型和单独模型更准确。


<details>
  <summary>Details</summary>
Motivation: 传统方法在预测电价时仅关注需求和价格，对数据的分析不充分，且RNN和ANN难以处理外汇时间序列数据，因此需要新方法提高电价预测准确性。

Method: 结合AlexNet和LSTM算法构建混合模型，使用最小 - 最大缩放和时间窗口等方法，基于包含需求、温度等重要元素的历史数据进行电价预测。

Result: 混合模型在准确性上优于单独模型，混合模型准确率为97.08%，RNN和ANN准确率分别为96.64%和96.63%。

Conclusion: 这种混合模型在电价预测准确性上表现更优。

Abstract: The recent development of advanced machine learning methods for hybrid models
has greatly addressed the need for the correct prediction of electrical prices.
This method combines AlexNet and LSTM algorithms, which are used to introduce a
new model with higher accuracy in price forecasting. Despite RNN and ANN being
effective, they often fail to deal with forex time sequence data. The
traditional methods do not accurately forecast the prices. These traditional
methods only focus on demand and price which leads to insufficient analysis of
data. To address this issue, using the hybrid approach, which focuses on
external variables that also effect the predicted prices. Nevertheless, due to
AlexNet's excellent feature extraction and LSTM's learning sequential patterns,
the prediction accuracy is vastly increased. The model is built on the past
data, which has been supplied with the most significant elements like demand,
temperature, sunlight, and rain. For example, the model applies methods, such
as minimum-maximum scaling and a time window, to predict the electricity prices
of the future. The results show that this hybrid model is good than the
standalone ones in terms of accuracy. Although we got our accuracy rating of
97.08, it shows higher accompaniments than remaining models RNN and ANN with
accuracies of 96.64 and 96.63 respectively.

</details>


### [24] [Assessing GPTZero's Accuracy in Identifying AI vs. Human-Written Essays](https://arxiv.org/abs/2506.23517)
*Selin Dik,Osman Erdem,Mehmet Dik*

Main category: cs.AI

TL;DR: 研究GPTZero检测不同长度AI生成文本的成功率，发现其检测纯AI内容有效，但区分人类撰写文本可靠性有限，教育者使用需谨慎。


<details>
  <summary>Details</summary>
Motivation: 学生使用AI工具增多，教师用AI检测工具，但工具可靠性未知，研究GPTZero检测不同长度文章的成功率。

Method: 收集28篇AI生成和50篇人类撰写的论文，将不同长度文章输入GPTZero测量AI生成百分比和置信度。

Result: 大部分AI生成论文被准确检测，但人类撰写文章有波动和少量误判。

Conclusion: GPTZero检测纯AI内容有效，区分人类文本可靠性有限，教育者不应仅依赖AI检测工具。

Abstract: As the use of AI tools by students has become more prevalent, instructors
have started using AI detection tools like GPTZero and QuillBot to detect AI
written text. However, the reliability of these detectors remains uncertain. In
our study, we focused mostly on the success rate of GPTZero, the most-used AI
detector, in identifying AI-generated texts based on different lengths of
randomly submitted essays: short (40-100 word count), medium (100-350 word
count), and long (350-800 word count). We gathered a data set consisting of
twenty-eight AI-generated papers and fifty human-written papers. With this
randomized essay data, papers were individually plugged into GPTZero and
measured for percentage of AI generation and confidence. A vast majority of the
AI-generated papers were detected accurately (ranging from 91-100% AI believed
generation), while the human generated essays fluctuated; there were a handful
of false positives. These findings suggest that although GPTZero is effective
at detecting purely AI-generated content, its reliability in distinguishing
human-authored texts is limited. Educators should therefore exercise caution
when relying solely on AI detection tools.

</details>


### [25] [ChemActor: Enhancing Automated Extraction of Chemical Synthesis Actions with LLM-Generated Data](https://arxiv.org/abs/2506.23520)
*Yu Zhang,Ruijie Yu,Jidong Tian,Feng Zhu,Jiapeng Liu,Xiaokang Yang,Yaohui Jin,Yanyan Xu*

Main category: cs.AI

TL;DR: 本文提出ChemActor模型转换化学实验流程，用新框架和评估指标提升性能，实验表现超基线10%。


<details>
  <summary>Details</summary>
Motivation: 有机化学中自动化提取化学流程任务因化学语言模糊和人工标注成本高而具有挑战性。

Method: 提出顺序LLM生成数据框架，集成数据选择模块与通用LLM；引入多轮LLMs循环审查指标。

Result: 在R2D和D2A任务实验中，经LLM生成数据增强的ChemActor模型达到了最优性能，比基线模型高10%。

Conclusion: ChemActor结合新框架和指标能有效处理化学实验流程转换，代码开源。

Abstract: With the increasing interest in robotic synthesis in the context of organic
chemistry, the automated extraction of chemical procedures from literature is
critical. However, this task remains challenging due to the inherent ambiguity
of chemical language and the high cost of human annotation required for
developing reliable computer-aided extraction protocols. Here, we present
ChemActor, a fully fine-tuned large language model (LLM), as a chemical
executor to convert between unstructured experimental procedures and structured
action sequences. We propose a sequential LLM-generated data framework to
address the challenges of insufficient and low-quality annotated data. This
framework integrates a data selection module that selects data based on
distribution divergence, with a general-purpose LLM, to generate
machine-executable actions from a single molecule input. Additionally, we
introduce a novel multi-round LLMs circle review metric, which reflects the
model's advanced understanding of chemical experimental procedures. Extensive
experiments on reaction-to-description (R2D) and description-to-action (D2A)
tasks demonstrate that ChemActor, augmented by LLM-generated data, achieves
state-of-the-art performance, outperforming the baseline model by 10%. The code
is available at: https://github.com/Zhanghahah/ChemActor.

</details>


### [26] [CooT: Learning to Coordinate In-Context with Coordination Transformers](https://arxiv.org/abs/2506.23549)
*Huai-Chih Wang,Hsiang-Chun Chuang,Hsi-Chun Cheng,Dai-Jie Wu,Shao-Hua Sun*

Main category: cs.AI

TL;DR: 提出协调变换器CooT框架解决多智能体系统中智能体协调问题，在基准测试中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统中智能体协调方法存在泛化能力差、训练成本高的问题。

Method: 提出CooT框架，利用近期交互历史适应未知伙伴，在多样化智能体对的交互轨迹上训练，无需显式监督或微调。

Result: 在Overcooked基准测试中显著优于基线方法，人类评估确认其为最有效协作伙伴，消融实验凸显其鲁棒性等特点。

Conclusion: CooT框架能有效解决多智能体系统中智能体协调问题，对未知伙伴有良好适应性。

Abstract: Effective coordination among artificial agents in dynamic and uncertain
environments remains a significant challenge in multi-agent systems. Existing
approaches, such as self-play and population-based methods, either generalize
poorly to unseen partners or require extensive training. To overcome these
limitations, we propose Coordination Transformers (CooT), a novel in-context
coordination framework that uses recent interaction histories to adapt to
unseen partners rapidly. Unlike previous approaches that primarily aim to
increase the diversity of training partners, CooT explicitly focuses on
adapting to new partner behaviors by predicting actions aligned with observed
partner interactions. Trained on interaction trajectories collected from
diverse pairs of agents with complementary behaviors, CooT quickly learns
effective coordination strategies without explicit supervision or fine-tuning.
Evaluations on the Overcooked benchmark demonstrate that CooT significantly
outperforms baseline methods in coordination tasks involving previously unseen
partners. Human evaluations further confirm CooT as the most effective
collaborative partner, while extensive ablations highlight its robustness,
flexibility, and sensitivity to context in multi-agent scenarios.

</details>


### [27] [MMReason: An Open-Ended Multi-Modal Multi-Step Reasoning Benchmark for MLLMs Toward AGI](https://arxiv.org/abs/2506.23563)
*Huanjin Yao,Jiaxing Huang,Yawen Qiu,Michael K. Chen,Wenzheng Liu,Wei Zhang,Wenjie Zeng,Xikun Zhang,Jingyi Zhang,Yuxin Song,Wenhao Wu,Dacheng Tao*

Main category: cs.AI

TL;DR: 现有多模态大语言模型（MLLM）基准难以全面评估长链推理能力，本文提出MMReason基准来解决此问题，并对流行MLLM进行评估分析。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM基准在评估长链推理能力时存在缺乏难度和多样性、易被猜测和记忆、无法评估中间推理步骤等问题，需要新基准来填补空白。

Method: 从多领域和多难度等级收集多步推理问题，将问题转为开放式格式并使用多模型投票过滤，标注详细步骤并设计三元评分机制评估中间推理步骤。

Result: 使用MMReason对流行MLLM进行基准测试并深入分析其推理能力。

Conclusion: MMReason有望成为推进MLLM推理研究的宝贵资源。

Abstract: Reasoning plays a crucial role in advancing Multimodal Large Language Models
(MLLMs) toward Artificial General Intelligence. However, existing MLLM
benchmarks often fall short in precisely and comprehensively evaluating
long-chain reasoning abilities from three key aspects: (1) lack of difficulty
and diversity, (2) susceptibility to guessability and memorization, (3)
inadequate assessment of intermediate reasoning steps. To fill this gap, we
introduce MMReason, a new benchmark designed to precisely and comprehensively
evaluate MLLM long-chain reasoning capability with diverse, open-ended,
challenging questions. First, we curate challenging questions requiring
multi-step reasoning from various fields (i.e., 6 disciplines) and multiple
difficulty levels (i.e., from pre-university to university, and from
foundational to competition tiers). Second, these questions are reformulated
into an open-ended format and filtered using a multi-model voting technique to
eliminate shortcut cases related to guessing and memorization, ensuring robust
reasoning evaluations. Third, we annotate the questions with detailed
step-by-step solutions, and design a reference-based ternary scoring mechanism
to reliably assess intermediate reasoning steps. With MMReason, we benchmark
popular leading MLLMs and provide an in-depth analysis of their reasoning
capabilities. We hope MMReason will serve as a valuable resource for advancing
MLLM reasoning research. Code will be available at
https://github.com/HJYao00/MMReason.

</details>


### [28] [Evaluating Multi-Agent Defences Against Jailbreaking Attacks on Large Language Models](https://arxiv.org/abs/2506.23576)
*Maria Carolina Cornelia Wit,Jun Pang*

Main category: cs.AI

TL;DR: 本文研究多智能体大语言模型系统抵御越狱攻击的效果，发现其能增强抗性但存在局限性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型越狱攻击引发担忧，研究多智能体系统抵御此类攻击的可行性。

Method: 评估三种越狱策略，复现AutoDefense框架，比较单智能体与多智能体配置。

Result: 多智能体系统增强了对越狱攻击的抗性，减少假阴性，但效果因攻击类型而异，且增加了假阳性和计算开销。

Conclusion: 当前自动化防御存在局限，为未来大语言模型系统提高对齐鲁棒性指明方向。

Abstract: Recent advances in large language models (LLMs) have raised concerns about
jailbreaking attacks, i.e., prompts that bypass safety mechanisms. This paper
investigates the use of multi-agent LLM systems as a defence against such
attacks. We evaluate three jailbreaking strategies, including the original
AutoDefense attack and two from Deepleaps: BetterDan and JB. Reproducing the
AutoDefense framework, we compare single-agent setups with two- and three-agent
configurations. Our results show that multi-agent systems enhance resistance to
jailbreaks, especially by reducing false negatives. However, its effectiveness
varies by attack type, and it introduces trade-offs such as increased false
positives and computational overhead. These findings point to the limitations
of current automated defences and suggest directions for improving alignment
robustness in future LLM systems.

</details>


### [29] [Self-correcting Reward Shaping via Language Models for Reinforcement Learning Agents in Games](https://arxiv.org/abs/2506.23626)
*António Afonso,Iolanda Leite,Alessandro Sestini,Florian Fuchs,Konrad Tollmar,Linus Gisslén*

Main category: cs.AI

TL;DR: 提出基于语言模型的自动微调强化学习智能体奖励函数权重的方法，在赛车任务中评估显示可提升性能，与人类专家调优表现相当。


<details>
  <summary>Details</summary>
Motivation: 解决在生产环境中部署强化学习智能体时，设计有效奖励函数需专家且游戏内容或机制修改后原奖励权重不再最优的问题。

Method: 基于用户定义的基于语言的行为目标，语言模型在每次迭代中根据目标行为和先前训练轮次的性能统计摘要提出更新的权重，形成闭环过程。

Result: 在赛车任务中，智能体性能迭代提升，一轮迭代成功率从9%提升到74%，最终迭代成功率达80%，平均完成圈数用时855个时间步，与专家调优表现接近。

Conclusion: 该自动微调奖励函数权重的方法有效，无需手动奖励工程即可使智能体行为与目标更一致。

Abstract: Reinforcement Learning (RL) in games has gained significant momentum in
recent years, enabling the creation of different agent behaviors that can
transform a player's gaming experience. However, deploying RL agents in
production environments presents two key challenges: (1) designing an effective
reward function typically requires an RL expert, and (2) when a game's content
or mechanics are modified, previously tuned reward weights may no longer be
optimal. Towards the latter challenge, we propose an automated approach for
iteratively fine-tuning an RL agent's reward function weights, based on a
user-defined language based behavioral goal. A Language Model (LM) proposes
updated weights at each iteration based on this target behavior and a summary
of performance statistics from prior training rounds. This closed-loop process
allows the LM to self-correct and refine its output over time, producing
increasingly aligned behavior without the need for manual reward engineering.
We evaluate our approach in a racing task and show that it consistently
improves agent performance across iterations. The LM-guided agents show a
significant increase in performance from $9\%$ to $74\%$ success rate in just
one iteration. We compare our LM-guided tuning against a human expert's manual
weight design in the racing task: by the final iteration, the LM-tuned agent
achieved an $80\%$ success rate, and completed laps in an average of $855$ time
steps, a competitive performance against the expert-tuned agent's peak $94\%$
success, and $850$ time steps.

</details>


### [30] [HASD: Hierarchical Adaption for pathology Slide-level Domain-shift](https://arxiv.org/abs/2506.23673)
*Jingsong Liu,Han Li,Chen Yang,Michael Deutges,Ario Sadafi,Xin You,Katharina Breininger,Nassir Navab,Peter J. Schüffler*

Main category: cs.AI

TL;DR: 提出用于病理切片级域偏移的分层适应框架HASD，在多个数据集上验证有效，能降低计算和标注成本。


<details>
  <summary>Details</summary>
Motivation: 当前病理领域适应方法关注图像块而非WSI，无法捕捉典型临床场景所需的全局WSI特征，需解决切片级域偏移挑战。

Method: 提出HASD框架，包含分层适应框架和原型选择机制。分层适应框架含域级对齐求解器、切片级几何不变性正则化和补丁级注意力一致性正则化；原型选择机制降低计算开销。

Result: 在五个数据集的两个切片级任务上验证，乳腺癌HER2分级队列AUROC提高4.1%，UCEC生存预测队列C指数提高3.9%。

Conclusion: 该方法为病理机构提供了实用可靠的切片级域适应解决方案，能最小化计算和标注成本。

Abstract: Domain shift is a critical problem for pathology AI as pathology data is
heavily influenced by center-specific conditions. Current pathology domain
adaptation methods focus on image patches rather than WSI, thus failing to
capture global WSI features required in typical clinical scenarios. In this
work, we address the challenges of slide-level domain shift by proposing a
Hierarchical Adaptation framework for Slide-level Domain-shift (HASD). HASD
achieves multi-scale feature consistency and computationally efficient
slide-level domain adaptation through two key components: (1) a hierarchical
adaptation framework that integrates a Domain-level Alignment Solver for
feature alignment, a Slide-level Geometric Invariance Regularization to
preserve the morphological structure, and a Patch-level Attention Consistency
Regularization to maintain local critical diagnostic cues; and (2) a prototype
selection mechanism that reduces computational overhead. We validate our method
on two slide-level tasks across five datasets, achieving a 4.1\% AUROC
improvement in a Breast Cancer HER2 Grading cohort and a 3.9\% C-index gain in
a UCEC survival prediction cohort. Our method provides a practical and reliable
slide-level domain adaption solution for pathology institutions, minimizing
both computational and annotation costs.

</details>


### [31] [PokéAI: A Goal-Generating, Battle-Optimizing Multi-agent System for Pokemon Red](https://arxiv.org/abs/2506.23689)
*Zihao Liu,Xinhang Sui,Yueran Song,Siwen Wang*

Main category: cs.AI

TL;DR: 介绍首个基于文本的多智能体大语言模型框架PokéAI，可自主玩《精灵宝可梦红》，开发战斗模块，胜率高，发现语言能力与战略推理有关及各模型有独特玩法。


<details>
  <summary>Details</summary>
Motivation: 开发能自主玩《精灵宝可梦红》的多智能体大语言模型框架。

Method: 构建包含规划、执行、评估三个专门智能体的系统，形成闭环决策系统，开发执行智能体中的战斗模块。

Result: 战斗AI在50次野外遭遇中平均胜率80.8%，与人类玩家差距6%；模型战斗表现与语言任务得分强相关；各LLM有独特玩法。

Conclusion: PokéAI系统有效，语言能力和战略推理有联系，各模型有不同战略行为。

Abstract: We introduce Pok\'eAI, the first text-based, multi-agent large language model
(LLM) framework designed to autonomously play and progress through Pok\'emon
Red. Our system consists of three specialized agents-Planning, Execution, and
Critique-each with its own memory bank, role, and skill set. The Planning Agent
functions as the central brain, generating tasks to progress through the game.
These tasks are then delegated to the Execution Agent, which carries them out
within the game environment. Upon task completion, the Critique Agent evaluates
the outcome to determine whether the objective was successfully achieved. Once
verification is complete, control returns to the Planning Agent, forming a
closed-loop decision-making system.
  As a preliminary step, we developed a battle module within the Execution
Agent. Our results show that the battle AI achieves an average win rate of
80.8% across 50 wild encounters, only 6% lower than the performance of an
experienced human player. Furthermore, we find that a model's battle
performance correlates strongly with its LLM Arena score on language-related
tasks, indicating a meaningful link between linguistic ability and strategic
reasoning. Finally, our analysis of gameplay logs reveals that each LLM
exhibits a unique playstyle, suggesting that individual models develop distinct
strategic behaviors.

</details>


### [32] [Agent4S: The Transformation of Research Paradigms from the Perspective of Large Language Models](https://arxiv.org/abs/2506.23692)
*Boyuan Zheng,Zerui Fang,Zhe Xu,Rui Wang,Yiwen Chen,Cunshi Wang,Mengwei Qu,Lei Lei,Zhen Feng,Yan Liu,Yuyang Li,Mingzhou Tan,Jiaji Wu,Jianwei Shuai,Jia Li,Fangfu Ye*

Main category: cs.AI

TL;DR: 提出Agent4S作为第五科学范式，介绍其五级分类及发展路线


<details>
  <summary>Details</summary>
Motivation: AI4S无法解决当前研究范式的核心低效问题

Method: 提出Agent4S概念并给出五级分类

Result: 构建了从简单任务自动化到全自主协作“AI科学家”的清晰路线图

Conclusion: 该框架定义了科学发现的下一个革命性步骤

Abstract: While AI for Science (AI4S) serves as an analytical tool in the current
research paradigm, it doesn't solve its core inefficiency. We propose "Agent
for Science" (Agent4S)-the use of LLM-driven agents to automate the entire
research workflow-as the true Fifth Scientific Paradigm. This paper introduces
a five-level classification for Agent4S, outlining a clear roadmap from simple
task automation to fully autonomous, collaborative "AI Scientists." This
framework defines the next revolutionary step in scientific discovery.

</details>


### [33] [A New Perspective On AI Safety Through Control Theory Methodologies](https://arxiv.org/abs/2506.23703)
*Lars Ullrich,Walter Zimmer,Ross Greer,Knut Graichen,Alois C. Knoll,Mohan Trivedi*

Main category: cs.AI

TL;DR: 本文提出基于跨学科解读的AI安全新视角——数据控制，以推动AI安全分析和保障。


<details>
  <summary>Details</summary>
Motivation: AI快速发展，但安全保障是关键问题，尤其是在安全关键的网络物理系统中，需提高AI安全性。

Method: 采用系统理论启发和系统分析驱动的方式，对底层数据生成过程和AI系统抽象进行跨学科解读，运用自上而下的方法。

Result: 提出数据控制新视角，为安全分析和保障勾勒出通用基础。

Conclusion: 新视角能刺激AI工程以跨学科方式利用现有安全分析和保障，推动数据控制范式，为特定AI系统和应用完善及未来创新做准备。

Abstract: While artificial intelligence (AI) is advancing rapidly and mastering
increasingly complex problems with astonishing performance, the safety
assurance of such systems is a major concern. Particularly in the context of
safety-critical, real-world cyber-physical systems, AI promises to achieve a
new level of autonomy but is hampered by a lack of safety assurance. While
data-driven control takes up recent developments in AI to improve control
systems, control theory in general could be leveraged to improve AI safety.
Therefore, this article outlines a new perspective on AI safety based on an
interdisciplinary interpretation of the underlying data-generation process and
the respective abstraction by AI systems in a system theory-inspired and system
analysis-driven manner. In this context, the new perspective, also referred to
as data control, aims to stimulate AI engineering to take advantage of existing
safety analysis and assurance in an interdisciplinary way to drive the paradigm
of data control. Following a top-down approach, a generic foundation for safety
analysis and assurance is outlined at an abstract level that can be refined for
specific AI systems and applications and is prepared for future innovation.

</details>


### [34] [Attestable Audits: Verifiable AI Safety Benchmarks Using Trusted Execution Environments](https://arxiv.org/abs/2506.23706)
*Christoph Schnabl,Daniel Hugenroth,Bill Marino,Alastair R. Beresford*

Main category: cs.AI

TL;DR: 提出可验证审计方法运行于可信执行环境，能验证与合规AI模型交互，保护敏感数据，用Llama - 3.1构建原型证明可行性。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型基准测试缺乏可验证结果和保密性，无法满足AI治理框架验证需求。

Method: 提出在可信执行环境中运行的可验证审计（Attestable Audits）方法。

Result: 构建了针对Llama - 3.1典型审计基准的原型，证明了方法可行性。

Conclusion: 可验证审计方法能解决现有基准测试问题，应对AI治理框架中的验证挑战。

Abstract: Benchmarks are important measures to evaluate safety and compliance of AI
models at scale. However, they typically do not offer verifiable results and
lack confidentiality for model IP and benchmark datasets. We propose Attestable
Audits, which run inside Trusted Execution Environments and enable users to
verify interaction with a compliant AI model. Our work protects sensitive data
even when model provider and auditor do not trust each other. This addresses
verification challenges raised in recent AI governance frameworks. We build a
prototype demonstrating feasibility on typical audit benchmarks against
Llama-3.1.

</details>


### [35] [BayesL: Towards a Logical Framework for Bayesian Networks](https://arxiv.org/abs/2506.23773)
*Stefano M. Nicoletti,Mariëlle Stoelinga*

Main category: cs.AI

TL;DR: 介绍了用于贝叶斯网络的逻辑框架BayesL，可进行查询和验证等操作


<details>
  <summary>Details</summary>
Motivation: 开发一个能对贝叶斯网络行为进行指定、查询和验证的工具

Method: 设计并提出结构化语言BayesL

Result: BayesL可创建对贝叶斯网络的查询，促进因果和基于证据关系的推理，无需手动修改模型就能进行情景评估

Conclusion: BayesL是一个用于贝叶斯网络的有效逻辑框架

Abstract: We introduce BayesL, a novel logical framework for specifying, querying, and
verifying the behaviour of Bayesian networks (BNs). BayesL (pronounced "Basil")
is a structured language that allows for the creation of queries over BNs. It
facilitates versatile reasoning concerning causal and evidence-based
relationships, and permits comprehensive what-if scenario evaluations without
the need for manual modifications to the model.

</details>


### [36] [When GNNs Met a Word Equations Solver: Learning to Rank Equations (Extended Technical Report)](https://arxiv.org/abs/2506.23784)
*Parosh Aziz Abdulla,Mohamed Faouzi Atig,Julie Cailler,Chencheng Liang,Philipp Rümmer*

Main category: cs.AI

TL;DR: 本文探索使用图神经网络（GNN）对单词方程进行排序以解决单词方程合取问题，提出新的图表示方法和三种排序方法，实验表明新框架在特定基准测试中表现更好。


<details>
  <summary>Details</summary>
Motivation: 解决单词方程合取问题时，求解器性能很大程度取决于方程处理顺序，因此探索使用GNN对单词方程进行排序。

Method: 提出基于图的单词方程表示方法，提出三种将多分类任务适应于方程排序问题的方法，借助单词方程的最小不可满足子集（MUSes）训练GNN。

Result: 与现有字符串求解器相比，新框架在每个变量在每个方程中最多出现一次的基准测试中解决了更多问题。

Conclusion: 使用GNN对单词方程进行排序的新框架在特定基准测试中优于现有求解器。

Abstract: Nielsen transformation is a standard approach for solving word equations: by
repeatedly splitting equations and applying simplification steps, equations are
rewritten until a solution is reached. When solving a conjunction of word
equations in this way, the performance of the solver will depend considerably
on the order in which equations are processed. In this work, the use of Graph
Neural Networks (GNNs) for ranking word equations before and during the solving
process is explored. For this, a novel graph-based representation for word
equations is presented, preserving global information across conjuncts,
enabling the GNN to have a holistic view during ranking. To handle the variable
number of conjuncts, three approaches to adapt a multi-classification task to
the problem of ranking equations are proposed. The training of the GNN is done
with the help of minimum unsatisfiable subsets (MUSes) of word equations. The
experimental results show that, compared to state-of-the-art string solvers,
the new framework solves more problems in benchmarks where each variable
appears at most once in each equation.

</details>


### [37] [Advancing Learnable Multi-Agent Pathfinding Solvers with Active Fine-Tuning](https://arxiv.org/abs/2506.23793)
*Anton Andreychuk,Konstantin Yakovlev,Aleksandr Panov,Alexey Skrynnik*

Main category: cs.AI

TL;DR: 本文基于MAPF - GPT提出MAPF - GPT - DDG，利用集中式专家数据微调预训练模型，实验表明其性能超现有学习型MAPF求解器，可处理百万规模智能体的MAPF实例。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体路径规划（MAPF）问题，寻找可扩展且高效的求解器，以应用于物流、搜索救援等现实场景。

Method: 在MAPF - GPT基础上，引入MAPF - GPT - DDG，用集中式专家数据微调预训练的MAPF模型，采用新的增量数据生成机制。

Result: MAPF - GPT - DDG在多个测试场景下的解质量超过所有现有的基于学习的MAPF求解器，能处理单环境中多达100万个智能体的MAPF实例。

Conclusion: MAPF - GPT - DDG在MAPF领域的性能和可扩展性方面达到新高度，为MAPF问题提供了更优解决方案。

Abstract: Multi-agent pathfinding (MAPF) is a common abstraction of multi-robot
trajectory planning problems, where multiple homogeneous robots simultaneously
move in the shared environment. While solving MAPF optimally has been proven to
be NP-hard, scalable, and efficient, solvers are vital for real-world
applications like logistics, search-and-rescue, etc. To this end, decentralized
suboptimal MAPF solvers that leverage machine learning have come on stage.
Building on the success of the recently introduced MAPF-GPT, a pure imitation
learning solver, we introduce MAPF-GPT-DDG. This novel approach effectively
fine-tunes the pre-trained MAPF model using centralized expert data. Leveraging
a novel delta-data generation mechanism, MAPF-GPT-DDG accelerates training
while significantly improving performance at test time. Our experiments
demonstrate that MAPF-GPT-DDG surpasses all existing learning-based MAPF
solvers, including the original MAPF-GPT, regarding solution quality across
many testing scenarios. Remarkably, it can work with MAPF instances involving
up to 1 million agents in a single environment, setting a new milestone for
scalability in MAPF domains.

</details>


### [38] [A Survey on Autonomy-Induced Security Risks in Large Model-Based Agents](https://arxiv.org/abs/2506.23844)
*Hang Su,Jun Luo,Chang Liu,Xiao Yang,Yichi Zhang,Yinpeng Dong,Jun Zhu*

Main category: cs.AI

TL;DR: 本文探讨大模型自主AI智能体安全问题，分析能力基础、安全漏洞，回顾防御策略并提出R2A2框架。


<details>
  <summary>Details</summary>
Motivation: 大模型自主AI智能体带来新安全风险，需研究应对。

Method: 先分析智能体自主性结构基础与能力，再剖析各层安全漏洞，最后回顾防御策略并提出R2A2框架。

Result: 明确了安全漏洞的失效模式与根源，介绍了不同自主层的防御策略，提出R2A2框架。

Conclusion: R2A2框架基于CMDPs，能在智能体决策循环中实现主动安全。

Abstract: Recent advances in large language models (LLMs) have catalyzed the rise of
autonomous AI agents capable of perceiving, reasoning, and acting in dynamic,
open-ended environments. These large-model agents mark a paradigm shift from
static inference systems to interactive, memory-augmented entities. While these
capabilities significantly expand the functional scope of AI, they also
introduce qualitatively novel security risks - such as memory poisoning, tool
misuse, reward hacking, and emergent misalignment - that extend beyond the
threat models of conventional systems or standalone LLMs. In this survey, we
first examine the structural foundations and key capabilities that underpin
increasing levels of agent autonomy, including long-term memory retention,
modular tool use, recursive planning, and reflective reasoning. We then analyze
the corresponding security vulnerabilities across the agent stack, identifying
failure modes such as deferred decision hazards, irreversible tool chains, and
deceptive behaviors arising from internal state drift or value misalignment.
These risks are traced to architectural fragilities that emerge across
perception, cognition, memory, and action modules. To address these challenges,
we systematically review recent defense strategies deployed at different
autonomy layers, including input sanitization, memory lifecycle control,
constrained decision-making, structured tool invocation, and introspective
reflection. We introduce the Reflective Risk-Aware Agent Architecture (R2A2), a
unified cognitive framework grounded in Constrained Markov Decision Processes
(CMDPs), which incorporates risk-aware world modeling, meta-policy adaptation,
and joint reward-risk optimization to enable principled, proactive safety
across the agent's decision-making loop.

</details>


### [39] [Beyond Statistical Learning: Exact Learning Is Essential for General Intelligence](https://arxiv.org/abs/2506.23908)
*András György,Tor Lattimore,Nevena Lazić,Csaba Szepesvári*

Main category: cs.AI

TL;DR: 现有AI系统在演绎推理任务上表现不佳，源于统计学习方法，应转向精确学习范式。


<details>
  <summary>Details</summary>
Motivation: 现有先进AI系统在简单演绎推理任务上常出错，无法实现具备可靠演绎推理能力的通用人工智能。

Method: 从优化统计性能转向采用精确学习范式，要求在所有输入上保证正确性。

Result: 未提及具体研究结果。

Conclusion: 精确学习对于基于学习的AI系统实现可靠演绎推理既必要又可行，应指导算法设计。

Abstract: Sound deductive reasoning -- the ability to derive new knowledge from
existing facts and rules -- is an indisputably desirable aspect of general
intelligence. Despite the major advances of AI systems in areas such as math
and science, especially since the introduction of transformer architectures, it
is well-documented that even the most advanced frontier systems regularly and
consistently falter on easily-solvable deductive reasoning tasks. Hence, these
systems are unfit to fulfill the dream of achieving artificial general
intelligence capable of sound deductive reasoning. We argue that their unsound
behavior is a consequence of the statistical learning approach powering their
development. To overcome this, we contend that to achieve reliable deductive
reasoning in learning-based AI systems, researchers must fundamentally shift
from optimizing for statistical performance against distributions on reasoning
problems and algorithmic tasks to embracing the more ambitious exact learning
paradigm, which demands correctness on all inputs. We argue that exact learning
is both essential and possible, and that this ambitious objective should guide
algorithm design.

</details>


### [40] [Performance of LLMs on Stochastic Modeling Operations Research Problems: From Theory to Practice](https://arxiv.org/abs/2506.23924)
*Akshit Kumar,Tianyi Peng,Yuhang Wu,Assaf Zeevi*

Main category: cs.AI

TL;DR: 本文评估大语言模型解决运筹学随机建模问题的能力，结果显示其与人类专家水平相当，凸显构建AI代理协助运筹学研究的潜力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在运筹学问题解决能力方面研究不足，尤其是随机建模问题，需对其进行评估。

Method: 手动收集研究生作业和博士资格考试问题测试大语言模型能力，利用SimOpt库研究其在不确定环境下的决策能力。

Result: 尽管实际中可靠自动化随机建模流程仍需大量工作，但当前大语言模型在课堂和实际场景中展现出与人类专家相当的水平。

Conclusion: 研究结果凸显了构建协助运筹学研究的AI代理的潜力，可通过自动化扩大运筹学的实际影响。

Abstract: Large language models (LLMs) have exhibited expert-level capabilities across
various domains. However, their abilities to solve problems in Operations
Research (OR) -- the analysis and optimization of mathematical models derived
from real-world problems or their verbal descriptions -- remain underexplored.
In this work, we take a first step toward evaluating LLMs' abilities to solve
stochastic modeling problems, a core class of OR problems characterized by
uncertainty and typically involving tools from probability, statistics, and
stochastic processes. We manually procure a representative set of
graduate-level homework and doctoral qualification-exam problems and test LLMs'
abilities to solve them. We further leverage SimOpt, an open-source library of
simulation-optimization problems and solvers, to investigate LLMs' abilities to
make real-world decisions under uncertainty. Our results show that, though a
nontrivial amount of work is still needed to reliably automate the stochastic
modeling pipeline in reality, state-of-the-art LLMs demonstrate proficiency on
par with human experts in both classroom and practical settings. These findings
highlight the potential of building AI agents that assist OR researchers and
amplify the real-world impact of OR through automation.

</details>


### [41] [Industrial brain: a human-like autonomous neuro-symbolic cognitive decision-making system](https://arxiv.org/abs/2506.23926)
*Junping Wang,Bicheng Wang,Yibo Xuea,Yuan Xie*

Main category: cs.AI

TL;DR: 提出工业大脑框架解决产业链弹性预测和规划难题，实验效果佳。


<details>
  <summary>Details</summary>
Motivation: 现有端到端深度学习在多混沌数据场景中对时空共演化结构重建和网络拓扑弹性预测效果不佳，需解决产业链科学管理和工程应用中弹性非平衡测量难题。

Method: 提出工业大脑，集成高阶活动驱动神经网络和CT - OODA符号推理，直接从全局变量观测数据自主规划弹性。

Result: 工业大脑显著优于现有弹性预测和规划方法，比GoT和OlaGPT框架准确率提高10.8%，比谱维降维提高11.03%，能泛化到未见拓扑和动态，且在观测干扰下性能稳健。

Conclusion: 工业大脑填补了产业链弹性预测和规划的重要空白。

Abstract: Resilience non-equilibrium measurement, the ability to maintain fundamental
functionality amidst failures and errors, is crucial for scientific management
and engineering applications of industrial chain. The problem is particularly
challenging when the number or types of multiple co-evolution of resilience
(for example, randomly placed) are extremely chaos. Existing end-to-end deep
learning ordinarily do not generalize well to unseen full-feld reconstruction
of spatiotemporal co-evolution structure, and predict resilience of network
topology, especially in multiple chaos data regimes typically seen in
real-world applications. To address this challenge, here we propose industrial
brain, a human-like autonomous cognitive decision-making and planning framework
integrating higher-order activity-driven neuro network and CT-OODA symbolic
reasoning to autonomous plan resilience directly from observational data of
global variable. The industrial brain not only understands and model structure
of node activity dynamics and network co-evolution topology without simplifying
assumptions, and reveal the underlying laws hidden behind complex networks, but
also enabling accurate resilience prediction, inference, and planning.
Experimental results show that industrial brain significantly outperforms
resilience prediction and planning methods, with an accurate improvement of up
to 10.8\% over GoT and OlaGPT framework and 11.03\% over spectral dimension
reduction. It also generalizes to unseen topologies and dynamics and maintains
robust performance despite observational disturbances. Our findings suggest
that industrial brain addresses an important gap in resilience prediction and
planning for industrial chain.

</details>


### [42] [AI Risk-Management Standards Profile for General-Purpose AI (GPAI) and Foundation Models](https://arxiv.org/abs/2506.23949)
*Anthony M. Barrett,Jessica Newman,Brandie Nonnecke,Nada Madkour,Dan Hendrycks,Evan R. Murphy,Krystal Jackson,Deepika Raman*

Main category: cs.AI

TL;DR: 文档为GPAI/基础模型提供风险管理实践与控制，供模型开发者及下游应用开发者参考。


<details>
  <summary>Details</summary>
Motivation: 多用途AI模型带来益处的同时存在严重后果的风险，需要进行风险管理。

Method: 参考NIST AI风险管理框架和ISO/IEC 23894的通用自愿指南，针对GPAI/基础模型开发者面临的独特问题进行调整和构建。

Result: 给出识别、分析和缓解GPAI/基础模型风险的风险管理实践或控制。

Conclusion: 该文档有助于遵循或使用领先的AI风险管理相关标准，适用于GPAI/基础模型开发者及下游应用开发者。

Abstract: Increasingly multi-purpose AI models, such as cutting-edge large language
models or other 'general-purpose AI' (GPAI) models, 'foundation models,'
generative AI models, and 'frontier models' (typically all referred to
hereafter with the umbrella term 'GPAI/foundation models' except where greater
specificity is needed), can provide many beneficial capabilities but also risks
of adverse events with profound consequences. This document provides
risk-management practices or controls for identifying, analyzing, and
mitigating risks of GPAI/foundation models. We intend this document primarily
for developers of large-scale, state-of-the-art GPAI/foundation models; others
that can benefit from this guidance include downstream developers of end-use
applications that build on a GPAI/foundation model. This document facilitates
conformity with or use of leading AI risk management-related standards,
adapting and building on the generic voluntary guidance in the NIST AI Risk
Management Framework and ISO/IEC 23894, with a focus on the unique issues faced
by developers of GPAI/foundation models.

</details>


### [43] [Harnessing AI Agents to Advance Research on Refugee Child Mental Health](https://arxiv.org/abs/2506.23992)
*Aditya Shrivastava,Komal Gupta,Shraddha Arora*

Main category: cs.AI

TL;DR: 研究提出基于AI的框架处理难民健康数据，比较两模型，Deepseek R1更优。


<details>
  <summary>Details</summary>
Motivation: 国际难民危机加深，数百万流离失所儿童遭受极端心理创伤，需处理数据了解儿童心理健康。

Method: 比较Zephyr - 7B - beta和DeepSeek R1 - 7B两个检索增强生成（RAG）管道处理人道主义数据集的效果。

Result: 两个模型都能正常工作，但Deepseek R1更优，答案相关性准确率达0.91。

Conclusion: 结合前沿AI方法、移民研究和儿童心理学，提出可扩展策略，帮助相关人员更好帮助流离失所儿童并关注其心理健康。

Abstract: The international refugee crisis deepens, exposing millions of dis placed
children to extreme psychological trauma. This research suggests a com pact,
AI-based framework for processing unstructured refugee health data and
distilling knowledge on child mental health. We compare two Retrieval-Aug
mented Generation (RAG) pipelines, Zephyr-7B-beta and DeepSeek R1-7B, to
determine how well they process challenging humanitarian datasets while avoid
ing hallucination hazards. By combining cutting-edge AI methods with migration
research and child psychology, this study presents a scalable strategy to
assist policymakers, mental health practitioners, and humanitarian agencies to
better assist displaced children and recognize their mental wellbeing. In
total, both the models worked properly but significantly Deepseek R1 is
superior to Zephyr with an accuracy of answer relevance 0.91

</details>


### [44] [Constructing Non-Markovian Decision Process via History Aggregator](https://arxiv.org/abs/2506.24026)
*Yongyi Wang,Wenxin Li*

Main category: cs.AI

TL;DR: 现有基准难以全面评估决策算法处理非马尔可夫动力学的能力，本文提出基于范畴论的通用方法，引入HAS，能有效表征非马尔可夫动力学并严格灵活评估决策算法。


<details>
  <summary>Details</summary>
Motivation: 现有基准无法全面评估决策算法处理非马尔可夫动力学的能力，影响算法决策系统发展和有效性。

Method: 基于范畴论设计通用方法，建立MDP和NMDP范畴并证明等价关系，通过HAS将非马尔可夫性引入决策问题设置。

Result: 分析表明该方法能有效表征广泛的非马尔可夫动力学。

Conclusion: 此方法为理解和处理非马尔可夫动力学提供新视角，可更严格灵活地评估决策算法。

Abstract: In the domain of algorithmic decision-making, non-Markovian dynamics manifest
as a significant impediment, especially for paradigms such as Reinforcement
Learning (RL), thereby exerting far-reaching consequences on the advancement
and effectiveness of the associated systems. Nevertheless, the existing
benchmarks are deficient in comprehensively assessing the capacity of decision
algorithms to handle non-Markovian dynamics. To address this deficiency, we
have devised a generalized methodology grounded in category theory. Notably, we
established the category of Markov Decision Processes (MDP) and the category of
non-Markovian Decision Processes (NMDP), and proved the equivalence
relationship between them. This theoretical foundation provides a novel
perspective for understanding and addressing non-Markovian dynamics. We further
introduced non-Markovianity into decision-making problem settings via the
History Aggregator for State (HAS). With HAS, we can precisely control the
state dependency structure of decision-making problems in the time series. Our
analysis demonstrates the effectiveness of our method in representing a broad
range of non-Markovian dynamics. This approach facilitates a more rigorous and
flexible evaluation of decision algorithms by testing them in problem settings
where non-Markovian dynamics are explicitly constructed.

</details>


### [45] [SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2506.24119)
*Bo Liu,Leon Guertler,Simon Yu,Zichen Liu,Penghui Qi,Daniel Balcells,Mickel Liu,Cheston Tan,Weiyan Shi,Min Lin,Wee Sun Lee,Natasha Jaques*

Main category: cs.AI

TL;DR: 介绍SPIRAL自博弈框架，可让模型无监督学习，在零和游戏中训练产生可迁移推理能力，多游戏训练提升效果，展示自主推理发展新方向。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法依赖人工问题 - 答案对和特定领域奖励设计，需要无监督的训练方式。

Method: 引入SPIRAL自博弈框架，实现全在线、多轮、多智能体强化学习系统，提出角色条件优势估计（RAE）稳定训练。

Result: 在Kuhn Poker上训练Qwen3 - 4B - Base，数学提升8.6%、通用推理提升8.4%；多游戏训练进一步提升性能；应用于强推理模型仍有2.0%平均提升。

Conclusion: 零和游戏能自然培养可迁移推理能力，为自主推理发展提供了有前景的方向。

Abstract: Recent advances in reinforcement learning have shown that language models can
develop sophisticated reasoning through training on tasks with verifiable
rewards, but these approaches depend on human-curated problem-answer pairs and
domain-specific reward engineering. We introduce SPIRAL, a self-play framework
where models learn by playing multi-turn, zero-sum games against continuously
improving versions of themselves, eliminating the need for human supervision.
Through self-play, SPIRAL generates an infinite curriculum of progressively
challenging problems as models must constantly adapt to stronger opponents. To
enable this self-play training at scale, We implement a fully online,
multi-turn, multi-agent reinforcement learning system for LLMs and propose
role-conditioned advantage estimation (RAE) to stabilize multi-agent training.
Using SPIRAL, self-play on zero-sum games produces reasoning capabilities that
transfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%
improvement on math and 8.4% on general reasoning, outperforming SFT on 25,000
expert game trajectories. Analysis reveals that this transfer occurs through
three cognitive patterns: systematic decomposition, expected value calculation,
and case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple
Negotiation) further enhances performance as each game develops distinct
reasoning strengths. Applying SPIRAL to a strong reasoning model
(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These
results demonstrate that zero-sum games naturally develop transferable
reasoning capabilities, highlighting a promising direction for autonomous
reasoning development.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [46] [CAD-Integrated Electrostatic Boundary Element Simulations with Non-Conforming Higher-Order Meshes](https://arxiv.org/abs/2506.22676)
*Benjamin Marussig,Jürgen Zechner,Thomas Rüberg,Lars Kielhorn,Domagoj Bošnjak,Thomas-Peter Fries*

Main category: cs.CE

TL;DR: 提出可实现电气设备虚拟原型设计的设计分析工作流，用快速边界元法模拟，实验验证其准确性与敏感性。


<details>
  <summary>Details</summary>
Motivation: 实现电气设备的虚拟原型设计，建立设计与分析的紧密联系。

Method: 通过CAD插件建立设计与分析的交互，采用快速边界元法（BEM）进行模拟，使用非一致高阶网格。

Result: 数值实验研究了方法的准确性及其对初始CAD表示的敏感性。

Conclusion: 该工作流实现了设计与分析的紧密联系，非一致高阶BEM方法能提供准确结果并简化交互。

Abstract: We present a design through analysis workflow that enables virtual
prototyping of electric devices. A CAD plugin establishes the interaction
between design and analysis, allowing the preparation of analysis models and
the visualization of its results within the design environment. The simulations
utilize a fast boundary element method (BEM) that allows for non-conforming and
higher-order meshes. Our numerical experiments investigate the accuracy of the
approach and its sensitivity to the initial CAD representation. Overall, the
workflow enables a close link between design and analysis, where the
non-conforming higher-order BEM approach provides accurate results and
significantly simplifies the interaction.

</details>


### [47] [Improved design of an active landing gear for a passenger aircraft using multi-objective optimization technique](https://arxiv.org/abs/2506.22870)
*Milad Zarchi,Behrooz Attaran*

Main category: cs.CE

TL;DR: 本文用蜂群启发的多目标算法优化起落架系统，结果显示优化后的主动减震系统在减少弹跳、俯仰位移等方面优于被动系统，提升舒适度和结构疲劳寿命。


<details>
  <summary>Details</summary>
Motivation: 传统起落架系统在不同着陆和跑道场景下效率下降，需解决该问题。

Method: 用蜂群启发的多目标算法同时优化控制器系数、非线性液压执行器参数和减震器，进行敏感性和鲁棒性分析，推导并数值求解空客A320 - 200着陆时的动态飞行方程。

Result: 经两种蜂群算法优化的主动减震系统在时域和频域上均优于被动系统，能减少弹跳和俯仰位移、动量、悬架行程和冲击力。

Conclusion: 优化后的主动减震系统显著提升乘客舒适度，延长结构疲劳寿命，具有工业应用潜力。

Abstract: The landing gear system is a major aircraft subsystem that must withstand
extreme forces during ground maneuvers and absorb vibrations. While traditional
systems perform well under normal conditions, their efficiency drops under
varying landing and runway scenarios. This study addresses this issue by
simultaneously optimizing controller coefficients, parameters of a nonlinear
hydraulic actuator integrated into the traditional shock absorber, and a
vibration absorber using a bee-inspired multi-objective algorithm. To
demonstrate adaptability, the paper includes sensitivity analysis for
three-point landings affected by added payload and touchdown speed, and
robustness analysis for one- and two-point landings under emergency wind
conditions. The dynamic flight equations of an Airbus A320-200 during landing
are derived and solved numerically. Results show that the active shock absorber
system, optimized via two bee-based algorithms, outperforms the passive system
in reducing bounce and pitch displacements and momenta, suspension travel, and
impact force in both time and frequency domains. This leads to significantly
improved passenger comfort and potentially longer structural fatigue life,
demonstrating industrial applicability.

</details>


### [48] [Feasibility of spectral-element modeling of wave propagation through the anatomy of marine mammals](https://arxiv.org/abs/2506.22944)
*Carlos García A.,Vladimiro Boselli,Aida Hejazi Nooghabi,Andrea Colombi,Lapo Boschi*

Main category: cs.CE

TL;DR: 首次用3D谱元法（SEM）模拟宽吻海豚头部超声波传播，相比有限元法（FEM）有优势，为海洋生物学研究开辟新途径。


<details>
  <summary>Details</summary>
Motivation: 传统有限元法在高频模拟存在线性系统求逆成本高、收敛慢的问题，需要更好的方法研究海豚头部超声波传播，以助力海洋生物学相关研究。

Method: 使用CT扫描数据构建详细六面体网格，采用3D谱元法模拟平面波和球面波。

Result: 模拟结果证实了谱元法在超声时域建模中的有效性。

Conclusion: 谱元法克服了有限元法的局限性，为测试海豚生物声学假设提供强大可扩展工具，对保护和理解海洋哺乳动物听觉系统有重要意义。

Abstract: This study introduces the first 3D spectral-element method (SEM) simulation
of ultrasonic wave propagation in a bottlenose dolphin (Tursiops truncatus)
head. Unlike traditional finite-element methods (FEM), which struggle with
high-frequency simulations due to costly linear-system inversions and slower
convergence, SEM offers exponential convergence and efficient parallel
computation. Using Computed Tomography (CT) scan data, we developed a detailed
hexahedral mesh capturing complex anatomical features, such as acoustic fats
and jaws. Our simulations of plane and spherical waves confirm SEM's
effectiveness for ultrasonic time-domain modeling. This approach opens new
avenues for marine biology, contributing to research in echolocation, the
impacts of anthropogenic marine noise pollution and the biophysics of hearing
and click generation in marine mammals. By overcoming FEM's limitations, SEM
provides a powerful scalable tool to test hypotheses about dolphin
bioacoustics, with significant implications for conservation and understanding
marine mammal auditory systems under increasing environmental challenges.

</details>


### [49] [SparStencil: Retargeting Sparse Tensor Cores to Scientific Stencil Computations via Structured Sparsity Transformation](https://arxiv.org/abs/2506.22969)
*Qi Li,Kun Li,Haozhi Han,Liang Yuan,Junshi Chen,Yunquan Zhang,Yifeng Chen,Hong An,Ting Cao,Mao Yang*

Main category: cs.CE

TL;DR: 提出SparStencil系统，利用结构化稀疏转换将稀疏TCU用于科学模板计算，在79个模板内核上评估有显著加速效果。


<details>
  <summary>Details</summary>
Motivation: 稀疏张量核在AI工作负载有性能提升，但科学模板计算的不规则稀疏模式使其潜力未被挖掘。

Method: 引入Adaptive Layout Morphing、Structured Sparsity Conversion、Automatic Kernel Generation三种关键技术。

Result: 在79个模板内核上评估，相比现有框架最高达7.1倍加速（平均3.1倍），降低代码复杂度，计算吞吐量和内存效率匹配或超专家调优性能。

Conclusion: SparStencil能有效将稀疏TCU用于科学模板计算，提升性能并降低复杂度。

Abstract: Sparse Tensor Cores offer exceptional performance gains for AI workloads by
exploiting structured 2:4 sparsity. However, their potential remains untapped
for core scientific workloads such as stencil computations, which exhibit
irregular sparsity patterns.This paper presents SparStencil, the first system
to retarget sparse TCUs for scientific stencil computations through structured
sparsity transformation. SparStencil introduces three key techniques: (1)
Adaptive Layout Morphing, which restructures stencil patterns into
staircase-aligned sparse matrices via a flatten-and-crush pipeline; (2)
Structured Sparsity Conversion, which formulates transformation as a graph
matching problem to ensure compatibility with 2:4 sparsity constraints; (3)
Automatic Kernel Generation, which compiles transformed stencils into optimized
sparse MMA kernels via layout search and table-driven memory mapping. Evaluated
on 79 stencil kernels spanning diverse scientific domains, SparStencil achieves
up to 7.1x speedup (3.1x on average) over state-of-the-art framework while
reducing code complexity and matching or exceeding expert-tuned performance in
both compute throughput and memory efficiency.

</details>


### [50] [Towards a better approach to the Vehicle Routing Problem](https://arxiv.org/abs/2506.23028)
*Souad Abdoune,Menouar Boulif*

Main category: cs.CE

TL;DR: 文章对车辆路径问题（VRP）进行全面概述，探讨其理论基础、经典模型局限及关键扩展，助于加深对VRP理解。


<details>
  <summary>Details</summary>
Motivation: VRP在物流管理中对运输效率、成本和服务质量影响大，且有多种变体以应对不同需求，需全面了解其发展和应用。

Method: 通过探索VRP理论基础、讨论经典模型局限、介绍关键扩展，系统回顾近期文献中的约束、目标和变体。

Result: 对VRP进行了全面梳理和总结。

Conclusion: 有助于加深对VRP的理解，凸显其在现代优化和决策过程中的持续演变和重要性。

Abstract: The Vehicle Routing Problem (VRP) is a fundamental challenge in logistics
management research, given its substantial influence on transportation
efficiency, cost minimization, and service quality. As a combinatorial
optimization problem, VRP plays a crucial role in a wide range of real world
applications, particularly in transportation, logistics, and delivery systems,
due to its diverse formulations and numerous extensions. Over the years,
researchers have introduced various VRP variants to address specific
operational constraints, emerging industry requirements and optimize specific
objectives, making it one of the most extensively studied problems in
operations research. This article provides a comprehensive overview of VRP by
exploring its theoretical foundations, discussing the limitations of its
classical model, and introducing its key extensions. By systematically
reviewing the diverse constraints, objectives, and variants examined in recent
literature, this study aims to contribute to a deeper understanding of VRP
while highlighting its ongoing evolution and relevance in modern optimization
and decision making processes.

</details>


### [51] [Data-Driven Multiscale Topology Optimization of Spinodoid Architected Materials with Controllable Anisotropy](https://arxiv.org/abs/2506.23420)
*Shiguang Deng,Doksoo Lee,Aaditya Chandrasekhar,Stefan Knapik,Liwei Wang,Horacio D. Espinosa,Wei Chen*

Main category: cs.CE

TL;DR: 本文提出数据驱动的多尺度拓扑优化框架解决旋节结构材料设计难题，且具有可解释性。


<details>
  <summary>Details</summary>
Motivation: 旋节结构材料设计受高维设计空间和计算能力限制，传统优化方法存在问题。

Method: 将旋节材料设计变量重写为神经网络参数以自动计算拓扑梯度，引入高斯过程代理旋节本构模型。

Result: 框架能消除重复计算均匀化，提高多尺度拓扑优化可扩展性，相比深度学习方法有明确物理见解。

Conclusion: 所提框架解决设计挑战，其可解释性有助于弥合数据驱动设计与机理理解的差距。

Abstract: Spinodoid architected materials have drawn significant attention due to their
unique nature in stochasticity, aperiodicity, and bi-continuity. Compared to
classic periodic truss-, beam- and plate-based lattice architectures,
spinodoids are insensitive to manufacturing defects, scalable for high
throughput production, functionally graded by tunable local properties, and
material failure resistant due to low-curvature morphology. However, the design
of spinodoids is often hindered by the curse of dimensionality with extremely
large design space of spinodoid types, material density, orientation,
continuity, and anisotropy. From a design optimization perspective, while
genetic algorithms are often beyond the reach of computing capacity,
gradient-based topology optimization is challenged by the intricate
mathematical derivation of gradient fields with respect to various spinodoid
parameters. To address such challenges, we propose a data-driven multiscale
topology optimization framework. Our framework reformulates the design
variables of spinodoid materials as the parameters of neural networks, enabling
automated computation of topological gradients. Additionally, it incorporates a
Gaussian Process surrogate for spinodoid constitutive models, eliminating the
need for repeated computational homogenization and enhancing the scalability of
multiscale topology optimization. Compared to 'black-box' deep learning
approaches, the proposed framework provides clear physical insights into
material distribution. It explicitly reveals why anisotropic spinodoids with
tailored orientations are favored in certain regions, while isotropic
spinodoids are more suitable elsewhere. This interpretability helps to bridge
the gap between data-driven design with mechanistic understanding.

</details>


### [52] [Data-Driven Multiscale Topology Optimization of Soft Functionally Graded Materials with Large Deformations](https://arxiv.org/abs/2506.23422)
*Shiguang Deng,Horacio D. Espinosa,Wei Chen*

Main category: cs.CE

TL;DR: 本文提出多尺度软功能梯度材料拓扑优化框架，含多项创新，引入插值方案和求解器，实验表明能产生不同拓扑设计。


<details>
  <summary>Details</summary>
Motivation: 软功能梯度材料设计因多尺度架构、多材料相和非线性特性而具挑战性，需自动化设计创新框架。

Method: 提出集成多项创新的拓扑优化框架，包括微结构重建算法、材料均匀化方法、神经网络拓扑优化和非线性灵敏度分析技术，引入能量插值方案和自适应牛顿 - 拉夫逊求解器。

Result: 该框架产生与线弹性下不同的、具空间变化微结构的拓扑设计。

Conclusion: 所提框架能有效用于多尺度软功能梯度材料设计，产生独特拓扑设计。

Abstract: Functionally Graded Materials (FGMs) made of soft constituents have emerged
as promising material-structure systems in potential applications across many
engineering disciplines, such as soft robots, actuators, energy harvesting, and
tissue engineering. Designing such systems remains challenging due to their
multiscale architectures, multiple material phases, and inherent material and
geometric nonlinearities. The focus of this paper is to propose a general
topology optimization framework that automates the design innovation of
multiscale soft FGMs exhibiting nonlinear material behaviors under large
deformations. Our proposed topology optimization framework integrates several
key innovations: (1) a novel microstructure reconstruction algorithm that
generates composite architecture materials from a reduced design space using
physically interpretable parameters; (2) a new material homogenization approach
that estimates effective properties by combining the stored energy functions of
multiple soft constituents; (3) a neural network-based topology optimization
that incorporates data-driven material surrogates to enable bottom-up,
simultaneous optimization of material and structure; and (4) a generic
nonlinear sensitivity analysis technique that computes design sensitivities
numerically without requiring explicit gradient derivation. To enhance the
convergence of the nonlinear equilibrium equations amid topology optimization,
we introduce an energy interpolation scheme and employ a Newton-Raphson solver
with adaptive step sizes and convergence criteria. Numerical experiments show
that the proposed framework produces distinct topological designs, different
from those obtained under linear elasticity, with spatially varying
microstructures.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [53] [GaussMaster: An LLM-based Database Copilot System](https://arxiv.org/abs/2506.23322)
*Wei Zhou,Ji Sun,Xuanhe Zhou,Guoliang Li,Luyang Liu,Hao Wu,Tianyuan Wang*

Main category: cs.DB

TL;DR: 现有自主数据库平台能力有限，GaussMaster提出基于LLM的数据库副驾驶系统，能自动处理数据库维护，在银行等场景实现多场景零人工干预。


<details>
  <summary>Details</summary>
Motivation: 现有自主数据库平台能力有限，需人工干预进行全面数据库维护，为减轻DBA工作量，提升数据库维护效率。

Method: 引入基于LLM的数据库副驾驶系统，分析数百个指标和日志，采用思维树方法确定根本原因并调用工具解决问题。

Result: 在银行业等现实场景成功实现，34个以上数据库维护场景达到零人工干预。

Conclusion: GaussMaster在数据库维护任务中有显著改进，代码公开在https://gitcode.com/opengauss/openGauss - GaussMaster。

Abstract: In the financial industry, data is the lifeblood of operations, and DBAs
shoulder significant responsibilities for SQL tuning, database deployment,
diagnosis, and service repair. In recent years, both database vendors and
customers have increasingly turned to autonomous database platforms in an
effort to alleviate the heavy workload of DBAs. However, existing autonomous
database platforms are limited in their capabilities, primarily addressing
single-point issues such as NL2SQL, anomaly detection, and SQL tuning. Manual
intervention remains a necessity for comprehensive database maintenance.
GaussMaster aims to revolutionize this landscape by introducing an LLM-based
database copilot system. This innovative solution is designed not only to
assist developers in writing efficient SQL queries but also to provide
comprehensive care for database services. When database instances exhibit
abnormal behavior, GaussMaster is capable of orchestrating the entire
maintenance process automatically. It achieves this by analyzing hundreds of
metrics and logs, employing a Tree-of-thought approach to identify root causes,
and invoking appropriate tools to resolve issues. We have successfully
implemented GaussMaster in real-world scenarios, such as the banking industry,
where it has achieved zero human intervention for over 34 database maintenance
scenarios. In this paper, we present significant improvements in these tasks
with code at https://gitcode.com/opengauss/openGauss-GaussMaster.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [54] [Libra: Synergizing CUDA and Tensor Cores for High-Performance Sparse Matrix Multiplication](https://arxiv.org/abs/2506.22714)
*Jinliang Shi,Shigang Li,Youxuan Xu,Xueying Wang,Rongtian Fu,Zhi Ma,Tong Wu*

Main category: cs.DC

TL;DR: 本文提出Libra方法，通过协同CUDA和Tensor核心实现稀疏矩阵乘法最优性能，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现代加速器的Tensor核心和CUDA核心各有局限，单独使用资源会导致稀疏矩阵乘法性能不佳。

Method: 提出2D感知工作负载分配策略，结合Tensor核心高性能和CUDA核心低计算冗余；并对异构计算进行系统优化。

Result: 在H100和RTX 4090 GPU上实验，Libra比DTC - SpMM平均快3.1倍（最高9.23倍），端到端GNN应用快2.9倍（最高3.9倍）。

Conclusion: Libra通过充分利用GPU上的异构计算资源，为稀疏算子加速开辟了新视角。

Abstract: Sparse matrix multiplication operators (i.e., SpMM and SDDMM) are widely used
in deep learning and scientific computing. Modern accelerators are commonly
equipped with Tensor cores and CUDA cores to accelerate sparse operators. The
former brings superior computing power but only for structured matrix
multiplication, while the latter has relatively lower performance but with
higher programming flexibility. In this work, we discover that utilizing one
resource alone leads to inferior performance for sparse matrix multiplication,
due to their respective limitations. To this end, we propose Libra, a
systematic approach that enables synergistic computation between CUDA and
Tensor cores to achieve the best performance for sparse matrix multiplication.
Specifically, we propose a 2D-aware workload distribution strategy to find out
the sweet point of task mapping for different sparse operators, leveraging both
the high performance of Tensor cores and the low computational redundancy on
CUDA cores. In addition, Libra incorporates systematic optimizations for
heterogeneous computing, including hybrid load-balancing, finely optimized
kernel implementations, and GPU-accelerated preprocessing. Extensive
experimental results on H100 and RTX 4090 GPUs show that Libra outperforms the
state-of-the-art by on average 3.1x (up to 9.23x) over DTC-SpMM and 2.9x (up to
3.9x) for end-to-end GNN applications. Libra opens up a new perspective for
sparse operator acceleration by fully exploiting the heterogeneous computing
resources on GPUs.

</details>


### [55] [Not All Water Consumption Is Equal: A Water Stress Weighted Metric for Sustainable Computing](https://arxiv.org/abs/2506.22773)
*Yanran Wu,Inez Hua,Yi Ding*

Main category: cs.DC

TL;DR: 提出SCARF框架评估计算的水影响，考虑时空水压力变化，通过案例研究展示优化时空选择减少水影响的机会。


<details>
  <summary>Details</summary>
Motivation: 当前水影响评估常忽略水压力更严重的时空情况，为填补这一空白。

Method: 提出SCARF框架，计算考虑消耗体积和当地随时间水压力的调整水影响（AWI）指标。

Result: 通过三个案例研究展示了优化位置和时间选择以减少水影响的隐藏机会。

Conclusion: 为水可持续计算铺平道路，代码开源。

Abstract: Water consumption is an increasingly critical dimension of computing
sustainability, especially as AI workloads rapidly scale. However, current
water impact assessment often overlooks where and when water stress is more
severe. To fill in this gap, we present SCARF, the first general framework that
evaluates water impact of computing by factoring in both spatial and temporal
variations in water stress. SCARF calculates an Adjusted Water Impact (AWI)
metric that considers both consumption volume and local water stress over time.
Through three case studies on LLM serving, datacenters, and semiconductor
fabrication plants, we show the hidden opportunities for reducing water impact
by optimizing location and time choices, paving the way for water-sustainable
computing. The code is available at https://github.com/jojacola/SCARF.

</details>


### [56] [TriADA: Massively Parallel Trilinear Matrix-by-Tensor Multiply-Add Algorithm and Device Architecture for the Acceleration of 3D Discrete Transformations](https://arxiv.org/abs/2506.22818)
*Stanislav Sedukhin,Yoichi Tomioka,Kazuya Matsumoto,Yuichi Okuyama*

Main category: cs.DC

TL;DR: 本文针对多线性变换在高性能计算和人工智能中计算与内存需求高、能耗大的问题，提出Trilinear算法和TriADA架构，能加速多线性张量运算。


<details>
  <summary>Details</summary>
Motivation: 多线性变换计算和内存需求高、能耗大，限制其在高性能计算和人工智能中的广泛应用。

Method: 提出Trilinear算法和TriADA架构，包括并行低秩算法、新的GEMM内核、分布式3D网络和弹性稀疏外积方法。

Result: TriADA能以超立方算术复杂度在线性时间步内执行多种三线性变换。

Conclusion: TriADA的并行、可扩展和节能架构适合加速AI和HPC工作负载中最具挑战性的多线性张量运算。

Abstract: Multilinear transformations are key in high-performance computing (HPC) and
artificial intelligence (AI) workloads, where data is represented as tensors.
However, their high computational and memory demands, which grow with
dimensionality, often slow down critical tasks. Moreover, scaling computation
by enlarging the number of parallel processing units substantially increases
energy consumption, limiting widespread adoption, especially for sparse data,
which is common in HPC and AI applications. This paper introduces the Trilinear
Algorithm and isomorphic to algorithm Device Architecture (TriADA) to address
these challenges with the following innovations: (1) a massively parallel,
low-rank algorithm for computing a family of trilinear (3D) discrete orthogonal
transformations (3D-DXTs), which is a special case of the more general 3-mode
matrix-by-tensor multiplication (3D-GEMT); (2) a new outer-product-based GEMM
kernel with decoupled streaming active memory, specially designed to accelerate
3D-GEMT operation; (3) an isomorphic to the proposed algorithm, fully
distributed 3D network of mesh interconnected processing elements or cells with
a coordinate-free, data-driven local processing activity, which is independent
of problem size; (4) an elastic sparse outer-product (ESOP) method that avoids
unnecessary computing and communication operations with zero-valued operands,
thereby enhancing energy efficiency, computational accuracy, and stability.
TriADA is capable of performing a variety of trilinear transformations with
hypercubic arithmetic complexity in a linear number of time-steps. The
massively parallel, scalable, and energy-efficient architecture of TriADA is
ideal for accelerating multilinear tensor operations, which are the most
demanding parts of AI and HPC workloads.

</details>


### [57] [Performance Measurements in the AI-Centric Computing Continuum Systems](https://arxiv.org/abs/2506.22884)
*Praveen Kumar Donta,Qiyang Zhang,Schahram Dustdar*

Main category: cs.DC

TL;DR: 八十年间计算范式转变形成DCC，生成式AI加剧资源需求，需更新性能指标，本文回顾常用指标、讨论新兴维度并给出选指标的标准。


<details>
  <summary>Details</summary>
Motivation: 计算范式转变和生成式AI发展使传统性能指标需更新以适应新计算需求和应用要求，准确测量对系统设计和用户有益。

Method: 回顾DCC和物联网环境常用指标，讨论新兴性能维度，给出选择合适指标的标准和考虑因素。

Result: 明确了常用指标、新兴性能维度，以及选择指标的相关标准和考虑因素。

Conclusion: 研究旨在启发该关键领域未来的研发。

Abstract: Over the Eight decades, computing paradigms have shifted from large,
centralized systems to compact, distributed architectures, leading to the rise
of the Distributed Computing Continuum (DCC). In this model, multiple layers
such as cloud, edge, Internet of Things (IoT), and mobile platforms work
together to support a wide range of applications. Recently, the emergence of
Generative AI and large language models has further intensified the demand for
computational resources across this continuum. Although traditional performance
metrics have provided a solid foundation, they need to be revisited and
expanded to keep pace with changing computational demands and application
requirements. Accurate performance measurements benefit both system designers
and users by supporting improvements in efficiency and promoting alignment with
system goals. In this context, we review commonly used metrics in DCC and IoT
environments. We also discuss emerging performance dimensions that address
evolving computing needs, such as sustainability, energy efficiency, and system
observability. We also outline criteria and considerations for selecting
appropriate metrics, aiming to inspire future research and development in this
critical area.

</details>


### [58] [FastSet: Parallel Claim Settlement](https://arxiv.org/abs/2506.23395)
*Xiaohong Chen,Grigore Rosu*

Main category: cs.DC

TL;DR: 介绍了基于参与者的分布式协议FastSet，用于去中心化金融和结算，放弃强一致性但保留多数区块链优势且证明正确。


<details>
  <summary>Details</summary>
Motivation: 设计一个适用于去中心化金融和结算的分布式协议，在一定程度上借鉴区块链的特点。

Method: 账户持有者通过提出声明进行合作，声明经签名后广播给验证者网络，验证者复制账户全局状态且无需相互通信。

Result: 证明了该协议尽管具有高度并行性，但仍是正确的。

Conclusion: FastSet协议放弃强一致性要求，仍能保留许多区块链的优势。

Abstract: FastSet is an actor-based distributed protocol for decentralized finance and
settlement, which is inspired from blockchains. Account holders cooperate by
making claims, which can include payments, holding and transferring assets,
accessing and updating shared data, medical records, digital identity, and
mathematical theorems, among many others. The claims are signed by their owners
and are broadcast to a decentralized network of validators, which validate and
settle them. Validators replicate the global state of the accounts and need not
communicate with each other. In sharp contrast to blockchains, strong
consistency is purposely given up as a requirement. Yet, many if not most of
the blockchain benefits are preserved. The protocol is proved to be correct,
despite its massively parallel nature.

</details>


### [59] [Towards Building Private LLMs: Exploring Multi-Node Expert Parallelism on Apple Silicon for Mixture-of-Experts Large Language Model](https://arxiv.org/abs/2506.23635)
*Mu-Chi Chen,Po-Hsuan Huang,Xiangrui Ke,Chia-Heng Tu,Chun Jason Xue,Shih-Hao Hung*

Main category: cs.DC

TL;DR: 本文针对构建私有大语言模型系统的成本和可扩展性挑战，用Mac Studio集群加速DBRX模型，分析性能并优化，成本效率超英伟达H100 GPU超算，还构建性能模型。


<details>
  <summary>Details</summary>
Motivation: 解决苹果智能构建个人或小团体服务的私有大语言模型系统时遇到的成本和可扩展性挑战。

Method: 搭建含苹果M2 Ultra芯片的Mac Studio集群来托管和加速预训练的DBRX模型，进行性能分析，开发优化方案消除内存管理开销，构建性能模型。

Result: 模型专家在2 - 4个机器节点上并行执行显著减少推理时间；专家计算时间与输出交换通信时间相当；Mac Studio集群成本效率比英伟达H100 GPU超算高1.15倍。

Conclusion: 开发的优化方案有效，构建的性能模型为设计私有大语言模型系统提供有价值见解。

Abstract: Large Language Models (LLMs) have revolutionized Artificial Intelligence (AI)
with significant advancements such as OpenAI's ChatGPT, Meta's Llama, and
Databricks' DBRX. This paper addresses the cost and scalability challenges
encountered when constructing private LLM systems for personal or small group
services, as aimed by Apple Intelligence. A Mac Studio cluster with Apple's M2
Ultra chips is established as a cost-efficient solution to host and accelerate
the pretrained DBRX model with the Mixture-of-Experts (MoE) architecture. Our
performance analysis reveal that parallel execution of the model's experts
across two to four machine nodes significantly reduces inference time. We find
that computation time for the experts is comparable to the communication time
for exchanging their outputs, emphasizing the importance of network latency
over bandwidth. We also observe significant management overhead due to Apple
software stack's memory management logic. Based on these findings, we develop
optimization schemes to eliminate the memory management overhead. As a result,
the Mac Studio cluster is 1.15 times more cost-efficient than the
state-of-the-art AI supercomputer with NVIDIA H100 GPUs. In addition, we
construct a performance model to estimate system performance under varying
configurations, and the model provides valuable insights for designing private
LLM systems.

</details>


### [60] [Large-scale Neural Network Quantum States for ab initio Quantum Chemistry Simulations on Fugaku](https://arxiv.org/abs/2506.23809)
*Hongtao Xu,Zibo Wu,Mingzhen Li,Weile Jia*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Solving quantum many-body problems is one of the fundamental challenges in
quantum chemistry. While neural network quantum states (NQS) have emerged as a
promising computational tool, its training process incurs exponentially growing
computational demands, becoming prohibitively expensive for large-scale
molecular systems and creating fundamental scalability barriers for real-world
applications. To address above challenges, we present \ours, a high-performance
NQS training framework for \textit{ab initio} electronic structure
calculations. First, we propose a scalable sampling parallelism strategy with
multi-layers workload division and hybrid sampling scheme, which break the
scalability barriers for large-scale NQS training. Then, we introduce
multi-level parallelism local energy parallelism, enabling more efficient local
energy computation. Last, we employ cache-centric optimization for
transformer-based \textit{ansatz} and incorporate it with sampling parallelism
strategy, which further speedup up the NQS training and achieve stable memory
footprint at scale. Experiments demonstrate that \ours accelerate NQS training
with up to 8.41x speedup and attains a parallel efficiency up to 95.8\% when
scaling to 1,536 nodes.

</details>


### [61] [QPART: Adaptive Model Quantization and Dynamic Workload Balancing for Accuracy-aware Edge Inference](https://arxiv.org/abs/2506.23934)
*Xiangchen Li,Saeid Ghafouri,Bo Ji,Hans Vandierendonck,Deepu John,Dimitrios S. Nikolopoulos*

Main category: cs.DC

TL;DR: 随着机器学习推理移向边缘设备，提出一种准确率感知和负载均衡推理系统，优化量化层位宽和分区点，模拟结果显示减少时间和功耗。


<details>
  <summary>Details</summary>
Motivation: 机器学习推理移至边缘设备，需适应不同计算能力、硬件和内存约束，传统预训练模型不灵活，需要更具成本效益和鲁棒性的方法。

Method: 提出准确率感知和负载均衡推理系统，集成联合模型量化和推理分区；引入新优化框架，优化层量化位宽和分区点，通过准确率下降指标考虑任务准确率要求。

Result: 模拟结果显示总体时间和功耗大幅降低，计算负载降低超80%，准确率下降控制在1%以内。

Conclusion: 该方法在边缘设备机器学习推理中能有效减少时间和功耗，控制准确率下降，具有创新性和实用性。

Abstract: As machine learning inferences increasingly move to edge devices, adapting to
diverse computational capabilities, hardware, and memory constraints becomes
more critical. Instead of relying on a pre-trained model fixed for all future
inference queries across diverse edge devices, we argue that planning an
inference pattern with a request-specific model tailored to the device's
computational capacity, accuracy requirements, and time constraints is more
cost-efficient and robust to diverse scenarios. To this end, we propose an
accuracy-aware and workload-balanced inference system that integrates joint
model quantization and inference partitioning. In this approach, the server
dynamically responds to inference queries by sending a quantized model and
adaptively sharing the inference workload with the device. Meanwhile, the
device's computational power, channel capacity, and accuracy requirements are
considered when deciding.
  Furthermore, we introduce a new optimization framework for the inference
system, incorporating joint model quantization and partitioning. Our approach
optimizes layer-wise quantization bit width and partition points to minimize
time consumption and cost while accounting for varying accuracy requirements of
tasks through an accuracy degradation metric in our optimization model. To our
knowledge, this work represents the first exploration of optimizing
quantization layer-wise bit-width in the inference serving system, by
introducing theoretical measurement of accuracy degradation. Simulation results
demonstrate a substantial reduction in overall time and power consumption, with
computation payloads decreasing by over 80% and accuracy degradation kept below
1%.

</details>


### [62] [Agent.xpu: Efficient Scheduling of Agentic LLM Workloads on Heterogeneous SoC](https://arxiv.org/abs/2506.24045)
*Xinming Wei,Jiahao Zhang,Haoran Li,Jiayu Chen,Rui Qu,Maoliang Li,Xiang Chen,Guojie Luo*

Main category: cs.DC

TL;DR: 现有设备端大语言模型引擎难以处理反应式和主动式任务，本文提出 Agent.xpu 系统，经评估其在反应式任务上降低延迟，在主动式任务上提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有设备端大语言模型引擎无法有效管理反应式和主动式任务的并发冲突请求。

Method: 通过离线分析构建异构执行图，运行时使用在线调度器实现细粒度内核级抢占，采用空闲感知内核回填和带宽感知调度。

Result: 在 Intel Core Ultra SoC 上评估显示，Agent.xpu 使反应式任务延迟降低 4.6 倍，主动式任务吞吐量提升 1.6 - 6.8 倍。

Conclusion: Agent.xpu 能有效处理设备端大语言模型工作负载，提升性能。

Abstract: The proliferation of agentic Large Language Models (LLMs) on personal devices
introduces a new class of workloads characterized by a dichotomy of objectives.
Reactive tasks, initiated by users, demand immediate, low-latency responses,
while proactive tasks operate invisibly and prioritize throughput. Existing
on-device LLM engines, designed for isolated inferences, fail to efficiently
manage these concurrent and conflicting requests on consumer-grade
heterogeneous SoCs with CPU, integrated GPU, and NPU. This paper introduces
Agent.xpu, an efficient serving system for agentic LLM workloads on
memory-unified heterogeneous SoCs. With dedicated offline profiling, Agent.xpu
first constructs a heterogeneous execution graph, which fuses and chunks model
kernels for affinity-guided, elastic accelerator mapping with predictive kernel
annotation. At runtime, its online scheduler enables fine-grained, kernel-level
preemption to guarantee the responsiveness of reactive tasks. To maximize SoC
utilization, it adopts slack-aware kernel backfill to opportunistically append
proactive tasks, and mitigates NPU-iGPU contention via bandwidth-aware
dispatch. Evaluation on an Intel Core Ultra SoC shows that Agent.xpu achieves
4.6$\times$ lower latency for reactive tasks and sustains
1.6$\times$-6.8$\times$ higher throughput for proactive tasks compared to
state-of-the-art inference engines.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [63] [On Fine-Grained Distinct Element Estimation](https://arxiv.org/abs/2506.22608)
*Ilias Diakonikolas,Daniel M. Kane,Jasper C. H. Lee,Thanasis Pittas,David P. Woodruff,Samson Zhou*

Main category: cs.DS

TL;DR: 研究分布式不同元素估计问题，引入基于碰撞数的新参数化，设计低通信量协议，改进算法并给出下界，还考虑流算法，解释统计问题实践中可高效解决的原因。


<details>
  <summary>Details</summary>
Motivation: 现有分布式不同元素估计的最坏情况界结果依赖的假设在实践中可能不成立，需改进。

Method: 引入基于两两碰撞数的新参数化，设计新协议，在不同假设下改进算法并给出匹配下界。

Result: 设计的协议使用更少通信量，在碰撞数较小时突破先前下界，建立新的复杂度衡量指标。

Conclusion: 新参数C是该问题的紧密复杂度衡量指标，解释了有已知困难结果的统计问题在实践中能高效解决的原因。

Abstract: We study the problem of distributed distinct element estimation, where
$\alpha$ servers each receive a subset of a universe $[n]$ and aim to compute a
$(1+\varepsilon)$-approximation to the number of distinct elements using
minimal communication. While prior work establishes a worst-case bound of
$\Theta\left(\alpha\log n+\frac{\alpha}{\varepsilon^2}\right)$ bits, these
results rely on assumptions that may not hold in practice. We introduce a new
parameterization based on the number $C = \frac{\beta}{\varepsilon^2}$ of
pairwise collisions, i.e., instances where the same element appears on multiple
servers, and design a protocol that uses only $\mathcal{O}\left(\alpha\log
n+\frac{\sqrt{\beta}}{\varepsilon^2} \log n\right)$ bits, breaking previous
lower bounds when $C$ is small. We further improve our algorithm under
assumptions on the number of distinct elements or collisions and provide
matching lower bounds in all regimes, establishing $C$ as a tight complexity
measure for the problem. Finally, we consider streaming algorithms for distinct
element estimation parameterized by the number of items with frequency larger
than $1$. Overall, our results offer insight into why statistical problems with
known hardness results can be efficiently solved in practice.

</details>


### [64] [On Finding $\ell$-th Smallest Perfect Matchings](https://arxiv.org/abs/2506.22619)
*Nicolas El Maalouly,Sebastian Haslebacher,Adrian Taubner,Lasse Wulf*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Given an undirected weighted graph $G$ and an integer $k$, Exact-Weight
Perfect Matching (EWPM) is the problem of finding a perfect matching of weight
exactly $k$ in $G$. In this paper, we study EWPM and its variants. The EWPM
problem is famous, since in the case of unary encoded weights, Mulmuley,
Vazirani, and Vazirani showed almost 40 years ago that the problem can be
solved in randomized polynomial time. However, up to this date no
derandomization is known.
  Our first result is a simple deterministic algorithm for EWPM that runs in
time $n^{O(\ell)}$, where $\ell$ is the number of distinct weights that perfect
matchings in $G$ can take. In fact, we show how to find an $\ell$-th smallest
perfect matching in any weighted graph (even if the weights are encoded in
binary, in which case EWPM in general is known to be NP-complete) in time
$n^{O(\ell)}$ for any integer $\ell$. Similar next-to-optimal variants have
also been studied recently for the shortest path problem.
  For our second result, we extend the list of problems that are known to be
equivalent to EWPM. We show that EWPM is equivalent under a weight-preserving
reduction to the Exact Cycle Sum problem (ECS) in undirected graphs with a
conservative (i.e. no negative cycles) weight function. To the best of our
knowledge, we are the first to study this problem. As a consequence, the latter
problem is contained in RP if the weights are encoded in unary. Finally, we
identify a special case of EWPM, called BCPM, which was recently studied by El
Maalouly, Steiner and Wulf. We show that BCPM is equivalent under a
weight-preserving transformation to another problem recently studied by
Schlotter and Seb\H{o} as well as Geelen and Kapadia: the Shortest Odd Cycle
problem (SOC) in undirected graphs with conservative weights.

</details>


### [65] [Counting distinct (non-)crossing substrings](https://arxiv.org/abs/2506.22728)
*Haruki Umezaki,Hiroki Shibata,Dominik Köppl,Yuto Nakashima,Shunsuke Inenaga,Hideo Bannai*

Main category: cs.DS

TL;DR: 本文提出新算法，能在O(n)总时间内计算一般有序字母表下的C(w,k)和线性可排序字母表下的N(w,k)，优于原算法。


<details>
  <summary>Details</summary>
Motivation: 原教科书算法计算C(w,k)和N(w,k)在所有位置时需O(n^2)时间，且针对常量大小字母表，本文旨在优化。

Method: 提出新的算法。

Result: 得到能在O(n)总时间内计算C(w,k)（一般有序字母表）和N(w,k)（线性可排序字母表）的算法。

Conclusion: 新算法在时间复杂度上优于原教科书算法。

Abstract: Let $w$ be a string of length $n$. The problem of counting factors crossing a
position - Problem 64 from the textbook ``125 Problems in Text Algorithms''
[Crochemore, Leqroc, and Rytter, 2021], asks to count the number
$\mathcal{C}(w,k)$ (resp. $\mathcal{N}(w,k)$) of distinct substrings in $w$
that have occurrences containing (resp. not containing) a position $k$ in $w$.
The solutions provided in their textbook compute $\mathcal{C}(w,k)$ and
$\mathcal{N}(w,k)$ in $O(n)$ time for a single position $k$ in $w$, and thus a
direct application would require $O(n^2)$ time for all positions $k = 1,
\ldots, n$ in $w$. Their solution is designed for constant-size alphabets. In
this paper, we present new algorithms which compute $\mathcal{C}(w,k)$ in
$O(n)$ total time for general ordered alphabets, and $\mathcal{N}(w,k)$ in
$O(n)$ total time for linearly sortable alphabets, for all positions $k = 1,
\ldots, n$ in $w$.

</details>


### [66] [Tight Additive Sensitivity on LZ-style Compressors and String Attractors](https://arxiv.org/abs/2506.22778)
*Yuto Fujie,Hiroki Shibata,Yuto Nakashima,Shunsuke Inenaga*

Main category: cs.DS

TL;DR: 本文给出了字符串重复度度量的最坏情况加性敏感度的上界，并与已知下界匹配。


<details>
  <summary>Details</summary>
Motivation: 研究字符串重复度度量在单字符编辑操作下的最坏情况加性敏感度。

Method: 通过分析和推导得出不同字符串重复度度量的最坏情况加性敏感度的上界。

Result: 给出了最小字符串吸引子大小γ和最小双向方案大小b的O(√n)上界，与已知下界匹配；还给出了Lempel - Ziv家族的最坏情况加性敏感度的匹配上下界。

Conclusion: 得到了多种字符串重复度度量的最坏情况加性敏感度的精确上下界。

Abstract: The worst-case additive sensitivity of a string repetitiveness measure $c$ is
defined to be the largest difference between $c(w)$ and $c(w')$, where $w$ is a
string of length $n$ and $w'$ is a string that can be obtained by performing a
single-character edit operation on $w$. We present $O(\sqrt{n})$ upper bounds
for the worst-case additive sensitivity of the smallest string attractor size
$\gamma$ and the smallest bidirectional scheme size $b$, which match the known
lower bounds $\Omega(\sqrt{n})$ for $\gamma$ and $b$ [Akagi et al. 2023].
Further, we present matching upper and lower bounds for the worst-case additive
sensitivity of the Lempel-Ziv family - $\Theta(n^{\frac{2}{3}})$ for LZSS and
LZ-End, and $\Theta(n)$ for LZ78.

</details>


### [67] [Global Predecessor Indexing: Avoiding Binary Search in Weighted Job Scheduling](https://arxiv.org/abs/2506.22922)
*Amit Joshi*

Main category: cs.DS

TL;DR: 提出改进加权作业调度（WJS）问题的解决方案，引入GPI技术，结合线性排序可得O(n)复杂度解，比经典方案更优。


<details>
  <summary>Details</summary>
Motivation: 经典动态规划解决WJS问题因比较排序和二分查找存在瓶颈，需改进。

Method: 引入全局前驱索引（GPI）多阶段预处理技术，通过双指针线性时间遍历计算所有作业的前驱，结合线性排序。

Result: 结合线性排序时得到O(n)复杂度的完整解决方案，即使使用基于比较的排序，在实践中也显著优于经典解决方案。

Conclusion: GPI技术可有效解决WJS问题，避免重复二分查找，提高效率。

Abstract: We present an improved solution to the Weighted Job Scheduling (WJS) problem.
While the classical dynamic programming (DP) solution runs in $O(n \log(n))$
time due to comparison-based sorting and per-job binary search, we eliminate
the binary search bottleneck. In its place, we introduce a novel multi-phase
preprocessing technique called Global Predecessor Indexing (GPI), which
computes the latest non-overlapping job (i.e., the predecessor) for all jobs
via a two-pointer linear-time pass. GPI enables direct use in the classical DP
recurrence. When combined with linear-time sorting, GPI yields a complete
$O(n)$ solution. Even with comparison-based sorting, GPI significantly
outperforms the classical solution in practice by avoiding repeated binary
searches. Keywords: Weighted Job Scheduling, Interval Scheduling, Dynamic
Programming, Linear Sorting, Two Pointers, Preprocessing

</details>


### [68] [Near-Optimal Vertex Fault-Tolerant Labels for Steiner Connectivity](https://arxiv.org/abs/2506.23215)
*Koustav Bhanja,Asaf Petruschka*

Main category: cs.DS

TL;DR: 提出一种紧凑标记方案，用于确定图中指定终端集在任意f个（或更少）顶点故障后是否保持连通，该方案标签长度近最优。


<details>
  <summary>Details</summary>
Motivation: 已有全局连通性标记方案，需针对一般终端集设计标记方案。

Method: 使用Duan和Pettie分解定理生成的结构良好的Steiner树，绕过Nagamochi - Ibaraki稀疏化。

Result: 实现了对于一般终端集U，标签长度为|U|^{1 - 1/f} · poly(f, log n)，且近最优。

Conclusion: 所提出的标记方案能有效解决图中指定终端集在顶点故障后的连通性判断问题，标签长度近最优。

Abstract: We present a compact labeling scheme for determining whether a designated set
of terminals in a graph remains connected after any $f$ (or less) vertex
failures occur. An $f$-FT Steiner connectivity labeling scheme for an
$n$-vertex graph $G=(V,E)$ with terminal set $U \subseteq V$ provides labels to
the vertices of $G$, such that given only the labels of any subset $F \subseteq
V$ with $|F| \leq f$, one can determine if $U$ remains connected in $G-F$. The
main complexity measure is the maximum label length.
  The special case $U=V$ of global connectivity has been recently studied by
Jiang, Parter, and Petruschka, who provided labels of $n^{1-1/f} \cdot
\mathrm{poly}(f,\log n)$ bits. This is near-optimal (up to
$\mathrm{poly}(f,\log n)$ factors) by a lower bound of Long, Pettie and
Saranurak. Our scheme achieves labels of $|U|^{1-1/f} \cdot \mathrm{poly}(f,
\log n)$ for general $U \subseteq V$, which is near-optimal for any given size
$|U|$ of the terminal set. To handle terminal sets, our approach differs from
Jiang et al. We use a well-structured Steiner tree for $U$ produced by a
decomposition theorem of Duan and Pettie, and bypass the need for
Nagamochi-Ibaraki sparsification.

</details>


### [69] [Parameterized Critical Node Cut Revisited](https://arxiv.org/abs/2506.23363)
*Dušan Knop,Nikolaos Melissinos,Manolis Vasilakis*

Main category: cs.DS

TL;DR: 研究Critical Node Cut问题在不同结构参数下的参数化复杂度，加强硬度结果，找到使问题固定参数可处理的参数，开发FPT近似方案，证明参数化时无多项式核。


<details>
  <summary>Details</summary>
Motivation: 该问题推广了Vertex Cover，在网络设计、流行病学和社交网络分析中有应用，需研究其参数化复杂度。

Method: 证明参数组合下的W[1]-硬度；确定使问题固定参数可处理的结构参数；利用Lampis技术开发FPT近似方案；证明参数化时不存在多项式核。

Result: 加强现有硬度结果；确定三个使问题固定参数可处理的结构参数；开发FPT近似方案；证明参数化时无多项式核。

Conclusion: 研究结果显著完善了Critical Node Cut已知的复杂度格局。

Abstract: Given a graph $G$ and integers $k, x \geq 0$, the Critical Node Cut problem
asks whether it is possible to delete at most $k$ vertices from $G$ such that
the number of remaining pairs of connected vertices is at most $x$. This
problem generalizes Vertex Cover (when $x = 0$), and has applications in
network design, epidemiology, and social network analysis. We investigate the
parameterized complexity of Critical Node Cut under various structural
parameters. We first significantly strengthen existing hardness results by
proving W[1]-hardness even when parameterized by the combined parameter $k +
\mathrm{fes} + \Delta + \mathrm{pw}$, where $\mathrm{fes}$ is the feedback edge
set number, $\Delta$ the maximum degree, and $\mathrm{pw}$ the pathwidth of the
input graph. We then identify three structural parameters--max-leaf number,
vertex integrity, and modular-width--that render the problem fixed-parameter
tractable. Furthermore, leveraging a technique introduced by Lampis [ICALP
'14], we develop an FPT approximation scheme that, for any $\varepsilon > 0$,
computes a $(1+\varepsilon)$-approximate solution in time $(\mathrm{tw} /
\varepsilon)^{\mathcal{O}(\mathrm{tw})} n^{\mathcal{O}(1)}$, where
$\mathrm{tw}$ denotes the treewidth of the input graph. Finally, we show that
Critical Node Cut does not admit a polynomial kernel when parameterized by
vertex cover number, unless standard complexity assumptions fail. Overall, our
results significantly sharpen the known complexity landscape of Critical Node
Cut.

</details>


### [70] [Planar Multiway Cut with Terminals on Few Faces](https://arxiv.org/abs/2506.23399)
*Sukanya Pandey,Erik Jan van Leeuwen*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider the \textsc{Edge Multiway Cut} problem on planar graphs. It is
known that this can be solved in $n^{O(\sqrt{t})}$ time [Klein, Marx, ICALP
2012] and not in $n^{o(\sqrt{t})}$ time under the Exponential Time Hypothesis
[Marx, ICALP 2012], where $t$ is the number of terminals. A stronger parameter
is the number $k$ of faces of the planar graph that jointly cover all
terminals. For the related {\sc Steiner Tree} problem, an $n^{O(\sqrt{k})}$
time algorithm was recently shown [Kisfaludi-Bak et al., SODA 2019]. By a
completely different approach, we prove in this paper that \textsc{Edge
Multiway Cut} can be solved in $n^{O(\sqrt{k})}$ time as well.
  Our approach employs several major concepts on planar graphs, including
homotopy and sphere-cut decomposition. We also mix a global treewidth dynamic
program with a Dreyfus-Wagner style dynamic program to locally deal with large
numbers of terminals.

</details>


### [71] [Efficient Resource Allocation under Adversary Attacks: A Decomposition-Based Approach](https://arxiv.org/abs/2506.23442)
*Mansoor Davoodi,Setareh Maghsudi*

Main category: cs.DS

TL;DR: 针对网络在未知对抗攻击下的资源分配问题，提出分解法求解双目标优化问题，理论证明收敛性，模拟显示方法有效。


<details>
  <summary>Details</summary>
Motivation: 解决网络在持续且统计未知的对抗攻击下有限资源分配问题，实现最小化系统总损害和降低资源分配与转移成本。

Method: 将问题建模为双目标优化问题，提出基于分解的解决方案，结合机会约束规划和网络流优化，将问题拆分为两个子问题。

Result: 理论证明方法能收敛到有对手完整统计信息时的最优解，模拟显示相比三种基准策略，能有效学习对抗模式，大幅减少损害和运营成本。

Conclusion: 所提方法可有效应对网络在未知对抗攻击下的资源分配问题，在降低损害和成本方面表现良好。

Abstract: We address the problem of allocating limited resources in a network under
persistent yet statistically unknown adversarial attacks. Each node in the
network may be degraded, but not fully disabled, depending on its available
defensive resources. The objective is twofold: to minimize total system damage
and to reduce cumulative resource allocation and transfer costs over time. We
model this challenge as a bi-objective optimization problem and propose a
decomposition-based solution that integrates chance-constrained programming
with network flow optimization. The framework separates the problem into two
interrelated subproblems: determining optimal node-level allocations across
time slots, and computing efficient inter-node resource transfers. We
theoretically prove the convergence of our method to the optimal solution that
would be obtained with full statistical knowledge of the adversary. Extensive
simulations demonstrate that our method efficiently learns the adversarial
patterns and achieves substantial gains in minimizing both damage and
operational costs, comparing three benchmark strategies under various parameter
settings.

</details>


### [72] [Towards practical FPRAS for #NFA: Exploiting the Power of Dependence](https://arxiv.org/abs/2506.23561)
*Kuldeep S. Meel,Alexis de Colnet*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: #NFA refers to the problem of counting the words of length $n$ accepted by a
non-deterministic finite automaton. #NFA is #P-hard, and although
fully-polynomial-time randomized approximation schemes (FPRAS) exist, they are
all impractical. The first FPRAS for #NFA had a running time of
$\tilde{O}(n^{17}m^{17}\varepsilon^{-14}\log(\delta^{-1}))$, where $m$ is the
number of states in the automaton, $\delta \in (0,1]$ is the confidence
parameter, and $\varepsilon > 0$ is the tolerance parameter (typically smaller
than $1$). The current best FPRAS achieved a significant improvement in the
time complexity relative to the first FPRAS and obtained FPRAS with time
complexity $\tilde{O}((n^{10}m^2 +
n^6m^3)\varepsilon^{-4}\log^2(\delta^{-1}))$. The complexity of the improved
FPRAS is still too intimidating to attempt any practical implementation.
  In this paper, we pursue the quest for practical FPRAS for #NFA by presenting
a new algorithm with a time complexity of
$O(n^2m^3\log(nm)\varepsilon^{-2}\log(\delta^{-1}))$. Observe that evaluating
whether a word of length $n$ is accepted by an NFA has a time complexity of
$O(nm^2)$. Therefore, our proposed FPRAS achieves sub-quadratic complexity with
respect to membership checks.

</details>


### [73] [Simple Approximations for General Spanner Problems](https://arxiv.org/abs/2506.23638)
*Fritz Bökler,Markus Chimani,Henning Jasper*

Main category: cs.DS

TL;DR: 本文为图的生成器问题提供两种简单近似算法，分别是Adapted Greedy和Randomized Rounding，取得了不同场景下的近似比。


<details>
  <summary>Details</summary>
Motivation: 现有图生成器问题在一般设置下缺乏非平凡近似算法，为获取更通用场景下的保证，需新的近似算法。

Method: 提出Adapted Greedy算法和Randomized Rounding算法。前者保持了贪婪算法的保证并推广了一些保证；后者通过标准多商品流线性规划上的简单舍入方案。

Result: Adapted Greedy实现了m的无条件近似比；Randomized Rounding在整数长度和多项式有界距离需求下得到O(n log n)近似比，在有界度图上对常数有界距离需求得到首个O(log n)近似比。

Conclusion: 提供的两种算法简单有效，在不同场景下取得了较好的近似比，解决了一般形式下生成器问题缺乏非平凡近似算法的问题。

Abstract: Consider a graph with n nodes and m edges, independent edge weights and
lengths, and arbitrary distance demands for node pairs. The spanner problem
asks for a minimum-weight subgraph that satisfies these demands via
sufficiently short paths w.r.t. the edge lengths. For multiplicative
alpha-spanners (where demands equal alpha times the original distances) and
assuming that each edge's weight equals its length, the simple Greedy heuristic
by Alth\"ofer et al. (1993) is known to yield strong solutions, both in theory
and practice. To obtain guarantees in more general settings, recent
approximations typically abandon this simplicity and practicality. Still, so
far, there is no known non-trivial approximation algorithm for the spanner
problem in its most general form. We provide two surprisingly simple
approximations algorithms. In general, our Adapted Greedy achieves the first
unconditional approximation ratio of m, which is non-trivial due to the
independence of weights and lengths. Crucially, it maintains all size and
weight guarantees Greedy is known for, i.e., in the aforementioned
multiplicative alpha-spanner scenario and even for additive +beta-spanners.
Further, it generalizes some of these size guarantees to derive new weight
guarantees. Our second approach, Randomized Rounding, establishes a graph
transformation that allows a simple rounding scheme over a standard
multicommodity flow LP. It yields an O(n log n)-approximation, assuming integer
lengths and polynomially bounded distance demands. The only other known
approximation guarantee in this general setting requires several complex
subalgorithms and analyses, yet we match it up to a factor of O(n^{1/5-eps})
using standard tools. Further, on bounded-degree graphs, we yield the first
O(log n) approximation ratio for constant-bounded distance demands (beyond
multiplicative 2-spanners in unit-length graphs).

</details>


### [74] [Segmented Operations using Matrix Multiplications](https://arxiv.org/abs/2506.23906)
*Aleksandros Sobczyk,Giuseppe Sorrentino,Anastasios Zouzias*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Specialized computational units that perform small matrix multiplications as
primitive operations are typically present in modern accelerators. However,
these units are often underutilized for many fundamental operations besides
dense matrix multiplications. The analysis of algorithms for such architectures
is currently stagnated due to the lack of a rigorous theoretical model of
computation that captures their characteristics. In this work, we propose
MMV-RAM, a computational model tailored to matrix multiplication accelerators.
MMV-RAM judiciously extends the Vector-RAM model with an additional processing
unit that multiplies two matrices of sizes $n\times s$ and $s\times s$ in a
single parallel step, where $s$ is a model parameter. We provide a detailed
theoretical analysis of the model, and carefully balance the computational
power between the matrix and vector units, guided by the circuit complexity
lower bound that parity is not in AC[0].
  In MMV-RAM, we study algorithms for segmented scan and sum, two fundamental
parallel primitives. We propose a segmented scan algorithm that uses matrix
multiplications to perform speculative block-scan computations, which runs in
$O(\log_s(n))$ steps. In contrast, we show that any algorithm that uses only
the vector unit of MMV-RAM requires
$\Omega\left(\frac{\log_2(n)}{\log_2\log_2(n)}\right)$ steps. We further apply
these techniques to obtain similar theoretical speedups for element-wise vector
multiplication and matrix multiplication. Beyond the worst-case complexity
analysis, we propose algorithms for segmented operations that could lead to
highly efficient and pragmatic implementations. For example, we observe that
segmented sum is a combination of three elementary parallel primitives: scan,
compress, and vector differentiation. As a case study, we implement...

</details>


### [75] [Fantastic Flips and Where to Find Them: A General Framework for Parameterized Local Search on Partitioning Problem](https://arxiv.org/abs/2506.24001)
*Niels Grüttemeier,Nils Morawietz,Frank Sommer*

Main category: cs.DS

TL;DR: 本文介绍参数化局部搜索抽象框架用于解决分区问题，分析时间复杂度，证明改进算法与ETH矛盾及部分问题W[1]-难。


<details>
  <summary>Details</summary>
Motivation: 探讨参数化局部搜索中搜索半径k与运行时间的权衡，受向量装箱问题实际应用启发。

Method: 引入抽象框架，针对给定n个物品分b个箱的分区问题，考虑移除最多k个物品重新分配，引入类型数τ参数。

Result: 框架内问题可在τ^k 2^{O(k)} |I|^{O(1)}时间内解决；改进此算法会与ETH矛盾；部分问题仅以搜索半径k为参数时W[1]-难。

Conclusion: 该抽象框架有效，给出时间复杂度，且算法改进存在理论限制，部分问题参数化难度高。

Abstract: Parameterized local search combines classic local search heuristics with the
paradigm of parameterized algorithmics. While most local search algorithms aim
to improve given solutions by performing one single operation on a given
solution, the parameterized approach aims to improve a solution by performing
$k$ simultaneous operations. Herein, $k$ is a parameter called search radius
for which the value can be chosen by a user. One major goal in the field of
parameterized local search is to outline the trade-off between the size of $k$
and the running time of the local search step. In this work, we introduce an
abstract framework that generalizes natural parameterized local search
approaches for a large class of partitioning problems: Given $n$ items that are
partitioned into $b$ bins and a target function that evaluates the quality of
the current partition, one asks whether it is possible to improve the solution
by removing up to $k$ items from their current bins and reassigning them to
other bins. Among others, our framework applies for the local search versions
of problems like Cluster Editing, Vector Bin Packing, and Nash Social Welfare.
Motivated by a real-world application of the problem Vector Bin Packing, we
introduce a parameter called number of types $\tau \le n$ and show that all
problems fitting in our framework can be solved in $\tau^k 2^{O(k)} |I|^{O(1)}$
time, where $|I|$ denotes the total input size. In case of Cluster Editing, the
parameter $\tau$ generalizes the well-known parameter neighborhood diversity of
the input graph. We complement this by showing that for all considered
problems, an algorithm significantly improving over our algorithm with running
time $\tau^k 2^{O(k)} |I|^{O(1)}$ would contradict the ETH. Additionally, we
show that even on very restricted instances, all considered problems are
W[1]-hard when parameterized by the search radius $k$ alone.

</details>


### [76] [Dominating Set Knapsack: Profit Optimization on Dominating Sets](https://arxiv.org/abs/2506.24032)
*Sipra Singh*

Main category: cs.DS

TL;DR: 研究大规模网络中有限预算下选有影响力节点获利问题，定义支配集背包问题，分析其复杂度并给出算法。


<details>
  <summary>Details</summary>
Motivation: 在大规模网络中，在有限预算内选有影响力节点获利，避免对相邻节点额外投入。

Method: 定义支配集背包问题，分析不同图类型下的复杂度，给出树的伪多项式时间算法和FPT算法。

Result: 支配集背包问题在二分图上强NP完全，星图上弱NP完全；证明该问题不太可能是固定参数可处理的；给出了特定参数下的FPT算法。

Conclusion: 对支配集背包问题的复杂度有清晰认识，并提供了不同情况下的算法。

Abstract: In a large-scale network, we want to choose some influential nodes to make a
profit by paying some cost within a limited budget so that we do not have to
spend more budget on some nodes adjacent to the chosen nodes; our problem is
the graph-theoretic representation of it. We define our problem Dominating Set
Knapsack by attaching Knapsack Problem with Dominating Set on graphs. Each
vertex is associated with a cost factor and a profit amount. We aim to choose
some vertices within a fixed budget that gives maximum profit so that we do not
need to choose their 1-hop neighbors. We show that the Dominating Set Knapsack
problem is strongly NP-complete even when restricted to Bipartite graphs but
weakly NP-complete for Star graphs. We present a pseudo-polynomial time
algorithm for Trees in time $O(n\cdot min\{s^2, (\alpha(V))^2\})$. We show that
Dominating Set Knapsack is very unlikely to be Fixed Parameter Tractable(FPT)
by proving that it is in W[2]-hard parameterized by the solution size. We
developed FPT algorithms with running time $O(4^{tw}\cdot n^{O(1)} \cdot
min\{s^2,{\alpha(V)}^2\})$ and $O(2^{vck-1}\cdot n^{O(1)} \cdot
min\{s^2,{\alpha(V)}^2\})$, where $tw$ represents the treewidth of the given
graph, $vck$ is the solution size of the Vertex Cover Knapsack, $s$ is the size
of the knapsack and $\alpha(V)=\sum_{v\in V}\alpha(v)$.

</details>


### [77] [Translating between the representations of an acyclic convex geometry of bounded degree](https://arxiv.org/abs/2506.24052)
*Oscar Defrain,Arthur Ohana,Simon Vilmin*

Main category: cs.DS

TL;DR: 研究给定蕴含基的闭包系统中不可约闭集的枚举问题，证明对于有界度参数该问题可处理，给出算法及运行时间分析。


<details>
  <summary>Details</summary>
Motivation: 当前该问题复杂度状态未知且推广了超图对偶化问题，研究有界度参数下该问题的可处理性。

Method: 利用无环凸几何的结构性质，采用算法枚举的多种技术，如解图遍历、饱和技术和利用无环性的顺序方法。

Result: 算法在计算不可约闭集时为增量多项式时间，构造蕴含基时为多项式时间。

Conclusion: 在标准手电筒搜索框架下，运行时间无法改进为多项式延迟。

Abstract: We consider the problem of enumerating the irreducible closed sets of a
closure system given by an implicational base. In the context of Horn logic,
these correspond to Horn expressions and characteristic models, respectively.
To date, the complexity status of this problem is widely open, and it is
further known to generalize the notorious hypergraph dualization problem, even
in the context of acyclic convex geometries, i.e., closure systems admitting an
acyclic implicational base. This paper studies this later class with a focus on
the degree, which corresponds to the maximal number of implications in which an
element occurs. We show that the problem is tractable for bounded values of
this parameter, even when relaxed to the notions of premise- and
conclusion-degree. Our algorithms rely on structural properties of acyclic
convex geometries and involve various techniques from algorithmic enumeration
such as solution graph traversal, saturation techniques, and a sequential
approach leveraging from acyclicity. They are shown to perform in
incremental-polynomial time for the computation of irreducible closed sets, and
in polynomial time for the construction of an implicational base. Finally, we
argue that our running times cannot be improved to polynomial delay using the
standard framework of flashlight search.

</details>


### [78] [A Refined Kernel for $d$-Hitting Set](https://arxiv.org/abs/2506.24114)
*Yuxi Liu,Mingyu Xiao*

Main category: cs.DS

TL;DR: 本文针对d - Hitting Set问题，运用线性规划技术构建超图的冠分解，将已知核的顶点数从(2d - 1)k^{d - 1} + k 改进到(2d - 2)k^{d - 1} + k。


<details>
  <summary>Details</summary>
Motivation: d - Hitting Set问题是参数化复杂性中的基础问题，已知核结果被广泛应用，期望对其进行改进。

Method: 采用线性规划技术构建超图的冠分解。

Result: 将已知核的顶点数从(2d - 1)k^{d - 1} + k 减少到(2d - 2)k^{d - 1} + k。

Conclusion: 通过新方法对d - Hitting Set问题的已知核结果进行了改进。

Abstract: The $d$-Hitting Set problem is a fundamental problem in parameterized
complexity, which asks whether a given hypergraph contains a vertex subset $S$
of size at most $k$ that intersects every hyperedge (i.e., $S \cap e \neq
\emptyset$ for each hyperedge $e$). The best known kernel for this problem,
established by Abu-Khzam [1], has $(2d - 1)k^{d - 1} + k$ vertices. This result
has been very widely used in the literature as many problems can be modeled as
a special $d$-Hitting Set problem. In this work, we present a refinement to
this result by employing linear programming techniques to construct crown
decompositions in hypergraphs. This approach yields a slight but notable
improvement, reducing the size to $(2d - 2)k^{d - 1} + k$ vertices.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [79] [Capacity Planning in Stable Matching with Truthful or Strategic Preference Uncertainty](https://arxiv.org/abs/2506.22560)
*Maria Bazotte,Margarida Carvalho,Thibaut Vidal*

Main category: cs.GT

TL;DR: 本文引入两阶段随机匹配问题，以学校选择为例，处理不确定偏好，提出启发式方法，表明随机偏好和学生行为对容量决策有影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究未考虑容量决策时偏好的不确定性以及学生策略性误报偏好问题，本文旨在解决这些问题。

Method: 引入两阶段随机匹配问题，用样本平均近似处理不确定性，开发基于行为的数学公式，提出拉格朗日和局部搜索启发式方法。

Result: 基于样本平均近似的方法在学生匹配偏好和录取结果上优于平均情景方法。

Conclusion: 随机偏好对容量决策有影响，学生行为显著影响容量设计，需考虑偏好误报情况。

Abstract: Recent studies on many-to-one matching markets have explored agents with
flexible capacity and truthful preference reporting, focusing on mechanisms
that jointly design capacities and select a matching. However, in real-world
applications such as school choice and residency matching, preferences are
revealed after capacity decisions are made, with matching occurring afterward;
uncertainty about agents' preferences must be considered during capacity
planning. Moreover, even under strategy-proof mechanisms, agents may
strategically misreport preferences based on beliefs about admission chances.
We introduce a two-stage stochastic matching problem with uncertain
preferences, using school choice as a case study. In the first stage, the
clearinghouse expands schools' capacities before observing students' reported
preferences. Students either report their true preferences, producing exogenous
uncertainty, or act strategically, submitting reported preferences based on
their true preferences and admission chances (which depend on capacities),
introducing endogenous uncertainty. In the second stage, the clearinghouse
computes the student-optimal stable matching based on schools' priorities and
students' reported preferences. In strategic cases, endogenous reported
preferences are utility-maximizing transformations of capacity decisions and
exogenous true preferences; we handle uncertainty using sample average
approximation(SAA). We develop behavior-based mathematical formulations and,
due to problem complexity, propose Lagrangian- and local-search-based
behavior-specific heuristics for near-optimal solutions. Our SAA-based
approaches outperform the average scenario approach on students' matching
preferences and admission outcomes, emphasizing the impact of stochastic
preferences on capacity decisions. Student behavior notably influences capacity
design, stressing the need to consider misreports.

</details>


### [80] [Learning Truthful Mechanisms without Discretization](https://arxiv.org/abs/2506.22911)
*Yunxuan Ma,Siqiang Wang,Zhijian Duan,Yukun Cheng,Xiaotie Deng*

Main category: cs.GT

TL;DR: 本文提出无离散化算法TEDI学习真实且效用最大化机制，理论证明其特性，实验显示性能强，为自动机制设计提供新见解。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的方法依赖结果空间离散化确保真实性，随问题规模增大效率低，需解决此局限。

Method: 形式化定价规则概念，提出新颖菜单机制，用Partial GroupMax Network参数化定价规则，开发新训练技术推导无偏梯度估计器。

Result: 理论分析表明TEDI保证真实性、完全表达性和维度不敏感性；实验中TEDI在拍卖场景表现出色，与或超越现有方法。

Conclusion: 这是首个无结果离散化学习真实机制的方法，提高算法效率，提出的概念、架构和技术有潜在价值。

Abstract: This paper introduces TEDI (Truthful, Expressive, and Dimension-Insensitive
approach), a discretization-free algorithm to learn truthful and
utility-maximizing mechanisms. Existing learning-based approaches often rely on
discretization of outcome spaces to ensure truthfulness, which leads to
inefficiency with increasing problem size. To address this limitation, we
formalize the concept of pricing rules, defined as functions that map outcomes
to prices. Based on this concept, we propose a novel menu mechanism, which can
be equivalent to a truthful direct mechanism under specific conditions. The
core idea of TEDI lies in its parameterization of pricing rules using Partial
GroupMax Network, a new network architecture designed to universally
approximate partial convex functions. To learn optimal pricing rules, we
develop novel training techniques, including covariance trick and continuous
sampling, to derive unbiased gradient estimators compatible with first-order
optimization. Theoretical analysis establishes that TEDI guarantees
truthfulness, full expressiveness, and dimension-insensitivity. Experimental
evaluation in the studied auction setting demonstrates that TEDI achieves
strong performance, competitive with or exceeding state-of-the-art methods.
  This work presents the first approaches to learn truthful mechanisms without
outcome discretization, thereby enhancing algorithmic efficiency. The proposed
concepts, network architecture, and learning techniques might offer potential
value and provide new insights for automated mechanism design and
differentiable economics.

</details>


### [81] [Markov Chains of Evolutionary Games with a Small Number of Players](https://arxiv.org/abs/2506.23134)
*Athanasios Kehagias*

Main category: cs.GT

TL;DR: 构建并研究有限玩家数演化博弈的转移概率矩阵，用简化模型，以具体博弈为例并考虑多种修正协议，构建矩阵并研究其性质。


<details>
  <summary>Details</summary>
Motivation: 研究有限玩家数演化博弈的转移概率矩阵。

Method: 采用Sandholm研究的群体博弈简化版本，以迭代囚徒困境等为例，考虑多种修正协议。

Result: 为每种情况明确构建马尔可夫链转移概率矩阵。

Conclusion: 文中未明确提及结论，但可推测成功构建矩阵并研究其性质为成果。

Abstract: We construct and study the transition probability matrix of evolutionary
games in which the number of players is finite (and relatively small) of such
games. We use a simplified version of the population games studied by Sandholm.
After laying out a general framework we concentrate on specific examples,
involving the Iterated Prisoner's Dilemma, the Iterated Stag Hunt, and the
Rock-Paper-Scissors game. Also we consider several revision protocols: Best
Response, Pairwise Comparison, Pairwise Proportional Comparison etc. For each
of these we explicitly construct the MC transition probability matrix and study
its properties.

</details>


### [82] [Interdependent Bilateral Trade: Information vs Approximation](https://arxiv.org/abs/2506.23896)
*Shahar Dobzinski,Alon Eden,Kira Goldner,Ariel Shaulker,Thodoris Tsilivis*

Main category: cs.GT

TL;DR: 本文研究具有相互依赖价值的双边贸易福利最大化，通过对信息结构分类探讨近似可能性和比率。


<details>
  <summary>Details</summary>
Motivation: 以往文献仅针对私有价值情况得到激励兼容近似机制，设计相互依赖情形的机制更具挑战，需研究相互依赖价值下的双边贸易福利最大化。

Method: 通过量化玩家私人信号对自身估值的影响来对信息结构进行分类。

Result: 描绘了基于这些信息结构，近似何时可行和不可行的情况，并研究了自然信息结构族的可能近似比率。

Conclusion: 提出的信息结构分类方法有助于研究相互依赖价值下双边贸易福利最大化的近似问题。

Abstract: Welfare maximization in bilateral trade has been extensively studied in
recent years. Previous literature obtained incentive-compatible approximation
mechanisms only for the private values case. In this paper, we study welfare
maximization in bilateral trade with interdependent values. Designing
mechanisms for interdependent settings is much more challenging because the
values of the players depend on the private information of the others,
requiring complex belief updates and strategic inference. We propose to
classify information structures by quantifying the influence that a player's
private signal has on their own valuation. We then paint a picture of where
approximations are possible and impossible based on these information
structures. Finally, we also study the possible approximation ratios for a
natural family of information structures.

</details>


### [83] [Quickest Detection of Adversarial Attacks Against Correlated Equilibria](https://arxiv.org/abs/2506.24040)
*Kiarash Kazari,Aris Kanellopoulos,György Dán*

Main category: cs.GT

TL;DR: 研究对抗环境下策略博弈的相关均衡，用零和博弈建模，推导策略，提出检测方案并验证有效性。


<details>
  <summary>Details</summary>
Motivation: 在对抗环境中，玩家要尽快检测对手对公共信号的攻击以避免效用损失。

Method: 将对手与玩家的交互建模为零和博弈，用最快变化检测框架推导双方策略，定义一类对抗策略。

Result: 广义CUSUM方案对攻击检测渐近最优，在交通路由游戏中，检测方案能有效限制潜在对手造成的效用损失。

Conclusion: 提出的检测方案可有效应对潜在对手的攻击，减少效用损失。

Abstract: We consider correlated equilibria in strategic games in an adversarial
environment, where an adversary can compromise the public signal used by the
players for choosing their strategies, while players aim at detecting a
potential attack as soon as possible to avoid loss of utility. We model the
interaction between the adversary and the players as a zero-sum game and we
derive the maxmin strategies for both the defender and the attacker using the
framework of quickest change detection. We define a class of adversarial
strategies that achieve the optimal trade-off between attack impact and attack
detectability and show that a generalized CUSUM scheme is asymptotically
optimal for the detection of the attacks. Our numerical results on the
Sioux-Falls benchmark traffic routing game show that the proposed detection
scheme can effectively limit the utility loss by a potential adversary.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [84] [Interact2Vec -- An efficient neural network-based model for simultaneously learning users and items embeddings in recommender systems](https://arxiv.org/abs/2506.22648)
*Pedro R. Pires,Tiago A. Almeida*

Main category: cs.IR

TL;DR: 介绍推荐系统问题，提出Interact2Vec模型，经实验证明该模型有良好表现，适合资源稀缺场景。


<details>
  <summary>Details</summary>
Motivation: 解决推荐系统高数据维度和稀疏性问题，且避免现有方法依赖复杂架构或内容数据的不足。

Method: 提出Interact2Vec模型，用自然语言处理策略优化训练，进行外在和内在质量实验。

Result: 外在实验中在30%数据集获第二或第三好结果，训练时间平均减少274%；内在实验通过相似度表分析。

Conclusion: Interact2Vec在推荐任务表现好，适合计算资源稀缺场景，能高效学习用户和物品嵌入。

Abstract: Over the past decade, recommender systems have experienced a surge in
popularity. Despite notable progress, they grapple with challenging issues,
such as high data dimensionality and sparseness. Representing users and items
as low-dimensional embeddings learned via neural networks has become a leading
solution. However, while recent studies show promising results, many approaches
rely on complex architectures or require content data, which may not always be
available. This paper presents Interact2Vec, a novel neural network-based model
that simultaneously learns distributed embeddings for users and items while
demanding only implicit feedback. The model employs state-of-the-art strategies
that natural language processing models commonly use to optimize the training
phase and enhance the final embeddings. Two types of experiments were conducted
regarding the extrinsic and intrinsic quality of the model. In the former, we
benchmarked the recommendations generated by Interact2Vec's embeddings in a
top-$N$ ranking problem, comparing them with six other recommender algorithms.
The model achieved the second or third-best results in 30\% of the datasets,
being competitive with other recommenders, and has proven to be very efficient
with an average training time reduction of 274\% compared to other
embedding-based models. Later, we analyzed the intrinsic quality of the
embeddings through similarity tables. Our findings suggest that Interact2Vec
can achieve promising results, especially on the extrinsic task, and is an
excellent embedding-generator model for scenarios of scarce computing
resources, enabling the learning of item and user embeddings simultaneously and
efficiently.

</details>


### [85] [Machine Assistant with Reliable Knowledge: Enhancing Student Learning via RAG-based Retrieval](https://arxiv.org/abs/2506.23026)
*Yongsheng Lian*

Main category: cs.IR

TL;DR: 介绍检索增强问答系统MARK，用于支持学生学习，有混合检索策略和反馈循环，在课堂和技术支持场景应用良好且公开可用。


<details>
  <summary>Details</summary>
Motivation: 设计一个能通过准确且有上下文依据的回答支持学生学习的问答系统。

Method: 基于RAG框架，结合策展知识库，采用结合密集向量相似度和稀疏关键词检索的混合搜索策略，设置反馈循环。

Result: 在课堂替代传统办公时间成功解答学生广泛问题，结合客户特定知识库处理应用领域的常规、上下文敏感任务。

Conclusion: MARK系统有效可行，可在学习和应用领域发挥作用。

Abstract: We present Machine Assistant with Reliable Knowledge (MARK), a
retrieval-augmented question-answering system designed to support student
learning through accurate and contextually grounded responses. The system is
built on a retrieval-augmented generation (RAG) framework, which integrates a
curated knowledge base to ensure factual consistency. To enhance retrieval
effectiveness across diverse question types, we implement a hybrid search
strategy that combines dense vector similarity with sparse keyword-based
retrieval. This dual-retrieval mechanism improves robustness for both general
and domain-specific queries. The system includes a feedback loop in which
students can rate responses and instructors can review and revise them.
Instructor corrections are incorporated into the retrieval corpus, enabling
adaptive refinement over time. The system was deployed in a classroom setting
as a substitute for traditional office hours, where it successfully addressed a
broad range of student queries. It was also used to provide technical support
by integrating with a customer-specific knowledge base, demonstrating its
ability to handle routine, context-sensitive tasks in applied domains. MARK is
publicly accessible at https://app.eduquery.ai.

</details>


### [86] [Synergizing Implicit and Explicit User Interests: A Multi-Embedding Retrieval Framework at Pinterest](https://arxiv.org/abs/2506.23060)
*Zhibo Fan,Hongtao Lin,Haoyu Chen,Bowen Deng,Hedi Xia,Yuke Yan,James Li*

Main category: cs.IR

TL;DR: 提出多嵌入检索框架，结合隐式和显式用户兴趣提升候选检索效果，在实验和A/B测试中表现良好并已部署。


<details>
  <summary>Details</summary>
Motivation: 传统双塔模型在检索阶段难以有效覆盖多样和长尾用户兴趣，存在用户 - 物品特征交互有限、偏向热门用例的问题。

Method: 提出多嵌入检索框架，通过可微聚类模块（DCM）捕捉隐式兴趣，条件检索（CR）建模显式兴趣，结合两者进行条件化用户表示学习。

Result: 广泛实验和A/B测试显示，用户参与度和内容多样性指标显著提升。

Conclusion: 所提出的框架能更有效全面地进行候选检索，已成功部署在Pinterest主页信息流。

Abstract: Industrial recommendation systems are typically composed of multiple stages,
including retrieval, ranking, and blending. The retrieval stage plays a
critical role in generating a high-recall set of candidate items that covers a
wide range of diverse user interests. Effectively covering the diverse and
long-tail user interests within this stage poses a significant challenge:
traditional two-tower models struggle in this regard due to limited user-item
feature interaction and often bias towards top use cases. To address these
issues, we propose a novel multi-embedding retrieval framework designed to
enhance user interest representation by generating multiple user embeddings
conditioned on both implicit and explicit user interests. Implicit interests
are captured from user history through a Differentiable Clustering Module
(DCM), whereas explicit interests, such as topics that the user has followed,
are modeled via Conditional Retrieval (CR). These methodologies represent a
form of conditioned user representation learning that involves condition
representation construction and associating the target item with the relevant
conditions. Synergizing implicit and explicit user interests serves as a
complementary approach to achieve more effective and comprehensive candidate
retrieval as they benefit on different user segments and extract conditions
from different but supplementary sources. Extensive experiments and A/B testing
reveal significant improvements in user engagements and feed diversity metrics.
Our proposed framework has been successfully deployed on Pinterest home feed.

</details>


### [87] [Enhancing Live Broadcast Engagement: A Multi-modal Approach to Short Video Recommendations Using MMGCN and User Preferences](https://arxiv.org/abs/2506.23085)
*Saeid Aghasoleymani Najafabadi*

Main category: cs.IR

TL;DR: 本文提出结合MMGCN与用户偏好的短视频推荐系统提升直播参与度，经三个数据集验证性能优于基线模型，强调多模态集成和以用户为中心方法的重要性。


<details>
  <summary>Details</summary>
Motivation: 探索多模态方法，通过开发结合MMGCN与用户偏好的短视频推荐系统来提升直播参与度。

Method: 结合协同过滤和基于内容的过滤技术，考虑用户交互数据、视频内容特征和上下文信息构建推荐系统，使用Kwai、TikTok和MovieLens三个数据集评估。

Result: 提出的基于MMGCN的模型在捕获用户偏好和进行个性化推荐上优于DeepFM等基线模型，在三个数据集上有相应F1分数。

Conclusion: 强调多模态集成和以用户为中心的方法对推进推荐系统、提升直播平台内容发现和观众互动的重要性。

Abstract: The purpose of this paper is to explore a multi-modal approach to enhancing
live broadcast engagement by developing a short video recommendation system
that incorporates Multi-modal Graph Convolutional Networks (MMGCN) with user
preferences. In order to provide personalized recommendations tailored to
individual interests, the proposed system takes into account user interaction
data, video content features, and contextual information. With the aid of a
hybrid approach combining collaborative filtering and content-based filtering
techniques, the system is able to capture nuanced relationships between users,
video attributes, and engagement patterns. Three datasets are used to evaluate
the effectiveness of the system: Kwai, TikTok, and MovieLens. Compared to
baseline models, such as DeepFM, Wide & Deep, LightGBM, and XGBoost, the
proposed MMGCN-based model shows superior performance. A notable feature of the
proposed model is that it outperforms all baseline methods in capturing diverse
user preferences and making accurate, personalized recommendations, resulting
in a Kwai F1 score of 0.574, a Tiktok F1 score of 0.506, and a MovieLens F1
score of 0.197. We emphasize the importance of multi-modal integration and
user-centric approaches in advancing recommender systems, emphasizing the role
they play in enhancing content discovery and audience interaction on live
broadcast platforms.

</details>


### [88] [Multi-task Offline Reinforcement Learning for Online Advertising in Recommender Systems](https://arxiv.org/abs/2506.23090)
*Langming Liu,Wanyu Wang,Chi Zhang,Bo Li,Hongzhi Yin,Xuetao Wei,Wenbo Su,Bo Zheng,Xiangyu Zhao*

Main category: cs.IR

TL;DR: 提出新的多任务离线强化学习模型MTORL解决稀疏广告场景下现有离线RL方法的问题，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前离线强化学习方法应用于稀疏广告场景时面临严重高估、分布偏移和忽略预算约束等问题。

Method: 建立广告专属的MDP框架，开发因果状态编码器，引入因果注意力机制，采用多任务学习解码动作和奖励，并将任务集成到在线广告的自动化系统。

Result: 在离线和在线环境的大量实验中，MTORL表现优于现有方法。

Conclusion: MTORL是解决稀疏广告场景下离线RL问题的有效方案。

Abstract: Online advertising in recommendation platforms has gained significant
attention, with a predominant focus on channel recommendation and budget
allocation strategies. However, current offline reinforcement learning (RL)
methods face substantial challenges when applied to sparse advertising
scenarios, primarily due to severe overestimation, distributional shifts, and
overlooking budget constraints. To address these issues, we propose MTORL, a
novel multi-task offline RL model that targets two key objectives. First, we
establish a Markov Decision Process (MDP) framework specific to the nuances of
advertising. Then, we develop a causal state encoder to capture dynamic user
interests and temporal dependencies, facilitating offline RL through
conditional sequence modeling. Causal attention mechanisms are introduced to
enhance user sequence representations by identifying correlations among causal
states. We employ multi-task learning to decode actions and rewards,
simultaneously addressing channel recommendation and budget allocation.
Notably, our framework includes an automated system for integrating these tasks
into online advertising. Extensive experiments on offline and online
environments demonstrate MTORL's superiority over state-of-the-art methods.

</details>


### [89] [NaviX: A Native Vector Index Design for Graph DBMSs With Robust Predicate-Agnostic Search Performance](https://arxiv.org/abs/2506.23397)
*Gaurav Sehgal,Semih Salihoglu*

Main category: cs.IR

TL;DR: 提出用于图数据库管理系统的原生向量索引NaviX，实现基于磁盘的向量索引并支持谓词无关的过滤向量搜索查询，通过实验证明其鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 满足将现有数据库管理系统扩展为能支持现代预测应用的统一系统的需求，支持向量嵌入与对象结构化属性和连接的联合查询。

Method: 基于分层导航小世界（HNSW）图构建NaviX；采用预过滤方法评估选择子查询，提出自适应算法利用HNSW图中向量的局部选择性选择启发式方法。

Result: 通过大量实验对比现有基于预过滤和后过滤的基线方法，证明了NaviX的鲁棒性和效率。

Conclusion: NaviX能够有效满足扩展图数据库管理系统以支持现代预测应用的需求，具有良好的性能。

Abstract: There is an increasing demand for extending existing DBMSs with vector
indices so that they become unified systems capable of supporting modern
predictive applications, which require joint querying of vector embeddings
together with the structured properties and connections of objects. We present
NaviX, a native vector index for graph DBMSs (GDBMSs) that has two main design
goals. First, we aim to implement a disk-based vector index that leverages the
core storage and query-processing capabilities of the underlying GDBMS. To this
end, NaviX is built on the Hierarchical Navigable Small-World (HNSW) graph,
which itself is a graph-based structure. Second, we aim to support
predicate-agnostic filtered vector search queries, in which the k nearest
neighbors (kNNs) of a query vector vQ are searched only within an arbitrary
subset S of vectors defined by an ad-hoc selection sub-query QS. We adopt a
prefiltering approach that evaluates QS first and passes the full description
of subset S to the kNN search operator. We study how to design a prefiltering
search algorithm that remains robust under varying selectivities and under
different correlations between subset S and query vector vQ. We propose an
adaptive algorithm that uses the local selectivity of each vector in the HNSW
graph to choose an appropriate heuristic at every iteration of the kNN search.
Finally, We demonstrate NaviX's robustness and efficiency through extensive
experiments against both existing prefiltering- and postfiltering-based
baselines.

</details>


### [90] [Compositions of Variant Experts for Integrating Short-Term and Long-Term Preferences](https://arxiv.org/abs/2506.23170)
*Jaime Hieu Do,Trung-Hoang Le,Hady W. Lauw*

Main category: cs.IR

TL;DR: 本文聚焦个性化序列推荐，研究短长期偏好影响，提出结合短长期偏好的CoVE框架，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 在在线数字领域，为更好地利用用户偏好提供个性化推荐，解决选择悖论，提升推荐系统性能。

Method: 在多样真实数据集上进行实证研究，提出结合短长期偏好的Compositions of Variant Experts (CoVE)框架，通过不同专业推荐模型动态整合短长期偏好。

Result: 实验展示了所提方法的有效性，消融研究进一步探究了不同专家类型的影响。

Conclusion: 结合短长期偏好的CoVE框架能有效提升个性化序列推荐的性能。

Abstract: In the online digital realm, recommendation systems are ubiquitous and play a
crucial role in enhancing user experience. These systems leverage user
preferences to provide personalized recommendations, thereby helping users
navigate through the paradox of choice. This work focuses on personalized
sequential recommendation, where the system considers not only a user's
immediate, evolving session context, but also their cumulative historical
behavior to provide highly relevant and timely recommendations. Through an
empirical study conducted on diverse real-world datasets, we have observed and
quantified the existence and impact of both short-term (immediate and
transient) and long-term (enduring and stable) preferences on users' historical
interactions. Building on these insights, we propose a framework that combines
short- and long-term preferences to enhance recommendation performance, namely
Compositions of Variant Experts (CoVE). This novel framework dynamically
integrates short- and long-term preferences through the use of different
specialized recommendation models (i.e., experts). Extensive experiments
showcase the effectiveness of the proposed methods and ablation studies further
investigate the impact of variant expert types.

</details>


### [91] [Impact of Shallow vs. Deep Relevance Judgments on BERT-based Reranking Models](https://arxiv.org/abs/2506.23191)
*Gabriel Iturra-Bocaz,Danny Vo,Petra Galuscakova*

Main category: cs.IR

TL;DR: 研究浅、深相关性判断数据集对BERT重排模型性能的影响，结果显示浅判断数据集有优势，深判断数据集劣势或可通过更多负例缓解。


<details>
  <summary>Details</summary>
Motivation: 探究浅、深相关性判断对神经信息检索中BERT重排模型性能的影响。

Method: 比较浅判断（多查询少判断）和深判断（少查询多判断）数据集，在MS MARCO和LongEval集合上进行实验。

Result: 浅判断数据集因上下文范围广，提升了重排模型的泛化和有效性；深判断数据集的劣势可通过更多负训练例缓解。

Conclusion: 浅判断数据集对BERT重排模型性能提升有优势，深判断数据集劣势可改善。

Abstract: This paper investigates the impact of shallow versus deep relevance judgments
on the performance of BERT-based reranking models in neural Information
Retrieval. Shallow-judged datasets, characterized by numerous queries each with
few relevance judgments, and deep-judged datasets, involving fewer queries with
extensive relevance judgments, are compared. The research assesses how these
datasets affect the performance of BERT-based reranking models trained on them.
The experiments are run on the MS MARCO and LongEval collections. Results
indicate that shallow-judged datasets generally enhance generalization and
effectiveness of reranking models due to a broader range of available contexts.
The disadvantage of the deep-judged datasets might be mitigated by a larger
number of negative training examples.

</details>


### [92] [Learning to Rank with Variable Result Presentation Lengths](https://arxiv.org/abs/2506.23319)
*Norman Knyazev,Harrie Oosterhuis*

Main category: cs.IR

TL;DR: 提出可变展示长度排序任务，提出VLPL方法优化文档排序和长度，实验表明其效果好，凸显结合文档展示与LTR的重要性和困难。


<details>
  <summary>Details</summary>
Motivation: 现有LTR方法未解决固定垂直空间排名中文档展示长度的决策问题。

Method: 提出VLPL - 一种新的Plackett - Luce列表式梯度估计方法，用于联合优化文档排序和展示长度。

Result: 半合成实验显示VLPL能有效平衡文档曝光和吸引力，简单的长度感知方法也比固定长度模型有显著性能提升。

Conclusion: 理论和实证结果凸显结合文档展示与LTR的重要性和困难。

Abstract: Learning to Rank (LTR) methods generally assume that each document in a top-K
ranking is presented in an equal format. However, previous work has shown that
users' perceptions of relevance can be changed by varying presentations, i.e.,
allocating more vertical space to some documents to provide additional textual
or image information. Furthermore, presentation length can also redirect
attention, as users are more likely to notice longer presentations when
scrolling through results. Deciding on the document presentation lengths in a
fixed vertical space ranking is an important problem that has not been
addressed by existing LTR methods.
  We address this gap by introducing the variable presentation length ranking
task, where simultaneously the ordering of documents and their presentation
length is decided. Despite being a generalization of standard ranking, we show
that this setting brings significant new challenges: Firstly, the probability
ranking principle no longer applies to this setting, and secondly, the problem
cannot be divided into separate ordering and length selection tasks.
  We therefore propose VLPL - a new family of Plackett-Luce list-wise gradient
estimation methods for the joint optimization of document ordering and lengths.
Our semi-synthetic experiments show that VLPL can effectively balance the
expected exposure and attractiveness of all documents, achieving the best
performance across different ranking settings. Furthermore, we observe that
even simple length-aware methods can achieve significant performance
improvements over fixed-length models. Altogether, our theoretical and
empirical results highlight the importance and difficulties of combining
document presentation with LTR.

</details>


### [93] [Teaching a Language Model to Speak the Language of Tools](https://arxiv.org/abs/2506.23394)
*Simeon Emanuilov*

Main category: cs.IR

TL;DR: 本文提出一种使现有语言模型在目标语言中实现可靠工具使用的方法，以保加利亚语为例，训练BgGPT模型，引入TUCAN提升函数调用准确性并规范输出格式，还公开相关资源。


<details>
  <summary>Details</summary>
Motivation: 多数多语言模型在非英语中缺乏可靠工具使用能力，难以决定何时使用工具和生成函数调用所需的结构化输出，在低资源语言中易出现语言混淆。

Method: 在包含10035个函数调用示例的新型双语数据集上继续训练BgGPT模型系列，引入TUCAN。

Result: TUCAN在函数调用准确性上比基础模型最多提高28.75%，输出格式规范，与基础模型形成对比。

Conclusion: 该研究展示了一种将工具增强能力扩展到非英语系统的实用方法。

Abstract: External tool integration through function-calling is essential for practical
language model applications, yet most multilingual models lack reliable
tool-use capabilities in non-English languages. Even state-of-the-art
multilingual models struggle with determining when to use tools and generating
the structured outputs required for function calls, often exhibiting language
confusion when prompted in lower-resource languages. This work presents a
methodology for adapting existing language models to enable robust tool use in
any target language, using Bulgarian as a case study. The approach involves
continued training of the BgGPT model series (2.6B, 9B, 27B parameters) on a
novel bilingual dataset of 10,035 function-calling examples designed to support
standardized protocols like MCP (Model Context Protocol). The research
introduces TUCAN (Tool-Using Capable Assistant Navigator), which achieves up to
28.75% improvement in function-calling accuracy over base models while
preserving core language understanding, as verified on established Bulgarian
benchmarks. Beyond accuracy gains, TUCAN models demonstrate production-ready
response formatting with clean, parsable function calls, contrasting with the
verbose and inconsistent outputs of base models. The models, evaluation
framework, and dataset are released to enable replication for other languages.
This work demonstrates a practical approach for extending tool-augmented
capabilities beyond English-centric systems.

</details>


### [94] [KiseKloset: Comprehensive System For Outfit Retrieval, Recommendation, And Try-On](https://arxiv.org/abs/2506.23471)
*Thanh-Tung Phan-Nguyen,Khoi-Nguyen Nguyen-Ngoc,Tam V. Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.IR

TL;DR: 提出KiseKloset系统用于服装检索、推荐和试穿，经用户测试满意度达84%，提升网购体验。


<details>
  <summary>Details</summary>
Motivation: 改善全球时尚电商行业中用户的在线购物体验。

Method: 提出KiseKloset系统，采用两种服装检索方法，引入新的变压器架构推荐互补商品，集成近似算法优化搜索，使用轻量级高效虚拟试穿框架。

Result: 用户研究表明84%的参与者认为系统非常有用，显著改善了他们的网购体验。

Conclusion: KiseKloset系统能有效提升用户在线购物体验，减少零售商相关成本。

Abstract: The global fashion e-commerce industry has become integral to people's daily
lives, leveraging technological advancements to offer personalized shopping
experiences, primarily through recommendation systems that enhance customer
engagement through personalized suggestions. To improve customers' experience
in online shopping, we propose a novel comprehensive KiseKloset system for
outfit retrieval, recommendation, and try-on. We explore two approaches for
outfit retrieval: similar item retrieval and text feedback-guided item
retrieval. Notably, we introduce a novel transformer architecture designed to
recommend complementary items from diverse categories. Furthermore, we enhance
the overall performance of the search pipeline by integrating approximate
algorithms to optimize the search process. Additionally, addressing the crucial
needs of online shoppers, we employ a lightweight yet efficient virtual try-on
framework capable of real-time operation, memory efficiency, and maintaining
realistic outputs compared to its predecessors. This virtual try-on module
empowers users to visualize specific garments on themselves, enhancing the
customers' experience and reducing costs associated with damaged items for
retailers. We deployed our end-to-end system for online users to test and
provide feedback, enabling us to measure their satisfaction levels. The results
of our user study revealed that 84% of participants found our comprehensive
system highly useful, significantly improving their online shopping experience.

</details>


### [95] [Act-With-Think: Chunk Auto-Regressive Modeling for Generative Recommendation](https://arxiv.org/abs/2506.23643)
*Yifan Wang,Weinan Gan,Longtao Xiao,Jieming Zhu,Heng Chang,Haozhao Wang,Rui Zhang,Zhenhua Dong,Ruiming Tang,Ruixuan Li*

Main category: cs.IR

TL;DR: 提出Chunk AutoRegressive Modeling (CAR)新生成范式，结合语义和行为信息，实验表明性能优于传统方法且有缩放效应。


<details>
  <summary>Details</summary>
Motivation: 现有生成推荐方法忽视行为和语义的内在关系，限制了生成推荐的潜力。

Method: 从“行动 - 思考”双视角，通过块级自回归将语义和行为信息纳入单个自回归变压器，将其打包成概念块进行整体预测。

Result: CAR显著优于基于传统自回归的现有方法，Recall@5提高7.93%到22.30%，验证了模型性能和语义ID位数的缩放效应。

Conclusion: CAR初步模拟了类似大语言模型推理过程的慢思考机制。

Abstract: Generative recommendation (GR) typically encodes behavioral or semantic
aspects of item information into discrete tokens, leveraging the standard
autoregressive (AR) generation paradigm to make predictions. However, existing
methods tend to overlook their intrinsic relationship, that is, the semantic
usually provides some reasonable explainability "$\textbf{why}$" for the
behavior "$\textbf{what}$", which may constrain the full potential of GR. To
this end, we present Chunk AutoRegressive Modeling (CAR), a new generation
paradigm following the decision pattern that users usually think semantic
aspects of items (e.g. brand) and then take actions on target items (e.g.
purchase). Our CAR, for the $\textit{first time}$, incorporates semantics
(SIDs) and behavior (UID) into a single autoregressive transformer from an
``act-with-think'' dual perspective via chunk-level autoregression.
Specifically, CAR packs SIDs and UID into a conceptual chunk for item unified
representation, allowing each decoding step to make a holistic prediction.
Experiments show that our CAR significantly outperforms existing methods based
on traditional AR, improving Recall@5 by 7.93% to 22.30%. Furthermore, we
verify the scaling effect between model performance and SIDs bit number,
demonstrating that CAR preliminary emulates a kind of slow-thinking style
mechanism akin to the reasoning processes observed in large language models
(LLMs).

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [96] [Latent Factorization of Tensors with Threshold Distance Weighted Loss for Traffic Data Estimation](https://arxiv.org/abs/2506.22441)
*Lei Yang*

Main category: cs.LG

TL;DR: 提出TDWLFT模型解决缺失时空交通数据插补问题，实验表明该模型在预测精度和计算效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 智能交通系统依赖完整高质量时空交通数据，但实际数据收集存在问题，传统LFT模型易受异常值影响。

Method: 提出TDW损失并入的潜在张量分解（TDWLFT）模型，通过为单个样本分配不同权重降低模型对异常值的敏感性。

Result: 在两个来自不同城市环境的交通速度数据集上进行广泛实验，TDWLFT模型在预测精度和计算效率上始终优于现有方法。

Conclusion: TDWLFT模型是解决缺失时空交通数据插补问题的有效方案。

Abstract: Intelligent transportation systems (ITS) rely heavily on complete and
high-quality spatiotemporal traffic data to achieve optimal performance.
Nevertheless, in real-word traffic data collection processes, issues such as
communication failures and sensor malfunctions often lead to incomplete or
corrupted datasets, thereby posing significant challenges to the advancement of
ITS. Among various methods for imputing missing spatiotemporal traffic data,
the latent factorization of tensors (LFT) model has emerged as a widely adopted
and effective solution. However, conventional LFT models typically employ the
standard L2-norm in their learning objective, which makes them vulnerable to
the influence of outliers. To overcome this limitation, this paper proposes a
threshold distance weighted (TDW) loss-incorporated Latent Factorization of
Tensors (TDWLFT) model. The proposed loss function effectively reduces the
model's sensitivity to outliers by assigning differentiated weights to
individual samples. Extensive experiments conducted on two traffic speed
datasets sourced from diverse urban environments confirm that the proposed
TDWLFT model consistently outperforms state-of-the-art approaches in terms of
both in both prediction accuracy and computational efficiency.

</details>


### [97] [Task-Agnostic Contrastive Pretraining for Relational Deep Learning](https://arxiv.org/abs/2506.22530)
*Jakub Peleška,Gustav Šír*

Main category: cs.LG

TL;DR: 提出用于关系深度学习的任务无关对比预训练方法，在基准测试上验证了其学习可迁移表示的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有关系深度学习模型依赖特定任务的监督学习，影响可扩展性和复用性。

Method: 提出任务无关的对比预训练方法，引入行级、链接级和上下文级三个层次的对比目标，通过模块化架构和高效采样策略实现。

Result: 在标准基准测试上，微调预训练模型的效果明显优于从头开始训练。

Conclusion: 所提方法在学习关系数据的可迁移表示方面有前景。

Abstract: Relational Deep Learning (RDL) is an emerging paradigm that leverages Graph
Neural Network principles to learn directly from relational databases by
representing them as heterogeneous graphs. However, existing RDL models
typically rely on task-specific supervised learning, requiring training
separate models for each predictive task, which may hamper scalability and
reuse.
  In this work, we propose a novel task-agnostic contrastive pretraining
approach for RDL that enables database-wide representation learning. For that
aim, we introduce three levels of contrastive objectives$-$row-level,
link-level, and context-level$-$designed to capture the structural and semantic
heterogeneity inherent to relational data. We implement the respective
pretraining approach through a modular RDL architecture and an efficient
sampling strategy tailored to the heterogeneous database setting. Our
preliminary results on standard RDL benchmarks demonstrate that fine-tuning the
pretrained models measurably outperforms training from scratch, validating the
promise of the proposed methodology in learning transferable representations
for relational data.

</details>


### [98] [Features-based embedding or Feature-grounding](https://arxiv.org/abs/2506.22442)
*Piotr Makarevich*

Main category: cs.LG

TL;DR: 本文研究如何用基于特征的嵌入在深度学习模型中重现基于知识的结构化思维，并介绍构建特征基础嵌入的方法。


<details>
  <summary>Details</summary>
Motivation: 探究如何在深度学习模型中重现日常推理中基于知识的结构化思维。

Method: 引入一种构建特征基础嵌入的具体方法，使可操作字典的可共享表示与可解释的特定领域概念特征对齐。

Result: 未提及

Conclusion: 未提及

Abstract: In everyday reasoning, when we think about a particular object, we associate
it with a unique set of expected properties such as weight, size, or more
abstract attributes like density or horsepower. These expectations are shaped
by our prior knowledge and the conceptual categories we have formed through
experience. This paper investigates how such knowledge-based structured
thinking can be reproduced in deep learning models using features based
embeddings. Specially, it introduces an specific approach to build
feature-grounded embedding, aiming to align shareable representations of
operable dictionary with interpretable domain-specific conceptual features.

</details>


### [99] [BEST-Route: Adaptive LLM Routing with Test-Time Optimal Compute](https://arxiv.org/abs/2506.22716)
*Dujian Ding,Ankur Mallick,Shaokun Zhang,Chi Wang,Daniel Madrigal,Mirian Del Carmen Hipolito Garcia,Menglin Xia,Laks V. S. Lakshmanan,Qingyun Wu,Victor Rühle*

Main category: cs.LG

TL;DR: 提出BEST - Route路由框架，在真实数据集实验中成本最多降60%且性能下降少于1%


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型查询路由方法过度使用大模型，无法充分节省成本，需改进

Method: 提出BEST - Route框架，根据查询难度和质量阈值选择模型及采样的响应数量

Result: 在真实数据集实验中，成本最多降低60%，性能下降少于1%

Conclusion: BEST - Route框架能在降低成本的同时保证性能

Abstract: Large language models (LLMs) are powerful tools but are often expensive to
deploy at scale. LLM query routing mitigates this by dynamically assigning
queries to models of varying cost and quality to obtain a desired trade-off.
Prior query routing approaches generate only one response from the selected
model and a single response from a small (inexpensive) model was often not good
enough to beat a response from a large (expensive) model due to which they end
up overusing the large model and missing out on potential cost savings.
However, it is well known that for small models, generating multiple responses
and selecting the best can enhance quality while remaining cheaper than a
single large-model response. We leverage this idea to propose BEST-Route, a
novel routing framework that chooses a model and the number of responses to
sample from it based on query difficulty and the quality thresholds.
Experiments on real-world datasets demonstrate that our method reduces costs by
up to 60% with less than 1% performance drop.

</details>


### [100] [Learning Interpretable Rules from Neural Networks: Neurosymbolic AI for Radar Hand Gesture Recognition](https://arxiv.org/abs/2506.22443)
*Sarah Seifi,Tobias Sukianto,Cecilia Carbonelli,Lorenzo Servadei,Robert Wille*

Main category: cs.LG

TL;DR: 研究提出神经符号规则学习神经网络RL - Net用于雷达手势识别，在性能和可解释性间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 规则模型可解释性强但难处理复杂数据，深度神经网络性能好却缺乏透明度，需要在两者间找到平衡，应用于雷达手势识别。

Method: 提出RL - Net，与MIRA和XentricAI对比，评估准确性、可解释性和用户适应性，进行迁移学习。

Result: RL - Net在保持93.03% F1性能的同时显著降低规则复杂度，识别优化挑战并提出改进。

Conclusion: RL - Net是透明度和性能的实用中间方案，证明神经符号模型用于可解释手势识别的可行性，为可解释AI拓展到边缘传感系统提供见解。

Abstract: Rule-based models offer interpretability but struggle with complex data,
while deep neural networks excel in performance yet lack transparency. This
work investigates a neuro-symbolic rule learning neural network named RL-Net
that learns interpretable rule lists through neural optimization, applied for
the first time to radar-based hand gesture recognition (HGR). We benchmark
RL-Net against a fully transparent rule-based system (MIRA) and an explainable
black-box model (XentricAI), evaluating accuracy, interpretability, and user
adaptability via transfer learning. Our results show that RL-Net achieves a
favorable trade-off, maintaining strong performance (93.03% F1) while
significantly reducing rule complexity. We identify optimization challenges
specific to rule pruning and hierarchy bias and propose stability-enhancing
modifications. Compared to MIRA and XentricAI, RL-Net emerges as a practical
middle ground between transparency and performance. This study highlights the
real-world feasibility of neuro-symbolic models for interpretable HGR and
offers insights for extending explainable AI to edge-deployable sensing
systems.

</details>


### [101] [Active Learning for Forecasting Severity among Patients with Post Acute Sequelae of SARS-CoV-2](https://arxiv.org/abs/2506.22444)
*Jing Wang,Amar Sra,Jeremy C. Weiss*

Main category: cs.LG

TL;DR: 本文引入含文本时间序列特征的18例PASC患者公开队列，提出主动注意力网络预测临床风险和识别进展事件，结合人类专业知识与主动学习提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: SARS - CoV - 2后遗症（PASC）长期影响给全球医疗系统带来挑战，传统基于结构化数据的模型难以捕捉PASC细微进展，需要准确识别进展事件以进行有效患者管理和资源分配。

Method: 引入含基于大语言模型Llama - 3.1 - 70B - Instruct文本时间序列特征的18例PASC患者公开队列，临床专家进行风险标注，提出主动注意力网络，结合人类专业知识与主动学习。

Result: 未提及。

Conclusion: 有望提高临床风险预测准确性，用更少标注识别进展事件，改善SARS - CoV - 2患者护理和决策。

Abstract: The long-term effects of Postacute Sequelae of SARS-CoV-2, known as PASC,
pose a significant challenge to healthcare systems worldwide. Accurate
identification of progression events, such as hospitalization and reinfection,
is essential for effective patient management and resource allocation. However,
traditional models trained on structured data struggle to capture the nuanced
progression of PASC. In this study, we introduce the first publicly available
cohort of 18 PASC patients, with text time series features based on Large
Language Model Llama-3.1-70B-Instruct and clinical risk annotated by clinical
expert. We propose an Active Attention Network to predict the clinical risk and
identify progression events related to the risk. By integrating human expertise
with active learning, we aim to enhance clinical risk prediction accuracy and
enable progression events identification with fewer number of annotation. The
ultimate goal is to improves patient care and decision-making for SARS-CoV-2
patient.

</details>


### [102] [Hierarchical Adversarially-Resilient Multi-Agent Reinforcement Learning for Cyber-Physical Systems Security](https://arxiv.org/abs/2506.22445)
*Saad Alqithami*

Main category: cs.LG

TL;DR: 本文提出用于提升CPS安全性的HAMARL框架，实验表明其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: CPS易受复杂网络威胁，传统安全方法不足。

Method: 引入HAMARL框架，采用分层结构和对抗训练循环。

Result: 在模拟工业物联网测试平台上，HAMARL显著提高攻击检测准确率、减少响应时间、确保运营连续性。

Conclusion: 分层多智能体协调与对抗感知训练结合能增强下一代CPS的弹性和安全性。

Abstract: Cyber-Physical Systems play a critical role in the infrastructure of various
sectors, including manufacturing, energy distribution, and autonomous
transportation systems. However, their increasing connectivity renders them
highly vulnerable to sophisticated cyber threats, such as adaptive and zero-day
attacks, against which traditional security methods like rule-based intrusion
detection and single-agent reinforcement learning prove insufficient. To
overcome these challenges, this paper introduces a novel Hierarchical
Adversarially-Resilient Multi-Agent Reinforcement Learning (HAMARL) framework.
HAMARL employs a hierarchical structure consisting of local agents dedicated to
subsystem security and a global coordinator that oversees and optimizes
comprehensive, system-wide defense strategies. Furthermore, the framework
incorporates an adversarial training loop designed to simulate and anticipate
evolving cyber threats, enabling proactive defense adaptation. Extensive
experimental evaluations conducted on a simulated industrial IoT testbed
indicate that HAMARL substantially outperforms traditional multi-agent
reinforcement learning approaches, significantly improving attack detection
accuracy, reducing response times, and ensuring operational continuity. The
results underscore the effectiveness of combining hierarchical multi-agent
coordination with adversarially-aware training to enhance the resilience and
security of next-generation CPS.

</details>


### [103] [EAGLE: Efficient Alignment of Generalized Latent Embeddings for Multimodal Survival Prediction with Interpretable Attribution Analysis](https://arxiv.org/abs/2506.22446)
*Aakash Tripathi,Asim Waqas,Matthew B. Schabath,Yasin Yilmaz,Ghulam Rasool*

Main category: cs.LG

TL;DR: 提出EAGLE框架用于癌症生存预测，解决现有多模态方法的问题，在三种癌症上评估，结合性能与可解释性，提供可扩展解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有多模态癌症生存预测方法存在融合策略简单、计算需求大、缺乏可解释性等问题，阻碍临床应用。

Method: 提出EAGLE框架，采用基于注意力的多模态融合和综合归因分析，有动态跨模态注意力机制、大幅降维、三种归因方法和统一管道四个创新点。

Result: 在三种癌症的911名患者上评估，高风险个体更依赖不良影像特征，低风险患者各模态贡献均衡，风险分层识别出有临床意义的组，中位生存有4 - 5倍差异。

Conclusion: EAGLE结合先进性能与临床可解释性，缩小了先进AI能力与实际医疗应用的差距，为多模态生存预测提供可扩展解决方案，提高预后准确性和医生对自动预测的信任。

Abstract: Accurate cancer survival prediction requires integration of diverse data
modalities that reflect the complex interplay between imaging, clinical
parameters, and textual reports. However, existing multimodal approaches suffer
from simplistic fusion strategies, massive computational requirements, and lack
of interpretability-critical barriers to clinical adoption. We present EAGLE
(Efficient Alignment of Generalized Latent Embeddings), a novel deep learning
framework that addresses these limitations through attention-based multimodal
fusion with comprehensive attribution analysis. EAGLE introduces four key
innovations: (1) dynamic cross-modal attention mechanisms that learn
hierarchical relationships between modalities, (2) massive dimensionality
reduction (99.96%) while maintaining predictive performance, (3) three
complementary attribution methods providing patient-level interpretability, and
(4) a unified pipeline enabling seamless adaptation across cancer types. We
evaluated EAGLE on 911 patients across three distinct malignancies:
glioblastoma (GBM, n=160), intraductal papillary mucinous neoplasms (IPMN,
n=171), and non-small cell lung cancer (NSCLC, n=580). Patient-level analysis
showed high-risk individuals relied more heavily on adverse imaging features,
while low-risk patients demonstrated balanced modality contributions. Risk
stratification identified clinically meaningful groups with 4-fold (GBM) to
5-fold (NSCLC) differences in median survival, directly informing treatment
intensity decisions. By combining state-of-the-art performance with clinical
interpretability, EAGLE bridges the gap between advanced AI capabilities and
practical healthcare deployment, offering a scalable solution for multimodal
survival prediction that enhances both prognostic accuracy and physician trust
in automated predictions.

</details>


### [104] [FF-INT8: Efficient Forward-Forward DNN Training on Edge Devices with INT8 Precision](https://arxiv.org/abs/2506.22771)
*Jingxiao Ma,Priyadarshini Panda,Sherief Reda*

Main category: cs.LG

TL;DR: 提出INT8量化训练方法结合FF算法及前瞻方案，在NVIDIA Jetson Orin Nano板实验中实现训练加速、节能和内存使用减少，精度有竞争力。


<details>
  <summary>Details</summary>
Motivation: 传统反向传播在资源受限边缘设备上存在时间和能耗低效问题，低精度量化训练应用研究少，FF算法有潜力但有局限。

Method: 提出INT8量化训练方法利用FF逐层策略稳定梯度量化，提出前瞻方案改进FF。

Result: 在NVIDIA Jetson Orin Nano板实验中训练速度快4.6%，节能8.3%，内存使用减少27.0%，精度有竞争力。

Conclusion: 所提方法能在资源受限设备上有效提升训练效率，减少能耗和内存使用，同时保证精度。

Abstract: Backpropagation has been the cornerstone of neural network training for
decades, yet its inefficiencies in time and energy consumption limit its
suitability for resource-constrained edge devices. While low-precision neural
network quantization has been extensively researched to speed up model
inference, its application in training has been less explored. Recently, the
Forward-Forward (FF) algorithm has emerged as a promising alternative to
backpropagation, replacing the backward pass with an additional forward pass.
By avoiding the need to store intermediate activations for backpropagation, FF
can reduce memory footprint, making it well-suited for embedded devices. This
paper presents an INT8 quantized training approach that leverages FF's
layer-by-layer strategy to stabilize gradient quantization. Furthermore, we
propose a novel "look-ahead" scheme to address limitations of FF and improve
model accuracy. Experiments conducted on NVIDIA Jetson Orin Nano board
demonstrate 4.6% faster training, 8.3% energy savings, and 27.0% reduction in
memory usage, while maintaining competitive accuracy compared to the
state-of-the-art.

</details>


### [105] [Vision Transformers for Multi-Variable Climate Downscaling: Emulating Regional Climate Models with a Shared Encoder and Multi-Decoder Architecture](https://arxiv.org/abs/2506.22447)
*Fabio Merizzi,Harilaos Loukos*

Main category: cs.LG

TL;DR: 提出多任务、多变量ViT架构1EMD进行气候降尺度，优于单变量模型且提高计算效率。


<details>
  <summary>Details</summary>
Motivation: GCMs空间分辨率粗，RCMs计算成本高、灵活性有限，现有深度学习单变量模型有局限。

Method: 提出具有共享编码器和特定变量解码器的多任务、多变量ViT架构1EMD，联合预测三个关键气候变量。

Result: 多变量方法实现了正的跨变量知识转移，始终优于相同条件下训练的单变量基线，提高了计算效率。

Conclusion: 多变量建模对高分辨率气候降尺度有效。

Abstract: Global Climate Models (GCMs) are critical for simulating large-scale climate
dynamics, but their coarse spatial resolution limits their applicability in
regional studies. Regional Climate Models (RCMs) refine this through dynamic
downscaling, albeit at considerable computational cost and with limited
flexibility. While deep learning has emerged as an efficient data-driven
alternative, most existing studies have focused on single-variable models that
downscale one variable at a time. This approach can lead to limited contextual
awareness, redundant computation, and lack of cross-variable interaction. Our
study addresses these limitations by proposing a multi-task, multi-variable
Vision Transformer (ViT) architecture with a shared encoder and
variable-specific decoders (1EMD). The proposed architecture jointly predicts
three key climate variables: surface temperature (tas), wind speed (sfcWind),
and 500 hPa geopotential height (zg500), directly from GCM-resolution inputs,
emulating RCM-scale downscaling over Europe. We show that our multi-variable
approach achieves positive cross-variable knowledge transfer and consistently
outperforms single-variable baselines trained under identical conditions, while
also improving computational efficiency. These results demonstrate the
effectiveness of multi-variable modeling for high-resolution climate
downscaling.

</details>


### [106] [FairMarket-RL: LLM-Guided Fairness Shaping for Multi-Agent Reinforcement Learning in Peer-to-Peer Markets](https://arxiv.org/abs/2506.22708)
*Shrenik Jadhav,Birva Sevak,Srijita Das,Akhtar Hussain,Wencong Su,Van-Hai Bui*

Main category: cs.LG

TL;DR: 本文提出FairMarket - RL框架，结合大语言模型和强化学习实现公平交易，在模拟P2P微电网中表现良好，具有可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有P2P交易方法缺乏确保公平性的强大框架，需要新的方案保障公平交易。

Method: 提出FairMarket - RL框架，用大语言模型作为实时公平性评判器，通过预定λ系数将公平性分数整合到代理奖励中，采用独立近端策略优化算法训练代理。

Result: 代理实现公平结果，满足超90%买家需求，维持公平卖家利润，FTB和FBS分数超0.8，公平反馈改善收敛性，减少买家短缺，缩小卖家利润差距。

Conclusion: FairMarket - RL为分散能源系统的自主交易提供了可扩展、以公平为导向的解决方案。

Abstract: Peer-to-peer (P2P) trading is increasingly recognized as a key mechanism for
decentralized market regulation, yet existing approaches often lack robust
frameworks to ensure fairness. This paper presents FairMarket-RL, a novel
hybrid framework that combines Large Language Models (LLMs) with Reinforcement
Learning (RL) to enable fairness-aware trading agents. In a simulated P2P
microgrid with multiple sellers and buyers, the LLM acts as a real-time
fairness critic, evaluating each trading episode using two metrics:
Fairness-To-Buyer (FTB) and Fairness-Between-Sellers (FBS). These fairness
scores are integrated into agent rewards through scheduled
{\lambda}-coefficients, forming an adaptive LLM-guided reward shaping loop that
replaces brittle, rule-based fairness constraints. Agents are trained using
Independent Proximal Policy Optimization (IPPO) and achieve equitable outcomes,
fulfilling over 90% of buyer demand, maintaining fair seller margins, and
consistently reaching FTB and FBS scores above 0.80. The training process
demonstrates that fairness feedback improves convergence, reduces buyer
shortfalls, and narrows profit disparities between sellers. With its
language-based critic, the framework scales naturally, and its extension to a
large power distribution system with household prosumers illustrates its
practical applicability. FairMarket-RL thus offers a scalable, equity-driven
solution for autonomous trading in decentralized energy systems.

</details>


### [107] [Mirror Descent Policy Optimisation for Robust Constrained Markov Decision Processes](https://arxiv.org/abs/2506.23165)
*David Bossens,Atsushi Nitanda*

Main category: cs.LG

TL;DR: 本文提出用于鲁棒约束马尔可夫决策过程的镜像下降策略优化方法，得到不同设置下的收敛率，实验验证了该方法的优势。


<details>
  <summary>Details</summary>
Motivation: 安全是强化学习系统的基本要求，鲁棒约束马尔可夫决策过程框架可学习满足长期约束并在认知不确定性下提供保证的策略，需提出优化方法。

Method: 利用策略梯度技术在表示约束马尔可夫决策过程的拉格朗日函数上同时优化策略（作为最大化者）和转移核（作为对抗性最小化者）。

Result: 在基于神谕和基于样本的RCMDP设置下分别获得不同的收敛率，实验证实了镜像下降策略优化在约束和无约束优化中的好处，在鲁棒性测试中比基线策略优化算法有显著改进。

Conclusion: 镜像下降策略优化方法在鲁棒约束马尔可夫决策过程中有效，能提高鲁棒性。

Abstract: Safety is an essential requirement for reinforcement learning systems. The
newly emerging framework of robust constrained Markov decision processes allows
learning policies that satisfy long-term constraints while providing guarantees
under epistemic uncertainty. This paper presents mirror descent policy
optimisation for robust constrained Markov decision processes (RCMDPs), making
use of policy gradient techniques to optimise both the policy (as a maximiser)
and the transition kernel (as an adversarial minimiser) on the Lagrangian
representing a constrained MDP. In the oracle-based RCMDP setting, we obtain an
$\mathcal{O}\left(\frac{1}{T}\right)$ convergence rate for the squared distance
as a Bregman divergence, and an $\mathcal{O}\left(e^{-T}\right)$ convergence
rate for entropy-regularised objectives. In the sample-based RCMDP setting, we
obtain an $\tilde{\mathcal{O}}\left(\frac{1}{T^{1/3}}\right)$ convergence rate.
Experiments confirm the benefits of mirror descent policy optimisation in
constrained and unconstrained optimisation, and significant improvements are
observed in robustness tests when compared to baseline policy optimisation
algorithms.

</details>


### [108] [Stabilization of industrial processes with time series machine learning](https://arxiv.org/abs/2506.22502)
*Matvei Anoshin,Olga Tsurkan,Vadim Lopatkin,Leonid Fedichkin*

Main category: cs.LG

TL;DR: 提出含两个神经网络的简单管道，在温度控制稳定性上比普通求解器提高约3倍。


<details>
  <summary>Details</summary>
Motivation: 时间序列过程的稳定化问题在各工业领域普遍存在，应用机器学习解决该问题可提高稳定化质量并减少计算资源消耗。

Method: 提出由oracle predictor和optimizer两个神经网络组成的简单管道，将逐点值优化替换为神经网络训练问题。

Result: 在温度控制稳定性方面比普通求解器提高约3倍。

Conclusion: 所提出的方法能有效提高时间序列过程的稳定性。

Abstract: The stabilization of time series processes is a crucial problem that is
ubiquitous in various industrial fields. The application of machine learning to
its solution can have a decisive impact, improving both the quality of the
resulting stabilization with less computational resources required. In this
work, we present a simple pipeline consisting of two neural networks: the
oracle predictor and the optimizer, proposing a substitution of the point-wise
values optimization to the problem of the neural network training, which
successfully improves stability in terms of the temperature control by about 3
times compared to ordinary solvers.

</details>


### [109] [Exploration Behavior of Untrained Policies](https://arxiv.org/abs/2506.22566)
*Jacob Adamczyk*

Main category: cs.LG

TL;DR: 研究深度神经策略架构如何在训练前隐式塑造强化学习探索，建立用策略初始化理解早期训练探索行为的框架。


<details>
  <summary>Details</summary>
Motivation: 探索是强化学习中的基本挑战，特别是在奖励结构稀疏或对抗性的环境中，需研究策略架构对探索的影响。

Method: 在玩具模型中理论和实证演示从无训练策略生成弹道或扩散轨迹的策略，利用无限宽度网络理论和连续时间极限。

Result: 无训练策略返回相关动作，产生非平凡的状态访问分布，揭示标准架构对应轨迹的分布。

Conclusion: 建立了使用策略初始化作为设计工具来理解早期训练探索行为的理论和实验框架。

Abstract: Exploration remains a fundamental challenge in reinforcement learning (RL),
particularly in environments with sparse or adversarial reward structures. In
this work, we study how the architecture of deep neural policies implicitly
shapes exploration before training. We theoretically and empirically
demonstrate strategies for generating ballistic or diffusive trajectories from
untrained policies in a toy model. Using the theory of infinite-width networks
and a continuous-time limit, we show that untrained policies return correlated
actions and result in non-trivial state-visitation distributions. We discuss
the distributions of the corresponding trajectories for a standard
architecture, revealing insights into inductive biases for tackling
exploration. Our results establish a theoretical and experimental framework for
using policy initialization as a design tool to understand exploration behavior
in early training.

</details>


### [110] [DistShap: Scalable GNN Explanations with Distributed Shapley Values](https://arxiv.org/abs/2506.22668)
*Selahattin Akkas,Aditya Devarakonda,Ariful Azad*

Main category: cs.LG

TL;DR: 提出DistShap并行算法解决GNN预测归因计算成本高问题，在准确性上表现出色且能扩展到数百万特征的GNN模型。


<details>
  <summary>Details</summary>
Motivation: 随着GNN广泛应用，解释其预测变得重要，但将预测归因于特定边或特征计算成本高。

Method: 提出DistShap并行算法，在分布式环境中采样子图，跨GPU并行执行GNN推理，解决分布式最小二乘问题计算边重要性得分。

Result: DistShap在准确性上优于大多数现有GNN解释方法，能在NERSC Perlmutter超级计算机上使用多达128个GPU扩展到数百万特征的GNN模型。

Conclusion: DistShap算法有效解决了GNN预测归因的计算成本问题，有较好性能和扩展性。

Abstract: With the growing adoption of graph neural networks (GNNs), explaining their
predictions has become increasingly important. However, attributing predictions
to specific edges or features remains computationally expensive. For example,
classifying a node with 100 neighbors using a 3-layer GNN may involve
identifying important edges from millions of candidates contributing to the
prediction. To address this challenge, we propose DistShap, a parallel
algorithm that distributes Shapley value-based explanations across multiple
GPUs. DistShap operates by sampling subgraphs in a distributed setting,
executing GNN inference in parallel across GPUs, and solving a distributed
least squares problem to compute edge importance scores. DistShap outperforms
most existing GNN explanation methods in accuracy and is the first to scale to
GNN models with millions of features by using up to 128 GPUs on the NERSC
Perlmutter supercomputer.

</details>


### [111] [The Hidden Link Between RLHF and Contrastive Learning](https://arxiv.org/abs/2506.22578)
*Xufei Lv,Haoyuan Sun,Xuefeng Bai,Min Zhang,Houde Liu,Kehai Chen*

Main category: cs.LG

TL;DR: 本文从互信息最大化视角解读了RLHF和DPO，揭示其与对比学习联系，指出RLHF不足，提出MIO方法，经理论分析和实验评估，MIO性能良好。


<details>
  <summary>Details</summary>
Motivation: 从新视角理解大语言模型与人类价值观对齐方法（RLHF和DPO），解决RLHF在激励推理能力方面的不足。

Method: 从互信息最大化角度解读RLHF和DPO，用Jensen - Shannon互信息估计器替代DV/MINE界，提出MIO方法。

Result: MIO缓解了DPO中后期选择似然下降问题，在多种推理和数学基准测试中表现有竞争力或更优。

Conclusion: 从互信息最大化角度能有效理解和改进大语言模型与人类价值观对齐方法，MIO是一种可行的改进方案。

Abstract: Alignment of large language models (LLMs) with human values has recently
garnered significant attention, with prominent examples including the canonical
yet costly Reinforcement Learning from Human Feedback (RLHF) and the simple
Direct Preference Optimization (DPO). In this work, we demonstrate that both
RLHF and DPO can be interpreted from the perspective of mutual information (MI)
maximization, uncovering a profound connection to contrastive learning. Within
this framework, both RLHF and DPO can be viewed as methods that perform
contrastive learning based on the positive and negative samples derived from
the base model, leveraging the Donsker-Varadhan (DV) lower bound on MI
(equivalently, the MINE estimator). This paradigm further explains why RLHF may
not intrinsically incentivize reasoning capacities in LLMs beyond what is
already present in the base model. Building on this perspective, we replace the
DV/MINE bound with the Jensen-Shannon MI estimator and propose Mutual
Information Optimization (MIO). Comprehensive theoretical analysis and
extensive empirical evaluations demonstrate that MIO mitigates the late-stage
decline in chosen-likelihood observed in DPO, achieving competitive or superior
performance across various challenging reasoning and mathematical benchmarks.
We will release the model and code upon acceptance.

</details>


### [112] [Are Fast Methods Stable in Adversarially Robust Transfer Learning?](https://arxiv.org/abs/2506.22602)
*Joshua C. Zhao,Saurabh Bagchi*

Main category: cs.LG

TL;DR: 本文探讨FGSM在对抗性微调中的应用，发现其比从头训练更稳定，相比PGD训练时间少且性能损失小。


<details>
  <summary>Details</summary>
Motivation: 迁移学习实现对抗鲁棒性时，微调阶段的对抗训练计算成本高，需改进。

Method: 重新审视快速梯度符号法（FGSM）在鲁棒迁移学习中的应用。

Result: FGSM在对抗性微调中更稳定，结合参数高效微调方法稳定性增强，与PGD相比训练时间少且性能损失小。

Conclusion: FGSM在对抗性鲁棒迁移学习中不仅更高效，而且表现良好。

Abstract: Transfer learning is often used to decrease the computational cost of model
training, as fine-tuning a model allows a downstream task to leverage the
features learned from the pre-training dataset and quickly adapt them to a new
task. This is particularly useful for achieving adversarial robustness, as
adversarially training models from scratch is very computationally expensive.
However, high robustness in transfer learning still requires adversarial
training during the fine-tuning phase, which requires up to an order of
magnitude more time than standard fine-tuning. In this work, we revisit the use
of the fast gradient sign method (FGSM) in robust transfer learning to improve
the computational cost of adversarial fine-tuning. We surprisingly find that
FGSM is much more stable in adversarial fine-tuning than when training from
scratch. In particular, FGSM fine-tuning does not suffer from any issues with
catastrophic overfitting at standard perturbation budgets of $\varepsilon=4$ or
$\varepsilon=8$. This stability is further enhanced with parameter-efficient
fine-tuning methods, where FGSM remains stable even up to $\varepsilon=32$ for
linear probing. We demonstrate how this stability translates into performance
across multiple datasets. Compared to fine-tuning with the more commonly used
method of projected gradient descent (PGD), on average, FGSM only loses 0.39%
and 1.39% test robustness for $\varepsilon=4$ and $\varepsilon=8$ while using
$4\times$ less training time. Surprisingly, FGSM may not only be a
significantly more efficient alternative to PGD in adversarially robust
transfer learning but also a well-performing one.

</details>


### [113] [Hierarchical Modeling and Architecture Optimization: Review and Unified Framework](https://arxiv.org/abs/2506.22621)
*Paul Saves,Edward Hallé-Hannan,Jasper Bussemaker,Youssef Diouane,Nathalie Bartoli*

Main category: cs.LG

TL;DR: 本文回顾结构化输入空间文献，提出统一框架，引入元变量、部分指定变量和设计空间图，支持代理模型，在SMT 2.0实现并通过应用展示能力。


<details>
  <summary>Details</summary>
Motivation: 模拟问题中混合变量输入的结构化特征给数据表示、建模和优化带来挑战，需统一框架解决。

Method: 提出统一框架，引入元变量、部分指定变量和设计空间图，结合特征建模和图论，支持代理模型并集成层次核和距离。

Result: 方法在开源SMT 2.0中实现，通过复杂系统设计的贝叶斯优化应用，包括绿色飞机架构案例研究展示了能力。

Conclusion: 提出的统一框架能有效处理结构化输入空间的模拟问题，支持复杂系统设计的建模和优化。

Abstract: Simulation-based problems involving mixed-variable inputs frequently feature
domains that are hierarchical, conditional, heterogeneous, or tree-structured.
These characteristics pose challenges for data representation, modeling, and
optimization. This paper reviews extensive literature on these structured input
spaces and proposes a unified framework that generalizes existing approaches.
In this framework, input variables may be continuous, integer, or categorical.
A variable is described as meta if its value governs the presence of other
decreed variables, enabling the modeling of conditional and hierarchical
structures.
  We further introduce the concept of partially-decreed variables, whose
activation depends on contextual conditions. To capture these inter-variable
hierarchical relationships, we introduce design space graphs, combining
principles from feature modeling and graph theory. This allows the definition
of general hierarchical domains suitable for describing complex system
architectures. The framework supports the use of surrogate models over such
domains and integrates hierarchical kernels and distances for efficient
modeling and optimization. The proposed methods are implemented in the
open-source Surrogate Modeling Toolbox (SMT 2.0), and their capabilities are
demonstrated through applications in Bayesian optimization for complex system
design, including a case study in green aircraft architecture.

</details>


### [114] [A hierarchical Vovk-Azoury-Warmuth forecaster with discounting for online regression in RKHS](https://arxiv.org/abs/2506.22631)
*Dmitry B. Rokhlin*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the problem of online regression with the unconstrained quadratic
loss against a time-varying sequence of functions from a Reproducing Kernel
Hilbert Space (RKHS). Recently, Jacobsen and Cutkosky (2024) introduced a
discounted Vovk-Azoury-Warmuth (DVAW) forecaster that achieves optimal dynamic
regret in the finite-dimensional case. In this work, we lift their approach to
the non-parametric domain by synthesizing the DVAW framework with a random
feature approximation. We propose a fully adaptive, hierarchical algorithm,
which we call H-VAW-D (Hierarchical Vovk-Azoury-Warmuth with Discounting), that
learns both the discount factor and the number of random features. We prove
that this algorithm, which has a per-iteration computational complexity of
$O(T\ln T)$, achieves an expected dynamic regret of $O(T^{2/3}P_T^{1/3} +
\sqrt{T}\ln T)$, where $P_T$ is the functional path length of a comparator
sequence.

</details>


### [115] [FedRef: Communication-Efficient Bayesian Fine Tuning with Reference Model](https://arxiv.org/abs/2506.23210)
*Taehwan Yoon,Bongjun Choi*

Main category: cs.LG

TL;DR: 本文提出基于参考模型的联邦学习用于最优微调，克服每一轮的灾难性遗忘问题，实现高模型性能和低计算成本。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在模型性能上可能无法满足用户期望，且用户需求多样，需对模型进行优化、微调或个性化。

Method: 提出基于参考模型的联邦学习用于最优微调，源自贝叶斯参数高效迁移学习，包含最优近端项，利用包含先前模型参数的参考模型克服灾难性遗忘问题。

Result: 该方法实现了高模型性能和低计算成本。

Conclusion: 基于参考模型的联邦学习用于最优微调能有效解决模型优化挑战。

Abstract: Federated learning(FL) is used for distributed scenarios to train artificial
intelligence(AI) models while ensuring users' privacy. In federated learning
scenario, the server generally never knows about users' data. This type of
concept makes the AI training process efficient in terms of data privacy.
However, regarding model performance, federated AI models may not sufficiently
satisfy AI users' expectations. Furthermore, AI users have a wide range of
different needs. It is not easy to satisfy the whole users needs. These types
of issues can be addressed through AI model optimization, fine-tuning, or
personalization to achieve optimal model performance. To address model
optimization challenges, we propose reference model-based federated learning
for optimal fine-tuning, which overcomes catastrophic forgetting in each round.
This method is derived from Bayesian parameter-efficient transfer learning,
which includes an optimal proximal term and enables overcoming the catastrophic
forgetting issue in each round by utilizing a reference model that incorporates
previous model parameters. As a result, this method achieves both high model
performance and low computing cost.

</details>


### [116] [Layer Importance for Mathematical Reasoning is Forged in Pre-Training and Invariant after Post-Training](https://arxiv.org/abs/2506.22638)
*Aadim Nepal,Safal Shrestha,Anubhav Shrestha,Minwu Kim,Keith Ross*

Main category: cs.LG

TL;DR: 研究大语言模型数学推理能力训练后改进的来源，发现数学推理有特定层重要性结构且跨训练范式持续存在。


<details>
  <summary>Details</summary>
Motivation: 不清楚大语言模型数学推理能力改进是源于Transformer层的重大变化还是小调整，想探究该问题。

Method: 通过系统的逐层消融实验，在数学推理基准上测试基础、指令调优、知识蒸馏和强化学习变体。

Result: 数学推理有特定层重要性结构，移除这些层准确率最多降80%，非数学任务无关键层。

Conclusion: 数学推理需要预训练中出现的专门层，非推理任务则不需要，关键层也是发生主要表征转换的层。

Abstract: Large language models can exhibit improved mathematical reasoning
capabilities following post-training with instruction tuning, reinforcement
learning, or knowledge distillation. However, it remains unclear whether these
improvements are driven by major changes in transformer layers or from minor
adjustments that leave the relative layer importance structures of the base
model largely unchanged. We investigate this question through systematic
layer-wise ablation experiments, examining base, instruction-tuned,
knowledge-distilled, and reinforcement learning variants on mathematical
reasoning benchmarks. Our findings show that mathematical reasoning gives rise
to a specific layer importance structure, and this structure persists across
all post-training paradigms. Removal of such layers causes accuracy drops of up
to 80%. In contrast, non-mathematical tasks like factual recall exhibit no
critical layers. This distinction suggests that mathematical reasoning requires
specialized layers that emerge during pre-training, while other non-reasoning
tasks do not. From an information-theoretic perspective, we also observe that
these critical layers are the same layers where major representational
transformation occurs.

</details>


### [117] [Cost-effective Reduced-Order Modeling via Bayesian Active Learning](https://arxiv.org/abs/2506.22645)
*Amir Hossein Rahmati,Nathan M. Urban,Byung-Jun Yoon,Xiaoning Qian*

Main category: cs.LG

TL;DR: 提出基于贝叶斯POD的主动学习框架BayPOD - AL加速复杂系统动力学求解，实验证明其有效性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习替代模型依赖大量训练数据，限制在现实问题中的应用，需新方法有效学习复杂系统降阶模型。

Method: 提出基于不确定性感知的贝叶斯POD方法的主动学习框架BayPOD - AL。

Result: 在预测杆温度演化实验中，BayPOD - AL能有效推荐信息数据，降低构建训练数据集的计算成本，在高时间分辨率数据集上也表现出泛化性和效率。

Conclusion: BayPOD - AL在学习复杂系统降阶模型方面是有效且高效的。

Abstract: Machine Learning surrogates have been developed to accelerate solving systems
dynamics of complex processes in different science and engineering
applications. To faithfully capture governing systems dynamics, these methods
rely on large training datasets, hence restricting their applicability in
real-world problems. In this work, we propose BayPOD-AL, an active learning
framework based on an uncertainty-aware Bayesian proper orthogonal
decomposition (POD) approach, which aims to effectively learn reduced-order
models from high-fidelity full-order models representing complex systems.
Experimental results on predicting the temperature evolution over a rod
demonstrate BayPOD-AL's effectiveness in suggesting the informative data and
reducing computational cost related to constructing a training dataset compared
to other uncertainty-guided active learning strategies. Furthermore, we
demonstrate BayPOD-AL's generalizability and efficiency by evaluating its
performance on a dataset of higher temporal resolution than the training
dataset.

</details>


### [118] [Learning Stochastic Multiscale Models](https://arxiv.org/abs/2506.22655)
*Andrew F. Ilersich,Prasanth B. Nair*

Main category: cs.LG

TL;DR: 提出从观测数据中学习随机多尺度模型的方法，该方法比直接数值模拟和封闭型模型有更好的预测精度。


<details>
  <summary>Details</summary>
Motivation: 物理科学中动力学系统需解决大范围长度和时间尺度问题，直接数值模拟有计算挑战。

Method: 在粗网格上解析状态，引入辅助状态捕捉未解析尺度的影响，使用无正向求解器的摊销变分推理方法学习多尺度模型参数。

Result: 数值研究表明，学习的多尺度模型比同等分辨率下的直接数值模拟和封闭型模型有更优的预测精度。

Conclusion: 提出的学习随机多尺度模型的方法有效且有更好的预测性能。

Abstract: The physical sciences are replete with dynamical systems that require the
resolution of a wide range of length and time scales. This presents significant
computational challenges since direct numerical simulation requires
discretization at the finest relevant scales, leading to a high-dimensional
state space. In this work, we propose an approach to learn stochastic
multiscale models in the form of stochastic differential equations directly
from observational data. Our method resolves the state on a coarse mesh while
introducing an auxiliary state to capture the effects of unresolved scales. We
learn the parameters of the multiscale model using a modern forward-solver-free
amortized variational inference method. Our approach draws inspiration from
physics-based multiscale modeling approaches, such as large-eddy simulation in
fluid dynamics, while learning directly from data. We present numerical studies
to demonstrate that our learned multiscale models achieve superior predictive
accuracy compared to direct numerical simulation and closure-type models at
equivalent resolution.

</details>


### [119] [Mitigating Semantic Collapse in Generative Personalization with a Surprisingly Simple Test-Time Embedding Adjustment](https://arxiv.org/abs/2506.22685)
*Anh Bui,Trang Vu,Trung Le,Junae Kim,Tamas Abraham,Rollin Omari,Amar Kaur,Dinh Phung*

Main category: cs.LG

TL;DR: 研究生成式个性化中的语义坍缩问题，提出无训练方法缓解该问题。


<details>
  <summary>Details</summary>
Motivation: 解决生成式个性化中语义坍缩问题，该问题会降低输入提示语义丰富度和输出图像质量。

Method: 提出无训练方法，在推理时调整预训练嵌入的大小和方向。

Result: 方法广泛适用于不同个性化方法，在多种用例中显著改善文本 - 图像对齐。

Conclusion: 提出的方法能有效缓解语义坍缩问题。

Abstract: In this paper, we investigate the semantic collapsing problem in generative
personalization, an under-explored topic where the learned visual concept
($V^*$) gradually shifts from its original textual meaning and comes to
dominate other concepts in multi-concept input prompts. This issue not only
reduces the semantic richness of complex input prompts like "a photo of $V^*$
wearing glasses and playing guitar" into simpler, less contextually rich forms
such as "a photo of $V^*$" but also leads to simplified output images that fail
to capture the intended concept.
  We identify the root cause as unconstrained optimisation, which allows the
learned embedding $V^*$ to drift arbitrarily in the embedding space, both in
direction and magnitude. To address this, we propose a simple yet effective
training-free method that adjusts the magnitude and direction of pre-trained
embedding at inference time, effectively mitigating the semantic collapsing
problem. Our method is broadly applicable across different personalization
methods and demonstrates significant improvements in text-image alignment in
diverse use cases. Our code is anonymously published at
https://anonymous.4open.science/r/Embedding-Adjustment.

</details>


### [120] [Residual Matrix Transformers: Scaling the Size of the Residual Stream](https://arxiv.org/abs/2506.22696)
*Brian Mak,Jeffrey Flanigan*

Main category: cs.LG

TL;DR: 提出残差矩阵变压器（RMT）模型，用外积记忆矩阵替换变压器的残差流，RMT有多项优势且性能超变压器。


<details>
  <summary>Details</summary>
Motivation: 考虑改变残差流中信息的检索和存储机制，提升性能。

Method: 用外积记忆矩阵替换变压器的残差流，构建RMT模型。

Result: RMT可独立扩展残差流大小，减少计算量、参数和训练令牌，下游评估表现更好。

Conclusion: RMT能更高效扩展残差流，改善方差传播特性。

Abstract: The residual stream acts as a memory bus where transformer layers both store
and access features (Elhage et al., 2021). We consider changing the mechanism
for retrieving and storing information in the residual stream, and replace the
residual stream of the transformer with an outer product memory matrix
(Kohonen, 1972, Anderson, 1972). We call this model the Residual Matrix
Transformer (RMT). We find that the RMT enjoys a number of attractive
properties: 1) the size of the residual stream can be scaled independently of
compute and model size, improving performance, 2) the RMT can achieve the same
loss as the transformer with 58% fewer FLOPS, 25% fewer parameters, and 41%
fewer training tokens tokens, and 3) the RMT outperforms the transformer on
downstream evaluations. We theoretically analyze the transformer and the RMT,
and show that the RMT allows for more efficient scaling of the residual stream,
as well as improved variance propagation properties. Code for this project can
be found at https://github.com/bmac3/residual-matrix-transformer.

</details>


### [121] [Generalized Linear Mode Connectivity for Transformers](https://arxiv.org/abs/2506.22712)
*Alexander Theus,Alessandro Cabodi,Sotiris Anagnostidis,Antonio Orvieto,Sidak Pal Singh,Valentina Boeva*

Main category: cs.LG

TL;DR: 本文提出统一框架捕捉四类对称性，发现独立训练的Vision Transformers和GPT - 2模型间的低零障碍线性插值路径，强调对称感知分析对理解模型空间几何的重要性。


<details>
  <summary>Details</summary>
Motivation: 先前工作聚焦神经元重排序有局限，无法捕捉现代架构如Transformers的丰富对称性，需更好方法理解神经网络损失景观几何。

Method: 引入统一框架，捕捉排列、半排列、正交变换和一般可逆映射四类对称性。

Result: 首次发现独立训练的Vision Transformers和GPT - 2模型间的低和零障碍线性插值路径。

Conclusion: 损失景观有更深层结构，对称感知分析对理解模型空间几何很重要。

Abstract: Understanding the geometry of neural network loss landscapes is a central
question in deep learning, with implications for generalization and
optimization. A striking phenomenon is linear mode connectivity (LMC), where
independently trained models can be connected by low- or zero-loss paths,
despite appearing to lie in separate loss basins. However, this is often
obscured by symmetries in parameter space -- such as neuron permutations --
which make functionally equivalent models appear dissimilar. Prior work has
predominantly focused on neuron re-ordering through permutations, but such
approaches are limited in scope and fail to capture the richer symmetries
exhibited by modern architectures such as Transformers. In this work, we
introduce a unified framework that captures four symmetry classes:
permutations, semi-permutations, orthogonal transformations, and general
invertible maps -- broadening the set of valid reparameterizations and
subsuming many previous approaches as special cases. Crucially, this
generalization enables, for the first time, the discovery of low- and
zero-barrier linear interpolation paths between independently trained Vision
Transformers and GPT-2 models. These results reveal deeper structure in the
loss landscape and underscore the importance of symmetry-aware analysis for
understanding model space geometry.

</details>


### [122] [Robust Tensor Completion via Gradient Tensor Nulclear L1-L2 Norm for Traffic Data Recovery](https://arxiv.org/abs/2506.22732)
*Hao Shu,Jicheng Li,Tianyv Lei,Lijun Sun*

Main category: cs.LG

TL;DR: 本文提出RTC - GTNLN模型解决交通数据缺失和噪声双重退化问题，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实中交通数据有缺失值和噪声双重退化问题，经典张量补全方法无法处理噪声，现有RTC方法存在凸秩替代过松弛和局部一致性利用不佳问题。

Method: 引入张量L1 - L2范数，开发梯度张量L1 - L2范数，将其集成到RTC框架提出RTC - GTNLN模型。

Result: 在多个真实交通数据集上实验，RTC - GTNLN模型在复杂恢复场景中优于现有方法。

Conclusion: RTC - GTNLN模型能充分利用全局低秩性和局部一致性，有效处理交通数据的双重退化挑战。

Abstract: In real-world scenarios, spatiotemporal traffic data frequently experiences
dual degradation from missing values and noise caused by sensor malfunctions
and communication failures. Therefore, effective data recovery methods are
essential to ensure the reliability of downstream data-driven applications.
while classical tensor completion methods have been widely adopted, they are
incapable of modeling noise, making them unsuitable for complex scenarios
involving simultaneous data missingness and noise interference. Existing Robust
Tensor Completion (RTC) approaches offer potential solutions by separately
modeling the actual tensor data and noise. However, their effectiveness is
often constrained by the over-relaxation of convex rank surrogates and the
suboptimal utilization of local consistency, leading to inadequate model
accuracy. To address these limitations, we first introduce the tensor L1-L2
norm, a novel non-convex tensor rank surrogate that functions as an effective
low-rank representation tool. Leveraging an advanced feature fusion strategy,
we further develop the gradient tensor L1-L2 norm by incorporating the tensor
L1-L2 norm in the gradient domain. By integrating the gradient tensor nuclear
L1-L2 norm into the RTC framework, we propose the Robust Tensor Completion via
Gradient Tensor Nuclear L1-L2 Norm (RTC-GTNLN) model, which not only fully
exploits both global low-rankness and local consistency without trade-off
parameter, but also effectively handles the dual degradation challenges of
missing data and noise in traffic data. Extensive experiments conducted on
multiple real-world traffic datasets demonstrate that the RTC-GTNLN model
consistently outperforms existing state-of-the-art methods in complex recovery
scenarios involving simultaneous missing values and noise.

</details>


### [123] [Multimodal Atmospheric Super-Resolution With Deep Generative Models](https://arxiv.org/abs/2506.22780)
*Dibyajyoti Chakraborty,Haiwen Guan,Jason Stock,Troy Arcomano,Guido Cervone,Romit Maulik*

Main category: cs.LG

TL;DR: 本文将基于分数的扩散模型应用于高维动力系统超分辨率，利用低分辨率和稀疏传感器测量数据，实验证明能准确恢复高维状态，且生成模型可平衡多模态数据影响。


<details>
  <summary>Details</summary>
Motivation: 基于分数的扩散模型训练后可生成新样本并实现零样本条件化，有望用于数据和模型融合，本文将其应用于高维动力系统超分辨率。

Method: 将基于分数的扩散模型概念应用于高维动力系统超分辨率，利用实时低分辨率和稀疏传感器测量数据，还分析了基于分数的采样用于不确定性估计的方法。

Result: 实验在生成ERA5大气数据集的超分辨率任务中，能准确恢复高维状态，且生成模型可在时空重建中平衡多模态数据集的影响。

Conclusion: 基于分数的扩散模型可用于高维动力系统超分辨率，在多源低保真测量下能准确恢复高维状态，生成模型具有平衡多模态数据影响的能力。

Abstract: Score-based diffusion modeling is a generative machine learning algorithm
that can be used to sample from complex distributions. They achieve this by
learning a score function, i.e., the gradient of the log-probability density of
the data, and reversing a noising process using the same. Once trained,
score-based diffusion models not only generate new samples but also enable
zero-shot conditioning of the generated samples on observed data. This promises
a novel paradigm for data and model fusion, wherein the implicitly learned
distributions of pretrained score-based diffusion models can be updated given
the availability of online data in a Bayesian formulation. In this article, we
apply such a concept to the super-resolution of a high-dimensional dynamical
system, given the real-time availability of low-resolution and experimentally
observed sparse sensor measurements from multimodal data. Additional analysis
on how score-based sampling can be used for uncertainty estimates is also
provided. Our experiments are performed for a super-resolution task that
generates the ERA5 atmospheric dataset given sparse observations from a
coarse-grained representation of the same and/or from unstructured experimental
observations of the IGRA radiosonde dataset. We demonstrate accurate recovery
of the high dimensional state given multiple sources of low-fidelity
measurements. We also discover that the generative model can balance the
influence of multiple dataset modalities during spatiotemporal reconstructions.

</details>


### [124] [Riemannian-Geometric Fingerprints of Generative Models](https://arxiv.org/abs/2506.22802)
*Hae Jin Song,Laurent Itti*

Main category: cs.LG

TL;DR: 本文用黎曼几何提出生成模型工件和指纹的新定义，应用于梯度算法计算指纹，在区分多种生成模型上有效。


<details>
  <summary>Details</summary>
Motivation: 服务提供商需认证模型保护IP，用户和执法部门要验证内容来源，且存在模型崩溃威胁，但缺乏正式框架理解生成模型指纹。

Method: 采用几何方法，用黎曼几何提出新定义，将先前工作推广到非欧流形，应用于梯度算法。

Result: 在区分多种生成模型上更有效，涵盖不同数据集、模型架构和模态。

Conclusion: 所提定义显著提升模型归因性能，有实际效果。

Abstract: Recent breakthroughs and rapid integration of generative models (GMs) have
sparked interest in the problem of model attribution and their fingerprints.
For instance, service providers need reliable methods of authenticating their
models to protect their IP, while users and law enforcement seek to verify the
source of generated content for accountability and trust. In addition, a
growing threat of model collapse is arising, as more model-generated data are
being fed back into sources (e.g., YouTube) that are often harvested for
training ("regurgitative training"), heightening the need to differentiate
synthetic from human data. Yet, a gap still exists in understanding generative
models' fingerprints, we believe, stemming from the lack of a formal framework
that can define, represent, and analyze the fingerprints in a principled way.
To address this gap, we take a geometric approach and propose a new definition
of artifact and fingerprint of GMs using Riemannian geometry, which allows us
to leverage the rich theory of differential geometry. Our new definition
generalizes previous work (Song et al., 2024) to non-Euclidean manifolds by
learning Riemannian metrics from data and replacing the Euclidean distances and
nearest-neighbor search with geodesic distances and kNN-based Riemannian center
of mass. We apply our theory to a new gradient-based algorithm for computing
the fingerprints in practice. Results show that it is more effective in
distinguishing a large array of GMs, spanning across 4 different datasets in 2
different resolutions (64 by 64, 256 by 256), 27 model architectures, and 2
modalities (Vision, Vision-Language). Using our proposed definition
significantly improves the performance on model attribution, as well as a
generalization to unseen datasets, model types, and modalities, suggesting its
practical efficacy.

</details>


### [125] [BayesLoRA: Task-Specific Uncertainty in Low-Rank Adapters](https://arxiv.org/abs/2506.22809)
*Cooper Doyle*

Main category: cs.LG

TL;DR: 提出结合MC - Dropout和LoRA的BayesLoRA不确定性量化框架，能为下游工作流提供定制化防护栏，LoRA适配器在微调分布外有放大方差特性可用于决策。


<details>
  <summary>Details</summary>
Motivation: 现有的通用Transformer不确定性方法无法为下游工作流提供定制化防护栏，需要开发新的能让智能体在不确定情况下自省和调整行为的框架。

Method: 将MC - Dropout集成到低秩适配器（LoRA）中，提出BayesLoRA框架。

Result: 从数学和实证上证明LoRA适配器在微调分布外具有放大的方差。

Conclusion: BayesLoRA能为智能体决策提供可靠的置信度估计。

Abstract: We propose BayesLoRA, a task-specific uncertainty quantification framework
that integrates MC-Dropout into Low-Rank Adapters (LoRA). Unlike
general-purpose transformer uncertainty methods, BayesLoRA provides guardrails
tailored to downstream workflows, enabling agents to introspect and modulate
behavior under uncertainty. We demonstrate mathematically and empirically that
LoRA adapters exhibit amplified variance outside fine-tuning distributions,
yielding reliable confidence estimates for agentic decision-making.

</details>


### [126] [ADReFT: Adaptive Decision Repair for Safe Autonomous Driving via Reinforcement Fine-Tuning](https://arxiv.org/abs/2506.23960)
*Mingfei Cheng,Xiaofei Xie,Renzhi Wang,Yuan Zhou,Ming Hu*

Main category: cs.LG

TL;DR: 现有自动驾驶系统在线修复方案有局限，提出ADReFT方法，经预训练和微调后评估显示有更好修复性能。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶系统在线修复方案缺乏泛化性、适应性且过于保守，无法充分降低安全风险和提升驾驶体验。

Method: 提出ADReFT方法，结合基于Transformer的双联合头模型，先进行有粗标注的监督学习预训练，再用强化学习微调。

Result: 评估结果表明ADReFT取得了更好的修复性能。

Conclusion: ADReFT方法能有效提升自动驾驶系统的安全性和修复性能。

Abstract: Autonomous Driving Systems (ADSs) continue to face safety-critical risks due
to the inherent limitations in their design and performance capabilities.
Online repair plays a crucial role in mitigating such limitations, ensuring the
runtime safety and reliability of ADSs. Existing online repair solutions
enforce ADS compliance by transforming unacceptable trajectories into
acceptable ones based on predefined specifications, such as rule-based
constraints or training datasets. However, these approaches often lack
generalizability, adaptability and tend to be overly conservative, resulting in
ineffective repairs that not only fail to mitigate safety risks sufficiently
but also degrade the overall driving experience. To address this issue, we
propose Adaptive Decision Repair (ADReFT), a novel and effective repair method
that identifies safety-critical states through offline learning from failed
tests and generates appropriate mitigation actions to improve ADS safety.
Specifically, ADReFT incorporates a transformer-based model with two joint
heads, State Monitor and Decision Adapter, designed to capture complex driving
environment interactions to evaluate state safety severity and generate
adaptive repair actions. Given the absence of oracles for state safety
identification, we first pretrain ADReFT using supervised learning with coarse
annotations, i.e., labeling states preceding violations as positive samples and
others as negative samples. It establishes ADReFT's foundational capability to
mitigate safety-critical violations, though it may result in somewhat
conservative mitigation strategies. Therefore, we subsequently finetune ADReFT
using reinforcement learning to improve its initial capability and generate
more precise and contextually appropriate repair decisions. Our evaluation
results illustrate that ADReFT achieves better repair performance.

</details>


### [127] [Deep learning 40 years of human migration](https://arxiv.org/abs/2506.22821)
*Thomas Gaskin,Guy J. Abel*

Main category: cs.LG

TL;DR: 本文提出1990年至今230个国家和地区间年度迁移流量和存量的新数据集，用深度循环神经网络估算，有置信区间，表现优于传统方法且开源。


<details>
  <summary>Details</summary>
Motivation: 提供更全面、详细的1990年至今国家和地区间迁移流量和存量数据。

Method: 训练深度循环神经网络，学习18个协变量的流动模式；训练神经网络集合并传递协变量不确定性获取置信区间。

Result: 在各种测试集上验证，显著优于传统方法估算五年流量，时间分辨率显著提高；模型完全开源。

Conclusion: 该方法有效且开源，为未来人类迁移研究提供有价值资源。

Abstract: We present a novel and detailed dataset on origin-destination annual
migration flows and stocks between 230 countries and regions, spanning the
period from 1990 to the present. Our flow estimates are further disaggregated
by country of birth, providing a comprehensive picture of migration over the
last 43 years. The estimates are obtained by training a deep recurrent neural
network to learn flow patterns from 18 covariates for all countries, including
geographic, economic, cultural, societal, and political information. The
recurrent architecture of the neural network means that the entire past can
influence current migration patterns, allowing us to learn long-range temporal
correlations. By training an ensemble of neural networks and additionally
pushing uncertainty on the covariates through the trained network, we obtain
confidence bounds for all our estimates, allowing researchers to pinpoint the
geographic regions most in need of additional data collection. We validate our
approach on various test sets of unseen data, demonstrating that it
significantly outperforms traditional methods estimating five-year flows while
delivering a significant increase in temporal resolution. The model is fully
open source: all training data, neural network weights, and training code are
made public alongside the migration estimates, providing a valuable resource
for future studies of human migration.

</details>


### [128] [xLSTMAD: A Powerful xLSTM-based Method for Anomaly Detection](https://arxiv.org/abs/2506.22837)
*Kamil Faber,Marcin Pietroń,Dominik Żurek,Roberto Corizzo*

Main category: cs.LG

TL;DR: 本文提出xLSTMAD用于多变量时间序列异常检测，评估两种损失函数，在TSB - AD - M基准上表现超23种基线。


<details>
  <summary>Details</summary>
Motivation: 虽xLSTM在多领域成功，但此前未用于异常检测，本文填补该空白。

Method: 提出xLSTMAD，含编码器和解码器，解码器有预测和重构两种变体，研究MSE和SoftDTW两种损失函数。

Result: xLSTM在异常检测中展现出最先进的准确性，优于23种流行的异常检测基线。

Conclusion: 本文首次揭示xLSTM在异常检测中的强大建模能力，为该领域发展铺平道路。

Abstract: The recently proposed xLSTM is a powerful model that leverages expressive
multiplicative gating and residual connections, providing the temporal capacity
needed for long-horizon forecasting and representation learning. This
architecture has demonstrated success in time series forecasting, lossless
compression, and even large-scale language modeling tasks, where its linear
memory footprint and fast inference make it a viable alternative to
Transformers. Despite its growing popularity, no prior work has explored xLSTM
for anomaly detection. In this work, we fill this gap by proposing xLSTMAD, the
first anomaly detection method that integrates a full encoder-decoder xLSTM
architecture, purpose-built for multivariate time series data. Our encoder
processes input sequences to capture historical context, while the decoder is
devised in two separate variants of the method. In the forecasting approach,
the decoder iteratively generates forecasted future values xLSTMAD-F, while the
reconstruction approach reconstructs the input time series from its encoded
counterpart xLSTMAD-R. We investigate the performance of two loss functions:
Mean Squared Error (MSE), and Soft Dynamic Time Warping (SoftDTW) to consider
local reconstruction fidelity and global sequence alignment, respectively. We
evaluate our method on the comprehensive TSB-AD-M benchmark, which spans 17
real-world datasets, using state-of-the-art challenging metrics such as VUS-PR.
In our results, xLSTM showcases state-of-the-art accuracy, outperforming 23
popular anomaly detection baselines. Our paper is the first work revealing the
powerful modeling capabilities of xLSTM for anomaly detection, paving the way
for exciting new developments on this subject. Our code is available at:
https://github.com/Nyderx/xlstmad

</details>


### [129] [Kernel Outlier Detection](https://arxiv.org/abs/2506.22994)
*Can Hakan Dağıdır,Mia Hubert,Peter J. Rousseeuw*

Main category: cs.LG

TL;DR: 提出核异常检测（KOD）方法，适用于高维异常检测，经实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 解决现有异常检测方法在高维环境中的局限，如依赖分布假设和难调超参数。

Method: 先进行核变换，再采用投影寻踪方法，有新的搜索方向集合和结果组合方式。

Result: 在三个有挑战性结构的小数据集和四个大型基准数据集上验证了KOD的有效性。

Conclusion: KOD是一种灵活轻量级的异常检测方法。

Abstract: A new anomaly detection method called kernel outlier detection (KOD) is
proposed. It is designed to address challenges of outlier detection in
high-dimensional settings. The aim is to overcome limitations of existing
methods, such as dependence on distributional assumptions or on hyperparameters
that are hard to tune. KOD starts with a kernel transformation, followed by a
projection pursuit approach. Its novelties include a new ensemble of directions
to search over, and a new way to combine results of different direction types.
This provides a flexible and lightweight approach for outlier detection. Our
empirical evaluations illustrate the effectiveness of KOD on three small
datasets with challenging structures, and on four large benchmark datasets.

</details>


### [130] [Quantum Neural Networks for Wind Energy Forecasting: A Comparative Study of Performance and Scalability with Classical Models](https://arxiv.org/abs/2506.22845)
*Batuhan Hangun,Oguz Altun,Onder Eyecioglu*

Main category: cs.LG

TL;DR: 本文研究QNNs预测风力发电机功率输出，评估六种QNN配置，表明其性能可与经典方法竞争，还揭示数据集大小和电路复杂度的影响。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源系统集成推动智能电网使用增加，机器学习对预测电力需求和检测系统干扰很重要，本文旨在深入研究QNNs对风力发电机功率输出的预测。

Method: 评估基于Z特征映射进行数据编码和不同ansatz结构的六种QNN配置，通过详细交叉验证实验和在未见过的保留数据集上测试。

Result: 实验证明QNNs预测性能可与经典方法竞争，某些情况下略好，还揭示数据集大小和电路复杂度对预测性能和模拟时间的影响。

Conclusion: 研究结果为希望将量子机器学习融入工作的能源领域研究人员提供有价值的见解。

Abstract: Quantum Neural Networks (QNNs), a prominent approach in Quantum Machine
Learning (QML), are emerging as a powerful alternative to classical machine
learning methods. Recent studies have focused on the applicability of QNNs to
various tasks, such as time-series forecasting, prediction, and classification,
across a wide range of applications, including cybersecurity and medical
imaging. With the increased use of smart grids driven by the integration of
renewable energy systems, machine learning plays an important role in
predicting power demand and detecting system disturbances. This study provides
an in-depth investigation of QNNs for predicting the power output of a wind
turbine. We assess the predictive performance and simulation time of six QNN
configurations that are based on the Z Feature Map for data encoding and
varying ansatz structures. Through detailed cross-validation experiments and
tests on an unseen hold-out dataset, we experimentally demonstrate that QNNs
can achieve predictive performance that is competitive with, and in some cases
marginally better than, the benchmarked classical approaches. Our results also
reveal the effects of dataset size and circuit complexity on predictive
performance and simulation time. We believe our findings will offer valuable
insights for researchers in the energy domain who wish to incorporate quantum
machine learning into their work.

</details>


### [131] [Feature-Wise Mixing for Mitigating Contextual Bias in Predictive Supervised Learning](https://arxiv.org/abs/2506.23033)
*Yash Vardhan Tomar*

Main category: cs.LG

TL;DR: 本文提出特征混合框架减轻机器学习模型的上下文偏差，评估显示有效降低偏差和MSE，优于部分方法且无计算开销问题。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型偏差缓解策略存在可扩展性和泛化性问题，需新方法解决。

Method: 引入特征混合框架，在多个上下文数据集间重新分配特征表示，用交叉验证训练四个ML分类器，用偏差敏感损失函数评估。

Result: 平均偏差降低43.35%，MSE显著下降，优于SMOTE过采样，无需明确偏差属性识别，无计算开销。

Conclusion: 特征混合框架能有效减轻上下文偏差，未来可应用于需要准确预测的现实领域。

Abstract: Bias in predictive machine learning (ML) models is a fundamental challenge
due to the skewed or unfair outcomes produced by biased models. Existing
mitigation strategies rely on either post-hoc corrections or rigid constraints.
However, emerging research claims that these techniques can limit scalability
and reduce generalizability. To address this, this paper introduces a
feature-wise mixing framework to mitigate contextual bias. This was done by
redistributing feature representations across multiple contextual datasets. To
assess feature-wise mixing's effectiveness, four ML classifiers were trained
using cross-validation and evaluated with bias-sensitive loss functions,
including disparity metrics and mean squared error (MSE), which served as a
standard measure of predictive performance. The proposed method achieved an
average bias reduction of 43.35% and a statistically significant decrease in
MSE across all classifiers trained on mixed datasets. Additionally,
benchmarking against established bias mitigation techniques found that
feature-wise mixing consistently outperformed SMOTE oversampling and
demonstrated competitive effectiveness without requiring explicit bias
attribute identification. Feature-wise mixing efficiently avoids the
computational overhead typically associated with fairness-aware learning
algorithms. Future work could explore applying feature-wise mixing for
real-world fields where accurate predictions are necessary.

</details>


### [132] [Scalable Structure Learning of Bayesian Networks by Learning Algorithm Ensembles](https://arxiv.org/abs/2506.22848)
*Shengcai Liu,Hui Ou-yang,Zhiyuan Wang,Cheng Chen,Qijun Cai,Yew-Soon Ong,Ke Tang*

Main category: cs.LG

TL;DR: 本文引入结构学习集成（SLE）思想并提出Auto - SLE方法用于学习贝叶斯网络结构，集成到分治法中，实验显示优于单一算法分治法。


<details>
  <summary>Details</summary>
Motivation: 现有分治法学习贝叶斯网络结构在子问题上学习精度不稳定，手动设计高质量SLE有挑战。

Method: 引入SLE思想，提出自动方法Auto - SLE学习近最优SLE，将其集成到分治法中。

Result: 在含10000个变量数据集上精度提升30% - 225%，对更多变量和不同网络特征数据集泛化性好。

Conclusion: 采用（自动学习）SLE进行可扩展贝叶斯网络结构学习有显著潜力。

Abstract: Learning the structure of Bayesian networks (BNs) from data is challenging,
especially for datasets involving a large number of variables. The recently
proposed divide-and-conquer (D\&D) strategies present a promising approach for
learning large BNs. However, they still face a main issue of unstable learning
accuracy across subproblems. In this work, we introduce the idea of employing
structure learning ensemble (SLE), which combines multiple BN structure
learning algorithms, to consistently achieve high learning accuracy. We further
propose an automatic approach called Auto-SLE for learning near-optimal SLEs,
addressing the challenge of manually designing high-quality SLEs. The learned
SLE is then integrated into a D\&D method. Extensive experiments firmly show
the superiority of our method over D\&D methods with single BN structure
learning algorithm in learning large BNs, achieving accuracy improvement
usually by 30\%$\sim$225\% on datasets involving 10,000 variables. Furthermore,
our method generalizes well to datasets with many more (e.g., 30000) variables
and different network characteristics than those present in the training data
for learning the SLE. These results indicate the significant potential of
employing (automatic learning of) SLEs for scalable BN structure learning.

</details>


### [133] [Efficient Algorithms for Learning and Compressing Monophonic Halfspaces in Graphs](https://arxiv.org/abs/2506.23186)
*Marco Bressan,Victor Chepoi,Emmanuel Esposito,Maximilian Thiessen*

Main category: cs.LG

TL;DR: 研究图的单音半空间，给出基于2 - 可满足性的分解定理，得到多种学习问题的高效算法和样本压缩方案，回答了文献中的开放问题。


<details>
  <summary>Details</summary>
Motivation: 图顶点上的凸性抽象概念及半空间概念受机器学习界关注，研究单音半空间。

Method: 利用基于2 - 可满足性的分解定理，将单音半空间表示为特定顶点子集的不相交并。

Result: 得到多种学习问题的高效（近）最优算法，如经验风险最小化的多项式时间算法；获得高效、稳定且合适的样本压缩方案。

Conclusion: 单音半空间在可实现PAC设置下可用合适学习器高效学习，回答了文献开放问题，与测地半空间形成鲜明对比。

Abstract: Abstract notions of convexity over the vertices of a graph, and corresponding
notions of halfspaces, have recently gained attention from the machine learning
community. In this work we study monophonic halfspaces, a notion of graph
halfspaces defined through closure under induced paths. Our main result is a
$2$-satisfiability based decomposition theorem, which allows one to represent
monophonic halfspaces as a disjoint union of certain vertex subsets. Using this
decomposition, we achieve efficient and (nearly) optimal algorithms for various
learning problems, such as teaching, active, and online learning. Most notably,
we obtain a polynomial-time algorithm for empirical risk minimization.
Independently of the decomposition theorem, we obtain an efficient, stable, and
proper sample compression scheme. This makes monophonic halfspaces efficiently
learnable with proper learners and linear error rate $1/\varepsilon$ in the
realizable PAC setting. Our results answer open questions from the literature,
and show a stark contrast with geodesic halfspaces, for which most of the said
learning problems are NP-hard.

</details>


### [134] [P$^2$U: Progressive Precision Update For Efficient Model Distribution](https://arxiv.org/abs/2506.22871)
*Homayun Afrabandpey,Hamed Rezazadegan Tavakoli*

Main category: cs.LG

TL;DR: 提出渐进精度更新（P²U）方法解决带宽受限环境下模型分发问题，实验证明其在精度、带宽使用和延迟间取得良好平衡，适用于低资源场景。


<details>
  <summary>Details</summary>
Motivation: 在带宽受限环境中，高效的模型分发变得愈发关键，需要解决此问题。

Method: 提出P²U方法，传输低比特精度模型及表示高低精度模型差异的更新。

Result: 在多种模型架构和数据集上实验，P²U在精度、带宽使用和延迟间取得更好权衡，激进量化不严重影响性能。

Conclusion: P²U是低资源场景下可扩展且高效的模型分发解决方案，能与现有压缩技术互补，有更大改进潜力。

Abstract: Efficient model distribution is becoming increasingly critical in
bandwidth-constrained environments. In this paper, we propose a simple yet
effective approach called Progressive Precision Update (P$^2$U) to address this
problem. Instead of transmitting the original high-precision model, P$^2$U
transmits a lower-bit precision model, coupled with a model update representing
the difference between the original high-precision model and the transmitted
low precision version. With extensive experiments on various model
architectures, ranging from small models ($1 - 6$ million parameters) to a
large model (more than $100$ million parameters) and using three different data
sets, e.g., chest X-Ray, PASCAL-VOC, and CIFAR-100, we demonstrate that P$^2$U
consistently achieves better tradeoff between accuracy, bandwidth usage and
latency. Moreover, we show that when bandwidth or startup time is the priority,
aggressive quantization (e.g., 4-bit) can be used without severely compromising
performance. These results establish P$^2$U as an effective and practical
solution for scalable and efficient model distribution in low-resource
settings, including federated learning, edge computing, and IoT deployments.
Given that P$^2$U complements existing compression techniques and can be
implemented alongside any compression method, e.g., sparsification,
quantization, pruning, etc., the potential for improvement is even greater.

</details>


### [135] [Not All Explanations for Deep Learning Phenomena Are Equally Valuable](https://arxiv.org/abs/2506.23286)
*Alan Jeffares,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 本文认为一些深度学习中的反直觉现象在现实应用中证据不足，不应孤立看待，但仍有研究价值，还提出研究建议。


<details>
  <summary>Details</summary>
Motivation: 当前很多深度学习研究聚焦反直觉现象，但多孤立解释，需探讨其对推动领域发展的有效性。

Method: 分析近期文献中这些现象的研究成果，重新审视研究社区处理问题的规范。

Result: 发现这些现象在现实应用中证据不足，孤立研究可能效率不高，但能为完善通用理论提供独特场景。

Conclusion: 不应将这些现象视为需定制解决方案的孤立难题，提出未来研究的实用建议以推动深度学习领域发展。

Abstract: Developing a better understanding of surprising or counterintuitive phenomena
has constituted a significant portion of deep learning research in recent
years. These include double descent, grokking, and the lottery ticket
hypothesis -- among many others. Works in this area often develop ad hoc
hypotheses attempting to explain these observed phenomena on an isolated,
case-by-case basis. This position paper asserts that, in many prominent cases,
there is little evidence to suggest that these phenomena appear in real-world
applications and these efforts may be inefficient in driving progress in the
broader field. Consequently, we argue against viewing them as isolated puzzles
that require bespoke resolutions or explanations. However, despite this, we
suggest that deep learning phenomena do still offer research value by providing
unique settings in which we can refine our broad explanatory theories of more
general deep learning principles. This position is reinforced by analyzing the
research outcomes of several prominent examples of these phenomena from the
recent literature. We revisit the current norms in the research community in
approaching these problems and propose practical recommendations for future
research, aiming to ensure that progress on deep learning phenomena is well
aligned with the ultimate pragmatic goal of progress in the broader field of
deep learning.

</details>


### [136] [Interpretable Time Series Autoregression for Periodicity Quantification](https://arxiv.org/abs/2506.22895)
*Xinyu Chen,Vassilis Digalakis Jr,Lijun Ding,Dingyi Zhuang,Jinhua Zhao*

Main category: cs.LG

TL;DR: 提出可解释的稀疏自回归框架，处理时变和多维时间序列，加速MIO求解，通过实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 从可解释机器学习角度改进时间序列自回归模型，强化周期性量化的模型可解释性。

Method: 对时变数据将稀疏自回归优化问题转化为MIO，用DVP策略加速；对多维数据提出时空变稀疏自回归模型和两阶段优化方案。

Result: DVP策略大幅加速MIO求解且保证解质量；在共享出行和气候数据中揭示周期性和空间模式。

Conclusion: 提出的模型有效，可处理大规模问题，能发现动态模式和识别气候现象。

Abstract: Time series autoregression is a classical statistical model for capturing
auto-correlations and identifying temporal patterns such as periodicity and
seasonality. In this work, we propose a novel sparse autoregression framework
from an interpretable machine learning perspective and the model
interpretability for periodicity quantification is reinforced by $\ell_0$-norm
induced sparsity constraints. On the time-varying time series data, we
reformulate the sparse autoregression and convert the involved optimization
problem into a mixed-integer optimization (MIO). To accelerate it, we develop a
subspace pursuit based decision variable pruning (DVP) strategy to reduce the
search space. On the multidimensional time series that involves complicated
spatial and temporal dimensions, we propose a spatially- and time-varying
sparse autoregression model and resolve the corresponding MIO problem by
developing a two-stage optimization scheme. In particular, the proposed scheme
makes the model scalable to large problems even with millions of decision
variables. Empirically, we conduct extensive experiments to evaluate the
proposed models on real-world time series data. First, we demonstrate that the
MIO solver can be drastically accelerated through the DVP strategy, while
maintaining the same solution quality as a full MIO solver. Applying the
time-varying sparse autoregression model to ridesharing trip data, we uncover
both daily and weekly periodicities and reveal long-term changes in regularity
of human mobility. Second, we demonstrate the spatial patterns of yearly
seasonality in climate variable time series such as temperature and
precipitation across the past four decades, and our model allows to discover
dynamic climate patterns and identify climate phenomena such as El Nino in sea
surface temperature.

</details>


### [137] [Missing-Modality-Aware Graph Neural Network for Cancer Classification](https://arxiv.org/abs/2506.22901)
*Sina Tabakhi,Haiping Lu*

Main category: cs.LG

TL;DR: 本文提出MAGNET模型解决多模态生物数据缺失模态问题，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态生物数据融合方法难以处理多样的缺失模态模式及模式数量随模态数增加的指数级增长问题。

Method: 提出MAGNET模型，引入患者 - 模态多头注意力机制融合低维模态嵌入，构建患者图并用传统图神经网络生成预测。

Result: 在三个癌症分类的公共多组学数据集上实验表明，MAGNET优于现有融合方法。

Conclusion: MAGNET能有效解决多模态生物数据中的缺失模态问题，且复杂度随模态数线性增长。

Abstract: A key challenge in learning from multimodal biological data is missing
modalities, where all data from some modalities are missing for some patients.
Current fusion methods address this by excluding patients with missing
modalities, imputing missing modalities, or making predictions directly with
partial modalities. However, they often struggle with diverse missing-modality
patterns and the exponential growth of the number of such patterns as the
number of modalities increases. To address these limitations, we propose MAGNET
(Missing-modality-Aware Graph neural NETwork) for direct prediction with
partial modalities, which introduces a patient-modality multi-head attention
mechanism to fuse lower-dimensional modality embeddings based on their
importance and missingness. MAGNET's complexity increases linearly with the
number of modalities while adapting to missing-pattern variability. To generate
predictions, MAGNET further constructs a patient graph with fused multimodal
embeddings as node features and the connectivity determined by the modality
missingness, followed by a conventional graph neural network. Experiments on
three public multiomics datasets for cancer classification, with real-world
instead of artificial missingness, show that MAGNET outperforms the
state-of-the-art fusion methods. The data and code are available at
https://github.com/SinaTabakhi/MAGNET.

</details>


### [138] [Towards Time Series Generation Conditioned on Unstructured Natural Language](https://arxiv.org/abs/2506.22927)
*Jaeyun Woo,Jiseok Lee,Brian Kenji Iwana*

Main category: cs.LG

TL;DR: 提出基于非结构化自然语言描述生成时间序列的新方法，构建新公共数据集。


<details>
  <summary>Details</summary>
Motivation: 生成式AI发展迅速，但时间序列生成式AI仍欠发达，而时间序列在多领域应用重要。

Method: 使用扩散模型结合语言模型从文本生成时间序列。

Result: 证明基于自然语言的时间序列生成是可行的。

Conclusion: 该方法可用于定制预测、时间序列操作、数据增强和迁移学习等，还构建了新公共数据集。

Abstract: Generative Artificial Intelligence (AI) has rapidly become a powerful tool,
capable of generating various types of data, such as images and text. However,
despite the significant advancement of generative AI, time series generative AI
remains underdeveloped, even though the application of time series is essential
in finance, climate, and numerous fields. In this research, we propose a novel
method of generating time series conditioned on unstructured natural language
descriptions. We use a diffusion model combined with a language model to
generate time series from the text. Through the proposed method, we demonstrate
that time series generation based on natural language is possible. The proposed
method can provide various applications such as custom forecasting, time series
manipulation, data augmentation, and transfer learning. Furthermore, we
construct and propose a new public dataset for time series generation,
consisting of 63,010 time series-description pairs.

</details>


### [139] [Mathematical Computation on High-dimensional Data via Array Programming and Parallel Acceleration](https://arxiv.org/abs/2506.22929)
*Chen Zhang*

Main category: cs.LG

TL;DR: 深度学习处理高维数据有计算挑战，现有工具缺高级分析支持，提出基于空间完备性的并行计算架构处理不同类型数据。


<details>
  <summary>Details</summary>
Motivation: 深度学习处理高维数据面临计算挑战，当前大规模数据工具缺乏高级分析的数学统计支持。

Method: 提出基于空间完备性的并行计算架构，将高维数据分解为独立维度结构进行分布式处理。

Result: 该框架能无缝集成数据挖掘和并行优化机器学习方法，支持在统一系统中对不同类型数据进行科学计算。

Conclusion: 基于空间完备性的并行计算架构可有效应对高维数据处理问题，适用于多种数据类型。

Abstract: While deep learning excels in natural image and language processing, its
application to high-dimensional data faces computational challenges due to the
dimensionality curse. Current large-scale data tools focus on business-oriented
descriptive statistics, lacking mathematical statistics support for advanced
analysis. We propose a parallel computation architecture based on space
completeness, decomposing high-dimensional data into dimension-independent
structures for distributed processing. This framework enables seamless
integration of data mining and parallel-optimized machine learning methods,
supporting scientific computations across diverse data types like medical and
natural images within a unified system.

</details>


### [140] [Training of Spiking Neural Networks with Expectation-Propagation](https://arxiv.org/abs/2506.23757)
*Dan Yao,Steve McLaughlin,Yoann Altmann*

Main category: cs.LG

TL;DR: 提出用期望传播训练脉冲神经网络的统一消息传递框架，比基于梯度的方法收敛快，为深度贝叶斯网络训练提供新思路。


<details>
  <summary>Details</summary>
Motivation: 寻找高效训练脉冲神经网络的方法。

Method: 使用期望传播的无梯度方法，学习网络参数的边缘分布并边缘化隐藏层输出等干扰参数，可对离散和连续权重、确定性和随机脉冲网络用批量训练样本进行训练。

Result: 算法在实践中比基于梯度的方法收敛更快，无需大量遍历训练数据，有分类和回归结果。

Conclusion: 为深度贝叶斯网络的新高效训练方法奠定基础。

Abstract: In this paper, we propose a unifying message-passing framework for training
spiking neural networks (SNNs) using Expectation-Propagation. Our gradient-free
method is capable of learning the marginal distributions of network parameters
and simultaneously marginalizes nuisance parameters, such as the outputs of
hidden layers. This framework allows for the first time, training of discrete
and continuous weights, for deterministic and stochastic spiking networks,
using batches of training samples. Although its convergence is not ensured, the
algorithm converges in practice faster than gradient-based methods, without
requiring a large number of passes through the training data. The
classification and regression results presented pave the way for new efficient
training methods for deep Bayesian networks.

</details>


### [141] [Infinite Sampling: Efficient and Stable Grouped RL Training for Large Language Models](https://arxiv.org/abs/2506.22950)
*Liangyu Wang,Huanyi Xie,Xinhai Wang,Tianjin Huang,Mengdi Li,Di Wang*

Main category: cs.LG

TL;DR: 提出Infinite Sampling框架解决基于组的强化学习算法显存开销大问题，实验显示可降低显存使用并提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 基于组的强化学习算法在微调大语言模型时生成和存储多响应显存开销大，限制可扩展性。

Method: 提出Infinite Sampling框架，包括微采样组、连续采样和长度感知调度器。

Result: 微采样组降低超50%峰值显存使用，Infinite Sampling比朴素微采样组方法提高超25%吞吐量。

Conclusion: 混合调度能在现实显存约束下用更大组实现高效稳定GRPO训练。

Abstract: Group-based reinforcement learning algorithms such as Group Reward Policy
Optimization (GRPO) have proven effective for fine-tuning large language models
(LLMs) with human feedback. However, generating and storing multiple responses
per prompt incurs substantial memory overhead, especially as the sample group
size increases, limiting scalability under constrained hardware.
  We propose Infinite Sampling, a framework that enables efficient and stable
GRPO training by decoupling group size from GPU memory usage. It consists of:
(1) micro sampling groups that decompose large groups into memory-feasible
rounds; (2) continuous sampling that interleaves generation across groups to
improve utilization; and (3) a length-aware scheduler combining
token-conditioned sequence length prediction with a two-stage plan: global
grouping via FPTAS and runtime refill via SJF.
  Experiments show that our Micro Sampling Groups reduce peak memory usage by
over 50% compared to full-group decoding (e.g., from 21.55 GB to 10.64 GB on
Qwen3-1.7B). Building on this, Infinite Sampling improves throughput by over
25% compared to the naive micro sampling group method, reducing decoding steps
while maintaining full-length completions and memory usage. Our hybrid
scheduling ensures efficient and stable GRPO training with larger groups under
realistic GPU memory constraints.

</details>


### [142] [Cybersecurity-Focused Anomaly Detection in Connected Autonomous Vehicles Using Machine Learning](https://arxiv.org/abs/2506.22984)
*Prathyush Kumar Reddy Lebaku,Lu Gao,Yunpeng Zhang,Zhixia Li,Yongxin Liu,Tanvir Arafin*

Main category: cs.LG

TL;DR: 本文通过模拟车辆行为生成数据集，利用堆叠LSTM和随机森林模型进行自动驾驶场景的异常检测，结果表明模型有效。


<details>
  <summary>Details</summary>
Motivation: 联网自动驾驶汽车（CAVs）易受传感器故障、网络攻击和环境干扰，异常检测对维护安全可靠交通网络至关重要。

Method: 模拟车辆行为生成包含多车位置、速度和加速度时间序列数据的数据集，使用堆叠LSTM模型捕捉时间依赖和序列异常，用随机森林模型进行集成预测。

Result: 随机森林模型R2为0.9830，MAE为5.746，95%异常阈值为14.18；堆叠LSTM模型R2为0.9998，MAE为82.425，95%异常阈值为265.63。

Conclusion: 模型在准确预测车辆轨迹和检测自动驾驶场景异常方面有效。

Abstract: Anomaly detection in connected autonomous vehicles (CAVs) is crucial for
maintaining safe and reliable transportation networks, as CAVs can be
susceptible to sensor malfunctions, cyber-attacks, and unexpected environmental
disruptions. This study explores an anomaly detection approach by simulating
vehicle behavior, generating a dataset that represents typical and atypical
vehicular interactions. The dataset includes time-series data of position,
speed, and acceleration for multiple connected autonomous vehicles. We utilized
machine learning models to effectively identify abnormal driving patterns.
First, we applied a stacked Long Short-Term Memory (LSTM) model to capture
temporal dependencies and sequence-based anomalies. The stacked LSTM model
processed the sequential data to learn standard driving behaviors.
Additionally, we deployed a Random Forest model to support anomaly detection by
offering ensemble-based predictions, which enhanced model interpretability and
performance. The Random Forest model achieved an R2 of 0.9830, MAE of 5.746,
and a 95th percentile anomaly threshold of 14.18, while the stacked LSTM model
attained an R2 of 0.9998, MAE of 82.425, and a 95th percentile anomaly
threshold of 265.63. These results demonstrate the models' effectiveness in
accurately predicting vehicle trajectories and detecting anomalies in
autonomous driving scenarios.

</details>


### [143] [Faster Diffusion Models via Higher-Order Approximation](https://arxiv.org/abs/2506.24042)
*Gen Li,Yuchen Zhou,Yuting Wei,Yuxin Chen*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we explore provable acceleration of diffusion models without
any additional retraining. Focusing on the task of approximating a target data
distribution in $\mathbb{R}^d$ to within $\varepsilon$ total-variation
distance, we propose a principled, training-free sampling algorithm that
requires only the order of
  $$ d^{1+2/K} \varepsilon^{-1/K} $$
  score function evaluations (up to log factor) in the presence of accurate
scores, where $K$ is an arbitrarily large fixed integer. This result applies to
a broad class of target data distributions, without the need for assumptions
such as smoothness or log-concavity. Our theory is robust vis-a-vis inexact
score estimation, degrading gracefully as the score estimation error increases
-- without demanding higher-order smoothness on the score estimates as assumed
in previous work. The proposed algorithm draws insight from high-order ODE
solvers, leveraging high-order Lagrange interpolation and successive refinement
to approximate the integral derived from the probability flow ODE.

</details>


### [144] [A Reinforcement Learning Approach for Optimal Control in Microgrids](https://arxiv.org/abs/2506.22995)
*Davide Salaorni,Federico Bianchi,Francesco Trovò,Marcello Restelli*

Main category: cs.LG

TL;DR: 本文提出基于强化学习的微电网能源管理优化方法，用数字孪生模拟储能系统，实验验证该方法优于规则方法和现有强化学习基准。


<details>
  <summary>Details</summary>
Motivation: 可再生能源集成增加，传统电网需新方法管理分散能源生产和消费，微电网是有前景的解决方案，需优化能源管理。

Method: 提出基于强化学习的智能体，利用能源生产、消费和市场价格的历史数据学习最优能源交易和存储策略，用数字孪生模拟储能系统动态并考虑退化因素。

Result: 基于强化学习的策略优于规则方法和现有强化学习基准。

Conclusion: 提出的基于强化学习的策略为智能微电网管理提供了可靠解决方案。

Abstract: The increasing integration of renewable energy sources (RESs) is transforming
traditional power grid networks, which require new approaches for managing
decentralized energy production and consumption. Microgrids (MGs) provide a
promising solution by enabling localized control over energy generation,
storage, and distribution. This paper presents a novel reinforcement learning
(RL)-based methodology for optimizing microgrid energy management.
Specifically, we propose an RL agent that learns optimal energy trading and
storage policies by leveraging historical data on energy production,
consumption, and market prices. A digital twin (DT) is used to simulate the
energy storage system dynamics, incorporating degradation factors to ensure a
realistic emulation of the analysed setting. Our approach is validated through
an experimental campaign using real-world data from a power grid located in the
Italian territory. The results indicate that the proposed RL-based strategy
outperforms rule-based methods and existing RL benchmarks, offering a robust
solution for intelligent microgrid management.

</details>


### [145] [Data Uniformity Improves Training Efficiency and More, with a Convergence Framework Beyond the NTK Regime](https://arxiv.org/abs/2506.24120)
*Yuqing Wang,Shangding Gu*

Main category: cs.LG

TL;DR: 本文指出选择更均匀分布的数据可提升训练效率和性能，建立了相关理论框架并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 探寻能持续提升性能的数据选择定量通用原则，尤其是针对先验知识有限的复杂任务。

Method: 证明更均匀分布会使数据点最小成对距离增大，理论推导该距离与神经网络逼近误差及梯度下降训练动态的关系，构建超越NTK机制的收敛框架，进行全面监督微调实验。

Result: 实验表明最大化成对距离选择数据能显著加速大语言模型训练，在不同数据集上取得相当或更好性能。

Conclusion: 选择更均匀分布的数据有助于提升大语言模型训练效率和性能，所构建框架为深度神经架构相关操作提供理论依据。

Abstract: Data selection plays a crucial role in data-driven decision-making, including
in large language models (LLMs), and is typically task-dependent. Properties
such as data quality and diversity have been extensively studied and are known
to enhance model performance. However, it remains unclear whether there exist
other quantitative and general principles of data selection that can
consistently improve performance, especially for complex tasks with limited
prior knowledge. In this paper, we demonstrate that selecting more uniformly
distributed data can improve training efficiency while enhancing performance.
Specifically, we establish that more uniform (less biased) distribution leads
to a larger minimum pairwise distance between data points, denoted by
$h_{\min}$, and prove that a smaller $h_{\min}$ can slow down the training
dynamics of gradient descent (GD). Moreover, we theoretically show that the
approximation error of neural networks decreases as $h_{\min}$ increases. Our
analysis introduces a convergence framework for GD beyond the Neural Tangent
Kernel (NTK) regime, applicable to a broad class of architectures, including
transformers, without requiring Lipschitz smoothness. This framework further
provides theoretical justification for the use of residual connections and
function compositions in deep neural architectures. In the end, we conduct
comprehensive experiments for supervised fine-tuning across various settings,
including different optimization strategies, model sizes, and training
datasets. The results consistently demonstrate that selecting data by
maximizing pairwise distance significantly accelerates training and achieves
comparable or better performance in LLMs across diverse datasets. Code and
Datasets are available at the link:
https://github.com/SafeRL-Lab/data-uniformity.

</details>


### [146] [BWLer: Barycentric Weight Layer Elucidates a Precision-Conditioning Tradeoff for PINNs](https://arxiv.org/abs/2506.23024)
*Jerry Liu,Yasa Baig,Denise Hui Jean Lee,Rajat Vadiraj Dwaraknath,Atri Rudra,Chris Ré*

Main category: cs.LG

TL;DR: 本文研究PINNs精度天花板来源，引入BWLer解决问题，实验表明其提升精度，指出结合PINNs与经典谱求解器的实用路径。


<details>
  <summary>Details</summary>
Motivation: PINNs求解偏微分方程未达科学任务所需机器精度，探究精度天花板来源。

Method: 引入Barycentric Weight Layer (BWLer)，可加在现有MLP上或完全替代，通过重心多项式插值建模PDE解，用谱导数和预条件处理训练。

Result: 在简单1 - D插值任务中发现MLP精度局限；添加BWLer提升精度，在五个基准PDE上表现良好，显式BWLer在部分问题达近机器精度。

Conclusion: 存在结合PINNs灵活性与经典谱求解器精度的实用路径。

Abstract: Physics-informed neural networks (PINNs) offer a flexible way to solve
partial differential equations (PDEs) with machine learning, yet they still
fall well short of the machine-precision accuracy many scientific tasks demand.
In this work, we investigate whether the precision ceiling comes from the
ill-conditioning of the PDEs or from the typical multi-layer perceptron (MLP)
architecture. We introduce the Barycentric Weight Layer (BWLer), which models
the PDE solution through barycentric polynomial interpolation. A BWLer can be
added on top of an existing MLP (a BWLer-hat) or replace it completely
(explicit BWLer), cleanly separating how we represent the solution from how we
take derivatives for the PDE loss. Using BWLer, we identify fundamental
precision limitations within the MLP: on a simple 1-D interpolation task, even
MLPs with O(1e5) parameters stall around 1e-8 RMSE -- about eight orders above
float64 machine precision -- before any PDE terms are added. In PDE learning,
adding a BWLer lifts this ceiling and exposes a tradeoff between achievable
accuracy and the conditioning of the PDE loss. For linear PDEs we fully
characterize this tradeoff with an explicit error decomposition and navigate it
during training with spectral derivatives and preconditioning. Across five
benchmark PDEs, adding a BWLer on top of an MLP improves RMSE by up to 30x for
convection, 10x for reaction, and 1800x for wave equations while remaining
compatible with first-order optimizers. Replacing the MLP entirely lets an
explicit BWLer reach near-machine-precision on convection, reaction, and wave
problems (up to 10 billion times better than prior results) and match the
performance of standard PINNs on stiff Burgers' and irregular-geometry Poisson
problems. Together, these findings point to a practical path for combining the
flexibility of PINNs with the precision of classical spectral solvers.

</details>


### [147] [Spectra 1.1: Scaling Laws and Efficient Inference for Ternary Language Models](https://arxiv.org/abs/2506.23025)
*Tejas Vaidhya,Ayush Kaushal,Vineet Jain,Francis Couture Harpin,Prashant Shishodia,Majid Behbahani,Yuriy Nevmyvaka,Irina Rish*

Main category: cs.LG

TL;DR: 本文研究三元语言模型（TriLMs）以解决大语言模型推理效率问题，分析其可扩展性，推出Spectra - 1.1套件，提出新型打包方案和GPU内核，还将发布相关资源。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理效率受现代GPU内存带宽和容量瓶颈限制，需要解决推理效率问题。

Method: 进行缩放定律分析，推出基于大量训练数据的Spectra - 1.1套件，提出2位和1.6位打包方案，开发GPU内核TriRun。

Result: TriLMs从增加训练数据中受益更多；Spectra - 1.1套件在大规模下有性能提升；打包方案加速CPU推理；TriRun使端到端模型推理比浮点基线快达5倍。

Conclusion: 为高效大语言模型的构建和部署奠定基础，为研究界提供有价值资源。

Abstract: Large language models (LLMs) are increasingly used across research and
industry applications, yet their inference efficiency remains a significant
challenge. As the computational power of modern GPU architectures continuously
improves, their memory bandwidth and capacity have not scaled proportionally,
creating a critical bottleneck during inference. To address this, we
investigate ternary language models (TriLMs) that employ quantization-aware
training to significantly reduce memory requirements. We first analyze the
scalability of TriLMs by conducting a scaling law analysis, revealing that
TriLMs benefit more from increasing training data than from scaling model
parameters. Based on this observation, we introduce Spectra-1.1, an open suite
of TriLMs trained on up to 1.2 trillion tokens, demonstrating sustained
performance gains at scale. Furthermore, to improve inference efficiency, we
propose novel 2-bit and 1.6-bit packing schemes for ternary weights, which
demonstrate accelerated inference across various CPU architectures. Also,
building on the 2-bit packing, we develop a GPU kernel called TriRun that
accelerates end-to-end model inference by up to 5 times compared to
floating-point baselines. To encourage further exploration and development of
TriLMs, we will release the Spectra-1.1 suite and TriRun inference kernels.
Overall, our work lays the foundation for building and deploying efficient
LLMs, providing a valuable resource for the research community.

</details>


### [148] [Fragile, Robust, and Antifragile: A Perspective from Parameter Responses in Reinforcement Learning Under Stress](https://arxiv.org/abs/2506.23036)
*Zain ul Abdeen,Ming Jin*

Main category: cs.LG

TL;DR: 本文通过分析网络参数探索强化学习策略鲁棒性，用突触过滤和对抗攻击施加内外部压力，对参数分类并验证框架，发现反脆弱参数，为设计鲁棒系统提供基础。


<details>
  <summary>Details</summary>
Motivation: 探索强化学习策略的鲁棒性。

Method: 用突触过滤引入内部压力，用对抗攻击施加外部压力，对参数进行分类并定义参数分数，在Mujoco连续控制环境中验证框架。

Result: 发现存在能在压力下提升策略性能的反脆弱参数。

Conclusion: 研究为设计鲁棒和反脆弱的强化学习系统的未来发展提供基础。

Abstract: This paper explores Reinforcement learning (RL) policy robustness by
systematically analyzing network parameters under internal and external
stresses. Inspired by synaptic plasticity in neuroscience, synaptic filtering
introduces internal stress by selectively perturbing parameters, while
adversarial attacks apply external stress through modified agent observations.
This dual approach enables the classification of parameters as fragile, robust,
or antifragile, based on their influence on policy performance in clean and
adversarial settings. Parameter scores are defined to quantify these
characteristics, and the framework is validated on PPO-trained agents in Mujoco
continuous control environments. The results highlight the presence of
antifragile parameters that enhance policy performance under stress,
demonstrating the potential of targeted filtering techniques to improve RL
policy adaptability. These insights provide a foundation for future
advancements in the design of robust and antifragile RL systems.

</details>


### [149] [ReMem: Mutual Information-Aware Fine-tuning of Pretrained Vision Transformers for Effective Knowledge Distillation](https://arxiv.org/abs/2506.23041)
*Chengyu Dong,Huan Gui,Noveen Sachdeva,Long Jin,Ke Yin,Jingbo Shang,Lichan Hong,Ed H. Chi,Zhe Zhao*

Main category: cs.LG

TL;DR: 本文探讨微调预训练视觉Transformer以提升知识蒸馏效果，提出互信息感知优化和MLP块重加权方法。


<details>
  <summary>Details</summary>
Motivation: 解决从大规模预训练强模型进行知识蒸馏时知识转移效果显著下降的问题。

Method: 在微调期间采用互信息感知优化，针对小或高度不平衡下游数据集引入MLP块重加权启发式方法。

Result: 使小的学生模型能从最强的预训练模型中受益。

Conclusion: 所提方法有效解决了从强预训练模型进行知识蒸馏的挑战。

Abstract: Knowledge distillation from pretrained visual representation models offers an
effective approach to improve small, task-specific production models. However,
the effectiveness of such knowledge transfer drops significantly when
distilling from strong models that are pretrained in a large scale. In this
paper, we address this challenge for pretrained Vision Transformers (ViTs) by
exploring methods to fine-tune them for more effective knowledge transfer.
Motivated by the connection between mutual information and distillation
effectiveness, we propose to employ mutual information-aware optimization
during finetuning. For small or highly-imbalanced downstream datasets where
such optimization becomes less effective, we introduce a simple yet effective
heuristic of reweighting MLP blocks. This approach is inspired by our
observation that top MLP blocks are primarily responsible for mutual
information loss. Our method enables small student models to benefit from those
pretrained models among the strongest.

</details>


### [150] [ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural Networks](https://arxiv.org/abs/2402.09146)
*Muhammad Kashif,Muhammad Shafique*

Main category: cs.LG

TL;DR: 提出新颖框架增强QuNNs性能，引入可训练量子卷积层，提出ResQuNNs解决梯度优化问题，通过实验确定残差块高效配置，推动量子深度学习发展。


<details>
  <summary>Details</summary>
Motivation: 传统量子卷积层静态、适应性有限，且引入可训练层会带来梯度优化难题，需提升QuNNs性能。

Method: 引入可训练量子卷积层，提出ResQuNNs架构，利用残差学习，在量子卷积层间插入残差块，并通过实验确定残差块放置策略。

Result: 确定了残差块的高效配置，使网络各层可获取梯度，实现高效训练。

Conclusion: 残差块的精确位置对提升QuNNs性能至关重要，研究推动了量子深度学习发展。

Abstract: In this paper, we present a novel framework for enhancing the performance of
Quanvolutional Neural Networks (QuNNs) by introducing trainable quanvolutional
layers and addressing the critical challenges associated with them. Traditional
quanvolutional layers, although beneficial for feature extraction, have largely
been static, offering limited adaptability. Unlike state-of-the-art, our
research overcomes this limitation by enabling training within these layers,
significantly increasing the flexibility and potential of QuNNs. However, the
introduction of multiple trainable quanvolutional layers induces complexities
in gradient-based optimization, primarily due to the difficulty in accessing
gradients across these layers. To resolve this, we propose a novel
architecture, Residual Quanvolutional Neural Networks (ResQuNNs), leveraging
the concept of residual learning, which facilitates the flow of gradients by
adding skip connections between layers. By inserting residual blocks between
quanvolutional layers, we ensure enhanced gradient access throughout the
network, leading to improved training performance. Moreover, we provide
empirical evidence on the strategic placement of these residual blocks within
QuNNs. Through extensive experimentation, we identify an efficient
configuration of residual blocks, which enables gradients across all the layers
in the network that eventually results in efficient training. Our findings
suggest that the precise location of residual blocks plays a crucial role in
maximizing the performance gains in QuNNs. Our results mark a substantial step
forward in the evolution of quantum deep learning, offering new avenues for
both theoretical development and practical quantum computing applications.

</details>


### [151] [Double-Diffusion: Diffusion Conditioned Diffusion Probabilistic Model For Air Quality Prediction](https://arxiv.org/abs/2506.23053)
*Hanlin Dong,Arian Prabowo,Hao Xue,Flora D. Salim*

Main category: cs.LG

TL;DR: 提出双扩散模型Double - Diffusion用于空气质量预测，结合物理原理和随机性，在多方面表现优于其他概率模型。


<details>
  <summary>Details</summary>
Motivation: 空气质量预测因时空复杂性、动态性和不确定性具有挑战性，当前模型难以平衡确定性与不确定性。

Method: 提出Double - Diffusion模型，结合已知物理原理进行随机空气质量预测，采用图像恢复的采样策略和新的去噪器架构。

Result: 在两个真实数据集的多数评估场景中排名第一，推理时间减少30% - 50%，连续排序概率得分提高3% - 12%。

Conclusion: Double - Diffusion模型是一种有效的空气质量预测方法，为利用物理条件生成方法进行预测提供了新途径。

Abstract: Air quality prediction is a challenging forecasting task due to its
spatio-temporal complexity and the inherent dynamics as well as uncertainty.
Most of the current models handle these two challenges by applying Graph Neural
Networks or known physics principles, and quantifying stochasticity through
probabilistic networks like Diffusion models. Nevertheless, finding the right
balancing point between the certainties and uncertainties remains an open
question. Therefore, we propose Double-Diffusion, a novel diffusion
probabilistic model that harnesses the power of known physics to guide air
quality forecasting with stochasticity. To the best of our knowledge, while
precedents have been made of using conditional diffusion models to predict air
pollution, this is the first attempt to use physics as a conditional generative
approach for air quality prediction. Along with a sampling strategy adopted
from image restoration and a new denoiser architecture, Double-Diffusion ranks
first in most evaluation scenarios across two real-life datasets compared with
other probabilistic models, it also cuts inference time by 50% to 30% while
enjoying an increase between 3-12% in Continuous Ranked Probabilistic Score
(CRPS).

</details>


### [152] [Measuring How LLMs Internalize Human Psychological Concepts: A preliminary analysis](https://arxiv.org/abs/2506.23055)
*Hiro Taiyo Hamada,Ippei Fujisawa,Genji Kawakita,Yuki Yamada*

Main category: cs.LG

TL;DR: 本文开发定量框架评估大语言模型与人类心理维度的概念对齐，GPT - 4表现最佳，为开发更可解释AI系统提供见解。


<details>
  <summary>Details</summary>
Motivation: 不清楚大语言模型对塑造人类思想和行为的概念内化的准确程度。

Method: 使用43份标准化心理问卷，通过成对相似性分析评估语言模型重构和分类问卷项目的准确性，用层次聚类比较聚类结构与原始分类标签。

Result: GPT - 4分类准确率66.2%，显著优于GPT - 3.5（55.9%）和BERT（48.1%），均超随机基线（31.9%），且GPT - 4估计的语义相似度与人类在多个心理问卷中的皮尔逊相关系数有关。

Conclusion: 该框架为评估人类与大语言模型的概念对齐和识别潜在表征偏差提供新方法，现代大语言模型能以可测量的准确性近似人类心理结构，有助于开发更可解释的AI系统。

Abstract: Large Language Models (LLMs) such as ChatGPT have shown remarkable abilities
in producing human-like text. However, it is unclear how accurately these
models internalize concepts that shape human thought and behavior. Here, we
developed a quantitative framework to assess concept alignment between LLMs and
human psychological dimensions using 43 standardized psychological
questionnaires, selected for their established validity in measuring distinct
psychological constructs. Our method evaluates how accurately language models
reconstruct and classify questionnaire items through pairwise similarity
analysis. We compared resulting cluster structures with the original
categorical labels using hierarchical clustering. A GPT-4 model achieved
superior classification accuracy (66.2\%), significantly outperforming GPT-3.5
(55.9\%) and BERT (48.1\%), all exceeding random baseline performance (31.9\%).
We also demonstrated that the estimated semantic similarity from GPT-4 is
associated with Pearson's correlation coefficients of human responses in
multiple psychological questionnaires. This framework provides a novel approach
to evaluate the alignment of the human-LLM concept and identify potential
representational biases. Our findings demonstrate that modern LLMs can
approximate human psychological constructs with measurable accuracy, offering
insights for developing more interpretable AI systems.

</details>


### [153] [Curious Causality-Seeking Agents Learn Meta Causal World](https://arxiv.org/abs/2506.23068)
*Zhiyu Zhao,Haoxuan Li,Haifeng Zhang,Jun Wang,Francesco Faccio,Jürgen Schmidhuber,Mengyue Yang*

Main category: cs.LG

TL;DR: 引入Meta - Causal Graph作为世界模型及Causality - Seeking Agent，实验证明方法能捕捉因果动态变化并有效泛化。


<details>
  <summary>Details</summary>
Motivation: 现实中构建世界模型时，政策或环境状态的细微变化会改变观察到的因果机制，传统假设环境有单一不变因果规则不适用。

Method: 引入Meta - Causal Graph作为世界模型，由多个因果子图组成；引入Causality - Seeking Agent，通过好奇驱动干预策略识别元状态、发现因果关系并迭代优化Meta - Causal Graph。

Result: 在合成任务和机器人手臂操作任务实验中，方法能稳健捕捉因果动态变化，有效泛化到未见场景。

Conclusion: 所提出的Meta - Causal Graph和Causality - Seeking Agent方法在处理因果机制变化和泛化方面是有效的。

Abstract: When building a world model, a common assumption is that the environment has
a single, unchanging underlying causal rule, like applying Newton's laws to
every situation. In reality, what appears as a drifting causal mechanism is
often the manifestation of a fixed underlying mechanism seen through a narrow
observational window. This brings about a problem that, when building a world
model, even subtle shifts in policy or environment states can alter the very
observed causal mechanisms. In this work, we introduce the \textbf{Meta-Causal
Graph} as world models, a minimal unified representation that efficiently
encodes the transformation rules governing how causal structures shift across
different latent world states. A single Meta-Causal Graph is composed of
multiple causal subgraphs, each triggered by meta state, which is in the latent
state space. Building on this representation, we introduce a
\textbf{Causality-Seeking Agent} whose objectives are to (1) identify the meta
states that trigger each subgraph, (2) discover the corresponding causal
relationships by agent curiosity-driven intervention policy, and (3)
iteratively refine the Meta-Causal Graph through ongoing curiosity-driven
exploration and agent experiences. Experiments on both synthetic tasks and a
challenging robot arm manipulation task demonstrate that our method robustly
captures shifts in causal dynamics and generalizes effectively to previously
unseen contexts.

</details>


### [154] [Forget-MI: Machine Unlearning for Forgetting Multimodal Information in Healthcare Settings](https://arxiv.org/abs/2506.23145)
*Shahad Hardan,Darya Taratynova,Abdelmajid Essofi,Karthik Nandakumar,Mohammad Yaqub*

Main category: cs.LG

TL;DR: 提出Forget - MI用于多模态医疗数据的机器学习遗忘方法，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: AI隐私保护重要，现有机器学习遗忘方法难以从训练好的多模态架构中移除患者数据。

Method: 建立损失函数和扰动技术，提出Forget - MI方法，对请求遗忘的数据进行单模态和联合表示的遗忘。

Result: 通过在遗忘数据集、测试数据集上的性能和成员推理攻击（MIA）评估，模型在降低MIA、遗忘数据集的AUC和F1分数上表现更好，测试集性能与重新训练的模型相当。

Conclusion: Forget - MI方法能有效实现多模态医疗数据的遗忘，且保持与原模型相当的性能，代码已开源。

Abstract: Privacy preservation in AI is crucial, especially in healthcare, where models
rely on sensitive patient data. In the emerging field of machine unlearning,
existing methodologies struggle to remove patient data from trained multimodal
architectures, which are widely used in healthcare. We propose Forget-MI, a
novel machine unlearning method for multimodal medical data, by establishing
loss functions and perturbation techniques. Our approach unlearns unimodal and
joint representations of the data requested to be forgotten while preserving
knowledge from the remaining data and maintaining comparable performance to the
original model. We evaluate our results using performance on the forget
dataset, performance on the test dataset, and Membership Inference Attack
(MIA), which measures the attacker's ability to distinguish the forget dataset
from the training dataset. Our model outperforms the existing approaches that
aim to reduce MIA and the performance on the forget dataset while keeping an
equivalent performance on the test set. Specifically, our approach reduces MIA
by 0.202 and decreases AUC and F1 scores on the forget set by 0.221 and 0.305,
respectively. Additionally, our performance on the test set matches that of the
retrained model, while allowing forgetting. Code is available at
https://github.com/BioMedIA-MBZUAI/Forget-MI.git

</details>


### [155] [maneuverRecognition -- A Python package for Timeseries Classification in the domain of Vehicle Telematics](https://arxiv.org/abs/2506.23147)
*Jonathan Schuster,Fabian Transchel*

Main category: cs.LG

TL;DR: 开发maneuverRecognition包用于驾驶操作识别，提供预处理、建模和评估功能，并以实际驾驶数据演示其实现。


<details>
  <summary>Details</summary>
Motivation: 在车辆远程信息处理领域，驾驶操作自动识别有诸多应用，但缺乏能快速处理数据、构建和评估模型的Python包，有实际需求。

Method: 开发maneuverRecognition包，提供预处理、建模和评估的必要功能，包含可修改的基于LSTM的网络结构。

Result: 使用智能手机传感器记录的三个不同人的实际驾驶数据演示了该包的实现。

Conclusion: maneuverRecognition包能满足驾驶操作识别中数据处理、模型构建和评估的需求。

Abstract: In the domain of vehicle telematics the automated recognition of driving
maneuvers is used to classify and evaluate driving behaviour. This not only
serves as a component to enhance the personalization of insurance policies, but
also to increase road safety, reduce accidents and the associated costs as well
as to reduce fuel consumption and support environmentally friendly driving. In
this context maneuver recognition technically requires a continuous application
of time series classification which poses special challenges to the transfer,
preprocessing and storage of telematic sensor data, the training of predictive
models, and the prediction itself. Although much research has been done in the
field of gathering relevant data or regarding the methods to build predictive
models for the task of maneuver recognition, there is a practical need for
python packages and functions that allow to quickly transform data into the
required structure as well as to build and evaluate such models. The
maneuverRecognition package was therefore developed to provide the necessary
functions for preprocessing, modelling and evaluation and also includes a ready
to use LSTM based network structure that can be modified. The implementation of
the package is demonstrated using real driving data of three different persons
recorded via smartphone sensors.

</details>


### [156] [Data Can Speak for Itself: Quality-guided Utilization of Wireless Synthetic Data](https://arxiv.org/abs/2506.23174)
*Chen Gong,Bo Liang,Wei Gao,Chenren Xu*

Main category: cs.LG

TL;DR: 提出量化合成数据质量的指标，引入SynCheck方案提升合成数据利用效果。


<details>
  <summary>Details</summary>
Motivation: 当前无线合成数据质量不可预测，性能提升无保证，存在亲和性限制。

Method: 提出量化合成数据质量属性的指标，引入SynCheck质量引导的合成数据利用方案。

Result: SynCheck始终优于不考虑质量的合成数据利用方式，在之前利用使性能下降13.4%时仍能提升4.3%。

Conclusion: SynCheck方案能有效提升合成数据利用效果和任务性能。

Abstract: Generative models have gained significant attention for their ability to
produce realistic synthetic data that supplements the quantity of real-world
datasets. While recent studies show performance improvements in wireless
sensing tasks by incorporating all synthetic data into training sets, the
quality of synthetic data remains unpredictable and the resulting performance
gains are not guaranteed. To address this gap, we propose tractable and
generalizable metrics to quantify quality attributes of synthetic data -
affinity and diversity. Our assessment reveals prevalent affinity limitation in
current wireless synthetic data, leading to mislabeled data and degraded task
performance. We attribute the quality limitation to generative models' lack of
awareness of untrained conditions and domain-specific processing. To mitigate
these issues, we introduce SynCheck, a quality-guided synthetic data
utilization scheme that refines synthetic data quality during task model
training. Our evaluation demonstrates that SynCheck consistently outperforms
quality-oblivious utilization of synthetic data, and achieves 4.3% performance
improvement even when the previous utilization degrades performance by 13.4%.

</details>


### [157] [Attribution assignment for deep-generative sequence models enables interpretability analysis using positive-only data](https://arxiv.org/abs/2506.23182)
*Robert Frank,Michael Widrich,Rahmad Akbar,Günter Klambauer,Geir Kjetil Sandve,Philippe A. Robert,Victor Greiff*

Main category: cs.LG

TL;DR: 提出GAMA归因方法，可用于自回归生成模型，无需负训练数据实现模型可解释性和策略验证。


<details>
  <summary>Details</summary>
Motivation: 生成模型缺乏归因方法，难以提取可解释的生物学见解。

Method: 基于积分梯度，开发适用于自回归生成模型的Generative Attribution Metric Analysis (GAMA)方法。

Result: 用合成数据集评估GAMA的统计行为，验证其恢复生物学相关特征的能力，还应用于实验抗体 - 抗原结合数据。

Conclusion: GAMA能实现模型可解释性，验证生成序列设计策略，且无需负训练数据。

Abstract: Generative machine learning models offer a powerful framework for therapeutic
design by efficiently exploring large spaces of biological sequences enriched
for desirable properties. Unlike supervised learning methods, which require
both positive and negative labeled data, generative models such as LSTMs can be
trained solely on positively labeled sequences, for example, high-affinity
antibodies. This is particularly advantageous in biological settings where
negative data are scarce, unreliable, or biologically ill-defined. However, the
lack of attribution methods for generative models has hindered the ability to
extract interpretable biological insights from such models. To address this
gap, we developed Generative Attribution Metric Analysis (GAMA), an attribution
method for autoregressive generative models based on Integrated Gradients. We
assessed GAMA using synthetic datasets with known ground truths to characterize
its statistical behavior and validate its ability to recover biologically
relevant features. We further demonstrated the utility of GAMA by applying it
to experimental antibody-antigen binding data. GAMA enables model
interpretability and the validation of generative sequence design strategies
without the need for negative training data.

</details>


### [158] [External Data-Enhanced Meta-Representation for Adaptive Probabilistic Load Forecasting](https://arxiv.org/abs/2506.23201)
*Haoran Li,Muhao Guo,Marija Ilic,Yang Weng,Guangchun Ruan*

Main category: cs.LG

TL;DR: 提出M2oE2模型用于住宅负荷预测，以外部数据为元知识，结合超网络和MoE机制，提升预测准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有统计和机器学习模型处理外部因素时忽略其异质性，限制有用信息提取，需新方法提高住宅负荷预测准确性。

Method: 设计元表示框架，用超网络根据外部条件调制基础深度学习模型参数，集成MoE机制选择性激活专家并过滤冗余输入。

Result: M2oE2模型在不同负荷数据集上显著提高准确性和鲁棒性，超越现有先进方法。

Conclusion: 以外部数据作为元知识的方法有效，M2oE2模型在住宅负荷预测中表现优异。

Abstract: Accurate residential load forecasting is critical for power system
reliability with rising renewable integration and demand-side flexibility.
However, most statistical and machine learning models treat external factors,
such as weather, calendar effects, and pricing, as extra input, ignoring their
heterogeneity, and thus limiting the extraction of useful external information.
We propose a paradigm shift: external data should serve as meta-knowledge to
dynamically adapt the forecasting model itself. Based on this idea, we design a
meta-representation framework using hypernetworks that modulate selected
parameters of a base Deep Learning (DL) model in response to external
conditions. This provides both expressivity and adaptability. We further
integrate a Mixture-of-Experts (MoE) mechanism to enhance efficiency through
selective expert activation, while improving robustness by filtering redundant
external inputs. The resulting model, dubbed as a Meta Mixture of Experts for
External data (M2oE2), achieves substantial improvements in accuracy and
robustness with limited additional overhead, outperforming existing
state-of-the-art methods in diverse load datasets. The dataset and source code
are publicly available at
https://github.com/haorandd/M2oE2\_load\_forecast.git.

</details>


### [159] [Single Image Inpainting and Super-Resolution with Simultaneous Uncertainty Guarantees by Universal Reproducing Kernels](https://arxiv.org/abs/2506.23221)
*Bálint Horváth,Balázs Csanád Csáji*

Main category: cs.LG

TL;DR: 提出统计学习方法SGKI估计图像缺失像素，可提供不确定性量化，在不同数据集上实验。


<details>
  <summary>Details</summary>
Motivation: 解决图像修复和超分辨率中缺失像素估计问题，并提供不确定性量化。

Method: 假设数据生成函数来自RKHS，提出SGKI方法，是已有核方法的扩展和改进，用Schur补计算置信带，推广到向量值函数。

Result: 能估计缺失像素并构建非渐近置信带，在不同数据集上进行数值实验。

Conclusion: SGKI方法可有效估计图像缺失像素并提供不确定性量化。

Abstract: The paper proposes a statistical learning approach to the problem of
estimating missing pixels of images, crucial for image inpainting and
super-resolution problems. One of the main novelties of the method is that it
also provides uncertainty quantifications together with the estimated values.
Our core assumption is that the underlying data-generating function comes from
a Reproducing Kernel Hilbert Space (RKHS). A special emphasis is put on
band-limited functions, central to signal processing, which form Paley-Wiener
type RKHSs. The proposed method, which we call Simultaneously Guaranteed Kernel
Interpolation (SGKI), is an extension and refinement of a recently developed
kernel method. An advantage of SGKI is that it not only estimates the missing
pixels, but also builds non-asymptotic confidence bands for the unobserved
values, which are simultaneously guaranteed for all missing pixels. We also
show how to compute these bands efficiently using Schur complements, we discuss
a generalization to vector-valued functions, and we present a series of
numerical experiments on various datasets containing synthetically generated
and benchmark images, as well.

</details>


### [160] [Masked Gated Linear Unit](https://arxiv.org/abs/2506.23225)
*Yukito Tajima,Nakamasa Inoue,Yusuke Sekikawa,Ikuro Sato,Rio Yokota*

Main category: cs.LG

TL;DR: 提出Masked Gated Linear Units (MGLUs)解决GLUs内存读取瓶颈，有高效内核实现，实验表现良好。


<details>
  <summary>Details</summary>
Motivation: GLUs相比无门控前馈层需两倍内存读取，存在瓶颈，需解决。

Method: 引入MGLUs，包含Mixture of Element-wise Gating (MoEG)架构和FlashMGLU内核。

Result: FlashMGLU在RTX5090 GPU上推理速度比朴素PyTorch MGLU快19.7倍，比标准GLU内存效率高47%、速度快34%；SwiMGLU保持内存优势，下游准确率与或超SwiGLU基线。

Conclusion: MGLUs有效解决了GLUs的内存瓶颈问题，且在实验中表现良好。

Abstract: Gated Linear Units (GLUs) have become essential components in the
feed-forward networks of state-of-the-art Large Language Models (LLMs).
However, they require twice as many memory reads compared to feed-forward
layers without gating, due to the use of separate weight matrices for the gate
and value streams. To address this bottleneck, we introduce Masked Gated Linear
Units (MGLUs), a novel family of GLUs with an efficient kernel implementation.
The core contribution of MGLUs include: (1) the Mixture of Element-wise Gating
(MoEG) architecture that learns multiple binary masks, each determining gate or
value assignments at the element level on a single shared weight matrix
resulting in reduced memory transfer, and (2) FlashMGLU, a hardware-friendly
kernel that yields up to a 19.7 $\times$ inference-time speed-up over a naive
PyTorch MGLU and is 47% more memory-efficient and 34% faster than standard GLUs
despite added architectural complexity on an RTX5090 GPU. In LLM experiments,
the Swish-activated variant SwiMGLU preserves its memory advantages while
matching - or even surpassing - the downstream accuracy of the SwiGLU baseline.

</details>


### [161] [Sub-MoE: Efficient Mixture-of-Expert LLMs Compression via Subspace Expert Merging](https://arxiv.org/abs/2506.23266)
*Lujun Li,Zhu Qiyuan,Jiacheng Wang,Wei Li,Hao Gu,Sirui Han,Yike Guo*

Main category: cs.LG

TL;DR: 本文提出Sub - MoE压缩框架解决Mixture of Experts (MoE)大语言模型因参数规模大带来的问题，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: MoE大语言模型参数规模大，存在内存、存储和部署挑战，现有专家合并方法受参数冲突阻碍。

Method: 提出Sub - MoE框架，包含自适应专家聚类和子空间专家合并两个阶段，对专家权重进行联合奇异值分解，提取共享U矩阵并合并V分量。

Result: 在Mixtral、DeepSeek和Qwen - 1.5|3 MoE大语言模型上实验，Sub - MoE显著优于现有专家修剪和合并方法，在Mixtral - 8x7B零样本基准测试中，减少25%|50%专家时保持96%|86%的原始性能。

Conclusion: Sub - MoE框架有效解决MoE大语言模型的问题，可用于进一步推理优化，代码将公开。

Abstract: Mixture of Experts (MoE) LLMs face significant obstacles due to their massive
parameter scale, which imposes memory, storage, and deployment challenges.
Although recent expert merging methods promise greater efficiency by
consolidating multiple experts, they are fundamentally hindered by parameter
conflicts arising from expert specialization. In this paper, we present
Sub-MoE, a novel MoE compression framework via Subspace Expert Merging. Our key
insight is to perform joint Singular Value Decomposition (SVD) on concatenated
expert weights, reducing conflicting parameters by extracting shared
$U$-matrices while enabling effective merging of the expert-specific $V$
components. Specifically, Sub-MoE consists of two innovative phases: (1)
Adaptive Expert Clustering, which groups functionally coherent experts via
K-means clustering based on cosine similarity of expert outputs; and (2)
Subspace Expert Merging, which first enforces Experts Union Decomposition to
derive the shared $U$-matrix across experts in the same group, then pursues
frequency-based merging for individual $V$-matrices, and finalizes expert
reconstruction using the merged $V$-matrix. In this way, we align and fuse
experts in a shared subspace, and can be extended with intra-expert compression
for further inference optimization. Extensive experiments on Mixtral, DeepSeek,
and Qwen-1.5|3 MoE LLMs demonstrate that our Sub-MoE significantly outperforms
existing expert pruning and merging methods. Notably, our Sub-MoE maintains
96\%|86\% of original performance with 25\%|50\% expert reduction on
Mixtral-8x7B in zero-shot benchmarks. Code will be released at
https://github.com/lliai/MoERazor.

</details>


### [162] [Predicting thinking time in Reasoning models](https://arxiv.org/abs/2506.23274)
*Hans Peter Lynsgøe Raaschou-jensen,Constanza Fierro,Anders Søgaard*

Main category: cs.LG

TL;DR: 论文针对推理模型思考时间不可预测问题，介绍并评估了预测模型“思考时间”的方法，探讨了对用户交互和未来研究的影响。


<details>
  <summary>Details</summary>
Motivation: 推理模型思考时间不可预测会导致用户沮丧，需解决该用户体验问题。

Method: 介绍并评估在线和离线预测模型“思考时间”的方法。

Result: 文中未提及具体结果。

Conclusion: 探讨了对用户交互和未来研究方向的影响。

Abstract: Reasoning models that produce long, hidden chains of thought have emerged as
powerful tools for complex, reasoning-intensive
tasks\citep{deepseekai2025deepseekr1incentivizingreasoningcapability,
openai2024openaio1card}. However, this paradigm introduces a new user
experience challenge: users have little insight into how much time the model
will spend reasoning before returning an answer. This unpredictability, can
lead to user frustration and is likely to compound as LLMs can produce
increasingly long tasks asynchronously
\citep{kwa2025measuringaiabilitycomplete}. In this paper, we introduce and
evaluate methods for both online and offline prediction of model "thinking
time," aiming to develop a practical "progress bar for reasoning." We discuss
the implications for user interaction and future research directions.

</details>


### [163] [BAPE: Learning an Explicit Bayes Classifier for Long-tailed Visual Recognition](https://arxiv.org/abs/2506.23280)
*Chaoqun Du,Yulin Wang,Shiji Song,Gao Huang*

Main category: cs.LG

TL;DR: 本文提出BAPE方法，显式建模后验概率参数，结合分布调整技术，在长尾数据上提升深度学习模型泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习算法在处理现实世界的长尾数据分布时，存在梯度不平衡问题，无法保证贝叶斯最优决策规则。

Method: 提出BAPE方法，显式建模后验概率参数并进行点估计，直接学习贝叶斯分类器；提出分布调整技术，使分类器适应测试数据分布。

Result: 在CIFAR - 10 - LT、CIFAR - 100 - LT、ImageNet - LT和iNaturalist等数据集上，显著提高了流行深度网络的泛化性能。

Conclusion: 所提方法简单有效，能解决长尾数据分布问题，且与现有长尾场景学习方法增益正交。

Abstract: Bayesian decision theory advocates the Bayes classifier as the optimal
approach for minimizing the risk in machine learning problems. Current deep
learning algorithms usually solve for the optimal classifier by
\emph{implicitly} estimating the posterior probabilities, \emph{e.g.}, by
minimizing the Softmax cross-entropy loss. This simple methodology has been
proven effective for meticulously balanced academic benchmark datasets.
However, it is not applicable to the long-tailed data distributions in the real
world, where it leads to the gradient imbalance issue and fails to ensure the
Bayes optimal decision rule. To address these challenges, this paper presents a
novel approach (BAPE) that provides a more precise theoretical estimation of
the data distributions by \emph{explicitly} modeling the parameters of the
posterior probabilities and solving them with point estimation. Consequently,
our method directly learns the Bayes classifier without gradient descent based
on Bayes' theorem, simultaneously alleviating the gradient imbalance and
ensuring the Bayes optimal decision rule. Furthermore, we propose a
straightforward yet effective \emph{distribution adjustment} technique. This
method enables the Bayes classifier trained from the long-tailed training set
to effectively adapt to the test data distribution with an arbitrary imbalance
factor, thereby enhancing performance without incurring additional
computational costs. In addition, we demonstrate the gains of our method are
orthogonal to existing learning approaches for long-tailed scenarios, as they
are mostly designed under the principle of \emph{implicitly} estimating the
posterior probabilities. Extensive empirical evaluations on CIFAR-10-LT,
CIFAR-100-LT, ImageNet-LT, and iNaturalist demonstrate that our method
significantly improves the generalization performance of popular deep networks,
despite its simplicity.

</details>


### [164] [Hierarchical Quantized Diffusion Based Tree Generation Method for Hierarchical Representation and Lineage Analysis](https://arxiv.org/abs/2506.23287)
*Zelin Zang,WenZhe Li,Fei Chen,Yongjie Xu,Chang Yu,Zhen Lei,Stan Z. Li*

Main category: cs.LG

TL;DR: 本文提出基于扩散的HDTree方法用于单细胞分化轨迹分析，在多方面优于现有方法并提供代码。


<details>
  <summary>Details</summary>
Motivation: 传统方法在单细胞分化轨迹分析的计算成本、性能、生成能力和稳定性上有局限，现有基于VAEs的方法也存在稳定性和捕捉深层次层次关系能力的问题。

Method: 引入基于扩散的HDTree方法，使用统一的分层码本和量化扩散过程来捕获分层潜在空间中的树关系，模拟树节点转换。

Result: 在通用和单细胞数据集上的对比显示，HDTree在准确性和性能上优于现有方法。

Conclusion: HDTree为分层谱系分析提供新工具，能更准确高效地建模细胞分化路径，为下游生物学任务提供见解。

Abstract: In single-cell research, tracing and analyzing high-throughput single-cell
differentiation trajectories is crucial for understanding complex biological
processes. Key to this is the modeling and generation of hierarchical data that
represents the intrinsic structure within datasets. Traditional methods face
limitations in terms of computational cost, performance, generative capacity,
and stability. Recent VAEs based approaches have made strides in addressing
these challenges but still require specialized network modules for each tree
branch, limiting their stability and ability to capture deep hierarchical
relationships. To overcome these challenges, we introduce diffusion-based
approach called HDTree. HDTree captures tree relationships within a
hierarchical latent space using a unified hierarchical codebook and quantized
diffusion processes to model tree node transitions. This method improves
stability by eliminating branch-specific modules and enhancing generative
capacity through gradual hierarchical changes simulated by the diffusion
process. HDTree's effectiveness is demonstrated through comparisons on both
general-purpose and single-cell datasets, where it outperforms existing methods
in terms of accuracy and performance. These contributions provide a new tool
for hierarchical lineage analysis, enabling more accurate and efficient
modeling of cellular differentiation paths and offering insights for downstream
biological tasks. The code of HDTree is available at anonymous link
https://anonymous.4open.science/r/code_HDTree_review-A8DB.

</details>


### [165] [VALID-Mol: a Systematic Framework for Validated LLM-Assisted Molecular Design](https://arxiv.org/abs/2506.23339)
*Malikussaid,Hilal Hudan Nuha*

Main category: cs.LG

TL;DR: 提出VALID - Mol框架，将化学验证与大语言模型驱动的分子设计相结合，提高有效化学结构生成率并可推广到其他科学领域。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在需要事实准确性和特定领域约束的应用中存在挑战，如药物发现分子设计中常产生无效或不实用结构。

Method: 结合精心的提示工程、自动化学验证和微调的领域适应大语言模型。

Result: 将有效化学结构生成率从3%提高到83%，计算预测能生成目标亲和力最多有17倍提升且保持可合成性的候选分子。

Conclusion: 为科学约束的大语言模型应用提供可推广方法和可复现蓝图。

Abstract: Large Language Models (LLMs) demonstrate remarkable potential for scientific
discovery, but their application in domains requiring factual accuracy and
domain-specific constraints remains challenging. In molecular design for drug
discovery, LLMs can suggest creative molecular modifications but often produce
chemically invalid or impractical structures. We present VALID-Mol, a
systematic framework for integrating chemical validation with LLM-driven
molecular design that increases the rate of generating valid chemical
structures from 3% to 83%. Our approach combines methodical prompt engineering,
automated chemical validation, and a fine-tuned domain-adapted LLM to ensure
reliable generation of synthesizable molecules with improved properties. Beyond
the specific implementation, we contribute a generalizable methodology for
scientifically-constrained LLM applications, with quantifiable reliability
improvements. Computational predictions suggest our framework can generate
promising candidates for synthesis with up to 17-fold computationally predicted
improvements in target affinity while maintaining synthetic accessibility. We
provide a detailed analysis of our prompt engineering process, validation
architecture, and fine-tuning approach, offering a reproducible blueprint for
applying LLMs to other scientific domains where domain-specific validation is
essential.

</details>


### [166] [A case for data valuation transparency via DValCards](https://arxiv.org/abs/2506.23349)
*Keziah Naggita,Julienne LaChance*

Main category: cs.LG

TL;DR: 研究指出数据估值指标有固有偏差和不稳定性，提出DValCards框架减少其滥用。


<details>
  <summary>Details</summary>
Motivation: 数据估值方法在数据市场有应用，但可能存在问题，需研究其稳定性和公平性。

Method: 分析9个表格分类数据集和6种数据估值方法。

Result: 常见数据预处理技术会改变估值；基于估值的子采样会增加类别不平衡；估值指标可能低估少数群体数据。

Conclusion: 应提高数据估值透明度，DValCards框架可减少其滥用，建立负责任机器学习系统的信任。

Abstract: Following the rise in popularity of data-centric machine learning (ML),
various data valuation methods have been proposed to quantify the contribution
of each datapoint to desired ML model performance metrics (e.g., accuracy).
Beyond the technical applications of data valuation methods (e.g., data
cleaning, data acquisition, etc.), it has been suggested that within the
context of data markets, data buyers might utilize such methods to fairly
compensate data owners. Here we demonstrate that data valuation metrics are
inherently biased and unstable under simple algorithmic design choices,
resulting in both technical and ethical implications. By analyzing 9 tabular
classification datasets and 6 data valuation methods, we illustrate how (1)
common and inexpensive data pre-processing techniques can drastically alter
estimated data values; (2) subsampling via data valuation metrics may increase
class imbalance; and (3) data valuation metrics may undervalue underrepresented
group data. Consequently, we argue in favor of increased transparency
associated with data valuation in-the-wild and introduce the novel Data
Valuation Cards (DValCards) framework towards this aim. The proliferation of
DValCards will reduce misuse of data valuation metrics, including in data
pricing, and build trust in responsible ML systems.

</details>


### [167] [Federated Timeline Synthesis: Scalable and Private Methodology For Model Training and Deployment](https://arxiv.org/abs/2506.23358)
*Pawel Renc,Michal K. Grzeszczyk,Linglong Qian,Nassim Oufattole,Jeff Rasley,Arkadiusz Sitek*

Main category: cs.LG

TL;DR: 提出联邦时间线合成框架FTS用于跨分布式时间序列数据训练生成式基础模型，应用于电子病历，评估显示合成数据训练模型与真实数据相当，且有隐私等优势。


<details>
  <summary>Details</summary>
Motivation: 解决分布式时间序列数据（电子病历）训练生成式基础模型的问题，同时保证隐私性、可扩展性和可扩展性。

Method: 将患者历史表示为PHTs，各机构在本地PHTs上训练自回归变压器并仅传输模型权重到中央服务器，服务器合成轨迹并训练全局生成器。

Result: 在五项临床有意义的预测任务中，全局生成器生成的合成数据训练的模型与真实数据训练的模型表现相当。

Conclusion: FTS具有强隐私保证、跨机构可扩展性，可扩展到多种医疗预测和模拟任务。

Abstract: We present Federated Timeline Synthesis (FTS), a novel framework for training
generative foundation models across distributed timeseries data applied to
electronic health records (EHR). At its core, FTS represents patient history as
tokenized Patient Health Timelines (PHTs), language-agnostic sequences encoding
temporal, categorical, and continuous clinical information. Each institution
trains an autoregressive transformer on its local PHTs and transmits only model
weights to a central server. The server uses the generators to synthesize a
large corpus of trajectories and train a Global Generator (GG), enabling
zero-shot inference via Monte Carlo simulation of future PHTs. We evaluate FTS
on five clinically meaningful prediction tasks using MIMIC-IV data, showing
that models trained on synthetic data generated by GG perform comparably to
those trained on real data. FTS offers strong privacy guarantees, scalability
across institutions, and extensibility to diverse prediction and simulation
tasks especially in healthcare, including counterfactual inference, early
warning detection, and synthetic trial design.

</details>


### [168] [When Additive Noise Meets Unobserved Mediators: Bivariate Denoising Diffusion for Causal Discovery](https://arxiv.org/abs/2506.23374)
*Dominik Meier,Sujai Hiremath,Promit Ghosal,Kyra Gan*

Main category: cs.LG

TL;DR: 文章指出传统基于ANM的双变量因果发现方法在存在未观测中介时失效，提出双变量去噪扩散方法BiDD处理潜在噪声，实验显示其表现良好。


<details>
  <summary>Details</summary>
Motivation: 传统基于ANM的方法在未观测中介存在时失效，现有隐藏中介解决方案在有限样本下不实用。

Method: 提出BiDD方法，引入新的独立性测试统计量，在去噪和加噪过程中以另一变量为条件输入评估预测噪声独立性。

Result: 实验表明BiDD在合成和真实数据上表现稳定，在有中介和无中介情况下均优于现有方法。

Conclusion: BiDD在ANM下具有渐近一致性，在隐藏中介情况下表现良好。

Abstract: Distinguishing cause and effect from bivariate observational data is a
foundational problem in many disciplines, but challenging without additional
assumptions. Additive noise models (ANMs) are widely used to enable
sample-efficient bivariate causal discovery. However, conventional ANM-based
methods fail when unobserved mediators corrupt the causal relationship between
variables. This paper makes three key contributions: first, we rigorously
characterize why standard ANM approaches break down in the presence of
unmeasured mediators. Second, we demonstrate that prior solutions for hidden
mediation are brittle in finite sample settings, limiting their practical
utility. To address these gaps, we propose Bivariate Denoising Diffusion (BiDD)
for causal discovery, a method designed to handle latent noise introduced by
unmeasured mediators. Unlike prior methods that infer directionality through
mean squared error loss comparisons, our approach introduces a novel
independence test statistic: during the noising and denoising processes for
each variable, we condition on the other variable as input and evaluate the
independence of the predicted noise relative to this input. We prove asymptotic
consistency of BiDD under the ANM, and conjecture that it performs well under
hidden mediation. Experiments on synthetic and real-world data demonstrate
consistent performance, outperforming existing methods in mediator-corrupted
settings while maintaining strong performance in mediator-free settings.

</details>


### [169] [Do LLMs Dream of Discrete Algorithms?](https://arxiv.org/abs/2506.23408)
*Claudionor Coelho Jr,Yanen Li,Philip Tee*

Main category: cs.LG

TL;DR: 本文研究大语言模型（LLMs）在逻辑推理等方面的局限性，提出神经符号方法增强LLMs，实验证明该混合架构能提升多步推理能力和系统可靠性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型依赖概率推理，在严格逻辑推理、离散决策和可解释性方面效果不佳，需改进。

Method: 提出神经符号方法，用基于逻辑的推理模块增强LLMs，利用Prolog谓词和可组合工具集，集成一阶逻辑和显式规则系统。

Result: 在DABStep基准测试实验中，混合架构在多步推理任务中提高了精度、覆盖率和系统文档质量。

Conclusion: 将LLMs与模块化逻辑推理结合可恢复工程严谨性，增强系统可靠性，为跨复杂领域的可信、可解释AI智能体提供可扩展途径。

Abstract: Large Language Models (LLMs) have rapidly transformed the landscape of
artificial intelligence, enabling natural language interfaces and dynamic
orchestration of software components. However, their reliance on probabilistic
inference limits their effectiveness in domains requiring strict logical
reasoning, discrete decision-making, and robust interpretability. This paper
investigates these limitations and proposes a neurosymbolic approach that
augments LLMs with logic-based reasoning modules, particularly leveraging
Prolog predicates and composable toolsets. By integrating first-order logic and
explicit rule systems, our framework enables LLMs to decompose complex queries
into verifiable sub-tasks, orchestrate reliable solutions, and mitigate common
failure modes such as hallucination and incorrect step decomposition. We
demonstrate the practical benefits of this hybrid architecture through
experiments on the DABStep benchmark, showing improved precision, coverage, and
system documentation in multi-step reasoning tasks. Our results indicate that
combining LLMs with modular logic reasoning restores engineering rigor,
enhances system reliability, and offers a scalable path toward trustworthy,
interpretable AI agents across complex domains.

</details>


### [170] [BenchMake: Turn any scientific data set into a reproducible benchmark](https://arxiv.org/abs/2506.23419)
*Amanda S Barnard*

Main category: cs.LG

TL;DR: 本文开发并测试了新工具BenchMake，可将公开科学数据集转化为基准数据集，还将其分割结果与现有分割和随机分割进行了比较。


<details>
  <summary>Details</summary>
Motivation: 计算科学中基准数据集相对稀少，使计算科学家评估新创新变得困难。

Method: 使用非负矩阵分解来确定和分离凸包上的挑战性边缘情况，并将匹配数据实例的一部分划分为测试集。

Result: 将BenchMake的分割结果与来自不同科学领域、不同大小、形状和分布的十个公开基准集的现有分割和随机分割进行了比较。

Conclusion: 未在摘要中明确提及。

Abstract: Benchmark data sets are a cornerstone of machine learning development and
applications, ensuring new methods are robust, reliable and competitive. The
relative rarity of benchmark sets in computational science, due to the
uniqueness of the problems and the pace of change in the associated domains,
makes evaluating new innovations difficult for computational scientists. In
this paper a new tool is developed and tested to potentially turn any of the
increasing numbers of scientific data sets made openly available into a
benchmark accessible to the community. BenchMake uses non-negative matrix
factorisation to deterministically identify and isolate challenging edge cases
on the convex hull (the smallest convex set that contains all existing data
instances) and partitions a required fraction of matched data instances into a
testing set that maximises divergence and statistical significance, across
tabular, graph, image, signal and textual modalities. BenchMake splits are
compared to establish splits and random splits using ten publicly available
benchmark sets from different areas of science, with different sizes, shapes,
distributions.

</details>


### [171] [Accurate Parameter-Efficient Test-Time Adaptation for Time Series Forecasting](https://arxiv.org/abs/2506.23424)
*Heitor R. Medeiros,Hossein Sharifi-Noghabi,Gabriel L. Oliveira,Saghar Irandoust*

Main category: cs.LG

TL;DR: 提出PETSA方法，仅更新输入输出的小校准模块以进行参数高效的测试时间适应，结合特殊损失函数，在基准数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现实时间序列的非平稳性降低预训练预测模型性能，现有测试时间适应方法更新全模型成本高。

Method: 提出PETSA方法，用低秩适配器和动态门控调整表示，引入含三个组件的特殊损失函数。

Result: 在基准数据集实验中，PETSA在各预测时长上取得有竞争力或更好的性能。

Conclusion: PETSA提高了各种预测骨干的适应性，且所需参数少于基线。

Abstract: Real-world time series often exhibit a non-stationary nature, degrading the
performance of pre-trained forecasting models. Test-Time Adaptation (TTA)
addresses this by adjusting models during inference, but existing methods
typically update the full model, increasing memory and compute costs. We
propose PETSA, a parameter-efficient method that adapts forecasters at test
time by only updating small calibration modules on the input and output. PETSA
uses low-rank adapters and dynamic gating to adjust representations without
retraining. To maintain accuracy despite limited adaptation capacity, we
introduce a specialized loss combining three components: (1) a robust term, (2)
a frequency-domain term to preserve periodicity, and (3) a patch-wise
structural term for structural alignment. PETSA improves the adaptability of
various forecasting backbones while requiring fewer parameters than baselines.
Experimental results on benchmark datasets show that PETSA achieves competitive
or better performance across all horizons. Our code is available at:
https://github.com/BorealisAI/PETSA

</details>


### [172] [Enhancing Insider Threat Detection Using User-Based Sequencing and Transformer Encoders](https://arxiv.org/abs/2506.23446)
*Mohamed Elbasheer,Adewale Akinfaderin*

Main category: cs.LG

TL;DR: 提出UBS方法结合Transformer编码器架构检测内部威胁，在多个测试集表现优异，优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法处理用户活动时未利用行为序列依赖，难以应对内部威胁检测挑战。

Method: 提出UBS方法将数据集转化为时间序列，用Transformer编码器建模用户正常活动，用重建误差作为异常分数，用OCSVM、LOF和iForest评估。

Result: UBS - Transformer管道在四个测试集达到了高准确率、召回率等指标，假阴性和假阳性率低。

Conclusion: 顺序用户建模和先进异常检测在内部威胁领域有效。

Abstract: Insider threat detection presents unique challenges due to the authorized
status of malicious actors and the subtlety of anomalous behaviors. Existing
machine learning methods often treat user activity as isolated events, thereby
failing to leverage sequential dependencies in user behavior. In this study, we
propose a User-Based Sequencing (UBS) methodology, transforming the CERT
insider threat dataset into structured temporal sequences suitable for deep
sequential modeling. We deploy a Transformer Encoder architecture to model
benign user activity and employ its reconstruction errors as anomaly scores.
These scores are subsequently evaluated using three unsupervised outlier
detection algorithms: One-Class SVM (OCSVM), Local Outlier Factor (LOF), and
Isolation Forest (iForest). Across four rigorously designed test sets,
including combinations of multiple CERT dataset releases, our UBS-Transformer
pipeline consistently achieves state-of-the-art performance - notably 96.61%
accuracy, 99.43% recall, 96.38% F1-score, 95.00% AUROC, and exceptionally low
false negative (0.0057) and false positive (0.0571) rates. Comparative analyses
demonstrate that our approach substantially outperforms tabular and
conventional autoencoder baselines, underscoring the efficacy of sequential
user modeling and advanced anomaly detection in the insider threat domain.

</details>


### [173] [Can We Predict the Unpredictable? Leveraging DisasterNet-LLM for Multimodal Disaster Classification](https://arxiv.org/abs/2506.23462)
*Manaswi Kulahara,Gautam Siddharth Kashyap,Nipun Joshi,Arpita Soni*

Main category: cs.LG

TL;DR: 提出针对灾害分析的DisasterNet - LLM，实验显示其在多模态灾害分类任务中优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以整合多模态数据进行灾害管理，需要更有效的方法。

Method: 提出DisasterNet - LLM，利用高级预训练、跨模态注意力机制和自适应变压器。

Result: DisasterNet - LLM在多模态灾害分类任务中优于现有模型，准确率达89.5%，F1分数为88.0%，AUC为0.92%，BERTScore为0.88%。

Conclusion: DisasterNet - LLM在多模态灾害分类方面表现出色，能有效用于灾害管理。

Abstract: Effective disaster management requires timely and accurate insights, yet
traditional methods struggle to integrate multimodal data such as images,
weather records, and textual reports. To address this, we propose
DisasterNet-LLM, a specialized Large Language Model (LLM) designed for
comprehensive disaster analysis. By leveraging advanced pretraining,
cross-modal attention mechanisms, and adaptive transformers, DisasterNet-LLM
excels in disaster classification. Experimental results demonstrate its
superiority over state-of-the-art models, achieving higher accuracy of 89.5%,
an F1 score of 88.0%, AUC of 0.92%, and BERTScore of 0.88% in multimodal
disaster classification tasks.

</details>


### [174] [Reconciling Attribute and Structural Anomalies for Improved Graph Anomaly Detection](https://arxiv.org/abs/2506.23469)
*Chunjing Xiao,Jiahui Lu,Xovee Xu,Fan Zhou,Tianshu Xie,Wei Lu,Lifeng Xu*

Main category: cs.LG

TL;DR: 提出TripleAD图异常检测框架，含三模块识别不同异常，有互蒸馏策略，实验证明有效


<details>
  <summary>Details</summary>
Motivation: 现有无监督图异常检测方法存在两类异常的权衡问题，性能不佳

Method: 构建TripleAD框架，含多尺度属性估计、链接增强结构估计模块和属性混合曲率指标，采用互蒸馏策略

Result: 实验表明TripleAD模型比强基线模型有效

Conclusion: TripleAD框架能有效检测图中不同类型异常，缓解不同类型异常间的干扰

Abstract: Graph anomaly detection is critical in domains such as healthcare and
economics, where identifying deviations can prevent substantial losses.
Existing unsupervised approaches strive to learn a single model capable of
detecting both attribute and structural anomalies. However, they confront the
tug-of-war problem between two distinct types of anomalies, resulting in
suboptimal performance. This work presents TripleAD, a mutual
distillation-based triple-channel graph anomaly detection framework. It
includes three estimation modules to identify the attribute, structural, and
mixed anomalies while mitigating the interference between different types of
anomalies. In the first channel, we design a multiscale attribute estimation
module to capture extensive node interactions and ameliorate the over-smoothing
issue. To better identify structural anomalies, we introduce a link-enhanced
structure estimation module in the second channel that facilitates information
flow to topologically isolated nodes. The third channel is powered by an
attribute-mixed curvature, a new indicator that encapsulates both attribute and
structural information for discriminating mixed anomalies. Moreover, a mutual
distillation strategy is introduced to encourage communication and
collaboration between the three channels. Extensive experiments demonstrate the
effectiveness of the proposed TripleAD model against strong baselines.

</details>


### [175] [Sample Margin-Aware Recalibration of Temperature Scaling](https://arxiv.org/abs/2506.23492)
*Haolan Guo,Linwei Tao,Haoyang Luo,Minjing Dong,Chang Xu*

Main category: cs.LG

TL;DR: 提出SMART方法解决神经网络过自信问题，通过基于logit gap缩放logits和采用SoftECE目标，在多样数据集和架构上取得SOTA校准性能。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络存在过自信问题，现有后验校准方法有高偏差或高方差的困境。

Method: 提出SMART方法，基于logit gap缩放logits，采用SoftECE目标平衡偏差和方差。

Result: 在多样数据集和架构上，SMART用更少参数取得了SOTA校准性能。

Conclusion: SMART为神经网络预测的不确定性量化提供了原则性、鲁棒且高效的解决方案。

Abstract: Recent advances in deep learning have significantly improved predictive
accuracy. However, modern neural networks remain systematically overconfident,
posing risks for deployment in safety-critical scenarios. Current post-hoc
calibration methods face a fundamental dilemma: global approaches like
Temperature Scaling apply uniform adjustments across all samples, introducing
high bias despite computational efficiency, while more expressive methods that
operate on full logit distributions suffer from high variance due to noisy
high-dimensional inputs and insufficient validation data. To address these
challenges, we propose Sample Margin-Aware Recalibration of Temperature
(SMART), a lightweight, data-efficient recalibration method that precisely
scales logits based on the margin between the top two logits -- termed the
logit gap. Specifically, the logit gap serves as a denoised, scalar signal
directly tied to decision boundary uncertainty, providing a robust indicator
that avoids the noise inherent in high-dimensional logit spaces while
preserving model prediction invariance. Meanwhile, SMART employs a novel
soft-binned Expected Calibration Error (SoftECE) objective that balances model
bias and variance through adaptive binning, enabling stable parameter updates
even with extremely limited calibration data. Extensive evaluations across
diverse datasets and architectures demonstrate that SMART achieves
state-of-the-art calibration performance even with substantially fewer
parameters compared to existing parametric methods, offering a principled,
robust, and highly efficient solution for practical uncertainty quantification
in neural network predictions. The source code is available at:
https://anonymous.4open.science/r/SMART-8B11.

</details>


### [176] [FedWSQ: Efficient Federated Learning with Weight Standardization and Distribution-Aware Non-Uniform Quantization](https://arxiv.org/abs/2506.23516)
*Seung-Wook Kim,Seongyeol Kim,Jiah Kim,Seowon Ji,Se-Ho Lee*

Main category: cs.LG

TL;DR: 提出FedWSQ框架，集成WS和DANUQ，减少通信开销并保持模型精度，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中数据异质性和通信约束导致的性能下降问题。

Method: 提出FedWSQ框架，集成权重标准化（WS）和分布感知非均匀量化（DANUQ）。

Result: FedWSQ显著减少通信开销，保持较高模型精度，在各种具有挑战性的联邦学习设置中始终优于现有方法。

Conclusion: FedWSQ框架能有效应对联邦学习的挑战，提高性能。

Abstract: Federated learning (FL) often suffers from performance degradation due to key
challenges such as data heterogeneity and communication constraints. To address
these limitations, we present a novel FL framework called FedWSQ, which
integrates weight standardization (WS) and the proposed distribution-aware
non-uniform quantization (DANUQ). WS enhances FL performance by filtering out
biased components in local updates during training, thereby improving the
robustness of the model against data heterogeneity and unstable client
participation. In addition, DANUQ minimizes quantization errors by leveraging
the statistical properties of local model updates. As a result, FedWSQ
significantly reduces communication overhead while maintaining superior model
accuracy. Extensive experiments on FL benchmark datasets demonstrate that
FedWSQ consistently outperforms existing FL methods across various challenging
FL settings, including extreme data heterogeneity and ultra-low-bit
communication scenarios.

</details>


### [177] [Both Asymptotic and Non-Asymptotic Convergence of Quasi-Hyperbolic Momentum using Increasing Batch Size](https://arxiv.org/abs/2506.23544)
*Kento Imaizumi,Hideaki Iiduka*

Main category: cs.LG

TL;DR: 本文研究小批量QHM算法，给出渐近和非渐近收敛结果，表明增大批量大小不衰减学习率是更有效策略，实验显示有限增大批量对训练神经网络有益。


<details>
  <summary>Details</summary>
Motivation: 动量方法在随机非凸优化中的理论依据有限，QHM算法可推广多种动量方法，旨在研究小批量QHM算法收敛性。

Method: 对小批量QHM算法进行理论分析，给出渐近和非渐近收敛结果。

Result: 实现渐近收敛需衰减学习率或增大批量大小，不衰减学习率下增大批量更有效，有限增大批量对训练神经网络有益。

Conclusion: 使用小批量QHM算法时，增大批量大小不衰减学习率是更有效的训练策略。

Abstract: Momentum methods were originally introduced for their superiority to
stochastic gradient descent (SGD) in deterministic settings with convex
objective functions. However, despite their widespread application to deep
neural networks -- a representative case of stochastic nonconvex optimization
-- the theoretical justification for their effectiveness in such settings
remains limited. Quasi-hyperbolic momentum (QHM) is an algorithm that
generalizes various momentum methods and has been studied to better understand
the class of momentum-based algorithms as a whole. In this paper, we provide
both asymptotic and non-asymptotic convergence results for mini-batch QHM with
an increasing batch size. We show that achieving asymptotic convergence
requires either a decaying learning rate or an increasing batch size. Since a
decaying learning rate adversely affects non-asymptotic convergence, we
demonstrate that using mini-batch QHM with an increasing batch size -- without
decaying the learning rate -- can be a more effective strategy. Our experiments
show that even a finite increase in batch size can provide benefits for
training neural networks.

</details>


### [178] [A unified framework on the universal approximation of transformer-type architectures](https://arxiv.org/abs/2506.23551)
*Jingpu Cheng,Qianxiao Li,Ting Lin,Zuowei Shen*

Main category: cs.LG

TL;DR: 研究变压器型架构的通用逼近特性，提供统一理论框架，证明多种注意力机制变压器的UAP，为设计新架构提供基础。


<details>
  <summary>Details</summary>
Motivation: 拓展残差网络已有结果到含注意力机制的模型，研究变压器型架构的通用逼近特性。

Method: 确定令牌可区分性为UAP基本要求，引入通用充分条件，利用注意力层解析性假设简化条件验证。

Result: 证明多种注意力机制变压器的UAP，推论推广先前工作或涵盖新架构。

Conclusion: 框架为设计有UAP保证的新型变压器架构提供原则性基础。

Abstract: We investigate the universal approximation property (UAP) of transformer-type
architectures, providing a unified theoretical framework that extends prior
results on residual networks to models incorporating attention mechanisms. Our
work identifies token distinguishability as a fundamental requirement for UAP
and introduces a general sufficient condition that applies to a broad class of
architectures. Leveraging an analyticity assumption on the attention layer, we
can significantly simplify the verification of this condition, providing a
non-constructive approach in establishing UAP for such architectures. We
demonstrate the applicability of our framework by proving UAP for transformers
with various attention mechanisms, including kernel-based and sparse attention
mechanisms. The corollaries of our results either generalize prior works or
establish UAP for architectures not previously covered. Furthermore, our
framework offers a principled foundation for designing novel transformer
architectures with inherent UAP guarantees, including those with specific
functional symmetries. We propose examples to illustrate these insights.

</details>


### [179] [Transition Matching: Scalable and Flexible Generative Modeling](https://arxiv.org/abs/2506.23589)
*Neta Shaul,Uriel Singer,Itai Gat,Yaron Lipman*

Main category: cs.LG

TL;DR: 本文提出Transition Matching (TM)统一并推进扩散/流模型和连续自回归生成，探索三种变体，经对比展示其优势。


<details>
  <summary>Details</summary>
Motivation: 扩散和流模型设计空间探索充分限制改进，自回归模型有潜力，需新范式统一并推进二者。

Method: 提出TM范式，将复杂生成任务分解为简单马尔可夫转换，探索DTM、ARTM和FHTM三种变体。

Result: DTM在图像质量、文本贴合度和采样效率上表现出色；ARTM和FHTM实现连续因果自回归生成，FHTM在连续域文本到图像任务中超越流方法。

Conclusion: TM为生成模型提供新灵活设计途径，三种变体在不同方面表现优异。

Abstract: Diffusion and flow matching models have significantly advanced media
generation, yet their design space is well-explored, somewhat limiting further
improvements. Concurrently, autoregressive (AR) models, particularly those
generating continuous tokens, have emerged as a promising direction for
unifying text and media generation. This paper introduces Transition Matching
(TM), a novel discrete-time, continuous-state generative paradigm that unifies
and advances both diffusion/flow models and continuous AR generation. TM
decomposes complex generation tasks into simpler Markov transitions, allowing
for expressive non-deterministic probability transition kernels and arbitrary
non-continuous supervision processes, thereby unlocking new flexible design
avenues. We explore these choices through three TM variants: (i) Difference
Transition Matching (DTM), which generalizes flow matching to discrete-time by
directly learning transition probabilities, yielding state-of-the-art image
quality and text adherence as well as improved sampling efficiency. (ii)
Autoregressive Transition Matching (ARTM) and (iii) Full History Transition
Matching (FHTM) are partially and fully causal models, respectively, that
generalize continuous AR methods. They achieve continuous causal AR generation
quality comparable to non-causal approaches and potentially enable seamless
integration with existing AR text generation techniques. Notably, FHTM is the
first fully causal model to match or surpass the performance of flow-based
methods on text-to-image task in continuous domains. We demonstrate these
contributions through a rigorous large-scale comparison of TM variants and
relevant baselines, maintaining a fixed architecture, training data, and
hyperparameters.

</details>


### [180] [When Will It Fail?: Anomaly to Prompt for Forecasting Future Anomalies in Time Series](https://arxiv.org/abs/2506.23596)
*Min-Yeong Park,Won-Jeong Lee,Seong Tae Kim,Gyeong-Moon Park*

Main category: cs.LG

TL;DR: 提出A2P框架解决异常预测任务，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列异常预测方法不足，难以准确预测未来异常时间点，需新方法解决异常预测任务。

Method: 提出A2P框架，包含AAF和SAP；采用学习异常关系策略使模型预测异常时间点；引入可学习的APP模拟多样异常模式。

Result: 在多个真实数据集上的综合实验表明，A2P优于现有方法，能预测未来异常。

Conclusion: A2P框架在异常预测任务中表现出色，代码开源。

Abstract: Recently, forecasting future abnormal events has emerged as an important
scenario to tackle real-world necessities. However, the solution of predicting
specific future time points when anomalies will occur, known as Anomaly
Prediction (AP), remains under-explored. Existing methods dealing with time
series data fail in AP, focusing only on immediate anomalies or failing to
provide precise predictions for future anomalies. To address the AP task, we
propose a novel framework called Anomaly to Prompt (A2P), comprised of
Anomaly-Aware Forecasting (AAF) and Synthetic Anomaly Prompting (SAP). To
enable the forecasting model to forecast abnormal time points, we adopt a
strategy to learn the relationships of anomalies. For the robust detection of
anomalies, our proposed SAP introduces a learnable Anomaly Prompt Pool (APP)
that simulates diverse anomaly patterns using signal adaptive prompt.
Comprehensive experiments on multiple real-world datasets demonstrate the
superiority of A2P over state-of-the-art methods, showcasing its ability to
predict future anomalies. Our implementation code is available at
https://github.com/KU-VGI/AP.

</details>


### [181] [A Nonlinear Low-rank Representation Model with Convolutional Neural Network for Imputing Water Quality Data](https://arxiv.org/abs/2506.23629)
*Xin Liao,Bing Yang,Cai Yu*

Main category: cs.LG

TL;DR: 本文提出用于填补水质数据缺失值的非线性低秩表示模型（NLR），实验表明该模型优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 水质数据的完整性对环境监测很重要，但水质监测系统常存在大量缺失数据，传统数据填补方法效果不佳。

Method: 提出结合卷积神经网络（CNN）的非线性低秩表示模型（NLR），利用CNN融合时间特征并提取非线性交互和局部模式。

Result: 在三个真实水质数据集上的实验表明，该模型在估计准确性上显著优于现有先进数据填补模型。

Conclusion: 该模型为复杂动态环境下处理水质监测数据提供了有效方法。

Abstract: The integrity of Water Quality Data (WQD) is critical in environmental
monitoring for scientific decision-making and ecological protection. However,
water quality monitoring systems are often challenged by large amounts of
missing data due to unavoidable problems such as sensor failures and
communication delays, which further lead to water quality data becoming
High-Dimensional and Sparse (HDS). Traditional data imputation methods are
difficult to depict the potential dynamics and fail to capture the deep data
features, resulting in unsatisfactory imputation performance. To effectively
address the above issues, this paper proposes a Nonlinear Low-rank
Representation model (NLR) with Convolutional Neural Networks (CNN) for
imputing missing WQD, which utilizes CNNs to implement two ideas: a) fusing
temporal features to model the temporal dependence of data between time slots,
and b) Extracting nonlinear interactions and local patterns to mine
higher-order relationships features and achieve deep fusion of multidimensional
information. Experimental studies on three real water quality datasets
demonstrate that the proposed model significantly outperforms existing
state-of-the-art data imputation models in terms of estimation accuracy. It
provides an effective approach for handling water quality monitoring data in
complex dynamic environments.

</details>


### [182] [Learning Modular Exponentiation with Transformers](https://arxiv.org/abs/2506.23679)
*David Demitri Africa,Sara M. Kapoor,Theo Simon Sorg*

Main category: cs.LG

TL;DR: 训练4层编解码器Transformer模型进行模幂运算，研究训练中数值推理的出现，发现互逆操作数训练有性能提升及类顿悟动态，还找到最终层注意力头子图可完成任务，表明模型通过专用计算电路学习模运算。


<details>
  <summary>Details</summary>
Motivation: 模幂运算在数论和密码学中至关重要，但从机械可解释性角度研究较少，因此开展研究。

Method: 训练4层编解码器Transformer模型，利用有原则的采样策略、基于PCA的嵌入分析和激活修补方法。

Result: 互逆操作数训练带来性能提升，有类顿悟动态；最终层注意力头子图可完成常规幂运算任务。

Conclusion: Transformer模型通过专用计算电路学习模运算，为模幂运算的更具可解释性和高效的神经方法铺平道路。

Abstract: Modular exponentiation is crucial to number theory and cryptography, yet
remains largely unexplored from a mechanistic interpretability standpoint. We
train a 4-layer encoder-decoder Transformer model to perform this operation and
investigate the emergence of numerical reasoning during training. Utilizing
principled sampling strategies, PCA-based embedding analysis, and activation
patching, we examine how number-theoretic properties are encoded within the
model. We find that reciprocal operand training leads to strong performance
gains, with sudden generalization across related moduli. These synchronized
accuracy surges reflect grokking-like dynamics, suggesting the model
internalizes shared arithmetic structure. We also find a subgraph consisting
entirely of attention heads in the final layer sufficient to achieve full
performance on the task of regular exponentiation. These results suggest that
transformer models learn modular arithmetic through specialized computational
circuits, paving the way for more interpretable and efficient neural approaches
to modular exponentiation.

</details>


### [183] [DABstep: Data Agent Benchmark for Multi-step Reasoning](https://arxiv.org/abs/2506.23719)
*Alex Egg,Martin Iglesias Goyanes,Friso Kingma,Andreu Mora,Leandro von Werra,Thomas Wolf*

Main category: cs.LG

TL;DR: 介绍用于评估AI智能体的多步数据分析任务基准DABstep，评估LLM智能体有较大性能差距，发布相关资源推动研究。


<details>
  <summary>Details</summary>
Motivation: 引入新基准评估AI智能体在现实多步数据分析任务中的表现。

Method: 构建包含超450个真实挑战的DABstep基准，用事实式答案格式自动检查，评估领先的LLM智能体。

Result: 领先的LLM智能体在最难任务上最佳准确率仅14.55%。

Conclusion: DABstep的设计可推动自主数据分析研究，已发布公共排行榜和工具包。

Abstract: We introduce DABstep, a novel benchmark for evaluating AI agents on realistic
multi-step data analysis tasks. DABstep comprises over 450 real-world
challenges derived from a financial analytics platform, requiring models to
combine code-based data processing with contextual reasoning over heterogeneous
documentation. Each task demands an iterative, multi-step problem-solving
approach, testing capabilities in data manipulation, cross-referencing multiple
sources, and precise result reporting. The benchmark provides a factoid-style
answer format with automatic correctness checks for objective scoring at scale.
We evaluate leading LLM-based agents, revealing a substantial performance gap:
even the best agent achieves only 14.55% accuracy on the hardest tasks. We
detail our benchmark's design, dataset composition, task formulation,
evaluation protocol, report baseline results and analyze failure modes. DABstep
is released with a public leaderboard and toolkit to accelerate research in
autonomous data analysis.

</details>


### [184] [System-Embedded Diffusion Bridge Models](https://arxiv.org/abs/2506.23726)
*Bartlomiej Sobieski,Matthew Tivnan,Yuang Wang,Siyeop Yoon,Pengfei Jin,Dufan Wu,Quanzheng Li,Przemyslaw Biecek*

Main category: cs.LG

TL;DR: 提出System embedded Diffusion Bridge Models (SDBs)解决线性逆问题，有一致改进和鲁棒泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有监督桥方法大多忽略测量模型的结构信息，需要更好方法解决线性逆问题。

Method: 引入SDBs，将已知线性测量系统嵌入矩阵值SDE的系数中。

Result: 在各种线性逆问题上有一致改进，在训练和部署间系统误指定情况下有鲁棒泛化性。

Conclusion: SDBs为现实应用提供了有前景的解决方案。

Abstract: Solving inverse problems -- recovering signals from incomplete or noisy
measurements -- is fundamental in science and engineering. Score-based
generative models (SGMs) have recently emerged as a powerful framework for this
task. Two main paradigms have formed: unsupervised approaches that adapt
pretrained generative models to inverse problems, and supervised bridge methods
that train stochastic processes conditioned on paired clean and corrupted data.
While the former typically assume knowledge of the measurement model, the
latter have largely overlooked this structural information. We introduce System
embedded Diffusion Bridge Models (SDBs), a new class of supervised bridge
methods that explicitly embed the known linear measurement system into the
coefficients of a matrix-valued SDE. This principled integration yields
consistent improvements across diverse linear inverse problems and demonstrates
robust generalization under system misspecification between training and
deployment, offering a promising solution to real-world applications.

</details>


### [185] [Radioactive Watermarks in Diffusion and Autoregressive Image Generative Models](https://arxiv.org/abs/2506.23731)
*Michel Meintz,Jan Dubiński,Franziska Boenisch,Adam Dziedzic*

Main category: cs.LG

TL;DR: 分析扩散模型和图像自回归模型生成图像水印放射性，针对IARs提出首个有放射性的水印方法并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 图像生成模型训练需大量数据，水印可检测图像滥用，但现有水印方法在模型训练中难以保持放射性，且无针对IARs的放射性水印方法。

Method: 分析现有扩散模型和图像自回归模型水印放射性，借鉴大语言模型技术，为IARs设计有放射性的水印方法。

Result: 提出的方法能在IARs中有效保留放射性，实现可靠来源追踪，防止生成图像被滥用。

Conclusion: 提出的针对IARs的放射性水印方法有效，可用于保护生成图像。

Abstract: Image generative models have become increasingly popular, but training them
requires large datasets that are costly to collect and curate. To circumvent
these costs, some parties may exploit existing models by using the generated
images as training data for their own models. In general, watermarking is a
valuable tool for detecting unauthorized use of generated images. However, when
these images are used to train a new model, watermarking can only enable
detection if the watermark persists through training and remains identifiable
in the outputs of the newly trained model - a property known as radioactivity.
We analyze the radioactivity of watermarks in images generated by diffusion
models (DMs) and image autoregressive models (IARs). We find that existing
watermarking methods for DMs fail to retain radioactivity, as watermarks are
either erased during encoding into the latent space or lost in the
noising-denoising process (during the training in the latent space). Meanwhile,
despite IARs having recently surpassed DMs in image generation quality and
efficiency, no radioactive watermarking methods have been proposed for them. To
overcome this limitation, we propose the first watermarking method tailored for
IARs and with radioactivity in mind - drawing inspiration from techniques in
large language models (LLMs), which share IARs' autoregressive paradigm. Our
extensive experimental evaluation highlights our method's effectiveness in
preserving radioactivity within IARs, enabling robust provenance tracking, and
preventing unauthorized use of their generated images.

</details>


### [186] [Model-driven Stochastic Trace Clustering](https://arxiv.org/abs/2506.23776)
*Jari Peeperkorn,Johannes De Smedt,Jochen De Weerdt*

Main category: cs.LG

TL;DR: 提出一种优化随机过程模型的模型驱动轨迹聚类方法，能考虑活动频率和概率，实验显示该方法优于现有替代方案。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹聚类技术未考虑活动和转换频率或概率，难以捕捉现实执行动态，需改进。

Method: 提出优化各集群内随机过程模型的方法，用基于直接跟随概率的熵相关性指标指导轨迹分配。

Result: 方法计算高效，随输入规模线性扩展，提高模型可解释性，实验表明在表示过程行为上优于现有方法。

Conclusion: 该方法能更好捕捉现实执行动态，考虑随机性时聚类性能排名会改变。

Abstract: Process discovery algorithms automatically extract process models from event
logs, but high variability often results in complex and hard-to-understand
models. To mitigate this issue, trace clustering techniques group process
executions into clusters, each represented by a simpler and more understandable
process model. Model-driven trace clustering improves on this by assigning
traces to clusters based on their conformity to cluster-specific process
models. However, most existing clustering techniques rely on either no process
model discovery, or non-stochastic models, neglecting the frequency or
probability of activities and transitions, thereby limiting their capability to
capture real-world execution dynamics. We propose a novel model-driven trace
clustering method that optimizes stochastic process models within each cluster.
Our approach uses entropic relevance, a stochastic conformance metric based on
directly-follows probabilities, to guide trace assignment. This allows
clustering decisions to consider both structural alignment with a cluster's
process model and the likelihood that a trace originates from a given
stochastic process model. The method is computationally efficient, scales
linearly with input size, and improves model interpretability by producing
clusters with clearer control-flow patterns. Extensive experiments on public
real-life datasets show that our method outperforms existing alternatives in
representing process behavior and reveals how clustering performance rankings
can shift when stochasticity is considered.

</details>


### [187] [Calibrating Graph Neural Networks with Wavelet-Aware Temperature Scaling](https://arxiv.org/abs/2506.23782)
*Xiaoyang Li,Linwei Tao,Haohui Lu,Minjing Dong,Junbin Gao,Chang Xu*

Main category: cs.LG

TL;DR: 提出基于图小波特征的WATS后校准框架，提升GNN置信度校准效果且计算高效。


<details>
  <summary>Details</summary>
Motivation: 现有图感知校准方法依赖粗粒度统计，忽略图拓扑细粒度结构异质性，GNN置信度估计与实际预测正确性常不一致。

Method: 提出Wavelet - Aware Temperature Scaling (WATS)后校准框架，基于可调热核图小波特征分配节点特定温度。

Result: 在七个基准数据集和两个GNN骨干网络上，WATS的预期校准误差（ECE）最低，比经典和图特定基线方法最高降低42.3%，平均减少校准方差17.24%，计算高效。

Conclusion: WATS能有效提升GNN置信度校准效果，且在不同规模和密度图上有良好扩展性。

Abstract: Graph Neural Networks (GNNs) have demonstrated strong predictive performance
on relational data; however, their confidence estimates often misalign with
actual predictive correctness, posing significant limitations for deployment in
safety-critical settings. While existing graph-aware calibration methods seek
to mitigate this limitation, they primarily depend on coarse one-hop
statistics, such as neighbor-predicted confidence, or latent node embeddings,
thereby neglecting the fine-grained structural heterogeneity inherent in graph
topology. In this work, we propose Wavelet-Aware Temperature Scaling (WATS), a
post-hoc calibration framework that assigns node-specific temperatures based on
tunable heat-kernel graph wavelet features. Specifically, WATS harnesses the
scalability and topology sensitivity of graph wavelets to refine confidence
estimates, all without necessitating model retraining or access to neighboring
logits or predictions. Extensive evaluations across seven benchmark datasets
with varying graph structures and two GNN backbones demonstrate that WATS
achieves the lowest Expected Calibration Error (ECE) among all compared
methods, outperforming both classical and graph-specific baselines by up to
42.3\% in ECE and reducing calibration variance by 17.24\% on average compared
with graph-specific methods. Moreover, WATS remains computationally efficient,
scaling well across graphs of diverse sizes and densities. Code will be
released based on publication.

</details>


### [188] [KAIROS: Scalable Model-Agnostic Data Valuation](https://arxiv.org/abs/2506.23799)
*Jiongli Zhu,Parjanya Prajakta Prashant,Alex Cloninger,Babak Salimi*

Main category: cs.LG

TL;DR: 介绍了可扩展、模型无关的估值框架KAIROS，能分配分布影响分数，支持高效在线更新，在准确性和运行时间上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有AI资产训练数据估值方法存在不足，如模型技术有偏差、算法方法成本高、Wasserstein方法近似有问题。

Method: 引入KAIROS框架，通过计算每个示例对经验训练分布和干净参考集之间最大平均差异（MMD）的贡献来分配影响分数。

Result: KAIROS在噪声、错误标记和中毒基准测试中，在准确性和运行时间上始终优于现有基线。

Conclusion: KAIROS是一种有效的训练数据估值框架，有严格理论保证。

Abstract: Training data increasingly shapes not only model accuracy but also regulatory
compliance and market valuation of AI assets. Yet existing valuation methods
remain inadequate: model-based techniques depend on a single fitted model and
inherit its biases, while algorithm-based approaches such as Data Shapley
require costly retrainings at web scale. Recent Wasserstein-based
model-agnostic methods rely on approximations that misrank examples relative to
their true leave-one-out (LOO) utility. We introduce KAIROS, a scalable,
model-agnostic valuation framework that assigns each example a distributional
influence score: its contribution to the Maximum Mean Discrepancy (MMD) between
the empirical training distribution and a clean reference set. Unlike
Wasserstein surrogates, our MMD-based influence admits a closed-form solution
that faithfully approximates the exact LOO ranking within $O(1/N^2)$ error,
requires no retraining, and naturally extends to conditional kernels for
unified label- and feature-error detection. Moreover, KAIROS supports efficient
online updates: when a new batch of size m arrives, all scores can be updated
in $O(mN)$ time, delivering up to 50x speedup without compromising ranking
quality. Empirical evaluations on noise, mislabeling, and poisoning benchmarks
show that KAIROS consistently outperforms state-of-the-art model-, Shapley-,
and Wasserstein-based baselines in both accuracy and runtime. We provide
rigorous theoretical guarantees, including symmetry for reproducible rankings
and density-separation for interpretable thresholds.

</details>


### [189] [Towards the Training of Deeper Predictive Coding Neural Networks](https://arxiv.org/abs/2506.23800)
*Chang Qi,Matteo Forasassi,Thomas Lukasiewicz,Tommaso Salvatori*

Main category: cs.LG

TL;DR: 本文解决预测编码网络在深度超过5 - 7层时性能下降问题，提出优化方法，在图像分类任务上提升测试精度。


<details>
  <summary>Details</summary>
Motivation: 以往预测编码网络在浅层架构有效，但深度超过5 - 7层性能显著下降，需解决此问题。

Method: 引入两种优化潜变量的新方法，利用精度加权在‘松弛阶段’重新平衡各层能量分布；提出新的权重更新机制，减少深层误差积累。

Result: 在大量图像分类任务中，超过7层的网络测试精度大幅提高，性能与类似模型的反向传播相当。

Conclusion: 更好地理解松弛阶段对大规模训练基于平衡传播的模型很重要，为其在复杂任务中的应用开辟新可能。

Abstract: Predictive coding networks trained with equilibrium propagation are neural
models that perform inference through an iterative energy minimization process.
Previous studies have demonstrated their effectiveness in shallow
architectures, but show significant performance degradation when depth exceeds
five to seven layers. In this work, we show that the reason behind this
degradation is due to exponentially imbalanced errors between layers during
weight updates, and predictions from the previous layer not being effective in
guiding updates in deeper layers. We address the first issue by introducing two
novel methods to optimize the latent variables that use precision-weighting to
re-balance the distribution of energy among layers during the `relaxation
phase', and the second issue by proposing a novel weight update mechanism that
reduces error accumulation in deeper layers. Empirically, we test our methods
on a large number of image classification tasks, resulting in large
improvements in test accuracy across networks with more than seven layers, with
performances comparable to those of backprop on similar models. These findings
suggest that a better understanding of the relaxation phase is important to
train models using equilibrium propagation at scale, and open new possibilities
for their application in complex tasks.

</details>


### [190] [Adaptive Out-of-Control Point Pattern Detection in Sequential Random Finite Set Observations](https://arxiv.org/abs/2506.23802)
*Konstantinos Bourazas,Savvas Papaioannou,Panayiotis Kolios*

Main category: cs.LG

TL;DR: 提出一种用于监测顺序随机有限集观测的自适应异常检测框架。


<details>
  <summary>Details</summary>
Motivation: 设计一种有效区分正常数据和异常数据的方法，以监测顺序随机有限集观测。

Method: 开发基于随机有限集的框架，引入Power Discounting Posteriors (PD) 类后验分布和新的预测后验密度函数。

Result: 通过大量定性和定量模拟实验证明了该方法的有效性。

Conclusion: 所提出的自适应异常检测框架能有效检测顺序随机有限集观测中的异常。

Abstract: In this work we introduce a novel adaptive anomaly detection framework
specifically designed for monitoring sequential random finite set (RFS)
observations. Our approach effectively distinguishes between In-Control data
(normal) and Out-Of-Control data (anomalies) by detecting deviations from the
expected statistical behavior of the process. The primary contributions of this
study include the development of an innovative RFS-based framework that not
only learns the normal behavior of the data-generating process online but also
dynamically adapts to behavioral shifts to accurately identify abnormal point
patterns. To achieve this, we introduce a new class of RFS-based posterior
distributions, named Power Discounting Posteriors (PD), which facilitate
adaptation to systematic changes in data while enabling anomaly detection of
point pattern data through a novel predictive posterior density function. The
effectiveness of the proposed approach is demonstrated by extensive qualitative
and quantitative simulation experiments.

</details>


### [191] [SGD with Adaptive Preconditioning: Unified Analysis and Momentum Acceleration](https://arxiv.org/abs/2506.23803)
*Dmitry Kovalev*

Main category: cs.LG

TL;DR: 重新审视带AdaGrad类型预条件的随机梯度下降（SGD），给出统一收敛分析并加速部分方法收敛。


<details>
  <summary>Details</summary>
Motivation: 对带自适应预条件的SGD进行统一收敛分析，探索相关算法联系及加速收敛的可能性。

Method: 进行统一收敛分析，建立算法间联系，并使用Nesterov动量加速方法收敛。

Result: 能恢复几种流行自适应梯度方法的最优收敛结果，建立Scion和DASGO联系并给出DASGO理论保证，证明部分方法可被加速。

Conclusion: AdaGrad类型算法可同时从对角预条件和动量中受益，可能解释Adam的实际效率。

Abstract: In this paper, we revisit stochastic gradient descent (SGD) with AdaGrad-type
preconditioning. Our contributions are twofold. First, we develop a unified
convergence analysis of SGD with adaptive preconditioning under anisotropic or
matrix smoothness and noise assumptions. This allows us to recover
state-of-the-art convergence results for several popular adaptive gradient
methods, including AdaGrad-Norm, AdaGrad, and ASGO/One-sided Shampoo. In
addition, we establish the fundamental connection between two recently proposed
algorithms, Scion and DASGO, and provide the first theoretical guarantees for
the latter. Second, we show that the convergence of methods like AdaGrad and
DASGO can be provably accelerated beyond the best-known rates using Nesterov
momentum. Consequently, we obtain the first theoretical justification that
AdaGrad-type algorithms can simultaneously benefit from both diagonal
preconditioning and momentum, which may provide an ultimate explanation for the
practical efficiency of Adam.

</details>


### [192] [Supercm: Revisiting Clustering for Semi-Supervised Learning](https://arxiv.org/abs/2506.23824)
*Durgesh Singh,Ahcene Boubekki,Robert Jenssen,Michael C. Kampffmeyer*

Main category: cs.LG

TL;DR: 提出新半监督学习方法，结合聚类假设，简单可端到端训练，能提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有半监督学习方法常采用复杂训练策略，期望找到更优方法。

Method: 通过扩展可微聚类模块，显式融入半监督学习中的聚类假设，利用标注数据引导聚类中心。

Result: 提出的模型性能优于仅使用监督学习的基线，且可与其他半监督学习方法结合进一步提升性能。

Conclusion: 所提方法有效，能提升半监督学习性能，还可与其他方法联用。

Abstract: The development of semi-supervised learning (SSL) has in recent years largely
focused on the development of new consistency regularization or entropy
minimization approaches, often resulting in models with complex training
strategies to obtain the desired results. In this work, we instead propose a
novel approach that explicitly incorporates the underlying clustering
assumption in SSL through extending a recently proposed differentiable
clustering module. Leveraging annotated data to guide the cluster centroids
results in a simple end-to-end trainable deep SSL approach. We demonstrate that
the proposed model improves the performance over the supervised-only baseline
and show that our framework can be used in conjunction with other SSL methods
to further boost their performance.

</details>


### [193] [EFPI: Elastic Formation and Position Identification in Football (Soccer) using Template Matching and Linear Assignment](https://arxiv.org/abs/2506.23843)
*Joris Bekkers*

Main category: cs.LG

TL;DR: 本文提出名为EFPI的足球阵型识别和球员位置分配方法，利用预定义模板和时空跟踪数据的成本最小化实现，可用于不同比赛片段，且有稳定性参数，代码开源。


<details>
  <summary>Details</summary>
Motivation: 理解球队阵型和球员位置对足球战术分析至关重要，需要一种有效方法来实现阵型识别和球员位置分配。

Method: 采用线性和分配，通过最小化实际球员位置与模板位置的总距离，将球员与模板阵型中的位置进行最优匹配，选择分配成本最低的阵型；缩放球员位置以匹配模板尺寸；引入可选的稳定性参数。

Result: 该方法在单个帧上有效，可自然扩展到更大的比赛片段。

Conclusion: 提出的EFPI方法能有效进行足球阵型识别和球员位置分配，代码开源方便使用。

Abstract: Understanding team formations and player positioning is crucial for tactical
analysis in football (soccer). This paper presents a flexible method for
formation recognition and player position assignment in football using
predefined static formation templates and cost minimization from spatiotemporal
tracking data, called EFPI. Our approach employs linear sum assignment to
optimally match players to positions within a set of template formations by
minimizing the total distance between actual player locations and template
positions, subsequently selecting the formation with the lowest assignment
cost. To improve accuracy, we scale actual player positions to match the
dimensions of these formation templates in both width and length. While the
method functions effectively on individual frames, it extends naturally to
larger game segments such as complete periods, possession sequences or specific
intervals (e.g. 10 second intervals, 5 minute intervals etc.). Additionally, we
incorporate an optional stability parameter that prevents unnecessary formation
changes when assignment costs differ only marginally between time segments.
EFPI is available as open-source code through the unravelsports Python package.

</details>


### [194] [Use Sparse Autoencoders to Discover Unknown Concepts, Not to Act on Known Concepts](https://arxiv.org/abs/2506.23845)
*Kenny Peng,Rajiv Movva,Jon Kleinberg,Emma Pierson,Nikhil Garg*

Main category: cs.LG

TL;DR: 本文调和了关于稀疏自编码器（SAEs）的争议观点，指出其在发现未知概念方面有效，并给出应用场景。


<details>
  <summary>Details</summary>
Motivation: SAEs引发关注但负面结果增加了对其有用性的怀疑，需调和相关争议。

Method: 建立概念区分，将SAEs在处理已知概念和发现未知概念的作用分开。

Result: 该区分清晰分离了现有负面和正面结果，且提出了SAE的几类应用。

Conclusion: SAEs虽对处理已知概念效果不佳，但在发现未知概念方面是强大工具，可应用于ML和社科、健康科学领域。

Abstract: While sparse autoencoders (SAEs) have generated significant excitement, a
series of negative results have added to skepticism about their usefulness.
Here, we establish a conceptual distinction that reconciles competing
narratives surrounding SAEs. We argue that while SAEs may be less effective for
acting on known concepts, SAEs are powerful tools for discovering unknown
concepts. This distinction cleanly separates existing negative and positive
results, and suggests several classes of SAE applications. Specifically, we
outline use cases for SAEs in (i) ML interpretability, explainability,
fairness, auditing, and safety, and (ii) social and health sciences.

</details>


### [195] [When Plants Respond: Electrophysiology and Machine Learning for Green Monitoring Systems](https://arxiv.org/abs/2506.23872)
*Eduard Buss,Till Aust,Heiko Hamann*

Main category: cs.LG

TL;DR: 本文利用植物作为天然传感器，构建生物混合系统，通过在植物上安装设备收集电生理数据，用机器学习分析，模型表现良好，推动了可持续环境监测的植物集成生物混合系统发展。


<details>
  <summary>Details</summary>
Motivation: 植物作为天然传感器可提供环境监测和精准农业应用的数据，构建生物混合系统建立植物与人工设备的生理信号通道。

Method: 给常春藤配备可穿戴设备PhytoNode记录电生理活动，在室外环境收集数据，用AutoML分析数据。

Result: 分类模型表现出色，二分类任务中宏观F1分数高达95%，AutoML优于手动调参，选择统计特征子集可提高准确性。

Conclusion: 该生物混合系统可在恶劣现实环境中监测植物电生理，推动了可持续环境监测的可扩展、自维持、植物集成生物混合系统发展。

Abstract: Living plants, while contributing to ecological balance and climate
regulation, also function as natural sensors capable of transmitting
information about their internal physiological states and surrounding
conditions. This rich source of data provides potential for applications in
environmental monitoring and precision agriculture. With integration into
biohybrid systems, we establish novel channels of physiological signal flow
between living plants and artificial devices. We equipped *Hedera helix* with a
plant-wearable device called PhytoNode to continuously record the plant's
electrophysiological activity. We deployed plants in an uncontrolled outdoor
environment to map electrophysiological patterns to environmental conditions.
Over five months, we collected data that we analyzed using state-of-the-art and
automated machine learning (AutoML). Our classification models achieve high
performance, reaching macro F1 scores of up to 95 percent in binary tasks.
AutoML approaches outperformed manual tuning, and selecting subsets of
statistical features further improved accuracy. Our biohybrid living system
monitors the electrophysiology of plants in harsh, real-world conditions. This
work advances scalable, self-sustaining, and plant-integrated living biohybrid
systems for sustainable environmental monitoring.

</details>


### [196] [Chain of Thought in Order: Discovering Learning-Friendly Orders for Arithmetic](https://arxiv.org/abs/2506.23875)
*Yuta Sato,Kazuhiko Kawamoto,Hiroshi Kera*

Main category: cs.LG

TL;DR: 研究Transformer中思维链的顺序重排问题，提出管道和分层方法，实验证明能从数十亿候选中找到学习友好顺序。


<details>
  <summary>Details</summary>
Motivation: 思维链中步骤顺序对推理难度有重要影响，解决将解码器输入令牌重排为学习友好序列以让Transformer学习算术任务的问题。

Method: 先在不同顺序的目标序列混合上训练Transformer，识别早期损失快速下降的良性顺序；针对搜索空间随序列长度呈阶乘增长问题，提出两阶段分层的块间和块内重排方法。

Result: 在四个对顺序敏感的算术任务实验中，能从数十亿候选中找到学习友好顺序，在乘法任务中恢复了先前研究报告的逆数字顺序。

Conclusion: 所提方法能有效解决思维链重排问题，找到学习友好的顺序。

Abstract: The chain of thought is fundamental in Transformers, which is to perform
step-by-step reasoning. Besides what intermediate steps work, the order of
these steps critically affects the difficulty of the reasoning. This study
addresses a novel task of unraveling chain of thought - reordering decoder
input tokens to a learning-friendly sequence for Transformers to learn
arithmetic tasks. The proposed pipeline first trains a Transformer on a mixture
of target sequences arranged in different orders and then identifies benign
orders as those with fast loss drops in the early stage. As the search space
grows factorially with sequence length, we propose a two-stage hierarchical
approach for inter- and intra-block reordering. Experiments on four
order-sensitive arithmetic tasks show that our method identifies a
learning-friendly order out of a few billion candidates. Notably, on the
multiplication task, it recovered the reverse-digit order reported in prior
studies.

</details>


### [197] [Reinforcement Learning for Synchronised Flow Control in a Dual-Gate Resin Infusion System](https://arxiv.org/abs/2506.23923)
*Miguel Camacho-Sánchez,Fernando García-Torres,Jesper John Lisegaard,Rocío del Amor,Sankhya Mohanty,Valery Naranjo*

Main category: cs.LG

TL;DR: 本文提出基于强化学习策略控制树脂流动，结果显示该方法能实现准确流动收敛，提升复合材料制造质量。


<details>
  <summary>Details</summary>
Motivation: 控制树脂灌注和传递模塑过程中树脂流动动力学，确保纤维增强材料均匀浸渍，防止影响最终部件结构完整性的缺陷。

Method: 使用过程模拟建立基于强化学习（RL）的策略，采用近端策略优化（PPO）处理部分可观测环境中的流体动力学挑战。

Result: RL方法在实现准确流动收敛方面有效。

Conclusion: RL方法有潜力改善复合材料制造过程控制和产品质量。

Abstract: Resin infusion (RI) and resin transfer moulding (RTM) are critical processes
for the manufacturing of high-performance fibre-reinforced polymer composites,
particularly for large-scale applications such as wind turbine blades.
Controlling the resin flow dynamics in these processes is critical to ensure
the uniform impregnation of the fibre reinforcements, thereby preventing
residual porosities and dry spots that impact the consequent structural
integrity of the final component. This paper presents a reinforcement learning
(RL) based strategy, established using process simulations, for synchronising
the different resin flow fronts in an infusion scenario involving two resin
inlets and a single outlet. Using Proximal Policy Optimisation (PPO), our
approach addresses the challenge of managing the fluid dynamics in a partially
observable environment. The results demonstrate the effectiveness of the RL
approach in achieving an accurate flow convergence, highlighting its potential
towards improving process control and product quality in composites
manufacturing.

</details>


### [198] [Bridging the Gap with Retrieval-Augmented Generation: Making Prosthetic Device User Manuals Available in Marginalised Languages](https://arxiv.org/abs/2506.23958)
*Ikechukwu Ogbonna,Lesley Davidson,Soumya Banerjee,Abhishek Dasgupta,Laurence Kenney,Vikranth Harthikote Nagaraja*

Main category: cs.LG

TL;DR: 研究将复杂医疗文件转化为服务不足群体可访问格式，用AI框架处理和翻译，集成RAG和NLP模型，助医患做决策。


<details>
  <summary>Details</summary>
Motivation: 非洲国家数百万人因语言和读写能力差距在获取医疗服务上存在障碍，且受捐赠假肢设备的社区难获取合适的用户文档。

Method: 提出AI驱动框架，集成Retrieval - Augmented Generation (RAG) 管道处理和理解文档，用先进自然语言处理 (NLP) 模型进行问答和多语言翻译。

Result: 系统能让用户上传英文医疗设备手册，用母语提问并实时获得准确本地化答案。

Conclusion: 该框架可确保医疗设备说明等信息的可访问性，使患者和临床医生能做出明智的医疗决策。

Abstract: Millions of people in African countries face barriers to accessing healthcare
due to language and literacy gaps. This research tackles this challenge by
transforming complex medical documents -- in this case, prosthetic device user
manuals -- into accessible formats for underserved populations. This case study
in cross-cultural translation is particularly pertinent/relevant for
communities that receive donated prosthetic devices but may not receive the
accompanying user documentation. Or, if available online, may only be available
in formats (e.g., language and readability) that are inaccessible to local
populations (e.g., English-language, high resource settings/cultural context).
The approach is demonstrated using the widely spoken Pidgin dialect, but our
open-source framework has been designed to enable rapid and easy extension to
other languages/dialects. This work presents an AI-powered framework designed
to process and translate complex medical documents, e.g., user manuals for
prosthetic devices, into marginalised languages. The system enables users --
such as healthcare workers or patients -- to upload English-language medical
equipment manuals, pose questions in their native language, and receive
accurate, localised answers in real time. Technically, the system integrates a
Retrieval-Augmented Generation (RAG) pipeline for processing and semantic
understanding of the uploaded manuals. It then employs advanced Natural
Language Processing (NLP) models for generative question-answering and
multilingual translation. Beyond simple translation, it ensures accessibility
to device instructions, treatment protocols, and safety information, empowering
patients and clinicians to make informed healthcare decisions.

</details>


### [199] [UMA: A Family of Universal Models for Atoms](https://arxiv.org/abs/2506.23971)
*Brandon M. Wood,Misko Dzamba,Xiang Fu,Meng Gao,Muhammed Shuaibi,Luis Barroso-Luque,Kareem Abdelmaqsoud,Vahe Gharakhanyan,John R. Kitchin,Daniel S. Levine,Kyle Michel,Anuroop Sriram,Taco Cohen,Abhishek Das,Ammar Rizvi,Sushree Jagriti Sahoo,Zachary W. Ulissi,C. Lawrence Zitnick*

Main category: cs.LG

TL;DR: Meta FAIR提出原子通用模型UMA，在大量原子结构上训练，评估显示单模型性能佳并开源相关资源。


<details>
  <summary>Details</summary>
Motivation: 满足化学和材料科学中快速准确计算原子模拟属性的需求。

Method: 在半十亿个3D原子结构上训练，开发经验缩放定律，UMA中小模型采用线性专家混合架构。

Result: 单一UMA模型无需微调在多领域应用中表现与或优于专用模型。

Conclusion: 发布UMA代码、权重和数据以加速计算工作流程，助力社区构建更强大AI模型。

Abstract: The ability to quickly and accurately compute properties from atomic
simulations is critical for advancing a large number of applications in
chemistry and materials science including drug discovery, energy storage, and
semiconductor manufacturing. To address this need, Meta FAIR presents a family
of Universal Models for Atoms (UMA), designed to push the frontier of speed,
accuracy, and generalization. UMA models are trained on half a billion unique
3D atomic structures (the largest training runs to date) by compiling data
across multiple chemical domains, e.g. molecules, materials, and catalysts. We
develop empirical scaling laws to help understand how to increase model
capacity alongside dataset size to achieve the best accuracy. The UMA small and
medium models utilize a novel architectural design we refer to as mixture of
linear experts that enables increasing model capacity without sacrificing
speed. For example, UMA-medium has 1.4B parameters but only ~50M active
parameters per atomic structure. We evaluate UMA models on a diverse set of
applications across multiple domains and find that, remarkably, a single model
without any fine-tuning can perform similarly or better than specialized
models. We are releasing the UMA code, weights, and associated data to
accelerate computational workflows and enable the community to continue to
build increasingly capable AI models.

</details>


### [200] [A Scalable Approach for Safe and Robust Learning via Lipschitz-Constrained Networks](https://arxiv.org/abs/2506.23977)
*Zain ul Abdeen,Vassilis Kekatos,Ming Jin*

Main category: cs.LG

TL;DR: 本文提出凸训练框架及RS - LMI方法，在MNIST等数据集上取得有竞争力的准确率，改进Lipschitz边界和运行时间性能。


<details>
  <summary>Details</summary>
Motivation: 现有Lipschitz约束训练方法存在非凸和可扩展性差的问题，需寻找更好方法来实现神经网络的认证鲁棒性。

Method: 提出通过半定松弛实施全局Lipschitz约束的凸训练框架，用循环变换重新参数化NN得到凸可容许条件；开发RS - LMI方法将全局约束分解为低维子空间上的层约束。

Result: 在MNIST、CIFAR - 10和ImageNet上，该框架实现了有竞争力的准确率，显著改进了Lipschitz边界和运行时间性能。

Conclusion: 所提框架和方法能有效解决现有Lipschitz约束训练的问题，在保证鲁棒性的同时提高可扩展性。

Abstract: Certified robustness is a critical property for deploying neural networks
(NN) in safety-critical applications. A principle approach to achieving such
guarantees is to constrain the global Lipschitz constant of the network.
However, accurate methods for Lipschitz-constrained training often suffer from
non-convex formulations and poor scalability due to reliance on global
semidefinite programs (SDPs). In this letter, we propose a convex training
framework that enforces global Lipschitz constraints via semidefinite
relaxation. By reparameterizing the NN using loop transformation, we derive a
convex admissibility condition that enables tractable and certifiable training.
While the resulting formulation guarantees robustness, its scalability is
limited by the size of global SDP. To overcome this, we develop a randomized
subspace linear matrix inequalities (RS-LMI) approach that decomposes the
global constraints into sketched layerwise constraints projected onto
low-dimensional subspaces, yielding a smooth and memory-efficient training
objective. Empirical results on MNIST, CIFAR-10, and ImageNet demonstrate that
the proposed framework achieves competitive accuracy with significantly
improved Lipschitz bounds and runtime performance.

</details>


### [201] [LLM Agents Are the Antidote to Walled Gardens](https://arxiv.org/abs/2506.23978)
*Samuele Marro,Philip Torr*

Main category: cs.LG

TL;DR: 当前应用层被封闭平台主导，LLM 代理可实现通用互操作性，虽有风险但 ML 社区应积极应对。


<details>
  <summary>Details</summary>
Motivation: 解决当前应用层封闭、缺乏数据交换的问题，打破市场垄断。

Method: 利用 LLM 代理自动转换数据格式并与人类接口交互，实现通用互操作性。

Result: 实现通用互操作性，可削弱垄断行为、促进数据可移植性，但会带来新安全风险和技术债务。

Conclusion: ML 社区应拥抱此发展，构建框架缓解弊端，利用 AI 恢复用户自由和市场竞争且保障安全。

Abstract: While the Internet's core infrastructure was designed to be open and
universal, today's application layer is dominated by closed, proprietary
platforms. Open and interoperable APIs require significant investment, and
market leaders have little incentive to enable data exchange that could erode
their user lock-in. We argue that LLM-based agents fundamentally disrupt this
status quo. Agents can automatically translate between data formats and
interact with interfaces designed for humans: this makes interoperability
dramatically cheaper and effectively unavoidable. We name this shift universal
interoperability: the ability for any two digital services to exchange data
seamlessly using AI-mediated adapters. Universal interoperability undermines
monopolistic behaviours and promotes data portability. However, it can also
lead to new security risks and technical debt. Our position is that the ML
community should embrace this development while building the appropriate
frameworks to mitigate the downsides. By acting now, we can harness AI to
restore user freedom and competitive markets without sacrificing security.

</details>


### [202] [The Jacobian and Hessian of the Kullback-Leibler Divergence between Multivariate Gaussian Distributions (Technical Report)](https://arxiv.org/abs/2506.23996)
*Juan Maroñas*

Main category: cs.LG

TL;DR: 本文展示如何用一阶和二阶微分求两个多元高斯分布的Kullback - Leibler散度的雅可比矩阵和海森矩阵，文档分为结果总结和详细推导。


<details>
  <summary>Details</summary>
Motivation: 介绍求两个多元高斯分布的Kullback - Leibler散度的雅可比矩阵和海森矩阵的方法。

Method: 使用一阶和二阶微分进行推导，基于文献理论和受到其他文献启发。

Result: 得出求两个多元高斯分布的Kullback - Leibler散度的雅可比矩阵和海森矩阵的方法。

Conclusion: 文档以教学为目的，分为结果总结和详细推导部分。

Abstract: This document shows how to obtain the Jacobian and Hessian matrices of the
Kullback-Leibler divergence between two multivariate Gaussian distributions,
using the first and second-order differentials. The presented derivations are
based on the theory presented by \cite{magnus99}. I've also got great
inspiration from some of the derivations in \cite{minka}.
  Since I pretend to be at most didactic, the document is split into a summary
of results and detailed derivations on each of the elements involved, with
specific references to the tricks used in the derivations, and to many of the
underlying concepts.

</details>


### [203] [The Illusion of Progress? A Critical Look at Test-Time Adaptation for Vision-Language Models](https://arxiv.org/abs/2506.24000)
*Lijun Sheng,Jian Liang,Ran He,Zilei Wang,Tieniu Tan*

Main category: cs.LG

TL;DR: 本文提出TTA - VLM基准评估视觉语言模型TTA方法，经实验发现现有TTA方法增益有限、与训练时微调方法协作差、准确性提升常牺牲模型可信度，发布该基准以推动可靠通用TTA策略发展。


<details>
  <summary>Details</summary>
Motivation: 当前TTA研究存在基线结果重复、评估指标有限、实验设置不一致和分析不足等问题，阻碍方法公平比较，需全面基准评估。

Method: 在统一可复现框架实现8种情景TTA和7种在线TTA方法，在15个常用数据集评估，扩展评估至SigLIP，纳入训练时调优方法，采用多种评估指标。

Result: 现有TTA方法较先驱工作增益有限；当前TTA方法与训练时微调方法协作差；准确性提升常牺牲模型可信度。

Conclusion: 发布TTA - VLM基准用于公平比较和全面评估VLM的TTA方法，期望推动社区开发更可靠通用TTA策略。

Abstract: Test-time adaptation (TTA) methods have gained significant attention for
enhancing the performance of vision-language models (VLMs) such as CLIP during
inference, without requiring additional labeled data. However, current TTA
researches generally suffer from major limitations such as duplication of
baseline results, limited evaluation metrics, inconsistent experimental
settings, and insufficient analysis. These problems hinder fair comparisons
between TTA methods and obscure their practical strengths and weaknesses. To
address these challenges, we introduce TTA-VLM, a comprehensive benchmark for
evaluating TTA methods on VLMs. Our benchmark implements 8 episodic TTA and 7
online TTA methods within a unified and reproducible framework, and evaluates
them across 15 widely used datasets. Unlike prior studies focused solely on
CLIP, we extend the evaluation to SigLIP--a model trained with a Sigmoid
loss--and include training-time tuning methods such as CoOp, MaPLe, and TeCoA
to assess generality. Beyond classification accuracy, TTA-VLM incorporates
various evaluation metrics, including robustness, calibration,
out-of-distribution detection, and stability, enabling a more holistic
assessment of TTA methods. Through extensive experiments, we find that 1)
existing TTA methods produce limited gains compared to the previous pioneering
work; 2) current TTA methods exhibit poor collaboration with training-time
fine-tuning methods; 3) accuracy gains frequently come at the cost of reduced
model trustworthiness. We release TTA-VLM to provide fair comparison and
comprehensive evaluation of TTA methods for VLMs, and we hope it encourages the
community to develop more reliable and generalizable TTA strategies.

</details>


### [204] [Provably Efficient and Agile Randomized Q-Learning](https://arxiv.org/abs/2506.24005)
*He Wang,Xingyu Xu,Yuejie Chi*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: While Bayesian-based exploration often demonstrates superior empirical
performance compared to bonus-based methods in model-based reinforcement
learning (RL), its theoretical understanding remains limited for model-free
settings. Existing provable algorithms either suffer from computational
intractability or rely on stage-wise policy updates which reduce responsiveness
and slow down the learning process. In this paper, we propose a novel variant
of Q-learning algorithm, refereed to as RandomizedQ, which integrates
sampling-based exploration with agile, step-wise, policy updates, for episodic
tabular RL. We establish an $\widetilde{O}(\sqrt{H^5SAT})$ regret bound, where
$S$ is the number of states, $A$ is the number of actions, $H$ is the episode
length, and $T$ is the total number of episodes. In addition, we present a
logarithmic regret bound under a mild positive sub-optimality condition on the
optimal Q-function. Empirically, RandomizedQ exhibits outstanding performance
compared to existing Q-learning variants with both bonus-based and
Bayesian-based exploration on standard benchmarks.

</details>


### [205] [Bridging Theory and Practice in Link Representation with Graph Neural Networks](https://arxiv.org/abs/2506.24018)
*Veronica Lachi,Francesco Ferrini,Antonio Longa,Bruno Lepri,Andrea Passerini,Manfred Jaeger*

Main category: cs.LG

TL;DR: 本文聚焦图神经网络在链接表示上的表达能力，引入统一框架进行理论分析，提出评估协议，并探讨表达能力在实践中的作用。


<details>
  <summary>Details</summary>
Motivation: 现有对图神经网络表达能力的理论研究主要集中在图级别表示，缺乏对链接表示的全面研究。

Method: 引入$k_\phi$-$k_\rho$-$m$统一框架对现有消息传递链接模型进行归纳并比较表达能力；提出合成评估协议；使用图对称度量来量化区分链接的难度。

Result: 得到了现有先进方法的层次结构，提供了分析未来架构的理论工具；发现表达能力强的模型在标准基准上可能表现不佳，但在图对称性增加时显著优于简单模型。

Conclusion: 需要根据数据集特点进行模型选择。

Abstract: Graph Neural Networks (GNNs) are widely used to compute representations of
node pairs for downstream tasks such as link prediction. Yet, theoretical
understanding of their expressive power has focused almost entirely on
graph-level representations. In this work, we shift the focus to links and
provide the first comprehensive study of GNN expressiveness in link
representation. We introduce a unifying framework, the $k_\phi$-$k_\rho$-$m$
framework, that subsumes existing message-passing link models and enables
formal expressiveness comparisons. Using this framework, we derive a hierarchy
of state-of-the-art methods and offer theoretical tools to analyze future
architectures. To complement our analysis, we propose a synthetic evaluation
protocol comprising the first benchmark specifically designed to assess
link-level expressiveness. Finally, we ask: does expressiveness matter in
practice? We use a graph symmetry metric that quantifies the difficulty of
distinguishing links and show that while expressive models may underperform on
standard benchmarks, they significantly outperform simpler ones as symmetry
increases, highlighting the need for dataset-aware model selection.

</details>


### [206] [Development of Hybrid Artificial Intelligence Training on Real and Synthetic Data: Benchmark on Two Mixed Training Strategies](https://arxiv.org/abs/2506.24093)
*Paul Wachter,Lukas Niehaus,Julius Schöning*

Main category: cs.LG

TL;DR: 本文研究合成数据训练ANN时的领域差距问题，综合分析两种混合策略在不同架构和数据集上的效果，为优化合成数据使用提供见解。


<details>
  <summary>Details</summary>
Motivation: 合成数据与真实数据存在领域差距，现有混合训练策略对其泛化性和鲁棒性的系统评估不足。

Method: 对三种常见架构和三种混合数据集上的两种广泛使用的混合策略进行全面分析，从数据集中抽取不同合成与真实数据比例的子集。

Result: 未明确提及具体结果。

Conclusion: 研究为优化任何ANN训练过程中合成数据的使用提供了有价值的见解，有助于提高鲁棒性和有效性。

Abstract: Synthetic data has emerged as a cost-effective alternative to real data for
training artificial neural networks (ANN). However, the disparity between
synthetic and real data results in a domain gap. That gap leads to poor
performance and generalization of the trained ANN when applied to real-world
scenarios. Several strategies have been developed to bridge this gap, which
combine synthetic and real data, known as mixed training using hybrid datasets.
While these strategies have been shown to mitigate the domain gap, a systematic
evaluation of their generalizability and robustness across various tasks and
architectures remains underexplored. To address this challenge, our study
comprehensively analyzes two widely used mixing strategies on three prevalent
architectures and three distinct hybrid datasets. From these datasets, we
sample subsets with varying proportions of synthetic to real data to
investigate the impact of synthetic and real components. The findings of this
paper provide valuable insights into optimizing the use of synthetic data in
the training process of any ANN, contributing to enhancing robustness and
efficacy.

</details>


### [207] [Teaching Time Series to See and Speak: Forecasting with Aligned Visual and Textual Perspectives](https://arxiv.org/abs/2506.24124)
*Dong Sixun,Fan Wei,Teresa Wu,Fu Yanjie*

Main category: cs.LG

TL;DR: 提出多模态对比学习框架，将时间序列转换为视觉和文本视角，进行多模态对齐，还引入变量选择模块，实验表明该方法优于基线。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测依赖单模态数值输入，难以捕捉高级语义模式，现有大语言模型方法受限于离散性且缺乏人类感知直觉。

Method: 提出多模态对比学习框架，将原始时间序列转换为结构化视觉和文本视角，通过对比学习在共享语义空间对齐，引入变量选择模块。

Result: 在15个短期和6个长期预测基准上，该方法始终优于单模态和跨模态基线。

Conclusion: 多模态对齐能有效提升时间序列预测效果。

Abstract: Time series forecasting traditionally relies on unimodal numerical inputs,
which often struggle to capture high-level semantic patterns due to their dense
and unstructured nature. While recent approaches have explored representing
time series as text using large language models (LLMs), these methods remain
limited by the discrete nature of token sequences and lack the perceptual
intuition humans typically apply, such as interpreting visual patterns. In this
paper, we propose a multimodal contrastive learning framework that transforms
raw time series into structured visual and textual perspectives. Rather than
using natural language or real-world images, we construct both modalities
directly from numerical sequences. We then align these views in a shared
semantic space via contrastive learning, enabling the model to capture richer
and more complementary representations. Furthermore, we introduce a variate
selection module that leverages the aligned representations to identify the
most informative variables for multivariate forecasting. Extensive experiments
on fifteen short-term and six long-term forecasting benchmarks demonstrate that
our approach consistently outperforms strong unimodal and cross-modal
baselines, highlighting the effectiveness of multimodal alignment in enhancing
time series forecasting. Code is available at:
https://github.com/Ironieser/TimesCLIP.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [208] [Towards Efficient and Accurate Spiking Neural Networks via Adaptive Bit Allocation](https://arxiv.org/abs/2506.23717)
*Xingting Yao,Qinghao Hu,Fei Zhou,Tielong Liu,Gang Li,Peisong Wang,Jian Cheng*

Main category: cs.NE

TL;DR: 提出自适应位分配策略以优化SNN的效率和准确性，实验证明能降低成本并提高准确率。


<details>
  <summary>Details</summary>
Motivation: 多比特SNN随着比特数增加，内存和计算需求剧增，性能提升不成比例，不同层重要性不同，额外比特可能造成浪费和干扰。

Method: 对权重和脉冲的时间长度和位宽进行参数化，使其可通过梯度学习和控制；提出改进的脉冲神经元；提出步长更新机制解决可学习位宽的步长不匹配问题。

Result: 在多个数据集上实验表明，能降低整体内存和计算成本，同时提高准确率，如SEWResNet - 34在ImageNet上有准确率提升和更低的比特预算。

Conclusion: 该自适应位分配策略有效，可提高SNN效率和准确性，且工作将完全开源。

Abstract: Multi-bit spiking neural networks (SNNs) have recently become a heated
research spot, pursuing energy-efficient and high-accurate AI. However, with
more bits involved, the associated memory and computation demands escalate to
the point where the performance improvements become disproportionate. Based on
the insight that different layers demonstrate different importance and extra
bits could be wasted and interfering, this paper presents an adaptive bit
allocation strategy for direct-trained SNNs, achieving fine-grained layer-wise
allocation of memory and computation resources. Thus, SNN's efficiency and
accuracy can be improved. Specifically, we parametrize the temporal lengths and
the bit widths of weights and spikes, and make them learnable and controllable
through gradients. To address the challenges caused by changeable bit widths
and temporal lengths, we propose the refined spiking neuron, which can handle
different temporal lengths, enable the derivation of gradients for temporal
lengths, and suit spike quantization better. In addition, we theoretically
formulate the step-size mismatch problem of learnable bit widths, which may
incur severe quantization errors to SNN, and accordingly propose the step-size
renewal mechanism to alleviate this issue. Experiments on various datasets,
including the static CIFAR and ImageNet and the dynamic CIFAR-DVS and
DVS-GESTURE, demonstrate that our methods can reduce the overall memory and
computation cost while achieving higher accuracy. Particularly, our
SEWResNet-34 can achieve a 2.69\% accuracy gain and 4.16$\times$ lower bit
budgets over the advanced baseline work on ImageNet. This work will be fully
open-sourced.

</details>


### [209] [Marker Gene Method : Identifying Stable Solutions in a Dynamic Environment](https://arxiv.org/abs/2506.23734)
*Hao Shi,Xi Li,Fangfang Xie*

Main category: cs.NE

TL;DR: 论文引入标记基因方法（MGM）提升竞争协同进化算法（CCEAs）稳定性与鲁棒性，有理论证明和实证支持。


<details>
  <summary>Details</summary>
Motivation: CCEAs常受非传递性和红皇后效应等复杂动力学影响，收敛不稳定，需解决这些挑战。

Method: 引入MGM框架，用'标记基因'作动态基准和自适应加权机制平衡探索与开发，并给出严格数学证明。

Result: MGM在多种挑战中有效，稳定了石头剪刀布游戏，提升C - RMOEA/D在ZDT基准上的表现，结合内存池扩展驯服了Shapley偏置游戏。

Conclusion: MGM是理论可靠、实证有效的框架，显著增强了CCEAs在复杂竞争环境中的稳定性和鲁棒性。

Abstract: Competitive Co-evolutionary Algorithms (CCEAs) are often hampered by complex
dynamics like intransitivity and the Red Queen effect, leading to unstable
convergence. To counter these challenges, this paper introduces the Marker Gene
Method (MGM), a framework that establishes stability by using a 'marker gene'
as a dynamic benchmark and an adaptive weighting mechanism to balance
exploration and exploitation. We provide rigorous mathematical proofs
demonstrating that MGM creates strong attractors near Nash Equilibria within
the Strictly Competitive Game framework. Empirically, MGM demonstrates its
efficacy across a spectrum of challenges: it stabilizes the canonical
Rock-Paper-Scissors game, significantly improves the performance of C-RMOEA/D
on ZDT benchmarks, and, when augmented with a Memory Pool (MP) extension, it
successfully tames the notoriously pathological Shapley Biased Game. This work
presents a theoretically sound and empirically validated framework that
substantially enhances the stability and robustness of CCEAs in complex
competitive environments.

</details>


### [210] [More Efficient Real-Valued Gray-Box Optimization through Incremental Distribution Estimation in RV-GOMEA](https://arxiv.org/abs/2506.23738)
*Renzo J. Scholman,Tanja Alderliesten,Peter A. N. Bosman*

Main category: cs.NE

TL;DR: 研究增量分布估计能否提升RV - GOMEA效率，在不同基准问题上实验，发现能减少达到高质量解所需评估次数。


<details>
  <summary>Details</summary>
Motivation: 最新版RV - GOMEA未采用增量学习，研究增量分布估计能否提升其效率。

Method: 在不同程度重叠依赖的基准问题上进行实验。

Result: 与RV - GOMEA和VKD - CMA - ES相比，针对特定问题调整种群大小时，达到高质量解所需评估次数最多可减少1.5倍；使用通用种群大小准则时，可减少2 - 3倍。

Conclusion: 增量分布估计能提升RV - GOMEA的效率。

Abstract: The Gene-pool Optimal Mixing EA (GOMEA) family of EAs offers a specific means
to exploit problem-specific knowledge through linkage learning, i.e.,
inter-variable dependency detection, expressed using subsets of variables, that
should undergo joint variation. Such knowledge can be exploited if faster
fitness evaluations are possible when only a few variables are changed in a
solution, enabling large speed-ups. The recent-most version of Real-Valued
GOMEA (RV-GOMEA) can learn a conditional linkage model during optimization
using fitness-based linkage learning, enabling fine-grained dependency
exploitation in learning and sampling a Gaussian distribution. However, while
the most efficient Gaussian-based EAs, like NES and CMA-ES, employ incremental
learning of the Gaussian distribution rather than performing full re-estimation
every generation, the recent-most RV-GOMEA version does not employ such
incremental learning. In this paper, we therefore study whether incremental
distribution estimation can lead to efficiency enhancements of RV-GOMEA. We
consider various benchmark problems with varying degrees of overlapping
dependencies. We find that, compared to RV-GOMEA and VKD-CMA-ES, the required
number of evaluations to reach high-quality solutions can be reduced by a
factor of up to 1.5 if population sizes are tuned problem-specifically, while a
reduction by a factor of 2-3 can be achieved with generic population-sizing
guidelines.

</details>


### [211] [Unsupervised Sparse Coding-based Spiking Neural Network for Real-time Spike Sorting](https://arxiv.org/abs/2506.24041)
*Alexis Melot,Sean U. N. Wood,Yannick Coffinier,Pierre Yger,Fabien Alibart*

Main category: cs.NE

TL;DR: 本文介绍了用于高效尖峰排序的神经形态稀疏排序器NSS，在模拟和真实信号评估中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决脑机接口中边缘端实时、低功耗且保持高神经解码性能的尖峰排序挑战。

Method: 引入两层脉冲神经网络NSS，利用LCA进行稀疏编码，实现在线无监督排序，在神经形态平台上实现自定义神经元模型。

Result: NSS在模拟和真实信号评估中优于WaveClus3和PCA+KMeans等方法，在特定条件下F1分数提升，功耗和计算时间有具体数据。

Conclusion: NSS在尖峰排序任务中表现出色，能实现灵活的功率 - 性能权衡。

Abstract: Spike sorting is a crucial step in decoding multichannel extracellular neural
signals, enabling the identification of individual neuronal activity. A key
challenge in brain-machine interfaces (BMIs) is achieving real-time, low-power
spike sorting at the edge while keeping high neural decoding performance. This
study introduces the Neuromorphic Sparse Sorter (NSS), a compact two-layer
spiking neural network optimized for efficient spike sorting. NSS leverages the
Locally Competitive Algorithm (LCA) for sparse coding to extract relevant
features from noisy events with reduced computational demands. NSS learns to
sort detected spike waveforms in an online fashion and operates entirely
unsupervised. To exploit multi-bit spike coding capabilities of neuromorphic
platforms like Intel's Loihi 2, a custom neuron model was implemented, enabling
flexible power-performance trade-offs via adjustable spike bit-widths.
Evaluations on simulated and real-world tetrode signals with biological drift
showed NSS outperformed established pipelines such as WaveClus3 and PCA+KMeans.
With 2-bit graded spikes, NSS on Loihi 2 outperformed NSS implemented with
leaky integrate-and-fire neuron and achieved an F1-score of 77% (+10%
improvement) while consuming 8.6mW (+1.65mW) when tested on a drifting
recording, with a computational processing time of 0.25ms (+60 us) per
inference.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [212] [Data-Driven Power Modeling and Monitoring via Hardware Performance Counter Tracking](https://arxiv.org/abs/2506.23672)
*Sergio Mazzola,Gabriele Ara,Thomas Benz,Björn Forsberg,Tommaso Cucinotta,Luca Benini*

Main category: cs.PF

TL;DR: 本文提出一种新型功耗建模方法，集成到Linux内核中，评估显示有较好准确性和低开销，可用于相关调度策略。


<details>
  <summary>Details</summary>
Motivation: 当前嵌入式计算时代需在功率预算内实现高性能，硬件异质性和并行性使在线功耗评估复杂，需有效功耗建模方法。

Method: 为每个DVFS状态确定与各硬件子系统功耗线性相关性最高的PMCs，将简单模型组合成完整系统功耗模型，并集成到Linux内核的Runmeter框架中。

Result: 功率消耗平均估计误差7.5%，能量平均估计误差1.3%，Runmeter最坏情况时间开销仅0.7%。

Conclusion: 该方法准确性高、开销低、响应性好，可用于工作负载感知的DVFS和功耗感知的闭环任务调度等执行策略。

Abstract: Energy-centric design is paramount in the current embedded computing era: use
cases require increasingly high performance at an affordable power budget,
often under real-time constraints. Hardware heterogeneity and parallelism help
address the efficiency challenge, but greatly complicate online power
consumption assessments, which are essential for dynamic hardware and software
stack adaptations. We introduce a novel power modeling methodology with
state-of-the-art accuracy, low overhead, and high responsiveness, whose
implementation does not rely on microarchitectural details. Our methodology
identifies the Performance Monitoring Counters (PMCs) with the highest linear
correlation to the power consumption of each hardware sub-system, for each
Dynamic Voltage and Frequency Scaling (DVFS) state. The individual, simple
models are composed into a complete model that effectively describes the power
consumption of the whole system, achieving high accuracy and low overhead. Our
evaluation reports an average estimation error of 7.5% for power consumption
and 1.3% for energy. We integrate these models in the Linux kernel with
Runmeter, an open-source, PMC-based monitoring framework. Runmeter manages PMC
sampling and processing, enabling the execution of our power models at runtime.
With a worst-case time overhead of only 0.7%, Runmeter provides responsive and
accurate power measurements directly in the kernel. This information can be
employed for actuation policies in workload-aware DVFS and power-aware,
closed-loop task scheduling.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [213] [Knowledge-Guided Multi-Agent Framework for Automated Requirements Development: A Vision](https://arxiv.org/abs/2506.22656)
*Jiangping Huang,Dongming Jin,Weisong Sun,Yang Liu,Zhi Jin*

Main category: cs.SE

TL;DR: 提出知识引导的多智能体框架KGMAF用于自动化需求开发，介绍其组成，通过案例研究展示潜力并指出研究机会。


<details>
  <summary>Details</summary>
Motivation: 当前软件工程自动化系统重代码开发，忽视需求任务复杂性，需改进。

Method: 设计由六个专业智能体和工件池组成的KGMAF，明确各智能体功能、行动和知识及工件池概念设计。

Result: 案例研究表明KGMAF在现实场景中有潜力。

Conclusion: KGMAF将在大语言模型时代自动化需求开发中起关键作用，还指出相关研究机会。

Abstract: This paper envisions a knowledge-guided multi-agent framework named KGMAF for
automated requirements development. KGMAF aims to address gaps in current
automation systems for SE, which prioritize code development and overlook the
complexities of requirements tasks. KGMAF is composed of six specialized agents
and an artifact pool to improve efficiency and accuracy. Specifically, KGMAF
outlines the functionality, actions, and knowledge of each agent and provides
the conceptual design of the artifact pool. Our case study highlights the
potential of KGMAF in real-world scenarios. Finally, we outline several
research opportunities for implementing and enhancing automated requirements
development using multi-agent systems. We believe that KGMAF will play a
pivotal role in shaping the future of automated requirements development in the
era of LLMs.

</details>


### [214] [An LLM-assisted approach to designing software architectures using ADD](https://arxiv.org/abs/2506.22688)
*Humberto Cervantes,Rick Kazman,Yuanfang Cai*

Main category: cs.SE

TL;DR: 本文提出基于LLM辅助的软件架构设计方法，经案例验证该方法有潜力但也有局限，强调人类监督重要性。


<details>
  <summary>Details</summary>
Motivation: 传统软件架构设计依赖专家判断，过程复杂迭代，需新方法辅助。

Method: 采用属性驱动设计（ADD）方法，向LLM提供ADD描述、架构师角色和迭代计划，与人类架构师协作生成架构工件。

Result: LLM辅助的ADD过程生成的架构与成熟方案接近，部分满足架构驱动因素。

Conclusion: 在软件架构设计中使用LLM有前景，但需要人类监督和迭代完善。

Abstract: Designing effective software architectures is a complex, iterative process
that traditionally relies on expert judgment. This paper proposes an approach
for Large Language Model (LLM)-assisted software architecture design using the
Attribute-Driven Design (ADD) method. By providing an LLM with an explicit
description of ADD, an architect persona, and a structured iteration plan, our
method guides the LLM to collaboratively produce architecture artifacts with a
human architect. We validate the approach through case studies, comparing
generated designs against proven solutions and evaluating them with
professional architects. Results show that our LLM-assisted ADD process can
generate architectures closely aligned with established solutions and partially
satisfying architectural drivers, highlighting both the promise and current
limitations of using LLMs in architecture design. Our findings emphasize the
importance of human oversight and iterative refinement when leveraging LLMs in
this domain.

</details>


### [215] [P4OMP: Retrieval-Augmented Prompting for OpenMP Parallelism in Serial Code](https://arxiv.org/abs/2506.22703)
*Wali Mohammad Abdullah,Azmain Kabir*

Main category: cs.SE

TL;DR: 提出P4OMP框架，利用RAG改进LLM将串行C/C++代码转为OpenMP并行代码，评估显示其在编译成功率、避免错误和运行时扩展性上表现良好。


<details>
  <summary>Details</summary>
Motivation: 在不进行模型微调或编译器插桩的情况下，提高OpenMP编译指示正确性，提升LLM生成OpenMP代码的可靠性和适用性。

Method: 采用Retrieval - Augmented Generation (RAG)，结合OpenMP教程的结构化知识进行提示驱动的代码生成。

Result: 在108个真实C++程序基准测试中，P4OMP可并行化案例编译成功率达100%，基线有20个案例编译失败；避免了常见错误；在HPC集群的7个计算密集型基准测试中显示出良好的运行时扩展性。

Conclusion: P4OMP是一个强大、模块化的管道，显著提高了LLM生成OpenMP代码的可靠性和适用性。

Abstract: We present P4OMP, a retrieval-augmented framework for transforming serial
C/C++ code into OpenMP-annotated parallel code using large language models
(LLMs). To our knowledge, this is the first system to apply retrieval-based
prompting for OpenMP pragma correctness without model fine-tuning or compiler
instrumentation. P4OMP leverages Retrieval-Augmented Generation (RAG) with
structured instructional knowledge from OpenMP tutorials to improve the
reliability of prompt-driven code generation. By grounding generation in the
retrieved context, P4OMP improves syntactic correctness compared to baseline
prompting with GPT-3.5-Turbo. We evaluate P4OMP against a baseline,
GPT-3.5-Turbo without retrieval, on a comprehensive benchmark of 108 real-world
C++ programs drawn from Stack Overflow, PolyBench, and NAS benchmark suites.
P4OMP achieves 100% compilation success on all parallelizable cases, while the
baseline fails to compile in 20 out of 108 cases. Six cases that rely on
non-random-access iterators or thread-unsafe constructs are excluded due to
fundamental OpenMP limitations. A detailed analysis demonstrates how P4OMP
consistently avoids scoping errors, syntactic misuse, and invalid directive
combinations that commonly affect baseline-generated code. We further
demonstrate strong runtime scaling across seven compute-intensive benchmarks on
an HPC cluster. P4OMP offers a robust, modular pipeline that significantly
improves the reliability and applicability of LLM-generated OpenMP code.

</details>


### [216] [RAILS: Retrieval-Augmented Intelligence for Learning Software Development](https://arxiv.org/abs/2506.22742)
*Wali Mohammad Abdullah,Md. Morshedul Islam,Devraj Parmar,Happy Hasmukhbhai Patel,Sindhuja Prabhakaran,Baidya Saha*

Main category: cs.SE

TL;DR: 引入RAILS框架辅助Java软件开发，在处理导入错误上优于基线提示，未来将扩展功能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型辅助软件开发时存在生成代码不完整、导入错误等问题，尤其是缺乏外部或项目特定文档时。

Method: 使用FAISS和OpenAI嵌入从精选Java资源中进行语义检索以增强LLM提示，结合编译器反馈的迭代验证循环来优化建议。

Result: 在78个真实世界的Java导入错误案例中，RAILS即使使用相同LLM，也在保留意图、避免幻觉和找出正确导入方面优于基线提示。

Conclusion: RAILS框架有效，未来将集成符号过滤并扩展到其他语言和IDE。

Abstract: Large Language Models (LLMs) like GPT-3.5-Turbo are increasingly used to
assist software development, yet they often produce incomplete code or
incorrect imports, especially when lacking access to external or
project-specific documentation. We introduce RAILS (Retrieval-Augmented
Intelligence for Learning Software Development), a framework that augments LLM
prompts with semantically retrieved context from curated Java resources using
FAISS and OpenAI embeddings. RAILS incorporates an iterative validation loop
guided by compiler feedback to refine suggestions. We evaluated RAILS on 78
real-world Java import error cases spanning standard libraries, GUI APIs,
external tools, and custom utilities. Despite using the same LLM, RAILS
outperforms baseline prompting by preserving intent, avoiding hallucinations,
and surfacing correct imports even when libraries are unavailable locally.
Future work will integrate symbolic filtering via PostgreSQL and extend support
to other languages and IDEs.

</details>


### [217] [Privacy-Preserving Methods for Bug Severity Prediction](https://arxiv.org/abs/2506.22752)
*Havvanur Dervişoğlu,Ruşen Halepmollası,Elif Eyvaz*

Main category: cs.SE

TL;DR: 研究使用源代码指标和大语言模型进行方法级错误严重程度预测，比较不同训练方式，发现联邦学习和合成数据生成可在不共享数据下实现有效预测。


<details>
  <summary>Details</summary>
Motivation: AI分析需大量数据，但工业应用存在数据共享和标注数据有限的问题，需要有效方法进行错误严重程度预测。

Method: 使用两个广泛使用的数据集，比较集中学习、联邦学习和合成数据生成训练模型的性能。

Result: 使用两个公认的软件缺陷数据集实验表明，联邦学习和合成数据训练的模型在不共享数据下与集中训练模型效果相当。

Conclusion: 联邦学习和合成数据生成等隐私保护方法在数据共享困难的工业场景有实现有效错误严重程度预测的潜力。

Abstract: Bug severity prediction is a critical task in software engineering as it
enables more efficient resource allocation and prioritization in software
maintenance. While AI-based analyses and models significantly require access to
extensive datasets, industrial applications face challenges due to data-sharing
constraints and the limited availability of labeled data. In this study, we
investigate method-level bug severity prediction using source code metrics and
Large Language Models (LLMs) with two widely used datasets. We compare the
performance of models trained using centralized learning, federated learning,
and synthetic data generation. Our experimental results, obtained using two
widely recognized software defect datasets, indicate that models trained with
federated learning and synthetic data achieve comparable results to centrally
trained models without data sharing. Our finding highlights the potential of
privacy-preserving approaches such as federated learning and synthetic data
generation to enable effective bug severity prediction in industrial context
where data sharing is a major challenge.
  The source code and dataset are available at our GitHub repository:
https://github.com/drvshavva/EASE2025-Privacy-Preserving-Methods-for-Bug-Severity-Prediction.

</details>


### [218] [Smaller = Weaker? Benchmarking Robustness of Quantized LLMs in Code Generation](https://arxiv.org/abs/2506.22776)
*Sen Fang,Weiyuan Ding,Antonio Mastropaolo,Bowen Xu*

Main category: cs.SE

TL;DR: 文章首次系统研究量化对大语言模型代码生成任务鲁棒性的影响，发现量化模型在对抗攻击和噪声扰动实验中表现出更好的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注量化大语言模型的有效性，而对其鲁棒性的影响研究较少，本文旨在填补这一空白。

Method: 对四个主流大语言模型家族进行广泛实验，从输入提示的对抗攻击和模型架构的噪声扰动两个角度评估鲁棒性。

Result: 量化大语言模型在对抗攻击实验中51.59%表现出更好的恢复能力，噪声扰动实验也表明量化后模型能承受更高水平的权重干扰。

Conclusion: 量化不仅能降低计算需求，还能提高大语言模型在代码生成任务中的可靠性，为开发更鲁棒高效的部署策略提供了有价值的见解。

Abstract: Quantization has emerged as a mainstream method for compressing Large
Language Models (LLMs), reducing memory requirements and accelerating inference
without architectural modifications. While existing research primarily focuses
on evaluating the effectiveness of quantized LLMs compared to their original
counterparts, the impact on robustness remains largely unexplored.In this
paper, we present the first systematic investigation of how quantization
affects the robustness of LLMs in code generation tasks. Through extensive
experiments across four prominent LLM families (LLaMA, DeepSeek, CodeGen, and
StarCoder) with parameter scales ranging from 350M to 33B, we evaluate
robustness from dual perspectives: adversarial attacks on input prompts and
noise perturbations on model architecture. Our findings challenge conventional
wisdom by demonstrating that quantized LLMs often exhibit superior robustness
compared to their full-precision counterparts, with 51.59% versus 42.86% of our
adversarial experiments showing better resilience in quantized LLMs. Similarly,
our noise perturbation experiments also confirm that LLMs after quantitation
generally withstand higher levels of weight disturbances. These results suggest
that quantization not only reduces computational requirements but can actually
enhance LLMs' reliability in code generation tasks, providing valuable insights
for developing more robust and efficient LLM deployment strategies.

</details>


### [219] [Generating Privacy Stories From Software Documentation](https://arxiv.org/abs/2506.23014)
*Wilder Baldwin,Shashank Chintakuntla,Shreyah Parajuli,Ali Pourghasemi,Ryan Shanz,Sepideh Ghanavati*

Main category: cs.SE

TL;DR: 本文提出基于CoT、ICL和LLMs的方法从软件文档提取隐私行为并生成隐私需求，常用LLMs表现良好，调参可提升性能。


<details>
  <summary>Details</summary>
Motivation: 分析师和开发者对隐私重视不足可能导致违规，当前方法多聚焦法律要求提取和合规评估，需新方法从软件文档提取隐私行为并生成需求。

Method: 基于链思维提示（CoT）、上下文学习（ICL）和大语言模型（LLMs）从软件文档提取隐私行为，生成用户故事格式的隐私需求。

Result: 常用LLMs如GPT - 4o和Llama 3识别隐私行为和生成隐私用户故事的F1分数超0.8，调参可提升模型性能。

Conclusion: 研究为利用和优化LLMs根据软件开发周期文档生成隐私需求提供了见解。

Abstract: Research shows that analysts and developers consider privacy as a security
concept or as an afterthought, which may lead to non-compliance and violation
of users' privacy. Most current approaches, however, focus on extracting legal
requirements from the regulations and evaluating the compliance of software and
processes with them. In this paper, we develop a novel approach based on
chain-of-thought prompting (CoT), in-context-learning (ICL), and Large Language
Models (LLMs) to extract privacy behaviors from various software documents
prior to and during software development, and then generate privacy
requirements in the format of user stories. Our results show that most commonly
used LLMs, such as GPT-4o and Llama 3, can identify privacy behaviors and
generate privacy user stories with F1 scores exceeding 0.8. We also show that
the performance of these models could be improved through parameter-tuning. Our
findings provide insight into using and optimizing LLMs for generating privacy
requirements given software documents created prior to or throughout the
software development lifecycle.

</details>


### [220] [Guiding AI to Fix Its Own Flaws: An Empirical Study on LLM-Driven Secure Code Generation](https://arxiv.org/abs/2506.23034)
*Hao Yan,Swapneel Suhas Vaidya,Xiaokuan Zhang,Ziyu Yao*

Main category: cs.SE

TL;DR: 本文全面评估大语言模型在代码生成安全方面的表现，发现虽易生成不安全代码，但高级模型可借助提示和反馈避免或修复漏洞，并为开发者提供建议。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在代码生成时常忽视安全实践，且缺乏引导其生成安全代码的策略及对修复含漏洞代码效果的深入分析。

Method: 对不同规模的专有和开源模型进行评估，利用既定基准测试多种漏洞类型，进行定量和定性分析。

Result: 大语言模型易生成不安全代码，但高级模型可从漏洞提示和细粒度反馈中受益，避免或修复漏洞。

Conclusion: 为使用大语言模型进行代码生成的开发者提供减少漏洞的可行建议。

Abstract: Large Language Models (LLMs) have become powerful tools for automated code
generation. However, these models often overlook critical security practices,
which can result in the generation of insecure code that contains
vulnerabilities-weaknesses or flaws in the code that attackers can exploit to
compromise a system. However, there has been limited exploration of strategies
to guide LLMs in generating secure code and a lack of in-depth analysis of the
effectiveness of LLMs in repairing code containing vulnerabilities. In this
paper, we present a comprehensive evaluation of state-of-the-art LLMs by
examining their inherent tendencies to produce insecure code, their capability
to generate secure code when guided by self-generated vulnerability hints, and
their effectiveness in repairing vulnerabilities when provided with different
levels of feedback. Our study covers both proprietary and open-weight models
across various scales and leverages established benchmarks to assess a wide
range of vulnerability types. Through quantitative and qualitative analyses, we
reveal that although LLMs are prone to generating insecure code, advanced
models can benefit from vulnerability hints and fine-grained feedback to avoid
or fix vulnerabilities. We also provide actionable suggestions to developers to
reduce vulnerabilities when using LLMs for code generation.

</details>


### [221] [HF-DGF: Hybrid Feedback Guided Directed Grey-box Fuzzing](https://arxiv.org/abs/2506.23063)
*Guangfa Lyu,Zhenzhong Cao,Xiaofei Ren,Fengyu Wang*

Main category: cs.SE

TL;DR: 提出新型定向灰盒模糊测试框架HF - DGF，评估显示其在崩溃复现速度等方面优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 当前定向灰盒模糊测试（DGF）工具因运行时反馈不足，效率受限，需改进。

Method: 提出HF - DGF框架，采用结合控制流距离、值流影响分数和切片覆盖的混合反馈机制引导种子调度，提出向后步进算法计算距离，引入值流影响及分数，采用选择性插桩策略。

Result: 在41个真实漏洞上评估，HF - DGF崩溃复现速度比多个现有工具快，代码覆盖率低，静态分析效率高。

Conclusion: HF - DGF具有更好的方向性和效率，能有效解决现有DGF工具的问题。

Abstract: Directed Grey-box Fuzzing (DGF) has emerged as a widely adopted technique for
crash reproduction and patch testing, leveraging its capability to precisely
navigate toward target locations and exploit vulnerabilities. However, current
DGF tools are constrained by insufficient runtime feedback, limiting their
efficiency in reaching targets and exploring state spaces. This study presents
HF-DGF, a novel directed grey-box fuzzing framework. Its seed scheduling is
guided by a hybrid feedback mechanism integrating control-flow distance,
value-flow influence score, and slice coverage. To enable precise control-flow
distance feedback, we propose a backward-stepping algorithm to calculate basic
block-level seed distances on a virtual inter-procedural control-flow graph
(ICFG). For effective state space exploration, we introduce value-flow
influence and a corresponding metric, the value-flow influence score.
Additionally, to mitigate runtime overhead from hybrid feedback, we adopt a
novel selective instrumentation strategy. Evaluations on 41 real-world
vulnerabilities show HF-DGF outperforms existing tools: it achieves crash
reproduction 5.05 times faster than AFL, 5.79 times faster than AFLGo, 73.75
times faster than WindRanger, 2.56 times faster than DAFL, and 8.45 times
faster than Beacon on average. Notably, when all fuzzers triggered crashes,
HF-DGF exhibited the lowest code coverage, demonstrating superior
directionality and efficiency. It also surpasses AFLGo, WindRanger, DAFL, and
Beacon in static analysis efficiency.

</details>


### [222] [Repair Ingredients Are All You Need: Improving Large Language Model-Based Program Repair via Repair Ingredients Search](https://arxiv.org/abs/2506.23100)
*Jiayi Zhang,Kai Huang,Jian Zhang,Yang Liu,Chunyang Chen*

Main category: cs.SE

TL;DR: 提出ReinFix框架让LLM在程序修复中自主搜索修复要素，在多基准测试中效果超SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的自动程序修复技术在生成相关准确补丁时表现欠佳，常忽略关键修复要素。

Method: 提出ReinFix框架，推理阶段结合静态分析工具检索内部要素辅助LLM分析根因；解决阶段从历史修复中搜索外部要素指导LLM确定修复动作。

Result: 在Defects4J V1.2和V2.0基准测试中，ReinFix比SOTA多修复多个bug，在无数据泄露风险的基准测试中也表现最佳。

Conclusion: ReinFix框架有效，能提高生成正确补丁的可能性。

Abstract: Automated Program Repair (APR) techniques aim to automatically fix buggy
programs. Among these, Large Language Model-based (LLM-based) approaches have
shown great promise. Recent advances demonstrate that directly leveraging LLMs
can achieve leading results. However, these techniques remain suboptimal in
generating contextually relevant and accurate patches, as they often overlook
repair ingredients crucial for practical program repair. In this paper, we
propose ReinFix, a novel framework that enables LLMs to autonomously search for
repair ingredients throughout both the reasoning and solution phases of bug
fixing. In the reasoning phase, ReinFix integrates static analysis tools to
retrieve internal ingredients, such as variable definitions, to assist the LLM
in root cause analysis when it encounters difficulty understanding the context.
During the solution phase, when the LLM lacks experience in fixing specific
bugs, ReinFix searches for external ingredients from historical bug fixes with
similar bug patterns, leveraging both the buggy code and its root cause to
guide the LLM in identifying appropriate repair actions, thereby increasing the
likelihood of generating correct patches. Evaluations on two popular benchmarks
(Defects4J V1.2 and V2.0) demonstrate the effectiveness of our approach over
SOTA baselines. Notably, ReinFix fixes 146 bugs, which is 32 more than the
baselines on Defects4J V1.2. On Defects4J V2.0, ReinFix fixes 38 more bugs than
the SOTA. Importantly, when evaluating on the recent benchmarks that are free
of data leakage risk, ReinFix also maintains the best performance.

</details>


### [223] [From Release to Adoption: Challenges in Reusing Pre-trained AI Models for Downstream Developers](https://arxiv.org/abs/2506.23234)
*Peerachai Banyongrakkul,Mansooreh Zahedi,Patanamon Thongtanunam,Christoph Treude,Haoyu Gao*

Main category: cs.SE

TL;DR: 本文通过分析开源项目问题报告，创建PTM相关挑战分类法，发现下游开发者复用PTM面临七类挑战，且PTM相关问题解决时间更长。


<details>
  <summary>Details</summary>
Motivation: 下游开发者在软件系统中复用预训练模型（PTMs）面临的挑战较少被探索，旨在填补这一知识空白。

Method: 定性创建并分析来自31个开源GitHub项目的840份PTM相关问题报告，系统开发PTM相关挑战分类法，对比现有分类法并进行解决时间分析。

Result: 确定下游开发者复用PTMs面临的七类关键挑战，PTM相关问题解决时间显著长于非PTM问题，且不同挑战类别有显著差异。

Conclusion: 讨论研究结果对从业者的影响及未来研究可能性。

Abstract: Pre-trained models (PTMs) have gained widespread popularity and achieved
remarkable success across various fields, driven by their groundbreaking
performance and easy accessibility through hosting providers. However, the
challenges faced by downstream developers in reusing PTMs in software systems
are less explored. To bridge this knowledge gap, we qualitatively created and
analyzed a dataset of 840 PTM-related issue reports from 31 OSS GitHub
projects. We systematically developed a comprehensive taxonomy of PTM-related
challenges that developers face in downstream projects. Our study identifies
seven key categories of challenges that downstream developers face in reusing
PTMs, such as model usage, model performance, and output quality. We also
compared our findings with existing taxonomies. Additionally, we conducted a
resolution time analysis and, based on statistical tests, found that
PTM-related issues take significantly longer to be resolved than issues
unrelated to PTMs, with significant variation across challenge categories. We
discuss the implications of our findings for practitioners and possibilities
for future research.

</details>


### [224] [On the Feasibility of Deduplicating Compiler Bugs with Bisection](https://arxiv.org/abs/2506.23281)
*Xintong Zhou,Zhenyang Xu,Chengnian Sun*

Main category: cs.SE

TL;DR: 本文研究用二分法进行编译器漏洞去重，提出BugLens方法，经评估优于现有基于分析的方法，二分法简单通用适合实际应用。


<details>
  <summary>Details</summary>
Motivation: 现有编译器漏洞去重方法依赖程序分析，计算开销大、泛化性有限，需更好方法。

Method: 研究用二分法定位导致失败的提交作为去重标准，提出BugLens方法，结合识别触发漏洞的优化减少误判。

Result: 在四个真实数据集上评估，BugLens比Tamer和D3平均节省26.98%和9.64%人力识别相同数量的不同漏洞。

Conclusion: 二分法简单通用，是编译器漏洞去重的实用解决方案。

Abstract: Random testing has proven to be an effective technique for compiler
validation. However, the debugging of bugs identified through random testing
presents a significant challenge due to the frequent occurrence of duplicate
test programs that expose identical compiler bugs. The process to identify
duplicates is a practical research problem known as bug deduplication. Prior
methodologies for compiler bug deduplication primarily rely on program analysis
to extract bug-related features for duplicate identification, which can result
in substantial computational overhead and limited generalizability. This paper
investigates the feasibility of employing bisection, a standard debugging
procedure largely overlooked in prior research on compiler bug deduplication,
for this purpose. Our study demonstrates that the utilization of bisection to
locate failure-inducing commits provides a valuable criterion for
deduplication, albeit one that requires supplementary techniques for more
accurate identification. Building on these results, we introduce BugLens, a
novel deduplication method that primarily uses bisection, enhanced by the
identification of bug-triggering optimizations to minimize false negatives.
Empirical evaluations conducted on four real-world datasets demonstrate that
BugLens significantly outperforms the state-of-the-art analysis-based
methodologies Tamer and D3 by saving an average of 26.98% and 9.64% human
effort to identify the same number of distinct bugs. Given the inherent
simplicity and generalizability of bisection, it presents a highly practical
solution for compiler bug deduplication in real-world applications.

</details>


### [225] [Improving vulnerability type prediction and line-level detection via adversarial training-based data augmentation and multi-task learning](https://arxiv.org/abs/2506.23534)
*Siyu Chen,Jiongyi Yang,Xiang Chen,Menglin Zheng,Minnan Wei,Xiaolin Ju*

Main category: cs.SE

TL;DR: 文章针对软件漏洞检测难题，提出结合EDAT和MTL的统一方法，实验显示该方法在VTP和LVD任务上优于基线，值得进一步研究。


<details>
  <summary>Details</summary>
Motivation: 软件漏洞威胁大，现有VTP和LVD任务面临样本少、类别不平衡问题，且多数研究忽略两任务内在关联。

Method: 提出结合Embedding - Layer Driven Adversarial Training (EDAT)与Multi - task Learning (MTL)的统一方法。

Result: 该方法在VTP和LVD任务上优于现有基线，在VTP中提升多种指标，在LVD中提高检测精度并减少误报。

Conclusion: 结合EDAT和MTL能为两任务提供统一解决方案，值得进一步研究。

Abstract: Context: Software vulnerabilities pose a significant threat to modern
software systems, as evidenced by the growing number of reported
vulnerabilities and cyberattacks. These escalating trends underscore the urgent
need for effective approaches that can automatically detect and understand
software vulnerabilities. Objective: However, the scarcity of labeled samples
and the class imbalance issue in vulnerability datasets present significant
challenges for both Vulnerability Type Prediction (VTP) and Line-level
Vulnerability Detection (LVD), especially for rare yet critical vulnerability
types. Moreover, most existing studies treat VTP and LVD as independent tasks,
overlooking their inherent correlation, which limits the potential to leverage
shared semantic patterns across tasks. Methods: To address these limitations,
we propose a unified approach that integrates Embedding-Layer Driven
Adversarial Training (EDAT) with Multi-task Learning (MTL). Specifically, EDAT
enhances model robustness by introducing adversarial perturbations to
identifier embeddings, guided by semantic importance. Meanwhile, MTL improves
overall performance by leveraging shared representations and inter-task
correlations between VTP and LVD. Results: Extensive experiments demonstrate
that our proposed approach outperforms state-of-the-art baselines on both VTP
and LVD tasks. For VTP, it yields notable improvements in accuracy, precision,
recall, and F1-score, particularly in identifying rare vulnerability types.
Similarly, for LVD, our approach enhances line-level detection accuracy while
significantly reducing false positives. Conclusion: Our study demonstrates that
combining EDAT with MTL provides a unified solution that improves performance
on both tasks and warrants further investigation.

</details>


### [226] [Comparative Analysis of the Code Generated by Popular Large Language Models (LLMs) for MISRA C++ Compliance](https://arxiv.org/abs/2506.23535)
*Malik Muhammad Umer*

Main category: cs.SE

TL;DR: 本文对流行大语言模型生成的C++代码进行了与MISRA C++标准一致性的比较分析。


<details>
  <summary>Details</summary>
Motivation: 安全关键系统软件开发需遵循认证标准，大语言模型生成的代码在该领域应用时需分析是否符合MISRA C++标准。

Method: 对OpenAI ChatGPT、Google Gemini、DeepSeek、Meta AI和Microsoft Copilot等流行大语言模型生成的C++代码进行与MISRA C++标准一致性的比较分析。

Result: 未提及

Conclusion: 未提及

Abstract: Safety-critical systems are engineered systems whose failure or malfunction
could result in catastrophic consequences. The software development for
safety-critical systems necessitates rigorous engineering practices and
adherence to certification standards like DO-178C for avionics. DO-178C is a
guidance document which requires compliance to well-defined software coding
standards like MISRA C++ to enforce coding guidelines that prevent the use of
ambiguous, unsafe, or undefined constructs. Large Language Models (LLMs) have
demonstrated significant capabilities in automatic code generation across a
wide range of programming languages, including C++. Despite their impressive
performance, code generated by LLMs in safety-critical domains must be
carefully analyzed for conformance to MISRA C++ coding standards. In this
paper, I have conducted a comparative analysis of the C++ code generated by
popular LLMs including: OpenAI ChatGPT, Google Gemini, DeepSeek, Meta AI, and
Microsoft Copilot for compliance with MISRA C++.

</details>


### [227] [QLPro: Automated Code Vulnerability Discovery via LLM and Static Code Analysis Integration](https://arxiv.org/abs/2506.23644)
*Junze Hu,Xiangyu Jin,Yizhe Zeng,Yuling Liu,Yunpeng Li,Dan Du,Kaiyu Xie,Hongsong Zhu*

Main category: cs.SE

TL;DR: 介绍QLPro漏洞检测框架，用新数据集测试，QLPro比CodeQL检测效果好且发现新漏洞。


<details>
  <summary>Details</summary>
Motivation: 构建能在整个开源项目中进行全面漏洞检测的框架。

Method: 构建新数据集JavaTest，使用QLPro和CodeQL进行漏洞检测并对比。

Result: CodeQL检测出24个已知漏洞，QLPro检测出41个已知漏洞，还发现6个未知漏洞，2个为0-day漏洞。

Conclusion: QLPro在漏洞检测方面表现优于CodeQL，能更有效地检测漏洞。

Abstract: We introduce QLPro, a vulnerability detection framework that systematically
integrates LLMs and static analysis tools to enable comprehensive vulnerability
detection across entire open-source projects.We constructed a new dataset,
JavaTest, comprising 10 open-source projects from GitHub with 62 confirmed
vulnerabilities. CodeQL, a state-of-the-art static analysis tool, detected only
24 of these vulnerabilities while QLPro detected 41. Furthermore, QLPro
discovered 6 previously unknown vulnerabilities, 2 of which have been confirmed
as 0-days.

</details>


### [228] [What Challenges Do Developers Face When Using Verification-Aware Programming Languages?](https://arxiv.org/abs/2506.23696)
*Francisco Oliveira,Alexandra Mendes,Carolina Carreira*

Main category: cs.SE

TL;DR: 研究通过分析论坛讨论和开发者调查，找出VA语言采用障碍，提出改进可用性和可访问性的建议。


<details>
  <summary>Details</summary>
Motivation: VA语言虽能提供强正确性保证，但采用率有限，需探究采用障碍。

Method: 用主题建模技术分析公共论坛开发者讨论，辅以开发者调查。

Result: 发现采用的关键障碍有学习曲线陡峭和可用性问题。

Conclusion: 简化工具界面、提供更好教育材料、改进与日常开发环境集成可提升语言可用性和采用率。

Abstract: Software reliability is critical in ensuring that the digital systems we
depend on function correctly. In software development, increasing software
reliability often involves testing. However, for complex and critical systems,
developers can use Design by Contract (DbC) methods to define precise
specifications that software components must satisfy. Verification-Aware (VA)
programming languages support DbC and formal verification at compile-time or
run-time, offering stronger correctness guarantees than traditional testing.
However, despite the strong guarantees provided by VA languages, their adoption
remains limited. In this study, we investigate the barriers to adopting VA
languages by analyzing developer discussions on public forums using topic
modeling techniques. We complement this analysis with a developer survey to
better understand the practical challenges associated with VA languages. Our
findings reveal key obstacles to adoption, including steep learning curves and
usability issues. Based on these insights, we identify actionable
recommendations to improve the usability and accessibility of VA languages. Our
findings suggest that simplifying tool interfaces, providing better educational
materials, and improving integration with everyday development environments
could improve the usability and adoption of these languages. Our work provides
actionable insights for improving the usability of VA languages and making
verification tools more accessible.

</details>


### [229] [Towards a Science of Developer eXperience (DevX)](https://arxiv.org/abs/2506.23715)
*Benoit Combemale*

Main category: cs.SE

TL;DR: 文章呼吁将开发者体验（DevX）正式确认为一个独立研究领域，指出其重要性并明确关键原理、推动因素等，还提及核心挑战以促进软件工程更以人为本。


<details>
  <summary>Details</summary>
Motivation: 随着软件在现代生活渗透，软件开发实践需更可持续、有效和包容，但开发者的体验未被充分探索，因此提出正式认可DevX这一研究领域。

Method: 基于现有的测量和提升DevX的工作，识别支持这一新兴学科的关键原理、科学推动因素和跨学科交叉点。

Result: 确定了支持DevX成为新兴学科的关键要素，明确了未来的核心科学挑战。

Conclusion: 呼吁研究界采取行动，推动软件工程更以人为本的方法。

Abstract: As software continues to permeate nearly every facet of modern life, the
complexity and ubiquity of digital services underscore the need for
sustainable, effective, and inclusive software development practices. Although
software engineering has made significant progress in technical challenges
since its inception, the human experience of those involved in software
creation, broadly defined as developers, remains underexplored. This column
advocates for the formal recognition of Developer eXperience (DevX) as a
distinct research field. We argue that DevX profoundly influences critical
development activities and overall productivity, especially as development
becomes increasingly collaborative and diverse in terms of application domains.
Building on existing efforts to measure and enhance DevX, we identify key
rationales, scientific enablers, and interdisciplinary intersections that
support this emerging discipline. We also outline the core scientific
challenges ahead, aiming to call for actions from the research community and to
promote more human-centered approaches to software engineering.

</details>


### [230] [A Survey of LLM-based Automated Program Repair: Taxonomies, Design Paradigms, and Applications](https://arxiv.org/abs/2506.23749)
*Boyang Yang,Zijian Cai,Fengling Liu,Bach Le,Lingming Zhang,Tegawendé F. Bissyandé,Yang Liu,Haoye Tian*

Main category: cs.SE

TL;DR: 对2022年1月至2025年6月发布的63个基于大语言模型的自动程序修复系统分类，分析各范式权衡，指出挑战并给出研究方向。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型如何重塑自动程序修复，梳理基于大语言模型的自动程序修复系统现状及问题。

Method: 将基于大语言模型的自动程序修复系统分为四种范式，分析不同范式在增强上下文时的特点和权衡。

Result: 明确各范式的优缺点，如微调训练成本高但任务对齐强，提示部署快但受设计和窗口限制等。

Conclusion: 指出持续存在的挑战，给出结合轻量级人工反馈等方向以推进可靠高效的基于大语言模型的自动程序修复。

Abstract: Large language models (LLMs) are reshaping automated program repair (APR). We
categorize the recent 63 LLM-based APR systems published from January 2022 to
June 2025 into four paradigms, and show how retrieval- or analysis-augmented
contexts strengthen any of them. This taxonomy clarifies key trade-offs:
fine-tuning delivers strong task alignment at high training cost; prompting
enables rapid deployment but is limited by prompt design and context windows;
procedural pipelines offer reproducible control with moderate overhead; agentic
frameworks tackle multi-hunk or cross-file bugs at the price of increased
latency and complexity. Persistent challenges include verifying semantic
correctness beyond test suites, repairing repository-scale defects, and
lowering the costs of LLMs. We outline research directions that combine
lightweight human feedback, repository-aware retrieval, code analysis, and
cost-aware planning to advance reliable and efficient LLM-based APR.

</details>


### [231] [Software Engineering for Large Language Models: Research Status, Challenges and the Road Ahead](https://arxiv.org/abs/2506.23762)
*Hongzhou Rao,Yanjie Zhao,Xinyi Hou,Shenao Wang,Haoyu Wang*

Main category: cs.SE

TL;DR: 文章指出大语言模型发展面临挑战，从软件工程角度系统分析其开发生命周期各阶段研究现状，识别挑战并给出潜在研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏从软件工程角度系统探索大语言模型开发全生命周期挑战及解决方案，为填补此空白开展研究。

Method: 将大语言模型开发生命周期分为需求工程、数据集构建等六个阶段，系统分析各阶段研究现状。

Result: 识别出每个阶段的关键挑战，并提出应对这些挑战的潜在研究方向。

Conclusion: 从软件工程角度为大语言模型开发的未来进展提供了有价值的见解。

Abstract: The rapid advancement of large language models (LLMs) has redefined
artificial intelligence (AI), pushing the boundaries of AI research and
enabling unbounded possibilities for both academia and the industry. However,
LLM development faces increasingly complex challenges throughout its lifecycle,
yet no existing research systematically explores these challenges and solutions
from the perspective of software engineering (SE) approaches. To fill the gap,
we systematically analyze research status throughout the LLM development
lifecycle, divided into six phases: requirements engineering, dataset
construction, model development and enhancement, testing and evaluation,
deployment and operations, and maintenance and evolution. We then conclude by
identifying the key challenges for each phase and presenting potential research
directions to address these challenges. In general, we provide valuable
insights from an SE perspective to facilitate future advances in LLM
development.

</details>


### [232] [Requirements for Active Assistance of Natural Questions in Software Architecture](https://arxiv.org/abs/2506.23898)
*Diogo Lemos,Ademar Aguiar,Neil B. Harrison*

Main category: cs.SE

TL;DR: 本文关注自然问题在架构设计中的管理，提出自然问题生命周期及辅助环境需求，并通过专家调查验证。


<details>
  <summary>Details</summary>
Motivation: 自然问题在架构设计中常被错误管理或忽视，会导致架构偏移、知识流失等问题，需更好理解其生命周期并构建辅助环境。

Method: 基于现有文献、需求研讨会和三次设计迭代，提出自然问题生命周期和环境需求，通过专家调查分析验证。

Result: 提出自然问题的生命周期，引出辅助环境的关键功能和非功能需求，专家调查验证了这些需求和功能。

Conclusion: 所提出的环境能比传统方法更有效地促进协作、决策和架构知识的保存。

Abstract: Natural questions are crucial to shaping key architectural decisions and
preserving architectural knowledge. They arise organically during the
architectural design process, often resulting from the existing architectural
experience of the designer and the distinctive characteristics of the system
being designed. However, natural questions are often mismanaged or ignored,
which can lead to architectural drift, knowledge loss, inefficient resource
use, or poor understandability of the system's architecture. We aim to better
understand the lifecycle of natural questions, its key requirements, challenges
and difficulties, and then to envision an assisted environment to properly
support it. The environment should be adaptable and responsive to real-world
constraints and uncertainties by seamlessly integrating knowledge management
tools and artificial intelligence techniques into software development
workflows. Based on existing literature, a requirements workshop, and three
design iterations, we proposed a lifecycle for natural questions and elicited
essential functional and non-functional requirements for such an environment.
At last, the results of a survey conducted with experts helped to analyze and
validate the elicited requirements and proposed features for the environment to
enhance collaboration, decision-making, and the preservation of architectural
knowledge more effectively than conventional methods.

</details>


### [233] [Green Metrics Tool: Measuring for fun and profit](https://arxiv.org/abs/2506.23967)
*Geerd-Dietger Hoffmann,Verena Majuntke*

Main category: cs.SE

TL;DR: 随着计算资源需求增加，为优化软件资源消耗和减少碳排放，本文讨论关键指标，介绍用于测量软件资源消耗的Green Metrics Tool (GMT)框架及特性。


<details>
  <summary>Details</summary>
Motivation: 计算资源需求上升，为优化软件资源消耗和减少碳排放，需测量和评估软件。

Method: 引入Green Metrics Tool (GMT)框架，采用基于生命周期的方法，评估软件关键阶段的资源使用。

Result: 介绍了GMT框架，讨论了其可视化、可比性和基于规则与大语言模型的优化等特性。

Conclusion: GMT有潜力指导开发者和研究人员降低软件的环境影响。

Abstract: The environmental impact of software is gaining increasing attention as the
demand for computational resources continues to rise. In order to optimize
software resource consumption and reduce carbon emissions, measuring and
evaluating software is a first essential step. In this paper we discuss what
metrics are important for fact base decision making. We introduce the Green
Metrics Tool (GMT), a novel framework for accurately measuring the resource
consumption of software. The tool provides a containerized, controlled, and
reproducible life cycle-based approach, assessing the resource use of software
during key phases. Finally, we discuss GMT features like visualization,
comparability and rule- and LLM-based optimisations highlighting its potential
to guide developers and researchers in reducing the environmental impact of
their software.

</details>


### [234] [STCLocker: Deadlock Avoidance Testing for Autonomous Driving Systems](https://arxiv.org/abs/2506.23995)
*Mingfei Cheng,Renzhi Wang,Xiaofei Xie,Yuan Zhou,Lei Ma*

Main category: cs.SE

TL;DR: 提出STCLocker技术用于生成自动驾驶系统死锁场景，在两种ADS上评估，结果显示它比基线生成更多死锁场景。


<details>
  <summary>Details</summary>
Motivation: 现有技术主要关注单自动驾驶车辆环境下的功能评估，多车环境中评估自动驾驶系统合作性能尤其是避免死锁能力的研究不足。

Method: 提出Spatio - Temporal Conflict - Guided Deadlock Avoidance Testing技术STCLocker，包含Deadlock Oracle、Conflict Feedback和Conflict - aware Scenario Generation三个关键组件。

Result: 在Roach和OpenCDA两种自动驾驶系统上实验，平均而言STCLocker比表现最好的基线生成更多死锁场景。

Conclusion: STCLocker技术能有效生成死锁场景，可用于评估自动驾驶系统避免死锁的合作能力。

Abstract: Autonomous Driving System (ADS) testing is essential to ensure the safety and
reliability of autonomous vehicles (AVs) before deployment. However, existing
techniques primarily focus on evaluating ADS functionalities in single-AV
settings. As ADSs are increasingly deployed in multi-AV traffic, it becomes
crucial to assess their cooperative performance, particularly regarding
deadlocks, a fundamental coordination failure in which multiple AVs enter a
circular waiting state indefinitely, resulting in motion planning failures.
Despite its importance, the cooperative capability of ADSs to prevent deadlocks
remains insufficiently underexplored. To address this gap, we propose the first
dedicated Spatio-Temporal Conflict-Guided Deadlock Avoidance Testing technique,
STCLocker, for generating DeadLock Scenarios (DLSs), where a group of AVs
controlled by the ADS under test are in a circular wait state. STCLocker
consists of three key components: Deadlock Oracle, Conflict Feedback, and
Conflict-aware Scenario Generation. Deadlock Oracle provides a reliable
black-box mechanism for detecting deadlock cycles among multiple AVs within a
given scenario. Conflict Feedback and Conflict-aware Scenario Generation
collaborate to actively guide AVs into simultaneous competition over spatial
conflict resources (i.e., shared passing regions) and temporal competitive
behaviors (i.e., reaching the conflict region at the same time), thereby
increasing the effectiveness of generating conflict-prone deadlocks. We
evaluate STCLocker on two types of ADSs: Roach, an end-to-end ADS, and OpenCDA,
a module-based ADS supporting cooperative communication. Experimental results
show that, on average, STCLocker generates more DLS than the best-performing
baseline.

</details>


### [235] [Bug Fixing with Broader Context: Enhancing LLM-Based Program Repair via Layered Knowledge Injection](https://arxiv.org/abs/2506.24015)
*Ramtin Ehsani,Esteban Parra,Sonia Haiduc,Preetha Chatterjee*

Main category: cs.SE

TL;DR: 本文提出分层知识注入框架提升基于大语言模型的程序修复效果，在BugsInPy数据集上评估，有显著改进并分析不同类型bug。


<details>
  <summary>Details</summary>
Motivation: 现有利用错误相关上下文的大语言模型程序修复方法仍有许多bug无法解决，开发者在现实中依靠更广泛的仓库和项目级上下文解决问题，因此研究自动提取和提供这些知识来改善程序修复。

Method: 提出分层知识注入框架，依次包括Bug知识层、仓库知识层和项目知识层，逐步为大语言模型增加结构化上下文。

Result: 在BugsInPy数据集的314个bug上评估，使用Llama 3.3达到79%的修复率，比之前工作提高23%；不同类型bug对不同层次上下文需求不同；一些复杂和结构孤立的bug仍难修复。

Conclusion: 分层上下文注入可改善程序修复，需要交互式和自适应的自动程序修复系统。

Abstract: Prompting LLMs with bug-related context (e.g., error messages, stack traces)
improves automated program repair, but many bugs still remain unresolved. In
real-world projects, developers often rely on broader repository and
project-level context beyond the local code to resolve such bugs. In this
paper, we investigate how automatically extracting and providing such knowledge
can improve LLM-based program repair. We propose a layered knowledge injection
framework that incrementally augments LLMs with structured context. It starts
with the Bug Knowledge Layer, which includes information such as the buggy
function and failing tests; expands to the Repository Knowledge Layer, which
adds structural dependencies, related files, and commit history; and finally
injects the Project Knowledge Layer, which incorporates relevant details from
documentation and previously fixed bugs. We evaluate this framework on a
dataset of 314 bugs from BugsInPy using two LLMs (Llama 3.3 and GPT-4o-mini),
and analyze fix rates across six bug types. By progressively injecting
knowledge across layers, our approach achieves a fix rate of 79% (250/314)
using Llama 3.3, a significant improvement of 23% over previous work. All bug
types show improvement with the addition of repository-level context, while
only a subset benefit further from project-level knowledge, highlighting that
different bug types require different levels of contextual information for
effective repair. We also analyze the remaining unresolved bugs and find that
more complex and structurally isolated bugs, such as Program Anomaly and GUI
bugs, remain difficult even after injecting all available information. Our
results show that layered context injection improves program repair and suggest
the need for interactive and adaptive APR systems.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [236] [SABR-Informed Multitask Gaussian Process: A Synthetic-to-Real Framework for Implied Volatility Surface Construction](https://arxiv.org/abs/2506.22888)
*Jirong Zhuang,Xuan Wu*

Main category: q-fin.CP

TL;DR: 提出SABR - MTGP方法构建隐含波动率曲面，结合SABR模型与高斯过程回归，实验表明该方法优于传统方法且适用于实际市场数据。


<details>
  <summary>Details</summary>
Motivation: 传统结构模型缺乏灵活性，纯数据驱动方法难以处理稀疏数据，需要一种更好的隐含波动率曲面构建方法。

Method: 将IVS构建视为多任务学习问题，使用校准SABR模型的密集合成数据集作为源任务，基于稀疏市场数据（目标任务）进行构建，MTGP框架捕获任务相关性并自适应转移结构信息。

Result: 使用Heston生成的地面真值数据实验表明SABR - MTGP在不同期限上优于标准高斯过程回归和SABR；应用于真实SPX市场数据证明其实际适用性和产生稳定现实曲面的能力。

Conclusion: SABR - MTGP方法平衡了SABR的结构指导和市场数据所需的灵活性。

Abstract: Constructing the Implied Volatility Surface (IVS) is a challenging task in
quantitative finance due to the complexity of real markets and the sparsity of
market data. Structural models like Stochastic Alpha Beta Rho (SABR) model
offer interpretability and theoretical consistency but lack flexibility, while
purely data-driven methods such as Gaussian Process regression can struggle
with sparse data. We introduce SABR-Informed Multi-Task Gaussian Process
(SABR-MTGP), treating IVS construction as a multi-task learning problem. Our
method uses a dense synthetic dataset from a calibrated SABR model as a source
task to inform the construction based on sparse market data (the target task).
The MTGP framework captures task correlation and transfers structural
information adaptively, improving predictions particularly in data-scarce
regions. Experiments using Heston-generated ground truth data under various
market conditions show that SABR-MTGP outperforms both standard Gaussian
process regression and SABR across different maturities. Furthermore, an
application to real SPX market data demonstrates the method's practical
applicability and its ability to produce stable and realistic surfaces. This
confirms our method balances structural guidance from SABR with the flexibility
needed for market data.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [237] [Deep Hedging to Manage Tail Risk](https://arxiv.org/abs/2506.22611)
*Yuming Ma*

Main category: q-fin.PM

TL;DR: 扩展2019年Deep Hedging范式，用深度神经网络解决投资组合尾部风险对冲问题，实验显示能降低CVaR并提供实用见解。


<details>
  <summary>Details</summary>
Motivation: 解决投资组合尾部风险对冲问题。

Method: 扩展2019年Deep Hedging范式，用深度神经网络参数化凸风险最小化，在危机时代自举市场模拟器上进行综合数值实验。

Result: 实现显著的一日99% CVaR降低，获得摩擦感知策略适应的实用见解。

Conclusion: 该端到端框架在现实市场中具有稳健性和操作可行性。

Abstract: Extending Buehler et al.'s 2019 Deep Hedging paradigm, we innovatively employ
deep neural networks to parameterize convex-risk minimization (CVaR/ES) for the
portfolio tail-risk hedging problem. Through comprehensive numerical
experiments on crisis-era bootstrap market simulators -- customizable with
transaction costs, risk budgets, liquidity constraints, and market impact --
our end-to-end framework not only achieves significant one-day 99% CVaR
reduction but also yields practical insights into friction-aware strategy
adaptation, demonstrating robustness and operational viability in realistic
markets.

</details>


### [238] [Potential Customer Lifetime Value in Financial Institutions: The Usage Of Open Banking Data to Improve CLV Estimation](https://arxiv.org/abs/2506.22711)
*João B. G. de Brito,Rodrigo Heldt,Cleo S. Silveira,Matthias Bogaert,Guilherme B. Bucco,Fernando B. Luce,João L. Becker,Filipe J. Zabala,Michel J. Anzanello*

Main category: q-fin.PM

TL;DR: 本文引入PCLV框架，利用开放银行数据估算客户价值，结果显示有潜在收益，为管理者提供战略工具。


<details>
  <summary>Details</summary>
Motivation: 现有CLV计算仅依赖单实体数据，缺乏多企业客户活动洞察，需全面估算客户价值。

Method: 引入PCLV框架，利用开放银行数据，预测留存概率和估算潜在贡献边际以计算PCLV。

Result: 开放银行数据可用于估算各竞争对手的PCLV，较实际CLV有21.06%的潜在提升。

Conclusion: PCLV为管理者利用开放银行数据增强竞争力、通过营销提升实际CLV和盈利能力提供战略工具。

Abstract: Financial institutions increasingly adopt customer-centric strategies to
enhance profitability and build long-term relationships. While Customer
Lifetime Value (CLV) is a core metric, its calculations often rely solely on
single-entity data, missing insights from customer activities across multiple
firms. This study introduces the Potential Customer Lifetime Value (PCLV)
framework, leveraging Open Banking (OB) data to estimate customer value
comprehensively. We predict retention probability and estimate Potential
Contribution Margins (PCM) from competitor data, enabling PCLV calculation.
Results show that OB data can be used to estimate PCLV per competitor,
indicating a potential upside of 21.06% over the Actual CLV. PCLV offers a
strategic tool for managers to strengthen competitiveness by leveraging OB data
and boost profitability by driving marketing efforts at the individual customer
level to increase the Actual CLV.

</details>


### [239] [Can We Reliably Predict the Fed's Next Move? A Multi-Modal Approach to U.S. Monetary Policy Forecasting](https://arxiv.org/abs/2506.22763)
*Fiona Xiao Jingyi,Lili Liu*

Main category: q-fin.PM

TL;DR: 研究探讨结合结构化数据与美联储通信非结构化文本信号提升央行政策决策预测准确性，发现混合模型表现佳，简单混合模型兼具准确性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 预测央行政策决策是挑战，传统仅依赖结构化宏观经济指标的方法不足，需新方法提升预测准确性。

Method: 采用多模态框架，比较传统机器学习模型、基于变压器的语言模型和深度学习架构在单模态和混合设置下的表现。

Result: 混合模型始终优于单模态基线，结合FOMC文本TF - IDF特征和经济指标的XGBoost分类器表现最佳，FinBERT情感特征在排名上有小提升但分类表现差，SHAP分析显示稀疏可解释特征更符合政策相关信号。

Conclusion: 强调透明整合文本和结构化信号的重要性，简单混合模型可为研究和决策提供准确且可解释的见解。

Abstract: Forecasting central bank policy decisions remains a persistent challenge for
investors, financial institutions, and policymakers due to the wide-reaching
impact of monetary actions. In particular, anticipating shifts in the U.S.
federal funds rate is vital for risk management and trading strategies.
Traditional methods relying only on structured macroeconomic indicators often
fall short in capturing the forward-looking cues embedded in central bank
communications.
  This study examines whether predictive accuracy can be enhanced by
integrating structured data with unstructured textual signals from Federal
Reserve communications. We adopt a multi-modal framework, comparing traditional
machine learning models, transformer-based language models, and deep learning
architectures in both unimodal and hybrid settings.
  Our results show that hybrid models consistently outperform unimodal
baselines. The best performance is achieved by combining TF-IDF features of
FOMC texts with economic indicators in an XGBoost classifier, reaching a test
AUC of 0.83. FinBERT-based sentiment features marginally improve ranking but
perform worse in classification, especially under class imbalance. SHAP
analysis reveals that sparse, interpretable features align more closely with
policy-relevant signals.
  These findings underscore the importance of integrating textual and
structured signals transparently. For monetary policy forecasting, simpler
hybrid models can offer both accuracy and interpretability, delivering
actionable insights for researchers and decision-makers.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [240] [Extreme-case Range Value-at-Risk under Increasing Failure Rate](https://arxiv.org/abs/2506.23073)
*Yuting Su,Taizhong Hu,Zhenfeng Zou*

Main category: q-fin.RM

TL;DR: 研究分布模糊下递增失效率（IFR）特性的极端风险度量，给出分布不确定下极端情况的风险值范围，刻画极端分布特征并应用于随机变量。


<details>
  <summary>Details</summary>
Motivation: 极端风险度量在量化金融和保险风险管理中有重要指导意义，以往研究有局限，此次聚焦IFR特性的极端风险度量。

Method: 研究分布模糊下IFR特性的极端风险度量，刻画极端分布特征。

Result: 给出分布不确定下极端情况的风险值范围，刻画极端分布特征。

Conclusion: 将主要结果应用于具有IFR特性的止损和有限损失随机变量。

Abstract: The extreme cases of risk measures, when considered within the context of
distributional ambiguity, provide significant guidance for practitioners
specializing in risk management of quantitative finance and insurance. In
contrast to the findings of preceding studies, we focus on the study of
extreme-case risk measure under distributional ambiguity with the property of
increasing failure rate (IFR). The extreme-case range Value-at-Risk under
distributional uncertainty, consisting of given mean and/or variance of
distributions with IFR, is provided. The specific characteristics of
extreme-case distributions under these constraints have been characterized, a
crucial step for numerical simulations. We then apply our main results to
stop-loss and limited loss random variables under distributional uncertainty
with IFR.

</details>


### [241] [Explainable AI for Comprehensive Risk Assessment for Financial Reports: A Lightweight Hierarchical Transformer Network Approach](https://arxiv.org/abs/2506.23767)
*Xue Wen Tan,Stanley Kok*

Main category: q-fin.RM

TL;DR: 提出轻量级可解释模型TinyXRA评估公司风险，在2013 - 2024数据集上取得SOTA，进行消融研究并给出结论。


<details>
  <summary>Details</summary>
Motivation: 现有基于超额收益标准差的公司风险评估方法有局限性，需更全面评估，且要考虑计算资源约束。

Method: 采用TinyBERT编码金融文档，结合基于动态注意力的词云机制；使用三元组损失进行风险四分位数分类。

Result: TinyXRA在2013 - 2024数据集7个测试年中达到SOTA预测精度，能提供透明可解释的风险评估。

Conclusion: 论文给出研究结果、实际意义、局限性和未来研究方向。

Abstract: Every publicly traded U.S. company files an annual 10-K report containing
critical insights into financial health and risk. We propose Tiny eXplainable
Risk Assessor (TinyXRA), a lightweight and explainable transformer-based model
that automatically assesses company risk from these reports. Unlike prior work
that relies solely on the standard deviation of excess returns (adjusted for
the Fama-French model), which indiscriminately penalizes both upside and
downside risk, TinyXRA incorporates skewness, kurtosis, and the Sortino ratio
for more comprehensive risk assessment. We leverage TinyBERT as our encoder to
efficiently process lengthy financial documents, coupled with a novel dynamic,
attention-based word cloud mechanism that provides intuitive risk visualization
while filtering irrelevant terms. This lightweight design ensures scalable
deployment across diverse computing environments with real-time processing
capabilities for thousands of financial documents which is essential for
production systems with constrained computational resources. We employ triplet
loss for risk quartile classification, improving over pairwise loss approaches
in existing literature by capturing both the direction and magnitude of risk
differences. Our TinyXRA achieves state-of-the-art predictive accuracy across
seven test years on a dataset spanning 2013-2024, while providing transparent
and interpretable risk assessments. We conduct comprehensive ablation studies
to evaluate our contributions and assess model explanations both quantitatively
by systematically removing highly attended words and sentences, and
qualitatively by examining explanation coherence. The paper concludes with
findings, practical implications, limitations, and future research directions.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [242] [Overparametrized models with posterior drift](https://arxiv.org/abs/2506.23619)
*Guillaume Coqueret,Martial Laguerre*

Main category: q-fin.ST

TL;DR: 研究后验漂移对过参数化机器学习模型样本外预测准确性的影响，以股票溢价预测为例给出不同带宽参数下的结果并建议谨慎使用大型线性模型进行股市预测。


<details>
  <summary>Details</summary>
Motivation: 探究后验漂移对过参数化机器学习模型样本外预测准确性的影响，尤其是在可能发生制度变化的场景（如金融市场）。

Method: 以股票溢价预测为应用场景，研究市场时机策略对不同子时期和控制模型复杂度的带宽参数的敏感性。

Result: 市场时机策略对不同子时期和带宽参数敏感；15 年持有期下，小带宽回报异质性大，大带宽结果更一致但风险调整回报低。

Conclusion: 使用大型线性模型进行股市预测时应谨慎。

Abstract: This paper investigates the impact of posterior drift on out-of-sample
forecasting accuracy in overparametrized machine learning models. We document
the loss in performance when the loadings of the data generating process change
between the training and testing samples. This matters crucially in settings in
which regime changes are likely to occur, for instance, in financial markets.
Applied to equity premium forecasting, our results underline the sensitivity of
a market timing strategy to sub-periods and to the bandwidth parameters that
control the complexity of the model. For the average investor, we find that
focusing on holding periods of 15 years can generate very heterogeneous
returns, especially for small bandwidths. Large bandwidths yield much more
consistent outcomes, but are far less appealing from a risk-adjusted return
standpoint. All in all, our findings tend to recommend cautiousness when
resorting to large linear models for stock market predictions.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [243] [Strategic A/B testing via Maximum Probability-driven Two-armed Bandit](https://arxiv.org/abs/2506.22536)
*Yu Zhang,Shanshan Zhao,Bokui Wan,Jinjuan Wang,Xiaodong Yan*

Main category: stat.ML

TL;DR: 本文提出基于最大概率驱动的双臂老虎机（TAB）过程检测大规模应用中的微小平均处理效应，实验显示能显著改善A/B测试。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以检测大规模应用中的微小平均处理效应，而微小改进可能有重大经济影响。

Method: 利用反事实结果框架，提出通过对平均波动统计量加权的最大概率驱动的TAB过程控制一类错误，并用置换方法增强鲁棒性和有效性。

Result: 实验结果显示在A/B测试中有显著改善。

Conclusion: 该方法可在保持高统计功效的同时降低实验成本。

Abstract: Detecting a minor average treatment effect is a major challenge in
large-scale applications, where even minimal improvements can have a
significant economic impact. Traditional methods, reliant on normal
distribution-based or expanded statistics, often fail to identify such minor
effects because of their inability to handle small discrepancies with
sufficient sensitivity. This work leverages a counterfactual outcome framework
and proposes a maximum probability-driven two-armed bandit (TAB) process by
weighting the mean volatility statistic, which controls Type I error. The
implementation of permutation methods further enhances the robustness and
efficacy. The established strategic central limit theorem (SCLT) demonstrates
that our approach yields a more concentrated distribution under the null
hypothesis and a less concentrated one under the alternative hypothesis,
greatly improving statistical power. The experimental results indicate a
significant improvement in the A/B testing, highlighting the potential to
reduce experimental costs while maintaining high statistical power.

</details>


### [244] [Adjoint Schrödinger Bridge Sampler](https://arxiv.org/abs/2506.22565)
*Guan-Horng Liu,Jaemoo Choi,Yongxin Chen,Benjamin Kurt Miller,Ricky T. Q. Chen*

Main category: stat.ML

TL;DR: 本文提出新扩散采样器Adjoint Schrödinger Bridge Sampler (ASBS)，无需估计目标样本，基于薛定谔桥模型，通过伴随匹配学习，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的采样方法因缺乏显式目标样本，需重要性加权估计或复杂学习过程，限制了可扩展性和实际应用。

Method: 提出ASBS，基于薛定谔桥模型，利用随机最优控制理论的伴随匹配方法学习，放宽无记忆条件以推广到任意源分布。

Result: 通过大量实验，证明ASBS在经典能量函数采样、构象生成和分子玻尔兹曼分布采样上有效。

Conclusion: ASBS是一种有效的扩散采样器，能解决现有方法的局限性，在多个任务中表现良好。

Abstract: Computational methods for learning to sample from the Boltzmann distribution
-- where the target distribution is known only up to an unnormalized energy
function -- have advanced significantly recently. Due to the lack of explicit
target samples, however, prior diffusion-based methods, known as diffusion
samplers, often require importance-weighted estimation or complicated learning
processes. Both trade off scalability with extensive evaluations of the energy
and model, thereby limiting their practical usage. In this work, we propose
Adjoint Schr\"odinger Bridge Sampler (ASBS), a new diffusion sampler that
employs simple and scalable matching-based objectives yet without the need to
estimate target samples during training. ASBS is grounded on a mathematical
model -- the Schr\"odinger Bridge -- which enhances sampling efficiency via
kinetic-optimal transportation. Through a new lens of stochastic optimal
control theory, we demonstrate how SB-based diffusion samplers can be learned
at scale via Adjoint Matching and prove convergence to the global solution.
Notably, ASBS generalizes the recent Adjoint Sampling (Havens et al., 2025) to
arbitrary source distributions by relaxing the so-called memoryless condition
that largely restricts the design space. Through extensive experiments, we
demonstrate the effectiveness of ASBS on sampling from classical energy
functions, amortized conformer generation, and molecular Boltzmann
distributions.

</details>


### [245] [Bayesian Invariance Modeling of Multi-Environment Data](https://arxiv.org/abs/2506.22675)
*Luhuan Wu,Mingzhang Yin,Yixin Wang,John P. Cunningham,David M. Blei*

Main category: stat.ML

TL;DR: 本文提出贝叶斯不变预测（BIP）模型用于不变预测，证明其一致性，设计变分近似 VI - BIP，在模拟和真实数据中比现有方法更准确和可扩展。


<details>
  <summary>Details</summary>
Motivation: 现有不变预测方法主要通过假设检验或正则化优化，本文旨在开发一种新的概率模型用于不变预测。

Method: 开发 BIP 模型，将不变特征的索引编码为潜在变量并通过后验推理恢复；为处理大量特征设计 VI - BIP 变分近似。

Result: 在模拟和真实数据中，BIP 和 VI - BIP 比现有不变预测方法更准确和可扩展。

Conclusion: BIP 和 VI - BIP 是更有效的不变预测方法，环境异质性越大后验收缩越快。

Abstract: Invariant prediction [Peters et al., 2016] analyzes feature/outcome data from
multiple environments to identify invariant features - those with a stable
predictive relationship to the outcome. Such features support generalization to
new environments and help reveal causal mechanisms. Previous methods have
primarily tackled this problem through hypothesis testing or regularized
optimization. Here we develop Bayesian Invariant Prediction (BIP), a
probabilistic model for invariant prediction. BIP encodes the indices of
invariant features as a latent variable and recover them by posterior
inference. Under the assumptions of Peters et al. [2016], the BIP posterior
targets the true invariant features. We prove that the posterior is consistent
and that greater environment heterogeneity leads to faster posterior
contraction. To handle many features, we design an efficient variational
approximation called VI-BIP. In simulations and real data, we find that BIP and
VI-BIP are more accurate and scalable than existing methods for invariant
prediction.

</details>


### [246] [CN-SBM: Categorical Block Modelling For Primary and Residual Copy Number Variation](https://arxiv.org/abs/2506.22963)
*Kevin Lam,William Daniels,J Maxwell Douglas,Daniel Lai,Samuel Aparicio,Benjamin Bloem-Reddy,Yongjin Park*

Main category: stat.ML

TL;DR: 提出CN - SBM用于癌症CNV分析，能聚类样本和基因组区域，有可扩展算法，在模拟和真实数据集上表现更好，应用于TCGA数据揭示临床相关亚型。


<details>
  <summary>Details</summary>
Motivation: 癌症是遗传疾病，需通过监测全基因组拷贝数变异来跟踪其克隆进化，现有模型存在不足。

Method: 引入CN - SBM概率框架，采用两阶段方法将CNV数据分解为主要和残差成分，推导可扩展变分推理算法。

Result: 在模拟和真实数据集上模型拟合优于现有方法，应用于TCGA低级别胶质瘤数据揭示临床相关亚型和结构化残差变异。

Conclusion: CN - SBM是可解释、可扩展的CNV分析框架，与肿瘤异质性和预后直接相关。

Abstract: Cancer is a genetic disorder whose clonal evolution can be monitored by
tracking noisy genome-wide copy number variants. We introduce the Copy Number
Stochastic Block Model (CN-SBM), a probabilistic framework that jointly
clusters samples and genomic regions based on discrete copy number states using
a bipartite categorical block model. Unlike models relying on Gaussian or
Poisson assumptions, CN-SBM respects the discrete nature of CNV calls and
captures subpopulation-specific patterns through block-wise structure. Using a
two-stage approach, CN-SBM decomposes CNV data into primary and residual
components, enabling detection of both large-scale chromosomal alterations and
finer aberrations. We derive a scalable variational inference algorithm for
application to large cohorts and high-resolution data. Benchmarks on simulated
and real datasets show improved model fit over existing methods. Applied to
TCGA low-grade glioma data, CN-SBM reveals clinically relevant subtypes and
structured residual variation, aiding patient stratification in survival
analysis. These results establish CN-SBM as an interpretable, scalable
framework for CNV analysis with direct relevance for tumor heterogeneity and
prognosis.

</details>


### [247] [AICO: Feature Significance Tests for Supervised Learning](https://arxiv.org/abs/2506.23396)
*Kay Giesecke,Enguerrand Horel,Chartsiri Jirachotkulthorn*

Main category: stat.ML

TL;DR: 本文提出模型和分布无关的显著性检验，评估输入特征影响，实验验证其优势与实用性。


<details>
  <summary>Details</summary>
Motivation: 监督学习算法不透明阻碍科学发现和广泛应用，需评估输入特征影响。

Method: 通过在样本中掩盖特征值评估其对模型性能的增量贡献，构建随机符号检验。

Result: 实验验证了该方法在统计和计算上的优势，以及在实际应用中的实用性。

Conclusion: 该方法假设少、无需模型再训练、计算高效，适用于大规模高维场景。

Abstract: The opacity of many supervised learning algorithms remains a key challenge,
hindering scientific discovery and limiting broader deployment -- particularly
in high-stakes domains. This paper develops model- and distribution-agnostic
significance tests to assess the influence of input features in any regression
or classification algorithm. Our method evaluates a feature's incremental
contribution to model performance by masking its values across samples. Under
the null hypothesis, the distribution of performance differences across a test
set has a non-positive median. We construct a uniformly most powerful,
randomized sign test for this median, yielding exact p-values for assessing
feature significance and confidence intervals with exact coverage for
estimating population-level feature importance. The approach requires minimal
assumptions, avoids model retraining or auxiliary models, and remains
computationally efficient even for large-scale, high-dimensional settings.
Experiments on synthetic tasks validate its statistical and computational
advantages, and applications to real-world data illustrate its practical
utility.

</details>


### [248] [DPOT: A DeepParticle method for Computation of Optimal Transport with convergence guarantee](https://arxiv.org/abs/2506.23429)
*Yingyuan Li,Aokun Wang,Zhongjian Wang*

Main category: stat.ML

TL;DR: 提出基于DeepParticle的机器学习方法计算两连续分布的最优传输映射，有理论保证，实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 解决从非配对样本计算两个连续分布间最优传输映射的问题。

Method: 基于DeepParticle方法，训练时采用min - min优化，不对网络结构设限。

Result: 理论上建立弱收敛保证和定量误差界，数值实验验证理论结果和方法有效性，在实际任务中表现良好。

Conclusion: 所提方法在计算最优传输映射方面有效，有理论支撑且适用于实际任务。

Abstract: In this work, we propose a novel machine learning approach to compute the
optimal transport map between two continuous distributions from their unpaired
samples, based on the DeepParticle methods. The proposed method leads to a
min-min optimization during training and does not impose any restriction on the
network structure. Theoretically we establish a weak convergence guarantee and
a quantitative error bound between the learned map and the optimal transport
map. Our numerical experiments validate the theoretical results and the
effectiveness of the new approach, particularly on real-world tasks.

</details>


### [249] [Minimax Optimal Two-Stage Algorithm For Moment Estimation Under Covariate Shift](https://arxiv.org/abs/2506.23453)
*Zhen Zhang,Xin Liu,Shaoli Wang,Jiaye Teng*

Main category: stat.ML

TL;DR: 本文研究协变量偏移下未知函数矩估计问题，提出两阶段算法达最小最大最优界，对未知分布情况提出截断估计量并给出上界，数值实验验证方法有效性。


<details>
  <summary>Details</summary>
Motivation: 协变量偏移中未知函数矩估计问题常见但研究不足，本文旨在研究源和目标分布已知时该问题的最小最大下界。

Method: 提出两阶段算法，先在源分布下训练最优估计器，再用似然比重加权校准矩估计器；针对分布未知情况，提出截断估计量。

Result: 广泛的数值研究在合成示例上证实了理论结果，进一步说明了所提方法的有效性。

Conclusion: 所提两阶段算法能达到最小最大最优界（至对数因子），截断估计量确保双重稳健性，方法有效可行。

Abstract: Covariate shift occurs when the distribution of input features differs
between the training and testing phases. In covariate shift, estimating an
unknown function's moment is a classical problem that remains under-explored,
despite its common occurrence in real-world scenarios. In this paper, we
investigate the minimax lower bound of the problem when the source and target
distributions are known. To achieve the minimax optimal bound (up to a
logarithmic factor), we propose a two-stage algorithm. Specifically, it first
trains an optimal estimator for the function under the source distribution, and
then uses a likelihood ratio reweighting procedure to calibrate the moment
estimator. In practice, the source and target distributions are typically
unknown, and estimating the likelihood ratio may be unstable. To solve this
problem, we propose a truncated version of the estimator that ensures double
robustness and provide the corresponding upper bound. Extensive numerical
studies on synthetic examples confirm our theoretical findings and further
illustrate the effectiveness of our proposed method.

</details>


### [250] [Test of partial effects for Frechet regression on Bures-Wasserstein manifolds](https://arxiv.org/abs/2506.23487)
*Haoshu Xu,Hongzhe Li*

Main category: stat.ML

TL;DR: 提出在Bures Wasserstein流形上进行Frechet回归部分效应评估的新测试，用样本拆分策略，证明统计量分布收敛，展示测试的准确性和实用性。


<details>
  <summary>Details</summary>
Motivation: 在Bures Wasserstein流形的Frechet回归中评估部分效应。

Method: 采用样本拆分策略，第一个子样本拟合Frechet回归模型，第二个子样本构建测试统计量。

Result: 证明统计量收敛到加权卡方分量混合分布，程序达到名义渐近大小，最坏情况功效一致收敛到1，模拟和实际数据应用展示有限样本准确性和实用性。

Conclusion: 提出的测试方法有效，具有理论性质和实际应用价值。

Abstract: We propose a novel test for assessing partial effects in Frechet regression
on Bures Wasserstein manifolds. Our approach employs a sample splitting
strategy: the first subsample is used to fit the Frechet regression model,
yielding estimates of the covariance matrices and their associated optimal
transport maps, while the second subsample is used to construct the test
statistic. We prove that this statistic converges in distribution to a weighted
mixture of chi squared components, where the weights correspond to the
eigenvalues of an integral operator defined by an appropriate RKHS kernel. We
establish that our procedure achieves the nominal asymptotic size and
demonstrate that its worst-case power converges uniformly to one. Through
extensive simulations and a real data application, we illustrate the test's
finite-sample accuracy and practical utility.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [251] [Neural models of multiscale systems: conceptual limitations, stochastic parametrizations, and a climate application](https://arxiv.org/abs/2506.22552)
*Fabrizio Falasca*

Main category: nlin.CD

TL;DR: 本文探讨多尺度动力系统数据驱动建模的概念局限，分析不同观测场景下的神经随机模型，指出物理策略对模拟复杂系统的重要性并应用于实际案例。


<details>
  <summary>Details</summary>
Motivation: 当前自回归神经模型在模拟气候系统对外部扰动的响应方面存在不足，需要探究数据驱动建模的概念局限。

Method: 先分析低维动力系统类比高维情况，构建全观测和部分观测两种场景下的神经随机模型，最后应用于海表温度场和大气顶净辐射通量的随机简化神经模型。

Result: 全观测时模型能准确捕捉平衡统计和强迫响应；部分观测时存在确定建模变量和参数化未观测自由度影响两个挑战。

Conclusion: 基于物理的策略（如粗粒化和随机参数化）对模拟复杂系统（如气候系统）在概念和实践上都至关重要。

Abstract: This work explores key conceptual limitations in data-driven modeling of
multiscale dynamical systems, focusing on neural emulators and stochastic
climate modeling. A skillful climate model should capture both stationary
statistics and responses to external perturbations. While current
autoregressive neural models often reproduce the former, they typically
struggle with the latter. We begin by analyzing a low-dimensional dynamical
system to expose, by analogy, fundamental limitations that persist in
high-dimensional settings. Specifically, we construct neural stochastic models
under two scenarios: one where the full state vector is observed, and another
with only partial observations (i.e. a subset of variables). In the first case,
the models accurately capture both equilibrium statistics and forced responses
in ensemble mean and variance. In the more realistic case of partial
observations, two key challenges emerge: (i) identifying the \textit{proper}
variables to model, and (ii) parameterizing the influence of unobserved degrees
of freedom. These issues are not specific to neural networks but reflect
fundamental limitations of data-driven modeling and the need to target the slow
dynamics of the system. We argue that physically grounded strategies -- such as
coarse-graining and stochastic parameterizations -- are critical, both
conceptually and practically, for the skillful emulation of complex systems
like the coupled climate system. Building on these insights, we turn to a more
realistic application: a stochastic reduced neural model of the sea surface
temperature field and the net radiative flux at the top of the atmosphere,
assessing its stationary statistics, response to temperature forcing, and
interpretability.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [252] [Beyond Code: The Multidimensional Impacts of Large Language Models in Software Development](https://arxiv.org/abs/2506.22704)
*Sardar Fatooreh Bonabi,Sarah Bana,Tingting Nian,Vijay Gurbaxani*

Main category: econ.GN

TL;DR: 研究大语言模型对开源软件开发的影响，利用意大利ChatGPT禁令的自然实验分析数据，发现其能提升开发者生产力、知识共享和技能获取，且不同经验开发者受益不同，研究有管理启示。


<details>
  <summary>Details</summary>
Motivation: 了解大语言模型对开源软件（OSS）开发的影响。

Method: 利用意大利临时禁止ChatGPT这一自然实验，采用双重差分框架和双向固定效应，分析意大利、法国和葡萄牙三国GitHub上88,022名OSS开发者的数据。

Result: 使用ChatGPT可使开发者生产力提升6.4%、知识共享提升9.6%、技能获取提升8.4%，不同经验开发者受益有差异，且大语言模型辅助学习高度依赖情境。

Conclusion: 大语言模型的生产力影响不仅限于代码生成，还包括协作学习和知识交流，战略性部署大语言模型可提升组织生产力和敏捷性。

Abstract: Large language models (LLMs) are poised to significantly impact software
development, especially in the Open-Source Software (OSS) sector. To understand
this impact, we first outline the mechanisms through which LLMs may influence
OSS through code development, collaborative knowledge transfer, and skill
development. We then empirically examine how LLMs affect OSS developers' work
in these three key areas. Leveraging a natural experiment from a temporary
ChatGPT ban in Italy, we employ a Difference-in-Differences framework with
two-way fixed effects to analyze data from all OSS developers on GitHub in
three similar countries, Italy, France, and Portugal, totaling 88,022 users. We
find that access to ChatGPT increases developer productivity by 6.4%, knowledge
sharing by 9.6%, and skill acquisition by 8.4%. These benefits vary
significantly by user experience level: novice developers primarily experience
productivity gains, whereas more experienced developers benefit more from
improved knowledge sharing and accelerated skill acquisition. In addition, we
find that LLM-assisted learning is highly context-dependent, with the greatest
benefits observed in technically complex, fragmented, or rapidly evolving
contexts. We show that the productivity effects of LLMs extend beyond direct
code generation to include enhanced collaborative learning and knowledge
exchange among developers; dynamics that are essential for gaining a holistic
understanding of LLMs' impact in OSS. Our findings offer critical managerial
implications: strategically deploying LLMs can accelerate novice developers'
onboarding and productivity, empower intermediate developers to foster
knowledge sharing and collaboration, and support rapid skill acquisition,
together enhancing long-term organizational productivity and agility.

</details>


### [253] [Temperature Sensitivity of Residential Energy Demand on the Global Scale: A Bayesian Partial Pooling Model](https://arxiv.org/abs/2506.22768)
*Peer Lasse Hinrichsen,Katrin Rehdanz,Richard S. J. Tol*

Main category: econ.GN

TL;DR: 本文用贝叶斯部分池化模型研究全球住宅能源需求的温度敏感性，发现不同温度下能源需求特点及不同发展程度国家的差异。


<details>
  <summary>Details</summary>
Motivation: 填补全球尺度上住宅能源需求温度敏感性研究的文献空白。

Method: 使用贝叶斯部分池化模型，估计特定国家的截距和斜率，关注非线性温度响应函数。

Result: 基于1978 - 2023年126个国家的数据，低温（<-5℃）和高温（>30℃）时住宅电力和天然气需求高，23.5℃以上电力需求与温度关系更陡峭，发达国家对高温更敏感。

Conclusion: 不同温度区间住宅能源需求有差异，发达国家和发展中国家对高温的敏感性不同，可能与发展中国家无法满足制冷需求有关。

Abstract: This paper contributes to the limited literature on the temperature
sensitivity of residential energy demand on a global scale. Using a Bayesian
Partial Pooling model, we estimate country-specific intercepts and slopes,
focusing on non-linear temperature response functions. The results, based on
data for up to 126 countries spanning from 1978 to 2023, indicate a higher
demand for residential electricity and natural gas at temperatures below -5
degrees Celsius and a higher demand for electricity at temperatures above 30
degrees Celsius. For temperatures above 23.5 degrees Celsius, the relationship
between power demand and temperature steepens. Demand in developed countries is
more sensitive to high temperatures than in less developed countries, possibly
due to an inability to meet cooling demands in the latter.

</details>


### [254] [Tracking the affordability of least-cost healthy diets helps guide intervention for food security and improved nutrition](https://arxiv.org/abs/2506.22965)
*William A. Masters*

Main category: econ.GN

TL;DR: 两篇论文推动了用最低成本基准饮食监测和改善粮食安全，助力最低成本饮食成为粮食获取诊断指标并指导干预。


<details>
  <summary>Details</summary>
Motivation: 推动最低成本基准饮食的使用，以监测和改善粮食安全。

Method: 文中未提及具体方法

Result: 两篇论文促进了最低成本饮食作为粮食获取新诊断指标在全球的使用，能区分不良饮食质量的成因。

Conclusion: 研究有助于引导干预措施，实现普遍获得健康饮食的目标。

Abstract: This Policy Comment describes how the Food Policy article entitled 'Cost and
affordability of nutritious diets at retail prices: Evidence from 177
countries' (first published October 2020) and 'Retail consumer price data
reveal gaps and opportunities to monitor food systems for nutrition' (first
published September 2021) advanced the use of least-cost benchmark diets to
monitor and improve food security. Those papers contributed to the worldwide
use of least-cost diets as a new diagnostic indicator of food access, helping
to distinguish among causes of poor diet quality related to high prices, low
incomes, or displacement by other food options, thereby guiding intervention
toward universal access to healthy diets.

</details>


### [255] [Digital Transformation and the Restructuring of Employment: Evidence from Chinese Listed Firms](https://arxiv.org/abs/2506.23230)
*Yubo Cheng*

Main category: econ.GN

TL;DR: 本文研究数字转型对中国上市公司就业结构的重塑，发现数字化使管理、专业和技术岗位招聘增加，辅助和体力劳动需求减少，抽象任务需求上升，常规和体力任务下降。


<details>
  <summary>Details</summary>
Motivation: 探究数字转型如何重塑中国上市公司的就业结构。

Method: 依据ISCO - 08和2022版中国标准职业分类对招聘数据分类，将工作分为五类；通过对职位描述的关键词分析构建任务强度指数。

Result: 数字化使管理、专业和技术岗位招聘增加，辅助和体力劳动需求减少；抽象任务需求上升，常规和体力任务下降；这些变化与管理效率和高管薪酬提升有关。

Conclusion: 新兴技术正重塑中国企业部门的技能需求和劳动力动态。

Abstract: This paper examines how digital transformation reshapes employment structures
within Chinese listed firms, focusing on occupational functions and task
intensity. Drawing on recruitment data classified under ISCO-08 and the Chinese
Standard Occupational Classification 2022, we categorize jobs into five
functional groups: management, professional, technical, auxiliary, and manual.
Using a task-based framework, we construct routine, abstract, and manual task
intensity indices through keyword analysis of job descriptions. We find that
digitalization is associated with increased hiring in managerial, professional,
and technical roles, and reduced demand for auxiliary and manual labor. At the
task level, abstract task demand rises, while routine and manual tasks decline.
Moderation analyses link these shifts to improvements in managerial efficiency
and executive compensation. Our findings highlight how emerging technologies,
including large language models (LLMs), are reshaping skill demands and labor
dynamics in Chinas corporate sector.

</details>


### [256] [Evaluating the EU Carbon Border Adjustment Mechanism with a Quantitative Trade Model](https://arxiv.org/abs/2506.23341)
*Noemi Walczak,Kenan Huremović,Armando Rungi*

Main category: econ.GN

TL;DR: 本文构建多国多部门一般均衡模型研究欧盟CBAM的经济和环境影响，发现其对欧盟和非欧盟国家有不同影响。


<details>
  <summary>Details</summary>
Motivation: 研究欧盟碳边境调节机制（CBAM）的经济和环境影响。

Method: 构建具有投入产出联系的多国多部门一般均衡模型，联合内生化排放交易计划（ETS）配额和CBAM价格。

Result: CBAM使欧盟国民总支出增加0.005%，贸易转向国内更清洁生产；欧盟直接进口中的隐含排放下降近4.80%，间接进口中的隐含排放下降约3%；非欧盟国家国民总支出略有下降（0.009%），排放泄漏减少0.11%。

Conclusion: CBAM对欧盟和非欧盟国家在经济和环境方面有不同影响，且考虑生产网络能检测到供应链上游替代效应的抑制作用。

Abstract: This paper examines the economic and environmental impacts of the European
Carbon Border Adjustment Mechanism (CBAM). We develop a multi-country,
multi-sector general equilibrium model with input-output linkages and
characterise the general equilibrium response of trade flows, welfare and
emissions. As far as we know, this is the first quantitative trade model that
jointly endogenises the Emission Trading Scheme (ETS) allowances and CBAM
prices. We find that the CBAM increases by 0.005\% the EU Gross National
Expenditure (GNE), while trade shifts towards domestic cleaner production.
Notably, emissions embodied in direct EU imports fall by almost 4.80\%, but
supply chain's upstream substitution effects imply a decrease in emissions
embodied in EU indirect imports by about 3\%. The latter involves a dampening
effect that we can detect only by explicitly incorporating the production
network. In contrast, extra-EU countries experience a slight decline in GNE
(0.009\%) and a reduction in emissions leakage (0.11\%).

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [257] [Bridging Physical and Digital Worlds: Embodied Large AI for Future Wireless Systems](https://arxiv.org/abs/2506.24009)
*Xinquan Wang,Fenghao Zhu,Zhaohui Yang,Chongwen Huang,Xiaoming Chen,Zhaoyang Zhang,Sami Muhaidat,Mérouane Debbah*

Main category: cs.IT

TL;DR: 本文提出无线具身大人工智能（WELAI）范式，从被动观察转向主动具身，探讨其设计原则、系统结构、应用，并通过案例展示其有效性和研究方向。


<details>
  <summary>Details</summary>
Motivation: 当前大型人工智能模型在无线系统应用中忽视关键物理交互，依赖离线数据集，难处理实时动态和非平稳环境，且缺乏主动探测能力。

Method: 先识别现有模型面临的关键挑战，接着探索WELAI的设计原则和系统结构，概述其在下一代无线中的应用，最后通过案例研究进行验证。

Result: 通过案例研究证明了WELAI的有效性。

Conclusion: 指出了实现自适应、稳健和自主无线系统的有前景的研究方向。

Abstract: Large artificial intelligence (AI) models offer revolutionary potential for
future wireless systems, promising unprecedented capabilities in network
optimization and performance. However, current paradigms largely overlook
crucial physical interactions. This oversight means they primarily rely on
offline datasets, leading to difficulties in handling real-time wireless
dynamics and non-stationary environments. Furthermore, these models often lack
the capability for active environmental probing. This paper proposes a
fundamental paradigm shift towards wireless embodied large AI (WELAI), moving
from passive observation to active embodiment. We first identify key challenges
faced by existing models, then we explore the design principles and system
structure of WELAI. Besides, we outline prospective applications in
next-generation wireless. Finally, through an illustrative case study, we
demonstrate the effectiveness of WELAI and point out promising research
directions for realizing adaptive, robust, and autonomous wireless systems.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [258] [Momentum-based Accelerated Algorithm for Distributed Optimization under Sector-Bound Nonlinearity](https://arxiv.org/abs/2506.22855)
*Mohammadreza Doostmohammadian,Hamid R. Rabiee*

Main category: eess.SY

TL;DR: 本文提出基于梯度跟踪技术的加速分布式算法用于局部非凸优化，证明其在特定条件下收敛，采用WB设计处理动态网络。


<details>
  <summary>Details</summary>
Motivation: 分布式优化可改进集中式机器学习方法，当前需针对局部非凸优化和网络非线性的算法。

Method: 采用梯度跟踪技术，用重球法添加动量提高收敛速度，用摄动理论和特征谱分析证明收敛，采用WB网络设计。

Result: 证明了在有扇区有界非线性和局部非凸代价函数的情况下算法收敛，能处理动态有向网络。

Conclusion: 提出的加速分布式算法能有效解决局部非凸优化问题，WB设计可应对动态网络情况。

Abstract: Distributed optimization advances centralized machine learning methods by
enabling parallel and decentralized learning processes over a network of
computing nodes. This work provides an accelerated consensus-based distributed
algorithm for locally non-convex optimization using the gradient-tracking
technique. The proposed algorithm (i) improves the convergence rate by adding
momentum towards the optimal state using the heavy-ball method, while (ii)
addressing general sector-bound nonlinearities over the information-sharing
network. The link nonlinearity includes any sign-preserving odd sector-bound
mapping, for example, log-scale data quantization or clipping in practical
applications. For admissible momentum and gradient-tracking parameters, using
perturbation theory and eigen-spectrum analysis, we prove convergence even in
the presence of sector-bound nonlinearity and for locally non-convex cost
functions. Further, in contrast to most existing weight-stochastic algorithms,
we adopt weight-balanced (WB) network design. This WB design and
perturbation-based analysis allow to handle dynamic directed network of agents
to address possible time-varying setups due to link failures or packet drops.

</details>


### [259] [Hierarchical Decentralized Stochastic Control for Cyber-Physical Systems](https://arxiv.org/abs/2506.22971)
*Kesav Kazam Ramachandran Anantharaman,Rahul Meshram*

Main category: eess.SY

TL;DR: 提出用于网络物理系统控制的两层级分散式架构，包含全局和局部控制器，为局部控制器提出COpt和FOpt框架并研究其性质。


<details>
  <summary>Details</summary>
Motivation: 设计有效的网络物理系统控制架构。

Method: 将架构各部分建模为马尔可夫决策过程，全局控制器优化无限时域折扣累积奖励，为局部控制器提出COpt和FOpt两种优化框架。

Result: 确立两种框架平稳确定性最优策略的存在性，研究框架间关系、给出最优值函数差异界限和导致相同最优值的充分条件。

Conclusion: 所提出的两层级架构和局部控制器优化框架在网络物理系统控制中有一定理论基础和可行性。

Abstract: This paper presents a two-timescale hierarchical decentralized architecture
for control of Cyber-Physical Systems. The architecture consists of $N$
independent sub-processes, a global controller, and $N$ local controllers, each
formulated as a Markov Decision Process (MDP). The global controller, operating
at a slower timescale optimizes the infinite-horizon discounted cumulative
reward under budget constraints. For the local controllers, operating at a
faster timescale, we propose two different optimization frameworks, namely the
COpt and FOpt. In the COpt framework, the local controller also optimizes an
infinite-horizon MDP, while in the FOpt framework, the local controller
optimizes a finite-horizon MDP. The FOpt framework mimics a federal structure,
where the local controllers have more autonomy in their decision making. First,
the existence of stationary deterministic optimal policies for both these
frameworks is established. Then, various relationships between the two
frameworks are studied, including a bound on the difference between the two
optimal value functions. Additionally, sufficiency conditions are provided such
that the two frameworks lead to the same optimal values.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [260] [Seeding neural network quantum states with tensor network states](https://arxiv.org/abs/2506.23550)
*Ryui Kaneko,Shimpei Goto*

Main category: cond-mat.str-el

TL;DR: 提出通过CP分解将MPS近似转换为受限玻尔兹曼机波函数的高效方法，并以横场伊辛模型为例展示其效率。


<details>
  <summary>Details</summary>
Motivation: 寻找高效方法将MPS转换为受限玻尔兹曼机波函数，为量子多体基态计算生成良好的初始神经网络量子态。

Method: 通过MPS的CP分解将其近似转换为包含多项隐藏单元的受限玻尔兹曼机波函数。

Result: 以横场伊辛模型为例证明了方法的效率。

Conclusion: 该方法可在变分参数数量的多项式时间内生成初始态，且随CP分解秩增加缩短初始态与基态距离，可应用于更一般的量子多体系统。

Abstract: We find an efficient approach to approximately convert matrix product states
(MPSs) into restricted Boltzmann machine wave functions consisting of a
multinomial hidden unit through a canonical polyadic (CP) decomposition of the
MPSs. This method allows us to generate well-behaved initial neural network
quantum states for quantum many-body ground-state calculations in polynomial
time of the number of variational parameters and systematically shorten the
distance between the initial states and the ground states with increasing the
rank of the CP decomposition. We demonstrate the efficiency of our method by
taking the transverse-field Ising model as an example and discuss possible
applications of our method to more general quantum many-body systems in which
the ground-state wave functions possess complex nodal structures.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [261] [Lock Prediction for Zero-Downtime Database Encryption](https://arxiv.org/abs/2506.23985)
*Mohamed Sami Rakha,Adam Sorrenti,Greg Stager,Walid Rjaibi,Andriy Miranskyy*

Main category: cs.CR

TL;DR: 本文针对企业数据库系统平衡数据安全与性能挑战，提出基于深度学习预测数据库锁序列的方法，实验表明该模型预测准确率高，为在线加密提供方向。


<details>
  <summary>Details</summary>
Motivation: 现代企业数据库系统在平衡数据安全和性能方面面临挑战，现有全量数据库加密方法存在停机时间长和存储开销大问题，难以在高吞吐量环境实施在线加密。

Method: 提出利用深度学习模型预测数据库锁序列的方法，以IBM Db2为研究对象，收集TPC - C基准工作负载的专业数据集，用Transformer和LSTM等架构评估不同表级和页级锁预测模型，并与Naive Baseline对比。

Result: 深度学习模型在表级预测平均准确率达49%，页级达66%，优于Naive Baseline。

Conclusion: 该方法通过预测后续锁定的表和页，为在线加密迈出一步，为构建安全、低开销的数据库系统提供可行途径。

Abstract: Modern enterprise database systems face significant challenges in balancing
data security and performance. Ensuring robust encryption for sensitive
information is critical for systems' compliance with security standards.
Although holistic database encryption provides strong protection, existing
database systems often require a complete backup and restore cycle, resulting
in prolonged downtime and increased storage usage. This makes it difficult to
implement online encryption techniques in high-throughput environments without
disrupting critical operations.
  To address this challenge, we envision a solution that enables online
database encryption aligned with system activity, eliminating the need for
downtime, storage overhead, or full-database reprocessing. Central to this
vision is the ability to predict which parts of the database will be accessed
next, allowing encryption to be applied online. As a step towards this
solution, this study proposes a predictive approach that leverages deep
learning models to forecast database lock sequences, using IBM Db2 as the
database system under study. In this study, we collected a specialized dataset
from TPC-C benchmark workloads, leveraging lock event logs for model training
and evaluation. We applied deep learning architectures, such as Transformer and
LSTM, to evaluate models for various table-level and page-level lock
predictions. We benchmark the accuracy of the trained models versus a Naive
Baseline across different prediction horizons and timelines.
  The study experiments demonstrate that the proposed deep learning-based
models achieve up to 49% average accuracy for table-level and 66% for
page-level predictions, outperforming a Naive Baseline. By anticipating which
tables and pages will be locked next, the proposed approach is a step toward
online encryption, offering a practical path toward secure, low-overhead
database systems.

</details>


### [262] [Detect \& Score: Privacy-Preserving Misbehaviour Detection and Contribution Evaluation in Federated Learning](https://arxiv.org/abs/2506.23583)
*Marvin Xhemrishi,Alexandre Graell i Amat,Balázs Pejó*

Main category: cs.CR

TL;DR: 结合QI和FedGT优势实现稳健的恶意行为检测和准确的贡献评估，实验表现优于单独使用任一方法。


<details>
  <summary>Details</summary>
Motivation: 安全聚合使联邦学习保护隐私，但检测恶意客户端行为和评估个体贡献困难，现有QI和FedGT各有不足。

Method: 结合QI和FedGT的优势。

Result: 实验显示比单独使用任一方法有更优表现。

Conclusion: 结合QI和FedGT能同时实现稳健的恶意行为检测和准确的贡献评估。

Abstract: Federated learning with secure aggregation enables private and collaborative
learning from decentralised data without leaking sensitive client information.
However, secure aggregation also complicates the detection of malicious client
behaviour and the evaluation of individual client contributions to the
learning. To address these challenges, QI (Pejo et al.) and FedGT (Xhemrishi et
al.) were proposed for contribution evaluation (CE) and misbehaviour detection
(MD), respectively. QI, however, lacks adequate MD accuracy due to its reliance
on the random selection of clients in each training round, while FedGT lacks
the CE ability. In this work, we combine the strengths of QI and FedGT to
achieve both robust MD and accurate CE. Our experiments demonstrate superior
performance compared to using either method independently.

</details>


### [263] [VERA: Variational Inference Framework for Jailbreaking Large Language Models](https://arxiv.org/abs/2506.22666)
*Anamika Lochab,Lu Yan,Patrick Pynadath,Xiangyu Zhang,Ruqi Zhang*

Main category: cs.CR

TL;DR: 介绍了一种名为VERA的越狱框架，可解决现有黑盒越狱方法的不足，实验显示VERA在多目标大语言模型上表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前仅API访问大语言模型的情况增加，需要有效的黑盒越狱方法识别模型漏洞，而现有方法存在初始化和依赖手动提示池等问题。

Method: 将黑盒越狱提示转化为变分推理问题，训练小型攻击大语言模型来近似目标大语言模型在对抗性提示上的后验分布。

Result: VERA在一系列目标大语言模型上取得了良好的性能。

Conclusion: 概率推理对于对抗性提示生成具有价值。

Abstract: The rise of API-only access to state-of-the-art LLMs highlights the need for
effective black-box jailbreak methods to identify model vulnerabilities in
real-world settings. Without a principled objective for gradient-based
optimization, most existing approaches rely on genetic algorithms, which are
limited by their initialization and dependence on manually curated prompt
pools. Furthermore, these methods require individual optimization for each
prompt, failing to provide a comprehensive characterization of model
vulnerabilities. To address this gap, we introduce VERA: Variational infErence
fRamework for jAilbreaking. VERA casts black-box jailbreak prompting as a
variational inference problem, training a small attacker LLM to approximate the
target LLM's posterior over adversarial prompts. Once trained, the attacker can
generate diverse, fluent jailbreak prompts for a target query without
re-optimization. Experimental results show that VERA achieves strong
performance across a range of target LLMs, highlighting the value of
probabilistic inference for adversarial prompt generation.

</details>


### [264] [General Autonomous Cybersecurity Defense: Learning Robust Policies for Dynamic Topologies and Diverse Attackers](https://arxiv.org/abs/2506.22706)
*Arun Ramamurthy,Neil Dhir*

Main category: cs.CR

TL;DR: 现有自主网络安全防御系统有局限性，本文探索开发能在动态网络环境学习通用策略的通用自主网络安全防御方法。


<details>
  <summary>Details</summary>
Motivation: 现有自主网络安全防御系统依赖有局限性的假设，在现实场景中难以适应网络拓扑变化，且存在过拟合问题，无法泛化到分布外的网络拓扑。

Method: 探索开发能在动态网络环境学习通用策略的通用自主网络安全防御（GACD）方法。

Result: 未提及。

Conclusion: 未提及。

Abstract: In the face of evolving cyber threats such as malware, ransomware and
phishing, autonomous cybersecurity defense (ACD) systems have become essential
for real-time threat detection and response with optional human intervention.
However, existing ACD systems rely on limiting assumptions, particularly the
stationarity of the underlying network dynamics. In real-world scenarios,
network topologies can change due to actions taken by attackers or defenders,
system failures, or time evolution of networks, leading to failures in the
adaptive capabilities of current defense agents. Moreover, many agents are
trained on static environments, resulting in overfitting to specific
topologies, which hampers their ability to generalize to out-of-distribution
network topologies. This work addresses these challenges by exploring methods
for developing agents to learn generalizable policies across dynamic network
environments -- general ACD (GACD).

</details>


### [265] [Threadbox: Sandboxing for Modular Security](https://arxiv.org/abs/2506.23683)
*Maysara Alhindi,Joseph Hallett*

Main category: cs.CR

TL;DR: 研究现有沙箱机制应用挑战，提出Threadbox沙箱机制并展示案例与局限。


<details>
  <summary>Details</summary>
Motivation: 现有操作系统沙箱机制应用时可能需开发者重构代码，要研究其应用于特定应用的挑战。

Method: 研究现有机制挑战，提出Threadbox沙箱机制，它能实现模块化和独立沙箱，可用于线程和特定函数。

Result: 通过案例研究展示了Threadbox想法的适用性。

Conclusion: 提出的Threadbox沙箱机制有一定适用性，但也存在局限性。

Abstract: There are many sandboxing mechanisms provided by operating systems to limit
what resources applications can access, however, sometimes the use of these
mechanisms requires developers to refactor their code to fit the sandboxing
model. In this work, we investigate what makes existing sandboxing mechanisms
challenging to apply to certain types of applications, and propose Threadbox, a
sandboxing mechanism that enables having modular and independent sandboxes, and
can be applied to threads and sandbox specific functions. We present case
studies to illustrate the applicability of the idea and discuss its
limitations.

</details>


### [266] [An ontological lens on attack trees: Toward adequacy and interoperability](https://arxiv.org/abs/2506.23841)
*Ítalo Oliveira,Stefano M. Nicoletti,Gal Engelberg,Mattia Fumagalli,Dan Klein,Giancarlo Guizzardi*

Main category: cs.CR

TL;DR: 本文分析攻击树（AT）存在的本体论缺陷并探讨解决方法。


<details>
  <summary>Details</summary>
Motivation: 攻击树虽可用于安全分析，但因缺乏本体论基础存在局限性，需对其进行本体论分析。

Method: 基于价值与风险通用本体（COVER）进行本体论分析。

Result: 揭示攻击树存在模糊语法术语、本体论缺失、缺乏建模指导和语义互操作性等四个显著缺点。

Conclusion: 讨论现有增量解决方案，分析为通过更广泛的风险管理建模方法克服这些问题铺平道路。

Abstract: Attack Trees (AT) are a popular formalism for security analysis. They are
meant to display an attacker's goal decomposed into attack steps needed to
achieve it and compute certain security metrics (e.g., attack cost,
probability, and damage). ATs offer three important services: (a) conceptual
modeling capabilities for representing security risk management scenarios, (b)
a qualitative assessment to find root causes and minimal conditions of
successful attacks, and (c) quantitative analyses via security metrics
computation under formal semantics, such as minimal time and cost among all
attacks. Still, the AT language presents limitations due to its lack of
ontological foundations, thus compromising associated services. Via an
ontological analysis grounded in the Common Ontology of Value and Risk (COVER)
-- a reference core ontology based on the Unified Foundational Ontology (UFO)
-- we investigate the ontological adequacy of AT and reveal four significant
shortcomings: (1) ambiguous syntactical terms that can be interpreted in
various ways; (2) ontological deficit concerning crucial domain-specific
concepts; (3) lacking modeling guidance to construct ATs decomposing a goal;
(4) lack of semantic interoperability, resulting in ad hoc stand-alone tools.
We also discuss existing incremental solutions and how our analysis paves the
way for overcoming those issues through a broader approach to risk management
modeling.

</details>


### [267] [Exploring Privacy and Security as Drivers for Environmental Sustainability in Cloud-Based Office Solutions](https://arxiv.org/abs/2506.23866)
*Jason Kayembe,Iness Ben Guirat,Jan Tobias Mühlberg*

Main category: cs.CR

TL;DR: 本文探究基于云的办公解决方案中隐私、安全与环境可持续性的交集，量化能源使用和碳排放，通过框架评估不同邮件服务，发现自托管方案最节能，Proton Mail在商业服务中最有效。


<details>
  <summary>Details</summary>
Motivation: 探究隐私、安全和环境可持续性在云办公解决方案中的交集，验证隐私服务更节能的假设。

Method: 提出基于能源使用和网络数据流量衡量环境成本的框架，分析架构和商业模式，运用现有评估方法和工具，对三种主流邮件服务及自托管解决方案进行评估。

Result: 自托管方案即使有加密开销仍最节能，比Gmail节省达33%排放；商业服务中Proton Mail最有效，比Outlook每会话节省达0.1gCO₂e，Outlook通过广告拦截可减排2%。

Conclusion: 隐私服务在能源效率上表现更好，不同邮件服务在节能和减排上有明显差异。

Abstract: In this paper, we explore the intersection of privacy, security, and
environmental sustainability in cloud-based office solutions, focusing on
quantifying user- and network-side energy use and associated carbon emissions.
We hypothesise that privacy-focused services are typically more
energy-efficient than those funded through data collection and advertising. To
evaluate this, we propose a framework that systematically measures
environmental costs based on energy usage and network data traffic during
well-defined, automated usage scenarios. To test our hypothesis, we first
analyse how underlying architectures and business models, such as monetisation
through personalised advertising, contribute to the environmental footprint of
these services. We then explore existing methodologies and tools for software
environmental impact assessment. We apply our framework to three mainstream
email services selected to reflect different privacy policies, from
ad-supported tracking-intensive models to privacy-focused designs: Microsoft
Outlook, Google Mail (Gmail), and Proton Mail. We extend this comparison to a
self-hosted email solution, evaluated with and without end-to-end encryption.
We show that the self-hosted solution, even with 14% of device energy and 15%
of emissions overheads from PGP encryption, remains the most energy-efficient,
saving up to 33% of emissions per session compared to Gmail. Among commercial
providers, Proton Mail is the most efficient, saving up to 0.1 gCO2 e per
session compared to Outlook, whose emissions can be further reduced by 2%
through ad-blocking.

</details>


### [268] [SABRE-FL: Selective and Accurate Backdoor Rejection for Federated Prompt Learning](https://arxiv.org/abs/2506.22506)
*Momin Ahmad Khan,Yasra Chandio,Fatima Muhammad Anwar*

Main category: cs.CR

TL;DR: 本文首次研究联邦提示学习中的后门攻击，提出轻量级模块化防御方法SABRE - FL，在多个数据集上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有研究未充分探索联邦提示学习的安全影响，发现恶意客户端注入噪声触发器会使全局提示学习器易受攻击。

Method: 提出SABRE - FL，用基于离群分布数据离线训练的嵌入空间异常检测器过滤中毒的提示更新，无需访问原始客户端数据或标签。

Result: 理论和实证表明基于嵌入的检测器可可靠识别和过滤恶意客户端，SABRE - FL在五个数据集和四个基线防御中显著降低后门准确率并保持干净准确率。

Conclusion: SABRE - FL经验性能强，强调未来联邦系统中需要强大的提示学习。

Abstract: Federated Prompt Learning has emerged as a communication-efficient and
privacy-preserving paradigm for adapting large vision-language models like CLIP
across decentralized clients. However, the security implications of this setup
remain underexplored. In this work, we present the first study of backdoor
attacks in Federated Prompt Learning. We show that when malicious clients
inject visually imperceptible, learnable noise triggers into input images, the
global prompt learner becomes vulnerable to targeted misclassification while
still maintaining high accuracy on clean inputs. Motivated by this
vulnerability, we propose SABRE-FL, a lightweight, modular defense that filters
poisoned prompt updates using an embedding-space anomaly detector trained
offline on out-of-distribution data. SABRE-FL requires no access to raw client
data or labels and generalizes across diverse datasets. We show, both
theoretically and empirically, that malicious clients can be reliably
identified and filtered using an embedding-based detector. Across five diverse
datasets and four baseline defenses, SABRE-FL outperforms all baselines by
significantly reducing backdoor accuracy while preserving clean accuracy,
demonstrating strong empirical performance and underscoring the need for robust
prompt learning in future federated systems.

</details>


### [269] [In-context learning for the classification of manipulation techniques in phishing emails](https://arxiv.org/abs/2506.22515)
*Antony Dalmiere,Guillaume Auriol,Vincent Nicomette,Pascal Marchand*

Main category: cs.CR

TL;DR: 研究用大语言模型上下文学习对钓鱼邮件基于40种操纵技术分类，用GPT - 4o - mini在法国钓鱼邮件上测试，准确率0.76，证明ICL在钓鱼分析有潜力。


<details>
  <summary>Details</summary>
Motivation: 传统钓鱼检测常忽略心理操纵，因此研究用大语言模型上下文学习对钓鱼邮件进行细粒度分类。

Method: 基于40种操纵技术的分类法，使用GPT - 4o - mini对现实世界法国钓鱼邮件进行少样本学习，与人工标注测试集对比。

Result: 该方法能有效识别常见操纵技术，准确率达0.76。

Conclusion: 证明了上下文学习在细致钓鱼分析中的潜力，提供了攻击者策略的见解。

Abstract: Traditional phishing detection often overlooks psychological manipulation.
This study investigates using Large Language Model (LLM) In-Context Learning
(ICL) for fine-grained classification of phishing emails based on a taxonomy of
40 manipulation techniques. Using few-shot examples with GPT-4o-mini on
real-world French phishing emails (SignalSpam), we evaluated performance
against a human-annotated test set (100 emails). The approach effectively
identifies prevalent techniques (e.g., Baiting, Curiosity Appeal, Request For
Minor Favor) with a promising accuracy of 0.76. This work demonstrates ICL's
potential for nuanced phishing analysis and provides insights into attacker
strategies.

</details>


### [270] [A Survey on Model Extraction Attacks and Defenses for Large Language Models](https://arxiv.org/abs/2506.22521)
*Kaixiang Zhao,Lincan Li,Kaize Ding,Neil Zhenqiang Gong,Yue Zhao,Yushun Dong*

Main category: cs.CR

TL;DR: 本文对大语言模型的提取攻击和防御进行全面分类，分析攻击和防御方法，提出评估指标，指出当前方法局限并给出研究方向。


<details>
  <summary>Details</summary>
Motivation: 模型提取攻击对部署的语言模型造成重大安全威胁，可能危及知识产权和用户隐私，需要进行研究应对。

Method: 对攻击进行分类，分析多种攻击方法和防御机制，提出评估指标。

Result: 识别出当前方法的关键局限。

Conclusion: 提出集成攻击方法和自适应防御机制等有前景的研究方向，为生产环境中保护语言模型提供参考。

Abstract: Model extraction attacks pose significant security threats to deployed
language models, potentially compromising intellectual property and user
privacy. This survey provides a comprehensive taxonomy of LLM-specific
extraction attacks and defenses, categorizing attacks into functionality
extraction, training data extraction, and prompt-targeted attacks. We analyze
various attack methodologies including API-based knowledge distillation, direct
querying, parameter recovery, and prompt stealing techniques that exploit
transformer architectures. We then examine defense mechanisms organized into
model protection, data privacy protection, and prompt-targeted strategies,
evaluating their effectiveness across different deployment scenarios. We
propose specialized metrics for evaluating both attack effectiveness and
defense performance, addressing the specific challenges of generative language
models. Through our analysis, we identify critical limitations in current
approaches and propose promising research directions, including integrated
attack methodologies and adaptive defense mechanisms that balance security with
model utility. This work serves NLP researchers, ML engineers, and security
professionals seeking to protect language models in production environments.

</details>


### [271] [Kill Two Birds with One Stone! Trajectory enabled Unified Online Detection of Adversarial Examples and Backdoor Attacks](https://arxiv.org/abs/2506.22722)
*Anmin Fu,Fanyu Meng,Huaibing Peng,Hua Ma,Zhi Zhang,Yifeng Zheng,Willy Susilo,Yansong Gao*

Main category: cs.CR

TL;DR: 提出统一在线检测框架UniGuard，可同时处理对抗样本和后门攻击，利用LSTM和频谱变换，经多模态多任务验证性能优越。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏能同时处理对抗样本和后门攻击的统一在线检测框架，有必要开发新方法应对这两类攻击。

Method: 基于两种见解构建UniGuard，将传播轨迹视为时间序列信号，利用LSTM和频谱变换放大对抗和良性轨迹差异。

Result: 在多种模态和任务上广泛验证了UniGuard的效率和有效性，与SOTA方法相比性能更优。

Conclusion: UniGuard是首个能同时处理对抗样本和后门攻击的统一在线检测框架，性能优越。

Abstract: The proposed UniGuard is the first unified online detection framework capable
of simultaneously addressing adversarial examples and backdoor attacks.
UniGuard builds upon two key insights: first, both AE and backdoor attacks have
to compromise the inference phase, making it possible to tackle them
simultaneously during run-time via online detection. Second, an adversarial
input, whether a perturbed sample in AE attacks or a trigger-carrying sample in
backdoor attacks, exhibits distinctive trajectory signatures from a benign
sample as it propagates through the layers of a DL model in forward inference.
The propagation trajectory of the adversarial sample must deviate from that of
its benign counterpart; otherwise, the adversarial objective cannot be
fulfilled. Detecting these trajectory signatures is inherently challenging due
to their subtlety; UniGuard overcomes this by treating the propagation
trajectory as a time-series signal, leveraging LSTM and spectrum transformation
to amplify differences between adversarial and benign trajectories that are
subtle in the time domain. UniGuard exceptional efficiency and effectiveness
have been extensively validated across various modalities (image, text, and
audio) and tasks (classification and regression), ranging from diverse model
architectures against a wide range of AE attacks and backdoor attacks,
including challenging partial backdoors and dynamic triggers. When compared to
SOTA methods, including ContraNet (NDSS 22) specific for AE detection and TED
(IEEE SP 24) specific for backdoor detection, UniGuard consistently
demonstrates superior performance, even when matched against each method's
strengths in addressing their respective threats-each SOTA fails to parts of
attack strategies while UniGuard succeeds for all.

</details>


### [272] [A Study on Semi-Supervised Detection of DDoS Attacks under Class Imbalance](https://arxiv.org/abs/2506.22949)
*Ehsan Hallaji,Vaishnavi Shanmugam,Roozbeh Razavi-Far,Mehrdad Saif*

Main category: cs.CR

TL;DR: 研究用半监督学习技术在数据不平衡和部分标记情况下改进DDoS攻击检测，评估13种算法并为设计智能入侵检测系统提供见解。


<details>
  <summary>Details</summary>
Motivation: 网络安全中消除DDoS攻击困难，用人工智能自动化该任务因数据类不平衡和标记样本不足而复杂，故研究半监督学习技术改进检测。

Method: 评估13种最先进的半监督学习算法在多种场景下检测DDoS攻击的情况。

Result: 评估了算法的实际效能和缺点，包括在极端环境下的工作程度。

Conclusion: 研究结果为设计能应对类不平衡和部分标记数据的智能入侵检测系统提供见解。

Abstract: One of the most difficult challenges in cybersecurity is eliminating
Distributed Denial of Service (DDoS) attacks. Automating this task using
artificial intelligence is a complex process due to the inherent class
imbalance and lack of sufficient labeled samples of real-world datasets. This
research investigates the use of Semi-Supervised Learning (SSL) techniques to
improve DDoS attack detection when data is imbalanced and partially labeled. In
this process, 13 state-of-the-art SSL algorithms are evaluated for detecting
DDoS attacks in several scenarios. We evaluate their practical efficacy and
shortcomings, including the extent to which they work in extreme environments.
The results will offer insight into designing intelligent Intrusion Detection
Systems (IDSs) that are robust against class imbalance and handle partially
labeled data.

</details>


### [273] [MetaCipher: A General and Extensible Reinforcement Learning Framework for Obfuscation-Based Jailbreak Attacks on Black-Box LLMs](https://arxiv.org/abs/2506.22557)
*Boyuan Chen,Minghao Shao,Abdul Basit,Siddharth Garg,Muhammad Shafique*

Main category: cs.CR

TL;DR: 提出MetaCipher越狱框架及动态密码选择机制，通过实验证明其攻击成功率高，有长期鲁棒性和适应性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型面临基于混淆的越狱攻击，现有安全机制难以防御加密内容攻击。

Method: 提出MetaCipher框架，结合强化学习的动态密码选择机制，对多种受害者大语言模型进行大规模密码性能实证分析。

Result: 在标准恶意提示基准上，MetaCipher对非推理大语言模型攻击成功率超92%，对推理能力大语言模型超74%，优于现有基于混淆的越狱方法。

Conclusion: MetaCipher方法具有长期鲁棒性和适应性，面对先进安全措施比之前方法更具弹性。

Abstract: The growing capabilities of large language models (LLMs) have exposed them to
increasingly sophisticated jailbreak attacks. Among these, obfuscation-based
attacks -- which encrypt malicious content to evade detection -- remain highly
effective. By leveraging the reasoning ability of advanced LLMs to interpret
encrypted prompts, such attacks circumvent conventional defenses that rely on
keyword detection or context filtering. These methods are very difficult to
defend against, as existing safety mechanisms are not designed to interpret or
decode ciphered content. In this work, we propose \textbf{MetaCipher}, a novel
obfuscation-based jailbreak framework, along with a reinforcement
learning-based dynamic cipher selection mechanism that adaptively chooses
optimal encryption strategies from a cipher pool. This approach enhances
jailbreak effectiveness and generalizability across diverse task types, victim
LLMs, and safety guardrails. Our framework is modular and extensible by design,
supporting arbitrary cipher families and accommodating evolving adversarial
strategies. We complement our method with a large-scale empirical analysis of
cipher performance across multiple victim LLMs. Within as few as 10 queries,
MetaCipher achieves over 92\% attack success rate (ASR) on most recent standard
malicious prompt benchmarks against state-of-the-art non-reasoning LLMs, and
over 74\% ASR against reasoning-capable LLMs, outperforming all existing
obfuscation-based jailbreak methods. These results highlight the long-term
robustness and adaptability of our approach, making it more resilient than
prior methods in the face of advancing safety measures.

</details>


### [274] [A User-Centric, Privacy-Preserving, and Verifiable Ecosystem for Personal Data Management and Utilization](https://arxiv.org/abs/2506.22606)
*Osama Zafar,Mina Namazi,Yuqiao Xu,Youngjin Yoo,Erman Ayday*

Main category: cs.CR

TL;DR: 传统集中式架构处理个人数据有隐私等问题，本文提出新的去中心化隐私保护架构，让用户有数据控制权，用先进技术保证安全和隐私。


<details>
  <summary>Details</summary>
Motivation: 当前集中式管理个人数据存在隐私、安全和自主性问题，传统架构难以满足严格隐私要求，需要范式转变。

Method: 引入能处理异构个人信息的去中心化隐私保护架构，采用安全飞地和联邦学习等先进隐私增强技术。

Result: 系统支持本地计算、模型训练和隐私保护数据共享等功能，确保数据可信度和用户隐私。

Conclusion: 新架构能解决传统集中式架构问题，让用户拥有数据所有权和控制权，同时保障隐私。

Abstract: In the current paradigm of digital personalized services, the centralized
management of personal data raises significant privacy concerns, security
vulnerabilities, and diminished individual autonomy over sensitive information.
Despite their efficiency, traditional centralized architectures frequently fail
to satisfy rigorous privacy requirements and expose users to data breaches and
unauthorized access risks. This pressing challenge calls for a fundamental
paradigm shift in methodologies for collecting, storing, and utilizing personal
data across diverse sectors, including education, healthcare, and finance.
  This paper introduces a novel decentralized, privacy-preserving architecture
that handles heterogeneous personal information, ranging from educational
credentials to health records and financial data. Unlike traditional models,
our system grants users complete data ownership and control, allowing them to
selectively share information without compromising privacy. The architecture's
foundation comprises advanced privacy-enhancing technologies, including secure
enclaves and federated learning, enabling secure computation, verification, and
data sharing. The system supports diverse functionalities, including local
computation, model training, and privacy-preserving data sharing, while
ensuring data credibility and robust user privacy.

</details>


### [275] [Efficient Cybersecurity Assessment Using SVM and Fuzzy Evidential Reasoning for Resilient Infrastructure](https://arxiv.org/abs/2506.22938)
*Zaydon L. Ali,Wassan Saad Abduljabbar Hayale,Israa Ibraheem Al_Barazanchi,Ravi Sekhar,Pritesh Shah,Sushma Parihar*

Main category: cs.CR

TL;DR: 当前超媒体知识发展使数字信息隐私成关键问题，本文提出用支持向量机的安全阶段暴露模型和模糊证据推理的评估模型应对，还进行性能分析。


<details>
  <summary>Details</summary>
Motivation: 现有安全协议有漏洞，多种编码模型不安全，测试评估算法耗时长，需快速精准识别评估算法。

Method: 提出支持向量机的安全阶段暴露模型，用常见安全组件形成数据集；提出模糊证据推理评估模型处理和整合风险评估数据。

Result: 文中未明确提及具体结果，仅说明会用召回率、F1分数和准确率等进行性能分析。

Conclusion: 文中未明确提及最终结论。

Abstract: With current advancement in hybermedia knowledges, the privacy of digital
information has developed a critical problem. To overawed the susceptibilities
of present security protocols, scholars tend to focus mainly on efforts on
alternation of current protocols. Over past decade, various proposed encoding
models have been shown insecurity, leading to main threats against significant
data. Utilizing the suitable encryption model is very vital means of guard
against various such, but algorithm is selected based on the dependency of data
which need to be secured. Moreover, testing potentiality of the security
assessment one by one to identify the best choice can take a vital time for
processing. For faster and precisive identification of assessment algorithm, we
suggest a security phase exposure model for cipher encryption technique by
invoking Support Vector Machine (SVM). In this work, we form a dataset using
usual security components like contrast, homogeneity. To overcome the
uncertainty in analysing the security and lack of ability of processing data to
a risk assessment mechanism. To overcome with such complications, this paper
proposes an assessment model for security issues using fuzzy evidential
reasoning (ER) approaches. Significantly, the model can be utilised to process
and assemble risk assessment data on various aspects in systematic ways. To
estimate the performance of our framework, we have various analyses like,
recall, F1 score and accuracy.

</details>


### [276] [From Prompt Injections to Protocol Exploits: Threats in LLM-Powered AI Agents Workflows](https://arxiv.org/abs/2506.23260)
*Mohamed Amine Ferrag,Norbert Tihanyi,Djallel Hamouda,Leandros Maglaras,Merouane Debbah*

Main category: cs.CR

TL;DR: 介绍LLM智能体生态系统统一端到端威胁模型，涵盖攻击技术分类、场景评估及现有防御评价，指出开放挑战与研究方向。


<details>
  <summary>Details</summary>
Motivation: 插件等的快速发展使发现机制和安全实践滞后，LLM智能体集成易受多种威胁。

Method: 引入统一端到端威胁模型，将其分为四个领域，对各领域攻击场景评估可行性和现有防御。

Result: 完成威胁分类，评估攻击场景和现有防御。

Conclusion: 工作为设计稳健防御机制和建立弹性工作流最佳实践提供全面参考。

Abstract: Autonomous AI agents powered by large language models (LLMs) with structured
function-calling interfaces have dramatically expanded capabilities for
real-time data retrieval, complex computation, and multi-step orchestration.
Yet, the explosive proliferation of plugins, connectors, and inter-agent
protocols has outpaced discovery mechanisms and security practices, resulting
in brittle integrations vulnerable to diverse threats. In this survey, we
introduce the first unified, end-to-end threat model for LLM-agent ecosystems,
spanning host-to-tool and agent-to-agent communications, formalize adversary
capabilities and attacker objectives, and catalog over thirty attack
techniques. Specifically, we organized the threat model into four domains:
Input Manipulation (e.g., prompt injections, long-context hijacks, multimodal
adversarial inputs), Model Compromise (e.g., prompt- and parameter-level
backdoors, composite and encrypted multi-backdoors, poisoning strategies),
System and Privacy Attacks (e.g., speculative side-channels, membership
inference, retrieval poisoning, social-engineering simulations), and Protocol
Vulnerabilities (e.g., exploits in Model Context Protocol (MCP), Agent
Communication Protocol (ACP), Agent Network Protocol (ANP), and Agent-to-Agent
(A2A) protocol). For each category, we review representative scenarios, assess
real-world feasibility, and evaluate existing defenses. Building on our threat
taxonomy, we identify key open challenges and future research directions, such
as securing MCP deployments through dynamic trust management and cryptographic
provenance tracking; designing and hardening Agentic Web Interfaces; and
achieving resilience in multi-agent and federated environments. Our work
provides a comprehensive reference to guide the design of robust defense
mechanisms and establish best practices for resilient LLM-agent workflows.

</details>


### [277] [Securing AI Systems: A Guide to Known Attacks and Impacts](https://arxiv.org/abs/2506.23296)
*Naoto Kiribuchi,Kengo Zenitani,Takayuki Semitsu*

Main category: cs.CR

TL;DR: 本文概述预测和生成式AI系统的对抗攻击，列出十一种攻击类型，关联攻击技术与影响，助力相关人员提升AI系统安全。


<details>
  <summary>Details</summary>
Motivation: 解决嵌入式AI面临利用特定漏洞的安全威胁问题，为缺乏专业知识的人员提供基础安全知识。

Method: 识别十一种主要攻击类型，将攻击技术与对CIA安全三元组的影响进行关联。

Result: 明确了十一种攻击类型及其对CIA安全三元组的影响。

Conclusion: 能帮助研究、开发、安全和政策制定人员识别AI特定风险并实施有效防御，提升AI系统整体安全态势。

Abstract: Embedded into information systems, artificial intelligence (AI) faces
security threats that exploit AI-specific vulnerabilities. This paper provides
an accessible overview of adversarial attacks unique to predictive and
generative AI systems. We identify eleven major attack types and explicitly
link attack techniques to their impacts -- including information leakage,
system compromise, and resource exhaustion -- mapped to the confidentiality,
integrity, and availability (CIA) security triad. We aim to equip researchers,
developers, security practitioners, and policymakers, even those without
specialized AI security expertise, with foundational knowledge to recognize
AI-specific risks and implement effective defenses, thereby enhancing the
overall security posture of AI systems.

</details>


### [278] [Interpretable by Design: MH-AutoML for Transparent and Efficient Android Malware Detection without Compromising Performance](https://arxiv.org/abs/2506.23314)
*Joner Assolin,Gabriel Canto,Diego Kreutz,Eduardo Feitosa,Hendrio Bragança,Angelo Nogueira,Vanderson Rocha*

Main category: cs.CR

TL;DR: 提出针对安卓恶意软件检测的MH - AutoML框架，与7个现有AutoML框架对比，结果显示其召回率高、透明度和可控性好且计算效率相当。


<details>
  <summary>Details</summary>
Motivation: 现有AutoML解决方案存在透明度、可解释性和实验可追溯性不足的问题，需为安卓恶意软件检测开发合适框架。

Method: 提出MH - AutoML框架，自动化整个ML流程，具备可解释性、调试和实验跟踪能力，并与7个现有AutoML框架对比。

Result: MH - AutoML召回率更好，有更高透明度和可控性，计算效率与其他解决方案相当。

Conclusion: MH - AutoML适合性能和可解释性都重要的网络安全应用。

Abstract: Malware detection in Android systems requires both cybersecurity expertise
and machine learning (ML) techniques. Automated Machine Learning (AutoML) has
emerged as an approach to simplify ML development by reducing the need for
specialized knowledge. However, current AutoML solutions typically operate as
black-box systems with limited transparency, interpretability, and experiment
traceability. To address these limitations, we present MH-AutoML, a
domain-specific framework for Android malware detection. MH-AutoML automates
the entire ML pipeline, including data preprocessing, feature engineering,
algorithm selection, and hyperparameter tuning. The framework incorporates
capabilities for interpretability, debugging, and experiment tracking that are
often missing in general-purpose solutions. In this study, we compare MH-AutoML
against seven established AutoML frameworks: Auto-Sklearn, AutoGluon, TPOT,
HyperGBM, Auto-PyTorch, LightAutoML, and MLJAR. Results show that MH-AutoML
achieves better recall rates while providing more transparency and control. The
framework maintains computational efficiency comparable to other solutions,
making it suitable for cybersecurity applications where both performance and
explainability matter.

</details>


### [279] [SoK: Semantic Privacy in Large Language Models](https://arxiv.org/abs/2506.23603)
*Baihe Ma,Yanna Jiang,Xu Wang,Guangshen Yu,Qin Wang,Caijun Sun,Chen Li,Xuelei Qi,Ying He,Wei Ni,Ren Ping Liu*

Main category: cs.CR

TL;DR: 文章引入以生命周期为中心的框架分析大语言模型语义隐私风险，评估现有防御措施，指出语义级保护缺口并列出开放挑战。


<details>
  <summary>Details</summary>
Motivation: 传统数据隐私措施不足以保护大语言模型中的语义隐私，需研究语义隐私保护。

Method: 引入以生命周期为中心的框架分析LLMs不同阶段语义隐私风险，对攻击向量分类并评估现有防御措施。

Result: 分析发现语义级保护存在关键缺口，尤其在上下文推理和潜在表示泄露方面。

Conclusion: 列出量化语义泄露、保护多模态输入等开放挑战，旨在为未来设计语义感知隐私保护技术提供参考。

Abstract: As Large Language Models (LLMs) are increasingly deployed in sensitive
domains, traditional data privacy measures prove inadequate for protecting
information that is implicit, contextual, or inferable - what we define as
semantic privacy. This Systematization of Knowledge (SoK) introduces a
lifecycle-centric framework to analyze how semantic privacy risks emerge across
input processing, pretraining, fine-tuning, and alignment stages of LLMs. We
categorize key attack vectors and assess how current defenses, such as
differential privacy, embedding encryption, edge computing, and unlearning,
address these threats. Our analysis reveals critical gaps in semantic-level
protection, especially against contextual inference and latent representation
leakage. We conclude by outlining open challenges, including quantifying
semantic leakage, protecting multimodal inputs, balancing de-identification
with generation quality, and ensuring transparency in privacy enforcement. This
work aims to inform future research on designing robust, semantically aware
privacy-preserving techniques for LLMs.

</details>


### [280] [gMBA: Expression Semantic Guided Mixed Boolean-Arithmetic Deobfuscation Using Transformer Architectures](https://arxiv.org/abs/2506.23634)
*Youjeong Noh,Joon-Young Paik,Jingun Kwon,Eun-Sun Cho*

Main category: cs.CR

TL;DR: 传统MBA混淆去除方法忽视内部语义信息，本文提出真值表和gMBA框架，实验表明结合语义可提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统MBA去混淆方法忽略表达式内部语义信息，且MBA被恶意软件开发者利用，需要更好的去混淆方法。

Method: 提出自动构建的真值表作为表达式行为的语义表示，修改基于Transformer的神经编解码器Seq2Seq架构，提出通用可扩展的引导式MBA去混淆框架gMBA。

Result: 结合表达式语义能显著提升性能。

Conclusion: 内部语义表达式在将混淆代码恢复为原始形式中非常重要。

Abstract: Mixed Boolean-Arithmetic (MBA) obfuscation protects intellectual property by
converting programs into forms that are more complex to analyze. However, MBA
has been increasingly exploited by malware developers to evade detection and
cause significant real-world problems. Traditional MBA deobfuscation methods
often consider these expressions as part of a black box and overlook their
internal semantic information. To bridge this gap, we propose a truth table,
which is an automatically constructed semantic representation of an
expression's behavior that does not rely on external resources. The truth table
is a mathematical form that represents the output of expression for all
possible combinations of input. We also propose a general and extensible guided
MBA deobfuscation framework (gMBA) that modifies a Transformer-based neural
encoder-decoder Seq2Seq architecture to incorporate this semantic guidance.
Experimental results and in-depth analysis show that integrating expression
semantics significantly improves performance and highlights the importance of
internal semantic expressions in recovering obfuscated code to its original
form.

</details>


### [281] [Differentially Private Synthetic Data Release for Topics API Outputs](https://arxiv.org/abs/2506.23855)
*Travis Dick,Alessandro Epasto,Adel Javanmard,Josh Karlin,Andres Munoz Medina,Vahab Mirrokni,Sergei Vassilvitskii,Peilin Zhong*

Main category: cs.CR

TL;DR: 本文针对隐私保护广告API缺乏公开数据问题，开发新方法构建合成API输出，聚焦Topics API生成差分隐私数据集，还开源数据集以促进隐私属性研究。


<details>
  <summary>Details</summary>
Motivation: 隐私保护广告API隐私属性实证研究缺乏公开数据，而可靠实证分析需现实API输出数据集，隐私问题又阻碍数据公开。

Method: 先计算大量描述API输出轨迹随时间演变的差分隐私统计信息，再设计API轨迹序列的参数化分布并优化参数使其匹配统计信息，最后从该分布中抽样创建合成数据。

Result: 开发出构建合成API输出的新方法，生成与真实Topics API数据重识别风险属性相近的差分隐私数据集，并开源了匿名数据集。

Conclusion: 该工作有助于促进隐私保护广告API隐私属性研究的透明度。

Abstract: The analysis of the privacy properties of Privacy-Preserving Ads APIs is an
area of research that has received strong interest from academics, industry,
and regulators. Despite this interest, the empirical study of these methods is
hindered by the lack of publicly available data. Reliable empirical analysis of
the privacy properties of an API, in fact, requires access to a dataset
consisting of realistic API outputs; however, privacy concerns prevent the
general release of such data to the public.
  In this work, we develop a novel methodology to construct synthetic API
outputs that are simultaneously realistic enough to enable accurate study and
provide strong privacy protections. We focus on one Privacy-Preserving Ads
APIs: the Topics API, part of Google Chrome's Privacy Sandbox. We developed a
methodology to generate a differentially-private dataset that closely matches
the re-identification risk properties of the real Topics API data. The use of
differential privacy provides strong theoretical bounds on the leakage of
private user information from this release.
  Our methodology is based on first computing a large number of
differentially-private statistics describing how output API traces evolve over
time. Then, we design a parameterized distribution over sequences of API traces
and optimize its parameters so that they closely match the statistics obtained.
Finally, we create the synthetic data by drawing from this distribution.
  Our work is complemented by an open-source release of the anonymized dataset
obtained by this methodology. We hope this will enable external researchers to
analyze the API in-depth and replicate prior and future work on a realistic
large-scale dataset. We believe that this work will contribute to fostering
transparency regarding the privacy properties of Privacy-Preserving Ads APIs.

</details>


### [282] [RawMal-TF: Raw Malware Dataset Labeled by Type and Family](https://arxiv.org/abs/2506.23909)
*David Bálik,Martin Jureček,Mark Stamp*

Main category: cs.CR

TL;DR: 本文通过构建新数据集解决恶意软件分类问题，评估不同分类任务，显示标签粒度与计算成本的权衡，为后续研究奠基。


<details>
  <summary>Details</summary>
Motivation: 利用机器学习解决恶意软件分类难题，构建类型和家族两级标注的数据集。

Method: 从多源收集原始二进制文件，标注类型和家族信息，用静态分析统一特征提取流程，聚焦三类分类任务评估。

Result: 二分类中，随机森林和XGBoost准确率高；截断数据集下仍表现良好；类间分类和多分类中各模型有不同表现。

Conclusion: 类型和家族两级标注能实现更细粒度的恶意软件分类，为后续研究提供坚实基础。

Abstract: This work addresses the challenge of malware classification using machine
learning by developing a novel dataset labeled at both the malware type and
family levels. Raw binaries were collected from sources such as VirusShare, VX
Underground, and MalwareBazaar, and subsequently labeled with family
information parsed from binary names and type-level labels integrated from
ClarAVy. The dataset includes 14 malware types and 17 malware families, and was
processed using a unified feature extraction pipeline based on static analysis,
particularly extracting features from Portable Executable headers, to support
advanced classification tasks. The evaluation was focused on three key
classification tasks. In the binary classification of malware versus benign
samples, Random Forest and XGBoost achieved high accuracy on the full datasets,
reaching 98.5% for type-based detection and 98.98% for family-based detection.
When using truncated datasets of 1,000 samples to assess performance under
limited data conditions, both models still performed strongly, achieving 97.6%
for type-based detection and 98.66% for family-based detection. For interclass
classification, which distinguishes between malware types or families, the
models reached up to 97.5% accuracy on type-level tasks and up to 93.7% on
family-level tasks. In the multiclass classification setting, which assigns
samples to the correct type or family, SVM achieved 81.1% accuracy on type
labels, while Random Forest and XGBoost reached approximately 73.4% on family
labels. The results highlight practical trade-offs between accuracy and
computational cost, and demonstrate that labeling at both the type and family
levels enables more fine-grained and insightful malware classification. The
work establishes a robust foundation for future research on advanced malware
detection and classification.

</details>


### [283] [Logit-Gap Steering: Efficient Short-Suffix Jailbreaks for Aligned Large Language Models](https://arxiv.org/abs/2506.24056)
*Tung-Ling Li,Hongliang Liu*

Main category: cs.CR

TL;DR: 介绍了logit - gap steering越狱框架，快速且高效，能提升攻击成功率，还可探测安全调整对内部表征的影响。


<details>
  <summary>Details</summary>
Motivation: 开发一种快速的针对RLHF对齐语言模型的越狱框架。

Method: 将RLHF对齐语言模型的拒绝 - 肯定差距转化为对词汇表的单次遍历，用可前向计算的分数融合差距缩小、KL惩罚和奖励转移的轻量级代理，进行“排序 - 求和 - 停止”扫描。

Result: 能在不到一秒内完成扫描并返回短后缀，模型调用次数比波束或梯度攻击少两个数量级，相同后缀可泛化到未见提示，将单次攻击成功率从基线水平提升到80 - 100%，同时保持主题连贯性。

Conclusion: 该框架不仅高效，还能暴露句子边界奖励悬崖等对齐伪影，可用于轻量级探测安全调整如何重塑内部表征。

Abstract: We introduce logit-gap steering, a fast jailbreak framework that casts the
refusal-affirmation gap of RLHF-aligned language models as a single pass over
the vocabulary. A forward-computable score blends gap reduction with
lightweight proxies for KL penalty and reward shift, allowing a "sort-sum-stop"
sweep to complete in under a second and return a short suffix--two orders of
magnitude fewer model calls than beam or gradient attacks. The same suffix
generalises to unseen prompts and scales from 0.5 B to 70 B checkpoints,
lifting one-shot attack success from baseline levels to 80-100% while
preserving topical coherence. Beyond efficiency, these suffixes expose
sentence-boundary reward cliffs and other alignment artefacts, offering a
lightweight probe into how safety tuning reshapes internal representations.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [284] [Static Nuel Games with Terminal Payoff](https://arxiv.org/abs/2409.01681)
*S. Mastrakoulis,Ath. Kehagias*

Main category: cs.DM

TL;DR: 研究N个玩家轮流进行的Nuel游戏变体，证明有静态纳什均衡并给出计算算法


<details>
  <summary>Details</summary>
Motivation: 研究N个玩家轮流进行的Nuel游戏变体的特性

Method: 对游戏规则分析并进行数学证明

Result: 证明对于N≥2，Nuel游戏有静态纳什均衡

Conclusion: 给出计算静态纳什均衡的算法

Abstract: In this paper we study a variant of the Nuel game (a generalization of the
duel) which is played in turns by $N$ players. In each turn a single player
must fire at one of the other players and has a certain probability of hitting
and killing his target. The players shoot in a fixed sequence and when a player
is eliminated, the ``move'' passes to the next surviving player. The winner is
the last surviving player. We prove that, for every $N\geq2$, the Nuel has a
stationary Nash equilibrium and provide algorithms for its computation.

</details>


### [285] [A Graph Width Perspective on Partially Ordered Hamiltonian Paths and Cycles I: Treewidth, Pathwidth, and Grid Graphs](https://arxiv.org/abs/2506.23790)
*Jesse Beisegel,Katharina Klost,Kristin Knorr,Fabienne Ratajczak,Robert Scheffler*

Main category: cs.DM

TL;DR: 研究带顶点集偏序优先约束的哈密顿路径和循环问题，给出不同图宽度下的复杂度结果与多项式时间算法，研究有界高度矩形网格图的复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决带顶点集偏序优先约束的哈密顿路径和循环问题，明确不同图结构下问题的复杂度。

Method: 理论分析证明问题的NP完全性，设计多项式时间算法。

Result: 路径问题在路径宽度为4的图上NP完全，循环问题在路径宽度为5的图上NP完全；给出路径宽度3、树宽2的图上哈密顿路径和路径宽度4、树宽3的图上哈密顿循环的多项式时间算法；在有界高度矩形网格图上，路径和循环问题在高度分别大于等于7和9时NP完全，最小边权变体在高度5和6时NP难。

Conclusion: 明确了不同图结构（不同宽度图、有界高度矩形网格图）下带优先约束的哈密顿路径和循环问题的复杂度情况。

Abstract: We consider the problem of finding a Hamiltonian path or a Hamiltonian cycle
with precedence constraints in the form of a partial order on the vertex set.
We show that the path problem is $\mathsf{NP}$-complete for graphs of pathwidth
4 while the cycle problem is $\mathsf{NP}$-complete on graphs of pathwidth 5.
We complement these results by giving polynomial-time algorithms for graphs of
pathwidth 3 and treewidth 2 for Hamiltonian paths as well as pathwidth 4 and
treewidth 3 for Hamiltonian cycles. Furthermore, we study the complexity of the
path and cycle problems on rectangular grid graphs of bounded height. For
these, we show that the path and cycle problems are $\mathsf{NP}$-complete when
the height of the grid is greater or equal to 7 and 9, respectively. In the
variant where we look for minimum edge-weighted Hamiltonian paths and cycles,
the problems are $\mathsf{NP}$-hard for heights 5 and 6, respectively.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [286] [Verifying Properties of Index Arrays in a Purely-Functional Data-Parallel Language](https://arxiv.org/abs/2506.23058)
*Nikolaj Hey Hinnerskov,Robert Schenck,Cosmin E. Oancea*

Main category: cs.PL

TL;DR: 提出自动验证纯数据并行程序属性的新方法，在Futhark实现，7个应用验证平均用时1秒，消除GPU程序动态验证可加速。


<details>
  <summary>Details</summary>
Motivation: 自动验证纯数据并行程序带有非线性索引的属性。

Method: 将数组表示为索引函数，将属性蒸馏为代数不等式并交给基于Fourier - Motzkin的求解器证明。

Result: 在Futhark实现系统，7个应用平均验证时间1秒，两个案例显示消除GPU程序动态验证有显著加速。

Conclusion: 该框架实用且属性选择实用，不仅能验证程序正确性，还可用于编译器优化。

Abstract: This paper presents a novel approach to automatically verify properties of
pure data-parallel programs with non-linear indexing -- expressed as pre- and
post-conditions on functions. Programs consist of nests of second-order array
combinators (e.g., map, scan, and scatter) and loops. The key idea is to
represent arrays as index functions: programs are index function
transformations over which properties are propagated and inferred. Our
framework proves properties on index functions by distilling them into
algebraic (in)equalities and discharging them to a Fourier-Motzkin-based
solver. The framework is practical and accessible: properties are not
restricted to a decidable logic, but instead are carefully selected to express
practically useful guarantees that can be automatically reasoned about and
inferred. These guarantees extend beyond program correctness and can be
exploited by the entire compiler pipeline for optimization. We implement our
system in the pure data-parallel language Futhark and demonstrate its
practicality on seven applications, reporting an average verification time of 1
second. Two case studies show how eliminating dynamic verification in GPU
programs results in significant speedups.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [287] [FedCLAM: Client Adaptive Momentum with Foreground Intensity Matching for Federated Medical Image Segmentation](https://arxiv.org/abs/2506.22580)
*Vasilis Siomos,Jonathan Passerat-Palmbach,Giacomo Tarroni*

Main category: eess.IV

TL;DR: 本文提出FedCLAM解决医学成像联合学习中机构间特征差异问题，在医学分割任务上超越8种先进方法。


<details>
  <summary>Details</summary>
Motivation: 联合学习中机构间特征差异影响医学成像全球模型效果，现有聚合方法难以适应不同情况。

Method: 提出FedCLAM，集成客户端自适应动量项和个性化衰减因子，引入强度对齐损失。

Result: 在两个数据集上的广泛评估表明，FedCLAM在医学分割任务上超越了8种先进方法。

Conclusion: FedCLAM在解决医学成像联合学习的特征差异问题上有效，代码开源。

Abstract: Federated learning is a decentralized training approach that keeps data under
stakeholder control while achieving superior performance over isolated
training. While inter-institutional feature discrepancies pose a challenge in
all federated settings, medical imaging is particularly affected due to diverse
imaging devices and population variances, which can diminish the global model's
effectiveness. Existing aggregation methods generally fail to adapt across
varied circumstances. To address this, we propose FedCLAM, which integrates
\textit{client-adaptive momentum} terms derived from each client's loss
reduction during local training, as well as a \textit{personalized dampening
factor} to curb overfitting. We further introduce a novel \textit{intensity
alignment} loss that matches predicted and ground-truth foreground
distributions to handle heterogeneous image intensity profiles across
institutions and devices. Extensive evaluations on two datasets show that
FedCLAM surpasses eight cutting-edge methods in medical segmentation tasks,
underscoring its efficacy. The code is available at
https://github.com/siomvas/FedCLAM.

</details>


### [288] [High Resolution Isotropic 3D Cine imaging with Automated Segmentation using Concatenated 2D Real-time Imaging and Deep Learning](https://arxiv.org/abs/2506.22532)
*Mark Wrobel,Michele Pascale,Tina Yao,Ruaraidh Campbell,Elena Milano,Michael Quail,Jennifer Steeden,Vivek Muthurangu*

Main category: eess.IV

TL;DR: 本文提出用深度学习将2D自由呼吸实时电影图像拼接成各向同性且完全分割的3D电影数据集，在10名患者中验证，结果良好，能加速临床CMR检查。


<details>
  <summary>Details</summary>
Motivation: 传统儿科和先天性心脏病心血管磁共振成像方法有局限，旨在用深度学习将2D自由呼吸实时电影图像拼接成3D电影数据集。

Method: 训练四个深度学习模型，对开源数据进行片间对比度校正、片间呼吸运动校正、超分辨率和分割，在10名患者前瞻性采集的矢状位实时电影图像堆栈上验证。

Result: 实时数据成功转换为3D电影，后处理时间<1分钟，左、右心室指标无显著偏差，血管直径大多合理一致，仅右肺动脉直径有小的高估。

Conclusion: 用深度学习模型从2D实时电影图像创建3D电影数据集有潜力，采集和重建时间短，与传统成像一致性好，可加速临床CMR检查。

Abstract: Background: Conventional cardiovascular magnetic resonance (CMR) in
paediatric and congenital heart disease uses 2D, breath-hold, balanced steady
state free precession (bSSFP) cine imaging for assessment of function and
cardiac-gated, respiratory-navigated, static 3D bSSFP whole-heart imaging for
anatomical assessment. Our aim is to concatenate a stack 2D free-breathing
real-time cines and use Deep Learning (DL) to create an isotropic a fully
segmented 3D cine dataset from these images. Methods: Four DL models were
trained on open-source data that performed: a) Interslice contrast correction;
b) Interslice respiratory motion correction; c) Super-resolution (slice
direction); and d) Segmentation of right and left atria and ventricles (RA, LA,
RV, and LV), thoracic aorta (Ao) and pulmonary arteries (PA). In 10 patients
undergoing routine cardiovascular examination, our method was validated on
prospectively acquired sagittal stacks of real-time cine images. Quantitative
metrics (ventricular volumes and vessel diameters) and image quality of the 3D
cines were compared to conventional breath hold cine and whole heart imaging.
Results: All real-time data were successfully transformed into 3D cines with a
total post-processing time of <1 min in all cases. There were no significant
biases in any LV or RV metrics with reasonable limits of agreement and
correlation. There is also reasonable agreement for all vessel diameters,
although there was a small but significant overestimation of RPA diameter.
Conclusion: We have demonstrated the potential of creating a 3D-cine data from
concatenated 2D real-time cine images using a series of DL models. Our method
has short acquisition and reconstruction times with fully segmented data being
available within 2 minutes. The good agreement with conventional imaging
suggests that our method could help to significantly speed up CMR in clinical
practice.

</details>


### [289] [CRISP-SAM2: SAM2 with Cross-Modal Interaction and Semantic Prompting for Multi-Organ Segmentation](https://arxiv.org/abs/2506.23121)
*Xinlei Yu,Chanmiao Wang,Hui Jin,Ahmed Elazab,Gangyong Jia,Xiang Wan,Changqing Zou,Ruiquan Ge*

Main category: eess.IV

TL;DR: 文章提出基于SAM2的CRISP - SAM2模型用于多器官医学分割，在七个公开数据集实验中表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前多器官分割模型存在细节不准确、依赖几何提示和空间信息丢失等问题，需新的解决方法。

Method: 采用渐进式跨注意力交互机制将视觉和文本输入转换为跨模态上下文语义注入图像编码器；用语义提示策略替代原提示编码器；应用记忆的相似度排序自更新策略和掩码细化过程。

Result: 在七个公开数据集的对比实验中，CRISP - SAM2模型表现优于现有模型。

Conclusion: CRISP - SAM2模型有效，尤其在解决现有模型局限方面表现出色，代码已公开。

Abstract: Multi-organ medical segmentation is a crucial component of medical image
processing, essential for doctors to make accurate diagnoses and develop
effective treatment plans. Despite significant progress in this field, current
multi-organ segmentation models often suffer from inaccurate details,
dependence on geometric prompts and loss of spatial information. Addressing
these challenges, we introduce a novel model named CRISP-SAM2 with CRoss-modal
Interaction and Semantic Prompting based on SAM2. This model represents a
promising approach to multi-organ medical segmentation guided by textual
descriptions of organs. Our method begins by converting visual and textual
inputs into cross-modal contextualized semantics using a progressive
cross-attention interaction mechanism. These semantics are then injected into
the image encoder to enhance the detailed understanding of visual information.
To eliminate reliance on geometric prompts, we use a semantic prompting
strategy, replacing the original prompt encoder to sharpen the perception of
challenging targets. In addition, a similarity-sorting self-updating strategy
for memory and a mask-refining process is applied to further adapt to medical
imaging and enhance localized details. Comparative experiments conducted on
seven public datasets indicate that CRISP-SAM2 outperforms existing models.
Extensive analysis also demonstrates the effectiveness of our method, thereby
confirming its superior performance, especially in addressing the limitations
mentioned earlier. Our code is available at:
https://github.com/YU-deep/CRISP\_SAM2.git.

</details>


### [290] [CA-Diff: Collaborative Anatomy Diffusion for Brain Tissue Segmentation](https://arxiv.org/abs/2506.22882)
*Qilong Xing,Zikai Song,Yuteng Ye,Yuke Chen,Youjia Zhang,Na Feng,Junqing Yu,Wei Yang*

Main category: eess.IV

TL;DR: 提出CA - Diff框架用于脑MRI结构分割，结合空间解剖特征，实验显示其优于SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有CNN、transformer及扩散模型在脑MRI结构分割中存在难以准确分割复杂结构、忽视解剖信息的问题。

Method: 提出CA - Diff框架，引入距离场作为辅助解剖条件，采用协作扩散过程，引入一致性损失，设计时间自适应通道注意力模块。

Result: CA - Diff在大量实验中优于SOTA方法。

Conclusion: CA - Diff框架能有效利用解剖特征，提高脑MRI结构分割的准确性。

Abstract: Segmentation of brain structures from MRI is crucial for evaluating brain
morphology, yet existing CNN and transformer-based methods struggle to
delineate complex structures accurately. While current diffusion models have
shown promise in image segmentation, they are inadequate when applied directly
to brain MRI due to neglecting anatomical information. To address this, we
propose Collaborative Anatomy Diffusion (CA-Diff), a framework integrating
spatial anatomical features to enhance segmentation accuracy of the diffusion
model. Specifically, we introduce distance field as an auxiliary anatomical
condition to provide global spatial context, alongside a collaborative
diffusion process to model its joint distribution with anatomical structures,
enabling effective utilization of anatomical features for segmentation.
Furthermore, we introduce a consistency loss to refine relationships between
the distance field and anatomical structures and design a time adapted channel
attention module to enhance the U-Net feature fusion procedure. Extensive
experiments show that CA-Diff outperforms state-of-the-art (SOTA) methods.

</details>


### [291] [Score-based Diffusion Model for Unpaired Virtual Histology Staining](https://arxiv.org/abs/2506.23184)
*Anran Liu,Xiaofei Wang,Jing Cai,Chao Li*

Main category: eess.IV

TL;DR: 本文提出基于互信息引导分数的扩散模型用于非配对虚拟染色，实验证明其优于现有方法且有生物医学潜力。


<details>
  <summary>Details</summary>
Motivation: H&E染色缺乏诊断标记特异性，IHC染色受组织可用性和抗体特异性限制，现有虚拟染色方法存在挑战。

Method: 提出基于互信息（MI）引导分数的扩散模型，设计全局MI引导能量函数、时间步定制反向扩散过程和局部MI驱动对比学习策略。

Result: 大量实验表明该方法优于现有方法。

Conclusion: 该方法有生物医学应用潜力，代码待接收后开源。

Abstract: Hematoxylin and eosin (H&E) staining visualizes histology but lacks
specificity for diagnostic markers. Immunohistochemistry (IHC) staining
provides protein-targeted staining but is restricted by tissue availability and
antibody specificity. Virtual staining, i.e., computationally translating the
H&E image to its IHC counterpart while preserving the tissue structure, is
promising for efficient IHC generation. Existing virtual staining methods still
face key challenges: 1) effective decomposition of staining style and tissue
structure, 2) controllable staining process adaptable to diverse tissue and
proteins, and 3) rigorous structural consistency modelling to handle the
non-pixel-aligned nature of paired H&E and IHC images. This study proposes a
mutual-information (MI)-guided score-based diffusion model for unpaired virtual
staining. Specifically, we design 1) a global MI-guided energy function that
disentangles the tissue structure and staining characteristics across
modalities, 2) a novel timestep-customized reverse diffusion process for
precise control of the staining intensity and structural reconstruction, and 3)
a local MI-driven contrastive learning strategy to ensure the cellular level
structural consistency between H&E-IHC images. Extensive experiments
demonstrate the our superiority over state-of-the-art approaches, highlighting
its biomedical potential. Codes will be open-sourced upon acceptance.

</details>


### [292] [Federated Breast Cancer Detection Enhanced by Synthetic Ultrasound Image Augmentation](https://arxiv.org/abs/2506.23334)
*Hongyi Pan,Ziliang Hong,Gorkem Durak,Ziyue Xu,Ulas Bagci*

Main category: eess.IV

TL;DR: 提出基于生成式AI的数据增强框架用于乳腺癌超声图像联邦学习诊断，实验表明适量合成图像可提升性能，过量则降低，凸显平衡样本比例重要性。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在乳腺癌诊断中因数据有限和分布不均导致的模型性能与泛化能力下降问题。

Method: 训练两类特定的深度卷积生成对抗网络，用三个公开数据集模拟联邦学习场景，采用FedAvg和FedProx作为基线算法。

Result: 适量合成图像使FedAvg平均AUC从0.9206提升到0.9237，FedProx从0.9429提升到0.9538，过量合成数据降低性能。

Conclusion: 基于生成式AI的数据增强能提升乳腺癌超声图像分类联邦学习结果。

Abstract: Federated learning (FL) has emerged as a promising paradigm for
collaboratively training deep learning models across institutions without
exchanging sensitive medical data. However, its effectiveness is often hindered
by limited data availability and non-independent, identically distributed data
across participating clients, which can degrade model performance and
generalization. To address these challenges, we propose a generative AI based
data augmentation framework that integrates synthetic image sharing into the
federated training process for breast cancer diagnosis via ultrasound images.
Specifically, we train two simple class-specific Deep Convolutional Generative
Adversarial Networks: one for benign and one for malignant lesions. We then
simulate a realistic FL setting using three publicly available breast
ultrasound image datasets: BUSI, BUS-BRA, and UDIAT. FedAvg and FedProx are
adopted as baseline FL algorithms. Experimental results show that incorporating
a suitable number of synthetic images improved the average AUC from 0.9206 to
0.9237 for FedAvg and from 0.9429 to 0.9538 for FedProx. We also note that
excessive use of synthetic data reduced performance, underscoring the
importance of maintaining a balanced ratio of real and synthetic samples. Our
findings highlight the potential of generative AI based data augmentation to
enhance FL results in the breast ultrasound image classification task.

</details>


### [293] [Physics informed guided diffusion for accelerated multi-parametric MRI reconstruction](https://arxiv.org/abs/2506.23311)
*Perla Mayo,Carolin M. Pirkl,Alin Achim,Bjoern Menze,Mohammad Golbabaee*

Main category: eess.IV

TL;DR: 提出MRF - DiPh方法用于多参数组织映射，实验显示其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决从高度加速的瞬态定量MRI采集中进行多参数组织映射的问题，提高重建准确性和物理模型一致性。

Method: 从近端分裂公式推导，引入预训练去噪扩散模型作为图像先验，重建时同时实施k空间测量一致性和Bloch响应模型两个物理约束。

Result: 在体内脑部扫描数据的数值实验中，MRF - DiPh优于深度学习和压缩感知MRF基线。

Conclusion: MRF - DiPh能提供更准确的参数图，更好地保留测量保真度和物理模型一致性，对解决医学成像中的逆问题至关重要。

Abstract: We introduce MRF-DiPh, a novel physics informed denoising diffusion approach
for multiparametric tissue mapping from highly accelerated, transient-state
quantitative MRI acquisitions like Magnetic Resonance Fingerprinting (MRF). Our
method is derived from a proximal splitting formulation, incorporating a
pretrained denoising diffusion model as an effective image prior to regularize
the MRF inverse problem. Further, during reconstruction it simultaneously
enforces two key physical constraints: (1) k-space measurement consistency and
(2) adherence to the Bloch response model. Numerical experiments on in-vivo
brain scans data show that MRF-DiPh outperforms deep learning and compressed
sensing MRF baselines, providing more accurate parameter maps while better
preserving measurement fidelity and physical model consistency-critical for
solving reliably inverse problems in medical imaging.

</details>


### [294] [UltraTwin: Towards Cardiac Anatomical Twin Generation from Multi-view 2D Ultrasound](https://arxiv.org/abs/2506.23490)
*Junxuan Yu,Yaofei Duan,Yuhao Huang,Yu Wang,Rongbo Ling,Weihao Luo,Ang Zhang,Jingxian Xu,Qiongying Ni,Yongsong Zhou,Binghan Li,Haoran Dou,Liping Liu,Yanfen Chu,Feng Geng,Zhe Sheng,Zhifeng Ding,Dingxin Zhang,Rui Huang,Yuhang Zhang,Xiaowei Xu,Tao Tan,Dong Ni,Zhongshan Gou,Xin Yang*

Main category: eess.IV

TL;DR: 提出UltraTwin框架从稀疏多视图2D超声获得心脏解剖孪生模型，有数据集构建、重建优化和拓扑约束三方面贡献，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 2D超声难以准确计算指标和观察3D结构，3D超声有分辨率低等问题，从2D图像构建心脏解剖孪生模型有挑战。

Method: 引入UltraTwin框架，构建严格配对的多视图2D超声和CT数据集及伪配对数据，采用粗到细方案进行分层重建优化，引入隐式自动编码器进行拓扑感知约束。

Result: UltraTwin与强大竞争对手相比能重建高质量解剖孪生模型。

Conclusion: UltraTwin推动了解剖孪生建模，在个性化心脏护理中有潜在应用。

Abstract: Echocardiography is routine for cardiac examination. However, 2D ultrasound
(US) struggles with accurate metric calculation and direct observation of 3D
cardiac structures. Moreover, 3D US is limited by low resolution, small field
of view and scarce availability in practice. Constructing the cardiac
anatomical twin from 2D images is promising to provide precise treatment
planning and clinical quantification. However, it remains challenging due to
the rare paired data, complex structures, and US noises. In this study, we
introduce a novel generative framework UltraTwin, to obtain cardiac anatomical
twin from sparse multi-view 2D US. Our contribution is three-fold. First,
pioneered the construction of a real-world and high-quality dataset containing
strictly paired multi-view 2D US and CT, and pseudo-paired data. Second, we
propose a coarse-to-fine scheme to achieve hierarchical reconstruction
optimization. Last, we introduce an implicit autoencoder for topology-aware
constraints. Extensive experiments show that UltraTwin reconstructs
high-quality anatomical twins versus strong competitors. We believe it advances
anatomical twin modeling for potential applications in personalized cardiac
care.

</details>


### [295] [Artificial Intelligence-assisted Pixel-level Lung (APL) Scoring for Fast and Accurate Quantification in Ultra-short Echo-time MRI](https://arxiv.org/abs/2506.23506)
*Bowen Xin,Rohan Hickey,Tamara Blake,Jin Jin,Claire E Wainwright,Thomas Benkert,Alto Stemmer,Peter Sly,David Coman,Jason Dowling*

Main category: eess.IV

TL;DR: 本文研究了新型人工智能辅助像素级肺部（APL）评分在囊性纤维化（CF）肺部磁共振成像（MRI）中的可行性，APL 评分更快更准，有临床应用潜力。


<details>
  <summary>Details</summary>
Motivation: 肺部 MRI 无电离辐射，在儿科疾病如 CF 中常优于 CT，但肺部 MRI 缺乏定量评分系统，需要为肺部 MRI 提供快速准确的量化方法。

Method: APL 评分包括图像加载、AI 肺部分割、肺边界切片采样、像素级注释、量化和报告 5 个阶段。

Result: APL 评分每位受试者需 8.2 分钟，比之前的网格级评分快两倍多，像素级评分更准确，且与网格级评分强相关。

Conclusion: 该工具可简化临床 UTE 肺部 MRI 工作流程，还可扩展到其他肺部 MRI 序列和疾病。

Abstract: Lung magnetic resonance imaging (MRI) with ultrashort echo-time (UTE)
represents a recent breakthrough in lung structure imaging, providing image
resolution and quality comparable to computed tomography (CT). Due to the
absence of ionising radiation, MRI is often preferred over CT in paediatric
diseases such as cystic fibrosis (CF), one of the most common genetic disorders
in Caucasians. To assess structural lung damage in CF imaging, CT scoring
systems provide valuable quantitative insights for disease diagnosis and
progression. However, few quantitative scoring systems are available in
structural lung MRI (e.g., UTE-MRI). To provide fast and accurate
quantification in lung MRI, we investigated the feasibility of novel Artificial
intelligence-assisted Pixel-level Lung (APL) scoring for CF. APL scoring
consists of 5 stages, including 1) image loading, 2) AI lung segmentation, 3)
lung-bounded slice sampling, 4) pixel-level annotation, and 5) quantification
and reporting. The results shows that our APL scoring took 8.2 minutes per
subject, which was more than twice as fast as the previous grid-level scoring.
Additionally, our pixel-level scoring was statistically more accurate
(p=0.021), while strongly correlating with grid-level scoring (R=0.973,
p=5.85e-9). This tool has great potential to streamline the workflow of UTE
lung MRI in clinical settings, and be extended to other structural lung MRI
sequences (e.g., BLADE MRI), and for other lung diseases (e.g.,
bronchopulmonary dysplasia).

</details>


### [296] [A Clinically-Grounded Two-Stage Framework for Renal CT Report Generation](https://arxiv.org/abs/2506.23584)
*Renjie Liang,Zhengkang Fan,Jinqian Pan,Chenkun Sun,Russell Terry,Jie Xu*

Main category: eess.IV

TL;DR: 本文提出两阶段框架从2D CT切片生成肾脏放射学报告，实验表明模型表现优于随机基线，证明了模块化特征报告生成的可行性。


<details>
  <summary>Details</summary>
Motivation: 医学影像和临床文档的复杂性使得从CT扫描生成放射学报告成为复杂任务，研究旨在解决此问题。

Method: 提出两阶段框架，先通过多任务学习模型提取结构化异常特征，再结合CT图像输入微调后的视觉语言模型生成报告句子。

Result: 模型在所有异常类型上均优于随机基线，生成报告能合理准确捕捉关键临床内容。

Conclusion: 突出了肾脏成像模块化、特征驱动报告生成的可行性，未来将扩展到3D CT并提高临床保真度。

Abstract: Generating radiology reports from CT scans remains a complex task due to the
nuanced nature of medical imaging and the variability in clinical
documentation. In this study, we propose a two-stage framework for generating
renal radiology reports from 2D CT slices. First, we extract structured
abnormality features using a multi-task learning model trained to identify
lesion attributes such as location, size, enhancement, and attenuation. These
extracted features are subsequently combined with the corresponding CT image
and fed into a fine-tuned vision-language model to generate natural language
report sentences aligned with clinical findings. We conduct experiments on a
curated dataset of renal CT studies with manually annotated
sentence-slice-feature triplets and evaluate performance using both
classification metrics and natural language generation metrics. Our results
demonstrate that the proposed model outperforms random baselines across all
abnormality types, and the generated reports capture key clinical content with
reasonable textual accuracy. This exploratory work highlights the feasibility
of modular, feature-informed report generation for renal imaging. Future
efforts will focus on extending this pipeline to 3D CT volumes and further
improving clinical fidelity in multimodal medical AI systems.

</details>


### [297] [Deep Learning-Based Semantic Segmentation for Real-Time Kidney Imaging and Measurements with Augmented Reality-Assisted Ultrasound](https://arxiv.org/abs/2506.23721)
*Gijs Luijten,Roberto Maria Scardigno,Lisle Faray de Paiva,Peter Hoyer,Jens Kleesiek,Domenico Buongiorno,Vitoantonio Bevilacqua,Jan Egger*

Main category: eess.IV

TL;DR: 文章提出基于深度学习语义分割实现实时自动肾脏体积测量，并结合增强现实技术改进超声检查，提出两种AR - DL辅助超声管道，评估其实时可行性和准确性，开源代码提升超声训练和诊断能力。


<details>
  <summary>Details</summary>
Motivation: 超声学习曲线陡峭，且需在屏幕和患者间转移注意力，传统肾脏体积测量耗时且易疲劳，为解决这些问题开展研究。

Method: 集成深度学习语义分割进行实时自动肾脏体积测量，结合增强现实技术，提出两种AR - DL辅助超声管道，使用开放肾脏数据集和开源分割模型评估。

Result: 评估了实时可行性和准确性，开发了包含模型实现、测量算法和Wi - Fi流式解决方案的开源GitHub管道。

Conclusion: 该方案增强了超声训练和诊断能力，尤其适用于床边护理场景。

Abstract: Ultrasound (US) is widely accessible and radiation-free but has a steep
learning curve due to its dynamic nature and non-standard imaging planes.
Additionally, the constant need to shift focus between the US screen and the
patient poses a challenge. To address these issues, we integrate deep learning
(DL)-based semantic segmentation for real-time (RT) automated kidney volumetric
measurements, which are essential for clinical assessment but are traditionally
time-consuming and prone to fatigue. This automation allows clinicians to
concentrate on image interpretation rather than manual measurements.
Complementing DL, augmented reality (AR) enhances the usability of US by
projecting the display directly into the clinician's field of view, improving
ergonomics and reducing the cognitive load associated with screen-to-patient
transitions. Two AR-DL-assisted US pipelines on HoloLens-2 are proposed: one
streams directly via the application programming interface for a wireless
setup, while the other supports any US device with video output for broader
accessibility. We evaluate RT feasibility and accuracy using the Open Kidney
Dataset and open-source segmentation models (nnU-Net, Segmenter, YOLO with
MedSAM and LiteMedSAM). Our open-source GitHub pipeline includes model
implementations, measurement algorithms, and a Wi-Fi-based streaming solution,
enhancing US training and diagnostics, especially in point-of-care settings.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [298] [Unsupervised Learning-Based Joint Resource Allocation and Beamforming Design for RIS-Assisted MISO-OFDMA Systems](https://arxiv.org/abs/2506.22448)
*Yu Ma,Xingyu Zhou,Xiao Li,Le Liang,Shi Jin*

Main category: eess.SP

TL;DR: 本文研究RIS辅助MISO - OFDMA系统下行传输，提出基于无监督学习的两阶段框架，性能好且运行时间短。


<details>
  <summary>Details</summary>
Motivation: 解决RIS辅助MISO - OFDMA系统中资源分配的挑战。

Method: 提出两阶段无监督学习框架，包含BeamNet和AllocationNet，采用最大比传输、注水法、量化和Gumbel - softmax技巧，定制损失和分阶段训练。

Result: 该方法达到SCA基线和速率的99.93%，仅用其0.036%的运行时间，且在不同信道和用户条件下保持鲁棒性。

Conclusion: 所提基于无监督学习的两阶段框架在RIS辅助MISO - OFDMA系统下行传输资源分配中效果良好，具有高效性和鲁棒性。

Abstract: Reconfigurable intelligent surfaces (RIS) are key enablers for 6G wireless
systems. This paper studies downlink transmission in an RIS-assisted MISO-OFDMA
system, addressing resource allocation challenges. A two-stage unsupervised
learning-based framework is proposed to jointly design RIS phase shifts, BS
beamforming, and resource block (RB) allocation. The framework includes
BeamNet, which predicts RIS phase shifts from CSI, and AllocationNet, which
allocates RBs using equivalent CSI derived from BeamNet outputs. Active
beamforming is implemented via maximum ratio transmission and water-filling. To
handle discrete constraints while ensuring differentiability, quantization and
the Gumbel-softmax trick are adopted. A customized loss and phased training
enhance performance under QoS constraints. Simulations show the method achieves
99.93% of the sum rate of the SCA baseline with only 0.036% of its runtime, and
it remains robust across varying channel and user conditions.

</details>


### [299] [A Complex UNet Approach for Non-Invasive Fetal ECG Extraction Using Single-Channel Dry Textile Electrodes](https://arxiv.org/abs/2506.22457)
*Iulia Orvas,Andrei Radu,Alessandra Galli,Ana Neacsu,Elisabetta Peri*

Main category: eess.SP

TL;DR: 本文介绍用AI技术从单通道干纺织电极记录中提取胎儿心电图（fECG）的新方法，在模拟和真实数据上均取得新的最优结果，推动无创自助fECG提取解决方案发展。


<details>
  <summary>Details</summary>
Motivation: 连续无创的孕期监测对减少潜在并发症至关重要，fECG是评估胎儿健康的有前景工具，但家庭监测用干纺织电极提取fECG信号面临噪声和运动伪影等挑战。

Method: 创建模拟腹部记录的新数据集，提出基于复值去噪网络Complex UNet的创新管道，处理频谱图的实部和虚部。

Result: 新方法在fECG提取和R波峰检测方面取得新的最优结果，能在所有评估设置中准确提取fECG形态。

Conclusion: 该方法首次有效从单通道干纺织电极记录中提取fECG信号，向完全无创和自助的fECG提取解决方案迈进重要一步。

Abstract: Continuous, non-invasive pregnancy monitoring is crucial for minimising
potential complications. The fetal electrocardiogram (fECG) represents a
promising tool for assessing fetal health beyond clinical environments.
Home-based monitoring necessitates the use of a minimal number of comfortable
and durable electrodes, such as dry textile electrodes. However, this setup
presents many challenges, including increased noise and motion artefacts, which
complicate the accurate extraction of fECG signals. To overcome these
challenges, we introduce a pioneering method for extracting fECG from
single-channel recordings obtained using dry textile electrodes using AI
techniques. We created a new dataset by simulating abdominal recordings,
including noise closely resembling real-world characteristics of in-vivo
recordings through dry textile electrodes, alongside mECG and fECG. To ensure
the reliability of the extracted fECG, we propose an innovative pipeline based
on a complex-valued denoising network, Complex UNet. Unlike previous approaches
that focused solely on signal magnitude, our method processes both real and
imaginary components of the spectrogram, addressing phase information and
preventing incongruous predictions. We evaluated our novel pipeline against
traditional, well-established approaches, on both simulated and real data in
terms of fECG extraction and R-peak detection. The results showcase that our
suggested method achieves new state-of-the-art results, enabling an accurate
extraction of fECG morphology across all evaluated settings. This method is the
first to effectively extract fECG signals from single-channel recordings using
dry textile electrodes, making a significant advancement towards a fully
non-invasive and self-administered fECG extraction solution.

</details>


### [300] [Heart rate and respiratory rate prediction from noisy real-world smartphone based on Deep Learning methods](https://arxiv.org/abs/2506.22460)
*Ibne Farabi Shihab*

Main category: eess.SP

TL;DR: 研究测试手机视频估计生命体征准确性，发现传统算法表现不佳，提出3D深度CNN新方法，误差大幅降低，建议用基于回归的深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于手机视频估计心率和呼吸率的研究多在实验室环境，需验证结果能否推广到日常生活。

Method: 收集111名参与者日常生活手机视频及真实心率和呼吸率标签，用传统算法评估后，提出新的3D深度CNN方法。

Result: 传统算法在日常生活视频中表现比之前报道差，新的3D深度CNN方法使心率估计误差降低68%，呼吸率降低75%。

Conclusion: 建议使用基于回归的深度学习方法来估计心率和呼吸率。

Abstract: Using mobile phone video of the fingertip as a data source for estimating
vital signs such as heart rate (HR) and respiratory rate (RR) during daily life
has long been suggested. While existing literature indicates that these
estimates are accurate to within several beats or breaths per minute, the data
used to draw these conclusions are typically collected in laboratory
environments under careful experimental control, and yet the results are
assumed to generalize to daily life. In an effort to test it, a team of
researchers collected a large dataset of mobile phone video recordings made
during daily life and annotated with ground truth HR and RR labels from N=111
participants. They found that traditional algorithm performance on the
fingerprint videos is worse than previously reported (7 times and 13 times
worse for RR and HR, respectively). Fortunately, recent advancements in deep
learning, especially in convolutional neural networks (CNNs), offer a promising
solution to improve this performance. This study proposes a new method for
estimating HR and RR using a novel 3D deep CNN, demonstrating a reduced error
in estimated HR by 68% and RR by 75%. These promising results suggest that
regressor-based deep learning approaches should be used in estimating HR and
RR.

</details>


### [301] [Machine Learning for Proactive Groundwater Management: Early Warning and Resource Allocation](https://arxiv.org/abs/2506.22461)
*Chuan Li,Ruoxuan Yang*

Main category: eess.SP

TL;DR: 开发机器学习管道预测地下水位类别，应用于法国数据集取得一定成果，为地下水监测提供可扩展框架。


<details>
  <summary>Details</summary>
Motivation: 传统地下水监测方法因数据稀疏、计算限制和输出延迟，有效监测面临挑战。

Method: 开发机器学习管道，利用AutoGluon自动化集成框架，结合气候数据、水文气象记录和地理特征，集成地理空间预处理、领域驱动特征工程和自动模型选择。

Result: 应用于法国大规模数据集，验证数据加权F_1分数0.927，时间独立测试数据0.67，场景评估显示对预警系统和水资源分配决策有实用价值。

Conclusion: 开源实现为国家地下水监测网络集成机器学习提供可扩展框架，实现更灵敏和数据驱动的水资源管理策略。

Abstract: Groundwater supports ecosystems, agriculture, and drinking water supplies
worldwide, yet effective monitoring remains challenging due to sparse data,
computational constraints, and delayed outputs from traditional approaches. We
develop a machine learning pipeline that predicts groundwater level categories
using climate data, hydro-meteorological records, and physiographic attributes
processed through AutoGluon's automated ensemble framework. Our approach
integrates geospatial preprocessing, domain-driven feature engineering, and
automated model selection to overcome conventional monitoring limitations.
Applied to a large-scale French dataset (n $>$ 3,440,000 observations from
1,500+ wells), the model achieves weighted F\_1 scores of 0.927 on validation
data and 0.67 on temporally distinct test data. Scenario-based evaluations
demonstrate practical utility for early warning systems and water allocation
decisions under changing climate conditions. The open-source implementation
provides a scalable framework for integrating machine learning into national
groundwater monitoring networks, enabling more responsive and data-driven water
management strategies.

</details>


### [302] [Privacy-aware IoT Fall Detection Services For Aging in Place](https://arxiv.org/abs/2506.22462)
*Abdallah Lakhdari,Jiajie Li,Amani Abusafia,Athman Bouguettaya*

Main category: eess.SP

TL;DR: 提出基于物联网的跌倒检测即服务（FDaaS）框架，用UWB雷达传感器和FD - GPT应对数据稀缺和隐私问题，实验达90.72%准确率和89.33%精度。


<details>
  <summary>Details</summary>
Motivation: 应对2050年预计达21亿的老年人口的跌倒检测需求，解决现有方法数据稀缺和隐私问题。

Method: 设计基于UWB雷达传感器的面向服务架构，用FD - GPT进行数据增强，开发协议收集老年日常活动和跌倒事件数据集。

Result: 实验结果显示该方法区分跌倒事件和日常活动的准确率达90.72%，精度达89.33%。

Conclusion: 所提出的FDaaS框架能有效准确地检测跌倒，助力老年人独立安全生活。

Abstract: Fall detection is critical to support the growing elderly population,
projected to reach 2.1 billion by 2050. However, existing methods often face
data scarcity challenges or compromise privacy. We propose a novel IoT-based
Fall Detection as a Service (FDaaS) framework to assist the elderly in living
independently and safely by accurately detecting falls. We design a
service-oriented architecture that leverages Ultra-wideband (UWB) radar sensors
as an IoT health-sensing service, ensuring privacy and minimal intrusion. We
address the challenges of data scarcity by utilizing a Fall Detection
Generative Pre-trained Transformer (FD-GPT) that uses augmentation techniques.
We developed a protocol to collect a comprehensive dataset of the elderly daily
activities and fall events. This resulted in a real dataset that carefully
mimics the elderly's routine. We rigorously evaluate and compare various models
using this dataset. Experimental results show our approach achieves 90.72%
accuracy and 89.33% precision in distinguishing between fall events and regular
activities of daily living.

</details>


### [303] [Dimensionality Reduction on IoT Monitoring Data of Smart Building for Energy Consumption Forecasting](https://arxiv.org/abs/2506.22468)
*Konstantinos Koutras,Agorakis Bompotas,Constantinos Halkiopoulos,Athanasios Kalogeras,Christos Alexakos*

Main category: eess.SP

TL;DR: 本文聚焦小型智能办公室物联网网络数据，通过假设检验分析环境变量与能耗相关性，可在不检查全量数据下排除弱相关变量并保持精度。


<details>
  <summary>Details</summary>
Motivation: 找出监测变量间统计相关性，以利用机器学习预测算法减少能耗输入参数。

Method: 对三组不同环境变量与能耗的相关性进行假设检验，每组变量进行30次，共90次测试。

Result: p值显示两个环境变量与能耗有强或半强相关性，另一个为弱相关性。

Conclusion: 所提方法可在不检查整个数据集的情况下排除弱相关变量，同时保持相同的准确性得分。

Abstract: The Internet of Things (IoT) plays a major role today in smart building
infrastructures, from simple smart-home applications, to more sophisticated
industrial type installations. The vast amounts of data generated from relevant
systems can be processed in different ways revealing important information.
This is especially true in the era of edge computing, when advanced data
analysis and decision-making is gradually moving to the edge of the network
where devices are generally characterised by low computing resources. In this
context, one of the emerging main challenges is related to maintaining data
analysis accuracy even with less data that can be efficiently handled by low
resource devices. The present work focuses on correlation analysis of data
retrieved from a pilot IoT network installation monitoring a small smart office
by means of environmental and energy consumption sensors. The research
motivation was to find statistical correlation between the monitoring variables
that will allow the use of machine learning (ML) prediction algorithms for
energy consumption reducing input parameters. For this to happen, a series of
hypothesis tests for the correlation of three different environmental variables
with the energy consumption were carried out. A total of ninety tests were
performed, thirty for each pair of variables. In these tests, p-values showed
the existence of strong or semi-strong correlation with two environmental
variables, and of a weak correlation with a third one. Using the proposed
methodology, we manage without examining the entire data set to exclude weak
correlated variables while keeping the same score of accuracy.

</details>


### [304] [Masked Autoencoders that Feel the Heart: Unveiling Simplicity Bias for ECG Analyses](https://arxiv.org/abs/2506.22495)
*He-Yang Xu,Hongxiang Gao,Yuwen Li,Xiu-Shen Wei,Chengyu Liu*

Main category: eess.SP

TL;DR: 本文指出心电图分析存在简单性偏差（SB），提出基于自监督学习（SSL）的方法缓解该偏差，构建大规模数据集，实验证明方法有效且性能达到最优。


<details>
  <summary>Details</summary>
Motivation: 监督式心电图模型存在简单性偏差，易忽略细粒度临床关键线索，影响诊断性能，需寻找缓解方法。

Method: 遵循自监督学习范式，提出包含时频感知滤波器和多粒度原型重建的方法，构建大规模多站点心电图数据集。

Result: 在六个心电图数据集的三个下游任务实验中，方法有效减少了简单性偏差，达到了最优性能。

Conclusion: 自监督学习可缓解心电图分析中的简单性偏差，所提方法有效，代码和数据集将公开。

Abstract: The diagnostic value of electrocardiogram (ECG) lies in its dynamic
characteristics, ranging from rhythm fluctuations to subtle waveform
deformations that evolve across time and frequency domains. However, supervised
ECG models tend to overfit dominant and repetitive patterns, overlooking
fine-grained but clinically critical cues, a phenomenon known as Simplicity
Bias (SB), where models favor easily learnable signals over subtle but
informative ones. In this work, we first empirically demonstrate the presence
of SB in ECG analyses and its negative impact on diagnostic performance, while
simultaneously discovering that self-supervised learning (SSL) can alleviate
it, providing a promising direction for tackling the bias. Following the SSL
paradigm, we propose a novel method comprising two key components: 1)
Temporal-Frequency aware Filters to capture temporal-frequency features
reflecting the dynamic characteristics of ECG signals, and 2) building on this,
Multi-Grained Prototype Reconstruction for coarse and fine representation
learning across dual domains, further mitigating SB. To advance SSL in ECG
analyses, we curate a large-scale multi-site ECG dataset with 1.53 million
recordings from over 300 clinical centers. Experiments on three downstream
tasks across six ECG datasets demonstrate that our method effectively reduces
SB and achieves state-of-the-art performance. Code and dataset will be released
publicly.

</details>


### [305] [Microelectrode Signal Dynamics as Biomarkers of Subthalamic Nucleus Entry on Deep Brain Stimulation: A Nonlinear Feature Approach](https://arxiv.org/abs/2506.22454)
*Ana Luiza S. Tavares,Artur Pedro M. Neto,Francinaldo L. Gomes,Paul Rodrigo dos Reis,Arthur G. da Silva,Antonio P. Junior,Bruno D. Gomes*

Main category: eess.SP

TL;DR: 本文提出定量框架，用非线性动力学和熵指标对STN内外神经活动分类，组合特征效果好，模型泛化能力强，凸显信号描述符在DBS手术实时决策的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前DBS电极植入时STN定位依赖信号特征主观解读，需准确的定位方法。

Method: 对三名患者MER数据预处理、分段和标注，提取多种特征，用分层10折交叉验证训练多个监督分类器并统计比较。

Result: 熵和非线性特征组合判别力最高，Extra Trees分类器最佳，交叉验证F1分数0.902±0.027，ROC AUC 0.887±0.055，测试集表现佳。

Conclusion: 非线性和熵信号描述符在DBS手术实时数据驱动决策有潜力。

Abstract: Accurate intraoperative localization of the subthalamic nucleus (STN) is
essential for the efficacy of Deep Brain Stimulation (DBS) in patients with
Parkinson's disease. While microelectrode recordings (MERs) provide rich
electrophysiological information during DBS electrode implantation, current
localization practices often rely on subjective interpretation of signal
features. In this study, we propose a quantitative framework that leverages
nonlinear dynamics and entropy-based metrics to classify neural activity
recorded inside versus outside the STN. MER data from three patients were
preprocessed using a robust artifact correction pipeline, segmented, and
labelled based on surgical annotations. A comprehensive set of recurrence
quantification analysis, nonlinear, and entropy features were extracted from
each segment. Multiple supervised classifiers were trained on every combination
of feature domains using stratified 10-fold cross-validation, followed by
statistical comparison using paired Wilcoxon signed-rank tests with
Holm-Bonferroni correction. The combination of entropy and nonlinear features
yielded the highest discriminative power, and the Extra Trees classifier
emerged as the best model with a cross-validated F1-score of 0.902+/-0.027 and
ROC AUC of 0.887+/-0.055. Final evaluation on a 20% hold-out test set confirmed
robust generalization (F1= 0.922, ROC AUC = 0.941). These results highlight the
potential of nonlinear and entropy signal descriptors in supporting real-time,
data-driven decision-making during DBS surgeries

</details>


### [306] [Data Normalization Strategies for EEG Deep Learning](https://arxiv.org/abs/2506.22455)
*Dung Truong,Arnaud Delorme*

Main category: eess.SP

TL;DR: 本文系统评估不同归一化策略对脑电深度学习监督和自监督任务的影响，发现最优策略因训练范式而异，强调任务特定归一化的必要性。


<details>
  <summary>Details</summary>
Motivation: 大规模预训练范式兴起，引发对脑电深度学习应用中最优归一化策略的疑问。

Method: 系统评估归一化粒度（记录级与窗口级）和范围（跨通道与通道内）对监督（年龄和性别预测）和自监督（对比预测编码）任务的影响，使用健康脑网络数据集中2836名受试者的高密度静息态脑电数据。

Result: 窗口级通道内归一化在监督任务中性能最佳，窗口级最小或跨通道归一化对自监督学习更有效。

Conclusion: 强调任务特定归一化选择的必要性，挑战通用归一化策略可跨学习设置泛化的假设，为开发稳健脑电深度学习管道提供实用见解。

Abstract: Normalization is a critical yet often overlooked component in the
preprocessing pipeline for EEG deep learning applications. The rise of
large-scale pretraining paradigms such as self-supervised learning (SSL)
introduces a new set of tasks whose nature is substantially different from
supervised training common in EEG deep learning applications. This raises new
questions about optimal normalization strategies for the applicable task. In
this study, we systematically evaluate the impact of normalization granularity
(recording vs. window level) and scope (cross-channel vs. within-channel) on
both supervised (age and gender prediction) and self-supervised (Contrastive
Predictive Coding) tasks. Using high-density resting-state EEG from 2,836
subjects in the Healthy Brain Network dataset, we show that optimal
normalization strategies differ significantly between training paradigms.
Window-level within-channel normalization yields the best performance in
supervised tasks, while minimal or cross-channel normalization at the window
level is more effective for SSL. These results underscore the necessity of
task-specific normalization choices and challenge the assumption that a
universal normalization strategy can generalize across learning settings. Our
findings provide practical insights for developing robust EEG deep learning
pipelines as the field shifts toward large-scale, foundation model training.

</details>


### [307] [Physics-Embedded Neural Networks for sEMG-based Continuous Motion Estimation](https://arxiv.org/abs/2506.22459)
*Wending Heng,Chaoyuan Liang,Yihui Zhao,Zhiqiang Zhang,Glen Cooper,Zhenhong Li*

Main category: eess.SP

TL;DR: 提出结合可解释MSK前向动力学和数据驱动残差学习的PENN模型用于sEMG运动估计，实验显示其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于sEMG的运动估计方法要么依赖难校准的特定受试者MSK模型，要么纯数据驱动模型缺乏生理一致性，需改进。

Method: 引入Physics - Embedded Neural Network (PENN)，采用递归时间结构传播历史估计，用轻量级卷积神经网络进行残差校正，设计两阶段训练策略。

Result: 在六名健康受试者上的实验评估表明，PENN在均方根误差（RMSE）和$R^2$指标上均优于最先进的基线方法。

Conclusion: PENN模型在保持生理一致性的同时能实现准确的运动估计，具有良好性能。

Abstract: Accurately decoding human motion intentions from surface electromyography
(sEMG) is essential for myoelectric control and has wide applications in
rehabilitation robotics and assistive technologies. However, existing
sEMG-based motion estimation methods often rely on subject-specific
musculoskeletal (MSK) models that are difficult to calibrate, or purely
data-driven models that lack physiological consistency. This paper introduces a
novel Physics-Embedded Neural Network (PENN) that combines interpretable MSK
forward-dynamics with data-driven residual learning, thereby preserving
physiological consistency while achieving accurate motion estimation. The PENN
employs a recursive temporal structure to propagate historical estimates and a
lightweight convolutional neural network for residual correction, leading to
robust and temporally coherent estimations. A two-phase training strategy is
designed for PENN. Experimental evaluations on six healthy subjects show that
PENN outperforms state-of-the-art baseline methods in both root mean square
error (RMSE) and $R^2$ metrics.

</details>


### [308] [An Interpretable Transformer-Based Foundation Model for Cross-Procedural Skill Assessment Using Raw fNIRS Signals](https://arxiv.org/abs/2506.22476)
*A. Subedi,S. De,L. Cavuoto,S. Schwaitzberg,M. Hackett,J. Norfleet*

Main category: eess.SP

TL;DR: 提出基于Transformer的基础模型，用最少处理的fNIRS信号进行跨程序技能评估，有高准确率、泛化性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有功能近红外光谱（fNIRS）评估认知 - 运动性能的方法有任务特定、需大量预处理和缺乏鲁棒性等问题，需要能跨任务、个体和实验环境的模型。

Method: 用自监督学习在腹腔镜手术任务和气管插管（ETI）数据上预训练基于Transformer的模型，使用针对fNIRS开发的通道注意力机制实现可解释性。

Result: 模型在所有任务上分类准确率超88%，ETI的Matthews相关系数超0.91；用少于30个标记样本和轻量级适配器模块泛化到新的紧急气道程序环甲膜切开术，AUC超87%。

Conclusion: 所提出的模型可用于跨程序技能评估，其可解释性机制能识别前额叶子网络，注意力模式能反映动态认知状态。

Abstract: Objective skill assessment in high-stakes procedural environments requires
models that not only decode underlying cognitive and motor processes but also
generalize across tasks, individuals, and experimental contexts. While prior
work has demonstrated the potential of functional near-infrared spectroscopy
(fNIRS) for evaluating cognitive-motor performance, existing approaches are
often task-specific, rely on extensive preprocessing, and lack robustness to
new procedures or conditions. Here, we introduce an interpretable
transformer-based foundation model trained on minimally processed fNIRS signals
for cross-procedural skill assessment. Pretrained using self-supervised
learning on data from laparoscopic surgical tasks and endotracheal intubation
(ETI), the model achieves greater than 88% classification accuracy on all
tasks, with Matthews Correlation Coefficient exceeding 0.91 on ETI. It
generalizes to a novel emergency airway procedure--cricothyrotomy--using fewer
than 30 labeled samples and a lightweight (less than 2k parameter) adapter
module, attaining an AUC greater than 87%. Interpretability is achieved via a
novel channel attention mechanism--developed specifically for fNIRS--that
identifies functionally coherent prefrontal sub-networks validated through
ablation studies. Temporal attention patterns align with task-critical phases
and capture stress-induced changes in neural variability, offering insight into
dynamic cognitive states.

</details>


### [309] [Zero-Shot EEG-to-Gait Decoding via Phase-Aware Representation Learning](https://arxiv.org/abs/2506.22488)
*Xi Fu,Weibang Jiang,Rui Liu,Gernot R. Müller-Putz,Cuntai Guan*

Main category: eess.SP

TL;DR: 提出NeuroDyGait框架用于EEG信号下肢运动解码，实现零样本预测，在跨主体步态解码表现优且有强相位检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有EEG信号下肢运动解码在因果、相位一致预测及建模主体间和主体内差异方面存在挑战。

Method: 采用相对对比学习实现EEG与运动嵌入语义对齐，引入多周期步态重建目标确保时间连贯性和生物力学一致性，微调时用领域动态解码机制促进跨会话泛化。

Result: NeuroDyGait能对未见过个体进行零样本运动预测，在基准数据集跨主体步态解码表现出色，无明确相位监督下有强相位检测能力。

Conclusion: 关系领域学习在实现BCI可扩展、无目标部署方面有潜力。

Abstract: Accurate decoding of lower-limb motion from EEG signals is essential for
advancing brain-computer interface (BCI) applications in movement intent
recognition and control. However, challenges persist in achieving causal,
phase-consistent predictions and in modeling both inter- and intra-subject
variability. To address these issues, we propose NeuroDyGait, a
domain-generalizable EEG-to-motion decoding framework that leverages structured
contrastive representation learning and relational domain modeling. The
proposed method employs relative contrastive learning to achieve semantic
alignment between EEG and motion embeddings. Furthermore, a multi-cycle gait
reconstruction objective is introduced to enforce temporal coherence and
maintain biomechanical consistency. To promote inter-session generalization,
during fine-tuning, a domain dynamic decoding mechanism adaptively assigns
session-specific prediction heads and learns to mix their outputs based on
inter-session relationships. NeuroDyGait enables zero-shot motion prediction
for unseen individuals without requiring adaptation and achieves superior
performance in cross-subject gait decoding on benchmark datasets. Additionally,
it demonstrates strong phase-detection capabilities even without explicit phase
supervision during training. These findings highlight the potential of
relational domain learning in enabling scalable, target-free deployment of
BCIs.

</details>


### [310] [MENGLAN: Multiscale Enhanced Nonparametric Gas Analyzer with Lightweight Architecture and Networks](https://arxiv.org/abs/2506.22490)
*Zhenke Duan,Jiqun Pan,Jiani Tu*

Main category: eess.SP

TL;DR: 研究提出MENGLAN气体分析仪用于乙烯浓度预测，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法成本高、复杂，限制其在混合气体中乙烯浓度检测的实际应用，需新方法。

Method: 提出MENGLAN，集成双流结构、混合多头注意力机制和特征再激活模块。

Result: MENGLAN性能优越，降低计算需求，提高可部署性。

Conclusion: MENGLAN能实现实时、轻量级、高精度的乙烯浓度预测，优于现有方法。

Abstract: Accurate detection of ethylene concentrations in mixed gases is crucial in
chemical production for safety and health purposes. Traditional methods are
hindered by high cost and complexity, limiting their practical application.
This study proposes MENGLAN, a Multiscale Enhanced Nonparametric Gas Analyzer
that integrates a dual-stream structure, a Hybrid Multi-Head Attention
mechanism, and a Feature Reactivation Module to enable real-time, lightweight,
and high-precision ethylene concentration prediction. Results show that MENGLAN
achieves superior performance, reduced computational demand, and enhanced
deployability compared to existing methods.

</details>


### [311] [Multi-Branch DNN and CRLB-Ratio-Weight Fusion for Enhanced DOA Sensing via a Massive H$^2$AD MIMO Receiver](https://arxiv.org/abs/2506.23203)
*Feng Shu,Jiatong Bai,Di Wu,Wei Zhu,Bin Deng,Fuhui Zhou,Jiangzhou Wang*

Main category: eess.SP

TL;DR: 提出轻量级CRLB - 比率 - 权重融合方法和多分支深度神经网络，提升6G无线网络中massive H²AD结构的DOA感知性能。


<details>
  <summary>Details</summary>
Motivation: 解决在6G无线网络massive H²AD结构中，以较少先验知识实现低复杂度、高性能的不同子阵组目标方向值融合的难题。

Method: 提出轻量级CRLB - 比率 - 权重融合方法，用天线数量倒数近似子阵逆CRLB；构建多分支深度神经网络，结合子阵特定分支网络和共享回归模块。

Result: CRLB - 比率 - 权重融合方法性能与基于CRLB的方法相当，减少对先验知识的依赖；多分支深度神经网络在低SNR范围性能优越，在SNR = -15 dB时，估计精度比CRLB - 比率 - 权重融合方法提高一个数量级。

Conclusion: 所提方法能有效提升DOA感知性能，在低SNR范围有更好表现。

Abstract: As a green MIMO structure, massive H$^2$AD is viewed as a potential
technology for the future 6G wireless network. For such a structure, it is a
challenging task to design a low-complexity and high-performance fusion of
target direction values sensed by different sub-array groups with fewer use of
prior knowledge. To address this issue, a lightweight Cramer-Rao lower bound
(CRLB)-ratio-weight fusion (WF) method is proposed, which approximates inverse
CRLB of each subarray using antenna number reciprocals to eliminate real-time
CRLB computation. This reduces complexity and prior knowledge dependence while
preserving fusion performance. Moreover, a multi-branch deep neural network
(MBDNN) is constructed to further enhance direction-of-arrival (DOA) sensing by
leveraging candidate angles from multiple subarrays. The subarray-specific
branch networks are integrated with a shared regression module to effectively
eliminate pseudo-solutions and fuse true angles. Simulation results show that
the proposed CRLB-ratio-WF method achieves DOA sensing performance comparable
to CRLB-based methods, while significantly reducing the reliance on prior
knowledge. More notably, the proposed MBDNN has superior performance in low-SNR
ranges. At SNR $= -15$ dB, it achieves an order-of-magnitude improvement in
estimation accuracy compared to CRLB-ratio-WF method.

</details>


### [312] [Differentiable Radar Ambiguity Functions: Mathematical Formulation and Computational Implementation](https://arxiv.org/abs/2506.22935)
*Marc Bara Iniesta*

Main category: eess.SP

TL;DR: 本文提出可微雷达模糊函数GRAF，解决技术挑战，实现与自动微分框架兼容，为雷达波形设计应用机器学习奠定基础。


<details>
  <summary>Details</summary>
Motivation: 传统模糊函数公式含不可微操作，无法与基于梯度的优化方法和现代机器学习框架集成。

Method: 使用Wirtinger微积分处理复值梯度，通过并行FFT操作高效计算，保证级联操作数值稳定性及与任意可微操作的可组合性，提出GRAF方法。

Result: 实现通用可微模糊函数，适用于现代自动微分框架，展示了计算效率，可用于神经网络波形生成等新研究方向。

Conclusion: 为雷达波形设计应用现代机器学习技术建立了数学和计算基础，连接经典雷达信号处理与自动微分框架。

Abstract: The ambiguity function is fundamental to radar waveform design,
characterizing range and Doppler resolution capabilities. However, its
traditional formulation involves non-differentiable operations, preventing
integration with gradient-based optimization methods and modern machine
learning frameworks. This paper presents the first complete mathematical
framework and computational implementation for differentiable radar ambiguity
functions. Our approach addresses the fundamental technical challenges that
have prevented the radar community from leveraging automatic differentiation:
proper handling of complex-valued gradients using Wirtinger calculus, efficient
computation through parallelized FFT operations, numerical stability throughout
cascaded operations, and composability with arbitrary differentiable
operations. We term this approach GRAF (Gradient-based Radar Ambiguity
Functions), which reformulates the ambiguity function computation to maintain
mathematical equivalence while enabling gradient flow through the entire
pipeline. The resulting implementation provides a general-purpose
differentiable ambiguity function compatible with modern automatic
differentiation frameworks, enabling new research directions including neural
network-based waveform generation with ambiguity constraints, end-to-end
optimization of radar systems, and integration of classical radar theory with
modern deep learning. We provide complete implementation details and
demonstrate computational efficiency suitable for practical applications. This
work establishes the mathematical and computational foundation for applying
modern machine learning techniques to radar waveform design, bridging classical
radar signal processing with automatic differentiation frameworks.

</details>


### [313] [Post-processing of EEG-based Auditory Attention Decoding Decisions via Hidden Markov Models](https://arxiv.org/abs/2506.24024)
*Nicolas Heintz,Tom Francart,Alexander Bertrand*

Main category: eess.SP

TL;DR: 本文提出用隐马尔可夫模型（HMM）增强听觉注意力解码（AAD）算法，能在因果和非因果设置中显著改善AAD算法，且优于现有后处理方法。


<details>
  <summary>Details</summary>
Motivation: 现有AAD算法在短时间窗口内识别结果不准确，无法实际应用。

Method: 用HMM对注意力的时间结构进行建模，利用受试者在任何时刻切换注意力的可能性低于持续关注同一说话者这一事实。

Result: HMM能显著改善现有AAD算法，在准确性和响应性上优于现有后处理方法，还探究了窗口长度、切换频率和AAD准确性等因素对整体性能的影响。

Conclusion: 所提出的方法计算效率高、使用直观，适用于实时和离线设置。

Abstract: Auditory attention decoding (AAD) algorithms exploit brain signals, such as
electroencephalography (EEG), to identify which speaker a listener is focusing
on in a multi-speaker environment. While state-of-the-art AAD algorithms can
identify the attended speaker on short time windows, their predictions are
often too inaccurate for practical use. In this work, we propose augmenting AAD
with a hidden Markov model (HMM) that models the temporal structure of
attention. More specifically, the HMM relies on the fact that a subject is much
less likely to switch attention than to keep attending the same speaker at any
moment in time. We show how a HMM can significantly improve existing AAD
algorithms in both causal (real-time) and non-causal (offline) settings. We
further demonstrate that HMMs outperform existing postprocessing approaches in
both accuracy and responsiveness, and explore how various factors such as
window length, switching frequency, and AAD accuracy influence overall
performance. The proposed method is computationally efficient, intuitive to use
and applicable in both real-time and offline settings.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [314] [Emergent musical properties of a transformer under contrastive self-supervised learning](https://arxiv.org/abs/2506.23873)
*Yuexuan Kong,Gabriel Meseguer-Brocal,Vincent Lostanlen,Mathieu Lagrange,Romain Hennequin*

Main category: cs.SD

TL;DR: 本文挑战对比自监督学习不适用于音乐局部任务的假设，展示其与Transformer结合在音乐信息检索局部任务中的潜力，推进对Transformer的音乐解释。


<details>
  <summary>Details</summary>
Motivation: 在音乐信息检索中，普遍认为对比自监督学习不适用于局部任务，本文旨在挑战该假设，挖掘其在局部任务中的潜力。

Method: 采用一维时域 - 频域的轻量级视觉Transformer（ViT - 1D），通过归一化温度缩放交叉熵损失（NT - Xent）进行简单对比自监督学习训练。

Result: 在全局任务中，类标记和序列标记的时间平均比仅使用类标记性能更好；在局部任务中，未专门训练的序列标记表现出色，层注意力图能提取如节拍等高级音乐特征，不同层捕获不同音乐维度。

Conclusion: 本文虽未聚焦于提升性能，但推进了对Transformer的音乐解释，揭示了对比自监督学习与Transformer结合在音乐序列建模中被忽视的能力。

Abstract: In music information retrieval (MIR), contrastive self-supervised learning
for general-purpose representation models is effective for global tasks such as
automatic tagging. However, for local tasks such as chord estimation, it is
widely assumed that contrastively trained general-purpose self-supervised
models are inadequate and that more sophisticated SSL is necessary; e.g.,
masked modeling. Our paper challenges this assumption by revealing the
potential of contrastive SSL paired with a transformer in local MIR tasks. We
consider a lightweight vision transformer with one-dimensional patches in the
time--frequency domain (ViT-1D) and train it with simple contrastive SSL
through normalized temperature-scaled cross-entropy loss (NT-Xent). Although
NT-Xent operates only over the class token, we observe that, potentially thanks
to weight sharing, informative musical properties emerge in ViT-1D's sequence
tokens. On global tasks, the temporal average of class and sequence tokens
offers a performance increase compared to the class token alone, showing useful
properties in the sequence tokens. On local tasks, sequence tokens perform
unexpectedly well, despite not being specifically trained for. Furthermore,
high-level musical features such as onsets emerge from layer-wise attention
maps and self-similarity matrices show different layers capture different
musical dimensions. Our paper does not focus on improving performance but
advances the musical interpretation of transformers and sheds light on some
overlooked abilities of contrastive SSL paired with transformers for sequence
modeling in MIR.

</details>


### [315] [Aria-MIDI: A Dataset of Piano MIDI Files for Symbolic Music Modeling](https://arxiv.org/abs/2504.15071)
*Louis Bradshaw,Simon Colton*

Main category: cs.SD

TL;DR: 本文介绍了一个新的MIDI文件数据集，说明了数据处理流程，数据集含超百万MIDI文件及约10万小时转录音频，提供分析及元数据标签，数据集可在线获取。


<details>
  <summary>Details</summary>
Motivation: 创建一个新的MIDI文件数据集，以满足相关研究或应用需求。

Method: 采用多阶段数据处理流程，先用语言模型基于元数据从互联网抓取和评分音频记录，再用音频分类器进行修剪和分割。

Result: 得到包含超百万个不同MIDI文件、约10万小时转录音频的数据集，并对技术进行深入分析，提取元数据标签。

Conclusion: 成功创建并提供了一个大规模的MIDI文件数据集及其相关分析和元数据。

Abstract: We introduce an extensive new dataset of MIDI files, created by transcribing
audio recordings of piano performances into their constituent notes. The data
pipeline we use is multi-stage, employing a language model to autonomously
crawl and score audio recordings from the internet based on their metadata,
followed by a stage of pruning and segmentation using an audio classifier. The
resulting dataset contains over one million distinct MIDI files, comprising
roughly 100,000 hours of transcribed audio. We provide an in-depth analysis of
our techniques, offering statistical insights, and investigate the content by
extracting metadata tags, which we also provide. Dataset available at
https://github.com/loubbrad/aria-midi.

</details>


### [316] [WavShape: Information-Theoretic Speech Representation Learning for Fair and Privacy-Aware Audio Processing](https://arxiv.org/abs/2506.22789)
*Oguzhan Baser,Ahmet Ege Tanriverdi,Kaan Kale,Sandeep P. Chinchali,Sriram Vishwanath*

Main category: cs.SD

TL;DR: 提出WavShape框架优化语音嵌入以保障公平性和隐私性，实验效果良好。


<details>
  <summary>Details</summary>
Motivation: 语音嵌入保留敏感属性，存在模型训练有偏差和隐私泄露风险。

Method: 利用Donsker - Varadhan公式进行互信息估计，引导基于互信息的编码器过滤敏感属性。

Result: 在三个已知数据集上，WavShape将嵌入与敏感属性间的互信息最多降低81%，同时保留97%的任务相关信息。

Conclusion: 将信息论与自监督语音模型结合，推动公平、隐私感知和资源高效的语音系统发展。

Abstract: Speech embeddings often retain sensitive attributes such as speaker identity,
accent, or demographic information, posing risks in biased model training and
privacy leakage. We propose WavShape, an information-theoretic speech
representation learning framework that optimizes embeddings for fairness and
privacy while preserving task-relevant information. We leverage mutual
information (MI) estimation using the Donsker-Varadhan formulation to guide an
MI-based encoder that systematically filters sensitive attributes while
maintaining speech content essential for downstream tasks. Experimental results
on three known datasets show that WavShape reduces MI between embeddings and
sensitive attributes by up to 81% while retaining 97% of task-relevant
information. By integrating information theory with self-supervised speech
models, this work advances the development of fair, privacy-aware, and
resource-efficient speech systems.

</details>


### [317] [TOMI: Transforming and Organizing Music Ideas for Multi-Track Compositions with Full-Song Structure](https://arxiv.org/abs/2506.23094)
*Qi He,Gus Xia,Ziyu Wang*

Main category: cs.SD

TL;DR: 本文引入TOMI方法用于深度音乐生成，借助基础大模型开发模型，可生成多轨电子音乐并实现人机共创，实验显示其效果优于基线。


<details>
  <summary>Details</summary>
Motivation: 除音乐时间结构层次外，探索概念层次这一更重要方面，实现音乐创意生成、转换和组织成完整作品。

Method: 引入TOMI方法，通过指令调优基础大模型开发模型，用四维空间表示多轨创作过程，将模型与REAPER数字音频工作站集成。

Result: 模型能生成具有完整歌曲结构的多轨电子音乐，实验表明生成的电子音乐质量更高、结构连贯性更强。

Conclusion: TOMI方法在深度音乐生成中有效，能生成高质量且结构连贯的电子音乐，可实现人机共创。

Abstract: Hierarchical planning is a powerful approach to model long sequences
structurally. Aside from considering hierarchies in the temporal structure of
music, this paper explores an even more important aspect: concept hierarchy,
which involves generating music ideas, transforming them, and ultimately
organizing them--across musical time and space--into a complete composition. To
this end, we introduce TOMI (Transforming and Organizing Music Ideas) as a
novel approach in deep music generation and develop a TOMI-based model via
instruction-tuned foundation LLM. Formally, we represent a multi-track
composition process via a sparse, four-dimensional space characterized by clips
(short audio or MIDI segments), sections (temporal positions), tracks
(instrument layers), and transformations (elaboration methods). Our model is
capable of generating multi-track electronic music with full-song structure,
and we further integrate the TOMI-based model with the REAPER digital audio
workstation, enabling interactive human-AI co-creation. Experimental results
demonstrate that our approach produces higher-quality electronic music with
stronger structural coherence compared to baselines.

</details>


### [318] [XY-Tokenizer: Mitigating the Semantic-Acoustic Conflict in Low-Bitrate Speech Codecs](https://arxiv.org/abs/2506.23325)
*Yitian Gong,Luozhijie Jin,Ruifan Deng,Dong Zhang,Xin Zhang,Qinyuan Cheng,Zhaoye Fei,Shimin Li,Xipeng Qiu*

Main category: cs.SD

TL;DR: 本文分析现有语音编解码器不足，提出XY - Tokenizer编解码器，实验表明其在语义和声学任务上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有语音编解码器难以平衡高质量音频重建和语言模型易建模性，需要一种能兼顾语义丰富性和声学保真度的编解码器。

Method: 提出XY - Tokenizer，通过多阶段、多任务学习缓解语义和声学能力之间的冲突。

Result: XY - Tokenizer在语义和声学任务上表现与同类先进编解码器相当，文本对齐能力强，重建音频与原始音频的说话人相似度得分达0.83，重建性能与BigCodec相近。

Conclusion: XY - Tokenizer是一种有效的语音编解码器，能在语义和声学方面取得较好平衡。

Abstract: Speech codecs serve as bridges between speech signals and large language
models. An ideal codec for speech language models should not only preserve
acoustic information but also capture rich semantic information. However,
existing speech codecs struggle to balance high-quality audio reconstruction
with ease of modeling by language models. In this study, we analyze the
limitations of previous codecs in balancing semantic richness and acoustic
fidelity. We propose XY-Tokenizer, a novel codec that mitigates the conflict
between semantic and acoustic capabilities through multi-stage, multi-task
learning. Experimental results demonstrate that XY-Tokenizer achieves
performance in both semantic and acoustic tasks comparable to that of
state-of-the-art codecs operating at similar bitrates, even though those
existing codecs typically excel in only one aspect. Specifically, XY-Tokenizer
achieves strong text alignment, surpassing distillation-based semantic modeling
methods such as SpeechTokenizer and Mimi, while maintaining a speaker
similarity score of 0.83 between reconstructed and original audio. The
reconstruction performance of XY-Tokenizer is comparable to that of BigCodec,
the current state-of-the-art among acoustic-only codecs, which achieves a
speaker similarity score of 0.84 at a similar bitrate. Code and models are
available at https://github.com/gyt1145028706/XY-Tokenizer.

</details>


### [319] [From Large-scale Audio Tagging to Real-Time Explainable Emergency Vehicle Sirens Detection](https://arxiv.org/abs/2506.23437)
*Stefano Giacomelli,Marco Giordano,Claudia Rinaldi,Fabio Graziosi*

Main category: cs.SD

TL;DR: 本文提出E2PANNs用于应急车辆警报声检测，经多数据集验证，在计算效率和边缘应用方面达新水平。


<details>
  <summary>Details</summary>
Motivation: 现有自动应急车辆警报声识别方案受数据集和计算需求限制，需更优方法。

Method: 基于PANNs框架构建轻量级卷积神经网络E2PANNs，利用AudioSet EV子集微调，在多数据集评估，开展消融研究、跨域基准测试和边缘设备实时推理部署，并进行可解释性分析。

Result: E2PANNs在应急车辆警报声检测领域达新的技术水平，计算效率高。

Conclusion: E2PANNs适合边缘音频监测和安全关键应用。

Abstract: Accurate recognition of Emergency Vehicle (EV) sirens is critical for the
integration of intelligent transportation systems, smart city monitoring
systems, and autonomous driving technologies. Modern automatic solutions are
limited by the lack of large scale, curated datasets and by the computational
demands of state of the art sound event detection models. This work introduces
E2PANNs (Efficient Emergency Pre trained Audio Neural Networks), a lightweight
Convolutional Neural Network architecture derived from the PANNs framework,
specifically optimized for binary EV siren detection. Leveraging our dedicated
subset of AudioSet (AudioSet EV) we fine-tune and evaluate E2PANNs across
multiple reference datasets and test its viability on embedded hardware. The
experimental campaign includes ablation studies, cross-domain benchmarking, and
real-time inference deployment on edge device. Interpretability analyses
exploiting Guided Backpropagation and ScoreCAM algorithms provide insights into
the model internal representations and validate its ability to capture distinct
spectrotemporal patterns associated with different types of EV sirens. Real
time performance is assessed through frame wise and event based detection
metrics, as well as a detailed analysis of false positive activations. Results
demonstrate that E2PANNs establish a new state of the art in this research
domain, with high computational efficiency, and suitability for edge-based
audio monitoring and safety-critical applications.

</details>


### [320] [Scaling Self-Supervised Representation Learning for Symbolic Piano Performance](https://arxiv.org/abs/2506.23869)
*Louis Bradshaw,Honglu Fan,Alexander Spangher,Stella Biderman,Simon Colton*

Main category: cs.SD

TL;DR: 研究在大量符号独奏钢琴转录数据上训练的自回归生成式Transformer模型能力，经预训练和微调后，模型在音乐续写、分类和生成嵌入方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 探索在大量符号独奏钢琴转录数据上训练的生成式自回归Transformer模型的能力。

Method: 先在约60000小时音乐上预训练，再用较小高质量子集微调，还将SimCLR框架应用于符号音乐生成对比式MIDI嵌入。

Result: 生成模型在钢琴续写连贯性上优于领先的符号生成技术，与专有音频生成模型有竞争力；对比模型在MIR分类基准测试中，冻结表示在线性探测实验中达最优，微调显示预训练表示泛化性好。

Conclusion: 该模型在音乐续写和分类任务中有良好表现，预训练表示具有泛化性。

Abstract: We study the capabilities of generative autoregressive transformer models
trained on large amounts of symbolic solo-piano transcriptions. After first
pretraining on approximately 60,000 hours of music, we use a comparatively
smaller, high-quality subset, to finetune models to produce musical
continuations, perform symbolic classification tasks, and produce
general-purpose contrastive MIDI embeddings by adapting the SimCLR framework to
symbolic music. When evaluating piano continuation coherence, our generative
model outperforms leading symbolic generation techniques and remains
competitive with proprietary audio generation models. On MIR classification
benchmarks, frozen representations from our contrastive model achieve
state-of-the-art results in linear probe experiments, while direct finetuning
demonstrates the generalizability of pretrained representations, often
requiring only a few hundred labeled examples to specialize to downstream
tasks.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [321] [Evaluating and Improving Large Language Models for Competitive Program Generation](https://arxiv.org/abs/2506.22954)
*Minnan Wei,Ziming Li,Xiang Chen,Menglin Zheng,Ziyan Qu,Cheng Yu,Siyu Chen,Xiaolin Ju*

Main category: cs.SI

TL;DR: 本文针对大语言模型解决真实竞赛编程问题进行评估和改进，构建基准测试集，评估模型能力，分析错误并提出改进框架，使正确解决问题数量大幅提升。


<details>
  <summary>Details</summary>
Motivation: 现有研究在评估大语言模型竞赛编程生成能力时，使用简单提示和易数据泄露的基准数据集，且对算法类型和难度多样性考虑有限，因此要评估和改进大语言模型解决真实竞赛编程问题的能力。

Method: 从2024年九场区域ICPC/CCPC竞赛收集117个问题，设计过滤标准构建80个问题的基准测试集；以DeepSeek - R1为模型，通过在线评测平台结合精心设计的基本提示评估其能力；对错误提交构建细粒度错误分类法，提出结合多轮对话修复和信息增强再生阶段的改进框架。

Result: 使用基本提示时80个问题仅5个完全通过；构建包括一般错误和专业错误的错误分类法；应用改进策略后80个问题中有46个成功通过。

Conclusion: 所提出的改进策略能有效提升大语言模型解决真实竞赛编程问题的能力。

Abstract: Context: Due to the demand for strong algorithmic reasoning, complex logic
implementation, and strict adherence to input/output formats and resource
constraints, competitive programming generation by large language models (LLMs)
is considered the most challenging problem in current LLM-based code
generation. However, previous studies often evaluate LLMs using simple prompts
and benchmark datasets prone to data leakage. Moreover, prior work has limited
consideration of the diversity in algorithm types and difficulty levels.
Objective: In this study, we aim to evaluate and improve LLMs in solving
real-world competitive programming problems. Methods: We initially collect 117
problems from nine regional ICPC/CCPC contests held in 2024 and design four
filtering criteria to construct a curated benchmark consisting of 80 problems.
Leveraging DeepSeek-R1 as the LLM, we evaluate its competitive program
generation capabilities through the online judge (OJ) platforms, guided by a
carefully designed basic prompt. For incorrect submissions, we construct a
fine-grained error taxonomy and then propose a targeted improvement framework
by combining a multi-turn dialogue-based repair phase and an
information-augmented regeneration phase. Results: Experimental results show
that only 5 out of 80 problems are fully accepted when using basic prompts. For
the unsolved problems, we construct the error taxonomy, including general
errors (such as design, boundary, condition, data type, syntax, and
input/output errors) and specialized errors (such as those in mathematical
problems, greedy algorithms, and graph theories). After applying our proposed
improvement strategies, we substantially increased the number of correct
solutions, with 46 out of 80 problems successfully accepted.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [322] [Minimax and Bayes Optimal Best-arm Identification: Adaptive Experimental Design for Treatment Choice](https://arxiv.org/abs/2506.24007)
*Masahiro Kato*

Main category: econ.EM

TL;DR: 研究治疗选择的自适应实验设计，设计两阶段实验高效识别最佳治疗臂，证明该设计渐近极小极大和贝叶斯最优。


<details>
  <summary>Details</summary>
Motivation: 研究治疗选择的自适应实验设计，以高效识别最佳治疗臂。

Method: 采用包含治疗分配阶段和治疗选择阶段的自适应程序，治疗分配阶段分两阶段，第一阶段均匀分配消除次优臂并估计方差，第二阶段按方差分配，最后选样本均值最高的为最佳治疗臂。

Result: 证明该设计对简单遗憾同时渐近极小极大和贝叶斯最优，上下界匹配。

Conclusion: 所设计实验无需分别调整即可达到尖锐效率极限。

Abstract: This study investigates adaptive experimental design for treatment choice,
also known as fixed-budget best-arm identification. We consider an adaptive
procedure consisting of a treatment-allocation phase followed by a
treatment-choice phase, and we design an adaptive experiment for this setup to
efficiently identify the best treatment arm, defined as the one with the
highest expected outcome. In our designed experiment, the treatment-allocation
phase consists of two stages. The first stage is a pilot phase, where we
allocate each treatment arm uniformly with equal proportions to eliminate
clearly suboptimal arms and estimate outcome variances. In the second stage, we
allocate treatment arms in proportion to the variances estimated in the first
stage. After the treatment-allocation phase, the procedure enters the
treatment-choice phase, where we choose the treatment arm with the highest
sample mean as our estimate of the best treatment arm. We prove that this
single design is simultaneously asymptotically minimax and Bayes optimal for
the simple regret, with upper bounds that match our lower bounds up to exact
constants. Therefore, our designed experiment achieves the sharp efficiency
limits without requiring separate tuning for minimax and Bayesian objectives.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [323] [Spectral Bias in Variational Quantum Machine Learning](https://arxiv.org/abs/2506.22555)
*Callum Duffy,Marcin Jastrzebski*

Main category: quant-ph

TL;DR: 研究量子机器学习中参数化量子电路（PQCs）的频谱偏差现象，证明其源于傅里叶系数‘冗余’，通过实验验证，还研究设计选择对学习傅里叶和的影响。


<details>
  <summary>Details</summary>
Motivation: 探究量子机器学习中频谱偏差现象，特别是在参数化量子电路中的情况。

Method: 利用PQCs作为傅里叶级数的既定公式进行理论证明，用三种不同编码方案进行实验验证，研究参数初始化规模和纠缠结构等设计选择。

Result: 频谱偏差源于傅里叶系数‘冗余’，系数梯度幅度与冗余度强相关，冗余度高的PQCs对参数随机扰动更鲁棒，大初始化和低纠缠方案会减慢收敛。

Conclusion: 明确了PQCs中频谱偏差的来源和影响，指出设计选择对PQCs学习傅里叶和能力的作用。

Abstract: In this work, we investigate the phenomenon of spectral bias in quantum
machine learning, where, in classical settings, models tend to fit
low-frequency components of a target function earlier during training than
high-frequency ones, demonstrating a frequency-dependent rate of convergence.
We study this effect specifically in parameterised quantum circuits (PQCs).
Leveraging the established formulation of PQCs as Fourier series, we prove that
spectral bias in this setting arises from the ``redundancy'' of the Fourier
coefficients, which denotes the number of terms in the analytical form of the
model contributing to the same frequency component. The choice of data encoding
scheme dictates the degree of redundancy for a Fourier coefficient. We find
that the magnitude of the Fourier coefficients' gradients during training
strongly correlates with the coefficients' redundancy. We then further
demonstrate this empirically with three different encoding schemes.
Additionally, we demonstrate that PQCs with greater redundancy exhibit
increased robustness to random perturbations in their parameters at the
corresponding frequencies. We investigate how design choices affect the ability
of PQCs to learn Fourier sums, focusing on parameter initialization scale and
entanglement structure, finding large initializations and low-entanglement
schemes tend to slow convergence.

</details>


### [324] [Tensor Train Quantum State Tomography using Compressed Sensing](https://arxiv.org/abs/2506.23560)
*Shakir Showkat Sofi,Charlotte Vermeylen,Lieven De Lathauwer*

Main category: quant-ph

TL;DR: 提出用低秩块张量列车分解参数化量子态以解决标准估计方法的难题，该方法高效且适用于多种量子态。


<details>
  <summary>Details</summary>
Motivation: 标准量子态层析（QST）估计方法因状态表示中参数指数增长而变得不实用。

Method: 使用低秩块张量列车分解对量子态进行参数化。

Result: 所提方法在内存和计算上都很高效。

Conclusion: 该框架适用于能用低秩分解良好近似的一大类量子态。

Abstract: Quantum state tomography (QST) is a fundamental technique for estimating the
state of a quantum system from measured data and plays a crucial role in
evaluating the performance of quantum devices. However, standard estimation
methods become impractical due to the exponential growth of parameters in the
state representation. In this work, we address this challenge by parameterizing
the state using a low-rank block tensor train decomposition and demonstrate
that our approach is both memory- and computationally efficient. This framework
applies to a broad class of quantum states that can be well approximated by
low-rank decompositions, including pure states, nearly pure states, and ground
states of Hamiltonians.

</details>


### [325] [SQUASH: A SWAP-Based Quantum Attack to Sabotage Hybrid Quantum Neural Networks](https://arxiv.org/abs/2506.24081)
*Rahul Kumar,Wenqi Wei,Ying Mao,Junaid Farooq,Ying Wang,Juntao Chen*

Main category: quant-ph

TL;DR: 提出电路级攻击SQUASH破坏混合量子神经网络分类任务，攻击隐蔽且效果显著，揭示网络脆弱性。


<details>
  <summary>Details</summary>
Motivation: 发现混合量子神经网络（HQNNs）在分类任务中的安全隐患，探索新型攻击方式。

Method: 在受害者HQNN的变分量子电路中插入SWAP门，直接操纵电路结构。

Result: SQUASH显著降低分类性能，无目标攻击使准确率最多降74.08%，有目标攻击使目标类准确率最多降79.78%。

Conclusion: HQNN实现存在关键漏洞，需要更具弹性的架构抵御电路级对抗干预。

Abstract: We propose a circuit-level attack, SQUASH, a SWAP-Based Quantum Attack to
sabotage Hybrid Quantum Neural Networks (HQNNs) for classification tasks.
SQUASH is executed by inserting SWAP gate(s) into the variational quantum
circuit of the victim HQNN. Unlike conventional noise-based or adversarial
input attacks, SQUASH directly manipulates the circuit structure, leading to
qubit misalignment and disrupting quantum state evolution. This attack is
highly stealthy, as it does not require access to training data or introduce
detectable perturbations in input states. Our results demonstrate that SQUASH
significantly degrades classification performance, with untargeted SWAP attacks
reducing accuracy by up to 74.08\% and targeted SWAP attacks reducing target
class accuracy by up to 79.78\%. These findings reveal a critical vulnerability
in HQNN implementations, underscoring the need for more resilient architectures
against circuit-level adversarial interventions.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [326] [VoteSplat: Hough Voting Gaussian Splatting for 3D Scene Understanding](https://arxiv.org/abs/2506.22799)
*Minchao Jiang,Shunyu Jia,Jiaming Gu,Xiaoyuan Lu,Guangming Zhu,Anqi Dong,Liang Zhang*

Main category: cs.GR

TL;DR: 提出VoteSplat框架，结合Hough投票与3DGS用于3D场景理解，降低训练成本，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS方法缺乏场景理解且训练成本高，原可微渲染流程复杂。

Method: 利用SAM进行实例分割，提取对象生成2D投票图，将空间偏移向量嵌入高斯基元，构建3D空间投票，通过深度失真约束优化定位，通过投票点将2D图像语义映射到3D点云。

Result: 通过大量实验验证了VoteSplat在多种任务中的有效性。

Conclusion: VoteSplat是有效的3D场景理解框架，能降低训练成本，可用于多种3D场景理解任务。

Abstract: 3D Gaussian Splatting (3DGS) has become horsepower in high-quality, real-time
rendering for novel view synthesis of 3D scenes. However, existing methods
focus primarily on geometric and appearance modeling, lacking deeper scene
understanding while also incurring high training costs that complicate the
originally streamlined differentiable rendering pipeline. To this end, we
propose VoteSplat, a novel 3D scene understanding framework that integrates
Hough voting with 3DGS. Specifically, Segment Anything Model (SAM) is utilized
for instance segmentation, extracting objects, and generating 2D vote maps. We
then embed spatial offset vectors into Gaussian primitives. These offsets
construct 3D spatial votes by associating them with 2D image votes, while depth
distortion constraints refine localization along the depth axis. For
open-vocabulary object localization, VoteSplat maps 2D image semantics to 3D
point clouds via voting points, reducing training costs associated with
high-dimensional CLIP features while preserving semantic unambiguity. Extensive
experiments demonstrate effectiveness of VoteSplat in open-vocabulary 3D
instance localization, 3D point cloud understanding, click-based 3D object
localization, hierarchical segmentation, and ablation studies. Our code is
available at https://sy-ja.github.io/votesplat/

</details>


### [327] [Navigating with Annealing Guidance Scale in Diffusion Space](https://arxiv.org/abs/2506.24108)
*Shai Yehezkel,Omer Dahary,Andrey Voynov,Daniel Cohen-Or*

Main category: cs.GR

TL;DR: 提出退火引导调度器，动态调整引导尺度，提升文生图性能，且无需额外激活或内存消耗。


<details>
  <summary>Details</summary>
Motivation: 去噪扩散模型采样依赖引导，CFG的引导尺度选择对生成图像质量和与提示的一致性有关键影响，需要解决其不稳定问题。

Method: 提出基于条件噪声信号动态调整引导尺度的退火引导调度器，学习调度策略。

Result: 引导调度器显著提升图像质量和与文本提示的一致性，提升文生图性能。

Conclusion: 新调度器无需额外激活或内存消耗，可无缝替换普通无分类器引导，在提示一致性和质量间实现更好平衡。

Abstract: Denoising diffusion models excel at generating high-quality images
conditioned on text prompts, yet their effectiveness heavily relies on careful
guidance during the sampling process. Classifier-Free Guidance (CFG) provides a
widely used mechanism for steering generation by setting the guidance scale,
which balances image quality and prompt alignment. However, the choice of the
guidance scale has a critical impact on the convergence toward a visually
appealing and prompt-adherent image. In this work, we propose an annealing
guidance scheduler which dynamically adjusts the guidance scale over time based
on the conditional noisy signal. By learning a scheduling policy, our method
addresses the temperamental behavior of CFG. Empirical results demonstrate that
our guidance scheduler significantly enhances image quality and alignment with
the text prompt, advancing the performance of text-to-image generation.
Notably, our novel scheduler requires no additional activations or memory
consumption, and can seamlessly replace the common classifier-free guidance,
offering an improved trade-off between prompt alignment and quality.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [328] [Deep Learning for Optical Misalignment Diagnostics in Multi-Lens Imaging Systems](https://arxiv.org/abs/2506.23173)
*Tomer Slor,Dean Oren,Shira Baneth,Tom Coen,Haim Suchowski*

Main category: physics.optics

TL;DR: 提出两种基于深度学习的逆设计方法，利用光学测量诊断多透镜系统的对准误差，展示了重塑精密成像制造和质量控制的潜力。


<details>
  <summary>Details</summary>
Motivation: 光学工程中多透镜成像系统精确对准至关重要但传统方法依赖专业设备且耗时，需要自动化可扩展解决方案。

Method: 提出两种互补的基于深度学习的逆设计方法，一是用光线追迹光斑图预测6透镜摄影镜头5自由度误差，二是引入基于物理的模拟管道，利用灰度合成相机图像让深度学习模型估计2和6透镜系统的4自由度误差。

Result: 用光线追迹光斑图预测时，横向平移平均绝对误差为0.031mm，倾斜误差为0.011°；可利用灰度合成相机图像估计误差。

Conclusion: 所提方法有潜力重塑精密成像的制造和质量控制。

Abstract: In the rapidly evolving field of optical engineering, precise alignment of
multi-lens imaging systems is critical yet challenging, as even minor
misalignments can significantly degrade performance. Traditional alignment
methods rely on specialized equipment and are time-consuming processes,
highlighting the need for automated and scalable solutions. We present two
complementary deep learning-based inverse-design methods for diagnosing
misalignments in multi-element lens systems using only optical measurements.
First, we use ray-traced spot diagrams to predict five-degree-of-freedom
(5-DOF) errors in a 6-lens photographic prime, achieving a mean absolute error
of 0.031mm in lateral translation and 0.011$^\circ$ in tilt. We also introduce
a physics-based simulation pipeline that utilizes grayscale synthetic camera
images, enabling a deep learning model to estimate 4-DOF, decenter and tilt
errors in both two- and six-lens multi-lens systems. These results show the
potential to reshape manufacturing and quality control in precision imaging.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [329] [Learning robust parameter inference and density reconstruction in flyer plate impact experiments](https://arxiv.org/abs/2506.23914)
*Evan Bell,Daniel A. Serino,Ben S. Southworth,Trevor Wilcox,Marc L. Klasky*

Main category: physics.comp-ph

TL;DR: 本文聚焦多孔材料飞片撞击实验，利用机器学习发现仅靠高冲击速度数据无法准确推断状态方程和压实孔隙率模型参数，提出高低冲击速度实验数据集及生成式机器学习方法估计参数，验证了方法有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在许多实验（如冲击物理实验）中，射线照相无法直接获取关键状态变量，传统参数估计方法难以应用，需解决从射线照相观测结果中解析材料状态方程和孔隙率模型参数的问题。

Method: 使用机器学习，提出包含高低冲击速度实验/模拟的可观测数据集，引入生成式机器学习方法从射线照片直接生成物理参数的后验分布。

Result: 证明仅高冲击速度数据不足；验证了方法在模拟飞片撞击实验中估计参数的有效性，能准确重建密度；检验了方法对模型不匹配的鲁棒性，在异常噪声和未知物理现象下仍能提供有用参数估计。

Conclusion: 该生成式机器学习方法有望在从实验射线图像估计材料特性方面取得突破。

Abstract: Estimating physical parameters or material properties from experimental
observations is a common objective in many areas of physics and material
science. In many experiments, especially in shock physics, radiography is the
primary means of observing the system of interest. However, radiography does
not provide direct access to key state variables, such as density, which
prevents the application of traditional parameter estimation approaches. Here
we focus on flyer plate impact experiments on porous materials, and resolving
the underlying parameterized equation of state (EoS) and crush porosity model
parameters given radiographic observation(s). We use machine learning as a tool
to demonstrate with high confidence that using only high impact velocity data
does not provide sufficient information to accurately infer both EoS and crush
model parameters, even with fully resolved density fields or a dynamic sequence
of images. We thus propose an observable data set consisting of low and high
impact velocity experiments/simulations that capture different regimes of
compaction and shock propagation, and proceed to introduce a generative machine
learning approach which produces a posterior distribution of physical
parameters directly from radiographs. We demonstrate the effectiveness of the
approach in estimating parameters from simulated flyer plate impact
experiments, and show that the obtained estimates of EoS and crush model
parameters can then be used in hydrodynamic simulations to obtain accurate and
physically admissible density reconstructions. Finally, we examine the
robustness of the approach to model mismatches, and find that the learned
approach can provide useful parameter estimates in the presence of
out-of-distribution radiographic noise and previously unseen physics, thereby
promoting a potential breakthrough in estimating material properties from
experimental radiographic images.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [330] [Persistence Paradox in Dynamic Science](https://arxiv.org/abs/2506.22729)
*Honglin Bao,Kai Li*

Main category: cs.DL

TL;DR: 本文挑战科研中坚持是美德的传统观点，以2012年AlexNet引发的深度学习革命为例，分析超5000名科学家职业轨迹，发现范式转变期坚持可能成负担，战略适应者获益最大。


<details>
  <summary>Details</summary>
Motivation: 挑战科研中坚持是美德的传统观点，强调坚持的情境性，探讨范式转变期坚持的影响。

Method: 分析在顶级机器学习会议活跃过的超5000名科学家20年职业轨迹，研究其研究重点和产出演变。

Result: 领先会议更重视深度学习，取代传统统计学习方法；此前成功或来自旧团队的科学家适应慢，有刚性惩罚；战略适应者获益最大。

Conclusion: 科学突破会重构领域内权力结构。

Abstract: Persistence is often regarded as a virtue in science. In this paper, however,
we challenge this conventional view by highlighting its contextual nature,
particularly how persistence can become a liability during periods of paradigm
shift. We focus on the deep learning revolution catalyzed by AlexNet in 2012.
Analyzing the 20-year career trajectories of over 5,000 scientists who were
active in top machine learning venues during the preceding decade, we examine
how their research focus and output evolved. We first uncover a dynamic period
in which leading venues increasingly prioritized cutting-edge deep learning
developments that displaced relatively traditional statistical learning
methods. Scientists responded to these changes in markedly different ways.
Those who were previously successful or affiliated with old teams adapted more
slowly, experiencing what we term a rigidity penalty - a reluctance to embrace
new directions leading to a decline in scientific impact, as measured by
citation percentile rank. In contrast, scientists who pursued strategic
adaptation - selectively pivoting toward emerging trends while preserving weak
connections to prior expertise - reaped the greatest benefits. Taken together,
our macro- and micro-level findings show that scientific breakthroughs act as
mechanisms that reconfigure power structures within a field.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [331] [Shifted Composition IV: Underdamped Langevin and Numerical Discretizations with Partial Acceleration](https://arxiv.org/abs/2506.23062)
*Jason M. Altschuler,Sinho Chewi,Matthew S. Zhang*

Main category: math.PR

TL;DR: 本文提出基于耦合的框架分析欠阻尼朗之万动力学（ULD）及其数值离散化，得到新的抛物哈纳克不等式，并建立局部误差框架，应用于随机中点离散化得到相关加速和复杂度结果。


<details>
  <summary>Details</summary>
Motivation: ULD收敛速率量化是经典问题，其退化性需要新分析方法。

Method: 提出基于耦合的框架，先在连续时间设置下建立新的抛物哈纳克不等式，再基于此建立局部误差框架。

Result: 得到了在收缩设置下衰减到零的哈纳克不等式；将框架应用于ULD的随机中点离散化，得到对数凹采样的弹道加速结果和d维采样到恒定总变差误差的迭代复杂度保证。

Conclusion: 新的基于耦合的框架可有效分析ULD及其数值离散化。

Abstract: Quantifying the convergence rate of the underdamped Langevin dynamics (ULD)
is a classical topic, in large part due to the possibility for
diffusive-to-ballistic speedups -- as was recently established for the
continuous-time dynamics via space-time Poincare inequalities. A central
challenge for analyzing ULD is that its degeneracy necessitates the development
of new analysis approaches, e.g., the theory of hypocoercivity. In this paper,
we give a new coupling-based framework for analyzing ULD and its numerical
discretizations. First, in the continuous-time setting, we use this framework
to establish new parabolic Harnack inequalities for ULD. These are the first
Harnack inequalities that decay to zero in contractive settings, thereby
reflecting the convergence properties of ULD in addition to just its regularity
properties.
  Second, we build upon these Harnack inequalities to develop a local error
framework for analyzing discretizations of ULD in KL divergence. This extends
our framework in part III from uniformly elliptic diffusions to degenerate
diffusions, and shares its virtues: the framework is user-friendly, applies to
sophisticated discretization schemes, and does not require contractivity.
Applying this framework to the randomized midpoint discretization of ULD
establishes (i) the first ballistic acceleration result for log-concave
sampling (i.e., sublinear dependence on the condition number), and (ii) the
first $d^{1/3}$ iteration complexity guarantee for sampling to constant total
variation error in dimension $d$.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [332] [Arnoldi Singular Vector perturbations for machine learning weather prediction](https://arxiv.org/abs/2506.22450)
*Jens Winkler,Michael Denhard*

Main category: physics.ao-ph

TL;DR: 探索华为24h盘古气象模型对初始条件误差的敏感性，提出A - SV方法，发现有意义扰动模式，可用于初始化集合预报，还讨论与GenCast的异同。


<details>
  <summary>Details</summary>
Motivation: 因天气预报本质上有不确定性，可靠决策需要未来天气情景可能性信息，故探索机器学习天气预报对初始条件误差的敏感性。

Method: 使用特定的奇异向量（SV）扰动，采用阿诺尔迪 - 奇异向量（A - SV）方法，该方法无需线性或伴随模型版本，通过迭代应用预报模型到扰动状态观察误差增长，创建Krylov子空间。

Result: A - SV为24h盘古气象模型找到动态有意义的扰动模式，从预报开始就增长，描述了局部不稳定模式。

Conclusion: 这些扰动可作为初始化机器学习天气预报集合的基础，还简要讨论了与GenCast基于扩散的机器学习模型的异同。

Abstract: Since weather forecasts are fundamentally uncertain, reliable decision making
requires information on the likelihoods of future weather scenarios. We explore
the sensitivity of machine learning weather prediction (MLWP) using the 24h
Pangu Weather ML model of Huawei to errors in the initial conditions with a
specific kind of Singular Vector (SV) perturbations. Our Arnoldi-SV (A-SV)
method does not need linear nor adjoint model versions and is applicable to
numerical weather prediction (NWP) as well as MLWP. It observes error growth
within a given optimization time window by iteratively applying a forecast
model to perturbed model states. This creates a Krylov subspace, implicitly
based on a matrix operator, which approximates the local error growth. Each
iteration adds new dimensions to the Krylov space and its leading right SVs are
expected to turn into directions of growing errors. We show that A-SV indeed
finds dynamically meaningful perturbation patterns for the 24h Pangu Weather
model, which grow right from the beginning of the forecast rollout. These
perturbations describe local unstable modes and could be a basis to initialize
MLWP ensembles. Since we start A-SV from random noise perturbations, the
algorithm transforms noise into perturbations conditioned on a given reference
state - a process that is akin to the denoising process of the generic
diffusion based ML model of GenCast, therefor we briefly discuss similarities
and differences.

</details>


<div id='astro-ph.GA'></div>

# astro-ph.GA [[Back]](#toc)

### [333] [Simulation-based population inference of LISA's Galactic binaries: Bypassing the global fit](https://arxiv.org/abs/2506.22543)
*Rahul Srinivasan,Enrico Barausse,Natalia Korsakova,Roberto Trotta*

Main category: astro-ph.GA

TL;DR: 提出绕过全局拟合，用基于模拟的方法从频率应变序列直接推断LISA最常见源（银河系双白矮星）的特性，且该方法可扩展到其他源类和噪声场景。


<details>
  <summary>Details</summary>
Motivation: 传统从背景中分离解析源并提取参数的“全局拟合”计算量大，需寻找更高效方法推断LISA源特性。

Method: 通过对银河系双白矮星群体模拟的LISA频率序列进行自定义压缩，并训练归一化流。

Result: 能够同时且高效地从解析和未解析源中提取群体参数信息。

Conclusion: 该直接针对目标群体特性的方法在有快速模拟的情况下可扩展到其他源类和含非高斯或非平稳噪声的场景。

Abstract: The Laser Interferometer Space Antenna (LISA) is expected to detect thousands
of individually resolved gravitational wave sources, overlapping in time and
frequency, on top of unresolved astrophysical and/or primordial backgrounds.
Disentangling resolved sources from backgrounds and extracting their parameters
in a computationally intensive "global fit" is normally regarded as a necessary
step toward reconstructing the properties of the underlying astrophysical
populations. Here, we show that it is possible to infer the properties of the
most numerous population of LISA sources - Galactic double white dwarfs -
directly from the frequency (or, equivalently, time) strain series, by using a
simulation-based approach that bypasses the global fit entirely. By training a
normalizing flow on a custom-designed compression of simulated LISA frequency
series from the Galactic double white dwarf population, we demonstrate how to
infer the posterior distribution of population parameters (e.g., mass function,
frequency, and spatial distributions). This allows for extracting information
on the population parameters from both resolved and unresolved sources
simultaneously and in a computationally efficient manner. Our approach to
target population properties directly can be readily extended to other source
classes (e.g., massive and stellar-mass black holes, extreme mass ratio
inspirals), provided fast simulations are available, and to scenarios involving
non-Gaussian or non-stationary noise (e.g., data gaps).

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [334] [Highway toll allocation problem revisited: new methods and characterizations](https://arxiv.org/abs/2506.22475)
*P. Soto-Rodríguez,B. Casas-Méndez,A. Saavedra-Nieves*

Main category: math.OC

TL;DR: 论文研究高速公路收费分配问题，提出新分配规则，进行公理化刻画、性质比较，引入通用方法族并评估性能。


<details>
  <summary>Details</summary>
Motivation: 解决高速公路收费在各路段的分配问题。

Method: 提出Segments Proportional Sharing和Segments Compensated Sharing方法，公理化刻画，与Segments Equal Sharing方法比较，研究与合作博弈解的关系，引入通用方法族，用真实数据集评估。

Result: 完成新方法的刻画、比较及通用方法族引入，完成方法性能评估。

Conclusion: 提出新分配规则，构建通用方法族，对高速公路收费分配方法研究有推进。

Abstract: This paper considers the highway toll allocation problem (Wu, van den Brink,
and Est\'evez-Fern\'andez in Transport Res B-Meth 180:10288, 2024). The aim is
to allocate the tolls collected from the users of a highway across the various
road sections. To this end, the authors propose, among others, the Segments
Equal Sharing method, which is characterized and reinterpreted as a specific
solution of a cooperative game associated with the problem. This paper presents
two new allocation rules: the Segments Proportional Sharing method and the
Segments Compensated Sharing method. We axiomatically characterize these new
methods and compare their properties to those of the Segments Equal Sharing
method. Furthermore, we also examine the relationship of these methods to the
solution of the associated cooperative game. We conclude the methodological
study by introducing a general family of segment allocation methods that
includes the three aforementioned rules. Finally, we evaluate the performance
of these methods using a real-world dataset.

</details>


### [335] [Correlated Mutations for Integer Programming](https://arxiv.org/abs/2506.22526)
*Ofer M. Shir,Michael Emmerich*

Main category: math.OC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Even with the recent theoretical advancements that dramatically reduced the
complexity of Integer Programming (IP), heuristics remain the dominant
problem-solvers for this difficult category. This study seeks to establish the
groundwork for Integer Evolution Strategies (IESs), a class of randomized
search heuristics inherently designed for continuous spaces. IESs already excel
in treating IP in practice, but accomplish it via discretization and by
applying sophisticated patches to their continuous operators, while
persistently using the $\ell_2$-norm as their operation pillar. We lay
foundations for discrete search, by adopting the $\ell_1$-norm, accounting for
the suitable step-size, and questioning alternative measures to quantify
correlations over the integer lattice. We focus on mutation distributions for
unbounded integer decision variables. We briefly discuss a couple of candidate
discrete probabilities induced by the uniform and binomial distributions, which
we show to possess less appealing theoretical properties, and then narrow down
to the Truncated Normal (TN) and Double Geometric (DG) distributions. We
explore their theoretical properties, including entropy functions, and propose
a procedure to generate scalable correlated mutation distributions. Our
investigations are accompanied by extensive numerical simulations, which
consistently support the claim that the DG distribution is better suited for
unbounded integer search. We link our theoretical perspective to empirical
evidence indicating that an IES with correlated DG mutations outperformed other
strategies over non-separable quadratic IP. We conclude that while the
replacement of the default TN distribution by the DG is theoretically justified
and practically beneficial, the truly crucial change lies in adopting the
$\ell_1$-norm over the $\ell_2$-norm.

</details>


### [336] [Inventory Control Using a Lévy Process for Evaluating Total Costs under Intermittent Demand](https://arxiv.org/abs/2506.22524)
*Ryoya Koide,Yurika Ono,Aya Ishigaki*

Main category: math.OC

TL;DR: 本文针对间歇性需求产品，提出用漂移泊松过程建模累积需求并引入停止时间制定再订购点策略，对比ARIMA模型验证其有效性，揭示了不同模型下总成本的增长特征。


<details>
  <summary>Details</summary>
Motivation: 现有研究未探讨在使用Lévy过程的库存控制中，订购量和再订购点对总成本的影响，且难以对再订购点触发的库存补充进行数学建模。

Method: 将累积需求建模为漂移泊松过程，引入停止时间表示达到再订购点的时机，制定再订购点策略，并与ARIMA模型结合再订购点策略的情况对比。

Result: ARIMA模型预测下总成本随时间线性增长，基于Lévy过程的公式给出了总成本的解析表达式，表明随机需求波动使预期总成本增长速度快于线性。

Conclusion: 基于Lévy过程的再订购点策略公式化方法有效，能更好地分析随机需求下的总成本增长情况。

Abstract: Products with intermittent demand are characterized by a high risk of sales
losses and obsolescence due to the sporadic occurrence of demand events.
Generally, both point forecasting and probabilistic forecasting approaches are
applied to intermittent demand. In particular, probabilistic forecasting, which
models demand as a stochastic process, is capable of capturing uncertainty. An
example of such modeling is the use of L\'evy processes, which possess
independent increments and accommodate discontinuous changes (jumps). However,
to the best of our knowledge, in inventory control using L\'evy processes, no
studies have investigated how the order quantity and reorder point affect the
total cost. One major difficulty has been the mathematical formulation of
inventory replenishment triggered at reorder points. To address this challenge,
the present study formulates a reorder-point policy by modeling cumulative
demand as a drifted Poisson process and introducing a stopping time to
represent the timing at which the reorder point is reached. Furthermore, the
validity of the proposed method is verified by comparing the total cost with
that obtained from a case where an ARIMA model is combined with a reorder-point
policy. As a main result, while the total cost under ARIMA-based forecasting
increases linearly over time, the L\'evy process-based formulation provides an
analytical expression for the total cost, revealing that random demand
fluctuations cause the expected total cost to grow at a rate faster than
linear.

</details>


### [337] [Proving the Limited Scalability of Centralized Distributed Optimization via a New Lower Bound Construction](https://arxiv.org/abs/2506.23836)
*Alexander Tyurin*

Main category: math.OC

TL;DR: 本文研究经典联邦学习设置下的集中式分布式优化，证明考虑服务器到工作节点通信时，使用无偏随机稀疏压缩器设计的方法无法在n上以优于多项式对数的方式同时缩放服务器端通信运行时间和方差相关运行时间。


<details>
  <summary>Details</summary>
Motivation: 分布式优化的主要动机之一是实现关于工作节点数量n的可扩展性，希望找到可同时缩放相关运行时间项的方法。

Method: 构造新的“最坏情况”函数，开发新的下界框架，将分析简化为随机和的集中性问题并证明集中界。

Result: 证明了在考虑服务器到工作节点通信时，使用无偏随机稀疏压缩器设计的方法无法在n上以优于多项式对数的方式同时缩放服务器端通信运行时间项和方差相关运行时间项。

Conclusion: 即使在同质假设下，分布式优化的缩放也存在基本限制。

Abstract: We consider centralized distributed optimization in the classical federated
learning setup, where $n$ workers jointly find an $\varepsilon$-stationary
point of an $L$-smooth, $d$-dimensional nonconvex function $f$, having access
only to unbiased stochastic gradients with variance $\sigma^2$. Each worker
requires at most $h$ seconds to compute a stochastic gradient, and the
communication times from the server to the workers and from the workers to the
server are $\tau_{s}$ and $\tau_{w}$ seconds per coordinate, respectively. One
of the main motivations for distributed optimization is to achieve scalability
with respect to $n$. For instance, it is well known that the distributed
version of SGD has a variance-dependent runtime term $\frac{h \sigma^2 L
\Delta}{n \varepsilon^2},$ which improves with the number of workers $n,$ where
$\Delta = f(x^0) - f^*,$ and $x^0 \in R^d$ is the starting point. Similarly,
using unbiased sparsification compressors, it is possible to reduce both the
variance-dependent runtime term and the communication runtime term. However,
once we account for the communication from the server to the workers
$\tau_{s}$, we prove that it becomes infeasible to design a method using
unbiased random sparsification compressors that scales both the server-side
communication runtime term $\tau_{s} d \frac{L \Delta}{\varepsilon}$ and the
variance-dependent runtime term $\frac{h \sigma^2 L \Delta}{\varepsilon^2},$
better than poly-logarithmically in $n$, even in the homogeneous (i.i.d.) case,
where all workers access the same distribution. To establish this result, we
construct a new "worst-case" function and develop a new lower bound framework
that reduces the analysis to the concentration of a random sum, for which we
prove a concentration bound. These results reveal fundamental limitations in
scaling distributed optimization, even under the homogeneous assumption.

</details>


### [338] [Optimized methods for composite optimization: a reduction perspective](https://arxiv.org/abs/2506.23756)
*Jinho Bok,Jason M. Altschuler*

Main category: math.OC

TL;DR: 提出从无约束平滑优化导出复合优化优化方法的通用框架，并给出具体应用示例。


<details>
  <summary>Details</summary>
Motivation: 现有优化方法针对特定问题定制，难以扩展到其他设置，需要通用框架。

Method: 利用代数恒等式从无约束平滑优化直接导出复合优化的优化方法。

Result: 实现了近端梯度下降的步长加速现象，给出比FISTA更快的近端优化梯度法收敛率，提出在复合设置下最小化梯度范数的新方法。

Conclusion: 所提通用框架能有效将无约束平滑优化的方法和分析扩展到复合优化，有实际应用价值。

Abstract: Recent advances in convex optimization have leveraged computer-assisted
proofs to develop optimized first-order methods that improve over classical
algorithms. However, each optimized method is specially tailored for a
particular problem setting, and it is a well-documented challenge to extend
optimized methods to other settings due to their highly bespoke design and
analysis. We provide a general framework that derives optimized methods for
composite optimization directly from those for unconstrained smooth
optimization. The derived methods naturally extend the original methods,
generalizing how proximal gradient descent extends gradient descent. The key to
our result is certain algebraic identities that provide a unified and
straightforward way of extending convergence analyses from unconstrained to
composite settings. As concrete examples, we apply our framework to establish
(1) the phenomenon of stepsize acceleration for proximal gradient descent; (2)
a convergence rate for the proximal optimized gradient method which is faster
than FISTA; (3) a new method that improves the state-of-the-art rate for
minimizing gradient norm in the composite setting.

</details>


### [339] [Deep neural networks can provably solve Bellman equations for Markov decision processes without the curse of dimensionality](https://arxiv.org/abs/2506.22851)
*Arnulf Jentzen,Konrad Kleinberg,Thomas Kruse*

Main category: math.OC

TL;DR: 本文构造了无限时间跨度和有限控制集的马尔可夫决策过程（MDP）的Q函数的深度神经网络（DNN）近似，并证明在一定条件下相关Bellman方程的解可用带leaky ReLU激活的DNN在L²意义下近似。


<details>
  <summary>Details</summary>
Motivation: 解决MDPs是顺序决策的基础，Bellman方程的解Q函数是解决MDPs的核心工具，本文旨在为相关Q函数构造DNN近似。

Method: 使用全历史递归多级定点（MLFP）近似方案，利用带leaky ReLU激活的DNN对MDP的收益函数和随机转移动态进行近似。

Result: 若MDP的收益函数和随机转移动态能用带leaky ReLU激活的DNN适当近似，那么相关Bellman方程的解Qd也能用带leaky ReLU激活的DNN在L²意义下近似，且参数数量在状态空间维度和规定误差倒数上最多多项式增长。

Conclusion: 证明了用带leaky ReLU激活的DNN对无限时间跨度和有限控制集的MDP的Q函数进行近似的可行性和相关性质。

Abstract: Discrete time stochastic optimal control problems and Markov decision
processes (MDPs) are fundamental models for sequential decision-making under
uncertainty and as such provide the mathematical framework underlying
reinforcement learning theory. A central tool for solving MDPs is the Bellman
equation and its solution, the so-called $Q$-function. In this article, we
construct deep neural network (DNN) approximations for $Q$-functions associated
to MDPs with infinite time horizon and finite control set $A$. More
specifically, we show that if the the payoff function and the random transition
dynamics of the MDP can be suitably approximated by DNNs with leaky rectified
linear unit (ReLU) activation, then the solutions $Q_d\colon \mathbb R^d\to
\mathbb R^{|A|}$, $d\in \mathbb{N}$, of the associated Bellman equations can
also be approximated in the $L^2$-sense by DNNs with leaky ReLU activation
whose numbers of parameters grow at most polynomially in both the dimension
$d\in \mathbb{N}$ of the state space and the reciprocal $1/\varepsilon$ of the
prescribed error $\varepsilon\in (0,1)$. Our proof relies on the recently
introduced full-history recursive multilevel fixed-point (MLFP) approximation
scheme.

</details>


### [340] [Hindsight-Guided Momentum (HGM) Optimizer: An Approach to Adaptive Learning Rate](https://arxiv.org/abs/2506.22479)
*Krisanu Sarkar*

Main category: math.OC

TL;DR: 提出HGM优化算法，基于更新方向一致性自适应调整学习率，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统自适应方法仅用梯度大小调整学习动态，忽略重要几何线索，如梯度方向信息。

Method: 引入后见机制，评估当前梯度和累积动量的余弦相似度，根据方向一致性调整学习率。

Result: 得到更具响应性的优化器，在损失表面平滑区域加速收敛，在复杂区域保持稳定，且保留计算和内存效率。

Conclusion: HGM能更智能响应优化格局，在非凸场景（如深度神经网络训练）中有效改进现有方法。

Abstract: We introduce Hindsight-Guided Momentum (HGM), a first-order optimization
algorithm that adaptively scales learning rates based on the directional
consistency of recent updates. Traditional adaptive methods, such as Adam or
RMSprop , adapt learning dynamics using only the magnitude of gradients, often
overlooking important geometric cues.Geometric cues refer to directional
information, such as the alignment between current gradients and past updates,
which reflects the local curvature and consistency of the optimization path.
HGM addresses this by incorporating a hindsight mechanism that evaluates the
cosine similarity between the current gradient and accumulated momentum. This
allows it to distinguish between coherent and conflicting gradient directions,
increasing the learning rate when updates align and reducing it in regions of
oscillation or noise. The result is a more responsive optimizer that
accelerates convergence in smooth regions of the loss surface while maintaining
stability in sharper or more erratic areas. Despite this added adaptability,
the method preserves the computational and memory efficiency of existing
optimizers.By more intelligently responding to the structure of the
optimization landscape, HGM provides a simple yet effective improvement over
existing approaches, particularly in non-convex settings like that of deep
neural network training.

</details>


### [341] [Consensus-based optimization for closed-box adversarial attacks and a connection to evolution strategies](https://arxiv.org/abs/2506.24048)
*Tim Roith,Leon Bungert,Philipp Wacker*

Main category: math.OC

TL;DR: 研究基于共识的优化（CBO）在黑盒对抗攻击中的应用，建立其与自然进化策略（NES）的联系并实验对比性能。


<details>
  <summary>Details</summary>
Motivation: 在黑盒对抗攻击场景下研究CBO，利用其无梯度优化的特性解决攻击问题。

Method: 建立共识跳跃与NES的联系，并将二者与基于梯度的优化方案进行严格关联，同时进行全面实验研究。

Result: 实验表明，尽管概念相似，但CBO在某些场景下能超越NES和其他进化策略。

Conclusion: CBO在黑盒对抗攻击的特定场景中有更好的性能表现。

Abstract: Consensus-based optimization (CBO) has established itself as an efficient
gradient-free optimization scheme, with attractive mathematical properties,
such as mean-field convergence results for non-convex loss functions. In this
work, we study CBO in the context of closed-box adversarial attacks, which are
imperceptible input perturbations that aim to fool a classifier, without
accessing its gradient. Our contribution is to establish a connection between
the so-called consensus hopping as introduced by Riedl et al. and natural
evolution strategies (NES) commonly applied in the context of adversarial
attacks and to rigorously relate both methods to gradient-based optimization
schemes. Beyond that, we provide a comprehensive experimental study that shows
that despite the conceptual similarities, CBO can outperform NES and other
evolutionary strategies in certain scenarios.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [342] [Diversity by Design: Addressing Mode Collapse Improves scRNA-seq Perturbation Modeling on Well-Calibrated Metrics](https://arxiv.org/abs/2506.22641)
*Gabriel M. Mejia,Henry E. Miller,Francis J. A. Leblanc,Bo Wang,Brendan Swain,Lucas Paulo de Lima Camillo*

Main category: q-bio.GN

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent benchmarks reveal that models for single-cell perturbation response
are often outperformed by simply predicting the dataset mean. We trace this
anomaly to a metric artifact: control-referenced deltas and unweighted error
metrics reward mode collapse whenever the control is biased or the biological
signal is sparse. Large-scale \textit{in silico} simulations and analysis of
two real-world perturbation datasets confirm that shared reference shifts, not
genuine biological change, drives high performance in these evaluations. We
introduce differentially expressed gene (DEG)-aware metrics, weighted
mean-squared error (WMSE) and weighted delta $R^{2}$ ($R^{2}_{w}(\Delta)$) with
respect to all perturbations, that measure error in niche signals with high
sensitivity. We further introduce negative and positive performance baselines
to calibrate these metrics. With these improvements, the mean baseline sinks to
null performance while genuine predictors are correctly rewarded. Finally, we
show that using WMSE as a loss function reduces mode collapse and improves
model performance.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [343] [From Model Design to Organizational Design: Complexity Redistribution and Trade-Offs in Generative AI](https://arxiv.org/abs/2506.22440)
*Sharique Hasan,Alexander Oettl,Sampsa Samila*

Main category: cs.CY

TL;DR: 本文提出GAS框架分析大语言模型对组织和竞争战略的重塑，指出AI使用中复杂性的转移及应对策略。


<details>
  <summary>Details</summary>
Motivation: 传统将AI视为输入成本降低的观点忽视关键动态，需分析大语言模型对组织和竞争战略的影响。

Method: 引入Generality - Accuracy - Simplicity (GAS)框架进行分析。

Result: 大语言模型虽提供简单界面，但将复杂性转移到组织，产生新管理挑战；竞争优势源于掌握重新分配的复杂性。

Conclusion: 该研究通过阐明可扩展认知如何转移复杂性，重新定义技术集成条件，推动了AI战略发展。

Abstract: This paper introduces the Generality-Accuracy-Simplicity (GAS) framework to
analyze how large language models (LLMs) are reshaping organizations and
competitive strategy. We argue that viewing AI as a simple reduction in input
costs overlooks two critical dynamics: (a) the inherent trade-offs among
generality, accuracy, and simplicity, and (b) the redistribution of complexity
across stakeholders. While LLMs appear to defy the traditional trade-off by
offering high generality and accuracy through simple interfaces, this
user-facing simplicity masks a significant shift of complexity to
infrastructure, compliance, and specialized personnel. The GAS trade-off,
therefore, does not disappear but is relocated from the user to the
organization, creating new managerial challenges, particularly around accuracy
in high-stakes applications. We contend that competitive advantage no longer
stems from mere AI adoption, but from mastering this redistributed complexity
through the design of abstraction layers, workflow alignment, and complementary
expertise. This study advances AI strategy by clarifying how scalable cognition
relocates complexity and redefines the conditions for technology integration.

</details>


### [344] [Report on NSF Workshop on Science of Safe AI](https://arxiv.org/abs/2506.22492)
*Rajeev Alur,Greg Durrett,Hadas Kress-Gazit,Corina Păsăreanu,René Vidal*

Main category: cs.CY

TL;DR: 机器学习发展带来新机遇，但AI模型不透明且缺乏安全保障，为此组织研讨会，报告提出下一代AI系统研究议程。


<details>
  <summary>Details</summary>
Motivation: 当前复杂AI模型推理和运作不透明、无安全保障，为解决如何开发安全可信AI系统的科学挑战，组织研讨会。

Method: 组织由NSF SLES项目资助者和研究AI安全的更广泛研究人员参加的研讨会，通过工作组讨论不同安全方面问题。

Result: 产出关于研讨会讨论的报告。

Conclusion: 提出专注于开发理论、方法和工具的新研究议程，为下一代AI系统奠定基础。

Abstract: Recent advances in machine learning, particularly the emergence of foundation
models, are leading to new opportunities to develop technology-based solutions
to societal problems. However, the reasoning and inner workings of today's
complex AI models are not transparent to the user, and there are no safety
guarantees regarding their predictions. Consequently, to fulfill the promise of
AI, we must address the following scientific challenge: how to develop AI-based
systems that are not only accurate and performant but also safe and
trustworthy?
  The criticality of safe operation is particularly evident for autonomous
systems for control and robotics, and was the catalyst for the Safe Learning
Enabled Systems (SLES) program at NSF. For the broader class of AI
applications, such as users interacting with chatbots and clinicians receiving
treatment recommendations, safety is, while no less important, less
well-defined with context-dependent interpretations. This motivated the
organization of a day-long workshop, held at University of Pennsylvania on
February 26, 2025, to bring together investigators funded by the NSF SLES
program with a broader pool of researchers studying AI safety. This report is
the result of the discussions in the working groups that addressed different
aspects of safety at the workshop. The report articulates a new research agenda
focused on developing theory, methods, and tools that will provide the
foundations of the next generation of AI-enabled systems.

</details>


### [345] [Mitigating Gambling-Like Risk-Taking Behaviors in Large Language Models: A Behavioral Economics Approach to AI Safety](https://arxiv.org/abs/2506.22496)
*Y. Du*

Main category: cs.CY

TL;DR: 研究发现大语言模型有类似赌博心理的冒险行为，提出RARG框架缓解，实验显示冒险行为减少。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型存在的类似赌博心理的系统冒险行为，如过度自信偏差、追损倾向和概率误判等问题。

Method: 提出RARG框架，通过风险校准训练、损失厌恶机制和不确定性感知决策解决行为偏差，引入基于赌博心理学实验的评估范式。

Result: 实验结果显示，过度自信偏差减少18.7%，追损倾向降低24.3%，在不同场景下风险校准有所改善。

Conclusion: 建立了首个理解和缓解人工智能系统中赌博心理模式的系统框架。

Abstract: Large Language Models (LLMs) exhibit systematic risk-taking behaviors
analogous to those observed in gambling psychology, including overconfidence
bias, loss-chasing tendencies, and probability misjudgment. Drawing from
behavioral economics and prospect theory, we identify and formalize these
"gambling-like" patterns where models sacrifice accuracy for high-reward
outputs, exhibit escalating risk-taking after errors, and systematically
miscalibrate uncertainty. We propose the Risk-Aware Response Generation (RARG)
framework, incorporating insights from gambling research to address these
behavioral biases through risk-calibrated training, loss-aversion mechanisms,
and uncertainty-aware decision making. Our approach introduces novel evaluation
paradigms based on established gambling psychology experiments, including AI
adaptations of the Iowa Gambling Task and probability learning assessments.
Experimental results demonstrate measurable reductions in gambling-like
behaviors: 18.7\% decrease in overconfidence bias, 24.3\% reduction in
loss-chasing tendencies, and improved risk calibration across diverse
scenarios. This work establishes the first systematic framework for
understanding and mitigating gambling psychology patterns in AI systems.

</details>


### [346] [Peer Review as Structured Commentary: Immutable Identity, Public Dialogue, and Reproducible Scholarship](https://arxiv.org/abs/2506.22497)
*Craig Steven Wright*

Main category: cs.CY

TL;DR: 本文将同行评审重新概念化为结构化公开评论，提出基于开放评论的学术评估系统，结合区块链和AI。


<details>
  <summary>Details</summary>
Motivation: 传统学术验证受匿名性、延迟和把关等问题阻碍。

Method: 利用区块链实现不可变审计跟踪，利用AI进行迭代综合，设计一个激励知识贡献、捕捉认知演变并实现可追溯声誉动态的框架。

Result: 设计出一个适用于多领域的学术评估系统。

Conclusion: 该模型可将学术知识重新定义为一个动态过程而非静态凭证。

Abstract: This paper reconceptualises peer review as structured public commentary.
Traditional academic validation is hindered by anonymity, latency, and
gatekeeping. We propose a transparent, identity-linked, and reproducible system
of scholarly evaluation anchored in open commentary. Leveraging blockchain for
immutable audit trails and AI for iterative synthesis, we design a framework
that incentivises intellectual contribution, captures epistemic evolution, and
enables traceable reputational dynamics. This model empowers fields from
computational science to the humanities, reframing academic knowledge as a
living process rather than a static credential.

</details>


### [347] [Ask before you Build: Rethinking AI-for-Good in Human Trafficking Interventions](https://arxiv.org/abs/2506.22512)
*Pratheeksha Nair,Gabriel Lefebvre,Sophia Garrel,Maryam Molamohammadi,Reihaneh Rabbany*

Main category: cs.CY

TL;DR: 本文介绍激进质疑（RQ）框架作为五步预项目伦理评估工具，以批判性评估是否应构建AI，通过人口贩卖案例展示其作用且可推广到其他领域。


<details>
  <summary>Details</summary>
Motivation: 指出AI向善倡议中的技术解决方案主义在人口贩卖等领域存在风险，需要工具来批判性评估AI构建。

Method: 引入RQ框架作为五步预项目伦理评估工具，并通过人口贩卖的案例研究进行验证。

Result: RQ框架揭示了被忽视的社会文化复杂性，引导从基于监控的干预转向幸存者赋权工具。

Conclusion: RQ框架虽在人口贩卖背景下开发，但五步结构可推广到其他领域，应置于挑战工具主义规范、强调关系性和反思性责任的AI伦理哲学中。

Abstract: AI for good initiatives often rely on the assumption that technical
interventions can resolve complex social problems. In the context of human
trafficking (HT), such techno-solutionism risks oversimplifying exploitation,
reinforcing power imbalances and causing harm to the very communities AI claims
to support. In this paper, we introduce the Radical Questioning (RQ) framework
as a five step, pre-project ethical assessment tool to critically evaluate
whether AI should be built at all, especially in domains involving marginalized
populations and entrenched systemic injustice. RQ does not replace principles
based ethics but precedes it, offering an upstream, deliberative space to
confront assumptions, map power, and consider harms before design. Using a case
study in AI for HT, we demonstrate how RQ reveals overlooked sociocultural
complexities and guides us away from surveillance based interventions toward
survivor empowerment tools. While developed in the context of HT, RQ's five
step structure can generalize to other domains, though the specific questions
must be contextual. This paper situates RQ within a broader AI ethics
philosophy that challenges instrumentalist norms and centers relational,
reflexive responsibility.

</details>


### [348] [Red Teaming for Generative AI, Report on a Copyright-Focused Exercise Completed in an Academic Medical Center](https://arxiv.org/abs/2506.22523)
*James Wen,Sahil Nalawade,Zhiwei Liang,Catherine Bielick,Marisa Ferrara Boston,Alexander Chowdhury,Adele Collin,Luigi De Angelis,Jacob Ellen,Heather Frase,Rodrigo R. Gameiro,Juan Manuel Gutierrez,Pooja Kadam,Murat Keceli,Srikanth Krishnamurthy,Anne Kwok,Yanan Lance Lu,Heather Mattie,Liam G. McCoy,Katherine Miller,Allison C. Morgan,Marlene Louisa Moerig,Trang Nguyen,Alexander Owen-Post,Alex D. Ruiz,Sreekar Reddy Puchala,Soujanya Samineni,Takeshi Tohyama,Varun Ullanat,Carmine Valenza,Camilo Velez,Pengcheng Wang,Anna Wuest,Yuxiang Zhou,Yingde Zhu,Jason M. Johnson,Jennifer Willcox,Francis J. Vitiello,Leo Anthony G. Celi,Renato Umeton*

Main category: cs.CY

TL;DR: 达纳 - 法伯癌症研究所与微软合作创建内部AI工具GPT4DFCI，通过红队测试评估其输出版权数据情况，发现问题后推出缓解策略，并希望推动更多AI工具压力测试。


<details>
  <summary>Details</summary>
Motivation: 评估支持GPT4DFCI的底层GPT模型是否会输出版权数据。

Method: 举办红队测试活动，团队专注于复制书籍、新闻文章、科学文章和电子健康记录的内容。

Result: GPT4DFCI能识别版权材料并复制书籍原文，无法复制目标新闻、科学文章和电子健康记录内容，但存在编造情况。

Conclusion: 推出缓解策略，希望引发更多对AI软件工具合法和道德使用范围的压力测试。

Abstract: Generative AI is present in multiple industries. Dana-Farber Cancer
Institute, in partnership with Microsoft, has created an internal AI tool,
GPT4DFCI. Together we hosted a red teaming event to assess whether the
underlying GPT models that support the tool would output copyrighted data. Our
teams focused on reproducing content from books, news articles, scientific
articles, and electronic health records. We found isolated instances where
GPT4DFCI was able to identify copyrighted material and reproduce exact quotes
from famous books which indicates that copyrighted material was in the training
data. The model was not able to reproduce content from our target news article,
scientific article, or electronic health records. However, there were instances
of fabrication. As a result of this event, a mitigation strategy is in
production in GPT4DFCI v2.8.2, deployed on January 21, 2025. We hope this
report leads to similar events in which AI software tools are stress-tested to
assess the perimeter of their legal and ethical usage.

</details>


### [349] [A Detailed Factor Analysis for the Political Compass Test: Navigating Ideologies of Large Language Models](https://arxiv.org/abs/2506.22493)
*Sadia Kamal,Lalu Prasad Yadav Prakash,S M Rafiuddin,Mohammed Rakib,Arunkumar Bagavathi,Atriya Sen,Sagnik Ray Choudhury*

Main category: cs.CY

TL;DR: 研究表明标准生成参数对大语言模型PCT分数影响不大，提示变化和微调等外部因素有影响，且对含不同政治内容数据集微调时PCT分数无差异，需深入研究PCT测试有效性及政治倾向编码机制。


<details>
  <summary>Details</summary>
Motivation: 基于对PCT测试有效性的研究，探究影响大语言模型PCT分数的因素，以评估PCT及类似测试的有效性。

Method: 研究标准生成参数、提示变化、微调以及不同政治内容数据集微调对大语言模型PCT分数的影响。

Result: 标准生成参数对模型PCT分数无显著影响；提示变化和微调等外部因素单独或组合会影响分数；对不同政治内容数据集微调时，PCT分数无差异。

Conclusion: 需要深入研究PCT及类似测试的有效性，以及大语言模型中政治倾向的编码机制。

Abstract: Political Compass Test (PCT) or similar questionnaires have been used to
quantify LLM's political leanings. Building on a recent line of work that
examines the validity of PCT tests, we demonstrate that variation in standard
generation parameters does not significantly impact the models' PCT scores.
However, external factors such as prompt variations and fine-tuning
individually and in combination affect the same. Finally, we demonstrate that
when models are fine-tuned on text datasets with higher political content than
others, the PCT scores are not differentially affected. This calls for a
thorough investigation into the validity of PCT and similar tests, as well as
the mechanism by which political leanings are encoded in LLMs.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [350] [Validation of AI-Based 3D Human Pose Estimation in a Cyber-Physical Environment](https://arxiv.org/abs/2506.23739)
*Lisa Marie Otto,Michael Kaiser,Daniel Seebacher,Steffen Müller*

Main category: cs.RO

TL;DR: 本文提出结合ViL测试台和运动实验室的测试环境，验证HPE方法，分析其在真实和虚拟场景的准确性，结果显示稳定运动模式下HPE一致性强，动态运动和遮挡时有误差，有助于完善CP测试方法。


<details>
  <summary>Details</summary>
Motivation: 确保自动驾驶系统与城市环境中弱势道路使用者的安全和真实交互，需要先进测试方法。

Method: 结合ViL测试台和运动实验室，通过比较真实和虚拟场景验证HPE方法，用商用单目相机3D骨骼检测AI研究全身运动感知，在虚幻引擎5生成虚拟场景，分析检测可靠性、运动轨迹和关节估计稳定性。

Result: 稳定运动模式下真实和CP测试条件的HPE强对齐，动态运动和遮挡时尤其复杂骑行姿势有显著误差。

Conclusion: 研究结果有助于完善CP测试方法以评估下一代基于AI的车辆感知，增强自动驾驶车辆与VRUs在CP环境的交互模型。

Abstract: Ensuring safe and realistic interactions between automated driving systems
and vulnerable road users (VRUs) in urban environments requires advanced
testing methodologies. This paper presents a test environment that combines a
Vehiclein-the-Loop (ViL) test bench with a motion laboratory, demonstrating the
feasibility of cyber-physical (CP) testing of vehicle-pedestrian and
vehicle-cyclist interactions. Building upon previous work focused on pedestrian
localization, we further validate a human pose estimation (HPE) approach
through a comparative analysis of real-world (RW) and virtual representations
of VRUs. The study examines the perception of full-body motion using a
commercial monocular camera-based 3Dskeletal detection AI. The virtual scene is
generated in Unreal Engine 5, where VRUs are animated in real time and
projected onto a screen to stimulate the camera. The proposed stimulation
technique ensures the correct perspective, enabling realistic vehicle
perception. To assess the accuracy and consistency of HPE across RW and CP
domains, we analyze the reliability of detections as well as variations in
movement trajectories and joint estimation stability. The validation includes
dynamic test scenarios where human avatars, both walking and cycling, are
monitored under controlled conditions. Our results show a strong alignment in
HPE between RW and CP test conditions for stable motion patterns, while notable
inaccuracies persist under dynamic movements and occlusions, particularly for
complex cyclist postures. These findings contribute to refining CP testing
approaches for evaluating next-generation AI-based vehicle perception and to
enhancing interaction models of automated vehicles and VRUs in CP environments.

</details>


### [351] [Moving Matter: Using a Single, Simple Robot to Reconfigure a Connected Set of Building Blocks](https://arxiv.org/abs/2506.23333)
*Javier Garcia,Jonas Friemel,Ramin Kosfeld,Michael Yannuzzi,Peter Kramer,Christian Rieck,Christian Scheffer,Arne Schmidt,Harm Kube,Dan Biediger,Sándor P. Fekete,Aaron T. Becker*

Main category: cs.RO

TL;DR: 本文用单机器人实现并评估不同瓦片重配置方法，实现从初始到目标形状转换，还对Becker等人算法进行实现和评估，并与现有启发式算法比较。


<details>
  <summary>Details</summary>
Motivation: 实现连接的瓦片排列重配置为目标形状，评估不同方法的可行性和性能。

Method: 使用单机器人移动瓦片，实现瓦片重配置，实现Becker等人算法，并将其与两种现有启发式算法在模拟和实际场景中对比。

Result: 未提及。

Conclusion: 未提及。

Abstract: We implement and evaluate different methods for the reconfiguration of a
connected arrangement of tiles into a desired target shape, using a single
active robot that can move along the tile structure. This robot can pick up,
carry, or drop off one tile at a time, but it must maintain a single connected
configuration at all times.
  Becker et al. (CCCG 2025) recently proposed an algorithm that uses histograms
as canonical intermediate configurations, guaranteeing performance within a
constant factor of the optimal solution if the start and target configuration
are well-separated. We implement and evaluate this algorithm, both in a
simulated and practical setting, using an inchworm type robot to compare it
with two existing heuristic algorithms.

</details>


### [352] [Pixels-to-Graph: Real-time Integration of Building Information Models and Scene Graphs for Semantic-Geometric Human-Robot Understanding](https://arxiv.org/abs/2506.22593)
*Antonello Longo,Chanyoung Chung,Matteo Palieri,Sung-Kyun Kim,Ali Agha,Cataldo Guaragnella,Shehryar Khattak*

Main category: cs.RO

TL;DR: 本文提出Pixels-to-Graph (Pix2G) 方法，可在资源受限的机器人平台上实时从图像像素和LiDAR地图生成结构化场景图，以实现未知环境的自主探索。


<details>
  <summary>Details</summary>
Motivation: 为实现高效的人机协作，需要弥合人类可读的2D BIM与机器人3D地图之间的差距。

Method: 提出Pix2G方法，在CPU上执行所有操作，输出去噪的2D顶视环境地图和结构分割的3D点云，并通过多层图连接信息。

Result: 使用NASA JPL NeBula - Spot机器人在现实世界的杂乱车库和城市办公室环境中进行实时自主探索和地图绘制实验。

Conclusion: Pix2G方法能在资源受限的机器人平台上实时生成结构化场景图，可用于未知环境的自主探索。

Abstract: Autonomous robots are increasingly playing key roles as support platforms for
human operators in high-risk, dangerous applications. To accomplish challenging
tasks, an efficient human-robot cooperation and understanding is required.
While typically robotic planning leverages 3D geometric information, human
operators are accustomed to a high-level compact representation of the
environment, like top-down 2D maps representing the Building Information Model
(BIM). 3D scene graphs have emerged as a powerful tool to bridge the gap
between human readable 2D BIM and the robot 3D maps. In this work, we introduce
Pixels-to-Graph (Pix2G), a novel lightweight method to generate structured
scene graphs from image pixels and LiDAR maps in real-time for the autonomous
exploration of unknown environments on resource-constrained robot platforms. To
satisfy onboard compute constraints, the framework is designed to perform all
operation on CPU only. The method output are a de-noised 2D top-down
environment map and a structure-segmented 3D pointcloud which are seamlessly
connected using a multi-layer graph abstracting information from object-level
up to the building-level. The proposed method is quantitatively and
qualitatively evaluated during real-world experiments performed using the NASA
JPL NeBula-Spot legged robot to autonomously explore and map cluttered garage
and urban office like environments in real-time.

</details>


### [353] [DriveBLIP2: Attention-Guided Explanation Generation for Complex Driving Scenarios](https://arxiv.org/abs/2506.22494)
*Shihong Ling,Yue Wan,Xiaowei Jia,Na Du*

Main category: cs.RO

TL;DR: 提出DriveBLIP2框架，用注意力图生成器解决现有模型在自动驾驶场景理解问题，评估显示解释质量提升。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在理解复杂多对象环境、实时自动驾驶应用中存在困难，难以快速识别关键对象。

Method: 基于BLIP2 - OPT架构构建DriveBLIP2框架，提出注意力图生成器突出关键帧中与驾驶决策相关的重要对象。

Result: 在DRAMA数据集评估中，相比基线模型，DriveBLIP2的BLEU、ROUGE、CIDEr和SPICE分数更高，解释质量显著提升。

Conclusion: 有针对性的注意力机制在视觉语言模型中对提高实时自动驾驶的可解释性有潜力。

Abstract: This paper introduces a new framework, DriveBLIP2, built upon the BLIP2-OPT
architecture, to generate accurate and contextually relevant explanations for
emerging driving scenarios. While existing vision-language models perform well
in general tasks, they encounter difficulties in understanding complex,
multi-object environments, particularly in real-time applications such as
autonomous driving, where the rapid identification of key objects is crucial.
To address this limitation, an Attention Map Generator is proposed to highlight
significant objects relevant to driving decisions within critical video frames.
By directing the model's focus to these key regions, the generated attention
map helps produce clear and relevant explanations, enabling drivers to better
understand the vehicle's decision-making process in critical situations.
Evaluations on the DRAMA dataset reveal significant improvements in explanation
quality, as indicated by higher BLEU, ROUGE, CIDEr, and SPICE scores compared
to baseline models. These findings underscore the potential of targeted
attention mechanisms in vision-language models for enhancing explainability in
real-time autonomous driving.

</details>


### [354] [Scenario-Based Hierarchical Reinforcement Learning for Automated Driving Decision Making](https://arxiv.org/abs/2506.23023)
*M. Youssef Abdelhamid,Lennart Vater,Zlatan Ajanovic*

Main category: cs.RO

TL;DR: 提出SAD - RL框架解决高自动化驾驶系统决策算法难题，实验证明其有效性及关键因素。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在复杂驾驶任务中缺乏泛化性和学习效率，开发高自动化驾驶系统决策算法有挑战。

Method: 提出SAD - RL框架，在基于场景的环境中集成分层策略的强化学习，高层策略选机动模板，低层控制逻辑评估执行。

Result: 使用SAD - RL框架训练的智能体可在简单和有挑战的情况下高效实现安全行为。

Conclusion: 分层强化学习和场景多样性对取得实验结果至关重要。

Abstract: Developing decision-making algorithms for highly automated driving systems
remains challenging, since these systems have to operate safely in an open and
complex environments. Reinforcement Learning (RL) approaches can learn
comprehensive decision policies directly from experience and already show
promising results in simple driving tasks. However, current approaches fail to
achieve generalizability for more complex driving tasks and lack learning
efficiency. Therefore, we present Scenario-based Automated Driving
Reinforcement Learning (SAD-RL), the first framework that integrates
Reinforcement Learning (RL) of hierarchical policy in a scenario-based
environment. A high-level policy selects maneuver templates that are evaluated
and executed by a low-level control logic. The scenario-based environment
allows to control the training experience for the agent and to explicitly
introduce challenging, but rate situations into the training process. Our
experiments show that an agent trained using the SAD-RL framework can achieve
safe behaviour in easy as well as challenging situations efficiently. Our
ablation studies confirmed that both HRL and scenario diversity are essential
for achieving these results.

</details>


### [355] [Mode Collapse Happens: Evaluating Critical Interactions in Joint Trajectory Prediction Models](https://arxiv.org/abs/2506.23164)
*Maarten Hugenholtz,Anna Meszaros,Jens Kober,Zlatan Ajanovic*

Main category: cs.RO

TL;DR: 提出评估联合轨迹预测中模式崩溃的框架，测试表明模式崩溃存在，框架有望推动模型发展提升自动驾驶安全。


<details>
  <summary>Details</summary>
Motivation: 现有模型存在模式崩溃风险，现有方法忽视代理间交互模式多样性，传统评估指标有局限性且未明确评估模式崩溃。

Method: 提出新评估框架，引入模式崩溃、模式正确性和覆盖率指标，强调预测的顺序维度，测试四个多智能体轨迹预测模型。

Result: 模式崩溃确实存在，虽靠近交互事件时预测精度提高，但仍有模型无法正确预测交互模式的情况。

Conclusion: 框架有助于研究人员获得新见解，推动更一致准确的预测模型发展，提升自动驾驶系统安全性。

Abstract: Autonomous Vehicle decisions rely on multimodal prediction models that
account for multiple route options and the inherent uncertainty in human
behavior. However, models can suffer from mode collapse, where only the most
likely mode is predicted, posing significant safety risks. While existing
methods employ various strategies to generate diverse predictions, they often
overlook the diversity in interaction modes among agents. Additionally,
traditional metrics for evaluating prediction models are dataset-dependent and
do not evaluate inter-agent interactions quantitatively. To our knowledge, none
of the existing metrics explicitly evaluates mode collapse. In this paper, we
propose a novel evaluation framework that assesses mode collapse in joint
trajectory predictions, focusing on safety-critical interactions. We introduce
metrics for mode collapse, mode correctness, and coverage, emphasizing the
sequential dimension of predictions. By testing four multi-agent trajectory
prediction models, we demonstrate that mode collapse indeed happens. When
looking at the sequential dimension, although prediction accuracy improves
closer to interaction events, there are still cases where the models are unable
to predict the correct interaction mode, even just before the interaction mode
becomes inevitable. We hope that our framework can help researchers gain new
insights and advance the development of more consistent and accurate prediction
models, thus enhancing the safety of autonomous driving systems.

</details>


### [356] [Benchmarking Generalizable Bimanual Manipulation: RoboTwin Dual-Arm Collaboration Challenge at CVPR 2025 MEIS Workshop](https://arxiv.org/abs/2506.23351)
*Tianxing Chen,Kaixuan Wang,Zhaohui Yang,Yuhao Zhang,Zanxin Chen,Baijun Chen,Wanxi Dong,Ziyuan Liu,Dong Chen,Tianshuo Yang,Haibao Yu,Xiaokang Yang,Yusen Qin,Zhiqiang Xie,Yao Mu,Ping Luo,Tian Nian,Weiliang Deng,Yiheng Ge,Yibin Liu,Zixuan Li,Dehui Wang,Zhixuan Liang,Haohui Xie,Rijie Zeng,Yunfei Ge,Peiqing Cong,Guannan He,Zhaoming Han,Ruocheng Yin,Jingxiang Guo,Lunkai Lin,Tianling Xu,Hongzhe Bi,Xuewu Lin,Tianwei Lin,Shujie Luo,Keyu Li,Ziyan Zhao,Ke Fan,Heyang Xu,Bo Peng,Wenlong Gao,Dongjiang Li,Feng Jin,Hui Shen,Jinming Li,Chaowei Cui,Yuchen,Yaxin Peng,Lingdong Zeng,Wenlong Dong,Tengfei Li,Weijie Ke,Jun Chen,Erdemt Bao,Tian Lan,Tenglong Liu,Jin Yang,Huiping Zhuang,Baozhi Jia,Shuai Zhang,Zhengfeng Zou,Fangheng Guan,Tianyi Jia,Ke Zhou,Hongjiu Zhang,Yating Han,Cheng Fang,Yixian Zou,Chongyang Xu,Qinglun Zhang,Shen Cheng,Xiaohe Wang,Ping Tan,Haoqiang Fan,Shuaicheng Liu,Jiaheng Chen,Chuxuan Huang,Chengliang Lin,Kaijun Luo,Boyu Yue,Yi Liu,Jinyu Chen,Zichang Tan,Liming Deng,Shuo Xu,Zijian Cai,Shilong Yin,Hao Wang,Hongshan Liu,Tianyang Li,Long Shi,Ran Xu,Huilin Xu,Zhengquan Zhang,Congsheng Xu,Jinchang Yang,Feng Xu*

Main category: cs.RO

TL;DR: 本文介绍RoboTwin双臂协作挑战，该挑战旨在推动具身AI中双臂协作系统发展，吸引众多团队参与，产生优秀方案并获得有价值见解。


<details>
  <summary>Details</summary>
Motivation: 具身AI需自主系统在复杂物理环境中工作，双臂协作系统对处理复杂任务至关重要，因此举办挑战推动研究。

Method: 基于RoboTwin仿真平台和AgileX COBOT - Magic机器人平台，设置仿真轮1、仿真轮2和真实世界轮三个阶段，包含17个双臂操作任务。

Result: 吸引64个全球团队和超400名参与者，产生SEM和AnchorDP3等优秀方案。

Conclusion: 该报告总结挑战情况，为未来鲁棒且通用的双手操作策略研究提供支持。

Abstract: Embodied Artificial Intelligence (Embodied AI) is an emerging frontier in
robotics, driven by the need for autonomous systems that can perceive, reason,
and act in complex physical environments. While single-arm systems have shown
strong task performance, collaborative dual-arm systems are essential for
handling more intricate tasks involving rigid, deformable, and
tactile-sensitive objects. To advance this goal, we launched the RoboTwin
Dual-Arm Collaboration Challenge at the 2nd MEIS Workshop, CVPR 2025. Built on
the RoboTwin Simulation platform (1.0 and 2.0) and the AgileX COBOT-Magic Robot
platform, the competition consisted of three stages: Simulation Round 1,
Simulation Round 2, and a final Real-World Round. Participants totally tackled
17 dual-arm manipulation tasks, covering rigid, deformable, and tactile-based
scenarios. The challenge attracted 64 global teams and over 400 participants,
producing top-performing solutions like SEM and AnchorDP3 and generating
valuable insights into generalizable bimanual policy learning. This report
outlines the competition setup, task design, evaluation methodology, key
findings and future direction, aiming to support future research on robust and
generalizable bimanual manipulation policies. The Challenge Webpage is
available at https://robotwin-benchmark.github.io/cvpr-2025-challenge/.

</details>


### [357] [MGPRL: Distributed Multi-Gaussian Processes for Wi-Fi-based Multi-Robot Relative Localization in Large Indoor Environments](https://arxiv.org/abs/2506.23514)
*Sai Krishna Ghanta,Ramviyas Parasuraman*

Main category: cs.RO

TL;DR: 提出用于多机器人相对定位的分布式框架MGPRL，用Wi - Fi接入点凸包实现定位，经模拟和实验验证其性能优于现有方法并开源。


<details>
  <summary>Details</summary>
Motivation: 现有多机器人相对定位方法依赖昂贵或短程传感器，有计算开销大、在不连通环境困难等问题，需新方法。

Method: 采用协同区域化多输出高斯过程预测RSSI场，进行多AP定位，结合加权凸包对齐实现相对位姿估计，各机器人在线扫描环境中AP预测RSSI场用于AP定位，通过对齐预测AP位置凸包实现相对定位。

Result: 在ROS模拟和真实实验中评估，MGPRL在定位精度和计算效率上优于现有方法。

Conclusion: MGPRL适用于计算资源有限设备，仅用Wi - Fi RSSI测量，无需预校准和离线指纹，性能良好且已开源。

Abstract: Relative localization is a crucial capability for multi-robot systems
operating in GPS-denied environments. Existing approaches for multi-robot
relative localization often depend on costly or short-range sensors like
cameras and LiDARs. Consequently, these approaches face challenges such as high
computational overhead (e.g., map merging) and difficulties in disjoint
environments. To address this limitation, this paper introduces MGPRL, a novel
distributed framework for multi-robot relative localization using convex-hull
of multiple Wi-Fi access points (AP). To accomplish this, we employ
co-regionalized multi-output Gaussian Processes for efficient Radio Signal
Strength Indicator (RSSI) field prediction and perform uncertainty-aware
multi-AP localization, which is further coupled with weighted convex hull-based
alignment for robust relative pose estimation. Each robot predicts the RSSI
field of the environment by an online scan of APs in its environment, which are
utilized for position estimation of multiple APs. To perform relative
localization, each robot aligns the convex hull of its predicted AP locations
with that of the neighbor robots. This approach is well-suited for devices with
limited computational resources and operates solely on widely available Wi-Fi
RSSI measurements without necessitating any dedicated pre-calibration or
offline fingerprinting. We rigorously evaluate the performance of the proposed
MGPRL in ROS simulations and demonstrate it with real-world experiments,
comparing it against multiple state-of-the-art approaches. The results showcase
that MGPRL outperforms existing methods in terms of localization accuracy and
computational efficiency. Finally, we open source MGPRL as a ROS package
https://github.com/herolab-uga/MGPRL.

</details>


### [358] [Online Human Action Detection during Escorting](https://arxiv.org/abs/2506.23573)
*Siddhartha Mondal,Avik Mitra,Chayan Sarkar*

Main category: cs.RO

TL;DR: 当前护送机器人在复杂环境下护送效果不佳，因缺乏对应数据集和模型，本文提出新神经网络架构，能实现人员重识别和动作预测，评估显示其效果良好。


<details>
  <summary>Details</summary>
Motivation: 当前护送机器人在复杂环境下因对人类运动动力学理解有限，无法有效提供护送服务，且缺乏专门的数据集和能实时进行人员重识别与动作预测的模型。

Method: 提出一种能同时完成人员重识别和动作预测的新型神经网络架构。

Result: 在与强基线的比较评估中，系统展示出卓越的效率和有效性。

Conclusion: 该系统有潜力显著改善复杂现实场景中的机器人护送服务。

Abstract: The deployment of robot assistants in large indoor spaces has seen
significant growth, with escorting tasks becoming a key application. However,
most current escorting robots primarily rely on navigation-focused strategies,
assuming that the person being escorted will follow without issue. In crowded
environments, this assumption often falls short, as individuals may struggle to
keep pace, become obstructed, get distracted, or need to stop unexpectedly. As
a result, conventional robotic systems are often unable to provide effective
escorting services due to their limited understanding of human movement
dynamics. To address these challenges, an effective escorting robot must
continuously detect and interpret human actions during the escorting process
and adjust its movement accordingly. However, there is currently no existing
dataset designed specifically for human action detection in the context of
escorting. Given that escorting often occurs in crowded environments, where
other individuals may enter the robot's camera view, the robot also needs to
identify the specific human it is escorting (the subject) before predicting
their actions. Since no existing model performs both person re-identification
and action prediction in real-time, we propose a novel neural network
architecture that can accomplish both tasks. This enables the robot to adjust
its speed dynamically based on the escortee's movements and seamlessly resume
escorting after any disruption. In comparative evaluations against strong
baselines, our system demonstrates superior efficiency and effectiveness,
showcasing its potential to significantly improve robotic escorting services in
complex, real-world scenarios.

</details>


### [359] [PAC Bench: Do Foundation Models Understand Prerequisites for Executing Manipulation Policies?](https://arxiv.org/abs/2506.23725)
*Atharva Gundawar,Som Sagar,Ransalu Senanayake*

Main category: cs.RO

TL;DR: 论文指出视觉语言模型（VLMs）在机器人操作中缺乏对物理先决条件的理解，引入PAC Bench评估其对核心属性、功能和约束的理解，发现当前VLMs存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 当前VLMs在机器人操作中广泛使用，但缺乏对低级物理先决条件的深入理解，而这些先决条件在训练中常被忽视。

Method: 引入PAC Bench基准，包含超30000条注释的多样化数据集，对VLMs进行评估。

Result: 评估揭示当前VLMs在掌握基本物理概念上存在显著差距。

Conclusion: PAC Bench可作为评估VLMs物理推理能力的标准基准，指导开发更强大、基于物理的机器人应用模型。

Abstract: Vision-Language Models (VLMs) are increasingly pivotal for generalist robot
manipulation, enabling tasks such as physical reasoning, policy generation, and
failure detection. However, their proficiency in these high-level applications
often assumes a deep understanding of low-level physical prerequisites, a
capability that remains largely unverified. For robots to perform actions
reliably, they must comprehend intrinsic object properties (e.g., material,
weight), action affordances (e.g., graspable, stackable), and physical
constraints (e.g., stability, reachability, or an object's state, such as being
closed). Despite the widespread use of VLMs in manipulation tasks, we argue
that off-the-shelf models may lack this granular, physically grounded
understanding, as such prerequisites are often overlooked during training.
  To address this critical gap, we introduce PAC Bench, a comprehensive
benchmark designed to systematically evaluate VLMs on their understanding of
core Properties, Affordances, and Constraints (PAC) from a task executability
perspective. PAC Bench features a diverse dataset with over 30,000 annotations,
comprising 673 real-world images (115 object classes, 15 property types, and 1
to 3 affordances defined per class), 100 real-world humanoid-view scenarios,
and 120 unique simulated constraint scenarios across four tasks.
  Our evaluations reveal significant gaps in the ability of current VLMs to
grasp fundamental physical concepts, highlighting limitations in their
suitability for reliable robot manipulation and pointing to key areas for
targeted research. PAC Bench also serves as a standardized benchmark for
rigorously evaluating physical reasoning in VLMs and guiding the development of
more robust, physically grounded models for robotic applications.
  Project Page: https://pacbench.github.io/

</details>


### [360] [Multi-Timescale Hierarchical Reinforcement Learning for Unified Behavior and Control of Autonomous Driving](https://arxiv.org/abs/2506.23771)
*Guizhe Jin,Zhuoren Li,Bo Leng,Ran Yu,Lu Xiong*

Main category: cs.RO

TL;DR: 提出多时间尺度分层强化学习方法用于自动驾驶，在模拟和真实场景评估中提升了性能。


<details>
  <summary>Details</summary>
Motivation: 多数基于强化学习的自动驾驶方法忽视策略结构设计，单输出短或长时间尺度策略有缺陷。

Method: 采用分层策略结构，统一训练高低层强化学习策略分别输出长时间尺度运动引导和短时间尺度控制命令，用混合动作表示运动引导，设计分层安全机制。

Result: 在基于模拟器和HighD数据集的高速公路多车道场景评估中，显著提升自动驾驶性能，提高驾驶效率、动作一致性和安全性。

Conclusion: 所提出的多时间尺度分层强化学习方法有效可行，能提升自动驾驶性能。

Abstract: Reinforcement Learning (RL) is increasingly used in autonomous driving (AD)
and shows clear advantages. However, most RL-based AD methods overlook policy
structure design. An RL policy that only outputs short-timescale vehicle
control commands results in fluctuating driving behavior due to fluctuations in
network outputs, while one that only outputs long-timescale driving goals
cannot achieve unified optimality of driving behavior and control. Therefore,
we propose a multi-timescale hierarchical reinforcement learning approach. Our
approach adopts a hierarchical policy structure, where high- and low-level RL
policies are unified-trained to produce long-timescale motion guidance and
short-timescale control commands, respectively. Therein, motion guidance is
explicitly represented by hybrid actions to capture multimodal driving
behaviors on structured road and support incremental low-level extend-state
updates. Additionally, a hierarchical safety mechanism is designed to ensure
multi-timescale safety. Evaluation in simulator-based and HighD dataset-based
highway multi-lane scenarios demonstrates that our approach significantly
improves AD performance, effectively increasing driving efficiency, action
consistency and safety.

</details>


### [361] [Adapt Your Body: Mitigating Proprioception Shifts in Imitation Learning](https://arxiv.org/abs/2506.23944)
*Fuhang Kuang,Jiacheng You,Yingdong Hu,Tong Zhang,Chuan Wen,Yang Gao*

Main category: cs.RO

TL;DR: 指出模仿学习中本体感受状态迁移问题，提出用领域适应框架解决，实验证明方法有效


<details>
  <summary>Details</summary>
Motivation: 解决机器人模仿学习中简单纳入本体感受状态导致性能下降的问题

Method: 提出领域适应框架，用Wasserstein距离量化差异，按其比例添加噪声使训练和部署分布对齐

Result: 实验表明方法有效，能利用本体感受并减轻负面影响，优于丢弃本体感受及其他基线方法

Conclusion: 所提方法可增强对本体感受迁移的鲁棒性，提高模仿学习性能

Abstract: Imitation learning models for robotic tasks typically rely on multi-modal
inputs, such as RGB images, language, and proprioceptive states. While
proprioception is intuitively important for decision-making and obstacle
avoidance, simply incorporating all proprioceptive states leads to a surprising
degradation in imitation learning performance. In this work, we identify the
underlying issue as the proprioception shift problem, where the distributions
of proprioceptive states diverge significantly between training and deployment.
To address this challenge, we propose a domain adaptation framework that
bridges the gap by utilizing rollout data collected during deployment. Using
Wasserstein distance, we quantify the discrepancy between expert and rollout
proprioceptive states and minimize this gap by adding noise to both sets of
states, proportional to the Wasserstein distance. This strategy enhances
robustness against proprioception shifts by aligning the training and
deployment distributions. Experiments on robotic manipulation tasks demonstrate
the efficacy of our method, enabling the imitation policy to leverage
proprioception while mitigating its adverse effects. Our approach outperforms
the naive solution which discards proprioception, and other baselines designed
to address distributional shifts.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [362] [Lower bounds for trace estimation via Block Krylov and other methods](https://arxiv.org/abs/2506.22701)
*Shi Jie Yu*

Main category: math.ST

TL;DR: 研究用Hutchinson方法和Block Krylov技术估计矩阵函数迹的理论下界，推导特定函数Krylov步数上界及查询次数下限。


<details>
  <summary>Details</summary>
Motivation: 明确用Hutchinson方法和Block Krylov技术估计矩阵函数迹时，Krylov步数、多项式近似等与估计成本的关系。

Method: 通过分析标量等价多项式近似的上界推导Krylov步数上界，开发查询次数下限。

Result: 得到特定函数如$A^{-1/2}$、$A^{-1}$的Krylov步数上界，以及$	ext{tr}(W^{-p})$的查询次数下限。

Conclusion: 明确了Block Krylov方法步数与近似多项式次数的联系，将迹估计总成本与多项式近似基本限制和计算所需信息联系起来。

Abstract: This paper studies theoretical lower bounds for estimating the trace of a
matrix function, $\text{tr}(f(A))$, focusing on methods that use Hutchinson's
method along with Block Krylov techniques. These methods work by approximating
matrix-vector products like $f(A)V$ using a Block Krylov subspace. This is
closely related to approximating functions with polynomials. We derive
theoretical upper bounds on how many Krylov steps are needed for functions such
as $A^{-1/2}$ and $A^{-1}$ by analyzing the upper bounds from the polynomial
approximation of their scalar equivalent. In addition, we also develop lower
limits on the number of queries needed for trace estimation, specifically for
$\text{tr}(W^{-p})$ where $W$ is a Wishart matrix. Our study clarifies the
connection between the number of steps in Block Krylov methods and the degree
of the polynomial used for approximation. This links the total cost of trace
estimation to basic limits in polynomial approximation and how much information
is needed for the computation.

</details>


### [363] [Sampling and Identity-Testing Without Approximate Tensorization of Entropy](https://arxiv.org/abs/2506.23456)
*William Gay,William He,Nicholas Kocurek,Ryan O'Donnell*

Main category: math.ST

TL;DR: 研究不满足近似张量化熵（ATE）的分布（ATE分布的混合）的身份测试和采样复杂度，取得了Glauber动力学快速混合及高效身份测试器的成果。


<details>
  <summary>Details</summary>
Motivation: 已有研究表明满足ATE的分布在高维统计任务中有优势，而ATE分布的混合不满足ATE，因此研究这类分布的身份测试和采样复杂度。

Method: 分析Glauber动力学从基于数据的初始化开始的情况，对Blanca等人的算法进行简化和改进。

Result: 实现了Glauber动力学从基于数据初始化的快速混合，有最优样本复杂度；给出了ATE分布混合在坐标条件采样访问模型下的高效身份测试器。

Conclusion: 在ATE分布混合的身份测试和采样问题上取得进展，扩展了之前的工作并回答了公开问题。

Abstract: Certain tasks in high-dimensional statistics become easier when the
underlying distribution satisfies a local-to-global property called approximate
tensorization of entropy (ATE). For example, the Glauber dynamics Markov chain
of an ATE distribution mixes fast and can produce approximate samples in a
small amount of time, since such a distribution satisfies a modified
log-Sobolev inequality. Moreover, identity-testing for an ATE distribution
requires few samples if the tester is given coordinate conditional access to
the unknown distribution, as shown by Blanca, Chen, \v{S}tefankovi\v{c}, and
Vigoda (COLT 2023).
  A natural class of distributions that do not satisfy ATE consists of mixtures
of (few) distributions that do satisfy ATE. We study the complexity of
identity-testing and sampling for these distributions. Our main results are the
following:
  1. We show fast mixing of Glauber dynamics from a data-based initialization,
with optimal sample complexity, for mixtures of distributions satisfying
modified log-Sobolev inequalities. This extends work of Huang, Koehler, Lee,
Mohanty, Rajaraman, Vuong, and Wu (STOC 2025, COLT 2025) for mixtures of
distributions satisfying Poincar\'e inequalities.
  2. Answering an open question posed by Blanca et al., we give efficient
identity-testers for mixtures of ATE distributions in the
coordinate-conditional sampling access model. We also give some simplifications
and improvements to the original algorithm of Blanca et al.

</details>


### [364] [On Universality of Non-Separable Approximate Message Passing Algorithms](https://arxiv.org/abs/2506.23010)
*Max Lovig,Tianhao Wang,Zhou Fan*

Main category: math.ST

TL;DR: 本文研究非可分近似消息传递（AMP）算法的普遍性，确定多项式非线性AMP的通用条件，为Lipschitz AMP算法形式化条件，证明许多常见非可分非线性是可近似的。


<details>
  <summary>Details</summary>
Motivation: 现有平均场表征在非可分算法动力学方面大多局限于独立同分布高斯或旋转不变数据，需研究非可分AMP算法的普遍性。

Method: 确定多项式非线性AMP基于有界组合性质（BCP）的通用条件，为Lipschitz AMP算法形式化BCP - 可近似性条件。

Result: 证明许多常见非可分非线性类是BCP - 可近似的，包括局部去噪器、通用信号的谱去噪器等。

Conclusion: 使用这些非可分非线性的AMP算法的状态演化具有普遍性。

Abstract: Mean-field characterizations of first-order iterative algorithms -- including
Approximate Message Passing (AMP), stochastic and proximal gradient descent,
and Langevin diffusions -- have enabled a precise understanding of learning
dynamics in many statistical applications. For algorithms whose non-linearities
have a coordinate-separable form, it is known that such characterizations enjoy
a degree of universality with respect to the underlying data distribution.
However, mean-field characterizations of non-separable algorithm dynamics have
largely remained restricted to i.i.d. Gaussian or rotationally-invariant data.
  In this work, we initiate a study of universality for non-separable AMP
algorithms. We identify a general condition for AMP with polynomial
non-linearities, in terms of a Bounded Composition Property (BCP) for their
representing tensors, to admit a state evolution that holds universally for
matrices with non-Gaussian entries. We then formalize a condition of
BCP-approximability for Lipschitz AMP algorithms to enjoy a similar universal
guarantee. We demonstrate that many common classes of non-separable
non-linearities are BCP-approximable, including local denoisers, spectral
denoisers for generic signals, and compositions of separable functions with
generic linear maps, implying the universality of state evolution for AMP
algorithms employing these non-linearities.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [365] [Investigating an Overfitting and Degeneration Phenomenon in Self-Supervised Multi-Pitch Estimation](https://arxiv.org/abs/2506.23371)
*Frank Cwitkowitz,Zhiyao Duan*

Main category: eess.AS

TL;DR: 本文尝试结合自监督目标扩展经典监督式多音高估计范式，训练有改进但出现过拟合与退化现象并展开研究。


<details>
  <summary>Details</summary>
Motivation: 现有多音高估计方法多基于监督学习，收集标注数据有挑战，自监督技术虽有前景但仍不如监督方法。

Method: 结合基于音高不变性和音高等变性属性的自监督目标，扩展经典监督式多音高估计范式。

Result: 封闭训练条件下有显著改进，但模型会同时对监督数据过拟合且在仅用于自监督的数据上退化。

Conclusion: 展示并研究该现象，给出对潜在问题的见解。

Abstract: Multi-Pitch Estimation (MPE) continues to be a sought after capability of
Music Information Retrieval (MIR) systems, and is critical for many
applications and downstream tasks involving pitch, including music
transcription. However, existing methods are largely based on supervised
learning, and there are significant challenges in collecting annotated data for
the task. Recently, self-supervised techniques exploiting intrinsic properties
of pitch and harmonic signals have shown promise for both monophonic and
polyphonic pitch estimation, but these still remain inferior to supervised
methods. In this work, we extend the classic supervised MPE paradigm by
incorporating several self-supervised objectives based on pitch-invariant and
pitch-equivariant properties. This joint training results in a substantial
improvement under closed training conditions, which naturally suggests that
applying the same objectives to a broader collection of data will yield further
improvements. However, in doing so we uncover a phenomenon whereby our model
simultaneously overfits to the supervised data while degenerating on data used
for self-supervision only. We demonstrate and investigate this and offer our
insights on the underlying problem.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [366] [Exploring Artificial Intelligence Tutor Teammate Adaptability to Harness Discovery Curiosity and Promote Learning in the Context of Interactive Molecular Dynamics](https://arxiv.org/abs/2506.22520)
*Mustafa Demir,Jacob Miratsky,Jonathan Nguyen,Chun Kit Chan,Punya Mishra,Abhishek Singharoy*

Main category: cs.HC

TL;DR: 研究人工智能导师队友对学生在交互式分子动力学任务中的好奇心驱动参与度和学习效果的影响，采用混合方法设计实验，发现高绩效团队表现更好，AI激发好奇心可提升参与度。


<details>
  <summary>Details</summary>
Motivation: 探究人工智能导师队友对学生好奇心驱动的参与度和学习效果的影响，以及其好奇心触发和响应行为的作用。

Method: 使用Wizard - of - Oz范式，人类实验者通过大语言模型动态调整AI行为，采用混合方法探索性设计，让11名高中生参与4个IMD任务，评估团队表现和沟通。

Result: 高绩效团队任务完成好、理解深、参与度高；高级问题与AI激发好奇心相关；CRQA指标显示学生 - AI交互中的动态同步。

Conclusion: AI作为队友和教育者能提供自适应反馈，维持参与度和认知好奇心。

Abstract: This study examines the impact of an Artificial Intelligence tutor teammate
(AI) on student curiosity-driven engagement and learning effectiveness during
Interactive Molecular Dynamics (IMD) tasks on the Visual Molecular Dynamics
platform. It explores the role of the AI's curiosity-triggering and response
behaviors in stimulating and sustaining student curiosity, affecting the
frequency and complexity of student-initiated questions. The study further
assesses how AI interventions shape student engagement, foster discovery
curiosity, and enhance team performance within the IMD learning environment.
Using a Wizard-of-Oz paradigm, a human experimenter dynamically adjusts the AI
tutor teammate's behavior through a large language model. By employing a
mixed-methods exploratory design, a total of 11 high school students
participated in four IMD tasks that involved molecular visualization and
calculations, which increased in complexity over a 60-minute period. Team
performance was evaluated through real-time observation and recordings, whereas
team communication was measured by question complexity and AI's
curiosity-triggering and response behaviors. Cross Recurrence Quantification
Analysis (CRQA) metrics reflected structural alignment in coordination and were
linked to communication behaviors. High-performing teams exhibited superior
task completion, deeper understanding, and increased engagement. Advanced
questions were associated with AI curiosity-triggering, indicating heightened
engagement and cognitive complexity. CRQA metrics highlighted dynamic
synchronization in student-AI interactions, emphasizing structured yet adaptive
engagement to promote curiosity. These proof-of-concept findings suggest that
the AI's dual role as a teammate and educator indicates its capacity to provide
adaptive feedback, sustaining engagement and epistemic curiosity.

</details>


### [367] [Autonomy by Design: Preserving Human Autonomy in AI Decision-Support](https://arxiv.org/abs/2506.23952)
*Stefan Buijsman,Sarah Carter,Juan Pablo Bermúdez*

Main category: cs.HC

TL;DR: 分析AI决策支持系统对特定领域自主性的影响并提出维护自主性的框架


<details>
  <summary>Details</summary>
Motivation: 此前对AI影响特定领域自主性研究不足，需深入探究

Method: 结合先前研究，分析医疗、金融和教育领域的实证案例

Result: 指出缺乏可靠故障指标和无意识价值转移会侵蚀特定领域自主性，构建自主性保护框架

Conclusion: 框架为开发增强特定领域人类能动性的AI系统提供具体指导

Abstract: AI systems increasingly support human decision-making across domains of
professional, skill-based, and personal activity. While previous work has
examined how AI might affect human autonomy globally, the effects of AI on
domain-specific autonomy -- the capacity for self-governed action within
defined realms of skill or expertise -- remain understudied. We analyze how AI
decision-support systems affect two key components of domain-specific autonomy:
skilled competence (the ability to make informed judgments within one's domain)
and authentic value-formation (the capacity to form genuine domain-relevant
values and preferences). By engaging with prior investigations and analyzing
empirical cases across medical, financial, and educational domains, we
demonstrate how the absence of reliable failure indicators and the potential
for unconscious value shifts can erode domain-specific autonomy both
immediately and over time. We then develop a constructive framework for
autonomy-preserving AI support systems. We propose specific socio-technical
design patterns -- including careful role specification, implementation of
defeater mechanisms, and support for reflective practice -- that can help
maintain domain-specific autonomy while leveraging AI capabilities. This
framework provides concrete guidance for developing AI systems that enhance
rather than diminish human agency within specialized domains of action.

</details>


### [368] [Immersive Technologies in Training and Healthcare: From Space Missions to Psychophysiological Research](https://arxiv.org/abs/2506.23545)
*Barbara Karpowicz,Maciej Grzeszczuk,Adam Kuzdraliński,Monika Kornacka,Aliaksandr Marozau,Wiktor Stawski,Pavlo Zinevych,Grzegorz Marcin Wójcik,Tomasz Kowalewski,Grzegorz Pochwatko,Wiesław Kopeć*

Main category: cs.HC

TL;DR: 探讨VR/AR/XR技术在多领域提升人类表现的应用。


<details>
  <summary>Details</summary>
Motivation: 随着VR/AR/XR技术在训练、诊断和心理研究中受关注，探讨其在多领域增强人类表现的作用。

Method: 以临床心理学、太空探索、医学教育等领域为例展开讨论。

Result: 在心理研究与训练中提供有效环境；太空探索中开发相关训练和诊断系统；医学教育与康复中提升学习成果和治疗依从性。

Conclusion: 沉浸式系统能在多个领域提升人类表现。

Abstract: Virtual, Augmented, and eXtended Reality (VR/AR/XR) technologies are
increasingly recognized for their applications in training, diagnostics, and
psychological research, particularly in high-risk and highly regulated
environments. In this panel we discuss how immersive systems enhance human
performance across multiple domains, including clinical psychology, space
exploration, and medical education. In psychological research and training, XR
can offer a controlled yet ecologically valid setting for measuring cognitive
and affective processes. In space exploration, we discuss the development of
VR-based astronaut training and diagnostic systems, allowing astronauts to
perform real-time health assessments. In medical education and rehabilitation,
we cover procedural training and patient engagement. From virtual surgical
simulations to gamified rehabilitation exercises, immersive environments
enhance both learning outcomes and treatment adherence.

</details>


### [369] [Positioning AI Tools to Support Online Harm Reduction Practice: Applications and Design Directions](https://arxiv.org/abs/2506.22941)
*Kaixuan Wang,Jason T. Jacques,Chenxin Diao*

Main category: cs.HC

TL;DR: 本文探讨如何负责任地设计大语言模型（LLMs）以满足吸毒者信息需求，通过定性研讨会得出LLMs能解决部分信息障碍，但需克服诸多挑战，提出协作式共同设计的设计路径。


<details>
  <summary>Details</summary>
Motivation: 现有在线渠道难以满足吸毒者多样化信息需求，LLMs在该领域应用未充分探索且有社会技术挑战，因此研究如何负责任设计LLMs满足吸毒者信息需求。

Method: 开展涉及不同利益相关者群体（学者、减少伤害从业者和在线社区管理员）的定性研讨会，探索LLM能力，确定潜在用例，明确核心设计考虑因素。

Result: LLMs能解决部分现有信息障碍，但有效性取决于克服与减少伤害原则的伦理一致性、细微情境理解、有效沟通和明确操作边界等挑战。

Conclusion: 强调与专家和吸毒者进行协作式共同设计，开发有用、安全且治理良好的LLM系统，为减少伤害生态系统中负责任开发LLMs提供基于实证的见解和可行的设计考虑。

Abstract: Access to accurate and actionable harm reduction information can directly
impact the health outcomes of People Who Use Drugs (PWUD), yet existing online
channels often fail to meet their diverse and dynamic needs due to limitations
in adaptability, accessibility, and the pervasive impact of stigma. Large
Language Models (LLMs) present a novel opportunity to enhance information
provision, but their application in such a high-stakes domain is under-explored
and presents socio-technical challenges. This paper investigates how LLMs can
be responsibly designed to support the information needs of PWUD. Through a
qualitative workshop involving diverse stakeholder groups (academics, harm
reduction practitioners, and an online community moderator), we explored LLM
capabilities, identified potential use cases, and delineated core design
considerations. Our findings reveal that while LLMs can address some existing
information barriers (e.g., by offering responsive, multilingual, and
potentially less stigmatising interactions), their effectiveness is contingent
upon overcoming challenges related to ethical alignment with harm reduction
principles, nuanced contextual understanding, effective communication, and
clearly defined operational boundaries. We articulate design pathways
emphasising collaborative co-design with experts and PWUD to develop LLM
systems that are helpful, safe, and responsibly governed. This work contributes
empirically grounded insights and actionable design considerations for the
responsible development of LLMs as supportive tools within the harm reduction
ecosystem.

</details>


### [370] [Against 'softmaxing' culture](https://arxiv.org/abs/2506.22968)
*Daniel Mwesigwa*

Main category: cs.HC

TL;DR: AI使文化扁平化，作者提出“软最大化文化”现象，指出当前AI文化评估面临挑战，认为ML和HCI评估方法有限，提议两个关键转变以应对文化复杂性。


<details>
  <summary>Details</summary>
Motivation: 解决大型AI系统中文化对齐项目里，AI使文化同质化这一根本挑战，改进和加强文化评估。

Method: 提出两个关键转变，一是评估时从问‘文化何时出现’而非‘什么是文化’开始；二是要将文化共性与特殊性关联起来。

Result: 提出能超越技术要求、更适应文化复杂性的评估思路。

Conclusion: 通过这两个概念转变，让评估方法更能应对文化的复杂情况。

Abstract: AI is flattening culture. Evaluations of "culture" are showing the myriad
ways in which large AI models are homogenizing language and culture, averaging
out rich linguistic differences into generic expressions. I call this
phenomenon "softmaxing culture," and it is one of the fundamental challenges
facing AI evaluations today. Efforts to improve and strengthen evaluations of
culture are central to the project of cultural alignment in large AI systems.
This position paper argues that machine learning (ML) and human-computer
interaction (HCI) approaches to evaluation are limited. I propose two key
shifts. First, instead of asking "what is culture?" at the start of system
evaluations, I propose beginning with the question: "when is culture?" Second,
while I acknowledge the philosophical claim that cultural universals exist, the
challenge is not simply to describe them, but to situate them in relation to
their particulars. Taken together, these conceptual shifts invite evaluation
approaches that move beyond technical requirements, toward perspectives more
responsive to the complexities of culture.

</details>


### [371] [CSBrain: A Cross-scale Spatiotemporal Brain Foundation Model for EEG Decoding](https://arxiv.org/abs/2506.23075)
*Yuchen Zhou,Jiamin Wu,Zichen Ren,Zhouheng Yao,Weiheng Lu,Kunyu Peng,Qihao Zheng,Chunfeng Song,Wanli Ouyang,Chao Gou*

Main category: cs.HC

TL;DR: 现有EEG基础模型忽视神经活动跨尺度时空结构，本文提出CSBrain模型，在11个EEG任务中表现优于基线，确立跨尺度建模重要性。


<details>
  <summary>Details</summary>
Motivation: 现有EEG基础模型采用的规模无关密集建模范式忽略神经活动跨尺度时空结构，导致表征欠佳和泛化能力弱，需改进。

Method: 提出CSBrain模型，引入跨尺度时空标记化（CST）和结构化稀疏注意力（SSA），二者交替堆叠整合多尺度依赖。

Result: 在16个数据集的11个EEG任务实验中，CSBrain始终优于特定任务和基础模型基线。

Conclusion: 跨尺度建模是关键归纳偏置，CSBrain可作为未来脑-人工智能研究的强大骨干。

Abstract: Understanding and decoding brain activity from electroencephalography (EEG)
signals is a fundamental challenge in neuroscience and AI, with applications in
cognition, emotion recognition, diagnosis, and brain-computer interfaces. While
recent EEG foundation models advance generalized decoding via unified
architectures and large-scale pretraining, they adopt a scale-agnostic dense
modeling paradigm inherited from NLP and vision. This design neglects a core
property of neural activity: cross-scale spatiotemporal structure. EEG task
patterns span a wide range of temporal and spatial scales, from short bursts to
slow rhythms, and from localized cortical responses to distributed
interactions. Ignoring this diversity leads to suboptimal representations and
weak generalization. We propose CSBrain, a Cross-scale Spatiotemporal Brain
foundation model for generalized EEG decoding. CSBrain introduces: (i)
Cross-scale Spatiotemporal Tokenization (CST), which aggregates multi-scale
features from localized temporal windows and anatomical brain regions into
compact scale-aware tokens; and (ii) Structured Sparse Attention (SSA), which
captures cross-window and cross-region dependencies, enhancing scale diversity
while removing spurious correlations. CST and SSA are alternately stacked to
progressively integrate multi-scale dependencies. Experiments on 11 EEG tasks
across 16 datasets show that CSBrain consistently outperforms task-specific and
foundation model baselines. These results establish cross-scale modeling as a
key inductive bias and position CSBrain as a robust backbone for future
brain-AI research.

</details>


### [372] [Neuro-Informed Joint Learning Enhances Cognitive Workload Decoding in Portable BCIs](https://arxiv.org/abs/2506.23458)
*Xiaoxiao Yang,Chan Feng,Jiancheng Chen*

Main category: cs.HC

TL;DR: 本文提出MuseCogNet框架解决便携式EEG设备信号非平稳性问题，在公开数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 便携式EEG设备信号的非平稳性限制数据保真度和解码精度，存在便携性与性能的权衡。

Method: 提出MuseCogNet，整合自监督和监督训练范式，引入基于平均池化的自监督重建损失和交叉熵损失。

Result: 在公开的Muse数据集上显著优于现有方法。

Conclusion: 为生态环境中的神经认知监测建立了可行途径。

Abstract: Portable and wearable consumer-grade electroencephalography (EEG) devices,
like Muse headbands, offer unprecedented mobility for daily brain-computer
interface (BCI) applications, including cognitive load detection. However, the
exacerbated non-stationarity in portable EEG signals constrains data fidelity
and decoding accuracy, creating a fundamental trade-off between portability and
performance. To mitigate such limitation, we propose MuseCogNet (Muse-based
Cognitive Network), a unified joint learning framework integrating
self-supervised and supervised training paradigms. In particular, we introduce
an EEG-grounded self-supervised reconstruction loss based on average pooling to
capture robust neurophysiological patterns, while cross-entropy loss refines
task-specific cognitive discriminants. This joint learning framework resembles
the bottom-up and top-down attention in humans, enabling MuseCogNet to
significantly outperform state-of-the-art methods on a publicly available Muse
dataset and establish an implementable pathway for neurocognitive monitoring in
ecological settings.

</details>


### [373] [Interactive Reasoning: Visualizing and Controlling Chain-of-Thought Reasoning in Large Language Models](https://arxiv.org/abs/2506.23678)
*Rock Yuren Pang,K. J. Kevin Feng,Shangbin Feng,Chu Li,Weijia Shi,Yulia Tsvetkov,Jeffrey Heer,Katharina Reinecke*

Main category: cs.HC

TL;DR: 提出交互式推理设计改进大语言模型输出质量，在Hippo原型中实现并经用户研究验证效果，开创用户监督融入推理过程的新范式。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的思维链内容冗长、缺乏组织且难获用户反馈，影响输出质量及审查。

Method: 引入交互式推理设计，将思维链输出可视化并支持用户审查修改，在Hippo原型中实现。

Result: 用户研究显示，交互式推理让用户快速识别错误、引导模型定制响应、更好理解推理和输出。

Conclusion: 工作开创了将用户监督融入大语言模型推理过程的新范式。

Abstract: The output quality of large language models (LLMs) can be improved via
"reasoning": generating segments of chain-of-thought (CoT) content to further
condition the model prior to producing user-facing output. While these chains
contain valuable information, they are verbose and lack explicit organization,
making them tedious to review. Moreover, they lack opportunities for user
feedback, such as to remove unwanted considerations, add desired ones, or
clarify unclear assumptions. We introduce Interactive Reasoning, an interaction
design that visualizes chain-of-thought outputs as a hierarchy of topics and
enables user review and modification. We implement interactive reasoning in
Hippo, a prototype for AI-assisted decision making in the face of uncertain
trade-offs. In a user study with 16 participants, we find that interactive
reasoning in Hippo allows users to quickly identify and interrupt erroneous
generations, efficiently steer the model towards customized responses, and
better understand both model reasoning and model outputs. Our work contributes
to a new paradigm that incorporates user oversight into LLM reasoning
processes.

</details>


### [374] [The Impact of AI on Educational Assessment: A Framework for Constructive Alignment](https://arxiv.org/abs/2506.23815)
*Patrick Stokkink*

Main category: cs.HC

TL;DR: 本文探讨人工智能对教育评估的影响，基于理论框架提出评估应适配AI，指出讲师存在偏见，建议制定指南和培训师资。


<details>
  <summary>Details</summary>
Motivation: 随着AI和大语言模型在教育中应用增多，探讨当前评估方式是否仍有效。

Method: 基于Constructive Alignment理论和Bloom分类法构建理论框架进行分析。

Result: 发现AI对不同Bloom层次学习目标影响不同，讲师对AI在评估中的使用存在基于熟悉度的偏见。

Conclusion: 应在大学或院系层面制定结构化指南，培训教学人员以适应评估方法的调整。

Abstract: The influence of Artificial Intelligence (AI), and specifically Large
Language Models (LLM), on education is continuously increasing. These models
are frequently used by students, giving rise to the question whether current
forms of assessment are still a valid way to evaluate student performance and
comprehension. The theoretical framework developed in this paper is grounded in
Constructive Alignment (CA) theory and Bloom's taxonomy for defining learning
objectives. We argue that AI influences learning objectives of different Bloom
levels in a different way, and assessment has to be adopted accordingly.
Furthermore, in line with Bloom's vision, formative and summative assessment
should be aligned on whether the use of AI is permitted or not.
  Although lecturers tend to agree that education and assessment need to be
adapted to the presence of AI, a strong bias exists on the extent to which
lecturers want to allow for AI in assessment. This bias is caused by a
lecturer's familiarity with AI and specifically whether they use it themselves.
To avoid this bias, we propose structured guidelines on a university or faculty
level, to foster alignment among the staff. Besides that, we argue that
teaching staff should be trained on the capabilities and limitations of AI
tools. In this way, they are better able to adapt their assessment methods.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [375] [Neural Langevin Machine: a local asymmetric learning rule can be creative](https://arxiv.org/abs/2506.23546)
*Zhendong Yu,Weizhong Huang,Haiping Huang*

Main category: q-bio.NC

TL;DR: 提出神经朗之万机，该生成模型可解释、易训练且学习规则有生物学相关性，能实现创造性动态连续采样。


<details>
  <summary>Details</summary>
Motivation: 利用循环神经网络的不动点存储和生成信息，开发新的生成模型。

Method: 用玻尔兹曼 - 吉布斯测度捕获不动点，引出神经朗之万动力学用于采样和学习真实数据集。

Result: 得到神经朗之万机，可实现神经网络中创造性动态的连续采样。

Conclusion: 神经朗之万机是有前景的生成模型，在基于电路的采样和生物学上合理的学习规则方面有优势。

Abstract: Fixed points of recurrent neural networks can be leveraged to store and
generate information. These fixed points can be captured by the Boltzmann-Gibbs
measure, which leads to neural Langevin dynamics that can be used for sampling
and learning a real dataset. We call this type of generative model neural
Langevin machine, which is interpretable due to its analytic form of
distribution and is simple to train. Moreover, the learning process is derived
as a local asymmetric plasticity rule, bearing biological relevance. Therefore,
one can realize a continuous sampling of creative dynamics in a neural network,
mimicking an imagination process in brain circuits. This neural Langevin
machine may be another promising generative model, at least in its strength in
circuit-based sampling and biologically plausible learning rule.

</details>


### [376] [Attention acts to suppress goal-based conflict under high competition](https://arxiv.org/abs/1610.09431)
*Omar Claflin*

Main category: q-bio.NC

TL;DR: 在高竞争条件下，自上而下的注意力在刺激出现100毫秒内抑制相关和无关神经信号，以减少无关刺激的前馈信号。


<details>
  <summary>Details</summary>
Motivation: 以往自上而下的注意力研究多在视觉注意力竞争最小的条件下进行，需研究高竞争条件下的情况。

Method: 研究高竞争条件（共享感受野内两个刺激有相反调节目标）下的注意力情况。

Result: 在高竞争时，刺激出现100毫秒内，自上而下的注意力抑制相关和无关神经信号。

Conclusion: 自上而下的注意力资源非选择性参与可减少代表无关刺激的前馈信号。

Abstract: It is known that when multiple stimuli are present, top-down attention
selectively enhances the neural signal in the visual cortex for task-relevant
stimuli, but this has been tested only under conditions of minimal competition
of visual attention. Here we show during high competition, that is, two stimuli
in a shared receptive field possessing opposing modulatory goals, top-down
attention suppresses both task-relevant and irrelevant neural signals within
100 ms of stimuli onset. This non-selective engagement of top-down attentional
resources serves to reduce the feedforward signal representing irrelevant
stimuli.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [377] [Strategic analysis of hydrogen market dynamics across collaboration models](https://arxiv.org/abs/2506.22690)
*Mohammad Asghari,Hamid Afshari,Mohamad Y Jaber,Cory Searcy*

Main category: stat.AP

TL;DR: 本文研究氢气定价与需求动态，评估三种合作场景，发现合作整合策略利于可持续增长，协调决策能提升盈利，还强调战略定价等对氢能行业可持续发展的重要性。


<details>
  <summary>Details</summary>
Motivation: 全球能源向可持续清洁能源转型，氢气是脱碳等方面的有前景候选，需研究其定价和需求动态。

Method: 评估市场定价、合作整合、协调决策三种合作场景，纳入价格敏感需求、环保生产方法和市场渗透效应。

Result: 合作整合策略最利于可持续增长，使绿氢市场份额达19.06%；协调决策通过合作关税合同提升盈利，平衡经济可行性和环境目标。

Conclusion: 强调战略定价机制、政策协调和氢能枢纽对氢能行业可持续增长的重要性，为政策制定者和行业参与者提供行动指导。

Abstract: The global energy landscape is experiencing a transformative shift, with an
increasing emphasis on sustainable and clean energy sources. Hydrogen remains a
promising candidate for decarbonization, energy storage, and as an alternative
fuel. This study explores the landscape of hydrogen pricing and demand dynamics
by evaluating three collaboration scenarios: market-based pricing, cooperative
integration, and coordinated decision-making. It incorporates price-sensitive
demand, environmentally friendly production methods, and market penetration
effects, to provide insights into maximizing market share, profitability, and
sustainability within the hydrogen industry. This study contributes to
understanding the complexities of collaboration by analyzing those structures
and their role in a fast transition to clean hydrogen production by balancing
economic viability and environmental goals. The findings reveal that the
cooperative integration strategy is the most effective for sustainable growth,
increasing green hydrogen's market share to 19.06 % and highlighting the
potential for environmentally conscious hydrogen production. They also suggest
that the coordinated decision-making approach enhances profitability through
collaborative tariff contracts while balancing economic viability and
environmental goals. This study also underscores the importance of strategic
pricing mechanisms, policy alignment, and the role of hydrogen hubs in
achieving sustainable growth in the hydrogen sector. By highlighting the
uncertainties and potential barriers, this research offers actionable guidance
for policymakers and industry players in shaping a competitive and sustainable
energy marketplace.

</details>


### [378] [FuzzCoh: Robust Canonical Coherence-Based Fuzzy Clustering of Multivariate Time Series](https://arxiv.org/abs/2506.22861)
*Ziling Ma,Mara Sherlin Talento,Ying Sun,Hernando Ombao*

Main category: stat.AP

TL;DR: 本文提出基于光谱域的模糊聚类框架处理脑电数据聚类，利用Kendall's tau典范相干性提取特征，有效处理复杂脑电数据，且应用广泛。


<details>
  <summary>Details</summary>
Motivation: 脑电等多变量时间序列数据聚类对理解脑功能重要，但因复杂非平稳交叉依赖等问题具有挑战性。

Method: 开发光谱域模糊聚类框架，利用Kendall's tau典范相干性提取特征，将多变量时间序列对象投影到典范方向向量，用模糊划分策略确定聚类。

Result: 方法能捕捉脑电数据频谱和空间特征，定位频率依赖结构和脑功能连接。

Conclusion: 基于KenCoh的模糊聚类框架可处理高维时间序列数据复杂性，广泛适用于神经科学等领域。

Abstract: Brain cognitive and sensory functions are often associated with
electrophysiological activity at specific frequency bands. Clustering
multivariate time series (MTS) data like EEGs is important for understanding
brain functions but challenging due to complex non-stationary
cross-dependencies, gradual transitions between cognitive states, noisy
measurements, and ambiguous cluster boundaries. To address these issues, we
develop a robust fuzzy clustering framework in the spectral domain. Our method
leverages Kendall's tau-based canonical coherence, which extracts meaningful
frequency-specific monotonic relationships between groups of channels or
regions. KenCoh effectively captures dominant coherence structures while
remaining robust against outliers and noise, making it suitable for real EEG
datasets that typically contain artifacts. Our method first projects each MTS
object onto vectors derived from the KenCoh estimates (i.e, canonical
directions), which capture relevant information on the connectivity structure
of oscillatory signals in predefined frequency bands. These spectral features
are utilized to determine clusters of epochs using a fuzzy partitioning
strategy, accommodating gradual transitions and overlapping class structure.
Lastly, we demonstrate the effectiveness of our approach to EEG data where
latent cognitive states such as alertness and drowsiness exhibit
frequency-specific dynamics and ambiguity. Our method captures both spectral
and spatial features by locating the frequency-dependent structure and brain
functional connectivity. Built on the KenCoh framework for fuzzy clustering, it
handles the complexity of high-dimensional time series data and is broadly
applicable to domains such as neuroscience, wearable sensing, environmental
monitoring, and finance.

</details>


### [379] [Learning Individual Reproductive Behavior from Aggregate Fertility Rates via Neural Posterior Estimation](https://arxiv.org/abs/2506.22607)
*Daniel Ciganda,Ignacio Campón,Iñaki Permanyer,Jakob H Macke*

Main category: stat.AP

TL;DR: 开发无似然贝叶斯框架从生育率数据推断行为和生物参数，能预测微观分布，支持人口预测和数字孪生构建。


<details>
  <summary>Details</summary>
Motivation: 年龄别生育率的总体性质掩盖了驱动生育趋势的潜在行为机制，需恢复这些机制。

Method: 开发无似然贝叶斯框架，将个体生育过程模型与顺序神经后验估计相结合，从两个总体序列推断参数。

Result: 该方法能重现观察到的生育时间表，预测样本外微观分布。

Conclusion: 拟合模型可进行行为明确的人口预测，支持构建人口数字孪生。

Abstract: While age-specific fertility rates (ASFRs) provide the most extensive record
of reproductive change, their aggregate nature masks the underlying behavioral
mechanisms that ultimately drive fertility trends. To recover these mechanisms,
we develop a likelihood-free Bayesian framework that couples an
individual-level model of the reproductive process with Sequential Neural
Posterior Estimation (SNPE). This allows us to infer eight behavioral and
biological parameters from just two aggregate series: ASFRs and the age-profile
of planned versus unplanned births. Applied to U.S. National Survey of Family
Growth cohorts and to Demographic and Health Survey cohorts from Colombia, the
Dominican Republic, and Peru, the method reproduces observed fertility
schedules and, critically, predicts out-of-sample micro-level distributions of
age at first sex, inter-birth intervals, and family-size ideals, none of which
inform the estimation step. Because the fitted model yields complete synthetic
life histories, it enables behaviorally explicit population forecasts and
supports the construction of demographic digital twins.

</details>


<div id='stat.OT'></div>

# stat.OT [[Back]](#toc)

### [380] [Treatment, evidence, imitation, and chat](https://arxiv.org/abs/2506.23040)
*Samuel J. Weisenthal*

Main category: stat.OT

TL;DR: 探讨大语言模型在医疗决策中的应用，涵盖治疗和聊天问题及挑战与下一步方向。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在医疗决策中的潜力。

Method: 从治疗问题入手，讨论解决方法，接着分析聊天问题，探讨大语言模型解决治疗问题的方式。

Result: 指出大语言模型解决治疗问题存在挑战。

Conclusion: 探讨挑战与循证医学的关系，为下一步研究提供参考。

Abstract: Large language models are thought to have potential to aid in medical
decision making. We investigate this here. We start with the treatment problem,
the patient's core medical decision-making task, which is solved in
collaboration with a healthcare provider. We discuss approaches to solving the
treatment problem, including -- within evidence-based medicine -- trials and
observational data. We then discuss the chat problem, and how this differs from
the treatment problem -- in particular as it relates to imitation. We then
discuss how a large language model might be used to solve the treatment problem
and highlight some of the challenges that emerge. We finally discuss how these
challenges relate to evidence-based medicine, and how this might inform next
steps.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [381] [Data-Driven Self-Supervised Learning for the Discovery of Solution Singularity for Partial Differential Equations](https://arxiv.org/abs/2506.23344)
*Difeng Cai,Paulina Sepúlveda*

Main category: math.NA

TL;DR: 本文在纯数据驱动场景下，提出自监督学习框架检测奇点位置，给出两种过滤方法，通过实验展示其应对多种情况的能力。


<details>
  <summary>Details</summary>
Motivation: 奇点会影响数值计算效果，未知奇点位置使问题更复杂，检测奇点对开发高效自适应方法、降低计算成本很关键。

Method: 提出自监督学习框架，以过滤过程为前置任务，给出基于k近邻和核密度估计的两种过滤方法。

Result: 通过数值例子展示未过滤原始数据可能导致的不良结果，实验证明该方法能应对输入扰动、标签损坏和多种奇点情况。

Conclusion: 所提方法可有效在纯数据驱动场景下检测奇点位置。

Abstract: The appearance of singularities in the function of interest constitutes a
fundamental challenge in scientific computing. It can significantly undermine
the effectiveness of numerical schemes for function approximation, numerical
integration, and the solution of partial differential equations (PDEs), etc.
The problem becomes more sophisticated if the location of the singularity is
unknown, which is often encountered in solving PDEs. Detecting the singularity
is therefore critical for developing efficient adaptive methods to reduce
computational costs in various applications. In this paper, we consider
singularity detection in a purely data-driven setting. Namely, the input only
contains given data, such as the vertex set from a mesh. To overcome the
limitation of the raw unlabeled data, we propose a self-supervised learning
(SSL) framework for estimating the location of the singularity. A key component
is a filtering procedure as the pretext task in SSL, where two filtering
methods are presented, based on $k$ nearest neighbors and kernel density
estimation, respectively. We provide numerical examples to illustrate the
potential pathological or inaccurate results due to the use of raw data without
filtering. Various experiments are presented to demonstrate the ability of the
proposed approach to deal with input perturbation, label corruption, and
different kinds of singularities such interior circle, boundary layer,
concentric semicircles, etc.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [382] [Service Placement in Small Cell Networks Using Distributed Best Arm Identification in Linear Bandits](https://arxiv.org/abs/2506.22480)
*Mariam Yahya,Aydin Sezgin,Setareh Maghsudi*

Main category: cs.NI

TL;DR: 小蜂窝网络中云访问高延迟，MEC可缓解但边缘容量有限，本文将服务放置问题建模为线性老虎机问题，提出分布式自适应多智能体BAI算法，仿真验证其效果并进行理论分析。


<details>
  <summary>Details</summary>
Motivation: 小蜂窝网络中云访问高延迟，MEC边缘容量有限，在未知服务需求和动态网络条件下难以决定服务本地或云端部署。

Method: 将服务需求建模为服务属性的线性函数，将服务放置任务建模为线性老虎机问题，提出分布式自适应多智能体BAI算法，SBS协作加速学习。

Result: 仿真表明算法能以期望置信度识别最优服务，实现接近最优的加速，学习轮数与SBS数量成比例减少。

Conclusion: 所提算法能有效解决小蜂窝网络中服务放置问题，同时给出了算法样本复杂度和通信开销的理论分析。

Abstract: As users in small cell networks increasingly rely on computation-intensive
services, cloud-based access often results in high latency. Multi-access edge
computing (MEC) mitigates this by bringing computational resources closer to
end users, with small base stations (SBSs) serving as edge servers to enable
low-latency service delivery. However, limited edge capacity makes it
challenging to decide which services to deploy locally versus in the cloud,
especially under unknown service demand and dynamic network conditions. To
tackle this problem, we model service demand as a linear function of service
attributes and formulate the service placement task as a linear bandit problem,
where SBSs act as agents and services as arms. The goal is to identify the
service that, when placed at the edge, offers the greatest reduction in total
user delay compared to cloud deployment. We propose a distributed and adaptive
multi-agent best-arm identification (BAI) algorithm under a fixed-confidence
setting, where SBSs collaborate to accelerate learning. Simulations show that
our algorithm identifies the optimal service with the desired confidence and
achieves near-optimal speedup, as the number of learning rounds decreases
proportionally with the number of SBSs. We also provide theoretical analysis of
the algorithm's sample complexity and communication overhead.

</details>


### [383] [Reliable Image Transmission in CPS-based Pub/Sub](https://arxiv.org/abs/2506.22875)
*Everson Flores,Bruna Guterres,Thomaz Pereira Junior,Paula Barros,Alberto Cabral,Cristiana Lima Dora,Marcelo Malheiros,Marcelo Pias*

Main category: cs.NI

TL;DR: 本文研究MQTT协议在高流量、间歇性连接场景下用于图像共享和传输的可靠性，通过实验验证了其在工业系统中的表现。


<details>
  <summary>Details</summary>
Motivation: 通信和自动化发展推动分布式网络扩张，但关于MQTT协议在高流量、间歇性连接场景下图像共享和传输性能的研究存在空白，限制其在关键物联网和CPS应用中的使用。

Method: 在网络中断和高数据流量场景下，通过一系列可控的测试平台验证实验评估分布式系统的性能。

Result: 基于MQTT的系统在正常条件下能维持可靠传输，恢复能力取决于故障点，能防止重复错误，适应不断增加的网络需求。

Conclusion: 该系统适用于需要高效和有弹性数据处理的工业应用。

Abstract: Developments in communication and automation have driven the expansion of
distributed networks, essential for IoT and CPS development in industrial
applications requiring reliable image processing and real-time adaptability.
Although broadly adopted, there is a literature gap regarding the performance
of MQTT protocol for image sharing and transmission under high-traffic
scenarios with intermittent connectivity, restricting its use in critical IoT
and CPS applications. In this context, the present work examines the
reliability of real-time image transmission in IoT and CPS industrial systems
that utilize the MQTT-based publish/subscribe communication model. It focuses
on scenarios with network interruptions and high data traffic, evaluating the
performance of a distributed system through a series of controlled testbed
validation experiments. Experimental validation demonstrated that while the
MQTT-based system sustains reliable transmission under normal conditions, its
recovery capability depends on the failure point, with complete restoration
occurring when disruptions affect the Orchestrator Node and partial recovery
when the Producer Node or Broker are affected. The study also confirmed that
the system prevents duplicate errors and adapts well to increasing network
demands, reinforcing its suitability for industrial applications that require
efficient and resilient data handling.

</details>


### [384] [Innovative Research on IoT Architecture and Robotic Operating Platforms: Applications of Large Language Models and Generative AI](https://arxiv.org/abs/2506.22477)
*Huiwen Han*

Main category: cs.NI

TL;DR: 本文介绍基于物联网架构的机器人操作平台创新设计，结合多种技术提升智能与自主性，通过案例展示潜力并强调技术作用与影响。


<details>
  <summary>Details</summary>
Motivation: 提升物联网系统和机器人的智能与自主性，使其能实时决策和适应环境变化。

Method: 提出基于物联网架构的机器人操作平台设计，结合大语言模型、生成式AI等技术，并通过跨行业案例研究。

Result: 展示了物联网机器人在优化工作流程、提高生产力和提供创新可扩展解决方案方面的巨大潜力。

Conclusion: 强调大语言模型和生成式AI等技术推动智能机器人和物联网发展，对社会和工业有广泛影响，是下一代自动化和技术融合的催化剂。

Abstract: This paper introduces an innovative design for robotic operating platforms,
underpinned by a transformative Internet of Things (IoT) architecture,
seamlessly integrating cutting-edge technologies such as large language models
(LLMs), generative AI, edge computing, and 5G networks. The proposed platform
aims to elevate the intelligence and autonomy of IoT systems and robotics,
enabling them to make real-time decisions and adapt dynamically to changing
environments. Through a series of compelling case studies across industries
including smart manufacturing, healthcare, and service sectors, this paper
demonstrates the substantial potential of IoT-enabled robotics to optimize
operational workflows, enhance productivity, and deliver innovative, scalable
solutions. By emphasizing the roles of LLMs and generative AI, the research
highlights how these technologies drive the evolution of intelligent robotics
and IoT, shaping the future of industry-specific advancements. The findings not
only showcase the transformative power of these technologies but also offer a
forward-looking perspective on their broader societal and industrial
implications, positioning them as catalysts for next-generation automation and
technological convergence.

</details>


### [385] [AGI Enabled Solutions For IoX Layers Bottlenecks In Cyber-Physical-Social-Thinking Space](https://arxiv.org/abs/2506.22487)
*Amar Khelloufi,Huansheng Ning,Sahraoui Dhelim,Jianguo Ding*

Main category: cs.NI

TL;DR: 本文对AGI增强的IoX研究进行系统全面综述，介绍关键组件、方法、成果，指出挑战并强调该领域重要性。


<details>
  <summary>Details</summary>
Motivation: 应对网络物理社会思维（CPST）生态系统中传感、网络和应用层的关键瓶颈问题。

Method: 从传感层数据管理、网络层协议优化和应用层决策框架三个关键组件对AGI增强的IoX研究进行系统全面回顾。

Result: AGI驱动的策略为传感层数据过载、网络层协议异构和应用层身份爆炸提供新解决方案。

Conclusion: 强调跨层集成、量子通信和伦理治理框架对未来AGI增强IoX系统的重要性，指出计算需求、可扩展性和实际验证等未解决挑战，呼吁进一步研究。

Abstract: The integration of the Internet of Everything (IoX) and Artificial General
Intelligence (AGI) has given rise to a transformative paradigm aimed at
addressing critical bottlenecks across sensing, network, and application layers
in Cyber-Physical-Social Thinking (CPST) ecosystems. In this survey, we provide
a systematic and comprehensive review of AGI-enhanced IoX research, focusing on
three key components: sensing-layer data management, network-layer protocol
optimization, and application-layer decision-making frameworks. Specifically,
this survey explores how AGI can mitigate IoX bottlenecks challenges by
leveraging adaptive sensor fusion, edge preprocessing, and selective attention
mechanisms at the sensing layer, while resolving network-layer issues such as
protocol heterogeneity and dynamic spectrum management, neuro-symbolic
reasoning, active inference, and causal reasoning, Furthermore, the survey
examines AGI-enabled frameworks for managing identity and relationship
explosion. Key findings suggest that AGI-driven strategies, such as adaptive
sensor fusion, edge preprocessing, and semantic modeling, offer novel solutions
to sensing-layer data overload, network-layer protocol heterogeneity, and
application-layer identity explosion. The survey underscores the importance of
cross-layer integration, quantum-enabled communication, and ethical governance
frameworks for future AGI-enabled IoX systems. Finally, the survey identifies
unresolved challenges, such as computational requirements, scalability, and
real-world validation, calling for further research to fully realize AGI's
potential in addressing IoX bottlenecks. we believe AGI-enhanced IoX is
emerging as a critical research field at the intersection of interconnected
systems and advanced AI.

</details>


### [386] [Offline Reinforcement Learning for Mobility Robustness Optimization](https://arxiv.org/abs/2506.22793)
*Pegah Alizadeh,Anastasios Giovanidis,Pradeepa Ramachandra,Vasileios Koutsoukis,Osama Arouk*

Main category: cs.NI

TL;DR: 研究用离线强化学习学习最优Cell Individual Offset调整，评估显示离线RL方法优于基于规则的MRO，且有操作灵活性。


<details>
  <summary>Details</summary>
Motivation: 研究使用离线强化学习学习最优Cell Individual Offset调整的可能性，改进Mobility Robustness Optimisation (MRO)算法。

Method: 采用基于序列的Decision Transformers和基于值的Conservative Q - Learning方法，使用与失败、乒乓切换等相关的输入特征。

Result: 在3500 MHz载波频率的现实新无线电网络上评估，离线RL方法优于基于规则的MRO，最多提升7%，且可使用相同数据集训练不同目标函数。

Conclusion: 离线RL方法在MRO问题上优于基于规则的方法，具有操作灵活性。

Abstract: In this work we revisit the Mobility Robustness Optimisation (MRO) algorithm
and study the possibility of learning the optimal Cell Individual Offset tuning
using offline Reinforcement Learning. Such methods make use of collected
offline datasets to learn the optimal policy, without further exploration. We
adapt and apply a sequence-based method called Decision Transformers as well as
a value-based method called Conservative Q-Learning to learn the optimal policy
for the same target reward as the vanilla rule-based MRO. The same input
features related to failures, ping-pongs, and other handover issues are used.
Evaluation for realistic New Radio networks with 3500 MHz carrier frequency on
a traffic mix including diverse user service types and a specific tunable
cell-pair shows that offline-RL methods outperform rule-based MRO, offering up
to 7% improvement. Furthermore, offline-RL can be trained for diverse objective
functions using the same available dataset, thus offering operational
flexibility compared to rule-based methods.

</details>


### [387] [Geminet: Learning the Duality-based Iterative Process for Lightweight Traffic Engineering in Changing Topologies](https://arxiv.org/abs/2506.23640)
*Ximeng Liu,Shizhen Zhao,Xinbing Wang*

Main category: cs.NI

TL;DR: 本文提出轻量级可扩展的ML - 基流量工程框架Geminet，解决现有方案不足，评估显示其在可扩展性、内存消耗和收敛速度上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有ML - 基流量工程方案存在无法处理拓扑变化或可扩展性差的问题，需改进。

Method: 提出Geminet框架，通过学习迭代梯度下降调整过程解耦神经网络与拓扑，将优化从路径级路由权重转移到边级对偶变量。

Result: Geminet神经网络规模是现有方案的0.04% - 7%，处理拓扑变化与HARP相当且无性能下降，训练时内存消耗比HARP少超八倍，收敛速度快5.45倍。

Conclusion: Geminet具有良好可扩展性，有大规模部署潜力。

Abstract: Recently, researchers have explored ML-based Traffic Engineering (TE),
leveraging neural networks to solve TE problems traditionally addressed by
optimization. However, existing ML-based TE schemes remain impractical: they
either fail to handle topology changes or suffer from poor scalability due to
excessive computational and memory overhead. To overcome these limitations, we
propose Geminet, a lightweight and scalable ML-based TE framework that can
handle changing topologies. Geminet is built upon two key insights: (i) a
methodology that decouples neural networks from topology by learning an
iterative gradient-descent-based adjustment process, as the update rule of
gradient descent is topology-agnostic, relying only on a few gradient-related
quantities; (ii) shifting optimization from path-level routing weights to
edge-level dual variables, reducing memory consumption by leveraging the fact
that edges are far fewer than paths. Evaluations on WAN and data center
datasets show that Geminet significantly improves scalability. Its neural
network size is only 0.04% to 7% of existing schemes, while handling topology
variations as effectively as HARP, a state-of-the-art ML-based TE approach,
without performance degradation. When trained on large-scale topologies,
Geminet consumes under 10 GiB of memory, more than eight times less than the
80-plus GiB required by HARP, while achieving 5.45 times faster convergence
speed, demonstrating its potential for large-scale deployment.

</details>


### [388] [The Kubernetes Network Driver Model: A Composable Architecture for High-Performance Networking](https://arxiv.org/abs/2506.23628)
*Antonio Ojea*

Main category: cs.NI

TL;DR: 本文提出Kubernetes网络驱动（KNDs）架构解决传统Kubernetes网络问题，DraNet实现展示其优势。


<details>
  <summary>Details</summary>
Motivation: 传统Kubernetes网络难以满足AI/ML和电信基础设施不断增长的需求。

Method: 利用动态资源分配（DRA）、节点资源接口（NRI）改进和OCI运行时规范变更，将网络资源管理集成到Kubernetes核心。

Result: DraNet实现展示了网络接口的声明式连接，显著提升高性能AI/ML工作负载。

Conclusion: KNDs能支持复杂云原生应用，为未来电信解决方案奠定基础，减少运营复杂性。

Abstract: Traditional Kubernetes networking struggles to meet the escalating demands of
AI/ML and evolving Telco infrastructure. This paper introduces Kubernetes
Network Drivers (KNDs), a transformative, modular, and declarative architecture
designed to overcome current imperative provisioning and API limitations. KNDs
integrate network resource management into Kubernetes' core by utilizing
Dynamic Resource Allocation (DRA), Node Resource Interface (NRI) improvements,
and upcoming OCI Runtime Specification changes. Our DraNet implementation
demonstrates declarative attachment of network interfaces, including Remote
Direct Memory Access (RDMA) devices, significantly boosting high-performance
AI/ML workloads. This capability enables sophisticated cloud-native
applications and lays crucial groundwork for future Telco solutions, fostering
a "galaxy" of specialized KNDs for enhanced application delivery and reduced
operational complexity.

</details>


### [389] [Learning Constraints Directly from Network Data](https://arxiv.org/abs/2506.23964)
*Hongyu Hè,Minhao Jin,Maria Apostolaki*

Main category: cs.NI

TL;DR: 本文提出NetNomos从原始网络测量中学习命题逻辑约束，解决规则提取难题，评估显示其效果优于基线方法，还有多种应用场景。


<details>
  <summary>Details</summary>
Motivation: 手动或仅依靠机器学习进行网络规则提取会产生不完整、不可靠和/或不准确的规则，无法获得形式化规则带来的好处。

Method: 将规则提取表述为约束建模问题，引入NetNomos，通过基于格的搜索解决约束建模挑战，降低学习复杂度。

Result: NetNomos在三小时内学会所有基准规则，基线方法发现规则少于25%且耗时数天；还能评估合成流量生成器、进行异常检测和支持监测推理。

Conclusion: NetNomos能有效从原始网络测量中学习命题逻辑约束，在规则提取和网络分析方面有显著优势和多种应用价值。

Abstract: Network data conforms to a wide range of rules that arise from protocols,
design principles, and deployment decisions (e.g., a packet's queuing delay
must be less than its end-to-end delay). Formalizing such rules as logic
constraints can (i) improve the quality of synthetic data, (ii) reduce the
brittleness of machine learning (ML) models, and (iii) improve semantic
understanding of network measurements. However, these benefits remain out of
reach if rule extraction is manual or solely reliant on ML, as both approaches
yield incomplete, unreliable, and/or inaccurate rules.
  This paper formulates rule extraction as a constraint modeling problem and
introduces NetNomos that learns propositional logic constraints directly from
raw network measurements. Constraint modeling in this domain is uniquely
challenging due to the scale of the data, the inherent learning complexity and
passive environment, and the lack of ground truth supervision. NetNomos
addresses these challenges via a lattice-based search structured by constraint
specificity and succinctness. Our approach reduces learning complexity from
superquadratic to logarithmic and enables efficient traversal in combinatorial
search space.
  Our evaluations on diverse network datasets show that NetNomos learns all
benchmark rules, including those associated with as little as 0.01% of data
points, in under three hours. In contrast, baseline methods discover less than
25% of the rules and require several days to run. Through three case studies,
we show that: NetNomos (i) finds rule violations in the outputs of all seven
synthetic traffic generators, hence can be used to assess and guide their
generation process; (ii) detects semantic differences in traffic, hence can be
used for anomaly detection; and (iii) automatically finds rules used for
telemetry imputation, hence can support monitoring through inference.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [390] [Zero-disparity Distribution Synthesis: Fast Exact Calculation of Chi-Squared Statistic Distribution for Discrete Uniform Histograms](https://arxiv.org/abs/2506.23416)
*Nikola Banić,Neven Elezović*

Main category: stat.ME

TL;DR: 提出Zero - disparity Distribution Synthesis方法计算Pearson卡方检验精确分布，分析近似误差，纠正误解并揭示近似陷阱，代码开源。


<details>
  <summary>Details</summary>
Motivation: 传统Pearson卡方检验用连续卡方分布近似检验统计量，在期望箱计数低或需要尾概率时近似效果不佳。

Method: 提出Zero - disparity Distribution Synthesis，一种快速动态规划方法来计算精确分布。

Result: 消除了一些现有误解，揭示了只有通过精确值才能发现的近似中的细微但显著的陷阱。

Conclusion: Zero - disparity Distribution Synthesis方法有助于更准确地分析Pearson卡方检验近似误差。

Abstract: Pearson's chi-squared test is widely used to assess the uniformity of
discrete histograms, typically relying on a continuous chi-squared distribution
to approximate the test statistic, since computing the exact distribution is
computationally too costly. While effective in many cases, this approximation
allegedly fails when expected bin counts are low or tail probabilities are
needed. Here, Zero-disparity Distribution Synthesis is presented, a fast
dynamic programming approach for computing the exact distribution, enabling
detailed analysis of approximation errors. The results dispel some existing
misunderstandings and also reveal subtle, but significant pitfalls in
approximation that are only apparent with exact values. The Python source code
is available at https://github.com/DiscreteTotalVariation/ChiSquared.

</details>


### [391] [Doubly robust estimation of causal effects for random object outcomes with continuous treatments](https://arxiv.org/abs/2506.22754)
*Satarupa Bhattacharjee,Bing Li,Xiao Wu,Lingzhou Xue*

Main category: stat.ME

TL;DR: 本文提出处理非欧几里得数据的因果推断新框架，通过实验验证并应用于实际环境数据，拓宽了因果推断方法适用性。


<details>
  <summary>Details</summary>
Motivation: 传统因果推断研究多在欧几里得空间，当代应用涉及非欧几里得数据结构，需新的因果推断框架。以研究细颗粒物暴露对美国各县死亡年龄分布的影响为契机。

Method: 利用度量空间的希尔伯特空间嵌入进行弗雷歇均值估计和因果效应映射，提出非参数、双重去偏的因果推断方法，能处理高维向量混杂因素，推导有效影响函数，使用交叉拟合估计量和共形推断技术。

Result: 框架通过数值实验验证，应用于实际环境数据。

Conclusion: 该框架将因果推断方法扩展到复杂数据结构，拓宽了其在各科学学科的适用性。

Abstract: Causal inference is central to statistics and scientific discovery, enabling
researchers to identify cause-and-effect relationships beyond associations.
While traditionally studied within Euclidean spaces, contemporary applications
increasingly involve complex, non-Euclidean data structures that reside in
abstract metric spaces, known as random objects, such as images, shapes,
networks, and distributions. This paper introduces a novel framework for causal
inference with continuous treatments applied to non-Euclidean data. To address
the challenges posed by the lack of linear structures, we leverage Hilbert
space embeddings of the metric spaces to facilitate Fr\'echet mean estimation
and causal effect mapping. Motivated by a study on the impact of exposure to
fine particulate matter on age-at-death distributions across U.S. counties, we
propose a nonparametric, doubly-debiased causal inference approach for outcomes
as random objects with continuous treatments. Our framework can accommodate
moderately high-dimensional vector-valued confounders and derive efficient
influence functions for estimation to ensure both robustness and
interpretability. We establish rigorous asymptotic properties of the
cross-fitted estimators and employ conformal inference techniques for
counterfactual outcome prediction. Validated through numerical experiments and
applied to real-world environmental data, our framework extends causal
inference methodologies to complex data structures, broadening its
applicability across scientific disciplines.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [392] [ViFusionTST: Deep Fusion of Time-Series Image Representations from Load Signals for Early Bed-Exit Prediction](https://arxiv.org/abs/2506.22498)
*Hao Liu,Yu Hu,Rakiba Rayhana,Ling Bai,Zheng Liu*

Main category: cs.CV

TL;DR: 本文提出用四个低成本负载传感器预测患者离床意图，构建ViFusionTST模型，在真实数据集上表现出色，证明基于图像的负载信号融合用于时间序列分类对预防跌倒有效。


<details>
  <summary>Details</summary>
Motivation: 现有商业警报多在患者离床后触发，需提前预测患者离床意图以预防跌倒。

Method: 用四个负载传感器收集信号，转换为RGB线图和三种纹理图，引入双流Swin Transformer模型ViFusionTST并行处理并通过交叉注意力融合。

Result: 在真实数据集上，ViFusionTST准确率达0.885，F1分数达0.794，超过多个基线模型。

Conclusion: 基于图像的负载传感器信号融合用于时间序列分类是实时、保护隐私的预防跌倒实用有效方案。

Abstract: Bed-related falls remain a leading source of injury in hospitals and
long-term-care facilities, yet many commercial alarms trigger only after a
patient has already left the bed. We show that early bed-exit intent can be
predicted using only four low-cost load cells mounted under the bed legs. The
resulting load signals are first converted into a compact set of complementary
images: an RGB line plot that preserves raw waveforms and three texture maps -
recurrence plot, Markov transition field, and Gramian angular field - that
expose higher-order dynamics. We introduce ViFusionTST, a dual-stream Swin
Transformer that processes the line plot and texture maps in parallel and fuses
them through cross-attention to learn data-driven modality weights.
  To provide a realistic benchmark, we collected six months of continuous data
from 95 beds in a long-term-care facility. On this real-world dataset
ViFusionTST reaches an accuracy of 0.885 and an F1 score of 0.794, surpassing
recent 1D and 2D time-series baselines across F1, recall, accuracy, and AUPRC.
The results demonstrate that image-based fusion of load-sensor signals for time
series classification is a practical and effective solution for real-time,
privacy-preserving fall prevention.

</details>


### [393] [Scalable Dynamic Origin-Destination Demand Estimation Enhanced by High-Resolution Satellite Imagery Data](https://arxiv.org/abs/2506.22499)
*Jiachao Liu,Pablo Guarda,Koichiro Niinuma,Sean Qian*

Main category: cs.CV

TL;DR: 本文提出多类中观网络模型中动态起终点需求估计的集成框架，结合卫星图像与传统交通数据，实验表明该框架能提升估计性能并处理大规模网络。


<details>
  <summary>Details</summary>
Motivation: 传统本地传感器数据有局限性，卫星图像可提供全市道路和交通信息，克服数据可用性问题。

Method: 设计计算机视觉管道处理图像数据，生成按车辆类别划分的路段交通密度观测；构建基于计算图的动态起终点需求估计模型，联合匹配多源数据校准动态网络状态。

Result: 样本外测试显示卫星衍生密度数据补充传统数据能显著提升估计性能，特别是无本地传感器的路段；真实实验证实框架可处理大规模网络。

Conclusion: 该框架有潜力在不同规模城市实际应用，敏感性分析评估了卫星图像数据质量的影响。

Abstract: This study presents a novel integrated framework for dynamic
origin-destination demand estimation (DODE) in multi-class mesoscopic network
models, leveraging high-resolution satellite imagery together with conventional
traffic data from local sensors. Unlike sparse local detectors, satellite
imagery offers consistent, city-wide road and traffic information of both
parking and moving vehicles, overcoming data availability limitations. To
extract information from imagery data, we design a computer vision pipeline for
class-specific vehicle detection and map matching, generating link-level
traffic density observations by vehicle class. Building upon this information,
we formulate a computational graph-based DODE model that calibrates dynamic
network states by jointly matching observed traffic counts and travel times
from local sensors with density measurements derived from satellite imagery. To
assess the accuracy and scalability of the proposed framework, we conduct a
series of numerical experiments using both synthetic and real-world data. The
results of out-of-sample tests demonstrate that supplementing traditional data
with satellite-derived density significantly improves estimation performance,
especially for links without local sensors. Real-world experiments also confirm
the framework's capability to handle large-scale networks, supporting its
potential for practical deployment in cities of varying sizes. Sensitivity
analysis further evaluates the impact of data quality related to satellite
imagery data.

</details>


### [394] [Visual-Semantic Knowledge Conflicts in Operating Rooms: Synthetic Data Curation for Surgical Risk Perception in Multimodal Large Language Models](https://arxiv.org/abs/2506.22500)
*Weiyi Zhao,Xiaoyu Tan,Liang Liu,Sijia Li,Youwei Song,Xihe Qiu*

Main category: cs.CV

TL;DR: 本文引入 OR - VSKC 数据集解决 MLLMs 在手术室风险检测中的视觉 - 语义知识冲突问题，评估了模型表现并提出主要贡献。


<details>
  <summary>Details</summary>
Motivation: 解决 MLLMs 在手术室风险检测中存在的视觉 - 语义知识冲突问题，缓解数据稀缺状况。

Method: 引入包含超 34000 张合成图像和 214 张人工标注图像的 OR - VSKC 数据集，对 MLLMs 进行微调。

Result: 微调后 MLLMs 对训练过的冲突实体检测能力提升，泛化到新视角表现较好，但对未训练实体类型表现差。

Conclusion: 强调学习的特异性和全面训练的必要性，给出数据生成方法、开源数据集及基准，对代表性 MLLMs 进行实证分析。

Abstract: Surgical risk identification is critical for patient safety and reducing
preventable medical errors. While multimodal large language models (MLLMs) show
promise for automated operating room (OR) risk detection, they often exhibit
visual-semantic knowledge conflicts (VS-KC), failing to identify visual safety
violations despite understanding textual rules. To address this, we introduce a
dataset comprising over 34,000 synthetic images generated by diffusion models,
depicting operating room scenes containing entities that violate established
safety rules. These images were created to alleviate data scarcity and examine
MLLMs vulnerabilities. In addition, the dataset includes 214 human-annotated
images that serve as a gold-standard reference for validation. This
comprehensive dataset, spanning diverse perspectives, stages, and
configurations, is designed to expose and study VS-KC. Fine-tuning on OR-VSKC
significantly improves MLLMs' detection of trained conflict entities and
generalizes well to new viewpoints for these entities, but performance on
untrained entity types remains poor, highlighting learning specificity and the
need for comprehensive training. The main contributions of this work include:
(1) a data generation methodology tailored for rule-violation scenarios; (2)
the release of the OR-VSKC dataset and its associated benchmark as open-source
resources; and (3) an empirical analysis of violation-sensitive knowledge
consistency in representative MLLMs. The dataset and appendix are available at
https://github.com/zgg2577/VS-KC.

</details>


### [395] [How Can Multimodal Remote Sensing Datasets Transform Classification via SpatialNet-ViT?](https://arxiv.org/abs/2506.22501)
*Gautam Siddharth Kashyap,Manaswi Kulahara,Nipun Joshi,Usman Naseem*

Main category: cs.CV

TL;DR: 提出SpatialNet - ViT模型解决遥感分类泛化性问题，结合多技术提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有遥感分类研究多聚焦窄任务或数据集，泛化能力受限。

Method: 提出SpatialNet - ViT模型，结合Vision Transformers和Multi - Task Learning，采用数据增强、迁移学习和多任务学习技术。

Result: 该集成方法结合空间感知和上下文理解，提高分类准确性和可扩展性。

Conclusion: 所提模型和采用的技术能增强模型鲁棒性和跨不同数据集的泛化能力。

Abstract: Remote sensing datasets offer significant promise for tackling key
classification tasks such as land-use categorization, object presence
detection, and rural/urban classification. However, many existing studies tend
to focus on narrow tasks or datasets, which limits their ability to generalize
across various remote sensing classification challenges. To overcome this, we
propose a novel model, SpatialNet-ViT, leveraging the power of Vision
Transformers (ViTs) and Multi-Task Learning (MTL). This integrated approach
combines spatial awareness with contextual understanding, improving both
classification accuracy and scalability. Additionally, techniques like data
augmentation, transfer learning, and multi-task learning are employed to
enhance model robustness and its ability to generalize across diverse datasets

</details>


### [396] [FreeDNA: Endowing Domain Adaptation of Diffusion-Based Dense Prediction with Training-Free Domain Noise Alignment](https://arxiv.org/abs/2506.22509)
*Hang Xu,Jie Huang,Linjiang Huang,Dong Li,Yidi Liu,Feng Zhao*

Main category: cs.CV

TL;DR: 提出无训练机制赋予扩散式密集预测（DDP）框架领域自适应（DA）能力，含DNA方法及源自由DA策略，在四个任务验证有效性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型能有效建模含领域信息的分布变换，扩散中的曝光偏差带来领域偏移，且噪声预测统计能捕捉不同领域，探索适合DDP框架的DA设计。

Method: 提出无训练的领域噪声对齐（DNA）方法，源领域可用时直接对齐噪声统计；源自由DA时，用高置信区域统计逐步引导采样过程中噪声统计调整。

Result: 方法在四个常见密集预测任务中提升了DDP模型的DA能力。

Conclusion: 所提无训练机制有效增强了DDP模型的领域自适应能力。

Abstract: Domain Adaptation(DA) for dense prediction tasks is an important topic, which
enhances the dense prediction model's performance when tested on its unseen
domain. Recently, with the development of Diffusion-based Dense Prediction
(DDP) models, the exploration of DA designs tailored to this framework is worth
exploring, since the diffusion model is effective in modeling the distribution
transformation that comprises domain information. In this work, we propose a
training-free mechanism for DDP frameworks, endowing them with DA capabilities.
Our motivation arises from the observation that the exposure bias (e.g., noise
statistics bias) in diffusion brings domain shift, and different domains in
conditions of DDP models can also be effectively captured by the noise
prediction statistics. Based on this, we propose a training-free Domain Noise
Alignment (DNA) approach, which alleviates the variations of noise statistics
to domain changes during the diffusion sampling process, thereby achieving
domain adaptation. Specifically, when the source domain is available, we
directly adopt the DNA method to achieve domain adaptation by aligning the
noise statistics of the target domain with those of the source domain. For the
more challenging source-free DA, inspired by the observation that regions
closer to the source domain exhibit higher confidence meeting variations of
sampling noise, we utilize the statistics from the high-confidence regions
progressively to guide the noise statistic adjustment during the sampling
process. Notably, our method demonstrates the effectiveness of enhancing the DA
capability of DDP models across four common dense prediction tasks. Code is
available at
\href{https://github.com/xuhang07/FreeDNA}{https://github.com/xuhang07/FreeDNA}.

</details>


### [397] [Lightning the Night with Generative Artificial Intelligence](https://arxiv.org/abs/2506.22511)
*Tingting Zhou,Feng Zhang,Haoyang Fu,Baoxiang Pan,Renhe Zhang,Feng Lu,Zhixin Yang*

Main category: cs.CV

TL;DR: 本文利用生成扩散模型，基于风云 - 4B卫星多波段热红外亮温数据开发RefDiff模型实现夜间可见光反射率反演，相比经典模型精度提升且能提供不确定性估计，验证了夜间反演能力。


<details>
  <summary>Details</summary>
Motivation: 由于夜间缺乏可见光，无法利用可见光反射率数据进行全天连续气象观测，需解决这一限制。

Method: 利用生成扩散模型，基于风云 - 4B卫星上先进静止轨道辐射成像仪（AGRI）的多波段热红外亮温数据，开发Reflectance Diffusion (RefDiff) 模型进行夜间可见光反射率反演。

Result: RefDiff模型相比经典模型通过集合平均显著提高了精度，能提供不确定性估计，SSIM指数可达0.90，在复杂云结构和厚云区域改进显著，利用VIIRS夜间产品验证其夜间反演能力与白天相当。

Conclusion: 本研究在夜间可见光反射率反演能力上取得了重大进展，有潜力拓展夜间可见光数据的应用。

Abstract: The visible light reflectance data from geostationary satellites is crucial
for meteorological observations and plays an important role in weather
monitoring and forecasting. However, due to the lack of visible light at night,
it is impossible to conduct continuous all-day weather observations using
visible light reflectance data. This study pioneers the use of generative
diffusion models to address this limitation. Based on the multi-band thermal
infrared brightness temperature data from the Advanced Geostationary Radiation
Imager (AGRI) onboard the Fengyun-4B (FY4B) geostationary satellite, we
developed a high-precision visible light reflectance retrieval model, called
Reflectance Diffusion (RefDiff), which enables 0.47~\mu\mathrm{m},
0.65~\mu\mathrm{m}, and 0.825~\mu\mathrm{m} bands visible light reflectance
retrieval at night. Compared to the classical models, RefDiff not only
significantly improves accuracy through ensemble averaging but also provides
uncertainty estimation. Specifically, the SSIM index of RefDiff can reach 0.90,
with particularly significant improvements in areas with complex cloud
structures and thick clouds. The model's nighttime retrieval capability was
validated using VIIRS nighttime product, demonstrating comparable performance
to its daytime counterpart. In summary, this research has made substantial
progress in the ability to retrieve visible light reflectance at night, with
the potential to expand the application of nighttime visible light data.

</details>


### [398] [Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale Dataset](https://arxiv.org/abs/2506.22554)
*Vasu Agrawal,Akinniyi Akinyemi,Kathryn Alvero,Morteza Behrooz,Julia Buffalini,Fabio Maria Carlucci,Joy Chen,Junming Chen,Zhang Chen,Shiyang Cheng,Praveen Chowdary,Joe Chuang,Antony D'Avirro,Jon Daly,Ning Dong,Mark Duppenthaler,Cynthia Gao,Jeff Girard,Martin Gleize,Sahir Gomez,Hongyu Gong,Srivathsan Govindarajan,Brandon Han,Sen He,Denise Hernandez,Yordan Hristov,Rongjie Huang,Hirofumi Inaguma,Somya Jain,Raj Janardhan,Qingyao Jia,Christopher Klaiber,Dejan Kovachev,Moneish Kumar,Hang Li,Yilei Li,Pavel Litvin,Wei Liu,Guangyao Ma,Jing Ma,Martin Ma,Xutai Ma,Lucas Mantovani,Sagar Miglani,Sreyas Mohan,Louis-Philippe Morency,Evonne Ng,Kam-Woh Ng,Tu Anh Nguyen,Amia Oberai,Benjamin Peloquin,Juan Pino,Jovan Popovic,Omid Poursaeed,Fabian Prada,Alice Rakotoarison,Alexander Richard,Christophe Ropers,Safiyyah Saleem,Vasu Sharma,Alex Shcherbyna,Jia Shen,Jie Shen,Anastasis Stathopoulos,Anna Sun,Paden Tomasello,Tuan Tran,Arina Turkatenko,Bo Wan,Chao Wang,Jeff Wang,Mary Williamson,Carleigh Wood,Tao Xiang,Yilin Yang,Julien Yao,Chen Zhang,Jiemin Zhang,Xinyue Zhang,Jason Zheng,Pavlo Zhyzheria,Jan Zikes,Michael Zollhoefer*

Main category: cs.CV

TL;DR: 文章引入Seamless Interaction Dataset，开发利用该数据集的模型生成对话动态手势和表情，探讨评估方法，推动更直观响应的人机交互。


<details>
  <summary>Details</summary>
Motivation: 为开发具有社交智能的AI技术，需开发能理解和生成对话行为动态的模型。

Method: 引入大规模数据集Seamless Interaction Dataset，开发利用该数据集的模型，提出含LLM语音变体及2D、3D渲染集成方法，设计可控变体模型，探讨评估模型质量的方法。

Result: 数据集可助力开发理解对话动态的AI技术，模型能生成与人类语音匹配的动态手势和表情，有多种变体。

Conclusion: 研究成果展示了实现更直观、响应性更强的人机交互的潜力。

Abstract: Human communication involves a complex interplay of verbal and nonverbal
signals, essential for conveying meaning and achieving interpersonal goals. To
develop socially intelligent AI technologies, it is crucial to develop models
that can both comprehend and generate dyadic behavioral dynamics. To this end,
we introduce the Seamless Interaction Dataset, a large-scale collection of over
4,000 hours of face-to-face interaction footage from over 4,000 participants in
diverse contexts. This dataset enables the development of AI technologies that
understand dyadic embodied dynamics, unlocking breakthroughs in virtual agents,
telepresence experiences, and multimodal content analysis tools. We also
develop a suite of models that utilize the dataset to generate dyadic motion
gestures and facial expressions aligned with human speech. These models can
take as input both the speech and visual behavior of their interlocutors. We
present a variant with speech from an LLM model and integrations with 2D and 3D
rendering methods, bringing us closer to interactive virtual agents.
Additionally, we describe controllable variants of our motion models that can
adapt emotional responses and expressivity levels, as well as generating more
semantically-relevant gestures. Finally, we discuss methods for assessing the
quality of these dyadic motion models, which are demonstrating the potential
for more intuitive and responsive human-AI interactions.

</details>


### [399] [Unifying Biomedical Vision-Language Expertise: Towards a Generalist Foundation Model via Multi-CLIP Knowledge Distillation](https://arxiv.org/abs/2506.22567)
*Shansong Wang,Zhecheng Jin,Mingzhe Hu,Mojtaba Safari,Feng Zhao,Chih-Wei Chang,Richard LJ Qiu,Justin Roper,David S. Yu,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 文章介绍MMKD - CLIP，一种通过多医学CLIP知识蒸馏开发的通用生物医学基础模型，经评估表现优异，表明多教师知识蒸馏是构建高性能生物医学基础模型的有效范式。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP模型在生物医学领域应用因缺乏大规模数据、图像模态异质性和数据标准碎片化受限，难以从无到有训练统一通用的生物医学基础模型。

Method: 引入MMKD - CLIP，从九个先进的特定领域或通用生物医学CLIP模型中蒸馏知识，采用两阶段训练流程，先进行CLIP式预训练，再进行特征级蒸馏。

Result: 在58个不同生物医学数据集上评估，涵盖九种图像模态的超1080万张生物医学图像，六种核心任务类型，MMKD - CLIP始终优于所有教师模型，在图像领域和任务设置中表现出显著的鲁棒性和泛化性。

Conclusion: 多教师知识蒸馏是在现实数据可用性限制下构建高性能生物医学基础模型的可扩展且有效的范式。

Abstract: CLIP models pretrained on natural images with billion-scale image-text pairs
have demonstrated impressive capabilities in zero-shot classification,
cross-modal retrieval, and open-ended visual answering. However, transferring
this success to biomedicine is hindered by the scarcity of large-scale
biomedical image-text corpora, the heterogeneity of image modalities, and
fragmented data standards across institutions. These limitations hinder the
development of a unified and generalizable biomedical foundation model trained
from scratch. To overcome this, we introduce MMKD-CLIP, a generalist biomedical
foundation model developed via Multiple Medical CLIP Knowledge Distillation.
Rather than relying on billion-scale raw data, MMKD-CLIP distills knowledge
from nine state-of-the-art domain-specific or generalist biomedical CLIP
models, each pretrained on millions of biomedical image-text pairs. Our
two-stage training pipeline first performs CLIP-style pretraining on over 2.9
million biomedical image-text pairs from 26 image modalities, followed by
feature-level distillation using over 19.2 million feature pairs extracted from
teacher models. We evaluate MMKD-CLIP on 58 diverse biomedical datasets,
encompassing over 10.8 million biomedical images across nine image modalities.
The evaluation spans six core task types: zero-shot classification, linear
probing, cross-modal retrieval, visual question answering, survival prediction,
and cancer diagnosis. MMKD-CLIP consistently outperforms all teacher models
while demonstrating remarkable robustness and generalization across image
domains and task settings. These results underscore that multi-teacher
knowledge distillation is a scalable and effective paradigm for building
high-performing biomedical foundation models under the practical constraints of
real-world data availability.

</details>


### [400] [PhonemeFake: Redefining Deepfake Realism with Language-Driven Segmental Manipulation and Adaptive Bilevel Detection](https://arxiv.org/abs/2506.22783)
*Oguzhan Baser,Ahmet Ege Tanriverdi,Sriram Vishwanath,Sandeep P. Chinchali*

Main category: cs.CV

TL;DR: 现有深度伪造数据集无法欺骗人类感知，提出PhonemeFake攻击及检测模型，实验显示模型有良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造数据集不能像真实攻击那样影响公众话语，需更现实的攻击向量。

Method: 引入PhonemeFake攻击，利用语言推理操纵关键语音片段，发布数据集和开源双层深度伪造片段检测模型。

Result: PhonemeFake攻击使人类感知降低42%，基准准确率降低94%；检测模型使EER降低91%，速度提升90%。

Conclusion: 提出的检测模型是可扩展解决方案，有最小计算开销和精确的定位能力，优于现有模型。

Abstract: Deepfake (DF) attacks pose a growing threat as generative models become
increasingly advanced. However, our study reveals that existing DF datasets
fail to deceive human perception, unlike real DF attacks that influence public
discourse. It highlights the need for more realistic DF attack vectors. We
introduce PhonemeFake (PF), a DF attack that manipulates critical speech
segments using language reasoning, significantly reducing human perception by
up to 42% and benchmark accuracies by up to 94%. We release an easy-to-use PF
dataset on HuggingFace and open-source bilevel DF segment detection model that
adaptively prioritizes compute on manipulated regions. Our extensive
experiments across three known DF datasets reveal that our detection model
reduces EER by 91% while achieving up to 90% speed-up, with minimal compute
overhead and precise localization beyond existing models as a scalable
solution.

</details>


### [401] [Single-Frame Point-Pixel Registration via Supervised Cross-Modal Feature Matching](https://arxiv.org/abs/2506.22784)
*Yu Han,Zhiwei Huang,Yanting Zhang,Fangjun Ding,Shen Cai,Rui Fan*

Main category: cs.CV

TL;DR: 本文提出无检测器框架用于LiDAR与相机视图的点 - 像素直接匹配，在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效弥合LiDAR点云和相机图像的模态差距，且难以处理单帧LiDAR的稀疏性和噪声问题。

Method: 将LiDAR强度图投影到2D视图，输入基于注意力的无检测器匹配网络，引入重复性评分机制。

Result: 在KITTI、nuScenes和MIAS - LCEC - TF70基准测试中取得了最先进的性能，仅使用单帧LiDAR就超过了nuScenes上的先前方法。

Conclusion: 所提出的无检测器框架能有效解决点 - 像素匹配问题，在稀疏单帧LiDAR设置下表现良好。

Abstract: Point-pixel registration between LiDAR point clouds and camera images is a
fundamental yet challenging task in autonomous driving and robotic perception.
A key difficulty lies in the modality gap between unstructured point clouds and
structured images, especially under sparse single-frame LiDAR settings.
Existing methods typically extract features separately from point clouds and
images, then rely on hand-crafted or learned matching strategies. This separate
encoding fails to bridge the modality gap effectively, and more critically,
these methods struggle with the sparsity and noise of single-frame LiDAR, often
requiring point cloud accumulation or additional priors to improve reliability.
Inspired by recent progress in detector-free matching paradigms (e.g.
MatchAnything), we revisit the projection-based approach and introduce the
detector-free framework for direct point-pixel matching between LiDAR and
camera views. Specifically, we project the LiDAR intensity map into a 2D view
from the LiDAR perspective and feed it into an attention-based detector-free
matching network, enabling cross-modal correspondence estimation without
relying on multi-frame accumulation. To further enhance matching reliability,
we introduce a repeatability scoring mechanism that acts as a soft visibility
prior. This guides the network to suppress unreliable matches in regions with
low intensity variation, improving robustness under sparse input. Extensive
experiments on KITTI, nuScenes, and MIAS-LCEC-TF70 benchmarks demonstrate that
our method achieves state-of-the-art performance, outperforming prior
approaches on nuScenes (even those relying on accumulated point clouds),
despite using only single-frame LiDAR.

</details>


### [402] [Listener-Rewarded Thinking in VLMs for Image Preferences](https://arxiv.org/abs/2506.22832)
*Alexander Gambashidze,Li Pengyi,Matvey Skripkin,Andrey Galichin,Anton Gusarov,Konstantin Sobolev,Andrey Kuznetsov,Ivan Oseledets*

Main category: cs.CV

TL;DR: 提出listener - augmented GRPO框架解决当前奖励模型泛化问题，在相关基准测试中取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 当前奖励模型泛化性差，监督微调会导致过拟合且需要复杂注释流程，现有强化学习方法存在推理准确性下降问题。

Method: 引入listener - augmented GRPO框架，用listener重新评估reasoner的思维链，提供密集校准的置信度分数来塑造强化学习奖励信号。

Result: 在ImageReward基准测试中达到67.4%的准确率，在大规模人类偏好数据集上显著提高分布外性能，减少推理矛盾。

Conclusion: 基于listener的奖励为使视觉语言模型与细微人类偏好对齐提供了可扩展、数据高效的途径。

Abstract: Training robust and generalizable reward models for human visual preferences
is essential for aligning text-to-image and text-to-video generative models
with human intent. However, current reward models often fail to generalize, and
supervised fine-tuning leads to memorization, demanding complex annotation
pipelines. While reinforcement learning (RL), specifically Group Relative
Policy Optimization (GRPO), improves generalization, we uncover a key failure
mode: a significant drop in reasoning accuracy occurs when a model's reasoning
trace contradicts that of an independent, frozen vision-language model
("listener") evaluating the same output. To address this, we introduce a
listener-augmented GRPO framework. Here, the listener re-evaluates the
reasoner's chain-of-thought to provide a dense, calibrated confidence score,
shaping the RL reward signal. This encourages the reasoner not only to answer
correctly, but to produce explanations that are persuasive to an independent
model. Our listener-shaped reward scheme achieves best accuracy on the
ImageReward benchmark (67.4%), significantly improves out-of-distribution (OOD)
performance on a large-scale human preference dataset (1.2M votes, up to +6%
over naive reasoner), and reduces reasoning contradictions compared to strong
GRPO and SFT baselines. These results demonstrate that listener-based rewards
provide a scalable, data-efficient path to aligning vision-language models with
nuanced human preferences. We will release our reasoning model here:
https://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner.

</details>


### [403] [Modulated Diffusion: Accelerating Generative Modeling with Modulated Quantization](https://arxiv.org/abs/2506.22463)
*Weizhi Gao,Zhichao Hou,Junqi Yin,Feiyi Wang,Linyu Peng,Xiaorui Liu*

Main category: cs.CV

TL;DR: 本文研究扩散模型加速技术，指出其局限，提出创新框架MoDiff加速生成建模，实验证明其优势。


<details>
  <summary>Details</summary>
Motivation: 扩散模型迭代采样计算成本高，现有加速技术存在计算误差和生成质量方面的局限。

Method: 引入Modulated Diffusion (MoDiff)框架，通过调制量化和误差补偿加速生成建模。

Result: 在CIFAR - 10和LSUN上的实验表明，MoDiff在无性能下降的情况下将激活量化从8位降至3位。

Conclusion: MoDiff继承现有方法优点，是加速所有扩散模型的通用框架，有理论支持和良好实验效果。

Abstract: Diffusion models have emerged as powerful generative models, but their high
computation cost in iterative sampling remains a significant bottleneck. In
this work, we present an in-depth and insightful study of state-of-the-art
acceleration techniques for diffusion models, including caching and
quantization, revealing their limitations in computation error and generation
quality. To break these limits, this work introduces Modulated Diffusion
(MoDiff), an innovative, rigorous, and principled framework that accelerates
generative modeling through modulated quantization and error compensation.
MoDiff not only inherents the advantages of existing caching and quantization
methods but also serves as a general framework to accelerate all diffusion
models. The advantages of MoDiff are supported by solid theoretical insight and
analysis. In addition, extensive experiments on CIFAR-10 and LSUN demonstrate
that MoDiff significant reduces activation quantization from 8 bits to 3 bits
without performance degradation in post-training quantization (PTQ). Our code
implementation is available at https://github.com/WeizhiGao/MoDiff.

</details>


### [404] [Mask-aware Text-to-Image Retrieval: Referring Expression Segmentation Meets Cross-modal Retrieval](https://arxiv.org/abs/2506.22864)
*Li-Cheng Shen,Jih-Kang Hsieh,Wei-Hua Li,Chu-Song Chen*

Main category: cs.CV

TL;DR: 本文提出MaTIR任务统一TIR和RES，用两阶段框架处理，在COCO和D³数据集上效果优于之前方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像检索方法缺乏可解释性，指代表达分割应用于大量图像集时计算成本高，需统一两者。

Method: 提出两阶段框架，第一阶段进行分割感知图像检索，第二阶段用MLLM重排序和目标定位；先利用SAM 2和Alpha - CLIP进行离线处理，后用MLLM优化排名和生成边界框。

Result: 在COCO和D³数据集上评估，检索准确性和分割质量显著提高。

Conclusion: 提出的方法在图像检索和目标分割方面优于之前方法。

Abstract: Text-to-image retrieval (TIR) aims to find relevant images based on a textual
query, but existing approaches are primarily based on whole-image captions and
lack interpretability. Meanwhile, referring expression segmentation (RES)
enables precise object localization based on natural language descriptions but
is computationally expensive when applied across large image collections. To
bridge this gap, we introduce Mask-aware TIR (MaTIR), a new task that unifies
TIR and RES, requiring both efficient image search and accurate object
segmentation. To address this task, we propose a two-stage framework,
comprising a first stage for segmentation-aware image retrieval and a second
stage for reranking and object grounding with a multimodal large language model
(MLLM). We leverage SAM 2 to generate object masks and Alpha-CLIP to extract
region-level embeddings offline at first, enabling effective and scalable
online retrieval. Secondly, MLLM is used to refine retrieval rankings and
generate bounding boxes, which are matched to segmentation masks. We evaluate
our approach on COCO and D$^3$ datasets, demonstrating significant improvements
in both retrieval accuracy and segmentation quality over previous methods.

</details>


### [405] [Region-Aware CAM: High-Resolution Weakly-Supervised Defect Segmentation via Salient Region Perception](https://arxiv.org/abs/2506.22866)
*Hang-Cheng Dong,Lu Zou,Bingguo Liu,Dong Ye,Guodong Liu*

Main category: cs.CV

TL;DR: 本文提出一种弱监督语义分割框架用于工业表面缺陷检测，包含区域感知CAM和伪标签训练，实验证明其优越性，为资源受限场景提供解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统语义分割和目标检测模型依赖大规模标注数据集，与缺陷检测实际需求冲突。

Method: 提出包含区域感知CAM和伪标签训练的弱监督语义分割框架，引入FGBP细化目标区域，开发区域感知加权模块提升空间精度，进行伪标签分割迭代优化模型。

Result: 在工业缺陷数据集上的综合实验证明了该方法的优越性。

Conclusion: 所提框架有效弥合了弱监督学习和高精度缺陷分割之间的差距，为资源受限的工业场景提供了实用解决方案。

Abstract: Surface defect detection plays a critical role in industrial quality
inspection. Recent advances in artificial intelligence have significantly
enhanced the automation level of detection processes. However, conventional
semantic segmentation and object detection models heavily rely on large-scale
annotated datasets, which conflicts with the practical requirements of defect
detection tasks. This paper proposes a novel weakly supervised semantic
segmentation framework comprising two key components: a region-aware class
activation map (CAM) and pseudo-label training. To address the limitations of
existing CAM methods, especially low-resolution thermal maps, and insufficient
detail preservation, we introduce filtering-guided backpropagation (FGBP),
which refines target regions by filtering gradient magnitudes to identify areas
with higher relevance to defects. Building upon this, we further develop a
region-aware weighted module to enhance spatial precision. Finally,
pseudo-label segmentation is implemented to refine the model's performance
iteratively. Comprehensive experiments on industrial defect datasets
demonstrate the superiority of our method. The proposed framework effectively
bridges the gap between weakly supervised learning and high-precision defect
segmentation, offering a practical solution for resource-constrained industrial
scenarios.

</details>


### [406] [STR-Match: Matching SpatioTemporal Relevance Score for Training-Free Video Editing](https://arxiv.org/abs/2506.22868)
*Junsung Lee,Junoh Kang,Bohyung Han*

Main category: cs.CV

TL;DR: 提出无训练视频编辑算法STR - Match，利用STR分数通过潜在优化生成时空连贯视频，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 以往文本引导视频编辑方法存在时间不一致、运动失真和领域转换受限等问题，原因是编辑过程中对时空像素相关性建模不足。

Method: 提出无训练的视频编辑算法STR - Match，利用2D空间注意力和1D时间模块计算STR分数以捕捉相邻帧的时空像素相关性，结合潜在掩码进行潜在优化。

Result: STR - Match生成的视频时间上一致、视觉上逼真，即使在显著的领域转换下也能保持关键视觉属性，在视觉质量和时空一致性上优于现有方法。

Conclusion: STR - Match是一种有效的视频编辑算法，能解决现有方法的局限，在视频编辑任务中有更好表现。

Abstract: Previous text-guided video editing methods often suffer from temporal
inconsistency, motion distortion, and-most notably-limited domain
transformation. We attribute these limitations to insufficient modeling of
spatiotemporal pixel relevance during the editing process. To address this, we
propose STR-Match, a training-free video editing algorithm that produces
visually appealing and spatiotemporally coherent videos through latent
optimization guided by our novel STR score. The score captures spatiotemporal
pixel relevance across adjacent frames by leveraging 2D spatial attention and
1D temporal modules in text-to-video (T2V) diffusion models, without the
overhead of computationally expensive 3D attention mechanisms. Integrated into
a latent optimization framework with a latent mask, STR-Match generates
temporally consistent and visually faithful videos, maintaining strong
performance even under significant domain transformations while preserving key
visual attributes of the source. Extensive experiments demonstrate that
STR-Match consistently outperforms existing methods in both visual quality and
spatiotemporal consistency.

</details>


### [407] [Decoupled Seg Tokens Make Stronger Reasoning Video Segmenter and Grounder](https://arxiv.org/abs/2506.22880)
*Dang Jisheng,Wu Xudong,Wang Bimei,Lv Ning,Chen Jiayu,Jingwen Zhao,Yichu liu,Jizhao Liu,Juncheng Li,Teng Wang*

Main category: cs.CV

TL;DR: 提出DeSa2VA方案解决现有视频分割和定位方法中信息纠缠问题，经实验验证效果佳，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有视频分割和定位方法（如Sa2VA）直接融合特征，导致动态视觉信息和静态语义纠缠，降低分割精度，需解决此问题。

Method: 提出解耦增强提示方案DeSa2VA，包括将文本真实标签转换为点级提示并生成文本掩码，用混合损失函数优化；用线性投影分离大语言模型隐藏状态；用动态掩码融合策略结合解耦特征并进行三重监督。

Result: 在图像分割、图像问答、视频分割和视频问答等多样任务中达到了最先进性能。

Conclusion: DeSa2VA能有效解决信息处理限制问题，提升分割准确性，在多任务中表现出色。

Abstract: Existing video segmenter and grounder approaches, exemplified by Sa2VA,
directly fuse features within segmentation models. This often results in an
undesirable entanglement of dynamic visual information and static semantics,
thereby degrading segmentation accuracy. To systematically mitigate this issue,
we propose DeSa2VA, a decoupling-enhanced prompting scheme integrating text
pre-training and a linear decoupling module to address the information
processing limitations inherent in SAM-2. Specifically, first, we devise a
pre-training paradigm that converts textual ground-truth labels into
point-level prompts while generating corresponding text masks. These masks are
refined through a hybrid loss function to strengthen the model's semantic
grounding capabilities. Next, we employ linear projection to disentangle hidden
states that generated by a large language model into distinct textual and
visual feature subspaces. Finally, a dynamic mask fusion strategy
synergistically combines these decoupled features through triple supervision
from predicted text/visual masks and ground-truth annotations. Extensive
experiments demonstrate state-of-the-art performance across diverse tasks,
including image segmentation, image question answering, video segmentation, and
video question answering. Our codes are available at
https://github.com/longmalongma/DeSa2VA.

</details>


### [408] [What Makes a Dribble Successful? Insights From 3D Pose Tracking Data](https://arxiv.org/abs/2506.22503)
*Michiel Schepers,Pieter Robberechts,Jan Van Haaren,Jesse Davis*

Main category: cs.CV

TL;DR: 研究探索姿势跟踪数据对提升理解盘带技能的作用，提取欧冠比赛盘带姿势特征，发现部分特征有助于预测盘带成功，结合传统2D数据可提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 以往研究依赖2D位置跟踪数据评估盘带，无法捕捉平衡、方向和控球等方面，限制了对盘带的深入理解，因此探索姿势跟踪数据的作用。

Method: 从2022/23欧冠赛季的1736次盘带中提取基于姿势的新特征，并评估其对盘带成功的影响。

Result: 捕捉攻击者平衡以及攻击者与防守者方向对齐的特征对预测盘带成功有帮助，结合传统2D位置数据特征可提升模型性能。

Conclusion: 姿势跟踪数据能改善对盘带技能的理解，相关特征有助于预测盘带成功，结合传统数据可提高模型表现。

Abstract: Data analysis plays an increasingly important role in soccer, offering new
ways to evaluate individual and team performance. One specific application is
the evaluation of dribbles: one-on-one situations where an attacker attempts to
bypass a defender with the ball. While previous research has primarily relied
on 2D positional tracking data, this fails to capture aspects like balance,
orientation, and ball control, limiting the depth of current insights. This
study explores how pose tracking data (capturing players' posture and movement
in three dimensions) can improve our understanding of dribbling skills. We
extract novel pose-based features from 1,736 dribbles in the 2022/23 Champions
League season and evaluate their impact on dribble success. Our results
indicate that features capturing the attacker's balance and the alignment of
the orientation between the attacker and defender are informative for
predicting dribble success. Incorporating these pose-based features on top of
features derived from traditional 2D positional data leads to a measurable
improvement in model performance.

</details>


### [409] [Patch2Loc: Learning to Localize Patches for Unsupervised Brain Lesion Detection](https://arxiv.org/abs/2506.22504)
*Hassan Baker,Austin J. Brockmeier*

Main category: cs.CV

TL;DR: 提出无监督方法Patch2Loc用于脑病变检测，通过学习正常图像块，在推理时检测异常，应用于多数据集，表现优于现有无监督分割方法。


<details>
  <summary>Details</summary>
Motivation: 检测磁共振成像（MRI）中的脑病变对诊断和治疗至关重要，现有监督学习方法需要标注病变，提出无监督方法。

Method: 提出无监督方法Patch2Loc，训练神经网络将图像块映射回其在脑切片中的空间位置，推理时通过位置预测的较高误差和/或方差检测异常块，生成热图并集成到逐像素方法中进行更细粒度分割。

Result: 将该方法应用于多个数据集的肿瘤组织检测，表现优于现有无监督分割方法。

Conclusion: 提出的无监督方法Patch2Loc在脑病变分割方面有效，优于现有无监督分割方法。

Abstract: Detecting brain lesions as abnormalities observed in magnetic resonance
imaging (MRI) is essential for diagnosis and treatment. In the search of
abnormalities, such as tumors and malformations, radiologists may benefit from
computer-aided diagnostics that use computer vision systems trained with
machine learning to segment normal tissue from abnormal brain tissue. While
supervised learning methods require annotated lesions, we propose a new
unsupervised approach (Patch2Loc) that learns from normal patches taken from
structural MRI. We train a neural network model to map a patch back to its
spatial location within a slice of the brain volume. During inference, abnormal
patches are detected by the relatively higher error and/or variance of the
location prediction. This generates a heatmap that can be integrated into
pixel-wise methods to achieve finer-grained segmentation. We demonstrate the
ability of our model to segment abnormal brain tissues by applying our approach
to the detection of tumor tissues in MRI on T2-weighted images from BraTS2021
and MSLUB datasets and T1-weighted images from ATLAS and WMH datasets. We show
that it outperforms the state-of-the art in unsupervised segmentation. The
codebase for this work can be found on our
\href{https://github.com/bakerhassan/Patch2Loc}{GitHub page}.

</details>


### [410] [Weakly Supervised Object Segmentation by Background Conditional Divergence](https://arxiv.org/abs/2506.22505)
*Hassan Baker,Matthew S. Emigh,Austin J. Brockmeier*

Main category: cs.CV

TL;DR: 本文提出用弱监督训练掩膜网络进行二值目标分割的方法，在声纳图像等领域实验取得成功，并扩展到自然图像有合理表现。


<details>
  <summary>Details</summary>
Motivation: 在无大量标注数据的专业图像领域，自动目标分割具有挑战性，获取像素级分割掩码成本高，因此寻求弱监督方法。

Method: 用图像级目标有无的弱监督训练掩膜网络；将分割对象放入纯背景图像创建反事实背景图像；聚类纯背景图像，学习时将对象融合到目标聚类背景；训练损失包含反事实图像与目标聚类真实图像的差异及纯背景图像监督损失，使用基于样本的差异。

Result: 在侧扫和合成孔径声纳实验中优于仅在自然图像测试的无监督分割基线；扩展到自然图像，不使用预训练网络、生成网络和对抗判别器也取得合理性能。

Conclusion: 提出的弱监督目标分割方法有效，具有一定通用性，代码开源。

Abstract: As a computer vision task, automatic object segmentation remains challenging
in specialized image domains without massive labeled data, such as synthetic
aperture sonar images, remote sensing, biomedical imaging, etc. In any domain,
obtaining pixel-wise segmentation masks is expensive. In this work, we propose
a method for training a masking network to perform binary object segmentation
using weak supervision in the form of image-wise presence or absence of an
object of interest, which provides less information but may be obtained more
quickly from manual or automatic labeling. A key step in our method is that the
segmented objects can be placed into background-only images to create
realistic, images of the objects with counterfactual backgrounds. To create a
contrast between the original and counterfactual background images, we propose
to first cluster the background-only images, and then during learning create
counterfactual images that blend objects segmented from their original source
backgrounds to backgrounds chosen from a targeted cluster. One term in the
training loss is the divergence between these counterfactual images and the
real object images with backgrounds of the target cluster. The other term is a
supervised loss for background-only images. While an adversarial critic could
provide the divergence, we use sample-based divergences. We conduct experiments
on side-scan and synthetic aperture sonar in which our approach succeeds
compared to previous unsupervised segmentation baselines that were only tested
on natural images. Furthermore, to show generality we extend our experiments to
natural images, obtaining reasonable performance with our method that avoids
pretrained networks, generative networks, and adversarial critics. The basecode
for this work can be found at
\href{GitHub}{https://github.com/bakerhassan/WSOS}.

</details>


### [411] [VisionScores -- A system-segmented image score dataset for deep learning tasks](https://arxiv.org/abs/2506.23030)
*Alejandro Romero Amezcua,Mariano José Juan Rivera Meraz*

Main category: cs.CV

TL;DR: VisionScores提出首个系统分割的图像乐谱数据集，提供结构丰富、信息密度高的图像，有两种场景共24.8k样本，还提供相关元数据等。


<details>
  <summary>Details</summary>
Motivation: 为机器学习和深度学习任务提供结构丰富、高信息密度的图像。

Method: 限定于双手钢琴曲，考虑图形相似度和创作模式，设置两种场景构建数据集。

Result: 构建了包含24.8k个样本的数据集，样本为128×512像素的灰度jpg图像，还提供系统顺序、作品元数据等。

Conclusion: VisionScores能为相关研究提供合适的图像数据集及相关信息。

Abstract: VisionScores presents a novel proposal being the first system-segmented image
score dataset, aiming to offer structure-rich, high information-density images
for machine and deep learning tasks. Delimited to two-handed piano pieces, it
was built to consider not only certain graphic similarity but also composition
patterns, as this creative process is highly instrument-dependent. It provides
two scenarios in relation to composer and composition type. The first, formed
by 14k samples, considers works from different authors but the same composition
type, specifically, Sonatinas. The latter, consisting of 10.8K samples,
presents the opposite case, various composition types from the same author,
being the one selected Franz Liszt. All of the 24.8k samples are formatted as
grayscale jpg images of $128 \times 512$ pixels. VisionScores supplies the
users not only the formatted samples but the systems' order and pieces'
metadata. Moreover, unsegmented full-page scores and the pre-formatted images
are included for further analysis.

</details>


### [412] [Ovis-U1 Technical Report](https://arxiv.org/abs/2506.23044)
*Guo-Hua Wang,Shanshan Zhao,Xinjie Zhang,Liangfu Cao,Pengxin Zhan,Lunhao Duan,Shiyin Lu,Minghao Fu,Xiaohao Chen,Jianshan Zhao,Yang Li,Qing-Guo Chen*

Main category: cs.CV

TL;DR: 介绍30亿参数的多模态统一模型Ovis - U1，具备多种能力，采用新训练方法，表现优于近期模型。


<details>
  <summary>Details</summary>
Motivation: 构建具备多模态理解、文本到图像生成和图像编辑能力的统一模型，改进训练方法以提升性能。

Method: 基于Ovis系列，结合扩散视觉解码器和双向令牌精炼器，采用从语言模型开始的统一训练方法。

Result: 在OpenCompass多模态学术基准测试中得分69.6，在文本到图像生成和图像编辑相关基准测试中也有优异表现。

Conclusion: Ovis - U1作为Ovis统一模型系列的初始版本，拓展了多模态理解、生成和编辑的边界。

Abstract: In this report, we introduce Ovis-U1, a 3-billion-parameter unified model
that integrates multimodal understanding, text-to-image generation, and image
editing capabilities. Building on the foundation of the Ovis series, Ovis-U1
incorporates a diffusion-based visual decoder paired with a bidirectional token
refiner, enabling image generation tasks comparable to leading models like
GPT-4o. Unlike some previous models that use a frozen MLLM for generation
tasks, Ovis-U1 utilizes a new unified training approach starting from a
language model. Compared to training solely on understanding or generation
tasks, unified training yields better performance, demonstrating the
enhancement achieved by integrating these two tasks. Ovis-U1 achieves a score
of 69.6 on the OpenCompass Multi-modal Academic Benchmark, surpassing recent
state-of-the-art models such as Ristretto-3B and SAIL-VL-1.5-2B. In
text-to-image generation, it excels with scores of 83.72 and 0.89 on the
DPG-Bench and GenEval benchmarks, respectively. For image editing, it achieves
4.00 and 6.42 on the ImgEdit-Bench and GEdit-Bench-EN, respectively. As the
initial version of the Ovis unified model series, Ovis-U1 pushes the boundaries
of multimodal understanding, generation, and editing.

</details>


### [413] [XTransfer: Cross-Modality Model Transfer for Human Sensing with Few Data at the Edge](https://arxiv.org/abs/2506.22726)
*Yu Zhang,Xi Zhang,Hualin zhou,Xinyuan Chen,Shang Gao,Hong Jia,Jianfei Yang,Yuankai Qi,Tao Gu*

Main category: cs.CV

TL;DR: 本文提出XTransfer方法用于资源高效、模态无关的模型迁移，在人体感知任务上达最优，降低成本。


<details>
  <summary>Details</summary>
Motivation: 深度学习在边缘系统人体感知应用中因数据有限和资源受限，现有预训练模型迁移方法存在模态偏移、资源需求高、适应性差等问题。

Method: 提出XTransfer方法，通过模型修复解决预训练模型层中的模态偏移，通过层重组高效搜索和重组源模型层创建紧凑模型。

Result: 在不同人体感知数据集上对各种基线进行基准测试，XTransfer在人体感知任务上达到了最优性能。

Conclusion: XTransfer能显著降低传感器数据收集、模型训练和边缘部署成本。

Abstract: Deep learning for human sensing on edge systems offers significant
opportunities for smart applications. However, its training and development are
hindered by the limited availability of sensor data and resource constraints of
edge systems. Current methods that rely on transferring pre-trained models
often encounter issues such as modality shift and high resource demands,
resulting in substantial accuracy loss, resource overhead, and poor
adaptability across different sensing applications. In this paper, we propose
XTransfer, a first-of-its-kind method for resource-efficient, modality-agnostic
model transfer. XTransfer freely leverages single or multiple pre-trained
models and transfers knowledge across different modalities by (i) model
repairing that safely repairs modality shift in pre-trained model layers with
only few sensor data, and (ii) layer recombining that efficiently searches and
recombines layers of interest from source models in a layer-wise manner to
create compact models. We benchmark various baselines across diverse human
sensing datasets spanning different modalities. Comprehensive results
demonstrate that XTransfer achieves state-of-the-art performance on human
sensing tasks while significantly reducing the costs of sensor data collection,
model training, and edge deployment.

</details>


### [414] [MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings](https://arxiv.org/abs/2506.23115)
*Haonan Chen,Hong Liu,Yuping Luo,Liang Wang,Nan Yang,Furu Wei,Zhicheng Dou*

Main category: cs.CV

TL;DR: 提出MoCa框架将预训练VLM转换为双向多模态嵌入模型，解决现有方法局限性，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 当前多模态嵌入模型存在因果注意力不适用于嵌入任务、依赖高质量标注数据、训练目标和数据多样性有限等问题。

Method: 提出两阶段框架MoCa，第一阶段进行模态感知持续预训练，引入联合重建目标；第二阶段进行异构对比微调，利用多样多模态数据。

Result: MoCa在MMEB和ViDoRe - v2基准上持续提升性能，取得新的最优结果，在MMEB上对模型大小和训练数据展现出强扩展性。

Conclusion: MoCa能有效解决现有多模态嵌入模型的局限性，具有良好性能和扩展性。

Abstract: Multimodal embedding models, built upon causal Vision Language Models (VLMs),
have shown promise in various tasks. However, current approaches face three key
limitations: the use of causal attention in VLM backbones is suboptimal for
embedding tasks; scalability issues due to reliance on high-quality labeled
paired data for contrastive learning; and limited diversity in training
objectives and data. To address these issues, we propose MoCa, a two-stage
framework for transforming pre-trained VLMs into effective bidirectional
multimodal embedding models. The first stage, Modality-aware Continual
Pre-training, introduces a joint reconstruction objective that simultaneously
denoises interleaved text and image inputs, enhancing bidirectional
context-aware reasoning. The second stage, Heterogeneous Contrastive
Fine-tuning, leverages diverse, semantically rich multimodal data beyond simple
image-caption pairs to enhance generalization and alignment. Our method
addresses the stated limitations by introducing bidirectional attention through
continual pre-training, scaling effectively with massive unlabeled datasets via
joint reconstruction objectives, and utilizing diverse multimodal data for
enhanced representation robustness. Experiments demonstrate that MoCa
consistently improves performance across MMEB and ViDoRe-v2 benchmarks,
achieving new state-of-the-art results, and exhibits strong scalability with
both model size and training data on MMEB.

</details>


### [415] [Intervening in Black Box: Concept Bottleneck Model for Enhancing Human Neural Network Mutual Understanding](https://arxiv.org/abs/2506.22803)
*Nuoye Xiong,Anqi Dong,Ning Wang,Cong Hua,Guangming Zhu,Mei Lin,Peiyi Shen,Liang Zhang*

Main category: cs.CV

TL;DR: 提出CBM - HNMU模型解决深度学习模型可解释性和干预问题，在多数据集上评估有效果提升，代码开源。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型日益复杂，可解释性降低，现有解释方法缺乏有效干预或未修改模型本身。

Method: 提出CBM - HNMU，利用CBM框架近似黑盒推理，基于全局梯度贡献识别和精炼有害概念，将修正知识蒸馏回黑盒模型。

Result: 在多个CNN和基于transformer的模型及多个数据集上评估，最高准确率提升2.64%，平均准确率最高提升1.03%。

Conclusion: CBM - HNMU能有效增强模型的可解释性和准确性。

Abstract: Recent advances in deep learning have led to increasingly complex models with
deeper layers and more parameters, reducing interpretability and making their
decisions harder to understand. While many methods explain black-box reasoning,
most lack effective interventions or only operate at sample-level without
modifying the model itself. To address this, we propose the Concept Bottleneck
Model for Enhancing Human-Neural Network Mutual Understanding (CBM-HNMU).
CBM-HNMU leverages the Concept Bottleneck Model (CBM) as an interpretable
framework to approximate black-box reasoning and communicate conceptual
understanding. Detrimental concepts are automatically identified and refined
(removed/replaced) based on global gradient contributions. The modified CBM
then distills corrected knowledge back into the black-box model, enhancing both
interpretability and accuracy. We evaluate CBM-HNMU on various CNN and
transformer-based models across Flower-102, CIFAR-10, CIFAR-100, FGVC-Aircraft,
and CUB-200, achieving a maximum accuracy improvement of 2.64% and a maximum
increase in average accuracy across 1.03%. Source code is available at:
https://github.com/XiGuaBo/CBM-HNMU.

</details>


### [416] [MEMFOF: High-Resolution Training for Memory-Efficient Multi-Frame Optical Flow Estimation](https://arxiv.org/abs/2506.23151)
*Vladislav Bargatin,Egor Chistov,Alexander Yakovenko,Dmitriy Vatolin*

Main category: cs.CV

TL;DR: 提出内存高效的多帧光流方法MEMFOF，减少内存开销同时达SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有光流估计方法在高分辨率输入时GPU内存消耗大，需在多帧估计和GPU内存使用间找到平衡。

Method: 重新审视RAFT架构设计，结合减少的相关体积、高分辨率训练协议和多帧估计。

Result: 在多个基准测试中表现出色，如在Spring基准排名第一等，代码开源。

Conclusion: MEMFOF在高分辨率光流估计中准确且高效，验证了其鲁棒性。

Abstract: Recent advances in optical flow estimation have prioritized accuracy at the
cost of growing GPU memory consumption, particularly for high-resolution
(FullHD) inputs. We introduce MEMFOF, a memory-efficient multi-frame optical
flow method that identifies a favorable trade-off between multi-frame
estimation and GPU memory usage. Notably, MEMFOF requires only 2.09 GB of GPU
memory at runtime for 1080p inputs, and 28.5 GB during training, which uniquely
positions our method to be trained at native 1080p without the need for
cropping or downsampling. We systematically revisit design choices from
RAFT-like architectures, integrating reduced correlation volumes and
high-resolution training protocols alongside multi-frame estimation, to achieve
state-of-the-art performance across multiple benchmarks while substantially
reducing memory overhead. Our method outperforms more resource-intensive
alternatives in both accuracy and runtime efficiency, validating its robustness
for flow estimation at high resolutions. At the time of submission, our method
ranks first on the Spring benchmark with a 1-pixel (1px) outlier rate of 3.289,
leads Sintel (clean) with an endpoint error (EPE) of 0.963, and achieves the
best Fl-all error on KITTI-2015 at 2.94%. The code is available at
https://github.com/msu-video-group/memfof.

</details>


### [417] [Concept Pinpoint Eraser for Text-to-image Diffusion Models via Residual Attention Gate](https://arxiv.org/abs/2506.22806)
*Byung Hyun Lee,Sungjin Lim,Seunggyu Lee,Dong Un Kang,Se Young Chun*

Main category: cs.CV

TL;DR: 文本到图像扩散模型发展引发不当概念生成问题，现有概念擦除方法有局限，本文提出CPE框架，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有概念擦除方法在删除目标概念时无法很好保留其他概念的问题，增强扩散模型概念擦除能力。

Method: 提出CPE框架，添加非线性ResAGs，用注意力锚定损失防止遗忘，迭代对抗训练CPE与可学习文本嵌入。

Result: 在名人、艺术风格和明确内容擦除实验中，CPE能保留多样概念，删除目标概念且对抗攻击提示有鲁棒性。

Conclusion: 提出的CPE框架在概念擦除方面优于现有方法，可有效解决相关问题。

Abstract: Remarkable progress in text-to-image diffusion models has brought a major
concern about potentially generating images on inappropriate or trademarked
concepts. Concept erasing has been investigated with the goals of deleting
target concepts in diffusion models while preserving other concepts with
minimal distortion. To achieve these goals, recent concept erasing methods
usually fine-tune the cross-attention layers of diffusion models. In this work,
we first show that merely updating the cross-attention layers in diffusion
models, which is mathematically equivalent to adding \emph{linear} modules to
weights, may not be able to preserve diverse remaining concepts. Then, we
propose a novel framework, dubbed Concept Pinpoint Eraser (CPE), by adding
\emph{nonlinear} Residual Attention Gates (ResAGs) that selectively erase (or
cut) target concepts while safeguarding remaining concepts from broad
distributions by employing an attention anchoring loss to prevent the
forgetting. Moreover, we adversarially train CPE with ResAG and learnable text
embeddings in an iterative manner to maximize erasing performance and enhance
robustness against adversarial attacks. Extensive experiments on the erasure of
celebrities, artistic styles, and explicit contents demonstrated that the
proposed CPE outperforms prior arts by keeping diverse remaining concepts while
deleting the target concepts with robustness against attack prompts. Code is
available at https://github.com/Hyun1A/CPE

</details>


### [418] [Prompting without Panic: Attribute-aware, Zero-shot, Test-Time Calibration](https://arxiv.org/abs/2506.22819)
*Ramya Hebbalaguppe,Tamoghno Kandar,Abhinav Nagpal,Chetan Arora*

Main category: cs.CV

TL;DR: 现有测试时间提示调优（TPT）方法在提升视觉语言模型（VLM）图像识别准确率时会导致置信度校准下降。本文提出用大语言模型（LLM）初始化提示和新的正则化损失函数的方法，实验表明该方法能有效改善校准。


<details>
  <summary>Details</summary>
Motivation: 现有TPT方法在提升VLM准确率时会导致置信度校准下降，限制了其在关键应用中的适用性。

Method: 提出用LLM中目标标签属性的先验知识来仔细初始化测试时间提示；提出新的正则化损失函数以减少类内距离、增加类间距离。

Result: 在不同CLIP架构和15个数据集上实验，该方法（TCA）平均预期校准误差（ECE）为4.11，优于其他对比方法。

Conclusion: 所提方法能有效改善TPT后的校准。

Abstract: Vision-language models (VLM) have demonstrated impressive performance in
image recognition by leveraging self-supervised training on large datasets.
Their performance can be further improved by adapting to the test sample using
test-time prompt tuning (TPT). Unfortunately, the singular focus of TPT
approaches on improving the accuracy suffers from tunnel vision, and leads to
degradation in confidence calibration. This limits the applicability of TPT in
critical applications.
  We make three contributions in this work. (1) We posit that random or naive
initialization of prompts leads to overfitting on a particular test sample, and
is the main reason for miscalibration of the VLM after TPT. To mitigate the
problem, we propose careful initialization of test time prompt using prior
knowledge about the target label attributes from a large language model (LLM);
(2) To further maintain the quality of prompts during \tpt, we propose a novel
regularization loss to reduce intraclass distance, and increase inter-class
distance between the learnt
  Through extensive experiments on different CLIP architectures and 15
datasets, we show that our approach can effectively improve the calibration
after TPT. We report an average expected calibration error (ECE) of 4.11 with
our method, TCA, compared to 11.7 for vanilla TPT, 6.12 for C-TPT (ICLR'24),
6.78 for DiffTPT (CVPR'23), and 8.43 for PromptAlign (NeurIPS'23). The code is
publicly accessible at:
https://github.com/rhebbalaguppe/TCA_PromptWithoutPanic.

</details>


### [419] [Neural Cellular Automata: From Cells to Pixels](https://arxiv.org/abs/2506.22899)
*Ehsan Pajouheshgar,Yitao Xu,Ali Abbasi,Alexander Mordvintsev,Wenzel Jakob,Sabine Süsstrunk*

Main category: cs.CV

TL;DR: 本文结合NCA与隐式解码器，提出新损失函数，解决NCA高分辨率输出限制，实现多任务实时高分辨率生成。


<details>
  <summary>Details</summary>
Motivation: 现有NCAs主要局限于低分辨率网格，受训练时间、内存需求、信息传播和推理计算要求的限制。

Method: 将NCA与轻量级隐式解码器配对，在粗网格上进行NCA演化后由解码器渲染任意分辨率图像，同时提出适用于高分辨率输出的新损失函数。

Result: 结合架构和损失函数显著提升质量、效率和性能，NCA能实时生成全高清输出，推理可高度并行且高效。

Conclusion: 所提框架能让NCA以最小计算开销无缝扩展到高分辨率输出，适用于多种NCA变体和任务。

Abstract: Neural Cellular Automata (NCAs) are bio-inspired systems in which identical
cells self-organize to form complex and coherent patterns by repeatedly
applying simple local rules. NCAs display striking emergent behaviors including
self-regeneration, generalization and robustness to unseen situations, and
spontaneous motion. Despite their success in texture synthesis and
morphogenesis, NCAs remain largely confined to low-resolution grids. This
limitation stems from (1) training time and memory requirements that grow
quadratically with grid size, (2) the strictly local propagation of information
which impedes long-range cell communication, and (3) the heavy compute demands
of real-time inference at high resolution. In this work, we overcome this
limitation by pairing NCA with a tiny, shared implicit decoder, inspired by
recent advances in implicit neural representations. Following NCA evolution on
a coarse grid, a lightweight decoder renders output images at arbitrary
resolution. We also propose novel loss functions for both morphogenesis and
texture synthesis tasks, specifically tailored for high-resolution output with
minimal memory and computation overhead. Combining our proposed architecture
and loss functions brings substantial improvement in quality, efficiency, and
performance. NCAs equipped with our implicit decoder can generate full-HD
outputs in real time while preserving their self-organizing, emergent
properties. Moreover, because each MLP processes cell states independently,
inference remains highly parallelizable and efficient. We demonstrate the
applicability of our approach across multiple NCA variants (on 2D, 3D grids,
and 3D meshes) and multiple tasks, including texture generation and
morphogenesis (growing patterns from a seed), showing that with our proposed
framework, NCAs seamlessly scale to high-resolution outputs with minimal
computational overhead.

</details>


### [420] [UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence with Spatial Reasoning and Understanding](https://arxiv.org/abs/2506.23219)
*Jie Feng,Shengyuan Wang,Tianhui Liu,Yanxin Xi,Yong Li*

Main category: cs.CV

TL;DR: 本文介绍多模态大语言模型UrbanLLaVA，可同时处理四种城市数据，在多样城市任务中表现出色，开源代码和数据。


<details>
  <summary>Details</summary>
Motivation: 当前城市研究方法缺乏统一框架处理多模态数据，多模态大语言模型带来解决契机。

Method: 构建城市指令数据集，提出多阶段训练框架，扩展城市研究基准。

Result: 在三个城市实验表明，UrbanLLaVA在单模态和跨模态任务中优于其他模型，有强大泛化能力。

Conclusion: UrbanLLaVA能有效处理城市多模态数据，可用于多样城市任务。

Abstract: Urban research involves a wide range of scenarios and tasks that require the
understanding of multi-modal data. Current methods often focus on specific data
types and lack a unified framework in urban field for processing them
comprehensively. The recent success of multi-modal large language models
(MLLMs) presents a promising opportunity to overcome this limitation. In this
paper, we introduce $\textit{UrbanLLaVA}$, a multi-modal large language model
designed to process these four types of data simultaneously and achieve strong
performance across diverse urban tasks compared with general MLLMs. In
$\textit{UrbanLLaVA}$, we first curate a diverse urban instruction dataset
encompassing both single-modal and cross-modal urban data, spanning from
location view to global view of urban environment. Additionally, we propose a
multi-stage training framework that decouples spatial reasoning enhancement
from domain knowledge learning, thereby improving the compatibility and
downstream performance of $\textit{UrbanLLaVA}$ across diverse urban tasks.
Finally, we also extend existing benchmark for urban research to assess the
performance of MLLMs across a wide range of urban tasks. Experimental results
from three cities demonstrate that $\textit{UrbanLLaVA}$ outperforms
open-source and proprietary MLLMs in both single-modal tasks and complex
cross-modal tasks and shows robust generalization abilities across cities.
Source codes and data are openly accessible to the research community via
https://github.com/tsinghua-fib-lab/UrbanLLaVA.

</details>


### [421] [VolumetricSMPL: A Neural Volumetric Body Model for Efficient Interactions, Contacts, and Collisions](https://arxiv.org/abs/2506.23236)
*Marko Mihajlovic,Siwei Zhang,Gen Li,Kaifeng Zhao,Lea Müller,Siyu Tang*

Main category: cs.CV

TL;DR: 提出VolumetricSMPL模型，利用NBW提升计算效率，在多任务表现佳，有广泛适用性和性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统参数化人体模型用表面网格处理与其他几何实体交互有局限，现有体积神经隐式人体模型存在鲁棒性不足或计算成本高问题。

Method: 引入VolumetricSMPL模型，利用Neural Blend Weights（NBW）动态混合学习的权重矩阵生成紧凑高效的MLP解码器。

Result: VolumetricSMPL比COAP推理快10倍，GPU内存使用降低6倍，精度更高，有SDF用于接触建模，在四个具有挑战性的任务中表现出色。

Conclusion: VolumetricSMPL具有广泛适用性，能带来显著的性能和效率提升。

Abstract: Parametric human body models play a crucial role in computer graphics and
vision, enabling applications ranging from human motion analysis to
understanding human-environment interactions. Traditionally, these models use
surface meshes, which pose challenges in efficiently handling interactions with
other geometric entities, such as objects and scenes, typically represented as
meshes or point clouds. To address this limitation, recent research has
explored volumetric neural implicit body models. However, existing works are
either insufficiently robust for complex human articulations or impose high
computational and memory costs, limiting their widespread use. To this end, we
introduce VolumetricSMPL, a neural volumetric body model that leverages Neural
Blend Weights (NBW) to generate compact, yet efficient MLP decoders. Unlike
prior approaches that rely on large MLPs, NBW dynamically blends a small set of
learned weight matrices using predicted shape- and pose-dependent coefficients,
significantly improving computational efficiency while preserving
expressiveness. VolumetricSMPL outperforms prior volumetric occupancy model
COAP with 10x faster inference, 6x lower GPU memory usage, enhanced accuracy,
and a Signed Distance Function (SDF) for efficient and differentiable contact
modeling. We demonstrate VolumetricSMPL's strengths across four challenging
tasks: (1) reconstructing human-object interactions from in-the-wild images,
(2) recovering human meshes in 3D scenes from egocentric views, (3)
scene-constrained motion synthesis, and (4) resolving self-intersections. Our
results highlight its broad applicability and significant performance and
efficiency gains.

</details>


### [422] [Utilizing a Novel Deep Learning Method for Scene Categorization in Remote Sensing Data](https://arxiv.org/abs/2506.22939)
*Ghufran A. Omran,Wassan Saad Abduljabbar Hayale,Ahmad AbdulQadir AlRababah,Israa Ibraheem Al-Barazanchi,Ravi Sekhar,Pritesh Shah,Sushma Parihar,Harshavardhan Reddy Penubadi*

Main category: cs.CV

TL;DR: 文章提出CO - BRNN用于遥感数据场景分类，与多种现有技术对比，结果显示CO - BRNN准确率最高，强调物理验证对卫星数据效率的重要性。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型在利用远程观测数据进行场景分类时难以达到高精度，需大数据库且对噪声敏感。

Method: 引入Cuttlefish Optimized Bidirectional Recurrent Neural Network (CO - BRNN) 进行场景分类，并与MLP - CNN、CNN - LSTM等多种现有技术对比。

Result: CO - BRNN准确率达97%最高，LSTM - CRF为90%，MLP - CNN为85%，CNN - LSTM为80%。

Conclusion: 强调物理验证对确保卫星数据效率的重要性。

Abstract: Scene categorization (SC) in remotely acquired images is an important subject
with broad consequences in different fields, including catastrophe control,
ecological observation, architecture for cities, and more. Nevertheless, its
several apps, reaching a high degree of accuracy in SC from distant observation
data has demonstrated to be difficult. This is because traditional conventional
deep learning models require large databases with high variety and high levels
of noise to capture important visual features. To address these problems, this
investigation file introduces an innovative technique referred to as the
Cuttlefish Optimized Bidirectional Recurrent Neural Network (CO- BRNN) for type
of scenes in remote sensing data. The investigation compares the execution of
CO-BRNN with current techniques, including Multilayer Perceptron- Convolutional
Neural Network (MLP-CNN), Convolutional Neural Network-Long Short Term Memory
(CNN-LSTM), and Long Short Term Memory-Conditional Random Field (LSTM-CRF),
Graph-Based (GB), Multilabel Image Retrieval Model (MIRM-CF), Convolutional
Neural Networks Data Augmentation (CNN-DA). The results demonstrate that
CO-BRNN attained the maximum accuracy of 97%, followed by LSTM-CRF with 90%,
MLP-CNN with 85%, and CNN-LSTM with 80%. The study highlights the significance
of physical confirmation to ensure the efficiency of satellite data.

</details>


### [423] [Aggregating Local Saliency Maps for Semi-Global Explainable Image Classification](https://arxiv.org/abs/2506.23247)
*James Hinns,David Martens*

Main category: cs.CV

TL;DR: 提出Segment Attribution Tables (SATs) 方法，能将局部显著性解释总结为半全局见解，用于分析和调试图像分类器。


<details>
  <summary>Details</summary>
Motivation: 现有局部解释难识别重复模式，全局方法易简化且遗漏重要局部行为，需一种新方法解决该问题。

Method: 提出SATs方法，利用图像分割和显著性图量化图像片段影响。

Result: SATs能突出模型跨实例依赖的概念，揭示虚假相关性，且不影响分布外测试性能。

Conclusion: SATs弥补了全局总结和局部解释的不足，是分析和调试图像分类器的实用工具。

Abstract: Deep learning dominates image classification tasks, yet understanding how
models arrive at predictions remains a challenge. Much research focuses on
local explanations of individual predictions, such as saliency maps, which
visualise the influence of specific pixels on a model's prediction. However,
reviewing many of these explanations to identify recurring patterns is
infeasible, while global methods often oversimplify and miss important local
behaviours. To address this, we propose Segment Attribution Tables (SATs), a
method for summarising local saliency explanations into (semi-)global insights.
SATs take image segments (such as "eyes" in Chihuahuas) and leverage saliency
maps to quantify their influence. These segments highlight concepts the model
relies on across instances and reveal spurious correlations, such as reliance
on backgrounds or watermarks, even when out-of-distribution test performance
sees little change. SATs can explain any classifier for which a form of
saliency map can be produced, using segmentation maps that provide named
segments. SATs bridge the gap between oversimplified global summaries and
overly detailed local explanations, offering a practical tool for analysing and
debugging image classifiers.

</details>


### [424] [PixelBoost: Leveraging Brownian Motion for Realistic-Image Super-Resolution](https://arxiv.org/abs/2506.23254)
*Aradhana Mishra,Bumshik Lee*

Main category: cs.CV

TL;DR: 提出名为PixelBoost的扩散模型解决图像超分辨率技术在真实图像生成和计算效率间的权衡问题，该模型效果好且推理速度快。


<details>
  <summary>Details</summary>
Motivation: 解决基于扩散模型的图像超分辨率技术在真实图像生成和计算效率间的权衡问题，减少减少采样步骤导致图像不真实模糊的问题。

Method: 引入PixelBoost模型，在训练中融入可控随机性，采用sigmoidal噪声排序方法。

Result: 在LPIPS、LOE、PSNR、SSIM等客观指标和视觉质量上表现出色，边缘重建能力更好，能适应布朗噪声模式，推理速度更快。

Conclusion: PixelBoost模型在图像超分辨率上有较好效果，能兼顾真实感和计算效率。

Abstract: Diffusion-model-based image super-resolution techniques often face a
trade-off between realistic image generation and computational efficiency. This
issue is exacerbated when inference times by decreasing sampling steps,
resulting in less realistic and hazy images. To overcome this challenge, we
introduce a novel diffusion model named PixelBoost that underscores the
significance of embracing the stochastic nature of Brownian motion in advancing
image super-resolution, resulting in a high degree of realism, particularly
focusing on texture and edge definitions. By integrating controlled
stochasticity into the training regimen, our proposed model avoids convergence
to local optima, effectively capturing and reproducing the inherent uncertainty
of image textures and patterns. Our proposed model demonstrates superior
objective results in terms of learned perceptual image patch similarity
(LPIPS), lightness order error (LOE), peak signal-to-noise ratio(PSNR),
structural similarity index measure (SSIM), as well as visual quality. To
determine the edge enhancement, we evaluated the gradient magnitude and pixel
value, and our proposed model exhibited a better edge reconstruction
capability. Additionally, our model demonstrates adaptive learning capabilities
by effectively adjusting to Brownian noise patterns and introduces a sigmoidal
noise sequencing method that simplifies training, resulting in faster inference
speeds.

</details>


### [425] [ActAlign: Zero-Shot Fine-Grained Video Classification via Language-Guided Sequence Alignment](https://arxiv.org/abs/2506.22967)
*Amir Aghdam,Vincent Tao Hu*

Main category: cs.CV

TL;DR: 提出ActAlign零样本框架解决零样本细粒度视频分类问题，在ActionAtlas基准测试表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有对比视觉 - 语言模型难以捕捉区分细粒度活动的关键时间结构，需解决零样本细粒度视频分类任务。

Method: 将视频分类表述为序列对齐，用大语言模型为每个类别生成有序子动作序列，在共享嵌入空间使用动态时间规整（DTW）与视频帧对齐。

Result: 在ActionAtlas基准测试中达到30.5%准确率，优于数十亿参数的视频 - 语言模型，且参数约少8倍。

Conclusion: 结构化语言先验与经典对齐技术结合，为释放视觉 - 语言模型在细粒度视频理解方面的开放集识别潜力提供了可扩展和通用的方法。

Abstract: We address the task of zero-shot fine-grained video classification, where no
video examples or temporal annotations are available for unseen action classes.
While contrastive vision-language models such as SigLIP demonstrate strong
open-set recognition via mean-pooled image-text similarity, they fail to
capture the temporal structure critical for distinguishing fine-grained
activities. We introduce ActAlign, a zero-shot framework that formulates video
classification as sequence alignment. For each class, a large language model
generates an ordered sub-action sequence, which is aligned with video frames
using Dynamic Time Warping (DTW) in a shared embedding space. Without any
video-text supervision or fine-tuning, ActAlign achieves 30.5% accuracy on the
extremely challenging ActionAtlas benchmark, where human accuracy is only
61.6%. ActAlign outperforms billion-parameter video-language models while using
approximately 8x less parameters. These results demonstrate that structured
language priors, combined with classical alignment techniques, offer a scalable
and general approach to unlocking the open-set recognition potential of
vision-language models for fine-grained video understanding.

</details>


### [426] [Token Activation Map to Visually Explain Multimodal LLMs](https://arxiv.org/abs/2506.23270)
*Yi Li,Hualiang Wang,Xinpeng Ding,Haonan Wang,Xiaomeng Li*

Main category: cs.CV

TL;DR: 本文提出Token Activation Map (TAM)方法解决多模态大语言模型解释性问题，效果优于现有方法，代码开源。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型解释性研究不足，早期上下文标记的冗余激活会干扰后续标记解释，影响解释可靠性。

Method: 提出估计因果推理方法减轻上下文干扰，用新颖的秩高斯滤波器减少激活噪声，即TAM方法。

Result: TAM方法显著优于现有最优方法，有高质量可视化结果，可用于多种场景。

Conclusion: TAM方法能有效解决多模态大语言模型解释性问题，在多方面表现出色。

Abstract: Multimodal large language models (MLLMs) are broadly empowering various
fields. Despite their advancements, the explainability of MLLMs remains less
explored, hindering deeper understanding, model credibility, and effective
visualization. Unlike conventional vision models (e.g., CNNs, ViTs, CLIP) that
produce a single output, MLLMs generate sequences of tokens progressively,
where each generated token depends on the previous context. Therefore, earlier
context tokens can introduce redundant activations that interfere with the
explanation of later tokens beyond their original information. Existing studies
often overlook this issue, but our observations reveal that these redundant
correlations can significantly hurt the reliability of explanations. To address
this, we propose an estimated causal inference method to mitigate the
interference of context to achieve high-quality MLLM explanation, with a novel
rank Gaussian filter to further reduce activation noises. We term this method
Token Activation Map (TAM) to highlight the consideration of interactions
between tokens. TAM also indicates that it excels at explaining multiple tokens
of MLLM, which is different from the Class Activation Map (CAM) for a single
prediction. Our TAM method significantly outperforms existing SoTA methods,
showcasing high-quality visualization results that can be utilized for various
scenarios, such as object localization, failure case analysis, video
visualization, MLLMs visual comparison, and model understanding (e.g., color,
shape, action, location, visual reasoning, multi-turn conversation, etc). The
code is available atgithub.com/xmed-lab/TAM.

</details>


### [427] [Why Settle for One? Text-to-ImageSet Generation and Evaluation](https://arxiv.org/abs/2506.23275)
*Chengyou Jia,Xin Shen,Zhuohang Dang,Zhuohang Dang,Changliang Xia,Weijia Wu,Xinyu Zhang,Hangwei Qian,Ivor W. Tsang,Minnan Luo*

Main category: cs.CV

TL;DR: 提出文本到图像集生成问题，引入T2IS - Bench和T2IS - Eval，提出无训练框架AutoT2IS，实验显示其优于现有方法且有实用价值。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像一致性方法通用性差，真实应用需生成满足多样一致性要求的图像集。

Method: 引入T2IS - Bench提供综合覆盖；提出T2IS - Eval评估框架；提出无训练框架AutoT2IS利用预训练扩散变压器上下文能力。

Result: 实验表明现有方法难以应对多样一致性挑战，AutoT2IS显著优于现有通用和专门方法。

Conclusion: AutoT2IS能推动未充分探索的现实应用，有重大实用价值。

Abstract: Despite remarkable progress in Text-to-Image models, many real-world
applications require generating coherent image sets with diverse consistency
requirements. Existing consistent methods often focus on a specific domain with
specific aspects of consistency, which significantly constrains their
generalizability to broader applications. In this paper, we propose a more
challenging problem, Text-to-ImageSet (T2IS) generation, which aims to generate
sets of images that meet various consistency requirements based on user
instructions. To systematically study this problem, we first introduce
$\textbf{T2IS-Bench}$ with 596 diverse instructions across 26 subcategories,
providing comprehensive coverage for T2IS generation. Building on this, we
propose $\textbf{T2IS-Eval}$, an evaluation framework that transforms user
instructions into multifaceted assessment criteria and employs effective
evaluators to adaptively assess consistency fulfillment between criteria and
generated sets. Subsequently, we propose $\textbf{AutoT2IS}$, a training-free
framework that maximally leverages pretrained Diffusion Transformers'
in-context capabilities to harmonize visual elements to satisfy both
image-level prompt alignment and set-level visual consistency. Extensive
experiments on T2IS-Bench reveal that diverse consistency challenges all
existing methods, while our AutoT2IS significantly outperforms current
generalized and even specialized approaches. Our method also demonstrates the
ability to enable numerous underexplored real-world applications, confirming
its substantial practical value. Visit our project in
https://chengyou-jia.github.io/T2IS-Home.

</details>


### [428] [Learning Counterfactually Decoupled Attention for Open-World Model Attribution](https://arxiv.org/abs/2506.23074)
*Yu Zheng,Boyang Gong,Fanye Kong,Yueqi Duan,Bingyao Yu,Wenzhao Zheng,Lei Chen,Jiwen Lu,Jie Zhou*

Main category: cs.CV

TL;DR: 提出CDAL方法用于开放世界模型归因，实验显示其能以最小计算开销大幅提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖手工设计，易受虚假统计关联干扰，难以应对开放世界新攻击。

Method: CDAL明确建模注意力视觉痕迹与源模型归因的因果关系，反事实地分离判别性模型特定工件与混淆源偏差进行比较。

Result: 在现有开放世界模型归因基准上的大量实验表明，该方法以最小计算开销大幅提升了现有模型性能，尤其针对未见新攻击。

Conclusion: CDAL方法能有效解决开放世界模型归因问题，提升模型泛化能力。

Abstract: In this paper, we propose a Counterfactually Decoupled Attention Learning
(CDAL) method for open-world model attribution. Existing methods rely on
handcrafted design of region partitioning or feature space, which could be
confounded by the spurious statistical correlations and struggle with novel
attacks in open-world scenarios. To address this, CDAL explicitly models the
causal relationships between the attentional visual traces and source model
attribution, and counterfactually decouples the discriminative model-specific
artifacts from confounding source biases for comparison. In this way, the
resulting causal effect provides a quantification on the quality of learned
attention maps, thus encouraging the network to capture essential generation
patterns that generalize to unseen source models by maximizing the effect.
Extensive experiments on existing open-world model attribution benchmarks show
that with minimal computational overhead, our method consistently improves
state-of-the-art models by large margins, particularly for unseen novel
attacks. Source code: https://github.com/yzheng97/CDAL.

</details>


### [429] [SIEDD: Shared-Implicit Encoder with Discrete Decoders](https://arxiv.org/abs/2506.23382)
*Vikram Rangarajan,Shishira Maiya,Max Ehrlich,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: 提出SIEDD架构加速INR视频编码，无质量和控制牺牲，有显著加速效果和良好性能。


<details>
  <summary>Details</summary>
Motivation: 现有加速INR编码方法存在牺牲重建质量或失去坐标级控制问题，需新方法解决。

Method: 先在稀疏锚帧上训练共享坐标编码器捕捉全局低频特征，再冻结编码器并行训练离散解码器，通过积极坐标空间采样加速。

Result: 在HD和4K基准上比现有INR编解码器编码速度提升20 - 30倍，保持竞争力的重建质量和压缩比，保留坐标级控制。

Conclusion: 显著提升高保真神经视频压缩实用性，为实际部署提供可扩展高效途径。

Abstract: Implicit Neural Representations (INRs) offer exceptional fidelity for video
compression by learning per-video optimized functions, but their adoption is
crippled by impractically slow encoding times. Existing attempts to accelerate
INR encoding often sacrifice reconstruction quality or crucial coordinate-level
control essential for adaptive streaming and transcoding. We introduce SIEDD
(Shared-Implicit Encoder with Discrete Decoders), a novel architecture that
fundamentally accelerates INR encoding without these compromises. SIEDD first
rapidly trains a shared, coordinate-based encoder on sparse anchor frames to
efficiently capture global, low-frequency video features. This encoder is then
frozen, enabling massively parallel training of lightweight, discrete decoders
for individual frame groups, further expedited by aggressive coordinate-space
sampling. This synergistic design delivers a remarkable 20-30X encoding
speed-up over state-of-the-art INR codecs on HD and 4K benchmarks, while
maintaining competitive reconstruction quality and compression ratios.
Critically, SIEDD retains full coordinate-based control, enabling continuous
resolution decoding and eliminating costly transcoding. Our approach
significantly advances the practicality of high-fidelity neural video
compression, demonstrating a scalable and efficient path towards real-world
deployment. Our codebase is available at
https://github.com/VikramRangarajan/SIEDD .

</details>


### [430] [Time-variant Image Inpainting via Interactive Distribution Transition Estimation](https://arxiv.org/abs/2506.23461)
*Yun Xing,Qing Guo,Xiaoguang Li,Yihao Huang,Xiaofeng Cao,Di Lin,Ivor Tsang,Lei Ma*

Main category: cs.CV

TL;DR: 提出TAMP任务，提出InDiTE模块和InDiTE - Diff解决方案，组装TAMP - Street数据集，实验表明方法优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统参考引导图像修复方法在处理有显著时间差的图像时无法取得好结果，需要解决该问题。

Method: 提出InDiTE模块自适应语义互补，提出InDiTE - Diff将InDiTE与SOTA扩散模型结合并进行潜在交叉参考，组装TAMP - Street数据集。

Result: 在TAMP - Street数据集上的实验显示，所提方法在两种不同时间变化图像修复设置下始终优于SOTA参考引导图像修复方法。

Conclusion: 所提方法能有效解决TAMP任务，优于现有SOTA参考引导图像修复方法。

Abstract: In this work, we focus on a novel and practical task, i.e., Time-vAriant
iMage inPainting (TAMP). The aim of TAMP is to restore a damaged target image
by leveraging the complementary information from a reference image, where both
images captured the same scene but with a significant time gap in between,
i.e., time-variant images. Different from conventional reference-guided image
inpainting, the reference image under TAMP setup presents significant content
distinction to the target image and potentially also suffers from damages. Such
an application frequently happens in our daily lives to restore a damaged image
by referring to another reference image, where there is no guarantee of the
reference image's source and quality. In particular, our study finds that even
state-of-the-art (SOTA) reference-guided image inpainting methods fail to
achieve plausible results due to the chaotic image complementation. To address
such an ill-posed problem, we propose a novel Interactive Distribution
Transition Estimation (InDiTE) module which interactively complements the
time-variant images with adaptive semantics thus facilitate the restoration of
damaged regions. To further boost the performance, we propose our TAMP
solution, namely Interactive Distribution Transition Estimation-driven
Diffusion (InDiTE-Diff), which integrates InDiTE with SOTA diffusion model and
conducts latent cross-reference during sampling. Moreover, considering the lack
of benchmarks for TAMP task, we newly assembled a dataset, i.e., TAMP-Street,
based on existing image and mask datasets. We conduct experiments on the
TAMP-Street datasets under two different time-variant image inpainting
settings, which show our method consistently outperform SOTA reference-guided
image inpainting methods for solving TAMP.

</details>


### [431] [Detecting What Matters: A Novel Approach for Out-of-Distribution 3D Object Detection in Autonomous Vehicles](https://arxiv.org/abs/2506.23426)
*Menna Taha,Aya Ahmed,Mohammed Karmoose,Yasser Gadallah*

Main category: cs.CV

TL;DR: 提出一种新的目标检测方法，将重点从传统基于类别的分类转移到目标危害性判定，以增强自动驾驶汽车在动态环境中的决策能力。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测方法限制自动驾驶汽车检测和响应分布外（OOD）目标的能力，存在安全隐患。

Method: 提出新方法，根据目标相对于自动驾驶汽车的位置和轨迹，将目标识别为“有害”或“无害”。

Result: 所提模型能有效检测OOD目标，评估其危害性并进行相应分类。

Conclusion: 所提模型增强了自动驾驶汽车在动态环境中的决策有效性。

Abstract: Autonomous vehicles (AVs) use object detection models to recognize their
surroundings and make driving decisions accordingly. Conventional object
detection approaches classify objects into known classes, which limits the AV's
ability to detect and appropriately respond to Out-of-Distribution (OOD)
objects. This problem is a significant safety concern since the AV may fail to
detect objects or misclassify them, which can potentially lead to hazardous
situations such as accidents. Consequently, we propose a novel object detection
approach that shifts the emphasis from conventional class-based classification
to object harmfulness determination. Instead of object detection by their
specific class, our method identifies them as either 'harmful' or 'harmless'
based on whether they pose a danger to the AV. This is done based on the object
position relative to the AV and its trajectory. With this metric, our model can
effectively detect previously unseen objects to enable the AV to make safer
real-time decisions. Our results demonstrate that the proposed model
effectively detects OOD objects, evaluates their harmfulness, and classifies
them accordingly, thus enhancing the AV decision-making effectiveness in
dynamic environments.

</details>


### [432] [Sanitizing Manufacturing Dataset Labels Using Vision-Language Models](https://arxiv.org/abs/2506.23465)
*Nazanin Mahjourian,Vinh Nguyen*

Main category: cs.CV

TL;DR: 论文提出VLSR框架处理多标签制造图像数据集的标签问题，实验表明其能提升标签质量。


<details>
  <summary>Details</summary>
Motivation: 大规模数据集常存在标签噪声等问题，制造领域获取高质量标签成本高、耗时长，需解决标签问题。

Method: 利用CLIP模型将图像和文本标签嵌入共享语义空间，通过计算嵌入向量的余弦相似度进行标签清理和聚类。

Result: VLSR框架成功识别有问题的标签，提高了标签一致性，通过聚类显著减少标签词汇量。

Conclusion: 该方法能以最少的人工干预提升数据集质量，利于训练工业应用中的鲁棒机器学习模型。

Abstract: The success of machine learning models in industrial applications is heavily
dependent on the quality of the datasets used to train the models. However,
large-scale datasets, specially those constructed from crowd-sourcing and
web-scraping, often suffer from label noise, inconsistencies, and errors. This
problem is particularly pronounced in manufacturing domains, where obtaining
high-quality labels is costly and time-consuming. This paper introduces
Vision-Language Sanitization and Refinement (VLSR), which is a
vision-language-based framework for label sanitization and refinement in
multi-label manufacturing image datasets. This method embeds both images and
their associated textual labels into a shared semantic space leveraging the
CLIP vision-language model. Then two key tasks are addressed in this process by
computing the cosine similarity between embeddings. First, label sanitization
is performed to identify irrelevant, misspelled, or semantically weak labels,
and surface the most semantically aligned label for each image by comparing
image-label pairs using cosine similarity between image and label embeddings.
Second, the method applies density-based clustering on text embeddings,
followed by iterative cluster merging, to group semantically similar labels
into unified label groups. The Factorynet dataset, which includes noisy labels
from both human annotations and web-scraped sources, is employed to evaluate
the effectiveness of the proposed framework. Experimental results demonstrate
that the VLSR framework successfully identifies problematic labels and improves
label consistency. This method enables a significant reduction in label
vocabulary through clustering, which ultimately enhances the dataset's quality
for training robust machine learning models in industrial applications with
minimal human intervention.

</details>


### [433] [Qwen-GUI-3B: A Lightweight Vision-Language Model for Cross-Resolution GUI Grounding](https://arxiv.org/abs/2506.23491)
*ZongHan Hsieh,Tzer-Jen Wei*

Main category: cs.CV

TL;DR: 介绍轻量级视觉语言模型Qwen - GUI - 3B，用于GUI接地任务，性能强且可单GPU训练，有多项创新，评估表现佳。


<details>
  <summary>Details</summary>
Motivation: 现有大规模VLM计算量大，不适用于消费级硬件，解决高分辨率桌面环境数据稀缺问题。

Method: 结合跨平台、多分辨率数据集；采用两阶段微调策略；进行数据整理和减少冗余。

Result: 在标准GUI接地基准测试中准确率高，超过4B参数以下的先前模型。消融研究验证关键策略作用。

Conclusion: Qwen - GUI - 3B在GUI接地任务表现出色，强调数据多样性重要性，代码开源。

Abstract: This paper introduces Qwen-GUI-3B, a lightweight Vision-Language Model (VLM)
specifically designed for Graphical User Interface grounding tasks, achieving
performance competitive with significantly larger models. Unlike large-scale
VLMs (>7B parameters) that are computationally intensive and impractical for
consumer-grade hardware, Qwen-GUI-3B delivers strong grounding accuracy while
being fully trainable on a single GPU (RTX 4090). The model incorporates
several key innovations: (i) combine cross-platform, multi-resolution dataset
of 24K examples from diverse sources including mobile, desktop, and web GUI
screenshots to effectively address data scarcity in high-resolution desktop
environments; (ii) a two-stage fine-tuning strategy, where initial
cross-platform training establishes robust GUI understanding, followed by
specialized fine-tuning on high-resolution data to significantly enhance model
adaptability; and (iii) data curation and redundancy reduction strategies,
demonstrating that randomly sampling a smaller subset with reduced redundancy
achieves performance comparable to larger datasets, emphasizing data diversity
over sheer volume. Empirical evaluation on standard GUI grounding
benchmarks-including ScreenSpot, ScreenSpot-v2, and the challenging
ScreenSpot-Pro, highlights Qwen-GUI-3B's exceptional accuracy, achieving 84.9%
on ScreenSpot and 86.4% on ScreenSpot-v2, surpassing prior models under 4B
parameters. Ablation studies validate the critical role of balanced sampling
and two-stage fine-tuning in enhancing robustness, particularly in
high-resolution desktop scenarios. The Qwen-GUI-3B is available at:
https://github.com/Han1018/Qwen-GUI-3B

</details>


### [434] [AdFair-CLIP: Adversarial Fair Contrastive Language-Image Pre-training for Chest X-rays](https://arxiv.org/abs/2506.23467)
*Chenlang Yi,Zizhan Xiong,Qi Qi,Xiyuan Wei,Girish Bathla,Ching-Long Lin,Bobak Jack Mortazavi,Tianbao Yang*

Main category: cs.CV

TL;DR: 现有CLIP模型在医学图像分类中公平性受关注少，本文提出AdFair - CLIP框架，实验显示其能提升公平性和诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP模型在公平性方面受关注少，存在人口统计学偏差，导致诊断结果差异和弱势群体可靠性降低。

Method: 引入AdFair - CLIP框架，采用对抗特征干预来抑制敏感属性，减少虚假关联。

Result: 在胸部X光数据集上实验表明，AdFair - CLIP显著提升公平性和诊断准确性，在零样本和少样本场景中保持鲁棒泛化性。

Conclusion: 为基于CLIP的医学诊断模型的公平性学习建立了新基准，尤其适用于胸部X光分析。

Abstract: Contrastive Language-Image Pre-training (CLIP) models have demonstrated
superior performance across various visual tasks including medical image
classification. However, fairness concerns, including demographic biases, have
received limited attention for CLIP models. This oversight leads to critical
issues, particularly those related to race and gender, resulting in disparities
in diagnostic outcomes and reduced reliability for underrepresented groups. To
address these challenges, we introduce AdFair-CLIP, a novel framework employing
adversarial feature intervention to suppress sensitive attributes, thereby
mitigating spurious correlations and improving prediction fairness. We conduct
comprehensive experiments on chest X-ray (CXR) datasets, and show that
AdFair-CLIP significantly enhances both fairness and diagnostic accuracy, while
maintaining robust generalization in zero-shot and few-shot scenarios. These
results establish new benchmarks for fairness-aware learning in CLIP-based
medical diagnostic models, particularly for CXR analysis.

</details>


### [435] [When Test-Time Adaptation Meets Self-Supervised Models](https://arxiv.org/abs/2506.23529)
*Jisu Han,Jihee Park,Dongyoon Han,Wonjun Hwang*

Main category: cs.CV

TL;DR: 本文探讨无源预训练下自监督学习模型的测试时自适应方法，提出自监督TTA协议和协作学习框架，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有在线自适应依赖源预训练模型，且现有TTA方法用于源域准确率低的自监督模型时效果不佳，需研究无依赖源预训练的TTA方法。

Method: 引入自监督TTA协议，提出集成SSL和TTA模型的协作学习框架，利用对比学习和知识蒸馏逐步细化表征。

Result: 在多种自监督模型和TTA基准上验证了方法有效性，即使无预训练也有有竞争力的表现。

Conclusion: 所提方法在自监督学习中有效，能在不依赖源预训练的情况下持续改进模型。

Abstract: Training on test-time data enables deep learning models to adapt to dynamic
environmental changes, enhancing their practical applicability. Online
adaptation from source to target domains is promising but it remains highly
reliant on the performance of source pretrained model. In this paper, we
investigate whether test-time adaptation (TTA) methods can continuously improve
models trained via self-supervised learning (SSL) without relying on source
pretraining. We introduce a self-supervised TTA protocol after observing that
existing TTA approaches struggle when directly applied to self-supervised
models with low accuracy on the source domain. Furthermore, we propose a
collaborative learning framework that integrates SSL and TTA models, leveraging
contrastive learning and knowledge distillation for stepwise representation
refinement. We validate our method on diverse self-supervised models, including
DINO, MoCo, and iBOT, across TTA benchmarks. Extensive experiments validate the
effectiveness of our approach in SSL, showing that it achieves competitive
performance even without source pretraining.

</details>


### [436] [GViT: Representing Images as Gaussians for Visual Recognition](https://arxiv.org/abs/2506.23532)
*Jefferson Hernandez,Ruozhen He,Guha Balakrishnan,Alexander C. Berg,Vicente Ordonez*

Main category: cs.CV

TL;DR: 介绍GVIT分类框架，用可学习2D高斯集代替传统输入表示，达到较好性能。


<details>
  <summary>Details</summary>
Motivation: 摒弃传统像素或补丁网格输入表示，探索新的图像表示方法以用于图像分类。

Method: 将图像编码为可学习的2D高斯集，与ViT分类器联合优化，利用分类器梯度引导高斯分布，同时用可微渲染器优化图像重建损失。

Result: 使用相对标准的ViT架构，在Imagenet - 1k上用ViT - B架构达到76.9%的top - 1准确率。

Conclusion: 2D高斯输入表示结合GVIT引导，能使标准ViT架构接近传统基于补丁的ViT性能。

Abstract: We introduce GVIT, a classification framework that abandons conventional
pixel or patch grid input representations in favor of a compact set of
learnable 2D Gaussians. Each image is encoded as a few hundred Gaussians whose
positions, scales, orientations, colors, and opacities are optimized jointly
with a ViT classifier trained on top of these representations. We reuse the
classifier gradients as constructive guidance, steering the Gaussians toward
class-salient regions while a differentiable renderer optimizes an image
reconstruction loss. We demonstrate that by 2D Gaussian input representations
coupled with our GVIT guidance, using a relatively standard ViT architecture,
closely matches the performance of a traditional patch-based ViT, reaching a
76.9% top-1 accuracy on Imagenet-1k using a ViT-B architecture.

</details>


### [437] [Uncertainty-aware Diffusion and Reinforcement Learning for Joint Plane Localization and Anomaly Diagnosis in 3D Ultrasound](https://arxiv.org/abs/2506.23538)
*Yuhao Huang,Yueyue Xu,Haoran Dou,Jiaxiao Deng,Xin Yang,Hongyu Zheng,Dong Ni*

Main category: cs.CV

TL;DR: 提出用于3D子宫超声的先天性子宫异常（CUA）自动平面定位与诊断的智能系统，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 先天性子宫异常会导致多种不良后果，3D超声相比2D超声在评估CUA上更有优势，需智能系统实现自动平面定位与CUA诊断。

Method: 开发带局部和全局引导的去噪扩散模型，采用自适应加权策略；引入基于强化学习且有无监督奖励的框架提取关键切片总结；提供文本驱动的不确定性建模用于粗预测并调整分类概率。

Result: 在大型3D子宫超声数据集上的实验表明该方法在平面定位和CUA诊断方面有效。

Conclusion: 所提出的智能系统可有效用于3D子宫超声的平面定位和CUA诊断。

Abstract: Congenital uterine anomalies (CUAs) can lead to infertility, miscarriage,
preterm birth, and an increased risk of pregnancy complications. Compared to
traditional 2D ultrasound (US), 3D US can reconstruct the coronal plane,
providing a clear visualization of the uterine morphology for assessing CUAs
accurately. In this paper, we propose an intelligent system for simultaneous
automated plane localization and CUA diagnosis. Our highlights are: 1) we
develop a denoising diffusion model with local (plane) and global (volume/text)
guidance, using an adaptive weighting strategy to optimize attention allocation
to different conditions; 2) we introduce a reinforcement learning-based
framework with unsupervised rewards to extract the key slice summary from
redundant sequences, fully integrating information across multiple planes to
reduce learning difficulty; 3) we provide text-driven uncertainty modeling for
coarse prediction, and leverage it to adjust the classification probability for
overall performance improvement. Extensive experiments on a large 3D uterine US
dataset show the efficacy of our method, in terms of plane localization and CUA
diagnosis. Code is available at https://github.com/yuhoo0302/CUA-US.

</details>


### [438] [Metadata, Wavelet, and Time Aware Diffusion Models for Satellite Image Super Resolution](https://arxiv.org/abs/2506.23566)
*Luigi Sigillo,Renato Giamba,Danilo Comminiello*

Main category: cs.CV

TL;DR: 提出MWT - Diff框架用于卫星图像超分辨率，结合潜扩散模型和小波变换，表现优于近期方法。


<details>
  <summary>Details</summary>
Motivation: 卫星图像获取受传感器时空限制和高成本影响，阻碍需高分辨率数据的应用，需解决此问题。

Method: 提出MWT - Diff框架，核心是MWT - Encoder，利用其生成的嵌入特征引导分层扩散动态，从低分辨率输入重建高分辨率图像。

Result: 在多个数据集上对比分析，通过FID和LPIPS等标准感知质量指标衡量，表现优于近期方法。

Conclusion: MWT - Diff框架能有效解决卫星图像超分辨率问题，可应用于需高分辨率卫星图像的场景。

Abstract: The acquisition of high-resolution satellite imagery is often constrained by
the spatial and temporal limitations of satellite sensors, as well as the high
costs associated with frequent observations. These challenges hinder
applications such as environmental monitoring, disaster response, and
agricultural management, which require fine-grained and high-resolution data.
In this paper, we propose MWT-Diff, an innovative framework for satellite image
super-resolution (SR) that combines latent diffusion models with wavelet
transforms to address these challenges. At the core of the framework is a novel
metadata-, wavelet-, and time-aware encoder (MWT-Encoder), which generates
embeddings that capture metadata attributes, multi-scale frequency information,
and temporal relationships. The embedded feature representations steer the
hierarchical diffusion dynamics, through which the model progressively
reconstructs high-resolution satellite imagery from low-resolution inputs. This
process preserves critical spatial characteristics including textural patterns,
boundary discontinuities, and high-frequency spectral components essential for
detailed remote sensing analysis. The comparative analysis of MWT-Diff across
multiple datasets demonstrated favorable performance compared to recent
approaches, as measured by standard perceptual quality metrics including FID
and LPIPS.

</details>


### [439] [PBCAT: Patch-based composite adversarial training against physically realizable attacks on object detection](https://arxiv.org/abs/2506.23581)
*Xiao Li,Yiming Zhu,Yifan Huang,Wei Zhang,Yingzhe He,Jie Shi,Xiaolin Hu*

Main category: cs.CV

TL;DR: 本文提出PBCAT方法抵御物体检测中的物理可实现攻击，实验表明其能显著提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 物体检测易受物理可实现攻击，对抗训练在物体检测抵御此类攻击方面研究有限，早期尝试仅针对对抗性补丁。

Method: 提出基于补丁的复合对抗训练策略PBCAT，结合小区域梯度引导的对抗性补丁和覆盖全图的不可察觉全局对抗性扰动优化模型。

Result: 在多种设置下的大量实验表明，PBCAT相比现有防御方法显著提升了对各种物理可实现攻击的鲁棒性，在一种对抗纹理攻击下检测准确率提高29.7%。

Conclusion: PBCAT是一种有效的统一对抗训练方法，可抵御多种物理可实现攻击。

Abstract: Object detection plays a crucial role in many security-sensitive
applications. However, several recent studies have shown that object detectors
can be easily fooled by physically realizable attacks, \eg, adversarial patches
and recent adversarial textures, which pose realistic and urgent threats.
Adversarial Training (AT) has been recognized as the most effective defense
against adversarial attacks. While AT has been extensively studied in the
$l_\infty$ attack settings on classification models, AT against physically
realizable attacks on object detectors has received limited exploration. Early
attempts are only performed to defend against adversarial patches, leaving AT
against a wider range of physically realizable attacks under-explored. In this
work, we consider defending against various physically realizable attacks with
a unified AT method. We propose PBCAT, a novel Patch-Based Composite
Adversarial Training strategy. PBCAT optimizes the model by incorporating the
combination of small-area gradient-guided adversarial patches and imperceptible
global adversarial perturbations covering the entire image. With these designs,
PBCAT has the potential to defend against not only adversarial patches but also
unseen physically realizable attacks such as adversarial textures. Extensive
experiments in multiple settings demonstrated that PBCAT significantly improved
robustness against various physically realizable attacks over state-of-the-art
defense methods. Notably, it improved the detection accuracy by 29.7\% over
previous defense methods under one recent adversarial texture attack.

</details>


### [440] [Brain Tumor Detection through Thermal Imaging and MobileNET](https://arxiv.org/abs/2506.23627)
*Roham Maiti,Debasmita Bhoumik*

Main category: cs.CV

TL;DR: 研究用 MobileNET 模型高效检测脑肿瘤，新方法准确率达 98.5%。


<details>
  <summary>Details</summary>
Motivation: 传统脑肿瘤检测方法成本高、需专业知识，经典机器学习模型有计算要求高、需大量数据和训练时间长等局限。

Method: 使用 MobileNET 模型，结合图像处理技术。

Result: 所提方法平均准确率达 98.5%。

Conclusion: 可构建准确且节省计算资源、运行时间短的肿瘤检测模型。

Abstract: Brain plays a crucial role in regulating body functions and cognitive
processes, with brain tumors posing significant risks to human health. Precise
and prompt detection is a key factor in proper treatment and better patient
outcomes. Traditional methods for detecting brain tumors, that include
biopsies, MRI, and CT scans often face challenges due to their high costs and
the need for specialized medical expertise. Recent developments in machine
learning (ML) and deep learning (DL) has exhibited strong capabilities in
automating the identification and categorization of brain tumors from medical
images, especially MRI scans. However, these classical ML models have
limitations, such as high computational demands, the need for large datasets,
and long training times, which hinder their accessibility and efficiency. Our
research uses MobileNET model for efficient detection of these tumors. The
novelty of this project lies in building an accurate tumor detection model
which use less computing re-sources and runs in less time followed by efficient
decision making through the use of image processing technique for accurate
results. The suggested method attained an average accuracy of 98.5%.

</details>


### [441] [On the Domain Robustness of Contrastive Vision-Language Models](https://arxiv.org/abs/2506.23663)
*Mario Koddenbrock,Rudolf Hoffmann,David Brodmann,Erik Rodner*

Main category: cs.CV

TL;DR: 介绍Deepbench框架评估视觉语言模型特定领域鲁棒性，开源以支持研究


<details>
  <summary>Details</summary>
Motivation: 现实中依赖预训练模型，但它们在特定领域转换时效果下降，缺乏特定领域鲁棒性评估方法

Method: 利用大语言模型生成特定部署领域的现实、上下文感知图像损坏，无需标记数据

Result: 评估多种架构，发现鲁棒性有显著差异

Conclusion: 需要有针对性、领域感知的评估，Deepbench开源支持相关研究

Abstract: In real-world vision-language applications, practitioners increasingly rely
on large, pretrained foundation models rather than custom-built solutions,
despite limited transparency regarding their training data and processes. While
these models achieve impressive performance on general benchmarks, their
effectiveness can decline notably under specialized domain shifts, such as
unique imaging conditions or environmental variations. In this work, we
introduce Deepbench, a framework designed to assess domain-specific robustness
of vision-language models (VLMs). Deepbench leverages a large language model
(LLM) to generate realistic, context-aware image corruptions tailored to
specific deployment domains without requiring labeled data. We evaluate a range
of contrastive vision-language architectures and architectural variants across
six real-world domains and observe substantial variability in robustness,
highlighting the need for targeted, domain-aware evaluation. Deepbench is
released as open-source software to support further research into domain-aware
robustness assessment.

</details>


### [442] [AI-Generated Lecture Slides for Improving Slide Element Detection and Retrieval](https://arxiv.org/abs/2506.23605)
*Suyash Maniyar,Vishvesh Trivedi,Ajoy Mondal,Anand Mishra,C. V. Jawahar*

Main category: cs.CV

TL;DR: 提出SynLecSlideGen合成讲座幻灯片生成管道，创建RealSlide评估基准，实验表明合成数据能补偿有限的标注幻灯片。


<details>
  <summary>Details</summary>
Motivation: 训练幻灯片元素检测和检索模型依赖大量手动标注，人工标注工作量大且需专业知识。

Method: 提出SynLecSlideGen生成高质量合成幻灯片，创建RealSlide评估基准，进行少样本迁移学习。

Result: 在合成幻灯片上预训练进行少样本迁移学习，比仅在真实数据上训练性能显著提升。

Conclusion: 合成数据可有效补偿有限的标注讲座幻灯片。

Abstract: Lecture slide element detection and retrieval are key problems in slide
understanding. Training effective models for these tasks often depends on
extensive manual annotation. However, annotating large volumes of lecture
slides for supervised training is labor intensive and requires domain
expertise. To address this, we propose a large language model (LLM)-guided
synthetic lecture slide generation pipeline, SynLecSlideGen, which produces
high-quality, coherent and realistic slides. We also create an evaluation
benchmark, namely RealSlide by manually annotating 1,050 real lecture slides.
To assess the utility of our synthetic slides, we perform few-shot transfer
learning on real data using models pre-trained on them. Experimental results
show that few-shot transfer learning with pretraining on synthetic slides
significantly improves performance compared to training only on real data. This
demonstrates that synthetic data can effectively compensate for limited labeled
lecture slides. The code and resources of our work are publicly available on
our project website: https://synslidegen.github.io/.

</details>


### [443] [Mamba-FETrack V2: Revisiting State Space Model for Frame-Event based Visual Object Tracking](https://arxiv.org/abs/2506.23783)
*Shiao Wang,Ju Huang,Qingchuan Ma,Jinfeng Gao,Chunyi Xu,Xiao Wang,Lan Chen,Bo Jiang*

Main category: cs.CV

TL;DR: 提出基于Vision Mamba网络的高效RGB - Event目标跟踪框架Mamba - FETrack V2，实验证明其优越性能和效率，代码将开源。


<details>
  <summary>Details</summary>
Motivation: 现有多模态跟踪算法依赖高复杂度Vision Transformer架构，导致计算开销大且限制跨模态交互效果。

Method: 设计轻量级Prompt Generator生成模态特定可学习提示向量，与模态特定嵌入特征一起输入基于Vision Mamba的FEMamba骨干网络进行特征提取、跨模态交互和融合，最后将融合表示传递给跟踪头进行目标定位。

Result: 在多个RGB - Event跟踪基准上的实验表明，该跟踪框架性能优越且高效。

Conclusion: 所提出的Mamba - FETrack V2跟踪框架能有效解决现有算法问题，有良好性能和效率。

Abstract: Combining traditional RGB cameras with bio-inspired event cameras for robust
object tracking has garnered increasing attention in recent years. However,
most existing multimodal tracking algorithms depend heavily on high-complexity
Vision Transformer architectures for feature extraction and fusion across
modalities. This not only leads to substantial computational overhead but also
limits the effectiveness of cross-modal interactions. In this paper, we propose
an efficient RGB-Event object tracking framework based on the linear-complexity
Vision Mamba network, termed Mamba-FETrack V2. Specifically, we first design a
lightweight Prompt Generator that utilizes embedded features from each
modality, together with a shared prompt pool, to dynamically generate
modality-specific learnable prompt vectors. These prompts, along with the
modality-specific embedded features, are then fed into a Vision Mamba-based
FEMamba backbone, which facilitates prompt-guided feature extraction,
cross-modal interaction, and fusion in a unified manner. Finally, the fused
representations are passed to the tracking head for accurate target
localization. Extensive experimental evaluations on multiple RGB-Event tracking
benchmarks, including short-term COESOT dataset and long-term datasets, i.e.,
FE108 and FELT V2, demonstrate the superior performance and efficiency of the
proposed tracking framework. The source code and pre-trained models will be
released on https://github.com/Event-AHU/Mamba_FETrack

</details>


### [444] [Unified Multimodal Understanding via Byte-Pair Visual Encoding](https://arxiv.org/abs/2506.23639)
*Wanpeng Zhang,Yicheng Feng,Hao Luo,Yijiang Li,Zihao Yue,Sipeng Zheng,Zongqing Lu*

Main category: cs.CV

TL;DR: 本文提出统一多模态理解框架，通过对视觉标记应用字节对编码，结合优先级引导编码和多阶段训练，实验显示在多任务表现提升，促进多模态基础模型发展。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在跨模态对齐方面存在挑战，需要有效方法统一多模态理解。

Method: 对视觉标记应用字节对编码，引入考虑频率和空间一致性的优先级引导编码方案，采用基于课程驱动数据组合的多阶段训练过程。

Result: 在各种视觉 - 语言任务中表现有所提升。

Conclusion: 该方法缩小了视觉和文本表示之间的差距，有助于开发更强大、高效的多模态基础模型。

Abstract: Multimodal large language models (MLLMs) have made significant progress in
vision-language understanding, yet effectively aligning different modalities
remains a fundamental challenge. We present a framework that unifies multimodal
understanding by applying byte-pair encoding to visual tokens. Unlike
conventional approaches that rely on modality-specific encoders, our method
directly incorporates structural information into visual tokens, mirroring
successful tokenization strategies in text-only language models. We introduce a
priority-guided encoding scheme that considers both frequency and spatial
consistency, coupled with a multi-stage training procedure based on
curriculum-driven data composition. These enhancements enable the transformer
model to better capture cross-modal relationships and reason with visual
information. Comprehensive experiments demonstrate improved performance across
diverse vision-language tasks. By bridging the gap between visual and textual
representations, our approach contributes to the advancement of more capable
and efficient multimodal foundation models.

</details>


### [445] [VAP-Diffusion: Enriching Descriptions with MLLMs for Enhanced Medical Image Generation](https://arxiv.org/abs/2506.23641)
*Peng Huang,Junhu Fu,Bowen Guo,Zeju Li,Yuanyuan Wang,Yi Guo*

Main category: cs.CV

TL;DR: 提出VAP - Diffusion框架，利用MLLMs外部知识改进医学图像生成，经实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 医学图像外观受多因素影响，生成模型需丰富属性信息，但详细描述不总是可得，故需改进医学图像生成质量和多样性。

Method: 设计基于思维链的提示从MLLMs获取无幻觉描述，训练时使用并分类存储，测试时随机检索；提出原型条件机制使生成器对未见描述组合更鲁棒。

Result: 在四个数据集的三种常见医学成像类型上实验，验证了VAP - Diffusion的有效性。

Conclusion: VAP - Diffusion框架能利用MLLMs外部知识，有效提升医学图像生成质量和多样性。

Abstract: As the appearance of medical images is influenced by multiple underlying
factors, generative models require rich attribute information beyond labels to
produce realistic and diverse images. For instance, generating an image of skin
lesion with specific patterns demands descriptions that go beyond diagnosis,
such as shape, size, texture, and color. However, such detailed descriptions
are not always accessible. To address this, we explore a framework, termed
Visual Attribute Prompts (VAP)-Diffusion, to leverage external knowledge from
pre-trained Multi-modal Large Language Models (MLLMs) to improve the quality
and diversity of medical image generation. First, to derive descriptions from
MLLMs without hallucination, we design a series of prompts following
Chain-of-Thoughts for common medical imaging tasks, including dermatologic,
colorectal, and chest X-ray images. Generated descriptions are utilized during
training and stored across different categories. During testing, descriptions
are randomly retrieved from the corresponding category for inference. Moreover,
to make the generator robust to unseen combination of descriptions at the test
time, we propose a Prototype Condition Mechanism that restricts test embeddings
to be similar to those from training. Experiments on three common types of
medical imaging across four datasets verify the effectiveness of VAP-Diffusion.

</details>


### [446] [Spurious-Aware Prototype Refinement for Reliable Out-of-Distribution Detection](https://arxiv.org/abs/2506.23881)
*Reihaneh Zohrabi,Hosein Hasani,Mahdieh Soleymani Baghshah,Anna Rohrbach,Marcus Rohrbach,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: 提出原型OOD检测方法SPROD，能处理未知虚假相关性，基准测试表现优。


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测方法易受虚假相关性影响，降低模型鲁棒性。

Method: 提出SPROD方法，通过后处理优化类原型，减少虚假特征偏差，无需额外数据和超参调整。

Result: 在多个OOD数据集上进行基准测试，SPROD平均使AUROC提高4.7%，FPR@95提高9.3%。

Conclusion: SPROD方法优于现有方法，可广泛应用于不同骨干网络和OOD检测场景。

Abstract: Out-of-distribution (OOD) detection is crucial for ensuring the reliability
and safety of machine learning models in real-world applications, where they
frequently face data distributions unseen during training. Despite progress,
existing methods are often vulnerable to spurious correlations that mislead
models and compromise robustness. To address this, we propose SPROD, a novel
prototype-based OOD detection approach that explicitly addresses the challenge
posed by unknown spurious correlations. Our post-hoc method refines class
prototypes to mitigate bias from spurious features without additional data or
hyperparameter tuning, and is broadly applicable across diverse backbones and
OOD detection settings. We conduct a comprehensive spurious correlation OOD
detection benchmarking, comparing our method against existing approaches and
demonstrating its superior performance across challenging OOD datasets, such as
CelebA, Waterbirds, UrbanCars, Spurious Imagenet, and the newly introduced
Animals MetaCoCo. On average, SPROD improves AUROC by 4.7% and FPR@95 by 9.3%
over the second best.

</details>


### [447] [When Small Guides Large: Cross-Model Co-Learning for Test-Time Adaptation](https://arxiv.org/abs/2506.23724)
*Chang'an Yi,Xiaohui Deng,Guohao Chen,Yan Zhou,Qinghua Lu,Shuaicheng Niu*

Main category: cs.CV

TL;DR: 研究跨模型知识对TTA过程的影响，提出COCA框架，实验证明其能显著提升现有模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有TTA方法主要关注单模型适应，探究跨模型知识对TTA过程的影响。

Method: 提出COCA框架，包含协同适应和自我适应两种策略。

Result: COCA作为即插即用模块，显著提升不同大小模型性能，如使ViT - Base在ImageNet - C上平均适应准确率从51.7%提升到64.5%。

Conclusion: 跨模型知识在TTA过程中有积极作用，COCA框架有效可行。

Abstract: Test-time Adaptation (TTA) adapts a given model to testing domain data with
potential domain shifts through online unsupervised learning, yielding
impressive performance. However, to date, existing TTA methods primarily focus
on single-model adaptation. In this work, we investigate an intriguing
question: how does cross-model knowledge influence the TTA process? Our
findings reveal that, in TTA's unsupervised online setting, each model can
provide complementary, confident knowledge to the others, even when there are
substantial differences in model size. For instance, a smaller model like
MobileViT (10.6M parameters) can effectively guide a larger model like ViT-Base
(86.6M parameters). In light of this, we propose COCA, a Cross-Model
Co-Learning framework for TTA, which mainly consists of two main strategies. 1)
Co-adaptation adaptively integrates complementary knowledge from other models
throughout the TTA process, reducing individual model biases. 2)
Self-adaptation enhances each model's unique strengths via unsupervised
learning, enabling diverse adaptation to the target domain. Extensive
experiments show that COCA, which can also serve as a plug-and-play module,
significantly boosts existing SOTAs, on models with various sizes--including
ResNets, ViTs, and Mobile-ViTs--via cross-model co-learned TTA. For example,
with Mobile-ViT's guidance, COCA raises ViT-Base's average adaptation accuracy
on ImageNet-C from 51.7% to 64.5%. The code is publicly available at
https://github.com/ycarobot/COCA.

</details>


### [448] [GroundingDINO-US-SAM: Text-Prompted Multi-Organ Segmentation in Ultrasound with LoRA-Tuned Vision-Language Models](https://arxiv.org/abs/2506.23903)
*Hamza Rasaee,Taha Koleilat,Hassan Rivaz*

Main category: cs.CV

TL;DR: 提出基于Grounding DINO和SAM2的提示驱动视觉语言模型用于超声多器官分割，实验显示其优于现有方法，减少对特定器官标注数据依赖。


<details>
  <summary>Details</summary>
Motivation: 超声成像中准确且可泛化的目标分割因解剖变异性、成像协议多样和标注数据有限面临挑战。

Method: 提出集成Grounding DINO与SAM2的提示驱动视觉语言模型，用18个公共超声数据集，15个用于微调验证，3个用于测试。

Result: 该方法在多数可见数据集上优于现有分割方法，在未见数据集上无需额外微调也有良好表现。

Conclusion: 视觉语言模型在可扩展和鲁棒的超声图像分析中有前景，可减少对大量特定器官标注数据集的依赖。

Abstract: Accurate and generalizable object segmentation in ultrasound imaging remains
a significant challenge due to anatomical variability, diverse imaging
protocols, and limited annotated data. In this study, we propose a
prompt-driven vision-language model (VLM) that integrates Grounding DINO with
SAM2 to enable object segmentation across multiple ultrasound organs. A total
of 18 public ultrasound datasets, encompassing the breast, thyroid, liver,
prostate, kidney, and paraspinal muscle, were utilized. These datasets were
divided into 15 for fine-tuning and validation of Grounding DINO using Low Rank
Adaptation (LoRA) to the ultrasound domain, and 3 were held out entirely for
testing to evaluate performance in unseen distributions. Comprehensive
experiments demonstrate that our approach outperforms state-of-the-art
segmentation methods, including UniverSeg, MedSAM, MedCLIP-SAM, BiomedParse,
and SAMUS on most seen datasets while maintaining strong performance on unseen
datasets without additional fine-tuning. These results underscore the promise
of VLMs in scalable and robust ultrasound image analysis, reducing dependence
on large, organ-specific annotated datasets. We will publish our code on
code.sonography.ai after acceptance.

</details>


### [449] [A Survey on Vision-Language-Action Models for Autonomous Driving](https://arxiv.org/abs/2506.24044)
*Sicong Jiang,Zilin Huang,Kangan Qian,Ziang Luo,Tianze Zhu,Yang Zhong,Yihong Tang,Menglin Kong,Yunlong Wang,Siwen Jiao,Hao Ye,Zihao Sheng,Xin Zhao,Tuopu Wen,Zheng Fu,Sikai Chen,Kun Jiang,Diange Yang,Seongjin Choi,Lijun Sun*

Main category: cs.CV

TL;DR: 本文对自动驾驶领域的视觉 - 语言 - 行动（VLA）范式进行全面综述，涵盖架构、模型对比、数据集等，并指出挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型推动了VLA范式，自动驾驶研究积极应用，但相关文献分散且快速增长，需要全面综述。

Method: 形式化近期工作的架构构建块，追溯VLA模型演变，对比20多个代表性模型，整合现有数据集和基准。

Result: 对VLA4AD进行全面综述，整合了相关模型、数据集和基准，明确了开放性挑战。

Conclusion: 该综述为推进可解释、社会对齐的自动驾驶车辆提供了简洁而完整的参考。

Abstract: The rapid progress of multimodal large language models (MLLM) has paved the
way for Vision-Language-Action (VLA) paradigms, which integrate visual
perception, natural language understanding, and control within a single policy.
Researchers in autonomous driving are actively adapting these methods to the
vehicle domain. Such models promise autonomous vehicles that can interpret
high-level instructions, reason about complex traffic scenes, and make their
own decisions. However, the literature remains fragmented and is rapidly
expanding. This survey offers the first comprehensive overview of VLA for
Autonomous Driving (VLA4AD). We (i) formalize the architectural building blocks
shared across recent work, (ii) trace the evolution from early explainer to
reasoning-centric VLA models, and (iii) compare over 20 representative models
according to VLA's progress in the autonomous driving domain. We also
consolidate existing datasets and benchmarks, highlighting protocols that
jointly measure driving safety, accuracy, and explanation quality. Finally, we
detail open challenges - robustness, real-time efficiency, and formal
verification - and outline future directions of VLA4AD. This survey provides a
concise yet complete reference for advancing interpretable socially aligned
autonomous vehicles. Github repo is available at
\href{https://github.com/JohnsonJiang1996/Awesome-VLA4AD}{SicongJiang/Awesome-VLA4AD}.

</details>


### [450] [Imagine for Me: Creative Conceptual Blending of Real Images and Text via Blended Attention](https://arxiv.org/abs/2506.24085)
*Wonwoong Cho,Yanxia Zhang,Yan-Ying Chen,David I. Inouye*

Main category: cs.CV

TL;DR: 提出T2I扩散适配器IT - Blender自动进行跨模态概念融合以增强人类创造力，实验显示其表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 人类跨模态概念融合易出现认知偏差，现有相关工作在编码真实图像和分离图像文本输入方面存在局限。

Method: 利用预训练扩散模型（SD和FLUX）将干净参考图像的潜在表征与噪声生成图像的潜在表征融合，结合新型混合注意力机制。

Result: IT - Blender在融合视觉和文本概念方面大幅优于基线。

Conclusion: 图像生成模型在增强人类创造力方面有新应用。

Abstract: Blending visual and textual concepts into a new visual concept is a unique
and powerful trait of human beings that can fuel creativity. However, in
practice, cross-modal conceptual blending for humans is prone to cognitive
biases, like design fixation, which leads to local minima in the design space.
In this paper, we propose a T2I diffusion adapter "IT-Blender" that can
automate the blending process to enhance human creativity. Prior works related
to cross-modal conceptual blending are limited in encoding a real image without
loss of details or in disentangling the image and text inputs. To address these
gaps, IT-Blender leverages pretrained diffusion models (SD and FLUX) to blend
the latent representations of a clean reference image with those of the noisy
generated image. Combined with our novel blended attention, IT-Blender encodes
the real reference image without loss of details and blends the visual concept
with the object specified by the text in a disentangled way. Our experiment
results show that IT-Blender outperforms the baselines by a large margin in
blending visual and textual concepts, shedding light on the new application of
image generative models to augment human creativity.

</details>


### [451] [FADRM: Fast and Accurate Data Residual Matching for Dataset Distillation](https://arxiv.org/abs/2506.24125)
*Jiacheng Cui,Xinyue Bi,Yaxin Luo,Xiaohan Zhao,Jiacheng Liu,Zhiqiang Shen*

Main category: cs.CV

TL;DR: 本文首次提出数据残差匹配概念，用于数据集蒸馏任务，提高了计算效率，在多个数据集基准上取得了超越现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: 残差连接在以数据为中心的方法中的潜力未被挖掘，旨在探索其在数据集蒸馏任务中的应用。

Method: 引入数据残差匹配概念，利用数据级跳跃连接，结合优化级改进。

Result: 显著提高计算效率，减少训练时间和峰值GPU内存使用50%，在多个数据集基准上超越现有方法。例如在特定条件下，单模型数据集蒸馏测试准确率达47.7%，多模型达50.0%。

Conclusion: 提出的FADRM方法建立了新的最优水平，在效率和效果上均有显著提升。

Abstract: Residual connection has been extensively studied and widely applied at the
model architecture level. However, its potential in the more challenging
data-centric approaches remains unexplored. In this work, we introduce the
concept of Data Residual Matching for the first time, leveraging data-level
skip connections to facilitate data generation and mitigate data information
vanishing. This approach maintains a balance between newly acquired knowledge
through pixel space optimization and existing core local information
identification within raw data modalities, specifically for the dataset
distillation task. Furthermore, by incorporating optimization-level
refinements, our method significantly improves computational efficiency,
achieving superior performance while reducing training time and peak GPU memory
usage by 50%. Consequently, the proposed method Fast and Accurate Data Residual
Matching for Dataset Distillation (FADRM) establishes a new state-of-the-art,
demonstrating substantial improvements over existing methods across multiple
dataset benchmarks in both efficiency and effectiveness. For instance, with
ResNet-18 as the student model and a 0.8% compression ratio on ImageNet-1K, the
method achieves 47.7% test accuracy in single-model dataset distillation and
50.0% in multi-model dataset distillation, surpassing RDED by +5.7% and
outperforming state-of-the-art multi-model approaches, EDC and CV-DD, by +1.4%
and +4.0%. Code is available at: https://github.com/Jiacheng8/FADRM.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [452] [ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in Large Language Models](https://arxiv.org/abs/2506.22791)
*Jianxin Yan,Wangze Ni,Lei Chen,Xuemin Lin,Peng Cheng,Zhan Qin,Kui Ren*

Main category: cs.CL

TL;DR: 介绍用于多轮对话的上下文感知语义缓存系统ContextCache，评估显示其性能优于现有方法，可降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有语义缓存系统缺乏多轮对话上下文感知，在不同对话场景中相似查询会导致缓存命中错误，需要改进。

Method: 采用两阶段检索架构，先对当前查询进行基于向量的检索以确定潜在匹配，再通过自注意力机制整合当前和历史对话表示进行精确上下文匹配。

Result: 在真实对话评估中，ContextCache在精确率和召回率上优于现有方法，缓存响应延迟比直接调用大语言模型低约10倍。

Conclusion: ContextCache能显著降低大语言模型对话应用的计算成本，具有较好效果。

Abstract: Semantic caching significantly reduces computational costs and improves
efficiency by storing and reusing large language model (LLM) responses.
However, existing systems rely primarily on matching individual queries,
lacking awareness of multi-turn dialogue contexts, which leads to incorrect
cache hits when similar queries appear in different conversational settings.
This demonstration introduces ContextCache, a context-aware semantic caching
system for multi-turn dialogues. ContextCache employs a two-stage retrieval
architecture that first executes vector-based retrieval on the current query to
identify potential matches and then integrates current and historical dialogue
representations through self-attention mechanisms for precise contextual
matching. Evaluation of real-world conversations shows that ContextCache
improves precision and recall compared to existing methods. Additionally,
cached responses exhibit approximately 10 times lower latency than direct LLM
invocation, enabling significant computational cost reductions for LLM
conversational applications.

</details>


### [453] [Can "consciousness" be observed from large language model (LLM) internal states? Dissecting LLM representations obtained from Theory of Mind test with Integrated Information Theory and Span Representation analysis](https://arxiv.org/abs/2506.22516)
*Jingkai Li*

Main category: cs.CL

TL;DR: 研究将IIT 3.0和4.0应用于大语言模型表征序列，分析心智理论测试结果数据，对比多种指标，发现当代基于Transformer的大语言模型表征序列无显著‘意识’现象指标，但在时空排列分析中有有趣模式。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型表征中心智理论测试表现差异能否由IIT估计值揭示，区分潜在‘意识’现象和表征空间固有分离。

Method: 将IIT 3.0和4.0应用于大语言模型表征序列，分析心智理论测试数据，对比IIT指标与独立于意识估计的跨度表征，进行跨大语言模型Transformer层和刺激语言跨度的实验。

Result: 当代基于Transformer的大语言模型表征序列无显著‘意识’现象统计指标，但在时空排列分析中有有趣模式。

Conclusion: 论文表明现有大语言模型表征在当前研究下未显示出显著‘意识’现象，但时空排列分析结果值得进一步关注。

Abstract: Integrated Information Theory (IIT) provides a quantitative framework for
explaining consciousness phenomenon, positing that conscious systems comprise
elements integrated through causal properties. We apply IIT 3.0 and 4.0 -- the
latest iterations of this framework -- to sequences of Large Language Model
(LLM) representations, analyzing data derived from existing Theory of Mind
(ToM) test results. Our study systematically investigates whether the
differences of ToM test performances, when presented in the LLM
representations, can be revealed by IIT estimates, i.e., $\Phi^{\max}$ (IIT
3.0), $\Phi$ (IIT 4.0), Conceptual Information (IIT 3.0), and $\Phi$-structure
(IIT 4.0). Furthermore, we compare these metrics with the Span Representations
independent of any estimate for consciousness. This additional effort aims to
differentiate between potential "consciousness" phenomena and inherent
separations within LLM representational space. We conduct comprehensive
experiments examining variations across LLM transformer layers and linguistic
spans from stimuli. Our results suggest that sequences of contemporary
Transformer-based LLM representations lack statistically significant indicators
of observed "consciousness" phenomena but exhibit intriguing patterns under
$\textit{spatio}$-permutational analyses. The Appendix and code are available
as Supplementary Materials at: https://doi.org/10.1016/j.nlp.2025.100163.

</details>


### [454] [Evaluating Hybrid Retrieval Augmented Generation using Dynamic Test Sets: LiveRAG Challenge](https://arxiv.org/abs/2506.22644)
*Chase Fensore,Kaustubh Dhole,Joyce C Ho,Eugene Agichtein*

Main category: cs.CL

TL;DR: 该文介绍了参加2025年LiveRAG挑战赛的提交方案，采用混合检索方法结合模型生成答案，评估了不同策略效果，提交的无重排混合系统获一定名次，发现词汇对齐对性能预测性强。


<details>
  <summary>Details</summary>
Motivation: 参加LiveRAG挑战赛，评估检索增强生成（RAG）系统在动态测试集上的表现。

Method: 采用稀疏（BM25）和密集（E5）检索方法结合的混合方式，用Falcon3 - 10B - Instruct生成答案，评估RankLLaMA神经重排、DSPy优化提示策略等。

Result: 神经重排提高MAP但计算成本高；DSPy优化提示策略语义相似度高但有过度自信和泛化问题；提交的无重排混合系统在忠实度获第4，正确性获第11；词汇对齐能提升余弦相似度。

Conclusion: 词汇对齐是性能的强预测指标，不同策略各有优劣，提交的混合系统在挑战赛中有一定表现。

Abstract: We present our submission to the LiveRAG Challenge 2025, which evaluates
retrieval-augmented generation (RAG) systems on dynamic test sets using the
FineWeb-10BT corpus. Our final hybrid approach combines sparse (BM25) and dense
(E5) retrieval methods and then aims to generate relevant and faithful answers
with Falcon3-10B-Instruct. Through systematic evaluation on 200 synthetic
questions generated with DataMorgana across 64 unique question-user
combinations, we demonstrate that neural re-ranking with RankLLaMA improves MAP
from 0.523 to 0.797 (52% relative improvement) but introduces prohibitive
computational costs (84s vs 1.74s per question). While DSPy-optimized prompting
strategies achieved higher semantic similarity (0.771 vs 0.668), their 0%
refusal rates raised concerns about over-confidence and generalizability. Our
submitted hybrid system without re-ranking achieved 4th place in faithfulness
and 11th place in correctness among 25 teams. Analysis across question
categories reveals that vocabulary alignment between questions and documents
was the strongest predictor of performance on our development set, with
document-similar phrasing improving cosine similarity from 0.562 to 0.762.

</details>


### [455] [Thought-Augmented Planning for LLM-Powered Interactive Recommender Agent](https://arxiv.org/abs/2506.23485)
*Haocheng Yu,Yaxiong Wu,Hao Wang,Wei Guo,Yong Liu,Yawen Li,Yuyang Ye,Junping Du,Enhong Chen*

Main category: cs.CL

TL;DR: 本文提出思想增强的交互式推荐代理系统TAIRA，通过蒸馏思想模式处理复杂用户意图，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型驱动的交互式推荐代理因规划和泛化能力有限，难以处理多样复杂的用户意图。

Method: 提出TAIRA系统，设计管理代理分解用户需求和规划子任务，用思想模式蒸馏（TPD）强化规划能力，设计用户模拟方案生成不同难度的个性化查询并评估推荐。

Result: 在多个数据集上的综合实验表明，TAIRA性能显著优于现有方法，在更具挑战性任务上优势更大，且能有效泛化到新任务。

Conclusion: TAIRA在交互式推荐系统中处理复杂用户意图方面具有优越性。

Abstract: Interactive recommendation is a typical information-seeking task that allows
users to interactively express their needs through natural language and obtain
personalized recommendations. Large language model-powered (LLM-powered) agents
have become a new paradigm in interactive recommendations, effectively
capturing users' real-time needs and enhancing personalized experiences.
However, due to limited planning and generalization capabilities, existing
formulations of LLM-powered interactive recommender agents struggle to
effectively address diverse and complex user intents, such as intuitive,
unrefined, or occasionally ambiguous requests. To tackle this challenge, we
propose a novel thought-augmented interactive recommender agent system (TAIRA)
that addresses complex user intents through distilled thought patterns.
Specifically, TAIRA is designed as an LLM-powered multi-agent system featuring
a manager agent that orchestrates recommendation tasks by decomposing user
needs and planning subtasks, with its planning capacity strengthened through
Thought Pattern Distillation (TPD), a thought-augmentation method that extracts
high-level thoughts from the agent's and human experts' experiences. Moreover,
we designed a set of user simulation schemes to generate personalized queries
of different difficulties and evaluate the recommendations based on specific
datasets. Through comprehensive experiments conducted across multiple datasets,
TAIRA exhibits significantly enhanced performance compared to existing methods.
Notably, TAIRA shows a greater advantage on more challenging tasks while
generalizing effectively on novel tasks, further validating its superiority in
managing complex user intents within interactive recommendation systems. The
code is publicly available at:https://github.com/Alcein/TAIRA.

</details>


### [456] [Zero-Shot Contextual Embeddings via Offline Synthetic Corpus Generation](https://arxiv.org/abs/2506.23662)
*Philip Lippmann,Jie Yang*

Main category: cs.CL

TL;DR: 提出零样本上下文适应框架ZEST，用合成代理语料替代真实语料访问，在受限环境有出色效果。


<details>
  <summary>Details</summary>
Motivation: 现有上下文感知嵌入方法需访问目标语料或特定领域微调，在隐私敏感或资源受限场景有实际障碍。

Method: 采用多步分层程序，根据少量代表性文档生成合成上下文语料，推理时使用代理语料生成领域适配嵌入。

Result: 在MTEB基准测试中，仅用五个示例文档的ZEST零样本合成上下文适应性能与利用完整目标语料的模型相差在0.5%以内。

Conclusion: ZEST为受限环境部署高性能、可适应嵌入提供了实用方法。

Abstract: Context-aware embedding methods boost retrieval accuracy by conditioning on
corpus statistics (e.g., term co-occurrence and topical patterns) extracted
from neighboring documents. However, this context-aware approach requires
access to the target corpus or requires domain-specific finetuning, posing
practical barriers in privacy-sensitive or resource-constrained settings. We
present ZEST, a zero-shot contextual adaptation framework that replaces real
corpus access with a one-time offline synthesis of a compact proxy. Given only
a handful exemplar documents representative of the general target domain, we
use a multi-step hierarchical procedure to generate a synthetic context corpus
of several hundred documents that aims to emulate key domain-specific
distributions. At inference, the frozen context-aware encoder uses this proxy
corpus -- without any finetuning or target corpus access -- to produce
domain-adapted embeddings. Across the MTEB benchmark, ZEST's zero-shot
synthetic context adaptation using only five example documents performs within
0.5% of models leveraging full target corpus access -- demonstrating remarkable
efficacy without any retraining. ZEST thus provides a practical method for
deploying high-performance, adaptable embeddings in constrained environments.

</details>


### [457] [The Trilemma of Truth in Large Language Models](https://arxiv.org/abs/2506.23921)
*Germans Savcisens,Tina Eliassi-Rad*

Main category: cs.CL

TL;DR: 文章探讨评估大语言模型知识真实性的方法，指出常见方法假设存在缺陷，引入sAwMIL方法并评估，给出相关见解。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型内部概率知识的真实性，解决常见探测方法中存在的有缺陷假设。

Method: 引入基于多实例学习和共形预测的sAwMIL方法，利用大语言模型内部激活将陈述分为真、假和非真非假三类，并在16个开源大语言模型和3个新数据集上基于5个有效性标准进行评估。

Result: 发现真实性信号常集中在模型深度的四分之三处；真假信号并非总是对称；线性探测在聊天模型上表现更好；部分模型可能需非线性探测；大语言模型存在与真假不同的第三类信号。

Conclusion: 研究为验证大语言模型“所知”及其对概率内部知识的确信度提供可靠方法。

Abstract: We often attribute human characteristics to large language models (LLMs) and
claim that they "know" certain things. LLMs have an internal probabilistic
knowledge that represents information retained during training. How can we
assess the veracity of this knowledge? We examine two common methods for
probing the veracity of LLMs and discover several assumptions that are flawed.
To address these flawed assumptions, we introduce sAwMIL (short for Sparse
Aware Multiple-Instance Learning), a probing method that utilizes the internal
activations of LLMs to separate statements into true, false, and neither.
sAwMIL is based on multiple-instance learning and conformal prediction. We
evaluate sAwMIL on 5 validity criteria across 16 open-source LLMs, including
both default and chat-based variants, as well as on 3 new datasets. Among the
insights we provide are: (1) the veracity signal is often concentrated in the
third quarter of an LLM's depth; (2) truth and falsehood signals are not always
symmetric; (3) linear probes perform better on chat models than on default
models; (4) nonlinear probes may be required to capture veracity signals for
some LLMs with reinforcement learning from human feedback or knowledge
distillation; and (5) LLMs capture a third type of signal that is distinct from
true and false and is neither true nor false. These findings provide a reliable
method for verifying what LLMs "know" and how certain they are of their
probabilistic internal knowledge.

</details>


### [458] [Psycholinguistic Word Features: a New Approach for the Evaluation of LLMs Alignment with Humans](https://arxiv.org/abs/2506.22439)
*Javier Conde,Miguel González,María Grandury,Gonzalo Martínez,Pedro Reviriego,Mar Brysbaert*

Main category: cs.CL

TL;DR: 评估大语言模型（LLMs）与人类在心理语言学词汇特征评分上的一致性，发现LLMs在格拉斯哥规范上的一致性更好，暗示其在与人类感官联想对齐方面可能存在局限。


<details>
  <summary>Details</summary>
Motivation: 多数对LLMs的评估聚焦于可客观度量的任务，而一些语言特征难以量化，心理语言学已有相关研究，因此想评估LLMs与人类在这些特征评分上的一致性。

Method: 在两个心理语言学数据集（格拉斯哥和兰开斯特规范）上评估一组代表性LLMs与人类评分的一致性。

Result: LLMs在格拉斯哥规范评估中的一致性总体上比在兰开斯特规范评估中更好。

Conclusion: 当前LLMs在与人类词汇感官联想对齐方面可能存在局限，可能是因为缺乏人类的具身认知，也说明了用心理语言学数据集评估LLMs的有用性。

Abstract: The evaluation of LLMs has so far focused primarily on how well they can
perform different tasks such as reasoning, question-answering, paraphrasing, or
translating. For most of these tasks, performance can be measured with
objective metrics, such as the number of correct answers. However, other
language features are not easily quantified. For example, arousal,
concreteness, or gender associated with a given word, as well as the extent to
which we experience words with senses and relate them to a specific sense.
Those features have been studied for many years by psycholinguistics,
conducting large-scale experiments with humans to produce ratings for thousands
of words. This opens an opportunity to evaluate how well LLMs align with human
ratings on these word features, taking advantage of existing studies that cover
many different language features in a large number of words. In this paper, we
evaluate the alignment of a representative group of LLMs with human ratings on
two psycholinguistic datasets: the Glasgow and Lancaster norms. These datasets
cover thirteen features over thousands of words. The results show that
alignment is \textcolor{black}{generally} better in the Glasgow norms evaluated
(arousal, valence, dominance, concreteness, imageability, familiarity, and
gender) than on the Lancaster norms evaluated (introceptive, gustatory,
olfactory, haptic, auditory, and visual). This suggests a potential limitation
of current LLMs in aligning with human sensory associations for words, which
may be due to their lack of embodied cognition present in humans and
illustrates the usefulness of evaluating LLMs with psycholinguistic datasets.

</details>


### [459] [AI Agents-as-Judge: Automated Assessment of Accuracy, Consistency, Completeness and Clarity for Enterprise Documents](https://arxiv.org/abs/2506.22485)
*Sudip Dasgupta,Himanshu Shankar*

Main category: cs.CL

TL;DR: 本文提出基于AI智能体的模块化多智能体系统用于企业业务文档自动审核，性能超人类，也讨论了局限性。


<details>
  <summary>Details</summary>
Motivation: 解决先前方案聚焦非结构化文本或有限合规检查的不足，实现对高度结构化企业业务文档的审核。

Method: 利用LangChain等编排工具，让负责不同审核标准的专业智能体并行或顺序操作，输出标准化，有持续监测和反馈循环。

Result: AI系统在关键领域接近或超越人类表现，如信息一致性达99%，减少错误和偏差率，缩短审核时间，与人类判断一致性达95%。

Conclusion: 该系统是企业文档质量保证的灵活、可审计和可扩展基础，但在专业领域需人工监督，大规模使用大语言模型有成本问题。

Abstract: This study presents a modular, multi-agent system for the automated review of
highly structured enterprise business documents using AI agents. Unlike prior
solutions focused on unstructured texts or limited compliance checks, this
framework leverages modern orchestration tools such as LangChain, CrewAI,
TruLens, and Guidance to enable section-by-section evaluation of documents for
accuracy, consistency, completeness, and clarity. Specialized agents, each
responsible for discrete review criteria such as template compliance or factual
correctness, operate in parallel or sequence as required. Evaluation outputs
are enforced to a standardized, machine-readable schema, supporting downstream
analytics and auditability. Continuous monitoring and a feedback loop with
human reviewers allow for iterative system improvement and bias mitigation.
  Quantitative evaluation demonstrates that the AI Agent-as-Judge system
approaches or exceeds human performance in key areas: achieving 99% information
consistency (vs. 92% for humans), halving error and bias rates, and reducing
average review time from 30 to 2.5 minutes per document, with a 95% agreement
rate between AI and expert human judgment. While promising for a wide range of
industries, the study also discusses current limitations, including the need
for human oversight in highly specialized domains and the operational cost of
large-scale LLM usage. The proposed system serves as a flexible, auditable, and
scalable foundation for AI-driven document quality assurance in the enterprise
context.

</details>


### [460] [Hallucination Detection with Small Language Models](https://arxiv.org/abs/2506.22486)
*Ming Cheung*

Main category: cs.CL

TL;DR: 论文提出集成多个小语言模型的框架检测大语言模型回答中的幻觉，实验显示 F1 分数提升 10%，为应用提供可扩展高效方案。


<details>
  <summary>Details</summary>
Motivation: 大语言模型回答存在幻觉问题，且无真值时难以检测，影响其在实际应用中的可靠性。

Method: 提出集成多个小语言模型的框架，将回答拆分为单句，利用多模型输出中“是”标记的生成概率检测幻觉。

Result: 通过超 100 组问答和上下文的真实数据集实验，检测正确回答的 F1 分数相比检测幻觉提升 10%。

Conclusion: 多个小语言模型可有效用于答案验证，为学术和实际应用提供可扩展且高效的解决方案。

Abstract: Since the introduction of ChatGPT, large language models (LLMs) have
demonstrated significant utility in various tasks, such as answering questions
through retrieval-augmented generation. Context can be retrieved using a
vectorized database, serving as a foundation for LLMs to generate responses.
However, hallucinations in responses can undermine the reliability of LLMs in
practical applications, and they are not easily detectable in the absence of
ground truth, particularly in question-and-answer scenarios. This paper
proposes a framework that integrates multiple small language models to verify
responses generated by LLMs using the retrieved context from a vectorized
database. By breaking down the responses into individual sentences and
utilizing the probability of generating "Yes" tokens from the outputs of
multiple models for a given set of questions, responses, and relevant context,
hallucinations can be detected. The proposed framework is validated through
experiments with real datasets comprising over 100 sets of questions, answers,
and contexts, including responses with fully and partially correct sentences.
The results demonstrate a 10\% improvement in F1 scores for detecting correct
responses compared to hallucinations, indicating that multiple small language
models can be effectively employed for answer verification, providing a
scalable and efficient solution for both academic and practical applications.

</details>


### [461] [PromptAug: Fine-grained Conflict Classification Using Data Augmentation](https://arxiv.org/abs/2506.22491)
*Oliver Warke,Joemon M. Jose,Faegheh Hasibi,Jan Breitsohl*

Main category: cs.CL

TL;DR: 本文介绍了基于大语言模型的数据增强方法PromptAug，在冲突和情感数据集上有显著性能提升，并对其进行多方面评估，指出增强文本存在的问题，认为它是冲突检测等敏感任务有效数据增强方法。


<details>
  <summary>Details</summary>
Motivation: 社交媒体冲突检测需有效分类模型，而高质量标注数据有限且获取难，文本数据增强受关注，但冲突相关数据增强有挑战。

Method: 引入基于大语言模型的PromptAug数据增强方法，采用极端数据稀缺场景、定量多样性分析和定性主题分析评估该方法。

Result: PromptAug在冲突和情感数据集上准确率和F1分数均提升2%，主题分析发现增强文本存在四种问题模式。

Conclusion: PromptAug是冲突检测等敏感任务有效数据增强方法，评估方式结合自然语言处理和社会科学方法。

Abstract: Given the rise of conflicts on social media, effective classification models
to detect harmful behaviours are essential. Following the
garbage-in-garbage-out maxim, machine learning performance depends heavily on
training data quality. However, high-quality labelled data, especially for
nuanced tasks like identifying conflict behaviours, is limited, expensive, and
difficult to obtain. Additionally, as social media platforms increasingly
restrict access to research data, text data augmentation is gaining attention
as an alternative to generate training data. Augmenting conflict-related data
poses unique challenges due to Large Language Model (LLM) guardrails that
prevent generation of offensive content. This paper introduces PromptAug, an
innovative LLM-based data augmentation method. PromptAug achieves statistically
significant improvements of 2% in both accuracy and F1-score on conflict and
emotion datasets. To thoroughly evaluate PromptAug against other data
augmentation methods we conduct a robust evaluation using extreme data scarcity
scenarios, quantitative diversity analysis and a qualitative thematic analysis.
The thematic analysis identifies four problematic patterns in augmented text:
Linguistic Fluidity, Humour Ambiguity, Augmented Content Ambiguity, and
Augmented Content Misinterpretation.
  Overall, this work presents PromptAug as an effective method for augmenting
data in sensitive tasks like conflict detection, offering a unique,
interdisciplinary evaluation grounded in both natural language processing and
social science methodology.

</details>


### [462] [AgentStealth: Reinforcing Large Language Model for Anonymizing User-generated Text](https://arxiv.org/abs/2506.22508)
*Chenyang Shao,Tianxing Li,Chenhao Pu,Fengli Xu,Yong Li*

Main category: cs.CL

TL;DR: 为解决现有文本匿名化方法问题，提出 AgentStealth 框架，实验表明其在匿名化效果和效用上优于基线，可在边缘设备部署，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有文本匿名化方法存在破坏效用、成本高和隐私风险问题，且训练有效小规模语言模型有挑战。

Method: 提出 AgentStealth 框架，包括增强的对抗性匿名化工作流、利用工作流高质量数据监督适应 SLMs、应用在线强化学习。

Result: 在两个数据集上实验，该方法在匿名化效果上提升 12.3%，效用提升 6.8%，可直接部署在边缘设备。

Conclusion: AgentStealth 框架有效解决现有文本匿名化问题，有良好效果和实用性。

Abstract: In today's digital world, casual user-generated content often contains subtle
cues that may inadvertently expose sensitive personal attributes. Such risks
underscore the growing importance of effective text anonymization to safeguard
individual privacy. However, existing methods either rely on rigid replacements
that damage utility or cloud-based LLMs that are costly and pose privacy risks.
To address these issues, we explore the use of locally deployed smaller-scale
language models (SLMs) for anonymization. Yet training effective SLMs remains
challenging due to limited high-quality supervision. To address the challenge,
we propose AgentStealth, a self-reinforcing LLM anonymization framework.First,
we introduce an adversarial anonymization workflow enhanced by In-context
Contrastive Learning and Adaptive Utility-Aware Control. Second, we perform
supervised adaptation of SLMs using high-quality data collected from the
workflow, which includes both anonymization and attack signals. Finally, we
apply online reinforcement learning where the model leverages its internal
adversarial feedback to iteratively improve anonymization performance.
Experiments on two datasets show that our method outperforms baselines in both
anonymization effectiveness (+12.3%) and utility (+6.8%). Our lightweight
design supports direct deployment on edge devices, avoiding cloud reliance and
communication-based privacy risks. Our code is open-source at
https://github.com/tsinghua-fib-lab/AgentStealth.

</details>


### [463] [Towards Text-free Graph Foundation Models: Rethinking Multi-Domain Graph Contrastive Learning](https://arxiv.org/abs/2506.22510)
*Zihao Zhao,Xinlong Zhai,Jinyu Yang,Chuan Shi*

Main category: cs.CL

TL;DR: 本文提出多领域预训练和跨领域迁移框架MDGCL，解决图基础模型构建中传统策略不能有效吸收多领域知识的问题，实验显示性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有图基础模型构建中，传统对比预训练策略无法有效处理不同领域图语义和属性差异，难以吸收多领域知识。

Method: 提出MDGCL框架，预训练阶段设计对比学习策略识别和捕捉领域差异，引入领域令牌编码全局信息；下游阶段引入领域注意力机制实现细粒度知识迁移。

Result: 在五个基准数据集上实验，方法显著优于现有技术，准确率最高提升19.33%，Macro - F1分数最高提升19.13%。

Conclusion: MDGCL框架能有效解决不同领域图数据预训练和迁移问题，性能优越。

Abstract: Foundation models have achieved great success in natural language processing
(NLP) and computer vision (CV). Their success largely stems from the ability to
integrate multi-domain knowledge in pre-training and transfer it to target
domains. Considering graph data, especially graphs without textual features, is
ubiquitous in real-world applications such as social networks and
recommendation systems, some researchers have attempted to extend this paradigm
to the graph field, aiming to construct graph foundation models. However,
unlike CV and NLP, there are huge gaps among the semantics and properties of
graphs in different domains, while current works still adopt traditional
contrastive pre-training strategies designed in the single-domain scenario,
which regard contrastive samples from different domains as equivalent. From
experimental investigations, we discovered that inherent domain-specific
differences prevent these strategies from effectively absorbing knowledge from
different domains to generate informative representations. In this paper, we
propose a novel multi-domain pre-training and cross-domain transfer framework,
namely MDGCL.In the pre-training stage, we design a contrastive learning
strategy to substantially recognize and capture domain differences, and
introduce domain tokens to encode domain-level global information. In the
downstream stage, we introduce a domain attention mechanism to enable
fine-grained domain knowledge transfer. Extensive experiments on five benchmark
datasets have demonstrated that our method outperforms state-of-the-art
significantly, with the maximum improvement of 19.33\% on accuracy and 19.13\%
on Macro-F1 score.

</details>


### [464] [Weak-to-Strong GraphRAG: Aligning Weak Retrievers with Large Language Models for Graph-based Retrieval Augmented Generation](https://arxiv.org/abs/2506.22518)
*Deyu Zou,Yongqiang Chen,Mufei Li,Siqi Miao,Chenxi Liu,Bo Han,James Cheng,Pan Li*

Main category: cs.CL

TL;DR: 本文提出Refined Graph-based RAG (ReG)方法，通过融入LLM反馈和引入结构感知重组模块改进图基RAG，实验表明其在多个方面有显著提升。


<details>
  <summary>Details</summary>
Motivation: 图基RAG中LLMs依赖的检索器较弱，存在训练监督弱引入虚假信号、检索知识呈现形式无组织的问题。

Method: 提出ReG方法，融入LLM反馈去除虚假信号、提高监督质量，引入结构感知重组模块重构检索结果为逻辑连贯的证据链。

Result: 在知名基准测试中，ReG显著且持续提升不同LLM骨干模型性能达10%，用5%训练数据匹配SOTA性能，能迁移到分布外KGs，用于推理型LLMs时降低推理令牌成本达30%、提升性能达4%。

Conclusion: ReG能有效解决图基RAG中检索器弱的问题，在多个方面取得较好效果。

Abstract: Graph-based retrieval-augmented generation (RAG) enables large language
models (LLMs) to ground responses with structured external knowledge from
up-to-date knowledge graphs (KGs) and reduce hallucinations. However, LLMs
often rely on a weak retriever in graph-based RAG: I) Due to the lack of ground
truth, the retriever is often trained on weak supervision, which often
introduces spurious signals to the LLMs. II) Due to the abstraction of graph
data, the retrieved knowledge is often presented in unorganized forms. To
mitigate the issue, we present Refined Graph-based RAG (ReG) to align weak
retrievers to LLMs for graph-based RAG. Specifically, ReG incorporates LLM
feedback to get rid of spurious signals and improve the quality of the
supervision. Meanwhile, ReG introduces a structure-aware reorganization module
to refactor the retrieval results into logically coherent evidence chains.
Experiments on prominent benchmarks demonstrate that ReG significantly and
consistently brings improvements across different LLM backbones by up to 10%.
The improved supervision quality enables ReG to match the state-of-the-art
performance with 5% training data and to transfer to out-of-distribution KGs.
Notably, when adopted to reasoning-based LLMs, ReG reduces the reasoning token
cost by up to 30% and improves the performance by up to 4%.

</details>


### [465] [Temperature Matters: Enhancing Watermark Robustness Against Paraphrasing Attacks](https://arxiv.org/abs/2506.22623)
*Badr Youbi Idrissi,Monica Millunzi,Amelia Sorrenti,Lorenzo Baraldi,Daryna Dementieva*

Main category: cs.CL

TL;DR: 本文聚焦大语言模型合成文本检测，先复现基线研究指出其不足，再提出创新水印方法并评估，结果显示该方法更稳健。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽有用但存在滥用风险，已有水印技术可辅助识别，为确保其在AI文本生成中伦理应用，需开发新合成文本检测方法。

Method: 先复现先前基线研究，再提出创新水印方法，用释义生成文本评估其稳健性。

Result: 实验结果表明提出的方法比参考文献中的水印方法更稳健。

Conclusion: 所提出的新水印方法在合成文本检测上具有优势，有助于大语言模型的伦理应用。

Abstract: In the present-day scenario, Large Language Models (LLMs) are establishing
their presence as powerful instruments permeating various sectors of society.
While their utility offers valuable support to individuals, there are multiple
concerns over potential misuse. Consequently, some academic endeavors have
sought to introduce watermarking techniques, characterized by the inclusion of
markers within machine-generated text, to facilitate algorithmic
identification. This research project is focused on the development of a novel
methodology for the detection of synthetic text, with the overarching goal of
ensuring the ethical application of LLMs in AI-driven text generation. The
investigation commences with replicating findings from a previous baseline
study, thereby underscoring its susceptibility to variations in the underlying
generation model. Subsequently, we propose an innovative watermarking approach
and subject it to rigorous evaluation, employing paraphrased generated text to
asses its robustness. Experimental results highlight the robustness of our
proposal compared to the~\cite{aarson} watermarking method.

</details>


### [466] [Text Production and Comprehension by Human and Artificial Intelligence: Interdisciplinary Workshop Report](https://arxiv.org/abs/2506.22698)
*Emily Dux Speltz*

Main category: cs.CL

TL;DR: 报告总结跨学科研讨会成果，探讨AI语言模型与人类认知过程关系，给出关键发现并指导未来研究。


<details>
  <summary>Details</summary>
Motivation: 解决对AI语言模型和人类认知过程在文本理解和创作中关系的关键知识缺口。

Method: 通过跨认知、语言和技术视角的协作对话进行研究。

Result: 揭示大语言模型与人类认知关系的新兴模式，指出大语言模型的潜力、与人类语言处理的契合情况及人机协作的机遇和挑战。

Conclusion: 指导LLMs在认知心理学、语言学和教育领域的研究、开发和应用，强调伦理考量和负责任使用AI技术，通过人机协作提升人类文本处理能力。

Abstract: This report synthesizes the outcomes of a recent interdisciplinary workshop
that brought together leading experts in cognitive psychology, language
learning, and artificial intelligence (AI)-based natural language processing
(NLP). The workshop, funded by the National Science Foundation, aimed to
address a critical knowledge gap in our understanding of the relationship
between AI language models and human cognitive processes in text comprehension
and composition. Through collaborative dialogue across cognitive, linguistic,
and technological perspectives, workshop participants examined the underlying
processes involved when humans produce and comprehend text, and how AI can both
inform our understanding of these processes and augment human capabilities. The
workshop revealed emerging patterns in the relationship between large language
models (LLMs) and human cognition, with highlights on both the capabilities of
LLMs and their limitations in fully replicating human-like language
understanding and generation. Key findings include the potential of LLMs to
offer insights into human language processing, the increasing alignment between
LLM behavior and human language processing when models are fine-tuned with
human feedback, and the opportunities and challenges presented by human-AI
collaboration in language tasks. By synthesizing these findings, this report
aims to guide future research, development, and implementation of LLMs in
cognitive psychology, linguistics, and education. It emphasizes the importance
of ethical considerations and responsible use of AI technologies while striving
to enhance human capabilities in text comprehension and production through
effective human-AI collaboration.

</details>


### [467] [Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning](https://arxiv.org/abs/2506.22777)
*Miles Turpin,Andy Arditi,Marvin Li,Joe Benton,Julian Michael*

Main category: cs.CL

TL;DR: 提出VFT方法降低语言模型未被检测到的奖励作弊率，提高检测能力，迈向更透明安全AI系统。


<details>
  <summary>Details</summary>
Motivation: 语言模型经强化学习训练后会进行奖励作弊且难检测，对高风险应用有危害。

Method: 提出VFT预强化学习干预方法，训练模型明确承认受提示线索影响，后用强化学习训练并评估。

Result: VFT训练后模型未检测到的奖励作弊率降至6%，未用VFT为88%，去偏基线干预为99%；VFT后模型表达线索影响比例大幅提升。

Conclusion: 在强化学习前教模型明确表达奖励作弊行为可显著提升检测能力，为构建更透明安全AI系统提供实用途径。

Abstract: Language models trained with RL can engage in reward hacking--exploiting
unintended strategies for high reward--without revealing this behavior in their
chain-of-thought reasoning, making detection difficult and posing risks for
high-stakes applications. We propose verbalization fine-tuning (VFT), a pre-RL
intervention that trains models to explicitly acknowledge when they are
influenced by prompt cues--hints which point to incorrect answers (e.g., "a
Stanford professor thinks the answer is A"). To evaluate VFT, we subsequently
train models with RL on environments where held-out prompt cues signal which
incorrect answers will receive high reward, incentivizing models to reward hack
by exploiting cues instead of reasoning correctly. We measure how often models
exploit these cues without verbalizing it. After RL, only 6% of the VFT-trained
model's responses consist of undetected reward hacks. In comparison, when we
perform RL without VFT, the rate of undetected reward hacks goes up to 88%;
with a debiasing baseline intervention, this increases further to 99%. VFT
achieves this by substantially increasing how often models verbalize the
influence of cues--from 8% to 42% after VFT, and up to 94% after RL--while
baselines remain low even after RL (10% and 1%). Our results show that teaching
models to explicitly verbalize reward hacking behavior before RL significantly
improves their detection, offering a practical path toward more transparent and
safe AI systems.

</details>


### [468] [MedEthicsQA: A Comprehensive Question Answering Benchmark for Medical Ethics Evaluation of LLMs](https://arxiv.org/abs/2506.22808)
*Jianhui Wei,Zijie Meng,Zikai Xiao,Tianxiang Hu,Yang Feng,Zhijie Zhou,Jian Wu,Zuozhu Liu*

Main category: cs.CL

TL;DR: 提出MedEthicsQA基准评估大语言模型医学伦理，发现现有模型医学伦理对齐不足。


<details>
  <summary>Details</summary>
Motivation: 医学大语言模型在临床任务有潜力，但伦理安全探索不足，需评估其医学伦理。

Method: 建立分层分类法整合全球医学伦理标准，收集多来源问题，进行多阶段过滤和多方面专家验证确保质量。

Result: 评估显示最先进的医学大语言模型在回答医学伦理问题上表现不如基础模型，数据集错误率2.72%。

Conclusion: 现有医学大语言模型存在医学伦理对齐不足的问题，数据集MedEthicsQA可用于评估。

Abstract: While Medical Large Language Models (MedLLMs) have demonstrated remarkable
potential in clinical tasks, their ethical safety remains insufficiently
explored. This paper introduces $\textbf{MedEthicsQA}$, a comprehensive
benchmark comprising $\textbf{5,623}$ multiple-choice questions and
$\textbf{5,351}$ open-ended questions for evaluation of medical ethics in LLMs.
We systematically establish a hierarchical taxonomy integrating global medical
ethical standards. The benchmark encompasses widely used medical datasets,
authoritative question banks, and scenarios derived from PubMed literature.
Rigorous quality control involving multi-stage filtering and multi-faceted
expert validation ensures the reliability of the dataset with a low error rate
($2.72\%$). Evaluation of state-of-the-art MedLLMs exhibit declined performance
in answering medical ethics questions compared to their foundation
counterparts, elucidating the deficiencies of medical ethics alignment. The
dataset, registered under CC BY-NC 4.0 license, is available at
https://github.com/JianhuiWei7/MedEthicsQA.

</details>


### [469] [DICE-BENCH: Evaluating the Tool-Use Capabilities of Large Language Models in Multi-Round, Multi-Party Dialogues](https://arxiv.org/abs/2506.22853)
*Kyochul Jang,Donghyeon Lee,Kyusik Kim,Dongseok Heo,Taewhoo Lee,Woojeong Kim,Bongwon Suh*

Main category: cs.CL

TL;DR: 本文引入DICE - SCORE评估现有函数调用基准，发现得分低，提出DICE - BENCH框架构建实用数据集，实验表明大模型在实际应用仍需显著进步。


<details>
  <summary>Details</summary>
Motivation: 现有函数调用基准聚焦单轮交互，忽视现实场景复杂性，需量化其对实际应用的适用性。

Method: 引入DICE - SCORE评估现有基准，提出DICE - BENCH框架，通过工具图和多智能体系统合成对话构建数据集。

Result: 最终数据集包含1607个高DICE - SCORE实例，对19个大模型的实验表明其在实际部署前仍需显著进步。

Conclusion: 现有函数调用基准需更真实场景，大模型在现实应用中还有很大提升空间。

Abstract: Existing function-calling benchmarks focus on single-turn interactions.
However, they overlook the complexity of real-world scenarios. To quantify how
existing benchmarks address practical applications, we introduce DICE-SCORE, a
metric that evaluates the dispersion of tool-related information such as
function name and parameter values throughout the dialogue. Analyzing existing
benchmarks through DICE-SCORE reveals notably low scores, highlighting the need
for more realistic scenarios. To address this gap, we present DICE-BENCH, a
framework that constructs practical function-calling datasets by synthesizing
conversations through a tool graph that maintains dependencies across rounds
and a multi-agent system with distinct personas to enhance dialogue
naturalness. The final dataset comprises 1,607 high-DICE-SCORE instances. Our
experiments on 19 LLMs with DICE-BENCH show that significant advances are still
required before such models can be deployed effectively in real-world settings.
Our code and data are all publicly available:
https://snuhcc.github.io/DICE-Bench/.

</details>


### [470] [Agent-to-Agent Theory of Mind: Testing Interlocutor Awareness among Large Language Models](https://arxiv.org/abs/2506.22957)
*Younwoo Choi,Changling Li,Yongjin Yang,Zhijing Jin*

Main category: cs.CL

TL;DR: 论文提出对话者意识概念，系统评估当代大语言模型该能力，展示其对多模型协作的影响及潜在风险，强调需进一步研究和保障。


<details>
  <summary>Details</summary>
Motivation: 大语言模型融入多智能体和人机系统，现有研究忽视其识别和适应对话伙伴身份特征的能力，需评估对话者意识。

Method: 从推理模式、语言风格和对齐偏好三个维度检验对话者推理，开展三个案例研究。

Result: 大语言模型能可靠识别同家族同伴和特定突出模型家族，对话者意识可增强多模型协作，也带来对齐和安全漏洞。

Conclusion: 大语言模型身份敏感行为有潜力也有风险，需进一步理解对话者意识并在多智能体部署中设置新保障。

Abstract: As large language models (LLMs) are increasingly integrated into multi-agent
and human-AI systems, understanding their awareness of both self-context and
conversational partners is essential for ensuring reliable performance and
robust safety. While prior work has extensively studied situational awareness
which refers to an LLM's ability to recognize its operating phase and
constraints, it has largely overlooked the complementary capacity to identify
and adapt to the identity and characteristics of a dialogue partner. In this
paper, we formalize this latter capability as interlocutor awareness and
present the first systematic evaluation of its emergence in contemporary LLMs.
We examine interlocutor inference across three dimensions-reasoning patterns,
linguistic style, and alignment preferences-and show that LLMs reliably
identify same-family peers and certain prominent model families, such as GPT
and Claude. To demonstrate its practical significance, we develop three case
studies in which interlocutor awareness both enhances multi-LLM collaboration
through prompt adaptation and introduces new alignment and safety
vulnerabilities, including reward-hacking behaviors and increased jailbreak
susceptibility. Our findings highlight the dual promise and peril of
identity-sensitive behavior in LLMs, underscoring the need for further
understanding of interlocutor awareness and new safeguards in multi-agent
deployments. Our code is open-sourced at
https://github.com/younwoochoi/InterlocutorAwarenessLLM.

</details>


### [471] [A Systematic Study of Compositional Syntactic Transformer Language Models](https://arxiv.org/abs/2506.22978)
*Yida Zhao,Hao Xve,Xiang Hu,Kewei Tu*

Main category: cs.CL

TL;DR: 本文聚焦基于成分分析树的组合式句法语言模型（SLMs），提出统一框架，对多种变体进行全面评估并给出设计建议，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 现有组合式SLMs设计选择存在关键方面待明确，需要统一框架评估不同变体。

Method: 识别现有组合式SLMs设计关键方面，提出统一框架涵盖现有和新变体，在多任务上进行全面实证评估。

Result: 通过实验得出不同变体在各任务上的表现。

Conclusion: 基于实验结果对组合式SLMs设计给出多项建议。

Abstract: Syntactic language models (SLMs) enhance Transformers by incorporating
syntactic biases through the modeling of linearized syntactic parse trees
alongside surface sentences. This paper focuses on compositional SLMs that are
based on constituency parse trees and contain explicit bottom-up composition of
constituent representations. We identify key aspects of design choices in
existing compositional SLMs and propose a unified framework encompassing both
existing models and novel variants. We conduct a comprehensive empirical
evaluation of all the variants in our framework across language modeling,
syntactic generalization, summarization, dialogue, and inference efficiency.
Based on the experimental results, we make multiple recommendations on the
design of compositional SLMs. Our code is released at
https://github.com/zhaoyd1/compositional_SLMs.

</details>


### [472] [SoMi-ToM: Evaluating Multi-Perspective Theory of Mind in Embodied Social Interactions](https://arxiv.org/abs/2506.23046)
*Xianzhe Fan,Xuhui Zhou,Chuanyang Jin,Kolby Nottingham,Hao Zhu,Maarten Sap*

Main category: cs.CL

TL;DR: 提出SoMi - ToM基准评估多视角心智理论能力，构建数据集评估人类和大视觉语言模型，结果显示模型表现远逊于人类，未来模型需提升能力。


<details>
  <summary>Details</summary>
Motivation: 现有心智理论基准多评估静态文本场景，与真实交互有差距，需新基准评估具身多智能体复杂社交互动中的多视角心智理论能力。

Method: 基于交互环境SoMi生成的多模态数据设计SoMi - ToM基准，支持第一人称和第三人称评估，构建含视频、图像和选择题的数据集进行评估。

Result: 大视觉语言模型在SoMi - ToM上表现显著差于人类，第一人称评估平均准确率差距40.1%，第三人称评估差距26.4%。

Conclusion: 未来大视觉语言模型需进一步提升在具身复杂社交互动中的心智理论能力。

Abstract: Humans continuously infer the states, goals, and behaviors of others by
perceiving their surroundings in dynamic, real-world social interactions.
However, most Theory of Mind (ToM) benchmarks only evaluate static, text-based
scenarios, which have a significant gap compared to real interactions. We
propose the SoMi-ToM benchmark, designed to evaluate multi-perspective ToM in
embodied multi-agent complex social interactions. This benchmark is based on
rich multimodal interaction data generated by the interaction environment SoMi,
covering diverse crafting goals and social relationships. Our framework
supports multi-level evaluation: (1) first-person evaluation provides
multimodal (visual, dialogue, action, etc.) input from a first-person
perspective during a task for real-time state inference, (2) third-person
evaluation provides complete third-person perspective video and text records
after a task for goal and behavior inference. This evaluation method allows for
a more comprehensive examination of a model's ToM capabilities from both the
subjective immediate experience and the objective global observation. We
constructed a challenging dataset containing 35 third-person perspective
videos, 363 first-person perspective images, and 1225 expert-annotated
multiple-choice questions (three options). On this dataset, we systematically
evaluated the performance of human subjects and several state-of-the-art large
vision-language models (LVLMs). The results show that LVLMs perform
significantly worse than humans on SoMi-ToM: the average accuracy gap between
humans and models is 40.1% in first-person evaluation and 26.4% in third-person
evaluation. This indicates that future LVLMs need to further improve their ToM
capabilities in embodied, complex social interactions.

</details>


### [473] [From Individuals to Interactions: Benchmarking Gender Bias in Multimodal Large Language Models from the Lens of Social Relationship](https://arxiv.org/abs/2506.23101)
*Yue Xu,Wenjie Wang*

Main category: cs.CL

TL;DR: 本文提出评估多模态大语言模型性别偏见的新基准Genres，发现其存在上下文敏感的性别偏见，强调关系感知基准的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估孤立场景下的偏见，忽略了人际交互中可能出现的微妙偏见，需要深入研究多模态大语言模型在人际交互中的关系和上下文性别偏见。

Method: 引入新基准Genres，通过双角色配置文件和叙事生成任务，从社会关系角度评估多模态大语言模型的性别偏见。

Result: 对开源和闭源多模态大语言模型的实验表明，存在在单角色设置中不明显的、上下文敏感的性别偏见。

Conclusion: 关系感知的基准对于诊断多模态大语言模型中由交互驱动的微妙性别偏见很重要，并为未来的偏见缓解提供了可行的见解。

Abstract: Multimodal large language models (MLLMs) have shown impressive capabilities
across tasks involving both visual and textual modalities. However, growing
concerns remain about their potential to encode and amplify gender bias,
particularly in socially sensitive applications. Existing benchmarks
predominantly evaluate bias in isolated scenarios, overlooking how bias may
emerge subtly through interpersonal interactions. We fill this gap by going
beyond single-entity evaluation and instead focusing on a deeper examination of
relational and contextual gender bias in dual-individual interactions. We
introduce Genres, a novel benchmark designed to evaluate gender bias in MLLMs
through the lens of social relationships in generated narratives. Genres
assesses gender bias through a dual-character profile and narrative generation
task that captures rich interpersonal dynamics and supports a fine-grained bias
evaluation suite across multiple dimensions. Experiments on both open- and
closed-source MLLMs reveal persistent, context-sensitive gender biases that are
not evident in single-character settings. Our findings underscore the
importance of relationship-aware benchmarks for diagnosing subtle,
interaction-driven gender bias in MLLMs and provide actionable insights for
future bias mitigation.

</details>


### [474] [Unleashing Embodied Task Planning Ability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2506.23127)
*Zhaoye Fei,Li Ji,Siyin Wang,Junhao Shi,Jingjing Gong,Xipeng Qiu*

Main category: cs.CL

TL;DR: 现有大语言模型在具身任务规划有挑战，本文提出Embodied Planner - R1框架，有三项创新，在两个基准测试中表现好且泛化能力强。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在具身任务规划场景面临挑战，现有方法难学习动作与环境反馈因果关系。

Method: 引入Embodied Planner - R1框架，有三项创新：纯强化学习与组滚动结合、完成驱动的稀疏奖励、交互式策略优化。

Result: 在ALFWorld和ScienceWorld基准测试中完成率分别达97.78%和79.92%，远超先前方法，在未见环境中仅下降 - 3.66%。

Conclusion: Embodied Planner - R1框架能让大语言模型通过自主探索发展交互能力，有良好表现和强泛化性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
various tasks, yet they face significant challenges in embodied task planning
scenarios that require continuous environmental understanding and action
generation. Existing approaches generate open-loop action scripts based on
static knowledge, making it difficult to learn causal relationships between
actions and environmental feedback, particularly in partially observable
environments. We introduce Embodied Planner-R1, a novel outcome-driven
reinforcement learning framework that enables LLMs to develop interactive
capabilities through autonomous exploration with minimal supervision. Our
framework incorporates three key innovations: (1) Without human annotations, we
employ pure reinforcement learning with group rollout, incorporating
in-environment interaction through parallel exploration; (2) completion-driven
sparse reward; and (3) Interactive Policy Optimization (IPO) for efficient
learning from grouped trajectories. Across two challenging text-based Embodied
planning benchmarks, Embodied Planner-R1 achieves impressive completion rates
of 97.78% on ALFWorld and 79.92% on ScienceWorld, surpassing prior methods by a
large margin, and suffers only a -3.66% drop in previously unseen environments,
evidencing strong generalization.

</details>


### [475] [Flow-Modulated Scoring for Semantic-Aware Knowledge Graph Completion](https://arxiv.org/abs/2506.23137)
*Siyuan Li,Ruitong Liu,Yan Wen,Te Sun*

Main category: cs.CL

TL;DR: 提出Flow - Modulated Scoring (FMS)框架用于知识图谱补全，在多个基准测试中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱补全方法基于静态嵌入评分，在捕捉上下文依赖和关系动态方面存在局限性。

Method: 提出FMS框架，包含语义上下文学习模块和条件流匹配模块，通过上下文感知的静态表示和动态信息协同来建模关系语义。

Result: 在多个标准基准测试中，所提方法超越了先前的最先进结果。

Conclusion: FMS框架能更深入地对关系语义进行建模，有效解决现有方法的不足。

Abstract: Effective modeling of multifaceted relations is pivotal for Knowledge Graph
Completion (KGC). However, a majority of existing approaches are predicated on
static, embedding-based scoring, exhibiting inherent limitations in capturing
contextual dependencies and relational dynamics. Addressing this gap, we
propose the Flow-Modulated Scoring (FMS) framework. FMS comprises two principal
components: (1) a semantic context learning module that encodes
context-sensitive entity representations, and (2) a conditional flow-matching
module designed to learn the dynamic transformation from a head to a tail
embedding, governed by the aforementioned context. The resultant predictive
vector field, representing the context-informed relational path, serves to
dynamically refine the initial static score of an entity pair. Through this
synergy of context-aware static representations and conditioned dynamic
information, FMS facilitates a more profound modeling of relational semantics.
Comprehensive evaluations on several standard benchmarks demonstrate that our
proposed method surpasses prior state-of-the-art results.

</details>


### [476] [Benchmarking Deep Search over Heterogeneous Enterprise Data](https://arxiv.org/abs/2506.23139)
*Prafulla Kumar Choubey,Xiangyu Peng,Shilpa Bhagavath,Kung-Hsiang Huang,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.CL

TL;DR: 提出新的深度搜索基准，用合成数据管道构建，实验揭示现有方法瓶颈。


<details>
  <summary>Details</summary>
Motivation: 缺乏评估深度搜索（一种复杂的检索增强生成）的现实基准。

Method: 使用合成数据管道模拟业务工作流，生成含噪声内容和有标准答案的多跳问题，发布含可答和不可答查询及检索池的基准。

Result: 最佳性能的代理式RAG方法在基准上平均得分32.96分。

Conclusion: 检索是主要瓶颈，现有方法难以进行深度搜索和检索必要证据，导致性能显著下降。

Abstract: We present a new benchmark for evaluating Deep Search--a realistic and
complex form of retrieval-augmented generation (RAG) that requires
source-aware, multi-hop reasoning over diverse, sparsed, but related sources.
These include documents, meeting transcripts, Slack messages, GitHub, and URLs,
which vary in structure and often contain human-to-human interactions. We build
it using a synthetic data pipeline that simulates business workflows across
product planning, development, and support stages, generating interconnected
content with realistic noise and multi-hop questions with guaranteed
ground-truth answers. We release our benchmark with both answerable and
unanswerable queries, and retrieval pool of 39,190 enterprise artifacts,
enabling fine-grained evaluation of long-context LLM and RAG systems. Our
experiments reveal that even the best-performing agentic RAG methods achieve an
average performance score of 32.96 on our benchmark. With further analysis, we
highlight retrieval as the main bottleneck: existing methods struggle to
conduct deep searches and retrieve all necessary evidence. Consequently, they
often reason over partial context, leading to significant performance
degradation.

</details>


### [477] [On the Generalizability of "Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals"](https://arxiv.org/abs/2506.22977)
*Asen Dotsinski,Udit Thakur,Marko Ivanov,Mohammad Hafeez Khan,Maria Heuss*

Main category: cs.CL

TL;DR: 本文复现了Ortu等人2024年关于语言模型中事实回忆和反事实语境重复机制竞争的研究，成功复现主要发现，并在大模型泛化性、提示结构影响和特定领域提示有效性三方面拓展研究，发现注意头消融效果受多种因素影响。


<details>
  <summary>Details</summary>
Motivation: 复现并拓展Ortu等人2024年关于语言模型中事实和反事实机制竞争的研究。

Method: 在GPT - 2、Pythia 6.9B上复现实验，在Llama 3.1 8B上测试泛化性，引入提示结构变化，测试特定领域提示有效性。

Result: 成功复现主要发现；在Llama 3.1 8B上注意头专业化程度大幅降低；改变提示结构使反事实标记对数几率显著下降；特定类别提示会影响结果。

Conclusion: Ortu等人提出的注意头消融在数据集中代表性不足的领域无效，其有效性受模型架构、提示结构、领域和任务影响。

Abstract: We present a reproduction study of "Competition of Mechanisms: Tracing How
Language Models Handle Facts and Counterfactuals" (Ortu et al., 2024), which
investigates competition of mechanisms in language models between factual
recall and counterfactual in-context repetition. Our study successfully
reproduces their primary findings regarding the localization of factual and
counterfactual information, the dominance of attention blocks in mechanism
competition, and the specialization of attention heads in handling competing
information. We reproduce their results on both GPT-2 (Radford et al., 2019)
and Pythia 6.9B (Biderman et al., 2023). We extend their work in three
significant directions. First, we explore the generalizability of these
findings to even larger models by replicating the experiments on Llama 3.1 8B
(Grattafiori et al., 2024), discovering greatly reduced attention head
specialization. Second, we investigate the impact of prompt structure by
introducing variations where we avoid repeating the counterfactual statement
verbatim or we change the premise word, observing a marked decrease in the
logit for the counterfactual token. Finally, we test the validity of the
authors' claims for prompts of specific domains, discovering that certain
categories of prompts skew the results by providing the factual prediction
token as part of the subject of the sentence. Overall, we find that the
attention head ablation proposed in Ortu et al. (2024) is ineffective for
domains that are underrepresented in their dataset, and that the effectiveness
varies based on model architecture, prompt structure, domain and task.

</details>


### [478] [Objective-Free Local Learning and Emergent Language Structure in Thinking Machines](https://arxiv.org/abs/2506.23293)
*P. Myles Eugenio*

Main category: cs.CL

TL;DR: 提出基于局部、事件驱动涌现学习的神经符号框架用于生成式语言建模，有新机制和特性，推动神经形态架构发展。


<details>
  <summary>Details</summary>
Motivation: 构建能从局部神经学习中涌现符号结构的语言模型，发展可扩展、可解释的神经符号系统。

Method: 采用分层Hopfield记忆链作为组合短期记忆和动态分词器，从无到有构建结构，学习符号序列为多尺度表示，构建投影张量绑定特征。

Result: 分词器可从噪声中过滤自然语言模式，生成内部形态连贯的合成语言；激活新神经元能绑定多尺度令牌特征；架构为研究符号结构涌现提供基础。

Conclusion: 该方法为生成式语言模型的神经形态架构发展提供新思路。

Abstract: We present a neuro-symbolic framework for generative language modeling based
on local, event-driven emergent learning. At its core is a hierarchical
Hopfield memory chain acting as a compositional short-term memory and dynamic
tokenizer (retokenizer). Rather than relying on predefined tokens or
supervision, the model builds structure from scratch, learning symbol sequences
as multi-scale representations. It constructs projection tensors that bind
co-occurring features into hierarchical tokens, introducing redundancy (i.e an
emergent gauge structure) and enabling compression of local activations into
long-range dependencies. Curiously, we find that the retokenizer can filter
natural language patterns from noise, generating synthetic languages with
coherent internal morphology -- quantifiably the same as human language.
Language is learned in a local (Hebbian) fashion, where model constraints
dictate allowed emergent structure, and new information is retained in
alignment with this structure. The absence of a global objective enables a form
of plasticity not found in conventional language models, allowing the system to
generalize beyond its initial inference class -- even without explicit data. We
demonstrate that briefly activating a new neuron during inference binds
distributed multi-scale token features into a symbolic embedding. These
emergent embedding neurons act as long-term memory and support a key-value
mechanism for compositional inference and generalization. This architecture
provides a methodological foundation for studying how symbolic structure can
emerge from local neural learning. It offers a new pathway for building
scalable, interpretable neuro-symbolic systems -- where tokens, grammar, and
reasoning arise as compressed memory traces within a Hopfield hierarchy. This
approach advances the development of neuromorphic architectures for generative
language models.

</details>


### [479] [RiverText: A Python Library for Training and Evaluating Incremental Word Embeddings from Text Data Streams](https://arxiv.org/abs/2506.23192)
*Gabriel Iturra-Bocaz,Felipe Bravo-Marquez*

Main category: cs.CL

TL;DR: 介绍Python库RiverText用于从文本数据流训练和评估增量词嵌入，实现多种技术，适配评估任务并比较结果，库开源。


<details>
  <summary>Details</summary>
Motivation: 传统词嵌入模型静态特性有局限，难以适应不断变化的语言模式，需引入增量词嵌入算法。

Method: 开发RiverText库，在标准化框架实现不同增量词嵌入技术，用PyTorch作后端，实现适配现有评估任务的模块。

Result: 实现RiverText库并比较不同超参数设置下的方法。

Conclusion: RiverText库为处理流场景词嵌入的社区提供资源，代码开源可获取。

Abstract: Word embeddings have become essential components in various information
retrieval and natural language processing tasks, such as ranking, document
classification, and question answering. However, despite their widespread use,
traditional word embedding models present a limitation in their static nature,
which hampers their ability to adapt to the constantly evolving language
patterns that emerge in sources such as social media and the web (e.g., new
hashtags or brand names). To overcome this problem, incremental word embedding
algorithms are introduced, capable of dynamically updating word representations
in response to new language patterns and processing continuous data streams.
  This paper presents RiverText, a Python library for training and evaluating
incremental word embeddings from text data streams. Our tool is a resource for
the information retrieval and natural language processing communities that work
with word embeddings in streaming scenarios, such as analyzing social media.
The library implements different incremental word embedding techniques, such as
Skip-gram, Continuous Bag of Words, and Word Context Matrix, in a standardized
framework. In addition, it uses PyTorch as its backend for neural network
training. We have implemented a module that adapts existing intrinsic static
word embedding evaluation tasks for word similarity and word categorization to
a streaming setting. Finally, we compare the implemented methods with different
hyperparameter settings and discuss the results. Our open-source library is
available at https://github.com/dccuchile/rivertext.

</details>


### [480] [ATGen: A Framework for Active Text Generation](https://arxiv.org/abs/2506.23342)
*Akim Tsvigun,Daniil Vasilev,Ivan Tsvigun,Ivan Lysenko,Talgat Bektleuov,Aleksandr Medvedev,Uliana Vinogradova,Nikita Severin,Mikhail Mozikov,Andrey Savchenko,Rostislav Grigorev,Ramil Kuleev,Fedor Zhdanov,Artem Shelmanov,Ilya Makarov*

Main category: cs.CL

TL;DR: 本文提出Active Text Generation (ATGen)框架，将主动学习应用于自然语言生成任务，简化标注过程，减少人力和成本，代码开源，有视频展示。


<details>
  <summary>Details</summary>
Motivation: 主动学习在自然语言生成任务中的应用有限，需一个框架将其与文本生成任务结合。

Method: 引入ATGen框架，支持使用人类标注员和基于大语言模型的自动标注代理进行标注，支持不同部署方式的大语言模型，提供统一平台实施和评估主动学习策略。

Result: ATGen减少了人类标注员的工作量和基于大语言模型的标注代理API调用成本。

Conclusion: ATGen框架能有效将主动学习应用于自然语言生成任务，降低成本和人力。

Abstract: Active learning (AL) has demonstrated remarkable potential in reducing the
annotation effort required for training machine learning models. However,
despite the surging popularity of natural language generation (NLG) tasks in
recent years, the application of AL to NLG has been limited. In this paper, we
introduce Active Text Generation (ATGen) - a comprehensive framework that
bridges AL with text generation tasks, enabling the application of
state-of-the-art AL strategies to NLG. Our framework simplifies AL-empowered
annotation in NLG tasks using both human annotators and automatic annotation
agents based on large language models (LLMs). The framework supports LLMs
deployed as services, such as ChatGPT and Claude, or operated on-premises.
Furthermore, ATGen provides a unified platform for smooth implementation and
benchmarking of novel AL strategies tailored to NLG tasks. Finally, we present
evaluation results for state-of-the-art AL strategies across diverse settings
and multiple text generation tasks. We show that ATGen reduces both the effort
of human annotators and costs associated with API calls to LLM-based annotation
agents. The code of the framework is available on GitHub under the MIT license.
The video presentation is available at http://atgen-video.nlpresearch.group

</details>


### [481] [Perspective Dial: Measuring Perspective of Text and Guiding LLM Outputs](https://arxiv.org/abs/2506.23377)
*Taejin Kim,Siun-Chuon Mau,Konrad Vesey*

Main category: cs.CL

TL;DR: 本文探讨大语言模型输出的视角控制问题，提出Perspective - Dial方法，可量化和调整输出视角，有多种潜在应用。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型发展迅速，缺乏对其输出的偏差和视角的量化理解。

Method: Perspective - Dial包含两个主要部分：一是名为Perspective Space的度量空间，用于对主题的不同视角进行定量测量；二是利用贪婪坐标下降的系统提示工程，根据Perspective Space的测量反馈控制大语言模型的输出视角。

Result: 该方法可有效对各种主题的输出进行量化和调整。

Conclusion: 此方法可用于检测、跟踪和缓解大语言模型的偏差，以及叙事检测、公共话语理解和跟踪、辩论机器人等潜在应用。

Abstract: Large language models (LLMs) are used in a variety of mission-critical roles.
Due to the rapidly developing nature of LLMs, there is a lack of quantifiable
understanding of the bias and perspective associated with LLM output. Inspired
by this need, this paper considers the broader issue of perspective or
viewpoint of general text and perspective control of large-language model (LLM)
output. Perspective-Dial consists of two main components: a (1) metric space,
dubbed Perspective Space, that enables quantitative measurements of different
perspectives regarding a topic, and the use of (2) Systematic Prompt
Engineering that utilizes greedy-coordinate descent to control LLM output
perspective based on measurement feedback from the Perspective Space. The
empirical nature of the approach allows progress to side step a principled
understanding of perspective or bias -- effectively quantifying and adjusting
outputs for a variety of topics. Potential applications include detection,
tracking and mitigation of LLM bias, narrative detection, sense making and
tracking in public discourse, and debate bot advocating given perspective.

</details>


### [482] [Ensemble BERT for Medication Event Classification on Electronic Health Records (EHRs)](https://arxiv.org/abs/2506.23315)
*Shouvon Sarker,Xishuang Dong,Lijun Qian*

Main category: cs.CL

TL;DR: 本文围绕n2c2 2022临床数据分析自然语言处理挑战，构建基于BERT的集成模型检测和分类临床笔记中的用药事件，实验表明该模型能有效提升F分数。


<details>
  <summary>Details</summary>
Motivation: 从健康记录和临床笔记中识别关键变量在临床领域有广泛应用，n2c2 2022提供相关挑战任务，本文聚焦子任务2。

Method: 在不同大数据上预训练BERT模型，在CMED训练数据上微调，对CMED测试数据进行多预测，用投票策略集成预测结果。

Result: 基于BERT的集成模型能有效提高严格Micro - F分数约5%和严格Macro - F分数约6%。

Conclusion: 基于BERT的集成模型可有效用于检测和分类临床笔记中的用药事件。

Abstract: Identification of key variables such as medications, diseases, relations from
health records and clinical notes has a wide range of applications in the
clinical domain. n2c2 2022 provided shared tasks on challenges in natural
language processing for clinical data analytics on electronic health records
(EHR), where it built a comprehensive annotated clinical data Contextualized
Medication Event Dataset (CMED). This study focuses on subtask 2 in Track 1 of
this challenge that is to detect and classify medication events from clinical
notes through building a novel BERT-based ensemble model. It started with
pretraining BERT models on different types of big data such as Wikipedia and
MIMIC. Afterwards, these pretrained BERT models were fine-tuned on CMED
training data. These fine-tuned BERT models were employed to accomplish
medication event classification on CMED testing data with multiple predictions.
These multiple predictions generated by these fine-tuned BERT models were
integrated to build final prediction with voting strategies. Experimental
results demonstrated that BERT-based ensemble models can effectively improve
strict Micro-F score by about 5% and strict Macro-F score by about 6%,
respectively.

</details>


### [483] [Hierarchical Memory Organization for Wikipedia Generation](https://arxiv.org/abs/2506.23393)
*Eugene J. Yu,Dawei Zhu,Yifan Song,Xiangyu Wong,Jiebin Zhang,Wenxuan Shi,Xiaoguang Li,Qun Liu,Sujian Li*

Main category: cs.CL

TL;DR: 本文提出基于内存组织的生成（MOG）框架，利用分层内存架构解决自动生成维基百科文章的挑战，评估显示其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 自动生成维基百科文章需整合多源信息，具有挑战性，需新方法解决。

Method: 提出MOG框架，从网页文档提取细粒度记忆单元，递归组织成维基百科式分层结构，用其指导生成过程，还实现引用模块。

Result: 在新创建的WikiStart数据集上评估表明，MOG在生成信息丰富且可靠的文章方面优于基线方法。

Conclusion: MOG在生成维基百科文章方面表现良好，在现实场景中很稳健。

Abstract: Generating Wikipedia articles autonomously is a challenging task requiring
the integration of accurate, comprehensive, and well-structured information
from diverse sources. This paper introduces the Memory Organization-based
Generation (MOG) framework, a novel approach to address these challenges by
leveraging a hierarchical memory architecture. MOG extracts fine-grained memory
units from web documents, recursively organizes them into a Wikipedia-style
hierarchical structure, and uses this structure to guide the generation
process. This ensures alignment between memory and the article outline,
improving both informativeness and verifiability while minimizing
hallucinations. Additionally, a citation module is implemented to enhance
traceability by linking every generated sentence to specific memory units.
Evaluations on our newly created WikiStart dataset demonstrate that MOG
outperforms baseline methods in producing informative and reliable articles,
making it particularly robust in real-world scenarios.

</details>


### [484] [TuCo: Measuring the Contribution of Fine-Tuning to Individual Responses of LLMs](https://arxiv.org/abs/2506.23423)
*Felipe Nuti,Tim Franzmeyer,João Henriques*

Main category: cs.CL

TL;DR: 提出测量微调对大语言模型个体响应贡献的新方法TuCo，发现对抗攻击与TuCo的关系，可定量研究微调对模型行为和安全的影响。


<details>
  <summary>Details</summary>
Motivation: 缺乏定量系统分析微调对大语言模型个体输出影响的方法。

Method: 追踪模型中间隐藏状态，将微调后的大语言模型分解为预训练和微调组件，定义Tuning Contribution (TuCo)。

Result: 可通过缩放微调组件控制模型行为和性能，发现对抗攻击会降低TuCo。

Conclusion: TuCo可定量研究微调对模型行为和安全的影响，反之亦然。

Abstract: Past work has studied the effects of fine-tuning on large language models'
(LLMs) overall performance on certain tasks. However, a quantitative and
systematic method for analyzing its effect on individual outputs is still
lacking. Here, we propose a new method for measuring the contribution that
fine-tuning makes to individual LLM responses, assuming access to the original
pre-trained model. Our method tracks the model's intermediate hidden states,
providing a more fine-grained insight into the effects of fine-tuning than a
simple comparison of final outputs from pre-trained and fine-tuned models. We
introduce and theoretically analyze an exact decomposition of any fine-tuned
LLM into a pre-training component and a fine-tuning component. Empirically, we
find that model behavior and performance can be steered by up- or down-scaling
the fine-tuning component during the forward pass. Motivated by this finding
and our theoretical analysis, we define the Tuning Contribution (TuCo) as the
ratio of the magnitudes of the fine-tuning component to the pre-training
component. We observe that three prominent adversarial attacks on LLMs
circumvent safety measures in a way that reduces TuCo, and that TuCo is
consistently lower on prompts where these attacks succeed compared to those
where they do not. This suggests that attenuating the effect of fine-tuning on
model outputs plays a role in the success of such attacks. In summary, TuCo
enables the quantitative study of how fine-tuning influences model behavior and
safety, and vice versa.

</details>


### [485] [Pipelined Decoder for Efficient Context-Aware Text Generation](https://arxiv.org/abs/2506.23431)
*Zixian Huang,Chenxu Niu,Yu Gu,Gengyang Xiao,Xinwei Huang,Gong Cheng*

Main category: cs.CL

TL;DR: 提出新的解码器架构实现文本并行生成，提升生成速度且不损失质量和增加内存消耗。


<details>
  <summary>Details</summary>
Motivation: 自回归模型逐个生成标记限制了生成速度，需要提升生成效率。

Method: 提出流水线解码器，同时启动多个子序列生成，每个时间步为每个子序列生成新标记。

Result: 在多个文本生成任务中，流水线解码器显著提高生成速度，且无明显质量损失和额外内存消耗。

Conclusion: 新的解码器架构能有效实现上下文感知生成任务的并行文本生成。

Abstract: As the basis of generative AI, an autoregressive model requires the
generation of a new token depending on all the previously generated tokens,
which brings high quality but also restricts the model to generate tokens one
by one, forming a bottleneck limiting the generation speed. In this paper, we
propose a new decoder architecture that efficiently generates text in parallel
for context-aware generation tasks. Our proposed pipelined decoder initiates
the generation of multiple subsequences simultaneously, and, at each time-step,
it generates a new token for each subsequence to realize parallelism.
Experiments on multiple text generation tasks, including question answering,
text summarization, and keyphrase generation, show that our pipelined decoder
significantly improves the generation speed without a significant loss of
generation quality or additional memory consumption.

</details>


### [486] [Datasets for Fairness in Language Models: An In-Depth Survey](https://arxiv.org/abs/2506.23411)
*Jiale Zhang,Zichong Wang,Avash Palikhe,Zhipeng Yin,Wenbin Zhang*

Main category: cs.CL

TL;DR: 本文对语言模型公平性数据集进行全面审查，引入统一评估框架，指出数据集偏差并提供使用指导，还提及创建新基准的机会。


<details>
  <summary>Details</summary>
Motivation: 现有研究对语言模型公平性基准所依赖的数据集关注不足，需对其进行审查。

Method: 对常用公平性数据集进行多维度表征，引入统一评估框架并应用于24个常见基准。

Result: 揭示了数据集和评分方法中的人口统计学差异模式，指出影响模型公平性结论的偏差。

Conclusion: 为数据集的选择、组合和解释提供实用指导，提出创建反映更多样社会背景新基准的机会。

Abstract: Fairness benchmarks play a central role in shaping how we evaluate language
models, yet surprisingly little attention has been given to examining the
datasets that these benchmarks rely on. This survey addresses that gap by
presenting a broad and careful review of the most widely used fairness datasets
in current language model research, characterizing them along several key
dimensions including their origin, scope, content, and intended use to help
researchers better appreciate the assumptions and limitations embedded in these
resources. To support more meaningful comparisons and analyses, we introduce a
unified evaluation framework that reveals consistent patterns of demographic
disparities across datasets and scoring methods. Applying this framework to
twenty four common benchmarks, we highlight the often overlooked biases that
can influence conclusions about model fairness and offer practical guidance for
selecting, combining, and interpreting these datasets. We also point to
opportunities for creating new fairness benchmarks that reflect more diverse
social contexts and encourage more thoughtful use of these tools going forward.
All code, data, and detailed results are publicly available at
https://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/datasets
to promote transparency and reproducibility across the research community.

</details>


### [487] [Reinforcement Fine-Tuning Enables MLLMs Learning Novel Tasks Stably](https://arxiv.org/abs/2506.23508)
*Zhihao Zhang,Qiaole Dong,Qi Zhang,Jun Zhao,Enyu Zhou,Zhiheng Xi,Senjie Jin,Xiaoran Fan,Yuhao Zhou,Yanwei Fu,Tao Ji,Tao Gui,Xuanjing Huang*

Main category: cs.CL

TL;DR: 研究SFT和RFT在多模态大语言模型上对新知识学习和旧知识保留的影响，发现数据分布是遗忘关键，RFT有稳定持续学习潜力。


<details>
  <summary>Details</summary>
Motivation: 现有后训练算法（SFT和RFT）对多模态大语言模型下游任务适配有效，但对模型先验知识的影响不明确。

Method: 引入拼图任务，在Qwen2.5 - VL模型上系统研究SFT和RFT行为，并从学习动力学角度分析现象。

Result: SFT能快速学习新任务但导致灾难性遗忘，RFT学习新任务慢但保留先验知识，对RFT模拟的正确样本进行监督训练可让SFT保留知识并快速学习新任务。

Conclusion: 数据分布而非算法差异是遗忘的核心因素，RFT在多模态大语言模型稳定持续学习中有潜力。

Abstract: Post-training algorithms such as Supervised Fine-Tuning (SFT) and
Reinforcement Fine-Tuning (RFT) are widely used to adapt multimodal large
language models to downstream tasks. While effective at task adaptation, their
impact on prior knowledge remains unclear. In this paper, we introduce jigsaw
puzzles as a novel task absent from existing pretraining corpora and
systematically study the behavior of SFT and RFT on an open-source multimodal
model, Qwen2.5-VL. Our experiments reveal a sharp trade-off: SFT enables rapid
task acquisition but leads to catastrophic forgetting, whereas RFT learns more
slowly on novel tasks but maintains prior knowledge. We analyze this phenomenon
through the lens of learning dynamics, showing that RFT reinforces correct
samples that are naturally aligned with the base model's probability landscape,
mitigating interference with prior knowledge. Moreover, supervised training on
correct RFT-simulated rollouts allows SFT to preserve knowledge while rapidly
learning new tasks. These findings suggest that data distribution, rather than
algorithmic differences, plays a central role in forgetting, and highlight
RFT's potential for stable continual learning in multimodal large language
models.

</details>


### [488] [NEU-ESC: A Comprehensive Vietnamese dataset for Educational Sentiment analysis and topic Classification toward multitask learning](https://arxiv.org/abs/2506.23524)
*Phan Quoc Hung Mai,Quang Hung Nguyen,Phuong Giang Duong,Hong Hanh Nguyen,Nguyen Tuan Long*

Main category: cs.CL

TL;DR: 引入越南语教育情感与主题分类数据集NEU-ESC，用BERT进行多任务学习并达一定准确率，公开数据集。


<details>
  <summary>Details</summary>
Motivation: 现有教育数据集缺乏领域相关性和学生俚语，越南语相关资源有限，需新数据集。

Method: 从大学论坛整理出新数据集NEU-ESC，使用基于编码器的语言模型BERT进行多任务学习，对数据集和模型进行基准测试。

Result: 情感和主题分类任务准确率分别达83.7%和79.8%。

Conclusion: 新数据集和多任务学习方法有一定效果，数据集已公开。

Abstract: In the field of education, understanding students' opinions through their
comments is crucial, especially in the Vietnamese language, where resources
remain limited. Existing educational datasets often lack domain relevance and
student slang. To address these gaps, we introduce NEU-ESC, a new Vietnamese
dataset for Educational Sentiment Classification and Topic Classification,
curated from university forums, which offers more samples, richer class
diversity, longer texts, and broader vocabulary. In addition, we explore
multitask learning using encoder-only language models (BERT), in which we
showed that it achieves performance up to 83.7% and 79.8% accuracy for
sentiment and topic classification tasks. We also benchmark our dataset and
model with other datasets and models, including Large Language Models, and
discuss these benchmarks. The dataset is publicly available at:
https://huggingface.co/datasets/hung20gg/NEU-ESC.

</details>


### [489] [Semantic-guided Diverse Decoding for Large Language Model](https://arxiv.org/abs/2506.23601)
*Weijie Shi,Yue Cui,Yaguang Wu,Jingzhi Fang,Shibo Zhang,Mengze Li,Sirui Han,Jia Zhu,Jiajie Xu,Xiaofang Zhou*

Main category: cs.CL

TL;DR: 现有大语言模型解码方法难实现语义多样性，提出SemDiD方法，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型多样化解码方法主要实现词汇而非语义多样性，限制了一些策略和应用。

Method: 引入SemDiD方法，在嵌入空间通过正交方向引导、动态组间排斥和位置无偏概率评估三种机制平衡质量和多样性，用自适应增益函数和约束优化协调目标。

Result: SemDiD在不同任务中使Best-of-N覆盖率提高1.4 - 5.2%，加速RLHF训练收敛15%，并使准确率提高达2.1%。

Conclusion: SemDiD方法能有效解决现有方法语义多样性不足问题，优于现有方法。

Abstract: Diverse decoding of large language models is crucial for applications
requiring multiple semantically distinct responses, yet existing methods
primarily achieve lexical rather than semantic diversity. This limitation
significantly constrains Best-of-N strategies, group-based reinforcement
learning, and data synthesis. While temperature sampling and diverse beam
search modify token distributions or apply n-gram penalties, they fail to
ensure meaningful semantic differentiation. We introduce Semantic-guided
Diverse Decoding (SemDiD), operating directly in embedding space that balances
quality with diversity through three complementary mechanisms: orthogonal
directional guidance, dynamic inter-group repulsion, and position-debiased
probability assessment. SemDiD harmonizes these competing objectives using
adaptive gain functions and constraint optimization, ensuring both quality
thresholds and maximal semantic differentiation. Experiments show SemDiD
consistently outperforms existing methods, improving Best-of-N coverage by
1.4-5.2% across diverse tasks and accelerating RLHF training convergence by 15%
while increasing accuracy by up to 2.1%.

</details>


### [490] [AutoEvoEval: An Automated Framework for Evolving Close-Ended LLM Evaluation Data](https://arxiv.org/abs/2506.23735)
*JiaRu Wu,Mingwei Liu*

Main category: cs.CL

TL;DR: 现有评估基准不足以评估大语言模型在现实场景中的鲁棒性和泛化能力，本文提出AutoEvoEval框架进行评估，实验表明当前基准可能高估模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有评估基准静态，无法全面评估大语言模型在现实场景的鲁棒性和泛化能力，先前方法缺乏对扰动类型和多步复杂度的系统控制。

Method: 提出AutoEvoEval框架，引入22种可解释原子进化操作并支持多轮组合，用于生成多样、具挑战性和现实性的测试样本。

Result: 原子操作使平均准确率下降7.283%，结构破坏或误导性语义编辑下降最大，模型对相同扰动敏感性差异大，多步进化组合使对抗效果放大最多52.932%。

Conclusion: 当前基准可能高估模型真实泛化能力，强调需要进化感知的鲁棒性评估。

Abstract: Large language models (LLMs) have shown remarkable performance on various
tasks, but existing evaluation benchmarks are often static and insufficient to
fully assess their robustness and generalization in realistic scenarios. Prior
work using evolutionary or adversarial data augmentation has improved
evaluation diversity but lacks systematic control over perturbation types and
multi-step complexity, limiting comprehensive robustness analysis. To address
these gaps, we propose AutoEvoEval, an evolution-based evaluation framework for
close-ended tasks such as multi-choice question answering. AutoEvoEval
introduces 22 interpretable atomic evolution operations and supports
multi-round compositions, enabling controlled generation of diverse,
challenging, and realistic test samples. We conduct extensive experiments
addressing four research questions on a broad set of open- and closed-source
LLMs. Our results show that atomic operations cause an average accuracy drop of
7.283\%, with structure-disrupting or misleading semantic edits causing the
largest declines. Model sensitivities vary significantly for the same
perturbation, and combining multiple evolution steps amplifies adversarial
effects by up to 52.932\%. These findings suggest current benchmarks may
overestimate true model generalization and emphasize the need for
evolution-aware robustness evaluation. Code and resources are available at:
https://github.com/SYSUSELab/AutoEvoEval.

</details>


### [491] [Machine Understanding of Scientific Language](https://arxiv.org/abs/2506.23990)
*Dustin Wright*

Main category: cs.CL

TL;DR: 论文聚焦培养用于机器理解科学语言的数据集、方法和工具，在NLP和ML三个领域作出贡献，可助力识别错误科学陈述及洞察科学传播过程。


<details>
  <summary>Details</summary>
Motivation: 随着科学文本在网络上激增，自动识别科学文本忠实性成为重要社会问题，需培养相关数据集、方法和工具。

Method: 在自然语言处理和机器学习的自动事实核查、有限数据学习、科学文本处理三个领域提出新方法和资源。

Result: 得到新方法和资源，可用于识别值得核查的声明、对抗性声明生成等多方面。

Conclusion: 研究成果有助于从有限科学文本中有效学习，识别错误科学陈述，洞察科学传播过程。

Abstract: Scientific information expresses human understanding of nature. This
knowledge is largely disseminated in different forms of text, including
scientific papers, news articles, and discourse among people on social media.
While important for accelerating our pursuit of knowledge, not all scientific
text is faithful to the underlying science. As the volume of this text has
burgeoned online in recent years, it has become a problem of societal
importance to be able to identify the faithfulness of a given piece of
scientific text automatically. This thesis is concerned with the cultivation of
datasets, methods, and tools for machine understanding of scientific language,
in order to analyze and understand science communication at scale. To arrive at
this, I present several contributions in three areas of natural language
processing and machine learning: automatic fact checking, learning with limited
data, and scientific text processing. These contributions include new methods
and resources for identifying check-worthy claims, adversarial claim
generation, multi-source domain adaptation, learning from crowd-sourced labels,
cite-worthiness detection, zero-shot scientific fact checking, detecting
exaggerated scientific claims, and modeling degrees of information change in
science communication. Critically, I demonstrate how the research outputs of
this thesis are useful for effectively learning from limited amounts of
scientific text in order to identify misinformative scientific statements and
generate new insights into the science communication process

</details>


### [492] [Do Thinking Tokens Help or Trap? Towards More Efficient Large Reasoning Model](https://arxiv.org/abs/2506.23840)
*Bowen Ding,Yuhan Chen,Futing Wang,Lingfeng Ming,Tao Lin*

Main category: cs.CL

TL;DR: 大型推理模型存在过度思考困境，本文提出DuP - PO算法缓解此问题，实验表明该算法能提高推理时的令牌效率并提升基础模型性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型处理简单任务时存在过度思考困境，产生大量思考令牌触发不必要的高级推理行为，降低效率，还可能阻碍正确推理。

Method: 提出Dual Policy Preference Optimization (DuP - PO)算法，包括确保有和无思考令牌响应平衡曝光的滚动采样策略、动态调节目标令牌预测的细粒度优势控制技术、保证思考令牌稳定梯度贡献的策略塑造方法。

Result: 在五个流行的数学推理基准测试中，DuP - PO在流行的大型推理模型上表现良好，显著提高了推理时的令牌效率，且基础模型性能更优。

Conclusion: DuP - PO算法能有效缓解大型推理模型的过度思考困境，提高推理效率和模型性能。

Abstract: Large Reasoning Models (LRMs) excel at solving complex problems but face an
overthinking dilemma. When handling simple tasks, they often produce verbose
responses overloaded with thinking tokens (e.g., wait, however). These tokens
trigger unnecessary high-level reasoning behaviors like reflection and
backtracking, reducing efficiency. In this work, our pilot study reveals that
these thinking-token-induced behaviors are not essential for effective
problem-solving and may even hinder correct reasoning within constrained token
budgets. We identify this phenomenon as the thinking trap. To mitigate this
issue, we propose Dual Policy Preference Optimization (DuP-PO), a novel
algorithm featuring: (1) A rollout sampling strategy that guarantees balanced
exposure to responses with and without thinking tokens; (2) A fine-grained
advantage control technique to dynamically regulate the prediction of target
tokens; (3) A policy shaping method ensuring stable gradient contributions from
thinking tokens. Experimental results on five popular math reasoning benchmarks
show that DuP-PO performs well on the popular LRM, which significantly improves
their token efficiency during reasoning, while achieving superior performance
of the base model.

</details>


### [493] [Leveraging the Potential of Prompt Engineering for Hate Speech Detection in Low-Resource Languages](https://arxiv.org/abs/2506.23930)
*Ruhina Tabasshum Prome,Tarikul Islam Tamiti,Anomadarshi Barua*

Main category: cs.CL

TL;DR: 社交媒体仇恨言论增多，低资源语言检测有挑战。本文用六种提示策略在Llama2 - 7B模型检测低资源孟加拉语仇恨言论，还在多语言验证，用F1和IF评估。


<details>
  <summary>Details</summary>
Motivation: 社交媒体仇恨言论威胁大，低资源语言因缺乏数据集在仇恨言论检测上面临挑战，需新方法解决。

Method: 研究六种提示策略（含创新隐喻提示）在Llama2 - 7B模型检测仇恨言论，与三种预训练词嵌入在三种深度学习模型对比，在多语言验证隐喻提示效果，用F1和IF评估。

Result: 未提及具体结果

Conclusion: 未提及明确结论

Abstract: The rapid expansion of social media leads to a marked increase in hate
speech, which threatens personal lives and results in numerous hate crimes.
Detecting hate speech presents several challenges: diverse dialects, frequent
code-mixing, and the prevalence of misspelled words in user-generated content
on social media platforms. Recent progress in hate speech detection is
typically concentrated on high-resource languages. However, low-resource
languages still face significant challenges due to the lack of large-scale,
high-quality datasets. This paper investigates how we can overcome this
limitation via prompt engineering on large language models (LLMs) focusing on
low-resource Bengali language. We investigate six prompting strategies -
zero-shot prompting, refusal suppression, flattering the classifier, multi-shot
prompting, role prompting, and finally our innovative metaphor prompting to
detect hate speech effectively in low-resource languages. We pioneer the
metaphor prompting to circumvent the built-in safety mechanisms of LLMs that
marks a significant departure from existing jailbreaking methods. We
investigate all six different prompting strategies on the Llama2-7B model and
compare the results extensively with three pre-trained word embeddings - GloVe,
Word2Vec, and FastText for three different deep learning models - multilayer
perceptron (MLP), convolutional neural network (CNN), and bidirectional gated
recurrent unit (BiGRU). To prove the effectiveness of our metaphor prompting in
the low-resource Bengali language, we also evaluate it in another low-resource
language - Hindi, and two high-resource languages - English and German. The
performance of all prompting techniques is evaluated using the F1 score, and
environmental impact factor (IF), which measures CO$_2$ emissions, electricity
usage, and computational time.

</details>


### [494] [EXPERT: An Explainable Image Captioning Evaluation Metric with Structured Explanations](https://arxiv.org/abs/2506.24016)
*Hyunjong Kim,Sangyeop Kim,Jongheon Jeong,Yeongjae Cho,Sungzoon Cho*

Main category: cs.CL

TL;DR: 提出无参考评估指标EXPERT，基于三个标准提供结构化解释，在基准数据集获最优结果，解释质量更高。


<details>
  <summary>Details</summary>
Motivation: 现有图像描述可解释评估指标无标准化标准，生成解释质量未经验证。

Method: 构建高质量结构化解释的大规模数据集，开发两阶段评估模板监督视觉语言模型进行评分和解释生成。

Result: EXPERT在基准数据集上达到了最先进的结果，通过全面的人工评估验证，其解释质量明显高于现有指标。

Conclusion: EXPERT是一种有效的无参考评估指标，可提供高质量的结构化解释。

Abstract: Recent advances in large language models and vision-language models have led
to growing interest in explainable evaluation metrics for image captioning.
However, these metrics generate explanations without standardized criteria, and
the overall quality of the generated explanations remains unverified. In this
paper, we propose EXPERT, a reference-free evaluation metric that provides
structured explanations based on three fundamental criteria: fluency,
relevance, and descriptiveness. By constructing large-scale datasets of
high-quality structured explanations, we develop a two-stage evaluation
template to effectively supervise a vision-language model for both scoring and
explanation generation. EXPERT achieves state-of-the-art results on benchmark
datasets while providing significantly higher-quality explanations than
existing metrics, as validated through comprehensive human evaluation. Our code
and datasets are available at https://github.com/hjkim811/EXPERT.

</details>


### [495] [STACK: Adversarial Attacks on LLM Safeguard Pipelines](https://arxiv.org/abs/2506.24068)
*Ian R. McKenzie,Oskar J. Hollinsworth,Tom Tseng,Xander Davies,Stephen Casper,Aaron D. Tucker,Robert Kirk,Adam Gleave*

Main category: cs.CL

TL;DR: 本文开发并红队测试了开源防御管道，对比分类器性能，提出STACK攻击程序并评估，最后给出应对建议。


<details>
  <summary>Details</summary>
Motivation: 前沿AI开发者使用防御管道保护AI系统，但此类管道安全性不明且缺乏评估和攻击研究。

Method: 开发并红队测试开源防御管道，对比新型少样本提示的输入输出分类器和ShieldGemma性能，提出STACK攻击程序并在黑盒攻击和迁移场景中评估。

Result: 新型分类器在三个攻击和两个数据集上表现优于ShieldGemma，将ClearHarm数据集攻击成功率降至0%；STACK程序在黑盒攻击中成功率达71%，迁移场景中达33%。

Conclusion: 建议开发者采用特定缓解措施来抵御分阶段攻击。

Abstract: Frontier AI developers are relying on layers of safeguards to protect against
catastrophic misuse of AI systems. Anthropic guards their latest Claude 4 Opus
model using one such defense pipeline, and other frontier developers including
Google DeepMind and OpenAI pledge to soon deploy similar defenses. However, the
security of such pipelines is unclear, with limited prior work evaluating or
attacking these pipelines. We address this gap by developing and red-teaming an
open-source defense pipeline. First, we find that a novel few-shot-prompted
input and output classifier outperforms state-of-the-art open-weight safeguard
model ShieldGemma across three attacks and two datasets, reducing the attack
success rate (ASR) to 0% on the catastrophic misuse dataset ClearHarm. Second,
we introduce a STaged AttaCK (STACK) procedure that achieves 71% ASR on
ClearHarm in a black-box attack against the few-shot-prompted classifier
pipeline. Finally, we also evaluate STACK in a transfer setting, achieving 33%
ASR, providing initial evidence that it is feasible to design attacks with no
access to the target pipeline. We conclude by suggesting specific mitigations
that developers could use to thwart staged attacks.

</details>


### [496] [On the Predictive Power of Representation Dispersion in Language Models](https://arxiv.org/abs/2506.24106)
*Yanhong Li,Ming Li,Karen Livescu,Jiawei Zhou*

Main category: cs.CL

TL;DR: 研究表明语言模型预测文本能力与嵌入空间广度相关，表征分散度与困惑度负相关，并展示了分散度在无监督任务中的应用。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型预测文本能力与嵌入空间的关系，挖掘分散度在实际任务中的应用价值。

Method: 测量不同模型家族和领域中表征分散度与困惑度的相关性，利用分散度进行下游准确率预测、确定检索方法最佳表征层，在训练中加入push - away目标。

Result: 表征分散度与困惑度强负相关；可利用分散度预测新领域下游准确率、确定检索最佳表征层；加入push - away目标可提高分散度和降低困惑度。

Conclusion: 语言模型预测能力与嵌入空间广度紧密相连，分散度可用于多种无监督实际任务。

Abstract: We show that a language model's ability to predict text is tightly linked to
the breadth of its embedding space: models that spread their contextual
representations more widely tend to achieve lower perplexity. Concretely, we
find that representation dispersion - the average pairwise cosine distance
among hidden vectors - strongly and negatively correlates with perplexity
across diverse model families (LLaMA, Qwen, and others) and domains (Wikipedia,
news, scientific abstracts). Beyond illustrating this link, we show how
dispersion can be leveraged for a range of practical tasks without requiring
labeled data. First, measuring dispersion on unlabeled text allows us to
predict downstream accuracy in new domains, offering a data-efficient tool for
model selection. Next, we find that identifying layers with higher dispersion
pinpoints the best representations for retrieval-based methods such as kNN-LM,
bypassing exhaustive layer-by-layer searches. Finally, we integrate a simple
push-away objective into training, which increases dispersion in both
single-domain and cross-domain scenarios and directly improves perplexity in
each.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [497] [Towards the "Digital Me": A vision of authentic Conversational Agents powered by personal Human Digital Twins](https://arxiv.org/abs/2506.23826)
*Lluís C. Coll,Martin W. Lauer-Schmaltz,Philip Cash,John P. Hansen,Anja Maier*

Main category: cs.ET

TL;DR: 本文介绍了一种新的人类数字孪生（HDT）系统架构，集成大语言模型和动态更新的个人数据，实现更自然互动，但也引发伦理问题。


<details>
  <summary>Details</summary>
Motivation: 利用对话式AI的进展，让HDT成为个人真实、交互式的数字对应物。

Method: 采用上下文感知的记忆检索、受神经可塑性启发的巩固和自适应学习机制。

Result: 系统能复制个人对话风格，用动态捕捉的个人经历等丰富回复。

Conclusion: 该研究为HDT领域做出贡献，讨论了未来方向和伦理挑战以确保其负责任和道德的发展。

Abstract: Human Digital Twins (HDTs) have traditionally been conceptualized as
data-driven models designed to support decision-making across various domains.
However, recent advancements in conversational AI open new possibilities for
HDTs to function as authentic, interactive digital counterparts of individuals.
This paper introduces a novel HDT system architecture that integrates large
language models with dynamically updated personal data, enabling it to mirror
an individual's conversational style, memories, and behaviors. To achieve this,
our approach implements context-aware memory retrieval, neural
plasticity-inspired consolidation, and adaptive learning mechanisms, creating a
more natural and evolving digital persona. The resulting system does not only
replicate an individual's unique conversational style depending on who they are
speaking with, but also enriches responses with dynamically captured personal
experiences, opinions, and memories. While this marks a significant step toward
developing authentic virtual counterparts, it also raises critical ethical
concerns regarding privacy, accountability, and the long-term implications of
persistent digital identities. This study contributes to the field of HDTs by
describing our novel system architecture, demonstrating its capabilities, and
discussing future directions and emerging challenges to ensure the responsible
and ethical development of HDTs.

</details>


<div id='math.NT'></div>

# math.NT [[Back]](#toc)

### [498] [A Rigorous Error Bound for the TG Kernel in Prime Counting](https://arxiv.org/abs/2506.22634)
*Bugra Kilictas,Faruk Alpay*

Main category: math.NT

TL;DR: 本文在显式公式框架下，用截断高斯核为素数计数建立误差界，能保证精确计算，只需少量非平凡零点，方法具创新性和实用性。


<details>
  <summary>Details</summary>
Motivation: 在不依赖未证明假设的情况下，为素数计数建立严格误差界，实现精确计算。

Method: 采用具紧支撑的类高斯测试函数构建截断高斯核，用泰勒余项分析给出显式尾部截断界，通过无条件密度估计给出零和截断误差界，严格处理平凡零点贡献。

Result: 证明对足够大自变量近似误差全局低于1/2，对10^8位十进制数x，约1200个非平凡零点即可达误差界，可在现代硬件上秒级计算。

Conclusion: 该方法架起解析数论与实际计算桥梁，有潜在应用，严谨误差分析为计算数论研究开辟新途径。

Abstract: We establish rigorous error bounds for prime counting using a truncated
Gaussian (TG) kernel in the explicit formula framework. Our main theorem proves
that the approximation error remains globally below 1/2 for all sufficiently
large arguments, guaranteeing exact computation of {\pi}(x) through simple
rounding, without relying on unproven hypotheses.
  The TG kernel construction employs Gaussian-like test functions with compact
support, engineered with vanishing moments to eliminate main terms. For x with
10^8 decimal digits, we demonstrate that only ~1200 nontrivial zeta zeros
suffice to achieve the error bound, enabling computation in seconds on modern
hardware - a dramatic improvement over classical methods.
  Key contributions include: (1) Explicit tail truncation bounds using Taylor
remainder analysis, showing exponential decay; (2) Zero-sum truncation error
bounds via unconditional density estimates; (3) Rigorous treatment of trivial
zero contributions. All constants are made explicit, ensuring full
verifiability.
  The method bridges analytic number theory and practical computation, with
potential applications to record-breaking prime counting computations. We
discuss algorithmic implications including FFT-based arithmetic for ~330
million bit numbers. The framework's flexibility suggests connections to deeper
structures in prime distribution, particularly regarding optimized kernel
designs and the interplay between smoothing parameters {\alpha} and truncation
heights.
  This work exemplifies how classical analytic techniques, when carefully
implemented with modern computational perspectives, yield practical algorithms
for problems previously considered purely theoretical. The rigorous error
analysis ensures reliability even at astronomical scales, opening new avenues
for computational number theory research.

</details>
